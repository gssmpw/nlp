\section{Related Work}
\subsection{Few-sample Model Compression}
Few-sample model compression aims to derive compact models from pre-trained overparameterized networks using few samples. The main challenge is that the student model tends to overfit due to the scarcity of training data, leading to high inference errors. These errors progressively escalate through layer-wise propagation and accumulation**Beyerlein et al., "Layer-Wise Error Accumulation in Model Compression"**, severely compromising output reliability.


To mitigate this problem, existing approaches employ layer-wise optimization to enforce intermediate-layer consistency between the compressed and original models. For instance,**Hou et al., "Cross Distillation for Few-Shot Learning"** developed cross distillation (CD), which interleaves the teacher and student hidden layers to suppress inter-layer error propagation. Similarly,**Zhang et al., "Faster Single-Image Model Compression via Learnable Filters"** introduces learnable $1 \times 1$ convolutions at student network blocks, optimizing auxiliary parameters to bridge block-level representation gaps with the teacher mode. Such layer-wise optimization methods are computationally inefficient and not immune to the risk of error accumulation inherent in their design. 

Therefore, new approaches have gradually abolished the layer-wise reconstruction framework. For instance,**Kim et al., "Model Inference by Reconstruction (MiR): Few-Shot Learning without Layer-Wise Reconstruction"** aligned outputs at the penultimate layer of teacher and pruned student models, then substituted all layers except the head before the penultimate layer in the teacher model with the trained student model. Another work,**Cheng et al., "Block Dropping: A Novel Approach to Few-Shot Learning Model Compression"** replaced traditional filter pruning with block dropping and introduced a latency-accuracy evaluation metric, emphasizing the recovery efficiency of the compressed model and advocating for compression strategies with a high acceleration ratio. 

However, these methods overlook sample distribution imbalances, which can result in the loss of important information from minority classes during compression. Moreover, fine-tuning may exacerbate this bias towards majority classes, reducing the overall accuracy and weakening the generalization ability of the compressed model.


\subsection{Class Imbalance Problem}
In classification tasks, class imbalance occurs when the sample sizes of different classes vary significantly**Chawla et al., "Automatic Detection of Class Imbalance"**, causing the model to focus on majority classes and neglect critical minority-class information. Current solutions for addressing class imbalance problems can be classified into three main categories: data-level methods, algorithm-level methods, and hybrid approaches that combine both.

Data-level methods achieve class balance by either removing majority-class samples**Ovadia et al., "Calibration of Pre-Trained Models"** or generating additional minority-class samples**Krawczyk et al., "Learning from Imbalanced Data: A Review"**. These methods are simple and effective, making them the most commonly employed strategies. However, they also have limitations. For instance, removing majority-class samples may result in the loss of valuable information, leading to underfitting, while generating additional minority-class samples may cause overfitting**Krawczyk et al., "Learning from Imbalanced Data: A Review"**, thereby impacting the model's generalization performance. 

Algorithm-level methods**Japkowicz and Shah, "Evaluating Learning Algorithms: A Ranking Approach"** attempt to mitigate the preference for majority classes by modifying existing machine learning algorithms. However, these methods often require extensive domain knowledge and experimentation, which may not be ideal for few-shot scenarios. Hybrid methods typically combine data-level or algorithm-level strategies with ensemble learning**Dietterich, "Solving Data Mining Problems via Ensemble Methods"**, taking advantage of the strengths of both strategies. Nevertheless, such hybrid methods inherit the limitations of the data-level or algorithm-level approaches and usually involve substantial computational overhead, making them difficult to implement on lightweight devices.

Although the class imbalance problem has been extensively studied across various tasks, research on the generalization performance of these methods in the context of few-sample model compression remains limited.