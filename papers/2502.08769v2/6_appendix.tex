\clearpage
\appendix

\section{Detailed overview}
In \autoref{fig:code_fig}, we provide a detailed overview of the complete method, with tensor sizes annotated for a reference CAPI ViT-L/14 model.

\begin{figure}[p]
  \centering
  \includegraphics[width=\linewidth]{code_fig}
  \caption{Detailed overview of our method with reference tensor sizes for a CAPI ViT-L/14 model.
  We denote in red the parts that are trained by the main loss, in purple the parts that are trained with the clustering loss, and in blue the parts that are updated by the EMA.}
  \label{fig:code_fig}
\end{figure}

\section{Loss curve}
We report in \autoref{fig:plot_loss_curve} the loss curve of our CAPI ViT-L model.
After an initial adjustment period, the loss trends smoothly downwards, with no sign of instability or plateauing.
Compared to other latent masked image modeling methods such as I-JEPA or iBOT, this trend is reassuring, and might indicate good potential for further scaling.
\begin{figure}
  \centering
  \begin{minipage}{0.49\linewidth}
  \centering
  \includegraphics[width=0.9\linewidth]{plot_loss_curve}
  \caption{The loss curve of our CAPI ViT-L during training.}
  \label{fig:plot_loss_curve}
  \end{minipage}
  \hfill
  \begin{minipage}{0.49\linewidth}
      \centering
      \includegraphics[width=\linewidth]{plot_distillation}
      \caption{Comparative downstream scores of the teacher model and the student model throughout training.}
      \label{fig:plot_distillation}
  \end{minipage}
  \end{figure}

\section{Blockwise masking strategy}
The so-called ``block masking'' strategy used in many masked image modeling methods is by no means standardizes and can actually refer to several different implementations.
The most common block masking implementation was proposed in BeiT~\citep{beit}, then reused in iBOT~\citep{ibot} and MAE~\citep{he2021masked}.
It involves sampling many rectangular regions and doing multiple attempts to mask out approximately the right number of patches.
Another implementation was proposed in I-JEPA, adding multiple constraints on the masks, to obtain a similar multi-block mask. 
Additionally, some methods postprocess the proposed mask to obtain a constant number of masked patches, in order to keep the same sequence length in all batch elements.

In CAPI, we propose a simpler heuristic: we sample a single rectangular mask, and truncate out the excess patches at the lower right end.
Conversely, our implementation of inverse block masking is to sample a block mask, then simply invert it.

\section{Self-distillation interpretation}
It was observed in DINO~\citep{dino} that the downstream scores of the EMA model were consistently higher than the ones of the online model during training.
This led to the interpretation of DINO as a self-distillation method, where the EMA model, the "teacher" distilled its slightly better representations into the online model, the "student".
We observe that this interpretation still seems to hold in CAPI, albeit to a lesser extent, as evidenced by the comparison of teacher and student performance in \autoref{fig:plot_distillation}.



\section{Modified Sinkhorn-Knopp}
We provide the pseudo-code for the standard Sinkhorn-Knopp and for our modified version in \autoref{fig:sinkhorn}.
Both the original code and the proposed change are very simple.
The actual code additionally contains an initial additive shift to prevent numerical instabilities in the exponential, as well as a collective \texttt{all\_reduce} for distributed training.


\begin{figure}
  \centering
    \hfill
    \begin{subfigure}{0.49\linewidth}
    \centering
      \includegraphics[width=0.6\linewidth]{sinkhorn_standard}
      \caption{Standard SK}
      \label{fig:sinkhorn:standard}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\linewidth}
      \centering
      \includegraphics[width=0.6\linewidth]{sinkhorn_modified}
      \caption{Modified algorithm}
      \label{fig:sinkhorn:modified}
    \end{subfigure}
    \hfill
    \caption{
      PyTorch pseudo-code for the proposed modified Sinkhorn-Knopp algorithm.
      We normalize by the sum of the tokens for every given position, instead of normalizing across all positions.
    }
    % implementation detail: across-gpu reductions, and row-wise shift for numerical precision
    \label{fig:sinkhorn}
\end{figure}
\begin{table}
\centering
    \begin{tabular}{lcc}
      \toprule
      Hyperparameter & Value \\
      \midrule
      Batch size & 16384 \\
      Optimizer & AdamW \\
      Learning rate & 1e-3 \\
      Teacher momentum & $1-lr$ \\
      Clustering lr & $\frac{1}{2}lr$ \\
      lr schedule & linear warmup + trunc. cosine\\
      Warmup length & 10\% \\
      cosine truncation & 20\% \\
      Weight decay & 0.1 \\
      AdamW $\beta$ & (0.9, 0.95) \\
      Number of prototypes & 16384 \\
      Student temperature & 0.12 \\
      Teacher temperature & 0.06 \\
      Num SK iter & 3 \\
      Stochastic depth & 0.2 \\
      Weight init & xavier\_uniform \\
      Norm layer & RMSnorm \\
      Norm $\varepsilon$ & 1e-5 \\
      Patch embed lr & $0.2\cdot{}$lr \\
      Norm layer wd & $0.1\cdot{}$wd \\
      Image size & 224 \\
      Augmentations & RRCrop, HFlip \\
      Training dtype & bf16 \\
      Parallelism & FSDP \\
      Pred. / im & 7 \\
      Layerscale & No \\
      Biases & No \\
      Rope frequencies & logspace(7e-4, 7), axial \\
      Masking type & inverse block\texttt{+roll} \\
      Masking ratio & 65\% \\
      \bottomrule
    \end{tabular}
    \caption{
      CAPI pretraining recipe
    }
    \label{tab:pretraining_recipe}
\end{table}

% \begin{table}
% \centering
% \begin{tabular}{llll}
% mask & patch & ADE & IN1k \\
% layer & layer & knn & attn. \\
% \midrule
% 0 & 23 & 21.7 & 73.0 \\
% 24 & 23 & 24.2 & 77.9 \\
% 24 & 35 & 24.2 & 74.0 \\
% \end{tabular}
% \caption{JEA vs JEPA}
% \label{tab:generalized_vit}
% \end{table}

\section{Detailed evaluation protocol}
\label{sec:eval_protocol}
In all cases, our evaluations are performed with a frozen model, and use only the patch tokens outputted by the vision transformer.
The input images are always at resolution 224$\times$224.
\subsection{Classification}
The backbone model is kept frozen, and we extract only the patch tokens from its output.
On top of these features, we train an attentive pooling classifier, consisting of a learned query, two learned k anv v projections, and a final projection to the number of classes.
The attention is multi-head, with the head dimension being fixed at 64 and the number of heads being $\frac{d_{model}}{64}$.
This head is optimized with a cross-entropy loss and the AdamW optimizer~\citep{adam,adamw} for 12500 iterations at batch size 1024 (10 ImageNet epochs).
The learning rate is warmed up linearly for 1250 iterations then annealed with a cosine schedule.
We grid the weight decay over (5e-4, 1e-3, 5e-2) and the peak learning rate over (1e-5, 2e-5, 5e-5, 1e-4, 2e-4, 5e-4, 1e-3, 2e-3, 5e-3, 1e-2), training one classifier head for each pair of hyperparameters (30 in total).
We choose the optimal hyperparameters using the accuracy on a 10\% held-out part of the training set, then finally report the accuracy of this classifier on the test set.
The training dataset is lightly augmented using a \texttt{torchvision} \texttt{RandomResizedCrop}~\citep{torchvision} with default hyperparameters and a random horizontal flip.
During evaluation, the images are resized to 256 then center cropped to 224$\times$224 pixels.

\subsection{Segmentation}
We compute the features for the train and test set considered at resolution 224, and hold out 10\% of the train set as a validation set.
Using these frozen features, the segmentation problem is reduced to a simple classification problem, on which we can use simple $k$-NN and linear classifiers.
The linear classifier is trained for logistic regression with L-BFGS~\citep{lbfgs} regularized with L2 penalty, as implemented in the \texttt{cuml} library~\citep{cuml}.
In both cases, a grid of hyperparameters is tested, and the ones performing best on the validation set are retained.
For the $k$-NN classifier, we grid the number of neighbors over (1, 3, 10, 30), and the distance used over (L2, cosine).
For the linear classifier, we grid the regularisation parameter \texttt{C}, testing 8 values along a log-space between $10^{-6}$ and $10^5$.

\subsection{Feature standardization}
Some of the baselines suffer from poor conditioning of their features, which can cause very bad results when fitting a logistic regression over these features.
To reduce this issue, in the segmentation evaluation we standardize features by substracting their mean and dividing them by their standard deviation.
These statistics are computed using the features from the training set only.
This significantly improves the scores of pixel reconstruction-based methods, while the other methods are mostly unaffected.
We report a comparison of segmentation results with and without standardization in \cref{tab:standardization}.
In the rest of the paper, all segmentation results are obtained with standardization.

      % \cmidrule{3-5} \cmidrule{6-8}
\begin{table}
\begin{tabular}{llcccccc}
\toprule
 &  & \multicolumn{3}{c}{knn} & \multicolumn{3}{c}{logreg} \\
      \cmidrule{3-5} \cmidrule{6-8}
model & standardization & ADE & Cityscapes & VOC2012 & ADE & Cityscapes & VOC2012 \\
\midrule
CAPI & False & {\cellcolor[HTML]{0E8245}} \color[HTML]{F1F1F1} 32.5 & {\cellcolor[HTML]{3FAA59}} \color[HTML]{F1F1F1} 39.2 & {\cellcolor[HTML]{016A38}} \color[HTML]{F1F1F1} 64.9 & {\cellcolor[HTML]{097940}} \color[HTML]{F1F1F1} 37.9 & {\cellcolor[HTML]{249D53}} \color[HTML]{F1F1F1} 44.7 & {\cellcolor[HTML]{016A38}} \color[HTML]{F1F1F1} 73.2 \\
CAPI & True & {\cellcolor[HTML]{097940}} \color[HTML]{F1F1F1} 33.0 & {\cellcolor[HTML]{3FAA59}} \color[HTML]{F1F1F1} 39.2 & {\cellcolor[HTML]{006837}} \color[HTML]{F1F1F1} 65.2 & {\cellcolor[HTML]{0A7B41}} \color[HTML]{F1F1F1} 37.7 & {\cellcolor[HTML]{33A456}} \color[HTML]{F1F1F1} 44.3 & {\cellcolor[HTML]{006837}} \color[HTML]{F1F1F1} 73.3 \\
aim 600M & False & {\cellcolor[HTML]{F88C51}} \color[HTML]{F1F1F1} 14.3 & {\cellcolor[HTML]{F99355}} \color[HTML]{000000} 27.8 & {\cellcolor[HTML]{FEEDA1}} \color[HTML]{000000} 38.5 & {\cellcolor[HTML]{A50026}} \color[HTML]{F1F1F1} 7.1 & {\cellcolor[HTML]{A50026}} \color[HTML]{F1F1F1} 28.3 & {\cellcolor[HTML]{A50026}} \color[HTML]{F1F1F1} 61.3 \\
aim 600M & True & {\cellcolor[HTML]{000000}} \color[HTML]{F1F1F1} nan & {\cellcolor[HTML]{FFFAB6}} \color[HTML]{000000} 32.1 & {\cellcolor[HTML]{1B9950}} \color[HTML]{F1F1F1} 60.2 & {\cellcolor[HTML]{7DC765}} \color[HTML]{000000} 31.5 & {\cellcolor[HTML]{F2FAAE}} \color[HTML]{000000} 38.2 & {\cellcolor[HTML]{FFF8B4}} \color[HTML]{000000} 67.0 \\
dinov2 vitg14+reg & False & {\cellcolor[HTML]{016A38}} \color[HTML]{F1F1F1} 33.8 & {\cellcolor[HTML]{006837}} \color[HTML]{F1F1F1} 42.0 & {\cellcolor[HTML]{0B7D42}} \color[HTML]{F1F1F1} 63.1 & {\cellcolor[HTML]{006837}} \color[HTML]{F1F1F1} 38.9 & {\cellcolor[HTML]{006837}} \color[HTML]{F1F1F1} 46.8 & {\cellcolor[HTML]{08773F}} \color[HTML]{F1F1F1} 72.9 \\
dinov2 vitg14+reg & True & {\cellcolor[HTML]{006837}} \color[HTML]{F1F1F1} 34.0 & {\cellcolor[HTML]{006837}} \color[HTML]{F1F1F1} 42.0 & {\cellcolor[HTML]{0B7D42}} \color[HTML]{F1F1F1} 63.0 & {\cellcolor[HTML]{006837}} \color[HTML]{F1F1F1} 39.0 & {\cellcolor[HTML]{006837}} \color[HTML]{F1F1F1} 46.8 & {\cellcolor[HTML]{097940}} \color[HTML]{F1F1F1} 72.8 \\
ijepa vith14 in1k & False & {\cellcolor[HTML]{FFF6B0}} \color[HTML]{000000} 20.1 & {\cellcolor[HTML]{E75337}} \color[HTML]{F1F1F1} 25.9 & {\cellcolor[HTML]{48AE5C}} \color[HTML]{F1F1F1} 57.4 & {\cellcolor[HTML]{E5F49B}} \color[HTML]{000000} 25.2 & {\cellcolor[HTML]{FBA35C}} \color[HTML]{000000} 33.5 & {\cellcolor[HTML]{E44C34}} \color[HTML]{F1F1F1} 63.0 \\
ijepa vith14 in1k & True & {\cellcolor[HTML]{FFFEBE}} \color[HTML]{000000} 20.8 & {\cellcolor[HTML]{EF633F}} \color[HTML]{F1F1F1} 26.4 & {\cellcolor[HTML]{54B45F}} \color[HTML]{F1F1F1} 56.7 & {\cellcolor[HTML]{DFF293}} \color[HTML]{000000} 25.7 & {\cellcolor[HTML]{FDC171}} \color[HTML]{000000} 34.5 & {\cellcolor[HTML]{F26841}} \color[HTML]{F1F1F1} 63.6 \\
ijepa vith14 in22k & False & {\cellcolor[HTML]{FED884}} \color[HTML]{000000} 17.9 & {\cellcolor[HTML]{A50026}} \color[HTML]{F1F1F1} 22.9 & {\cellcolor[HTML]{5DB961}} \color[HTML]{F1F1F1} 56.2 & {\cellcolor[HTML]{F2FAAE}} \color[HTML]{000000} 24.1 & {\cellcolor[HTML]{F88950}} \color[HTML]{F1F1F1} 32.8 & {\cellcolor[HTML]{F26841}} \color[HTML]{F1F1F1} 63.6 \\
ijepa vith14 in22k & True & {\cellcolor[HTML]{FEE999}} \color[HTML]{000000} 18.9 & {\cellcolor[HTML]{AD0826}} \color[HTML]{F1F1F1} 23.2 & {\cellcolor[HTML]{6EC064}} \color[HTML]{000000} 55.0 & {\cellcolor[HTML]{D7EE8A}} \color[HTML]{000000} 26.4 & {\cellcolor[HTML]{FDB768}} \color[HTML]{000000} 34.2 & {\cellcolor[HTML]{F88950}} \color[HTML]{F1F1F1} 64.2 \\
mae vitl16 & False & {\cellcolor[HTML]{A50026}} \color[HTML]{F1F1F1} 7.8 & {\cellcolor[HTML]{DE402E}} \color[HTML]{F1F1F1} 25.3 & {\cellcolor[HTML]{A50026}} \color[HTML]{F1F1F1} 17.3 & {\cellcolor[HTML]{C7E77F}} \color[HTML]{000000} 27.4 & {\cellcolor[HTML]{EEF8A8}} \color[HTML]{000000} 38.4 & {\cellcolor[HTML]{AF0926}} \color[HTML]{F1F1F1} 61.5 \\
mae vitl16 & True & {\cellcolor[HTML]{F7FCB4}} \color[HTML]{000000} 21.5 & {\cellcolor[HTML]{F8FCB6}} \color[HTML]{000000} 32.8 & {\cellcolor[HTML]{7FC866}} \color[HTML]{000000} 53.7 & {\cellcolor[HTML]{C7E77F}} \color[HTML]{000000} 27.4 & {\cellcolor[HTML]{EBF7A3}} \color[HTML]{000000} 38.5 & {\cellcolor[HTML]{AD0826}} \color[HTML]{F1F1F1} 61.5 \\
\bottomrule
\end{tabular}
\caption{Comparison of segmentation results with and without standardization}
\label{tab:standardization}
\end{table}
\subsection{Baselines}
All baselines are vision transformers, allowing us to use the same evaluation protocol.
We feed the 224$\times{}$224 image to the model after imagenet normalization of the pixel values, and extract the patch tokens after the last transformer block.
For the specific case of AIM~\citep{aim}, we follow the advice from the original paper and extract the patch tokens after before the end of the ViT, specifically after layer 18.

% TODO eval at all layers

\section{Compute cost and environmental footprint}
% 6016 A100.h for our ViT-L model
% 5763 if using 4 nodes only
% The A100 were well used, 0.4W
% PUE 1.15? Azure is 1.17, AWS 1.11, GCP 1.10, and the range of PUE for datacenters is between 1.09 and 1.58
% RSC is in Virginia, which has an avg carbon intensity of 0.245 kgCO2eq/kWH. But it's near Richmond, were the power plants are mostly natural gas, ie 0.429. I'll assume 0.350
% That makes it 928 kgCO2eq for the main run
% Whole project: slurm accounting recorded 2.5M gpu.h since early may. I'll add 50% to account for the ~3-4 missing month
We measure the training of a CAPI ViT-L model to take 180h on 32 A100 GPUs, amounting to 5763 A100 hours.
This consumed around 2651 kWh of electricity, which we estimate to amount to approximately 928 kgCO2eq.
The entire project used 3.75M A100 hours, which we similarly estimate to have emitted 604 tCO2eq for the electricity consumption.
Note that the carbon footprint estimations here are purely scope 2 estimations, \ie limited to electricity consumption, and are further limited to the electricity consumption of the GPUs.
A full carbon accounting should additionally include many other harder to estimate emissions, such as the electricity consumption of the other server components and the rest of the datacenter appliances, and scope 3 emissions from the component manufacturing, datacenter construction, and their respective end-of-life.
2

\section{List of models used}
We provide in \autoref{tab:gen_table_our_models} the list of all models presented in this paper, along with a unique hash identifier and the relevant hyperparameters.
Non-listed hyperparameters are detailed in \autoref{tab:pretraining_recipe}.
To disambiguate any possible unclarities in the presented results, \autoref{tab:fig_to_model} provides the mapping from tables and figures to model identifiers.

\input{resources/gen_table_our_models.tex}
\input{resources/fig_to_model.tex}

\section{Visualisations}

In Figures \autoref{fig:pca-ours-vs-others} and \ref{fig:pca-ours-channels}, we provide visualisations of the feature maps of CAPI compared to other state-of-the-art self-supervised vision models.

\begin{figure*}
  \centering
  \includegraphics{dump_pca_vs_others}
  \caption{Visualization of the features produced by CAPI and other vision models at various resolutions:
  CAPI ViT-L/14,
  DINOv2+reg ViT-g/14 \citep{vitneedreg},
  BEiT ViT-L/16 \citep{beit},
  AIM ViT-3B/14 \citep{aim},
  MAE ViT-H/14 \citep{aim},
  I-JEPA ViT-H/14 \citep{ijepa},
  and data2vec2 ViT-L/16 \citep{data2vec}.
  We apply a PCA decomposition to the dense outputs produced by each model for each image individually, and rescale the three first components to the RGB range for visualization.
  }
  \label{fig:pca-ours-vs-others}
\end{figure*}

\input{pca_capi_channels.tex}

