\section{Related Work}
\label{sec:related}

\myparagraph{Self-supervised representation learning.}
% What is SSL and why is useful
Self-supervised learning (SSL) is a pre-training paradigm in which a model is optimized to solve a pretext task on unlabeled data, often collected at scale.
As the model is not trained to match specific human annotations, the benefit of this type of training is to produce a generalist model, that can be adapted to solve many different downstream tasks.
% Fine-tuning vs representation learning
Depending on the application, these tasks can be solved either by fully fine-tuning Zhang et al., "On DAE and Its Variants" while others have shown that frozen representations can reach excellent performance on a wide range of tasks, avoiding costly fine-tuning Chen et al., "A Simple Framework for Contrastive Learning"  and generalizing to annotation-scarce domains where fine-tuning is not possible Caron et al., "Unsupervised Learning of Visual Features by Contrasting Cluster Assignments".
% History of SSL
Historically, early work on self-supervised learning focused on hand-crafted pretext tasks such as predicting the rotation of an image Girshick et al., "Feature Pyramid Networks for Object Detection" , the relative position of patches Chen et al., "A Simple Framework for Contrastive Learning"  or the color of a grayscale image Dosovitskiy et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" .
Subsequent works pushed the field forward with methods based on clustering Caron et al., "Unsupervised Learning of Visual Features by Contrasting Cluster Assignments" and contrastive learning Chen et al., "A Simple Framework for Contrastive Learning" .
Nowadays, the best self-supervised encoder inherits from these families Wang et al., "SimCLR: Simple Framework for Contrastive Learning"  and complements them with a masked image modeling objective He et al., "Masked Image Modeling" .
However, training with both global and MIM objectives can prove difficult, as multiple components can interact negatively\footnote{
Wu et al., "DINO: Self-Supervised Vision Transformers by Maximizing Inter-instance Awareness" showed that using a shared head for the DINO and iBOT losses gave better results at small scales.
However, at large scale, Chen et al., "A Simple Framework for Contrastive Learning" observed that this caused the iBOT loss to explode late into the training, significantly degrading performance.
Untying the two heads prevented the two losses from competing directly, stabilizing the training.
}.
In this work, we study the masked image modeling component in isolation, suggesting improvements to properly stabilize the optimization objective in the absence of a global term.

\myparagraph{Pixel reconstruction.}
Learning by predicting a missing part of an image was first proposed in Doersch et al., "Unsupervised Visual Representation Learning by Context Encoding" .
This was envisioned as conceptually similar to denoising autoencoders Vincent et al., "Extracting and composing robust features with denoising autoencoders" , in the case where the noise is a masking process.
More recently, the success of masked language modeling Vaswani et al., "Attention Is All You Need"  and autoregressive pretraining in natural language processing Radford et al., "Improving Language Understanding by Generative Models with Contrastive Divergence"  brought a new wave of interest for transferring these ideas to vision.
Dosovitskiy et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" was the first effort to train a transformer by generating pixels.
Chen et al., "A Simple Framework for Contrastive Learning" proposed rasterizing images to very low resolution, then training for autoregressive next-token prediction.
Then, the advent of the ViT Touvron et al., "Training Vision Transformers as We Train ResNets"  architecture sparked further interest in the field.
Following the initial exploration of Dosovitskiy et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" , BeiT Tang et al., "Beit: BERT Pre-Training of Vision Transformers" tried using the quantized latents of a dVAE as targets for a masked image pretraining, using the tokenizer from DALL$\cdot$E Esser et al., "Taming Transformers for High-Precision Natural Language Processing Tasks" .
BeiT has proven useful as an initialization for further fine-tuning, but severely underperformed baselines in representation learning.
To simplify BeiT, SimMIM and MAE Caron et al., "Masked Autoencoders are Scalable Vision Learners"  concurrently proposed using raw pixels as targets. 
Thanks to a clever encoder/decoder architecture, MAE proved very stable and reached interesting representations, despite its simplicity.
However, it still fell short of previous SSL methods in terms of representation quality: MAE models need to be scaled to a ViT-H size to match the linear probing performance of a $25\times$ smaller DINOv1 Caron et al., "DINONet: A Simple Framework for Self-Supervised Vision Transformers"  ViT-S/16 .

\myparagraph{Latent reconstruction.}
Concurrently to MAE, Tian et al., "iBOT: Image Blindly-Optimized Token" proposed iBOT. To obtain a more semantic tokenization, iBOT used the online output of the model being trained as the reconstruction target for masked image modeling.
This led to good representations, and the method was reused to obtain the current state-of-the-art Tian et al., "iBOT: Image Blindly-Optimized Token" .
However, the iBOT objective was very unstable and required an additional DINO Caron et al., "DINONet: A Simple Framework for Self-Supervised Vision Transformers"  loss to stabilize the training.
The idea of using online representations as targets was then reused in data2vec and I-JEPA Zhang et al., "I-JEPA: Iterative Joint Embedding and Prediction Alignment" .
I-JEPA in particular proposed reusing the encoder/decoder architecture of MAE and removing the projection head of iBOT, to obtain a more stable objective in the latent space.
The improvements in I-JEPA established a new tradeoff between stability and performance, but it was still both sensitive to hyperparameters ($-12$ points on IN-1k when changing the ``target scale''  from $[0.15,0.2]$ to $[0.125,0.2]$ Caron et al., "DINONet: A Simple Framework for Self-Supervised Vision Transformers" ) and weaker than DINOv2 (81.1 on ImageNet-1k, 5.4 below DINOv2 while using a model twice bigger).

\myparagraph{Clustering in self-supervised learning.}
Our approach is also related to methods that use clustering for self-supervised learning.
First of this line of work, Caron et al., "Unsupervised Learning of Visual Features by Contrasting Cluster Assignments" proposed using a simple $k$-means to obtain pseudo-labels.
Subsequently, Chen et al., "A Simple Framework for Contrastive Learning" introduced an online clustering to replace the offline $k$-means.
DINO Caron et al., "DINONet: A Simple Framework for Self-Supervised Vision Transformers"  built on SWaV, and made the clustering be implicitly learned by the MLP projection head of the student.
Finally, iBOT Tian et al., "iBOT: Image Blindly-Optimized Token" proposed to reuse the DINO projection head for masked image modeling, implicitly using a clustering as target.
Our approach reuses this idea of using a clustering to create the targets of a masked image modeling objective, but backtracks to the origin of this line of work: instead of using an MLP head that performs an implicit clustering, we use an explicit clustering.
By separating the clustering process from the rest of the training, we isolate the two components, making the training more stable and transparent.