\section{Approach}
\label{sec:approach}

\begin{figure}[t]
  \centering
  \includegraphics{anatomy_2}
  \caption{
    Overview of the components of a reconstruction-based model.
    We identify three main choices involved in designing a masked image model:
    the choice of targets (\cref{fig:target_repr}), the loss function (\Cref{sec:loss}, \cref{fig:loss_formulation}) and the architecture of the predictor (\Cref{sec:predictor-arch}, \cref{fig:pred_arch}).
  }
  \label{fig:anatomy}
\end{figure}

At a high level, masked image modeling involves masking a part of the input, feeding the visible region to a prediction model, and optimizing it to predict the content of the missing parts.
Despite the simple formulation, the effectiveness of reconstruction-based methods and the properties of the trained models are dramatically influenced by a number of design choices.
In this section, we discuss the three aspects of masked image modeling depicted in \Cref{fig:anatomy}: the type of patch representation used as target (\cref{fig:target_repr}), the loss formulation (\Cref{sec:loss}, \cref{fig:loss_formulation}), and the prediction architecture (\Cref{sec:predictor-arch}, \cref{fig:pred_arch}).
Based on our findings, we introduce CAPI, an SSL model that enjoys stable learning and strong representation capabilities.

\captionsetup[subfigure]{justification=centering}
\begin{figure}[b]
\centering
\hfill
\begin{subfigure}[b]{0.32\linewidth}
  \centering
    \includegraphics{pixel_teacher}
    \caption{Pixel targets\\\textcolor{gray}{(iGPT, MAE, AIM)}}
    \label{fig:target_repr:pixel}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\linewidth}
  \centering
    \includegraphics{frozen_teacher}
    \caption{Frozen teacher\\\textcolor{gray}{(BeiT,PeCo,EVA)}}
    \label{fig:target_repr:frozen}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\linewidth}
  \centering
    \includegraphics{ema_teacher}
    \caption{EMA teacher\\\textcolor{gray}{(iBOT, Data2Vec, I-JEPA, Ours)}}
    \label{fig:target_repr:ema}
\end{subfigure}
\caption{
The target representations commonly used in MIM.
We focus on the EMA representations.
}
\label{fig:target_repr}
\end{figure}

\myparagraph{Overview of training.}
In short, our main design choices are: first, we reconstruct images in latent space with a teacher-student framework, following iBOT.
Then, we formulate our loss using a clustering component, inspired by DeepCluster, SwAV, and DINO, and draw inspiration from the regularization methods they introduce, in particular the Sinkhorn-Knopp~\citep{sinkhorn1967concerning}. 
Finally, we employ a cross-attention predictor model, separate from the encoder, to perform reconstructions, following crossMAE~\citep{crossmae}.

Our encoder and the predictor are transformers \citep{dosovitskiy2020image} and, during pre-training, they operate in tandem as the student (\cref{fig:pullfig} left). The teacher is an EMA of the encoder.
The typical values for one training iteration using square images of side 224 pixels are:
\begin{itemize}
    \itemsep0em 
    \item We pass the full image to the teacher, collect $n = 14\times{}14 = 196$ patch tokens, and apply an online clustering to obtain soft assignments that will be used as learning targets.
    \item The encoder receives a partial view of the input image: we apply a patch embedding layer to obtain $n$ patch tokens, we drop $p_\text{drop}{\times}n$ of these patches and pass to the encoder the remaining $n_\text{keep}=(1-p_\text{drop}){\times}n=69$ patches ($p_\text{drop}=65\%$).
    \item The encoder takes these $n_\text{keep}=69$ tokens along with $n_{reg}=16$ learnable register tokens~\citep{vitneedreg}, and processes them to obtain $n_\text{encoded}=n_\text{keep} + n_\text{reg} = 85$ encoded tokens.
    \item We sample $n_\text{pred}=7$ coordinates among the dropped set, and for each we forward a \texttt{[MSK]} token through the predictor, which predicts their assignment by cross-attending to the encoded view.
\end{itemize}
A detailed visual diagram of the method can be found in \cref{fig:code_fig}.


%%%%%%%%%%%%%%%%%%%%%
%%% LOSS FUNCTION %%%
%%%%%%%%%%%%%%%%%%%%%

\subsection{Clustering-based loss formulation}
\label{sec:loss}


Latent clustering methods such as SwAV and DINO employ a cross-entropy loss between the student and teacher output distributions.
These distributions, produced by a linear or MLP head, can be seen as soft cluster memberships, where the centroids correspond to the prototypes.
Replicating the DINO objective, iBOT proposes to pass the student predictions through an MLP head and to pass the teacher embeddings through the EMA of this head.
But this ignores a specificity of masked image modeling: while in DINO both targets and predictions are \texttt{[CLS]} tokens, in iBOT the targets are patch tokens while the predictions are special \texttt{[MSK]} tokens.
This causes a \emph{distribution mismatch}: the MLP head of the student is trained with \texttt{[MSK]} inputs but is instead applied to regular patch tokens in the teacher.
This mismatch would be even stronger in an asymmetric architecture as in MAE or I-JEPA, where the targets and predictions come from two different networks (see \cref{fig:pred_arch}).
Without the stabilizing effect of the DINO loss, the iBOT formulation is unable to bootstrap itself and results in trivial representations (see \cref{tab:ablations:proj_head}).
Our proposition is simple: we decouple the training of the teacher projection from the student's head by directly learning an online clustering of the teacher patch tokens.
This way, the training remains stable even in the absence of a stabilizing loss.


\captionsetup[subfigure]{justification=centering}
\begin{figure}[t]
\begin{subfigure}[b]{0.32\linewidth}
  \centering
  \includegraphics{micro_direct_loss}
    \caption{Direct loss\\\textcolor{gray}{(MAE, I-JEPA)}}
    \label{fig:loss_formulation:direct}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\linewidth}
  \centering
  \includegraphics{micro_ibot_loss}
    \caption{DINO loss\\\textcolor{gray}{(iBOT, DINOv2)}}
    \label{fig:loss_formulation:dino}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\linewidth}
  \centering
  \includegraphics{micro_clustering_loss}
    \caption{Clustering\\\textcolor{gray}{(proposed)}}
    \label{fig:loss_formulation:clustering}
\end{subfigure}
\caption{
  The different loss formulations considered here. 
  We depict in red the flow of the gradient.
  }
  \label{fig:loss_formulation}
\end{figure}

\begin{figure}[b]
  \centering
  \hfill
    \begin{subfigure}[b]{0.32\linewidth}
      \centering
      \includegraphics{fused_predictor}
        \caption{Fused predictor\\\textcolor{gray}{(BeiT, iBOT)}}
        \label{fig:pred_arch:fused}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\linewidth}
      \centering
      \includegraphics{split_predictor}
        \caption{Self-att. predictor\\\textcolor{gray}{(MAE, I-JEPA)}}
        \label{fig:pred_arch:split}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\linewidth}
      \centering
      \includegraphics{cross_predictor}
        \caption{Cross-att. predictor\\\textcolor{gray}{(CrossMAE, ours)}}
        \label{fig:pred_arch:cross}
    \end{subfigure}
    \caption{
        The different predictor architectures discussed in the paper. 
        Here, the boxes each represent a transformer.
        The black lines represent the residual stream for a token.
    }
    \label{fig:pred_arch}
\end{figure}

%%% ONLINE CLUSERING %%%

\myparagraph{Online clustering.}
Inspired by the minibatch $k$-means algorithm and the SwaV loss, we define our online clustering process as follows.
Let $X \in \mathbb{R}^{n \times d}$ be the output of the teacher, with row vectors $x_i$ for $i\in\{1,\ldots,n\}$.
We apply an L2 normalization and a linear projection to obtain the assignment logits $l_i \in \mathbb{R}^p$:
\begin{equation}
l_i = C \cdot \frac{x_i}{||x_i||},
\end{equation}
where $C$ is a matrix in $\mathbb{R}^{p\times d}$, whose rows are the $p$ ``centroids'' in dimension $d$.
We then apply a softmax with temperature $\tau$ to obtain soft assignments:
\begin{equation}
a_i = \frac{\exp(l_i/\tau)}{\sum_{k=1}^p{\exp(l_k/\tau)}}.
\end{equation}
We wish to estimate $C$ by solving the following problem:
\begin{equation}
    \min_{C} \quad -\sum_{i=1}^n \sum_{k=1}^p a_i^{(k)} \log a_i^{(k)},
    \label{eq:prob}
\end{equation}
where $a_i^{(k)}$ is the $k$-th coordinate in $a_i$.
By minimizing the entropy, we force the assignments to be as close to one-hot as possible, which pushes the centroids towards their ``assigned'' samples.
This can be seen as a form of clustering with a logistic loss.
However, simply solving this problem can result in empty clusters.
This is a common problem in mini-batch $k$-means~\citep{minibatchkmeans}, and is usually solved by adding a reassignment phase:
the centroids of the empty clusters are discarded and moved next to another non-empty centroid.
In our case, we do not wish the centroids to move that abruptly, as it could disturb the training of the student.
Instead, we propose to use the Sinkhorn-Knopp (SK) algorithm~\citep{sinkhorn1967concerning}, used in SwAV~\citep{swav} and DINOv2~\citep{dinov2}.
Using the SK algorithm, we obtain $a'$, an assignment where the distribution of tokens over the clusters is near-uniform:
\begin{equation}
\label{eq:sk}
    \{a'_1, \dots, a'_n\} \leftarrow \text{SK}(\{l_1, \dots, l_n\}, \tau'),
\end{equation}
with $\tau'$ another temperature parameter.
Note that we do not backpropagate through the SK algorithm.
We adapt the loss in Problem~(\ref{eq:prob}), and learn the centroids $C$ by minimizing the cross-entropy between $a$ and $a'$:
\begin{equation}
 - \sum_{i=1}^n \sum_{k=1}^p {a'}_{i}^{(k)} \log a_{i}^{(k)}.
\end{equation}
We minimize this loss with AdamW~\citep{adamw} alongside the training of the main model.


%%% POSITION COLLAPSE %%%

\myparagraph{Positional collapse.}
A crucial point of self-supervised learning is avoiding trivial solutions, \ie situations where the model minimizes the loss without learning useful features.
These failure modes, also known as representation collapse, are usually addressed by adding regularization mechanisms.
For example, color jittering~\citep{simclr} helps prevent the model from focusing uniquely on color.
While training CAPI, we observed a specific type of representation collapse as the positional encoding started outweighing the content of the patch embeddings.
In the extreme case, the model learns to predict the position of the masked tokens instead of their content, resulting in a zero loss with a trivial model.
In most observed cases, both content and position information are \emph{entangled} in the target representation, while in the ideal scenario, the target would consist purely of semantic features.

We propose a simple solution to alleviate the problem. 
By running the SK separately at each position in Eq.~(\ref{eq:sk}), we force the joint distribution of tokens over positions and clusters to be near-uniform.
Uniformity of the joint distribution directly implies zero mutual information between the clustering and the targets.
This way, the modified SK ensures that our targets contain no positional information.


\subsection{Predictor architecture}
\label{sec:predictor-arch}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% predictor architecture %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





Model architecture is another important component in reconstruction-based SSL.
Two broad categories are widely used in previous work.
BeiT, iBOT, and SimMIM use a \emph{fused} architecture: a single vision transformer that takes as inputs patches and mask tokens (Fig.~\ref{fig:pred_arch:fused}).
Fused architectures are difficult to train and yield poor results, as reported in previous work~\citep{ibot} and confirmed by our experiments.
MAE uses a \emph{split} architecture, with an encoder that only forwards the patches, saving memory and compute, and a predictor that forwards both patches and mask tokens (Fig.~\ref{fig:pred_arch:split}).

In this work, we use an even lighter architecture, which was initially explored in crossMAE~\citep{crossmae} for pixel-based reconstruction.
In this case, the predictor forwards only the mask tokens (Fig.~\ref{fig:pred_arch:cross}), which can access the context of the encoded patches via cross attention.
Using a cross-attention predictor has two main advantages. 
% 1 - less patches to forward in general
First, it allows further efficiency gains, as we only forward a reduced set of tokens in the predictor, and we can even subsample this set of tokens.
% 2 - all predictions are independent
Second, mask tokens do not interact with each other in the cross-attention mechanism, \ie each prediction is independent of other positions.
This alleviates the need for multiple predictor forward passes with different prediction sets, as used in I-JEPA.
