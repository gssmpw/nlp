

\section{Experiments}
\label{sec:experiments}

In this section, we report empirical evaluations of our model.
We describe experimental details and present some ablation studies.
Then we discuss whole-image understanding results and dense prediction results.

\subsection{Experimental setup}

\myparagraph{Pretraining dataset.}
Most methods from the self-supervised learning literature choose to pretrain on ImageNet-1k.
This dataset is usually chosen because of its relatively small size, allowing for easy experimentation, and the ability to compare to existing methods pretrained on it.
However, this has led to an overspecialization of SSL methods to the type of object-centric images found in ImageNet-1k.
Recent foundation models obtain state-of-the-art results by exploiting much larger datasets, such as ImageNet-22k~\citep{ibot} and LVD-142M~\citep{dinov2}.
If we are to design a method that can produce new foundation models, we believe it is crucial to design it to be able to handle such large datasets.

To this end, we carry out all ablation experiments on ImageNet-22k.
It is composed of 14M images from 22k categories taken from the WordNet ontology.
Although it is close to ImageNet-1k in nature, its much larger size and diversity make it suitable to train excellent foundation models, as reported by \citet{dinov2}.

For our longer experiments, we train on multiple datasets: ImageNet-1k, for comparability with previous works, ImageNet-22k, to test scaling, Places205, to test training on more diverse and less object-centric data,
% and finally a large-scale automatically curated dataset. Following \citet{dinov2}, we filter web-crawled images to obtain a dataset of roughly 140M images, which we call \ourdataset.  % anonymous
and finally \ourdataset, a large-scale automatically curated dataset used in previous SSL foundation models.  % preprint

\myparagraph{Model architecture.}
We do all our experiments with a Vision Transformer~\citep{dosovitskiy2020image} of 300M parameters (ViT-L).
This architecture is widely used in various computer vision tasks, and most baselines provide a model of comparable size.
We equip the vision transformer with registers~\citep{vitneedreg}.
These additional tokens were recently proposed as a way to add an information buffer, which enabled the model to produce smoother feature maps.
For the decoder, we use 12 transformer blocks that cross-attend to the output of the encoder.
This is similar to a standard transformer decoder~\citep{vaswani2017attention}, with the difference that we do not include self-attention layers.
In this decoder, every token is forwarded independently and separately attends to the encoder output.
When using a different encoder size, we align the embedding dimension, MLP ratio, and number of attention heads of the decoder to those of the encoder, and use a decoder depth equal to half that of the encoder.

\myparagraph{Implementation Details.}
The learning rate follows a linear warmup followed by a cosine annealing.
We truncate out the last 20\% of the cosine, as proposed in I-JEPA~\citep{ijepa}.
To simplify the choices of parameters and schedules, we set the teacher EMA momentum to $\mu = 1-lr$, and we set the learning rate for the clustering to half of the backbone learning rate.
The impact of the most important hyperparameters will be discussed in \cref{sec:ablation}.
All our pretraining hyperparameters are summarized in \cref{tab:pretraining_recipe}.

\myparagraph{Evaluation protocol.}
All the evaluations reported in this paper fall into two categories: image classification and semantic segmentation.
For all classification tasks, we use an \emph{attentive probe}~\citep{ijepa,aim,bardes2023v}.
We use this evaluation protocol because our model does not learn a single global image representation, preventing the use of a linear probe.
In this evaluation, we train an attentive pooling to extract a global vector and use this vector as input to a linear layer.
The parameters of this probe are trained in a supervised fashion, and we report accuracy on the validation set.
For segmentation tasks, we use lightweight classifiers on top of frozen local features.
Previous works used a linear head trained with gradient descent on features of images augmented with various augmentations~\citep{dinov2}.
To obtain a more lightweight evaluation, we simply extract the features from all images in the dataset without augmentations, then train a linear logistic regression on these features with L-BFGS~\citep{lbfgs} using an off-the-shelf library~\citep{cuml}.
Although this results in lower mIoU numbers, the simplicity of the evaluation allows us to grid over different hyperparameters, producing very robust results.
For an even more lightweight classifier, we also consider a non-parametric $k$-NN segmentation evaluation.
For each test patch, we retrieve $k$ most similar patches in the training data and pool the segmentation label for that patch.
We chose the optimal regularization parameters by doing a grid search using $10\%$ of the training set.
For all segmentation tasks, we measure performance using mIoU.

\myparagraph{Baselines.}
We compare to the performance of previous models trained using masked image modeling: BeiT~\citep{beit}, MAE~\citep{he2021masked}, data2vec 2.0~\citep{data2vec2}, I-JEPA~\citep{ijepa}, and AIM~\citep{aim}. To provide additional points of comparison, we report in grey the performance of iBOT~\citep{ibot} and DINOv2+reg~\citep{dinov2,vitneedreg}, who use a DINO loss to stabilize a MIM objective.

%%%%%%%%%%%%%%%%%
%%% ABLATIONS %%%
%%%%%%%%%%%%%%%%%


\begin{table*}[t]
  \centering
  \small{
    \begin{tabular}[b]{ccc}
      \begin{subtable}[t]{0.3\textwidth}
        \centering
        \begin{tabular}{lll}
          \phantom{ADE} \\
          & ADE & IN1k \\
          \midrule
          Fused & 23.8 & 73.1 \\
          Split, self-attn & 27.9 & 77.7 \\
          \rowcolor{lightgray}
          Split, cross-attn & \bfseries 29.1 & \bfseries 81.4 \\
        \end{tabular}
        \caption{Predictor architecture}
        \label{tab:ablations:pred_arch}
      \end{subtable}
      &
      \begin{subtable}[t]{0.29\textwidth}
        \centering
        \begin{tabular}{lll}
          & ADE & IN1k \\
          \midrule
          random & 23.6 & 76.4 \\
          block & 25.6 & 79.9 \\
          inv. block & 27.2 & 80.7 \\
          \rowcolor{lightgray}
          inv. block \texttt{+roll} & \bfseries 29.1 & \bfseries 81.4 \\
        \end{tabular}
        \caption{Masking strategy}
        \label{tab:ablations:mask_type}
      \end{subtable}
      &
      \begin{subtable}[t]{0.4\textwidth}
        \centering
        \begin{tabular}{llll}
          head & loss & ADE & IN1k \\
          \midrule
          $\varnothing$ & I-JEPA & 23.7 & 79.3 \\
          MLP & iBOT & \phantom{0}1.7 & 11.1 \\
          MLP & CAPI & 26.4 & 80.8 \\
          \rowcolor{lightgray}
          Linear & CAPI & \bfseries 29.1 & \bfseries 81.4 \\
        \end{tabular}
        \caption{Loss formulation}
        \label{tab:ablations:proj_head}
      \end{subtable}
      \\
      \\
      \begin{subtable}[t]{0.3\textwidth}
        \centering
        \begin{tabular}{lll}
          & ADE & IN1k \\
          \midrule
          $[0.2, 1.0]$ & 27.9 & 81.4 \\
          \rowcolor{lightgray}
          $[0.6, 1.0]$ & \bfseries 29.1 & \bfseries 81.4 \\
          \rowcolor{white}
          $[1.0, 1.0]$ & 28.9 & 80.9 \\
        \end{tabular}
        \caption{Crop range}
        \label{tab:ablations:crop_range}
      \end{subtable}
      &
      \begin{subtable}[t]{0.29\textwidth}
        \centering
        \begin{tabular}{lll}
          & ADE & IN1k \\
          \midrule
          55\% & 28.0 & 81.1 \\
          \rowcolor{lightgray}
          65\% & \bfseries 29.1 & \bfseries 81.4 \\
          \rowcolor{white}
          75\% & 28.1 & 81.2 \\
        \end{tabular}
        \caption{Masking ratio}
        \label{tab:ablations:mask_ratio}
      \end{subtable}
      &
      \begin{subtable}[t]{0.4\textwidth}
        \centering
        \begin{tabular}{llll}
          depth & width & ADE & IN1k \\
          \midrule
          5 & 1536 & \bfseries 30.9 & \bfseries 81.5 \\
          \rowcolor{lightgray}
          12 & 1024 & 29.1 & 81.4 \\
          \rowcolor{white}
          21 & 768 & 28.3 & 81.3 \\
        \end{tabular}
        \caption{Predictor shape}
        \label{tab:ablations:pred_shape}
      \end{subtable}
      \\
      \\
      \begin{subtable}[t]{0.3\textwidth}
        \centering
        \begin{tabular}{lll}
          & ADE & IN1k \\
          \midrule
          0 & 25.9 & 79.3 \\
          \rowcolor{lightgray}
          16 & \bfseries 29.1 & \bfseries 81.4 \\
        \end{tabular}
        \caption{Number of registers}
        \label{tab:ablations:n_reg}
      \end{subtable}
      &
      \begin{subtable}[t]{0.29\textwidth}
        \centering
        \begin{tabular}{lll}
          & ADE & IN1k \\
          \midrule
          learnable & \bfseries 30.0 & \bfseries 81.6 \\
          \rowcolor{lightgray}
          RoPE & 29.1 & 81.4 \\
        \end{tabular}
        \caption{Positional encoding}
        \label{tab:ablations:posenc}
      \end{subtable}
      &
      \begin{subtable}[t]{0.4\textwidth}
        \centering
        \begin{tabular}{lll}
          & ADE & IN1k \\
          \midrule
          Standard & 28.5 & 81.3 \\
          \rowcolor{lightgray}
          Proposed & \bfseries 29.1 & \bfseries 81.4 \\
        \end{tabular}
        \caption{Sinkhorn-Knopp algorithm}
        \label{tab:ablations:sinkhorn}
      \end{subtable}
      \\
\end{tabular}
  }
  \caption{
    Ablation study of the main parameters and design choices in our algorithm. 
    We report both image segmentation and classification.
    We highlight the default setting in gray, and bold the best-performing solution.
    An in-depth analysis of these results is provided in Sec.~\ref{sec:ablation}.
  }
  \label{tab:ablations}
\end{table*}


\subsection{Ablation Studies}
\label{sec:ablation}
We conduct extensive ablation studies to study the effect of design choices on performance.
To make the ablation study more tractable, we train on the ImageNet-22k dataset for 100k iterations with a patch size of 16.
To provide slightly more robust results, the default setting was run twice with different seeds, and the results of the two runs were averaged.
All results are presented in Table~\ref{tab:ablations}.

%%% FIRST LINE

\myparagraph{Predictor architecture.}
We evaluate the different predictor architectures.
The split predictor produces better representations while training 32\% faster than the fused predictor (\cref{tab:ablations:pred_arch}).
Using pure cross-attention in the predictor obtains better representations and allows an additional 18\% speedup by avoiding a forward on patch tokens.
It also removes the dependency between different predictions, alleviating the need for repeated forwards of the predictor as in I-JEPA.

\myparagraph{Masking strategy.}
The most common masking strategies in the literature are random masking, block masking (inpainting), or inverse block masking (outpainting).
Inverse block masking induces a bias on the position of the masked patches: most often, the model will see the center of the image, and predict the edges.
To prevent this, we propose applying a random circular shift to the mask before using it (\texttt{+roll}).
This ensures that all positions in the image are equally likely to be masked.
We ablate the type of masking in \cref{tab:ablations:mask_type}.
The masking ratio is 65\% for all strategies except random masking, where it is set to 90\%, which increases performance.
Random masking is much less effective than the other strategies, and inverse block masking works best, with a clear improvement when using \texttt{+roll}.

\myparagraph{Loss formulation.}
We evaluate the different strategies for computing a loss function discussed in Sect.~\ref{sec:loss}.
We compare the performance of direct Huber loss, an iBOT loss with an MLP head, as well as our clustering-based loss with a linear or MLP head.
With no head and direct loss, the model starts well but quickly regresses to worse representations.
The iBOT head does not work in our setup, probably because of the split predictor design.
Finally, the proposed clustering head alleviates all these issues and allows for stable training and good representations.



\myparagraph{Crop range.}
To prevent overfitting, we use random cropping and flipping augmentations.
We tweak the bounds of the cropping scale to $[c, 1.0]$ and try various $c$.
We observe that the method can train well without any augmentation, resizing the training images to a fixed 224x224 resolution ($80.9$ on ImageNet with $c=1.0$).
We get better scores with minor cropping augmentation with a range of $[0.6,1.0]$.

\myparagraph{Masking ratio.}
In our model, we need to set the ratio $m$ of image patches that are masked, and reconstructed.
We train our model for various $m$ and check the final performance.
The optimal masking ratio seems to be around $0.65$, but the algorithm seems quite robust to the choice of this parameter.

\myparagraph{Predictor shape.}
We study the impact of the predictor depth on performance.
We train the model with predictors of various depths and adapt the width to match the total number of parameters.
Shallow networks will have a larger width.
We see that a more shallow predictor leads to better performance, but in our informal experiments, we have observed that such architectures are less stable in long schedules.
For this reason, we stick to the predictor with $12$ layers and a width of $1024$.

%%% THIRD ROW

\myparagraph{Registers.}
In our model, the local feature maps serve as a supervisory signal to train the model, so high-quality feature maps are crucial.
To this end, we use register tokens, which were proposed to improve the quality of feature maps.
We see that using registers has a large effect on performance, with an improvement of $3.2$ points on ADE and $2.1$ points on ImageNet when using 16 registers (\cref{tab:ablations:n_reg}).

\myparagraph{Positional encoding.}
In our experiments, we consider using different versions of positional encoding.
We try the classic learnable position embeddings as well as relative ones such as RoPE~\citep{rope}.
Because of the ease of use and transferability to higher resolutions, we settle on using RoPE.
% I used rope because I wanted to avoid positional collapse. But maybe now we can switch back
% I have an exp with learnable + standard SK, it does worse, maybe we can see some positional collapse there

\myparagraph{Sinkhorn-Knopp algorithm.}
We describe the problem of positional collapse in \cref{sec:loss}.
Changing the set of points considered in the Sinkhorn solves it, improving stability and granting a small performance increase (\cref{tab:ablations:sinkhorn}).
We did not observe any positional collapse when using it.

%%% ADDITIONAL

\begin{figure}
  \begin{minipage}{0.3\linewidth}
      \centering
      \begin{tabular}{lll}
        \toprule
        \#prototypes & ADE & IN1k \\
      \midrule
      1024 & 14.9 & 73.8 \\
      2048 & 19.8 & 77.4 \\
      4096 & 27.5 & 80.7 \\
      8192 & 28.5 & 81.3 \\
      \rowcolor{lightgray}
      16384 & \bfseries29.1 & 81.4 \\
      \rowcolor{white}
      32768 & \bfseries29.1 & \bfseries81.7 \\
      \bottomrule
      \end{tabular}
  \end{minipage}
  \hfill
  \begin{minipage}{0.35\linewidth}
    \includegraphics{plot_ablation_train_len}
  \end{minipage}
  \hfill
  \begin{minipage}{0.3\linewidth}
    \small{
      \begin{tabular}{l c c}
        \toprule
        dataset & ADE20K & IN1k \\
        \midrule
        IN1k & 28.0 & \bfseries 81.7 \\
        \rowcolor{lightgray}
        IN22k & 29.1 & 81.4 \\
        \rowcolor{white}
        \ourdataset & \bfseries 30.6 & 81.2 \\
        \bottomrule
      \end{tabular}
    }
  \end{minipage}
  \caption{
    Additional ablation experiments.
    \textbf{(Left)} Influence of the number of prototypes.
    \textbf{(center)} Influence of the training length.
    Each point here is an independent training.
    \textbf{(right)} Influence of the training dataset.
  }
  \label{fig:scaling}
\end{figure}


\myparagraph{Number of prototypes.}
We evaluate the effect of varying the number of prototypes $p$ in our clustering-based loss.
The performance generally increases with the number of prototypes, at the cost of a higher memory footprint.
We use $K=16384$, which strikes a good balance between memory and performance.


\myparagraph{Scaling.}
We study the effect of three scaling axes on the performance of our model: number of parameters, training length, and dataset size.
To study the scaling potential of our algorithm, we train additional ViT-S and ViT-B models.
We report the performance of the family of models in Fig.~\ref{fig:pullfig}.
We see that performance consistently improves with model size, and for all models, our algorithm outperforms the state-of-the-art.
In Fig.~\ref{fig:scaling}~(center), we report the effect of the number of training iterations and pretraining dataset on performance.
We train models using the ablation configs, on ImageNet-22k and with patch size 16.
In Fig.~\ref{fig:scaling}~(right), we show the influence of the training data on model performance.
Using a larger dataset has a positive effect on segmentation, while only slightly deteriorating the ImageNet-1k accuracy.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% IMAGE CLASSIFICATION %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Results}


%%%%%%%%%%%%%%%%%%%%
%%% MAIN RESULTS %%%
%%%%%%%%%%%%%%%%%%%%

% Tables maintained at REDACTED
% bold must be done manually
\begin{table*}[t]
  \centering
  \resizebox{\linewidth}{!}{
    \begin{tabu}{lll c ccccc c ccc}
      \toprule
      &&&& \multicolumn{5}{c}{ImageNet} && \\
      \cmidrule{5-9}
	Model & Arch. & Dataset &  & val & v2 & ReaL & A & ObjectNet &  & iNat & Places205 & SUN397 \\
	\midrule
	\rowfont{\color{gray}}iBOT & ViT-L/16 & IN1k &  & 80.9 & 86.5 & 70.3 & 41.9 & 28.9 &  & 70.5 & 62.0 & 64.6 \\
	I-JEPA & ViT-H/14 & IN1k &  & 79.5 & 85.3 & 68.7 & 37.0 & 22.6 &  & 64.2 & 59.9 & 61.9 \\
	MAE & ViT-L/16 & IN1k &  & 79.4 & 85.3 & 68.9 & 38.3 & 19.3 &  & 69.3 & 60.6 & 61.8 \\
	Data2Vec 2.0 & ViT-L/16 & IN1k &  & 80.1 & 85.7 & 69.4 & 42.1 & 24.6 &  & 65.4 & 61.9 & 64.4 \\
	\vspace{-0.2cm} &  &  &  &  &  &  &  &  &  &  &  &  \\
	CAPI & ViT-L/14 & IN1k &  & \bfseries82.9 & \bfseries87.6 & \bfseries72.9 & \bfseries47.5 & \bfseries43.7 &  & \bfseries76.8 & \bfseries65.4 & \bfseries70.9 \\
	\midrule\rowfont{\color{gray}}DINOv2 & ViT-g/14 & LVD-142M &  & 87.4 & 90.3 & 80.1 & 67.0 & 81.7 &  & 88.3 & 68.8 & 79.3 \\
	BeiT & ViT-L/16 & IN22k &  & 40.8 & 46.1 & 30.7 & \phantom{0}8.7 & \phantom{0}2.1 &  & 26.5 & 36.8 & 29.9 \\
	I-JEPA & ViT-H/14 & IN22k &  & 78.1 & 84.3 & 67.4 & 38.6 & 25.1 &  & 67.7 & 60.2 & 65.5 \\
	AIM & ViT-600M/14 & DFN-2B+ &  & 79.0 & 84.8 & 67.9 & 41.0 & 20.4 &  & 73.5 & 62.5 & 66.1 \\
	\vspace{-0.2cm} &  &  &  &  &  &  &  &  &  &  &  &  \\
	CAPI & ViT-L/14 & IN22k &  & 83.6 & 88.1 & 74.3 & 55.2 & 52.4 &  & \bfseries82.0 & 66.3 & 74.5 \\
	CAPI & ViT-L/14 & Places205 &  & 79.2 & 84.7 & 68.4 & 39.1 & 33.0 &  & 73.4 & \bfseries68.6 & \bfseries77.5 \\
	CAPI & ViT-L/14 & \ourdataset &  & \bfseries83.8 & \bfseries88.2 & \bfseries74.8 & \bfseries55.3 & \bfseries56.8 &  & 81.2 & 67.1 & 75.6 \\
	\bottomrule
    \end{tabu}
  }
  \caption{
    Image classification results.
    For each baseline method, we report the model size closest to ViT-L/14.
  }
  \label{tab:main_results}
\end{table*}

\myparagraph{Image classification.}
We evaluate our model and compare it to state-of-the-art reconstruction-based SSL models.
We run the evaluation on four datasets including object recognition, fine-grained classification, and scene recognition.
We use ImageNet-1k~\citep{russakovsky2015imagenet}, iNaturalist 2021~\citep{van2021benchmarking}, Places205~\citep{zhou2017scene}, and SUN397~\citep{xiao2010sun}.
For ImageNet, we also report OOD robustness by running inference on additional test sets:
ImageNet-V2~\citep{recht2019imagenet}, ImageNet-ReaL~\citep{beyer2020imagenetreal}, ImageNet-A~\citep{hendrycks2021natural}, and ObjectNet~\citep{barbu2019objectnet}.
For each model, we resize the image to 224$\times$224 and collect the patch tokens output by the model.
We feed those to an attentive probe implemented as a single layer of multi-head cross-attention with a single query (head size $d//64$, no residual).
For $c$ classes and an embedding size $d$, the probe contains $2d^2 + (3+c)d$ parameters, which are trained with AdamW for $10$ epochs, selecting the best learning rate for each model/task on a held-out split of the training set.
More details on the protocol are available in \cref{sec:eval_protocol}.
All the results are summarized in Table~\ref{tab:main_results}.

We see that our model outperforms all previous state-of-the-art models, by a large margin.
When training on ImageNet-1k, we observe very good results, outperforming all other reconstruction-based models of comparable size.
CAPI excels particularly on out-of-distribution generalization, outperforming all baselines by more than 19 points on ObjectNet.
Additionally, while the gap with other methods is somewhat limited on ImageNet, the difference in scene classification (SUN397) is much larger.
Interestingly, we observe that CAPI works particularly well on larger and more diverse datasets.
Our three CAPI models outperform all baselines on all datasets, except the CAPI-Places205 which is slightly below AIM on ImageNet-A.
It should be noted, however, that AIM was trained on more than 2 billion images, with a sampling distribution tailored towards ImageNet.
CAPI significantly reduces the gap between reconstruction-based methods and our topline DINOv2+reg: the gap on ImageNet goes from 8.4 points to 3.6, and on SUN397 goes from 13.2 to 1.8.

%%%%%%%%%%%%%%%%%%%
%%% DENSE EVALS %%%
%%%%%%%%%%%%%%%%%%%

\begin{table*}[t]
  \centering
  \small
  {    \begin{tabu}{lll c cc c cc c cc}
      \toprule
      &&&& \multicolumn{2}{c}{ADE-20k} && \multicolumn{2}{c}{Pascal-VOC} && \multicolumn{2}{c}{Cityscapes} \\
      \cmidrule{5-6} \cmidrule{8-9} \cmidrule{11-12}
	Model & Arch. & Dataset &  & $k$-NN & linear &  & $k$-NN & linear &  & $k$-NN & linear \\
	\midrule
	\rowfont{\color{gray}}iBOT & ViT-L/16 & IN1k &  & 26.0 & 30.7 &  & 60.2 & 68.8 &  & 35.7 & 39.8 \\
	I-JEPA & ViT-H/14 & IN1k &  & 20.8 & 25.7 &  & 56.7 & 63.6 &  & 26.4 & 34.5 \\
	MAE & ViT-L/16 & IN1k &  & 21.5 & 27.4 &  & 53.7 & 61.5 &  & 32.8 & 38.5 \\
	Data2Vec 2.0 & ViT-L/16 & IN1k &  & 24.2 & 27.6 &  & 57.5 & 58.1 &  & 32.8 & 38.2 \\
	\vspace{-0.2cm} &  &  &  &  &  &  &  &  &  &  &  \\
	CAPI & ViT-L/14 & IN1k &  & \bfseries29.2 & \bfseries34.4 &  & \bfseries60.7 & \bfseries69.7 &  & \bfseries35.6 & \bfseries41.7 \\
	\midrule\rowfont{\color{gray}}DINOv2 & ViT-g/14 & LVD-142M &  & 34.0 & 39.0 &  & 63.0 & 72.8 &  & 42.0 & 46.8 \\
	BeiT & ViT-L/16 & IN22k &  & \phantom{0}3.5 & \phantom{0}8.3 &  & \phantom{0}6.9 & 19.1 &  & 15.6 & 24.0 \\
	I-JEPA & ViT-H/14 & IN22k &  & 18.9 & 26.3 &  & 55.0 & 64.2 &  & 23.2 & 34.2 \\
	AIM & ViT-600M/14 & DFN-2B+ &  & 26.1 & 31.4 &  & 60.2 & 67.0 &  & 32.1 & 38.2 \\
	\vspace{-0.2cm} &  &  &  &  &  &  &  &  &  &  &  \\
	CAPI & ViT-L/14 & IN22k &  & 29.7 & 35.2 &  & 61.1 & 70.4 &  & 35.2 & 41.0 \\
	CAPI & ViT-L/14 & Places205 &  & \bfseries35.2 & \bfseries39.1 &  & 61.7 & 69.4 &  & \bfseries39.5 & \bfseries44.6 \\
	CAPI & ViT-L/14 & \ourdataset &  & 32.1 & 37.2 &  & \bfseries63.8 & \bfseries72.7 &  & 38.9 & 44.3 \\
	\bottomrule
\end{tabu}
  }
  \caption{
    Comparison with the state of the art on image segmentation using frozen features.
    We report both $k$-NN and linear segmentation performance.
    For reference, we also report the performance of some other non-MIM SSL models.
    This shows that CAPI narrows the gap using only a MIM approach.
  }
  \vspace{-0.6em}
  \label{tab:dense}
\end{table*}

\myparagraph{Dense image understanding.}
As we have seen above, our model allows training high-quality local features that can be successfully pooled to solve image-level tasks.
We also want to evaluate our model on dense prediction problems such as image segmentation.
To this end, we run $k$-NN and linear segmentation following the protocol described in the experimental details.
We run this for all the baselines reported above on three datasets.
We use ADE-20k~\citep{zhou2017scene}, Pascal VOC 2012~\citep{everingham2010pascal}, and Cityscapes~\citep{cordts2016cityscapes}.
We report mIoU for all configurations in Table~\ref{tab:dense}.

As in the classification evals, CAPI trained on ImageNet-1k outperforms all reconstruction-based baselines by quite a wide margin on all evaluation setups.
When training on larger datasets, the conclusion is similar: CAPI outperforms all baselines by a wide margin and even beats DINOv2+reg in some setups.
When trained on Places205, CAPI achieves mIoU 1.2 points higher than DINOv2+reg.
To our knowledge, this is the first time DINOv2+reg is bested on segmentation with frozen features on ADE20K.
DINOv2+reg, however, remains the most versatile model, with good results across the board and the best results on Cityscapes.

\subsection{Additional explorations}
\label{sec:finalexp}
As a final set of experiments, we investigate some additional properties of our model.
We investigate its robustness to change of input resolution and try to obtain global representations using the predictor.



\begin{table}[b]
    \centering
    \begin{tabular}{ll c cc}
      \toprule
      Method  & train res. && eval@224 & eval@448 \\
      \midrule
      I-JEPA-H  & 224 && 79.4 & 78.4 \\
      I-JEPA-H  & 448 && 79.6 & 82.5  \\
      CAPI-L    & 224 && \bfseries 83.8 & \bfseries 83.5 \\
      \bottomrule
    \end{tabular}
    \caption{
    ImageNet-1k attentive probing accuracy of I-JEPA and CAPI at different input resolutions.
    }
    \label{tab:high_res}
\end{table}

\myparagraph{High-resolution image understanding.}
Our model was trained on $224 \times 224$ images.
To compare with the I-JEPA model trained natively at high resolution, we try to evaluate our model on $448\times{}448$ images.
In \cref{tab:high_res}, we see that our model does not require high-resolution training or evaluation to achieve the best performance.
Our model trained at 224 and evaluated at 224 outperforms the large I-JEPA model trained at 448 and evaluated at 448.
Moreover, using RoPE embeddings, CAPI is more robust to resolution changes: it loses only $0.5\%$ (versus $1.0\%$ for I-JEPA) when increasing the resolution.


\begin{figure*}
  \centering
  \includegraphics[width=\linewidth]{pca_viz}
  \caption{
    Visualization of the features of CAPI and baseline models.
    We apply a PCA to the features and map the three first components to RGB.
    The features produced by CAPI are discriminative and smooth.
  }
  \label{fig:quali}
\end{figure*}

\myparagraph{Qualitative feature analysis.}
We propose a qualitative assessment of the dense features in Fig.~\ref{fig:quali}.
The dense features computed with CAPI are amongst the most discriminative and smooth.
We see the emergence of distinct objects, without much noise in uniform regions.
We observe that the CAPI features are less noisy than the ones from DINOv2+reg, while being more focused on semantics and less on colors than the MAE features.
For example, the shaded building in the first image has CAPI features similar to the other buildings, while in MAE the features are closer to other dark areas of the image.

\begin{table}
\centering
    \begin{tabular}{l c cc c c}
      \toprule
      && \multicolumn{2}{c}{IN1k} & iNat21 & \multicolumn{1}{c}{SUN397} \\
      \cmidrule{3-4} \cmidrule{5-5} \cmidrule{6-6}
      Representation  && $k$-NN & Linear & Linear & Linear \\
      \midrule
      avg. pooling  && 57.1 & 77.1 & 49.1 & 73.3 \\
      predictor pooling  && \bfseries 73.8 & \bfseries 81.1 & \bfseries 69.6 & \bfseries 77.4 \\
      \bottomrule
    \end{tabular}
  \caption{
    Classification using predictor representations, compared to the average pooling of the patch tokens.
  }
  \label{tab:global_repr}
\end{table}

\myparagraph{Obtaining global representations.}
In all the previous evaluations reported in the experimental section, we trained a head on top of local features.
Our model does not provide an aggregate representation like the \texttt{[CLS]} token in DINOv2.
In this experiment, we try to exploit the predictor to obtain global image representations.
We forward the whole image through the encoder, and then pass the same amount of mask tokens through the first attention layer of the predictor, cross-attending to the patch tokens.
We obtain a global representation by average-pooling the output of this predictor attention layer.
We train a linear model on this representation on several classification datasets and compare it with the average pooling of the patch embeddings in \cref{tab:global_repr}.
We see that using the attention pooling learned by the predictor provides better representations than averaging local features from the encoder.
