% main story:
% self sup representations works in vision
% People wonder if it's possible to do as in NLP, eg BERT
% In vision, it's called MIM
% It's not SOTA yet
% We do an empirical study of it, an ablation, to reveal its potential


\section{Introduction}

Recent advances in large-scale visual representation learning have established foundation models as a cornerstone of modern computer vision.
Self-supervised representations have proven particularly effective in domains with limited annotations, such as satellite imagery~\citep{tolan2024very} and medical imaging~\citep{gigapath,virchow,chen2024towards,moutakanni2024advancing,endodino}, while enabling breakthroughs in more fundamental vision tasks like monocular depth estimation~\citep{depthanything,depthpro,depthanythingv2}, keypoint matching~\citep{roma}, and tracking~\citep{dinotracker}.
This shift from small supervised models to large self-supervised generalist models mirrors the evolution in natural language processing since the publication of BERT~\citep{bert} and GPT~\citep{gpt}, where large-scale models pretrained on web-scale unlabeled data have become ubiquitous foundation models.

However, the impressive scalability of language models remains unmatched in vision: the best self-supervised visual encoders contain around one billion parameters~\citep{dinov2}, hundreds of times smaller than current language models~\citep{deepseekv3}.
A reason for this gap may be found in the discrepancy between the pretraining tasks used in vision and in language.
The success of language models stems from the generality of the language modeling task: modeling the distribution of data, conditioned on context.
Naturally, researchers have attempted to adapt this approach to computer vision, which resulted in masked image modeling (MIM): the prediction of missing image content given surrounding context.

Yet, existing MIM approaches have not matched the representation quality of alternative self-supervised methods.
Pixel-level reconstruction objectives, \eg MAE~\citep{he2021masked}, provide good initialization for fine-tuning, but yield poor frozen representations, possibly because the target is too low-level to capture the task's inherent uncertainty~\citep{lecun2022path,ijepa}.
Instead of reconstructing pixels, other works propose reconstructing in a pretrained encoder's latent space~\citep{caev2,eva,eva02}.
However, this approach requires an existing encoder and cannot learn representations from scratch.

The most promising direction uses the latent representation of the \emph{online} model -- or an exponential moving average (EMA) of it -- as a learning target for the model, bootstrapping an informative latent space from scratch.
This approach is used in the current state-of-the-art method for self-supervised learning of representations, DINOv2~\citep{dinov2}.
However, these methods often suffer from poor stability~\citep{ibot} and sensitivity to hyperparameters~\citep{ijepa}, requiring additional objectives like contrastive learning to produce competitive representations~\citep{ibot,dinov2,mimrefiner}.

In this work, we focus on online latent masked image modeling.
We isolate it from other stabilizing objectives, and systematically study the design choices it involves.
We center the discussion around three aspects of the masked image modeling principle: the target representation, the formulation of the loss, and the architecture used to perform predictions.
We show that with the right implementation, a simple masked image modeling objective can lead to features competitive with the current state of the art in SSL.
In brief, our method relies on a pair of teacher-student vision transformers trained with self-distillation (\cref{fig:pullfig} left).
The training signal for the student comes from the patch representations of the teacher, which is updated through an exponential moving average (EMA).
The patch embeddings of the teacher are converted to soft assignments on pseudo-categories using a learned online clustering, while the student receives a partially masked image as input and is trained to predict the clustering assignments of the missing patches.
Using this method, we train CAPI, a 300-million parameter visual encoder whose representations approach DINOv2's performance while significantly outperforming previous masked image models (\cref{fig:pullfig} right).

\begin{figure}[t]
  \centering
  \hfill
  \begin{subfigure}{.32\textwidth}
    \includegraphics{pullfig_compact}
  \end{subfigure}
  \hfill
  \begin{subfigure}{.64\textwidth}
    \centering
    \includegraphics{plot_scaling}
  \end{subfigure}
  \hfill
  \caption{
    \textbf{CAPI Method overview:}
    image patches embedded by a teacher are grouped into clusters.
    Their assignments are then used as the training signal for the student.
    The teacher and the student are jointly learned via self-distillation.
    The loss is purely about predicting the content of missing patches and does not rely on augmentations or a contrastive loss.
    \textbf{Evaluation scores:}
    we evaluate frozen representations on ADE20K segmentation with a $k$-nn and linear probe and on ImageNet-1k classification with an attentive probe.
    We compare to MAE, data2vec 2.0, I-JEPA, and AIM.
    Compared to other masked image models, CAPI achieves higher performance with fewer FLOP, scaling well with model size, and approaches the scores of DINOv2+reg.
    }
  \label{fig:pullfig}
\end{figure}


