\section{Related Work}
\label{sec:related}

\myparagraph{Self-supervised representation learning.}
% What is SSL and why is useful
Self-supervised learning (SSL) is a pre-training paradigm in which a model is optimized to solve a pretext task on unlabeled data, often collected at scale.
As the model is not trained to match specific human annotations, the benefit of this type of training is to produce a generalist model, that can be adapted to solve many different downstream tasks.
% Fine-tuning vs representation learning
Depending on the application, these tasks can be solved either by fully fine-tuning the mode, or by using the representations extracted with the frozen model.
In the SSL literature, some works have focused mainly on full fine-tuning~\citep{he2021masked,huang2023contrastive}, while others have shown that frozen representations can reach excellent performance on a wide range of tasks, avoiding costly fine-tuning~\citep{dinov2} and generalizing to annotation-scarce domains where fine-tuning is not possible~\citep{tolan2024very,gigapath}.
% History of SSL
Historically, early work on self-supervised learning focused on hand-crafted pretext tasks such as predicting the rotation of an image~\citep{gidaris2018unsupervised}, the relative position of patches~\citep{doersch2015unsupervised} or the color of a grayscale image~\citep{zhang2016colorful}.
Subsequent works pushed the field forward with methods based on clustering~\citep{deepcluster,swav,dino} and contrastive learning~\citep{simclr,he2020momentum}.
Nowadays, the best self-supervised encoder inherits from these families~\citep{dinov2} and complements them with a masked image modeling objective~\citep{ibot}.
However, training with both global and MIM objectives can prove difficult, as multiple components can interact negatively\footnote{
\citet{ibot} showed that using a shared head for the DINO and iBOT losses gave better results at small scales.
However, at large scale, \citet{dinov2} observed that this caused the iBOT loss to explode late into the training, significantly degrading performance.
Untying the two heads prevented the two losses from competing directly, stabilizing the training.
}.
In this work, we study the masked image modeling component in isolation, suggesting improvements to properly stabilize the optimization objective in the absence of a global term.

\myparagraph{Pixel reconstruction.}
Learning by predicting a missing part of an image was first proposed in Context encoders~\citep{pathakCVPR16context}.
This was envisioned as conceptually similar to denoising autoencoders~\citep{vincent2008extracting}, in the case where the noise is a masking process.
More recently, the success of masked language modeling~\citep{bert} and autoregressive pretraining in natural language processing~\citep{gpt} brought a new wave of interest for transferring these ideas to vision.
iGPT~\citep{igpt} was the first effort to train a transformer~\citep{vaswani2017attention} by generating pixels.
\citet{igpt} proposed rasterizing images to very low resolution, then training for autoregressive next-token prediction.
Then, the advent of the ViT~\citep{dosovitskiy2020image} architecture sparked further interest in the field.
Following the initial exploration of \citet{dosovitskiy2020image}, BeiT~\citep{beit} tried using the quantized latents of a dVAE as targets for a masked image pretraining, using the tokenizer from DALL$\cdot$E~\citep{ramesh2021clip}.
BeiT has proven useful as an initialization for further fine-tuning, but severely underperformed baselines in representation learning.
To simplify BeiT, SimMIM, and MAE~\citep{xie2021simmim, he2021masked} concurrently proposed using raw pixels as targets. 
Thanks to a clever encoder/decoder architecture, MAE proved very stable and reached interesting representations, despite its simplicity.
However, it still fell short of previous SSL methods in terms of representation quality: MAE models need to be scaled to a ViT-H size to match the linear probing performance of a $25\times$ smaller DINOv1 ViT-S/16\citep{dino}.

\myparagraph{Latent reconstruction.}
Concurrently to MAE, \citet{ibot} proposed iBOT. To obtain a more semantic tokenization, iBOT used the online output of the model being trained as the reconstruction target for masked image modeling.
This led to good representations, and the method was reused to obtain the current state-of-the-art~\citep{dinov2}.
However, the iBOT objective was very unstable and required an additional DINO~\citep{dino} loss to stabilize the training.
The idea of using online representations as targets was then reused in data2vec~\citep{data2vec} and I-JEPA~\citep{ijepa,barstochastic}.
I-JEPA in particular proposed reusing the encoder/decoder architecture of MAE and removing the projection head of iBOT, to obtain a more stable objective in the latent space.
The improvements in I-JEPA established a new tradeoff between stability and performance, but it was still both sensitive to hyperparameters ($-12$ points on IN-1k when changing the ``target scale''  from $[0.15,0.2]$ to $[0.125,0.2]$~\citep{ijepa})) and weaker than DINOv2 (81.1 on ImageNet-1k, 5.4 below DINOv2 while using a model twice bigger).

\myparagraph{Clustering in self-supervised learning.}
Our approach is also related to methods that use clustering for self-supervised learning.
First of this line of work, DeepCluster~\citep{deepcluster} proposed using a simple $k$-means to obtain pseudo-labels.
Subsequently, SwAV~\citep{swav} introduced an online clustering to replace the offline $k$-means.
DINO~\citep{dino} built on SWaV, and made the clustering be implicitly learned by the MLP projection head of the student.
Finally, iBOT~\citep{ibot} proposed to reuse the DINO projection head for masked image modeling, implicitly using a clustering as target.
Our approach reuses this idea of using a clustering to create the targets of a masked image modeling objective, but backtracks to the origin of this line of work: instead of using an MLP head that performs an implicit clustering, we use an explicit clustering.
By separating the clustering process from the rest of the training, we isolate the two components, making the training more stable and transparent.