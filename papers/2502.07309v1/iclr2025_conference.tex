
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\usepackage{graphicx}
\usepackage{booktabs} % for professional tables
\usepackage{adjustbox}
\usepackage{color, colortbl}
\usepackage{multirow}
\usepackage{eucal}
\usepackage{wrapfig}
\usepackage{subcaption}
% \usepackage{mathtools}
% \usepackage{marvosym}
% \usepackage{algorithmic}

\title{Semi-Supervised Vision-Centric 3D Occupancy World Model for Autonomous Driving}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Xiang Li,~ 
Pengfei Li,~ 
Yupeng Zheng,~ 
Wei Sun,~ 
Yan Wang\thanks{Corresponding authors.}~,~ 
Yilun Chen\footnotemark[1] \\
Institute for AI Industry Research (AIR), Tsinghua University \\
\texttt{l-x21@mails.tsinghua.edu.cn; wangyan@air.tsinghua.edu.cn}
}

% \author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
% about author (webpage, alternative address)---\emph{not} for acknowledging
% funding agencies.  Funding acknowledgements go at the end of the paper.} \\
% Department of Computer Science\\
% Cranberry-Lemon University\\
% Pittsburgh, PA 15213, USA \\
% \texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
% \And
% Ji Q. Ren \& Yevgeny LeNet \\
% Department of Computational Neuroscience \\
% University of the Witwatersrand \\
% Joburg, South Africa \\
% \texttt{\{robot,net\}@wits.ac.za} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
% }

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Understanding world dynamics is crucial for planning in autonomous driving. Recent methods attempt to achieve this by learning a 3D occupancy world model that forecasts future surrounding scenes based on current observation. However, 3D occupancy labels are still required to produce promising results. Considering the high annotation cost for 3D outdoor scenes, we propose a semi-supervised vision-centric 3D occupancy world model, \textbf{PreWorld}, to leverage the potential of 2D labels through a novel two-stage training paradigm: the self-supervised pre-training stage and the fully-supervised fine-tuning stage. Specifically, during the pre-training stage, we utilize an attribute projection head to generate different attribute fields of a scene (e.g., RGB, density, semantic), thus enabling temporal supervision from 2D labels via volume rendering techniques. Furthermore, we introduce a simple yet effective state-conditioned forecasting module to recursively forecast future occupancy and ego trajectory in a direct manner. Extensive experiments on the nuScenes dataset validate the effectiveness and scalability of our method, and demonstrate that PreWorld achieves competitive performance across 3D occupancy prediction, 4D occupancy forecasting and motion planning tasks.\footnote{Codes and models can be accessed at \url{https://github.com/getterupper/PreWorld}.}
\end{abstract}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\textwidth]{figures/teaser_revised.pdf}
  \caption{
  \textbf{(a) Self-Supervised 3D Occupancy Model} can be trained using solely 2D labels as supervision. However, it lacks the capability to forecast future occupancy. 
  In contrast, \textbf{(b) Fully-Supervised 3D Occupancy World Model} can forecast future occupancy, but it relies on 3D occupancy labels for meaningful results due to its indirect architecture, which employs a frozen 3D occupancy model. 
  To tackle these challenges, our \textbf{(c) Semi-Supervised 3D Occupancy World Model}, featuring 2D rendering supervision and an end-to-end architecture, can forecast future occupancy straightly from image inputs while taking advantage of 2D labels.}
  \label{fig:teaser}
\end{figure}

\section{Introduction}\label{intro}

3D scene understanding forms the cornerstone of autonomous driving, exerting a direct influence on downstream tasks such as planning and navigation. Among various 3D scene understanding tasks~\citep{wang2022detr3d, li2022hdmapnet, wei2023surroundocc, jin2024tod3cap}, \textit{3D Occupancy Prediction} plays a crucial role in autonomous systems. Its objective is to predict the semantic occupancy of each voxel throughout the entire scene from limited observation. To this end, some previous methods~\citep{liong2020amvnet, cheng20212, xia2023scpnet} prioritize LiDAR as input modality due to its robust performance in capturing accurate geometric information. Nevertheless, they are often considered hardware-expensive. Consequently, there has been a shift towards vision-centric solutions in recent years~\citep{zhang2023occformer, li2023voxformer, zheng2024monoocc}.

Despite significant advancements in aforementioned methods, they primarily focus on enhancing better perception of the current scene. For advanced collision avoidance and route planning, autonomous vehicles need to not only comprehend the current scene but also forecast the evolution of future scenes based on the understanding of world dynamics. Therefore, \textit{4D Occupancy Forecasting} has been introduced to forecast future 3D occupancy given historical observations. Recent works have aimed to achieve this by learning a 3D occupancy world model~\citep{zheng2023occworld, wei2024occllama}. However, when processing image inputs, these methods follow an circuitous path, as shown in Fig~\ref{fig:teaser} (b). Typically, a pre-trained 3D occupancy model is employed to obtain current occupancy, which is then fed into a forecasting module to generate future occupancy. The forecasting module includes a tokenizer that encodes occupancy into discrete tokens, an autoregressive architecture to generate future tokens, and a decoder to obtain future occupancy. Information loss is prone to occur in such repeated encoding and decoding processes. Hence, existing methods heavily rely on 3D occupancy labels as supervision to produce meaningful results, leading to notable annotation costs.

In contrast to 3D occupancy labels, 2D labels are relatively easier to acquire. Recently, employing purely 2D labels for self-supervised learning has shown some promising results in 3D occupancy prediction task, as illustrated in Fig~\ref{fig:teaser} (a). By utilizing volumetric rendering, RenderOcc~\citep{pan2024renderocc} employs 2D depth maps and semantic labels to train the model. Methods like SelfOcc~\citep{huang2024selfocc} and OccNerf~\citep{zhang2023occnerf} take a step further, using only image sequences as supervision. However, there have not been similar attempts in 4D occupancy forecasting task. 
% In fact, the cutting-edge self-supervised methods consistently exhibit a performance gap compared to fully-supervised methods on both tasks.

Based on the above observations, we propose \textbf{PreWorld}, a semi-supervised vision-centric 3D occupancy world model, designed to fulfill the utility of 2D labels during training, while achieving competitive performance across both 3D occupancy prediction and 4D occupancy forecasting tasks, as shown in Fig~\ref{fig:teaser} (c). To this end, we propose a novel two-stage training paradigm: the self-supervised pre-training stage and the fully-supervised fine-tuning stage. Inspired by RenderOcc, during the pre-training stage, we introduce an attribute projection head to obtain diverse attribute fields of current and future scenes (e.g., RGB, density, semantic), facilitating temporal supervision through 2D labels using volume rendering techniques. Moreover, we propose a simple yet effective state-conditioned forecasting module, which allows us to simultaneously optimize occupancy network and forecasting module, and directly forecast future 3D occupancy based on multi-view image inputs in an end-to-end manner, thus avoiding possible information loss.

To demonstrate the effectiveness of PreWorld, we conduct extensive experiments on the widely used Occ3D-nuScenes benchmark~\citep{tian2024occ3d} and compare with recent methods using both 2D and/or 3D supervision. Experimental results indicate that our approach can yield competitive performance across multiple tasks. For 3D occupancy prediction, PreWorld outperforms the previous best method OccFlowNet~\citep{boeder2024occflownet} with an mIoU of 34.69 over 33.86. For 4D occupancy forecasting, PreWorld sets the new SOTA performance, outperforming existing methods OccWorld~\citep{zheng2023occworld} and OccLLaMA~\citep{wei2024occllama}. For motion planning, PreWorld yields comparable and often better results than other vision-centric methods~\citep{hu2022st,jiang2023vad,tong2023scene}. Furthermore, we validate the scalability of our two-stage training paradigm, showcasing its potential for large-scale training.

Our main contributions are as follows:
\begin{itemize}
    \item A semi-supervised vision-centric 3D occupancy world model, PreWorld, which takes advantage of both 2D labels and 3D occupancy labels during training.
    \item A novel two-stage training paradigm, the effectiveness and scalability of which has been validated by extensive experiments.
    \item A simple yet effective state-conditioned forecasting module, enabling simultaneous optimization with occupancy network and direct future forecasting based on visual inputs.
    \item Extensive experiments compared to SOTA method, demonstrating that our method achieves competitive performance across multiple tasks, including 3D occupancy prediction, 4D occupancy forecasting and motion planning.
\end{itemize}

\section{Related Work}\label{related_work}

\subsection{3D Occupancy Prediction}

Due to its vital application in autonomous driving, 3D occupancy prediction has attracted considerable attention. According to the input modality, existing methods can be broadly categorized into LiDAR-based and vision-centric methods. While LiDAR-based methods excel in capturing geometric details~\citep{tang2020searching, ye2021drinet++, ye2023lidarmultinet}, vision-centric methods have garnered growing interest in recent years due to their rich semantic information, cost-effectiveness, and ease of deployment~\citep{philion2020lift, liu2023fully, ma2024cotr}. However, these methods focus solely on understanding the current scene while ignoring the forecasting of future scene changes. Therefore in this paper, we follow the approach of OccWorld~\citep{zheng2023occworld} and endeavor to address both of these tasks in a unified manner.

\subsection{World Models for Autonomous Driving}

The objective of world models is to forecast future scenes based on action and past observations~\citep{ha2018world}. In autonomous driving, world models can be utilized to generate synthetic data and aid in decision making. Some previous approaches~\citep{hu2023gaia, gao2023magicdrive, wang2024driving} aim to generate image sequences of outdoor driving scenarios using large pre-trained generative models. However, relying on 2D images as scene representations leads to the lack of structural information. Some works~\citep{khurana2022differentiable, khurana2023point, zhang2023learning} tend to generate 3D point clouds, which on the other hand, fail to capture the semantic of the scene.

Recent attempts have emerged to generate 3D occupancy representations, which combine an understanding of both semantic and geometric information. The pioneering OccWorld~\citep{zheng2023occworld} introduces the 3D occupancy world model that, employing an autoregressive architecture, can forecast future occupancy based on current observation. Taking it a step further, OccLLaMA~\citep{wei2024occllama} integrates occupancy, action, and language, enabling 3D occupancy world model to possess reasoning capabilities. However, when it comes to vision-centric approaches, they both adopt an indirect path, requiring the usage of pre-trained 3D occupancy models for current occupancy prediction, succeeded by an arduous encoding-decoding process to forecast future occupancy. This manner poses challenges in model training, thus necessitating 3D occupancy labels as supervision to yield effective results. Considering this, we explore a straightforward way to directly forecast future occupancy using image inputs.

\subsection{Self-Supervised 3D Occupancy Prediction}

While 3D occupancy provides rich structural information for training, it necessitates expensive and laborious annotation processes. In contrast, 2D labels are more readily obtainable, presenting an opportunity for self-supervised 3D occupancy prediction. Recently, some works have explored using Neural Radiance Fields (NeRFs)~\citep{mildenhall2021nerf} to perform volume rendering of scenes, thereby enabling 2D supervision for the model. RenderOcc~\citep{pan2024renderocc} tends to use 2D depth maps and semantic labels for training. Despite significant performance gaps compared to existing methods, SelfOcc~\citep{huang2024selfocc} and OccNeRF~\citep{zhang2023occnerf} have made meaningful attempts, aiming to solely utilize image sequences for self-supervised learning.

On the contrary, self-supervised approaches have not yet been observed in the realm of 4D occupancy forecasting tasks. Although OccWorld~\citep{zheng2023occworld} offers a self-supervised setting, it merely relies on an existing self-supervised 3D occupancy model to produce current occupancy without engaging in novel endeavors, and it also suffers from subpar performance. Different from OccWorld, we attempt to directly supervise future scenes using 2D labels, thereby optimizing our performance in both 3D occupancy prediction and 4D occupancy forecasting tasks simultaneously.

\section{Method}
\label{method}

\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\textwidth]{figures/main_revised.pdf}
  \caption{
  \textbf{The architecture of our proposed PreWorld.} Firstly, volume features are extracted from multi-view images with an occupancy network. Subsequently, a state-conditioned forecasting module is employed to recursively forecast future volume features using historical features. In the self-supervised pre-training stage, volume features are projected into various attribute fields and supervised by 2D labels through volume rendering techniques. In the fully-supervised fine-tuning stage, the attribute projection head no longer participates in the computations, occupancy predictions are directly obtained via an occupancy head and supervised by 3D occupancy labels.}
  \label{fig:main}
\end{figure}

\subsection{Revisiting 4D Occpuancy Forecasting}

For the vehicle at timestamp $T$, vision-centric 3D occupancy prediction task takes $N$ views of images $S_T = \{ I^1, I^2, ..., I^N \}$ as input and predicts current 3D occupancy $\hat{Y}_T \in \mathbb{R}^{X\times Y\times Z \times C}$ as output, where $(X, Y, Z)$ denote the resolution of the 3D volume and $C$ represents the number of semantic categories, including non-occupied~\citep{huang2023tri, zhang2023occformer, liu2023fully, pan2024renderocc}. A 3D occupancy model $\mathbb{O}$ typically comprises an occupancy network $\mathcal{N}$ and an occupancy head $\mathcal{H}$. The process of occupancy prediction can be formulated as:
\begin{equation}
    F_T = \mathcal{N}(S_T),\ \hat{Y}_T = \mathcal{H}(F_T),
\end{equation}
where $\mathcal{N}$ extracts 3D volume features $F_T \in \mathbb{R}^{X\times Y\times Z \times D}$ from 2D image inputs ($D$ denotes the dimension of volume features), and $\mathcal{H}$ serves as a decoder to convert $F_T$ into 3D occupancy.

Vision-centric 4D occupancy forecasting task, on the other hand, utilizes an image sequence of past $k$ frames $\{S_T, S_{T-1},..., S_{T-k}\}$ as input, aiming at forecasting 3D occupancy of future $f$ frames~\citep{zheng2023occworld, wei2024occllama}. A 3D occupancy world model $\mathbb{W}$ attempt to achieve this by adopting an auto-regressive manner:
\begin{equation}
    \hat{Y}_{T+1} = \mathbb{W}(S_T, S_{T-1},..., S_{T-k}).
\end{equation}
To this end, $\mathbb{W}$ employs an available 3D occupancy model $\mathbb{O}$ to predict 3D occupancy of past $k$ frames $\{\hat{Y}_{T}, ..., \hat{Y}_{T-k}\}$, and leverages a scene tokenizer $\mathcal{T}$, an autoregressive architecture $\mathcal{A}$ and a decoder $\mathcal{D}$ to forecast future 3D occupancy. After obtaining historial occupancy, $\mathbb{W}$ encodes 3D occupancy into discrete tokens $\{z_{T}, ..., z_{T-k}\}$ through $\mathcal{T}$. Subsequently, $\mathcal{A}$ is utilized to forecast future token $z_{T+1}$ based on these tokens, which is then input into $\mathcal{D}$ to generate future occupancy $\hat{Y}_{T+1}$. Formally, the process of occupancy forecasting can be formulated as follows:
\begin{equation}
\begin{aligned}
    &\hat{Y}_{T}, ..., \hat{Y}_{T-k} = \mathbb{O}(S_T),..., \mathbb{O}(S_{T-k}),\\
    &z_{T}, ..., z_{T-k} = \mathcal{T}(\hat{Y}_{T}), ..., \mathcal{T}(\hat{Y}_{T-k}),\\
    &z_{T+1} = \mathcal{A}(z_{T}, ..., z_{T-k}),\ \hat{Y}_{T+1} = \mathcal{D}(z_{T+1}).
\end{aligned}
\end{equation}
Here, we need to mention that $\mathbb{O}$ is pre-trained and frozen during training. For example, OccWorld~\citep{zheng2023occworld} utilizes TPVFormer~\citep{huang2023tri} as $\mathbb{O}$, while OccLLaMA~\citep{wei2024occllama} chooses FB-OCC~\citep{li2023fb}.

\subsection{State-Conditioned Forecasting Module}
\label{sec:forecast-module}

Different from these approaches, we tend to a more straightforward path, which enables us to optimize 3D occupancy model and forecasting module simultaneously. Specially, we employ a state-conditioned forecasting module $\mathcal{F}$ instead of the combination of $\mathcal{T}$, $\mathcal{A}$ and $\mathcal{D}$, as illustrated in Fig~\ref{fig:forecast}. We formulate our approach of occupancy forecasting as follows:
\begin{equation}
    \tilde{F}_{T} = \mathcal{N}(S_T, S_{T-1},..., S_{T-k}),\ \tilde{F}_{T+1} = \mathcal{F}(\tilde{F}_{T}),\ \hat{Y}_{T+1} = \mathcal{H}(\tilde{F}_{T+1}),
\end{equation}
where we leverages $\mathcal{N}$ to extract volume features $\tilde{F}_{T}$ from temporal images, $\mathcal{F}$ to directly forecast future volume features $\tilde{F}_{T+1}$ and $\mathcal{H}$ to transform $\tilde{F}_{T+1}$ into future occupancy $\hat{Y}_{T+1}$.

\begin{wrapfigure}{l}{0.5\linewidth}
  \vspace{-2mm}
  \centering
  \includegraphics[width=0.5\textwidth]{figures/forecast.pdf}
  \vspace{-4mm}
  \caption{
  The proposed state-conditioned forecasting module is simply composed of two MLPs. Ego states can be optionally integrated into the network, as denoted by the dashed arrows.}
  \label{fig:forecast}
  \vspace{-2mm}
\end{wrapfigure}

Without loss of generality, our forecasting module is simply composed of two MLPs. We demonstrate that even without intricate design, this simple architecture can still achieve comparable and even superior results to state-of-the-art methods. This design showcases that previous practice of solely optimizing the forecasting module during training has its limitations. By simultaneously optimizing the occupancy network and forecasting module, 3D occupancy world models can achieve stronger performance. Additionally, our module can optionally incorporate ego-state information such as speed, acceleration and historical trajectories into the network. In Section~\ref{subsec: ablation}, we demonstrate that this approach can further enhance the forecasting capabilities of the model.

Furthermore, this architecture brings an additional benefit for us. Given that previous forecasting modules encode scenes into discrete tokens, they cannot directly supervise future predictions with 2D labels via volume rendering, as done by self-supervised 3D occupancy models~\citep{zhang2023occnerf, huang2024selfocc}. Since our module preserves the volume features of future scenes, it provides an opportunity to train 3D occupancy world models in a self-supervised manner.

\subsection{Temporal 2D Rendering Self-Supervision}
\label{subsec:supervision}

\paragraph{Attribute Projection.} Inspired by \citet{pan2024renderocc}, we transform the temporal volume feature sequence of current and future $f$ frames $\{ \tilde{F} \}_t = \{\tilde{F}_T, \tilde{F}_{T+1}, ..., \tilde{F}_{T+f}\}$ into temporal attribute fields $\{ \tilde{A} \}_t$ through an attribute projection head $\mathcal{P}$:
\begin{equation}
    \{ \tilde{A} \}_t = \{ (\tilde{\sigma}, \tilde{s}, \tilde{c}) \}_t = \mathcal{P}(\{ \tilde{F} \}_t),
\end{equation}
where $\tilde{\sigma}\in \mathbb{R}^{X\times Y\times Z \times 1}$, $\tilde{s}\in \mathbb{R}^{X\times Y\times Z \times D}$ and $\tilde{c}\in \mathbb{R}^{X\times Y\times Z \times 3}$ denote the density, semantic and RGB fields of the 3D volume, respectively. In implementation, $\mathcal{P}$ comprises several MLPs, which is validated to be a simple yet effective method~\citep{boeder2024occflownet}.

\paragraph{Ray Generation.} Given the intrinsic and extrinsic parameters of camera $j$ at timestamp $i$, we can extract a set of 3D rays $\{r\}_{i}^{j}$, where each ray $r$ originates from camera $j$ and corresponds to a pixel of the image $I_{i}^{j}$. Additionally, we can utilize ego pose matrices to transform rays from adjacent $n$ frames to current frame, enabling better capture of surrounding information. These rays collectively constitute the set $\{r\}_i$ utilized for supervising $\tilde{A}_i=(\tilde{\sigma}_i, \tilde{s}_i, \tilde{c}_i)$.

\paragraph{Volume Rendering.} For each $r\in\{r\}_i$, we sample $M$ points $\{u_m\}_{m=1}^{M}$ along the ray. Then the rendering weight $w(u_m)$ of each sampled point $u_m$ can be computed by:
\begin{equation}
\begin{aligned}
    T(u_m) = \text{exp}(-\sum_{p=1}^{m-1}\tilde{\sigma}_i(u_p)\delta_p), \ w(u_m) = T(u_m)(1 - \text{exp}(-\tilde{\sigma}_i(u_m)\delta_m)),
\end{aligned}
\end{equation}
where $T(u_m)$ denotes the accumulated transmittance until $u_m$, and $\delta_m = u_{m+1} - u_m$ denotes the interval between adjacent sampled points. Finally, the 2D rendered depth, semantic and RGB predictions $(\hat{d}^{2D}_i(r), \hat{s}^{2D}_i(r), \hat{c}^{2D}_i(r))$ can be computed by cumulatively summing the products of the values corresponding to each point along the ray and their respective rendering weights:
\begin{equation}
\begin{aligned}
    &\hat{d}^{2D}_i(r)=\sum_{m=1}^{M}w(u_m)u_m,\ \hat{s}^{2D}_i(r)=\sum_{m=1}^{M}w(u_m)\tilde{s}_i(u_m),\ \hat{c}^{2D}_i(r)=\sum_{m=1}^{M}w(u_m)\tilde{c}_i(u_m).
\end{aligned}
\end{equation}

\paragraph{Temporal 2D Rendering Supervision.} After acquiring 2D rendered predictions $(\hat{d}^{2D}_i, \hat{s}^{2D}_i, \hat{c}^{2D}_i)$ with 3D ray set $\{r\}_i$, the temporal 2D rendering loss can be formulated as:
\begin{equation}
\label{eq:loss_2d}
    \mathcal{L}_{2D} = \sum_{i=T}^{T+f}\lambda_{dep}\mathcal{L}_{dep}(d^{2D}_i, \hat{d}^{2D}_i) + \lambda_{sem}\mathcal{L}_{sem}(s^{2D}_i, \hat{s}^{2D}_i) + \lambda_{RGB}\mathcal{L}_{RGB}(c^{2D}_i, \hat{c}^{2D}_i),
\end{equation}
where $(d^{2D}_i, s^{2D}_i, c^{2D}_i)$ represents 2D depth map, semantic
label and RGB of corresponding pixels.

\subsection{Two-Stage Training Paradigm}
\label{paradigm}

\paragraph{Training Scheme.} As illustrated in Fig~\ref{fig:main}, our training scheme for PreWorld includes two stages: In the self-supervised pre-training stage, as illustrated in Section~\ref{subsec:supervision}, we employs the attribute projection head $\mathcal{P}$ to enable temporal supervision with 2D labels. This approach allows us to leverage the abundant and easily obtainable 2D labels, while preemptively optimizing both the occupancy network $\mathcal{N}$ and forecasting module $\mathcal{F}$. In the subsequent fine-tuning stage, we utilize a occupancy head $\mathcal{H}$ to produce occupancy results and use 3D occupancy labels for further optimization.

\paragraph{Training Loss.} For pre-training stage, we employ temporal 2D rendering loss $\mathcal{L}_{2D}$ as formulated in Eq.~\ref{eq:loss_2d}. Specially, we utilize SILog loss and cross-entropy loss from \citet{pan2024renderocc} as $\mathcal{L}_{dep}$ and $\mathcal{L}_{sem}$, respectively, and use L1 loss as $\mathcal{L}_{RGB}$. For fine-tuning stage, we employ focal loss $\mathcal{L}_{f}$, lovasz-softmax loss $\mathcal{L}_{l}$ and scene-class affinity loss $\mathcal{L}_{scal}^{sem}$ and $\mathcal{L}_{scal}^{geo}$, following the practice of \citet{li2023fb}. Therefore, the total loss function for fine-tuning stage can be represented as follows:
\begin{equation}
    \mathcal{L}_{3D} = \lambda_{f}\mathcal{L}_{f} + \lambda_{l}\mathcal{L}_{l} + \lambda_{scal}^{sem}\mathcal{L}_{scal}^{sem} + \lambda_{scal}^{geo}\mathcal{L}_{scal}^{geo}.
\end{equation}

\section{Experiments}
\label{exp}

\subsection{Experiment Settings}

\paragraph{Dataset and Metrics.} Our experiments are conducted on the Occ3D-nuScenes benchmark~\citep{tian2024occ3d}, which provides dense semantic occupancy annotations for the widely used nuScenes dataset~\citep{caesar2020nuscenes}. Each annotation covers a range of $[-40\!\sim\!40m, -40\!\sim\!40\rm m, -1\!\sim\!5.4\rm m]$ around the ego vehicle. The ground-truth semantic occupancy is represented as $200\!\times\! 200\!\times\! 16$ 3D voxel grids with $0.4\rm m$ resolution. Each voxel is annotated with 18 classes (17 semantic classes and 1 free). The official split for training and validation sets is employed. Following common practices, we use mIoU and IoU as the evaluation metric for 3D occupancy prediction and 4D occupancy forecasting tasks, and use L2 error and collision rate for motion planning task.

\paragraph{Implementation Details.} We use the identical network architecture for all the three tasks, yet for the non-temporal 3D occupancy prediction task, we omit temporal supervision and losses accordingly. We adopt BEVStereo~\citep{li2023bevstereo} as the occupancy network $\mathcal{N}$, only replacing its detection head with the occupancy head $\mathcal{H}$ from FB-OCC~\cite{li2023fb} to produce occupancy prediction. For training, we set the batch size to 16, use Adam as the optimizer, and train with a learning rate of $1\times 10^{-4}$. All the hyperparameters $\lambda$ in the loss functions have been set to $1.0$. For 3D occupancy prediction task, PreWorld undergoes $6$ epochs in self-supervised pre-training stage and $12$ epochs in fully-supervised fine-tuning stage. For 4D occupancy forecasting and motion planning task, PreWorld undergoes $8$ epochs in self-supervised pre-training stage and $18$ epochs in fully-supervised fine-tuning stage. All experiments are conducted on 8 NVIDIA A100 GPUs.

\subsection{Results and Analysis}

\begin{table}[t]
\scriptsize
\caption{\textbf{3D occupancy prediction performance on the Occ3D-nuScenes dataset.} GT represents the type of labels used during training. The best and second-best performances are represented by \textbf{bold} and \underline{underline} respectively.}
\vspace{-2mm}
\setlength{\tabcolsep}{0.003\linewidth}
% \setlength{\tabcolsep}{0.0001\linewidth}
\def\mystrut{\rule{0pt}{1.5\normalbaselineskip}}
\centering
\begin{adjustbox}{width=0.99\columnwidth,center}
\label{table:3d-occ}
\begin{tabular}{l c| c c c c c c c c c c c c c c c c c |c}

    \toprule
    Method 
    & GT
    & \rotatebox{90}{others} 
    & \rotatebox{90}{barrier}
    & \rotatebox{90}{bicycle} 
    & \rotatebox{90}{bus} 
    & \rotatebox{90}{car} 
    & \rotatebox{90}{cons. veh} 
    & \rotatebox{90}{motorcycle} 
    & \rotatebox{90}{pedestrian} 
    & \rotatebox{90}{traffic cone} 
    & \rotatebox{90}{trailer} 
    & \rotatebox{90}{truck} 
    & \rotatebox{90}{dri. sur} 
    & \rotatebox{90}{other flat} 
    & \rotatebox{90}{sidewalk} 
    & \rotatebox{90}{terrain} 
    & \rotatebox{90}{manmade} 
    & \rotatebox{90}{vegetation} 
    & \rotatebox{90}{mIoU (\%)}  \\
    \midrule
    SelfOcc~\citep{huang2024selfocc} & 2D & 0.00 & 0.15 & 0.66 & 5.46 & 12.54 & 0.00 & 0.80 & 2.10 & 0.00 & 0.00 & 8.25 & 55.49 & 0.00 & 26.30 & 26.54 & 14.22 & 5.60 & 9.30 \\
    OccNeRF~\citep{zhang2023occnerf} & 2D & 0.00 & 0.83 & 0.82 & 5.13 & 12.49 & 3.50 & 0.23 & 3.10 & 1.84 & 0.52 & 3.90 & 52.62 & 0.00 & 20.81 & 24.75 & 18.45 & 13.19 & 9.53 \\
    RenderOcc~\citep{pan2024renderocc} & 2D &  5.69 & 27.56 & 14.36 & 19.91 & 20.56 & 11.96 & 12.42 & 12.14 & 14.34 & 20.81 & 18.94 & \textbf{68.85} & 33.35 & 42.01 & \underline{43.94} & 17.36 & 22.61 & 23.93 \\
    OccFlowNet~\citep{boeder2024occflownet} & 2D & 1.60 & 27.50 & 26.00 & 34.00 & 32.00 & 20.40 & 25.90 & 18.60 & 20.20 & 26.00 & 28.70 & 62.00 & 27.20 & 37.80 & 39.50 & 29.00 & 26.80 & 28.42 \\
    \midrule
    
    MonoScene~\citep{cao2022monoscene} & 3D & 1.75 & 7.23 & 4.26 & 4.93 & 9.38 & 5.67 & 3.98 & 3.01 & 5.90 & 4.45 & 7.17 & 14.91 & 6.32 & 7.92 & 7.43 & 1.01 & 7.65 & 6.06 \\
    TPVFormer~\citep{huang2023tri} & 3D  & 7.22 & 38.90 & 13.67 & 40.78 & 45.90 & 17.23 & 19.99 & 18.85 & 14.30 & 26.69 & 34.17 & 55.65 & 35.47 & 37.55 & 30.70 & 19.40 & 16.78 & 27.83 \\
    BEVDet~\citep{huang2021bevdet} & 3D  & 4.39 & 30.31 & 0.23 & 32.26 & 34.47 & 12.97 & 10.34 & 10.36 & 6.26 & 8.93 & 23.65 & 52.27 & 24.61 & 26.06 & 22.31 & 15.04 & 15.10 & 19.38 \\
    OccFormer~\citep{zhang2023occformer} & 3D  & 5.94 & 30.29 & 12.32 & 34.40 & 39.17 & 14.44 & 16.45 & 17.22 & 9.27 & 13.90 & 26.36 & 50.99 & 30.96 & 34.66 & 22.73 & 6.76 & 6.97 & 21.93 \\
    BEVFormer~\citep{li2022bevformer} & 3D  & 5.85 & 37.83 & 17.87 & 40.44 & 42.43 & 7.36 & 23.88 & 21.81 & 20.98 & 22.38 & 30.70 & 55.35 & 28.36 & 36.00 & 28.06 & 20.04 & 17.69 & 26.88 \\
    RenderOcc~\citep{pan2024renderocc} & 2D+3D & 4.84 & 31.72 & 10.72 & 27.67 & 26.45 & 13.87 & 18.20 & 17.67 & 17.84 & 21.19 & 23.25 & 63.20 & 36.42 & \textbf{46.21} & \textbf{44.26} & 19.58 & 20.72 & 26.11 \\
    CTF-Occ~\citep{tian2024occ3d} & 3D  & 8.09 & 39.33 & 20.56 & 38.29 & 42.24 & 16.93 & 24.52 & 22.72 & 21.05 & 22.98 & 31.11 & 53.33 & 33.84 & 37.98 & 33.23 & 20.79 & 18.00 & 28.53 \\
    SparseOcc~\citep{liu2023fully} & 3D & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & 30.90 \\
    OccFlowNet~\citep{boeder2024occflownet} & 2D+3D & 8.00 & 37.60 & 26.00 & 42.10 & 42.50 & 21.60 & \underline{29.20} & 22.30 & 25.70 & \underline{29.70} & 34.40 & 64.90 & \underline{37.20} & \underline{44.30} & 43.20 & \textbf{34.30} & \textbf{32.50} & 33.86 \\
    \midrule
    \bf PreWorld (Ours) & 3D & \underline{10.83} & \underline{44.13} & \textbf{26.35} & \underline{42.16} & \underline{46.15} & \underline{22.92} & 28.86 & \underline{26.89} & \underline{26.44} & 28.29 & \underline{34.43} & \underline{65.67} & 35.91 & 41.09 & 37.41 & \underline{30.16} & 29.54 & \underline{33.95} \\
    \bf \ \ \ \ \ + Pre-training & 2D+3D & \textbf{11.81} & \textbf{45.01} & \underline{26.29} & \textbf{43.32} & \textbf{47.71} & \textbf{24.23} & \textbf{31.29} & \textbf{27.41} & \textbf{27.68} & \textbf{30.62} & \textbf{35.64} & 63.71 & \textbf{37.27} & 41.20 & 37.54 & 29.36 & \underline{29.70} & \textbf{34.69} \\
    
\bottomrule
\end{tabular}
\end{adjustbox}

\end{table}

\begin{table}[htbp]
\scriptsize
\caption{\textbf{4D occupancy forecasting performance on the Occ3D-nuScenes dataset.} The latest vision-centric approaches of OccWorld~\citep{zheng2023occworld} and OccLLaMA~\citep{wei2024occllama} are taken as baselines for fair comparison. Aux. Sup. represents auxiliary supervision apart from the ego trajectory. Avg. reprersents the average performance of that in 1s, 2s, and 3s. The best and second-best performances are represented by \textbf{bold} and \underline{underline} respectively.
}
\vspace{-2mm}
% \setlength{\tabcolsep}{0.008\linewidth}
\label{table:4d-occ}
% \def\mystrut{\rule{0pt}{1.5\normalbaselineskip}}
\centering
% \begin{adjustbox}{width=0.99\columnwidth,center}
\resizebox{0.99\linewidth}{!}{
\begin{tabular}{l|c|cccc|cccc}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{Aux. Sup.} &
\multicolumn{4}{c|}{mIoU (\%) $\uparrow$} & 
\multicolumn{4}{c}{IoU (\%) $\uparrow$}  \\
& & 1s & 2s & 3s & \cellcolor{gray!30}Avg. & 1s & 2s & 3s & \cellcolor{gray!30}Avg.  \\
\midrule
    OccWorld-S &  None & 0.28 & 0.26 & 0.24 & \cellcolor{gray!30}0.26 & 5.05 & 5.01 & 4.95 & \cellcolor{gray!30}5.00 \\
    OccWorld-T &  Semantic LiDAR & 4.68 & 3.36 & 2.63 & \cellcolor{gray!30}3.56 & 9.32 & 8.23 & 7.47 & \cellcolor{gray!30}8.34 \\
    OccWorld-D &  3D Occ & 11.55 & 8.10 & 6.22 & \cellcolor{gray!30}8.62 & 18.90 & 16.26 & 14.43 & \cellcolor{gray!30}16.53 \\
    OccLLaMA-F &  3D Occ & 10.34 & 8.66 & \underline{6.98} & \cellcolor{gray!30}8.66 & \textbf{25.81} & \textbf{23.19} & \textbf{19.97} & \cellcolor{gray!30}\textbf{22.99} \\
    \midrule
    \bf PreWorld (Ours) &  3D Occ  & \underline{11.69} & \underline{8.72} & 6.77 & \cellcolor{gray!30}\underline{9.06} & 23.01 & 20.79 & 18.84 & \cellcolor{gray!30}20.88 \\
    \bf \ \ \ \ \ + Pre-training &  2D Labels \& 3D Occ & \textbf{12.27} & \textbf{9.24} & \textbf{7.15} & \cellcolor{gray!30}\textbf{9.55} & \underline{23.62} & \underline{21.62} & \underline{19.63} & \cellcolor{gray!30}\underline{21.62} \\
\bottomrule
\end{tabular}%
% \end{adjustbox}
}
\end{table}

\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\textwidth]{figures/visualization-exp-3docc.pdf}
  \vspace{-1mm}
  \caption{
  \textbf{Qualitative results of 3D occupancy prediction on the Occ3D-nuScenes validation set.} The \textbf{\textcolor[RGB]{255,192,0}{holistic structure}} and \textbf{\textcolor[RGB]{255, 0, 0}{fine-grained details}} of the scene are highlighted by \textcolor[RGB]{255, 192, 0}{orange boxes} and \textcolor[RGB]{255,0,0}{red boxes} respectively. Compared with existing fully-supervised methods and self-supervised methods, PreWorld can obtain better scene structure and capture finer local details.
  }
  \label{fig:vis-3docc}
  \vspace{-2mm}
\end{figure}

% for fair comparison, while the approaches in \textcolor{gray}{gray}
\begin{table}[t]
\scriptsize
\caption{\textbf{Motion planning performance on the Occ3D-nuScenes dataset.} The latest vision-centric approaches of OccWorld~\citep{zheng2023occworld} and OccLLaMA~\citep{wei2024occllama} are taken as baselines for fair comparison. $\dagger$ represents training and inference with ego state information introduced. The best and second-best performances are represented by \textbf{bold} and \underline{underline} respectively.
}
\vspace{-1mm}
\setlength{\tabcolsep}{0.005\linewidth}
\label{table:planning}
\def\mystrut{\rule{0pt}{1.5\normalbaselineskip}}
\centering
\begin{adjustbox}{width=0.99\columnwidth,center}
\begin{tabular}{l|c|cccc|cccc}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{Aux. Sup.} &
\multicolumn{4}{c|}{L2 (m) $\downarrow$} & 
\multicolumn{4}{c}{Collision Rate (\%) $\downarrow$}  \\
&& 1s & 2s & 3s & \cellcolor{gray!30}Avg. & 1s & 2s & 3s & \cellcolor{gray!30}Avg.  \\
\midrule
    ST-P3~\citep{hu2022st} &  Map \& Box \& Depth & 1.33 & 2.11 & 2.90 & \cellcolor{gray!30}2.11 & 0.23 & 0.62 & 1.27 & \cellcolor{gray!30}0.71 \\
    UniAD~\citep{hu2023planning} &  Map \& Box \& Motion \& Track \& 3D Occ & 0.48 & 0.96 & 1.65 & \cellcolor{gray!30}\underline{1.03} & \underline{0.05} & \textbf{0.17} & \textbf{0.71} & \cellcolor{gray!30}\textbf{0.31} \\
    VAD~\citep{jiang2023vad} &  Map \& Box \& Motion & 0.54 & 1.15 & 1.98 & \cellcolor{gray!30}1.22 & \textbf{0.04} & \underline{0.39} & 1.17 & \cellcolor{gray!30}\underline{0.53} \\
    OccNet~\citep{tong2023scene} &  Map \& Box \& 3D Occ & 1.29  & 2.13  & 2.99  & \cellcolor{gray!30}2.14  & 0.21  & 0.59  & 1.37 &  \cellcolor{gray!30}0.72 \\
    \midrule
    OccWorld-S$^{\dagger}$ &  None & 0.67 & 1.69 & 3.13 &  \cellcolor{gray!30}1.83 & 0.19 & 1.28 & 4.59 & \cellcolor{gray!30}2.02 \\
    OccWorld-T$^{\dagger}$ &  Semantic LiDAR & 0.54 & 1.36 & 2.66 & \cellcolor{gray!30}1.52 & 0.12 & 0.40 & 1.59 & \cellcolor{gray!30}0.70 \\
    OccWorld-D$^{\dagger}$ &  3D Occ & 0.52 & 1.27 & 2.41 & \cellcolor{gray!30}1.40 & 0.12 & 0.40 & 2.08 & \cellcolor{gray!30}0.87 \\
    OccLLaMA-F$^{\dagger}$ &  3D Occ & \underline{0.38} & 1.07 & 2.15 & \cellcolor{gray!30}1.20 & 0.06 & 0.39 & 1.65 & \cellcolor{gray!30}0.70 \\
    \midrule
    \bf PreWorld (Ours) &  3D Occ & 0.49 & 1.22 & 2.32 & \cellcolor{gray!30}1.34 & 0.19 & 0.57 & 2.65 & \cellcolor{gray!30}1.14 \\
    \bf \ \ \ \ \ + Pre-training &  2D Label \& 3D Occ & 0.41 & 1.16 & 2.32 & \cellcolor{gray!30}1.30 & 0.50 & 0.88 & 2.42 & \cellcolor{gray!30}1.27 \\
    \color{gray} \bf PreWorld (Ours)$^{\dagger}$ & \color{gray} \color{gray}3D Occ & \color{gray}\textbf{0.22} & \color{gray}0.31 & \color{gray}\underline{0.41} & \color{gray} \cellcolor{gray!30}\textbf{0.31} & \color{gray}0.36 & \color{gray}0.52 & \color{gray}\underline{0.73} & \color{gray} \cellcolor{gray!30}0.54 \\
    \color{gray} \bf \ \ \ \ \ + Pre-training$^{\dagger}$ & \color{gray} \color{gray}2D Label \& 3D Occ & \color{gray}\textbf{0.22} & \color{gray}\textbf{0.30}  & \color{gray}\textbf{0.40}  & \color{gray} \cellcolor{gray!30}\textbf{0.31} & \color{gray}0.21 & \color{gray}0.66 & \color{gray}\textbf{0.71} & \color{gray} \cellcolor{gray!30}\underline{0.53} \\
\bottomrule
\end{tabular}%
\end{adjustbox}
\end{table}

\paragraph{3D Occupancy Prediction.}
We first compare the 3D occupancy prediction performance of our PreWorld model with the latest methods on the Occ3D-nuScenes dataset. As shown in Table~\ref{table:3d-occ}, PreWorld achieves an mIoU of 34.69, surpassing the previous state-of-the-art method, OccFlowNet~\citep{boeder2024occflownet}, which has an mIoU of 33.86, as well as other methods using 2D, 3D, or combined supervision. This highlights the effectiveness of PreWorld in perceiving the current scene. Additionally, the proposed 2D pre-training stage boosts performance by 0.74 mIoU, with improvements observed across nearly all categories, both static and dynamic. These results underscore the importance of the proposed 2D pre-training stage for enhanced scene understanding.

In Figure~\ref{fig:vis-3docc}, we further compare the qualitative results of PreWorld with the latest fully-supervised method SparseOcc~\citep{liu2023fully} and self-supervised method RenderOcc~\citep{pan2024renderocc}. RenderOcc can project scene voxels onto multi-view images to obtain comprehensive supervision from various ray directions, thus capturing abundant geometric and semantic information from 2D labels. However, as shown in the last column, it struggles in predicting unseen regions and understanding the overall scene structure. On the other hand, SparseOcc excels in predicting scene structures. Yet owing to insufficient supervision for small objects and long-tailed objects from 3D occupancy labels, it often encounters information loss when predicting objects like \textit{poles} and \textit{motorcycles}, as shown in the second and the last row. In contrast, our model is initially pre-trained with 2D labels, thereby gaining a sufficient understanding of the scene geometry and semantics. In the fine-tuning stage, the model is further optimized using 3D occupancy labels, enabling PreWorld to better predict scene structures. Consequently, PreWorld performs comparably to SparseOcc in holistic structure predictions but exhibits a clear advantage in predicting fine-grained local details, underscoring the superiority of our training paradigm.

\paragraph{4D Occupancy Forecasting.}
Table~\ref{table:4d-occ} presents the 4D occupancy forecasting performance of PreWorld compared to existing baseline models, OccWorld\citep{zheng2023occworld} and OccLLaMA\citep{wei2024occllama}. When using only 3D occupancy supervision, our method achieves the highest mIoU over the future 3-second interval, outperforming the baselines. This demonstrates the effectiveness of our cooperative training approach for both occupancy feature extraction and forecasting modules in an end-to-end manner. Similar to the results for 3D occupancy prediction, incorporating the 2D pre-training stage further improves both mIoU and IoU across all future timestamps. This highlights how pre-training provides valuable geometric and semantic auxiliary information from dense 2D image representations. Given that 2D labels are more readily available than costly 3D occupancy annotations, the performance boost from the two-stage training paradigm of PreWorld is noteworthy.
% IOU?

\paragraph{Motion Planning.}
% For our approach in \textcolor{gray}{gray}, we utilize ego state information from AD-MLP~\citep{zhai2023rethinking}.
The motion planning results are further compared in Table~\ref{table:planning}. Without incorporating ego-state information, our model performs comparably to occupancy world models and even some well-designed planning models. When ego-state information is utilized following the same configuration as OccWorld and OccLLaMA (indicated in \textcolor{gray}{gray}), our method achieves SOTA performance with significant improvements, further enhanced by the pre-training stage. Since PreWorld follows a direct training paradigm, taking the original images as input and producing planning results, the impact of ego-state is notably different from that in world model baselines. We attribute this difference to the "shortcut" effect observed in prior work~\citep{zhai2023rethinking, li2024ego}. We leave the detailed analysis of the relationship between input ego-state, forecasted occupancy, and planning outcomes for future investigation.

\subsection{Ablation Study}
\label{subsec: ablation}

\begin{minipage}[c]{0.49\textwidth}
\centering
\captionof{table}{Ablation study of different supervision attributes utilized in pre-training stage.
}
\label{table:pretraining-effectiveness}
\begin{tabular}{ccc|l}
\toprule
RGB & Depth & Semantic & mIoU (\%) $\uparrow$ \\
\midrule
 & & & 33.95 \\
\checkmark & & & 34.11 \textcolor[RGB]{0, 0, 255}{(+0.16)}\\
\checkmark & \checkmark & & 34.43 \textcolor[RGB]{0, 0, 255}{(+0.48)}\\
\checkmark & \checkmark & \checkmark & \bf 34.69 \textcolor[RGB]{0, 0, 255}{(+0.74)}\\
\bottomrule
\end{tabular}%
\end{minipage}
\begin{minipage}[c]{0.49\textwidth}
\centering
\captionof{table}{Ablation study of different data scale utilized in pre-training and fine-tuning stage.
}
\label{table:pretraining-scalability}
\begin{tabular}{c|c|l}
\toprule
Fine-tuning & Pre-training & mIoU (\%) $\uparrow$ \\
\midrule
\multirow{2}{*}{150 Scenes} & $\times$ & 18.66 \\
 & 700 Scenes & 25.02 \textcolor[RGB]{0, 0, 255}{(+6.36)} \\
\midrule
\multirow{2}{*}{450 Scenes} & $\times$ & 31.99 \\
 & 700 Scenes & 33.37 \textcolor[RGB]{0, 0, 255}{(+1.38)} \\
\midrule
\multirow{3}{*}{700 Scenes} & $\times$ & 33.95 \\
 & 450 Scenes & 34.28 \textcolor[RGB]{0, 0, 255}{(+0.33)} \\
 & 700 Scenes & \bf 34.69 \textcolor[RGB]{0, 0, 255}{(+0.74)} \\
\bottomrule
\end{tabular}
\end{minipage}

\paragraph{Effectiveness of Pre-training.}The effectiveness of different supervision attributes of the 2D pre-training stage is analyzed in this section. As noted earlier, the benefits of pre-training are consistent across both 3D occupancy prediction and 4D occupancy forecasting. Therefore, to conserve computational resources, we perform ablation experiments on the 3D occupancy prediction task. Table~\ref{table:pretraining-effectiveness} shows that as RGB, depth, and semantic attributes are progressively added during the pre-training stage, the final mIoU results steadily improve. This demonstrates the effectiveness of the three 2D supervision attributes, with even the simplest RGB attribute providing a boost in performance.


\paragraph{Scalability of Pre-training. }To validate the scalability of our approach, we conduct ablation studies on the data scale used in both pre-training and fine-tuning stages, as shown in Table~\ref{table:pretraining-scalability}. Firstly, the introduction of the pre-training stage consistently improves performance across all fine-tuning data scales, where larger pre-training scale leads to better results. Secondly, when the fine-tuning dataset is small (150 scenes), which means costly 3D occupancy labels are limited, the pre-training stage significantly boosts the mIoU from 18.66 to 25.02. Thirdly, with pre-training, the model fine-tuned on a smaller dataset (450 scenes) achieves comparable performance to a model without pre-training but fine-tuned on a larger dataset (700 scenes), with mIoU of 33.37 and 33.95, respectively. These results highlight the effectiveness and scalability of our two-stage training paradigm.

\begin{table}[htbp]
\scriptsize
\caption{\textbf{Ablation study of different components in our approach.} The Copy\&Paste employs our best model for 3D occupancy prediction task. Ego denotes using ego-state information during training. SSP denotes self-supervised pre-training for model. TS denotes trajectory supervision.
}
\vspace{-2mm}
\setlength{\tabcolsep}{0.01\linewidth}
\label{table:components}
\def\mystrut{\rule{0pt}{1.5\normalbaselineskip}}
\centering
\begin{adjustbox}{width=0.99\columnwidth,center}
\begin{tabular}{c|ccc|cccl|cccl}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{Ego} & \multirow{2}{*}{SSP} & \multirow{2}{*}{TS} & 
\multicolumn{4}{c|}{mIoU (\%) $\uparrow$} & 
\multicolumn{4}{c}{IoU (\%) $\uparrow$}  \\
& & &  & 1s & 2s & 3s & \cellcolor{gray!30}Avg. & 1s & 2s & 3s & \cellcolor{gray!30}Avg.  \\
\midrule
    Copy\&Paste & & & & 9.76 & 7.37 & 6.23 & \cellcolor{gray!30}7.79 & 20.44 & 17.73 & 16.20 & \cellcolor{gray!30}18.12 \\
\midrule
    \multirow{5}{*}{PreWorld}& & & & 11.12 & 7.73 & 5.89 & \cellcolor{gray!30}8.25 \textcolor[RGB]{0, 0, 255}{(+0.46)} & 22.91 & 20.31 & 17.84 & \cellcolor{gray!30}20.35 \textcolor[RGB]{0, 0, 255}{(+2.23)} \\
    & \checkmark & & & 11.17 & 8.54 & 6.83 & \cellcolor{gray!30}8.85 \textcolor[RGB]{0, 0, 255}{(+1.06)} & 23.27 & 20.83 & 18.51 & \cellcolor{gray!30}20.87 \textcolor[RGB]{0, 0, 255}{(+2.75)} \\
    & \checkmark & & \checkmark & 11.69	& 8.72	& 6.77	& \cellcolor{gray!30}9.06 \textcolor[RGB]{0, 0, 255}{(+1.27)}	& 23.01 & 	20.79	& 18.84	& \cellcolor{gray!30}20.88 \textcolor[RGB]{0, 0, 255}{(+2.76)} \\
    & \checkmark & \checkmark & & 11.58 & 9.14 & \textbf{7.34} & \cellcolor{gray!30}9.35 \textcolor[RGB]{0, 0, 255}{(+1.56)} & 23.27 & 21.41 & 19.49 & \cellcolor{gray!30}21.39 \textcolor[RGB]{0, 0, 255}{(+3.27)} \\
    & \checkmark &\checkmark &\checkmark & \textbf{12.27} & \textbf{9.24} & 7.15 & \cellcolor{gray!30}\textbf{9.55 \textcolor[RGB]{0, 0, 255}{(+1.76)}}	 & \textbf{23.62} & \textbf{21.62} & \textbf{19.63} & \cellcolor{gray!30}\textbf{21.62 \textcolor[RGB]{0, 0, 255}{(+3.50)}} \\
\bottomrule
\end{tabular}%
\end{adjustbox}
\end{table}

\begin{table}[htbp]
\scriptsize
\caption{\textbf{Ablation study of joint training.} All results in the table are obtained utilizing ego-state information. Traj, 2D and 3D denote ego trajectory, 2D labels and 3D occupancy labels, respectively.
}
\vspace{-2mm}
\setlength{\tabcolsep}{0.01\linewidth}
\label{table:joint}
\def\mystrut{\rule{0pt}{1.5\normalbaselineskip}}
\centering
\begin{adjustbox}{width=0.70\columnwidth,center}
\begin{tabular}{ccc|cccl|cccl}
\toprule
\multicolumn{3}{c|}{Supervision} &
\multicolumn{4}{c|}{L2 (m) $\downarrow$} & 
\multicolumn{4}{c}{Collision Rate (\%) $\downarrow$}  \\
Traj & 2D & 3D & 1s & 2s & 3s & \cellcolor{gray!30}Avg. & 1s & 2s & 3s & \cellcolor{gray!30}Avg.  \\
\midrule
\checkmark & & & \textbf{0.20} & 0.34 & 0.80 & \cellcolor{gray!30}0.45 & 0.50 & \textbf{0.62} & 0.90 & \cellcolor{gray!30}0.67 \\
\checkmark & & \checkmark & 0.22 & 0.31 & 0.41 &  \cellcolor{gray!30}\textbf{0.31} & 0.36 & 0.52 & 0.73 & \cellcolor{gray!30}0.54 \\
\checkmark & \checkmark & \checkmark & 0.22 & \textbf{0.30}  & \textbf{0.40}  &  \cellcolor{gray!30}\textbf{0.31} & \textbf{0.21} & 0.66 & \textbf{0.71} &  \cellcolor{gray!30}\textbf{0.53} \\
\bottomrule
\end{tabular}%
\end{adjustbox}
\end{table}

\paragraph{Model Components.}
We perform ablation studies on the effectiveness of various components in our approach for 4D occupancy forecasting, as shown in Table~\ref{table:components}. For comparison, we first present a Copy\&Paste baseline, which simply copys the current occupancy prediction results of our best 3D occupancy prediction model and calculates the mIoU between these results and the ground truth of the future frames. This serves as a lower bound for PreWorld, showcasing the performance of a model without any future forecasting capabilities. The results in row 1 and row 2 demonstrate that our proposed forecasting module has effectively equipped the model with future forecasting capabilities. By introducing this straightforward design, the model can produce non-trivial results and achieve significant performance enhancements, particularly evident in the IoU metric. Additionally, incorporating ego-state information and employing self-supervised pre-training further enhance both mIoU and IoU, as shown in row 3 and row 5. These findings underscore the importance and contribution of each component in our approach.

\paragraph{Joint Training.}
We further demonstrate the effectiveness of joint training. As shown in the row 4 and row 6 of Table~\ref{table:components}, when simultaneously optimizing both 4D occupancy forecasting and motion planning tasks, the forecasting capabilities of PreWorld are further enhanced. The introduction of trajectory supervision has improved model performance regardless of the utilization of self-supervised pre-training, with an increase from 8.85 and 9.35 to 9.06 and 9.55 in mIoU, respectively. Furthermore, joint training has also enhanced the planning capabilities of our model. As shown in Table~\ref{table:joint}, compared to the model supervised solely by ego trajectory, model supervised using both ego trajectory and 3D occupancy labels exhibits a significant improvement in both L2 error and collision rates, while the introduction of 2D labels further elevates the model performance. These results collectively demonstrate that jointly training 4D occupancy forecasting and motion planning tasks, as opposed to training them separately, provides additional performance benefits for the model.


\section{Conclusion}
\label{conclusion}

In this paper, we propose PreWorld, a semi-supervised vision-centric 3D occupancy world model for autonomous driving. We propose a novel two-stage training paradigm that allows our method to leverage abundant and easily accessible 2D labels for self-supervised pre-training. In the subsequent fine-tuning stage, the model is further optimized using 3D occupancy labels. Furthermore, we introduce a simple yet effective state-conditioned forecasting module, which addresses the challenge faced by existing methods in simultaneously optimizing the occupancy network and forecasting module. This module reduces information loss during training, while enabling the model to directly forecast future scenes and ego trajectory based on visual inputs. Through extensive experiments, we demonstrate the robustness of PreWorld across 3D occupancy prediction, 4D occupancy forecasting and motion planning tasks. Particularly, we validate the effectiveness and scalability of our training paradigm, outlining a viable path for scalable model training in autonomous driving scenarios.

\section*{Acknowledgments}

This project is supported by National Science and Technology Major Project (2022ZD0115502) and Lenovo Research.

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\clearpage
\appendix
\section{More Evaluations}
% You may include other additional sections here.

\subsection{3D Occupancy Prediction with RayIoU}
\label{sec:rayiou}

To address the inconsistent depth penalty issue within the mIoU metric, SparseOcc~\citep{liu2023fully} introduces a novel metric, RayIoU, designed to enhance the evaluation of 3D occupancy model performance. In order to demonstrate the robustness of our approach as a 3D occupancy model across various metrics, we opt to evaluate PreWorld on the 3D occupancy prediction task using RayIoU as metric, and compare the results with existing methods in this section.

% \begin{wraptable}{l}{0.5\linewidth}
\begin{table}[htbp]
\scriptsize
\caption{\textbf{3D occupancy prediction performance on the Occ3D-nuScenes dataset.} We use RayIoU as the evaluation metric~\citep{liu2023fully}. The best and second-best performances are represented by \textbf{bold} and \underline{underline} respectively.
}
\vspace{2mm}
% \setlength{\tabcolsep}{0.05\linewidth}
\label{table:rayiou}
% \def\mystrut{\rule{0pt}{0.5\normalbaselineskip}}
\centering
% \begin{adjustbox}{width=0.6\columnwidth,center}
\resizebox{0.65\linewidth}{!}{
\begin{tabular}{l|c|ccc}
\toprule
Method & \cellcolor{gray!30}RayIoU & \multicolumn{3}{c}{$\text{RayIoU}_{\text{1m, 2m, 4m}}$} \\
\midrule
    BEVFormer(4f) & \cellcolor{gray!30}32.4 & 26.1 & 32.9 & 38.0 \\
    RenderOcc & \cellcolor{gray!30}19.5 & 13.4 & 19.6 & 25.5 \\
    SimpleOcc & \cellcolor{gray!30}22.5 & 17.0 & 22.7 & 27.9 \\
    BEVDet-Occ (2f) & \cellcolor{gray!30}29.6 & 23.6 & 30.0 & 35.1 \\
    BEVDet-Occ-Long (8f) & \cellcolor{gray!30}32.6 & 26.6 & 33.1 & 38.2 \\
    OccFlowNet & \cellcolor{gray!30}32.6 & 25.6 & 33.3 & 38.8 \\
    FB-OCC (16f) &  \cellcolor{gray!30}33.5 & 26.7 & 34.1 & 39.7 \\
    SparseOcc (8f) & \cellcolor{gray!30}34.0 &  28.0 & 34.7 & 39.4 \\
    SparseOcc (16f) & \cellcolor{gray!30}36.1 & \underline{30.2} & 36.8 & 41.2 \\
\midrule
    \bf PreWorld (Ours) & \cellcolor{gray!30}\underline{36.4} & 30.0 & \underline{37.2} & \underline{41.9} \\
    \bf \ \ \ \ \ + Pre-training & \cellcolor{gray!30}\textbf{38.7} & \textbf{32.5} & \textbf{39.6} & \textbf{44.0} \\
\bottomrule
\end{tabular}%
}
% \end{wraptable}
\end{table}

As shown in Table~\ref{table:rayiou}, PreWorld achieves a RayIoU of 38.7, outperforming the previous SOTA method SparseOcc~\citep{liu2023fully} by 2.6 RayIoU. Comparing to purely 3D occupancy supervision, the proposed self-supervised pre-training stage provides a significant boost in RayIoU from 36.4 to 38.7, which reaffirms the effectiveness of our two-stage training paradigm for PreWorld. Altogether, Table~\ref{table:3d-occ} and~\ref{table:rayiou} showcases the strong performance of PreWorld across various metrics.

More importantly, the introduction of RayIoU explains the reason why our PreWorld does not outperform the baseline in certain categories. As shown in Table~\ref{table:3d-occ}, these situations are predominantly focused on large static categories. For instance, in categories like \textit{manmade} and \textit{sidewalk}, the performance of PreWorld is surpassed by RenderOcc~\citep{pan2024renderocc}. SparseOcc points out that common practice in mIoU computation involves the utilization of visible masks, which only accounts for voxels within the visible region, without penalizing predictions outside this area. Consequently, many models can achieve higher mIoU scores by predicting thicker surfaces for large static categories. As demonstrated in the last column of Figure~\ref{fig:vis-3docc}, RenderOcc, despite lacking an understanding of the overall scene structure, manages to attain higher scores in these categories through this strategy.

On the contrary, due to RayIoU considering the distance between voxels and the ego vehicle during computation, the model cannot gain an advantage by predicting thicker surfaces under this evaluation metric. Therefore, we believe that RayIoU is a more reasonable metric for comparing model performance in predicting large static categories. As shown in Table~\ref{table:rayiou}, when using RayIoU as the evaluation metric, the scores of both RenderOcc and OccFlowNet~\citep{boeder2024occflownet} have decreased. While OccFlowNet outperforms SparseOcc in the mIoU metric with 33.86 over 30.90, its performance notably lags behind SparseOcc in terms of RayIoU. These results indicate that the performance of our PreWorld is not inferior in some categories; rather, our model tends to generate more reasonable predictions, which can be reflected in the RayIoU metric.

Likewise, we can explain why pre-training leads to a decline in model performance in certain categories. It can be observed that these instances also primarily focus on large static categories. For example, after pre-training, there is a significant mIoU performance decline in the \textit{driveable surface} category. Based on the previous analysis, we showcase the RayIoU performance for the models on large static categories with and without pre-training.

\begin{table}[htbp]
\footnotesize
\caption{\textbf{Detailed 3D occupancy prediction performance of the large static categories on the Occ3D-nuScenes dataset.} We use RayIoU as the evaluation metric~\citep{liu2023fully}. GT represents the type of labels used during training. The best performances are represented by \textbf{bold}.
}
\vspace{-3mm}
\centering
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{c|lc|c|ccccccc}
    \toprule
    Metric & Method & GT & RayIoU
    & \rotatebox{90}{barrier}
    & \rotatebox{90}{Dri. Sur}
    & \rotatebox{90}{other flat}
    & \rotatebox{90}{sidewalk}
    & \rotatebox{90}{terrain}
    & \rotatebox{90}{manmade}
    & \rotatebox{90}{vegetation} \\
    \midrule
    \multirow{2}{*}{$\text{RayIoU}_{\text{1m}}$} & PreWorld & 3D & 30.0	 & 39.4	 & 56.4 & 	24.2	 & 25.5	 & 23.8 & 	32.9 & 	23.2 \\
    & + Pre-training & 2D+3D & \textbf{32.5}	 & \textbf{40.1}	 & \textbf{57.8}	 & \textbf{29.8} & 	\textbf{27.2} & 	\textbf{27.3}	 & \textbf{35.1}	 & \textbf{25.9} \\
    \midrule
    \multirow{2}{*}{$\text{RayIoU}_{\text{2m}}$} & PreWorld & 3D & 37.2	 & 44.6 & 	64.4	 & 29.2	 & 30.7	 & 31.3	 & 43.4 & 	36.0 \\
    & + Pre-training & 2D+3D & \textbf{39.6}	& \textbf{45.4}	& \textbf{65.9} & 	\textbf{34.3} & 	\textbf{32.7} & 	\textbf{34.6}	& \textbf{44.7}	& \textbf{37.8} \\
    \midrule
    \multirow{2}{*}{$\text{RayIoU}_{\text{4m}}$} & PreWorld & 3D & 41.9	 & 46.6	 & 72.4 & 	33.2 & 	35.6	 & 38.1	 & 49.7 & 	\textbf{47.3} \\
    & + Pre-training & 2D+3D & \textbf{44.0}	& \textbf{47.4}	& \textbf{74.3}	& \textbf{38.3}	& \textbf{37.7} & 	\textbf{41.1} & 	\textbf{50.5} & 	47.2 \\
    \bottomrule
\end{tabular}
}
\label{tab:rayiou_detailed}
\end{table}

As shown in Table~\ref{tab:rayiou_detailed}, the pre-trained model surpasses the model without pre-training on almost all large static categories across all thresholds. The results under the RayIoU metric indicate that pre-training steers the model towards predicting more plausible scene structures, rather than leading to performance decline. In conclusion, we believe that the results under RayIoU metric validate the effectiveness of pre-training and better showcase the robust prediction capabilities of our PreWorld.

% after pre-training, there is a significant mIoU performance decline in the \textit{driveable surface} category. I

\subsection{How Pre-training Works?}

Due to time constraints, when conducting experiments on smaller datasets in Table~\ref{table:pretraining-effectiveness}, models fine-tuned on 150 scenes and 450 scenes are trained for 24 and 18 epochs, respectively, while the model on the full dataset is trained for 12 epochs. Considering the ratio of data reduction to extended training time, we believe that we do not allocate sufficient additional training time for experiments on the smaller datasets. Therefore in this section, to delve into how pre-training benefits the model, we extend the training duration across various settings to obtain more comprehensive experimental results, as presented in Table~\ref{tab:longed_exp}.

\begin{table}[htbp]
\footnotesize
\caption{\textbf{The extended ablation study of different data scale utilized in pre-training and fine-tuning stage.} The best performances are represented by \textbf{bold}.
}
\vspace{-3mm}
\centering
\resizebox{0.75\linewidth}{!}{
\begin{tabular}{c|c|cccccc}
    \toprule
    \multirow{2}{*}{Fine-tuning} & \multirow{2}{*}{Pre-training} & \multicolumn{6}{c}{Epoch} \\
    & & 12 & 18 & 24 & 36 & 48 & 60 \\
    \midrule
    \multirow{2}{*}{150 Scenes} & $\times$	&11.18	&13.85	&18.66	&29.30&	\textbf{30.26}	&30.00 \\
    & 700 Scenes & 13.01	&21.83	&25.02&	\textbf{31.65}	&31.56	&30.98 \\
    \midrule
    \multirow{2}{*}{450 Scenes} & $\times$	& 25.54	&31.99	&32.89	&\textbf{33.32} & - & -\\
    & 700 Scenes & 29.52	&33.37	&\textbf{34.19}	&34.08 &- &- \\
    \midrule
    \multirow{3}{*}{700 Scenes} & $\times$	& 33.95	& \textbf{33.99} & - & - & - &- \\
     & 450 Scenes & \textbf{34.28}&	34.15& - & - & - &- \\
     & 700 Scenes & 34.69&	\textbf{34.89} & - & - & - &- \\
    \bottomrule
\end{tabular}
}
\label{tab:longed_exp}
\end{table}

As shown in the results, we believe that pre-training has benefited the model in two key aspects: on one hand, pre-training accelerates the convergence of the model; on the other hand, pre-training continues to enhance the model performance after convergence, thereby improving the data efficiency. Taking models fine-tuned on 150 scenes as an example, it can be observed that during the first 24 epochs, employing pre-training accelerates the convergence. Subsequently, both models have converged, with the pre-trained model still maintaining an advantage in prediction performance.

Furthermore, it can be observed that pre-training leads to a 0.87 mIoU improvement for the model fine-tuned on 450 scenes, while it results in a 0.90 mIoU improvement for the model fine-tuned on 700 scenes. We believe this situation is still related to the reasons analyzed in Section~\ref{sec:rayiou}, that is, for large static categories, existing evaluation metric does not adequately reflect the actual performance of the model. Therefore, we have detailed the corresponding mIoU for large static categories and small objects in Table~\ref{tab:detail-long}.

\begin{table}[htbp]
\footnotesize
\caption{Detailed 3D occupancy prediction performance of different data scale utilized in pre-training and fine-tuning stage.
}
\vspace{-3mm}
\centering
\resizebox{0.75\linewidth}{!}{
\begin{tabular}{c|c|ccc}
    \toprule
    \multirow{2}{*}{Fine-tuning} & \multirow{2}{*}{Pre-training} & \multicolumn{3}{c}{mIoU} \\
    & & Overall & Large Static & Small \\
    \midrule
    \multirow{2}{*}{150 Scenes} & $\times$	&30.26~~~~~~~~~~~~~	& 35.82~~~~~~~~~~~~~	& 24.08~~~~~~~~~~~~~ \\
    & 700 Scenes & 31.65 \textcolor[RGB]{0, 0, 255}{(+1.39)}	& 37.18 \textcolor[RGB]{0, 0, 255}{(+1.36)}	& 25.50 \textcolor[RGB]{0, 0, 255}{(+1.42)} \\
    \midrule
    \multirow{2}{*}{450 Scenes} & $\times$	&33.32~~~~~~~~~~~~~	& 38.40~~~~~~~~~~~~~	& 26.29~~~~~~~~~~~~~ \\
    & 700 Scenes & 34.19 \textcolor[RGB]{0, 0, 255}{(+0.87)}&	39.04 \textcolor[RGB]{0, 0, 255}{(+0.64)}&	27.44 \textcolor[RGB]{0, 0, 255}{(+1.15)} \\
    \midrule
    \multirow{2}{*}{700 Scenes} & $\times$	&33.99~~~~~~~~~~~~~	& 39.83~~~~~~~~~~~~~	& 26.98~~~~~~~~~~~~~ \\
    & 700 Scenes & 34.89 \textcolor[RGB]{0, 0, 255}{(+0.90)}&	40.72 \textcolor[RGB]{0, 0, 255}{(+0.89)}&	27.96 \textcolor[RGB]{0, 0, 255}{(+0.98)} \\
    \bottomrule
\end{tabular}
}
\label{tab:detail-long}
\end{table}

It can be observed that the mIoU for large categories does not always effectively reflect the performance improvement of the model. For the model fine-tuned with 450 scenes, pre-training leads to a 0.64 increase in mIoU for large categories, while the model fine-tuned with 700 scenes sees an increase of 0.89. In contrast, the increase in mIoU for small objects can better reflect the effectiveness of pre-training, aligning with the expectations: 2D pre-training yields more significant performance improvements for smaller 3D fine-tuning datasets. In order to better showcase the effectiveness of pre-training, we use RayIoU as the evaluation metric, and the results obtained are as follows:

\begin{table}[htbp]
\footnotesize
\caption{\textbf{Detailed 3D occupancy prediction performance of different data scale utilized in pre-training and fine-tuning stage.} We use RayIoU as the evaluation metric~\citep{liu2023fully}.
}
\vspace{-3mm}
\centering
\resizebox{0.75\linewidth}{!}{
\begin{tabular}{c|c|c|cccc}
    \toprule
    \multirow{2}{*}{Fine-tuning} & \multirow{2}{*}{Pre-training} & \multirow{2}{*}{RayIoU} & \multicolumn{4}{c}{Large Static RayIoU} \\
    & & & Overall & 1m & 2m & 4m \\
    \midrule
    \multirow{2}{*}{150 Scenes} & $\times$	& 29.5~~~~~~~~~~~		& 32.5~~~~~~~~~~~	& 	26.2	& 	32.9		& 38.5 \\
    & 700 Scenes & 33.4 \textcolor[RGB]{0, 0, 255}{(+3.9)}	& 36.6 \textcolor[RGB]{0, 0, 255}{(+4.1)} & 	30.2& 	37.0	& 42.5 \\
    \midrule
    \multirow{2}{*}{450 Scenes} & $\times$	& 35.1~~~~~~~~~~~	& 37.9~~~~~~~~~~~	& 31.5	& 38.3	& 44.0 \\
    & 700 Scenes & 37.8 \textcolor[RGB]{0, 0, 255}{(+2.7)}	&40.2 \textcolor[RGB]{0, 0, 255}{(+2.3)}	&33.1	&40.6	&46.8 \\
    \midrule
    \multirow{2}{*}{700 Scenes} & $\times$	& 36.8~~~~~~~~~~~	& 	39.5~~~~~~~~~~~	& 	32.4	& 	39.9		& 46.2 \\
    & 700 Scenes & 39.0 \textcolor[RGB]{0, 0, 255}{(+2.2)} & 	41.4 \textcolor[RGB]{0, 0, 255}{(+1.9)}	 & 34.7	 & 41.9 & 	47.6 \\
    \bottomrule
\end{tabular}
}
\label{tab:detail-long-rayiou}
\end{table}

As shown in Table~\ref{tab:detail-long-rayiou}, when using RayIoU as the evaluation metric, the improvements in overall RayIoU and RayIoU for large categories follow a similar trend, indicating that as the scale of the 3D fine-tuning dataset increases, the benefits of 2D pre-training do indeed gradually diminish.

\subsection{Self-Supervised 4D Occupancy Forecasting and Motion Planning}

Instead of generating occupancy predictions through the occupancy head $\mathcal{H}$, we support an alternative approach by utilizing the attribute projection head $\mathcal{P}$. Specially, by setting a threshold value $\tau$ for the 3D volume density field $\tilde{\sigma}$ of the scene, we can determinate whether a voxel is occupied. Subsequently, the semantic occupancy of the voxel $v_k$ can be formulated as:
\begin{equation}
    \hat{Y}(v_k) = \text{argmax}(\tilde{s}(v_k)), \ \ \text{if } \tilde{\sigma}(v_k) \ge \tau,
\end{equation}
where $\tilde{s}$ denotes the semantic field of the scene, and we regard $v_k$ as non-occupied if $\tilde{\sigma}(v_k) < \tau$.

In this manner, we can also obtain occupancy predictions during the pre-training stage. In other words, PreWorld is capable of engaging in self-supervised tasks as well. Therefore, to validate its performance as a self-supervised 3D occupancy world model, we compare it against state-of-the-art self-supervised methods on both 4D occupancy forecasting and motion planning tasks on the Occ3D-nuScenes dataset~\citep{tian2024occ3d}, denoting as \textbf{PreWorld-S}.

\paragraph{4D Occupancy Forecasting.} Table~\ref{table:self-4d-occ} presents the 4D occupancy forecasting performance of PreWorld-S compared to previous self-supervised approach of OccWorld~\citep{zheng2023occworld}. In comparison to OccWorld-S, our approach yields significant outcomes. The IoU over the future 3-second interval nearly doubles, while the average future mIoU demonstrates an remarkable increase of over 1300\%, soaring from 0.26 to 3.78. These results highlight the superiority of our method in self-supervised learning and open up more possibilities for future research on the architecture of self-supervised 3D occupancy world models.

\begin{table}[htbp]
\scriptsize
\caption{\textbf{Self-supervised 4D occupancy forecasting performance on the Occ3D-nuScenes dataset.} We take the self-supervised vision-centric approach of OccWorld~\citep{zheng2023occworld} as baseline for fair comparison. Aux. Sup. represents auxiliary supervision apart from the ego trajectory. Avg. reprersents the average performance of that in 1s, 2s, and 3s. The best and second-best performances are represented by \textbf{bold} and \underline{underline} respectively.
}
\vspace{2mm}
% \setlength{\tabcolsep}{0.008\linewidth}
\label{table:self-4d-occ}
% \def\mystrut{\rule{0pt}{1.5\normalbaselineskip}}
\centering
% \begin{adjustbox}{width=0.99\columnwidth,center}
\resizebox{0.99\linewidth}{!}{
\begin{tabular}{l|c|cccc|cccc}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{Aux. Sup.} &
\multicolumn{4}{c|}{mIoU (\%) $\uparrow$} & 
\multicolumn{4}{c}{IoU (\%) $\uparrow$}  \\
& & 1s & 2s & 3s & \cellcolor{gray!30}Avg. & 1s & 2s & 3s & \cellcolor{gray!30}Avg.  \\
\midrule
    OccWorld-S &  None & 0.28 & 0.26 & 0.24 & \cellcolor{gray!30}0.26 & 5.05 & 5.01 & 4.95 & \cellcolor{gray!30}5.00 \\
    \bf PreWorld-S (Ours) & 2D Labels & \textbf{4.36} & \textbf{3.72} & \textbf{3.27} & \cellcolor{gray!30}\textbf{3.78} & \textbf{9.49} & \textbf{9.17} & \textbf{8.90} & \cellcolor{gray!30}\textbf{9.19} \\
\bottomrule
\end{tabular}%
}
% \end{adjustbox}
\end{table}

\begin{table}[htbp]
\scriptsize
\caption{\textbf{Self-supervised motion planning performance on the Occ3D-nuScenes dataset.} We take the self-supervised vision-centric approach of OccWorld~\citep{zheng2023occworld} as baseline for fair comparison. $\dagger$ represents training and inference with ego state information introduced. The bestperformances are represented by \textbf{bold}.
}
\vspace{2mm}
% \setlength{\tabcolsep}{0.005\linewidth}
\label{table:self-planning}
% \def\mystrut{\rule{0pt}{1.5\normalbaselineskip}}
\centering
% \begin{adjustbox}{width=0.99\columnwidth,center}
\resizebox{0.99\linewidth}{!}{
\begin{tabular}{l|c|cccc|cccc}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{Aux. Sup.} &
\multicolumn{4}{c|}{L2(m) (\%) $\downarrow$} & 
\multicolumn{4}{c}{Collision Rate (\%) $\downarrow$}  \\
&& 1s & 2s & 3s & \cellcolor{gray!30}Avg. & 1s & 2s & 3s & \cellcolor{gray!30}Avg.  \\
\midrule
    OccWorld-S$^{\dagger}$ &  None & 0.67 & 1.69 & 3.13 &  \cellcolor{gray!30}1.83 & \textbf{0.19} & 1.28 & 4.59 & \cellcolor{gray!30}2.02 \\
    \midrule
    \bf PreWorld-S (Ours) & 2D Labels & \underline{0.66} & \underline{1.49} & \underline{2.60} & \cellcolor{gray!30}\underline{1.58} & \underline{0.57} & \underline{1.26} & \underline{2.23} & \cellcolor{gray!30}\underline{1.35} \\
    \color{gray}\bf PreWorld-S (Ours)$^{\dagger}$ & \color{gray}2D Labels & \color{gray}\textbf{0.20} & \color{gray}\textbf{0.61} & \color{gray}\textbf{1.57} & \color{gray}\cellcolor{gray!30}\textbf{0.79} & \color{gray}\underline{0.57} & \color{gray}\textbf{0.64} & \color{gray}\textbf{1.42} & \color{gray}\cellcolor{gray!30}\textbf{0.88} \\
\bottomrule
\end{tabular}%
}
% \end{adjustbox}
\end{table}

\paragraph{Motion Planning.} As illustrated in Table~\ref{table:self-planning}, PreWorld-S significantly surpasses OccWorld-S on both metrics even without the incorporation of ego-state information. When ego-state information is introduced (indicated in \textcolor{gray}{gray}), the performance of our self-supervised approach has received a notable enhancement, yielding results comparable to or outperforming those fully-supervised methods such as OccWorld-D. These findings once again demonstrate the effectiveness of our approach.


\section{More Visualizations}

We provide additional visualized comparison in this section.

Fig~\ref{fig:vis-3docc-more} shows more qualitative results of 3D occupancy prediction task compared with the latest fully-supervised method SparseOcc~\citep{liu2023fully} and self-supervised method RenderOcc~\citep{pan2024renderocc}, further substantiating the robustness of our PreWorld model and the effectiveness of our novel two-stage training paradigm. The \textcolor[RGB]{255,0,0}{red boxes} highlight fine-grained details of the 3D occupancy predictions and the ground truth, while the \textcolor[RGB]{255, 192, 0}{orange boxes} mark holistic structure of an area within the scene. Compared to prior approaches, PreWorld demonstrates superior performance in preserving the structural information of the scene and capturing fine-grained details. In contrast, RenderOcc struggles with comprehending the scene structure accurately and exhibits inaccurate predictions for unsupervised occluded regions. SparseOcc, on the other hand, fails to effectively predict small objects like \textit{poles} and long-tailed objects like \textit{construction vehicles}, resulting in detail loss. These findings are consistent with the observations in the main text.

In Fig~\ref{fig:vis-occluded}, we further provide a detailed showcase of the prediction results for both visible and occluded regions. Consistent with the quantitative analysis in Section~\ref{sec:rayiou}, it can be observed that RenderOcc~\citep{pan2024renderocc} tends to predict thicker surfaces for large static categories. However, while this approach may lead to higher mIoU scores, its predictions for occluded regions are chaotic, indicating a lack of true understanding of the scene structure. On the contrary, our PreWorld makes more cautious predictions for occluded regions, demonstrating a more comprehensive understanding of the holistic scene structure.


\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\textwidth]{figures/visualization-exp-3docc-more.pdf}
  \caption{
  \textbf{More qualitative results of 3D occupancy prediction on the Occ3D-nuScenes validation set.} The \textbf{\textcolor[RGB]{255,192,0}{holistic structure}} and \textbf{\textcolor[RGB]{255, 0, 0}{fine-grained details}} of the scene are highlighted by \textcolor[RGB]{255, 192, 0}{orange boxes} and \textcolor[RGB]{255,0,0}{red boxes} respectively.
  }
  \label{fig:vis-3docc-more}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\textwidth]{figures/occluded.pdf}
  \caption{
  \textbf{More qualitative results of 3D occupancy prediction on the Occ3D-nuScenes validation set.} The shaded area represents occluded regions where the voxels are not included in the evaluation. In contrast to RenderOcc, our PreWorld makes more cautious predictions for occluded regions, tending to preserve the overall structure of the scene. 
  }
  \label{fig:vis-occluded}
\end{figure}


\end{document}
