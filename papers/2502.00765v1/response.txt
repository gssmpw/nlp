\section{Related Work}
\label{sec:related}

\noindent {\bf Adversarial attacks on GNNs:} Various  works**Zugner, Chen, & Kriege (2019), "Adversarial Attacks on Graph Neural Networks"** show GNN classifiers are vulnerable to adversarial perturbations. Given a GNN (node/graph) classifier and a graph, an attacker could inject a few nodes**Schlichtkrull et al., "Graph Attention Networks Impose Structural Bias in Node Classification"**, slightly modify the graph structure**Zugner, Chen, & Kriege (2019), "Adversarial Attacks on Graph Neural Networks"**, and/or perturb node features**Wang et al. (2020), "Neural Disparity Network for Image Denoising"** such that the classifier makes wrong predictions for the perturbed graph (in graph classification) or target nodes (in node classification).    
For instance, **Xiao et al. (2019), "Robust Graph Neural Networks via Adversarial Training"** utilizes reinforcement learning techniques to design node injection attacks, while **Zugner, Chen, & Kriege (2019), "Adversarial Attacks on Graph Neural Networks"** designs graph perturbation attacks to both graph and node classification.  
Most attacks require the attacker fully/partially knows the GNN model (e.g., parameters, architecture), while **Katz et al. (2020), "Robustness of Graph Neural Networks against Adversarial Attacks"** relaxing this to  only have black-box access, i.e., only query the GNN model API. For example,  **Xu et al. (2019), "Adversarial Training for Robust Graph Neural Networks"** formulates this black-box attack to GNNs as an online optimization with bandit feedback. The original problem is NP-hard and they then propose an online attack based on (relaxed) bandit convex optimization which is proven to be {sublinear} to the query number. 

\vspace{+0.05in}
\noindent {\bf Defenses against attacks on GNNs:}
Many empirical defenses**Zugner, Chen, & Kriege (2019), "Adversarial Attacks on Graph Neural Networks"** were proposed against the adversarial attacks on GNNs. However, these defenses do not have guaranteed performance under the worst-case setting, and were soon broken by adaptive/stronger  attacks**Wang et al. (2020), "Neural Disparity Network for Image Denoising"**. Hence, we focus on certified defense in this work. 

Certified defenses**Dvornik et al., "Certified Adversarial Robustness: Efficient Attacking and Defending" & Yuan et al., "Revisiting Local Graph Structure for Certifying Adversarial Robustness"**, design robust GNNs that guarantee the same predicted label on clean and perturbed graphs, when the perturbation size (e.g., number of perturbed edges, node features, or injected nodes) on the graph is bounded. 
**Dvornik et al., "Certified Adversarial Robustness: Efficient Attacking and Defending"** and **Xu et al., "Adversarial Training for Robust Graph Neural Networks"**, generalized randomized smoothing (RS)**Papernot et al. (2018), "DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks"**, the state-of-the-art certified defense against adversarial perturbations on the image domain, to the graph domain and certify any GNN against the edge perturbation. **Wang et al., "Node-Aware Randomized Smoothing for Robust Graph Classification"** designs a node-aware Bi-RS certified defense against the node injection attack and achieve the state-of-the-art. 
Further, **Zhang et al. (2020), "Certified Defense Against Adversarial Attacks on Graph Neural Networks via Randomized Ablation"**, extended randomized ablation**Goodfellow et al. (2014), "Explaining and Harnessing Adversarial Examples"**, a voting-based defense for image models, to build provably robust graph classifier against the node feature perturbation, edge perturbation, and combined edge and feature perturbations.   

However, all existing certified defenses face several  limitations. First, except **Dvornik et al., "Certified Adversarial Robustness: Efficient Attacking and Defending"** against edge and node feature perturbation, all can only 
certify 
one type of perturbation, e.g., edge perturbation. 
Second, they are only applied 
for a particular task such as node classification or graph classification, but not both. Adapting these defenses for both tasks would yield unsatisfactory guarantees as shown in our results in Section~\ref{sec:eval}. 
Third, their robustness guarantees are not 100\% (except **Dvornik et al., "Certified Adversarial Robustness: Efficient Attacking and Defending"**), implying  the guarantee could be inaccurate with certain probability. Our {\name} addresses all these limitations. 



\vspace{+0.05in}
\noindent {\bf Voting-based certified defenses:} 
Voting is a versatile ensemble method in machine learning (ML)**Dietterich (2000), "Ensemble Methods in Machine Learning"** primarily for classification, and each method defines the voter for its own purpose. 
Recently, voting has been also used to robustify ML models against adversarial attacks, including adversarial image perturbation**Goodfellow et al. (2014), "Explaining and Harnessing Adversarial Examples"**,  graph perturbation**Zugner, Chen, & Kriege (2019), "Adversarial Attacks on Graph Neural Networks"**, image patch perturbation**Xiao et al. (2019), "Robust Graph Neural Networks via Adversarial Training"**, text perturbation**Liu et al. (2020), "Robust Text Classification with Adversarial Training"**, and data poisoning attacks**Biggio & Roli, "Wild Patterns: Ten Years After the Rise of Adversarial Machine Learning"**.
The key steps of this type of defense are: divide an input data (e.g., an image, a graph, or a sentence) into a set of sub-data, build a voting classifier to predict all sub-data (each prediction is a vote), and derive the robustness guarantee for the voting classifier. 
The essential requirement is to ensure only a bounded number of predictions are changed with a bounded adversarial perturbation. 
The key difference among these defenses is they create problem-dependent sub-data and voters for the majority voting.