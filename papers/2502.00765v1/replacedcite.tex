\section{Related Work}
\label{sec:related}

\noindent {\bf Adversarial attacks on GNNs:} Various  works____ show GNN classifiers are vulnerable to adversarial perturbations. Given a GNN (node/graph) classifier and a graph, an attacker could inject a few nodes____, slightly modify the graph structure____, and/or perturb node features____ such that the classifier makes wrong predictions for the perturbed graph (in graph classification) or target nodes (in node classification).    
For instance, ____ utilizes reinforcement learning techniques to design node injection attacks, while ____ designs graph perturbation attacks to both graph and node classification.  
Most attacks require the attacker fully/partially knows the GNN model (e.g., parameters, architecture), while ____ relaxing this to  only have black-box access, i.e., only query the GNN model API. For example,  ____ formulates this black-box attack to GNNs as an online optimization with bandit feedback. The original problem is NP-hard and they then propose an online attack based on (relaxed) bandit convex optimization which is proven to be {sublinear} to the query number. 

\vspace{+0.05in}
\noindent {\bf Defenses against attacks on GNNs:}
Many empirical defenses____ were proposed against the adversarial attacks on GNNs. However, these defenses do not have guaranteed performance under the worst-case setting, and were soon broken by adaptive/stronger  attacks____. Hence, we focus on certified defense in this work. 

Certified defenses____ design robust GNNs that guarantee the same predicted label on clean and perturbed graphs, when the perturbation size (e.g., number of perturbed edges, node features, or injected nodes) on the graph is bounded. 
____ and____ generalized randomized smoothing (RS)____, the state-of-the-art certified defense against adversarial perturbations on the image domain, to the graph domain and certify any GNN against the edge perturbation. ____ designs a node-aware Bi-RS certified defense against the node injection attack and achieve the state-of-the-art. 
Further, ____ extended randomized ablation____, a voting-based defense for image models, to build provably robust graph classifier against the node feature perturbation, edge perturbation, and combined edge and feature perturbations.   

However, all existing certified defenses face several  limitations. First, except ____ against edge and node feature perturbation, all can only 
certify 
one type of perturbation, e.g., edge perturbation. 
Second, they are only applied 
for a particular task such as node classification or graph classification, but not both. Adapting these defenses for both tasks would yield unsatisfactory guarantees as shown in our results in Section~\ref{sec:eval}. 
Third, their robustness guarantees are not 100\% (except ____), implying  the guarantee could be inaccurate with certain probability. Our {\name} addresses all these limitations. 



\vspace{+0.05in}
\noindent {\bf Voting-based certified defenses:} 
Voting is a versatile ensemble method in machine learning (ML)____ primarily for classification, and each method defines the voter for its own purpose. 
Recently, voting has been also used to robustify ML models against adversarial attacks, including adversarial image perturbation____,  graph perturbation____, image patch perturbation____, text perturbation____, and data poisoning attacks____.
The key steps of this type of defense are: divide an input data (e.g., an image, a graph, or a sentence) into a set of sub-data, build a voting classifier to predict all sub-data (each prediction is a vote), and derive the robustness guarantee for the voting classifier. 
The essential requirement is to ensure only a bounded number of predictions are changed with a bounded adversarial perturbation. 
The key difference among these defenses is they create problem-dependent sub-data and voters for the majority voting.