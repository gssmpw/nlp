\section{Evaluations on Real-World Graph Datasets}
\label{sec:realworld}

In this section, we will evaluate {\name} on two real-world  graph datasets, i.e., Amazon2M co-purchasing dataset \cite{clusterGCN} for node classification and Big-Vul code vulnerability dataset \cite{big_vul} for graph classification.  

\subsection{Experimental Settings}

Amazon2M dataset is a network representation of products from Amazon, where nodes signify products, and edges indicate two products are frequently purchased together. 
This dataset consists of 2,449,029 nodes and 61,859,140 edges and is used for node classification --- each node has 100 features and is labeled as one of 47 products and the task is to classify products. 
We divide nodes into 30\% for training, 20\% for validation, and 50\% for testing. 

Big-Vul is widely-used code vulnerability dataset, which comprises extensive source code vulnerabilities extracted from 348 open-source C/C++ GitHub projects, spanning from 2002 to 2019. It contains 188,636 C/C++ functions, 
including 10,900 vulnerable ones (covering 91 vulnerability types), 
and 7,203 benign ones. Following the recent work \cite{chu2024graph}, 
 we built code graphs by taking code statements as nodes, control-flow or data-flow
dependencies as edges and utilizing GraphCodeBERTâ€™s\cite{guo2020graphcodebert} token embedding layer to initialize node features.  Afterwards, we labeled these code graphs as vulnerable or benign,   
and formed vulnerability detection problem as a binary graph classification task \cite{chu2024graph}. We divide the graphs into 80\% for training, 10\% for validation, and 10\% for testing.

 We use GCN as the base GNN in {\name} (MD5 as the hash function) to train Big-Vul, and cluster-GCN  \cite{clusterGCN} (a more computation- and memory- efficient variant of GCN) as a base GNN in {\name} to train the large-scale Amazon2M.  
 

\subsection{Experimental Results}

{\bf Runtime and accuracy:} Table \ref{tab:real-graph} shows the  training and test time, and test accuracy of {\name} and the base GNN on Amazon2M ($T=80$) and Big-Vul ($T=30$).   
We observe that: 1) Test accuracies of {\name} and base GNN are close, indicating {\name} maintains the utility in these real-world graphs; 2) {\name} is about $T$ times slower than the base GNN, again consistent with the Big-O analysis in Table \ref{computation-cost-training-testing}. 


\noindent {\bf Certified accuracy:} Figure~\ref{fig:realworld} reports the certified accuracies of {\name} on the two datasets. The results validate that {\name} is also an effective defense for safeguarding real-world GNN applications against graph perturbations. 
For instance, {\nameN} can tolerate up to 50 edges and nodes on Amazon2M with arbitrary perturbations; and {\name}-E can defend against 24 arbitrarily perturbed edges on Big-Vul. 



\begin{table}[!t]
\caption{Runtime and test accuracy of {\name} and the base undefended GNN on Amazon2M (T=80) and Big-Vul (T=30). As {\nameE} and  {\nameN} have close runtime and test accuracy, we simplicity use {\name} for brevity.}
    \centering
    \footnotesize
    \addtolength{\tabcolsep}{-3pt}
    \begin{tabular}{|c|c|c|c|c|}
    %\toprule
    \hline
         {\bf Dataset}&{\bf Method}&{\bf Train time/epoch} &{\bf Test time} & {\bf Test acc.}  \\
         \hline
         %\midrule
         \multirow{2}{*}{\bf Amazon2M}& {\bf Cluster-GCN}&3.2s&1.1s&0.72\\
         %\cline{2-5}
         &{\name}&287s&107s&0.68\\ \hline
         \multirow{2}{*}{\bf Big-Vul}& {\bf GCN}&27.8s&2.3s&0.70\\
         %\cline{2-5}
         &{\name}&827s&65s&0.69\\ \hline
         %\bottomrule
    \end{tabular}
    \label{tab:real-graph}
   % \vspace{-4mm}
\end{table}






