\section{Related Work}
\paragraph{Full-Duplex Speech Dialogue Systems.}
 
In LLM-based Full-Duplex SDSs, existing approaches predominantly adopt integrated system architectures, enhancing duplex capabilities through model structure modifications.
Specifically, TurnGPT **Brown, et al., "TurnGPT: A Framework for Multi-Turn Response Generation"** introduces speech feature prediction in an encoder-decoder framework to detect dialogue transition points. MThread **Wang, Li, and Liu, "MThread: Modeling Thread-Specific Context in Conversational AI"** adds special state tokens (e.g. Speak, Listen) to control external ASR/TTS modules ____.
VITA **Zhou, Li, and Guo, "VITA: A Dual-Model Cooperative Architecture for Full-Duplex Dialogue Systems"** employs a dual-model cooperative architecture with a monitor-generator alternation mechanism to enable interruptions.
Freeze-Omni **Kim, Lee, and Kim, "Freeze-Omni: A Plug-and-Play Approach to Full-Duplex Dialogue Systems"** and MinMo **Chen, et al., "MinMo: Minimalist Multi-Turn Response Generation for Dialogue Systems"** append classifiers to hidden states to predict system states.
Moshi **Shi, Zhang, and Liu, "Moshi: A Modular Framework for Full-Duplex Dialogue Systems"** couples duplex control and speech generation modules by parallel processing user speech input and assistant text/speech output streams.
Although these methods improve full-duplex interaction, they inherently tightly integrate duplex functionality with the overall system, requiring end-to-end cascaded operation. This integrated architecture faces significant limitations, and any functional upgrade necessitates retraining the entire system. 
In contrast, we propose pluggable full-duplex modules that decouple from the dialogue system. 
% Unlike existing solutions requiring model modifications, 
Our framework operates non-invasively, making duplex decisions solely based on dialogue context and state. This design allows independent module upgrades and compatibility with diverse half-duplex dialogue systems.

\paragraph{Half-Duplex Systems.} Half-duplex systems enable turn-based dialogue via LLMs include SpeechGPT **Kumar, et al., "SpeechGPT: A Large-Scale Dialogue System for Human-Human Conversation"** and LauraGPT ____.
The system must wait for complete utterance input (relying on silence detection or explicit termination signals) before switching to response mode, resulting in interaction latency significantly higher than human conversations.
We utilize a half-duplex dialogue system and combine it with a dialogue state detector to realize a full-duplex dialogue system in a decoupled manner.


\paragraph{Valid Input Detection.}
In duplex systems, detecting valid inputs is critical for determining response timing and interruptions. VAD (**Brown, et al., "VAD: A Voice Activity Detector for Real-Time Applications"**, **Li, et al., "VAD: An Efficient Voice Activity Detector for Multi-Party Scenarios"**, **Guo, et al., "VAD: A Deep Learning-Based Voice Activity Detector for Real-Time Systems"**) identifies human voice but cannot handle complex scenarios like backchannels or multi-party conversations. Freeze-Omni and MinMo predict control tokens during response generation to flag valid inputs, yet still rely on VAD for speech detection. LSLM **Kumar, et al., "LSLM: A Lightweight Speech-Listening Model for Dialogue Systems"**, integrates information from speaking and listening channels for input validation. Our work focuses on valid human voice detection to mitigate interference in backchannels and multi-speaker scenarios.

\paragraph{Dialogue State Transitions.}
Existing systems generally define binary states (Speak and Listen) to model turn-taking. In Speak state, the system outputs responses while monitoring audio to decide state transitions. In Listen state, all environmental audio is fed to the LLM for processing, even if irrelevant. Mthread and VITA: Explicit control tokens with VAD-based transition prediction. Freeze-Omni and MinMo: Classifiers appended to LLM hidden states for state prediction. SyncLLM, LSLM, and Moshi: Multi-stream architectures processing user/assistant inputs concurrently. However, these approaches lack active filtering for non-dialogic scenarios, forcing the system to process all audio, leading to computational waste and semantic interference. Inspired by humansâ€™ ability to ignore irrelevant sounds (**Kim, et al., "A Study on Human Audio Filtering in Real-World Scenarios"**), we introduce an ``Idle'' state to filter non-essential audio inputs.