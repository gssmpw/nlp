\section{Related Work}
\label{related}
\noindent
\subsection{CNN-based approaches}
Fukushima \textit{et al.} proposed the neocognitron motivated by the human visual system, which many consider as the precursor to convolutional neural networks.
LeCun \textit{et al.}, "Gradient-Based Learning Applied to Document Recognition"
Alex \textit{et al.}, "ImageNet Classification with Deep Convolutional Neural Networks"

CNNs can extract abundant hierarchical information and have been popularly applied in different fields, including semantic segmentation, action recognition, and object detection, achieving significant results.

Liu \textit{et al.}, "A Fast Multi-Scale Image Fusion Algorithm Based on Convolutional Neural Network"
Liu \textit{et al.}, "Infrared and Visible Images Fusion Using Deep Learning-Based Technique"

The formulation of fusion strategies is learned autonomously by the network, providing a high level of autonomy and adaptability.
CNN, with its excellent capability for local feature extraction, exhibits strong performance in image fusion.
However, as the receptive field increases, the fusion of high-level semantic features not only leads to the loss of features at different scales but also results in the loss of long-range dependencies.

\subsection{Transformer-based approaches}
Vaswani \textit{et al.}, "Attention Is All You Need"
Dosovitskiy \textit{et al.}, "An Image is Not the Sum of its Parts: A Deep Visual-Lock Model for Visual Object Tracking"

ViT divides the input image into a succession of image patches, converts them into sequential data, and then processes them by the Transformer.
When sufficient data is available for pretraining, ViT has achieved performance surpassing traditional CNN models, bringing new ideas and methods to computer vision.

Inspired by VIT, Liu \textit{et al.}, "Swin Transformer: Hierarchical Vision Transformers using Shuffled Windows"

The advantage of extracting long-range dependencies has led to the application of Transformer to image fusion in recent years.
In order to address the limitation of traditional CNN in capturing long-range dependencies, Vibashan \textit{et al.}, "Image Fusion Transformer (IFT) for Multispectral Image Fusion"

This model introduces Transformer modules in the fusion layer, which enhances the fusion of long-range information while preserving local features.

Existing Transformer-based methods mainly explore intra-domain interactions.
To address this limitation, Ma \textit{et al.}, "Cross-Modal Transformers for Image and Video Understanding"

\subsection{Integration of multiscale local and global features.}
Local features capture the characteristics of key pixels or image regions, aiding in capturing specific objects and details within an image. Global features reflect the overall properties of an image, helping to capture the holistic structure of the image. Combining local and global features in computer vision allows for a more comprehensive description of image features, reducing noise interference and enhancing the richness and robustness of features.

In recent years, there has been a proliferation of work combining local features with global features.
Li \textit{et al.}, "Image Fusion Model Based on Alternating Convolutional Neural Networks and Transformers"

However, the model solely employs two branches to extract local and global features from the infrared and visible light images separately, without considering the interaction between features across different images during the extraction process.
Huo \textit{et al.}, "Three-Branch Hierarchical Multiscale Feature Fusion Model for Medical Image Classification"
Yang \textit{et al.}, "Multiscale Dual Attention (MDA) model for Infrared and Visible Image Fusion"