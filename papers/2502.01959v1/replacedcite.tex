\section{Related Work}
\label{related}
\noindent
\subsection{CNN-based approaches}
Fukushima textit{et al.} proposed the neocognitron motivated by the human visual system, which many consider as the precursor to convolutional neural networks.
LeCun \textit{et al.} ____ designed LeNet-5 for handwritten digit recognition, which was the earliest framework of a convolutional neural network.
However, due to limited computing power, there was no substantial progress in the subsequent decade.
Alex \textit{et al.} ____ proposed AlexNet, which demonstrated significant advantages in image classification tasks, marking the resurgence of neural networks, setting the prototype for today's CNN structures.
CNNs can extract abundant hierarchical information and have been popularly applied in different fields, including semantic segmentation, action recognition, and object detection, achieving significant results.

Liu \textit{et al.} ____ foremost applied CNN in image fusion. Their method involved learning weight maps using CNN and fusing the original pixels based on image segmentation principles.
However, although this method achieves satisfactory results in multi-focal image fusion, simple segmentation is not realistic because of the different imaging modes of different sensors, so the effect on the fusion of infrared and visible images is not ideal.
To tackle this issue, Liu \textit{et al.} ____ presented a deep learning-based technique that is specifically designed for fusing infrared and visible images, which straight inputs the original images into the network to acquire weight maps.
The formulation of fusion strategies is learned autonomously by the network, providing a high level of autonomy and adaptability.
CNN, with its excellent capability for local feature extraction, exhibits strong performance in image fusion.
However, as the receptive field increases, the fusion of high-level semantic features not only leads to the loss of features at different scales but also results in the loss of long-range dependencies.

\subsection{Transformer-based approaches}
Transformer ____ was first proposed by Google and applied to Natural Language Processing (NLP), which attracted widespread attention and achieved remarkable achievements in multiple tasks in the field.
With the superior performance of Transformer based on global self-attention, more researchers have attempted to apply it to computer vision.
Dosovitskiy \textit{et al.} ____ proposed and introduced the specific architecture of the ViT model and clarified the feasibility of applying Transformer to computer vision.
ViT divides the input image into a succession of image patches, converts them into sequential data, and then processes them by the Transformer.
When sufficient data is available for pretraining, ViT has achieved performance surpassing traditional CNN models, bringing new ideas and methods to computer vision.
Inspired by VIT, Liu \textit{et al.} ____ introduced a model called Swin Transformer, which incorporates a hierarchical window attention mechanism into the traditional Transformer model, reducing computational complexity.
It divides the image into a series of windows and processes them using window-level attention while handling the details within each window using image patch-level attention.

The advantage of extracting long-range dependencies has led to the application of Transformer to image fusion in recent years.
In order to address the limitation of traditional CNN in capturing long-range dependencies, Vibashan \textit{et al.} ____ proposed the Image Fusion Transformer (IFT).
This model introduces Transformer modules in the fusion layer, which enhances the fusion of long-range information while preserving local features.
Existing Transformer-based methods mainly explore intra-domain interactions.
To address this limitation, Ma \textit{et al.} ____ proposed a Swin Transformer model that incorporates self-attention-based intra-domain fusion units and cross-attention-based inter-domain fusion units.
This design enables the integration of cross-domain dependencies and global interactions.
While Transformer exhibits excellent global performance and avoids generating artifacts, its ability to extract local features is less strong than CNN.
It may struggle to capture subtle variations between local features, potentially leading to blurriness or distortion in the fusion results at the detail level.

\subsection{Integration of multiscale local and global features.}
Local features capture the characteristics of key pixels or image regions, aiding in capturing specific objects and details within an image. Global features reflect the overall properties of an image, helping to capture the holistic structure of the image. Combining local and global features in computer vision allows for a more comprehensive description of image features, reducing noise interference and enhancing the richness and robustness of features.

In recent years, there has been a proliferation of work combining local features with global features.
Li \textit{et al.} ____ proposed an infrared and visible light fusion model that alternates between convolutional and transformer layers.
This model combined the local features from convolutional networks with the long-range dependency features from transformers, yielding satisfactory results.
However, the model solely employs two branches to extract local and global features from the infrared and visible light images separately, without considering the interaction between features across different images during the extraction process.
Huo \textit{et al.} ____ proposed a three-branch hierarchical multiscale feature fusion model for medical image classification.
This model integrates a global feature module and a local feature module into a parallel hierarchical structure, utilizing an adaptive hierarchical feature fusion module to fuse multilayered global features with local features.
In addition, Yang \textit{et al.} ____ proposed multiscale dual attention (MDA) model for infrared and visible image fusion.
The model architecture consists of down-sampling blocks, attention-based fusion blocks, up-sampling blocks and reconstruction block.
The down-sampling module decomposes the original image into different scales, where each scale utilizes dual attention-based fusion blocks to extract and fuse features independently.
However, down-sampling blocks might lead to a loss of high-frequency details and fine-grained information in the images, which can affect the quality of the final fused output.