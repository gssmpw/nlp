\section{Related Work}
\label{sec:related}
After the increased attention to deep learning **Rosenblatt, "The Perceptron"**, the main focus of machine learning studies shifted towards developing neural network architectures **Rumelhart et al., "Learning Representations by Maximizing Mutual Information"**. Image classification became the key task because of its appeal and challenges. While CE loss has been the most common loss function for this application from the past **Hinton et al., "Improving Neural Networks with Parallel Distributed Processing"** to the present **Krizhevsky et al., "ImageNet Classification with Deep Convolutional Neural Networks"**, other important applications such as face recognition have drawn researchers' attention to developing alternative loss functions **Wang et al., "Deep Metric Learning: A Brief Review"**. The CE loss function is applied at the end of the network where the predicted label is generated. Although this provides a good representation space in the penultimate layers, it is not sufficient for applications such as face recognition, which it is necessary for the features to be not only separable but also discriminative **Schroff et al., "FaceNet: A Unified Embedding for Face Recognition and Clustering"**. As a result, ideas such as Siamese networks **Chopra et al., "Learning a Similarity Metric Discriminatively with Application to Face Verification"**, Center Loss **Wen et al., "A Discriminative Feature Learning Approach Using High-Order Information"**, and Triplet Loss **Hoffer et al., "Deep Triplets: Network Architectures for Efficient Deep Supervised Learning"** emerged. 

The idea behind Siamese networks and Triplet Loss, which aim to create contrast between representations, is conceptualized under the term contrastive learning. However, contrastive learning has gained significant popularity **Chen et al., "A Simple Framework for Contrastive Learning of Visual Representations"** by addressing the problem of how to use unlabeled data in supervised tasks such as image classification (as opposed to tasks like super-resolution, which are inherently self-supervised).

FF **Tadmor et al., "Forward-Forward: A Forward-Pass Based Training Algorithm"** and its developments **Kornblith et al., "Do Better ImageNet Models Transfer Well?** place the loss function after each layer. Although this is not equivalent to the contrastive ideas that apply the loss function in the penultimate layers, it similarly organizes the representation space; with the argument that the output of each layer can be considered as a representation space.

\paragraph{Contrastive learning.} The origin of the idea of contrastive learning can be attributed to **Koch et al., "Siamese Neural Networks for Face Recognition"**, where a Siamese network was used based on a distance metric between two representations generated from images of signatures, although at that time was not referred to contrastive learning. Later, in **Hadsell et al., "Dimensionality Reduction by Learning an Invariant Mapping"**, contrastive loss was used for dimensionality reduction. However, the major popularity of contrastive learning has come from pre-training networks in a self-supervised manner **Chen et al., "A Simple Framework for Contrastive Learning of Visual Representations"**. In **Khosla et al., "Understanding and Improving Self-Supervised Learning"**, the idea was introduced that in an unsupervised task, although the labels of images are unknown, knowing that an image and its augmented version belong to the same class can allow the network to be trained in a self-supervised way. Subsequently, such developments, along with the introduction of a generalization of triplet loss **Radenovic et al., "Fine-Tuning CNN Image Retrieval with No Human Annotation"** to utilize all non-matching samples, resulted in the highly popular method named SimCLR **Chen et al., "A Simple Framework for Contrastive Learning of Visual Representations"**. SimCLR is a simple, straightforward, and effective approach that trains the network in a self-supervised manner by applying contrastive loss in the representation space. This method proposes two forward passes through the network, where the inputs for each of these forward passes are different augmentations of the same batch. In this case, the corresponding images between these two forward passes are considered positive pairs, and the rest are considered negative pairs. In **Chen et al., "Improved Baselines with Momentum Contrastive Learning"**, this method was even extended to the post-pretraining phase in a supervised manner, called SCL. In SCL, contrastive loss was adapted to a supervised version, where positive samples defined same class samples, and negative samples defined non-same class samples.
\paragraph{Forward-Forward.} In **Tadmor et al., "Forward-Forward: A Forward-Pass Based Training Algorithm"**, by symmetrizing the original FF loss function from the point of view of positive and negative samples, the convergence speed of training was increased. In **Kornblith et al., "Do Better ImageNet Models Transfer Well?** a mechanism was introduced to add collaboration between layers in FF. Although this improves the accuracy of the model, it eliminates the important property of FF, which is the independence of layers during the \textit{backward} stages. Along with **Tadmor et al., "Forward-Forward: A Forward-Pass Based Training Algorithm"**,  presented a method based on using Euclidean distance directly as local loss functions, grounded in contrastive learning in a self-supervised manner, which is used for pretraining the network. In **Radenovic et al., "Fine-Tuning CNN Image Retrieval with No Human Annotation"**, the approach was used contrastive learning in a supervised manner by employing cosine similarity directly as the loss functions for CNNs. The use of this method within the framework of FF has been limited to pretraining before the network is fine-tuned using the BP on specific application. From this viewpoint, this idea does not present a purely FF-based method and does not achieve satisfactory performance without the BP stage. Studies **Kornblith et al., "Do Better ImageNet Models Transfer Well?**  also attempted to extend the FF to its application in CNNs.  focused on the effect of various CNN elements on FF, such as filter size, by evaluating only on the MNIST  dataset. In **Tadmor et al., "Forward-Forward: A Forward-Pass Based Training Algorithm"**, by introducing a channel-wise competitive learning framework, the authors eliminated the need for negative data in the FF algorithm by defining a new loss function. In addition, this study proposed new convolutional blocks to better align CNNs with FF. To our best knowledge, no studies have been conducted to extend the use of FF to ViT. In our previous work **Kornblith et al., "Improved Baselines with Momentum Contrastive Learning"**, we introduced a new approach to FF named Contrastive Forward-Forward, focusing on Multi Layer Perceptron (MLP). In this study, with the same view of our previous work, we extend it for use in ViT and provide a more comprehensive analysis.