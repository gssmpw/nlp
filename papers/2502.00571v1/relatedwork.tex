\section{Related Work}
\label{sec:related}
After the increased attention to deep learning \cite{krizhevsky2012imagenet}, the main focus of machine learning studies shifted towards developing neural network architectures \cite{simonyan2014very, he2016deep, hu2018squeeze, pmlr-v97-tan19a}. Image classification became the key task because of its appeal and challenges. While CE loss has been the most common loss function for this application from the past \cite{lecun1998gradient} to the present \cite{woo2023convnext}, other important applications such as face recognition have drawn researchers' attention to developing alternative loss functions \cite{koch2015siamese, wen2016discriminative, schroff2015facenet, liu2016large}. The CE loss function is applied at the end of the network where the predicted label is generated. Although this provides a good representation space in the penultimate layers, it is not sufficient for applications such as face recognition, which it is necessary for the features to be not only separable but also discriminative \cite{wen2016discriminative}. As a result, ideas such as Siamese networks \cite{koch2015siamese, Bromley1993SignatureVU}, Center Loss \cite{wen2016discriminative}, and Triplet Loss \cite{schroff2015facenet} emerged. 

The idea behind Siamese networks and Triplet Loss, which aim to create contrast between representations, is conceptualized under the term contrastive learning. However, contrastive learning has gained significant popularity \cite{chen2020simple, khosla2020supervised, he2020momentum, chen2020improved, radford2021learning} by addressing the problem of how to use unlabeled data in supervised tasks such as image classification (as opposed to tasks like super-resolution, which are inherently self-supervised).

FF \cite{hinton2022forward} and its developments \cite{lee2023symba, lorberbom2024layer, zhu2022contrastive, ahamed2023ffcl, scodellaro2023training, papachristodoulou2024convolutional, aghagolzadeh2024marginal} place the loss function after each layer. Although this is not equivalent to the contrastive ideas that apply the loss function in the penultimate layers, it similarly organizes the representation space; with the argument that the output of each layer can be considered as a representation space.

\paragraph{Contrastive learning.} The origin of the idea of contrastive learning can be attributed to \cite{Bromley1993SignatureVU}, where a Siamese network was used based on a distance metric between two representations generated from images of signatures, although at that time was not referred to contrastive learning. Later, in \cite{hadsell2006dimensionality}, contrastive loss was used for dimensionality reduction. However, the major popularity of contrastive learning has come from pre-training networks in a self-supervised manner \cite{chen2020simple, he2020momentum, chen2020improved, radford2021learning}. In \cite{NIPS2014_07563a3f, xie2020unsupervised}, the idea was introduced that in an unsupervised task, although the labels of images are unknown, knowing that an image and its augmented version belong to the same class can allow the network to be trained in a self-supervised way. Subsequently, such developments, along with the introduction of a generalization of triplet loss \cite{schroff2015facenet} to utilize all non-matching samples \cite{sohn2016improved}, resulted in the highly popular method named SimCLR \cite{chen2020simple}. SimCLR is a simple, straightforward, and effective approach that trains the network in a self-supervised manner by applying contrastive loss in the representation space. This method proposes two forward passes through the network, where the inputs for each of these forward passes are different augmentations of the same batch. In this case, the corresponding images between these two forward passes are considered positive pairs, and the rest are considered negative pairs. In \cite{khosla2020supervised}, this method was even extended to the post-pretraining phase in a supervised manner, called SCL. In SCL, contrastive loss was adapted to a supervised version, where positive samples defined same class samples, and negative samples defined non-same class samples.
\paragraph{Forward-Forward.} In \cite{lee2023symba}, by symmetrizing the original FF loss function from the point of view of positive and negative samples, the convergence speed of training was increased. In \cite{lorberbom2024layer}, a mechanism was introduced to add collaboration between layers in FF. Although this improves the accuracy of the model, it eliminates the important property of FF, which is the independence of layers during the \textit{backward} stages. Along with FF, \cite{zhu2022contrastive} presented a method based on using Euclidean distance directly as local loss functions, grounded in contrastive learning in a self-supervised manner, which is used for pretraining the network. In \cite{ahamed2023ffcl}, the approach was used contrastive learning in a supervised manner by employing cosine similarity directly as the loss functions for CNNs. The use of this method within the framework of FF has been limited to pretraining before the network is fine-tuned using the BP on specific application. From this viewpoint, this idea does not present a purely FF-based method and does not achieve satisfactory performance without the BP stage. Studies \cite{scodellaro2023training, papachristodoulou2024convolutional} also attempted to extend the FF to its application in CNNs. \cite{scodellaro2023training} focused on the effect of various CNN elements on FF, such as filter size, by evaluating only on the MNIST \cite{lecun1998gradient} dataset. In \cite{papachristodoulou2024convolutional}, by introducing a channel-wise competitive learning framework, the authors eliminated the need for negative data in the FF algorithm by defining a new loss function. In addition, this study proposed new convolutional blocks to better align CNNs with FF. To our best knowledge, no studies have been conducted to extend the use of FF to ViT. In our previous work \cite{aghagolzadeh2024marginal}, we introduced a new approach to FF named Contrastive Forward-Forward, focusing on Multi Layer Perceptron (MLP). In this study, with the same view of our previous work, we extend it for use in ViT and provide a more comprehensive analysis.