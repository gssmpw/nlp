\section{Related work}
\label{section:related}
The existing literature provides targeted assessment of the environmental impact of certain AI workloads, but often lacks a comprehensive and consistent view of these impacts. Here we provide the context of many of these studies following the cradle-to-grave life-cycle that Figure~\ref{fig:lifecycle-diagram} above defines.

\textbf{AI accelerator hardware manufacturing emissions} and life-cycle assessment studies are relatively limited in the literature. \citet{Kuo2022}~show that improvements in semiconductor manufacturing processes and node size have led to reduced life-cycle manufacturing emissions of DRAM; DRAM is an important component of AI accelerators,  as DRAM memory is one of key drivers of the overall AI hardware footprint in our results. \citet{Luccioni2022}~estimate emissions associated with server and GPU hardware of the open-access BLOOM language model. For the embodied emissions of AI, they use a placeholder for the GPU hardware, which the original source calls an ``arbitrary carbon footprint value'' of 150 kg ~\citep{Davy2021}.  Our TPU results are 1.5x--4x larger at 208--585 kg (TPU M+T in Table ~\ref{tab:specs}). Extrapolating from a 2019 Apple Mac Pro LCA ~\citep{Apple2019}, \citet{Wu2022} use 1808 kg for a whole GPU server; TPU machines average 1.7x larger at 2300--4672 kg.
%Table 1 for Embodied CO$_{2}$e of TPU Manufacturing + Transportation). 
~\citet{Gupta2022} estimate the embodied carbon of logic chips, DRAMs, SSDs, and hard disks.
\citet{Wang2024}~sketch the design of a carbon-efficient compute server using low-carbon components; they address standard servers, not AI hardware. 

While there's no full LCA for AI hardware,~\citet{Ji2024} reviews 6 known public LCAs of server computers. Their embodied emissions vary by 40x, from 423–15,593 kgCO$_{2}$e, averaging $\sim$3900 $\pm \sim$5900 kg. \citet{Wu2022} use 909 kg of embodied emissions for a CPU server host extrapolated from an Apple Mac Pro LCA. The embodied carbon for our machines average $\sim$3300 $\pm \sim$900 kgg, including the CPUs and TPUs. (The CPUs without TPUs average $\sim$1200 $\pm \sim$500 kgCO$_{2}$e.) For the compute servers in~\citep{Ji2024}, using a 4 or 5 year lifetime, operational emissions are 70\%–90\% of total LCA emissions for 2 Dell servers, 66\%–94\% for 2 HP servers, and 39\%–97\% for 2 Lenovo servers. For our servers, they are 70-90\% of emissions over a 6 year lifetime (using MB at the low-end, LB at the high end).

\textbf{AI accelerator hardware retirement emissions} associated with ultimate disposal, recycling, and cascaded use of data center hardware have limited study for AI hardware in the literature. This paper shows its size relative to other AI hardware lifecycle emissions to help studies of AI accelerator hardware retirement.

\textbf{AI accelerator operational emissions}, on the other hand,  have been a substantial focus of research. Individual papers often focus specifically on AI model training (development) or AI serving. Emissions associated with energy usage of AI model training have been the most studied emissions source in the literature. 

Early studies~\citep{Strubell2019,Patterson2021} focused primarily on estimating the energy-related emissions of training workloads. These studies found results to be very sensitive to input assumptions, such as the \textcolor{black}{electricity emission factor} and the sampling window of the training energy. A generational meta-analysis by \citet{Luccioni2023} found that training emissions per model have increased by a factor of approximately 100 from 2012 to 2023. Informed by previous studies, and using best-practices to improve training carbon efficiency, \citet{Touvron2023}~published an estimate for Meta’s production of Llama 2 emissions at 300 tonnes of CO$_{2}$e. 

Serving emissions associated with the inference and querying of AI models have garnered interest as inference begins to scale with consumer usage of AI products~\citep{patterson2024energy}. \citet{Luccioni2022}~first estimated weekly inference emissions associated with the BLOOM model. \citet{Patterson2022}~estimate 60\% of machine learning energy use at Google is attributable to inference, and \citet{Wu2022}~find some Meta use cases (namely LLMs) generate two thirds of their operational emissions during inference. These results illustrate the importance of taking a holistic approach to AI emissions measurement, since inference emissions are likely to increase their share of the total emissions of AI products as their adoption continues. 

\citet{Gupta2021} uses Google and Meta corporate emissions inventories to observe that data center emissions are shifting from operations to hardware design, manufacturing, and construction, e.g., they were half of Facebook’s 2019 Scope 3 emissions. Using LCAs, our results differ. Section ~\ref{appendix:EIR} explains the differences in accounting of ERs and LCAs. In particular, emissions for new data centers and new hardware must be fully accounted for in a single year for an ER rather than amortized over their lifetimes in an LCA.  \textcolor{black}{In addition, Section \ref{result: lca} points out the market-based emissions accounting practices are not identical across companies.}

\textbf{AI hardware \textcolor{black}{carbon intensity} metrics} are necessary to normalize emissions relative to their performance or value-creation. There is not yet consensus on a consistent \textcolor{black}{carbon intensity} metric, but typically CO$_{2}$e or energy / performance is deemed suitable~\citep{Jouppi2023,Vahdat2024}. Our proposal for the metric is CO$_{2}$e/ExaFLOP, labelled \textit{compute carbon intensity (CCI)}.

Some use machine TDP to infer hardware energy consumption (e.g., \citep{bouza2023estimate,lannelongue2021green,trebaol2020cumulator}), but we found that the ratio of TDP to actual average power varies from 2X--6X, so TDP dramatically overestimates the real result for TPUs.

\textcolor{black}{\textbf{Software optimizations} led to even larger improvements than hardware gains. For example, \citet{hernandez2020measuring} found algorithmic advances yielded a 44x performance improvement in 7 years and \citet{ho2024algorithmic} calculated that over 11 years they halve computation demands every 8 months. Similarly, the cost per token of commercial inference services has dropped by 10X in the past 2 years. Thus, it is likely that software optimizations will further reduce the carbon footprint per token in future years.}