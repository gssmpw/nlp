%Version 3 October 2023
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove Numbered in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst%  
 
%%\documentclass[sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[sn-mathphys-num]{sn-jnl}% Math and Physical Sciences Numbered Reference Style 
\usepackage{amsmath}
%%\documentclass[sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
%%\documentclass[sn-apa]{sn-jnl}% APA Reference Style 
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
\usepackage[utf8]{inputenc}
%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[Article Title]{An Enhanced Large Language Model For Cross Modal Query Understanding System Using DL-KeyBERT Based CAZSSCL-MPGPT}

%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg} 
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%

\author[1]{\fnm{Shreya} \sur{Singh}}\email{shreya.singh@dituniversity.edu.in}

\affil[1]{\orgname{DIT University}, \orgaddress{\city{Dehradun}, \postcode{248009}, \state{UK}, \country{India}}}

\abstract{Large Language Models (LLMs) are advanced deep-learning models designed to understand and generate human language. They work together with models that process data like images, enabling cross-modal understanding. However, none of the existing works concentrated on avoiding the echo chamber effect in the image to enhance the accuracy of the system. Thus, the proposed system considered this limitation and developed an enhanced LLM-based framework for cross-modal query understanding using DL-KeyBERT-based CAZSSCL-MPGPT. Firstly, the collected dataset consists of images and texts. Initially, the images are pre-processed. These preprocessed images then undergo object segmentation using Easom-You Only Look Once (E-YOLO). Then, the object skeletons are generated, and a knowledge graph is constructed using a Conditional Random Knowledge Graph (CRKG). After that, features are extracted from the knowledge graph, generated skeletons, and segmented objects. The optimal features are then selected using the Fossa Optimization Algorithm (FOA). Meanwhile, the text undergoes word embedding using DL-KeyBERT. Finally, the cross-modal query understanding system utilizes CAZSSCL-MPGPT to generate accurate and contextually relevant image descriptions as text. The proposed CAZSSCL-MPGPT achieved an accuracy of 99.14187362\% in the COCO dataset 2017 and 98.43224393\% in the vqav2-val dataset.}

\keywords{Cross Attention Layer and Zero-Shot Semantic Consistency Learning-based Mixup Phish Generative Pre-trained Transformer (CAZSSCL-MPGPT), Damerau-Levenshtein-based KeyBidirectional Encoder for Word Representations (DL-KeyBERT), Large Language Model, Cross-Modal Query Understand, Image Captioning, Echo chamber effect, Visual Question Answering (VQA).}

\maketitle

\section{Introduction}\label{sec1}

LLMs have significantly impacted Artificial Intelligence (AI) in recent years, enabling machines to understand and generate human-like text (Wang et al., 2024). These models are widely applied across various domains, including Natural Language Processing (NLP), and powering conversational agents like chatbots (Fan et al., 2024). Prominent examples include the Generative Pre-trained Transformer (GPT) family, such as ChatGPT and GPT-4, which are well-known for their strong performance and wide range of applications (Messina et al., 2021). 

Multimodal Large Language Models (MLLMs) extend the capabilities of LLMs by integrating both textual and visual data for tasks, such as cross-modal retrieval and VQA (Chen et al., 2024) (Chen et al., 2021). One key area of the MLLMs application is cross-modal queries, which involve retrieving information across different modalities, such as text and images (Liu et al., 2023) (Xu et al., 2024). For instance, in cross-modal retrieval, queries and results are represented as vectors in a common space. Also, similarity functions are used to identify relevant matches (Kalyan, 2024). Another prominent application of this approach is VQA, where an AI system answers questions about images (Lu et al., 2023). Other tasks like image captioning and image retrieval also fall under Visio-linguistic tasks that integrate visual and textual information (Zhang et al., 2021) (Chen et al., 2021a). These advancements are essential in applications, such as search engines, enabling accurate retrieval of relevant data across modalities (Liu et al., 2024).

Recent advancements in VQA and other cross-modal tasks include methods like Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM, and Graph Convolutional Networks (GCNs) (Salaberria et al., 2023) (Ye et al., 2024). Despite these advancements, challenges like difficulty in capturing long-term dependencies and scalability issues occur when processing complex data (Tingting et al., 2023). Also, none of the existing works concentrated on avoiding the echo chamber effect that arose from the images. To mitigate this issue, a DL-KeyBERT-based CAZSSCL-MPGPT framework is proposed, enhancing performance in cross-modal tasks. 

\subsection{Problem Statement}\label{subsec1}
The existing research methodologies for cross-modal query understanding systems faced several limitations, which are outlined below,
\begin{itemize}
    \item Existing Multi-Modal Query Understanding systems, such as VQA and Image Captioning, did not address the echo chamber effect in image processing. This effect arose from over-reliance on frequent patterns in the training data, which impacted accuracy.
    \item The existing (Cheng et al., 2021) failed to capture the intricate semantic relationships and dependencies between modalities. As a result, the existing work compromised the contextual significance of each data type, limiting the improvement of the LLM.
    \item In the existing work (Zhang et al., 2024), the dynamic alignment between multiple modalities was not analyzed and unseen data was not classified correctly, limiting the improvement of the LLM. 
    \item The prevailing (Li et al., 2024) struggled with captioning highly detailed or crowded images, which often included numerous objects, scenes, or backgrounds. It faced difficulty in highlighting the most relevant features of such images.
    \item Most of the Multi-Modal Query Processing Systems used unprocessed images, leading to misleading content due to excessive noise.
\end{itemize}

\subsection{Objectives}\label{subsec2}
The objectives of the proposed work for understanding cross-modal queries are listed below.
\begin{itemize}
    \item To address the echo chamber effect in images and improve accuracy, the proposed solution involves constructing a knowledge graph using CRKG. This approach mitigates the model's overreliance on frequent patterns present in the training data.
    \item To improve LLM by capturing intricate semantic relationships and dependencies between modalities, DL-KeyBERT is utilized.
    \item The analysis of dynamic alignment between multiple modalities and the accurate classification of unseen data is achieved by introducing the cross-attention layer and zero-shot semantic consistency learning (CAZSSCL) to enhance the LLM.
    \item An E-YOLO-based object detection technique is used to better analyze complex image structures, enhancing LLM performance.
    \item The issue of misleading content caused by unprocessed images is solved by processing the image using an MF and PG-CLAHE to reduce noise and enhance image quality for improved LLM accuracy.
\end{itemize}
The remaining structure of the paper is organized as follows: Section 2 discusses the related works; Section 3 presents the proposed methodology; Section 4 reveals the results and provides a discussion; and Section 5 concludes the paper with future work.

\section{Literature Survey}\label{sec2}

(Cheng et al., 2021) deployed a deep semantic alignment network in Remote Sensing (RS) for cross-modal image-text retrieval. The Semantic Alignment Module used attention and gate mechanisms to optimize features, enhancing image-text retrieval. The model achieved improved performance in image-text matching. However, it struggled to capture intricate semantic relationships and dependencies between modalities, which compromised the contextual significance of each data type.

(Zhang et al., 2024) developed EarthGPT, a universal MLLM for multi-sensor image understanding in the RS domain. EarthGPT integrated a visual-enhanced perception mechanism to refine both coarse and fine visual information. It used cross-modal comprehension and unified instruction tuning, improving RS visual interpretation across sensor data. However, the model failed to classify unseen images that it had never encountered during the training phase. It also struggled to effectively analyze the dynamic alignment between multiple modalities, such as text and image.

(Li et al., 2024) presented an interactive perception network for LLMs. The framework extracted global and fine-grained image features using the Contrastive Language–Image Pre-training (CLIP). The Request-based Visual Information-seeking module enabled dynamic interaction, allowing LLMs to generate effective responses. Thus, the model allowed LLMs to incorporate the desired visual information for various human queries. Nevertheless, the model struggled with captioning images by highlighting the most relevant or interesting features due to the presence of multiple objects, scenes, or backgrounds presented in the crowded or detailed images.

(Lim et al., 2024) introduced a framework for Unification, Retrieval, and Generation in multimodal question answering using pre-trained language models. This framework utilized a Large Language and Vision Assistant (LLaVA) to generate detailed image descriptions. Next, the Flan-Text-to-Text Transfer Transformer (T5)-base model was fine-tuned for answer generation. The model excelled in both question-answering and retrieval tasks. However, the LLaVA model struggled with complex visual details, specialized domains, and multiple images.

(Mashrur et al., 2024) presented a VQA model using semantic Cross-Modal Augmentation (CMA). The input images were processed through CMA's mixer to create augmented replicas. Next, a back-translator was used to create multiple augmentations of the question text. Then, batched predictions were generated using the vision-language model. The model effectively handled unanswerable questions and demonstrated strong generalizability. However, the back translation failed to identify typos and punctuation issues, thus limiting its accuracy in certain cases.

(Verma et al., 2022) introduced an encoder-decoder model for automatic image caption generation. Firstly, the images were collected, and the features were extracted using the Visual Geometry Group16 Hybrid Places 1365 model as an encoder. Next, these features were fed into an LSTM-based decoder to generate captions word by word. Thus, the model produced grammatically correct captions for the input images. Nevertheless, LSTM’s limited parallelization made them less suited for large-scale or real-time applications.

(Zhu et al., 2021) developed a Multi-grained Cross-modal similarity Query with Interpretability (MCQI) framework for processing queries. Firstly, a Region Convolutional Neural Network was used to detect objects in the image. Then, LSTM and an attention mechanism identified latent semantic relationships. When a query was executed, the MCQI framework processed it through the index using refined k-Nearest Neighbors. This approach improved the scalability of the MCQI framework. However, the model was heavily dependent on large training data from specific areas, limiting its generalization.

(Dong et al., 2021) illustrated cross-modal retrieval using an Adversarial GCN (AGCN). The model employed a graph feature generator based on a GCN. It utilized a minimax game strategy with a graph feature discriminator to ensure modality-invariant feature representations. The results showed that AGCN improved cross-modal retrieval accuracy. However, the minimax game strategy in the AGCN model introduced significant computational complexity, making it resource-intensive and less efficient.

(Sun et al., 2021) demonstrated a Cross-Modal Pre-Aligned method with Global and Local information (CMPAGL) for efficient RS image-text retrieval. The model used a Gswin transformer block for feature extraction. Then, text features were embedded using a Bidirectional Encoder Representations from Transformers (BERT)-based encoder. The model showed superior performance in retrieving image-text pairs. However, BERT was slow to train due to its large size and the extensive number of weights that needed updating.

(Zhang et al., 2024a) explored an enhanced feature extraction framework to enhance cross-modal image-text retrieval. The Enhanced Vision Transformer extracted the image features. Then, a triplet loss function optimized the model. The framework achieved high detection accuracy. However, it struggled to balance dynamic adaptation to both large and small feature extraction requirements, affecting model efficiency and generalization.

\section{Proposed Methodology For Cross-Modal Query Understanding System Using Enhanced LLM}
This research paper proposes an advanced LLM for understanding cross-model queries using the DL-KeyBERT-based CAZSSCL-MPGPT technique. The main phases of the proposed work are: dataset collection, image preprocessing, object segmentation, object skeleton generation, knowledge graph construction, text word embedding, feature extraction, feature selection, and the cross-modal query understanding system. The structural diagram of the proposed framework is illustrated in Figure 1.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{image.png}
    \caption{Structural Diagram of the Proposed Framework}
    \label{fig:enter-label}
\end{figure}

\subsection{Input Dataset}
The input data is collected from a public dataset consisting of images and corresponding textual descriptions. The texts (A) and images (B) in the dataset can be expressed as,
\begin{equation}
 A = \{ A^1, A^2, A^3, A^4, A^5, \ldots, A^{a-1}, A^a \}
\label{eq:1}
\end{equation}

\begin{equation}
 B = \{ B^1, B^2, B^3, B^4, B^5, \ldots, B^{b-1}, B^b \}
\label{eq:2}
\end{equation}
Where, (\textit{a}) and (\textit{b}) illustrate the total number of (A) and (B). 

\subsection{Pre-Processing}
In this phase, (B) is pre-processed to improve the quality of the cross-modal query understanding system. The pre-processing steps are as follows,

\subsubsection{Resize}
In this step, (B) are resized to a uniform dimension to ensure consistency for further processing. The resized image ($\Re$) is represented as
\begin{equation}
\Re=\psi(B)
\label{eq:placeholder_label}
\end{equation}
Where, ($\psi$) indicates the resizing function. 

\subsubsection{Noise Removal}
Next, the unwanted noises in ($\Re$) are removed using MF, which eliminates random noise that could degrade data quality. Thus, only relevant visual content is retained. At first, a window ($\omega$) is selected for each pixel and moves across the image to process it pixel by pixel. Next, within ($\omega$), the pixel values are collected and then sorted in ascending order. The sorted pixels are denoted as ($\Re$'), and the median value (M) is calculated by,

% Requires: \usepackage{amsmath}
\begin{equation}
    M = 
    \begin{cases} 
        \mathfrak{R'}_{(\frac{n+1}{2})}, & \text{if } n \text{ is odd} \\ 
        \frac{\mathfrak{R'}_{(\frac{n}{2})} + \mathfrak{R'}_{(\frac{n+1}{2})}}{2}, & \text{if } n \text{ is even}
    \end{cases}
    \label{eq:placeholder_label}
\end{equation}


Where, (\textit{n}) depicts the number of pixel values present in ($\Re$'). Next, the central pixel (C) is replaced with the calculated (M). It is formulated as,
\begin{equation}
C = M
\label{eq:placeholder}
\end{equation}

By replacing each pixel with (M) from its neighborhood, the random noise is effectively removed. Finally, the noise-removed image is represented as (N).

\subsubsection{Contrast Enhancement}
In this step, the contrast of (N) is enhanced using PG-CLAHE to make the relevant features more distinguishable and to enhance the visual quality of the image. Contrast Limited Adaptive Histogram Equalization (CLAHE) improves local contrast in areas with varying intensities. It also avoids over-enhancing the noise in uniform or homogeneous regions. However, CLAHE uses a clip limit parameter, and an improper setting of this parameter can result in over-enhancement of the image. To address this issue, the Pareto Gini distribution technique is utilized to determine the optimal clip limit for the CLAHE algorithm. The steps involved in PG-CLAHE are described below,

Initially, (N) is divided into small, non-overlapping tiles (G) for local processing and region-specific enhancement. Next, for each (G), histogram equalization is performed to enhance the contrast. The histogram ($\xi$) of each tile is calculated as,

\begin{equation}
        \xi = \sum_{(x,y) \in G} G(x,y)
    \label{eq:placeholder_label}
\end{equation}

Where, \textit{G(x,y)}  denotes the pixel intensity value at co-ordinates \textit{(x,y)} within the \textit{(G)}. Next, a clipping limit ($\chi$) is applied to prevent excessive contrast enhancement. The Pareto Gini distribution technique is used to determine the optimal clipping limit. 
\begin{equation}
        \chi \xrightarrow{} \widetilde{\chi}\ = 1- \frac{2}{\sigma + 1}
    \label{eq:placeholder_label}
\end{equation}

Here, ($\sigma$)  signifies the Pareto exponent and ($\widetilde{\chi}$)  denotes the Gini index. The clipped image (H) is represented as,
\begin{equation}
        H=min(\xi,\widetilde{\chi})
    \label{eq:placeholder_label}
\end{equation}
For each (H), the Cumulative Distribution Function (CDF) is calculated to map the intensity values to the desired output image. The CDF ($H'''$) is equated as,
\begin{equation}
        H'''=\frac{\sum_{G=0}^{g} H(G)}{\iota_{pix}}
    \label{eq:placeholder_label}
\end{equation}
Where, ($\iota_{pix}$)  represents the total number of pixels in (H) and (\textit{g}) indicates the total number of tiles. Next, to enhance the dynamic range and maximize contrast, the CDF is normalized. The normalized CDF ($\mu(x,y)$)  is formulated as,
\begin{equation}
        \mu(x,y)=\frac{H'''-min(H''')}{max(H''')-min(H''')}
    \label{eq:placeholder_label}
\end{equation}
Where, ($min(H''')$) and ($max(H''')$) denotes the minimum and maximum of ($H'''$), respectively. To prevent boundary artifacts and ensure smooth transitions between adjacent tiles, interpolation is applied. The interpolated pixel value ($\breve{\mu}$)  is computed as,
\begin{equation}
    \breve{\mu}=(1-p)(1-q)\mu(x,y)+p(1-q)\mu(x+1,y)+pq\mu(x+1,y+1)+(1-p)q\mu(x,y+1)
    \label{eq:placeholder_label}
\end{equation}
Here, (\textit{p}) and (\textit{q}) illustrates the fractional horizontal and vertical distances between two adjacent tiles, respectively. Thus, the contrast-enhanced image is specified as ($E'$). The pseudocode for the proposed PG-CLAHE is illustrated as, \break

\hrule
\vspace{0.2cm}
\noindent
\textbf{\large Pseudocode for PG-CLAHE}

\vspace{0.2cm}
\noindent
\textbf{Input:} Noise Removed Image (\(N\))\\
\textbf{Output:} Contrast Enhanced Image (\(E'\))

\hrule
\vspace{0.2cm}

\noindent
\textbf{Begin}

\quad \textbf{Initialize} (\(G\)), (\(x,y\)), (\(p\)), (\(q\)), (\(\sigma\)), (\(\widetilde{\chi}\))

\quad \textbf{For each} (\(N\)) \textbf{do}

\quad\quad \textbf{Divide} (\(N\)) \(\xrightarrow{to} G\)

\quad\quad \textbf{Calculate} histogram (\(\xi\)) \textbf{for each} (\(G\))

\quad\quad \textbf{Compute} (\(\chi\))

\quad\quad\quad $\chi \xrightarrow{} \widetilde{\chi}\ = 1- \frac{2}{\sigma + 1}$

\quad\quad \textbf{Select} clipping limit (\(\chi\))

\quad\quad \textbf{Obtain} (H) by applying (\(\chi\))

\quad\quad\quad $H=min(\xi,\widetilde{\chi})$

\quad\quad \textbf{Evaluate} ($H'''$) \#CDF

\quad\quad\quad $H'''=\frac{\sum_{G=0}^{g} H(G)}{\iota_{pix}}$

\quad\quad \textbf{Normalize} the ($H'''$)

\quad\quad\quad $\mu(x,y)=\frac{H'''-min(H''')}{max(H''')-min(H''')}$

\quad\quad \textbf{Compute} ($\breve{\mu}$)

\quad \textbf{End} for

\quad \textbf{Return} ($E'$)

\textbf{End} \break

\hrule
\vspace{0.2cm}

\noindent
Finally, the pre-processed image is denoted as ($\zeta$).

\subsection{Object Segmentation}
In this phase, the objects in ($\zeta$) are segmented using E-YOLO to differentiate various objects, scenes, or backgrounds present in the image. Here, You Only Look Once (YOLO) is utilized for its high speed and accuracy in identifying objects. YOLO provides an accurate bounding box for each detected object and enhances object localization. However, YOLO struggles to detect objects that are far from the camera or small in size. To address this limitation, the Easom function is used to find the scaling factor. This function is also integrated with overlapping elimination to effectively identify small objects, thereby improving the efficiency of localization. At first, ($\zeta$) is divided into a number of grids ($Q$) for detecting the objects that fall within its boundaries. Next, for each ($Q$), ($r$) numbers of bounding boxes ($\beta$) are predicted with their confidence score ($\ddot{c}^r$). 
 
\begin{equation}
    \beta=[o^r,\dddot{o}^r,\tau^r,\dddot{\tau}^r,\ddot{c}^r]^{r=1 \ to\ \widetilde{r}}
    \label{eq:placeholder_label}
\end{equation}
Where, ($o^r$)  and  ($\dddot{o}^r$) refers to the coordinates of the centre of the bounding box relative to the grid cell, respectively,   ($\widetilde{r}$) denotes the total number of ($r$), and ($\tau^r$)  and ($\dddot{\tau}^r$) indicate height and width of ($\beta$), respectively. Then, the Easom function ($\zeta$) is employed to identify a scaling factor for ($\beta$).
\begin{equation}
    \zeta=-cos(i) \cdot cos(j) \cdot exp(-((i-\pi)^2+ (j-\pi)^2))
    \label{eq:placeholder_label}
\end{equation}
Here, ($\pi$) indicates the mathematical constant and ($i$) and ($j$) represent the dimensions of ($\beta$). Thus, the scaling factor ($S$) is identified from ($\zeta$), and ($\beta$) is adjusted.
\begin{equation}
    \beta'=\beta * \zeta
    \label{eq:placeholder_label}
\end{equation}
Where, ($\beta'$) indicates the adjusted ($\beta$). Further, class probabilities ($P$) are predicted for each object-detected image. 
\begin{equation}
    P=\delta(\ddot{c}_{h'''} | \beta')
    \label{eq:placeholder_label}
\end{equation}
\begin{equation}
    \delta(\beta')=\frac{e^{\beta'_{y''}}}{\sum_{y'''}^{\widetilde{j}} e^{\beta'_{y'''}}}
    \label{eq:placeholder_label}
\end{equation}
Here, ($\ddot{c}_{h'''}$)  represents the class scores, which is the raw value indicating class likelihood, ($\delta$) indicates the softmax activation function, ($e$)  refers to the Euler’s value, ($\beta'_{y''}$)  indicates the ($y''^{th}$)  class of ($\beta'$), ($y'''=1 \ to \ \widetilde{j}$)  represents the sum of the exponentials of all class scores, ($\widetilde{j}$)  refers to the total number of ($y'''$), and ($\beta'_{y'''}$) signifies the ($y'''^{th}$)  index of ($\beta'$). Now, Non-Maximum Suppression is applied to eliminate redundant overlapping bounding boxes by calculating the Intersection Over Union (IOU) for each ($\beta'$), thus attaining final segmented objects ($\vartheta$).
\begin{equation}
    \vartheta=P*\kappa*\zeta
    \label{eq:placeholder_label}
\end{equation}
\begin{equation}
    \kappa=\frac{{|\beta^d \cap \beta^{\overleftrightarrow{d}|}}}{|\beta^d \cup \beta^{\overleftrightarrow{d}}|} 
    \label{eq:placeholder_label}
\end{equation}
Where, ($\kappa$) indicates the IOU, ($\beta^d$) and ($\beta^{\overleftrightarrow{d}}$) illustrate ($d^{th}$) and ($\overleftrightarrow{d}^{th}$) bounding boxes, correspondingly, and ($\cap$) and ($\cup$) signifies the intersection and union operations, respectively. Thus, ($\vartheta$) are obtained.


\subsection{Object Skeleton Generation}
Next, the object skeletons are generated from ($\vartheta$) to capture the core structure of each object. This simplifies the representation by focusing on the object's shape and removing unnecessary details. The skeletonized images ($\rho$) are illustrated as,
\begin{equation}
    \rho=\{\rho_1,\rho_2,\rho_3,\rho_4,......\rho_l\}
    \label{eq:placeholder_label}
\end{equation}
Where, ($l$) refers to the total number of ($\rho$).

\subsection{Knowledge Graph Construction}
Here, a knowledge graph is constructed from ($\rho$) using CRKG. This graph is designed to address the echo chamber effect in images and improve accuracy. The echo chamber effect arises when the model relies heavily on frequent patterns in its training data, leading to biased results. The Knowledge Graph (KG) integrates diverse data sources and provides a comprehensive view of the information. By representing entities and their relationships, the knowledge graph enables machines to understand the context of the data. However, KG can also perpetuate biases present in the underlying data and struggle to capture dynamic relationships in the images. To mitigate this issue, the conditional random technique is used. This technique reduces the echo chamber effect by accounting for dependencies between entities and relationships, ensuring a more balanced and accurate graph.

Firstly, the entities ($J^{en}$) are identified from ($\rho$)  by considering their context and spatial relationships between regions. Here, the conditional random technique is used to label the entities. 
\begin{equation}
    R(Y|\rho)=\frac{1}{\overleftrightarrow{R}(\rho)} exp(\sum_{l}w^lF^l(Y,\rho))
    \label{eq:placeholder_label}
\end{equation}
Here, ($R(Y|\rho)$)  denotes the conditional probability,   ($\overleftrightarrow{R}(\rho)$) refers normalization factor,  ($w^l$) signifies the weight parameter associated with a specific feature of ($l$), and ($F^l(Y,\rho)$) illustrates the feature function that computes the relationship between input ($\rho$) and output sequences ($Y$), which represents labelled entities derived from ($\rho$). Next, the relationships ($J^{re}$) between these  ($J^{en}$) are identified. 
\begin{equation}
    J^{re}=\{\overleftrightarrow{j} | J'^{en} \ is \ related \ to \ J''^{en}\}
    \label{eq:placeholder_label}
\end{equation}
Here, ($\overleftrightarrow{j}$)  refers to the nature of their relationship and ($J'^{en}$) and  ($J''^{en}$) indicates the entities in the graph. Next, the knowledge graph ($K$) is constructed by representing entities as nodes and relationships as edges.
\begin{equation}
    K=\langle J^{en},J^{re}\rangle
    \label{eq:placeholder_label}
\end{equation}
Thus, ($K$)  is then given as input to the feature extraction phase for further processing.

\subsection{Feature Extraction}
Now, various image features, such as color, edge, texture, Gray-Level Co-occurrence Matrix (GLCM), Histogram of Oriented Gradients (HOG), mean, variance, kurtosis, skewness, smoothness, correlation, etc., are extracted from ($\vartheta$) and ($\rho$) . Similarly, from ($K$), features, such as node attributes, edge relationships, node degree, closeness centrality, pagerank, etc., are also extracted. These extracted features ($E$) are represented as,
\begin{equation}
    E=\{E_1,E_2,E_3,E_4,......E_m\}
    \label{eq:placeholder_label}
\end{equation}
Where,  ($m$) represents the total numbers of ($E$).

\subsection{Feature Selection}
Then, optimal features are selected from ($E$)  using FOA. FOA is used to identify the optimal subset of features for enhancing the model performance by efficiently navigating the solution space. It balances exploration and exploitation, which helps to avoid local optima. Firstly, the population matrix ($Z$)  represents that the initial positions of the fossa are randomly initialized within the feature space based on ($E$). The ($Z$) is defined by,
\begin{equation}
Z=
    \begin{bmatrix}
    Z_1 \\
    \vdots \\
    Z_k \\
    \vdots \\
    Z_D
\end{bmatrix}
=
    \begin{bmatrix}
    z_{1,1}  & \hdots z_{1,t} & \hdots & z_{1,u} \\
    \vdots & \ \ \vdots & \ \ \ \ \  \vdots \\
    z_{k,1}  & \hdots z_{k,t} & \hdots & z_{k,u} \\
    \vdots & \ \ \vdots & \ \ \ \ \  \vdots \\
    z_{D,1}  & \hdots z_{D,t} & \hdots & z_{D,u} \\
\end{bmatrix}
    \label{eq:placeholder_label}
\end{equation}
\begin{equation}
    z_{k,t}=(ub_t -lb_t)*\varphi+lb_t
    \label{eq:placeholder_label}
\end{equation}
Where, ($Z_k$)  denotes the ($k^{th}$)  fossa, ($z_{k,t}$)  illustrates the ($t^{th}$)  dimension of ($k^{th}$)  fossa in the search space, ($u$)  refers to the number of decision variables,  ($D$) indicates the number of fossa, ($\varphi$)  signifies a random number, and ($ub_t$)  and ($lb_t$)  determines the upper and lower bound of ($t^{th}$) dimension variable, respectively. Next, the fitness ($\Phi_h$) of each fossa is evaluated based on the maximum classification accuracy ($max(\hbar^{class})$). 
\begin{equation}
    \Phi_h=max(\hbar^{class})
    \label{eq:placeholder_label}
\end{equation}
After that, in the exploration phase, the candidate lemurs ($\delta_k$)  (better solutions) for each fossa are determined as,
\begin{equation}
    \delta_k=\{Z_h:\Phi_h < \Phi_k \ and \ h \neq k\},where \ k=1,2,..,D\ and \ h \in \{1,2,..,D\}
    \label{eq:placeholder_label}
\end{equation}
Where, ($Z_h$)  expresses the position of the ($h^{th}$) fossa with a better ($\Phi_h$). Here, ($h$)  and  ($k$) denotes the index of the current fossa and selected fossa (lemur), respectively. Next, the fossa randomly selects a lemur ($\lambda_{k,t}$) from ($\delta_h$)  and adjusts its position as,
\begin{equation}
    z_{k,t}^{\hat{P}1}=z_{k,t} + \varphi_{k,t}*(\lambda_{k,t}-I_{k,t}*z_{k,t})
    \label{eq:placeholder_label}
\end{equation}
Where, ($Z_{k}^{\hat{P}1}$)  signifies the new position of the ($k^{th}$)  fossa during the attack, ($\Phi_{k,t}$)  illustrates the random values, and ($I_{k,t}$)  indicates the random integer. If the new position  ($Z_{k}^{\hat{P}1}$) improves the objective function, it replaces the current position as,
\begin{equation}
    Z_k=
    \begin{cases} 
    Z_{k}^{\hat{P}1},\ \ \Phi_{k}^{\hat{P}1} < \Phi_k \\
    Z_k, \  \ else
    \end{cases}
    \label{eq:placeholder_label}
\end{equation}
Where, ($\Phi_{k}^{\hat{P}1}$)  determines the objective function value for the new position. Next, in the exploitation phase, the fossa chases the lemur. The fossa adjusts its position to mimic the pursuit, which is given by,
\begin{equation}
    z_{k,t}^{\hat{P}2}=z_{k,t} + (1-2\varphi_{k,t})*\frac{ub_t-lb_t}{t''}
    \label{eq:placeholder_label}
\end{equation}
Here, ($t''$)  denotes the iteration count and ($Z_{k,t}^{\hat{P}2}$)  illustrates the updated position for the  ($k^{th}$) fossa. If the new position ($Z_{k}^{\hat{P}2}$)  improves the objective function, it replaces the current position as,
\begin{equation}
    Z_k=
    \begin{cases} 
    Z_{k}^{\hat{P}2},\ \ \Phi_{k}^{\hat{P}2} < \Phi_k \\
    Z_k, \  \ else
    \end{cases}
    \label{eq:placeholder_label}
\end{equation}
Where,  ($\Phi_{k}^{\hat{P}2}$) determines the objective function value for the updated position. Thus, after going through several iterations, the optimal features ($O$) are selected.

\subsection{Word Embedding}
In this phase, word embedding is performed for the collected texts  ($A$) using DL-KeyBERT to convert text data into a vector format. This embedding technique captures intricate semantic relationships and dependencies between modalities, ultimately improving the performance of the LLM. KeyBidirectional Encoder Representations from Transformers (KeyBERT) is a minimal and easy-to-use embedding technique that leverages BERT embeddings. It transforms text into vector representations that capture the semantic meaning of words and their context. However, KeyBERT’s embeddings are based on cosine similarity between word and document embeddings, which does not always guarantee relevance. Also, it may produce embeddings that are semantically close but contextually meaningless. To address this, Damerau-Levenshtein (DL), a method for orthographic similarity, is used to identify the top (\u{n})  most similar terms in KeyBERT for improved word embedding quality. Firstly, ($A$)  is divided into small units called tokens ($\widetilde{A}$). Next, an embedding vector of ($\widetilde{A}$)  is generated using a pre-trained model i.e., BERT. The embedded vector ($\dddot{E}$) is equated as,
\begin{equation}
    \dddot{E}=\Im(\widetilde{A})
    \label{eq:placeholder_label}
\end{equation}
Here, ($\Im$)  denotes the transformer model. The candidate keywords ($X$) are then extracted from ($E$). Next, candidate keyword embeddings ($\widetilde{X}$) are generated as,
\begin{equation}
    \widetilde{X}=\Im(X)
    \label{eq:placeholder_label}
\end{equation}
Then, the DL distance (L(\u{a},\newtie{b})) is computed between ($\widetilde{A}$)  and ($\widetilde{X}$)  to identify the top  (\u{n}) similar terms. It is computed as,
\begin{equation}
    L(\breve{a},\text{\newtie{b}})=
    \begin{cases} 
    max(\breve{a},\text{\newtie{b}}), \qquad\qquad\qquad\qquad\qquad if \ min(\breve{a},\text{\newtie{b}})=0 \\
    min(L(\breve{a}-1,\text{\newtie{b}})+1), \qquad\qquad\qquad\qquad (del) \\
    min(L(\breve{a},\text{\newtie{b}}-1)+1), \qquad\qquad\qquad\qquad (ins) \\
    min(L(\breve{a}-1,\text{\newtie{b}}-1)+c^{sub}(\breve{a},\text{\newtie{b}})), \qquad\quad (sub) \\
    min(L(\breve{a}-2,\text{\newtie{b}}-2)+c_{tra}(\breve{a},\text{\newtie{b}})), \qquad\quad (tra) \\
    \end{cases}
    \label{eq:placeholder_label}
\end{equation}
Here, (L(\u{a},\newtie{b}))  refers to the distance between the first (\u{a})  and  (\newtie{a}) characters of string  ($\widetilde{A}$)  and ($\widetilde{X}$), respectively, $c^{sub}(\breve{a},\text{\newtie{b}})$  and  $c_{tra}(\breve{a},\text{\newtie{b}})$ denotes substitution and transposition cost, correspondingly, and $(del)$ ,$(ins)$ ,$(sub)$ , and  $(tra)$ denotes deletion, insertion, substitution, and transposition operations, respectively. Next, ($\widetilde{X}$)  are ranked in the input text embedding based on (L(\u{a},\newtie{b})). Finally, the word-embedded text ($\Xi$) is obtained by extracting the top  (\u{n}) most relevant keywords ($\Xi$).
\begin{equation}
    \Xi=\langle{\Xi_1,\Xi_2,\Xi_3,\Xi_4,......\Xi_f}\rangle    \label{eq:placeholder_label}
\end{equation}
Here, ($f$)  denotes the total number of ($\Xi$). The pseudocode for the DL-KeyBERT is described below, \break

\hrule
\vspace{0.2cm}
\noindent
\textbf{\large Pseudocode for DL-KeyBERT}

\vspace{0.2cm}
\noindent
\textbf{Input:} Collected Text (A) \\
\textbf{Output:} Word Embedded Text (\(\Xi\))

\hrule
\vspace{0.2cm}

\noindent
\textbf{Begin}

\quad \textbf{Initialize} $(\Im)$, \u{a}, \newtie{b}

\quad \textbf{For each} (\(A\)) \textbf{do}

\quad\quad \textbf{Tokenize} (\(A\)) \ to \ ($\widetilde{A}$)

\quad\quad \textbf{Compute} embedded \ vector \  ($\dddot{E}$) \ \#by \ using \ BERT 

\quad\quad \textbf{Extract} (\(X\)) \ from \ ($\dddot{E}$)

\quad\quad \textbf{Generate} ($\widetilde{X}$)

\quad\quad\quad     $\widetilde{X}=\Im(X)$

\quad\quad \textbf{Calculate}\ DL \ distance \ L(\u{a}, \newtie{b})

\quad\quad\quad \textbf{If} max(\u{a}, \newtie{b})

\quad\quad\quad\quad \textbf{Compute} min(\u{a}, \newtie{b})=0

\quad\quad\quad \textbf{Else} 

\quad\quad\quad\quad \textbf{Compute} 

\quad\quad\quad \textbf{End} \ if

\quad\quad \textbf{Rank}\ the \ ($\widetilde{X}$) \ based \ on \ L(\u{a}, \newtie{b})

\quad\quad \textbf{Select}\ top \ keywords

\quad\quad \textbf{Obtain}\ vector \ values \ ($\Xi$)

\quad\quad\quad $\Xi=\langle{\Xi_1,\Xi_2,\Xi_3,\Xi_4,......\Xi_f}\rangle$

\quad \textbf{End} for

\quad \textbf{Return} ($\Xi$)

\noindent
\textbf{End} \break

\hrule
\vspace{0.2cm}


This output is further given as input to the cross-modal query understanding system for training the model to perform well on real-time unseen image caption generation.

\subsection{Cross-Modal Query Understanding System}

In this phase, the CAZSSCL-MPGPT model is trained using ($\vartheta$), ($O$), ($\Xi$), and ($\zeta$)  to generate image captions. The attention-based design of GPT helps the model keep track of context across different inputs, allowing it to understand complex queries that involve multiple types of data. GPT models are trained on massive datasets, which can inadvertently contain real-world biases. This can lead to the generation of biased or discriminatory outputs. To address this, a Mixup regularization scheme is used, thus selecting relevant features and reducing model complexity and bias. Further, the GPT model uses the GELU activation function; but, it is computationally complex and less efficient. To improve efficiency, the Phish activation function is used, thus maintaining linearity for positive inputs and slight non-linearity for negative inputs. The model also incorporates a cross-attention layer and Zero-Shot Learning (ZSL) to analyze the dynamic alignment between multiple modalities and correctly classify unseen data. ZSL enables a model to recognize objects or generate outputs for categories not seen during training. However, ZSL can struggle with semantic gaps between seen and unseen classes, leading to failures in recognizing or classifying unseen classes if their descriptions differ too much from the seen ones. To address this, Semantic Consistency Loss is used, thus aligning the semantic representations of seen and unseen classes for better recognition and classification accuracy. The diagrammatic representation of the proposed CAZSSCL-MPGPT is illustrated in Figure 2.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{image1.png}
    \caption{Diagrammatic Representation of the Proposed CAZSSCL-MPGPT}
    \label{fig:enter-label}
\end{figure}

\textbf{Input Embedding Layer:}
In the training time, initially, ($\vartheta$), ($O$), ($\Xi$), and ($\zeta$) are given as input to the CAZSSCL-MPGPT model and are commonly denoted as (${T}^{in}$). Then, (${T}^{in}$) is transformed into embeddings ($\widetilde{T}^{in}$), and positional encoding ($\overleftrightarrow{\wp}$) is added to provide token order information. The output of the embedding layer  ($\dddot{\Omega}$)  is equated as,
\begin{equation}
    \dddot{\Omega}=(\widetilde{T}^{in}) \oplus \overleftrightarrow{\wp}
    \label{eq:placeholder_label}
\end{equation}
Where, ($\dddot{\Omega}$) has ($o'$)  number of inputs.  Then, dropout is applied to ($\dddot{\Omega}$)  to prevent overfitting, and the output is denoted as ($\Omega$).


\textbf{Mixup Regularization}
The mixup regularization is used to select only the relevant inputs and neglect the irrelevant ones. It is illustrated as,
\begin{equation}
    \widetilde{\Omega}=\upsilon\Omega_{o'}+(1-\upsilon)\Omega_{o'+1}
    \label{eq:placeholder_label}
\end{equation}
\begin{equation}
    \Theta=\upsilon\Theta_{o'}+(1-\upsilon)\Theta_{o'+1}
    \label{eq:placeholder_label}
\end{equation}
Where, ($\widetilde{\Omega}$)  refers to the new input created by mixing the original inputs ($\Omega_{o'}$)  and ($\Omega_{o'+1}$),  ($\Theta$)  illustrates the new label created by mixing the original labels ($\Theta_{o'}$)  and  ($\Theta_{o'+1}$), and ($\upsilon$)  indicates random scalar taken from a Beta distribution, which is the mixing coefficient.

\textbf{Transformer Blocks}
After mixup regularization, the  ($\dddot{t}$) numbers of transformer blocks are computed. They are,

\textit{Layer Normalization}

Next, layer normalization ($\hat{L}(\widetilde{\Omega})$)  is employed to ensure that the output of each layer has a stable distribution, helping the model converge faster.
\begin{equation}
    \hat{L}(\widetilde{\Omega})=\frac{\widetilde{\Omega}-\dddot{\mu}}{\sqrt{\sigma^{m^2} + \epsilon}}
    \label{eq:placeholder_label}
\end{equation}
Here, ($\dddot{\mu}$), ($\sigma^{m^2}$), and ($\epsilon$)  indicates the mean, variance, and standard deviation of ($\widetilde{\Omega}$), correspondingly.

\textit{Cross Attention Layer}

The Cross-Attention Mechanism ($\widetilde{A}$) is crucial in aligning the text features with the image features by computing attention between the visual features (queries ($U$)) and text features (keys ($W$) and values ($\hat{U}$)) derived from ($\hat{L}(\widetilde{\Omega})$).
\begin{equation}
    \widetilde{A}(U,W,\hat{U})=\delta(\frac{UW^\Gamma}{\sqrt{d''_W}})\hat{U}
    \label{eq:placeholder_label}
\end{equation}
Where, ($\Gamma$)  denotes transpose, and ($d''_W$) illustrates the dimensionality keys.

\textit{Add and Norm}
Next, a residual connection is added, followed by layer normalization, to stabilize training and maintain the gradient flow. The normalized output is denoted as ($\breve{\Theta}$).

\textit{Feed Forward Network}
Here, the phish activation function ($\Psi$)  is used to solve the vanishing gradient problems. The feed-forward network output ($\breve{F}$) is given by,
\begin{equation}
    \breve{F}=\Psi(\breve{\Theta}*W^1+\hat{B}^2)*W^2+\hat{B}^2
    \label{eq:placeholder_label}
\end{equation}
\begin{equation}
    \Psi=T^{in}*tanh(GELU(T^{in}))
    \label{eq:placeholder_label}
\end{equation}
Here, ($W^1$) and ($W^2$) indicates the weight matrices, ($\hat{B}^1$) and ($\hat{B}^2$)  illustrates the bias terms, and ($tanh$) and ($GELU$) refers to the hyperbolic tangent and GELU activation functions, respectively.

\textit{Dropout}

Further, dropout is applied to ($\breve{F}$) to prevent overfitting, and the output is notated as ($\overleftrightarrow{F}$).

\textbf{Final Output Layer:}
After completing all the transformer layers, the output undergoes final layer normalization. Then, the output is passed through a linear layer to map it to the size of the vocabulary. Finally, a softmax layer is applied to generate the caption for the image. 
\begin{equation}
    \Xi'''=\delta(Li(\hat{L}(\overleftrightarrow{F})))
    \label{eq:placeholder_label}
\end{equation}
Where, ($\Xi'''$)  indicates the generated captions, ($Li$)  denotes the linear function, and ($\hat{L}(\overleftrightarrow{F})$) refers to the layer normalization performed on ($\overleftrightarrow{F}$).

\textbf{Zero-Shot Semantic Consistency Learning:}
To align the semantic representations of seen and unseen classes and to enable the model to recognize unseen classes, Zero-Shot Semantic Consistency Learning  ($\dddot{V}^{out}$) is utilized. It is given by,
\begin{equation}
    \dddot{V}^{out}=arg \ max \ \text{\newtie{P}}(\ddot{V}|\Xi''')-l_{loss}
    \label{eq:placeholder_label}
\end{equation}
\begin{equation}
    l_{loss}=\frac{1}{\hat{n}}\sum_{\hat{r}=1}^{\hat{n}}(||\widetilde{G}(\Xi'''_{\hat{r}})-\widetilde{T}(\Xi'''_{\hat{r}})||_2^2 + ||\widetilde{T}'''(\Xi'''_{\hat{r}})-\widetilde{T}(\Xi'''_{\hat{r}})||_2^2)
    \label{eq:placeholder_label}
\end{equation}
Where, ($l_{loss}$)  indicates semantic consistency loss, ($\text{\newtie{P}}(\ddot{V}|\Xi''')$)  refers to the probability distribution for the predicted class based on the input, ($\hat{n}$)  represents a total number of batch size, ($\hat{r}$) indicates batch size, ($\widetilde{G}(\Xi'''_{\hat{r}})$)  signifies generated output, ($\widetilde{T}(\Xi'''_{\hat{r}})$)  indicates target representation of the input, and ($\widetilde{T}'''(\Xi'''_{\hat{r}})$)  determines another target representation. Based on this loss function, the final ($\Xi'''$)  is generated. 

\subsection{Testing}
During the testing phase, the images are first pre-processed. Then, objects are segmented using E-YOLO, followed by the generation of object skeletons and the construction of a knowledge graph using CRKG. Afterward, features are extracted from the constructed knowledge graph, generated skeleton objects, and segmented objects. Next, optimal features are selected using FOA. Finally, the trained CAZSSCL-MPGPT model generates the image captions. Similarly, in the VQA process, an image and a question are provided as input. The image undergoes pre-processing, segmentation, and feature selection, and the question is converted into word-embedded text. In this process, for each question, the model provides the correct answer by utilizing the segmented objects, pre-processed images, and optimal features as inputs. Thus, the VQA process accurately generates answers based on the image and question. The performance evaluation of the proposed model is described in the following section.

\section{Results And Discussions}
In this section, the performance of the proposed work is evaluated and compared with existing models to demonstrate its effectiveness. The implementation is carried out using Python.

\subsection{Dataset Description}
The proposed work utilizes the Common Objects in Context (COCO) Dataset 2017 and the visual question answering v2 validation (vqav2-val) dataset to evaluate the efficiency of the proposed model. Both datasets are publicly available and are mentioned in the reference section. The COCO Dataset 2017 is a large-scale dataset, containing 328K images. The vqav2-val dataset includes both text and image data for the VQA task. Here, 80\% of the data is used for training the cross-modal query understanding system, while 20\% is reserved for testing its efficacy.

\subsection{Performance Evaluation}
This subsection compares the performance of the proposed techniques with the existing techniques, highlighting their effectiveness based on key performance metrics.

\subsubsection{Performance Analysis of Contrast Enhancement}
The proposed PG-CLAHE’s performance is analyzed and compared with the existing techniques like CLAHE, Adaptive Histogram Equalization (AHE), Histogram Equalization (HE), and Bilateral Filtering (BF).
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{image3.png}
    \caption{Analysis of MAE, MSE, and RMSE.}
    \label{fig:enter-label}
\end{figure}

Figure 3 illustrates the Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE) for both proposed and existing techniques. The proposed PG-CLAHE attained lower errors with an MAE of 99.87370556, MSE of 76.0186875, and RMSE of 8.718869623. In comparison, existing techniques, including CLAHE, AHE, HE, and BF, had higher errors and attained an average MAE of 146.6632203, MSE of 89.10923906, and RMSE of 9.424607049. The reduced errors in PG-CLAHE result from the Pareto Gini distribution technique for clipping limit adjustment. This enhances contrast more effectively than existing techniques.

\subsubsection{Performance Analysis of Object Segmentation}
The performance of the proposed E-YOLO and the existing techniques are analyzed and compared in terms of Mean Average Precision (MAP), Average Precision (AvP), and IOU.

% Table Example
\begin{table}[h]
    \centering
    \begin{tabular}{lllrrr}
        \toprule
        Techniques & MAP & AvP \\
        \midrule
        Proposed E-YOLO & 0.94563884 & 0.943820261 \\
        YOLO & 0.899554092 & 0.917271493 \\
        FRCNN & 0.899459486 & 0.916879817 \\
        SSD & 0.8855592782 & 0.886889476 \\
        VJ & 0.857678542 & 0.861389579 \\
        \bottomrule
    \end{tabular}
    \caption{MAP and AvP Analysis.}
\end{table}

Table 1 shows the MAP and AvP performance analysis of the proposed E-YOLO and the existing techniques like YOLO, Faster Region-based CNN (FRCNN), Single Shot MultiBox Detector (SSD), and Viola-Jones (VJ). The proposed E-YOLO incorporates the Easom function with overlapping elimination to identify small objects. As a result, it achieved a MAP of 0.94563884 and an AvP of 0.943820261. In contrast, the existing YOLO, FRCNN, SSD, and VJ achieved MAP between 0.857678542 and 0.899554092 and AvP between 0.861389579 and 0.917271493. The higher MAP and AvP of E-YOLO demonstrate its superior object segmentation performance.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{image4.png}
    \caption{IOU Analysis}
    \label{fig:enter-label}
\end{figure}

Figure 4 illustrates the IOU performance of the proposed E-YOLO and existing techniques. The proposed E-YOLO achieved an IOU of 0.948156196, with a high value indicating its strong performance. In comparison, the existing models like YOLO, FRCNN, SSD, and VJ reached IOU values of 0.918258453, 0.90875172, 0.907456583, and 0.852800155, respectively, showing comparatively lower performance. The higher IOU of E-YOLO highlights its superior efficacy in object segmentation.

\subsubsection{Performance Analysis of Knowledge Graph Construction}
Here, the performance analysis of the proposed CRKG and the existing techniques based on Graph Generation Time (GGT) is shown in Figure 5.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{image5.png}
    \caption{Analysis of Graph Generation Time}
    \label{fig:enter-label}
\end{figure}

The proposed CRKG utilized the Conditional Random technique to eliminate biases present in the image and achieved a GGT of 2.079750538s. In comparison, the existing KG, Spatial Relationship Detection (SRD), Affinity propagation (AP), and Minimum Spanning Tree (MST) took GGT of 2.203727722s, 2.406022787s, 2.506807089s, and 2.607764006s, correspondingly. This demonstrates that CRKG outperforms existing techniques in graph generation efficiency.

\subsubsection{Performance Analysis of Cross-Modal Query Understanding System}
The proposed CAZSSCL-MPGPT is compared and analyzed with the existing techniques, such as GPT, BERT, Bidirectional and Auto-Regressive Transformer (BART), and T5 using the COCO Dataset 2017 and vqav2-val Dataset. 

\textbf{COCO Dataset 2017}
In Figure 6, the performance analysis of the proposed CAZSSCL-MPGPT and existing techniques is validated on the COCO dataset 2017 in terms of F1-score and specificity.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{image6.png}
    \caption{Performance Analysis of F1-Score and Specificity}
    \label{fig:enter-label}
\end{figure}

The proposed CAZSSCL-MPGPT achieved an F1-score of 99.14277995\% and a specificity of 99.15805022\%, outperforming the existing models, such as GPT, BERT, BART, and T5, which attained average F1-score and specificity values of 91.77962625\% and 91.75406204\%, respectively. This improvement is due to the use of mixup regularization that selects relevant features, and the Phish activation function boosts efficiency. Cross-attention and zero-shot learning align modalities and classify unseen data. Thus, the proposed CAZSSCL-MPGPT demonstrates superior performance in caption generation.

\begin{table}[h]
    \centering
    \begin{tabular}{lllrrr}
        \toprule
        Techniques &	Accuracy (\%) &	Precision (\%)	& BLEU & 	METEOR \\
Proposed CAZSSCL-MPGPT	& 99.14187362 &	99.15982902	& 0.991598 &	0.991428 \\
GPT &	96.05851979	& 96.09732121 &	0.960973 &	0.960619 \\
BERT &	93.62183428 &	93.5782967 &	0.935783 &	0.936334 \\
BART &	90.23604623 &	90.2381537 &	0.902382 &	0.902492 \\
T5	& 87.16006885 &	87.18679375 &	0.871868 &	0.871739 \\
        \bottomrule
    \end{tabular}
    \caption{Accuracy and Precision, BLEU, and METEOR Analysis.}
\end{table}

Table 2 shows the accuracy, precision, Bilingual Evaluation Understudy (BLEU), and Metric for Evaluation of Translation with Explicit Ordering (METEOR) analysis of the proposed CAZSSCL-MPGPT and existing techniques. The proposed CAZSSCL-MPGPT achieved an accuracy of 99.14187362\% and a precision of 99.15982902\%. In contrast, existing techniques, such as GPT, BERT, BART, and T5 demonstrated lower performance in both accuracy and precision. Likewise, the proposed CAZSSCL-MPGPT achieved BLEU and METEOR of 0.991598 and 0.991428, respectively. However, the existing GPT attained a BLEU of 0.960973 and T5 attained a METEOR of 0.871739. This demonstrates the superior performance of CAZSSCL-MPGPT in generating captions using the COCO dataset 2017.

\textbf{vqav2-val Dataset}
Figure 7 and Table 3 demonstrate the performance of the proposed CAZSSCL-MPGPT and the existing techniques in terms of False Positive Rate (FPR), False Negative Rate (FNR), accuracy, precision, BLEU, and METEOR for the vqav2-val Dataset.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{image7.png}
    \caption{Analysis of FPR and FNR }
    \label{fig:enter-label}
\end{figure}


% Table Example
\begin{table}[h]
    \centering
    \begin{tabular}{lllrrr}
        \toprule
Techniques	& Accuracy (\%) &	Precision (\%)	& BLEU &	METEOR \\
Proposed CAZSSCL-MPGPT	& 98.43224393 &	98.39792963	& 0.983979296 &	0.984343216 \\
GPT	& 94.56531803 &	94.46905377 &	0.944690538 &	0.945766992 \\
BERT &	94.56531803	 & 94.46905377&	0.944690538&	0.945766992\\
BART&	0.919205012	&0.918353535&	0.918353535	&0.919371728\\
T5	&88.56587353&	88.44168613&	0.884416861&	0.885968422\\

        \bottomrule
    \end{tabular}
    \caption{Analysis of Accuracy, Precision, BLEU, and METEOR.}
\end{table}

The proposed CAZSSCL-MPGPT achieved a low FPR of 0.016063265 and a low FNR of 0.015292594, while existing techniques like GPT, BERT, BART, and T5 had higher average FPR and FNR values of 0.077319906 and 0.074597644, respectively. Moreover, the proposed CAZSSCL-MPGPT attained the accuracy, precision, BLEU, and METEOR of 98.43224393\%, 98.39792963\%, 0.983979296, and 0.984343216, respectively, outperforming the existing techniques. In contrast, the existing techniques like GPT, BERT, BART, and T5 achieved lower average accuracy, precision, BLEU, and METEOR of 69.65392865\%, 69.5745368\%, 0.923037868, and 0.924218533, respectively. This clearly demonstrates that the proposed CAZSSCL-MPGPT outperforms caption generation over the existing techniques.

\subsection{Comparative Analysis}
The efficacy of the proposed system is demonstrated by a comparison with related works.

% Table Example
\begin{table}[h]
    \centering
    \begin{tabular}{lllrrr}
        \toprule
        Author’s Name&	Technique/Method Used&	METEOR&	Drawbacks\\
Proposed Framework	&CAZSSCL-MPGPT&	0.991428&	The model didn’t concentrate on domain-specific contexts.\\
(Nursikuwagus et al., 2024)& SNN and LSTM&	0.670&	Ineffective for non-linearly separable data.\\
(Xiang et al., 2023)&	EVSD&	25.8&	Potential bias affected model generalizability.\\
(Im \& Chan, 2023)&	CNN-to-Bi-CARU	&31.23&	Lack of additional feature incorporation.\\
(Duhayyim et al., 2022)&	BiGRU&	30.00&	Bi-GRUs required significant computational resources.\\
(Iwamura et al., 2021)	&Motion-CNN	&26.7&	Object detection failures degraded performance.\\

        \bottomrule
    \end{tabular}
    \caption{Comparative Analysis with Related Works.}
\end{table}

In Table 4, the comparative analysis of the proposed CAZSSCL-MPGPT framework with existing techniques is evaluated on the COCO dataset. The existing techniques, such as the hybrid SNN with LSTM and EVSD have METEOR scores of 0.670 and 25.8, respectively. Also, they face challenges in providing accurate captions due to inefficiencies and biases. The CNN-to-Bi-CARU model and BiGRU-based systems, with METEOR scores of 31.23 and 30.00, respectively, struggle to fully capture contextual relationships between image content and textual descriptions. Models like Motion-CNN also face difficulty in object detection, resulting in a METEOR score of 26.7. In contrast, the proposed CAZSSCL-MPGPT framework outperforms these techniques with a METEOR score of 0.991428 on the COCO dataset. This higher score confirms its effectiveness, making it a strong choice for cross-modal query understanding.

\section{Conclusion}
An efficient LLM for the cross-modal query understanding system using DL-KeyBERT-based CAZSSCL-MPGPT is proposed in this framework. The experimental results show that the proposed CRKG achieves a GGT of 2.079750538s for knowledge graph construction. Similarly, the proposed PG-CLAHE achieved MAE and MSE values of 99.87370556 and 76.0186875, respectively. Additionally, the proposed E-YOLO segmented objects with an IOU of 0.948156196. The proposed CAZSSCL-MPGPT demonstrated outstanding performance in caption generation and VQA on both the COCO dataset 2017 and the vqav2-val dataset. Specifically, it achieved an accuracy of 99.14187362\%, BLEU of 0.983979296, and METEOR of 0.984343216 for the COCO 2017 dataset. Then, for the vqav2-val dataset, the model attained an accuracy of 98.43224393\% with BLEU scores of 0.983979296 and METEOR scores of 0.984343216. These results underscore the high performance and robustness of the proposed system in both caption generation and visual question-answering tasks.

\textbf{Future Scope}
Although the proposed cross-modal query understanding system generates captions efficiently, it does not focus on domain-specific contexts. In the future, the system can be enhanced by concentrating on domain-based datasets, such as healthcare, education, travel, and hospitality, along with integrating cultural events and polarity considerations.

\begin{thebibliography}{00}

\bibitem{b1} https://www.kaggle.com/datasets/sabahesaraki/2017-2017

\bibitem{b2} https://www.kaggle.com/datasets/aniketvp68/vqav2-val

\bibitem{b3} Chen, J., Liu, Z., Huang, X., Wu, C., Liu, Q., Jiang, G., Pu, Y., Lei, Y., Chen, X., Wang, X., Zheng, K., Lian, D., \& Chen, E. (2024). When large language models meet personalization: perspectives of challenges and opportunities. World Wide Web, 27(4), 1–45. https://doi.org/10.1007/s11280-024-01276-1

\bibitem{b4} Chen, Y., Huang, R., Chang, H., Tan, C., Xue, T., \& Ma, B. (2021). Cross-Modal Knowledge Adaptation for Language-Based Person Search. IEEE Transactions on Image Processing, 30, 1–13. https://doi.org/10.1109/TIP.2021.3068825

\bibitem{b5} Chen, Z., Chen, J., Geng, Y., Pan, J. Z., Yuan, Z., \& Chen, H. (2021a). Zero-Shot Visual Question Answering Using Knowledge Graph. The Semantic Web–ISWC 2021: 20th International Semantic Web Conference, 1–17. https://doi.org/10.1007/978-3-030-88361-4\_9 

\bibitem{b6} Cheng, Q., Zhou, Y., Fu, P., Xu, Y., \& Zhang, L. (2021). A Deep Semantic Alignment Network for the Cross-Modal Image-Text Retrieval in Remote Sensing. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 14, 1–14. https://doi.org/10.1109/JSTARS.2021.3070872 

\bibitem{b7} Dong, X., Liu, L., Zhu, L., Nie, L., \& Zhang, H. (2021). Adversarial Graph Convolutional Network for Cross-Modal Retrieval. IEEE Transactions on Circuits and Systems for Video Technology, 32(3), 1–13. https://doi.org/10.1109/TCSVT.2021.3075242 

\bibitem{b8} Duhayyim, M. Al, Alazwari, S., Mengash, H. A., Marzouk, R., Alzahrani, J. S., Mahgoub, H., Althukair, F., \& Salama, A. S. (2022). Metaheuristics Optimization with Deep Learning Enabled Automated Image Captioning System. Applied Sciences, 12(15), 1–18. https://doi.org/10.3390/app12157724 

\bibitem{b9} Fan, Y., Cao, Y., Zhao, Z., Liu, Z., \& Li, S. (2024). Unbridled Icarus: A Survey of the Potential Perils of Image Inputs in Multimodal Large Language Model Security. 2024 IEEE International Conference on Systems, Man, and Cybernetics, 1–8. http://arxiv.org/abs/2404.05264 

\bibitem{b10} Im, S. K., \& Chan, K. H. (2023). Context-Adaptive-Based Image Captioning by Bi-CARU. IEEE Access, 11, 1–10. https://doi.org/10.1109/ACCESS.2023.3302512 

\bibitem{b11} Iwamura, K., Kasahara, J. Y. L., Moro, A., Yamashita, A., \& Asama, H. (2021). Image Captioning Using Motion-CNN with Object Detection. Sensors, 21(4), 1–13. https://doi.org/10.3390/s21041270 

\bibitem{} Kalyan, K. S. (2024). A survey of GPT-3 family large language models including ChatGPT and GPT-4. Natural Language Processing Journal, 6, 1–48. https://doi.org/10.1016/j.nlp.2023.100048 

\bibitem{} Li, Y., Hu, B., Chen, X., Ma, L., Xu, Y., \& Zhang, M. (2024). LMEye: An Interactive Perception Network for Large Language Models. IEEE Transactions on Multimedia, 26, 1–13. https://doi.org/10.1109/TMM.2024.3428317 

\bibitem{} Lim, Q. Z., Lee, C. P., Lim, K. M., \& Samingan, A. K. (2024). UniRaG: Unification, Retrieval, and Generation for Multimodal Question Answering With Pre-Trained Language Models. IEEE Access, 12, 1–15. https://doi.org/10.1109/ACCESS.2024.3403101 

\bibitem{} Liu, P., Ren, Y., Tao, J., \& Ren, Z. (2024). GIT-Mol: A multi-modal large language model for molecular science with graph, image, and text. Computers in Biology and Medicine, 171, 1–14. https://doi.org/10.1016/j.compbiomed.2024.108073 

\bibitem{} Liu, Y., Li, G., \& Lin, L. (2023). Cross-Modal Causal Relational Reasoning for Event-Level Visual Question Answering. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(10), 1–18. https://doi.org/10.1109/TPAMI.2023.3284038 

\bibitem{} Lu, H., Huo, Y., Ding, M., Fei, N., \& Lu, Z. (2023). Cross-modal Contrastive Learning for Generalizable and Efficient Image-text Retrieval. Machine Intelligence Research, 20(4), 1–15. https://doi.org/10.1007/s11633-022-1386-4 

\bibitem{} Mashrur, A., Luo, W., Zaidi, N. A., \& Robles-Kelly, A. (2024). Robust visual question answering via semantic cross modal augmentation. Computer Vision and Image Understanding, 238, 1–9. https://doi.org/10.1016/j.cviu.2023.103862 

\bibitem{} Messina, N., Amato, G., Falchi, F., Gennaro, C., \& Marchand-Maillet, S. (2021). Towards Efficient Cross-Modal Visual Textual Retrieval using Transformer-Encoder Deep Features. Proceedings - International Workshop on Content-Based Multimedia Indexing, 1–6. https://doi.org/10.1109/CBMI50038.2021.9461890  

\bibitem{} Nursikuwagus, A., Munir, R., Khodra, M. L., \& Dewi, D. A. (2024). Model Semantic Attention (SemAtt) With Hybrid Learning Separable Neural Network and Long Short-Term Memory to Generate Caption. IEEE Access, 12, 1–15. https://doi.org/10.1109/ACCESS.2024.3481499 

\bibitem{} Salaberria, A., Azkune, G., Lacalle, O. L. de, Soroa, A., \& Agirre, E. (2023). Image captioning for effective use of language models in knowledge-based visual question answering. Expert Systems with Applications, 212, 1–10. https://doi.org/10.1016/j.eswa.2022.118669 

\bibitem{} Sun, Z., Zhao, M., Liu, G., \& Kaup, A. (2021). Cross-Modal Pre-Aligned Method with Global and Local Information for Remote-Sensing Image and Text Retrieval. IEEE Transactions on Geoscience and Remote Sensing, 14(8), 1–18. https://doi.org/10.1109/TGRS.2024.3489224 

\bibitem{} Tingting, W., Weilin, Y., Junren, L., Wanpeng, Z., \& Lina, L. (2023). VLCA: vision-language aligning model with cross-modal attention for bilingual remote sensing image captioning. Journal of Systems Engineering and Electronics, 34(1), 1–10. https://doi.org/10.23919/JSEE.2023.000035 

\bibitem{} Verma, A., Yadav, A. K., Kumar, M., \& Yadav, D. (2022). Automatic image caption generation using deep learning. Multimedia Tools and Applications, 83(2), 1–15. https://doi.org/10.1063/5.0225620 

\bibitem{} Wang, T., Li, F., Zhu, L., Li, J., Zhang, Z., \& Shen, H. T. (2024). Cross-Modal Retrieval: A Systematic Review of Methods and Future Directions. Proceedings of the IEEE, 1–35. https://doi.org/10.1109/JPROC.2024.3525147 

\bibitem{} Xiang, N., Chen, L., Liang, L., Rao, X., \& Gong, Z. (2023). Semantic-Enhanced Cross-Modal Fusion for Improved Unsupervised Image Captioning. Electronics, 12(17), 1–16. https://doi.org/10.3390/electronics12173549 

\bibitem{} Xu, Z., Zhang, Y., Xie, E., Zhao, Z., Guo, Y., Wong, K. Y. K., Li, Z., \& Zhao, H. (2024). DriveGPT4: Interpretable End-to-End Autonomous Driving Via Large Language Model. IEEE Robotics and Automation Letters, 1–8. https://doi.org/10.1109/LRA.2024.3440097 

\bibitem{} Ye, Q., Yu, Z., Shao, R., Xie, X., Torr, P., \& Cao, X. (2024). CAT: Enhancing Multimodal Large Language Model to Answer Questions in Dynamic Audio-Visual Scenarios. European Conference on Computer Vision, 1–19. https://doi.org/10.1007/978-3-031-72684-2\_9 

\bibitem{} Zhang, J., Wang, L., Zheng, F., Wang, X., \& Zhang, H. (2024a). An Enhanced Feature Extraction Framework for Cross-Modal Image–Text Retrieval. Remote Sensing, 16(12), 1–18. https://doi.org/10.3390/rs16122201 

\bibitem{} Zhang, L., Wu, H., Chen, Q., Deng, Y., Siebert, J., Li, Z., Han, Y., Kong, D., \& Cao, Z. (2021). VLDeformer: Vision–Language Decomposed Transformer for fast cross-modal retrieval. Knowledge-Based Systems, 252, 1–10. https://doi.org/10.1016/j.knosys.2022.109316 

\bibitem{} Zhang, W., Cai, M., Zhang, T., Zhuang, Y., \& Mao, X. (2024). EarthGPT: A Universal Multi-modal Large Language Model for Multi-sensor Image Comprehension in Remote Sensing Domain. IEEE Transactions on Geoscience and Remote Sensing, 1–17. https://doi.org/10.1109/TGRS.2024.3409624 

\bibitem{} Zhu, M., Shen, D., Xu, L., \& Wang, X. (2021). Scalable Multi-grained Cross-modal Similarity Query with Interpretability. Data Science and Engineering, 6(3), 1–14. https://doi.org/10.1007/s41019-021-00162-4 

\end{thebibliography}

\end{document}


\section{This is an example for first level head---section head}\label{sec3}

\subsection{This is an example for second level head---subsection head}\label{subsec2}

\subsubsection{This is an example for third level head---subsubsection head}\label{subsubsec2}

Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. 

\section{Equations}\label{sec4}

Equations in \LaTeX\ can either be inline or on-a-line by itself (``display equations''). For
inline equations use the \verb+$...$+ commands. E.g.: The equation
$H\psi = E \psi$ is written via the command \verb+$H \psi = E \psi$+.

For display equations (with auto generated equation numbers)
one can use the equation or align environments:
\begin{equation}
\|\tilde{X}(k)\|^2 \leq\frac{\sum\limits_{i=1}^{p}\left\|\tilde{Y}_i(k)\right\|^2+\sum\limits_{j=1}^{q}\left\|\tilde{Z}_j(k)\right\|^2 }{p+q}.\label{eq1}
\end{equation}
where,
\begin{align}
D_\mu &=  \partial_\mu - ig \frac{\lambda^a}{2} A^a_\mu \nonumber \\
F^a_{\mu\nu} &= \partial_\mu A^a_\nu - \partial_\nu A^a_\mu + g f^{abc} A^b_\mu A^a_\nu \label{eq2}
\end{align}
Notice the use of \verb+\nonumber+ in the align environment at the end
of each line, except the last, so as not to produce equation numbers on
lines where no equation numbers are required. The \verb+\label{}+ command
should only be used at the last line of an align environment where
\verb+\nonumber+ is not used.
\begin{equation}
Y_\infty = \left( \frac{m}{\textrm{GeV}} \right)^{-3}
    \left[ 1 + \frac{3 \ln(m/\textrm{GeV})}{15}
    + \frac{\ln(c_2/5)}{15} \right]
\end{equation}
The class file also supports the use of \verb+\mathbb{}+, \verb+\mathscr{}+ and
\verb+\mathcal{}+ commands. As such \verb+\mathbb{R}+, \verb+\mathscr{R}+
and \verb+\mathcal{R}+ produces $\mathbb{R}$, $\mathscr{R}$ and $\mathcal{R}$
respectively (refer Subsubsection~\ref{subsubsec2}).

\section{Tables}\label{sec5}

Tables can be inserted via the normal table and tabular environment. To put
footnotes inside tables you should use \verb+\footnotetext[]{...}+ tag.
The footnote appears just below the table itself (refer Tables~\ref{tab1} and \ref{tab2}). 
For the corresponding footnotemark use \verb+\footnotemark[...]+

\begin{table}[h]
\caption{Caption text}\label{tab1}%
\begin{tabular}{@{}llll@{}}
\toprule
Column 1 & Column 2  & Column 3 & Column 4\\
\midrule
row 1    & data 1   & data 2  & data 3  \\
row 2    & data 4   & data 5\footnotemark[1]  & data 6  \\
row 3    & data 7   & data 8  & data 9\footnotemark[2]  \\
\botrule
\end{tabular}
\footnotetext{Source: This is an example of table footnote. This is an example of table footnote.}
\footnotetext[1]{Example for a first table footnote. This is an example of table footnote.}
\footnotetext[2]{Example for a second table footnote. This is an example of table footnote.}
\end{table}

\noindent
The input format for the above table is as follows:

%%=============================================%%
%% For presentation purpose, we have included  %%
%% \bigskip command. Please ignore this.       %%
%%=============================================%%
\bigskip
\begin{verbatim}
\begin{table}[<placement-specifier>]
\caption{<table-caption>}\label{<table-label>}%
\begin{tabular}{@{}llll@{}}
\toprule
Column 1 & Column 2 & Column 3 & Column 4\\
\midrule
row 1 & data 1 & data 2	 & data 3 \\
row 2 & data 4 & data 5\footnotemark[1] & data 6 \\
row 3 & data 7 & data 8	 & data 9\footnotemark[2]\\
\botrule
\end{tabular}
\footnotetext{Source: This is an example of table footnote. 
This is an example of table footnote.}
\footnotetext[1]{Example for a first table footnote.
This is an example of table footnote.}
\footnotetext[2]{Example for a second table footnote. 
This is an example of table footnote.}
\end{table}
\end{verbatim}
\bigskip
%%=============================================%%
%% For presentation purpose, we have included  %%
%% \bigskip command. Please ignore this.       %%
%%=============================================%%

\begin{table}[h]
\caption{Example of a lengthy table which is set to full textwidth}\label{tab2}
\begin{tabular*}{\textwidth}{@{\extracolsep\fill}lcccccc}
\toprule%
& \multicolumn{3}{@{}c@{}}{Element 1\footnotemark[1]} & \multicolumn{3}{@{}c@{}}{Element 2\footnotemark[2]} \\\cmidrule{2-4}\cmidrule{5-7}%
Project & Energy & $\sigma_{calc}$ & $\sigma_{expt}$ & Energy & $\sigma_{calc}$ & $\sigma_{expt}$ \\
\midrule
Element 3  & 990 A & 1168 & $1547\pm12$ & 780 A & 1166 & $1239\pm100$\\
Element 4  & 500 A & 961  & $922\pm10$  & 900 A & 1268 & $1092\pm40$\\
\botrule
\end{tabular*}
\footnotetext{Note: This is an example of table footnote. This is an example of table footnote this is an example of table footnote this is an example of~table footnote this is an example of table footnote.}
\footnotetext[1]{Example for a first table footnote.}
\footnotetext[2]{Example for a second table footnote.}
\end{table}

In case of double column layout, tables which do not fit in single column width should be set to full text width. For this, you need to use \verb+\begin{table*}+ \verb+...+ \verb+\end{table*}+ instead of \verb+\begin{table}+ \verb+...+ \verb+\end{table}+ environment. Lengthy tables which do not fit in textwidth should be set as rotated table. For this, you need to use \verb+\begin{sidewaystable}+ \verb+...+ \verb+\end{sidewaystable}+ instead of \verb+\begin{table*}+ \verb+...+ \verb+\end{table*}+ environment. This environment puts tables rotated to single column width. For tables rotated to double column width, use \verb+\begin{sidewaystable*}+ \verb+...+ \verb+\end{sidewaystable*}+.

\begin{sidewaystable}
\caption{Tables which are too long to fit, should be written using the ``sidewaystable'' environment as shown here}\label{tab3}
\begin{tabular*}{\textheight}{@{\extracolsep\fill}lcccccc}
\toprule%
& \multicolumn{3}{@{}c@{}}{Element 1\footnotemark[1]}& \multicolumn{3}{@{}c@{}}{Element\footnotemark[2]} \\\cmidrule{2-4}\cmidrule{5-7}%
Projectile & Energy	& $\sigma_{calc}$ & $\sigma_{expt}$ & Energy & $\sigma_{calc}$ & $\sigma_{expt}$ \\
\midrule
Element 3 & 990 A & 1168 & $1547\pm12$ & 780 A & 1166 & $1239\pm100$ \\
Element 4 & 500 A & 961  & $922\pm10$  & 900 A & 1268 & $1092\pm40$ \\
Element 5 & 990 A & 1168 & $1547\pm12$ & 780 A & 1166 & $1239\pm100$ \\
Element 6 & 500 A & 961  & $922\pm10$  & 900 A & 1268 & $1092\pm40$ \\
\botrule
\end{tabular*}
\footnotetext{Note: This is an example of table footnote this is an example of table footnote this is an example of table footnote this is an example of~table footnote this is an example of table footnote.}
\footnotetext[1]{This is an example of table footnote.}
\end{sidewaystable}

\section{Figures}\label{sec6}

As per the \LaTeX\ standards you need to use eps images for \LaTeX\ compilation and \verb+pdf/jpg/png+ images for \verb+PDFLaTeX+ compilation. This is one of the major difference between \LaTeX\ and \verb+PDFLaTeX+. Each image should be from a single input .eps/vector image file. Avoid using subfigures. The command for inserting images for \LaTeX\ and \verb+PDFLaTeX+ can be generalized. The package used to insert images in \verb+LaTeX/PDFLaTeX+ is the graphicx package. Figures can be inserted via the normal figure environment as shown in the below example:

%%=============================================%%
%% For presentation purpose, we have included  %%
%% \bigskip command. Please ignore this.       %%
%%=============================================%%
\bigskip
\begin{verbatim}
\begin{figure}[<placement-specifier>]
\centering
\includegraphics{<eps-file>}
\caption{<figure-caption>}\label{<figure-label>}
\end{figure}
\end{verbatim}
\bigskip
%%=============================================%%
%% For presentation purpose, we have included  %%
%% \bigskip command. Please ignore this.       %%
%%=============================================%%

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{fig.eps}
\caption{This is a widefig. This is an example of long caption this is an example of long caption  this is an example of long caption this is an example of long caption}\label{fig1}
\end{figure}

In case of double column layout, the above format puts figure captions/images to single column width. To get spanned images, we need to provide \verb+\begin{figure*}+ \verb+...+ \verb+\end{figure*}+.

For sample purpose, we have included the width of images in the optional argument of \verb+\includegraphics+ tag. Please ignore this. 

\section{Algorithms, Program codes and Listings}\label{sec7}

Packages \verb+algorithm+, \verb+algorithmicx+ and \verb+algpseudocode+ are used for setting algorithms in \LaTeX\ using the format:

%%=============================================%%
%% For presentation purpose, we have included  %%
%% \bigskip command. Please ignore this.       %%
%%=============================================%%
\bigskip
\begin{verbatim}
\begin{algorithm}
\caption{<alg-caption>}\label{<alg-label>}
\begin{algorithmic}[1]
. . .
\end{algorithmic}
\end{algorithm}
\end{verbatim}
\bigskip
%%=============================================%%
%% For presentation purpose, we have included  %%
%% \bigskip command. Please ignore this.       %%
%%=============================================%%

You may refer above listed package documentations for more details before setting \verb+algorithm+ environment. For program codes, the ``verbatim'' package is required and the command to be used is \verb+\begin{verbatim}+ \verb+...+ \verb+\end{verbatim}+. 

Similarly, for \verb+listings+, use the \verb+listings+ package. \verb+\begin{lstlisting}+ \verb+...+ \verb+\end{lstlisting}+ is used to set environments similar to \verb+verbatim+ environment. Refer to the \verb+lstlisting+ package documentation for more details.

A fast exponentiation procedure:

\lstset{texcl=true,basicstyle=\small\sf,commentstyle=\small\rm,mathescape=true,escapeinside={(*}{*)}}
\begin{lstlisting}
begin
  for $i:=1$ to $10$ step $1$ do
      expt($2,i$);  
      newline() od                (*\textrm{Comments will be set flush to the right margin}*)
where
proc expt($x,n$) $\equiv$
  $z:=1$;
  do if $n=0$ then exit fi;
     do if odd($n$) then exit fi;                 
        comment: (*\textrm{This is a comment statement;}*)
        $n:=n/2$; $x:=x*x$ od;
     { $n>0$ };
     $n:=n-1$; $z:=z*x$ od;
  print($z$). 
end
\end{lstlisting}

\begin{algorithm}
\caption{Calculate $y = x^n$}\label{algo1}
\begin{algorithmic}[1]
\Require $n \geq 0 \vee x \neq 0$
\Ensure $y = x^n$ 
\State $y \Leftarrow 1$
\If{$n < 0$}\label{algln2}
        \State $X \Leftarrow 1 / x$
        \State $N \Leftarrow -n$
\Else
        \State $X \Leftarrow x$
        \State $N \Leftarrow n$
\EndIf
\While{$N \neq 0$}
        \If{$N$ is even}
            \State $X \Leftarrow X \times X$
            \State $N \Leftarrow N / 2$
        \Else[$N$ is odd]
            \State $y \Leftarrow y \times X$
            \State $N \Leftarrow N - 1$
        \EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}

%%=============================================%%
%% For presentation purpose, we have included  %%
%% \bigskip command. Please ignore this.       %%
%%=============================================%%
\bigskip
\begin{minipage}{\hsize}%
\lstset{frame=single,framexleftmargin=-1pt,framexrightmargin=-17pt,framesep=12pt,linewidth=0.98\textwidth,language=pascal}% Set your language (you can change the language for each code-block optionally)
%%% Start your code-block
\begin{lstlisting}
for i:=maxint to 0 do
begin
{ do nothing }
end;
Write('Case insensitive ');
Write('Pascal keywords.');
\end{lstlisting}
\end{minipage}

\section{Cross referencing}\label{sec8}

Environments such as figure, table, equation and align can have a label
declared via the \verb+\label{#label}+ command. For figures and table
environments use the \verb+\label{}+ command inside or just
below the \verb+\caption{}+ command. You can then use the
\verb+\ref{#label}+ command to cross-reference them. As an example, consider
the label declared for Figure~\ref{fig1} which is
\verb+\label{fig1}+. To cross-reference it, use the command 
\verb+Figure \ref{fig1}+, for which it comes up as
``Figure~\ref{fig1}''. 

To reference line numbers in an algorithm, consider the label declared for the line number 2 of Algorithm~\ref{algo1} is \verb+\label{algln2}+. To cross-reference it, use the command \verb+\ref{algln2}+ for which it comes up as line~\ref{algln2} of Algorithm~\ref{algo1}.

\subsection{Details on reference citations}\label{subsec7}

Standard \LaTeX\ permits only numerical citations. To support both numerical and author-year citations this template uses \verb+natbib+ \LaTeX\ package. For style guidance please refer to the template user manual.

Here is an example for \verb+\cite{...}+: \cite{bib1}. Another example for \verb+\citep{...}+: \citep{bib2}. For author-year citation mode, \verb+\cite{...}+ prints Jones et al. (1990) and \verb+\citep{...}+ prints (Jones et al., 1990).

All cited bib entries are printed at the end of this article: \cite{bib3}, \cite{bib4}, \cite{bib5}, \cite{bib6}, \cite{bib7}, \cite{bib8}, \cite{bib9}, \cite{bib10}, \cite{bib11}, \cite{bib12} and \cite{bib13}.


\section{Examples for theorem like environments}\label{sec10}

For theorem like environments, we require \verb+amsthm+ package. There are three types of predefined theorem styles exists---\verb+thmstyleone+, \verb+thmstyletwo+ and \verb+thmstylethree+ 

%%=============================================%%
%% For presentation purpose, we have included  %%
%% \bigskip command. Please ignore this.       %%
%%=============================================%%
\bigskip
\begin{tabular}{|l|p{19pc}|}
\hline
\verb+thmstyleone+ & Numbered, theorem head in bold font and theorem text in italic style \\\hline
\verb+thmstyletwo+ & Numbered, theorem head in roman font and theorem text in italic style \\\hline
\verb+thmstylethree+ & Numbered, theorem head in bold font and theorem text in roman style \\\hline
\end{tabular}
\bigskip
%%=============================================%%
%% For presentation purpose, we have included  %%
%% \bigskip command. Please ignore this.       %%
%%=============================================%%

For mathematics journals, theorem styles can be included as shown in the following examples:

\begin{theorem}[Theorem subhead]\label{thm1}
Example theorem text. Example theorem text. Example theorem text. Example theorem text. Example theorem text. 
Example theorem text. Example theorem text. Example theorem text. Example theorem text. Example theorem text. 
Example theorem text. 
\end{theorem}

Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text.

\begin{proposition}
Example proposition text. Example proposition text. Example proposition text. Example proposition text. Example proposition text. 
Example proposition text. Example proposition text. Example proposition text. Example proposition text. Example proposition text. 
\end{proposition}

Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text.

\begin{example}
Phasellus adipiscing semper elit. Proin fermentum massa
ac quam. Sed diam turpis, molestie vitae, placerat a, molestie nec, leo. Maecenas lacinia. Nam ipsum ligula, eleifend
at, accumsan nec, suscipit a, ipsum. Morbi blandit ligula feugiat magna. Nunc eleifend consequat lorem. 
\end{example}

Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text.

\begin{remark}
Phasellus adipiscing semper elit. Proin fermentum massa
ac quam. Sed diam turpis, molestie vitae, placerat a, molestie nec, leo. Maecenas lacinia. Nam ipsum ligula, eleifend
at, accumsan nec, suscipit a, ipsum. Morbi blandit ligula feugiat magna. Nunc eleifend consequat lorem. 
\end{remark}

Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text.

\begin{definition}[Definition sub head]
Example definition text. Example definition text. Example definition text. Example definition text. Example definition text. Example definition text. Example definition text. Example definition text. 
\end{definition}

Additionally a predefined ``proof'' environment is available: \verb+\begin{proof}+ \verb+...+ \verb+\end{proof}+. This prints a ``Proof'' head in italic font style and the ``body text'' in roman font style with an open square at the end of each proof environment. 

\begin{proof}
Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. 
\end{proof}

Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text.

\begin{proof}[Proof of Theorem~{\upshape\ref{thm1}}]
Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. 
\end{proof}

\noindent
For a quote environment, use \verb+\begin{quote}...\end{quote}+
\begin{quote}
Quoted text example. Aliquam porttitor quam a lacus. Praesent vel arcu ut tortor cursus volutpat. In vitae pede quis diam bibendum placerat. Fusce elementum
convallis neque. Sed dolor orci, scelerisque ac, dapibus nec, ultricies ut, mi. Duis nec dui quis leo sagittis commodo.
\end{quote}

Sample body text. Sample body text. Sample body text. Sample body text. Sample body text (refer Figure~\ref{fig1}). Sample body text. Sample body text. Sample body text (refer Table~\ref{tab3}). 

\section{Methods}\label{sec11}

Topical subheadings are allowed. Authors must ensure that their Methods section includes adequate experimental and characterization data necessary for others in the field to reproduce their work. Authors are encouraged to include RIIDs where appropriate. 

\textbf{Ethical approval declarations} (only required where applicable) Any article reporting experiment/s carried out on (i)~live vertebrate (or higher invertebrates), (ii)~humans or (iii)~human samples must include an unambiguous statement within the methods section that meets the following requirements: 

\begin{enumerate}[1.]
\item Approval: a statement which confirms that all experimental protocols were approved by a named institutional and/or licensing committee. Please identify the approving body in the methods section

\item Accordance: a statement explicitly saying that the methods were carried out in accordance with the relevant guidelines and regulations

\item Informed consent (for experiments involving humans or human tissue samples): include a statement confirming that informed consent was obtained from all participants and/or their legal guardian/s
\end{enumerate}

If your manuscript includes potentially identifying patient/participant information, or if it describes human transplantation research, or if it reports results of a clinical trial then  additional information will be required. Please visit (\url{https://www.nature.com/nature-research/editorial-policies}) for Nature Portfolio journals, (\url{https://www.springer.com/gp/authors-editors/journal-author/journal-author-helpdesk/publishing-ethics/14214}) for Springer Nature journals, or (\url{https://www.biomedcentral.com/getpublished/editorial-policies\#ethics+and+consent}) for BMC.

\section{Discussion}\label{sec12}

Discussions should be brief and focused. In some disciplines use of Discussion or `Conclusion' is interchangeable. It is not mandatory to use both. Some journals prefer a section `Results and Discussion' followed by a section `Conclusion'. Please refer to Journal-level guidance for any specific requirements. 

\section{Conclusion}\label{sec13}

Conclusions may be used to restate your hypothesis or research question, restate your major findings, explain the relevance and the added value of your work, highlight any limitations of your study, describe future directions for research and recommendations. 

In some disciplines use of Discussion or 'Conclusion' is interchangeable. It is not mandatory to use both. Please refer to Journal-level guidance for any specific requirements. 

\backmatter

\bmhead{Supplementary information}

If your article has accompanying supplementary file/s please state so here. 

Authors reporting data from electrophoretic gels and blots should supply the full unprocessed scans for key as part of their Supplementary information. This may be requested by the editorial team/s if it is missing.

Please refer to Journal-level guidance for any specific requirements.

\bmhead{Acknowledgements}

Acknowledgements are not compulsory. Where included they should be brief. Grant or contribution numbers may be acknowledged.

Please refer to Journal-level guidance for any specific requirements.

\section*{Declarations}

Some journals require declarations to be submitted in a standardised format. Please check the Instructions for Authors of the journal to which you are submitting to see if you need to complete this section. If yes, your manuscript must contain the following sections under the heading `Declarations':

\begin{itemize}
\item Funding
\item Conflict of interest/Competing interests (check journal-specific guidelines for which heading to use)
\item Ethics approval and consent to participate
\item Consent for publication
\item Data availability 
\item Materials availability
\item Code availability 
\item Author contribution
\end{itemize}

\noindent
If any of the sections are not relevant to your manuscript, please include the heading and write `Not applicable' for that section. 

%%===================================================%%
%% For presentation purpose, we have included        %%
%% \bigskip command. Please ignore this.             %%
%%===================================================%%
\bigskip
\begin{flushleft}%
Editorial Policies for:

\bigskip\noindent
Springer journals and proceedings: \url{https://www.springer.com/gp/editorial-policies}

\bigskip\noindent
Nature Portfolio journals: \url{https://www.nature.com/nature-research/editorial-policies}

\bigskip\noindent
\textit{Scientific Reports}: \url{https://www.nature.com/srep/journal-policies/editorial-policies}

\bigskip\noindent
BMC journals: \url{https://www.biomedcentral.com/getpublished/editorial-policies}
\end{flushleft}

\begin{appendices}

\section{Section title of first appendix}\label{secA1}

An appendix contains supplementary information that is not an essential part of the text itself but which may be helpful in providing a more comprehensive understanding of the research problem or it is information that is too cumbersome to be included in the body of the paper.

%%=============================================%%
%% For submissions to Nature Portfolio Journals %%
%% please use the heading ``Extended Data''.   %%
%%=============================================%%

%%=============================================================%%
%% Sample for another appendix section			       %%
%%=============================================================%%

%% \section{Example of another appendix section}\label{secA2}%
%% Appendices may be used for helpful, supporting or essential material that would otherwise 
%% clutter, break up or be distracting to the text. Appendices can consist of sections, figures, 
%% tables and equations etc.

\end{appendices}

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

\bibliography{sn-bibliography}% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl


\end{document}
