% ICCV 2025 Paper Template

\documentclass{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage{iccv}              % To produce the CAMERA-READY version
\usepackage[review]{iccv}      % To produce the REVIEW version
% \usepackage[pagenumbers]{iccv} % To force page numbers, e.g. for an arXiv version

% Import additional packages in the preamble file, before hyperref
\input{preamble}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, 
% e.g. with the file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete *.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you should be clear).
\definecolor{iccvblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=iccvblue]{hyperref}


%%%%%%our added packages
\usepackage{multirow}
\usepackage{array}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{amsmath}



%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{9740} % *** Enter the Paper ID here
\def\confName{ICCV}
\def\confYear{2025}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Personalization Toolkit: Training Free Personalization of Large Vision Language Models}

\begin{document}
\maketitle
\appendix
%\section{Text-Only Question Answering}

 %\begin{table}[h!]
 %\centering
 %\small
 %   \input{tabs/supp_text_qa}
 %     \caption{ }
 %     \label{tab:supp_text_qa}
%\end{table}

\section{Time and Memory Requirements}
Table \ref{tab:supp_time_memory} details the memory usage and runtime of the various modules in our method for the \VLM personalization task. By default, we employ GroundingDINO~\cite{liu2023grounding} for extracting views from reference images and proposing objects during inference, DINOv2 features for retrieving personalized instances, and LLaVA as our \VLM for generating answers. All experiments are performed on an A5000 RTX GPU. As indicated in the table, our plug-and-play modules are efficient and introduce minimal overhead to the original LLaVA model.


 \begin{table}[h!]
 \centering
    \input{tabs/supp_time_memory}
      \caption{\small\textbf{Time and Memory Requirement}. View extraction is performed on 5 images per personalized object category,while the remaining modules are executed for each image during the inference process.}
      \label{tab:supp_time_memory}
\end{table}






\section{Retrieval Module Backbone Ablation}


In this section we perform an ablation of the image encoder $F_{emb}$, which serves as the backbone of the Retrieval Module and is responsible for extracting object features and performing matching. We observe a noticeable gap between CLIP and DINO, indicating that DINO's visual features are more discriminative and better suited for retrieval tasks. Between the two DINO variants (base and large), the larger version delivers slightly better performance, the difference is marginal indicating that any DINOv2 backbone can be deployed.

 \begin{table}[h!]
 \centering
    \input{tabs/supp_retrieval_abl}
   \caption{\small\textbf{Feature Extractor Ablation}. We observe that DINO features significantly outperform CLIP features.}
      \label{tab:supp_retrieval_abl}
\end{table}

\section{Threshold Selection}

In the main paper, we used a fixed threshold of $0.75$ for all settings in order to decide whether a personalized object is present in an image or not. For completeness and flexibility, here we  offer a straightforward method for tuning the thresholds on a per-object basis. To adjust the threshold for a given  
personalized object $i$, we first compute the distribution of distances for the reference images of the object, and then calculate the mean $\mu_i$ and standard deviation $\sigma_i$ of the distribution. The threshold is then set as $\tau_i = \mu_i - \sigma_i/2$. (We note that by using this method, one can easily adjust the thresholds to be stricter or more loose by adding a scalar multiplier, as $\tau_i = \mu_i - \alpha (\sigma_i/2)$.)

 \begin{table}[h!]
 \centering
    \input{tabs/supp_tuned}
        \caption{\small\textbf{ Automatic object threshold selection}.}
      \label{tab:supp_tuned}
\end{table}


\section{Number of Reference Images.}
Since our approach does not require a training phase, a key question is how many reference images are needed for robust visual recognition of personalized objects. Figure~\ref{fig:number_ref} shows that our method performs well with just one reference image and matches state-of-the-art performance with two images on  MyVLM dataset. On  Yo'LLaVa dataset, we achieve comparable performance to Yo'LLaVa~\cite{nguyen2024yo} with only three images, even though the full set  includes up to 10 images for some objects.

\begin{figure}[h]
\vspace{-0.4cm}
    \centering
    \includegraphics[width=.5\linewidth]{fig/training_views.pdf}
    \vspace{-0.2cm}
    \caption{Average weighted visual recognition accuracy as a function of number of reference images. Increasing the number of reference images improves performance, but \ours is robust with just one reference image.} 
    \label{fig:number_ref}
\end{figure}

\section{Open-Vocabulary Categories}
As mentioned in Section 3.1 of the main paper, we employ each object's semantic category $k_p$ on the reference image $I_p$ to extract its object-level mask $S_p$. For the MyVLM dataset, where the object names are generic, we directly input them into our open-vocabulary detector. For the Yo'LLaVA dataset, some concept names such as Vietnamese individual names do not directly indicate the semantic categories. To address this, we use the following categories to cover the 40 objects in this dataset:


\begin{center}
Cartoon Cat/Dog, Cartoon Man/Woman, Cat, Church, Cup, Dog, Face, Keyboard, Plush, Temple
\end{center}

Note that we use \textit{Face} for detecting humans, as matching during inference should focus on facial features rather than clothing. Using \textit{Man/Woman} would prompt the open-world detector to segment the entire person, including their clothing which might change for test images.

For the This-Is-My-Img benchmark, we use the following categories to cover all 13 objects in the dataset. 
\begin{center}
Bag, Bike, Dog, Car, Chair, Face, Hat, Keyboard, Shoes, Skateboard
\end{center}
It is worth noting that, aside from the people in the dataset (queried with the category `Face'), all other object names in this dataset include their semantic category (e.g., Alex's hat).
\section{Semantic Category Ablation}
We compare our method's performance on the Yo'LLaVA dataset by providing the open-vocabulary segmentation model with semantic categories, dataset-provided names, or the generic term `main' during reference view extraction. As shown in Table~\ref{tab:gsam_vocab}, PeKit achieves state-of-the-art performance even without relying on semantic categories of personalized objects.
 \begin{table}
 \centering
\input{ICCV/tabs/category_ablation}
  \caption{\textbf{{Reference View Vocabulary Ablation on Yo'LLaVA Dataset.}}}
  \label{tab:gsam_vocab}
\end{table}

\section{Prompt Template}
In this section, we present the specific query format used for our experiments. As mentioned in the main paper (Sec 3.4), we employ visual prompting to indicate the personalized object’s location in the image. Next we query our \VLM to personalize its answer given the instance's name and context.  Our prompt to the \VLM for various tasks follows this general structure:

\begin{center}
\small
In this image, the entity enclosed in a `\textbf{COLOR}' box is called `\textbf{NAME}'. 

Without mentioning the bounding box and its color, {`\textbf{TASK}'}.

[Optional] Give more details using the information from {`\textbf{CONTEXT}'}.


\end{center}

The \textbf{COLOR} placeholder indicates the color of the bounding box overlaid on the image, the \textbf{NAME} placeholder specifies the instance name, and the \textbf{TASK} placeholder contains the task-specific query. Optionally, the {\textbf{CONTEXT}} placeholder can contain the prior knowledge about the personalized object retrieved from the memory module.

Note that when multiple objects are detected in one image, the query’s grammatical structure changes to a plural format, and the \textbf{COLOR} and \textbf{NAME} placeholders will contain multiple values separated by commas. The \textbf{CONTEXT} placeholder for each object is included in angle brackets ($<>$) and contains the \textbf{NAME} for the corresponding instance.

For the experiments in the paper the \textbf{TASK} placeholder can be any of the following prompts:

\begin{center}
\footnotesize
\textbf{Personalized Captioning}: Describe what is `\textbf{NAME}' doing. Describe the image too.

\textbf{VQA}: Answer the following question about `\textbf{NAME}'.
\end{center}

Figure~\ref{fig:supp_query} illustrates an example of our full prompt for each one of the tasks.

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{fig/supp_prompts.png}
    \caption{\small\textbf{Query Format.} Personalized VQA and captioning on Yo'LLaVA (Left) and MyVLM (Right) datasets. The context used for the `red chicken' is imaginary and generated by ChatGPT.}
    \label{fig:supp_query}
\end{figure*}


\section{This-Is-My-Img Dataset Details}
The This-Is-My (video) dataset includes 104 training and 582 validation short video segments across 15 categories. However, some of the segments are no longer available for download. Consequently, one personalized object category, ‘Alex’s Piano’ has been removed from our proposed benchmark due to the unavailability of its training segments. Table~\ref{tab:supp_this_is_my} provides more details on the validation split of our benchmark.

Note that three categories do not contain a \textbf{Negative (Hard)} frames set, as the corresponding instances are visible in all frames of their validation segments. Additionally, different categories have varying numbers of \textbf{Positive} and \textbf{Negative (Hard)} and \textbf{Negative (Other)} images due to different segment lengths and number of segments per object. However, for the \textbf{Fake} sets, we generated a similar number of 10 validation frames for all classes. Figure~\ref{fig:supp_this_is_more} presents further illustration on our proposed benchmark.

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{fig/this_is_my_example_more.png}
    \caption{\small\textbf{This-Is-My-Img Benchmark.} Our proposed benchmark contains various concepts in realistic indoor/outdoor scenes. The negative and fake validation sets are designed to test the robustness of current and future methods by imitating the appearance of personalized objects or their general environment.}
    \label{fig:supp_this_is_more}
    \vspace{-0.5cm}
\end{figure*}



 \begin{table}[h!]
 \centering
 \small
    \input{tabs/supp_this_is_my}
      \caption{\small\textbf{This-Is-My-Img Categories.} The validation frames for each category are divided into two sets: one with the object visible in the image and one without.}
      \label{tab:supp_this_is_my}
\end{table}
\section{Limitations}
\noindent\textbf{Wrong Reference Views.} Our approach depends on extracting instance masks through a segmentation network. Noisy or incorrect masks can increase the likelihood of false positives during inference. As shown in Figure~\ref{fig:failure_masks}, noisy masks, overlaid with red on the personalized object, can lead to incorrect matching during inference, depicted with red bounding boxes. Incorporating a clustering method on the extracted reference features to remove the wrong views might enhance the performance of our method by mitigating this issue.



\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{ICCV/fig/failure_noisy_masks.pdf}
    \caption{\small\textbf{Noisy Reference Views:} Poor segmentation masks may affect the visual prompting stage and degrade PeKit's performance.}
    \vspace{-0.2cm}
    \label{fig:failure_masks}
\end{figure*}


\noindent\textbf{Small Objects.} Our method performs feature matching at a resolution lower than the original image due to the stride factor (=14) in the DINOv2 encoder. This may lead to suboptimal performance for personalized objects that appear small in the reference images. As shown in Figure~\ref{fig:failure_small}, with a limited number of patches for the personalized object, DinoV2 may not encode enough fine-grained details, capturing only general attributes like the semantic category (e.g., sneakers) or color, which can result in a higher false positive rate. In case the original reference images are available in high resolution, a potential solution is to crop the (small) personalized objects and resize them to DinoV2’s native resolution ($518\times518$) before extracting the embeddings.

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{ICCV/fig/failure_small.pdf}
    \caption{\small\textbf{Small Reference Objects:} The native image resolution ($518\times518$) and stride factor (14) of DinoV2 might result in embeddings of small personalized objects, such as Blippi's shoes, capturing only general attributes, which can increase the likelihood of false positive detections. The incorrect detections are depicted on our proposed Fake validation set.}
    \label{fig:failure_small}
\end{figure*}


\noindent\textbf{Object Correlations.} Our method, which relies on instance-level detection of personalized objects, could benefit from incorporating contextual information around these objects. This is particularly relevant in scenarios with look-alike objects in contexts that differ from the reference images. For instance, \textit{Alex’s everyday bag} is more likely to appear in an image featuring \textit{Alex} rather than someone else. As a potential future direction, we plan to explore methods for extracting object relationships from reference images and integrating them into our \VLM as prior knowledge.

%\section{Qualitative Comparison to Yo'LLaVA}


%As illustrated in Figure~\ref{fig:yo_comparison},Yo’LLaVA’s prompt template requires the query to include the target object’s concept identifier, making it unsuitable for general captioning tasks and tailored for Visual Question Answering. Additionally, fine-tuning the last layer of the language model reduces the LVLM’s captioning capabilities (first row, first image). Since Yo’LLaVA operates on image-level embeddings of reference views, it needs clutter-free object-centered reference views of the personalized objects. Performance can decline if the personalized object is not in the foreground or if there are other objects/people interacting with the personalized object in the reference views (second row, 1st, 2nd, and 4th images).

%\begin{figure*}[h]
%    \centering
%    \includegraphics[width=\textwidth]{ICCV/fig/qualitative_vs_yollava_2.pdf}
%    \caption{\textbf{Comparison to Yo'LLaVA:} Yo’LLaVA’s prompt template relies on including the concept identifier of the target personalized objects within the query to enable personalization (first row). This constraint limits the method's ability to generalize at test time, as it requires the user to already know which personalized objects are present in the image. Additionally, training with image-level embeddings from the reference views can cause Yo’LLaVA to confuse the correct personalized object (e.g., confusing Alex and Alex’s everyday bag). Modifying the LLM’s head weights degrades the original LVLM’s captioning quality (first row, leftmost image). For each image, the "Query" shows Yo’LLaVA's main prompt, where the concept identifier <sks> is substituted with the name of the personalized object, along with an added system prompt. PeKit employs its own prompt template described in Appendix E.}
%    \label{fig:yo_comparison}
%\end{figure*}


\section{Qualitative Comparison to MyVLM \cite{alaluf2024myvlm}}
Figure~\ref{fig:my_comparison} compares PeKit with MyVLM on images from the MyVLM dataset. We used the original checkpoints and code provided by the authors of MyVLM to generate the results. Checkpoints for the VQA task were not provided.

MyVLM shares the same limitation as Yo'LLaVA, functioning exclusively when the concept identifier is included in the query. Additionally, MyVLM exhibits low precision in detecting personalized objects, as shown in the main paper for the This-is-my dataset. This can lead to misidentifying objects as personalized ones. The leftmost image in Figure~\ref{fig:my_comparison} demonstrates this limitation. MyVLM is asked to provide a caption for the target concept ‘Cat Statue’ while the provided image includes another personalized object, ‘Asian Doll.’ As shown, MyVLM incorrectly identifies the ‘Asian Doll’ as the ‘Cat Statue’ and generates an incorrect personalized caption. Our method addresses this issue by first detecting the correct personalized object(s) and then generating a caption based on the prompt template provided in Appendix G.

Furthermore, MyVLM’s training appears to degrade the original LVLM’s captioning capabilities, leading to short captions with hallucinations and sometimes incomprehensible text, leading to a low CLIPScore as demonstrated in the main paper.


\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{ICCV/fig/qualitative_vs_myvlm.pdf}
    \caption{\textbf{Comparison to MyVLM:} MyVLM often misidentifies personalized objects because of its low precision. In the leftmost figure, when prompted to caption an image containing a 'Cat Statue'—which is actually absent—MyVLM incorrectly labels the 'Asian doll' and the headset as the 'Cat Statue' instead of rejecting the query. Additionally, MyVLM training interferes with the original captioning capabilities of the LVLM, leading to hallucinations, short captions, and sometimes incomprehensible text. For each image, `Query' depicts MyVLM's sytem prompt where the concept identifier $<$sks$>$ is replaced with the personalized object's name. PeKit employs its own prompt template described in Appendix G.}
    \vspace{-0.2cm}
    \label{fig:my_comparison}
\end{figure*}

\section{Qualitative Comparison to LLaVA}
Figure~\ref{fig:qualitative_llava} presents examples of \ours model compared to the base LLaVA~\cite{liu2024visual} on VQA and personalized captioning tasks using images from all three benchmarks discussed in the main paper. When LLaVA does not recognize an object from the given name in the query, it makes guesses, leading to hallucinations or incorrect statements. In contrast, \ours accurately identifies objects and uses in-context information to guide LLaVA in answering questions or providing details about the image, effectively incorporating in-context information and object appearance. While more advanced prompting or personalized response examples could enhance \ours, we opted for simplicity and standard design, leaving such improvements for future work.
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{fig/qualitative_2.pdf}
    %\includegraphics[width=0.9\linewidth]{ICCV/fig/qual4.pdf}
    %\includegraphics[width=0.9\textwidth]{ICCV/fig/qualitative_vs_yollava_2.pdf}
        %\caption{\small\textbf{Qualitative Comparison to Yo'LLaVA:} Yo’LLaVA’s prompt template requires specifying the personalized object's identifier in the query (first row), limiting generalization since users must already know which objects are in the image. Using image-level embeddings can also cause confusion between similar objects (e.g., Alex vs. Alex’s bag). Adjusting the LLM’s head weights further harms captioning quality. PeKit achieves better captioning quality without any training. In each example, the ``Query'' shows Yo’LLaVA's prompt with the concept identifier replaced by the object's name and an added system prompt. PeKit uses a different template, detailed in the Appendix.}
        \vspace{-0.5cm}
    \caption{\small\textbf{Comparison to LLaVA}: Right: Our method detects personalized objects and integrates provided context (for qualitative comparison) in caption generation. Left: While the original model struggles with specific questions about named objects, our method easily identifies the referred object.}
    \label{fig:qualitative_llava}
\end{figure*}
{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{main}
}

\end{document}