\section{Introduction}
\vspace{-0.2cm}
\label{sec:intro}
\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{fig/tease_pkit.pdf}
    \vspace{-0.5cm}
    \caption{\textbf{Illustration of the personalization task and our  \ours.}
    A reference image is introduced to the \VLM with information and possible context. The \VLM should later be able to answer questions about the introduced object using only the name of the object in the query. Our approach, \ours, extracts patch-level features from the reference image and stores them in a memory module,  $\mathcal{M}$. During personalized inference, our retrieval module, $\mathcal{R}$, queries  $\mathcal{M}$ to detect the object. \ours then informs the \VLM via a visual prompt, providing the name and possible context.}
    \vspace{-0.5cm}
    \label{fig:teaser}
\end{figure*}

Large Vision Language Models (LVLMs)~\cite{liu2024visual,liu2024improved,chen2024internvl,zhu2023minigpt,li2023blip,agrawal2024pixtral12b,wang2024qwen2} have demonstrated impressive capabilities in reasoning about visual content and answering visual questions across various domains. This suggests a great potential for deployment as visual assistants that can aid users in their daily lives. However, current LVLMs are designed to provide generic, user-independent responses and recognize objects at the category level. This limits their ability to refer to specific instances by their unique names. Figure \ref{fig:teaser} shows an example of LVLM generic inference - i.e. giving a generic response to a user query.
%Large Vision Language Models (LVLMs)~\cite{liu2024visual,liu2024improved,chen2024internvl,zhu2023minigpt,li2023blip,agrawal2024pixtral12b,wang2024qwen2} have shown impressive performance in reasoning about visual content and answering adequate visual questions of various types and domains. With these advancements, one can imagine future deployment of these LVLMs as visual assistants that aid users in their daily lives, for example, by reminding an elderly user where he stored his medicine, fetching a certain object for the user~\cite{barsellotti2024personalized}, or simply telling a little girl where she had left her teddy bear. 

%Current LVLMs are generic, provide user-independent responses, and only recognize objects at the category level; as such, they are unable to discriminate instances of categories and refer to these instances by their used names (e.g., “my teddy bear” or "my dog Karl"). Figure.1 shows an example of LVLM generic inference - i.e. giving generic response to a user query.

The task of personalizing vision-language models was recently introduced by~\cite{alaluf2024myvlm} to enable LVLMs to recognize specific object instances and answer relevant questions accordingly. However, existing approaches~\cite{alaluf2024myvlm,nguyen2024yo} require  training the model for each personalized object, which can be computationally expensive and limits the model's ability to incrementally learn new personalized concepts.
%The task of personalizing vision language models has been recently introduced by~\cite{alaluf2024myvlm} to enable LVLMs to recognize specific instances of objects and answer relevant questions accordingly. Existing approaches~\cite{alaluf2024myvlm,nguyen2024yo} rely on training for a specific personalized object, diverting the LVLM from its original capabilities, incurring a large compute cost and making it difficult to incrementally add other personalized objects.

%Ideally, personalization should happen via a plug-and-play procedure that seamlessly integrates user-specific instances without requiring fine-tuning of the LVLM. This is similar to the personalization of LLMs, where it has been suggested by~\cite{zollo2024personalllmtailoringllmsindividual} that LLM personalization requires a new algorithmic toolkit, as it is not practical to train and store a separate copy of the model or even low-rank adapter weights for every user.

In this work, we build upon the strengths of pre-trained vision foundation models, the emerging capabilities of LLMs in in-context learning~\cite{dong2022surveyincontext} and retrieval-augmented generation (RAG)~\cite{lewis2020retrieval}. Our training-free approach localizes instances using open-world object detectors~\cite{liu2023grounding, kirillov2023segment, oquab2023dinov2} and stores reference instance-level features in memory banks, alongside their name and context. During inference, our retrieval module queries the memory bank and visual-prompts the LVLM. By storing only feature embeddings, we avoid privacy concerns derived from training on user images. An overview of our approach is shown in Figure \ref{fig:teaser}.
%In this work, by relying on the power of pretrained vision foundation models and the emerging abilities of LLMs in in-context learning ~\vdoro{maybe cite ICL}   and retrieval-augmented generation (RAG)~\cite{lewis2020retrieval}, we set a strong baseline for LVLM personalization that surpasses training-based approaches with zero training and limited latency. 

%Specifically, ~\cite{liu2023grounding, kirillov2023segment, oquab2023dinov2} We utilize open-world object detectors on reference images of objects and create a memory bank of instance-level features coupled with objects' names and context.

%During deployment, a retrieval module detects the instance, extracts its memory ~\vdoro{} and context, and use visual prompting that relieves the LVLM from a training-based instance recognition. Additionally, we provide in-context information and instruct the LVLM to follow a specific style in responses via in-context learning~\cite{dong2022surveyincontext}. We refer to Figure.1~\ref{fig:teaser} for an illustration of our approach.

In addition to existing benchmark datasets, we evaluate our approach using a video personalization dataset, converted to an LVLM personalization format, and further augmented with a challenging evaluation set, which reflects a realistic personalization scenario and highlights the difficulty of the personalization task. It also shows the effectiveness of patch-level features for robust instance personalization. We also discuss the limitations of existing benchmark datasets and highlight opportunities for future research.
%We observe that the two LVLM personalization benchmarks~\cite{alaluf2024myvlm,nguyen2024yo} mainly consist of object-centric images with one instance prominently in the image. This is a much easier scenario compared to what a realistic personalization use case could be. Thus, we leverage an existing video personalization dataset~\cite{yeh2023meta}, convert it to an LVLM personalization format, and further augment it with a challenging evaluation set that illustrates the difficulty of the personalization task and shows the effectiveness of patch-level features over image-level features for strong instance personalization.

The key contributions of our approach are:
1) We demonstrate that LVLM personalization can be achieved without training, enabling fast and efficient deployment.
%1) We tackle the problem of large vision language model personalization from a training-free perspective.
2) We propose a flexible approach that leverages pre-trained vision foundation models, RAG, and visual prompting to personalize any LVLM.
%2) We leverage the power of vision foundation models, retrieval-augmented generation, and visual prompting to personalize any LVLM to any object or user in a matter of seconds.
3) We achieve state-of-the-art results on three personalization benchmarks, outperforming existing approaches.
% 3) We achieve state-of-the-art results on three personalization benchmarks, illustrating how our approach overcomes the limitations of training-based approaches.
4) We create a challenging evaluation set that pushes the limits of existing approaches and highlights opportunities for future research.
%4) We curate a challenging personalization benchmark that challenges existing approaches and shows significant room for improvement in future research.

In the following, we discuss the related work in Section~\ref{sec:related_work} and present our vision toolkit for LVLMs personalization in Section~\ref{sec:approach}, evaluate our approach in Section~\ref{sec:experiments} and conclude in Section~\ref{sec:conclusion}.











%Large Vision Language Models (LVLMs)~\cite{liu2024visual,liu2024improved,chen2024internvl,zhu2023minigpt,li2023blip,agrawal2024pixtral12b,wang2024qwen2} have shown impressive performance in reasoning about visual content and answering adequate visual questions of various types and domains. With these advancements, one can imagine future deployment of these LVLMs as visual assistants that aid users in their daily lives, for example, by reminding an elderly user where they stored their medicine, fetching a certain object for the user~\cite{barsellotti2024personalized}, or simply telling a little girl where she had left her teddy bear. However, current LVLMs are generic, provide user-independent responses, and only recognize objects at the category level; as such, they are unable to discriminate instances of categories and refer to these instances by their used names (e.g., “my teddy bear”).

%The task of personalizing vision language models has been recently introduced by~\cite{alaluf2024myvlm} to enable LVLMs to recognize specific instances of objects and specific users and answer relevant questions accordingly. We refer to Figure~\rahaf{TEASER} for an illustration of the task of personalized vision language models.

%Existing approaches~\cite{alaluf2024myvlm,nguyen2024yo} rely on training for a specific concept and personalized object during deployment time, diverting the LVLM from its original capabilities and incurring a large compute cost, making these solutions less attractive for the tasks they are intended for. Further, training for  specific object(s) does not allow for adding other personalized objects and creates a specific version of the LVLM for that concept.

%Ideally, personalization should happen via a plug-and-play procedure that seamlessly integrates user-specific instances without requiring fine-tuning of the LVLM. This is similar to the personalization of LLMs, where it has been suggested by~\cite{zollo2024personalllmtailoringllmsindividual} that LLM personalization requires a new algorithmic toolkit, as it is not practical to train and store a separate copy of the model or even low-rank adapter weights for every user.

%In this work, we depart from the need to train and rely on the existing capabilities of vision foundation models to facilitate the task of personalizing vision language models. We show that it is possible to achieve state-of-the-art performance on the personalization task with zero training and limited latency.
%Specifically, we use pretrained vision models as tools for LVLMs like LLaVA~\cite{liu2024improved} to generate personalized outputs for specific user instances. From reference images, we utilize open-world object detectors and segmentation models such as GroundingDINO~\cite{liu2023grounding} and SAM~\cite{kirillov2023segany} to detect objects for personalization. We then create a memory of these objects using patch-level features from an expressive vision foundation model~\cite{oquab2023dinov2}, forming a memory bank of personalized instances with names and contexts.
%During deployment, a retrieval module detects the instance, extracts its memory and context, and relieves the LVLM from recognizing object instances. To address the multi-image limitation of LVLMs, we use visual prompting to indicate the object instance in the image. Additionally, we provide in-context information about detected instances and instruct the LVLM to follow a specific style in responses via in-context learning~\cite{dong2022surveyincontext}.

%By relying on the power of pretrained vision foundation models and the emerging abilities of LLMs in in-context learning and retrieval-augmented generation (RAG)~\cite{lewis2020retrieval}, we set a strong baseline for LVLM personalization that surpasses training-based approaches.

%We observe that the two LVLM personalization benchmarks~\cite{alaluf2024myvlm,nguyen2024yo} mainly consist of object-centric images with one instance prominently in the image. This is a much easier scenario compared to what a realistic personalization use case could be. Thus, we leverage an existing video personalization dataset~\cite{yeh2023meta}, convert it to an LVLM personalization format, and further augment it with a challenging evaluation set that illustrates the difficulty of the personalization task and shows the effectiveness of patch-level features over image-level features for strong instance personalization.

%Our contributions are as follows:
%1) We tackle the problem of large vision language model personalization from a training-free perspective.
%2) We leverage the power of vision foundation models, retrieval-augmented generation, and visual prompting to personalize any LVLM to any object or user in a matter of seconds.
%3) We achieve state-of-the-art results on three personalization benchmarks, illustrating how our approach overcomes the limitations of training-based approaches.
%4) We curate a challenging personalization benchmark that challenges existing approaches and shows significant room for improvement in future research.

%In the following we discuss the related work in Section~\ref{sec:related_work} and present our vision toolkit for LVLMs personalization in Section~\ref{sec:approach}, evaluate our approach in Section~\ref{sec:experiments} and conclude in Section~\ref{sec:conclusion}.
 
 
 
 
 
 
 
 
 
 
 
 
 % Rahaf: Some motivation from ICLR submission"We argue that building personalized LVLMs for vision-language multimodal understanding can be helpful in various scenarios
% like dialogue systems, QA (question-answering), human-computer interaction, etc.. See examples
% in Figure 1 bottom. with personalization, a user asks a model simply by “Where is ⟨billie⟩?”.
% Otherwise, the user has to describe the details of the queried subject like “Where is the person
% with blonde hair and blue jacket?” Other examples also showcase the exciting applications and
% practicality of personalized LVLMs"

% Motivation from another ICLR paper \textcolor{blue}{A qualified personalized assistant first needs to be able to recognize and remember user-related concepts, such as the dog named 〈Lala〉 adopted by the user. Although existing MLLMs have been
% trained on large-scale datasets and possess strong recognition and classification capabilities, directly
% transferring this knowledge to a user’s personal concepts remains challenging. For instance, current leading MLLMs cannot remember your dog’s name, even if you have mentioned it before, and
% they lack awareness of your identity and preferences. Furthermore, the assistant should generate responses tailored to the user’s preferences and requirements. However, collecting extensive personal
% information to train a unique assistant for each user is impractical}