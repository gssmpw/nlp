\section{Related Work}
\vspace{-0.2cm}
\label{sec:related_work}
\myparagraph{Personalization of Large Language Models (LLMs).} LLMs personalization~\cite{zhang2024personalizationlargelanguagemodels} enhances user-specific interactions without extensive fine-tuning. Approaches like prompt tuning, in-context learning, and Retrieval-Augmented Generation (RAG) offer dynamic personalization without additional model storage~\cite{li2024matching, brown2020language, zollo2024personalllmtailoringllmsindividual}. Our method avoids storing new models or adapters and is training-free.\\
% \myparagraph{Personalization of Large Language Models (LLMs).}
% LLMs personalization~\cite{zhang2024personalizationlargelanguagemodels} has been a significant research area, focusing on enhancing user-specific interactions without extensive fine-tuning. Various approaches have emerged, including prompt tuning, in-context learning, and fine-tuning, which have shown their importance for adaptability~\cite{li2024matching}. Notably, methods like in-context learning~\cite{brown2020language} and Retrieval-Augmented Generation (RAG) offer feasible solutions for dynamic personalization without extensive model storage, as suggested in \cite{zollo2024personalllmtailoringllmsindividual}. Similarly, our approach avoids storing new models or adapters for each new concept while being training-free.\\
\myparagraph{Text-to-Image Personalization.}
Personalizing text-to-image generation has been explored extensively, with early methods like Textual Inversion~\cite{gal2022image}, Dreambooth~\cite{ruiz2023dreambooth}, and HyperDreamVbooth~\cite{ruiz2024hyperdreambooth} requiring training for each identity, leading to scalability issues. Recent approaches~\cite{shi2024instantbooth,zeng2024jedi,he2024imagine} avoid test-time fine-tuning by pretraining for personalization. However,
we argue that, while personalized image generation does require training, LVLMs can be personalized without changing the large language model.\\
\myparagraph{Large Vision Language Models Personalization}
The task of personalized vision language models was introduced in MyVLM~\cite{alaluf2024myvlm} by training a concept head for specific objects on top of the CLIP \textit{cls} token. In a similar fashion as Dreambooth~\cite{ruiz2023dreambooth}, MyVLM employs rare tokens to encode personalized concepts. This may have unintended consequences for language assistants, and requires optimizing the LLM caption loss for personalized conversations. Yo'LLaVA~\cite{nguyen2024yo} adds extra tokens to the LLM head for each personalized object, learning concept tokens to describe them. Although it surpasses MyVLM in performance, the additional tokens create a challenging incremental classification problem~\cite{de2021continual}. Both models need test-time training for each new concept, limiting them to personalizing one concept at a time. Two recent works~\cite{hao2024rememberretrievegenerateunderstanding,pham2024personalizedlargevisionlanguagemodels} attempt to eliminate test-time training. While \citet{hao2024rememberretrievegenerateunderstanding} approaches the task by large-scale pretraining on personalized conversations, PLVLM~\cite{pham2024personalizedlargevisionlanguagemodels} pretrains an alignment module to incorporate the \texttt{CLS} token and image  embeddings from DINOv2~\cite{oquab2023dinov2} into the LLM. \citet{hao2024rememberretrievegenerateunderstanding} includes a reference image in the input to the LVLM, which may limit the capability to process multiple images and hinder in-context learning.
PLVLM~\cite{pham2024personalizedlargevisionlanguagemodels} relies on querying for a specific personalized object, limiting its applicability to only VQA. Our approach applies to both VQA and captioning (general and personalized) and is the first training-free personalization solution, featuring a modular, plug-and-play design that eliminates the need for retraining or adaptation.\\
\myparagraph{Vision Foundation Models.} Our approach uses vision foundation models, which have become powerful generalist models through large-scale pretraining~\cite{bommasani2021opportunities}. Vision-language models like CLIP~\cite{radford2021learning} and Align~\cite{jia2021scaling} map objects to higher-level semantics with contrastive pretraining. Self-supervised models like DINO~\cite{caron2021emerging,oquab2023dinov2} detect variations within the same class. Grounding DINO~\cite{liu2023grounding} redefines object detection as phrase grounding, and SAM~\cite{kirillov2023segment} excels in image segmentation.\\
\myparagraph{Visual Prompting} Visual prompting uses visual cues such as bounding boxes or arrows to guide Vision-Language Models. CLIP~\cite{radford2021clip} interprets these marks to modify its \textit{cls} token embedding accordingly~\cite{shtedritski2023does}. Set~of~Mark~Prompting~\cite{yang2023set} integrates GPT-4V with visual prompts using tools like MaskDINO~\cite{li2022mask}, SAM~\cite{kirillov2023segment}, Semantic SAM~\cite{li2023semantic}, and SEEM~\cite{zou2024seem}. ViPLLaVA~\cite{cai2024vip} enhances LLaVA~\cite{liu2024visual} to follow visual prompts by tuning on GPT-4V-labeled data. Contrastive Region Grounding (CRG)~\cite{Wan2024CRG} improves LLaVAâ€™s focus on objects by contrasting token probabilities with and without target object masking. Our experiments show that LLaVA and other LVLMs can describe objects accurately with proper instruction and context. Training-free methods like CRG~\cite{Wan2024CRG} can further enhance attention if needed.
