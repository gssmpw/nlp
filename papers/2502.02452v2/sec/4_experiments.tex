\section{Experiments}
\vspace{-0.2cm}
\begin{figure*}[t]
    \centering
    %\includegraphics[width=0.9\textwidth]{fig/this_is_my_example.pdf}
    \includegraphics[width=0.9\textwidth]{fig/this_is_my_example_blur.pdf}
        \vspace{-0.4cm}
    \caption{\textbf{Our new evaluation set \thisismy based on \textit{This-Is-My} dataset~\cite{yeh2023meta} }: Training and validation sets examples for the category \textit{Reynard's Work Chair.}}
        \vspace{-0.6cm}
    \label{fig:this_is}
\end{figure*}
\label{sec:experiments}
This section evaluates our approach against existing methods for vision language personalization.

%Our method is generic regarding the choice of the \VLM model. However, to maintain consistency with previous approaches, we use LLaVA~\cite{liu2024llavanext} (1.6 mistral 7b) as our primary \VLM model. Additionally, we further ablate our method using another \VLM model, InternVL \cite{chen2024internvl} (2 26b).
%\subsection{Experimental Setup}\vspace{-0.1cm}
% \myparagraph{ Datasets} We first consider the datasets introduced in   Yo'LLaVA~\cite{nguyen2024yo} and MyVLM~\cite{alaluf2024myvlm}.
% Yo’LLava~\cite{nguyen2024yo} includes 40 categories featuring objects, buildings, and people, with each category containing 4-10 images for both training and validation sets. MyVLM~\cite{alaluf2024myvlm} comprises 29 object categories, each with 7 to 17 images. For MyVLM, 4 images are randomly chosen as training views, while the remaining images are used for validation. The final validation accuracies are averaged over 5 different runs.
% For both datasets, we only use the reference images along with each object's semantic category for querying for training-view extraction (see \ref{sec:training_views}). For our approach we discard all other data, such as ground-truth captions and negative images used for training the corresponding methods.

\myparagraph{Datasets.} \textbf{MyVLM}~\cite{alaluf2024myvlm} dataset consists of 29 object categories and \textbf{Yo’LLaVA}~\cite{nguyen2024yo} dataset includes 40 categories of objects, buildings, and people. It also features a VQA benchmark with multiple-choice questions (A/B). For both MyVLM and Yo’LLaVA datasets, we use only the reference images, discarding the ground-truth captions and negative images. %Additionally, we use dataset-provided names as the vocabulary for our segmentation model. In the case of Yo’LLaVA, where the names are in Vietnamese, we assign a generic category to each personalized object to optimize performance. An ablation study on the impact of removing the semantic category requirement is provided in the appendix.
\textbf{\thisismy Dataset} Yo’LLaVA and MyVLM datasets are simple and object-centric, lacking real-world complexity. To address this, we propose a new evaluation set for LVLM personalization based on the \textit{This-Is-My} dataset~\cite{yeh2023meta}, originally created for video-level detection of personalized instances reflecting real-world scenarios where objects may be partially visible or in the background.
To make this dataset compatible with image-level training/evaluation, we process it as follows: We extract five reference frames per object from the training segments, ensuring the entire object is visible. For validation, we sample every 10th frame~\footnote{Due to privacy concerns, some original video segments are unavailable, limiting our benchmark to 14 categories. See the Appendix for more details.}. The validation frames are categorized into positives and negatives for each personalized object. 
\textbf{Positives}: frames containing the personalized object. \textbf{Negatives}: frames from the same validation segments where the object is not visible (denoted as \textbf{Hard} negatives) and frames from videos of other objects (\textbf{Other}). We further introduce an additional  negative validation set (\textbf{Fake}) by generating images similar to the personalized objects frames with varied perspectives and environments using ChatGPT-4o (see Figure \ref{fig:this_is} for examples). We ensure that objects in the fake images differ in characteristics from the personalized objects. Our evaluation set challenges \VLM personalization methods with images containing multiple foreground objects, background elements, motion blur, and synthetic images\footnote{\thisismy will be made publicly available.}. We further extend \thisismy 
 with a VQA evaluation set containing 70 multiple-choice (A/B) questions.\\ %The image–question pairs were manually selected to be ambiguous and challenge a vision-language model (VLM) without personalization.  \\
% \textbf{\thisismy Dataset}
%  Yo’LLaVA and MyVLM datasets are simple and object-centric, lacking real-world complexity. To address this, we propose a new evaluation set for LVLM personalization based on the \textit{This-Is-My} dataset \cite{yeh2023meta}. The dataset is originally created with the goal of video-level detection of the personalized instances.  
%  It is composed of video segments from YouTube divided into training/validation and reflects real-world scenarios where objects may be partially visible or in the background.
%  To make this dataset compatible with the image-level training/evaluation protocol of LVLM personalization,  we process the dataset as follows. We extract  five reference frames per object from the training segments, ensuring the entire object is visible. For the validation segments, we sample every 10th frame\footnote{Due to privacy concerns, some original video segments are unavailable, limiting our benchmark to 14 categories. See the Appendix for more details.}. Validation frames for each object are categorized into: \textbf{Positives}: Frames containing the personalized object, and \textbf{Negatives}: Frames from the same validation segments where the object is not visible. Additionally, we introduce a \textbf{Fake Positives} validation set by generating images similar to the training frames with varied perspectives and environments using ChatGPT-4 (see Figure \ref{fig:this_is} for examples). Our evaluation set is designed to challenge \VLM personalization methods with validation images containing multiple foreground objects, background elements, some blurring, and synthetic images\footnote{\thisismy will be made publicly available.}.
% \noindent\textbf{\thisismy Dataset}
%  Yo’LLaVA and MyVLM datasets are simple and object-centric, lacking real-world complexity. To address this, we propose a new benchmark for LVLM personalization based on  \textit{This-Is-My} dataset \cite{yeh2023meta}. The dataset is originally created for video-level personalization where the target is to detect whether the instance is present based on CLIP image language similarity.  The dataset is composed of video segments from YouTube divided into training and validation. The videos reflect real-world scenarios where objects may be partially visible or in the background.
% To make this dataset compatible with the training/evaluation protocol of existing \VLM methods, to allow personalization at image level  and to expand the evaluation protocol beyond current datasets, we create a new benchmark based on images from  \textit{This-Is-My} dataset and refer to it as \thisismy. 
% \thisismy benchmark: we extracted an image-level dataset by selecting 5 frames per object from the training segments, where the full object is present in the image, and sampling every 10th frame from the validation segments\footnote{ Due to privacy issues, some original video segments are unavailable, thus our benchmark includes 14 categories. See the Appendix for more details.}. For each instance: Validation frames are split into: \textbf{Positives}: 
%     Frames with the personalized object.
%    \textbf{ Negatives}: Frames from the corresponding validation segments but without the object visible.
%  Additionally, we introduce a \textit{hard-negative} validation set by generating images similar to the training frames but with varied perspectives and environments using ChatGPT-4. See figure \ref{fig:this_is} for examples.
% Our benchmark is designed to challenge \VLM personalization methods where validation images contain multiple object, or background object, some blurring in addition to  fake image\footnote{\thisismy will be made publicly available.}  
\myparagraph{Implementation Details.}
Our method is generic regarding the choice of the \VLM model. We use LLaVA1.5-13B~\cite{liu2024improved} as our primary \VLM model for consistency with previous works. %For view extraction, we use \ours with G-SAM~\cite{ren2024grounded} as the default choice and report G-DINO~\cite{ren2024grounded} for comparison. During inference, we query our segmentation model using the generic term `object' to generate object proposals. For reference view extraction, we use generic semantic categories.  We refer to the Appendix for more discussion and ablation.
% we rely on dataset-provided names for MyVLM and assign genetic categories For Yo’LLaVA and This-Is-My-Img. The specific semantic categories used are provided in the appendix, along with an ablation study comparing results when using the dataset provided (Vietnamese) names and the generic term `main' (referring to the main object) on the Yo'LLaVA dataset. 
We utilize Cosine Similarity with a constant threshold of $\tau$=0.75 for detecting personalized objects across all datasets (\ref{sec:retrieval}). Refer to the Appendix for further details and results with automatic object-specific threshold using reference image similarity. We also report the compute and memory overhead of our personalization toolkit and compare them to the backbone in the Appendix.
%Additionally, we further ablate our method using another \VLM model, InternVL2-26B~\cite{chen2024internvl}.

% \myparagraph{Evaluation Metrics}
% Following the evaluation protocol for Yo'LLaVA~\cite{nguyen2024yo} and MyVLM~\cite{alaluf2024myvlm} we report positive and negative \textit{recall} and their weighted average denoted as \textit{weighted accuracy} in previous works. Additionally, we report our method's positive and negative \textit{precision} for completeness.
% We report report the latency and memory/compute overhead of our approach in the Appendix.
\myparagraph{Evaluation Metrics.} We follow the metrics and notation of Yo’LLaVA~\cite{nguyen2024yo} and MyVLM~\cite{alaluf2024myvlm}, reporting \textbf{Positive Accuracy} (Recall), \textbf{Negative Accuracy} (Specificity) and \textbf{Weighted Accuracy} for $n$ personalized objects.  We further report \textbf{Precision}, defined as the ratio of true positive predictions to the total number of instances predicted as positive, averaged over $n$ personalized objects. We believe this is a more comprehensive evaluation recognizing the trade-off between the aforementioned metrics and precision. %We refer to the Appendix for more details on the metrics.
We report the accuracy (percentage of correct answers) for the VQA task. Following MyVLM, we  report CLIPScore (measuring the similarity between the caption and the image) and personalization recall (assessing the LVLM's ability to incorporate the personalized object's name into the caption) on the MyVLM dataset.
% \myparagraph{Positive Accuracy} For an evaluation set $D_p = \{ D_{p_1}, ... , D_{p_n} \}$,  where $D_{p_i}$ consists of images of personalized object $i$, positive accuracy  $PA_i$ and overall positive accuracy $PA$ are:
% $PA_i = \frac{|O_{p_i}|}{|D_{p_i}|}$ and $PA = \frac{1}{n} \sum_{i=1}^n PA_i$, where $|O_{p_i}|$  is the number of detections of object $i$ in $D_{p_i}$.
% \myparagraph{Negative Accuracy} For a subset $D_n = \{ D_{n_1}, ... , D_{n_n} \} $, where $D_{n_i}$ contains instances where object $i$ is absent, negative accuracy $NA_i$  and overall negative accuracy $NA$  are:
% $NA_i = 1-\frac{|O_{n_i}|}{|D_{n_i}|}$ and $NA = \frac{1}{n} \sum_{i=1}^n NA_i$, where  $|O_{n_i}|$  is the number of detections of object $i$ in $D_{n_i}$.
% % \myparagraph{Weighted Accuracy} Combining PA and NA gives weighted accuracy $WA$: 
% % $WA = \frac{1}{2}(PA+NA)$.
% \myparagraph{Precision} is conventionally defined as the ratio of correct detections to the total number of detections in $D_e = {D_p \cup D_n}$ .

% \myparagraph{Evaluation Metrics} We follow the pipeline and notation of Yo’LLaVA~\cite{nguyen2024yo} and MyVLM~\cite{alaluf2024myvlm}, and report positive, negative and weighted accuracy. For a dataset with $n$ personalized objects we define:\\
% \myparagraph{Positive Accuracy} Considering an evaluation set $D_p = \{ D_{p_1}, ... , D_{p_n} \}$,  where $D_{p_i}$ only consists of images of personalized object $i$, positive accuracy  $PA_i$  for each personalized object $i$ and overall positive accuracy $PA$ are calculated, as:
% \begin{equation}
%     PA_i = \frac{O_{p_i}}{S_{p_i}}, \quad PA = \frac{1}{n} \sum_{i=1}^n PA_i
% \end{equation}
% where $O_{p_i}$ is the number of detections of personalized object $i$ in $S_{p_i}$.
% \\
% \myparagraph{Negative Accuracy} Similarly, considering a subset $D_n = \{ D_{n_1}, ... , D_{n_n} \} $, where for each personalized object $i$, $D_{n_i}$ contains instances where the object is absent, negative accuracy $NA_i$  for a personalized object $i$ and overall negative accuracy $NA$ are defined as:
% \begin{equation}
%     NA_i = \frac{1-O_{n_i}}{D_{n_i}}, \quad NA = \frac{1}{n} \sum_{i=1}^n NA_i
% \end{equation}
% where $O_{n_i}$ is the number of detections of personalized object $i$ in $S_{n_i}$. \\
% \myparagraph{Weighted Accuracy} Combining $PA$ and $NA$ we get weighted accuracy $WA$: 
% \begin{equation}
%     WA = \frac{1}{2}(PA+NA)
% \end{equation}

%  \begin{table*}[t]
%  \centering
%     \input{tabs/recognition}
%       \caption{Visual Recognition Accuracy(\%) on MyVLM and YoLLaVA datasets. Our approach \ours achieves state of the art performance and  improves significantly over existing methods.  }
%       \label{tab:recognition}
% \end{table*}

 \begin{table}[t!]
 % \centering
    \input{tabs/recognition}
    \vspace{-0.2cm}
        \caption{Visual Recognition performance on MyVLM and YoLLaVA datasets. Our approach \ours achieves state of the art performance and improves significantly over existing methods.}
        \vspace{-0.5cm}
      %{} \textsuperscript{\textdagger}Note that previous works ~\cite{nguyen2024yo}~\cite{alaluf2024myvlm} use Positive Accuracy for Recall, Negative Accuracy for Specificity and Accuracy for their average. We use the same notion for consistency reasons. Higher is better for all of them}
      \label{tab:recognition}
\end{table}

%  \begin{table*}[h!]
%  \centering
%     \input{tabs/this_is_my_recognition}
%       \caption{Visual Recognition Accuracy (\%) on This-is-my-v2 dataset. MyVLM tends to say yes very often especially on images from the same scene regardless of the object's presence. \ours is more robust and improves on average by $\sim25\%$.}
%           \label{tab:this-is-my recognition}
% \end{table*}

 \begin{table} [t!]
 \centering
    \input{tabs/this_is_my_recognition}
    \vspace{-0.2cm}
      \caption{Visual Recognition Performance (\%) on This-Is-My-Img dataset. MyVLM and \yollava tend to capture the dataset's bias and produces many false positives when the images are taken from similar scenes regardless of the object's presence. } %{\ours is more robust improving accuracy on average by $\sim25\%$.} 
      \vspace{-0.6cm}
          \label{tab:this-is-my}
\end{table}
% Unlike previous methods that query a ‘Yes/No’ question for detecting each object in each image, our approach can simultaneously detect multiple personalized objects and ignore the others, all in a single pass.


\vspace{-0.3cm}
\subsection{Visual-Recognition }
\vspace{-0.2cm}
Table~\ref{tab:recognition} presents the performance of our approach compared to current methods, MyVLM~\cite{alaluf2024myvlm} and Yo’LLaVA~\cite{nguyen2024yo}, on their respective datasets. On the MyVLM dataset, our method \ours achieves SOTA results, improving both positive and negative accuracy, with an average improvement of 1.9\%. On the Yo’LLaVA dataset, our method significantly improves negative accuracy by 8.9\% while having a lower positive accuracy than Yo’LLaVA~\cite{nguyen2024yo}, resulting in an average improvement of 2.5\% on weighted accuracy, hence achieving best results on average. Our \ours might miss a few challenging appearances of personalized objects but has a very low false positive rate, meaning we avoid incorrect recognition of personalized objects. We believe it is better to resort to generic captions than to provide incorrect personalized answers.
 \begin{table}[t!]
 \centering
    \input{tabs/VQA}
    \vspace{-0.2cm}
      \caption{Visual Question Answering Performance. \ours achieves SOTA performance on the three datasets. }
      \vspace{-0.6cm}
      \label{tab:VQA}
\end{table}
%It is worth noting that previous works report a higher recall value (positive accuracy) than negative accuracy, suggesting a tendency for saying yes. This may be caused by overfitting to scenes of the training images. By relying on robust localized patch features, \ours avoids overfitting to a specific scene, thus reducing false positives.
Next, we evaluate the performance on our introduced benchmark \thisismy in Table~\ref{tab:this-is-my}. 
MyVLM tends to provide positive responses to test images, resulting in very high positive accuracy but much lower negative accuracy ($14.6\%$  on other  and $4.7\%$ on hard negatives). This indicates that MyVLM tends to capture the scene in which the object occurs rather than the specific object. Note that we followed exactly the training pipeline and requirements for positive and negative images of MyVLM. \yollava has similar but less pronounced trends with  high positive accuracy ($87.1\%$) and a moderate negative accuracy of $84.8\%$ on other negatives and $61.9\%$ on hard negatives. While \ours achieves a lower positive accuracy, we demonstrate a more robust negative accuracy ($99.9\%$ and $96.0\%$ on all and hard negatives respectively).
We achieve a  precision of $90.1\%$ which is $48\%$ higher than \yollava indicating that our approach refrains from providing wrong personalization. On the fake images, smaller differences appear between methods. The fake images sometimes contain very similar objects to the personalized object in terms of style, resulting in confusion by the methods, indicating room for future work. \yollava and MyVLM detect the domain shift on some images more than \ours. While \ours is quite robust to different objects, it  can be challenged on very similar fake and small (see limitations in Appendix). Overall, \ours shows striking robustness and balanced behavior on both positive and negative accuracy achieving an average visual recognition performance of $82.86$ compared to 67.38 by \yollava and 33.92 by MyVLM.  
% MyVLM and \yollava tend to provide positive responses to test images, resulting in very high positive accuracy but much lower negative accuracy ($14.6\%$ and $84.8\%$ respectively on all negatives and $4.7\%$ and $61.9\%$ on hard negatives ). This indicates that  \yollava and MyVLM tend to capture the scene in which the object occurs rather than the specific object, due to training on the CLIP cls token with limited training views ($5$). Note that we followed exactly the training pipeline and requirements for positive and negative images of MyVLM.\rahaf{TODO: add discussion on YollaVA}
% Our \ours shows striking robustness and balanced behavior on both positive and negative accuracy. Moreover, the high negative accuracy achieved by \ours compared to MyVLM highlights the advantages of patch-level over image-level features.
% Even on the challenging fake images, our method better identifies instances of personalized objects compared to MyVLM, achieving a 5\% improvement. The fake images sometimes contain very similar objects to the personalized object in terms of style, resulting in confusion by the methods, indicating room for future work. 
Overall, this evaluation set illustrates the difficulty of the personalization task beyond object-centric images, moving closer to realistic scenarios for intelligent visual assistants. This opens the door for future research on this challenging benchmark.
\vspace{-0.1cm}
\subsection{Visual-Question Answering}
\vspace{-0.1cm}
In Table~\ref{tab:VQA}, we evaluate our method’s ability to answer questions about personalized objects using  Yo’LLaVA~\cite{nguyen2024yo} and  \thisismy benchmarks.
For completeness we also compare \ours  to MyVLM's on the MyVLM dataset using  CLIPScore~\cite{hessel2021clipscore} and Personalization Recall.
%Table \ref{tab:VQA} reports VQA performance on the 3 benchmarks.% We also added the best results of GPT4+Prompt and LLaVA+Prompt from Yo’LLaVA~\cite{nguyen2024yo} with human-engineered prompts.

On the MyVLM dataset, PeKit achieves a higher performance than MyVLM method in generaing captions aligned with the images (CLIPScore) and incorporating the correct personalized concept names into its captions (Personalization Recall). On the \yollava dataset  our \ours outperforms Yo’LLaVA without requiring training, special tokens, or modifications to the original \VLM. %Additionally, because our method localizes the personalized object within the image precisely, it does not suffer from performance degradation with increasing token length, unlike Yo’LLaVA. 

Concerning \thisismy, all methods' VQA performance experience a drop compared to the results on the \yollava dataset, indicating the difficulty of our benchmark and room for future work. Due to this  challenging nature and objects being sometimes in the background, we observe that \yollava fails to improve over base LLaVA model. To the contrary, our \ours achieves significant improvement of ($\sim 5\%$). This highlights our method strong ability in both visual recognition and question answering. 
% We evaluate our method's ability to answer questions about the personalized objects using the benchmark provided by Yo'LLaVA~\cite{nguyen2024yo}. The VQA benchmark comprises 171 multiple choice questions (A/B) aimed at assessing a method's potential to understand the visual appearance of the personalized objects, their relationship to their immediate environment in each image and reason about them. \\
% Table \ref{tab:VQA} compares our results with those from Yo'LLaVA.
% We also added the best results of GPT4+Prompt and LLaVA+prompt from Yo'LLaVA~\cite{nguyen2024yo}  with human engineered prompt.

% Our \ours outperforms Yo'LLaVA without requiring training, special tokens or modifications to the original \VLM.  \ours performs on par with the GPT4 +human prompts.  Additionally, because our method precisely localizes the personalized object within the image, it does not suffer from performance degradation with increasing token length, unlike Yo’LLaVA.
% When a question is posed about a specific object (e.g., 'Shiba-Sleep') our method extracts the relevant features from the memory module, localizes the object proposal with the highest similarity
% (\ref{sec:retrieval}) and employs visual prompting (\ref{sec:vprompting}) to guide the \VLM for selecting the correct answer.



\begin{figure*}[t]
    \centering
    %\includegraphics[width=\textwidth]{fig/qualitative_2.pdf}
    %\includegraphics[width=0.9\linewidth]{ICCV/fig/qual4.pdf}
    \includegraphics[width=0.9\linewidth]{fig/qual4_blur.pdf}
    %\includegraphics[width=0.9\textwidth]{ICCV/fig/qualitative_vs_yollava_2.pdf}
        \vspace{-0.3cm}
        \caption{\small\textbf{Qualitative Comparison to Yo'LLaVA:} Yo’LLaVA’s prompt template requires specifying the personalized object's identifier in the query (first row), limiting generalization since users must already know which objects are in the image. Using image-level embeddings can also cause confusion between similar objects (e.g., Alex vs. Alex’s bag). Adjusting the LLM’s head weights further harms captioning quality. PeKit achieves better captioning quality without any training. In each example, the ``Query'' shows Yo’LLaVA's prompt with the concept identifier replaced by the object's name and an added system prompt. PeKit uses a different template, detailed in the Appendix.}
        \vspace{-0.55cm}
    %\caption{\small\textbf{Qualitative Results}:Comparison of our training-free method with original LLaVA captions. Right: Our method detects personalized objects and integrates prior knowledge in caption generation. Left: While the original model struggles with specific questions about named objects, our method easily identifies the referred object.}
    \label{fig:qualitative}
\end{figure*}

\vspace{-0.1cm}
\subsection{Ablations and Analysis}
\vspace{-0.1cm}
\myparagraph{Choice of $F_\text{ext}$.}
Our method employs an open-vocabulary approach to localize personalized objects in reference images. In Table~\ref{tab:recognition}, we ablate the choice of an open-world object detector, G-DINO~\cite{liu2023grounding}, compared to the open-world semantic segmentation model, G-SAM~\cite{ren2024grounded}, for~$F_\text{ext}$ in Eq.~\ref{eq:ext}. The results show that our method outperforms previous methods in both cases. However, the more precise segmentation model, which extracts only patches of the object of interest, achieves the best accuracy on average. This improvement is more pronounced on the MyVLM dataset, where objects are less centered in the training views compared to the Yo’LLaVA dataset.
% Our method employs an open-vocabulary method to localize the personalized objects in the reference images.
% In table~\ref{tab:recognition} and~\ref{tab:this-is-my}, we ablate the choice of   an open-world object detector, G-DINO~\cite{liu2023grounding} compared to the open-world semantic segmentation model, G-SAM~\cite{ren2024grounded}, for $F_{ext}$  in Eq.~\ref{eq:ext}. The results show that our method outperforms previous methods in both cases. However, the more precise segmentation model, which extracts only patches of the object of interest, achieves on average the best accuracy. This improvement is more pronounced on the MyVLM dataset, where objects are less centered in the training views compared to the YoLLaVA dataset. 

\myparagraph{Other LVLMs.} The modularity of our method allows for straightforward plug-and-play integration with any \VLM without tuning. We pair \ours with InternVL2-26B~\cite{chen2024internvl} , a state-of-the-art \VLM. Table~\ref{tab:VQA} shows VQA accuracy on the Yo’LLaVA and \thisismy datasets for \ours with LLaVA~\cite{liu2024improved} and InternVL~\cite{chen2024internvl}. With both backbones, our approach achieves SOTA results. The improved quality of InternVL~\cite{chen2024internvl}  boosts VQA accuracy on the \yollava dataset and achieves a significant   $7\%$ boost on \thisismy. \\
%\rahaf{Soroush if you want include InternVL on MyVLM and comment on it.}
\myparagraph{Qualitative VQA Results}
%\rahaf{this is to be updated with the new qualitative}
%Figure~\ref{fig:qualitative} shows examples of our \ours compared to the base LLaVA~\cite{liu2024llavanext} model on VQA and personalized captioning tasks. When LLaVA doesn’t recognize an object from the given name in the query, it guesses, leading to hallucinations or incorrect statements. 
Figure~\ref{fig:qualitative} shows examples of our \ours compared to the Yo’LLaVA for VQA and personalized captioning tasks on This-is-my-Img dataset. As seen on the first row, Yo'LLaVA's prompt template requires the query to include the target object’s concept identifier, making it unsuitable for general captioning tasks and tailored for Visual Question Answering. Besides, since Yo’LLaVA operates on image-level embeddings of reference views, it needs clutter-free object-centered reference views of the personalized objects. As seen on the second row, performance can decline if the personalized object is not in the foreground or if there are other objects/people interacting with the personalized object in the reference views. Besides, fine-tuning the last layer of the language model reduces the LVLM’s captioning capabilities for Yo'LLaVA. \\
In contrast, \ours accurately identifies objects and uses visual prompting to guide LLaVA in answering the questions of ambiguous situations where multiple object are present in the image (e.g, Nikki's car). %While more advanced prompting or personalized response examples could improve \ours, we chose simplicity and standard design, leaving such enhancements for future work.