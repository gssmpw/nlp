\begin{abstract}
\vspace{-0.8cm}

Large Vision Language Models (LVLMs) have significant potential to provide personalized assistance by adapting to the unique needs and preferences of individual users. The personalization of LVLMs has emerged as a field that focuses on customizing models to recognize specific object instances and provide tailored responses. However, current methodologies depend on time-consuming test-time training for each user and object, which proves to be impractical. This paper introduces a novel, training-free approach to LVLM personalization by leveraging pre-trained vision foundation models to extract distinct features, retrieval-augmented generation (RAG) techniques to recognize instances in the visual input, and visual prompting methods. Our model-agnostic vision toolkit enables flexible and efficient personalization without the need for extensive retraining. We demonstrate state-of-the-art results, surpassing conventional training-based approaches, and set a new benchmark for LVLM personalization.
%With the emerging abilities of Large Vision Language Models (LVLMs), there is great potential to deploy them as personal assistants tailored to individual users’ needs. LVLM personalization has emerged as a new field where models are customized to recognize users’ specific instances and provide personalized answers. Current approaches to LVLM personalization depend on test-time training for each user and object, leading to impractical solutions. This work introduces a training-free approach for LVLM personalization. We leverage pretrained vision foundation models to extract distinct features, retrieval-augmented generation (RAG) to recognize instances in the visual input, and visual prompting to instruct the model, forming a model-agnostic vision toolkit. We achieve state-of-the-art results, improving over training-based LVLM personalization approaches.
% With the emerging abilities of Large Vision Language Models (LVLMs), there is great potential to deploy them as personal assistants tailored to individual users’ needs. LVLM personalization has emerged as a new field where models are customized to recognize users’ specific instances and provide personalized answers. Current approaches to LVLM personalization depend on test-time training for each user and each object, leading to impractical solutions for their intended tasks. This work introduces a new approach for training-free personalization of Large Vision Language Models. We leverage the power of pretrained vision foundation models to extract distinct features for object instances, retrieval-augmented generation (RAG) to recognize the instance(s) present in the visual input, and visual prompting to instruct the model on how to respond, forming a model-agnostic vision toolkit. We achieve state-of-the-art results, improving over training-based LVLM personalization approaches.
% With the emerging abilities of Large Vision Language Models (LVLMs), there is great potential to deploy them as personal assistants tailored to individual users’ needs. LVLM personalization has emerged as a new field where models are customized to recognize users’ specific instances and provide personalized answers. Current approaches to LVLM personalization depend on test-time training for each user and each object, leading to impractical solutions for their intended tasks.
% This work introduces a new approach for training-free personalization of Large Vision Language Models. We leverage the power of pretrained vision foundation models to extract distinct features for object instances, retrieval-augmented generation (RAG) to recognize the instance(s) present in the visual input, and visual prompting to instruct the model on how to respond, forming a model-agnostic vision toolkit. We achieve state-of-the-art results, improving over training-based LVLM personalization approaches. 
%With the emerging abilities of Large Language Models (LLMs) and Large Vision Language Models (LVLMs), there is great potential to deploy them as personal assistants tailored to individual users’ needs. To overcome the generic nature of LVLMs and LLMs, model personalization has emerged as a new field in both language and vision research. Specifically, in the case of LVLMs, models are customized to recognize users’ specific instances and provide personalized answers. While existing LLM personalization approaches rely mostly on in-context learning and retrieval-augmented generation, current approaches to LVLM personalization depend on test-time training for each user and each object, leading to impractical solutions for their intended tasks.
%This work introduces a new approach for training-free personalization of Large Vision Language Models. We leverage the power of pretrained vision foundation models to extract distinct features for object instances, retrieval-augmented generation to recognize the instance(s) present in the visual input, and visual prompting to instruct the model on how to respond, forming a model-agnostic vision toolkit. We achieve state-of-the-art results, improving over training-based LVLM personalization approaches. 
\end{abstract}