\section{Approach}
\vspace{-0.2cm}
\label{sec:approach}
 This section outlines our personalization toolkit, coined as \textit{PeKit}, for enabling any LVLM to perform personalized object detection and answer generation. We employ a three-stage pipeline: \textbf{View Extraction} to extract robust object-level features from reference images and store them in a memory module, \textbf{Personalized Objects Retrieval} to identify objects in the query image, and \textbf{Personalized Answer Generation} via visual prompting. We refer to Fig~\ref{fig:teaser} for an illustration of our approach.
% This section outlines the components of our methodology as a toolkit to enable any large vision-language model (LVLM) to perform personalized object detection and answer generation. We extract robust representative features of a given object from reference images. This is achieved by localizing the object within the image and utilizing a discriminative vision encoder.
% Next, we use a similarity based retrieval 
% module to identify personalized objects in test images. Finally, we incorporate visual prompting to generate a personalized answer based on the precise locations of the detected objects and their object-specific prior context in the image. This approach ensures both accurate and relevant results in the personalized object detection and answer generation tasks. 
% leveraging an open-vocabulary object detection network to isolate relevant views of personalized objects from the training images.
\vspace{-0.1cm}
\subsection{Preliminary}
\vspace{-0.1cm}
We consider a set $P$ of all personalized objects introduced to a given \VLM. Each object $p\in P$ is associated with a set of reference images $\{I_p\}$, a name or identifier $n_p$ and optionally $c_p$ a context of the object.
Our objective is to generate a personalized response for all images containing $p$ during inference, while producing a general caption for any other image that does not contain any of the personalized objects.
The \VLM, e.g., LLaVA~\cite{liu2024visual} typically takes as input an image $I_p$, a text query $Q$ and additional text as context or instruction.
\vspace{-0.1cm}
\subsection{Training-free View Extraction}
\vspace{-0.1cm}
\label{sec:training_views}
Existing \VLM personalization techniques depend on image-level representations of the objects’ training views~\cite{alaluf2024myvlm,nguyen2024yo}, which can lead to overfitting to the background of each object in the reference images, particularly for training-based approaches. To avoid such a bias,
our method localizes the object in the image and extracts only its corresponding features.
% [NC] We achieve this by utilizing an open-vocabulary segmentation network ($F_\text{ext}$) to extract object-level masks ($S_p$) based on each object's semantic category ($k_p$).
We achieve this by utilizing an open-vocabulary segmentation network $F_\text{ext}$ to extract object-level masks $S_p$ based on each object's generic category $k_p$ which can be deducted from the name or the context\footnote{We refer to the Appendix for further discussion. }. 
\begin{equation}~\label{eq:ext}
    S_p = F_\text{ext}(I_p,k_p).% \quad \forall i_p \in I_p
\end{equation}
% We deploy GroundedSAM~\cite{ren2024grounded} as our segmentation network and further ablate our method with GroundingDINO~\cite{liu2023grounding} in which the mask is represented by the object's bounding box.
% [NC] We mask the embeddings generated by an image encoder ($F_\text{emb}$) on the entire image ($I_p$) with the set of object-level masks ($S_p$) followed by an average pooling operation of the patches embeddings covered by the mask.
We construct the average embedding vector $\be_p$ for object $p$ by average pooling of the embedding vectors produced by the image encoder $F_\text{emb}$ on the image $I_p$ over the region defined by the object-level mask $S_p$:
\vspace{-0.3cm}
\begin{equation}
    % [NC] \be_p=\text{AvgPool}(F_\text{emb}{(I_p),S_p)})
    \be_p=\AvgPool(F_\text{emb}{(I_p),S_p)})\in\mathbb{R}^{D_h}.
\end{equation}
% [NC] In the case of multiple ($N$) reference images  we construct  $E_p\in\mathbb{R}^{D_h \times N}$  {s.t.} the $i^{th}$ row $ E^i_{p}=e_p^i$.
Considering $N$ reference images, we concatenate all object embedding vectors $\be_p^i$ (of the object $p$ pooled over the $i$-th reference image $I_p^i$) into a matrix $E_p = \left[\be_p^1, \dots, \be_p^N\right]\in\mathbb{R}^{D_h \times N}$.

\myparagraph{Memory module.}  After extracting the personalized objects' embeddings, we store  each object's relevant properties in our  memory module.  The memory module is represented by a set $\mathcal{M}$ of object-specific entries:
\vspace{-0.15cm}
\begin{equation}
    % [NC] \mathcal{M}=\{ (E_p, (n_p, c_p)) \}_{p=1}^{|P|} 
    \mathcal{M}=\{ (E_p, (n_p, c_p)) \}_{p\in P},
\end{equation}
% \mathcal{M}=\{ (E_1, (n_1, c_1)), (E_2, (n_2, c_2)), \dots, (E_m, (n_m, c_m)) \} 
where $n_p$ is the identifier or the name of the personalized object $p$, and $c_p$ is the context of the object, which can contain prior knowledge such as characteristics, background story or even relation to other personalized objects.
When the number of personalized objects scales, the memory module $\mathcal{M}$ is easily converted into a Vector database, where nearest neighbor approximate search is deployed to retrieve instances matching a given query~\cite{han2023vector} ensuring the efficiency and scalability of our approach.

\subsection{Personalized Object Retrieval}
\vspace{-0.1cm}
\label{sec:retrieval}
%\vspace{-0.2cm}
During inference, our goal is to determine whether a personalized object is present in the provided image $I$.
We use any available object proposal technique $F_\text{prop}$ to generate a set of proposals (e.g., bounding boxes) $O=\{o_i\}_i=F_\text{prop}(I)$ for potential object occurrences within the image $I$.
Then for each proposal $o_i$, we calculate its object-level average embedding vector:
\begin{equation}
\be_{o_i} = \AvgPool(F_\text{emb}{(I),o_i}).
\end{equation}
We define the retrieval module $\mathcal{R}$ that takes an object embedding vector $\be_{o_i}$ and retrieves the memory entry $(E_j, (n_j, c_j))$ for a matching object $j$ as:% where  $E_j$ contains the most similar embedding to $\be_{o_i}$: $\similarity(E_j,\be_{o_i})= \max_{l=1, \dots, N}\left(\similarity(\be_j^l,\be_{o_i})\right)>\tau$.\rahaf{here}
%  Alternatively, the definition of the retrieval module could be introduced
% as the following equation:
% \begin{align*}
% \mathcal{R}(\be_{o_i}) &= (E_j, (n_j, c_j)),\ \text{if}\\
% j &= \arg\max_l\left\{\similarity(E_l, \be_{o_i})\right\}  \&\\
% \similarity(E_l,\be_{o_i}) &= \arg\max_k\left\{\similarity(\be_l^k, \be_{o_i})\right\}>\tau.
% \end{align*}
% %===============
\vspace{-0.2cm}
\begin{equation}
\mathcal{R}(\be_{o_i}) = 
\begin{cases} 
    (E_j, (n_j, c_j)),\\
    \quad \text{if } j = \argmax_l\left\{\similarity(E_l, \be_{o_i})\right\} \\
     \quad \text{and } \similarity(E_j,\be_{o_i}) > \tau \\
    \phi,\, \text{otherwise}
\end{cases}
\end{equation}
where $\similarity(E_l,\be_{o_i})= \argmax_k\left\{\similarity(\be_l^k, \be_{o_i})\right\}$.
We calculate the proposal's similarity to the embeddings of the training views for all personalized objects.
Any similarity measure (e.g., cosine similarity) can be employed for this purpose.
We set a constant threshold $\tau$ to identify the personalized objects.
We discard the object proposals in which no matching object is found by the retrieval module $\mathcal{R}$.
% \begin{equation}
%     p \quad
%     \begin{cases}
%         \text{detected} & \text{if } sim_{(e_{o_{I}},e_p)} > \tau  \\
%         \text{not detected} & \text{otherwise}
%     \end{cases}
%     \quad
%     \begin{rcases}
%         \forall p \in P \\
%         \forall e_p \in E_p \\
%         \forall o_{i} \in O 
%     \end{rcases}
%     \quad
% \end{equation}
 Our method inherently supports the detection of multiple personalized objects (Fig~\ref{fig:visual_prompting}).%, a capability not available in previous works~\cite{alaluf2024myvlm,nguyen2024yo} ( maybe more explanation). %In rare instances where multiple objects are assigned to the same bounding box, we retain the personalized object whose corresponding training view has the highest similarity to the object proposal's embeddings.
 \vspace{-0.1cm}
\subsection{Personalized Answer Generation}
\label{sec:vprompting}
%\vspace{-0.2cm}
Once a personalized object is identified, our method generates captions specifically about that object, distinct from general captions a standard \VLM would produce. This involves emphasizing the detected object and incorporating prior knowledge about it. We achieve this through visual prompting by overlaying bounding boxes on the image and querying the \VLM to generate captions or answer questions focused on these objects. We use distinctive colors to differentiate recognized objects. 
We provide the \VLM with the object identifier $n_j$ (e.g., the instance name) and possibly a context $c_j$ for each personalized object. The \VLM incorporates this context and responds to queries using the given name $n_j$. For multiple personalized objects, we instruct the \VLM for each bounding box, name, and context. Figure~\ref{fig:visual_prompting} illustrates how the \VLM in our approach responds to the  overlaid bounding boxes and our instruction.
We refer to the Appendix for the exact prompt format. Note that generating answers using a context pool with varied information (e.g., monument history) is inherently supported by our method.
% Once a personalized object is identified, our method generates captions specifically about that object, distinct from general captions a standard \VLM would produce. This involves emphasizing the detected object and incorporating prior knowledge about it.
% We achieve this through visual prompting by overlaying bounding boxes on the image and querying the \VLM to generate captions or answer questions focused on these objects. We use distinct colors to differentiate  recognized objects. Figure~\ref{fig:visual_prompting} illustrates how the \VLM responds to the overlaid bounding box and our instruction differently from the default response on the raw image. 
% We provide the \VLM with the object identifier $n_j$ (e.g., the instance name) and possibly a context $c_j$ for each personalized object. The \VLM incorporates this context and responds to queries using the given name $n_j$. For multiple personalized objects, we instruct the \VLM for each bounding box, name, and context. The exact prompt format is in the Appendix.
% Generating answers using a context pool with different information (e.g., monument history) is inherently  supported by our method.
% \subsection{Personalized Answer Generation}
% \label{sec:vprompting}
% Once a personalized object is identified in the image, our method must generate captions specifically about that object, distinct from the general captions a standard \VLM would produce. This means placing greater emphasis on the detected object and potentially incorporating prior knowledge about them.
% We accomplish this through visual prompting. We specify the personalized objects to the \VLM by overlaying their bounding boxes on the image and querying the \VLM to generate captions or answer questions with emphasis on the objects within these bounding boxes. 
% Figure \ref{fig:visual_prompting} demonstrates our visual prompting method. We use different colors to distinguish multiple bounding boxes, enabling us to query the \VLM to identify each instance within a specific colored bounding box separately. Initially, we ask the \VLM to generate a general caption for the entire image without any bounding boxes. Next, we show that when the \VLM is prompted to describe only the contents within the overlaid bounding boxes, it follows the instruction accordingly.

% We provide the \VLM with the object identifier $n_j$ e.g., the instance given name, and possibly a context $c_j$ for each personalized object. We instruct  the \VLM to incorporate information from this context  and respond to the user query about the personalized object using its given name $n_j$.  When multiple personalized objects are present in the image, we similarly instruct the \VLM for multiple bounding boxes, corresponding names and corresponding contexts. We refer to the Appendix for the exact prompt format. 
% It is worth mentioning that generating answers using a context pool which could contain different information (e.g location history) is a feature that is not currently supported by the previous methods.

% \begin{itemize}
%     \item context pool
% \end{itemize}% As mentioned earlier our approach relies on vision foundation models in various stages. 
\begin{figure}[t]
 \vspace{-0.5cm}
    \centering\includegraphics[width=0.5\textwidth]{fig/multiconcept.pdf}
       \vspace{-0.5cm}
    \caption{\small \textbf{Multi-concept Personalization with Visual Prompting}: \textit{PeKit} handles multiple personalized objects without additional requirements. The LVLM in PeKit uses the specified bounding boxes (via visual prompts) and instance identities, with optional context (here imaginary), for personalized caption generation.} 
    \label{fig:visual_prompting}
    \vspace{-0.5cm}
\end{figure}
%Multi-concept Personalization with Visual Prompting}: \textit{Pekit} can operate with multiple personalized objects without any additional requirements. The LVLM in PeKit is visually prompted with specified bounding boxes and given instance identities along with (optional) context. It incorporates this information for personalized caption generation,  imaginary context was provided for qualitative evaluation.
\subsection{Choice of Vision Tools}
We deploy  GroundedSAM~\cite{ren2024grounded} as the open-vocabulary segmentation network $F_\text{ext}$ and ablate using GroundingDINO~\cite{liu2023grounding}, where the mask is represented by the object’s bounding box. We use DINOv2~\cite{oquab2023dinov2} as the image encoder $F_\text{emb}$ to extract patch-level features of the personalized objects.
Image encoders trained with a self-supervised objective, such as DINOv2, produce distinctive features that improve the re-identification of personalized objects as we empirically show in the Appendix in a comparison  with CLIP~\cite{radford2021clip}.
%In the Appendix, we compare the use of DINOv2 to CLIP~\cite{radford2021clip}. 
During inference, GroundingDINO~\cite{liu2023grounding} is queried with the generic term `object' to generate object proposals.
%Note that DINO embedding was recently used in personalized image generation work~\cite{ruiz2023dreambooth} as a basis for evaluation metrics.

% \subsection{Choice of Vision Tools}
% When localizing personalization objects  we  deploy GroundedSAM~\cite{ren2024grounded} as ($F_\text{ext}$) in Eq.~\ref{eq:ext} and further ablate our method with GroundingDINO~\cite{liu2023grounding} in which the mask is represented by the object's bounding box.
% For extracting patch-level features of the personalized objects, we consider DINOv2~\cite{oquab2023dinov2}  for $F_\text{emb}$, it captures  distinctive features of objects due to its self-supervised objective, which we rely on for future recognition of the personalized objects.  Note that DINO embedding was recently used in personalized image generation work \cite{ruiz2023dreambooth} as a basis for evaluation metrics. In the Appendix we compare the use of DINOv2 to CLIP~\cite{radford2021clip} . 
% During deployment GroundingDINO~\cite{liu2023grounding} is deployed to extract object bounding boxes of known categories in the image.

% Daniel: commenting-out Section3.6 Summary
%============================================
\vspace{-0.2cm}
\subsection{Summary}
\vspace{-0.2cm}
To summarize our approach, there are two phases\\
\myparagraph{Personalized object introduction}: The user introduces  an object to the \VLM with one or more reference images, name, and optional context. We extract robust patch-level features and store the object in our memory module $\mathcal{M}$ as $(E_i, (n_i, c_i))$.\\
\myparagraph{Personalized Inference}: 
Given an image and a question $Q$, we extract object proposals and query our retrieval module $\mathcal{R}$ with the bounding boxes features.
If a match or multiple matches are found $\{(E_i, (n_i, c_i))\}$, we highlight the bounding box(es) on the query image with  unique color(s)  and instruct the \VLM to respond using $ \{(n_j,c_j)\}$.
If no match is found, the \VLM responds without further instructions.
% The user query the model regarding the object given an image, we extract object proposals and query our retrieval module R with the features of the bounding boxes. Our retrieval module R returns a matching object for the bounding boxes if any  $(E_i, (n_i, c_i))$. For each retrieved object, we indicate the bounding box in the query image with a unique color and instruct the \VLM to respond using the provided information$ (n_j,c_j)$. If no personalized object is detected, the \VLM responds with no further instructions.
% Daniel: commenting-out Section3.6 Summary
%============================================
% \subsection{Summary}
% To summarize our approach, we identify two stages. 

% \myparagraph{Personalized object introduction}:  The user introduces an object to the \VLM given  one or more reference images, name and context. Using an open segmentation or object detection network, and an expressive vision encoder, we extract patch level features and store the object in our memory module $\mathcal{M}$ represented by  $(E_i, (n_i, c_i))$.

% \myparagraph{Personalized object responses}:
% During inference, when provided with an image, we extract object proposals for possible objects and query our retrieval module $\mathcal{R}$ with the extracted features of the bounding boxes. Our retrieval module $\mathcal{R}$  returns a matching object for the given bounding boxes if any  $\{(E_j, (n_j, c_j))\}$.   For each retrieved object, we indicate the bounding box in the query image with a unique color and instruct the \VLM to response accordingly using the provided information $(n_j,c_j)$.
%  If no personalized object is detected, the \VLM  responds with no further instructions. 


%\label{sec:approach}