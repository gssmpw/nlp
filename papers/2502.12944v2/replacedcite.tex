\section{Related work and Preliminaries}
\label{sec:preliminaries}
\textbf{Zero-Shot FMs} \; Recently, many (zero-shot) FM models have been proposed for time series forecasting. Most of these models follow the same general design principles of: \textbf{a)} being a LLM-like transformer model and \textbf{b)} being pretrained on a large corpus of time series data, consisting of billions of training points ____. For instance, \textit{Chronos} ____, \textit{Moirai} ____ and \textit{TimesFM} ____ are FMs which adhere to both of these design principles. However, some newer FMs are designed differently, using non-transformer architectures and/or not being trained on real-world time series data. For example, TTM ____, VisionTS ____ and Mamba4Cast ____ all use non-transformer architectures. Additionally, Mamba4Cast is trained only on synthetic time series, while VisionTS is not trained on time series at all, using an ImageNet-pretrained masked auto-encoder ____ as its backbone. 

\textbf{Rolling Window Forecasting} \; Here, we describe the time series setting used in this work. A \emph{time series} consists of a sequence of fixed-dimension vectors $\bm{x}_t$, indexed by the time step $t$. Each dimension of $\bm{x}_t$ is called a \emph{channel}. We look at the realistic scenario of \textit{rolling window} forecasting ____, which models the deployment stage of a forecast model. In a rolling window setting, at each time step $t$ the forecaster gives a forecast for $H$ time steps into the future, where $H$ is called the \emph{forecast horizon}. To construct the forecast, the forecaster is given a \emph{context}: a look-back window of the $L$ previous time-series values, $\bm{x}_{t-L}, \ldots, \bm{x}_t$, where we call $L$ the \emph{context length}. To evaluate performance, forecasts are compared against the \emph{ground truth}, $\bm{x}_{t+1}, \ldots, \bm{x}_{t+H}$, i.e. the actual values the time series takes for the forecasted time steps.