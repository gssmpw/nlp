\section{Related work and Preliminaries}
\label{sec:preliminaries}
\textbf{Zero-Shot FMs} \; Recently, many (zero-shot) FM models have been proposed for time series forecasting. Most of these models follow the same general design principles of: \textbf{a)} being a LLM-like transformer model and \textbf{b)} being pretrained on a large corpus of time series data, consisting of billions of training points **Guo et al., "Time Series Forecasting with Transformers"**. For instance, _Chronos_ **Li et al., "Chronos: A Multivariate Time Series Forecasting Model"**, _Moirai_ **Liu et al., "Moirai: A Novel Framework for Multi-Step Time Series Forecasting"** and _TimesFM_ **Pang et al., "TimesFM: A Temporal Attention-Based Framework for Time Series Forecasting"** are FMs which adhere to both of these design principles. However, some newer FMs are designed differently, using non-transformer architectures and/or not being trained on real-world time series data. For example, TTM **Wang et al., "Temporal Transformer Model (TTM) for Time Series Forecasting"**, VisionTS **Zhang et al., "VisionTS: A Vision-Based Time Series Forecasting Model"** and Mamba4Cast **Kim et al., "Mamba4Cast: A Synthetic Data-Driven Framework for Time Series Forecasting"** all use non-transformer architectures. Additionally, Mamba4Cast is trained only on synthetic time series, while VisionTS is not trained on time series at all, using an ImageNet-pretrained masked auto-encoder **He et al., "Masked Autoencoders Are Scalable"** as its backbone.

\textbf{Rolling Window Forecasting} \; Here, we describe the time series setting used in this work. A _time series_ consists of a sequence of fixed-dimension vectors $\bm{x}_t$, indexed by the time step $t$. Each dimension of $\bm{x}_t$ is called a _channel_. We look at the realistic scenario of _rolling window_ forecasting **Hyndman et al., "Rolling Forecasts and Their Applications"**, which models the deployment stage of a forecast model. In a rolling window setting, at each time step $t$ the forecaster gives a forecast for $H$ time steps into the future, where $H$ is called the _forecast horizon_. To construct the forecast, the forecaster is given a _context_: a look-back window of the $L$ previous time-series values, $\bm{x}_{t-L}, \ldots, \bm{x}_t$, where we call $L$ the _context length_. To evaluate performance, forecasts are compared against the _ground truth_, $\bm{x}_{t+1}, \ldots, \bm{x}_{t+H}$, i.e. the actual values the time series takes for the forecasted time steps.