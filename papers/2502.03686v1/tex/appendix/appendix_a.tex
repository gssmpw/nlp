\section{Proofs}
\label{app:ftm}

\subsection{Simplification of the NDTM Objective for DDIM}
\label{subsec:ddim_proof}
We restate the theoretical result for convenience.
\begin{proposition}
    For the diffusion posterior parameterization in \citet{song2022denoisingdiffusionimplicitmodels}, the objective in Eq. \ref{eq:ndtm_cost} can be simplified as (see proof in Appendix \ref{subsec:ddim_proof}),
    \begin{align}
        \gC \leq \kappa_t^2\big\Vert \rvu_t \big\Vert_2^2 + \tau_t^2 \big\Vert \epsilon_\theta(\bar{\rvx}_t, t) - \epsilon_\theta(\rvx_t, t) \big\Vert_2^2 + w_T\Phi(\hat{\rvx}_0^t).
    \end{align}
    where $\bar{\rvx}_t = \rvx_t + \gamma \rvu_t$ is the guided state and the coefficients $\kappa_t = \frac{\gamma \sqrt{\alpha_{t-1}}}{\sqrt{\alpha_t}}$ and $\tau_t = \sqrt{1 - \alpha_{t-1} - \sigma_t^2} - \frac{\sqrt{\alpha_{t-1}(1 - \alpha_t)}}{\sqrt{\alpha_t}}$ are time-dependent scalars.
\end{proposition}
\begin{proof}
    In the case of DDIM \cite{song2022denoisingdiffusionimplicitmodels}, the diffusion posterior is parameterized as (Eqn. 12 in \citet{song2022denoisingdiffusionimplicitmodels}),
\begin{equation}
\vmu_\vtheta(\rvx_t, t) = \frac{\sqrt{\alpha_{t-1}}}{\sqrt{\alpha_t}}\rvx_t + \underbrace{\Big[\sqrt{1 - \alpha_{t-1} - \sigma_t^2} - \frac{\sqrt{\alpha_{t-1}(1 - \alpha_t)}}{\sqrt{\alpha_t}}\Big]}_{=\tau_t} \epsilon_\vtheta(\rvx_t, t).
\label{eq:ddim_post}
\end{equation}
where the diffusion noising process is parameterized as $p(\rvx_t|\rvx_0) = \gN(\sqrt{\alpha_t} \rvx_0, (1 - \alpha_t)\mI_d)$ and $\epsilon_\vtheta(\rvx_t, t)$ is a pretrained denoiser which models $\E[\epsilon|\rvx_t]$ and intuitively predicts the amount of noise added to $\rvx_0$ for a given noisy state $\rvx_t$ at time t. Additionally, for notational convenience, we denote the coefficient of the denoiser in Eq. \ref{eq:ddim_post} as $\tau_t$. Following \citet{song2022denoisingdiffusionimplicitmodels}, the coefficient $\sigma$ is further defined as,
\begin{equation}
\sigma_t = \sqrt{\frac{(1 - \alpha_{t-1})}{(1 - \alpha_t)}\Big(1 - \frac{\alpha_t}{\alpha_{t-1}}\Big)}
\end{equation}
It follows that,
\begin{align}
    \vmu_\vtheta(\rvx_t, t) &= \frac{\sqrt{\alpha_{t-1}}}{\sqrt{\alpha_t}}\rvx_t + \tau_t \epsilon_\vtheta(\rvx_t, t)\\
    \vmu_\vtheta(\rvx_t + \gamma\rvu_t, t) &= \frac{\sqrt{\alpha_{t-1}}}{\sqrt{\alpha_t}}(\rvx_t + \gamma\rvu_t) + \tau_t \epsilon_\vtheta(\rvx_t + \gamma\rvu_t, t) \\
    &= \frac{\sqrt{\alpha_{t-1}}}{\sqrt{\alpha_t}}\rvx_t + \underbrace{\frac{\gamma\sqrt{\alpha_{t-1}}}{\sqrt{\alpha_t}}}_{=\kappa_t}\rvu_t + \tau_t \epsilon_\vtheta(\rvx_t + \gamma\rvu_t, t)
\end{align}
where we denote the coefficient of the control signal $\rvu_t$ in the above equation as $\kappa_t$ for notational convenience. Consequently, the NDTM cost in Eq. \ref{eq:ndtm_cost} can be simplified for the DDIM posterior parameterization in Eq. \ref{eq:ddim_post} as,
\begin{align}
    \gC &= \big[\big\Vert \vmu_\vtheta(\rvx_t + \gamma\rvu_t, t) - \vmu_\vtheta(\rvx_t, t) \big\Vert_2^2 + w_T \Phi(\hat{\rvx}_0^t)\big] \\
    &=  \big[\big\Vert \kappa_t \rvu_t + \tau_t (\epsilon_\vtheta(\rvx_t + \gamma\rvu_t, t) - \epsilon_\vtheta(\rvx_t, t)) \big\Vert_2^2 + w_T \Phi(\hat{\rvx}_0^t)\big] \\
    &\leq^{(i)} \kappa_t^2\big\Vert \rvu_t \big\Vert_2^2 + \tau_t^2 \big\Vert\epsilon_\vtheta(\rvx_t + \gamma\rvu_t, t) - \epsilon_\vtheta(\rvx_t, t) \big\Vert_2^2 + w_T \Phi(\hat{\rvx}_0^t)
\end{align}
where $(i)$ follows from the triangle inequality. This completes the proof.
\end{proof}

\subsection{Continuous-Time Diffusion Trajectory Matching}
\label{subsec:cont_diff}



Analogous to the discrete case, we represent unguided diffusion dynamics using the following continuous-time reverse diffusion dynamics \citep{ANDERSON1982313, songscore},
\begin{equation}
d\rvx_t = \Big[f(t)\rvx_t - g(t)^2 \vs_\vtheta(\rvx_t, t)\Big]dt + g(t)d\vw_t,
\label{eq:unguided_cont_dyn}
\end{equation}
where $\vs_\vtheta(\rvx_t, t)$ is a pretrained score network.
Similarly, we parameterize the \emph{guided} continuous dynamics by inserting the control non-linearly into the score function follows,
\begin{equation}
d\rvx_t = \Big[f(t)\rvx_t - g(t)^2 \vs_\vtheta(\rvx_t, \rvu_t, t)\Big]dt + g(t)d\vw_t.
\end{equation}
Denote the unguided path measure as $\vu(\rvx(T \rightarrow 0))$ and the guided path measure as $\mu(\rvx(T \rightarrow 0)|\rvu(T \rightarrow 0))$.

Then, the optimal control problem reads, in analogy to \cref{eq:oc_cost}:
\begin{equation}
    \gC(\rvx_T, \rvu(T \rightarrow 0)) = w_T \underbrace{\E_{\mu}[\Phi(\rvx_0)]}_\text{Terminal Cost $\gC_\text{te}$} + \underbrace{\kl{\mu(x(T \rightarrow 0)|\rvx_T, \rvu(T \rightarrow 0))}{\nu(x(T \rightarrow 0)|\rvx_T)}}_\text{Transient Cost $\gC_\text{tr}$}.
    \label{eq:cont_dtm_loss}
\end{equation}
By \citep[Theorem 1 in Appendix A]{song2021maximum} (which follows from an application of Girsanov's Theorem), the transient cost reads:
\begin{align}
    \gC_\text{tr} &= \kl{\mu(x(T \rightarrow 0)|\rvx_T, \rvu(T \rightarrow 0))}{\nu(x(T \rightarrow 0)|\rvx_T)} \\&
    = \frac12 \int g(t)^2 \E_{\mu} \| \vs_\vtheta(\rvx_t, \rvu_t, t) - \vs_\vtheta(\rvx_t, t) \|^2 dt.
\end{align}
Taking the approximation that the control signal is optimized greedily, we find \cref{eq:ctdtm}.










\subsection{Extension to Flow Matching Models}
\label{subsec:flow_match}

For continuous flow matching models \citep{lipman2023flow, albergo2023building, liu2023flow} with a vector field $\vv_\vtheta(\rvx_t, t)$, 
\begin{equation}
    d\rvx_t = \vv_\vtheta(\rvx_t, t) dt,
\end{equation}
we insert the control signal into the dynamics through an additional dependence of the velocity field:
\begin{equation}
    \frac{d\rvx_t}{dt} = \vv_\vtheta(\rvx_t, \rvu_t, t).
\end{equation}
Since flow matching uses the squared loss, it is natural to regularize deviation from the unguided trajectory in terms of the velocity field:
\begin{equation}
    \gC_\text{tr} = \int \| \vv_\vtheta(\rvx_t, \rvu_t, t) - \vv_\vtheta(\rvx_t, t) \|^2 dt
    \label{eq:flow_control_cost}
\end{equation}




\section{Implementation Details}
\label{app:implm}
In this section, we include practical implementation details for the results presented in Section \ref{sec:experiments}.

\subsection{Task Details}
Here, we describe the task setup in more detail.

\textbf{Superresolution (x4)}: We follow the setup from DPS \citep{chung2022diffusion}, More specifically,
\begin{align}
    \vy \sim \gN(\vy| \mL^{f}\rvx, \sigma_y^2 \mI),\qquad
\end{align}
where $\mS^{f}$ represents the bicubic downsampling matrix with downsampling factor $f$. In this work, we fix $f$ to 4 for both datasets.

\textbf{Random Inpainting (90\%)}
We use random inpainting with a dropout probability of 0.9 (or 90\%). For this task, the forward model can be specified as,
\begin{align}
    \vy \sim \gN(\vy| \mM\vx, \sigma_y^2 \mI_d)
\end{align}
where $\mM \in \{0, 1\}^{d \times d}$ is the masking matrix.

\textbf{Non-Linear Deblurring} We use the non-linear deblurring setup from DPS. More specifically, we use the forward operator $\gF_\phi$ (modeled using a neural network) for the non-linear deblurring operation. Given pairs of blurred and sharp images, $\{\vx_i, \vy_i\}$, one can train a forward model estimator as \citep{Tran_2021_CVPR},
\begin{equation}
    \phi^* = \argmin_\phi \Vert \vy_i - \gF_\phi(\vx_i, \gG_\phi(\rvx_i, \rvy_i)) \Vert_2^2
\end{equation}
where $\gG$ extracts the kernel information from the training pairs. At inference, the operator $\gG$ can instead be replaced by a Gaussian random vector $\vg$. In this case, the inverse problem reduces to recovering $\vx_i$ from $\vy_i$. In this work, we directly adopt the default settings from DPS.

\textbf{Blind Image Deblurring (BID)} We directly adopt the setup for blind image deblurring from DMPlug (see Appendix C.4 in \citet{dmplug} for more details). More specifically, in the BID task, the goal is to recover the kernel $\vk$ in addition to the original signal $\rvx_0$ such that,
\begin{equation}
    \vy = \vk * \vx_0 + \sigma_y\rvz
\end{equation}
In this work, we adapt the default settings from DMPlug. For BID (Gaussian), the kernel size is 64 Ã— 64 with the standard deviation set to 3.0. For BID (Motion), the kernel intensity is adjusted to 0.5.

\subsection{Task Specific Hyperparameters}
Here, we provide a detailed overview of different hyperparameters for the baselines considered in this work. We optimize all baselines and our method for the best sample perceptual quality. We use the official code implementation for RED-Diff \citep{mardani2023variational} at \texttt{https://github.com/NVlabs/RED-diff}, \texttt{https://github.com/mandt-lab/c-pigdm} and \texttt{https://github.com/sun-umn/DMPlug} for running all competing baselines.

\subsubsection{DPS \citep{chung2022diffusion}} 
We adopt the DPS parameters from \citet{mardani2023variational}. More specifically, we fix the number of diffusion steps to 1000 using the DDIM sampler. We set $\eta=0.5$ for all tasks. Following \citet{chung2022diffusion}, we set,
\begin{equation}
    \zeta = \frac{\alpha}{\Vert\vy - \gA(\hat{\rvx}_0)\Vert_2^2}
\end{equation}
Table \ref{table:dps_hparams} illustrates different hyperparams for DPS on all tasks for the FFHQ and ImageNet datasets. 

\subsubsection{DDRM \citep{kawar2022denoising}}
Following \citet{kawar2022denoising}, we fix $\eta = 0.85$, $\eta_b = 1.0$, and the number of diffusion steps to 20 across all linear inverse problems.

\subsubsection{C-$\Pi$GDM \citep{pandey2024fast}} 
We set the number of diffusion steps to 20 for all tasks. It is also common to contract the reverse diffusion sampling for better sample quality by initializing the noisy state as proposed in \citet{chung2022comecloserdiffusefaster}. We denote the start time as $\tau$. We re-tune C-$\Pi$GDM for the best perceptual quality for all linear inverse problems. Table \ref{table:cpigdm_hparams} illustrates different hyperparams for linear inverse problems. We find that C-$\Pi$GDM fails to recover plausible images for random inpainting task after numerous tuning attempts.

\begin{table}[]
\centering
\begin{minipage}{0.45\textwidth}
    \caption{DPS hyperparameters used for different tasks}
    \centering
    \begin{tabular}{@{}ccc@{}}
    \toprule
                             & FFHQ      & ImageNet  \\ \midrule
    Task                     & $\alpha$  & $\alpha$  \\ \midrule
    Super-Resolution (4x)    &   1.0        & 1.0      \\
    Random Inpainting (90\%) & 1.0        & 1.0        \\
    Non-Linear Deblur        & 0.3 & 1.0 \\ \bottomrule
    \end{tabular}
    \label{table:dps_hparams}
\end{minipage}%
\hfill
\begin{minipage}{0.52\textwidth}
    \centering
    \caption{C-$\Pi$GDM hyperparameters used for different tasks. We find that C-$\Pi$GDM fails to recover plausible images for this task after numerous tuning attempts.}
    \begin{tabular}{@{}ccccccc@{}}
    \toprule
                             & \multicolumn{3}{c}{FFHQ} & \multicolumn{3}{c}{ImageNet} \\ \midrule
    Task                     & $\lambda$ & $w$ & $\tau$ & $\alpha$   & $w$   & $\tau$  \\ \midrule
    Super-Resolution (4x)    & -0.4      & 4.0 & 0.4    & -0.4       & 4.0   & 0.4     \\ \midrule
    Random Inpainting (90\%) & -         & -   & -      & -          & -     & -       \\ \bottomrule
    \end{tabular}
    \label{table:cpigdm_hparams}
\end{minipage}
\end{table}

\begin{table}[t]
\caption{RED-Diff hyperparameters used for different tasks.}
\centering
\begin{tabular}{@{}ccccc@{}}
\toprule
                         & \multicolumn{2}{c}{FFHQ} & \multicolumn{2}{c}{ImageNet} \\ \midrule
Task                     & lr       & $\lambda$     & lr         & $\lambda$       \\ \midrule
Super-Resolution (4x)    & 0.5      & 1.0           & 0.5        &   0.4              \\
Random Inpainting (90\%) & 0.5      & 0.25          & 0.5        & 0.25            \\
Non-Linear Deblur        & 0.5      & 0.25          & 0.5        & 0.25            \\ \bottomrule
\end{tabular}
    \label{table:reddiff_hparams}
\end{table}

\begin{table}[t]
\caption{BID hyperparameters for NDTM.}
\centering
\small
\begin{tabular}{cccccccc}
\toprule
\multicolumn{1}{c|}{}     & \multicolumn{7}{c}{FFHQ}                                                                              \\ \midrule
\multicolumn{1}{c|}{Task} & N  & $\gamma_t$ & $\eta$ & $\tau$ & $w_T$ & $w_\text{score}$ & \multicolumn{1}{c}{$w_\text{control}$} \\ \midrule
BID (Gaussian)            & 15 & 1.0        & 0.7    & 1000   & 50    & ddim             & ddim                                    \\ \midrule
BID (Motion)              & 15 & 1.0        & 0.7    & 1000   & 50    & ddim             & ddim                 \\\bottomrule                  
\end{tabular}
\label{table:bid_hparams}
\end{table}

\subsubsection{RED-Diff \citep{mardani2023variational}}
We set $\sigma_0=0$ with a linear weighting schedule and $lr=0.5$, $\lambda=0.25$ and perform 50 diffusion steps for all tasks across the FFHQ and ImageNet dataset. We highlight different hyperparams in Table \ref{table:reddiff_hparams}.


\begin{table}[t]
\caption{NDTM hyperparameters for different tasks.}
\centering
\small
\begin{tabular}{@{}c|ccccccc|ccccccc@{}}
\toprule
                         & \multicolumn{7}{c|}{FFHQ}                                                        & \multicolumn{7}{c}{ImageNet}                                                     \\ \midrule
Task                     & N & $\gamma$ & $\eta$ & $\tau$ & $w_T$ & $w_\text{score}$ & $w_\text{control}$ & N & $\gamma$ & $\eta$ & $\tau$ & $w_T$ & $w_\text{score}$ & $w_\text{control}$ \\ \midrule
Super-Resolution (4x)    & 5 & 1.0        & 0.7    & 400    & 50    & ddim             & ddim               & 2 & 2.0        & 0.1    & 600    & 50    & ddim             & ddim               \\
Random Inpainting (90\%) & 2 & 4.0        & 0.2    & 500    & 1     & 0                & 0                  & 2 & 4.0        & 0.0    & 600    & 50    & ddim             & ddim               \\
Non-Linear Deblur        & 5 & 5.0        & 0.1    & 400    & 1     & 0                & 0                  & 2 & 4.0        & 0.1    & 600    & 50    & ddim             & ddim               \\ \bottomrule
\end{tabular}
\label{table:ndtm_hparams}
\end{table}

\subsubsection{NDTM (Ours)}
\label{app:ndtm_config}
We use the Adam optimizer \citep{kingma2017adammethodstochasticoptimization} with default hyperparameters, fixing the learning rate to 0.01 for updating the control $\rvu_t$ across all tasks and fix the kernel learning rate in the BID task to 0.01. We refer to the loss weighting scheme in Eq. \ref{eq:ndtm_cost} as "DDIM weighting".  Moreover, we use linear decay for the learning rate. We perform 50 diffusion steps across all datasets and tasks. We tune the guidance weight $\gamma$, the number of optimization steps N, loss weighting ($w_T$, $w_\text{score}$, $w_\text{control}$), DDIM $\eta$ and the truncation time $\tau$ \citep{chung2022comecloserdiffusefaster} for best performance across different tasks. All these hyperparameters are listed in Table \ref{table:ndtm_hparams}.

\section{Additional Experimental Results}
\label{app:add_exp}

 \subsection{Evaluation on Distortion Metrics}
 In this work, we primarily optimize all competing methods for perceptual quality. However, for completeness, we compare the performance of our proposed method with other baselines on recovery metrics like PSNR and SSIM. Tables \ref{table:recovery_linear_ip} and \ref{table:recovery_nl_deblur} compare our proposed method, NDTM, with competing baselines for linear and non-linear inverse problems. We find that NDTM performs on par with other methods for the super-resolution task. However, for random inpainting and non-linear deblur, NDTM outperforms competing methods in terms of distortion metrics like PSNR. Since NDTM also outperforms existing baselines in terms of perceptual quality (see Table \ref{table:linear_ip}), our method provides a better distortion-perception tradeoff. 

\subsection{Runtime}
Below, we compare different methods in terms of the wall-clock time required for running on a single image for the superresolution task. From Table \ref{tab:runtime}, we observe that while our method requires an inner optimization loop, it is still faster than common baselines like DPS and DMPlug (see Table \ref{table:bid_runtime}).

\begin{table}[ht]
\caption{Runtime comparisons for different baselines vs NDTM for super-resolution task on both datasets. The runtime numbers are in wall-clock time (seconds) and tested on a single RTX A6000 GPU.}
\centering
\footnotesize
\setlength{\tabcolsep}{3pt}
\begin{tabular}{@{}cccccc|ccccc@{}}
\toprule
\multicolumn{1}{l}{}                     & \multicolumn{5}{c|}{FFHQ ($256 \times 256$)}       & \multicolumn{5}{c}{Imagenet ($256 \times 256$)}    \\ \midrule
                                         & DPS   & RED-diff & C-$\Pi$GDM & DDRM & NDTM (Ours) & DPS   & RED-diff & C-$\Pi$GDM & DDRM & NDTM (Ours) \\ \midrule
\multicolumn{1}{c}{Runtime (secs / Img)} & 199.1 & 5.8      & 3.68       & 1.3  & 13.6        & 399.3 & 7.1      & 16.4       & 2.4  & 38.3        \\ \bottomrule
\end{tabular}
\label{tab:runtime}
\end{table}

\begin{table}[t]
\caption{Runtime comparisons for DMPlug baseline vs NDTM for blind image deblurring (BID) task on FFHQ dataset. The runtime numbers are in wall-clock time (minutes) per image and tested on a single RTX A6000 GPU.}
\label{table:bid_runtime}
\small
\centering
\begin{tabular}{@{}ccc@{}}
\toprule
Method                     & \multicolumn{1}{c|}{Gaussian blur} & Motion blur \\ \midrule
                           & Timeâ†“                              & Timeâ†“       \\ \midrule
DMPlug                     & 51.24                              & 51.13       \\ \midrule
NDTM$^\dagger$ (Ours) & \textbf{7.17}                       & \textbf{7.17} \\ 
NDTM$^\zeta$ (Ours) & 18.07                               & 18.13       \\ \bottomrule
\end{tabular}
\end{table}


\begin{table}[ht]
\caption{Comparison between NDTM and existing methods for Linear IPs on distortion metrics like PSNR and SSIM. Missing entries indicate that the method was unstable for that specific task. \textbf{Bold}: best.}
\small
\centering
\begin{tabular}{@{}c|cc|cc|cc|cc@{}}
\toprule
\multicolumn{1}{c|}{} & \multicolumn{4}{c|}{\textbf{Super-Resolution (4x)}} & \multicolumn{4}{c}{\textbf{Random Inpainting (90\%)}} \\ \midrule
 & \multicolumn{2}{c|}{FFHQ (256 Ã— 256)} & \multicolumn{2}{c|}{Imagenet (256 Ã— 256)} & \multicolumn{2}{c|}{FFHQ (256 Ã— 256)} & \multicolumn{2}{c}{Imagenet (256 Ã— 256)} \\ \midrule
Method & PSNRâ†‘ & SSIMâ†‘ & PSNRâ†‘ & SSIMâ†‘ & PSNRâ†‘ & SSIMâ†‘ & PSNRâ†‘ & SSIMâ†‘ \\ \midrule
DPS & 29.06 & 0.832 & 23.61 & 0.676 & 27.76 & 0.832 & 20.96 & 0.657 \\
DDRM & \textbf{30.12} & \textbf{0.864} & \textbf{24.15} & \textbf{0.701} & 17.34 & 0.371 & 15.91 & 0.257 \\
RED-diff & 27.67 & 0.720 & 24.06 & 0.685 & 20.84 & 0.581 & 18.63 & 0.466 \\
C-$\Pi$GDM & 27.93 & 0.773 & 23.20 & 0.631 & - & - & - & - \\ \midrule
NDTM (ours) & 29.06 & 0.833 & 23.12 & 0.674 & \textbf{28.03} & \textbf{0.834} & \textbf{21.34} & \textbf{0.665} \\ \bottomrule
\end{tabular}
\label{table:recovery_linear_ip}
\end{table}

\begin{table}[!ht]
\caption{NDTM outperforms existing methods for Non-linear deblur on distortion metrics like PSNR and SSIM. \textbf{Bold}: best.}
\centering
\small
\begin{tabular}{@{}c|cc|cc@{}}
\toprule
      & \multicolumn{2}{c|}{FFHQ (256 Ã— 256)}  & \multicolumn{2}{c}{ImageNet (256 Ã— 256)}  \\ \midrule
Method & PSNRâ†‘ & SSIMâ†‘ & PSNRâ†‘ & SSIMâ†‘ \\ \midrule
DPS         & 8.12 & 0.262 & 6.67 & 0.156  \\
RED-diff    & 24.88 & 0.717 & 21.88 & 0.623  \\ \midrule
NDTM (ours) & \textbf{30.64} & \textbf{0.874} & \textbf{24.41} & \textbf{0.732}  \\ \bottomrule
\end{tabular}
\label{table:recovery_nl_deblur}
\end{table}

\clearpage




\begin{figure}
    \centering
    \includegraphics[width=.95\linewidth]{figures/additional_nld.pdf}
    \caption{Qualitative comparison between NDTM and competing baselines on the Non-Linear Deblurring task. NDTM better recovers the structure of the image compared to other baselines. We find that DPS is unstable for this task often generating reconstructions with no information.}
    \label{fig:add_nld}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/blind-appendix.pdf}
    \caption{Qualitative comparison between NDTM and competing baseline (DMPLug) on the blind image deblurring task. NDTM better recovers the details and structure of the image compared to the baseline. We find DMPlug introduces noisy artifacts and blurry images in some samples.}
    \label{fig:add_bid}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/additional_inp.pdf}
    \caption{Qualitative comparison between NDTM and competing baselines on the Random Inpainting (90\%) Task. NDTM better recovers the structure of the image compared to other baselines.}
    \label{fig:add_rinp}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/guidance-app_sr.pdf}
    \caption{Qualitative comparison between NDTM and competing baselines on 4x super-resolution task. NDTM better recovers the structure of the image compared to other baselines. Best viewed when zoomed in.}
    \label{fig:add_superres}
\end{figure}
