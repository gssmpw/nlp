\section{Related Work}
\label{sec:related}
\textbf{Conditional Diffusion Models.} In general, the conditional score $\nabla_{\rvx_t} \log p(\rvx_t|\vy)$ needed for guided sampling can be learned during training \citep{saharia2022palette,podell2023sdxl,rombach2022high} or approximated during inference. Here, we focus on training-free guidance during inference. 

In this context, there has been some recent progress in approximating the noisy likelihood score (see Eq. \ref{eq:class_guidance}) by approximating the diffusion posterior $p(\rvx_0|\rvx_t)$. For instance, DPS \citep{chung2022diffusion} approximates the diffusion posterior by a Dirac distribution centered on Tweedie's estimate \citep{tweedie}. This has the advantage that the guidance can be adapted to linear and non-linear tasks alike. However, due to a crude approximation, DPS converges very slowly and, in our observation, could be unstable for certain tasks (see Table \ref{deblur}). Consequently, some recent work \citep{Yu_2023_ICCV, bansal2024universal} adds a correction term on top of the DPS update rule to better satisfy the constraints. In addition to being more principled, our proposed method instead directly estimates the guided posterior at each sampling step, thus sidestepping the limitations of DPS in the first place.

More recent work \citep{song2022pseudoinverse, pandey2024fast, pokle2024trainingfree, boys2023tweedie} relies on expressive approximations of the diffusion posterior. While this can result in accurate guidance and faster sampling, a large proportion of these methods are limited to linear inverse problems. In contrast, our method can be adapted to generic inverse problems with some additional computational overhead. Lastly, another line of work in inverse problems approximates the data posterior $p(\rvx_0|\rvy)$ using variational inference \citep{zhang2018advances}. For instance, RED-diff \citep{mardani2023variational} proposes to learn an unimodal approximation to the data posterior by leveraging a diffusion prior. However, this can be too restrictive in practice and comes at the expense of blurry samples. We refer interested readers to \citet{daras2024surveydiffusionmodelsinverse} for a more detailed review of training-free methods for solving inverse problems in diffusion models.

\textbf{Optimal Control for Diffusion Models.} There has been some recent interest in exploring connections between stochastic optimal control and diffusion models \citep{berner2024an}. \citet{chen2024generative} leverage ideas from control theory for designing efficient diffusion models with straight-line trajectories in augmented spaces. Since our guided sampler can be used with any pretrained diffusion models, our approach is complementary to this line of work. More recently, SCG \citep{HuangGLHZSGOY24} leverages ideas from path integral control to design guidance schemes with non-differentiable constraints. In contrast, we only focus on differentiable terminal costs, and extending our framework to non-differentiable costs could be an important direction for future work. Lastly, \citet{rout2024rbmodulationtrainingfreepersonalizationdiffusion} propose RB-Modulation, a method based on control theory for personalization using diffusion models. Interestingly, while RB-Modulation is primarily inspired by a class of tractable problems in control theory, it is a special case of our framework in the limit of $ w \rightarrow \infty$ and $\gamma=1$. Therefore, our proposed framework is more flexible.
