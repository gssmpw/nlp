\section{Experiments}
\label{sec:experiments}
While our method serves as a general framework for guidance in diffusion models, here, we focus on solving inverse problems (IP). Through both quantitative and qualitative results, we demonstrate that our approach outperforms recent state-of-the-art baselines across various (blind) non-linear and linear inverse problems. Lastly, we emphasize key design parameters of our proposed method as ablations. We present full implementation details in Appendix \ref{app:implm}.

\textbf{Problem Setup}
Given a corruption model $\gA$ and a noisy measurement $\mathbf{y} \in R^d$ the goal is to recover the unknown sample $\rvx_0 \sim p_\text{data}$, from the degradation $\mathbf{y}=\gA (\vx_0)+\sigma_{y} \rvz, \quad\rvz \sim \gN\left(\mathbf{0}, \mI_d\right)$. For linear inverse problems, $\mathbf{y}=\gA\vx_0$ where $\gA$ can be any matrix. In the case where only the functional form of the degradation operator $\gA$ is known but its parameters are not, the problem is known as Blind IP. 

\textbf{Models and Datasets:} We conduct experiments on the FFHQ $(256 \times 256)$ \citep{ffhq} and ImageNet $(256 \times 256)$ \citep{deng2009imagenet} datasets, using a held-out validation set of $1,000$ samples from each. For FFHQ, we use the pre-trained model provided by \citet{chung2022diffusion}, and for ImageNet, we use the unconditional pre-trained checkpoint from OpenAI \citep{dhariwal2021diffusion}.

\textbf{Tasks and Metrics:}
We consider two linear and two non-linear inverse problems with noise level $\sigma_y=0.01$. For linear inverse problems, we use inpainting and super-resolution. For non-linear problems, we perform non-linear deblurring and blind image deblurring (BID) (see Appendix \ref{app:implm}).

To quantitatively evaluate our results, we report metrics optimized for perceptual quality, including Learned Perceptual Image Patch Similarity (LPIPS) \citep{lpips}, Fréchet Inception Distance (FID) \citep{fid}, and Kernel Inception Distance (KID) \citep{kid}. For completeness, recovery metrics like the Peak Signal-to-Noise Ratio (PSNR) are provided in Appendix \ref{app:add_exp}. With the exception of BID (for which we use 100 images), we evaluate all other inverse problems on 1k images.

\textbf{Baselines}
For linear tasks, we compare our proposed method against several state-of-the-art approaches, namely DPS \citep{chung2022diffusion}, RED-diff \citep{reddiff}, C-$\Pi$GDM \citep{pandey2024fast}, and DDRM \citep{kawar2022denoising}. Since the latter two are restricted to linear degradation settings, for non-linear deblurring, we only compare our method with DPS and RED-diff. We also consider a recently proposed method, DMPlug \citep{dmplug}, as a potential baseline. However, we find that DMPlug is too computationally expensive to evaluate on 1k samples  (see Appendix \ref{app:add_exp} for details on runtimes). Therefore, we only use this baseline for the BID task evaluated on 100 images from the FFHQ dataset since, to the best of our knowledge, it is currently state-of-the-art in this task.

\textbf{Sampling Setup.} We tune all samplers for the best perceptual quality. We highlight sampler parameters that are common across all tasks and discuss task-specific sampler configs in the Appendix \ref{app:implm}.
\begin{itemize}
    \item \textbf{DPS:} We adapt the tuned hyperparameters for DPS from \citep{reddiff}. More specifically, we set DDIM $\eta = 0.5$, number of sampling steps to 1000.
    \item \textbf{RED-diff:} We set $\sigma_0=0$ with a linear weighting schedule and $lr=0.5$ and perform 50 diffusion steps. We tune $\lambda$ depending on the task. 
    \item \textbf{DDRM:} We fix $\eta = 0.85$, $\eta_b = 1.0$, and the number of diffusion steps to 20 across all linear IPs.
    \item \textbf{C-$\Pi$GDM:} We set the number of diffusion steps to 20 since C-$\Pi$GDM \citep{pandey2024fast} saturates in performance within this budget. Furthermore, since \citet{pandey2024fast} report performance on noiseless inverse problems, we re-tune C-$\Pi$GDM for best perceptual quality (with $\sigma_y=0.01$) on all tasks and provide detailed hyperparameters in the Appendix.
    \item \textbf{DMPlug:} Following \citep{dmplug}, we fix diffusion steps to $3$, number of optimization steps to $10$k, and the (kernel, method) learning rates to (0.01, 0.1).  
    \item \textbf{(Ours) NDTM:} We fix the learning rate for the control, $\rvu_t$, to 0.01 across all tasks and apply a linear decay schedule over optimization steps. We tune the guidance weight $\gamma$, loss weighting, and DDIM $\eta$ for best performance across different tasks. We choose the number of optimization steps $N$ based on a tradeoff between sample quality and compute.
\end{itemize}




\subsection{Non-linear Inverse Problems}
\textbf{Non-Linear Deblurring.} 
As a first non-linear IP, we consider non-linear debluring with the same setup as in \citet{chung2022diffusion}. Figure \ref{fig:nld_bid_res} (Left) illustrates the comparison between competing baselines and our proposed method, NDTM, on this task. Qualitatively, similar to \citet{mardani2023variational}, we find that DPS is very sensitive to guidance step size and is usually unstable on this task. Moreover, while RED-diff does not have stability issues, we find that it is biased towards generating blurry samples. This is not surprising given their unimodal approximation to the data posterior $p(\rvx_0|\rvy)$. On the contrary, NDTM generates high-fidelity reconstructions with a stable sampling process. Similarly, our quantitative results in Table~\ref{deblur} validate our qualitative findings as our method outperforms competing baselines on perceptual quality for both datasets by a significant margin.

    

\begin{table}[t]
\centering
\caption{Comparisons on \textbf{noisy Non-linear Deblur}. NDTM outperforms competing baselines by a significant margin. \textbf{Bold}: best.}
\setlength{\tabcolsep}{4pt}
\resizebox{\columnwidth}{!}{
\begin{tabular}{@{}ccccccc@{}}
\toprule
      & \multicolumn{3}{c|}{FFHQ (256 × 256)}          & \multicolumn{3}{c}{ImageNet (256 × 256)}       \\ \midrule
    Method        & LPIPS↓ & FID $\downarrow$ & KID $\downarrow$ & LPIPS↓ & FID $\downarrow$ & KID $\downarrow$ \\ \midrule
DPS         & 0.752  & 249.01           & 0.139            & 0.888  & 346.82           & 0.2186          \\
RED-diff    & 0.362  & 64.57            & 0.0363           & 0.416  & 78.07            & 0.0224          \\ \midrule
NDTM (ours) & \textbf{0.046} & \textbf{14.198} & \textbf{0.0004}    & \textbf{0.163}      & \textbf{34.31}                & \textbf{0.0032}               \\ \bottomrule
\end{tabular}
}
\label{deblur}
\end{table}

\begin{table}[t]
\caption{Comparisons on \textbf{noisy Blind Image Deblurring (BID)} for the FFHQ 256$\times$256 dataset. NDTM outperforms DMPLug \citep{dmplug} by a significant margin while requiring an order of magnitude less sampling time (reported in minutes/img). \textbf{Bold}: best. $\dagger$: N=15, T=200, $\zeta$: N=15, T=400.}
\small
\centering
\setlength{\tabcolsep}{4pt}
\resizebox{\columnwidth}{!}{
\begin{tabular}{@{}ccccccc@{}}
\toprule
                           & \multicolumn{3}{c|}{Gaussian blur}              & \multicolumn{3}{c}{Motion blur}                 \\ \midrule
Method                     & LPIPS↓         & FID↓           & Time↓         & LPIPS↓         & FID↓          & Time↓          \\ \midrule
DMPlug                     & 0.147          & 69.36          & 51.24         & 0.118          & 72.85         & 51.13          \\ \midrule
NDTM$^\dagger$ (Ours) & 0.103          & 55.15          & \textbf{7.17} & 0.086          & 49.99         & \textbf{7.17} \\
NDTM$^\zeta$ (Ours) & \textbf{0.083} & \textbf{47.34} & 18.07          & \textbf{0.063} & \textbf{38.6} & 18.13           \\ \bottomrule
\end{tabular}
}
\label{table:bid}
\end{table}

\textbf{Blind Image Deblurring (BID).} Next, we extend our framework to blind image deblurring, maintaining the same setup as DMPlug \citep{dmplug}, which is the state-of-the-art method for this task. Interestingly, adapting our proposed method for blind inverse problems involves jointly optimizing the unknown blur kernel parameters along with the control $\rvu_t$. More specifically, for degradation of the form $\vy = \vk * \rvx_0 + \sigma_y \rvz$ with unknown blurring kernel $\vk$, we update line 11 in Algorithm \ref{alg:ndtm_ddim} as $\gC_\text{terminal} = w_T\Phi(\hat{\rvx}_0^{(i)}, k^{(i)})$ and optimizing for the trainable kernel for each image, as $\vk^{(i+1)} \gets \texttt{Update}(\vk^{(i)}, \nabla_{k} \gC_t^{(i)}$).

Figure \ref{fig:nld_bid_res} (Right) illustrates the comparison between DMPlug and NDTM adapted for this task. Qualitatively, we find that while DMPlug can introduce artifacts in generating reconstructions, NDTM generates high-quality reconstructions. Table~\ref{table:bid} further validates our qualitative findings as our method outperforms DMPlug on perceptual quality metrics. More interestingly, while DMPlug is extremely expensive for a single image, our method outperforms the former on sample quality by a significant margin while being an order of magnitude faster. This illustrates that our sampler has a more efficient way to trade sampling speed for quality. We additionally present quantitative results for a more efficient configuration ($N=15, T=200$) of our proposed method that maintains strong performance in Table \ref{table:bid}, with corresponding qualitative results provided in Appendix~\ref{app:add_exp}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/bid.pdf}
    \caption{\textbf{NDTM outperforms competing baselines on noisy blind image deblurring (BID) with Gaussian (top) and Motion (bottom) kernels}. NDTM accurately captures most details, while competing methods introduce artifacts in the generated reconstructions.}
    \label{fig:nld_bid_res}
\end{figure}





    







\begin{table*}[]
\caption{NDTM performs on-par/better than competing baselines on noisy linear inverse problems. Missing entries indicate unstable performance after multiple tuning attempts \textbf{Bold}: best.}
\small
\centering
\setlength{\tabcolsep}{4pt}
\begin{tabular}{@{}ccccccc|cccccc@{}}
\toprule
\multicolumn{7}{c|}{\textbf{Super-Resolution (4x)}}                                                                       & \multicolumn{6}{c}{\textbf{Random Inpainting (90\%)}}                                                       \\ \midrule
      & \multicolumn{3}{c|}{FFHQ (256 × 256)}                & \multicolumn{3}{c|}{Imagenet (256 × 256)}            & \multicolumn{3}{c|}{FFHQ (256 × 256)}                & \multicolumn{3}{c}{Imagenet (256 × 256)}             \\ \midrule
     Method       & LPIPS↓         & FID $\downarrow$ & KID $\downarrow$ & LPIPS↓         & FID $\downarrow$ & KID $\downarrow$ & LPIPS↓         & FID $\downarrow$ & KID $\downarrow$ & LPIPS↓         & FID $\downarrow$ & KID $\downarrow$ \\ \midrule
DPS         & 0.061          & 20.61            & 0.0029           & 0.195          & 30.67            & 0.0021           & \textbf{0.058} & 20.24            & \textbf{0.0019}  & 0.152          & 32.56            & 0.0023           \\
DDRM        & 0.116          & 36.13            & 0.0183           & 0.325          & 52.76            & 0.0151           & 0.582          & 167.57           & 0.1530           & 0.791          & 211.66           & 0.1517           \\
RED-diff    & 0.151          & 41.54            & 0.0179           & 0.354          & 51.83            & 0.0084           & 0.430          & 155.49           & 0.1370           & 0.633          & 218.88           & 0.1531           \\
C-$\Pi$GDM  & 0.106          & 29.61            & 0.0073           & 0.270          & 39.96            & 0.0024           & 0.551          & 137.85           & 0.1020           &    -            &       -           &            -      \\ \midrule
NDTM (ours) & \textbf{0.054} & \textbf{18.99}   & \textbf{0.0019}  & \textbf{0.158} & \textbf{28.75}   & \textbf{0.0011}  & 0.059          & \textbf{20.11}   & 0.0020           & \textbf{0.149} & \textbf{30.43}   & \textbf{0.0018}  \\ \bottomrule
\end{tabular}
\label{table:linear_ip}
\end{table*}

\subsection{Linear Inverse Problems}
Lastly, we compare competing methods on linear inverse problems: (4x) Super-resolution and Random inpainting with a 90\% masking probability. As illustrated in Table \ref{table:linear_ip}, for super-resolution, NDTM outperforms competing baselines for both datasets. For random inpainting, our method performs comparably with DPS on the FFHQ dataset. However, for a more difficult benchmark like ImageNet, NDTM outperforms the next best-competing baseline, DPS on this task. We present additional qualitative results for linear inverse problems in Appendix \ref{app:add_exp}

\subsection{Ablation Studies}

\begin{figure*}
    \centering
    \subfloat[Terminal Weight ($w_T$)]{\includegraphics[width=0.24\textwidth]{figures/ablation_plot_wT.png}}
    \subfloat[Guidance Weight ($\gamma$)]{\includegraphics[width=0.24\textwidth]{figures/ablation_plot_gamma_t.png}}
    \subfloat[Optimization Steps (N)]{\includegraphics[width=0.24\textwidth]{figures/ablation_plot_M.png}}
    \subfloat[Runtime vs N]{\includegraphics[width=0.24\textwidth]{figures/ablation_plot_runtime.png}}
    \caption{\textbf{Impact of different design choices} in NDTM on Distortion (PSNR) and Perception (LPIPS) for the non-linear deblur task. (a, b) The extent of guidance can be jointly controlled by varying the terminal loss weight ($w_T$) and the weight ($\gamma$). (c, d) Compute vs quality can be traded off by jointly varying the number of optimization steps (N) and the number of diffusion steps.}
    \label{fig:ablation}
\end{figure*}

Next, we analyze the impact of different design choices in NDTM on the perception (LPIPS) and distortion (PSNR) quality for the non-linear deblur task on the ImageNet dataset.

\textbf{Impact of Guidance.} Since the terminal cost weight $w_T$ and the parameter $\gamma$ affect the optimization of the variational control parameters $\rvu_t$, we analyze their impact on sample quality. From Fig. \ref{fig:ablation}a, we observe that increasing the terminal weight $w_T$ leads to an improvement in both perceptual and distortion quality. However, in the limit of $w_T \rightarrow \infty$ (i.e., where the regularization terms in Eq. \ref{eq:ntmc_ddim} can be ignored), the perceptual quality degrades, which highlights the importance of the transient cost in our framework. Similarly, increasing $\gamma$ also leads to an improved sample quality. However, a large $\gamma$ can also lead to overshooting which can lead to a degradation in sample quality.

\textbf{Impact of Optimization Steps.} It is common to trade sample quality for the number of sampling steps in diffusion models. Interestingly, NDTM provides a complementary axis to achieve this tradeoff in the form of adjusting the number of optimization steps per diffusion update. We illustrate this in Fig. \ref{fig:ablation}(b), where for a fixed sampling budget of 50 diffusion steps, NDTM can achieve better reconstruction quality by increasing the number of optimization steps (N). However, since the runtime increases linearly as N grows (see Figure \ref{fig:ablation}(d)), a practical choice depends on the available compute. We find that for this task, N=2 provides a favorable tradeoff between sampling time and quality and, therefore, use it for state-of-the-art comparisons on the ImageNet dataset in Table \ref{deblur}.

\begin{figure}[th]
    \centering
    \includegraphics[scale=0.36]{figures/controls.png}
    \caption{\textbf{The optimal variational controls hierarchically refine image features over time}. (Top Row) Non-Linear Deblur (Bottom Row) Random Inpainting. (Left to Right) We visualize optimal controls at different times $t_0 > t_1 > t_2 > t_3$ in diffusion sampling, progressively capturing coarse to fine details.}
    \label{fig:control_viz}
\end{figure}

\textbf{Control Visualizations}
We visualize the optimal controls $\rvu_t^*$ in Figure \ref{fig:control_viz}. We observe a hierarchical refinement of image features over time. More specifically, the control inference captures global structure at the start of diffusion sampling and gradually refines local details (like edges), thereby encoding high-frequency information at later steps.
