\section{Introduction}
\label{sec:intro}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\linewidth]{figures/final_figure.pdf}
    \caption{\textbf{Our method guides diffusion sampling to fulfill external constraints.} To this end, we optimize the local direction~$\rvu_t^*$ via external constraints while respecting the original trajectory, see \cref{eq:dtm_cost_final} (left, center). This recovers more accurate reconstructions accross tasks compared to classical guidance methods: Nonlinear deblurring (Right). Our method accurately captures most details, while competing methods introduce artifacts in the generated reconstructions.}
    \label{fig:main_fig}
\end{figure*}
Diffusion models \citep{sohl2015deep, ho2020denoising, songscore} and related families \citep{lipman2023flow, albergo2023building, liu2023flow} exhibit excellent synthesis quality in large-scale generative modeling applications. Additionally, due to their principled design, these models exhibit great potential in serving as powerful generative priors for downstream tasks \citep{daras2024surveydiffusionmodelsinverse}.

Consequently, guidance in diffusion models has received significant interest.
However, the dominant approaches to classifier guidance \citep{dhariwal2021diffusion} and classifier-free guidance \citep{ho2022classifier} require training additional models or retraining diffusion models for each conditioning task at hand, or are based on simplistic assumptions detrimental to sample quality \citep{kawar2022denoising, chung2022diffusion, song2022pseudoinverse, pandey2024fast}.


In this work, we generalize classifier guidance to a \emph{variational control} problem \citep{kappen2008stochastic}. Inspired by ideas such as  \emph{Control as Inference} \citep{Kappen_2012, levine2018reinforcementlearningcontrolprobabilistic}, we model guided diffusion dynamics as a Markov chain with the control signals defined as variational parameters. Variational inference is applied to optimize the control signals, ensuring that the process satisfies the desired terminal conditions while keeping the generated samples close to the unconditional sample manifold, see \cref{fig:main_fig}a. We denote this framework as \emph{Diffusion Trajectory Matching} (DTM).


Recent work on steering diffusion models has already incorporated ideas from optimal control \citep{HuangGLHZSGOY24, rout2024rbmodulationtrainingfreepersonalizationdiffusion}. However, these works focus on a restricted class of control problems. This obscures the available design choices revealed through our novel framework. Indeed, we find that DTM generalizes and explicitly contains a large class of prior work on guidance. We demonstrate the utility of this generalization by introducing a new sampling algorithm that seamlessly integrates with state-of-the-art diffusion model samplers like DDIM \citep{song2022denoisingdiffusionimplicitmodels} and adapts well to diverse downstream tasks.

To summarize, we make the following contributions:


\begin{itemize}
    \item We propose \emph{Diffusion Trajectory Matching (DTM)}, a generalized framework for training-free guidance based on a variational control perspective. DTM subsumes many existing and novel guidance methods.

    \item We instantiate our framework as \emph{Non-linear Diffusion Trajectory Matching (NDTM)}, which can be readily integrated with samplers like DDIM.

    \item NDTM outperforms previous state-of-the-art baselines for solving challenging noisy linear and (blind) non-linear inverse problems such as superresolution and inpainting with diffusion models \citep{daras2024surveydiffusionmodelsinverse} on FFHQ-256 \citep{8977347} and ImageNet-256 \citep{deng2009imagenet}. 
\end{itemize}

