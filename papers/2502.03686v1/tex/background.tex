\section{Background}
\label{sec:background}
\textbf{Diffusion Models.} Given a perturbation kernel $p(\rvx_t|\rvx_0)=\gN(\mu_t\rvx_0, \sigma_t^2\mI_d)$, diffusion models \citep{sohl2015deep, ho2020denoising} invert the noising process by learning a corresponding reverse process parameterized as,
\begin{equation}
\label{eq:unguided-diffusion}
\gQ: q(\rvx_{0:T-1}|\rvx_T) = \prod_{t} q(\rvx_{t-1}|\rvx_t).
\end{equation}
The reverse diffusion posterior is specified as $q(\rvx_{t-1}|\rvx_t) = \gN(\vmu_\theta(\rvx_t, t), \sigma_t^2\mI_d)$ where $\vmu_\vtheta(.,.)$ is learned via score matching \citep{hyvarinen2005estimation,vincent2011connection,song2019generative}. Analogously continuous-time diffusion models \citep{songscore, karraselucidating} assume that a \textit{forward process}
\begin{equation}
    d\rvx_t = f(t)\rvx_t \, dt + g(t) \, d\rvw_t, \quad t \in [0, T],
    \label{eqn:fwd_process}
\end{equation}
with an drift $f(t)$ and diffusion coefficients $g(t)$ and standard Wiener process $\rvw_t$, converts data $\rvx_0 \in \R^d$ into noise $\rvx_T$. A \textit{reverse } SDE specifies how data is generated from noise \citep{ANDERSON1982313, songscore},
\begin{equation}
    d \rvx_t = \left[f(t)\rvx_t - g(t)^2 \nabla_{\rvx_t} \log p_t(\rvx_t)\right] \, dt + g(t) d\bar \rvw_t,
    \label{eq:reverse_time_diffusion}
\end{equation}
which involves the \textit{score} $\nabla_{\rvx_t} \log p_t(\rvx_t)$ of the marginal distribution over $\rvx_t$ at time $t$. The score is intractable to compute and is approximated using a parametric estimator $\vs_{\theta}(\rvx_t, t)$, trained using denoising score matching.

\textbf{Classifier Guidance in Diffusion Models.} Given a pretrained diffusion model $\vs_{\theta}(\rvx_t, t)$, it is often desirable to guide the diffusion process conditioned on input $\vy$. Consequently, the conditional diffusion dynamics read
\begin{equation}
    d \rvx_t = \big[f(t)\rvx_t - g(t)^2 \nabla_{\rvx_t} \log p(\rvx_t|\vy)\big] dt + g(t)d\bar{\vw}_t.
\end{equation}
In classifier guidance \citep{dhariwal2021diffusion}, the conditional score can be decomposed as 
\begin{equation}
    \nabla_{\rvx_t} \log p(\rvx_t|\vy) = \vs_{\theta}(\rvx_t, t) + \rho_t \nabla_{\rvx_t} \log p(\vy|\rvx_t).
    \label{eq:class_guidance}
\end{equation}
where $\rho_t$ is the guidance weight. The noisy likelihood score is often estimated by training a noise-conditioned estimator. It is also common to estimate this likelihood via $p(\vy|\rvx_t)=\int p(\rvx_0|\rvx_t)p(\rvy|\rvx_0) d\rvx_0$. For example, Diffusion Posterior Sampling (DPS) \citep{chung2022diffusion} approximates the diffusion posterior as, $p(\rvx_0|\rvx_t) = \delta(\E[\rvx_0|\rvx_t])$, where $\E[\rvx_0|\rvx_t]$ is Tweedie's estimate of the posterior at $\rvx_t$ \citep{tweedie}. This approximation of the diffusion posterior in DPS results in a high sampling budget and high sensitivity to the gradient weight $\rho_t$. More expressive approximations \citep{song2022pseudoinverse, pandey2024fast} result in specificity to linear inverse problems. We refer to \citet{daras2024surveydiffusionmodelsinverse} for an in-depth discussion on explicit approximations of the diffusion posterior. We will show in Section \ref{sec:classifier-guidance} that classifier guidance in diffusion models is a special case of our proposed framework. Next, we discuss our proposed framework which can be directly applied to pretrained diffusion models and generalizes to linear and (blind) non-linear inverse problems.
