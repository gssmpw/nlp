\section{Related Work}
\label{sec:rel_work}

In this section, we discuss related work on constrained OCO, projection-free OCO, and constrained optimization.

\subsubsection{Constrained OCO}

____ studied OCO with a fixed convex constraint function $g$, and gave an algorithm with $\Oc(\sqrt{T})$ regret and $\Oc(T^{3/4})$ cumulative violation $\sum_{t=1}^T g(\bx_t)$ that used only first-order constraint feedback, $g_t = g(\bx_t), \bs_t \in \partial g(\bx_t)$.
This result was then generalized to $\Oc(T^{\max(\beta,1-\beta)})$ regret and $\Oc(T^{1 - \beta/2})$ cumulative violation for any $\beta \in (0,1)$ by ____.
The same bounds were shown for a stronger notion of constraint violation $\sum_{t=1}^T [g(x_t)]_+$ by ____, who also guaranteed that $\sum_{t=1}^T ([g(x_t)]_+)^2 = \Oc(T^{1 - \beta})$.
Finally, ____ showed $\Oc(\sqrt{T})$ regret and $\Oc(\sqrt{T})$ cumulative violation in the same setting.
With the additional assumption that the constraint gradient is lower bounded near the constraint boundary (Assumption 1 in ____), the aforementioned results can be extended to guarantee no cumulative violation $\sum_{t=1}^T g(\bx_t) \leq 0$ as stated in Table \ref{tbl:comp}.
We use the same assumption to show constraint satisfaction for all rounds $g(x_t) \leq 0\ \forall t$ provided that there is a known strictly-feasible point.
Furthermore, the aforementioned works use primal-dual algorithms, which are fundamentally different from our approach of Polyak feasibility steps.
In particular, our approach uses the Polyak step-size and a \emph{single} sequence of iterates, while primal-dual algorithms use two iterate sequences that are linked via the cost and constraint functions.

There is also a line of literature on OCO with constraints that solves an optimization problem involving the constraint function in each round, e.g. ____.
This differs from our algorithm and those compared in Table \ref{tbl:comp}, which only access the constraint with first-order feedback at the played actions.
We note that solving an optimization problem with the constraint function in each round can introduce significant computational cost.

Lastly, we point out that a related line of literature considers OCO with time-varying constraints, e.g. ____.
This generalizes the fixed constraints that we consider.
Nonetheless, to our knowledge, none of these works improve on the guarantees shown in Table \ref{tbl:comp} for the case of non-smooth constraints, and first-order feedback.
We also point out that ____ gives an algorithm that uses first-order feedback and enjoys bounds on the violation in each round, i.e. $g(x_t) \leq \Oc(\frac{1}{\sqrt{t}}$).
However, this work requires that constraint functions are smooth (i.e. have Lipschitz gradients), making it distinct from our work and those considered in Table \ref{tbl:comp}.
Furthermore, ____ uses a projection on to a polytope in each round, which introduces additional computational cost.

\subsubsection{Projection-free OCO}

In parallel to the literature on constrained OCO, there is a line of literature that considers projection-free OCO where the feasible set is not treated as the sub-level set of a function, but rather defined as an arbitrary convex set.
____ initiated this literature by giving an algorithm that accesses the feasible set via a linear optimization oracle (LOO) instead of using projections.
This is advantageous because the LOO is often computationally cheaper than the projection.
LOO-based algorithms have received a significant amount of attention, e.g. ____.
The state-of-the-art for LOO-based algorithms with general convex costs and general convex feasible sets is $\Oc(T^{3/4})$ regret and $1$ oracle call per a round ____.
Although our methods give smaller $\Oc(\sqrt{T})$ regret, we note that the first-order feedback that we use is generally incomparable to the LOO oracle in terms of computationally complexity.
Indeed, the first-order feedback is cheaper to compute for some constraint functions, while the LOO is cheaper to compute for other constraint functions.\footnote{An example of this, pointed out by ____, is the difference in computational complexity for first-order information and LOO for the nuclear norm ball $\Bb_*$ and spectral norm ball $\Bb_2$ in the space of matrices. Computing first-order information for $\Bb_*$ and $\Bb_2$ is at worst a full-rank SVD (which is expensive) and rank-one SVD (which is cheap), respectively. For the LOO, the opposite is true in that $\Bb_*$ requires a rank-one SVD and $\Bb_2$ a full-rank SVD.}

There is also a growing body of literature that uses the membership oracle (MO) or separation oracle (SO) to access the feasible set, e.g. ____.
The MO and SO are defined as follows.
Given a query point $\bx \in \Rb^d$, the MO specifies whether or not $\bx$ is in the feasible set, while the SO returns a hyperplane that separates $\bx$ from the feasible set (if $\bx$ is not in the feasible set).
If the feasible set is the sub-level set of a constraint function $g$ (as we consider), then the MO can be constructed by checking if $g(\bx) > 0$, and the SO can be constructed using the first-order information at $\bx$, i.e. $g(\bx),\partial g(\bx)$.\footnote{Given the query point $\bx$ and first-order information at this point $g = g(\bx),\bs \in \partial g(\bx)$, it follows from convexity that $\{ \by \in \Rb^d : g + \bs^\top(\by - \bx) \leq 0\}$ is a separating hyperplane w.r.t. $\Xc = \{ \bx \in \Rb^d : g(\bx) \leq 0 \}$.}
We use first-order information only at the played actions, which can therefore only be used to construct a separation oracle and membership oracle \emph{at the played action}.
This is distinct from existing MO and SO-based algorithms, which query the SO and MO at arbitrary points (not just the played actions).
Furthermore, existing MO-based and SO-based algorithms require \emph{multiple} oracle calls per a round.
Specifically, existing MO-based algorithms use $\Oc(d \log(T))$ oracle calls per a round ____, and existing SO-based algorithms use $\Oc(\log(T))$ oracle calls per a round ____ or $\Oc(\kappa)$ oracle calls per a round ____.
Note that we state these bounds for unrestricted $T$ and use $\kappa$ to refer to the eccentricity of the feasible set, i.e. $\kappa = R/r$ with $r \Bb \subseteq \Xc \subseteq R \Bb$, which can be arbitrarily large.
The fact that our algorithm only requires $1$ oracle call per a round can result in significant performance advantages over these methods, particularly when the constraint is costly to evaluate.
Furthermore, our approach is applicable to settings where there is only \emph{local} constraint information available.

\subsubsection{Constrained Optimization}

Our approach is inspired by a line of literature in constrained (offline) optimization that uses the Polyak step-size to ensure convergence to the feasible set, e.g. ____.
However, we point out two key difficulties that arise in the OCO setting: (a) the suboptimality gap of the iterates $f_t(\bx_t) - f_t(\bx^\star)$ is \emph{not} guaranteed to be non-negative, and (b) the constraint feedback is at the played action $\bx_t$ and not at the ``intermediate'' iterate (labeled $\by_t$ in Algorithm \ref{alg:ogd_pfs}).
Challenge (a) is particularly difficult to handle because the analysis approach used by ____ (and following works) relies on the suboptimality gap of the iterates being non-negative.
As a result, we require a new analysis approach.
However, our approach does suffer a larger dependence on the problem parameters (such as the subgradient bound $G_g$), which can be seen as the ``cost'' of the adversarial online setting.
To handle challenge (b), we use the first-order approximation of the cost function $g_t + \bs_t^\top (\by_t - \bx_t)$ in the Polyak step-size instead of the true cost function $g(\by_t)$.
This first-order approximation avoids the need for constraint information at the intermediate iterate $\by_t$, while maintaining the advantageous properties of the Polyak step-size (see Section \ref{sec:feas_anal} for the details).
Overall, we believe that these analysis techniques will be of interest to the constrained optimization literature, beyond just OCO.