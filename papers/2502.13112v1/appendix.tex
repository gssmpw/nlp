\onecolumn

% \startcontents[appendices]
% \printcontents[appendices]{l}{1}{\section*{Contents}}

\appendix

\section{Extension of Prior Work to No Cumulative Constraint Violation}
\label{apx:ext}

In this section, we show how the prior work \citet{yu2017online} and \citet{yuan2018online} can be extended to show cumulative constraint satisfaction $\sum_{t=1}^{T} g(\bx_t) \leq 0$ under Assumption \ref{ass:curve} (which is the same as Assumption 1 in \citet{mahdavi2012trading}).
This follows a similar approach to Theorem 8 in \cite{mahdavi2012trading}, but we give it for completeness.

The algorithm in \citet{yu2017online} guarantees $\reg_T \leq C_R \sqrt{T}$ and $\sum_{t=1}^{T} g(\bx_t) \leq C_V \sqrt{T}$ for some constants $C_R, C_V > 0$.
Therefore, by applying this algorithm to the tightened constraint $g_\rho (\bx) := g(\bx) + \rho$, we can guarantee that $\sum_{t=1}^{T} g_\rho (\bx_t) \leq C_V \sqrt{T}$ and,
\begin{equation}
    \label{eqn:tight_reg}
    \reg_T^\rho := \sum_{t=1}^{T} f_t(\bx_t) - \min_{\bx \in \Xc_\rho} \sum_{t=1}^{T} f_t(\bx) \leq C_R \sqrt{T},
\end{equation}
where $\Xc_\rho$ is the sub-level set of the tightened constraint defined in \eqref{eqn:tight}.
Choosing $\rho = \min\left( \epsilon, \frac{C_V}{\sqrt{T}} \right)$ (where $\epsilon$ is defined in Assumption \ref{ass:curve}) and taking $T \geq \frac{C_V^2}{\epsilon^2}$ ensures that,
\begin{equation}
    \label{eqn:tightening}
    \sum_{t=1}^{T} g (\bx_t)  = \sum_{t=1}^{T} g_\rho (\bx_t) - \rho T \leq C_V \sqrt{T} - \min\left( \epsilon, \frac{C_V}{\sqrt{T}} \right) T = 0.
\end{equation}
Then, with $\bx_\rho^\star \in \argmin_{\bx \in \Xc_\rho} \sum_{t=1}^{T} f_t(\bx)$,
\begin{equation*}
    \reg_T = \reg_T^\rho + \sum_{t=1}^{T} (f_t(\bx_\rho^\star) - f_t(\bx^\star)) \leq \reg_T^\rho + \sum_{t=1}^{T} (f_t(\Pi_{\Xc_\rho}(\bx^\star)) - f_t(\bx^\star)) \leq \reg_T^\rho + T G_g \dist(\bx^\star, \Xc_\rho) 
\end{equation*}
Then, applying Lemma \ref{lem:error_bound} and \eqref{eqn:tight_reg}, and using the fact that $g(\bx^\star) \leq 0$ yields,
\begin{equation*}
    \reg_T \leq C_R \sqrt{T} + T \frac{G_g}{\sigma} [g(\bx^\star) + \rho]_+ \leq C_R \sqrt{T} + T \frac{G_g}{\sigma} \rho \leq C_R \sqrt{T} + \frac{G_g C_V}{\sigma} \sqrt{T}.
\end{equation*}
Therefore, we have shown that the algorithm from \citet{yu2017online} can be extended to show $\Oc(\sqrt{T})$ regret and cumulative constraint satisfaction $\sum_{t=1}^{T} g(\bx_t) \leq 0$, provided that Assumption \ref{ass:curve} holds and that $T$ is sufficiently large, i.e. $T \geq \frac{C_V^2}{\epsilon^2}$.
Note that the requirement that $T$ is sufficiently large is also required by other works that guarantee cumulative constraint satisfaction, e.g. \cite{mahdavi2012trading,jenatton2016adaptive}.

We can use a similar process to give guarantees for \citet{yuan2018online}.
The original guarantees in \citet{yuan2018online} are of the form $\reg_T \leq C_R T^{\max(\beta,1-\beta)}$ and $\sum_{t=1}^{T} [g(\bx_t)]_+ \leq C_V T^{1 - \beta/2}$.
Therefore, choosing $\rho = \min\left( \epsilon, \frac{C_V}{T^{\beta/2}} \right)$ and taking $T \geq \frac{C_V^{2/\beta}}{\epsilon^{2/\beta}}$ ensures that $\sum_{t=1}^{T} g(\bx_t) \leq 0$.
Also, the resulting regret is,
\begin{equation*}
    \reg_T \leq C_R T^{\max(\beta,1-\beta)} + \frac{G_g C_V}{\sigma} T^{1 - \beta/2}.
\end{equation*}
The order of the bound $\max(\beta,1-\beta,1-\beta/2)$ is minimized when $\beta = \frac{2}{3}$ and therefore, we get that $\reg_T = \Oc(T^{2/3})$.
Note that \citet{yuan2018online} also gives a bound of the form $\sum_{t=1}^{T} ([g(\bx_t)]_+)^2 \leq C_V T^{1 - \beta}$, and therefore one might hope to guarantee $\sum_{t=1}^{T} ([g(\bx_t)]_+)^2 \leq 0$.
Unfortunately, the analysis approach does not immediately show this because the tightening parameter $\rho$ cannot be used to cancel out the violation as in \eqref{eqn:tightening}.

\section{Missing proofs}
\label{apx:ogd_pfs}

\subsection{Proof of Lemma \ref{lem:error_bound}}
\label{apx:err_bound}

In this section, we give the proof of Lemma \ref{lem:error_bound}.
This proof relies on the following lemma, which gives several properties of the projection on to the sub-level set of a convex function.

\begin{lemma}
    \label{lem:proj}
    Consider a closed convex function $h: \Rb^d \rightarrow \Rb$.
    Assume that, for all $\bx \in \Rb^d$, it holds that $\| \bs \| \leq G$ for all $\bs \in \partial h(\bx)$, and when $h(\bx) = 0$, it holds that $\| \bs \| \geq \sigma$ for all $\bs \in \partial h(\bx)$.
    Then, take any $\bx \in \Rb^d$ such that $h(\bx) > 0$.
    Let $\bz = \Pi_{\bx : h(\bx) \leq 0} (\bx)$ and $\bs_x \in \partial h(\bx)$.
    There exists $\gamma \geq 0$ and $\bs_z \in \partial h(z)$ such that:
    \begin{enumerate}
        \item $h (\bz) = 0$,
        \item $\bx - \bz = \gamma \frac{\bs_z}{\| \bs_z \|}$,
        \item $\gamma \leq \frac{h(\bx)}{\| \bs_z \|}$,
        \item $\| \bs_z \|^2 \leq \bs_x^\top \bs_z$,
        \item $\| \bs_x \| \geq \| \bs_z \|$.
    \end{enumerate}
    \end{lemma}
\begin{proof}
    First, note that the $\Sc := \{ \bx \in \Rb^d : h( \bx) \leq 0 \}$ is a closed and convex set so this projection is well-defined.
    Then, we show each point in the following.

    \paragraph{1.}
    Suppose the contrary, i.e. that $h( \bz) \leq -\epsilon$ for some $\epsilon > 0$.
    It follows that $\bz + \frac{\epsilon}{G} \Bb \subseteq \Sc$ as
    \begin{equation*}
        \sup_{\bv \in \Bb} h(\bz + \frac{\epsilon}{G} v) \leq  h(\bz) +  G \frac{\epsilon}{G} \leq -\epsilon + \epsilon = 0.
    \end{equation*}
    Therefore, $\bz' = \bz + \frac{\bx - \bz}{\| \bx - \bz \|} \frac{\epsilon}{G} \in \Sc$ and
    \begin{equation*}
        \| \bz' - \bx \| = \| \bz + \frac{\bx - \bz}{\| \bx - \bz \|} \frac{\epsilon}{G} - \bx \| = \left( 1 - \frac{\epsilon}{\| \bx - \bz \| G} \right) \| \bz - \bx \| < \| \bz - \bx \|,
    \end{equation*} 
    contradicting the optimality of $\bz$ as the minimizer of the distance to $\Sc$.

    \paragraph{2.}
    Since $\bz = \Pi_{\Sc} (\bx)$ and $\Sc$ is a closed convex set, we know that the vector $\bx - \bz$ is normal to $\Sc$.
    Furthermore, $\Sc = \{ \bx : h(\bx) \leq h(\bz) = 0 \}$ and the conditions of the lemma ensures that $\mathbf{0}$ is not a subgradient of $h$ at $\bz$ (and therefore $\bz$ is not a minimum of $h$) so \cite{rockafellar1970convex} (Corollary 23.7.1) tells us that there exists $\lambda \geq 0$ such that $\bx - \bz \in \lambda \partial h(\bz)$.
    Therefore, there exists $\gamma \geq 0$ and $\bs_z \in \partial h(\bz)$ such that $\bx - \bz = \gamma \frac{\bs_z}{\| \bs_z \|}$.

    \paragraph{3.}
    It follows from the definition of subgradient of $h$ that,
    \begin{equation*}
        h (\bx) \geq h(\bz) + \bs_z^\top(\bx - \bz) = \bs_z^\top(\bx - \bz) = \gamma \frac{\bs_z^\top \bs_z}{\| \bs_z \|} = \gamma \| \bs_z \|.
    \end{equation*}
    Rearranging yields,
    \begin{equation*}
        \gamma \leq \frac{h (\bx)}{\| \bs_z \|}.
    \end{equation*}

    \paragraph{4.}
    From the monotonicity of the subgradients of a convex function, it holds that
    \begin{align*}
        & 0 \leq (\bs_z - \bs_x)^\top (\bz - \bx) = -\frac{\gamma}{\| \bs_z \|} (\bs_z - \bs_x)^\top \bs_z\\
        & \implies \quad 0 \geq (\bs_z - \bs_x)^\top \bs_z\\
        & \implies \quad \| \bs_z \|^2 \leq \bs_x^\top \bs_z.
    \end{align*}
    % Note that $\zeta > 0$ because otherwise would imply that $x = z$ and therefore that $g (z) = g(x)

    \paragraph{5.}
    From \#4 and Cauchy-Schwarz,
    \begin{equation*}
        \| \bs_z \|^2 \leq \bs_x^\top \bs_z \leq \| \bs_x \| \| \bs_z \| \quad \implies \quad \| \bs_x \| \geq \| \bs_z \|.
    \end{equation*}
\end{proof}

Then, we give the proof of Lemma \ref{lem:error_bound} in the following.

\begin{proof}[Proof of Lemma \ref{lem:error_bound}]
    First note that when $g_\rho (\bx) \leq 0$, then $\bx \in \Xc_\rho$ and therefore \eqref{eqn:error_bound} holds with both sides zero.
    Next, we consider the case where $g_\rho (\bx) > 0$.
    Due to the fact that $\bx \in R\Bb$, and Assumptions \ref{ass:bound_cons} and \ref{ass:curve}, we can apply Lemma \ref{lem:proj} \#3 with $h \leftarrow g_\rho$, $G \leftarrow G_g$ and $\sigma \leftarrow \sigma$ to get that,
    \begin{equation*}
        \dist(\bx, \Xc_\rho) \leq \frac{g_\rho(\bx)}{\| \bs_z \|} \leq \frac{g_\rho(\bx)}{\sigma},
    \end{equation*}
    which verifies \eqref{eqn:error_bound}.
\end{proof}


\subsection{Proof of \eqref{eqn:tone}}
\label{apx:tone}

In this section, we prove \eqref{eqn:tone}.
This combines Fact \ref{fact:half} with the classical online gradient descent analysis from \cite{zinkevich2003online}.

% \begin{proposition}
%     \label{prop:reg}
%     Under the assumptions of Theorem \ref{thm:ogd_pfs}, Algorithm \ref{alg:ogd_pfs} ensures that $R_T \leq \frac{1}{2 \eta} D^2 + \frac{\eta}{2} G_f^2 T$.
% \end{proposition}
\begin{proof}[Proof of \eqref{eqn:tone}]
    Given that $\bx^\star \in \Xc \subseteq \Hc_t$ and $\Hc_t$ is convex, it holds that
    \begin{align*}
        & \| \bx_{t+1} - \bx^\star \|^2\\
        & = \| \Pi_{R \Bb} (\Pi_{\Hc_t} (\by_t)) - \bx^\star \|^2\\
        & \leq \| \Pi_{\Hc_t} (\by_t) - \bx^\star \|^2\\
        & \leq \| \by_t - \bx^\star \|^2\\
        & \leq \| \bx_t - \eta \nabla f_t(\bx_t) - \bx^\star \|^2\\
        & = \| \bx_t - \bx^\star \|^2 - 2 \eta \nabla f_t(\bx_t)^\top (\bx_t - \bx^\star) + \eta^2 \| \nabla f_t(\bx_t) \|^2.
    \end{align*}
    Using this and the convexity of $f_t$, it follows that
    \begin{align*}
        \tone & = \sum_{t=1}^{T} (f_t(\bx_t) - f_t(\bx^\star_\rho))\\
        & \leq \sum_{t=1}^{T} \nabla f_t(\bx_t)^\top (\bx_t - \bx^\star_\rho)\\
        & \leq \frac{1}{2 \eta} \sum_{t=1}^{T} (\| \bx_t - \bx^\star_\rho \|^2 - \| \bx_{t+1} - \bx^\star_\rho \|^2) + \frac{\eta}{2} \sum_{t=1}^{T} \| \nabla f_t(\bx_t) \|^2\\
        & = \frac{1}{2 \eta} (\| \bx_1 - \bx^\star_\rho \|^2 - \| \bx_{T+1} - \bx^\star_\rho \|^2) + \frac{\eta}{2} \sum_{t=1}^{T} \| \nabla f_t(\bx_t) \|^2\\
        & \leq \frac{1}{2 \eta} R^2 + \frac{\eta}{2} G_f^2 T
    \end{align*}
    where the third inequality uses Assumption \ref{ass:stand}.
\end{proof}

