\vspace{-.2cm}
\section{Evaluation Setup}\label{sec:setup}
\vspace{-.2cm}

We employ a dynamic framework to explore how the intensity of emotions evoked in readers impacts their judgments of convincingness. %in comparison to an anchor argument. 
In this work, \textbf{we treat emotional intensity as the overall strength of emotions felt by readers, without considering specific emotions}.
We follow previous works 
\citep{habernal-gurevych-2016-argument,toledo-etal-2019-automatic} to leverage pairwise comparisons for evaluation \se{because }it yields more reliable annotations compared to the absolute ratings, especially for such subjective evaluation tasks 
\citep{zhang2017improved,10.1162/coli_a_00426,gienapp-etal-2020-efficient}. 

Our setting is as Figure \ref{fig:example} shows:  
among one pair of arguments
that share \emph{the same stance} on a given topic but \emph{differ in their content}, 
\textbf{\rz{}}  
is \se{(set up to be)} 
\textbf{emotion-evoking}, while 
\textbf{\ro{} 
\se{does not (typically)} evoke emotions}.
We then use LLMs to generate a counterpart argument for $E$, \todo{SE: why? why is the comparison of N and E not enough? YC: need to motivate in the introduction, i guess, as discussed.} \textbf{\rth{}}, 
which retains the same meaning as \rz{} but \textbf{evokes less emotion}. To inspect how perceived argument convincingness is affected by emotions, we compare the convincingness ranking of (\rth{},\ro{}) to that of the original pair (\rz{},\ro{}). 
The reason to not compare the arguments with a similar content, i.e., \rz{} vs.\ \rth{} is that we want 
to minimize the effect of human's prior belief about whether emotions should contribute to argument convincingness.
Analogously, we generate a counterpart for \ro{}, \textbf{\rtw{}}, with \textbf{increased emotional intensity} and observe how the convincingness ranking changes from (\rz{}, \ro{}) to (\rz{}, \rtw{}). Finally, we include the fully LLM-generated pair (\rth{}, \rtw{}) in our evaluation. 

The goal \se{of $G^+/G^-$} is to maintain the core meaning of the argument while modifying its emotional appeal. Although humans could be used to create such counterparts \citep[e.g.][]{huffaker2020crowdsourced,velutharambath_wuehrl_klinger_2024a}, 
this approach is largely impractical at scale because it is costly.  Instead, we use LLMs to efficiently generate required variations and assess their capability in performing this task. 

Thus, \textbf{for each original argument pair \se{$(E,N)$}, we create three counterpart pairs} with varying levels of emotional intensity, resulting in a total of four argument pairs per test instance. The original argument pair serves as the anchor, from which we see how the convincingness rankings of the other argument pairs change.
We list 
all possible change scenarios in Table \ref{tab:comparison} 
and 
divide 
them into three categories: 
\ref{tab:comparison}: 
%\begin{itemize}
    %\item 
    (1) \textbf{Consistent}: convincingness ranking \emph{does not change} with varying emotional intensities. 
    (2) \textbf{Positive}: 
    an argument is perceived as more/less convincing when it evokes stronger/weaker emotions 
    \se{ 
    (convincingness and emotionality have the same directionality).}
    %\item 
    (3) \textbf{Negative}: 
    an argument is perceived as more/less
    convincing when it evokes weaker/stronger emotions,
    and less convincing when it evokes stronger emotions
    \se{%
    (convincingness and emotionality have the opposite directionality).}
%\end{itemize}


\begin{table}[!t]
    \centering
    \resizebox{.75\linewidth}{!}{
    \begin{tabular}{@{}cc>{\columncolor[RGB]{153,255,153}}c|>{\columncolor[RGB]{255,204,204}}cc>{\columncolor[RGB]{153,255,153}}c|>{\columncolor[RGB]{255,204,204}}cc@{}}
         \toprule
         Argument Pair & \multicolumn{7}{c}{Convincingness Ranking} \\ \midrule
Anchor: (\rz{}, \ro{})   & \multicolumn{2}{c|}{>} & \multicolumn{3}{c|}{=}            & \multicolumn{2}{c}{<}    \\ \midrule
(\rth{}, \ro{})  & > & $\le$ & > & = & < & $\ge$ & < \\
(\rz{}, \rtw{})  & > & $\le$ & > & = & < & $\ge$ & < \\
(\rth{}, \rtw{}) & > & $\le$ & > & = & < & $\ge$ & < \\ \bottomrule
\end{tabular}}
    \iffalse
    \includegraphics[width=\linewidth]{structure/figs/comparison.pdf}
    \fi
    \caption{All convincingness change scenarios. Cells marked in green indicate positive cases, red indicates negative cases, and consistent cases are left with a white background. \se{Math relation symbols $>,<,=$ refer to convincingness.}}
    \label{tab:comparison}
    \vspace{-.5cm}
\end{table}


\iffalse
Figure \ref{fig:comparison} illustrates all possible changes in convincingness rankings when transitioning from the anchor pair to the counterpart pair. The anchor pair (\rz{},\ro{}) serves as a reference point, with its convincingness ranking represented along the x-axis. The counterpart pairs modify the emotional intensity of one or both arguments: (1) decreasing the left argument’s emotional intensity (\rth{},\ro{}), (2) increasing the right argument’s emotional intensity (\rz{},\rtw{}), or (3) applying both modifications (\rth{},\rtw{}), with the y-axis representing their convincingness rankings.
Symbols correspond to different outcome types: \textbf{squares} ($\blacksquare$) represent cases where convincingness rankings remain \textbf{consistent} with the anchor pair. \textbf{Circles} ($\bullet$) denote scenarios where increasing emotional intensity enhances convincingness (\textbf{positive impact}); this occurs when the convincingness ranking shifts from the left being $>$ to $\leq$ the right argument, or from being $=$ to $<$ the right argument. In contrast, \textbf{crosses} (\ding{56}) mark cases where increasing emotional intensity reduces convincingness (\textbf{negative impact}).
\fi

The first 
row in the table presents all possible convincingness ranking\se{s} of the original argument pair (\rz{},\ro{}). The subsequent rows show the convincingness rankings of the counterpart argument pairs where the emotional intensity of the argument on the left has been reduced (\rth{},\ro{}), that of the argument on the right has been increased (\rz{},\rtw{}), 
or both (\rth{},\rtw{}). Cells highlighted in \hlc[green!50]{green} indicate cases where the convincingness of the left argument decreases as its emotional intensity decreases \emph{relative} to the right argument, 
suggesting a \hlc[green!50]{\textbf{positive}} impact of emotions on convincingness. 
This occurs when the convincingness ranking shifts from the left being $>$ to $\leq$ the right argument, or from being $=$ to $<$ the right argument. 
Conversely, cells highlighted in \hlc[pink]{red} indicate cases where the convincingness of the left argument increases as its emotional intensity decreases \emph{relative} to the right argument, reflecting a \hlc[pink]{\textbf{negative}} impact of emotions on convincingness.
Finally, cases where the convincingness rankings remain \hlc[white]{\textbf{consistent}} 
\se{retain} 
a \hlc[white]{white} background.

\vspace{-.1cm}
\paragraph{Metrics}
For each instance \se{$(E,N)$}, we calculate the percentages of consistent, positive, and negative cases. We then average the percentages of each category across all test instances to derive three metrics that indicate the overall frequencies of the three categories in humans. \todo{SE: one instance can have multiple cases because of multiple annotators? YC: yes. But I reported the averages across annotators and the 95\% confidence interval.}
We call the metrics: \textbf{consistency rate}, \textbf{positivity rate}, and \textbf{negativity rate}. Their formulas are as follows:

\vspace{-.2cm}
\begin{align}
    Rate_{category} = \frac{1}{n} \sum_{i=1}^{n} \frac{C_{category, i}}{3},
\end{align}
where $n$ is the total number of test instances, $\textit{C}_{\textit{category}}$ is the count  
of cases in the specified category for the $i$-th instance, and $category \in \{\text{consistent, positive, negative}\}$. 


\section{Dataset Construction}

We source \textbf{50 anchor argument pairs} from each of five datasets (\S\ref{sec:source}) and utilize GPT4o\footnote{\url{https://openai.com/index/gpt-4o-system-card/}} to generate their \textbf{counterparts} with variations in emotional intensity (\S\ref{sec:synthetic}).
\vspace{-.2cm}
\subsection{Anchor: \rz{} \& \ro{}}\label{sec:source}
We leverage two established datasets which have human annotations for argument convincingness and emotions, \dagstuhl{} \citep{wachsmuth-etal-2017-computational} and \lynn{} \citep{greschner2024fearfulfalconsangryllamas}. 
Besides, we create three datasets ourselves from political debates, \bill{}, \hansard{}, and \deuparl{}, since emotional appeal is a common strategy used by politicians to influence perceptions and decisions \citep{brader2005striking}; this domain is therefore expected to be rich in emotional content. 
From each data source, we select 50 argument pairs where \textbf{\rz{} is more likely and \ro{} is less likely to evoke emotions}. The subscripts in the dataset names indicate the language: `en' for English and `de' for German. \se{In the following, we }describe how we extract argument pairs from each data source.

\subsubsection{Arguments from Political Debates}\label{sec:data_ours} 

We \textbf{crawl} parliamentary debates for \hansard{} from the UK Hansard\footnote{\url{https://hansard.parliament.uk/}} and for \deuparl{}\footnote{We name it DeuParl following previous studies leveraging this corpus \citep[e.g.][]{walter2021diachronicanalysisgermanparliamentary,kostikova-etal-2024-fine,chen2024syntacticlanguagechangeenglish}.} from 
the German Bundestagsprotokolle.\footnote{\url{https://www.bundestag.de/protokolle}}
The datasets cover the past 3–5 years.\footnote{Hansard: 2022/01/05-2024/07/19; German Bundestagsprotokolle: 2020/01/15-2024/09/27}
We heuristically \textbf{segment} each speech into balanced-length paragraphs. The original crawled texts are divided by double line breaks. If a paragraph has fewer than 60 tokens or the next one has fewer than 20 tokens or starts with a left bracket, we merge them cumulatively. From these processed paragraphs, we select argumentative texts for evaluation.

In our \textbf{pilot annotations} with \hansard{}, we find that within a single debate on a broad topic, diverse subtopics make it difficult to pair arguments with the same topic. Additionally, the interactive nature of debates complicates determining a paragraph’s focus without context.
To address this, we first conduct \textbf{pre-annotation} on a small scale for five \emph{Second Reading debates of Bills} relevant to family and animals,\footnote{\url{https://www.parliament.uk/about/how/laws/passage-bill/commons/coms-commons-second-reading/}} which are easier to annotate because the Bill debated provides a clear topic. We then refine GPT-4o prompts to develop \textbf{classifiers} for identifying argument pairs that share a topic and stance but differ in emotional appeal. The final classifiers achieve precisions of 0.80 (English) and 0.76 (German) for detecting topic-aligned arguments and a macro F1 of $\sim$0.75 for distinguishing emotional from non-emotional arguments.
See Appendix \ref{app:pre} for details.

%\vspace{-.3cm}
\paragraph{\bill{}}
From the argument pairs labeled as having the same topic and stance during the pre-annotation phase, we randomly sample 50 pairs, with one argument labeled as emotion-evoking and the other as non-emotion-evoking. The topic for each argument pair is the brief introduction of the Bill crawled.

%\vspace{-.3cm}
\paragraph{\hansard{} \& \deuparl{}}
Debates are filtered using pre-selected keywords related to recent wars, refugee crises, and migration (see Table \ref{tab:keywords} in the appendix for the full list), as these highly debated topics are likely to evoke strong emotions. 
For \hansard{}, we retain debates whose titles contain these keywords. For \deuparl{}, we include debates whose introductions mention the keywords.
Finally, an annotator from the pre-annotation phase selects 50 argument pairs from the candidates generated by the automated pipeline for both \hansard{} and \deuparl{}. These argument pairs are \textbf{manually verified} to meet our criteria --- both arguments address the same topic with the same stance but differ in their emotional aspect. \todo{SE: perhaps a minor point: how well can the human do that? emotionality is subjective? YC: we can not ensure if one is more emotion-evoking than the other for all annotators, it's just about likelihood: one is more likely to evoke emotions than the other. But we can have this evaluation result using the pairwise comparison annotations to see if \rz{} is more emotional than \ro{}. Should i add this somewhere? SE: not sure}
A human-written topic is assigned to each pair.

%\vspace{-.2cm}
\subsubsection{Arguments from others}
%\vspace{-.1cm}
We randomly select 50 argument pairs from each of \textbf{\dagstuhl{}} and \textbf{\lynn{}} that meet our criteria, based on the emotion annotations in the original works. See Appendix \ref{app:other} for details.

\vspace{-.1cm}
\subsection{Counterpart: \rth{} \& \rtw{}}\label{sec:synthetic}
\vspace{-.1cm}
 
We leverage \textbf{GPT4o}\footnote{We used the version `gpt-4o-2024-08-06' with a temperature of 0.6 and a top\_p of 0.9 for GPT4o. The randomness was set to a moderate level to balance creativity and consistency, as the task involves generating content similar to creative writing while ensuring the meaning of the original argument is preserved.} to synthesize our counterpart arguments, namely \textbf{\rth{}} and \textbf{\rtw{}}. Specifically,
we prompt GPT4o (zero-shot) to either introduce or remove emotions by \textbf{rephrasing} the original arguments, using the prompts listed in Table \ref{tab:prompt_synthetic} (appendix),
since we aim for counterpart arguments that convey the same information as the original ones. During generation, if the output does not receive the expected label from the emotion classifiers used in \S\ref{sec:data_ours}, the process is repeated for up to five rounds. 

We randomly sample five argument pairs (original + synthetic) for each direction (introducing or removing emotions) from each dataset, totaling 50 argument pairs for content preservation \textbf{evaluation}. Each pair is rated by three crowdworkers for content similarity on a Likert scale of 1–5. The pairs receive an average score of 4.5, where 4 denotes ‘Same Claims, Minor Content Differences’ (minor details differ, but no major evidence changes), and 5 represents ‘Identical Content, Different Style/Tone’ (only rhetorical or emotional differences). Thus, we conclude that the main message is well preserved throughout the process.
The effectiveness of adjusting emotional appeal is further evaluated in our primary human study (\S\ref{sec:human}).


\begin{table*}[t]
\setlength\tabcolsep{2pt} 
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}lcccc|cc|ll@{}}
\toprule
\textbf{Subdataset} & \textbf{Lang} & \textbf{\#Instances} & \textbf{\#Pairs} & \textbf{\#Arguments} & \textbf{\#Tokens} & \textbf{\#Sents} & \textbf{Domain}              & \textbf{Topics}                   \\ \midrule
\bill{}    & en       & 50           & 200      & 128          & 147.4     & 6.1      & Parliamentary debates   & Bills related to family and animals                \\
\hansard{}          & en       & 50           & 200      & 154          & 159.3     & 6.4      & Parliamentary debates        & Refugees, wars, migrants          \\
\dagstuhl{}         & en       & 50           & 200      & 128          & 86.8      & 4.5      & Online portal                & -                                 \\
\deuparl{}         & de       & 50           & 200      & 126          & 144.3     & 7.4      & Parliamentary debates        & Refugees, wars, migrants          \\
\lynn{}          & de       & 50           & 200      & 160          & 92.8      & 4.5      & Curated human-written arguments & Health, law, finance and politics \\ \midrule
Total/Average  & -        & 250          & 1,000     & 696          & 126.1     & 5.7      & -                            & -                                 \\ \bottomrule
\end{tabular}}
\caption{Metadata of datasets used in this work. \textbf{Left}: number of test instances, argument pairs, and unique arguments. \textbf{Middle}: average number of tokens and sentences per argument, measured with the Stanza tokenzier \citep{qi-etal-2020-stanza}. \textbf{Right}: domains and topics of the datasets.}\label{tab:data}
%\vspace{-.6cm}
\end{table*}

\subsection{Final Datasets}
Our final datasets comprise 250 test instances, each consisting of one original argument pair and three counterpart pairs. The datasets include both English and German texts, spanning various domains and topics. The metadata of the datasets is summarized in Table \ref{tab:data}. 

\vspace{-.2cm}
\section{Human Annotation}
\vspace{-.2cm}
\subsection{Annotation}

We randomly divide the 50 instances (200 argument pairs) from each dataset into 10 batches, each with 5 instances (20 argument pairs). One annotator evaluates at least one batch, 
allowing us to calculate inter-annotator agreements and base observations on individual annotators. Every batch is annotated by 5 individuals. Although our primary focus is on how convincingness rankings change, we also include comparisons of emotional intensity to evaluate whether GPT4o adjusts the emotional appeal of arguments as intended.

Annotators compare emotions and convincingness of one argument pair by answering two \textbf{subjective} questions:
%\begin{itemize}
    %\item 
    (i) \textbf{Convincingness}: \emph{Which argumentative text \se{do} you find more convincing?}
    %\item 
    (ii) \textbf{Emotion}: \emph{Which argumentative text evokes stronger emotions in you?}
%\end{itemize}
\textbf{Equivocal judgments} are allowed, i.e., 
annotators 
can judge both arguments as equally convincing or evoking an equal level of emotion.
During annotation, argument pairs are shown with their topics. See Appendix \ref{app:anno} for screenshots of the annotation interface.

%\vspace{-.2cm}
\paragraph{Annotators}
We hire annotators from two sources: university students  
\se{and}
the crowd-sourcing platform Prolific:\footnote{\url{https://www.prolific.com/}}
\begin{itemize}[topsep=2pt,itemsep=-1pt,leftmargin=*]
    \item \textbf{Student}: 
4 students 
are hired for this task. All annotators possess fluent to native-level proficiency in the languages of the evaluated arguments and are all based in Germany. One of them is a PhD student, and the others are Master's students. Three of them are involved 
in the pre-annotation phase to select out the needed argument pairs.
    \item \textbf{Crowdsourcing}: 
As our dataset includes arguments from political debates, we assume native speakers in the corresponding countries provide more reliable annotations. Thus, we use Prolific's \textbf{prescreening} to select native English/German speakers in the UK/Germany. Furthermore, to filter out individuals who may randomly fill in their profiles, participants are asked to re-rate their language proficiency, 
and those with inconsistent responses are \textbf{screened out} from the tasks. We also include \textbf{three attention checks} by randomly inserting instruction sentences, such as `select the answer whose first number equals three minus two', into the arguments. 
Overall, 38\% of the crowdworkers fail at least two attention checks, and their submissions are excluded from our analysis. 
This process is repeated iteratively until we obtain sufficient submissions for each batch.
\end{itemize}
\vspace{.2cm}
We summarize the number of student and crowdsourcing annotators for each dataset in Table \ref{tab:annotator} (left side) in the appendix; the values indicate the total annotators involved in annotating each batch. The total annotation cost is around 1,500 Euros. 


\vspace{-.3cm}
\paragraph{Inter-annotator agreement}
\se{While} we acknowledge the inherent subjectivity in evaluating emotion and convincingness, 
we report inter-annotator agreement to present the level of consistency in these evaluations of emotional intensity (EMO) and convincingness (CONV).
Following \citet{wachsmuth-etal-2017-computational}, in Table \ref{tab:annotator} (right) in the appendix, we report the Krippendorf's $\alpha$ agreement \citep{castro-2017-fast-krippendorff} for the most agreeing annotator pairs\footnote{We average the agreements of the most agreeing annotators over batches per dataset since our sample size for calculating agreements is much smaller than \citet{wachsmuth-etal-2017-computational} (20 vs.\ 320).} (column `$\alpha$'), the percentages of annotation instances where all annotators agree on a certain label (column `Full'), and the percentages of annotation instances yielding a valid majority vote (column `Maj.'). 
The agreement among the most agreeing annotator pairs ranges from 0.352 to 0.729 for EMO and from 0.364 to 0.607 for CONV.
Full agreements are only up to 16.0\%, while 
majority agreements range from moderate to high across different datasets, with 68\% to 87.5\% for EMO and 62\% to 85\% for CONV. This suggests a decent level of annotation agreement, considering that \citet{wachsmuth-etal-2017-computational} reported 94.4\% majority agreement and a Krippendorff's $\alpha$ of 0.26–0.45 for the most agreeing annotator pairs when evaluating emotional appeal and argument effectiveness on a Likert scale of 1–3; both tasks can also be seen as three-way classifications similar to ours but involved only three annotators.




