
\begin{figure}[!t]
    \centering
    \includegraphics[width=1.07\linewidth]{structure/figs/rate_r7.pdf}
    \vspace{-1cm}
        \caption{Consistency, positivity, and negativity rates of human judgments on convincingness. %Consistency indicates that perceived convincingness remains unaffected by changes in emotional intensity. Positivity reflects cases where perceived convincingness and emotional intensity change in the same direction, while negativity represents instances where they change in opposite directions (see \S\ref{sec:setup} for exact definitions).
        }
    \label{fig:percent}
    \vspace{-.5cm}
\end{figure}

\subsection{Evaluation Results}\label{sec:human}

\paragraph{Effectiveness of GPT4o in adjusting emotional appeal}
We evaluate whether \rth{} evokes weaker emotion than \rz{} and whether \rtw{} evokes stronger emotion than \ro{}, as intended. To do so, we compute best-worst scaling (BWS) scores for each of the four argument groups based on emotion comparison annotations. Majority votes from the five annotators are used; if none exists, equivalent judgments are considered. While arguments with similar content (e.g., \rz{} vs.\ \rth{}) are not directly compared with each other, both are evaluated against the other two arguments within the same instance, making the BWS-based comparison between the arguments within the same content meaningful. Higher scores reflect greater perceived emotional intensity. Table \ref{tab:bws} (appendix) 
presents the BWS scores, showing that \rz{} consistently scores higher than \rth{} and \ro{} lower than \rtw{} across datasets. This suggests that \emph{GPT4o is overall effective in modifying arguments to be more or less emotion-evoking as intended}, supporting the premise for analyzing changes in convincingness rankings. 

\vspace{-.2cm}
\paragraph{Do emotions really affect convincingness?} 
Figure \ref{fig:percent} illustrates the consistency, positivity, and negativity rates. We present the averages across individual annotators, with error bars representing 95\% confidence intervals. 
While the metrics vary across datasets and domains, we observe that consistency achieves the highest rates consistently across datasets, roughly ranging from 54\% to 62\%. This indicates that, \textbf{in more than half of the cases, humans are not influenced by variations in perceived emotions when judging convincingness}. In political debate domain datasets --- \hansard{}, \bill{}, and \deuparl{} --- positive rates are consistently higher than negative rates, averaging an 8-percentage-point difference.
In contrast, in \dagstuhl{}, positive and negative rates are roughly equal ($\sim$18\%), whereas in \lynn{}, negative rates dramatically exceed positive rates (30\% vs.\ 14\%). These differences may be attributed to variations in dataset domains and argument topics. In Appendix \ref{app:anno}, we show examples where emotions have positive/negative impacts on argument convincingness from \hansard{}/\lynn{}; in \lynn{}, topics often require more factual evidence, making emotions less influential or even detrimental.
Finally, we observe slight differences between the English and German datasets, using \hansard{} and \deuparl{} as examples, where argument topics are similar and both originate from political debates: the rates are overall comparable, with German being less affected by emotions (consistency rates: 60\% vs.\ 56\%) and also less positively influenced by emotions (positivity rates: 20\% vs.\ 25\%) compared to English.

\begin{table}[!t]
\centering
\setlength\tabcolsep{1.5pt} 
\resizebox{.9\linewidth}{!}{
\begin{tabular}{@{}llc@{}}
\toprule
\textbf{Model Family} & \textbf{Checkpoint}                           & \textbf{Size} \\ \midrule
OpenAI%\footnote{{\url{https://platform.openai.com/docs/models}}}      
& gpt-3.5-turbo                        & -    \\
\citep{openai2024gpt4technicalreport}             & gpt-4o-mini                          & -    \\
             & gpt-4o-2024-08-06                    & -    \\ \midrule
Llama3%\footnote{\url{https://huggingface.co/meta-llama}}
& Llama-3.2-1B-Instruct     & 1B   \\
\citep{grattafiori2024llama3herdmodels}             & Llama-3.2-3B-Instruct     & 3B   \\
             & Llama-3.3-70B-Instruct    & 70B  \\ \midrule
Qwen2.5%\footnote{\url{https://huggingface.co/Qwen}} 
& Qwen2.5-0.5B                    & 0.5B \\
\citep{qwen2.5}             & Qwen2.5-7B-Instruct             & 7B   \\
           & Qwen2.5-72B-Instruct            & 72B  \\ \midrule
Mistral%\footnote{\url{https://huggingface.co/mistralai}}
& &  \\
\citep{jiang2023mistral7b} & Mistral-7B-Instruct-v0.3   & 7B   \\
\citep{jiang2024mixtralexperts}             & Mixtral-8x7B-Instruct-v0.1 & 47B  \\ \bottomrule
\end{tabular}}
\caption{LLMs used in this work.
}\label{tab:llms}
\vspace{-.3cm}
\end{table}

\begin{figure*}[!h]
\centering
    \includegraphics[width=\linewidth]{structure/figs/llm/dis_all_prompts.pdf}
    \vspace{-.3cm}
    \caption{Consistency, positivity, and negativity rates of LLMs' judgments on convincingness, averaged across prompts and instances in all datasets.}\label{fig:llm_dis}
    \vspace{-.3cm}
\end{figure*}

\section{Human vs.\ LLMs}\label{sec:exp}

\paragraph{Models}
We select a range of recent LLMs, including both open-source and commercial models, with varying model sizes from 0.5B to 72B parameters. We experiment with \textbf{11 LLMs from 4 model families}, as detailed in Table \ref{tab:llms}. For OpenAI models, we utilize the official API,\footnote{\url{https://platform.openai.com/}} while for open-source models, we retrieve checkpoints from HuggingFace.\footnote{\url{https://huggingface.co/}} 
For all models, we set the temperature to 0.6 and the top-p value to 0.9, to ensure diverse outputs that still remain contextually relevant and logical, running each model five times. For 70B/72B models, we use 4-bit quantization. We run the models on 1 to 8 A40 GPUs, each with 48GB of memory.


\paragraph{Prompts}
We use \textbf{three prompt templates} to prompt LLMs to compare perceived 
convincingness, mirroring human instructions. 
The final judgment is determined by a majority vote from the five runs; if none is reached, the arguments are considered equally convincing. 
\textbf{Zero-shot prompts} are employed to minimize biasing effects on model responses \citep{paech2024eqbenchemotionalintelligencebenchmark} and thus better capture the models' intrinsic behavior.
As shown in Table \ref{tab:promptc} (Appendix \ref{app:prompt}), \emph{Prompt 1} instructs models to provide a label without explanation. 
\emph{Prompt 2 and Prompt 3} additionally require an explanation and include an example answer to specify the response format. To examine potential biases from examples, they feature opposite label choices and differ in perspective, with Prompt 2 favoring an objective approach and Prompt 3 adopting a more subjective and emotional stance. 

%\vspace{-.3cm}
\paragraph{LLMs exhibit a similar sensitivity to emotions when judging argument convincingness.}

Figure \ref{fig:llm_dis} presents the consistency, positivity, and negativity rates of LLMs' convincingness judgments, averaged across prompts and instances in all datasets. Like humans, \emph{LLMs show a strong tendency toward consistency}, with rates consistently exceeding positivity and negativity ($\sim$48\%-68\% vs. $\sim$10\%-36\%); all models except Qwen2.5-0.5B, Llama-3.2-3B, and Qwen2.5-7B achieve a consistency rate above 50\%;
Moreover, 
\emph{emotions more often enhance rather than degrade convincingness}, except for Qwen2.5-7B, aligning with human patterns. As shown in Figure \ref{fig:dis_prompt} (Appendix \ref{app:llm}), most models exhibit comparable rates across different prompts, except for the smallest model, Qwen2.5-0.5B, and gpt-3.5-turbo, where negativity surpasses positivity with Prompt 2. In Prompt 2's example, logical fallacy is mentioned in the explanation, which may (mis)lead models to interpret emotions as a logical fallacy.

\paragraph{However, they do not align well with humans on individual judgements.}
Table \ref{tab:llm_ranking} in Appendix \ref{app:llm} displays macro F1 scores and model rankings for LLMs in predicting argument pair convincingness rankings (column `Static’) and the resulting categories of emotional effect (column `Dynamic’) in English and German. \todo{SE: is this the same static vs. dynamic given in the introduction?}
The best prompt result of each model is reported to demonstrate its potential. Human and LLM labels are determined by majority votes from different annotators and runs, respectively.
Overall, all scores remain low (~0.32–0.49), indicating performance ranging from random to slightly above random in a three-way classification task. GPT4o consistently ranks first in three of four tasks, except for dynamic label prediction in English, where it ranks second.
Larger models generally align better with humans, often achieving higher F1 scores than their smaller counterparts, with the largest models (GPT4o, Llama-3.3-70B, Qwen2.5-72B) frequently ranking among the top.



