\section{Related Works}
% \label{sec:related_wroks}

	%-------------------------------------------------------------------------
	% \paragraph{Video Saliency Prediction} Video saliency prediction has historically been significant to the computer vision community ____. Driscoll~\etal ____ were among the first to discuss using neural networks for saliency map prediction in humanoid robots. SP models thrived with the surge of deep learning over the last decade. Bak~\etal____ proposed two-stream networks that utilized two convolutional backbones for leveraging RGB images and optical flow maps as input, respectively, to extract spatio-temporal features and fuse their outputs for saliency prediction. Wu~\etal____, and Zhang and Chen____ additionally examined the two-stream structure and analyzed fusion methods to improve performance.

	% Since optical flow networks simply consider adjacent frames for modelling the temporal relations in the video, LSTM-based methods were employed to model long-term temporal dependencies efficiently. Gorji and Clark ____ make use of a multi-stream Convolutional Long Short-Term Memory network (ConvLSTM) that incorporates static saliency and learns long-term temporal dependencies to estimate SP. Wang~\etal____ propose ACLNet to extend CNN-LSTM architecture using a supervised attention mechanism. Lai~\etal____ presents STRA-Net, a spatio-temporal residual network based on ConvGRU, to model the attention transition across frames. SalEMA____ utilizes a simple exponential moving average for feature fusion in the temporal domain. SalSAC____ first extracts multi-level features and then uses a random shuffling mechanism for the multi-level attentions calculated from the multi-level features followed by a correlation-based ConvLSTM.

	% 3D Convolution-based architectures gained a lot of traction due to their ability to simultaneously model both spatial and temporal information. These methods typically employ action classification networks as their backbone. TASED-Net____ is a 3D convolutional encoder-decoder network that makes use of S3D____ pre-trained on the Kinetics dataset____ as the video encoder. Jain~\etal____ present ViNet, a fully convolutional encoder-decoder architecture. It utilizes hierarchical features from the encoder and passes them as skip connections to the decoder in a UNet____ like fashion to output the saliency map. We build upon this model due to its simplistic architecture, ease of training and real-time inference, augmenting the design with an efficient and lightweight decoder. STSANet____ utilizes multiple spatio-temporal self-attention modules at different levels of 3D convolutional backbone for capturing long-range relations between spatio-temporal features; however, its large model size renders it impractical for real-world applications. Recent efforts utilize transformers for SP. TMFI-Net____ employs Video Swin Transformer____ as the video encoder and a hierarchical decoder to predict saliency maps. THTD-Net____ makes use of Video Swin Transformer____ as well, gradually reducing the temporal dimension in the decoder layers to model long-range temporal dependencies.

	% 3D convolutional architectures are popular for modelling both spatial and temporal information, often using action classification networks as their backbone. TASED-Net____ is a 3D convolutional encoder-decoder network utilizing S3D____ pre-trained on the Kinetics dataset____. ViNet____, a fully convolutional encoder-decoder, uses hierarchical features with UNet-like____ skip connections. We extend this model with a lightweight decoder. STSANet____ employs spatio-temporal self-attention but is too large for practical use. Recent approaches like TMFI-Net____ and THTD-Net____ use Video Swin Transformer for saliency prediction, focusing on long-range temporal dependencies.

	% The field of action recognition has evolved significantly, shifting from action classification____ to STAL____. While existing saliency models leverage backbones pre-trained on action classification tasks____, they have yet to exploit recent STAL research advancements fully. Our work addresses this shortcoming and bridges the gap between the two fields.
	% --------------------------------------------------------------------------

	%vinet_plus_architecture
	%, which predicts the final saliency map by simply taking the pixel-wise average of the outputs from the proposed models. We elaborate on the suggested models in the following sections.

	% We propose two end-to-end trainable and lightweight visual-only models, ViNet-S and ViNet-A. They are fully 3D convolutional encoder-decoder methods that predict the saliency map for the corresponding set of sequential frames.

	%-------------------------------------------------------------------------
	%ViNet-A