@inproceedings{acarnet,
  title={Actor-context-actor relation network for spatio-temporal action localization},
  author={Pan, Junting and Chen, Siyu and Shou, Mike Zheng and Liu, Yu and Shao, Jing and Li, Hongsheng},
  booktitle=CVPR,
  pages={464--474},
  year={2021}
}

@article{cnn_vsp,
  author={Wu, Zhe and Su, Li and Huang, Qingming},
  journal=TCSVT, 
  title={Learning Coupled Convolutional Networks Fusion for Video Saliency Prediction}, 
  year={2019},
  volume={29},
  number={10},
  pages={2960-2971},
  keywords={Feature extraction;Visualization;Computational modeling;Spatiotemporal phenomena;Training;Biological system modeling;Data mining;Video saliency;feature integration;fully convolutional network},
  doi={10.1109/TCSVT.2018.2870954}
}

@article{dhf1k,
  title={Revisiting video saliency prediction in the deep learning era},
  author={Wang, Wenguan and Shen, Jianbing and Xie, Jianwen and Cheng, Ming-Ming and Ling, Haibin and Borji, Ali},
  journal={TPAMI},
  volume={43},
  number={1},
  pages={220--237},
  year={2019},
  publisher={IEEE}
}

@INPROCEEDINGS{image_to_vsp,
  author={Gorji, Siavash and Clark, James J.},
  booktitle=CVPR, 
  title={Going from Image to Video Saliency: Augmenting Image Salience with Dynamic Attentional Push}, 
  year={2018},
  volume={},
  number={},
  pages={7501-7511},
  keywords={Visualization;Computational modeling;Spatiotemporal phenomena;Color;Dynamics;Fuses;Optical imaging},
  doi={10.1109/CVPR.2018.00783}
}

@inproceedings{jain2021vinet,
  title={Vinet: Pushing the limits of visual modality for audio-visual saliency prediction},
  author={Jain, Samyak and Yarlagadda, Pradeep and Jyoti, Shreyank and Karthik, Shyamgopal and Subramanian, Ramanathan and Gandhi, Vineet},
  booktitle={IROS},
  pages={3520--3527},
  year={2021},
  organization={IEEE}
}

@article{kay2017kinetics,
  title={The kinetics human action video dataset},
  author={Kay, Will and Carreira, Joao and Simonyan, Karen and Zhang, Brian and Hillier, Chloe and Vijayanarasimhan, Sudheendra and Viola, Fabio and Green, Tim and Back, Trevor and Natsev, Paul and others},
  journal={arXiv preprint arXiv:1705.06950},
  year={2017}
}

@misc{linardos2019simplevscomplextemporal,
      title={Simple vs complex temporal recurrences for video saliency prediction}, 
      author={Panagiotis Linardos and Eva Mohedano and Juan Jose Nieto and Noel E. O'Connor and Xavier Giro-i-Nieto and Kevin McGuinness},
      year={2019},
      eprint={1907.01869},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1907.01869}, 
}

@inproceedings{min2019tased,
  title={Tased-net: Temporally-aggregating spatial encoder-decoder network for video saliency detection},
  author={Min, Kyle and Corso, Jason J},
  booktitle=ICCV,
  pages={2394--2403},
  year={2019}
}

@inproceedings{overt_va_robot,
  author={Vijayakumar, S. and Conradt, J. and Shibata, T. and Schaal, S.},
  booktitle={Proceedings 2001 IEEE/RSJ International Conference on Intelligent Robots and Systems. Expanding the Societal Role of Robotics in the the Next Millennium (Cat. No.01CH37180)}, 
  title={Overt visual attention for a humanoid robot}, 
  year={2001},
  volume={4},
  number={},
  pages={2332-2337 vol.4},
  keywords={Humanoid robots;Robot kinematics;Process control;Biological control systems;Control systems;Biology computing;Humans;Anthropomorphism;Artificial neural networks;Smoothing methods},
  doi={10.1109/IROS.2001.976418}
}

@inproceedings{ronneberger2015unet,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={MICCAI},
  year={2015}
}

@inproceedings{s3d,
  title={Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification},
  author={Xie, Saining and Sun, Chen and Huang, Jonathan and Tu, Zhuowen and Murphy, Kevin},
  booktitle=ECCV,
  year={2018}
}

@inproceedings{slowfast,
  title={Slowfast networks for video recognition},
  author={Feichtenhofer, Christoph and Fan, Haoqi and Malik, Jitendra and He, Kaiming},
  booktitle=ICCV,
  pages={6202--6211},
  year={2019}
}

@article{st_net_sp,
  author={Bak, Cagdas and Kocak, Aysun and Erdem, Erkut and Erdem, Aykut},
  journal=TMM, 
  title={Spatio-Temporal Saliency Networks for Dynamic Saliency Prediction}, 
  year={2018},
  volume={20},
  number={7},
  pages={1688-1698},
  keywords={Videos;Feature extraction;Predictive models;Computational modeling;Visualization;Dynamics;Dynamic saliency;deep learning},
  doi={10.1109/TMM.2017.2777665}
}

@ARTICLE{stranet,
  author={Lai, Qiuxia and Wang, Wenguan and Sun, Hanqiu and Shen, Jianbing},
  journal=TIP, 
  title={Video Saliency Prediction Using Spatiotemporal Residual Attentive Networks}, 
  year={2020},
  volume={29},
  number={},
  pages={1113-1126},
  keywords={Visualization;Computational modeling;Spatiotemporal phenomena;Dynamics;Task analysis;Data models;Predictive models;Dynamic eye-fixation prediction;residual attentive learning;attention mechanism;deep learning;video saliency},
  doi={10.1109/TIP.2019.2936112}
}

@article{stsanet,
  title={Spatio-temporal self-attention network for video saliency prediction},
  author={Wang, Ziqiang and Liu, Zhi and Li, Gongyang and Wang, Yang and Zhang, Tianhong and Xu, Lihua and Wang, Jijun},
  journal=TMM,
  volume={25},
  pages={1161--1174},
  year={2021},
  publisher={IEEE}
}

@article{thtdnet,
  title={Transformer-based Video Saliency Prediction with High Temporal Dimension Decoding},
  author={Moradi, Morteza and Palazzo, Simone and Spampinato, Concetto},
  journal={VISIGRAPP},
  year={2024}
}

@article{tmfinet,
  title={Transformer-based multi-scale feature integration network for video saliency prediction},
  author={Zhou, Xiaofei and Wu, Songhe and Shi, Ran and Zheng, Bolun and Wang, Shuai and Yin, Haibing and Zhang, Jiyong and Yan, Chenggang},
  journal=TCSVT,
  volume={33},
  number={12},
  pages={7696--7707},
  year={2023},
  publisher={IEEE}
}

@inproceedings{va_net_robot,
  author={Driscoll, J.A. and Peters, R.A. and Cave, K.R.},
  booktitle={Proceedings. 1998 IEEE/RSJ International Conference on Intelligent Robots and Systems. Innovations in Theory, Practice and Applications (Cat. No.98CH36190)}, 
  title={A visual attention network for a humanoid robot}, 
  year={1998},
  volume={3},
  number={},
  pages={1968-1974 vol.3},
  keywords={Humanoid robots;Robot sensing systems;Robot vision systems;Control systems;Layout;Cameras;Psychology;Human robot interaction;Concurrent computing;Distributed computing},
  doi={10.1109/IROS.1998.724894}
}

@inproceedings{videoswin,
  title={Video swin transformer},
  author={Liu, Ze and Ning, Jia and Cao, Yue and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Hu, Han},
  booktitle=CVPR,
  pages={3202--3211},
  year={2022}
}

@article{vsp_two_srteam,
  author={Zhang, Kao and Chen, Zhenzhong},
  journal=TCSVT, 
  title={Video Saliency Prediction Based on Spatial-Temporal Two-Stream Network}, 
  year={2019},
  volume={29},
  number={12},
  pages={3544-3557},
  keywords={Feature extraction;Predictive models;Streaming media;Visualization;Spatiotemporal phenomena;Computational modeling;Video saliency;spatial-temporal features;visual attention;deep learning},
  doi={10.1109/TCSVT.2018.2883305}
}

@inproceedings{wu2020salsac,
  title={Salsac: A video saliency prediction model with shuffled attentions and correlation-based convlstm},
  author={Wu, Xinyi and Wu, Zhenyao and Zhang, Jinglin and Ju, Lili and Wang, Song},
  booktitle=AAAI,
  volume={34},
  number={07},
  pages={12410--12417},
  year={2020}
}

