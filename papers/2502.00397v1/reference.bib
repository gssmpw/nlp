# Computer Vision
@STRING{CVPR="Conference on Computer Vision and Pattern Recognition (CVPR)"}
@STRING{CVPRW="Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"}
@STRING{ICCV="International Conference on Computer Vision (ICCV)"}
@STRING{ICCVW="International Conference on Computer Vision Workshops (ICCVW)"}
@STRING{ECCV="European Conference on Computer Vision (ECCV)"}
@STRING{ECCVW="European Conference on Computer Vision Workshops (ECCVW)"}
@STRING{BMVC="British Machine Vision Conference (BMVC)"}
@STRING{WACV="Winter Conference on Applications of Computer Vision (WACV)"}
@STRING{WACVold="Workshop on the Applications of Computer Vision (WACV)"}
@STRING{ACCV="Asian Conference on Computer Vision (ACCV)"}
@STRING{GCPR="German Conference on Pattern Recognition (GCPR)"}
@STRING{ICME="International Conference on Multimedia and Expo (ICME)"}
@STRING{ICMI="International Conference on Multimodal Interaction (ICMI)"}

# Machine Learning
@STRING{NIPS="Advances in Neural Information Processing Systems (NeurIPS)"}
@STRING{NeurIPS="Advances in Neural Information Processing Systems (NeurIPS)"}
@STRING{NeurIPSData="Advances in Neural Information Processing Systems (NeurIPS): Track on Datasets and Benchmarks"}
@STRING{ICML="International Conference on Machine Learning (ICML)"}
@STRING{IJCAI="International Joint Conference on Artificial Intelligence (IJCAI)"}
@STRING{ICLR="International Conference on Learning Representations (ICLR)"}
@STRING{AISTATS="International Conference on Artificial Intelligence and Statistics (AISTATS)"}
@STRING{AAAI="Association for the Advancement of Artificial Intelligence (AAAI)"}
@STRING{UAI="Uncertainty in Artificial Intelligence (UAI)"}

# NLP
@STRING{ACL="Association of Computational Linguistics (ACL)"}
@STRING{NAACL-HLT="North American Chapter of Association of Computational Linguistics: Human Language Technologies (NAACL-HLT)"}
@STRING{EACL="European Chapter of the Association for Computational Linguistics (EACL)"}
@STRING{EMNLP="Empirical Methods in Natural Language Processing (EMNLP)"}
@STRING{EMNLPFINDINGS="Findings of Empirical Methods in Natural Language Processing (EMNLP)"}
@STRING{IJCNLP="International Joint Conference on Natural Language Processing (IJCNLP)"}
@STRING{EMNLPIJCNLP="Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"}
@STRING{CoNLL="Computational Natural Language Learning (CoNLL)"}
@STRING{COLING="International Conference on Computational Linguistics (COLING)"}

# Image processing / visualization
@STRING{FG="International Conference on Automatic Face and Gesture Recognition (FG)"}
@STRING{AVSS="International Conference on Advanced Video and Signal-based Surveillance (AVSS)"}
@STRING{ICIP="International Conference on Image Processing (ICIP)"}
@STRING{ICIV="International Conference on Information Visualization (iV)"}
@STRING{ICPR="International Conference on Pattern Recognition (ICPR)"}

# Multimedia
@STRING{ACMMM="ACM Multimedia (MM)"}
@STRING{AMM="Advances in Multimedia Modeling"}
@STRING{ICMR="International Conference on Multimedia Retrieval (ICMR)"}
@STRING{CIVR="International Conference on Image and Video Retrieval (CIVR)"}
@STRING{ICME="International Conference on Multimedia and Expo (ICME)"}
@STRING{ICASSP="International Conference on Acoustics, Speech and Signal Processing (ICASSP)"}
@STRING{CBMI="International Workshop on Content-Based Multimedia Indexing (CBMI)"}

# Misc
@STRING{CHI="ACM Conference on Human Factors in Computing Systems (CHI)"}

# Journals
@STRING{IJCV="International Journal of Computer Vision (IJCV)"}
@STRING{TPAMI="IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)"}
@STRING{TNN="IEEE Transactions on Neural Networks"}
@STRING{TMM="IEEE Transactions on Multimedia"}
@STRING{TIP="IEEE Transactions on Image Processing (TIP)"}
@STRING{TVCG="IEEE Transactions on Visualization and Computer Graphics"}
@STRING{TCSVT="IEEE Transactions on Circuits and Systems for Video Technology"}
@STRING{JMLR="Journal of Machine Learning Research (JMLR)"}
@STRING{CVIU="Computer Vision and Image Understanding (CVIU)"}
@STRING{IVC="Image and Vision Computing"}
@STRING{SPMAG="IEEE Signal Processing Magazine"}
@STRING{SPL="IEEE Signal Processing Letters"}

@inproceedings{diffsal,
  title={DiffSal: Joint Audio and Video Learning for Diffusion Saliency Prediction},
  author={Xiong, Junwen and Zhang, Peng and You, Tao and Li, Chuanyue and Huang, Wei and Zha, Yufei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={27273--27283},
  year={2024}
}

@inproceedings{zhang2018shufflenet,
  title={Shufflenet: An extremely efficient convolutional neural network for mobile devices},
  author={Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian},
  booktitle=CVPR,
  year={2018}
}

@article{admoni2017social,
  title={Social eye gaze in human-robot interaction: a review},
  author={Admoni, Henny and Scassellati, Brian},
  journal={Journal of Human-Robot Interaction},
  volume={6},
  number={1},
  pages={25--63},
  year={2017},
  publisher={Journal of Human-Robot Interaction Steering Committee}
}

@inproceedings{dang2018visual,
  title={Visual saliency-aware receding horizon autonomous exploration with application to aerial robotics},
  author={Dang, Tung and Papachristos, Christos and Alexis, Kostas},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={2526--2533},
  year={2018},
  organization={IEEE}
}

@article{chan2020unseen,
  title={Unseen salient object discovery for monocular robot vision},
  author={Chan, Darren M and Riek, Laurel D},
  journal={IEEE Robotics and Automation Letters},
  volume={5},
  number={2},
  pages={1484--1491},
  year={2020},
  publisher={IEEE}
}

@article{ferreira2014attentional,
  title={Attentional mechanisms for socially interactive robots--a survey},
  author={Ferreira, Joao Filipe and Dias, Jorge},
  journal={IEEE Transactions on Autonomous Mental Development},
  volume={6},
  number={2},
  pages={110--125},
  year={2014},
  publisher={IEEE}
}

@inproceedings{agrawal2022does,
  title={Does Audio help in deep Audio-Visual Saliency prediction models?},
  author={Agrawal, Ritvik and Jyoti, Shreyank and Girmaji, Rohit and Sivaprasad, Sarath and Gandhi, Vineet},
  booktitle={Proceedings of the 2022 International Conference on Multimodal Interaction},
  pages={48--56},
  year={2022}
}

@article{schillaci2013evaluating,
  title={Evaluating the effect of saliency detection and attention manipulation in human-robot interaction},
  author={Schillaci, Guido and Bodiro{\v{z}}a, Sa{\v{s}}a and Hafner, Verena Vanessa},
  journal={International Journal of Social Robotics},
  volume={5},
  pages={139--152},
  year={2013},
  publisher={Springer}
}

@inproceedings{chang2019salgaze,
  title={Salgaze: Personalizing gaze estimation using visual saliency},
  author={Chang, Zhuoqing and Matias Di Martino, J and Qiu, Qiang and Espinosa, Steven and Sapiro, Guillermo},
  booktitle=ICCVW,
  year={2019}
}

@inproceedings{butko2008visual,
  title={Visual saliency model for robot cameras},
  author={Butko, Nicholas J and Zhang, Lingyun and Cottrell, Garrison W and Movellan, Javier R},
  booktitle={2008 IEEE International Conference on Robotics and Automation},
  pages={2398--2403},
  year={2008},
  organization={IEEE}
}

@inproceedings{mavani2017facial,
  title={Facial expression recognition using visual saliency and deep learning},
  author={Mavani, Viraj and Raman, Shanmuganathan and Miyapuram, Krishna P},
  booktitle=ICCVW,
  year={2017}
}

@inproceedings{rachavarapu2018watch,
  title={Watch to edit: Video retargeting using gaze},
  author={Rachavarapu, Kranthi Kumar and Kumar, Moneish and Gandhi, Vineet and Subramanian, Ramanathan},
  booktitle={Computer Graphics Forum},
  volume={37},
  number={2},
  pages={205--215},
  year={2018},
  organization={Wiley Online Library}
}

@article{ZHU2018511,
    title = {Spatiotemporal visual saliency guided perceptual high efficiency video coding with neural network},
    journal = {Neurocomputing},
    volume = {275},
    pages = {511-522},
    year = {2018},
    issn = {0925-2312},
    doi = {https://doi.org/10.1016/j.neucom.2017.08.054},
    url = {https://www.sciencedirect.com/science/article/pii/S0925231217314844},
    author = {Shiping Zhu and Ziyao Xu},
    keywords = {Perception, HD video, Saliency, Video compression, HEVC},
}


@INPROCEEDINGS{ava,
  author={Gu, Chunhui and Sun, Chen and Ross, David A. and Vondrick, Carl and Pantofaru, Caroline and Li, Yeqing and Vijayanarasimhan, Sudheendra and Toderici, George and Ricco, Susanna and Sukthankar, Rahul and Schmid, Cordelia and Malik, Jitendra},
  booktitle=CVPR, 
  title={AVA: A Video Dataset of Spatio-Temporally Localized Atomic Visual Actions}, 
  year={2018},
  volume={},
  number={},
  pages={6047-6056},
}

@article{LANGTON200050,
title = {Do the eyes have it? Cues to the direction of social attention},
journal = {Trends in Cognitive Sciences},
volume = {4},
number = {2},
pages = {50-59},
year = {2000},
issn = {1364-6613},
doi = {https://doi.org/10.1016/S1364-6613(99)01436-9},
url = {https://www.sciencedirect.com/science/article/pii/S1364661399014369},
author = {Stephen R.H. Langton and Roger J. Watt and Vicki Bruce},
keywords = {Social attention, Gaze perception, Gaze following, Pointing gestures, Gaze cueing},
}

@INPROCEEDINGS{7780459,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle=CVPR, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  keywords={Training;Degradation;Complexity theory;Image recognition;Neural networks;Visualization;Image segmentation},
  doi={10.1109/CVPR.2016.90}
}

@INPROCEEDINGS{8100116,
  author={Ioannou, Yani and Robertson, Duncan and Cipolla, Roberto and Criminisi, Antonio},
  booktitle=CVPR, 
  title={Deep Roots: Improving CNN Efficiency with Hierarchical Filter Groups}, 
  year={2017},
  volume={},
  number={}
}

@ARTICLE{dhf1kold,
  author={Wang, Wenguan and Shen, Jianbing and Xie, Jianwen and Cheng, Ming-Ming and Ling, Haibin and Borji, Ali},
  journal=TPAMI, 
  title={Revisiting Video Saliency Prediction in the Deep Learning Era}, 
  year={2021},
  volume={43},
  number={1},
  pages={220-237}
}

@article{dhf1k,
  title={Revisiting video saliency prediction in the deep learning era},
  author={Wang, Wenguan and Shen, Jianbing and Xie, Jianwen and Cheng, Ming-Ming and Ling, Haibin and Borji, Ali},
  journal={TPAMI},
  volume={43},
  number={1},
  pages={220--237},
  year={2019},
  publisher={IEEE}
}

@ARTICLE{hollywood-ucfold,
  author={Mathe, Stefan and Sminchisescu, Cristian},
  journal=TPAMI, 
  title={Actions in the Eye: Dynamic Gaze Datasets and Learnt Saliency Models for Visual Recognition}, 
  year={2015},
  volume={37},
  number={7},
  pages={1408-1424}
}

@article{hollywood-ucf,
  title={Actions in the eye: Dynamic gaze datasets and learnt saliency models for visual recognition},
  author={Mathe, Stefan and Sminchisescu, Cristian},
  journal={TPAMI},
  volume={37},
  number={7},
  pages={1408--1424},
  year={2014},
  publisher={IEEE}
}

@InProceedings{etmd,
author="Koutras, Petros
and Katsamanis, Athanasios
and Maragos, Petros",
editor="Harris, Don",
title="Predicting Eyes' Fixations in Movie Videos: Visual Saliency Experiments on a New Eye-Tracking Database",
booktitle="Engineering Psychology and Cognitive Ergonomics",
year="2014",
pages="183--194"
}

@inproceedings{mvva,
  title={Learning to predict salient faces: A novel visual-audio saliency model},
  author={Liu, Yufan and Qiao, Minglang and Xu, Mai and Li, Bing and Hu, Weiming and Borji, Ali},
  booktitle=ECCV,
  pages={413--429},
  year={2020}
}

@inproceedings{summe,
   author ={Gygli, Michael and Grabner, Helmut and Riemenschneider, Hayko and Van Gool, Luc},
   title = {Creating Summaries from User Videos},
   booktitle =ECCV,
   year = {2014}
}

@article{coutrot1,
author = {Coutrot, Antoine and Guyader, Nathalie and Ionescu, Gelu and Caplier, Alice},
year = {2012},
month = {08},
pages = {1-10},
title = {Influence of soundtrack on eye movements during video exploration},
volume = {5},
journal = {Journal of Eye Movement Research},
doi = {10.16910/jemr.5.4.2}
}

@article{coutrot1conf,
  title={How saliency, faces, and sound influence gaze in dynamic social scenes},
  author={Coutrot, Antoine and Guyader, Nathalie},
  journal={Journal of vision},
  volume={14},
  number={8},
  pages={5--5},
  year={2014},
  publisher={The Association for Research in Vision and Ophthalmology}
}

@INPROCEEDINGS{coutrot1journ,
  author={Coutrot, Antoine and Guyader, Nathalie},
  booktitle={2013 14th International Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS)}, 
  title={Toward the introduction of auditory information in dynamic visual attention models}, 
  year={2013},
  volume={},
  number={},
  pages={1-4},
  doi={10.1109/WIAMIS.2013.6616164}}

@inproceedings{coutrot2,
  title={An efficient audiovisual saliency model to predict eye positions when looking at conversations},
  author={Coutrot, Antoine and Guyader, Nathalie},
  booktitle={2015 23rd European Signal Processing Conference (EUSIPCO)},
  pages={1531--1535},
  year={2015},
  organization={IEEE}
}

@article{coutrot,
    author = {Coutrot, Antoine and Guyader, Nathalie},
    title = "{How saliency, faces, and sound influence gaze in dynamic social scenes}",
    journal = {Journal of Vision},
    volume = {14},
    number = {8},
    pages = {5-5},
    year = {2014},
    month = {07},
    issn = {1534-7362},
    doi = {10.1167/14.8.5},
    url = {https://doi.org/10.1167/14.8.5},
    eprint = {https://arvojournals.org/arvo/content\_public/journal/jov/933549/i1534-7362-14-8-5.pdf},
}

@article{avad,
author = {Min, Xiongkuo and Zhai, Guangtao and Gu, Ke and Yang, Xiaokang},
title = {Fixation Prediction through Multimodal Analysis},
year = {2016},
volume = {13},
number = {1},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.}
}

@INPROCEEDINGS{tinyhd,
  author={Hu, Feiyan and Palazzo, Simone and Salanitri, Federica Proietto and Bellitto, Giovanni and Moradi, Morteza and Spampinato, Concetto and McGuinness, Kevin},
  booktitle=WACV, 
  title={TinyHD: Efficient Video Saliency Prediction with Heterogeneous Decoders using Hierarchical Maps Distillation}, 
  year={2023},
  volume={},
  number={}
}

@article{ma2022video,
  title={Video saliency forecasting transformer},
  author={Ma, Cheng and Sun, Haowen and Rao, Yongming and Zhou, Jie and Lu, Jiwen},
  journal=TCSVT,
  volume={32},
  number={10},
  pages={6850--6862},
  year={2022},
  publisher={IEEE}
}

@article{diem,
  title={Clustering of Gaze During Dynamic Scene Viewing is Predicted by Motion},
  author={Parag K. Mital and Tim J. Smith and Robin L. Hill and John M. Henderson},
  journal={Cognitive Computation},
  year={2011},
  volume={3},
  pages={5-24}
}

@ARTICLE{metrics,
  author={Bylinskii, Zoya and Judd, Tilke and Oliva, Aude and Torralba, Antonio and Durand, Fr√©do},
  journal=PAMI, 
  title={What Do Different Evaluation Metrics Tell Us About Saliency Models?}, 
  year={2019},
  volume={41},
  number={3},
  pages={740-757},
  keywords={Measurement;Computational modeling;Analytical models;Visualization;Benchmark testing;Observers;Task analysis;Saliency models;evaluation metrics;benchmarks;fixation maps;saliency applications},
  doi={10.1109/TPAMI.2018.2815601}
}

@inproceedings{overt_va_robot,
  author={Vijayakumar, S. and Conradt, J. and Shibata, T. and Schaal, S.},
  booktitle={Proceedings 2001 IEEE/RSJ International Conference on Intelligent Robots and Systems. Expanding the Societal Role of Robotics in the the Next Millennium (Cat. No.01CH37180)}, 
  title={Overt visual attention for a humanoid robot}, 
  year={2001},
  volume={4},
  number={},
  pages={2332-2337 vol.4},
  keywords={Humanoid robots;Robot kinematics;Process control;Biological control systems;Control systems;Biology computing;Humans;Anthropomorphism;Artificial neural networks;Smoothing methods},
  doi={10.1109/IROS.2001.976418}
}

@inproceedings{va_net_robot,
  author={Driscoll, J.A. and Peters, R.A. and Cave, K.R.},
  booktitle={Proceedings. 1998 IEEE/RSJ International Conference on Intelligent Robots and Systems. Innovations in Theory, Practice and Applications (Cat. No.98CH36190)}, 
  title={A visual attention network for a humanoid robot}, 
  year={1998},
  volume={3},
  number={},
  pages={1968-1974 vol.3},
  keywords={Humanoid robots;Robot sensing systems;Robot vision systems;Control systems;Layout;Cameras;Psychology;Human robot interaction;Concurrent computing;Distributed computing},
  doi={10.1109/IROS.1998.724894}
}

@article{kocak2021gated,
  title={A gated fusion network for dynamic saliency prediction},
  author={Kocak, Aysun and Erdem, Erkut and Erdem, Aykut},
  journal={IEEE Transactions on Cognitive and Developmental Systems},
  volume={14},
  number={3},
  pages={995--1008},
  year={2021},
  publisher={IEEE}
}

@ARTICLE{8543830,
  author={Zhang, Kao and Chen, Zhenzhong},
  journal=TCSVT, 
  title={Video Saliency Prediction Based on Spatial-Temporal Two-Stream Network}, 
  year={2019},
  volume={29},
  number={12},
  pages={3544-3557},
  keywords={Feature extraction;Predictive models;Streaming media;Visualization;Spatiotemporal phenomena;Computational modeling;Video saliency;spatial-temporal features;visual attention;deep learning},
  doi={10.1109/TCSVT.2018.2883305}}

@article{st_net_sp,
  author={Bak, Cagdas and Kocak, Aysun and Erdem, Erkut and Erdem, Aykut},
  journal=TMM, 
  title={Spatio-Temporal Saliency Networks for Dynamic Saliency Prediction}, 
  year={2018},
  volume={20},
  number={7},
  pages={1688-1698},
  keywords={Videos;Feature extraction;Predictive models;Computational modeling;Visualization;Dynamics;Dynamic saliency;deep learning},
  doi={10.1109/TMM.2017.2777665}
}

@article{cnn_vsp,
  author={Wu, Zhe and Su, Li and Huang, Qingming},
  journal=TCSVT, 
  title={Learning Coupled Convolutional Networks Fusion for Video Saliency Prediction}, 
  year={2019},
  volume={29},
  number={10},
  pages={2960-2971},
  keywords={Feature extraction;Visualization;Computational modeling;Spatiotemporal phenomena;Training;Biological system modeling;Data mining;Video saliency;feature integration;fully convolutional network},
  doi={10.1109/TCSVT.2018.2870954}
}

@article{vsp_two_srteam,
  author={Zhang, Kao and Chen, Zhenzhong},
  journal=TCSVT, 
  title={Video Saliency Prediction Based on Spatial-Temporal Two-Stream Network}, 
  year={2019},
  volume={29},
  number={12},
  pages={3544-3557},
  keywords={Feature extraction;Predictive models;Streaming media;Visualization;Spatiotemporal phenomena;Computational modeling;Video saliency;spatial-temporal features;visual attention;deep learning},
  doi={10.1109/TCSVT.2018.2883305}
}

@INPROCEEDINGS{image_to_vsp,
  author={Gorji, Siavash and Clark, James J.},
  booktitle=CVPR, 
  title={Going from Image to Video Saliency: Augmenting Image Salience with Dynamic Attentional Push}, 
  year={2018},
  volume={},
  number={},
  pages={7501-7511},
  keywords={Visualization;Computational modeling;Spatiotemporal phenomena;Color;Dynamics;Fuses;Optical imaging},
  doi={10.1109/CVPR.2018.00783}
}

@ARTICLE{stranet,
  author={Lai, Qiuxia and Wang, Wenguan and Sun, Hanqiu and Shen, Jianbing},
  journal=TIP, 
  title={Video Saliency Prediction Using Spatiotemporal Residual Attentive Networks}, 
  year={2020},
  volume={29},
  number={},
  pages={1113-1126},
  keywords={Visualization;Computational modeling;Spatiotemporal phenomena;Dynamics;Task analysis;Data models;Predictive models;Dynamic eye-fixation prediction;residual attentive learning;attention mechanism;deep learning;video saliency},
  doi={10.1109/TIP.2019.2936112}
}

@misc{linardos2019simplevscomplextemporal,
      title={Simple vs complex temporal recurrences for video saliency prediction}, 
      author={Panagiotis Linardos and Eva Mohedano and Juan Jose Nieto and Noel E. O'Connor and Xavier Giro-i-Nieto and Kevin McGuinness},
      year={2019},
      eprint={1907.01869},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1907.01869}, 
}

@article{zhou2023transformer,
  title={Transformer-based multi-scale feature integration network for video saliency prediction},
  author={Zhou, Xiaofei and Wu, Songhe and Shi, Ran and Zheng, Bolun and Wang, Shuai and Yin, Haibing and Zhang, Jiyong and Yan, Chenggang},
  journal=TCSVT,
  volume={33},
  number={12},
  pages={7696--7707},
  year={2023},
  publisher={IEEE}
}

@inproceedings{wu2020salsac,
  title={Salsac: A video saliency prediction model with shuffled attentions and correlation-based convlstm},
  author={Wu, Xinyi and Wu, Zhenyao and Zhang, Jinglin and Ju, Lili and Wang, Song},
  booktitle=AAAI,
  volume={34},
  number={07},
  pages={12410--12417},
  year={2020}
}

@inproceedings{jain2021vinet,
  title={Vinet: Pushing the limits of visual modality for audio-visual saliency prediction},
  author={Jain, Samyak and Yarlagadda, Pradeep and Jyoti, Shreyank and Karthik, Shyamgopal and Subramanian, Ramanathan and Gandhi, Vineet},
  booktitle={IROS},
  pages={3520--3527},
  year={2021},
  organization={IEEE}
}

@inproceedings{tsiami2020stavis,
  title={Stavis: Spatio-temporal audiovisual saliency network},
  author={Tsiami, Antigoni and Koutras, Petros and Maragos, Petros},
  booktitle=CVPR,
  pages={4766--4776},
  year={2020}
}

@article{tsfpnet,
  title={Temporal-spatial feature pyramid for video saliency detection},
  author={Chang, Qinyao and Zhu, Shiping},
  journal={arXiv preprint arXiv:2105.04213},
  year={2021}
}

@inproceedings{min2019tased,
  title={Tased-net: Temporally-aggregating spatial encoder-decoder network for video saliency detection},
  author={Min, Kyle and Corso, Jason J},
  booktitle=ICCV,
  pages={2394--2403},
  year={2019}
}

@inproceedings{s3d,
  title={Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification},
  author={Xie, Saining and Sun, Chen and Huang, Jonathan and Tu, Zhuowen and Murphy, Kevin},
  booktitle=ECCV,
  year={2018}
}

@article{kay2017kinetics,
  title={The kinetics human action video dataset},
  author={Kay, Will and Carreira, Joao and Simonyan, Karen and Zhang, Brian and Hillier, Chloe and Vijayanarasimhan, Sudheendra and Viola, Fabio and Green, Tim and Back, Trevor and Natsev, Paul and others},
  journal={arXiv preprint arXiv:1705.06950},
  year={2017}
}

@article{hd2s,
  title={Hierarchical domain-adapted feature learning for video saliency prediction},
  author={Bellitto, Giovanni and Proietto Salanitri, Federica and Palazzo, Simone and Rundo, Francesco and Giordano, Daniela and Spampinato, Concetto},
  journal=IJCV,
  volume={129},
  pages={3216--3232},
  year={2021},
  publisher={Springer}
}

@inproceedings{ronneberger2015unet,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={MICCAI},
  year={2015}
}

@inproceedings{acrn,
  title={Actor-centric relation network},
  author={Sun, Chen and Shrivastava, Abhinav and Vondrick, Carl and Murphy, Kevin and Sukthankar, Rahul and Schmid, Cordelia},
  booktitle=ECCV,
  year={2018}
}

@inproceedings{slowfast,
  title={Slowfast networks for video recognition},
  author={Feichtenhofer, Christoph and Fan, Haoqi and Malik, Jitendra and He, Kaiming},
  booktitle=ICCV,
  pages={6202--6211},
  year={2019}
}

@inproceedings{acarnet,
  title={Actor-context-actor relation network for spatio-temporal action localization},
  author={Pan, Junting and Chen, Siyu and Shou, Mike Zheng and Liu, Yu and Shao, Jing and Li, Hongsheng},
  booktitle=CVPR,
  pages={464--474},
  year={2021}
}

@article{stsanet,
  title={Spatio-temporal self-attention network for video saliency prediction},
  author={Wang, Ziqiang and Liu, Zhi and Li, Gongyang and Wang, Yang and Zhang, Tianhong and Xu, Lihua and Wang, Jijun},
  journal=TMM,
  volume={25},
  pages={1161--1174},
  year={2021},
  publisher={IEEE}
}

@article{wang2021spatiotemporal,
  title={Spatiotemporal module for video saliency prediction based on self-attention},
  author={Wang, Yuhao and Liu, Zhuoran and Xia, Yibo and Zhu, Chunbo and Zhao, Danpei},
  journal=IVC,
  volume={112},
  pages={104216},
  year={2021},
  publisher={Elsevier}
}

@article{tmfinet,
  title={Transformer-based multi-scale feature integration network for video saliency prediction},
  author={Zhou, Xiaofei and Wu, Songhe and Shi, Ran and Zheng, Bolun and Wang, Shuai and Yin, Haibing and Zhang, Jiyong and Yan, Chenggang},
  journal=TCSVT,
  volume={33},
  number={12},
  pages={7696--7707},
  year={2023},
  publisher={IEEE}
}

@inproceedings{videoswin,
  title={Video swin transformer},
  author={Liu, Ze and Ning, Jia and Cao, Yue and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Hu, Han},
  booktitle=CVPR,
  pages={3202--3211},
  year={2022}
}

@inproceedings{Lin_2017,
author = {Lin, Tianwei and Zhao, Xu and Shou, Zheng},
title = {Single Shot Temporal Action Detection},
year = {2017},
isbn = {9781450349062},
publisher = {Association for Computing Machinery},
booktitle = {Proceedings of the 25th ACM International Conference on Multimedia},
}

@article{thtdnet,
  title={Transformer-based Video Saliency Prediction with High Temporal Dimension Decoding},
  author={Moradi, Morteza and Palazzo, Simone and Spampinato, Concetto},
  journal={VISIGRAPP},
  year={2024}
}

@inproceedings{hara2018can,
  title={Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?},
  author={Hara, Kensho and Kataoka, Hirokatsu and Satoh, Yutaka},
  booktitle=CVPR,
  pages={6546--6555},
  year={2018}
}

@article{aytar2016soundnet,
  title={Soundnet: Learning sound representations from unlabeled video},
  author={Aytar, Yusuf and Vondrick, Carl and Torralba, Antonio},
  journal=NeurIPS,
  volume={29},
  year={2016}
}

@inproceedings{fpn,
  title={Feature pyramid networks for object detection},
  author={Lin, Tsung-Yi and Doll{\'a}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
  booktitle=CVPR,
  pages={2117--2125},
  year={2017}
}

@inproceedings{xiong2023casp,
  title={CASP-Net: Rethinking video saliency prediction from an audio-visual consistency perceptual perspective},
  author={Xiong, Junwen and Wang, Ganglai and Zhang, Peng and Huang, Wei and Zha, Yufei and Zhai, Guangtao},
  booktitle=CVPR,
  pages={6441--6450},
  year={2023}
}


@inproceedings{moorthy2020gazed,
  title={Gazed--gaze-guided cinematic editing of wide-angle monocular video recordings},
  author={Moorthy, KL Bhanu and Kumar, Moneish and Subramanian, Ramanathan and Gandhi, Vineet},
  booktitle=CHI,
  pages={1--11},
  year={2020}
}

@article{lateef2021saliency,
  title={Saliency heat-map as visual attention for autonomous driving using generative adversarial network (GAN)},
  author={Lateef, Fahad and Kas, Mohamed and Ruichek, Yassine},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  volume={23},
  number={6},
  pages={5360--5373},
  year={2021},
  publisher={IEEE}
}

@article{hadizadeh2013saliency,
  title={Saliency-aware video compression},
  author={Hadizadeh, Hadi and Baji{\'c}, Ivan V},
  journal=TIP,
  volume={23},
  number={1},
  pages={19--33},
  year={2013},
  publisher={IEEE}
}

@article{vamnet,
  title={Joint Learning of Audio-Visual Saliency Prediction and Sound Source Localization on Multi-face Videos},
  author={Minglang Qiao and Yufan Liu and Mai Xu and Xin Deng and Bing Li and Weiming Hu and Ali Borji},
  journal=IJCV,
  year={2023},
  volume={132},
  pages={2003-2025}
}

@article{choi2019can,
  title={Why can't i dance in the mall? learning to mitigate scene bias in action recognition},
  author={Choi, Jinwoo and Gao, Chen and Messou, Joseph CE and Huang, Jia-Bin},
  journal=NeurIPS,
  volume={32},
  year={2019}
}

@inproceedings{droste2020unified,
  title={Unified image and video saliency modeling},
  author={Droste, Richard and Jiao, Jianbo and Noble, J Alison},
  booktitle=ECCV,
  pages={419--435},
  year={2020},
  organization={Springer}
}
