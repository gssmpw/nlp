\section{Related Works}
% \label{sec:related_wroks}

	%-------------------------------------------------------------------------
	% \paragraph{Video Saliency Prediction} Video saliency prediction has historically been significant to the computer vision community ~\cite{overt_va_robot}. Driscoll~\etal ~\cite{va_net_robot} were among the first to discuss using neural networks for saliency map prediction in humanoid robots. SP models thrived with the surge of deep learning over the last decade. Bak~\etal~\cite{st_net_sp} proposed two-stream networks that utilized two convolutional backbones for leveraging RGB images and optical flow maps as input, respectively, to extract spatio-temporal features and fuse their outputs for saliency prediction. Wu~\etal~\cite{cnn_vsp}, and Zhang and Chen~\cite{vsp_two_srteam} additionally examined the two-stream structure and analyzed fusion methods to improve performance.

	% Since optical flow networks simply consider adjacent frames for modelling the temporal relations in the video, LSTM-based methods were employed to model long-term temporal dependencies efficiently. Gorji and Clark ~\cite{image_to_vsp} make use of a multi-stream Convolutional Long Short-Term Memory network (ConvLSTM) that incorporates static saliency and learns long-term temporal dependencies to estimate SP. Wang~\etal~\cite{dhf1k} propose ACLNet to extend CNN-LSTM architecture using a supervised attention mechanism. Lai~\etal~\cite{stranet} presents STRA-Net, a spatio-temporal residual network based on ConvGRU, to model the attention transition across frames. SalEMA~\cite{linardos2019simplevscomplextemporal} utilizes a simple exponential moving average for feature fusion in the temporal domain. SalSAC~\cite{wu2020salsac} first extracts multi-level features and then uses a random shuffling mechanism for the multi-level attentions calculated from the multi-level features followed by a correlation-based ConvLSTM.

	% 3D Convolution-based architectures gained a lot of traction due to their ability to simultaneously model both spatial and temporal information. These methods typically employ action classification networks as their backbone. TASED-Net~\cite{min2019tased} is a 3D convolutional encoder-decoder network that makes use of S3D~\cite{s3d} pre-trained on the Kinetics dataset~\cite{kay2017kinetics} as the video encoder. Jain~\etal~\cite{jain2021vinet} present ViNet, a fully convolutional encoder-decoder architecture. It utilizes hierarchical features from the encoder and passes them as skip connections to the decoder in a UNet~\cite{ronneberger2015unet} like fashion to output the saliency map. We build upon this model due to its simplistic architecture, ease of training and real-time inference, augmenting the design with an efficient and lightweight decoder. STSANet~\cite{stsanet} utilizes multiple spatio-temporal self-attention modules at different levels of 3D convolutional backbone for capturing long-range relations between spatio-temporal features; however, its large model size renders it impractical for real-world applications. Recent efforts utilize transformers for SP. TMFI-Net~\cite{tmfinet} employs Video Swin Transformer~\cite{videoswin} as the video encoder and a hierarchical decoder to predict saliency maps. THTD-Net~\cite{thtdnet} makes use of Video Swin Transformer~\cite{videoswin} as well, gradually reducing the temporal dimension in the decoder layers to model long-range temporal dependencies.

	% 3D convolutional architectures are popular for modelling both spatial and temporal information, often using action classification networks as their backbone. TASED-Net~\cite{min2019tased} is a 3D convolutional encoder-decoder network utilizing S3D~\cite{s3d} pre-trained on the Kinetics dataset~\cite{kay2017kinetics}. ViNet~\cite{jain2021vinet}, a fully convolutional encoder-decoder, uses hierarchical features with UNet-like~\cite{ronneberger2015unet} skip connections. We extend this model with a lightweight decoder. STSANet~\cite{stsanet} employs spatio-temporal self-attention but is too large for practical use. Recent approaches like TMFI-Net~\cite{tmfinet} and THTD-Net~\cite{thtdnet} use Video Swin Transformer for saliency prediction, focusing on long-range temporal dependencies.

	% The field of action recognition has evolved significantly, shifting from action classification~\cite{s3d} to STAL~\cite{slowfast,acarnet}. While existing saliency models leverage backbones pre-trained on action classification tasks~\cite{s3d}, they have yet to exploit recent STAL research advancements fully. Our work addresses this shortcoming and bridges the gap between the two fields.
	% --------------------------------------------------------------------------

	%vinet_plus_architecture
	%, which predicts the final saliency map by simply taking the pixel-wise average of the outputs from the proposed models. We elaborate on the suggested models in the following sections.

	% We propose two end-to-end trainable and lightweight visual-only models, ViNet-S and ViNet-A. They are fully 3D convolutional encoder-decoder methods that predict the saliency map for the corresponding set of sequential frames.

	%-------------------------------------------------------------------------
	%ViNet-A