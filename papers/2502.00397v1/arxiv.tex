\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{xcolor}
\usepackage{pifont}
\usepackage{caption}
\usepackage{subcaption}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}

\usepackage{footnote}
\makesavenoteenv{tabular}

\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\usepackage{tikz}
\newcommand{\copyrighttext}{%
\footnotesize \textcopyright~ 2025 IEEE. Personal use of this material is
permitted. Permission from IEEE must be obtained for all other uses, in any
current or future media, including reprinting/republishing this material for advertising
or promotional purposes, creating new collective works, for resale or redistribution
to servers or lists, or reuse of any copyrighted component of this work in other
works.}
\newcommand{\copyrightnotice}{%
\begin{tikzpicture}[remember picture, overlay]
	\node[anchor=south,yshift=10pt] at (current page.south)
	{\fbox{\parbox{\dimexpr\textwidth-\fboxsep-\fboxrule\relax}{\copyrighttext}}};
\end{tikzpicture}%
}

\begin{document}
	\title{Minimalistic Video Saliency Prediction via Efficient Decoder \& Spatio
	Temporal Action Cues

	% A Minimalist Approach to Video Saliency Prediction: Leveraging Lightweight Decoder and Spatio Temporal Action Localization
	%Video Saliency Simplified: Leveraging Lightweight Decoder and Spatio-Temporal Action Detection

	% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
	% should not be used}
	% \thanks{Identify applicable funding agency here. If none, delete this.}
	}

	\author{ \IEEEauthorblockN{ Rohit Girmaji, Siddharth Jain, Bhav Beri, Sarthak Bansal, Vineet Gandhi}
	\IEEEauthorblockA{CVIT, IIIT Hyderabad, India\\ \{rohit.girmaji, siddharth.jain, bhav.beri\}@research.iiit.ac.in, sarthak.bansal@students.iiit.ac.in, vgandhi@iiit.ac.in}
	% \\
	% \IEEEauthorblockN{ Sarthak Bansal\IEEEauthorrefmark{2},
	%                    Vineet Gandhi\IEEEauthorrefmark{2}}
	% \IEEEauthorblockA{\IEEEauthorrefmark{2}IIIT Hyderabad, India\\
	%                   sarthak.bansal@students.iiit.ac.in, vgandhi@iiit.ac.in}
	}

	% \author{
	%     \IEEEauthorblockN{ Rohit Girmaji\IEEEauthorrefmark{1},
	%                        Siddharth Jain\IEEEauthorrefmark{1},
	%                        Bhav Beri\IEEEauthorrefmark{1},
	%                        Sarthak Bansal\IEEEauthorrefmark{1},
	%                        Vineet Gandhi\IEEEauthorrefmark{1}}
	%     \IEEEauthorblockA{\IEEEauthorrefmark{1}IIIT Hyderabad, India\\
	%                       \{rohit.girmaji, siddharth.jain, bhav.beri\}@research.iiit.ac.in, sarthak.bansal@students.iiit.ac.in, vgandhi@iiit.ac.in}
	%     % \\
	%     % \IEEEauthorblockN{ Sarthak Bansal\IEEEauthorrefmark{2},
	%     %                    Vineet Gandhi\IEEEauthorrefmark{2}}
	%     % \IEEEauthorblockA{\IEEEauthorrefmark{2}IIIT Hyderabad, India\\
	%     %                   sarthak.bansal@students.iiit.ac.in, vgandhi@iiit.ac.in}
	% }

	% \author{\IEEEauthorblockN{1\textsuperscript{st} Rohit Girmaji}
	% \IEEEauthorblockA{\textit{IIIT Hyderabad, India} \\
	% % \textit{}\\
	% % Hyderabad, India \\
	% rohit.girmaji@research.iiit.ac.in}
	% \and
	% \IEEEauthorblockN{2\textsuperscript{nd} Siddharth Jain}
	% \IEEEauthorblockA{\textit{IIIT Hyderabad, India} \\
	% % \textit{IIIT Hyderabad}\\
	% % Hyderabad, India \\
	% siddharth.jain@research.iiit.ac.in}
	% \and
	% \IEEEauthorblockN{3\textsuperscript{rd} Bhav Beri}
	% \IEEEauthorblockA{\textit{IIIT Hyderabad, India} \\
	% % \textit{IIIT Hyderabad}\\
	% % Hyderabad, India \\
	% bhav.beri@research.iiit.ac.in}
	% \and
	% \IEEEauthorblockN{4\textsuperscript{th} Sarthak Bansal}
	% \IEEEauthorblockA{\textit{IIIT Hyderabad, India} \\
	% % \textit{IIIT Hyderabad}\\
	% % Hyderabad, India \\
	% sarthak.bansal@students.iiit.ac.in}
	% \and
	% \IEEEauthorblockN{5\textsuperscript{th} Vineet Gandhi}
	% \IEEEauthorblockA{\textit{IIIT Hyderabad, India} \\
	% % \textit{IIIT Hyderabad}\\
	% % Hyderabad, India \\
	% vgandhi@iiit.ac.in}
	% }

	% \and
	% \IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
	% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
	% \textit{name of organization (of Aff.)}\\
	% City, Country \\
	% email address or ORCID}

	% \author{\IEEEauthorblockN{Rohit Girmaji}
	% \IEEEauthorblockA{\textit{CVIT, KCIS} \\
	% \textit{IIIT Hyderabad}\\
	% Hyderabad, India \\
	% rohit.girmaji@research.iiit.ac.in}
	% \and
	% \IEEEauthorblockN{Siddharth Jain}
	% \IEEEauthorblockA{\textit{CVIT, KCIS} \\
	% \textit{IIIT Hyderabad}\\
	% Hyderabad, India \\
	% siddharth.jain@research.iiit.ac.in}
	% \and
	% \IEEEauthorblockN{Bhav Beri}
	% \IEEEauthorblockA{\textit{CVIT, KCIS} \\
	% \textit{IIIT Hyderabad}\\
	% Hyderabad, India \\
	% bhav.beri@research.iiit.ac.in}
	% \and
	% \IEEEauthorblockN{Sarthak Bansal}
	% \IEEEauthorblockA{\textit{CVIT, KCIS} \\
	% \textit{IIIT Hyderabad}\\
	% Hyderabad, India \\
	% sarthak.bansal@students.iiit.ac.in}
	% \and
	% \IEEEauthorblockN{Vineet Gandhi}
	% \IEEEauthorblockA{\textit{CVIT, KCIS} \\
	% \textit{IIIT Hyderabad}\\
	% Hyderabad, India \\
	% vgandhi@iiit.ac.in}
	% % \and
	% % \IEEEauthorblockN{Siddharth Jain}
	% % \IEEEauthorblockA{\textit{CVIT, KCIS} \\
	% % \textit{IIIT Hyderabad}\\
	% % Hyderabad, India \\
	% % siddharth.jain@research.iiit.ac.in}
	% }

	\maketitle
	\copyrightnotice

	\begin{abstract}
		% This paper introduces ViNet-S, a 36MB model built upon the ViNet architecture, which follows a minimalistic U-Net-based design. Our approach incorporates a lightweight decoder over the original ViNet, resulting in over threefold reduction in model size and the number of parameters, all without compromising performance. Also, we propose a ViNet-A (148MB) model that utilizes spatio-temporal action localization (STAL) features, unlike the traditional video saliency prediction models that rely on action classification backbones. Our empirical studies reveal that a simple ensemble of ViNet-S and ViNet-A, achieved by averaging the predicted saliency maps, attains state-of-the-art performance on nine different visual-only and audio-visual saliency prediction datasets, even without using audio cues. Remarkably, the individual models and their ensemble have fewer parameters than the latest state-of-the-art transformer-based models. Benefiting from a non-autoregressive design, all our proposed models achieve real-time performance, while the smaller ViNet-S model achieves an impressive batched runtime performance of over 1000fps.

		This paper introduces ViNet-S, a 36MB model based on the ViNet architecture with
		a U-Net design, featuring a lightweight decoder that significantly reduces
		model size and parameters without compromising performance. Additionally,
		ViNet-A (148MB) incorporates spatio-temporal action localization (STAL)
		features, differing from traditional video saliency models that use action classification
		backbones. Our studies show that an ensemble of ViNet-S and ViNet-A, by averaging
		predicted saliency maps, achieves state-of-the-art performance on three
		visual-only and six audio-visual saliency datasets, outperforming transformer-based
		models in both parameter efficiency and real-time performance, with ViNet-S
		reaching over 1000fps.
	\end{abstract}

	\begin{IEEEkeywords}
		% component, formatting, style, styling, insert
		Video Saliency Prediction, Efficient Deep Learning, Spatio Temporal Action Cues
	\end{IEEEkeywords}

	% \section{Introduction}
	% Human visual attention (HVA) is the brain's remarkable ability to selectively focus on specific elements within the visual field while ignoring others, allowing individuals to efficiently process vast amounts of visual information by prioritizing the most relevant stimuli. Extending this capability to machines and robots represents an exciting frontier in machine learning and computer vision research. Computational Saliency Prediction (SP) endeavours to replicate HVA in dynamic scenes, enabling, for instance, a robot to exhibit intelligent behaviour by focusing on salient regions, such as a painting or a door, rather than a plain wall~\cite{butko2008visual, dang2018visual}. SP models have made substantial progress over the years and have shown considerable benefits across a wide range of applications, including automated cinematic editing~\cite{moorthy2020gazed}, human-computer interaction~\cite{chang2019salgaze, ferreira2014attentional, mavani2017facial, schillaci2013evaluating}, robotic camera control~\cite{butko2008visual}, autonomous driving~\cite{lateef2021saliency}, and video compression~\cite{ZHU2018511, hadizadeh2013saliency}.

	% The formal approach to addressing this task involves initially recording human gaze using an eye-tracking hardware device and subsequently employing this data as the reference point for training predictive models. Earlier methods for SP have either utilized a two-stream approach, which integrates optical flow and appearance features computed through separate branches~\cite{kocak2021gated,8543830}, or employed recurrent networks applied to image-level convolutional features~\cite{image_to_vsp,wu2020salsac,dhf1k}. These approaches were inadequate in capturing long-range dependencies or effectively utilizing spatial and temporal cues in tandem. Consequently, 3D convolution-based models were proposed, which yielded promising results~\cite{jain2021vinet, min2019tased}. More recently, the community has tilted towards transformer-based networks, obtaining state-of-the-art (SOTA) performance~\cite{stsanet,zhou2023transformer,thtdnet}. However, the performance improvements often entail a significant increase in parameters and network complexity.

	% We contend that 3D convolutions still hold promise and revisit the original UNet-based 3D Convolutions architecture to showcase its continued relevance. Our experiments utilize the publicly available ViNet architecture \cite{jain2021vinet}, upon which we propose several variations. Our first proposed model focuses on computational efficiency while preserving the performance of the original model. Owing to its small size, we name it ViNet-S. For ViNet-S, we introduce a lightweight decoder that leverages filter groups~\cite{8100116} and channel shuffle layers~\cite{zhang2018shufflenet}, resulting in arguably one of the most efficient models in the recent SP literature. ViNet-S achieves a threefold reduction in both model size and the number of parameters compared to the original ViNet model while also enhancing performance in the task of SP.

	% Furthermore, we observe that the majority of 3D convolution-based SP models utilize an action classification backbone such as S3D~\cite{s3d}, where the original model is trained to assign a single action label to each video. When multiple actions are present, these models can capture all movement; however, they are forced to focus on the most pronounced action during classification. This behaviour inhibits the feature learning for background actions or leads to strong motion bias. Furthermore, the classification can happen without necessarily spatially localizing the action. Action classification has also shown bias toward recognizing the scene or the objects present in the video instead of paying attention to the actual action the person is doing~\cite{choi2019can} (e.g., swimming pool scene biases the action prediction as swimming). We hypothesize that utilizing such single-label backbones limits the performance of the saliency prediction models, often requiring an understanding of the whole scene and the interaction of different participants and objects.

	% In contrast, Spatio-Temporal Action Localization (STAL) models~\cite{acarnet, slowfast, acrn} concurrently localize and classify all actions happening in the scene by considering individual proposals for each human present in the scene. Consequently, we propose a novel saliency prediction model called ViNet-A employing a STAL backbone, which is able to better capture the essence of the scene, in contrast to simply capturing motion cues or primary semantic cues (like faces). For example, \ref{fig:teaser} illustrates a frame from a video of the MVVA dataset~\cite{mvva}, where a group of people are being interviewed. While other SOTA models like ViNet~\cite{jain2021vinet} limit to head movements and end up capturing all the faces as salient, ViNet-A can correctly focus on the salient face (where all other people are looking or the person who is talking). We demonstrate that ViNet-A brings significant gains in performance, especially on human-centric datasets like MVVA.

	% Finally, we introduce ViNet-E, an ensemble of the ViNet-S and ViNet-A models. This ensemble employs a straightforward strategy of computing the pixel-wise mean of the two predicted maps to produce the final saliency map. The exceptionally compact design of ViNet-S ensures that the number of parameters remains low, allowing ViNet-E to maintain fewer parameters than recent transformer-based approaches. Our experiments demonstrate that ViNet-E surpasses the performance of prior methods when evaluated on a comprehensive set of three visual-only and six audio-visual saliency prediction datasets without utilizing the audio cues. Overall, we make the following research contributions:

	% \begin{itemize}
	%     \item We propose ViNet-S, a lightweight and efficient model utilizing merely 9 million parameters in total while still achieving superior performance compared to ViNet~\cite{jain2021vinet}.

	%     \item We also introduce ViNet-A, which utilizes a STAL backbone to enhance feature representation in videos with multiple subjects, resulting in significant performance improvements, especially in human-centric multi-person videos.

	%     \item We empirically demonstrate that an ensemble of ViNet-S and ViNet-A can achieve significant gains over the individual models and obtain SOTA results on almost all studied datasets.

	%     \item We present comprehensive qualitative and quantitative experiments utilizing nine different datasets.
	% \end{itemize}

	\section{Introduction}
	Human visual attention (HVA) enables selective focus on relevant stimuli, a
	capability that computational saliency prediction (SP) aims to replicate in
	dynamic scenes. The formal approach to addressing this task involves initially
	recording human gaze using an eye-tracking hardware device and subsequently employing
	this data as the reference point for training predictive models. SP models
	have made substantial progress over the years and have shown considerable
	benefits across a wide range of applications such as intelligent robotic
	behaviour~\cite{butko2008visual}, automated cinematic editing~\cite{moorthy2020gazed},
	human-computer interaction~\cite{chang2019salgaze, ferreira2014attentional,
	mavani2017facial, schillaci2013evaluating}, and autonomous driving~\cite{lateef2021saliency}.

	% Human visual attention (HVA) is the brain's remarkable ability to selectively focus on specific elements within the visual field while ignoring others, allowing individuals to efficiently process vast amounts of visual information by prioritizing the most relevant stimuli. Extending this capability to machines and robots represents an exciting frontier in machine learning and computer vision research. Computational Saliency Prediction (SP) endeavours to replicate HVA in dynamic scenes, enabling, for instance, a robot to exhibit intelligent behaviour by focusing on salient regions, such as a painting or a door, rather than a plain wall~\cite{butko2008visual, dang2018visual}. SP models have made substantial progress over the years and have shown considerable benefits across a wide range of applications, including automated cinematic editing~\cite{moorthy2020gazed}, human-computer interaction~\cite{chang2019salgaze, ferreira2014attentional, mavani2017facial, schillaci2013evaluating}, robotic camera control~\cite{butko2008visual}, autonomous driving~\cite{lateef2021saliency}, and video compression~\cite{ZHU2018511, hadizadeh2013saliency}.

	%
	%and more recently, transformer-based networks~\cite{tmfinet,thtdnet}, have advanced the field, albeit with increased model complexity.
	%video saliency prediction models started with recurrent neural networks ~\cite{droste2020unified,dhf1k} and then employed 3D convolutional encoder-decoder network~\cite{jain2021vinet,min2019tased}.
	%

	In the deep learning era, early SP methods used two-stream approaches~\cite{kocak2021gated,8543830}
	or recurrent networks~\cite{droste2020unified,dhf1k}, which struggled with long-range
	dependencies and spatial-temporal cues. 3D convolution-based model~\cite{jain2021vinet,
	min2019tased}
	architectures then followed, which typically utilize action classification backbones
	like S3D~\cite{s3d} pre-trained on the Kinetics dataset~\cite{kay2017kinetics}.
	ViNet~\cite{jain2021vinet}, a fully convolutional encoder-decoder, uses
	hierarchical features with UNet-like~\cite{ronneberger2015unet} skip
	connections. STSANet~\cite{stsanet} employs spatio-temporal self-attention but
	is too large for practical use. Recent approaches like TMFI-Net~\cite{tmfinet}
	and THTD-Net~\cite{thtdnet} use Video Swin Transformer for saliency prediction,
	focusing on long-range temporal dependencies.

	Prior works have also explored combining audio and visual modalities for saliency
	prediction. STAViS~\cite{tsiami2020stavis} combines spatio-temporal visual and
	auditory features with linear weighting. TSFP-Net~\cite{tsfpnet} builds a temporal-spatial
	feature pyramid, fusing audio and visual features with attention mechanisms. VAM-Net~\cite{vamnet},
	VASM~\cite{mvva} employs multi-stream and multi-modal networks to predict
	saliency maps. CASP-Net~\cite{xiong2023casp} associates video frames with sound
	sources using a two-stream encoder. Recently, DiffSal~\cite{diffsal} introduced
	a diffusion-based approach for audio-visual saliency modelling; however, it suffers
	from heightened computational complexity and substantially slower inference speeds.
	In contrast, Our work focuses solely on optimizing the visual modality.

	We revisit 3D convolutions with the ViNet architecture~\cite{jain2021vinet},
	proposing ViNet-S, a computationally efficient model with a lightweight decoder
	using filter groups~\cite{8100116} and channel shuffle layers~\cite{zhang2018shufflenet},
	achieving a threefold reduction in size and parameters while improving SP
	performance. We also identify limitations in using action classification backbones
	like S3D~\cite{s3d}, which may miss background actions due to a focus on
	primary motion. Instead, we propose ViNet-A, leveraging Spatio-Temporal Action
	Localization (STAL)~\cite{acarnet, slowfast} with our lightweight decoder,
	which localizes and classifies actions within the scene, better capturing scene
	essence. ViNet-A excels, particularly in human-centric datasets like MVVA~\cite{mvva},
	by focusing on the most relevant features, such as the salient face in group settings.

	We further introduce ViNet-E, an ensemble of ViNet-S and ViNet-A, combining their
	strengths by averaging their predicted saliency maps. Despite its compact
	design, ViNet-E outperforms transformer-based approaches on various datasets without
	using audio cues. Our contributions include: 1) ViNet-S: A lightweight model with
	9 million parameters, surpassing the original ViNet~\cite{jain2021vinet} in
	performance. 2) ViNet-A: Utilizing a STAL backbone for enhanced performance in
	videos with multiple subjects. 3) ViNet-E: An ensemble of ViNet-S and ViNet-A,
	achieving SOTA results across multiple datasets. 4) Extensive experiments on nine
	datasets, providing qualitative and quantitative insights.
	% \begin{itemize}
	%     \item ViNet-S: A lightweight model with 9 million parameters, surpassing the original ViNet~\cite{jain2021vinet} in performance.
	%     \item ViNet-A: Utilizing a STAL backbone for enhanced performance in videos with multiple subjects.
	%     \item ViNet-E: An ensemble of ViNet-S and ViNet-A, achieving SOTA results across multiple datasets.
	%     \item Extensive experiments on nine datasets, providing qualitative and quantitative insights.
	% \end{itemize}

	%1) We propose ViNet-S, a lightweight and efficient model utilizing merely 9 million parameters in total while still achieving superior performance compared to ViNet~\cite{jain2021vinet}. 2) We also introduce ViNet-A, which utilizes a STAL backbone to enhance feature representation in videos with multiple subjects, resulting in significant performance improvements, especially in human-centric multi-person videos. 3) We empirically demonstrate that an ensemble of ViNet-S and ViNet-A can achieve significant gains over the individual models and obtain SOTA results on almost all studied datasets. 4) We present comprehensive qualitative and quantitative experiments utilizing nine different datasets.
	% \section{Proposed Model Architecture}

	\begin{figure*}[t!]
		\centering
		\includegraphics[width=\textwidth]{Images/eeaa_architecture.drawio.png}
		\caption{Our Model (ViNet-A) Architecture for SP (Best viewed in colour)}
		\label{fig:arch}
	\end{figure*}

	% \section{Related Works}
	% \label{sec:related_wroks}

	%-------------------------------------------------------------------------
	% \paragraph{Video Saliency Prediction} Video saliency prediction has historically been significant to the computer vision community ~\cite{overt_va_robot}. Driscoll~\etal ~\cite{va_net_robot} were among the first to discuss using neural networks for saliency map prediction in humanoid robots. SP models thrived with the surge of deep learning over the last decade. Bak~\etal~\cite{st_net_sp} proposed two-stream networks that utilized two convolutional backbones for leveraging RGB images and optical flow maps as input, respectively, to extract spatio-temporal features and fuse their outputs for saliency prediction. Wu~\etal~\cite{cnn_vsp}, and Zhang and Chen~\cite{vsp_two_srteam} additionally examined the two-stream structure and analyzed fusion methods to improve performance.

	% Since optical flow networks simply consider adjacent frames for modelling the temporal relations in the video, LSTM-based methods were employed to model long-term temporal dependencies efficiently. Gorji and Clark ~\cite{image_to_vsp} make use of a multi-stream Convolutional Long Short-Term Memory network (ConvLSTM) that incorporates static saliency and learns long-term temporal dependencies to estimate SP. Wang~\etal~\cite{dhf1k} propose ACLNet to extend CNN-LSTM architecture using a supervised attention mechanism. Lai~\etal~\cite{stranet} presents STRA-Net, a spatio-temporal residual network based on ConvGRU, to model the attention transition across frames. SalEMA~\cite{linardos2019simplevscomplextemporal} utilizes a simple exponential moving average for feature fusion in the temporal domain. SalSAC~\cite{wu2020salsac} first extracts multi-level features and then uses a random shuffling mechanism for the multi-level attentions calculated from the multi-level features followed by a correlation-based ConvLSTM.

	% 3D Convolution-based architectures gained a lot of traction due to their ability to simultaneously model both spatial and temporal information. These methods typically employ action classification networks as their backbone. TASED-Net~\cite{min2019tased} is a 3D convolutional encoder-decoder network that makes use of S3D~\cite{s3d} pre-trained on the Kinetics dataset~\cite{kay2017kinetics} as the video encoder. Jain~\etal~\cite{jain2021vinet} present ViNet, a fully convolutional encoder-decoder architecture. It utilizes hierarchical features from the encoder and passes them as skip connections to the decoder in a UNet~\cite{ronneberger2015unet} like fashion to output the saliency map. We build upon this model due to its simplistic architecture, ease of training and real-time inference, augmenting the design with an efficient and lightweight decoder. STSANet~\cite{stsanet} utilizes multiple spatio-temporal self-attention modules at different levels of 3D convolutional backbone for capturing long-range relations between spatio-temporal features; however, its large model size renders it impractical for real-world applications. Recent efforts utilize transformers for SP. TMFI-Net~\cite{tmfinet} employs Video Swin Transformer~\cite{videoswin} as the video encoder and a hierarchical decoder to predict saliency maps. THTD-Net~\cite{thtdnet} makes use of Video Swin Transformer~\cite{videoswin} as well, gradually reducing the temporal dimension in the decoder layers to model long-range temporal dependencies.

	% 3D convolutional architectures are popular for modelling both spatial and temporal information, often using action classification networks as their backbone. TASED-Net~\cite{min2019tased} is a 3D convolutional encoder-decoder network utilizing S3D~\cite{s3d} pre-trained on the Kinetics dataset~\cite{kay2017kinetics}. ViNet~\cite{jain2021vinet}, a fully convolutional encoder-decoder, uses hierarchical features with UNet-like~\cite{ronneberger2015unet} skip connections. We extend this model with a lightweight decoder. STSANet~\cite{stsanet} employs spatio-temporal self-attention but is too large for practical use. Recent approaches like TMFI-Net~\cite{tmfinet} and THTD-Net~\cite{thtdnet} use Video Swin Transformer for saliency prediction, focusing on long-range temporal dependencies.

	% The field of action recognition has evolved significantly, shifting from action classification~\cite{s3d} to STAL~\cite{slowfast,acarnet}. While existing saliency models leverage backbones pre-trained on action classification tasks~\cite{s3d}, they have yet to exploit recent STAL research advancements fully. Our work addresses this shortcoming and bridges the gap between the two fields.
	% --------------------------------------------------------------------------

	%vinet_plus_architecture
	%, which predicts the final saliency map by simply taking the pixel-wise average of the outputs from the proposed models. We elaborate on the suggested models in the following sections.

	% We propose two end-to-end trainable and lightweight visual-only models, ViNet-S and ViNet-A. They are fully 3D convolutional encoder-decoder methods that predict the saliency map for the corresponding set of sequential frames.

	%-------------------------------------------------------------------------
	%ViNet-A
	\section{Proposed Model Architecture}
	\label{sec:model_arch} We propose an end-to-end trainable visual-only model
	called ViNet-A (Figure \ref{fig:arch}). It is a fully 3D-convolutional encoder-decoder
	architecture consisting of a SlowFast network~\cite{slowfast} as the video encoder,
	a convolutional neck, and an efficient, lightweight decoder to reduce
	computational costs for predicting the saliency map. We also propose a variation
	of the ViNet architecture~\cite{jain2021vinet}, ViNet-S, which utilizes our efficient
	decoder, resulting in a small model while surpassing the original ViNet's
	performance. Lastly, we propose an ensemble of the two proposed models, ViNet-E.
	We elaborate on the proposed models in the following sections.
	\subsection{ViNet-A}
	\subsubsection{Backbone}
	% The architecture employs the SlowFast network ~\cite{9008780} as the video encoder. We use the model pre-trained on the AVA dataset ~\cite{8578731}, an action detection dataset with dense annotations for atomic actions localized across space and time. We utilize SlowFast due to its dual-path architecture that effectively captures spatial semantics at low temporal resolution and rapidly changing motion at high temporal resolution.
	% Moreover, we chose an AVA pre-trained backbone because it encodes localized action features and human motion information in the input frames. This capability is crucial for identifying salient regions influenced by context and background activities%, as humans naturally pay more attention to other humans ~\cite{LANGTON200050}.

	% Moreover, the choice of AVA pre-trained backbone arose from its dense annotations of all people in all keyframes which is crucial for identifying salient regions influenced by context and background activities (as humans naturally pay more attention to other humans. ~\cite{LANGTON200050} ?)

	% We use ResNet-50 ~\cite{7780459} as backbone for our SlowFast video encoder, which consists of 4 convolutional blocks that outputs features from two different pathways, namely, a Slow pathway operating at low frame rate to capture spatial semantics and a Fast pathway operating at high frame rate to capture motion at fine temporal resolution, and hierarchical features $X_{1}$, $X_{2}$, $X_{3}$ and $X_{4}$ at different spatial and temporal scales.

	% Our model architecture utilizes the SlowFast network \cite{slowfast} as the video encoder, which is pre-trained on the AVA actions dataset \cite{ava}. This dataset provides dense annotations of human atomic visual actions, including actions related to poses, such as sitting and walking, as well as interactions between people and objects or other people. The choice of this specific backbone is motivated by its ability to effectively map each spatial location to its corresponding action, enabling the model to capture the localized actions of each entity present in the scene

	% The choice of this specific backbone is motivated by its ability to effectively map each spatial location to its corresponding action, allowing it to capture the localized action of each entity present in the scene.

	Our model utilizes the SlowFast network~\cite{slowfast}, pre-trained on the AVA
	actions dataset~\cite{ava} as its video encoder. This backbone effectively
	captures localized actions across spatial and temporal dimensions. The SlowFast
	network comprises of two parallel pathways: the Slow pathway, which captures spatial
	semantics at a low frame rate, and the Fast pathway, which focuses on fine-grained
	temporal motion at a high frame rate.
	% Both pathways are 3D convolutional networks with lateral connections between them to combine information. These connections are then used as skip connections to the saliency decoder.
	Both pathways are 3D convolutional networks that combine information through
	lateral connections, which are subsequently used as skip connections in the saliency
	decoder.
	% The SlowFast network consists of two parallel pathways: the Slow and Fast pathways. The Slow pathway operates at a low frame rate and captures spatial semantics, while the Fast pathway operates at a high frame rate and captures motion at fine temporal resolution. Both pathways are 3D convolutional networks with varying input sizes. Each pathway produces features at different stages of the network. To combine information from both pathways, lateral connections are made from the Fast pathway to the Slow pathway at these stages. These lateral connections are then passed to different layers of the saliency decoder as skip connections.

	%, specifically pool1, res2, res3, res4, and res5

	% We input a video clip $\textit{x}_{clip} \in \mathbb{R}^{3 \times 32 \times 256 \times 456}$ to the encoder which outputs slow features, $X_{slow} \in \mathbb{R}^{2048 \times 8 \times 16 \times 29}$, fast features, $X_{fast} \in \mathbb{R}^{256 \times 32 \times 16 \times 29}$ and hierarchical features $X_{1}$, $X_{2}$, $X_{3}$ and $X_{4}$.
	% $X_{slow} \in \mathbb{R}^{2048 \times 8 \times 16 \times 29}$, fast features, $X_{fast} \in \mathbb{R}^{256 \times 32 \times 16 \times 29}$, and multi-scale features:

	% $X_{1} \in \mathbb{R}^{80 \times 8 \times 64 \times 114}$, $X_{2} \in \mathbb{R}^{320 \times 8 \times 64 \times 114}$

	% $X_{3} \in \mathbb{R}^{640 \times 8 \times 32 \times 57}$, $X_{4} \in \mathbb{R}^{1280 \times 8 \times 16 \times 29}$

	%-------------------------------------------------------------------------
	% \subsection{Neck}
	\subsubsection{Neck}

	% consequently decreasing the -> lowering
	The neck uses $1 \times 1$ convolutional blocks to reduce the number of
	channels, lowering computational overhead. We reduce the number of channels in
	$X_{slow}$ by half. $X_{fast}$ is reshaped to double its channels while halving
	its temporal dimension and then passed through an adaptive max pool to align its
	temporal dimension with $X_{slow}$. The two are concatenated channel-wise, resulting
	in fused SlowFast features,
	$X_{slowfast}\in \mathbb{R}^{1536 \times 8 \times 16 \times 29}$.
	% Hierarchical features $X_{1}$, $X_{2}$, $X_{3}$ and $X_{4}$ are also processed with $1 \times 1$ convolutional blocks to reduce their channels by half and improve computational efficiency. %In short, the process can be described as follows:
	Similarly, hierarchical features $X_{1}$, $X_{2}$, $X_{3}$, and $X_{4}$ are processed
	through $1 \times 1$ convolutional blocks to halve their channels, improving
	computational efficiency.

	% The neck fuses the slow and fast features and processes each of the Hierarchical features $X_{1}$, $X_{2}$, $X_{3}$ and $X_{4}$ before passing them to the saliency decoder. The slow and fast features are processed according to Equations \ref{eq:slow,eq:fast} for computational efficiency and concatenated to obtain
	% SlowFast features, $X_{slowfast} \in \mathbb{R}^{1536 \times 8 \times 16 \times 29}$. Hierarchial features are processed with
	% \begin{equation}
	%   X^{'}_{slow} = ReLU(Conv^{1 \times 1}(X_{slow}))
	%   \label{eq:slow}
	% \end{equation}
	% \begin{equation}
	%     X^{'}_{fast} = AdaptiveMaxPool(Reshape(X_{fast}))
	%     \label{eq:fast}
	% \end{equation}
	% \begin{equation}
	%     X_{slowfast} = [X^{'}_{slow}, X^{'}_{fast}]
	%     \label{eq:slowfast}
	% \end{equation}
	% \begin{equation}
	%     X_{i} = ReLU(Conv^{1 \times 1}(X_{i})), i \in {1, 2, 3, 4}
	%     \label{eq:Xi}
	% \end{equation}
	% Here, ReLU represents the ReLU activation function $Conv^{1 \times 1}$ represents ${1 \times 1}$ convolutions for halving the channels and $[,]$ represents concatenation.

	%-------------------------------------------------------------------------
	% \subsection{Saliency Decoder}
	\subsubsection{Saliency Decoder}
	% The Saliency Decoder comprises six decoding blocks consisting of 3D convolutions , trilinear upsampling and channel shuffle ~\cite{zhang2018shufflenet} layers. We use 3D convolutions with filter groups ~\cite{8100116} and channel shuffle to greatly reduce computation costs while maintaining accuracy. SlowFast features, $X_{slowfast}$, are input to the decoder and hierarchical features $X_{1}$, $X_{2}$, $X_{3}$, $X_{4}$ are passed as skip connections sequentially. All 3D convolutions except the last block utilize filter groups with 32, 16, 8, 8, 4 and 2 groups, respectively. Furthermore, channel shuffle layers are inserted after the first three grouped convolution layers. We experimented with different filter groups and channel shuffle layer configurations and found this optimal. We use the ReLU activation function after every convolutional layer except the last one, which employs the Sigmoid activation function to output the predicted saliency map.

	The Saliency Decoder consists of six decoding blocks with 3D convolutions
	using filter groups~\cite{8100116}, trilinear upsampling and channel shuffle~\cite{zhang2018shufflenet}
	layers to reduce computational costs while preserving accuracy. SlowFast
	features, $X_{slowfast}$, are fed into the decoder, with hierarchical features
	$X_{i}$ passed as skip connections. %. These design choices greatly reduce computation costs while maintaining accuracy. SlowFast features, $X_{slowfast}$, are input to the decoder and hierarchical features $X_{i}$ are passed as skip connections sequentially.
	All 3D convolutions, except the last block, utilize filter groups with 32, 16,
	8, 8, 4 and 2 groups, respectively, with channel shuffle layers applied after the
	first three grouped convolutions. %. Furthermore, channel shuffle layers are inserted after the first three grouped convolution layers.
	We experimented with different filter groups and channel shuffle layer configurations
	and found this setup optimal. ReLU activations follow every convolutional
	layer, except for the last, which uses Sigmoid to predict the saliency map. % We use the ReLU activation function after every convolutional layer except the last one, which employs the Sigmoid activation function to output the predicted saliency map.

	%-------------------------------------------------------------------------
	\subsection{ViNet-S \& ViNet-E}

	ViNet-S employs the S3D~\cite{s3d} backbone as its video encoder and the
	lightweight decoder with grouped convolutions and channel shuffle layers, similar
	to the ViNet-A saliency decoder described above.

	ViNet-E is an ensemble of the proposed models, ViNet-S and ViNet-A, which generates
	a saliency map by performing a simple pixel-wise mean of the two predicted saliency
	maps. Since both models predict saliency maps of different sizes, the ViNet-S prediction
	is upsampled to match ViNet-A before averaging. %we upsample the ViNet-S prediction to match that of ViNet-A before computing the pixel-wise mean.

	% \begin{equation}
	%     Sal_{E} = Mean(Sal_{A}, Upsample(Sal_{S}))
	% \end{equation}

	% Here, $Sal_{S}$, $Sal_{A}$ and $Sal_{E}$ represent the output saliency maps of ViNet-S, ViNet-A and ViNet-E, respectively.

	\section{Experiments}

	% Bhav
	% \subsection{Datasets}

	% We conducted experiments on three visual-only saliency datasets—DHF1K, Hollywood-2, and UCF-Sports—and six audio-visual saliency prediction datasets—AVAD, Coutrot1, Coutrot2, DIEM, ETMD, and MVVA. The \textbf{\textit{DHF1K}} dataset includes 1,000 videos, split into training, validation, and testing sets, though the test set's ground truth is unavailable. \textbf{\textit{Hollywood-2}}, the largest video SP dataset with 1,707 videos from Hollywood movies, was used with a standard training-test split. \textbf{\textit{UCF-Sports}} comprises 150 sports-related videos with a standard train-test split. \textbf{\textit{AVAD}} features 45 brief video clips of various scenes; \textbf{\textit{Coutrot1}} has 60 clips of natural scenes; \textbf{\textit{Coutrot2}} includes 15 clips of meetings with eye-tracking data; \textbf{\textit{DIEM}} contains 84 video clips from various genres; \textbf{\textit{ETMD}} consists of 12 videos from Hollywood movies; and \textbf{\textit{MVVA}}, a large-scale eye-tracking database, includes 300 multi-face videos across six categories, capturing eye movements of 34 subjects in diverse settings. For the experiments on audio-visual datasets we have used only the visual components.

	\paragraph{Datasets}
	We conduct experiments on three visual-only saliency datasets - DHF1K~\cite{dhf1k},
	Hollywood-2~\cite{hollywood-ucf}, and UCF-Sports~\cite{hollywood-ucf} and six
	audio-visual saliency prediction datasets- AVAD~\cite{avad}, Coutrot1~\cite{coutrot1conf,
	coutrot1journ}, Coutrot2~\cite{coutrot2}, DIEM~\cite{diem}, ETMD~\cite{etmd} and
	MVVA~\cite{mvva}.

	% \subsubsection{Visual Datasets}

	% We conduct experiments on three major visual-only saliency datasets, namely DHF1K, Hollywood-2 and UCF-Sports.

	% \textbf{DHF1K} \cite{dhf1k} comprises 1,000 videos, with 600 designated for training and 100 for validation. Additionally, a test set of 300 videos is provided, though the ground truth for this set is not publicly available.

	% \textbf{Hollywood-2} \cite{6942210} is the largest video SP dataset in terms of the number of videos, comprising 1707 videos. This dataset features short video sequences from 69 Hollywood movies, spanning 12 distinct human action categories. Following the standard protocol, we utilize the split of 823 training videos and 884 test videos for evaluation.

	% \textbf{UCF-Sports} \cite{6942210} contains 150 videos of various sports-related actions. We employ the standard split of 103 videos for training and 47 videos for testing.

	% \subsubsection{Audio-Visual Datasets}

	% We perform extensive experiments on six audio-visual saliency prediction datasets: AVAD, Coutrot1, Coutrot2, DIEM, ETMD and MVVA. Notably, our model utilizes only the visual components of these datasets.

	% \textbf{AVAD dataset} \cite{avad} comprises 45 brief video clips, each lasting between 5-10 seconds, featuring a variety of audio-visual scenes such as dancing, guitar playing, birds singing, and more.

	% \textbf{Coutrot1} \cite{coutrot1conf, coutrot1journ} contains 60 clips depicting dynamic natural scenes, mainly consisting of four visual categories: one/several moving objects, landscapes, and faces.

	% \textbf{Coutrot2} \cite{coutrot2} comprises 15 video clips featuring four individuals engaged in a meeting, along with eye-tracking data collected from 40 participants.

	% \textbf{DIEM} \cite{diem} consists of 84 video clips based on advertisements, game trailers, movie trailers, documentaries, music videos, news clips and time-lapse footage. It contains 64 training videos and 20 testing videos.

	% \textbf{ETMD dataset} \cite{etmd} consists of 12 videos extracted from six diverse Hollywood movies.

	% % \textbf{SumMe dataset} \cite{summe} consists of 25 unstructured videos, specifically user-generated content, along with multiple summaries created by humans, which were obtained through a controlled psychological experiment.

	% \textbf{MVVA} \cite{mvva} is a large-scale, extensive eye-tracking database of multi-face videos. It contains eye movement data of 34 subjects on 300 videos captured under audio-visual conditions. The dataset consists of videos with indoor or outdoor scenes classified into six categories: TV play/movie, interview, video conference, variety show, music, and group discussion.

	% \subsection{Training}

	% \begin{table}[!t]
	%     \caption{Comparison results on the DHF1K validation set and UCF-Sports and Hollywood2 test sets. The best scores are shown in red, and the second-best scores are in blue.}
	%     \centering
	%     \begin{tabular}{ |p{1cm}|p{0.25cm}|p{0.25cm}|p{0.25cm}|p{0.25cm}|p{0.25cm}|p{0.25cm}|p{0.25cm}|p{0.25cm}|p{0.25cm}|p{0.25cm}|p{0.25cm}||p{0.25cm}|  }
	%     \hline
	%      \multirow{2}{*}{METHOD} & \multicolumn{4}{|c|}{DHF1K (Validation Set)}  & \multicolumn{4}{|c|}{UCF-Sports} & \multicolumn{4}{|c|}{Hollywood2} \\
	%      \cline{2-13}
	%     & CC$\uparrow$                      & NSS$\uparrow$                     & AUC-J$\uparrow$                   & SIM$\uparrow$
	%     & CC$\uparrow$                      & NSS$\uparrow$                    & AUC-J$\uparrow$                   & SIM$\uparrow$
	%     & CC$\uparrow$                      & NSS$\uparrow$                     & AUC-J$\uparrow$                   & SIM$\uparrow$                     &
	%     \hline
	%      & & Metric 1 & Metric 2 & Metric 3 & Metric 4 & Metric 1 & Metric 2 & Metric 3 & Metric 4 & Metric 1 & Metric 2 & Metric 3 & Metric 4 \\
	%     \hline
	%     % Add data rows here
	%     \end{tabular}
	%     \label{tab:my_label}
	% \end{table}

	% \begin{table*}[!t]
	%   \caption{Comparison results on the DHF1K validation set and UCF-Sports and Hollywood2 test sets. The best scores are shown in red, and the second-best scores are in blue.}
	%   \centering
	%   \scriptsize
	%   \begin{tabular}{@{}|l|llll|llll|llll|@{}}
	%     \hline
	%     \multirow{2}{*}{METHOD}      & \multicolumn{4}{|c|}{DHF1K (Validation Set)}  & \multicolumn{4}{|c|}{UCF-Sports} & \multicolumn{4}{|c|}{Hollywood2} \\
	%     \cline{2-13}
	%               &CC$\uparrow$                      & NSS$\uparrow$                     & AUC-J$\uparrow$                   & SIM$\uparrow$            &    CC$\uparrow$                      & NSS$\uparrow$                    & AUC-J$\uparrow$                   & SIM$\uparrow$                     & CC$\uparrow$                      & NSS$\uparrow$                     & AUC-J$\uparrow$                   & SIM$\uparrow$                     &
	%     \hline
	%     ACLNet~\cite{dhf1k} & 0.434 & 2.35 & 0.890 & 0.315 & 0.510 & 2.56 & 0.897 & 0.406 & 0.623 & 3.08 & 0.913 & 0.542 &
	%     \hline
	%     TASED-Net~\cite{min2019tased} & 0.481                   & 2.706                   & 0.894                   & 0.362                   & 0.582                   & 2.920                  & 0.899                   & 0.469                   & 0.646                   & 3.302                   & 0.918                   & 0.507                   &
	%     \hline
	%     UNISAL~\cite{droste2020unified} & 0.490 & 2.77 & 0.901 & 0.390 & 0.644 & 3.38 & 0.918 & 0.523 & 0.673 & 3.90 & 0.934 & 0.542 &
	%     \hline
	%     ViNet~\cite{jain2021vinet}     & 0.521                   & 2.957                   & 0.919                   & 0.388                   & 0.673                   & 3.620                  & 0.924                   & 0.522                   & 0.693                   & 3.730                   & 0.930                   & 0.550                   &
	%     \hline
	%     TSFP-Net~\cite{tsfpnet}  & 0.529                   & 3.009                   & 0.919                   & 0.397                   & 0.685                   & 3.698                  & 0.923                   & 0.561                   & 0.711                   & 3.910                   & 0.936                   & 0.571                   &
	%     \hline
	%     STSANet~\cite{stsanet}  & 0.539                   & 3.082                   & 0.920 & 0.411                   & 0.721  & 3.927                  & 0.936                   & 0.560                   & 0.705                   & 3.908 & 0.938                   & 0.579                   &
	%     \hline
	%     TMFI-Net~\cite{tmfinet} & \textcolor{red}{\underline{0.554}}  & \textcolor{red}{\underline{3.201}}  & \textcolor{red}{\underline{0.924}}  & \textcolor{red}{\underline{0.428}}  & 0.707                   & 3.863                  & 0.936 & 0.565 & 0.739 & 4.095  & 0.940 & \textcolor{blue}{0.607}  & \hline
	%     THTD-Net~\cite{thtdnet} & \textcolor{blue}{0.553} & \textcolor{blue}{3.188} & \textcolor{red}{\underline{0.924}}  & \textcolor{blue}{0.425} & 0.711 & 3.840                  & 0.933                   & 0.565 & 0.726                   & 3.965                   & 0.939                   & 0.585                   &
	%     \hline
	%     \hline
	%     ViNet-S & 0.529                    &  3.008                  & 0.919                   & 0.399                   & 0.673                   & 3.652                  &  0.930                  & 0.530                   & 0.728                    & 3.941                   & 0.941                   &  0.582                  &
	%     \hline
	%     ViNet-A & 0.525                   & 3.019                   & 0.916                   & 0.399                   & \textcolor{blue}{0.734}  & \textcolor{blue}{4.108} & \textcolor{blue}{0.940}   & \textcolor{blue}{0.586}  & \textcolor{blue}{0.756}  & \textcolor{blue}{4.119} & \textcolor{blue}{0.945}  & 0.604 &
	%     \hline
	%     ViNet-E & 0.549                   & 3.134                   & \textcolor{blue}{0.922}                   & 0.409                   &  \textcolor{red}{\underline{0.744}}                   & \textcolor{red}{\underline{4.156}}                  & \textcolor{red}{\underline{0.941}}                   & \textcolor{red}{\underline{0.587}}                   & \textcolor{red}{\underline{0.766}}                    & \textcolor{red}{\underline{4.168}}                   & \textcolor{red}{\underline{0.947}}                    & \textcolor{red}{\underline{0.609}}                   &
	%     \hline
	%   \end{tabular}
	%   \label{table:1}
	% \end{table*}

	\begin{table*}
		[!t]
		\centering
		\caption{Quantitative comparison of model sizes and performance on visual-only
		datasets. }
		\begin{subtable}
			[t]{0.63\textwidth}
			\centering
			\caption{Results on DHF1K validation set, UCF-Sports and Hollywood2 test sets.
			Best results highlighted in red and second best in blue.}
			\label{table:1a}
			\resizebox{1.17\columnwidth}{!}{ %
			\begin{tabular}{@{}|l|llll|llll|llll|@{}}
				\hline
				\multirow{2}{*}{METHOD}         & \multicolumn{4}{|c|}{DHF1K (Validation Set)} & \multicolumn{4}{|c|}{UCF-Sports}   & \multicolumn{4}{|c|}{Hollywood2}    \\
				\cline{2-13}                    & CC$\uparrow$                                 & NSS$\uparrow$                      & AUC-J$\uparrow$                    & SIM$\uparrow$                      & CC$\uparrow$                       & NSS$\uparrow$                      & AUC-J$\uparrow$                    & SIM$\uparrow$                      & CC$\uparrow$                       & NSS$\uparrow$                      & AUC-J$\uparrow$                    & SIM$\uparrow$                      \\  \hline
				ACLNet~\cite{dhf1k}             & 0.434                                        & 2.35                               & 0.890                              & 0.315                              & 0.510                              & 2.56                               & 0.897                              & 0.406                              & 0.623                              & 3.08                               & 0.913                              & 0.542                              \\  \hline
				TASED-Net~\cite{min2019tased}   & 0.481                                        & 2.706                              & 0.894                              & 0.362                              & 0.582                              & 2.920                              & 0.899                              & 0.469                              & 0.646                              & 3.302                              & 0.918                              & 0.507                              \\  \hline
				UNISAL~\cite{droste2020unified} & 0.490                                        & 2.77                               & 0.901                              & 0.390                              & 0.644                              & 3.38                               & 0.918                              & 0.523                              & 0.673                              & 3.90                               & 0.934                              & 0.542                              \\  \hline
				ViNet~\cite{jain2021vinet}      & 0.521                                        & 2.957                              & 0.919                              & 0.388                              & 0.673                              & 3.620                              & 0.924                              & 0.522                              & 0.693                              & 3.730                              & 0.930                              & 0.550                              \\  \hline
				TSFP-Net~\cite{tsfpnet}         & 0.529                                        & 3.009                              & 0.919                              & 0.397                              & 0.685                              & 3.698                              & 0.923                              & 0.561                              & 0.711                              & 3.910                              & 0.936                              & 0.571                              \\  \hline
				STSANet~\cite{stsanet}          & 0.539                                        & 3.082                              & 0.920                              & 0.411                              & 0.721                              & 3.927                              & 0.936                              & 0.560                              & 0.705                              & 3.908                              & 0.938                              & 0.579                              \\  \hline
				TMFI-Net~\cite{tmfinet}         & \textcolor{red}{\underline{0.554}}           & \textcolor{red}{\underline{3.201}} & \textcolor{red}{\underline{0.924}} & \textcolor{red}{\underline{0.428}} & 0.707                              & 3.863                              & 0.936                              & 0.565                              & 0.739                              & 4.095                              & 0.940                              & 0.607                              \\  \hline
				THTD-Net~\cite{thtdnet}         & \textcolor{blue}{0.553}                      & \textcolor{blue}{3.188}            & \textcolor{red}{\underline{0.924}} & \textcolor{blue}{0.425}            & 0.711                              & 3.840                              & 0.933                              & 0.565                              & 0.726                              & 3.965                              & 0.939                              & 0.585                              \\  \hline
				DiffSal~\cite{diffsal}          & 0.533                                        & 3.066                              & 0.918                              & 0.405                              & 0.685                              & 3.483                              & 0.928                              & 0.543                              & \textcolor{blue}{0.765}            & 3.955                              & \textcolor{red}{\underline{0.951}} & \textcolor{red}{\underline{0.610}} \\  \hline
				\hline
				ViNet-S                         & 0.529                                        & 3.008                              & 0.919                              & 0.399                              & 0.673                              & 3.652                              & 0.930                              & 0.530                              & 0.728                              & 3.941                              & 0.941                              & 0.582                              \\  \hline
				ViNet-A                         & 0.525                                        & 3.019                              & 0.916                              & 0.399                              & \textcolor{blue}{0.734}            & \textcolor{blue}{4.108}            & \textcolor{blue}{0.940}            & \textcolor{blue}{0.586}            & 0.756                              & \textcolor{blue}{4.119}            & 0.945                              & 0.604                              \\  \hline
				ViNet-E                         & 0.549                                        & 3.134                              & \textcolor{blue}{0.922}            & 0.409                              & \textcolor{red}{\underline{0.744}} & \textcolor{red}{\underline{4.156}} & \textcolor{red}{\underline{0.941}} & \textcolor{red}{\underline{0.587}} & \textcolor{red}{\underline{0.766}} & \textcolor{red}{\underline{4.168}} & \textcolor{blue}{0.947}            & \textcolor{blue}{0.609}            \\  \hline
			\end{tabular}
			}
		\end{subtable}
		\hfill
		\begin{subtable}
			[t]{0.258\textwidth}
			\begin{flushright}
				\caption{Quantitative comparison of model sizes \& parameters}
				\resizebox{\columnwidth}{!}{ %
				\begin{tabular}{@{}|l|P{1.5cm}|P{1.5cm}|@{}} %P{0.85cm}
					\hline
					Model                           & Size (MB) & \# Params (Million) \\  \hline
					ACLNet~\cite{dhf1k}             & 250       & 65.54               \\  \hline
					TASED-Net~\cite{min2019tased}   & 82        & 21.5                \\  \hline
					STAViS~\cite{tsiami2020stavis}  & 79.19     & 20.76               \\  \hline
					UNISAL~\cite{droste2020unified} & 15.5      & 4.06                \\  \hline
					ViNet~\cite{jain2021vinet}      & 124       & 32.5                \\  \hline
					TSFP-Net~\cite{tsfpnet}         & 58.4      & 15.3                \\ \hline
					STSA-Net~\cite{stsanet}         & 643       & 168.56              \\  \hline
					TMFI-Net~\cite{tmfinet}         & 234       & 61.34               \\  \hline
					THTD-Net~\cite{thtdnet}         & 220       & 57.67               \\  \hline
					CASP-Net~\cite{xiong2023casp}   & 196.91    & 51.62               \\  \hline
					DiffSal~\cite{diffsal}          & 269       & 70.54               \\  \hline
					\hline
					ViNet-S                         & 36.24     & 9.5                 \\  \hline
					ViNet-A                         & 147.6     & 38.69               \\  \hline
					ViNet-E                         & 183.84    & 48.19               \\  \hline
				\end{tabular}

				\label{table:sizes} }
			\end{flushright}
		\end{subtable}

		\label{table:1}
	\end{table*}

	\begin{table*}
		[!t]
		\centering
		\caption{Quantitative comparison results on the AVAD, Coutrot1, Coutrot2 and
		ETMD test sets.}
		\resizebox{\textwidth}{!}{ %
		\begin{tabular}{@{}|l|llll|llll|llll|llll|@{}}
			\hline
			\multirow{2}{*}{METHOD}        & \multicolumn{4}{|c|}{Coutrot1}     & \multicolumn{4}{|c|}{Coutrot2}     & \multicolumn{4}{|c|}{ETMD}         & \multicolumn{4}{|c|}{AVAD}          \\ %& \multicolumn{4}{|c|}{ADDED-COL} \\ % Add new column header here
			\cline{2-17}                    % Adjust the range to cover the new columns
			                               & CC$\uparrow$                       & NSS$\uparrow$                      & AUC-J$\uparrow$                    & SIM$\uparrow$                      & CC$\uparrow$                       & NSS$\uparrow$                    & AUC-J$\uparrow$                    & SIM$\uparrow$                      & CC$\uparrow$                       & NSS$\uparrow$                      & AUC-J$\uparrow$                    & SIM$\uparrow$                      & CC$\uparrow$                       & NSS$\uparrow$                      & AUC-J$\uparrow$                    & SIM$\uparrow$                      \\
			% & CC$\uparrow$                      & NSS$\uparrow$                    & AUC-J$\uparrow$                   & SIM$\uparrow$   \\ % Add metrics for the new column
			\hline
			ACLNet~\cite{dhf1k}            & 0.425                              & 1.92                               & 0.85                               & 0.361                              & 0.448                              & 3.16                             & 0.926                              & 0.322                              & 0.477                              & 2.36                               & 0.915                              & 0.329                              & 0.580                              & 3.17                               & 0.905                              & 0.446                              \\ %& 0.400 & 1.90 & 0.84 & 0.355 \\ % Add new row data here
			\hline
			TASED-Net~\cite{min2019tased}  & 0.479                              & 2.18                               & 0.867                              & 0.388                              & 0.437                              & 3.17                             & 0.921                              & 0.314                              & 0.509                              & 2.63                               & 0.916                              & 0.366                              & 0.601                              & 3.16                               & 0.914                              & 0.439                              \\ %& 0.469 & 2.20 & 0.870 & 0.389 \\ % Add new row data here
			\hline
			STAViS~\cite{tsiami2020stavis} & 0.458                              & 1.99                               & 0.861                              & 0.384                              & 0.652                              & 4.19                             & 0.940                              & 0.447                              & 0.560                              & 2.84                               & 0.929                              & 0.412                              & 0.604                              & 3.07                               & 0.915                              & 0.443                              \\ %& 0.654 & 4.20 & 0.945 & 0.450 \\ % Add new row data here
			\hline
			ViNet~\cite{jain2021vinet}     & 0.551                              & 2.68                               & 0.886                              & 0.423                              & 0.724                              & 5.61                             & 0.95                               & 0.466                              & 0.569                              & 3.06                               & 0.928                              & 0.409                              & 0.694                              & 3.82                               & 0.928                              & 0.504                              \\ %& 0.558 & 3.05 & 0.930 & 0.415 \\ % Add new row data here
			\hline
			TSFP-Net~\cite{tsfpnet}        & 0.57                               & 2.75                               & 0.894                              & 0.451                              & 0.718                              & 5.30                             & 0.957                              & 0.516                              & 0.576                              & 3.09                               & 0.932                              & 0.433                              & 0.688                              & 3.79                               & 0.932                              & 0.530                              \\ %& 0.575 & 3.10 & 0.935 & 0.430 \\ % Add new row data here
			\hline
			CASP-Net~\cite{xiong2023casp}  & 0.561                              & 2.65                               & 0.889                              & 0.456                              & 0.788                              & 6.34                             & 0.963                              & 0.585                              & 0.620                              & 3.34                               & 0.940                              & \textcolor{red}{\underline{0.478}} & 0.691                              & 3.81                               & 0.933                              & 0.528                              \\ %& 0.630 & 3.35 & 0.945 & 0.480 \\ % Add new row data here
			\hline
			\hline
			ViNet-S                        & 0.574                              & 2.876                              & 0.898                              & 0.449                              & 0.754                              & 6.103                            & 0.958                              & 0.547                              & 0.599                              & 3.268                              & \textcolor{blue}{0.941}            & 0.458                              & \textcolor{blue}{0.712}            & 4.090                              & \textcolor{blue}{0.935}            & \textcolor{blue}{0.540}            \\ %& 0.605                        & 3.275                        & 0.942                        & 0.459                        \\ % Add new row data here
			\hline
			ViNet-A                        & \textcolor{blue}{0.600}            & \textcolor{blue}{3.033}            & \textcolor{blue}{0.900}            & \textcolor{blue}{0.459}            & \textcolor{red}{\underline{0.862}} & \textcolor{red}{\underline{6.8}} & \textcolor{blue}{0.961}            & \textcolor{red}{\underline{0.638}} & \textcolor{blue}{0.623}            & \textcolor{blue}{3.379}            & \textcolor{blue}{0.941}            & 0.458                              & 0.709                              & \textcolor{blue}{4.094}            & 0.933                              & 0.534                              \\ %& \textcolor{blue}{0.625}  & \textcolor{blue}{3.380}  & \textcolor{blue}{0.943}   & \textcolor{blue}{0.460} \\ % Add new row data here
			\hline
			ViNet-E                        & \textcolor{red}{\underline{0.614}} & \textcolor{red}{\underline{3.085}} & \textcolor{red}{\underline{0.905}} & \textcolor{red}{\underline{0.465}} & \textcolor{blue}{0.854}            & \textcolor{blue}{6.762}          & \textcolor{red}{\underline{0.962}} & \textcolor{blue}{0.628}            & \textcolor{red}{\underline{0.632}} & \textcolor{red}{\underline{3.437}} & \textcolor{red}{\underline{0.943}} & \textcolor{blue}{0.468}            & \textcolor{red}{\underline{0.729}} & \textcolor{red}{\underline{4.167}} & \textcolor{red}{\underline{0.938}} & \textcolor{red}{\underline{0.547}} \\ %& \textcolor{red}{\underline{0.635}}  & \textcolor{red}{\underline{3.450}}  & \textcolor{red}{\underline{0.945}}   & \textcolor{red}{\underline{0.470}} \\ % Add new row data here
			\hline
		\end{tabular}
		}
		\label{table:2}
	\end{table*}

	% \vspace{10pt}

	\begin{table*}
		[!t]
		\centering
		\caption{Quantitative comparison results on the DIEM and MVVA test sets.}
		\begin{subtable}
			[t]{0.49\textwidth}
			\centering
			% \caption{Results on DIEM dataset.}
			\resizebox{0.72\columnwidth}{!}{ %
			\begin{tabular}{@{}|l|llll|@{}}
				\hline
				\multirow{2}{*}{METHOD}        & \multicolumn{4}{|c|}{DIEM}          \\
				\cline{2-5}                    & CC$\uparrow$                       & NSS$\uparrow$                      & AUC-J$\uparrow$                    & SIM$\uparrow$                      \\
				\hline
				ACLNet~\cite{dhf1k}            & 0.522                              & 2.02                               & 0.869                              & 0.427                              \\
				\hline
				TASED-Net~\cite{min2019tased}  & 0.557                              & 2.16                               & 0.881                              & 0.461                              \\
				\hline
				STAViS~\cite{tsiami2020stavis} & 0.579                              & 2.26                               & 0.883                              & 0.482                              \\
				\hline
				ViNet~\cite{jain2021vinet}     & 0.626                              & 2.47                               & 0.898                              & 0.483                              \\
				\hline
				TSFP-Net~\cite{tsfpnet}        & 0.651                              & 2.62                               & 0.906                              & 0.527                              \\
				\hline
				CASP-Net~\cite{xiong2023casp}  & 0.655                              & 2.61                               & 0.906                              & 0.543                              \\
				\hline
				\hline
				ViNet-S                        & 0.673                              & 2.732                              & \textcolor{blue}{0.908}            & 0.533                              \\
				\hline
				ViNet-A                        & \textcolor{blue}{0.675}            & \textcolor{blue}{2.742}            & \textcolor{blue}{0.908}            & \textcolor{blue}{0.547}            \\
				\hline
				ViNet-E                        & \textcolor{red}{\underline{0.701}} & \textcolor{red}{\underline{2.840}} & \textcolor{red}{\underline{0.913}} & \textcolor{red}{\underline{0.566}} \\
				\hline
			\end{tabular}
			}
		\end{subtable}
		\hfill
		\begin{subtable}
			[t]{0.49\textwidth}
			\centering
			% \caption{Results on MVVA dataset.}
			\resizebox{0.8\columnwidth}{!}{ %
			\begin{tabular}{@{}|l|llll|@{}}
				\hline
				\multirow{2}{*}{METHOD}        & \multicolumn{4}{|c|}{MVVA}          \\
				\cline{2-5}                    & CC$\uparrow$                       & NSS$\uparrow$                      & AUC-J$\uparrow$                    & KLDIV$\downarrow$                  \\
				\hline
				VASM~\cite{mvva}               & 0.722                              & 3.976                              & 0.905                              & 0.823                              \\
				\hline
				VAM-Net~\cite{vamnet}          & 0.741                              & 4.002                              & 0.912                              & 0.783                              \\
				\hline
				TASED-Net~\cite{min2019tased}  & 0.653                              & 3.319                              & 0.905                              & 0.970                              \\
				\hline
				STAViS~\cite{tsiami2020stavis} & 0.77                               & 3.060                              & 0.91                               & 0.80                               \\
				\hline
				ViNet~\cite{jain2021vinet}     & 0.81                               & 4.470                              & 0.93                               & 0.75                               \\
				\hline
				\hline
				ViNet-S                        & 0.802                              & 4.617                              & 0.933                              & 0.715                              \\
				\hline
				ViNet-A                        & \textcolor{blue}{0.825}            & \textcolor{red}{\underline{4.823}} & \textcolor{blue}{0.934}            & \textcolor{blue}{0.678}            \\
				\hline
				ViNet-E                        & \textcolor{red}{\underline{0.828}} & \textcolor{blue}{4.816}            & \textcolor{red}{\underline{0.936}} & \textcolor{red}{\underline{0.663}} \\
				\hline
			\end{tabular}
			}
		\end{subtable}

		\label{table:3}
	\end{table*}

	% \begin{table*}[!t]
	%   \centering

	%   \caption{Comparison results on the DIEM test sets. The best scores are shown in red, and the second-best scores are in blue.}
	% \begin{tabular}{@{}|l|P{0.85cm}|llll|@{}}
	%     \hline
	%     \multirow{2}{*}{METHOD} & Uses & \multicolumn{4}{|c|}{AVAD} \\
	%     \cline{3-6}
	%               & Audio & CC$\uparrow$                      & NSS$\uparrow$                    & AUC-J$\uparrow$                   & SIM$\uparrow$                     \\
	%     \hline
	%     ACLNet~\cite{dhf1k}    & \xmark & 0.580                   & 3.17                   & 0.905                   & 0.446                   \\
	%     \hline
	%     TASED-Net~\cite{min2019tased} & \xmark & 0.601                   & 3.16                   & 0.914                   & 0.439                   \\
	%     \hline
	%     STAViS~\cite{tsiami2020stavis}    & \cmark & 0.604                   & 3.07                   & 0.915                   & 0.443                   \\
	%     \hline
	%     ViNet~\cite{jain2021vinet}     & \xmark & 0.694 & 3.82 & 0.928                   & 0.504                   \\
	%     \hline
	%     TSFP-Net~\cite{tsfpnet}  & \cmark & 0.688                   & 3.79                   & 0.932 & 0.530 \\
	%     \hline
	%     CASP-Net~\cite{xiong2023casp}  & \cmark & 0.691                   & 3.81                   & 0.933                   & 0.528                   \\
	%     \hline
	%     \hline
	%     ViNet-S & \xmark & \textcolor{blue}{0.712}                        & 4.090                       & \textcolor{blue}{0.935}                        & \textcolor{blue}{0.540}                        \\
	%     \hline
	%     ViNet-A & \xmark & 0.709  & \textcolor{blue}{4.094} & 0.933  & 0.534  \\
	%     \hline
	%     ViNet-E & \xmark & \textcolor{red}{\underline{0.729}}                        & \textcolor{red}{\underline{4.167}}                       & \textcolor{red}{\underline{0.938}}                        & \textcolor{red}{\underline{0.547}}                        \\
	%     \hline
	% \end{tabular}

	%   \begin{tabular}{@{}|l|P{0.85cm}|llll|@{}}
	%     \hline
	%     \multirow{2}{*}{METHOD} & Uses  & \multicolumn{4}{|c|}{MVVA}   \\
	%     \cline{3-6}
	%               & Audio & CC$\uparrow$                     & NSS$\uparrow$                     & AUC-J$\uparrow$                  & KLDIV$\downarrow$                  & \hline
	%     VASM~\cite{mvva}   & \cmark & 0.722                  & 3.976                   & 0.905                  & 0.823                  & \hline
	%     VAM-Net~\cite{vamnet}   & \cmark & 0.741                  & 4.002                   & 0.912                  & 0.783                  & \hline
	%     TASED-Net~\cite{min2019tased} & \xmark & 0.653                  & 3.319                   & 0.905                  & 0.970 & \hline
	%     STAViS~\cite{tsiami2020stavis}    & \cmark & 0.77                   & 3.060                   & 0.91                   & 0.80                   & \hline
	%     ViNet~\cite{jain2021vinet}     & \xmark & 0.81 & 4.470 & 0.93 & 0.75 & \hline
	%     \hline
	%     ViNet-S & \xmark & 0.802                       & 4.617                        & 0.933                       & 0.715                       & \hline
	%     ViNet-A & \xmark & \textcolor{blue}{0.825} & \textcolor{red}{\underline{4.823}}  & \textcolor{blue}{0.934} & \textcolor{blue}{0.678} & \hline
	%     ViNet-E & \xmark & \textcolor{red}{\underline{0.828}}                       & \textcolor{blue}{4.816}                        & \textcolor{red}{\underline{0.936}}                       & \textcolor{red}{\underline{0.663}}                       & \hline
	%   \end{tabular}
	%   \label{table:2}
	% \end{table*}
	% \begin{table*}[!t]
	%     \centering

	%     % First row with a single centered sub-table
	%     \begin{minipage}{\textwidth}
	%       \caption{Comparison results on the AVAD, Coutrot1, Coutrot2, DIEM and ETMD test sets. The best scores are shown in red, and the second-best scores are in blue.}
	%         \centering
	%   \begin{tabular}{@{}|l|P{0.45cm}|llll|llll|@{}}
	%     \hline
	%     \multirow{2}{*}{METHOD} & Uses   & \multicolumn{4}{|c|}{Coutrot1} & \multicolumn{4}{|c|}{Coutrot2}  & \multicolumn{4}{|c|}{ETMD}  \\
	%     \cline{3-14}
	%     & Audio & CC$\uparrow$                      & NSS$\uparrow$                    & AUC-J$\uparrow$                   & SIM$\uparrow$
	%     & CC$\uparrow$                      & NSS$\uparrow$                    & AUC-J$\uparrow$                   & SIM$\uparrow$                     & CC$\uparrow$                      & NSS$\uparrow$                    & AUC-J$\uparrow$                   & SIM$\uparrow$                     &
	%     \hline
	%     ACLNet~\cite{dhf1k}    & \xmark & 0.425                  & 1.92                   & 0.85                    & 0.361                   & 0.448                   & 3.16                   & 0.926                   & 0.322                   & 0.477                   & 2.36                   & 0.915                   & 0.329                   &
	%     \hline
	%     TASED-Net~\cite{min2019tased} & \xmark & 0.479                  & 2.18                   & 0.867                   & 0.388                   & 0.437                   & 3.17                   & 0.921                   & 0.314                   & 0.509                   & 2.63                   & 0.916                   & 0.366                   &
	%     \hline
	%     STAViS~\cite{tsiami2020stavis}    & \cmark & 0.458                  & 1.99                   & 0.861                   & 0.384                   & 0.652                   & 4.19                   & 0.940                   & 0.447                   & 0.560                   & 2.84                   & 0.929                   & 0.412                   &
	%     \hline
	%     ViNet~\cite{jain2021vinet}     & \xmark & 0.551                  & 2.68                   & 0.886                   & 0.423                   & 0.724                   & 5.61                   & 0.95                    & 0.466                   & 0.569                   & 3.06                   & 0.928                   & 0.409                   &
	%     \hline
	%     TSFP-Net~\cite{tsfpnet}  & \cmark & 0.57 & 2.75 & 0.894 & 0.451 & 0.718                   & 5.30                   & 0.957                   & 0.516                   & 0.576                   & 3.09                   & 0.932                   & 0.433                   &
	%     \hline
	%     CASP-Net~\cite{xiong2023casp}  & \cmark & 0.561                  & 2.65                   & 0.889                   & 0.456                   & 0.788 & 6.34 & 0.963  & 0.585 & 0.620 & 3.34 & 0.940 & 0.478  &
	%     \hline
	%     \hline
	%     ViNet-S & \xmark & 0.574                       & 2.876                       & 0.898                        & 0.449                        & 0.754                        & \textcolor{red}{\underline{6.103}}                       & 0.958                        & 0.547                        & 0.599                        & 3.268                       & \textcolor{blue}{0.941}                        & \textcolor{blue}{0.458}                        &
	%     \hline
	%     ViNet-A & \xmark & \textcolor{blue}{0.600} & \textcolor{blue}{3.033} & \textcolor{blue}{0.900}  & \textcolor{blue}{0.459}  & \textcolor{red}{\underline{0.862}}  & \textcolor{blue}{6.8}   & \textcolor{blue}{0.961} & \textcolor{red}{\underline{0.638}}  & \textcolor{blue}{0.623}  & \textcolor{blue}{3.379} & \textcolor{blue}{0.941}   & \textcolor{blue}{0.458} &
	%     \hline
	%     ViNet-E & \xmark & \textcolor{red}{\underline{0.614}}                       & \textcolor{red}{\underline{3.085}}                       & \textcolor{red}{\underline{0.905}}                        & \textcolor{red}{\underline{0.465}}                        & \textcolor{blue}{0.854}                        & 6.762                       & \textcolor{red}{\underline{0.962}}                        & \textcolor{blue}{0.628}                        & \textcolor{red}{\underline{0.632}}                        & \textcolor{red}{\underline{3.437}}                       & \textcolor{red}{\underline{0.943}}                        & \textcolor{red}{\underline{0.468}}                        &
	%     \hline
	%   \end{tabular}
	%     \end{minipage}

	%     \vspace{0.5cm} % Space between the two rows of tables

	%     % Second row with two side-by-side sub-tables
	%     \begin{minipage}{0.3\textwidth}
	%     \caption{Sub-table 2}
	%         \centering
	%   \begin{tabular}{@{}|l|P{0.45cm}|llll|llll|@{}}
	%     \hline
	%     \multirow{2}{*}{METHOD} & Uses & \multicolumn{4}{|c|}{AVAD} & \multicolumn{4}{|c|}{DIEM} \\
	%     \cline{3-10}
	%               & Audio & CC$\uparrow$                      & NSS$\uparrow$                    & AUC-J$\uparrow$                   & SIM$\uparrow$                     & CC$\uparrow$    & NSS$\uparrow$   & AUC-J$\uparrow$ & SIM$\uparrow$   &
	%     \hline
	%     ACLNet~\cite{dhf1k}    & \xmark & 0.580                   & 3.17                   & 0.905                   & 0.446                   & 0.522 & 2.02  & 0.869 & 0.427 &
	%     \hline
	%     TASED-Net~\cite{min2019tased} & \xmark & 0.601                   & 3.16                   & 0.914                   & 0.439                   & 0.557 & 2.16  & 0.881 & 0.461 & \hline
	%     STAViS~\cite{tsiami2020stavis}    & \cmark & 0.604                   & 3.07                   & 0.915                   & 0.443                   & 0.579 & 2.26  & 0.883 & 0.482 & \hline
	%     ViNet~\cite{jain2021vinet}     & \xmark & 0.694 & 3.82 & 0.928                   & 0.504                   & 0.626 & 2.47  & 0.898 & 0.483 & \hline
	%     TSFP-Net~\cite{tsfpnet}  & \cmark & 0.688                   & 3.79                   & 0.932 & 0.530 & 0.651 & 2.62  & 0.906 & 0.527 & \hline
	%     CASP-Net~\cite{xiong2023casp}  & \cmark & 0.691                   & 3.81                   & 0.933                   & 0.528                   & 0.655 & 2.61  & 0.906 & 0.543 & \hline
	%     \hline
	%     ViNet-S & \xmark & \textcolor{blue}{0.712}                        & 4.090                       & \textcolor{blue}{0.935}                        & \textcolor{blue}{0.540}                        & 0.673      & 2.732      & \textcolor{blue}{0.908}      & 0.533      & \hline
	%     ViNet-A & \xmark & 0.709  & \textcolor{blue}{4.094} & 0.933  & 0.534  & \textcolor{blue}{0.675} & \textcolor{blue250}{2.742} & \textcolor{blue}{0.908} & \textcolor{blue}{0.547} & \hline
	%     ViNet-E & \xmark & \textcolor{red}{\underline{0.729}}                        & \textcolor{red}{\underline{4.167}}                       & \textcolor{red}{\underline{0.938}}                        & \textcolor{red}{\underline{0.547}}                        & \textcolor{red}{\underline{0.701}}      & \textcolor{red}{\underline{2.840}}      & \textcolor{red}{\underline{0.913}}      & \textcolor{red}{\underline{0.566}}      & \hline
	%   \end{tabular}

	%     \end{minipage}
	%     \hfill
	%     \hspace{2.5cm}
	%     \begin{minipage}{0.48\textwidth}
	%     \caption{Sub-table 3}
	%         \centering
	%   \begin{tabular}{@{}|l|P{0.45cm}|llll|@{}}
	%     \hline
	%     \multirow{2}{*}{METHOD} & Uses  & \multicolumn{4}{|c|}{MVVA}   \\
	%     \cline{3-6}
	%               & Audio & CC$\uparrow$                     & NSS$\uparrow$                     & AUC-J$\uparrow$                  & KLDIV$\downarrow$                  & \hline
	%     VASM~\cite{mvva}   & \cmark & 0.722                  & 3.976                   & 0.905                  & 0.823                  & \hline
	%     VAM-Net~\cite{vamnet}   & \cmark & 0.741                  & 4.002                   & 0.912                  & 0.783                  & \hline
	%     TASED-Net~\cite{min2019tased} & \xmark & 0.653                  & 3.319                   & 0.905                  & 0.970 & \hline
	%     STAViS~\cite{tsiami2020stavis}    & \cmark & 0.77                   & 3.060                   & 0.91                   & 0.80                   & \hline
	%     ViNet~\cite{jain2021vinet}     & \xmark & 0.81 & 4.470 & 0.93 & 0.75 & \hline
	%     \hline
	%     ViNet-S & \xmark & 0.802                       & 4.617                        & 0.933                       & 0.715                       & \hline
	%     ViNet-A & \xmark & \textcolor{blue}{0.825} & \textcolor{red}{\underline{4.823}}  & \textcolor{blue}{0.934} & \textcolor{blue}{0.678} & \hline
	%     ViNet-E & \xmark & \textcolor{red}{\underline{0.828}}                       & \textcolor{blue}{4.816}                        & \textcolor{red}{\underline{0.936}}                       & \textcolor{red}{\underline{0.663}}                       & \hline
	%   \end{tabular}

	%     \end{minipage}

	% \end{table*}

	% \begin{table*}[!t]
	%   \caption{Comparison results on the AVAD, Coutrot1, Coutrot2, DIEM and ETMD test sets. The best scores are shown in red, and the second-best scores are in blue.}

	%   \scriptsize
	%   \begin{tabular}{@{}|l|P{0.45cm}|P{0.4cm}P{0.4cm}P{0.85cm}P{0.4cm}|P{0.4cm}P{0.4cm}P{0.85cm}P{0.4cm}|P{0.4cm}P{0.4cm}P{0.85cm}P{0.4cm}|@{}}
	%     \hline
	%     \multirow{2}{*}{METHOD} & Uses   & \multicolumn{4}{|c|}{Coutrot1} & \multicolumn{4}{|c|}{Coutrot2}  & \multicolumn{4}{|c|}{ETMD}  \\
	%     \cline{3-14}
	%     & Audio & CC$\uparrow$                      & NSS$\uparrow$                    & AUC-J$\uparrow$                   & SIM$\uparrow$
	%     & CC$\uparrow$                      & NSS$\uparrow$                    & AUC-J$\uparrow$                   & SIM$\uparrow$                     & CC$\uparrow$                      & NSS$\uparrow$                    & AUC-J$\uparrow$                   & SIM$\uparrow$                     &
	%     \hline
	%     ACLNet~\cite{dhf1k}    & \xmark & 0.425                  & 1.92                   & 0.85                    & 0.361                   & 0.448                   & 3.16                   & 0.926                   & 0.322                   & 0.477                   & 2.36                   & 0.915                   & 0.329                   &
	%     \hline
	%     TASED-Net~\cite{min2019tased} & \xmark & 0.479                  & 2.18                   & 0.867                   & 0.388                   & 0.437                   & 3.17                   & 0.921                   & 0.314                   & 0.509                   & 2.63                   & 0.916                   & 0.366                   &
	%     \hline
	%     STAViS~\cite{tsiami2020stavis}    & \cmark & 0.458                  & 1.99                   & 0.861                   & 0.384                   & 0.652                   & 4.19                   & 0.940                   & 0.447                   & 0.560                   & 2.84                   & 0.929                   & 0.412                   &
	%     \hline
	%     ViNet~\cite{jain2021vinet}     & \xmark & 0.551                  & 2.68                   & 0.886                   & 0.423                   & 0.724                   & 5.61                   & 0.95                    & 0.466                   & 0.569                   & 3.06                   & 0.928                   & 0.409                   &
	%     \hline
	%     TSFP-Net~\cite{tsfpnet}  & \cmark & 0.57 & 2.75 & 0.894 & 0.451 & 0.718                   & 5.30                   & 0.957                   & 0.516                   & 0.576                   & 3.09                   & 0.932                   & 0.433                   &
	%     \hline
	%     CASP-Net~\cite{xiong2023casp}  & \cmark & 0.561                  & 2.65                   & 0.889                   & 0.456                   & 0.788 & 6.34 & 0.963  & 0.585 & 0.620 & 3.34 & 0.940 & 0.478  &
	%     \hline
	%     \hline
	%     ViNet-S & \xmark & 0.574                       & 2.876                       & 0.898                        & 0.449                        & 0.754                        & \textcolor{red}{\underline{6.103}}                       & 0.958                        & 0.547                        & 0.599                        & 3.268                       & \textcolor{blue}{0.941}                        & \textcolor{blue}{0.458}                        &
	%     \hline
	%     ViNet-A & \xmark & \textcolor{blue}{0.600} & \textcolor{blue}{3.033} & \textcolor{blue}{0.900}  & \textcolor{blue}{0.459}  & \textcolor{red}{\underline{0.862}}  & \textcolor{blue}{6.8}   & \textcolor{blue}{0.961} & \textcolor{red}{\underline{0.638}}  & \textcolor{blue}{0.623}  & \textcolor{blue}{3.379} & \textcolor{blue}{0.941}   & \textcolor{blue}{0.458} &
	%     \hline
	%     ViNet-E & \xmark & \textcolor{red}{\underline{0.614}}                       & \textcolor{red}{\underline{3.085}}                       & \textcolor{red}{\underline{0.905}}                        & \textcolor{red}{\underline{0.465}}                        & \textcolor{blue}{0.854}                        & 6.762                       & \textcolor{red}{\underline{0.962}}                        & \textcolor{blue}{0.628}                        & \textcolor{red}{\underline{0.632}}                        & \textcolor{red}{\underline{3.437}}                       & \textcolor{red}{\underline{0.943}}                        & \textcolor{red}{\underline{0.468}}                        &
	%     \hline
	%   \end{tabular}
	%   \vspace{10pt}

	%   \begin{tabular}{@{}|l|P{0.45cm}|P{0.4cm}P{0.4cm}P{0.85cm}P{0.4cm}|P{0.4cm}P{0.4cm}P{0.85cm}P{0.4cm}|@{}}
	%     \hline
	%     \multirow{2}{*}{METHOD} & Uses & \multicolumn{4}{|c|}{AVAD} & \multicolumn{4}{|c|}{DIEM} \\
	%     \cline{3-10}
	%               & Audio & CC$\uparrow$                      & NSS$\uparrow$                    & AUC-J$\uparrow$                   & SIM$\uparrow$                     & CC$\uparrow$    & NSS$\uparrow$   & AUC-J$\uparrow$ & SIM$\uparrow$   &
	%     \hline
	%     ACLNet~\cite{dhf1k}    & \xmark & 0.580                   & 3.17                   & 0.905                   & 0.446                   & 0.522 & 2.02  & 0.869 & 0.427 &
	%     \hline
	%     TASED-Net~\cite{min2019tased} & \xmark & 0.601                   & 3.16                   & 0.914                   & 0.439                   & 0.557 & 2.16  & 0.881 & 0.461 & \hline
	%     STAViS~\cite{tsiami2020stavis}    & \cmark & 0.604                   & 3.07                   & 0.915                   & 0.443                   & 0.579 & 2.26  & 0.883 & 0.482 & \hline
	%     ViNet~\cite{jain2021vinet}     & \xmark & 0.694 & 3.82 & 0.928                   & 0.504                   & 0.626 & 2.47  & 0.898 & 0.483 & \hline
	%     TSFP-Net~\cite{tsfpnet}  & \cmark & 0.688                   & 3.79                   & 0.932 & 0.530 & 0.651 & 2.62  & 0.906 & 0.527 & \hline
	%     CASP-Net~\cite{xiong2023casp}  & \cmark & 0.691                   & 3.81                   & 0.933                   & 0.528                   & 0.655 & 2.61  & 0.906 & 0.543 & \hline
	%     \hline
	%     ViNet-S & \xmark & \textcolor{blue}{0.712}                        & 4.090                       & \textcolor{blue}{0.935}                        & \textcolor{blue}{0.540}                        & 0.673      & 2.732      & \textcolor{blue}{0.908}      & 0.533      & \hline
	%     ViNet-A & \xmark & 0.709  & \textcolor{blue}{4.094} & 0.933  & 0.534  & \textcolor{blue}{0.675} & \textcolor{blue}{2.742} & \textcolor{blue}{0.908} & \textcolor{blue}{0.547} & \hline
	%     ViNet-E & \xmark & \textcolor{red}{\underline{0.729}}                        & \textcolor{red}{\underline{4.167}}                       & \textcolor{red}{\underline{0.938}}                        & \textcolor{red}{\underline{0.547}}                        & \textcolor{red}{\underline{0.701}}      & \textcolor{red}{\underline{2.840}}      & \textcolor{red}{\underline{0.913}}      & \textcolor{red}{\underline{0.566}}      & \hline
	%   \end{tabular}
	% % }

	%   \label{table:2}
	% \end{table*}

	\paragraph{Training}
	Following~\cite{jain2021vinet}, we input a clip of 32 consecutive frames to the
	ViNet-S model and use the ground truth saliency map of the 32\textsuperscript{nd}
	frame for supervision and prediction. For the ViNet-A model, the input consists
	of 32 frames sampled from a window of 64 consecutive frames by selecting every
	alternate frame. We use the ground truth saliency map of the 33\textsuperscript{rd}
	frame for supervision and prediction, akin to action label predictions in STAL
	models \cite{acarnet}. Both models are trained using the Adam optimizer with a
	learning rate of $10^{-4}$ and batch size of 8 for ViNet-S and 6 for ViNet-A.

	For evaluating our model on DHF1K, we use the validation set due to unavailable
	annotations for the test set, as in prior efforts~\cite{tinyhd, ma2022video}. We
	use the standard train and test sets provided for training on datasets Hollywood-2,
	UCF-Sports and DIEM. For Coutrot1, Coutrot2, AVAD, and ETMD, we perform 3-fold
	cross-validation and report average metrics across the splits. For MVVA, we follow~\cite{vamnet}
	and perform training on a random split.

	%We use the validation/test set of the datasets for early stopping.

	% \subsection{Evaluation Metrics}

	% We evaluate our method on five standard evaluation metrics \cite{metrics}. Specifically, we employ the following metrics: \textit{Correlation Coefficient} (CC) calculates the Pearson correlation between the ground truth and the predicted saliency maps. \textit{Similarity} (SIM) measures the overlap between the predicted and ground truth saliency maps by computing the histogram intersection. \textit{Area Under the Receiver Operating Characteristic Curve} (AUC-J), treats the saliency map as a binary classifier and evaluates its performance across different threshold values. A ROC curve is generated by plotting the true positive rate against the false positive rate at varying thresholds. \textit{Normalized Scanpath Saliency} (NSS) assesses the average normalized saliency at fixated locations. \textit{Kullback-Leibler Divergence} (KLDiv) Loss measures the difference between the predicted and ground truth saliency distributions using an information-theoretic approach.

	\paragraph{Evaluation Metrics}
	We evaluate our method on five standard evaluation metrics whose details can be
	found in \cite{metrics}: AUC-Judd (AUC-J), Similarity Metric (SIM),
	Correlation Coefficient(CC), Normalized Scanpath Saliency(NSS) and Kullback-Leibler
	Divergence(KLDiv). Except for KLDiv, higher metric values indicate better
	model performance.%Except for KLDiv, the larger the metric value, the better the model's performance.

	%------------------------------------------------------------------------------------------
	% \subsection{Loss Function}
	\paragraph{Loss Function}
	We utilize a combination of the above evaluation metrics, a standard technique
	in saliency tasks \cite{metrics}.
	% After experimenting with various combinations, we found that for most datasets, the best results were achieved using the loss function:  \(Loss = KLDiv(P,Q) - CC(P,Q)\), where $P$ \& $Q$ are the predicted saliency map and ground truth respectively.
	Through experimentation with different combinations, we found that the optimal
	results for most datasets were achieved with the loss function:
	$Loss = KLDiv(P, Q) - CC(P, Q)$, where $P$ \& $Q$ represent the predicted saliency
	map and ground truth, respectively.

	% \begin{equation}
	%   Loss = KLDiv(P, Q) - CC(P, Q)
	%   \label{eq:loss_function}
	% \end{equation}

	% % \[Loss = KLDiv(P, Q) - CC(P, Q)\]
	% \begin{equation}
	%   KLDiv(P,Q)=\sum\limits_i P_i\log(\epsilon+\frac{Q_i}{P_i+\epsilon})
	%   \label{eq:kldiv_loss}
	% \end{equation}

	% \begin{equation}
	%   CC(P,Q)=\frac{\sigma(P,Q)}{\sigma(P,P) \times \sigma(Q,Q)}
	%   \label{eq:cc_loss}
	% \end{equation}

	% where $P$ \& $Q$ are the predicted saliency map and ground truth respectively, $\epsilon$ is a regularization term and \( \sigma(P,Q)\) represents covariance between \(P\) and \(Q\)
	%----------------------------------------------------------------------------

	%-------------------------------------------------------
	\section{Results and Discussions}
	We evaluate the proposed models by comparing them against thirteen different
	methods from previous research. These include four 3D convolution-based approaches:
	ViNet~\cite{jain2021vinet}, TASED-Net~\cite{min2019tased}, STAVIS~\cite{tsiami2020stavis},
	and TSFP-Net~\cite{tsfpnet}; two methods utilizing recurrent networks: ACLNet~\cite{dhf1k}
	and UNISAL~\cite{droste2020unified}; four models employing transformers: STSANet~\cite{stsanet},
	THTD-Net~\cite{thtdnet}, CASP-Net~\cite{xiong2023casp}, TMFI-Net~\cite{tmfinet};
	one diffusion-based model: DiffSal~\cite{diffsal} and a couple of multi-branch
	network methods: VAM-Net~\cite{vamnet} and VASM~\cite{mvva}. Six of these
	models (STAVIS, CASP-Net, TSFP-Net, VAM-Net, DiffSal and VASM) additionally
	employ audio information in their approach. We report results directly from the
	corresponding papers when available. If the code is publicly available and
	executable, we compute their results on other datasets.

	% \footnotetext{The best scores are shown in red, and the second-best scores are in blue. Table best viewed in colour.}

	% \begin{table}[t]
	%   \caption{Comparison results on MVVA dataset.}
	%   \centering
	%   \centering
	% \resizebox{\columnwidth}{!}{
	%   \begin{tabular}{@{}|l|P{0.85cm}|llll|@{}}
	%     \hline
	%     \multirow{2}{*}{METHOD} & Uses  & \multicolumn{4}{|c|}{MVVA}   \\
	%     \cline{3-6}
	%               & Audio & CC$\uparrow$                     & NSS$\uparrow$                     & AUC-J$\uparrow$                  & KLDIV$\downarrow$                  & \hline
	%     VASM~\cite{mvva}   & \cmark & 0.722                  & 3.976                   & 0.905                  & 0.823                  & \hline
	%     VAM-Net~\cite{vamnet}   & \cmark & 0.741                  & 4.002                   & 0.912                  & 0.783                  & \hline
	%     TASED-Net~\cite{min2019tased} & \xmark & 0.653                  & 3.319                   & 0.905                  & 0.970 & \hline
	%     STAViS~\cite{tsiami2020stavis}    & \cmark & 0.77                   & 3.060                   & 0.91                   & 0.80                   & \hline
	%     ViNet~\cite{jain2021vinet}     & \xmark & 0.81 & 4.470 & 0.93 & 0.75 & \hline
	%     \hline
	%     ViNet-S & \xmark & 0.802                       & 4.617                        & 0.933                       & 0.715                       & \hline
	%     ViNet-A & \xmark & \textcolor{blue}{0.825} & \textcolor{red}{\underline{4.823}}  & \textcolor{blue}{0.934} & \textcolor{blue}{0.678} & \hline
	%     ViNet-E & \xmark & \textcolor{red}{\underline{0.828}}                       & \textcolor{blue}{4.816}                        & \textcolor{red}{\underline{0.936}}                       & \textcolor{red}{\underline{0.663}}                       & \hline
	%   \end{tabular}
	% }
	%   \label{table:3}
	% \end{table}

	% \begin{table}[t]
	%   \caption{Quantitative comparison of model sizes \& parameters}
	%   \centering
	%   \begin{tabular}{@{}|l|P{1.5cm}|P{1.5cm}|@{}} %P{0.85cm}
	%   \hline
	%   Model & Size (MB) & \# Params (Million) & \hline
	%   ACLNet~\cite{dhf1k} & 250 & 65.54 & \hline
	%   TASED-Net~\cite{min2019tased} & 82 & 21.5 & \hline
	%   STAViS~\cite{tsiami2020stavis} & 79.19 & 20.76 & \hline
	%   UNISAL~\cite{droste2020unified} & 15.5 & 4.06 & \hline
	%   ViNet~\cite{jain2021vinet} & 124  & 32.5 & \hline
	%   TSFP-Net~\cite{tsfpnet} & 58.4 & 15.3 & \hline
	%   STSA-Net~\cite{stsanet} & 643 & 168.56 & \hline
	%   TMFI-Net~\cite{tmfinet} & 234 & 61.34 & \hline
	%   THTD-Net~\cite{thtdnet} & 220 & 57.67 & \hline
	%   CASP-Net~\cite{xiong2023casp} & 196.91 & 51.62 & \hline
	%   \hline
	%   ViNet-S & 36.24 & 9.5 & \hline
	%   ViNet-A & 147.6 & 38.69 & \hline
	%   ViNet-E & 183.84 & 48.19 & \hline
	%   \end{tabular}
	%   \label{table:4}
	% \end{table}

	\paragraph{Visual Only Datasets}
	Table \ref{table:1a} presents results on the visual-only datasets. The model
	sizes and the number of parameters of the studied models are presented in
	Table \ref{table:sizes}. We observe that ViNet-E achieves the best performance
	on UCF-Sports and Hollywood2 datasets~\cite{hollywood-ucf}, while achieving
	competent results on the DHF1K dataset~\cite{dhf1k}. Interestingly, ViNet-A
	also outperforms the previous methods on the UCF-Sports and Hollywood2
	datasets. Its strong performance on these two human-centric datasets clearly demonstrates
	the advantages of using an STAL backbone over an action classification backbone.
	Notably, all three proposed models, including ViNet-S, consistently surpass the
	base ViNet model. %Another notable observation is that all three proposed models, including ViNet-S, consistently outperform the base ViNet model.

	The ViNet-S model recovers most of the underlying performance in all the cases
	while using only a tiny fraction of the parameters. For instance, on the
	Hollywood2 dataset, the largest SP dataset with 884 videos in the test set,
	ViNet-S recovers over 98.5\% performance on the CC metric compared to the transformer-based
	SOTA TMFI-Net, while bringing over six-fold reduction in terms of number of
	parameters (Table \ref{table:1}). Interestingly, ViNet-S outperforms TMFI-Net
	on the AUC-J metric. UNISAL is the only model lighter than the ViNet-S model.
	However, it consistently underperforms in comparison, possibly due to its
	recurrent architecture.

	\paragraph{Audio Visual Datasets}
	Table \ref{table:2} and Table \ref{table:3} present results on the audio-visual
	datasets. The proposed ViNet-S, ViNet-A, and ViNet-E consistently outperform
	prior models across all six datasets, consistently ranking among the top two models.
	The videos in the Coutrot2 and MVVA datasets emphasize multi-person
	interactions. Notably, ViNet-A achieves significant improvements on both
	datasets, maintaining a consistent performance trend with the other human-centric
	datasets. On MVVA (the largest audio-visual saliency dataset), while only using
	the visual modality ViNet-A brings over 20\% gains on NSS metric over the
	complex multi-branch VAM-Net, which uses an explicit combination of motion,
	texture, face and audio features.

	On four out of the six audio-visual datasets, i.e. DIEM, AVAD, Coutrot1, and
	MVVA, the smaller ViNet-S surpasses all the previous methods. The consistent
	performance improvements of the ViNet-E model validate the effectiveness of
	the proposed ensemble strategy, establishing a new SOTA in most datasets.
	Another notable observation is that incorporating audio information does not
	appear to provide a significant advantage for the task of SP. Consistent with prior
	studies~\cite{agrawal2022does,jain2021vinet,tsfpnet}, we found that several
	audio-visual models~\cite{diffsal,tsiami2020stavis}, in reality, are not
	exploiting the audio information. At inference, the models appear agnostic to
	the audio information, i.e., the results remain the same irrespective of sending
	the random audio or zero audio. This represents a significant scientific flaw
	that requires further investigation in future research, and comparisons with their
	results should be approached with caution. Although ViNet-E outperforms their
	audio-visual version on several datasets, we limit our comparisons only to their
	visual only model.

	% \begin{figure*}[t!]
	%   \centering
	%   \includegraphics[width=0.9\columnwidth]{Images/qual_v2.drawio.png}
	%   \caption{Qualitative results: Comparing Ground Truth Saliency Maps with the predicted output of our model and previous SOTA models ViNet~\cite{jain2021vinet} and STAViS~\cite{tsiami2020stavis} on four different datasets - AVAD~\cite{avad}, Coutrot1~\cite{coutrot1conf, coutrot1journ}, MVVA~\cite{mvva}, and Coutrot2~\cite{coutrot2}}
	%   \label{fig:qual}

	% \end{figure*}

	% \begin{figure}[t!]
	%   \centering
	%   \includegraphics[width=\columnwidth]{Images/qual_v2.drawio.png}
	%   \caption{Qualitative results: Comparing Ground Truth Saliency Maps with the predicted output of our model and previous SOTA model ViNet and STAViS on four different datasets - AVAD, Coutrot1, MVVA, and Coutrot2.}
	%   \label{fig:qual}
	% \end{figure}

	\begin{figure}[t!]
		\centering
		\includegraphics[width=\columnwidth]{Images/icassp_qual.drawio.png}
		\caption{Qualitative results: Comparing Ground Truth with the predicted saliency
		maps of our models and STSANet on three different datasets - DHF1K, UCF-Sports
		and DIEM.}
		\label{fig:qual}
	\end{figure}

	\paragraph{Qualitative comparisons}
	Figure \ref{fig:qual} shows the qualitative performance of our ViNet-S, ViNet-A
	and ViNet-E models on video sequences from three different datasets: DHF1K, UCF-Sports
	and DIEM. We observe that STAL features efficiently capture the interaction
	between an actor/object with the context (surrounding) as evident in the
	strong performance of our model. ViNet-E is consistently closer to the ground truth
	in different settings than all other models, including STSANet. %ViNet-E incorporates both global and localized action features to better model the saliency and is consistently closer to the ground truth in different scenarios.
	%We observe that ViNet-E is consistently closer to the ground truth in different settings than all other models, including STSANet. % We obseve that ViNet-A is more closer to the ground truth in multi-person scenario

	% shows the qualitative performance of our ViNet-A model on video sequences from four different datasets: AVAD, Coutrot1, MVVA, and Coutrot2. We observe that the STAL features efficiently capture the interaction between an actor/object with the context (surroundings), as evident in the strong performance of our model. It also retains performance in videos where humans are absent. This observation underscores our model's capacity to comprehensively grasp the entirety of a scene, including the intricate interplay among various objects and participants. This stands in contrast to the limitations exhibited by single-label backbone models (as observed in samples from Coutrot2 and AVAD datasets).

	\paragraph{Computational load}
	Table \ref{table:sizes} compares the different models in terms of models size and
	number of parameters. The proposed decoder significantly reduces the number of
	parameters in ViNet-S compared to the original ViNet model. Aside from UNISAL,
	ViNet-S is the most efficient in terms of model size and parameters among the
	compared models. % We observe that the proposed decoder brings significant parameter reduction in ViNet-S, compared to the original ViNet model. Except for UNISAL, ViNet-S fares best among the other models in terms of both the model size and number of parameters.
	We observe that switching from the S3D backbone in ViNet-S to the SlowFast
	backbone in ViNet-A leads to significant parameter gains. Notably, ViNet-A's
	decoder contains only 1.6 million parameters, while the SlowFast backbone accounts
	for the remaining 37 million. Lastly, the ViNet-E model remains smaller than state-of-the-art
	transformer-based models (e.g., TMFI-Net and THTD-Net) in both model size and parameter
	count.
	% It is worth noting that the decoder of ViNet-A comprises only 1.6 million parameters, while the remaining 37 million parameters are contributed by the SlowFast backbone. Finally, the ViNet-E model remains below the SOTA transformer-based models (e.g. TFMI-Net, and THTD-Net), both in terms of model size and parameters.

	The non-autoregressive design of the proposed ViNet models enables parallel processing,
	providing a significant advantage over autoregressive models such as UNISAL, which
	rely on frame-level recurrence. On an Nvidia RTX 4090 GPU, ViNet-S, ViNet-A, and
	ViNet-E models achieve runtimes of approximately 200fps, 120fps, and 90fps,
	respectively, in a real-time processing setup (with a batch size of one). With
	a batch size of eight, ViNet-S reaches an impressive 1070fps. %the ViNet-S model achieves an impressive runtime performance of 1070fps.

	\section{Conclusion}
	% This work presents two efficient models, ViNet-S and ViNet-A, based on simple architecture design choices. ViNet-S is lightweight yet retains or improves upon most of the convolutional methods. Considering the complementary nature of information inherent in action classification and action detection, we propose ViNet-A, achieving SOTA results on visual as well as audio-visual datasets even without utilizing audio cues. Interestingly, an ensemble of straightforward pixel-wise mean of predictions from the two proposed models further pulls away from the competition. This opens up a new avenue for integrating the global and localized features from action classification and detection. Furthermore, in the current work we only tackle model optimization from the perspective of model architecture, future works should also address this from the lens of model compression and knowledge distillation.

	This work introduces two efficient models, ViNet-S and ViNet-A, characterized by
	their simple architectural design choices. %built on simple architectural design choices.
	ViNet-S is lightweight yet matches or surpasses most convolutional methods,
	while ViNet-A, which utilizes localized action features, consistently performs
	well on human-centric datasets with multiple subjects. ViNet-E, the ensemble
	model, leverages the complementary nature of action classification and detection
	to achieve state-of-the-art results on both visual and audio-visual datasets,
	even without audio cues. Using pixel-wise averaging enhances performance,
	suggesting new avenues for integrating global and localized action features.
	% While this study focuses on model optimization through architectural enhancements, future efforts should explore model compression and knowledge distillation.
	While this study emphasizes model optimization primarily through architectural
	refinements, future work would aim to investigate and integrate ideas from
	model compression and knowledge distillation methodologies.

	% ViNet-E leverages the complementary nature of action classification and detection to achieve state-of-the-art results on both visual and audio-visual datasets, even without audio cues. An ensemble of the two models using pixel-wise averaging further enhances performance, suggesting new avenues for integrating global and localized action features. %opening new possibilities for integrating global and localized action features.

	%%%%%%%%% REFERENCES
	\bibliographystyle{IEEEtran}
	\bibliography{reference}
\end{document}

% \section{Ease of Use}

% \subsection{Maintaining the Integrity of the Specifications}

% The IEEEtran class file is used to format your paper and style the text. All margins,
% column widths, line spaces, and text fonts are prescribed; please do not
% alter them. You may note peculiarities. For example, the head margin
% measures proportionately more than is customary. This measurement
% and others are deliberate, using specifications that anticipate your paper
% as one part of the entire proceedings, and not as an independent document.
% Please do not revise any of the current designations.

% \section{Prepare Your Paper Before Styling}
% Before you begin to format your paper, first write and save the content as a
% separate text file. Complete all content and organizational editing before
% formatting. Please note sections \ref{AA}--\ref{SCM} below for more information on
% proofreading, spelling and grammar.

% Keep your text and graphic files separate until after the text has been
% formatted and styled. Do not number text heads---{\LaTeX} will do that
% for you.

% \subsection{Abbreviations and Acronyms}\label{AA}
% Define abbreviations and acronyms the first time they are used in the text,
% even after they have been defined in the abstract. Abbreviations such as
% IEEE, SI, MKS, CGS, ac, dc, and rms do not have to be defined. Do not use
% abbreviations in the title or heads unless they are unavoidable.

% \subsection{Units}
% \begin{itemize}
% \item Use either SI (MKS) or CGS as primary units. (SI units are encouraged.) English units may be used as secondary units (in parentheses). An exception would be the use of English units as identifiers in trade, such as ``3.5-inch disk drive''.
% \item Avoid combining SI and CGS units, such as current in amperes and magnetic field in oersteds. This often leads to confusion because equations do not balance dimensionally. If you must use mixed units, clearly state the units for each quantity that you use in an equation.
% \item Do not mix complete spellings and abbreviations of units: ``Wb/m\textsuperscript{2}'' or ``webers per square meter'', not ``webers/m\textsuperscript{2}''. Spell out units when they appear in text: ``. . . a few henries'', not ``. . . a few H''.
% \item Use a zero before decimal points: ``0.25'', not ``.25''. Use ``cm\textsuperscript{3}'', not ``cc''.)
% \end{itemize}

% \subsection{Equations}
% Number equations consecutively. To make your
% equations more compact, you may use the solidus (~/~), the exp function, or
% appropriate exponents. Italicize Roman symbols for quantities and variables,
% but not Greek symbols. Use a long dash rather than a hyphen for a minus
% sign. Punctuate equations with commas or periods when they are part of a
% sentence, as in:
% \begin{equation}
% a+b=\gamma\label{eq}
% \end{equation}

% Be sure that the
% symbols in your equation have been defined before or immediately following
% the equation. Use ``\eqref{eq}'', not ``Eq.~\eqref{eq}'' or ``equation \eqref{eq}'', except at
% the beginning of a sentence: ``Equation \eqref{eq} is . . .''

% \subsection{\LaTeX-Specific Advice}

% Please use ``soft'' (e.g., \verb|\eqref{Eq}|) cross references instead
% of ``hard'' references (e.g., \verb|(1)|). That will make it possible
% to combine sections, add equations, or change the order of figures or
% citations without having to go through the file line by line.

% Please don't use the \verb|{eqnarray}| equation environment. Use
% \verb|{align}| or \verb|{IEEEeqnarray}| instead. The \verb|{eqnarray}|
% environment leaves unsightly spaces around relation symbols.

% Please note that the \verb|{subequations}| environment in {\LaTeX}
% will increment the main equation counter even when there are no
% equation numbers displayed. If you forget that, you might write an
% article in which the equation numbers skip from (17) to (20), causing
% the copy editors to wonder if you've discovered a new method of
% counting.

% {\BibTeX} does not work by magic. It doesn't get the bibliographic
% data from thin air but from .bib files. If you use {\BibTeX} to produce a
% bibliography you must send the .bib files.

% {\LaTeX} can't read your mind. If you assign the same label to a
% subsubsection and a table, you might find that Table I has been cross
% referenced as Table IV-B3.

% {\LaTeX} does not have precognitive abilities. If you put a
% \verb|\label| command before the command that updates the counter it's
% supposed to be using, the label will pick up the last counter to be
% cross referenced instead. In particular, a \verb|\label| command
% should not go before the caption of a figure or a table.

% Do not use \verb|\nonumber| inside the \verb|{array}| environment. It
% will not stop equation numbers inside \verb|{array}| (there won't be
% any anyway) and it might stop a wanted equation number in the
% surrounding equation.

% \subsection{Some Common Mistakes}\label{SCM}
% \begin{itemize}
% \item The word ``data'' is plural, not singular.
% \item The subscript for the permeability of vacuum $\mu_{0}$, and other common scientific constants, is zero with subscript formatting, not a lowercase letter ``o''.
% \item In American English, commas, semicolons, periods, question and exclamation marks are located within quotation marks only when a complete thought or name is cited, such as a title or full quotation. When quotation marks are used, instead of a bold or italic typeface, to highlight a word or phrase, punctuation should appear outside of the quotation marks. A parenthetical phrase or statement at the end of a sentence is punctuated outside of the closing parenthesis (like this). (A parenthetical sentence is punctuated within the parentheses.)
% \item A graph within a graph is an ``inset'', not an ``insert''. The word alternatively is preferred to the word ``alternately'' (unless you really mean something that alternates).
% \item Do not use the word ``essentially'' to mean ``approximately'' or ``effectively''.
% \item In your paper title, if the words ``that uses'' can accurately replace the word ``using'', capitalize the ``u''; if not, keep using lower-cased.
% \item Be aware of the different meanings of the homophones ``affect'' and ``effect'', ``complement'' and ``compliment'', ``discreet'' and ``discrete'', ``principal'' and ``principle''.
% \item Do not confuse ``imply'' and ``infer''.
% \item The prefix ``non'' is not a word; it should be joined to the word it modifies, usually without a hyphen.
% \item There is no period after the ``et'' in the Latin abbreviation ``et al.''.
% \item The abbreviation ``i.e.'' means ``that is'', and the abbreviation ``e.g.'' means ``for example''.
% \end{itemize}
% An excellent style manual for science writers is \cite{b7}.

% \subsection{Authors and Affiliations}
% \textbf{The class file is designed for, but not limited to, six authors.} A
% minimum of one author is required for all conference articles. Author names
% should be listed starting from left to right and then moving down to the
% next line. This is the author sequence that will be used in future citations
% and by indexing services. Names should not be listed in columns nor group by
% affiliation. Please keep your affiliations as succinct as possible (for
% example, do not differentiate among departments of the same organization).

% \subsection{Identify the Headings}
% Headings, or heads, are organizational devices that guide the reader through
% your paper. There are two types: component heads and text heads.

% Component heads identify the different components of your paper and are not
% topically subordinate to each other. Examples include Acknowledgments and
% References and, for these, the correct style to use is ``Heading 5''. Use
% ``figure caption'' for your Figure captions, and ``table head'' for your
% table title. Run-in heads, such as ``Abstract'', will require you to apply a
% style (in this case, italic) in addition to the style provided by the drop
% down menu to differentiate the head from the text.

% Text heads organize the topics on a relational, hierarchical basis. For
% example, the paper title is the primary text head because all subsequent
% material relates and elaborates on this one topic. If there are two or more
% sub-topics, the next level head (uppercase Roman numerals) should be used
% and, conversely, if there are not at least two sub-topics, then no subheads
% should be introduced.

% \subsection{Figures and Tables}
% \paragraph{Positioning Figures and Tables} Place figures and tables at the top and
% bottom of columns. Avoid placing them in the middle of columns. Large
% figures and tables may span across both columns. Figure captions should be
% below the figures; table heads should appear above the tables. Insert
% figures and tables after they are cited in the text. Use the abbreviation
% ``Fig.~\ref{fig}'', even at the beginning of a sentence.

% \begin{table}[htbp]
% \caption{Table Type Styles}
% \begin{center}
% \begin{tabular}{|c|c|c|c|}
% \hline
% \textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
% \cline{2-4}
% \textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
% \hline
% copy& More table copy$^{\mathrm{a}}$& &  \\
% \hline
% \multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
% \end{tabular}
% \label{tab1}
% \end{center}
% \end{table}

% \begin{figure}[htbp]
% \centerline{\includegraphics{fig1.png}}
% \caption{Example of a figure caption.}
% \label{fig}
% \end{figure}

% Figure Labels: Use 8 point Times New Roman for Figure labels. Use words
% rather than symbols or abbreviations when writing Figure axis labels to
% avoid confusing the reader. As an example, write the quantity
% ``Magnetization'', or ``Magnetization, M'', not just ``M''. If including
% units in the label, present them within parentheses. Do not label axes only
% with units. In the example, write ``Magnetization (A/m)'' or ``Magnetization
% \{A[m(1)]\}'', not just ``A/m''. Do not label axes with a ratio of
% quantities and units. For example, write ``Temperature (K)'', not
% ``Temperature/K''.

% \section*{Acknowledgment}

% The preferred spelling of the word ``acknowledgment'' in America is without
% an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B.
% G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor
% acknowledgments in the unnumbered footnote on the first page.

% \section*{References}

% Please number citations consecutively within brackets \cite{b1}. The
% sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference
% number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at
% the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''

% Number footnotes separately in superscripts. Place the actual footnote at
% the bottom of the column in which it was cited. Do not put footnotes in the
% abstract or reference list. Use letters for table footnotes.

% Unless there are six authors or more give all authors' names; do not use
% ``et al.''. Papers that have not been published, even if they have been
% submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers
% that have been accepted for publication should be cited as ``in press'' \cite{b5}.
% Capitalize only the first word in a paper title, except for proper nouns and
% element symbols.

% For papers published in translation journals, please give the English
% citation first, followed by the original foreign-language citation \cite{b6}.

% \begin{thebibliography}{00}
% \bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
% \bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
% \bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
% \bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
% \bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
% \bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
% \bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
% \end{thebibliography}
% \vspace{12pt}
% \color{red}
% IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.