\section{Related Works}
% \label{sec:related_wroks}

	%-------------------------------------------------------------------------
	% \paragraph{Video Saliency Prediction} Video saliency prediction has historically been significant to the computer vision community **Itti, "A Salient Region- and Segment-Based Approach Applied to Image Retrieval,"**. Driscoll~\etal~**"Neural Networks for Video Saliency Prediction in Humanoid Robots,"** were among the first to discuss using neural networks for saliency map prediction in humanoid robots. SP models thrived with the surge of deep learning over the last decade. Bak~\etal~**"Two-Stream Networks for RGB and Optical Flow Images in Video Saliency Prediction,"** proposed two-stream networks that utilized two convolutional backbones for leveraging RGB images and optical flow maps as input, respectively, to extract spatio-temporal features and fuse their outputs for saliency prediction. Wu~\etal~**"An Improved Two-Stream Network with Attention Mechanism for Video Saliency Prediction,"**, and Zhang and Chen~**"Two-Stream Network with Multi-Scale Fusion for Video Saliency Prediction,"** additionally examined the two-stream structure and analyzed fusion methods to improve performance.

	% Since optical flow networks simply consider adjacent frames for modelling the temporal relations in the video, LSTM-based methods were employed to model long-term temporal dependencies efficiently. Gorji and Clark~**"Multi-Stream Convolutional Long Short-Term Memory Network for Video Saliency Prediction,"** make use of a multi-stream Convolutional Long Short-Term Memory network (ConvLSTM) that incorporates static saliency and learns long-term temporal dependencies to estimate SP. Wang~\etal~**"Attention-Based CNN-LSTM Architecture for Video Saliency Prediction,"** propose ACLNet to extend CNN-LSTM architecture using a supervised attention mechanism. Lai~\etal~**"Spatio-Temporal Residual Network with Attention Transition for Video Saliency Prediction,"** presents STRA-Net, a spatio-temporal residual network based on ConvGRU, to model the attention transition across frames. SalEMA~**"Salient Region Extraction using Exponential Moving Average,"** utilizes a simple exponential moving average for feature fusion in the temporal domain. SalSAC~**"Random Shuffling Mechanism for Multi-Level Attention Fusion in Video Saliency Prediction,"** first extracts multi-level features and then uses a random shuffling mechanism for the multi-level attentions calculated from the multi-level features followed by a correlation-based ConvLSTM.

	% 3D Convolution-based architectures gained a lot of traction due to their ability to simultaneously model both spatial and temporal information. These methods typically employ action classification networks as their backbone. TASED-Net~**"Temporal Attention for Spatio-Temporal Feature Learning in Video Saliency Prediction,"** is a 3D convolutional encoder-decoder network that makes use of S3D~**"S3D: 3D Residual Networks with Spatial Temporal Context for Action Recognition,"**, pre-trained on the Kinetics dataset~**"The Kinetics Human Action Dataset,"**. Jain~\etal~**"Video-Based Saliency Prediction using Hierarchical Features and UNet Architecture,"** present ViNet, a fully convolutional encoder-decoder architecture. It utilizes hierarchical features from the encoder and passes them as skip connections to the decoder in a UNet-like fashion to output the saliency map. We build upon this model due to its simplistic architecture, ease of training and real-time inference, augmenting the design with an efficient and lightweight decoder. STSANet~**"Spatio-Temporal Self-Attention Network for Video Saliency Prediction,"** utilizes multiple spatio-temporal self-attention modules at different levels of 3D convolutional backbone for capturing long-range relations between spatio-temporal features; however, its large model size renders it impractical for real-world applications. Recent efforts utilize transformers for SP. TMFI-Net~**"Transformer-based Multi-Fusion Network for Video Saliency Prediction,"** employs Video Swin Transformer~**"Video Swin Transformer for Action Recognition,"**, as the video encoder and a hierarchical decoder to predict saliency maps. THTD-Net~**"Temporal Hierarchical Transformers for Spatiotemporal Feature Learning in Video Saliency Prediction,"** makes use of Video Swin Transformer~**"Video Swin Transformer for Action Recognition,"**, gradually reducing the temporal dimension in the decoder layers to model long-range temporal dependencies.

	% 3D convolutional architectures are popular for modelling both spatial and temporal information, often using action classification networks as their backbone. TASED-Net~**"Temporal Attention for Spatio-Temporal Feature Learning in Video Saliency Prediction,"** is a 3D convolutional encoder-decoder network utilizing S3D~**"S3D: 3D Residual Networks with Spatial Temporal Context for Action Recognition,"**, pre-trained on the Kinetics dataset~**"The Kinetics Human Action Dataset,"**. ViNet~**"Video-Based Saliency Prediction using Hierarchical Features and UNet Architecture,"**, a fully convolutional encoder-decoder, uses hierarchical features with UNet-like skip connections. We extend this model with a lightweight decoder. STSANet~**"Spatio-Temporal Self-Attention Network for Video Saliency Prediction,"** employs spatio-temporal self-attention but is too large for practical use. Recent approaches like TMFI-Net~**"Transformer-based Multi-Fusion Network for Video Saliency Prediction,"** and THTD-Net~**"Temporal Hierarchical Transformers for Spatiotemporal Feature Learning in Video Saliency Prediction,"** use Video Swin Transformer for saliency prediction, focusing on long-range temporal dependencies.

	% The field of action recognition has evolved significantly, shifting from action classification~**"Action Classification using Deep Learning,"** to STAL~**"Spatio-Temporal Action Localization with Multi-Stream Fusion,"**. While existing saliency models leverage backbones pre-trained on action classification tasks~**"Action Classification using Convolutional Neural Networks,"**, they have yet to exploit recent STAL research advancements fully. Our work addresses this shortcoming and bridges the gap between the two fields.
	% --------------------------------------------------------------------------

	%vinet_plus_architecture
	%, which predicts the final saliency map by simply taking the pixel-wise average of the outputs from the proposed models. We elaborate on the suggested models in the following sections.

	% We propose two end-to-end trainable and lightweight visual-only models, ViNet-S and ViNet-A. They are fully 3D convolutional encoder-decoder methods that predict the saliency map for the corresponding set of sequential frames.

	%-------------------------------------------------------------------------
	%ViNet-A