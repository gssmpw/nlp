%

\documentclass{article}

\PassOptionsToPackage{table}{xcolor}
\usepackage{xcolor}
\definecolor{lightblue}{RGB}{173, 216, 230} %

%
\usepackage{microtype}
\usepackage{graphicx}
%
\usepackage{subcaption}
\usepackage{booktabs} %

%
%
%
%
\usepackage{hyperref}


%
\newcommand{\theHalgorithm}{\arabic{algorithm}}

%
%

%
\usepackage[accepted]{icml2025}

%
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{algpseudocode}

%
%

%
%
%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

%
%
%
\usepackage[textsize=tiny]{todonotes}

\input{defns}
\newcommand{\epsv}{\boldsymbol{\eps}}

\newcommand{\redt}[1]{\textcolor{red}{#1}}
\newcommand{\bluet}[1]{\textcolor{blue}{#1}}
\newcommand{\yesmark}{\redt{Y}}
\newcommand{\nomark}{\bluet{N}}

\newcommand{\sjsd}{\text{D}_{\text{JSD}}}
\newcommand{\kl}{\text{D}_{\text{KL}}}

\newcommand{\softplus}{\mathsf{sp}}
\newcommand{\logsumexp}{\mathsf{LogSumExp}}

\newcommand{\xtv}{\tilde{\xv}}
\newcommand{\dpm}{\mathsf{DPM}}
\DeclareMathOperator{\sech}{sech}
\usepackage{nicefrac}

\usepackage{makecell}
\usepackage{multirow}
\usepackage{arydshln}
\usepackage{soul}

\usepackage{scrwfile}
\TOCclone[\appendixname]{toc}{atoc}
\newcommand\StartAppendixEntries{}
\AfterTOCHead[toc]{%
  \renewcommand\StartAppendixEntries{\value{tocdepth}=-10000\relax}%
}
\AfterTOCHead[atoc]{%
  \edef\maintocdepth{\the\value{tocdepth}}%
  \value{tocdepth}=-10000\relax%
  \renewcommand\StartAppendixEntries{\value{tocdepth}=\maintocdepth\relax}%
}

%

%
%
\icmltitlerunning{Score-of-Mixture Training: Training One-Step Generative Models Made Simple}

\begin{document}
%

\twocolumn[
\icmltitle{
Score-of-Mixture Training: One-Step Generative Model Training Made Simple\\ via Score Estimation of Mixture Distributions
%
%
%
}
%

%
%
%
%

%
%
%
%

%
%
%
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Tejas Jayashankar$^*$}{mit}
\icmlauthor{J. Jon Ryu$^*$}{mit}
\icmlauthor{Gregory Wornell}{mit}
%
%
\end{icmlauthorlist}

\icmlaffiliation{mit}{Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology (MIT), Cambridge, Massachusetts, USA}

\icmlcorrespondingauthor{Firstname Lastname}{email}

%
%
%
\icmlkeywords{}

\vskip 0.3in
]

%

%
%
%
%
%

%
\printAffiliationsAndNotice{\icmlEqualContribution} %

\begin{abstract}
We propose \emph{Score-of-Mixture Training} (SMT), a novel framework for training one-step generative models by minimizing a class of divergences called the
$\a$-skew Jensen–Shannon divergence. At its core, SMT estimates the score of mixture distributions between real and fake samples across multiple noise levels.
Similar to consistency models, our approach supports both training from scratch (SMT) and distillation using a pretrained diffusion model, which we call \emph{Score-of-Mixture Distillation} (SMD).
It is simple to implement, requires minimal hyperparameter tuning, and ensures stable training. Experiments on CIFAR-10 and ImageNet 64×64 show that SMT/SMD are competitive with and can even outperform existing methods.
\end{abstract}

\section{Introduction}

Fast and efficient sampling is a key characteristic sought after in modern generative samplers. For many years, generative adversarial networks (GANs) \cite{goodfellow2014generative} set the benchmark for high-quality one-step generative sampling. However, due to the inherent training instabilities associated with discriminator training, attention has recently shifted toward diffusion-based generative models \cite{Sohl-Dickstein--Weiss--Mehswaranathan--Ganguli, Ho--Jain--Abbeel2020, karras2022elucidating}. These models trade-off sampling efficiency for more stable training and significantly improved downstream sample quality through iterative sampling.

More recently, the diffusion distillation approach has been studied as an appealing option to significantly reduce the number of sampling steps. Early work~\citep{luhman2021knowledge, salimans2022progressive, meng2023distillation, berthelot2023tract} focused on training a student model with a lower sampling budget by condensing multiple teacher denoising steps into one.  The most recent works on distillation improve performance further by leveraging a pretrained model for distribution matching via minimization of the reverse KL divergence~\citep{luo2024diff, yin2024onestep, yin2024improved, salimans2024multistep, xie2024distillation}.
While attractive, distillation approaches necessitate a pretrained diffusion model which adds a significant overhead on the required compute. 

As yet another alternative, consistency models \citep{song2023consistency, songimproved} and their variants \citep{kim2023consistency} have been proposed for training few-step generative models from scratch by simulating the trajectories of the induced probability flow ODE \cite{Song--Garg--Shi--Ermon2020} of a diffusion process.
While consistency models have demonstrated promising results in both distillation and training from scratch, training is sensitive to the choice of noise schedule and distance measure~\citep{Geng--Pokle--Luo--Lin--Kolter2024}.

In this paper, we tackle the problem of training high-quality one-step generative models more directly, \ie \emph{without} simulating an iterative reverse diffusion process for sampling or leveraging a pretrained diffusion model during training.
%
%
%
%
%
%
Starting from first principles of statistical divergence minimization, 
we show that a high-quality one-step generative model can be trained from scratch in a stable manner, via the multi-noise-level denoising score matching (DSM) technique~\citep{vincent2011connection} used in diffusion models.
We emphasize that we do not require a simulation of the reverse diffusion process in our framework.
%

The proposed framework
achieves the best of several worlds: (1) a new, simple statistical divergence minimization framework without probability paths of ODE (like GAN), (2) stable training using denoising score matching (like diffusion models), (3) training from scratch without a pretrained diffusion model (like consistency models), and (4) near state-of-the-art one-step image generative performance (like GAN and consistency models). 
We also demonstrate that the proposed method can be extended to distill from a pretrained diffusion model, and can achieve performance similar to state-of-the-art methods for the same.
See Table~\ref{tab:comparison} for the overview of comparison.

\begin{table*}[t]
    \centering
    \caption{Comparison of different generative modeling techniques capable of high-quality sample generation.}
    \vspace{.5em}
    \scalebox{0.825}{
    \begin{tabular}{llccc}
    \toprule
    \textbf{Generative models} & \textbf{Training idea} & \textbf{\makecell[c]{Generation}} & \textbf{\makecell[c]{Training\\stability}} & \textbf{\makecell[c]{Require\\pretrained model?}}\\
    \midrule
    GAN & minimizing JSD, with discriminator & \bluet{one-step} & \redt{unstable} & \nomark \\ 
    \midrule
    Diffusion models & training multi-noise-level denoisers via DSM & \redt{multi-step} & \bluet{stable} & \nomark \\
    Diffusion distillation & (mostly) minimizing reverse KLD (in DMD) & \{\bluet{one},few\}-step & \bluet{stable} & \yesmark\\
    \midrule
    Consistency distillation 
    & \multirow{2}{*}{simulating trajectories of probability flow ODE} 
    & \multirow{2}{*}{\{\bluet{one},few\}-step}
    & \bluet{stable} & \yesmark\\
    Consistency training & 
    %
    & %
    & \redt{unstable} & \nomark\\
    \midrule
    \textbf{SMT} (ours) & \multirow{2}{*}{\makecell[l]{minimizing $\{\a{\text{-JSD}}\}_{\a\in[0,1]}$ with multi-noise-level training, \\ \quad \& scores of \emph{mixtures} via DSM}} &
    \multirow{2}{*}{\bluet{one-step}} & \multirow{2}{*}{\bluet{stable}} & \nomark\\
    \textbf{SMD} (ours) & & & & \yesmark\\
    \bottomrule
    \end{tabular}}  \label{tab:comparison}
\end{table*}

The rest of the paper is organized as follows: In Sec. \ref{sec:preliminaries} we introduce the necessary background and related works central to our proposed method.  In Sec. \ref{sec:training_from_scratch} we introduce our novel one-step generative modeling approach and in Sec. \ref{sec:distillation} we detail how our framework can be modified to perform diffusion distillation. We describe practical implementation details in both the latter sections and present experimental results in Sec.~\ref{sec:experiments}. We conclude with remarks in Sec.~\ref{sec:conclusion}.
Proofs and training details are deferred to Appendix.

%

%
%
%
%
%


\section{Preliminaries and Related Work}
\label{sec:preliminaries}

In one-step generative modeling, we wish to align the generated sample distribution $q_\th(\xv)\defeq \int \d(\xv-\gv_\th(\zv))q(\zv)\diff\zv$ with the true data distribution $p(\xv)$. Here, $\gv_\th\colon \Zc \rightarrow \Xc$ is a parametric neural sampler which is also often called an implicit generative model that transforms samples from a base measure $q(\zv)$.
In this section, we review some popular methods for training generative models, which will serve as preliminaries for our framework. 
More detailed discussion on the literature is deferred to Appendix \ref{sec:related_work}.

\textbf{Generative Adversarial Networks.}
The most prominent approach in training implicit generative models is the generative adversarial network (GAN) \citep{goodfellow2014generative}.
In its most standard and widely used form, it alternates between the gradient steps of discriminator and generator training, which are
%
%
%
%
\begin{align}
\min_\psi &~ \E_{p(\xv)}[\softplus(-\ell_\psi(\xv))] + \E_{q_\th(\xv)}[\softplus(\ell_\psi(\xv))],\label{eq:gan_disc}\\
\min_\th &~ \E_{q_\th(\xv)}[\softplus(-\ell_\psi(\xv))],\label{eq:gan_gen}
\end{align}
respectively, where $\softplus(y)\defeq \log(1+e^y)$ denotes the softplus function.\footnote{The generator loss $\E_{q_\th(\xv)}[\softplus(-\ell_\psi(\xv))]$ in the second line is the so-called \emph{non-saturating} version, while the original GAN generator loss $\E_{q_\th(\xv)}[-\softplus(\ell_\psi(\xv))]$ is referred to as \emph{saturating}.}
Here, we will call $\ell_\psi(\xv)$ the \emph{discriminator}, which is supposed to capture the \emph{log density ratio} $\log\frac{p(\xv)}{q_\th(\xv)}$.\footnote{Note the one-to-one correspondence between the standard definition of discriminator $D_\psi(\xv)\defeq \frac{\exp({\ell_\psi(\xv)})}{1+\exp({\ell_\psi(\xv)})}\in[0,1]$. }
%
This so-called adversarial training can be understood as minimizing the Jensen--Shannon divergence (JSD) with the help of discriminator, via the variational characterization of JSD.
%
%
%
%
%
%
%

Despite the popularity of GANs, training them is notoriously difficult. 
Although various techniques have been proposed to regularize the GAN objective---through alternatives to JSD~\citep{Nozowin--Cseke--Tomioka2016, Arjovsky--Chintala--Bottou2017,Mao--Li--Xie--Lau--Wang--Paul2017}, novel regularizers~\citep{Miyato--Kataoka--Koyama--Yoshida2018}, and specialized network architectures~\citep{Karras2019, Brock--Donahue--Simonyan2019,Sauer--Schwarz--Geiger2022}---the discriminator training remains unstable. 
This has sparked increasing interest in developing new objectives for training generative models which we briefly discuss below.

\textbf{Diffusion Models.} Diffusion models or score-based generative models \cite{Sohl-Dickstein--Weiss--Mehswaranathan--Ganguli, Ho--Jain--Abbeel2020} are state-of-the-art generative models that are based on the principles of thermodynamic diffusion. Given a \textit{forward stochastic differential equation (SDE) process}
\begin{equation*}
    \text{d}\xv_t = f(\xv_t, t)\text{d}t + g(t) \text{d}\wv_t,
\end{equation*}
where $f(\xv_t, t)$ is the drift function, $g(t)$ is the diffusion function, and $\wv_t$ represents a Brownian noise process, diffusion models simulate the \textit{reverse (generative) process}, which is also an SDE
\begin{equation*}
    \text{d}\xv_t = [f(\xv_t, t) - g(t)^2 \nabla_{\xv_t} \log p(\xv_t)]\text{d}t + g(t) \text{d}\bar{\wv}_t.
\end{equation*}
An equivalent deterministic \textit{probability flow ODE} with the same marginals as the SDE can also be used in practice:
\begin{equation*}
    \text{d}\xv_t = \Bigl(f(\xv_t, t) - \frac{1}{2}g(t)^2 \nabla_{\xv_t} \log p(\xv_t) \Bigr)\text{d}t.
\end{equation*}
Thus, to generate samples, diffusion models are trained to learn the score of the data distribution at multiple noise levels $\sigma_t$ via \textit{denoising score matching (DSM)} \cite{vincent2011connection}, \ie by minimizing
\begin{equation*}
    \Lc_{\text{DSM}}(\th) = \E_{p(\xv)q(\zv)p(t)} \left[w(t)\|\sv_\th(\xv_t; t) - \sv(\xv_t|\xv)\|^2\right],
\end{equation*}
where $p(t)$ denotes a distribution over different noise levels, $\xv_t \defeq \xv + \sigma_t \epsv, \epsv \sim \Nc(0, \mathbf{I})$, and $\sv(\xv_t|\xv) \defeq \nabla_{\xv_t} \log p(\xv_t|\xv)$. 
It is easy to show that $\sv_\th(\xv_t; t) = \nabla_{\xv_t} \log p(\xv_t)$ using Tweedie's formula \citep{Robbins1956AnEB}.
Sampling can then be achieved by Langevin dynamics \citep{Song--Ermon2019, Song--Garg--Shi--Ermon2020} or via black-box ODE solvers \citep{karras2022elucidating, lu2022dpm, lu2022dpmpp}.

\textbf{Diffusion Distillation.} 
In practical applications, running a diffusion model for multiple steps to generate a single sample can be prohibitively expensive. 
Distilling few-step generative models from a high-quality pretrained diffusion model has thus become popular~\citep{luo2024diff, yin2024onestep, yin2024improved, salimans2024multistep, xie2024distillation}.
To learn the generator's parameters, most, if not all, approaches aim to minimize the reverse Kullback--Leibler divergence (KLD) $\text{D}_{\text{KL}}(q_\th \| p)$ averaged across multiple noise levels:
\begin{equation*}
    \text{D}^{\text{avg}}_{\text{KL}}(q_\th \| p) \defeq \E_{q_\th(\xv)p(t)q(\epsv)}[\log q_\th(\xv_t) - \log p(\xv_t)].
\end{equation*}
To update the parameters via gradient descent, the gradient of this divergence is computed as
\begin{align}
    &\nabla_\th \text{D}^{\text{avg}}_{\text{KL}}(q_\th \| p)     \label{eq:reverse_kl_grad}\\ 
    &=\E_{ q(\zv)p(t)q(\epsv)}[\nabla_\th \gv_\th(\zv)(\sv_{q_\th}(\xv_t;t) - \sv_p(\xv_t;t)) \mid_{\xv = \gv_\th(\zv)}],
    \nonumber
\end{align}
where $\sv_{q_\th}$ and $\sv_p$ are the noisy scores of the fake and true samples, respectively. 
In the distillation setup, a pretrained diffusion model is plugged in as a close proxy to the true noisy score $\sv_p(\xv_t;t)$, while the fake noisy score $\sv_{q_\th}(\xv_t;t)$ is trained along with the generator to assist the training. 

\textbf{Consistency Models.} Distillation approaches often rely on pretrained score models and may use expensive regularizers to address issues like mode collapse and improve sample quality \citep{yin2024onestep, salimans2024multistep}. In contrast, consistency models \citep{song2023consistency, songimproved}, which can be trained from scratch, are trained to simulate the underlying probability flow ODE and ensure each sample along the trajectory maps to the origin. 
Consistency training, however, can be unstable and is known to sensitive to the noise schedule and distance function~\citep{Geng--Pokle--Luo--Lin--Kolter2024}. Additionally, the architecture for consistency models need to be carefully chosen, as the approach relies on a single-sample approximation of Tweedie's formula, which is only valid when noise levels are closely spaced.


\section{Training from Scratch}
\label{sec:training_from_scratch}

In this section, we introduce our new framework, \emph{Score-of-Mixture Training} (SMT). We describe how to efficiently train one-step generative models from scratch, \ie, without a pretrained diffusion model. In Sec.~\ref{sec:distillation}, we explain how the framework can be adapted to leverage a pretrained diffusion model when available, referring to this variant as \emph{Score-of-Mixture Distillation} (SMD).

The key ingredient of this framework is \emph{distribution matching} using a new family of statistical divergences (Sec.~\ref{sec:skew_jsd}), whose gradient can be approximated by estimating the score of \emph{mixture distributions} of real and fake distributions (Sec.~\ref{sec:score_mixture}), hence the name \emph{Score of Mixture} Training.  
We adopt the concept of multi-noise level learning from diffusion models and propose multi-divergence minimization for stable training (Sec.~\ref{sec:multi_noise_alpha}).  
A practical implementation of our method is described in Sec.~\ref{sec:practical_design_of_amortized_score}, followed by details of the training procedure in Sec.~\ref{sec:training_from_scratch_training}.

\subsection{Minimizing \texorpdfstring{$\a$}{alpha}-Skew Jensen--Shannon Divergences}
\label{sec:skew_jsd}
The crux of the new framework lies in minimizing a class of statistical divergences between $p(\xv)$ and $q_\th(\xv)$ defined as
\begin{align*}
\sjsd^{(\a)}(q_\th, p) 
&\defeq \frac{1}{\a} \kl(q_\th ~\|~ \a p + (1 - \a) q_\th) \\
&~\quad+ \frac{1}{1-\a}\kl(p ~\|~ \a p + (1 - \a) q_\th)
\end{align*}
for some $\a\in(0,1)$,
which we call the \emph{$\a$-skew Jensen-Shannon divergence} ($\a$-JSD)~\citep{Nielsen2010}.
This divergence belongs to $f$-divergences~\citep{csiszar2004information}. 

Interestingly, $\a$-skew JSD naturally interpolates between the forward Kullback--Leibler divergence (KLD) $\kl(p~\|~q_\th)$ (when $\a\to0$), the standard definition of JSD (when $\a=\half$), and the reverse KLD $\kl(q_\th~\|~p)$ (when $\a\to 1$).
In contrast to the forward KLD and reverse KLD, the $\a$-skew JSD with $\a\in(0,1)$ is well-defined even when there is a support mismatch in $p$ and $q_\th$, which may be the case especially in the beginning of training.

\bluet{\textbf{\ul{Feature 1: Multi-Divergence Training.}}}
Hence, we propose to minimize a weighted sum of the $\a$-JSD's for different $\a$'s, as divergences with different $\a$'s exploit different geometries between two distributions.
For example, it is known that minimizing the forward and reverse KLD leads to mode-covering and mode-seeking behaviors, respectively, and 
we can enforce better support matching behavior by considering the entire range of $\a$.


To minimize this family of divergences in practice,
we consider its gradient expression:
\begin{proposition}
\label{prop:gradient}
Suppose that $\E_{q_\th(\xv)}[\nabla_\th \log q_\th(\xv)]=0$.\footnote{It is a standard assumption in the literature~\citep{Hyvarinen2005}, which holds under a mild regularity assumption on the parametric model $q_\th(\xv)$ so that $\int \nabla_\th q_\th(\xv)\diff\xv = \nabla_\th \int q_\th(\xv)\diff\xv$.}
Then, we have
\begin{align}
&\nabla_\th \textnormal{D}_\textnormal{JSD}^{(\a)}(q_\th,p) \label{eq:grad_sjsd}\\
&= \frac{1}{\a}\E_{q(\zv)}\Bigl[\nabla_\th\gv_\th(\zv) (\sv_{\th;0}(\xv)-\sv_{\th;\a}(\xv))\Big|_{\xv=\gv_\th(\zv)}\Bigr],\nonumber
\end{align}
where we define the score of the mixture distribution
\[
\sv_{\th;\a}(\xv) \defeq \nabla_\xv\log(\a p(\xv) + (1-\a)q_\th(\xv)).
\]
\end{proposition}
This proposition suggests that we can update the generator $\gv_\th(\zv)$ using this gradient expression, provided that we can estimate the \emph{score of the mixture distribution} $\sv_{\th;\a}(\xv)$.
%
%
%

\bluet{\textbf{\ul{Feature 2: Amortized Score Model.}}}
To implement this idea, in this paper, we propose to use an \emph{amortized score model} $(\xv,\a)\mapsto\sv_{\psi}(\xv; \a)$, to approximate the score of mixture $\sv_{\th; \a}(\xv)$. 
Through our experiments we show that learning the scores of mixture over different $\a$'s using a single model is effective and helps training.
In Sec.~\ref{sec:score_mixture}, we explain how we can train the amortized score model $(\xv,\a)\mapsto\sv_{\psi}(\xv;\a)$ using samples from $p(\xv)$ and $q_\th(\xv)$.

%
\subsection{Learning with Multiple Noise Levels}
\label{sec:multi_noise_alpha}
%
%
To achieve stable training,
we opt to minimize the divergence at different noise levels by considering the convolved distributions, 
$p_t \defeq p * \Nc(\mathbf{0}, \sigma_t^2\mathbf{I}_D)$ and $q_{\th, t} \defeq q_\th * \Nc(\mathbf{0}, \sigma_t^2\mathbf{I}_D)$. 
This idea is widely used in the existing distillation methods. 
We borrow the variance-exploding Gaussian noising process notation from \citet{karras2022elucidating} where $\sigma_t \in [\sigma_{\text{min}}, \sigma_{\text{max}}]$.
As we also integrate over different $\alpha$'s,
the final objective becomes
\begin{equation}
    \Lc_{\text{gen}}(\th) \defeq \E_{p(\alpha)p(t)}[\sjsd^{(\a)}(q_{\th, t}, p_t)],
\end{equation}
where we will prescribe the choice of $p(\a)$ in Sec.~\ref{sec:training_from_scratch_training}.
%
%
%
%
%
Similar to Eq. \eqref{eq:grad_sjsd}, the gradient of the divergence at noise level $t$ can be approximated via the amortized score as
\begin{align}
&\nabla_\th \sjsd^{(\a)}(q_{\th, t},p_t) 
\approx \boldsymbol{\gamma}_{\psi}(\theta; \a,t) 
\label{eq:grad_sjsd_imp}\\
&\defeq \E_{q(\zv)}\Bigl[\nabla_\th\gv_\th(\zv)\frac{\sv_{\psi}(\xv_t; 0, t)-\sv_{\psi}(\xv_t; \a, t)}{\a}\Big|_{\xv=\gv_\th(\zv)}\Bigr],\nonumber
\end{align}
where
the amortized score model $\sv_{\psi}(\xv_t; \a, t)$, which is conditioned on the noise level $t$, is an estimate of $\sv_{\th; \a, t}(\xv_t) \defeq \nabla_{\xv_t} \log (\a p(\xv_t) + (1-\a)q_\th(\xv_t))$.
We provide a practical implementation of the amortized score model as a small modification of a diffusion model architecture in Sec.~\ref{sec:practical_design_of_amortized_score}.
We remark in passing that this expression can be understood as a generalization of the gradient update of Eq.~\eqref{eq:reverse_kl_grad} used in the existing reverse-KLD-based distillation schemes.

Finally, we can then approximate the generator gradient as
\begin{align*}
&\nabla_\th \Lc_{\text{gen}}(\th) \approx \E_{p(\a)p(t)}[\boldsymbol{\gamma}_{\psi}(\theta; \a,t) ].
\end{align*}
Importantly, similar to existing distillation methods, the gradient only involves the output of the score model, but not its gradient. 
This is beneficial since such extra gradient information requires expensive backpropagation through the score model to the generator~\citep{zhou2024score}.
%

%
\subsection{Estimating Score of Mixture Distributions}
\label{sec:score_mixture}
Estimating the score of the mixture distribution turns out to be as simple as minimizing a mixture of the score matching losses, as stated in the following proposition:
\newpage

\begin{proposition}
\label{prop:interpolated_score}
For any $\a\in[0,1]$,
the minimizer of the objective function
\begin{align}
\Lc(\psi; \a) 
= \a\,\E_{p(\xv)}&[\|\sv_{\psi}(\xv; \a) - \sv_p(\xv)\|^2] \nonumber\\
+ (1 - \a)\,\E_{q_\th(\xv)}&[\|\sv_{\psi}(\xv; \a) - \sv_{q_\th}(\xv)\|^2]
\label{eq:mixture_score_matching}
\end{align}
satisfies $\sv_{\psi^*}(\xv; \a) = \sv_{\th;\a}(\xv).$
%
%
%
\end{proposition}
%
%
%

Since we train with multiple noise levels, we are interested in the marginal score of $\xv_t = \xv + \sigma_t \epsv, \epsv \sim \Nc(0, \mathbf{I})$ at some noise level $\sigma_t$. We can use denoising score matching \cite{vincent2011connection} to define an equivalent \emph{sample-only} objective to learn the score using Tweedie's formula.
Namely, to approximate $\sv_{\th;\a, t}(\xv)$ using the amortized score model $\sv_{\psi}(\xv; \a, t)$, 
we can minimize 
\begin{align*}
\Lc_{\text{score}}(\psi) \defeq \E_{p(\a)p(t)}[\Lc_{\text{score}}(\psi; \a, t) ],
\end{align*}
where
\begin{align}
\Lc_{\text{score}}&(\psi; \a, t) 
\defeq \a\,\E_{p(\xv)q(\epsv)}[\|\sv_{\psi}(\xv; \a, t) + \epsv/\sigma_t\|^2] \nonumber\\
&~~~ + (1 - \a)\,\E_{q_\th(\xv)q(\epsv)}[\|\sv_{\psi}(\xv; \a, t) + \epsv/\sigma_t\|^2].
\label{eq:mixture_score_matching_imp}
\end{align}
See Proposition~\ref{prop:interpolated_score_dsm} for a formal statement.
In practice, we parametrize the score model in the form of a \emph{denoiser} and reconstruct the score from the denoiser output via Tweedie's formula; see Appendix~\ref{sec:appendix_amortized_denoiser}.

\bluet{\textbf{\ul{Feature 3: Leveraging Real and Fake Samples via Amortized Score Estimation.}}} 
We remark that our score learning objective seamlessly utilizes both real and fake samples throughout the training, helping the generator better generalize.
This is in contrast to some existing diffusion distillation methods, which introduce expensive regularizers to integrate real samples, or backpropagate through the pretrained score model~\citep{yin2024onestep, yin2024improved, salimans2024multistep}. 

\subsection{Practical Design of Amortized Score Network}
\label{sec:practical_design_of_amortized_score}

With an additional conditioning scheme to embed auxiliary information about $\a$ in addition to the noise level $\sigma_t$, any existing diffusion model backbone can be used to parameterize the amortized score network $\sv_{\psi}(\xv;\a,t)$.
Here, we describe how we can modify the popular UNet-based score architectures~\citep{Song--Garg--Shi--Ermon2020, Nichol--Dhariwal2021, karras2022elucidating} with minimal modifications.

First, drawing from the noise embedding sensitivity analysis by \citet{songimproved}, we opt for a Fourier embedding $\cv_\a$ with a default scale of 16. This choice ensures that the embedding is sufficiently sensitive to fluctuations in $\a$, particularly during the early stage of training. 

Then, we concatenate the $\alpha$-embedding with the embedding of other auxiliary information (\eg $t$ and labels) and apply a single SiLU \cite{elfwing2018sigmoid} activated linear layer: 
%
\begin{equation*}
\cv_{\text{out}} = {\sf silu}(\mathbf{W}_{\text{aux}} \cv_{\text{aux}} + \mathbf{W}_\alpha \cv_\alpha).
\end{equation*}
The rationale behind this choice is as follows: as training progresses, the real and fake distributions begin to overlap, making it natural for the amortized score model to become less sensitive to $\a$.
Thanks to the additional linear layer $\mathbf{W}_\alpha$ after the $\a$-embedding $\cv_\a$, this behavior can be realized when $\mathbf{W}_\alpha \approx \mathbf{0}$, when necessary.



\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{figs/alpha_jsd.pdf}
\caption{{Overview of SMT.}
\textbf{Top:} To update the generator, we compute the gradient of the $\a$-JSD on noisy fake samples with the \textit{frozen} amortized score model using Eq.~\eqref{eq:grad_sjsd_imp}. 
\textbf{Bottom:} The amortized score model is updated by computing the score of the mixture distribution on both fake and real noisy samples, and then updating the weights using the gradient in Eq.~\eqref{eq:mixture_score_matching_imp}.}
    \label{fig:smt_scratch}
\end{figure*}

\subsection{Training}
\label{sec:training_from_scratch_training}
\textbf{Alternating Training.} Our training scheme alternates between the score estimation with the score matching objective in Eq.~\eqref{eq:mixture_score_matching_imp}, and the generator training with Eq.~\eqref{eq:grad_sjsd_imp}, where we plug-in $\sv_{\psi}(\xv_t; \a, t)$ in place of $\sv_{\th;\a, t}(\xv_t)$.
This is similar in spirit to GAN training, but the DSM technique in our framework in place of the discriminator training naturally stabilizes training.
The overall training framework is summarized in Fig.~\ref{fig:smt_scratch} and Alg.~\ref{alg:smt} in Appendix \ref{sec:appendix_amortized_score_training}.

%
%

\textbf{Initialization.} 
We warm up the generator with a standard denoising task as in diffusion models for several steps to better initialize the weights, as we empirically found that initializing the generator with pretrained weights from a denoiser significantly accelerated convergence. 
The amortized score network is randomly initialized.

\textbf{Choice of $p(\alpha)$.} 
The choice of $p(\a)$ is crucial in our framework. To train both the generator and score model, we sample $\a$ from a uniform distribution over 1000 equally spaced points in  $[0,1]$, ensuring a dense enough grid to generalize to any $\a$. For score training, we further ensure that 25\% of the sampled $\a$'s are zero, since this is always used in our gradient update; see Eq.~\eqref{eq:grad_sjsd_imp}. 

\textbf{Adaptive Weighting.} In practice we compute the gradient with an adaptive weight $w(\xv_t, \xv, \a, t)$ to ensure that the scale of the gradient for each minibatch sample is roughly uniform for different values of $\a$ and $t$. Hence, we modify the generator gradient in Eq.~\eqref{eq:grad_sjsd_imp} as
%
%
%
%
\begin{align}
    &\mathbf{\gamma}^w_\psi(\th; \a, t) \defeq  \E_{q(\zv)}\Bigl[\nabla_\th\gv_\th(\zv) \times     \label{eq:weighted gradient}\\
    &\Bigl\lbrace w(\xv_t, \xv, \a, t)\frac{\sv_{\psi}(\xv_t; 0, t)-\sv_{\psi}(\xv_t; \a, t)}{\a}\Bigr\rbrace\Big|_{\xv=\gv_\th(\zv)}\Bigr] \nonumber,
\end{align}
where the weighting is defined as
\begin{equation}
w(\xv_t, \xv, \a, t) \defeq w_\a(\xv_t, t)w_{\text{DMD}}(\xv_t, \xv, t).
\label{eq:adaptive_weight}
\end{equation}
Here $w_{\text{DMD}}$ is the adaptive noise weighting introduced by \cite{yin2024onestep} (see Eq.~\eqref{eq:dmd_weighting} in Appendix \ref{sec:related_work}) and $w_\a(\xv_t, t)$ is a new weighting inspired by the pseudo-Huber norm \cite{song2024improved, Geng--Pokle--Luo--Lin--Kolter2024}
\begin{equation}
    w_\a(\xv_t, t) \defeq \a \sqrt{\frac{\|\sv_\psi(\xv_t; 0, t) - \sv_\psi(\xv_t; 1, t)\|^2}{\|\sv_\psi(\xv_t; 0, t) - \sv_\psi(\xv_t; \a, t)\|^2}}.\nonumber
\end{equation}
This weighting still preserves the limiting forward KLD behavior of the objective as $\alpha \rightarrow 0$ and simplifies to DMD gradient when $\alpha=1$. We empirically show the efficacy of our adaptive weighting term $w_\a(\xv_t, t)$ through ablation studies on the CIFAR-10 dataset in Sec.~\ref{sec:ablation_studies}; see Fig.~\ref{fig:ablations}.

\textbf{Regularization with GAN.}  
We empirically found that a GAN-type regularization can accelerate convergence even further in the beginning of training.
More concretely, we can train the discriminator $\ell_\psi(\xv_t; t)\approx \log \frac{p(\xv)}{q_\th(\xv)}$ by the GAN discriminator training in Eq.~\eqref{eq:gan_disc}.
In our implementation, we opt to train a discriminator using a variant based on the $\a$-JSD, as described in Appendix~\ref{app:gan_type_reg}.
Given a discriminator $\ell_\psi(\xv_t; t)$, we minimize a \textit{non-saturating version} of the $\alpha$-JSD loss (cf.~Eq.~\eqref{eq:gan_gen}), 
\begin{equation}
\Lc^{(\a, t)}_{\text{GAN}}(\th) = \E_{q_\th(\xv_t)}\Bigl[{\sf sp}\Bigl(-\ell_\psi(\xv_t; t)-\log \frac{\a}{1 - \a}\Bigr)\Bigr].
\label{eq:non_sat_amortized_gan_loss}
\end{equation}
The derivation can be found in Appendix~\ref{app:gan_type_reg}.
Similar to \citet{yin2024improved}, we parameterized the discriminator by a stack of convolution layers, applied on top of an intermediate feature of the amortized score network at $\alpha = 1/2$. 


%
%
%
%
%
%

\section{Distilling from Pretrained Diffusion Model}
\label{sec:distillation}
%
In our development so far, we do not assume access to a pretrained diffusion model.  
%
In this section, we show how a practitioner can train an one-step generative model leveraging
a pretrained diffusion model, if available, within our framework. 
The proposed distillation scheme is comparable or even outperforms the state-of-the-art distillation schemes.

\subsection{How To Leverage Pretrained Diffusion Model}
In the distillation setup, we treat the pretrained diffusion model as the data score $\sv_p(\xv_t;t)$, and 
thus training the score of mixture $\sv_{\th;\a}(\xv_t;t)$ using a single, amortized model may not be the most efficient parameterization.
Hence, instead, we consider the following expression 
%
\begin{equation*}
%
\sv_{\th;\a}(\xv) 
= D_{\th;\a}(\xv)\sv_p(\xv) + (1-D_{\th;\a}(\xv))\sv_{q_\th}(\xv),
\end{equation*}
where
\begin{align*}
D_{\th;\a}(\xv)
&\defeq 
\frac{\a p(\xv)}{\a p(\xv)+(1-\a)q_\th(\xv)}
\\
&=\sigma\Bigl(\log\frac{p(\xv)}{q_\th(\xv)}+\log\frac{\a}{1-\a}\Bigr).
\end{align*}
See Proposition~\ref{prop:alternate_parametrization} for a formal statement.
In words, we can express the score of mixture $\sv_{\th;\a}(\xv)$ as a mixture of scores $\sv_p$ and $\sv_{q_\th}$, where the weight is $(D_{\th;\a}(\xv),1-D_{\th;\a}(\xv))$.
This suggests that instead of an amortized modeling of the score of mixture, we can use an alternative parameterization,
\begin{align*}
\sv_{\psi}^{\sf exp}(\xv; \a)
&\defeq D_{\psi}(\xv; \a)\sv_p(\xv) 
%
%
+ (1-D_{\psi}(\xv; \a))\sv_{\psi}^{\sf fake}(\xv),
\end{align*}
where 
\begin{align*}
D_{\psi}(\xv; \a)
\defeq \sigma\Bigl(\ell_\psi(\xv)+\log\frac{\a}{1-\a}\Bigr).
\end{align*}
Here, we can parameterize the \emph{discriminator} $\xv\mapsto \ell_\psi(\xv)$ in the same way as we do for the GAN discriminator.

We can extend this to multiple noise levels easily. Hence, an alternative parameterization for $\sv_{\th;\a}(\xv_t; t)$ is 
\begin{align}
\sv_{\psi}^{\sf exp}(\xv_t; \a, t)
&\defeq D_{\psi}(\xv_t; \a, t)\sv_p(\xv_t; t) \\
&\qquad+ (1-D_{\psi}(\xv_t; \a, t))\sv_{\psi}^{\sf fake}(\xv_t; t), \nonumber
\end{align}
where
\begin{align}
D_{\psi}(\xv_t; \a, t)
\defeq \sigma\Bigl(\ell_\psi(\xv_t; t)+\log\frac{\a}{1-\a}\Bigr).
\end{align}
Plugging this explicit score model into Eq.~\eqref{eq:mixture_score_matching_imp}, we can learn both the fake score model $\sv_\psi^{\sf fake}$ and the discriminator $\ell_\psi$ at different noise levels.

\begin{corollary}
\label{cor:interpolated_score_dsm_alternative}
Let $\a \in [0, 1]$ be fixed and $\sigma_t$ be some fixed noise level. 
Then, the minimizer of the objective function
\begin{align}
&\Lc^{\sf exp}(\psi; \a, t) 
\defeq \a\,\E_{p(\xv)q(\epsv)}[\|\sv_{\psi}^{\sf exp}(\xv_t; \a, t) + \epsv/\sigma_t\|^2] \nonumber\\
&~~~~ + (1 - \a)\,\E_{q_\th(\xv)q(\epsv)}[\|\sv_{\psi}^{\sf exp}(\xv_t; \a, t) + \epsv/\sigma_t\|^2]
\label{eq:mixture_score_matching_imp_alternative}
\end{align}
satisfies 
\begin{equation*}
\sv_{\psi^*}^{\sf fake}(\xv; t) = \sv_{q_\th}(\xv; t)
~\text{ and }~
\ell_{\psi^*}(\xv_t;t) = \log\frac{p(\xv_t)}{q_\th(\xv_t)}.
\end{equation*}
\end{corollary}

We remark that this new regression objective in Eq.~\eqref{eq:mixture_score_matching_imp_alternative} provides a new way to compute the log density ratio, as an alternative to the GAN training (see Eq.~\eqref{eq:gan_disc}). 
In Appendix~\ref{app:lsgan}, we establish a connection between this objective for training a discriminator to an existing GAN discriminator objective in the literature.
%
%
%
%
%
%

With this new, explicit parameterization, we can approximate the gradient expression in Eq.~\eqref{eq:grad_sjsd_imp} as
%
%
%
%
%
%
%
%
\begin{align}
&\nabla_\th\sjsd^{(\a)}(q_{\th, t}, p_t) \approx \boldsymbol{\gamma}_{\psi}^{\sf exp}(\theta; \a,t) \label{eq:explicit grad}\\
&\defeq\E_{q(\zv)}\Bigl[D_{\psi}(\xv_t; \a, t)\times \nonumber\\
&\qquad\qquad
\nabla_\th\gv_\th(\zv)
\frac{\sv_\psi^{\sf fake}(\xv_t; t)-\sv_p(\xv_t, t)}{\a}\Big|_{\xv=\gv_\th(\zv)}\Bigr].\nonumber
\end{align}


\subsection{Implementation and Training}

\textbf{Model Architectures.}
We can leverage any existing diffusion model architectures directly for the fake score $\sv_{\psi}^{\sf fake}(\xv_t;t)$. 
We parametrize the discriminator $\ell_\psi(\xv_t;t)$ similar to the noise-conditional discriminator in our training from scratch setting (see Sec.~\ref{sec:training_from_scratch_training}). 
The difference is that we can train the discriminator by minimizing the DSM loss in Eq.~\eqref{eq:mixture_score_matching_imp_alternative} naturally, without an additional GAN loss.  
When training the generator, we plug in this approximate log density ratio into Eq.~\eqref{eq:non_sat_amortized_gan_loss} to regularize the generator updates.

\textbf{Training.} We also train in an alternating fashion. 
Since we have access to a pretrained score model, we use this to initialize the weights of both the generator and the fake score model.
We utilize the same sampling distribution for $\alpha$ as in our training from scratch setup (see Sec. \ref{sec:training_from_scratch_training}).
The procedure is summarized in Fig.~\ref{fig:smt_distillation} and Alg.~\ref{alg:smt distillation} in Appendix~\ref{sec:appendix_amortized_score_training}.


\begin{figure*}[t] %
    \centering
    \begin{minipage}{0.6\textwidth} %
        \centering
        \captionsetup{type=table} %
        \caption{{Image generation results on ImageNet 64x64 (class-conditional) and CIFAR-10 32x32 (unconditional).} The size of the sampler is denoted by the number of parameters (\# params), and NFE stands for the Number of Function Evaluations.
        The best FIDs from each category are highlighted in bold, and our methods \textbf{SMT} and \textbf{SMD} are highlighted with a blue shade.
        }\vspace{.5em}
        \scalebox{0.765}{
        \begin{tabular}{@{}lcccccc@{}}
        \toprule
        \multicolumn{1}{c}{} & \multicolumn{3}{c}{ImageNet 64x64} & \multicolumn{3}{c}{CIFAR-10 32x32} \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-7}
        Method & \# params & NFE & FID$\downarrow$ & \# params & NFE & FID$\downarrow$ \\ \midrule
        \multicolumn{7}{c}{\textit{Training from scratch: Diffusion models}} \\
        DDPM \citep{Ho--Jain--Abbeel2020}  & - & - & - & 56M & 1000 & 3.17 \\
        ADM \citep{dhariwal2021diffusion} & 296M & 250 & 2.07 & - & - & -\\
        EDM \citep{karras2022elucidating} & 296M & 512 & \textbf{1.36} & 56M & 35 & \textbf{1.97} \\
        \hdashline
        \multicolumn{7}{c}{\textit{Training from scratch: One-step models}} \\
        CT \cite{song2023consistency} & 296M & 1 & 13.0 & 56M & 1 & 8.70 \\
        iCT \citep{song2024improved} & 296M & 1 & 4.02 & 56M & 1 & 2.83 \\
        iCT-deep \citep{song2024improved} & 592M & 1 & 3.25 & 112M & 1 & \textbf{2.51} \\
        ECT \cite{Geng--Pokle--Luo--Lin--Kolter2024} & 280M & 1 & 5.51 & 56M & 1 & 3.60 \\
        \rowcolor{lightblue}
        \textbf{SMT} (ours) & 296M & 1 & \textbf{3.23} & 56M & 1 & 3.13 \\
        \midrule
        \multicolumn{7}{c}{\textit{Diffusion distillation}}\\
        PD \citep{salimans2022progressive}  & 296M & 1 & 10.7 & 60M & 1 & 9.12 \\
        TRACT \citep{berthelot2023tract} & 296M & 1 & 7.43 & 56M & 1 & 3.78 \\
        CD (LPIPS) \citep{song2023consistency} & 296M & 1 & 6.20 & 56M & 1 & 4.53 \\
        Diff-Instruct \citep{luo2024diff} & 296M & 1 & 5.57 & 56M & 1 & 4.53 \\
        MultiStep-CD \citep{heek2024multistepconsistencymodels} & 1200M & 1 & 3.20 & - & - & -\\
        DMD w/o reg \citep{yin2024onestep} & 296M & 1 & 5.60 & 56M & 1 & 5.58 \\
        DMD2 w/ GAN \cite{yin2024improved} & 296M & 1 & 1.51 & 56M & 1 & 2.43 \\
        MMD \citep{salimans2024multistep} & 400M & 1 & 3.00 & - & - & - \\
        SiD \cite{zhou2024score} & 296M & 1 & 1.52 & 56M & 1 & \textbf{1.92} \\
        SiM \cite{luoone} & - & - & - & 56M & 1 & 2.02 \\
        \rowcolor{lightblue}
        \textbf{SMD} (ours) & 296M & 1 & \textbf{1.48} & 56M & 1 & 2.22 \\
        \hdashline
        \multicolumn{7}{c}{\textit{w/ expensive regularizer or finetuning}} \\
        CTM  \cite{kim2023consistency} & 296M & 1 & 1.92 & 56M & 1 & \textbf{1.98} \\
        DMD w/ reg \citep{yin2024onestep} & 296M & 1 & 2.62 & 56M & 1 & 2.66 \\
        DMD2 (finetuned) \cite{yin2024improved} & 296M & 1 & \textbf{1.23} & - & - & - \\
        \bottomrule
        \end{tabular}}
        \label{tab:exp}
        \end{minipage}
    \hfill
    \begin{minipage}{0.39\textwidth} %
    \centering   
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/smt_imagenet.pdf}
        \caption{ImageNet 64$\times$64 (scratch and distillation).}
        \label{fig:imagenet_training}
    \end{subfigure}\vspace{1em}
    
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/smt_cifar10.pdf}
        \caption{CIFAR-10 with ablation studies (scratch).}
        \label{fig:ablations}
    \end{subfigure}%
    \nextfloat
    \caption{{FID evolution with training.}}
    \label{fig:global}
    \end{minipage}
    %
    %
    %
    %
    %
    %
    %
    %
\end{figure*}



\section{Experiments}
\label{sec:experiments}
In this section, we first present results on the ImageNet $64 \times 64$ dataset. 
We then demonstrate the competitiveness of our method on the CIFAR-10 dataset and conduct a series of ablation studies.  
We measure performance through sample quality as measured by the Fr\'echet Inception Distance (FID)~\citep{heusel2017gans}. 
The exact hyerparameters, training configurations used and additional results can be found in Appendix~\ref{sec:appendix_experiments_and_results}.
%


%

\subsection{Class-conditional ImageNet 64x64 Generation}

\textbf{Experimental Setup.} We trained class-conditional one-step generative models on ImageNet $64 \times 64$~\citep{imagenet}, experimenting with both distillation and training from scratch.
\ul{In both cases}, we used the ADM architecture \citep{Nichol--Dhariwal2021} as the base score model architecture, and the discriminator $\ell_\psi(\xv_t;t)$ was implemented as a stack of convolution layers operating on the bottleneck feature from the score network, similar to DMD2 \citep{yin2024improved}.
For \ul{training from scratch}, we augmented the score architecture using an $\a$-embedding as described in Sec.~\ref{sec:practical_design_of_amortized_score}. The total number of parameters of the amortized score model remained unchanged otherwise. As a warmup stage, we pretrained the generator on the dataset using a standard diffusion denoising objective for 40k steps to initialize the weights.
For \ul{distillation}, we used a pretrained diffusion model from \citep{karras2022elucidating}.  


%
%
%
%

\begin{figure}[tb]
    \centering
    \includegraphics[width=\linewidth]{figs/imagenet_snippet_classed.png}
    \caption{Samples from SMT on ImageNet 64$\times$64. Each row represents a unique class.
    Additional samples can be found in Appendix~\ref{sec:samples}.
    }%
    \label{fig:imagenet snippet}
\end{figure}
%

\newpage
\textbf{Results.} We evaluated our method against several published baselines for both training from scratch and distillation. 
As shown in Table \ref{tab:exp}, when trained from scratch, our generator with 296M parameters outperforms both consistency training and its improved variant \cite{song2023consistency, song2024improved}, with a much smaller training budget (200k iterations with batch size of 40 vs. 800k iterations with batch size of 512). 
Our model also competes favorably with iCT-deep, despite using a generator with half the number of parameters: FID of 3.23 with 296M parameters (ours) vs. 3.25 with 592M parameters (iCT-deep). 
We observed stable training throughout, without requiring extensive hyperparameter tuning or special noise schedule adjustments as in consistency training, as visualized in Fig.~\ref{fig:imagenet_training}. 
We also surpass the ECT model \cite{Geng--Pokle--Luo--Lin--Kolter2024} of similar size and training budget that includes several modifications to induce stability in consistency training. Samples generated using our method can be found in Fig. \ref{fig:imagenet snippet} and Appendix \ref{sec:appendix_experiments_and_results}.

In the distillation setting, our model achieves a competitive FID of 1.48, outperforming several baselines. 
Notably, we outperform consistency distillation methods, such as multistep consistency distillation~\citep{heek2024multistepconsistencymodels}, despite using only a fraction of the model size (256M parameters against 1200M parameters).
Our model also surpasses consistency trajectory models (CTM)~\citep{kim2023consistency}, without the need for expensive simulation of the probability flow ODE. 
We also outperform reverse-KLD methods with similar compute or regularizers such as DMD \cite{yin2024onestep} and DMD2 with FIDs of 5.60 and 1.51 respectively.  
We note that on spending significant extra compute, DMD and DMD2 achieved improved results with expensive regularizers that require simulation of the pretrained model or lengthy finetuning stages of 400k steps. 
We did not resort to these techniques and sought to find an approach that worked best with a single execution of the training pipeline.


\subsection{Unconditional CIFAR-10 Generation}

\textbf{Experimental Setup.} We evaluated our method on the CIFAR-10 dataset~\citep{Krizhevsky--Hinton2009} for unconditional one-step generative modeling, considering both training from scratch and distillation. \ul{In both cases}, we employed a DDPM++ architecture~\citep{Song--Garg--Shi--Ermon2020} with EDM preconditioning~\citep{karras2022elucidating}. The discriminator again followed the convolutional stack used in DMD2.
For \ul{training from scratch}, we modified the score model to incorporate the $\alpha$-embedding (Sec.~\ref{sec:practical_design_of_amortized_score}) while maintaining a similar network size. To mitigate overfitting due to the dataset's small size, we enabled dropout with $p=0.13$, as in EDM.
In the \ul{distillation setting}, we initialized the generator with a pretrained unconditional diffusion model from~\citep{karras2022elucidating}, using the same UNet backbone and weights. Distillation performed well without dropout. 
%


\textbf{Results.} The last three columns in Table~\ref{tab:exp} highlight the performance of our method on CIFAR-10 compared to various baselines. In our training from scratch setting, despite utilizing a lower training budget (150k steps with a batch size of 40) than many methods, our approach remains highly competitive. 
In terms of training budget, the most comparable baseline is ECT, which we are able to outperform without requiring excessive design considerations and hyperparameter tuning. 
Our distillation results are also competitive.
In particular, we outperform DiffInstruct and DMD2, which are only based on minimizing the reverse KLD. 
This corroborates the benefit of our multi-divergence minimization approach. Image samples can be found in Appendix~\ref{sec:appendix_experiments_and_results}.

\subsection{Ablation Studies}
\label{sec:ablation_studies}

We use the CIFAR-10 dataset to study the effectiveness of the design choices that we have proposed; see Fig.~\ref{fig:ablations}.

\textbf{Choice of Adaptive Gradient Weighting.} 
Starting with our base objective without the GAN regularizer, we tested our $(\a,t)$-adaptive weighting in Eq.~\eqref{eq:adaptive_weight}. 
Fig.~\ref{fig:ablations} demonstrates the benefits of our weighting scheme, compared to the DMD weight function that only depends on $t$.

\textbf{Learning with Single vs. Multiple $\alpha$'s.} 
The $\a$-JSD reduces to the reverse KLD of DMD and other distillation methods, when $\alpha=1$. 
To test the efficacy with multi-$\a$ learning, we implemented an amortized variant, training the score model only with $\alpha \in \{0,1\}$. 
Results show that conditioning on a range of \(\alpha\)-values not only minimizes multiple divergences but also strengthens the \(\alpha\) embedding as a conditioning signal thereby facilitating more accurate divergence minimization.

\textbf{Accelerated Convergence with GAN Regularizer.} 
We finally verify the benefits of our novel GAN-type regularizer for $\a$-JSD minimization. As demonstrated by the second and fourth curves in Fig.~\ref{fig:ablations}, the GAN regularizer helps accelerate convergence especially in the beginning of training.

\section{Concluding Remarks}
\label{sec:conclusion}

In this paper, we show that high-quality one-step generative models can be trained from scratch and in a stable manner, without  simulating the reverse diffusion process or probability flow ODE as in diffusion models and consistency models.
The key distinctive idea in our framework is a new multi-divergence minimization paradigm implemented by estimating the score of mixture distributions.
For stable training, 
we borrow multi-level noise learning and denoising score matching techniques from the diffusion literature. 
Our empirical results show that accurate score estimation facilitates stable minimization of statistical divergences. We hope this work offers a fresh perspective on generative modeling and inspires further research in the field.

\textbf{Limitations and Future Work.}
While SMT/SMD achieve strong empirical performance, there is still room for improvement in both architecture and training strategies. Additionally, although SMT/SMD attain highly competitive FID for one-step generation from scratch, models with a few-step generation capability--such as consistency models--can further improve FID with additional iterations. Finally, given the generality of our framework, we believe these ideas could extend to other complex modalities, including speech and audio synthesis.
We leave such directions for future work.


%
%

%

%
%
%
%
%
%
%
%
%

%

%

%

%
%
%
%
%
%
%
%
%
%
%
%
%
%

%

%

%



%
%


\section*{Impact Statement}
We introduce Score-of-Mixture Training, a simple yet effective one-step generative modeling framework that requires minimal design effort and hyperparameter tuning. We hope its ease of implementation will drive further research into efficient, state-of-the-art neural sampling. However, we acknowledge the potential risks of misuse, including the generation of fake, biased, or misleading content. Our work focuses on fundamental research using standard machine learning datasets, but we recognize the importance of ensuring generative models are secure and privacy-preserving to democratize this technology responsibly.

\section*{Acknowledgements}
This work was supported in part by the MIT-IBM Watson AI Lab under Agreement No. W1771646, and by AFRL and the USAF AI Accelerator under Cooperative Agreement Number FA8750-19-2-1000.


%
%
%

\bibliography{ref}
\bibliographystyle{icml2025}


%
%
%
%
%
\newpage
\appendix
\onecolumn

\addtocontents{toc}{\protect\StartAppendixEntries}
\listofatoc

\section{Deferred Statements and Proofs}
\label{sec:appendix_a}

\subsection{Proof of Proposition~\ref{prop:gradient}}
\begin{proof}[Proof of Proposition~\ref{prop:gradient}]
We can simplify the gradient of each term separately as follows:
\begin{align*}
\nabla_\th \kl(q_\th \| \a p + (1 - \a) q_\th) 
&= \E_{q_\th(\xv)}\Bigl[\nabla_\th \log\frac{q_\th(\xv)}{\a p(\xv)+(1-\a)q_\th(\xv)}\Bigr]
%
%
+\E_{q(\zv)}\Bigl[\nabla_\th\gv_\th(\zv) (\sv_{\th;0}(\xv)-\sv_{\th;\a}(\xv))\Big|_{\xv=\gv_\th(\zv)}\Bigr],\\
\nabla_\th \kl(p \| \a p + (1 - \a) q_\th) &= -\E_{p(\xv)}\left[\nabla_\th \log (\a p(\xv) + (1-\a)q_\th(\xv))\right].
\end{align*}
Here, note that in the first expression, we invoke the chain rule: for some function $f_\th\suchthat \Xc\to\Real$, we have
\[
\nabla_\th f_\th(\gv_\th(\zv))
= (\nabla_\th f_\th(\xv))|_{\xv=\gv_\th(\zv)} + \nabla_\th \gv_\th(\zv)(\nabla_\xv f_\th(\xv))|_{\xv=\gv_\th(\zv)}.
\]
Combining these two terms with the weights, we get the gradient of the $\a$-skew JSD:
\begin{align*}
\nabla_\th\sjsd^{(\a)}(q_\th,p) 
&= \frac{1}{\a}\nabla_\th \kl(q_\th \| \a p + (1 - \a) q_\th) + \frac{1}{1-\a} \nabla_\th \kl(p \| \a p + (1 - \a) q_\th)\\
&= \frac{1}{\a}\E_{q(\zv)}\Bigl[\nabla_\th\gv_\th(\zv) (\sv_{\th;0}(\xv)-\sv_{\th;\a}(\xv))\Big|_{\xv=\gv_\th(\zv)}\Bigr]
\\&\qquad 
-\frac{1}{\a(1-\a)}\E_{\a p(\xv)+(1-\a)q_\th(\xv)}[\nabla_\th\log(\a p(\xv)+(1-\a)q_\th(\xv))]
\\&\qquad 
+\frac{1}{\a}\E_{q_\th(\xv)}[\nabla_\th\log q_\th(\xv)]\\
&= \frac{1}{\a}\E_{q(\zv)}\Bigl[\nabla_\th\gv_\th(\zv) (\sv_{\th;0}(\xv)-\sv_{\th;\a}(\xv))\Big|_{\xv=\gv_\th(\zv)}\Bigr].
\end{align*}
Here, we use the assumption that $\E_{q_\th(\xv)}[\nabla_\th \log q_\th(\xv)]= 0$.
\end{proof}


\subsection{Proof of Proposition~\ref{prop:interpolated_score}}

\begin{proof}[Proof of Proposition~\ref{prop:interpolated_score}]
We can write the objective $\Lc(\psi; \a)$ as 
\begin{align*}
\Lc(\psi; \a)
&= \int \Big\{(\a p(\xv)+(1-\a)q_\th(\xv))\|\sv_{\psi}(\xv;\a)\|^2 -2(\a p(\xv) s_p(\xv)+(1-\a)q_\th(\xv))^\intercal \sv_{\psi}(\xv;\a)\Bigr\} \diff\xv + C\\
&= \int (\a p(\xv)+(1-\a)q_\th(\xv))\Bigl\|\sv_{\psi}(\xv;\a)- \frac{\a p(\xv)\sv_p(\xv) + (1 - \a) q_\th(\xv)\sv_{q_\th}(\xv)}{\a p(\xv) + (1 - \a)q_\th(\xv)}\Bigr\|^2\diff \xv + C'.
\end{align*}
    %
    %
    %
    %
    %
    Hence, it is clear that the global minimizer should be
    \begin{align*}
        \sv_{\psi^*}(\xv; \a) &= \frac{\a p(\xv)\sv_p(\xv) + (1 - \a) q_\th(\xv)\sv_{q_\th}(\xv)}{\a p(\xv) + (1 - \a)q_\th(\xv)}\\
            &= \frac{\a \nabla_{\xv} p(\xv) + (1 - \a) \nabla_{\xv}q_\th(\xv)}{\a p(\xv) + (1 - \a)q_\th(\xv)} \\
            &= \frac{\nabla_{\xv}(\a p(\xv) + (1 - \a)q_\th(\xv))}{\a p(\xv) + (1 - \a)q_\th(\xv)}\\
            &= \nabla_{\xv} \log (\a p(\xv) + (1-\a)q_\th(\xv)).\qedhere
    \end{align*}
\end{proof}

\subsection{Deferred Statements}

\begin{proposition}
\label{prop:interpolated_score_dsm}
Let $\a \in [0, 1]$ be fixed and $\sigma_t$ be some fixed noise level. 
Then, the minimizer of the objective function
\begin{align}
\Lc_\text{score}&(\psi; \a, t) 
\defeq \a\,\E_{p(\xv)q(\epsv)}[\|\sv_{\psi}(\xv; \a, t) + \epsv/\sigma_t\|^2] 
%
%
+ (1 - \a)\,\E_{q_\th(\xv)q(\epsv)}[\|\sv_{\psi}(\xv; \a, t) + \epsv/\sigma_t\|^2]
\end{align}
satisfies 
\begin{equation*}
\sv_{\psi^*}(\xv; \a, t) = \sv_{\th;\a, t}(\xv). 
%
\end{equation*}
\end{proposition}

\begin{proof}[Proof of Proposition~\ref{prop:interpolated_score_dsm}]
We can write the objective $\Lc(\psi; \a, t)$ as 
\begin{align*}
\Lc_\text{score}(\psi; \a, t)
&= \iint (\a p(\xv_t)+(1-\a)q_\th(\xv_t))\Bigl\|\sv_{\psi}(\xv_t; \a, t) + \frac{\epsv}{\sigma_t}\Bigr\|^2\diff \xv \diff \epsv.
\end{align*}
This is a standard minimum mean square estimation (MMSE) problem for which the global minimizer is the conditional mean,
\begin{align*}
    \sv_{\psi^*}(\xv_t; \a, t) &= -\frac{1}{\sigma_t}\E_{\a p_t + (1 - \a)q_{\th, t}}\left[\epsv| \xv_t\right] \\
    &= -\frac{1}{\sigma_t^2}\E_{\a p_t + (1 - \a)q_{\th, t}}\left[\xv_t - \xv| \xv_t\right] \\
    &= -\frac{1}{\sigma_t^2}\xv_t + \frac{1}{\sigma_t^2}\E_{\a p_t + (1 - \a)q_{\th, t}}\left[\xv| \xv_t\right] \\
    &= \nabla_{\xv_t} \log (\a p(\xv_t) + (1-\a)q_\th(\xv_t)).
\end{align*}
Here we use that $\xv_t = \xv + \sigma_t \epsv$ and make the connection to the marginal score in the last line using Tweedie's formula~\citep{Robbins1956AnEB}.
\end{proof}

\begin{proposition}
    \label{prop:alternate_parametrization}
    Let $\a \in [0, 1]$, $\sv_p(\xv)$ be the data score, $\sv_{q_\th}(\xv)$ be the score of the generated samples.  Then, the score of the mixture distribution can be expressed as
    \begin{equation}
    \sv_{\th;\a}(\xv) 
    = D_{\th;\a}(\xv)\sv_p(\xv) + (1-D_{\th;\a}(\xv))\sv_{q_\th}(\xv),
    \end{equation}
    where
    \begin{equation}
    D_{\th;\a}(\xv)\defeq \sigma\Bigl(\log\frac{p(\xv)}{q_\th(\xv)}+\log\frac{\a}{1-\a}\Bigr),
    \end{equation}
\end{proposition}

\begin{proof}[Proof of Proposition~\ref{prop:alternate_parametrization}]
The amortized score can be expressed as
\begin{align*}
    \nabla_\xv \log (\a p(\xv) + (1 - \a) q_\th(\xv)) &= \frac{\nabla_\xv (\a p(\xv) + (1 - \a) q_\th(\xv))}{\a p(\xv) + (1 - \a) q_\th(\xv)} \\
    &= \frac{\a p(\xv)}{\a p(\xv) + (1 - \a) q_\th(\xv)} \nabla_\xv \log p(\xv) +  \frac{(1 - \a)q_\th(\xv)}{\a p(\xv) + (1 - \a) q_\th(\xv)} \nabla_\xv \log q_\th(\xv) \\
    &= D(\xv; \a) \nabla_\xv \log p(\xv) + (1 - D(\xv; \a))\nabla_\xv \log q_\th(\xv).
\end{align*}
We can now simplify the scaling factor as
\begin{align*}
    D(\xv; \a) &= \frac{\a p(\xv)}{\a p(\xv) + (1 - \a) q_\th(\xv)} 
    %
    %
    %
    %
    = \sigma\left(\log \frac{p(\xv)}{q_\th(\xv)} + \log \frac{\a}{1 - \a}\right).\qedhere
\end{align*}    
\end{proof}

%
\section{Detailed Discussions on Related Work}
\label{sec:related_work}

\subsection{Diffusion Models}
\label{app: diffusion background}

\textbf{Prior Work.} \citet{Sohl-Dickstein--Weiss--Mehswaranathan--Ganguli} first introduced diffusion probabilistic models (DPMs) as deep variational autoencoders \cite{kingma2013auto} based on the principles of thermodynamic diffusion with a Markov-chain variational posterior that maximizes the evidence lower bound (ELBO).  Several years later, \citet{Ho--Jain--Abbeel2020} re-introduced DPMs (DDPMs) with modern neural network architectures and a simplified loss function that set a new state-of-the-art in image generation. Since then, numerous connections to existing literature in statistics, information theory and stochastic differential equations (SDEs) have helped bolster the quality of these models. For example, \citet{Song--Ermon2019} illustrate the equivalence between DDPMs and DSM at multiple noise levels, thus bridging the areas of diffusion-based models and score-based models.  Subsequently, \citet{Song--Sohl-Dickstein--Kingma--Kumar--Ermano--Poole2020} showed that in continuous time, DPMs can be appropriately interpreted as solving for the reverse of a noising process that evolves as an SDE while \citet{Kingma--Salimans--Poole--Ho2021} demonstrated that continuous-time DPMs can interpreted as VAEs and that the variational lower bound is invariant to the noise schedule except for its endpoints, thus bolstering its density estimation capabilities.  Following the latter discovery, \citet{Kong--Brekelmans--Steeg2023} show that DPMs can in-fact be used for \emph{exact} likelihood computation by leveraging techniques from information theory. To further improve DPMs, extensive research has gone into the choice of noise schedules, network architectures and loss functions \cite{Nichol--Dhariwal2021, Hoogeboom--Heek--Salimans2023, Karras--Aittala--Aila--Laine2022, Kingma--Gao2024}. Many tangentially discovered frameworks such as rectified flows \cite{Liu--Gong--Liu2023} and conditional normalizing flows trained with Gaussian conditional flow matching \cite{Lipman--Chen--Ben-Hamu--Nickel--Le2022}, are also particular instances of (Gaussian) diffusion models with specialized noise schedules and weighted loss functions, as show in \cite{Kingma--Gao2024}. 

\textbf{Formulation.} We take the following unified view in our definition of DPMs as inspired by \cite{Kingma--Gao2024} and \cite{Karras--Aittala--Aila--Laine2022}. Let $p(\xv)$ be the data distribution and let $\lambda(t)$ define a \textit{variance exploding} noise schedule with distribution $p(t)$ where $t  \sim \Uc(0, 1)$.  Under this noise schedule we can define a noisy version of $\xv$ at noise level $\sigma_t$ as
\begin{equation}
    \xv_{t} \defeq \xv + \sigma_{t} \epsv \quad \text{where} \quad \epsv \sim \Nc(0, \mathbf{I}).
    \label{eq:forward process}
\end{equation}
 Given noisy samples of data, the diffusion objective can be reduced to a weighted denoising objective,
\begin{equation}
    \Lc_{\dpm}(\epsv_{\th}) = \half \E_{p(t)p(\xv)q(\epsv)} \left[w(t)\|\epsv - \epsv_{\th}(\xv_t; t)\|^2\right],
    \label{eq:dpm}
\end{equation}
where $w(t)$ is a positive scalar-valued weighting function. Note that for the forward process defined in Eq.~\eqref{eq:forward process}, the conditional score is $\sv(\xv_t|\xv) = -\epsv / \sigma_t$. Thus, Eq.~\eqref{eq:dpm} can be interpreted as a weighted denoising score matching loss \cite{vincent2011connection} over multiple noise levels,
\begin{equation}
    \Lc_{\dpm}(\epsv_{\th}) = \half \E_{p(t)p(\xv)p(\xv_t|\xv)} \left[w'(t)\bigg\|\sv(\xv_t|\xv) + \frac{\epsv_{\th}(\xv_t; t)}{\sigma_t}\bigg\|^2\right],
    \label{eq:weighted-dsm}
\end{equation}
where $w'(t) \defeq \sigma_t^2 w(t)$ and the marginal score estimator is $\sv_{\th}(\xv_t; t) \defeq -\epsv_{\th}(\xv_t; t) / \sigma_t$.

\textbf{Sampling.} It is often beneficial to view DPMs as SDEs \cite{Song--Sohl-Dickstein--Kingma--Kumar--Ermano--Poole2020} where the forward process can be expressed as
\begin{equation*}
    \text{d}\xv_t = \fv(\xv_t, t)\text{d}t + g(t)\text{d}\wv,
\end{equation*}
where $\wv$ is a standard Wiener process and $\xv_0 = \xv$.  The time reversal of this process (i.e., the generative process) is known to follow the reverse SDE,
\begin{equation*}
    \text{d}\xv_t = \left[\fv(\xv_t, t) - g^2(t)\nabla_{\xv_t}\log p(\xv_t)\right]\text{d}t + g(t)\text{d}\bar{\wv}.
\end{equation*}
Note that in practice $\nabla_{\xv_t}\log p(\xv_t)$ would be estimated by the score function $\sv_{\th}(\xv_t; t)$ from a variant of DSM as in Eq.~\eqref{eq:weighted-dsm}.

Sampling can be simulated through techniques such as annealed Langevin dynamics or ancestral sampling \cite{Song--Sohl-Dickstein--Kingma--Kumar--Ermano--Poole2020}. While the above reverse SDE is stochastic in nature, there also exists a deterministic process known as the \emph{probability flow ODE} that satisfies the same intermediate marginal distributions,
\begin{equation}
    \text{d}\xv_t = \left[\fv(\xv_t, t) - \half g^2(t)\nabla_{\xv_t}\log p(\xv_t)\right]\text{d}t.
    \label{eq: probability flow ode}
\end{equation}
The benefit of the ODE formulation is that it can discretized more coarsely and hence sampling can done in fewer timesteps. Furthermore, sampling is possible by plugging in the updates from Eq.~\eqref{eq: probability flow ode} into black-box ODE solvers, e.g., the Heun 2\textsuperscript{nd} order solver \cite{Karras--Aittala--Aila--Laine2022}. Sampling can be sped even further if Eq.~\eqref{eq: probability flow ode} can be solved exactly. \citet{Lu--Zhou--Bao--Chen--Li--Zhu2022} show that the exact solution to Eq.~\eqref{eq: probability flow ode} at timestep $t$ given an initial value at timestep $s < t$ is,
\begin{equation}
    \xv_t = \xv_s +  2\int_{\sigma_s}^{\sigma_t} \sigma_u \epsv_{\th}\left(\xv_u; u\right)\text{d}\sigma_u.
    \label{eq:pf-ode}
\end{equation}
Various samplers can be derived by approximating the exponentially weighted integral in different ways. 
 For example, the widely used DDIM sampler \cite{Song--Meng--Ermon2021} is an example of a first-order Taylor expansion of the integral term. At the core of all these algorithms is a score estimator/denoiser, which if learned accurately could improve the quality of samples produced.

\textbf{EDM Diffusion Architecture.} The  EDM preconditioning diffusion model utilizes a base DDPM++ architecture from \cite{Song--Sohl-Dickstein--Kingma--Kumar--Ermano--Poole2020} for CIFAR-10 and the ADM architecture \cite{Nichol--Dhariwal2021} for ImageNet $64\times64$. The EDM model uses a noise schedule that is defined as
\begin{equation}
    \log \sigma_t \sim \Nc(-1.2, 1.2^2).
    \label{eq:edm definitions}
\end{equation}
Rather than regressing against the unscaled additive noise as in DSM, EDM regresses against the original sample expressed in the following form,
\begin{equation}
    \xv = \frac{\sigma_{\text{data}}^2}{\sigma_t^2 + \sigma_{\text{data}}^2} \xv_t + \frac{\sigma_t\cdot \sigma_{\text{data}}}{\sqrt{\sigma_t^2 + \sigma_{\text{data}}^2}} \gv,
    \label{eq:edm target}
\end{equation}
where $ \sigma_{\text{data}} = 0.5$. To this end, EDM is parametrized with a denoising neural network,
\begin{equation}
    \fv_\th(\xv_t; t) = \frac{\sigma_{\text{data}}^2}{\sigma_t^2 + \sigma_{\text{data}}^2} \xv_t + \frac{\sigma_t\cdot \sigma_{\text{data}}}{\sqrt{\sigma_t^2 + \sigma_{\text{data}}^2}}\gv_\th(\xv_t; t),
\end{equation}
which is trained by minimizing
\begin{equation*}
    \min_{\th} \E_{p(\xv)q(\epsv)p(t)} [\tilde{w}(t)\|\xv - \fv_\th(\xv_t; t)\|^2],
\end{equation*}
where 
\begin{equation}
    w_{\text{EDM}}(t) = \frac{\sigma_t\sigma_{\text{data}}}{\sqrt{\sigma_t^2 + \sigma_{\text{data}}^2}}.
    \label{eq:edm weighting}
\end{equation}
This is equivalent to estimating $\gv$ by minimizing the objective,
\begin{equation}
    \Lc_{\text{EDM}}(\gv_{\theta}) \defeq \E_{p(\xv)q(\epsv)p(t)}\left[\|\gv - \gv_{\theta}\left(\xv_t; t\right)\|^2\right].
    \label{eq:edm objective}
\end{equation}
Using Eq.~\eqref{eq:edm definitions} and Eq.~\eqref{eq:edm target} we can show that,
\begin{align}
    \gv &= \frac{\sqrt{\sigma_t^2 + \sigma^2_{\text{data}}}}{\sigma_t\sigma_{\text{data}}} \xv - \frac{\sigma_{\text{data}}}{\sigma_t\sqrt{\sigma_t^2 + \sigma^2_{\text{data}}}} \xv_t\\
        &=-\frac{\sqrt{\sigma_t^2 + \sigma^2_{\text{data}}}}{\sigma_{\text{data}}} \epsv + \frac{\sigma_t}{\sqrt{\sigma_t^2 + \sigma^2_{\text{data}}}\sigma_{\text{data}}} \xv_t.
\end{align}
Therefore, in terms of Eq.~\eqref{eq:dpm} the EDM objective boils down to the unified diffusion objective with weighting function,
\begin{equation}
    w(t) =\frac{\sigma_t^2 + \sigma_{\text{data}}^2}{\sigma_{\text{data}}^2}.
    \label{eq:edm weights}
\end{equation}

\subsection{Diffusion Distillation}
\label{sec:diffusion_distillation_background}

Achieving state-of-the-art generation results on CIFAR-10 and ImageNet $64\times64$ using a Heun 2$^\text{nd}$ order sampler with the EDM architecture requires 35 and 512 function evaluations (FEs) respectively.  The goal of diffusion distillation is to distill a teacher model into a student model that can achieve high quality signal generation with few FEs.

The earliest works on distillation such as progressive distillation \cite{salimans2022progressive} and knowledge distillation \cite{huang2023knowledge} train a student diffusion model with drastically reduced sampling budget to match the performance of a teacher model that is simulated in reverse.  For example, given a teacher diffusion model parametrized as a denoiser $\fv_{\phi}$ and a noisy sample $\xv_t$, a ``clean'' target $\xv_\phi^{(k)}$ is constructed by running the teacher model for $k$ steps in reverse.  The student denoiser $\fv_{\th}$ is then optimized by minimizing the loss,
\begin{equation*}
    \Lc(\phi) \defeq \E_{p(\xv)p(\zv)p(t)}[w(t)\|\fv_{\th}(\xv_t; t) - \xv_\phi^{(k)}\|^2].
\end{equation*}
Knowledge distillation on the other hand conditions the student model on intermediate features from the teacher diffusion model so as to regularize the learned weights more effectively and retain knowledge from the teacher model.  These methods are expensive as it requires either simulating multiple steps of a teacher diffusion model or additionally probing it for feature extraction.

More recently a class of new diffusion distillation techniques grounded in reverse KL divergence minimization have gained popularity as discussed in Sec.~\ref{sec:preliminaries}. DiffInstruct \cite{luo2024diff}, DMD \cite{yin2024onestep} and DMD2 \cite{yin2024improved} all train a one-step generator $\gv_\th$ mapping noise $\zv \sim \Nc(0, \mathbf{I})$ to generated samples by updating the generator in the direction of minimizing the reverse KLD,
\begin{align*}
    \nabla_\th \text{D}^{\text{avg}}_{\text{KL}}(q_\th \| p) = 
    \E_{ q(\zv)p(t)q(\epsv)}[\nabla_\th \gv_\th(\zv)(\sv_{q_\th}(\xv_t) - \sv_p(\xv_t)) \mid_{\xv = \gv_\th(\zv)}],
\end{align*}
where $\sv_p(\xv_t) = \nabla_{\xv_t} \log p(\xv_t)$ and $\sv_{q_\th}(\xv_t) = \nabla_{\xv_t} \log q_\th(\xv_t)$. Assuming that the score model was learned using a parametrization similar to EDM, DMD scales the gradient and uses Tweedie's formula \cite{Robbins1956AnEB} to express it in terms of a pretrained denoiser $\fv_\phi$ and a denoiser for the fake samples $\fv_{\psi}$,
\begin{align*}
    \nabla_\th \Lc_\text{DMD}(\th) = 
    \E_{ q(\zv)p(t)q(\epsv)}[ w_{\text{DMD}}(\xv_t, \xv, t)\nabla_\th \gv_\th(\zv)(\fv_{\psi}(\xv_t; t) - \fv_\phi(\xv_t; t)) \mid_{\xv = \gv_\th(\zv)}],
\end{align*}
where an adaptive weight is used to ensure that the scale of the gradient is roughly uniform across noise levels,
\begin{equation}
    w_{\text{DMD}}(\xv_t, \xv, t) \defeq \frac{\sigma^2_t}{\|\xv - \fv_\phi(\xv_t; t)\|_1}.
    \label{eq:dmd_weighting}
\end{equation}
To mitigate mode collapse and enhance sample diversity, DMD employs an ODE-based regularizer by simulating the pretrained diffusion model in reverse. This process generates noise-image pairs, which are then used to further supervise the generator's training. However, collecting this dataset becomes prohibitively expensive for high-dimensional samples. To address this limitation, DMD2 introduces a GAN-based regularizer, which effectively minimizes the Jensen-Shannon divergence alongside the reverse KLD, or a variant of the forward KLD when implemented in a non-saturating manner. For further details on GAN training, refer to Appendix \ref{sec:appendix_on_gan_training}.

Several methods build upon the divergence minimization framework by introducing regularizers based on alternative statistical distance measures. For instance, Moment Matching Distillation (MMD) \cite{salimans2024multistep}, Score Identity Distillation (SiD) \cite{zhou2024score}, and Score Implicit Matching (SiM) \cite{luoone} align the fake score model with the pretrained score model using a variant of the Fisher divergence:
\begin{equation*}
    \Lc_{\text{Fisher}}(\psi) \defeq \E_{q_\th(\xv)p(t)q(\epsv)}[w'(t)\|\fv_\psi(\xv_t; t) - {\sf sg}[\fv_\phi(\xv_t; t)]\|^2].
\end{equation*}
Here ${\sf sg}$ stands for the stop gradient operator.  Additionally, both SiD and SiM extend this approach to generator training by minimizing the Fisher divergence, which requires a computationally expensive gradient calculation through the entire score model. To address this, they employ statistical approximations to make these gradient computations more practical.

\subsection{Consistency Models}

Consistency models are a new class of generative models introduced by \citet{song2023consistency} that learn a consistency function between all points along the trajectory of the probability flow ODE of a reverse diffusion sampler. Concisely, given points along one such trajectory, $\xv_t, t \in [\eps, 1]$, where $\xv_1 \sim \Nc(0, \mathbf{I})$, the consistency function satisfies,
\[\fv(\xv_t, t) = 
\begin{cases}
    \xv & \text{if } t = \eps\\
    \fv(\xv_s, s) & s \in [\eps, 1]
\end{cases}
\]
Given the boundary condition at the origin, the consistency function can be parametrized using a neural network similar to EDM ,
\begin{equation*}
    \fv_\th(\xv_t, t) = \frac{\sigma_{\text{data}}^2}{(\sigma_t -\sigma_\eps)^2 + \sigma_{\text{data}}^2} \xv_t + \frac{(\sigma_t - \sigma_\eps)\cdot \sigma_{\text{data}}}{\sqrt{\sigma_t^2 + \sigma_{\text{data}}^2}}\gv_\th(\xv_t; t).
\end{equation*}
Given a noisy sample $\xv_t = \xv + \sigma_t \epsv, \epsv \sim \Nc(0, \mathbf{I})$, first a single step of the probability flow ODE is simulated using the Euler sampler by running one step of sampling using Eq.~\eqref{eq:pf-ode},
\begin{equation*}
    \xv_s = \xv_t + (t - s)t \nabla_{\xv_t} \log p(\xv_t)
\end{equation*}
This can be computed using either a pretrained score model or via a single sample Monte-Carlo estimate.  In the latter setting, it is important that the timesteps $s$ and $t$ are very close to each other for the approximation to hold. In consistency distillation a pretrained score model $\sv_\phi$ is available and a single sampling step along the PF-ODE is simulated as
\begin{equation*}
    \xv^\phi_s = \xv_t + (t - s)t \sv_\phi(\xv_t; t).
\end{equation*}
Then the consistency function is learned by minimizing
\begin{equation*}
    \Lc_{\text{CD}}(\th) = \E_{p(\xv)q(\epsv)p(t)}[w(t)d(\fv_\th(\xv_t; t), {\sf sg}[\fv_\th(\xv^\phi_{t-\Delta t}; t - \Delta t)])],
\end{equation*}
where $d$ is some distance measure, $w(t)$ is some positive weighting function and $s = t - \Delta t$, with $\Delta t$ some fixed timestep difference. \citet{song2023consistency} initially proposed using the LPIPS distance but subsequent works \cite{song2024improved, Geng--Pokle--Luo--Lin--Kolter2024} have shown that similar performance can be achieved by using the $\ell_2$ distance or a pseudo-Huber norm.

Unlike distillation techniques, consistency models can also be trained from scratch.  Assume that $s = t - \delta t, \delta t \rightarrow 0$.  Then, the sampling step can be approximated using Tweedie's formula \cite{Robbins1956AnEB},
\begin{align*}
        \xv_s &\approx \xv_t + (t - s) \frac{\xv - \xv_t}{t} \\
        &= \xv + s \epsv.
\end{align*}
Thus, the consistency function can now be learned by minimizing,
\begin{equation*}
    \Lc_{\text{CT}}(\th) = \E_{p(\xv)q(\epsv)p(t)}[w(t)d(\fv_\th(\xv + t \epsv; t), {\sf sg}[\fv_\th(\xv + (t - \delta t) \epsv; t - \delta t)])],
\end{equation*}
Consistency distillation still lags behind distillation methods based on reverse KL minimization, but consistency training often demonstrates more impressive results.  However, consistency training is still inherently unstable and requires careful design of both the noise schedule due to limiting nature of $\delta t$ and distance measure \cite{song2024improved, Geng--Pokle--Luo--Lin--Kolter2024}.  Stabilizing and making this objective simpler is the focus of a lot of current research in the area.

%
\section{Detailed Description of Score-of-Mixture Training and Distillation}
\label{sec:appendix_amortized_score_training}

\subsection{Amortized Denoiser}
\label{sec:appendix_amortized_denoiser}

Modern diffusion architectures such as the EDM architecture \cite{karras2022elucidating} are specially designed for denoising purposes (see Appendix \ref{sec:related_work}).  Hence, in practice we choose to train an \textit{amortized denoiser}, $\fv_\psi(\xv_t; \a, t) \approx \E_{\a p_t + (1 - \a)q_{\th, t}}[\xv | \xv_t]$,
upon which the amortized score can be recovered using Tweedie's formula \cite{Robbins1956AnEB},
\begin{equation*}
    \sv_\psi(\xv_t; \a, t) = -\frac{1}{\sigma_t^2}\xv_t + \frac{1}{\sigma_t^2}\fv_\psi(\xv_t; \a, t).
\end{equation*}
The mixture score matching loss in Eq.~\eqref{eq:mixture_score_matching_imp} can be expressed with this denoiser as
\begin{align*}
    \Lc^{\text{denoise}}_{\text{gen}}&(\psi; \a, t) 
\defeq \a\,\E_{p(\xv)q(\epsv)}[\|\fv_{\psi}(\xv; \a, t) - \xv\|^2] + (1 - \a)\,\E_{q_\th(\xv)q(\epsv)}[\|\fv_{\psi}(\xv; \a, t) - \xv\|^2].
\end{align*}
%
%
%
%

%

\subsection{Score-of-Mixture Training}
Here, we present a pseudocode for Score-of-Mixture Training (SMT).
See Algorithm~\ref{alg:smt}.
\begin{algorithm}[!hbt]
    \caption{Score-of-Mixture Training}
    \label{alg:smt}
    \begin{algorithmic}
        \State \textbf{Inputs:} Randomly initialized generator $\gv_\th$, amortized score model $\sv_\psi$, discriminator $\ell_\psi$, real dataset $\mathcal{D}$, score training sub-iterations $= 5$, learning rates $(\eta_\text{gen}, \eta_\text{score})$, GAN regularizer weights $(\text{score}=\mu, \text{gen}=\lambda)$
        
        \vspace{0.5em}
        \State \textbf{Pretraining:} Train $\gv_\th$ with DSM using $\mathcal{D}$
        \For{each pretraining iteration}
            \State Sample mini-batch $\xv \sim \mathcal{D}$ and add noise $\xv_t = \xv + \sigma_t \epsv, \epsv \sim \Nc(0, \mathbf{I})$
            \State Compute DSM loss $\Lc_\text{DSM}(\th)$ (see Sec.~\ref{sec:preliminaries})
            \State Update parameters: $\theta \gets \theta - \eta_\text{DSM} \nabla_\th \Lc_{\text{DSM}}(\th)$
        \EndFor
        
        \vspace{0.5em}
        \State \textbf{Training:} Alternating updates of $\gv_\th$ and $\sv_\psi$
        \For{each training iteration}
        
            \vspace{0.5em}
            \State \textbf{Generator Training:} Freeze $\sv_\psi$
            \State Sample mini-batch of fake samples $\xv^{\sf fake} = \gv_\th(\zv), \zv \sim \Nc(0, \mathbf{I})$
            \State Sample $t \sim p(t)$ and $\a$ as described in Sec.~\ref{sec:training_from_scratch_training}
            \State Compute weighted generator gradient $\mathbf{\gamma}^w_\psi(\th; \a, t)$ from Eq.~\eqref{eq:weighted gradient}
            \State Compute GAN regularizer loss $\Lc_\text{GAN}^{(\a, t)}$ from Eq.~\eqref{eq:non_sat_amortized_gan_loss}
            \State Update parameters:
            \[
            \theta \gets \theta - \eta_\text{gen} \E_{p(\a)p(t)}[w(\xv^{\sf fake}_t, \xv^{\sf fake}, \a, t)\mathbf{\gamma}^w_\psi(\th; \a, t)+ \lambda \nabla_\th\Lc_\text{GAN}^{(\a, t)}(\th)]
            \]
            
            \vspace{0.5em}
            \State \textbf{Amortized Score Training:} Freeze $\gv_\th$
            \For{each sub-iteration}
                \State Sample mini-batch of real samples $\xv^{\sf real} \sim \mathcal{D}$
                \State Sample $t \sim p(t)$ and $\a$
                \State Compute score matching loss $\Lc_{\text{score}}(\psi; \a, t)$ from Eq.~\eqref{eq:mixture_score_matching_imp}
                \State Compute non-saturated discriminator loss $\Lc_{\text{disc}}(\psi)$ (see Appendix~\ref{sec:appendix_on_gan_training})
                \State Update parameters:
                \[
                \psi \gets \psi - \eta_\text{score}\nabla_\psi (\E_{p(\a)p(t)}[\Lc_{\text{score}}(\psi; \a, t)] + \mu \Lc_{\text{disc}}(\psi))
                \]
            \EndFor
        \EndFor
        
        \vspace{0.5em}
        \State \textbf{Return:} Trained model parameters $\theta, \psi$
        
    \end{algorithmic}
\end{algorithm}

\newpage
\subsection{Score-of-Mixture Distillation}
We present an overview figure and pseudocode of Score-of-Mixture Distillation in Fig.~\ref{fig:smt_distillation} and Alg.~\ref{alg:smt distillation}.
We highlight the central differences in the distillation training from the training from scratch in  Alg.~\ref{alg:smt}.


\begin{itemize}
    \item The pretrained score model on the real data is available during distillation.  Thus, rather than defining an amortized score model that takes in a conditioning variable for $\a$, we show that we only need to learn a score model on the fake samples to make the objective simpler (see Proposition \ref{prop:alternate_parametrization}).  This is more effective as learning the score of multiple interpolated distributions is a generally more challenging task.
    \item We can initialize the fake score and generator with the weights from the pretrained model and don't need to run a separate pretraining phase as in training from scratch.
    \item Notice that in amortized score training in the distillation setting there is no explicit GAN loss to learn the discriminator in Algorithm \ref{alg:smt distillation}.  Learning the discriminator implicit to our proposed parametrization and once learned we can use this to minimize the skewed divergence via a non-saturating GAN regularizer for generator training.
\end{itemize}


\begin{figure}[!tbh]
    \centering
    \includegraphics[width=.75\linewidth]{figs/alpha_jsd_dist.pdf}
    \caption{Overview of Score-of-Mixture Distillation. \textbf{Top:} To update the generator weights, the fake image is diffused at noise level $t$ and then used to compute the gradient of the $\a$-skew divergence with the explicitly parametrized amortized score model using Eq.~\eqref{eq:explicit grad}. \textbf{Bottom:} Amortized score model training involves computing the score of the mixture distribution on both fake and real samples diffused with noise level $t$ and then updating the weights using the gradient of Eq.~\eqref{eq:mixture_score_matching_imp_alternative}.}
    \label{fig:smt_distillation}
\end{figure}


\newpage
\begin{algorithm}[!hbt]
    \caption{Score-of-Mixture Distillation}
    \label{alg:smt distillation}
    \begin{algorithmic}
        \State \textbf{Inputs:} Randomly initialized generator $\gv_\th$, fake score model $\sv^{\sf fake}_\psi$, discriminator $\ell_\psi$, pretrained score model $\sv_p$, real dataset $\mathcal{D}$, score training sub-iterations $= 5$, learning rates $(\eta_\text{gen}, \eta_\text{score})$, GAN generator regularizer weight $\lambda$
        
        \vspace{0.5em}
        \State \textbf{Initialization} Initialize $\sv_\psi^{\sf fake}$ and $\gv_\th$ with weights from $\sv_p$
        
        \vspace{0.5em}
        \State \textbf{Training:} Alternating updates of $\gv_\th$ and $\sv_\psi$
        \For{each training iteration}
        
            \vspace{0.5em}
            \State \textbf{Generator Training:} Freeze $\sv^{\sf fake}_\psi$ and $\ell_\psi$
            \State Sample mini-batch of fake samples $\xv^{\sf fake} = \gv_\th(\zv), \zv \sim \Nc(0, \mathbf{I})$
            \State Sample $t \sim p(t)$ and $\a$ as described in Sec.~\ref{sec:training_from_scratch_training}
            \State Compute generator gradient $\mathbf{\gamma}^{\sf ext}_\psi(\th; \a, t)$ from Eq.~\eqref{eq:explicit grad}
            \State Compute GAN regularizer loss $\Lc_\text{GAN}^{(\a, t)}$ from Eq.~\eqref{eq:non_sat_amortized_gan_loss}
            \State Update parameters:
            \[
            \theta \gets \theta - \eta_\text{gen} \E_{p(\a)p(t)}[\mathbf{\gamma}^{\sf ext}_\psi(\th; \a, t)+ \lambda \nabla_\th\Lc_\text{GAN}^{(\a, t)}(\th)]
            \]
            
            \vspace{0.5em}
            \State \textbf{Amortized Score Training:} Freeze $\gv_\th$
            \For{each sub-iteration}
                \State Sample mini-batch of real samples $\xv^{\sf real} \sim \mathcal{D}$
                \State Sample $t \sim p(t)$ and $\a$
                \State Compute score matching loss using explicit parametrization $\Lc^{\sf ext}_{\text{score}}(\psi; \a, t)$ from Eq.~\eqref{eq:mixture_score_matching_imp_alternative}
                \State Update parameters:
                \[
                \psi \gets \psi - \eta_\text{score}\nabla_\psi \E_{p(\a)p(t)}[\Lc_{\text{score}}^{\sf ext}(\psi; \a, t)] 
                \]
            \EndFor
        \EndFor
        
        \vspace{0.5em}
        \State \textbf{Return:} Trained model parameters $\theta, \psi$
        
    \end{algorithmic}
\end{algorithm}


%
\section{On GAN Training}
\label{sec:appendix_on_gan_training}

The vanilla GAN is
\[
\min_\th \max_\psi
\{\E_{p(\xv)}[\log D_\psi(\xv)] + \E_{q_\th(\xv)}[\log (1-D_\psi(\xv))]\}.
\]
Breaking down, the discriminator training is 
\[
\min_\psi
-\E_{p(\xv)}[\log D_\psi(\xv)] - \E_{q_\th(\xv)}[\log (1-D_\psi(\xv))]\},
\]
and the generator training is
\[
\min_\th \E_{q_\th(\xv)}[\log (1-D_\psi(\xv))].
\]
Note that
\[
D_\psi(\xv) = \frac{r_\psi(\xv)}{1+r_\psi(\xv)}
= \frac{1}{1+r_\psi^{-1}(\xv)}.
\]
Further, the sigmoid logit $C_\psi(\xv)$, \ie $D_\psi(\xv)=\sigma(C_\psi(\xv))$, is $\log r_\psi(\xv)$.
Note that
the optimal discriminator for each $\th$ is $D^\star(\xv)=\frac{p(\xv)}{p(\xv)+q_\th(\xv)}$ or $r^\star(\xv)=\frac{p(\xv)}{q_\th(\xv)}$.

The non-saturating versions is
training generator based on
\[
\min_\th -\E_{q_\th(\xv)}[\log D_\psi(\xv))]
\approx \E_{q_\th(\xv)}\Bigl[\log\Bigl(\frac{q_\th(\xv)}{p(\xv)}+1\Bigr)\Bigr].
\]

The StyleGAN uses the non-saturating loss:
\begin{align*}
\min_\psi &~ \E_{p(\xv)}[\softplus(-\log r_\psi(\xv))] + \E_{q_\th(\xv)}[\softplus(\log r_\psi(\xv))]\},\\
\min_\th &~ \E_{q_\th(\xv)}[\softplus(-\log r_\psi(\xv))].
\end{align*}
Note that $\softplus(y)\defeq \log(1+e^y)$.
Hence, note that
\begin{align*}
\softplus(-\log r_\psi(\xv))
&= \log(1+r_\psi^{-1}(\xv))
= -\log D_\psi(\xv),\\
\softplus(\log r_\psi(\xv))
&= \log(1+r_\psi(\xv)) = -\log (1-D_\psi(\xv)).
\end{align*}

\subsection{On the Non-Saturating Generative Loss}
The original, saturating version of the generator objective is
\[
\min_\th \E_{q_\th(\xv)}[-\softplus(\log r_\psi(\xv))]
=\E_{q_\th(\xv)}[-\log(1+ r_\psi(\xv))],
\]
whose gradient is
\begin{align*}
\nabla_\th \E_{q(\zv)}[-\log (1 +r_\psi(\gv_\th(\zv)))]
&= \E_{q(\zv)}\Bigl[-\frac{r_\psi'(\gv_\th(\zv))}{1+r_\psi(\gv_\th(\zv))} \nabla_\th \gv_\th(\zv)\Bigr].
\end{align*}


Note that the \emph{plug-in} reverse KL-divergence loss is
\[
\min_\th \E_{q_\th(\xv)}[-\log r_\psi(\xv)].
\]
Compared to this, the non-saturating loss has the additional $\softplus(\cdot)$:
\[
\min_\th \E_{q_\th(\xv)}[\softplus(-\log r_\psi(\xv))]
=\E_{q_\th(\xv)}[\log(1+ r_\psi(\xv)^{-1})].
\]
This seems to help prevent vanishing gradients. 
Consider 
\[
\min_\th \E_{q_\th(\xv)}[\log (\tau +r_\psi(\xv)^{-1})]
=\E_{q(\zv)}[\log (\tau +r_\psi(\gv_\th(\zv))^{-1})],
\]
If $\tau=0$, it boils down to the plug-in reverse KL divergence, and $\tau=1$ recovers the non-saturating loss.
If we consider a gradient with respect to $\th$, we get
\begin{align*}
\nabla_\th \E_{q(\zv)}[\log (\tau +r_\psi(\gv_\th(\zv))^{-1})]
&= \E_{q(\zv)}\Bigl[-\frac{r_\psi'(\gv_\th(\zv))}{r_\psi(\gv_\th(\zv))(1+\tau r_\psi(\gv_\th(\zv))) } \nabla_\th \gv_\th(\zv)\Bigr]
\end{align*}
Here, recall that ${r_\psi(\xv)}\approx\frac{p(\xv)}{q_\th(\xv)}$ is supposed to be small for generated samples $\xv=\gv_\th(\zv)$.
Therefore, the plug-in loss with $\tau=0$ is inherently prone to vanishing gradient.

\subsection{GAN-Type Regularization with \texorpdfstring{$\a$}{alpha}-JSD}
\label{app:gan_type_reg}
To train a discriminator for our GAN-type regularization, we opt to use a modified GAN discriminator objective defined as
\begin{align*}
\min_\psi
-\a\E_{p(\xv)}[\log D_\psi(\xv;\a)] - (1 - \a)\E_{q_\th(\xv)}[\log (1-D_\psi(\xv;\a))].
\end{align*}
Similar to the vanilla GAN,
the optimal discriminator for each $\th$ and $\a$ in this case is
\[
D^\star_\psi(\xv;\a)=\frac{\a p(\xv)}{\a p(\xv)+(1-\a)q_\th(\xv)} \]
Then,the $\a$-JSD can be approximated as
\[
\sjsd^{(\a)}(q_\th,p) \approx
\frac{1}{1-\a}\E_{p(\xv)}\left[\log \frac{D_{\psi}(\xv;\a)}{\a}\right] + \frac{1}{\a}\E_{q_\th(\xv)}\left[\log\frac{1-D_\psi(\xv;\a)}{1-\a}\right].
\]
Hence, with this approximation, the generator update can be done via
\[
\min_\th \frac{1}{\a}\E_{q_\th(\xv)}\left[\log\frac{1-D_\psi(\xv;\a)}{1-\a}\right].
\]
In practice, we can use a weighted non-saturating version of the loss as well,
\begin{align*}
\min_\th -\E_{q_\th(\xv)}\left[\log\frac{D_\psi(\xv;\a)}{\a}\right] = \min_\th  \E_{q_\th(\xv)}\left[{\sf sp}\left(-\ell_\psi(\xv)-\log \frac{\a}{1 - \a}\right) \right] + \log \a,
\end{align*}
where $\ell_\psi(\xv) = \log \frac{p(\xv)}{q_\th(\xv)}$.


\subsection{On Discriminator Training with Mixture Score Matching Loss}
\label{app:lsgan}
In Sec.~\ref{sec:distillation}, we plugged in the explicit parameterization
\[
\sv_{\psi}^{\sf exp}(\xv; \a)
\defeq D_{\psi}(\xv; \a)\sv_p(\xv) 
%
%
+ (1-D_{\psi}(\xv; \a))\sv_{\psi}^{\sf fake}(\xv),
\]
to the mixture regression loss in Eq.~\eqref{eq:mixture_score_matching}, to train the fake score and the discriminator simultaneously.
If we consider an ideal scenario where we have the perfect score models for both $p$ and $q$, then all we need to train is the discriminator andthe mixture regression objective can be interpreted as a discriminator objective.
Here we reveal its connection to an instance of $f$-GAN discriminator objective.

Let $\sv_p(\xv)$ and $\sv_q(\xv)$ be the underlying score functions for $p$ and $q$, respectively.
Then, the explicit parameterization becomes
\[
\sv_{\psi}^{\sf exp}(\xv; \a)
= D_{\psi}(\xv; \a)\sv_p(\xv) 
%
%
+ (1-D_{\psi}(\xv; \a))\sv_q(\xv),
\]
and the mixture regression objective becomes only a function of the discriminator, \ie 
\begin{align*}
\Lc(\psi; \a) 
&= \a\,\E_{p(\xv)}[\|\sv_{\psi}^{\sf exp}(\xv; \a) - \sv_p(\xv)\|^2] 
+ (1 - \a)\,\E_{q_\th(\xv)}[\|\sv_{\psi}^{\sf exp}(\xv; \a) - \sv_{q}(\xv)\|^2]\\
&= \a\,\E_{p(\xv)}[(1-D_\psi(\xv;\a))^2\|\sv_p(\xv)-\sv_q(\xv)\|^2] 
+ (1 - \a)\,\E_{q(\xv)}[D_\psi(\xv;\a)^2\|\sv_p(\xv)-\sv_q(\xv)\|^2]\\
&= \int \Bigl\{
\a p(\xv) (1-D_\psi(\xv;\a))^2
+ (1-\a) q(\xv) D_\psi(\xv;\a)^2\Bigr\}
\|\sv_p(\xv)-\sv_q(\xv)\|^2 \diff\xv.
\end{align*}
Here, we note that the term $\|\sv_p(\xv) - \sv_q(\xv)\|^2$ is common in both expectation, and can be safely dropped to train the discriminator, which leads to a simplified objective
\begin{align*}
\Lc'(\psi; \a) 
&= \a\,\E_{p(\xv)}[(1-D_\psi(\xv;\a))^2] 
+ (1 - \a)\,\E_{q(\xv)}[D_\psi(\xv;\a)^2]\\
&= \int \Bigl\{
\a p(\xv) (1-D_\psi(\xv;\a))^2
+ (1-\a) q(\xv) D_\psi(\xv;\a)^2\Bigr\} \diff\xv.
\end{align*}
We note that this is equivalent to the discriminator objective induced by the following $f$-divergence
\begin{align*}
D_{f_\a} (p~\|~q)
&\defeq 
%
%
%
1 - \int \frac{p(\xv)q(\xv)}{\a p(\xv) + (1-\a) q(\xv)}\diff \xv
\defeq D_{\text{\sf $\a$-LC}}(p ~\|~ q),
\end{align*}
where $f_\a(r)\defeq \frac{(1-\a)(1-r)}{\a r + (1-\a)}$ is a convex function over $[0,\infty)$ for $\a\in(0,1)$.
For $\a=\half$, this divergence becomes symmetric in $p$ and $q$ and is known as the Le Cam distance~\citep[p.~47]{LeCam2012} in the literature~\citep{Polyanskiy--Wu2019}.
We thus call the general divergence for $\a\in(0,1)$ the $\a$-Le Cam distance.
In the GAN literature, this is known as the LSGAN objective~\citep{Mao--Li--Xie--Lau--Wang--Paul2017}.

As we revealed, our discriminator training in distillation can also be done separately using the $\alpha$-Le Cam-distance-based objective.
However, we conjecture that our score-regression-based end-to-end objective may have benefit, as our primary goal of discriminator training is to use it in the generator update in the form of an approximate score of mixture.
We leave the further exploration of such alternative methods as a future work.

\section{More on Experiments and Additional Results}
\label{sec:appendix_experiments_and_results}

We present some additional experiments and results in this section.  We first provide a more detailed training configuration for our experiments in Sec.~\ref{sec:experiments} and then evaluate our proposed method on a synthetic swiss-roll dataset in Appendix \ref{sec:toy_swiss_roll}.  
Finally, we present some samples generated from Score-of-Mixture Training and Score-of-Mixture Distillation in Figs.~\ref{fig:cifar10 scratch}-\ref{fig:imagenet distillation}.

\subsection{Training Configuration}
\label{sec:training configuration}

We summarize the detailed training configuration in Table~\ref{tab:hyperparameters}.


\begin{table}[!htb]
    \setlength{\tabcolsep}{15pt}
    \caption{Hyperparameters used for training one-step generators with Score-of-Mixture Training and Distillation.}\label{tab:hyperparameters}
    \vspace{.5em}
    \centering
    \begin{tabular}{l|cc|cc}
        \toprule
        Hyperparameter & \multicolumn{2}{c|}{CIFAR-10} & \multicolumn{2}{c}{ImageNet $64\times 64$} \\
        & Scratch & Distillation & Scratch & Distillation \\
        \midrule
        Generator learning rate & 1e-4 & 5e-5 & 5e-6 & 2e-6 \\
        Score learning rate & 5e-4 & 5e-5 & 5e-5 & 2e-6 \\
        Score learning rate decay & cosine & None & cosine & None\\
        Batch size & 280 & 280 & 280 & 280 \\
        Diffusion pretraining steps & 15k & N/A & 40k & N/A\\  
        Training iterations & 150k & 150k & 200k & 200k \\
        Score dropout probability & 0.13 & 0.00 & 0.00 & 0.00 \\
        Number of GPUs & 2 $\times$ A100 & 4$\times$ A100 & 7$\times$ A100 & 7$\times$ A100 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Toy Swiss Roll}
\label{sec:toy_swiss_roll}

We tested our proposed framework and ablated various design choices on a synthetic swiss roll dataset.  
We followed the dataset setup by \citet{che2020your}. 
We trained models with SMT and SMD and compared this against an amortized version of reverse KL minimization with DMD weighting ($\a \in \{0, 1\}$) similar to the ablations in Sec. \ref{sec:ablation_studies}. Additionally we compared against non-score-based baselines including the vanilla GAN and Diffusion-GAN~\citep{Wang--Zheng--He--Chen--Zhou2023}.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.9\linewidth]{toy_figs/toy_swiss_roll.pdf}
    \caption{Samples produced by generators trained using different methods. All figures are created using 10,000 samples from the respective generator.}
    \label{fig:swiss roll}
\end{figure}

Across all experiments, we use the same generator architecture --- a two-layer MLP with a hidden dimension of 128 and leaky ReLU nonlinearity. 
We train all models for 200k steps on a single NVIDIA 3090 GPU with a batch size of 256. All score-based methods leverage a learning rate of 1e-5 for the generator and 1e-4 for the amortized score (and discriminator when applicable) whereas the GAN-based methods use a learning rate of 1e-4 for both generator and discriminator. We use the AdamW optimizer without any learning rate schedulers.

The samples produced are shown in Fig.~\ref{fig:swiss roll}.  Notice how the GAN is unable to perfectly cover the entire continuous mode of the swiss roll. The multi-noise-level extension of GAN-based on Diffusion-GAN covers the mode but also samples from areas of low density.  We found the latter to be sensitive to the chosen noise levels in comparison to the methods based on updating the generator using the score.

Our results for training from scratch and distillation are presented in Fig.~\ref{fig:swiss roll}(b)-(d). All three methods successfully capture the modes of the underlying distribution. While the impact of the GAN regularizer is less pronounced than in our high-dimensional experiments, we observe that enabling it reduces the number of samples in low-density regions. The distillation results appear slightly noisy, likely due to the quality of the pre-trained score model. This highlights the advantage of training from scratch, as it avoids amplifying existing estimation errors in the pre-trained model.

Fig.~\ref{fig:swiss roll}(e) presents the results of ablating the $\alpha$-sampler. Unlike in the high-dimensional setting, the choice of the number of $\alpha$ values is less critical in this case. However, our method in (b) still produces fewer noisy samples, suggesting that training the amortized score model with multiple $\alpha$ values may be beneficial.

%
\subsection{Samples}
\label{sec:samples}

We present some image samples generated by SMT and SMD in Figs.~\ref{fig:cifar10 scratch}-\ref{fig:imagenet distillation}.

\vspace{8em}
\begin{figure}[!h]
    \centering
    \includegraphics[width=\textwidth]{figs/cifar10_scratch.png}
    \caption{{One-step generated samples from SMT on CIFAR-10 (unconditional).}}
    \label{fig:cifar10 scratch}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figs/cifar10_distillation.png}
    \caption{{One-step generated samples from SMD on CIFAR-10 (unconditional).}}
    \label{fig:cifar10 distillation}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figs/imagenet_scratch.png}
    \caption{{One-step generated samples from SMT on ImageNet 64$\times$64  (conditional).}}
    \label{fig:imagenet scratch}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figs/imagenet_distilled.png}
    \caption{{One-step generated samples from SMD on ImageNet 64$\times$64  (conditional).}}
    \label{fig:imagenet distillation}
\end{figure}

%
%


\end{document}


%
%
%
%
%
%
%
%
%
%
%
%
%
