\section{Introduction}
We consider the computational task of estimating a nested expectation, which is the expectation of a function that itself depends on another unknown conditional expectation. More precisely, let $\Qb$ be a Borel probability measure with density $q$ on $\Theta$ and $\Pb_\theta$ a Borel probability measure with density $p_\theta$ on $\calX \subseteq \mathbb{R}^{d_{\calX}}$ which is parameterized by $\theta \in \Theta \subseteq \mathbb{R}^{d_{\Theta}}$.  Given integrable functions $f: \R \to \R$ and $g:\calX \times \Theta \to \R$, we are interested in estimating:
\begin{align}\label{eq:nested}
    I & := \E_{\theta \sim \Qb} \left[ f \left(\E_{X \sim \Pb_\theta} \left[ g(X, \theta) \right] \right) \right] = \underbrace{\int_{\Theta} f\Bigg(\underbrace{\int_{\mathcal{X}} g(x,\theta) p_{\theta}(x) dx}_{\text{ inner conditional expectation}} \Bigg) q(\theta) d\theta}_{\text{outer expectation}}. 
\end{align}
Nested expectations arise within a wide range of tasks, such as the computation of objectives in Bayesian experimental design \citep{Beck2020,Goda2020,Rainforth2024}, of acquisition functions in active learning and Bayesian optimisation \citep{Ginsbourger2010,Yang2024}, of objectives in distributionally-robust optimisation \citep{Shapiro2023,Bariletto2024,Dellaporta2024}, and of statistical divergences \citep{Song2020,Kanagawa2019}.
Computing nested expectations is also a key task beyond machine learning, including in fields ranging from health economics \citep{giles2019decision} to finance and insurance \citep{Gordy2010,Giles2019}, manufacturing \citep{Andradottir2016} 
and geology \citep{Goda2018}.

The estimation of nested expectations is particularly challenging since there are two levels of intractability: the inner conditional expectation, and the outer expectation, both of which must be approximated accurately in order to approximate $I$ accurately. The most widely used algorithm for this problem is \emph{nested Monte Carlo (NMC)} \citep{Lee2003,Hong2009,rainforth2018nesting}. It approximates the inner and outer expectations using Monte Carlo estimators with $N$ and $T$ samples respectively. NMC is consistent under mild conditions, but has a relatively slow rate of convergence. Depending on the regularity of the problem, existing results indicate that we require either $\mathcal{O}(\Delta^{-3})$ or $\calO(\Delta^{-4})$ evaluations of $g$ to obtain a root mean squared error smaller or equal to $\Delta$. 
This tends to be prohibitively expensive; for example, we would expect in the order of either $1$ or $100$ million observations to obtain an error of $\Delta=0.01$. This will be infeasible for the many applications where obtaining samples or evaluating $g$ is expensive.


This issue has led to the development of a number of methods aiming to reduce the cost. \citet{Bartuska2023} proposed replacing the Monte Carlo estimators with quasi-Monte Carlo (QMC) \citep{Dick2013}. This algorithm, called \emph{nested QMC (NQMC)}, requires only $\calO(\Delta^{-2.5})$ function evaluations to obtain an error of size $\Delta$ (so that we only need in the order of $100,000$ observations for an error of $\Delta=0.01$). However, NQMC requires strong regularity assumptions which may not hold in practice (a monotone second and third derivative for $f$). Separately, \citet{bujok2015multilevel,Giles2019, giles2019decision} proposed to use \emph{multi-level Monte Carlo (MLMC)} and showed that this can further reduce the number of function evaluations to $\calO(\Delta^{-2})$ (so that we only need in the order of $10,000$ observations for an error of $\Delta=0.01$). The algorithm has relatively mild assumptions on $f$ and $g$, which makes it broadly applicable but sub-optimal for applications where $f$ and $g$ are smooth and where we might therefore expect further reductions in cost. 

To fill this gap in the literature, we propose a novel algorithm called \emph{nested kernel quadrature (NKQ)}, which is presented in \Cref{sec:methodology}. NKQ replaces the inner and outer MC estimators of NMC with kernel quadrature (KQ) estimators \citep{sommariva2006numerical}. We show in \Cref{sec:theory} that NKQ requires only $\tilde{\calO}(\Delta^{-\frac{d_\calX}{s_\calX} - \frac{d_\Theta}{s_\Theta}})$ function evaluations to guarantee an error smaller or equal to $\Delta$. Here $\tilde{\calO}$ denotes $\calO$ up to logarithmic terms. 
Here, $s_{\mathcal{X}}, s_{\Theta}$ are constants relating to the smoothness of $f$ and $g$ in $\mathcal{X}$ and $\Theta$, and we have $s_{\mathcal{X}} > d_{\mathcal{X}}/2$ and $s_{\Theta} > d_{\Theta}/2$. In the least favorable case, we therefore recover the $\calO(\Delta^{-4})$ of NMC, but when the integrand is smooth and the dimension is not too large, we are able to have a cost which scales better than $\calO(\Delta^{-2})$ and the method significantly outperforms all competitors. In those cases, we may only need in the order of a few hundred or thousands observations for an error of $\Delta=0.01$. This fast rate is demonstrated numerically in \Cref{sec:experiments}, where we show that NKQ can provide significant accuracy gains in problems from Bayesian optimisation to option pricing and health economics. Moreover, we show that NKQ can be combined with QMC and MLMC, providing an avenue to further accelerate convergence.
