\section{Background}\label{sec:background}


\paragraph{Notation}
Let $\N_+$ denote the positive integers and $\N = \N_+ \cup \{ 0 \}$. 
For $h:\calX \subseteq \R^d \to \R$, $x_{1:N}$ and $h(x_{1:N})$ are vectorized notation for $[x_1, \ldots, x_N]^\top \in \R^{N \times d}$ and $[h(x_1), \ldots, h(x_N)]^\top \in \R^{N \times 1}$ respectively.
For a vector $a = [a_1 ,\ldots, a_d]^\top \in \R^{d}$, define $\|a\|_{b} = ( \sum_{i=1}^{d} a_i^b)^{1/b}$.
For a distribution $\pi$ supported on $\calX$ and $0 < p \leq \infty$, $L_p(\pi)$ is the space of functions $h : \calX \to \R$ such that $\Vert h \Vert_{L_p(\pi)} := \mathbb{E}_{X \sim \pi} [|h(X)|^p] < \infty$ and 
$L_\infty(\pi)$ is the space of functions that are bounded $\pi$-almost everywhere. 
When $\pi$ is the Lebesgue measure $\calL_\calX$ over $\calX$, we write $L_p(\calX) := L_p(\calL_\calX)$. 
For $\beta \in \mathbb{N}$, $C^\beta(\calX)$ denotes the space of functions whose partial derivatives of up to and including order $\beta$ are continuous. 
For two positive sequences $\{a_n\}_{n \in \N_+}$ and $\{b_n\}_{n \in \N_+}$, $a_n \asymp b_n$ means that $\lim_{n\to \infty} \frac{a_n}{b_n}$ is a positive constant. For two positive functions $f_1(z), f_2(z) > 0$, $f_1(z) = \calO(f_2(z))$ means that $\lim_{z \to \infty} \frac{f_1(z)}{f_2(z)} < \infty$ and $f_1(z) = \Tilde{\calO}(f_2(z))$ means that $f_1(z) = \calO(f_2(z) (\log f_2(z))^r)$ for some positive constant $r$.


\paragraph{Existing Methods for Nested Expectations}
Standard Monte Carlo (MC) is an estimator which can be used to approximate expectations/integrals through samples \citep{Robert2004}. Given an arbitrary function $h:\calX \rightarrow \mathbb{R}$ with $h\in L_1(\pi)$, and $N$ independent and identically distributed (i.i.d.) realisations $x_{1:N}$ from $\pi$, standard MC approximates the expectation of $h$ under $\pi$ as follows:
\begin{equation*}
    \mathbb{E}_{X \sim \pi}[h(X)] \approx \frac{1}{N}\sum_{n=1}^N h(x_n). 
\end{equation*}
For the nested expectation $I$ in \eqref{eq:nested}, the use of a MC estimator for both the inner and outer expectation leads to the \emph{nested Monte Carlo (NMC)} estimator \citep{Hong2009,rainforth2018nesting} given by
\begin{align}\label{eq:nmc}
    \hat{I}_{\text{NMC}} := \frac{1}{T}\sum_{t=1}^T f\left( \frac{1}{N} \sum_{n=1}^N g(x_n^{(t)}, \theta_t) \right) ,
\end{align}
where $\theta_{1:T}$ are $T$ i.i.d. realisations from $\Qb$ and $x_{1:N}^{(t)}$ are $N$ i.i.d. realisations from $\Pb_{\theta_t}$ for each $t \in \{1, \ldots, T\}$. 
The root mean-squared error of this estimator goes to zero at rate $\calO(N^{-\frac{1}{2}} + T^{-\frac{1}{2}} )$ when $f$ is Lipschitz continuous~\citep{rainforth2018nesting}. 
Hence, taking $N=T=\calO(\Delta^{-2})$ leads to an algorithm which requires $N\times T=\calO(\Delta^{-4})$ function evaluations to obtain error smaller or equal to $\Delta$. When $f$ has bounded second order derivatives, the root mean-squared error converges at the improved rate of $\calO (N^{-1} + T^{-\frac{1}{2}})$~\citep{rainforth2018nesting}.
Taking $N = \sqrt{T} =\calO(\Delta^{-1})$ therefore leads to an algorithm requiring $N\times T = \calO(\Delta^{-3})$ function evaluations to get an error of $\Delta$~\citep{Gordy2010,rainforth2018nesting}. 
Despite its simplicity, NMC therefore requires a large number of evaluations to reach a given $\Delta$. 

As a result, two extensions have been proposed. Firstly, \citet{Bartuska2023} proposed to use  \eqref{eq:nmc}, but to replace the i.i.d. samples with QMC points. 
QMC points are points which aim to fill $\mathcal{X}$ in a somewhat 
uniform fashion~\citep{Dick2013}, with well-known examples including Sobol or Halton sequences. \citet{Bartuska2023} used randomized QMC points, which removes the bias of standard QMC by using a randomized low discrepancy sequence~\citep{ owen2003quasi}. For nested expectations, they showed that nesting randomized QMC estimators can lead to a faster convergence rate and hence a smaller cost of $\calO(\Delta^{-2.5})$. However, the approach is only applicable when $\Pb_\theta$ and $\Qb$ are Lebesgue measures on unit cubes (or smooth transformations thereof), and the rate only holds when $f$ has monotone second and third order derivatives.

Alternatively, \citet{bujok2015multilevel,Giles2015,Giles2019, giles2019decision} proposed to use \emph{multi-level Monte Carlo} (MLMC), which decomposes the nested expectation using a telescoping sum on the outer integral, then approximates each term with MC. The integrand with the $\ell$'th fidelity level is constructed as the composition of $f$ with an inner MC estimator based on $N_\ell$ samples. 
More precisely, the MLMC treatment of nested expectations consist of using:
\begin{align}\label{eq:mlmc}
    & \hat{I}_{\text{MLMC}}  :=  
    \sum_{l=1}^L  \frac{1}{T_\ell}\sum_{t=1}^{T_\ell} (f(J_{\ell,t})-f(J_{\ell-1,t})) + \frac{1}{T_0} \sum_{t=1}^{T_0} f(J_{0,t}) \nonumber \\
    & \text{where } J_{\ell,t}  := \frac{1}{N_\ell} \sum_{n=1}^{N_\ell} g(x_n^{(t)}, \theta_t) \text{ for } \ell \in \{0,\ldots,L\},
\end{align}
Under some regularity conditions, Theorem 1 from \citet{Giles2015} shows that taking $N_\ell = \calO(2^\ell)$ and $T_\ell= \calO(2^{-2 \ell} \Delta^{-2})$ leads to an estimator requiring $\calO(\Delta^{-2})$ function evaluations to obtain root mean squared error smaller or equal to $\Delta$.  
Although MLMC has the best known efficiency 
for nested expectations, the $N_l$ and $T_l$ need to grow exponentially with $l$, and we therefore need a very large sample size for its theoretical convergence rate to become evident in practice~\citep{Giles2019,giles2019decision}.
MLMC also requires making several challenging design choices, including the coarsest level to use, and the number of samples per level.
Most importantly, MLMC as well as all existing methods fail to account for the smoothness of the functions $f$ and $g$. 

% \vspace{-1mm}
\paragraph{Kernel Quadrature}

\emph{Kernel quadrature (KQ)} \citep{sommariva2006numerical,Rasmussen2003,Briol2019PI} provides an alternative to standard MC for (non-nested) expectations. Consider an arbitrary function $h:\calX \rightarrow \mathbb{R}$ and distribution $\pi$ on $\calX$, and suppose we would like to approximate $\E_{X \sim \pi}[h(X)]$. KQ is an estimator which can be used when $h$ is sufficiently regular, in the sense that it belongs to a reproducing kernel Hilbert space (RKHS) \cite{Berlinet2004} $\calH_k$ with kernel $k$. We recall that for a positive semi-definite kernel $k : \calX \times \calX \to \R$, the RKHS $\calH_k$ is a Hilbert space with inner product $\langle \cdot,\cdot \rangle_{\calH_k}$ and norm $\Vert \cdot \Vert_{\calH_k}$~\citep{aronszajn1950theory} such that: (i) $k(x,\cdot) \in \calH_k$ for all $x \in \calX$, and (ii) the reproducing property holds, i.e. for all $h \in \calH_k$, $x\in \calX$,  $h(x)=\langle h,k(x,\cdot) \rangle_{\calH_k}$. An important example of  RKHS is the Sobolev space $W_2^s(\calX)$ ($s > \frac{d}{2}$), which consists of functions of certain smoothness encoded through the square integrability of their weak partial derivatives up to order $s$, 
\begin{align}\label{eq:defi_sobolev}
    W_2^s(\calX) := &\big \{h \in L_2(\calX): D^\beta h \in L_2(\calX) \text { for all } \beta \in \mathbb{N}^d \text { with }|\beta| \leq s \big \}, \quad s \in \N_+
\end{align}
where $D^\beta f$ denotes the $\beta$-th (weak) partial derivative of $f$. 

Assuming $h \in \calH_k$ and $\mathbb{E}_{X \sim \pi}[\sqrt{k(X, X)}] < \infty$, 
the KQ estimator $\hat{I}_{\kq} = \sum_{n=1}^N w_n h(x_n)$ uses weights obtained by minimizing an upper bound on the absolute error:
\begin{align*}
    \left|I- \hat{I}_{\kq} \right| &= \Big| \E_{X \sim \pi}[h(X)] - \sum_{n=1}^N w_n h(x_n) \Big| \leq \|h\|_{\calH_k} \Big\| \mu_\pi(X) - \sum_{n=1}^N w_n k\left(x_n,\cdot \right)  \Big\|_{\calH_k},
\end{align*}
where $\mu_\pi(\cdot) = \mathbb{E}_{X \sim \pi}[k(X,\cdot)]$ is the \emph{kernel mean embedding (KME)} of $\pi$ in the RKHS $\mathcal{H}_k$~\citep{smola2007hilbert}.
Minimizing the right hand side with an additive regulariser term $\lambda \|f\|_{\calH_k}$ over the choice of weights leads to the KQ estimator:
\begin{align}\label{eq:kq}
     \hat{I}_\kq := \mu_\pi(x_{1:N}) \left( \boldsymbol{K} + N \lambda \boldsymbol{I}_N \right)^{-1}h(x_{1:N}),
\end{align}
where $\boldsymbol{I}_N$ is the $N \times N$ identity matrix, $\boldsymbol{K} = k(x_{1:N}, x_{1:N}) \in \mathbb{R}^{N \times N}$ is the Gram matrix and $\lambda \geq 0 $ is a regularisation parameter ensuring the matrix is numerically invertible. 
The KQ weights are given by $w_{1:N} = \mu_\pi(x_{1:N}) ( \boldsymbol{K} + N \lambda \boldsymbol{I}_N )^{-1}$ and are optimal when $\lambda=0$. 


KQ takes into account the structural information that $h \in \calH_k$ so the absolute error $|I- \hat{I}_{\kq}|$ goes to $0$ at a fast rate as $N \rightarrow \infty$.
Specifically, when the RKHS $\calH_k$ is the Sobolev space $W_2^s(\calX)$ ($s > \frac{d}{2}$), KQ achieves the rate $\calO(N^{-\frac{s}{d}})$~\citep{Kanagawa2019adaptive,Kanagawa2017convergence}. This is known to be minimax optimal ~\citep{novak2006deterministic,novak2016some}, and significantly faster than the $\calO(N^{-\frac{1}{2}})$ rate of standard MC. 
Interestingly, existing proof techniques that obtain this rate take $\lambda = 0$ in \eqref{eq:kq} and require the Gram matrix $\boldsymbol{K}$ to be invertible, whilst the new proof technique based on kernel ridge regression in this paper obtains the same optimal rate while allowing a positive regularization $\lambda \asymp N^{-\frac{2s}{d}}(\log N)^{\frac{2s+2}{d}}$, which improves numerical stability when inverting $\boldsymbol{K}$. (See \Cref{rem:stage_one_error_and_standard_kq})

Despite the optimality of the KQ convergence rate, the rate constant can be reduced by selecting points $x_{1:N}$ other than through i.i.d. sampling. Strategies include importance sampling~\citep{Bach2015,Briol2017SMCKQ}, QMC point sets~\citep{Briol2019PI,Jagadeeswaran2018,Bharti2023,Kaarnioja2025}, realisations from determinental point processes~\citep{belhadji2019kernel}, point sets with symmetry properties~\citep{Karvonen2017symmetric, Karvonen2019} and adaptive designs~\citep{osborne2012active,gunter2014sampling,Briol2015, gessner2020active}. 
Most relevant to our work is the combination of KQ with MLMC to improve accuracy in multifidelity settings \citep{li2023multilevel}.


The two main drawbacks of KQ compared to MC are the worst-case computational cost of $\calO(N^3)$ (due to computation of the inverse of the Gram matrix), and the need for a closed-form expression of the KME $\mu_\pi$. Fortunately, numerous approaches can mitigate these drawbacks. To reduce the cost, one can use geometric properties of the point set~\citep{Karvonen2017symmetric, Karvonen2019,Kuo2024}, Nyström approximations~\citep{Hayakawa2022,Hayakawa2023}, randomly pivoted Cholesky~\citep{Epperly2023}, or the fast Fourier transform~\citep{Zeng2009}.
To obtain a closed-form KME, KQ users typically refer to existing derivations (see Table 1 in \citet{Briol2019PI} or   \citet{Wenger2021}), or use Stein reproducing kernels \citep{Oates2017,Oates2016CF2,Si2020,Sun2021}. 

In this paper, we tackle both drawbacks  through a change of variable trick. 
Suppose we can find a continuous transformation map $\Phi$ such that $x_{1:N} = \Phi(u_{1:N})$ where $u_{1:N}$ are samples from a simpler distribution $\Ub$ of our choice. 
A direct application of change of variables theorem (Section 8.2 of \citet{stirzaker2003elementary}) proves that $\E_{X \sim \pi} h(X)  = \int_\calU h(\Phi(u)) d\Ub(u)$, so the integrand changes from $h: \calX \to \R$ to $h \circ \Phi: \calU \to \R$ and the kernel quadrature estimator becomes $\hat{I}_{\kq} = \mu_{\Ub}(u_{1:N}) \left(  \boldsymbol{K}_{\calU} + N \lambda \Id_N \right)^{-1} (h \circ \Phi)(u_{1:N})$, where $\boldsymbol{K}_{\calU} = k_{\calU}(u_{1:N}, u_{1:N})$. 
The measure $\Ub$ is typically chosen such that the KME is known in closed-form, and the KQ weights $\mu_{\Ub}(u_{1:N}) \left(  \boldsymbol{K}_{\calU} + N \lambda \Id_N \right)^{-1}$ can be pre-computed and stored so that KQ becomes a weighted average of function evaluations with $\calO(N)$ computational complexity. 
See  \Cref{sec:reparam} for further details.


Before concluding, we note that the KQ estimator is often called \emph{Bayesian quadrature (BQ)} \citep{Diaconis1988,OHagan1991,Rasmussen2003,Briol2019PI,Hennig2022} since it can be derived as the mean of the pushforward of a Gaussian measure on $h$ conditioned on $h(x_{1:N})$ \citep{kanagawa2018gaussian}.
The advantage of the Bayesian interpretation is that it provides finite-sample uncertainty quantification, and it also allows for efficient hyperparameter selection through empirical Bayes. 
