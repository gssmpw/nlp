\section{Additional Experimental Details}
\subsection{``Change of Variable'' Trick for Kernel Quadrature}\label{sec:reparam}
In the main text, we have shown that the two major bottlenecks of KQ/NKQ are: 
\begin{itemize}
    \item The closed-form KME $\E_{X \sim \Pb}[k(X, x)]$. 
    \item The $\calO(N^3)$ computational cost of inverting the Gram matrix $k(x_{1:N}, x_{1:N})$.
\end{itemize}
Fortunately, both two challenges can be solved with the ``change of variable'' trick. Here, we only present the idea for KQ but the same holds for NKQ in both stages. 

The integral of interest is $I = \int_\calX h(x) \Pb(dx)$. 
Suppose we can find a continuous transformation $\Phi$ such that $X = \Phi(U)$, where $U \sim \Ub$ is another random variable which is easy to sample from. 
Then the integral $I$ can be equivalently expressed as $I = \int_\calU h(\Phi(u)) d\Ub(u)$, by a direct application of change of variables theorem (Section 8.2 of \cite{stirzaker2003elementary}. 
Now the integrand changes from $h: \calX \to \R$ to $h \circ \Phi: \calU \to \R$ and the kernel quadrature estimator becomes
\begin{align*}
    \hat{I}_{\kq} = \E_{U \sim \Ub} [k_\calU(U, u_{1:N})] \left( k_\calU(u_{1:N}, u_{1:N}) + N \lambda \Id_N \right)^{-1} (h \circ \Phi)(u_{1:N}) .
\end{align*}
Here $k_\calU$ is a reproducing kernel on $\calU$.
Since $\Ub$ is a simple probability distribution, we can find its closed-form KME in Table 1 in \citet{Briol2019PI} or the \texttt{ProbNum} package \citep{Wenger2021}, which addresses the first challenge.
Additionally, notice that both the Gram matrix $k(u_{1:N}, u_{1:N})$ and the KME $\E_{U \sim \Ub} [k(U, u_{1:N})]$ are independent of $h$ and $\Phi$, so the KQ weights $w_{1:N}^{\kq} = \E_{U \sim \Ub} [k(U, u_{1:N})] \left( k(u_{1:N}, u_{1:N}) + N \lambda \Id_N \right)^{-1}$ can be pre-computed and stored. 
As a result, KQ becomes a simple weighted average of function evaluations $\sum_{i=1}^N w_i^{\kq} h(x_i)$. Therefore, the computational cost reduces to linear cost $\calO(N)$ and hence the second challenge is addressed. 
The downside of the ``change of variable'' trick is that the Sobolev smoothness of $h \circ \Phi: \calU \to \R$ is unclear when $\Phi$ is not smooth, so we lose the theoretical convergence rate from \Cref{thm:main}.


\subsection{Synthetic Experiment}\label{sec:appendix_toy}
\paragraph{Assumptions from \Cref{thm:main}}
We would like to check whether the assumptions made in Theorem 1 hold in this synthetic experiment. Recall that we use both $k_\calX$ and $k_\Theta$ to be Mat\'{e}rn-3/2 kernels so we need to verify Assumptions \ref{as:equivalence} --- \ref{as:app_lipschitz} with $s_\Theta=s_\calX=2$.
\begin{enumerate}
    \item Both distributions $\Pb_\theta$ and $\Qb$ are uniform distributions over $[0,1]$, so Assumption \ref{as:equivalence} is satisfied.
    \item $D_\theta^{\beta} g(\cdot, \theta) \in W_2^2(\calX)$ and $D_\theta^{\beta} p(\cdot, \theta) \in L_2(\calX)$ for $\beta = 0, 1, 2$ so Assumption \ref{as:app_true_g_smoothness} is satisfied.
    \item Both $g(x, \cdot), p(x, \cdot) \in W_2^{2}(\Theta)$ so Assumption \ref{as:app_true_J_smoothness} is satisfied.
    \item $f \in C^{3}(\R)$ so Assumption \ref{as:app_lipschitz} is satisfied.
\end{enumerate}

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/toy_ablation.pdf}
    \caption{Further ablation studies in the synthetic experiment. \textbf{Left:} NKQ with different proportionality coefficients $\lambda_0$ for regularization parameter $\lambda_\calX, \lambda_\Theta$. 
    \textbf{Middle:} NKQ with different kernel lengthscales $\gamma$ in both stages. 
    \textbf{Right:} NKQ with different kernels in both stages. The nested Monte Carlo (NMC) in blue is presented as a benchmark in all figures.}
    \label{fig:toy-ablation}
    \vspace{-10pt}
\end{figure}

The synthetic problem can be modified to have higher dimensions $d$. In this synthetic experiment, we set both $d_\calX = d_\Theta =d$.
For $a = [a_1 ,\ldots, a_d]^\top \in \R^{d}$, define $\|a\|_{b} = ( \sum_{i=1}^{d} a_i^b)^{1/b}$.
\begin{align}\label{eq:toy_high_d}
    & x \sim \operatorname{U}[0,1]^{d}, \quad \theta \sim \operatorname{U}[0,1]^{d}, \quad g(x, \theta) = \|x\|_{2.5}^{2.5} +  \| \theta \|_{2.5}^{2.5}, \quad f( z) = z^2,
\end{align}
The true value of the nested expectation can be computed in closed-form: $I = \frac{16}{49}d^2 + \frac{25}{294} d$. In \Cref{fig:toy_experiment}, we study the mean absolute error of NMC and NKQ as dimension $d$ grows. 
We see that NKQ outperforms NMC by a huge margin in low dimensions, but the performance gap closes down in higher dimensions, which is expected because the rate proved in \Cref{cor:nkq} is $\calO(\Delta^{- \frac{ s_\calX}{d_\calX} - \frac{ s_\Theta}{d_\Theta}})$ which becomes slower when dimension increases yet the smoothness of the problem remains the same.

In \Cref{fig:toy-ablation}, we conduct a series of ablation studies on the hyperparameter of NKQ in the synthetic experiment. 
Although \Cref{thm:main} suggests choosing the regularization parameters $\lambda_\calX, \lambda_\Theta $ that are proportionate to $N^{-2\frac{s_\calX}{d_\calX}}$ and $T^{-2\frac{s_\Theta}{d_\Theta}}$ respectively, it is unclear in practice how to pick the exact proportionality coefficients $\lambda_0$. 
\Cref{fig:toy-ablation} \textbf{Left} shows that $\lambda_0 = 1.0$ and $\lambda_0 = 0.1$ give the best performances, while using $\lambda_0$ too big ($\lambda_0 = 10.0$) suffers from slower convergence rate and using $\lambda_0$ too small ($\lambda_0 = 0.01, 0.001$) causes numerical issues when $N, T$ become large.
\Cref{fig:toy-ablation} \textbf{Middle} shows that kernel lengthscale, if too big ($\gamma=10.0$) or too small ($\gamma=1.0$), would result in worse performance for NKQ and that the widely-used median heuristic is good enough to select a satisfying lengthscale. 
\Cref{fig:toy-ablation} \textbf{Right} shows that NKQ with Matérn-3/2 kernels has better performance than with Matérn-1/2 kernels, which agrees with \Cref{thm:main} indicating that it is preferable to use Sobolev kernels with the highest permissible orders of smoothness.
Interestingly, we see that NKQ with Gaussian kernels has similar performance as with Matérn-3/2 kernels. 
Similar phenomenon have been shown both theoretically and empirically that kernel ridge regression with Gaussian kernels are optimal in learning Sobolev space functions when the lengthscales are chosen appropriately~\citep{hang2021optimal, eberts2013optimal}.


\subsection{Risk Management in Finance}\label{sec:finance}
In this experiment, we consider specifically an asset whose price $S({\tau})$ at time $\tau$ follows the Black-Scholes formula $S(\tau) = S_0 \exp \left(\sigma W(\tau) - \sigma^2 \tau/2 \right)$ for $\tau \geq 0$, where $\sigma$ is the underlying volatility, $S_0$ is the initial price and $W$ is the standard Brownian motion.
The financial derivative we are interested in is a butterfly call option whose payoff at time $\tau$ can be expressed as $\psi(S({\tau}))=\max (S(\tau)-K_1, 0) + \max (S(\tau)-K_2, 0) - 2\max (S(\tau) - (K_1+K_2)/2, 0)$.
We follow the setting in \cite{alfonsi2021multilevel, alfonsi2022many, chen2024conditional} assuming that a shock occur at time $\eta$, at which time the option price is $S(\eta)=\theta$, and this shock multiplies the option price by $1 + s$. The option price at maturity time $\zeta$ is denoted as $S(\zeta) = x$. To summarize, the expected loss caused by the shock can be expressed as the following nested expectation:
\begin{align*}
    I = \E [f(J(\theta))], \quad f(J(\theta)) = \max(J(\theta), 0), \quad J(\theta) = \int_0^\infty g(x) \Pb_\theta(dx), \quad g(x) = \psi(x)-\psi((1+s)x).
\end{align*}
Following the setting in \cite{alfonsi2021multilevel, alfonsi2022many, chen2024conditional}, we consider the initial price $S_0 = 100$, the volatility $\sigma = 0.3$, the strikes $K_1 = 50, K_2 = 150$, the option maturity $\zeta=2$ and the shock happens at $\eta=1$ with strength $s = 0.2$. 
The option price at which the shock occurs are $\theta_{1:T}$ sampled from the log normal distribution deduced from the Black-Scholes formula $\theta_{1:T} \sim \Qb = \operatorname{Lognormal}( \log S_0 - \frac{\sigma^2}{2} \eta, \sigma^2 \eta)$. 
Then $x^{(t)}_{1:N}$ are sampled from another log normal distribution also deduced from the Black-Scholes formula $x^{(t)}_{1:N} \sim \Pb_{\theta_t} = \operatorname{Lognormal}( \log \theta_t - \frac{\sigma^2}{2} (\zeta - \eta), \sigma^2 (\zeta - \eta))$ for $t = 1, \ldots, T$.

In this experimental setting, although both  $g$ only depends on $x$ and it is a combination of piece-wise linear functions so $g \in W_2^{1}(\calX)$. The probability density function of $\Pb_{\theta}$ is infinitely times differentiable 

Notice that log normal distribution $\operatorname{LogNormal}(\bar{m}, \bar{\sigma}^2) $ can be expressed as the following transformation from uniform distribution over $[0,1]$. 
\begin{align*}
    u \sim U[0,1], \quad \exp(\Psi^{-1}(u)\bar{\sigma} + \bar{m}) \sim \operatorname{LogNormal}(\bar{m}, \bar{\sigma}^2) .
\end{align*}
Here, $\Psi^{-1}$ is the inverse cumulative distribution function of a standard normal distribution.
Therefore, we can use the ``change of variables'' trick from \Cref{sec:reparam} such that we have closed-form KME against uniform distribution from \textit{Probnum}~\cite{Wenger2021}, and also the computational complexity of NKQ becomes $\calO(N \times T)$. 
Although $\Psi^{-1}$ is infinitely times differentiable, we still use Mat\'{e}rn-1/2 kernels in both stages to be conservative of the smoothness of the integrand after applying the ``change of variables'' trick.

\begin{figure*}[t]
    \centering
    \begin{minipage}{\textwidth}
    \begin{subfigure}[b]{1.0\linewidth}
        \centering
        \includegraphics[width=0.4\linewidth]{figures/finance_legend_only_with_mlkq.pdf}
    \end{subfigure}
    \vspace{-17pt}
    \end{minipage}
    
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/toy_mlmc.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/finance_with_mlkq.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/evppi_with_mlkq.pdf}
    \end{subfigure}
    \vspace{-10pt}
    \caption{Comparison of all the methods including MLKQ on the synthetic experiment (\textbf{Left}), risk management in finance (\textbf{Middle}) and health economics (\textbf{Right}). }
    \label{fig:combined_all}
    \vspace{-10pt}
\end{figure*}

\subsection{Health Economics}\label{sec:decision}
In the medical world, it is important to compare the cost and the relative advantages of conducting extra medical experiments. 
The expected value of partial perfect information (EVPPI) quantifies the expected gain from conducting extra experiments to obtain precise knowledge of some unknown variables \citep{brennan2007calculating}:
\begin{align*}
    \text{EVPPI} = \E \Bigl[\max_c J_c(\theta) \Bigr] - \max_c \E \Bigl[J_c(\theta) \Bigr], \quad J_c(\theta) = \int_{\calX} g_c(x, \theta) \Pb_\theta(dx)
\end{align*}
where $c \in \mathcal{C}$ is a set of potential treatments and $g_c$ measures the potential outcome of treatment $c$. EVPPI consists of $|\mathcal{C}| + 1$ nested expectations.

We adopt the same experimental setup as delineated in \cite{Giles2019}, wherein $x$ and $\theta$ have a joint 19-dimensional Gaussian distribution, meaning that the conditional distribution $\Pb_\theta$ is also Gaussian. 
The specific meanings of all $x$ and $\theta$ are outlined in \Cref{tab:mytable}.
All these variables are independent except that $\theta_1, \theta_2, x_6, x_{14}$ are pairwise correlated with a correlation coefficient $0.6$.
We are interested in estimating the EVPPI of a binary decision-making problem ($\calC = \{1, 2\}$) with $g_1(x, \theta)=10^4 (\theta_1 x_5 x_6 + x_7 x_8 x_{9})-(x_1 + x_2 x_3 x_4)$ and $g_2(x, \theta) = 10^4 (\theta_2 x_{13} x_{14} + x_{15} x_{16} x_{17})-(x_{10} + x_{11} x_{12} x_4)$. 
The ground truth EVPPI under this setting is $I=538$ provided in \cite{giles2019decision}.

For estimating $I_1$ with NKQ, we select $k_\calX$ to be Gaussian kernel and $k_\Theta$ to be Mat\'ern-1/2 kernel, because $I_1$ contains a \textit{max} function which breaks the smoothness so we use Mat\'ern-1/2 kernel to be conservative. 
For estimating $I_{2,c}$ with NKQ, we select both to be Gaussian kernels because both $g_1, g_2$ and the probability densities are all infinitely times continuously differentiable. 
We have access to the closed-form KME for both Mat\'ern-1/2 and Gaussian kernels under a Gaussian distribution from \textit{Probnum}~\cite{Wenger2021}.

\begin{table}[t]
\centering
\begin{tabular}{
>{\centering\arraybackslash}p{1.5cm}
>{\centering\arraybackslash}p{1cm}
>{\centering\arraybackslash}p{1cm}
>{\centering\arraybackslash}p{5cm}}
\toprule
Variables & Mean & Std & Meaning \\
\midrule
$X_1$ & 1000 & 1.0 & Cost of treatment \\
$X_2$ & 0.1 & 0.02 & Probability of admissions \\
$X_3$ & 5.2 & 1.0 & Days of hospital \\
$X_4$ & 400 & 200 & Cost per day \\
$X_5$ & 0.3 & 0.1 & Utility change if response \\
$X_6$ & 3.0 & 0.5 & Duration of response \\
$X_7$ & 0.25 & 0.1 & Probability of side effects \\
$X_8$ & -0.1 & 0.02 & Change in utility if side effect \\
$X_{9}$ & 0.5 & 0.2 & Duration of side effects \\
$X_{10}$ & 1500 & 1.0 & Cost of treatment \\
$X_{11}$ & 0.08 & 0.02 & Probability of admissions \\
$X_{12}$ & 6.1 & 1.0 & Days of hospital \\
$X_{13}$ & 0.3 & 0.05 & Utility change if response \\
$X_{14}$ & 3.0 & 1.0 & Duration of response \\
$X_{15}$ & 0.2 & 0.05 & Probability of side effects \\
$X_{16}$ & -0.1 & 0.02 & Change in utility if side effect \\
$X_{17}$ & 0.5 & 0.2 & Duration of side effects \\
$\theta_1$ & 0.7 & 0.1 & Probability of responding \\
$\theta_2$ & 0.8 & 0.1 & Probability of responding \\
\bottomrule

\end{tabular}
\vspace{5pt}
\caption{Variables in the health economics experiment.}
\label{tab:mytable}
\end{table}

