\paragraph{Additional notations}
For two normed vector spaces $A, B$, $A \cong B$ means that $A$ and $B$ are norm equivalent, i.e. their sets coincide and the corresponding norms are equivalent. In other words, there are constants $c_1,c_2>0$ such that $c_1\|h\|_{A} \leq\|h\|_{B} \leq c_2\|h\|_{A}$ holds for all $h \in A$, written as $\| \cdot \|_A \cong \| \cdot \|_B$.
For $A \subseteq B$, $A$ is said to be continuously embedded in $B$ if the inclusion map between them is continuous, written as $A \hookrightarrow B$.
$\|T \|$  denotes the norm of an operator $T: A\to B$.
For a function $f:\calX \subseteq \R^d \to \R$ and $\alpha \in \mathbb{N}^d$, we use $\partial_x^\alpha f$ to denote the standard derivative $\partial x_1^{\alpha_1} \cdots \partial x_n^{\alpha_n} f$ and $D_x^\alpha f$ to denote the weak derivative.
For $f\in W_2^s(\calX)$, we use $\|f\|_{s,2} \coloneq \|f\|_{W_2^s(\calX)}$ to denote its Sobolev space norm.
$\lesssim$ means $\leq$ up to some positive multiplicative constants. 


\section{Existing Results on Kernel Ridge Regression}

In this section, we present \Cref{prop:krr_bias} to \ref{prop:krr_all} which are adaptation of theorems from \citet{fischer2020sobolev} applied to Sobolev spaces. These propositions are foundations of the proof of \Cref{thm:main} in \Cref{sec:proof}.

In the standard regression setting, we are given $N$ observations $\{x_i, y_i\}_{i=1}^N$ which are i.i.d sampled from an unknown joint distribution $\mathbf{P}$ on $\calX \times \R$.
Here, $\calX \subset \R^d$ is a compact domain. 
The marginal distribution of $\mathbf{P}$ on $\calX$ is $\pi$, and the conditional distribution $\mathbf{P}(\cdot \mid x)$ satisfies the Bernstein moment condition~\citep{fischer2020sobolev}. In other words, there exists constants $\sigma, L > 0$ independent of $x$ such that
\begin{align}\label{eq:mom}
    \int_{\mathbb{R}} \left|y -h^*(x)\right|^m \mathbf{P}(d y \mid x) \leq \frac{1}{2} m!\sigma^2 L^{m-2}
\end{align}
is satisfied for $\pi$-almost all $x \in \calX$ and all $m \geq 2$. For example,  \eqref{eq:mom} is satisfied with $\sigma=L=\sigma_0$ when $\mathbf{P}(\cdot \mid x)$ is a Gaussian distribution with bounded variance $\sigma_0$. 
Additionally, \eqref{eq:mom} is also satisfied when there is no noise in the observation so $\sigma=L=0$, which will be discussed in \Cref{sec:noiseless_krr}.

In a regression problem, the target of interest is the Bayes predictor $h^\ast : \calX \to \R$, $x \mapsto \E[Y \mid X = x]$. 
One way of estimating $h^\ast$ is through kernel ridge regression~\citep{fischer2020sobolev}: given a reproducing kernel $k:\calX \times \calX \to \R$, the kernel ridge regression estimator $\hat{h}_\lambda: \calX \to \R$ is defined as the solution to the following optimization problem ($\lambda > 0$):
\begin{align}\label{eq:opt}
    \hat{h}_\lambda
    = \argmin_{h \in \calH_k} \left\{ \lambda\|h\|_{\calH_k}^2 + \frac{1}{N} \sum_{i=1}^N \left( y_i - h (x_i) \right)^2 \right\}.
\end{align}
$\calH_k$ is the reproducing kernel Hilbert space (RKHS) associated with a kernel $k$.
Fortunately, it has the following closed-form expression~\citep[Section 7]{gretton2013introduction} 
% \fxb{Need to make sure we use the same notation for identity matrix as in the main text.}
\begin{align*}
    \hat{h}_\lambda = k(\cdot, x_{1:N}) \left( k(x_{1:N}, x_{1:N}) + N \lambda \Id_N \right)^{-1} y_{1:N} .
\end{align*}
We also introduce an auxiliary function $h_\lambda: \calX \to \calY$ which is the solution to another optimization problem:
\begin{align}\label{eq:opt_dummy}
    h_\lambda = \argmin_{f \in \calH_k } \left\{ \lambda\|f\|_\calH^2 + \int_{\calX \times \R} ( y - f(x))^2 \mathbf{P}(dx, dy) \right\} .
\end{align}
In regression setting, it is of interest to study the generalization error between the estimator $\hat{h}_\lambda$ and the Bayes optimal predictor $h^\ast$, $\| \hat{h}_\lambda - h^\ast \|_{L_2(\pi)}$, and particularly its asymptotic rate of convergence towards $0$ as the number of samples $N$ tend to infinity.
The generalization error can be decomposed into two terms, through a triangular inequality,
\begin{align}\label{eq:bias_variance_decompose}
    \| \hat{h}_\lambda - h^\ast \|_{L_2(\pi)} \leq \| \hat{h}_\lambda - h_\lambda \|_{L_2(\pi)} + \left\| h_\lambda  - h^\ast \right\|_{L_2(\pi)},
\end{align}
where the first term $\| \hat{h}_\lambda - h_\lambda \|_{L_2(\pi)}$ is known as the estimation error and the second term $ \left\| h_\lambda  - h^\ast \right\|_{L_2(\pi)} $ is known as the approximation error.
Next, we are going to present propositions that study these two terms separately under the following list of conditions.
\begin{enumerate}[leftmargin=1.0cm]
    \item [(S1)] $k$ is a Sobolev reproducing kernel of smoothness $s > \frac{d}{2}$.
    \customlabel{as:kernel}{(S1)} 
    \item [(S2)] $\pi$ is a probability measure on $\calX$ with density $p: \calX \to \R$. There exist positive constants $G_0, G_1$ such that $G_0 \leq p(x) \leq G_1$ for any $x \in \calX$. 
    \customlabel{as:density}{(S2)} 
    \item [(S3)] The Bayes predictor $h^\ast \in W_2^s(\calX)$.
    \customlabel{as:bayes_predictor}{(S3)} 
    \item [(S4)] There exists universal constants $\sigma, L > 0$ such that \eqref{eq:mom} holds. 
    \customlabel{as:noise}{(S4)} 
\end{enumerate}

\begin{prop}[Approximation error]\label{prop:krr_bias}
Under Assumptions \ref{as:kernel}-\ref{as:noise}, 
\begin{align*}
    \left\| h_\lambda - h^\ast \right\|_{L_2(\pi)} \leq \left\| h^\ast \right\|_{s,2} \lambda^{\frac{1}{2}} .
\end{align*}
\end{prop}
\begin{proof}
    This is direct application of Lemma 14 of \cite{fischer2020sobolev} with $\beta = 1$ and $ \gamma = 0$.
\end{proof}
\begin{prop}[Estimation error]\label{prop:krr_variance}
Suppose Assumptions \ref{as:kernel}-\ref{as:noise} hold. Let $\calN(\lambda)$ be the effective dimension defined in \Cref{lem:dof},
and $k_{\alpha}$ be defined in \Cref{lem:embedding}.
If $N > A_{\lambda, \tau}$, then with probability at least $1 - 4e^{-\tau}$,
\begin{align}
    \| h_\lambda - \hat{h}_\lambda \|_{L_2(\pi)}^2 \leq \frac{576 \tau^2}{N} \left( L^2 D \lambda^{ -\frac{d}{2s} } + M^2 \lambda^{1 - \frac{d}{2s}} \left\| h^\ast \right \|_{s,2}^2 + M^2 \frac{L_\lambda^2}{N} \lambda^{-\frac{d}{2s}} \right),
\end{align}
where $D$ and $M$ are constants independent of $N$, and $g_\lambda, A_{\lambda, \tau}, L_\lambda$ are defined as follows
\begin{align*}
    g_\lambda &:=\log \left(2 e \mathcal{N}(\lambda) \frac{ \|\Sigma_\pi \| + \lambda}{\|\Sigma_\pi \|} \right), 
    \quad A_{\lambda, \tau} := 8 k_{\alpha}^2 \tau g_\lambda \lambda^{-\frac{d}{2s} }, \\
    L_\lambda &:= \max \left \{ L, \lambda^{\frac{1}{2} - \frac{d}{4s} } \left( \| h^\ast \|_{L_\infty(\pi)} + k_{\alpha} \|  h^\ast  \|_{s,2} \right) \right\} .
\end{align*}

\end{prop}
\begin{proof}
This proposition is a special case of Theorem 16 in \citet{fischer2020sobolev} under the following adaptations towards our Sobolev space setting: 1) \Cref{lem:dof} proves that $\calN(\lambda) \leq D \lambda^{ -\frac{d}{2s} }$ and \Cref{lem:embedding} proves that $k_{\alpha} \leq M$ for $\alpha = \frac{d}{2s}$.
2) $\|h^\ast- h_\lambda \|_{L_{\infty}(\pi)}$ is upper bounded by $\lambda^{\frac{1}{2} - \frac{d}{4s} } \left( \| h^\ast \|_{L_\infty(\pi)} + k_{\alpha} \| h^\ast \|_{s,2} \right) $ proved in Corollary 15 of \citet{fischer2020sobolev}. $\| \Sigma_\pi\|$ is the norm of the covariance operator defined in \eqref{eq:covariance_operator}.
\end{proof}
\begin{prop}\label{prop:krr_all}
Suppose Assumptions \ref{as:kernel}-\ref{as:noise} hold. For $A_{\lambda, \tau}$ and $L_\lambda$ defined above in \Cref{prop:krr_variance}, if $N > A_{\lambda, \tau}$, then with probability at least $1 - 4e^{-\tau}$,
\begin{align*}
    \left\| \hat{h}_\lambda - h^\ast \right\|_{L_2(\pi)}^2 \leq \frac{576 \tau^2}{N} \left( L^2 D \lambda^{ -\frac{d}{2s} } + M^2 \lambda^{1 - \frac{d}{2s}} \left\| h^\ast \right \|_{s,2}^2  + 2 M^2  \frac{L_\lambda^2}{N} \lambda^{-\frac{d}{2s}} \right) + \left\|h^\ast\right\|_{s,2}^2 \lambda .
\end{align*}
\end{prop}
\begin{proof}
By the triangle inequality in \eqref{eq:bias_variance_decompose}, combining \Cref{prop:krr_bias} and \Cref{prop:krr_variance} finishes the proof.
\end{proof}

\section{Noiseless Kernel Ridge Regression (Kernel Quadrature)}\label{sec:noiseless_krr}
In this section, we present the upper bound on the generalization error $\| h^\ast - \hat{h}_{\lambda} \|_{L_2(\pi)}$ in \Cref{prop:krr_all} adapted to the noiseless regression setting, which will be employed in the proof of \Cref{thm:main} in the next section.
Our proof follows the outline of the proof for Theorem 1 in \cite{fischer2020sobolev}, modified for our choice of regularization parameter $\lambda$.
Note that this section is of independent interest to some readers as it presents the first standalone proof on the convergence rate of kernel quadrature that 1): it allows positive regularization parameter $\lambda > 0$ and 2): it provides convergence in high probability rather than in expectation.
The closely-related work is \citet{Bach2015} which requires i.i.d samples from an intractable distribution; and
\citet{long2024duality} which provides a more general analysis on noiseless kernel ridge regression in both well-specified and mis-specified setting.

Suppose we have $N$ observations $x_{1:N}$ which are i.i.d sampled from an unknown distribution $\pi$ on $\calX$ along with $N$ \emph{noiseless} function evaluations $h^\ast(x_{1:N})$ where $h^\ast : \calX \subset \R^d \to \R$. 
The setting appears for instance when the measurement of the output
values is very accurate, or when the output values are obtained as a result of computer experiments. 

\begin{prop}\label{prop:noiseless_krr}
Let $\mathcal{X} \subset \mathbb{R}^d$ be compact, and $x_{1:N}$ be $N$ i.i.d. samples from $\pi$. Define
$\hat{h}_{\lambda_N}(\cdot) \coloneq k(\cdot, x_{1:N}) \left( k(x_{1:N}, x_{1:N}) + N \lambda_N \Id_N \right)^{-1} h^\ast(x_{1:N})$, and suppose conditions \ref{as:kernel}-\ref{as:noise} are satisfied. Then, if $\lambda_N \asymp N^{-\frac{2 s}{d}} (\log N)^{\frac{2s+2}{d}}$, there exists an $N_0>0$ such that for all $N > N_0$, 
\begin{align}
\| h^\ast - \hat{h}_{\lambda_N} \|_{L_2(\pi)} \leq \mathfrak{C} \tau N^{-\frac{s}{d}} (\log N)^{\frac{s+1}{d}} \left\| h^\ast \right \|_{s,2}
\end{align} 
holds with probability at least $1 - 4e^{-\tau}$, for a constant $\mathfrak{C}=\mathfrak{C}(\calX, G_0, G_1)$ that only depends on $\calX, G_0, G_1$.
\end{prop}
\begin{proof}
Notice that $ \hat{h}_{\lambda_N} $ is precisely the solution to the optimization problem defined in  \eqref{eq:opt} only with $y_i$ replaced by $h^\ast(x_i)$. 
Similarly, we define $h_{\lambda_N}$ as the solution to the optimization problem defined in \eqref{eq:opt_dummy} only with $y$ replaced by $h^\ast(x)$. 
Note that Assumption \ref{as:noise} is instantly satisfied with $L = 0$. 

Similar to the proof of \Cref{prop:krr_all}, we decompose the generalization error into an estimation error term $\| h_{\lambda_N} - \hat{h}_{\lambda_N} \|_{L_2(\pi)}$ and an approximation error term $\| h_{\lambda_N} - h^\ast \|_{L_2(\pi)}$. 

\paragraph{Approximation error}
Take $\lambda_N \asymp N^{-\frac{2s}{d}} (\log N)^{\frac{2s+2}{d}}$, then from \Cref{prop:krr_bias}, we have 
\begin{align*}
    \left\| h_{\lambda_N} - h^\ast \right\|_{L_2(\pi)} \leq \lambda_N^{\frac{1}{2}} \| h^\ast \|_{s,2} \asymp N^{-\frac{s}{d}}  (\log N)^{\frac{s+1}{d}} \| h^\ast \|_{s,2}.
\end{align*}
\paragraph{Estimation error}
Recall all the constants $g_{\lambda_N}$, $A_{\lambda_N, \tau}$ and $ L_{\lambda_N}$ defined in \Cref{prop:krr_variance}.
Since $L=0$, we know the constant $L_{\lambda_N} = \lambda_N^{\frac{1}{2} - \frac{d}{4s} } \left( \| h^\ast \|_{L_\infty(\pi)} + k_{\alpha} \|  h^\ast  \|_{s,2} \right)$.
In order to apply \Cref{prop:krr_variance}, we need to check there indeed exists $N_0$ such that $N \geq A_{\lambda_N, \tau}$ is satisfied for all $N \geq N_0$. 
To this end, we are going to verify that $\lim_{N \to \infty} A_{\lambda_N, \tau} / N \to 0$. 
Notice that
\begin{align*}
    \lim_{N \to \infty} \frac{A_{\lambda_N, \tau}}{N} = \frac{8 k_{\alpha}^2 \tau g_{\lambda_N} \lambda_N^{-\frac{d}{2s} }}{N} &= 8 (\log N)^{- \frac{s+1}{s}} k_{\alpha}^2 \tau \log \left( 2 e \mathcal{N}(\lambda_N) \frac{\| \Sigma_\pi \| + \lambda_N}{ \| \Sigma_\pi \| } \right) 
\end{align*}
where $\calN(\lambda_N)$ and $k_{\alpha}^2$ are defined in \Cref{lem:dof} and \Cref{lem:embedding}.

Since $\lim_{N \to \infty} \lambda_N = \lim_{N \to \infty} N^{-\frac{2s}{d}} (\log N)^{\frac{2s+2}{d}} = 0$, there exists $N^\prime$ such that $\lambda_N \leq \| \Sigma_\pi \|$ for all $N \geq N^\prime$. 
Therefore, as $N$ tends to infinity,
\begin{align}\label{eq:ratio_N_A}
\begin{aligned}
    \lim_{N \to \infty} \frac{A_{\lambda_N, \tau}}{N} &\leq \lim_{N \to \infty} 8 (\log N)^{- \frac{s+1}{s}} k_{\alpha}^2 \tau \log \left(4 e \mathcal{N}(\lambda_N) \right) \\
    &\leq \lim_{N \to \infty} 8 (\log N)^{- \frac{s+1}{s}} k_{\alpha}^2 \tau \log \left(4 e D \lambda_N^{-\frac{d}{2s}} \right) \\
    &= \lim_{N \to \infty} 8 (\log N)^{- \frac{s+1}{s}} k_{\alpha}^2 \tau \log \left(4 e D \right)  + \lim_{N \to \infty} 8 (\log N)^{- \frac{s+1}{s}} k_{\alpha}^2 \tau \log \left( N (\log N)^{-\frac{s+1}{s}} \right)  \\
    &\leq \lim_{N \to \infty} 16 (\log N)^{- \frac{s+1}{s}} k_{\alpha}^2 \tau \log \left( N \right) \\
    &= 0,
\end{aligned}
\end{align}
where $M$ and $D$ are constants defined in \Cref{lem:dof} and \Cref{lem:embedding}. 
So there exists $N''$ such that $N \geq A_{\lambda_N, \tau}$ for all $N \geq N''$. Taking $N_0 = \max \{ N^\prime, N'' \}$, then we have $N \geq A_{\lambda_N, \tau}$ for all $N \geq N_0$. From \Cref{prop:krr_variance}, we know that with probability at least $1 - 4e^{-\tau}$, 
\begin{align*}
    &\quad \| h_{\lambda_N} - \hat{h}_{\lambda_N} \|_{L_2(\pi)}^2 \\ &\leq \frac{576 \tau^2}{N} \left( M^2 \lambda_N^{1 - \frac{d}{2s}} \left\| h^\ast \right \|_{s,2}^2  + M^2 \lambda_N^{1 - \frac{d}{2s} } \left( \|h^\ast \|_{L_\infty(\pi)} + M \| h^\ast  \|_{s,2} \right)^2 \frac{1}{N} \lambda_N^{-\frac{d}{2s}} \right) \\
    &\asymp \frac{576 \tau^2}{N} \left( M^2 N^{1 - \frac{2s}{d}} (\log N)^{\frac{s+1}{s} \frac{2s-d}{d}} \left\| h^\ast \right \|_{s,2}^2  + M^2 \left( \|h^\ast \|_{L_\infty(\pi)} + M \| h^\ast  \|_{s,2} \right)^2 N^{1 -\frac{2s}{d}} (\log N)^{\frac{s+1}{2s} \frac{4s-d}{d}} \right) \\
    &\leq 576 \tau^2 N^{-\frac{2s}{d}} (\log N)^{\frac{2s+2}{d} } \left( M^2 \left\| h^\ast \right \|_{s,2}^2  + M^2 \left( \|h^\ast \|_{L_\infty(\pi)} + M \| h^\ast  \|_{s,2} \right)^2  \right) .
\end{align*}
So we have, with probability at least $1 - 4e^{-\tau}$, 
\begin{align*}
    \| h_{\lambda_N} - \hat{h}_{\lambda_N} \|_{L_2(\pi)} \leq 24 \tau N^{-\frac{s}{d}} (\log N)^{\frac{s+1}{d} } \left( (M + M^2) \left\| h^\ast \right \|_{s,2} + M \|h^\ast \|_{L_\infty(\pi)} \right).
\end{align*}
\paragraph{Combine approximation and estimation error}
Combining the above two inequalities on approximation error $\| h_{\lambda_N} - h^\ast \|_{L_2(\pi)}$ and estimation error $\| h_{\lambda_N} - \hat{h}_{\lambda_N} \|_{L_2(\pi)}$, we have with probability at least $1 - 4e^{-\tau}$, 
\begin{align*}
    \| h^\ast - \hat{h}_{\lambda_N} \|_{L_2(\pi)} \leq 24 \tau N^{-\frac{s}{d}} (\log N)^{\frac{s+1}{d}} \left( (1 + M + M^2) \left\| h^\ast \right \|_{s,2} + M \|h^\ast \|_{L_\infty(\pi)} \right).
\end{align*}
Finally, following the arguments of \Cref{lem:embedding} that the operator norm of $W_2^s(\calX) \hookrightarrow L_\infty(\calX) $ is bounded, we have $ \|h^\ast \|_{L_\infty(\pi)} \leq R \left\| h^\ast \right \|_{s,2}$ where $R$ is a constant that depends on $\calX, G_0, G_1$. With probability at least $1 - 4e^{-\tau}$,
\begin{align*}
    \| h^\ast - \hat{h}_{\lambda_N} \|_{L_2(\pi)} \leq 24 \tau N^{-\frac{s}{d}} (\log N)^{\frac{s+1}{d}} (1 + (1 + R) M + M^2) \left\| h^\ast \right \|_{s,2}  = \mathfrak{C} \tau N^{-\frac{s}{d}} (\log N)^{\frac{s+1}{d}} \| h^\ast\|_{s,2},
\end{align*}
for $\mathfrak{C} := 24 (1 + (1 + R) M + M^2)$, which concludes the proof.
\end{proof}
\begin{cor}\label{cor:kq_rate}
    Let $\mathcal{X}$ be a compact domain in $\mathbb{R}^d$ and $x_{1:N}$ are $N$ i.i.d samples from $\pi$. 
    $\hat{I}_\kq \coloneq \E_{X \sim \pi}[k(X, x_{1:N})] \left( k(x_{1:N}, x_{1:N}) + N \lambda_N \Id_N \right)^{-1} h^\ast(x_{1:N})$ is the KQ estimator defined in \eqref{eq:kq}.
    Suppose conditions (A1)-(A3) are satisfied. Take $\lambda_N \asymp N^{-\frac{2 s}{d}} (\log N)^{\frac{2s+2}{d}}$, then there exists $N_0>0$ such that for $N > N_0$, 
    \begin{align}
    \left| \hat{I}_\kq - \int_\calX h^\ast(x) d\pi(x) \right| \leq \mathfrak{C} \tau N^{-\frac{s}{d}} (\log N)^{\frac{s+1}{d}} 
    \end{align} 
holds with probability at least $1 - 4e^{-\tau}$. 
% Here $M$, $R$ are two constants that depend on $\calX, G_0, G_1$.   
Here $\mathfrak{C}$ is a constant that is independent of $N$. 
\end{cor}
The proof of \Cref{cor:kq_rate} is a direct application of \Cref{prop:noiseless_krr} after observing the following.
\begin{align*}
    \left| \hat{I}_\kq - \int h^\ast(x) d\pi(x) \right| \leq \int_\calX \left| \hat{h}_{\lambda_{N}}(x) - h^\ast(x) \right| d \pi(x) = \| h^\ast - \hat{h}_{\lambda_N} \|_{L_1(\pi)} \leq \| h^\ast - \hat{h}_{\lambda_N} \|_{L_2(\pi)} .
\end{align*}

\begin{rem}
    We prove in \Cref{prop:noiseless_krr} that the generalization error of $\hat{h}_{\lambda_N}$ in noiseless regression setting is $\tilde{\calO}(N^{-\frac{s}{d}})$, which is faster than the minimax optimal rate $\calO(N^{-\frac{s}{2s+d}})$ in standard regression setting. The fast rate is expected because we are in the noiseless regime so ``overfitting" is not a problem --- hence our choice of regularization parameter $\lambda_N \asymp N^{-\frac{2s}{d}} (\log N)^{\frac{2s+2}{d}}$ decays to $0$ at a faster rate than $\lambda_N \asymp N^{-\frac{2s}{2s + d}}$ in standard kernel ridge regression~\citep[Corollary 5]{fischer2020sobolev}. 
    The $\tilde{\calO}(N^{-\frac{s}{d}})$ rate is also optimal (up to logarithm terms) and cannot be further improved because it matches the lower bound of interpolation (Sections 1.3.11
    and 1.3.1 of \citet{novak2006deterministic},  Section 1.2, Chapter V of \citet{ritter2000average}). 
\end{rem}

\begin{rem}[Comparison to existing upper bound of kernel (Bayesian) quadrature] \label{rem:stage_one_error_and_standard_kq}
The upper bound in \Cref{cor:kq_rate} matches existing analysis based on scattered data approximation in the literature of both kernel quadrature and Bayesian quadrature~\citep{sommariva2006numerical, Briol2019PI, wynne2021convergence} and is known to be minimax optimal~\citep{novak2016some, novak2006deterministic}. 
Existing analysis takes $\lambda = 0$ and requires the Gram matrix $ k(x_{1:N}, x_{1:N})$ to be invertible, in contrast, our result allows a positive regularization parameter $\lambda_N \asymp N^{-\frac{2s}{d}} (\log N)^{\frac{2s+2}{d}}$ which improves numerical stability of matrix inversion in practice. 
Closely-related work is \citet{Bach2015} which requires i.i.d samples from an intractable distribution; and
\citet{long2024duality} which provides a more general analysis on noiseless kernel ridge regression in both well-specified and mis-specified setting.
\end{rem}