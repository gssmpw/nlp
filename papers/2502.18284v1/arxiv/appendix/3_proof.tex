\section{Proof of \Cref{thm:main}}\label{sec:proof}
\begin{rem}
    In this section, we use $p(x; \theta)$ to denote the density $p_\theta(x)$ so that we can use $p(x; \cdot)$ to denote the mapping $\theta \mapsto p_\theta(x)$. Although we introduce a shorthand notation of kernel mean embedding in the main text, $\mu_\pi = \mathbb{E}_{X \sim \pi}[k(X,\cdot)]$, in this section we are going to write it out with its explicit formulation.
\end{rem}

For any $\theta \in \Theta$, $\hat{F}_{\kq}: \Theta \to \R$  and $\hat{J}_{\kq}: \Theta \to \R$ are two functions that generalize the definition of $\hat{F}_{\kq}(\theta_t)$ and $\hat{J}_{\kq}(\theta_t)$ in \eqref{eq:F_J_KQ} to all $\theta \in \Theta$. 
To be more specific, for any $\theta \in \Theta$, given samples $x_{1:N}^{(\theta)} := \big[ x_1^{(\theta)}, \ldots, x_N^{(\theta)} \big]^\top$ consisting of $N$ i.i.d. samples from $\Pb_\theta$,
\begin{align}\label{eq:hat_F_KQ_all_theta}
    \hat{J}_{\kq} (\theta; x_{1:N}^{(\theta)}) &:= \left( \int_\calX k_\calX (x, x_{1:N}^{(\theta)}) d \Pb_{\theta}(x) \right) \left( k_\calX (x_{1:N}^{(\theta)}, x_{1:N}^{(\theta)}) + N \lambda_\calX \Id_N \right)^{-1} g(x_{1:N}^{(\theta)}, \theta), \\
    \quad \hat{F}_{\kq}(\theta; x_{1:N}^{(\theta)}) &:= f(\hat{J}_{\kq} (\theta; x_{1:N}^{(\theta)})),
\end{align}
where we explicitly specify the dependence of samples $x_{1:N}^{(\theta)}$ on $\theta$ in the above two equations. 
Next, we define 
\begin{align}\label{eq:bar_F_KQ_all_theta}
\begin{aligned}
    \bar{J}_{\kq} (\theta) &:= \E_{x_{1:N}^{(\theta)} \sim \Pb_\theta} \left[\hat{J}_{\kq} (\theta; x_{1:N}^{(\theta)})\right] = \int \hat{J}_{\kq} (\theta; x_{1:N}) \prod_{i=1}^N p(x_i; \theta) dx_{1:N}, \\ 
    \bar{F}_{\kq} (\theta) &:= \E_{x_{1:N}^{(\theta)} \sim \Pb_\theta} \left[\hat{F}_{\kq} (\theta; x_{1:N}^{(\theta)})\right] = \int \hat{F}_{\kq} (\theta; x_{1:N}) \prod_{i=1}^N p(x_i; \theta) dx_{1:N},
\end{aligned}
\end{align}
which marginalize out the dependence on samples $x_{1:N}^{(\theta)}$. 
We can see that $\bar{J}_{\kq} \in L_2(\Qb)$ since $g(x, \cdot) \in W_2^{s_\Theta}(\Theta) \subset L_2(\Theta) \cong L_2(\Qb)$ from Assumption \ref{as:equivalence} and \ref{as:app_true_J_smoothness}; and $p(x_i; \cdot) \in L_2(\Qb)$. Also $\bar{F}_{\kq} \in L_2(\Qb)$ because $f$ is Lipschitz continuous from Assumption \ref{as:app_lipschitz}.
Therefore, the absolute error $| I - \hat{I}_{\nkq}|$ can be decomposed as follows:
\begin{align}\label{eq:stage_i_stage_ii_decomposition}
& \quad \left| I - \hat{I}_{\nkq} \right| \nonumber \\
& = \left| \int_\Theta F(\theta) q(\theta) d\theta  - \left( \int_\Theta k_\Theta(\theta, \theta_{1:T}) q(\theta) d\theta  \right) \left( k_\Theta(\theta_{1:T}, \theta_{1:T}) + T \lambda_\Theta \Id_T \right)^{-1} \hat{F}_{\kq}(\theta_{1:T}) \right| \nonumber \\
&\leq \left| \int_\Theta F(\theta) q(\theta) d\theta  - \int_\Theta \bar{F}_{\kq}(\theta) q(\theta) d\theta  \right| \nonumber \\
& \quad\quad + \left| \int_\Theta \bar{F}_{\kq}(\theta) q(\theta) d\theta  - \left( \int_\Theta k_\Theta(\theta, \theta_{1:T}) q(\theta) d\theta  \right) \left( k_\Theta(\theta_{1:T}, \theta_{1:T}) + T \lambda_\Theta \Id_T \right)^{-1} \hat{F}_{\kq}(\theta_{1:T}) \right| \nonumber \\
&\leq \underbrace{  \E_{\theta \sim \Qb} \left[ \left| F(\theta) - \bar{F}_{\kq}(\theta) \right| \right]}_{\text{Stage I error}} + \underbrace{ \left \| \bar{F}_{\kq}(\cdot) - k(\cdot, \theta_{1:T}) (k_\Theta(\theta_{1:T}, \theta_{1:T}) + T \lambda_\Theta \Id_T )^{-1} \hat{F}_{\kq}(\theta_{1:T}) \right \|_{L_2(\Qb )} }_{\text{Stage II error}}  .
\end{align}
The last inequality holds because $\|\cdot\|_{L_1(\Qb)} \leq \|\cdot\|_{L_2(\Qb)}$. Next, we analyze Stage I error and Stage II error separately.

\paragraph{Stage I Error}
From Assumption \ref{as:app_lipschitz}, $f$ is Lipschitz continuous and the Lipschitz constant is bounded by $S_4$,
\begin{align}\label{eq:lipschitz_F}
     \left| \bar{F}_{\kq}(\theta) - F(\theta) \right| &= \left| \E_{x_{1:N}^{(\theta)} \sim \Pb_\theta} \hat{F}_{\kq} (\theta; x_{1:N}^{(\theta)}) - F(\theta) \right| \nonumber \\
     &\leq \E_{x_{1:N}^{(\theta)} \sim \Pb_\theta} \left| \hat{F}_{\kq} (\theta; x_{1:N}^{(\theta)}) - F(\theta) \right| \nonumber \\
     &\leq S_4 \E_{x_{1:N}^{(\theta)} \sim \Pb_\theta} \left| \hat{J}_{\kq}(\theta; x_{1:N}^{(\theta)}) - J(\theta) \right| ,
\end{align}
where the first inequality holds by Jensen inequality and the last inequality holds by Lipschitz continuity of $f$.
Define 
\begin{align}
\label{eq:defn_of_g}
    \hat{g}(x, \theta; x_{1:N}^{(\theta)}) = k_\calX(x, x_{1:N}^{(\theta)}) (k_\calX(x_{1:N}^{(\theta)}, x_{1:N}^{(\theta)}) + N \lambda_\calX \Id_N)^{-1} g(x_{1:N}^{(\theta)}, \theta) .
\end{align}
Here $\hat{g}(\cdot, \theta; x_{1:N}^{(\theta)}) \in L_2(\Pb_\theta)$ because the Sobolev reproducing kernel $k_\calX$ is bounded and measurable; and $g(\cdot, \theta) \in L_2(\Pb_\theta)$ by Assumption \ref{as:app_true_J_smoothness}. Thus, 
\begin{align}\label{eq:F_L_g}
    \left| \hat{J}_{\kq}(\theta; x_{1:N}^{(\theta)}) - J(\theta) \right| &= \left| \int ( \hat{g}(x, \theta; x_{1:N}^{(\theta)}) - g(x, \theta) ) p(x; \theta) dx \right| \leq \left\| \hat{g}(\cdot, \theta; x_{1:N}^{(\theta)}) - g(\cdot, \theta) \right\|_{L_2(\Pb_\theta)} .
\end{align}
Based on Assumption~\ref{as:app_true_g_smoothness}, $g(\cdot, \theta) \in W_2^{s_\calX}(\calX)$ for any $\theta \in \Theta$.
Therefore, based on \Cref{prop:noiseless_krr}, if one takes $\lambda_{\calX, N} \asymp N^{-2 \frac{s_\calX}{d_\calX}} (\log N)^{\frac{2s_\calX+2}{d_\calX}}$, then there exists $N_0$ such that for $N > N_0$,
\begin{align}\label{eq:high_prob_hat_g_g}
    \left\| \hat{g}(\cdot, \theta; x_{1:N}^{(\theta)}) - g(\cdot, \theta) \right\|_{L_2( \Pb_\theta ) } \leq \mathfrak{C} \tau N^{-\frac{s_\calX}{d_\calX}} (\log N)^{\frac{s_\calX+1}{d_\calX}} \| g(\cdot, \theta)\|_{s_\calX,2},
\end{align}
holds with probability at least $1 - 4 e^{-\tau}$. The probability is taken over the distribution of $x_{1:N}^{(\theta)}$, i.e $\Pb_\theta$.
Here $\mathfrak{C}$ is a constant independent of $N$. 
Hence, with \Cref{lem:prob_to_expectation}, we have
\begin{align}\label{eq:F_KQ_F}
     \E_{x_{1:N}^{(\theta)} \sim \Pb_\theta} \left\| \hat{g}(\cdot, \theta; x_{1:N}^{(\theta)}) - g(\cdot, \theta) \right\|_{L_2( \Pb_\theta ) } \leq \mathfrak{C} N^{-\frac{s_\calX}{d_\calX}} (\log N)^{\frac{s_\calX+1}{d_\calX}} \| g(\cdot, \theta)\|_{s_\calX,2}.
\end{align}
By plugging the above inequality back into \eqref{eq:F_L_g}, we obtain
\begin{align*}
    \E_{x_{1:N}^{(\theta)} \sim \Pb_\theta} \left| \hat{J}_{\kq}(\theta; x_{1:N}^{(\theta)}) - J(\theta) \right| \leq \mathfrak{C} N^{-\frac{s_\calX}{d_\calX}} (\log N)^{\frac{s_\calX+1}{d_\calX}} \| g(\cdot, \theta)\|_{s_\calX,2}.
\end{align*}
Therefore, the Stage I error can be upper bounded by
\begin{align}\label{eq:stage_1}
 \E_{\theta \sim \Qb} \left| F(\theta) - \bar{F}_{\kq}(\theta) \right| 
&\leq S_4 \E_{x_{1:N}^{(\theta)} \sim \Pb_\theta} \left| \hat{J}_{\kq}(\theta; x_{1:N}^{(\theta)}) - J(\theta) \right| \| g(\cdot, \theta)\|_{s_\calX,2} \nonumber \\
&\leq S_4 S_1 \mathfrak{C} N^{-\frac{ s_\calX}{d_\calX}} (\log N)^{\frac{s_\calX+1}{d_\calX}} \nonumber \\
&= C_3 N^{-\frac{ s_\calX}{d_\calX}} (\log N)^{\frac{s_\calX+1}{d_\calX}},
\end{align}
where $C_3 := S_4 S_1 \mathfrak{C}$ is a constant independent of $N$.

\paragraph{Stage II Error}
The upper bound on the stage II error is done in five steps. In step one, we prove that $\hat{J}_{\kq}(\cdot; x_{1:N}^{(\theta)}) \in W_2^{s_\Theta} (\Theta)$ given fixed samples $x_{1:N}^{(\theta)}$. In step two, we show that $J \in W_2^{s_\Theta} (\Theta)$. 
In step three, we upper bound $\|\hat{J}_{\kq}(\cdot; x_{1:N}^{(\theta)})\|_{s_\Theta,2}$ through the triangular inequality that
$\|\hat{J}_{\kq}(\cdot; x_{1:N}^{(\theta)})\|_{s_\Theta,2} \leq \|J\|_{s_\Theta,2} + \|J - \hat{J}_{\kq}(\cdot; x_{1:N}^{(\theta)})\|_{s_\Theta,2}$.
In step four, we upper bound $\bar{F}_{\kq}(\theta) = \E_{x_{1:N}^{(\theta)}}\left[f\left( \hat{J}_{\kq}(\cdot; x_{1:N}^{(\theta)}) \right)\right]$ through marginalizing out the samples $x_{1:N}^{(\theta)}$. In the last step, we use kernel ridge regression bound proved in \Cref{prop:krr_all} to upper bound the stage II error.

\underline{\emph{Step One.}} In this step, we are going to show that $\hat{J}_{\kq}$ lies in the Sobolev space $W_2^{s_\Theta} (\Theta)$ given fixed samples $x_{1:N}^{(\theta)}$. 
Notice that the dependence of $\hat{J}_{\kq}(\theta)$ on $\theta$ is through two mappings: $\theta \mapsto \int_\calX k_\calX (x, x_{1:N}^{(\theta)}) p(x; \theta) dx$ and $\theta \mapsto g(x_{1:N}^{(\theta)}, \theta)$. 
We are going to show that $ \theta \mapsto \int_\calX k_\calX (x, x_i^{(\theta)}) p(x; \theta) dx$ lies in the Sobolev space $W_2^{s_\Theta} (\Theta)$ for any $i \in \{1, \ldots, N\}$. 
To this end, we are going to demonstrate it possesses weak derivatives up to and including order $s_\Theta$ that lie in $\calL^2(\Theta)$.
Take $\varphi : \Theta \to \R$ to be any infinitely differentiable function with compact support in $\Theta$ (commonly denoted as $\varphi \in C_c^\infty(\Theta)$), with its standard, non-weak derivative of order $\beta$ denoted by $\partial^\beta \varphi$. Since $\theta \mapsto p(x; \theta) \in W_2^{s_\Theta}(\Theta)$, for any $| \beta| \leq s_\Theta$ it has a weak derivative $\theta \mapsto D_\theta^\beta p(x; \theta) \in \calL^2(\Theta)$. Then,
\begin{align}\label{eq:weak_derivative}
\begin{aligned}
    &\quad \int_\Theta \varphi(\theta) \int_\calX k_\calX (x, x_i^{(\theta)}) D_\theta^\beta p(x; \theta) dx \stackrel{(i)}{=} \int_\calX k_\calX (x, x_i^{(\theta)}) \int_\Theta \varphi(\theta) D_\theta^\beta p(x; \theta) d\theta dx \\
    &\stackrel{(ii)}{=} (-1)^{|\beta|} \int_\calX k_\calX (x, x_i^{(\theta)}) \int_\Theta \partial^\beta \varphi(\theta) p(x; \theta) d\theta dx \stackrel{(iii)}{=} (-1)^{|\beta|} \int_\Theta \partial^\beta \varphi(\theta) \int_\calX k_\calX (x, x_i^{(\theta)}) p(x; \theta) dx d\theta. 
\end{aligned}
\end{align}
In the above chain of derivations, we are allowed to swap the integration order in $(i)$ by the Fubini theorem~\citep{rudin1964principles} because $k_\calX$ is bounded and the fact that $\theta \mapsto \varphi(\theta) \cdot D_\theta^\beta p(x; \theta) \in L_1(\Theta)$ since $D_\theta^\beta p(x; \cdot) \in L_2(\Theta)$ (Assumption \ref{as:app_true_J_smoothness}) and $\varphi \in L_2(\Theta)$; $(ii)$ holds by definition of weak derivatives for $D_\theta^\beta p(x; \theta)$; and $(iii)$ holds again by the Fubini theorem. By definition of weak derivatives,~\eqref{eq:weak_derivative} shows that $\int_\calX k_\calX (x, x_i^{(\theta)}) p(x; \theta) dx$ has a weak derivative of order $\beta$ of the form
\begin{equation*}
    D_\theta^\beta \left[\int_\calX k_\calX (x, x_i^{(\theta)}) p(x; \theta) dx \right] = \int_\calX k_\calX (x, x_i^{(\theta)}) D_\theta^\beta p(x; \theta) dx 
\end{equation*}
Also, since $k_\calX$ is bounded and $\theta \mapsto D_\theta^\beta p(x; \theta) \in \calL^2(\Theta)$, the weak derivative above is in $\calL^2(\Theta)$.
Consequently, we have
\begin{align*}
    \sum_{ | \beta| \leq s_\Theta }  \int_\Theta \left| D_\theta^\beta \int_\calX k_\calX (x, x_i^{(\theta)}) p(x; \theta) dx \right|^2 d \theta 
    &= \sum_{ | \beta| \leq s_\Theta }  \int_\Theta \left| \int_\calX k_\calX (x, x_i^{(\theta)}) D_\theta^\beta p(x; \theta) dx  \right|^2 d \theta \\
    &\stackrel{(i)}{\leq} \text{Vol}(\calX) \sum_{ | \beta| \leq s_\Theta }  \int_\Theta \int_\calX \left|  k_\calX (x, x_i^{(\theta)}) D_\theta^\beta p(x; \theta) dx  \right|^2 d \theta \\
    &\stackrel{(ii)}{\leq} \text{Vol}(\calX) \sum_{ | \beta| \leq s_\Theta }  \kappa^2 \int_\Theta \int_\calX \left| D_\theta^\beta p(x; \theta) \right|^2 dx d \theta \\
    &\stackrel{(iii)}{=} \text{Vol}(\calX) \kappa^2 \int_\calX \left\| p(x; \cdot) \right\|_{s_\Theta, 2}^2 dx .
\end{align*}
In the above chain of derivations, $(i)$ holds because $| \int_\calX f(x) dx |^2 \leq \text{Vol}(\calX) \int_\calX | f(x) |^2 dx $ for compact $\calX$, $(ii)$ holds because $k_\calX$ is upper bounded by $\kappa$ and $\int_\Theta |D_\theta^\beta p(x; \theta)|^2 d\theta < \infty$ from Assumption \ref{as:app_true_J_smoothness}, $(iii)$ holds because 
$p(x; \cdot) \in W_2^{s_\Theta}(\Theta)$ for any $x \in \calX$ based on Assumption \ref{as:app_true_J_smoothness}. Also, one can interchange the order of integration in $(iii)$ by the Fubini's theorem~\citep{rudin1964principles}.

As a result, for any $i, j \in \{1, \ldots, N\}$, we have $f_{1, i}: \theta \mapsto \int_\calX k_\calX (x, x_i^{(\theta)}) p(x; \theta) dx \in W_2^{s_\Theta}(\Theta)$ and $f_{2, j}: \theta \mapsto g(x_j^{(\theta)}, \theta) \in W_2^{s_\Theta}(\Theta)$ from Assumption \ref{as:app_true_J_smoothness}. 
Therefore, we know from \Cref{lem:sobolev_algebra} that their product $f_{1, i} \cdot f_{2,j} \in W_2^{s_\Theta} (\Theta)$ hence $\hat{J}_{\kq}$ as a linear combination of $f_{1, i} \cdot f_{2,j}$ is in $W_2^{s_\Theta} (\Theta)$.


\underline{\emph{Step Two.}} In this step, we are going to show that $J: \theta \mapsto \int_\calX g(x, \theta)p(x; \theta) dx$ is also in the Sobolev space $W_2^{s_\Theta} (\Theta)$. 
Since both $g(x, \cdot)\in W_2^{s_\Theta}(\Theta)$ and $p(x; \cdot)\in W_2^{s_\Theta}(\Theta)$, we know from \Cref{lem:sobolev_algebra} that $\theta \mapsto g(x, \theta) \cdot p(x,\theta) \in W_2^{s_\Theta}(\Theta)$. 
By following the same steps as in \eqref{eq:weak_derivative}, we obtain that for any $| \beta| \leq s_\Theta$, 
\begin{align}\label{eq:interchange_integral_derivative}
    D_\theta^\beta \int_\calX g(x, \theta) p(x; \theta) dx = \int_\calX D_\theta^\beta \Big( g(x, \theta) p(x; \theta) \Big) dx .
\end{align}
We are now ready to study the Sobolev norm of $J$,
\begin{align}\label{eq:J}
    \left\| J \right\|_{s_\Theta, 2}^2 
    &:= \sum_{ | \beta| \leq s_\Theta } \int_\Theta \left| D_\theta^\beta \int_\calX p(x; \theta) g(x, \theta) dx \right|^2 d\theta \nonumber \\
    &\stackrel{(i)}{=} \sum_{ | \beta| \leq s_\Theta } \int_\Theta \left| \int_\calX D_\theta^\beta \Big( p(x; \theta) g(x, \theta) \Big) dx \right|^2 d \theta \nonumber \\
    &\stackrel{(ii)}{\leq} \text{Vol}(\calX) \int_\calX \sum_{ | \beta| \leq s_\Theta } \int_\Theta \left| D_\theta^\beta \Big( p(x; \theta) g(x, \theta) \Big) \right|^2 d \theta dx \nonumber \\
    &\stackrel{(iii)}{=} \text{Vol}(\calX) \int_\calX \left\| p(x; \cdot) g(x, \cdot)  \right\|_{s_\Theta, 2}^2 dx \nonumber \\
    &\stackrel{(iv)}{\leq} \text{Vol}(\calX)^2  S_2^2 S_3^2
\end{align}
Here, $(i)$ holds by \eqref{eq:interchange_integral_derivative}, $(ii)$ holds since $| \int_\calX f(x) dx |^2 \leq \text{Vol}(\calX) \int_\calX | f(x) |^2 dx $ for compact $\calX$, 
$(iii)$ follows from the definition of Sobolev norm, and $(iv)$ holds by \Cref{lem:sobolev_algebra} and Assumption \ref{as:app_true_J_smoothness} that $\left\| g(x, \cdot) \right\|_{s_\Theta, 2} \leq S_2$, $\left\| p(x; \cdot) \right\|_{s_\Theta, 2} \leq S_3$. 

\underline{\emph{Step Three.}}
In this step, we study the Sobolev norm of $\hat{J}_{\kq}$ for some fixed $x_{1:N}^{(\theta)}$, by upper bounding it with $\| J - \hat{J}_{\kq}(\cdot ; x_{1:N}^{(\theta)}) \|_{s_\Theta, 2} + \| J \|_{s_\Theta, 2}$. 
Since $g(x_i^{(\theta)}, \cdot) \in W_2^{s_\Theta}(\Theta) $ by Assumption \ref{as:app_true_J_smoothness}, it holds that $\hat{g}(x, \cdot; x_{1:N}^{(\theta)}) = k_\calX(x, x_{1:N}^{(\theta)}) (k_\calX(x_{1:N}^{(\theta)}, x_{1:N}^{(\theta)}) + N \lambda_\calX \Id_N)^{-1} g(x_{1:N}^{(\theta)}, \cdot)$ is in $W_2^{s_\Theta}(\Theta)$ for any fixed $x_{1:N}^{(\theta)}$. Therefore,
\begin{align}\label{eq:p_g_hat_g}
    \left\| p(x; \cdot) \Big( g(x, \cdot) - \hat{g}(x, \cdot; x_{1:N}^{(\theta)}) \Big) \right\|_{s_\Theta, 2} 
    &\leq  \left\| p(x; \cdot) \right\|_{s_\Theta, 2} \left\| g(x, \cdot) - \hat{g}(x, \cdot; x_{1:N}^{(\theta)}) \right\|_{s_\Theta, 2} \nonumber \\
    &\leq  S_3 \left\| g(x, \cdot) - \hat{g}(x, \cdot; x_{1:N}^{(\theta)}) \right\|_{s_\Theta, 2},
\end{align}
where the first inequality holds by \Cref{lem:sobolev_algebra} and the second inequality holds by Assumption \ref{as:app_true_J_smoothness} that $\left\| p(x; \cdot) \right\|_{s_\Theta, 2} \leq S_3$. Now, we consider the Sobolev norm of $\hat{J}_{\kq} - J$, 
\begin{align}\label{eq:hat_J_J}
    \left\| \hat{J}_{\kq}(\cdot ; x_{1:N}^{(\theta)}) - J \right\|_{s_\Theta, 2}^2 
    &= \sum_{ | \beta| \leq s_\Theta } \int_\Theta \left| D_\theta^\beta \int_\calX p(x; \theta) \left( g(x, \theta) - \hat{g}(x, \theta ; x_{1:N}^{(\theta)}) \right) dx \right|^2 d\theta \nonumber \\
    &\stackrel{(i)}{=} \sum_{ | \beta| \leq s_\Theta } \int_\Theta \left| \int_\calX D_\theta^\beta \Big( p(x; \theta) \left( g(x, \theta) - \hat{g}(x, \theta; x_{1:N}^{(\theta)}) \right) \Big) dx \right|^2 d \theta \nonumber \\
    &\stackrel{(ii)}{\leq} \text{Vol}(\calX) 
    \int_\calX \sum_{ | \beta| \leq s_\Theta } \int_\Theta \left| D_\theta^\beta \Big( p(x; \theta) \left( g(x, \theta) - \hat{g}(x, \theta ; x_{1:N}^{(\theta)}) \right) \Big) \right|^2 d \theta dx \nonumber \\
    &\stackrel{(iii)}{=} \text{Vol}(\calX) 
    \int_\calX \left\| p(x; \cdot) \Big( g(x, \cdot) - \hat{g}(x, \cdot ; x_{1:N}^{(\theta)}) \Big) \right\|_{s_\Theta, 2}^2 dx \nonumber \\
    &\stackrel{(iv)}{\leq} \text{Vol}(\calX)  S_3^2 \int_\calX \left\| g(x, \cdot) - \hat{g}(x, \cdot ; x_{1:N}^{(\theta)}) \right\|_{s_\Theta, 2}^2 dx,
\end{align}
where the above chain of derivations (i) --- (iv) follow the exact same reasoning as \eqref{eq:weak_derivative} and \eqref{eq:J}. Next, notice that
\begin{align}\label{eq:g_g_hat_sobolev}
    \int_\calX \left\| g(x, \cdot) - \hat{g}(x, \cdot; x_{1:N}^{(\theta)}) \right\|_{s_\Theta, 2}^2 dx &= \int_\calX \sum_{ | \beta| \leq s_\Theta } \int_\Theta \left| D_\theta^\beta \left( g(x, \theta) - \hat{g}(x, \theta; x_{1:N}^{(\theta)}) \right) \right|^2 d \theta dx \nonumber \\
    &= \sum_{ | \beta| \leq s_\Theta } \int_\Theta \left\| D_\theta^\beta  g(\cdot, \theta) - D_\theta^\beta \hat{g}(\cdot, \theta; x_{1:N}^{(\theta)}) \right\|_{L_2(\calX)}^2 d\theta .
\end{align}
% 
By Assumption \ref{as:app_true_g_smoothness}, $D_\theta^\beta  g(\cdot, \theta) \in W_2^{s_\calX}(\calX)$ for any $|\beta| \leq s_\Theta$. Therefore, by applying~\Cref{prop:noiseless_krr} with $h^\ast(\cdot):= D_\theta^\beta g(\cdot, \theta)$, and $\hat{h}_{\lambda}(\cdot) := D_\theta^\beta \hat{g}(\cdot, \theta; x_{1:N}^{(\theta)}) = k_\calX(\cdot, x_{1:N}^{(\theta)}) (k_\calX(x_{1:N}^{(\theta)}, x_{1:N}^{(\theta)}) + N \lambda_\calX \Id_N)^{-1} D_\theta^\beta g(x_{1:N}^{(\theta)}, \theta; x_{1:N}^{(\theta)})$, we get that
\begin{align}\label{eq:D_g_D_g_hat}
    \left\| D_\theta^\beta  g(\cdot, \theta) - D_\theta^\beta \hat{g}(\cdot, \theta; x_{1:N}^{(\theta)}) \right\|_{L_2(\Pb_\theta)} \leq \mathfrak{C} \tau N^{-\frac{s_\calX}{d_\calX}} (\log N)^{\frac{s_\calX+1}{d_\calX}} \left\| D_\theta^\beta g(\cdot, \theta) \right \|_{s_\calX,2}
\end{align}
holds with probability at least $ 1 - 4e^{-\tau}$, for a $\mathfrak{C}$ that only depends on $\calX, G_{0, \calX}, G_{1, \calX}$. 
From Assumption 
\ref{as:equivalence}, we know that $L_2(\Pb_\theta) \cong L_2(\calX)$ (they are norm equivalent) and $\|f\|_{L_2(\calX)} \leq \text{Vol}(\calX)^{-1} G_{0, \calX}^{-1} \|f\|_{L_2(\Pb_\theta)}$ for any $f \in L_2(\Pb_\theta)$. 
Therefore, for any $\theta \in \Theta$ and any $|\beta| \leq s_\Theta$, with probability at least $ 1 - 4e^{-\tau}$,
\begin{align}\label{eq:D_g_D_g_hat_lebesgue}
    \left\| D_\theta^\beta  g(\cdot, \theta) - D_\theta^\beta \hat{g}(\cdot, \theta; x_{1:N}^{(\theta)}) \right\|_{L_2(\calX)} &\leq \mathfrak{C} \tau \text{Vol}(\calX)^{-1} G_{0, \calX}^{-1} N^{-\frac{s_\calX}{d_\calX}}  (\log N)^{\frac{s_\calX+1}{d_\calX}} \left\| D_\theta^\beta g(\cdot, \theta) \right \|_{s_\calX,2} \nonumber \\
    &\leq \mathfrak{C} \tau \text{Vol}(\calX)^{-1} G_{0, \calX}^{-1} N^{-\frac{s_\calX}{d_\calX}}  (\log N)^{\frac{s_\calX+1}{d_\calX}} S_1.
\end{align}
By plugging \eqref{eq:D_g_D_g_hat_lebesgue} into \eqref{eq:g_g_hat_sobolev}, and then plugging the result into \eqref{eq:hat_J_J}, we get that with probability at least $ 1 - 4e^{-\tau}$, 
\begin{align}\label{eq:hat_j_j_sobolev}
    \left\| \hat{J}_{\kq}(\cdot; x_{1:N}^{(\theta)}) - J \right\|_{s_\Theta, 2} &\leq \left(\sum_{|\beta| \leq s_\Theta } 1 \right) \mathfrak{C} \tau 
    G_{0, \calX}^{-1} \text{Vol}(\calX)^{-1}  S_1 S_3 N^{-\frac{s_\calX}{d_\calX}}  (\log N)^{\frac{s_\calX+1}{d_\calX}} \nonumber \\
    &= \binom{s_\Theta + d_\Theta -1}{d_\Theta -1} \mathfrak{C} \tau 
    G_{0, \calX}^{-1} \text{Vol}(\calX)^{-1}  S_1 S_3 N^{-\frac{s_\calX}{d_\calX}}  (\log N)^{\frac{s_\calX+1}{d_\calX}}.
\end{align}
By combining this result with the bound $\left\| J \right\|_{s_\Theta, 2} \leq \text{Vol}(\calX)  S_2 S_3$ proven in \eqref{eq:J}, we get that with probability at least $ 1 - 4e^{-\tau}$ and any $N>N_0$ it holds that
\begin{align}\label{eq:hat_J_sobolev_norm}
    \left\| \hat{J}_{\kq}(\cdot; x_{1:N}^{(\theta)}) \right\|_{s_\Theta, 2} &\leq \left\| \hat{J}_{\kq}(\cdot; x_{1:N}^{(\theta)}) - J \right\|_{s_\Theta, 2} + \left\| J \right\|_{s_\Theta, 2} \nonumber \\
    &\leq \binom{s_\Theta + d_\Theta -1}{d_\Theta -1} \mathfrak{C}  \tau 
    G_{0, \calX}^{-1} \text{Vol}(\calX)^{-1}  S_1 S_3 N^{-\frac{s_\calX}{d_\calX}}  (\log N)^{\frac{s_\calX+1}{d_\calX}} + \text{Vol}(\calX)  S_2 S_3 \nonumber \\
    &\leq 2 \text{Vol}(\calX)  S_2 S_3,
\end{align}
where $N_0$ is defined as the smallest integer for which the first term is subsumed by the second term.

\underline{\emph{Step Four.}} 
In this step, we are going to upper bound the Sobolev norm of $\bar{F}_{\kq}$.
From Chapter 5, Exercise 16 of \cite{evans2022partial}, we have 
$\hat{F}_{\kq} = f \circ \hat{J}_{\kq} $ is in $W_2^{s_\Theta} (\Theta)$ because $f$ has bounded derivatives up to including $s_\Theta+1$ and $\|\hat{J}(\cdot; x_{1:N}^{(\theta)})\|_{s_\Theta, 2} \leq \text{Vol}(\calX)  S_2 S_3$ with probability at least $1- 4e^{-\tau}$ proved in \eqref{eq:hat_J_sobolev_norm}. 
Hence, $\|\hat{F}_{\kq}(\cdot; x_{1:N}^{(\theta)})\|_{s_\Theta, 2} \leq C_6$ holds with probability at least $1- 4e^{-\tau}$.
Next, recall the definition of $\bar{F}_{\kq}(\theta)$ in \eqref{eq:bar_F_KQ_all_theta},
\begin{align*}
    \bar{F}_{\kq}(\theta) &= \mathbb{E}_{x_{1: N}^{(\theta)} \sim \mathbb{P}_\theta}\left[\hat{F}_{\mathrm{KQ}}\left(\theta ; x_{1: N}^{(\theta)}\right)\right] \\
    &= \int_{\calX} \cdots \int_{\calX}  \hat{F}_{\kq}(\theta; x_{1:N}^{(\theta)}) p(x_1^{(\theta)}; \theta) p(x_2^{(\theta)}; \theta) \cdots p(x_N^{(\theta)}; \theta) d x_1^{(\theta)} d x_2^{(\theta)} \cdots d x_N^{(\theta)} .
\end{align*}
For any $i = 1, \ldots, N$, we know that $\| p(x_i^{(\theta)}; \cdot) \|_{s_\Theta, 2} \leq S_3$ from Assumption \ref{as:app_true_J_smoothness} and $\| \hat{F}_{\kq}(\cdot; x_{1:N}^{(\theta)}) \|_{s_\Theta, 2} \leq C_6$ proved above.
Therefore, from \Cref{lem:sobolev_algebra} we have $\| p(x_i^{(\theta)}; \cdot) \hat{F}_{\kq}(\cdot; x_{1:N}^{(\theta)})\|_{s_\Theta, 2}$ is bounded, so $x_i^{(\theta)} \mapsto p(x_i^{(\theta)}; \cdot) \hat{F}_{\kq}(\cdot; x_{1:N}^{(\theta)})$ is Bochner integrable with respect to the Lebesgue measure $\calL_\calX$. 
From \Cref{lem:integral_in_hilbert}, we have,
\begin{align}\label{eq:bar_F_norm}
    \left\| \bar{F}_{\kq} \right\|_{s_\Theta, 2}
    &\leq \int_{\calX} \cdots \int_{\calX} \left\| \hat{F}_{\kq}(\cdot; x_{1:N}^{(\theta)}) \prod_{n=1}^N p(x_n^{(\theta)}; \cdot)  \right\|_{s_\Theta, 2} d x_1^{(\theta)} d x_2^{(\theta)} \cdots d x_N^{(\theta)}  \nonumber \\
    & \leq \int_{\calX} \cdots \int_{\calX} \left\| \hat{F}_{\kq}(\cdot; x_{1:N}^{(\theta)}) \right\|_{s_\Theta, 2} \left( \prod_{n=1}^N \| p(x_n^{(\theta)}; \cdot) \|_{s_\Theta, 2} \right) d x_1^{(\theta)} d x_2^{(\theta)} \cdots d x_N^{(\theta)} \nonumber \\
    &\leq C_6 S_3^N \text{Vol}(\calX)^N \nonumber \\
    &\leq C_6 .
\end{align}
The last inequality holds by $S_3\leq1$ from Assumption \ref{as:app_true_J_smoothness} and $\calX=[0,1]^{d_\calX}$ so $\text{Vol}(\calX)=1$.

\underline{\emph{Step Five.}} 
We are now ready to upper bound the stage II error, which was defined as
\begin{align*}
    \text {Stage II error } = \left\|\bar{F}_{\kq}(\cdot) - k\left(\cdot, \theta_{1: T}\right)\left(k_{\Theta}\left(\theta_{1: T}, \theta_{1: T}\right)+T \lambda_{\Theta} \Id_T\right)^{-1} \hat{F}_{\kq}\left(\theta_{1: T}\right)\right\|_{L_2(\Qb)} .
\end{align*}
The idea is to treat the stage II error as the generalization error of kernel ridge regression---which can be bounded via \Cref{prop:krr_all}. Given i.i.d. observations $(\theta_1, \hat{F}_{\kq}(\theta_1, x_{1:N}^{(\theta_1)}) ), \ldots, (\theta_T, \hat{F}_{\kq}(\theta_T, x_{1:N}^{(\theta_T)}) )$, the target of interest in the context of regression is the conditional mean, which in our case is precisely $ \bar{F}_{\kq}(\theta) = \E_{x_{1:N}^{(\theta)} \sim \Pb_\theta} \hat{F}_{\kq} (\theta; x_{1:N}^{(\theta)})$ defined in \eqref{eq:bar_F_KQ_all_theta}. 
Alternatively, $\hat{F}_{\kq}(\theta; x_{1:N}^{(\theta)})$ can be treated as noisy observation of the target function $\bar{F}_{\kq}(\theta)$ where the observation noise is defined as $ r: \Theta \to \R $ with $r(\theta;x_{1:N}^{(\theta)}) = \hat{F}_{\kq}(\theta; x_{1:N}^{(\theta)}) - \bar{F}_{\kq}(\theta)$. So we automatically have $\E_{x_{1:N}^{(\theta)} \sim \Pb_{\theta} }[r(\theta)] = 0$. For any positive integer $m \geq 2$,
\begin{align}\label{eq:bernstein_noise}
    \E_{x_{1:N}^{(\theta)} \sim \Pb_{\theta}} [|r(\theta)|^m] &= \E_{x_{1:N}^{(\theta)} \sim \Pb_{\theta}} \left| \hat{F}_{\kq}(\theta ; x_{1:N}^{(\theta)} ) - \bar{F}_{\kq}(\theta) \right|^m \nonumber \\
    &\stackrel{(i)}{\leq} 2^{m-1} \E_{x_{1:N}^{(\theta)} \sim \Pb_{\theta}} \left| \hat{F}_{\kq}(\theta ; x_{1:N}^{(\theta)} ) - F(\theta) \right|^m + 2^{m-1} \left| \bar{F}_{\kq}(\theta) - F(\theta) \right|^m \nonumber \\
    &= 2^{m-1} \E_{x_{1:N}^{(\theta)} \sim \Pb_{\theta}} \left| \hat{F}_{\kq}(\theta ; x_{1:N}^{(\theta)}) - F(\theta) \right|^m + 2^{m-1} \left| \E_{x_{1:N}^{(\theta)} \sim \Pb_{\theta}} \hat{F}_{\kq}(\theta; x_{1:N}^{(\theta)}) - F(\theta) \right|^m \nonumber \\
    &\leq 2^{m-1} \E_{x_{1:N}^{(\theta)} \sim \Pb_{\theta}} \left| \hat{F}_{\kq}(\theta; x_{1:N}^{(\theta)}) - F(\theta) \right|^m + 2^{m-1} \E_{x_{1:N}^{(\theta)} \sim \Pb_{\theta}} \left| \hat{F}_{\kq}(\theta; x_{1:N}^{(\theta)}) - F(\theta) \right|^m \nonumber \\
    &= 2^m \E_{x_{1:N}^{(\theta)} \sim \Pb_{\theta}} \left| \hat{F}_{\kq}(\theta; x_{1:N}^{(\theta)}) - F(\theta) \right|^m \nonumber \\
    &\leq 2^m S_4^m \E_{x_{1:N}^{(\theta)} \sim \Pb_{\theta}} \left| \hat{J}_{\kq}(\theta; x_{1:N}^{(\theta)}) - J(\theta) \right|^m \nonumber \\
    &\stackrel{(ii)}{\leq} 2^m m! S_4^m S_1^m \mathfrak{C}^m N^{-m\frac{s_\calX}{d_\calX}} (\log N)^{m\frac{s_\calX+1}{d_\calX}} .
\end{align}
In the above chain of derivations, $(i)$ holds because $(a+ b)^m \leq 2^{m-1}(a^m + b^m)$. $(ii)$ holds because we know from \eqref{eq:F_L_g} and \eqref{eq:high_prob_hat_g_g} that $| \hat{J}_{\kq}(\theta; x_{1:N}^{(\theta)}) - J(\theta) | \leq \mathfrak{C} \tau N^{-\frac{s_\calX}{d_\calX}} (\log N)^{\frac{s_\calX+1}{d_\calX}} S_1$ holds with probability at least $1 - 4 e^{-\tau}$, and so $\E_{x_{1:N}^{(\theta)} \sim \Pb_{\theta}} | \hat{J}_{\kq}(\theta; x_{1:N}^{(\theta)}) - J(\theta)|^m$ can be bounded via \Cref{lem:prob_to_expectation}. Therefore, by comparing \eqref{eq:bernstein_noise} with \eqref{eq:mom}, we can see that the observation noise $r$ indeed satisfy the Bernstein noise moment condition with 
\begin{align*}
    \sigma = L = 2 S_4 S_1 \mathfrak{C} N^{-\frac{s_\calX}{d_\calX}}(\log N)^{\frac{s_\calX+1}{d_\calX}} = C_7 N^{-\frac{s_\calX}{d_\calX}} (\log N)^{\frac{s_\calX+1}{d_\calX}},
\end{align*} 
for $C_7 := 2 S_4 S_1 \mathfrak{C}$ a constant independent of $N,T$.
Before we employ \Cref{prop:krr_all}, we need to check the Assumptions \ref{as:kernel}---\ref{as:noise}. Assumption \ref{as:kernel} is satisfied for our choice of kernel $k_\Theta$. Assumption \ref{as:density} is satisfied due to Assumption \ref{as:equivalence}. 
Assumption \ref{as:bayes_predictor} is satisfied due to \eqref{eq:bar_F_norm}. Assumption \ref{as:noise} is satisfied for the Bernstein noise moment condition verified above.
Next, we compute all the constants in \Cref{prop:krr_all} in the current context. 
$\calN(\lambda_\Theta)$ is the effective dimension defined in \Cref{lem:dof} upper bounded by $D_\Theta \lambda_\Theta^{- d_\Theta / 2s_\Theta}$, $k_{\alpha}$ with $\alpha = \frac{2s_\Theta}{d_\Theta}$ defined in \Cref{lem:embedding} is upper bounded by a constant $M_\Theta$, 
$\| \Sigma_{\Qb}\|$ is the norm of the covariance operator defined in \eqref{eq:covariance_operator}.  Hence 
\begin{align*}
    g_{\lambda_\Theta} &:=\log \left(2 e \mathcal{N}(\lambda_\Theta) \frac{ \| \Sigma_{\Qb}\| + \lambda_\Theta }{ \| \Sigma_{\Qb}\| } \right), \qquad A_{\lambda_\Theta, \tau} := 8 k_{\alpha}^2 \tau g_{\lambda_\Theta} \lambda_\Theta^{-\frac{d_\Theta}{2s_\Theta} },  \\
    L_{\lambda_\Theta} &:= \max \left \{ L, \lambda_\Theta^{\frac{1}{2} - \frac{d_\Theta}{4s_\Theta} } \left( \| \bar{F}_{\kq} \|_{L_\infty(\Qb)} + k_{\alpha} \| \bar{F}_{\kq} \|_{s_\Theta, 2} \right) \right\} .
\end{align*}
Applying \Cref{prop:krr_all} shows that, for $T > A_{\lambda_\Theta, \tau}$, 
\begin{align}\label{eq:stage_ii_1}
     & \left\|\bar{F}_{\kq}-k\left(\cdot, \theta_{1: T}\right)\left(k_{\Theta}\left(\theta_{1: T}, \theta_{1: T}\right) + T \lambda_{\Theta} \Id_T\right)^{-1} \hat{F}_{\kq}\left(\theta_{1: T}\right)\right\|_{L_2(\Qb)}^2 \nonumber \\
     &\quad \leq \frac{576 \tau^2}{T} \left( L^2 D_\Theta \lambda_\Theta^{ -\frac{d_\Theta}{2s_\Theta} } + M_\Theta^2 \lambda_\Theta^{1 - \frac{d_\Theta}{2s_\Theta}} \left\| \bar{F}_{\kq} \right \|_{s_\Theta, 2}^2  + 2 M_\Theta^2 \frac{L_{\lambda_\Theta}^2}{T} \lambda_\Theta^{-\frac{d_\Theta}{2s_\Theta}} \right) + \left\| \bar{F}_{\kq} \right\|_{s_\Theta, 2}^2 \lambda_\Theta ,
\end{align}
holds with probability at least $1 - 4 e^{-\tau}$.
We take $\lambda_\Theta \asymp T^{-2 \frac{s_\Theta}{d_\Theta}} (\log T)^{\frac{2s_\Theta+2}{d_\Theta}}$, then similar to the derivations from \eqref{eq:ratio_N_A},
\begin{align}\label{eq:T_large_enough}
    \lim_{T \to \infty} \frac{A_{\lambda_\Theta, \tau}}{T} \leq \lim_{T \to \infty} 16 (\log T)^{- \frac{s_\Theta+1}{s_\Theta}} k_{\alpha}^2 \tau \log \left( T\right) = 0 .
\end{align}
It means there exists a finite $T_0 > 0$ such that $T > A_{\lambda_\Theta, \tau}$ holds for any $T > T_0$. Notice that, with probability at least $1-4e^{-\tau}$,
\begin{align}\label{eq:C6_use}
    \| \bar{F}_{\kq} \|_{L_\infty(\Qb)} = \| \bar{F}_{\kq} \|_{L_\infty(\Theta)} \leq R_\Theta \| \bar{F}_{\kq} \|_{s_\Theta, 2} \leq R_\Theta C_6
\end{align} 
based on \eqref{eq:bar_F_norm} and the fact that $W_2^{s_\Theta}(\Theta) \hookrightarrow L_\infty(\Theta)$ with $\|W_2^{s_\Theta}(\Theta) \hookrightarrow L_\infty(\Theta)\| \leq R_\Theta$, we have
\begin{align*}
    L_{\lambda_\Theta} &\leq \max \{ L, T^{-\frac{s_\Theta}{d_\Theta} + \frac{1}{2}} (\log T)^{\frac{s_\Theta+1}{2s_\Theta} \frac{2s_\Theta-d_\Theta}{d_\Theta}} \left( R_\Theta + M_\Theta \right) C_6  \} \\
    &= \max \{ C_7 N^{-\frac{s_\calX}{d_\calX}} (\log N)^{\frac{s_\calX+1}{d_\calX}}, T^{-\frac{s_\Theta}{d_\Theta} + \frac{1}{2}} (\log T)^{\frac{s_\Theta+1}{2s_\Theta} \frac{2s_\Theta-d_\Theta}{d_\Theta}} \left( R_\Theta + M_\Theta \right) C_6 \}.
\end{align*}
We plug back the definition of $L$ and $L_{\lambda_\Theta}$, so the above \eqref{eq:stage_ii_1} can be further upper bounded by 
\begin{small}
\begin{align*}
    &\leq \frac{576 \tau^2}{T} \left( C_7^2 N^{- 2 \frac{s_\calX}{d_\calX}} (\log N)^{\frac{2s_\calX+2}{d_\calX}} D_\Theta T (\log T)^{-\frac{s_\Theta+1}{s_\Theta}} + M_\Theta^2 T^{- \frac{ 2 s_\Theta}{d_\Theta} + 1} (\log T)^{\frac{s_\Theta+1}{s_\Theta} \frac{2s_\Theta-d_\Theta}{d_\Theta}} C_6^2 \right) \\
    &\quad\quad + \frac{576 \tau^2}{T} \cdot 2 M_\Theta^2 \frac{\max \left \{ C_7^2 N^{-2 \frac{s_\calX}{d_\calX}} (\log N)^{\frac{2s_\calX+2}{d_\calX}} , T^{-2 \frac{s_\Theta}{d_\Theta} + 1} (\log T)^{\frac{s_\Theta+1}{s_\Theta} \frac{2s_\Theta-d_\Theta}{d_\Theta}}  \left( R_\Theta + M_\Theta\right)^2 C_6^2 \right\}}{T} T (\log T)^{-\frac{s_\Theta+1}{s_\Theta}} \\
    &\qquad\qquad\qquad + C_6^2  T^{- 2 \frac{ s_\Theta}{d_\Theta}} (\log T)^{\frac{2s_\Theta+2}{d_\Theta}} \\
    &= 576 \tau^2 \left( C_7^2 N^{- 2 \frac{s_\calX}{d_\calX}} (\log N)^{\frac{2s_\calX+2}{d_\calX}} D_\Theta (\log T)^{-\frac{s_\Theta+1}{s_\Theta}} + M_\Theta^2 T^{- \frac{ 2 s_\Theta}{d_\Theta}} (\log T)^{\frac{s_\Theta+1}{s_\Theta} \frac{2s_\Theta-d_\Theta}{d_\Theta}} C_6^2 \right) \\
    &\quad\quad + 576 \tau^2 \cdot 2 M_\Theta^2 \max \left \{ C_7^2 N^{-2 \frac{s_\calX}{d_\calX}} (\log N)^{\frac{2s_\calX+2}{d_\calX}} T^{-1}, T^{-\frac{2s_\Theta}{d_\Theta}} (\log T)^{\frac{s_\Theta+1}{s_\Theta} \frac{2s_\Theta-d_\Theta}{d_\Theta}} \left( R_\Theta + M_\Theta\right)^2  C_6^2 \right\} \cdot (\log T)^{-\frac{s_\Theta+1}{s_\Theta}} \\
    &\qquad\qquad+ C_6^2  T^{- 2 \frac{ s_\Theta}{d_\Theta}} (\log T)^{\frac{2s_\Theta+2}{d_\Theta}} \\
    &\stackrel{(i)}{\leq} 576 \tau^2 C_7^2 N^{- 2 \frac{s_\calX}{d_\calX}} (\log N)^{\frac{2s_\calX+2}{d_\calX}} D_\Theta + 576 \tau^2 M_\Theta^2 T^{- \frac{ 2 s_\Theta}{d_\Theta}} (\log T)^{\frac{2s_\Theta+2}{d_\Theta}} C_6^2 \\
    &\quad\quad + 576 \tau^2 \cdot 2 M_\Theta^2 C_7^2 N^{-2 \frac{s_\calX}{d_\calX}} (\log N)^{\frac{2s_\calX+2}{d_\calX}} + 576 \tau^2 \cdot 2 M_\Theta^2 T^{-\frac{2s_\Theta}{d_\Theta}} (\log T)^{\frac{2s_\Theta+2}{d_\Theta}} \left( R_\Theta + M_\Theta\right)^2  C_6^2 \\
    &\qquad\qquad + C_6^2  T^{- 2 \frac{ s_\Theta}{d_\Theta}} (\log T)^{\frac{2s_\Theta+2}{d_\Theta}}\\
    &=: \tau^2 \left( C_8^2 N^{- \frac{ 2 s_\calX}{d_\calX}} (\log N)^{\frac{2s_\calX+2}{d_\calX}} + C_9^2 T^{- \frac{ 2 s_\Theta}{d_\Theta}} (\log T)^{\frac{2s_\Theta+2}{d_\Theta}} \right).
\end{align*}
\end{small}
$C_8, C_9$ are two constants independent of $N,T$.
In $(i)$, we use $\max\{ a_1, a_2\}\leq a_1 + a_2$, we also use the following
\begin{align*}
    (\log T)^{\frac{s_\Theta+1}{s_\Theta} \frac{2s_\Theta-d_\Theta}{d_\Theta}} \leq (\log T)^{\frac{2s_\Theta+2}{d_\Theta}} , \quad (\log T)^{-\frac{s_\Theta+1}{s_\Theta}} \leq 1 .
\end{align*}
Therefore, we have that, 
\begin{align}\label{eq:stage_2}
    \text{Stage II error} &:= \left\| \bar{F}_{\kq} - k(\cdot, \theta_{1:T}) (k_\Theta(\theta_{1:T}, \theta_{1:T}) + T \lambda_\Theta \Id_T )^{-1} 
    \hat{F}_{\kq}(\theta_{1:T}) \right\|_{L_2( \Qb ) } \nonumber \\
    &\leq \tau \left( C_8 N^{- \frac{ s_\calX}{d_\calX}} (\log N)^{\frac{s_\calX+1}{d_\calX}} + C_9 T^{- \frac{ s_\Theta}{d_\Theta}} (\log T)^{\frac{s_\Theta+1}{d_\Theta}} \right) ,
\end{align}
holds with probability at least $1 - 8 e^{-\tau}$.
\paragraph{Combine stage I and stage II error}
Combining the stage I error of \eqref{eq:stage_1} and the stage II error of \eqref{eq:stage_2}, we obtain
\begin{align*}
    \left| I - \hat{I}_{\nkq} \right| &\leq \text{Stage I error} + \text{Stage II error} \\
    &\leq C_3 N^{-\frac{ s_\calX}{d_\calX}} (\log N)^{\frac{s_\calX+1}{d_\calX}} + \tau \left( C_8 N^{- \frac{ s_\calX}{d_\calX} } (\log N)^{\frac{s_\calX+1}{d_\calX}} + C_9 T^{- \frac{ s_\Theta}{d_\Theta}} (\log T)^{\frac{s_\Theta+1}{d_\Theta}} \right) \\
    &\leq \tau \left( (C_8 + C_3) N^{- \frac{ s_\calX}{d_\calX} } (\log N)^{\frac{s_\calX+1}{d_\calX}} + C_9 T^{- \frac{ s_\Theta}{d_\Theta} }  (\log T)^{\frac{s_\Theta+1}{d_\Theta}} \right) \\
    &=: \tau \left( C_1 N^{- \frac{ s_\calX}{d_\calX} } (\log N)^{\frac{s_\calX+1}{d_\calX}} + C_2 T^{- \frac{ s_\Theta}{d_\Theta} } (\log T)^{\frac{s_\Theta+1}{d_\Theta}} \right), 
\end{align*}
holds with probability at least $1 - 8 e^{-\tau}$.
Here $C_1, C_2$ are two constants independent of $N,T$ so the proof concludes here.


