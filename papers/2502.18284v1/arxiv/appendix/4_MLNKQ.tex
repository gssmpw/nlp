\section{Multi-Level Nested Kernel Quadrature}
In this section, we are going to introduce a novel method that combines nested kernel quadrature (NKQ) with multi-level construction as mentioned in \Cref{sec:theory}.

\subsection{Multi-Level Monte Carlo for Nested Expectation}
First, we briefly review multi-level Monte Carlo (MLMC) applied to nested expectations $I = \E_{\theta \sim \Qb} [ f(\E_{X \sim \Pb_\theta} [ g(X, \theta) ] ) ]$ introduced in Section 9 of \citet{Giles2015} and \citet{giles2019decision}.
At each level $\ell$, we are given $T_\ell$ samples $\theta_{1:T_\ell}$ sampled i.i.d from $\Qb$ and we have $N_\ell$ samples $x_{1:N_\ell}^{(\theta_t)}$ sampled i.i.d from $\Pb_{\theta_t}$ for each $t = 1, \ldots, T_\ell$. 
The MLMC implementation is to construct an estimator $P_\ell$ at each level $\ell$ such that $I$ can be decomposed into the sum of $P_\ell$.
\begin{align*}
    I \approx \E_{\theta \sim \Qb}[P_L] = \E_{\theta \sim \Qb}\left[P_0\right] + \sum_{\ell=1}^L \E_{\theta \sim \Qb}\left[P_{\ell}-P_{\ell-1}\right], \qquad P_{\ell} \coloneq f \left( \frac{1}{N_{\ell}} \sum_{n=1}^{N_\ell} g\left(x_n^{(\theta)}, \theta \right) \right) .
\end{align*}
The estimator $Y_\ell$ for $\E_{\theta \sim \Qb}[P_\ell - P_{\ell-1}]$ is
\begin{align*}
    Y_{\ell} &= \frac{1}{T_\ell} \sum_{t=1}^{T_{\ell}} \left\{ f \left( \frac{1}{N_\ell} \sum_{n=1}^{N_\ell} g\left(x_n^{(t)}, \theta_t\right)\right) - \frac{1}{2} f\left( \frac{1}{N_{\ell-1}} \sum_{n=1}^{N_{\ell-1}} g\left(x_n^{(t)}, \theta_t\right)\right) \right. \\
    &\qquad\qquad \left.- \frac{1}{2} f\left( \frac{1}{N_{\ell-1}} \sum_{n=N_{\ell-1}+1}^{N_\ell} g\left(x_n^{(t)}, \theta_t\right)\right)\right\}, \\
    Y_0 &\coloneq \frac{1}{T_0} \sum_{t=1}^{T_0} f \left( \frac{1}{N_0} \sum_{n=1}^{N_0} g \left(x_n^{(t)}, \theta_t \right) \right) .
\end{align*}
Compared with \eqref{eq:mlmc} in the main text, notice that here we use the `antithetic' approach which further improves the performance of MLMC~\citep[Section 9]{Giles2015}.
The MLMC estimator for nested expectation can be written as 
\begin{align}
    \hat{I}_{\text{MLMC}} \coloneq \sum_{\ell=0}^L Y_\ell .
\end{align}
At each level $\ell$, the cost of $Y_\ell$ is $\calO(N_\ell \times T_\ell)$ and the expected squared error $\E[(Y_\ell - \E_{\theta \sim \Qb}[P_\ell - P_{\ell-1}])^2] = \calO(N_\ell^{-2} \times T_\ell^{-1})$ provided that $f$ has bounded second order derivative~\citep[Section 9]{Giles2015}\footnote{Section 9 of \cite{Giles2015} uses variance $\E[Y_\ell^2]$, which is equivalent to the expected square error since $Y_\ell$ is an unbiased estimate of $\E_{\theta \sim \Qb}[P_\ell - P_{\ell-1}]$.}. 
Here the expectation is taken over the randomness of samples. So the total cost and expected absolute error of MLMC for nested expectation can be written as
\begin{align}\label{eq:cost_error_mlmc}
    \text{Cost} = \calO\left( \sum_{\ell=0}^L N_\ell \times T_\ell \right), \qquad \E|I - \hat{I}_{\mlmc}| = \calO\left( \sum_{\ell=0}^L N_\ell^{-1} \times T_\ell^{-\frac{1}{2}} \right).
\end{align}
Theorem 1 of \cite{Giles2015} shows that, in order to reach error threshold $\Delta$, one can take $N_\ell \propto 2^\ell$ and $T_\ell \propto 2^{-2 \ell} \Delta^{-2}$. Therefore, one has $\E |I - \hat{I}_{\mlmc}| = \calO(\Delta)$ along with $\text{Cost} = \calO(\Delta^{-2})$.


\subsection{Multi-Level Kernel Quadrature for Nested Expectation (MLKQ)}\label{sec:mlnkq}
In this section, we present \emph{multi-level kernel quadrature} applied to nested expectation (MLKQ).
Note that MLKQ is different from the multi-level Bayesian quadrature proposed in \citet{li2023multilevel} because our MLKQ is designed specifically for nested expectations. 
At each level $\ell$, we have $T_\ell$ samples $\theta_{1:T_\ell}$ sampled i.i.d from $\Qb$ and we have $N_\ell$ samples $x_{1:N_\ell}^{(\theta_t)}$ sampled i.i.d from $\Pb_{\theta_t}$ for each $t = 1, \ldots, T_\ell$. 
Different from MLMC above, we define 
\begin{align*}
    I &\approx \E_{\theta \sim \Qb}[P_{\nkq, L}] = \E_{\theta \sim \Qb}\left[P_{\nkq, 0}\right] + \sum_{\ell=1}^L \E_{\theta \sim \Qb}\left[P_{\nkq, \ell} - P_{\nkq, \ell-1} \right], \quad \\ P_{\nkq, \ell} &\coloneq \E_{x_{1:N_\ell}^{(\theta)} \sim \Pb_{\theta} } f \left(\hat{J}_{\kq} \left( \theta; x_{1:N_\ell}^{(\theta)} \right) \right).
\end{align*}
The estimator $Y_{\nkq, \ell}$ for $\E_{\theta \sim \Qb}[P_{\nkq, \ell} - P_{\nkq, \ell-1}]$ when $\ell \geq 1$ is the difference of two nested kernel quadrature estimator defined in \eqref{eq:NKQ_estimator}.
\begin{align*}
    Y_{\nkq, \ell} &\coloneq \E_{\theta \sim \Qb} \left[k_{\Theta}\left(\theta, \theta_{1: T_\ell} \right)\right] \left(\boldsymbol{K}_{\Theta, T_\ell} + T_\ell \lambda_{\Theta, \ell} \Id_{T_\ell} \right)^{-1} \left( \hat{F}_{\kq}\left(\theta_{1: T_\ell}; x_{1:N_\ell}^{(\theta_{1:T_\ell} )} \right) - \hat{F}_{\kq}\left(\theta_{1: T_\ell}; x_{1:N_{\ell - 1} }^{ (\theta_{1:T_\ell}) } \right) \right) 
\end{align*}
where $\hat{F}_{\kq} (\theta_{1: T_{\ell} }; x_{1:N_{\ell} }^{ (\theta_{1:T_{\ell} } ) })$ is a vectorized notation for $[\hat{F}_{\kq} (\theta_{1}; x_{1:N_\ell}^{(\theta_1)}), \ldots, \hat{F}_{\kq} (\theta_{T_{\ell} }; x_{1:N_\ell}^{(\theta_{T_{\ell}})} )] \in \R^{T_{\ell}}$ and similarly for $\hat{F}_{\kq} (\theta_{1: T_{\ell} }; x_{1:N_{\ell-1} }^{ (\theta_{1:T_{\ell} } ) })$.
At level $0$, we define $Y_{\nkq, 0} \coloneq \E_{\theta \sim \Qb} \left[k_{\Theta}(\theta, \theta_{1: T_0})\right] (\boldsymbol{K}_{\Theta, T_0} + T_0 \lambda_{\Theta, 0} \Id_{T_0})^{-1} \hat{F}_{\kq}\left(\theta_{1: T_0} \right)$.
The multi-level nested kernel quadrature estimator is constructed as
\begin{align*}
    \hat{I}_{\text{MLKQ}} \coloneq \sum_{\ell=0}^L Y_{\nkq, \ell} .
\end{align*}
Same as MLMC above, the cost of $Y_{\nkq, \ell}$ is $\calO(N_\ell \times T_\ell)$. The following theorem studies the error $| Y_{\nkq, \ell} - \E_{\theta \sim \Qb}[P_{\nkq, \ell} - P_{\nkq, \ell-1} ]|$. 

\begin{thm}\label{thm:level_nkq}
Let $\calX = [0,1]^{d_\calX}$ and $\Theta = [0,1]^{d_\Theta}$. 
At level $\ell \geq 1$, $\theta_1, \ldots, \theta_{T_\ell}$ are $T_\ell$ i.i.d. samples from $\Qb$ and $x_1^{(t)}, \ldots, x_{N_\ell}^{(t)}$ are $N_\ell$ i.i.d. samples from $\Pb_{\theta_t}$ for all $t \in \{1, \cdots, T_\ell \}$. 
Both kernels $k_\calX$ and $k_\Theta$ are Sobolev reproducing kernels of smoothness $s_\calX > d_\calX / 2$ and $s_\Theta > d_\Theta/2$.
Suppose the Assumptions \ref{as:equivalence}, \ref{as:app_true_g_smoothness}, \ref{as:app_true_J_smoothness}, \ref{as:app_lipschitz} in Theorem 1 hold.
Suppose $2^{\frac{d_\calX}{s_\calX}} N_{\ell - 1} > N_\ell > N_{\ell - 1}$. 
Then, for sufficiently large $N_\ell \geq 1$ and $T_\ell \geq 1$, with $\lambda_{\calX, \ell} \asymp N_\ell^{-2\frac{s_\calX}{d_\calX}} \cdot (\log N_\ell)^{\frac{2s_\calX+2}{d_\calX}}$ and $\lambda_{\Theta, \ell} \asymp T_\ell^{-\frac{2 s_\Theta}{2 s_\Theta +d_\Theta }}$,
\begin{align*}
    \Big| Y_{\nkq, \ell} - \E_{\theta \sim \Qb}[P_{\nkq, \ell} - P_{\nkq, \ell-1} ] \Big|  \lesssim \tau \left(  N_\ell^{- \frac{ s_\calX}{d_\calX}} (\log N)^{\frac{s_\calX+1}{d_\calX}} \times T_\ell^{-\frac{s_\Theta}{2 s_\Theta +d_\Theta }} \right)
\end{align*}
holds with probability at least $1 - 12 e^{-\tau}$. 
\end{thm}
The proof of the theorem is relegated to \Cref{sec:proof_thm_level_nkq}. 
Under \Cref{thm:level_nkq}, the expected error $\E[Y_{\nkq, \ell} - \E_{\theta \sim \Qb}[P_{\nkq, \ell} - P_{\nkq, \ell-1} ] ] = \tilde{\calO}(N_\ell^{-\frac{s_\calX}{d_\calX} } \times T_\ell^{-\frac{s_\Theta}{2 s_\Theta +d_\Theta }})$ based on \Cref{lem:prob_to_expectation}, up to logarithm terms. 
Here, the expectation is taken over the randomness of samples.
Therefore, similarly to \eqref{eq:cost_error_mlmc}, the total cost and expected absolute error of multi-level nested kernel quadrature can be written as 
\begin{align}\label{eq:cost_error_mlnkq}
    \text{Cost} = \calO \left(\sum_{\ell=0}^L N_\ell \times T_\ell\right), \qquad \E|I - \hat{I}_{\text{MLKQ}}| = \tilde{\calO}\left( \sum_{\ell=0}^L N_\ell^{-\frac{s_\calX}{d_\calX}} \times T_\ell^{-\frac{s_\Theta}{2 s_\Theta +d_\Theta }} \right).
\end{align}
If we take $N_\ell \propto 2^{ \frac{d_\calX}{s_\calX} \ell} \Delta^{-\frac{d_\calX}{2 s_\calX}}, T_\ell \propto 2^{- \frac{2 s_\Theta + d_\Theta}{s_\Theta} \ell} \Delta^{- \frac{2 s_\Theta + d_\Theta}{2 s_\Theta}}$, then the error $\E|I - \hat{I}_{\text{MLKQ}}| = \tilde{\calO}(\Delta)$ and the cost is 
\begin{align*}
    \sum_{\ell=0}^L N_\ell \times T_\ell &= \left( \sum_{\ell=0}^L 2^{ \frac{d_\calX}{s_\calX} \ell - \frac{2 s_\Theta + d_\Theta}{s_\Theta} \ell} \right) \cdot \Delta^{-1 - \frac{d_\calX}{2s_\calX} -\frac{d_\Theta}{2s_\Theta}} \\
    &\leq \left( \sum_{\ell=0}^L 2^{ \left(\frac{d_\calX}{s_\calX} - 2 \right) \ell} \right) \cdot \Delta^{-1 - \frac{d_\calX}{2s_\calX} -\frac{d_\Theta}{2s_\Theta}} \\
    &= \calO(\Delta^{-1 - \frac{d_\calX}{2s_\calX} -\frac{d_\Theta}{2s_\Theta}}) .
\end{align*}
Equivalently, to reach error $\calO(\Delta)$, the cost is $\tilde{\calO}(\Delta^{-1 - \frac{d_\calX}{2 s_\calX} -\frac{d_\Theta}{2 s_\Theta}})$.
\begin{rem}[Comparison of MLKQ and MLMC]\label{rem:mlnkq_mlmc}
    To reach a given threshold $\Delta$, 
    the cost of MLKQ is $\tilde{\calO}(\Delta^{-1 - \frac{d_\calX}{2 s_\calX} -\frac{d_\Theta}{2 s_\Theta}})$, which is smaller than the cost of MLMC $\calO(\Delta^{-2})$ when the problem has sufficient smoothness, i.e. when $\frac{d_\calX}{s_\calX} + \frac{d_\Theta}{s_\Theta} < 2$.  
    If we compare \eqref{eq:cost_error_mlmc} and \eqref{eq:cost_error_mlnkq}, the superior performance of MLKQ can be explained by the faster rate of convergence in terms of $N_\ell$ at each level when $\frac{d_\calX}{s_\calX} \leq 1$. 
    Nevertheless, we can see in \eqref{eq:cost_error_mlnkq} that the MLKQ rate at each level in terms of $T_\ell$ is $\calO(T_\ell^{-\frac{s_\Theta}{2 s_\Theta +d_\Theta }})$ which is slower than the MLMC rate $\calO(T_\ell^{-\frac{1}{2}})$ in \eqref{eq:cost_error_mlmc}.  
    An empirical study of MLKQ is included in \Cref{fig:combined_all} which shows that MLKQ is better than MLMC in some settings but both are outperformed by NKQ by a huge margin.
    A more refined analysis of MLKQ is reserved for future work.
\end{rem}

\subsection{Proof of \Cref{thm:level_nkq}}\label{sec:proof_thm_level_nkq}
The proof uses essentially the same analysis as in \underline{\emph{Step Five}} of \Cref{sec:proof} which translates $| Y_\ell - \E_{\theta \sim \Qb}[P_\ell - P_{\ell-1}]|$ into the generalization error of kernel ridge regression. 
First, we know that by following the same derivations as in \eqref{eq:bar_F_norm} that 
\begin{align*}
    \bar{F}_{\kq, \ell}(\theta) &\coloneq \E_{x_{1:N_\ell}^{(\theta)} \sim \Pb_{\theta} }\left[\hat{F}_{\kq} \left( \theta; x_{1:N_\ell}^{(\theta)} \right) \right], \quad \bar{F}_{\kq, \ell} \in W_2^{s_\Theta}(\Theta) \text{  and  } \left\| \bar{F}_{\kq, \ell} \right\|_{s_\Theta} \leq  C_6, \\
    \bar{F}_{\kq, \ell-1}(\theta) &\coloneq \E_{x_{1:N_{\ell-1} }^{(\theta)} \sim \Pb_{\theta} } \left[ \hat{F}_{\kq} \left( \theta; x_{1:N_{\ell-1}}^{(\theta)} \right) \right], \quad \bar{F}_{\kq, \ell-1} \in W_2^{s_\Theta}(\Theta) \text{  and  } \left\| \bar{F}_{\kq, \ell-1} \right\|_{s_\Theta} \leq  C_6 .
\end{align*}
Given $T_\ell$ i.i.d. observations $(\theta_1, \hat{F}_{\kq}(\theta_1, x_{1:N_\ell}^{(\theta_1)}) - \hat{F}_{\kq}(\theta_1, x_{1:N_{\ell-1}}^{(\theta_1)})), \ldots, (\theta_{T_\ell}, \hat{F}_{\kq}(\theta_{T_\ell}, x_{1:N_\ell}^{(\theta_{T_\ell})}) - \hat{F}_{\kq}(\theta_{T_\ell}, x_{1:N_{\ell-1}}^{(\theta_{T_\ell})}))$, the target of interest in the context of regression is the conditional mean, which in our case is precisely 
\begin{align*}
    \theta \mapsto \bar{F}_{\kq, \ell}(\theta) - \bar{F}_{\kq, \ell-1}(\theta) = \E_{x_{1:N_\ell}^{(\theta)} \sim \Pb_{\theta} }\left[\hat{F}_{\kq} \left( \theta; x_{1:N_\ell}^{(\theta)} \right) \right] - \E_{x_{1:N_{\ell-1} }^{(\theta)} \sim \Pb_{\theta} } \left[ \hat{F}_{\kq} \left( \theta; x_{1:N_{\ell-1}}^{(\theta)} \right) \right] .
\end{align*}
Alternatively, $\theta \mapsto \hat{F}_{\kq}(\theta, x_{1:N_\ell}^{(\theta)}) - \hat{F}_{\kq}(\theta, x_{1:N_{\ell-1}}^{(\theta)}))$ can be viewed as noisy observation of the true function $\bar{F}_{\kq, \ell} - \bar{F}_{\kq, \ell-1}$ where the noise satisfied the following condition.
For each $\theta \in \Theta$ and positive integer $m \geq 2$, similar to \eqref{eq:bernstein_noise} we have,
\begin{align*}
    &\quad \E \Big| \left[ \hat{F}_{\kq} \left(\theta; x_{1:N_\ell}^{(\theta)} \right) -  \hat{F}_{\kq}\left(\theta; x_{1:N_{\ell - 1} }^{ (\theta) } \right) \right] \\
    &\qquad\qquad - \left[ \E_{x_{1:N_\ell}^{(\theta)} \sim \Pb_{\theta} } \hat{F}_{\kq} \left( \theta; x_{1:N_\ell}^{(\theta)} \right) - \E_{x_{1:N_{ \ell - 1} }^{(\theta)} \sim \Pb_{\theta} } \hat{F}_{\kq} \left( \theta; x_{1:N_{\ell-1} }^{(\theta)} \right) \right] \Big|^m \\
    &\leq 2^m \E \left| \hat{F}_{\kq} \left(\theta; x_{1:N_\ell}^{(\theta)} \right) 
    -  \E_{x_{1:N_\ell}^{(\theta)} \sim \Pb_{\theta} } \hat{F}_{\kq} \left( \theta; x_{1:N_\ell}^{(\theta)} \right) \right|^m 
    \\
    &\qquad\qquad + 2^m \E \left| \hat{F}_{\kq}\left(\theta; x_{1:N_{\ell - 1} }^{ (\theta) } \right) - \E_{x_{1:N_{ \ell - 1} }^{(\theta)} \sim \Pb_{\theta} } \hat{F}_{\kq} \left( \theta; x_{1:N_{\ell-1} }^{(\theta)} \right) \right|^m \\
    &\lesssim N_\ell^{-m \frac{s_\calX}{d_\calX}} (\log N_\ell)^{m\frac{s_\calX+1}{d_\calX}} + N_{\ell-1}^{-m \frac{s_\calX}{d_\calX}} (\log N_{\ell-1})^{m\frac{s_\calX+1}{d_\calX}} \\
    &\lesssim N_\ell^{-m \frac{s_\calX}{d_\calX}} (\log N_\ell)^{m\frac{s_\calX+1}{d_\calX}},
\end{align*}
where the second last inequality follows by replicating the same steps in \eqref{eq:bernstein_noise}, and the last inequality is true because $2^{ d_\calX / s_\calX} N_{\ell-1} > N_\ell > N_{\ell - 1}$. 
As a result, by replicating the steps for \eqref{eq:stage_ii_1}, we have
\begin{align}\label{eq:Y_l_P_l}
    &\quad \left| Y_{\nkq, \ell} - \E_{\theta \sim \Qb}[P_{\nkq, \ell} - P_{\nkq, \ell-1}] \right|^2 \nonumber \\
    &\leq \left\| \left( \bar{F}_{\kq, \ell} - \bar{F}_{\kq, \ell-1} \right) \right. \nonumber \\
    &\quad \left. - k_{\Theta}\left(\cdot, \theta_{1: T_\ell} \right) \left(\boldsymbol{K}_{\Theta, T_\ell} + T_\ell \lambda_{\Theta, \ell} \Id_{T_\ell} \right)^{-1} \left( \hat{F}_{\kq}\left(\theta_{1: T_\ell}; x_{1:N_\ell}^{(\theta_{1:T_\ell} )} \right) - \hat{F}_{\kq}\left(\theta_{1: T_\ell}; x_{1:N_{\ell - 1} }^{ (\theta_{1:T_\ell}) } \right) \right) \right\|_{L_2(\mathbb{Q})}^2 \nonumber \\
    & \lesssim \tau^2 \left( T_\ell^{-1} \lambda_{\Theta, \ell}^{ -\frac{d_\Theta}{2s_\Theta} } N_\ell^{-\frac{2s_\calX}{d_\calX}} (\log N_\ell)^{\frac{2s_\calX+2}{d_\calX}} + \lambda_{\Theta, \ell}^{1 - \frac{d_\Theta}{2s_\Theta}} T_\ell^{-1} \left\|\bar{F}_{\kq, \ell} - \bar{F}_{\kq, \ell-1} \right\|_{s_{\Theta}}^2 \right. \nonumber \\
    &\qquad + \left. \lambda_{\Theta, \ell}^{-\frac{d_\Theta}{2s_\Theta}} T_\ell^{-1 - \frac{2s_\Theta}{d_\Theta}}  \left\|\bar{F}_{\kq, \ell} - \bar{F}_{\kq, \ell-1} \right\|_{s_{\Theta}}^2 \right) + \left\| \bar{F}_{\kq, \ell} - \bar{F}_{\kq, \ell-1} \right\|_{s_{\Theta}}^2 \lambda_{\Theta, \ell} , 
\end{align}
holds with probability at least $1 - 4e^{-\tau}$. 
Next, we are going to upper bound $ \left\| \bar{F}_{\kq, \ell} - \bar{F}_{\kq, \ell-1} \right\|_{s_{\Theta}}$. To this end, notice that 
\begin{align*}
    \left\| \bar{F}_{\kq, \ell} - \bar{F}_{\kq, \ell-1} \right\|_{s_{\Theta}}^2 \leq 2 \left\| \bar{F}_{\kq, \ell} - F \right\|_{s_{\Theta}}^2 + 2 \left\| \bar{F}_{\kq, \ell-1} - F \right\|_{s_{\Theta}}^2 .
\end{align*}
Using the same steps in \eqref{eq:bar_F_norm} and \eqref{eq:hat_j_j_sobolev} subsequently, we have
\begin{align*}
    \left\| \bar{F}_{\kq, \ell} - F \right\|_{s_{\Theta}} 
    &\leq \left\| \hat{F}_{\kq} \left( \cdot; x_{1:N_\ell}^{(\theta)} \right) - F \right\|_{s_{\Theta}} \cdot S_3^{N_\ell} \cdot \text{Vol}(\calX)^{N_\ell} \\
    &\lesssim \left\| \hat{J}_{\kq} \left( \cdot; x_{1:N_\ell}^{(\theta)} \right) - J \right\|_{s_{\Theta}} \lesssim N_\ell^{-\frac{s_\calX}{d_\calX}} (\log N_\ell)^{\frac{s_\calX+1}{d_\calX}},
\end{align*}
holds with probability at least $1 - 4e^{-\tau}$.
Similarly, we have $\left\| \bar{F}_{\kq, \ell-1} - F \right\|_{s_{\Theta}} \lesssim N_{\ell - 1}^{-\frac{s_\calX}{d_\calX}} (\log N_{\ell-1})^{\frac{s_\calX+1}{d_\calX}}$ which holds with probability at least $1 - 4e^{-\tau}$. 
Consequently, we have $\| \bar{F}_{\kq, \ell} - \bar{F}_{\kq, \ell-1}\|_{s_{\Theta}} \lesssim N_\ell^{-\frac{s_\calX}{d_\calX}} (\log N_{\ell})^{\frac{s_\calX+1}{d_\calX}}$ which holds with probability at least $1 - 8 e^{-\tau}$. Therefore, plugging it back to \eqref{eq:Y_l_P_l}, we obtain
\begin{align*}
    &\qquad \left| Y_{\nkq, \ell} - \E_{\theta \sim \Qb}[P_{\nkq, \ell} - P_{\nkq, \ell-1} ] \right|^2 \\
    &\lesssim \tau^2 \left( T_\ell^{-1} \lambda_{\Theta, \ell}^{ -\frac{d_\Theta}{2s_\Theta} } N_\ell^{-\frac{2s_\calX}{d_\calX}} (\log N_{\ell})^{\frac{2s_\calX+2}{d_\calX}} + \lambda_{\Theta, \ell}^{1 - \frac{d_\Theta}{2s_\Theta}} T_\ell^{-1} N_\ell^{-\frac{2 s_\calX}{d_\calX}} (\log N_{\ell})^{\frac{2s_\calX+2}{d_\calX}} \right. \\
    &\qquad\qquad \left. + \lambda_{\Theta, \ell}^{-\frac{d_\Theta}{2s_\Theta}} T_\ell^{-1 - \frac{2s_\Theta}{d_\Theta}}  N_\ell^{-\frac{2 s_\calX}{d_\calX}} (\log N_{\ell})^{\frac{2s_\calX+2}{d_\calX}} + N_\ell^{-\frac{2 s_\calX}{d_\calX}} (\log N_{\ell})^{\frac{2s_\calX+2}{d_\calX}} \lambda_{\Theta, \ell}  \right) ,
\end{align*}
holds with probability at least $1 - 12 e^{-\tau}$.
Therefore, by taking $\lambda_{\Theta, \ell} \asymp T_\ell^{-\frac{2 s_\Theta}{2 s_\Theta +d_\Theta }}$, we obtain with probability at least $1 - 8 e^{-\tau}$,
\begin{align*}
    \left| Y_{\nkq, \ell} - \E_{\theta \sim \Qb}[P_{\nkq, \ell} - P_{\nkq, \ell-1}] \right| \lesssim \tau T_\ell^{-\frac{s_\Theta}{2 s_\Theta +d_\Theta }} \times N_\ell^{-\frac{s_\calX}{d_\calX}} (\log N_{\ell})^{\frac{s_\calX+1}{d_\calX}}.
\end{align*}
The proof is concluded.


