\section{Experiments}\label{sec:experiments}

We now illustrate NKQ over a range of applications, including some where the theory does not hold but where we still observe significant gains in accuracy. 
The code to reproduce experiments is available at \url{https://github.com/hudsonchen/nest_kq}.


\paragraph{Synthetic Experiment}
We start by verifying the bound in \Cref{thm:main} using the following synthetic example: $\Qb = \operatorname{U}[0,1]$, $\Pb_\theta = \operatorname{U}[0,1]$, $g(x, \theta) = x^{\frac{5}{2}} + \theta^{\frac{5}{2}}$, and $f(z) = z^2$, in which case $I=0.4115$ can be computed analytically. We estimate $I$ with i.i.d. samples $\theta_{1:T} \sim \operatorname{U}[0,1]$ and i.i.d. samples $x_{1:N}^{(t)} \sim \operatorname{U}[0,1]$ for $t \in \{1, \ldots, T\}$.
The assumptions from \Cref{thm:main} are satisfied with $s_\calX = s_\Theta = 2$ and $d_\calX = d_\Theta = 1$ (see \Cref{sec:appendix_toy}).
Therefore, to reach the absolute error threshold $\Delta$, we choose $N = T = \Delta^{-0.5}$ for NKQ  following \Cref{cor:nkq}. 
On the other hand, based on Theorem 3 of \citet{rainforth2018nesting}, the optimal way of assigning samples for NMC is to choose $N = \sqrt{T} = \Delta^{-1}$. 


In \Cref{fig:toy_experiment} \textbf{Top}, we see that the optimal choice of $N$ and $T$ suggested by the theory indeed results in a faster rate of convergence for both NMC and NKQ. 
For this synthetic problem, we confirm that both the theoretical rates of NKQ ($\text{Cost} = \Delta^{-1}$) and NMC ($\text{Cost} = \Delta^{-1/3}$) from \Cref{thm:main} and \citet[Theorem 3]{rainforth2018nesting} are indeed realized. We also adapt the synthetic problem to higher dimensions ($d_\calX=d_\Theta=d$) in \eqref{eq:toy_high_d} and observe in \Cref{fig:toy_experiment} \textbf{Bottom} that the performance gap between NKQ and NMC closes down as dimension grows. 
Such behaviour is expected because the cost of NKQ is $\tilde{\calO}(\Delta^{- \frac{ s_\calX}{d_\calX} - \frac{ s_\Theta}{d_\Theta}})$ and therefore degrades as the dimensions $d_\calX$ and $d_\Theta$ increase; whilst the cost of NMC remains the same.

We also conduct ablation studies, which are reserved for \Cref{fig:toy-ablation} in the appendix. 
In the left-most plot, we see that the result are not too sensitive to $\lambda_0$, although very large values decrease accuracy whilst very small values cause numerical issues.
In the middle plot, we see that selecting the kernel lengthscale using the median heuristic provides very good performance.
In the right-most plot, we see that NKQ with Mat\'{e}rn-$\frac{3}{2}$ kernels outperforms Mat\'{e}rn-$\frac{1}{2}$ kernel, indicating practitioners should use Sobolev kernels with the highest order of smoothness permissible by \Cref{thm:main}. 

\begin{figure}[t]
\centering
\begin{subfigure}{0.46\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/toy_new.pdf}
\end{subfigure}
\vspace{-9pt}
\begin{subfigure}{0.46\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/toy_ablation_dimension.pdf}
\end{subfigure}
\caption{\emph{Synthetic experiment.}
\textbf{Top:} Verification of theoretical results. The thin grey lines are theoretical rates of 
$\Delta = \text{Cost}^{-1}$ and $\Delta = \text{Cost}^{-1/3}$.
\textbf{Bottom:} Comparison of NKQ and NMC as dimension $d$ increases.
Results are averaged over 1000 independent runs, while shaded regions 
give the 25\%-75\% quantiles.}
\label{fig:toy_experiment}
\end{figure}

\paragraph{Risk Management in Finance} We now move beyond synthetic examples, starting in finance.
Financial institutions often face the challenge of estimating the expected loss of their portfolios in the presence of potential economic shocks, which amounts to numerically solving stochastic differential equations (SDEs) over long time horizons \cite{achdou2005computational}. 
Given the high cost of such simulations, data-efficient methods like NKQ are particularly desirable.

Suppose a shock occurs at time $\eta$ and alters the price of an asset by a factor of $1 + s$ for some $s \geq 0$. 
Conditioned on the asset price $S(\eta)=\theta$ at the time of shock, the loss of an option associated with that asset at maturity $\zeta$ with price $S(\zeta) = x$ can be expressed as $J(\theta) = \E_{X \sim \Pb_\theta}[g(X)]$, where $g(x) = \psi(x) - \psi((1+s)x)$ measures the shortfall in option payoff and the distribution $\Pb_\theta$ is induced by the price of the asset which is described by the Black-Scholes formula. 
The payoff function we consider is that of a butterfly call option: $\psi(x) = \max (x-K_1, 0) + \max (x - K_2, 0) - 2\max (x - (K_1 + K_2)/2, 0)$ for $K_1,K_2\geq 0$.
Since we incur a loss only if the final shortfall is positive, the expected loss of the option at maturity can be expressed as $I = \E_{\theta \sim \mathbb{Q}} [ \max( \E_{X \sim \Pb_\theta}[g(X)], 0) ]$. 
Under this setting, $d_\Theta=d_\calX=1$ and $I= 3.077$ can be computed analytically.

In this experiment, Assumptions \ref{as:app_true_g_smoothness}\ref{as:app_true_J_smoothness} are satisfied with $s_\Theta=s_\calX=1$, but the \textit{max} function is not in $C^2(\R)$ which violates Assumption \ref{as:app_lipschitz} (see \Cref{sec:finance}).
Nevertheless, we still run NKQ with $k_\calX$ and $k_\Theta$ being Mat\'{e}rn-$\frac{1}{2}$ kernels and choose $N=T=\Delta^{-1}$ for NKQ following \Cref{cor:nkq}.
For NMC, we follow \citet{Gordy2010} and choose $N = \sqrt{T} =\Delta^{-1}$. For MLMC, we use $L=5$ levels and allocate samples at each level following~\citet{giles2019decision}. 

In \Cref{fig:finance_health} \textbf{Top}, we present the mean absolute error of NKQ, NMC and MLMC with increasing cost. 
We see that NKQ outperforms both NMC and MLMC as expected.
For each method, we obtain the empirical rate $r$ by linear regression in log-log space, and compare this against the theoretical rate in \Cref{tab:table}. For NMC, our estimate of $\hat{r}=2.97$ matches theory ($r=3$), but when using QMC samples instead, our estimate of $\hat{r} = 2.74$ shows we under-perform compared to the theoretical rate ($r=2.5$). This is likely because the domains are unbounded and the measures are not uniform, breaking key assumptions. 
Finally, for NKQ, we obtain $\hat{r}=1.90$ for i.i.d samples and $\hat{r}=1.91$ for QMC samples which match (and even slightly outperform) the theoretical rate ($r=2$). 



\paragraph{Health Economics}
In medical decision-making, a key metric to evaluate the cost-benefit trade-off of conducting additional tests on patients is the \textit{expected value of partial perfect information (EVPPI)}
~\citep{brennan2007calculating,Heath2017}. 
Formally, let $g_c$ denote the patient outcome (such as quality-adjusted life-years) under treatment $c$ in a set of possible treatments $\mathcal{C}$, and $\theta$ represent the additional variables that may be measured. Then,  
$J_c(\theta) = \E_{X \sim \Pb_\theta} [g_c(X, \theta)]$ represents the expected patient outcome given the measurement of $\theta$. 
The EVPPI is defined as $I = I_1 - \max_{c \in \mathcal{C}}(I_{2,c})$, where $I_1 = \E_{\theta \sim \mathbb{Q}} \left[\max_{c \in \mathcal{C}} J_c(\theta)\right]$ and $I_{2,c} = \E_{\theta \sim \mathbb{Q}}\left[J_c(\theta)\right]$
and therefore $I$ consists of $|\mathcal{C}| + 1$ nested expectations.


\begin{figure}[t]
\vspace{-5pt}
\centering
\begin{subfigure}{\linewidth}
    \centering
    \includegraphics[width=0.45\linewidth]{figures/finance_legend_only_without_mlkq.pdf}
\end{subfigure}
\vspace{-5pt}
\begin{subfigure}{0.45\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/finance_without_mlkq.pdf}
\end{subfigure}
\vspace{-5pt}
\begin{subfigure}{0.45\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/evppi_without_mlkq.pdf}
\end{subfigure}
\caption{
\textbf{Top:} \emph{Financial risk management.}
\textbf{Bottom:} \emph{Health economics.}
Results are averaged over 100 independent runs, while shaded 
regions give the 25\%-75\% quantiles.
}
\label{fig:finance_health}
\end{figure}

We follow Section 4.2 of \citet{giles2019decision}, where both $\mathbb{P}_{\theta}$ and $\mathbb{Q}$ are Gaussians, and $g_1(x, \theta)=10^4 (\theta_1 x_5 x_6 + x_7 x_8 x_{9})-(x_1 + x_2 x_3 x_4)$ and $g_2(x, \theta) = 10^4 (\theta_2 x_{13} x_{14} + x_{15} x_{16} x_{17})-(x_{10} + x_{11} x_{12} x_4)$. 
The exact practical meanings of each dimension of $x$ and $\theta$ can be found in \Cref{sec:decision}, but includes quantities such as `cost of treatment' and `duration of side effects'.
Here we have $d_\calX = 17$ and $d_\Theta = 2$, the former being relatively high dimensional.
The ground truth EVPPI under this setting is $I=538$ provided in \citet{giles2019decision}.

For estimating both $I_1$ and $I_{2,c}$, Assumptions \ref{as:app_true_g_smoothness}\ref{as:app_true_J_smoothness} are satisfied with infinite smoothness $s_\calX= s_\Theta =\infty$, but the \textit{max} function in $I_{1}$ is only in $C^0(\R)$ which violates Assumption \ref{as:app_lipschitz}. 
As a result, for estimating $I_1$ we take $k_\calX$ to be a Gaussian kernel and $k_\Theta$ to be Mat\'ern-$\frac{1}{2}$ kernel (so as to be conservative about the smoothness in $\theta$). For estimating $I_{2,c}$, we select both $k_\calX$ and $k_\Theta$ to be Gaussian kernels. 
For NKQ, we choose $N = T =\Delta^{-1}$ whereas 
for NMC, we choose $N = \sqrt{T} =\Delta^{-1}$. 
For MLMC, we use $L=5$ levels and allocate the samples at each level following~\citet{giles2019decision}. 
We run NKQ and NMC with both i.i.d. samples and QMC samples.
In \Cref{fig:finance_health} \textbf{Bottom}, we present the mean absolute error of NKQ, NMC and MLMC with increasing cost. 
We can see that NKQ consistently outperforms other baselines. 


\paragraph{Bayesian Optimization}
We conclude with an application in Bayesian optimization. Typical acquisition functions are greedy approaches that maximize the immediate reward, while look-ahead acquisition functions optimize accumulated reward over a planning horizon, which results in reduced number of required function evaluations~\citep{Ginsbourger2010,gonzalez2016glasses,wu2019practical,Yang2024}. 
The utility of a two-step look ahead acquisition functions can be written as the following nested expectation.
\begin{align*}
    \alpha(z ; \calD) := \E_{f_{\mid \calD} } \left[g(f_{\mid \calD}, z) + \max _{z^\prime} \E_{f_{\mid \calD^\prime}  } \left[g \left(f_{\mid \calD^\prime}, z^\prime \right)\right]\right],
\end{align*}
where $f_{\mid \calD}, f_{\mid \calD^\prime}$ are the posterior distributions given data $\calD$ and $\calD^\prime:= \calD \cup (z , f_{\mid \calD}(z))$. 
In this experiment, the prior is a Gaussian process with zero mean and Mat\'{e}rn-$\frac{1}{2}$ covariance so the posterior $f_{\mid \calD}$ remain a Gaussian process.
Here, $g$ is the reward function and we use q-expected improvement~\cite{wang2020parallel} with $q=2$ so $z=(z_1, z_2)$ and
$g(f_{\mid \calD}, z) = \max _{j=1, 2} ( f_{\mid \calD}(z_j) - r_{\max} ), 0)$. The constant 
$r_{\max}$ is the maximum reward obtained from previous queries.
Although $f_{\mid \calD} $ (resp. $f_{\mid \calD^\prime}$) is a Gaussian process, we only ever consider its evaluation on $z$ (resp. $z^\prime$), and we therefore only have to integration against two-dimensional Gaussians. 
Notationally speaking, $f_{\mid \calD^\prime}(z_1, z_2)$ correspond to $x$ and $f_{\mid \calD}(z_1, z_2)$ correspond to $\theta$ in \eqref{eq:nested} (i.e. $d_\calX = d_\Theta = 2$), but we use the notation of $f_{\mid \calD^\prime}, f_{\mid \calD}$ to stay consistent with the GP literature. 
As a result of the \textit{max} operation, $s_\calX = 1$ but we do not have sufficient smoothness in $\Theta$.


\begin{figure*}[t]
\vspace{-5pt}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/bo.pdf}
    \vspace{-20pt}
    \caption{\emph{Bayesian optimization with look ahead acquisition function}. The plots are NMSE against accumulative wall clock time.
    Results are averaged over 100 independent runs, while shaded regions give the 25\%-75\% quantiles. 
    MLMC is reproduced with the same code from \citet{Yang2024}.
    }
    \label{fig:bo}
\end{figure*}


We benchmark NKQ, NMC and MLMC on three synthetic tasks from BoTorch~\citep{balandat2020botorch}. 
For NKQ, both $k_\calX$ and $k_\Theta$ are Mat\'ern-$\frac{1}{2}$ kernels since we want to be conservative about the smoothness. 
Although both $\Qb$ and $\Pb_\theta$ are Gaussian so closed-form KMEs are available, we use the ``change of variable trick'' in \Cref{sec:reparam} to reduce the computational complexity of NKQ to $\calO(T \times N)$.
To reach a specific error threshold $\Delta$, following \Cref{tab:table}, we choose $N=T=\Delta^{-2}$ for NMC and $N=T=\Delta^{-1}$ for NKQ. 
For MLMC, we use the same code as Section 5 of \citet{Yang2024}.
The normalized mean squared error (NMSE) $\frac{\|\max _{z \in \calD_{\mathcal{S}} } f_{\text{BB}}(z)-f_{\text{BB}}(z^*)\|^2}{\|\max_{x \in \calD_0} f_{\text{BB}}(z) - f_{\text{BB}}(z^*)\|^2}$ is used as the performance metric, where $\calD_0$ (resp. $\calD_{\mathcal{S}}$) is queried data at initialization (resp. after $\mathcal{S}$ iterations),  $f_{\text{BB}}$ is the black box function to be optimized and $f_{\text{BB}}(z^*)$ is the maximum reward. 

In \Cref{fig:bo}, we compare the efficiency of each method by plotting their NMSE against cumulative computational time in wall clock. We can see that NKQ achieves the lowest NMSE among all methods under a fixed amount of computational time across all three datasets. 
Even though the assumptions of \Cref{thm:main} are not all satisfied, NKQ still outperforms NMC and MLMC.
Furthermore, since the \texttt{Dropwave}, \texttt{Ackley}, and \texttt{Cosine8} functions are synthetic and computationally cheap, we expect the advantages of NKQ to be more pronounced for Bayesian optimization on real-world expensive problems.

\vspace{-1mm}
\section{Conclusion}
This paper introduced a novel estimator for nested expectations based on kernel quadrature. 
We proved in \Cref{thm:main} that our method has a faster rate of convergence than existing methods provided that the problem has sufficient smoothness. 
This theoretical result is consistent with the empirical evidence in several numerical experiments. 
Additionally, even when the problem is not as smooth as the theory requires, NKQ can still outperform baseline methods potentially due to the use of non-equal weights.

Following our work, there remain a number of interesting future problems and we now highlight two main ones. Firstly, we proposed a combination of KQ and MLMC that we call MLKQ in \Cref{sec:mlnkq}. However, we believe our current theoretical rate for MLKQ is sub-optimal due to the sub-optimal allocation of samples at each level. Further work will therefore be needed to determine whether this is a viable approach in some cases.
Secondly, for applications where function evaluations are extremely expensive, NKQ could be extended to its Bayesian counterpart. This would allow us to use the finite sample uncertainty quantification for adaptive selection of samples, which could further improve performance.




 \subsection*{Acknowledgments}
 The authors acknowledge useful discussions with Philipp Hennig and support from the Engineering and Physical Sciences Research Council (ESPRC) through grants [EP/S021566/1] (for ZC and MN) and [EP/Y022300/1] (for FXB).



