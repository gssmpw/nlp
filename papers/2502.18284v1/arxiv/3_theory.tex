\section{Theoretical Results}\label{sec:theory}

In this section, we derive a convergence rate for the absolute error $|\hat{I}_{\nkq} - I|$ as the number of samples $N, T \to \infty$. Before doing so, we recall the connection between RKHSs and Sobolev spaces. A kernel $k$ on $\mathbb{R}^d$ is said to be translation invariant if $k(x, x^\prime) = \Psi(x-x^\prime)$ for some positive definite function $\Psi$ whose Fourier transform $\hat{\Psi}(\omega)$ is a finite non-negative measure on $\mathbb{R}^d$~\citep[Theorem 6.6]{wendland2004scattered}. 
Suppose $\calX$ has a Lipschitz boundary, if $k$ is translation invariant and its Fourier transform $\hat{\Psi}(\omega)$ decays as $\mathcal O (1 + \|\omega \|_2^2)^{-s}$ when $\omega \to \infty$ for $s > d/2$, then its RKHS $\calH_k$ is norm equivalent to the Sobolev space $W_2^s(\calX)$~\citep[Corollary 10.48]{wendland2004scattered}.
More specifically, it means that their set of functions coincide and there are constants $c_1,c_2>0$ such that $c_1\|h\|_{\calH_k} \leq\|h\|_{W_2^s(\calX)} \leq c_2\|h\|_{\calH_k}$ holds for all $h \in \calH_k$. 
In this paper, we call such kernel a \emph{Sobolev reproducing kernel of smoothness $s$}.
An important example of Sobolev kernel is the Matérn kernel--- the RKHS of a Matérn-$\nu$ kernel is norm-equivalent to $W_2^s(\calX)$ with $s=\nu + d/2$.
All Sobolev kernels are bounded, i.e. $\sup_{x \in \calX} k(x, x) \leq \kappa$ for some positive constant $\kappa$. 
When the context is clear, we use $\|f\|_{s,2} \coloneq \|f\|_{W_2^s(\calX)}$ to denote the Sobolev space norm. 

\begin{thm}\label{thm:main}
Let $\calX =[0,1]^{d_\calX}$ and $\Theta =[0,1]^{d_\Theta}$. Suppose $\theta_{1:T}$ are i.i.d. samples from $\Qb$ and $x_{1:N}^{(t)}$ are i.i.d samples from $\Pb_{\theta_t}$ for all $t \in \{1, \cdots, T\}$. 
Suppose further that $k_\calX$ and $k_\Theta$ are Sobolev kernels of smoothness $s_\calX > d_\calX / 2$ and $s_\Theta > d_\Theta/2$, and that the following conditions hold
\begin{enumerate}[leftmargin=1.0cm]
    \item [(1)] There exist $G_{0, \Theta}, G_{1, \Theta}, G_{0, \calX}, G_{1, \calX} > 0$ such that $G_{0, \Theta} \leq \|q\|_{L_\infty(\Theta)} \leq G_{1, \Theta}$; and for any $\theta \in \Theta$,  $G_{0, \calX} \leq \|p_\theta(\cdot) \|_{L_\infty(\calX)} \leq G_{1, \calX}$.
    \customlabel{as:equivalence}{(1)} 
    \item [(2)] There exists $S_1 >0$ such that for any $\theta \in \Theta$ and any $\beta \in \N^{d_\Theta}$ with $|\beta| \leq s_\Theta$, $\| D_\theta^\beta g(\cdot, \theta) \|_{ s_\calX,2 } \leq S_1$.
    \customlabel{as:app_true_g_smoothness}{(2)} 
    \item[(3)] There exist $S_2,S_3 >0$ such that for any $x \in \calX$, $\| g(x, \cdot) \|_{ s_\Theta ,2} \leq S_2$ and $\|\theta \mapsto p_\theta(x) \|_{ s_\Theta ,2} \leq S_3 \leq 1$. 
    \customlabel{as:app_true_J_smoothness}{(3)} 
    \item[(4)] 
    There exists $S_4 >0$ such that derivatives of $f$ up to and including order $s_\Theta + 1$ are bounded by $S_4$.
    \customlabel{as:app_lipschitz}{(4)} 
\end{enumerate}
Then, there exists $N_0, T_0 \in \mathbb{N}^{+}$ such that for $N>N_0, T>T_0$, we can take $\lambda_{\calX} \asymp N^{-2\frac{s_\calX}{d_\calX}} (\log N)^{\frac{2s_\calX+2}{d_\calX}}$ and $\lambda_{\Theta} \asymp T^{-2\frac{s_\Theta}{d_\Theta}} (\log T)^{\frac{2s_\Theta+2}{d_\Theta}}$ to obtain the following bound
\begin{align*}
    \left| I - \hat{I}_{\nkq} \right| \leq \tau \left( C_1 N^{- \frac{ s_\calX}{d_\calX} } (\log N)^{\frac{s_\calX+1}{d_\calX}} + C_2 T^{- \frac{ s_\Theta}{d_\Theta} } (\log T)^{\frac{s_\Theta+1}{d_\Theta}} \right), 
\end{align*}
which holds with probability at least $1 - 8 e^{-\tau}$.
$C_1, C_2$ are two constants independent of $N, T, \tau$.
\end{thm}
\begin{cor}\label{cor:nkq}
    Suppose all assumptions in \Cref{thm:main} hold. If we set $N = \tilde{\calO} (\Delta^{-\frac{d_\calX}{s_\calX} })$ and $T = \tilde{\calO}(\Delta^{- \frac{d_\Theta}{s_\Theta}})$, then $N \times T = \tilde{\calO}(\Delta^{-\frac{d_\calX}{s_\calX} - \frac{d_\Theta}{s_\Theta}})$ samples are sufficient to guarantee that $|I - \hat{I}_{\nkq}| \leq \Delta$ holds with high probability. 
\end{cor}
To prove these results, we can decompose $| I - \hat{I}_{\nkq}|$ into the sum of stage I and stage II errors, which can be bounded by terms of order $
N^{- \frac{ s_\calX}{d_\calX}} (\log N)^{\frac{s_\calX+1}{d_\calX}}$ and $T^{- \frac{ s_\Theta}{d_\Theta}} (\log T)^{\frac{s_\Theta+1}{d_\Theta}}$ respectively; 
see  \Cref{sec:proof}. 
Interestingly, note that the stage II error does not suffer from the fact that we are using noisy observations $\hat{F}_{\kq}(\theta_{1:T})$ and we maintain the standard KQ rate up to logarithmic terms (see \Cref{rem:noise_stage_2}).
We emphasize that our bound indicates that the tail behavior of $|I - \hat{I}_{\nkq}|$ is sub-exponential. This contrasts with existing work on Monte Carlo methods, which typically only provides upper bounds on the expectation of error with no constraints on its tails~\citep{Giles2015, Bartuska2023}.

We now briefly discuss our assumptions.
Assumption \ref{as:equivalence} is mild and allows $L_2(\Pb_\theta)$ (resp. $L_2(\Qb)$) to be norm equivalent to $L_2(\calX)$ (resp. $L_2(\Theta)$), which is widely used in statistical learning theory that involves Sobolev spaces \citep{fischer2020sobolev, suzuki2021deep}.
Since our proof essentially translates quadrature error into generalization error of  kernel ridge regression, Assumptions \ref{as:app_true_g_smoothness}, \ref{as:app_true_J_smoothness}, \ref{as:app_lipschitz} ensure that functions $f, g$ and the density $p$ have enough regularity so that the regression targets in both stage I and stage II belong to the correct Sobolev spaces. These are more restrictive, but are essential to obtain our fast rate and are common assumptions in the KQ literature.
Assumptions \ref{as:app_true_g_smoothness}, \ref{as:app_true_J_smoothness}, \ref{as:app_lipschitz} can be relaxed if mis-specification is allowed; see e.g.~\citet{fischer2020sobolev,Kanagawa2019, zhang2023optimality}. 
\Cref{thm:main} shows that for NKQ to have a fast convergence rate, one ought to use Sobolev kernels which are as smooth as possible in both stages.
Furthermore, when $s_\calX = s_\Theta = \infty$ (e.g. when the integrand and kernels belong to Gaussian RKHSs), our proof could be modified to show an exponential rate of convergence in a similar fashion as \citet[Theorem 10]{briol2018statistical} or \citet{Karvonen2020}.

\begin{rem}[Noisy observations in Stage II of NKQ]\label{rem:noise_stage_2}
    Note that NKQ employs noisy observations $\{\theta_t, \hat{F}_{\kq}(\theta_t)\}_{t=1}^T$ in stage II kernel quadrature rather than the ground truth observations $\{\theta_t, F(\theta_t)\}_{t=1}^T$. 
    Although \citet{Cai2023} establishes that kernel quadrature (KQ) with noisy observations converges at a slower rate than KQ with noiseless observations, a key distinction in our setting is that, as shown in \eqref{eq:bernstein_noise}, the observation noise in stage II KQ is of order $\tilde{\calO}(N^{-\frac{s_\calX}{d_\calX}})$, whereas the noise in \citet{Cai2023} remains at a constant level.
    As a result, we can still use KQ in stage II as if the observations $\{\hat{F}_{\kq}(\theta_t)\}_{t=1}^T$ are noiseless, and the additional error it introduces happens to be of the same order as the stage I error $\tilde{\calO}(N^{-\frac{s_\calX}{d_\calX}})$ and is therefore subsumed by it.
\end{rem}

\begin{rem}[Convergence rate for CKQ]\label{rem:ckq_rate}
    For CKQ estimator $\hat{J}_{\text{CKQ}}$ defined in \eqref{eq:ckq} from \Cref{sec:methodology} that approximates the parametric expectation $J(\theta)$ uniformly over all $\theta \in \Theta$, its error can be upper bounded in the same way as NKQ. For $\bar{J}_{\kq}$ defined in \eqref{eq:bar_F_KQ_all_theta}, the error of $\hat{J}_{\text{CKQ}}$ can be decomposed as following:
    \begin{align*}
        \| J - \hat{J}_{\ckq} \|_{L_2(\Qb)} \leq \| J - \bar{J}_{\kq} \|_{L_2(\Qb)} + \| \bar{J}_{\kq} - \hat{J}_{\ckq} \|_{L_2(\Qb)}.
    \end{align*}
    The first term corresponds to the \emph{stage I error} and can be shown to be $\tilde{\calO}(N^{-\frac{s_\calX}{d_\calX}})$ using the same analysis from \eqref{eq:lipschitz_F} to \eqref{eq:high_prob_hat_g_g}. The second term corresponds to the \emph{stage II error} and can be shown to be $\tilde{\calO}(T^{-\frac{s_\Theta}{d_\Theta}})$ using the same analysis from \eqref{eq:stage_1} to \eqref{eq:stage_2}. 
    Combining the two error terms, we have $\| J - \hat{J}_{\ckq} \|_{L_2(\Qb)} = \tilde{\calO}(N^{-\frac{s_\calX}{d_\calX}} + T^{-\frac{s_\Theta}{d_\Theta}})$ holds with probability at least $1 - 8 e^{-\tau}$.
    The rate is better than the best known rate $\calO(N^{-\frac{s_\calX}{d_\calX}} + T^{-\frac{1}{4}})$ of CBQ proved in Theorem 1 of \cite{chen2024conditional} since $\frac{s_\Theta}{d_\Theta} > \frac{1}{2} > \frac{1}{4}$.
    The intuition behind the faster rate is that CKQ benefits from the extra flexibility of choose regularization parameters $\lambda_\calX, \lambda_\Theta$; while CBQ, as a two stage Gaussian Process based approach, is limited to choose $\lambda_\Theta$ equal to the heteroskedastic noise from the first stage. It may be possible to modify the proof of \cite{chen2024conditional} to improve the rate further, but this has not been explored to date.
\end{rem}


\begin{table}[t]
\centering
\renewcommand{\arraystretch}{1.8}
\setlength{\tabcolsep}{5pt}
\begin{tabular}{|c|c|}
\hline
\textbf{Method} & \textbf{Cost} \\ \hline
NMC & $\calO(\Delta^{-3} )$ or $\calO(\Delta^{-4} )$ \\ \hline
NQMC & $\calO(\Delta^{-2.5})$ \\ \hline
MLMC & $\calO(\Delta^{-2}) $  \\ \hline
\textbf{NKQ (\Cref{cor:nkq})}  & $\tilde{\calO}\Big(\Delta^{-\frac{d_\calX}{s_\calX} - \frac{d_\Theta}{s_\Theta}} \Big)$ \\ \hline
\end{tabular}
\caption{Cost of methods for nested expectations, measured through the number of function evaluations required to ensure $|I - \hat{I}| \leq \Delta$. 
NMC rate is taken from Theorem 3 of \citet{rainforth2018nesting}, NQMC rate is taken from Proposition 5 of \citet{Bartuska2023}, MLMC rate is taken from Section 3.1 of \citet{giles2018mlmc}. Smaller exponents $r$ in $\Delta^{-r}$ indicate a cheaper method. } \label{tab:table}
\end{table}

In \Cref{tab:table}, we compare the \emph{cost} of all methods evaluated by the number of  evaluations required to ensure $|\hat{I} - I| \leq \Delta$.
We can see that NKQ is the only method that explicitly exploits the smoothness of $g, p, f$ in the problem so that it outperforms all other methods when $\frac{d_{\calX}}{s_{\calX}} + \frac{d_\Theta}{s_\Theta} <2$.

We have previously mentioned that KQ could potentially be combined with other algorithms to further improve efficiency, and studied this for both MLMC and QMC. 
For the former (i.e. NKQ+MLMC), we derived a new method called \emph{multi-level NKQ (MLKQ)}, which closely related to multilevel BQ algorithm of \citet{li2023multilevel} and for which we were able to prove a rate of $\tilde{\calO}(\Delta^{-1 - \frac{d_\calX}{2s_\calX} -\frac{d_\Theta}{2s_\Theta}})$. 
Similarly to NKQ, when $\frac{d_\calX}{s_\calX} + \frac{d_\Theta}{s_\Theta} < 2$, the rate for MLKQ is faster than that of NMC, NQMC and MLMC. However, the rate we managed to prove is slower than that for NKQ, and a slower convergence was also observed empirically (see \Cref{fig:combined_all}).
We speculate that the worse performance is caused by the accumulation of bias from the KQ estimators at each level. See \Cref{sec:mlnkq} for details. 

We also consider combining NKQ and QMC. In this case, we expect the same rate as in \Cref{thm:main} can be recovered by resorting to the fill distance technique in scatter data approximation~\cite{wendland2004scattered}. This is confirmed empirically in \Cref{sec:experiments}, where we observe that using QMC points can achieve similar or even better performance than NKQ with i.i.d. samples.

