\section{Nested Kernel Quadrature}\label{sec:methodology}

We can now present our novel algorithm: \emph{nested kernel quadrature (NKQ)}.
To simplify the formulas, we write
\begin{equation}\label{eq:J_F_defi}
    J(\theta) := \E_{X \sim \Pb_\theta} \left[ g(X, \theta) \right], \quad F(\theta) := f(J(\theta)),  
\end{equation}
so that the nested expectation in \eqref{eq:nested} can be written as $I = \mathbb{E}_{\theta \sim \mathbb{Q}}[F(\theta)]$. We will assume that we have access to
\begin{align*}
  \theta_{1:T} & := [\theta_1,\ldots,\theta_T]^\top \in \Theta^T, \\
  x_{1:N}^{(t)} & := \Big[x_{1}^{(t)},\ldots,x_{N}^{(t)} \Big] \in \calX^N,\\
  g\big(x_{1:N}^{(t)},\theta_t \big) & := \Big[ g\big(x_{1}^{(t)},\theta_t \big), \ldots, g\big(x_{N}^{(t)},\theta_t \big)\Big]  \in \mathbb{R}^N,
\end{align*}
for all $t \in \{1, \ldots, T\}$, and $f$ is a function that can be evaluated.
We do not specify how the point sets are generated, although further (mild) assumptions will be imposed for our theory in \Cref{sec:theory}. Using the same number of function evaluations $N$ per $\theta_t$ is not essential, but we assume this as it significantly simplifies our notation. 
Given the above, we are now ready to define NKQ as the following two-stage algorithm, which is illustrated in \Cref{fig:illustration}.

\paragraph{Stage I}
For each $t \in \{1, \ldots, T\}$, we estimate the inner conditional expectation $J$ evaluated at $\theta_t$ with $N$ observations $x_{1:N}^{(t)}$ and $g(x_{1:N}^{(t)},\theta_t)$ using a KQ estimator: 
\begin{equation}\label{eq:F_J_KQ}
\hat{J}_{\kq} (\theta_t) :=\mu_{\mathbb{P}_{ \theta_t} } \big(x_{1:N}^{(t)} \big) \big( \boldsymbol{K}_\calX^{(t)} + N \lambda_\calX \boldsymbol{I}_N \big)^{-1} g \big(x_{1:N}^{(t)}, \theta_t \big) .
\end{equation}
Here $k_\calX$ is a reproducing kernel on $\calX$, $\mu_{\mathbb{P}_{\theta_t}}(\cdot) = \mathbb{E}_{X \sim \mathbb{P}_{\theta_t}}[k_{\calX}(X, \cdot)]$ is the KME of $\mathbb{P}_{\theta_t}$ and $\boldsymbol{K}_\calX^{(t)} = k_\calX(x_{1:N}^{(t)}, x_{1:N}^{(t)})$ is an $N \times N$ Gram matrix. 
Using the same kernel $k_\calX$ for each $t\in \{1, \ldots, T\}$ is not essential, but we assume this to be the case for simplicity.
Given these KQ estimates, we then we apply the function $f$ to get $\hat{F}_{\kq}(\theta_t) = f(\hat{J}_{\kq} (\theta_t))$. 

\paragraph{Stage II}
We use a KQ estimator to approximate the outer expectation using the output of Stage I:
\begin{align}\label{eq:NKQ_estimator}
    \hat{I}_{\nkq} &:= \mu_{\mathbb{Q}}(\theta_{1:T}) ( \boldsymbol{K}_\Theta + T \lambda_\Theta \boldsymbol{I}_T)^{-1} \hat{F}_{\kq}( \theta_{1:T} ).
\end{align}
Here $k_\Theta$ is a reproducing kernel on $\Theta$,  $\mu_{\mathbb{Q}} = \mathbb{E}_{\theta \sim \mathbb{Q}}[k_{\Theta}(\theta, \cdot)]$ is the embedding of $\mathbb{Q}$ and $\boldsymbol{K}_\Theta = k_\Theta(\theta_{1:T}, \theta_{1:T})$ is a $T\times T$ Gram matrix. 

\definecolor{blueviolet}{rgb}{0.541, 0.169, 0.886}
\definecolor{brown}{rgb}{0.647, 0.165, 0.165}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/illustration.pdf}
    \caption{\textit{Illustration of NKQ.} In stage I, we estimate $J(\theta_t)$ using $\hat{J}_{\kq}(\theta_t) = \sum_{n=1}^N \textcolor{blueviolet}{w_{n, t}^\mathcal{X}} g(x_n^{(t)}, \theta_t)$ for all $t \in \{ 1, \ldots, T\}$. 
    In stage II, we estimate $I$ with $\hat{I}_{\nkq} = \sum_{t=1}^T \textcolor{brown}{w^\Theta_t} \hat{F}_{\kq}(\theta_t)$ where $\hat{F}_{\kq}(\theta_t) \coloneq f(\hat{J}_{\kq}(\theta_t))$. 
    The shaded areas depict $\Pb_\theta$ (for stage I) and $\Qb$ (for stage II).}
    \label{fig:illustration}
\end{figure}


Combining stage I and II, NKQ can be expressed in a single equation as a nesting of two quadrature rules:
\begin{align}\label{eq:nkq_weights}
    \hat{I}_{\nkq} = \sum_{t=1}^{T} w^{\Theta}_t f\left(\sum_{n=1}^N w^{\calX}_{n, t} g(x_n^{(t)}, \theta_t) \right),
\end{align}
where $w^{\calX}_{1, t},\ldots,w^{\calX}_{N, t}$ are the KQ weights used in stage I for $\hat{J}_{\text{KQ}}(\theta_t)$ and $w^{\Theta}_1,\ldots, w^{\Theta}_T$ are the KQ weights used in stage II.
Although these weights are stage-wise optimal when $\lambda_{\mathcal{X}} = \lambda_\Theta = 0$ thanks to the optimality of KQ weights, it is unclear whether they are globally optimal due to the non-linearity of $f$. Note that NMC can be recovered by taking all stage I weights to be $1/N$ and all stage II weights to be $1/T$, which is sub-optimal. 


NKQ inherits the two main drawbacks of KQ. Firstly, solving the linear systems to obtain the stage I and II weights has a worst-case  computational complexity of $\calO(T N^3 + T^3)$. 
Secondly, NKQ requires closed-form KMEs at both stages: $\mu_{\mathbb{P}_{ \theta_t}}$ for all $t \in \{1,\ldots,T\}$ in stage I, and $\mu_{\mathbb{Q}}$ in stage II. Fortunately, we can use the approaches discussed in the previous section to reduce the complexity to $\calO(TN + T)$ and obtain closed-form kernel embeddings. 
 



NKQ requires the selection of hyperparameters, including for the kernels in both stage I and II. 
We typically take $k_{\mathcal{X}}$ and $k_{\Theta}$ to be Mat\'ern kernels whose orders are determined by the smoothness of $f$ and $g$ (as justified by \Cref{thm:main}; see \Cref{sec:theory} for details).
This leaves us with a choice of kernel hyperparameters which include lengthscales $\gamma_{\mathcal{X}}, \gamma_{\Theta}$ and amplitudes $A_{\mathcal{X}}, A_{\Theta}$. 
The lengthscales are selected via the median heuristic. 
The regularizers are set to $\lambda_{\mathcal{X}} = \lambda_0 N^{-\frac{2s_\calX}{d_\calX}} (\log N)^{\frac{2s_\calX+2}{d_\calX}}$ and $ \lambda_{\Theta} = \lambda_0 T^{-\frac{2s_\Theta}{d_\Theta}} (\log T)^{\frac{2s_\Theta+2}{d_\Theta}}$ where $\lambda_0$ is selected with grid search over $\{0.01, 0.1, 1.0\}$ following \Cref{thm:main}. Finally, we standardise our function values (by subtracting the empirical mean then dividing by the empirical standard deviation), and then set the amplitudes to $A_{\mathcal{X}}=A_{\Theta}=1$. This last choice could further be improved using a grid search, but we do not do this as we do not notice significant improvements when doing so in experiments and this tends to increase the cost.


Before presenting our theoretical results, we briefly comment on the connection with existing KQ methods.
If we could evaluate the exact expression for the inner conditional expectation $J(\theta)$ pointwise, then (following \eqref{eq:kq}) the KQ estimator for $I$ would be $\bar{I}_{\kq} = \mu_{\mathbb{Q}}(\theta_{1:T}) ( \boldsymbol{K}_\Theta + T \lambda_\Theta \boldsymbol{I}_T)^{-1} F( \theta_{1:T})$. Comparing with \eqref{eq:NKQ_estimator}, NKQ can thus be seen as KQ with noisy function values $\hat{F}_{\kq}( \theta_{1:T} )$ (replacing the exact values $F( \theta_{1:T})$ in \eqref{eq:NKQ_estimator}). 
Although it is proved in \citet{Cai2023} that noisy observations make KQ converge at a slower rate, we prove that the stage II observation noise is of the same order as the stage I error, and consequently, we can still treat stage II KQ as noiseless kernel ridge regression and the additional error caused by the stage II observation noise would be \emph{subsumed} by the stage I error (See \Cref{rem:noise_stage_2}).
NKQ is also closely related to a family of regression-based methods for estimating conditional expectations~\citep{longstaff2001valuing,chen2024conditional}. Indeed, with a slight modification of Stage II in \eqref{eq:NKQ_estimator}, we can obtain an estimator of $J(\theta)$ that we call \emph{conditional kernel quadrature (CKQ)} 
\begin{align}\label{eq:ckq}
    \hat{J}_{\text{CKQ}}(\theta) := k_\Theta(\theta, \theta_{1:T}) ( \boldsymbol{K}_\Theta + T \lambda_\Theta \boldsymbol{I}_T)^{-1} \hat{J}_{\kq}( \theta_{1:T}) .
\end{align}
CKQ highly resembles \textit{conditional BQ (CBQ)}~\citep{chen2024conditional}; the difference is in stage II, where CBQ uses heteroskedastic Gaussian process regression whilst CKQ uses kernel ridge regression. Interestingly, the proof in this paper leads to a much better rate for CKQ than the best known rate for CBQ (see \Cref{rem:ckq_rate}).






