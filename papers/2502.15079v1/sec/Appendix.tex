\section{Appendices}
\label{sec:appendix}

\subsection{Implementation Details}
\label{app:implementation}

\paragraph{Fine-tuning from pretrained models.}
We use the visual representations for video and language model embeddings pretrained from Video-LLMs to perform instruction fine-tuning using different objectives, including \method{} (\autoref{sec:hallucination_correction}), entailment (\autoref{sec:prelim}), and masking correction (\autoref{sec:masking_correction}).
During finetuning, visual representations are frozen, and the embeddings from the visual-text adapter layers and LLM are learnable.


\paragraph{Hyperparameters and computation.}

For Video-LLaVA, we finetune our models for $3$ epochs, using a learning rate of $2e^{-4}$ and AdamW optimizer. We also use a LoRA adapter~\cite{hu2022lora} of rank $128$ and alpha $256$. Since we freeze the video encoder, the number of trainable parameters is significantly reduced to $241$M for Video-LLaVA. The number of video frames processed per video is $8$, with a batch size of $8$, using $2$ RTXA6000 GPUs, for a total of $\sim 72$ hours.

For VideoChat2, we finetune our models for $3$ epochs, using a learning rate of $2e^{-5}$ and AdamW optimizer. We use a LoRA adapter~\cite{hu2022lora} of rank $16$ and alpha $32$. We also freeze the visual encoders and reduce the number of trainable parameters to $193$M.  The number of video frames processed per video is $8$, with a batch size of $2$, using $1$ RTXA6000 GPU, for around $\sim 72$ hours.

\paragraph{Tools.}
We implement our models with Pytorch 2.0.1, Huggingface Transformers 4.31.0, scikit-learn 1.2.2. We use SciPy 1.6.0 to find content words from ground truth video description by excluding words with part-of-speech tags: AUX, SYM, DET, PUNCT.




\subsection{Ablation studies}
\label{app:ablation}

\paragraph{Performance on text-to-video retrieval.}
In \autoref{table:ssv2}, \method{} (\autoref{sec:hallucination_correction}) demonstrates competitive performance on SSv2 downstream tasks, surpassing the pretrained model by up to 5.7 mAP points and outperforming the model fine-tuned with the entailment task by up to 2.0 mAP points.
Masking correction augmentation typically enhances video-language alignment when jointly trained with \method{} or the entailment task.

\input{tables/ssv2}


\paragraph{Effect of different mask ratios.}
\autoref{table:mask_ratio} shows the performance when jointly finetuning Video-LLaVA using \method{} and masking correction task (\autoref{sec:masking_correction}) with different masking ratio.
The results indicate that using masking ratio of 45\% achieves higher average accuracy.

\input{tables/different_mask_ratio}


\paragraph{Comparing \method{} and natural language explanations.}
To assess the effectiveness of \method{} as a finetuning task, we compare it against natural language explanations (NLE) generated by external natural language inference models \cite{bansal2024videocon}, used alongside the entailment task. We fine-tune Video-LLaVA with both the entailment and NLE training objectives and report the results in \autoref{table:nle}. Our findings show that \method{} outperforms Video-LLaVA trained with entailment and NLE objectives, even without our proposed masking objective.

\input{tables/nle}



\subsection{Additional Qualitative Analysis}
\label{app:qualitative_analysis}

\input{figures/qualitative_app}

\autoref{fig:qualitative_app} shows additional success and failure cases of \method{} and the other models we tested.



\subsection{Pretrained Video-LLMs.}
\label{app:pretrained_vlm}
We use two pre-trained Video-LLMs with different model architectures.
\paragraph{Video-LLaVA.} Video-LLaVA~\cite{lin2023video} consists of LanguageBind~\cite{zhu2024languagebind} encoders for the visual inputs, a large language model~\cite{vicuna}, visual projection layers and a word embedding layer. It is finetuned via visual instruction tuning with 665k image-text pairs from LLaVA 1.5~\cite{llava} and a 100k video-text instruction set from Video-ChatGPT~\cite{videochatgpt}.
We use this model under their Apache License 2.0.

\paragraph{VideoChat2.} VideoChat2~\cite{li2023videochat} performs a progressive multi-modal training for three stages. In the first stage, it is trained to aling the visual encoder with a Querying Transformer (Q-Former)~\cite{blip2} which acts as an information bottleneck between the image and textual encoders and distill relevant information to the textual context. The second stage connects the visual encoder with a pretrained LLM. In the third stage, finetunes the model via instruction tuning, using 5 different tasks including: captioning, conversations,  visual question answering, reasoning and classification, with data coming from LLaVA~\cite{llava}, VideoChat~\cite{videochat}, VideoChatGPT~\cite{videochatgpt}, COCO Captions~\cite{coco}, WebVid~\cite{Bain2021FrozenIT}, YouCook~\cite{Das2013ATF}, OK-VQA~\cite{okvqa}, AOK-VQA~\cite{aokvqa}, DocVQA~\cite{docvqa}, CLEVR~\cite{clevr}, CLEVRER~\cite{clevrer} and NExT-QA~\cite{nextqa} among others.
We use this model under their MIT License.

\subsection{Datasets}
\label{app:datasets}

\paragraph{VideoCon.}
VideoCon is constructed by generating contrastive video captions and explanations for different subset of videos~\cite{Xu2016MSRVTTAL, wang2019vatex, hendricks-etal-2018-localizing}. 
This dataset contains seven misaligned types that include replacement of objects, actions, attributes, counts and relations, and adds hallucinations (i.e. unrelated but plausible information). We use this dataset under their MIT License.

\paragraph{VELOCITI.}
The duration of the video clips in the dataset is 10 seconds, and has dense text annotations on action and role descriptions.
The perception-based tests require discriminating video-caption pairs that share similar entities, and the binding tests require models to associate the correct entity to a given situation while ignoring the different yet plausible entities that also appear in the same video.
There are 1000 tests using 643 videos for Agent Iden, 1676 tests using 707 videos for Agent Bind, 418 tests using 270 videos for Agent Coref, 500 tests using 400 videos for Action Adv, 1625 tests using 590 videos for Action Bind, 500 tests using 411 videos for Action Mod, and 1908 tests using 669 videos for Chrono.  
We use this dataset under their Creative Commons Public Licenses.

\paragraph{SSv2-Temporal and SSv2-Events.}
SSv2-Temporal contains a list of 18 actions that require models to capture rich temporal information in the video, consisting of 216 (18 ×12) candidate videos for every text
action query.
SSv2-Events has 49 actions that consist two verbs in the action templates that are indicative of multiple events in the video,  consisting of 2888 (49×12) candidate videos for
every text action query.
