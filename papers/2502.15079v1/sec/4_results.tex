\section{Experimental Setup}
\label{sec:exp_setup}

\looseness=-1
\paragraph{Data (detailed in \autoref{app:datasets}).}

We train \method{} using videos and their ground-truth and contrastive descriptions from VideoCon \cite{bansal2024videocon}, generating 115,536 (video, description, correction) triplets for training and 8,312 for validation, which is used for model selection. Synthetic contrast captions are also used to fine-tune the \textit{baseline} entailment task with the same dataset sizes.

We evaluate our trained models on text-to-video retrieval using the temporally-challenging SSv2-Temporal \cite{sevilla2021only} and action-intensive SSv2-Events \cite{bagad2023test}  datasets.
Additionally, we evaluate our models on 
compositional ability over time using the VELOCITI benchmark \cite{saravanan2024velociti}. Each video in the dataset includes a correct caption and an incorrect one.


\paragraph{Baselines.}
(i) \textbf{Pretrained Video-LLMs}:
we employ two pre-trained models with different architectures, \textit{Video-LLaVA}~\cite{lin2023video} 
and
\textit{VideoChat2}~\cite{li2023videochat}. 
More details in \autoref{app:pretrained_vlm}.
(ii) \textbf{Entailment}:
we fine-tune the pretrained Video-LLMs using the entailment task described in \autoref{sec:prelim}. 
More details about the implementation are in \autoref{app:implementation}.


\paragraph{Evaluation metrics.}

We report the accuracy on the VELOCITI benchmark as the proportion of examples in which the positive video-caption pair receives a higher \textpr{Yes} entailment probability than the corresponding negative video-caption pair.
For SSv2, we compute \textpr{Yes} probabilities for each text-video pair, rank their scores, and report mean Average Precision (mAP).


\input{figures/ssv2_chart}
\input{tables/velociti}

