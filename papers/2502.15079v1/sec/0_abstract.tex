\begin{abstract}
Large Vision-Language Models often generate hallucinated content that is not grounded in its visual inputs.
While prior work focuses on mitigating hallucinations, we instead explore leveraging hallucination correction as a training objective to improve video-language alignment.
We introduce \method{}, a self-training framework learning to correct hallucinations in descriptions that do not align with the video content.
By identifying and correcting inconsistencies, \method{} enhances the modelâ€™s ability to align video and textual representations for spatio-temporal reasoning.
Our experimental results show consistent gains in video-caption binding 
and text-to-video retrieval %
tasks, demonstrating that hallucination correction-inspired tasks serve as an effective strategy for improving vision and language alignment. 


\end{abstract}
