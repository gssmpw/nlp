\input{figures/all_tasks}

\section{HACA: Hallucination Correction for Video-language Alignment}
To investigate whether hallucination correction can improve video-language alignment, we introduce \method{}, 
a fine-tuning objective for Video-LLM
as a sequence-to-sequence generation task.

\subsection{Preliminaries: Video-LLMs}
\label{sec:prelim}

Video-LLMs typically consist of three parts: i) a visual encoder to map images and videos to visual representations; ii)
an LLM that takes text instructions as inputs to generate text responses; and iii) an adapter between visual and text representations. Our approach finetunes (ii) the text decoder and (iii) the adapter, freezing (i) the visual encoder.


\paragraph{Pre-training.}
A Video-LLM $M_\theta$ parameterized by $\theta$ takes a textual question or instruction $\instr$, a video $\video$ as input, and generates a text response $\answer$ = $(\answer_1, \answer_2, ..., \answer_T)$ autoregressively using a decoder-based language model (LLM) as output, by estimating a conditional distribution  $M(\answer \mid \instr, \video)$.
This is achieved by training the model using the maximum-likelihood estimation (MLE) objective:
\begin{equation}
 L(\theta) = \sum_{\mathcal{D}_{\textrm{train}}}  \sum_{t=1}^{T} \log M_{\theta} (\answer_t \mid \answer_{<t}, \instr, \video)
 \label{equation:videollm_training}
\end{equation}
where $\answer_t$ is $t$-th word of the text response, and $\answer_{<t}$ are the first $t - 1$ words of the response.
The dataset $\mathcal{D}_{\textrm{train}}$ consists of samples in the form $(\instr, \answer, \video)$.


\paragraph{Fine-tuning with entailment.}
\label{method:prior_finetuning}
Following \citet{bansal2024videocon}, we finetune the Video-LLM using an entailment task, where
the text input $\instr$ is formatted as an entailment question %
as $\instr(\sent) =$ \textpr{Does this caption accurately describe the video? Caption: \{$\sent$\}}.
In this task, the output of the model $\answer$ is \textpr{Yes} or \textpr{No} (\autoref{fig:tasks} (a)).
Given a dataset $D_{train}$ consisting of ground-truth answers $\answer$ for $\instr(\sent)$ and $\video$, the model is fine-tuned to have a better estimation of $M_\theta(\textpr{Yes} ~|~ \instr(\sent), \video) $ and $M_{\theta}(\textpr{No} ~|~ \instr(\sent), \video)$ using the MLE objective:
\begin{equation}
 L_{ent}(\theta) = \sum_ {\mathcal{D}_{\textrm{train}}}   \log M_{\theta} (\answer \mid \instr(\sent), \video)
 \label{equation:entail_training}
\end{equation}


\subsection{Learning from Hallucination Correction}
\label{sec:hallucination_correction}

Building on the work of \citet{bansal2024videocon}, the Video-LLM takes the question $\instr$ and the video $\video$ as input to determine whether a text description $\sent$ entails the video (similar to~\autoref{method:prior_finetuning}). 
However, in our setting, if $\sent$ does not entail $\video$, the model generates a \textit{corrected} caption $\des=(w_1, w_2, ..., w_n)$ to align the description with the video content.
During fine-tuning, if $\sent$ entails $\video$, the model is trained to generate the response as
$\answer(\textpr{Yes}) =$ \textpr{Yes, the caption accurately describes the video}.
If $\sent$ does not entail $\video$, the model is trained to generate a corrected description $\des$ as its response, formatted as
$\answer(\textpr{No}, \des) =$ \textpr{No. This caption shall be corrected as: \{$\des$\}}.
We show an example in \autoref{fig:tasks} (b). In contrast to finetuning using \textit{entailment} only, 
our \textit{hallucination correction} objective trains the model 
to have better estimation of $M_\theta(\answer(\textpr{Yes}) ~|~ \instr(\sent), \video) $ and $M_\theta(\answer(\textpr{No}, \des) | \instr(\sent), \video) $.

Instead of using ~\autoref{equation:entail_training}, given a training dataset $\mathcal{D}_{\textrm{train}}$ that consists of video $\video$ and ground-truth text description $\des$ pairs,
we fine-tune the Video-LLM using the MLE objective:
\begin{equation}
 L_{c}(\theta) = \sum_{\mathcal{D}_{\textrm{train}}} \sum_{t=1}^{T} \log M_{\theta} (\answer_t \mid \answer_{<t}, \instr(\sent), \video)
 \label{equation:haca}
\end{equation}
where $\answer_t$ is the $t$-th word of the text response of $\answer(\textpr{Yes})$ or $\answer$(\textpr{No}, $\des$), and $\answer_{<t}$ is the first $t - 1$ words of the text response.


\subsection{Masking Correction as Augmentation}
\label{sec:masking_correction}

We also incorporate a masking correction task as data augmentation (\autoref{fig:tasks} (c)),
where an instruction $\instr$ prompts the Video-LLM to make corrections to a masked caption $\mask$, teaching the model to generate a corrected caption that contains a sequence of words $\des=(w_1, w_2, ..., w_n)$ as its answer
by estimating conditional probability $M(\des \mid \instr(\mask), \video)$.
Specifically, $\instr$ is a function that formats the text instruction as 
$\instr(\mask) = $ \textpr{Please correct this caption to accurately describe the video. Caption: $\mask$},
where $\mask$ is masked from $\des$: $\mask = (w_1, \textpr{[MASK]}, ..., w_n )$, by randomly masking 45\% of the content words in the ground truth video description $\des$. 

We finetune the model using two objectives: the MLE objective to estimate the probability for masking correction $M_\theta(\des| \instr(\mask), \video)$, and the \method{} objective (\autoref{equation:haca}).
The model is tasked with providing responses corresponding to different instructions. 

