Our proposed method assumes the availability of ground-truth video caption annotations for fine-tuning using hallucination correction. 
Additionally, the method assumes a clear separation between the parameters of the video representations and those of the language model, as we freeze the video encoder parameters during fine-tuning to align video-language representations. 
Another limitation is that our approach has not been evaluated on long videos, due to the limitation of computational resources. We envision future work in this direction. 
