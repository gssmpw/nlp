\section{Related Work}
\label{app:related_work}


\noindent\textbf{Alignment in Video-Language Models} is fundamental for the logical integration of video and textual information. To align both modalities, prior work has focused on pre-training models with different objectives to capture the temporal dynamics in video. 
While these self-supervised correction objectives are highly effective during pre-training~\cite{li2023lavender, wang2022ofa, zhu2024languagebind, ge2022bridging}, fine-tuning is typically required to adapt Video-LLMs to specific downstream tasks~\cite{videochat, videollama, bansal2024videocon} (e.g., classification, retrieval, or question answering).
However, these objectives rely on coarse-grained alignment labels and do not provide detailed feedback for resolving inconsistencies between video and language.

\noindent\textbf{Hallucination Correction} methods aim to mitigate the generation of content that does not align with the data a model was trained on, or the model describes content that does not exist in the provided input~\cite{huang2024opera}. Orthogonal to our proposed method, LURE~\cite{zhou2024analyzing} uses statistical analysis to identify and rectify errors in generated descriptions, addressing co-occurrence, uncertainty, and positional factors via masking. In our work, we randomly mask the video description so that the model is required to output the corrected sentence, which is also conditioned in the input video via visual entailment. 
\citet{yin2023woodpecker, wang2023paxion} uses external models and measures to correct hallucinations to be consistent with images or videos.
\citet{zhou-etal-2021-detecting, liu2023mitigating, xiao2024detecting, zhao-etal-2024-successfully} create a synthetic dataset to train a specialized model to detect and correct hallucinations.
\citet{dale2022detecting, huang2024opera} shows promising results on correcting hallucinations without an external model for machine translation and image captioning.
In our work, we investigate leveraging hallucination as a training objective to improve video-language alignment, by exploring the potential of using a video-LLM model itself to correct hallucinations through fine-tuning on a synthetic dataset.
