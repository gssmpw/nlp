\section{Introduction}
\label{sec:intro}


Aligning representations across modalities involves creating joint embeddings that map visual and linguistic features to a shared space, enabling the model to assess their similarity.
This is crucial for tasks including cross-modal retrieval \cite{xu2016msr}, mitigating hallucinations \cite{jiang2024hallucination},
compositional reasoning \cite{cascante2024natural}, 
visual-to-text generation \cite{blip2}, visual question answering \cite{shen2022how}, and visual-language navigation \cite{zhao2021evaluation}.
While progress has been made in aligning image representations with text \cite{li2020oscar, zeng2021multi, wang2023image, lin2025evaluating}, advancements in video-language alignment remain limited.
Videos pose unique challenges due to their rich spatio-temporal information, involving multiple entities and scenes that dynamically interact and change over time.
Video-language models (Video-LLMs in \autoref{sec:prelim}) can compute alignment scores \cite{lin2023video, li2023videochat} but struggle to distinguish between similar videos and descriptions \cite{park2022exposing, wang2023paxion, saravanan2024velociti},
as illustrated in \autoref{fig:haca}. 
One promising approach is to fine-tune Video-LLMs on entailment tasks using similar captions~\cite{bansal2024videocon}, where the model is prompted to answer \textpr{Yes} or \textpr{No} to whether a video is aligned with a given caption (\autoref{sec:prelim}). 
However, using a single binary label as a learning signal fails to indicate which parts of the description misalign with the video. 
\citet{bansal2024videocon} generates natural language explanations for mismatches but requires costly dataset construction with additional models and annotations.

\input{figures/haca}

To this end, we introduce \textbf{\method{}}, a self-training framework grounded in \textbf{HA}llucination \textbf{C}orrection for video-language \textbf{A}lignment (\autoref{sec:hallucination_correction}).
\textit{Hallucination} (or confabulation) refers to a mismatch between textual descriptions and the corresponding factual content of an image or video \cite{liu2024survey}.
\method{} requires the model to predict whether a description entails the video content. If the description does not align, the model corrects the hallucinations to better match the video.
Instead of relying solely on a binary entailment label, \method{} uses hallucination correction as a finer-grained learning signal to enhance the alignment of video and language representations.
Given that misalignment between modalities is a key factor in hallucination \cite{biten2022let, sun-etal-2024-aligning}, we hypothesize that introducing a hallucination correction task can improve video-language alignment.
\method{} also requires no external models or annotations beyond the ground-truth video description. 
To further enhance \method{}, we introduce a masking correction task as data augmentation
(\autoref{sec:masking_correction}).

We fine-tune two Video-LLMs with \method{},
and evaluate these fine-tuned models in a zero-shot manner on two spatio-temporally challenging downstream tasks (\autoref{sec:exp_setup}): VELOCITI~\cite{saravanan2024velociti}, a video-caption binding dataset, and SSv2-Temporal \cite{sevilla2021only} and SSv2-Events \cite{bagad2023test}, which are text-to-video retrieval datasets emphasizing action recognition.
The models fine-tuned with \method{} outperform baseline models by up to 17.9\% accuracy and 5.7 mAP points, 
demonstrating that \method{} effectively improves video-text alignments, and generalizes beyond in-domain data (\autoref{sec:analysis}). 
Our code and data will be available upon acceptance.


