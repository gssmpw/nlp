
@inproceedings{schulz_learning_2024,
	title = {Learning {When} to {Trust} the {Expert} for {Guided} {Exploration} in {RL}},
	booktitle = {{ICML} 2024 {Workshop}: {Foundations} of {Reinforcement} {Learning} and {Control} – {Connections} and {Perspectives}},
	author = {Schulz, Felix and Hoffmann, Jasper and Zhang, Yuan and Boedecker, Joschka},
	year = {2024},
}

@inproceedings{hoffmann_plannetx_2024,
	title = {{PlanNetX}: {Learning} an efficient neural network planner from {MPC} for longitudinal control},
	shorttitle = {{PlanNetX}},
	abstract = {Model predictive control (MPC) is a powerful, optimization-based approach for controlling dynamical systems. However, the computational complexity of online optimization can be problematic on embedded devices. Especially, when we need to guarantee fixed control frequencies. Thus, previous work proposed to reduce the computational burden using imitation learning (IL) approximating the MPC policy by a neural network. In this work, we instead learn the whole planned trajectory of the MPC. We introduce a combination of a novel neural network architecture PlanNetX and a simple loss function based on the state trajectory that leverages the parameterized optimal control structure of the MPC. We validate our approach in the context of autonomous driving by learning a longitudinal planner and benchmarking it extensively in the CommonRoad simulator using synthetic scenarios and scenarios derived from real data. Our experimental results show that we can learn the open-loop MPC trajectory with high accuracy while improving the closed-loop performance of the learned control policy over other baselines like behavior cloning.},
	language = {en},
	urldate = {2025-01-30},
	booktitle = {Proceedings of the 6th {Annual} {Learning} for {Dynamics} \& {Control} {Conference}},
	publisher = {PMLR},
	author = {Hoffmann, Jasper and Clausen, Diego Fernandez and Brosseit, Julien and Bernhard, Julian and Esterle, Klemens and Werling, Moritz and Karg, Michael and Bödecker, Joschka Joschka},
	month = jun,
	year = {2024},
	note = {ISSN: 2640-3498},
	pages = {1214--1227},
}

@article{diehl_formulation_2007,
	title = {Formulation of {Closed} {Loop} {Min}-{Max} {MPC} as a {Quadratically} {Constrained} {Quadratic} {Program}},
	volume = {52},
	doi = {10.1109/TAC.2006.890372},
	number = {2},
	journal = {IEEE Transactions on Automatic Control},
	author = {Diehl, Moritz},
	year = {2007},
	keywords = {optec mpc robust optimal control syscop-public},
	pages = {339--343},
}

@article{van_parys_distributionally_2016,
	title = {Distributionally {Robust} {Control} of {Constrained} {Stochastic} {Systems}},
	volume = {61},
	doi = {10.1109/TAC.2015.2444134},
	number = {2},
	journal = {IEEE Transactions on Automatic Control},
	author = {Van Parys, Bart P. G. and Kuhn, Daniel and Goulart, Paul J. and Morari, Manfred},
	year = {2016},
	keywords = {Control design, Linear time-invariant (LTI), Loss measurement, Q measurement, Random variables, Reactive power, Robustness, Tin, stochastic system},
	pages = {430--442},
}

@article{grune_economic_2013,
	title = {Economic receding horizon control without terminal constraints},
	volume = {49},
	issn = {0005-1098},
	url = {https://www.sciencedirect.com/science/article/pii/S0005109812006024},
	doi = {https://doi.org/10.1016/j.automatica.2012.12.003},
	abstract = {We consider a receding horizon control scheme without terminal constraints in which the stage cost is defined by economic criteria, i.e., not necessarily linked to a stabilization or tracking problem. We analyze the performance of the resulting receding horizon controller with a particular focus on the case of optimal steady states for the corresponding averaged infinite horizon problem. Using a turnpike property and suitable controllability properties we prove near optimal performance of the controller and convergence of the closed loop solution to a neighborhood of the optimal steady state. Two examples illustrate our findings numerically and show how to verify the imposed assumptions.},
	number = {3},
	journal = {Automatica},
	author = {Grüne, Lars},
	year = {2013},
	keywords = {Controllability, Economic MPC, Turnpike property},
	pages = {725--734},
}

@incollection{binder_introduction_2001,
	title = {Introduction to {Model} {Based} {Optimization} of {Chemical} {Processes} on {Moving} {Horizons}},
	url = {http://www.kuleuven.be/optec/OLD/research/subgroups/fastMPC/publications/Binder2001.php},
	booktitle = {Online {Optimization} of {Large} {Scale} {Systems}: {State} of the {Art}},
	publisher = {Springer},
	author = {Binder, T. and Blank, L. and Bock, H. G. and Bulirsch, R. and Dahmen, W. and Diehl, M. and Kronseder, T. and Marquardt, W. and Schlöder, J. P. and Stryk, O. v},
	editor = {Grötschel, M. and Krumke, S. O. and Rambau, J.},
	year = {2001},
	keywords = {chemistry optimal control NMPC agbock syscop-public},
	pages = {295--340},
}

@article{sutton_between_1999,
	title = {Between {MDPs} and semi-{MDPs}: {A} framework for temporal abstraction in reinforcement learning},
	volume = {112},
	number = {1-2},
	journal = {Artificial intelligence},
	author = {Sutton, Richard S and Precup, Doina and Singh, Satinder},
	year = {1999},
	note = {Publisher: Elsevier},
	pages = {181--211},
}

@article{robbins_stochastic_1951,
	title = {A {Stochastic} {Approximation} {Method}},
	volume = {22},
	issn = {0003-4851},
	abstract = {Let M(x) denote the expected value at level x of the response to a certain experiment. M(x) is assumed to be a monotone function of x but is unknown to the experimenter, and it is desired to find the solution x = θ of the equation M(x) = α, where α is a given constant. We give a method for making successive experiments at levels x1,x2,⋯ in such a way that xn will tend to θ in probability.},
	number = {3},
	urldate = {2024-03-05},
	journal = {The Annals of Mathematical Statistics},
	author = {Robbins, Herbert and Monro, Sutton},
	year = {1951},
	pages = {400--407},
}

@article{zarrouki_safe_2024,
	series = {{IEEE} {Intelligent} {Vehicles} {Symposium}, {Proceedings}},
	title = {A {Safe} {Reinforcement} {Learning} driven {Weights}-varying {Model} {Predictive} {Control} for {Autonomous} {Vehicle} {Motion} {Control}: 35th {IEEE} {Intelligent} {Vehicles} {Symposium}, {IV} 2024},
	shorttitle = {A {Safe} {Reinforcement} {Learning} driven {Weights}-varying {Model} {Predictive} {Control} for {Autonomous} {Vehicle} {Motion} {Control}},
	doi = {10.1109/IV55156.2024.10588747},
	abstract = {Determining the optimal cost function parameters of Model Predictive Control (MPC) to optimize multiple control objectives is a challenging and time-consuming task. Multi-objective Bayesian Optimization (BO) techniques solve this problem by determining a Pareto optimal parameter set for an MPC with static weights. However, a single parameter set may not deliver the most optimal closed-loop control performance when the context of the MPC operating conditions changes during its operation, urging the need to adapt the cost function weights at runtime. Deep Reinforcement Learning (RL) algorithms can automatically learn context-dependent optimal parameter sets and dynamically adapt for a Weights-varying MPC (WMPC). However, learning cost function weights from scratch in a continuous action space may lead to unsafe operating states. To solve this, we propose a novel approach limiting the RL action space within a safe learning space that we represent by a catalog of pre-optimized feasible BO Pareto-optimal weight sets. We conceive an RL agent not to learn in a continuous space but to select the most optimal discrete actions, each corresponding to a single set of Pareto optimal weights, by proactively anticipating upcoming control tasks in a context-dependent manner. This approach introduces a two-step optimization: (1) safety-critical with BO and (2) performance-driven with RL. Hence, even an untrained RL agent guarantees a safe and optimal performance. Simulation results demonstrate that an untrained RL-WMPC shows Pareto-optimal closed-loop behavior and training the RL-WMPC helps exhibit a performance beyond the Pareto-front. The code used in this research is publicly accessible as open-source software: https://github.com/bzarr/TUM-CONTROL},
	urldate = {2025-01-04},
	journal = {35th IEEE Intelligent Vehicles Symposium, IV 2024},
	author = {Zarrouki, Baha and Spanakakis, Marios and Betz, Johannes},
	year = {2024},
	pages = {1401--1408},
}

@misc{li_safe_2021,
	title = {A {Safe} {Hierarchical} {Planning} {Framework} for {Complex} {Driving} {Scenarios} based on {Reinforcement} {Learning}},
	abstract = {Autonomous vehicles need to handle various trafﬁc conditions and make safe and efﬁcient decisions and maneuvers. However, on the one hand, a single optimization/samplingbased motion planner cannot efﬁciently generate safe trajectories in real time, particularly when there are many interactive vehicles near by. On the other hand, end-to-end learning methods cannot assure the safety of the outcomes. To address this challenge, we propose a hierarchical behavior planning framework with a set of low-level safe controllers and a high-level reinforcement learning algorithm (H-CtRL) as a coordinator for the low-level controllers. Safety is guaranteed by the low-level optimization/sampling-based controllers, while the high-level reinforcement learning algorithm makes H-CtRL an adaptive and efﬁcient behavior planner. To train and test our proposed algorithm, we built a simulator that can reproduce trafﬁc scenes using real-world datasets. The proposed HCtRL is proved to be effective in various realistic simulation scenarios, with satisfying performance in terms of both safety and efﬁciency.},
	language = {en},
	urldate = {2024-08-17},
	publisher = {arXiv},
	author = {Li, Jinning and Sun, Liting and Chen, Jianyu and Tomizuka, Masayoshi and Zhan, Wei},
	month = jun,
	year = {2021},
	note = {arXiv:2101.06778 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
}

@article{moraes_neural_2020,
	title = {A {Neural} {Network} {Architecture} to {Learn} {Explicit} {MPC} {Controllers} from {Data}},
	issn = {2405-8963},
	doi = {10.1016/j.ifacol.2020.12.546},
	abstract = {We present a methodology to learn explicit Model Predictive Control (eMPC) laws from sample data points with tunable complexity. The learning process is cast in a special Neural Network setting where the coefficients of two linear layers and a parametric quadratic program (pQP) implicit layer are optimized to fit the training data. Thanks to this formulation, powerful tools from the machine learning community can be exploited to speed up the offline computations through high parallelization. The final controller can be deployed via low-complexity eMPC and the resulting closed-loop system can be certified for stability using existing tools available in the literature. A numerical example on the voltage-current regulation of a multicell DC-DC converter is provided, where the storage and on-line computational demands of the initial controller are drastically reduced with negligible performance impact. Copyright (C) 2020 The Authors},
	journal = {Ifac Papersonline},
	editor = {Moraes, C. G. da S. and Waltrich, G. and Jones, Colin and Maddalena, Emilio},
	year = {2020},
	keywords = {Automation \& Control Systems, complexity, data-driven control, explicit model predictive control, machine learning, neural networks, point location problem, power electronics, predictive control},
}

@book{gauss_theoria_1809,
	title = {Theoria motus corporum coelestium in sectionibus conicis solem ambientium},
	volume = {7},
	publisher = {Perthes et Besser},
	author = {Gauss, Carl Friedrich},
	year = {1809},
}

@article{rakovic_homothetic_2022,
	title = {Homothetic {Tube} {Model} {Predictive} {Control} for {Nonlinear} {Systems}},
	volume = {68},
	number = {8},
	journal = {IEEE Trans. Automat. Control},
	author = {Raković, Saša V. and Dai, Li and Xia, Yuanqing},
	year = {2022},
}

@inproceedings{gillis_positive_2013,
	title = {A {Positive} {Definiteness} {Preserving} {Discretization} {Method} for nonlinear {Lyapunov} {Differential} {Equations}},
	booktitle = {{CDC}},
	author = {Gillis, J. and Diehl, M.},
	year = {2013},
	keywords = {syscop-public highwind},
}

@book{kurzhanski_ellipsoidal_1997,
	title = {Ellipsoidal {Calculus} for {Estimation} and {Control}},
	publisher = {Birkhäuser Boston},
	author = {Kurzhanski, A. B. and Valyi, P.},
	year = {1997},
}

@article{scokaert_min-max_1998,
	title = {Min-max feedback model predictive control for constrained linear systems},
	volume = {43},
	journal = {IEEE Transactions on Automatic Control},
	author = {Scokaert, P. O. M. and Mayne, D. Q.},
	year = {1998},
	keywords = {optimal control NMPC},
	pages = {1136--1142},
}

@incollection{rakovic_robust_2019,
	address = {London},
	title = {Robust {Model} {Predictive} {Control}},
	isbn = {978-1-4471-5102-9},
	booktitle = {Encyclopedia of {Systems} and {Control}},
	publisher = {Springer London},
	author = {Raković, Saša V.},
	editor = {Baillieul, John and Samad, Tariq},
	year = {2019},
	doi = {10.1007/978-1-4471-5102-9_2-3},
	pages = {1--11},
}

@article{fleming_robust_2015,
	title = {Robust {Tube} {MPC} for {Linear} {Systems} {With} {Multiplicative} {Uncertainty}},
	volume = {60},
	number = {4},
	journal = {IEEE Trans. Automat. Control},
	author = {Fleming, James and Kouvaritakis, Basil and Cannon, Mark},
	year = {2015},
}

@phdthesis{houska_robust_2011,
	type = {{PhD} {Thesis}},
	title = {Robust {Optimization} of {Dynamic} {Systems}},
	school = {KU Leuven},
	author = {Houska, B.},
	year = {2011},
	keywords = {syscop-public highwind},
}

@inproceedings{reiter_inverse_2022,
	title = {An {Inverse} {Optimal} {Control} {Approach} for {Trajectory} {Prediction} of {Autonomous} {Race} {Cars}},
	doi = {10.23919/ECC55457.2022.9838100},
	booktitle = {2022 {European} {Control} {Conference} ({ECC})},
	author = {Reiter, Rudolf and Messerer, Florian and Schratter, Markus and Watzenig, Daniel and Diehl, Moritz},
	year = {2022},
	keywords = {Approximation algorithms, Costs, Optimal control, Prediction algorithms, Real-time systems, Trajectory, Uncertainty},
	pages = {146--153},
}

@article{reiter_mixed-integer_2021,
	series = {7th {IFAC} {Conference} on {Nonlinear} {Model} {Predictive} {Control} {NMPC} 2021},
	title = {Mixed-integer optimization-based planning for autonomous racing with obstacles and rewards},
	volume = {54},
	issn = {2405-8963},
	doi = {10.1016/j.ifacol.2021.08.530},
	abstract = {Trajectory planning with the consideration of obstacles is a classical task in autonomous driving and robotics applications. This paper introduces a novel solution approach for the subclass of autonomous racing problems which is additionally capable of dealing with reward objects. This special type of objects is representing particular regions in state space, whose optional reaching is somehow beneficial (e.g. results in bonus points during a race). First, a homotopy class is selected which represents the left/right and catch/ignore decisions related to obstacle avoidance and reward collection, respectively. For this purpose, a linear mixed-integer problem is posed such that an approximated combinatorial problem is solved and repetitive switching decisions between solver calls are avoided. Secondly, an optimal control problem (OCP) based on a single-track vehicle model is solved within this homotopy class. In the corresponding nonlinear program, homotopy iterations are performed on the race track boundaries which correspond to the previously chosen homotopy class. This leads to an improved convergence of the solver compared to the direct approach. The mixed-integer method’s effectiveness is demonstrated within a real-world test scenario during the autonomous racing competition Roborace. Furthermore, its combination with the OCP as well as the performance gain resulting from the homotopy iterations are shown in simulation.},
	number = {6},
	urldate = {2024-05-06},
	journal = {IFAC-PapersOnLine},
	author = {Reiter, Rudolf and Kirchengast, Martin and Watzenig, Daniel and Diehl, Moritz},
	month = jan,
	year = {2021},
	keywords = {autonomous mobile robots, autonomous vehicles, integer programming, obstacle avoidance, optimal trajectory, planning},
	pages = {99--106},
}

@article{kiureghian_aleatory_2009,
	series = {Risk {Acceptance} and {Risk} {Communication}},
	title = {Aleatory or epistemic? {Does} it matter?},
	volume = {31},
	issn = {0167-4730},
	shorttitle = {Aleatory or epistemic?},
	url = {https://www.sciencedirect.com/science/article/pii/S0167473008000556},
	doi = {10.1016/j.strusafe.2008.06.020},
	abstract = {The sources and characters of uncertainties in engineering modeling for risk and reliability analyses are discussed. While many sources of uncertainty may exist, they are generally categorized as either aleatory or epistemic. Uncertainties are characterized as epistemic, if the modeler sees a possibility to reduce them by gathering more data or by refining models. Uncertainties are categorized as aleatory if the modeler does not foresee the possibility of reducing them. From a pragmatic standpoint, it is useful to thus categorize the uncertainties within a model, since it then becomes clear as to which uncertainties have the potential of being reduced. More importantly, epistemic uncertainties may introduce dependence among random events, which may not be properly noted if the character of uncertainties is not correctly modeled. Influences of the two types of uncertainties in reliability assessment, codified design, performance-based engineering and risk-based decision-making are discussed. Two simple examples demonstrate the influence of statistical dependence arising from epistemic uncertainties on systems and time-variant reliability problems.},
	number = {2},
	urldate = {2025-01-28},
	journal = {Structural Safety},
	author = {Kiureghian, Armen Der and Ditlevsen, Ove},
	month = mar,
	year = {2009},
	keywords = {Aleatory, Epistemic, Ergodicity, Parameter uncertainty, Predictive models, Probability distribution choice, Statistical dependence, Systems, Time-variant reliability, Uncertainty},
	pages = {105--112},
}

@inproceedings{nikishin_control-oriented_2022,
	title = {Control-oriented model-based reinforcement learning with implicit differentiation},
	volume = {36},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Nikishin, Evgenii and Abachi, Romina and Agarwal, Rishabh and Bacon, Pierre-Luc},
	year = {2022},
	note = {Issue: 7},
	pages = {7886--7894},
}

@article{schrittwieser_mastering_2020,
	title = {Mastering atari, go, chess and shogi by planning with a learned model},
	volume = {588},
	number = {7839},
	journal = {Nature},
	author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore},
	year = {2020},
	note = {ISBN: 1476-4687
Publisher: Nature Publishing Group},
	pages = {604--609},
}

@misc{silver_mastering_2017,
	title = {Mastering {Chess} and {Shogi} by {Self}-{Play} with a {General} {Reinforcement} {Learning} {Algorithm}},
	doi = {10.48550/arXiv.1712.01815},
	abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
	urldate = {2025-01-15},
	publisher = {arXiv},
	author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
	month = dec,
	year = {2017},
	note = {arXiv:1712.01815 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{silver_mastering_2017-1,
	title = {Mastering the game of {Go} without human knowledge},
	volume = {550},
	copyright = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
	issn = {1476-4687},
	doi = {10.1038/nature24270},
	abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.},
	language = {en},
	number = {7676},
	urldate = {2025-01-15},
	journal = {Nature},
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
	month = oct,
	year = {2017},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational science, Computer science, Reward},
	pages = {354--359},
}

@misc{ball_offcon3_2021,
	title = {{OffCon3}: {What} is state of the art anyway?},
	shorttitle = {{OffCon}\${\textasciicircum}3\$},
	doi = {10.48550/arXiv.2101.11331},
	abstract = {Two popular approaches to model-free continuous control tasks are SAC and TD3. At first glance these approaches seem rather different; SAC aims to solve the entropy-augmented MDP by minimising the KL-divergence between a stochastic proposal policy and a hypotheical energy-basd soft Q-function policy, whereas TD3 is derived from DPG, which uses a deterministic policy to perform policy gradient ascent along the value function. In reality, both approaches are remarkably similar, and belong to a family of approaches we call `Off-Policy Continuous Generalized Policy Iteration'. This illuminates their similar performance in most continuous control benchmarks, and indeed when hyperparameters are matched, their performance can be statistically indistinguishable. To further remove any difference due to implementation, we provide OffCon\${\textasciicircum}3\$ (Off-Policy Continuous Control: Consolidated), a code base featuring state-of-the-art versions of both algorithms.},
	urldate = {2025-01-14},
	publisher = {arXiv},
	author = {Ball, Philip J. and Roberts, Stephen J.},
	month = mar,
	year = {2021},
}

@article{lambrechts_recurrent_2022,
	title = {Recurrent networks, hidden states and beliefs in partially observable environments},
	issn = {2835-8856},
	abstract = {Reinforcement learning aims to learn optimal policies from interaction with environments whose dynamics are unknown. Many methods rely on the approximation of a value function to derive near-optimal policies. In partially observable environments, these functions depend on the complete sequence of observations and past actions, called the history. In this work, we show empirically that recurrent neural networks trained to approximate such value functions internally filter the posterior probability distribution of the current state given the history, called the belief. More precisely, we show that, as a recurrent neural network learns the Q-function, its hidden states become more and more correlated with the beliefs of state variables that are relevant to optimal control. This correlation is measured through their mutual information. In addition, we show that the expected return of an agent increases with the ability of its recurrent architecture to reach a high mutual information between its hidden states and the beliefs. Finally, we show that the mutual information between the hidden states and the beliefs of variables that are irrelevant for optimal control decreases through the learning process. In summary, this work shows that in its hidden states, a recurrent neural network approximating the Q-function of a partially observable environment reproduces a sufficient statistic from the history that is correlated to the relevant part of the belief for taking optimal actions.},
	language = {en},
	urldate = {2025-01-21},
	journal = {Transactions on Machine Learning Research},
	author = {Lambrechts, Gaspard and Bolland, Adrien and Ernst, Damien},
	month = may,
	year = {2022},
}

@inproceedings{achiam_constrained_2017,
	title = {Constrained {Policy} {Optimization}},
	abstract = {For many applications of reinforcement learning it can be more convenient to specify both a reward function and constraints, rather than trying to design behavior through the reward function. For example, systems that physically interact with or around humans should satisfy safety constraints. Recent advances in policy search algorithms (Mnih et al., 2016, Schulman et al., 2015, Lillicrap et al., 2016, Levine et al., 2016) have enabled new capabilities in high-dimensional control, but do not consider the constrained setting. We propose Constrained Policy Optimization (CPO), the first general-purpose policy search algorithm for constrained reinforcement learning with guarantees for near-constraint satisfaction at each iteration. Our method allows us to train neural network policies for high-dimensional control while making guarantees about policy behavior all throughout training. Our guarantees are based on a new theoretical result, which is of independent interest: we prove a bound relating the expected returns of two policies to an average divergence between them. We demonstrate the effectiveness of our approach on simulated robot locomotion tasks where the agent must satisfy constraints motivated by safety.},
	language = {en},
	urldate = {2025-01-27},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {22--31},
}

@article{calvo-fullana_state_2024,
	title = {State {Augmented} {Constrained} {Reinforcement} {Learning}: {Overcoming} the {Limitations} of {Learning} {With} {Rewards}},
	volume = {69},
	issn = {1558-2523},
	shorttitle = {State {Augmented} {Constrained} {Reinforcement} {Learning}},
	doi = {10.1109/TAC.2023.3319070},
	abstract = {A common formulation of constrained reinforcement learning involves multiple rewards that must individually accumulate to given thresholds. In this class of problems, we show a simple example in which the desired optimal policy cannot be induced by any weighted linear combination of rewards. Hence, there exist constrained reinforcement learning problems for which neither regularized nor classical primal-dual methods yield optimal policies. This work addresses this shortcoming by augmenting the state with Lagrange multipliers and reinterpreting primal-dual methods as the portion of the dynamics that drives the multiplier evolution. This approach provides a systematic state augmentation procedure that is guaranteed to solve reinforcement learning problems with constraints. Thus, as we illustrate by an example, while previous methods can fail at finding optimal policies, running the dual dynamics while executing the augmented policy yields an algorithm that provably samples actions from the optimal policy.},
	number = {7},
	urldate = {2025-01-27},
	journal = {IEEE Transactions on Automatic Control},
	author = {Calvo-Fullana, Miguel and Paternain, Santiago and Chamon, Luiz F. O. and Ribeiro, Alejandro},
	month = jul,
	year = {2024},
	note = {Conference Name: IEEE Transactions on Automatic Control},
	pages = {4275--4290},
}

@misc{zhang_constrained_2024,
	title = {Constrained {Reinforcement} {Learning} with {Smoothed} {Log} {Barrier} {Function}},
	doi = {10.48550/arXiv.2403.14508},
	abstract = {Reinforcement Learning (RL) has been widely applied to many control tasks and substantially improved the performances compared to conventional control methods in many domains where the reward function is well defined. However, for many real-world problems, it is often more convenient to formulate optimization problems in terms of rewards and constraints simultaneously. Optimizing such constrained problems via reward shaping can be difficult as it requires tedious manual tuning of reward functions with several interacting terms. Recent formulations which include constraints mostly require a pre-training phase, which often needs human expertise to collect data or assumes having a sub-optimal policy readily available. We propose a new constrained RL method called CSAC-LB (Constrained Soft Actor-Critic with Log Barrier Function), which achieves competitive performance without any pre-training by applying a linear smoothed log barrier function to an additional safety critic. It implements an adaptive penalty for policy learning and alleviates the numerical issues that are known to complicate the application of the log barrier function method. As a result, we show that with CSAC-LB, we achieve state-of-the-art performance on several constrained control tasks with different levels of difficulty and evaluate our methods in a locomotion task on a real quadruped robot platform.},
	urldate = {2025-01-27},
	publisher = {arXiv},
	author = {Zhang, Baohe and Zhang, Yuan and Frison, Lilli and Brox, Thomas and Bödecker, Joschka},
	month = mar,
	year = {2024},
	note = {arXiv:2403.14508 [cs]},
}

@inproceedings{sootla_saute_2022,
	title = {Saute {RL}: {Almost} {Surely} {Safe} {Reinforcement} {Learning} {Using} {State} {Augmentation}},
	shorttitle = {Saute {RL}},
	abstract = {Satisfying safety constraints almost surely (or with probability one) can be critical for the deployment of Reinforcement Learning (RL) in real-life applications. For example, plane landing and take-off should ideally occur with probability one. We address the problem by introducing Safety Augmented (Saute) Markov Decision Processes (MDPs), where the safety constraints are eliminated by augmenting them into the state-space and reshaping the objective. We show that Saute MDP satisfies the Bellman equation and moves us closer to solving Safe RL with constraints satisfied almost surely. We argue that Saute MDP allows viewing the Safe RL problem from a different perspective enabling new features. For instance, our approach has a plug-and-play nature, i.e., any RL algorithm can be "Sauteed”. Additionally, state augmentation allows for policy generalization across safety constraints. We finally show that Saute RL algorithms can outperform their state-of-the-art counterparts when constraint satisfaction is of high importance.},
	language = {en},
	urldate = {2025-01-27},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Sootla, Aivar and Cowen-Rivers, Alexander I. and Jafferjee, Taher and Wang, Ziyan and Mguni, David H. and Wang, Jun and Ammar, Haitham},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {20423--20443},
}

@article{aswani_provably_2013,
	title = {Provably safe and robust learning-based model predictive control},
	volume = {49},
	issn = {0005-1098},
	doi = {10.1016/j.automatica.2013.02.003},
	abstract = {Controller design faces a trade-off between robustness and performance, and the reliability of linear controllers has caused many practitioners to focus on the former. However, there is renewed interest in improving system performance to deal with growing energy constraints. This paper describes a learning-based model predictive control (LBMPC) scheme that provides deterministic guarantees on robustness, while statistical identification tools are used to identify richer models of the system in order to improve performance; the benefits of this framework are that it handles state and input constraints, optimizes system performance with respect to a cost function, and can be designed to use a wide variety of parametric or nonparametric statistical tools. The main insight of LBMPC is that safety and performance can be decoupled under reasonable conditions in an optimization framework by maintaining two models of the system. The first is an approximate model with bounds on its uncertainty, and the second model is updated by statistical methods. LBMPC improves performance by choosing inputs that minimize a cost subject to the learned dynamics, and it ensures safety and robustness by checking whether these same inputs keep the approximate model stable when it is subject to uncertainty. Furthermore, we show that if the system is sufficiently excited, then the LBMPC control action probabilistically converges to that of an MPC computed using the true dynamics.},
	number = {5},
	journal = {Automatica},
	author = {Aswani, Anil and Gonzalez, Humberto and Sastry, S. Shankar and Tomlin, Claire},
	year = {2013},
	keywords = {Learning control, Predictive control, Robustness, Safety analysis, Statistics},
	pages = {1216--1226},
}

@inproceedings{didier_approximate_2023,
	title = {Approximate {Predictive} {Control} {Barrier} {Functions} using {Neural} {Networks}: {A} {Computationally} {Cheap} and {Permissive} {Safety} {Filter}},
	shorttitle = {Approximate {Predictive} {Control} {Barrier} {Functions} using {Neural} {Networks}},
	doi = {10.23919/ECC57647.2023.10178263},
	abstract = {A predictive control barrier function (PCBF) based safety filter is a modular framework to verify safety of a control input by predicting a future trajectory. The approach relies on the solution of two optimization problems, first computing the minimal state constraint violation given the current state in the form of slacks on the constraint, and then computing the minimal deviation from a proposed input given the previously computed minimal slacks. This paper presents an approximation procedure that uses a neural network to approximate the optimal value function of the first optimization problem, which defines a control barrier function (CBF). By including this explicit approximation in a CBF-based safety filter formulation, the online computation becomes independent of the prediction horizon. It is shown that this approximation guarantees convergence to a neighborhood of the feasible set of the PCBF safety filter problem with zero constraint violation. The convergence result relies on a novel class {\textbackslash}mathcalK lower bound on the PCBF decrease and depends on the approximation error of the neural network. Lastly, we demonstrate our approach in simulation for an autonomous driving example and show that the proposed approximation leads to a significant decrease in computation time compared to the original approach.},
	urldate = {2025-01-22},
	booktitle = {European {Control} {Conference} ({ECC})},
	author = {Didier, Alexandre and Jacobs, Robin C. and Sieber, Jerome and Wabersich, Kim P. and Zeilinger, Melanie N.},
	month = jun,
	year = {2023},
	keywords = {Approximation error, Computational modeling, Design methodology, Europe, Neural networks, Safety, Trajectory},
	pages = {1--7},
}

@misc{anand_all_2025,
	title = {All {AI} {Models} are {Wrong}, but {Some} are {Optimal}},
	doi = {10.48550/arXiv.2501.06086},
	abstract = {AI models that predict the future behavior of a system (a.k.a. predictive AI models) are central to intelligent decision-making. However, decision-making using predictive AI models often results in suboptimal performance. This is primarily because AI models are typically constructed to best fit the data, and hence to predict the most likely future rather than to enable high-performance decision-making. The hope that such prediction enables high-performance decisions is neither guaranteed in theory nor established in practice. In fact, there is increasing empirical evidence that predictive models must be tailored to decision-making objectives for performance. In this paper, we establish formal (necessary and sufficient) conditions that a predictive model (AI-based or not) must satisfy for a decision-making policy established using that model to be optimal. We then discuss their implications for building predictive AI models for sequential decision-making.},
	urldate = {2025-01-27},
	publisher = {arXiv},
	author = {Anand, Akhil S. and Sawant, Shambhuraj and Reinhardt, Dirk and Gros, Sebastien},
	month = jan,
	year = {2025},
	note = {arXiv:2501.06086 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{wang_benchmarking_2019,
	title = {Benchmarking {Model}-{Based} {Reinforcement} {Learning}},
	doi = {10.48550/arXiv.1907.02057},
	abstract = {Model-based reinforcement learning (MBRL) is widely seen as having the potential to be significantly more sample efficient than model-free RL. However, research in model-based RL has not been very standardized. It is fairly common for authors to experiment with self-designed environments, and there are several separate lines of research, which are sometimes closed-sourced or not reproducible. Accordingly, it is an open question how these various existing MBRL algorithms perform relative to each other. To facilitate research in MBRL, in this paper we gather a wide collection of MBRL algorithms and propose over 18 benchmarking environments specially designed for MBRL. We benchmark these algorithms with unified problem settings, including noisy environments. Beyond cataloguing performance, we explore and unify the underlying algorithmic differences across MBRL algorithms. We characterize three key research challenges for future MBRL research: the dynamics bottleneck, the planning horizon dilemma, and the early-termination dilemma. Finally, to maximally facilitate future research on MBRL, we open-source our benchmark in http://www.cs.toronto.edu/{\textasciitilde}tingwuwang/mbrl.html.},
	urldate = {2025-01-27},
	publisher = {arXiv},
	author = {Wang, Tingwu and Bao, Xuchan and Clavera, Ignasi and Hoang, Jerrick and Wen, Yeming and Langlois, Eric and Zhang, Shunshi and Zhang, Guodong and Abbeel, Pieter and Ba, Jimmy},
	month = jul,
	year = {2019},
	note = {arXiv:1907.02057 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning},
}

@inproceedings{cheng_end--end_2019,
	title = {End-to-end safe reinforcement learning through barrier functions for safety-critical continuous control tasks},
	volume = {33},
	isbn = {2374-3468},
	booktitle = {Proceedings of the {AAAI} conference on artificial intelligence},
	author = {Cheng, Richard and Orosz, Gábor and Murray, Richard M. and Burdick, Joel W.},
	year = {2019},
	note = {Issue: 01},
	pages = {3387--3395},
}

@article{ray_benchmarking_2019,
	title = {Benchmarking {Safe} {Exploration} in {Deep} {Reinforcement} {Learning}},
	url = {https://cdn.openai.com/safexp-short.pdf},
	author = {Ray, Alex and Achiam, Joshua and Amodei, Dario},
	year = {2019},
}

@book{altman_constrained_1999,
	address = {New York},
	title = {Constrained {Markov} {Decision} {Processes}},
	isbn = {978-1-315-14022-3},
	publisher = {Routledge},
	author = {Altman, Eitan},
	editor = {{1st}},
	year = {1999},
	doi = {10.1201/9781315140223},
	keywords = {Engineering \& Technology},
}

@inproceedings{yoon_sampling_2022,
	title = {Sampling {Complexity} of {Path} {Integral} {Methods} for {Trajectory} {Optimization}},
	doi = {10.23919/ACC53348.2022.9867607},
	booktitle = {2022 {American} {Control} {Conference} ({ACC})},
	author = {Yoon, Hyung-Jin and Tao, Chuyuan and Kim, Hunmin and Hovakimyan, Naira and Voulgaris, Petros},
	year = {2022},
	keywords = {Complexity theory, Costs, Frequency locked loops, Graphics processing units, Nonlinear systems, Parallel processing, Real-time systems},
	pages = {3482--3487},
}

@article{bertsekas_dynamic_2005,
	title = {Dynamic {Programming} and {Suboptimal} {Control}: {A} {Survey} from \{{ADP}\} to \{{MPC}\}*},
	volume = {11},
	issn = {0947-3580},
	shorttitle = {Dynamic {Programming} and {Suboptimal} {Control}},
	doi = {10.3166/ejc.11.310-334},
	abstract = {We survey some recent research directions within the field of approximate dynamic programming, with a particular emphasis on rollout algorithms and model predictive control (MPC). We argue that while they are motivated by different concerns, these two methodologies are closely connected, and the mathematical essence of their desirable properties (cost improvement and stability, respectively) is couched on the central dynamic programming idea of policy iteration. In particular, among other things, we show that the most common MPC schemes can be viewed as rollout algorithms and are related to policy iteration methods. Furthermore, we embed rollout and MPC within a new unifying suboptimal control framework, based on a concept of restricted or constrained structure policies, which contains these schemes as special cases.},
	number = {4},
	urldate = {2024-02-22},
	journal = {European Journal of Control},
	author = {Bertsekas, Dimitri P.},
	month = jan,
	year = {2005},
	keywords = {dynamic programming, model predictive control, rollout algorithm, stochastic optimal control},
	pages = {310--334},
}

@article{bellman_dynamic_1966,
	title = {Dynamic programming},
	volume = {153},
	number = {3731},
	journal = {American Association for the Advancement of Science},
	author = {Bellman, Richard},
	year = {1966},
	pages = {34--37},
}

@inproceedings{kang_decomposition_2015,
	title = {Decomposition via {ADMM} for scenario-based model predictive control},
	booktitle = {American {Control} {Conference} ({ACC})},
	publisher = {IEEE},
	author = {Kang, Jia and Raghunathan, Arvind U and Di Cairano, Stefano},
	year = {2015},
	pages = {1246--1251},
}

@inproceedings{giftthaler_family_2018,
	title = {A family of iterative gauss-newton shooting methods for nonlinear optimal control},
	booktitle = {{IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	publisher = {IEEE},
	author = {Giftthaler, Markus and Neunert, Michael and Stäuble, Markus and Buchli, Jonas and Diehl, Moritz},
	year = {2018},
	pages = {1--9},
}

@phdthesis{nurkanovic_numerical_2023,
	type = {{PhD} {Thesis}},
	title = {Numerical {Methods} for {Optimal} {Control} of {Nonsmooth} {Dynamical} {Systems}},
	school = {University of Freiburg},
	author = {Nurkanović, Armin},
	year = {2023},
	keywords = {Filippov systems, MPCC, MPEC, NMPC, nonsmooth optimal control, state jumps, syscop-public, time-freezing},
}

@misc{inc_matlab_2022,
	address = {Natick, Massachusetts, United States},
	title = {{MATLAB} version: 9.13.0 ({R2022b})},
	url = {https://www.mathworks.com},
	publisher = {The MathWorks Inc.},
	author = {Inc, The MathWorks},
	year = {2022},
}

@article{cplex_v12_2009,
	title = {V12. 1: {User}’s {Manual} for {CPLEX}},
	volume = {46},
	number = {53},
	journal = {International Business Machines Corporation},
	author = {Cplex, IBM ILOG},
	year = {2009},
	pages = {157},
}

@misc{domahidi_forces_2014,
	title = {{FORCES} {Professional}},
	author = {Domahidi, Alexander and Jerez, Juan},
	year = {2014},
	note = {Published: Embotech AG, url=https://embotech.com/FORCES-Pro},
}

@misc{gurobi_optimization_llc_gurobi_2023,
	title = {Gurobi {Optimizer} {Reference} {Manual}},
	url = {https://www.gurobi.com},
	author = {{Gurobi Optimization, LLC}},
	year = {2023},
}

@book{aps_mosek_2024,
	title = {The {MOSEK} optimization toolbox for {MATLAB} manual. {Version} 10.1.},
	url = {http://docs.mosek.com/latest/toolbox/index.html},
	author = {ApS, MOSEK},
	year = {2024},
}

@article{fleming_stochastic_1971,
	title = {Stochastic {Control} for {Small} {Noise} {Intensities}},
	volume = {9},
	number = {3},
	journal = {SIAM J. Control},
	author = {Fleming, Wendell H.},
	year = {1971},
}

@article{parsi_once_2024,
	title = {Once upon a time step: {A} closed-loop approach to robust {MPC} design},
	doi = {10.1109/TAC.2024.3465522},
	journal = {IEEE Transactions on Automatic Control},
	author = {Parsi, Anilkumar and Bartos, Marcell and Srivastava, Amber and Gros, Sébastien and Smith, Roy S.},
	year = {2024},
	keywords = {Constrained control, Optimization, Perturbation methods, Predictive control, Predictive control for linear systems, Q measurement, Radio frequency, Robust control, Trajectory, Uncertain systems, Uncertainty},
	pages = {1--8},
}

@article{langson_robust_2004,
	title = {Robust model predictive control using tubes},
	volume = {40},
	number = {1},
	journal = {Automatica},
	author = {Langson, W. and I. Chryssochoos, S.V. Rakovic and Mayne, D. Q.},
	year = {2004},
	keywords = {mpc robust},
	pages = {125--133},
}

@article{pannocchia_conditions_2011,
	title = {Conditions under which suboptimal nonlinear {MPC} is inherently robust},
	volume = {60},
	number = {9},
	journal = {System \& Control Letters},
	author = {Pannocchia, G. and Rawlings, J. and Wright, S.},
	year = {2011},
	pages = {747--755},
}

@article{chen_quasi-infinite_1998,
	title = {A {Quasi}-{Infinite} {Horizon} {Nonlinear} {Model} {Predictive} {Control} {Scheme} with {Guaranteed} {Stability}},
	volume = {34},
	number = {10},
	journal = {Automatica},
	author = {Chen, H. and Allgöwer, F.},
	year = {1998},
	pages = {1205--1218},
}

@article{messerer_fourth-order_2024,
	title = {Fourth-order suboptimality of nominal model predictive control in the presence of uncertainty},
	journal = {arXiv},
	author = {Messerer, Florian and Baumgärtner, Katrin and Lucia, Sergio and Diehl, Moritz},
	year = {2024},
}

@article{karapetyan_finite-time_2023,
	title = {On the {Finite}-{Time} {Behavior} of {Suboptimal} {Linear} {Model} {Predictive} {Control}},
	journal = {Proceedings of the IEEE Conference on Decision and Control (CDC)},
	author = {Karapetyan, Aren and Balta, Efe C. and Iannelli, Andrea and Lygeros, John},
	year = {2023},
}

@article{grune_infinite_2008,
	title = {On the infinite horizon performance of receding horizon controllers},
	volume = {53},
	number = {9},
	journal = {IEEE Trans. Automat. Control},
	author = {Grüne, Lars and Rantzer, Anders},
	year = {2008},
	note = {Publisher: University of Bayreuth},
}

@article{ben-tal_adjustable_2004,
	title = {Adjustable robust solutions of uncertain linear programs},
	volume = {99},
	number = {2},
	journal = {Mathematical Programming},
	author = {Ben-Tal, Aharon and Goryashko, Alexander and Guslitzer, Elana and Nemirovski, Arkadi},
	year = {2004},
	pages = {351--376},
}

@inproceedings{bock_multiple_1984,
	title = {A {Multiple} {Shooting} {Algorithm} for {Direct} {Solution} of {Optimal} {Control} {Problems}},
	booktitle = {Proceedings of the {IFAC} {World} {Congress}},
	publisher = {Pergamon Press},
	author = {Bock, H. G. and Plitt, K. J.},
	year = {1984},
	pages = {242--247},
}

@inproceedings{tassa_control-limited_2014,
	title = {Control-{Limited} {Differential} {Dynamic} {Programming}},
	booktitle = {{IEEE} {International} {Conference} on {Robotics} and {Automation}},
	author = {Tassa, Yuval and Mansard, Nicolas and Todorov, Emo},
	year = {2014},
}

@phdthesis{pantoja_algorithms_1984,
	type = {{PhD} {Thesis}},
	title = {Algorithms for constrained optimization},
	school = {Imperial College London},
	author = {Pantoja, JFA de O},
	year = {1984},
}

@article{sideris_efficient_2005,
	title = {An efficient sequential linear quadratic algorithm for solving unconstrained nonlinear optimal control problems},
	volume = {50},
	number = {12},
	journal = {IEEE Transactions on Automatic Control},
	author = {Sideris, A. and Bodrow, J.},
	year = {2005},
	pages = {2043--2047},
}

@article{rao_application_1998,
	title = {Application of {Interior}-{Point} {Methods} to {Model} {Predictive} {Control}},
	volume = {99},
	journal = {Journal of Optimization Theory and Applications},
	author = {Rao, C. V. and Wright, S. J. and Rawlings, J. B.},
	year = {1998},
	pages = {723--757},
}

@inproceedings{frey_efficient_2024,
	title = {Efficient {Zero}-{Order} {Robust} {Optimization} for {Real}-{Time} {Model} {Predictive} {Control} with acados},
	booktitle = {Proceedings of the {European} {Control} {Conference} ({ECC})},
	author = {Frey, Jonathan and Gao, Yunfan and Messerer, Florian and Lahr, Amon and Zeilinger, Melanie N. and Diehl, Moritz},
	year = {2024},
	keywords = {syscop-public},
}

@article{richter_computational_2011,
	title = {Computational complexity certification for real-time {MPC} with input constraints based on the fast gradient method},
	volume = {57},
	number = {6},
	journal = {IEEE Transactions on Automatic Control},
	author = {Richter, Stefan and Jones, Colin Neil and Morari, Manfred},
	year = {2011},
	note = {Publisher: IEEE},
	pages = {1391--1403},
}

@article{feng_inexact_2020,
	title = {Inexact adjoint-based {SQP} algorithm for real-time stochastic nonlinear {MPC}},
	volume = {53},
	number = {2},
	journal = {IFAC-PapersOnLine},
	author = {Feng, Xuhui and Di Cairano, Stefano and Quirynen, Rien},
	year = {2020},
	note = {Publisher: Elsevier},
	pages = {6529--6535},
}

@article{zanelli_zero-order_2021,
	title = {Zero-{Order} {Robust} {Nonlinear} {Model} {Predictive} {Control} with {Ellipsoidal} {Uncertainty} {Sets}},
	doi = {https://doi.org/10.1016/j.ifacol.2021.08.523},
	journal = {Proceedings of the IFAC Conference on Nonlinear Model Predictive Control (NMPC)},
	author = {Zanelli, Andrea and Frey, Jonathan and Messerer, Florian and Diehl, Moritz},
	year = {2021},
	keywords = {syscop-public},
}

@inproceedings{frison_high-performance_2017,
	title = {A high-performance {Riccati} based solver for tree-structured quadratic programs},
	volume = {50},
	booktitle = {{IFAC}},
	author = {Frison, Gianluca and Kouzoupis, Dimitris and Diehl, Moritz and Jørgensen, John Bagterp},
	year = {2017},
	note = {Issue: 1},
	keywords = {syscop-public},
	pages = {14399--14405},
}

@incollection{patil_scaling_2015,
	address = {Cham},
	title = {Scaling up {Gaussian} {Belief} {Space} {Planning} {Through} {Covariance}-{Free} {Trajectory} {Optimization} and {Automatic} {Differentiation}},
	isbn = {978-3-319-16595-0},
	abstract = {Belief space planningAbbeel, PieterprovidesGoldberg, Kena principledKahn, GregoryframeworkLaskey, Michaelto computePatil, SachinmotionSchulman, Johnplans that explicitly gather information from sensing, as necessary, to reduce uncertainty about the robot and the environment. We consider the problem of planning in Gaussian belief spaces, which are parameterized in terms of mean states and covariances describing the uncertainty. In this work, we show that it is possible to compute locally optimal plans without including the covariance in direct trajectory optimization formulations of the problem. As a result, the dimensionality of the problem scales linearly in the state dimension instead of quadratically, as would be the case if we were to include the covariance in the optimization. We accomplish this by taking advantage of recent advances in numerical optimal control that include automatic differentiation and state of the art convex solvers. We show that the running time of each optimization step of the covariance-free trajectory optimization is \vphantom{\{}\}\vphantom{\{}\}O(n{\textasciicircum}3T)\vphantom{\{}\}\vphantom{\{}\}O(n3T), where \vphantom{\{}\}\vphantom{\{}\}n\vphantom{\{}\}\vphantom{\{}\}nis the dimension of the state space and \vphantom{\{}\}\vphantom{\{}\}T\vphantom{\{}\}\vphantom{\{}\}Tis the number of time steps in the trajectory. We present experiments in simulation on a variety of planning problems under uncertaintyMotion planning under uncertaintyincluding manipulator planning, estimating unknown model parameters for dynamical systems, and active simultaneous localization and mapping (active SLAM). Our experiments suggest that our method can solve planning problems in \vphantom{\{}\}\vphantom{\{}\}100\vphantom{\{}\}\vphantom{\{}\}100dimensional state spaces and obtain computational speedups of \vphantom{\{}\}\vphantom{\{}\}400{\textbackslash}backslashtimes \vphantom{\{}\}\vphantom{\{}\}400×over related trajectory optimization methodsTrajectory optimization methods.},
	booktitle = {Algorithmic {Foundations} of {Robotics} {XI}: {Selected} {Contributions} of the {Eleventh} {International} {Workshop} on the {Algorithmic} {Foundations} of {Robotics}},
	publisher = {Springer International Publishing},
	author = {Patil, Sachin and Kahn, Gregory and Laskey, Michael and Schulman, John and Goldberg, Ken and Abbeel, Pieter},
	editor = {Akin, H. Levent and Amato, Nancy M. and Isler, Volkan and van der Stappen, A. Frank},
	year = {2015},
	doi = {10.1007/978-3-319-16595-0_30},
	pages = {515--533},
}

@inproceedings{frey_active-set_2020,
	title = {Active-{Set} based {Inexact} {Interior} {Point} {QP} {Solver} for {Model} {Predictive} {Control}},
	booktitle = {Proceedings of the {IFAC} {World} {Congress}},
	author = {Frey, Jonathan and Cairano, Stefano Di and Quirynen, Rien},
	year = {2020},
	keywords = {syscop-public},
}

@inproceedings{kapernick_gradient_2014,
	title = {The gradient based nonlinear model predictive control software {GRAMPC}},
	booktitle = {2014 {European} {Control} {Conference} ({ECC})},
	publisher = {IEEE},
	author = {Käpernick, Bartosz and Graichen, Knut},
	year = {2014},
	pages = {1170--1175},
}

@article{messerer_dual-control_2023,
	title = {A dual-control effect preserving formulation for nonlinear output-feedback stochastic model predictive control with constraints},
	volume = {7},
	number = {1171–1176},
	journal = {IEEE Control Systems Letters},
	author = {Messerer, Florian and Baumgärtner, Katrin and Diehl, Moritz},
	year = {2023},
	keywords = {syscop-public},
}

@inproceedings{ross_reduction_2011,
	address = {Fort Lauderdale, FL, USA},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {A {Reduction} of {Imitation} {Learning} and {Structured} {Prediction} to {No}-{Regret} {Online} {Learning}},
	volume = {15},
	abstract = {Sequential prediction problems such as imitation learning, where future observations depend on previous predictions (actions), violate the common i.i.d. assumptions made in statistical learning. This leads to poor performance in theory and often in practice. Some recent approaches provide stronger guarantees in this setting, but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations. In this paper, we propose a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning setting. We show that any such no regret algorithm, combined with additional reduction assumptions, must find a policy with good performance under the distribution of observations it induces in such sequential settings. We demonstrate that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem.},
	booktitle = {Proceedings of the {Fourteenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Ross, Stephane and Gordon, Geoffrey and Bagnell, Drew},
	editor = {Gordon, Geoffrey and Dunson, David and Dudík, Miroslav},
	month = apr,
	year = {2011},
	pages = {627--635},
}

@article{gill_snopt_2002,
	title = {{SNOPT}: {An} {SQP} {Algorithm} for {Large}-{Scale} {Constrained} {Optimization}},
	volume = {12},
	doi = {10.1137/S1052623499350013},
	number = {4},
	journal = {SIAM Journal on Optimization},
	author = {Gill, Philip E. and Murray, Walter and Saunders, Michael A.},
	year = {2002},
	pages = {979--1006},
}

@article{morari_model_1999,
	title = {Model predictive control: past, present and future},
	volume = {23},
	issn = {0098-1354},
	doi = {https://doi.org/10.1016/S0098-1354(98)00301-9},
	number = {4},
	journal = {Computers \& Chemical Engineering},
	author = {Morari, Manfred and Lee, Jay H.},
	year = {1999},
	pages = {667--682},
}

@article{angeli_average_2012,
	title = {On {Average} {Performance} and {Stability} of {Economic} {Model} {Predictive} {Control}},
	volume = {57},
	doi = {10.1109/TAC.2011.2179349},
	number = {7},
	journal = {IEEE Transactions on Automatic Control},
	author = {Angeli, David and Amrit, Rishi and Rawlings, James B.},
	year = {2012},
	keywords = {Asymptotic stability, Economics, Nonlinear systems, Optimization, Stability analysis, Steady-state, Transient analysis, Zinc, optimal control, predictive control, process control},
	pages = {1615--1626},
}

@inproceedings{simpson_efficient_2023,
	title = {An {Efficient} {Method} for the {Joint} {Estimation} of {System} {Parameters} and {Noise} {Covariances} for {Linear} {Time}-{Variant} {Systems}},
	doi = {10.1109/CDC49753.2023.10383686},
	booktitle = {62nd {IEEE} {Conference} on {Decision} and {Control} ({CDC})},
	author = {Simpson, Léo and Ghezzi, Andrea and Asprion, Jonas and Diehl, Moritz},
	year = {2023},
	keywords = {Computational modeling, Estimation, Numerical models, Numerical simulation, Optimization},
	pages = {4524--4529},
}

@misc{tilbury_revisiting_2023,
	title = {Revisiting the {Gumbel}-{Softmax} in {MADDPG}},
	publisher = {arXiv},
	author = {Tilbury, Callum Rhys and Christianos, Filippos and Albrecht, Stefano V.},
	year = {2023},
	note = {arXiv:2302.11793 [cs]},
}

@inproceedings{eysenbach_maximum_2022,
	title = {Maximum {Entropy} {RL} ({Provably}) {Solves} {Some} {Robust} {RL} {Problems}.},
	booktitle = {10th {International} {Conference} on {Learning} {Representations}, {ICLR}},
	author = {Eysenbach, Benjamin and Levine, Sergey},
	year = {2022},
}

@inproceedings{schulman_high-dimensional_2016,
	title = {High-{Dimensional} {Continuous} {Control} {Using} {Generalized} {Advantage} {Estimation}.},
	booktitle = {4th {International} {Conference} on {Learning} {Representations}, {ICLR}},
	author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael I. and Abbeel, Pieter},
	year = {2016},
}

@inproceedings{rashid_optimistic_2020,
	title = {Optimistic {Exploration} even with a {Pessimistic} {Initialisation}},
	language = {English},
	booktitle = {8th {International} {Conference} on {Learning} {Representations}, {ICLR}},
	author = {Rashid, Tabish and Peng, Bei and Böhmer, Wendelin and Whiteson, Shimon},
	year = {2020},
}

@inproceedings{byravan_evaluating_2022,
	title = {Evaluating {Model}-{Based} {Planning} and {Planner} {Amortization} for {Continuous} {Control}},
	booktitle = {10th {International} {Conference} on {Learning} {Representations}, {ICLR}},
	author = {Byravan, A. and Hasenclever, L. and Trochim, P. and Mirza, M. and Ialongo, A. D. and Tassa, Y. andSpringenberg, J. T. and Abdolmaleki, A. and Heess, N. and Merel, J. and Riedmiller, M.},
	month = apr,
	year = {2022},
}

@inproceedings{lowrey_plan_2019,
	title = {Plan {Online}, {Learn} {Offline}: {Efficient} {Learning} and {Exploration} via {Model}-{Based} {Control}},
	shorttitle = {Plan {Online}, {Learn} {Offline}},
	abstract = {We propose a "plan online and learn offline" framework for the setting where an agent, with an internal model, needs to continually act and learn in the world. Our work builds on the synergistic relationship between local model-based control, global value function learning, and exploration. We study how local trajectory optimization can cope with approximation errors in the value function, and can stabilize and accelerate value function learning. Conversely, we also study how approximate value functions can help reduce the planning horizon and allow for better policies beyond local solutions. Finally, we also demonstrate how trajectory optimization can be used to perform temporally coordinated exploration in conjunction with estimating uncertainty in value function approximation. This exploration is critical for fast and stable learning of the value function. Combining these components enable solutions to complex control tasks, like humanoid locomotion and dexterous in-hand manipulation, in the equivalent of a few minutes of experience in the real world.},
	language = {en},
	urldate = {2024-01-18},
	booktitle = {7th {International} {Conference} on {Learning} {Representations}},
	author = {Lowrey, Kendall and Rajeswaran, Aravind and Kakade, Sham and Todorov, Emanuel and Mordatch, Igor},
	year = {2019},
}

@inproceedings{lillicrap_continuous_2016,
	title = {Continuous control with deep reinforcement learning},
	booktitle = {4th {International} {Conference} on {Learning} {Representations}, {ICLR}},
	author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
	editor = {Bengio, Yoshua and LeCun, Yann},
	year = {2016},
}

@book{bertsekas_lessons_2022,
	title = {Lessons from {AlphaZero} for {Optimal}, {Model} {Predictive}, and {Adaptive} {Control}},
	isbn = {978-1-886529-17-5},
	publisher = {Athena Scientific},
	author = {Bertsekas, Dimitri},
	year = {2022},
}

@article{bertsekas_newtons_2022,
	title = {Newton’s method for reinforcement learning and model predictive control},
	volume = {7},
	issn = {2666-7207},
	doi = {10.1016/j.rico.2022.100121},
	abstract = {The purpose of this paper is to propose and develop a new conceptual framework for approximate Dynamic Programming (DP) and Reinforcement Learning (RL). This framework centers around two algorithms, which are designed largely independently of each other and operate in synergy through the powerful mechanism of Newton’s method. We call these the off-line training and the on-line play algorithms; the names are borrowed from some of the major successes of RL involving games. Primary examples are the recent (2017) AlphaZero program (which plays chess), and the similarly structured and earlier (1990s) TD-Gammon program (which plays backgammon). In these game contexts, the off-line training algorithm is the method used to teach the program how to evaluate positions and to generate good moves at any given position, while the on-line play algorithm is the method used to play in real time against human or computer opponents. Both AlphaZero and TD-Gammon were trained off-line extensively using neural networks and an approximate version of the fundamental DP algorithm of policy iteration. Yet the AlphaZero player that was obtained off-line is not used directly during on-line play (it is too inaccurate due to approximation errors that are inherent in off-line neural network training). Instead a separate on-line player is used to select moves, based on multistep lookahead minimization and a terminal position evaluator that was trained using experience with the off-line player. The on-line player performs a form of policy improvement, which is not degraded by neural network approximations. As a result, it greatly improves the performance of the off-line player. Similarly, TD-Gammon performs on-line a policy improvement step using one-step or two-step lookahead minimization, which is not degraded by neural network approximations. To this end it uses an off-line neural network-trained terminal position evaluator, and importantly it also extends its on-line lookahead by rollout (simulation with the one-step lookahead player that is based on the position evaluator). An important lesson from AlphaZero and TD-Gammon is that the performance of an off-line trained policy can be greatly improved by on-line approximation in value space, with long lookahead (involving minimization or rollout with the off-line policy, or both), and terminal cost approximation that is obtained off-line. This performance enhancement is often dramatic and is due to a simple fact, which is couched on algorithmic mathematics and is the focal point of this work: (a) Approximation in value space with one-step lookahead minimization amounts to a step of Newton’s method for solving Bellman’s equation. (b) The starting point for the Newton step is based on the results of off-line training, and may be enhanced by longer lookahead minimization and on-line rollout. Indeed the major determinant of the quality of the on-line policy is the Newton step that is performed on-line, while off-line training plays a secondary role by comparison. Significantly, the synergy between off-line training and on-line play also underlies Model Predictive Control (MPC), a major control system design methodology that has been extensively developed since the 1980s. This synergy can be understood in terms of abstract models of infinite horizon DP and simple geometrical constructions, and helps to explain the all-important stability issues within the MPC context. In this work we aim to provide insights (often based on visualization), which explain the beneficial effects of on-line decision making on top of off-line training. In the process, we will bring out the strong connections between the artificial intelligence view of RL, and the control theory views of MPC and adaptive control. While we will deemphasize mathematical proofs, there is considerable related analysis, which supports our conclusions and can be found in the author’s recent RL books (Bertsekas, 2019; Bertsekas, 2020), and the abstract DP monograph (Bertsekas, 2022). One of our principal aims is to show, through the algorithmic ideas of Newton’s method and the unifying principles of abstract DP, that the AlphaZero/TD-Gammon methodology of approximation in value space and rollout applies very broadly to deterministic and stochastic optimal control problems, involving both discrete and continuous search spaces, as well as finite and infinite horizon.},
	urldate = {2025-01-15},
	journal = {Results in Control and Optimization},
	author = {Bertsekas, Dimitri},
	month = jun,
	year = {2022},
	keywords = {AlphaZero, Dynamic programming over an infinite horizon, Model predictive control, Off-line training, On-line play, Reinforcement learning},
	pages = {100121},
}

@inproceedings{muller_essentially_2024,
	title = {Essentially {Sharp} {Estimates} on the {Entropy} {Regularization} {Error} in {Discounted} {Markov} {Decision} {Processes}},
	abstract = {We study the error introduced by entropy regularization of infinite-horizon discrete discounted Markov decision processes. We show that this error decreases exponentially in the inverse regularization strength both in a weighted KL-divergence and in value with a problem-specific exponent. We provide a lower bound matching our upper bound up to a polynomial factor. Our proof relies on the correspondence of the solutions of entropy-regularized Markov decision processes with gradient flows of the unregularized reward with respect to a Riemannian metric common in natural policy gradient methods. Further, this correspondence allows us to identify the limit of the gradient flow as the generalized maximum entropy optimal policy, thereby characterizing the implicit bias of the Kakade gradient flow which corresponds to a time-continuous version of the natural policy gradient method. We use this to show that for entropy-regularized natural policy gradient methods the overall error decays exponentially in the square root of the number of iterations.},
	language = {en},
	urldate = {2025-01-14},
	author = {Müller, Johannes and Cayci, Semih},
	month = jun,
	year = {2024},
}

@misc{haarnoja_soft_2019,
	title = {Soft {Actor}-{Critic} {Algorithms} and {Applications}},
	doi = {10.48550/arXiv.1812.05905},
	abstract = {Model-free deep reinforcement learning (RL) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. However, these methods typically suffer from two major challenges: high sample complexity and brittleness to hyperparameters. Both of these challenges limit the applicability of such methods to real-world domains. In this paper, we describe Soft Actor-Critic (SAC), our recently introduced off-policy actor-critic algorithm based on the maximum entropy RL framework. In this framework, the actor aims to simultaneously maximize expected return and entropy. That is, to succeed at the task while acting as randomly as possible. We extend SAC to incorporate a number of modifications that accelerate training and improve stability with respect to the hyperparameters, including a constrained formulation that automatically tunes the temperature hyperparameter. We systematically evaluate SAC on a range of benchmark tasks, as well as real-world challenging tasks such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, SAC achieves state-of-the-art performance, outperforming prior on-policy and off-policy methods in sample-efficiency and asymptotic performance. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving similar performance across different random seeds. These results suggest that SAC is a promising candidate for learning in real-world robotics tasks.},
	urldate = {2025-01-14},
	publisher = {arXiv},
	author = {Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
	month = jan,
	year = {2019},
	note = {arXiv:1812.05905 [cs]},
}

@article{schulman_proximal_2017,
	title = {Proximal policy optimization algorithms},
	volume = {abs/1707.06347},
	journal = {CoRR},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	year = {2017},
}

@article{schulman_trust_2015,
	title = {Trust region policy optimization},
	volume = {abs/1502.05477},
	journal = {CoRR},
	author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
	year = {2015},
}

@incollection{lecun_efficient_1998,
	address = {Berlin, Heidelberg},
	title = {Efficient {BackProp}},
	isbn = {978-3-540-49430-0},
	abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observedb y practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposedin serious technical publications. This paper gives some of those tricks, ando.ers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most “classical” second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.},
	language = {en},
	urldate = {2025-01-10},
	booktitle = {Neural {Networks}: {Tricks} of the {Trade}},
	publisher = {Springer},
	author = {LeCun, Yann and Bottou, Leon and Orr, Genevieve B. and Müller, Klaus -Robert},
	editor = {Orr, Genevieve B. and Müller, Klaus-Robert},
	year = {1998},
	doi = {10.1007/3-540-49430-8_2},
	pages = {9--50},
}

@article{jenelten_tamols_2022,
	title = {{TAMOLS}: {Terrain}-{Aware} {Motion} {Optimization} for {Legged} {Systems}},
	volume = {38},
	issn = {1552-3098, 1941-0468},
	shorttitle = {{TAMOLS}},
	doi = {10.1109/TRO.2022.3186804},
	abstract = {Terrain geometry is, in general, non-smooth, non-linear, non-convex, and, if perceived through a robot-centric visual unit, appears partially occluded and noisy. This work presents the complete control pipeline capable of handling the aforementioned problems in real-time. We formulate a trajectory optimization problem that jointly optimizes over the base pose and footholds, subject to a heightmap. To avoid converging into undesirable local optima, we deploy a graduated optimization technique. We embed a compact, contact-force free stability criterion that is compatible with the non-flat ground formulation. Direct collocation is used as transcription method, resulting in a non-linear optimization problem that can be solved online in less than ten milliseconds. To increase robustness in the presence of external disturbances, we close the tracking loop with a momentum observer. Our experiments demonstrate stair climbing, walking on stepping stones, and over gaps, utilizing various dynamic gaits.},
	number = {6},
	urldate = {2025-01-14},
	journal = {IEEE Transactions on Robotics},
	author = {Jenelten, Fabian and Grandia, Ruben and Farshidian, Farbod and Hutter, Marco},
	month = dec,
	year = {2022},
	note = {arXiv:2206.14049 [cs]},
	keywords = {Computer Science - Robotics},
	pages = {3395--3413},
}

@inproceedings{ziebart_maximum_2008,
	title = {Maximum entropy inverse reinforcement learning},
	booktitle = {Proceedings of the 23rd {AAAI} conference on artificial intelligence},
	publisher = {AAAI Press},
	author = {Ziebart, Brian D. and Maas, Andrew L. and Bagnell, J. Andrew and Dey, Anind K.},
	year = {2008},
	pages = {1433--1438},
}

@misc{reinhardt_economic_2024,
	title = {Economic model predictive control as a solution to markov decision processes},
	doi = {10.48550/arXiv.2407.16500},
	publisher = {arXiv},
	author = {Reinhardt, Dirk and Anand, Akhil S. and Sawant, Shambhuraj and Gros, Sebastien},
	year = {2024},
}

@misc{lahr_l4acados_2024,
	title = {L4acados: {Learning}-based models for acados, applied to {Gaussian} process-based predictive control},
	shorttitle = {L4acados},
	author = {Lahr, Amon and Näf, Joshua and Wabersich, Kim P. and Frey, Jonathan and Siehl, Pascal and Carron, Andrea and Diehl, Moritz and Zeilinger, Melanie N.},
	month = nov,
	year = {2024},
	doi = {10.48550/arXiv.2411.19258},
	note = {arXiv: 2411.19258
tex.pubstate: prepublished},
}

@inproceedings{nagabandi_neural_2018,
	title = {Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning},
	booktitle = {{IEEE} international conference on robotics and automation ({ICRA})},
	publisher = {IEEE},
	author = {Nagabandi, Anusha and Kahn, Gregory and Fearing, Ronald S and Levine, Sergey},
	year = {2018},
	pages = {7559--7566},
}

@misc{noauthor_scipy_nodate,
	title = {{SciPy} v1.15.0 {Manual}},
	url = {https://docs.scipy.org/doc/scipy/reference/optimize.minimize-slsqp.html},
	urldate = {2025-01-06},
}

@inproceedings{yang_data_2020,
	title = {Data {Efficient} {Reinforcement} {Learning} for {Legged} {Robots}},
	abstract = {We present a model-based reinforcement learning framework for robot locomotion that achieves walking based on only 4.5 minutes of data collected on a quadruped robot. To accurately model the robot’s dynamics over a long horizon, we introduce a loss function that tracks the model’s prediction over multiple timesteps. We adapt model predictive control to account for planning latency, which allows the learned model to be used for real time control. Additionally, to ensure safe exploration during model learning, we embed prior knowledge of leg trajectories into the action space. The resulting system achieves fast and robust locomotion. Unlike model-free methods, which optimize for a particular task, our planner can use the same learned dynamics for various tasks, simply by changing the reward function.1 To the best of our knowledge, our approach is more than an order of magnitude more sample efficient than current model-free methods.},
	language = {en},
	urldate = {2024-07-23},
	booktitle = {Proceedings of the {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Yang, Yuxiang and Caluwaerts, Ken and Iscen, Atil and Zhang, Tingnan and Tan, Jie and Sindhwani, Vikas},
	month = may,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1--10},
}

@misc{bang_rl-augmented_2024,
	title = {{RL}-augmented {MPC} {Framework} for {Agile} and {Robust} {Bipedal} {Footstep} {Locomotion} {Planning} and {Control}},
	abstract = {This paper proposes an online bipedal footstep planning strategy that combines model predictive control (MPC) and reinforcement learning (RL) to achieve agile and robust bipedal maneuvers. While MPC-based foot placement controllers have demonstrated their effectiveness in achieving dynamic locomotion, their performance is often limited by the use of simplified models and assumptions. To address this challenge, we develop a novel foot placement controller that leverages a learned policy to bridge the gap between the use of a simplified model and the more complex full-order robot system. Specifically, our approach employs a unique combination of an ALIP-based MPC foot placement controller for sub-optimal footstep planning and the learned policy for refining footstep adjustments, enabling the resulting footstep policy to capture the robot’s whole-body dynamics effectively. This integration synergizes the predictive capability of MPC with the flexibility and adaptability of RL. We validate the effectiveness of our framework through a series of experiments using the fullbody humanoid robot DRACO 3. The results demonstrate significant improvements in dynamic locomotion performance, including better tracking of a wide range of walking speeds, enabling reliable turning and traversing challenging terrains while preserving the robustness and stability of the walking gaits compared to the baseline ALIP-based MPC approach.},
	language = {en},
	urldate = {2024-08-16},
	publisher = {arXiv},
	author = {Bang, Seung Hyeon and Jové, Carlos Arribalzaga and Sentis, Luis},
	month = jul,
	year = {2024},
	note = {arXiv:2407.17683 [cs]},
	keywords = {Computer Science - Robotics},
}

@inproceedings{stella_simple_2017,
	title = {A simple and efficient algorithm for nonlinear model predictive control},
	doi = {10.1109/CDC.2017.8263933},
	booktitle = {56th {IEEE} {Conference} on {Decision} and {Control} ({CDC})},
	author = {Stella, Lorenzo and Themelis, Andreas and Sopasakis, Pantelis and Patrinos, Panagiotis},
	year = {2017},
	pages = {1939--1944},
}

@inproceedings{ceder_birds-eye-view_2024,
	title = {Bird’s-{Eye}-{View} {Trajectory} {Planning} of {Multiple} {Robots} using {Continuous} {Deep} {Reinforcement} {Learning} and {Model} {Predictive} {Control}},
	doi = {10.1109/IROS58592.2024.10801434},
	abstract = {Efficient motion planning and control for multiple mobile robots in industrial automation and indoor logistics face challenges such as trajectory generation and collision avoidance in complex environments. We propose a hybrid, sequential method combining Bird’s-Eye-View vision-based continuous Deep Reinforcement Learning (DRL) with Model Predictive Control (MPC). DRL generates candidate trajectories in complex environments, while MPC refines these trajectories to ensure adherence to kinematic and dynamic constraints of the robot, as well as constraints modeling humans’ current and predicted future positions. In this study, the DRL utilizes a Deep Deterministic Policy Gradient model for trajectory generation, demonstrating its capability to navigate non-convex obstacles, a task that might pose challenges for MPC. We demonstrate that the proposed hybrid DRL-MPC model performs favorably in handling new scenarios, computational efficiency, time to destination, and adaptability to complex multi-robot situations when compared to pure DRL or pure MPC approaches.},
	urldate = {2025-01-04},
	booktitle = {2024 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Ceder, Kristian and Zhang, Ze and Burman, Adam and Kuangaliyev, Ilya and Mattsson, Krister and Nyman, Gabriel and Petersén, Arvid and Wisell, Lukas and Åkesson, Knut},
	month = oct,
	year = {2024},
	note = {ISSN: 2153-0866},
	keywords = {Adaptation models, Collision avoidance, Computational modeling, Deep reinforcement learning, Planning, Predictive control, Predictive models, Refining, Trajectory, Trajectory planning},
	pages = {8002--8008},
}

@inproceedings{li_robust_2020,
	title = {Robust {Model} {Predictive} {Shielding} for {Safe} {Reinforcement} {Learning} with {Stochastic} {Dynamics}},
	doi = {10.1109/ICRA40945.2020.9196867},
	abstract = {We propose a framework for safe reinforcement learning that can handle stochastic nonlinear dynamical systems. We focus on the setting where the nominal dynamics are known, and are subject to additive stochastic disturbances with known distribution. Our goal is to ensure the safety of a control policy trained using reinforcement learning, e.g., in a simulated environment. We build on the idea of model predictive shielding (MPS), where a backup controller is used to override the learned policy as needed to ensure safety. The key challenge is how to compute a backup policy in the context of stochastic dynamics. We propose to use a tube-based robust nonlinear model predictive controller (NMPC) as the backup controller. We estimate the tubes using sampled trajectories, leveraging ideas from statistical learning theory to obtain high-probability guarantees. We empirically demonstrate that our approach can ensure safety in stochastic systems, including cart-pole and a non-holonomic particle with random obstacles.},
	urldate = {2024-07-23},
	booktitle = {2020 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Li, Shuo and Bastani, Osbert},
	month = may,
	year = {2020},
	note = {ISSN: 2577-087X},
	keywords = {Heuristic algorithms, Nonlinear dynamical systems, Robots, Robustness, Safety, Stochastic processes, Trajectory},
	pages = {7166--7172},
}

@misc{zhang_mamps_2019,
	title = {{MAMPS}: {Safe} {Multi}-{Agent} {Reinforcement} {Learning} via {Model} {Predictive} {Shielding}},
	shorttitle = {{MAMPS}},
	doi = {10.48550/arXiv.1910.12639},
	abstract = {Reinforcement learning is a promising approach to learning control policies for performing complex multi-agent robotics tasks. However, a policy learned in simulation often fails to guarantee even simple safety properties such as obstacle avoidance. To ensure safety, we propose multi-agent model predictive shielding (MAMPS), an algorithm that provably guarantees safety for an arbitrary learned policy. In particular, it operates by using the learned policy as often as possible, but instead uses a backup policy in cases where it cannot guarantee the safety of the learned policy. Using a multi-agent simulation environment, we show how MAMPS can achieve good performance while ensuring safety.},
	urldate = {2024-07-23},
	publisher = {arXiv},
	author = {Zhang, Wenbo and Bastani, Osbert and Kumar, Vijay},
	month = dec,
	year = {2019},
	note = {arXiv:1910.12639 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
}

@inproceedings{bastani_safe_2021,
	title = {Safe {Reinforcement} {Learning} with {Nonlinear} {Dynamics} via {Model} {Predictive} {Shielding}},
	doi = {10.23919/ACC50511.2021.9483182},
	abstract = {Reinforcement learning is a promising approach to synthesizing policies for challenging robotics tasks. A key problem is how to ensure safety of the learned policy-e.g., that a walking robot does not fall over or that an autonomous car does not run into an obstacle. We focus on the setting where the dynamics are known, and the goal is to ensure that a policy trained in simulation satisfies a given safety constraint. We propose an approach, called model predictive shielding (MPS), that switches on-the-fly between a learned policy and a backup policy to ensure safety. We prove that our approach guarantees safety, and empirically evaluate it on the cart-pole.},
	urldate = {2024-04-23},
	booktitle = {2021 {American} {Control} {Conference} ({ACC})},
	author = {Bastani, Osbert},
	month = may,
	year = {2021},
	note = {ISSN: 2378-5861},
	keywords = {Heuristic algorithms, Legged locomotion, Nonlinear dynamical systems, Prediction algorithms, Predictive models, Reinforcement learning, Safety},
	pages = {3488--3494},
}

@inproceedings{wen_collision-free_2024,
	title = {Collision-{Free} {Robot} {Navigation} in {Crowded} {Environments} using {Learning} based {Convex} {Model} {Predictive} {Control}},
	doi = {10.1109/IROS58592.2024.10801893},
	abstract = {Navigating robots safely and efficiently in crowded and complex environments remains a significant challenge. However, due to the dynamic and intricate nature of these settings, planning efficient and collision-free paths for robots to track is particularly difficult. In this paper, we uniquely bridge the robot’s perception, decision-making and control processes by utilizing the convex obstacle-free region computed from 2D LiDAR data. The overall pipeline is threefold: (1) We proposes a robot navigation framework that utilizes deep reinforcement learning (DRL), conceptualizing the observation as the convex obstacle-free region, a departure from general reliance on raw sensor inputs. (2) We design the action space, derived from the intersection of the robot’s kinematic limits and the convex region, to enable efficient sampling of inherently collision-free reference points. These actions assists in guiding the robot to move towards the goal and interact with other obstacles during navigation. (3) We employ model predictive control (MPC) to track the trajectory formed by the reference points while satisfying constraints imposed by the convex obstacle-free region and the robot’s kinodynamic limits. The effectiveness of proposed improvements has been validated through two sets of ablation studies and a comparative experiment against the Timed Elastic Band (TEB), demonstrating improved navigation performance in crowded and complex environments.},
	urldate = {2025-01-04},
	booktitle = {2024 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Wen, Zhuanglei and Dong, Mingze and Chen, Xiai},
	month = oct,
	year = {2024},
	note = {ISSN: 2153-0866},
	keywords = {Aerospace electronics, Collision avoidance, Navigation, Pipelines, Planning, Predictive control, Robot sensing systems, Robots, Tracking, Trajectory},
	pages = {5452--5459},
}

@inproceedings{zarrouki_adaptive_2024,
	title = {Adaptive {Stochastic} {Nonlinear} {Model} {Predictive} {Control} with {Look}-ahead {Deep} {Reinforcement} {Learning} for {Autonomous} {Vehicle} {Motion} {Control}},
	doi = {10.1109/IROS58592.2024.10801876},
	abstract = {Propagating uncertainties through nonlinear system dynamics in the context of Stochastic Nonlinear Model Predictive Control (SNMPC) is challenging, especially for high-dimensional systems requiring real-time control and operating under time-variant uncertainties such as autonomous vehicles. In this work, we propose an Adaptive SNMPC (aSNMPC) driven by Deep Reinforcement Learning (DRL) to optimize uncertainty handling, constraints robustification, feasibility, and closed-loop performance. To this end, our SNMPC uses Polynomial Chaos Expansion (PCE) for efficient uncertainty propagation, limits its propagation time through an Uncertainty Propagation Horizon (UPH), and transforms nonlinear chance constraints into robustified deterministic ones. We conceive a DRL agent to proactively anticipate upcoming control tasks and to dynamically reduce conservatism by determining the most suitable constraints robustification factor κ, and to enhance feasibility by choosing optimal UPH length Tu. We analyze the trained DRL agent’s decision-making process and highlight its ability to learn context-dependent optimal parameters. We showcase the enhanced robustness and feasibility of our DRL-driven aSNMPC through the real-time motion control task of an autonomous passenger vehicle when confronted with significant time-variant disturbances while achieving a minimum solution frequency of 110Hz. The code used in this research is publicly accessible as open-source software: https://github.com/bzarr/TUM-CONTROL},
	urldate = {2025-01-04},
	booktitle = {2024 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Zarrouki, Baha and Wang, Chenyang and Betz, Johannes},
	month = oct,
	year = {2024},
	note = {ISSN: 2153-0866},
	keywords = {Autonomous vehicles, Deep reinforcement learning, Motion control, Predictive control, Real-time systems, Stochastic processes, Time-frequency analysis, Transforms, Uncertainty, Vehicle dynamics},
	pages = {12726--12733},
}

@inproceedings{romero_actor-critic_2024,
	address = {Yokohama, Japan},
	title = {Actor-{Critic} {Model} {Predictive} {Control}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350384574},
	doi = {10.1109/ICRA57147.2024.10610381},
	abstract = {An open research question in robotics is how to combine the benefits of model-free reinforcement learning (RL)—known for its strong task performance and flexibility in optimizing general reward formulations—with the robustness and online replanning capabilities of model predictive control (MPC). This paper provides an answer by introducing a new framework called Actor-Critic Model Predictive Control. The key idea is to embed a differentiable MPC within an actorcritic RL framework. The proposed approach leverages the short-term predictive optimization capabilities of MPC with the exploratory and end-to-end training properties of RL. The resulting policy effectively manages both short-term decisions through the MPC-based actor and long-term prediction via the critic network, unifying the benefits of both model-based control and end-to-end learning. We validate our method in both simulation and the real world with a quadcopter platform across various high-level tasks. We show that the proposed architecture can achieve real-time control performance, learn complex behaviors via trial and error, and retain the predictive properties of the MPC to better handle out of distribution behaviour.},
	language = {en},
	urldate = {2024-08-12},
	booktitle = {2024 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Romero, Angel and Song, Yunlong and Scaramuzza, Davide},
	month = may,
	year = {2024},
	pages = {14777--14784},
}

@inproceedings{hatch_value_2021,
	title = {The {Value} of {Planning} for {Infinite}-{Horizon} {Model} {Predictive} {Control}},
	doi = {10.1109/ICRA48506.2021.9561718},
	abstract = {Model Predictive Control (MPC) is a classic tool for optimal control of complex, real-world systems. Although it has been successfully applied to a wide range of challenging tasks in robotics, it is fundamentally limited by the prediction horizon, which, if too short, will result in myopic decisions. Recently, several papers have suggested using a learned value function as the terminal cost for MPC. If the value function is accurate, it effectively allows MPC to reason over an infinite horizon. Unfortunately, Reinforcement Learning (RL) solutions to value function approximation can be difficult to realize for robotics tasks. In this paper, we suggest a more efficient method for value function approximation that applies to goal-directed problems, like reaching and navigation. In these problems, MPC is often formulated to track a path or trajectory returned by a planner. However, this strategy is brittle in that unexpected perturbations to the robot will require replanning, which can be costly at runtime. Instead, we show how the intermediate data structures used by modern planners can be interpreted as an approximate value function. We show that that this value function can be used by MPC directly, resulting in more efficient and resilient behavior at runtime.},
	urldate = {2024-07-23},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Hatch, Nathan and Boots, Byron},
	month = may,
	year = {2021},
	note = {ISSN: 2577-087X},
	keywords = {Function approximation, Perturbation methods, Planning, Reinforcement learning, Runtime, Tools, Trajectory},
	pages = {7372--7378},
}

@article{kenneally_design_2016,
	title = {Design {Principles} for a {Family} of {Direct}-{Drive} {Legged} {Robots}},
	volume = {1},
	issn = {2377-3766},
	doi = {10.1109/LRA.2016.2528294},
	abstract = {This letter introduces Minitaur, a dynamically running and leaping quadruped, which represents a novel class of direct-drive (DD) legged robots. We present a methodology that achieves the well-known benefits of DD robot design (transparency, mechanical robustness/efficiency, high-actuation bandwidth, and increased specific power), affording highly energetic behaviors across our family of machines despite severe limitations in specific force. We quantify DD drivetrain benefits using a variety of metrics, compare our machines' performance to previously reported legged platforms, and speculate on the potential broad-reaching value of “transparency” for legged locomotion.},
	number = {2},
	urldate = {2024-08-16},
	journal = {IEEE Robotics and Automation Letters},
	author = {Kenneally, Gavin and De, Avik and Koditschek, D. E.},
	month = jul,
	year = {2016},
	keywords = {Actuators, Couplings, Force, Kinematics, Legged locomotion, Mechanism Design of Mobile Robots, Multilegged Robots, Novel Actuators for Natural Machine Motion, Torque},
	pages = {900--907},
}

@book{rubinstein_cross-entropy_2004,
	address = {New York, NY},
	series = {Information {Science} and {Statistics}},
	title = {The {Cross}-{Entropy} {Method}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-1-4419-1940-3 978-1-4757-4321-0},
	urldate = {2024-08-16},
	publisher = {Springer},
	author = {Rubinstein, Reuven Y. and Kroese, Dirk P.},
	editor = {Jordan, Michael and Kleinberg, Jon and Schölkopf, Bernhard and Kelly, Frank P. and Witten, Ian},
	year = {2004},
	doi = {10.1007/978-1-4757-4321-0},
	keywords = {Excel, Monte-Carlo Simulation, Simulation, algorithm, algorithms, combinatorial optimization, computer, image processing, learning, machine learning, modeling, numerical methods, operations research, optimization, programming},
}

@article{sawant_bridging_2022,
	title = {Bridging the gap between {QP}-based and {MPC}-based {Reinforcement} {Learning}},
	copyright = {Navngivelse-Ikkekommersiell 4.0 Internasjonal},
	issn = {2405-8963},
	doi = {10.1016/j.ifacol.2022.07.600},
	abstract = {publishedVersion},
	language = {eng},
	journal = {IFAC-PapersOnLine},
	author = {Sawant, Shambhuraj Vijaysinh and Gros, Sebastien Nicolas},
	year = {2022},
}

@inproceedings{sawant_model-free_2023,
	title = {Model-{Free} {Data}-{Driven} {Predictive} {Control} {Using} {Reinforcement} {Learning}},
	doi = {10.1109/CDC49753.2023.10383431},
	abstract = {This paper proposes a novel approach for Predictive Control utilizing Reinforcement Learning (RL) and Data-Driven techniques to derive optimal control policies for real systems. Using pure input-output multi-step predictors based on Subspace Identification and RL techniques, the resulting predictive control scheme can approximate the optimal control policy of a system with high accuracy, even if the predictor cannot accurately capture the true system dynamics. One of the key contributions of the proposed approach is the extension of the framework connecting Model Predictive Control (MPC) and RL to one that does not require explicit state-space models, nor to define a notion of state at all. The paper demonstrates the efficacy of the proposed approach through an illustrative example, highlighting the ability of our approach to provide an optimal control policy for a real system without requiring any prior knowledge about its internal dynamics.},
	urldate = {2024-03-25},
	booktitle = {62nd {IEEE} {Conference} on {Decision} and {Control} ({CDC})},
	author = {Sawant, Shambhuraj and Reinhardt, Dirk and Kordabad, Arash Bahari and Gros, Sebastien},
	month = dec,
	year = {2023},
	keywords = {Aerospace electronics, Optimal control, Predictive models, Reinforcement learning, State-space methods, System dynamics, Task analysis},
	pages = {4046--4052},
}

@inproceedings{ho_generative_2016,
	title = {Generative {Adversarial} {Imitation} {Learning}},
	volume = {29},
	abstract = {Consider learning a policy from example expert behavior, without interaction with the expert or access to a reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free  methods in imitating complex behaviors in large, high-dimensional environments.},
	urldate = {2024-12-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ho, Jonathan and Ermon, Stefano},
	year = {2016},
}

@misc{fu_learning_2018,
	title = {Learning {Robust} {Rewards} with {Adversarial} {Inverse} {Reinforcement} {Learning}},
	doi = {10.48550/arXiv.1710.11248},
	abstract = {Reinforcement learning provides a powerful and general framework for decision making and control, but its application in practice is often hindered by the need for extensive feature and reward engineering. Deep reinforcement learning methods can remove the need for explicit engineering of policy or value features, but still require a manually specified reward function. Inverse reinforcement learning holds the promise of automatic reward acquisition, but has proven exceptionally difficult to apply to large, high-dimensional problems with unknown dynamics. In this work, we propose adverserial inverse reinforcement learning (AIRL), a practical and scalable inverse reinforcement learning algorithm based on an adversarial reward learning formulation. We demonstrate that AIRL is able to recover reward functions that are robust to changes in dynamics, enabling us to learn policies even under significant variation in the environment seen during training. Our experiments show that AIRL greatly outperforms prior methods in these transfer settings.},
	urldate = {2024-12-10},
	publisher = {arXiv},
	author = {Fu, Justin and Luo, Katie and Levine, Sergey},
	month = aug,
	year = {2018},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{mastalli_crocoddyl_2020,
	address = {Paris / Virtual, France},
	title = {Crocoddyl: {An} {Efficient} and {Versatile} {Framework} for {Multi}-{Contact} {Optimal} {Control}},
	shorttitle = {Crocoddyl},
	doi = {10.1109/ICRA40945.2020.9196673},
	abstract = {We introduce Crocoddyl (Contact RObot COntrol by Differential DYnamic Library), an open-source framework tailored for efficient multi-contact optimal control. Crocoddyl efficiently computes the state trajectory and the control policy for a given predefined sequence of contacts. Its efficiency is due to the use of sparse analytical derivatives, exploitation of the problem structure, and data sharing. It employs differential geometry to properly describe the state of any geometrical system, e.g. floating-base systems. We have unified dynamics, costs, and constraints into a single concept-action-for greater efficiency and easy prototyping. Additionally, we propose a novel multiple-shooting method called Feasibility-prone Differential Dynamic Programming (FDDP). Our novel method shows a greater globalization strategy compared to classical Differential Dynamic Programming (DDP) algorithms, and it has similar numerical behavior to state-of-the-art multiple-shooting methods. However, our method does not increase the computational complexity typically encountered by adding extra variables to describe the gaps in the dynamics. Concretely, we propose two modifications to the classical DDP algorithm. First, the backward pass accepts infeasible state-control trajectories. Second, the rollout keeps the gaps open during the early "exploratory" iterations (as expected in multiple-shooting methods). We showcase the performance of our framework using different tasks. With our method, we can compute highly-dynamic maneuvers for legged robots (e.g. jumping, front-flip) in the order of milliseconds.},
	urldate = {2024-08-16},
	booktitle = {{ICRA} 2020 {IEEE} {International} {Conference} on {Robotics} and {Automation}},
	author = {Mastalli, Carlos and Budhiraja, Rohan and Merkt, Wolfgang and Saurel, Guilhem and Hammoud, Bilal and Naveau, Maximilien and Carpentier, Justin and Vijayakumar, Sethu and Mansard, Nicolas},
	month = may,
	year = {2020},
}

@article{johansen_approximate_2003,
	title = {Approximate explicit constrained linear model predictive control via orthogonal search tree},
	volume = {48},
	journal = {IEEE Trans. Automatic Control},
	author = {Johansen, T.A. and Grancharova, A.},
	year = {2003},
	keywords = {correct},
	pages = {810--815},
}

@article{bohn_reinforcement_2021,
	title = {Reinforcement learning of the prediction horizon in model predictive control},
	volume = {54},
	number = {6},
	urldate = {2024-03-07},
	journal = {IFAC-PapersOnLine},
	author = {Bøhn, Eivind and Gros, Sebastien and Moe, Signe and Johansen, Tor Arne},
	year = {2021},
	pages = {314--320},
}

@inproceedings{mamedov_safe_2024,
	title = {Safe {Imitation} {Learning} of {Nonlinear} {Model} {Predictive} {Control} for {Flexible} {Robots}},
	doi = {10.1109/IROS58592.2024.10801854},
	abstract = {Flexible robots may overcome some of the industry’s major challenges, such as enabling intrinsically safe human-robot collaboration and achieving a higher payload-to-mass ratio. However, controlling flexible robots is complicated due to their complex dynamics, which include oscillatory behavior and a high-dimensional state space. Nonlinear model predictive control (NMPC) offers an effective means to control such robots, but its significant computational demand often limits its application in real-time scenarios. To enable fast control of flexible robots, we propose a framework for a safe approximation of NMPC using imitation learning and a predictive safety filter. Our framework significantly reduces computation time while incurring a slight loss in performance. Compared to NMPC, our framework shows more than an eightfold improvement in computation time when controlling a three-dimensional flexible robot arm in simulation, all while guaranteeing safety constraints. Notably, our approach out-performs state-of-the-art reinforcement learning methods. The development of fast and safe approximate NMPC holds the potential to accelerate the adoption of flexible robots in industry. The project code is available at: tinyurl.com/anmpc4fr},
	urldate = {2025-01-04},
	booktitle = {2024 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Mamedov, Shamil and Reiter, Rudolf and Azad, Seyed Mahdi B. and Viljoen, Ruan and Boedecker, Joschka and Diehl, Moritz and Swevers, Jan},
	month = oct,
	year = {2024},
	note = {ISSN: 2153-0866},
	keywords = {Approximation algorithms, Imitation learning, Manipulators, Predictive control, Reinforcement learning, Robots, Safety, Service robots, Soft robotics, Trajectory tracking},
	pages = {3613--3619},
}

@inproceedings{van_den_berg_efficient_2012,
	title = {Efficient approximate value iteration for continuous {Gaussian} {POMDPs}},
	volume = {26},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Van Den Berg, Jur and Patil, Sachin and Alterovitz, Ron},
	year = {2012},
	pages = {1832--1838},
}

@misc{vlahov_mppi-generic_2024,
	title = {{MPPI}-{Generic}: {A} {CUDA} {Library} for {Stochastic} {Optimization}},
	shorttitle = {{MPPI}-{Generic}},
	doi = {10.48550/arXiv.2409.07563},
	abstract = {This paper introduces a new C++/CUDA library for GPU-accelerated stochastic optimization called MPPI-Generic. It provides implementations of Model Predictive Path Integral control, Tube-Model Predictive Path Integral Control, and Robust Model Predictive Path Integral Control, and allows for these algorithms to be used across many pre-existing dynamics models and cost functions. Furthermore, researchers can create their own dynamics models or cost functions following our API definitions without needing to change the actual Model Predictive Path Integral Control code. Finally, we compare computational performance to other popular implementations of Model Predictive Path Integral Control over a variety of GPUs to show the real-time capabilities our library can allow for. Library code can be found at: https://acdslab.github.io/mppi-generic-website/ .},
	urldate = {2024-12-09},
	publisher = {arXiv},
	author = {Vlahov, Bogdan and Gibson, Jason and Gandhi, Manan and Theodorou, Evangelos A.},
	month = sep,
	year = {2024},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Mathematical Software, Computer Science - Robotics, Computer Science - Systems and Control, Electrical Engineering and Systems Science - Systems and Control},
}

@misc{bou_torchrl_2023,
	title = {{TorchRL}: {A} data-driven decision-making library for {PyTorch}},
	shorttitle = {{TorchRL}},
	doi = {10.48550/arXiv.2306.00577},
	abstract = {PyTorch has ascended as a premier machine learning framework, yet it lacks a native and comprehensive library for decision and control tasks suitable for large development teams dealing with complex real-world data and environments. To address this issue, we propose TorchRL, a generalistic control library for PyTorch that provides well-integrated, yet standalone components. We introduce a new and flexible PyTorch primitive, the TensorDict, which facilitates streamlined algorithm development across the many branches of Reinforcement Learning (RL) and control. We provide a detailed description of the building blocks and an extensive overview of the library across domains and tasks. Finally, we experimentally demonstrate its reliability and flexibility and show comparative benchmarks to demonstrate its computational efficiency. TorchRL fosters long-term support and is publicly available on GitHub for greater reproducibility and collaboration within the research community. The code is open-sourced on GitHub.},
	urldate = {2024-12-09},
	publisher = {arXiv},
	author = {Bou, Albert and Bettini, Matteo and Dittert, Sebastian and Kumar, Vikash and Sodhani, Shagun and Yang, Xiaomeng and Fabritiis, Gianni De and Moens, Vincent},
	month = nov,
	year = {2023},
	note = {arXiv:2306.00577 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{goldfain_autorally_2019,
	title = {{AutoRally}: {An} {Open} {Platform} for {Aggressive} {Autonomous} {Driving}},
	volume = {39},
	issn = {1941-000X},
	shorttitle = {{AutoRally}},
	doi = {10.1109/MCS.2018.2876958},
	abstract = {The technical challenge of creating a self-driving vehicle remains an open problem despite significant advancements from universities, car manufacturers, and technology companies. Full autonomy, known as level 5 (see "Society of Automotive Engineers Levels of Driving Automation"), is defined as full-time performance by an automated driving system of all aspects of the dynamic driving task under all roadway and environmental conditions that can be managed by a human driver. It is estimated that level 5 autonomous vehicles on public roads will help eliminate more than 90\% [1] of the 35,000 annual traffic fatalities caused by human error in the United States [2]; reduce commute time, road congestion, and pollution; and increase driving resource utilization [3].},
	number = {1},
	urldate = {2024-12-09},
	journal = {IEEE Control Systems Magazine},
	author = {Goldfain, Brian and Drews, Paul and You, Changxi and Barulic, Matthew and Velev, Orlin and Tsiotras, Panagiotis and Rehg, James M.},
	month = feb,
	year = {2019},
	note = {Conference Name: IEEE Control Systems Magazine},
	keywords = {Autonomous automobiles, Autonomous vehicles, Task analysis, Vehicle dynamics, Vehicular ad hoc networks, Wireless sensor networks},
	pages = {26--55},
}

@inproceedings{fujimoto_addressing_2018,
	series = {Proceedings of machine learning research},
	title = {Addressing function approximation error in actor-critic methods},
	volume = {80},
	booktitle = {Proceedings of the 35th international conference on machine learning, {ICML} 2018},
	publisher = {PMLR},
	author = {Fujimoto, Scott and van Hoof, Herke and Meger, David},
	editor = {Dy, Jennifer G. and Krause, Andreas},
	year = {2018},
	pages = {1582--1591},
}

@article{rawlings_model_2017-1,
	title = {Model predictive control with discrete actuators: {Theory} and application},
	volume = {78},
	journal = {Automatica},
	author = {Rawlings, James B. and Risbeck, Michael J.},
	year = {2017},
}

@misc{akbari_tiny_2024,
	title = {Tiny {Learning}-{Based} {MPC} for {Multirotors}: {Solver}-{Aware} {Learning} for {Efficient} {Embedded} {Predictive} {Control}},
	shorttitle = {Tiny {Learning}-{Based} {MPC} for {Multirotors}},
	url = {http://arxiv.org/abs/2410.23634},
	doi = {10.48550/arXiv.2410.23634},
	abstract = {Tiny aerial robots show promise for applications like environmental monitoring and search-and-rescue but face challenges in control due to their limited computing power and complex dynamics. Model Predictive Control (MPC) can achieve agile trajectory tracking and handle constraints. Although current learning-based MPC methods, such as Gaussian Process (GP) MPC, improve control performance by learning residual dynamics, they are computationally demanding, limiting their onboard application on tiny robots. This paper introduces Tiny Learning-Based Model Predictive Control (LB MPC), a novel framework for resource-constrained micro multirotor platforms. By exploiting multirotor dynamics' structure and developing an efficient solver, our approach enables high-rate control at 100 Hz on a Crazyflie 2.1 with a Teensy 4.0 microcontroller. We demonstrate a 23\% average improvement in tracking performance over existing embedded MPC methods, achieving the first onboard implementation of learning-based MPC on a tiny multirotor (53 g).},
	urldate = {2024-11-22},
	publisher = {arXiv},
	author = {Akbari, Babak and Frank, Justin and Greeff, Melissa},
	month = nov,
	year = {2024},
	note = {arXiv:2410.23634},
	keywords = {Computer Science - Robotics, Computer Science - Systems and Control, Electrical Engineering and Systems Science - Systems and Control},
}

@misc{sombolestan_hierarchical_2024,
	title = {Hierarchical {Adaptive} {Motion} {Planning} with {Nonlinear} {Model} {Predictive} {Control} for {Safety}-{Critical} {Collaborative} {Loco}-{Manipulation}},
	url = {http://arxiv.org/abs/2411.10699},
	doi = {10.48550/arXiv.2411.10699},
	abstract = {As legged robots take on roles in industrial and autonomous construction, collaborative loco-manipulation is crucial for handling large and heavy objects that exceed the capabilities of a single robot. However, ensuring the safety of these multi-robot tasks is essential to prevent accidents and guarantee reliable operation. This paper presents a hierarchical control system for object manipulation using a team of quadrupedal robots. The combination of the motion planner and the decentralized locomotion controller in a hierarchical structure enables safe, adaptive planning for teams in complex scenarios. A high-level nonlinear model predictive control planner generates collision-free paths by incorporating control barrier functions, accounting for static and dynamic obstacles. This process involves calculating contact points and forces while adapting to unknown objects and terrain properties. The decentralized loco-manipulation controller then ensures each robot maintains stable locomotion and manipulation based on the planner's guidance. The effectiveness of our method is carefully examined in simulations under various conditions and validated in real-life setups with robot hardware. By modifying the object's configuration, the robot team can maneuver unknown objects through an environment containing both static and dynamic obstacles. We have made our code publicly available in an open-source repository at {\textbackslash}url\{https://github.com/DRCL-USC/collaborative\_loco\_manipulation\}.},
	urldate = {2024-11-22},
	publisher = {arXiv},
	author = {Sombolestan, Mohsen and Nguyen, Quan},
	month = nov,
	year = {2024},
	note = {arXiv:2411.10699},
	keywords = {Computer Science - Robotics},
}

@book{rawlings_model_2017,
	address = {Santa Barbara, California},
	edition = {2nd edition},
	title = {Model {Predictive} {Control}: {Theory}, {Computation}, and {Design}},
	isbn = {978-0-9759377-5-4},
	shorttitle = {Model {Predictive} {Control}},
	language = {English},
	publisher = {Nob Hill Publishing},
	author = {Rawlings, James B. and Mayne, David Q. and Diehl, Moritz M.},
	year = {2017},
}

@article{qu_rl-driven_2024,
	title = {{RL}-{Driven} {MPPI}: {Accelerating} {Online} {Control} {Laws} {Calculation} {With} {Offline} {Policy}},
	volume = {9},
	issn = {2379-8904},
	shorttitle = {{RL}-{Driven} {MPPI}},
	doi = {10.1109/TIV.2023.3348134},
	abstract = {Model Predictive Path Integral (MPPI) is a recognized sampling-based approach for finite horizon optimal control problems. However, the efficacy and computational efficiency of prevailing MPPI methods are heavily reliant on the quality of rollouts. This is problematic because it is hard to sample a low-cost trajectory using random control sequences, thereby leading to inferior performance and computational efficiency, especially under constrained resources. To address this issue, we propose a data-efficient MPPI method called reinforcement learning-driven MPPI (RL-driven MPPI), which significantly reduces the dependency on the quantity and quality of samples. RL-driven MPPI employs an offline-online policy learning scheme, where the offline policy learned by RL serves as the initial solution and the initial rollout generator of MPPI, effectively combining the strengths of both RL and MPPI. The rollouts generated by RL typically correspond to a lower cost-to-go compared to random sampling, which significantly boosts the sample efficiency and convergence speed of MPPI. Moreover, the value function learned by RL offers an accurate estimation for infinite-horizon cost-to-go, enabling it to serve as a terminal term for the cost criteria of MPPI. This approach empowers MPPI to approximate an infinite-horizon cost with a shorter prediction horizon, thus enhancing real-time performance at each time step. An unmanned aerial vehicle control task is conducted to evaluate the proposed method. Results indicate that the proposed RL-driven MPPI method exhibits superior control performance and sample efficiency.},
	number = {2},
	urldate = {2024-11-22},
	journal = {IEEE Transactions on Intelligent Vehicles},
	author = {Qu, Yue and Chu, Hongqing and Gao, Shuhua and Guan, Jun and Yan, Haoqi and Xiao, Liming and Li, Shengbo Eben and Duan, Jingliang},
	month = feb,
	year = {2024},
	keywords = {Complex systems, Costs, Model predictive control (MPC), Optimal control, Real-time systems, Task analysis, Trajectory, Vehicle dynamics, reinforcement learning (RL), unmanned aerial vehicle (UAV)},
	pages = {3605--3616},
}

@misc{dyro_particle_2021,
	title = {Particle {MPC} for {Uncertain} and {Learning}-{Based} {Control}},
	url = {http://arxiv.org/abs/2104.02213},
	abstract = {As robotic systems move from highly structured environments to open worlds, incorporating uncertainty from dynamics learning or state estimation into the control pipeline is essential for robust performance. In this paper we present a nonlinear particle model predictive control (PMPC) approach to control under uncertainty, which directly incorporates any particle-based uncertainty representation, such as those common in robotics. Our approach builds on scenario methods for MPC, but in contrast to existing approaches, which either constrain all or only the ﬁrst timestep to share actions across scenarios, we investigate the impact of a partial consensus horizon. Implementing this optimization for nonlinear dynamics by leveraging sequential convex optimization, our approach yields an efﬁcient framework that can be tuned to the particular information gain dynamics of a system to mitigate both overconservatism and over-optimism. We investigate our approach for two robotic systems across three problem settings: timevarying, partially observed dynamics; sensing uncertainty; and model-based reinforcement learning, and show that our approach improves performance over baselines in all settings.},
	language = {en},
	urldate = {2024-08-17},
	publisher = {arXiv},
	author = {Dyro, Robert and Harrison, James and Sharma, Apoorva and Pavone, Marco},
	month = sep,
	year = {2021},
	note = {arXiv:2104.02213 [cs, eess]},
	keywords = {Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
}

@misc{chisari_learning_2021,
	title = {Learning from {Simulation}, {Racing} in {Reality}},
	url = {http://arxiv.org/abs/2011.13332},
	abstract = {We present a reinforcement learning-based solution to autonomously race on a miniature race car platform. We show that a policy that is trained purely in simulation using a relatively simple vehicle model, including model randomization, can be successfully transferred to the real robotic setup. We achieve this by using a novel policy output regularization approach and a lifted action space which enables smooth actions but still aggressive race car driving. We show that this regularized policy does outperform the Soft Actor Critic (SAC) baseline method, both in simulation and on the real car, but it is still outperformed by a Model Predictive Controller (MPC) state-of-the-art method. The reﬁnement of the policy with three hours of real-world interaction data allows the reinforcement learning policy to achieve lap times similar to the MPC controller while reducing track constraint violations by 50\%.},
	language = {en},
	urldate = {2024-08-17},
	publisher = {arXiv},
	author = {Chisari, Eugenio and Liniger, Alexander and Rupenyan, Alisa and Van Gool, Luc and Lygeros, John},
	month = may,
	year = {2021},
	note = {arXiv:2011.13332 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
}

@misc{viereck_valuenetqp_2022,
	title = {{ValueNetQP}: {Learned} one-step optimal control for legged locomotion},
	shorttitle = {{ValueNetQP}},
	url = {http://arxiv.org/abs/2201.04090},
	abstract = {Optimal control is a successful approach to generate motions for complex robots, in particular for legged locomotion. However, these techniques are often too slow to run in real time for model predictive control or one needs to drastically simplify the dynamics model. In this work, we present a method to learn to predict the gradient and hessian of the problem value function, enabling fast resolution of the predictive control problem with a one-step quadratic program. In addition, our method is able to satisfy constraints like friction cones and unilateral constraints, which are important for high dynamics locomotion tasks. We demonstrate the capability of our method in simulation and on a real quadruped robot performing trotting and bounding motions.},
	language = {en},
	urldate = {2024-08-17},
	publisher = {arXiv},
	author = {Viereck, Julian and Meduri, Avadesh and Righetti, Ludovic},
	month = jan,
	year = {2022},
	note = {arXiv:2201.04090 [cs]},
	keywords = {Computer Science - Robotics},
}

@article{gao_reinforcement_2022,
	title = {Reinforcement {Learning} {Based} {Online} {Parameter} {Adaptation} for {Model} {Predictive} {Tracking} {Control} {Under} {Slippery} {Condition}},
	copyright = {https://doi.org/10.15223/policy-029},
	url = {https://ieeexplore.ieee.org/document/9867368/},
	doi = {10.23919/ACC53348.2022.9867368},
	abstract = {Wheeled mobile robots have a great variety of applications both indoors and outdoors. They are often required to work in various environments such as on rough terrains, wet roads, icy roads, and routes with rapid cornering. Therefore, it is important to design a control scheme that delivers robust tracking performance on various terrains, especially the ones that induce skidding and slipping easily. In this work, we focus on the challenge of mobile robot motion under slippery conditions, and propose a hierarchical framework that learns to adapt the control parameters online for robust model predictive tracking control of mobile robots. Concretely, the high level module based on reinforcement learning (RL) actively adjusts the model predictive control (MPC) scheme to be more aggressive or conservative, by adapting key parameters in the MPC optimization formulation. Experiments demonstrate our framework achieves adaptive and robust tracking performance, especially at rejecting slipping and reducing tracking errors when the mobile robot travels through various terrains. Our framework neither relies on knowing specific dynamic parameters nor requires data fitting. The model trained in simulation is validated in a zero-shot manner with completely different real-world terrain conditions to demonstrate its adaptability.},
	urldate = {2024-08-17},
	journal = {2022 American Control Conference (ACC)},
	author = {Gao, Huidong and Zhou, Rui and Tomizuka, Masayoshi and Xu, Zhuo},
	month = jun,
	year = {2022},
	note = {Conference Name: 2022 American Control Conference (ACC)
ISBN: 9781665451963
Place: Atlanta, GA, USA
Publisher: IEEE},
	pages = {2675--2682},
}

@inproceedings{lidec_enforcing_2023,
	title = {Enforcing the consensus between {Trajectory} {Optimization} and {Policy} {Learning} for precise robot control},
	doi = {10.1109/ICRA48891.2023.10160387},
	abstract = {Reinforcement learning (RL) and trajectory opti-mization (TO) present strong complementary advantages. On one hand, RL approaches are able to learn global control policies directly from data, but generally require large sample sizes to properly converge towards feasible policies. On the other hand, TO methods are able to exploit gradient-based information extracted from simulators to quickly converge towards a locally optimal control trajectory which is only valid within the vicinity of the solution. Over the past decade, several approaches have aimed to adequately combine the two classes of methods in order to obtain the best of both worlds. Following on from this line of research, we propose several improvements on top of these approaches to learn global control policies quicker, notably by leveraging sensitivity information stemming from TO methods via Sobolev learning, and Augmented Lagrangian (AL) techniques to enforce the consensus between TO and policy learning. We evaluate the benefits of these improvements on various classical tasks in robotics through comparison with existing approaches in the literature.},
	urldate = {2024-07-23},
	booktitle = {{IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Lidec, Quentin Le and Jallet, Wilson and Laptev, Ivan and Schmid, Cordelia and Carpentier, Justin},
	month = may,
	year = {2023},
	keywords = {Automation, Data mining, Optimal control, Reinforcement learning, Robot control, Robot sensing systems, Sensitivity},
	pages = {946--952},
}

@inproceedings{morgan_model_2021,
	title = {Model {Predictive} {Actor}-{Critic}: {Accelerating} {Robot} {Skill} {Acquisition} with {Deep} {Reinforcement} {Learning}},
	shorttitle = {Model {Predictive} {Actor}-{Critic}},
	doi = {10.1109/ICRA48506.2021.9561298},
	abstract = {Substantial advancements to model-based reinforcement learning algorithms have been impeded by the model-bias induced by the collected data, which generally hurts performance. Meanwhile, their inherent sample efficiency warrants utility for most robot applications, limiting potential damage to the robot and its environment during training. Inspired by information theoretic model predictive control and advances in deep reinforcement learning, we introduce Model Predictive Actor-Critic (MoPAC)\&lt;sup\&gt;\&amp;\#x2020;\&lt;/sup\&gt;, a hybrid model-based/model-free method that combines model predictive rollouts with policy optimization as to mitigate model bias. MoPAC leverages optimal trajectories to guide policy learning, but explores via its model-free method, allowing the algorithm to learn more expressive dynamics models. This combination guarantees optimal skill learning up to an approximation error and reduces necessary physical interaction with the environment, making it suitable for real-robot training. We provide extensive results showcasing how our proposed method generally outperforms current state-of-the-art and conclude by evaluating MoPAC for learning on a physical robotic hand performing valve rotation and finger gaiting\&amp;\#x2013;a task that requires grasping, manipulation, and then regrasping of an object.},
	urldate = {2024-07-23},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE Press},
	author = {Morgan, Andrew S. and Nandha, Daljeet and Chalvatzaki, Georgia and D’Eramo, Carlo and Dollar, Aaron M. and Peters, Jan},
	year = {2021},
	pages = {6672--6678},
}

@misc{yang_coordinating_2024,
	title = {Coordinating {Planning} and {Tracking} in {Layered} {Control} {Policies} via {Actor}-{Critic} {Learning}},
	url = {http://arxiv.org/abs/2408.01639},
	abstract = {We propose a reinforcement learning (RL)-based algorithm to jointly train (1) a trajectory planner and (2) a tracking controller in a layered control architecture. Our algorithm arises naturally from a rewrite of the underlying optimal control problem that lends itself to an actorcritic learning approach. By explicitly learning a dual network to coordinate the interaction between the planning and tracking layers, we demonstrate the ability to achieve an effective consensus between the two components, leading to an interpretable policy. We theoretically prove that our algorithm converges to the optimal dual network in the Linear Quadratic Regulator (LQR) setting and empirically validate its applicability to nonlinear systems through simulation experiments on a unicycle model.},
	language = {en},
	urldate = {2024-08-16},
	publisher = {arXiv},
	author = {Yang, Fengjun and Matni, Nikolai},
	month = aug,
	year = {2024},
	note = {arXiv:2408.01639 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control},
}

@inproceedings{piche_probabilistic_2018,
	title = {Probabilistic {Planning} with {Sequential} {Monte} {Carlo} methods},
	abstract = {In this work, we propose a novel formulation of planning which views it as a probabilistic inference problem over future optimal trajectories. This enables us to use sampling methods, and thus, tackle planning in continuous domains using a fixed computational budget. We design a new algorithm, Sequential Monte Carlo Planning, by leveraging classical methods in Sequential Monte Carlo and Bayesian smoothing in the context of control as inference. Furthermore, we show that Sequential Monte Carlo Planning can capture multimodal policies and can quickly learn continuous control tasks.},
	language = {en},
	author = {Piche, Alexandre and Thomas, Valentin and Ibrahim, Cyril and Bengio, Yoshua and Pal, Chris},
	month = sep,
	year = {2018},
}

@inproceedings{abdolmaleki_maximum_2018,
	title = {Maximum a {Posteriori} {Policy} {Optimisation}},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Abdolmaleki, Abbas and Springenberg, Jost Tobias and Tassa, Yuval and Munos, Remi and Heess, Nicolas and Riedmiller, Martin},
	year = {2018},
}

@inproceedings{fiedler_relationship_2021,
	title = {On the relationship between data-enabled predictive control and subspace predictive control},
	doi = {10.23919/ECC54610.2021.9654975},
	abstract = {Data-enabled predictive control (DeePC) is a recently proposed approach that combines system identification, estimation and control in a single optimization problem, for which only recorded input/output data of the examined system is required. The same premise holds for the subspace predictive control (SPC) method in which a multi-step prediction model is identified from the same data as required for DeePC. This model is then used to formulate a similar optimal control problem. In this work we investigate the relationship between DeePC and SPC. Our primary contribution is to show that SPC is equivalent to DeePC in the deterministic case. We also show the equivalence of both methods in a special case for the non-deterministic formulation. We investigate the advantages and shortcomings of DeePC as opposed to SPC with and without measurement noise and illustrate them with a simulation example.},
	booktitle = {2021 european control conference ({ECC})},
	author = {Fiedler, Felix and Lucia, Sergio},
	month = jun,
	year = {2021},
	pages = {222--229},
}

@inproceedings{pfrommer_safe_2022,
	title = {Safe {Reinforcement} {Learning} with {Chance}-constrained {Model} {Predictive} {Control}},
	abstract = {Real-world reinforcement learning (RL) problems often demand that agents behave safely by obeying a set of designed constraints. We address the challenge of safe RL by coupling a safety guide based on model predictive control (MPC) with a modified policy gradient framework in a linear setting with continuous actions. The guide enforces safe operation of the system by embedding safety requirements as chance constraints in the MPC formulation. The policy gradient training step then includes a safety penalty which trains the base policy to behave safely. We show theoretically that this penalty allows for a provably safe optimal base policy and illustrate our method with a simulated linearized quadrotor experiment.},
	language = {en},
	urldate = {2024-07-23},
	booktitle = {Proceedings of {The} 4th {Annual} {Learning} for {Dynamics} and {Control} {Conference}},
	publisher = {PMLR},
	author = {Pfrommer, Samuel and Gautam, Tanmay and Zhou, Alec and Sojoudi, Somayeh},
	month = may,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {291--303},
}

@article{tao_difftune-mpc_2024,
	title = {{DiffTune}-{MPC}: {Closed}-{Loop} {Learning} for {Model} {Predictive} {Control}},
	journal = {IEEE Robotics and Automation Letters},
	author = {Tao, Ran and Cheng, Sheng and Wang, Xiaofeng and Wang, Shenlong and Hovakimyan, Naira},
	year = {2024},
	note = {Publisher: IEEE},
}

@article{adhau_fast_2023,
	series = {22nd {IFAC} {World} {Congress}},
	title = {Fast {Reinforcement} {Learning} {Based} {MPC} based on {NLP} {Sensitivities}},
	volume = {56},
	issn = {2405-8963},
	doi = {10.1016/j.ifacol.2023.10.586},
	abstract = {This paper proposes a comprehensive approach to improve the computational efficiency of Reinforcement Learning (RL) based Model Predictive Controller (MPC). Although MPC will ensure controller safety and RL can generate optimal control policies, combining the two requires substantial time and computational effort, particularly for larger data sets. In a typical RL-based MPC and Q-learning workflow, two not-so-different MPC problems must be evaluated at each RL iteration, i.e. one for the action-value and one for the value function, which is time-consuming and prohibitively expensive in terms of computations. We employ nonlinear programming (NLP) sensitivities to approximate the action-value function using the optimal solution from the value function, reducing computational time. The proposed approach can achieve comparable performance to the conventional method but with significantly lower computational time. We demonstrate the proposed approach on two examples: Linear Quadratic Regulator (LQR) problem and Continuously Stirred Tank Reactor (CSTR).},
	number = {2},
	urldate = {2024-03-25},
	journal = {IFAC-PapersOnLine},
	author = {Adhau, Saket and Reinhardt, Dirk and Skogestad, Sigurd and Gros, Sebastien},
	month = jan,
	year = {2023},
	keywords = {Economic model predictive control, Nonlinear programming, Reinforcement learning, Sensitivity},
	pages = {11841--11846},
}

@article{drgona_learning_2024,
	title = {Learning {Constrained} {Parametric} {Differentiable} {Predictive} {Control} {Policies} {With} {Guarantees}},
	copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {2168-2216, 2168-2232},
	doi = {10.1109/TSMC.2024.3368026},
	abstract = {We present differentiable predictive control (DPC), a method for ofﬂine learning of constrained neural control policies for nonlinear dynamical systems with performance guarantees. We show that the sensitivities of the parametric optimal control problem can be used to obtain direct policy gradients. Speciﬁcally, we employ automatic differentiation (AD) to efﬁciently compute the sensitivities of the model predictive control (MPC) objective function and constraints penalties. To guarantee safety upon deployment, we derive probabilistic guarantees on closed-loop stability and constraint satisfaction based on indicator functions and Hoeffding’s inequality. We empirically demonstrate that the proposed method can learn neural control policies for various parametric optimal control tasks. In particular, we show that the proposed DPC method can stabilize systems with unstable dynamics, track time-varying references, and satisfy nonlinear state and input constraints. Our DPC method has practical time savings compared to alternative approaches for fast and memoryefﬁcient controller design. Speciﬁcally, DPC does not depend on a supervisory controller as opposed to approximate MPC based on imitation learning. We demonstrate that, without losing performance, DPC is scalable with greatly reduced demands on memory and computation compared to implicit and explicit MPC while being more sample efﬁcient than model-free reinforcement learning (RL) algorithms.},
	language = {en},
	urldate = {2024-05-13},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
	author = {Drgoňa, Ján and Tuor, Aaron and Vrabie, Draguna},
	year = {2024},
	keywords = {cite},
	pages = {1--12},
}

@article{kordabad_safe_2022,
	title = {Safe {Reinforcement} {Learning} {Using} {Wasserstein} {Distributionally} {Robust} {MPC} and {Chance} {Constraint}},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3228922},
	abstract = {In this paper, we address the chance-constrained safe Reinforcement Learning (RL) problem using the function approximators based on Stochastic Model Predictive Control (SMPC) and Distributionally Robust Model Predictive Control (DRMPC). We use Conditional Value at Risk (CVaR) to measure the probability of constraint violation and safety. In order to provide a safe policy by construction, we first propose using parameterized nonlinear DRMPC at each time step. DRMPC optimizes a finite-horizon cost function subject to the worst-case constraint violation in an ambiguity set. We use a statistical ball around the empirical distribution with a radius measured by the Wasserstein metric as the ambiguity set. Unlike the sample average approximation SMPC, DRMPC provides a probabilistic guarantee of the out-of-sample risk and requires lower samples from the disturbance. Then the Q-learning method is used to optimize the parameters in the DRMPC to achieve the best closed-loop performance. Wheeled Mobile Robot (WMR) path planning with obstacle avoidance will be considered to illustrate the efficiency of the proposed method.},
	urldate = {2024-03-25},
	journal = {IEEE Access},
	author = {Kordabad, Arash Bahari and Wisniewski, Rafael and Gros, Sebastien},
	year = {2022},
	note = {Conference Name: IEEE Access},
	keywords = {Costs, Predictive control, Probabilistic logic, Q-learning, Reactive power, Reinforcement learning, Risk management, Safe reinforcement learning, Safety, Stochastic processes, chance constraint, conditional value at risk, distributionally robust optimization, model predictive control},
	pages = {130058--130067},
}

@article{norouzi_integrating_2023,
	title = {Integrating {Machine} {Learning} and {Model} {Predictive} {Control} for automotive applications: {A} review and future directions},
	volume = {120},
	issn = {0952-1976},
	shorttitle = {Integrating {Machine} {Learning} and {Model} {Predictive} {Control} for automotive applications},
	doi = {10.1016/j.engappai.2023.105878},
	abstract = {In this review paper, the integration of Machine Learning (ML) and Model Predictive Control (MPC) in Automotive Control System (ACS) applications are discussed. ACS can be divided into these three main subsystems: enhancing safety, improving comfort, and reducing fuel consumption and emissions. Due to the development of new technologies such as advancing autonomous and connected vehicles the complexity of these subsystems is increasing. The ACS is meant to encompass the vehicle dynamics, powertrain control, passenger comfort, and accessories. Since vehicle manufacturers must meet stringent performance and emission requirements, optimal control methods for ACS applications are seen as a promising technology. MPC is an optimal control method for closed-loop control applications that allows constraints to be enforced in real-time while an objective function is minimized. The application of MPC in the automotive industry has been shown in the past decade. An important challenge in the design and real-time implementation of MPC is having a accurate predictive model that also does not require excessive real-time computation. Using ML to provide an accurate model at decreased computational cost improves MPC performance of ACS and is the main focus of this paper. How MPC in ML-based ACS applications ensures stability while meeting constraint is also discussed. Method to combine MPC and ML for the ACS subsystems of vehicle dynamics and powertrain control are reviewed and an outlook on future ACS is discussed.},
	urldate = {2024-05-27},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Norouzi, Armin and Heidarifar, Hamed and Borhan, Hoseinali and Shahbakhti, Mahdi and Koch, Charles Robert},
	month = apr,
	year = {2023},
	keywords = {Automotive Control Systems, Machine Learning, Model Predictive Control},
	pages = {105878},
}

@article{hewing_learning-based_2020,
	title = {Learning-based model predictive control: {Toward} safe learning in control},
	volume = {3},
	doi = {10.1146/annurev-control-090419-075625},
	abstract = {Recent successes in the field of machine learning, as well as the availability of increased sensing and computational capabilities in modern control systems, have led to a growing interest in learning and data-driven control techniques. Model predictive control (MPC), as the prime methodology for constrained control, offers a significant opportunity to exploit the abundance of data in a reliable manner, particularly while taking safety constraints into account. This review aims at summarizing and categorizing previous research on learning-based MPC, i.e., the integration or combination of MPC with learning methods, for which we consider three main categories. Most of the research addresses learning for automatic improvement of the prediction model from recorded data. There is, however, also an increasing interest in techniques to infer the parameterization of the MPC controller, i.e., the cost and constraints, that lead to the best closed-loop performance. Finally, we discuss concepts that leverage MPC to augment learning-based controllers with constraint satisfaction properties.},
	number = {1},
	journal = {Annual Review of Control, Robotics, and Autonomous Systems},
	author = {Hewing, Lukas and Wabersich, Kim P. and Menner, Marcel and Zeilinger, Melanie N.},
	year = {2020},
	pages = {269--296},
}

@article{dulac-arnold_challenges_2021,
	title = {Challenges of real-world reinforcement learning: definitions, benchmarks and analysis},
	volume = {110},
	issn = {1573-0565},
	shorttitle = {Challenges of real-world reinforcement learning},
	doi = {10.1007/s10994-021-05961-4},
	abstract = {Reinforcement learning (RL) has proven its worth in a series of artificial domains, and is beginning to show some successes in real-world scenarios. However, much of the research advances in RL are hard to leverage in real-world systems due to a series of assumptions that are rarely satisfied in practice. In this work, we identify and formalize a series of independent challenges that embody the difficulties that must be addressed for RL to be commonly deployed in real-world systems. For each challenge, we define it formally in the context of a Markov Decision Process, analyze the effects of the challenge on state-of-the-art learning algorithms, and present some existing attempts at tackling it. We believe that an approach that addresses our set of proposed challenges would be readily deployable in a large number of real world problems. Our proposed challenges are implemented in a suite of continuous control environments called realworldrl-suite which we propose an as an open-source benchmark.},
	language = {en},
	number = {9},
	urldate = {2024-05-24},
	journal = {Machine Learning},
	author = {Dulac-Arnold, Gabriel and Levine, Nir and Mankowitz, Daniel J. and Li, Jerry and Paduraru, Cosmin and Gowal, Sven and Hester, Todd},
	month = sep,
	year = {2021},
	pages = {2419--2468},
}

@article{kordabad_equivalence_2024,
	title = {Equivalence of {Optimality} {Criteria} for {Markov} {Decision} {Process} and {Model} {Predictive} {Control}},
	volume = {69},
	issn = {1558-2523},
	doi = {10.1109/TAC.2023.3277309},
	abstract = {This article shows that the optimal policy and value functions of a Markov decision process (MDP), either discounted or not, can be captured by a finite-horizon undiscounted optimal control problem (OCP), even if based on an inexact model. This can be achieved by selecting a proper stage cost and terminal cost for the OCP. A very useful particular case of OCP is a model predictive control (MPC) scheme where a deterministic (possibly nonlinear) model is used to reduce the computational complexity. This observation leads us to parameterize an MPC scheme fully, including the cost function. In practice, reinforcement learning algorithms can then be used to tune the parameterized MPC scheme. We verify the developed theorems analytically in an LQR case and we investigate some other nonlinear examples in simulations.},
	number = {2},
	urldate = {2024-03-25},
	journal = {IEEE Transactions on Automatic Control},
	author = {Kordabad, Arash Bahari and Zanon, Mario and Gros, Sebastien},
	month = feb,
	year = {2024},
	note = {Conference Name: IEEE Transactions on Automatic Control},
	keywords = {Computational modeling, Costs, Markov decision process (MDP), Markov processes, Mathematical models, Predictive models, Stability criteria, Trajectory, model predictive control (MPC), optimality, reinforcement learning (RL)},
	pages = {1149--1156},
}

@book{stengel_optimal_1994,
	title = {Optimal control and estimation},
	isbn = {0-486-68200-5},
	publisher = {Courier Corporation},
	author = {Stengel, Robert F.},
	year = {1994},
}

@article{grandesso_cacto_2023,
	title = {{CACTO}: {Continuous} {Actor}-{Critic} {With} {Trajectory} {Optimization}—{Towards} {Global} {Optimality}},
	volume = {8},
	issn = {2377-3766},
	shorttitle = {{CACTO}},
	doi = {10.1109/LRA.2023.3266985},
	abstract = {This letter presents a novel algorithm for the continuous control of dynamical systems that combines Trajectory Optimization (TO) and Reinforcement Learning (RL) in a single framework. The motivations behind this algorithm are the two main limitations of TO and RL when applied to continuous nonlinear systems to minimize a non-convex cost function. Specifically, TO can get stuck in poor local minima when the search is not initialized close to a “good” minimum. On the other hand, when dealing with continuous state and control spaces, the RL training process may be excessively long and strongly dependent on the exploration strategy. Thus, our algorithm learns a “good” control policy via TO-guided RL policy search that, when used as initial guess provider for TO, makes the trajectory optimization process less prone to converge to poor local optima. Our method is validated on several reaching problems featuring non-convex obstacle avoidance with different dynamical systems, including a car model with 6D state, and a 3-joint planar manipulator. Our results show the great capabilities of CACTO in escaping local minima, while being more computationally efficient than the Deep Deterministic Policy Gradient (DDPG) and Proximal Policy Optimization (PPO) RL algorithms.},
	number = {6},
	urldate = {2024-07-09},
	journal = {IEEE Robotics and Automation Letters},
	author = {Grandesso, Gianluigi and Alboni, Elisa and Papini, Gastone P. Rosati and Wensing, Patrick M. and Prete, Andrea Del},
	month = jun,
	year = {2023},
	keywords = {Approximation algorithms, Cost function, Costs, Heuristic algorithms, Optimal control, Training, Trajectory optimization, continuous control, reinforcement learning},
	pages = {3318--3325},
}

@misc{gros_towards_2019,
	title = {Towards {Safe} {Reinforcement} {Learning} {Using} {NMPC} and {Policy} {Gradients}: {Part} {I} - {Stochastic} case},
	shorttitle = {Towards {Safe} {Reinforcement} {Learning} {Using} {NMPC} and {Policy} {Gradients}},
	doi = {10.48550/arXiv.1906.04057},
	abstract = {We present a methodology to deploy the stochastic policy gradient method, using actor-critic techniques, when the optimal policy is approximated using a parametric optimization problem, allowing one to enforce safety via hard constraints. For continuous input spaces, imposing safety restrictions on the stochastic policy can make the sampling and evaluation of its density difficult. This paper proposes a computationally effective approach to solve that issue. We will focus on policy approximations based on robust Nonlinear Model Predictive Control (NMPC), where safety can be treated explicitly. For the sake of brevity, we will detail safe policies in the robust linear MPC context only. The extension to the nonlinear case is possible but more complex. We will additionally present a technique to maintain the system safety throughout the learning process in the context of robust linear MPC. This paper has a companion paper treating the deterministic policy gradient case.},
	urldate = {2024-03-25},
	publisher = {arXiv},
	author = {Gros, Sebastien and Zanon, Mario},
	month = jun,
	year = {2019},
	note = {arXiv:1906.04057 [cs]},
	keywords = {Electrical Engineering and Systems Science - Systems and Control},
}

@misc{jacquet_n-mpc_2024,
	title = {N-{MPC} for {Deep} {Neural} {Network}-{Based} {Collision} {Avoidance} exploiting {Depth} {Images}},
	abstract = {This paper introduces a Nonlinear Model Predictive Control (N-MPC) framework exploiting a Deep Neural Network for processing onboard-captured depth images for collision avoidance in trajectory-tracking tasks with UAVs. The network is trained on simulated depth images to output a collision score for queried 3D points within the sensor field of view. Then, this network is translated into an algebraic symbolic equation and included in the N-MPC, explicitly constraining predicted positions to be collision-free throughout the receding horizon. The N-MPC achieves real time control of a UAV with a control frequency of 100Hz. The proposed framework is validated through statistical analysis of the collision classifier network, as well as Gazebo simulations and real experiments to assess the resulting capabilities of the N-MPC to effectively avoid collisions in cluttered environments. The associated code is released open-source.},
	language = {en},
	urldate = {2024-05-03},
	publisher = {arXiv},
	author = {Jacquet, Martin and Alexis, Kostas},
	month = feb,
	year = {2024},
	note = {arXiv:2402.13038 [cs]},
	keywords = {Computer Science - Robotics},
}

@article{kordabad_reinforcement_2023,
	series = {22nd {IFAC} {World} {Congress}},
	title = {Reinforcement {Learning} for {MPC}: {Fundamentals} and {Current} {Challenges}},
	volume = {56},
	issn = {2405-8963},
	shorttitle = {Reinforcement {Learning} for {MPC}},
	doi = {10.1016/j.ifacol.2023.10.548},
	abstract = {Recent publications have laid a solid theoretical foundation for the combination of Reinforcement Learning and Model Predictive Control, in view of obtaining high-performance data-driven MPC policies. Early practical results, both in simulation and in experiments, have shown the potential of this combination but have also revealed certain challenges. In addition, the technical complexity of these results makes it difficult for interested readers to gather the fundamental ideas and principles behind this combination. This paper aims to provide a coherent and more accessible picture of these results and to offer significantly deeper and more mature insights into their meaning than has been proposed before. It also aims at identifying the current challenges in the field.},
	number = {2},
	urldate = {2024-03-25},
	journal = {IFAC-PapersOnLine},
	author = {Kordabad, Arash Bahari and Reinhardt, Dirk and Anand, Akhil S and Gros, Sebastien},
	month = jan,
	year = {2023},
	keywords = {Learning for MPC, MPC, Reinforcement Learning, Stability \& Safety},
	pages = {5773--5780},
}

@inproceedings{wang_bregman_2014,
	title = {Bregman {Alternating} {Direction} {Method} of {Multipliers}},
	volume = {27},
	urldate = {2024-05-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Wang, Huahua and Banerjee, Arindam},
	year = {2014},
}

@article{brito_where_2021,
	title = {Where to go {Next}: {Learning} a {Subgoal} {Recommendation} {Policy} for {Navigation} in {Dynamic} {Environments}},
	volume = {6},
	issn = {2377-3766},
	shorttitle = {Where to go {Next}},
	doi = {10.1109/LRA.2021.3068662},
	abstract = {Robotic navigation in environments shared with other robots or humans remains challenging because the intentions of the surrounding agents are not directly observable and the environment conditions are continuously changing. Local trajectory optimization methods, such as model predictive control (MPC), can deal with those changes but require global guidance, which is not trivial to obtain in crowded scenarios. This letter proposes to learn, via deep Reinforcement Learning (RL), an interaction-aware policy that provides long-term guidance to the local planner. In particular, in simulations with cooperative and non-cooperative agents, we train a deep network to recommend a subgoal for the MPC planner. The recommended subgoal is expected to help the robot in making progress towards its goal and accounts for the expected interaction with other agents. Based on the recommended subgoal, the MPC planner then optimizes the inputs for the robot satisfying its kinodynamic and collision avoidance constraints. Our approach is shown to substantially improve the navigation performance in terms of number of collisions as compared to prior MPC frameworks, and in terms of both travel time and number of collisions compared to deep RL methods in cooperative, competitive and mixed multiagent scenarios.},
	number = {3},
	urldate = {2024-01-18},
	journal = {IEEE Robotics and Automation Letters},
	author = {Brito, Bruno and Everett, Michael and How, Jonathan P. and Alonso-Mora, Javier},
	month = jul,
	year = {2021},
	pages = {4616--4623},
}

@inproceedings{everett_neural_2021,
	address = {Austin, TX, USA},
	title = {Neural {Network} {Verification} in {Control}},
	isbn = {978-1-66543-659-5},
	doi = {10.1109/CDC45484.2021.9683154},
	abstract = {Learning-based methods could provide solutions to many of the long-standing challenges in control. However, the neural networks (NNs) commonly used in modern learning approaches present substantial challenges for analyzing the resulting control systems’ safety properties. Fortunately, a new body of literature could provide tractable methods for analysis and veriﬁcation of these high dimensional, highly nonlinear representations. This tutorial ﬁrst introduces and uniﬁes recent techniques (many of which originated in the computer vision and machine learning communities) for verifying robustness properties of NNs. The techniques are then extended to provide formal guarantees of neural feedback loops (e.g., closed-loop system with NN control policy). The provided tools are shown to enable closed-loop reachability analysis and robust deep reinforcement learning.},
	language = {en},
	urldate = {2024-05-14},
	booktitle = {2021 60th {IEEE} {Conference} on {Decision} and {Control} ({CDC})},
	publisher = {IEEE},
	author = {Everett, Michael},
	month = dec,
	year = {2021},
	keywords = {cite},
	pages = {6326--6340},
}

@article{carius_mpc-net_2020,
	title = {{MPC}-{Net}: {A} {First} {Principles} {Guided} {Policy} {Search}},
	volume = {5},
	issn = {2377-3766, 2377-3774},
	shorttitle = {{MPC}-{Net}},
	doi = {10.1109/LRA.2020.2974653},
	abstract = {We present an Imitation Learning approach for the control of dynamical systems with a known model. Our policy search method is guided by solutions from MPC. Typical policy search methods of this kind minimize a distance metric between the guiding demonstrations and the learned policy. Our loss function, however, corresponds to the minimization of the control Hamiltonian, which derives from the principle of optimality. Therefore, our algorithm directly attempts to solve the optimality conditions with a parameterized class of control laws. Additionally, the proposed loss function explicitly encodes the constraints of the optimal control problem and we provide numerical evidence that its minimization achieves improved constraint satisfaction. We train a mixture-of-expert neural network architecture for controlling a quadrupedal robot and show that this policy structure is well suited for such multimodal systems. The learned policy can successfully stabilize different gaits on the real walking robot from less than 10 min of demonstration data.},
	number = {2},
	urldate = {2023-11-27},
	journal = {IEEE Robotics and Automation Letters},
	author = {Carius, Jan and Farshidian, Farbod and Hutter, Marco},
	month = apr,
	year = {2020},
	keywords = {correct},
	pages = {2897--2904},
}

@inproceedings{cosner_end--end_2022,
	address = {Cancun, Mexico},
	title = {End-to-{End} {Imitation} {Learning} with {Safety} {Guarantees} using {Control} {Barrier} {Functions}},
	isbn = {978-1-66546-761-2},
	doi = {10.1109/CDC51059.2022.9993193},
	abstract = {Imitation learning (IL) is a learning paradigm which can be used to synthesize controllers for complex systems that mimic behavior demonstrated by an expert (user or control algorithm). Despite their popularity, IL methods generally lack guarantees of safety, which limits their utility for complex safety-critical systems. In this work we consider safety, formulated as set-invariance, and the associated formal guarantees endowed by Control Barrier Functions (CBFs). We develop conditions under which robustly-safe expert controllers, utilizing CBFs, can be used to learn end-to-end controllers (which we refer to as CBF-Compliant controllers) that have safety guarantees. These guarantees are presented from the perspective of input-to-state safety (ISSf) which considers safety in the context of disturbances, wherein it is shown that IL using robustly safe expert demonstrations results in ISSf with the disturbance directly related to properties of the learning problem. We demonstrate these safety guarantees in simulated vision-based end-to-end control of an inverted pendulum and a car driving on a track.},
	language = {en},
	urldate = {2023-12-10},
	booktitle = {2022 {IEEE} 61st {Conference} on {Decision} and {Control} ({CDC})},
	publisher = {IEEE},
	author = {Cosner, Ryan K. and Yue, Yisong and Ames, Aaron D.},
	month = dec,
	year = {2022},
	keywords = {correct},
	pages = {5316--5322},
}

@inproceedings{pinneri_extracting_2021,
	title = {Extracting {Strong} {Policies} for {Robotics} {Tasks} from {Zero}-{Order} {Trajectory} {Optimizers}},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Pinneri, Cristina and Sawant, Shambhuraj and Blaes, Sebastian and Martius, Georg},
	year = {2021},
}

@inproceedings{kordabad_bias_2023,
	title = {Bias {Correction} of {Discounted} {Optimal} {Steady}-{State} using {Cost} {Modification}},
	doi = {10.23919/ECC57647.2023.10178359},
	abstract = {In the literature of Economic Model Predictive Control (EMPC) and undiscounted Optimal Control Problem (OCP), the optimal steady-state point is an equilibrium point with the minimum stage cost. If the Economic MPC is discounted, this property does not hold, and the optimal steady-state point is not the same as the one obtained from the undiscounted EMPC. Therefore the discounted steady-state point does not yield minimum stage cost and has a bias with respect to the undiscounted one. In this paper, we propose a computationally inexpensive cost modification in the discounted MPC that results in the undiscounted optimal steady-state point, i.e., the steady-state point that leads to the best stage cost. Moreover, we show that this modification does not affect the closed-loop system behavior. We illustrate the proposed method in a numerical example.},
	urldate = {2024-03-25},
	booktitle = {2023 {European} {Control} {Conference} ({ECC})},
	author = {Kordabad, Arash Bahari and Gros, Sebastien},
	month = jun,
	year = {2023},
	keywords = {Closed loop systems, Costs, Optimal control, Steady-state, Taylor series, Thermal stability, Trajectory},
	pages = {1--6},
}

@inproceedings{kordabad_bias_2021,
	title = {Bias {Correction} in {Deterministic} {Policy} {Gradient} {Using} {Robust} {MPC}},
	doi = {10.23919/ECC54610.2021.9654962},
	abstract = {In this paper, we discuss the deterministic policy gradient using the Actor-Critic methods based on the linear compatible advantage function approximator, where the input spaces are continuous. When the policy is restricted by hard constraints, the exploration may not be Centred or Isotropic (non-CI). As a result, the policy gradient estimation can be biased. We focus on constrained policies based on Model Predictive Control (MPC) schemes and to address the bias issue, we propose an approximate Robust MPC approach accounting for the exploration. The RMPC-based policy ensures that a Centered and Isotropic (CI) exploration is approximately feasible. A posterior projection is used to ensure its exact feasibility, we formally prove that this approach does not bias the gradient estimation.},
	urldate = {2024-03-25},
	booktitle = {2021 {European} {Control} {Conference} ({ECC})},
	author = {Kordabad, Arash Bahari and Nejatbakhsh Esfahani, Hossein and Gros, Sebastien},
	month = jun,
	year = {2021},
	keywords = {Aerospace electronics, Estimation, Europe, Function approximation, Predictive control},
	pages = {1086--1091},
}

@article{gros_data-driven_2020,
	title = {Data-{Driven} {Economic} {NMPC} {Using} {Reinforcement} {Learning}},
	volume = {65},
	issn = {1558-2523},
	doi = {10.1109/TAC.2019.2913768},
	abstract = {Reinforcement learning (RL) is a powerful tool to perform data-driven optimal control without relying on a model of the system. However, RL struggles to provide hard guarantees on the behavior of the resulting control scheme. In contrast, nonlinear model predictive control (NMPC) and economic NMPC (ENMPC) are standard tools for the closed-loop optimal control of complex systems with constraints and limitations, and benefit from a rich theory to assess their closed-loop behavior. Unfortunately, the performance of (E)NMPC hinges on the quality of the model underlying the control scheme. In this paper, we show that an (E)NMPC scheme can be tuned to deliver the optimal policy of the real system even when using a wrong model. This result also holds for real systems having stochastic dynamics. This entails that ENMPC can be used as a new type of function approximator within RL. Furthermore, we investigate our results in the context of ENMPC and formally connect them to the concept of dissipativity, which is central for the ENMPC stability. Finally, we detail how these results can be used to deploy classic RL tools for tuning (E)NMPC schemes. We apply these tools on both, a classical linear MPC setting and a standard nonlinear example, from the ENMPC literature.},
	number = {2},
	urldate = {2024-01-18},
	journal = {IEEE Transactions on Automatic Control},
	author = {Gros, Sébastien and Zanon, Mario},
	month = feb,
	year = {2020},
	pages = {636--648},
}

@article{kordabad_verification_2021,
	series = {7th {IFAC} {Conference} on {Nonlinear} {Model} {Predictive} {Control} {NMPC} 2021},
	title = {Verification of {Dissipativity} and {Evaluation} of {Storage} {Function} in {Economic} {Nonlinear} {MPC} using {Q}-{Learning}},
	volume = {54},
	issn = {2405-8963},
	doi = {10.1016/j.ifacol.2021.08.562},
	abstract = {In the Economic Nonlinear Model Predictive (ENMPC) context, closed-loop stability relates to the existence of a storage function satisfying a dissipation inequality. Finding the storage function is in general– for nonlinear dynamics and cost– challenging, and has attracted attentions recently. Q-Learning is a well-known Reinforcement Learning (RL) techniques that attempts to capture action-value functions based on the state-input transitions and stage cost of the system. In this paper, we present the use of the Q-Learning approach to obtain the storage function and verify the dissipativity for discrete-time systems subject to state-input constraints. We show that undiscounted Q-learning is able to capture the storage function for dissipative problems when the parameterization is rich enough. The efficiency of the proposed method will be illustrated in the different case studies.},
	number = {6},
	urldate = {2024-03-25},
	journal = {IFAC-PapersOnLine},
	author = {Kordabad, Arash Bahari and Gros, Sebastien},
	month = jan,
	year = {2021},
	keywords = {Dissipativity, Economic Nonlinear Model Predictive Control, Q-learning, Reinforcement Learning, Storage Function},
	pages = {308--313},
}

@inproceedings{kordabad_functional_2022,
	title = {Functional {Stability} of {Discounted} {Markov} {Decision} {Processes} {Using} {Economic} {MPC} {Dissipativity} {Theory}},
	doi = {10.23919/ECC55457.2022.9838064},
	abstract = {This paper discusses the functional stability of closed-loop Markov Chains under optimal policies resulting from a discounted optimality criterion, forming Markov Decision Processes (MDPs). We investigate the stability of MDPs in the sense of probability measures (densities) underlying the state distributions and extend the dissipativity theory of Economic Model Predictive Control in order to characterize the MDP stability. This theory requires a so-called storage function satisfying a dissipativity inequality. In the probability measures space and for the discounted setting, we introduce new dissipativity conditions ensuring the MDP stability. We then use finite-horizon optimal control problems in order to generate valid storage functionals. In practice, we propose to use Q-learning to compute the storage functionals.},
	urldate = {2024-03-25},
	booktitle = {2022 {European} {Control} {Conference} ({ECC})},
	author = {Kordabad, Arash Bahari and Gros, Sebastien},
	month = jul,
	year = {2022},
	keywords = {Costs, Density measurement, Europe, Markov processes, Optimal control, Q-learning, Stability criteria},
	pages = {1858--1863},
}

@article{kordabad_q-learning_2022,
	title = {Q-learning of the storage function in {Economic} {Nonlinear} {Model} {Predictive} {Control}},
	volume = {116},
	issn = {0952-1976},
	doi = {10.1016/j.engappai.2022.105343},
	abstract = {The closed-loop stability of an optimal policy provided by an Economic Nonlinear Model Predictive Control (ENMPC) scheme requires the existence of a storage function satisfying dissipativity conditions. Unfortunately, finding such a storage function is difficult in general. In contrast, tracking NMPC scheme uses a stage cost that is lower-bounded by a class-K∞ function and the closed-loop stability is fairly straightforward to establish. Under the dissipativity conditions, ENMPC has an equivalent tracking MPC that delivers the same optimal policy. In this paper, we use this idea and parameterize the stage cost and terminal cost of a tracking MPC with an additional parameterized storage function. We show that, if the parameterization of the tracking MPC scheme is rich enough to capture the exact optimal action-value function of the ENMPC scheme, then the parameterized storage function for the optimal parameters satisfies the dissipativity conditions for both discounted and undiscounted ENMPC schemes. In fact, we show that these conditions are met for dissipative problems. We propose to use Q-learning as a practical way of adjusting the parameters of the tracking MPC. Different numerical examples are provided to illustrate the efficiency of the proposed method, including LQR, non-dissipative, non-polynomial and a nonlinear chemical case studies. For instance, in the provided non-polynomial case study, the learning method can improve the storage function estimation by about 60\% and 99.5\% after 10 and 50 learning steps, respectively, compared with the Sum-of-Square method.},
	urldate = {2024-03-25},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Kordabad, Arash Bahari and Gros, Sebastien},
	month = nov,
	year = {2022},
	keywords = {Dissipativity, Economic Nonlinear Model Predictive Control, Q-learning, Reinforcement Learning, Storage function},
	pages = {105343},
}

@article{esfahani_learning-based_2023,
	title = {Learning-based state estimation and control using {MHE} and {MPC} schemes with imperfect models},
	volume = {73},
	issn = {0947-3580},
	doi = {10.1016/j.ejcon.2023.100880},
	abstract = {This paper presents a reinforcement learning-based observer/controller using Moving Horizon Estimation (MHE) and Model Predictive Control (MPC) schemes where the models used in the MHE-MPC cannot accurately capture the dynamics of the real system. We first show how an MHE cost modification can improve the performance of the MHE scheme such that a true state estimation is delivered even if the underlying MHE model is imperfect. A compatible Deterministic Policy Gradient (DPG) algorithm is then proposed to directly tune the parameters of both the estimator (MHE) and controller (MPC) in order to achieve the best closed-loop performance based on inaccurate MHE-MPC models. To demonstrate the effectiveness of the proposed learning-based estimator-controller, three numerical examples are illustrated.},
	urldate = {2024-03-25},
	journal = {European Journal of Control},
	author = {Esfahani, Hossein Nejatbakhsh and Bahari Kordabad, Arash and Cai, Wenqi and Gros, Sebastien},
	month = sep,
	year = {2023},
	keywords = {Imperfect models, Model predictive control, Moving horizon estimation, Reinforcement learning},
	pages = {100880},
}

@inproceedings{anand_painless_2023,
	title = {A {Painless} {Deterministic} {Policy} {Gradient} {Method} for {Learning}-based {MPC}},
	doi = {10.23919/ECC57647.2023.10178119},
	abstract = {The combination of Reinforcement Learning (RL) and Model Predictive Control (MPC) has gained a lot of interest in the recent literature as a way of computing the optimal policies from MPC schemes based on inaccurate models. In that context, the Deterministic Policy Gradient (DPG) methods are often observed to be the most reliable class of RL methods to improve the MPC closed-loop performance. The DPG methods are fairly easy to formulate when used with compatible function approximation as an advantage function. However, this formulation requires an additional value function approximation, often carried out using Deep Neural Networks (DNNs). In this paper, we propose to estimate the required value function approximation as a first-order expansion of the value function estimate from the MPC scheme providing the policy. The proposed approach drastically simplifies the use of DPG methods for learning-based MPC as no additional structure for approximating the value function needs to be constructed. We illustrate the proposed approach with two numerical examples of varying complexity.},
	booktitle = {European {Control} {Conference} ({ECC})},
	author = {Anand, Akhil S and Reinhardt, Dirk and Sawant, Shambhuraj and Gravdahl, Jan Tommy and Gros, Sebastien},
	month = jun,
	year = {2023},
	keywords = {Complexity theory, Computational modeling, Europe, Gradient methods, Pain, Predictive models, Reinforcement learning},
	pages = {1--7},
}

@article{seel_convex_2022,
	title = {Convex {Neural} {Network}-{Based} {Cost} {Modifications} for {Learning} {Model} {Predictive} {Control}},
	volume = {1},
	issn = {2694-085X},
	doi = {10.1109/OJCSYS.2022.3221063},
	abstract = {Developing model predictive control (MPC) schemes can be challenging for systems where an accurate model is not available, or too costly to develop. With the increasing availability of data and tools to treat them, learning-based MPC has of late attracted wide attention. It has recently been shown that adapting not only the MPC model, but also its cost function is conducive to achieving optimal closed-loop performance when an accurate model cannot be provided. In the learning context, this modification can be performed via parametrizing the MPC cost and adjusting the parameters via, e.g., reinforcement learning (RL). In this framework, simple cost parametrizations can be effective, but the underlying theory suggests that rich parametrizations in principle can be useful. In this paper, we propose such a cost parametrization using a class of neural networks (NNs) that preserves convexity. This choice avoids creating difficulties when solving the MPC problem via sensitivity-based solvers. In addition, this choice of cost parametrization ensures nominal stability of the resulting MPC scheme. Moreover, we detail how this choice can be applied to economic MPC problems where the cost function is generic and therefore does not necessarily fulfill any specific property.},
	urldate = {2024-03-25},
	journal = {IEEE Open Journal of Control Systems},
	author = {Seel, Katrine and Kordabad, Arash Bahari and Gros, Sébastien and Gravdahl, Jan Tommy},
	year = {2022},
	note = {Conference Name: IEEE Open Journal of Control Systems},
	keywords = {Cost function, Costs, Dissipativity, Numerical stability, Predictive control, Predictive models, Stability analysis, Standards, economic nonlinear model predictive control, neural networks, reinforcement learning},
	pages = {366--379},
}

@inproceedings{seel_combining_2023,
	title = {Combining {Q}-learning and {Deterministic} {Policy} {Gradient} for {Learning}-{Based} {MPC}},
	doi = {10.1109/CDC49753.2023.10383562},
	abstract = {This paper considers adjusting a fully parametrized model predictive control (MPC) scheme to approximate the optimal policy for a system as accurately as possible. By adopting MPC as a function approximator in reinforcement learning (RL), the MPC parameters can be adjusted using Q-learning or policy gradient methods. However, each method has its own specific shortcomings when used alone. Indeed, Q-learning does not exploit information about the policy gradient and therefore may fail to capture the optimal policy, while policy gradient methods miss any cost function corrections not affecting the policy directly. The former is a general problem, whereas the latter is an issue when dealing with economic problems specifically. Moreover, it is notoriously difficult to perform second-order steps in the context of policy gradient methods while it is straightforward in the context of Q-learning. This calls for an organic combination of these learning algorithms, in order to fully exploit the MPC parameterization as well as speed up convergence in learning.},
	urldate = {2024-05-16},
	booktitle = {2023 62nd {IEEE} {Conference} on {Decision} and {Control} ({CDC})},
	author = {Seel, Katrine and Gros, Sébastien and Gravdahl, Jan Tommy},
	month = dec,
	year = {2023},
	note = {ISSN: 2576-2370},
	keywords = {Convergence, Cost function, Deterministic policy gradient method, Economics, Gradient methods, Model predictive control, Predictive control, Predictive models, Q-learning, Reinforcement learning},
	pages = {610--617},
}

@inproceedings{bakker_reinforcement_2001,
	title = {Reinforcement {Learning} with {Long} {Short}-{Term} {Memory}},
	volume = {14},
	abstract = {This paper presents reinforcement learning with a Long Short(cid:173) Term Memory recurrent neural network: RL-LSTM. Model-free RL-LSTM using Advantage(,x) learning and directed exploration can solve non-Markovian tasks with long-term dependencies be(cid:173) tween relevant events. This is demonstrated in a T-maze task, as well as in a difficult variation of the pole balancing task.},
	urldate = {2024-05-24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Bakker, Bram},
	year = {2001},
}

@inproceedings{kalweit_deep_2020,
	title = {Deep {Inverse} {Q}-learning with {Constraints}},
	volume = {33},
	abstract = {Popular Maximum Entropy Inverse Reinforcement Learning approaches require the computation of expected state visitation frequencies for the optimal policy under an estimate of the reward function. This usually requires intermediate value estimation in the inner loop of the algorithm, slowing down convergence considerably. In this work, we introduce a novel class of algorithms that only needs to solve the MDP underlying the demonstrated behavior once to recover the expert policy. This is possible through a formulation that exploits a probabilistic behavior assumption for the demonstrations within the structure of Q-learning. We propose Inverse Action-value Iteration which is able to fully recover an underlying reward of an external agent in closed-form analytically. We further provide an accompanying class of sampling-based variants which do not depend on a model of the environment. We show how to extend this class of algorithms to continuous state-spaces via function approximation and how to estimate a corresponding action-value function, leading to a policy as close as possible to the policy of the external agent, while optionally satisfying a list of predefined hard constraints. We evaluate the resulting algorithms called Inverse Action-value Iteration, Inverse Q-learning and Deep Inverse Q-learning on the Objectworld benchmark, showing a speedup of up to several orders of magnitude compared to (Deep) Max-Entropy algorithms. We further apply Deep Constrained Inverse Q-learning on the task of learning autonomous lane-changes in the open-source simulator SUMO achieving competent driving after training on data corresponding to 30 minutes of demonstrations.},
	urldate = {2024-05-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Kalweit, Gabriel and Huegle, Maria and Werling, Moritz and Boedecker, Joschka},
	year = {2020},
	pages = {14291--14302},
}

@inproceedings{alboni_cacto-sl_2024,
	title = {{CACTO}-{SL}: {Using} {Sobolev} learning to improve continuous actor-critic with trajectory optimization},
	shorttitle = {{CACTO}-{SL}},
	abstract = {Trajectory Optimization (TO) and Reinforcement Learning (RL) are powerful and complementary tools to solve optimal control problems. On the one hand, TO can efficiently compute locally-optimal solutions, but it tends to get stuck in local minima if the problem is not convex. On the other hand, RL is typically less sensitive to non-convexity, but it requires a much higher computational effort. Recently, we have proposed CACTO (Continuous Actor-Critic with Trajectory Optimization), an algorithm that uses TO to guide the exploration of an actor-critic RL algorithm. In turns, the policy encoded by the actor is used to warm-start TO, closing the loop between TO and RL. In this work, we present an extension of CACTO exploiting the idea of Sobolev learning. To make the training of the critic network faster and more data efficient, we enrich it with the gradient of the Value function, computed via a backward pass of the differential dynamic programming algorithm. Our results show that the new algorithm is more efficient than the original CACTO, reducing the number of TO episodes by a factor ranging from 3 to 10, and consequently the computation time. Moreover, we show that CACTO-SL helps TO to find better minima and to produce more consistent results.},
	language = {en},
	urldate = {2024-07-09},
	booktitle = {Proceedings of the 6th {Annual} {Learning} for {Dynamics} \& {Control} {Conference}},
	publisher = {PMLR},
	author = {Alboni, Elisa and Grandesso, Gianluigi and Papini, Gastone Pietro Rosati and Carpentier, Justin and Prete, Andrea Del},
	month = jun,
	year = {2024},
	note = {ISSN: 2640-3498},
	pages = {1452--1463},
}

@article{brown_language_2020,
	title = {Language models are few-shot learners},
	volume = {33},
	journal = {Advances in neural information processing systems},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D. and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda},
	year = {2020},
	pages = {1877--1901},
}

@inproceedings{grudzien_mirror_2022,
	title = {Mirror learning: {A} unifying framework of policy optimisation},
	isbn = {2640-3498},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Grudzien, Jakub and De Witt, Christian A. Schroeder and Foerster, Jakob},
	year = {2022},
	pages = {7825--7844},
}

@inproceedings{finn_model-agnostic_2017,
	title = {Model-agnostic meta-learning for fast adaptation of deep networks},
	isbn = {2640-3498},
	booktitle = {International conference on machine learning},
	publisher = {PMLR},
	author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
	year = {2017},
	pages = {1126--1135},
}

@misc{reiter_ac4mpc_2024,
	title = {{AC4MPC}: {Actor}-{Critic} {Reinforcement} {Learning} for {Nonlinear} {Model} {Predictive} {Control}},
	shorttitle = {{AC4MPC}},
	doi = {10.48550/arXiv.2406.03995},
	abstract = {{\textbackslash}Ac\{MPC\} and {\textbackslash}ac\{RL\} are two powerful control strategies with, arguably, complementary advantages. In this work, we show how actor-critic {\textbackslash}ac\{RL\} techniques can be leveraged to improve the performance of {\textbackslash}ac\{MPC\}. The {\textbackslash}ac\{RL\} critic is used as an approximation of the optimal value function, and an actor roll-out provides an initial guess for primal variables of the {\textbackslash}ac\{MPC\}. A parallel control architecture is proposed where each {\textbackslash}ac\{MPC\} instance is solved twice for different initial guesses. Besides the actor roll-out initialization, a shifted initialization from the previous solution is used. Thereafter, the actor and the critic are again used to approximately evaluate the infinite horizon cost of these trajectories. The control actions from the lowest-cost trajectory are applied to the system at each time step. We establish that the proposed algorithm is guaranteed to outperform the original {\textbackslash}ac\{RL\} policy plus an error term that depends on the accuracy of the critic and decays with the horizon length of the {\textbackslash}ac\{MPC\} formulation. Moreover, we do not require globally optimal solutions for these guarantees to hold. The approach is demonstrated on an illustrative toy example and an {\textbackslash}ac\{AD\} overtaking scenario.},
	urldate = {2024-06-14},
	publisher = {arXiv},
	author = {Reiter, Rudolf and Ghezzi, Andrea and Baumgärtner, Katrin and Hoffmann, Jasper and McAllister, Robert D. and Diehl, Moritz},
	month = jun,
	year = {2024},
	keywords = {Computer Science - Artificial Intelligence, Electrical Engineering and Systems Science - Systems and Control},
}

@article{bishop_there_1975,
	title = {There is {More} than {One} {Way} to {Frame} a {Curve}},
	volume = {82},
	issn = {0002-9890},
	url = {https://www.jstor.org/stable/2319846},
	doi = {10.2307/2319846},
	number = {3},
	urldate = {2024-05-29},
	journal = {The American Mathematical Monthly},
	author = {Bishop, Richard L.},
	year = {1975},
	note = {Publisher: Mathematical Association of America},
	pages = {246--251},
}

@misc{salzmann2023learning,
	title = {Learning for {CasADi}: {Data}-driven models in numerical optimization},
	author = {Salzmann, Tim and Arrizabalaga, Jon and Andersson, Joel and Pavone, Marco and Ryll, Markus},
	year = {2023},
	note = {arXiv: 2312.05873 [eess.SY]},
}

@inproceedings{nasvytis_rethinking_2024,
	title = {Rethinking {Out}-of-{Distribution} {Detection} for {Reinforcement} {Learning}: {Advancing} {Methods} for {Evaluation} and {Detection}},
	booktitle = {Proceedings of the 23rd {International} {Conference} on {Autonomous} {Agents} and {Multiagent} {Systems}},
	author = {Nasvytis, Linas and Sandbrink, Kai and Foerster, Jakob and Franzmeyer, Tim and Schroeder de Witt, Christian},
	year = {2024},
	pages = {1445--1453},
}

@inproceedings{song_rapidly_2020,
	title = {Rapidly adaptable legged robots via evolutionary meta-learning},
	isbn = {1-72816-212-2},
	booktitle = {2020 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	publisher = {IEEE},
	author = {Song, Xingyou and Yang, Yuxiang and Choromanski, Krzysztof and Caluwaerts, Ken and Gao, Wenbo and Finn, Chelsea and Tan, Jie},
	year = {2020},
	pages = {3769--3776},
}

@inproceedings{lu_imitation_2023,
	title = {Imitation is not enough: {Robustifying} imitation with reinforcement learning for challenging driving scenarios},
	isbn = {1-66549-190-6},
	booktitle = {2023 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	publisher = {IEEE},
	author = {Lu, Yiren and Fu, Justin and Tucker, George and Pan, Xinlei and Bronstein, Eli and Roelofs, Rebecca and Sapp, Benjamin and White, Brandyn and Faust, Aleksandra and Whiteson, Shimon},
	year = {2023},
	pages = {7553--7560},
}

@inproceedings{anand_akhil_s_data-driven_2024,
	title = {Data-{Driven} {Predictive} {Control} and {MPC}: {Do} we achieve optimality?},
	author = {{Anand, Akhil S} and {Sawant, Shambhuraj} and {Reinhardt, Dirk} and {Gros, Sebastien}},
	year = {2024},
}

@inproceedings{tallec_making_2019,
	title = {Making deep q-learning methods robust to time discretization},
	isbn = {2640-3498},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Tallec, Corentin and Blier, Léonard and Ollivier, Yann},
	year = {2019},
	pages = {6096--6104},
}

@inproceedings{frank_reinforcement_2008,
	title = {Reinforcement learning in the presence of rare events},
	booktitle = {Proceedings of the 25th international conference on {Machine} learning},
	author = {Frank, Jordan and Mannor, Shie and Precup, Doina},
	year = {2008},
	pages = {336--343},
}

@article{smith_demonstrating_2023,
	title = {Demonstrating a walk in the park: {Learning} to walk in 20 minutes with model-free reinforcement learning},
	volume = {2},
	number = {3},
	journal = {Robotics: Science and Systems (RSS) Demo},
	author = {Smith, Laura and Kostrikov, Ilya and Levine, Sergey},
	year = {2023},
	pages = {4},
}

@article{zanon_stability-constrained_2022,
	title = {Stability-constrained {Markov} {Decision} {Processes} using {MPC}},
	volume = {143},
	issn = {0005-1098},
	doi = {10.1016/j.automatica.2022.110399},
	abstract = {In this paper, we consider solving discounted Markov Decision Processes (MDPs) under the constraint that the resulting policy is stabilizing. In practice MDPs are solved based on some form of policy approximation. We will leverage recent results proposing to use Model Predictive Control (MPC) as a structured approximator in the context of Reinforcement Learning, which makes it possible to introduce stability requirements directly inside the MPC-based policy. This will restrict the solution of the MDP to stabilizing policies by construction. Because the stability theory for MPC is most mature for the undiscounted MPC case, we will first show in this paper that stable discounted MDPs can be reformulated as undiscounted ones. This observation will entail that the undiscounted MPC-based policy with stability guarantees will produce the optimal policy for the discounted MDP if it is stable, and the best stabilizing policy otherwise.},
	urldate = {2024-01-18},
	journal = {Automatica},
	author = {Zanon, Mario and Gros, Sébastien and Palladino, Michele},
	month = sep,
	year = {2022},
	keywords = {Markov Decision Processes, Model Predictive Control, Safe reinforcement learning, Stability},
	pages = {110399},
}

@inproceedings{van_wingerden_data-enabled_2022,
	title = {Data-enabled predictive control with instrumental variables: the direct equivalence with subspace predictive control},
	doi = {10.1109/CDC51059.2022.9992824},
	abstract = {Direct data-driven control has attracted substantial interest since it enables optimization-based control without the need for a parametric model. This paper presents a new Instrumental Variable (IV) approach to Data-enabled Predictive Control (DeePC) that results in favorable noise mitigation properties, and demonstrates the direct equivalence between DeePC and Subspace Predictive Control (SPC). The methodology relies on the derivation of the characteristic equation in DeePC along the lines of subspace identification algorithms. A particular choice of IVs is presented that is uncorrelated with future noise, but at the same time highly correlated with the data matrix. A simulation study demonstrates the improved performance of the proposed algorithm in the presence of process and measurement noise.},
	booktitle = {{IEEE} 61st conference on decision and control ({CDC})},
	author = {van Wingerden, Jan-Willem and Mulders, Sebastiaan P. and Dinkla, Rogier and Oomen, Tom and Verhaegen, Michel},
	month = dec,
	year = {2022},
	note = {ISSN: 2576-2370},
	pages = {2111--2116},
}

@article{favoreel_spc_1999,
	series = {14th {IFAC} {World} {Congress} 1999, {Beijing}, {Chia}, 5-9 {July}},
	title = {{SPC}: {Subspace} {Predictive} {Control}},
	volume = {32},
	issn = {1474-6670},
	shorttitle = {{SPC}},
	doi = {10.1016/S1474-6670(17)56683-5},
	abstract = {Subspace identification has proven to be an excellent system identification method under peculiar industrial situations. Model predictive control on the other hand also turned out to be a very competitive method, especially in chemical industry. In practise, the identification and the control of a system are almost always considered as two separate problems. In the present paper some remarkable analogies between subspace identification and model predictive control are uncovered. Both methods can be combined in a very elegant way to form a numerically robust and easily implementable control/identification algorithm. This is the reason why we refer to it as subspace predictive control. The main result is that the system identification step and the controller design are done simultaneously. Starting from input and output measurements of the unknown system, only a QR-decomposition followed by a SV-decomposition are required to find the controller parameters.},
	language = {en},
	number = {2},
	urldate = {2022-09-27},
	journal = {IFAC Proceedings Volumes},
	author = {Favoreel, Wouter and Moor, Bart De and Gevers, Michel},
	month = jul,
	year = {1999},
	keywords = {Identification, LQG control method, Linear systems, Predictive control, Subspace methods},
	pages = {4004--4009},
}

@article{airaldi_learning_2023,
	series = {22nd {IFAC} {World} {Congress}},
	title = {Learning safety in model-based {Reinforcement} {Learning} using {MPC} and {Gaussian} {Processes}},
	volume = {56},
	issn = {2405-8963},
	url = {https://www.sciencedirect.com/science/article/pii/S2405896323009308},
	doi = {10.1016/j.ifacol.2023.10.563},
	abstract = {This paper proposes a method to encourage safety in Model Predictive Control (MPC)-based Reinforcement Learning (RL) via Gaussian Process (GP) regression. The framework consists of 1) a parametric MPC scheme that is employed as model-based controller with approximate knowledge on the real system's dynamics, 2) an episodic RL algorithm tasked with adjusting the MPC parametrization in order to increase its performance, and 3) GP regressors used to estimate, directly from data, constraints on the MPC parameters capable of predicting, up to some probability, whether the parametrization is likely to yield a safe or unsafe policy. These constraints are then enforced onto the RL updates in an effort to enhance the learning method with a probabilistic safety mechanism. Compared to other recent publications combining safe RL with MPC, our method does not require further assumptions on, e.g., the prediction model in order to retain computational tractability. We illustrate the results of our method in a numerical example on the control of a quadrotor drone in a safety-critical environment.},
	number = {2},
	urldate = {2024-03-25},
	journal = {IFAC-PapersOnLine},
	author = {Airaldi, Filippo and Schutter, Bart De and Dabiri, Azita},
	month = jan,
	year = {2023},
	keywords = {Gaussian Processes, Learning-based Model Predictive Control, Safe Reinforcement Learning},
	pages = {5759--5764},
}

@article{cybenko_approximation_1989,
	title = {Approximation by superpositions of a sigmoidal function},
	volume = {2},
	journal = {Mathematics of Control, Signals and Systems},
	author = {Cybenko, George V.},
	year = {1989},
	pages = {303--314},
}

@article{lambert_low-level_2019,
	title = {Low-{Level} {Control} of a {Quadrotor} {With} {Deep} {Model}-{Based} {Reinforcement} {Learning}},
	volume = {4},
	journal = {IEEE Robotics and Automation Letters},
	author = {Lambert, Nathan and Drew, Daniel S. and Yaconelli, Joseph and Levine, Sergey and Calandra, Roberto and Pister, Kristofer S. J.},
	year = {2019},
	pages = {4224--4230},
}

@inproceedings{bemporad_explicit_1999,
	address = {Sydney, Australia},
	title = {The explicit solution of constrained {LP}-{Based} receding horizon control},
	booktitle = {Proceedings of the {IEEE} conference on decision and control ({CDC})},
	author = {Bemporad, A. and Borrelli, F. and Morari, M.},
	year = {1999},
	keywords = {correct, optimal control robust hybrid realtime},
}

@inproceedings{neumann_variational_2011,
	address = {Bellevue, Washington, USA},
	title = {Variational inference for policy search in changing situations},
	isbn = {978-1-4503-0619-5},
	booktitle = {Proceedings of the 28th {International} {Conference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {Omnipress},
	author = {Neumann, Gerhard},
	year = {2011},
	pages = {817--824},
}

@inproceedings{kordabad_quasi-newton_2022,
	title = {Quasi-{Newton} {Iteration} in {Deterministic} {Policy} {Gradient}},
	doi = {10.23919/ACC53348.2022.9867217},
	abstract = {This paper presents a model-free approximation for the Hessian of the performance of deterministic policies to use in the context of Reinforcement Learning based on Quasi-Newton steps in the policy parameters. We show that the approximate Hessian converges to the exact Hessian at the optimal policy, and allows for a superlinear convergence in the learning, provided that the policy parametrization is rich. The natural policy gradient method can be interpreted as a particular case of the proposed method. We analytically verify the formulation in a simple linear case and compare the convergence of the proposed method with the natural policy gradient in a nonlinear example.},
	urldate = {2024-03-25},
	booktitle = {American {Control} {Conference} ({ACC})},
	author = {Kordabad, Arash Bahari and Nejatbakhsh Esfahani, Hossein and Cai, Wenqi and Gros, Sébastien},
	month = jun,
	year = {2022},
	note = {ISSN: 2378-5861},
	keywords = {Approximation algorithms, Context modeling, Convergence, Gradient methods, Optimization, Reinforcement learning},
	pages = {2124--2129},
}

@article{gros_learning_2022,
	title = {Learning for {MPC} with stability \& safety guarantees},
	volume = {146},
	issn = {0005-1098},
	doi = {10.1016/j.automatica.2022.110598},
	abstract = {The combination of learning methods with Model Predictive Control (MPC) has attracted a significant amount of attention in the recent literature. The hope of this combination is to reduce the reliance of MPC schemes on accurate models, and to tap into the fast developing machine learning and reinforcement learning tools to exploit the growing amount of data available for many systems. In particular, the combination of reinforcement learning and MPC has been proposed as a viable and theoretically justified approach to introduce explainable, safe and stable policies in reinforcement learning. However, a formal theory detailing how the safety and stability of an MPC-based policy can be maintained through the parameter updates delivered by the learning tools is still lacking. This paper addresses this gap. The theory is developed for the generic robust MPC case, and applied in simulation in the robust tube-based linear MPC case, where the theory is fairly easy to deploy in practice. The paper focuses on reinforcement learning as a learning tool, but it applies to any learning method that updates the MPC parameters online.},
	urldate = {2024-03-25},
	journal = {Automatica},
	author = {Gros, Sebastien and Zanon, Mario},
	month = dec,
	year = {2022},
	keywords = {Robust MPC, Safe MPC learning, Safe MPC-based policies, Safe reinforcement learning, Stability},
	pages = {110598},
}

@inproceedings{kordabad_reinforcement_2021,
	title = {Reinforcement {Learning} based on {Scenario}-tree {MPC} for {ASVs}},
	doi = {10.23919/ACC50511.2021.9483100},
	abstract = {In this paper, we present the use of Reinforcement Learning (RL) based on Robust Model Predictive Control (RMPC) for the control of an Autonomous Surface Vehicle (ASV). The RL-MPC strategy is utilized for obstacle avoidance and target (set-point) tracking. A scenario-tree robust MPC is used to handle potential failures of the ship thrusters. Besides, the wind and ocean current are considered as unknown stochastic disturbances in the real system, which are handled via constraints tightening. The tightening and other cost parameters are adjusted by RL, using a Q-Iearning technique. An economic cost is considered, minimizing the time and energy required to achieve the ship missions. The method is illustrated in simulation on a nonlinear 3-DOF model of a scaled version of the Cybership II.},
	urldate = {2024-03-25},
	booktitle = {2021 {American} {Control} {Conference} ({ACC})},
	author = {Kordabad, Arash Bahari and Esfahani, Hossein Nejatbakhsh and Lekkas, Anastasios M. and Gros, Sébastien},
	month = may,
	year = {2021},
	note = {ISSN: 2378-5861},
	keywords = {Attitude control, Economics, Oceans, Propellers, Reinforcement learning, Stochastic processes, Target tracking},
	pages = {1985--1990},
}

@article{gros_reinforcement_2020,
	series = {21st {IFAC} {World} {Congress}},
	title = {Reinforcement {Learning} for mixed-integer problems based on {MPC}},
	volume = {53},
	issn = {2405-8963},
	doi = {10.1016/j.ifacol.2020.12.1196},
	abstract = {Model Predictive Control has been recently proposed as policy approximation for Reinforcement Learning, offering a path towards safe and explainable Reinforcement Learning. This approach has been investigated for Q-learning and actor-critic methods, both in the context of nominal economic MPC and Robust (N)MPC, showing very promising results. In that context, actor-critic methods seem to be the most reliable approach. Many applications include a mixture of continuous and integer inputs, for which the classical actor-critic methods need to be adapted. In this paper, we present a policy approximation based on mixed-integer MPC schemes, and propose a computationally inexpensive technique to generate exploration in the mixed-integer input space that ensures a satisfaction of the constraints. We then propose a simple compatible advantage function approximation for the proposed policy, that allows one to build the gradient of the mixed-integer MPC-based policy.},
	number = {2},
	urldate = {2024-03-25},
	journal = {IFAC-PapersOnLine},
	author = {Gros, Sebastien and Zanon, Mario},
	month = jan,
	year = {2020},
	keywords = {Mixed-Integer Model Predictive Control, Reinforcement Learning, actor-critic methods, deterministic policy gradient, stochastic},
	pages = {5219--5224},
}

@inproceedings{levine_variational_2013,
	title = {Variational {Policy} {Search} via {Trajectory} {Optimization}},
	volume = {26},
	abstract = {In order to learn effective control policies for dynamical systems, policy search methods must be able to discover successful executions of the desired task. While random exploration can work well in simple domains, complex and high-dimensional tasks present a serious challenge, particularly when combined with high-dimensional policies that make parameter-space exploration infeasible. We present a method that uses trajectory optimization as a powerful exploration strategy that guides the policy search. A variational decomposition of a maximum likelihood policy objective allows us to use standard trajectory optimization algorithms such as differential dynamic programming, interleaved with standard supervised learning for the policy itself. We demonstrate that the resulting algorithm can outperform prior methods on two challenging locomotion tasks.},
	urldate = {2023-11-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Levine, Sergey and Koltun, Vladlen},
	year = {2013},
	keywords = {correct, optimal tracking},
}

@article{tearle_predictive_2021,
	title = {A {Predictive} {Safety} {Filter} for {Learning}-{Based} {Racing} {Control}},
	volume = {6},
	issn = {2377-3766},
	doi = {10.1109/LRA.2021.3097073},
	abstract = {The growing need for high-performance controllers in safety-critical applications like autonomous driving motivated the development of formal safety verification techniques. In this letter, we design and implement a predictive safety filter that is able to maintain vehicle safety with respect to track boundaries when paired alongside any potentially unsafe control signal, such as those found in learning-based methods. A model predictive control (MPC) framework is used to create a minimally invasive algorithm that certifies whether a desired control input is safe and can be applied to the vehicle, or that provides an alternate input to keep the vehicle in bounds. To this end, we provide a principled procedure to compute a safe and invariant set for nonlinear dynamic bicycle models using efficient convex approximation techniques. To fully support an aggressive racing performance without conservative safety interventions, the safe set is extended in real-time through predictive control backup trajectories. Applications for assisted manual driving and deep imitation learning on a miniature remote-controlled vehicle demonstrate the safety filter's ability to ensure vehicle safety during aggressive maneuvers.},
	number = {4},
	urldate = {2024-04-23},
	journal = {IEEE Robotics and Automation Letters},
	author = {Tearle, Ben and Wabersich, Kim P. and Carron, Andrea and Zeilinger, Melanie N.},
	month = oct,
	year = {2021},
	keywords = {Control systems, Nonlinear dynamical systems, Prediction algorithms, Predictive models, Robot safety, Safety, Task analysis, Trajectory, machine learning for control, optimization and optimal control},
	pages = {7635--7642},
}

@article{schwan_stability_2023,
	title = {Stability {Verification} of {Neural} {Network} {Controllers} {Using} {Mixed}-{Integer} {Programming}},
	issn = {1558-2523},
	doi = {10.1109/TAC.2023.3283213},
	abstract = {We propose a framework for the stability verification of Mixed-Integer Linear Programming (MILP) representable control policies. This framework compares a fixed candidate policy, which admits an efficient parameterization and can be evaluated at a low computational cost, against a fixed baseline policy, which is known to be stable but expensive to evaluate. We provide sufficient conditions for the closed-loop stability of the candidate policy in terms of the worst-case approximation error with respect to the baseline policy, and we show that these conditions can be checked by solving a Mixed-Integer Quadratic Program (MIQP). Additionally, we demonstrate that an outer and inner approximation of the stability region of the candidate policy can be computed by solving an MILP. The proposed framework is sufficiently general to accommodate a broad range of candidate policies including ReLU Neural Networks (NNs), optimal solution maps of parametric quadratic programs, and Model Predictive Control (MPC) policies. We also present an open-source toolbox in Python based on the proposed framework, which allows for the easy verification of custom NN architectures and MPC formulations. We showcase the flexibility and reliability of our framework in the context of a DC-DC power converter case study and investigate its computational complexity.},
	urldate = {2023-11-27},
	journal = {IEEE Transactions on Automatic Control},
	author = {Schwan, Roland and Jones, Colin N. and Kuhn, Daniel},
	year = {2023},
	note = {Conference Name: IEEE Transactions on Automatic Control},
	keywords = {correct},
	pages = {1--16},
}

@article{vaupel_accelerating_2020,
	title = {Accelerating nonlinear model predictive control through machine learning},
	volume = {92},
	issn = {0959-1524},
	doi = {10.1016/j.jprocont.2020.06.012},
	abstract = {The high computational requirements of nonlinear model predictive control (NMPC) are a long-standing issue and, among other methods, learning the control policy with machine learning (ML) methods has been proposed in order to improve computational tractability. However, these methods typically do not explicitly consider constraint satisfaction. We propose two methods based on learning the optimal control policy by an artificial neural network (ANN) and using this for initialization to accelerate computations while meeting constraints and achieving good objective function value. In the first, the ANN prediction serves as the initial guess for the solution of the optimal control problem (OCP) solved in NMPC. In the second, the ANN prediction is improved by solving a single quadratic program (QP). We compare the performance of the two proposed strategies against two benchmarks representing the extreme cases of (i) solving the NMPC problem to convergence using the shift-initialization strategy and (ii) implementing the controls predicted by the ANN prediction without further correction to reduce the computational delay. We find that the proposed ANN initialization strategy mostly results in the same control policy as the shift-initialization strategy. The computational times are on average ∼45\% longer but the maximum time is ∼42\% smaller and the distribution is tighter, thus more predictable. The proposed QP-based method yields a good compromise between finding the optimal control policy and solution time. Closed-loop infeasibilities are negligible and the objective function is typically greatly improved as compared to benchmark (ii). The computational time required for the necessary second-order sensitivity integration is typically an order of magnitude smaller than for solving the NMPC problem to convergence.},
	urldate = {2024-05-13},
	journal = {Journal of Process Control},
	author = {Vaupel, Yannic and Hamacher, Nils C. and Caspari, Adrian and Mhamdi, Adel and Kevrekidis, Ioannis G. and Mitsos, Alexander},
	month = aug,
	year = {2020},
	keywords = {cite},
	pages = {261--270},
}

@article{diehl_efficient_2004,
	title = {Efficient {NMPC} of unstable periodic systems using approximate infinite horizon closed loop costing},
	volume = {28},
	issn = {1367-5788},
	doi = {10.1016/j.arcontrol.2004.01.011},
	number = {1},
	journal = {Annual Reviews in Control},
	author = {Diehl, Moritz and Magni, Lalo and Nicolao, Giuseppe De},
	year = {2004},
	pages = {37--45},
}

@article{karg_efficient_2020,
	title = {Efficient {Representation} and {Approximation} of {Model} {Predictive} {Control} {Laws} via {Deep} {Learning}},
	volume = {50},
	issn = {2168-2275},
	doi = {10.1109/TCYB.2020.2999556},
	abstract = {We show that artificial neural networks with rectifier units as activation functions can exactly represent the piecewise affine function that results from the formulation of model predictive control (MPC) of linear time-invariant systems. The choice of deep neural networks is particularly interesting as they can represent exponentially many more affine regions compared to networks with only one hidden layer. We provide theoretical bounds on the minimum number of hidden layers and neurons per layer that a neural network should have to exactly represent a given MPC law. The proposed approach has a strong potential as an approximation method of predictive control laws, leading to a better approximation quality and significantly smaller memory requirements than previous approaches, as we illustrate via simulation examples. We also suggest different alternatives to correct or quantify the approximation error. Since the online evaluation of neural networks is extremely simple, the approximated controllers can be deployed on low-power embedded devices with small storage capacity, enabling the implementation of advanced decision-making strategies for complex cyber-physical systems with limited computing capabilities.},
	number = {9},
	urldate = {2024-01-18},
	journal = {IEEE Transactions on Cybernetics},
	author = {Karg, Benjamin and Lucia, Sergio},
	month = sep,
	year = {2020},
	pages = {3866--3878},
}

@inproceedings{ghezzi_imitation_2023,
	title = {Imitation {Learning} from {Nonlinear} {MPC} via the {Exact} {Q}-{Loss} and its {Gauss}-{Newton} {Approximation}},
	doi = {10.1109/CDC49753.2023.10383323},
	abstract = {This work presents a novel loss function for learning nonlinear Model Predictive Control policies via Imitation Learning. Standard approaches to Imitation Learning neglect information about the expert and generally adopt a loss function based on the distance between expert and learned controls. In this work, we present a loss based on the Q-function directly embedding the performance objectives and constraint satisfaction of the associated Optimal Control Problem (OCP). However, training a Neural Network with the Q-loss requires solving the associated OCP for each new sample. To alleviate the computational burden, we derive a second Q-loss based on the Gauss-Newton approximation of the OCP resulting in a faster training time. We validate our losses against Behavioral Cloning, the standard approach to Imitation Learning, on the control of a nonlinear system with constraints. The final results show that the Q-function-based losses significantly reduce the amount of constraint violations while achieving comparable or better closed-loop costs.},
	urldate = {2024-04-24},
	booktitle = {62nd {IEEE} {Conference} on {Decision} and {Control} ({CDC})},
	author = {Ghezzi, Andrea and Hoffman, Jasper and Frey, Jonathan and Boedecker, Joschka and Diehl, Moritz},
	month = dec,
	year = {2023},
	note = {ISSN: 2576-2370},
	keywords = {Behavioral sciences, Cloning, Costs, Nonlinear systems, Optimal control, Reinforcement learning, Training},
	pages = {4766--4771},
}

@book{powell_approximate_2007,
	title = {Approximate {Dynamic} {Programming}: {Solving} the curses of dimensionality},
	volume = {703},
	publisher = {John Wiley \& Sons},
	author = {Powell, Warren B},
	year = {2007},
}

@article{magni_stability_1997,
	title = {Stability margins of nonlinear receding-horizon control via inverse optimality},
	volume = {32},
	journal = {Systems \& Control Letters},
	author = {Magni, L. and Sepulchre, R.},
	year = {1997},
	keywords = {NMPC, control theory},
	pages = {241--245},
}

@article{nicolao_robustness_1996,
	title = {On the {Robustness} of {Receding}-{Horizon} {Control} with {Terminal} {Constraints}},
	volume = {41},
	number = {3},
	journal = {IEEE Trans. Automat. Control},
	author = {Nicolao, G. De and Magni, L. and Scattolini, R.},
	year = {1996},
}

@article{allan_inherent_2017,
	title = {On the inherent robustness of optimal and suboptimal nonlinear {MPC}},
	volume = {106},
	journal = {Systems \& Control Letters},
	author = {Allan, D. A. and Bates, C. N. and Risbeck, M. J. and Rawlings, J. B.},
	year = {2017},
	pages = {68--78},
}

@article{diehl_nominal_2005,
	title = {Nominal {Stability} of the {Real}-{Time} {Iteration} {Scheme} for {Nonlinear} {Model} {Predictive} {Control}},
	volume = {152},
	doi = {10.1049/ip-cta:20040008},
	number = {3},
	journal = {IEE Proc.-Control Theory Appl.},
	author = {Diehl, M. and Findeisen, R. and Allgöwer, F. and Bock, H. G. and Schlöder, J. P.},
	year = {2005},
	note = {Publisher: IEE},
	pages = {296--308},
}

@article{scokaert_suboptimal_1999,
	title = {Suboptimal {Model} {Predictive} {Control} ({Feasibility} {Implies} {Stability})},
	volume = {44},
	number = {3},
	journal = {IEEE Transactions on Automatic Control},
	author = {Scokaert, P. O. M. and Mayne, D. Q. and Rawlings, J.B.},
	year = {1999},
	pages = {648--654},
}

@article{mcallister_inherent_2024,
	title = {On the {Inherent} {Distributional} {Robustness} of {Stochastic} and {Nominal} {Model} {Predictive} {Control}},
	volume = {69},
	number = {2},
	journal = {IEEE Trans. Automat. Control},
	author = {McAllister, Robert D. and Rawlings, James B.},
	year = {2024},
}

@article{mcallister_inherent_2022,
	title = {Inherent {Stochastic} {Robustness} of {Model} {Predictive} {Control} to {Large} and {Infrequent} {Disturbances}},
	volume = {67},
	number = {10},
	journal = {IEEE Trans. Automat. Control},
	author = {McAllister, Robert D. and Rawlings, James B.},
	year = {2022},
}

@article{yu_inherent_2014,
	title = {Inherent robustness properties of quasi-infinite horizon nonlinear model predictive control},
	volume = {50},
	journal = {Automatica},
	author = {Yu, Shuyou and Reble, Marcus and Chen, Hong and Allgöwer, Frank},
	year = {2014},
}

@article{grimm_examples_2004,
	title = {Examples when nonlinear model predictive control is nonrobust},
	volume = {40},
	journal = {Automatica},
	author = {Grimm, G. and Messina, M. J. and Tuna, S. E. and Teel, A. R.},
	year = {2004},
	pages = {1729--1738},
}

@article{scokaert_discrete-time_1997,
	title = {Discrete-time {Stability} with {Perturbations}: {Application} to {Model} {Predictive} {Control}},
	volume = {33},
	number = {3},
	journal = {Automatica},
	author = {Scokaert, P.O.M. and Rawlings, J.B. and Meadows, E.S.},
	year = {1997},
	pages = {463--470},
}

@article{mayne_apologia_2013,
	title = {An apologia for stabilising terminal conditions in model predictive control},
	volume = {11},
	journal = {Internat. J. Control},
	author = {Mayne, David},
	year = {2013},
}

@article{grune_nmpc_2012,
	title = {{NMPC} without terminal constraints},
	volume = {45},
	number = {17},
	journal = {IFAC Proceedings Volumes},
	author = {Grüne, Lars},
	year = {2012},
	note = {Publisher: Elsevier},
	pages = {1--13},
}

@incollection{limon_input--state_2009,
	title = {Input-to-{State} {Stability}: {A} {Unifying} {Framework} for {Robust} {Model} {Predictive} {Control}},
	volume = {384},
	booktitle = {Nonlinear {Model} {Predictive} {Control}. {Lecture} {Notes} in {Control} and {Information} {Sciences}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Limon, D. and {others}},
	editor = {L. Magni, D. M. Raimondo, F. Allgöwer},
	year = {2009},
}

@article{lofberg_oops_2012,
	title = {Oops! {I} cannot do it again: {Testing} for recursive feasibility in {MPC}},
	volume = {48},
	number = {3},
	journal = {Automatica},
	author = {Löfberg, Johan},
	year = {2012},
}

@article{mayne_constrained_2000,
	title = {Constrained model predictive control: {Stability} and optimality},
	volume = {26},
	number = {6},
	journal = {Automatica},
	author = {Mayne, D. Q. and Rawlings, J. B. and Rao, C. V. and Scokaert, P. O. M.},
	year = {2000},
	pages = {789--814},
}

@phdthesis{kerrigan_robust_2000,
	type = {{PhD} {Thesis}},
	title = {Robust {Constraint} {Satisfaction}: {Invariant} {Sets} and {Predictive} {Control}},
	school = {University of Cambridge, UK},
	author = {Kerrigan, E. C.},
	year = {2000},
}

@article{lin_bounded-regret_2022,
	title = {Bounded-{Regret} {MPC} via {Perturbation} {Analysis}: {Prediction} {Error}, {Constraints}, and {Nonlinearity}},
	journal = {NeurIPS},
	author = {Lin, Yiheng and Hu, Yang and Qu, Guannan and Li, Tongxin and Wierman, Adam},
	year = {2022},
}

@article{mohamed_feedback_2022,
	title = {On the {Feedback} {Law} in {Stochastic} {Optimal} {Nonlinear} {Control}},
	journal = {Proceedings of the American Control Conference (ACC)},
	author = {Mohamed, Mohamed Naveed Gul and Chakravorty, Suman and Goyal, Raman and Wang, Ran},
	year = {2022},
}

@article{hadjiyiannis_efficient_2011,
	title = {An {Efficient} {Method} to {Estimate} the {Suboptimality} of {Affine} {Controllers}},
	volume = {56},
	number = {12},
	journal = {IEEE Trans. Automat. Control},
	author = {Hadjiyiannis, Michael J. and Goulart, Paul J. and Kuhn, Daniel},
	year = {2011},
}

@book{shapiro_lectures_2009,
	title = {Lectures on {Stochastic} {Programming}: {Modelling} and {Theory}},
	publisher = {SIAM},
	author = {Shapiro, A. and Dentcheva, D. and Ruszczynski, A.},
	year = {2009},
}

@article{li_performance_2023,
	title = {Performance {Bounds} of {Model} {Predictive} {Control} for {Unconstrained} and {Constrained} {Linear} {Quadratic} {Problems} and {Beyond}},
	journal = {Proceedings of the IFAC World Congress},
	author = {Li, Yuchao and Karapetyan, Aren and Lygeros, John and Johansson, Karl H. and Mårtensson, Jonas},
	year = {2023},
}

@book{nocedal_numerical_2006,
	edition = {2},
	series = {Springer {Series} in {Operations} {Research} and {Financial} {Engineering}},
	title = {Numerical {Optimization}},
	publisher = {Springer},
	author = {Nocedal, J. and Wright, S. J.},
	year = {2006},
}

@book{anderson_optimal_1990,
	title = {Optimal {Control} - {Linear} {Quadratic} {Methods}},
	publisher = {Dover},
	author = {Anderson, B. D. O. and Moore, J. B.},
	year = {1990},
}

@inproceedings{diehl_online_2003,
	address = {Bratislava, Slovak Republic},
	title = {Online {NMPC} of a looping kite using approximate infinite horizon closed loop costing},
	booktitle = {Proceedings of the {IFAC} {Conference} on {Control} {Systems} {Design}},
	publisher = {IFAC},
	author = {Diehl, M. and Magni, L. and Nicolao, G. D.},
	month = sep,
	year = {2003},
}

@inproceedings{nicolao_stabilizing_1996,
	address = {Lille},
	title = {Stabilizing nonlinear receding horizon control via a nonquadratic terminal state penalty},
	booktitle = {Symposium on {Control}, {Optimization} and {Supervision}, {CESA}'96 {IMACS} {Multiconference}},
	author = {Nicolao, G. De and Magni, L. and Scattolini, R.},
	year = {1996},
	pages = {185--187},
}

@article{nagy_open-loop_2004,
	title = {Open-loop and closed-loop robust optimal control of batch processes using distributional and worst-case analysis},
	volume = {14},
	journal = {Journal of Process Control},
	author = {Nagy, Z. K. and Braatz, R. D.},
	year = {2004},
	pages = {411--422},
}

@article{nicolao_stabilizing_1998,
	title = {Stabilizing {Receding}-{Horizon} control of nonlinear time varying systems},
	volume = {AC-43},
	number = {7},
	journal = {IEEE Transactions on Automatic Control},
	author = {Nicolao, G. De and Magni, L. and Scattolini, R.},
	year = {1998},
	pages = {1030--1036},
}

@article{goulart_optimization_2006,
	title = {Optimization over state feedback policies for robust control with constraints},
	volume = {42},
	journal = {Automatica},
	author = {Goulart, P. J. and Kerrigan, E. C. and Maciejowski, J. M.},
	year = {2006},
	pages = {523--533},
}

@inproceedings{messerer_efficient_2021,
	title = {An {Efficient} {Algorithm} for {Tube}-based {Robust} {Nonlinear} {Optimal} {Control} with {Optimal} {Linear} {Feedback}},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Decision} and {Control} ({CDC})},
	author = {Messerer, Florian and Diehl, Moritz},
	year = {2021},
}

@article{villanueva_configuration-constrained_2024,
	title = {Configuration-{Constrained} {Tube} {MPC}},
	volume = {163},
	journal = {Automatica},
	author = {Villanueva, Mario Eduardo and Müller, Matthias A. and Houska, Boris},
	year = {2024},
}

@article{calafiore_scenario_2006,
	title = {The {Scenario} {Approach} to {Robust} {Control} {Design}},
	journal = {IEEE Trans. Automat. Control},
	author = {Calafiore, Giuseppe C. and Campi, Marco C.},
	year = {2006},
}

@inproceedings{kouzoupis_first-order_2015,
	title = {First-{Order} {Methods} in {Embedded} {Nonlinear} {Model} {Predictive} {Control}},
	booktitle = {Proceedings of the {European} {Control} {Conference} ({ECC})},
	author = {Kouzoupis, D. and Ferreau, H. J. and Peyrl, H. and Diehl, M.},
	year = {2015},
	keywords = {syscop-public highwind},
	pages = {2617--2622},
}

@article{rakovic_parameterized_2012,
	title = {Parameterized {Tube} {Model} {Predictive} {Control}},
	volume = {57},
	issn = {0018-9286},
	doi = {10.1109/TAC.2012.2191174},
	number = {11},
	journal = {Automatic Control, IEEE Transactions on},
	author = {Rakovic, S.V. and Kouvaritakis, B. and Cannon, M. and Panos, C. and Findeisen, R.},
	month = nov,
	year = {2012},
	pages = {2746--2761},
}

@article{villanueva_robust_2017,
	title = {Robust {MPC} via min-max differential inequalities},
	volume = {77},
	journal = {Automatica},
	author = {Villanueva, M. E. and Quirynen, R. and Diehl, M. and Chachuat, B. and Houska, B.},
	month = mar,
	year = {2017},
	pages = {311--321},
}

@article{mayne_tube-based_2011,
	title = {Tube-based robust nonlinear model predictive control},
	volume = {21},
	journal = {International Journal of Robust and Nonlinear Control},
	author = {Mayne, D. and Kerrigan, E. and Wyk, E. J. van and Falugi, P.},
	year = {2011},
	pages = {1341--1353},
}

@book{grune_nonlinear_2017,
	edition = {2},
	title = {Nonlinear {Model} {Predictive} {Control}. {Theory} and {Algorithms}},
	publisher = {Springer},
	author = {Grüne, Lars and Pannek, Jürgen},
	year = {2017},
}

@article{jerez_embedded_2014,
	title = {Embedded online optimization for model predictive control at megahertz rates},
	volume = {59},
	number = {12},
	journal = {IEEE Transactions on Automatic Control},
	author = {Jerez, Juan L and Goulart, Paul J and Richter, Stefan and Constantinides, George A and Kerrigan, Eric C and Morari, Manfred},
	year = {2014},
	note = {Publisher: IEEE},
	pages = {3238--3251},
}

@article{messerer_survey_2021,
	title = {Survey of {Sequential} {Convex} {Programming} and {Generalized} {Gauss}-{Newton} {Methods}},
	volume = {71},
	journal = {ESAIM: Proceedings and Surveys},
	author = {Messerer, Florian and Baumgärtner, Katrin and Diehl, Moritz},
	year = {2021},
	keywords = {syscop-public},
	pages = {64--88},
}

@inproceedings{marti-saumell_squash-box_2020,
	title = {Squash-box feasibility driven differential dynamic programming},
	booktitle = {2020 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	publisher = {IEEE},
	author = {Marti-Saumell, Josep and Solà, Joan and Mastalli, Carlos and Santamaria-Navarro, Angel},
	year = {2020},
	pages = {7637--7644},
}

@article{patrinos_accelerated_2013,
	title = {An accelerated dual gradient-projection algorithm for embedded linear model predictive control},
	volume = {59},
	number = {1},
	journal = {IEEE Transactions on Automatic Control},
	author = {Patrinos, Panagiotis and Bemporad, Alberto},
	year = {2013},
	note = {Publisher: IEEE},
	pages = {18--33},
}

@article{osborne_shooting_1969,
	title = {On shooting methods for boundary value problems},
	volume = {27},
	journal = {Journal of Mathematical Analysis and Applications},
	author = {Osborne, M. R.},
	year = {1969},
	pages = {417--433},
}

@incollection{bock_recent_1983,
	title = {Recent {Advances} in {Parameter} {Identification} {Techniques} for {ODE}},
	booktitle = {Numerical {Treatment} of {Inverse} {Problems} in {Differential} and {Integral} {Equations}},
	publisher = {Birk{\textbackslash}-häu{\textbackslash}-ser},
	author = {Bock, H. G.},
	year = {1983},
	pages = {95--121},
}

@inproceedings{todorov_generalized_2005,
	title = {A generalized iterative {LQG} method for locally-optimal feedback control of constrained nonlinear stochastic systems},
	booktitle = {Proceedings of the {American} {Control} {Conference} ({ACC})},
	author = {Todorov, Emanuel and Li, Weiwei},
	year = {2005},
}

@techreport{shoemaker_proof_1990,
	title = {Proof of the quadratic convergence of differential dynamic programming},
	institution = {Cornell University Operations Research and Industrial Engineering},
	author = {Shoemaker, C and Liao, LZ},
	year = {1990},
}

@article{murray_differential_1984,
	title = {Differential {Dynamic} {Programming} and {Newton}'s {Method} for {Discrete} {Optimal} {Control} {Problems}},
	journal = {Journal of Optimization Theory and Applications},
	author = {Murray, D. M. and Yakowitz, S. J.},
	year = {1984},
	pages = {395--414},
}

@book{jacobson_differential_1970,
	series = {Modern {Analytic} and {Computational} {Methods} in {Science} and {Mathematics}},
	title = {Differential dynamic programming},
	volume = {24},
	publisher = {American Elsevier Pub. Co.},
	author = {Jacobson, D. H. and Mayne, D. Q.},
	year = {1970},
}

@article{mayne_second-order_1966,
	title = {A {Second}-order {Gradient} {Method} for {Determining} {Optimal} {Trajectories} of {Non}-linear {Discrete}-time {Systems}},
	volume = {3},
	number = {1},
	journal = {Int. J. Control},
	author = {Mayne, D.},
	year = {1966},
	pages = {85--96},
}

@inproceedings{li_iterative_2004,
	title = {Iterative {Linear} {Quadratic} {Regulator} {Design} for {Nonlinear} {Biological} {Movement} {Systems}},
	booktitle = {Proceedings of the 1st {International} {Conference} on {Informatics} in {Control}, {Automation} and {Robotics}},
	author = {Li, Weiwei and Todorov, Emanuel},
	year = {2004},
}

@article{rao_survey_2009,
	title = {A survey of numerical methods for optimal control},
	volume = {135},
	number = {1},
	journal = {Advances in the astronautical Sciences},
	author = {Rao, Anil V},
	year = {2009},
	note = {Publisher: Univelt, Inc.},
	pages = {497--528},
}

@article{chua_deep_2018,
	title = {Deep reinforcement learning in a handful of trials using probabilistic dynamics models},
	volume = {31},
	journal = {Advances in neural information processing systems},
	author = {Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
	year = {2018},
}

@article{rubinstein_optimization_1997,
	title = {Optimization of computer simulation models with rare events},
	volume = {99},
	number = {1},
	journal = {European Journal of Operational Research},
	author = {Rubinstein, Reuven Y},
	year = {1997},
	note = {Publisher: Elsevier},
	pages = {89--112},
}

@article{kobilarov_cross-entropy_2012,
	title = {Cross-entropy motion planning},
	volume = {31},
	number = {7},
	journal = {The International Journal of Robotics Research},
	author = {Kobilarov, Marin},
	year = {2012},
	note = {Publisher: SAGE Publications Sage UK: London, England},
	pages = {855--871},
}

@inproceedings{bharadhwaj_model-predictive_2020,
	title = {Model-predictive control via cross-entropy and gradient-based optimization},
	booktitle = {Learning for {Dynamics} and {Control}},
	publisher = {PMLR},
	author = {Bharadhwaj, Homanga and Xie, Kevin and Shkurti, Florian},
	year = {2020},
	pages = {277--286},
}

@inproceedings{nagabandi_deep_2020,
	title = {Deep dynamics models for learning dexterous manipulation},
	booktitle = {Conference on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Nagabandi, Anusha and Konolige, Kurt and Levine, Sergey and Kumar, Vikash},
	year = {2020},
	pages = {1101--1112},
}

@phdthesis{kouzoupis_structure-exploiting_2019,
	type = {{PhD} {Thesis}},
	title = {Structure-exploiting numerical methods for tree-sparse optimal control problems},
	school = {University of Freiburg},
	author = {Kouzoupis, D.},
	year = {2019},
	keywords = {syscop-public},
}

@article{kouzoupis_dual_2019,
	title = {A dual {Newton} strategy for tree-sparse quadratic programs and its implementation in the open-source software {treeQP}},
	journal = {International Jounal of Robust and Nonlinear Control},
	author = {Kouzoupis, D. and Klintberg, E. and Frison, G. and Gros, S. and Diehl, M.},
	year = {2019},
	keywords = {syscop-public},
}

@inproceedings{piovesan_randomized_2009,
	title = {Randomized model predictive control for robot navigation},
	booktitle = {2009 {IEEE} {International} {Conference} on {Robotics} and {Automation}},
	publisher = {IEEE},
	author = {Piovesan, Jorge L and Tanner, Herbert G},
	year = {2009},
	pages = {94--99},
}

@incollection{magni_efficient_2009,
	series = {Lecture {Notes} in {Control} and {Information} {Sciences}},
	title = {Efficient {Numerical} {Methods} for {Nonlinear} {MPC} and {Moving} {Horizon} {Estimation}},
	volume = {384},
	booktitle = {Nonlinear model predictive control},
	publisher = {Springer},
	author = {Diehl, M. and Ferreau, H. J. and Haverbeke, N.},
	editor = {Magni, L. and Raimondo, M. D. and Allgöwer, F.},
	year = {2009},
	keywords = {Newton type methods, numerical optimal control, optec real-time optimization, structure exploitation syscop-public},
	pages = {391--417},
}

@article{kantas_sequential_2009,
	title = {Sequential {Monte} {Carlo} for model predictive control},
	journal = {Nonlinear model predictive control: Towards new challenging applications},
	author = {Kantas, Nikolas and Maciejowski, JM and Lecchini-Visintini, A},
	year = {2009},
	note = {Publisher: Springer},
	pages = {263--273},
}

@article{gros_linear_2016,
	title = {From {Linear} to {Nonlinear} {MPC}: bridging the gap via the {Real}-{Time} {Iteration}},
	journal = {International Journal of Control},
	author = {Gros, S. and Zanon, M. and Quirynen, R. and Bemporad, A. and Diehl, M.},
	year = {2016},
	keywords = {syscop-public highwind},
}

@inproceedings{klintberg_improved_2016,
	title = {An improved dual {Newton} strategy for scenario-tree {MPC}},
	booktitle = {{CDC}},
	author = {Klintberg, Emil and Dahl, John and Fredriksson, Jonas and Gros, Sebastien},
	year = {2016},
	pages = {3675--3681},
}

@article{steinbach_tree-sparse_2002,
	title = {Tree-{Sparse} {Convex} {Programs}},
	volume = {56},
	number = {3},
	journal = {Mathematical Methods of Operations Research},
	author = {Steinbach, Marc C.},
	year = {2002},
	keywords = {optimal control robust},
	pages = {347--376},
}

@inproceedings{baumgartner_unified_2023,
	title = {A {Unified} {Local} {Convergence} {Analysis} of {Differential} {Dynamic} {Programming}, {Direct} {Single} {Shooting}, and {Direct} {Multiple} {Shooting}},
	booktitle = {Proceedings of the {European} {Control} {Conference} ({ECC})},
	author = {Baumgärtner, Katrin and Messerer, Florian and Diehl, Moritz},
	year = {2023},
	keywords = {syscop-public},
}

@inproceedings{leeman_fast_2024,
	title = {Fast {System} {Level} {Synthesis}: {Robust} {Model} {Predictive} {Control} using {Riccati} {Recursions}},
	booktitle = {Proceedings of the {IFAC} {Conference} on {Nonlinear} {Model} {Predictive} {Control} ({NMPC})},
	author = {Leeman, Antoine P. and Köhler, Johannes and Messerer, Florian and Lahr, Amon and Diehl, Moritz and Zeilinger, Melanie N.},
	year = {2024},
}

@inproceedings{vanroye_fatrop_2023,
	title = {Fatrop: {A} fast constrained optimal control problem solver for robot trajectory optimization and control},
	booktitle = {2023 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	publisher = {IEEE},
	author = {Vanroye, Lander and Sathya, Ajay and De Schutter, Joris and Decré, Wilm},
	year = {2023},
	pages = {10036--10043},
}

@inproceedings{van_den_berg_motion_2017,
	title = {Motion planning under uncertainty using differential dynamic programming in belief space},
	booktitle = {Robotics research: {The} 15th international symposium {ISRR}},
	publisher = {Springer},
	author = {Van Den Berg, Jur and Patil, Sachin and Alterovitz, Ron},
	year = {2017},
	pages = {473--490},
}

@article{wurman_outracing_2022,
	title = {Outracing champion {Gran} {Turismo} drivers with deep reinforcement learning},
	volume = {602},
	journal = {Nature},
	author = {Wurman, Peter R. and Barrett, Samuel and Kawamoto, Kenta and MacGlashan, James and Subramanian, Kaushik and Walsh, Thomas J. and Capobianco, Roberto and Devlic, Alisa and Eckert, Franziska and Fuchs, Florian and Gilpin, Leilani and Khandelwal, Piyush and Kompella, Varun and Lin, HaoChih and MacAlpine, Patrick and Oller, Declan and Seno, Takuma and Sherstan, Craig and Thomure, Michael D. and Aghabozorgi, Houmehr and Barrett, Leon and Douglas, Rory and Whitehead, Dion and Dürr, Peter and Stone, Peter and Spranger, Michael and Kitano, Hiroaki},
	year = {2022},
	pages = {223 -- 228},
}

@book{dantzig_simplex_1956,
	address = {Santa Monica, CA},
	title = {The {Simplex} {Method}.},
	publisher = {RAND Corporation},
	author = {Dantzig, George Bernard},
	year = {1956},
}

@article{schwenzer_review_2021,
	title = {Review on model predictive control: an engineering perspective},
	volume = {117},
	journal = {The International Journal of Advanced Manufacturing Technology},
	author = {Schwenzer, Max and Ay, Muzaffer and Bergs, Thomas and Abel, Dirk},
	year = {2021},
	pages = {1327 -- 1349},
}

@article{goulart_efficient_2008,
	title = {Efficient robust optimization for robust control with constraints},
	volume = {114},
	number = {1},
	journal = {Mathematical Programming},
	author = {Goulart, Paul J and Kerrigan, Eric C and Ralph, Daniel},
	year = {2008},
	note = {Publisher: Springer},
	pages = {115--147},
}

@article{levine_end--end_2016,
	title = {End-to-end training of deep visuomotor policies},
	volume = {17},
	number = {39},
	journal = {Journal of Machine Learning Research},
	author = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
	year = {2016},
	note = {ISBN: 1533-7928},
	pages = {1--40},
}

@inproceedings{zhang_importance_2021,
	title = {On the importance of hyperparameter optimization for model-based reinforcement learning},
	isbn = {2640-3498},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Zhang, Baohe and Rajan, Raghu and Pineda, Luis and Lambert, Nathan and Biedenkapp, André and Chua, Kurtland and Hutter, Frank and Calandra, Roberto},
	year = {2021},
	pages = {4015--4023},
}

@article{hewing_cautious_2020,
	title = {Cautious {Model} {Predictive} {Control} {Using} {Gaussian} {Process} {Regression}},
	volume = {28},
	issn = {1558-0865},
	doi = {10.1109/TCST.2019.2949757},
	abstract = {Gaussian process (GP) regression has been widely used in supervised machine learning due to its flexibility and inherent ability to describe uncertainty in function estimation. In the context of control, it is seeing increasing use for modeling of nonlinear dynamical systems from data, as it allows the direct assessment of residual model uncertainty. We present a model predictive control (MPC) approach that integrates a nominal system with an additive nonlinear part of the dynamics modeled as a GP. We describe a principled way of formulating the chance-constrained MPC problem, which takes into account residual uncertainties provided by the GP model to enable cautious control. Using additional approximations for efficient computation, we finally demonstrate the approach in a simulation example, as well as in a hardware implementation for autonomous racing of remote-controlled race cars with fast sampling times of 20 ms, highlighting improvements with regard to both performance and safety over a nominal controller.},
	number = {6},
	urldate = {2024-05-06},
	journal = {IEEE Transactions on Control Systems Technology},
	author = {Hewing, Lukas and Kabzan, Juraj and Zeilinger, Melanie N.},
	month = nov,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Control Systems Technology},
	keywords = {Autonomous racing, Computational modeling, Data models, Gaussian processes, Gaussian processes (GPs), Kernel, Predictive control, Predictive models, Uncertainty, learning-based control, model learning, model predictive control (MPC)},
	pages = {2736--2743},
}

@inproceedings{coulson_data-enabled_2019,
	title = {Data-{Enabled} {Predictive} {Control}: {In} the {Shallows} of the {DeePC}},
	shorttitle = {Data-{Enabled} {Predictive} {Control}},
	doi = {10.23919/ECC.2019.8795639},
	abstract = {We consider the problem of optimal trajectory tracking for unknown systems. A novel data-enabled predictive control (DeePC) algorithm is presented that computes optimal and safe control policies using real-time feedback driving the unknown system along a desired trajectory while satisfying system constraints. Using a finite number of data samples from the unknown system, our proposed algorithm uses a behavioural systems theory approach to learn a non-parametric system model used to predict future trajectories. The DeePC algorithm is shown to be equivalent to the classical and widely adopted Model Predictive Control (MPC) algorithm in the case of deterministic linear time-invariant systems. In the case of nonlinear stochastic systems, we propose regularizations to the DeePC algorithm. Simulations are provided to illustrate performance and compare the algorithm with other methods.},
	urldate = {2024-05-06},
	booktitle = {18th {European} {Control} {Conference} ({ECC})},
	author = {Coulson, Jeremy and Lygeros, John and Dörfler, Florian},
	month = jun,
	year = {2019},
	pages = {307--312},
}

@inproceedings{di_natale_lessons_2022,
	title = {Lessons {Learned} from {Data}-{Driven} {Building} {Control} {Experiments}: {Contrasting} {Gaussian} {Process}-based {MPC}, {Bilevel} {DeePC}, and {Deep} {Reinforcement} {Learning}},
	shorttitle = {Lessons {Learned} from {Data}-{Driven} {Building} {Control} {Experiments}},
	doi = {10.1109/CDC51059.2022.9992445},
	abstract = {This manuscript offers the perspective of experimentalists on a number of modern data-driven techniques: model predictive control relying on Gaussian processes, adaptive data-driven control based on behavioral theory, and deep reinforcement learning. These techniques are compared in terms of data requirements, ease of use, computational burden, and robustness in the context of real-world applications. Our remarks and observations stem from a number of experimental investigations carried out in the field of building control in diverse environments, from lecture halls and apartment spaces to a hospital surgery center. The final goal is to support others in identifying what technique is best suited to tackle their own problems.},
	urldate = {2024-05-06},
	booktitle = {{IEEE} 61st {Conference} on {Decision} and {Control} ({CDC})},
	author = {Di Natale, Loris and Lian, Yingzhao and Maddalena, Emilio T. and Shi, Jicheng and Jones, Colin N.},
	month = dec,
	year = {2022},
	note = {ISSN: 2576-2370},
	keywords = {Aerospace electronics, Deep learning, Gaussian processes, Hospitals, Reinforcement learning, Robustness, Surgery},
	pages = {1111--1117},
}

@article{bohn_optimization_2023,
	title = {Optimization of the model predictive control meta-parameters through reinforcement learning},
	volume = {123},
	issn = {0952-1976},
	url = {https://www.sciencedirect.com/science/article/pii/S0952197623003950},
	doi = {10.1016/j.engappai.2023.106211},
	abstract = {Model predictive control (MPC) is increasingly being considered for control of fast systems and embedded applications. However, MPC has some significant challenges for such systems, such as its high computational complexity. Further, the MPC parameters must be tuned, which is largely a trial-and-error process that affects the control performance, the robustness, and the computational complexity of the controller to a high degree. This paper presents a multivariate optimization method based on reinforcement learning (RL) that automatically tunes the control algorithm’s parameters from data to achieve optimal closed-loop performance. The main contribution of our method is the inclusion of state-dependent optimization of the meta-parameters of MPC, i.e. parameters that are non-differentiable wrt. the MPC solution. Our control algorithm is based on an event-triggered MPC, where we learn when the MPC should be re-computed, and a dual-mode MPC and linear state feedback control law applied in between MPC computations. We formulate a novel mixture-distribution RL policy determining the meta-parameters of our control algorithm and show that with joint optimization we achieve improvements that do not present themselves with univariate optimization of the same parameters. We demonstrate our framework on the inverted pendulum control task, reducing the total computation time of the control system by 36\% while also improving the control performance by 18.4\%.},
	urldate = {2024-03-25},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Bøhn, Eivind and Gros, Sebastien and Moe, Signe and Johansen, Tor Arne},
	month = aug,
	year = {2023},
	keywords = {Event-triggered control, Linear quadratic regulator, Model predictive control, Reinforcement learning},
	pages = {106211},
}

@article{bohn_optimization_2021,
	series = {3rd {IFAC} {Conference} on {Modelling}, {Identification} and {Control} of {Nonlinear} {Systems} {MICNON} 2021},
	title = {Optimization of the {Model} {Predictive} {Control} {Update} {Interval} {Using} {Reinforcement} {Learning}⁎},
	volume = {54},
	issn = {2405-8963},
	url = {https://www.sciencedirect.com/science/article/pii/S2405896321017687},
	doi = {10.1016/j.ifacol.2021.10.362},
	abstract = {In control applications there is often a compromise that needs to be made with respect to the complexity and performance of the controller, and the computational resources that are available. For instance, the typical hardware platform in embedded control applications is a microcontroller with limited memory and processing power, and for battery powered applications the control system can account for a significant portion of the energy consumption. We propose a controller architecture in which the computational cost is explicitly optimized along with the control objective. This is achieved by a three-part architecture where a high-level, computationally expensive controller generates plans, which a computationally simpler controller executes by compensating for prediction errors, while a recomputation policy decides when the plan should be recomputed. In this paper, we employ model predictive control (MPC) as the high-level plan-generating controller, a linear state feedback controller as the simpler compensating controller, and reinforcement learning (RL) to learn the recomputation policy. Simulation results for the classic control task of balancing an inverted pendulum show that not only is the total processor time reduced by 60\% — the RL policy is even able to uncover a non-trivial synergistic relationship between the MPC and the state feedback controller - improving the control performance by 20\% over the MPC alone.},
	number = {14},
	urldate = {2024-03-25},
	journal = {IFAC-PapersOnLine},
	author = {Bøhn, Eivind and Gros, Sebastien and Moe, Signe and Johansen, Tor Arne},
	month = jan,
	year = {2021},
	keywords = {event-driven control, model predictive control, reinforcement learning},
	pages = {257--262},
}

@article{esfahani_policy_2022,
	series = {6th {IFAC} {Conference} on {Intelligent} {Control} and {Automation} {Sciences} {ICONS} 2022},
	title = {Policy {Gradient} {Reinforcement} {Learning} for {Uncertain} {Polytopic} {LPV} {Systems} based on {MHE}-{MPC}},
	volume = {55},
	issn = {2405-8963},
	url = {https://www.sciencedirect.com/science/article/pii/S2405896322010102},
	doi = {10.1016/j.ifacol.2022.07.599},
	abstract = {In this paper, we propose a learning-based Model Predictive Control (MPC) approach for the polytopic Linear Parameter-Varying (LPV) systems with inexact scheduling parameters (as exogenous signals with inexact bounds), where the Linear Time Invariant (LTI) models (vertices) captured by combinations of the scheduling parameters becomes wrong. We first propose to adopt a Moving Horizon Estimation (MHE) scheme to simultaneously estimate the convex combination vector and unmeasured states based on the observations and model matching error. To tackle the wrong LTI models used in both the MPC and MHE schemes, we then adopt a Policy Gradient (PG) Reinforcement Learning (RL) to learn both the estimator (MHE) and controller (MPC) so that the best closed-loop performance is achieved. The effectiveness of the proposed RL-based MHE/MPC design is demonstrated using an illustrative example.},
	number = {15},
	urldate = {2024-03-25},
	journal = {IFAC-PapersOnLine},
	author = {Esfahani, Hossein Nejatbakhsh and Gros, Sébastien},
	month = jan,
	year = {2022},
	keywords = {Model Predictive Control, Moving Horizon Estimation, Multi-Model Linear System, Polytopic LPV, Reinforcement Learning},
	pages = {1--6},
}

@inproceedings{esfahani_approximate_2021,
	title = {Approximate {Robust} {NMPC} using {Reinforcement} {Learning}},
	url = {https://ieeexplore.ieee.org/abstract/document/9655129},
	doi = {10.23919/ECC54610.2021.9655129},
	abstract = {We present a Reinforcement Learning-based Robust Nonlinear Model Predictive Control (RL-RNMPC) framework for controlling nonlinear systems in the presence of disturbances and uncertainties. An approximate Robust Nonlinear Model Predictive Control (RNMPC) of low computational complexity is used in which the state trajectory uncertainty is modelled via ellipsoids. Reinforcement Learning is then used in order to handle the ellipsoidal approximation and improve the closed-loop performance of the scheme by adjusting the MPC parameters generating the ellipsoids. The approach is tested on a simulated Wheeled Mobile Robot (WMR) tracking a desired trajectory while avoiding static obstacles.},
	urldate = {2024-03-25},
	booktitle = {2021 {European} {Control} {Conference} ({ECC})},
	author = {Esfahani, Hossein Nejatbakhsh and Kordabad, Arash Bahari and Gros, Sébastien},
	month = jun,
	year = {2021},
	keywords = {Approximation algorithms, Mobile robots, Predictive models, Regulators, Reinforcement learning, Trajectory, Uncertainty},
	pages = {132--137},
}

@inproceedings{kordabad_multi-agent_2021,
	title = {Multi-agent {Battery} {Storage} {Management} using {MPC}-based {Reinforcement} {Learning}},
	url = {https://ieeexplore.ieee.org/abstract/document/9659202},
	doi = {10.1109/CCTA48906.2021.9659202},
	abstract = {In this paper, we present the use of Model Predictive Control (MPC) based on Reinforcement Learning (RL) to find the optimal policy for a multi-agent battery storage system. A time-varying prediction of the power price and production-demand uncertainty are considered. We focus on optimizing an economic objective cost while avoiding very low or very high state of charge, which can damage the battery. We consider the bounded power provided by the main grid and the constraints on the power input and state of each agent. A parametrized MPC-scheme is used as a function approximator for the deterministic policy gradient method and RL optimizes the closed-loop performance by updating the parameters. Simulation results demonstrate that the proposed method is able to tackle the constraints and deliver the optimal policy.},
	urldate = {2024-03-25},
	booktitle = {2021 {IEEE} {Conference} on {Control} {Technology} and {Applications} ({CCTA})},
	author = {Kordabad, Arash Bahari and Cai, Wenqi and Gros, Sebastien},
	month = aug,
	year = {2021},
	note = {ISSN: 2768-0770},
	keywords = {Costs, Gradient methods, Power system dynamics, Reinforcement learning, Simulation, Storage management, Uncertainty},
	pages = {57--62},
}

@article{cai2023energy,
	title = {Energy management in residential microgrid using model predictive control-based reinforcement learning and {Shapley} value},
	volume = {119},
	issn = {0952-1976},
	url = {https://www.sciencedirect.com/science/article/pii/S0952197622007837},
	doi = {https://doi.org/10.1016/j.engappai.2022.105793},
	abstract = {This paper presents an Energy Management (EM) strategy for residential microgrid systems using Model Predictive Control (MPC)-based Reinforcement Learning (RL) and Shapley value. We construct a typical residential microgrid system that considers fluctuating spot-market prices, highly uncertain user demand and renewable generation, and collective peak power penalties. To optimize the benefits for all residential prosumers, the EM problem is formulated as a Cooperative Coalition Game (CCG). The objective is to first find an energy trading policy that reduces the collective economic cost (including spot-market cost and peak-power cost) of the residential coalition, and then to distribute the profits obtained through cooperation to all residents. An MPC-based RL approach, which compensates for the shortcomings of MPC and RL and benefits from the advantages of both, is proposed to reduce the monthly collective cost despite the system uncertainties. To determine the amount of monthly electricity bill each resident should pay, we transfer the cost distribution problem into a profit distribution problem. Then, the Shapley value approach is applied to equitably distribute the profits (i.e., cost savings) gained through cooperation to all residents based on the weighted average of their respective marginal contributions. Finally, simulations are performed on a three-household microgrid system located in Oslo, Norway, to validate the proposed strategy, where a real-world dataset of April 2020 is used. Simulation results show that the proposed MPC-based RL approach could effectively reduce the long-term economic cost by about 17.5\%, and the Shapley value method provides a solution for allocating the collective bills fairly.},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Cai, Wenqi and Kordabad, Arash Bahari and Gros, Sébastien},
	year = {2023},
	keywords = {Cooperative coalition game (CCG), Energy management(EM), Microgrid (MG), Model predictive control (MPC), Profit distribution, Reinforcement learning (RL), Shapley value},
	pages = {105793},
}

@inproceedings{cai2021mpcbased,
	title = {{MPC}-based reinforcement learning for a simplified freight mission of autonomous surface vehicles},
	doi = {10.1109/CDC45484.2021.9683750},
	booktitle = {2021 60th {IEEE} conference on decision and control ({CDC})},
	author = {Cai, Wenqi and Kordabad, Arash B. and Esfahani, Hossein N. and Lekkas, Anastasios M. and Gros, Sébastien},
	year = {2021},
	pages = {2990--2995},
}

@article{lin_reinforcement_2024,
	title = {Reinforcement {Learning}-{Based} {Model} {Predictive} {Control} for {Discrete}-{Time} {Systems}},
	volume = {35},
	issn = {2162-2388},
	doi = {10.1109/TNNLS.2023.3273590},
	abstract = {This article proposes a novel reinforcement learning-based model predictive control (RLMPC) scheme for discrete-time systems. The scheme integrates model predictive control (MPC) and reinforcement learning (RL) through policy iteration (PI), where MPC is a policy generator and the RL technique is employed to evaluate the policy. Then the obtained value function is taken as the terminal cost of MPC, thus improving the generated policy. The advantage of doing so is that it rules out the need for the offline design paradigm of the terminal cost, the auxiliary controller, and the terminal constraint in traditional MPC. Moreover, RLMPC proposed in this article enables a more flexible choice of prediction horizon due to the elimination of the terminal constraint, which has great potential in reducing the computational burden. We provide a rigorous analysis of the convergence, feasibility, and stability properties of RLMPC. Simulation results show that RLMPC achieves nearly the same performance as traditional MPC in the control of linear systems and exhibits superiority over traditional MPC for nonlinear ones.},
	number = {3},
	urldate = {2024-04-25},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Lin, Min and Sun, Zhongqi and Xia, Yuanqing and Zhang, Jinhui},
	month = mar,
	year = {2024},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Convergence, Costs, Discrete-time systems, Predictive control, Stability criteria, Sun, Task analysis, model predictive control, policy iteration (PI), reinforcement learning (RL)},
	pages = {3312--3324},
}

@inproceedings{bhardwaj_blending_2021,
	title = {Blending {MPC} \& {Value} {Function} {Approximation} for {Efficient}                   {Reinforcement} {Learning}},
	abstract = {Model-Predictive Control (MPC) is a powerful tool for controlling complex, real-world systems that uses a model to make predictions about future behavior. For each state encountered, MPC solves an online optimization problem to choose a control action that will minimize future cost. This is a surprisingly effective strategy, but real-time performance requirements warrant the use of simple models. If the model is not sufficiently accurate, then the resulting controller can be biased, limiting performance. We present a framework for improving on MPC with model-free reinforcement learning (RL). The key insight is to view MPC as constructing a series of local Q-function approximations. We show that by using a parameter \${\textbackslash}lambda\$, similar to the trace decay parameter in TD(\${\textbackslash}lambda\$), we can systematically trade-off learned value estimates against the local Q-function approximations. We present a theoretical analysis that shows how error from inaccurate models in MPC and value function estimation in RL can be balanced. We further propose an algorithm that changes \${\textbackslash}lambda\$ over time to reduce the dependence on MPC as our estimates of the value function improve, and test the efficacy our approach on challenging high-dimensional manipulation tasks with biased models in simulation. We demonstrate that our approach can obtain performance comparable with MPC with access to true dynamics even under severe model bias and is more sample efficient as compared to model-free RL.},
	language = {en},
	booktitle = {9th {International} {Conference} on {Learning} {Representations}, {ICLR}},
	author = {Bhardwaj, Mohak and Choudhury, Sanjiban and Boots, Byron},
	year = {2021},
}

@inproceedings{haarnoja_soft_2018,
	series = {Proceedings of machine learning research},
	title = {Soft actor-critic: {Off}-policy maximum entropy deep reinforcement learning with a stochastic actor},
	volume = {80},
	booktitle = {Proceedings of the 35th international conference on machine learning, {ICML}},
	publisher = {PMLR},
	author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
	editor = {Dy, Jennifer G. and Krause, Andreas},
	year = {2018},
	pages = {1856--1865},
}

@article{recht_tour_2019,
	title = {A {Tour} of {Reinforcement} {Learning}: {The} {View} from {Continuous} {Control}},
	volume = {2},
	issn = {2573-5144},
	shorttitle = {A {Tour} of {Reinforcement} {Learning}},
	doi = {10.1146/annurev-control-053018-023825},
	abstract = {This article surveys reinforcement learning from the perspective of optimization and control, with a focus on continuous control applications. It reviews the general formulation, terminology, and typical experimental implementations of reinforcement learning as well as competing solution paradigms. In order to compare the relative merits of various techniques, it presents a case study of the linear quadratic regulator (LQR) with unknown dynamics, perhaps the simplest and best-studied problem in optimal control. It also describes how merging techniques from learning theory and control can provide nonasymptotic characterizations of LQR performance and shows that these characterizations tend to match experimental behavior. In turn, when revisiting more complex applications, many of the observed phenomena in LQR persist. In particular, theory and experiment demonstrate the role and importance of models and the cost of generality in reinforcement learning algorithms. The article concludes with a discussion of some of the challenges in designing learning systems that safely and reliably interact with complex and uncertain environments and how tools from reinforcement learning and control might be combined to approach these challenges.},
	language = {en},
	number = {Volume 2, 2019},
	urldate = {2024-04-23},
	journal = {Annual Review of Control, Robotics, and Autonomous Systems},
	author = {Recht, Benjamin},
	month = may,
	year = {2019},
	note = {Publisher: Annual Reviews},
	pages = {253--279},
}

@inproceedings{tram_learning_2019,
	title = {Learning {When} to {Drive} in {Intersections} by {Combining} {Reinforcement} {Learning} and {Model} {Predictive} {Control}},
	doi = {10.1109/ITSC.2019.8916922},
	abstract = {In this paper, we propose a decision making algorithm intended for automated vehicles that negotiate with other possibly non-automated vehicles in intersections. The decision algorithm is separated into two parts: a high-level decision module based on reinforcement learning, and a low-level planning module based on model predictive control. Traffic is simulated with numerous predefined driver behaviors and intentions, and the performance of the proposed decision algorithm was evaluated against another controller. The results show that the proposed decision algorithm yields shorter training episodes and an increased performance in success rate compared to the other controller.},
	urldate = {2024-04-23},
	booktitle = {2019 {IEEE} {Intelligent} {Transportation} {Systems} {Conference} ({ITSC})},
	author = {Tram, Tommy and Batkovic, Ivo and Ali, Mohammad and Sjöberg, Jonas},
	month = oct,
	year = {2019},
	keywords = {Acceleration, Decision making, Learning (artificial intelligence), Prediction algorithms, Predictive models, Roads, Trajectory},
	pages = {3263--3268},
}

@article{wabersich_data-driven_2023,
	title = {Data-{Driven} {Safety} {Filters}: {Hamilton}-{Jacobi} {Reachability}, {Control} {Barrier} {Functions}, and {Predictive} {Methods} for {Uncertain} {Systems}},
	volume = {43},
	issn = {1941-000X},
	shorttitle = {Data-{Driven} {Safety} {Filters}},
	doi = {10.1109/MCS.2023.3291885},
	abstract = {Today’s control engineering problems exhibit an unprecedented complexity, with examples including the reliable integration of renewable energy sources into power grids [1], safe collaboration between humans and robotic systems [2], and dependable control of medical devices [3] offering personalized treatment [4]. In addition to compliance with safety criteria, the corresponding control objective is often multifaceted. It ranges from relatively simple stabilization tasks to unknown objective functions, which are, for example, accessible only through demonstrations from interactions between robots and humans [5]. Classical control engineering methods are, however, often based on stability criteria with respect to set points and reference trajectories, and they can therefore be challenging to apply in such unstructured tasks with potentially conflicting safety specifications [6, Secs. 3 and 6]. While numerous efforts have started to address these challenges, missing safety certificates often still prohibit the widespread application of innovative designs outside research environments. As described in “Summary,” this article presents safety filters and advanced data-driven enhancements as a flexible framework for overcoming these limitations by ensuring that safety requirements codified as static state constraints are satisfied under all physical limitations of the system.},
	number = {5},
	urldate = {2024-04-23},
	journal = {IEEE Control Systems Magazine},
	author = {Wabersich, Kim P. and Taylor, Andrew J. and Choi, Jason J. and Sreenath, Koushil and Tomlin, Claire J. and Ames, Aaron D. and Zeilinger, Melanie N.},
	month = oct,
	year = {2023},
	note = {Conference Name: IEEE Control Systems Magazine},
	keywords = {Control engineering, Data models, Filtering theory, Power grids, Predictive models, Reliability engineering, Renewable energy sources, Safety, Scalability, Stability criteria, Uncertain systems, Uncertainty},
	pages = {137--177},
}

@inproceedings{wabersich_linear_2018,
	title = {Linear {Model} {Predictive} {Safety} {Certification} for {Learning}-{Based} {Control}},
	doi = {10.1109/CDC.2018.8619829},
	abstract = {While it has been repeatedly shown that learning-based controllers can provide superior performance, they often lack of safety guarantees. This paper aims at addressing this problem by introducing a model predictive safety certification (MPSC) scheme for linear systems with additive disturbances. The scheme verifies safety of a proposed learning-based input and modifies it as little as necessary in order to keep the system within a given set of constraints. Safety is thereby related to the existence of a model predictive controller (MPC) providing a feasible trajectory towards a safe target set. A robust MPC formulation accounts for the fact that the model is generally uncertain in the context of learning, which allows for proving constraint satisfaction at all times under the proposed MPSC strategy. The MPSC scheme can be used in order to expand any potentially conservative set of safe states and we provide an iterative technique for enlarging the safe set. Finally, a practical data-based design procedure for MPSC is proposed using scenario optimization.},
	urldate = {2024-04-23},
	booktitle = {2018 {IEEE} {Conference} on {Decision} and {Control} ({CDC})},
	author = {Wabersich, Kim P. and Zeilinger, Melanie N.},
	month = dec,
	year = {2018},
	note = {ISSN: 2576-2370},
	keywords = {Additives, Linear systems, Optimization, Predictive models, Safety, Trajectory},
	pages = {7130--7135},
}

@inproceedings{lubars_combining_2021,
	address = {Indianapolis, IN, USA},
	title = {Combining {Reinforcement} {Learning} with {Model} {Predictive} {Control} for {On}-{Ramp} {Merging}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-72819-142-3},
	doi = {10.1109/ITSC48978.2021.9564954},
	abstract = {We consider the problem of designing an algorithm to allow a car to autonomously merge on to a highway from an on-ramp. Two broad classes of techniques have been proposed to solve motion planning problems in autonomous driving: Model Predictive Control (MPC) and Reinforcement Learning (RL). In this paper, we ﬁrst establish the strengths and weaknesses of state-of-the-art MPC and RL-based techniques through simulations. We show that the performance of the RL agent is worse than that of the MPC solution from the perspective of safety and robustness to out-of-distribution trafﬁc patterns, i.e., trafﬁc patterns which were not seen by the RL agent during training. On the other hand, the performance of the RL agent is better than that of the MPC solution when it comes to efﬁciency and passenger comfort. We subsequently present an algorithm which blends the model-free RL agent with the MPC solution and show that it provides better tradeoffs between all metrics – passenger comfort, efﬁciency, crash rate and robustness.},
	language = {en},
	urldate = {2024-04-24},
	booktitle = {2021 {IEEE} {International} {Intelligent} {Transportation} {Systems} {Conference} ({ITSC})},
	publisher = {IEEE},
	author = {Lubars, Joseph and Gupta, Harsh and Chinchali, Sandeep and Li, Liyun and Raja, Adnan and Srikant, R. and Wu, Xinzhou},
	month = sep,
	year = {2021},
	pages = {942--947},
}

@inproceedings{mordatch_combining_2014,
	title = {Combining the benefits of function approximation and trajectory optimization},
	volume = {10},
	isbn = {978-0-9923747-0-9},
	urldate = {2024-04-24},
	author = {Mordatch, Igor and Todorov, Emo},
	month = jul,
	year = {2014},
}

@inproceedings{watter_embed_2015,
	title = {Embed to {Control}: {A} {Locally} {Linear} {Latent} {Dynamics} {Model} for {Control} from {Raw} {Images}},
	volume = {28},
	shorttitle = {Embed to {Control}},
	url = {https://proceedings.neurips.cc/paper/2015/hash/a1afc58c6ca9540d057299ec3016d726-Abstract.html},
	abstract = {We introduce Embed to Control (E2C), a method for model learning and control of non-linear dynamical systems from raw pixel images. E2C consists of a deep generative model, belonging to the family of variational autoencoders, that learns to generate image trajectories from a latent space in which the dynamics is constrained to be locally linear. Our model is derived directly from an optimal control formulation in latent space, supports long-term prediction of image sequences and exhibits strong performance on a variety of complex control problems.},
	urldate = {2024-04-24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Watter, Manuel and Springenberg, Jost and Boedecker, Joschka and Riedmiller, Martin},
	year = {2015},
}

@article{baes_every_2008,
	title = {Every {Continuous} {Nonlinear} {Control} {System} {Can} be {Obtained} by {Parametric} {Convex} {Programming}},
	volume = {53},
	issn = {1558-2523},
	doi = {10.1109/TAC.2008.928131},
	abstract = {In this short note, we define parametric convex programming (PCP) in a slightly different manner than it is usually done by extending convexity not only to variables but also to the parameters, and we show that the widely applied model predictive control (MPC) technique is a particular case of PCP. The main result of the note is an answer to the inverse question of PCP: which feedback laws can be generated by PCP? By employing results of convex analysis, we provide a constructive proof-yet not computational-that allows us to conclude that every continuous feedback law can be obtained by PCP.},
	number = {8},
	urldate = {2024-04-23},
	journal = {IEEE Transactions on Automatic Control},
	author = {Baes, Michel and Diehl, Moritz and Necoara, Ion},
	month = sep,
	year = {2008},
	note = {Conference Name: IEEE Transactions on Automatic Control},
	keywords = {Circuit stability, Continuous feedback laws, Feedback, Hypercubes, Iterative methods, Linear matrix inequalities, Linear systems, Nonlinear control systems, Predictive control, Predictive models, Transmission line matrix methods, model predictive control (MPC), parametric convex programming (PCP)},
	pages = {1963--1967},
}

@article{williams_model_2017,
	title = {Model {Predictive} {Path} {Integral} {Control}: {From} {Theory} to {Parallel} {Computation}},
	volume = {40},
	doi = {10.2514/1.G001921},
	abstract = {In this paper, a model predictive path integral control algorithm based on a generalized importance sampling scheme is developed and parallel optimization via sampling is performed using a graphics processing unit. The proposed generalized importance sampling scheme allows for changes in the drift and diffusion terms of stochastic diffusion processes and plays a significant role in the performance of the model predictive control algorithm. The proposed algorithm is compared in simulation with a model predictive control version of differential dynamic programming on nonlinear systems. Finally, the proposed algorithm is applied on multiple vehicles for the task of navigating through a cluttered environment. The current simulations illustrate the efficiency and robustness of the proposed approach and demonstrate the advantages of computational frameworks that incorporate concepts from statistical physics, control theory, and parallelization against more traditional approaches of optimal control theory.},
	number = {2},
	journal = {Journal of Guidance, Control, and Dynamics},
	author = {Williams, Grady and Aldrich, Andrew and Theodorou, Evangelos A.},
	year = {2017},
	pages = {344--357},
}

@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	doi = {10.1038/NATURE14236},
	number = {7540},
	journal = {Nat.},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin A. and Fidjeland, Andreas and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	year = {2015},
	pages = {529--533},
}

@article{brunke_safe_2022,
	title = {Safe {Learning} in {Robotics}: {From} {Learning}-{Based} {Control} to {Safe} {Reinforcement} {Learning}},
	volume = {5},
	shorttitle = {Safe {Learning} in {Robotics}},
	doi = {10.1146/annurev-control-042920-020211},
	abstract = {The last half decade has seen a steep rise in the number of contributions on safe learning methods for real-world robotic deployments from both the control and reinforcement learning communities. This article provides a concise but holistic review of the recent advances made in using machine learning to achieve safe decision-making under uncertainties, with a focus on unifying the language and frameworks used in control theory and reinforcement learning research. It includes learning-based control approaches that safely improve performance by learning the uncertain dynamics, reinforcement learning approaches that encourage safety or robustness, and methods that can formally certify the safety of a learned control policy. As data- and learning-based robot control methods continue to gain traction, researchers must understand when and how to best leverage them in real-world scenarios where safety is imperative, such as when operating in close proximityto humans. We highlight some of the open challenges that will drive the field of robot learning in the coming years, and emphasize the need for realistic physics-based benchmarks to facilitate fair comparisons between control and reinforcement learning approaches.},
	number = {1},
	urldate = {2024-01-17},
	journal = {Annual Review of Control, Robotics, and Autonomous Systems},
	author = {Brunke, Lukas and Greeff, Melissa and Hall, Adam W. and Yuan, Zhaocong and Zhou, Siqi and Panerati, Jacopo and Schoellig, Angela P.},
	year = {2022},
	keywords = {adaptive control, benchmarks, learning-based control, machine learning, model predictive control, robot learning, robotics, robust control, safe learning, safe reinforcement learning},
	pages = {411--444},
}

@inproceedings{silver_deterministic_2014,
	series = {{JMLR} workshop and conference proceedings},
	title = {Deterministic policy gradient algorithms},
	volume = {32},
	booktitle = {Proceedings of the 31th international conference on machine learning, {ICML} 2014, beijing, china, 21-26 june 2014},
	publisher = {JMLR.org},
	author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin A.},
	year = {2014},
	pages = {387--395},
}

@article{oh_quantitative_2024,
	title = {Quantitative comparison of reinforcement learning and data-driven model predictive control for chemical and biological processes},
	volume = {181},
	issn = {0098-1354},
	doi = {10.1016/j.compchemeng.2023.108558},
	abstract = {As manufacturing processes transition towards digitalization, data-driven process control is emerging as a key area of interest in future artificial intelligence technology. A crucial aspect in implementing data-driven process control is “What should we learn from the data?”. In general, the data-driven control method can be categorized into two main approaches: Learning the model and learning the value. To assist in selecting the more suitable approach, this paper applies six different control methods, with three falling under each approach, to three distinct manufacturing process systems. The simulation results indicate that the model-learning approaches display higher data efficiency and exhibit lower variance in total cost. These methods prove to be particularly advantageous for addressing the regulation problems. Conversely, value-learning approaches show competitive potential in closed-loop identification and in managing economic cost problems. The remaining challenges associated with each technique are discussed, along with practical considerations for their implementation.},
	urldate = {2024-02-22},
	journal = {Computers \& Chemical Engineering},
	author = {Oh, Tae Hoon},
	month = feb,
	year = {2024},
	keywords = {Model predictive control, Optimal control, Process control, Reinforcement learning, System identification},
	pages = {108558},
}

@article{jenelten_dtc_2024,
	title = {{DTC}: {Deep} {Tracking} {Control}},
	volume = {9},
	shorttitle = {{DTC}},
	doi = {10.1126/scirobotics.adh5401},
	abstract = {Legged locomotion is a complex control problem that requires both accuracy and robustness to cope with real-world challenges. Legged systems have traditionally been controlled using trajectory optimization with inverse dynamics. Such hierarchical model-based methods are appealing because of intuitive cost function tuning, accurate planning, generalization, and, most importantly, the insightful understanding gained from more than one decade of extensive research. However, model mismatch and violation of assumptions are common sources of faulty operation. Simulation-based reinforcement learning, on the other hand, results in locomotion policies with unprecedented robustness and recovery skills. Yet, all learning algorithms struggle with sparse rewards emerging from environments where valid footholds are rare, such as gaps or stepping stones. In this work, we propose a hybrid control architecture that combines the advantages of both worlds to simultaneously achieve greater robustness, foot-placement accuracy, and terrain generalization. Our approach uses a model-based planner to roll out a reference motion during training. A deep neural network policy is trained in simulation, aiming to track the optimized footholds. We evaluated the accuracy of our locomotion pipeline on sparse terrains, where pure data-driven methods are prone to fail. Furthermore, we demonstrate superior robustness in the presence of slippery or deformable ground when compared with model-based counterparts. Last, we show that our proposed tracking controller generalizes across different trajectory optimization methods not seen during training. In conclusion, our work unites the predictive capabilities and optimality guarantees of online planning with the inherent robustness attributed to offline learning.},
	number = {86},
	urldate = {2024-03-11},
	journal = {Science Robotics},
	author = {Jenelten, Fabian and He, Junzhe and Farshidian, Farbod and Hutter, Marco},
	month = jan,
	year = {2024},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eadh5401},
}

@article{hoffmann_comparison_2024,
	title = {Comparison of {Reinforcement} {Learning} and {Model} {Predictive} {Control} for {Automated} {Generation} of {Optimal} {Control} for {Dynamic} {Systems} within a {Design} {Space} {Exploration} {Framework}},
	volume = {15},
	issn = {2185-0984, 2185-0992},
	doi = {10.20485/jsaeijae.15.1_19},
	abstract = {This work provides a study of methods for the automated derivation of control strategies for over-actuated systems. For this purpose, Reinforcement Learning (RL) and Model Predictive Control (MPC) approximating the solution of the Optimal Control Problem (OCP) are compared using the example of an over-actuated vehicle model executing an ISO Double Lane Change (DLC). This exemplary driving maneuver is chosen due to its critical vehicle dynamics for the comparison of algorithms in terms of control performance and possible automation within a design space exploration framework. The algorithms show reasonable control results for the goal of this study, although there are differences in terms of driving stability. While Model Predictive Control first requires the optimization of the trajectory, which should then be optimally tracked, RL may combine both in one step. In addition, manual effort required to adapt the OCP problem to new design variants for solving it with RL and MPC is evaluated and assessed with respect to its automation. As a result of this study, an Actor-Critic Reinforcement Learning method is recommended for the automated derivation of control strategies in the context of a design space exploration.},
	language = {en},
	number = {1},
	urldate = {2024-02-21},
	journal = {International Journal of Automotive Engineering},
	author = {Hoffmann, Patrick and Gorelik, Kirill and Ivanov, Valentin},
	year = {2024},
	pages = {19--26},
}

@article{wang_comparison_2023,
	title = {Comparison of reinforcement learning and model predictive control for building energy system optimization},
	volume = {228},
	issn = {1359-4311},
	doi = {10.1016/j.applthermaleng.2023.120430},
	abstract = {Advanced controls could enhance buildings’ energy efficiency and operational flexibility while guaranteeing the indoor comfort. The control performance of reinforcement learning (RL) and model predictive control (MPC) have been widely studied in the literature. However, in existing studies, the reinforcement learning and model predictive control are tested in separate environments, making it challenging to directly compare their performance. In this paper, RL and MPC controls are implemented and compared with traditional rule-based controls in an open-source virtual environment to control a heat pump system of a residential house. The RL controllers were developed with three widely-used algorithms: Deep Deterministic Policy Gradient (DDPG), Dueling Deep Q Networks (DDQN), and Soft Actor Critic (SAC), and the MPC controller was developed using reduced-order thermal resistance-capacity network model. The building optimization testing (BOPTEST) framework is employed as a standardized virtual building simulator to conduct this study. The test case BOPTEST Hydronic Heat Pump is selected for the assessment and benchmarking of the control performance, data efficiency, implementation efforts and computational demands of the RL and MPC controllers. The comparison results revealed that for the RL controllers, only the DDPG algorithm outperforms the baseline controller in both the typical and peak heating scenarios. The MPC controller is superior to the RL and baseline controllers in both two scenarios because it can take the best possible action based on the current system state even with a model that deviates to a certain degree from reality. The findings of this study shed light on the selection of advanced building controllers among two promising candidates: MPC and RL.},
	urldate = {2024-02-21},
	journal = {Applied Thermal Engineering},
	author = {Wang, Dan and Zheng, Wanfu and Wang, Zhe and Wang, Yaran and Pang, Xiufeng and Wang, Wei},
	month = jun,
	year = {2023},
	keywords = {BOPTEST, Building controls, Model predictive control, Reinforcement learning},
	pages = {120430},
}

@article{song_reaching_2023,
	title = {Reaching the limit in autonomous racing: {Optimal} control versus reinforcement learning},
	volume = {8},
	shorttitle = {Reaching the limit in autonomous racing},
	doi = {10.1126/scirobotics.adg1462},
	abstract = {A central question in robotics is how to design a control system for an agile mobile robot. This paper studies this question systematically, focusing on a challenging setting: autonomous drone racing. We show that a neural network controller trained with reinforcement learning (RL) outperformed optimal control (OC) methods in this setting. We then investigated which fundamental factors have contributed to the success of RL or have limited OC. Our study indicates that the fundamental advantage of RL over OC is not that it optimizes its objective better but that it optimizes a better objective. OC decomposes the problem into planning and control with an explicit intermediate representation, such as a trajectory, that serves as an interface. This decomposition limits the range of behaviors that can be expressed by the controller, leading to inferior control performance when facing unmodeled effects. In contrast, RL can directly optimize a task-level objective and can leverage domain randomization to cope with model uncertainty, allowing the discovery of more robust control responses. Our findings allowed us to push an agile drone to its maximum performance, achieving a peak acceleration greater than 12 times the gravitational acceleration and a peak velocity of 108 kilometers per hour. Our policy achieved superhuman control within minutes of training on a standard workstation. This work presents a milestone in agile robotics and sheds light on the role of RL and OC in robot control.},
	number = {82},
	urldate = {2024-03-14},
	journal = {Science Robotics},
	author = {Song, Yunlong and Romero, Angel and Müller, Matthias and Koltun, Vladlen and Scaramuzza, Davide},
	month = sep,
	year = {2023},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eadg1462},
}

@article{shi_model-based_2023,
	title = {Model-{Based} {Predictive} {Control} and {Reinforcement} {Learning} for {Planning} {Vehicle}-{Parking} {Trajectories} for {Vertical} {Parking} {Spaces}},
	volume = {23},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	doi = {10.3390/s23167124},
	abstract = {This paper proposes a vehicle-parking trajectory planning method that addresses the issues of a long trajectory planning time and difficult training convergence during automatic parking. The process involves two stages: finding a parking space and parking planning. The first stage uses model predictive control (MPC) for trajectory tracking from the initial position of the vehicle to the starting point of the parking operation. The second stage employs the proximal policy optimization (PPO) algorithm to transform the parking behavior into a reinforcement learning process. A four-dimensional reward function is set to evaluate the strategy based on a formal reward, guiding the adjustment of neural network parameters and reducing the exploration of invalid actions. Finally, a simulation environment is built for the parking scene, and a network framework is designed. The proposed method is compared with the deep deterministic policy gradient and double-delay deep deterministic policy gradient algorithms in the same scene. Results confirm that the MPC controller accurately performs trajectory-tracking control with minimal steering wheel angle changes and smooth, continuous movement. The PPO-based reinforcement learning method achieves shorter learning times, totaling only 30\% and 37.5\% of the deep deterministic policy gradient (DDPG) and twin-delayed deep deterministic policy gradient (TD3), and the number of iterations to reach convergence for the PPO algorithm with the introduction of the four-dimensional evaluation metrics is 75\% and 68\% shorter compared to the DDPG and TD3 algorithms, respectively. This study demonstrates the effectiveness of the proposed method in addressing a slow convergence and long training times in parking trajectory planning, improving parking timeliness.},
	language = {en},
	number = {16},
	urldate = {2024-03-14},
	journal = {Sensors},
	author = {Shi, Junren and Li, Kexin and Piao, Changhao and Gao, Jun and Chen, Lizhi},
	month = jan,
	year = {2023},
	note = {Number: 16
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {MPC, PPO, autoparking, reinforcement learning},
	pages = {7124},
}

@inproceedings{shen_reinforcement_2023,
	title = {Reinforcement {Learning} and {Distributed} {Model} {Predictive} {Control} for {Conflict} {Resolution} in {Highly} {Constrained} {Spaces}},
	doi = {10.1109/IV55152.2023.10186560},
	abstract = {This work presents a distributed algorithm for resolving cooperative multi-vehicle conflicts in highly constrained spaces. By formulating the conflict resolution problem as a Multi-Agent Reinforcement Learning (RL) problem, we can train a policy offline to drive the vehicles towards their destinations safely and efficiently in a simplified discrete environment. During the online execution, each vehicle first simulates the interaction among vehicles with the trained policy to obtain its strategy, which is used to guide the computation of a reference trajectory. A distributed Model Predictive Controller (MPC) is then proposed to track the reference while avoiding collisions. The preliminary results show that the combination of RL and distributed MPC has the potential to guide vehicles to resolve conflicts safely and smoothly while being less computationally demanding than the centralized approach.},
	urldate = {2024-01-18},
	booktitle = {{IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
	author = {Shen, Xu and Borrelli, Francesco},
	month = jun,
	year = {2023},
	note = {ISSN: 2642-7214},
	pages = {1--6},
}

@inproceedings{reiter_hierarchical_2023,
	title = {A {Hierarchical} {Approach} for {Strategic} {Motion} {Planning} in {Autonomous} {Racing}},
	doi = {10.23919/ECC57647.2023.10178143},
	abstract = {We present an approach for safe trajectory planning, where a strategic task related to autonomous racing is learned sample efficiently within a simulation environment. A high-level policy, represented as a neural network, outputs a reward specification that is used within the function of a parametric nonlinear model predictive controller. By including constraints and vehicle kinematics in the nonlinear program, we can guarantee safe and feasible trajectories related to the used model. Compared to classical reinforcement learning, our approach restricts the exploration to safe trajectories, starts with an excellent prior performance and yields complete trajectories that can be passed to a tracking lowest-level controller. We do not address the lowest-level controller in this work and assume perfect tracking of feasible trajectories. We show the superior performance of our algorithm on simulated racing tasks that include high-level decision-making. The vehicle learns to efficiently overtake slower vehicles and avoids getting overtaken by blocking faster ones.},
	urldate = {2024-01-16},
	booktitle = {European {Control} {Conference} ({ECC})},
	author = {Reiter, Rudolf and Hoffmann, Jasper and Boedecker, Joschka and Diehl, Moritz},
	month = jun,
	year = {2023},
	pages = {1--8},
}

@article{moreno-mora_predictive_2023,
	series = {22nd {IFAC} {World} {Congress}},
	title = {Predictive {Control} with {Learning}-{Based} {Terminal} {Costs} {Using} {Approximate} {Value} {Iteration}},
	volume = {56},
	issn = {2405-8963},
	doi = {10.1016/j.ifacol.2023.10.1320},
	abstract = {Stability under model predictive control (MPC) schemes is frequently ensured by terminal ingredients. Employing a (control) Lyapunov function as the terminal cost constitutes a common choice. Learning-based methods may be used to construct the terminal cost by relating it to, for instance, an infinite-horizon optimal control problem in which the optimal cost is a Lyapunov function. Value iteration, an approximate dynamic programming (ADP) approach, refers to one particular cost approximation technique. In this work, we merge the results of terminally unconstrained predictive control and approximate value iteration to draw benefits from both fields. A prediction horizon is derived in dependence on different factors, such as approximation-related errors, to render the closed-loop asymptotically stable further allowing a suboptimality estimate in comparison to an infinite-horizon optimal cost. The result extends recent studies on predictive control with ADP-based terminal costs, not requiring a local initial stabilizing controller. We compare this controller in simulation with other terminal cost options to show that the proposed approach leads to a shorter minimal horizon in comparison to previous results.},
	number = {2},
	urldate = {2024-04-15},
	journal = {IFAC-PapersOnLine},
	author = {Moreno-Mora, Francisco and Beckenbach, Lukas and Streif, Stefan},
	month = jan,
	year = {2023},
	keywords = {learning-based control, model predictive control, reinforcement learning},
	pages = {3874--3879},
}

@article{morcego_reinforcement_2023,
	title = {Reinforcement {Learning} versus {Model} {Predictive} {Control} on greenhouse climate control},
	volume = {215},
	issn = {0168-1699},
	doi = {10.1016/j.compag.2023.108372},
	abstract = {The greenhouse system plays a crucial role to ensure an adequate supply of fresh food for the growing global population. However, maintaining an optimal growing climate within a greenhouse requires resources and operational costs. To achieve economical and sustainable crop growth, efficient climate control in greenhouse production is paramount. Model Predictive Control (MPC) and Reinforcement Learning (RL) are the two approaches representing model-based and learning-based control, respectively. Each one has its own way to formulate control problems, define control objectives, and seek for optimal control actions that provide sustainable crop growth. Although certain forms of MPC and RL have been applied to greenhouse climate control, limited research has comprehensively analyzed the connections, differences, advantages, and disadvantages between these two approaches, both mathematically and in terms of performance. Therefore, this paper aims to address this gap by: (1) introducing a novel RL approach that utilizes Deep Deterministic Policy Gradient (DDPG) for large and continuous state–action space environments; (2) formulating the MPC and RL approaches for greenhouse climate control within a unified framework; (3) exploring the mathematical connections and differences between MPC and RL; (4) conducting a simulation study to analyze and compare the performance of MPC and RL; (5) presenting and interpreting the comparative results to provide valuable insights for the application of these control approaches in different scenarios. By undertaking these objectives, this paper seeks to contribute to the understanding and advancement of both MPC and RL methods in greenhouse climate control, fostering more informed decision-making regarding their selection and implementation based on specific requirements and constraints.},
	urldate = {2024-03-11},
	journal = {Computers and Electronics in Agriculture},
	author = {Morcego, Bernardo and Yin, Wenjie and Boersma, Sjoerd and van Henten, Eldert and Puig, Vicenç and Sun, Congcong},
	month = dec,
	year = {2023},
	keywords = {Greenhouse climate control, Model Predictive Control, Reinforcement Learning},
	pages = {108372},
}

@article{liu_learning_2023,
	title = {Learning to {Play} {Trajectory} {Games} {Against} {Opponents} {With} {Unknown} {Objectives}},
	volume = {8},
	issn = {2377-3766},
	doi = {10.1109/LRA.2023.3280809},
	abstract = {Many autonomous agents, such as intelligent vehicles, are inherently required to interact with one another. Game theory provides a natural mathematical tool for robot motion planning in such interactive settings. However, tractable algorithms for such problems usually rely on a strong assumption, namely that the objectives of all players in the scene are known. To make such tools applicable for ego-centric planning with only local information, we propose an adaptive model-predictive game solver, which jointly infers other players' objectives online and computes a corresponding generalized Nash equilibrium (GNE) strategy. The adaptivity of our approach is enabled by a differentiable trajectory game solver whose gradient signal is used for maximum likelihood estimation (MLE) of opponents' objectives. This differentiability of our pipeline facilitates direct integration with other differentiable elements, such as neural networks (NNs). Furthermore, in contrast to existing solvers for cost inference in games, our method handles not only partial state observations but also general inequality constraints. In two simulated traffic scenarios, we find superior performance of our approach over both existing game-theoretic methods and non-game-theoretic model-predictive control (MPC) approaches. We also demonstrate our approach's real-time planning capabilities and robustness in two-player hardware experiments.},
	number = {7},
	urldate = {2024-01-18},
	journal = {IEEE Robotics and Automation Letters},
	author = {Liu, Xinjie and Peters, Lasse and Alonso-Mora, Javier},
	month = jul,
	year = {2023},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	pages = {4139--4146},
}

@article{kaufmann_champion-level_2023,
	title = {Champion-level drone racing using deep reinforcement learning},
	volume = {620},
	copyright = {2023 The Author(s)},
	issn = {1476-4687},
	doi = {10.1038/s41586-023-06419-4},
	abstract = {First-person view (FPV) drone racing is a televised sport in which professional competitors pilot high-speed aircraft through a 3D circuit. Each pilot sees the environment from the perspective of their drone by means of video streamed from an onboard camera. Reaching the level of professional pilots with an autonomous drone is challenging because the robot needs to fly at its physical limits while estimating its speed and location in the circuit exclusively from onboard sensors1. Here we introduce Swift, an autonomous system that can race physical vehicles at the level of the human world champions. The system combines deep reinforcement learning (RL) in simulation with data collected in the physical world. Swift competed against three human champions, including the world champions of two international leagues, in real-world head-to-head races. Swift won several races against each of the human champions and demonstrated the fastest recorded race time. This work represents a milestone for mobile robotics and machine intelligence2, which may inspire the deployment of hybrid learning-based solutions in other physical systems.},
	language = {en},
	number = {7976},
	urldate = {2024-01-18},
	journal = {Nature},
	author = {Kaufmann, Elia and Bauersfeld, Leonard and Loquercio, Antonio and Müller, Matthias and Koltun, Vladlen and Scaramuzza, Davide},
	month = aug,
	year = {2023},
	note = {Number: 7976
Publisher: Nature Publishing Group},
	keywords = {Aerospace engineering, Computer science, Electrical and electronic engineering, Mechanical engineering},
	pages = {982--987},
}

@article{kang_rl_2023,
	title = {{RL} + {Model}-{Based} {Control}: {Using} {On}-{Demand} {Optimal} {Control} to {Learn} {Versatile} {Legged} {Locomotion}},
	volume = {8},
	issn = {2377-3766, 2377-3774},
	shorttitle = {{RL} + {Model}-{Based} {Control}},
	doi = {10.1109/LRA.2023.3307008},
	abstract = {This paper presents a control framework that combines model-based optimal control and reinforcement learning (RL) to achieve versatile and robust legged locomotion. Our approach enhances the RL training process by incorporating on-demand reference motions generated through finite-horizon optimal control, covering a broad range of velocities and gaits. These reference motions serve as targets for the RL policy to imitate, leading to the development of robust control policies that can be learned with reliability. Furthermore, by utilizing realistic simulation data that captures whole-body dynamics, RL effectively overcomes the inherent limitations in reference motions imposed by modeling simplifications. We validate the robustness and controllability of the RL training process within our framework through a series of experiments. In these experiments, our method showcases its capability to generalize reference motions and effectively handle more complex locomotion tasks that may pose challenges for the simplified model, thanks to RL’s flexibility. Additionally, our framework effortlessly supports the training of control policies for robots with diverse dimensions, eliminating the necessity for robot-specific adjustments in the reward function and hyperparameters.},
	language = {en},
	number = {10},
	urldate = {2024-03-11},
	journal = {IEEE Robotics and Automation Letters},
	author = {Kang, Dongho and Cheng, Jin and Zamora, Miguel and Zargarbashi, Fatemeh and Coros, Stelian},
	month = oct,
	year = {2023},
	pages = {6619--6626},
}

@inproceedings{imran_comparison_2023,
	title = {Comparison of {Traffic} {Control} with {Model} {Predictive} {Control} and {Deep} {Reinforcement} {Learning}},
	doi = {10.1109/CoDIT58514.2023.10284162},
	abstract = {Traffic congestion is among the worst causes of pollution, and the time spent in traffic can cost the world tens of billions of dollars every year. Solutions to mitigate this problem are at hand thanks to the advent of advanced control techniques and artificial intelligence (AI). Traditional traffic light control strategies based on fixed timing of the green, yellow and red phases are simple to implement, but at the same time very inefficient, in particular for busy intersections. This paper discusses both a model predictive control (MPC) approach and a model-free deep reinforcement learning (DRL) algorithm for controlling the traffic lights at a single intersection, with the aim of improving the traffic flow. Firstly, a detailed linear mathematical model of an intersection is formulated and successively tested in a MPC framework; secondly, a DRL algorithm is proposed and verified by comparing it with the currently implemented baseline controller. Finally, the results for the three approaches, MPC, DRL and the baseline controller, are validated through the SUMO (Simulation of Urban Mobility) microscopic traffic simulator.},
	urldate = {2024-02-21},
	booktitle = {9th {International} {Conference} on {Control}, {Decision} and {Information} {Technologies} ({CoDIT})},
	author = {Imran, Muhammad and Izzo, Riccardo and Tortorelli, Andrea and Liberati, Francesco},
	month = jul,
	year = {2023},
	note = {ISSN: 2576-3555},
	keywords = {Artificial intelligence, Deep learning, Mathematical models, Prediction algorithms, Reinforcement learning, Timing, Traffic control},
	pages = {989--994},
}

@article{hu_model_2023,
	title = {Model predictive optimization for imitation learning from demonstrations},
	volume = {163},
	issn = {0921-8890},
	doi = {10.1016/j.robot.2023.104381},
	abstract = {“Motion generation by imitating” enables a robot to generate its trajectory in a new environment. Research works on dynamic movement primitives (DMP) has reported promising results, with good imitation effect and convergence to the target. However, DMP still has issues such as learning from multiple demonstrations for different initial conditions and achieving obstacle avoidance considering the distribution and motion of obstacles. One of the effective solutions is combining DMP and model predictive control (MPC). The imitation process was transformed into a receding horizon planning procedure, letting the robot to learn more from nearer demonstrations. It is solved as an optimization problem with obstacles modeled as constraints. However, its drawback includes the heavy computation burden, which can be even aggravated in a multi-obstacle scenario where complicated constraints occur. Thus, in this paper, we propose an enhanced MPDMP+ method that combines the advantages of MPC with potential function for both multi-demonstration imitation and multi-obstacle avoidance effect. A proximal augmented Lagrangian method is proposed to solve the optimization problem. This proposed method has a faster convergence rate and small errors. We conducted the simulation and robot experiments for imitation learning for obstacle avoidance scenarios. Our results illustrate the superior performance of the proposed method.},
	urldate = {2024-04-16},
	journal = {Robotics and Autonomous Systems},
	author = {Hu, Yingbai and Cui, Mingyang and Duan, Jianghua and Liu, Wenjun and Huang, Dianye and Knoll, Alois and Chen, Guang},
	month = may,
	year = {2023},
	keywords = {Dynamic movement primitives, Imitation learning, Model predictive control},
	pages = {104381},
}

@inproceedings{dawood_handling_2023,
	title = {Handling {Sparse} {Rewards} in {Reinforcement} {Learning} {Using} {Model} {Predictive} {Control}},
	doi = {10.1109/ICRA48891.2023.10161492},
	abstract = {Reinforcement learning (RL) has recently proven great success in various domains. Yet, the design of the reward function requires detailed domain expertise and tedious fine-tuning to ensure that agents are able to learn the desired behaviour. Using a sparse reward conveniently mitigates these challenges. However, the sparse reward represents a challenge on its own, often resulting in unsuccessful training of the agent. In this paper, we therefore address the sparse reward problem in RL. Our goal is to find an effective alternative to reward shaping, without using costly human demonstrations, that would also be applicable to a wide range of domains. Hence, we propose to use model predictive control (MPC) as an experience source for training RL agents in sparse reward environments. Without the need for reward shaping, we successfully apply our approach in the field of mobile robot navigation both in simulation and real-world experiments with a Kuboki Turtlebot 2. We furthermore demonstrate great improvement over pure RL algorithms in terms of success rate as well as number of collisions and timeouts. Our experiments show that MPC as an experience source improves the agent's learning process for a given task in the case of sparse rewards.},
	urldate = {2024-01-18},
	booktitle = {{IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Dawood, Murad and Dengler, Nils and de Heuvel, Jorge and Bennewitz, Maren},
	month = may,
	year = {2023},
	pages = {879--885},
}

@article{cai_learning-based_2023,
	title = {A {Learning}-{Based} {Model} {Predictive} {Control} {Strategy} for {Home} {Energy} {Management} {Systems}},
	volume = {11},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2023.3346324},
	abstract = {This paper presents a model predictive control (MPC)-based reinforcement learning (RL) approach for a home energy management system (HEMS). The house consists of an air-to-water heat pump connected to a hot water tank that supplies thermal energy to a water-based floor heating system. Additionally, it includes a photovoltaic (PV) array and a battery storage system. The HEMS is supposed to exploit the house thermal inertia and battery storage to shift demand from peak hours to off-peak periods and earn benefits by selling excess energy to the utility grid during periods of high electricity prices. However, designing such a HEMS is challenging because the discrepancies due to model mismatch make erroneous predictions of the system dynamics, leading to a non-optimal decision making. Besides, uncertainties in the house thermodynamics, misprediction in the forecasting of PV generation, outdoor temperature, and user load demand make the problem more challenging. We solve this issue by approximating the optimal policy by a parameterized MPC scheme and updating the parameters via a compatible delayed deterministic actor-critic (with gradient Q-learning critic, i.e., CDDAC-GQ) algorithm. Simulation results show that the proposed MPC-based RL HEMS can effectively deliver a policy that satisfies both indoor thermal comfort and economic costs even in the case of inaccurate model and system uncertainties. Furthermore, we conduct a thorough comparison between the CDDAC-GQ algorithm and the conventional twin delayed deep deterministic policy gradient (TD3) algorithm, the results of which affirm the efficacy of our proposed method in addressing complex HEMS problems.},
	language = {en},
	urldate = {2024-03-12},
	journal = {IEEE Access},
	author = {Cai, Wenqi and Sawant, Shambhuraj and Reinhardt, Dirk and Rastegarpour, Soroush and Gros, Sébastien},
	year = {2023},
	pages = {145264--145280},
}

@inproceedings{boone_identification_2023,
	title = {Identification of {Blackwell} {Optimal} {Policies} for {Deterministic} {MDPs}},
	abstract = {This paper investigates a new learning problem, the identification of Blackwell optimal policies on deterministic MDPs (DMDPs): A learner has to return a Blackwell optimal policy with fixed confidence using a minimal number of queries. First, we characterize the maximal set of DMDPs for which the identification is possible. Then, we focus on the analysis of algorithms based on product-form confidence regions. We minimize the number of queries by efficiently visiting the state-action pairs with respect to the shape of confidence sets. Furthermore, these confidence sets are themselves optimized to achieve better performances. The performances of our methods compare to the lower bounds up to a factor \$n{\textasciicircum}2\$ in the worst case – where \$n\$ is the number of states, and constant in certain classes of DMDPs.},
	language = {en},
	urldate = {2024-01-24},
	booktitle = {Proceedings of {The} 26th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Boone, Victor and Gaujal, Bruno},
	month = apr,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {7392--7424},
}

@article{arnstrom_real-time_2023,
	title = {Real-{Time} {Certified} {MPC} : {Reliable} {Active}-{Set} {QP} {Solvers}},
	shorttitle = {Real-{Time} {Certified} {MPC}},
	abstract = {DiVA portal is a finding tool for research publications and student theses written at the following 50 universities and research institutions.},
	language = {eng},
	urldate = {2024-02-23},
	author = {Arnström, Daniel},
	year = {2023},
	note = {Publisher: Linköping University Electronic Press},
}

@inproceedings{ahn_model_2023,
	title = {Model {Predictive} {Control} via {On}-{Policy} {Imitation} {Learning}},
	abstract = {In this paper, we leverage the rapid advances in imitation learning, a topic of intense recent focus in the Reinforcement Learning (RL) literature, to develop new sample complexity results and performance guarantees for  data-driven Model Predictive Control (MPC) for constrained linear systems. In its simplest form, imitation learning is an approach that tries to learn an expert policy by querying samples from an expert. Recent approaches to data-driven MPC have used the simplest form of imitation learning  known as behavior cloning to learn  controllers that mimic the performance of MPC by online  sampling of the trajectories of the closed-loop MPC system. Behavior cloning, however, is  a method  that is known to be data inefficient and suffer from distribution shifts. As an alternative, we develop a variant of the forward training algorithm which is an on-policy imitation learning method proposed by (Ross et al. 2010). Our algorithm uses the structure of constrained linear MPC, and our analysis uses the properties of the explicit MPC solution to  theoretically bound the number of online MPC trajectories needed to achieve optimal performance. We validate our results through simulations and show that the forward training algorithm is indeed superior to behavior cloning when applied to MPC.},
	language = {en},
	urldate = {2024-04-16},
	booktitle = {Proceedings of {The} 5th {Annual} {Learning} for {Dynamics} and {Control} {Conference}},
	publisher = {PMLR},
	author = {Ahn, Kwangjun and Mhammedi, Zakaria and Mania, Horia and Hong, Zhang-Wei and Jadbabaie, Ali},
	month = jun,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {1493--1505},
}

@inproceedings{zhang_learning-based_2022,
	title = {Learning-{Based} {Model} {Predictive} {Control} for {Quadruped} {Locomotion} on {Slippery} {Ground}},
	doi = {10.1109/ICCR55715.2022.10053909},
	abstract = {Nowadays, reinforcement learning (RL) and model predictive control (MPC) are two of the most widely used methods in robotics community. Model-based MPC enable the robot with stable locomotion capabilities, while Model-free RL provide an automatic approach to learn the policy to maximization the corresponding task performance. In this work, be aiming at utilize the advantages of these two approaches, we propose a Learning-Based Model Predictive Control (LBMPC) methodology for quadruped robot which improves MPC performance by learning the upper-layer decision parameters for MPC though a Heuristic Monte-Carlo Expectation-Maximization (HMCEM) algorithm. We validate this framework with the problem of dynamic locomotion on slippery ground by learning the friction factor which be fixed in standard MPC algorithm. Simulation results show that our LBMPC succeeds in find the optimal friction factor respect to different ground, and our heuristic overcome the problem that the conventional EM algorithms is sensitive to the initial value of policy. At last, we deduce a heuristic strategy for crude but fast ground classification based on empirical data.},
	urldate = {2024-01-18},
	booktitle = {4th {International} {Conference} on {Control} and {Robotics} ({ICCR})},
	author = {Zhang, Zhitong and An, Honglei and Wei, Qing and Ma, Hongxu},
	month = dec,
	year = {2022},
	pages = {47--52},
}

@article{zhang_building_2022,
	title = {Building {Energy} {Management} {With} {Reinforcement} {Learning} and {Model} {Predictive} {Control}: {A} {Survey}},
	volume = {10},
	issn = {2169-3536},
	shorttitle = {Building {Energy} {Management} {With} {Reinforcement} {Learning} and {Model} {Predictive} {Control}},
	doi = {10.1109/ACCESS.2022.3156581},
	abstract = {Building energy management has been recognized as of signiﬁcant importance on improving the overall system efﬁciency and reducing the greenhouse gas emission. However, the building energy management system is now facing more challenges and uncertainties with the increasing penetration of renewable energy and increasing adoption of different types of electrical appliances and equipment. Classical model predictive control (MPC) has shown effective in building energy management, although it suffers from labour-intensive modelling and complex online control optimization. Recently, with the growing accessibility to building control and automation data, data-driven solutions such as data-driven MPC and reinforcement learning (RL)-based methods have attracted more research interest. However, the potential of integrating these two types of methods and how to choose suitable control algorithms have not been well discussed. In this work, we ﬁrst present a compact review of the recent advances in data-driven MPC and RL-based control methods for building energy management. Furthermore, the main challenges in these approaches and general discussions on the selection of control methods are discussed.},
	language = {en},
	urldate = {2024-02-21},
	journal = {IEEE Access},
	author = {Zhang, Huiliang and Seal, Sayani and Wu, Di and Bouffard, Francois and Boulet, Benoit},
	year = {2022},
	pages = {27853--27862},
}

@article{verschueren_acadosmodular_2022,
	title = {acados—a modular open-source framework for fast embedded optimal control},
	volume = {14},
	issn = {1867-2957},
	doi = {10.1007/s12532-021-00208-8},
	abstract = {This paper presents the acados software package, a collection of solvers for fast embedded optimization intended for fast embedded applications. Its interfaces to higher-level languages make it useful for quickly designing an optimization-based control algorithm by putting together different algorithmic components that can be readily connected and interchanged. Since the core of acados is written on top of a high-performance linear algebra library, we do not sacrifice computational performance. Thus, we aim to provide both flexibility and performance through modularity, without the need to rely on automatic code generation, which facilitates maintainability and extensibility. The main features of acados are: efficient optimal control algorithms targeting embedded devices implemented in C, linear algebra based on the high-performance BLASFEO Frison (ACM Transactions on Mathematical Software (TOMS) 44: 1–30, 2018) library, user-friendly interfaces to Matlab and Python, and compatibility with the modeling language of CasADi Andersson (Mathematical Programming Computation 11: 136, 2019). acados is free and open-source software released under the permissive BSD 2-Clause license.},
	language = {en},
	number = {1},
	urldate = {2024-01-18},
	journal = {Mathematical Programming Computation},
	author = {Verschueren, Robin and Frison, Gianluca and Kouzoupis, Dimitris and Frey, Jonathan and Duijkeren, Niels van and Zanelli, Andrea and Novoselnik, Branimir and Albin, Thivaharan and Quirynen, Rien and Diehl, Moritz},
	month = mar,
	year = {2022},
	keywords = {04, 49, Direct optimal control, Optimization algorithms},
	pages = {147--183},
}

@inproceedings{sacks_learning_2022,
	title = {Learning to {Optimize} in {Model} {Predictive} {Control}},
	doi = {10.1109/ICRA46639.2022.9812369},
	abstract = {Sampling-based Model Predictive Control (MPC) is a flexible control framework that can reason about non-smooth dynamics and cost functions. Recently, significant work has focused on the use of machine learning to improve the performance of MPC, often through learning or fine-tuning the dynamics or cost function. In contrast, we focus on learning to optimize more effectively. In other words, to improve the update rule within MPC. We show that this can be particularly useful in sampling-based MPC, where we often wish to minimize the number of samples for computational reasons. Unfortunately, the cost of computational efficiency is a reduction in performance; fewer samples results in noisier updates. We show that we can contend with this noise by learning how to update the control distribution more effectively and make better use of the few samples that we have. Our learned controllers are trained via imitation learning to mimic an expert which has access to substantially more samples. We test the efficacy of our approach on multiple simulated robotics tasks in sample-constrained regimes and demonstrate that our approach can outperform a MPC controller with the same number of samples.},
	urldate = {2024-01-18},
	booktitle = {International {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Sacks, Jacob and Boots, Byron},
	month = may,
	year = {2022},
	pages = {10549--10556},
}

@article{moradimaryamnegari_model_2022,
	title = {Model {Predictive} {Control}-{Based} {Reinforcement} {Learning} {Using} {Expected} {Sarsa}},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3195530},
	abstract = {Recent studies have shown the potential of Reinforcement Learning (RL) algorithms in tuning the parameters of Model Predictive Controllers (MPC), including the weights of the cost function and unknown parameters of the MPC model. However, a framework for easy and straightforward implementation that allows training in just a few episodes and overcoming the need for imposing extra constraints as required by state-of-the-art methods, is still missing. In this study, we present two implementations to achieve these goals. In the first approach, a nonlinear MPC plays the role of a function approximator for an Expected Sarsa RL algorithm. In the second approach, only the MPC cost function is considered as the function approximator, while the unknown parameters of the MPC model are updated based on more classical system identification. In order to evaluate the performance of the proposed algorithms, first numerical simulations are performed on a coupled tanks system. Then, both algorithms are applied to the real system and their closed-loop performance and convergence speed are compared with each other. The results indicate that the proposed algorithms allow tuning of MPCs over very few episodes. Finally, also the disturbance rejection ability of the proposed methods is demonstrated.},
	urldate = {2024-01-18},
	journal = {IEEE Access},
	author = {Moradimaryamnegari, Hoomaan and Frego, Marco and Peer, Angelika},
	year = {2022},
	note = {Conference Name: IEEE Access},
	pages = {81177--81191},
}

@article{moos_robust_2022,
	title = {Robust {Reinforcement} {Learning}: {A} {Review} of {Foundations} and {Recent} {Advances}},
	volume = {4},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2504-4990},
	shorttitle = {Robust {Reinforcement} {Learning}},
	doi = {10.3390/make4010013},
	abstract = {Reinforcement learning (RL) has become a highly successful framework for learning in Markov decision processes (MDP). Due to the adoption of RL in realistic and complex environments, solution robustness becomes an increasingly important aspect of RL deployment. Nevertheless, current RL algorithms struggle with robustness to uncertainty, disturbances, or structural changes in the environment. We survey the literature on robust approaches to reinforcement learning and categorize these methods in four different ways: (i) Transition robust designs account for uncertainties in the system dynamics by manipulating the transition probabilities between states; (ii) Disturbance robust designs leverage external forces to model uncertainty in the system behavior; (iii) Action robust designs redirect transitions of the system by corrupting an agent’s output; (iv) Observation robust designs exploit or distort the perceived system state of the policy. Each of these robust designs alters a different aspect of the MDP. Additionally, we address the connection of robustness to the risk-based and entropy-regularized RL formulations. The resulting survey covers all fundamental concepts underlying the approaches to robust reinforcement learning and their recent advances.},
	language = {en},
	number = {1},
	urldate = {2024-02-23},
	journal = {Machine Learning and Knowledge Extraction},
	author = {Moos, Janosch and Hansel, Kay and Abdulsamad, Hany and Stark, Svenja and Clever, Debora and Peters, Jan},
	month = mar,
	year = {2022},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {min-max optimization, reinforcement learning, robustness},
	pages = {276--315},
}

@inproceedings{mesbah_fusion_2022,
	title = {Fusion of {Machine} {Learning} and {MPC} under {Uncertainty}: {What} {Advances} {Are} on the {Horizon}?},
	shorttitle = {Fusion of {Machine} {Learning} and {MPC} under {Uncertainty}},
	doi = {10.23919/ACC53348.2022.9867643},
	abstract = {This paper provides an overview of the recent research efforts on the integration of machine learning and model predictive control under uncertainty. The paper is organized as a collection of four major categories: learning models from system data and prior knowledge; learning control policy parameters from closed-loop performance data; learning efficient approximations of iterative online optimization from policy data; and learning optimal cost-to-go representations from closed-loop performance data. In addition to reviewing the relevant literature, the paper also offers perspectives for future research in each of these areas.},
	urldate = {2024-01-17},
	booktitle = {American {Control} {Conference} ({ACC})},
	author = {Mesbah, Ali and Wabersich, Kim P. and Schoellig, Angela P. and Zeilinger, Melanie N. and Lucia, Sergio and Badgwell, Thomas A. and Paulson, Joel A.},
	month = jun,
	year = {2022},
	note = {ISSN: 2378-5861},
	pages = {342--357},
}

@article{lee_comparison_2022,
	title = {Comparison of {Deep} {Reinforcement} {Learning} and {PID} {Controllers} for {Automatic} {Cold} {Shutdown} {Operation}},
	volume = {15},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1996-1073},
	doi = {10.3390/en15082834},
	abstract = {Many industries apply traditional controllers to automate manual control. In recent years, artificial intelligence controllers applied with deep-learning techniques have been suggested as advanced controllers that can achieve goals from many industrial domains, such as humans. Deep reinforcement learning (DRL) is a powerful method for these controllers to learn how to achieve their specific operational goals. As DRL controllers learn through sampling from a target system, they can overcome the limitations of traditional controllers, such as proportional-integral-derivative (PID) controllers. In nuclear power plants (NPPs), automatic systems can manage components during full-power operation. In contrast, startup and shutdown operations are less automated and are typically performed by operators. This study suggests DRL-based and PID-based controllers for cold shutdown operations, which are a part of startup operations. By comparing the suggested controllers, this study aims to verify that learning-based controllers can overcome the limitations of traditional controllers and achieve operational goals with minimal manipulation. First, to identify the required components, operational goals, and inputs/outputs of operations, this study analyzed the general operating procedures for cold shutdown operations. Then, PID- and DRL-based controllers are designed. The PID-based controller consists of PID controllers that are well-tuned using the Ziegler–Nichols rule. The DRL-based controller with long short-term memory (LSTM) is trained with a soft actor-critic algorithm that can reduce the training time by using distributed prioritized experience replay and distributed learning. The LSTM can process a plant time-series data to generate control signals. Subsequently, the suggested controllers were validated using an NPP simulator during the cold shutdown operation. Finally, this study discusses the operational performance by comparing PID- and DRL-based controllers.},
	language = {en},
	number = {8},
	urldate = {2024-02-21},
	journal = {Energies},
	author = {Lee, Daeil and Koo, Seoryong and Jang, Inseok and Kim, Jonghyun},
	month = jan,
	year = {2022},
	note = {Number: 8
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {artificial intelligence, autonomous operation, deep reinforcement learning, nuclear power plant, soft actor-critic algorithm},
	pages = {2834},
}

@article{gros_economic_2022,
	title = {Economic {MPC} of {Markov} {Decision} {Processes}: {Dissipativity} in undiscounted infinite-horizon optimal control},
	volume = {146},
	issn = {0005-1098},
	shorttitle = {Economic {MPC} of {Markov} {Decision} {Processes}},
	doi = {10.1016/j.automatica.2022.110602},
	abstract = {Economic Model Predictive Control (MPC) dissipativity theory is central to discussing the stability of policies resulting from minimizing economic stage costs. In its current form, the dissipativity theory for economic MPC applies to problems based on deterministic dynamics or to very specific classes of stochastic problems, and does not readily extend to generic Markov decision processes. In this paper, we clarify the core reason for this difficulty, and propose a generalization of the economic MPC dissipativity theory that circumvents it. This generalization focuses on undiscounted infinite-horizon problems and is based on nonlinear stage cost functionals, allowing one to discuss the Lyapunov asymptotic stability of policies for Markov decision processes in terms of the probability measures underlying their stochastic dynamics. This theory is illustrated for the stochastic linear quadratic regulator with Gaussian process noise, for which a storage functional can be provided explicitly. For the sake of brevity, we limit our discussion to undiscounted Markov decision processes.},
	urldate = {2024-02-23},
	journal = {Automatica},
	author = {Gros, Sebastien and Zanon, Mario},
	month = dec,
	year = {2022},
	keywords = {Dissipativity for economic MPC, Economic costs, Markov Decision Processes, Storage functions},
	pages = {110602},
}

@article{dobriborsci_experimental_2022,
	series = {10th {IFAC} {Conference} on {Manufacturing} {Modelling}, {Management} and {Control} {MIM} 2022},
	title = {An experimental study of two predictive reinforcement learning methods and comparison with model-predictive control},
	volume = {55},
	issn = {2405-8963},
	doi = {10.1016/j.ifacol.2022.09.610},
	abstract = {Reinforcement learning (RL) has been successfully used in various simulations and computer games. Industry-related applications, such as autonomous mobile robot motion control, are somewhat challenging for RL up to date though. This paper presents an experimental evaluation of predictive RL controllers for optimal mobile robot motion control. As a baseline for comparison, model-predictive control (MPC) is used. Two RL methods are tested: a rollout Q-learning, which may be considered as MPC with terminal cost being a Q-function approximation, and a so-called stacked Q-learning, which in turn is like MPC with the running cost substituted for a Q-function approximation. The experimental foundation is a mobile robot with a differential drive (Robotis Turtlebot3). Experimental results showed that both RL methods beat the baseline in terms of the accumulated cost, whereas the stacked variant performed best. Provided the series of previous works on stacked Q-learning, this particular study supports the idea that MPC with a running cost adaptation inspired by Q-learning possesses potential of performance boost while retaining the nice properties of MPC.},
	number = {10},
	urldate = {2024-02-22},
	journal = {IFAC-PapersOnLine},
	author = {Dobriborsci, Dmitrii and Osinenko, Pavel and Aumer, Wolfgang},
	month = jan,
	year = {2022},
	keywords = {experimental study, mobile robots, model predictive control, predictive control, reinforcement learning},
	pages = {1545--1550},
}

@article{brito_learning_2022,
	title = {Learning {Interaction}-{Aware} {Guidance} for {Trajectory} {Optimization} in {Dense} {Traffic} {Scenarios}},
	volume = {23},
	issn = {1558-0016},
	doi = {10.1109/TITS.2022.3160936},
	abstract = {Autonomous navigation in dense traffic scenarios remains challenging for autonomous vehicles (AVs) because the intentions of other drivers are not directly observable and AVs have to deal with a wide range of driving behaviors. To maneuver through dense traffic, AVs must be able to reason how their actions affect others (interaction model) and exploit this reasoning to navigate through dense traffic safely. This paper presents a novel framework for interaction-aware motion planning in dense traffic scenarios. We explore the connection between human driving behavior and their velocity changes when interacting. Hence, we propose to learn, via deep Reinforcement Learning (RL), an interaction-aware policy providing global guidance about the cooperativeness of other vehicles to an optimization-based planner ensuring safety and kinematic feasibility through constraint satisfaction. The learned policy can reason and guide the local optimization-based planner with interactive behavior to pro-actively merge in dense traffic while remaining safe in case other vehicles do not yield. We present qualitative and quantitative results in highly interactive simulation environments (highway merging and unprotected left turns) against two baseline approaches, a learning-based and an optimization-based method. The presented results show that our method significantly reduces the number of collisions and increases the success rate with respect to both learning-based and optimization-based baselines.},
	number = {10},
	urldate = {2024-01-18},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Brito, Bruno and Agarwal, Achin and Alonso-Mora, Javier},
	month = oct,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Intelligent Transportation Systems},
	pages = {18808--18821},
}

@article{brandi_comparison_2022,
	title = {Comparison of online and offline deep reinforcement learning with model predictive control for thermal energy management},
	volume = {135},
	issn = {0926-5805},
	doi = {10.1016/j.autcon.2022.104128},
	abstract = {This paper proposes a comparison between an online and offline Deep Reinforcement Learning (DRL) formulation with a Model Predictive Control (MPC) architecture for energy management of a cold-water buffer tank linking an office building and a chiller subject to time-varying energy prices, with the objective of minimizing operating costs. The intrinsic model-free approach of DRL is generally lost in common implementations for energy management, as they are usually pre-trained offline and require a surrogate model for this purpose. Simulation results showed that the online-trained DRL agent, while requiring an initial 4 weeks adjustment period achieving a relatively poor performance (160\% higher cost), it converged to a control policy almost as effective as the model-based strategies (3.6\% higher cost in the last month). This suggests that the DRL agent trained online may represent a promising solution to overcome the barrier represented by the modelling requirements of MPC and offline-trained DRL approaches.},
	urldate = {2024-02-22},
	journal = {Automation in Construction},
	author = {Brandi, Silvio and Fiorentini, Massimo and Capozzoli, Alfonso},
	month = mar,
	year = {2022},
	keywords = {Building energy consumption, Building energy management, Deep reinforcement learning, Energy savings, HVAC control, Model predictive control},
	pages = {104128},
}

@inproceedings{beckenbach_approximate_2022,
	title = {Approximate infinite-horizon predictive control},
	doi = {10.1109/CDC51059.2022.9992741},
	abstract = {Predictive control is frequently used for control problems involving constraints. Being an optimization based technique utilizing a user specified so-called stage cost, performance properties, i.e., bounds on the infinite horizon accumulated stage cost, aside closed-loop stability are of interest. To achieve good performance and to influence the region of attraction associated with the prediction horizon, the terminal cost of the predictive controller’s optimization objective is a key design factor. Approximate dynamic programming refers to one particular approximation paradigm that pursues iterative cost adaptation over a state domain. Troubled by approximation errors, the associated approximate optimal controller is, in general, not necessarily stabilizing nor is its performance quantifiable on the entire approximation domain. Using a parametric terminal cost trained via approximate dynamic programming, a stabilizing predictive controller is proposed whose performance can directly be related to cost approximation errors. The controller further ensures closed-loop asymptotic stability beyond the training domain of the approximate optimal controller associated to the terminal cost.},
	urldate = {2024-04-15},
	booktitle = {{IEEE} 61st {Conference} on {Decision} and {Control} ({CDC})},
	author = {Beckenbach, Lukas and Streif, Stefan},
	month = dec,
	year = {2022},
	note = {ISSN: 2576-2370},
	keywords = {Approximation error, Asymptotic stability, Cost function, Costs, Dynamic programming, Stability analysis, Training},
	pages = {3711--3717},
}

@misc{acerbo_mpc-based_2022,
	title = {{MPC}-based {Imitation} {Learning} for {Safe} and {Human}-like {Autonomous} {Driving}},
	abstract = {To ensure user acceptance of autonomous vehicles (AVs), control systems are being developed to mimic human drivers from demonstrations of desired driving behaviors. Imitation learning (IL) algorithms serve this purpose, but struggle to provide safety guarantees on the resulting closed-loop system trajectories. On the other hand, Model Predictive Control (MPC) can handle nonlinear systems with safety constraints, but realizing human-like driving with it requires extensive domain knowledge. This work suggests the use of a seamless combination of the two techniques to learn safe AV controllers from demonstrations of desired driving behaviors, by using MPC as a differentiable control layer within a hierarchical IL policy. With this strategy, IL is performed in closed-loop and end-to-end, through parameters in the MPC cost, model or constraints. Experimental results of this methodology are analyzed for the design of a lane keeping control system, learned via behavioral cloning from observations (BCO), given human demonstrations on a fixed-base driving simulator.},
	urldate = {2024-03-20},
	publisher = {arXiv},
	author = {Acerbo, Flavia Sofia and Swevers, Jan and Tuytelaars, Tinne and Son, Tong Duy},
	month = jun,
	year = {2022},
	note = {arXiv:2206.12348 [cs, eess]},
}

@article{zehnder_sgn_2021,
	title = {{SGN}: {Sparse} {Gauss}-{Newton} for {Accelerated} {Sensitivity} {Analysis}},
	volume = {41},
	issn = {0730-0301},
	shorttitle = {{SGN}},
	doi = {10.1145/3470005},
	abstract = {We present a sparse Gauss-Newton solver for accelerated sensitivity analysis with applications to a wide range of equilibrium-constrained optimization problems. Dense Gauss-Newton solvers have shown promising convergence rates for inverse problems, but the cost of assembling and factorizing the associated matrices has so far been a major stumbling block. In this work, we show how the dense Gauss-Newton Hessian can be transformed into an equivalent sparse matrix that can be assembled and factorized much more efficiently. This leads to drastically reduced computation times for many inverse problems, which we demonstrate on a diverse set of examples. We furthermore show links between sensitivity analysis and nonlinear programming approaches based on Lagrange multipliers and prove equivalence under specific assumptions that apply for our problem setting.},
	number = {1},
	urldate = {2024-03-11},
	journal = {ACM Transactions on Graphics},
	author = {Zehnder, Jonas and Coros, Stelian and Thomaszewski, Bernhard},
	month = sep,
	year = {2021},
	keywords = {Sensitivity analysis, equilibrium-constrained optimization, nonlinear least-squares, sparse Gauss-Newton},
	pages = {4:1--4:10},
}

@article{zanon_safe_2021,
	title = {Safe {Reinforcement} {Learning} {Using} {Robust} {MPC}},
	volume = {66},
	issn = {1558-2523},
	doi = {10.1109/TAC.2020.3024161},
	abstract = {Reinforcement learning (RL) has recently impressed the world with stunning results in various applications. While the potential of RL is now well established, many critical aspects still need to be tackled, including safety and stability issues. These issues, while secondary for the RL community, are central to the control community that has been widely investigating them. Model predictive control (MPC) is one of the most successful control techniques because, among others, of its ability to provide such guarantees even for uncertain constrained systems. Since MPC is an optimization-based technique, optimality has also often been claimed. Unfortunately, the performance of MPC is highly dependent on the accuracy of the model used for predictions. In this article, we propose to combine RL and MPC in order to exploit the advantages of both, and therefore, obtain a controller that is optimal and safe. We illustrate the results with two numerical examples in simulations.},
	number = {8},
	urldate = {2024-03-07},
	journal = {IEEE Transactions on Automatic Control},
	author = {Zanon, Mario and Gros, Sebastien},
	month = aug,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Automatic Control},
	pages = {3638--3652},
}

@article{wabersich_predictive_2021,
	title = {A predictive safety filter for learning-based control of constrained nonlinear dynamical systems},
	volume = {129},
	issn = {0005-1098},
	doi = {10.1016/j.automatica.2021.109597},
	abstract = {The transfer of reinforcement learning (RL) techniques into real-world applications is challenged by safety requirements in the presence of physical limitations. Most RL methods, in particular the most popular algorithms, do not support explicit consideration of state and input constraints. In this paper, we address this problem for nonlinear systems with continuous state and input spaces by introducing a predictive safety filter, which is able to turn a constrained dynamical system into an unconstrained safe system and to which any RL algorithm can be applied ‘out-of-the-box’. The predictive safety filter receives the proposed control input and decides, based on the current system state, if it can be safely applied to the real system, or if it has to be modified otherwise. Safety is thereby established by a continuously updated safety policy, which is based on a model predictive control formulation using a data-driven system model and considering state and input dependent uncertainties.},
	urldate = {2024-02-23},
	journal = {Automatica},
	author = {Wabersich, Kim Peter and Zeilinger, Melanie N.},
	month = jul,
	year = {2021},
	keywords = {Control of constrained systems, Data-based control, Robust control of nonlinear systems, Safe learning-based control},
	pages = {109597},
}

@inproceedings{reske_imitation_2021,
	address = {Xi'an, China},
	title = {Imitation {Learning} from {MPC} for {Quadrupedal} {Multi}-{Gait} {Control}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-72819-077-8},
	doi = {10.1109/ICRA48506.2021.9561444},
	abstract = {We present a learning algorithm for training a single policy that imitates multiple gaits of a walking robot. To achieve this, we use and extend MPC-Net, which is an Imitation Learning approach guided by Model Predictive Control (MPC). The strategy of MPC-Net differs from many other approaches since its objective is to minimize the control Hamiltonian, which derives from the principle of optimality. To represent the policies, we employ a mixture-of-experts network (MEN) and observe that the performance of a policy improves if each expert of a MEN specializes in controlling exactly one mode of a hybrid system, such as a walking robot. We introduce new loss functions for single- and multi-gait policies to achieve this kind of expert selection behavior. Moreover, we benchmark our algorithm against Behavioral Cloning and the original MPC implementation on various rough terrain scenarios. We validate our approach on hardware and show that a single learned policy can replace its teacher to control multiple gaits.},
	language = {en},
	urldate = {2024-04-16},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Reske, Alexander and Carius, Jan and Ma, Yuntao and Farshidian, Farbod and Hutter, Marco},
	month = may,
	year = {2021},
	pages = {5014--5020},
}

@article{lin_comparison_2021,
	title = {Comparison of {Deep} {Reinforcement} {Learning} and {Model} {Predictive} {Control} for {Adaptive} {Cruise} {Control}},
	volume = {6},
	issn = {2379-8904},
	doi = {10.1109/TIV.2020.3012947},
	abstract = {This study compares Deep Reinforcement Learning (DRL) and Model Predictive Control (MPC) for Adaptive Cruise Control (ACC) design in car-following scenarios. A first-order system is used as the Control-Oriented Model (COM) to approximate the acceleration command dynamics of a vehicle. Based on the equations of the control system and the multi-objective cost function, we train a DRL policy using Deep Deterministic Policy Gradient (DDPG) and solve the MPC problem via Interior-Point Optimization (IPO). Simulation results for the episode costs show that, when there are no modeling errors and the testing inputs are within the training data range, the DRL solution is equivalent to MPC with a sufficiently long prediction horizon. Particularly, the DRL episode cost is only 5.8\% higher than the benchmark optimal control solution provided by optimizing the entire episode via IPO. The DRL control performance degrades when the testing inputs are outside the training data range, indicating inadequate machine learning generalization. When there are modeling errors due to control delay, disturbances, and/or testing with a High-Fidelity Model (HFM) of the vehicle, the DRL-trained policy performs better when the modeling errors are large while having similar performances as MPC when the modeling errors are small.},
	number = {2},
	urldate = {2024-02-21},
	journal = {IEEE Transactions on Intelligent Vehicles},
	author = {Lin, Yuan and McPhee, John and Azad, Nasser L.},
	month = jun,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Intelligent Vehicles},
	keywords = {Adaptive Cruise Control (ACC), Cost function, Deep reinforcement learning, Delays, Learning (artificial intelligence), Mathematical model, Model Predictive Control (MPC), Optimal control, Testing},
	pages = {221--231},
}

@inproceedings{kordabad_mpc-based_2021,
	title = {{MPC}-based reinforcement learning for economic problems with application to battery storage},
	urldate = {2024-03-07},
	booktitle = {European {Control} {Conference} ({ECC})},
	publisher = {IEEE},
	author = {Kordabad, Arash Bahari and Cai, Wenqi and Gros, Sebastien},
	year = {2021},
	pages = {2573--2578},
}

@inproceedings{hasankhani_comparison_2021,
	title = {Comparison of {Deep} {Reinforcement} {Learning} and {Model} {Predictive} {Control} for {Real}-{Time} {Depth} {Optimization} of a {Lifting} {Surface} {Controlled} {Ocean} {Current} {Turbine}},
	doi = {10.1109/CCTA48906.2021.9659089},
	abstract = {This paper evaluates two strategies, deep reinforcement learning (DRL) and model predictive control (MPC), for maximizing harnessed power from a lifting surface controlled ocean current turbine (OCT) through depth optimization. To address spatiotemporal uncertainties in the ocean current, an online Gaussian Process (GP) is applied, where the prediction error of the ocean current speed is also modeled. We compare the performance of the MPC-based optimization with the DRL-based algorithm (i.e., deep Q-networks (DQN)) using over one week of field collected acoustic doppler current profiler (ADCP) data. The DRL-based algorithm is almost equivalent to the MPC-based algorithm in real-time optimization when the ocean current speed prediction is perfect. However, the performance of the DQN-based algorithm surpasses the MPC-based algorithm when ocean current prediction error is considered. The importance of using the DQN in improving the error-tolerance of the proposed spatiotemporal optimization is verified through the comparative results.},
	urldate = {2024-02-21},
	booktitle = {{IEEE} {Conference} on {Control} {Technology} and {Applications} ({CCTA})},
	author = {Hasankhani, Arezoo and Tang, Yufei and VanZwieten, James and Sultan, Cornel},
	month = aug,
	year = {2021},
	note = {ISSN: 2768-0770},
	keywords = {Prediction algorithms, Predictive models, Real-time systems, Reinforcement learning, Sea surface, Spatiotemporal phenomena, Uncertainty},
	pages = {301--308},
}

@inproceedings{gros_reinforcement_2021,
	title = {Reinforcement {Learning} based on {MPC} and the {Stochastic} {Policy} {Gradient} {Method}},
	doi = {10.23919/ACC50511.2021.9482765},
	abstract = {In this paper, we present a methodology to implement the stochastic policy gradient method using actor-critic techniques, when the policy is approximated using an MPC scheme. The paper proposes a computationally inexpensive approach to build a stochastic policy generating samples that are guaranteed to be feasible for the MPC constraints. For a continuous input space, imposing hard constraints on the policy poses technical difficulties in the computation of the score function of the policy, required in the policy gradient computation. We propose an approach that solves this issue, and detail how the score function can be computed based on parametric Nonlinear Programming and primal-dual interior point. The approach is illustrated on a simple example.},
	urldate = {2024-03-07},
	booktitle = {American {Control} {Conference} ({ACC})},
	author = {Gros, Sébastien and Zanon, Mario},
	month = may,
	year = {2021},
	note = {ISSN: 2378-5861},
	pages = {1947--1952},
}

@inproceedings{gros_bias_2021,
	title = {Bias {Correction} in {Reinforcement} {Learning} via the {Deterministic} {Policy} {Gradient} {Method} for {MPC}-{Based} {Policies}},
	doi = {10.23919/ACC50511.2021.9483016},
	abstract = {In this paper, we discuss the implementation of the Deterministic Policy Gradient using the Actor-Critic technique based on linear compatible advantage function approximations in the context of constrained policies. We focus on MPC-based policies, though the discussion is general. We show that in that context, the classic linear compatible advantage function approximation fails to deliver a correct policy gradient due to the exploration becoming distorted by the constraints, and we propose a generalized linear compatible advantage function approximation that corrects the problem. We show that this correction requires an estimation of the mean and covariance of the constrained exploration. The validity of that generalization is formally established and demonstrated on a simple example.},
	urldate = {2024-03-07},
	booktitle = {American {Control} {Conference} ({ACC})},
	author = {Gros, Sébastien and Zanon, Mario},
	month = may,
	year = {2021},
	note = {ISSN: 2378-5861},
	pages = {2543--2548},
}

@inproceedings{esfahani_reinforcement_2021,
	title = {Reinforcement learning based on {MPC}/{MHE} for unmodeled and partially observable dynamics},
	urldate = {2024-03-07},
	booktitle = {American {Control} {Conference} ({ACC})},
	publisher = {IEEE},
	author = {Esfahani, Hossein Nejatbakhsh and Kordabad, Arash Bahari and Gros, Sébastien},
	year = {2021},
	pages = {2121--2126},
}

@article{ceusters_model-predictive_2021,
	title = {Model-predictive control and reinforcement learning in multi-energy system case studies},
	volume = {303},
	issn = {0306-2619},
	doi = {10.1016/j.apenergy.2021.117634},
	abstract = {Model predictive control (MPC) offers an optimal control technique to establish and ensure that the total operation cost of multi-energy systems remains at a minimum while fulfilling all system constraints. However, this method presumes an adequate model of the underlying system dynamics, which is prone to modelling errors and is not necessarily adaptive. This has an associated initial and ongoing project-specific engineering cost. In this paper, we present an on- and off-policy multi-objective reinforcement learning (RL) approach that does not assume a model a priori, benchmarking this against a linear MPC (LMPC — to reflect current practice, though non-linear MPC performs better) - both derived from the general optimal control problem, highlighting their differences and similarities. In a simple multi-energy system (MES) configuration case study, we show that a twin delayed deep deterministic policy gradient (TD3) RL agent offers the potential to match and outperform the perfect foresight LMPC benchmark (101.5\%). This while the realistic LMPC, i.e. imperfect predictions, only achieves 98\%. While in a more complex MES system configuration, the RL agent’s performance is generally lower (94.6\%), yet still better than the realistic LMPC (88.9\%). In both case studies, the RL agents outperformed the realistic LMPC after a training period of 2 years using quarterly interactions with the environment. We conclude that reinforcement learning is a viable optimal control technique for multi-energy systems given adequate constraint handling and pre-training, to avoid unsafe interactions and long training periods, as is proposed in fundamental future work.},
	urldate = {2024-02-22},
	journal = {Applied Energy},
	author = {Ceusters, Glenn and Rodríguez, Román Cantú and García, Alberte Bouso and Franke, Rüdiger and Deconinck, Geert and Helsen, Lieve and Nowé, Ann and Messagie, Maarten and Camargo, Luis Ramirez},
	month = dec,
	year = {2021},
	keywords = {Model-predictive control, Multi-energy systems, Optimal control, Reinforcement learning},
	pages = {117634},
}

@inproceedings{cai_optimal_2021,
	address = {Austin, TX, USA},
	title = {Optimal {Management} of the {Peak} {Power} {Penalty} for {Smart} {Grids} {Using} {MPC}-based {Reinforcement} {Learning}},
	isbn = {978-1-66543-659-5},
	doi = {10.1109/CDC45484.2021.9683333},
	abstract = {The cost of the power distribution infrastructures is driven by the peak power encountered in the system. Therefore, the distribution network operators consider billing consumers behind a common transformer in the function of their peak demand and leave it to the consumers to manage their collective costs. This management problem is, however, not trivial. In this paper, we consider a multi-agent residential smart grid system, where each agent has local renewable energy production and energy storage, and all agents are connected to a local transformer. The objective is to develop an optimal policy that minimizes the economic cost consisting of both the spot-market cost for each consumer and their collective peakpower cost. We propose to use a parametric Model Predictive Control (MPC)-scheme to approximate the optimal policy. The optimality of this policy is limited by its ﬁnite horizon and inaccurate forecasts of the local power production-consumption. A Deterministic Policy Gradient (DPG) method is deployed to adjust the MPC parameters and improve the policy. Our simulations show that the proposed MPC-based Reinforcement Learning (RL) method can effectively decrease the long-term economic cost for this smart grid problem.},
	language = {en},
	urldate = {2024-03-12},
	booktitle = {60th {IEEE} {Conference} on {Decision} and {Control} ({CDC})},
	publisher = {IEEE},
	author = {Cai, Wenqi and Esfahani, Hossein N. and Kordabad, Arash B. and Gros, Sebastien},
	month = dec,
	year = {2021},
	pages = {6365--6370},
}

@article{bertsimas_voice_2021,
	title = {The voice of optimization},
	volume = {110},
	issn = {1573-0565},
	doi = {10.1007/s10994-020-05893-5},
	abstract = {We introduce the idea that using optimal classification trees (OCTs) and optimal classification trees with-hyperplanes (OCT-Hs), interpretable machine learning algorithms developed by Bertsimas and Dunn (Mach Learn 106(7):1039–1082, 2017), we are able to obtain insight on the strategy behind the optimal solution in continuous and mixed-integer convex optimization problem as a function of key parameters that affect the problem. In this way, optimization is not a black box anymore. Instead, we redefine optimization as a multiclass classification problem where the predictor gives insights on the logic behind the optimal solution. In other words, OCTs and OCT-Hs give optimization a voice. We show on several realistic examples that the accuracy behind our method is in the 90–100\% range, while even when the predictions are not correct, the degree of suboptimality or infeasibility is very low. We compare optimal strategy predictions of OCTs and OCT-Hs and feedforward neural networks (NNs) and conclude that the performance of OCT-Hs and NNs is comparable. OCTs are somewhat weaker but often competitive. Therefore, our approach provides a novel insightful understanding of optimal strategies to solve a broad class of continuous and mixed-integer optimization problems.},
	language = {en},
	number = {2},
	urldate = {2024-03-26},
	journal = {Machine Learning},
	author = {Bertsimas, Dimitris and Stellato, Bartolomeo},
	month = feb,
	year = {2021},
	keywords = {Interpretability, Multiclass classification, Parametric optimization, Sampling},
	pages = {249--277},
}

@article{bengio_machine_2021,
	title = {Machine learning for combinatorial optimization: {A} methodological tour d’horizon},
	volume = {290},
	issn = {0377-2217},
	shorttitle = {Machine learning for combinatorial optimization},
	doi = {10.1016/j.ejor.2020.07.063},
	abstract = {This paper surveys the recent attempts, both from the machine learning and operations research communities, at leveraging machine learning to solve combinatorial optimization problems. Given the hard nature of these problems, state-of-the-art algorithms rely on handcrafted heuristics for making decisions that are otherwise too expensive to compute or mathematically not well defined. Thus, machine learning looks like a natural candidate to make such decisions in a more principled and optimized way. We advocate for pushing further the integration of machine learning and combinatorial optimization and detail a methodology to do so. A main point of the paper is seeing generic optimization problems as data points and inquiring what is the relevant distribution of problems to use for learning on a given task.},
	number = {2},
	urldate = {2024-01-17},
	journal = {European Journal of Operational Research},
	author = {Bengio, Yoshua and Lodi, Andrea and Prouvost, Antoine},
	month = apr,
	year = {2021},
	keywords = {Branch and bound, Combinatorial optimization, Machine learning, Mixed-integer programming solvers},
	pages = {405--421},
}

@article{zanon_reinforcement_2020,
	series = {21st {IFAC} {World} {Congress}},
	title = {Reinforcement {Learning} {Based} on {Real}-{Time} {Iteration} {NMPC}},
	volume = {53},
	issn = {2405-8963},
	doi = {10.1016/j.ifacol.2020.12.1195},
	abstract = {Reinforcement Learning (RL) has proven a stunning ability to learn optimal policies from data without any prior knowledge on the process. The main drawback of RL is that it is typically very difficult to guarantee stability and safety. On the other hand, Nonlinear Model Predictive Control (NMPC) is an advanced model-based control technique which does guarantee safety and stability, but only yields optimality for the nominal model. Therefore, it has been recently proposed to use NMPC as a function approximator within RL. While the ability of this approach to yield good performance has been demonstrated, the main drawback hindering its applicability is related to the computational burden of NMPC, which has to be solved to full convergence. In practice, however, computationally efficient algorithms such as the Real-Time Iteration (RTI) scheme are deployed in order to return an approximate NMPC solution in very short time. In this paper we bridge this gap by extending the existing theoretical framework to also cover RL based on RTI NMPC. We demonstrate the effectiveness of this new RL approach with a nontrivial example modeling a challenging nonlinear system subject to stochastic perturbations with the objective of optimizing an economic cost.},
	number = {2},
	urldate = {2024-03-07},
	journal = {IFAC-PapersOnLine},
	author = {Zanon, Mario and Kungurtsev, Vyacheslav and Gros, Sébastien},
	month = jan,
	year = {2020},
	pages = {5213--5218},
}

@misc{nota_is_2020,
	title = {Is the {Policy} {Gradient} a {Gradient}?},
	abstract = {The policy gradient theorem describes the gradient of the expected discounted return with respect to an agent's policy parameters. However, most policy gradient methods drop the discount factor from the state distribution and therefore do not optimize the discounted objective. What do they optimize instead? This has been an open question for several years, and this lack of theoretical clarity has lead to an abundance of misstatements in the literature. We answer this question by proving that the update direction approximated by most methods is not the gradient of any function. Further, we argue that algorithms that follow this direction are not guaranteed to converge to a "reasonable" fixed point by constructing a counterexample wherein the fixed point is globally pessimal with respect to both the discounted and undiscounted objectives. We motivate this work by surveying the literature and showing that there remains a widespread misunderstanding regarding discounted policy gradient methods, with errors present even in highly-cited papers published at top conferences.},
	urldate = {2024-02-22},
	publisher = {arXiv},
	author = {Nota, Chris and Thomas, Philip S.},
	month = feb,
	year = {2020},
	note = {arXiv:1906.07073 [cs, stat]},
	keywords = {arxiv link},
}

@article{martinsen_combining_2020,
	title = {Combining system identification with reinforcement learning-based {MPC}},
	volume = {53},
	number = {2},
	urldate = {2024-03-07},
	journal = {IFAC-PapersOnLine},
	author = {Martinsen, Andreas B. and Lekkas, Anastasios M. and Gros, Sébastien},
	year = {2020},
	note = {Publisher: Elsevier},
	pages = {8130--8135},
}

@inproceedings{karnchanachari_practical_2020,
	title = {Practical {Reinforcement} {Learning} {For} {MPC}: {Learning} from sparse objectives in under an hour on a real robot},
	shorttitle = {Practical {Reinforcement} {Learning} {For} {MPC}},
	abstract = {Model Predictive Control (MPC) is a powerful control technique that handles constraints, takes the system’s dynamics into account, and is optimal with respect to a given cost function. In practice, however, it often requires an expert to craft and tune this cost function and find trade-offs between different state penalties to satisfy simple high level objectives. In this paper, we use Reinforcement Learning and in particular value learning to approximate the value function given only high level objectives, which can be sparse and binary. Building upon previous works, we present improvements that allowed us to successfully deploy the method on a real world unmanned ground vehicle. Our experiments show that our method can learn the cost function from scratch and without human intervention, while reaching a performance level similar to that of an expert-tuned MPC. We perform a quantitative comparison of these methods with standard MPC approaches both in simulation and on the real robot.},
	language = {en},
	urldate = {2024-01-18},
	booktitle = {Proceedings of the 2nd {Conference} on {Learning} for {Dynamics} and {Control}},
	publisher = {PMLR},
	author = {Karnchanachari, Napat and Valls, Miguel Iglesia and Hoeller, David and Hutter, Marco},
	month = jul,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {211--224},
}

@inproceedings{hoeller_deep_2020,
	title = {Deep {Value} {Model} {Predictive} {Control}},
	abstract = {In this paper, we introduce an actor-critic algorithm called Deep Value Model Predictive Control (DMPC), which combines model-based trajectory optimization with value function estimation. The DMPC actor is a Model Predictive Control (MPC) optimizer with an objective function defined in terms of a value function estimated by the critic. We show that our MPC actor is an importance sampler, which minimizes an upper bound of the cross-entropy to the state distribution of the optimal sampling policy. In our experiments with a Ballbot system, we show that our algorithm can work with sparse and binary reward signals to efficiently solve obstacle avoidance and target reaching tasks. Compared to previous work, we show that including the value function in the running cost of the trajectory optimizer speeds up the convergence. We also discuss the necessary strategies to robustify the algorithm in practice.},
	language = {en},
	urldate = {2024-01-18},
	booktitle = {Proceedings of the {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Hoeller, David and Farshidian, Farbod and Hutter, Marco},
	month = may,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {990--1004},
}

@article{gros_safe_2020,
	title = {Safe reinforcement learning via projection on a safe set: {How} to achieve optimality?},
	volume = {53},
	shorttitle = {Safe reinforcement learning via projection on a safe set},
	number = {2},
	urldate = {2024-03-07},
	journal = {IFAC-PapersOnLine},
	author = {Gros, Sebastien and Zanon, Mario and Bemporad, Alberto},
	year = {2020},
	note = {Publisher: Elsevier},
	pages = {8076--8081},
}

@article{frison_hpipm_2020,
	series = {21st {IFAC} {World} {Congress}},
	title = {{HPIPM}: a high-performance quadratic programming framework for model predictive control⁎⁎{This} research was supported by the {German} {Federal} {Ministry} for {Economic} {Affairs} and {Energy} ({BMWi}) via eco4wind ({0324125B}) and {DyConPV} ({0324166B}), and by {DFG} via {Research} {Unit} {FOR} 2401.},
	volume = {53},
	issn = {2405-8963},
	shorttitle = {{HPIPM}},
	doi = {10.1016/j.ifacol.2020.12.073},
	abstract = {This paper introduces HPIPM, a high-performance framework for quadratic programming (QP), designed to provide building blocks to efficiently and reliably solve model predictive control problems. HPIPM currently supports three QP types, and provides interior point method (IPM) solvers as well (partial) condensing routines. In particular, the IPM for optimal control QPs is intended to supersede the HPMPC solver, and it largely improves robustness while keeping the focus on speed. Numerical experiments show that HPIPM reliably solves challenging QPs, and that it outperforms other state-of-the-art solvers in speed.},
	number = {2},
	urldate = {2024-01-18},
	journal = {IFAC-PapersOnLine},
	author = {Frison, Gianluca and Diehl, Moritz},
	month = jan,
	year = {2020},
	keywords = {embedded optimization, model predictive control, quadratic programming, software},
	pages = {6563--6569},
}

@article{feher_hierarchical_2020,
	title = {Hierarchical {Evasive} {Path} {Planning} {Using} {Reinforcement} {Learning} and {Model} {Predictive} {Control}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3031037},
	abstract = {Motion planning plays an essential role in designing self-driving functions for connected and autonomous vehicles. The methods need to provide a feasible trajectory for the vehicle to follow, fulﬁlling different requirements, such as safety, efﬁciency, and passenger comfort. In this area, algorithms must also meet strict real-time expectations, since, especially in an emergency, the decision time is limited, which raises a trade-off for the feasibility requirements. This article proposes a hierarchical path planning solution for evasive maneuvering, where a Twin Delayed DDPG reinforcement learning agent generates the parameters of a geometric path consisting of chlotoids and straight sections, and an underlying model predictive control loop fulﬁlls the trajectory following tasks. The method is applied to the automotive double lane-change test, a common emergency situation, comparing its results with human drivers’ performance using a dynamic simulation environment. Besides the test’s standardized parameters, a broader range of topological layouts is chosen, both for the training and performance evaluation. The results show that the proposed method highly outperforms human drivers, especially in challenging situations, while meeting the computational requirements, as the pre-trained neural network and path generation algorithm can provide a solution in an instant, based on the experience gained during the training process.},
	language = {en},
	urldate = {2024-02-22},
	journal = {IEEE Access},
	author = {Feher, Arpad and Aradi, Szilard and Becsi, Tamas},
	year = {2020},
	pages = {187470--187482},
}

@article{cao_deep_2020,
	series = {21st {IFAC} {World} {Congress}},
	title = {Deep {Neural} {Network} {Approximation} of {Nonlinear} {Model} {Predictive} {Control}},
	volume = {53},
	issn = {2405-8963},
	doi = {10.1016/j.ifacol.2020.12.538},
	abstract = {This paper focuses on developing effective computational methods to enable the real-time application of model predictive control (MPC) for nonlinear systems. To achieve this goal, we follow the idea of approximating the MPC control law with a Deep Neural Network (DNN). To train the deep neural network offline, we propose a new “optimize and train” method that combines the steps of data generation and neural network training into a single high-dimensional stochastic optimization problem. This approach directly optimizes the closed loop performance of the DNN controller over a finite horizon for a number of initial states. The large-scale optimization problem can be solved efficiently using parallel computing techniques. The benefits of this approach over the conventional “optimize then train” protocol is illustrated through numerical results.},
	number = {2},
	urldate = {2024-01-18},
	journal = {IFAC-PapersOnLine},
	author = {Cao, Yankai and Gopaluni, R. Bhushan},
	month = jan,
	year = {2020},
	keywords = {Deep Neural Networks, Model Predictive Control, Nonlinear Systems, Stochastic Optimization},
	pages = {11319--11324},
}

@article{beckenbach_q-learning_2020,
	title = {A {Q}-learning predictive control scheme with guaranteed stability},
	volume = {56},
	issn = {0947-3580},
	doi = {10.1016/j.ejcon.2020.03.001},
	abstract = {Model-based predictive controllers are used to tackle control tasks in which constraints on state, input or both need to be satisfied. These controllers commonly optimize a fixed finite-horizon cost, which relates to an infinite-horizon (IH) cost profile, while the resulting closed-loop under the predictive controller yields an in general suboptimal IH cost. To capture the optimal IH cost and the associated control policy, reinforcement learning methods, such as Q-learning, that approximate said cost via a parametric architecture can be employed. Conversely to predictive controllers, however, closed-loop stability has rarely been investigated under the approximation associated controller in explicit dependence of these parameters. It is the aim of this work to incorporate model-based Q-learning into a predictive control setup as to provide closed-loop stability in online learning, while eventually improving the performance of finite-horizon controllers. The proposed scheme provides nominal asymptotic stability and the observation was made that the suggested learning approach could in fact improve the performance against a baseline predictive controller.},
	urldate = {2024-04-15},
	journal = {European Journal of Control},
	author = {Beckenbach, Lukas and Osinenko, Pavel and Streif, Stefan},
	month = nov,
	year = {2020},
	keywords = {Cost shaping, Nominal stability, Predictive control, Q-Learning},
	pages = {167--178},
}

@inproceedings{zanon_practical_2019,
	title = {Practical reinforcement learning of stabilizing economic {MPC}},
	urldate = {2024-03-07},
	booktitle = {18th {European} {Control} {Conference} ({ECC})},
	publisher = {IEEE},
	author = {Zanon, Mario and Gros, Sébastien and Bemporad, Alberto},
	year = {2019},
	pages = {2258--2263},
}

@inproceedings{wang_exploring_2019,
	title = {Exploring {Model}-based {Planning} with {Policy} {Networks}},
	abstract = {Model-based reinforcement learning (MBRL) with model-predictive control or online planning has shown great potential for locomotion control tasks in both sample efficiency and asymptotic performance. Despite the successes, the existing planning methods search from candidate sequences randomly generated in the action space, which is inefficient in complex high-dimensional environments. In this paper, we propose a novel MBRL algorithm, model-based policy planning (POPLIN), that combines policy networks with online planning. More specifically, we formulate action planning at each time-step as an optimization problem using neural networks. We experiment with both optimization w.r.t. the action sequences initialized from the policy network, and also online optimization directly w.r.t. the parameters of the policy network. We show that POPLIN obtains state-of-the-art performance in the MuJoCo benchmarking environments, being about 3x more sample efficient than the state-of-the-art algorithms, such as PETS, TD3 and SAC. To explain the effectiveness of our algorithm, we show that the optimization surface in parameter space is smoother than in action space. Further more, we found the distilled policy network can be effectively applied without the expansive model predictive control during test time for some environments such as Cheetah. Code is released.},
	language = {en},
	urldate = {2024-01-17},
	author = {Wang, Tingwu and Ba, Jimmy},
	month = sep,
	year = {2019},
}

@inproceedings{saeed_distributed_2019,
	title = {Distributed {Nonlinear} {Model} {Predictive} {Control} and {Reinforcement} {Learning}},
	doi = {10.1109/ANZCC47194.2019.8945719},
	abstract = {Coordinating two or more dynamic systems such as autonomous vehicles or satellites in a distributed manner poses an important research challenge. Multiple approaches to this problem have been proposed including Nonlinear Model Predictive Control (NMPC) and its model-free counterparts in reinforcement learning (RL) literature such as Deep QNetwork (DQN). This initial study aims to compare and contrast the optimal control technique, NMPC, where the model is known, with the popular model-free RL method, DQN. Simple distributed variants of these for the specific problem of balancing and synchronising two highly unstable cart-pole systems are investigated numerically. We found that both NMPC and trained DQN work optimally under ideal model and small communication delays. While NMPC performs sub-optimally under a model-mismatch scenario, DQN performance naturally does not suffer from this. Distributed DQN needs a lot of realworld experience to be trained but once it is trained, it does not have to spend its time finding the optimal action at every time-step like NMPC. This illustrative comparison lays a foundation for hybrid approaches, which can be applied to complex multi-agent scenarios.},
	urldate = {2024-02-22},
	booktitle = {Australian \& {New} {Zealand} {Control} {Conference} ({ANZCC})},
	author = {Saeed, Ifrah and Alpcan, Tansu and Erfani, Sarah M. and Yilmaz, M. Berkay},
	month = nov,
	year = {2019},
	keywords = {Adaptation models, Dynamical systems, Numerical models, Optimal control, Predictive control, Reinforcement learning, System dynamics},
	pages = {1--3},
}

@incollection{li_chapter_2019,
	series = {Emerging {Methodologies} and {Applications} in {Modelling}},
	title = {Chapter 1 - {Basic} {Concepts}},
	isbn = {978-0-12-815372-7},
	abstract = {In Chapter 1, first, it studies stability analysis, including Lyapunov stability, Lyapunov asymptotic stability robust stability, Lyapunov uniform asymptotic stability, Lyapunov global asymptotic stability, Lyapunov instability, positive definite unction, Lyapunov function, Lyapunov stability theorem, and Lyapunov global uniform asymptotic stability theorem. Second, it studies nonlinear design tools, including adaptive control, sliding mode control, and U-model method.},
	urldate = {2024-02-23},
	booktitle = {Adaptive {Sliding} {Mode} {Neural} {Network} {Control} for {Nonlinear} {Systems}},
	publisher = {Academic Press},
	editor = {Li, Yang and Zhang, Jianhua and Wu, Qiong},
	month = jan,
	year = {2019},
	doi = {10.1016/B978-0-12-815372-7.00001-X},
	keywords = {Adaptive Control, Lyapunov Stability, Robust Stability, Sliding Mode Control, U-model Method},
	pages = {1--16},
}

@misc{hong_model-based_2019,
	title = {Model-based {Lookahead} {Reinforcement} {Learning}},
	doi = {10.48550/arXiv.1908.06012},
	abstract = {Model-based Reinforcement Learning (MBRL) allows data-efficient learning which is required in real world applications such as robotics. However, despite the impressive data-efficiency, MBRL does not achieve the final performance of state-of-the-art Model-free Reinforcement Learning (MFRL) methods. We leverage the strengths of both realms and propose an approach that obtains high performance with a small amount of data. In particular, we combine MFRL and Model Predictive Control (MPC). While MFRL's strength in exploration allows us to train a better forward dynamics model for MPC, MPC improves the performance of the MFRL policy by sampling-based planning. The experimental results in standard continuous control benchmarks show that our approach can achieve MFRL`s level of performance while being as data-efficient as MBRL.},
	urldate = {2024-01-17},
	publisher = {arXiv},
	author = {Hong, Zhang-Wei and Pajarinen, Joni and Peters, Jan},
	month = aug,
	year = {2019},
	note = {arXiv:1908.06012 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{greatwood_reinforcement_2019,
	title = {Reinforcement learning and model predictive control for robust embedded quadrotor guidance and control},
	volume = {43},
	issn = {1573-7527},
	doi = {10.1007/s10514-019-09829-4},
	abstract = {A new method for enabling a quadrotor micro air vehicle (MAV) to navigate unknown environments using reinforcement learning (RL) and model predictive control (MPC) is developed. An efficient implementation of MPC provides vehicle control and obstacle avoidance. RL is used to guide the MAV through complex environments where dead-end corridors may be encountered and backtracking is necessary. All of the presented algorithms were deployed on embedded hardware using automatic code generation from Simulink. Results are given for flight tests, demonstrating that the algorithms perform well with modest computing requirements and robust navigation.},
	language = {en},
	number = {7},
	urldate = {2024-02-21},
	journal = {Autonomous Robots},
	author = {Greatwood, Colin and Richards, Arthur G.},
	month = oct,
	year = {2019},
	keywords = {Exploration, Micro air vehicle, Model predictive control, Reinforcement learning},
	pages = {1681--1693},
}

@article{englert_software_2019,
	title = {A software framework for embedded nonlinear model predictive control using a gradient-based augmented {Lagrangian} approach ({GRAMPC})},
	volume = {20},
	issn = {1573-2924},
	doi = {10.1007/s11081-018-9417-2},
	abstract = {A nonlinear MPC framework is presented that is suitable for dynamical systems with sampling times in the (sub)millisecond range and that allows for an efficient implementation on embedded hardware. The algorithm is based on an augmented Lagrangian formulation with a tailored gradient method for the inner minimization problem. The algorithm is implemented in the software framework GRAMPC and is a fundamental revision of an earlier version. Detailed performance results are presented for a test set of benchmark problems and in comparison to other nonlinear MPC packages. In addition, runtime results and memory requirements for GRAMPC on ECU level demonstrate its applicability on embedded hardware.},
	language = {en},
	number = {3},
	urldate = {2024-01-18},
	journal = {Optimization and Engineering},
	author = {Englert, Tobias and Völz, Andreas and Mesmer, Felix and Rhein, Sönke and Graichen, Knut},
	month = sep,
	year = {2019},
	keywords = {Augmented Lagrangian method, Embedded optimization, Gradient method, Moving horizon estimation, Nonlinear model predictive control, Real-time implementation},
	pages = {769--809},
}

@inproceedings{deits_lvis_2019,
	address = {Montreal, QC, Canada},
	title = {{LVIS}: {Learning} from {Value} {Function} {Intervals} for {Contact}-{Aware} {Robot} {Controllers}},
	shorttitle = {{LVIS}},
	doi = {10.1109/ICRA.2019.8794352},
	abstract = {Guided policy search is a popular approach for training controllers for high-dimensional systems, but it has a number of pitfalls. Non-convex trajectory optimization has local minima, and non-uniqueness in the optimal policy itself can mean that independently-optimized samples do not describe a coherent policy from which to train. We introduce LVIS, which circumvents the issue of local minima through global mixed-integer optimization and the issue of non-uniqueness through learning the optimal value function rather than the optimal policy. To avoid the expense of solving the mixed-integer programs to full global optimality, we instead solve them only partially, extracting intervals containing the true cost-to-go from early termination of the branch-and-bound algorithm. These interval samples are used to weakly supervise the training of a neural net which approximates the true cost-to-go. Online, we use that learned cost-to-go as the terminal cost of a one-step model-predictive controller, which we solve via a small mixed-integer optimization. We demonstrate LVIS on piecewise affine models of a cart-pole system with walls and a planar humanoid robot and show that it can be applied to a fundamentally hard problem in feedback control\&\#x2013;control through contact.},
	urldate = {2024-01-18},
	booktitle = {International {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE Press},
	author = {Deits, Robin and Koolen, Twan and Tedrake, Russ},
	year = {2019},
	pages = {7762--7768},
}

@inproceedings{coulson_regularized_2019,
	title = {Regularized and {Distributionally} {Robust} {Data}-{Enabled} {Predictive} {Control}},
	doi = {10.1109/CDC40024.2019.9028943},
	abstract = {In this paper, we study a data-enabled predictive control (DeePC) algorithm applied to unknown stochastic linear time-invariant systems. The algorithm uses noise-corrupted input/output data to predict future trajectories and compute optimal control policies. To robustify against uncertainties in the input/output data, the control policies are computed to minimize a worst-case expectation of a given objective function. Using techniques from distributionally robust stochastic optimization, we prove that for certain objective functions, the worst-case optimization problem coincides with a regularized version of the DeePC algorithm. These results support the previously observed advantages of the regularized algorithm. We illustrate the robustness of the regularized algorithm through a numerical case study.},
	urldate = {2024-03-13},
	booktitle = {{IEEE} 58th {Conference} on {Decision} and {Control} ({CDC})},
	author = {Coulson, Jeremy and Lygeros, John and Dörfler, Florian},
	month = dec,
	year = {2019},
	note = {ISSN: 2576-2370},
	keywords = {Linear programming, Optimization, Prediction algorithms, Predictive control, Robustness, Stochastic processes, Trajectory},
	pages = {2696--2701},
}

@inproceedings{sun_fast_2018,
	title = {A {Fast} {Integrated} {Planning} and {Control} {Framework} for {Autonomous} {Driving} via {Imitation} {Learning}},
	doi = {10.1115/DSCC2018-9249},
	abstract = {Safety and efficiency are two key elements for planning and control in autonomous driving. Theoretically, model-based optimization methods, such as Model Predictive Control (MPC), can provide such optimal driving policies. Their computational complexity, however, grows exponentially with horizon length and number of surrounding vehicles. This makes them impractical for real-time implementation, particularly when nonlinear models are considered. To enable a fast and approximately optimal driving policy, we propose a safe imitation framework, which contains two hierarchical layers. The first layer, defined as the policy layer, is represented by a neural network that imitates a long-term expert driving policy via imitation learning. The second layer, called the execution layer, is a short-term model-based optimal controller that tracks and further fine-tunes the reference trajectories proposed by the policy layer with guaranteed short-term collision avoidance. Moreover, to reduce the distribution mismatch between the training set and the real world, Dataset Aggregation is utilized so that the performance of the policy layer can be improved from iteration to iteration. Several highway driving scenarios are demonstrated in simulations, and the results show that the proposed framework can achieve similar performance as sophisticated long-term optimization approaches but with significantly improved computational efficiency.},
	language = {en},
	urldate = {2024-04-16},
	publisher = {American Society of Mechanical Engineers Digital Collection},
	author = {Sun, Liting and Peng, Cheng and Zhan, Wei and Tomizuka, Masayoshi},
	month = nov,
	year = {2018},
}

@article{pon_kumar_deep_2018,
	series = {10th {IFAC} {Symposium} on {Advanced} {Control} of {Chemical} {Processes} {ADCHEM} 2018},
	title = {A {Deep} {Learning} {Architecture} for {Predictive} {Control}},
	volume = {51},
	issn = {2405-8963},
	doi = {10.1016/j.ifacol.2018.09.373},
	abstract = {Model predictive control (MPC) is a popular control strategy that computes control actions by solving an optimization problem in real-time. Uncertainty and nonlinearity of a process, and the non-convexity of the resulting optimization problem can make online implementation of MPC nontrivial. Consequently, MPC is most often used in processes where the time constants are large and/or high-performance computing support is available. We propose a deep neural network (DNN) controller architecture to reduce the computational cost of implementing an MPC. This is done by training a DNN controller on simulated input-output data from a well-designed MPC. The online implementation of a DNN controller does not require solving an optimization problem. Once the DNN is trained, the MPC is fully replaced with the DNN controller. The benefits of this approach are illustrated through a simulated example.},
	number = {18},
	urldate = {2024-01-18},
	journal = {IFAC-PapersOnLine},
	author = {Pon Kumar, Steven Spielberg and Tulsyan, Aditya and Gopaluni, Bhushan and Loewen, Philip},
	month = jan,
	year = {2018},
	keywords = {artificial intelligence, deep neural networks, model predictive control, optimization},
	pages = {512--517},
}

@article{hertneck_learning_2018,
	title = {Learning an {Approximate} {Model} {Predictive} {Controller} {With} {Guarantees}},
	volume = {2},
	issn = {2475-1456},
	doi = {10.1109/LCSYS.2018.2843682},
	abstract = {A supervised learning framework is proposed to approximate a model predictive controller (MPC) with reduced computational complexity and guarantees on stability and constraint satisfaction. The framework can be used for a wide class of nonlinear systems. Any standard supervised learning technique (e.g., neural networks) can be employed to approximate the MPC from samples. In order to obtain closed-loop guarantees for the learned MPC, a robust MPC design is combined with statistical learning bounds. The MPC design ensures robustness to inaccurate inputs within given bounds, and Hoeffding's Inequality is used to validate that the learned MPC satisfies these bounds with high confidence. The result is a closed-loop statistical guarantee on stability and constraint satisfaction for the learned MPC. The proposed learning-based MPC framework is illustrated on a nonlinear benchmark problem, for which we learn a neural network controller with guarantees.},
	number = {3},
	urldate = {2024-01-18},
	journal = {IEEE Control Systems Letters},
	author = {Hertneck, Michael and Köhler, Johannes and Trimpe, Sebastian and Allgöwer, Frank},
	month = jul,
	year = {2018},
	note = {Conference Name: IEEE Control Systems Letters},
	pages = {543--548},
}

@inproceedings{chen_approximating_2018,
	title = {Approximating {Explicit} {Model} {Predictive} {Control} {Using} {Constrained} {Neural} {Networks}},
	doi = {10.23919/ACC.2018.8431275},
	abstract = {This paper presents a method to compute an approximate explicit model predictive control (MPC) law using neural networks. The optimal MPC control law for constrained linear quadratic regulator (LQR) systems is piecewise affine on polytopes. However, computing this optimal control law becomes computationally intractable for large problems, and motivates the application of reinforcement learning techniques using neural networks with rectified linear units. We introduce a modified reinforcement learning policy gradient algorithm that utilizes knowledge of the system model to efficiently train the neural network. We guarantee that the network generates feasible control inputs by projecting onto polytope regions derived from the maximal control invariant set of the system. Finally, we present numerical examples that demonstrate the characteristics and performance of our algorithm.},
	urldate = {2024-01-18},
	booktitle = {2018 {Annual} {American} {Control} {Conference} ({ACC})},
	author = {Chen, Steven and Saulnier, Kelsey and Atanasov, Nikolay and Lee, Daniel D. and Kumar, Vijay and Pappas, George J. and Morari, Manfred},
	month = jun,
	year = {2018},
	pages = {1520--1527},
}

@article{andersson_sensitivity_2018,
	series = {6th {IFAC} {Conference} on {Nonlinear} {Model} {Predictive} {Control} {NMPC} 2018},
	title = {Sensitivity {Analysis} for {Nonlinear} {Programming} in {CasADi}⁎},
	volume = {51},
	issn = {2405-8963},
	doi = {10.1016/j.ifacol.2018.11.055},
	abstract = {We present an extension of the CasADi numerical optimization framework that allows arbitrary order NLP sensitivities to be calculated automatically and efficiently. The approach, which can be used together with any NLP solver available in CasADi, is based on a sparse QR factorization and an implementation of a primal-dual active set method. The whole toolchain is freely available as open-source software and allows generation of thread-safe, self-contained C code with small memory footprint. We illustrate the toolchain using three examples; a sparse QP, an optimal control problem and a parameter estimation problem.},
	number = {20},
	urldate = {2024-03-20},
	journal = {IFAC-PapersOnLine},
	author = {Andersson, Joel A. E. and Rawlings, James B.},
	month = jan,
	year = {2018},
	pages = {331--336},
}

@inproceedings{amos_differentiable_2018,
	title = {Differentiable {MPC} for {End}-to-end {Planning} and {Control}},
	volume = {31},
	abstract = {We present foundations for using Model Predictive Control (MPC) as a differentiable policy class for reinforcement learning. This provides one way of leveraging and combining the advantages of model-free and model-based approaches. Specifically, we differentiate through MPC by using the KKT conditions of the convex approximation at a fixed point of the controller. Using this strategy, we are able to learn the cost and dynamics of a controller via end-to-end learning. Our experiments focus on imitation learning in the pendulum and cartpole domains, where we learn the cost and dynamics terms of an MPC policy class. We show that our MPC policies are significantly more data-efficient than a generic neural network and that our method is superior to traditional system identification in a setting where the expert is unrealizable.},
	urldate = {2024-03-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Amos, Brandon and Jimenez, Ivan and Sacks, Jacob and Boots, Byron and Kolter, J. Zico},
	year = {2018},
}

@article{ahmed_comparison_2018,
	series = {10th {IFAC} {Symposium} on {Fault} {Detection}, {Supervision} and {Safety} for {Technical} {Processes} {SAFEPROCESS} 2018},
	title = {Comparison of {Model} {Predictive} and {Reinforcement} {Learning} {Methods} for {Fault} {Tolerant} {Control}},
	volume = {51},
	issn = {2405-8963},
	doi = {10.1016/j.ifacol.2018.09.583},
	abstract = {A desirable property in fault-tolerant controllers is adaptability to system changes as they evolve during systems operations. An adaptive controller does not require optimal control policies to be enumerated for possible faults. Instead it can approximate one in real-time. We present two adaptive fault-tolerant control schemes for a discrete time system based on hierarchical reinforcement learning. We compare their performance against a model predictive controller in presence of sensor noise and persistent faults. The controllers are tested on a fuel tank model of a C-130 plane. Our experiments demonstrate that reinforcement learning-based controllers perform more robustly than model predictive controllers under faults, partially observable system models, and varying sensor noise levels.},
	number = {24},
	urldate = {2024-02-22},
	journal = {IFAC-PapersOnLine},
	author = {Ahmed, Ibrahim and Khorasgani, Hamed and Biswas, Gautam},
	month = jan,
	year = {2018},
	keywords = {Reinforcement learning control, fault tolerance, hierarchical reinforcement learning, model predictive control, model-based control},
	pages = {233--240},
}

@article{gorges_relations_2017,
	series = {20th {IFAC} {World} {Congress}},
	title = {Relations between {Model} {Predictive} {Control} and {Reinforcement} {Learning}},
	volume = {50},
	issn = {2405-8963},
	doi = {10.1016/j.ifacol.2017.08.747},
	abstract = {In this paper relations between model predictive control and reinforcement learning are studied for discrete-time linear time-invariant systems with state and input constraints and a quadratic value function. The principles of model predictive control and reinforcement learning are reviewed in a tutorial manner. From model predictive control theory it is inferred that the optimal value function is piecewise quadratic on polyhedra and that the optimal policy is piecewise affine on polyhedra. Various ideas for exploiting the knowledge on the structure and the properties of the optimal value function and the optimal policy in reinforcement learning theory and practice are presented. The ideas can be used for deriving stability and feasibility criteria and for accelerating the learning process which can facilitate reinforcement learning for systems with high order, fast dynamics, and strict safety requirements.},
	number = {1},
	urldate = {2024-01-17},
	journal = {IFAC-PapersOnLine},
	author = {Görges, Daniel},
	month = jul,
	year = {2017},
	keywords = {Model predictive control, actor-critic structure, approximate dynamic programming, multi-parametric programming, reinforcement learning},
	pages = {4920--4928},
}

@inproceedings{williams_aggressive_2016,
	title = {Aggressive driving with model predictive path integral control},
	doi = {10.1109/ICRA.2016.7487277},
	abstract = {In this paper we present a model predictive control algorithm designed for optimizing non-linear systems subject to complex cost criteria. The algorithm is based on a stochastic optimal control framework using a fundamental relationship between the information theoretic notions of free energy and relative entropy. The optimal controls in this setting take the form of a path integral, which we approximate using an efficient importance sampling scheme. We experimentally verify the algorithm by implementing it on a Graphics Processing Unit (GPU) and apply it to the problem of controlling a fifth-scale Auto-Rally vehicle in an aggressive driving task.},
	urldate = {2024-01-17},
	booktitle = {{IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Williams, Grady and Drews, Paul and Goldfain, Brian and Rehg, James M. and Theodorou, Evangelos A.},
	month = may,
	year = {2016},
	pages = {1433--1440},
}

@article{mesbah_stochastic_2016,
	title = {Stochastic {Model} {Predictive} {Control}: {An} {Overview} and {Perspectives} for {Future} {Research}},
	volume = {36},
	issn = {1941-000X},
	shorttitle = {Stochastic {Model} {Predictive} {Control}},
	doi = {10.1109/MCS.2016.2602087},
	abstract = {Model predictive control (MPC) has demonstrated exceptional success for the high-performance control of complex systems. The conceptual simplicity of MPC as well as its ability to effectively cope with the complex dynamics of systems with multiple inputs and outputs, input and state/output constraints, and conflicting control objectives have made it an attractive multivariable constrained control approach. This article gives an overview of the main developments in the area of stochastic model predictive control (SMPC) in the past decade and provides the reader with an impression of the different SMPC algorithms and the key theoretical challenges in stochastic predictive control without undue mathematical complexity. The general formulation of a stochastic OCP is first presented, followed by an overview of SMPC approaches for linear and nonlinear systems. Suggestions of some avenues for future research in this rapidly evolving field concludes the article.},
	number = {6},
	urldate = {2024-01-17},
	journal = {IEEE Control Systems Magazine},
	author = {Mesbah, Ali},
	month = dec,
	year = {2016},
	note = {Conference Name: IEEE Control Systems Magazine},
	pages = {30--44},
}

@article{theodorou_nonlinear_2015,
	title = {Nonlinear {Stochastic} {Control} and {Information} {Theoretic} {Dualities}: {Connections}, {Interdependencies} and {Thermodynamic} {Interpretations}},
	volume = {17},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1099-4300},
	shorttitle = {Nonlinear {Stochastic} {Control} and {Information} {Theoretic} {Dualities}},
	doi = {10.3390/e17053352},
	abstract = {In this paper, we present connections between recent developments on the linearly-solvable stochastic optimal control framework with early work in control theory based on the fundamental dualities between free energy and relative entropy. We extend these connections to nonlinear stochastic systems with non-affine controls by using the generalized version of the Feynman–Kac lemma. We present alternative formulations of the linearly-solvable stochastic optimal control framework and discuss information theoretic and thermodynamic interpretations. On the algorithmic side, we present iterative stochastic optimal control algorithms and applications to nonlinear stochastic systems. We conclude with an overview of the frameworks presented and discuss limitations, differences and future directions.},
	language = {en},
	number = {5},
	urldate = {2024-01-17},
	journal = {Entropy},
	author = {Theodorou, Evangelos A.},
	month = may,
	year = {2015},
	note = {Number: 5
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Information Theory, Stochastic Optimal Control, Thermodynamics},
	pages = {3352--3375},
}

@inproceedings{ohtsuka_tutorial_2015,
	title = {A tutorial on {C}/{GMRES} and automatic code generation for nonlinear model predictive control},
	doi = {10.1109/ECC.2015.7330528},
	abstract = {This paper provides a tutorial on the continuation/ GMRES method (C/GMRES), which is a real-time optimization algorithm tailored for nonlinear model predictive control (NMPC) and a Maple version of AutoGenU, which is an automatic code generation system utilizing a symbolic computation language to simulate NMPC with C/GMRES. Once such settings of NMPC as the state equation and performance index are specified in a Maple worksheet, a problem-dependent C source file is automatically generated by symbolic computation, compiled, and executed. The worksheet also loads data files of simulation results and plots their time histories. This paper describes the usage and settings of AutoGenU for Maple.},
	urldate = {2024-01-18},
	booktitle = {2015 {European} {Control} {Conference} ({ECC})},
	author = {Ohtsuka, Toshiyuki},
	month = jul,
	year = {2015},
	pages = {73--86},
}

@article{ferreau_qpoases_2014,
	title = {{qpOASES}: a parametric active-set algorithm for quadratic programming},
	volume = {6},
	issn = {1867-2957},
	shorttitle = {{qpOASES}},
	doi = {10.1007/s12532-014-0071-1},
	abstract = {Many practical applications lead to optimization problems that can either be stated as quadratic programming (QP) problems or require the solution of QP problems on a lower algorithmic level. One relatively recent approach to solve QP problems are parametric active-set methods that are based on tracing the solution along a linear homotopy between a QP problem with known solution and the QP problem to be solved. This approach seems to make them particularly suited for applications where a-priori information can be used to speed-up the QP solution or where high solution accuracy is required. In this paper we describe the open-source C++ software package qpOASES, which implements a parametric active-set method in a reliable and efficient way. Numerical tests show that qpOASES can outperform other popular academic and commercial QP solvers on small- to medium-scale convex test examples of the Maros-Mészáros QP collection. Moreover, various interfaces to third-party software packages make it easy to use, even on embedded computer hardware. Finally, we describe how qpOASES can be used to compute critical points of nonconvex QP problems.},
	language = {en},
	number = {4},
	urldate = {2024-01-18},
	journal = {Mathematical Programming Computation},
	author = {Ferreau, Hans Joachim and Kirches, Christian and Potschka, Andreas and Bock, Hans Georg and Diehl, Moritz},
	month = dec,
	year = {2014},
	keywords = {65K05 (Mathematical programming methods), 90C20 (Quadratic programming), Active set method, Model predictive control, Parametric quadratic programming},
	pages = {327--363},
}

@inproceedings{zhong_value_2013,
	title = {Value function approximation and model predictive control},
	doi = {10.1109/ADPRL.2013.6614995},
	abstract = {Both global methods and on-line trajectory optimization methods are powerful techniques for solving optimal control problems; however, each has limitations. In order to mitigate the undesirable properties of each, we explore the possibility of combining the two. We explore two methods of deriving a descriptive final cost function to assist model predictive control (MPC) in selecting a good policy without having to plan as far into the future or having to fine-tune delicate cost functions. First, we exploit the large amount of data which is generated in MPC simulations (based on the receding horizon iterative LQG method) to learn, off-line, the global optimal value function for use as a final cost. We demonstrate that, while the global function approximation matches the value function well on some problems, there is relatively little improvement to the original MPC. Alternatively, we solve the Bellman equation directly using aggregation methods for linearly-solvable Markov Decision Processes to obtain an approximation to the value function and the optimal policy. Using both pieces of information in the MPC framework, we find controller performance of similar quality to MPC alone with long horizon, but now we may drastically shorten the horizon. Implementation of these methods shows that Bellman equation-based methods and on-line trajectory methods can be combined in real applications to the benefit of both.},
	urldate = {2024-01-18},
	booktitle = {{IEEE} {Symposium} on {Adaptive} {Dynamic} {Programming} and {Reinforcement} {Learning} ({ADPRL})},
	author = {Zhong, Mingyuan and Johnson, Mikala and Tassa, Yuval and Erez, Tom and Todorov, Emanuel},
	month = apr,
	year = {2013},
	note = {ISSN: 2325-1867},
	pages = {100--107},
}

@inproceedings{levine_guided_2013,
	title = {Guided {Policy} {Search}},
	abstract = {Direct policy search can effectively scale to high-dimensional systems, but complex policies with hundreds of parameters often present a challenge for such methods, requiring numerous samples and often falling into poor local optima. We present a guided policy search algorithm that uses trajectory optimization to direct policy learning and avoid poor local optima. We show how differential dynamic programming can be used to generate suitable guiding samples, and describe a regularized importance sampled policy optimization that incorporates these samples into the policy search. We evaluate the method by learning neural network controllers for planar swimming, hopping, and walking, as well as simulated 3D humanoid running.},
	language = {en},
	urldate = {2024-01-18},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Levine, Sergey and Koltun, Vladlen},
	month = may,
	year = {2013},
	note = {ISSN: 1938-7228},
	pages = {1--9},
}

@inproceedings{theodorou_relative_2012,
	title = {Relative entropy and free energy dualities: {Connections} to {Path} {Integral} and {KL} control},
	shorttitle = {Relative entropy and free energy dualities},
	doi = {10.1109/CDC.2012.6426381},
	abstract = {This paper integrates recent work on Path Integral (PI) and Kullback Leibler (KL) divergence stochastic optimal control theory with earlier work on risk sensitivity and the fundamental dualities between free energy and relative entropy. We derive the path integral optimal control framework and its iterative version based on the aforemetioned dualities. The resulting formulation of iterative path integral control is valid for general feedback policies and in contrast to previous work, it does not rely on pre-specified policy parameterizations. The derivation is based on successive applications of Girsanov's theorem and the use of Radon-Nikodým derivative as applied to diffusion processes due to the change of measure in the stochastic dynamics. We compare the PI control derived based on Dynamic Programming with PI based on the duality between free energy and relative entropy. Moreover we extend our analysis on the applicability of the relationship between free energy and relative entropy to optimal control of markov jump diffusions processes. Furthermore, we present the links between KL stochastic optimal control and the aforementioned dualities and discuss its generalizability.},
	urldate = {2024-01-17},
	booktitle = {{IEEE} 51st {IEEE} {Conference} on {Decision} and {Control} ({CDC})},
	author = {Theodorou, Evangelos A. and Todorov, Emanuel},
	month = dec,
	year = {2012},
	note = {ISSN: 0743-1546},
	pages = {1466--1473},
}

@book{grancharova_explicit_2012,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Control} and {Information} {Sciences}},
	title = {Explicit {Nonlinear} {Model} {Predictive} {Control}: {Theory} and {Applications}},
	volume = {429},
	isbn = {978-3-642-28779-4 978-3-642-28780-0},
	shorttitle = {Explicit {Nonlinear} {Model} {Predictive} {Control}},
	language = {en},
	urldate = {2024-01-18},
	publisher = {Springer},
	author = {Grancharova, Alexandra and Johansen, Tor Arne},
	year = {2012},
	doi = {10.1007/978-3-642-28780-0},
	keywords = {Constrained Nonlinear Systems, Interconnected Nonlinear Systems, Nonlinear Model Predictive Control, Parametric Programming, Systems with Quantized Inputs, complexity},
}

@article{houska_acado_2011,
	title = {{ACADO} toolkit—{An} open-source framework for automatic control and dynamic optimization},
	volume = {32},
	copyright = {Copyright © 2010 John Wiley \& Sons, Ltd.},
	issn = {1099-1514},
	doi = {10.1002/oca.939},
	abstract = {In this paper the software environment and algorithm collection ACADO Toolkit is presented, which implements tools for automatic control and dynamic optimization. It provides a general framework for using a great variety of algorithms for direct optimal control, including model predictive control as well as state and parameter estimation. The ACADO Toolkit is implemented as a self-contained C++ code, while the object-oriented design allows for convenient coupling of existing optimization packages and for extending it with user-written optimization routines. We discuss details of the software design of the ACADO Toolkit 1.0 and describe its main software modules. Along with that we highlight a couple of algorithmic features, in particular its functionality to handle symbolic expressions. The user-friendly syntax of the ACADO Toolkit to set up optimization problems is illustrated with two tutorial examples: an optimal control and a parameter estimation problem. Copyright © 2010 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {3},
	journal = {Optimal Control Applications and Methods},
	author = {Houska, Boris and Ferreau, Hans Joachim and Diehl, Moritz},
	year = {2011},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/oca.939},
	keywords = {dynamic optimization, model predictive control, optimization software, parameter estimation},
	pages = {298--312},
}

@article{ernst_reinforcement_2009,
	title = {Reinforcement {Learning} {Versus} {Model} {Predictive} {Control}: {A} {Comparison} on a {Power} {System} {Problem}},
	volume = {39},
	issn = {1941-0492},
	shorttitle = {Reinforcement {Learning} {Versus} {Model} {Predictive} {Control}},
	doi = {10.1109/TSMCB.2008.2007630},
	abstract = {This paper compares reinforcement learning (RL) with model predictive control (MPC) in a unified framework and reports experimental results of their application to the synthesis of a controller for a nonlinear and deterministic electrical power oscillations damping problem. Both families of methods are based on the formulation of the control problem as a discrete-time optimal control problem. The considered MPC approach exploits an analytical model of the system dynamics and cost function and computes open-loop policies by applying an interior-point solver to a minimization problem in which the system dynamics are represented by equality constraints. The considered RL approach infers in a model-free way closed-loop policies from a set of system trajectories and instantaneous cost values by solving a sequence of batch-mode supervised learning problems. The results obtained provide insight into the pros and cons of the two approaches and show that RL may certainly be competitive with MPC even in contexts where a good deterministic system model is available.},
	number = {2},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
	author = {Ernst, Damien and Glavic, Mevludin and Capitanescu, Florin and Wehenkel, Louis},
	month = apr,
	year = {2009},
	note = {Conference Name: IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
	keywords = {Approximate dynamic programming (ADP), Control system synthesis, Damping, Learning, Open loop systems, Optimal control, Power system control, Power system modeling, Power systems, Predictive control, Predictive models, electric power oscillations damping, fitted Q iteration, interior-point method (IPM), model predictive control (MPC), reinforcement learning (RL), tree-based supervised learning (SL)},
	pages = {517--529},
}

@article{wachter_implementation_2006,
	title = {On the implementation of an interior-point filter line-search algorithm for large-scale nonlinear programming},
	volume = {106},
	issn = {1436-4646},
	doi = {10.1007/s10107-004-0559-y},
	abstract = {We present a primal-dual interior-point algorithm with a filter line-search method for nonlinear programming. Local and global convergence properties of this method were analyzed in previous work. Here we provide a comprehensive description of the algorithm, including the feasibility restoration phase for the filter method, second-order corrections, and inertia correction of the KKT matrix. Heuristics are also considered that allow faster performance. This method has been implemented in the IPOPT code, which we demonstrate in a detailed numerical study based on 954 problems from the CUTEr test set. An evaluation is made of several line-search options, and a comparison is provided with two state-of-the-art interior-point codes for nonlinear programming.},
	language = {en},
	number = {1},
	urldate = {2024-01-18},
	journal = {Mathematical Programming},
	author = {Wächter, Andreas and Biegler, Lorenz T.},
	month = mar,
	year = {2006},
	keywords = {49M37, 65K05, 90C30, 90C51, Barrier method, Filter method, Interior-point method, Line search, Nonconvex constrained optimization, Nonlinear programming},
	pages = {25--57},
}

@article{akesson_neural_2006,
	title = {A neural network model predictive controller},
	volume = {16},
	issn = {0959-1524},
	doi = {10.1016/j.jprocont.2006.06.001},
	abstract = {A neural network controller is applied to the optimal model predictive control of constrained nonlinear systems. The control law is represented by a neural network function approximator, which is trained to minimize a control-relevant cost function. The proposed procedure can be applied to construct controllers with arbitrary structures, such as optimal reduced-order controllers and decentralized controllers.},
	number = {9},
	urldate = {2024-01-18},
	journal = {Journal of Process Control},
	author = {Åkesson, Bernt M. and Toivonen, Hannu T.},
	month = oct,
	year = {2006},
	keywords = {Model predictive control, Neural networks, Nonlinear control},
	pages = {937--946},
}

@article{kappen_linear_2005,
	title = {Linear {Theory} for {Control} of {Nonlinear} {Stochastic} {Systems}},
	volume = {95},
	doi = {10.1103/PhysRevLett.95.200201},
	abstract = {We address the role of noise and the issue of efficient computation in stochastic optimal control problems. We consider a class of nonlinear control problems that can be formulated as a path integral and where the noise plays the role of temperature. The path integral displays symmetry breaking and there exists a critical noise value that separates regimes where optimal control yields qualitatively different solutions. The path integral can be computed efficiently by Monte Carlo integration or by a Laplace approximation, and can therefore be used to solve high dimensional stochastic control problems.},
	number = {20},
	urldate = {2024-01-17},
	journal = {Physical Review Letters},
	author = {Kappen, Hilbert J.},
	month = nov,
	year = {2005},
	note = {Publisher: American Physical Society},
	pages = {200201},
}

@article{bemporad_model_2002,
	title = {Model predictive control based on linear programming - the explicit solution},
	volume = {47},
	issn = {1558-2523},
	doi = {10.1109/TAC.2002.805688},
	number = {12},
	urldate = {2024-01-18},
	journal = {IEEE Transactions on Automatic Control},
	author = {Bemporad, A. and Borrelli, F. and Morari, M.},
	month = dec,
	year = {2002},
	note = {Conference Name: IEEE Transactions on Automatic Control},
	pages = {1974--1985},
}

@inproceedings{morimoto_robust_2000,
	title = {Robust {Reinforcement} {Learning}},
	volume = {13},
	abstract = {This paper proposes a  new  reinforcement  learning  (RL)  paradigm  that explicitly takes into account input disturbance as well as mod(cid:173) eling errors.  The use of environmental models  in  RL  is  quite pop(cid:173) ular  for  both  off-line  learning  by  simulations  and  for  on-line  ac(cid:173) tion planning.  However, the difference between the model and the  real environment can lead to unpredictable, often unwanted results.  Based on the theory of H oocontrol, we consider a  differential game  in  which  a  'disturbing'  agent  (disturber)  tries  to  make  the  worst  possible  disturbance  while  a  'control'  agent  (actor)  tries  to  make  the best control input.  The problem is formulated as finding a min(cid:173) max solution of a  value function that takes into account the norm  of the output deviation and the norm of the disturbance.  We derive  on-line  learning  algorithms  for  estimating  the  value  function  and  for  calculating the worst disturbance and the best  control in refer(cid:173) ence to the value function.  We  tested the paradigm, which we call  "Robust  Reinforcement  Learning  (RRL),"  in  the task  of inverted  pendulum.  In  the  linear  domain,  the  policy  and  the  value  func(cid:173) tion learned by the on-line algorithms coincided with those derived  analytically  by the linear  H ootheory.  For a  fully  nonlinear  swing(cid:173) up task, the control by RRL  achieved  robust  performance against  changes in the pendulum weight  and friction  while a  standard RL  control could not  deal with such environmental changes.},
	urldate = {2024-03-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Morimoto, Jun and Doya, Kenji},
	year = {2000},
}

@article{parisini_receding-horizon_1995,
	title = {A receding-horizon regulator for nonlinear systems and a neural approximation},
	volume = {31},
	issn = {0005-1098},
	doi = {10.1016/0005-1098(95)00044-W},
	abstract = {A receding-horizon (RH) optimal control scheme for a discrete-time nonlinear dynamic system is presented. A nonquadratic cost function is considered, and constraints are imposed on both the state and control vectors. Two main contributions are reported. The first consists in deriving a stabilizing regulator by adding a proper terminal penalty function to the process cost. The control vector is generated by means of a feedback control law computed off line instead of computing it on line, as is done for existing RH regulators. The off-line computation is performed by approximating the RH regulator by means of a multilayer feedforward neural network (this is the second contribution of the paper). Bounds to this approximation are established. Simulation results show the effectiveness of the proposed approach.},
	number = {10},
	urldate = {2024-01-18},
	journal = {Automatica},
	author = {Parisini, T. and Zoppoli, R.},
	month = oct,
	year = {1995},
	keywords = {Optimal control, neural networks, nonlinear systems, receding-horizon regulators, stabilization},
	pages = {1443--1451},
}

@article{williams_simple_1992,
	title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
	volume = {8},
	issn = {1573-0565},
	doi = {10.1007/BF00992696},
	abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
	language = {en},
	number = {3},
	urldate = {2024-03-07},
	journal = {Machine Learning},
	author = {Williams, Ronald J.},
	month = may,
	year = {1992},
	pages = {229--256},
}

@article{watkins_q-learning_1992,
	title = {Q-learning},
	volume = {8},
	issn = {1573-0565},
	doi = {10.1007/BF00992698},
	abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
	language = {en},
	number = {3},
	urldate = {2020-08-09},
	journal = {Machine Learning},
	author = {Watkins, Christopher J. C. H. and Dayan, Peter},
	month = may,
	year = {1992},
	pages = {279--292},
}

@book{ljung_system_1999,
	title = {System {Identification}: {Theory} for the {User}},
	isbn = {978-0-13-656695-3},
	shorttitle = {System {Identification}},
	abstract = {Lennart Ljung's System Identification: Theory for the User is a complete, coherent description of the theory, methodology, and practice of System Identification. This completely revised Second Edition introduces subspace methods, methods that utilize frequency domain data, and general non-linear black box methods, including neural networks and neuro-fuzzy modeling. The book contains many new computer-based examples designed for Ljung's market-leading software, System Identification Toolbox for MATLAB. Ljung combines careful mathematics, a practical understanding of real-world applications, and extensive exercises. He introduces both black-box and tailor-made models of linear as well as non-linear systems, and he describes principles, properties, and algorithms for a variety of identification techniques.},
	language = {en},
	publisher = {Prentice Hall PTR},
	author = {Ljung, Lennart},
	year = {1999},
	note = {Google-Books-ID: nHFoQgAACAAJ},
	keywords = {Science / System Theory, Technology / Engineering / Electrical},
}

@book{sutton_reinforcement_2018,
	address = {Cambridge, Massachusetts},
	edition = {Second edition},
	series = {Adaptive computation and machine learning series},
	title = {Reinforcement learning: an introduction},
	isbn = {978-0-262-03924-6},
	shorttitle = {Reinforcement learning},
	abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
	language = {en},
	publisher = {The MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	year = {2018},
}

@book{puterman_markov_2005,
	address = {Hoboken, NJ},
	series = {Wiley series in probability and statistics},
	title = {Markov decision processes: discrete stochastic dynamic programming},
	isbn = {978-0-471-72782-8},
	shorttitle = {Markov decision processes},
	language = {eng},
	publisher = {Wiley-Interscience},
	author = {Puterman, Martin L.},
	year = {2005},
	note = {OCLC: 254152847},
}

@inproceedings{bemporad_robust_1999,
	address = {London},
	series = {Lecture {Notes} in {Control} and {Information} {Sciences}},
	title = {Robust model predictive control: {A} survey},
	isbn = {978-1-84628-538-7},
	shorttitle = {Robust model predictive control},
	doi = {10.1007/BFb0109870},
	abstract = {This paper gives an overview of robustness in Model Predictive Control (MPC). After reviewing the basic concepts of MPC, we survey the uncertainty descriptions considered in the MPC literature, and the techniques proposed for robust constraint handling, stability, and performance. The key concept of “closedloop prediction” is discussed at length. The paper concludes with some comments on future research directions.},
	language = {en},
	booktitle = {Robustness in identification and control},
	publisher = {Springer},
	author = {Bemporad, Alberto and Morari, Manfred},
	editor = {Garulli, A. and Tesi, A.},
	year = {1999},
	keywords = {Generalize Predictive Control, Model Predictive Control, Model Predictive Control Algorithm, Robust Stability, Stability Constraint},
	pages = {207--226},
}

@article{singh_reinforcement_2022,
	title = {Reinforcement learning in robotic applications: a comprehensive survey},
	volume = {55},
	shorttitle = {Reinforcement learning in robotic applications},
	doi = {10.1007/s10462-021-09997-9},
	abstract = {In recent trends, artificial intelligence (AI) is used for the creation of complex automated control systems. Still, researchers are trying to make a completely autonomous system that resembles human beings. Researchers working in AI think that there is a strong connection present between the learning pattern of human and AI. They have analyzed that machine learning (ML) algorithms can effectively make self-learning systems. ML algorithms are a sub-field of AI in which reinforcement learning (RL) is the only available methodology that resembles the learning mechanism of the human brain. Therefore, RL must take a key role in the creation of autonomous robotic systems. In recent years, RL has been applied on many platforms of the robotic systems like an air-based, under-water, land-based, etc., and got a lot of success in solving complex tasks. In this paper, a brief overview of the application of reinforcement algorithms in robotic science is presented. This survey offered a comprehensive review based on segments as (1) development of RL (2) types of RL algorithm like; Actor-Critic, DeepRL, multi-agent RL and Human-centered algorithm (3) various applications of RL in robotics based on their usage platforms such as land-based, water-based and air-based, (4) RL algorithms/mechanism used in robotic applications. Finally, an open discussion is provided that potentially raises a range of future research directions in robotics. The objective of this survey is to present a guidance point for future research in a more meaningful direction.},
	journal = {Artificial Intelligence Review},
	author = {Singh, Bharat and Kumar, Rajesh and Singh, Vinay},
	month = feb,
	year = {2022},
}

@inproceedings{ernst_iteratively_2003,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Iteratively {Extending} {Time} {Horizon} {Reinforcement} {Learning}},
	isbn = {978-3-540-39857-8},
	doi = {10.1007/978-3-540-39857-8_11},
	abstract = {Reinforcement learning aims to determine an (infinite time horizon) optimal control policy from interaction with a system. It can be solved by approximating the so-called Q-function from a sample of four-tuples (xt, ut, rt, xt + 1) where xtdenotes the system state at time t, utthe control action taken, rtthe instantaneous reward obtained and xt + 1 the successor state of the system, and by determining the optimal control from the Q-function. Classical reinforcement learning algorithms use an ad hoc version of stochastic approximation which iterates over the Q-function approximations on a four-tuple by four-tuple basis. In this paper, we reformulate this problem as a sequence of batch mode supervised learning problems which in the limit converges to (an approximation of) the Q-function. Each step of this algorithm uses the full sample of four-tuples gathered from interaction with the system and extends by one step the horizon of the optimality criterion. An advantage of this approach is to allow the use of standard batch mode supervised learning algorithms, instead of the incremental versions used up to now. In addition to a theoretical justification the paper provides empirical tests in the context of the “Car on the Hill” control problem based on the use of ensembles of regression trees. The resulting algorithm is in principle able to handle efficiently large scale reinforcement learning problems.},
	language = {en},
	booktitle = {Machine {Learning}: {ECML} 2003},
	publisher = {Springer},
	author = {Ernst, Damien and Geurts, Pierre and Wehenkel, Louis},
	editor = {Lavrač, Nada and Gamberger, Dragan and Blockeel, Hendrik and Todorovski, Ljupčo},
	year = {2003},
	pages = {96--107},
}

@book{bertsekas_neuro-dynamic_1996,
	address = {Belmont, Mass},
	series = {Optimization and neural computation series},
	title = {Neuro-dynamic programming},
	isbn = {978-1-886529-10-6},
	publisher = {Athena Scientific},
	author = {Bertsekas, Dimitri P. and Tsitsiklis, John N.},
	year = {1996},
}

@article{kouzoupis_recent_2018,
	title = {Recent advances in quadratic programming algorithms for nonlinear model predictive control},
	volume = {46},
	number = {4},
	journal = {Vietnam Journal of Mathematics},
	author = {Kouzoupis, D. and Frison, G. and Zanelli, A. and Diehl, M.},
	year = {2018},
	keywords = {syscop-public},
	pages = {863--882},
}

@inproceedings{bemporad_explicit_2000,
	title = {The explicit solution of model predictive control via multiparametric quadratic programming},
	volume = {2},
	booktitle = {Proceedings of the 2000 {American} {Control} {Conference}. {ACC} ({IEEE} {Cat}. {No}. {00CH36334})},
	publisher = {IEEE},
	author = {Bemporad, Alberto and Morari, Manfred and Dua, Vivek and Pistikopoulos, Efstratios N},
	year = {2000},
	pages = {872--876},
}

@incollection{bock_constrained_2007,
	title = {Constrained {Optimal} {Feedback} {Control} of {Systems} {Governed} by {Large} {Differential} {Algebraic} {Equations}},
	booktitle = {Real-{Time} and {Online} {PDE}-{Constrained} {Optimization}},
	publisher = {SIAM},
	author = {Bock, H. G. and Diehl, M. and Kostina, E. A. and Schlöder, J. P.},
	year = {2007},
	doi = {10.1137/1.9780898718935.ch1},
	keywords = {agbock optec optimal control NMPC DAE multiple shooting syscop-public},
	pages = {3--22},
}

@book{diehl_real-time_2002,
	address = {Düsseldorf},
	series = {Fortschritt-{Berichte} {VDI} {Reihe} 8, {Meß}-, {Steuerungs}- und {Regelungstechnik}},
	title = {Real-{Time} {Optimization} for {Large} {Scale} {Nonlinear} {Processes}},
	volume = {920},
	publisher = {VDI Verlag},
	author = {Diehl, M.},
	year = {2002},
	keywords = {agbock NMPC chemistry multiple shooting optimal control syscop-public},
}

@book{kouvaritakis_model_2015,
	address = {Cham Heidelberg New York Dordrecht London},
	edition = {1st ed. 2016 edition},
	title = {Model {Predictive} {Control}: {Classical}, {Robust} and {Stochastic}},
	isbn = {978-3-319-24851-6},
	shorttitle = {Model {Predictive} {Control}},
	abstract = {For the first time, a textbook that brings together classical predictive control with treatment of up-to-date robust and stochastic techniques.Model Predictive Control describes the development of tractable algorithms for uncertain, stochastic, constrained systems. The starting point is classical predictive control and the appropriate formulation of performance objectives and constraints to provide guarantees of closed-loop stability and performance. Moving on to robust predictive control, the text explains how similar guarantees may be obtained for cases in which the model describing the system dynamics is subject to additive disturbances and parametric uncertainties. Open- and closed-loop optimization are considered and the state of the art in computationally tractable methods based on uncertainty tubes presented for systems with additive model uncertainty. Finally, the tube framework is also applied to model predictive control problems involving hard or probabilistic constraints for the cases of multiplicative and stochastic model uncertainty. The book provides:extensive use of illustrative examples;sample problems; anddiscussion of novel control applications such as resource allocation for sustainable development and turbine-blade control for maximized power capture with simultaneously reduced risk of turbulence-induced damage.Graduate students pursuing courses in model predictive control or more generally in advanced or process control and senior undergraduates in need of a specialized treatment will find Model Predictive Control an invaluable guide to the state of the art in this important subject. For the instructor it provides an authoritative resource for the construction of courses.},
	language = {English},
	publisher = {Springer},
	author = {Kouvaritakis, Basil and Cannon, Mark},
	month = dec,
	year = {2015},
}

@book{bertsekas_reinforcement_2019,
	address = {Belmont, Massachusetts},
	edition = {First Edition},
	title = {Reinforcement {Learning} and {Optimal} {Control}},
	isbn = {978-1-886529-39-7},
	abstract = {This book considers large and challenging multistage decision problems, which can be solved in principle by dynamic programming, but their exact solution is computationally intractable. It can be used as a textbook or for self-study in conjunction with instructional videos and slides, and other supporting material, which are available from the author's website. The book discusses solution methods that rely on approximations to produce suboptimal policies with adequate performance. These methods are known by several essentially equivalent names: reinforcement learning, approximate dynamic programming, and neuro-dynamic programming. They underlie, among others, the recent impressive successes of self-learning in the context of games such as chess and Go. One of the aims of the book is to explore the common boundary between artificial intelligence and optimal control, and to form a bridge that is accessible by workers with background in either field. Another aim is to organize coherently the broad mosaic of methods that have proved successful in practice while having a solid theoretical and/or logical foundation. This may help researchers and practitioners to find their way through the maze of competing ideas that constitute the current state of the art. The mathematical style of this book is somewhat different than other books by the same author. While we provide a rigorous, albeit short, mathematical account of the theory of finite and infinite horizon dynamic programming, and some fundamental approximation methods, we rely more on intuitive explanations and less on proof-based insights. We also illustrate the methodology with many example algorithms and applications.},
	language = {English},
	publisher = {Athena Scientific},
	author = {Bertsekas, Dimitri},
	month = jul,
	year = {2019},
}
