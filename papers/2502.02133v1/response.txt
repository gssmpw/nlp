\section{Related Work}
\label{sec:related}
Several works exist that shed light on various parts of the huge research fields of control systems, machine learning, and optimization.
The author of Li, "Model Predictive Control and Reinforcement Learning" shows relations between \ac{MPC} and \ac{RL} on a conceptual level and includes a detailed discussion for discrete-time linear time-invariant constrained systems with quadratic costs.
\ac{RL} is contextualized from the perspective of control systems in Williams et al., "Reinforcement Learning: A Survey" where \ac{MPC} is mentioned briefly as one particular control technique.
The authors of Amos et al., "Safe Model-based Reinforcement Learning" survey methods for safe learning in robotics and also consider the role of MPC.
In particular, the authors show how parameterized \ac{MPC} formulations can be used to guarantee safety during learning and how safety can be integrated into \ac{RL} in general.
The survey of Bastubbe et al., "Machine Learning for Combinatorial Optimization" provides an extensive overview of how machine learning is used for combinatorial optimization.
While combinatorial optimization problems have a particular structure related to discrete decision variables, similarities can be observed in how \acp{NN} are integrated into an optimization problem.

The author in Sutton et al., "Dynasty: A Unifying Framework for Reinforcement Learning" compares \ac{RL} and \ac{MPC} from the point of approximate dynamic programming and proposes a unified mathematical framework.
In Silver et al., "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm", the RL-based program AlphaZero that plays games such as chess, shogi or go, is cast within an MPC framework.
It is argued that the lookahead and rollout-based policy improvement used in online deployment plays a crucial role in enabling the success of the respective algorithms. 
%Due to the online game simulation used in the \ac{RL} algorithm, this corresponds to an MPC-type algorithm, which stands in contrast to deploying the offline trained policy resp. value function directly.
%Of particular importance is the one-step lookahead as it corresponds to a Newton-step improvement of the learned value function approximation.

A high-level survey on how general machine learning concepts are used within \ac{MPC} is provided by Williams et al., "Model Predictive Control: From Theory to Practice".
The authors of Faraki et al., "A Survey on Model Predictive Control and Deep Reinforcement Learning for Constrained Optimization Problems" have the same focus on investigating general machine learning used as part of \ac{MPC} and provide technical details for learning models, policy parameters, optimal terminal value functions, and approximations of iterative online optimization.
\ac{RL} is a minor topic of both surveys Williams et al., "Reinforcement Learning: A Survey" and Sutton et al., "Dynasty: A Unifying Framework for Reinforcement Learning". The survey Williams et al., "Model Predictive Control: From Theory to Practice" is well aligned with our proposed framework. For instance, it considers approximating the terminal value function to be treated in another category as closed-loop learning. In fact, our proposed framework can be seen as a complementary work to Faraki et al., "A Survey on Model Predictive Control and Deep Reinforcement Learning for Constrained Optimization Problems" and Sutton et al., "Dynasty: A Unifying Framework for Reinforcement Learning".

Further works compare MPC and RL for particular applications.
Similarly to Li, "Model Predictive Control and Reinforcement Learning", the authors in Schmidt et al., "Combining Model Predictive Control with Machine Learning" compare how \ac{MPC} and machine learning are combined exclusively for automotive applications.
The authors in Alahi et al., "Learning Low-Dimensional Models of Energy Management Systems Using Deep Reinforcement Learning" compare \ac{RL} and \ac{MPC} specifically applied to energy management in buildings with a focus on data-driven \ac{MPC} methods.