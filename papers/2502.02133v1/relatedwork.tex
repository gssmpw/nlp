\section{Related Work}
\label{sec:related}
Several works exist that shed light on various parts of the huge research fields of control systems, machine learning, and optimization.
The author of~\cite{gorges_relations_2017} shows relations between \ac{MPC} and \ac{RL} on a conceptual level and includes a detailed discussion for discrete-time linear time-invariant constrained systems with quadratic costs.
\ac{RL} is contextualized from the perspective of control systems in~\cite{recht_tour_2019} where \ac{MPC} is mentioned briefly as one particular control technique.
The authors of~\cite{brunke_safe_2022} survey methods for safe learning in robotics and also consider the role of MPC.
In particular, the authors show how parameterized \ac{MPC} formulations can be used to guarantee safety during learning and how safety can be integrated into \ac{RL} in general.
The survey of~\cite{bengio_machine_2021} provides an extensive overview of how machine learning is used for combinatorial optimization.
While combinatorial optimization problems have a particular structure related to discrete decision variables, similarities can be observed in how \acp{NN} are integrated into an optimization problem.

The author in~\cite{bertsekas_reinforcement_2019} compares \ac{RL} and \ac{MPC} from the point of approximate dynamic programming and proposes a unified mathematical framework.
In \cite{bertsekas_lessons_2022,bertsekas_newtons_2022}, the RL-based program AlphaZero \cite{silver_mastering_2017,silver_mastering_2017-1} that plays games such as chess, shogi or go, is cast within an MPC framework.
It is argued that the lookahead and rollout-based policy improvement used in online deployment plays a crucial role in enabling the success of the respective algorithms. 
%Due to the online game simulation used in the \ac{RL} algorithm, this corresponds to an MPC-type algorithm, which stands in contrast to deploying the offline trained policy resp. value function directly.
%Of particular importance is the one-step lookahead as it corresponds to a Newton-step improvement of the learned value function approximation.

A high-level survey on how general machine learning concepts are used within \ac{MPC} is provided by~\cite{hewing_learning-based_2020}. 
The authors of~\cite{mesbah_fusion_2022} have the same focus on investigating general machine learning used as part of \ac{MPC} and provide technical details for learning models, policy parameters, optimal terminal value functions, and approximations of iterative online optimization.
\ac{RL} is a minor topic of both surveys~\cite{hewing_learning-based_2020} and \cite{mesbah_fusion_2022}. The survey~\cite{mesbah_fusion_2022} is well aligned with our proposed framework. For instance, it considers approximating the terminal value function to be treated in another category as closed-loop learning. In fact, our proposed framework can be seen as a complementary work to~\cite{bertsekas_reinforcement_2019} and~\cite{mesbah_fusion_2022}.

Further works compare MPC and RL for particular applications.
Similarly to~\cite{mesbah_fusion_2022}, the authors in~\cite{norouzi_integrating_2023} compare how \ac{MPC} and machine learning are combined exclusively for automotive applications.
The authors in~\cite{zhang_building_2022} compare \ac{RL} and \ac{MPC} specifically applied to energy management in buildings with a focus on data-driven \ac{MPC} methods.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%