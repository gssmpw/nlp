\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{When End-to-End is Overkill: \\
Rethinking Cascaded Speech-to-Text Translation
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Anna Min}
\IEEEauthorblockA{\textit{School of Software} \\
\textit{Tsinghua University}\\
Beijing, China \\
man20@mails.tsinghua.edu.cn}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Chenxu Hu}
\IEEEauthorblockA{\textit{IIIS} \\
\textit{Tsinghua University}\\
Beijing, China \\
hu-cx21@mails.tsinghua.edu.cn}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Yi Ren}
\IEEEauthorblockA{
\textit{TickTok}\\
% City, Country \\
ren.yi@bytedance.com}
\and
\IEEEauthorblockN{4\textsuperscript{th} Hang Zhao}
\IEEEauthorblockA{\textit{IIIS} \\
\textit{Tsinghua University}\\
Beijing, China \\
hangzhao@tsinghua.edu.cn}

}

\maketitle

\begin{abstract}
Though end-to-end speech-to-text translation has been a great success, we argue that the cascaded speech-to-text translation model still has its place, which is usually criticized for the error propagation between automatic speech recognition (ASR) and machine translation (MT) models. In this paper, we explore the benefits of incorporating multiple candidates from ASR and self-supervised speech features into MT. Our analysis reveals that the primary cause of cascading errors stems from the increased divergence between similar samples in the speech domain when mapped to the text domain. By including multiple candidates and self-supervised speech features, our approach allows the machine translation model to choose the right words and ensure precise translation using various speech samples. This strategy minimizes error spread and takes advantage of large ASR and MT datasets, along with pre-trained ASR/MT models, while addressing associated issues.
\end{abstract}

\begin{IEEEkeywords}
speech-to-text translation,machine translation.
\end{IEEEkeywords}



\section{Introduction}

In recent years, the academic community has been intrigued by the rapid advancement of end-to-end speech-to-text translation models~\cite{berard2016listen}. These efficient encoder-decoder architectures provide a direct avenue for translating speech, bypassing the need for complex intermediate symbolic representations. However, the arduous task of assembling and curating end-to-end data poses a significant challenge, entailing considerable costs and extensive efforts. These end-to-end methods need the careful selection of high-quality data or argumentation~\cite{popuri2022enhanced,jia2019leveraging,stoian2020analyzing,pino2020self,wang2021large}, encompassing both speech and translated transcripts, and the scrupulous exclusion of erroneous examples.



Nonetheless, cascaded speech-to-text translation models have encountered substantial criticism due to an intrinsic shortcoming called “cascaded loss” or “error propagation”. Studies on ASR+MT systems have explored various methods to enhance the integration of ASR output lattices into MT models~\cite{matusov2005integration,quan2005integrated,ma2020neural}. To mitigate error propagation, several approaches~\cite{bertoldi2005new,beck2019neural,sperber2019self,peitz2012spoken,cheng2019breaking,di2019robust,dalmia2021searchable,inaguma2021fast} have been proposed to integrate ASR and MT models for end-to-end models, which necessitate the addition of supplementary modules and substantial additional training. In contrast, the method proposed in our research employs an n-best strategy that does not require additional parameters and can significantly enhance performance with minimal fine-tuning.



% \vspace{-0.2cm}

\begin{figure}[ht]
  \centering
    \centering
    \includegraphics[width=\linewidth]{images/1.png}
\caption{The correct words are scattered among various candidates, while the former cascaded system directly selected the first candidate, resulting in similar pronunciation errors in the ASR output that are further propagated through the translation model, causing cascading losses.}
\end{figure}

% \vspace{-0.3cm}

Recent studies~\cite{bommasani2021opportunities,han2021pre} have demonstrated the performance improvements achieved by scaling up pre-trained models for downstream natural language processing tasks. To fully exploit the potential of pre-trained MT and ASR models, we present a novel perspective on error propagation and the preservation of essential speech information. We propose the idea of utilizing multiple ASR candidates for machine translation, integrated with self-supervised speech representations, to enhance the accuracy of the translation. Our comprehensive analysis reveals the primary causes of error propagation in cascaded systems, which originate from the misalignment between the acoustic and semantic dimensions of speech. Factors such as homophones with different meanings and word elisions contribute to inaccuracies in ASR results, which consequently propagate to the machine translation model. Furthermore, we explore the use of self-supervised language representations to preserve fine-grained linguistic information in speech.  %Through extensive experimentation, we demonstrate that a simple augmentation of the machine translation model with additional modules can effectively mitigate error propagation in the cascade S2T model, harnessing the potential of large-scale translation models at low cost. Our method outperforms the end-to-end S2T models, indicating the effectiveness of our model in addressing the limitations of the current paradigm.
%In addition, we underscore the urgent need to reevaluate the cascade approach for speech-to-text translation, exposing the limitations of end-to-end models while revealing the vast potential of exploiting multiple candidates and self-supervised representations. By addressing the challenges of error propagation and fine-grained linguistic information loss, our work finds a new way to achieve enhanced performance for speech-to-text translation.

Our model has the following advantages:\vspace{-2pt} 
% 1. Our model achieves the best performance among cascaded models in the current Speech-to-Text (S2T) task.\\
% 2. Our model can leverage various known ASR and machine translation (MT) pre-trained models. Specifically, even without modifying the model parameters, our approach utilizes aligning candidates and calculating attention to achieve improved performance, particularly in the MT model. 
% As the effectiveness of the MT pre-trained model improves, our approach has a higher potential to bridge the gap between speech translation and machine translation in terms of performance. This indicates the untapped potential of our method.\\
% 3. Unlike end-to-end models, our approach does not require an extensive large amount of costly <speech, transcript, target> paired data.\\
% 4. Our model demonstrates rapid training speed and attains exceptional results with minimal data utilization for fine-tuning the Machine Translation (MT) model. Even when not subjected to fine-tuning in the domain of MT, our model still yields noteworthy improvements. Furthermore, our model introduces no additional parameters to the base model, thereby conferring significant adaptability and facilitating lightweight modifications.\\


\begin{enumerate}
    \item Our model achieves the best performance among cascaded models in the Speech-to-Text(S2T) translation task.

    \item Our method can leverage variously known ASR and machine translation (MT) pre-trained models. Specifically, it can effortlessly adapt to different model architectures without the need to adjust the model parameters. % Our approach utilizes aligning candidates and calculating attention to achieve improved performance, particularly in the MT model. As the effectiveness of the MT pre-trained model improves, our approach has a higher potential to bridge the gap between speech translation and machine translation in terms of performance. This indicates the untapped potential of our method.

    \item Unlike end-to-end models, our approach does not require an extensive large amount of costly \textless speech, transcript, target\textgreater     paired data. 

    \item Our model demonstrates rapid training speed and attains exceptional results with minimal data utilization for fine-tuning the Machine Translation (MT) model. %Even when not subjected to fine-tuning in the domain of MT, our model still yields noteworthy improvements. Furthermore, our model introduces no additional parameters to the base model, thereby conferring significant adaptability and facilitating lightweight modifications.
\end{enumerate}

\section{Analysis}

To see where the error propagation lies in the cascaded system and how ASR errors propagate to MT, we pose two questions: 1) Can the top-ranked ASR candidate cover all the lexicons? 2) Is the top-ranked ASR candidate always the best translation result?

\subsection{Preliminary experiments}

In the cascaded system, we extract the top 20 results based on the scores from the ASR system and then select the top $n$ candidates. We calculate the lexical overlap between these candidates $\{c_1, c_2 ... c_n\}$ and the ground truth text $\{gt\}$ of the ASR, which means the length of set of all words in candidates $\{ w| w \in c_k, 1 \leq k \leq n\}$ intersected with $\{ w| w \in gt\}$ divided by the length of the latter. In Table 1, “average” refers to the average lexical overlap between each candidate and the ground truth. At the same time “cumulative” represents the lexical overlap when considering a combination of $n$ candidates with the ground truth. We observe that as $n$ increases, although the average lexical overlap decreases, the cumulative overlap improves. This indicates that apart from the top-ranked candidate, the ASR system fails to include some vocabulary, which remains in the lower-ranked candidates.


\begin{table}[th]
  \caption{Lexical overlap between ASR candidates and GT}
  % \label{tab:example}
  \centering
  \begin{tabular}{ |c|c|c| }
    \hline
    \multicolumn{1}{|c}{\textbf{$n$ candidates}} & 
    \multicolumn{1}{|c|}{\textbf{Average}}  & 
    \multicolumn{1}{c|}{\textbf{Cumulative}} \\
    \hline
1                 & 92.0\% & 92.0\%  \\
5                 & 90.0\% & 94.3\%  \\ 
10                 & 89.4\% & 95.0\%  \\ 
% 15                 & 89.2 & 95.3 & 95.5 \\ \hline
20                 & 89.5\% & 95.5\% \\
    \hline
  \end{tabular}
\end{table}

\vspace{-0.2cm}

Additionally, we use a trained MT model to translate and calculate BLEU scores for the top 5 candidates based on ASR scores. We then analyze the index and percentage of the highest BLEU score and find that only 45.35\% of the candidates with the highest BLEU score corresponded to the candidate with the lowest word error rate. It indicates that the top-ranked ASR candidate always performs the best translation result. 

We also compare the BLEU score of the translation for the candidate with the lowest word error rate with the average BLEU score of the top 5 translations. The BLEU result of the translated best ASR candidate is 36.0, while the BLEU result of the translated first ASR candidate is 32.9. This indicates that combining multiple candidates can significantly improve translation results.

\begin{table}[th]
  \caption{The proportion of the ASR result index of the best candidate based on the BLEU score of the translation result} %最好的BLEU在前5个candidate中的比例分配，强调BLUE, ASR candidate
  % \label{tab:example}
  \centering
  \begin{tabular}{|c|ccccc|}
    \hline
    {Best BLEU idx}  & 1  & 2 & 3 & 4 & 5  \\ 
    \hline
    {percentage(\%)}  & 45.35 & 16.73  & 14.07 & 12.12 & 11.74 \\
    \hline
  \end{tabular}
\end{table}




% \begin{table}[ht]
% \caption{\small 
% The translation BLEU score between the best complete sentence candidate and the first candidate (selected in the former cascaded system)}
% \centering
% \begin{tabular}{c|c|c}
% \toprule
%   & \small first candidate  &\small best candidate  \\ \midrule
% BLEU               & 32.9 & 36.0   \\ 
% \bottomrule
% \end{tabular}
% \end{table}




\vspace{-0.6cm}


\subsection{Source of cascade loss}

% \begin{figure}[h]
%   \centering
%     \centering
%     \includegraphics[width=0.8\linewidth]{images/4.png}
%     \caption{Non-correspondence in pronunciation space and semantic space: When similar samples in the speech domain are mapped to the text domain, the differences between them can increase. These dissimilarities have the potential to propagate errors to the machine translation model.}
% \end{figure}

% \paragraph{Non-correspondence in pronunciation space and semantic space}
% When similar samples in the speech domain are mapped to the text domain, the differences between them can increase. These dissimilarities have the potential to propagate errors to the machine translation model. For instance, ``What did it feel to read that letter'' and ``What did it fail to read that letter'' are two utterances that are pronounced similarly but have significant differences in their intended meaning. The former is the correct ASR result that aligns with human language expression. However, during the ASR's beam search, the latter obtains the highest beam score. In traditional cascaded models, the correct ASR result ``What did it feel to read that letter'' is discarded as the second-highest probability beam. Subsequently, the machine translation result is poor.

% 既然asr环节的最好结果不代表端到端的最好结果，那么一定是一个地方产生了级联误差，所以我们深入分析这个级联误差，做出猜测:The source of cascaded loss mainly comes from non-correspondence in pronunciation space and semantic space and that ASR models struggle to select the results that best match the language patterns.


Since the best result in the ASR stage does not necessarily represent the best result in an end-to-end manner, there must be a point where cascaded errors occur. Therefore, we delve deeper into analyzing these cascaded errors and make the hypothesis that the source of cascaded loss mainly arises from discrepancies between the pronunciation space and semantic space and that ASR models encounter difficulties in selecting results that most accurately match the language patterns. 

ASR is trained using paired speech and text data, but the language patterns it captures are not as rich as those in translation models. Therefore, when the recognition results are similar, it is difficult for the ASR model to select the candidate that best aligns with human common sense and grammar based on scores alone.

As an example, consider the phrase “has put the race on the top.” The highest-scoring candidate in ASR recognizes “race” as “rays,” which contradicts common sense. However, subsequent candidates include various results with similar pronunciations, such as “race,” “raised,” and “raise.” Another example is “Recording the transaction in an immutable distributed ledger,” while the highest-scoring result in ASR is “Recording the transaction in an immutable distributed lecture,” which is not a common expression. The subsequent candidates include “legend,” “literature,” “letter,” “ledger,” and other results.

% In the ASR part, when encountering domain-specific vocabulary that is not in the lexicon, it tends to be translated as a non-existent word with a similar spelling. This has a significant impact on machine translation models. We have an example sentence: ``Where the Golgi apparatus, sometimes called the Golgi body, receives them.'' The first candidate from ASR misrecognizes the pronunciation of the ``golgi apparatus'' as two non-existent words, ``golgy'' and ``golji''. 

\vspace{-0.35cm}

\begin{figure}[ht]
  \centering
    \centering
    \includegraphics[width=0.9\linewidth]{images/2.png}
    \caption{Overview of our proposed MC-sslS system}
  % \hfill
\end{figure}

\section{Method}

To leverage the powerful capability of machine translation in capturing semantic patterns, we propose utilizing multi-candidate ASR inputs and averaging attention computation in the MT model. Furthermore, to address the issue of error propagation in ASR caused by homophones, we employ self-supervised speech representations to enhance accuracy. The combination of methods we propose is represented by the model depicted in Figure 2.

\subsection{Multi-candidate from ASR}

The correspondence between the speech domain and the semantic domain is not always perfect. However, compared to machine translation models, ASR models have limited ability to capture semantic patterns effectively. Consequently, ASR is prone to mapping speech samples that are acoustically similar to semantically distant outputs. Moreover, ASR cannot selectively choose samples that align more closely with human language conventions and patterns. For a given speech input, the correct vocabulary might be dispersed among multiple candidates generated during beam search. Unfortunately, previous cascaded systems only consider the top-scoring candidate and pass it to the machine translation model, resulting in error propagation within the cascaded system.
Our cascaded model first uses Wenet~\cite{yao2021wenet} to perform ASR on GigaST~\cite{ye2022gigast}; we store the top $20$ ASR result sentences based on the cumulative log probability value from the end of beam search. 

\subsubsection{Aligned by common substrings} 
We follow the following steps to align the candidates:
\begin{enumerate}
    \item The top $n$ ranked texts $t_k(1 \leq k \leq n)$ consisting of $n_k$ words ($w_1,w_2....w_{n_k}$) are selected (in our following experiments, we set $n$ to be 5).
    \item The dynamic programming algorithm for finding the longest common substrings ~\cite{charalampopoulos_et_al:LIPIcs.ESA.2021.30} is used for $n-1$ times, in the preprocessing stage before putting into the model. There are $n-1$ processes in total. We denote $t_i^{m+1}$ as the aligned result of $t_i$ after the $m^{th}(1 \leq m \leq n-1)$ process. $t_1^{1}$ is equal to $t_1$.
    \item During the $m^{th}$ process, (1) we calculate the longest common subsequences of $\{t_1^{m},t_{m+1}\}$, and get $\{t_1^{m+1},t_{m+1}^{m+1}\}$ by connecting the substrings and the largest length of uncommon substrings. (2) The common substrings are aligned, and the remaining parts are padded with “unk” tokens. (3) When $2 \leq m \leq n-1$, the $\{t_2^{m}...t_{m+1}^{m}\}$ are padded with “unk” tokens at the same indexes where $\{t_1^{m}\}$ is padded to $\{t_1^{m+1}\}$. Then, one process ends. 
    \item  After $n-1$ processes, the aligned and padded texts of the same length are used as input to the attention-based machine translation model. 
\end{enumerate}
% The top $n$ ranked texts $t_k(1 \leq k \leq n)$ consisting of $n_k$ words ($w_1,w_2....w_{n_k}$) are selected (in our following experiments, we set $n$ to be 5). In the preprocessing stage before putting into the model, the dynamic programming algorithm for finding the longest common substrings ~\cite{charalampopoulos_et_al:LIPIcs.ESA.2021.30} is used for $n-1$ times. There are $n-1$ processes in total. We denote $t_i^{m+1}$ as the aligned result of $t_i$ after the $m^{th}(1 \leq m \leq n-1)$ process. $t_1^{1}$ is equal to $t_1$. During the $m^{th}$ process, firstly, we calculate the longest common subsequences of $\{t_1^{m},t_{m+1}\}$, and get $\{t_1^{m+1},t_{m+1}^{m+1}\}$ by connecting the substrings and the largest length of uncommon substrings. Secondly, the common substrings are aligned, and the remaining parts are padded with “unk” tokens. Thirdly, when $2 \leq m \leq n-1$, the $\{t_2^{m}...t_{m+1}^{m}\}$ are padded with “unk” tokens at the same indexes where $\{t_1^{m}\}$ is padded to $\{t_1^{m+1}\}$. Then, one process ends. After $n-1$ processes, the aligned and padded texts of the same length are used as input to the mBART-based machine translation model. 
\vspace{-0.2cm}
\begin{figure}[ht]
  \centering
    \centering
    \includegraphics[width=\linewidth]{images/6.png}
    \caption{Above is how the $3^{rd}$ process is calculated. After finding the longest common subsequences, candidates are aligned and padded. The orange circles denote “unk” tokens.}
  % \hfill
\end{figure}

\vspace{-0.2cm}

% ~\AN{xxx add formula}
\subsubsection{Average attention among candidates} Thus, we propose an innovative approach, incorporating multiple ASR candidates into a single attention-based machine translation model. Here we use mBART as the backbone architecture. The multi-candidate model can share identical parameters with the translation model, differing only in its attention-averaging technique. 

During the training stage, this method averages attention to get $\text{A}^\prime(Q)$ at the sentence level for multiple input candidates corresponding to the same translation result. The average calculation is performed only once after all decoder layers, just before the final layer normalization, and then assign $\text{A}^\prime(Q)$ to $\text{A}_{candidate_i}(Q)$. We find that calculating only once at the final layer yields the best results. 
% \CX{need more clear, maybe add equations to illustrate} 

% \red {one line}
% \vspace{-0.5cm}

% \begin{align}
% \text{A}_{candidate_i}(Q)= \text{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V
% \end{align}

\vspace{-0.5cm}

\begin{align}
\text{A}^\prime(Q) = \frac{1}{n_{\text{candidate}}} \sum_{i=1}^{n_{\text{candidate}}} \text{A}_{candidate_i}(Q)
\end{align}





During the inference stage, the attention mechanism is utilized to compute the average attention score for beams that share the same candidate sequence number. This aggregation of attention scores forms a pool of candidate beams. Consider there are $m$ beams in total. Each time predicting the next token, the $n$ candidates generate $mn$ results in total. For each candidate, there are $m$ beams, and the $\text{A}_{beam_k}(Q)$ is calculated as follows. The calculation of attention occurs at the same position and training stage, after all decoder layers, just before the final layer normalization. Attention is calculated once for each token generated. Subsequently, the other calculations are the same as regular beam searches.

\vspace{-0.2cm}
\begin{align}
\text{A}_{beam_k}(Q) = \frac{1}{n_{\text{candidate}}} \sum_{i=1}^{n_{\text{candidate}}} \text{A}_{candidate_i\_{beam_k}}(Q)
\end{align}


% \begin{algorithm}[!h]
% \caption{MC-mBART Inference Algorithm}
% \begin{algorithmic}[1]

% \Procedure{Inference}{$n, m, k, \text{max\_len}$}
%     % \State Initialize an empty list of candidate sequences with an initial score of 1.0
%     % \State Initialize an empty list of completed sequences
%     \State Add the start token to the candidate sequences

%     \While{there are incomplete candidate sequences}
%         % \State $next\_candidates \gets []$
%         \For{$candidate_i$ in $\{c_1,c_2...c_n\}$}
%             \State calculate $beam_j$ for j in $\{1,2...m\}$ with $\text{A}_{beam_k}(Q)$
%                 \EndFor
%         \State Sort next\_candidates by score and keep only top k candidates
%         \State Update candidate sequences with next\_candidates
%     \EndWhile

%     \State \textbf{return} completed sequences
% \EndProcedure
% \end{algorithmic}
% \end{algorithm}


It's worth noting that this does not add any parameters to the MT model and can be used without further training, compared with ~\cite{matusov2005integration,quan2005integrated,ma2020neural}. 
It is a lightweight method that can be readily adapted to various attention-based architectures.

\subsection{Acoustic and linguistic features fusion}

% \red{what acoustic features, here we use HuBERT}
The process of converting audio recognition to text and then to machine translation further results in the loss of a large amount of acoustic information in speech. We propose a multi-candidate self-supervised learning speech machine translation model(shown in Figure 2) to enhance the accuracy of filtering correct results by leveraging language patterns from machine translation and fusing acoustic and linguistic features, following the approach utilized in previous works~\cite{lee2021direct} and utilizing HuBERT~\cite{hsu2021HuBERT} for generating target self-supervised discrete units. This choice was influenced by the superior performance demonstrated by HuBERT in various tasks such as ASR, spoken language modeling, and speech synthesis, as shown in by ~\cite{yang2021superb,lakhotia2021generative,polyak2021speech}. It outperforms other unsupervised representations, including VAE-based representations employed in ~\cite{tjandra2019speech,zhang2021uwspeech}.

We follow~\cite{lee2021direct} to use the $11^{th}$ layer of HuBERT as input, which contains richer linguistic information, and transform them into tokens with a word list of 1000 for the number of speech units using the K-means model trained on English speech. Due to the high length of the original unit, we reduced the consecutive repetitive units to a single unit. We create a new trainable vocabulary for word embeddings. After being encoded, it interacts with the decoder's cross-attention to obtain deeper language information. The calculation process is the same as that of how the output of the source text encoder computes cross-attention with the production of self-attention. The rest of the process is similar to that of the multi-candidate machine translation model. 



\section{Experiments \& Results}

\subsection{Data}
For ST datasets, we use GigaST~\cite{ye2022gigast}, a large-scale pseudo speech translation (ST) corpus containing 7.5M en-zh pairs. It is created by translating the text in GigaSpeech~\cite{chen2021gigaspeech}, an English ASR corpus, into German and Chinese. The training set is translated by a robust machine translation system, and the test set is translated by humans. %and MuST-C~\cite{di2019must}, a multilingual speech translation corpus whose size and quality will facilitate the training of end-to-end systems for SLT from English into 8 languages. %For each target language, MuST-C comprises at least 385 hours of audio recordings from English TED Talks, which are automatically aligned at the sentence level with their manual transcriptions and translations. It is created by translating the text in GigaSpeech~\cite{chen2021gigaspeech}, an English ASR corpus, into German and Chinese. The training set is translated by a robust machine translation system and the test set is translated by humans. We use its training set and use TED dev2010 and tst2015~\cite{liu2019end} as the validation set. We apply the en-zh test set, which contains the translation of all GigaSpeech test utterances. The test sets are produced by human translators looking at the transcriptions. 
 
% \begin{table}[ht]
%   \label{sample-table}
%   \centering
%   \begin{tabular}{lllll}
%   \toprule
%          & GigaST    & MuST-C & WMT  & Total\\
%     \midrule
%     en-zh  & 7.4M  & -  & 25.1M & 32.5M \\
%  en-de  & 7.4M  & 0.2M  & 5.9M & 13.5M \\
%   en-fr  & -  & -  & - & - \\
%    en-es  & -  & -  & - & - \\
%     \bottomrule
%     \caption{Statistics of all datasetsets}
%   \end{tabular}
% \end{table}

% \begin{table}[ht]
%   \centering
  
%   \label{sample-table}
%   \begin{tabularx}{\columnwidth}{X|cccc|ll|ll}
%     \toprule
%             & train | & train | valid & test \\
%            & GigaST & MuST-C & WMT   & Total & tst-COMMON & GigaST\\
%     \midrule
%     en-zh  & 7.4M   & -      & 25.1M & 32.5M & - & 18.7k\\
%     en-de  & 7.4M   & 0.2M   & 5.9M  & 13.5M & 2.6k & 4.0k\\
%     en-fr  & -      & 0.3M      & 15.2M     & 15.4M  & 2.5k & -\\
%     en-es  & -      & 0.3M      & 15.2M     & 15.4M  & 2.6k & - \\
%     \bottomrule
%     \caption{Statistics of all datasets}
%   \end{tabularx}
% \end{table}

% \begin{table}[ht]
% \caption{Statistics of language pairs across all datasets}
%   \centering
%   \label{sample-table}
%   \begin{tabular}{l|rrrr|rr}
%     \toprule
%               & \multicolumn{4}{c|}{train} & \multicolumn{2}{c}{test} \\
%            & GigaST & MuST-C & WMT   & Total & tst-C & GigaST\\
%     \midrule
%     en-zh  & 7.4M   & -      & - & 7.4M & - & 18.7k\\
%     en-de  & 7.4M   & 0.2M   & 5.9M  & 13.5M & 2.6k & 4.0k\\
%     en-fr  & -      & 0.3M      & 15.2M     & 15.4M  & 2.5k & -\\
%     en-es  & -      & 0.3M      & 15.2M     & 15.4M  & 2.6k & - \\
%     \bottomrule
%   \end{tabular}
   
% \end{table}


% \begin{table*}[ht]
%     \centering
%     \begin{tabular}{l|cccc|ccc|c}
% \toprule \multirow{2}{*}{ Models } & \multicolumn{4}{c|}{ External Data } & \multicolumn{4}{c}{ BLEU } \\
%        & Speech & Text & ASR & MT & De & Es & Fr & Avg. \\
% \midrule \multicolumn{9}{l}{ End-to-End Model } \\
% \midrule MTL~\cite{tang2021general} & - & - & - & $\checkmark$ & 23.9 & 28.6 & 33.1 & 28.5 \\
%        FAT-ST~\cite{zheng2021fused} & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & 25.5 & 30.8 & - & - \\
%        JT-S-MT~\cite{tang2021improving} & - & - & - & $\checkmark$ & 26.8 & 31.0 & 37.4 & 31.7 \\
%        Chimera~\cite{han2021learning} & $\checkmark$ & - & - & $\checkmark$ & $27.1^{\dagger}$ & 30.6 & 35.6 & 31.1 \\
%        XSTNet~\cite{ye2021end} & $\checkmark$ & - & - & $\checkmark$ & 27.1 & 30.8 & 38.0 & 32.0 \\
%        SATE~\cite{xu2021stacked} & - & - & $\checkmark$ & $\checkmark$ & $28.1^{\dagger}$ & - & - & - \\
%        STEMM~\cite{fang2022stemm} & $\checkmark$ & - & - & $\checkmark$ & 28.7 & 31.0 & 37.4 & 32.4 \\
%        TaskAware~\cite{indurthi2021task} & - & - & $\checkmark$ & $\checkmark$ & 28.9 & - & - & - \\
%        STPT~\cite{tang2022unified} & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & - & 33.1 & 39.7 & - \\
%        ConST~\cite{ye2022cross} & $\checkmark$ & - & - & $\checkmark$ & 28.3 & 32.0 & 38.3 & 32.9 \\
% \midrule \multicolumn{9}{l}{ Cascade Model } \\
% \midrule Espnet~\cite{inaguma2021source} & - & - & - & - & 23.6 & - & 33.8 & - \\
%        W2V2-Transformer~\cite{fang2022stemm} & $\checkmark$ & - & - & $\checkmark$ & 26.9 & 30.0 & 36.6 & 31.2 \\
%        ~\cite{ye2021end} & - & - & $\checkmark$ & $\checkmark$ & 25.2 & - & 34.9 & - \\
%        ~\cite{xu2021stacked} & - & - & $\checkmark$ & $\checkmark$ & 28.1 & - & - & - \\
% \midrule
%        Machine Translation with mBART & - & - & - & $\checkmark$ & 31.0 & 33.8 & 39.5 & 34.8 \\
% \midrule
%        former cascaded system & - & - & - & $\checkmark$ & 27.5 & 28.8 & 35.7 & 30.7 \\
%        MC system(tuned on mBART) & - & - & - & $\checkmark$ & 28.5 & 30.1 & 36.8 & 31.8 \\
% \bottomrule
% \end{tabular}
%     \caption{Case-sensitive detokenized BLEU scores on MuST-C tst-COMMON set. “Speech” denotes unlabeled audio data, “Text” denotes unlabeled text data, e.g., Europarl V7~\cite{koehn-2005-europarl}, CC25~\cite{10.1162/tacl_a_00343}, † use 40M OpenSubtitles~\cite{lison2016opensubtitles2016} as external MT data. MC system is the system that applies a multi-candidate strategy and fine-tunes the machine translation model of mBART.}
%     \label{tab:mustc-results}
% \end{table*}

\subsection{Model setup}
\subsubsection{Cascaded system}

Our cascaded model first uses Wenet~\cite{yao2021wenet} to perform ASR on Gigaspeech with 7.4M valid en-zh pairs, and we store the speech recognition texts corresponding to the top 20 cumulative log probability value rankings at the end of beam search. The top $n$ ranked texts are input as encoders into the mBART-based machine translation model. 
We do not use any self-supervised speech models to initialize the ASR encoder. The encoder is trained from scratch following the WeNet paper. Taking inspiration from the approach proposed by ~\cite{lee2021direct}, We use the multilingual HuBERT (mHuBERT) model and K-means model to encode source speech into a vocabulary of 1000 units. The mHuBERT and the K-means models are learned from the combination of English, Spanish, and French unlabeled speech data from VoxPopuli~\cite{wang2021voxpopuli}, while we use them to encode English speech only.




\subsubsection{Multi-candidate mBART} 
The multi-candidate mBART is built upon the mBART model~\cite{liu2020multilingual}. We fine-tune the mBART model on 8 A100 GPUs. Our model configuration follows\footnotemark to use the same parameters for fine-tuning the mBART model. %In this process, input texts are noised through phrase masking and sentence permuting, and a single Transformer model~\cite{vaswani2017attention} is trained to reconstruct the texts. %mBART encompasses a fully autoregressive Seq2Seq model, trained once for all languages, providing a parameter set that can be fine-tuned for any language pair in both supervised and unsupervised settings without requiring task-specific or language-specific modifications or initialization schemes.

% The multi-candidate mBART model extends the original mBART architecture by introducing modifications to the input, decoder, and inference computations. In this enhanced framework, $N$ source language text candidates are fed into the mBART encoder, where they undergo cross-attention with the target language text in the decoder. The average attention is computed before the final layer norm, and the resulting output is generated. During the Inference stage, the average attention before the decoder's layer norm is calculated and reduced to n results through projection. In the beam search phase, the cumulative log probability values of $n^2$ beams are sorted, and the translation with the highest score is selected as the final result.

% During the fine-tuning phase, we conduct experiments on three distinct models: the pre-trained mBART model and the machine translation model fine-tuned on accurately aligned text pairs from Gigaspeech with varying steps. To enhance the fine-tuning process, we employed the multi-candidate approach and utilized 8 A100 GPUs for n epochs. The results of our study demonstrate the effectiveness of incorporating the method, leading to significant improvements in the performance of the mBART model in various language pairs and translation tasks.
% In our model configuration, several vital parameters were employed during the training process. The use of the pre-trained mBART model laid the foundation for our approach. Specifically, we apply parameter settings as follows: sharing the decoder's input and output embeddings, utilizing a single layer for the adaptor, incorporating normalization, applying dropout with a probability of 0.1, using the same dropout probability for attention and ReLU layers, loading the pre-trained decoder from a specified location, setting probabilities for masking and channel masking, implementing encoder projection, setting the learning rate to 0.0005, employing the inverse square root learning rate scheduler with an initial learning rate for warmup at 1e-7 and 10000 warmup updates, utilizing the Adam optimizer with specific beta values, clipping gradients to a maximum norm of 10.0, and defining maximum values for updates, tokens, tokens in validation, and source positions. These parameter choices played a crucial role in shaping the model's behavior during training.
 %We fine-tune the pre-trained mBART model, with settings of sharing decoder embeddings, 0.1 dropout probability, the same dropout for attention and ReLU layers, masking probabilities, a learning rate of $5e^{-4}$, inverse square root learning rate scheduler with warmup, adam optimizer with beta values of (0.9,0.98), masking probability of 0.3, mask channel length of 32, mask channel probability of 0.25, and a warmup initial learning rate of $1 \times 10^{-7}$. These parameter choices crucially influence the model's training behavior. \red{follow 's settings link}

\footnotetext{https://github.com/facebookresearch/fairseq/blob/main/examples/\\mbart/README.md}
 
% \paragraph{multi-candidate SSL-speech mBART}
% % We employ a pre-trained mBART model, a machine translation model fine-tuned on accurately paired data from Gigaspeech, and a high-quality model sourced from Hugging Face. By incorporating multiple candidates and SSL speech representation methods, we fine-tune these models for a total of XXX epochs using 8 A100 GPUs. The experimental results revealed the following outcomes:

% We extract the HuBERT features at the 11th layer for the input speech and transform them into tokens with a word list of 1000 for the number of speech units using the K-means model trained on English speech. Due to the high length of the original unit, we reduced the consecutive repetitive units to a single unit. 
% In our improved multi-candidate SSL-speech mBART model, we input the HuBERT tokens and multiple candidates of the same source language audio into different encoders, whose word lists are independent of each other, and input the output results of both encoders into the decoder and in the decoder stage, in each In one decoder layer, after the target language makes self-attention, it first crosses attention with the output of the multi-candidate encoder, then crosses attention with the production of SSL-speech encoder, and then the n candidates are averaged for layer norm.
% In the Inference stage, similar to the one in 3.1, the highest cumulative log probability score of $n^2$  beams is finally taken as the resulting output.

% \paragraph{single-candidate mBART} It is a baseline of the cascaded system. It has the same settings as the multi-candidate mBART. Apart from that, it uses the first candidate from ASR results as the input of mBART.

% \paragraph{fine-tuned mBART}It uses the ground truth source text but not the ASR results as the training input.





% On the MuST-C dataset, We compare our method with several robust ST systems, including Espnet~\cite{inaguma2021source}, W2V2-Transformer~\cite{fang2022stemm}, MTL~\cite{tang2021general}, FAT-ST~\cite{zheng2021fused}, JT-SMT~\cite{tang2021improving}, Chimera~\cite{han2021learning}, XSTNet~\cite{ye2021end}, SATE~\cite{xu2021stacked}, STEMM~\cite{fang2022stemm}, TaskAware~\cite{indurthi2021task}, STPT~\cite{tang2022unified}, ConST~\cite{ye2022cross}. 

On the GigaST dataset, we compare our method with the original cascaded system and two end-to-end ST systems, including SSL-Transformer and Speech-Transformer~\cite{ye2022gigast}.

% \paragraph{SSL-Transformer} This model serves as our benchmark for comparison. It incorporates self-supervised-learned audio representations, explicitly utilizing the HuBERT-large module which demonstrates the best performance. To address the challenge of sequence length, the authors introduce two layers of convolutional subsampler with a stride of 2 after the self-supervised learning module. The downstream module adopts the Transformer encoder-decoder architecture. For the downstream Transformer, the authors employ the same hyperparameters as in S-Trans L, consisting of 6 layers of both encoder and decoder. The dimensions are set as follows: dmodel = 1024, dff = 4096, and dhead = 8. Notably, the model takes the raw waveform of the entire speech as input, and during training, the self-supervised learning modules remain unfrozen.

% \paragraph{Speech-Transformer} This baseline model is a non-recurrent sequence-to-sequence architecture that heavily relies on attention mechanisms to capture positional dependencies. The encoder and decoder are constructed using multi-head attention and position-wise feed-forward networks. Each decoder block attends to the encoder outputs (h) separately, eliminating the need for intermediate attention steps found in recurrent seq2seq models. This direct and parallel attention mechanism improves the model's ability to align and extract information from the encoder representation.

\vspace{-0.3cm}


\begin{table}[ht]
\caption{Giga-ST main results: “MC” denotes the method of multi-candidate in Section 3.1.2, “Alignment” denotes the method in Section 3.1.1, “sslS" denotes the method of fusing acoustic and linguistic features in Section 3.2}
    \centering
    \begin{tabular}{|c|c|c|}
\hline Settings & Models & BLEU score  \\
     
\hline
  (1)   &  Machine translation  & 40.2  \\

\hline \multicolumn{3}{|l|}{ End-to-End Model } \\
\hline(2)   & SpeechTransformer & 36.3  \\
   (3)   &    SSL-Transformer  & 38.0  \\
       
\hline \multicolumn{3}{|l|}{ Cascaded Model } \\
\hline 
     (4)   & Wenet + mBART  & 36.8  \\
     (5)   &    (4) + MC & 36.9 \\
   (6)   &    (4) + MC + Alignment & 37.8 \\
  (7)   &    (4) + MC + Alignment + sslS   & 38.1\\
        % MC-sslS system  & - & 38.1 & -\\
        % MC system  & - & 37.8 & -\\
        % MC without alignment & - & 36.9 & - \\


% \midrule FST-ST & - & - & - & $\checkmark$ & 27.7 & 32.4 & 37.2 & 32.4 \\
%        FST-FT & - & - & - & $\checkmark$ & 29.2 & 33.9 & 38.9 & 34.0 \\
\hline
\end{tabular}
    
    \label{tab:mustc-results}
\end{table}



% \begin{table}[ht]
% \caption{Giga-ST main results: the term “constrained” refers to training solely on the GigaST's XL dataset, “unconstrained” indicates the use of a commercial machine translation model, while “semi-constrained” represents the results obtained by fine-tuning the pre-trained mBART on GigaST. “MC without alignment” denotes using only multi-candidates without aligning common strings in Sec 3.1.1}
%     \centering
%     \begin{tabularx}{\columnwidth}{l|l|ccc}
% \toprule Settings & Models & \multicolumn{3}{c}{ MT base model type }  \\
%        & constrained & semi & unconstrained   \\
% \midrule
%   (1)   &  Machine translation  & 24.9 & 40.2 & 44.3 \\

% \midrule \multicolumn{4}{l}{ End-to-End Model } \\
% \midrule(2)   & SpeechTransformer & 36.3 & - & - \\
%    (3)   &    SSL-Transformer  & 38.0 & - & -  \\
       
% \midrule \multicolumn{4}{l}{ Cascaded Model } \\
% \midrule 
%      (4)   & Wenet + mBART  & 22.3 & 36.8 & 39.8 \\
%      (5)   &    (4) + MC & - & 36.9 & - \\
%    (6)   &    (4) + MC + Alignment & - & 37.8 & -\\
%   (7)   &    (4) + MC + Alignment + sslS   & - & 38.1 & -\\
%         % MC-sslS system  & - & 38.1 & -\\
%         % MC system  & - & 37.8 & -\\
%         % MC without alignment & - & 36.9 & - \\


% % \midrule FST-ST & - & - & - & $\checkmark$ & 27.7 & 32.4 & 37.2 & 32.4 \\
% %        FST-FT & - & - & - & $\checkmark$ & 29.2 & 33.9 & 38.9 & 34.0 \\
% \bottomrule
% \end{tabularx}
    
%     \label{tab:mustc-results}
% \end{table}



\subsection{Results}
\subsubsection{Comparison with baselines}


Comparing settings (6) and (4), it shows that the Alignment and Multi-candidate methods enhance the performance of the conventional cascaded system in setting (4). When comparing settings (7), (6), and (3), it shows that fusing acoustic and linguistic features improves the performance of the conventional cascaded system in setting (4), making it comparable to the end-to-end model.

% \red{become better, from lines 1-3, main result, fendianxie, acoustic }
% We apply machine translation models trained at different steps to the multi-candidate cascaded system framework and single-candidate cascaded framework without modifying any parameters or altering the inference approach and input. 

Moreover, while the transition from high-quality machine translation models to improved multi-candidate translation models requires no parameter changes, in the era of emerging and popular large language models, it is evident that our approach holds more excellent practical value and prospects than the End-to-End model. Furthermore, our model achieves a BLEU score of 37.3 after just one epoch of training, requiring only one hour on our settings. With a concise duration of fine-tuning, it surpasses the performance of former cascaded models, demonstrating the effectiveness of our approach.

% In machine translation, for instance, mBART~\cite{liu2020multilingual} can be pre-trained by denoising complete text data from various languages. It is capable of transferring knowledge to language pairs without parallel text or those not included in the pre-training corpus. Languages that are not part of the pre-training corpus can benefit from machine translation, which strongly suggests that the initialization process is, to some extent, language-agnostic. Pre-training can capture common patterns in text. This further highlights the advantages and potential of cascaded models. Interestingly, as we increase the training steps used for machine translation, the gap between the multi-candidate cascaded system and the machine translation model gradually diminishes. In contrast, the gap between the multi-candidate cascaded system and the single-candidate cascaded system grows more prominent. 
% This discovery sheds light on the potential of harnessing multi-candidate utilization within established commercial machine translation models. 

\subsubsection{Ablation}
Comparing settings (5) and (6) shows that the Alignment method is crucial to make the multi-candidate method effective. Without lexical alignment, the sentence-level Multi-candidate average attention struggles to select the correct candidate words.



% In Table 4, we can observe that in the constrained scenario, the cascaded system significantly lags behind the end-to-end system in terms of BLEU score. However, in the unconstrained scenario, the mature machine translation model achieves an impressive BLEU score of 44.3. In the semi-constrained scenario, our multi-candidate cascaded system approaches the performance of the machine translation model and surpasses the former cascaded system by a significant margin. This clearly demonstrates the success of the multi-candidate approach. 

% Furthermore, our model achieves a BLEU score of 37.3 after just one epoch of training, requiring only one hour on our settings. With a concise duration of fine-tuning, it surpasses the performance of former cascaded models, demonstrating the effectiveness of our approach.


% In machine translation, for instance, mBART~\cite{liu2020multilingual} can be pre-trained by denoising complete text data from various languages. It is capable of transferring knowledge to language pairs without parallel text or those not included in the pre-training corpus. Languages that are not part of the pre-training corpus can benefit from machine translation, which strongly suggests that the initialization process is, to some extent, language-agnostic. Pre-training can capture common patterns in text. This further highlights the advantages and potential of cascaded models.


\subsubsection{Case study}
By employing the multi-candidate strategy, we can observe that among the top five candidates based on their scores, there exist samples that deviate from conventional human language expressions. Nevertheless, in the attention mechanism of the machine translation process, words that align more closely with human expression and convey correct semantic meaning receive greater attention. 
As for the example of ``Where the Golgi apparatus, sometimes called the Golgi body, receives them.'' The first candidate from ASR misrecognizes the pronunciation of the ``golgi apparatus'' as two non-existent words, ``golgy'' and ``golji''. However, the candidate with a BLEU score of 100 is ranked fifth. The machine translation model leverages rich text patterns through multi-candidates, allocates more attention to the correct candidate ``Golgi apparatus'' and effortlessly selects the proper translation, regarding the example of ``Recording the transaction in an immutable distributed ledger'' while the subsequent candidates include ``legend'', ``literature'', ``letter'', ``ledger'' and other alternatives. 

% \begin{table}[h]
% \tiny
%   \caption{Sample table title}
%   \label{sample-table}
%   \centering
%   \begin{tabular}{llll}
%     \toprule
%     BLEU     &  pretrained mBART    & 80k steps finetuned MT & 200k steps finetuned MT\\
%     \midrule
%      MC-SB & Input terminal  & $\sim$100  & $\sim$100   \\
%     MC-B & Input terminal  & $\sim$100  & $\sim$100   \\
%     SC-B     & Output terminal & $\sim$10  & $\sim$100    \\
%     MT     & 39.14       & - & - \\
%     \bottomrule
%   \end{tabular}
% \end{table}






% \begin{table}[h]
%   \caption{Sample table title}
%   % \tiny
%   \label{sample-table}
%   \centering
%   \begin{tabular}{llll}
%     \toprule
%     BLEU     & constrained  & semi-constrained  & unconstrained \\
%     \midrule
%     MC & -  & 36.90 & -   \\
%     MT   &  24.9 & 39.14 & 44.3  \\
%     cascade & 22.3 & 36.87 & 39.8   \\
%     SSL-T     & 38.0   & -    & -  \\
%     S-T     & 36.3 & -  & -   \\
%     \bottomrule
%   \end{tabular}
% \end{table}

% \begin{table}[h]
%   \caption{Sample table title}
%   \label{sample-table}
%   \centering
%   \begin{tabular}{llll}
%   \toprule
%     BLEU     & 8k steps MT    & 20k steps MT & 20k steps MT \\
%     \midrule
%     MC-mBART  & -  & 36.57  & 36.90   \\
%     single-candidate-mBART  & XXXX  & 36.62 & 36.87   \\
%     finetuned mBART  & XXXX  & 38.90 & 39.14   \\
%     \bottomrule
%   \end{tabular}
% \end{table}

% \begin{figure}[h]
%   \centering
%   \begin{subfigure}{0.48\linewidth}
%     \centering
%     \includegraphics[width=11cm]{images/5.png}
%     \caption{Right Image}
%   \end{subfigure}
%   \caption{Side-by-Side Images}
% \end{figure}

% \section{Ablation Study}
% \paragraph{Align or not align?} 
% We perform experiments on the same mBART model, both with and without the aligning module. Without the aligning module, the multi-candidate mBART yielded a lower result compared to the former cascaded system, achieving a BLEU score of 36.3. This, to some extent, indicates that the translation model's sentence-level average attention is related to the selection of vocabulary, thus providing partial validation for our explanation.

% \paragraph{Fine-tune or not fine-tune?}Our n-best strategy offers clear advantages over the lattice method, including resource efficiency, adaptability, and the ability to make lightweight modifications. For both the ASR and translation models, no additional weights are required. The change in the translation model involves adding average attention computation to each network layer (details below). The subsequent computations closely resemble those of a typical machine translation model. The difference between these two models boils down to a single computation step, highlighting the portability of the multi-candidate approach. Furthermore, with minimal fine-tuning, significant improvements in results can be achieved.

% \paragraph{The best candidate performance}While multiple ASR hypotheses can help mitigate some ASR errors, it's important to note that in some instances, the overall quality may deteriorate. We conduct experiments using the same model as presented in Table 2 (utilizing 5 candidates), the GigaSpeech test dataset, and the established evaluation methodology. In Table 2, we observe that 45\% of sentences with the lowest Word Error Rate (WER) also corresponded to the highest BLEU score. When considering the 20 best ASR candidates for these specific sentences, 45\% of them showed a marginal 0.09 decrease in BLEU compared to Machine Translation (MT). This subtle finding suggests that, for other sentences, there might be a more significant improvement, highlighting the role of language patterns within the translation model in selecting the optimal candidate.

% \paragraph{How are the correct candidates chosen?}  Regarding the multi-candidate approach, we employ several different methods to compute attention on mc-mBART and made an exciting discovery. \AN{TODO add 3 formula}

% This result indicates that the selection of the correct candidate vocabulary is not based on its frequency among multiple candidates but rather on the model's ability to identify the most probable candidate based on language information.

% \paragraph{How can the MT model affect the effect?} 我们在比较低的mBA

% \begin{table}[h]
%   \caption{Sample table title}
%   \label{sample-table}
%   \centering
%   \begin{tabular}{lllll}
%     \toprule
%     BLEU     & 1candidate    & 3candidate &5candidate &10candidate \\
%     \midrule
%     MC-mBART  & -  & $\sim$100  & -   \\
%     single-candidate-mBART  & 22.3  & 39.8 & -   \\
%     finetuned mBART  & 22.3  & 39.8 & -   \\
%     \bottomrule
%   \end{tabular}
% \end{table}

\section{Conclusion}
Our analysis pinpoints factors contributing to error propagation in cascaded systems, such as pronunciation disparities and semantic differences. Our multi-candidate approach notably enhances speech-to-text (S2T) translation, bridging the S2T-T2T gap without altering model parameters. With enhanced ASR and MT resources, our multi-candidate method narrows the S2T-T2T divide, providing increased accuracy and efficiency, all without additional parameters or modules.







\bibliographystyle{IEEEtran}
\bibliography{IEEE-conference-template-062824}





\end{document}
