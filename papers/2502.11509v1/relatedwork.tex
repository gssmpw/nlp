\section{Related Work}
Counterfactual explanation generation using diffusion models represents a cutting-edge approach in interpretable AI. Diffusion models, known for their ability to generate high-quality samples, are now being adapted to create counterfactualsâ€”hypothetical scenarios that explain model decisions by altering input features. By leveraging the stochastic nature of diffusion processes, these models can produce diverse and plausible counterfactuals, enhancing the transparency and robustness of AI systems in fields such as finance, healthcare, and legal decision-making.


Diffusion probabilistic models (DPM) have achieved superlative image generation quality \cite{DBLP:journals/corr/Sohl-DicksteinW15,DBLP:journals/corr/abs-1907-05600,DBLP:journals/corr/abs-2006-11239,DBLP:journals/corr/abs-2011-13456} and  Diffusion-based image editing has drawn much attention. There are two main categories of image editing and generation. Firstly, image-guided generation works to edit an image by mixing the latent variables of DPM and the input image \cite{DBLP:journals/corr/abs-2108-02938,DBLP:journals/corr/abs-2201-09865,DBLP:journals/corr/abs-2108-01073}. However, using images to specify the attributes for editing may cause ambiguity, as pointed out by \cite{kwon2023diffusion} . Secondly, the classifier guided works \cite{DBLP:journals/corr/abs-2105-05233, DBLP:journals/corr/abs-2111-14818, DBLP:journals/corr/abs-2112-05744} edit images by utilizing the gradient of an extra classifier. In our work, we build upon the second category, aiming to generate multiple counterfactual explanations with the the help of clustering and gradients of the classifier.

Bengio et al. introduced disentangled representation learning, where the target is to discover underlying explanatory factors of the observed data~\cite{DBLP:journals/corr/abs-1206-5538}. The disentangled representation is defined such that each dimension (or set of dimensions) of the representation correspond to an independent factor. Based on this definition, some VAE-based techniques achieve disentanglement  by constraining the probabilistic distributions of representations\cite{chen2018isolating,kim2018disentangling,higgins2016beta}. In \cite{DBLP:journals/corr/abs-1811-12359}, authors point out the identifiable problem by proving that only these constraints are insufficient for disentanglement and that extra inductive bias is required. 

A number of work has been done in generation of counterfactual explanations. Nemirovsky et al. uses a Residual Generative Adversarial Network (RGAN) to generate counterfactuals~\cite{DBLP:journals/corr/abs-2009-05199}. They enhance a regular GAN output with a residual that appear like perturbations used in counterfactual search, and then use a fixed target classifier to provide the counterfactuals. Support Vector Data description based counterfactual generation is described in~\cite{Carlevaro2023}. They use data envelopes extracted via singular value decomposition to generate counterfactuals for multi-class setting. However, different modes in a class does not seem to be addressed. In \cite{sun2023inherently}, authors generate class-specific attribution maps based on counterfactuals to find which region of the image are important for that classification and then use a simple logistic regression classifier to make predictions. Authors claim these methods to be inherently interpretable. In \cite{barberan2022neuroviewrnn} authors describe NeuroView-RNN as a family of new RNN architectures that explains how each hidden state per time step contributes to the decision-making process in a quantitative manner. Each member of the family is derived from a standard RNN architecture by concatenation of the hidden steps into a global linear classifier, so the weights of the classifier have a linear mapping to the hidden states and thus interpretability improves.

In \cite{jeanneret2022diffusion}, authors use gradient information from the target class for counterfactual generation of input instances to guide the generation process, instead of using one trained on noisy instances. They show that this approach is better than other methods. \cite{kanehira2019multimodal} have  proposed a method to generate counterfactual explanations with multimodal information such as visual and textual description information. They generate counterfactual explanations by training a classification model, which is the target of the explanation, and by training an auxiliary explanation model in a post-hoc manner by utilizing output and mid-level activation of the target classifier after freezing its weights to prevent change in output. 

Unfortunately, none of the above methods seem to address counterfactual explanation generation for multiple modes in a class. Our proposed \textit{DifClue} approach utilizes K-means clustering on the latent space obtained using Diffusion Autoencoder \cite{preechakul2022diffusion} to leverage the resulting disentanglement and generate counterfactuals via perturbations in the directions of the modes. We aim to use the realism as a measure to evaluate whether the generated images could have come from the original image set or not~\cite{DBLP:journals/corr/HeuselRUNKH17}.

% \begin{figure*}[t]
%     \centering
%     \includegraphics[scale=0.65]{Submission-2024/LaTeX/dissect_archi.png}
%     \caption{Overall Structure of DISSECT Model.}
%     \label{fig:fig_flowchart_dissect}
% \end{figure*}


\begin{figure*}[t]
    \centering
    \includegraphics[scale=0.6]{overall_archi.png}
    \caption{Overall flowchart for generating multiple counterfactual explanation using DifCluE}
    \label{fig:fig_flowchart}
\end{figure*}
% \vspace{-2em}