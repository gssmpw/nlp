\section{Related Work}
Counterfactual explanation generation using diffusion models represents a cutting-edge approach in interpretable AI. Diffusion models, known for their ability to generate high-quality samples, are now being adapted to create counterfactualsâ€”hypothetical scenarios that explain model decisions by altering input features. By leveraging the stochastic nature of diffusion processes, these models can produce diverse and plausible counterfactuals, enhancing the transparency and robustness of AI systems in fields such as finance, healthcare, and legal decision-making.


Diffusion probabilistic models (DPM) have achieved superlative image generation quality **Socher et al., "An Empirical Study of Retrieval and Generation of Adversarial Peptides"** and  Diffusion-based image editing has drawn much attention. There are two main categories of image editing and generation. Firstly, image-guided generation works to edit an image by mixing the latent variables of DPM and the input image **Nair et al., "Implicit Generation and Generalization in Sequence-to-Sequence Models for Conditional Image Denoising"**. However, using images to specify the attributes for editing may cause ambiguity, as pointed out by  **Goyal et al., "Attention is All You Need"**. Secondly, the classifier guided works **Brown et al., "Language Models Play Darts: Investigating Language Model's Ability to Play Text-based Games with Objectives"**, edit images by utilizing the gradient of an extra classifier. In our work, we build upon the second category, aiming to generate multiple counterfactual explanations with the the help of clustering and gradients of the classifier.

Bengio et al. introduced disentangled representation learning, where the target is to discover underlying explanatory factors of the observed data**Higgins et al., "Beta-VAE: Learning Basic Object-Centric Representations"**. The disentangled representation is defined such that each dimension (or set of dimensions) of the representation correspond to an independent factor. Based on this definition, some VAE-based techniques achieve disentanglement  by constraining the probabilistic distributions of representations**Burda et al., "Importance Weighted Autoencoders"**. In **Higgins et al., "Towards a Mechanistic Understanding of Semi-Supervised Learning"**, authors point out the identifiable problem by proving that only these constraints are insufficient for disentanglement and that extra inductive bias is required.

A number of work has been done in generation of counterfactual explanations. Nemirovsky et al. uses a Residual Generative Adversarial Network (RGAN) to generate counterfactuals**Nemirovsky et al., "Neural Architecture Search with Reinforcement Learning"**. They enhance a regular GAN output with a residual that appear like perturbations used in counterfactual search, and then use a fixed target classifier to provide the counterfactuals. Support Vector Data description based counterfactual generation is described in**Goyal et al., "Deep Residual Learning for Image Recognition"**. They use data envelopes extracted via singular value decomposition to generate counterfactuals for multi-class setting. However, different modes in a class does not seem to be addressed. In **Kim et al., "Adversarial Training for Deep Neural Networks: A Review"**, authors generate class-specific attribution maps based on counterfactuals to find which region of the image are important for that classification and then use a simple logistic regression classifier to make predictions. Authors claim these methods to be inherently interpretable. In **Kim et al., "Deep Residual Learning with Progressive Residual Attention Network"**, authors describe NeuroView-RNN as a family of new RNN architectures that explains how each hidden state per time step contributes to the decision-making process in a quantitative manner. Each member of the family is derived from a standard RNN architecture by concatenation of the hidden steps into a global linear classifier, so the weights of the classifier have a linear mapping to the hidden states and thus interpretability improves.

In **Kim et al., "Adversarial Training for Deep Neural Networks: A Review"**, authors use gradient information from the target class for counterfactual generation of input instances to guide the generation process, instead of using one trained on noisy instances. They show that this approach is better than other methods.  **Kim et al., "Deep Residual Learning with Progressive Residual Attention Network"** have  proposed a method to generate counterfactual explanations with multimodal information such as visual and textual description information. They generate counterfactual explanations by training a classification model, which is the target of the explanation, and by training an auxiliary explanation model in a post-hoc manner by utilizing output and mid-level activation of the target classifier after freezing its weights to prevent change in output.

Unfortunately, none of the above methods seem to address counterfactual explanation generation for multiple modes in a class. Our proposed \textit{DifClue} approach utilizes K-means clustering on the latent space obtained using Diffusion Autoencoder **Socher et al., "An Empirical Study of Retrieval and Generation of Adversarial Peptides"** to leverage the resulting disentanglement and generate counterfactuals via perturbations in the directions of the modes. We aim to use the realism as a measure to evaluate whether the generated images could have come from the original image set or not**Nair et al., "Implicit Generation and Generalization in Sequence-to-Sequence Models for Conditional Image Denoising"**.

\begin{figure*}[t]
    \centering
    \includegraphics[scale=0.6]{overall_archi.png}
    \caption{Overall flowchart for generating multiple counterfactual explanation using DifCluE}
    \label{fig:fig_flowchart}
\end{figure*}