\section{Related Work}
\textbf{Content Analysis.} Content analysis has long been a foundational method in the social sciences and humanities, providing a structured approach to converting qualitative text into quantitative data **Weber, "The Methodology of Social Research"**. Recently, content analysis has significantly advanced the understanding of complex social issues, ranging from political polarization **Noelle-Neumann, "The Spiral of Silence"** to emotional contagion **Harrison and Thomas, "Emotional Contagion in Social Networks"** and group dynamics **Bales, "Interaction Process Analysis"**. These traditional methods rely on manual annotation by human coders, who use predefined rules in the codebook to categorize text, often iteratively refining their coding schemes in multiple rounds of discussions **Miles and Huberman, "Qualitative Data Analysis: An Expanded Sourcebook"**. Although manual content analysis provides robust and theory-driven insights, it remains labor-intensive, time-consuming, and prone to subjectivity **LeCompte and Preissle, "Ethnography and Qualitative Design in Educational Research"**. Furthermore, as the volume of digital text increases, scaling traditional methods to accommodate larger datasets has become increasingly challenging **Krippendorff, "Content Analysis: An Introduction to Its Methodology"**. The advent of powerful AI and LLM offers an automated and more scalable solution **Vesely et al., "Large Language Models: A Review of the State-of-the-Art"**.

\noindent\textbf{Multi-agent Systems for Social Science.} Multi-agent systems (MAS) have become increasingly prevalent in computational social science, modeling social phenomena through agent individuals or groups with predefined behaviors or decision-making rules **Shoham and Leyton-Brown, "Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Approaches"**. Recent MAS explore to simulate human-like deliberation for more nuanced decision-making such as data interpretation **Brysbaert et al., "The LexTale Lexical Database: A Tool for the Study of Word Meaning and Context"**. However, existing systems often lack dynamic revolution of decision rules, collaborative interactions, and a systematic comprehension of social science principles **Gilbert and Karau, "How to Read Psychology Research"**, which cannot reflect the sophisticated and theory-informed process of content analysis.

\noindent\textbf{Human Intervention.} Human intervention remains essential for the reliable deployment of AI-driven systems **Dietz et al., "Designing Interactive Systems: From User Experience to Software Engineering"**. As a general framework, Human-in-the-loop (HITL) systems allow experts to refine AI outputs, ensuring alignment with domain-specific knowledge and mitigating algorithmic bias **Kulesza et al., "Why I Hate Active Learning"**. This is particularly important in social sciences and humanities, where interpretative depth and contextual sensitivity are critical **Giddens, "The Constitution of Society: Outline of the Theory of Structuration"**. Recent approaches **Brennan-Krause et al., "Learning to Reason with Contextualized Semantic Representations"** integrate expert feedback to adjust categories or schemes iteratively. Our framework significantly extends this line of work by designing different modes of human-AI collaboration informed by social influence theories **Festinger, "A Theory of Social Comparison Processes"** and human-computer interaction theories **Shneiderman, "Designing the User Interface: Strategies for Effective Human-Computer Interaction"**.

\begin{figure*}[th]
    \vspace{-6mm}
    \includegraphics[width=1\linewidth]{figure/main.pdf}
    \vspace{-6mm}
    % \captionsetup{width=0.45\textwidth, margin={3pt,0pt}}
    \caption{Proposed \methodName{} framework. (a) Coder Simulation. Initialize LLM agents with real-world personas and set up the codebook using predefined rules. (b) Bot Annotation. Each LLM agent independently annotates a batch of text entries into theory-informed categories based on the codebook. (c) Agent Discussion. Agents participate in multi-round discussions to resolve any inconsistencies in their annotations. Optional human interventions can be integrated to provide feedback and additional instructions. (d) Codebook Evolution. Leveraging the outcomes from the annotation and discussion phases, agents collaboratively and iteratively update the codebook. The refined codebook is then employed in subsequent iterations of the workflow. Data flows are tracked using color-coded text: \red{Red text:} codebook and rules; \blue{Blue text:} text entries; \purple{Purple text:} coding results; \orange{Orange text:} codebook evolution. A comprehensive illustration of \methodName{} framework can be found in Figure~\ref{fig:appendix_main} and Appendix~\ref{app:detailed_framework}.}
    \label{fig:main}
    \vspace{-3mm}
\end{figure*}