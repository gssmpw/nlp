    %%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[acmsmall,screen]{acmart}

% \usepackage{cite}
\usepackage{amsmath}
\let\Bbbk\oldBbbk
\usepackage{amssymb}
\usepackage{hyperref}
% \usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{array}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{colortbl}
\usepackage{svg}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{threeparttable}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{listings}
\usepackage{multicol}
\usepackage{wrapfig}
\usepackage{pdfpages}
\usepackage{tikz}
% \usepackage[numbers,sort&compress]{natbib}
% \usepackage[square,numbers]{natbib}
\newcommand{\mybox}[1]{%
  \begin{tcolorbox}[colback=white,colframe=black,lowerbox=invisible,savelowerto=\jobname_ex.tex]
    #1
  \end{tcolorbox}
}
\newcommand{\afif}[1]{\noindent\textcolor{blue}{[Afif: {#1}]}}
\newcommand{\jie}[1]{\noindent\textcolor{blue}{[Jie says: {#1}]}}
\newcommand{\gias}[1]{\noindent\textcolor{red}{[Gias: {#1}]}}
\newcommand{\borui}[1]{\noindent\textcolor{purple}{[Borui: {#1}]}}


\def\alg {Algorithm~}
% \setcitestyle{square,numbers}
 %% for \citeauthor{}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

% \setcopyright{acmlicensed}
% % \copyrightyear{2018}
% \acmYear{2025}
% \acmDOI{10.1145/3715735}

%%% The following is specific to FSE '25 and the paper
%%% 'Hallucination Detection in Large Language Models with Metamorphic Relations'
%%% by Borui Yang, Md Afif Al Mamun, Jie M. Zhang, and Gias Uddin.
%%%
\setcopyright{acmlicensed}
\acmDOI{10.1145/3715735}
\acmYear{2025}
\acmJournal{PACMSE}
\acmVolume{2}
\acmNumber{FSE}
\acmArticle{FSE020}
\acmMonth{7}
\received{2024-09-13}
\received[accepted]{2025-01-14}

\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation email}{June 03--05,
  2018}{Woodstock, NY}

% \acmISBN{978-1-4503-XXXX-X/18/06}
% \acmJournal{PACMSE}
% \acmVolume{2}
% \acmNumber{FSE}
% \acmArticle{FSE020}
% \acmMonth{7}
% \setcitestyle{numbers,sort&compress}

\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.

\title{Hallucination Detection in Large Language Models with Metamorphic Relations}

\author{Borui Yang}
\orcid{0009-0009-3482-7667}
\affiliation{%
  \institution{King's College London}
  \city{London}
  \country{United Kingdom}
}
\email{zbybr@bupt.edu.cn}

\author{Md Afif Al Mamun}
\orcid{0000-0002-9319-3483}
\affiliation{%
  \institution{University of Calgary}
  \city{Calgary}
  \country{Canada}
}
\email{afif.mamun@ucalgary.ca}

\author{Jie M. Zhang}
\orcid{0000-0003-0481-7264}
\affiliation{%
  \institution{King's College London}
  \city{London}
  \country{United Kingdom}
}
\email{jie.zhang@kcl.ac.uk}

\author{Gias Uddin}
\orcid{0000-0003-1376-095X}
\affiliation{%
  \institution{York University}
  \city{Toronto}
  \country{Canada}
}
\email{guddin@yorku.ca}

% \title{Hallucination Detection in Large Language Models with Metamorphic Relations}

% %%
% %% The "author" command and its associated commands are used to define
% %% the authors and their affiliations.
% %% Of note is the shared affiliation of the first two authors, and the
% %% "authornote" and "authornotemark" commands
% %% used to denote shared contribution to the research.
% \author{Borui Yang}
% \affiliation{%
%   \institution{King's College London}
%   \city{London}
%   \country{United Kingdom}
% }
% \email{zbybr@bupt.edu.cn}
% % \authornote{Both authors contributed equally to this research.}

% % \orcid{1234-5678-9012}

% \author{Md Afif Al Mamun}
% \affiliation{%
%   \institution{University of Calgary}
%   \city{Calgary}
%   \state{Alberta}
%   \country{Canada}
% }
% \email{afif.mamun@ucalgary.ca}

% \email{jie.zhang@kcl.ac.uk}
% \author{Jie M. Zhang}
% \affiliation{%
%   \institution{King's College London}
%   \city{London}
%   \country{United Kingdom}
% }
% \email{jie.zhang@kcl.ac.uk}
% %\authornote{Corresponding author.}

% \author{Gias Uddin}
% \affiliation{%
%   \institution{York University}
%   \city{Toronto}
%   \state{Ontario}
%   \country{Canada}
% }
% \email{guddin@yorku.ca}

% \affiliation{%
%   \institution{Institute for Clarity in Documentation}
%   \city{Dublin}
%   \state{Ohio}
%   \country{USA}
% }

% \author{Lars Th{\o}rv{\"a}ld}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \city{Hekla}
%   \country{Iceland}}
% \email{larst@affiliation.org}

% \author{Valerie B\'eranger}
% \affiliation{%
%   \institution{Inria Paris-Rocquencourt}
%   \city{Rocquencourt}
%   \country{France}
% }

% \author{Aparna Patel}
% \affiliation{%
%  \institution{Rajiv Gandhi University}
%  \city{Doimukh}
%  \state{Arunachal Pradesh}
%  \country{India}}

% \author{Huifen Chan}
% \affiliation{%
%   \institution{Tsinghua University}
%   \city{Haidian Qu}
%   \state{Beijing Shi}
%   \country{China}}

% \author{Charles Palmer}
% \affiliation{%
%   \institution{Palmer Research Laboratories}
%   \city{San Antonio}
%   \state{Texas}
%   \country{USA}}
% \email{cpalmer@prl.com}

% \author{John Smith}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \city{Hekla}
%   \country{Iceland}}
% \email{jsmith@affiliation.org}

% \author{Julius P. Kumquat}
% \affiliation{%
%   \institution{The Kumquat Consortium}
%   \city{New York}
%   \country{USA}}
% \email{jpkumquat@consortium.net}

% %%
% %% By default, the full list of authors will be used in the page
% %% headers. Often, this list is too long, and will overlap
% %% other information printed in the page headers. This command allows
% %% the author to define a more concise list
% %% of authors' names for this purpose.
% \renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
    % Large Language Models (LLMs) have achieved remarkable progress in natural language processing by generating text that closely resembles human language. Despite these advancements, 
    Large Language Models (LLMs) are prone to hallucinations, e.g.,   factually incorrect information, in their responses. These hallucinations present challenges for LLM-based applications that demand high factual accuracy.  Existing hallucination detection methods primarily depend on external resources, which can suffer from issues such as low availability, incomplete coverage, privacy concerns, high latency, low reliability, and poor scalability. There are also methods depending on output probabilities, which are often inaccessible for closed-source LLMs like GPT models. This paper presents MetaQA, a self-contained hallucination detection approach that leverages metamorphic relation and prompt mutation. 
    Unlike existing methods, MetaQA operates without any external resources and is compatible with both open-source and closed-source LLMs. 
    MetaQA is based on the hypothesis that if an LLM's response is a hallucination, the designed metamorphic relations will be violated. 
We compare MetaQA with the state-of-the-art zero-resource hallucination detection method, SelfCheckGPT, across multiple datasets, and on two open-source and two closed-source LLMs.
Our results reveal that    MetaQA outperforms SelfCheckGPT in terms of precision, recall, and f1 score.
     For the four LLMs we study, MetaQA outperforms SelfCheckGPT with a superiority margin ranging from 0.041 - 0.113 (for precision), 0.143 - 0.430 (for recall), and 0.154 - 0.368 (for F1-score).
     For instance, with Mistral-7B, MetaQA achieves an average
F1-score of 0.435, compared to SelfCheckGPTâ€™s F1-score of 0.205, representing an improvement rate of
112.2\%.
     MetaQA also demonstrates superiority across all different categories of questions. 

\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010178.10010179</concept_id>
       <concept_desc>Computing methodologies~Natural language processing</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10011007.10011074.10011099.10011102.10011103</concept_id>
       <concept_desc>Software and its engineering~Software testing and debugging</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Natural language processing}
\ccsdesc[300]{Software and its engineering~Software testing and debugging}
%%
\keywords{Large Language Models, Hallucination Detection, Metamorphic Relations}
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

Large Language Models (LLMs) like GPT-4 \cite{openai2023gpt} have revolutionized language processing by generating fluent and organized responses for various applications, such as drafting reports and summarization systems \cite{moramarco2021preliminary, lai2022exploration, zhang2023concepteva}. 
However, LLMs are prone to generating hallucinationsâ€”coherent but factually incorrect or irrelevant outputs,
such as non-factual responses in question-answering context \cite{huang2023survey, xu2024hallucination}.
This tendency poses significant challenges to the reliability of LLMs, undermining the effectiveness of LLMs in applications requiring high factual accuracy. 
Fact-conflicting hallucinations, where LLMs produce content that contradicts established facts, are particularly concerning as they can mislead users lacking expertise on the topic, leading to significant confusion and eroding the trust essential for various LLM applications. Figure \ref{example} illustrates a scenario where a legal question is posed to ChatGPT, which generates a response containing hallucinations. Without proper detection, such inaccuracies could have serious consequences for individuals who lack expertise in the legal field.

\begin{wrapfigure}[9]{r}{0.45\linewidth}
    \vspace{-1em}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/example.pdf}
    \vspace{-3mm}
    \caption{An example of a hallucinated output generated by ChatGPT in the legal domain.}
    \label{example}
\end{wrapfigure}



Many approaches have been introduced to detect or mitigate hallucination of LLMs that compare responses to factual information, often relying on databases or search engines \cite{guo2022survey, augenstein2019multifc, Hanselowski2019ARA, Atanasova2020GeneratingFC, 202307.1723, huo2023retrieving}. 
However, relying on external resources for hallucination detection often 
 limits the scope to specific domains where a comprehensive database does not exist.
Moreover, hallucinations are observed across a wide range of tasks that extend beyond simple fact verification~\cite{kryscinski2019evaluating}.
Other works use token-level information such as token confidence and entropy \cite{varshney2023stitch, yao2023llm}, which is often inaccessible in closed-source models. 
% Additionally, some approaches involve using another language model as an agent for hallucination detection \cite{cohen2023lm}, which is not feasible in zero-resource settings - when external resources are not available to validate LLM responses. While there are some approaches like 
To tackle these issues, Manakul et al. proposed SelfcheckGPT \cite{manakul2023selfcheckgpt}, a self-contained approach to addressing hallucination.
However, for the sample generation process in SelfCheckGPT, the LLM tends to produce samples that are identical to the original response, which significantly impacts its performance in hallucination detection. 


% they are often limited by the accuracy of detection or the type of questions being asked when there is no context available. Therefore, the motivating idea of our research is that, when posing a question to an LLM and receiving a generated response, it is challenging to determine whether hallucination issues have occurred without knowing the context of the question or employing external tools. 



This paper introduces MetaQA, a novel zero-resource technique that employs Metamorphic Relations (MRs) to detect hallucinations in LLM responses. MetaQA uses MRs to generate response mutations and verifies these mutations against expected outcomes to identify inconsistencies. Acting as a test oracleâ€”similar to mechanisms in software testing, MetaQA ensures reliable factual accuracy checks of the LLM outputs without requiring additional agents. This method is advantageous as it relies solely on the LLM itself. MetaQA is applicable to both open-source and closed-source LLMs. MetaQA can be used without the need for intermediate processes or external tools.

We evaluate MetaQA using three datasetsâ€”TruthfulQA, HotpotQA, and FreshQA â€” the widely studied benchmarks in hallucination evaluation, across four LLMs: GPT-4, GPT-3.5, Llama3, and Mistral. 
Our results reveal that 
MetaQA consistently outperforms SelfCheckGPT in terms of precision,
recall, and f1 score on all the four LLMs we study. In particular, for the four LLMs, MetaQA outperforms SelfCheckGPT with a superiority margin ranging from 0.154 to 0.368 in terms of F1-score.
Our ablation studies also show that
MetaQA has considerable stability across multiple runs, and has better performance with 
lower temperatures.


Our major contributions include:

\begin{enumerate}[leftmargin=20pt]
    \item To the best of our knowledge, we are the first to apply synonym and antonym-based metamorphic relations to detect hallucination LLMs responses.
    \item We improve the TruthfulQA \cite{lin2021truthfulqa} benchmark by updating 238 questions with new correct answers. We share the improved benchmark, which we name \textit{TruthfulQA-Enhanced}. This new benchmark can support more accurate hallucination detection research.
    \item We conduct a large-scale evaluation of MetaQA on TruthfulQA-Enhanced, FreshQA, and HotpotQA datasets, showing superior performance over the baseline method SelfCheckGPT.
\end{enumerate}

%\noindent\textbf{Replication Package.} \gias{put url}

% Overall, MetaQA offers a robust and effective method for hallucination detection, outperforming existing techniques and providing a valuable tool for evaluating LLM responses.

% In this paper, we propose MetaQA, a technique that uses Metamorphic Relations (MRs) to detect hallucination in LLM responses. MetaQA is a simple prompt-based approach that can self-detect fact-conflicting hallucinations in the question-answering process with LLMs. Particularly, in a zero-resource context, where only the LLM itself is accessible, MRs can be employed to generate mutations from the response. The fact verification results of the LLM for these mutations should also satisfy the expected outcomes of the metamorphic relations. In the context of software testing, a test oracle is a mechanism for determining whether a test has passed or failed. Here, metamorphic relations act as our test oracles. For example, if the negation of a response is verified as factual, it implies the original response may contain a hallucination, failing our test oracle. This methodology ensures that the LLM's outputs are consistently verified against reliable test oracles derived from metamorphic relations, thereby detecting hallucinations more effectively. Since MetaQA's entire detection process relies solely on the LLM itself and does not concern the intermediate processes, it has the added benefit of applying to both open-source and closed-source language models without needing any other agents. 
% % Two metamorphic relations are used in MetaQA: Synonymy Relation and Antonymy Relation, Synonymy Relation indicates that the response and its mutation share the same meaning and Antonymy Relation implies the response and its mutation convey opposite meanings. 

% %\subsection{Motivating Scenario}
% By combining multiple datasets and conducting experiments on various state-of-art open-source and closed-source models including GPT-4o, GPT-3.5, Llama3 \cite{dubey2024llama}, and Mistral \cite{jiang2023mistral}, we validated MetaQA's capability in detecting hallucinations in LLMs. The results concluded that MetaQA is a highly effective hallucination detection method, outperforming our baseline, the state-of-the-art zero-resource black-box method SelfCheckGPT\cite{manakul2023selfcheckgpt}, across most thresholds.


% This work makes the following major contributions:
% \begin{itemize}[leftmargin=10pt]
%     \item We are the first to combine metamorphic relations with hallucination detection in LLMs, introducing the MetaQA method. MetaQA employs two types of metamorphic relations, Synonymy Relation and Antonymy Relation, to generate mutations for detecting hallucinations in LLM responses.
%     \item To help with LLM hallucination detection research, we enhanced the state-of-the-art (SOTA) Q\&A benchmark TruthfulQA \cite{lin2021truthfulqa} by expanding the set of correct answers for certain questions\gias{explain what we did to enhance}\borui{Done}. We call this new benchmark TruthfulQA-Enhanced, and in this new version of TruthfulQA, totally 238 questions are updated with new correct answers.
%     \item We evaluated MetaQA on three datasets, TruthfulQA-Enhanced, FreshQA \cite{vu2023freshllms}, and HotpotQA \cite{yang2018hotpotqa}, across four LLMs to evaluate and compare MetaQA with the baseline method SelfCheckGPT. Overall statistics from our experiments across all datasets show that, at a threshold of 0.5, MetaQA detects 51.5\% of all hallucinations with an accuracy of 69.4\%, whereas SelfCheckGPT detects only 33.0\% of all hallucinations with an accuracy of 65.3\%. For thresholds \(\theta \in [0.2, 0.7]\), MetaQA demonstrates an overall performance improvement ranging from 16.41\% to 80.04\% compared to SelfCheckGPT, validating the effectiveness of the MetaQA approach.
    
%     %Additionally, we propose and share a hallucination mitigation strategy based on metamorphic relations as a potential avenue for future research.
% \end{itemize}


%\section{Preliminaries}

% A motivating example of MetaQA is presented in this section. We aim to detect whether the LLM's response to the question in Figure \ref{example} contains hallucination in LLM's response. Figure \ref{motivate} illustrates our motivating idea, thought process and a simple comparison process between MetaQA and our baseline method SelfCheckGPT, which involves using mutations generated by metamorphic relations to detect hallucinations in LLM's response. SelfCheckGPT fails to detect hallucinations in this sample because it relies on generating multiple responses by querying the LLM multiple times. It then determines whether the original response contains hallucinations by comparing the meanings of these N responses to the original. In this case, however, the generated comparison responses were very similar to each other and all conveyed the same incorrect information, which prevented SelfCheckGPT from detecting the hallucination. During the process of deploying MetaQA, we assess whether the response `If a private employer fires you for a Facebook post about vaccines, it may potentially violate your First Amendment right to free speech' is factually accurate and free of hallucinations. If so, its synonymous variations, which convey the same or similar meaning, should also be considered correct ground truth. Therefore, we conduct factual verification on the synonym mutations and record the results. In the second step, we consider antonym mutations, which share the opposite meaning to the original response. If the original response does not exhibit any hallucination issues, the factual verification of the antonym mutations should yield a non-factual result. We again verify and record the outcomes. Based on the verification results from these two metamorphic relations, we can assess the likelihood of hallucination in the original response. At this stage, we attempt to generate a hallucination score to quantify the potential for hallucination in the response. \afif{Not descriptive enough. Doesn't establish or discuss the steps involving Figure 2}\gias{explain Figure \ref{motivate} step by step and with more details here. Don't worry about page size for now.}\borui{Done}




\section{Preliminaries}
\subsection{Motivating Example}\label{sec:motivation}
%\jie{move Figure 5 here}

To further motivate our approach, we reexamine the hallucinated example in Figure \ref{example} using SelfCheckGPT, the method closely aligned with ours, but which fails to detect the hallucination. SelfCheckGPT operates by generating \( N \) response samples for the same query and comparing their semantic similarity to the base response. It calculates a \textit{hallucination score} based on whether these samples support the factual content of the base response. Figure \ref{fig:motivate} shows that when the LLM was repeatedly prompted with the same query using SelfCheckGPT, it consistently generated similar hallucinated responses most of the time. This led to a hallucination score of 0.1, as the generated samples mostly remained consistent with the base response, reinforcing the incorrect information. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=.9\textwidth]{figures/motivating.pdf}
    \caption{A sample of our motivating idea on MetaQA and a simple comparison where SelfCheckGPT fails to detect hallucination.}
    \label{fig:motivate}
\end{figure*}


In contrast, MetaQA applies Metamorphic Relations (MR)~\cite{chen2018metamorphic, zhang2014search} to introduce controlled mutations by generating a set of mutations from the base response. Formally, let \( B \) be the base response. MetaQA defines a function \( f(B) \) using the same LLM that produces two sets of mutations: a set of synonymous mutations \( S = f_s(B) = \{ s_0, s_1, s_2, \dots, s_N \} \) and a set of antonymous mutations \( A = f_a(B) = \{ a_0, a_1, a_2, \dots, a_M \} \). For each mutation, the system verifies whether the transformation remains factually consistent. This verification process assigns a hallucination score based on the factual alignment of both mutation sets \( S \) and \( A \). The advantage of MR over repeated prompts is that MR introduces changes to the input that make LLMs reveal inconsistencies more effectively, particularly when hallucinations are present \cite{hyun2024metal}. While repeated prompts often produce consistent outputsâ€”even if the base response contains hallucinationsâ€”MR's varied mutations expose these inconsistencies by prompting the LLM to produce divergent responses. Additionally, independently verifying each mutation reduces bias from previous outputs, resulting in a more accurate and reliable detection of hallucinations \cite{dhuliawala2023chain}. Revisiting the example shown in Figure \ref{fig:motivate}, we observe that while SelfCheckGPT computed a very low hallucination score, our approachâ€”which uses various MRs and validates each mutation individuallyâ€”resulted in a high hallucination score of $1.0$ as none of the mutations were validated as a fact. This indicates a greater likelihood of detecting hallucinations in this case.


%\subsection{Preliminaries}
\begin{wrapfigure}[14]{r}{0.5\linewidth}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/overallhalu.pdf}
    \caption{\textbf{}Overall Hallucination Rate of Different LLMs. }
    \label{overallhalu}
\end{wrapfigure}

\subsection{LLM Hallucination Severity}

Despite recent advancements in Large Language Models (LLMs), they still suffer significantly from the issue of hallucinations.
Figure~\ref{overallhalu} 
presents a heatmap depicting the hallucination rates of various experimental Large Language Models (LLMs) across three different datasets: TruthfulQA Enhanced, HotpotQA, and FreshQA. The hallucination rates are represented by the numerical values within each cell, where lower values indicate a lower rate of hallucinations and higher values indicate a higher rate.
We can observe that current LLMs have a significant tendency to produce hallucinations in question answering, with rates ranging from 17\% to 55\%. GPT-3.5, Llama3, and Mistral all exhibited high hallucination rates, particularly in the experiments on the HotpotQA dataset, with rates of 55\%, 50\%, and 51\% respectively. Only GPT-4 demonstrated a significantly lower hallucination rate, with rates of 18\% and 17\% on the TruthfulQA-Enhanced and FreshQA datasets, and 28\% on the HotpotQA dataset.


\subsection{LLM Hallucination Types} \textbf{Hallucination of LLMs} has been extensively studied in the realm of Natural Language Processing (NLP). Hallucination typically refers to a phenomenon where the generated content appears nonsensical or unfaithful to the provided source content \cite{ji2023survey}. Generally, hallucination in natural language generation tasks can be categorized into three primary types \cite{yao2024survey, zhang2023siren}, as detailed below:
\begin{itemize}[leftmargin=10pt]
    \item \textbf{Input-Conflicting Hallucination}: This type occurs when LLM generates outputs that are inconsistent with the user's input. Typically, such inconsistencies can present themselves in two primary ways: the LLM's response might conflict with the user's task instructions, suggesting a misinterpretation of the intent, or the generated output might contradict the task input, similar to common issues in machine translation, or summarization \cite{li2022faithfulness}. 
    \item \textbf{Context-Conflicting Hallucination}: This arises when the output of the LLM contradicts the contextual information provided by the user. Such inconsistencies often occur in lengthy or multi-turn interactions, where the model may lose track of the context or fail to maintain consistency throughout the conversation.
    \item \textbf{Fact-Conflicting Hallucination} occur when generated content contradicts established world knowledge \cite{chen2023complex, chern2023factool, min2023factscore}. Several factors throughout the life-cycle of an LLM, including the training dataset, pre-training procedures, and the inference process, can contribute to such type of hallucination. For instance, as shown in Figure \ref{example}, an LLM might present inaccurate information in response to a user query, potentially misleading users who lack expertise on the topic.
\end{itemize}
The dissemination of incorrect information through misleading answers can have serious consequences. Therefore, in this paper, we focus on the detection of fact-conflicting hallucinations. Such hallucinations can significantly undermine trust in the generated content, particularly when users lack knowledge of the correct ground truth.

\subsection{Metamorphic Relation (MR)} A metamorphic relationship $R$ is a necessary property between the set of inputs $X = <x_1, x_2, ..., x_n>$ and their corresponding outputs $Y = <f(x_1), f(x_2), ... f(x_n)>$ from a given function $f$ and is denoted as $R\subseteq X^n \times Y^n$ or simply as $R(x_1, x_2, ..., x_n, f(x_1), f(x_2), ... f(x_n))$~\cite{chen2018metamorphic,zhang2014search}. Metamorphic relations can be applied independently or in conjunction with other static and dynamic software analysis techniques, such as formal proofs and debugging. Essentially, an MR is a property or constraint that the output of a system should satisfy when specific transformations or modifications are applied to its input. If the system's output changes inappropriately in response to these transformations, it may indicate that the system is producing incorrect or unreliable results.

In the context of Natural Language Generation, combining Metamorphic Relations provides a robust method for detecting hallucination issues in LLM. Applying MRs allows verification of whether the model maintains expected semantic relationships when faced with various input transformations.
 nor it indicate the limitation of the existing approach (i.e. SelfcheckGPT)



\section{MetaQA Methodology}
% During the interactions with LLMs, particularly in a `zero-resource' scenario where no external database or search engine is used, we face the challenge of our potentially lacking knowledge of the correct ground truth, making it difficult to independently determine whether a LLM's response is hallucination. MetaQA is our proposed Zero-Resource fact-conflicting hallucination detection scheme, which operates by comparing the verification results of multiple mutations and evaluating the hallucination possibilities with LLM. MetaQA consists of five main modules, each of which will be detailed in the following paragraphs, and the workflow of MetaQA is illustrated in Figure \ref{workflow}.

% In interactions with large language models (LLMs), particularly in a \textit{zero-resource} setting where no external databases or search engines are available, a key challenge is the lack of access to ground truth data or the fact. In such setups, it is challenging to verify whether an LLM's response is accurate or a hallucination. To address this, we propose 
MetaQA is a hallucination detection framework that focuses on detecting factually conflicting outputs. MetaQA does this by comparing multiple responses for a given question, where follow-up questions to the given question are generated by using MRs. The MetaQA framework is structured into five steps (see Figure \ref{fig:workflow}): \textit{(1) Concise Question-Answering, (2) Mutation Generation, (3) Mutation Verification, and (4) Hallucination Evaluation}. Algorithm \ref{alg:mutation-generation} outlines the complete process from generating mutations based on the initial LLM response to calculating the hallucination score.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=1\linewidth]{figures/workflow.pdf}
    \caption{The workflow of MetaQA}
    \label{fig:workflow}
\end{figure}

\input{algo/algo}

\subsection{Step 1. Concise Question-Answering}
We guide LLM to produce concise and fact-based answers in response to an input question. In typical interactions, LLMs often generate detailed and lengthy explanations. However, within the MetaQA framework, excessive verbosity can hinder downstream modules involved in tasks such as mutation generation and verification. To address this, we employ system prompts that instruct the LLM to generate brief, contextually grounded answers. This ensures that responses remain precise. Recent studies have shown that LLMs are effective in evaluating the consistency of information between lengthy documents and concise summaries, even in zero-shot settings \cite{luo2023chatgpt}. Therefore, in addition to the standard question-answering process, we instruct the model to summarize its responses into short, accurate sentences. 
% Table \ref{table:system-prompt} illustrates the prompt structure used to achieve this concise answering strategy.
% \begin{table}[ht]
% \centering
% \begin{tabular}{c|p{10cm}}
%     \toprule
%     \rowcolor[rgb]{.886, .937, .855} \textbf{Role} & \textbf{Prompt} \\
%     \midrule
%     \rowcolor[rgb]{.886, .937, .855} \textbf{System} & For the question, please answer in 1 sentence including the question context. Please notice, that your answer must consider real facts, not myths, fairy tales, or legends. If possible, do not include "yes" or "no" at the beginning of the sentence. \\
%     \midrule
%     \rowcolor[rgb]{.886, .937, .855} \textbf{User} & \textbf{ACTUAL\_QUESTION} \\
%     \bottomrule
% \end{tabular}
% \caption{Prompts used for concise question-answering.}
% \label{table:system-prompt}
% \end{table}

\subsection{Step 2. Mutation Generation}
For a given response (we call it \textit{base} response)  to a question from Step 1, we create multiple distinct high-quality mutations to the response. Each mutation is produced as a follow-up question to the base response. MetaQA employs a prompt-based approach that exclusively uses the LLM to generate mutations\footnote{
We do not adopt traditional synonym- and antonym-based mutation generation methods~\cite{sun2020automatic,9000651} because LLMs have consistently demonstrated superior performance over traditional NLP techniques across almost all NLP tasks~\cite{qin2024large,foster2025mutation}.}. Thus, for potentially overly brief responses from the LLM, this method will generate complete and semantically accurate mutations based on the context of the question. A mutation is denoted as being generated in the following manner: 
\begin{equation}
    mutation = f(subject, relation)
\end{equation}
where $subject$ is constructed by the question-response pair. In Algorithm \ref{alg:mutation-generation}, the steps for generating mutations based on a predefined relation are detailed in lines \ref{line:init_mt_generation} to \ref{line:end_mt_generation}. These steps involve using a predefined prompt template for each type of metamorphic relationship. We will explore these relationships in greater detail in the following sections.

% \gias{briefly summarize the steps of Algorithm \ref{alg:mutation-generation} here before you discuss the MRs below.}

We deploy two general types of reasoning rules prevalently adopted in several literature \cite{abboud2020boxe, liang2024survey, ren2020beta, tian2022knowledge, zhou2019completing}, the types of metamorphic relations are detailed as follows:

\subsubsection{Relation 1: Synonymy Relation}

A synonymous mutation refers to a variation of the response generated by the LLM that maintains the same semantic meaning as the original response. For example, a general structure of the response sentence typically follows a specific order: ($subject$, $verb$, $object$). In the synonymy relation, the following types are included:
\begin{itemize}[leftmargin=10pt]
    \item \textbf{Lexical Substitution}: The subject and object in a sentence remain unchanged, semantic consistency is maintained by replacing other parts of the sentence with synonyms.
    \begin{equation}
        R(sub, verb_{1}, obj) \Rightarrow R'(sub, verb_{2}, obj)
    \end{equation}
    where $verb_{2}$ should share the close meaning to $verb_{1}$, plus fits the new structure.
    \item \textbf{Inversion}: In an inverse relationship, the subject and object can be reversely linked through a variant of the original relation, as demonstrated below:
    \begin{equation}
        R(sub, verb, obj) \Rightarrow R'(obj, verb', sub)
    \end{equation}
    where $verb'$ is the modified $verb$ that fits the new structure.
\end{itemize}
In this context, evaluating the synonymous mutation of a response aims to determine whether such mutation, which maintains semantic equivalence, is classified as factual or non-factual, thereby assessing whether the initial response manifests hallucination.

\subsubsection{Relation 2: Antonymy Relation}

In the domain of antonymy, given a question-response pair, we can verify the authenticity of the antonymous mutation, which conveys an opposite semantic meaning to the original response. With this relation, for a response $R$ without hallucination, its antonymous mutation $\overline{R}$ should not be considered as the correct ground truth. An antonymous mutation can be generated by the formula below:
\begin{equation}
    R(sub, verb, obj) \Rightarrow \overline{R}(sub, \neg{verb}, obj)
\end{equation}

As illustrated in Table \ref{tab1}, before initiating our interaction with LLM, we predefined specific instructions and prompt templates, requesting the model to use its inherent knowledge and inferential capabilities to deliver accurate and specific content as per our requirements. The primary aim is to ensure LLM provides easily analyzable responses by using standardized prompts and instructions.

\begin{table}[!htbp]
    \centering
    \caption{Templates for generating synonym and antonym mutations in the mutation process.}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{p{4.19em}llllll|p{4.19em}llllll}
        \toprule
        \multicolumn{7}{c|}{\textbf{Synonym Mutation}} & \multicolumn{7}{c}{\textbf{Antonym Mutation}} \\
        \midrule
        \multicolumn{7}{l|}{\textbf{Instruction}: Generate synonym mutations of the base response} & \multicolumn{7}{l}{\textbf{Instruction}: Generate antonym mutations of the base response} \\
        \midrule
        \multicolumn{7}{p{29.33em}|}{\textbf{Query}: Generate synonym mutations of the answer based on the context of the question and return a numbered list to me.\newline{}Do not add any information that's not provided in the answer nor asked by the question. Make sure the generated synonyms are meaningful sentences.\newline{}For example:\newline{}Question: What is the most popular sport in Japan?\newline{}Answer: Baseball is the most popular sport in Japan.\newline{}Mutations:\newline{}1. Japan holds baseball as its most widely embraced sport.\newline{}2. The sport with the highest popularity in Japan is baseball.\newline{}3. Baseball reigns as Japan's most favored sport among the populace.\newline{}Notice how the full context is included in each generated synonym. If\newline{}you generated just `baseball', it would not make a meaningful sentence.} & \multicolumn{7}{p{29.33em}}{\textbf{Query}: Generate negations of the answer based on the context of the question and return a numbered list to me.\newline{}Do not add any information that's not provided in the answer nor asked by the question. A correct negation should directly contradict the original sentence, rather than making a different statement. Make sure the generated antonyms are meaningful sentences.\newline{}For example:\newline{}Question: What is the most popular sport in Japan?\newline{}Answer: Baseball is the most popular sport in Japan.\newline{}Mutations:\newline{}1. The most popular sport in Japan is not baseball.\newline{}2. Baseball is not the most popular sport in Japan.\newline{}3. Japan does not consider baseball as the most popular sport.\newline{}Be careful about double negations which make the sentence semantically same to the provided one. The context of the question is really important. Notice how the negations are meaningful sentences in the example. You should negate the meaning of the sentence based on the question.} \\
        \bottomrule
        \end{tabular}%
        }
    \label{tab1}%
  \end{table}%

\subsection{Step 3. Mutation Verification}
Based on the generated mutations, MetaQA prepares test cases and obtains verification results through a straightforward yet effective question-answering process with LLMs. The Mutation Verification module within MetaQA serves as a specialized component for validating mutations derived from the original question-response pairs. This module independently verifies each mutation with respect to the MR used to generate the mutation (Line \ref{line:mt_verification} in Algorithm \ref{alg:mutation-generation}). It employs a prompt-based method that allows the LLM itself to rigorously assess whether the mutations conform to established facts and principles, thereby validating them as correct ground truths. This approach effectively addresses challenges presented by questions involving myths, fairy tales, or other fictional contexts, such as \textit{"How many days did it take to create the world?"}--which can lead to misleading or ambiguous responses. To ensure accuracy, prompt templates are used to guide the LLM in generating precise responses, thus enhancing the overall reliability of the hallucination detection process. Specifically, if the original response is correct, its synonymous mutations are expected to return \textit{"Yes"} when verified and its antonymous mutation should return \textit{"No."} In the instances, when the LLM is not sure of the correctness of the fact of a mutation is required to return \textit{"Not Sure"}.

\subsection{Step 4. Hallucination Evaluation}
% The objective of our proposed module is to enhance the detection of fact-conflicting hallucination within LLM outputs, specifically through the metamorphic relations between the mutated and original responses. In the final module, MetaQA automatically calculates the hallucination score of the original responses from the LLM, representing the likelihood of hallucination. This score is based on the verification results, assuming a response is considered completely correct, its synonymous mutation should verify as `Yes', while its antonymous mutation should be verified as `No'. Initial investigation showed that LLM will output either `Yes' or `No' 98\% of the time they verify the mutations, while any remaining outputs can be set to `Not Sure'. For response $R$, the verification of its synonymous mutation $S_(i)$ is converted to a $symscore_{i}$ using the mapping \{``Yes'': 0.0, ``No'': 1.0, ``Not Sure'': 0.5\}, and the verification of its antonymous mutation $A_(j)$ is converted to a $antscore_{j}$ using the mapping \{``Yes'': 1.0, ``No'': 0.0, ``Not Sure'': 0.5\}. The higher the hallucination score, the greater the likelihood of hallucination. The final hallucination score is then calculated as:
% \begin{equation}
%     Score(Q) = \frac{1}{N} \sum_{i=1}^{N} symscore_{i} + \frac{1}{M} \sum_{j=1}^{M} antscore_{j}
% \end{equation}
The objective of this module is to enhance the detection of fact-conflicting hallucinations in LLM outputs by analyzing the metamorphic relations between the original and mutated responses. MetaQA calculates a hallucination score based on the verification of synonymous and antonymous mutations representing the likelihood of hallucination in the base LLM response. Depending on the mutation and the verification response a hallucination score is attributed to each response.

In our observations, the LLM typically returns "Yes" or "No" for most of the verification responses, with "Not Sure" occurring rarely. Let \( RS_i \) and \( RA_j \) represent the verification responses for synonymous mutation \( S_i \) and antonymous mutation \( A_j \), respectively. We assign hallucination scores on a scale of \( [0, 1] \) based on the response type. "Not Sure" is considered equally likely to indicate hallucination or fact, thus receiving a score of 0.5. The scores are mapped as follows:

\begin{multicols}{2}
\textbf{Synonymous Mutation Score Mapping} \\
\[
\text{SynScore}(S_{i}) = \begin{cases} 
0.0 & \text{if } RS_i = \text{"Yes"} \\
1.0 & \text{if } RS_i = \text{"No"} \\
0.5 & \text{if } RS_i = \text{"Not Sure"}
\end{cases}
\]

\columnbreak

\textbf{Antonymous Mutation Score Mapping} \\
\[
\text{AntScore}(A_{j}) = \begin{cases} 
1.0 & \text{if } RA_j = \text{"Yes"} \\
0.0 & \text{if } RA_j = \text{"No"} \\
0.5 & \text{if } RA_j = \text{"Not Sure"}
\end{cases}
\]
\end{multicols}

The total hallucination score $S_{QB}$ for a question \( Q \) and base response B, is calculated as:

\begin{equation}\label{eq:hallu-threshold}
    S_{QB} = \frac{\sum_{i=1}^{N} \text{SynScore}(S_{i}) + \sum_{j=1}^{M} \text{AntScore}(A_{j})}{M + N}
\end{equation}

where \( N \) is the number of synonymous mutations and \( M \) is the number of antonymous mutations.
Hallucination Score \( S_{QB} \) indicates the likelihood of hallucination in the original LLM responses. In Algorithm \ref{alg:mutation-generation}, we calculate the hallucination score from Line \ref{line:start_mt_score} to \ref{line:end_mt_score}. Finally, to classify a response as a hallucination, we compare \( S_{QB} \) with a predefined threshold \( \theta \). Specifically, a response is classified as a hallucination if \( S_{QB} \ge \theta \).

In our experiments, we use this criterion to determine whether a response is a hallucination. Since SelfCheckGPT employs a similar methodology to generate Hallucination Scores, we can directly compare our results with those of SelfCheckGPT under the same benchmark.

\section{Experimental Setup}
We answer the following research questions (RQs):
\begin{enumerate}[leftmargin=30pt, label=\textbf{RQ\arabic{*}.}]
    \item \textbf{Effectiveness}: How effective is MetaQA in detecting hallucination in LLMs? 
    \item \textbf{Generalization}: How does MetaQA perform on questions from various categories? 
    \item \textbf{Stability}: How stable is MetaQA in its hallucination detection performance? 
    \item \textbf{Sensitivity to mutants}: How do the categories and number of mutations impact MetaQA's overall performance?
    \item \textbf{Sensitivity to threshold}: How does the performance of MetaQA and SelfCheckGPT vary across different threshold settings?
\end{enumerate}
RQ1 studies the effectiveness of MetaQA in identifying fact-conflicting hallucinations in LLMs and evaluates whether MetaQA outperforms baseline methods in hallucination detection. RQ2 categorizes the fact-conflicting hallucination issues of various LLMs identified by MetaQA and studies the performance of MetaQA on specific question categories. RQ3 examines whether MetaQA provides consistent and stable results across multiple runs. RQ4 explores the impact of using different numbers of mutations within MetaQA in identifying fact-conflicting hallucination issues. RQ5 explores the performance variations of MetaQA and SelfCheckGPT as a function of changing threshold values based on Equation \ref{eq:hallu-threshold}. 

% To rigorously evaluate the proposed method MetaQA, we conducted a series of experiments designed to address key research questions and validate our approach.

\subsection{Baseline}  
We use SelfCheckGPT \cite{manakul2023selfcheckgpt}, the state-of-the-art (SOTA) hallucination detection approach that does not need external resources. As outlined in Section \ref{sec:motivation}, by repeatedly querying an LLM to generate reference samples and by measuring their consistency with the original response, SelfCheckGPT calculates a hallucination score for user reference. As such, both MetaQA and SelfCheckGPT use a threshold based on Equation \ref{eq:hallu-threshold}) to determine whether a response is hallucinated or not. We used the publicly available version of SelfCheckGPT \cite{manakul2023selfcheckgpt}, which by default calls the ChatGPT API without specifying an explicit temperature value. In our preliminary experiments, we tested different temperature (T) values and found that the tool performed best with a temperature of 0.5 (specific results with different temperatures are shown on our homepage\footnote{\url{https://github.com/zbybr/LLMhalu/tree/ForMetaQAPaper}}). Therefore, in our evaluation, by default, we use threshold 0.5 to compare MetaQA and SelfCheckGPT.
We organize a separate RQ (RQ5) to compare these two methods across all threshold conditions. 
% SelfCheckGPT employs various techniques, including BERTScore, N-gram, NLI, and Prompt-based methods. Among these, SelfCheckGPT with prompt closely aligns with our method, making it a highly suitable baseline for this study. 

% The paper of SelfCheckGPT categorizes hallucinations into three types: Accurate (hallucination score = 0), minor inaccurate (hallucination score between 0 and 0.5), and major inaccurate (hallucination score between 0.5 and 1). 
% In our evaluation, we compare MetaQA and SelfCheckGPT across all threshold conditions. 
% Since SelfCheckGPT uses a similar method to generate Hallucination Scores, we can compare our results with those scores of SelfCheckGPT under the same benchmark.

\subsection{Datasets} We adopt three benchmark datasets: TruthfulQA \cite{lin2021truthfulqa},
HotpotQA \cite{yang2018hotpotqa},
and FreshQA \cite{vu2023freshllms}.
Overall, TruthfulQA serves as a test for misconceptions, HotpotQA emphasizes logical comparisons and reasoning, and FreshQA prioritizes the timeliness of the samples. 

\subsubsection{TruthfulQA} We use TruthfulQA \cite{lin2021truthfulqa} as our primary experimental dataset. This dataset contains 817 questions designed to reveal responses based on false beliefs or misconceptions, spanning 38 categories such as health, law, finance, and politics. During our initial experiments, we discovered that many potentially correct LLM responses were not included in the `correct answers' provided by TruthfulQA. Consequently, we conducted a thorough review using sources such as official government agencies to double-check all questions in the TruthfulQA dataset. This was necessary due to deficiencies identified in the dataset, including missing correct answers and outdated responses, which we uncovered through manual verification of the model's replies. As a result, we updated 238 entries in the TruthfulQA dataset. These updates were crucial, as they provided more accurate answers to many questions over time. We refer to this revised dataset as `TruthfulQA-Enhanced', a sample is shown in Table \ref{enhanced}.

\begin{table}[htbp]
  \centering
  \caption{An Improved Question Sample in TruthfulQA-Enhanced}
  %\resizebox{0.8\textwidth}{!}{
    \begin{tabular}{p{13cm}}
    \toprule
    \textbf{Question:} What are some cities that get more rainfall than Seattle? \\
    \midrule
    {\textbf{Best Answers:}\newline{}Miami and New Orleans get more rainfall than Seattle\newline{}\textbf{Correct Answers:}\newline{}Miami and New Orleans get more rainfall than Seattle; Boston and New York get more rainfall than Seattle; Pittsburgh and Montreal get more rainfall than Seattle; Even though Seattle is nicknamed the "Rain City", Miami gets more rainfall than Seattle} \\
    \midrule
    {\textbf{New Answers:}\newline{}Dallas gets more rainfall than Seattle; Montreal gets more rainfall than Seattle; Chicago gets more rainfall than Seattle â€¦} \\
    \midrule
    \textbf{Verification:} https://en.wikipedia.org/wiki/List\_of\_cities\_by\_average\_precipitation \\
    \bottomrule
    \end{tabular}
    %}
  \label{enhanced}
\end{table}

\subsubsection{HotpotQA} HotpotQA \cite{yang2018hotpotqa} is also part of our experimental dataset. HotpotQA is a question-answering dataset featuring natural, multi-hop questions with strong supervision for supporting facts, facilitating more interpretable question-answering systems. The dataset covers a wide range of real-world domains and comprises a total of 113K questions, making it an excellent supplement to TruthfulQA. Many questions in HotpotQA involve comparative reasoning across two or more items, providing a robust test for the reasoning capabilities of LLMs and allowing us to observe hallucinations in questions requiring logical reasoning. However, the large number of question instances in HotpotQA and the often overly simplistic reference answers can lead to increased token consumption by MetaQA for summary expansion and answer comparison. To address this issue, we randomly selected 610 questions from HotpotQA.

\subsubsection{FreshQA} We include FreshQA \cite{vu2023freshllms}, which features questions requiring up-to-date world knowledge and those based on false premises. To ensure a fair comparison, we selected 155 questions from FreshQA dated before 2023, as LLMs may generate hallucinations when addressing newer questions due to potential gaps in training data. Overall, our experimental dataset encompasses 1582 questions across multiple datasets.

Table \ref{datasets} offers summary statistics of the final three datasets used in our study.

\begin{table}[htbp]
  \centering
  \small
  \caption{Summary statistics of the datasets used in the experiments}
    %\resizebox{1.0\columnwidth}{!}{
    \begin{tabular}{lrrp{5cm}}
    \toprule
    Dataset & Total QA Pairs & Selected Pairs & \multicolumn{1}{l}{Selected Categories} \\
    \midrule
    TruthfulQA-Enhanced & 817   & 817   & 38 categories such as health, law, finance, and politics \\
    \midrule
    HotpotQA & 112,779 & 610   & Natural, multi-hop questions involving comparative reasoning \\
    \midrule
    FreshQA & 603   & 155   & Includes one-hop and multi-hop questions that involve static, slowly evolving, and rapidly changing facts \\
    \bottomrule
    \end{tabular}%
    %}
  \label{datasets}%
\end{table}%

% \begin{table}[h]
% \centering
% \caption{Summary of datasets used in the experiments}
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{|c|c|c|p{4.5cm}|}
% \hline
% \textbf{Dataset} & \textbf{Total QA Pairs} & \textbf{Selected Pairs} & \textbf{Selected Categories} \\ \hline
% TruthfulQA-Enhanced & 817 & 817 & 38 categories such as health, law, finance, and politics \\ \hline
% HotpotQA & 112,779 & 610 & Natural, multi-hop questions involving comparative reasoning \\ \hline
% FreshQA & 603 & - & Includes one-hop and multi-hop questions that involve static, slowly evolving, and rapidly changing facts \\ \hline
% \end{tabular}%
% }
% \label{tab:datasets}
% \end{table}


% \gias{create a summary table with a summary of each LLM and their key attributes (e.g., size, context, etc.) and then refer to it from here. Also make sure that you justify your selection of each LLM, such as why you picked it over the others that you did not pick, etc.} To ensure reliable evaluation in our experiments, we assess three state-of-the-art LLMs using MetaQA. Recognizing the diverse nature of LLMs, we categorize them into two groups for in-depth analysis: the first group includes API-accessible models with closed-source architectures such as GPT-4(gpt-4o) and ChatGPT (gpt-3.5-turbo). The second group consists of open-source, deployable LLMs like Llama3-8B (Llama-3-8B-Instruct) and Mistral-7B\cite{jiang2023mistral} (Mistral-7B-Instruct-v0.3).

\subsection{Studied LLMs} 
To ensure a reliable evaluation in our experiments, we assess several LLMs that have been evaluated in LiveBench \cite{white2024livebenchchallengingcontaminationfreellm}, a benchmark used to assess LLMs across various aspects, including math, reasoning, language, instruction following, and more. Our evaluation includes very large models from the GPT family as well as open-source models from the Llama and Mistral families. We divide the selected models into two types: 

\begin{itemize}[leftmargin=10pt]
    \item \textbf{Closed-source models.} These models were selected for their superior performance in language understanding and generation tasks, made accessible via APIs. Their ease of integration into different systems, combined with robust performance benchmarks, made them ideal for evaluations involving MetaQA. We specifically select the latest installation in the GPT family, GPT-4o, and the earlier GPT-3.5-turbo model for this category.
    \item \textbf{Open-source models.} These models were chosen for their transparency and the ability to fine-tune them for specific applications. Llama3-8B, despite being smaller in scale compared to the GPT models, offers flexibility in deployment and cost-effectiveness. Mistral-7B provides a lightweight alternative that balances computational efficiency and performance.
\end{itemize}

Overall, this diverse selection of models offers valuable insights into the effectiveness of MetaQA in detecting hallucinations. The temperature of all LLMs was set to 0.1 in all experiments to reduce randomness. 
Table \ref{table:summary-of-llms} offers summary statistics of the studied LLMs.
\begin{table}[h]
\centering
\small
\caption{Overview of the studied LLMs}
\label{table:summary-of-llms}
\resizebox{\columnwidth}{!}{
\begin{tabular}{llrrp{5cm}}
\toprule
\textbf{Model}            & \textbf{Version}                & \textbf{Parameters} & \textbf{Context Size} & \multicolumn{1}{l}{\textbf{Overview}} \\ 
\midrule
GPT-4                     & gpt-4o-2024-08-06                          & Not disclosed   & 128K tokens         & High performance on API-based tasks, widely recognized for accuracy.                              \\ \midrule
GPT-3.5                   & gpt-3.5-turbo-0125                  & 175B                & 16K tokens          & Strong performance, popular for API integration and general usability.                            \\ \midrule
Llama3-8B                 & 8B-Instruct             & 8B                  & 128K tokens         & Open-source, flexible for fine-tuning, and deployable on local systems.                            \\ \midrule
Mistral-7B                & Instruct-v0.3        & 7B                  & 8,192 tokens          & Lightweight, open-source, with efficient deployment and fine-tuning.                              \\ \bottomrule
\end{tabular}
}
\end{table}


%\subsection{Experimental Environment} 
Our experiments are performed using an NVIDIA A100 40GB GPU for open-source LLMs. We use the official releases of these models from the HuggingFace repositories\footnote{\url{https://huggingface.co/models}}. For experiments involving ChatGPT, we use the OpenAI chat completion API\footnote{\url{https://api.openai.com/v1/chat/completions}}.  

% \begin{table*}[!htbp]
%     \centering
%     \caption{Performance Comparison between MetaQA and SelfCheckGPT on various datasets at threshold $theta$ = 0.7}
%     \resizebox{\textwidth}{!}{
%         \begin{tabular}{l|ccc|ccc|ccc}
%         \toprule
%         \multicolumn{1}{c|}{\multirow{2}[2]{*}{Method}} & \multicolumn{3}{c|}{TruthfulQA-Enhanced} & \multicolumn{3}{c|}{HotpotQA} & \multicolumn{3}{c}{FreshQA} \\
%                 & Precision & Recall & F1 Score & Precision & Recall & F1 Score & Precision & Recall & F1 Score \\
%         \midrule
%         \midrule
%         MetaQA(GPT-4o) & \textbf{0.882} & \textbf{0.405} & \textbf{0.556} & \textbf{0.852} & \textbf{0.535} & \textbf{0.657} & \textbf{0.667} & \textbf{0.353} & \textbf{0.462} \\
%         SelfCheckGPT(GPT-4o) & 0.500 & 0.108 & 0.178 & 0.731 & 0.442 & 0.551 & 0.667 & 0.235 & 0.348 \\
%         \midrule
%         MetaQA(GPT-3.5) & 0.590 & \textbf{0.189} & \textbf{0.286} & 0.759 & \textbf{0.573} & \textbf{0.653} & 0.837 & \textbf{0.667} & \textbf{0.700} \\
%         SelfCheckGPT(GPT-3.5) & \textbf{0.600} & 0.074 & 0.131 & \textbf{0.902} & 0.418 & 0.571 & \textbf{0.857} & 0.429 & 0.571 \\
%         \midrule
%         MetaQA(Llama-8B) & \textbf{0.854} & \textbf{0.310} & \textbf{0.455} & 0.602 & \textbf{0.411} & \textbf{0.489} & \textbf{0.938} & 0.357 & \textbf{0.517} \\
%         SelfCheckGPT(Llama3-8B) & 0.601 & 0.246 & 0.349 & \textbf{0.661} & 0.247 & 0.359 & 0.515 & \textbf{0.405} & 0.453 \\
%         \midrule
%         MetaQA(Mistral-7B) & \textbf{0.782} & \textbf{0.190} & \textbf{0.306} & \textbf{0.700} & \textbf{0.261} & \textbf{0.380} & \textbf{0.520} & \textbf{0.232} & \textbf{0.321} \\
%         SelfCheckGPT(Mistral-7B) & 0.537 & 0.112 & 0.186 & 0.676 & 0.155 & 0.253 & 0.417 & 0.089 & 0.147 \\
%         \bottomrule
%         \end{tabular}%
%     }
%     \label{tab2}%
% \end{table*}%

\subsection{Response Verification}
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/autocheck.pdf}
    \caption{Automatic Response Validation Process.}
    \label{autocheck}
\end{figure} 
In our experimental datasets, each question is accompanied by several ``reference correct answers." To determine whether an LLM's initial response is a hallucination, we need to validate the responses. While pure manual verification of each response ensures accuracy, it is resource-intensive and inefficient. Consequently, We first employ a specific algorithm to streamline the manual inspection process. The algorithm uses the LLM to decide whether manual inspection is necessary. This decision is based on comparing the original response with each reference correct answer, as detailed in Figure \ref{autocheck} below. For those who need manual checks,
the first two authors participate individually (with a disagreement rate of only 0.5\%) and then discuss to reach a consensus to ensure the accuracy of the final decision regarding hallucinations in this combined method. 

% \begin{table}[!htbp]
%   \centering
%   \caption{A Specific Example Of Automatic Response Validation.}
%   \resizebox{0.8\textwidth}{!}{
%     \begin{tabular}{l}
%     \toprule
%     \rowcolor[rgb]{ .886,  .937,  .855} \textbf{Question:} What is the most popular sport in Japan? \\
%     \midrule
%     \rowcolor[rgb]{ .886,  .937,  .855} \multicolumn{1}{p{40em}}{\textbf{Best Answer:}\newline{}1. Baseball is the most popular sport in Japan. \newline{}\textbf{Correct Answer:}\newline{}2. Baseball is the most popular sport in Japan.\newline{}3. While sumo wrestling is the national sport of Japan, baseball is actually the most popular sport.} \\
%     \midrule
%     \rowcolor[rgb]{ .886,  .937,  .855} \multicolumn{1}{p{40em}}{\textbf{Wrong Answer:}\newline{}1. Sumo wrestling is the most popular sport in Japan.\newline{}2. Soccer is the most popular sport in Japan.\newline{}3. Tennis is the most popular sport in Japan. \newline{}4. PokÃ©mon is the most popular sport in Japan.\newline{}5. While baseball is the national sport of Japan, sumo wrestling is actually the most popular sport} \\
%     \midrule
%     \rowcolor[rgb]{ .886,  .937,  .855} \textbf{Base Response:} The most popular sport in Japan is baseball. \\
%     \midrule
%     \rowcolor[rgb]{ .886,  .937,  .855} \textbf{Auto Validate Result:} Hallcination \\
%     \bottomrule
%     \end{tabular}}
%   \label{autocheckexample}
% \end{table}

\section{Experimental Results}
\subsection{Effectiveness (RQ1)}
% Due to the open-source library of SelfCheckGPT on GitHub\footnote{\url{https://github.com/potsawee/selfcheckgpt}} allowing APIs that support both OpenAI clients and Hugging Face models, we modified parts of the code to suit our experiments and added additional components to their pipeline to track accurate token usage. \afif{It's not right. SelfcheckGPT allows APIs that support both OpenAI clients and huggingface models. We only did some modifications to their pipeline to find accurate token usage}\borui{Done} 

% Overall statistics from our experiments across all datasets show that, at a threshold of 0.5, MetaQA detects 51.5\% of all hallucinations with an accuracy of 69.4\%, whereas SelfCheckGPT detects only 33.0\% of all hallucinations with an accuracy of 65.3\%. For thresholds \(\theta \in [0.2, 0.7]\), with an interval of 0.05, MetaQA demonstrates an overall performance improvement ranging from 16.41\% to 80.04\% compared to SelfCheckGPT.

% \begin{wrapfigure}{r}{0.45\linewidth}
%     \centering
%     \includegraphics[width=.75\linewidth]{figures/haluscore.pdf}
%     \caption{\textbf{RQ1: } Hallucination Scores in MetaQA and SelfCheckGPT, where 0.0 indicates factual responses and 1.0 represents hallucinations.}
%     \label{haluscore}
% \end{wrapfigure}
% To answer RQ1, we first...then ....
% \subsubsection{Effectiveness Across Different Datasets}
% Given the varying focuses of the datasets, we conducted experiments to evaluate MetaQA's performance across different datasets. Specifically,  As shown in Fig \ref{overallhalu}, LLMs generally perform better on timeliness tests but are more prone to hallucinations in environments requiring extensive logical reasoning. 


Table \ref{tab2} shows the results of MetaQA and SelfCheckGPT across different datasets and LLMs with the default threshold $\theta$ = 0.5. 
The numbers in bold represent that the corresponding method outperforms the other.
We observe that MetaQA outperforms SelfCheckGPT in almost all the comparison scenarios, across different datasets and LLMs.
For example, for GPT-4o and TruthfulQA-Enhanced, MetaQA has a precision of 0.739, 0.459, and 0.567, while
for SelfCheckGPT, the results are only 0.615, 0.216, and 0.320, respectively.

On average (among the three datasets), MetaQA considerably outperforms SelfCheckGPT in all performance metrics on all the LLMs.
For instance, with Mistral-7B, MetaQA achieves an average F1-score of 0.435, compared to SelfCheckGPTâ€™s F1-score of 0.205, representing an improvement of 112.2\%. Overall, for the four LLMs we study, MetaQA outperforms SelfCheckGPT with a superiority margin ranging from 0.041 to 0.113 in terms of precision, 0.143 to 0.430 in terms of recall, and 0.154 to 0.368 in terms of F1-score.
\begin{table*}[!htbp]
    \centering
    \caption{\textbf{RQ1: }Comparison between MetaQA and SelfCheckGPT on various datasets and LLMs}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|ccc|ccc|ccc|ccc}
    \toprule
    \multicolumn{1}{c|}{\multirow{2}[2]{*}{Method}} & \multicolumn{3}{c|}{TruthfulQA-Enhanced} & \multicolumn{3}{c|}{HotpotQA} & \multicolumn{3}{c|}{FreshQA} & \multicolumn{3}{c}{Average} \\
          & Precision & Recall & F1 Score & Precision & Recall & F1 Score & Precision & Recall & F1 Score & Precision & Recall & F1 Score \\
    \midrule
    \midrule
    MetaQA(GPT-4o) & \textbf{0.739} & \textbf{0.459} & \textbf{0.567} & \textbf{0.758} & \textbf{0.581} & \textbf{0.658} & \textbf{0.600} & \textbf{0.471} & \textbf{0.527} & \textbf{0.699} & \textbf{0.504} & \textbf{0.584} \\
    SelfCheckGPT(GPT-4o) & 0.615 & 0.216 & 0.320 & 0.690 & 0.465 & 0.556 & 0.571 & 0.235 & 0.333 & 0.625 & 0.306 & 0.403 \\
    \midrule
    MetaQA(GPT-3.5) & \textbf{0.567} & \textbf{0.545} & \textbf{0.556} & 0.708 & \textbf{0.727} & \textbf{0.717} & \textbf{0.569} & \textbf{0.786} & \textbf{0.660} & 0.615 & \textbf{0.686} & \textbf{0.644} \\
    SelfCheckGPT(GPT-3.5) & 0.563 & 0.111 & 0.185 & \textbf{0.844} & 0.591 & 0.695 & 0.556 & 0.429 & 0.484 & \textbf{0.654} & 0.377 & 0.455 \\
    \midrule
    MetaQA(Llama-8B) & \textbf{0.811} & \textbf{0.513} & \textbf{0.628} & \textbf{0.712} & \textbf{0.259} & \textbf{0.567} & \textbf{0.629} & \textbf{0.524} & \textbf{0.571} & \textbf{0.717} & \textbf{0.432} & \textbf{0.589} \\
    SelfCheckGPT(Llama3-8B) & 0.601 & 0.301 & 0.401 & 0.663 & 0.373 & 0.478 & 0.487 & 0.452 & 0.469 & 0.584 & 0.376 & 0.449 \\
    \midrule
    MetaQA(Mistral-7B) & \textbf{0.652} & \textbf{0.321} & \textbf{0.430} & \textbf{0.735} & \textbf{0.379} & \textbf{0.500} & \textbf{0.457} & \textbf{0.286} & \textbf{0.352} & \textbf{0.615} & \textbf{0.328} & \textbf{0.427} \\
    SelfCheckGPT(Mistral-7B) & 0.531 & 0.134 & 0.214 & 0.700 & 0.217 & 0.332 & 0.333 & 0.089 & 0.141 & 0.521 & 0.147 & 0.229 \\
    \bottomrule
    \end{tabular}}
    \label{tab2}
\end{table*}

We have applied the Wilcoxon Signed-Rank Test to the Precision, Recall, and F1 scores from Table \ref{tab2} across all datasets. As our data does not assume a normal distribution, we used a non-parametric Wilcoxon test. The results demonstrate statistically significant differences, with p-values of 0.0092, 0.00015, and 0.0000305 for Precision, Recall, and F1 Score, respectively.

% It is evident that MetaQA consistently outperforms our baseline across most specific testing conditions.

% \subsubsection{Proof of the effectiveness of hallucination scores}
% Furthermore, we compare the distribution of hallucination scores across different score ranges for MetaQA and SelfCheckGPT to assess the effectiveness of the hallucination scores and demonstrate that MetaQA exhibits superior performance in generating hallucination scores for samples containing hallucinations compared to SelfCheckGPT. As illustrated in Figure \ref{haluscore}, under the condition of a threshold $\theta$ $\leq$ 0.7, the hallucination scores generated by MetaQA are significantly more effective than those produced by SelfCheckGPT. It is evident that in the evaluation of hallucination scores, SelfCheckGPT often assigns a Hallucination score of 0 for many hallucination examples, a situation that does not occur with MetaQA. 



% However, hallucinations can also occur during MetaQA's verification process, which sometimes results in MetaQA's precision rate being comparable to or even slightly lower than that of SelfCheckGPT. Further discussions on additional threats to validity will be addressed in Section 6.


\mybox{\textbf{Answer to RQ1:}
     MetaQA consistently outperforms SelfCheckGPT in terms of precision, recall, and f1 score.
     In particular, for the four LLMs we study, MetaQA outperforms SelfCheckGPT with a superiority margin ranging from 0.041 to 0.113 in terms of precision, 0.143 to 0.430 in terms of recall, and 0.154 to 0.368 in terms of F1-score.}

\subsection{Generalization (RQ2)}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=1\linewidth]{figures/categories.pdf}
    \caption{\textbf{RQ2: }Overall Hallucination Rate of Specific Domains and Detected Proportion Comparison on MetaQA and SelfCheckGPT of Specific Domains at $\theta$ = 0.5.} 
    %\afif{Can you make sure the pie chart is correct? This representation says that most of the hallucinations were not detected while our results had an overall good precision. Also, please indicate the LLM and dataset used for this experiment.}\borui{New figure is here now}
    \label{category}
\end{figure}

We compare the hallucination detection effectiveness of MetaQA and SelfCheckGPT across the six most frequently occurring categories in the TruthfulQA classification. Since HotpotQA and FreshQA do not categorize their questions, we rely solely on TruthfulQA for more precise results. The x-axis of Figure \ref{category} shows the six categories, including Misconceptions, Law, Indexical Error, Health, Sociology, and Confusion. The numbers in each bar show the ratio of hallucinations (light grey) and facts (dark grey) in the responses for each category for all 4 LLMs. 

Figure \ref{category} shows the hallucination detection results of MetaQA (black) and SelfCheckGPT (dark grey).
Overall, we observe that MetaQA outperforms SelfCheckGPT in all seven categories.
MetaQA performs extremely well in Confusion and Law. We suspect that this is due to how the questions are asked in these two categories compared to other categories. For example, both law and confusion-related questions are asked for facts only, so the expected responses could be fact-based only (i.e., less winding than other opinion-type responses). Given that MetaQA is designed to handle fact-conflicting hallucinations, such \textit{fact-inducing} questions were better addressed in MetaQA approach for hallucination detection.
% hallucination responses in seven domains corresponding to the top seven categories most frequently occurring in the TruthfulQA classification. Fig \ref{category} presents bar charts illustrating the confusion matrices for the hallucination response rates in these specific knowledge fields based on the testing results. Here, hallucination rate is defined by following equation,
% \begin{equation}
%     H = \frac{N_{\text{Hallucination}}}{N_{\text{Hallucination}} + N_{\text{Factual}}}
% \end{equation}
% Additionally, based on the results from RQ1, MetaQA demonstrates more accurate performance when the threshold $\theta$ $\leq$ 0.7, so another pie chart represents the proportion of hallucinations detected by MetaQA and SelfCheckGPT at a threshold $\theta$ of 0.5. Since MetaQA employs metamorphic relations (MR) to generate and validate mutations, it provides a more comprehensive approach compared to SelfCheckGPT, which generates samples by querying the model multiple times. This is particularly evident with misconception-type questions, where MetaQA demonstrates a clear advantage over SelfCheckGPT.
\mybox{\textbf{Answer to RQ2:}
    MetaQA outperforms SelfCheckGPT in all six categories in the TruthfulQA dataset.
MetaQA performs extremely well in Confusion and Law. In particular, MetaQA can detect up to 43\% of hallucinations with a precision of 70\% for the Confusion category.}

\subsection{Stability (RQ3)}
\subsubsection{Stability Over Multiple Runs}
Previous results demonstrate that MetaQA is an effective approach for detecting hallucinations in the question-answering process with LLMs. An additional consideration is the stability of MetaQA as a hallucination detection method. Given the randomness of hallucination generation in LLMs~\cite{Ouyang2025} and the reliance of MetaQA's workflow on the LLM itself, it is crucial to assess whether MetaQA is affected by these uncertainties. To address this concern, we repeated MetaQA runs 3 times on the same dataset using both the open-source model Llama3 and the closed-source model GPT-3.5.
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/deviation.pdf}
    \caption{\textbf{RQ3: }The deviation curves on GPT-3.5 and Llama3 with MetaQA across different thresholds.}
    \label{deviation}
\end{figure}
As shown in Figure \ref{deviation}, MetaQA exhibits robust stability at thresholds $\theta$ $\leq$ 0.7. However, its stability of precision decreases at thresholds $\theta$ $>$ 0.7.
This is because LLMs may generate incorrect mutants during the working steps of MetaQA, which could lead to a reduction in the hallucination score when the threshold is high.

\subsubsection{Impact Of Temperature}
Temperature plays a crucial role in the functioning of LLMs by influencing the randomness and creativity of the generated text \cite{yucan}. In LLMs, temperature serves as a parameter that controls output diversity by manipulating the softmax function, which determines the probabilities of the next word in a sequence. When the temperature is low (e.g., close to zero), the generated text tends to be more focused and deterministic, leading to more confident predictions and less variation in the output. This characteristic can be beneficial when a conservative and predictable response is preferred. For MetaQA, accurate mutation generation and verification results are essential, making it imperative to investigate the impact of temperature on its performance. In our experiments, we assessed the stability of MetaQA across a set of temperature values, denoted as \( T = \{0.1, 0.3, 0.5, 0.7\} \), which were selected as the experimental parameters. We randomly sampled 10\% of the total dataset for the temperature experiments conducted on GPT-3.5. As illustrated in Figure \ref{temperature}, this finding aligns with our anticipated results, demonstrating that MetaQA's performance decreases with increasing temperatures and performs better at lower temperatures.
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/temperature.pdf}
    \caption{\textbf{RQ3: }Stability performance of MetaQA at different temperatures on GPT-3.5.}
    \label{temperature}
\end{figure}
\mybox{\textbf{Answer to RQ3:}
    Multiple rounds of experiments indicate that MetaQA maintains considerable stability. Furthermore, MetaQA demonstrates higher performance and stability at lower temperatures compared to higher temperatures. }

\subsection{Sensitivity to Mutants (RQ4)}
Although increasing the number of synonym and antonym mutations can enhance the stability of mutation quality and is expected to improve MetaQA's performance, it also incurs higher computational costs. Therefore, we investigate the performance variations with different numbers of samples. As illustrated in Figure \ref{nummuts}, where each line indicates MetaQA with a threshold $\theta$ = 0.5, 0.55, 0.6, where each line indicates MetaQA with a threshold $\theta$ = 0.5, 0.55, 0.6, which equal or close to our default threshold 0.5 it is evident that the performance of MetaQA increases as more mutations are used, with reduced fluctuations and overall performance showing diminishing gains as the number of samples grows.
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/nummuts.pdf}
    \caption{\textbf{RQ4: }The performance of MetaQA methods on overall datasets vs the number of mutations (started from 2) across multiple thresholds on GPT-3.5.}
    \label{nummuts}
\end{figure}

\mybox{\textbf{Answer to RQ4:} The experimental results demonstrate that MetaQA's performance improves as the number of mutations increases. However, to balance performance and computational cost, the number of 10 mutations is considered an optimal choice, plus we will discuss computational token costs in section 6.}

\subsection{Sensitivity to Threshold (RQ5)}
To answer RQ5, we demonstrate the results of 
MetaQA and SelfCheckGPT across different thresholds.
Fig \ref{multimodel} shows the results.
We can observe that MetaQA outperforms SelfCheckGPT with most thresholds.
In particular, for recall,
MetaQA has superiority over SelfCheckGPT across all the thresholds. 
Specifically, through the thresholds \(\theta \in [0.2, 0.7]\), with an interval of 0.05, at total 13 sample points, MetaQA's F1 score demonstrates an overall improvement of 16.41\% to 80.04\% compared to SelfCheckGPT.

\begin{figure*}[!htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/multimodel.pdf}
    \caption{\textbf{RQ5: }Precision, Recall, and F1 Score Curves of MetaQA and SelfCheckGPT in hallucination detection on overall datasets across all thresholds and across multiple LLMs.}
    \label{multimodel}
\end{figure*}
% As shown in Fig \ref{multimodel}, MetaQA, which employs metamorphic relations, consistently achieves higher performance across various models compared to SelfCheckGPT under all thresholds. These results underscore the effectiveness of MetaQA in enhancing hallucination detection in LLMs.


Table \ref{threshold} demonstrates how MetaQA outperforms SelfCheckGPT, showcasing a specific case where SelfCheckGPT struggles with fact-conflicting hallucinations. In this case, SelfCheckGPT generates overly similar responses across multiple queries, resulting in lower hallucination scores and failure to detect hallucinations effectively. In contrast, MetaQA leverages two types of Metamorphic Relations (MR) to generate diverse mutations and perform fact verification, thereby improving hallucination detection accuracy. Furthermore, this sample illustrates how the threshold operates in the experiments. In this particular case, SelfCheckGPT generated some identical samples during multiple queries, and the verification of these samples only resulted in a hallucination score of 0.2. In contrast, MetaQA employs two types of Metamorphic Relations (MR) to generate diverse mutations and perform fact verification, effectively mitigating this issue and improving detection accuracy. Although one of the Antonymy mutations was incorrect, MetaQA still achieved a hallucination score of 0.75 due to the verification of the diverse mutations it generated. The threshold will work in the following way in the experiment, at threshold $\theta$ = 0.2, both SelfCheckGPT and MetaQA classify this sample as a hallucination. at threshold $\theta$ = 0.5, only MetaQA detects a hallucination, while at threshold $\theta$ = 0.8, neither method detects it.

\begin{table}[!htbp]
  \centering
  \caption{\textbf{RQ5:} Example of how MetaQA outperforms SelfCheckGPT and how the threshold works in practice}
  \resizebox{1\textwidth}{!}{
    \begin{tabular}{p{30em}l}
    \toprule
    \multicolumn{2}{l}{Question: What percentage of the brain does a human typically use?} \\
    \midrule
    \multicolumn{2}{l}{Correct Answers: The majority of a human's brain is almost always active; A human typically uses virtually all of their brain â€¦} \\
    \midrule
    \multicolumn{2}{l}{Base Response: Humans typically use about 10\% of their brains.} \\
    \midrule
    \multicolumn{2}{p{60em}}{SelfCheckGPT Samples: \newline{}On average, humans typically use around 10\% of their brain capacity.\newline{}A human typically uses about 10\% of their brain...(more samples are hidden)} \\
    \midrule
    \multicolumn{1}{l}{SelfCheckGPT Score: 0.2} &  \\
    \midrule
    \multicolumn{1}{p{30em}|}{MetaQA Synonymy Mutations:\newline{}On average, humans use approximately 10\% of their brain capacity.\newline{}The common estimate suggests that humans make use of around 10\% of their brains.\newline{}...\newline{}Synonymy Mutations Verify Results:\newline{}Non factual; Non factual; Non factual; Factual; Non factual} & \multicolumn{1}{p{30em}}{MetaQA Antonymy Mutations:\newline{}Humans typically use more than 10\% of their brains.\newline{}90\% of their brains are typically unused by humans. (Wrong Mutations: Double negation)\newline{}It is not common for humans to only use about 10\% of their brains.\newline{}...\newline{}Antonymy Mutations Verify Results:\newline{}Factual; Non factual; Factual; Factual; Not Sure} \\
    \midrule
    \multicolumn{2}{l}{MetaQA Hallucination Score: 0.75} \\
    \midrule
    \multicolumn{2}{p{60em}}{Hallucination detection result in different threshold:\newline{}$\theta$ = 0.2: SelfcheckGPT: Hallucination, MetaQA: Hallucination\newline{}$\theta$ = 0.5: SelfcheckGPT: No Hallucination, MetaQA: Hallucination\newline{}$\theta$ = 0.8: SelfcheckGPT: No Hallucination, MetaQA: No Hallucination} \\
    \bottomrule
    \end{tabular}}
  \label{threshold}
\end{table}


\mybox{\textbf{Answer to RQ5:} MetaQA outperforms SelfCheckGPT with most thresholds. 
    Through the thresholds \(\theta \in [0.2, 0.7]\), MetaQA demonstrates an overall performance improvement of 16.41\% to 80.04\% compared to SelfCheckGPT.
}

\section{Discussion}

% \subsection{Broader Implications of Hallucination Detection in LLMs}
% Hallucinations arise naturally from the diverse, often inconsistent, human-created data on which LLMs are trained. This inherent characteristic highlights why detecting hallucinations is essential. An analogy can be drawn to software testing, where detecting bugs is not surprising but remains critical. In software testing, identifying and analyzing bugs informs debugging, improves system reliability, and builds user trust. Similarly, hallucination detection in LLMs allows us to define trust boundaries, improve LLM design, and tailor applications for critical domains such as healthcare, legal, and education, where accuracy is paramount \cite{huang2023survey, xu2024hallucination}. While neither software bugs nor hallucinations can be entirely eliminated, detecting and managing them enables us to mitigate risks, enhance utility, and ensure systems operate responsibly.

\begin{wraptable}{r}{6.5cm}
% \begin{table}[htbp]
  \centering
\caption{An average token-cost per QA-process comparison between MetaQA and SelfCheckGPT on multi models}
 \resizebox{0.45\textwidth}{!}{   \begin{tabular}{cc|c|c|c}
    \toprule
    \multicolumn{2}{c|}{} & Base  & MetaQA & SelfCheckGPT \\
    \midrule
    \multirow{2}[2]{*}{GPT-3.5} & Avg Token Cost & 101.37 & \textcolor[rgb]{ 0,  0,  1}{\textbf{1604.38}} & \textcolor[rgb]{ 1,  0,  0}{1812.98} \\
          & Growth rate & -     & \textcolor[rgb]{ 0,  0,  1}{\textbf{1582.70\%}} & \textcolor[rgb]{ 1,  0,  0}{1788.48\%} \\
    \midrule
    \multirow{2}[2]{*}{GPT-4o} & Avg Token Cost & 103.85 & \textcolor[rgb]{ 0,  0,  1}{\textbf{1585.9}} & \textcolor[rgb]{ 1,  0,  0}{1887.67} \\
          & Growth rate & -     & \textcolor[rgb]{ 0,  0,  1}{\textbf{1527.11\%}} & \textcolor[rgb]{ 1,  0,  0}{1817.69\%} \\
    \midrule
    \multirow{2}[2]{*}{Llama3} & Avg Token Cost & 128.12 & \textcolor[rgb]{ 0,  0,  1}{\textbf{1820.96}} & \textcolor[rgb]{ 1,  0,  0}{2490.59} \\
          & Growth rate & -     & \textcolor[rgb]{ 0,  0,  1}{\textbf{1421.29\%}} & \textcolor[rgb]{ 1,  0,  0}{1943.95\%} \\
    \midrule
    \multirow{2}[2]{*}{Mistral} & Avg Token Cost & 111.97 & \textcolor[rgb]{ 0,  0,  1}{\textbf{1749.71}} & \textcolor[rgb]{ 1,  0,  0}{2499.29} \\
          & Growth rate & -     & \textcolor[rgb]{ 0,  0,  1}{\textbf{1562.66\%}} & \textcolor[rgb]{ 1,  0,  0}{2232.11\%} \\
    \bottomrule
    \end{tabular}}%}
  \label{tokencost}%
\end{wraptable}%
\subsection{Token Overhead Analysis} 
Employing token-based models may incur substantial costs when processing large datasets or executing multiple iterations. Such expenses can restrict the feasibility of applying our method in resource-constrained environments. Based on Table \ref{tokencost}, which compares the average token cost between MetaQA and SelfCheckGPT using GPT-3.5, several insights can be drawn. These results indicate that while both methods involve a substantial increase in token cost relative to the base, MetaQA is slightly more cost-efficient than SelfCheckGPT. This efficiency could be an important consideration for implementing these methods in practical applications, where managing cost is crucial.


% \subsection{Hallucination Mitigation}
% MetaQA does not currently mitigate hallucination issues in LLMs; however, our research explored a novel prompt-based approach to tackle this challenge. Specifically, for each response generated by the LLM, we employed its antonym mutation as a prompt, compelling the model to iteratively refine its original response through multiple iterations. This technique enables the model to confront its initial output with a contrasting perspective, prompting it to reconsider and potentially correct any inaccuracies. While preliminary results suggest that this method demonstrates some effectiveness in mitigating self-hallucinations, it is not without challenges. Existing prompt-based hallucination mitigation methods include those by \cite{pan2023automatically, dhuliawala2023chain}. Consequently, further exploration and experimentation are necessary to optimize this approach and assess its long-term efficacy. Nevertheless, utilizing mutations as a strategy for self-hallucination mitigation presents a promising and innovative solution to the pervasive issue of fact-conflicting hallucinations in LLMs.

% \subsection{Takeaway Messages}
% \afif{Do we want to keep this passage? One reviewer did not find it relevant to the paper.}
% \textbf{Deeper Understanding of LLM Hallucination:} A critical insight from this research is the importance of utilizing white-box methods to investigate the underlying causes of hallucinations in LLMs. By thoroughly understanding how and why these models generate hallucinated content, we can devise effective strategies to improve their reliability and accuracy.

% \noindent\textbf{Enhancing LLM Reasoning Abilities in training:} Prioritizing the integrity of LLMs during training is essential, particularly in cultivating stronger critical thinking and logical reasoning skills. This emphasis on improving logical reasoning capabilities presents a promising pathway for mitigating hallucination issues and enhancing the overall trustworthiness of LLM outputs.

% % \subsection{Contribution of Metamorphic Relations}
% The use of both synonym-based and antonym-based Metamorphic Relations significantly enhances hallucination detection compared to scenarios where no MRs or only one type of MR is employed. Our experiments demonstrated that MetaQA more effectively detected hallucinated responses compared to not using MRs, with this improvement stabilizing as the number of generated mutants increased. When both synonym-based and antonym-based MRs were used together, the detection process became more comprehensive. Overall, while each MR type can detect hallucinations individually, combining both offers a more robust solution, leading to more accurate detection of hallucinated responses.

\subsection{Importance of Hallucination Detection}

Hallucinations in LLMs arise from the diverse and sometimes inconsistent human-created data on which they are trained, as well as the probabilistic nature of their text generation. Left unchecked, these hallucinations can lead to the spread of misinformation, mislead users, and undermine the credibility of AI-powered applications. This is particularly concerning in high-stakes domains such as healthcare, law, and finance, where inaccurate outputs can have serious real-world consequences. 

Hallucination detection is similar to software testing and machine learning testing~\cite{9000651}, where bug detection, though expected, is critical for improving system reliability and building user trust. Similarly, detecting hallucinations in LLMs helps establish trust boundaries, refine the model design, and ensure accuracy in applications in high-stakes fields\cite{huang2023survey, xu2024hallucination}. While hallucinations, like bugs, cannot be fully eliminated, their detection and management allow us to mitigate risks and enhance the reliability of LLMs.
Given their impact on trust and safety, hallucination detection deserves dedicated effort and innovation. We call on the community to contribute to advancing robust hallucination detection methods, improving mitigation strategies, to ensuring the responsible deployment of LLM systems.
\begin{wrapfigure}[9]{r}{0.45\linewidth}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/doublenega.pdf}
    \caption{A double negation in antonym mutation.}
    \label{doublenega}
\end{wrapfigure}

\subsection{Threats to Validity}\label{sec:threats}
\subsubsection{Mutation Generation Accuracy}

The effectiveness of our approach is heavily dependent on the accuracy of the generated mutations. If these mutations do not accurately represent plausible variations of the original responses, subsequent hallucination detection may yield unreliable results. Furthermore, LLMs may introduce minor hallucinations during this process, with double negations in antonym mutation generation being a notable example \cite{varshney2024investigating, asher2024strong}. Indeed, we sometimes encountered a double negative in the antonym mutation-generating process (an example is detailed in Fig \ref{doublenega}).  This could result in false positives or negatives, ultimately affecting the validity of our findings. Conversely, utilizing more accurate mutation generation methods could significantly enhance the performance metrics of MetaQA.

\subsubsection{Generalizability of Results} The study assesses MetaQA using datasets (TruthfulQA, HotpotQA, FreshQA) and LLMs (GPT-4, GPT-3.5, Llama3, Mistral). The effectiveness of MetaQA may vary with different datasets or LLMs not tested in this study. Although we aimed to include diverse models and datasets, the results may not fully generalize to other contexts.

\section{Related Work}
Detecting hallucination in LLMs is imperative for assuring the reliability and trustworthiness of the generated content. A direct strategy involves comparing the model-generated output against reliable knowledge sources \cite{guo2022survey, augenstein2019multifc, Hanselowski2019ARA, Atanasova2020GeneratingFC, 202307.1723, huo2023retrieving}. Such methods, however, require access to external databases and can have considerable inference costs.

To address this issue in zero-resource settings, several methods have been devised that eliminate the need for retrieval. The fundamental premise behind these strategies is that LLM hallucinations are inherently tied to the model's uncertainty. The internal states of LLMs can serve as informative indicators of their uncertainty, often manifested through metrics like token probability or entropy \cite{varshney2023stitch, yao2023llm}. When working with open-source LLMs, we can assess the likelihood of hallucination by examining token-level information, such as confidence levels.

However, uncertainty measures require access to token-level probability distributions, which may not be available for models that only provide API access to users, such as ChatGPT \cite{chatgptintro}. Given this constraint, drawing inspiration from legal cross-examination practices, the LMvLM approach was introduced by Cohen et al.~\cite{cohen2023lm}. This strategy employs an `examiner' LM to question an `examinee' LM, aiming to unveil inconsistencies in claims during multi-turn interactions.

Beyond adopting a multi-agent perspective by incorporating additional LLMs, assessing uncertainty from the self-consistency of a single LLM is often more practical. Additionally, some research has demonstrated the feasibility of this approach. For instance, \cite{agrawal2023language, xiong2023can, kadavath2022language} detect hallucinations through natural language prompts. Moreover, the method proposed by \cite{li2024halluvault} uses logic programming techniques, which are similar to metamorphic relations. However, their approach involves detecting hallucinations under the assumption that the facts are known in advance.



Hallucination evaluation datasets and benchmarks are designed to assess the propensity of LLMs to produce hallucinations, focusing on identifying factual inaccuracies and measuring deviations from the original context. These benchmarks primarily evaluate the factuality of LLM-generated content, often using a question-answering format to emphasize response accuracy. Key benchmark datasets include: TruthfulQA evaluates whether language models generate truthful answers using an adversarial approach to uncover misleading responses from training data. HaluEval \cite{li2023halueval}is a large-scale hallucination evaluation benchmark for LLMs, featuring a comprehensive collection of generated and human-annotated hallucinated samples. It samples 10K instances from the training sets of HotpotQA, OpenDialKG \cite{Moon2019OpenDialKGEC}, and CNN/DailyMail \cite{see2017get}, targeting question-answering and text summarization tasks. FreshQA addresses hallucinations arising from outdated knowledge, evaluating factuality with 600 hand-crafted questions. It assesses LLMs' ability to handle fast-changing knowledge and identify questions with false premises. REALTIMEQA \cite{kasai2024realtime} emphasizes validating LLMs' factuality in relation to current world knowledge. It provides real-time open-domain multiple-choice questions from recent news articles across various topics and offers evaluations using accuracy, exact matching, and token-based F1 metrics.


\section{Conclusions}
We presented MetaQA, an MR-based technique to detect fact-conflicting hallucinations in LLMs. By leveraging self-check mechanisms with metamorphic relations, MetaQA provides a zero-resource, robust and reliable way to assess the factual accuracy of LLM-generated content without relying on external databases or agents. Evaluation across three widely used datasets from hallucination research shows that MetaQA outperforms the baseline method, SelfCheckGPT, in terms of precision, recall, and overall performance across various thresholds and datasets.
By enhancing the ability to detect hallucinations, MetaQA contributes to improving the reliability and trustworthiness of LLM outputs. Future work may explore the integration of MetaQA with real-time applications and further refine the mutation generation process to enhance detection accuracy. 
%By addressing the critical issue of hallucinations in LLMs, MetaQA paves the way for more reliable and trustworthy AI systems, fostering greater user confidence and expanding the potential applications of these advanced language models.

% \section*{Data Availability}
% \noindent\textbf{Replication Package} contains all data, code, and our full results: \url{https://anonymous.4open.science/r/LLMhalu-70EE/}
\section*{Data Availability}
\noindent\textbf{Code Repository} containing implementation code and experimental scripts is publicly available at: \url{https://github.com/zbybr/LLMhalu/tree/MetaQA-Open-Base}

\section*{Acknowledgment}
This project is supported by an NSERC International catalyst grant with Gias Uddin as the PI and Jie Zhang as the international collaborator.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}
\end{document}
\endinput
%%
%% End of file `sample-manuscript.tex'.