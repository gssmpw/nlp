\section{Related Work}
Detecting hallucination in LLMs is imperative for assuring the reliability and trustworthiness of the generated content. A direct strategy involves comparing the model-generated output against reliable knowledge sources ____. Such methods, however, require access to external databases and can have considerable inference costs.

To address this issue in zero-resource settings, several methods have been devised that eliminate the need for retrieval. The fundamental premise behind these strategies is that LLM hallucinations are inherently tied to the model's uncertainty. The internal states of LLMs can serve as informative indicators of their uncertainty, often manifested through metrics like token probability or entropy ____. When working with open-source LLMs, we can assess the likelihood of hallucination by examining token-level information, such as confidence levels.

However, uncertainty measures require access to token-level probability distributions, which may not be available for models that only provide API access to users, such as ChatGPT ____. Given this constraint, drawing inspiration from legal cross-examination practices, the LMvLM approach was introduced by Cohen et al.____. This strategy employs an `examiner' LM to question an `examinee' LM, aiming to unveil inconsistencies in claims during multi-turn interactions.

Beyond adopting a multi-agent perspective by incorporating additional LLMs, assessing uncertainty from the self-consistency of a single LLM is often more practical. Additionally, some research has demonstrated the feasibility of this approach. For instance, ____ detect hallucinations through natural language prompts. Moreover, the method proposed by ____ uses logic programming techniques, which are similar to metamorphic relations. However, their approach involves detecting hallucinations under the assumption that the facts are known in advance.



Hallucination evaluation datasets and benchmarks are designed to assess the propensity of LLMs to produce hallucinations, focusing on identifying factual inaccuracies and measuring deviations from the original context. These benchmarks primarily evaluate the factuality of LLM-generated content, often using a question-answering format to emphasize response accuracy. Key benchmark datasets include: TruthfulQA evaluates whether language models generate truthful answers using an adversarial approach to uncover misleading responses from training data. HaluEval ____is a large-scale hallucination evaluation benchmark for LLMs, featuring a comprehensive collection of generated and human-annotated hallucinated samples. It samples 10K instances from the training sets of HotpotQA, OpenDialKG ____, and CNN/DailyMail ____, targeting question-answering and text summarization tasks. FreshQA addresses hallucinations arising from outdated knowledge, evaluating factuality with 600 hand-crafted questions. It assesses LLMs' ability to handle fast-changing knowledge and identify questions with false premises. REALTIMEQA ____ emphasizes validating LLMs' factuality in relation to current world knowledge. It provides real-time open-domain multiple-choice questions from recent news articles across various topics and offers evaluations using accuracy, exact matching, and token-based F1 metrics.