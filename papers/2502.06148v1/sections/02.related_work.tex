\section{RELATED WORK}

\subsection{Retrieval-Augmented Generation}
Retrieval-Augmented Generation (RAG) \cite{Lewis2020RAG,Guu2020REALM} has been widely used for improving the performance of LLMs across various tasks by incorporating an Information Retriever (IR) module to leverage external knowledge. 
Most RAG systems~\cite{Lewis2020RAG,ram2023context,Izacard2023Atlas} integrate retrieved knowledge (e.g., passages) directly into the input, where the LLMs generate answers based on the external information obtained with the IR module.
There are also some methods utilizing Chain of Thought (CoT) \cite{wei2022cot, trivedi2023interleaving} or task decomposition \cite{Xu2024Search, wang2024selfdc, kim2024sure} to integrate external knowledge in intermediate reasoning steps or sub-tasks. 
Though effective, such indiscriminate use of external knowledge may introduce noise, degrading the quality of generated responses.
To address this problem, conditional use of external knowledge in RAG has been investigated.
Some works decide whether to retrieve and utilize external knowledge based on query characteristics, such as assessing query complexity through entity frequency \cite{mallen2023trust}, searching for similar questions \cite{wang2023skr}, or collecting question sets for training \cite{jeong2024adaptive}.
Some other works determine whether to integrate external knowledge according to the next token generation probability by LLMs, 
such as Self-RAG~\cite{asai2024selfrag}, FLARE\cite{jiang2023active}, DRAGIN~\cite{su2024dragin}, and Self-DC~\cite{wang2024selfdc}. 
In addition to these adaptive retrieval approaches, relevance-based methods~\cite{zhang2023merging,Xu2024Search,liu2024raisf} employ a relevance verification module to filter retrieved passages. 
For example, RA-ISF~\cite{liu2024raisf} assesses the relevance of retrieved passages by training a small LLM.
However, adaptive retrieval methods often rely solely on the input query or generated tokens, which limits their effectiveness as they may only acquire incomplete information. 
In comparison, relevance-based methods heavily rely on an additional verification module, leading to increased complexity of RAG systems and high sensitivity of the final response's quality to its verification accuracy. 
In this work, we propose a novel approach that leverages the LLM itself to holistically evaluate and reconcile responses from only its internal parametric knowledge and also from externally retrieved information, aiming to deliver more accurate responses.


\subsection{Preference Alignment for LLMs}
Preference alignment has emerged as an effective approach for improving the reliability of LLMs~\cite{Ouyang2022Training} by enabling them to evolve from their generated responses and environmental feedback.
Among existing preference alignment techniques, Reinforcement Learning from Human Feedback (RLHF) leverages human-provided feedback to train reward models, ensuring that LLMs produce responses aligned well with human preferences~\cite{Christiano2017RLHF,ziegler2019finetuning}.
RLHF has been shown to improve both the performance and the user-friendliness of LLMs in various Natural Language Processing (NLP) tasks, including summarization \cite{Stiennon2020summarize}, question answering \cite{Nakano2021WebGPT}, and instruction following \cite{Ouyang2022Training}.
However, RLHF requires extensive human annotation to train the reward model and involves a complex three-stage process, resulting in limited scalability and high training complexity.
To improve the scalability in preference alignment, Reinforcement Learning from AI Feedback (RLAIF)~\cite{Bai2022ConstitutionalAH,lee2024rlaif} utilizes the feedback from the LLM itself to train a reward model to optimize LLM performance through reinforcement learning. 
Direct Preference Optimization (DPO) \cite{Rafailov2023DPO} defines preference loss directly via a change of variables, which treats the LLM itself as its reward model.
By eliminating the need for an additional reward model, DPO substantially reduces the complexity involved in preference alignment training. 
In RAG systems, some studies utilize the signals generated by LLMs to optimize the retriever~\cite{Bonifacio2022InPars,shi2024replug} to retrieve LLM-preferred data, while other works align LLMs with specific domain knowledge and specific tasks through reinforcement learning~\cite{zhang2024knowledgeable,Yang2024IMRAG,Salemi2024Optimization,dong2024understandllmneedsdual,song2024measuring}.
In this work, we generate a preference dataset automatically and utilize it to strengthen LLMs' answer selection and generation capabilities in RAG systems via DPO.
