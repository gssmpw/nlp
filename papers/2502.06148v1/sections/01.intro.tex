
\section{INTRODUCTION}
Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks~\cite{Brown2020Few-Shot,Touvron2023LLaMA,openai2024gpt4}.
However, their reliance on static parametric knowledge~\cite{Kasai2023RealTimeQA, mallen2023trust} often leads to inaccuracy or hallucination in responses~\cite{Welleck2020Neural, min2023factscore}.
Retrieval-Augmented Generation (RAG)~\cite{Lewis2020RAG,Guu2020REALM,ram2023context,asai2023retrieval} supplements LLMs with relevant knowledge retrieved from external sources, attracting increasing research interest.
One critical challenge for existing RAG systems is how to effectively integrate internal parametric knowledge with external retrieved knowledge to generate more accurate and reliable results.
% However, indiscriminately utilizing the retrieved knowledge may introduce irrelevant or off-topic information, compromising the quality of the response\cite{shi2023Irrelevant}.


In existing RAG approaches, LLMs depend either \emph{highly} or \emph{conditionally} upon external knowledge.
The former consistently uses the retrieved content as supporting evidence~\cite{Lewis2020RAG, Guu2020REALM, trivedi2023interleaving}, which often introduces irrelevant or noisy information and overlooks the valuable internal knowledge in LLMs, resulting in sub-optimal results.
In comparison, the latter integrates external knowledge into LLMs conditionally based on specific strategies, such as characteristics of input query \cite{mallen2023trust, jeong2024adaptive, wang2023skr}, probability of generated tokens~\cite{jiang2023active, su2024dragin}, or relevance of retrieved content~\cite{zhang2023merging, Xu2024Search, liu2024raisf}.
The query-based and token-based strategies generally utilize a fixed question set or a predefined threshold to decide whether to incorporate external knowledge, limiting their effectiveness due to incomplete information; 
the relevance-based strategy employs an additional validation module to assess the retrieved content, with its accuracy largely determining the quality of the final responses.
%with the successful integration of external knowledge heavily dependent on the accuracy of this additional module.
 

In this work, we explore leveraging the \textbf{LLM itself} to determine the correct result by \textbf{holistically} evaluating the outputs generated with and without external knowledge.
As illustrated in \Figref{sample}, given a query ``What does Ctrl+Shift+T do?'', we instruct the LLM to generate the \emph{LLM Answer} (i.e., ``New tab'') and the corresponding explanation (i.e., reasoning steps) with its internal parametric knowledge.
Meanwhile, we employ a retriever to obtain the relevant passages from external knowledge bases and feed the query and the retrieved passages to the LLM to produce the \emph{RAG Answer} (i.e., ``T'') and the corresponding explanation. 
Next, we instruct the LLM to take the query, \emph{LLM Answer} with its explanation and \emph{RAG Answer} with its explanation as input to choose the more accurate one (i.e., ``New tab'').
In this manner, the relevant internal and external knowledge to the query is comprehensively considered, facilitating the LLM in generating accurate responses, while the RAG framework maintains its simplicity by not requiring additional modules.

% selects the final answer from the responses generated using its internal parametric knowledge and external retrieved knowledge, thereby achieving improved accuracy.


Accordingly, we devise a novel \framework\ RAG framework that empowers the LLM to identify the more accurate answer to a query by evaluating both \emph{LLM Answer} and \emph{RAG Answer}, along with their respective explanations.
We validate the performance of the proposed \framework~framework with two open-sourced LLMs (see Section \ref{sec:main-result}) and find that it tends to fail in some scenarios, which we attribute to its limited capacity in distinguishing the correct answer from the incorrect one. 
To enhance the accuracy of the LLM selecting the right one among multiple responses generated from different knowledge sources, we develop a~\approach~method, leveraging Direct Preference Optimization (DPO)~\cite{Rafailov2023DPO} to fine-tune the LLM with a curated Retrieval-Generation Preference (\textbf{RGP}) dataset.
To construct this RGP dataset, we employ GPT-3.5~\cite{openai2024gpt4} to generate an \emph{LLM Answer} and an \emph{RAG Answer} for each query sampled from  WebQuestions~\cite{berant2013webq}, SQuAD 2.0~\cite{rajpurkar2018squad} and SciQ~\cite{welbl2017sciq}, and then retain only the pairs consisting of one correct answer and one incorrect answer, each accompanied by its corresponding explanation.
It consists of $3,756$ pairs of \emph{LLM Answer} and \emph{RAG answer} with their respective explanations, which we promise to release to the public to facilitate future research. 
%With the constructed dataset, we apply the advanced Direct Preference Optimization (DPO)\cite{Rafailov2023DPO} to enhance open-source LLMs in both selecting and generating accurate responses.


%第五段 实验结果 insight 

With this dataset, we train two different LLMs, including Mistral-7B~\cite{jiang2023mistral7b} and LLaMa-2-13B-Chat~\cite{touvron2023llama2openfoundation}, and evaluate them on two widely used datasets, i.e., Natural Questions (NQ)~\cite{kwiatkowski2019natural} and TrivialQA~\cite{joshi2017triviaqa}.
It is demonstrated that our \approach~method consistently achieves high effectiveness across various retrieval settings and different LLMs, enhancing the robustness and stability of RAG systems.
Moreover, additional experiments reveal that our \approach~method not only enhances LLMs' ability to distinguish valid answers from noisy ones but also improves their answer generation capabilities.
We further validate the rationale of each design in our method through ablation studies, and conduct error case analyses to offer deeper insights into the limitations of our proposed method.


In summary, the major contributions of our paper are three-fold: 
\begin{itemize} []
\item  We introduce a novel \framework~ RAG framework that leverages LLMs to determine the correct answer by evaluating a pair of responses generated with internal parametric knowledge solely and also with external retrieved knowledge.
\item  We propose a \approach~method that applies Direct Preference Optimization (DPO) to enhance LLMs in both identifying and generating the correct answers with a curated Retrieval-Generation Preference (RGP) dataset.
\item Extensive experiments with two open-sourced LLMs achieve superior performance on two widely-used datasets, demonstrating the effectiveness of our proposed Self-Selection framework and Self-Selection-RGP method.
\end{itemize}
