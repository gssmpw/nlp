

\section{EXPERIMENTS}
\label{sec:experiments}
In this section, we evaluate the proposed method with extensive experiments.
Note that we conduct our experiments with two variants based on our proposed \framework~framework,  i) \textbf{Self-Selection-Ori}, which refers to the RAG method that applies Self-Selection on vanilla LLMs. ii) \textbf{Self-Selection-RGP}, which denotes the RAG method that applies Self-Selection on the LLMs trained with our augmented RGP dataset.
For a comprehensive evaluation, we seek to address the following research questions:
\begin{itemize} 
\item \textbf{RQ1}: How does Self-Selection-RGP perform compared to other compared methods? 
\item \textbf{RQ2}: Whether Self-Selection-RGP is generalizable across different base LLMs and retrieval settings? 
\item \textbf{RQ3}: To what extent can Self-Selection-RGP affect the LLMs' inherent ability in answer generation? 
\item \textbf{RQ4}: What is the effect of each design in our proposed Self-Selection-RGP? 
\end{itemize}

\subsection{Experimental Setup}
\subsubsection{\textbf{Datasets}}
 We verify the proposed method with two open-domain QA datasets, Natural Question (NQ)~\cite{kwiatkowski2019natural} and TriviaQA~\cite{joshi2017triviaqa}.
\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{Natural Questions}: Natural Questions is a widely used dataset for the evaluation of RAG systems. Each QA pair is annotated by human annotators based on a Wikipedia page.
    \item \textbf{TriviaQA}: TriviaQA is another popular dataset based on trivia questions, paired with independently collected evidence documents, which is designed to support challenging reading comprehension tasks.
\end{itemize}
Following prior works \cite{trivedi2023interleaving,jeong2024adaptive}, we use the same test split for each dataset with the same external corpus to evaluate RAG methods. 
We present the statistics in \tabref{tab:dataset}.  

\begin{table}[htbp]
  \centering
  \large
  \setlength\abovecaptionskip{-0.3pt}
  \setlength\belowcaptionskip{-0.3pt}
  \caption{Statistics of the test datasets.}
  \resizebox{\columnwidth}{!}{%
    \begin{tabular}{p{4cm}p{3cm}p{3cm}} % 调整列宽
    \toprule
    \bf Dataset & \bf \#Passages & \bf \#QA pairs \\
    \midrule
    Natural Questions (NQ) & 21,015,324 & 500 \\
    TriviaQA & 21,015,324 & 500 \\
    \bottomrule
    \end{tabular}%
  }
  \label{tab:dataset}%
\end{table}



\begin{table*}[t]
  \setlength\abovecaptionskip{-0.3pt}
  \setlength\belowcaptionskip{-0.3pt}
  \setlength{\tabcolsep}{1.9mm}
  \centering
  \caption{Main results of our proposed methods and all baseline methods. }
    \begin{tabular}{llrrrrrrrrrrrr}
    \toprule
          &       & \multicolumn{6}{c}{\textit{zero-shot}}        & \multicolumn{6}{c}{\textit{few-shot}} \\
          &       & \multicolumn{3}{c}{\textbf{NQ}} & \multicolumn{3}{c}{\textbf{TriviaQA}} & \multicolumn{3}{c}{\textbf{NQ}} & \multicolumn{3}{c}{\textbf{TriviaQA}} \\
    \multicolumn{1}{c}{Base LLM} & \multicolumn{1}{c}{Method} & \multicolumn{1}{c}{EM} & \multicolumn{1}{c}{F1} & \multicolumn{1}{c}{Acc} & \multicolumn{1}{c}{EM} & \multicolumn{1}{c}{F1} & \multicolumn{1}{c}{Acc} & \multicolumn{1}{c}{EM} & \multicolumn{1}{c}{F1} & \multicolumn{1}{c}{Acc} & \multicolumn{1}{c}{EM} & \multicolumn{1}{c}{F1} & \multicolumn{1}{c}{Acc} \\
    \midrule
    \multirow{6}[2]{*}{Mistral (7B)} & LLM Only & 21.8  & 35.5  & 34.0  & 41.2  & 53.4  & 52.6  & 20.8  & 34.9  & 32.2  & 43.8  & 54.1  & 53.4 \\
          & Standard RAG  & 35.8  & 51.2  & 51.0  & 45.8  & 58.1  & 59.8  & 37.8  & 51.6  & 50.6  & 47.2  & 59.4  & 60.4 \\
          & Self-RAG & -     & -     & -     & 29.0  & 43.2  & 60.6  & -     & -     & -     & 41.0  & 53.0  & 55.0 \\
          & SURE  & \textbf{39.0} & 52.4  & 47.6  & 48.6  & 59.7  & 61.6  & 38.8  & 51.1  & 47.6  & 49.2  & 60.1  & 61.4 \\
          & \textbf{Self-Selection-Ori} & 34.6  & 50.1  & 50.2  & 48.4  & 61.2  & 62.8  & 33.0  & 47.3  & 45.2  & 49.8  & 61.7  & 62.4 \\
          & \textbf{Self-Selection-RGP} & 37.8  & \textbf{52.5} & \textbf{53.6} & \textbf{54.4} & \textbf{66.2} & \textbf{67.0} & \textbf{40.2} & \textbf{53.8} & \textbf{53.2} & \textbf{54.4} & \textbf{66.2} & \textbf{65.4} \\
    \midrule
    \multirow{6}[2]{*}{Llama2-Chat (13B)} & LLM Only & 21.2  & 31.9  & 28.2  & 43.2  & 50.1  & 48.0  & 24.8  & 35.3  & 30.2  & 49.8  & 57.1  & 54.0 \\
          & Standard RAG  & 24.6  & 37.0  & 45.2  & 35.2  & 46.1  & 55.0  & 31.8  & 43.4  & 44.8  & 46.0  & 54.8  & 54.6 \\
          & Self-RAG & -     & -     & -     & 17.2  & 36.6  & 63.4  & -     & -     & -     & 39.0  & 52.2  & 59.0 \\
          & SURE  & \textbf{39.4} & \textbf{52.3} & \textbf{52.0} & 50.4  & 63.0  & 63.8  & \textbf{42.6} & \textbf{53.2} & 50.4  & 40.6  & 51.3  & 65.0 \\
          & \textbf{Self-Selection-Ori} & 31.6  & 43.8  & 45.2  & 43.0  & 53.5  & 56.6  & 33.2  & 44.6  & 43.2  & 49.2  & 59.9  & 60.0 \\
          & \textbf{Self-Selection-RGP} & 36.6  & 49.2  & 46.2  & \textbf{56.6} & \textbf{66.3} & \textbf{66.0} & 40.0  & 52.4  & \textbf{51.6} & \textbf{52.6} & \textbf{65.5} & \textbf{66.8} \\
    \bottomrule
    \end{tabular}%
  \label{tab:main}%
\end{table*}%
\subsubsection{\textbf{Baselines}}
We will compare our proposed methods with the following baseline methods:
\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{LLM Only}: The response to each query is generated solely by LLMs. 
    \item \textbf{Standard RAG}: The response to each query is produced by LLMs after appending the retrieved passages to the input.
    \item \textbf{Self-RAG}~\cite{asai2024selfrag}: Specialized reflection tokens are utilized to enable LLMs to control retrieval and evaluate the relevance of the retrieved content during reasoning. In the experiments, we use the open-source models fine-tuned with the SELF-RAG framework, including the fine-tuned models based on Mistral 7B and Llama2-13B\cite{asai2024selfrag,SciPhi-AI2024}.
    \item \textbf{SURE}~\cite{kim2024sure}: LLMs first generate summaries of the retrieved passages for each candidate answer, and then identify the most plausible answer by evaluating each summary’s validity and ranking. We employ Mistral 7B  and Llama2-13B-Chat \cite{touvron2023llama2openfoundation} as the backbone models, consistent with our method.
\end{itemize}

\subsubsection{\textbf{Evaluation Metrics}}
In our experiments, we use the Exact Match (EM), F1 score and Accuracy (Acc) as our evaluation metrics, following the standard evaluation protocol~\cite{mallen2023trust,jeong2024adaptive,kim2024sure}. 
The EM measures whether the prediction and the gold answer are exactly the same; the F1 score measures the number of overlapping words between the prediction and the gold answer; the Accuracy metric measures whether the prediction contains the gold answer.
We normalize the predictions and gold answers (i.e., lowercasing and punctuation) to compute the metrics, following the implementation in previous works~\cite{rajpurkar-etal-2016-squad,kim2024sure}.


\subsubsection{\textbf{Implementation Details}}
In total, we sample $11,756$ QA pairs from WebQuestions~\cite{berant2013webq}, SQuAD2.0~\cite{rajpurkar2018squad}, and SciQ~\cite{welbl2017sciq}.
We use the official 2018 English Wikipedia as the external knowledge base similar to prior works~\cite{karpukhin-etal-2020-dense,jeong2024adaptive,asai2024selfrag}. 
We use pre-trained BGE (i.e., bge-large-en-v1.5)~\cite{Xiao2024cpack} as the retriever to obtain the relevant passages for each question.
For each query, we retrieve a variable number of passages, ranging from $1$ to $5$.
We adopt GPT-3.5 as our LLM for generating answers and explanations for each question, as well as evaluating the consistency between two given answers.
We utilize a Sentence transformer model (i.e., all-mpnet-base-v2)~\cite{reimers-2020-multilingual-sentence-bert} to identify the top-K similar questions for dataset augmentation.
After the data augmentation, we retain $21,928$ preference instances for model training.


For model training, we adopt the widely-used Mistral 7B (i.e., Mistral-7B-Instruct-v0.2)~\cite{jiang2023mistral7b} and Llama2-13B-Chat~\cite{touvron2023llama2openfoundation} as the base LLMs.
We apply DPO with Low-Rank Adapters (LoRA) \cite{hu2022lora} to train LLMs.
We conduct all our experiments on a GPU machine with 4 A800 NVIDIA RTX GPUs.
For model inference,  we use the same retriever BGE (i.e., bge-large-en-v1.5)~\cite{Xiao2024cpack} across all compared methods for a fair comparison.
We retrieve the top $5$ passages for each query from the external knowledge base.
Our experiments are conducted underboth zero-shot and few-shot settings. For the zero-shot setting, we follow the official settings of prior work\cite{asai2024selfrag,kim2024sure}.
For the few-shot setting, we include three examples in the prompts formatted according to the objective of each method.
Since the training data of Self-RAG includes the NQ dataset, we do not consider the results of Self-RAG on the NQ dataset in our experiments.

\begin{figure*}[t]
\setlength\abovecaptionskip{0px}
\setlength\belowcaptionskip{0px}
  \centering
  \includegraphics[width=\linewidth]{figs/bm25_full_2.pdf}
  \caption{An illustration of the effects of using a different retriever. Our \approach~and Standard RAG both use BM25 as the new retriever. We adopt Mistral-7B as the base LLM for Standard RAG (7B) and Self-Selection-RGP (7B), and Llama2-13B-Chat for Standard RAG (13B) and Self-Selection-RGP (13B). } 
  \label{fig:retriever}
\end{figure*}
\subsection{Main Results (RQ1)}
\label{sec:main-result}

To verify the effectiveness of the proposed \framework~ framework, we compare the performance with the baseline methods.
In Table~\ref{tab:main}, we present the main results on the NQ and TriviaQA datasets with Mistral-7B~\cite{jiang2023mistral7b} and Llama2-13B-Chat~\cite{Touvron2023LLaMA}, under both zero-shot and few-shot settings. 
From the results, we make several key observations: 

\begin{itemize}[leftmargin=*,nosep]
\item With Mistral-7B as the base LLM, our Self-Selection-RGP method consistently outperforms all compared methods on NQ and TriviaQA datasets under both zero-shot and few-shot settings.
On the TriviaQA dataset, our Self-Selection-RGP model achieves an accuracy of $67.0$ in the zero-shot setting and $65.4$ in the few-shot setting, making significant improvements of $5.4$ and $4.0$ points, respectively, compared to the best baseline method SURE which scores  $61.6$ and $61.4$.
In contrast, the performance gains of our Self-Selection-RGP method on the NQ dataset are relatively smaller.
Our method scores $53.6$ and $53.2$ in accuracy under zero-shot and few-shot settings, respectively, which makes an improvement of $2.6$ and $2.6$ over the best baseline method Standard RAG which scores $51.0$ and  $50.6$.
These substantial performance improvements over prior methods highlight the effectiveness of our proposed Self-Selection-RGP method.
\item With LLama2-13B-Chat as the base LLM, our Self-Selection-RGP method consistently delivers strong performance on both TriviaQA and NQ datasets. 
In particular, Self-Selection-RGP exhibits superior performance on the TriviaQA dataset, achieving an accuracy of $66.0$ and $66.8$ under zero-shot and few-shot settings, respectively. 
These results reflect improvements of $2.2$ and $1.8$ over the best baseline method SURE, which scores $63.8$ and $65.0$ in accuracy.
In comparison, our Self-Selection-RGP demonstrates competitive performance on the NQ dataset relative to the SURE method.
In the few-shot setting, Self-Selection-RGP attains an accuracy of $51.6$, resulting in a $1.2$ improvement over SURE’s accuracy of $50.4$. 
However, in the zero-shot setting, our Self-Selection-RGP scores $46.2$, which is $5.8$ lower than SURE’s accuracy of $52.0$.
This performance disparity may be attributed to the fact that the questions in the NQ dataset are inherently more challenging for LLMs compared to those in TriviaQA, as evidenced by the performance of the LLM-only approach in \tabref{tab:main}.
This complexity hampers our method's ability to effectively distinguish the correct answers from the incorrect ones.
\item In comparison, the Self-Selection-Ori method demonstrates superior performance on the TriviaQA dataset in both zero-shot and few-shot settings when utilizing Mistral-7B as the base LLM, surpassing all baseline methods.
However, the Self-Selection-Ori method encounters difficulties in producing accurate results on the NQ dataset or when using LLama2-13B-Chat as the base LLM. 
The potential reasons for the performance disparity are twofold: 1) Mistral-7B exhibits more advanced reasoning capabilities compared to the LLama2-13B series models, as supported by the comparative analysis presented in the Mistral-7B technical report~\cite{jiang2023mistral7b}; and 2) the questions in the NQ dataset are relatively more challenging than those in TriviaQA for LLMs, as aforementioned.
\end{itemize}



\subsection{Impact of Different Retrieval Settings (RQ2)}
\subsubsection{\textbf{ Effect of Different Retriever}}
We experiment to demonstrate the compatibility of the proposed \framework~framework with different retrieval methods. 
Specifically, beyond the BGE retriever considered in Table \ref{tab:main}, we also use BM25 as the retriever, and compare our \approach~method with Standard RAG to analyze whether our method can still maintain superiority in performance over Standard RAG with the new retriever.
We adopt Mistral-7B and Llama2-13B-Chat as the base language models, respectively.
This results in the following four RAG systems with BM25 as their retriever: Standard RAG (7B) and Self-Selection-RGP (7B), where the base LLM is Mistral-7B, as well as Standard RAG (13B) and Self-Selection-RGP (13B), where the base LLM is Llama2-13B-Chat.
We present the performance of these four systems on both the NQ and TriviaQA datasets under zero-shot and few-shot settings in \Figref{fig:retriever}.
From the figure, we make the following observations: 

\begin{itemize}[leftmargin=*,nosep]
\item  With BM25 as the retriever, our proposed Self-Selection-RGP method consistently outperforms the Standard RAG method in each setting as illustrated in \Figref{fig:retriever}, revealing the compatibility and generalizability of our method regarding new retrieval techniques like BM25.
To be more specific, as shown in \Figref{fig:retriever} (a) and (c), in the zero-shot setting, our Self-Selection-RGP (7B) and Self-Selection-RGP (13B) outperform the Standard RAG (7B) and Standard RAG (13B) models by substantial margins on all three metrics over both NQ and TriviaQA datasets. 
This highlights the significant effectiveness of our Self-Selection-RGP method in the zero-shot scenarios.
Comparably, as shown in \Figref{fig:retriever} (b) and (d), in the few-shot settings, the performance gains achieved by our Self-Selection-RGP method are relatively small. 
This may be because the samples available in the few-shot scenarios can provide sufficient knowledge to LLMs, diminishing the relative advantage of our proposed approach compared to the more standard RAG method.

\item Our Self-Selection-RGP method demonstrates enhanced robustness and stability compared to the Standard RAG method. 
This is evident in the comparisons shown in \Figref{fig:retriever}, where the performance variation between zero-shot and few-shot settings for Self-Selection-RGP is significantly smaller than that observed for the Standard RAG method, as illustrated in both (a) versus (b) and (c) versus (d). 
This advantage stems from the Self-Selection-RGP's lower sensitivity to input noise or perturbations, achieved through its preference alignment training.
This makes Self-Selection-RGP more adaptable to diverse retrieval techniques, even when the retrieved passages are not precisely relevant. 
Therefore, the proposed Self-Selection-RGP is capable of producing more reliable and accurate results.
This improved resilience to input variation allows our Self-Selection-RGP to maintain more consistent performance across a wide range of task conditions, leading to its enhanced robustness and stability compared to the Standard RAG method. 
\end{itemize}

\begin{figure}[t]
\setlength\abovecaptionskip{-0.3px}
\setlength\belowcaptionskip{-8px}
  \centering
  \includegraphics[width=\linewidth]{figs/number.pdf}
  \caption{An illustration of the effects of varying the number of retrieved passages. } 
  \label{fig:num_passage}
  \vspace{-0.2cm}
\end{figure}


\subsubsection{\textbf{Effect of Number of Retrieved Passages}}
Next, we investigate the impact of the number of retrieved passages on the performance.
Simply increasing the number of retrieved passages by providing a broader range of external retrieval information has been one of the most straightforward and effective methods to improve the performance of retrieve-and-read systems~\cite{karpukhin-etal-2020-dense}. 
However, the effectiveness and applicability of this approach may be limited, as LLMs are constrained by the length of the input context~\cite{liu-etal-2024-lost}.
In the following, we evaluate the effects on our proposed methods by varying the number of retrieved passages, including $1$, $3$, $5$, $8$ and $10$.
Specifically, we adopt BGE and BM25 as the retriever and Mistral-7B as the base LLM in this experiment. 
We conduct a comparative analysis of performance across three RAG methods, i.e. Standard RAG, Self-Selection, and Self-Selection-RGP, on both NQ and TriviaQA datasets.
We summarize the experimental results in \Figref{fig:num_passage}.
From these results, we can make the following observations:
\begin{itemize}[leftmargin=*,nosep]
    \item Increasing the number of retrieved passages initially leads to noticeable improvements in the accuracy of all three RAG methods. However, once the number of retrieved passages exceeds a certain threshold, adding more passages has only marginal effects on the performance, and in some cases even degrades the overall performance.
    This may be due to information overload within the constrained context window of LLMs, which impairs the model’s capacity to accurately generate the correct responses.
    \item On the TriviaQA dataset, our Self-Selection-RGP method consistently yields the highest performance across varying numbers of retrieved passages and different retrievers. The Self-Selection method ranks second, while the Standard RAG method performs the worst.
    The performance comparison reveals the effectiveness of our proposed Self-Selection framework.
    \item On the NQ dataset, our performance of the three methods are similar to that on TriviaQA, with Self-Selection-RGP consistently ranking first across different numbers of retrieved passages.
    When the retriever BGE is adopted, all the three methods exhibit competitive performance across varying numbers of retrieved passages. 
    Note that Self-Selection-RGP method achieves the best performance when the number of retrieved passages is set to $5$
    Compared to the LLM-only approach, Standard RAG significantly enhances the performance by integrating retrieved passages into the LLM input, thereby establishing a strong baseline, as shown in \tabref{tab:main}. 
    Our method can still achieve comparable performance to this strong baseline, well showcasing its remarkable effectiveness across a varying number of retrieved passages.
\end{itemize}
 
\subsection{Analysis of Answer Generation Capability of LLMs (RQ3)}
\label{sec:llm-improve}


According to the experimental results presented above, the LLMs trained with the preference dataset have demonstrated remarkable improvements in distinguishing the correct responses from the incorrect ones.
One question naturally arises: ``To what extent does this training process affect the LLMs’ inherent ability in answer generation with or without the dependence on external retrieved knowledge?''
To answer this question, we design an experiment to compare LLMs' answer generation performance before and after preference alignment training. 
We adopt Mistral-7B as the base LLM in this analysis.
We first apply \approach~to train Mistral-7B using the augmented RGP dataset, resulting in a trained LLM that is referred to as Self-Selection-RGP-7B.

\begin{figure}[t]
\setlength\abovecaptionskip{-0.4px}
\setlength\belowcaptionskip{-4px}
  \centering
  \includegraphics[width=\linewidth]{figs/ablation.pdf}
  \caption{Ablation study. ``Std RAG'' refers to Standard RAG; ``w/o Aug'' indicates the method without Dataset Augmentation; ``w/o Align'' denotes the method without Preference Alignment; and ``SS-RGP'' represents our proposed Self-Selection RAG method.} 
  \label{fig:ablation}
  \vspace{-0.4cm}
\end{figure}



\begin{figure*}[t]
\setlength\abovecaptionskip{-0.3px}
\setlength\belowcaptionskip{-0.4px}
  \centering
  \includegraphics[width=\linewidth]{figs/RQ3_2.pdf}
  \caption{Analysis of the answer generation capability before and after preference alignment training. ``Mistral-7B'' denotes the vanilla LLM before preference alignment training; ``Self-Selection-RGP-7B'' refers to the LLM obtained after training Mistral-7B with the preference alignment dataset.} % Add a caption if needed
  \label{fig:llm-ans}
\end{figure*}


To comprehensively compare the answer generation capabilities of \emph{Mistral-7B} and \emph{Self-Selection-RGP-7B}, we conduct extensive experiments on NQ and TriviaQA datasets, under both zero-shot and few-shot settings, with and without the use of external knowledge.
We present the experimental results in \Figref{fig:llm-ans}, from which we make the following observations:
\begin{itemize} [leftmargin=*,nosep]
    \item As shown \Figref{fig:llm-ans} (a) and (b), without depending on external knowledge, the Self-Selection-RGP-7B exhibits improved capabilities in answer generation on TriviaQA while showing slightly worse performance on NQ, compared to the Mistral-7B, in both zero-shot and few-shot settings.  
    \item From \Figref{fig:llm-ans} (c) and (d), the Self-Selection-RGP-7B consistently outperforms Mistral-7B on both datasets when involving the retrieved relevant passages in their inputs, showing its enhanced answer generation abilities.
\end{itemize}
 
 The empirical findings presented above demonstrate that the Self-Selection-RGP-7B model exhibits enhanced capabilities not only in answer selection, but also in the broader task of answer generation.
 This suggests that the proposed Self-Selection-RGP method which trains LLMs on the augmented RGP dataset has led to notable improvements in LLMs' ability to generate high-quality answers, highlighting its potential for enhancing the overall response generation performance of LLMs.


\subsection{Ablation Study (RQ4)}

To verify the effect of each design in our proposed method, we conduct an ablation study with Mistral-7B on both NQ and TriviaQA datasets.
We utilize three methods for ablation experiments in addition to our proposed \approach~method, including 
1) \textbf{Standard RAG}, which appends the retrieved passages to the input of the vanilla LLM;
2) \textbf{w/o Dataset Augmentation}, which removes the step of dataset augmentation from our proposed method and trains Mistral-7B with the original RGP dataset only; 
3)\textbf{w/o Preference Alignment}, which removes the step of preference alignment for the LLM and applies the \framework~on the vanilla LLM.
All the methods are evaluated under the zero-shot setting for a fair comparison.
We present the results in \Figref{fig:ablation}, from which we make the following observations: 

\begin{itemize}[leftmargin=*,nosep]
\item Removing either the step of dataset augmentation or preference alignment results in performance drop in all evaluation metrics on both NQ and TriviaQA datasets, highlighting the rationale of the design in our proposed \approach~method. 
\item Comparably, the removal of preference alignment results in a more substantial decrease in performance, revealing the importance of teaching LLMs how to choose the correct answer from multiple candidates.
\item Compared to the Standard RAG method, our method consistently yields superior performance on both datasets, demonstrating its great effectiveness in enhancing RAG systems. 

\end{itemize}

\subsection{Error Analysis}
We conduct an error analysis to investigate the limitations of our \approach~method. 
With the Mistral-7B as the base LLM, we sample $100$ from the errors made by our \approach~method on the TriviaQA dataset to conduct our analysis. 
We categorize the errors into five groups, as shown in ~\tabref{tab:error-case}, each with an example:
(1) Lack of Evidence (51\%): The LLM itself does not have sufficient internal knowledge to answer the question, and the retrieved passages also fail to provide enough information;
(2) Partial Matching (20\%): The final prediction captures part of the correct answer only;
(3) Reasoning Error (14\%): The generated explanation(s) contain the answer or the relevant information to infer the answer, but the LLM fails to predict the answer;
(4) Selection Error(12\%): One of the pairwise predictions is correct,  but the model fails to identify the correct one;
(5) Formatting Error(3\%): The correct answer is included in the prediction but the output format does not follow the instruction, leading to a failed interpretation.


From the table, we make the following observations:
\begin{itemize}
    \item  Over half of the errors are caused by the failure to obtain the knowledge that is necessary for predicting the answer, i.e., Lack of Evidence. This reflects the importance of developing advanced techniques to fetch the relevant information given a question from external knowledge bases to complement LLMs' internal knowledge for producing more accurate results.
    \item Approximately $39\%$ of errors, including Partial Matching, Reasoning Errors, and Formatting Errors, originate from LLMs' inadequacies in accurately interpreting the human instruction or the relevant knowledge required to infer the precise answer in the correct format. To address this issue, LLMs with enhanced reasoning capabilities are required.  
    \item $12\%$ errors arise from the LLMs' inability to effectively distinguish the correct answer from plausible ones, underscoring the demand for LLMs with enhanced reasoning capabilities. 
\end{itemize}

