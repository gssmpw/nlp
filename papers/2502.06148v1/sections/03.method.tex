
\section{SELF-SELECTION FRAMEWORK}
In this section, we elaborate on the proposed \framework~framework for enhanced Retrieval-Augmented Generation (RAG). 
Before explaining our method, we first revisit two preliminary concepts, i.e. Large Language Model (LLM) and Retrieval-Augmented Generation (RAG).  
Then, we present the formulation of our \framework~framework with detailed notations.
In order to strengthen the capabilities of the LLM in accurately generating and selecting responses, we develop a novel \approach~method, which is essentially fining tuning the LLM over a newly built Retrieval-Generation Preference (RGP) dataset. 
%定义 

\subsection{Preliminaries}
%1.1
\subsubsection{\textbf{Large Language Model (LLM)}}
For an LLM represented by $\mathcal{M}$, given a prompt $\bar{p}$ and a query $q$ as inputs, it returns a textual answer $\bar{a}$ as the output, which is formally expressed as 
\begin{equation}\label{eq:llm-ans}
\bar{a} = \mathcal{M}(\bar{p}, q).
\end{equation}
\subsubsection{\textbf{Retrieval-Augmented Generation (RAG)}} 
An RAG system employs a retriever to enhance the capability of the LLM by enabling it to access external knowledge beyond its internal parametric knowledge~\cite{lee2019LatentRetrieval, Guu2020REALM}. 
Given a query $q$, the retriever $\mathcal{R}$ searches for the relevant knowledge (e.g., passages) $C$ from an external knowledge base or corpus. 
%Let $C$ denote the retrieved passages. 
A common approach for RAG is to include the retrieved passages $C$ in the input to the LLM to improve the response quality.
Formally,
\begin{equation}
C = \mathcal{R}(q)\label{eq:retrieval},
\end{equation}
\begin{equation}\label{eq:rag-ans}
\hat{a} = \mathcal{M}(\hat{p}, q, C),
\end{equation}
where $\hat{p}$ represents the prompt used in RAG and $\hat{a}$ denotes the answer predicted by the LLM taking into account the retrieved passages $C$.


\subsection{Task Formulation}

%\header{Task Definition}
In this part we present the formulation of our \framework~framework.
An illustration is provided in \Figref{sample}. 
Given a query $q$, we first prompt an LLM $\mathcal{M}$ with $\bar{p}$ denoting the prompt to output the answer $\bar{a}$ with its explanation $\bar{e}$, where we refer to $\bar{a}$ as the \emph{LLM Answer} and $\bar{e}$ as the \emph{LLM Explanation}.
Next, we use the retriever $\mathcal{R}$ to gather relevant passages $C$ (Eq. (\ref{eq:retrieval})) to the same query $q$.
Then, we prompt the LLM $\mathcal{M}$ with $\hat{p}$ while providing $q$ and $C$ in the input, to generate $\hat{a}$ with its explanation $\hat{e}$, where we refer to  $\hat{a}$ and $\hat{e}$  as the \emph{RAG Answer} and the \emph{RAG Explanation}.
Finally, we prompt the LLM $M$ with a prompt $p$ by taking the query $q$, the \emph{LLM Answer} $\bar{a}$ with its \emph{LLM Explanation} $\bar{e}$, the \emph{RAG Answer} $\hat{a}$ with its \emph{RAG Explanation} $\hat{e}$ as inputs to select one as the final answer $a$ and final explanation $e$.
Formally,
\begin{equation}\label{eq:llm-ans-e}
(\bar{a}, \bar{e}) = \mathcal{M}(\bar{p}, q);
\end{equation}
\begin{equation}\label{eq:rag-ans-e}
(\hat{a}, \hat{e}) = \mathcal{M}(\hat{p}, q, C);
\end{equation}
\begin{equation}\label{eq:final-ans-e}
(a, e) = \mathcal{M}(p, q, (\bar{a}, \bar{e}), (\hat{a}, \hat{e})).
\end{equation}
%We refer to this new RAG flow as a \framework~framework, as illustrated in \Figref{sample}. 



% \begin{equation}\label{eq:llm-ans-e}
% (\bar{a}, \bar{e}) = \mathcal{M}(\bar{p}, q);
% \end{equation}
% \begin{equation}\label{eq:rag-ans-e}
% (\hat{a}, \hat{e}) = \mathcal{M}(\hat{p}, q, C);
% \end{equation}
% \begin{equation}\label{eq:final-ans-e}
% (a, e) = \mathcal{M}(p, q, (\bar{a}, \bar{e}), (\hat{a}, \hat{e})).
% \end{equation}
% The whole pipeline of \framework~framework is illustrated in \Figref{sample}. 

% \begin{equation}\label{eq:sel}
% a = \mathcal{M}(p, q, \bar{a}, \hat{a}).
% \end{equation}


%\subsection{Self-Selection Framework} 
% Our goal is to empower an LLM in an RAG system to effectively harness both its internal parametric knowledge and retrieved external knowledge to produce the answer to a query.
% In our proposed \framework~framework, to better leverage the powerful reasoning capability of LLMs, we further instruct the LLM to provide the corresponding explanation (i.e., reasoning steps) to support the predicted answer.
% First, an LLM is adopted to infer the \emph{LLM Answer} with the corresponding explanation relying sorely on its internal knowledge.
% Meanwhile, the LLM generates the \emph{RAG Answer} with the corresponding explanation depending on the external retrieved knowledge $C$.
% After holistically evaluating both candidates, the LLM selects the most appropriate answer and provides an explanation, thereby providing a knowledge fusion process.
% In \framework~framework, the same LLM is responsible not only for generating candidate answers but also for selecting the most accurate one among them.


% Formally, given a query $q$, the LLM $\mathcal{M}$ is requested to output the \emph{LLM Answer} $\bar{a}$ with its explanation $\bar{e}$ and the \emph{RAG Answer} $\hat{a}$ with its explanation $\hat{e}$. 
% Next, the LLM is instructed to produce the explanation $e$ to the final answer $a$,  
% \begin{equation}\label{eq:llm-ans-e}
% (\bar{a}, \bar{e}) = \mathcal{M}(\bar{p}, q);
% \end{equation}
% \begin{equation}\label{eq:rag-ans-e}
% (\hat{a}, \hat{e}) = \mathcal{M}(\hat{p}, q, C);
% \end{equation}
% \begin{equation}\label{eq:final-ans-e}
% (a, e) = \mathcal{M}(p, q, (\bar{a}, \bar{e}), (\hat{a}, \hat{e})).
% \end{equation}
% The whole pipeline of \framework~framework is illustrated in \Figref{sample}. 





\subsection{Self-Selection-RGP}

\subsubsection{\textbf{Motivation}}
We evaluate the performance of the proposed \framework~framework on two widely used QA datasets, Natural Question (NQ)~\cite{kwiatkowski2019natural} and TriviaQA~\cite{joshi2017triviaqa}, using existing open-source models, including Mistral 7B~\cite{jiang2023mistral7b} and Llama2-13B-Chat~\cite{touvron2023llama2openfoundation} without any model parameter updates. 
We report the experimental results in \tabref{tab:main} of Section \ref{sec:experiments}.
We find that our \framework~framework is promising in enhancing LLMs' answer generation by fusing internal knowledge with external knowledge, but directly applying such knowledge fusion does not always bring enhancements. 
For instance, simply equipping Mistral-7B with a retriever outperforms applying our \framework~to Mistral-7B with the same retriever on the NQ dataset. 
One assumption is that LLMs struggle to reliably discern the correct answer between two candidates generated from different knowledge sources.
In essence, this knowledge selection process is consistent with the goal of preference alignment in LLMs, i.e. generating the desired (positive) sample while rejecting the undesired (negative) one from a pair of preference data.
To address this challenge, we explore tuning LLMs through preference alignment techniques to enhance their ability to discern and select the correct answer from two candidates generated by different knowledge sources.
To achieve this goal, we develop a novel \approach~method to enhance LLMs' capabilities in identifying and generating correct answers, as shown in \Figref{method}.
We first build a preference dataset, then employ a simple yet effective augmentation technique to expand it, and finally apply the augmented preference dataset to train open-sourced LLMs with Direct Preference Optimization (DPO)~\cite{Rafailov2023DPO}. 

\subsubsection{\textbf{Retrieval-Generation Preference Dataset}}
Here we explain how we build the Retrieval-Generation Preference (RGP) dataset used for fine-tuning LLMs in~\approach.

\header{Preference Candidate Generation}
We first employ an LLM to produce two sets of responses for each query $q$: (i) an \emph{LLM Answer} $\bar{a}$ with its \emph{LLM Explanation} $\bar{e}$, derived from the model’s internal parametric knowledge; and (ii) an \emph{RAG Answer} $\hat{a}$ with its \emph{RAG Explanation} $\hat{e}$, relying on the externally retrieved information.
Specifically, we randomly select a subset of QA pairs from three existing open-domain QA datasets, including WebQuestions~\cite{berant2013webq}, SQuAD2.0~\cite{rajpurkar2018squad}, and SciQ~\cite{welbl2017sciq}.
Let $\mathcal{D}$ denote the obtained set of QA pairs. Formally,
\begin{equation} \label{eq:gold}
    \mathcal{D} = \bigl\{q^{(i)}, a_g^{(i)}\bigr\}_{i=1}^N
\end{equation}
where $a_g$ is the golden answer to the query $q$, $N$ is the number of QA pairs and  $i$ is the $i$-th QA pair in $\mathcal{D}$.
For each query $q$ in $\mathcal{D}$, we utilize a retriever $\mathcal{R}$ to retrieve the top-K passages $C$ from a corpus (Eq. (\ref{eq:retrieval})).
To ensure the quality of the constructed preference dataset, we employ GPT-3.5~\cite{Ouyang2022Training} as the model $\mathcal{M}$ for candidate answer and explanation generation given a query.
According to Eq. (\ref{eq:llm-ans-e}) and Eq. (\ref{eq:rag-ans-e}), we generate the answers and explanations ($\bar{a}$, $\bar{e}$) and ($\hat{a}$, $\hat{e}$). 
Finally, we obtain a collection of preference candidates for constructing the RGP datasets.
Formally, the $D$ is expanded as
\begin{equation} \label{eq:gold}
    \mathcal{D} = \bigl\{q^{(i)}, a_g^{(i)}, \bar{a}^{(i)},\bar{e}^{(i)}, \hat{a}^{(i)}, \hat{e}^{(i)}\bigr\}_{i=1}^N.
\end{equation}

\header{Preference Data Filtering}
In the RGP dataset, each instance should include both a desired (positive) answer and an undesired (negative) answer.
We filter these required instances from the collection $\mathcal{D}$.
For each instance in $D$, we first employ GPT-3.5 to assess whether the \emph{LLM Answer} $\bar{a}$ and the \emph{RAG Answer} $\hat{a}$ are correct by comparing each to the golden answer $a_g$.
After that, we only retain the instances that contain one right answer and one wrong answer, where (i) $\bar{a}$ is correct but $\hat{a}$ is incorrect; or (ii) $\hat{a}$ is correct but $\bar{a}$ is incorrect.
Based on this strategy, we gather all appropriate instances in $\mathcal{D}$ to build our RGP dataset $\mathbb{D}$. Formally,
\begin{equation} \label{eq:gold}
    \mathbb{D} = \bigl\{q^{(j)}, a_g^{(j)}, (a_p^{(j)}, e_p^{(j)}), (a_n^{(j)}, e_n^{(j)})\bigr\}_{j=1}^M
\end{equation}
where $a_p$ and $e_p$ represent the positive answer and its explanation, $a_n$ and $e_n$ represent the negative answer and its explanation, $M$ denotes the number of instances and $j$ denotes the $j$-th instance in $\mathbb{D}$.
Finally, we retain $3,756$ preference instances in the RGP dataset.
We promise to release it for facilitating future reseach.


\subsubsection{\textbf{Retrieval-Generation Preference Alignment}}
With the constructed RGP dataset, we train open-source LLMs to enhance their ability to distinguish the positive answer from the negative counterpart.

\header{RGP Dataset Augmentation}
To improve LLMs' preference alignment, we first augment the RGP dataset through a simple yet effective approach to produce more preference instances. 
In particular, given a query $q$ in RGP, we search for the top-K similar queries in the RGP datasets and we regard all answers to these $K$ queries as negative answers to the query $q$.
Formally, for each query $q$ in the RGP dataset~$\mathbb{D}$,  we denote the obtained most similar queries and their responses in RGP as $\mathbb{G}$:  
\begin{equation} \label{eq:group}
    \small
    \mathbb{G}^{(i)} = \{ q^{(j)}, y_w^{(j)}, y_l^{(j)}~|~\underset{\text{top-}\mathrm{K}}{\mathrm{argmax}}\ S\left(q^{(i)}, q^{(j)}\right)\}~~~\forall q^{(i)} \in \mathbb{D}, i\neq j
\end{equation}
where $S(q^{(i)}, q^{(j)})$ represents the similarity score between $q^{(i)}$ and $q^{(j)}$, and $y_w^{(j)}$ and $y_l^{(j)}$ represent the corresponding positive and negative response (i.e., answer with its explanation) for $q^{(j)}$ in RGP.
For one query $q^{(i)}$, $y_w^{(i)}$ and $y_l^{(i)}$ are the original positive and negative response in RGP.
Then, we regard all $y_w^{(j)}$ and $y_l^{(j)}$ in the obtained set $\mathbb{G}^{(i)}$ as the additional negative responses to $q^{(i)}$.
For each query, now we have $1$ positive response and $2K+1$ negative responses, which can be used to form $2K+1$ pairs of preference instances. 
Let $\mathbb{D}_{aug}$ denote the augmented RGP dataset. Formally,
\begin{equation}
\mathbb{D}_{aug}^{(i)} = \left\{ \left( q^{(i)}, y_w^{(i)}, y_{lj}^{(i)} \right) \right\}_{i=1}^M, \, j=1,2,\dots,2K+1,
\end{equation}
where $y_{lj}^{(i)}$ is the $j$-th negative response to the query $q^{(i)}$. 

\header{Retrieval-Generation Preference Training} 
With the augmented preference dataset, our goal is to train open-sourced LLMs to enhance their capabilities in distinguishing the correct answers from the incorrect ones.
During the preference alignment phase of LLM training, each instance in the augmented dataset $\mathbb{D}_{aug}$ comprises three key elements: an input $x$, a desired response $y_w$, and an undesired response $y_l$,  which is denoted as $y_w \succ y_l \mid x$.
Specifically, the input $x$ consists of a query $q$, the desired response $y_w$, the undesired response $y_l$, and a prompt $p$ designed to instruct the LLM $M$ to choose between $y_w$ and $y_l$ (see Eq. (\ref{eq:final-ans-e})).
The desired response $y_w$ includes the correct answer along with its explanation, while the undesired response $y_l$ contains an incorrect answer and its explanation.
To enhance the robustness of the trained model, we randomly alternate the order of $y_w$ and $y_l$ within the input $x$.


We adopt Direct Preference Optimization (DPO)~\cite{Rafailov2023DPO} to train LLMs.
It enables preference data to be directly associated with the optimal policy, eliminating the need for any additional reward model.
DPO formulates a maximum likelihood objective as follows:
\begin{equation}\label{eq:optimum_model}
    \tiny
    \mathcal{L}_\text{DPO}(\pi_{\theta}; \pi_{ref}) = -\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\left[\log \sigma \left(\beta \log \frac{\pi_{\theta}(y_w\mid x)}{\pi_{ref}(y_w\mid x)} - \beta \log \frac{\pi_{\theta}(y_l\mid x)}{\pi_{ref}(y_l\mid x)}\right)\right]
\end{equation}
where  $\beta$ represents the deviation of the policy $\pi_{\theta}$ from the reference model $\pi_{ref}$.
Our proposed optimization method aims to enhance LLMs' answer selection capability, enabling them to holistically evaluate multiple responses generated from diverse knowledge sources and identify the most accurate one among them.
In addition, we also hope this optimization method can further improve the inherent ability of LLMs in answer generation (more analysis is provided in Section \ref{sec:llm-improve}).




