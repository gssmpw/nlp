% chktex-file 3 % disable {} warnings for exponents etc.

\section{Proof of Theorem~\ref{thm:residual-expression}}\label{append:proof-residual-expression}

\begin{proof}
%\todo{while I think having the notation close to where it is used in the proof is good, I also think this information is too important to be ``hidden'' in the appendix.}

%In addition, we define the \ac{RRE} weights $\rr_i\in\mathbb{R}$, $i=1\colon k$.
Before starting the proof we need the following auxiliary identities, which can
be shown, after some manipulation, from the modified RADI iteration
(Algorithm~\ref{alg:radi}), that is,
\begin{multline}\label{eq:AVkYinvVkT}
	A^\TT V_k\Y_k^{-1}V_k^\TT E
	= R_{k-1}T(R_k^\TT-R_{k-1}^\TT)
	+ E^\TT X_{k-1}BH^{-1}B^\TT V_k\Y_k^{-1}V_k^\TT E\\
	- \sigma_{k-1} E^\TT V_k\Y_k^{-1}V_k^\TT E
\end{multline}
and
\begin{multline}\label{eq:long-symterm}
	-\rr_k^2 E^\TT V_k\Y_k^{-1}V_k^\TT B H^{-1} B^\TT  V_k\Y_k^{-1}V^\TT_k E
	= -\rr_k^2 E^\TT V_k\Y_k^{-1}\cdot 2\sigma_{k-1}(T-\Y_k) \cdot \Y_k^{-1}V^\TT_k E \\
	= \rr_k^2(R_k-R_{k-1})T(R_k^\TT-R_{k-1}^\TT) +
	2\rr_k^2\sigma_{k-1}E^\TT V_k\Y_k^{-1}V_k^\TT E,
\end{multline}
where
\begin{equation}\label{eq:Re-symterm}
	2\sigma_k E^\TT V_k\Y_k^{-1}V_k^\TT E =-(R_k-R_{k-1})\Y_k(R_k^\TT-R_{k-1}^\TT).
\end{equation}

The proof is by induction on the number $n$ of extrapolation terms. First consider the case $n=2$. From the
assumption $\rr_1+\rr_2 = 1$ and the equation (with $V_1\equiv Z_1$ and $\Y_1^{-1}\equiv D_1$)
\begin{equation}\label{eq:X_k}
	X_k=\sum_{i=1}^{k}V_i\Y_i^{-1}V_i^\TT, \quad k=1,2,\dots,
\end{equation}
we have $\R(\rr_1X_1 + \rr_2X_2) =	\R(X_1 + \rr_2 V_2\Y_2^{-1}V_2^\TT)$. Using~\eqref{eqn:riccati},
%the identity for the residual matrix
%\[
%\R(\Xi) = A^\TT\Xi E + E^{\TT}\Xi A + C^\TT C - E^\TT \Xi B H^{-1} B^\TT\Xi E,
%\]
the expression then can be expanded as
\begin{multline*}
	\R(\rr_1X_1 + \rr_2X_2) = \R(X_1) + \rr_2(A^\TT V_2\Y_2^{-1}V_2^\TT E + \square^\TT) \\
	-(\rr_1+\rr_2)\rr_2(E^{H} X_1BH^{-1} B^\TT V_2\Y_2^{-1}V_2^\TT+\square^\TT)
	- \rr_2^2 E^{H}V_2\Y_2^{-1}V_2^\TT BH^{-1}B^\TT V_2\Y_2^{-1}V_2^\TT E,
\end{multline*}
where and henceforth we write the ``$\square^\TT$'' symbol rightmost inside a bracket as a shorthand for the transpose of the preceding terms (for the sake of brevity).
It follows from~\eqref{eq:AVkYinvVkT} and~\eqref{eq:long-symterm} and then~\eqref{eq:Re-symterm} that
\begin{align*}
	& \R(\rr_1X_1 + \rr_2X_2)= \R(X_1) + \rr_2\left(R_{1}T(R_2^\TT-R_{1}^\TT)
		+\square^\TT\right) - 2\rr_2\sigma_1 E^\TT V_2\Y_2^{-1}V_2^\TT E \\
	& + \big((\rr_2-(\rr_1+\rr_2)\rr_2)
		E^\TT X_1BH^{-1}B^\TT V_2\Y_2^{-1}V_2^\TT E+\square^\TT\big) \\
	& + \rr_2^2(R_2-R_{1})T(R_2^\TT-R_{1}^\TT) +
		2\rr_2^2\sigma_1E ^\TT V_2\Y_2^{-1}V_2^\TT E
	\\
	={}& \R(X_1) + \rr_2\left(R_{1}T(R_2^\TT-R_{1}^\TT) +\square^\TT\right)
	+ \rr_2^2(R_2-R_{1})T(R_2^\TT-R_{1}^\TT) \\
	& + 2(\rr_2^2-\rr_2)\sigma_1E^\TT V_2\Y_2^{-1}V_2^\TT E
	\\
	={}& \R(X_1) + \rr_2\left(R_{1}T(R_2^\TT-R_{1}^\TT) +\square^\TT\right)
	+ \rr_2^2(R_2-R_{1})T(R_2^\TT-R_{1}^\TT) \\
	& + (\rr_2-\rr_2^2)(R_2-R_{1})\Y_2(R_2^\TT-R_{1}^\TT).
\end{align*}
The final expression can equivalently be written as the factorization
\begin{equation*}%\label{eq:range-fac-2}
	\R(\rr_1X_1 + \rr_2X_2) =
	\begin{bmatrix}
		R_1 & R_2
	\end{bmatrix}\mathscr{H}_2\begin{bmatrix}
		R_1^\TT \\ R_2^\TT
	\end{bmatrix},\quad
	\mathscr{H}_2\equiv  \mathscr{H}_2(\rr_1,\rr_2) = \mathscr{A}_2 + \mathscr{B}_2,
\end{equation*}
where
\begin{equation*}
	\mathscr{A}_2 = \blkdiag(T, \textbf{0}_{p\times p}),\quad
	\mathscr{B}_2 = \begin{bmatrix}
		(\rr_2^2-2\rr_2)T+(\rr_2-\rr_2^2)\Y_2 & (\rr_2-\rr_2^2)(T-\Y_2) \\
		(\rr_2-\rr_2^2)(T-\Y_2)    &  \rr_2^2 T +(\rr_2-\rr_2^2)\Y_2
	\end{bmatrix}.
\end{equation*}
This clearly shows $\range (\R(\rr_1X_1 + \rr_2X_2))\subseteq \range([R_1\ R_2])$.
Although in the expression for $\mathscr{H}_2$ there is only one free parameter $\rr_2$ as a result of the constraint $\rr_1+\rr_2=1$, here we still write out explicitly the dependence on all the parameters as $\mathscr{H}_2(\rr_1,\rr_2)$; same for all the
$\mathscr{H}_k$ below.

We will see later that the case $n=2$ is special in that the terms
$E^\TT X_1BH^{-1}B^\TT V_2\Y_2^{-1}V_2^\TT E$ and its transpose disappear because their
coefficients cancel out as $\rr_2-(\rr_1+\rr_2)\rr_2=0$, which is a property that does not hold for
arbitrary $n\neq 2$.

In order to see the pattern, we also present the case of $n=3$ in detail (as the \emph{base case}
for induction).
We have, using the assumption $\rr_1+\rr_2+\rr_3=1$ and~\eqref{eq:AVkYinvVkT}--\eqref{eq:X_k},
\begin{align*}
	& \R(\rr_1X_1 + \rr_2X_2 + \rr_3X_3) = \R(\rr_1X_1 + (\rr_2+\rr_3)X_2 +
	\rr_3V_3\Y_3^{-1}V_3^\TT)
	\\
	={}& \R(\rr_1X_1 + (\rr_2+\rr_3)X_2) + \rr_3(A^\TT V_3\Y_3^{-1}V_3^\TT E + \square^\TT)\\
	&- \rr_3^2 E^\TT V_3\Y_3^{-1}V_3^\TT BH^{-1}B^\TT V_3\Y_3^{-1}V_3^\TT E\\
	&-\rr_3\big(E^\TT(\rr_1 X_1+(\rr_2+\rr_3)X_2)BH^{-1}B^\TT V_3\Y_3^{-1}V_3^\TT E+\square^\TT\big)
	\\
	={}& \R(\rr_1X_1 + (\rr_2+\rr_3)X_2) + \rr_3\left(R_{2}T(R_3^\TT-R_{2}^\TT) +\square^\TT\right)
	- 2\rr_3\sigma_2 E^\TT V_3\Y_3^{-1}V_3^\TT E\\
	&+ \rr_3\big(E^\TT((1-\rr_2-\rr_3)X_2-\rr_1X_1)BH^{-1}B^\TT V_3\Y_3^{-1}V_3^\TT E
	+\square^\TT\big) \\
	&+ \rr_3^2(R_3-R_{2})T(R_3^\TT-R_{2}^\TT) +
	2\rr_3^2\sigma_2E^\TT V_3\Y_3^{-1}V_3^\TT E
	\\
	={}& \R(\rr_1X_1 + (\rr_2+\rr_3)X_2) + \rr_3\left(R_{2}T(R_3^\TT-R_{2}^\TT) +\square^\TT\right) + \rr_3^2(R_3-R_{2})T(R_3^\TT-R_{2}^\TT) \\
	&+ (\rr_3-\rr_3^2)(R_3-R_{2})\Y_3(R_3^\TT-R_{2}^\TT)\\
	&+ \rr_3\big(E^\TT\rr_1(X_2-X_1)BH^{-1}B^\TT V_3\Y_3^{-1}V_3^\TT E+\square^\TT\big),
\end{align*}
where $\range (\R(\rr_1X_1 + (\rr_2+\rr_3)X_2))\subseteq \range([R_1\ R_2])$ from the case of $n=2$, and so we only need to investigate the last two terms:
\begin{multline*}%\label{eq:cross-term-2}
	(E^\TT(X_2-X_1)BH^{-1}B^\TT V_3\Y_3^{-1}V_3^\TT E+\square^\TT) \\
	=E^\TT V_2\Y_2^{-1}V_2^\TT BH^{-1}B^\TT V_3\Y_3^{-1}V_3^\TT E
	  + E^\TT V_3\Y_3^{-1}V_3^\TT BH^{-1}B^\TT V_2\Y_2^{-1}V_2^\TT E.
\end{multline*}
Defining
\begin{equation}\label{eq:alpha-Cik}
	\alpha_{ik}=
	\frac{1}{2\sqrt{\sigma_{i-1}\sigma_{k-1}}}\in\mathbb{R},\qquad
	C_{ik} = V_i^\TT B H^{-1}B^\TT V_k \in\mathbb{R}^{p\times p},
\end{equation}
we have
\begin{multline*}
	(E^\TT(X_2-X_1)BH^{-1}B^\TT V_3\Y_3^{-1}V_3^\TT E + \square^\TT)
	=\\  \alpha_{23}  ((R_2-R_1)  C_{23}  (R_3^\TT - R_2^\TT)
	    + (R_3 - R_2)C_{23}^\TT(R_2^\TT-R_1^\TT) ),
\end{multline*}
and, therefore,
\begin{equation*}
	\rr_3 (\rr_1E^\TT(X_2-X_1)BH^{-1}B^\TT V_3\Y_3^{-1}V_3^\TT E+\square^\TT) =
	[R_1\ R_2\ R_3]\mathscr{C}_3 [R_1\ R_2\ R_3]^\TT,
\end{equation*}
where
\begin{equation*}
	\mathscr{C}_3 = \rr_3\rr_1 \alpha_{23}
	\begin{bmatrix}
		\textbf{0} & C_{23} & -C_{23} \\
		C_{23}^\TT  & -C_{23} - C_{23}^\TT & C_{23} \\
		-C_{23}^\TT &  C_{23}^\TT & \textbf{0}
	\end{bmatrix}.
\end{equation*}
Then, the factorization follows:
\begin{align*}%\label{eq:range-fac-3}
	\R(\rr_1X_1 + \rr_2X_2 + \rr_3X_3) =
	[R_1 \ R_2 \ R_3] \mathscr{H}_3\begin{bmatrix}
		R_1^\TT \\ R_2^\TT \\ R_3^\TT
	\end{bmatrix},
\end{align*}
where
\begin{align*}
  \mathscr{H}_3 \equiv  \mathscr{H}_3(\rr_1,\rr_2,\rr_3) = \mathscr{A}_3 + \mathscr{B}_3 +
	\mathscr{C}_3,
\end{align*}
with
\begin{equation*}
	\mathscr{A}_3 = \blkdiag(\mathscr{H}_2(\rr_1,\rr_2+\rr_3), \textbf{0}_{p\times p})
\end{equation*}
and
\begin{equation*}
	\mathscr{B}_3 = \blkdiag\left(\textbf{0}_{p\times p},
	\begin{bmatrix}
		(\rr_3^2-2\rr_3)T+(\rr_3-\rr_3^2)\Y_3    &   (\rr_3-\rr_3^2)(T-\Y_3) \\
		(\rr_3-\rr_3^2)(T-\Y_3) & \rr_3^2 T +(\rr_3-\rr_3^2)\Y_3
	\end{bmatrix}\right).
\end{equation*}
This clearly shows $\range (\R(\rr_1X_1 + \rr_2X_2 + \rr_3X_3))
\subseteq \range([R_1\ R_2\ R_3])$.

Assume as the \textit{induction hypothesis} ($n=k-1$)
$\range (\R(\sum_{i=1}^{k-1}\zeta_i X_i))\subseteq \range([R_1\ R_2\ \dots\
R_{k-1}])$
for any set of real numbers $\{\zeta_1,\zeta_2,\dots,\zeta_{k-1}\}$ such that  $\sum_{i=1}^{k-1}\zeta_i=1$, and, specifically,
\begin{equation*}
	\R\left(\sum_{i=1}^{k-1}\zeta_i X_i\right) =
	[	R_1 \ R_2 \ \cdots \ R_{k-1}]
	\mathscr{H}_{k-1}(\zeta_1,\zeta_2,\dots,\zeta_{k-1})
	\begin{bmatrix}
		R_1^\TT \\ R_2^\TT \\ \dots \\ R_{k-1}^\TT
	\end{bmatrix}
\end{equation*}
holds for some symmetric matrix
$\mathscr{H}_{k-1}(\zeta_1,\zeta_2,\dots,\zeta_{k-1})\in\mathbb{R}^{(k-1)p\times (k-1)p}$.
We now prove
\begin{equation*}
  \range \left(\R\left(\sum_{i=1}^{k}\rr_i X_i\right)\right)\subseteq
  \range([R_1\ R_2\ \dots\ R_{k}])
\end{equation*}
with $\sum_{i=1}^{k}\rr_i=1$ and show how to efficiently obtain the factorization of the residual
of the $k$-term extrapolant. Similarly, by using the assumption $\sum_{i=1}^{k}\rr_i=1$
and~\eqref{eq:AVkYinvVkT}--\eqref{eq:X_k}, we have
\begin{align*}
	& \R(\rr_1 X_1 + \rr_2 X_2 + \cdots+ \rr_k X_k) = \textstyle \R\big(\sum_{j=1}^{k-2}\rr_{j}X_j
	+ (\rr_{k-1}+\rr_k)X_{k-1} + \rr_k V_k Y_k^{-1}V_k^\TT \big)
	\\
	={}& \textstyle \R\big(\sum_{j=1}^{k-2}\rr_{j}X_j + (\rr_{k-1}+\rr_k)X_{k-1}\big)
	+ \rr_k(A^\TT V_k\Y_k^{-1}V_k^\TT E + \square^\TT) \\
	&- \rr_k^2 E^\TT V_k\Y_k^{-1}V_k^\TT BH^{-1}B^\TT V_k\Y_k^{-1}V_k^\TT E \\
	& \textstyle -\rr_k\big(E^\TT(\sum_{j=1}^{k-2}\rr_{j}X_j +
	(\rr_{k-1}+\rr_k)X_{k-1})BH^{-1}B^\TT V_k\Y_k^{-1}V_k^\TT E+\square^\TT\big)
	\\
	={}& \textstyle \R\big(\sum_{j=1}^{k-2}\rr_{j}X_j + (\rr_{k-1}+\rr_k)X_{k-1}\big)\\
    &+\rr_k\big(R_{k-1}T(R_k^\TT-R_{k-1}^\TT) +\square^\TT\big) -
	  2\rr_k\sigma_{k-1} E^\TT V_k\Y_k^{-1}V_k^\TT E \\
	&+ \textstyle \rr_k\big(E^\TT((1-\rr_{k-1}-\rr_k)X_{k-1}-\sum_{j=1}^{k-2}\rr_{j}X_j)
		BH^{-1}B^\TT V_k\Y_k^{-1}V_k^\TT E + \square^\TT\big) \\
	&+  \textstyle  \rr_k^2(R_k-R_{k-1})T(R_k^\TT-R_{k-1}^\TT) +
	2\rr_k^2\sigma_{k-1} E^\TT V_k\Y_k^{-1}V_k^\TT E
	\\
	={}&\textstyle \R\big(\sum_{j=1}^{k-2}\rr_{j}X_j + (\rr_{k-1}+\rr_k)X_{k-1}\big)
	+ \rr_k(R_{k-1}T(R_k^\TT-R_{k-1}^\TT) +\square^\TT)\\
	&{} + \rr_k^2(R_k-R_{k-1})T(R_k^\TT-R_{k-1}^\TT) +
		(\rr_k-\rr_k^2)(R_k-R_{k-1})\Y_k(R_k^\TT-R_{k-1}^\TT) \\
	&\textstyle  + \rr_k\big(E^\TT\sum_{j=1}^{k-2}\rr_{j}(X_{k-1}-X_j)BH^{-1}B^\TT
	 V_k\Y_k^{-1}V_k^\TT E +\square^\TT\big).
\end{align*}
We have, using~\eqref{eq:X_k} again,
\begin{align*}
	&\textstyle \rr_k\sum_{j=1}^{k-2}\rr_{j}(X_{k-1}-X_j) = \rr_k\sum_{j=1}^{k-2}\rr_{j}\big(
	\sum_{i=1}^{k-1}V_i\Y_i^{-1}V_i^\TT - \sum_{i=1}^{j}V_i\Y_i^{-1}V_i^\TT \big)
	\\
	={}& \textstyle \rr_k\sum_{j=1}^{k-2}\rr_{j} \sum_{i=j+1}^{k-1}V_i\Y_i^{-1}V_i^\TT
	\\
	={}& \textstyle \rr_k\rr_1V_2\Y_2^{-1}V_2^\TT + \rr_k(\rr_1+\rr_2)V_3\Y_3^{-1}V_3^\TT + \cdots\\
	&+ \rr_k(\rr_1+\rr_2+\cdots+\rr_{k-2}) V_{k-1}\Y_{k-1}^{-1}V_{k-1}^\TT \\
	=:& \textstyle \sum_{i=2}^{k-1} \beta_{ki}V_{i}\Y_{i}^{-1}V_{i}^\TT,
\end{align*}
where we have defined $\beta_{ki}=\rr_k\sum_{j=1}^{i-1}\rr_j$.
Therefore, we have
\begin{equation*}
	E^\TT V_{i}\Y_{i}^{-1}V_{i}^\TT BH^{-1} B^\TT V_k\Y_k^{-1}V_k^\TT E
	=\alpha_{ik}(R_i-R_{i-1})C_{ik}(R_k^\TT-R_{k-1}^\TT),
\end{equation*}
using the definitions from~\eqref{eq:alpha-Cik}. And, after some manipulation,
\begin{multline*}
	\rr_k\left(\sum_{j=1}^{k-2}\rr_{j}E^\TT(X_{k-1}-X_j)BH^{-1}B^\TT V_k\Y_k^{-1}V_k^\TT E
	+\square^\TT\right) =\\
	[R_1\ R_2\ \dots\ R_k]\mathscr{C}_k [R_1\ R_2\ \dots\ R_k]^\TT,
\end{multline*}
where
\begin{equation*}
	\mathscr{C}_k = \widetilde{\mathscr{C}}_k + \widetilde{\mathscr{C}}_k^\TT,\quad
	\widetilde{\mathscr{C}}_k =
	\begin{bNiceArray}[
		first-row,code-for-first-row=\scriptstyle,
		last-col,code-for-last-col=\scriptstyle,
		]{ccc}
		(k-2)p       &  p           &  p            &  \\
		\textbf{0}   & M_k          & -M_k          & (k-1)p \\
		\textbf{0}   & \textbf{0}   & \textbf{0}    & p
	\end{bNiceArray}\ ,
\end{equation*}
and
\begin{equation*}
	M_k = \begin{bmatrix}
		\beta_{k2}\alpha_{2k}C_{2k}  \\
		\beta_{k3}\alpha_{3k}C_{3k}-\beta_{k2}\alpha_{2k}C_{2k} \\
		\beta_{k4}\alpha_{4k}C_{4k}-\beta_{k3}\alpha_{3k}C_{3k} \\
		\vdots \\
		\beta_{k,k-1}\alpha_{k-1,k}C_{k-1,k}-\beta_{k,k-2}\alpha_{k-2,k}C_{k-2,k} \\
		-\beta_{k.k-1}\alpha_{k-1,k}C_{k-1,k}
	\end{bmatrix},\quad k\ge 3.
\end{equation*}
Then, the factorization follows:
\begin{equation*}%\label{eq:range-fac-k}
	\R\left(\sum_{i=1}^{k}\rr_i X_i\right) =
		[R_1 \ R_2 \ \cdots \ R_k]
	\mathscr{H}_k\begin{bmatrix}
		R_1^\TT \\ R_2^\TT \\ \dots \\ R_k^\TT
	\end{bmatrix}, \quad
	\mathscr{H}_k\equiv  \mathscr{H}_k(\rr_1,\rr_2,\dots,\rr_k) = \mathscr{A}_k + \mathscr{B}_k +
	\mathscr{C}_k,
\end{equation*}
where
\begin{equation*}
	\mathscr{A}_k = \blkdiag
	\left(\mathscr{H}_{k-1}(\rr_1,\rr_2,\dots,\rr_{k-2},\rr_{k-1}+\rr_{k}), \textbf{0}_{p\times
	p}\right).
\end{equation*}
in which the existence of $\mathscr{H}_{k-1}(\rr_1,\rr_2,\dots,\rr_{k-2},\rr_{k-1}+\rr_{k})\equiv
\mathscr{H}_{k-1}$ such that
\[
\R\left(\sum_{j=1}^{k-2}\rr_{j}X_j + (\rr_{k-1}+\rr_k)X_{k-1}\right)= [R_1 \ R_2 \ \cdots \ R_{k-1}]
\mathscr{H}_{k-1}
\begin{bmatrix}
	R_1^\TT \\ R_2^\TT \\ \dots \\ R_{k-1}^\TT
\end{bmatrix}
\]
follows from the inductive hypothesis, and
\begin{equation*}
	\mathscr{B}_k = \blkdiag\left(\textbf{0}_{(k-2)p\times (k-2)p},
	\begin{bmatrix}
		(\rr_k^2-2\rr_k)T+(\rr_k-\rr_k^2)\Y_k    &   (\rr_k-\rr_k^2)(T-\Y_k) \\
		(\rr_k-\rr_k^2)(T-\Y_k) & \rr_k^2 T +(\rr_k-\rr_k^2)\Y_k
	\end{bmatrix}\right).
\end{equation*}
The range property $\range (\R(\sum_{i=1}^{k}\rr_i X_i))\subseteq \range([R_1\ R_2\ \dots\ R_{k}])$
follows (for $n=k$) and hence the induction is completed.
\end{proof}

\section{Proof of Theorem~\ref{thm:residual-psd-cond}}\label{append:proof-residual-psd-cond}
\begin{proof}
Clearly, if we can factorize $\mathscr{H}_2$ into the form of
$HH^\TT$ for some $H$, then a factorization of the same form is immediately available for $\R(\rr_1X_1 + \rr_2X_2)$, which is equivalent to say the matrix is positive semi-definite.

Since $T$ and $\Y_2$ are both positive semi-definite, we can write $T = F_1F_1^\TT$ and $\Y_2 = F_2F_2^\TT$, and then we have
\begin{align*}
	\mathscr{H}_2  &=
	\begin{bmatrix}
		\rr_1^2F_1F_1^\TT+\rr_1\rr_2F_2F_2^\TT & \rr_1\rr_2(F_1F_1^\TT-F_2F_2^\TT) \\
		\rr_1\rr_2(F_1F_1^\TT-F_2F_2^\TT)    &  \rr_2^2 F_1F_1^\TT +\rr_1\rr_2F_2F_2^\TT
	\end{bmatrix} \\
	& =: \begin{bmatrix}
		\alpha F_1 & \gamma F_2 \\
		\beta F_1    &  \eta F_2
	\end{bmatrix}
	\begin{bmatrix}
		\overline{\alpha} F_1^\TT & \overline{\beta} F_1^\TT \\
		\overline{\gamma} F_2^\TT    &  \overline{\eta} F_2^\TT
	\end{bmatrix},
\end{align*}
where the second identity holds if
\[
\alpha\overline{\alpha} = \rr_1^2,\quad \gamma\overline{\gamma} =\rr_1\rr_2= \overline{\eta}\eta ,\quad
\beta\overline{\beta} = \rr_2^2
\]
and
\[
\alpha \overline{\beta} = \rr_1\rr_2 = -\gamma\overline{\eta}, \quad
\overline{\alpha} \beta = \rr_1\rr_2 = -\eta\overline{\gamma}.
\]
Since $\rr_1$ and $\rr_2$ are real numbers, it holds that
$\overline{\rr_1\rr_2} = \rr_1\rr_2$ and therefore the set of conditions on the scalar coefficients
$\alpha$, $\beta$, $\gamma$, $\eta$ in the complex domain can be simplified as
\begin{equation*}
	|\alpha| = |\rr_1|,\quad
	|\gamma|^2 =  \rr_1\rr_2,\quad
	|\eta|^2 =\rr_1\rr_2,\quad
	|\beta| = |\rr_2|
\end{equation*}
and
\[
\alpha \overline{\beta} = \rr_1\rr_2 = -\gamma\overline{\eta}.
\]
A solution $\{\alpha, \beta, \gamma, \eta\}$ to this set of equations obviously exists if $\rr_1\rr_2\ge 0$: we can simply take
\[
\alpha = \pm \rr_1,\quad
\beta = \pm \rr_2
\]
and
\[
\gamma = \pm \sqrt{\rr_1\rr_2},\quad
\eta = \mp \sqrt{\rr_1\rr_2}
\]
to obtain four different sets of real solutions. In other words, since $\rr_1+\rr_2=1$, we have shown that if $0\le \rr_1,\rr_2 \le 1$, then the intermediate factor $\mathscr{H}_2$ is positive semi-definite and hence the residual of the extrapolant
$\R(\rr_1X_1 + \rr_2X_2)$ is positive semi-definite.

Next, we show by contradiction the condition $0\le \rr_1,\rr_2 \le 1$ on the other hand is a necessary condition for the residual of the extrapolant to be positive semi-definite.
Now suppose $\R(\rr_1X_1 + \rr_2X_2)$ is positive semi-definite, so the intermediate factor $\mathscr{H}_2$ must be positive semi-definite. Suppose, however, the condition $0\le \rr_1,\rr_2 \le 1$ is not satisfied, then from the relation $\rr_1+\rr_2=1$ it holds that
$\rr_1\rr_2<0$, that is,
$\rr_1 <0, \rr_2>1$ with $|\rr_2|>|\rr_1|$
or $\rr_2 <0, \rr_1>1$ with $|\rr_1|>|\rr_2|$. This is to say,
we have $\rr_1\rr_2<0$ with either $|\rr_1\rr_2|>\rr_1^2$ or $|\rr_1\rr_2|>\rr_2^2$.

As a consequence of positive semi-definiteness of $\mathscr{H}_2$, its $(1,1)$-block (one of the leading principal submatrix)
$\rr_1^2F_1F_1^\TT+\rr_1\rr_2F_2F_2^\TT\equiv \rr_1^2T+\rr_1\rr_2\Y_2$ is positive semi-definite and also $\rr_2^2F_1F_1^\TT+\rr_1\rr_2F_2F_2^\TT\equiv\rr_2^2 T +\rr_1\rr_2\Y_2$ is positive semi-definite (just consider the positive semi-definiteness of the Schur complement).
Without loss of generality, suppose, from the above discussion,
$\rr_1\rr_2<0$ holds with $|\rr_1\rr_2|>\rr_1^2$. Then, we have
\begin{equation*}
	\rr_1^2T+\rr_1\rr_2\Y_2 + \rr_1^2\Y_2 - \rr_1^2\Y_2  \succcurlyeq 0 \quad \Longleftrightarrow \quad
	\rr_1^2(T - \Y_2 )  \succcurlyeq (|\rr_1\rr_2| - \rr_1^2)\Y_2,
\end{equation*}
where the latter Loewner order relation implies that
$\rr_1^2(T - \Y_2 )$ is positive semi-definite because
$ (|\rr_1\rr_2| - \rr_1^2)\Y_2$ is positive semi-definite.
On the other hand,
recall from the modified RADI iteration (Algorithm~\ref{alg:radi}) that, with $\sigma_k<0$, we have
$\Y_2 \succcurlyeq T$, which is equivalent to say $T-\Y_2$ is negative semi-definite; a contradiction!

Therefore, we have proved above that
$0\le \rr_1,\rr_2 \le 1$ is a sufficient and necessary condition for the residual of the extrapolant $\R(\rr_1X_1 + \rr_2X_2)$ to be positive semi-definite.
%\reply{I think the main difficulty is revealed in the proof of the previous theorem: for $n\ge3$
%there are the $C_{ik}$ terms (see~\eqref{eq:alpha-Cik}) that destroy the nice structure for $n=2$.}
\end{proof}
