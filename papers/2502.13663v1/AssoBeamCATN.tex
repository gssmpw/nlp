\documentclass[journal]{IEEEtran}
\IEEEoverridecommandlockouts
%*^13 % %
% 
% 
% 
% 
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
% \usepackage{ctex}%!!!!!!!!!!!!!
% \usepackage[top=0.7in, bottom=0.95in]{geometry}
\usepackage{cite}
% \usepackage{hyperref}  
\usepackage[hidelinks]{hyperref}
% \usepackage[square,sort,comma,numbers]{natbib}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic} 
% \usepackage{algorithmicx} 
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{CJK}
\usepackage{algorithm}
\usepackage{pifont}
\usepackage{amsthm}
% \newtheorem{remark}{Remark}
% \newtheorem{proposition}{\bf Proposition}
% \newtheorem{thm}{\bf Theorem}
% \usepackage{cuted}
% \usepackage{subfigure}
\usepackage{subfig}
\usepackage{float}  %
% \usepackage{lipsum}
% \usepackage{pdfpages}
% \usepackage{tabularx}
% \allowdisplaybreaks[4]
\usepackage{makecell}
\usepackage{booktabs} 

\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{psfrag}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{multirow}
\usepackage{stfloats}
% \usepackage{subcaption}
\usepackage{color}
\usepackage[normalem]{ulem}
\usepackage{arydshln}
\usepackage{enumerate}
\usepackage{bbm}
\usepackage{verbatim}



% \def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
%     T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}
% Multi-Cell Downlink Beamforming in Cognitive Network
% \title{Safe DRL-based Coordinated Beamforming in Cognitive aerial-Terrestrial Network\\Dynamic
\title{User Association and Coordinated Beamforming in Cognitive Aerial-Terrestrial Networks: \\A Safe Reinforcement Learning Approach\\
% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and should not be used}
% \thanks{Identify applicable funding agency here. If none, delete this.}
}
% distributed dynamic


\author{
% Zizhen Zhou, Qianqian Zhang, and Jungang Ge, \\
% Zizhen Zhou, Qianqian Zhang, Jungang Ge, and Ying-Chang Liang, \emph{Fellow, IEEE}, \\
Zizhen Zhou, Jungang Ge, and Ying-Chang Liang, \IEEEmembership{Fellow, IEEE}
% Zizhen Zhou,  \\
% \IEEEauthorblockA{
% University of Electronic Science and Technology of China (UESTC), Chengdu, P. R. China\\
% }
% \IEEEauthorblockA{Emails: zhouzizhen@std.uestc.edu.cn, qqzhang@uestc.edu.cn, and gejungang@std.uestc.edu.cn}
% \IEEEauthorblockA{Emails: zhouzizhen@std.uestc.edu.cn, qqzhang\_kite@163.com, gejungang@std.uestc.edu.cn, and liangyc@ieee.org}
% \IEEEauthorblockA{Emails: zhouzizhen@std.uestc.edu.cn}

% Zizhen Zhou, \IEEEmembership{Graduate Student Member, IEEE}, 
% Qianqian Zhang, \IEEEmembership{Member, IEEE},
% Jungang Ge, \IEEEmembership{Graduate Student Member, IEEE},
% Zizhen Zhou
% , Qianqian Zhang, Jungang Ge, 
% Ying-Chang Liang, \IEEEmembership{Fellow, IEEE}


\thanks{
Part of this work was presented in IEEE GLOBECOM 2024 \cite{zhou2024dynamic}.

Z. Zhou is with the National Key Laboratory of Wireless Communications, University of Electronic Science and Technology of China (UESTC), Chengdu 611731, China (e-mail: zhouzizhen@std.uestc.edu.cn). 

J. Ge is with the Department of Mobile Communications and Terminal Research, China Telecom Research Institute, Guangzhou 510000, China (e-mail: gejg1@chinatelecom.cn).

Y.-C. Liang is with the Center for Intelligent Networking and Communications (CINC), University of Electronic Science and Technology of China (UESTC), Chengdu 611731, China (e-mail: liangyc@ieee.org).
}
}
% \name{Anonymous\thanks{Anonymous.}}
% \address{Anonymous}
% Z. Zhou and Q. Zhang are with the National Key Laboratory of Wireless Communications, University of Electronic Science and Technology of China (UESTC), Chengdu 611731, China (e-mails: zhouzizhen@std.uestc.edu.cn and qqzhang\_kite@163.com). 

\maketitle




\begin{abstract}
Cognitive aerial-terrestrial networks (CATNs) facilitate the sharing of spectrum resources between aerial and terrestrial networks, presenting a promising solution to the spectrum scarcity challenges posed by thriving aerial networks.
However, aerial users (AUs), such as airplanes and flying cars, which demand high-quality downlink communication, experience significant interference from numerous terrestrial base stations (BSs).
To alleviate such interference, in this paper, we investigate a user association and coordinated beamforming (CBF) problem in CATN, where the aerial network serves as the primary network sharing its spectrum with the terrestrial network.
Specifically, we maximize the sum rate of the secondary terrestrial users (TUs) while satisfying the interference temperature constraints of the AUs.
Traditional iterative optimization schemes are impractical for solving this problem due to their high computational complexity and information exchange overhead.
Although deep reinforcement learning (DRL) based schemes can address these challenges, their performance is sensitive to the weights of the weighted penalty terms for violating constraints in the reward function.
Motivated by these issues, we propose a safe DRL-based user association and CBF scheme for CATN, eliminating the need for training multiple times to find the optimal penalty weight before actual deployment.
Specifically, the CATN is modeled as a networked constrained partially observable Markov game.
Each TU acts as an agent to choose its associated BS, and each BS acts as an agent to decide its beamforming vectors, aiming to maximize the reward while satisfying the safety constraints introduced by the interference constraints of the AUs.
By exploiting a safe DRL algorithm, the proposed scheme incurs lower deployment expenses than the penalty-based DRL schemes since only one training is required before actual deployment.
Simulation results show that the proposed scheme can achieve a higher sum rate of TUs than a two-stage optimization scheme while the average received interference power of the AUs is generally below the threshold.

% In each time slot, 

% Cognitive aerial-terrestrial networks (CATNs), which enable aerial and terrestrial networks to share spectrum resources, have been identified as a promising solution to address the spectrum scarcity challenges posed by thriving aerial networks.
% However, AUs such as airplanes and flying cars, which have high requirements for downlink communication quality, suffer severe interference from numerous terrestrial base stations (BSs).

% However, when receiving signals in the same frequency band as terrestrial users, AUs, such as airplanes and flying cars, suffer severe interference from numerous terrestrial base stations (BSs).
% However, during spectrum sharing, AUs, such as airplanes and flying cars, suffer severe interference from numerous terrestrial base stations (BSs).
% where the primary AUs share their spectrum with the terrestrial network.
% keeping the interference to the AUs below a pre-defined threshold.
% Since AUs usually move fast, obtaining real-time global channel state information is challenging.
% Also, frequently calculating the beamformers with high-complexity algorithms is unaffordable. 
% Since AUs usually move fast, it is challenging to obtain real-time global channel state information.
% Also, the complexity of frequently calculating the beamformers is unaffordable. 
% These issues make traditional iterative optimization algorithms impractical in solving this problem.
% To address these issues, deep reinforcement learning (DRL) based algorithms can be applied.
% , which incurs a large economic cost.
% indicating the importance of constraint violation in the reward function.
% that balances objective and constraint violation in the reward function.
% of a weighted penalty term for violating constraints in the reward function.
% Although deep reinforcement learning (DRL) based algorithms, which include a weighted penalty term for violating constraints in the reward function, can be applied to address these issues, their performance is sensitive to the penalty weight.
% distributed dynamic 
% , where each TU is a user association agent and each BS is a beamforming agent.
% the received interference power of the primary users.
% the tricky adjustment of the penalty weight.
% multiple training to find the optimal penalty weight before actual deployment is avoided.
% there is no need to train multiple times to find the optimal penalty weight before actual deployment.
% it avoids training multiple times to find the optimal penalty weight before actual deployment.
% It is achieved by a safe DRL algorithm, which has a lower deployment cost than the penalty-based DRL algorithms.
% A safe DRL algorithm is applied to the BS agents, which maximizes the reward while satisfying the safety constraints and has a lower deployment cost than the penalty-based DRL algorithms.
%  since the tricky adjustment of the penalty weights in the penalty-based DRL algorithms is avoided.
% with cost sets to decouple the objective and constraints.
% The CMDP is solved by a safe DRL algorithm, which maximizes the reward while satisfying the safety constraints.
%  received interference power constraints are generally well satisfied.
\end{abstract}
% Simulation results show that the proposed algorithm can achieve a high sum rate of terrestrial users while slightly violating received interference power constraints.







\begin{IEEEkeywords}
    User association, beamforming, cognitive spectrum sharing, aerial-terrestrial network, safe reinforcement learning
    % Aerial-Terrestrial Network
    % deep reinforcement learning, 
\end{IEEEkeywords}

\section{Introduction}
\label{sec_introduction}
% beamforming
% 
% DRL
% 

% Aerial networks are thriving since various aircraft can provide various services.
% Aerial networks are experiencing rapid growth due to the diverse services offered by various aircraft \cite{baltaci2021survey, jiang20236g}.
% Aerial networks include various manned and unmanned aircrafts, such as unmanned aerial vehicles (UAVs), airships, balloons, flying cars, airplanes, and so on \cite{baltaci2021survey}. 
Aerial networks are experiencing rapid growth, leveraging the flexible deployment advantages of various aircraft to facilitate a wide range of applications, including the Internet of Things (IoT), transportation, logistics, tourism, agriculture, healthcare, rescue, and disaster monitoring \cite{ jiang20236g}. 
Aircrafts, such as unmanned aerial vehicles (UAVs), airplanes, or flying cars, can act as the aerial users (AUs) communicating with base stations (BSs).
For example, air-to-ground (ATG) networks can provide in-flight communications to passengers by deploying dedicated BSs on the ground \cite{baltaci2021survey, mozaffari2021toward}.
% \cite{lin20215g, mozaffari2021toward}.
% With the emergence of new applications, such as aircraft passenger communications, the need for high transmission rates for A2G communications is increasing, where airplanes as AUs are served by dedicated BSs on the ground \cite{lin20215g, mozaffari2021toward}.
% The aerial network, which mainly includes a variety of aircrafts, such as unmanned aerial vehicles (UAV), airships, balloons, airplanes, and so on, is also thriving due to its advantage of flexible deployment \cite{zeng2019accessing}. 
% These aircrafts can be used as aerial base stations (BSs) to serve hot spot areas, as aerial relays to improve coverage, or as mobile users to communicate with the aerial BS \cite{zeng2019accessing}. 
% In SAGIN, since airplanes suffer from interference from numerous terrestrial BSs and A2G BSs also interfere with other networks when A2G communications use the same spectrum as the satellite and terrestrial networks, interference control is needed to ensure their performance \cite{tadayon2016inflight, lin2021sky}.
% 
% 
% Besides, electric vertical takeoff and landing (eVTOL) aircraft, which enable urban air mobility (UAM), have attracted a great deal of attention over the past few years \cite{pan2021flying, zaid2023evtol}.
Besides, flying cars, e.g., electric vertical takeoff and landing (eVTOL) aircrafts, enable urban air mobility and provide infotainment for passengers \cite{pan2021flying, zaid2023evtol}. 
% , have attracted rising interest
% In flying cars, passenger infotainment systems
%  over the past few years
% flying car transportation (FCT)
% In addition, drones can also be used to perform tasks related to video transmission or delivery
% Surveillance, Remote Inspection, 3D Mapping, Search and Rescue, Border patrolling
% However, t
These emerging services bring the increasing downlink communication needs, which pose challenges for effective spectrum utilization \cite{baltaci2021survey, zaid2023evtol}.
% 
% 
% To improve spectrum utilization, cognitive spectrum sharing is a promising solution, which allows the secondary network to use the spectrum of the primary network if the causing interference is tolerable.
% Nonetheless, when terrestrial cellular networks use the spectrum of the aerial network including AUs, such as airplanes and flying cars, these AUs suffer severe interference from a large number of terrestrial base stations (BSs) \cite{tadayon2016inflight, lin2021sky}.
To improve spectrum utilization efficiency, cognitive spectrum sharing offers a promising solution by enabling secondary networks to access the spectrum of the primary network if the received interference power of the primary user remains below a certain threshold, a.k.a., the interference temperature limit \cite{liang2011cognitive}.
% , provided that the interference caused is acceptable.
% terrestrial cellular networks can act as secondary networks to use the spectrum of aerial networks.
% Nonetheless, meeting this condition is challenging since the AUs, such as airplanes and flying cars, experience significant interference from numerous terrestrial BSs due to the line-of-sight (LoS)-dominant ground-air channels \cite{baltaci2021survey, tadayon2016inflight, zeng2018cellular, 3GPP38876}. 
% Nonetheless, in cognitive aerial-terrestrial networks (CATNs), when terrestrial cellular networks use the spectrum of aerial networks, the AUs, such as airplanes and flying cars, experience significant interference from numerous terrestrial BSs due to the line-of-sight (LoS)-dominant ground-air channels \cite{baltaci2021survey, tadayon2016inflight, zeng2018cellular, 3GPP38876}. 
Nonetheless, when terrestrial cellular networks and aerial networks use the same spectrum for transmission, the AUs face significant interference from numerous terrestrial BSs due to the line-of-sight (LoS)-dominant channels \cite{baltaci2021survey, tadayon2016inflight, zeng2018cellular, 3GPP38876}. 
% , such as airplanes and flying cars,
% the LoS-dominated BS-aerial user channels at high UAV altitude


% To improve the performance of airplanes in ATG networks, zero-forcing beamforming method is designed in \cite{dinc2021total}.
% In \cite{zeng2018cellular}, the three-dimensional (3D) beamforming-based downlink communication from the BS to AUs is studied.
% In \cite{mei2021aerial}, for cellular-connected unmanned aerial vehicles, interference mitigation solutions with BSs collaboration are studied.
% Besides, in \cite{mei2021aerial}, interference mitigation solutions for cellular-connected unmanned aerial vehicles (UAVs) is studied.
% exploit the powerful sensing capability of UAVs and inactive BSs in the network
% 



% ，（CBF），，，（FP）[12]（WMMSE）[13]。，（CSI），，，。，（UA），，[14]。（RSS）UA，RSS[15]、[16]。，，。，WMMSEUA，[14]。，WMMSE，。，UAWMMSE，[17][14]。，[18]，（SINR）。[19]，SINR。UA[14]、[17]–[19]。


% a.k.a., the interference temperature limit
% In CATN, to mitigate the interference received by primary AUs, 
% optimization-based
% Considering
To satisfy the interference temperature constraints of the AUs, BSs can perform coordinated beamforming (CBF), where the beamformers of the BSs are optimized by some iterative methods, e.g., the iterative weighted minimum mean square error (WMMSE) algorithm \cite{shi2011iteratively} and the fractional programming (FP) algorithms \cite{shen2018fractional1}.
% the fractional programming (FP) algorithms \cite{shen2018fractional1} and the iterative weighted minimum mean square error (WMMSE) algorithm \cite{shi2011iteratively}.
% In particular, in \cite{shen2018fractional1}, the closed-form FP algorithm and the iterative weighted minimum mean square error (WMMSE) algorithm \cite{shi2011iteratively} is proved to be equivalent.
% Besides, the direct FP algorithm can achieve a higher sum data rate than the WMMSE algorithm.
% Besides, the direct FP algorithm can outperform the WMMSE algorithm.
% Besides, the direct FP algorithm can achieve a higher performance than the WMMSE algorithm.
% Besides, the performance of the direct FP algorithm is higher performance than the WMMSE algorithm.
% \footnote{The equivalence between the WMMSE algorithm and the closed-form FP algorithm is also proved in \cite{shen2018fractional1}}.
% However, massive MIMO cellular networks pose challenges to traditional iterative optimization algorithms since they require real-time global CSI and have high computational complexity.
% especially when there are fast-moving AUs.
% them impractical for application in CATN that includes fast-moving AUs.
% massive multiple-input multiple-output (MIMO) cellular networks.
% 
% 
% 
% 
% It is necessary to consider the user association while beamforming,
% allow users to associate with the BS with a strong channel
To further mitigate interference, the user association (UA) is optimized to enable users to associate with the BS that has a strong channel, thereby preventing this channel from becoming a strong interference link \cite{sanjabi2014optimal}.
% Optimizing user association (UA) along with beamforming is necessary since a strong interference link can be transformed into a direct link by changing the user association, thus effectively alleviating the interference \cite{sanjabi2014optimal}.
% User association (UA) needs to be optimized along with beamforming since the strong interference link can be transformed into a direct link by changing the user association, thus effectively alleviating the interference \cite{sanjabi2014optimal}.
% In addition, user association (UA) is a widely studied topic in terrestrial networks.
% in terrestrial networks, a user association (UA) scheme with low communication overhead and high performance is important.
% 
% In the user equipment (UE)-driven user access control, each UE autonomously accesses an appropriate BS.
% only requires little information and 
% The received signal strength (RSS)-based method is a common user equipment (UE)-driven user access method, where the UE tends to access the BS with the strongest RSS \cite{mahmoud2010performance, wang2018handover}.
% A common user equipment (UE)-driven user access method is the received signal strength (RSS)-based method, where the UE tends to access the BS with the strongest RSS \cite{mahmoud2010performance, wang2018handover}.
% The received signal strength (RSS)-based UA method is common, 
A common UA method is based on received signal strength (RSS), where the user equipment (UE) tends to access the BS with the strongest RSS \cite{mahmoud2010performance, wang2018handover}.
Unfortunately, numerous UEs may attempt to access the same BS, leading to access congestion and reduced throughput. 
% However, the same BS may be visited by numerous UEs, resulting in access congestion and throughput degradation.
% a large number of severe
% on a BS
% In terrestrial wireless networks, multiple BSs are deployed in fixed locations.
% In the network-driven user access control, a UE is scheduled by the radio access networks (RANs) to access an appropriate BS.
% To improve the throughput of UEs and support seamless service, the BSs can coordinate with each other to collect information including access decisions of all UEs and the channel conditions between all BSs and all UEs.
% To improve the throughput of UEs,
% To deal with this issue, the BSs can collect information from other BSs including the channel conditions between other BSs and all UEs.
% To deal with this issue, the BSs can coordinate with other BSs to collect information including access decisions of all UEs and the channel conditions between all BSs and all UEs.
% and may need 
% , then disseminate such complex information to all UEs.
% To deal with this issue, information including the channel conditions between all BSs and all UEs can be collected for centralized decision-making \cite{sanjabi2014optimal}.
% This results in high information interaction overhead and high scheduling complexity.
% Although network-driven user access control schemes provide better performance in terrestrial networks, they also have high complexity.
% One possible solution to this problem is the UE-driven user access control (i.e., each UE autonomously accesses an appropriate BS), which is also widely studied.
% The received signal strength (RSS)-based method requires little information, where the UE tends to access the BS with the strongest RSS \cite{mahmoud2010performance, wang2018handover}.
% However, a large number of UEs may visit the same BS with the strongest RSS, resulting in severe access congestion on a BS and throughput degradation.
% Therefore, a UA scheme with low communication overhead, low computational complexity, and high performance is important.
% 
% information including the channel conditions between all BSs and UEs
To address this issue, the WMMSE algorithm is modified to jointly optimize UA and beamforming vectors by collecting the real-time global channel state information (CSI) for centralized decision-making \cite{sanjabi2014optimal}.
% To deal with this issue, joint UA and beamforming have been extensively studied \cite{sanjabi2014optimal, shen2014distributed, xie2017sinr, dong2017joint}.
% In \cite{sanjabi2014optimal}, the WMMSE algorithm is modified to jointly optimize UA and beamforming vectors, where information including the channel conditions between all BSs and all UEs is collected for centralized decision-making.
% To deal with this issue, the WMMSE algorithm is modified to jointly optimize UA and beamforming vectors, where information including the channel conditions between all BSs and all UEs is collected for centralized decision-making \cite{sanjabi2014optimal}.
% when multiple antennas are deployed at the BSs with multiple users spatially multiplexed using multiple-input and multiple-output (MIMO) beamforming techniques
% In \cite{sanjabi2014optimal}, the WMMSE algorithm is modified to jointly optimize UA and beamforming vectors.
% To deal with the high computational complexity and excessive handover induced by the modified WMMSE algorithm in \cite{sanjabi2014optimal}, the pricing-based UA is incorporated with WMMSE beamforming design in \cite{shen2014distributed}, which can achieve almost the same performance as that in \cite{sanjabi2014optimal}.
% Nonetheless, this modified WMMSE algorithm has high computational complexity and introduces excessive handover.
% This modified WMMSE algorithm, while effective, still suffers from high computational complexity and introduces excessive handover.
Nonetheless, this modified WMMSE algorithm still suffers from high computational complexity and introduces excessive handover.
% [19]WMMSE，WMMSE。
% To deal with the high computational complexity and excessive handover induced by the modified WMMSE algorithm in \cite{sanjabi2014optimal}
% To tackle this, pricing-based 
As a solution, by decoupling UA and beamforming, a two-stage joint UA and WMMSE beamforming algorithm is proposed in \cite{shen2014distributed}, achieving almost the same sum-rate performance as that in \cite{sanjabi2014optimal}.
% Nonetheless, this iterative optimization-based algorithm still has high complexity and requires real-time global CSI.
% To deal with this, a pricing-based two-stage joint UA and WMMSE beamforming algorithm is proposed in \cite{shen2014distributed}, which can achieve almost the same performance as that in \cite{sanjabi2014optimal}.
% Specifically, in \cite{shen2014distributed}, a two-stage joint UA and WMMSE beamforming algorithm is developed.
% In the first stage, to obtain the UA, a joint UA and power control algorithm based on a SISO representation of the MIMO channel is runned.
% This algorithm iteratively optimizes the UA via a dual coordinate descent (DCD) algorithm and the power control via Newton's method.
In addition, the minimum weighted signal-to-interference-plus-noise ratio (SINR) is maximized in \cite{xie2017sinr}.
In \cite{dong2017joint}, with only the statistical CSI, the ergodic sum rate of all UEs is maximized subject to the SINR constraint of each UE.
% 
% In addition, joint UA and beamforming is studied in integrated satellite-high-altitude-platform-station (HAPS)-ground networks \cite{liu2023joint} and HAPS-terrestrial network \cite{shamsabadi2024enhancing}, where the sum rate and the minimum SINR are maximized respectively.
% In multi-operator cellular networks, to realize the mutual benefits among operators, both UA and beamforming are optimized in \cite{he2023joint}.
% % To realize the mutual benefits among operators in multi-operator cellular networks, both UA and beamforming are optimized in \cite{he2023joint}.
% (except for the algorithm in \cite{dong2017joint})
However, these traditional iterative optimization algorithms require real-time global CSI (except for \cite{dong2017joint}) and exhibit high computational complexity, making them difficult to apply in practice, particularly with fast-moving AUs \cite{sanjabi2014optimal, shen2014distributed, xie2017sinr, dong2017joint}.


% As we can see, traditional optimization-based UA and beamforming schemes have difficulty achieving high performance with low computational complexity and information exchange overhead \cite{sanjabi2014optimal, shen2014distributed, xie2017sinr, dong2017joint}. 
Recently, deep reinforcement learning (DRL) has been utilized to solve UA problems \cite{wang2018handover, cao2020deep, zhao2019deep} and beamforming problems \cite{liu2022deep, ge2024deep}, demonstrating advantages in computational complexity and information acquisition.
% and has shown
%  and needs to know 
% 
% In order to achieve high throughput performance without the demand of complex information, DRL-based UE-driven user access control method is studied \cite{zhao2019deep, cao2020deep}.
In \cite{cao2020deep}, each UE acts as a deep Q-network (DQN) agent and only needs to know the number of access users of all BSs in the previous time slot, without requiring real-time global RSS information.
% makes its own access decision independently based on the parameters of the DQN trained at the centralized training node
In \cite{zhao2019deep}, a DRL-based joint UA and channel allocation scheme is proposed, where each UE acts as a dueling double DQN (D3QN) agent and only requires knowing whether the quality of service (QoS) requirements of other UEs are met.
% exchanges small amounts of information with other agents.
% 
% real-time global CSI acquisition
% \cite{mismar2019deep, liu2022deep, ge2024deep}.downlink
% Deep reinforcement learning (DRL) has been utilized to solve problems such as beamforming, user association, and resource allocation in cellular networks, and has shown advantages in terms of computational complexity and the amount of information required \cite{liu2022deep, ge2024deep, zhao2019deep}.
%  
% In \cite{mismar2019deep}, a DRL-based beamforming scheme is developed for cellular networks with one user per cell.
% In \cite{liu2022deep, ge2024deep}, dynamic CBF for cellular networks, where each BS serves multiple single-antenna users, is achieved by a DRL algorithm for continuous control, namely, deep deterministic policy gradient (DDPG).
% In \cite{liu2022deep, ge2024deep}, dynamic CBF for multi-cell networks is achieved by a DRL algorithm for continuous control, namely, deep deterministic policy gradient (DDPG).
% multiple-input multiple-output (MIMO)
% 
% 
% In \cite{liu2022deep}, the DRL framework utilizes the location of users instead of the high-cost instantaneous CSI.
% In \cite{liu2022deep}, to achieve dynamic CBF for the cellular network, deep deterministic policy gradient (DDPG), a DRL algorithm for continuous control, is applied.
% In \cite{liu2022deep}, to avoid the high cost of acquiring instantaneous CSI, the DRL framework utilizes the location of users instead of CSI.
% In \cite{liu2022deep}, to avoid the high cost of acquiring instantaneous CSI, only the location of users is required without the need for instantaneous CSI.
% to alleviate the problem of analyzing high-dimensional data, 
% Besides, a meta policy-based hierarchical DDPG is further proposed to alleviate the sparse reward problem.
% 
% 
% In \cite{ge2024deep}, DRL-based dynamic CBF for multi-cell networks is studied, where a known solution structure is exploited to reduce the dimension of the action space, which determines the beamforming vectors.
% In \cite{ge2024deep}, to reduce the dimension of the action space, which determines the beamforming vectors, a known solution structure is exploited.
% In \cite{ge2024deep}, the action space is designed with smaller dimensions by treating a known solution structure as expert knowledge.
% In \cite{ge2024deep}, to achieve distributed dynamic CBF (DDUACBF), DDPG is applied, where the action space is designed with smaller dimensions by treating a known solution structure as expert knowledge.
% In \cite{ge2024deep}, deep deterministic policy gradient (DDPG), a DRL algorithm for continuous control, is applied to achieve distributed dynamic coordinated beamforming (DDUACBF) scheme for the massive MIMO cellular network, where each BS serves multiple single-antenna users.
% In \cite{ge2024deep}, a massive multiple-input multiple-output (MIMO) cellular network, where each BS serves multiple single-antenna users, is studied. 
In \cite{ge2024deep}, a DRL-based distributed dynamic CBF scheme is proposed for the massive MIMO cellular network, where each BS acts as an agent to serve multiple single-antenna users in its cell without requiring real-time global CSI.
% In particular, a known solution structure is treated as expert knowledge and is utilized to design the action space with smaller dimensions.
% deep deterministic policy gradient (DDPG) 







% Despite a better performance provided by a network-driven user access control scheme in terrestrial networks, there are also considerable discussions on the development of UE-driven access control schemes to alleviate the scheduling complexity in terrestrial networks, which mainly focus on the received signal strength (RSS) based method \cite{mahmoud2010performance, wang2018handover}, and a UE tends to access the BS with the strongest RSS.
% However, such RSS based method may not be suitable.
% A large number of UEs may access the same NT-BS with the strongest RSS, which results in severe access congestion on a certain NT-BS to harm the throughput. 
% How to effectively balance the access loads among NT-BSs thus emerges as another critical issue in deploying NTNs.




% To the best of our knowledge, DRL has not been applied to solve CBF problem with interference power constraints in cognitive networks.
% In cognitive network, CBF problem with interference power constraints can also be solved by DRL algorithms, where the interference power constraints can be constructed as a penalty term and added to the reward function.
% In cognitive networks, the CBF problem with interference power constraints can also be solved by DRL algorithms
% Besides, DRL algorithms can also be utilized to solve the CBF problem in cognitive radio networks, where the reward function contains the weighted penalty term for the violation of the interference temperature constraints \cite{zhao2019deep}.
When facing constrained CBF problems in cognitive radio networks, DRL algorithms can also be utilized, where the reward function contains the weighted penalty terms for the violation of the interference temperature constraints \cite{zhao2019deep}.
However, the performance of these penalty-based DRL algorithms is sensitive to the penalty weight in some applications \cite{achiam2017constrained, li2019constrained}.
Specifically, when the penalty weight is large, the algorithm is too conservative to achieve a high reward. 
When the penalty weight is small, the algorithm is too bold to effectively satisfy the constraint.
Therefore, it is necessary to train multiple times to find the optimal penalty weight before actual deployment, which incurs large expenses.
% which incurs a large economic cost.
% li2019constrained xu2024learning wu2024real
% Furthermore, the agent may confuse the penalty term with the goal, making decisions that lack interpretability.
Besides, the reward function with a penalty term complicates the value function, which makes it difficult for the critic network to converge \cite{wu2024real}.
% 
% 
% 
% To cope with CMDP, solve deal with
% To overcome this difficulty, 
To ensure safety when deploying DRL in real-world applications, the constrained Markov decision process (CMDP) with cost function sets has been studied \cite{gu2024review}.
% , where the reward needs to be maximized while making safety constraints satisfied \cite{gu2024review}.gu2023safe
To solve the CMDP problems, safe DRL algorithms have been developed to maximize the expected cumulative discounted reward while satisfying safety constraints, namely, the expected cumulative discounted costs are lower than the corresponding cost limits \cite{gu2024review}.
Without finding the optimal penalty weights described above, the safe DRL-based approaches exhibit lower deployment expenses.
% a lower economic cost of deployment.
% Compared with the penalty-based DRL algorithms, safe DRL algorithms are more interpretable since the objective and constraints are decoupled.
% Compared with Markov decision process (MDP), the CMDP formulation is more interpretable since the objective and constraints are decoupled.
% In CMDP, we need to maximize the reward while making safety constraints satisfied.
% To ensure safety when deploying DRL in real-world applications, safe DRL has been studied to deal with the constrained Markov decision process (CMDP) problems \cite{gu2024review}.
% In CMDP, we need to maximize the reward while making safety constraints satisfied.
% , whose idea is to make the constraints satisfied by constructing cost functions \cite{gu2024review}.
% Safe reinforcement learning is often modeled as a Constrained Markov Decision Process (CMDP) [11], in which we need to maximize the agent reward while making agents satisfy safety constraints.











% in this paper, 
Motivated by the above considerations, we propose a safe DRL-based distributed dynamic UA and CBF (DDUACBF) framework for the cognitive aerial-terrestrial networks (CATNs).
In CATN, the aerial network acts as a primary network to share its spectrum with the secondary terrestrial network if the interference from the terrestrial network can be tolerated.
We formulate a problem to maximize the sum rate of the terrestrial users (TUs) by optimizing the user association between TUs and BSs and the beamforming vectors of the terrestrial BSs under the interference temperature constraints of the AUs, i.e., the primary users.
Then, we model this problem as a networked constrained partially observable Markov game (NCPOMG) \cite{zhang2021decentralized, feriani2021single, gu2023safe}, which contains two types of agents, i.e., each TU is a UA agent to decide which BS to associate with, and each BS is a beamforming agent to decide the beamforming vectors for its associated TUs.
To alleviate the difficulties caused by non-stationarity and partial observability, we design observation of agents by enabling information exchange between agents.
Besides, the dimension of the action space for BS agents is reduced by exploiting a known solution structure derived from the traditional optimization algorithms \cite{ge2024deep}.
% Besides, a known solution structure based on the traditional optimization algorithms is exploited to reduce the dimension of the action space.
% , which determines the beamforming vector, .
In particular, the shared cost functions are introduced for all BS agents, which are linear functions of the received interference power of the AUs.
% are transformed from the interference temperature constraints of AUs.
%  are transformed into  to decouple the objective and the interference power constraints.
% Then, we model this problem as a CMDP, and the interference power constraints are transformed into cost functions.
% Thus, the objective and the interference power constraints are decoupled.
Finally, we propose a safe DRL-based scheme to solve the NCPOMG, where a safe DRL algorithm is applied to the BS agents to maximize the reward while satisfying the safety constraints, i.e., the expected cumulative discounted received interference power of the AUs are below the thresholds.
The main contributions of the paper are summarized as follows:
\begin{itemize}
\item We study the user association and beamforming problem in the cognitive aerial-terrestrial network, where under the interference temperature constraints of the AUs, the beamforming vectors of the terrestrial BSs and the user association are optimized to maximize the sum rate of the TUs.
% the sum rate of the terrestrial users is maximized by optimizing the beamforming vectors of the terrestrial BSs and the user association.
\item We develop a multi-agent DRL-based user association and beamforming framework for CATN.
% , which has lower computational complexity than the iterative optimization-based algorithms and does not require real-time global CSI.
In particular, the studied problem is modeled as an NCPOMG, where each TU is a UA agent and each BS is a beamforming agent.
To the best of our knowledge, DRL-based user association and beamforming is studied in the non-joint transmission scenario for the first time.
%  where one BS can serve multiple users and one user can only be served by one BS.
% To the best of our knowledge, research on DRL-based UA and beamforming remains limited in the non-joint-transmission scenario
% \item User association and beamforming in cognitive networks is studied for the first time. 
% Existing papers study user association and power control in cognitive networks.
\item 
A safe DRL algorithm is applied to the BS agents, which maximizes the reward while satisfying the safety constraints related to the interference temperature constraints of the AUs.
% To maximize the reward while satisfying the safety constraints related to the interference temperature constraints of the AUs, a safe DRL algorithm is applied to the BS agents.
This approach significantly reduces deployment expenses since it avoids training multiple times to find the optimal penalty weight before the actual deployment, which is required by the penalty-based DRL method.
% the tricky adjustment of the penalty weights in the penalty-based DRL algorithms is avoided.
% The safe DRL algorithm avoids the tricky adjustment of the penalty weights in the penalty-based DRL algorithms and has a lower deployment cost.
% avoid the tricky adjustment of the penalty weight
% Safe reinforcement learning algorithms do not need to adjust penalty weights and have a lower deployment cost than the penalty-based DRL algorithms.
% \item The terrestrial BS has only the statistical knowledge of the channels from the terrestrial BS to the AUs, namely the channel power parameter and the array response vector.
\item Simulation results show that the proposed safe DRL-based scheme can achieve a higher sum rate of the TUs than a two-stage optimization scheme while the safety constraints are well satisfied.
% \item the WMMSE-based schemes
% \item  transformed from the received interference power constraints
Moreover, the proposed scheme exhibits lower computational complexity than the iterative optimization-based schemes and does not require real-time global CSI.
% Simulation results show that the proposed algorithm has lower computational complexity and communication overhead than the optimization-based algorithms.
% Moreover, the proposed algorithm can achieve a high sum rate of terrestrial users while slightly violating interference power constraints.

\end{itemize}



The rest of this paper is organized as follows:
Section \ref{secRelatedWork} presents the related work.
In Section \ref{secSystemModel}, the system model of CATN considered in this study is introduced.
In Section \ref{secProblem}, a user association and beamforming problem in CATN is formulated and a two-stage optimization-based scheme is introduced.
% the sum rate maximization problem.
% In Section \ref{secPreliminaries}, the preliminaries of DRL including the NCPOMG and safe reinforcement learning is introduced.
In Section \ref{secproposedScheme}, we first introduce the NCPOMG and safe reinforcement learning, and then propose a safe DRL-based DDUACBF framework for the CATN.
%  to solve the formulated problem.
% Section \ref{sec_Low_Com_Scheme} provides three low-complexity suboptimal schemes.
Section \ref{sec_Simulation_Results} shows the simulation results.
% , compares the performance of the PIBF scheme with that of the low-complexity schemes, and illustrates the advantages of HCSSA compared with TCSSA.
% validate the superiority of the PIBF scheme compared with the low-complexity schemes and the advantages of HCSSA compared with TCSSA.
% Finally, this paper is concluded in Section \ref{sec_Conclusions}.
Finally, Section \ref{sec_Conclusions} concludes this paper.
% Section IV develops an efficient algorithm to solve the formulated problem and 
% Section V provides two low-complexity suboptimal algorithms to solve it in two cases, respectively. 
% Section VI shows the simulation results to evaluate the performances of the proposed algorithms. 


% We formulate the cognitive spectrum sharing problem as an optimization problem to 
% maximize the sum rate of the terrestrial network 
% by optimizing the beamforming vector of the aerial network and the terrestrial network under 
% the constraint of the interference temperature of the satellite network, 
% the constraints of the transmit power of the base station (BS) of the aerial network and the terrestrial network and 
% the quality of service (QoS) constraint of the aerial network.

Notations used in this paper are listed as follows.
The lowercase, bold lowercase, and bold uppercase, i.e., $a$, ${\bf{a}}$, and ${\bf{A}}$ are scalar, vector, and matrix, respectively.
$\mathbb{C}^{a \times b}$ denotes the space of $a \times b$ complex-valued matrices.
${\bf{I}}$ denotes an identity matrix.
% ${\bf{I}}_a$ denotes an identity matrix of size $a \times a$.
$| \cdot |$ denotes the absolute value. 
% $\exp( \cdot )$ denotes the exponential function with base $e$. 
${{\left\| {\bf{a}} \right\|}_2}$ denotes the $\ell_2$ norm of vector ${\bf{a}}$.
$(\cdot )^T$ and $(\cdot )^H$ denote transpose and conjugate transpose, respectively.
% $(\cdot )^T$, $(\cdot )^H$, and ${\rm{Tr}}(\cdot )$ denote transpose, conjugate transpose, and trace, respectively.
% $(\cdot )^T$, $(\cdot )^H$, ${\rm{rank}}(\cdot )$, and ${\rm{Tr}}(\cdot )$ denote transpose, conjugate transpose, rank, and trace, respectively.
% ${\rm{diag}}(\cdot )$ denotes the block diagonal matrix.
% ${\rm{Pr}}\{\cdot\}$ denotes the probability that the inequality holds.
${\mathbb{E}}\{\cdot\}$ denotes the average operation.
${\mathcal {CN}} (\mu, \sigma^2)$ denotes the complex Gaussian distribution with mean $\mu$ and variance $\sigma^2$.
${\rm{clip}}(x,a,b)$ denotes a function that clips $x$ to the interval $[a,b]$.
${\bf{a}} \odot {\bf{b}}$ denotes the element-wise product, which means multiplying the corresponding elements of vector ${\bf{a}}$ and vector ${\bf{b}}$.




\section{Related Work}
\label{secRelatedWork}
% (3D)cellular-connected 
Interference management for the AUs, e.g., UAVs, has been extensively studied \cite{zeng2018cellular, mei2021aerial, mei2019cooperative}.
% To achieve interference mitigation, i
In \cite{zeng2018cellular}, the three-dimensional maximal-ratio transmission beamforming is adopted by BSs to serve UAVs and ground users on the shared channel.
% In \cite{mei2021aerial}, by exploiting the sensing ability of UAV and idle BSs in the network, aerial-ground interference mitigation solutions for both uplink and downlink are introduced.
The work in \cite{mei2021aerial} reviews traditional aerial-ground interference mitigation solutions, including inter-cell cooperation, dynamic frequency reuse, coordinated multipoint (CoMP), non-orthogonal multiple access (NOMA), beamforming, along with their respective drawbacks.
Then, solutions utilizing the sensing ability of UAVs and idle BSs in the network are proposed.
% Then, new solutions for both uplink and downlink are introduced by exploiting the sensing ability of UAV and idle BSs in the network.
In \cite{mei2019cooperative}, downlink cooperative beamforming with interference transmission and cancellation is proposed to mitigate the interference to the UAV caused by the co-channel terrestrial transmissions.
% maximize the UAV's receive signal-to-interference-plus-noise ratio (SINR) by jointly optimizing the power allocations at all of its serving BSs for transmitting the UAV's and co-channel terrestrial users' messages.
% with UAVs and BSs collaboration are studied.
% 。
% With UAVs and BSs collaboration, aerial-ground interference mitigation solutions are studied in \cite{mei2021aerial}.
% To mitigate this interference, 
% several optimization-based methods, e.g., 


% In the cellular-connected UAV scenario, the downlink interference management has been studied \cite{li2023radio, burhanuddin2023inter}.
% The DRL-based downlink interference management for the cellular-connected UAV has been studied \cite{li2023radio, burhanuddin2023inter}.
% In aerial-terrestrial networks where the cellular-connected UAVs and terrestrial UEs coexist, the DRL-based downlink interference management has been studied \cite{li2023radio, burhanuddin2023inter}.
The DRL-based downlink interference management in aerial-terrestrial networks has been explored, where the cellular-connected UAVs and terrestrial UEs coexist \cite{li2023radio, burhanuddin2023inter}.
% In \cite{li2023radio}, to minimize the ergodic outage duration of the UAV, resource block allocation and downlink beamforming for UAV are determined in large-scale and in small-scale respectively.
In \cite{li2023radio}, to minimize the ergodic outage duration of the UAV, resource block allocation and downlink beamforming for UAV are determined by the D3QN agent in large-scale and an agent in small-scale respectively.
% the twin delayed DDPG (TD3)
% Each UAV can be associated with more than one BS. The power of all BSs is considered to be a fixed constant.
% In the cellular-connected UAV scenario, to minimize the ergodic outage duration of the UAV, DRL-based resource block (RB) allocation and downlink beamforming are proposed, where D3QN is used to determine RB allocation and twin delayed DDPG (TD3) is used to determine beamforming \cite{li2023radio}.
In \cite{burhanuddin2023inter}, to maximize the throughput of terrestrial UEs under the transmission rate threshold for UAVs and terrestrial UEs, the number of muting cells and the slice time allocation are determined by a DQN agent.

% to achieve load balancing and interference management
% In this article, we focus on addressing the interference management in CATN through the multi-agent DRL (MADRL) based joint UA and beamforming, whose degraded version, 
The joint UA and power control based on multi-agent DRL (MADRL) has been extensively studied \cite{guo2020joint, naderializadeh2021resource, alwarafy2022hierarchical, yang2022distributed}.
To address the challenge of environmental non-stationary posed by independent agents in \cite{wang2018handover}, the handover and power allocation problem is modeled as a fully cooperative multi-agent task in \cite{guo2020joint}, which is solved by a multi-agent proximal policy optimization (PPO) algorithm based on the centralized training with decentralized execution (CTDE) framework where each UE acts as an agent.
In \cite{naderializadeh2021resource}, a MADRL framework with scalable observation and action spaces is developed, where each access point (AP) acts as an agent.
In \cite{alwarafy2022hierarchical}, a hierarchical MADRL-based framework is proposed, where an edge server agent determines the UA, while multiple AP agents manage power control.
In \cite{yang2022distributed}, device association, spectrum allocation, and power allocation in heterogeneous networks are optimized, where each BS acts as a D3QN agent.
%  by a distributed coordinated multi-agent framework


% bai2022distributed, bai2024distributed
% In the joint-transmission scenario, where one UT can be served by a set of APs and each AP can only serve one UT at the same time, a joint UA and beamforming scheme based on multi-agent DRL has been studied \cite{yu2023distributed}, where a two-timescale framework is proposed.
A joint UA and beamforming scheme based on MADRL is proposed in \cite{yu2023distributed}, which adopts a two-timescale framework, i.e., UA and beamforming are determined on large and small time scales respectively.
This scheme is tailored for the joint transmission scenario, where one UE can be served by multiple APs.
%  and each AP can only serve one UE at the same time.
% small cell 
Besides, MADRL-based beamforming for noncoherent joint transmission is studied, where multi-antenna BSs jointly serve single-antenna UEs \cite{bai2024distributed}.
However, these schemes can not be directly applied to the non-joint transmission scenario, where one UE can be served by only one BS.
%  and each BS can serve multiple UEs at the same time.
To the best of our knowledge, research on DRL-based UA and beamforming scheme remains limited in the non-joint transmission scenario.
% DRL-based user association and beamforming is still an open research problem.


% based on safe DRL
Safe DRL has been studied for various applications due to its ability to deal with decision problems with safety constraints \cite{li2019constrained, wu2024real, zhao2024safe, yu2024causal}.
In \cite{li2019constrained}, the safe DRL-based electric vehicles charging scheduling is studied, where the cost measures the deviation of battery energy from the charging target.
In \cite{wu2024real}, safe DRL is applied to the real-time optimal power flow problem.
In \cite{zhao2024safe}, safe DRL is applied to the UAV-aided task offloading with the energy consumption constraint of the UAV.
In \cite{yu2024causal}, safe DRL is exploited to maximize the sum rate under the average rate per user constraint in the UAV-enabled wireless network.
% In \cite{yu2024causal}, safe DRL is applied to the UAV-enabled wireless network to maximize the sum-rate under the average rate per user constraint.
Nonetheless, the application of safe DRL in wireless communication is still rare, and the aforementioned studies are limited to a single agent.
To this end, in this study, we design a safe DRL-based DDUACBF framework for the CATNs to maximize the sum rate of the TUs while ensuring the protection of the AUs.



\section{System Model}
\label{secSystemModel}
\begin{figure}[t]
\centering
\includegraphics[width=0.8\linewidth]{figureslong/scenario0918.eps}
%\vspace{-0.3cm}
\caption{Cognitive aerial-terrestrial network.}
% \caption{System Model.}
\label{scenario}
\vspace{-0.4cm}
\end{figure}

% As shown in Fig. \ref{scenario}, we consider a CATN, where the terrestrial network acts as a secondary network to share the spectrum of the aerial network.
% It is required that the interference from the terrestrial network can be tolerated by the AUs, i.e., the primary users (AUs).
As shown in Fig. \ref{scenario}, we consider a CATN, where the aerial network acts as a primary network to share its spectrum with the secondary terrestrial network if the interference from the terrestrial network can be tolerated by the AUs.
% , i.e., the primary users (AUs).
In the aerial network, there are $L$ AUs receiving downlink signals.
In the terrestrial network, there are $N$ BSs and $K$ single-antenna TUs.
Each TU can only receive downlink signals from one associated BS.
Each BS is equipped with a uniform rectangular array (URA) with $M = M_h \times  M_v$ antennas, where $M_h$ and $M_v$ are the number of horizontal and vertical antennas, respectively.
Besides, we denote the set of the indices of all AUs, the set of the indices of all BSs, and the set of the indices of all TUs by ${\mathcal U}^{\rm{pri}} \triangleq  \left\{ 1, \cdots ,L \right\}$, ${\mathcal B} \triangleq  \left\{ {1, \cdots ,N} \right\}$, and ${\mathcal U} \triangleq  \left\{ {1, \cdots ,K} \right\}$, respectively.
% Besides, we denote the set of the indices of all BSs, the set of the indices of all TUs, and the set of the indices of all AUs as ${\mathcal B} \!\triangleq \! \left\{ {1, \cdots \!,N} \right\}$, ${\mathcal U} \!\triangleq \! \left\{ {1, \cdots ,N} \right\} \!\times\! \left\{ {1, \cdots ,K} \right\}$, and ${\mathcal U}^{\rm{pri}} = \! \left\{ 1, \cdots ,L \right\}$, respectively.
% The BS serves the TUs in a time slot manner.
At the beginning of each time slot, each TU needs to decide which BS to associate with.
The BS associated with TU $k$ in time slot $t$ is denoted as $\varrho_k\left( t \right) \in {\cal B}$.
If handover occurs in time slot $t$, i.e., BS $\varrho_k\left( t \right)$ is different from BS $\varrho_k\left( t-1 \right)$, then TU $k$ should spend part of the time slot for handover overhead.
% can exploit only part of the time slot $t$ to receive the transmitted data from the associated BS.
% 
% , i.e., $\chi_{\varrho_k,k}=1$.
Accordingly, the set of indices of TUs associated with BS $n$ is denoted as ${{\cal K}_n}\left( t \right) = \left\{ {k|{\varrho _k}\left( t \right) = n} \right\}$.
% ${{\mathcal K}_n}=\{k|\chi_{n,k}=1\}$.
% We denote the user association variable as ${\chi _{n,k}} \in \left\{ {0,1} \right\},\forall n,k$.
% The binary variable ${\chi _{n,k}}=1$ if TU $k$ is associated with BS $n$.
% Since each user is served by a unique BS, we have $\sum_{n = 1}^N {{\chi _{n,k}}}  = 1,\forall k$.
% \begin{align}
% \sum_{n = 1}^N {{\chi _{n,k}}}  = 1,\forall k.
% \end{align}
In the following, we omit the time slot index for brevity if there is no misunderstanding.


\subsection{Transmission Model}
% The channels from BS $n$ to TU $k$ and AU $l$ are denoted by ${\bf{h}}_{n,k} \in \mathbb{C}^{M \times 1}$ and ${\bf{g}}_{n,l} \in \mathbb{C}^{M \times 1}$, respectively.
% The information signal for TU $k$ is denoted by $x_{n,k}$ and the corresponding beamforming vector is denoted by ${{\bf{w}}_k} \in \mathbb{C}^{M \times 1}$.

The received signal at TU $k$ can be expressed as:
\begin{align}
% \label{recsig_kth_user_in_nth_cell}
{y_k} = {\bf{h}}_{\varrho_k,k}^H{{\bf{w}}_k}{x_k} + \sum\limits_{i \in {\cal K},i \ne k} {{\bf{h}}_{{\varrho _i},k}^H{{\bf{w}}_i}{x_i}}  + {z_k}, \nonumber
% {y_{n,k}} = {\bf{h}}_{n,k}^H{{\bf{w}}_k}{x_k} + \sum_{j = 1}^N {\sum_{i \in {{\mathcal K}_j},\left( {j,i} \right) \ne \left( {n,k} \right)} {{\bf{h}}_{j,k}^H{{\bf{w}}_{j,i}}{x_i}} }  + {z_k}, \nonumber
% {y_{n,k}} = {\bf{h}}_{n,k}^H{{\bf{w}}_k}{x_k} + \sum_{(j,i) \ne (n,k)} {{\bf{h}}_{j,k}^H{{\bf{w}}_{j,i}}{x_i}} + {z_k}, \nonumber
% {y_{n,k}} = \underbrace {{\bf{h}}_{n,k}^H{{\bf{w}}_k}{x_k}}_{{\rm{intended}}\;{\rm{signal}}} + \underbrace {\sum_{(j,i) \ne (n,k)} {{\bf{h}}_{j,k}^H{{\bf{w}}_{j,i}}{x_i}} }_{{\rm{interference}}} + {z_k}, \nonumber
\end{align}
where $x_{k}$ is the information signal for TU $k$, ${{\bf{w}}_k} \in \mathbb{C}^{M \times 1}$ is the beamforming vector of BS ${\varrho _k}$ for TU $k$, ${\bf{h}}_{n,k} \in \mathbb{C}^{M \times 1}$ is the channel from BS $n$ to TU $k$, and $z_{k}$ is the additive white complex Gaussian noise that follows ${\mathcal C}{\mathcal N} (0, \sigma _{k}^2)$.
% circularly symmetric
% Note that ${\bf{w}}_{n,k}$ is always zero if user $k$ is not served by BS $n$, i.e., ${\bf{w}}_{n,k}={\bf{0}}$ if ${\chi _{n,k}}=0$.

Then, the signal-to-interference-plus-noise ratio (SINR) of TU $k$ for decoding $x_{k}$ can be expressed as: 
\vspace{-0.08cm}
\begin{align}
\label{SINR_kth_user}
{\gamma _k} = \frac{{{{\left| {{\bf{h}}_{\varrho_k,k}^H{{\bf{w}}_k}} \right|}^2}}}{{\sum\limits_{i \in {\cal K},i \ne k} {{{\left| {{\bf{h}}_{{\varrho _i},k}^H{{\bf{w}}_i}} \right|}^2}}  + \sigma _k^2}}.
% {\gamma _{n,k}} = \frac{{{{\left| {{\bf{h}}_{n,k}^H{{\bf{w}}_k}} \right|}^2}}}{{\sum_{j = 1}^N {\sum_{i \in {{\mathcal K}_j}, \left( {j,i} \right) \ne \left( {n,k} \right)} {{{\left| {{\bf{h}}_{j,k}^H{{\bf{w}}_{j,i}}} \right|}^2}} }}  + \sigma _k^2}.
% {\gamma _{n,k}} = \frac{{{{\left| {{\bf{h}}_{n,k}^H{{\bf{w}}_k}} \right|}^2}}}{{\sum_{(j,i) \ne (n,k)} {{{\left| {{\bf{h}}_{j,k}^H{{\bf{w}}_{j,i}}} \right|}^2}}  + \sigma _k^2}}.
% \vspace{-0.4cm}
% {\gamma _{n,k}} = \frac{{{{\left| {{\bf{h}}_{n,n,k}^H{{\bf{w}}_k}} \right|}^2}}}{{\sum_{(j,i) \ne (n,k)} {{{\left| {{\bf{h}}_{j,n,k}^H{{\bf{w}}_{j,i}}} \right|}^2}}  + \sigma _{n,k}^2}}.
\end{align}
Therefore, the achievable rate of TU $k$ can be expressed as ${R_k} = {\log _2}\left( {1 + {\gamma _k}} \right)$.
% Since ${R_{n,k}} = 0$ for $k \notin {{\mathcal K}_n}$, we denote $R_{k}=R_{n,k}$ with $k \in {{\mathcal K}_n}$ for brevity.


\subsection{Channel Model}
\label{secChannelModel}
% \begin{align}
% {{\bf{h}}_{n,k}}\left( t \right) = \sqrt {L_{{\rm{UMa}},n,k}^{ - 1}} {\bf{h}}_{n,k}^{{\rm{NLoS}}}\left( t \right),\nonumber
% \end{align}
% 
% Rural Macro (RMa)
The channels from BS $n$ to TU $k$ in time slot $t$ is modeled by the Rayleigh channel model, i.e., 
\begin{align}
    {{\bf{h}}_{n,k}}\left( t \right) = \sqrt {L_{{\rm{UMa}},n,k}^{ - 1}\left( t \right)} {\bf{h}}_{n,k}^{{\rm{NLoS}}}\left( t \right),\nonumber
\end{align}
where $L_{{\rm{UMa}},n,k}\left( t \right)$ is the path loss generated according to the Urban Macro (UMa) scenario of 3GPP TR 38.901 \cite{3GPP38901}. 
It is probabilistic LoS and is a function of the distance between BS $n$ and TU $k$ ${d_{n,k}}$ and the carrier frequency $f_c$.
% Specifically, $L_{{\rm{UMa}},n,k}$ is a function of ${d_{n,k}}$ and $f_c$, where ${d_{n,k}}$ is the distance between BS $n$ and TU $k$ and $f_c$ is the carrier frequency.
Besides, the non-line-of-sight (NLoS) component is given by \cite{kim2011impact}:
\begin{align}
{\bf{h}}_{n,k}^{{\rm{NLoS}}}\left( {t + 1} \right) = \alpha {\bf{h}}_{n,k}^{{\rm{NLoS}}}\left( t \right) + \sqrt {1 - {\alpha ^2}} {{\bf{e}}_{n,k}}\left( t \right),\nonumber
\end{align}
where $\alpha$ is the correlation coefficient of the Rayleigh fading vector between adjacent time slots, ${\bf{h}}_{n,k}^{{\rm{NLoS}}}\left( 0 \right)\sim {\mathcal C}{\mathcal N}\left( {0,{{\bf{I}}_M}} \right)$, and ${{\bf{e}}_{n,k}}\left( t \right)\sim {\mathcal C}{\mathcal N}\left( {0,{{\bf{I}}_M}} \right)$.
% Jakes' model



The channel from BS $\!n$ to AU $\!l$ in time slot $\!t$ is modeled by the Rician channel model, i.e., 
\begin{align}
% \label{steering_vector}
{{\bf{g}}_{n,l}}\left( t \right) \!= \!\!\sqrt {L_{{\rm{FSP}},n,l}^{ - 1} \!\left( t \right)} \!\left(\! {\sqrt {\!\frac{\kappa}{{\kappa \!+\! 1}}} {\bf{g}}_{n,l}^{{\rm{LoS}}}\!\left( t \right) + \!\sqrt {\!\frac{1}{{\kappa \!+\! 1}}} {\bf{g}}_{n,l}^{{\rm{NLoS}}}\!\left( t \right)} \!\right)\!,\nonumber
\end{align}
where $\kappa$ is the Rician factor and $L_{{\rm{FSP}},n,l}(t)$ is the path loss generated under the free space propagation model according to 3GPP TR 38.876 \cite{3GPP38876}.
$L_{{\rm{FSP}},n,l}(t)$ is a function of the distance between BS $n$ and AU $l$ $d_{n,l}(t)$ and $f_c$. 
% The channel from BS $n$ to AU $l$ in time slot $t$ is modeled \vspace{0.1cm}as ${{\bf{g}}_{n,l}}\left( t \right) = \!\sqrt {L_{{\rm{FSP}},n,l}^{ - 1} \!\left( t \right)} \left( {\sqrt {\frac{\kappa}{{\kappa + 1}}} {\bf{g}}_{n,l}^{{\rm{LoS}}}\!\left( t \right) + \!\sqrt {\frac{1}{{\kappa + 1}}} {\bf{g}}_{n,l}^{{\rm{NLoS}}}\!\left( t \right)} \right)$, where $\kappa$ is the Rician factor and $L_{{\rm{FSP}},n,l}(t)$ is the path loss generated under the free space propagation model according to 3GPP TR 38.876 \cite{3GPP38876} and it is a function of the distance between BS $n$ and AU $l$ $d_{n,l}(t)$ and $f_c$. 
% Specifically, $L_{{\rm{FSP}},n,l}(t)$ is a function of the distance between BS $n$ and AU $l$ $d_{n,l}(t)$ and $f_c$. 
% We denote ${d_{n,l}}$ as the distance between BS $n$ and AU $l$.
% The distance between BS $n$ and AU $l$ is denoted by ${d_{n,l}}$.
% 
% 
% line-of-sight (LoS)
Moreover, the LoS component is given by:
\begin{align}
{\bf{g}}_{n,l}^{{\rm{LoS}}}\left( t \right) = {e^{ - j2\pi d_{n,l}(t)/\lambda }}{\bf{a}}\left( {{\theta _{n,l}}\left( t \right),{\phi _{n,l}}\left( t \right)} \right),\nonumber
\end{align}
where $2\pi d_{n,l}(t) f_c/c$ is the phase of the LoS path for the reference antenna and $c$ is the speed of light.
% Moreover, the line-of-sight (LoS) component is given by ${\bf{g}}_{n,l}^{{\rm{LoS}}}\left( t \right) = {e^{ - j2\pi d_{n,l}(t)/\lambda }}{\bf{a}}\left( {{\theta _{n,l}}\left( t \right),{\phi _{n,l}}\left( t \right)} \right)$, where $2\pi d_{n,l}(t)/\lambda$ is the phase of the LoS path for the reference antenna, $\lambda  = c/{f_c}$ is the carrier wavelength, and $c$ is the speed of light.
% We denote the carrier wavelength by $\lambda  = c/{f_c}$, where $c$ is the speed of light. 
% located at $(0, 0, 0)$.
% 
% the azimuth and elevation angles of the UAV
Besides, ${\theta _{n,l}}\left( t \right)$ and ${\phi _{n,l}}\left( t \right)$ are the elevation and azimuth angles of AU $l$ relative to BS $n$, respectively, and ${\bf{a}}\left( {\theta ,\phi } \right)$ is the steering vector, which is given by:
\begin{align}
% \label{steering_vector}
{\bf{a}}\left( {\theta ,\phi } \right) = \frac{1}{{\sqrt M }}[&0, \ldots ,{e^{j\frac{{2\pi }}{\lambda }\Delta d\left( {\left( {{m_h} - 1} \right)\sin \theta \sin \phi  + \left( {{m_v} - 1} \right)\cos \theta } \right)}}, \nonumber\\
&\ldots ,{e^{j\frac{{2\pi }}{\lambda }\Delta d\left( {\left( {{M_h} - 1} \right)\sin \theta \sin \phi  + \left( {{M_v} - 1} \right)\cos \theta } \right)}}]^T,\nonumber
\end{align}
% ${\bf{a}}\left( {\theta ,\phi } \right) = \frac{1}{{\sqrt M }}[0, \ldots ,{e^{j\frac{{2\pi }}{\lambda }\Delta d\left( {\left( {{m_h} - 1} \right)\sin \theta \sin \phi  + \left( {{m_v} - 1} \right)\cos \theta } \right)}}, \ldots ,$ ${e^{j\frac{{2\pi }}{\lambda }\Delta d\left( {\left( {{M_h} - 1} \right)\sin \theta \sin \phi  + \left( {{M_v} - 1} \right)\cos \theta } \right)}}]^T$, 
where $\Delta d$ is the horizontal/vertical antenna spacing.
% Besides, ${\theta _{n,l}}\left( t \right)$ is the angle between the direction of AU $l$ relative to BS $n$ and the z-axis, ${\phi _{n,l}}\left( t \right)$ is the angle between the direction of AU $l$ relative to BS $n$ and the x-axis, and ${\bf{a}}\left( {\theta ,\phi } \right)$ is the steering vector, which is given by:
% 
Moreover, the NLoS component is given by: 
\begin{align}
{\bf{g}}_{n,l}^{{\rm{NLoS}}}\left( {t + 1} \right) = \alpha {\bf{g}}_{n,l}^{{\rm{NLoS}}}\left( t \right) + \sqrt {1 - {\alpha ^2}} {{\bf{e}}_{n,l}}\left( t \right),\nonumber
\end{align}
where ${\bf{g}}_{n,l}^{{\rm{NLoS}}}\left( 0 \right)\sim {\mathcal C}{\mathcal N}\left( {0,{{\bf{I}}_M}} \right)$ and ${{\bf{e}}_{n,l}}\left( t \right)\sim {\mathcal C}{\mathcal N}\left( {0,{{\bf{I}}_M}} \right)$.
% $\alpha$ is the correlation coefficient of the NLoS component vector between adjacent time slots,







% \begin{align}
% \label{steering_vector}
% {\bf{a}}\left( {\theta ,\phi } \right) = {1 \over {\sqrt {M} }} [ &0,\dots ,{e^{j\left( {{\psi _v}\left( {{m_v} - 1} \right) + {\psi _h}\left( {{m_h} - 1} \right)} \right)}},\nonumber\\
% &\dots ,{e^{j\left( {{\psi _v}\left( {{M_v} - 1} \right) + {\psi _h}\left( {{M_h} - 1} \right)} \right)}} ]^T,
% \end{align}
% where ${\psi _v} = {{2\pi } \over \lambda }{\Delta d}\cos \theta $, ${\psi _h} = {{2\pi } \over \lambda }{\Delta d}\sin \theta \sin \phi$, and $\Delta d$ is the horizontal/vertical antenna spacing.




\section{Problem Formulation and Optimization-based Scheme}
\label{secProblem}
\subsection{Problem Formulation}
% the channel coefficients vary across different time slots yet remain correlated over time
% As mentioned in Section \ref{secChannelModel}, we consider that the channel coefficients of each time slot are different, but correlated in time.
% Therefore, the BSs need to perform dynamic beamforming according to the changing channel.
% to obtain high rates
% As mentioned in Section \ref{secChannelModel}, we consider that the channel coefficients of each time slot are different, but remain correlated in time.
As mentioned in Section \ref{secChannelModel}, we consider that the channel coefficients vary across different time slots while maintaining temporal correlation.
To obtain a high sum rate under changing channel conditions, the TUs need to select an appropriate BS to associate with and the BSs need to dynamically adjust their beamforming strategies.
Specifically, by optimizing the user association between TUs and BSs and the transmit beamforming vectors of the BSs, we aim to maximize the sum rate of the terrestrial network subject to the interference temperature constraints of the AUs and the maximum transmit power constraints of the BSs.
The optimization problem can be formulated as:  
\begin{subequations}
\label{Problem_ori}
\begin{align}
\label{Problem_ori_obj}
&{\max _{\left\{ {{\varrho _k}\left( t \right),{{\bf{w}}_k}\left( t \right)} \right\}}}\;
\sum\limits_{k = 1}^K {{{\log }_2}\left( {1 + {\gamma _k}\left( t \right)} \right)}\\
% &\mathop {\max }_{\left\{ {{{\chi _{n,k}}\left( t \right)}, {{\bf{w}}_{n,k}{\left( t \right)}}} \right\}} \;
% \sum_{n = 1}^N {\sum_{k = 1}^K {{{\chi _{n,k}}\left( t \right)}{{\log }_2}\left( {1 + {\gamma _{n,k}{\left( t \right)}}} \right)} }   \\ 
\label{cons_power}
&\;\;\;\;\;\;{\rm{s.t.}}\;\;\;\;\;\;
\sum\limits_{k \in {{\cal K}_n}} {\left\| {{{\bf{w}}_k}\left( t \right)} \right\|_2^2}  \le {P_{\max }},\forall n \in {\cal B},\\
% \sum_{k = 1}^K {\left\| {{\bf{w}}_{n,k}{\left( t \right)}} \right\|_2^2}  \le {P_{\max}},\forall n \in {\mathcal B},\\
\label{cons_interfer}
&\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;
\sum\limits_{k = 1}^K {{{\left| {{\bf{g}}_{{\varrho _k}\left( t \right),l}^H\left( t \right){{\bf{w}}_k}\left( t \right)} \right|}^2}} \! \le {I_{\max }},\forall l \!\in {{\cal U}^{{\rm{pri}}}},\\
% \sum_{n = 1}^N {\sum_{k = 1}^K {{{\left| {{\bf{g}}_{n,l}^H{\left( t \right)} {\bf{w}}_{n,k}{\left( t \right)}} \right|}^2}} }  \le {I_{\max}},\forall l \in {\mathcal U}^{\rm{pri}}, \\ 
&\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;
{\varrho _k}\left( t \right) \in {\cal B},
% &\;\;\;\;\;\;\;\;\;\;\;\;\;\;\sum_{n = 1}^N {{\chi _{n,k}}\left( t \right)}  = 1,\forall k,\\ 
% &\;\;\;\;\;\;\;\;\;\;\;\;\;\;{\chi _{n,k}}\left( t \right) \in \left\{ {0,1} \right\},\forall n,k,
\end{align}
\end{subequations}
% = 1, \ldots ,N,
% =  1,\dots ,L, 
where ${P_{\max}}$ is the maximum transmit power of the BSs and ${I_{\max}}$ is the received interference power threshold of the AUs, i.e., the interference temperature limit.
% maximum tolerable interference power
% 
Note that the beamforming vector can be determined by two parts, i.e., ${{\bf{w}}_k} = \sqrt {p_k} {\overline {\bf{w}} _k}$, where ${\overline {\bf{w}} _k} = {{\bf{w}}_k}/{\left\| {{{\bf{w}}_k}} \right\|_2}$ is the normalized part of ${{\bf{w}}_k}$ and ${p_k} = \left\| {{{\bf{w}}_k}} \right\|_2^2$ is the power part of ${{\bf{w}}_k}$.


% \subsection{Known Solution Structure}
% \subsection{Iterative Optimization-based CBF Algorithm}Traditional
\subsection{Optimization-based DCD-WMMSE Scheme}
\label{OptimizationAlgorithm}
% However, this CFFP-based algorithm requires real-time global CSI and has high computational complexity.
% Hence, it is not suitable for massive MIMO cellular networks.
Problem \eqref{Problem_ori} is NP-hard, which makes it difficult to find its optimal solution.
To find the near-optimal solution, we develop a two-stage UA and CBF scheme based on iterative optimization, which is a slightly modified version of the algorithm proposed in \cite{shen2014distributed}.
% This algorithm iteratively optimizes the UA via a dual coordinate descent (DCD) algorithm in Algorithm \ref{alg_DCD} and the power control via Newton's method \cite{shen2014distributed}.
% In each iteration, the DCD algorithm requires utility information for all BSs associated with all TUs, which is calculated from the corresponding channel strength and transmit power.Specifically, in the first stage, to obtain the UA, a joint UA and power control algorithm based on a SISO representation of the MIMO channel is applied.
% The main idea of the DCD algorithm is that each user chooses to associate with the BS that maximizes its utility minus the price, while the BSs choose their prices in an iterative manner to balance their loads.
Specifically, in the first stage, the UA and the power control are iteratively optimized via the dual coordinate descent (DCD) algorithm in Algorithm \ref{alg_DCD} and the Newton's method, respectively \cite{shen2014distributed}.
The main idea of the DCD algorithm is that each user associates with a BS to maximize its utility minus the price, while the BSs choose their prices iteratively to balance their loads.
Note that the DCD algorithm requires the transmit power of all BSs $p_n, \forall n$ and the real-time global channel strength $\left\|{\bf{h}}_{n,k}\right\|^2, \forall n,k$.
Therefore, to obtain the user association variables $\varrho_k, \forall k$, this joint UA and power control algorithm incurs high information exchange overhead and computational complexity.

\begin{algorithm}[h] % htbp
\small
\caption{Dual coordinate descent (DCD) algorithm}
% Safe DRL-based Coordinated Beamforming for Cognitive Network
\label{alg_DCD}
\begin{algorithmic}[1]
\STATE \textbf{Input:} the transmit power of all BSs $q_n, \forall n$ and all the channel strength $\left\|{\bf{h}}_{n,k}\right\|^2, \forall n,k$.
\STATE Calculate the utility of the TU $k$ if it is associated with BS $n$ by ${\overline{u}}_{n,k}=\log(M\log_2(1+{\mathrm{SINR}}_{n,k}))$, $\forall n,k$, where ${\mathrm{SINR}}_{n,k}=\frac{|{\bf{h}}_{n,k}|^2 q_n}{\sum_{m\neq n}{\left|{\bf{h}}_{m,k}\right|^2 q_m}+\sigma_k^2}$.
\STATE Set the dual variable, i.e., the price at BS $n$ ${\overline{\mu}}_n=0, \forall n$ and the dual variable ${\overline{\nu}}  = \log \left( {\frac{1}{K}\sum_n {{e^{{{\overline{\mu}} _n} - 1}}} } \right)$.
\REPEAT
\FOR{$n \in \left\{ {1, \cdots ,N} \right\}$} 
\STATE Update ${\overline{\mu}} _n^{(t + 1)} = \sup \!\left\{ {{{\overline{\mu}} _n}|f_2^{(t)}\!\left( {{{\overline{\mu}} _n}} \right) - f_1^{(t)}\!\left( {{{\overline{\mu}} _n}} \right) \!\le\! 0} \right\}$, where ${f_2}\left( {{{\overline{\mu}} _n}} \right) = {e^{{{\overline{\mu}} _n} - {\overline{\nu}}  - 1}}$, ${f_1}\left( {{{\overline{\mu}} _n}} \right) = \left| {{{\mathcal U}_n}} \right|$, and ${{\mathcal U}_n} = \left\{ {k|{{\overline{u}}_{n,k}} - {{\overline{\mu}} _n} = \mathop {\max }_m \left( {{{\overline{u}}_{m,k}} - {{\overline{\mu}} _m}} \right)} \right\}$.
\ENDFOR
\STATE Update ${\overline{\nu}} {^{(t + 1)}} = \log \left( {\frac{1}{K}\sum_n {{e^{{\overline{\mu}} _n^{(t)} - 1}}} } \right)$.
\UNTIL the dual objective value $g\left( {{\overline{\boldsymbol{\mu}}} ,{\overline{\nu}} } \right)$ converges, where $g\left( {{\overline{\boldsymbol{\mu}}} ,{\overline{\nu}} } \right) = \sum_k {\mathop {\max }_n } \left( {{{\overline{u}}_{n,k}} - {{\overline{\mu}} _n}} \right) + \sum_n {{e^{{{\overline{\mu}} _n} - {\overline{\nu}}  - 1}}}  + {\overline{\nu}} K$.
\STATE Set ${\varrho _k} = {{\mathop{\rm argmax}\nolimits} _m}\left( {{{\bar u}_{m,k}} - {{\bar \mu }_m}} \right),\forall k$.
% $\chi_{n,k}=\begin{cases}1, n=\operatorname{argmax}_m({\overline{u}}_{m,k}-{\overline{\mu}}_m)\\0, n\neq\operatorname{argmax}_m({\overline{u}}_{m,k}-{\overline{\mu}}_m)\end{cases}, \forall n,k$.
\RETURN the user association variables $\varrho_k, \forall k$.
\end{algorithmic}
\end{algorithm}



% closed-form FP (CFFP) \cite{shen2018fractional1} (, which is proved to be equivalent to the WMMSE algorithm), which is shown in Algorithm \ref{alg_Closed_Form_FP_cog}.
In the second stage, the beamforming vectors are designed based on the idea of the WMMSE algorithm \cite{shi2011iteratively}, as detailed in Algorithm \ref{alg_Closed_Form_FP_cog}.
However, this iterative optimization-based algorithm requires real-time global CSI and exhibits high computational complexity, rendering it unsuitable for deployment in CATN, which includes massive cellular networks.

\begin{algorithm}[h] % htbp
\small
\caption{WMMSE-based CBF algorithm}
% Alternating Optimization
% \caption{Ellipsoid method to solve the transformed problem \eqref{Prob_Transformed}}
\label{alg_Closed_Form_FP_cog}
\begin{algorithmic}[1]
 %\REQUIRE ${\bf{Y}}$
\STATE Initialize ${{\bf{w}}_k}, \forall k$, such that $\sum_{k \in {{\mathcal K}_n}} \!{\left\| {{{\bf{w}}_k}} \right\|_2^2}  \!=\! {P_{\max }}, \forall n$.
\REPEAT 
% \STATE ${u_{n,k}} = {\left| {{\bf{h}}_{n,n,k}^H{{\bf{w}}_k}} \right|^2}/\left( {\sum_{(j,i) \ne (n,k)} {{{\left| {{\bf{h}}_{j,n,k}^H{{\bf{w}}_{j,i}}} \right|}^2}}  + \sigma _{n,k}^2} \right)$.
\STATE ${u_k} = \frac{{{{\left| {{\bf{h}}_{{\varrho _k},k}^H{{\bf{w}}_k}} \right|}^2}}}{{\sum\limits_{i \in {\cal K},i \ne k} {{{\left| {{\bf{h}}_{{\varrho _i},k}^H{{\bf{w}}_i}} \right|}^2}}  + \sigma _k^2}}$, $\forall k$.
% \STATE Update $u_{n,k}, \forall n,k$ by \eqref{LagrangianDualTransform_variables} with ${\bf{w}}_{n,k}, \forall n,k$.
\STATE ${v_k} = \frac{{\sqrt {(1 + {u_k})} {\bf{h}}_{{\varrho _k},k}^H{{\bf{w}}_k}}}{{\sum\limits_{i \in {\cal K}} {{{\left| {{\bf{h}}_{{\varrho _i},k}^H{{\bf{w}}_i}} \right|}^2}}  + \sigma _k^2}}$, $\forall k$ and ${\alpha _k} = {\left| {{v_k}} \right|^2}$.
% ${\alpha _{j,i}} = {\left| {{v_{j,i}}} \right|^2}$.
% \STATE Update $v_{n,k}, \forall n,k$ by \eqref{QuadraticTransform_variables} with $u_{n,k}, \forall n,k$ and ${\bf{w}}_{n,k}, \forall n,k$.
\STATE ${{\bf{w}}_k} = {\bf{D}}_{{\varrho _k}}^{ - 1}\sqrt {(1 + {u_k})} {{\bf{h}}_{{\varrho _k},k}}{v_k}$, $\forall k$, where ${{\bf{D}}_n} = \sum_{i = 1}^K {{\alpha _i}{{\bf{h}}_{n,i}}{\bf{h}}_{n,i}^H}   + \sum_{l = 1}^L {{\mu _l}{{\bf{g}}_{n,l}}{\bf{g}}_{n,l}^H}  + {\eta _n}{\bf{I}}$.
% where ${{\bf{D}}_n} \!= \!\sum_{j = 1}^N {\!\sum_{i \in {{\mathcal K}_j}} {\!{\alpha _{j,i}}{{\bf{h}}_{n,i}}{\bf{h}}_{n,i}^H} }   \!+ \!\sum_{l = 1}^L {{\mu _l}{{\bf{g}}_{n,l}}{\bf{g}}_{n,l}^H}  + {\eta _n}{\bf{I}}$.
% Update ${\bf{w}}_{n,k}, \forall n,k$ by \eqref{update_beamforming_variables}.
% with $v_{n,k}, \forall n,k$ and $u_{n,k}, \forall n,k$.
\UNTIL the objective function \eqref{Problem_ori_obj} converges.
\RETURN ${{\bf{w}}_k}, \forall k$.
\end{algorithmic}
\end{algorithm}


% Nevertheless, the structure of the solution derived from Algorithm \ref{alg_Closed_Form_FP_cog} can serve as valuable expert knowledge \cite{ge2024deep}, which helps solve problem \eqref{Problem_ori}.
% Specifically, from the fifth line of Algorithm \ref{alg_Closed_Form_FP_cog}, we can learn that the normalized beamforming vector can be expressed as ${\overline {\bf{w}} _k} = {\bf{D}}_{{\varrho _k}}^{ - 1}{{\bf{h}}_{{\varrho _k},k}}/{\left\| {{\bf{D}}_{{\varrho _k}}^{ - 1}{{\bf{h}}_{{\varrho _k},k}}} \right\|_2}$.
% % $u_{n,k}$'s and $v_{n,k}$'s are the introduced auxiliary variables.
% % By denoting ${\alpha _{j,i}} = {\left| {{v_{j,i}}} \right|^2}$, 
% The beamforming vector ${{\bf{w}} _k}$ can be determined by local CSI ${\bf{h}}_{{\varrho _k},i}, \forall i$, ${\bf{g}}_{{\varrho _k},l}, \forall l$, and some parameters, i.e., ${\alpha _i}$'s, ${\mu _l}$'s, $\eta _{\varrho _k}$, and $p_k$.
% Thus, this structure can greatly reduce the parameters required to determine the beamforming vectors in massive cellular networks \cite{ge2024deep}.
% % ${\alpha _{j,i}}$'s,

% % However, the structure of the solution of Algorithm \ref{alg_Closed_Form_FP_cog} can be used as expert knowledge, which helps solve problem \eqref{Problem_ori} \cite{ge2024deep}.
% Nevertheless, the structure of the solution derived from Algorithm \ref{alg_Closed_Form_FP_cog} can serve as valuable expert knowledge, which helps solve problem \eqref{Problem_ori} \cite{ge2024deep}.
% Specifically, $u_{n,k}$'s and $v_{n,k}$'s are the introduced auxiliary variables.
% From the last step of Algorithm \ref{alg_Closed_Form_FP_cog}, by denoting ${\alpha _{j,i}} = {\left| {{v_{j,i}}} \right|^2}$, we can learn that the normalized beamforming vector can be expressed as ${\overline {\bf{w}} _k} = {{\bf{D}}^{ - 1}}{{\bf{h}}_{n,n,k}}/\left\| {{{\bf{D}}^{ - 1}}{{\bf{h}}_{n,n,k}}} \right\|$.
% % , where ${\bf{D}} = \sum_{(j,i)} {{\alpha _{j,i}}{{\bf{h}}_{n,k}}{\bf{h}}_{n,k}^H} + \sum_{l = 1}^L {{\mu _l}{{\bf{g}}_{n,l}}{\bf{g}}_{n,l}^H} + {\eta _n}{\bf{I}}$.
% Thus, ${{\bf{w}} _{n,k}}$ can be determined by local CSI and some parameters, i.e., ${\alpha _{j,i}}$'s, ${\mu _l}$'s, $\eta _n$ and ${p_k}$.
% Using this structure can greatly reduce the parameters required to determine beamforming vectors in massive MIMO cellular networks \cite{ge2024deep}.


% where ${\boldsymbol{\mu}}  = {\left[ {{\mu _1},\dots ,{\mu _L}} \right]^T} \in {{\mathbb{R}}^{L \times 1}}$ and ${\boldsymbol{\eta }}  = \left[ \eta _1,\dots ,\eta _N \right]^T \in {{\mathbb{R}}^{N \times 1}}$ are the component-wise non-negative \textcolor{purple}{Lagrange multipliers}. 

% where ${\boldsymbol{\eta }}$ can be obtained by using a linear search to make the equality hold in \eqref{cons_power}.













% \begin{algorithm}[htbp]
% \caption{FP/SCA based Alternating Optimization to Problem \eqref{Prob_Transformed} }
% % \caption{Ellipsoid method to solve the transformed problem \eqref{Prob_Transformed}}
% \label{alg_Closed_Form_FP_cog_old}
% \begin{algorithmic}[1]
%  %\REQUIRE ${\bf{Y}}$
% \STATE Initialize ${\bf{w}}_{n,k}, \forall n,k$, ${\boldsymbol{\mu}}$.
% % and ${\boldsymbol{\eta}}$.
% % $t = 0$, 
% \REPEAT 
% \STATE Update $u_{n,k}, \forall n,k$ by \eqref{LagrangianDualTransform_variables} with ${\bf{w}}_{n,k}, \forall n,k$.
% \STATE Update $v_{n,k}, \forall n,k$ by \eqref{QuadraticTransform_variables} with $u_{n,k}, \forall n,k$ and ${\bf{w}}_{n,k}, \forall n,k$.
% % \STATE Update ${\boldsymbol{\eta}}$ by \eqref{update_lag_mul_power}.
% \textcolor{blue}{\STATE Update the Lipschitz constant $C_n, \forall n$ by \eqref{update_grad_Lipschitz} with $v_{n,k}, \forall n,k$, ${\boldsymbol{\mu}}$.
% \STATE Update the extrapolated point ${\widehat{\bf{w}}_{n,k}^{\rm{old}}}, \forall n,k$ with $C_n, \forall n$.
% \STATE Update ${\bf{f}}_{n,k}, \forall n,k$ by \eqref{update_lag_interfer_gradient} with $u_{n,k}, \forall n,k$, $v_{n,k}, \forall n,k$, ${\boldsymbol{\mu}}$, ${\widehat{\bf{w}}_{n,k}^{\rm{old}}}, \forall n,k$.
% % \STATE Update ${\boldsymbol{\eta}}$ by \eqref{update_lag_mul_power} with $C_n$, ${\bf{f}}_{n,k}, \forall n,k$, ${\widehat{\bf{w}}_{n,k}^{\rm{old}}}, \forall n,k$.
% \STATE Update ${\bf{w}}_{n,k}, \forall n,k$ by \eqref{update_beamforming_proxlinear} with $C_n$, ${\bf{f}}_{n,k}, \forall n,k$,  ${\widehat{\bf{w}}_{n,k}^{\rm{old}}}, \forall n,k$.}
% % \STATE Update ${\bf{w}}_{n,k}, \forall n,k$ by \eqref{update_beamforming_proxlinear} with ${\bf{f}}_{n,k}$, $v_{n,k}, \forall n,k$, ${\boldsymbol{\mu}}$.
% % \STATE Update ${\bf{w}}_{n,k}, \forall n,k$ by \eqref{update_beamforming_proxlinear} with $u_{n,k}, \forall n,k$, $v_{n,k}, \forall n,k$, ${\boldsymbol{\mu}}$, and ${\boldsymbol{\eta}}$.
% \STATE Update ${\boldsymbol{\mu}}$ by \eqref{update_lag_mul_interfer} with ${\bf{w}}_{n,k}, \forall n,k$.
% % \STATE $t=t+1$.
% \UNTIL objective function converges.
% \RETURN ${\bf{w}}_{n,k}, \forall n,k$.
% \end{algorithmic}
% \end{algorithm}





% \addtolength{\topmargin}{+0.05in} % The top margin

% \section{Safe DRL for Distributed Dynamic Coordinated Beamforming}
% \section{Preliminaries of DRL}
% \label{secPreliminaries}
\section{Safe DRL-based Distributed Dynamic User Association and Coordinated Beamforming in CATN}
\label{secproposedScheme}
% \cite{nair2005networked, zhang2021decentralized, feriani2021single, gu2023safe}
% \subsection{Safe DRL-based DDUACBF Scheme for CATN}
% First, we model the studied system as a CMDP.
% Then, we propose a safe DRL-based DDUACBF scheme for the CATN.
According to the above analysis, the real-time global CSI is still required for the traditional optimization-based algorithms to obtain the optimal UA and beamforming.
To alleviate the strict requirements of CSI, we develop a DDUACBF scheme, where each TU or BS makes decisions based on local information and information obtained from other TUs or BSs, rather than relying on complete knowledge of the wireless environment.
Considering the delay of information transmission, part of the obtained non-local information is only available in the next time slot.
To optimize the agent's decision-making in an unknown environment, DRL algorithms can be exploited.

% \cite{bernstein2002complexity}
% decentralized partially observable Markov decision process (DEC-POMDP)

% In this section, we will introduce the basics of DRL including the NCPOMG, safe reinforcement learning (RL), a safe DRL algorithm, and the D3QN algorithm, which will be further exploited to develop a safe DRL-based DDUACBF scheme in the next section.
In this section, we first introduce the NCPOMG and safe reinforcement learning (RL).
Then, we show that in CATN, the considered problem can be described as an NCPOMG.
Subsequently, we present the workflow of the proposed safe DRL-based DDUACBF framework, where each TU is an agent for UA and each BS is an agent for beamforming.
We then elaborate on the composition of the BS agents and the TU agents, including the algorithms they use.
Finally, we summarize the safe DRL-based DDUACBF scheme.










% partially observable Markov decision process (POMDP)
% networked distributed POMDP (ND-POMDP)
\subsection{Preliminaries of NCPOMG and Safe Multi-Agent RL}
% Safe reinforcement learning is often modeled as a CMDP, in which we need to maximize the agent reward while making agents satisfy safety constraints.
A NCPOMG model for $N$ agents can be described with a tuple $\left\langle {{\mathcal N},{\mathcal S},\Omega, {\mathcal A},{\mathcal P},{\rho ^0},{\mathcal R},{\mathcal C},{\mathcal B},{\gamma _r},{\gamma _c}, {\left\{ {{{\cal G}_t}} \right\}}_{t \ge 0}} \right\rangle$, whose elements are respectively defined as follows \cite{zhang2021decentralized, feriani2021single, gu2023safe}:
\begin{itemize}
\item ${\mathcal N} = \left\{ {1, \ldots ,N} \right\}$ is the set of agents.
\item ${\mathcal S}$ denotes the state space.
% , where ${\mathcal S}_n$ denotes the set of the local states (state space) of agent $n$, ${\mathcal S}_u$ refers to the set of the uncontrollable environment states that can not be affected by the actions of the agents, and $s = \left\langle {{s_1}, \cdots ,{s_N},{s_u}} \right\rangle  \in {\mathcal S}$ denotes the joint state.
\item $\Omega = \prod_{n = 1}^N {\Omega_n}$ denotes the joint observation space, where ${\Omega_n}$ is the set of observations for agent $n$ and ${\bf{o}} = \left\langle {{{\bf{o}}_1}, \cdots ,{{\bf{o}}_N}} \right\rangle  \in \Omega$ denotes the joint observation.
% (observation space) (action space)
\item ${\mathcal A} = \prod_{n = 1}^N {{\mathcal{A}}_n}$ denotes the joint action space, where $\mathcal{A}_n$ denotes the set of actions for agent $n$ and ${\bf{a}} = \left\langle {{{\bf{a}}_1}, \cdots ,{{\bf{a}}_N}} \right\rangle  \in {\mathcal A}$ denotes the joint action.
% \item $\mathcal{A}=\times_{1\leq n\leq N}\mathcal{A}_{n}$
\item $P\left( {{\bf{s}}^{\prime}|{\bf{s}},{\bf{a}}} \right) \in {\mathcal P}$ is the probabilistic  transition function.
% \item ${\mathcal{P}}\left(s'\mid s,a\right):{\mathcal{S}}\times{\mathcal{A}}\times{\mathcal{S}}\to[0,1]$
\item ${\rho ^0}$ is the initial state distribution.
\item ${\mathcal{R}}=\left\{r_n \right\}_{n\in {\mathcal{N}}}$ is the set of reward functions, where $r_n :{\mathcal S} \times {\mathcal A}_n \times {\mathcal S} \to \mathbb{R}$ is the reward function for agent $n$.
\item ${\mathcal{C}}=\left\{c_{n,l}\right\}_{n\in {\mathcal{N}}, 1\leq l\leq L_n}$ is set of cost functions, where $c_{n,l}:{\mathcal S} \times {\mathcal A}_n \times {\mathcal S} \to \mathbb{R}$ is the $l$-th cost function for agent $n$ and agent $n$ has $L_n$ cost functions.
% , \forall l=1, \dots, L_n
% , $c_{n,l}$ is the $l$-th cost of agent $n$, $C_{n,l}:{\mathcal{S}}\times{\mathcal{A}}_i\to\mathbb{R}$.
\item ${\mathcal{B}}=\left\{b_{n,l}\right\}_{n\in {\mathcal{N}}, 1\leq l\leq L_n}$ is the set of corresponding cost limits, where $b_{n,l}$ is the $l$-th cost limit for agent $n$.
\item $\gamma_r,\gamma_c\in(0,1)$ are the discount factor for reward and cost.
% , i.e., the maximum cost value of agents can be violated.
\item ${{\cal G}_t}{\rm{ = }}\left( {{\cal N},{{\cal E}_t}} \right)$ is the time-varying communication network at time $t$, which links $N$ nodes with edges in ${{\cal E}_t}$. An edge $(i, j) \in {{\cal E}_t}$, $\forall i, j$ means that agents $i$ and $j$ can communicate mutually at time $t$.
\end{itemize}

Denote the policy for agent $n$ as $\pi_n:{\Omega_n}\times{\mathcal{A}}_n\to[0,1]$, ${\boldsymbol{\pi}}=\left\{\pi_n\right\}_{n\in {\mathcal{N}}}$, the expected discounted cumulative reward for agent $n$ is ${J_R}\left( {{\pi _n}} \right) = {{\mathbb{E}}_{{\bf{s}} \left( 0 \right),{{\bf{a}}_n}\left( 0 \right),...}}\left[ {\sum_{t = 0}^\infty  {\gamma _r^t{r_n}\left( t \right)} } \right]$.
NCPOMG aims to find an optimal joint policy $\boldsymbol{\pi}^*$ that maximizes the average ${J_R}\left( {{\pi _n}} \right)$ of all agents ${J_R}\left( {\boldsymbol{\pi}} \right) \buildrel \Delta \over = \frac{1}{N}\sum_{n \in {\mathcal N}} {{J_R}\left( {{\pi _n}} \right)}$ while satisfying the safety constraints, NCPOMG can be formulated as:
\begin{align}
\label{NCPOMG}
{{\boldsymbol{\pi}} ^{*}}=\mathop {{\rm{argmax}}}_{\left\{ \pi_n  \in {\Pi _{{\mathcal C}, n}} \right\}_{n\in {\mathcal{N}}} } {J_R}\left( {\boldsymbol{\pi}} \right),
\end{align}
where ${\Pi _{{\mathcal C}, n}} \triangleq \left\{ \pi_n  \in \Pi_n :{J_{C,n,l}}\left( \pi_n  \right) \le {b_{n,l}}, \forall l = 1,...,L_n \right\}$ is the feasible policy set, ${J_{C,n,l}}\left( {{\pi _n}} \right) \buildrel \Delta \over = {{\mathbb{E}}_{{{\bf{s}}\left( 0 \right)},{{\bf{a}}_n\left( 0 \right)},...}}\left[ {\sum_{t = 0}^\infty  {\gamma _c^t{c_{n,l}}\left( t \right)} } \right]$ is the $l$-th expected discounted cumulative cost for agent $n$.
The inequalities ${J_{C,n,l}}\left( \pi_n  \right) \le {b_{n,l}}, \forall l$ in ${\Pi _{{\mathcal C}, n}}$ are the safety constraints for agent $n$.
Besides, ${\bf{s}}\left( 0 \right) \sim{\rho ^0}$, ${\bf{a}}_n\left( t \right) \sim{\pi _n}\left( { \cdot |{\bf{o}}_n\left( t \right)} \right)$, and ${\bf{s}}\left( t+1 \right)\sim P\left( { \cdot |{\bf{s}}\left( t \right),{{\bf{a}}\left( t \right)}} \right)$.
To find the optimal joint policy $\boldsymbol{\pi}^*$, safe multi-agent RL algorithms has been studied \cite{gu2023safe}.



% In multi-agent systems, to achieve a joint goal, good coordination of agents is needed to make the joint action optimize the performance of mutual tasks.
% To cope with these difficulties, the exchange of information between agents is an effective solution.

In multi-agent systems, to achieve a joint goal, effective coordination among agents is essential to make the joint action optimize the performance of mutual tasks.
However, partial observability and non-stationarity pose challenges. 
Specifically, a single agent can only observe partial information about the state of the environment. 
In addition, when multiple agents interact and learn simultaneously in a shared environment, the environment may be non-stationary from the perspective of any individual agent.
To address these challenges, facilitating information exchange among agents proves to be an effective solution.
Specifically, the networked agents in decentralized multi-agent RL are allowed to exchange information with their neighbors over a communication network \cite{zhang2021decentralized, feriani2021single}. 
Based on the collected information and the local observations, each agent makes its own decision without the coordination of a central controller.
% Specifically, the agents are connected by a communication network allowing the information exchange with each other \cite{zhang2021decentralized, feriani2021single}. 
% It is a decentralized setting, which means that each agent makes its own decision, based on only local observations and information collected from its neighbors, and without the coordination of a central controller.
% The setting is decentralized in the sense that each agent makes its own decision, based on only local observations and information transmitted from its neighbors, and without coordination by a central controller.


% decentralized MARL with networked agents. 
% In this scenario, multiple agents perform sequential decision-making in a common environment, and without the coordination of any central controller, while being allowed to exchange information with their neighbors over a communication network.


















% \subsection{Workflow of the Proposed Framework}
% networked constrained partially observable Markov game (NCPOMG)
\subsection{NCPOMG-based Framework and its Workflow}
\begin{figure*}[t]
\centering
\includegraphics[width=0.8\linewidth]{figureslong/workflow_DRL.eps}
%\vspace{-0.3cm}
\caption{Illustration for the workflow of the proposed safe DRL-based DDUACBF framework.}
% \caption{System Model.}
\label{workflow_DRL}
\vspace{-0.4cm}
\end{figure*}

% It can be seen from \eqref{SINR_kth_user} that the sum rate of TUs associated with a BS is affected by the beamforming decisions of other BSs.
% Since the decisions of each agent may have an impact on the information observed later by itself or other agents, we can model this process as a Markov game.
In CATN, the BSs and the TUs can act as agents that observe the wireless network environment and then make decisions based on the observed information to achieve goals such as maximizing the rate.
% It can be seen from \eqref{SINR_kth_user} that
As indicated in \eqref{SINR_kth_user}, the optimal beamforming decision that maximizes the sum rate of TUs associated with a BS is affected by the beamforming decision of other BSs.
Thus, the information observed by an agent at time $t+1$ should contain information about the decisions made by other agents at time $t$.
This process can be modeled as a Markov game.
Since each agent cannot observe complete information about the environment, the system can be modeled as a partially observable Markov game (POMG) \cite{feriani2021single}.
In particular, to maximize the sum rate in \eqref{SINR_kth_user}, it is not enough for the BS to rely solely on the local CSI (i.e., the CSI from it to all TUs and AUs) and it is necessary to acquire CSI from other BSs.
To mitigate the challenge of partial observability, the agents are considered to be networked, i.e., they can exchange information through a communication network \cite{zhang2021decentralized}.
Moreover, for BS agents, their beamforming decisions need to satisfy the safe constraints related to the received interference power of the AUs. 
To this end, after making a decision, each BS agent will receive a cost vector that indicates how safe the decision is \cite{gu2023safe}.
In summary, the considered UA and beamforming problem in CATN can be modeled as NCPOMG, which can be solved by advanced safe DRL techniques.


% The workflow of the proposed safe DRL-based DDUACBF framework is shown in Fig. \ref{workflow_DRL}.
Fig. \ref{workflow_DRL} shows the workflow of the proposed safe DRL-based DDUACBF framework, where all agents work in the decentralized training and decentralized execution (DTDE) way with information exchange by communication.
Specifically, each TU is a UA agent to decide which BS to associate with and each BS is a beamforming agent to decide the beamforming vectors for its associated TUs.
The composition of each agent is elaborated in the subsequent two subsections.
% The DTDE approach with networked agents is widely adopted in wireless communication applications \cite{feriani2021single, zhao2019deep, ge2024deep}.
% Compared with the super-single-agent approach, the DTDE approach can avoid the difficulty of finding the optimal policy in a large action space.
% Compared with the parameter sharing approach, the DTDE approach can train a specific policy for each agent, rather than ignoring the particularity of the optimal policy for different agents.



% Specifically, the workflow for each time slot is as follows.
% In time slot $t$, BS $n$ obtains the CSI to all TUs and the statistical CSI to all AUs, $\forall n$. TU $k$ measures the channel strength from all BSs, $\forall k$.
% TU $k$ obtains observation ${\bf{o}}_k^{TU}(t)$, $\forall k$.
% TU $k$ stores the experience $\left\langle {{{\bf{o}}_k^{TU}}(t-1),{\varrho_k}(t-1),{r_k^{TU}}(t-1),{{\bf{o}}_k^{TU}}(t)} \right\rangle$ into $\xi _k^{TU}$, $\forall k$.
% TU $k$ obtains action ${\varrho_k}(t)$ and associates with BS ${\varrho_k}(t)$, $\forall k$.
% BS $n$ exchanges information with other BSs, $\forall n$.
% BS $n$ obtains observation ${\bf{o}}_n(t)$, and stores the experience $\left\langle {{{\bf{o}}_n}(t-1),{{\bf{a}}_n}(t-1),{r_n}(t-1),{{\bf{c}}_n}(t-1), {{\bf{o}}_n}(t)} \right\rangle$ into $\xi _n^{BS}$, $\forall n$.
% BS $n$ obtains action ${\bf{a}}_n(t)$, $\forall n$.
% BS $n$ executes ${\bf{a}}_n(t)$, then receives reward $r_n(t)$ and cost ${{\bf{c}}_n}(t)$, $\forall n$.
% TU $k$ receives reward $r_k^{TU}(t)$, $\forall k$.




\subsection{BS Agent for Beamforming}
% CMDP of 
The adopted safe DRL algorithm, observation space, action space, reward function, and cost function of the BS agents are detailed as follows.


    
\subsubsection{Algorithm}
% \subsection{Preliminaries of a Safe DRL Algorithm: CUP}
% To design the CBF algorithm for the CATN, 
% To achieve DDUACBF in CATN, w
% We adopt one of the well-known safe DRL algorithms, namely, constrained update projection (CUP) \cite{yang2022constrained}.
We apply a safe DRL algorithm, constrained update projection (CUP) \cite{yang2022constrained}, in each BS agent.
The CUP algorithm, which is on-policy and model-free, is one of the policy optimization-based approaches \cite{gu2024review}.
Compared with the constrained policy optimization (CPO) algorithm \cite{achiam2017constrained}, which is the first model-free policy gradient method to solve the CMDP problem and uses second-order proximal optimization, the CUP algorithm utilizes only first-order optimization, resulting in lower computational complexity.
% only uses the first-order optimization. [325] is motivated by the optimization-based idea [221], where they use the primal-dual method, address policy search in the nonparametric space and project the policy into the parameter space to proximal maximization optimization in CMDPs.
Specifically, a CUP agent comprises three main parts: the actor, the reward critic, and the cost critic.
These three parts are actually the policy network $\pi_{\boldsymbol{\vartheta}}$, the value network for reward $V_{\boldsymbol{\varphi}}$, and the value network for costs $V^C_{\boldsymbol{\psi}}$ with parameters ${\boldsymbol{\vartheta}}$, ${\boldsymbol{\varphi}}$, and ${\boldsymbol{\psi}}$, respectively.
% These three parts are actually the policy network $\pi_{\boldsymbol{\vartheta}}$ with parameters ${\boldsymbol{\vartheta}}$, the value network for reward $V_{\boldsymbol{\varphi}}$ with parameters ${\boldsymbol{\varphi}}$, and the value network for costs $V^C_{\boldsymbol{\psi}}$ with parameters ${\boldsymbol{\psi}}$, respectively.
% $\pi$ 
% the policy network, i.e., the actor, which can be denoted by a function as $a = \pi(s|{\boldsymbol{\vartheta}})$ with ${\boldsymbol{\vartheta}}$ the parameters of the policy network. 
The policy for each agent is represented by ${\pi _{\boldsymbol{\vartheta}} }\left( {\bf{a}}|{\bf{o}} \right)$, which denotes the probability of choosing ${\bf{a}}$ in observation ${\bf{o}}$. 
Moreover, the observation ${\bf{o}}$ is evaluated by the value network for reward with the V value ${V_{\boldsymbol{\varphi}}}({\bf{o}})$ and the value network for costs with the V values ${V_{\boldsymbol{\psi }}}({\bf{o}})$.


% The detailed update process of CUP algorithm is shown in Algorithm \ref{alg_CUP}.
% , where the function ${\rm{clip}}(x,a,b)$ clips $x$ to the interval $[a,b]$.
% Based on the collected $B_m^{BS}$ experiences, the BS agent updates its policy with the CUP algorithm, whose detailed update process is shown in Algorithm \ref{alg_CUP}.
% At each training step, Mb experiences are sampled to form a minibatch M with e = ⟨se, ae, re, s′e⟩ denoting an experience in the mini-batch, 
% Specifically, the loss functions of the value network for reward and the value network for cost are given by:
Based on the collected $B_m^{BS}$ experiences, the BS agent updates its neural networks with the CUP algorithm, i.e., Algorithm \ref{alg_CUP}.
Every time the agent updates the networks, $B_b$ experiences are sampled to form a minibatch.
The parameters ${\boldsymbol{\varphi}}$ and ${\boldsymbol{\psi}}$ are updated by minimizing the mean square error (MSE) of the value network output and the target value.
Specifically, the loss functions of $V_{\boldsymbol{\varphi}}$ and $V^C_{\boldsymbol{\psi}}$ are given by:
\begin{align}
\label{value_network_reward_loss_function}
{{\mathcal L}_V}({\boldsymbol{\varphi}} ) &= \frac{1}{B_b}\sum_{j = 1}^{B_b} {{{\left( {{V_{\boldsymbol{\varphi}} }\left( {{{\bf{o}}_j}} \right) - V_j^{{\rm{target}}}} \right)}^2}},\\
\label{value_network_cost_loss_function}
{{\mathcal L}_{{V^C}}}({\boldsymbol{\psi}} ) &= \frac{1}{B_b}\sum_{j = 1}^{B_b} {{{\left( {{V^C_{\boldsymbol{\psi}} }\left( {{{\bf{o}}_j}} \right) - V_j^{C,{\rm{ target }}}} \right)}^2}}.
\end{align}
% 
% Specifically, the loss function of the value network for reward is:
% \begin{align}
% \label{value_network_reward_loss_function}
% {{\mathcal L}_V}({\boldsymbol{\varphi}} ) = \frac{1}{B_b}\sum_{j = 1}^{B_b} {{{\left( {{V_{\boldsymbol{\varphi}} }\left( {{{\bf{o}}_j}} \right) - V_j^{{\rm{target}}}} \right)}^2}}.
% \end{align}
% The loss function of the value network for cost is:
% \begin{align}
% \label{value_network_cost_loss_function}
% {{\mathcal L}_{{V^C}}}({\boldsymbol{\psi}} ) = \frac{1}{B_b}\sum_{j = 1}^{B_b} {{{\left( {{V_{\boldsymbol{\psi}} }\left( {{{\bf{o}}_j}} \right) - V_j^{C,{\rm{ target }}}} \right)}^2}}.
% \end{align}
% 
% Specifically, in the first step
% In the second step
The policy is updated in a two-step approach, which contains performance improvement and projection \cite{yang2022constrained}.
Firstly, CUP performs a policy improvement, which may produce a temporary policy that violates the constraint. 
Secondly, the CUP algorithm projects the policy back onto the safe region to reconcile the constraint violation.
The gradients to update the parameters of the policy network are provided in \eqref{CUP_update_policy_Improvement} and \eqref{CUP_update_policy_Projection} at the top of this page, where the KL divergence between ${{\pi _{\boldsymbol{\vartheta}} }}$ and ${{\pi _{{{\boldsymbol{\vartheta}} ^\prime }}}}$ is denoted by ${D_{{\rm{KL}}}}\left( {{\pi _{\boldsymbol{\vartheta}} },{\pi _{{{\boldsymbol{\vartheta}} ^\prime }}}} \right)\left[ {\bf{o}} \right]$, i.e., ${D_{{\rm{KL}}}}\left( {{\pi _{\boldsymbol{\vartheta}} }\left( { \cdot |{\bf{o}}} \right),{\pi _{{{\boldsymbol{\vartheta}} ^\prime }}}\left( { \cdot |{\bf{o}}} \right)} \right)$.
Besides, $\hat{A}_j$ and $\hat{A}_j^C$ are the advantage functions estimated by the generalized advantage estimator (GAE) with parameter $\lambda$ \cite{yang2022constrained},
${\boldsymbol{\nu}} = {\left[ {{\nu _1}, \cdots ,{\nu _L}} \right]^T}$ is a cost constraint parameter vector.
% , where $\tau$ is a temperature parameter that is fixed and makes the strategy more exploratory when it increases,
% The gradients to update the parameters of the policy network are given by \eqref{L_safe_FOCOPS}, which is shown at the top of the next page.
% In \eqref{L_safe_FOCOPS}, $\tau$ is a temperature parameter that is fixed and makes the strategy more exploratory when it increases.
% 
% $\hat{A}_j$ and $\hat{A}_j^C$ are estimates of the advantage functions obtained from $V_{\boldsymbol{\varphi}}$ and $V^C_{\boldsymbol{\psi}}$, respectively.
% Lagrange multipliers vector.
% Besides, the indicator function $\mathbf{1}_{D_{\rm{KL}}\left(\pi_{\boldsymbol{\vartheta}} , \pi_{{\boldsymbol{\vartheta}}^{\prime}}\right)\left[{\bf{o}}_j\right] \leq \varepsilon}$ can eliminate the sampled states whose KL divergence is too large.
% ${{\hat \nabla }_{\boldsymbol{\vartheta}} }{{\mathcal L}_\pi }({\boldsymbol{\vartheta}} )$
We give a rough understanding about the policy update: the policy improvement in \eqref{CUP_update_policy_Improvement} aims to maximize the advantage function for reward without updating too much, and then the policy projection in \eqref{CUP_update_policy_Projection} aims to minimize the weighted sum of the policy difference and the advantage function for cost with the weight ${\boldsymbol{\nu}}$.



\begin{algorithm}[t] % htbp
\small
\caption{Constrained update projection (CUP) algorithm}
% Safe DRL-based Coordinated Beamforming for Cognitive Network
\label{alg_CUP}
\begin{algorithmic}[1]
\STATE \textbf{Input:} experiences $\langle {{{\bf{o}}_{i}},{{\bf{a}}_{i}},{r_{i}},{{\bf{o}}_{i + 1}},{{\bf{c}}_{i}}} \rangle$, $i = 1, \ldots, B_m^{BS}$.
\STATE Estimate advantage functions ${{\hat A}_{i}}$ by ${{\hat A}_{i}} = \gamma \lambda {{\hat A}_{i + 1}} + {\delta _{i}}$, where $\hat A_{ B_m + 1} = 0$ and ${\delta _{i}} = {r_{i}} + \gamma {V_{\boldsymbol{\varphi}} }\left( {{{\bf{o}}_{i + 1}}} \right) - {V_{\boldsymbol{\varphi}} }\left( {{{\bf{o}}_{i}}} \right)$.
\STATE Estimate advantage functions $\hat A_{i}^C$ by $\hat A_{i}^C = \gamma \lambda \hat A_{i + 1}^C + \delta _{i}^C$, where $\hat A_{ B_m + 1}^C = 0$ and $\delta _{i}^C = {{\bf{c}}_{i}} + \gamma V^C_{\boldsymbol{\psi}} \left( {{{\bf{o}}_{i + 1}}} \right) - V^C_{\boldsymbol{\psi}} \left( {{{\bf{o}}_{i}}} \right)$.

\STATE Get $V_{i}^{\rm{ target }} = {{\hat A}_{i}} + V_{\boldsymbol{\varphi}} \left( {{{\bf{o}}_{i}}} \right)$ and $V_{i}^{C,{\rm{ target }}} = \hat A_{i}^C + V^C_{\boldsymbol{\psi}} \left( {{{\bf{o}}_{i}}} \right)$.
% \STATE Get $C$-return ${{\hat J}_C} =  {{\bf{c}}_{i}}$.
\STATE Estimate $C$-return ${{\hat J}_C}$ by ${{\hat J}_C} = \frac{1}{B_m^{BS}}\sum_{i = 1}^{B_m^{BS}}  {{\bf{c}}_{i}}$.

\STATE Update ${\boldsymbol{\nu}}$ by ${\boldsymbol{\nu}}  \leftarrow {\rm{clip}}\left( {{\boldsymbol{\nu}}  + {\alpha _{\boldsymbol{\nu}} }\left( {{\hat J}_C} - {\bf{b}} \right), 0, {{\boldsymbol{\nu}} _{\max }}} \right)$.

% \setcounter{ALG@line}{\value{ALG@rem}}
\STATE \textbf{Step 1: Performance Improvement}
% \setcounter{ALG@line}{\value{ALG@line}}
\STATE Store old policy ${\boldsymbol{\vartheta}}^{\prime} \leftarrow {\boldsymbol{\vartheta}}$.
\FOR{$B_e$ epochs}
\FOR{each minibatch $\langle {{{\bf{o}}_j},{{\bf{a}}_j},{{\hat A}_j},{\hat A}_j^C,V_j^{{\rm{target }}},V_j^{C,{\rm{ target }}}} \rangle$ of size $B_b$}
\STATE Update ${\boldsymbol{\varphi}}  \leftarrow {\boldsymbol{\varphi}}  - {\alpha _V}{\nabla _{\boldsymbol{\varphi}} }{{\mathcal L}_V}({\boldsymbol{\varphi}} )$ with \eqref{value_network_reward_loss_function}.
\STATE Update ${\boldsymbol{\psi}}  \leftarrow {\boldsymbol{\psi}}  - {\alpha _V}{\nabla _{\boldsymbol{\psi}} }{{\mathcal L}_{{V^C}}}({\boldsymbol{\psi}} )$ with \eqref{value_network_cost_loss_function}.
\STATE Update ${\boldsymbol{\vartheta}}  \leftarrow {\boldsymbol{\vartheta}}  - {\alpha _\pi }{{\hat \nabla }_{\boldsymbol{\vartheta}} }{{\mathcal L}_{\pi,  {\rm{improve}}} }({\boldsymbol{\vartheta}} )$ with \eqref{CUP_update_policy_Improvement}.
\ENDFOR
\IF{$\frac{1}{B_m^{BS}}  {\sum_{i = 1}^{B_m^{BS}} {{D_{{\rm{KL}}}}\left( {{\pi _{\boldsymbol{\vartheta}} }, {\pi _{{{\boldsymbol{\vartheta}} ^\prime }}}} \right)\left[ {{{\bf{o}}_{i}}} \right]} }  > \varepsilon$} 
\STATE Break.
\ENDIF 
\ENDFOR

\STATE \textbf{Step 2: Projection}
\STATE Store old policy ${\boldsymbol{\vartheta}}^{\prime\prime} \leftarrow {\boldsymbol{\vartheta}}$.
\FOR{$B_e$ epochs}
\FOR{each minibatch $\langle {{{\bf{o}}_j},{{\bf{a}}_j},{{\hat A}_j},{\hat A}_j^C,V_j^{{\rm{target }}},V_j^{C,{\rm{ target }}}} \rangle$ of size $B_b$}
\STATE Update ${\boldsymbol{\vartheta}}  \leftarrow {\boldsymbol{\vartheta}}  - {\alpha _\pi }{{\hat \nabla }_{\boldsymbol{\vartheta}} }{{\mathcal L}_{\pi,  {\rm{project}}} }({\boldsymbol{\vartheta}} )$ with \eqref{CUP_update_policy_Projection}.
\ENDFOR
\IF{$\frac{1}{B_m^{BS}}  {\sum_{i = 1}^{B_m^{BS}} {{D_{{\rm{KL}}}}\left( {{\pi _{\boldsymbol{\vartheta}} }, {\pi _{{{\boldsymbol{\vartheta}} ^\prime }}}} \right)\left[ {{{\bf{o}}_{i}}} \right]} }  > \varepsilon$} 
\STATE Break.
\ENDIF 
\ENDFOR

\RETURN Policy network $\pi_{\boldsymbol{\vartheta}}$ and Value networks $V_{\boldsymbol{\varphi}}$, $V^C_{\boldsymbol{\psi}}$.
% \RETURN Policy network $\pi_{\boldsymbol{\vartheta}}$, Value network for reward $V_{\boldsymbol{\varphi}}$, and for cost $V^C_{\boldsymbol{\psi}}$.
\end{algorithmic}
\end{algorithm}
    


% % \approx
% \begin{figure*}[ht]
% \begin{equation}
% \label{L_safe_FOCOPS}
% \hat{\nabla}_{\boldsymbol{\vartheta}} \mathcal{L}_\pi({\boldsymbol{\vartheta}}) = \frac{1}{B_b} \sum_{j=1}^{B_b}\left[\nabla_{\boldsymbol{\vartheta}} D_{\rm{KL}}\left(\pi_{\boldsymbol{\vartheta}} , \pi_{{\boldsymbol{\vartheta}}^{\prime}}\right)\left[{\bf{o}}_j\right]-\frac{1}{\tau} \frac{\nabla_{\boldsymbol{\vartheta}} \pi_{\boldsymbol{\vartheta}}\left({\bf{a}}_j \mid {\bf{o}}_j\right)}{\pi_{{\boldsymbol{\vartheta}}^{\prime}}\left({\bf{a}}_j \mid {\bf{o}}_j\right)}\left(\hat{A}_j-{\boldsymbol{\nu}}^T \hat{A}_j^C\right)\right] \mathbf{1}_{D_{\rm{KL}}\left(\pi_{\boldsymbol{\vartheta}} , \pi_{{\boldsymbol{\vartheta}}^{\prime}}\right)\left[{\bf{o}}_j\right] \leq \varepsilon}.
% \end{equation}
% \hrule
% \vspace{-0.4cm}
% \end{figure*}

\begin{figure*}[htbp]
% \begin{subequations}
% \label{CUP_update}
\begin{align}
\label{CUP_update_policy_Improvement}
{{\mathcal L}_{\pi,  {\rm{improve}}}}({\boldsymbol{\vartheta}} ) =& -\frac{1}{B_b}\sum_{j = 1}^{B_b} {\min \left\{ {\frac{{{\pi _{\boldsymbol{\vartheta}} }\left( {{\bf{a}}_j\mid {\bf{o}}_j} \right)}}{{{\pi _{{{\boldsymbol{\vartheta}} ^{\prime}}}}\left( {{\bf{a}}_j\mid {\bf{o}}_j} \right)}}{{\hat A}_j},{\rm{clip}}\left( {\frac{{{\pi _{\boldsymbol{\vartheta}} }\left( {{\bf{a}}_j\mid {\bf{o}}_j} \right)}}{{{\pi _{{{\boldsymbol{\vartheta}} ^{\prime}}}}\left( {{\bf{a}}_j\mid {\bf{o}}_j} \right)}},1 - \varepsilon ,1 + \varepsilon } \right){{\hat A}_j}} \right\}},\\
\label{CUP_update_policy_Projection}
{{\mathcal L}_{\pi,  {\rm{project}}}}({\boldsymbol{\vartheta}} ) =& \frac{1}{B_b}\sum_{j = 1}^{B_b} \left( \nabla_{\boldsymbol{\vartheta}} D_{\rm{KL}}\left(\pi_{\boldsymbol{\vartheta}} , \pi_{{\boldsymbol{\vartheta}}^{\prime \prime}}\right)\left[{\bf{o}}_j\right] +  {\boldsymbol{\nu}}^T \frac{{1 - \gamma \lambda }}{{1 - \gamma }}\frac{{{\pi _{{\boldsymbol{\vartheta}}}}\left( {{\bf{a}}_j\mid {{\bf{o}}_j}} \right)}}{{{\pi _{{{{\boldsymbol{\vartheta}}}^{\prime}}}}\left( {{\bf{a}}_j\mid {{\bf{o}}_j}} \right)}}\hat A_{j}^C \right).
\end{align}
% \end{subequations}
\hrule
\vspace{-0.4cm}
\end{figure*}











\subsubsection{Observation Space}
First, to reduce the number of elements of the state vector and the amount of information exchanged between BSs, we compress the channel ${\bf{h}}$ into a compressed channel ${{\bf{h}}^{\rm{c}}}$ by using a codebook ${\bf{F}} = [{{\bf{f}}_1}, \cdots ,{{\bf{f}}_C}] \in {\mathbb{C}}^{M \times C}$, where ${{\bf{f}}_c} \triangleq 1/\sqrt M \left[ {1,{e^{j2\pi c/C}}, \cdots ,{e^{j2\pi (M - 1)c/C}}} \right]$ and $C$ is the size of the codebook. 
Specifically, we calculate ${{\bf{F}}^H}{\bf{h}} = {\left[ {{d_1},{d_2}, \cdots ,{d_C}} \right]^T}$, whose elements are then sorted as $\left| {{d_{{c_1}}}} \right| \ge \left| {{d_{{c_2}}}} \right| \ge  \cdots  \ge \left| {{d_{{c_C}}}} \right|$.
Thus, given a compression factor $N_c$, we can obtain ${{\bf{h}}^{\rm{c}}} = \left[ {{c_1},{d_{{c_1}}},{c_2},{d_{{c_2}}}, \cdots ,{c_{{N_c}}},{d_{{c_{{N_c}}}}}} \right]$.
% Thus, we obtain ${{\bf{h}}^{\rm{c}}} = \left[ {{c_1},{d_{{c_1}}},{c_2},{d_{{c_2}}}, \cdots ,{c_{{N_c}}},{d_{{c_{{N_c}}}}}} \right]$, where $N_c$ is a compression factor.




Then, we define some notations for the desired signal and interference information.
% of TUs.
For TU $k$, the received desired signal power is $p_k^{\rm{r}} = {p_k}{\left| {{\bf{h}}_{{\varrho _k},k}^H {\overline {\bf{w}}_k}} \right|^2}$, the received interference power from BS $j$ is ${\beta _{j,k}} = \sum_{i \in {{\mathcal K}_j},i \ne k} {{p_i}{{\left| {{\bf{h}}_{j,k}^H{{\overline {\bf{w}}_i}}} \right|}^2}}$, and the received interference plus noise power is ${\beta _{k}} = \sum_{j = 1}^N {{\beta _{j,k}}}  + \sigma _{k}^2$.
Besides, if ${\varrho _k}\! \left( t \right) = n$, then the UA variable ${\chi _{n,k}}\! \left( t \right)=1$, otherwise ${\chi _{n,k}}\! \left( t \right)=0$.
In slot $t$, the local information of BS $n$ about the TUs in ${\mathcal{K}}_n\left( t \right)$ is:
% associated with it
\begin{align}
% \label{state_local}
% {\bf{o}}_n^{{\rm{loc}}}\left( t \right) = \left[ {{{\left[ {{\chi _{n,k}}\left( t \right),{\chi _{n,k}}\left( t \right){\bf{h}}_{n,k}^{\rm{c}}\left( t \right),{\chi _{n,k}}\left( {t - 1} \right){p_k}\left( {t - 1} \right),{\chi _{n,k}}\left( {t - 1} \right){R_{n,k}}\left( {t - 1} \right),{\chi _{n,k}}\left( {t - 1} \right){p_k}^{\rm{r}}\left( {t - 1} \right),{\chi _{n,k}}\left( {t - 1} \right){\beta _{n,k}}\left( {t - 1} \right)} \right]}_{k \in {{\mathcal K}_n}}}} \right],\nonumber
% {\bf{o}}_n^{{\rm{loc}}}\left( t \right) = \left[ {{{\left[ {{\bf{h}}_{n,k}^{\rm{c}}\left( t \right),{p_k}\left( {t - 1} \right),{R_{n,k}}\left( {t - 1} \right),{p_k}^{\rm{r}}\left( {t - 1} \right),{\beta _{n,k}}\left( {t - 1} \right)} \right]}_{k \in {{\mathcal K}_n}}}} \right],\nonumber
{\bf{o}}_n^{{\rm{loc}}}(t) = [ &{\boldsymbol{\chi}}_{n}^{\rm{BS}}(t), {\bf{H}}_n^{\rm{c}}(t),{{\bf{p}}_n}\left( {t - 1} \right),{{\bf{R}}_n}\left( {t - 1} \right),\nonumber\\
&{\bf{p}}_n^{\rm{r}}\left( {t - 1} \right),{{\boldsymbol{\beta}}_n}\left( {t - 1} \right) ],\nonumber
\end{align}
where ${\boldsymbol{\chi}}_{n}^{\rm{BS}}\triangleq[\chi_{n,1},\cdots,\chi_{n,K}]$, ${\bf{H}}_n^{\rm{c}} \triangleq {\boldsymbol{\chi}}_{n}^{\rm{BS}} \odot [ {\bf{h}}_{n,1}^{\rm{c}}, \cdots ,{\bf{h}}_{n,K}^{\rm{c}} ]$, ${{\bf{p}}_n} \triangleq {\boldsymbol{\chi}}_{n}^{\rm{BS}} \odot [ {p_1}, \cdots ,{p_K} ]$, ${\bf{R}}_n \triangleq {\boldsymbol{\chi}}_{n}^{\rm{BS}} \odot [ {R_1}, \cdots ,{R_K} ]$, ${\bf{p}}_n^{\rm{r}} \triangleq {\boldsymbol{\chi}}_{n}^{\rm{BS}} \odot [ p_1^{\rm{r}}, \cdots ,p_K^{\rm{r}} ]$, and ${{\boldsymbol{\beta }}_n} \triangleq {\boldsymbol{\chi}}_{n}^{\rm{BS}} \odot [ {\beta _{n,1}},\cdots ,{\beta _{n,K}} ]$.
Here the vector element-wise product is used to exclude non-local information.
% where ${\bf{H}}_n^{\rm{c}}(t) \triangleq [ {\bf{h}}_{n,n,1}^{\rm{c}}(t), \cdots ,{\bf{h}}_{n,n,K}^{\rm{c}}(t) ]$, ${\bf{p}}_n^{\rm{r}}(t - 1) \triangleq [ p_{n,1}^{\rm{r}}(t - 1), \cdots ,p_{n,K}^{\rm{r}}(t - 1) ]$, ${{\boldsymbol{\beta }}_n}(t - 1) \triangleq [ {\beta _{n,1}}(t - 1),\cdots ,{\beta _{n,K}}(t - 1) ]$, ${\bf{G}}_n^{\rm{c}}\left( {t - 1} \right) \triangleq [ {\bf{g}}_{n,1}^{\rm{c}}\left( {t - 1} \right), \cdots ,{\bf{g}}_{n,L}^{\rm{c}}\left( {t - 1} \right) ]$, ${{\boldsymbol{\rho}}_n}(t - 1) \triangleq [ {\rho _{n,1}}(t - 1), \cdots ,{\rho _{n,L}}(t - 1) ]$, ${{\bf{p}}_n}(t - 1) \triangleq [ {p_{n,1}}(t - 1), \cdots ,{p_{n,K}}(t - 1) ]$ is the power allocation in the previous time slot, and ${\bf{R}}_n(t - 1) \triangleq [ {R_{n,1}}(t - 1), \cdots ,{R_{n,K}}(t - 1) ]$ is the achievable rates in the previous time slot.


% The terrestrial BS is assumed to have only the statistical knowledge of the channels from the terrestrial BS to the AUs, namely the distance between the BS and the aerial user, the pathloss parameter, and the array response vector.
The statistical knowledge of the channels includes the distance between the transmitter and the receiver, the pathloss parameter, and the array response vector.
Thus, the statistical information of channel from BS $n$ to AU $l$ is denoted by ${\bf{g}}_{n,l}^S = \left[ {{\theta _{n,l}},{\phi _{n,l}},L_{{\rm{FSP}},n,l}^{ - 1},{d_{n,l}}} \right]$.
% \in {{\mathcal K}_n}
% Then, we define some notations for the desired signal and interference information of AUs.
For AU $l$, the inferred received interference power from BS $n$ is ${\rho _{n,l}} = \sum_{k\in{\mathcal{K}}_{n}}{p_k}\left|\sqrt{L_{{\rm{FSP}},n,l}^{-1}}\left({\bf{g}}_{n,l}^{LoS}\right)^{H}{\overline {\bf{w}}_k}\right|^{2}$ and the received interference power from all BSs is ${\rho _l} = \sum\limits_{k \in {\cal K}} {{p_k}{{\left| {{\bf{g}}_{{\varrho _k},l}^H{{\overline {\bf{w}} }_k}} \right|}^2}}$.
% For AU $l$, the received interference power from BS $n$ is ${\rho _{n,l}} = \sum_{k = 1}^K {{p_k}{{\left| {{\bf{g}}_{n,l}^H{\overline {\bf{w}}_k}} \right|}^2}}$ and the received interference power from all BSs is ${\rho _{l}} = \sum_{n = 1}^N {\rho _{n,l}}$.
In slot $t$, the local information of BS $n$ about AUs is:
\begin{align}
% \label{state_local}
{\bf{o}}_n^{{\rm{loc}},{\rm{pri}}}\left( t \right) = \left[ {\left[ {{\bf{g}}_{n,l}^S\left( t \right), {\rho _{n,l}}\left( {t - 1} \right)} \right]}_{l \in {{\mathcal U}^{{\rm{pri}}}}} \right].\nonumber
% {\bf{o}}_n^{{\rm{loc}},{\rm{pri}}}\left( t \right) = \left[ {{{\left[ {{\theta _{n,l}}\left( t \right),{\phi _{n,l}}\left( t \right),L_{{\rm{FSP}},n,l}^{ - 1}\left( t \right),{d_{n,l}}\left( t \right),{\rho _{n,l}}\left( {t - 1} \right)} \right]}_{l \in {{\mathcal U}^{{\rm{pri}}}}}}} \right],\nonumber
% {\bf{o}}_n^{{\rm{loc,pri}}}(t) = \left[ {{{\boldsymbol{\theta }}_n}(t),{{\boldsymbol{\phi }} _n}(t),{\bf{L}}_{LoS,n}^{ - 1}(t),{{\bf{d}}_n}(t),{ {\boldsymbol{\rho }}_n}(t - 1)} \right],\nonumber
\end{align}
% ${\bf{g}}_n^S = \left[ {{{\left[ {{\theta _{n,l}}\left( t \right),{\phi _{n,l}}\left( t \right),L_{{\rm{FSP}},n,l}^{ - 1}\left( t \right),{d_{n,l}}\left( t \right)} \right]}_{l \in {{\mathcal U}^{{\rm{pri}}}}}}} \right]$
% ${{\boldsymbol{\theta }}_n} \triangleq \left[ {{\theta _{n,1}}, \cdots ,{\theta _{n,L}}} \right]$, ${{\boldsymbol{\phi }} _n} \triangleq \left[ {{\phi _{n,1}}, \cdots ,{\phi _{n,L}}} \right]$, ${\bf{L}}_{LoS,n}^{ - 1} \triangleq \left[ {L_{LoS,n,1}^{ - 1}, \cdots ,L_{LoS,n,L}^{ - 1}} \right]$, ${{\bf{d}}_n} \triangleq \left[ {{d_{n,1}}, \cdots ,{d_{n,L}}} \right]$, and ${{\boldsymbol{\rho}}_n} \triangleq [ {\rho _{n,1}}, \cdots ,{\rho _{n,L}} ]$.
% \begin{align}
% % \label{state_local}
% {\bf{o}}_n^{{\rm{loc,pri}}}(t) = [{\bf{G}}_n^{\rm{c}}\left( t \right),{{\boldsymbol{\rho}}_n}(t - 1) ],\nonumber
% \end{align}
% where ${\bf{G}}_n^{\rm{c}} \triangleq [ {\bf{g}}_{n,1}^{\rm{c}}, \cdots ,{\bf{g}}_{n,L}^{\rm{c}} ]$, and ${{\boldsymbol{\rho}}_n} \triangleq [ {\rho _{n,1}}, \cdots ,{\rho _{n,L}} ]$.




To describe the information obtained from other BSs, we first define the set of the interferer BSs of TU $k$ as ${\mathcal B}_{k}^{{\rm{in}}}(t) = \left\{ j \in {\mathcal B}\mid {\beta _{j,k}}(t) > {\varsigma _{k}}(t) \right\}$, where ${\varsigma _{k}}(t)$ is a threshold that ensures $\left| {{\mathcal B}_{k}^{{\rm{in}}}(t)} \right| = B_{\rm{in}}$.
% ${\mathcal B} \triangleq \left\{ {1, \cdots ,N} \right\}$ denotes the index of all BSs and 
% 
% Then, we define the information from the interferer BSs of TU $k$ as:
Then, we define the information from ${\mathcal B}_{k}^{{\rm{in}}}(t - 1)$ as $\overline {\bf{o}}_{k}^{{\rm{in}}}(t) = \left[ {k, \left[ {j,{\bf{H}}_j^{\rm{c}}(t -\! 1),{{\bf{p}}_j}(t - \!1),{\beta _{j,k}}(t -\! 1)} \right]}_{j \in {\mathcal B}_{k}^{{\rm{in}}}(t - 1)} \right]$.
% \begin{align}
% {\bf{o}}_{k}^{{\rm{in}}}(t) \!= \!\!\left[ {\left[ {j,{\bf{H}}_j^{\rm{c}}(t -\! 1),{{\bf{p}}_j}(t - \!1),{\beta _{j,k}}(t -\! 1)} \right]}_{j \in {\mathcal B}_{k}^{{\rm{in}}}(t - 1)} \right].\nonumber
% \end{align}
Thus, the information from the interferer BSs of some TUs in ${\mathcal{K}}_n\left( t-1 \right)$ is:
% associated with it at time slot $t-1$ 
\begin{align}
{\bf{o}}_n^{{\rm{in}}}\left( t \right) = \left[ {{{\left[ {\overline {\bf{o}} _k^{{\rm{in}}}\left( t \right)} \right]}_{k \in {\mathcal U}_n^{{\rm{in}}}\left( {t - 1} \right)}}} \right],\nonumber
% {\bf{o}}_n^{{\rm{in}}}(t) = \left[ {{\bf{o}}_{n,1}^{{\rm{in}}}(t), \cdots ,{\bf{o}}_{n,K}^{{\rm{in}}}(t)} \right].\nonumber
\end{align}
where ${\mathcal{U}}_n^\text{in}(t)=\{k\in{\mathcal{K}}_n\left( t \right) \mid \beta_k(t)>\bar{\varsigma}_n(t)\}$ is the set of the TUs in ${\mathcal{K}}_n\left( t \right)$ who suffer severe interference and $\bar{\varsigma}_n(t)$ is a threshold that ensures $\left|{\mathcal{U}}_{n}^{\rm{in}}(t)\right|=\min\{K_{\rm{in}}, \left| {{{\mathcal K}_n}\left( t \right)} \right|\}$.
If $\left| {{{\mathcal K}_n}\left( {t - 1} \right)} \right| < {K_{{\rm{in}}}}$, the empty information in ${\bf{o}}_n^{{\rm{in}}}\left( t \right)$ is complemented with zero so that the dimension of ${\bf{o}}_n^{{\rm{in}}}\left( t \right)$ is $\left((1+3N_{c}K+K+1)B_{\rm{in}}+1\right){K_{{\rm{in}}}}$.



Similarly, we define the set of the interferer BSs of AU $\!l$ as ${\mathcal B}_l^{{\rm{in,pri}}}(t) = \left\{ {j \in {\mathcal B}\mid {\rho _{j,l}}(t) > {\widetilde{\varsigma} _l}(t)} \right\}$, where ${\widetilde{\varsigma} _l}(t)$ is a threshold that ensures $\left| {{\mathcal B}_l^{{\rm{in,pri}}}(t)} \right| = \widetilde B_{\rm{in}}$.
% 
% Then, we define the information from the interferer BSs of AU $l$ as:
Then, we define the information from ${\mathcal B}_l^{{\rm{in}},{\rm{pri}}}(t - 1)$ as ${\bf{o}}_l^{{\rm{in}},{\rm{pri}}}(t) = \left[ {{{\left[ {j,{\bf{g}}_{j,l}^S(t - 1),{{\bf{p}}_j}(t - 1),{\rho _{j,l}}(t - 1)} \right]}_{j \in {\mathcal B}_l^{{\rm{in}},{\rm{pri}}}(t - 1)}}} \right]$.
% , where ${\bf{g}}_j^S = \left[ {{{\boldsymbol{\theta }}_j},{{\boldsymbol{\phi }} _j},{\bf{L}}_{LoS,j}^{ - 1},{{\bf{d}}_j}} \right]$.
% ${\bf{o}}_l^{{\rm{in}},{\rm{pri}}}(t) = \left[ {\left[ {j,{{\boldsymbol{\theta }}_j}(t - 1),{{\boldsymbol{\phi }} _j}(t - 1),{\bf{L}}_{LoS,j}^{ - 1}(t - 1),{{\bf{d}}_j}(t - 1),{{\bf{p}}_j}(t - 1),{\rho _{j,l}}(t-1)} \right]}_{j \in {\mathcal B}_l^{{\rm{in}},{\rm{pri}}}(t - 1)} \right]$.
% \begin{align}
% % {\bf{o}}_l^{{\rm{in,pri}}}(t) = \left[ {{{\left[ {j,{\bf{H}}_j^{\rm{c}}(t - 1),{{\bf{p}}_j}(t - 1),{\rho _{j,l}}(t)} \right]}_{j \in {\mathcal B}_l^{{\rm{in,pri}}}(t - 1)}}} \right].\nonumber
% {\bf{o}}_l^{{\rm{in}},{\rm{pri}}}(t) \!= \!\!\left[ {\left[ {j,{\bf{G}}_{j}^{\rm{c}}(t - \!1),{{\bf{p}}_j}(t -\! 1),{\rho _{j,l}}(t-\!1)} \right]}_{j \in {\mathcal B}_l^{{\rm{in}},{\rm{pri}}}(t - 1)} \right].\nonumber
% \end{align}
Thus, the information from the interferer BSs of all AUs is:
\begin{align}
{{\bf{o}}^{{\rm{in}},{\rm{pri}}}}(t) = \left[ {{{\left[ {{\bf{o}}_l^{{\rm{in}},{\rm{pri}}}\left( t \right)} \right]}_{l \in {{\mathcal U}^{{\rm{pri}}}}}}} \right].\nonumber
% {{\bf{o}}^{{\rm{in,pri}}}}(t) = \left[ {{\bf{o}}_1^{{\rm{in,pri}}}(t), \cdots ,{\bf{o}}_L^{{\rm{in,pri}}}(t)} \right].\nonumber
\end{align}



Then, we define the set of the interfered TUs of BS $n$ as ${\mathcal U}_n^{{\rm{out }}}(t) = \left\{ i \in {\mathcal U}\mid {\beta _{n,i}}(t) > {\varsigma _n}(t) \right\}$, where ${\varsigma _n}(t)$ is a threshold that ensures $\left| {{\mathcal U}_n^{{\rm{out }}}(t)} \right| = K_{\rm{out}} B_{\rm{in}}$.
If $\sum_{k = 1}^K {{\beta _{n,k}}} =0$, then ${\mathcal U}_n^{{\rm{out}}}\left( t \right) = \left\{ {i \in {\mathcal U}\mid \left\| {{{\bf{h}}_{n,i}}\left( t \right)} \right\|_2^2 > {\varsigma _n}\left( t \right)} \right\}$.
% ${\mathcal U} \triangleq \left\{ {1, \cdots ,N} \right\} \times \left\{ {1, \cdots ,K} \right\}$ denotes the index of all TUs and 
% 
% ${\bf{o}}_n^{{\rm{out }}}(t) = \left[ {\left[ {(j - 1)K + i,{R_{j,i}}(t - 1),{\beta _{n,k}}(t - 1),   {\beta _{n,k}}(t - 1)/{\beta _{j,i}}(t - 1)} \right]}_{(j,i) \in {\mathcal U}_n^{{\rm{out }}}(t - 1)} \right]$
% Then, we define the information from the interfered TUs of cell $n$ as:
Then, we define the information from ${\mathcal U}_n^{{\rm{out}}}(t - 1)$ as:
\begin{align}
{\bf{o}}_n^{{\rm{out }}}(t) = \Big[ &{\left[ {i,{R_{i}}(t - 1),{\beta _{n,i}}(t - 1),} \right.} \nonumber\\
&\left. {\left. {{\beta _{n,i}}(t - 1)/{\beta _{i}}(t - 1)} \right]}_{i \in {\mathcal U}_n^{{\rm{out}}}(t - 1)} \right]. \nonumber
\end{align}
% where $R_{i}=R_{j,i}$ with $i \in {{\mathcal K}_j}$.



% Similarly, we define the set of the interfered AUs of cell $n$ as ${\mathcal J}_n^{{\rm{out }}}(t) = \left\{ {l \in \widetilde {\mathcal U}\mid {\rho _{n,l}}(t) > {{\widetilde \varsigma }_n}(t)} \right\}$, where $\widetilde {\mathcal U} \triangleq \left\{ {1, \cdots ,L} \right\}$ and $\left| {{\mathcal J}_n^{{\rm{out }}}(t)} \right| = L$.
% Then, we define the information from the interfered AUs as:
% \begin{align}
% {\bf{o}}_n^{{\rm{out,pri}}}(t) = \left[ {\left[ {l,{\rho _{n,l}}(t - 1),{\rho _{n,l}}(t - 1)/{\rho _l}(t - 1),} \right.} \right.\nonumber\\
% \left. {{{\left. {\max \left( {{\rho _l}(t - 1) - {I_{\max }}0} \right)} \right]}_{l \in {\mathcal J}_n^{{\rm{out }}}(t - 1)}}} \right], \nonumber
% \end{align}
% which can be obtained from the aerial BS.


% Similarly, we define the set of the interfered AUs of BS $n$ as ${\mathcal U}_n^{{\rm{pri}}}(t)$.
Similarly, given the set of the interfered AUs of BS $n$ is ${\mathcal U}^{\rm{pri}}$, we define the information from ${\mathcal U}^{\rm{pri}}$ as:
\begin{align}
{\bf{o}}_n^{{\rm{out}},{\rm{pri}}}(t) = \Big[& {\left[ {l,{\rho _l}(t - 1)/{I_{\max }},{\rho _{n,l}}(t - 1),} \right.} \nonumber\\
&{{{\left. {{\rho _{n,l}}(t - 1)/{\rho _l}(t - 1)} \right]}_{l \in {{\mathcal U}^{{\rm{pri}}}}}}} \Big], \nonumber
\end{align}
which can be obtained from the dedicated BSs serving the AUs in ${\mathcal U}^{\rm{pri}}$, such as ATG BS.
% Similarly, we define the set of the interfered AUs as ${\mathcal U}^{\rm{pri}} = \left\{ 1, \cdots ,L \right\}$.
% Then, we define the information from ${\mathcal U}^{\rm{pri}}$ as:
% {\bf{o}}_n^{{\rm{out,pri }}}(t) = \left[ {\left[ {l,{\rho _{n,l}}(t - 1),} \right.} \right.{\rho _{n,l}}(t - 1)/{\rho _l}(t - 1)\nonumber\\
% \left. {{{\left. {\max \left( {\rho _l}(t - 1) - {I_{\max }},0 \right)} \right]}_{l \in {\mathcal U}^{\rm{pri}}}}} \right], \nonumber





Therefore, in slot $t$, the observation of BS $n$ is:
\begin{align}
\label{State_Space}
{{\bf{o}}_n}(t) = \big[ &{\bf{o}}_n^{{\rm{loc }}}(t), {\bf{o}}_n^{{\rm{loc,pri}}}(t), {\bf{o}}_n^{{\rm{in}}}(t),\nonumber\\
&{{\bf{o}}^{{\rm{in,pri}}}}(t), {\bf{o}}_n^{{\rm{out }}}(t), {\bf{o}}_n^{{\rm{out,pri }}}(t) \big].
% {{\bf{o}}_n}(t) = \left[ {{\bf{o}}_n^{{\rm{loc }}}(t), {\bf{o}}_n^{{\rm{loc,pri}}}(t), {\bf{o}}_n^{{\rm{in}}}(t), {{\bf{o}}^{{\rm{in,pri}}}}(t), {\bf{o}}_n^{{\rm{out }}}(t), {\bf{o}}_n^{{\rm{out,pri }}}(t)} \right].
\end{align}

% In Table \ref{table_Communication_overhead}, we measure the inter-BS communication overhead of each part of the observation by the number of real scalar values that each BS needs to obtain from other BSs.
% \begin{table}[h]
% \centering  % 
% \caption{Inter-BS communication overhead}  % 
% \label{table_Communication_overhead}  % 
% %，|
% % l，c，r
% \begin{tabular}{cccc}  
% \toprule  % 
% Observation & Communication overhead \\  % ，&，\\
% \midrule  % 
% ${\bf{o}}_n^{{\rm{in}}}\left( t \right)$ & $(1+3N_{c}K+K+1)B_{\rm{in}}{K_{{\rm{in}}}}$ \\ 
% ${{\bf{o}}^{{\rm{in}},{\rm{pri}}}}(t)$ & $(1+4+K+1)\tilde{B}_{\mathrm{in}}L$ \\ 
% ${\bf{o}}_n^{{\rm{out }}}(t)$ & $3 K_{\rm{out}} B_{\rm{in}}$ \\ 
% ${\bf{o}}_n^{{\rm{out}},{\rm{pri}}}(t)$ & $2L$ \\ 
% \bottomrule  % 
% \end{tabular}
% \vspace{-0.2cm}
% \end{table}


\addtolength{\topmargin}{-0.05in} % The top margin

\subsubsection{Action Space}
The beamforming vector can be calculated as ${{\bf{w}}_k} = \sqrt {{p_k}} {\overline {\bf{w}} _k}$.
By leveraging the structure of the solution derived from Algorithm \ref{alg_Closed_Form_FP_cog}, the parameters required to determine the beamforming vectors in massive cellular networks can be greatly reduced \cite{ge2024deep}.
Specifically, from the fifth line of Algorithm \ref{alg_Closed_Form_FP_cog}, we can learn that the normalized beamforming vector can be expressed as ${\overline {\bf{w}} _k} = \widetilde {\bf{D}}_{{\varrho _k}}^{ - 1}{{\bf{h}}_{{\varrho _k},k}}/{\left\| {\widetilde {\bf{D}}_{{\varrho _k}}^{ - 1}{{\bf{h}}_{{\varrho _k},k}}} \right\|_2}$, where ${\tilde{\bf{D}}}_n={\sum_{i = 1}^K {{\alpha _{n,i}}{{\bf{h}}_{n,i}}{\bf{h}}_{n,i}^H}  + \sum_{l = 1}^L {{\mu _{n,l}}{{\bf{g}}_{n,l}}{\bf{g}}_{n,l}^H}  + {\eta _n}{\bf{I}}}$.
% $u_{n,k}$'s and $v_{n,k}$'s are the introduced auxiliary variables.
% By denoting ${\alpha _{j,i}} = {\left| {{v_{j,i}}} \right|^2}$, 
It can be determined by the local CSI ${\bf{h}}_{{\varrho _k},i}, \forall i$ and ${\bf{g}}_{{\varrho _k},l}, \forall l$, some weights for the interference leakage power from BS $\varrho _k$ to TU $i$ and AU $l$, i.e., ${\alpha _{{\varrho _k},i}}$'s and ${\mu _{{\varrho _k},l}}$'s, and a scaling factor for noise $\eta _{\varrho _k}$.
% ${\alpha _{j,i}}$'s,
% 
% 
% By leveraging the expert knowledge described in Section \ref{OptimizationAlgorithm}, 
% By leveraging the structure of the solution in Algorithm \ref{alg_Closed_Form_FP_cog}, 
% the statistical information of channel from BS $n$ to AU $l$ $\sqrt{L_{{\rm{FSP}},n,l}^{-1}}{\bf{g}}_{n,l}^{LoS}, \forall l$
% The beamforming vector can be calculated as ${{\bf{w}}_k} = \sqrt {{p_k}} {\overline {\bf{w}} _k}$, where the normalized part ${\overline {\bf{w}} _k}$ can be determined by local CSI ${\bf{h}}_{{\varrho _k},i}, \forall i$, ${\bf{g}}_{{\varrho _k},l}, \forall l$, and some parameters, i.e., ${\alpha _i}$'s, ${\mu _l}$'s, and $\eta _{\varrho _k}$ by leveraging the structure of the solution in Algorithm \ref{alg_Closed_Form_FP_cog}.
% By leveraging the structure of the solution in Algorithm \ref{alg_Closed_Form_FP_cog}, the normalized part ${\overline {\bf{w}} _k}$ can be determined by local CSI ${\bf{h}}_{{\varrho _k},i}, \forall i$, ${\bf{g}}_{{\varrho _k},l}, \forall l$, and some parameters, i.e., ${\alpha _i}$'s, ${\mu _l}$'s, and $\eta _{\varrho _k}$ .
Thus, we design the action space of BS $n$ as:
\begin{align}
\label{Action_Space}
{{\bf{a}}_n} = [& q_n^{{\rm{total}}},{q_{n,1}}, \cdots ,{q_{n,K}},{\alpha _{n,1}}, \cdots ,{\alpha _{n,K}},{\eta _n},\nonumber\\ 
&{\mu _{n,1}}, \cdots ,{\mu _{n,L}} ],
\end{align}
where $q_n^{{\rm{total }}} \in (0,1]$ represents the ratio of the total transmit power of BS $n$ to the maximum transmit power ${P_{\max }}$, 
${q_{n,k}} \in \left( {0,1} \right]$ represents the proportion of transmitted power that BS $n$ allocates to TU $k$, and $\sum_{k = 1}^K {{q_{n,k}}}  = 1$. 
Therefore, the power can be calculated as ${p_k} = {P_{\max }}q_n^{{\rm{total }}}{q_{n,k}}$.
% Besides, ${\alpha _{n,k}} > 0, \forall j,i$, ${\eta _n} > 0$, and ${\mu _{n,l}} > 0, \forall l$ are the parameters needed to determine ${\overline {\bf{w}} _k}$.
% Besides, ${\overline {\bf{w}} _k} = \widetilde {\bf{D}}_{{\varrho _k}}^{ - 1}{{\bf{h}}_{{\varrho _k},k}}/{\left\| {\widetilde {\bf{D}}_{{\varrho _k}}^{ - 1}{{\bf{h}}_{{\varrho _k},k}}} \right\|_2}$, where ${\tilde{\bf{D}}}_n={\sum_{i = 1}^K {{\alpha _{n,i}}{{\bf{h}}_{n,i}}{\bf{h}}_{n,i}^H}  + \sum_{l = 1}^L {{\mu _{n,l}}{{\bf{g}}_{n,l}}{\bf{g}}_{n,l}^H}  + {\eta _n}{\bf{I}}}$.
% ${\tilde{\bf{D}}}_n={\sum_{i = 1}^K {{\alpha _{n,i}}{{\bf{h}}_{n,i}}{\bf{h}}_{n,i}^H}  + \sum_{l = 1}^L {{\mu _l} L_{{\rm{FSP}},n,l}^{ - 1} {\bf{g}}_{n,l}^{LoS} \left( {{\bf{g}}_{n,l}^{LoS}} \right)^H}  + {\eta _n}{\bf{I}}}$.
% ${\overline {\bf{w}} _k}$ is given by:
% \begin{align}
% % \label{Action_Space}
% {\overline {\bf{w}} _k} = \frac{{{{\left( {\sum_{i = 1}^K {{\alpha _{n,i}}{{\bf{h}}_{n,i}}{\bf{h}}_{n,i}^H}  + \sum_{l = 1}^L {{\mu _l}{{\bf{g}}_{n,l}}{\bf{g}}_{n,l}^H}  + {\eta _n}{\bf{I}}} \right)}^{ - 1}}{{\bf{h}}_{n,k}}}}{{\left\| {{{\left( {\sum_{i = 1}^K {{\alpha _{n,i}}{{\bf{h}}_{n,i}}{\bf{h}}_{n,i}^H}  + \sum_{l = 1}^L {{\mu _l}{{\bf{g}}_{n,l}}{\bf{g}}_{n,l}^H}  + {\eta _n}{\bf{I}}} \right)}^{ - 1}}{{\bf{h}}_{n,k}}} \right\|}},
% \end{align}
The dimension of action space is $(2K+L+2)$.
Note that BS $n$ only calculates the beamforming vectors for its associated TUs, i.e., ${{\bf{w}}_k}, \forall k \in {{\cal K}_n}$, from its action ${{\bf{a}}_n}$.

% $L_{{\rm{FSP}},n,l}^{ - 1} {\bf{g}}_{n,l}^{LoS} \left( {{\bf{g}}_{n,l}^{LoS}} \right)^H$

\subsubsection{Reward Function}
% The distributed reward function design in \cite{ge2024deep} is adopted and the reward function of BS $n$ is designed as:
According to the distributed reward function design in \cite{ge2024deep}, the reward function of BS $n$ is designed as:
\begin{equation}
\label{Reward_Function}
{r_n}\left( t \right) = \sum_{k \in {{\mathcal K}_n}\left( t \right)} {{R_k}} \left( t \right) - \sum_{i \in {\mathcal U}_n^{{\rm{out}}}\left( t \right)} {\left( {{R_{i\backslash n}}\left( t \right) - {R_i}\left( t \right)} \right)},
% {r_n}(t) = \sum_{k\in{\mathcal{K}}_{n}} {{R_{k}}} (t) - \sum_{i \in {\mathcal U}_n^{{\rm{out }}}(t)} {{P_{i}}} (t),
% {r_n}(t) = \sum_{k = 1}^K {{R_{n,k}}} (t) - \sum_{(j,i) \in {\mathcal U}_n^{{\rm{out }}}(t)} {{P_{j,i}}} (t),
% {r_n}(t) = \frac{1}{K}\left(\sum_{k = 1}^K {{R_{n,k}}} (t) - \sum_{(j,i) \in {\mathcal U}_n^{{\rm{out }}}(t)} {{P_{j,i}}} (t)\right),
\end{equation}
where ${R_k}\left( t \right) = {\log _2}\left( {1 + p_k^r\left( t \right)/{\beta _k}\left( t \right)} \right)$ and ${R_{i\backslash n}}\left( t \right) = {\log _2}\left( {1 + p_i^{\rm{r}}\left( t \right)/\left( {{\beta _i}\left( t \right) - {\beta _{n,i}}\left( t \right)} \right)} \right)$. 
The second term in \eqref{Reward_Function} is a penalty for interfering with the interfered TUs of BS $n$.
% ${R_k}(t) = {\log _2}\left( {1 + \frac{{p_{\varrho_k,k}^r(t)}}{{{\beta _k}(t)}}} \right)$  and ${R_{i\backslash n}}(t) = {\log _2}\left( {1 + \frac{{p_i^{\rm{r}}(t)}}{{{\beta _i}(t) - {\beta _{n,i}}(t)}}} \right)$
% ${P_{j,i}}(t) = {\log _2}\left( {1 + \frac{{p_{j,i}^{\rm{r}}(t)}}{{{\beta _{j,i}}(t) - {\beta _{n,k}}(t)}}} \right) - {R_{j,i}}(t)$ is a penalty for interfering with the interfered TUs of BS $n$.


\subsubsection{Cost Function}
The cost function of BS $n$ is ${{\bf{c}}_n}(t) = {\left[ {{c_{n,1}}(t), \cdots ,{c_{n,L}}(t)} \right]^T}$, which should reflects the received interference power of $L$ AUs in the constraint \eqref{cons_interfer}, i.e., ${\rho _l}(t) < {I_{\max }}, \forall l$.
% which can be equivalently written as
We want the designed $l$-th cost function of BS $n$ ${c_{n,l}}(t)$ to be a straightforward linear function of ${\rho _l}(t)$, while ensuring that ${c_{n,l}}(t) < 0$ when ${\rho _l}(t) < {I_{\max }}$.
Accordingly, the cost limit $b_{n,l}$, $\forall l$ should be set to 0 and ${c_{n,l}}(t)$ can be designed as:
\begin{equation}
\label{Cost_Function}
{c_{n,l}}(t) = {\rho _l}(t)/{I_{\max }} - 1.
% {c_{n,l}}(t) = \!\left\{ \begin{array}{l}
% \!\!{\log _2}\left( {10\left( {{\rho _l}(t)/{I_{\max }} -\! 1} \right) +\! 1} \right),{\rho _l}(t) \!>\! {I_{\max }}\\
% \!\!{\rho _l}(t)/{I_{\max }} - 1,0 \le {\rho _l}(t) \le {I_{\max }}
% \end{array} \right. \!\!\!.
% 
% {c_{n,l}}(t) = \left\{ {\begin{array}{*{20}{l}}
% {{{\log }_2}\left( {{\rho _l}(t)/{I_{\max }}} \right),{\rho _l}(t) > {I_{\max }}}\\
% {{\rho _l}(t)/{I_{\max }} - 1,0 \le {\rho _l}(t) \le {I_{\max }}}
% \end{array}} \right. .
\end{equation}
% ${c_{n,l}}(t) = \max \left\{ {{\rho _{n,l}}(t) + \sum_{m = 1,m \ne n}^N {{\rho _{m,l}}(t)}  - {{\bar I}_l},0} \right\}$
Therefore, according to the NCPOMG formulation in \eqref{NCPOMG}, the safety constraints of BS $n$ are given by:
% can be written as
% $\forall l = 1,...,L$
\begin{equation}
\label{safety_constraints}
{{\mathbb{E}}_{{{\bf{s}} \left( 0 \right)},{{\bf{a}}_n\left( 0 \right)},...}}\left[ {\sum_{t = 0}^\infty  {\gamma _c^t\left( {\rho _l}(t)/{I_{\max }} - 1 \right)} } \right] \le 0, \forall l.
\end{equation}





\subsection{TU Agent for User Association}
The adopted DRL algorithm, observation space, action space, and reward function of the TU agents are detailed as follows.


\subsubsection{Algorithm}
% \subsection{Preliminaries of a DRL Algorithm: D3QN}
% dueling double deep Q-network (D3QN)
% is a widely used DRL algorithm, which .
% Double DQN decouples action selection and action evaluation, preventing overestimation problems that can occur during DQN updates \cite{van2016deep}.
We apply the D3QN algorithm in each TU agent, which combines the ideas of double DQN and dueling DQN and has been successfully applied to user agents in user association tasks \cite{van2016deep, wang2016dueling, zhao2019deep}.
The update of the double DQN decouples action selection and action evaluation, which can prevent overestimation problems that occur during DQN updates \cite{van2016deep}.
Specifically, the online network ${Q_{\boldsymbol{\omega }}}$ with parameters ${\boldsymbol{\omega }}$ is used to select the action, while the target network ${Q_{{\boldsymbol{\omega }}^{-}}}$ with parameters ${\boldsymbol{\omega }}^{-}$ is used to evaluate the action.
Every time the agent updates the networks, $B_b^{TU}$ experiences $\langle {{\bf{o}}_{j}^{TU}}, {\varrho_j}, {r_{j}^{TU}}, {{\bf{o}}_{j + 1}^{TU}} \rangle$, $j = 1, \ldots, B_b^{TU}$ are sampled to form a batch.
The loss function of double DQN is: 
\begin{equation}
\label{loss_D3QN}
{{\mathcal L}_Q}\left( {\boldsymbol{\omega }} \right) = \frac{1}{{2{B_b^{TU}}}}\sum_{j = 1}^{{B_b^{TU}}} {{{\left( {{Q_{\boldsymbol{\omega }}}\left( {{{\bf{o}}_j^{TU}},{\varrho_j}} \right) - Q_j^{{\rm{target}}}} \right)}^2}},
\end{equation}
where $Q_j^{\rm{target}} \!=\! {r_j^{TU}} \!+ \gamma Q_{{\boldsymbol{\omega}}^{-} }\!\!\left( {{\bf{o}}_{j + 1}^{TU}}, {\arg\max }_{\varrho^{\prime}} Q_{\boldsymbol{\omega}}\!\left( {{\bf{o}}_{j + 1}^{TU}},{\varrho^{\prime}} \right) \!\right)$.
% 
Besides, in dueling DQN, the output Q value of each action is combined from two streams that estimate the scalar state value and the advantage of each action \cite{wang2016dueling}.
% Besides, dueling DQN introduces a new neural network architecture, where the output Q value of each action is combined from two streams that estimate the scalar state value and the advantage of each action \cite{wang2016dueling}.
% Besides, dueling DQN introduces a new neural network architecture consisting of two streams that estimate the scalar state value and the advantage of each action, which are then combined to output the Q value of each action \cite{wang2016dueling}.
% that decouples value and advantage in DQN.


\subsubsection{Observation Space}
% For the design of the observation vector in \eqref{association_observation}, we have an intuitive understanding 
% The design of the observation vector is based on an intuitive understanding that if the rate obtained by TU $i$ associating with BS $j$ is low, then TU $i$ can associate with a BS with few associated users and a strong channel.
The observation design is based on an intuitive understanding that if the rate obtained by TU $i$ associating with a BS is low, then TU $i$ can associate with another BS with fewer associated users and a stronger channel.
Thus, in slot $t$, the observation of TU $k$ is designed as:
\begin{align}
\label{association_observation}
{\bf{o}}_k^{TU}\!\left( t \right) =\! [& \left| {{{\mathcal K}_1}\left( {t -\! 1} \right)} \right|\!, \cdots\! ,\left| {{{\mathcal K}_N}\left( {t -\! 1} \right)} \right|\!,{\boldsymbol{\chi }}_k^{TU}(t -\! 1),{{\tilde{\bf{h}}} _k}\left( t \right)\!,\nonumber\\
&{p_k}\left( {t -\! 1} \right)\!,{R_k}\left( {t -\! 1} \right)\!,p_k^r\left( {t -\! 1} \right)\!,{\beta _k}\left( {t -\! 1} \right) ],
% {\bf{o}}_n^{{\rm{loc}}}(t) = [ &{\boldsymbol{\chi}}_{n}^{\rm{BS}}(t), {\bf{H}}_n^{\rm{c}}(t),{{\bf{p}}_n}\left( {t - 1} \right),{{\bf{R}}_n}\left( {t - 1} \right),\nonumber\\
% &{\bf{p}}_n^{\rm{r}}\left( {t - 1} \right),{{\boldsymbol{\beta}}_n}\left( {t - 1} \right) ],\nonumber
\end{align}
where $\left| {{{\mathcal K}_n}} \right|$ is the number of users associated with BS $n$, ${\boldsymbol{\chi }}_k^{TU} = \left[ {{\chi _{1,k}}, \cdots ,{\chi _{N,k}}} \right]$ is the user association for TU $k$, ${{\tilde{\bf{h}}} _k} = \left[ {\left\| {{{\bf{h}}_{1,k}}} \right\|_2^2, \cdots ,\left\| {{{\bf{h}}_{N,k}}} \right\|_2^2} \right]$ is the channel strength from all BSs to TU $k$.
At the beginning of each time step $t$, each BS broadcasts reference signals to all TUs, and then each TU can measure the channel strength from all BSs.
%  \neq \varrho_k\left( {t - 1} \right)
Besides, TU $k$ can obtain $\left| {{{\mathcal K}_n}\left( {t - 1} \right)} \right|, \forall n$ from BS $\varrho_k\left( {t - 1} \right)$.


\subsubsection{Action Space}
The action of TU $k$ is its associated BS, i.e., $\varrho_k$.
% The action of TU $k$ is $\varrho_k$, i.e., the BS associated with TU $k$.






\subsubsection{Reward Function}
The reward function of TU $k$ is designed as:
\begin{align}
\label{Reward_Function_asso}
{r_k^{TU}}\left( t \right) =& {\widetilde R_k}\left( t \right) - \sum_{i \in {\mathcal U}_{{\varrho _k}\left( t \right)}^{{\rm{out }}}\left( t \right),i \ne k} {\left( {{{\widetilde R}_{i\backslash k}}\left( t \right) - {R_i}\left( t \right)} \right)},\\
\label{Reward_Function_rate_discount}
{\widetilde R_k}\left( t \right) =& \left\{ \begin{array}{l}
{R_k}\left( t \right),{\varrho_k}\left( t \right) = {\varrho_k}\left( {t - 1} \right),\\
{\zeta _R}{R_k}\left( t \right),{\varrho_k}\left( t \right) \ne {\varrho_k}\left( {t - 1} \right),
\end{array} \right.
% {r_n}\left( t \right) = \sum_{k \in {{\mathcal K}_n}\left( t \right)} {{R_k}} \left( t \right) - \sum_{i \in {\mathcal U}_n^{{\rm{out}}}\left( t \right)} {\left( {{R_{i\backslash n}}\left( t \right) - {R_i}\left( t \right)} \right)}
\end{align}
where ${\zeta _R} \in \left[ {0,1} \right]$ is a handover discount factor, representing the fraction of time for data transmission in the time slot during which handover occurs, ${{\widetilde R}_{i\backslash k}}\left( t \right) - {R_i}\left( t \right)$ is a penalty term for the rate loss caused by the BS $\varrho_k$ to TU $i$ in order to serve TU $k$, and ${\widetilde R_{i\backslash k}}\left( t \right) = {\log _2}\left( {1 + {p_i^{\rm{r}}\left( t \right)}/ \left({{\beta _i}\left( t \right) - {p_k}\left( t \right){{\left| {{\bf{h}}_{{\varrho _k},i}^H\left( t \right){{\overline {\bf{w}} }_k}\left( t \right)} \right|}^2}}\right)} \right)$.
%  is the rate of user $i$ when user $k$ does not exist. 
% ${\widetilde R_{i\backslash k}}\left( t \right) = {\log _2}\!\left( {1 \!+\! {p_i^{\rm{r}}\left( t \right)}/\! \left({{\beta _i}\left( t \right) \!-\! {p_k}\left( t \right)\!{{\left| {{\bf{h}}_{{\varrho _k},i}^H\left( t \right){{\overline {\bf{w}} }_k}\left( t \right)} \right|}^2}}\right)} \!\right)$.
% ${\widetilde R_{i\backslash k}}\left( t \right) = {\log _2}\left( {1 + \frac{{p_i^{\rm{r}}\left( t \right)}}{{{\beta _i}\left( t \right) - {p_k}\left( t \right){{\left| {{\bf{h}}_{{\varrho _k},i}^H\left( t \right){{\overline {\bf{w}} }_k}\left( t \right)} \right|}^2}}}} \right)$. 
% The second term in \eqref{Reward_Function_asso}
% .
% interfering with the interfered TUs of BS $n$.





% We propose a safe DRL-based DDUACBF scheme for CATN in Algorithm \ref{alg_safeMADRL_UA_CBF}, where the function ${\rm{clip}}(x,a,b)$ clips $x$ to the interval $[a,b]$.
% , which is outlined
\subsection{The Overall Safe DRL Scheme}
% \addtolength{\bottommargin}{+0.05in} % The bottom margin
% \addtolength{\topmargin}{+0.05in} % The top margin
% \addtolength{\textfloatsep}{-0.05in} % The bottom margin


% Algorithm \ref{alg_safeMADRL_UA_CBF} shows the proposed safe DRL-based DDUACBF scheme for CATN.
The proposed safe DRL-based DDUACBF scheme for CATN is summarized in Algorithm \ref{alg_safeMADRL_UA_CBF}.
In each time slot, all TU agents first determine their associated BS, and then all BS agents determine their downlink beamforming vectors to serve their associated TUs.


% $L(\theta)=\frac1{M_b}\sum_{e\in\mathcal{M}}\left(y_e^\rm{tar}-Q(o_e,a_e|\theta)\right)^2$

% $y_e^{\rm{tar}}\triangleq r_e+\gamma Q^{\prime}(o_e^{\prime},\pi(o_e^{\prime}|\boldsymbol{\mu}^{\prime})|\boldsymbol{\theta}^{\prime})$

% $\nabla_{\boldsymbol{\mu}}J(\boldsymbol{\mu})=-\frac{1}{M_b}\sum_{e\in\mathcal{M}}\nabla_aQ(o,a|\boldsymbol{\theta})|_{o=o_e,a=a_e}\nabla_{\boldsymbol{\mu}}\pi(s|\boldsymbol{\mu})|_{s=s_e}$






\begin{algorithm}[t] % htbp
\small
% Hierarchical multi-agent DRL-based Joint User Association and Coordinated Beamforming algorithm for CATN
\caption{Safe DRL-based DDUACBF scheme for CATN}
\label{alg_safeMADRL_UA_CBF}
\begin{algorithmic}[1]
% \STATE {TU $k$ initializes D3QN $Q_{\boldsymbol{\omega}}$ and target D3QN $Q_{\boldsymbol{\omega}^{-}}\!\!$ by ${\boldsymbol{\omega}^{-}} \!\!\leftarrow\! {\boldsymbol{\omega}}$, $\forall k$.}
\STATE {TU $k$ initializes $Q_{\boldsymbol{\omega}}$ and $Q_{\boldsymbol{\omega}^{-}}$ by ${\boldsymbol{\omega}^{-}} \leftarrow {\boldsymbol{\omega}}$, $\forall k$.}
% \vspace{-0.35cm}
\STATE {TU $k$ initializes discount factor $\gamma$, learning rates $\alpha_{\boldsymbol{\omega}}$, $\epsilon$, target D3QN replacement frequency $T^{-}$, and the experience replay memory $\xi _k^{TU}$ with a FIFO queue of size $B_m^{TU}$, $\forall k$.}
\STATE {BS $n$ initializes $\pi_{\boldsymbol{\vartheta}}$, $V_{\boldsymbol{\varphi}}$, and $V^C_{\boldsymbol{\psi}}$, $\forall n$.}
\STATE {BS $n$ initializes discount factor $\gamma$, cost limit ${\bf{b}}$, GAE parameter $\lambda$, learning rates $\alpha_{\boldsymbol{\nu}}$, $\alpha_V$, $\alpha_{\pi}$, cost parameter ${\boldsymbol{\nu}}$ and its bound ${\boldsymbol{\nu}}_{\max}$, KL divergence bound $\varepsilon$, and the trajectory collector $\xi _n^{BS}$ of size $B_m^{BS}$, $\forall n$.}
% statistical
% \STATE {In time slot $t=0$, TU $k$ obtains the channel strength from all BSs and a random action ${\varrho_k}(0)$, $\forall k$.}
\STATE {In time slot $t=0$, TU $k$ obtains a random action ${\varrho_k}(0)$, $\forall k$.}
\STATE {BS $n$ obtains the CSI to all TUs, the CSI to all AUs, and a random action ${\bf{a}}_n(0)$, $\forall n$. Set $t=1$.}
% \STATE {In time slot $t=0$, TU $k$ obtains a random action ${\varrho_k}(0)$, $\forall k$. BS $n$ obtains a random action ${\bf{a}}_n(0)$, $\forall n$. Set $t=1$.}
% \STATE {TU $k$ obtains action ${\varrho_k}(0)$ by associating with the BS with the strongest channel, $\forall k$.}
% \STATE {TU $k$ obtains a random action ${\varrho_k}(0)$, $\forall k$.}
% \STATE {BS $n$ obtains a random action ${\bf{a}}_n(0)$, $\forall n$. Set $t=1$.}
% \STATE {$t=1$.}
% \STATE {BS $n$ takes action randomly, $\forall n$.}
% 
% t=1！！！！！！！
\WHILE {the training process is not over}
% statistical
% \STATE {When time slot $t$ begins, TU $k$ measures the channel strength from all BSs, $\forall k$.}
% \STATE {TU $k$ obtains observation ${\bf{o}}_k^{TU}(t)$, $\forall k$.}
\STATE {When time slot $t$ begins, TU $k$ measures the channel strength from all BSs and obtains observation ${\bf{o}}_k^{TU}(t)$, $\forall k$.}
\STATE {If ${{\bf{o}}_k^{TU}}(t-1)$ exists, TU $k$ stores the experience $\left\langle {{{\bf{o}}_k^{TU}}(t-1),{\varrho_k}(t-1),{r_k^{TU}}\!(t-1),{{\bf{o}}_k^{TU}}(t)} \right\rangle$ into $\xi _k^{TU}$, $\forall k$.}
\IF {$t-1 \geq B_b^{TU}$}
\STATE TU $k$ samples a batch of $B_b^{TU}$ experiences $\langle {{\bf{o}}_{j}^{TU}}\!, {\varrho_j}, {r_{j}^{TU}}\!, {{\bf{o}}_{j + 1}^{TU}} \rangle$, $\!j \!= \!1, \ldots\!, B_b^{TU}\!$ from $\!\xi _k^{TU}\!$, $\!\forall k$.
\vspace{-0.3cm}
\STATE TU $k$ updates ${\boldsymbol{\omega}}  \leftarrow {\boldsymbol{\omega}}  - {\alpha _{\boldsymbol{\omega}}}{\nabla _{\boldsymbol{\omega}} }{{\mathcal L}_Q}\left( {\boldsymbol{\omega }} \right)$ with \eqref{loss_D3QN}, $\forall k$.
\STATE In every $T^{-}$ slots, TU $k$ replaces the parameters of the target D3QN ${\boldsymbol{\omega}^{-}} \leftarrow {\boldsymbol{\omega}}$, $\forall k$.
\ENDIF
\IF {$t-1 \geq B_b^{TU}$}
\STATE {TU $k$ obtains action ${\varrho_k}(t)$ through $\epsilon$-greedy policy and associates with BS ${\varrho_k}(t)$, $\forall k$.}
\ELSE
\STATE {TU $k$ obtains a random action ${\varrho_k}(t)$, $\forall k$.}
\ENDIF
\STATE {BS $n$ obtains the CSI to all TUs and the CSI to all AUs, $\forall n$.}
\STATE {BS $n$ exchanges information with other BSs, $\forall n$.}
\STATE {BS $n$ obtains observation ${\bf{o}}_n(t)$, and stores the experience $\left\langle {{{\bf{o}}_n}(t-1),{{\bf{a}}_n}(t-1),{r_n}(t-1),{{\bf{c}}_n}(t-1), {{\bf{o}}_n}(t)} \right\rangle$ into $\xi _n^{BS}$ if ${{\bf{o}}_n}(t-1)$ exists, $\forall n$.}
% \vspace{-0.3cm}
\IF {if $\xi _n^{BS}$, $\forall n$ is full}
\STATE BS $n$ obtains all experiences $\langle {{{\bf{o}}_{i}},{{\bf{a}}_{i}},{r_{i}},{{\bf{o}}_{i + 1}},{{\bf{c}}_{i}}} \rangle$, $i = 1, \!\ldots\!, B_m^{BS}\!$ in $\xi _n^{BS}\!$ and updates $\pi_{\boldsymbol{\vartheta}}$, $\!V_{\boldsymbol{\varphi}}$, and $V^C_{\boldsymbol{\psi}}\!$ by the safe DRL algorithm, i.e., Algorithm \ref{alg_CUP}, and then clear $\xi _n^{BS}$, $\forall n$.
\ENDIF
\STATE {BS $n$ obtains action ${\bf{a}}_n(t)$ and calculates ${{\bf{w}}_k}, \forall k \in {{\cal K}_n}$, $\forall n$.}
% the beamforming vectors 
\STATE {BS $n$ transmits signals to its associated TUs, then receives reward $r_n(t)$ and cost ${{\bf{c}}_n}(t)$, $\forall n$.}
\STATE {TU $k$ receives reward $r_k^{TU}(t)$, $\forall k$. Set $t=t+1$.}
% \STATE {$t=t+1$.}
\ENDWHILE
\RETURN $\!\!\!$ Policy network $\!\pi_{\boldsymbol{\vartheta}}\!$ of each BS and D3QN $\!Q_{\boldsymbol{\omega}}\!$ of each TU.
\end{algorithmic}
\end{algorithm}


    












% \vspace{-0.8cm}

\section{Simulation Results}
\label{sec_Simulation_Results}
% \addtolength{\topmargin}{+0.05in} % The top margin
% compared to the penalty-based DRL algorithm.
In this section, simulation results are shown to evaluate the performance of the proposed safe DRL-based DDUACBF scheme in Algorithm \ref{alg_safeMADRL_UA_CBF} (denoted by D3QN-CUP) and some two-stage benchmarks, which first determine the UA and then the beamforming vectors in each time slot.
For UA, the benchmarks are as follows:
\begin{itemize}
\item DCD: The UA in each time slot is obtained by the joint UA and power control algorithm in \cite{shen2014distributed}, which iteratively optimizes the UA and the power control via the DCD algorithm in Algorithm \ref{alg_DCD} and the Newton's method, respectively. This algorithm requires real-time global channel strength information.
% the UA via a DCD algorithm in Algorithm \ref{alg_DCD} and the power control via Newton's method. 
%  and provides upper bound performance.
\item SC: Each TU is associated with the BS with the strongest channel strength. Thus, each TU only needs the received channel strength information from all BSs without information exchange.
\end{itemize}
For beamforming, the benchmarks are as follows:
\begin{itemize}
\item DFP: The beamformers are obtained by the direct FP algorithm with real-time global CSI, which can achieve a higher sum rate than the WMMSE algorithm and provides upper bound performance \cite{shen2018fractional1}.
% \item Direct FP: The beamformers are obtained by the direct FP algorithm with real-time global CSI \cite{shen2018fractional1}, where a convex problem is solved instead of the closed-form computations in each iteration.
% This algorithm can provide upper bound performance.
% This algorithm can provide upper bound performance by solving a convex problem instead of the closed-form computations in each iteration.
% Penalty-based 
\item PPO: The beamformers are obtained by PPO algorithm \cite{schulman2017proximal}, where the reward function is designed as ${\widetilde r_n}(t) = {r_n}(t) - \zeta \sum_{l = 1}^L {\max \left\{ {{c_{n,l}}(t),0} \right\}}$ with \eqref{Reward_Function}, \eqref{Cost_Function}, and a fixed penalty weight $\zeta$. 
Besides, the observation space and the action space are given by \eqref{State_Space} and \eqref{Action_Space}, respectively.
\item WMMSE: The beamformers are obtained by a low-complexity modified version of Algorithm \ref{alg_Closed_Form_FP_cog} with real-time global CSI.
Specifically, the high-complexity matrix inversion operation in line 5 is replaced by updating ${{\bf{w}}_k}$'s and $\eta _n$'s by the prox-linear method \cite{xu2013block} and updating ${\mu _l}$'s by the sub-gradient method.
% where matrix inversion operations with high complexity in the fifth line are avoided, i.e, ${{\bf{w}}_k}$'s and $\eta _n$'s are updated by the prox-linear method \cite{xu2013block} and ${\mu _l}$'s are updated by the sub-gradient method in each iteration.
% \item CFFP: The beamformers are obtained by Algorithm \ref{alg_Closed_Form_FP_cog}, where, in the fifth line, ${{\bf{w}}_k}$'s and $\eta _n$'s are updated by the prox-linear method \cite{xu2013block} and ${\mu _l}$'s are updated by the sub-gradient method in each iteration.
% This algorithm can aviod matrix inversion operations with high complexity by exploiting the prox-linear method.
\end{itemize}




\begin{figure}[t]
\centering
\includegraphics[width=9cm]{figureslong/simulation_scenario_UMa1029.eps}
%\vspace{-0.3cm}
\caption{Simulation scenario.}
\label{simulation_scenario}
\vspace{-0.4cm}
\end{figure}
We consider a CATN with $L=2$ AUs, $N=7$ BSs, and $K=21$ TUs.
The projection positions of each device on the ground are shown in Fig. \ref{simulation_scenario}, where the AUs are airplanes with a fixed height of $10{\rm{km}}$.
Note that the right figure in Fig. \ref{simulation_scenario} is an enlarged view of the middle part of the left figure.
Besides, the heights of the terrestrial BSs and TUs are $30{\rm{m}}$ and $1.5{\rm{m}}$, respectively.
We consider 6000 time slots with an interval of $0.02{\rm{s}}$, where both AUs fly with a fixed speed of $250{\rm{m/s}}$.
The movement trajectories of TUs are depicted by the blue curve in Fig. \ref{simulation_scenario}.
% in the positive direction of the Y-axis following the trajectory in Fig. \ref{simulation_scenario}.
% Specifically, AU 1 flies from the middle to the top right corner, and then in the 2501-th time slot, another airplane is considered as AU 1, which flies from the bottom left corner to the middle.
% % of the red line.
% In addition, AU 2 flies from left to right.
% In addition, AU 2 flies from right to left.
% In addition, AU 2 flies from the bottom right corner to the top left corner.
% of the blue line.
% 
% 
Other parameters are set as follows.
% Each BS is equipped with a uniform rectangular array (URA) of M=4\times 4=16 antennas.
The size of the URA at each BS is set as $M_h = M_v = 4$, i.e., $M = 16$.
The center frequency is $f_c=2{\rm{GHz}}$ and the bandwidth is $10{\rm{MHz}}$.
% The terrestrial network uses the spectrum of the aerial network, whose center frequency is $f_c=2{\rm{GHz}}$ and bandwidth is $10{\rm{MHz}}$.
The noise power is set to $-114{\rm{dBm/MHz}}$, i.e., $\sigma^2=3.98\times 10^{-11}{\rm{mW}}$.
% The maximum transmitting power of the BS is $P_{\max}=20{\rm{W}}$.
% $P_{\max}=43{\rm{dBm}}$.
For each channel ${{\bf{h}}_{n,k}}$, its path loss $L_{{\rm{UMa}},n,k}$ keeps LoS or NLoS during the simulation.
The channel correlation coefficient is $\alpha=0.64$.
The Rician factor is $\kappa=15{\rm{dB}}$.
% The channels from BS $j$ to TU $k$ and AU $l$ are denoted by ${\bf{h}}_{j,n,k} \in \mathbb{C}^{M \times 1}$ and ${\bf{g}}_{j,l} \in \mathbb{C}^{M \times 1}$, respectively.
% For ${{\bf{h}}_{n,k}}$, 
The handover discount factor ${\zeta _R}$ is 0.4.


The size of the codebook is set as $C = 128$ and the compression factor is set as $N_c = 4$. 
For the size of the sets, we set $\left| {{\mathcal B}_{n,k}^{{\rm{in}}}(t)} \right| = B_{\rm{in}}=4$, $\left| {{\mathcal B}_l^{{\rm{in,pri}}}(t)} \right| = \widetilde B_{\rm{in}}=5$, and $K_{\rm{out}}=3$, then we have $\left| {{\mathcal U}_n^{{\rm{out }}}(t)} \right| = K_{\rm{out}} B_{\rm{in}}=12$.
% As for the cardinalities of two kinds of sets, i.e., $Iin n,k(t)$'s and $Iout n(t)$'s, we set $U = 4$, then we have
% 
For the BS agent in the CUP-based schemes and the PPO-based schemes, the policy and value networks' hidden neural networks are all three-layer with 512, 128, and 64 neurons, respectively. 
% Besides, ReLU is adopted as the activation function of the value network and the hidden layers of the policy network. 
Besides, in the value network and the first two hidden layers of the policy network, ReLU is adopted as the activation function. 
In the third hidden layer of the policy network, the sigmoid activation function is adopted.
Moreover, the sigmoid function is also adopted to normalize the outputs of the policy network to the range $[0, 1]$ before scaling to the actual action values.
% Moreover, in the output layer of the policy network, the sigmoid activation function is adopted to normalize all the elements of the action space in \eqref{Action_Space} to the range $[0, 1]$.
For the TU agent in the D3QN-based schemes, the Q networks' hidden neural networks are all two-layer with 64 and 32 neurons. Besides, ReLU is adopted as the activation function.
% 
% 
The parameters of Algorithm \ref{alg_safeMADRL_UA_CBF} are set as follows: 
discount factor is $\gamma=0.5$, 
GAE parameter is $\lambda=0.1$,
learning rates are $\alpha_{\boldsymbol{\nu}}=\!0.06$, $\alpha_V\!=\!3\times 10^{-4}$, $\alpha_{\pi}\!=\!3\times 10^{-4}$, 
each element of initial cost constraint parameter ${\boldsymbol{\nu}}$ is set as 1, 
each element of cost constraint parameter bound ${\boldsymbol{\nu}}_{\max}$ is set as 10, 
cost bound is ${\bf{b}}=\!{\bf{0}}$, 
% temperature is $\tau=\!1.5$, 
KL divergence bound is $\varepsilon=\!0.02$. 
% early stopping trust region.
Besides, for BS agent, we set $B_m^{BS}=50$, minibatch size $B_b=10$, and number of epoch $B_e=20$.
For TU agent, we set $T^{-}=50$, $B_m^{TU}=2000$, and batch size $B_b^{TU}=200$.
Each time the TU agent obtains action through $\epsilon$-greedy policy, its $\epsilon$ is updated by $\epsilon = \max \left\{ {{\epsilon_{\min }},0.995\epsilon} \right\}$, where $\epsilon_{\min }=0.005$ and $\epsilon$ is initialized to 0.3.
% the size of the experience memory is $B_m=48$,
% the size of the mini-batch is $B_b=8$, and the number of epoch is $B_e=20$.






% \begin{figure}[t]
% \centering
% \includegraphics[width=6.5cm]{figureslong/sum_rate_time_slots.eps}
% %\vspace{-0.3cm}
% \caption{Sum rate of the terrestrial users: ${I_{\max}} = 1.6\times {10^{ - 10}}{\rm{mW}}$. }
% \label{sum_rate_time_slots}
% \vspace{-0.4cm}
% \end{figure}

% \begin{figure}[t]
% \centering
% \includegraphics[width=6.5cm]{figureslong/interference_power_PU_time_slots.eps}
% %\vspace{-0.3cm}
% \caption{Received interference power of two aerial users. }
% \label{interference_power_time_slots_PUs}
% \vspace{-0.4cm}
% \end{figure}


% Each value is a moving average over 25 time slots.
\begin{figure}[t]
\centering
\subfloat[Sum rate of the terrestrial users. \label{sum_rate_time_slots}]
{\includegraphics[width=6.5cm]{figureslong/sum_rate_time_slots.eps}}
\vspace{-0.2cm}
\\
\subfloat[Received interference power of two aerial users. \label{interference_power_time_slots_PUs}]
{\includegraphics[width=6.5cm]{figureslong/interference_power_PU_time_slots.eps}}
% \vspace{-0.4cm}
% 
\caption{Performance comparison of the proposed scheme and the benchmarks: $P_{\max}=20{\rm{W}}$ and ${I_{\max}} = 1.6\times {10^{ - 10}}{\rm{mW}}$.}
\label{time_slots_CUP_WMMSE} 
\vspace{-0.3cm}
\end{figure}

% First, we show the effectiveness of the proposed algorithm.
% Since the computational complexity of the direct FP algorithm to obtain the beamforming vectors for 5000 time slots is too high, we average the results for 60 time slots selected at equal time intervals and plot their performance with a dashed line.
% then averaged and 
First, we show the convergence performance of the proposed D3QN-CUP scheme and the benchmarks.
Due to the high computational complexity of the DFP-based scheme, we choose 100 time slots of equal time intervals to compute its results, which are plotted by dashed lines.
In addition, in all simulation figures, each value is a moving average with a span of 41 time slots.
Fig. \ref{time_slots_CUP_WMMSE}(a) shows the sum rate of the TUs during training.
The sum rate obtained by the CUP-based schemes gradually increases and is higher than that of the corresponding WMMSE-based schemes after convergence. 
Besides, the D3QN-based schemes can achieve a higher sum rate compared to the corresponding SC-based schemes.
Accordingly, Fig. \ref{time_slots_CUP_WMMSE}(b) shows the received interference power of two AUs ${\rho _{l}}, \forall l$ during training. 
The power ${\rho _{l}}, \forall l$ obtained by the CUP-based schemes gradually reduces and is generally close to $I_{\max}$ after convergence.
% During this phase, the sum rate obtained by the CUP-based schemes gradually increases.
% After convergence, the CUP-based schemes can achieve a higher sum rate than the corresponding WMMSE-based schemes.
% corresponding
% During this phase, the receiving interference power of the two AUs, i.e., ${\rho _{l}}, \forall l$, obtained by the proposed algorithm gradually reduces.
% During this phase, ${\rho _{l}}, \forall l$ obtained by the CUP-based schemes gradually reduces.
% After convergence, ${\rho _{l}}, \forall l$ obtained by the CUP-based schemes is generally close to $I_{\max}$. 
% the interference threshold



% \begin{figure}[t]
% \centering
% \includegraphics[width=6.5cm]{figureslong/sum_rate_time_slots_penalty_factor.eps}
% %\vspace{-0.3cm}
% \caption{Sum rate of the terrestrial users: ${I_{\max}} = 1.6\times {10^{ - 10}}{\rm{mW}}$. }
% \label{sum_rate_time_slots_penalty_factor}
% \vspace{-0.4cm}
% \end{figure}

% \begin{figure}[t]
% \centering
% \includegraphics[width=6.5cm]{figureslong/interference_power_PU_time_slots_penalty_factor.eps}
% %\vspace{-0.3cm}
% \caption{Received interference power of two aerial users. }
% \label{interference_power_time_slots_PUs_penalty_factor}
% \vspace{-0.4cm}
% \end{figure}

\begin{figure}[t]
\centering
\subfloat[Sum rate of the terrestrial users. \label{sum_rate_time_slots_penalty_factor}]
{\includegraphics[width=6.5cm]{figureslong/sum_rate_time_slots_penalty_factor.eps}}
\vspace{-0.2cm}
\\
\subfloat[Received interference power of two aerial users. \label{interference_power_time_slots_PUs_penalty_factor}]
{\includegraphics[width=6.5cm]{figureslong/interference_power_PU_time_slots_penalty_factor.eps}}
% \vspace{-0.4cm}
% 
\caption{Performance of the penalty-based PPO with different penalty weights $\zeta$: $P_{\max}=20{\rm{W}}$ and ${I_{\max}} = 1.6\times {10^{ - 10}}{\rm{mW}}$.}
\label{time_slots_penalty_factor} 
\vspace{-0.3cm}
\end{figure}
    
Fig. \ref{time_slots_penalty_factor} shows the performance of the D3QN-PPO scheme with different penalty weights $\zeta$.
When $\zeta$ is large, the D3QN-PPO scheme is too conservative to achieve a high sum rate and takes a long time to converge in Fig. \ref{time_slots_penalty_factor}(a). 
% When $\zeta$ is small, the D3QN-PPO scheme is too bold to effectively satisfy constraint.
When $\zeta$ is small, the D3QN-PPO scheme is too bold to effectively guarantee the constraint satisfaction in Fig. \ref{time_slots_penalty_factor}(b), despite achieving a higher sum rate than the D3QN-CUP scheme. 
Thus, the performance of the D3QN-PPO scheme is sensitive to the penalty weight $\zeta$.
% Specifically, when $\zeta$ is large, the DDPG algorithm is too conservative, and it is difficult to obtain a high sum rate. 
% When $\zeta$ is small, the DDPG algorithm is too bold, and it is difficult to ensure the constraint satisfaction effectively. 
In contrast, without tricky penalty weight adjustments, the D3QN-CUP scheme can maximize the reward while satisfying the safety constraints, which avoids the economic expenses of multiple training attempts in real-world deployments.
% In c algorithm is applied to the BS agents, which maximizes the reward while satisfying the safety constraints and has a lower deployment cost since the tricky adjustment of the penalty weights in the penalty-based DRL algorithms is avoided.



\begin{figure}[t]
\centering
\subfloat[Sum rate of the terrestrial users. \label{sum_rate_time_slots_max_interference}]
{\includegraphics[width=6.5cm]{figureslong/sum_rate_time_slots_max_interference.eps}}
\vspace{-0.2cm}
\\
\subfloat[Received interference power of two aerial users. \label{interference_power_time_slots_PUs_max_interference}]
{\includegraphics[width=6.5cm]{figureslong/interference_power_PU_time_slots_max_interference.eps}}
% \vspace{-0.4cm}
% received interference power threshold of the AUs
\caption{Performance for different interference temperature limits ${I_{\max}}$: $P_{\max}=20{\rm{W}}$.}
\label{time_slots_max_interference} 
\vspace{-0.3cm}
\end{figure}
Fig. \ref{time_slots_max_interference} shows the performance of the proposed D3QN-CUP scheme with different interference temperature limits ${I_{\max}}$.
In the case of different ${I_{\max}}$, the proposed scheme can gradually maximize the sum rate of the terrestrial network and reduce the interference power to the two AUs to near the threshold ${I_{\max}}$.
% Under different interference thresholds, CUP-based BS agents can reduce the power to near the threshold.
In addition, as ${I_{\max}}$ decreases, the sum rate also decreases to meet the more stringent interference requirements.

\begin{figure}[t]
\centering
\subfloat[Sum rate of the terrestrial users. \label{sum_rate_time_slots_max_power}]
{\includegraphics[width=6.5cm]{figureslong/sum_rate_time_slots_max_power.eps}}
\vspace{-0.2cm}
\\
\subfloat[Received interference power of two aerial users. \label{interference_power_time_slots_PUs_max_power}]
{\includegraphics[width=6.5cm]{figureslong/interference_power_PU_time_slots_max_power.eps}}
% \vspace{-0.4cm}
\caption{Performance for different maximum transmit power of the BSs ${P_{\max}}$: ${I_{\max}} = 1.6\times {10^{ - 10}}{\rm{mW}}$.}
\label{time_slots_max_power} 
\vspace{-0.3cm}
\end{figure}
Fig. \ref{time_slots_max_power} shows the performance of the proposed D3QN-CUP scheme with different maximum transmit power of the BSs ${P_{\max}}$.
It can be observed that as ${P_{\max}}$ increases, the proposed scheme requires more training slots to converge.
This may be attributed to the expanded action range resulting from the higher ${P_{\max}}$, which makes the policy need more exploration to reach the optimum.
Another possible reason is that when ${P_{\max}}$ is large, for fear that other agents will interfere too much with the AUs, the agents tend to adopt a more conservative policy to satisfy the safety constraints.
% This may be because the increase in ${P_{\max}}$ leads to a larger action range, which makes the policy need more exploration to reach the optimum.
% achieve optimal results.



% ：，，
% 
% 
% ：D3QN，
\begin{figure}[t]
\centering
\includegraphics[width=8.5cm]{figureslong/sum_throughput_handover.eps}
%\vspace{-0.3cm}
\caption{Performance for different user association schemes: $P_{\max}=20{\rm{W}}$ and ${I_{\max}} = 1.6\times {10^{ - 10}}{\rm{mW}}$.}
% ${\zeta _R}=0.4$ in \eqref{Reward_Function_rate_discount}
% The handover discount factor ${\zeta _R}$ is 0.8.
\label{simulation_user_association}
\vspace{-0.4cm}
\end{figure}
% Fig. \ref{simulation_user_association} shows the average sum throughput of the TUs and the average percentage of TUs who make a handover 
Fig. \ref{simulation_user_association} shows the sum throughput of the TUs in one second, the received interference power of the two AUs, and the percentage of TUs that perform a handover obtained by the proposed D3QN-CUP scheme, the DCD-CUP scheme, and the SC-CUP scheme in 6000 test time slots, respectively.
It can be found that the D3QN-CUP scheme achieves the highest throughput, the lowest received interference power, and the lowest handover percentage.
% ，bps
% ：D3QN-CUP，
% ？
% （，）
% ，，D3QN-CUPSC-CUP。D3QN，，SC（3%）。
% 




% ：，
% 
% 
% 
% ：D3QN，
% ：D3QNDCDSC
% ：




% \begin{figure}[t]
% \centering
% \subfloat[Sum rate of the terrestrial users. \label{sum_rate_time_slots_observe}]
% {\includegraphics[width=6.5cm]{figureslong/sum_rate_time_slots_observe.eps}}
% \vspace{-0.2cm}
% \\
% \subfloat[Received interference power of two aerial users. \label{interference_power_time_slots_PUs_observe}]
% {\includegraphics[width=6.5cm]{figureslong/interference_power_PU_time_slots_observe.eps}}
% % \vspace{-0.4cm}
% % 
% \caption{Performance for different observation designs: ${I_{\max}} = 1.6\times {10^{ - 10}}{\rm{mW}}$.}
% \label{time_slots_observe} 
% \vspace{-0.3cm}
% \end{figure}
% Fig. \ref{time_slots_observe} shows the performance of the D3QN-CUP algorithm with different observation designs.


% \begin{figure}[t]
% \centering
% \subfloat[Sum rate of the terrestrial users. \label{sum_rate_time_slots_interferer_BSs_of_PU}]
% {\includegraphics[width=6.5cm]{figureslong/simulation_scenario_UMa0910.eps}}
% \vspace{-0.2cm}
% \\
% \subfloat[Received interference power of two aerial users. \label{interference_power_time_slots_PUs_interferer_BSs_of_PU}]
% {\includegraphics[width=6.5cm]{figureslong/simulation_scenario_UMa0910.eps}}
% % \vspace{-0.4cm}
% % 
% \caption{Performance for different number of interferer BSs of each AU $\widetilde B_{\rm{in}}$: ${I_{\max}} = 1.6\times {10^{ - 10}}{\rm{mW}}$.}
% \label{time_slots_interferer_BSs_of_PU} 
% \vspace{-0.3cm}
% \end{figure}
% Fig. \ref{time_slots_interferer_BSs_of_PU} shows the performance of the D3QN-CUP algorithm with different number of interferer BSs of each AU $\widetilde B_{\rm{in}}$.


% \begin{figure}[t]
% \centering
% \subfloat[Sum rate of the terrestrial users. \label{sum_rate_time_slots_Rician_factor}]
% {\includegraphics[width=6.5cm]{figureslong/sum_rate_time_slots_Rician_factor.eps}}
% \vspace{-0.2cm}
% \\
% \subfloat[Received interference power of two aerial users. \label{interference_power_time_slots_PUs_Rician_factor}]
% {\includegraphics[width=6.5cm]{figureslong/interference_power_PU_time_slots_Rician_factor.eps}}
% % \vspace{-0.4cm}
% % 
% \caption{Performance for different Rician factor $\kappa$: ${I_{\max}} = 1.6\times {10^{ - 10}}{\rm{mW}}$.}
% \label{time_slots_Rician_factor} 
% \vspace{-0.3cm}
% \end{figure}
% Fig. \ref{time_slots_Rician_factor} shows the performance of the D3QN-CUP algorithm with different Rician factor $\kappa$.


\begin{table*}[t]
\centering  % 
\caption{Comparison of communication overhead and computational complexity}  % 
\label{table_Comparison}  % 
%，|
% l，c，r
\begin{tabular}{cccc}  
\toprule  % 
% Inter-BS  Average
% The amount of external information required
Scheme  & \makecell[c]{External information required\\ (for user association)} & \makecell[c]{Inter-BS communication overhead\\ (for beamforming)} & \makecell[c]{Average computation time\\($L=2$, $N=7$, $K=21$, $M=16$)} \\  % ，&，\\
\midrule  % 
DCD-DFP & $N(K-1)$ & $2M(N-1)K + 2M(N-1)L$ & 78.78s \\ 
DCD-CUP & $N(K-1)$ & \multirow{4}*{\makecell[c]{$\left( {3{N_c}K + K + 2} \right){B_{{\rm{in}}}}{K_{{\rm{in}}}} +$\\ $\left( {K + 6} \right){\tilde B_{{\rm{in}}}}L + 3{K_{{\rm{out}}}}{B_{{\rm{in}}}} + 2L$}} & 326.80ms \\ 
D3QN-CUP (proposed) & $N$ & ~ & 82.64ms \\ 
SC-CUP & 0 & ~ & 63.48ms \\ 
D3QN-PPO & $N$ & ~ & 78.72ms \\ 
D3QN-WMMSE  & $N$ & $2M(N-1)K + 2M(N-1)L$ & 1.257s \\ 
\bottomrule  % 
\end{tabular}
\vspace{-0.4cm}
\end{table*}
% Additionally, i
% inter-BS the centralized optimization algorithms.
% Specifically, for ease of comparison, we divide the communication overhead into two parts for user association and beamforming.
In Table \ref{table_Comparison}, we compare the communication overhead and the computational complexity of the proposed scheme and the benchmarks.
For clarity, we categorize the communication overhead into two parts: user association and beamforming. 
The external information required for user association is measured by the number of real scalar values that each TU needs to obtain from BSs.
The inter-BS communication overhead is measured by the number of real scalar values that each BS needs to obtain from other BSs.
Note that the information obtained by the CUP or PPO-based schemes through exchange is all from the previous time slot.
It can be found that when the number of antennas $M$ is large, the CUP or PPO-based schemes are more advantageous than the DFP or WMMSE-based schemes.
In our simulation setup, the inter-BS communication overhead required for the CUP or PPO-based schemes is 3610, while that for the DFP or WMMSE-based schemes is 4416.


Moreover, the computational complexity is measured by the average computation time to obtain the user association of all TUs and the beamforming vectors of all BSs of one time slot in the considered simulation scenario.
We run programs of these schemes on a computer equipped with Intel i7-11700K and NVIDIA GeForce RTX 2080 Ti.
Note that the DCD-DFP scheme is run in MATLAB, while other schemes are programmed in Python.
It can be seen that for the computational complexity of user association, the D3QN-based schemes are slightly higher than the SC-based schemes and significantly lower than the DCD-based schemes.
For beamforming, the computational complexity of the CUP or PPO-based schemes is considerably lower than that of the DFP or WMMSE-based schemes.
Thus, the proposed scheme demonstrates lower computational complexity than the iterative optimization-based schemes.
As illustrated in Table \ref{table_Comparison}, the proposed scheme with low communication overhead and computational complexity is more advantageous in the dynamic CATN environment.
%  in computing the beamforming vectors of BSs in multiple cells in the dynamic environment.



% In addition, we compare the computational complexity of the proposed algorithm and the centralized optimization algorithms by comparing the average computation time.
% Specifically, we run programs for these algorithms on a computer equipped with Intel i7-7700.
% In the considered simulation scenario, the beamforming vectors of all BSs are obtained in $30{\rm{ms}}$ by the proposed algorithm, while it takes $0.1{\rm{s}}$ for the CFFP algorithm and $69.26{\rm{s}}$ for the Direct FP algorithm.
% % 
% Moreover, we compare the inter-cell communication overhead of the proposed algorithm with the centralized optimization algorithms, which is measured by the number of real scalar values that each BS needs to obtain from other BSs.
% The inter-cell communication overhead of the proposed algorithm is 954, while that of the centralized optimization algorithms including Direct FP and CFFP is 4416.
% % 
% Therefore, the proposed algorithm with low computational complexity and communication overhead is more advantageous in computing the beamforming vectors of BSs in multiple cells in the dynamic environment.




% Then, we show the performance of the proposed algorithm and the comparison algorithm under different interference power threshold ${I_{\max}}$.
% We fixed the positions of the two AUs at $(-500,866){\rm{m}}$ and $(-500,-866){\rm{m}}$.

% Fig. \ref{sum_rate_interference_threshold} shows the impact of the interference power threshold ${I_{\max}}$ on the sum rate of the terrestrial users.


% \begin{figure}[t]
% \centering
% \includegraphics[width=6.5cm]{figures/sum_rate_interference_threshold_FOCOPS3.eps}
% %\vspace{-0.3cm}
% % \caption{Effect of ${P_{\max}},\forall n$ and ${\overline q}$ on the sum rate of terrestrial network: ${\overline I _S} = 2\times {10^{ - 12}}{\rm{mW}}$ and ${\overline R_A} = 3{\rm{bps/Hz}}$.}
% \caption{Effect of the interference power threshold ${I_{\max}}$ on the sum rate of the terrestrial users.}
% \label{sum_rate_interference_threshold}
% \vspace{-0.4cm}
% \end{figure}

% \begin{figure}[t]
% \centering
% \includegraphics[width=6.5cm]{figures/interference_power_interference_threshold_FOCOPS3.eps}
% %\vspace{-0.3cm}
% \caption{Effect of the interference power threshold ${I_{\max}}$ on the satisfaction of interference power constraints.}
% \label{interference_power_interference_threshold}
% \vspace{-0.4cm}
% \end{figure}

% Fig. \ref{interference_power_interference_threshold} shows the impact of the interference power threshold ${I_{\max}}$ on the satisfaction of interference power constraints, where ${\hat I}$ denotes the average value that exceeds the interference threshold per time slot per AU and ${\overline I}$ denotes the average ratio of time slots that exceed the interference threshold per AU.


% \begin{figure}[t]
% \centering
% \includegraphics[width=6.5cm]{figures/sum_rate_time_slots.eps}
% %\vspace{-0.3cm}
% % \caption{Effect of ${P_{\max}},\forall n$ and ${\overline q}$ on the sum rate of terrestrial network: ${\overline I _S} = 2\times {10^{ - 12}}{\rm{mW}}$ and ${\overline R_A} = 3{\rm{bps/Hz}}$.}
% \caption{Effect of the number of BS antennas $M$ on the sum rate of the terrestrial users.}
% \label{sum_rate_antennas}
% \vspace{-0.4cm}
% \end{figure}

% Fig. \ref{sum_rate_antennas} shows the impact of the number of BS antennas $M$ on the sum rate of the terrestrial users.





\section{Conclusions}
\label{sec_Conclusions}
In this paper, we have proposed a safe DRL-based DDUACBF scheme for the CATN.
Specifically, to maximize the sum rate of the TUs under the interference temperature constraints of the AUs, a problem has been formulated by optimizing the user association between TUs and BSs and the beamforming vectors of the terrestrial BSs.
Then, this problem has been modeled as an NCPOMG with cost functions derived from the received interference power of the AUs.
To solve the NCPOMG, a safe DRL-based DDUACBF scheme has been proposed, where a safe DRL algorithm is adopted in each BS agent to maximize reward while satisfying safety constraints.
Simulation results have shown that the proposed scheme has lower computational complexity and communication overhead than the optimization-based schemes.
Moreover, the proposed scheme can achieve a high sum rate of the TUs while the average received interference power of the AUs is generally below the threshold.
% the interference power constraints are generally well satisfied.
% Simulation results have shown that the proposed algorithm has lower computational complexity and communication overhead than the optimization-based algorithms and can achieve a high sum rate of terrestrial users while interference power constraints are generally well satisfied.
% \cite{du2024enhancing, du2024diffusion, ho2020denoising, du2024enabling}

% Simulation results have shown that with low computational complexity and communication overhead, the proposed algorithm can achieve a high sum rate of terrestrial users while slightly violating interference power constraints.
% Specifically, we have formulated a problem to maximize the sum rate of the terrestrial users by optimizing the beamforming vectors of the terrestrial BSs under the interference temperature constraints of the primary AUs.

\small
%\section*{Acknowledgement}
%This work was supported in part by
%% the National Natural Science Foundation of China under Grants U1801261, 61571100, and 61801101.
\bibliographystyle{IEEEtran}%By using IEEEtrans, the number can be displayed.

\bibliography{IEEEabrv,CogMultiCellBF}
\end{document}
