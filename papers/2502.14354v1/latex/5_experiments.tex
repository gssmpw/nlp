\section{Experiments} \label{sec:experiments}
\paragraph{Experimental Setup}

We conduct experiments on two widely-used MOA datasets. 
\textbf{BeaverTails} \cite{DBLP:conf/nips/JiLDPZB0SW023} contains AI safety-related questions, aiming for \textit{harmless} and \textit{helpful} LLM responses. 
We utilize the BeaverTails-10K subset and split the training and validation data as 9:1, and utilize an additional split from the BeaverTails-30K dataset as the test data. 
\textbf{HelpSteer} aims to promote response helpfulness, where we focus on two objectives, \textit{correctness}, \ie factuality precision and relevance, and \textit{verbosity}, \ie response length and level of detail. 
Since HelpSteer is not formulated as our definition of $\mathbfcal{D}$, we manually transform the dataset to follow the definition. 
Dataset preprocessing details and statistics can be found in Appendix~\ref{appd A1}. 

For backbone LLMs, we mainly utilize supervised fine-tuned LLaMA-2-7B \cite{DBLP:journals/corr/abs-2307-09288}, denoted as LLaMA-2-7B-sft. 
We fine-tune all the responses in the training dataset to obtain a LLaMA-2-7B-sft for each dataset. We also conduct experiments on Alpaca-7B \cite{alpaca} to show the applicability of SIPO on different LLMs (see more details in Appendix~\ref{appd A3}). 


\paragraph{Compared Methods}
We primarily focus on the comparison with DPO-based MOA approaches.
\begin{itemize}
 \setlength\itemsep{-0.3em}
    \item \textbf{MODPO}~\cite{DBLP:conf/acl/ZhouLS00O024}, a state-of-the-art DPO-based MOA approach which trains DPO models as reward models for $N-1$ objectives, and integrates the weighted reward differences of responses as margins into the DPO loss of the final objective. 
    \item \textbf{DPO soups}~\cite{DBLP:conf/nips/RameCDGSSC23}, the DPO version of model soup, which performing model weight merging on DPO models of each objective by the preference weight.
    \item \textbf{DPO LW}~\cite{DBLP:conf/acl/ZhouLS00O024}, the naive DPO-based MOA baseline, which linearly combines the DPO losses for each objective by the preference weight as the final DPO loss. 
\end{itemize}
In addition, we also include an outstanding decoding-time alignment method \textbf{MOD} \cite{DBLP:conf/nips/ShiCHLHSD24}, which combines the logits of $N$ DPO models by the preference weight for decoding. 
For all compared methods, we utilize six different preference weights $[w, 1-w], w \in \{0, 0.2, 0.4, 0.6, 0.8, 1.0\}$. For HelpSteer, we show the middle four weights for better visualization. 
See full implementation details in Appendix~\ref{appd A3}.  


\paragraph{Evaluation Metrics}
Following the practice of MODPO and MOD, we utilize the standard-released reward models as the ground-truth reward models to evaluate the LLM alignment performance. 
For HelpSteer, we utilize the reward model released by \citet{DBLP:conf/acl/WangLXYDQZZ24}. 
For BeaverTails, we utilize the standard released usefulness and cost reward models. See Appendix~\ref{sec:reward_model_detail} for details. 


\begin{figure}[t]
 \setlength{\abovecaptionskip}{0.05cm}
 \setlength{\belowcaptionskip}{0cm}
    \centering
    \includegraphics[width=\linewidth]{latex/figures/baseline.pdf}
    \caption{Pareto Fronts of compared methods on HelpSteer (left) and BeaverTails (right).}
    \label{fig:main_result}
\end{figure}

\subsection{Results}
Performance comparison on the Pareto Fronts of all compared methods is presented in Figure~\ref{fig:main_result}, with full results for HelpSteer shown in Figure~\ref{fig:baseline_full_helpsteer}.
We can observe that
(1) on both datasets, the Pareto Front of SIPO largely outperforms all baseline methods, demonstrating its effectiveness in achieving superior Pareto Front. 
(2) For DPO-based baseline, MODPO generally outperforms DPO soups and DPO LW, which is in line with the results of MODPO. 
(3) The decoding-based MOD outperforms all DPO-based methods, showing the great potential of LLM to generate outstanding responses through effective decoding strategy. 
(4) SIPO achieves larger performance improvement over MOD on BeaverTails than HelpSteer (\cf Figure~\ref{fig:avg increase}), potentially because BeaverTails has larger proportion of preference conflicts which is tackled by SIPO. 
(5) Particularly, the improvement on BeaverTails is mostly on the \emph{helpful} side, while for HelpSteer both sides improve. This corresponds to the improvement between MOD and DPO-based approaches since the Pareto-optimal response sampling is based on MOD. This might also be related to the stronger conflicts between \textit{helpful} and \textit{harmless}, making simultaneous improvement difficult.



\subsection{In-depth Analysis}

\begin{figure}[t]
 \setlength{\abovecaptionskip}{0.05cm}
 \setlength{\belowcaptionskip}{0cm}
    \centering
    \includegraphics[width=\linewidth]{latex/figures/ablation/4.pdf}
    \caption{Ablation studies on SIPO.}
    \label{fig:ablation_studies}
\end{figure}

\paragraph{Ablation Studies}
To validate the effectiveness of each component within our framework, we conduct the following ablation studies: removing the refinement stage, denoted as \textit{SIPO - refine}, and removing the filtering stage by randomly subsampling the refined responses to the same size, denoted as \textit{SIPO - filter}. 
As shown in Figure~\ref{fig:ablation_studies}, we can observe that 
(1) removing each component in our framework largely decreases the Pareto Front, validating their effectiveness. 
(2) Removing the filtering stage causes larger performance decrease on both datasets than removing the refinement stage. On HelpSteer, the performance of \textit{SIPO - filter} even gets lower than MOD under some preference weights, highlighting the necessity of ensuring response quality to meet the Pareto-optimal criteria. 
(3) The refinement stage has larger improvement on HelpSteer than BeaverTails,  potentially related to the larger improvement by MOD on HelpSteer. 
See Appendix~\ref{appd E} for a case study on SIPO components. 


\begin{figure}[t]
 \setlength{\abovecaptionskip}{0.05cm}
 \setlength{\belowcaptionskip}{0cm}
    \centering
    \includegraphics[width=\linewidth]{latex/figures/ablation/3.pdf}
    \caption{Performance comparison on alternative preference design in re-alignment.}
    \label{fig:preference}
\end{figure}

\paragraph{Rationality of Preference Design} \label{sec:preference_rationality}
We validate the rationality of our preference design as in Section~\ref{sec:method}.  
We consider the following four alternative preferences, denoting the preferred and dispreferred responses for a certain objective as $\textbf{y}_w$ and $\textbf{y}_l$. 
(1) $\textbf{y}_c \succ \textbf{y}_w$, only learning the preference with the preferred response,  
(2) $\textbf{y}_c \succ \textbf{y}_w, \textbf{y}_w \succ \textbf{y}_l$, learning a sequential of preferences,  
(3) $\textbf{y}_w \succ \textbf{y}_l$, ablating the preference of $\textbf{y}_c$, 
The results are shown in Figure~\ref{fig:preference}. We can observe that
(1) $\textbf{y}_w \succ \textbf{y}_l$ achieves the worst performance due to fine-tuning on the conflicting preference, even lower than MOD in HelpSteer. 
(2) $\textbf{y}_c \succ \textbf{y}_w, \textbf{y}_w \succ \textbf{y}_l$ is lower than SIPO, potentially related to fine-tuning on the conflicting preference $\textbf{y}_w \succ \textbf{y}_l$. 
(3) $\textbf{y}_c \succ \textbf{y}_w$ achieves comparable performance on BeaverTails, while lower than SIPO on HelpSteer. We think this is because fine-tuning on $\textbf{y}_c \succ \textbf{y}_w$ sometimes may lead to forgetting on the original $\textbf{y}_w \succ \textbf{y}_l$ preference. Therefore, we incorporate this preference as a non-conflicting $\textbf{y}_c \succ \textbf{y}_l$ in SIPO to avoid forgetting. 


\begin{figure}[t]
 \setlength{\abovecaptionskip}{0.05cm}
 \setlength{\belowcaptionskip}{0cm}
    \centering
    \includegraphics[width=\linewidth]{latex/figures/ablation/2.pdf}
    \caption{Combination of SIPO with different DPO-based MOA baselines.}
    \label{fig:integration}
\end{figure}



\paragraph{Combination with Other DPO-based Approaches} \label{sec:combination_wsoups}
To demonstrate the effectiveness of combining SIPO with other DPO-based approaches, we combine SIPO with DPO soups. 
The results are shown in Figure~\ref{fig:integration}.
We can observe that SIPO largely improves the performance of DPO soups on both datasets, showing its applicability and strong effectiveness on different approaches.



\paragraph{Studies on Resolving Preference Conflicts}
We examine two research questions on preference conflicts.
Firstly, how well does the sampled $\textbf{y}_c$ resolve preference conflicts? 
We compare the average reward of preferred and dispreferred responses with $\textbf{y}_c$. 
Table~\ref{tab:pareto_optimality} shows that $\textbf{y}_c$ has significantly better rewards for both objectives on HelpSteer, demonstrating its Pareto-optimality. On BeaverTails, $\textbf{y}_c$ enhances \textit{harmless} but slightly decreases \textit{helpful} by 0.4, while SIPO still improving the Pareto Front in the \textit{helpful} dimension. However, the improvements are imbalanced between objectives, indicating the need for further optimization.


Second, how effective is SIPO on non-conflicting preferences? We replace conflicting preferences with non-conflicting ones and analyze the results (\cf Figure~\ref{figure:combined}). SIPO does not enhance performance in this scenario and sometimes performs worse than the non-training MOD, confirming its suitability for resolving conflicts.


\begin{table}[t]
 \setlength{\abovecaptionskip}{0.05cm}
 \setlength{\belowcaptionskip}{0cm}
\setlength{\tabcolsep}{2pt}
\renewcommand*{\arraystretch}{0.9}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lll|lll}
\toprule
\multirow{2}{*}{$\textbf{y}$} & \multicolumn{2}{c|}{BeaverTails}     & \multirow{2}{*}{$\textbf{y}$} & \multicolumn{2}{c}{HelpSteer}             \\
                  & $r_1^*$(helpful) & $r_2^*$(harmless) &                   & $r_1^*$(corr) & $r_2^*$(verb) \\\midrule
helpful  & \textbf{3.8}  & -19.9      & corr      & {\ul 2.9}         & 1.6            \\
harmless & -1.0          & {\ul -11.5}        & verb      & 2.7                & {\ul 1.8}        \\
SIPO      & {\ul 3.4}     & \textbf{-0.4}  & SIPO      & \textbf{3.1}      & \textbf{2.3}     \\ \midrule
RI                & -11.75\%         & 96.83\%           & RI                & 6.03\%               & 27.44\%            \\ \bottomrule
\end{tabular}
}
\caption{Average response reward comparison between SIPO and the original responses. Bold font and underline indicate the best and second-best results. RI denotes the relative improvement to the second-best results.}
\label{tab:pareto_optimality}
\end{table}


\begin{figure}[t]
 \setlength{\abovecaptionskip}{0.05cm}
 \setlength{\belowcaptionskip}{0cm}
    \centering
    \includegraphics[width=\linewidth]{latex/figures/ablation/combined.pdf}
    \caption{Performance comparison on Alpaca-7B (\textbf{Left}). Experiments of adapting SIPO on non-conflicting preferences (\textbf{Right}). }
    \label{figure:combined}
\end{figure}


\paragraph{Generalization to Different Backbone LLMs}
To assess SIPO’s effectiveness across LLMs, we use Alpaca-7B as the backbone. Due to its context length limitation, we apply \emph{SIPO - refine}, the closest variation of SIPO. As shown in the left of Figure~\ref{figure:combined}, \emph{SIPO - refine} consistently outperforms all baselines on BeaverTails, demonstrating its effectiveness across different LLMs.


