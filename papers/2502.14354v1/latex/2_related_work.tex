\section{Related Work}

\paragraph{Learning from Human Feedback}
Learning from human feedback is essential for aligning LLMs with human values, enhancing safety, helpfulness, and factual accuracy \cite{DBLP:conf/nips/JiLDPZB0SW023, DBLP:conf/naacl/WangDZASEDSKSK24, DBLP:conf/nips/LinGOXLY024, DBLP:conf/icml/CuiY0YH0NXXL0024}. A key approach is RLHF \cite{DBLP:conf/nips/Ouyang0JAWMZASR22, DBLP:conf/nips/StiennonO0ZLVRA20, DBLP:journals/corr/abs-2204-05862, DBLP:journals/corr/abs-2307-09288}, where a reward model learns human preferences, and RL methods like PPO \cite{DBLP:journals/corr/SchulmanWDRK17} update the LLM accordingly. To improve the efficiency and stability of RL-based methods, DPO-based methods \cite{DBLP:conf/nips/RafailovSMMEF23, DBLP:journals/corr/abs-2402-01306, DBLP:conf/iclr/WangJYLC24, DBLP:conf/aistats/AzarGPMRVC24} bypass reward modeling by directly learning from preference data. More recently, AI-generated feedback has been explored to reduce human labeling efforts \cite{DBLP:conf/icml/0001PMMFLBHCRP24, DBLP:journals/corr/abs-2411-16646}.


\paragraph{Multi-Objective Alignment of LLM}
Human preferences are inherently heterogeneous and better modeled as multi-dimensional rather than a single-dimensional preferences.
RL-based methods \cite{DBLP:conf/nips/RameCDGSSC23, DBLP:journals/corr/abs-2310-11564, DBLP:conf/nips/ZhongMZYC0Q024, DBLP:conf/acl/WangLXYDQZZ24, DBLP:conf/emnlp/WangKSADMGLGDRF24} learns proxy reward model for each objective and update LLMs via RL, often aggregating preferences at the parameter level to reduce computational cost.
DPO-based approaches, such as MODPO \cite{DBLP:conf/acl/ZhouLS00O024}, aim to reduce reliance on multiple proxy rewards and RL optimization while maintaining alignment efficiency, as detailed in Section~\ref{sec:prelim_exp}. 
Besides, decoding-based methods offer alternative MOA strategies, such as logit manipulation \cite{DBLP:conf/nips/ShiCHLHSD24, DBLP:conf/emnlp/LiuZWYQ24, DBLP:journals/corr/abs-2410-08193, chen2025pad} and prompt-based techniques \cite{DBLP:journals/corr/abs-2408-05094}.
Additionally, some studies explore other constraints among objectives \cite{DBLP:journals/corr/abs-2403-02475, DBLP:journals/corr/abs-2408-15313} or conditional generation \cite{DBLP:conf/emnlp/GuoCY0SSCXZL0024, DBLP:conf/icml/0010PLQ00C24, DBLP:journals/corr/abs-2410-08316}, which remain orthogonal to our setting.

\paragraph{Self-Improvement}
LLMs can self-improve \cite{DBLP:journals/corr/abs-2210-11610, DBLP:conf/acl/WangKMLSKH23}, reducing the reliance on external data or feedback through self-data generation and self-feedback \cite{DBLP:journals/corr/abs-2210-11610, DBLP:conf/acl/WangKMLSKH23, DBLP:conf/iclr/PangWLC0Z024, DBLP:conf/icml/YuanPCLSXW24}. This technique has also been integrated with DPO \cite{DBLP:conf/nips/PangYHCSW24, xu2023some, DBLP:conf/icml/0015DYW0J0024}.
For application on MOA, our approach shares similarities with \citet{DBLP:conf/acl/WangLXYDQZZ24} but differs in that we focus on sampling Pareto-optimal responses.