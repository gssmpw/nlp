\section{Introduction}
% P1: alignment 应该是Multi objective. 
Aligning Large Language Models (LLMs) with human preferences \cite{DBLP:conf/nips/Ouyang0JAWMZASR22, DBLP:conf/nips/RafailovSMMEF23} has evolved from single-objective to multi-objective, aiming to comprehensively capture the inherent heterogeneity of human preferences.
Multi-Objective Alignment (MOA) \cite{DBLP:conf/nips/RameCDGSSC23, DBLP:conf/acl/WangLXYDQZZ24, DBLP:journals/corr/abs-2402-02030} has jointly considered multiple human preference objectives, such as safety, helpfulness, factuality, and diversity, to optimize the LLM. 
The optimization outcome of MOA is a set of LLMs optimized under various preference weights across these objectives, forming a (close-to) Pareto Front. 
 

\begin{figure}[ht]
    \centering
    \setlength{\abovecaptionskip}{0.05cm}
    \setlength{\belowcaptionskip}{0cm}
    \includegraphics[width=\linewidth]{latex/figures/intro.pdf}
    \caption{Illustration on the impact of preference conflicts to MOA, and how Pareto-optimal responses can mitigate such issue for superior Pareto Front. }
    \label{fig:intro}
\end{figure}

% P2: alignment的方法分RLHF和DPO两种。DPO的模式。
Existing MOA approaches can be broadly classified into two categories by their optimization strategies. 
Reinforcement Learning (RL)-based approaches \cite{DBLP:conf/nips/RameCDGSSC23, DBLP:conf/acl/WangLXYDQZZ24} learn a proxy reward model for each objective and then update the LLM using RL, targeting at a weighted combination of the proxy rewards. 
Direct Preference Optimization (DPO)-based approaches \cite{DBLP:conf/acl/ZhouLS00O024} 
follow a distinct paradigm, 
where DPO optimization targets are derived for each objective and jointly aggregated under the preference weight. 
% 这句话定义为optimization target 可以吗？有的是loss，有的是model parameter，有的是“reward”
Since DPO-based methods offer advantages in cost and stability over RL-based approaches \cite{DBLP:conf/nips/RafailovSMMEF23}, it has been a promising direction to study MOA via DPO. 


% P3: 但是DPO存在冲突数据的问题，
However, after comprehensively investigated DPO-based MOA (\cf Section~\ref{sec:prelim_exp}), we observe that these approaches are prone to be impacted by the widespread preference conflicts in the training data, which hinders the achievement of superior Pareto Front. 
Given question and a pair of responses, different objectives often favor different responses, resulting in preference conflicts among these objectives. 
These preference conflicts create contradictory optimization targets for different objectives under the aggregation of DPO-based approaches, potentially disrupting the alignment toward each objective and hindering the achievement of superior Pareto Front (see analysis in Section~\ref{sec:prelim_exp}). 
Given the high prevalence of conflicting preferences in existing datasets (\cf Table~\ref{tab:conflict_ratio}), simply discarding these instances in alignment is not a viable solution.


% P4: c 能解决这个问题。
To solve the issue of preference conflicts, we propose to construct \emph{Pareto-optimal} responses (\cf Figure~\ref{fig:intro}). 
Given a pair of responses with conflicting preferences, denoted as $\textbf{a}$ and $\textbf{b}$, where $\textbf{a}$ is better than $\textbf{b}$ in objective $1$ but worse in objective $2$ we propose to construct a Pareto-optimal response $\textbf{c}$, which surpasses $\textbf{a}$ and $\textbf{b}$ on both objectives. 
% Aligning LLMs on $\textbf{a} \prec \textbf{b}$ for $o_1$ and $\textbf{a} \succ \textbf{b}$ for $o_2$ simultaneously may result in contradictory optimization directions for the joint optimization of the two objectives.  
We think learning the preference between $\textbf{c}$ and $\textbf{a}$ for objective $1$ and $\textbf{c} $ and $\textbf{b}$ for objective $2$ not only incurs no preference conflicts, but also guides the LLM toward generating responses that perform well on both objectives, thus achieving a superior Pareto Front. 
To obtain Pareto-optimal responses, it is not advisable to manually write due to the large amount of preference conflict instances. 
Therefore, we consider using automatic approach to obtain Pareto-optimal responses from the LLM itself. 
% 讨论是否要用更大的模型生成y_c. 


% P5: 
To this end, we propose a novel \textbf{S}elf-\textbf{I}mprovement DPO framework towards \textbf{P}areto \textbf{O}ptimality (SIPO), which guides the LLM to self-generate and select Pareto-optimal responses, thereby mitigating preference conflicts and enhancing the Pareto Front. 
After initial alignment, SIPO samples high-quality responses with a self-refinement strategy, which are then evaluated and filtered for Pareto-optimality over original responses. 
Finally, the Pareto-optimal responses are paired with original responses for non-conflicting DPO-based preference fine-tuning. 
SIPO can be easily incorporated with existing DPO-based MOA approaches. 
Experimental results on HelpSteer \cite{DBLP:conf/acl/WangLXYDQZZ24} and BeaverTails \cite{DBLP:conf/nips/JiLDPZB0SW023} demonstrate significant improvement over baseline methods. Our contributions are three-fold:
\begin{itemize}
% \setlength\itemsep{-0.2em}
    \item We identify the negative impact of preference conflicts on achieving superior Pareto Front for DPO-based MOA approaches. 
    \item We propose to construct Pareto-optimal responses to mitigate the issue, and propose a novel framework for automatically generating, selecting and utilizing these responses. 
    \item We conduct extensive experiments to validate the effectiveness of our framework, achieving 2.1 and 3.0 average improvement on the \emph{helpful} and \emph{harmless} rewards of BeaverTails. 
\end{itemize}



 

