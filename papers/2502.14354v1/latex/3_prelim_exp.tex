\section{Preliminary Experiments} \label{sec:prelim_exp}

\paragraph{Background} 
The alignment objectives are denoted as a set of $N$ ground-truth reward functions, $\textbf{r}^*(\textbf{x}, \textbf{y}) = [r^*_1(\textbf{x}, \textbf{y}), ..., r^*_N(\textbf{x}, \textbf{y})]^{\intercal}$. 
The goal of MOA is to align the LLM based on a set of preference weights $\textbf{W} = \{\textbf{w}_m\}_{m=1}^M$. Each preference weight vector $\textbf{w}_m = [w_{m_1}, ..., w_{m_N}]^{\intercal}$ satisfies the constraint $\sum_{i=1}^N w_{m_i}=1$, which balances these objectives. 
Aligning the LLM to a given preference weight entails maximize the weighted reward $\textbf{w}^{\intercal}\textbf{r}^*(\textbf{x}, \textbf{y})$. The resulting set of aligned LLMs form a (close-to) Pareto Front. 

The alignment is typically achieved using a multi-objective preference dataset, $\mathbfcal{D} = \{\mathcal{D}_1, ..., \mathcal{D}_N\}$, where $\mathcal{D}_i = \{(\textbf{x}, \textbf{y}_w, \textbf{y}_l)\}$ represents the preference dataset for objective $i$. 
Here, $\textbf{x}$ is the input, while $\textbf{y}_w$ and $\textbf{y}_l$ denote the preferred and dispreferred responses, respectively. 
Frequently, the inputs and responses remain the same across all preference datasets in $\mathbfcal{D}$, with only the preference labels differing across objectives, as this format simplifies the annotation process for human annotators.
Thus we can reformulate the dataset as $\mathbfcal{D} = \{(\textbf{x}, \textbf{y}_{-   1}, \textbf{y}_1, p_1, ..., p_N)\}$, $p_i \in \{-1, 1\}$ as the label of the preferred response for objective $i$. 

\paragraph{The Impact of Preference Conflicts on DPO-based MOA}
Recently, DPO-based methods, such as MODPO \cite{DBLP:conf/acl/ZhouLS00O024} and DPO soups \cite{DBLP:conf/nips/RameCDGSSC23}, have been introduced to reduce the costs of proxy reward models and RL.
These methods generally follow such a paradigm: they define a DPO optimization target for each objective and then employ an aggregation strategy to combine these targets using $\textbf{w}$. 
The specific optimization targets and aggregation strategies vary across different approaches.
More specifically, DPO soups optimizes a separate LLM for each objective by DPO and then aggregate them at the model parameter level by weight merging. 
MODPO trains DPO LLMs as proxy reward models for certain objectives and aggregates them at the loss level by interpolating the weighted reward differences as margins into the DPO loss function.
The naive baseline, DPO Loss Weighting (LW), computes the DPO loss for each objective and aggregates them at the loss level by a weighted sum. 


However, we observe that this paradigm is easily hindered by preference conflicts in the data. 
Preference conflict refers to the instance where different objectives assign different preference labels, defined as:
$\{(\textbf{x}, \textbf{y}_1, \textbf{y}_2, p_1, ..., p_{N}) | \exists i, j \in [1, N+1],  p_i \neq p_j\}$. 
Aligning on these instances introduces contradictory optimization targets, disrupting the learning for individual objectives and ultimately hindering Pareto Front optimization. 

To illustrate this issue, we take an example on the naive DPO LW method with $N = 2$, where the loss is defined as a weighted sum of the DPO losses on each objective: $\mathcal{L}_{\mathrm{DPO\_LW}} = w_1 \mathcal{L}_1 + w_2 \mathcal{L}_2$. 
With conflicting preferences, \ie $p_1 \neq p_2$, the losses $\mathcal{L}_1$ and $\mathcal{L}_2$ are opposite, $\mathcal{L}_1 = - \mathcal{L}_2$, pulling the optimization in opposing directions. 
As a result, optimizing $\mathcal{L}_{\mathrm{DPO\_LW}}$ leads to conflicting gradient updates, preventing the LLM from effectively aligning with each objective and ultimately degrading the Pareto Front.
This issue extends to other DPO-based MOA approaches and holds for larger values of $N$. To further illustrate this issue, we conduct the following controlled experiment.
% 这儿是不是缺一个citation。

% \vspace{-0.3cm}
% \begin{center}
% \fcolorbox{black}{gray!6}{\parbox{0.98\linewidth}{ 
% \textit{Does preference conflicts in preference data affect the Pareto Front of DPO-based MOA?}
% }}
% \end{center}
\begin{figure}[t]
 \setlength{\abovecaptionskip}{0.05cm}
 \setlength{\belowcaptionskip}{0cm}
    \centering
    \includegraphics[width=\linewidth]{latex/figures/RQ1.pdf}
    \caption{The impact of preference conflicts on Pareto Front optimization. Experiments are conducted on DPO soups with Alpaca-7B. }
    \label{fig:RQ1}
\end{figure}

\paragraph{Evaluation Protocol}
To investigate \textbf{\textit{the impact of preference conflicts on Pareto Front optimization}}, we conduct experiments by controlling the ratio of preference conflicts in alignment. Specifically, we subsample equal-sized subsets from $\mathbfcal{D}$ with 0\%, 30\%, 60\%, and 90\% of conflicting preferences, and compare their optimized Pareto Front.
We examine this problem from multiple perspectives. 
Firstly, we evaluate two prominent DPO-based MOA approaches, MODPO and DPO soups.
Secondly, we utilize two widely-used multi-objective preference datasets with two different pairs of objectives. 
We choose the \textit{correctness} and \textit{verbosity} from HelpSteer,  \cite{DBLP:conf/acl/WangLXYDQZZ24}, \textit{harmless} and \textit{helpful} from BeaverTails \cite{DBLP:conf/nips/JiLDPZB0SW023}. 
Finally, we experiment with different backbone LLMs, including Alpaca-7B \cite{alpaca} and a supervised fine-tuned LLaMA-2-7B \cite{DBLP:journals/corr/abs-2307-09288}. More details on the methods, datasets and backbone LLMs can be found in Section~\ref{sec:experiments} and Appendix~\ref{appd A}. 

\begin{figure}[t]
 \setlength{\abovecaptionskip}{0.05cm}
 \setlength{\belowcaptionskip}{0cm}
    \centering
    \includegraphics[width=\linewidth]{latex/figures/RQ2.pdf}
    \caption{The impact of preference conflicts on Pareto Front optimization. Experiments are conducted on MODPO with Alpaca-7B. }
    \label{fig:RQ2}
\end{figure}


\paragraph{Results on Different Methods and Objectives}

Figure~\ref{fig:RQ1} shows the Pareto Fronts for DPO soups under varying conflict ratios of the alignment data. We also show the average performance decrease over different preference weights for each objective. 
Corresponding results on MODPO is shown in Figure~\ref{fig:RQ2}. 
We can observe that 
(1) as the ratio of conflicts in the training data increases, the Pareto Fronts gradually move downwards, showing significant performance decreases. 
This phenomenon holds for all datasets and methods, which validates the existence of the issue. 
For DPO soups, when the conflict ratio reaches 90\%, the Pareto Front even approaches the performance on the original LLM without alignment (denoted as SFT), showing severe alignment problem. 
(2) All objectives incur significant average performance decreases on both methods. \textit{Helpful} and \textit{harmless} have more significant performance decreases than \textit{correctness} and \textit{verbosity}, which may be related to the more conflicting nature of the definition of these objectives. 
(3) However, reducing the conflict ratio of the data generally hurts the steerability of the Pareto Fronts, meaning that the performance ranges of the two objectives across preference weights get tighter under smaller conflict ratios. We conjecture that controlling the conflict ratio in the data may hurt the versatility of the data, thus hindering the optimization of single objectives toward higher performance under certain preference weights. 


\begin{figure}[t]
 \setlength{\abovecaptionskip}{0.05cm}
 \setlength{\belowcaptionskip}{0cm}
    \centering
    \includegraphics[width=\linewidth]{latex/figures/RQ3.pdf}
    \caption{The impact of preference conflicts on Pareto Front optimization. Results of BeaverTails with DPO soups on LLaMA-2-7B. }
    \label{fig:RQ3}
\end{figure}


\begin{table}[t]
\centering
\setlength{\abovecaptionskip}{0.05cm}
\setlength{\belowcaptionskip}{0cm}
\setlength{\tabcolsep}{2pt}
\resizebox{0.45\textwidth}{!}{
    \begin{tabular}{l|ccc|c}
        \toprule
        \textbf{Dataset} & \multicolumn{3}{c|}{HelpSteer} & \multicolumn{1}{c}{BeaverTails} \\ 
        \textbf{\# Objectives} & 3 & 4 & 5 & 2 \\ \midrule 
        \textbf{Conflict Ratio (\%)}    & 11.86 & 15.89 & 17.94 & 53.83 \\ \bottomrule
    \end{tabular}
    }
\caption{Statistics on the conflict ratio in Helpsteer and BeaverTails datasets.}
\label{tab:conflict_ratio}
\end{table}


\paragraph{Results on Different Backbone LLMs}
To examine the consistency of this issue on different backbone LLMs, we utilize a supervised fine-tuned LLaMA-2-7B as an additional backbone LLM.
The results of BeaverTails on DPO soups is shown in Figure~\ref{fig:RQ3}. 
We can observe that even though LLaMA-2-7B achieves better performance on both objectives than Alpaca-7B (compared with Figure~\ref{fig:RQ1}), the conflict ratio consistently affects the Pareto Front, showing that stronger backbone LLM will also be affected by the preference conflicts, further demonstrating the existence of the issue.  


\paragraph{Statistics on the Percentage of Conflicting Data}
We have conducted statistics on the ratio of preference conflicts in these datasets, as shown in Table~\ref{tab:conflict_ratio}. 
For BeaverTails, we calculate the conflict ratio for the two objectives. 
For HelpSteer, we vary the number of objectives from three to five. 
We can observe that the \textit{helpful} and \textit{harmless} in BeaverTails has more than 50\% of conflict, showing strong conflicting nature. 
Statistics for HelpSteer are all more than 10\%, and increasing the number of objectives further increases the conflict ratio. 
The statistics reveals the severity of the preference conflicts in current datasets, stressing the need for mitigation. 
\label{3.4}