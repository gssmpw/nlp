\section{Method} \label{sec:method}

\begin{figure*}
 \setlength{\abovecaptionskip}{0.05cm}
 \setlength{\belowcaptionskip}{0cm}
    \centering
    \includegraphics[width=\textwidth]{latex/figures/framework.pdf}
    \caption{Illustration of our proposed SIPO framework. }
    \label{fig:framework}
\end{figure*}

In this section, we introduce our SIPO framework (\cf Figure~\ref{fig:framework}), which leverages self-generated Pareto-optimal responses to mitigate the impact of preference conflicts. 
We will introduce the definition of Pareto-optimal responses (ยง~\ref{subsection:pareto-optimal response}), and detail the SIPO framework design (ยง~\ref{subsection:self-pareto dpo framework}, ยง~\ref{subsection:pair_construction}).

\subsection{Pareto-Optimal Responses}
\label{subsection:pareto-optimal response}
To solve the issue of preference conflicts, we resort to Pareto-optimal responses. 
For an instance $(\textbf{x}, \textbf{y}_{-1}, \textbf{y}_1, p_1, ..., p_N) \in \mathbfcal{D}$ with conflicting preferences, the Pareto-optimal responses $\textbf{y}_c$ are defined as those responses that outperform both $\textbf{y}_{-1}$ and $\textbf{y}_1$ across all objectives:
\begin{equation}
\begin{split}
    \textbf{y}_c = \{ \textbf{y}|\forall i, \ r^*_i(\textbf{x}, \textbf{y}) &> r^*_i(\textbf{x}, \textbf{y}_1) \ \text{and}\\ r^*_i(\textbf{x}, \textbf{y}) &> r^*_i(\textbf{x}, \textbf{y}_{-1})\}.
\end{split}
\label{eq:definition_pareto_optimal}
\end{equation}
$\textbf{y}_c$ incurs no preference conflicts with $\textbf{y}_1$ and $\textbf{y}_{-1}$, thereby avoiding the issues outlined in Section~\ref{sec:prelim_exp}. 
$\textbf{y}_c$ also has better quality in terms of all objectives than $\textbf{y}_1$ and $\textbf{y}_{-1}$, also facilitating the achievement of a more optimal Pareto Front.


\subsection{SIPO Framework: Responses Generation}
\label{subsection:self-pareto dpo framework}
Given that human annotation of Pareto-optimal responses is prohibitively expensive and infeasible for large-scale datasets, our SIPO framework is designed to autonomously generate and leverage Pareto-optimal responses. 
We initially align $N$ policy LLMs to capture each objective using DPO, denoted as $\boldsymbol {\Pi} = \{\pi_{\theta_i}\}_{i=1}^N$. 
\begin{multline}
    \theta_i = \arg \min_{\theta} -\mathbb{E}_{\mathbfcal{D}}
    \Bigg[   
    \log \sigma \Bigg( p_i\beta \frac{\pi_\theta(\textbf{y}_1|\textbf{x})}{\pi_{\text{ref}}(\textbf{y}_1|\textbf{x})} \\ - p_i\beta \frac{\pi_\theta(\textbf{y}_{-1}|\textbf{x})}{\pi_{\text{ref}}(\textbf{y}_{-1}|\textbf{x})} \Bigg) \Bigg],
\label{eq:theta_i}
\end{multline}
where $\pi_{\text{ref}}$ denotes the reference LLM.
Then, we aim for generating Pareto-optimal responses with three stages, \emph{Sampling}, \emph{Refinement} and \emph{Filtering}. 


\paragraph{Stage 1: Sampling}
For the sampling stage, we aim to generate diverse high-quality responses based on the aligned policy LLMs. 
To enhance sampling diversity, we apply a set of preference weights $\textbf{W} = \{\textbf{w}_m\}_{m=1}^M$ and generate responses under each $\textbf{w}_m$ for $\textbf{x}$, denoted as $\textbf{y}_m^s$. 
To ensure sampling quality, we utilizing the outstanding decoding-based method MOD~\cite{DBLP:conf/nips/ShiCHLHSD24} to sample responses from $\boldsymbol \Pi$ under a given $\textbf{w}_m$. Denoting the MOD decoding function as $f^d(\cdot)$, 
\begin{align}
    \textbf{y}^s_m = f^d(\boldsymbol \Pi, \textbf{w}_m, \textbf{x}).
\end{align}

\paragraph{Stage 2: Refinement} 
To further enhance the quality of the sampled $\textbf{y}_m^s$, we employ a self-refinement strategy, prompting LLM to review the flaws of $\textbf{y}^s_m$ from the perspectives of different objectives and then revise it. 
Firstly, we employ an evaluator LLM to analyze and generate reviews from different perspectives. This evaluator needs to possess the ability of different objectives, thus we implement MOD on the policy LLMs with a preference weight $\textbf{w}_e$ to mix the objectives. 
\begin{equation}
    \textbf{y}_m^v = f^d(\boldsymbol \Pi, \textbf{w}_e, [\textbf{p}_v, \textbf{x}, \textbf{y}^s_m]),
\end{equation}
where $\textbf{y}_m^v$ is the generated review for $\textbf{y}^s_m$, $\textbf{p}_v$ is the instruction and in-context examples
guiding the review generation. 
Then, we revise the response $\textbf{y}_m^s$ based on $\textbf{y}^v_m$ using the original sampling policy of $\textbf{y}^s_m$ to obtain an enhanced response $\textbf{y}^a_m$. 
\begin{equation}
    \textbf{y}_m^a = f^d(\boldsymbol \Pi, \textbf{w}_m, [\textbf{p}_a, \textbf{x}, \textbf{y}^v_m, \textbf{y}^s_m]),
\end{equation}
where $\textbf{p}_a$ is the instruction and in-context examples. We hope choosing different weights of $\textbf{w}_e$ and $\textbf{w}_m$ for reviewing and rewriting policies can leverage their joint effectiveness. 

\paragraph{Stage 3: Filtering}
After obtaining the sampled responses, we apply a filtering stage to ensure the Pareto-optimality over the original responses $\textbf{y}_1$ and $\textbf{y}_{-1}$, as defined in Eq.~\eqref{eq:definition_pareto_optimal}. 
In the absence of the ground-truth reward functions $\textbf{r}^*(\cdot, \cdot)$, we leverage the implicit reward function from DPO models, \ie $\boldsymbol \Pi$, to estimate the rewards. 
\begin{equation}
    \hat{r}_i(\textbf{x}, \textbf{y}^a_m) = \beta\log\pi_{\theta_i}(\textbf{y}^a_m|\textbf{x}) + \beta\log Z(\textbf{x}),
\end{equation}
where $Z(\textbf{x})$ is a normalization constant independent of the responses, allowing us to disregard it. 
Each $\textbf{y}^a_m$ obtains a set of rewards, $\hat{\textbf{r}}_{\boldsymbol \Pi}(\textbf{x}, \textbf{y}^a_m) = [\hat{r}_1(\textbf{x}, \textbf{y}^a_m), ..., \hat{r}_N(\textbf{x}, \textbf{y}^a_m)]^{\intercal}$. 
Apart from the DPO models in $\boldsymbol \Pi$, we also utilize $M$ additional policy LLMs combined under preference weights $\textbf{W}$ to further calculate the rewards on mixed objectives, denoted as $\hat{\textbf{r}}_{\textbf{W}} (\textbf{x}, \textbf{y}^a_m)$. The combined policy LLMs are obtained via model weight merging following DPO soups~\cite{DBLP:conf/nips/RameCDGSSC23}. 

Finally, we select the Pareto-optimal $\textbf{y}^a_m$ with all rewards $\hat{\textbf{r}}_{ \boldsymbol \Pi}$ and $\hat{\textbf{r}}_{\textbf{W}}$ larger than the original responses. If multiple $\textbf{y}_m^a$ for a single $\textbf{x}$ satisfy such constraints, we choose the one with the largest average reward as $\textbf{y}_c$. 
\begin{multline}
    \textbf{y}_c = \{\textbf{y}_m^a| \hat{r}(\textbf{x}, \textbf{y}^a_m) > \hat{r} (\textbf{x}, \textbf{y}_1) , \text{and}  \\
    \hat{r} (\textbf{x}, \textbf{y}^a_m) > \hat{r}(\textbf{x}, \textbf{y}_{-1}), 
    \ \forall \hat{r} \in \hat{\textbf{r}}_{\textbf{W}} \cup \hat{\textbf{r}}_{ \boldsymbol\Pi} 
    \}.
\end{multline}

\subsection{SIPO Framework: Fine-Tuning}
\label{subsection:pair_construction}
After obtaining $\textbf{y}_c$, we update the policy LLMs to reduce the effect of preference conflicts and improve Pareto Front. 
Firstly, based on Eq.~\eqref{eq:definition_pareto_optimal}, we utilize two preference relationships
$\textbf{y}_c \succ \textbf{y}_{-1, }$ and $\textbf{y}_c \succ \textbf{y}_1$, and construct new preference dataset as $\mathcal{D}^c = \{(\textbf{x}, \textbf{y}_c, \textbf{y}_l)\}$, where $\textbf{y}_l$ represents either $\textbf{y}_{-1}$ or $\textbf{y}_1$. 
These new preferences are non-conflicting and prevent forgetting on the original responses. We validate the rationality of the preference design with experiments in Section~\ref{sec:preference_rationality}. 
Then, we perform DPO fine-tuning on $\mathcal{D}^c$ for policy LLMs. Following \citet{DBLP:journals/corr/abs-2404-19733}, we also add an NLL loss term to prevent forgetting. 
The objective is defined as follows, where $\alpha$ is the weight for NLL loss.
\begin{multline}
    \theta_i^{'} = \arg \min_{\theta} -\mathbb{E}_{\mathcal{D}^c}
    \Bigg[ 
    \log \sigma \Bigg(p_i\beta \frac{\pi_\theta(\textbf{y}_c|\textbf{x})}{\pi_{\theta_i}(\textbf{y}_c|\textbf{x})} \\
    - p_i\beta \frac{\pi_\theta(\textbf{y}_{l}|\textbf{x})}{\pi_{\theta_i}(\textbf{y}_{l}|\textbf{x})} \Bigg) - \alpha \frac{\log \pi_{\theta_i}(\textbf{y}^c | \textbf{x})}{|\textbf{y}^c|}
    \Bigg].
\label{eq:theta_i_prime}
\end{multline}

For final evaluation, we primarily apply the outstanding decoding method MOD on the updated policy LLMs $\boldsymbol \Pi^{\prime} = \{\pi_{\theta_i^{\prime}}\}_{i=1}^N$. 
In our experiments (\cf Section~\ref{sec:combination_wsoups}), we also combine SIPO evaluation with DPO soups to show its adaptability. 
