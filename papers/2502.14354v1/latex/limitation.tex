\section*{Limitation}

\paragraph{Experiments on More Objectives and DPO-based Approaches}
Our experiments is conducted on two objectives for each dataset, and we combine SIPO with MOD and DPO soups. 
We could extend SIPO to more than two objectives per dataset, and to more DPO-based approaches such as MODPO. 
We can also extend SIPO to larger backbone LLMs. 
Due to time and resource limits, we did not conduct these experiments, which we leave as future work. 


\paragraph{More Validation on the Effectiveness of Pareto-optimal Response}
Despite using Pareto-optimal responses, we also consider another potential setting to resolve preference conflicts. 
Given $\textbf{a} \succ \textbf{b}$ on objective 1 and $\textbf{a} \prec \textbf{b}$ on objective 2, we consider sampling two responses, $\textbf{c}$ and $\textbf{d}$, where $\textbf{c} \succ \textbf{a}$ on objective 1 and $\textbf{d} \succ \textbf{b}$ on objective 2. 
$\textbf{c}$ and $\textbf{d}$ are not Pareto optimal responses, but it is quite possible that this setting can improve performance on each objective, thus improve the Pareto Front. 
However, we think that these setting is not as effective as Pareto-optimal responses in pushing Pareto Fronts. 
In the future, we will explore the comparison of this setting with our SIPO. 

\paragraph{Generating Pareto-optimal Responses with Additional Stronger LLMs}
In this work, we employ self-improvement paradigm without resorting to additional human-labeled data or data labeled by stronger LLMs. 
Distilling Pareto-optimal response from stronger LLMs to improve Pareto Front may be another direction in the field of MOA, which we leave as future work. 

\paragraph{Multi-round Iterative Fine-tuning}
Our SIPO performs one round of response generation and fine-tuning due to the cost limits, which can be extended into multi-round iterative generation and fine-tuning. 
It remains an open problem whether Pareto-optimal responses can be sampled after multiple rounds, and whether new problems, such as sampling bias, will arise during the process. It is also an open problem to reduce the cost of SIPO. 

\section*{Ethical Consideration}
The examples shown in Appendix~\ref{appd C} and \ref{appd E} may contain harmful or offensive contents. 