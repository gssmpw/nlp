Vision-Language Models (VLMs) are a class of artificial intelligence models designed to process and integrate both medical imaging data (e.g., radiographs, histopathological slides) and clinical text (e.g., diagnostic reports, physician notes). The evolution of modern VLMs is rooted in advances in natural language processing, particularly the development of Bidirectional Encoder Representations of Transformers (BERT) \cite{devlin2019bertpretrainingdeepbidirectional}. BERT was originally developed for text classification and other language-specific tasks, including clinical text processing and medical literature analysis. Researchers soon recognized its potential for multimodal applications, enabling the integration of medical imaging with textual data within a unified framework. This advancement has paved the way for AI models capable of enhancing diagnostic interpretation, clinical decision support, and medical research.

\subsubsection{\textbf{Expanding BERT to Visual Data:} Several models extended BERT’s text-processing capabilities to incorporate visual data, enabling applications in medical imaging and clinical decision support. A notable early example is VisualBERT  \cite{li2019visualbertsimpleperformantbaseline}, which applies BERT’s transformer architecture to jointly model vision and language tasks. VisualBERT operates by stacking transformer layers, taking both a medical image (e.g., radiographs or histopathology slides) and its corresponding text (such as diagnostic reports or clinical notes) as input, and applying self-attention to learn interactions between the two modalities. . As illustrated in Figure \ref{fig:visualBERTarchitecture}, VisualBERT effectively captures the semantic alignment between visual and textual representations, making it a valuable tool for tasks such as automated medical report generation, radiology interpretation, and clinical decision-making support. }

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{images/visualBERT.png}
    \caption{\textbf{Architecture of VisualBERT\cite{li2019visualbertsimpleperformantbaseline}.} This model integrates visual and textual inputs using a transformer-based architecture. Text tokens (e.g., "No focal consolidation, effusion or pneumothorax") and visual features extracted from the corresponding image are combined, along with positional and segment embeddings. The model is trained with dual objectives: masked language modeling (Objective 1) and visual-text alignment (Objective 2). This allows VisualBERT to effectively learn contextual representations that align both modalities for downstream tasks.}
    \label{fig:visualBERTarchitecture}
\end{figure}

After VisualBERT, numerous other models were developed, each bringing different innovations to vision-language modeling.

ViLBERT (Vision-and-Language BERT) \cite{lu2019vilbertpretrainingtaskagnosticvisiolinguistic} introduces two parallel processing streams for medical images and clinical text. These streams interact through a co-attention mechanism, allowing for more refined cross-modal representations. ViLBERT is particularly effective in tasks such as  visual question answering (VQA), where integrating imaging findings with textual clinical context is essential for diagnostic reasoning.

LXMERT \cite{tan2019lxmertlearningcrossmodalityencoder} 
further refines this approach by explicitly modeling anatomical and pathological relationships within medical images, using region-based visual features to enhance the understanding of complex medical scenes in conjunction with clinical text. It employs separate transformer networks for medical imaging and textual data before aligning the two streams, enabling more precise integration of medical findings with diagnostic reports. 

UNITER \cite{chen2020uniteruniversalimagetextrepresentation} adopts a unified approach, learning a joint embedding space for both medical images and clinical text without relying on separate transformers for each modality. This unified representation enhances cross-modal retrieval and multimodal reasoning, making it particularly useful for tasks such as automated radiology report generation, pathology image interpretation, and clinical decision support. 

Pixel-BERT \cite{huang2020pixelbertaligningimagepixels} 
deviates from using object-level visual features by directly processing raw imaging data at the pixel level, aligning pixel-based features with text representations. This approach eliminates the need for object detection preprocessing steps, allowing for a more generalizable model applicable to various imaging modalities, such as radiology, pathology, and ophthalmology 

% \subsubsection{Video and Multimodal Extensions}    
While the previously mentioned models focus on static medical images, some frameworks extend to video-based medical data. 
VideoBERT \cite{sun2019videobertjointmodelvideo} incorporates temporal and visual aspects of medical videos, such as ultrasound sequences, and endoscopic footage, aligning them with corresponding textual data like procedural notes or transcriptions. Similarly,  VD-BERT \cite{wang2020vdbertunifiedvisiondialog} extends BERT-like architectures to video understanding by integrating frame-level visual features with associated clinical text, enabling applications such as automated surgical video analysis, real-time diagnostic assistance, and video-based medical question answering. 

    
\subsubsection{Specialized VLMs for Specific Domains}While some vision-language models are designed for specific industries, such as 
Fashion-BERT \cite{gao2020fashionberttextimagematching} for fashion-related tasks, specialized VLMs have also emerged in the medical domain. These models integrate visual data from medical imaging with textual descriptions, enhancing applications such as automated diagnosis, clinical decision support, and medical report generation. Similarly, M-BERT (Multimodal BERT) expands the applicability of vision-language integration across multiple medical modalities, improving AI-driven healthcare solutions. Each of these advancements represents incremental progress in bridging the gap between visual and textual understanding, extending the impact of VLMs across diverse fields, including medicine. In the healthcare sector, VLMs have demonstrated significant potential in various applications, such as automatically generating descriptive captions for medical images (e.g., X-rays, MRIs, and CT scans), assisting with diagnosis by interpreting imaging data and providing AI-driven diagnostic suggestions, generating coherent medical reports, and enabling cross-modal retrieval by linking medical images with corresponding clinical notes or retrieving relevant imaging studies based on textual queries.  
