\IEEEPARstart{L}everaging advanced algorithms and neural network architectures like Transformers~\cite{vaswani2023attentionneed}, AI has been empowered with strong reasoning ability and made tremendous progress in recent years. Breakthroughs in model design and training methodologies have allowed machines to excel in complex tasks, including Natural Language Processing (NLP) applications such as language translation, sentiment analysis, and text generation, achieving high accuracy and fostering intuitive human-computer interactions. Similarly, advancements in Computer Vision (CV) have empowered AI to analyze and interpret images, videos, and audio sequences with remarkable precision. In healthcare, Artificial Intelligence (AI) is revolutionizing medicine by enabling data-driven insights, improving diagnostics, and personalizing treatments~\cite{topol2019,Esteva2017,KOUROU20158}. These innovations have enabled significant applications, such as medical imaging analysis, disease diagnosis, pathology, radiology workflow optimization, and surgical assistance, transforming patient care and clinical workflows~\cite{empeek2024,pmc2021}.


The medical field faces unique challenges in data interpretation and decision-making for healthcare specialists; they must analyze diverse types of information including medical imaging (X-rays, MRIs, pathology slides), clinical notes, patient histories, and real-time observations. Medical images are critical for diagnostic checks and measurements, such as identifying anatomical abnormalities, quantifying disease progression, or assessing treatment efficacy. On the other hand, textual data, such as clinical notes, nurse evaluations, and patient histories, provide essential context for screening, understanding symptoms, and documenting disease progression. Textual outputs, such as radiology reports or discharge summaries, are equally vital, as they synthesize findings into actionable insights for clinicians. The complexity and volume of this multi-modal medical data often lead to cognitive overload, impacting the speed and accuracy of diagnoses. Traditional single-modality approaches, which treat images and text separately, fail to capture the intricate relationships between visual findings and clinical context. This limitation underscores the need for integrated vision-language models (VLMs) that can bridge the gap between these modalities\cite{bordes2024introductionvisionlanguagemodeling}, enabling more comprehensive and accurate decision-making in healthcare. This integrated approach promises to enhance clinical decision-making by providing more contextually informed insights and reducing the cognitive burden on healthcare providers.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{images/methods2.png}
    \caption{\textbf{Comprehensive Framework for Medical Vision-Language Models (VLMs)}. \textbf{(a)} Training involves processing diverse inputs such as images, texts, metadata, and historical data, followed by pre-training. \textbf{(b)} Benchmarking is conducted on a variety of medical datasets including GMAI-MMBench, OmniMedVQA, RadBench, and others. \textbf{(c)} Advanced training strategies are employed, such as vision-text alignment, knowledge distillation, masked language modeling, contrastive learning, and parameter-efficient tuning. \textbf{(d)} Evaluation strategies encompass automated metrics like BLEU, ROUGE, BERTScore, and clinical-specific tools like CheXpert Labeler and RadGraph, alongside human evaluation. \textbf{(e)} Integration of VLMs into the medical workflow leverages contextual data to provide actionable insights and improve clinical decision-making.}
    \label{fig:method}
\end{figure*}

However, visual and language provide totally different modalities that are not trivial to be integrated directly. As illustrated in Fig.~\ref{fig:method}, existing works address this challenge through various strategies, including vision-text alignment (in MedViL\cite{devlin2019bertpretrainingdeepbidirectional}, MedCLIP\cite{radford2021learningtransferablevisualmodels}, BioMedCLIP\cite{zhang2024biomedclipmultimodalbiomedicalfoundation}, VividMed\cite{luo2024vividmedvisionlanguagemodel}), knowledge distillation with VividMed\cite{luo2024vividmedvisionlanguagemodel}, masked language modeling (in MedViL\cite{devlin2019bertpretrainingdeepbidirectional} and BioMedCLIP\cite{zhang2024biomedclipmultimodalbiomedicalfoundation}) and contrastive learning in MedCLIP\cite{radford2021learningtransferablevisualmodels}, BioViL\cite{Boecking2022} \& ConVIRT\cite{zhang2022contrastivelearningmedicalvisual}. More recent advancements have introduced additional approaches, such as frozen encoders and Q-Former (e.g., BLIP-2\cite{li2023blip2bootstrappinglanguageimagepretraining}, InstructBLIP\cite{dai2023instructblipgeneralpurposevisionlanguagemodels}), image-text pair learning and fine-tuning (e.g., LLaVA\cite{liu2023visualinstructiontuning}, LLaVA-Med\cite{li2023llavamedtraininglargelanguageandvision}, BiomedGPT\cite{Zhang2024}, MedVInT\cite{zhang2024pmcvqavisualinstructiontuning}), parameter-efficient tuning (e.g., LLaMA-Adapter-V2\cite{zhang2024llamaadapterefficientfinetuninglanguage}), two-stage training (e.g., MiniGPT-4\cite{zhu2023minigpt4enhancingvisionlanguageunderstanding}), and modular multimodal pre-training (e.g., mPLUG-Owl\cite{ye2024mplugowlmodularizationempowerslarge}, Otter\cite{li2023ottermultimodalmodelincontext}) and the Sigmoid Loss for Language-Image Pre-Training (SigLIP)\cite{zhai2023sigmoidlosslanguageimage}. SigLIP replaces traditional softmax-based contrastive learning with a simpler sigmoid loss approach, enabling more efficient and scalable training by treating image-text pair alignment as a binary classification task. These methods aim to establish coherent relationships between visual inputs and textual outputs, enabling models to effectively interpret and generate relevant information across modalities. As a comparison, the contrastive learning-based methods leverage the similarities and differences between paired visual and textual data to enhance model robustness and generalization.

This paper provides a comprehensive review of VLMs and their applications in healthcare. We first discuss how VLMs are constructed by integrating advancements in NLP and computer vision. Next, we summarize key methodologies and advancements in the field, including state-of-the-art models like Qwen-VL\cite{bai2023qwenvlversatilevisionlanguagemodel}, RadFM\cite{wu2023generalistfoundationmodelradiology}, and DeepSeek-VL\cite{lu2024deepseekvlrealworldvisionlanguageunderstanding}. We then explore how VLMs are applied in the medical domain, highlighting their potential to improve diagnostic accuracy, clinical decision-making, and other healthcare tasks. Finally, we conclude by outlining future directions and challenges in the integration of VLMs into healthcare practices.
