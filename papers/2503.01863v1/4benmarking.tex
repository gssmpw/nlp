The progress of VLMs in medicine heavily depends on the availability of high-quality datasets. These datasets, along with established benchmarks, have played a pivotal role in driving both research and practical applications in the field. Several key datasets and benchmarks have already contributed significantly to the development and evaluation of VLMs, shaping the advancements in this domain.

\begin{table*}[ht]
    % \centering
    \caption{\textbf{Benchmarking in Medical VLMs}. This table summarizes key benchmarks in medical VLMs, including their modalities, dataset sizes, tasks, and sources. The benchmarks cover a wide range of medical imaging modalities and tasks, such as classification, segmentation, and VQA}
    \label{tab:benchmarking }
    \begin{tabular}{lllll}
        \toprule
        \textbf{Benchmark} & \textbf{Modality} & \textbf{Size} & \textbf{Task} & \textbf{Source} \\
        \midrule
        CheXpert\cite{irvin2019} & 1 & 224,316 & 14 (Pathology classification tasks) & from 65,240 patients \\
        CheXpert Plus\cite{chambon2024chexpertplusaugmentinglarge} & 1 & 224,316 & 14 (Classification + Localization tasks) & from 187,711 studies and  64,725 patient \\
        MIMIC-CXR-JPG\cite{johnson2019mimiccxrjpglargepubliclyavailable} & 2 & 377,110 & 14 (Pathology classification + Report generation) & from 227,827 imaging studies \\
        Medtrinity-25M\cite{xie2024medtrinity25mlargescalemultimodaldataset} & 3 & 25M & 30+ (Multi-task learning)& from radiology, pathology, and EHRs$^\dagger$ \\
        GMAI-MMBench\cite{chen2024gmaimmbenchcomprehensivemultimodalevaluation} & 38 & 26k & 18 (Diverse medical tasks) & 28 datasets from hospitals \\
        PMC-VQA\cite{zhang2024pmcvqavisualinstructiontuning} & 2 & 1.5M & 4 (Visual Question Answering tasks) & PubMed Central \\
        PathVQA\cite{he2020pathvqa30000questionsmedical} & 1 & 6k & 7 (Visual Question Answering tasks) & Textbook, PEIR \\
        VQA-RAD\cite{Lau2018} & 3 & 3k & 11 (Visual Question Answering tasks) & Teaching cases from Medpix \\
        BESTMVQA\cite{hong2023bestmvqabenchmarkevaluationmedical} & 2 & N/A & 5 (Visual Question Answering tasks) & as X-rays, CT scans, MRIs, and pathology slides. \\
        CT-RATE\cite{hamamci2024developinggeneralistfoundationmodels} & 2 & N/A & 3 (Classification, Segmentation tasks) & from radiology departments \& imaging repositories. \\
        SLAKE\cite{liu2021slakesemanticallylabeledknowledgeenhanceddataset} & 3 & 2k & 10 (Visual Question Answering tasks) & MSD, Chesx-ray8, CHAOS \\
        RadBench\cite{kuo2024radbenchevaluatinglargelanguage} & 6 & 137k & 5 (Image-text understanding tasks) & 13 image-text paired datasets \\
        OmniMedVQA\cite{hu2024omnimedvqanewlargescalecomprehensive} & 12 & 128k & 5 & 73 classification datasets\\
        \bottomrule
    \end{tabular}
    $\dagger$: Electronic Health Records (EHRs)
\end{table*}

\subsection{CheXpert \& CheXpert Plus}
Chest X-rays (CXR)\cite{Wang_2017} are one of the most commonly used imaging techniques in medical diagnostics. They provide a non-invasive and quick method to capture images of the lungs, heart, airways, blood vessels, and bones of the chest. CXRs are essential for diagnosing a variety of conditions, including:
pulmonary diseases \footnote{Pulmonary diseases include pneumonia, tuberculosis, and lung cancer},
cardiac conditions \footnote{Cardiac conditions include heart failure and cardiomegaly},
trauma \footnote{Trauma include fractures of the ribs or injuries to the lungs} and
infections \footnote{Infections related to bronchitis or pleural effusion (fluid in the lungs).}

Due to their widespread use in medical practice, chest X-rays generate a vast amount of clinical data, making them an ideal modality for training machine learning models. This volume of data, coupled with the relative simplicity of interpreting certain conditions on X-rays, makes chest X-rays a prime focus for AI-based research and development, including in the area of VLMs.

CheXpert \cite{irvin2019} is a large-scale dataset widely used in medical imaging research, particularly for chest X-ray analysis. Developed by researchers at Stanford University, it serves as a valuable resource for training and evaluating machine learning models in radiology. The dataset is notable for its size, diversity, and comprehensive labeling of medical conditions observed in chest X-rays. However, CheXpert has certain limitations, most notably the lack of demographic information, which can be critical for developing equitable and generalizable AI models.

To address these limitations, CheXpert Plus \cite{chambon2024chexpertplusaugmentinglarge} was introduced. This enhanced dataset includes 36 million tokens comprising images, radiology reports, demographic details, 14 pathology labels, RadGraph annotations, and pre-trained models for key machine learning tasks. By incorporating these additional elements, CheXpert Plus significantly expands the scope and utility of the original dataset, enabling more robust and inclusive research in medical AI.

\subsection{MIMIC-CXR-JPG}
The MIMIC-CXR-JPG \cite{johnson2019mimiccxrjpglargepubliclyavailable} dataset is a large collection of chest radiographs designed to facilitate research in automated analysis of chest images. It consists of 377,110 chest X-ray images associated with 227,827 imaging studies collected from the Beth Israel Deaconess Medical Center between 2011 and 2016. The dataset includes 14 binary labels indicating the presence or absence of various pathologies derived from NLP tools applied to radiology reports. 



\subsection{Medtrinity-25M}
Medtrinity-25M\cite{xie2024medtrinity25mlargescalemultimodaldataset} is a large-scale multimodal dataset with multigranular annotations for medicine. The dataset is a set of \verb|{image,ROI,description}| where \verb|ROI| (Region Of Interest) is associated with an abnormality and is represented by a bounding box or a segmentation mask, specifying the relevant region within the corresponding \verb|image|. Each of these images is associated with a multigranular textual \verb|description| describing the disease/lesion type and other discoveries as illustrated in the pipeline in Figure \ref{fig:medtrinity-25m}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{images/medtrinity-25m.png}
    \caption{\textbf{Data pipeline of Medtrinity-25M\cite{xie2024medtrinity25mlargescalemultimodaldataset}}. The framework consists of three main components:  (1) Data processing pipeline that integrates ROI localization, metadata integration, and medical knowledge retrieval from established databases like PubMed and STATPEARLS, (2) Multigranular textual description generation powered by MLLM incorporating prompts based on coarse captions and medical knowledge, and (3) Data triplet output displaying the original image with ROI annotation alongside comprehensive multigranular descriptions. The framework leverages classification, report analysis, and knowledge integration to generate detailed medical image interpretations.}
    \label{fig:medtrinity-25m}
\end{figure}

The images are collected from various sources including online resources (Kaggle, Zenodo, Hugging Face, GitHub,etc.), relevant medical dataset research (such as CheXpert). They contain either local or global annotations according to their sources, and 25,001,668 samples spanning 10 modalities and over 65 diseases have been collected. The captions and annotations like masks and bounding boxes from these sources are utilized to construct ROIs and corresponding textual descriptions.

As illustrated by Figure \ref{fig:medtrinity-25m}, the pipeline consists of two stages. The first stage is the data processing, where the lack of domain-specific knowledge is addressed by integrating metadata, ROI locating, and medical knowledge retrieval. These informations processed in the first stage are used in the second stage to prompt LLMs (GPT-4V, LLAVA-Med, LLaMA3) to produce the multigranular textual descriptions.

MedTrinity-25M being a large-scale dataset (25 million image-text pairs), is designed to advance research in medical AI,  providing a rich resource for various tasks in medical image analysis and natural language processing. It can be used to develop and fine-tune large multimodal VLMs capable of understanding both visual and textual medical data.

\subsection{GMAI-MMBench}
GMAI-MMBench\cite{chen2024gmaimmbenchcomprehensivemultimodalevaluation} is a comprehensive multimodal evaluation benchmark towards general medical AI. It features data from 284 datasets across 38 medical imaging modalities and spans 18 clinical-related tasks, 18 medical departments, and 4 perceptual granularities in VQA format. These datasets are organized in a way that allows for flexible and customizable evaluations through a lexical tree, making it easier for researchers to focus on specific medical AI challenges.

The GMAI-MMBench benchmark is constructed in three steps as illustrated in Figure \ref{fig:gmai-mmbench}. The first step consists of data collection and standardization where the datasets are sourced both from Internet searches and hospitals. As standardization, all 2D/3D medical images are converted into 2D RGB images and some additional processing such as expanding all abbreviations,  unifying different expressions for the same target, and merging labels with left and right distinctions. The second step is the label categorization and lexical tree construction where three subjects tailored for the biomedical field are proposed from data; they include clinical VQA tasks, departments, and perceptual granularities. The well-organized structure from this categorization is then used in the third step to generate VQA pairs for labels. 

First, candidate questions are formulated by combining modality, clinical task hints, and perceptual granularity information. For each label, 10 options are selected at random, and GPT-4o is employed to generate 10 candidate questions for each, ensuring diversity in question phrasing and content. After this automated generation process, the questions undergo manual review by medical professionals to ensure relevance, accuracy, and clarity. For the options generation, the process is carefully designed to handle different levels of perceptual granularity in medical images, separating global (image-level) and local (mask, bounding box, or contour-level) views. For the global view, answer options are sourced from other categories in the dataset, ensuring no overlap with the correct answer to avoid ambiguity. In the local view, options are derived from a shared pool based on modality, clinical task, and the specific detail being queried. Second, a question is selected for each image, and a set of answer options (including the correct one) is randomly generated, ensuring a well-structured and diverse set of question-answer pairs for evaluation. Third, manual validation and selection are performed to ensure data quality and balanced distribution.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{images/gmai-mmbench.png}
    \caption{\textbf{Overview of the GMAI-MMBench\cite{chen2024gmaimmbenchcomprehensivemultimodalevaluation} framework} comprising three main components: (1) Data collection and standardization pipeline integrating multiple medical imaging datasets (RadImageNet, ChinaSet, ADAM, Kvasir) with standardized labeling through Medical Subject Heading 2024, (2) Label categorization and hierarchical lexical tree construction organizing clinical VQA tasks across medical departments, and (3) Question-answering generation and selection system producing structured questions with modality-specific options and validation criteria. The framework enables comprehensive evaluation of medical visual question-answering capabilities across different imaging modalities, clinical tasks, and diagnostic scenarios.}
    \label{fig:gmai-mmbench}
\end{figure}

This benchmark has been used to evaluate several LVLMs, including general models and medical-specific models. 29 out of 50 models are picked and evaluated using macro-averaged accuracy (ACC) as the evaluation metric for single-choice questions.For multiple-choice questions, the models are evaluated using both macro-averaged accuracy ($\text{ACC}_{\text{mcq}}$) and recall ($\text{Recall}_{\text{mcq}}$) to account for the proportion of correct answers selected relative to the total predictions and the ground-truth options. This evaluation revealed that medical tasks are still challenging for all LVLMs, since even the most advanced model (GPT-4o) is limited to 54\% accuracy.

\subsection{PMC-VQA}
PMC-VQA (PubMed Central Visual Question Answering)\cite{zhang2024pmcvqavisualinstructiontuning} is a groundbreaking large-scale medical Visual Question Answering dataset, introduced by Zhang et al. in 2023. The dataset comprises 227,117 diverse image-question-answer triplets, extracted from over 40,000 medical research papers in PubMed Central. It features a wide range of medical images, including radiological scans, pathology slides, clinical photographs, and medical illustrations, paired with expert-authored questions and answers. The dataset's strength lies in its comprehensive coverage of various medical domains and question types, from diagnostic interpretations to anatomical identifications and technical analyses.

What sets PMC-VQA apart is its instruction tuning approach for medical visual understanding tasks. The dataset not only provides factual question-answer pairs but also incorporates complex reasoning scenarios that mirror real clinical decision-making processes. With a reported model performance achieving approximately 76\% overall accuracy and clinical correctness of 72\%, PMC-VQA has established itself as a benchmark for evaluating and advancing medical VQA systems. Its applications span clinical decision support, medical education, and research assistance, making it a valuable resource for developing more robust and clinically relevant medical AI systems.

\subsection{Hallucination benchmark}
The pretrained medical VLMs have shown great potentials for VQA tasks but are not tested on hallucination phenomenon in the clinical settings. To solve this problem, a hallucination benchmark in medical VQA \cite{wu2024hallucinationbenchmarkmedicalvisual} have been developed, aimed at evaluating the performance of LLMs in detecting hallucinations during medical VQA. It outlines the importance of minimizing hallucinations in healthcare settings, where inaccuracies could lead to misdiagnoses or inappropriate treatments. The authors modified existing VQA datasets (PMC-VQA, PathVQA and VQA-RAD) to assess model responses to both genuine and nonsensical questions, as well as mismatched image queries. They analyzed the performance of various models, including LLaVA and GPT-4, highlighting that while GPT shows strong overall performance, it poses privacy concerns in clinical settings. The findings suggest that the LLaVA-v1.5-13B model, when combined with an effective prompting strategy (L+D), exhibits the best accuracy and minimal irrelevant predictions, making it suitable for deployment as a visual assistant in healthcare environments.


\subsection{BESTMVQA}
The Benchmark Evaluation System for Medical Visual Question Answering (BESTMVQA) \cite{hong2023bestmvqabenchmarkevaluationmedical} involves several key components designed to address challenges such as data insufficiency and the need for a unified evaluation system. The process begins with data preparation, where users upload their self-collected clinical data. This data is then processed using a semi-automatic tool that extracts medical images and relevant texts for medical concept discovery. A human-in-the-loop framework assists in annotating these medical concepts, which are auto-labeled initially and later verified by professionals. The annotated data is utilized to generate high-quality question-answer pairs with the help of a pre-trained language model. Furthermore, BESTMVQA provides a comprehensive model library containing a variety of state-of-the-art models for users to select from, facilitating ease in evaluating different models against benchmark datasets. Users can conduct experiments with simple configurations, allowing the system to automatically train and evaluate models, ultimately generating comprehensive reports on their performance and applicability in medical practice.

\subsection{CT-RATE dataset}
The CT-RATE dataset \cite{hamamci2024developinggeneralistfoundationmodels} comprises pairs of 3D medical images and corresponding textual reports. Initially, it included 25,692 non-contrast 3D chest scans, which were subsequently expanded to over 14.3 million 2D slices using advanced reconstruction techniques as illustrated in figure \ref{fig:CT-RATE}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{images/CT_RATE.png}
    \caption{\textbf{Overview of the CT-RATE dataset and CT-CLIP \cite{hong2023bestmvqabenchmarkevaluationmedical}}: ( Finetuning CT-CLIP and the validation strategy. a. Illustration of linear probing finetuning method (ClassFine) for CT-CLIP, where a linear layer is incorporated into the vision encoder. b. ClassFine enables multi-abnormality classification but is limited to the classes predefined during finetuning. c. Illustration of CT-CLIP’s open vocabulary finetuning method (VocabFine) for each abnormality. d. VocabFine allows for open vocabulary abnormality classification even after finetuning, although it is constrained to the prompts provided during finetuning. e. Validation: models are trained on the CTRATE dataset and then tested on both an internal CT-RATE validation set and an external dataset ;  f. Comparison: a comprehensive evaluation is performed in multi-abnormality detection across two different cohorts, evaluating CT-CLIP, the two finetuned models, and a fully supervised method.}
    \label{fig:CT-RATE}
\end{figure}

The authors also introduced the CT-CLIP framework, which leverages contrastive language-image pre-training to enable broad applicability without requiring task-specific fine-tuning. This framework excels in multi-abnormality detection and case retrieval tasks, demonstrating robust performance not only on data with similar distributions but also under distribution shifts.

By integrating CT-CLIP's vision encoder with a pretrained large language model (LLM), the authors developed CT-CHAT, a model tailored for 3D chest CT volumes. CT-CHAT outperforms other multimodal AI assistants, such as LLaVa 1.6, LLaVa-Med, and CXR-LLaVa, in generating both concise and detailed responses, handling multiple-choice questions, and producing high-quality radiology reports. The strength of CT-CHAT lies in its ability to utilize 3D spatial information processed by CT-CLIP’s pretrained vision transformer, which captures more intricate anatomical details compared to 2D models. Additionally, the integration of 3D CT volumes with radiology-specific linguistic data from CT-RATE enables CT-CHAT to deliver highly accurate and clinically relevant outputs, surpassing models trained on general-purpose text data.

\subsection{OmniMedVQA}
OmniMedVQA\cite{hu2024omnimedvqanewlargescalecomprehensive} is a large-scale evaluation benchmark designed for medical Visual Question Answering. It includes 118,010 images and 127,995 question-answer items, covering different modalities across more than anatomical regions, sourced from 73 distinct medical datasets. This dataset aims to facilitate the evaluation of Large Vision-Language Models (LVLMs) in medical applications.  

The benchmark encompasses various imaging modalities, including: Magnetic Resonance Imaging (MRI), Computed Tomography (CT), X-Ray, histopathology, fundus photography, digital photography, ultrasound, endoscopy, dermoscopy, Optical Coherence Tomography (OCT), Infrared Reflectance Imaging (IRI) and colposcopy.

\subsection{Evaluation metrics}
\begin{table*}[ht]
    \centering
    \caption{\textbf{Evaluation metrics of Medical VLMs}}
    \begin{tabular}{lll}%{p{1.6cm}p{3cm}p{3cm}}
        \toprule
        \textbf{Evaluation Metrics} & \textbf{Description} & \textbf{Use Case} \\
        \midrule
        BLEU & N-gram overlap for text similarity. & General VQA evaluation. \\
        % \hline
        ROUGE & Recall-based keyword overlap. & Long medical descriptions. \\
        % \hline
        BERTScore & Semantic similarity using BERT embeddings. & Semantic relevance in medical text. \\
        % \hline
        CheXpert Labeler & Rule-based pathology detection. & Chest X-ray report evaluation. \\
        % \hline
        RadGraph & Graph-based entity-relationship evaluation. & Radiology report accuracy. \\
        % \hline
        Clinical Correctness Score & Expert evaluation of diagnostic accuracy. & Clinical report validation. \\
        \bottomrule
    \end{tabular}
    \label{tab:metrics}
\end{table*}
\subsubsection{Medical VQA metrics}
The performance of Visual Language Models (VLMs) on Visual Question Answering (VQA) tasks in the medical domain is evaluated based on how accurately and coherently they generate textual interpretations of medical images in response to specific questions. These metrics assess various aspects of the generated text in relation to a reference answer, measuring factors such as lexical similarity, semantic relevance, and clinical accuracy.

\paragraph{BLEU Score} BLEU measures the n-gram overlap between the model's generated response and a reference answer. While commonly used in general NLP tasks, in medical VQA, it highlights the degree of exact phrase matching, though it may not fully capture semantic relevance in complex medical descriptions.

Although this metric is well-established in NLP and effective for comparing concise, factual responses, they have limitations in capturing clinical significance. As shown by \cite{wieting_etal_2019_beyond}, it fails to assign partial credit for semantically correct answers that differ lexically from the reference, have limited output ranges, and can penalize clinically accurate responses if phrasing differs. This is especially pertinent in medical VQA, where correct clinical information is crucial, even if expressed differently from the reference text.

\paragraph{ROUGE Score}: ROUGE evaluates recall by measuring the overlap of key words or phrases, emphasizing the capture of critical content in the response. For medical VQA, it ensures the model includes essential terms that may be crucial for interpreting medical findings. Contrary to BLEU, ROUGE is better for longer medical descriptions and capture content coverage. But there are still questions about its relevance; since it measures overlap between generated and reference texts based on n-grams, it may fail to capture the nuanced, specialized language of medical contexts. Additionally, it can overlook clinical accuracy, focusing on surface-level similarities rather than the semantic or clinical relevance of terms, which can lead to misleadingly high scores even when hallucinations or inaccuracies are present in generated reports \cite{jiang2024comtchainofmedicalthoughtreduceshallucination}.

\paragraph{BERTScore} This metric leverages pre-trained BERT embeddings to compare the semantic similarity between the generated response and the reference answer. BERTScore is beneficial in medical contexts because it evaluates meaning beyond surface-level text matching, helping to assess the model's understanding of complex medical language. BERTScore can struggle with medical VLMs because it primarily focuses on semantic similarity, often missing finer distinctions crucial for medical accuracy. It may also underperform in scenarios where precise domain knowledge is necessary to assess the relevance of generated content, leading to challenges in accurately evaluating models used for clinical report generation or interpretation of radiological images  \cite{jiang2024comtchainofmedicalthoughtreduceshallucination}.

\paragraph{Clinical Accuracy Score} This metric assesses the clinical correctness of the generated response, going beyond textual similarity. It evaluates whether the response aligns with accurate medical knowledge and terminology, which is essential for medical applications where the precision of information is critical.


\subsubsection{Domain-Specific Metrics}
\paragraph{CheXpert Labeler}The CheXpert Labeler\cite{irvin2019} is a rule-based system designed to evaluate the presence or absence of various pathologies in chest X-ray reports. It automates the annotation process by identifying clinical findings such as pneumothorax, edema, and cardiomegaly. For report generation, this metric can serve as a validation tool by assessing whether generated reports accurately include or omit relevant clinical observations. The CheXpert Labeler has become widely used due to its reliability in automated evaluation for large-scale datasets, such as MIMIC-CXR\cite{Johnson2019}, enabling systematic comparison between generated and reference reports in clinical language models.

\paragraph{RadGraph} RadGraph\cite{jain2021radgraphextractingclinicalentities} is a graph-based evaluation framework designed to capture anatomical and pathological relationships in radiology reports. It structures reports into a directed graph of entities and relationships, representing anatomical locations, conditions, and their attributes. This enables more precise assessment of generated reports, especially for clinical accuracy and relational correctness. RadGraph is useful for models that need to convey not only individual findings but also their relationships, such as locating pathologies in specific anatomical regions, which is crucial in radiology.

\paragraph{Clinical Correctness Score} The Clinical Correctness Score (CCS)\cite{zhang2020radiologyreportgenerationmeets} involves expert human evaluators assessing the diagnostic accuracy and appropriateness of generated reports. This score addresses potential hallucinations and ensures that generated content aligns with medical standards, focusing on whether diagnoses, observations, and recommendations reflect clinical expertise. CCS is invaluable for assessing the real-world applicability of generated reports, as it captures nuances that automated metrics might miss.

\paragraph{Human evaluation metrics}
Human evaluation metrics are critical in the medical domain, especially for tasks such as report generation, where clinical accuracy and contextual relevance cannot be fully captured by automated metrics alone. Key human evaluation metrics commonly used for assessing the quality of medical language models are: (i) Content Quality and Relevance to determine whether the generated report includes the necessary medical findings, diagnoses, and insights relevant to the clinical task \cite{Johnson2019, zhu2024leveragingprofessionalradiologistsexpertise}; (ii) Fluency and Naturalness referring to the grammatical correctness, readability, and flow of the generated report, ensuring it resembles a report written by a clinical professional\cite{qin_song_2022_reinforced}; (iii) Diagnostic Accuracy; (iv) Coherence with Image; (v) Comprehensiveness and Coverage; (vi) etc.


% https://arxiv.org/abs/2312.07867