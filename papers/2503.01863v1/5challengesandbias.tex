The rapid advancement of VLMs in medicine has demonstrated their potential to revolutionize tasks such as medical image analysis, report generation, and visual question answering. However, significant challenges remain that hinder their widespread adoption and effectiveness in clinical settings. These limitations stem from issues such as data scarcity, narrow task generalization, lack of interpretability, ethical concerns, computational resources and the difficulty of integrating VLMs into clinical workflows.


\subsection{Scarcity and quality of medical datasets}
 A critical limitation remains the scarcity of high-quality, task-specific datasets, particularly for specialized applications like visual question answering (VQA) and report generation. For instance, while large-scale datasets such as Medtrinity-25M (25M samples across radiology, pathology, and EHRs) and PMC-VQA (1.5M VQA pairs) demonstrate progress in data volume, many widely used benchmarks—including VQA-RAD (3k samples), SLAKE (2k samples), and PathVQA (6k samples)—remain limited in size and diversity\cite{Lau2018,liu2021slakesemanticallylabeledknowledgeenhanceddataset,he2020pathvqa30000questionsmedical}. These smaller datasets often lack representation of rare conditions or underrepresented modalities, leading to biases in model performance. For example, CheXpert and MIMIC-CXR-JPG focus predominantly on chest X-rays, limiting their utility for training models to interpret MRI or CT scans (Irvin et al., 2019; Johnson et al., 2019). This imbalance is further compounded by the labor-intensive process of medical data annotation, which relies heavily on expert clinicians and radiologists, as seen in datasets like CheXpert Plus\cite{chambon2024chexpertplusaugmentinglarge}.

 \subsection{Narrow scope of task generalization}
 While benchmarks such as GMAI-MMBench (26k samples across 38 modalities) and OmniMedVQA (128k samples spanning 12 modalities) aim to evaluate multi-modal understanding, most models are still trained and validated on modality-specific tasks \cite{chen2024gmaimmbenchcomprehensivemultimodalevaluation,hu2024omnimedvqanewlargescalecomprehensive}. For instance, RadBench (137k samples across 6 modalities) emphasizes image-text alignment but does not fully address the complexity of real-world clinical workflows, where models must integrate heterogeneous data types like EHRs, pathology reports, and imaging studies \cite{kuo2024radbenchevaluatinglargelanguage}. This specialization limits the adaptability of VLMs to cross-modal tasks, such as correlating radiology findings with pathology results, a gap partially addressed by Medtrinity-25M but still underexplored in practice \cite{xie2024medtrinity25mlargescalemultimodaldataset}.

\subsection{Lack of interpretability and trust}
Despite advancements in models like PMC-VQA and Med-Flamingo, which generate detailed answers to medical questions, the reasoning processes behind these outputs are often opaque \cite{Zhang2024, moor2023medflamingomultimodalmedicalfewshot}. Benchmarks such as BESTMVQA and CT-RATE evaluate diagnostic accuracy but lack metrics to assess the clinical plausibility or explainability of model predictions, leaving clinicians skeptical of AI-generated reports \cite{hong2023bestmvqabenchmarkevaluationmedical,hamamci2024developinggeneralistfoundationmodels}. Furthermore, ethical and privacy concerns persist, particularly with datasets sourced from EHRs (e.g., Medtrinity-25M) or hospital repositories (e.g., GMAI-MMBench), where de-identification protocols may not fully eliminate re-identification risks \cite{xie2024medtrinity25mlargescalemultimodaldataset,chen2024gmaimmbenchcomprehensivemultimodalevaluation}.

Many of these models operate as "black boxes," making it difficult for clinicians to understand how they arrive at specific diagnoses or recommendations. For example, while models like MedViLL and BioViL demonstrate strong performance in tasks such as medical image captioning and visual question answering, their decision-making processes remain opaque \cite{Moon_2022, Boecking2022}. This lack of transparency can hinder clinician trust and adoption, particularly in high-stakes medical applications where understanding the rationale behind a diagnosis is critical. Efforts to improve interpretability, such as attention visualization or explainable AI (XAI) techniques, are still in their infancy and require further development to bridge this gap\cite{radford2021learningtransferablevisualmodels, Zhang2024}

\subsection{Ethical and privacy concerns}
The use of patient data for training these models raises issues related to data privacy and compliance with regulations such as HIPAA. For instance, datasets like MIMIC-CXR, while de-identified, still contain sensitive patient information, and their use in training VLMs must be carefully managed to avoid privacy breaches \cite{Johnson2019}. Additionally, there is a risk of perpetuating biases present in the training data, which could lead to disparities in healthcare outcomes for different demographic groups. For example, if a VLM is trained predominantly on data from a specific population, it may underperform when applied to patients from underrepresented groups, exacerbating existing healthcare inequities \cite{Boecking2022,Zhang2024}.

\subsection{Computational resources required}
The computational resources required to train and deploy VLMs are another limitation. Training state-of-the-art models like BLIP-2, LLaVA, and MiniGPT-4 requires substantial computational power, often involving large-scale GPU clusters and extensive training times \cite{li2023blip2bootstrappinglanguageimagepretraining,liu2023visualinstructiontuning,zhu2023minigpt4enhancingvisionlanguageunderstanding}. This makes it challenging for resource-constrained healthcare institutions to adopt these technologies. Furthermore, the need for continuous fine-tuning and updates to keep the models relevant adds to the computational burden, limiting their scalability and accessibility in low-resource settings\cite{Zhang2024,luo2024buildinguniversalfoundationmodels}.

\subsection{Difficulty of integrating VLMs into clinical workflows}
While the developed models show promise in tasks such as medical image analysis and report generation, their real-world application requires seamless integration into existing electronic health record (EHR) systems and clinical workflows. For example, models like Flamingo-CXR and Med-Flamingo have demonstrated the ability to generate radiology reports, but their adoption in clinical practice depends on their ability to provide real-time, actionable insights without disrupting existing workflows \cite{Tanno2024,moor2023medflamingomultimodalmedicalfewshot}. Clinicians may also require additional training to use these tools effectively, further complicating their integration into routine practice.