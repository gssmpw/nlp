VLMs represent a transformative advancement in AI, bridging the gap between visual and textual data to enable multimodal understanding and reasoning. These models leverage the synergy between computer vision and NLP, allowing them to interpret, analyze, and generate insights from complex datasets that combine images and text. In the medical domain, VLMs have emerged as powerful tools for tasks such as medical image analysis, report generation, and visual question answering (VQA). By integrating visual and textual modalities, VLMs can enhance diagnostic accuracy, streamline clinical workflows, and support medical education. This section explores the core concepts underlying VLMs, their state-of-the-art implementations, and their diverse applications in healthcare as summarized in the table \ref{tab:vlm_medicine}.


\begin{table*}[ht]
    % \centering
    \caption{State of the art VLMs and their applications in medicine}
    \label{tab:vlm_medicine}
    \begin{tabular}{p{0.2\textwidth}p{0.25\textwidth}p{0.2\textwidth}p{0.25\textwidth}}
        \toprule
        \textbf{Baseline} & \textbf{Training Approach} & \textbf{Key Benchmarks} & \textbf{Primary Applications} \\
        \midrule
        BLIP-2 \cite{li2023blip2bootstrappinglanguageimagepretraining}, InstructBLIP\cite{dai2023instructblipgeneralpurposevisionlanguagemodels} & Frozen encoders, Q-Former & COCO\cite{lin2015microsoftcococommonobjects}, VQA-Med\cite{Ben2019} & Medical Image Captioning, VQA \\
        \midrule
        LLaVA\cite{liu2023visualinstructiontuning}, LLaVA-Med\cite{li2023llavamedtraininglargelanguageandvision}, BiomedGPT\cite{Zhang2024}, MedVInT\cite{zhang2024pmcvqavisualinstructiontuning} & Image-text pair learning, fine-tuning & VQA-RAD\cite{Lau2018}, SLAKE\cite{liu2021slakesemanticallylabeledknowledgeenhanceddataset} & VQA, Clinical Reasoning \\
        \midrule
        LLaMA-Adapter-V2\cite{zhang2024llamaadapterefficientfinetuninglanguage} & Parameter-efficient tuning & Multi-modal datasets & Multimodal Instruction Following \\
        \midrule
        MiniGPT-4\cite{zhu2023minigpt4enhancingvisionlanguageunderstanding} & Two-stage training & Vicuna\cite{zhu2023minigpt4enhancingvisionlanguageunderstanding} & Advanced Image Understanding \\
        \midrule
        mPLUG-Owl\cite{ye2024mplugowlmodularizationempowerslarge}, Otter\cite{li2023ottermultimodalmodelincontext} & Multimodal pre-training & MIMIC-IT\cite{li2023mimicitmultimodalincontextinstruction} & VQA, Medical Dialogue \\
        \midrule
        Qwen-VL\cite{bai2023qwenvlversatilevisionlanguagemodel} & Three-stage training & MIMIC-CXR\cite{Johnson2019}, CheXpert\cite{irvin2019} & Medical Image Captioning, VQA \\
        \midrule
        MedViLL\cite{devlin2019bertpretrainingdeepbidirectional}, MedCLIP\cite{radford2021learningtransferablevisualmodels}, BioViL\cite{Boecking2022}, BioMedCLIP\cite{zhang2024biomedclipmultimodalbiomedicalfoundation}, ConVIRT\cite{zhang2022contrastivelearningmedicalvisual}, VividMed\cite{luo2024vividmedvisionlanguagemodel}, Flamingo-CXR\cite{Tanno2024}, Med-Flamingo\cite{moor2023medflamingomultimodalmedicalfewshot} & Contrastive learning, Medical image-text pair learning, Unsupervised pre-training, Three-stage training, Fine-tuning & MIMIC-CXR\cite{Johnson2019}, MS-CXR\cite{Boecking2022}, ChestX-ray\cite{Wang_2017}, VinDr-CXR\cite{nguyen2022vindrcxropendatasetchest}, IND1\cite{Tanno2024} & Medical Report Generation, VQA, Radiology Task Performance, Medical Image Classification, Retrieval, Image Captioning \\
        \midrule
        RadFM\cite{wu2023generalistfoundationmodelradiology} & Pre-trained on MedMD & RadBench\cite{wu2023generalistfoundationmodelradiology} & Medical Diagnosis, VQA \\
        \midrule
        DeepSeek-VL\cite{lu2024deepseekvlrealworldvisionlanguageunderstanding} & Three-stage training: adaptor, joint pretraining, fine-tuning & Common Crawl,Web Code, E-books, arXiv Articles, ScienceQA, ScreenQA, etc.   &  Multimodal understanding and reasoning\\
        \bottomrule
    \end{tabular}
\end{table*}

\subsection{Sate-of-the-art VL models}


Recent advancements in VLMs have led to groundbreaking architectures like BLIP-2, LLaVA, and MiniGPT-4. These models excel in tasks such as image captioning, visual question answering, and medical image analysis. This subsection examines the most influential VLMs, their methodologies, and their contributions to multimodal AI.

\subsubsection{\textbf{BLIP-2}}
 Bootstrapping Language-Image Pre-training (BLIP-2) \cite{li2023blip2bootstrappinglanguageimagepretraining} introduces a groundbreaking approach to vision-language pre-training that leverages frozen image encoders and LLMs to enhance performance across various vision-language tasks. This innovative method aims to achieve state-of-the-art results while minimizing trainable parameters, addressing the critical challenge of bridging visual and textual modalities. The research is fundamentally driven by the growing need for efficient multimodal AI systems that can effectively understand and generate language based on visual inputs, particularly in applications such as image captioning, visual question answering, and image-text retrieval.

The methodology of BLIP-2 centers on a two-stage pre-training process utilizing a Querying Transformer (Q-Former). The first stage emphasizes vision-language representation learning with a frozen image encoder, followed by a second stage focused on generative learning with a frozen LLM. This approach incorporates image-text contrastive learning to maximize mutual information between modalities, image-grounded text generation, and image-text matching to refine alignment between visual and textual elements. The methodology's efficiency is enhanced through the use of frozen models, which reduces computational costs and enables larger batch sizes during training. The implementation consistently employs the AdamW optimizer and cosine learning rate decay, with the overall design allowing for more samples per GPU compared to traditional end-to-end methods.

The experimental results reveal BLIP-2's superior performance across multiple benchmarks while maintaining a significantly reduced parameter count compared to existing models. The system demonstrates remarkable achievements in VQA tasks, outperforming models like Flamingo80B despite using 54 times fewer parameters. In image captioning and retrieval tasks, particularly on COCO datasets, BLIP-2 shows competitive performance and exhibits strong zero-shot capabilities in generating accurate text descriptions from unseen images. The model's architecture enables rapid pre-training, requiring less than a week of computational time on single GPU setups, while maintaining high efficiency and generalization across different tasks. The research also acknowledges potential limitations, including risks associated with frozen LLM outputs and the ongoing need for advancement in multimodal AI development.

\subsubsection{\textbf{LLaVa}}
The idea behind LLaVa\cite{liu2023visualinstructiontuning} is to develop an efficient foundation language model termed "LLaVA" (Language and Vision Assistant), which integrates both language understanding and visual comprehension. The primary objective is to enhance the capabilities of language models in multimodal contexts, enabling them to better interpret and generate text based on visual inputs. This involves not only improving performance in traditional language tasks but also expanding functionality to include complex visual reasoning and description tasks.

The authors employed a two-step training process. Initially, they pre-trained the LLaVA model on a filtered dataset comprising 595,000 image-text pairs, sourced from the CC3M dataset. This filtering process involved selecting noun-phrases based on their frequency to ensure diverse representation across concepts in the dataset. The model was pre-trained for one epoch using a learning rate of 2e-3 and a batch size of 128. Following pre-training, the model underwent fine-tuning on a specialized dataset, LLaVA-Instruct-158K, for three epochs with a reduced learning rate of 2e-5 and a batch size of 32. Various optimization techniques, such as using the Adam optimizer with no weight decay and enabling BF16 and TF32, were implemented to balance speed and precision during training.

The results demonstrate that the LLaVA model exhibits significant improvements in responding to multimodal tasks compared to previous models. It effectively generates detailed and contextually appropriate descriptions of images, showcasing an ability to understand and relate visual elements to textual instructions. Performance benchmarks indicate that LLaVA not only maintains high accuracy in traditional language tasks but also excels in complex visual reasoning scenarios, establishing it as a robust tool for applications requiring integration of language and vision.

\subsubsection{\textbf{LLaMa\_Adapter\_v2}}
LLaMA-Adapter\cite{zhang2024llamaadapterefficientfinetuninglanguage}, introduced in early 2023, was one of the pioneering attempts to extend Large Language Models (LLMs) to handle visual inputs through parameter-efficient adaptation. The model utilized a lightweight adapter architecture that could be trained while keeping the base LLaMA model frozen, making it computationally efficient. Its key innovation was the introduction of a perceiver-style architecture that processed visual inputs and injected them into the LLM's attention layers. However, the model faced limitations in complex visual reasoning tasks and struggled with detailed visual instruction following.

Building upon these foundations, LLaMA-Adapter-V2\cite{gao2023llamaadapterv2parameterefficientvisual} emerged as a parameter-efficient visual instruction model designed to enhance the capabilities of LLMs in handling multi-modal reasoning tasks. The goal is to improve the model's ability to follow visual instructions and perform complex visual reasoning, surpassing the limitations of previous models like LLaMA-Adapter and achieving performance closer to that of GPT-4.
The authors augmented the original LLaMA-Adapter by unlocking more learnable parameters and introducing an early fusion strategy to incorporate visual tokens into the early layers of the LLM. They also implemented a joint training paradigm using image-text pairs and instruction-following data, optimizing disjoint groups of learnable parameters. Additionally, they integrated expert models (e.g., captioning and OCR systems) to enhance image understanding without incurring additional training costs. It demonstrated superior performance in open-ended multi-modal instructions with only a small increase in parameters over the original LLaMA model. The new framework showed stronger language-only instruction-following capabilities and excelled in chat interactions. The model achieved significant improvements in multi-modal reasoning tasks, effectively balancing image-text alignment and instruction-following learning targets.

\subsubsection{\textbf{MiniGPT-4}}
MiniGPT-4\cite{zhu2023minigpt4enhancingvisionlanguageunderstanding}
is a vision-language model designed to replicate the advanced multi-modal capabilities of GPT-4. The authors aim to demonstrate that aligning visual features with an advanced LLM can enhance vision-language understanding and generation, similar to GPT-4’s abilities.

MiniGPT-4 aligns a frozen visual encoder with a frozen advanced LLM, Vicuna, using a single projection layer. The model is trained in two stages: initially on a large collection of image-text pairs to acquire vision-language knowledge, and then fine-tuned with a smaller, high-quality dataset to improve language generation reliability and usability.
It exhibits advanced capabilities such as generating detailed image descriptions, creating websites from hand-drawn drafts, and explaining visual phenomena. The model’s performance is significantly improved after the second-stage fine-tuning, demonstrating its potential to achieve vision-language tasks comparable to those of GPT-4.

\subsubsection{\textbf{mPLUG-Owl}}
A modularized multi-modal foundation model known as mPLUG-Owl\cite{ye2024mplugowlmodularizationempowerslarge}, is oriented aims to improve multimodal reasoning and action through advanced model architectures that allow for better comprehension and interaction across different data types. It focuses on tasks such as VQA and interaction in multi-round conversations.
The training procedure involves a two-stage scheme that includes multimodal pre-training and joint instruction tuning. This approach allows the model to develop a nuanced understanding of both visual and textual instructions, enhancing its performance on tasks that require integration of these modalities. The authors conducted quantitative and qualitative evaluations to assess the model's capabilities, comparing it against baseline models like MM-REACT and MiniGPT-4. A series of experiments focused on instruction understanding were performed, indicating that multimodal instruction tuning significantly improves the model's performance. The evaluation metrics included response accuracy and the ability to comprehend complex instructions involving spatial orientation and human behavior, which were systematically analyzed using datasets such as OwlEval.

mPLUG-Owl significantly outperformed baseline models in various multimodal tasks. For instance, in knowledge-intensive question answering, mPLUG-Owl was able to identify movie characters in images more accurately compared to other models. It showed a superior capability in multi-round conversations where it effectively responded to referential questions that required spatial and contextual reasoning. The quantitative analysis indicated that the model achieved the best performance metrics when both multimodal pre-training and joint instruction tuning were applied. Furthermore, the findings highlighted that while text-only instruction tuning improved comprehension, incorporating visual data was crucial for enhancing knowledge and reasoning capabilities.

\subsubsection{\textbf{Otter}}
Otter\cite{li2023ottermultimodalmodelincontext} is a multi-modal model based on OpenFlamingo\cite{awadalla2023openflamingoopensourceframeworktraining}, designed to improve instruction-following and in-context learning abilities. It is trained on the MIMIC-IT\cite{li2023ottermultimodalmodelincontext} dataset proposed in the same paper, which includes instruction-image-answer triplets and in-context examples. The training process involved fine-tuning specific layers while keeping the vision and language encoders frozen, optimizing the model to run on four RTX-3090 GPUs.

Otter demonstrated improved instruction-following and in-context learning capabilities compared to OpenFlamingo. It provided more detailed and accurate descriptions of images and better understood complex scenarios. The model’s performance was evaluated through various experiments, showing significant advancements in visual question answering and commonsense reasoning tasks.


\subsubsection{\textbf{InstructBLIP}}
InstructBLIP\cite{dai2023instructblipgeneralpurposevisionlanguagemodels} is a vision-language instruction tuning framework. A pre-trained BLIP-2 model and introduce an instruction-aware Query Transformer (Q-Former) are used to extract visual features tailored to given instructions. The authors gathered 26 publicly available datasets across 11 task categories, transforming them into instruction tuning format. The model is trained on 13 held-in datasets and evaluated on 13 held-out datasets to assess zero-shot performance. A balanced sampling strategy is employed to ensure synchronized learning progress across datasets.
It achieved state-of-the-art zero-shot performance on all 13 held-out datasets, significantly outperforming BLIP-2 and larger Flamingo models. The model also excels in fine-tuning on individual downstream tasks, such as achieving 90.7\% accuracy on ScienceQA questions with image contexts. Qualitative evaluations demonstrate InstructBLIP’s superior capabilities in complex visual reasoning, knowledge-grounded image description, and multi-turn visual conversations.

\subsubsection{\textbf{VPGTrans}}

a two-stage transfer framework designed to transfer a Visual Prompt Generator (VPG) across different Large Language Models (LLMs), VPGTrans\cite{zhang2023vpgtranstransfervisualprompt}. The first stage involves warming up the projector with a high learning rate to adapt the pre-trained VPG to a new LLM, preventing performance drops. The second stage is vanilla fine-tuning of both the VPG and projector. This approach aims to reduce computational costs and training data requirements compared to training a VPG from scratch.

The VPGTrans framework demonstrated significant efficiency improvements, achieving up to 10 times acceleration for small-to-large LLM transfers and up to 5 times acceleration for transfers between different model types. Notably, it enabled a BLIP-2 ViT-G OPT2.7B to 6.7B transfer with less than 10\% of GPU hours and 10.7\% of training data compared to original training. Additionally, VPGTrans outperformed the original models on several datasets, showing improvements in VQAv2 and OKVQA accuracy.

\subsubsection{\textbf{Qwen-VL}}
The Qwen-VL\cite{bai2023qwenvlversatilevisionlanguagemodel} series represents advanced large-scale vision-language models designed to understand and process both text and images. These models leverage a robust architecture that combines a large language model, a visual encoder, and a vision-language adapter, enabling them to perform a variety of tasks in the realm of vision and language.

Initially, the model undergoes pre-training using a large-scale dataset of image-text pairs, which is refined from an original set of five billion to 1.4 billion cleaned samples, predominantly in English and Chinese. In the second stage, multi-task pre-training is conducted, enhancing the input resolution of the visual encoder and employing interleaved image-text sequences. This phase utilizes a vast array of data for various tasks, including captioning and visual question answering, allowing the model to learn effectively from diverse sources. Finally, the supervised fine-tuning stage focuses on instruction tuning to improve the model's interaction capabilities, employing mixed dialogue data and manual annotations to enhance multi-image comprehension and localization skills. Throughout this process, the model architecture integrates a language-aligned visual encoder and a position-aware adapter, ensuring efficient processing of both visual and textual data, ultimately enabling the Qwen-VL-Chat model to perform a wide range of vision-language tasks with impressive accuracy and fine-grained understanding.

\subsubsection{\textbf{DeepSeek-VL}}
DeepSeek-VL\cite{lu2024deepseekvlrealworldvisionlanguageunderstanding} is an open-source vision-language model developed for real-world applications in vision and language understanding, available in two variants with 1.3B and 6.7B parameters. The model aims to enhance user experience in various scenarios by integrating diverse data and an efficient architecture.

The architecture is built on a diverse dataset, including web screenshots, PDFs, OCR content, and charts, to comprehensively represent real-world scenarios, with instruction-tuning datasets derived from real user interactions to enhance practical usability. The training pipeline involves three stages: initial training of the vision-language adaptor to link visual and textual elements, joint pretraining of the vision-language model, and supervised fine-tuning to refine multimodal capabilities. A modality warm-up strategy dynamically adjusts the ratio of visual and language data during training, preserving linguistic performance while improving multimodal understanding. The model employs a token economy, compressing high-resolution images into 576 tokens to balance visual richness and token efficiency, making it suitable for multi-turn inference. Designed for scalability, DeepSeek-VL plans to integrate Mixture of Experts (MoE) technology to further enhance its multimodal capabilities. It demonstrates exceptional performance across visually-centric benchmarks while maintaining strong language proficiency, surpassing existing generalist models. Additionally, its open-source availability encourages community-driven innovation and research.

DeepSeek-VL demonstrates state-of-the-art or competitive performance across various vision-language (VL) benchmarks at comparable model sizes, showcasing its robust multimodal capabilities. In language-centric tasks, it performs on par with or even surpasses its predecessor, DeepSeek-7B, achieving scores of 68.4 on HellaSwag and 52.4 on MMLU, underscoring its strong linguistic proficiency alongside visual understanding. However, the model exhibits a notable decline in mathematical reasoning tasks, scoring 18.0 on GSM8K compared to DeepSeek-7B's 55.0, highlighting a potential area for improvement. When compared to advanced models like GPT-4V, DeepSeek-VL-7B excels in areas such as Recognition, Conversion, and Commonsense Reasoning, but GPT-4V maintains an edge in logical reasoning tasks, indicating room for further refinement in complex reasoning capabilities. Overall, DeepSeek-VL establishes itself as a highly competitive model in the VL landscape, balancing strong performance across multiple domains while identifying specific areas for future enhancement.

\subsubsection{\textbf{Pre-training VL models with SigLIP}}
The Sigmoid Loss for Language-Image Pre-Training (SigLIP) \cite{zhai2023sigmoidlosslanguageimage} represents a significant shift in vision-language pre-training by replacing traditional softmax-based contrastive learning with a simpler and more efficient sigmoid loss approach. This innovation simplifies distributed loss computation and enhances training efficiency, making it particularly suitable for large-scale VL model training.

In traditional softmax-based contrastive learning, the objective function for training a VL model involves an image encoder $f$ and a text encoder $g$. The goal is to minimize the following loss:
\begin{equation}
    - \frac{1}{2 |\mathcal{B}|} \sum_{i=1}^{|\mathcal{B}|} 
\left( 
\underbrace{\log \frac{e^{t x_i \cdot y_i}}{\sum_{j=1}^{|\mathcal{B}|} e^{t x_i \cdot y_j}}}_{\text{image} \to \text{text softmax}}
+ 
\underbrace{\log \frac{e^{t x_i \cdot y_i}}{\sum_{j=1}^{|\mathcal{B}|} e^{t x_j \cdot y_i}}}_{\text{text} \to \text{image softmax}}
\right)
\end{equation}
Where $x_i=\frac{f(I_i)}{|| f(I_i) ||_2}$ and $y_i=\frac{g(T_i)}{|| g(T_i) ||_2}$ represent the normalized embeddings of the image $I_i$ and $T_i$ respectively. While effective, this approach requires computing pairwise similarities across the entire batch, which can be computationally expensive and complex to implement in distributed settings.

SigLIP addresses these limitations by reformulating the learning problem as a binary classification task. Instead of computing softmax probabilities over all pairs, SigLIP processes image-text pairs independently, assigning positive labels to matching pairs $(I_i, T_i)$ and negative labels to non-matching pairs $(I_i, T_{j\ne i})$. The objective function is then defined as:

\begin{equation}
    - \frac{1}{|\mathcal{B}|} \sum_{i=1}^{|\mathcal{B}|} \sum_{j=1}^{|\mathcal{B}|} 
\log \underbrace{\frac{1}{1 + e^{z_{ij} (-t x_i \cdot y_j + b)}}}_{\mathcal{L}_{ij}}
\end{equation}
Here, $z_{ij}$ is the label for a given image-text pair, where $z_{ij}=1$ for positive pairs and $z_{ij}=-1$ for negative pairs. The sigmoid loss directly optimizes the binary classification task, simplifying the training process and reducing computational overhead. This approach not only improves scalability but also maintains competitive performance in vision-language alignment tasks.



\subsection{Applications}
VLMs are particularly useful for healthcare applications that require the interpretation of visual data, such as medical images. They can also answer questions about this visual data, such as identifying anomalies in the medical data.

\subsubsection{\textbf{MedViLL}}
For the task of report generation from medical images, some models have been proposed in the literature. Medical Vision Language Learner (MedViLL) \cite{Moon_2022} for example, is a medical model based on BERT architecture combined with a novel multimodal attention masking scheme and maximizes the generalization of both vision-language understanding tasks including diagnosis classification, medical image-report retrieval, and medical visual question answering. It also enables vision-language generation task such as radiology report generation. MedViLL is pre-trained on MIMIC-CXR dataset \cite{Johnson2019}, containing 227,835 imaging studies for 65,379 patients presenting the Beth Israel Deaconess Medical Center Emergency Department between 2011–2016.

As illustrated in figure \ref{fig:MedViLL}, The visual embedding consist of extracting features from images using a CNN (ResNET-50). Practically, if $v$ is the flattened feature obtained from the CNN and $l$ the location feature, the final visual feature embedding is $v+l+s_v$ where $s_v$ is the semantic vector shared by all visual feature to differentiate themselves from language embedding. For language feature embedding, BERT \cite{devlin2019bertpretrainingdeepbidirectional} is used. Visual embedding and language embedding are concatenated to form the input of the joint embedding. 

The pre-training step of MedViLL consist of minimizing the negative log-likelihood:
\begin{equation}
    \mathcal{L}_{MLM}(\theta) = - \mathbf{E}_{(v,w) \sim D} \left[ log P_{\theta}(w_m|v,w_m) \right]
\end{equation}
Where $\theta$ is the trainable parameters, $(u,v)$ is a pair of images and its corresponding report. The pre-trained MedViLL achieved the score of complexity of 4.185, 84\% of accuracy and 6.6\% of BLUE4
\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{images/MedViLL.png}
    \caption{\textbf{Architecture of MedViLL \cite{Johnson2019}}. The model combines visual and language embeddings to enable joint representation learning for medical applications. (A) Visual embeddings are generated using random pixel sampling and positional encodings from medical images (e.g., X-rays). (B) Language embeddings incorporate tokens with segment and positional encodings from corresponding reports. Both embeddings are processed in (C) a joint embedding space through a bidirectional auto-regressive self-attention mechanism within a transformer. The model supports two primary tasks: image-report matching and masked language modeling, enabling robust multimodal understanding for clinical applications.}
    \label{fig:MedViLL}
\end{figure}

\subsubsection{\textbf{MedCLIP}}
Several visual language models have gained recognition for their potential in healthcare. Contrastive Language-Image Pre-training (CLIP) \cite{radford2021learningtransferablevisualmodels}  is a powerful open-source model that has shown strong performance on image-text tasks in healthcare, including classifying medical images and generating radiology reports. As illustrated in figure \ref{fig:CLIP}, the contrastive pre-training consists of using ResNet or Vision Transformer(ViT) to extract features from images. The ResNet versions include improvements like anti-aliasing and attention pooling. The text encoder utilizes a Transformer model to convert text into feature representations. It processes text using a byte pair encoding (BPE) with a large vocabulary. The contrastive learning is then the process of learning to predict which text matches which image by maximizing the cosine similarity of correct (image, text) pairs and minimizing it for incorrect pairs. When testing,  the model can classify images into new categories by embedding the names or descriptions of the target classes and comparing them to the image embeddings.

MedCLIP \cite{wang2022medclipcontrastivelearningunpaired} has extended the capabilities of CLIP to the medical domain. Unlike traditional methods that rely on paired image-text datasets, MedCLIP decouples images and texts, significantly increasing the amount of usable training data. It addresses the issue of false negatives by incorporating medical knowledge to create a semantic matching loss, ensuring that images and reports with similar medical meanings are correctly identified. MedCLIP demonstrates superior performance in zero-shot prediction, supervised classification, and image-text retrieval tasks, outperforming state-of-the-art methods with much less pre-training data (20k data). This approach promises to improve the efficiency and accuracy of medical image analysis, supporting better clinical decision-making.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{images/CLIP.png}
    \caption{\textbf{Summary of the approach for CLIP \cite{radford2021learningtransferablevisualmodels}}: Contrastive pre-training aligns image and text embeddings (1), enabling the creation of dataset classifiers from textual labels (2), and facilitating zero-shot predictions by matching image embeddings with textual prompts (3). }
    \label{fig:CLIP}
\end{figure}

\subsubsection{\textbf{BioViL \& BioMedCLIP}}
BioViL (Biomedical Vision-Language Model)\cite{Boecking2022} is a specialized adaptation of CLIP, designed specifically for biomedical tasks involving medical images and text. Its architecture features a CNN image encoder, which generates a grid of local image embeddings, and a text encoder known as CXR-BERT, optimized for radiology report processing. Like CLIP, BioViL fuses image and text modalities into a shared latent space to create joint representations. It is trained on the MIMIC-CXR dataset with data augmentation. A key advantage of BioViL is its reduced reliance on extensive text-prompt engineering, enhancing usability in zero-shot classification tasks. BioViL demonstrates superior performance across multiple downstream tasks compared to other state-of-the-art models, achieving improved segmentation results without needing additional local loss terms or separate object detection networks. In addition, BioViL introduces the MS-CXR dataset, designed for evaluating image-text understanding in the radiology domain, which enhances the availability of resources for future studies.

Similar to MedCLIP, BioMedCLIP\cite{zhang2024biomedclipmultimodalbiomedicalfoundation} uses medical image-text pairs and applies contrastive learning to align the visual and textual modalities. It improves over general CLIP models by incorporating domain-specific medical knowledge.

\subsubsection{\textbf{ConVIRT}}
Contrastive Vision-Language Pre-training (ConVIRT) \cite{zhang2022contrastivelearningmedicalvisual} 
leverages a contrastive learning framework to pre-train models on large-scale medical image-text pairs. It is an unsupervised strategy to learn medical visual representations by exploiting
naturally occurring paired descriptive text. The framework takes input a pair of images $x_v$ and the text sequence $x_u$ to describe the imaging information. The goal is to learn an image encoder $f_v$ (modelled as a CNN) and then transfer it into downstream tasks. A random view $\tilde{x}_v$  is sampled from the inputs using a transformation function from a family of stochastic image transformations. A span $\tilde{x}_u$ is also sampled. Both $\tilde{x}_v$ and $\tilde{x}_u$ are encoded in a fixed-dimensional vector followed by a projection transforming $\tilde{x}_v$ and $\tilde{x}_u$ into $v$ and $u$ respectively. The training step involve two loss: the first is an image-to-text contrastive loss for a pair $i$
\begin{equation}
    l_i^{(v\rightarrow u)} = -log\frac{exp(\langle v_i, u_i \rangle/\tau)}{\sum_{k=1}^{N} exp(\langle v_i, u_k \rangle/\tau)}
\end{equation}

the second is the text-to-image contrastive loss

\begin{equation}
    l_i^{(u\rightarrow v)} = -log\frac{exp(\langle u_i, v_i \rangle/\tau)}{\sum_{k=1}^{N} exp(\langle u_i, v_k \rangle/\tau)}
\end{equation}
The final training loss is then a weighted combination of these two losses averaged over all positive image-text pairs in each minibatch.

ConVIRT is pre-trained on the second version of the public MIMIC-CXR dataset \cite{Johnson2019} and additional image-text pairs collected from the Rhode Island Hospital system. The pre-trained model is evaluated for three medical imaging tasks: image classification, zero-shot image-image retrieval, and zero-shot text-image retrieval. On four medical image classification tasks and two image retrieval tasks, ConVIRT outperformed other prominent in-domain initialization methods, resulting in representations of significantly higher quality. In comparison to ImageNet pretraining, ConVIRT achieved comparable classification accuracy while requiring an order of magnitude less labeled data.

\subsubsection{\textbf{VividMed}}
Previous VLMs have shown their capability to process medical visual data. However, these models typically rely on a single method of visual grounding and are limited to processing only 2D images. This approach falls short in addressing the complexity of many medical tasks, which require more versatile techniques, especially since a significant portion of medical images are 3D. Additionally, the scarcity of medical data further challenges these models. To overcome these limitations, VividMed (Vision Language Model with Versatile Visual Grounding for Medicine)\cite{luo2024vividmedvisionlanguagemodel} has been proposed as a more adaptable solution.

In VividMed, the task is not only to generate responses based on the input image and language instructions but also to identify key phrases $\{r_i\}_{i=1}^k$ in the generated text that refer to ROIs in the image.

The architecture of VividMed, shown in Figure \ref{fig:VividMed}, is built on CogVLM \cite{wang2024cogvlmvisualexpertpretrained} as the base VLM, designed to generate responses based on input images and language instructions.  Special tokens such as \verb|<p>|, \verb|</p>|, \verb|<grd>| and \verb|</grd>| are incorporated to specify the target phrase for grounding and indicate when the model should perform visual grounding. These tokens enable more precise control over the grounding process.

The promptable localization module follows Segment Anything Model (SAM) \cite{kirillov2023segment}, consisting of a vision encoder and a transformer-based decoder. To ground each phrase identified by the VLM, an embedding is generated by extracting the last-layer hidden state of the closing token (\verb|</p>|), which is then passed through an MLP to serve as a prompt for the decoder. The decoder generates bounding boxes or segmentation masks based on this prompt and the encoded image.

However, adapting the SAM mask decoder to output bounding boxes is complex. SAM's mask decoder produces a single binary mask per prompt, which cannot capture multiple instances of the same phrase. A workaround, like merging bounding boxes, results in information loss.

To address this, the authors introduce a new branch in the decoder for instance-specific predictions, inspired by DETR-like methods \cite{carion2020endtoendobjectdetectiontransformers}. They add multiple query tokens, each potentially linked to a unique instance or a dummy negative. The set of ground-truth labels and predictions is padded with dummy negatives to match a predefined token count ( $m$), ensuring coverage of different instances.

During training, the Hungarian algorithm assigns each prediction a unique label to minimize a cost function, $( L_{\text{cost}} )$, which combines a bounding box regression loss $( L_{\text{box}} )$ and a discrimination loss $( L_{\text{disc}} )$. The authors employ $( \ell_1 )$ and GIoU losses for $( L_{\text{box}} $) and focal loss for $( L_{\text{disc}} )$, as used in DINO \cite{zhang2023dino}. For segmentation masks, the loss combines Dice and focal losses.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{images/VividMed.png}
    \caption{\textbf{Architecture of VividMed\cite{luo2024vividmedvisionlanguagemodel}}: Combines a ViT encoder for image embedding and a localization decoder for binary set prediction and spatial region identification, while leveraging a Large Language Model to generate medical descriptions based on multimodal inputs, including mask and box queries.}
    \label{fig:VividMed}
\end{figure}

Since most medical images consist of multiple 2D slices stacked to form a 3D structure, a single-slice X-ray represents a simpler case. To avoid unwanted artifacts from inter-slice interpolation in 3D images, the authors dynamically adjusted model weights based on the image slice count, following methods for universal backbones in medical imaging \cite{luo2024buildinguniversalfoundationmodels}. In the ViT-based vision encoder (Fig. \ref{fig:VividMed}), a maximum patch number $( t_d )$ and base patch size $( P_d )$ are set along the depth, adjusting the effective patch size $( P'_d )$ dynamically according to the input slices. For upsampling, transposed convolutions with a scale factor of 2 are used, disabling upsampling along the depth when feature maps reach the input depth, with kernel weights adjusted by mean pooling to ensure size consistency.

The training procedure of VividMed is conducted in 3 stages: (i) visual grounding pre-training, the model is instructed here to determine whether given targets detection exist on the image and list the target names along with their presence in the respons; (ii) medical visual instruction tuning, dedicated to training the model’s visual understanding and reasoning capabilities for medical images; (iii) alignment, consisting of finetuning the model to align both the visual grounding and medical image understanding abilities
trained by previous stages to unleash the combined strengths.

VividMed is trained on VinDr-CXR, MIMIC-CXR, and CT-RATE for the task of VQA; ROCOv2\cite{R_ckert_2024} dataset for Image Captioning; MIMIC-CXR\cite{Johnson2019} and CT-RATE\cite{hamamci2024developinggeneralistfoundationmodels} for Report Generation. The model is evaluated using BLEU, ROUGE and METEOR over VQA-RAD\cite{Lau2018}, SLAKE\cite{liu2021slakesemanticallylabeledknowledgeenhanceddataset}, VQA-Med\cite{Ben2019} (for VQA tasks) and the test sets of MIMIC-CXR and CT-RATE for Report Generation. As results, ividMed shows non-trivial general improvement over fine-tuned CogVLM and outperforms all other baselines in VQA. For Report Generation, VividMed outperforms all other baselines by a large margin on both datasets.

\subsubsection{\textbf{Flamingo-CXR \& Med-Flamingo}}
Flamingo-CXR\cite{Tanno2024} is a model designed to generate radiology reports for Chest X-rays. As many VLMs, Flamingo-CXR aims to improve patient care and reduce radiologists' workload by automating report generation. It addresses the challenge of evaluating the clinical quality of AI-generated reports by engaging a panel of board-certified radiologists for expert evaluation. 
It was fine-tuned using two large datasets (MIMIC-CXR from a US emergency department and IND1 from in/outpatient settings in India). The evaluation involved comparing AI-generated reports with human-written ones through a pairwise preference test and an error correction task. Radiologists assessed the reports' clinical quality as illustrated in Fig. \ref{fig:Flamingo-CXR}, identifying errors and providing corrections. The model showed that 56.1\% of Flamingo-CXR reports for intensive care were preferable or equivalent to clinician reports, rising to 77.7\% for in/outpatient X-rays and 94\% for cases with no pertinent abnormalities. Both human and AI-generated reports contained errors, with 24.8\% of in/outpatient cases having clinically significant errors in both types. The authors found that clinician-AI collaboration improved report quality, with AI-generated reports corrected by experts being preferable or equivalent to original clinician reports in 71.2\% of IND1 cases and 53.6\% of MIMIC-CXR cases.

Med-Flamingo\cite{moor2023medflamingomultimodalmedicalfewshot}, for its part, is based on OpenFlamingo-9B and is pre-trained on paired and interleaved medical image-text data from publications and textbooks. The training involved constructing a unique dataset from over 4,000 medical textbooks and the PMC-OA dataset, resulting in a comprehensive collection of medical images and text. The model was trained using multi-GPU setups and advanced optimization techniques to handle the complexity and multimodality of medical data. It achieved a great performance in medical VQAs, up to 20\% improvement in clinician ratings over previous models. The model was evaluated on several datasets, including the Visual USMLE dataset, and showed superior performance in generating clinically useful answers. The human evaluation study with clinical experts confirmed that Med-Flamingo’s answers were most preferred by clinicians, highlighting its potential for real-world medical applications.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{images/Flamingo-CXR.png}
    \caption{\textbf{Overview of the human evaluation framework by \cite{Tanno2024}}. \textbf{(a)} Two  evaluation methods for comparing radiology reports from an AI model with those created by human experts. The first method involves a pairwise preference test where an expert chooses which report—AI-generated or human-written—is better for patient care. The second method is an error correction task, where the expert reviews a single report, makes edits, and explains their significance. Additionally, \textbf{(b)} there is an assessment of the AI's effectiveness in an assistive role by comparing an edited AI report to a human-only report, using data from outpatient care in India and intensive care in the U.S., with board-certified radiologists evaluating regional differences.}
    \label{fig:Flamingo-CXR}
\end{figure}

\subsubsection{\textbf{BiomedGPT}}
BiomedGPT\cite{Zhang2024} is an open-source model, lightweight vision–language foundation model designed to perform diverse biomedical tasks. Unlike traditional AI models that are specialized for specific tasks, BiomedGPT aims to be a generalist, capable of interpreting different data types and generating tailored outputs. This model addresses the limitations of existing biomedical AI solutions, which are often heavyweight and closed-source, by offering a versatile and accessible alternative.

BiomedGPT was pretrained using a large and diverse dataset comprising 592,567 images, approximately 183 million text sentences, 46,408 object-label pairs, and 271,804 image-text pairs. The model was fine-tuned on various multimodal tasks, including visual question answering (VQA) and image captioning, using datasets like VQA-RAD, SLAKE, and PathVQA. The performance of BiomedGPT was benchmarked against leading models using recognized metrics from the literature, and human evaluations were conducted to assess its capabilities in radiology visual question answering, report generation, and summarization.

BiomedGPT achieved state-of-the-art results in 16 out of 25 experiments while maintaining a computing-friendly model scale. It demonstrated robust prediction ability with a low error rate of 3.8\% in question answering, satisfactory performance with an error rate of 8.3\% in writing complex radiology reports, and competitive summarization ability with a nearly equivalent preference score to human experts. The study highlights BiomedGPT’s potential in improving diagnosis and workflow efficiency in medical applications, although further enhancements are needed for clinical usability.

\subsubsection{\textbf{RadFM}}
The Radiology Foundation Model (RadFM)\cite{wu2023generalistfoundationmodelradiology} is designed to handle a wide range of clinical radiology tasks by learning from 2D and 3D medical scans and corresponding text descriptions. The authors constructed a large-scale Medical Multi-modal Dataset (MedMD), consisting of 16 million radiology scans and high-quality textual descriptions. The model was pre-trained on MedMD and fine-tuned on a subset called RadMD, which includes 3 million meticulously curated multi-modal samples. The evaluation of RadFM was conducted using RadBench, a comprehensive benchmark covering tasks like disease diagnosis, report generation, and visual question answering. It demonstrated significant improvements across all evaluated tasks compared to existing models like OpenFlamingo, MedVInT, MedFlamingo, and GPT-4V. It excelled in modality recognition, disease diagnosis, medical visual question answering (VQA), report generation, and rationale diagnosis. The model’s performance was validated through both automatic and human evaluations, showing superior results in terms of accuracy, precision, and recall. The study highlights RadFM’s potential to unify 2D and 3D radiologic images and support multiple medical tasks, marking a significant step towards developing a generalist foundation model for radiology.

\subsubsection{\textbf{LLaVa-Med}}
LLaVa-Med\cite{li2023llavamedtraininglargelanguageandvision} enhances medical VQA through the integration of vision-language models. The authors developed a structured approach that involves curating high-quality instruction-following data based on medical images and their corresponding textual descriptions. This process includes filtering a large dataset to focus on single-plot images across common imaging modalities, such as chest X-rays, CT scans, and MRIs. They manually curated few-shot examples to demonstrate effective conversation generation based on provided captions and contextual information extracted from PubMed papers. This structured approach aims to improve the model's ability to generate accurate and contextually relevant responses to visual questions in the medical domain. 
Quantitative results showcased significant improvements in accuracy for both open-ended and closed-ended questions compared to prior methods. For instance, their model variants demonstrated high recall rates and accuracy across various question types, indicating enhanced generalization capabilities. Notably, the LLaVA-Med model achieved new state-of-the-art results, underscoring the effectiveness of their instruction-following data generation strategy and the model's ability to integrate external knowledge for better contextual understanding.

\subsubsection{\textbf{MedVInT}}
The MedVInT\cite{zhang2024pmcvqavisualinstructiontuning} model utilizes a generative learning approach for Medical Visual Question Answering (MedVQA), achieved by aligning a pre-trained vision encoder with a large language model through visual instruction tuning. The model is pre-trained on the proposed PMC-VQA\cite{zhang2024pmcvqavisualinstructiontuning} dataset, which covers various medical modalities and diseases, significantly exceeding the size and diversity of existing datasets. The training process involves fine-tuning the model on established benchmarks such as VQA-RAD\cite{Lau2018} and SLAKE\cite{liu2021slakesemanticallylabeledknowledgeenhanceddataset}, leading to substantial performance improvements. Notably, two versions of MedVInT, named MedVInT-TE and MedVInT-TD, are tailored for different types of questions—open-ended and close-ended, respectively. The model's architecture is designed to leverage domain-specific pre-training, enhancing its ability to interpret medical visuals accurately and generate relevant answers to text-based queries.
For open-ended questions, the accuracy improved significantly, with MedVInT-TE achieving a 16\% increase on VQA-RAD and 4\% on SLAKE when pre-trained. In terms of specific results, the MedVInT-TE version reached an accuracy of 70.5\% on the ImageCLEF\cite{ImageCLEF2024} benchmark, surpassing the previous state-of-the-art accuracy of 62.4\%. Additionally, when compared to models trained from scratch, those utilizing the PMC-CLIP vision backbone showed consistent performance improvements, underscoring the importance of domain-specific pre-training. Overall, the MedVInT model outperformed several existing models, including LLaVA-Med, demonstrating its effectiveness in handling both open-ended and close-ended questions in medical image interpretation.




\subsection{VLMs integration in the medical workflow}
The practical application of VLMs in the medical domain is diverse and profound. VLMs offer capabilities such as multimodal data interpretation, where they integrate textual data (e.g., patient records) with visual inputs (e.g., medical images), enabling enhanced diagnostic and decision-making processes. For instance, these models can assist in generating detailed reports from radiological images or identifying anomalies in pathology slides. Such tools have the potential to improve the efficiency and accuracy of diagnostic workflows, especially in resource-constrained settings \cite{bedi_liu_2024, yiving_jesutofummi_2024}.

In clinical decision support systems, VLMs can interpret complex datasets to predict patient outcomes or recommend treatments. Models like GPT-4V have been used to evaluate dermatological conditions, providing differential diagnoses and triaging recommendations. However, their integration into clinical workflows requires addressing challenges such as bias in predictions, lower performance in rare conditions, and the need for robust privacy protections.

Another critical application is in medical education and training. VLMs can synthesize multimodal datasets to create case simulations for trainees, facilitating experiential learning. For instance, generating synthetic but realistic case scenarios from de-identified datasets can provide students and clinicians with valuable practice.

Despite their promise, integrating VLMs into the medical workflow requires rigorous validation and regulatory approval. Ethical considerations, such as ensuring equity in model performance across diverse patient populations, must also be addressed to avoid perpetuating existing disparities in healthcare access and quality.