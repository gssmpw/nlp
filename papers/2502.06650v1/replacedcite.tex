\section{Related Work}
\label{sec:related work}
\subsection{Contrastive Learning}
Self-supervised contrastive learning has achieved great success in image segmentation.  In contrastive learning, how to construct contrastive samples is the key operation. There are various strategies to select appropriate contrastive samples in existing work. The early sampling strategies ____ use a memory bank and momentum encoder to provide negative samples. Chen ____ select contrastive samples from multiple batches. However, since each pixel in semantic segmentation needs to be classified, the references in ____ extend image-level contrastive learning to pixel-level contrastive learning for semantic segmentation. Another representative strategy is to use the label of annotated images to determine the contrastive pixels ____. 
To further improve the performance, references ____ explore pixel contrastive learning using pseudo-label on semi-supervised semantic segmentation. Unreliable pseudo-labels are used to sample negative pixels for contrastive learning in ____. These semi-supervised methods focus on sampling partial pixels and usually need to design complex sampling mechanisms in selecting contrastive samples. Chen and Lian ____ simply view the same class's pixels as prototypes, which ignores semantic information of intra-prototypes. To alleviate this problem, a feasible strategy is to construct contrastive samples using signed distance map of the model's prediction and generate discriminative prototypes by fully utilizing all pixels of the image in contrastive learning.

\begin{figure*}[htbp]
	\centerline{\includegraphics[width=6.4in]{fig02}}
	\caption{Overview of the PCCS. Arrows of different colors indicate the processing flow of different data. PCCS includes three modules, they are prototype contrastive learning module, prototype guide prototype module, and uncertainty-guided consistency learning module. The prototype contrastive learning module obtains contrastive samples from the feature map of the encoder and performs uncertainty-weighted prototype contrastive consistency loss {$l_{pc}$}. The prototype guide prototype module can enhance the diversity of prototype and improve generalization ability by aux loss {$l_{aux}$}. 
		Uncertainty-guided consistency learning module enforces the model to make a consistent prediction for the output of two branches and reduce the uncertainty of the prediction by  uncertainty-consistency loss {$l_{c}$}.}
	\label{fig:main_arch}
\end{figure*}

\subsection{Semi-supervised medical image segmentation}
To reduce the burden of manual annotation, many semi-supervised medical image segmentation methods have been proposed, which use a few labeled data and a large amount of unlabeled data. Existing semi-supervised methods mainly include pseudo-label learning ____, collaborative training ____ and consistency learning ____, entropy minimization methods ____. Pseudo-label methods view predictions of the model for unlabeled images as pseudo-labels and then use the predicted pseudo-labels to supervise the model. 
Reference ____ proposes a strategy that pseudo-labels and network parameters are updated iteratively. Adversarial learning is introduced to medical image segmentation in ____, encouraging the segmentation output of unlabeled data to be similar to the labeled data. Entropy minimization method ____ believes that high-quality prediction results should have smaller entropy, so it performs model learning by minimizing the information entropy of the predicted probability distribution. Co-training ____ assumes that multiple decision views contain complementary information so that different classifiers are designed to learn different views to improve segmentation performance.



\subsection{Consistency Learning}
The effectiveness of consistency learning is based on the assumption that data points close to each other are likely to be from the same class. Based on this assumption, the output results of the model should make consistent predictions even if the input images have various perturbations, such as input perturbations, model perturbations, and so on.
The references in ____ construct different perturbed images through data augmentation for consistency learning. Luo ____  obtains perturbed versions at different feature scales from the decoder. Some others ____  perform consistency learning across multiple decoders in different tasks. The above methods make great progress in semantic segmentation. However, they do not take the prototype's contrastive consistency into consideration, leading to degrading segmentation performance.