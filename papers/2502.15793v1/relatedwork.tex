\section{Background and related works}
In this section, we discuss advancements in anomaly detection and multimodal learning for smart power grids. We first explore early anomaly detection methods, focusing on detecting, classifying, and localizing anomalies to ensure grid stability. Then, we introduce the MS-SVDD, which enhances one-class classification by optimizing a shared subspace for data from multiple modalities.
\subsection{Early anomaly detection in smart power grids}
Anomaly detection is only a small part of the whole anomaly management process and should be regarded within the frame of a more general framework. In \cite{zheng2021psml}, authors consider three critical questions issued by the System Operators for handling events and anomalies: (i) When is an event happening? (ii) What type of event is happening? (iii) Where is the source that caused this event?  
These three questions set up a comprehensive procedure by splitting anomaly management into three steps:

\begin{enumerate}
    \item Detect that an anomaly occurs as soon as possible: Event Detection, or ED.
    \item Classify the event that occurs as a type of anomaly: Event Classification, or EC.
    \item Localize the cause of the event in the whole power grid: Event Localization, or EL.
\end{enumerate}
Here, we only consider the ED problem, and handle it in the frame of early anomaly detection. Early classification of time series, or early anomaly detection, is a specific OCC problem where the primary goal is to classify an incomplete time series as soon as possible while ensuring a good level of accuracy \cite{9207873}. In other words, if $\tau_1$ is the starting time of the anomaly, $\tau_2$ is the ending time of the anomaly, and $T$ is the current time, we aim to classify correctly a time series as abnormal while minimizing $T - \tau_1$ (see Fig. \ref{fig:early_detection}). We should also ensure that the instant of classification $T$ respects the constraint $\tau_1 \leq T \leq \tau_2$. Otherwise, the classification would be considered a false trigger.

The quality of early classification is evaluated through three important metrics:  \textbf{earliness}, \textbf{reliability}, and \textbf{interpretability}. Guaranteeing the best efficiency within this frame implies maximizing all three metrics while considering several constraints and correlations
\begin{itemize}
    \item Earliness will often increase at the cost of reliability: a very sensitive model will provide a good earliness but will often generate false positives.
    \item Power grid anomalies can have many roots: branch or bus tripping, branch or bus faults, short circuits, and transient failures. This diversity leads to several measurement profiles and makes it harder to find a versatile model to detect all types of anomalies.
    \item To ensure reliability, the model has to be robust to noise. Noisy measurements should not lead to false positives.
    \item The huge amount of sensors and data from different quantities measured in the infrastructure system also makes the problem highly dimensional. We thus need to address the curse of dimensionality.
    \item Finally, interpretability is quite low for several popular models \cite{Nazir202163} but is critical in our case, as the corrective operations will influence the whole power grid.
\end{itemize}
% We consider the following hierarchy for objectives: RELIABILITY>EARLINESS>INTERPRETABILITY (interpretability is always guaranteed by the fact that we use MS-SVDD). -> explain that later?

\begin{figure}[t]
	\centering
    \includegraphics[scale=0.4]{early_detection.png}
		%\vspace{-2cm}
		%\rule{35em}{0.5pt}
	\caption{Anomaly detection over an incomplete time series, where $\tau_1$ is the beginning of the anomaly, $\tau_2$ is the end of the anomaly, and $T$ is the current time.}
	\label{fig:early_detection}
\end{figure}

Considering the critical need for stability in power grids and the challenges aroused by the stochastic nature of these new sources, early anomaly detection in electrical systems has been an important field of study in literature. In \cite{6808416}, the authors use a dimensionality reduction approach on synchrophasor data to set up an online application to detect events on a power grid. In \cite{SABER20201113}, a threshold-free searching scheme is proposed for faulted branch identification in multi-end lines. 

When a disturbance is detected, it has to be cleared in a specific time before the system becomes unstable. This limit is called Critical Clearing Time (CTT) and ranges between $100$ ms and $400$ ms depending on the estimations \cite{zheng2021psml} \cite{Wu2019} \cite{Banjar-Nahor20181183}. By evaluating the earliness of our model, we will be able to determine if an anomaly can be cleared within the CCT or not. 

\subsection{Multimodal Subspace Support Vector Data Description and applications}

MS-SVDD models are based on Support Vector Data Description (SVDD), whose aim is to find a hypersphere of minimal radius that separates target class instances from outliers \cite{Tax2004SupportVD}. This process has been first improved through subspace learning by projecting data into a lower-dimensionality space where we can find a better boundary, leading to Subspace Support Vector Data Description, or S-SVDD \cite{8545819}. Multimodal learning has then been leveraged to find a shared subspace that optimizes the boundary for data from different modality spaces (see Fig. \ref{fig:mssvdd}). 

Let us assume that the elements to be modeled lay into $M$ different modalities of dimensionality $D_m \in \mathbb{N^{*}}$, with $m \in \{1, ..., M\}$. Each instance is represented in all the $M$ modalities, and the set of instances into one modality is described as $\mathbf{X}_m = [\mathbf{x}_{m,1}, \mathbf{x}_{m,2}, ... \mathbf{x}_{m,N}]$, with $\mathbf{x}_{m,i} \in \mathbb{R}^{D_m}$ the instance nÂ°$i$ in modality $m$, and $N$ the number of instances in the dataset.
The goal of the MS-SVDD algorithm is to find a projection matrix $\mathbf{Q}_m \in \mathbb{R}^{d \times D_m}$ for each modality, in such a way that we can project every instance into a shared $d$-dimensional subspace optimized for OCC. This projection is done using  
\begin{equation}\label{eq:Y_i}
\mathbf{y}_{m,i} = \mathbf{Q}_m \mathbf{x}_{m,i} \:\:,\forall m \in \{1,\dots,M\} \:\:, \forall i \in \{1,\dots,N\}
\end{equation}
where $\mathbf{Y}_m = [\mathbf{y}_{m,1}, \mathbf{y}_{m,2}, ... \mathbf{y}_{m,N}]$ is the matrix containing all the instances of modality $m$ projected into the shared $d$-dimensional subspace.
\begin{figure}[t]
    \centering
    \includegraphics[scale=0.75]{mssvdd.png}
    \caption{Illustration of MS-SVDD with two modalities. The shared subspace is constructed using the positive instances information.}
    \label{fig:mssvdd}
\end{figure}
We aim to minimize the volume of the hypersphere constructed around the training data into the $d$-dimensional subspace, under the constraint that most of the instances must lay into the hypersphere i.e.
\begin{equation}\label{eq:F_R_a}
\min F(R,\textbf{a}) = R^2 + C\sum_{m=1}^{M}\sum_{i=1}^{N} \xi_{m,i}\nonumber
\end{equation}
s.t.
\begin{eqnarray}\label{eq:F_R_a_st}
\| {\mathbf{Q}_m\mathbf{x}_{m,i}} - \mathbf{a} \|^2_{2} \le R^2 + \xi_{m,i},\\
\xi_{m,i} \ge 0,\nonumber \\
\forall m \in \{1,\dots,M\}, \forall i \in \{1,\dots,N\} ,\nonumber 
\end{eqnarray}
where $R$ is the radius, $\mathbf{a} \in \mathbb{R}^d$ is the center of the hypersphere, $\xi_{v,i}$ are slack variables, and $C$ controls the outliers in the training set \cite{10081907}. We update $\mathbf{Q}_m$ iteratively with the equation 
\begin{equation}\label{eq:Q_m_update}
\mathbf{Q}_m \leftarrow \mathbf{Q}_m - \eta \Delta L_m,
\end{equation}
where $\Delta L_m$ is the gradient of the Lagrangian of Eq. \eqref{eq:F_R_a_st} for modality $m$. We calculate it with
\begin{equation}\label{eq:Delta_L}
\Delta L_m = \frac{\partial L}{\partial \mathbf{Q}_m} = 2\sum_{i=1}^{N} \alpha_{m,i} \mathbf{Q}_m \mathbf{x}_{m,i} \mathbf{x}_{m,i}^T
- 2\sum_{i=1}^{N}\sum_{j=1}^{N}\sum_{n=1}^{M} \mathbf{Q}_n \mathbf{x}_{n,j} \mathbf{x}_{m,i}^T \alpha_{m,i} \alpha_{n,j} + \beta\Delta \omega,
\end{equation}
where $\boldsymbol{\alpha} \in \mathbb{R}^{M \times N}$ is a matrix containing the Lagrangian coefficients, $L$ is the Lagrangian function of the problem, $\Delta \omega$ is the derivative of the regularization term with respect to $\mathbf{Q}_m$, and $\beta$ is a regularization parameter which controls the significance of this term. $\omega$ embeds the covariance of data from different modalities in the $d$-dimensional subspace, and is expressed in its general form as
\begin{equation}\label{eq:w_0123}
\omega = \sum_{m=1}^{M}  \text{tr}(\mathbf{Q}_m\mathbf{X}_m \boldsymbol{\nu}_m \boldsymbol{\nu}^T_m \mathbf{X}^T_m\mathbf{Q}^T_m),
\end{equation}
where $\boldsymbol{\nu}_m \in \mathbb{R}^N$ is a vector constructed from elements of $\boldsymbol{\alpha}_m$ with Eq. \eqref{eq:nu_1}, \eqref{eq:nu_2}, \eqref{eq:nu_3}, or \eqref{eq:nu_4}.
An alternative formulation allows us to cross modalities two-by-two such as
\begin{equation}\label{eq:w_456}
\omega_c = \sum_{m=1}^{M} \sum_{n=1}^{M}  \text{tr}(\mathbf{Q}_m\mathbf{X}_m \boldsymbol{\nu}_m \boldsymbol{\nu}^T_n \mathbf{X}^T_n\mathbf{Q}^T_n).
\end{equation}
By setting up different values for $\mathbf{\nu}_m$, we can create several regularizers for the training of the model. To create our different regularizers with $m \in \{1, ..., M\}$, we use
\begin{eqnarray}
\boldsymbol{\nu}_m = \mathbf{0}_N \label{eq:nu_1}\\
\boldsymbol{\nu}_m = \mathbf{1}_N \label{eq:nu_2}\\
\boldsymbol{\nu}_m = \boldsymbol{\alpha}_m \label{eq:nu_3}\\
\boldsymbol{\nu}_m = \boldsymbol{\lambda}_m\label{eq:nu_4}
\end{eqnarray}
where $\mathbf{0}_N$ is a null vector of size $N$, $\mathbf{1}_N$ is a vector of size $N$ filled with $1$ and $\boldsymbol{\lambda}_m$ is a vector having the elements of $\boldsymbol{\alpha}_m$ that are smaller than C, with values corresponding to the outliers ($\alpha_{m,i}>C$) replaced by zeros. We note that with $M=1$, Eq. (\ref{eq:w_0123}) and (\ref{eq:w_456}) are equivalent to the regularizers of S-SVDD.

We implement the Non-linear Projection Trick (NPT) as an alternative to the kernel trick for non-linear data description \cite{6584012}. For each modality, the NPT kernel matrix is calculated as follows
\begin{equation}\label{eq:kernel_npt}
[\mathbf{K}_{m}]_{ij} = \exp  \left( \frac{ -\| \mathbf{x}_{m,i} - \mathbf{x}_{m,j}\|_2^2 }{ 2\sigma^2 } \right),
\end{equation}
where $\sigma$ is a hyperparameter scaling the distance between the instances $\mathbf{x}_{m, i}$ and $\mathbf{x}_{m, j}$. This kernel matrix is then centralized with
\begin{equation}\label{eq:kernel_npt_centr}
    \hat{\mathbf{K}}_m = (\mathbf{I}_N - \frac{1}{N}\mathbf{1}_N\mathbf{1}^T_N)\mathbf{K}_m(\mathbf{I} - \frac{1}{N}\mathbf{1}_N\mathbf{1}^T_N),
\end{equation}
where $\mathbf{1}_N \in \mathbb{R}^N$ is a vector full of ones and $\mathbf{I}_N$ is the identity matrix of size $N\times N$. We can finally compute the matrix containing the data in the kernel space as
\begin{equation}\label{eq:kernel_space}
    \boldsymbol{\Phi}_m = (\mathbf{A}_m^{\frac{1}{2}})^{+}\mathbf{U}_m^{+}\mathbf{U}_m\mathbf{A}_m\mathbf{U}_m^T,
\end{equation}
where we consider the eigendecomposition $\hat{\mathbf{K}}_m = \mathbf{U}_m \mathbf{A}_m \mathbf{U}_m^T$. Here, $\mathbf{A}_m$ contains the non-negative eigenvalues of the centered kernel matrix, $\mathbf{U}_m$ contains the corresponding eigenvectors, and the $+$ exponent denotes the Moore-Penrose pseudo-inverse. We can then use the matrix $\boldsymbol{\Phi}_m$ as training data instead of $\mathbf{X}_m$ during the training phase. In the same way, for the testing phase, a centralized kernel vector $\boldsymbol{\Phi}_{m*}$ is computed for each instance $\mathbf{x}_{m*} \in \mathbb{R}^{D_m}$ with
\begin{equation}\label{test_npt}
    \boldsymbol{\Phi}_{m*} = (\boldsymbol{\Phi}_m^T)^{+}(\mathbf{I}_N - \frac{1}{N}\mathbf{1}_N\mathbf{1}^T_N)\left(\mathbf{K}_{m*} - \frac{1}{N} \mathbf{K}_m \mathbf{1}_N\right),
\end{equation}
where $\mathbf{K}_{m*}$ is the testing kernel matrix calculated with the test instances using Eq. \ref{eq:kernel_npt}. $\boldsymbol{\Phi}_{m*}$ is finally used as testing data instead of the testing dataset $\mathbf{X}_{m*}$.  


Numerous challenges have been addressed using SVDD and subspace learning \cite{laakom2023convolutional, kilickaya2023hyperspectral, al2024malware}. In \cite{trustworthiness2024}, the trustworthiness of $\mathbb{X}$ users is evaluated with a S-SVDD approach over social network features, such as the number of friends, the number of followers, or the number of retweets. In \cite{10372038}, authors address the problem of credit card fraud detection by leveraging various features over highly imbalanced datasets. Regarding multimodal learning, MS-SVDD has been applied over various OCC problems involving two modalities, such as Robot Execution Failures with torque and force, Handwritten characters with Zernike moment and morphological features, or SPECTF dataset with stress and rest condition images \cite{SOHRAB2021107648}. MS-SVDD has also recently been applied to medical data for early detection of myocardial infarction \cite{10081907,zahid2024refining}. 

The diversity of these different works emphasizes the versatility of S-SVDD and MS-SVDD in terms of possible applications. Therefore, some directions have not been explored yet. For instance, multimodal applications were always limited in practice to two modalities, even though MS-SVDD could theoretically be leveraged with any number of modalities. Also, as mentioned earlier, several improvements such as graph embedding were only developed for unimodal S-SVDD and not MS-SVDD. % We aim to extend these improvements to MS-SVDD in this paper.