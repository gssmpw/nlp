\section{Preliminaries}
\label{sec:prelim}
$\gG \triangleq (\gV, \gE, \mX)$ is an undirected graph with its set of nodes and edges denoted by $\gV$ and $\gE$ respectively.  The node feature matrix $\mX \in \mathbb{R}^{|\gV| \times F}$ contains node feature $\vx_v \in \mathbb{R}^F$ as a row vector for every node $v \in \gV$. The adjacency matrix $\mA_{\gG}$ of size $|\gV| \times |\gV|$ captures the neighborhood of each node in $\gG$. In node classification, the goal is to predict a label $y_v \in C$ for each node $v \in \gV$ among the $\abs{C}$ possible class labels. The training uses labeled nodes $\gV_L \subset \gV$, while the unlabeled nodes $\gV_U = \gV \setminus \gV_L$ are used for validation and testing. A single-layer Graph Convolutional Network (GCN)~\cite{kipf2016semi} is defined as:
%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
\mH^{(l+1)} = \sigma(\hat{\mA}_\gG\mH^{(l)}\mW^{(l)}),
\label{eq:gcnlayer}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%
where $\mH^{(l)}$ represents the node embedding at layer $l$, with $\mH^{(0)}=\mX$, and $\mW^{(l)}$ is the learnable weight matrix in layer $l$. $\hat{\mA}_\gG$ denotes the normalized adjacency matrix and $\sigma$ is an activation function such as \relu. The predicted probabilities can be expressed as
$\hat{\mY} = \texttt{Softmax}(f_{\text{GNN}, \theta}(\gG))$ where $f_{\text{GNN}, \theta}(\gG)$ is a GCN model with $L$ layers and learnable parameters $\theta$. The dimension of $\hat{\mY}$ is $|\gV| \times \abs{C}$. The training objective is to find parameters $\theta$ that minimize the cross-entropy loss,
%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-6pt}
\begin{equation}
\label{eq:loss}
\mathcal{L}_\mathrm{CE} = - \frac{1}{|\gV_L|} \sum_{v \in \gV_L} \sum_{c = 1}^{\abs{C}} Y_{vc} \log \hat{Y}_{vc},
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%
where $Y_{vc}$ indicates the true probability of node $v$ belonging to class $c \in C$. 

The \emph{homophily} of a graph characterizes the likelihood that nodes with the same labels are neighbors. Two commonly used measures are \emph{Node homophily} $\gH_n$~\citep{pei2020geom} and \emph{Edge homophily} $\gH_e$~\cite{zhu2020beyond} and defined as
%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-3pt}
\begin{align}
%\small
\gH_{n} = & \frac{1}{|\gV|} \sum_{u\in \gV} \frac{| \{v\in \gN(u) : y_v = y_u\}|}{|\gN(u)|},\\
\gH_{e} = & \frac{(u,v)\in \gE: y_u = y_v}{|\gE|}.
\end{align}\vspace{-3pt}
%%%%%%%%%%%%%%%%%%%%%%%%
The values of $\gH_n$ and $\gH_e$ range from $0$ to $1$, where a value close to $1$ indicates strong homophily, and a value close to $0$ indicates strong heterophily.
% 
%Graphs with $0 \leq \gH_{n} < 0.5$ are considered \emph{heterophilic}, while those with $0.5 \leq \gH_{n} \leq 1$ are considered \emph{homophilic}~\citep{pei2020geom}.


% The GNN iteratively computes node embeddings 
% $\vh_v^{(l)}$ through message passing. At the final layer $L$, the embeddings are denoted as:
% \[
% \mH^{(L)} = f_{\text{GNN},\theta}(\mX,\mA_{\gG}),
% \]
% where $f_{\text{GNN},\theta}$ encapsulates all message-passing and update steps across $L$ layers as well as learnable parameters $\theta$ and $\mH^{(L)}$ contains the final node embeddings. The predicted logits for node $v$ are obtained by applying a linear transformation to its final embedding:
% \[
% \vz_v = \mW^{(L)} \vh_v^{(L)},
% \]
% where $\mW^{(L)}$ is the learnable weight matrix for classification and $\vz_v$ represents the unnormalized scores (logits) for each class. The logits $\vz_v$ are converted to probabilities using the Softmax function:
% \[
% \hat{Y}_v = \text{Softmax}(\vz_v)
% \]
% Combining everything, the predicted probabilities can be expressed as a function of GNN as the following: 
% \[
% \hat{\mY} = \text{Softmax}(\mW^{(L)} f_{\text{GNN}, \theta}(\mX,\mA_{\gG}))
% \]
% Here $\hat{\mY}$ of dimension $|\gV| \times C$ and $C$ is the number of class labels of $\gG$.
% \begin{equation}
% \label{eq:loss}
% \mathcal{L}_\mathrm{CE} = - \frac{1}{|\gV_L|} \sum_{v \in \gV_L} \sum_{c = 1}^{C} Y_{vc} \log \tilde{Y}_{vc}
% \end{equation}
% where 
% \[
% \tilde{Y} = \text{Softmax}(\mW^{(L)} f_{\text{GNN}, \theta}(\mX,\mA_{\tilde{\gG}}))
% \]
%\subsection{Theoretical Motivation.} 
%As mentioned earlier, 
% We are interested in the space of sparse subgraphs $\gG_q$ where every subgraph in that space has the same sparsity. 
%It is clear that this space consist of all possible $\floor{\frac{q|\gE|}{100}}$-length subset of edges of $\gE$. 

\subsection{Problem Statement and Theoretical Motivation}
Given $q>0$, this paper aims to construct a sparse subgraph $\tilde{\gG} \triangleq (\gV, \tilde{\gE},\mX)$ with $q\%$ of original edges in $\gE$. Let $\gG_q$ be the space of all distinct sparse subgraphs of $\gG$ where every subgraph contains exactly $k = \floor{\frac{q|\gE|}{100}}$ edges, $\gG_q = \{\tilde{\gE} \subset \gE : \abs{\tilde{\gE}} = k\}$.
The objective of our supervised sparse graph construction is to find the parameters $\theta$ along with a sparse subgraph $\tilde{\gG} \in \gG_q$ that minimize $\gL_\mathrm{CE}$.

We can define a probability space $(\Omega,\gF,p)$ by considering $\gE$ as the sample space, $\gF = \gG_q \subseteq 2^{\Omega}$ as the event space and a suitable probability measure $p: \gF \rightarrow [0,1]$. 
%An important question is what kind of probability measure is suitable and how to obtain it. 
The probability measure is determined by which subgraphs result in a node representation that minimizes the loss in Eq.~\ref{eq:loss}, which, in turn, depends on the downstream task. As a result, it is unknown which probability distribution is suitable as a choice for $p$. Specifically, we can perform the following decomposition to predict the probability that a node $v$ belongs to a class $c\in C$,
% 
% \[
% P(Y|G) = \E_{\tilde{g}} P(Y|\tilde{g})  P(\tilde{g}|G)
% \]
\vspace{-3pt}
\begin{align}
\label{eq:theo1}
P(\tilde{Y}_{vc}|\gG) &= \sum_{\tilde{\gG} \in \gG_q} P(\tilde{Y}_{vc}|\tilde{\gG})  P(\tilde{\gG}|\gG).
    % & =  \E_{\tilde{G}\sim p^*} P(Y|\tilde{G}) f_\phi (p^*|G)
\end{align}
% \vspace{-4pt}
There are two issues with the above decomposition. First, it requires enumerating all possible candidate subgraphs $\tilde{\gG} \in \gG_q$. This is computationally challenging because there are $\binom{|\gE|}{k}$ subgraphs, and it is not possible to estimate the probabilities $P(\tilde{Y}_{vc}|\tilde{\gG})$,  $P(\tilde{\gG}|\gG)$ due to their dependence on the downstream task under consideration.

We address these issues by encoding $P(\tilde{\gG}|\gG)$ as a learnable neural network module $\tilde{p} = f_{\edgemlp,\phi}(\gG)$ that explicitly learns to estimate the probability measure $p$ for every edge based on the downstream task. The neural network searches the space $\gG_q$ by adjusting its learned probability estimate $\tilde{p}$ based on the gradient of the loss. Finally, we model $P(\tilde{Y}_{vc}|\tilde{\gG})$ as a GNN that takes the sparsified sample $\tilde{\gG}$ sampled from the learned distribution $\tilde{p}$. Hence, Eq.~\ref{eq:theo1} can be approximated by,
% sid: Naheed, can you check the following equation?
\vspace{-3pt}
% \begin{equation}
% \label{eq:theo_motiv}
% P(\tilde{Y}_{vc}|\gG) \approx (\E_{\tilde{\gG}\sim f_{\edgemlp,\phi}(\gG)} f_{\gnn,\theta}(\tilde{\gG}))f_{\edgemlp,\phi} (\gG).
% \end{equation}
\begin{equation}
\label{eq:theo_motiv}
P(\tilde{Y}_{vc}|\gG) \approx \E_{\tilde{\gG}\sim f_{\edgemlp,\phi}(\gG)} [f_{\gnn,\theta}(\tilde{\gG})f_{\edgemlp,\phi} (\gG)].
\end{equation}
% \vspace{-1pt}
Equation~\ref{eq:theo_motiv} follows since 
%is due to the fact that 
instead of directly searching over the space $\gG_q$, we rely on the distribution $\tilde{p}$ approximated via a neural network. Once this distribution is learned, the law of large numbers indicates that a sufficient number of samples from $\tilde{\gG} \sim \tilde{p}$ can estimate $P(\tilde{Y})$. Thus the summation can be replaced with the expected value from a learned GNN model using sparse subgraph samples. 
% This insight also guides us in designing aggregate node representations from ensembles of sparse subgraphs.
% Here $p^*$ is the population distribution we attempt to learn using parameterized encoding function $f_\phi$ with $\phi$ being the learnable parameters.
\begin{figure*}[t]
	\centering
	\includegraphics[width=1.0\linewidth]{Figures/SGS-GNN2.pdf}
	\caption{Illustration of the three modules in \sgs. The edge probability encoding module computes a probability distribution, the sampler module samples the subgraph, and downstream GNN makes predictions using that sparse subgraph.
		% Solid lines indicate forward pass and dashed lines indicate backpropagation pathways.
	}
	\label{fig:sgsarchitecture}
\end{figure*}