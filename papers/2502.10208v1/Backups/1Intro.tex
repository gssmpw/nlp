\section{Introduction}
Graph Neural Networks (GNNs) have emerged as powerful tools for learning from graph-structured data with applications in domains such as social networks, biological networks, and recommendation systems. Despite their success, the computational and memory requirements of GNNs can be significant, especially for large-scale graphs. To address this challenge, \emph{graph sparsification}, which is the process of reducing the number of edges in a graph while preserving its essential structure, has gained attention as a means to improve the efficiency and scalability of GNNs.

\emph{Unsupervised graph sparsification} methods, such as spectral sparsification~\cite{batson2013spectral}, spanners, and k-nearest neighbors (k-NN)~\cite{knnsparse} only consider graph structural information to construct the sparsified graph. As a result, the resulting
sparsified graphs may not favor the downstream tasks such as node classification or link prediction~\cite{zheng2020robust}. On the other hand, \emph{Supervised graph sparsification} methods aim to reduce the complexity of graphs while preserving their relevance to a specific downstream task~\cite{zheng2020robust,luo2021learning,wu2023alleviating,sparsegat}.

Existing supervised sparsification methods such as SparseGAT~\cite{sparsegat}, SimSparse~\cite{wu2023alleviating}, and NeuralSparse~\cite{zheng2020robust} are what we call \emph{implicit sparsifiers} because they do not explicitly construct a sparse subgraph that can be independently used outside of the training process, and more importantly, that can be used to lighten the memory requirements on the GNN. Instead, they operate on the full adjacency matrix (or the original dense graph) during message passing and dynamically prune edges during computation. For instance, SparseGAT~\cite{sparsegat} requires attention scores for every edge in the full adjacency matrix before pruning, meaning the full graph is still processed during message passing. SimSparse~\cite{wu2023alleviating} operates by evaluating feature similarity across all edges, relying on the full adjacency matrix for similarity computation before selecting edges. NeuralSparse~\cite{zheng2020robust} learns binary masks over the entire edge set during training, again requiring access to the full graph.  Due to such requirements, SparseGAT, Simparse, and NeuralSparse increase the memory requirements on the GNN while handling large graphs. PDTNet~\cite{luo2021learning} is also not suitable to handle large graphs because it requires Singular Value Decomposition (SVD) of the adjacency matrix which is computationally expensive on large graphs. Due to these limitations of existing supervised sparsification methods, we ask the following question: \emph{How to design a memory-efficient, supervised, spasifier that learns to explicitly construct the downstream-task (e.g. node classification) specific sparse subgraph?}

% 
% What can we say about \cite{Dspar}?



% \textbf{Motivation:} 
% Graph sparsification is necessary for scaling large graphs and preventing over-smoothing issues. For example, in node classification tasks, over-smoothing will cause the features of the vertices belonging to the same connected subgraph to be similar, and the vertices cannot be accurately classified. 

% Unsupervised graph sparsification methods, such as spectral sparsification, spanners, and k-nearest neighbors (k-NN), do not rely on labeled data. Instead, they use heuristics or mathematical properties to reduce graph size. These methods reduce edges based on graphs' structural information only, which limits their performance when combined with another learnable downstream model, such as Graph Neural Networks (GNNs). GNNs require both graph structural features and the features of vertices. Therefore, supervised graph sparsification methods offer superior performance through task-specific feedback and adapting to different graph types (homophilic and heterophilic) by learning from labeled data.

% Researchers have been actively working on developing neural network-based supervised graph sparsification approaches; some notable methods are NeuralSparse~\cite{zheng2020robust}, PDTNet~\cite{luo2021learning}, SimSparse~\cite{wu2023alleviating}. Most of the existing methods have their limitations. 
% NeuralSparse learns a $k$-neighbor subgraph by repeatedly sampling edges for each vertex of a graph. However, since vertex degree distribution is not uniform, the choice of hyperparameter $k$ limits its learning power and may lead to suboptimal performance.
% PDTNet achieves good performance on small graphs but is not applicable to very large graphs because it requires Singular Value Decomposition (SVD).
% SimSparse uses feature similarity to learn edge weight for sparsification and uses a threshold to sparsify, which, in turn, loses prune control. A variant of this algorithm uses a feature dimension reduction technique to fit large graphs into memory and requires additional training time.

%\noindent\textbf{Our Contributions.} 
In this paper, we propose \sgs; a fast, lightweight, practical, supervised graph sparsification algorithm that explicitly learns the probability distribution over the edges of the input graph and explicitly constructs sparse subgraph realizations from that learned distribution for the downstream GNNs, such as vanilla GCN, graph transformers, or other learnable models. This approach offers a significant benefit that other supervised sparsifiers cannot offer, which is that the downstream GNN operates only on the reduced sparse graph rather than the input graph. For instance, on an input graph $\gG = (\gV,\gE)$, NeuralSparse requires $\Theta(|\gV|^2)$ space to store the learned binary mask whereas \sgs requires storing only the edge probabilities, requiring only $\Theta(|\gE|)$ space.

\sgs has three learnable components: (i) the edge probability encoding module, (ii) the sparse subgraph sampler, and (iii) the downstream graph model (e.g. GNN).

The edge probability encoding module (module I), termed as \edgemlp is a mapping function that takes features of edges (e.g., attributes of connected nodes or edge-specific data) and encodes them into probabilities. These probabilities represent the likelihood of specific edges existing in the learned sparse subgraph. The subgraph sampler module (module II) samples a subgraph following the learned prescribed edge distribution. 
% Since each edge has a given probability of being present in the sampled subgraph, the constructed sparse subgraph is not necessarily unique. As will be shown later that, this non-uniqueness helps the model explore for better subgraphs of similar sparsity helpful for the downstream task. 
Finally, the GNN module (module III) could be any message passing neural network. In that sense, this module is not specific to \sgs, however \sgs is flexible enough to accommodate any GNNs as shown in the experiments section. 

%% \naheed{@Sid,expand upon the above paragraph based on the discussion below. Then comment this paragraph.}
% The \edgemlp employs a Multi-layer Perceptrons (MLPs) to learn the edge weight and Gumbel-Softmax activation to turn these into sampling probabilities. Then, we sample the required number of edges for the downstream model. The loss from downstream tasks back-propagates the need to update the GNNs and edge weight learning model. This approach works well for a graph that is small enough; however, we cannot expect to compute sampling probabilities of all edges simultaneously for a huge graph. Therefore, we will apply batch processing on edges to compute edge weight and then sample a subset of these edges for the downstream task. Note that we avoid gradient accumulation on batch processing of edges to make the training process stochastic, which helps train GNN models faster. One crucial step is selecting a batch of edges such that the vertices in a batch of edges have high locality and preferably from within a cluster. We consider very fast graph partitioning and vertex re-ordering techniques to achieve this.

% The proposed supervised sparsification drops edges that are not helpful for downstream tasks. The method could potentially help remove noisy edges from the graph and also potentially increase homophily to address harmful heterophiles.

Key contributions of our work can be summarized as follows:

\begin{enumerate}

    \item We propose a novel sampling-based supervised graph sparsification technique that works for homophilic and heterophilic graphs. \sgs is a fast and lightweight sampling probability distribution learning method. Our \edgemlp is embarrassingly parallel and can easily accommodate large graphs. \sgs has explicit control over the sparse subgraph's sparsity, which related works don't have.

    \item We provide theoretical bounds and empirical verification of each module of \sgs, including a guarantee on the quality of learned probability distribution. 

    \item Any state-of-the-art GNNs can be used with our sparse subgraph sampling method to make it applicable to large graphs.

    \item In this work, we experimented with $33$ benchmark graphs ($21$ heterophilic, $12$ homophilic). In the heterophilic scenario, learned sparse subgraphs of \sgs outperform original dense graphs by the geometric mean of {\color{blue}$\approx 7\%$} using only $20\%$ of edges. \edgemlp also outperforms fixed distribution samplers such as random, degree-weighted edge, and effective resistance edge samplers. \sgs outperforms competing supervised and unsupervised sparsification-based GNN methods for heterophilic graphs ({\color{blue}$--$} and shows competitive performance in homophilic graphs.

    \item \sgs converges {\color{blue}$\approx 2\times$} faster than fixed distribution samplers. On average, \sgs has {\color{blue}$\approx 1.75\times$} faster per epoch training than NeuralSparse.
\end{enumerate}