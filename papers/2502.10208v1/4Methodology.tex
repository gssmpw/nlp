%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{figure}[!htbp]
% \centering
% \includegraphics[width=1.0\linewidth]{Figures/EdgeMLP.png}
% \caption{(Placeholder) Mechanism of \edgemlp}
% \label{fig:edgemlp}
% \end{figure}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \FloatBarrier
\section{Proposed method: \sgs}
\label{sec:Method}
% \paragraph{}
Figure~\ref{fig:sgsarchitecture} depicts our proposed method \sgs. In the following, we discuss its major components.
%\paragraph{Edge Probability Encoding Module (module I).}
\subsection{Module I: Edge Probability Encoding}
Given input $\gG$, the edge probability encoding module (\edgemlp) maps the node features to edge weights in the range $[0,1]$ followed by normalization to turn the learned weights into probabilities. The learned edge weights represent the model's unnormalized confidence in the existence of each edge. 
%However, the sum of these scores across all edges may not necessarily sum to 1. Hence, the normalization is required.
% We use a Multi-layer Perceptron to learn the probability $p(e_{uv})$ for each edge $e_{uv}\in E$. $p(e_{uv})$ represents the likelihood of sampling  edge $(u,v)$ for sparsification. 
% 
%%%%%%% sid before
% \edgemlp learns the edge weights as a function of node encodings $\vh^{(i)}_u,\vh^{(i)}_v$:
% \begin{equation*}
%     \vh_{u}^{(i)} = \relu(\Phi^{(i)}, \vh_u^{(i-1)}\oplus\mathrm{AGG}(\{\vh_{r}^{(i-1)}:r\in \gN(u)\})),
% \end{equation*}
% \begin{equation}
% \label{eq:w_uv}
% w(e_{uv}) =  \texttt{Sigmoid}(\mlp_{\phi}((\vh^{(i)}_u - \vh^{(i)}_v) \oplus (\vh^{(i)}_u \odot \vh^{(i)}_v))).
% \end{equation}
% Here, $h^{(0)}_u = \vx_u$, $\gN(u)$ denotes the set of neighbors of node $u$, $\mathrm{AGG}$ indicates the aggregation operation from \texttt{GraphSAGE}~\cite{hamilton2017inductive}, $\oplus$ indicates concatenation and $\odot$ represents elementwise multiplication.
% There are several options available for encoding \(\vh^{(i)}_u\), such as \texttt{GCN} and \mlp. Special consideration is needed when using convolutional layers for large graphs. At this stage, we can create a random sparse subgraph or draw from a prior probability distribution (eq.~\ref{eq:prior}). This subgraph will differ from the one utilized later in the downstream GNN module. 
% Additionally, among other ways, concatenating the subtraction and multiplication of two embeddings~\cite{reimers2019sentence} is very effective.
% 
%%%%%%% sid after
% \edgemlp learns the edge weights of $(u,v)$ as a function of node embeddings $\vh^{(i)}_u,\vh^{(i)}_v$:
% \begin{equation}
% \label{eq:w_uv}
% w(e_{uv}) = \sigmoid(\mlp_{\phi}((\vh^{(i)}_u - \vh^{(i)}_v) \oplus (\vh^{(i)}_u \odot \vh^{(i)}_v))).
% \end{equation}
% Here, $\sigmoid$ refers to \texttt{Sigmoid} activation function, $\oplus$ indicates concatenation, and $\odot$ represents element-wise multiplication. The embedding $\vh^{(i)}_u$ of node $u$, can be computed from \texttt{MLP} or using convolutional layers such as SAGE~\cite{hamilton2017inductive} or GCN~\cite{kipf2016semi}. For example, if \texttt{SAGE} layer is used then, 
% \begin{equation}
% \label{eq:embh_u}
%     \vh_{u}^{(i)} = \relu(\mW^{(i)}, \vh_u^{(i-1)}\oplus\mathrm{AGG}(\{\vh_{r}^{(i-1)}:r\in \gN(u)\})).
% \end{equation}
% Here, $h^{(0)}_u = \vx_u$, $\mW^{(i)}$ is the learnable weight matrix at $i$-th layer, $\gN(u)$ denotes the set of neighbors of node $u$, $\mathrm{AGG}$ indicates the aggregation operation (e.g., \texttt{mean, max}).
% 
% \textcolor{blue}{
% For large-scale graphs, the neighborhood expansion $r \in \gN(u)$ in equation~\ref{eq:embh_u} is computationally heavy on memory. Thus instead of considering all neighbors, we propose to take a subset of neighbors $\tilde{\gN}(u) \subset \gN(u)$ following a fixed prior probability distribution, $q_u(\gN(u))$. We considered two such distributions: (i) Uniform distribution:  $\forall r \in \gN(u), q_u(r) = \frac{1}{\abs{\gN(u)}}$. and (ii) Degree-proportionate distribution: $\forall r \in \gN(u), q_u(r) \propto (\frac{1}{d_u} + \frac{1}{d_r})$
% }
\edgemlp learns the edge weights of $(u,v)$ as a function of node embeddings $\vh_u,\vh_v$:
\begin{equation}
\label{eq:w_uv}
w(e_{uv}) = \sigmoid(\mlp_{\phi}((\vh_u - \vh_v) \oplus (\vh_u \odot \vh_v))).
\end{equation}
Here, $\sigmoid$ refers to \texttt{Sigmoid} activation function, $\oplus$ indicates concatenation, and $\odot$ represents element-wise multiplication. Let us assume $\vh_u$ indicates the node embedding in matrix $\mH$ corresponding to node $u$. Thus the node embedding matrix $\mH$ can be computed from an \mlp in the following manner: 
$\mH = \relu(\mlp_\mW(\mX))$, where $\mlp_\mW$ is an MLP with weights $\mW$. 
% $h_u$ indicates the node embedding in $\mH$ corresponding to node $u$. 
\mlp is computationally efficient; however, it does not exploit the graph structural information. As a result, \mlp is not necessarily the most effective choice, as we have shown later in the Ablation study (section \ref{app:ablationstudy}).

A common way to incorporate graph structural information  is to use graph convolutions such as vanilla GCN~\cite{kipf2016semi} or SAGE convolution~\cite{hamilton2017inductive}.
For instance, one can compute graph-structure aware node embedding matrix using a single-layer \texttt{GCN} as the following: $\mH = \sigma(\hat{\mA}_\gG\mX\mW)$,  where, $\mW$ is the learnable weight matrix. However, considering the entire graph $\mA_\gG$ is memory intensive for large graphs. 

Thus \sgs takes a length $\floor{\frac{q|\gE|}{100})}$ subset of edges $\gE_\mathrm{sp} \subseteq \gE$ following a fixed prior probability distribution $p_\mathrm{prior}$ and uses the induced subgraph $\gG[\gE_\mathrm{sp}]$ for computing node embedding $\mH$. In order to maintain good connectivity in $\gG[\gE_{sp}]$, the prior distribution is defined as the following:
% We have considered two such prior distributions: (i) uniform: $\forall_{(u,v) \in \gE}~p_\mathrm{prior}(u,v) = \frac{1}{\abs{\gE}}$ and (ii) degree-proportionate:
$\forall_{(u,v) \in \gE}~p_\mathrm{prior}(u,v) \propto (\frac{1}{d_u} + \frac{1}{d_v})$, where $d_u,d_v$ are the degrees of nodes $u,v$.
% 
% The sampled sparse subgraph $\gE_\mathrm{sp}$ is used for node embedding. 
% This sparse graph is different from the one used later in the downstream GNN module, which is learned.
% Sid: I think mentioning SAGE here would be overkill; GCN explains our case.
% The only purpose of this sparse subgraph is to aid the efficient computation of embeddings, which is different from the one utilized later in the downstream GNN module.

% Furthermore, among other ways, concatenating the subtraction and multiplication of two embeddings~\cite{reimers2019sentence} is very effective.

% \subsubsection{Normalization.}
%\paragraph{Normalization.}
\noindent\textbf{Normalization.} 
Normalization turns the learned edge weights into a valid probability distribution. One simple choice is \emph{sum-normalization} computed as following: $\tilde{p}(e_{uv}) = {w(e_{uv})}/{\sum_{(u,v)\in \gE} w(e_{uv})}$.
Another choice is \emph{softmax-normalization} with temperature annealing,
\begin{equation}
\label{eq:softmaxnorm}
\tilde{p}(e_{uv}) = \frac{\exp(w(e_{uv})/T)}{\sum_{(u,v)\in \gE} \exp(w(e_{uv})/T)}.
\end{equation}
Here, $T>0$ is the temperature parameter.
% that interpolates the distribution from one-hot to uniform.
% When $T$ is large, the sampling distribution becomes close to uniform, while a small value of $T$ makes the samples identical to those from the categorical distribution~\cite{jang2016categorical}. 
When $T$ is large, the learned distribution $\tilde{p}$ approaches uniform distribution over edges, whereas the learned distribution $\tilde{p}$ approaches categorical distribution when $T$ is small~\cite{jang2016categorical}. 
% 
% A high and low $T$ value corresponds to the exploration and exploitation behavior of the distribution.
As the learned distribution approaches uniform distribution, the model tends to explore more diverse subgraphs from subgraph space $\gG_q$. On the other hand, as the learned distribution approaches categorical distribution, the model tends to explore less in $\gG_q$.
Hence, we vary $T$ as a function of training iterations such that in early iterations, the algorithm explores more while narrowing down to its preferred search space later on. 
We execute such an annealing mechanism with the following equation:
%where $t$ is the temperature that varies as a function of epoch in the following manner:
\begin{equation}
T = \max (T_\mathrm{min},T_0 - \mathrm{epoch} \cdot r),
\end{equation}
% 
where $r = (T_0 - T_\mathrm{min})/{\mathrm{max\_epochs}}$ is the annealing rate, $T_\mathrm{min}$ is the minimum allowable temperature, and $T_0$ is the initial temperature. The temperature linearly decreases from the initial value $T_0$ to its final value $T_\mathrm{min}$ with the epochs. We keep track of the $T$-value that gives the best validation accuracy and use it later during inference.
 
Alg.~\ref{alg:edgmlp} shows the pseudocode for \edgemlp. 

% %%%%%%%%%%%%%%%%%%
\begin{algorithm}[!hbt]
\caption{\edgemlp Module}
\begin{algorithmic}[1] % The [1] here is for line numbering
\small
\STATE \textbf{Input:} $\gG (\gV, \gE, \mX)$, sample \% $q$, \#layers $L$, $\mathrm{epoch}$,  $\mathrm{max\_epochs}$
\STATE $\forall_{(u,v)\in\gE}~p_\mathrm{prior}(u,v) \gets \frac{1/d_u + 1/d_v}{\sum_{i,j\in \gE} (1/d_i + 1/d_j)}$
\STATE $\gE_\mathrm{sp} \gets \text{Multinomial}(\gE, p_\mathrm{prior}, \floor{\frac{q|\gE|}{100}})$ %\COMMENT{if $\gG$ is large otherwise use $\gE$}
\STATE $\mH \gets \texttt{GCN}_\mW(\gE_\mathrm{sp},\mX, L)$
\STATE $\forall_{(u,v)\in \gE}~\vw(u,v) = \sigma(\mlp_{\phi}((\vh^{(i)}_u - \vh^{(i)}_v) \oplus (\vh^{(i)}_u \odot \vh^{(i)}_v))$
\STATE $T \gets \max (T_\mathrm{min},T_0 - \mathrm{epoch} \cdot \frac{T_0 - T_\mathrm{min}}{\mathrm{max}\_\mathrm{epochs}})$
%\STATE $p(e_{uv}) = \frac{\exp(w(e_{uv}))/t)}{\sum_{(u,v)\in \gE} \exp(w(e_{uv}))/t)}: \forall (u,v) \in \gE $
\STATE $\tilde{p} \gets \mathrm{Softmax}(\vw/T)$    
\STATE \textbf{Return} $\tilde{p}, \vw$
\end{algorithmic}
\label{alg:edgmlp}
\end{algorithm}
% %%%%%%%%%%%%%%%%%%
% 
% As we will show later that, this learned probabilities $\tilde{p}$ can approximate the true unknown probabilities with arbitrarily small error due to the universal approximation property of $f_{\text{MLP},\phi}$.
% \paragraph{Theoretical analysis.} Let us assume that (i) there is an idealized learning ORACLE that knows the true probability distribution over the edges, (ii) the true distribution $p^*$ is a continuous function of node features $\mX$ and (iii) $\mX$ is a compact subset (bounded and closed) of Euclidean space $\mathbb{R}^n$.
% Now, the Universal Approximation Theorem states that a feed-forward neural network with at least one hidden layer and a finite number of neurons can approximate any continuous function $f: \mathbb{R}^n \rightarrow \mathbb{R}$ on a compact subset of $\mathbb{R}^n$, given a suitable choice of weights and activation functions. We use this result to state the following proposition under the assumptions I-III.
% \begin{proposition} For any error $\epsilon>0$, there exists an \mlp $f_{\mlp,\phi} = \tilde{p}$ that approximates the function $p^*$.
% \begin{equation}
% \label{eq:uapp}
% \sup_{e \in \mathcal{E}} \|\tilde{p}(\vx_e) - p^*(\vx_e)\|_1 \leq \epsilon
% \end{equation}
% where both $\tilde{p}$ and $p^*$ are functions of edge features $\vx_e = \vx_e =  (\vh^{(i)}_u - \vh^{(i)}_v | \vh^{(i)}_u \odot \vh^{(i)}_v)$ as used in equation~\ref{eq:w_uv}.
% \end{proposition}

% More discussion on this result can be found in Appendix~\ref{theo:uap}.

%\paragraph{Sparse subgraph sampling (module II).}
\subsection{Module II: Sparse Subgraph Sampling}
Given the learned distribution $\tilde{p}$ over the edges of the input graph, sparse subgraph sampling aims to construct a sparse graph with the user-given sparsity constraint $q$. We do not know which discrete distribution has $\tilde{p}$ as parameters. A natural choice is to construct $\tilde{\gG} = (\gV,\tilde{\gE},\mX)$ by assuming that $\tilde{p}$ is a parameter of a \emph{Multinomial} distribution. Hence we can sample $k=\floor{\frac{q|\gE|}{100}}$ edges as
$\tilde{\gE} \sim \text{Multinomial}(\tilde{p},k)$.

We can also construct $\tilde{\gG}$ by assuming that $\tilde{p}$ is a parameter of some categorical distribution and use \emph{Gumbel Softmax trick}~\cite{jang2016categorical}. The idea is to induce \emph{Gumbel noise} $g_{uv}\sim Gumbel(0,1)$ to the edges and select Top-$K$ edges with the highest probabilities.
% the $k$ edges using $\texttt{TopK}$ function. 
In order to sample edges according to categorical distribution, we replace our softmax-normalization (Equation~\ref{eq:softmaxnorm}) with the following:
\begin{equation}
\tilde{p}(e_{uv}) = \frac{\exp(({\log w(e_{uv})+g_{uv}})/{T})}{\sum_{(u,v)\in \gE} \exp({(\log w(e_{uv})+g_{uv})}/{T
})}.    
\end{equation}
Adding noise ensures that we are taking different samples at each time, and with low temperatures ($T=0.1, T=0.5$), the samples become identical to samples from a categorical distribution~\cite{jang2016categorical}. 
%We can also select the top-$\floor{\frac{q|\gE|}{100}}$ edges with the highest probabilities but lack exploration capability during training. However, it can give us one of the best sparse subgraphs during inference. 
%\naheed{@Sid, Discuss how we sampled from the categorical distribution with noise using Gumbel-softmax trick.}
%Sid: resource: https://docs.google.com/document/d/1HrslfnxNP6dso6DX9OXFvXSzQYF9Q-mz/edit
%We have empirically evaluated the effect of these distribution choices on the performance of \sgs in the section on ablation studies.

%\paragraph{Theoretical analysis.}
\noindent\textbf{Theoretical analysis I.} 
Let $\mathcal{E}^*$ and $\mathcal{\tilde{E}}$ denote the ordered collection of edges sampled by the idealized learning ORACLE according to true distribution $p^*$ and by \sgs according to learned probability $\tilde{p}$ respectively. For analytical convenience, let us assume that the algorithm samples $k$ edges with replacement. We have the following theorem that lower-bounds the \#edges common between sampled subgraphs from \sgs and idealized learning ORACLE.

\begin{theorem}[Lower-bound] The expected number of edges sampled by both \sgs and idealized learning ORACLE satisfies
\vspace{-10pt}
\begin{equation} 
\mathbb{E}[|\mathcal{E}^* \cap \mathcal{\tilde{E}}|] \geq k \sum_{j=1}^{|\mathcal{E}|} \frac{(p^*_j + \tilde{p}_j - \epsilon)^2}{4},
\end{equation}
where $k = \floor{q|\mathcal{E}|/100}$ with $0 \leq q \leq 100$ as a user-specified parameter and $\epsilon\in [0,1]$ is the error.
\end{theorem}
The proof is in Appendix~\ref{theo:commonedges}. The implications are:
\begin{enumerate}[wide, labelwidth=!, labelindent=2pt,itemsep=1pt,topsep=1pt]
    \item Let the true distribution be uniform. In the best-case scenario $\epsilon \rightarrow 0$ and $\tilde{p} = p^* = \frac{1}{|\mathcal{E}|}$. Then there are at least $\frac{k}{|\mathcal{E}|}$ common edges between $\tilde{\gG}$ and $\gG^*$. 
    % However, since $k < \abs{\mathcal{E}}$, the lower-bound of $\mathbb{E}[|\mathcal{E}^* \cap \mathcal{\tilde{E}}|] \geq \frac{k}{|\mathcal{E}|}$ is not very useful even though the learned distribution is accurate. 
    Since $k << \abs{\mathcal{E}}$, this specific scenario suggests that the learned sparse subgraph may not overlap much with the true one even after we have learned the true distribution. When the true distribution is uniform, every subgraph from $\gG_q$ is a global minimizer of the task-specific loss $\mathcal{L}_{CE}$. Otherwise, the learning ORACLE would have put more mass on certain edges and the distribution $p^*$ would not have been uniform. As individual subgraphs are indistinguishable in terms of performance, this case beats the purpose of supervised sparsification.

    \item Let the true distribution be one-hot. In other words, suppose $\tilde{p} = p^* = \delta_{ij}$, where $\delta_{ij}$ is the \emph{Kronecker-delta}. In this case, as $\epsilon \rightarrow 0$, the lower bound reduces to 
    \vspace{-8pt}
    \begin{equation*}
    \mathbb{E}[|\mathcal{E}^* \cap \mathcal{\tilde{E}}|] \geq k \sum_{j=1}^{|\mathcal{E}|} (\tilde{p}_j)^2 = k.
    \end{equation*}
    % \vspace{-4pt}
    This identity suggests that the sampled edges are expected to completely overlap with the true sparse subgraph. 
    
\end{enumerate}

% In practice, it would be surprising if the true distribution is uniform. If it was, the loss landscape would have been flat. 
For strong heterophilic graphs ($\gH_n$ is small), the true distribution is less likely to be uniform. Because a uniform edge sample would retain a similar node homophily as in the input graph, and such a subgraph would not be able to minimize $\mathcal{L}_{CE}$~\cite{das2024ags}. Thus, it is important for the learned probability distribution to approximate $p^*$ so that the sampled subgraph is close enough to the true one.
 
We have analyzed $\tilde{\gG}$ generated by \sgs on a synthetic graph in Appendix~\ref{app:toymoon}.

\begin{comment}
\begin{theorem}[Upper-bound] The expected number of edges sampled by both \sgs and idealized learning ORACLE satisfies
\begin{equation}
\mathbb{E}[|\mathcal{E}^* \cap \mathcal{\tilde{E}}|] \leq k (1 - \frac{\|p^* - \tilde{p}\|_1}{2}) 
\end{equation}
where $k = \floor{q|\mathcal{E}|/100}$ with $0 \leq q \leq 100$ as a user-specified parameter.
\end{theorem}

\textit{The implication of the upper bound.} When $\tilde{p} \rightarrow p^*$, the norm $\|p^* - \tilde{p}\|_1 \rightarrow 0$; therefore, the number of common edges could be close to $k$.

\end{comment}

%\paragraph{GNN module (module III).}
\subsection{Module III: Downstream GNN and Loss Functions}
At this stage, we input the sampled subgraph to a downstream GNN that supports edge weights as computed in Equation~\ref{eq:w_uv}; since the edge weights of the sampled edges are one of the ways we optimize \edgemlp via backpropagation. An example \gnn would be 
\begin{equation}
\hat{\mY} = \texttt{Softmax}(f_{\gnn,\theta}(\gV, \tilde{\gE}, \mX, \tilde{\vw})),
\end{equation}
where $\tilde{\gE}$ refers to the edges of the sampled sparse subgraph $\gG'$ and $\tilde{\vw} = \vw[\tilde{\gE}]$ contains the edge weights. 
%One of the ways gradient optimizers update the parameters of \edgemlp is by backpropagating through these sampled weights.

% We do not explicitly construct the sparse adjacency matrix $A_{\tilde{G}}$

%\paragraph{Loss functions.}
\noindent\textbf{Loss functions.} 
We introduce two regularizers to engrain various inductive biases to \sgs and combined these functions with the Cross-Entropy loss $\gL_\mathrm{CE}$ as follows:
\begin{equation}
\mathcal{L} = \alpha_1\mathcal{L}_\mathrm{CE} + \alpha_2 \mathcal{L}_\mathrm{assor} + \alpha_3 \mathcal{L}_\mathrm{cons},
\end{equation}
where $0 \leq \alpha_1,\alpha_2,\alpha_3 \leq 1$ are regularizer coefficients.

The \textbf{Assortativity loss} $\mathcal{L}_\mathrm{assor}$ uses the labels of the training nodes to force nodes with similar labels to have higher edge weights while forcing dissimilarly labeled nodes to have a small nonzero weight. This regularizer encourages edge homophily in the sampled sparse graph.
\vspace{-7pt}
\begin{equation}
\small
     \mathcal{L}_\mathrm{assor} \triangleq -\sum_{(u,v) \in \gE:u \land \gV_L \land v \in \gV_L} \mathbb{I}(y_u=y_v)\cdot \log w(e_{uv}),
\end{equation}
% \vspace{-20pt}
where $\mathbb{I}(.)$ is an indicator function that returns $0$ or $1$. 

The \textbf{Consistency loss} defined below encourages learned edge probabilities to reflect the similarity between node embeddings or features:
\vspace{-8pt}
\begin{equation}
\mathcal{L}_\mathrm{cons} \triangleq \sum_{(u,v) \in \tilde{\gE}} \|w(e_{uv}) - \mathrm{cosine}(\vh_u^l,\vh_v^l)\|,
\end{equation}
where $\mathrm{cosine}(\vh_u^l,\vh_v^l) = {\vh_u^l\cdot \vh_v^l}/{\|\vh_u^l\|\|\vh_v^l\|}$ is the cosine similarity of the learned GNN embeddings $\vh_u^l,\vh_v^l$ of nodes $u$, $v$ from layer $l$, and $w(e_{uv})$ is the learned probability for edge $(u,v)$ in the sparse graph $\tilde{\gG}$. This mechanism aligns the edge probabilities with the global graph structure and ensures that the sparsifier learns to preserve edges consistent with the broader graph relationships. 
% In ablation studies, we have evaluated the utility of these additional regularizing functions.
% 
% The dashed arrows in Fig.~\ref{fig:sgsarchitecture} illustrate the pathways of \edgemlp and \gnn model parameter updates through these losses during backpropagation. 
%Alg.~\ref{alg:sgstraining} in Appendix~\ref{app:algorithm} shows the pseudocode of our base training algorithm.
%These loss functions allow backpropagation to update \edgemlp, using only the training edges with $\gL_\mathrm{assor}$, and all the edges with $\gL_\mathrm{cons}$.


%\paragraph{Theoretical analysis.}
\noindent\textbf{Theoretical analysis II.} 
We consider vanilla GCN as a downstream GNN to examine how the sparse subgraph, $\tilde{\gG}$ from \sgs, affects node embeddings compared to the ideal subgraph $\gG^*$ from a learning ORACLE. Suppose an $L$-layer GCN produces embeddings $\tmH^{(L)}$ and $\mH^{*(L)}$ when taking $\tilde{\gG}$ and $\gG^*$ as input, respectively.
%Our goal is to analyze the respective encodings produced by a $L$-layer GCN when the input subgraphs are $\gG^*$ and , respectively. 
%$\gG^*$ (corresponding to adjacency matrix $\mA_{\gG^*}$) and $\tilde{\gG}$ (corresponding to adjacency matrix $\mA_{\tilde{\gG}}$) respectively. 
% For simplicity, we will denote the ideal and our sampled sparse matrices, $\mA_{\gG^*}$ as $\mA^*$ and $\mA_{\tilde{\gG}}$ as $\tmA$ respectively.
% 
% From eq.~\ref{eq:gcnlayer}, a single GCN layer is defined as $\mH^{(l+1)} =\sigma(\hat{\mA}\mH^{(l)}\mW^{(l)})$,
% % where $\hat{\mA} = \mD^{-1/2}\mA\mD^{-1/2}$ is the normalized adjacency matrix, $\mH^{(l)}$ is the input to the $l$-th layer with $\mH^{(0)} = \mX$, $\mW^{(l)}$ is the learnable weight matrix for $l$-th layer and $\sigma$ is non-linear activation function. 
% and an $L$-layer GCN produces embeddings $\tmH^{(L)}$ and $\mH^{*(L)}$ when it takes sparse matrices $\tmA$ and $\mA^*$ as input. 
% 
Is there an upper bound of the difference in the downstream node encodings $\mathbb{E}[\normLtwo{\tmH^{(L)} - \mH^{*(L)}}]$, due to the use of a learned subgraph?

To that end, we assume for all $l\in L$, $\normLtwo{\mW} \leq \alpha < 1$ where $\alpha$ is a constant. This is reasonable since each $\mW^{(l)}$ is typically controlled during training using regularization techniques, e.g., weight decay. As input features in $\mX$ are bounded, we also assume that there exists a constant $\beta$ such that $\forall l>0$, $\normLtwo{\mH}^{(l)} \leq \beta$. We also assume that $\sigma$ is \textit{Lipschitz continuous} with \textit{Lipschitz constant} $L_\sigma$. 
% For instance,  activation functions such as \relu, \texttt{Sigmoid}, or \texttt{TanH} are \textit{Lipschitz continuous}. 
% In particular, 
We assume \relu activation to simplify our analysis since \relu has a Lipschitz constant $L_\sigma = 1$. Under these assumptions, we have the following theorem (proof in Appendix~\ref{theo:gcnembed}). 

\begin{theorem}[Error in GCN encodings]
For sufficiently deep L-layer GCN, the error in node embeddings  

\vspace{-15pt}
{\scriptsize
\[
\mathbb{E}[\lim_{L \to \infty} \normLtwo{\tmH^{(L)} - \mH^{*(L)}}] < \frac{\beta}{1-\alpha}\sqrt{2k (1 - \sum_{j=1}^{|\mathcal{E}|} \frac{(p^*_j + \tilde{p}_j - \epsilon)^2}{4})}.
\]
}
\vspace{-15pt}
\end{theorem}
% The proof is given in Appendix~\ref{theo:gcnembed}.
 % 
% 
\subsection{\sgs Training and Additional Details}
\label{subsec:largescale}
\begin{algorithm}[!ht]
\caption{\sgs Training}
\begin{algorithmic}[1] % The [1] here is for line numbering
\small
\STATE \textbf{Input:} $\gG (\gV, \gE, \mX)$, sample \% $q$, \#layers $L$, METIS Parts $n$
% \STATE \textbf{Output:} \texttt{EdgeMLP}, \texttt{GNN}
\STATE $p_\mathrm{prior}(u,v) \gets \frac{1/d_u + 1/d_v}{\sum_{i,j\in \gE} (1/d_i + 1/d_j)}$

\STATE $\gG_\mathrm{parts} \gets \{\gG_1,\gG_2,\cdots,\gG_n\}= \mathrm{METIS} (\gG(\gV,\gE, p_\mathrm{prior}), n)$

\FOR{$\mathrm{epoch}$ in $\mathrm{max\_epochs}$}

    \FOR {$\gG_i(\gV_i,\gE_i,\mX_i,p^i_\mathrm{prior}) \in \gG_\mathrm{parts}$}
        \STATE $\tilde{p}, \vw \gets \edgemlp(\gE_i, \mX_i, L)$ \COMMENT{\textbf{Algorithm~\ref{alg:edgmlp}}}    
        \STATE $\tilde{p}_a \gets \lambda \tilde{p}+(1-\lambda)p^i_\mathrm{prior}$/*\textbf{Augmenting $\tilde{p}$ with prior}*/
        \STATE $\tilde{\gE}, \tilde{\vw} \gets \mathrm{Sample}(\tilde{p}_a, \vw, \floor{\frac{q|\gE|}{100}})$   \COMMENT{\textbf{Module II}}
        \STATE $\hat{\mY}, \tilde{\mH} \gets \mathrm{GNN}_\theta(\tilde{\gE},\mX_i,\tilde{\vw})$ \COMMENT{\textbf{Module III}}

        \STATE Compute $\gL_{CE}, \gL_\mathrm{assor}$, and $\gL_\mathrm{cons}$ using $\hat{\mY},\tilde{\mH}$
        
        \STATE $\gL \gets \alpha_1\cdot \gL_\mathrm{CE}+ \alpha_2\cdot \gL_\mathrm{assor}+ \alpha_3\cdot \gL_\mathrm{cons}$
        % 
        \STATE Backward Propagate through $\gL$
        % 
    \ENDFOR
    
\ENDFOR
%\STATE \textbf{Return} \texttt{EdgeMLP}, \texttt{GNN} 
\end{algorithmic}
\label{alg:sgstraining}
\end{algorithm}

Alg.~\ref{alg:sgstraining} outlines the pseudocode for training \sgs. \sgs starts with two precomputation steps:
i) computing the degree-proportionate edge weight as a \emph{prior} to enhance the learned distribution $\tilde{p}$ (line 1), and
ii) partitioning the input graph using METIS~\cite{karypis1997metis} for batch processing (line 2). Towards computing the loss for every partition at each iteration,  \sgs executes Edge probability encoding, Learned distribution augmentation with a prior, Sparse subgraph sampling and node embedding via GNN. Finally, the loss is backpropagated, the update pathways of which have been illustrated in Figure~\ref{fig:sgsarchitecture} earlier.

\textbf{Batch processing.} 
We can use \edgemlp from Alg.~\ref{alg:edgmlp} to compute edge weights in large-scale graphs, but efficient batch processing on edges is necessary for stochastic training of GNNs so as to reduce the risk of getting stuck in local minima.
It is crucial to select a batch of edges that have high locality, preferably from within a cluster, and we utilize METIS to achieve this. We could have made partitions small enough to fit GPUs and then applying GNN without any sparsification, similar to ClusterGCN~\cite{chiang2019cluster}. However, certain edges, such as task-irrelevant edges, may negatively impact performance, particularly in heterophilic or noisy graphs. In such cases, a high-quality learned sparse subgraph performs better than full graph, as validated in our experiments (\S\ref{subsubsec:fixedsampler}). 
% Sparsification also enables larger partitions without significant information loss.

\textbf{Augmenting $\tilde{p}$ with prior.} 
% We begin training for each graph partition by learning the probability distribution and edge weights from \edgemlp (line 5). 
While $\tilde{p}$ can be directly used to sample sparse subgraphs, the resulting subgraph may be suboptimal for message passing due to missing bridge edges connecting low-degree node pairs.
Thus augmenting the sampler with  $p_\mathrm{prior}$, which favors such edges, results in better quality sparse subgraph. $p_\mathrm{prior}$ is defined as
% Although \emph{effective resistance} can be an option, it is impractical for large graphs. 
\vspace{-8pt}
\begin{equation}
\label{eq:prior}
 p_\mathrm{prior}(u,v) \triangleq \frac{1/d_u + 1/d_v}{\sum_{i,j\in \gE} (1/d_i + 1/d_j)},
\end{equation}
where $d_u,d_v$ are degrees of nodes $u,v$. We control the emphasis of prior on the learned distribution with a parameter $\lambda \in [0,1]$, resulting in the \emph{augmented probability distribution}: $\tilde{p}_{a}(u,v) = \lambda \tilde{p}(u,v) + (1-\lambda) p_\mathrm{prior}(u,v)$ (line 7). The impact of $p_\mathrm{prior}$ on \sgs is discussed in Appendix~\ref{app:parameters}.

Another enhancement we consider is the \textbf{conditional updates} to \edgemlp. Since backpropagation is computationally expensive, we only update \edgemlp when the training F1-score from the learned sparse subgraph exceeds the baseline subgraph from $p_\mathrm{prior}$. The detailed algorithm for \sgs with conditional updates is in Appendix~\ref{app:algorithm}.

During inference, we use the learned probability distribution from \edgemlp, sample an ensemble of sparse subgraphs, and mean-aggregate their representations to produce final prediction on a test node. The pseudocode for inference (Alg.~\ref{alg:sgsinference}) is in Appendix~\ref{app:algorithm}.

\begin{comment}%\paragraph{Degree bias augmentation with a learned probability distribution.}
\noindent\textbf{Degree bias augmentation with a prior.} 
The search space for learning algorithms to identify optimal sparse graphs can be extensive in large-scale graphs. To address this, we can use a prior probability distribution $p_\mathrm{prior}$ based on \emph{degree}~\cite{zeng2019graphsaint}. This distribution favors edges connected to vertices with low degrees and assigns lower selection probabilities to edges within denser clusters. Due to computational efficiency, we chose degree-proportionate prior over \emph{effective resistance}. We control the bias between the learned and prior probability distributions using the control parameter $\lambda \in [0,1]$. The probability distribution becomes, $\tilde{p}(u,v) = \lambda \tilde{p}(u,v) + (1-\lambda) p_\mathrm{prior}(u,v)$, where $\tilde{p}$ is the learned probability distribution and degree norm $p_\mathrm{prior}$ for edge ($u,v$) is,
\vspace{-8pt}
 \begin{equation}
\label{eq:prior}
 p_\mathrm{prior}(u,v) = \frac{1/d_u + 1/d_v}{\sum_{i,j\in \gE} (1/d_i + 1/d_j)}.
\end{equation}
%\vspace{8pt}
% \vspace{-10pt}
% Alg.~\ref{alg:sgstraining} outlines the pseudocode of \sgs.

\noindent\textbf{METIS graph partitioning.}
We can use \edgemlp from Alg.~\ref{alg:edgmlp} for edge weight computation in large graphs. However, we want efficient batch processing on edges, avoid gradient accumulation, and make the training process stochastic. This helps train GNN models with fewer epochs by reducing the likelihood of getting stuck in local minima.
One crucial step is selecting a batch of edges such that the vertices in a batch of edges have high locality and preferably from within a cluster. To achieve this, we consider very fast graph partitioning methods like METIS~\cite{karypis1997metis}.

For inference, we use the probability distribution from \edgemlp, sample multiple sparse subgraphs, and aggregate their predictions. The pseudocode of inference (Alg.~\ref{alg:sgsinference}) is provided in Appendix~\ref{app:algorithm} for brevity.
\end{comment}


% \paragraph{Dynamic temperature annealing.}
% \[
% t = \max (t_\mathrm{min},t_0 - \mathrm{epoch} \cdot r)
% \]
% where $r = \frac{(t_0 - t_\mathrm{min})}{\mathrm{total}\_\mathrm{epochs}}$ is the annealing rate, $t_\mathrm{min}$ is minimum allowable temperature, and $t_0$ is the initial temperature. 
% \paragraph{Consistency loss.}

% \[
% L_\mathrm{cons} = \sum_{(u,v) \in \tilde{\gG}} \|\tilde{p}(e_{uv}) - \mathrm{cosine}(\vh_u,\vh_v)\|
% \]
% where $\mathrm{cosine}(\vh_u,\vh_v) = \frac{\vh_u\cdot \vh_v}{\|\vh_u\|\|\vh_v\|}$ is the cosine similarity of the learned GNN embeddings $\vh_u,\vh_v$ corresponding to nodes $u$ and $v$ and $p(e_{uv})$ is the learned probability for edge $(u,v)$ in the sparse graph $\tilde{\gG}$.

% \paragraph{Performance-Gated Training.}
% %%%%%%%%%%%%%%%%%%

% \noindent\textbf{Conditional update of \edgemlp.}
% Backward propagation is often the most computationally intensive part of training, so we employ a conditional mechanism to update \edgemlp selectively. We evaluate the learned sparse subgraph against a subgraph from the prior probability distribution $p_\mathrm{prior}$. If the training F1-score from the learned sparse subgraph is better than the baseline, parameters of \edgemlp are updated; otherwise, the update is skipped. Detailed algorithm for \sgs with conditional updates (Alg.~\ref{alg:sgstrainingpriorfull}) is provided in Appendix~\ref{app:algorithm}.

% \subsection{Theoretical results}
% \label{subsec:theory}
% The final objective of our theoretical analysis is to obtain an upper bound on the learned node embedding produced by module III (section 4.4.4). To obtain this upper bound we consider as a reference an idealized learning ORACLE that knows the true sampling probability distribution. Toward our final theoretical goal, first, we obtain an upper bound on the learned probability distribution (section 4.4.1). Afterwards, we obtain an upper bound on the number of common edges (section 4.4.2) which helps us bound the error in the learned Adjacency matrix (section 4.4.3). Finally, in section 4.4.4, we obtain an upper bound on the learned node embedding produced by \sgs.


% \subsection{Computational Complexity}
\textbf{Computational Complexity.}
Suppose the number of hidden dimension $H\approx F$, where $F$ is the dimension of the node features. The cost of an $L$-layer GCN is $\bigO(L(|\gE|\cdot H + |\gV| \cdot H^2))$~\cite{chiang2019cluster}. 
The cost of Alg~\ref{alg:edgmlp} is $\bigO(L(|\gE_\mathrm{sp}|\cdot H + |\gV| \cdot H^2)+ \abs{\gE}\cdot H^2)$, since computing node-embedding (line 4, Alg.~\ref{alg:edgmlp}) using sparse graph $\gE_\mathrm{sp}$ costs $\bigO(L(|\gE_\mathrm{sp}|\cdot H + |\gV| \cdot H^2))$, and edge weight computation using \mlp (line 5, Alg.~\ref{alg:edgmlp}) costs $\bigO(\abs{\gE}\cdot H^2)$. 
% 
With an $L$-layer GCN used as downstream GNN acting on the sparse subgraph $\tilde{\gE}$, the downstream GNN costs $\bigO(L(|\tilde{\gE}|\cdot H + |\gV| \cdot H^2)$. 
% For loss, $L_\mathrm{assor}$, $L_\mathrm{cons}$ is $\bigO(|\tilde{\gE}$ and $\bigO(|\tilde{\gE}|\cdot H)$ respectively. 
Since, $\abs{\gE_\mathrm{sp}}=\abs{\tilde{\gE}}$ the total complexity of \sgs  (Alg.~\ref{alg:sgstraining}) is $\bigO(L(|\tilde{\gE}|\cdot H + |\gV| \cdot H^2)+ \abs{\gE}\cdot H^2)$.

\textbf{Space complexity.} Let $n$ partitions from METIS have similar sizes. The memory requirement for \sgs with $L$-layer GCN is  $\bigO\left(\frac{|\gE| + |\gV|\cdot H}{n} + L \cdot H^2 \right)$. 
% The former part is for graph-related information, and the latter is for learnable parameters.



% $\bigO(L \cdot H^2)$ for edge weight, 
% There is a precomputation cost of $\bigO(|\gE|)$ for $p_\mathrm{prior}$, and for large-scale graphs, there is an additional one-time partition cost from METIS.

% Then computation complexity of \edgemlp 

% If $H\approx F$, then the overall complexity of $L$ layer GCN can be approximated as $\bigO(L(|\gE|\cdot H + |\gV| \cdot H^2))$. If GCN is used at \edgemlp and \gnn, then the complexity of the computation using a sparse graph of size $|\tilde{\gE}|=\floor{\frac{q|\gE|}{100}}$ becomes $\bigO(L(|\tilde{\gE}| \cdot H + |\gV| \cdot H^2)$. The cost of \mlp for the edge weight computation is $\bigO(\abs{\gE}\cdot H^2)$. There is a pre-computation cost of $\bigO(|\gE|)$ for $p_\mathrm{prior}$, and for large-scale graphs, there is an additional one-time partition cost from METIS.
% Memory requirements:
% sid: maybe remove this or put this into appendix
% The memory requirements for considering the entire graph include several components: $\bigO(|\gE|)$ for edges, $\bigO(|\gV| \cdot F)$ for features, $\bigO(L|\gV| \cdot H)$ for the hidden state, $\bigO(L \cdot H^2)$ for trainable parameters, and $\bigO(L \cdot \gV \cdot H)$ for activations and gradients. If a partitioning method like METIS is employed, resulting in $n$ partitions, each partition will require approximately $\bigO\left((|\gE| + |\gV| F + L \cdot |\gV| \cdot H)/{n}\right) + \bigO(L \cdot H^2)$ of memory.