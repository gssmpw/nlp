\section{Additional algorithmic details of \sgs }
\label{app:algorithm}

\paragraph{Conditional update of \edgemlp.}
Backward propagation is often the most computationally intensive part of training, so we employ a conditional mechanism to update \edgemlp selectively. 
We evaluate the learned sparse subgraph (line 9, Alg.~\ref{alg:sgstrainingpriorfull}) against a subgraph from the prior probability distribution $p_\mathrm{prior}$ (line 11, Alg.~\ref{alg:sgstrainingpriorfull}). If the training F1-score from the learned sparse subgraph is better than the baseline, parameters of \edgemlp are updated (line 19, Alg.~\ref{alg:sgstrainingpriorfull}). Otherwise, the update to \edgemlp is skipped (line 22, Alg.~\ref{alg:sgstrainingpriorfull}). 

The detailed algorithm for \sgs with conditional updates is in Alg.~\ref{alg:sgstrainingpriorfull}.


% If the size of $\gG$ is large, we compute a sparse subgraph from a prior probability distribution $p_\mathrm{prior}$ for \edgemlp. Later, we sample another sparse subgraph from the learned distribution to use with downstream GNN. The degree-proportionate prior is,

%  \begin{equation}
%  p_\mathrm{prior}(u,v) = \frac{1/d_u + 1/d_v}{\sum_{i,j\in \gE} (1/d_i + 1/d_j)}.
% \end{equation}

% % %%%%%%%%%%%%%%%%%%
% % \begin{wrapfigure}{c}{0.8\textwidth}
% \begin{center}
% \begin{algorithm}[!htbp]
% \caption{\sgs Training}
% \label{alg:sgstraining}
% \begin{algorithmic}[1] % The [1] here is for line numbering
% \STATE \textbf{Input:} $\gG (\gV, \gE, \mX)$, sample percent $q$, $num\_hops$
% \STATE \textbf{Output:} \texttt{EdgeMLP}, \texttt{GNN}

% %\STATE Sample size, $Q =\frac{q\gE}{100}$
% %\STATE Degree Norm, $p(u,v) = \frac{1/d_u + 1/d_v}{\sum_{i,j\in \gE} (1/d_i + 1/d_j)}$

% \FOR{$\mathrm{epochs}$ in $\mathrm{max\_epochs}$}

%     \STATE $\tilde{p},\vw =\edgemlp(\gE,p_\mathrm{prior})$
    
%     \STATE $\gE',\vw' = \mathrm{Sample}(\tilde{p}, \vw, \floor{\frac{q|\gE|}{100})}$ \COMMENT{Sparse graph for downstream GNN}
%     \STATE $\hat{\mY}, \mH' = \mathrm{GNN}_\theta(\gE',\vw',\mX)$

%     \STATE $\gL_\mathrm{CE} = \mathrm{CrossEntropy} (\mY\mathrm{[train]}, \hat{\mY}\mathrm{[train]})$
%     \STATE $\mathrm{mask[u,v]}=True:\forall_{(u,v)\in \gE} u \in \gV_L \land v \in \gV_L$
%     \STATE $\gL_\mathrm{assor} = \mathrm{CrossEntropy}(\gE \mathrm{[mask]},\vw \mathrm{[mask]})$

%     \STATE $\gL_\mathrm{cons} = \mathrm{Similarity} (\vw, \mathrm{Cosine}(\mH'[\gE[s]],\mH'[\gE[t]]))$

%     \STATE $\gL = \alpha_1\cdot \gL_\mathrm{CE}+ \alpha_2\cdot \gL_\mathrm{assor}+ \alpha_3\cdot \gL_\mathrm{cons}$
%     \STATE Backward propagate through $\gL$ and optimize $\edgemlp_\phi, \mathrm{GNN}_\theta$.
% \ENDFOR

% \STATE \textbf{Return} \texttt{EdgeMLP}, \texttt{GNN} 
% \end{algorithmic}
% \end{algorithm}
% \end{center}
% % \end{wrapfigure}
% % %%%%%%%%%%%%%%%%%%

% \sgs also supports conditional updates of \edgemlp, and the pseudocode is outlined in Algorithm~\ref{alg:sgstrainingpriorfull}. 


% %%%%%%%%%%%%%%%%%%
\begin{algorithm}[!htbp]
\caption{\sgs Training with conditional updates}
\begin{algorithmic}[1] % The [1] here is for line numbering
\STATE \textbf{Input:} $\gG (\gV, \gE, \mX)$, sample percent $q$, $\mathrm{hops}$, METIS Parts, $n$
\STATE \textbf{Output:} \texttt{EdgeMLP}, \texttt{GNN}
\STATE Compute $p_\mathrm{prior}(u,v) \gets \frac{1/d_u + 1/d_v}{\sum_{i,j\in \gE} (1/d_i + 1/d_j)}$

\STATE $\gG_\mathrm{parts}=\{\gG_1,\gG_2,\cdots,\gG_n\}\gets \mathrm{METIS} (\gG(\gV,\gE, p), n)$

\FOR{$\mathrm{epochs}$ in $\mathrm{max\_epochs}$}

    \FOR {$\gG_i(\gV_i,\gE_i,\mX_i,p^i_\mathrm{prior}) \in \gG_\mathrm{parts}$}
        \STATE $\tilde{p},\vw \gets \edgemlp(\gE_i,p^i_\mathrm{prior}, \mX_i,\mathrm{hops})$        
        \STATE $\tilde{p}_a \gets \lambda \tilde{p}+(1-\lambda)p^i_\mathrm{prior}$
        \STATE $\tilde{\gE},\tilde{\vw} \gets \mathrm{Sample}(\tilde{p}_a,\vw,\floor{\frac{q|\gE_i|}{100}})$ \COMMENT{\textbf{Learned sparse subgraph}}
        
        \STATE $\hat{\mY}, \tilde{\mH} \gets \mathrm{GNN}_\theta(\tilde{\gE},\tilde{\vw},\mX)$
        
        \STATE $\gE_\mathrm{prior} \gets \mathrm{Sample}({p_i},\floor{\frac{q|\gE_i|}{100}})$ \COMMENT{\textbf{Sparse subgraph from prior}}

        \STATE $\hat{\mY}_\mathrm{prior}  \gets \mathrm{GNN}_\theta(\gE_\mathrm{prior},\mX)$

        \IF {Evaluate$(\hat{\mY}) \ge $ Evaluate$(\hat{\mY}_\mathrm{prior})$}
            \STATE $\gL_\mathrm{CE} \gets \mathrm{CrossEntropy} (\mY_{\gV_L}, \hat{\mY}_{\gV_L})$

            \STATE $\forall_{(u,v)\in \gE_i} u \in \gV_L \land v \in \gV_L : \mathrm{mask[u,v]} \gets \text{True}$
        
            \STATE $\gL_\mathrm{assor} \gets \mathrm{CrossEntropy}(\gE \mathrm{[mask]},\vw \mathrm{[mask]})$
        
            \STATE $\gL_\mathrm{cons} \gets \mathrm{Sim} (\vw, \mathrm{Cosine}(\vh_u,\vh_v): \forall_{(u,v)\in \gE})$ 
        
            \STATE $\gL \gets \alpha_1\cdot \gL_\mathrm{CE}+ \alpha_2\cdot \gL_\mathrm{assor}+ \alpha_3\cdot \gL_\mathrm{cons}$
            \STATE Backward Propagate through $\gL$ and optimize EdgeMLP$_\phi, \mathrm{GNN}_\theta$.
        
        \ELSE
            \STATE $\gL_\mathrm{CE} \gets \mathrm{CrossEntropy} (\mY_{\gV_L}, \hat{\mY}_{\gV_L})$
            \STATE Backward Propagate through $\gL_\mathrm{CE}$ and optimize $\mathrm{GNN}_\theta$.            
        \ENDIF
    
    \ENDFOR
    
\ENDFOR
\STATE \textbf{Return} \texttt{EdgeMLP}, \texttt{GNN} 
\end{algorithmic}
\label{alg:sgstrainingpriorfull}
\end{algorithm}
% %%%%%%%%%%%%%%%%%%

\clearpage
\paragraph{Inference.} 
During inference, we use the learned probability distribution from \edgemlp. We keep track of the best temperature $T$ that gave the best validation accuracy and use that to sample an ensemble of sparse subgraphs. Then, we mean-aggregate their representations to produce the final
prediction on a test node. 

The reason we consider ensemble of subgraphs is because there are variability in the edges of the sample subgraphs even if they are all sampled from the same distribution. Thus mean-aggregation of node embeddings is an effective way to improve the robustness of the learned node embeddings. 

The inference pseudocode is provided in Algorithm~\ref{alg:sgsinference}.

% %%%%%%%%%%%%%%%%%%
\begin{algorithm}[!htbp]
\caption{\sgs Inference}
\begin{algorithmic}[1] % The [1] here is for line numbering
\STATE \textbf{Input:} Graph $\gG (\gV, \gE, \mX)$, sample \% $q$, Ensemble size, $R$.
\STATE \textbf{Output:} Prediction, $\hat{\mY}$

%\STATE Sample size, $Q =\frac{q\gE}{100}$
%\STATE Degree Norm, $p(u,v) = \frac{1/d_u + 1/d_v}{\sum_{i,j\in \gE} (1/d_i + 1/d_j)}$
    \STATE $\vw, \tilde{p} = \edgemlp(\gE, \mX, T_\mathrm{best})$ \COMMENT{\textbf{Use $T$ that gave best validation accuracy}.}   
    \STATE $S_y \gets \emptyset$ \COMMENT{Predictions}
    
    \FOR {$i$ in $R$}
        \STATE $\tilde{\gE}, \tilde{\vw} \gets \mathrm{Sample}(\tilde{p},\floor{\frac{q|\gE|}{100}})$        
        \STATE $\hat{\mY}_i \gets \mathrm{GNN}_\theta(\tilde{\gE},\tilde{\vw},\mX)$
        \STATE $S_y \gets S_y \cup \hat{\mY}_i$
    \ENDFOR

    \STATE Predict, $\hat{\mY} \gets \mathrm{Mean} (S_y)$
    
\STATE \textbf{Return} $\hat{\mY}$
\end{algorithmic}
\label{alg:sgsinference}
\end{algorithm}
% %%%%%%%%%%%%%%%%%%
\clearpage