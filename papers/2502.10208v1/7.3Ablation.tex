\section{Runtime Comparison}
\label{app:runtime}

\subsection{Impact of Conditional Updates on Runtime}
Table.~\ref{tab:largescaleruntime} compares the runtime of \sgs with and without conditional updates for large-scale graphs (with $|\gE| \ge 1M$). The results indicate that conditional updates are similar to our standard training algorithm in terms of computational efficiency while providing improvements in F1-score under identical conditions. The additional computational costs of evaluation with prior get compensated by fewer updates of \edgemlp.

\input{Results/7.2RuntimeAblation}

\subsection{Comparison with Baseline GNN based Sparsifiers}
\label{app:runtimerelated}
Table~\ref{tab:runtimerelated} shows related algorithms' mean training time (s). Although \sgs is slower than the unsupervised sparsification-based GNNs, it is significantly faster than supervised sparsifiers.
\input{Results/7.22RuntimeRelated}


\section{Ablation Studies}
\label{app:ablationstudy}

This section investigates how different components of \sgs behave and contribute to overall performance. We organize this section as follows,

\begin{enumerate}
    \item Section~\ref{subsec:ab_edgemlpgnn} investigates $\gL_\mathrm{assor}, \gL_{cons}$, \edgemlp, \gnn, and Conditional Updates mechanism. We also compare its runtime against standard \sgs training vs \sgs with conditional updates. We also show \sgs can be used with other GNNs in Sec~\ref{app:othergnn}.

    \item Section~\ref{app:parameters} explores parameter settings with/without prior, different normalization and sampling methods, and inference with/without an ensemble of subgraphs.

    \item Section~\ref{app:gridsearch} shows ideal settings for regularizer coefficients $\alpha_1, \alpha_2, \alpha_3$. We also show the impact of $\lambda$ for augmenting the learned probability distribution $p$ using $p_\mathrm{prior}$.
\end{enumerate}


\subsection{$\gL_\mathrm{assor}, \gL_{cons}$, \edgemlp, \gnn, and Conditional Updates}
\label{subsec:ab_edgemlpgnn}

Table~\ref{tab:ablationgnn} illustrates the performance of \sgs with various combinations of regularizers, embedding layers in \edgemlp, and convolutional layers in \gnn. 

\begin{enumerate}
    \item $\gL_\mathrm{assor}$: Case 1, 2 shows improvement in results when $\gL_\mathrm{assor}$ is used.

    \item $\gL_\mathrm{cons}$: From cases 4, 6, 8 shows $L_\mathrm{cons}$ improves results when $\texttt{GCN}$ module is used in the GNN.

    \item \edgemlp: In general, we found that the \texttt{GCN} layers for \edgemlp encodings performs best (cases 5-6, 11-12). 
    
    \item \gnn: Both \texttt{GCN} and  \texttt{GAT} modules yielded overall the best results (case 6, 11).
    
    \item Conditional updates: Case 3 shows that conditional updates can benefit some graphs.     
    
    We also investigated the runtime and quality of \sgs with and without conditional updates for large-scale graphs. We found both have similar runtime as the condition check expense gets compensated by fewer updates of \edgemlp. Detailed comparisons of conditional updates in large graphs ($|\gE|\ge 1M$) are included in the Table~\ref{tab:largescaleruntime}.   
\end{enumerate}







\input{Results/5.1NNAblation}




\subsubsection{\sgs with other GNN modules}
\label{app:othergnn}
The sampled sparse subgraphs from \edgemlp can be fed into any downstream GNNs and demonstrate a couple of variants of \sgs. Chebnet from Chebyshev~\cite{he2022convolutional}, Graph Attention Network (GAT)~\cite{velivckovic2017graph}, Graph Isomorphic Network (GIN)~\cite{xu2018powerful}, Graph Convolutional Network (GCN)~\cite{kipf2016semi} are some of the GNNs used for demonstration. 

Fig.~\ref{fig:sparsityvsgnn} shows the performance of these GNNs on homophilic and heterophilic datasets. \texttt{SGS-GCN} and \texttt{SGS-GAT} are two best performing models.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!htbp]
\centering
% \includegraphics[width=0.5\linewidth]{Figures/SparsityvsGNN.png}
\includegraphics[width=0.6\linewidth]{Figures/SGS-differentgnn.pdf}

\caption{Performance of \sgs with different GNN modules using $20\%$ edges.}
\label{fig:sparsityvsgnn}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \subsection{Additional Ablation studies}
% \label{app:moreablation}

\subsection{Impact of  $p_\mathrm{prior}$, Normalization \& Sampling schemes, and Ensembling on \sgs}
\label{app:parameters}

Table~\ref{tab:ablation} highlights the impact of the following components: 

\input{Results/7.3ParamterAblation}

\begin{enumerate}
    \item Prior $p_\mathrm{prior}$: Cases 2-3 show that augmenting the learned probability distribution $\tilde{p}$ with prior $p_\mathrm{prior}$ can benefit some datasets. We have also conducted an in-depth comparison between the distributions $\tilde{p}$ and augmented distribution $\tilde{p}_a$. Figure~\ref{fig:augment_p} shows that there $\tilde{p}_a$ is left skewed whereas $\tilde{p}$ is not. Since rare edges still get some negligible mass, it is possible for $\tilde{\gG}$ constructed from $\tilde{p}_a$ to retain some bridge edges from these tails, if there are any. 
    
\begin{figure}[!htbp]
    \centering
    %\subfigure{\includegraphics[width=0.48\linewidth]{Figures/SparsityvsHomophily.png}
    \subfigure{\includegraphics[width=0.4\linewidth]{Figures/SGS-learnedp.png}
    \label{subfig:learnedp}} 
    %\hfill
    % \subfigure{\includegraphics[width=0.48\linewidth]{Figures/SparsityvsAccuracy2.png}
    %\hfill     
     \subfigure{\includegraphics[width=0.4\linewidth]{Figures/SGS-learnedp_a.png}
     \label{subfig:priorpa}} 
     \subfigure{\includegraphics[width=0.4\linewidth]{Figures/SGS-prior.png}
     \label{subfig:priorp}}    
    \caption{The learned probability distribution $\tilde{p}$ (top-left), augmented distribution $\tilde{p}_a$(top-right) and fixed prior $p_\mathrm{prior}$ (bottom). Augmentation puts negligible mass on some rare yet critical edges in the left tail of $\tilde{p}_a$.}
    \label{fig:augment_p}
\end{figure}
    \item Normalization and Sampling: We considered three normalization and sampling techniques. i) sum-normalization with multinomial sampling, ii) softmax-normalization with temperature with multinomial sampling, and iii) Gumbel softmax normalization with Topk selection. Cases 3-5 show that each of these techniques can improve results in certain datasets, and thus, it is difficult to nominate a single one as best. However, in our experiments, we opted for multinomial sampling with softmax temperature annealing for training to encourage exploration in early iterations.

    \item Ensemble subgraphs during inference: Case 2 demonstrates that using multiple subgraphs for ensemble prediction yields better results than a single subgraph (Case 1).    
\end{enumerate}

 % The addition of assortative loss $L_\mathrm{assor}$ in Case 3 enhances performance on heterophilic graphs by promoting homophily in sampled subgraphs. %Case 4 shows that incorporating consistency loss $L_\mathrm{cons}$ benefits certain graphs. 
%While our base training method updates \edgemlp at each iteration, conditional update with a prior distribution speeds up results by reducing the search space. Cases 5-7 indicate that hop size at \edgemlp influences performance. 

% \input{Results/7.3ParamterAblation}




\clearpage
\subsection{Choosing Values for Regularizer coefficient \(\alpha_3\) and Parameter \(\lambda\)}
\label{app:gridsearch}
Recall that \sgs computes the total loss at each epoch as
\[
\gL = \alpha_1\gL_{CE}+\alpha_2\gL_\mathrm{assor}+\alpha_3\gL_\mathrm{cons},
\]
where $0 \leq \alpha_1,\alpha_2,\alpha_3 \leq 1$ are regularizer coefficients corresponding to the cross-entropy loss $\gL_{CE}$, assortativity loss $L_\mathrm{assor}$ and  consistency loss $\gL_\mathrm{cons}$ respectively. 

Also recall that, when we use a prior probability distribution, the learned distribution values of $\tilde{p}$ are weighted through $\lambda$ in
$\Tilde{p} = \lambda\Tilde{p}+(1-\lambda) p_\mathrm{prior}$

To avoid numerous combinations of values of three coefficients + the parameter $\lambda$, we have fixed $\alpha_1 = 1$, and $\alpha_2 = 1$. In the following, we investigate the performance of \sgs with different values for $\alpha_3$ and $\lambda$.

Fig.~\ref{fig:consbias} shows a grid search for different combinations of $\lambda$ and $\alpha_3$. As per our observation, the recommended values are $\lambda \in [0.3, 0.7], \alpha_3=0.5$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[h]
\centering
\includegraphics[width=0.4\linewidth]{Figures/SGS-biasgrid.pdf}
\caption{Grid search for the parameter $\lambda$ for prior, and consistency loss, $\alpha_3$ (Cora dataset).}
\label{fig:consbias}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




