\section{Introduction}
\label{sec:Intro}
%FLOW: Introduce GNNs. State the need for sparsification. Introduce sparsification.
%State the contributions of this work.

% Graph Neural Networks (GNNs)~\cite{GNNBook2022,zhou2020graph} have emerged as powerful tools for learning from graph-structured data with applications in domains such as social networks~\cite{wu2021self}, biological networks~\cite{muzio2021biological}, and recommendation systems~\cite{yu2022graph}. Despite their success, the computational and memory requirements of GNNs can be significant, especially for large-scale graphs.
Given a graph $\gG \triangleq (\gV, \gE, \mX)$ with node features $\mX$, the goal of a graph neural network (GNN) with hidden dimension $H$ is to learn an encoding $\vh_v \in \mathbb{R}^H$ of each node $v \in \gV$ that encodes the neighborhood information of $v$. The encoding $\vh_v$ is used to compute an output $\vy_v$ for a variety of predictive tasks (detailed in \S\ref{sec:prelim}). 
GNNs \cite{GNNBook2022,zhou2020graph} are effective tools for learning from graph-structured data, utilized in areas like social networks~\cite{wu2021self}, biological networks~\cite{muzio2021biological}, computational physics~\cite{jessicafinite}, and recommender systems~\cite{yu2022graph}. However, their computational and memory demands can be substantial, particularly for large graphs. 
The two primary steps in a Graph Neural Network (GNN) are \textit{aggregation} and \textit{update}. For each node, the aggregation step involves accumulating embeddings from neighboring nodes using sparse matrix operations such as sparse-matrix dense-matrix multiplication (\texttt{SpMM}) and sampled dense-matrix dense-matrix multiplication (\texttt{SDDMM}) \cite{zhang2024graph}. During the update step, a node updates its own embedding based on the aggregated information using dense matrix operations (\texttt{MatMul}) \cite{zhang2024graph}. 
Approximately \textbf{70\%} of the total cost is attributed to the \texttt{SpMM} operations \cite{liu2023dspar}.
Thus, by systematically removing nonessential edges, \textit{graph sparsification}  \cite{chen2023demystifying, hashemi2024comprehensive} reduces computational costs and memory requirements,  and significantly speeds up GNN training and inference.

% Sid: Sparsification can reduce memory requirements when spraseTensor represents the adjacency matrix, which will have a much smaller entries.


%sid: parapharase the properly
%from introduction : https://arxiv.org/pdf/2405.14260
%GNNs work through iterative aggregation and update processes. Iterative aggregation accumulates embeddings from neighboring nodes through sparse matrix-based operations such as sparse-dense matrix multiplication (\texttt{SpMM}) and sampled dense-dense matrix multiplication (\texttt{SDDMM})~\cite{zhang2024graph}. The update process uses dense matrix-based operations to update the central nodesâ€™ embeddings (\texttt{MatMul})~\cite{zhang2024graph}. \texttt{SpMM} typically contributes the most substantial part ($\sim70\%$) to the computational demands~\cite{liu2023dspar}. 
%\emph{Graph sparsification}~\cite{chen2023demystifying, hashemi2024comprehensive} reduces the number of edges in GNNs while keeping the essential edges.
%Since the execution time of \texttt{SpMM} is directly related to the number of edges in the graph, sparse subgraphs can significantly accelerate GNN training and inference.

A sparse subgraph $\tilde{\gG} \triangleq (\gV, \tilde{\gE},\mX)$ contains at most $q\%$ of original edges, $\tilde{\gE} \subseteq \gE$ (defined in \S\ref{sec:prelim}; $q$ is an input parameter).
A graph can be sparsified using two broad approaches: unsupervised and supervised.
Unsupervised graph sparsification methods, such as spectral sparsification~\cite{batson2013spectral} and spanners~\cite{dragan2011spanners}, only focus on the structural characteristics of graphs, neglecting downstream tasks and the differences between homophilic (similar nodes connect) and heterophilic graphs (dissimilar nodes connect). This oversight can result in sparsified graphs that do not effectively support tasks like node classification or link prediction~\cite{zheng2020robust}. 
In contrast, supervised graph sparsification methods~\cite{zheng2020robust} aim to reduce graph complexity while maintaining relevance to downstream tasks~\cite{luo2021learning,wu2023alleviating,sparsegat}. Methods such as SparseGAT~\cite{sparsegat} and SuperGAT~\cite{kim2022find} are considered \emph{implicit sparsifiers} as they do not create a standalone sparse subgraph for independent use, and thus they do not reduce GNN memory requirements. In contrast, NeuralSparse~\cite{zheng2020robust} is an \emph{explicit sparsifier} that constructs a subgraph based on a neighborhood size, but this approach can lack precise control over sparsity and may retain unnecessary edges. 
%Another supervised sparsification method, PDTNet~\cite{luo2021learning}, is unsuitable for large graphs as it requires computationally expensive Singular Value Decomposition (SVD).
%The existing supervised sparser often lacks exact sparsity control. 
The vast search space for sparse subgraphs makes it hard for supervised sparsifiers to find an optimal sampling distribution within limited iterations. Further, sparsifier modules can be compute-intensive with limited support for batch processing.

%On the other hand, Supervised graph sparsification~\cite{zheng2020robust} methods aim to reduce the complexity of graphs while preserving their relevance to a specific downstream task~\cite{luo2021learning,wu2023alleviating,sparsegat} thus adapted across graph types.
%Supervised sparsification methods such as SparseGAT~\cite{sparsegat} are what we call \emph{implicit sparsifiers} because they do not explicitly construct a sparse subgraph that can be used independently outside of the training process to lighten the memory requirements on the GNN. 
% Instead, they operate on the full adjacency matrix (or the original dense graph) during message passing and dynamically prune edges during computation. 
% In contrast, a method like NeuralSparse~\cite{zheng2020robust}  is an \emph{explicit sparsifiers} that learns $k$-neighbors subgraph and uses it later in downstream GNNs. The sparsity is controlled through the parameter node neighbor sample size $k$, which may not give an exact number of edges required and often retains unnecessary edges from the node's neighborhood. 
% %Addtionally, NeuralSparse~\cite{zheng2020robust} learns binary masks over the entire edge set during training and doesn't support batch processing
% %For large graphs, these supervised sparsifiers
% Another supervised method, PDTNet~\cite{luo2021learning}, is also unsuitable for large graphs because it requires a Singular Value Decomposition (SVD) of the adjacency matrix, which is computationally expensive on large graphs. 
% Additionally, the batch-processing version may not be available or expensive due to neighborhood expansion problems.
% Finally, the sparse subgraph search space is vast, and often, the number of iterations is far too low for sparsifiers to find an ideal sampling distribution.
%Existing supervised sparsification methods such as SparseGAT~\cite{sparsegat} are what we call \emph{implicit sparsifiers} because they do not explicitly construct a sparse subgraph that can be independently used outside of the training process, and more importantly, they can be used to lighten the memory requirements on the GNN. Instead, they operate on the full adjacency matrix (or the original dense graph) during message passing and dynamically prune edges during computation. For instance, SparseGAT~\cite{sparsegat} requires attention scores for every edge in the full adjacency matrix before pruning, meaning the full graph is still processed during message passing. SimSparse~\cite{wu2023alleviating} operates by evaluating feature similarity across all edges, relying on the full adjacency matrix for similarity computation before selecting edges. 
%NeuralSparse~\cite{zheng2020robust} learns binary masks over the entire edge set during training, again requiring access to the full graph.  Due to such requirements, SparseGAT, Simparse, and NeuralSparse increase the memory requirements on the GNN while handling large graphs. PDTNet~\cite{luo2021learning} is also not suitable to handle large graphs because it requires Singular Value Decomposition (SVD) of the adjacency matrix which is computationally expensive on large graphs. 
%Due to these limitations of existing supervised sparsification methods, we ask the following question: \emph{How to design a memory-efficient, supervised spasifier that learns to explicitly construct the downstream-task (e.g., node classification) specific sparse subgraph?}
% 
% What can we say about \cite{Dspar}?
% \textbf{Motivation:} 
% Graph sparsification is necessary for scaling large graphs and preventing over-smoothing issues. For example, in node classification tasks, over-smoothing will cause the features of the vertices belonging to the same connected subgraph to be similar, and the vertices cannot be accurately classified. 
% Unsupervised graph sparsification methods, such as spectral sparsification, spanners, and k-nearest neighbors (k-NN), do not rely on labeled data. Instead, they use heuristics or mathematical properties to reduce graph size. These methods reduce edges based on graphs' structural information only, which limits their performance when combined with another learnable downstream model, such as Graph Neural Networks (GNNs). GNNs require both graph structural features and the features of vertices. Therefore, supervised graph sparsification methods offer superior performance through task-specific feedback and adapting to different graph types (homophilic and heterophilic) by learning from labeled data.
% Researchers have been actively working on developing neural network-based supervised graph sparsification approaches; some notable methods are NeuralSparse~\cite{zheng2020robust}, PDTNet~\cite{luo2021learning}, SimSparse~\cite{wu2023alleviating}. Most of the existing methods have their limitations. 
% NeuralSparse learns a $k$-neighbor subgraph by repeatedly sampling edges for each vertex of a graph. However, since vertex degree distribution is not uniform, the choice of hyperparameter $k$ limits its learning power and may lead to suboptimal performance.
% PDTNet achieves good performance on small graphs but is not applicable to very large graphs because it requires Singular Value Decomposition (SVD).
% SimSparse uses feature similarity to learn edge weight for sparsification and uses a threshold to sparsify, which, in turn, loses prune control. A variant of this algorithm uses a feature dimension reduction technique to fit large graphs into memory and requires additional training time.
%\noindent\textbf{Our Contributions.} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{figure}[!htbp]
\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{Figures/SGS-GNN0.pdf}
\caption{Simplified architecture of 
\sgs.}
\label{fig:simplesgsgnn}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%For instance, on an input graph $\gG = (\gV,\gE)$, NeuralSparse requires $\Theta(|\gV|^2)$ space to store the learned binary mask whereas \sgs requires storing only the edge probabilities, requiring only $\Theta(|\gE|)$ space.

% To address these limitations, we propose \sgs; a fast, lightweight, practical, supervised graph sparsification algorithm that explicitly learns the probability distribution over the edges of the input graph and explicitly constructs a sparse subgraph from that learned distribution for the downstream GNN.  
% In a nutshell, \sgs has three learnable components (Fig.~\ref{fig:simplesgsgnn}): (i) the edge probability encoding, (ii) the sparse subgraph sampler, and (iii) the downstream graph model (e.g. GNN).
% The edge probability encoding component, termed as \edgemlp is a mapping function that takes features of edges (e.g., attributes of connected nodes or edge-specific data) and encodes them into probabilities. These probabilities represent the likelihood of specific edges existing in the learned sparse subgraph. The subgraph sampler component samples a subgraph following the learned edge probabilities. 
% Since each edge has a given probability of being present in the sampled subgraph, the constructed sparse subgraph is not necessarily unique. As will be shown later that, this non-uniqueness helps the model explore for better subgraphs of similar sparsity helpful for the downstream task. 
To address these limitations, we propose \sgs (\S\ref{sec:Method}), a fast and lightweight supervised graph sparsifier that learns the edge probability distribution of an input graph to construct a sparse subgraph for downstream GNNs. \sgs consists of three components (Fig.~\ref{fig:simplesgsgnn}): (i) edge probability encoding, (ii) sparse subgraph sampler, and (iii) downstream GNN. The edge probability encoding, called \edgemlp, maps associated node features of edges into probabilities indicating the likelihood of specific edges to exist in the learned sparse subgraph. The subgraph sampler then samples a subgraph based on these learned probabilities. \emph{Finally, the GNN can be any message passing through a neural network.} 
\sgs provides a unique advantage by allowing the downstream GNN to operate solely on a reduced sparse graph instead of the input graph and supports batch processing for large graphs.
%and the conditional update of the probability learning module leads to faster training. 
In addition to the task-specific loss, one of the regularizers promotes homophily in sampled subgraphs, enhancing prediction quality in heterophilic scenarios.
% In that sense, this module is not specific to \sgs; 
%however, \sgs is flexible enough to accommodate any GNNs, as shown in the experiments section. 
% 
% \sgs offers a significant benefit that other supervised sparsifiers cannot offer, which is that the downstream GNN operates only on the reduced sparse graph rather than the input graph. \sgs supports batch processing for very large graphs, and the conditional update of the probability learning module offers a faster training run-time.
% In addition to the task-specific loss, one of the regularizers encourages homophily in the sampled subgraphs, which helps to improve prediction quality in heterophilic scenarios. 
% 
%% \naheed{@Sid,expand upon the above paragraph based on the discussion below. Then comment this paragraph.}
% The \edgemlp employs a Multi-layer Perceptrons (MLPs) to learn the edge weight and Gumbel-Softmax activation to turn these into sampling probabilities. Then, we sample the required number of edges for the downstream model. The loss from downstream tasks back-propagates the need to update the GNNs and edge weight learning model. This approach works well for a graph that is small enough; however, we cannot expect to compute sampling probabilities of all edges simultaneously for a huge graph. Therefore, we will apply batch processing on edges to compute edge weight and then sample a subset of these edges for the downstream task. Note that we avoid gradient accumulation on batch processing of edges to make the training process stochastic, which helps train GNN models faster. One crucial step is selecting a batch of edges such that the vertices in a batch of edges have high locality and preferably from within a cluster. We consider very fast graph partitioning and vertex re-ordering techniques to achieve this.
% 
% The proposed supervised sparsification drops edges that are not helpful for downstream tasks. The method could potentially help remove noisy edges from the graph and also potentially increase homophily to address harmful heterophiles.
The key contributions of our work are:
% labelwidth=!
\begin{enumerate}[wide, labelindent=2pt,itemsep=1pt,topsep=1pt]
    \item We propose a novel graph sparsification technique, \sgs, that works for both homophilic and heterophilic graphs (\S\ref{sec:Method}). With \sgs, users can control the amount of sparsity, efficiently learn sampling probabilities, and scale to large graphs with batch processing. \sgs incorporates degree-proportionate edge weight as \emph{prior} sampling distribution to guide the search for sparse subgraphs. 
    The conditional update module in \sgs encodes edge probabilities to further optimize performance and adaptability.

    \item We provide theoretical bounds and empirical validation for \sgs, ensuring quality of the learned embeddings.
    
    \item Extensive experiments on 33 benchmark graphs (21 heterophilic, 12 homophilic) show that \sgs outperforms almost all original dense heterophilic graphs with up to $30\%$ improvement in F1-scores (on a scale of 100) using only $20\%$ of the edges (\S\ref{subsubsec:fixedsampler}). In homophilic graphs, \sgs remains competitive. 
    In similar settings, \sgs outperforms sparsification-based GNN methods such as GraphSAINT~\cite{zeng2019graphsaint}, NeuralSparse~\cite{zheng2020robust}, SparseGAT~\cite{sparsegat}, MOG~\cite{zhang2024graph}, DropEdge~\cite{rong2019dropedge}, with geometric mean improvement of $4-7\%$  in F1-scores. 
    % with 20\% of edges (\S\ref{subsubsec:relatedsparsifier}).
    
    \item Additionally, \edgemlp surpasses fixed distribution sparsifiers and requires fewer epochs. \sgs is significantly faster than related supervised sparsifiers (\S\ref{subsubsec:runtime}).
\end{enumerate}