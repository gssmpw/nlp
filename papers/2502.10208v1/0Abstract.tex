
\begin{abstract}
We propose \sgs,  a novel supervised graph sparsifier that learns the sampling probability distribution of edges and samples sparse subgraphs of a user-specified size to reduce the computational costs required by GNNs for inference tasks on large graphs.  
\sgs employs regularizers in the loss function to enhance homophily in sparse subgraphs, boosting the accuracy of GNNs on heterophilic graphs, where a significant number of the neighbors of a node have dissimilar labels.  
\sgs also supports conditional updates of the probability distribution learning module based on a prior, which helps narrow the search space for sparse graphs.
\sgs requires fewer epochs to obtain high accuracies since it learns the search space of subgraphs more effectively than methods using fixed distributions such as random sampling. 
Extensive experiments using $33$ homophilic and heterophilic graphs demonstrate the following: 
$(i)$ with only $20\%$ of edges retained in the sparse subgraphs, \sgs improves the F1-scores by a geometric mean of $4\%$ relative to the original graph; on heterophilic graphs, the prediction accuracy is better up to $30\%$.  $(ii)$ \sgs outperforms state-of-the-art methods with improvement in F1-scores of $4-7\%$ in geometric mean with similar sparsities in the sampled subgraphs, and $(iii)$ compared to sparsifiers that employ fixed distributions, \sgs requires about half the number of epochs to converge.
\end{abstract}