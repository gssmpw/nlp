\section{Experiments}
\label{sec:experiment}
% In this section, we study the following questions.
% \textbf{Q1.} How does \sgs perform compared to baselines that employ a fixed distribution over edges (e.g., \textbf{Random} from DropEdge, \textbf{Edge} sampler from GraphSAINT and effective resistance from spectral sparsification), rather than learning it? 
% \textbf{Q2.} How does \sgs fair against related sparsification methods?
% \textbf{Q3.} How does \sgs perform with different levels of sparsity and homophily?
% \textbf{Q4.} How does \sgs perform with other GNNs?
% and finally, \textbf{Q5.} What are the contributions of each component of \sgs?
% \textbf{Q5.} What are the convergence and runtime of \sgs?

% \subsection{Dataset, Setup, Methods}
% \label{subsec:dataset}
We experimented with $21$ heterophilic and $12$ homophilic benchmark datasets of varying sizes and homophily.
%Additionally, we generated synthetic graphs to investigate different homophily levels while maintaining node features for ablation and scaling studies, with a focus on node classification tasks. 
Details about the datasets are provided in Appendix~\ref{app:dataset}.
All experiments are carried out $10$ times on a 24GB NVIDIA A10 Tensor Core GPU with 500GB internal memory, and with a data split of $20\%/40\%/40\%$ (train/validation/test) unless specified otherwise. For baseline models, we follow the settings set by respective authors. We use $2$ message passing layers for \sgs and a hidden layer dimension of $H=256$ for both $\edgemlp_\phi$ and $\texttt{GNN}_\theta$. We set a dropout rate of $0.2$ and use the Adam optimizer with a learning rate of $0.001$. We trained all models for a maximum epoch of $500$ with early stopping.
The edge batch size is $500K$, and the number of partitions for METIS is $n=\ceil{{|\gE|}/{500K}}$. The percentage of edge sample is set to $q=20\%$ following DropEdge~\cite{rong2019dropedge}. We take $10$ samples of sparse subgraphs during inference. The model with the best validation F1-score is selected for testing. 
% The implementations uses PyTorch~\cite{paszke2019pytorch}, PyTorch Geometric~\cite{fey2019fast}, and Deep Graph Library~\cite{wang2019deep}. 
Our source codes are anonymously provided on Github\footnote{\textcolor{blue}{\url{https://github.com/anonymousauthors001/SGS-GNN/}}}.
% GitHub\footnote{\href{https://github.com/}{GitHub link for \sgs and all other methods.}}.

\noindent\textbf{Baselines.} We use ClusterGCN~\cite{chiang2019cluster} to evaluate performance on the original large graphs. DropEdge~\cite{rong2019dropedge}, and GraphSAINT (GSAINT-E)~\cite{zeng2019graphsaint} are used as fixed distribution samplers. For Mixture of Graph (MOG)~\cite{zhang2024graph}, we use $3$ experts and their recommended settings. We adjust the neighborhood sample size of NeuralSparse~\cite{zheng2020robust} to have a similar sparsity as ours for a fair comparison. Additionally, we included SparseGAT~\cite{sparsegat} as another supervised method for generating sparse graphs.

\subsection{Key Findings}
\label{subsec:results}
In this section, we discuss the key findings supporting \sgs. Empirical evaluation of various design choices in \sgs and sensitivity of \sgs to different values of hyperparameters are presented in Appendix~\ref{app:ablationstudy}.

\subsubsection{\sgs vs. Fixed distr. sparsifiers} 
\label{subsubsec:fixedsampler}
%Tables~\ref{tab:hetero_graphs} and ~\ref{tab:homophilic_graphs} 
We compare \sgs with fixed edge distribution sparsifiers like \textit{Random} from DropEdge, \textit{Edge} sampler from GraphSAINT, and \textit{Effective resistance (ER)}. Table~\ref{tab:hetero_homo_graphs} presents F1-scores, with the last row summarizing overall performance. The \textit{Org. Graph} represents the original dense graph's performance computed using ClusterGCN. Our learnable sampler \edgemlp significantly outperforms fixed distribution samplers and original dense graphs in heterophilic datasets. 
% 
% The results indicate that using all edges in the downstream GNN is not optimal. Although we use the same METIS partition as ClusterGCN, the subgraph employed by \sgs is a more effective choice than the entire graph from that partition.
% 
In homophilic graphs, we observe a smaller margin of improvement, but \sgs still outperforms other baselines. 
% Effective resistance is a relatively better prior than others but it underperforms compared to \sgs.
% is computationally expensive 
% ; the faster degree weighted sampler yields similar performance and is utilized in \sgs.


\input{Results/4.1HeteroGraphs}
% \FloatBarrier
% \input{Results/4.2HomophilicGraphs}

\subsubsection{\sgs vs other GNN based Sparsifiers}
\label{subsubsec:relatedsparsifier}
% From NeuralSparse~\cite{zheng2020robust}, SparseGAT~\cite{sparsegat}, DropEdge~\cite{rong2019dropedge}, and GraphSAINT~\cite{zeng2019graphsaint}, ClusterGCN~\cite{chiang2019cluster}.
Table~\ref{tab:relatedsparse} compares \sgs with related sparsification-based GNNs. We use GCN as the GNN module in our \sgs. 
% The \texttt{OOM} refers to the Out of Memory error while executing the provided code. 
\sgs significantly outperforms competing methods with a geometric mean improvement of $4-5\%$ with only $20\%$ of edges. Under similar settings, \sgs significantly outperforms in heterophilic graphs and remains competitive in homophilic graphs.
We do not include results with large-scale graphs here since MOG and NeuralSparse goes out of memory in our computing environment. However, \sgs can handle large graphs and the corresponding results are provided in Table~\ref{tab:hetero_homo_graphs}. 
% as part of fixed distribution samplers. 
% More results on large-scale graphs are added in Appendix~\ref{app:runtime}.
% \Sid Update when all results done. OOM large graphs.

\input{Results/4.4RelatedSparse}

\subsubsection{Sparsity vs. Accuracy vs. Homophily}  
We analyzed the performance of \sgs across varying homophily levels using synthetic and benchmark graphs in Fig.~\ref{fig:sparsevshomophily}. Synthetic graphs were generated with node homophily $\gH_n$ at degree $d$ by connecting $\ceil{d\gH_n}$ edges to same-type neighbors and $d-\ceil{d\gH_n}$ edges randomly. Results in Fig.~\ref{subfig:sparsityvshomophily} show that high homophily yields good performance regardless of sparsity, while high sparsity benefits heterophily, with optimal performance seen at $30\%-40\%$ edge retention for heterophily levels $0.3-0.4$. Furthermore, Fig.~\ref{subfig:sparsityvsaccuray} indicates that while more edges improves accuracy on homophilic graphs, high sparsity can be advantageous on heterophilic graphs such as \texttt{reed98} and \texttt{amherst41}, where we observe best performance at $\sim20\%$ sparsity.

\begin{figure}[!htbp]
    \centering
    %\subfigure{\includegraphics[width=0.48\linewidth]{Figures/SparsityvsHomophily.png}
    \subfigure{\includegraphics[width=0.48\linewidth]{Figures/SGS-synthetichomophily.pdf}
    \label{subfig:sparsityvshomophily}} 
    \hfill
    % \subfigure{\includegraphics[width=0.48\linewidth]{Figures/SparsityvsAccuracy2.png}
    \subfigure{\includegraphics[width=0.48\linewidth]{Figures/SGS-sparsityvsdataset.pdf}
     \label{subfig:sparsityvsaccuray}}
    \caption{(Left) Heatmap of F1 scores (in \%) at different homophily and sparsity levels ($q$) in \texttt{Cora} synthetic graphs. (Right) F1-scores of \sgs at different sparsity for homophilic (solid line) and heterophilic (dashed line) graphs.}
    \label{fig:sparsevshomophily}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{figure}[!htbp]
% \centering
% \includegraphics[width=0.8\linewidth]{Figures/SparsityvsAccuracy.png}
% \caption{Figures shows F1 scores (in \%) of \sgs at different sparsity for homophilic (solid line) and heterophilic (dashed line) graphs.}
% \label{fig:sparsityvsaccuracy}
% \end{figure}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{figure}[!htbp]
% \centering
% %\includegraphics[width=1.0\linewidth]{Figures/SparsityvsHomophily.png}
% \includegraphics[width=1.0\linewidth]{Figures/SparsityvsHomophilySmallCora.png}
% \caption{Heatmap of F1 scores (in \%) at different homophily and sparsity levels at Corasynthetic graphs. }
% \label{fig:sparsityvshomophily}
% \end{figure}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %Sparsity vs Homophily. CoraSynthetic, Node 1050, Edges 21000, Average degree 20

Fig.~\ref{fig:edgehomophily} shows that the sampled subgraph of \sgs has a higher \emph{edge homophily} than those by the fixed distribution sparsifiers. This is expected due to our $\gL_\mathrm{assor}$ regularizer.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/SGS-homophily.pdf}    
    \caption{Edge homophily of selected subgraphs from different fixed distribution samplers vs. subgraphs from training and evaluation phase of \sgs.}
    \label{fig:edgehomophily}
\end{figure}

\subsubsection{Convergence}
\label{subsubsec:convergence}
To compare the sparsifiers in terms of convergence, we terminate training when the std. dev of loss in five consecutive epochs is $\leq 1e^{-3}$. Fig.~\ref{fig:convergence} shows the bar plot of the number of epochs required for the methods to converge. \sgs requires fewer iterations than other fixed distribution sparsifiers highlighting the benefit of learning the distribution.

\begin{figure}[!htbp]
    \centering    \includegraphics[width=1.0\linewidth]{Figures/SGS-Convergence.pdf}
    \caption{Number of epochs required by \sgs to converge compared to other samplers under the same settings.}
    \label{fig:convergence}
\end{figure}

% \begin{figure}[!htbp]
%     \centering
%     \subfigure[Fixed Distribution Samplers]{\includegraphics[width=0.48\linewidth]{Figures/SGS-Convergence.png}
%     \label{subfig:convergefixdistribution}} 
%     \hfill
%     \subfigure[With conditional updates]{\includegraphics[width=0.48\linewidth]{Figures/SGS-Convergence.png}
%      \label{subfig:convergeconditional}}
%     \caption{Number of epochs required by different approaches to converge.}
%     \label{fig:convergence}
% \end{figure}

\subsubsection{Efficiency}
\label{subsubsec:runtime}
Fig.~\ref{fig:runtime} compares the training times per epoch of \sgs with other GNN based sparsifier baselines. Under similar conditions, \sgs is more efficient than NeuralSparse, MOG, and competitive with SparsGAT. SparseGAT is an implicit sparsifier; thus, unlike \sgs, it is not memory efficient and cannot handle large graphs. Unsupervised sparsifiers such as DropEdge and GraphSAINT are more efficient but not always as effective as \sgs (c.f. Tab.~\ref{tab:relatedsparse}). 
% Numerical values of this plot are added in Appendix~\ref{app:runtimerelated} for brevity.
% % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{figure}[!htbp]
%     \centering
%     \subfigure[Runtime]{\includegraphics[width=0.48\linewidth]{Figures/SGS-runtime.png}
%     \label{subfig:runtimeneural}} 
%     \hfill
%     \subfigure[Conditional updates]{\includegraphics[width=0.48\linewidth]{Figures/SGS-runtime.png}
%      \label{subfig:runtimeconditional}}
%     \caption{Comparison of mean training time per epoch.}
%     \label{fig:runtime}
% \end{figure}
% % % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{figure}[!htbp]
% \centering
% \includegraphics[width=0.6\linewidth]{Figures/SGS-runtime2.png}
% \caption{Mean training time per epoch of \sgs vs NeuralSparse.}
% \label{fig:runtime}
% \end{figure}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!htbp]
\centering
\includegraphics[width=1.0\linewidth]{Figures/SGS-relatedruntime.pdf}
\caption{The bar shows the mean training time (s) per epoch ($\log$ scale) of \sgs and related methods. The $\times$ in the bar indicates out of memory. Under similar conditions, \sgs is faster than NeuralSparse and MOG and competitive with SparseGAT.}
\label{fig:runtime}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% 
% \Sid Memory footprint
% 