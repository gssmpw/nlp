\section{Theoretical Analysis}
\subsection{Notations}
We dedicate Table~\ref{tab:Notation} to index the notations used in this paper. Note that every notation is also defined when it is introduced.
\begin{table*}[h!]
\caption{Notations.}\label{tab:Notation}%\\
\centering  
% \resizebox{\textwidth}{!}{
\begin{tabular}{l l l}
\toprule
 $\gG$ &$\triangleq$ & Input graph with a vertex set $\gV$, an edge set $\gE$, and features $\mX$\\
 $\boldsymbol{A}$ &$\triangleq$ & Adjacency matrix of $\gG$\\
 $\gE$ & $\triangleq$ & Edges of $\gG$\\
 $\gV$ & $\triangleq$ & Nodes of $\gG$\\
 $\mX$ & $\triangleq$ & Matrix containing node features of $\gG$\\
 $\vy$ & $\triangleq$ & Vector of node labels of $\gG$\\
 $C$ & $\triangleq$ & An ordered set containing all possible node labels of $\gG$\\
 $F$ & $\triangleq$ & Dimension of node features in $\gG$\\
 $L$ & $\triangleq$ & Number of GNN layers\\
 $H$ & $\triangleq$ & Node embedding dimension\\ 
 ${\mH}$ & $\triangleq$ & Node embedding matrix\\
 $\vh_u$ & $\triangleq$ & Embedding of node u\\
 $\vw$ & $\triangleq$ & Vector of edge weights in  $\gG$\\
 $q$ & $\triangleq$ & Ratio of \# edges in sparse graph and \# edges in input graph in \%\\
 $k$ & $\triangleq$ & \# edges in the sparse graph, $k\triangleq\floor{\frac{q|\gE|}{100}}$\\ 
 $\tilde{p}$ & $\triangleq$ & Learned probability distribution by \sgs \\
 $\tilde{\gE}$ & $\triangleq$ & Set of edges sampled from $\gE$ by \sgs following $\tilde{p}$\\
$\tilde{\gG}$ &$\triangleq$ & Sparse subgraph $(\gV,\tilde{\gE},\mX)$ constructed by \sgs \\  
 $\mA_{\tilde{\gG}}$ or $\tmA$ & $\triangleq$ & Adjacency matrix of $\tilde{\gG}$\\
 $\tilde{\vw}$ & $\triangleq$ & Edge weight of sparse graph learned by \sgs \\
 $p_\mathrm{prior}$ & $\triangleq$ & Probability distribution of a fixed prior on $\gG$ \\
  $\tilde{p}_a$ & $\triangleq$ & Augmented learned probability distribution  \\
 $p^*$ & $\triangleq$ & True probability distribution known by the idealized learning ORACLE\\
 ${\gE^*}$ & $\triangleq$ & Set of edges sampled from $\gE$ by the learning ORACLE following distribution $p^*$\\   
 $\gG^*$ &$\triangleq$ & True sparse subgraph $(\gV,\gE^*,\mX)$ constructed by the learning ORACLE \\
 $\mA_{\gG^*}$ or $\mA^*$ & $\triangleq$ & Adjacency matrix of $\gG^*$\\
 % $\gG^*$ &$\triangleq$ & True sparse subgraph constructed by the idealized learning ORACLE \\
 % $\mA_{\gG^*}$ or $\mA^*$ & $\triangleq$ & Adjacency matrix of $\gG^*$\\
 $\gL_\mathrm{CE}$ & $\triangleq$ & Cross entropy loss\\
 $\gL_\mathrm{assor}$ & $\triangleq$ & Assortative loss\\
 $\gL_\mathrm{cons}$ & $\triangleq$ & Consistency loss\\
$\gL$ & $\triangleq$ & Total loss\\ 
 
 
 
 
 
 
 \bottomrule
\end{tabular}
% }
\end{table*}
\subsection{Bounding \#common edges wrt. true subgraph}
\label{theo:commonedges}
Let $\mathcal{E}^*$ and $\mathcal{\tilde{E}}$ denote the ordered collection of edges sampled by the idealized learning ORACLE according to true distribution $p^*$ and by \sgs according to learned probability $\tilde{p}$ respectively. For analytical convenience, let us assume that both learning algorithms sample $k = \floor{q|\mathcal{E}|/100}$ edges with replacement independently.
 
% We will first show that $\mathbf{Pr}(\mathcal{E}_i^* = \mathcal{\tilde{E}}_i) \geq \min_j (p^*_i, \tilde{p}_i)$ and $\min(p^*_i,\tilde{p}_i) = 1- \frac{1}{2} \|\tilde{p} - p^*\|_1$. These two results will lead us to prove that $\mathbf{Pr}(\mathcal{E}_i^* = \mathcal{\tilde{E}}_i) \geq 1 - \frac{\epsilon}{2}$. 
First, we will prove lemma~\ref{lem:singleedge}, which show that the probability of an edge chosen by \sgs coincides with that chosen by the ORACLE has a lower bound. Finally, we will prove one of the main results (Theorem~\ref{theo:commonedges}), which shows that given $q \in [0,100]$, we can lower-bound the expected number of common edges between \sgs and the learning ORACLE. 

\begin{lemma} 
\label{lem:singleedge}
For any arbitrarily chosen $i \in \{1,2,\ldots, k\}$
\[
\mathbf{Pr}(\mathcal{E}_i^* = \mathcal{\tilde{E}}_i) \geq \sum_{j=1}^{|\mathcal{E}|} \frac{(p^*_j + \tilde{p}_j - \epsilon)^2}{4},
\]
where $k = \floor{q|\mathcal{E}|/100}$ and $0 \leq q \leq 100$ is a user-specified parameter and $\epsilon\in [0,1]$ is the error.
\end{lemma}

\begin{proof} We prove the above lemma in two parts.


\paragraph{Part 1: Universal approximation of probability distribution over edges.}
%\label{tho:uap}
The Universal Approximation Theorem~\cite{cybenko1989approximation,augustine2024survey} states that a feed-forward neural network with at least one hidden layer and a finite number of neurons can approximate any continuous function $f: \mathbb{R}^n \rightarrow \mathbb{R}$ on a compact subset of $\mathbb{R}^n$, given a suitable choice of weights and activation functions. 

In our case, $p^* = f$ is the true edge probability distribution for the downstream task, $\tilde{p} = f_{\text{MLP},\phi}$ is the learned approximate distribution and $\vx_e$ is a vector of edge features, for instance, $\vx_e =  ((\vh_u - \vh_v) \oplus (\vh_u \odot \vh_v))$ as used in equation~\ref{eq:w_uv}. The following universal approximation property holds for the module I component of \sgs,
\begin{equation}
\label{eq:uapp}
\sup_{e \in \mathcal{E}} \|\tilde{p}(\vx_e) - p^*(\vx_e)\|_1 \leq \epsilon.
\end{equation}
 Here, we have two underlying assumptions: (i) the optimal distribution $p^*$ is a function of node features $\mX$ and (ii) $\mX$ is a compact subset (bounded and closed) of Euclidean space $\mathbb{R}^n$. The first assumption is made to simplify the problem. The second assumption is quite practical since the node features are typically normalized. Hence, we can show that the embeddings $\vh_u,\vh_v$, which are continuous images of $\mX$, are also compact due to the extreme value theorem. As a result, the edge features $\vx_e$ which, in a sense, \emph{lifts} the end-point node features into higher-dimensional Euclidean space are also compact. The approximation error $\epsilon$ can be made arbitrarily small by increasing the capacity of the MLP, e.g., adding more neurons or layers. 

\paragraph{Part 2: Common edges wrt. optimal subgraph.}

The event $\mathcal{E}_i^* = \mathcal{\tilde{E}}_i$ means that both $\mathcal{E}_i^*$ and $\mathcal{\tilde{E}}_i$ contain the same edge. But there are $|\mathcal{E}|$ such candidates. Hence, the probability of this event is given by,

\begin{align*}
    \mathbf{Pr}(\mathcal{E}_i^* = \mathcal{\tilde{E}}_i) &= \sum_{j=1}^{|\mathcal{E}|} \mathbf{Pr}(\mathcal{E}_i^* = \mathcal{E}_j \land \mathcal{\tilde{E}}_i = \mathcal{E}_j), \\
    &= \sum_{j=1}^{|\mathcal{E}|} \mathbf{Pr}(\mathcal{E}_i^* = \mathcal{E}_j) \cdot \mathbf{Pr}(\mathcal{\tilde{E}}_i = \mathcal{E}_j), \\
    & = \sum_{j=1}^{|\mathcal{E}|} p^*_j \cdot \tilde{p}_j, \\
    &\geq \sum_{j=1}^{|\mathcal{E}|} \frac{(p^*_j + \tilde{p}_j - |p^*_j - \tilde{p}_j|)^2}{4}, \\
    & \geq \sum_{j=1}^{|\mathcal{E}|} \frac{(p^*_j + \tilde{p}_j - \epsilon)^2}{4}.
\end{align*}
The second line follows since the optimal sampler is a different algorithm independent from the sampler used in \sgs. The last line follows because $\|p^*_j - \tilde{p}_j\|_1 \leq \epsilon \implies |p^*_j - \tilde{p}_j| \leq \epsilon$ (from eq.~\ref{eq:uapp}). 
\end{proof}

% \begin{lemma}
% \[
%     \min(p^*_i,\tilde{p}_i) = 1- \frac{1}{2} \|\tilde{p} - p^*\|_1
% \]
% \end{lemma}
% \begin{proof}
%     It is known (for instance, see~\cite{xie2024distributionally}) that the total variation distance $d_{TV}(\tilde{p},p^*)$ satisfies
%     \[
%     d_{TV}(\tilde{p},p^*) = \frac{1}{2}\|\tilde{p} - p^*\|_1
%     = 1 - \min({\tilde{p},p^*})
%     \]
% \end{proof}

We have the following theorem that lower-bounds the number of common edges with respect to the optimal sampler $|\mathcal{E} ^* \cap \mathcal{\tilde{E}}|$: 
\begin{theorem}[Lower-bound]
\begin{equation}
\mathbb{E}[|\mathcal{E}^* \cap \mathcal{\tilde{E}}|] \geq k \sum_{j=1}^{|\mathcal{E}|} \frac{(p^*_j + \tilde{p}_j - \epsilon)^2}{4},
\end{equation}
where $k = \floor{q|\mathcal{E}|/100}$ and $0 \leq q \leq 100$ is a user-specified parameter.
\end{theorem}
\begin{proof}
    Since we are drawing $k$ edges independently at random, the theorem follows by applying the linearity of expectation on the following:
\begin{align*}
\mathbb{E}[|\mathcal{E}^* \cap \mathcal{\tilde{E}}|] = \mathbb{E}[\sum_{i=1}^k \mathbb{I}(\mathcal{E}_i^* = \mathcal{\tilde{E}}_i)] &= \sum_{i=1}^k \mathbf{Pr}(\mathcal{E}_i^* = \mathcal{\tilde{E}}_i) \\
& = k\cdot \mathbf{Pr}(\mathcal{E}_i^* = \mathcal{\tilde{E}}_i)\\
& \geq k \sum_{j=1}^{|\mathcal{E}|} \frac{(p^*_j + \tilde{p}_j - \epsilon)^2}{4}
\end{align*}
\end{proof}
This theorem shows that the expected number of common edges between the sample subgraph obtained by \sgs $\mathcal{\tilde{G}}$ and the true optimal sample subgraph $\mathcal{G}^*$ is non-trivial. 

% \paragraph{The implication of the lower-bound.} 
% (1) Suppose, the true distribution is uniform. In the best case scenario $\epsilon \rightarrow 0$ and $\tilde{p} = p^* = \frac{1}{|\mathcal{E}|}$. Thus there are at least $\frac{k}{|\mathcal{E}|}$ common edges between $\tilde{\gG}$ and $\gG^*$. However, since $k < \abs{\mathcal{E}}$, the lower-bound of $\mathbb{E}[|\mathcal{E}^* \cap \mathcal{\tilde{E}}|] \geq \frac{k}{|\mathcal{E}|}$ is not very useful even though the learned distribution is accurate. This suggests that \emph{learning the optimum uniform distribution is less likely to produce the optimum sparse subgraph}. 

% (2) Suppose, the true distribution is the Dirac distribution (often called the $\delta$ distribution) where all probability mass is concentrated on a single edge. In other words, suppose $\tilde{p} = p^* = \delta_{ij}$ where $\delta_{ij}$ is the Kronecker-delta. In such a skewed distribution, as $\epsilon \rightarrow 0$, the lower bound reduces to 
% \[
% \mathbb{E}[|\mathcal{E}^* \cap \mathcal{\tilde{E}}|] \geq k \sum_{j=1}^{|\mathcal{E}|} (\tilde{p}_j)^2 = k.
% \]
% This identity suggests that the sampled edges are expected to 100\% overlap with the true, optimal sparse subgraph.

\begin{theorem}[Upper-bound]
\begin{equation}
\mathbb{E}[|\mathcal{E}^* \cap \mathcal{\tilde{E}}|] \leq k (1 - \frac{\|p^* - \tilde{p}\|_1}{2}), 
\end{equation}
where $k = \floor{q|\mathcal{E}|/100}$ and $0 \leq q \leq 100$ is a user-specified parameter.
\end{theorem}
\begin{proof}
\begin{align*}
    \mathbf{Pr}(\mathcal{E}_i^* = \mathcal{\tilde{E}}_i) &= \sum_{j=1}^{|\mathcal{E}|} p^*_j \cdot \tilde{p}_j \\
    & \leq \sum_{j=1}^{|\mathcal{E}|} \min(p^*_j,\tilde{p}_j) \\
    &= 1 - d_{TV}(p^*,\tilde{p}) \\
    &= 1 - \frac{1}{2} \|p^* - \tilde{p}\|_1    
\end{align*}
\end{proof}
Here $d_{TV}$ is the total variation distance. The result used in the last line regarding $d_{TV}$ can be found in~\citet{xie2024distributionally}. 

\paragraph{The implication of the upper-bound.} 
When $\tilde{p} \rightarrow p^*$, the norm $\|p^* - \tilde{p}\|_1 \rightarrow 0$; therefore, the number of common edges could be close to $k$.

\subsection{Upper-bounding the error in the learned Adjacency matrix} 
With the bound proven earlier on the \#common edges by the sparse subgraph of \sgs with that by a learning ORACLE, in this section, we want to obtain an upper-bound on the error in terms of the norm of the Adjacency matrices. As adjacency matrices are used by GNNs for computing node embeddings, such result is important for obtaining error bound on the embeddings later on.

Let $\mA_{\tilde{\gG}}$ and $\mA_{\gG^*}$ be the corresponding adjacency matrices of the learned sparse graph $\tilde{\gG}$ and true optimal sparse graph $\gG^*$. The dimension of these matrices is the same as the input adjacency matrix $\mA_{\mathcal{G}}$ except that $\mA_{\mathcal{G}}$ is denser. Let us also denote the Frobenius norm of a matrix $\mA$ as $\|\mA\|_F$ and the spectral norm of $\mA$ as $\|\mA\|_2$. The Frobenius norm of $\mA$ is defined as $\sqrt{\sum_{ij} \mA^2_{ij}}$, whereas the spectral norm of $\mA$ is the largest singular value $\sigma_{max}(\mA)$ of $\mA$.


Since \sgs do not know the true probability distribution $p^*$, error is introduced in the learned adjacency matrix $\mA_{\tilde{\gG}}$ of the downstream sparse subgraph. We are interested in analyzing the expected error introduced in $\mA_{\tilde{\gG}}$ in terms of the spectral norm, to be precise, $\mathbb{E}[\|\mA_{\tilde{\gG}} - \mA_{\gG^*}\|_2]$. To this end, we will exploit the lower bound derived in Theorem 1 and the fact that $\|\mA\|_2 \leq \|\mA\|_F$. 

\begin{lemma}[Error in Adjacency matrix approximation] Let $\mA_{\tilde{\gG}}$ and $\mA_{\gG^*}$ be the corresponding adjacency matrices of the learned sparse graph $\tilde{\gG}$ and true optimal sparse graph $\gG^*$. If the downstream sampler sampled $k$ edges independently at random (with replacement) to construct those matrices following their respective distributions $\tilde{p}$ and $p^*$, then 
    \[
    \mathbb{E}[\|\mA_{\tilde{\gG}} - \mA_{\gG^*}\|_2] \leq \sqrt{2k(1-\sum_{j=1}^{\abs{\mathcal{E}}} \frac{(p^*_j + \tilde{p}_j - \epsilon)^2}{4})},
    \]
    where $k = \floor{q|\mathcal{E}|/100}$ and $0 \leq q \leq 100$ is a user-specified parameter.
\end{lemma}
\begin{proof}
Since the entries in adjacency matrices are either $0$ or $1$, the difference $\mA_{\tilde{\gG}}(i,j) - \mA_{\gG^*}(i,j)$ are in $\{-1,0,1\}$ for all $i,j$. The following holds by definition of Frobenus norm,

\[
\|\mA_{\tilde{G}} - \mA_{G^*}\|^2_F = \sum_{ij}(\mA_{\tilde{\gG}}(i,j) - \mA_{\gG^*}(i,j))^2.
\] 
As a result, only the non-zero entries in $\mA_{\tilde{\gG}} - \mA_{\gG^*}$ contribute to the square of Frobenius norm $\|\mA_{\tilde{G}} - \mA_{G^*}\|^2_F$.
The expected number of non-zero entries in $\|\mA_{\tilde{\gG}} - \mA_{\gG^*}\|^2_F$ corresponds to the expected cardinality $\abs{(\mathcal{\tilde{E}} \setminus \mathcal{E}^*) \cup (\mathcal{E}^* \setminus \mathcal{\tilde{E}})}$. Thus

\begin{align*}
    \mathbb{E}[\|\mA_{\tilde{\gG}} - \mA_{\gG^*}\|^2_F] &= \mathbb{E}[\abs{(\mathcal{\tilde{E}} \setminus \mathcal{E}^*) \cup (\mathcal{\tilde{E}} \setminus \mathcal{E}^*)}] \\ 
    &= \mathbb{E}[\abs{\mathcal{\tilde{E}}} + \abs{\mathcal{E}^*} - 2 \abs{\mathcal{\tilde{E}} \cap \mathcal{E}^*}] \\
    &= 2k - 2\mathbb{E}[\abs{\mathcal{\tilde{E}} \cap \mathcal{E}^*}] \\
    &\leq 2k - 2k \sum_{j=1}^{|\mathcal{E}|} \frac{(p^*_j + \tilde{p}_j - \epsilon)^2}{4} \\
    &= 2k (1 - \sum_{j=1}^{|\mathcal{E}|} \frac{(p^*_j + \tilde{p}_j - \epsilon)^2}{4}).
\end{align*}
Applying Jensen's inequality for convex functions, in particular, applying $(\mathbb{E}[\rX])^2 \leq \mathbb{E}[\rX^2])$ yields,
\begin{align*}
     (\mathbb{E}[\|\mA_{\tilde{\gG}} - \mA_{\gG^*}\|_F])^2 &\leq  \mathbb{E}[\|\mA_{\tilde{\gG}} - \mA_{\gG^*}\|^2_F] \\
     &\leq 2k (1 - \sum_{j=1}^{|\mathcal{E}|} \frac{(p^*_j + \tilde{p}_j - \epsilon)^2}{4}).
\end{align*}
Taking square-root on both sides yields,
\[
 \mathbb{E}[\|\mA_{\tilde{\gG}} - \mA_{\gG^*}\|_F] \leq \sqrt{2k (1 - \sum_{j=1}^{|\mathcal{E}|} \frac{(p^*_j + \tilde{p}_j - \epsilon)^2}{4})}.
\]
We obtain the theorem using the following relation between the Frobenius and spectral norms.
\begin{align*}
    \|\mA_{\tilde{\gG}} - \mA_{\gG^*}\|_2 &\leq \|\mA_{\tilde{\gG}} - \mA_{\gG^*}\|_F \\
    \implies \mathbb{E}[\|\mA_{\tilde{\gG}} - \mA_{\gG^*}\|_2] &\leq \mathbb{E}[\|\mA_{\tilde{\gG}} - \mA_{\gG^*}\|_F] \\
    &= \sqrt{2k (1 - \sum_{j=1}^{|\mathcal{E}|} \frac{(p^*_j + \tilde{p}_j - \epsilon)^2}{4})}
\end{align*}
\end{proof}
% \begin{corollary} Let us assume $\epsilon \rightarrow 0$ and the model learned the true pmf. Then the spectral norm approximation error is
%     \[
%     \mathbb{E}[\|\mA_{\tilde{\gG}} - \mA_{\gG^*}\|_2] \leq \sqrt{2k(1-\frac{1}{4\abs{\mathcal{E}}})}
%     \]
%     where $k = \floor{q|\mathcal{E}|/100}$ and $0 \leq q \leq 100$ is a user-specified parameter.
% \end{corollary}
% \begin{proof}
% Since we assumed $\epsilon \rightarrow 0$ and $p^*_j = \tilde{p}_j$, Theorem 3 reduces to
% \[
% \mathbb{E}[\|\mA_{\tilde{\gG}} - \mA_{\gG^*}\|_2] \leq \sqrt{2k(1- \frac{\sum_{j=1}^{\abs{\mathcal{E}}} (p^*_j)^2}{4})}
% \]
% For any probability mass function the following inequality holds
% $\sum_{i=1}^n p^2_i \geq 1/n$. This holds with equality when the distribution is uniform. Thus,
% \[
% \sum_{j=1}^{\abs{\mathcal{E}}} (p^*_j)^2 \geq \frac{1}{\abs{\mathcal{E}}}\\
% \implies \mathbb{E}[\|\mA_{\tilde{\gG}} - \mA_{\gG^*}\|_2] \leq \sqrt{2k(1-\frac{1}{4\abs{\mathcal{E}}})}
% \]
% \end{proof}
\subsection{Upper-bounding the error in the predicted node embeddings}
\label{theo:gcnembed}
We consider vanilla GCN as proof of concept to understand how the changes in the sparse subgraph affect the node embeddings produced by a trained GCN. Our goal is to analyze the respective encodings produced by an $L$-layer GCN when the input subgraphs are $\gG^*$ (corresponding to $\mA_{\gG^*}$) and $\tilde{\gG}$ (corresponding to $\mA_{\tilde{\gG}}$) respectively. For simplicity, we will shorten the matrices $\mA_{\gG^*}$ as $\mA^*$ and $\mA_{\tilde{\gG}}$ as $\tmA$. 

A single GCN layer is defined as,

\[
\mH^{(l+1)} = \sigma(\hat{\mA}\mH^{(l)}\mW^{(l)}),
\]
where $\hat{\mA} = \mD^{-1/2}\mA\mD^{-1/2}$ is the normalized adjacency matrix, $\mH^{(l)}$ is the input to the $l$-th layer with $\mH^{(0)} = \mX$, $\mW^{(l)}$ is the learnable weight matrix for $l$-th layer and $\sigma$ is non-linear activation function. Let us suppose an $L$-layer GCN produces embeddings $\tmH^{(L)}$ and $\mH^{*(L)}$ when it takes sparse matrices $\tmA$ and $\mA^*$ as input. We want to upper-bound,
\[
\mathbb{E}[\normLtwo{\tmH^{(L)} - \mH^{*(L)}}],
\]
in other words, the loss in the downstream node encodings is due to using our learned subgraph. 

\paragraph{Assumptions.} We assume that for all $l$, $\normLtwo{\mW} \leq \alpha < 1$ where $\alpha$ is a constant no more than 1. This is reasonable since each $\mW^{(l)}$ is typically controlled during training using regularization techniques, e.g., weight decay. Assuming that the input features in $\mX$ are bounded, we can also assume that there exists a constant $\beta$ such that $\forall l$, $\normLtwo{H}^{(l)} \leq \beta$. We also assume that $\sigma$ is \emph{Lipschitz continuous} with \emph{Lipschitz constant} $L_\sigma$; for instance,  activation functions such as \relu, sigmoid, or tanH are Lipschitz continuous. In particular, we assume \relu activation for our theoretical analysis because \relu has \emph{Lipschitz constant} $L_\sigma = 1$, which simplifies our analysis.

Under these assumptions, we have the following theorem,
\begin{theorem}[Error in GCN encodings]
For sufficiently deep L-layer GCN (large L), the error 
{
\[
\mathbb{E}[\lim_{L \to \infty} \normLtwo{\tmH^{(L)} - \mH^{*(L)}}] < \frac{\beta}{1-\alpha}\sqrt{2k (1 - \sum_{j=1}^{|\mathcal{E}|} \frac{(p^*_j + \tilde{p}_j - \epsilon)^2}{4})}.
\]
}
\end{theorem}
\begin{proof}
{
\[
\tmH^{(L)} - \mH^{*(L)} = \sigma(\hat{\tmA}\tmH^{(L-1)}\mW^{(L-1)}) - \sigma(\hat{\mA}^*\mH^{*(L-1)}\mW^{(L-1)})
\]
}
Since $\sigma$ is a Lipschitz continuous function, we have
{
\begin{align*}
\normLtwo{\tmH^{(L)} - \mH^{*(L)}} \leq L_\sigma\normLtwo{\hat{\tmA}\tmH^{(L-1)}\mW^{(L-1)} - \hat{\mA}^*\mH^{*(L-1)}\mW^{(L-1)}} \\
= \normLtwo{\hat{\tmA}\tmH^{(L-1)}\mW^{(L-1)} - \hat{\mA}^*\mH^{*(L-1)}\mW^{(L-1)}}\\
= \normLtwo{(\hat{\tmA} -\hat{\mA}^*) \tmH^{(L-1)}\mW^{(L-1)} + \hat{\mA}^*(\tmH^{(L-1)}- \mH^{*(L-1)})\mW^{(L-1)}}
\end{align*}
}
For notational convenience, let us suppose $D^{(L)} = \normLtwo{\tmH^{(L)} - \mH^{*(L)}}$. Applying the sub-multiplicative property of the spectral norm and triangle inequality, we obtain the following recurrence relation
{
\begin{align*}
    D^{(L)} &\leq \normLtwo{(\hat{\tmA} -\hat{\mA}^*)}\normLtwo{\tmH^{(L-1)}}\normLtwo{\mW^{(L-1)}} +   \normLtwo{\hat{\mA}^*}D^{(L-1)}\normLtwo{\mW^{(L-1)}} \\
    &\leq \normLtwo{(\hat{\tmA} -\hat{\mA}^*)} \beta\alpha + \normLtwo{\hat{\mA}^*}D^{(L-1)}\alpha \\
    &\leq \normLtwo{(\hat{\tmA} -\hat{\mA}^*)} \beta\alpha + D^{(L-1)}\alpha 
\end{align*}
}
The last inequality holds because normalized adjacency matrix satisfies $\normLtwo{\hat{\mA}^*} \leq 1$. This is because $\hat{\mA}^*$ is symmetric, row-stochastic matrix. Thus the singular values of $\hat{\mA}^*$ is the absolute values of eigenvalues of $\hat{\mA}^*$ and the largest singular value of $\hat{\mA}^*$ is the largest eigenvalue of $\hat{\mA}^*$. But $\hat{\mA}^*$ being row-stochastic, its largest eigenvalue is at most 1 hence $\normLtwo{\hat{\mA}^*} = \sigma_{max}(\hat{\mA}^*) \leq 1$.

By unrolling the recursion from earlier inequality:
\begin{align*}
     D^{(L)} &\leq \normLtwo{(\hat{\tmA} -\hat{\mA}^*)} \beta \alpha\sum_{l=0}^{L-1} \alpha^{l} + D^{(0)}\alpha^L
\end{align*}
$D^{(0)} = \normLtwo{\tmH^{(0)} - \mH^{*(0)}} = \normLtwo{\mX - \mX} = 0$. Since $\alpha < 1$, The geometric series simplifies to:
\begin{align*}
\sum_{l=0}^{L-1} \alpha^{l} = \frac{1-\alpha^L}{1-\alpha} \\
\lim_{L \to \infty} \sum_{l=0}^{L-1} \alpha^{l} = \frac{1}{1-\alpha}
\end{align*}
Thus our earlier inequality becomes:
\[
\lim_{L \to \infty} D^{(L)} \leq \frac{\beta\alpha}{1-\alpha}\normLtwo{(\hat{\tmA} -\hat{\mA}^*)} < \frac{\beta}{1-\alpha}\normLtwo{(\hat{\tmA} -\hat{\mA}^*)}
\]
Taking expectation on both sides gives us our desired result:
\small{
\begin{align*}
    \mathbb{E}[\lim_{L \to \infty} \normLtwo{\tmH^{(L)} - \mH^{*(L)}}] = \mathbb{E}[D^{(L}] < \frac{\beta}{1-\alpha}\mathbb{E}[\normLtwo{(\hat{\tmA} -\hat{\mA}^*)}] \\
    < \frac{\beta}{1-\alpha}\mathbb{E}[\normLtwo{(\hat{\mA} - \mA^*)}] \\
    = \frac{\beta}{1-\alpha}\sqrt{2k (1 - \sum_{j=1}^{|\mathcal{E}|} \frac{(p^*_j + \tilde{p}_j - \epsilon)^2}{4})}
\end{align*}
}
% Expanding the difference in the RHS:
% {
% \[
% \normLtwo{\hat{\tmA}\tmH^{(L-1)}\mW^{(L-1)} - \hat{\mA}^*\mH^{*(L-1)}\mW^{(L-1)}} = 
% \normLtwo{(\hat{\tmA} -\hat{\mA}^*) \tmH^{(L-1)}\mW^{(L-1)} - \hat{\mA}^*\mH^{*(L-1)}\mW^{(L-1)}}
% \]
% }
\end{proof}
% \begin{corollary}[Condition for convergence in embedding]
%     Assuming infinite depth of GCN layer $L$, the learned node embeddings converge to the `true embeddings' if the number of sampled edges satisfies
%     \[
%     k < \frac{1}{2}(\frac{1-\alpha}{\beta})^2
%     \]
% Here, by `true embedding', we mean the embedding generated by applying GCN on a subgraph sampled following the optimal probability distribution.
% \end{corollary}
% \begin{proof}
    
% \end{proof}
% We know that the total variation distance between two probability distributions is 1/2 of the $\text{L}_1$-distance between them. Hence, the following holds by applying universal approximation theorem (equation~\ref{eq:uapp})

% \begin{align}
% \text{TVD}(\tilde{p} , p^*) = \frac{1}{2}\|\tilde{p} - p^*\|_1 \leq \frac{\epsilon}{2}
% \end{align}

% \paragraph{2. The induced sparse subgraphs spectrum is a good approximation to the true graphs spectrum}
% $\forall \vx, \exists \epsilon$ such that
% \[
% (1-\epsilon) \leq \frac{\vx^{\top}\tilde{L}\vx}{\vx^{\top}L\vx} \leq (1+\epsilon)
%  \]
%  where $\tilde{L}$ is the Laplacian of the sparse graph sampled following the learned distribution $\tilde{p}$ and L is the original graph Laplacian.

% How to bound the ratio of the quadratic forms. Some results here: https://arxiv.org/pdf/2403.13268

\FloatBarrier

