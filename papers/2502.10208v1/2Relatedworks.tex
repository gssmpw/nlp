\section{Related Work}
\label{sec:related}

%Graph sparsification techniques~\cite{hashemi2024comprehensive, chen2023demystifying} are commonly used to improve the training speed of GNNs for large graphs to improve the prediction quality. Unsupervised sparsification relies on heuristics or domain knowledge but does not include downstream task information; in contrast, supervised sparsification is tailored to the specific downstream task.


\noindent\textbf{Unsupervised Graph Sparsification.} 
%Some unsupervised methods sparsify graphs based solely on graph structure while maintaining theoretical guarantees on the preserved properties. 
Effective resistance (ER)~\cite{spielman2011graph} based sampling generates spectral sparse subgraphs while bounding the eigenvalues of the original graph's Laplacian. FastGAT~\cite{srinivasa2020fast} uses ER to improve GNN efficiency, but the high computational cost of ER makes it impractical for large graphs.  However, a major advantage of ER is its ability to produce multiple sparse subgraphs, minimizing information loss and improving GNN performance compared to single sparse graph methods such as $k$-NN.
The random sparsifier is the fastest approach to get sparse subgraphs and is widely used in GNNs such as DropEdge~\cite{rong2019dropedge} and GraphSAGE~\cite{hamilton2017inductive}. GraphSAINT~\cite{zeng2019graphsaint} uses the normalized \emph{degree} as edge weight to assign low sampling probability to edges in denser clusters. Later, it is used for sampling subgraphs and often produces better results than random sparsifiers. 
%Our \edgemlp learns the probability distribution of sampling to generate multiple subgraphs as well.
Spanner (e.g., $t$-spanner)~\cite{dragan2011spanners} is a topology-based sparsifier that preserves distances between nodes in a sparse subgraph by a factor of $t$. 
%For GNNs with $t$-hops, using $t$-spanner subgraphs is beneficial as it retains information during aggregation. 
Spanning Tree and Forest are useful sparsifiers, as they preserve node connectivity, which helps message propagation in GNNs. Although both of these lack control over sparsity, the notion of connectivity is essential. %Thus, we use \emph{degree} based edge-weight \emph{prior} to encourage connectivity among components.
Some other topology-based sparsifiers include  Rank Degree Sparsifier~\cite{voudigari2016rank}, Local Degree Sparsifier~\cite{hamann2016structure},
Forest Fire~\cite{leskovec2007graph}, and Degree-based sparsification~\cite{su2024generic, liu2023dspar}.
Another class of unsupervised sparsifiers first computes similarities between two nodes as edge weight and then 
samples. It could be structural similarity, such as \emph{Jaccard distance} on a portion of shared neighbors (SCAN~\cite{xu2007scan}) or feature similarity (SimSparse~\cite{wu2023alleviating}, AGS-GNN~\cite{das2024ags}).

% Reinforcement learning-based sparsification, SparL~\cite{wickman2021sparrl}
% Bayesian Edge Sampling~\cite{hasanzadeh2020bayesian}

\noindent\textbf{Supervised Graph Sparsification.}
Supervised graph sparsification may have computational overhead due to their training phase but is often compensated by prediction quality in noisy or heterophilic graphs. Methods like SparseGAT~\cite{sparsegat} and SuperGAT~\cite{kim2022find} use the entire graph information during GNN training and learn sparse subgraphs through regularizers. 
%These \textit{implicit sparsifiers} methods are not scalable for large graphs. 
SGCN~\cite{li2020sgcn} is another sparsification method that optimizes the runtime by alternating the sparsifier's and GNN's learning. 
NeuralSparse~\cite{zheng2020robust}, PDTNet~\cite{luo2021learning}, explicitly learn to sample sparse subgraphs. Ours \sgs falls into a similar category with distinctions. NeuralSparse samples $k$-neighbors from each node to generate the sparse graphs, whereas \sgs globally learns the sampling distribution of all edges and then samples from that distribution, which is significantly faster. 
%In addition, sampling neighbors $k$ limits the exact control over sparsity, and the sparser are bound to choose some edges of the neighborhood that might be useless.
\sgs uses a prior probability distribution to narrow the sparse subgraph search space; this notion of prior is useful~\cite{wang2024probability} and recent work like Mixture of Graphs (MOG)~\cite{zhang2024graph} uses \textit{Jaccard similarity}, \textit{gradient magnitude}, and \textit{effective resistance} as prior for sparse subgraph selection.
Additionally, LAGCN~\cite{chen2020label} employs an edge classifier to modify graphs based on training nodes, while our \edgemlp coupled with regularizer uses training edges to foster homophily in the sampled subgraph.
%
% \naheed{@Sid, Discuss Sparsification works (in general)}
%
% \naheed{@Sid, Discuss UnSupervised sparsification and their limitations}
%
% \naheed{@Sid, Supervised sparsification and their limitations. How our method fits there.}
%
% \naheed{@Sid, Any other works that has similar working principle as us but may not be tailored for graph sparsification.} perhaps~\cite{wang2024probability}?
% 
%\cite{wang2024probability}, Generic graph sparsification, unsupervised \cite{su2024generic}, 