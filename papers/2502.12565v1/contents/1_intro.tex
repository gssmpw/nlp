\section{Introduction}
Rapid advancements in large language models (LLMs) have yielded significant improvements across various downstream tasks and have raised a fundamental research question: \emph{How can we further improve an LLM's capabilities with minimal human supervision?} Traditional approaches, such as Reinforcement Learning from Human Feedback (RLHF)~\citep{Ouyang2022-gr} and its variants~\citep{Rafailov2023-pg, Tang2024-jl}, improve performance through classification tasks~\citep{Tang2024-jl}, yet they rely on extensive, costly, and time-consuming human-labeled datasets. Although recent methods like Reinforcement Learning from AI Feedback (RLAIF)~\citep{bai2022constitutional, Lee2024-og, Zhu2024-tf} aim to reduce these costs by replacing human annotations with model-generated signals, their success critically hinges on the reliability of the model's self-evaluation~\citep{Li2024-ni, Wang2024-yp}.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{fig/hard_results.pdf}
    \caption{Classification accuracy curves over five iterations on three challenging datasets. Ours (Oracle) uses the exact class prior for UU learning, while Ours (few-labeled) estimates these priors from only 50 labeled examples. Our method consistently improves accuracy and outperforms both LLM self-refinement by GPT-4o series and advanced reasoning model DeepSeek-R1}
    \label{fig:hard_dataset}
\end{figure}

When using an LLM as its own evaluator, it is observed that the model's inherent biases can harm the reliability of its assessments~\citep{Koo2024-az, Li2024-xi, Liu2024-oa, Wang2024-yp}, thereby undermining the effectiveness of downstream training~\citep{Baumgartner2024-yq}. While iterative self-refinement~\citep{Kim2023-zz, Madaan2023-fn} and multi-agent frameworks~\citep{Estornell2024-gh} can mitigate simpler biases (e.g., ordering or length biases), LLMs still encounter significant challenges in areas where their internal knowledge is limited~\citep{Kamoi2024-lc}. In such cases, external tools—such as accurate, domain-specific databases—can help address certain blind spots~\citep{Wu2024-cb}, but they do not fully eliminate the need for human supervision. Without either robust internal knowledge or dependable huge human input, conventional methods not only struggle to improve performance~\citep{Huang2024-co,Kamoi2024-lc} but also may even experience degradation due to inaccuracies and overconfidence~\citep{Huang2024-co}.

To address these challenges specifically for classification tasks—a crucial first step towards broader self-refinement in LLMs—we introduce an iterative pipeline that refines LLM-generated pseudo-labels using a weakly supervised learning technique known as the Unlabeled-Unlabeled (UU) learning framework~\citep{Lu2019-sd,Lu2020-dx}. Notably, while creating a large, well-maintained labeled dataset requires intensive human supervision, it is relatively straightforward to amass a vast corpus of unlabeled data in the modern era~\citep{sagiroglu2013big}. Motivated by this observation, our approach leverages two unlabeled datasets with differing positive class ratios. Under the simple assumption that one dataset contains a higher proportion of positive examples than the other, our system can effectively learn to distinguish between positive and negative instances without requiring explicit annotations for each example.%

Specifically, our pipeline first employs an LLM to generate initial pseudo-labels from an unlabeled corpus. These pseudo-positive and negative sets are then iteratively refined using UU learning. The classifier trained from UU learning subsequently re-labels the unlabeled corpus, progressively reducing noise and enhancing classification accuracy. By decoupling the refinement process from the LLM's internal knowledge and instead leveraging data-driven features extracted via UU learning, our method delivers improved performance even in domains where LLMs lack sufficient knowledge.

We evaluate our approach on several public datasets, including low-resource language corpora, patent classification tasks, and protein structure classification. Notably, as illustrated in Figure~\ref{fig:hard_dataset}, even in cases where self-refinement methods based on LLMs—or advanced reasoning models such as DeepSeek-R1~\citep{DeepSeek-AI2025-vk}—fail to produce any performance improvement, \emph{our iterative UU learning framework successfully refines its outputs and achieves classification performance that surpasses that of both the original LLM and existing self-improvement pipelines.} These results underscore the potential of our method not only for specialized text classification but also as a novel approach to achieving self-refinement in LLMs.

In summary, our contributions are threefold:
\begin{itemize}
\item We introduce an iterative pipeline that refines LLM-generated pseudo-labels via UU Learning, reducing noise and boosting classification accuracy with minimal human supervision.
\item We show that the proposed method outperforms both direct LLM classification and self-improvement methods across diverse tasks, including low-resource language, patent, and protein classification.
\item Our approach mitigates annotation noise and offers a scalable, cost-effective solution for high-quality classification in specialized domains with limited expert-labeled data.
\end{itemize}
