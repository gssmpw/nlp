\section{Preliminaries}
\label{sec:preliminaries}

\subsection{Supervised Binary Classification}
In many real-world tasks, one commonly encounters binary classification problems, in which an input $x \in \mathbb{R}^d$ is presented, and its label $y \in \{\pm 1\}$ needs to be predicted. Each sample is assumed to be independently and identically drawn from an unknown joint distribution $p(x,y)$. Let $\pi_{+} = p(y=+1)$ be the prior probability of the positive class (positive prior), and define
\begin{align*}
p_{\mathrm{p}}(x) = p(x \mid y=+1)
\text{, }
p_{\mathrm{n}}(x) = p(x \mid y=-1).
\end{align*}
Then, the marginal distribution of $x$ is given by
\begin{align*}
p(x)
= \pi_{+}p_{\mathrm{p}}(x) + (1-\pi_{+})p_{\mathrm{n}}(x).
\end{align*}

A classifier $g: \mathbb{R}^d \to \mathbb{R}$ outputs a real-valued score, whose sign determines the predicted label. For instance, a neural network can serve as $g$. A loss function $\ell:\mathbb{R} \times \{\pm 1\} \to [0,\infty)$ then measures how much the prediction disagrees with the true label. Let $R^+_p(g) = \mathbb{E}_{x \sim p_p} [\ell(g(x), +1)]$ denote the loss for true positive data, and $R^-_n(g) = \mathbb{E}_{x \sim p_n} [\ell(g(x), -1)]$ denote the loss for the true negative data. Then, the true risk is expressed as
\begin{align}
    R_{\mathrm{pn}}(g) =& \mathbb{E}_{(x,y)\sim p}[\ell(g(x),y)] \notag \\
    =& \pi_{+}R^+_p + (1 - \pi_{+})R^-_n \label{eq:risk} 
\end{align}

In supervised learning, positive dataset $\mathcal{C}_p = \{x^p_m\}_{m=1}^{m_p} \sim p_p(x)$ and negative dataset $\mathcal{C}_n = \{ x^n_m \}_{m=1}^{m_n} \sim p_n(x)$ are accessible. Replacing the expectations in \eqref{eq:risk} with sample mean, one obtains the empirical risk, and $g$ is trained to minimize it.


It is well known that having sufficient positive and negative samples typically allows one to train a highly accurate classifier for many tasks. However, in practice, obtaining large-scale positive and negative datasets with annotations is often challenging, especially in specialized domains where annotation costs become a significant obstacle.

\subsection{Unlabeled-Unlabeled (UU) Learning}
\label{subsec:uu}
UU learning~\citep{Lu2019-sd} is a technique that allows training a classifier without fully labeled positive and negative datasets, leveraging two unlabeled datasets with different class priors.

Concretely, suppose unlabeled corpora, $\widetilde{\mathcal{C}}_p = \{\widetilde{x}^p_m\}_{m=1}^{m_p}$ and $\widetilde{\mathcal{C}}_n = \{\widetilde{x}^n_m\}_{m=1}^{m_n}$, drawn from different mixture distributions. We denote $\theta_p = p(y=+1 \mid \widetilde{x}\in \widetilde{\mathcal{C}}_p)$ and $\theta_n = p(y=+1 \mid \widetilde{x}\in \widetilde{\mathcal{C}}_n)$ the \emph{positive prior} of these unlabeled corpora. In other words, $\theta_p$ is the fraction of true positives in $\widetilde{\mathcal{C}}_p$, and $\theta_n$ is the fraction of true positives in $\widetilde{\mathcal{C}}_n$. Then, the mixture distribution of each corpus is given as
\begin{align*}
    \widetilde{p}_{p}(x) &= \theta_p\, p_{p}(x) \;+\; \bigl(1 - \theta_p\bigr)\, p_{n}(x) \\
    \widetilde{p}_{_n}(x) &= \theta_n\, p_{p}(x) \;+\; \bigl(1 - \theta_n\bigr)\, p_{n}(x).        
\end{align*}

When $\theta_p > \theta_n$, we can treat $\widetilde{\mathcal{C}}_p$ as a pseudo-positive corpus (due to its larger proportion of actual positives) and $\widetilde{\mathcal{C}}_n$ as a pseudo-negative corpus (having a smaller proportion of actual positives). 

By appropriately combining these two unlabeled sets, one can construct an unbiased estimate of the true binary classification risk~\eqref{eq:risk}. Specifically, let $R_{\tilde{p}}^{\pm}(g)=\mathbb{E}_{x\sim \widetilde{p}_p}[\ell(g(x),\pm 1)]$, and $R_{\tilde{n}}^{\pm} (g)=\mathbb{E}_{x\sim \widetilde{p}_n}[\ell(g(x),\pm 1)]$. Then, the UU learning risk is given by
\begin{align}
    &R_{\mathrm{uu}}(g)\label{eq:uu}
    \\
    &\hspace{1em}= a R_{\tilde{p}}^+(g) - b R_{\tilde{p}}^-(g) - c R_{\tilde{n}}^+(g) + d R_{\tilde{n}}^-(g),\notag
\end{align}
where the coefficients $a$, $b$, $c$, $d$ are computed from $\pi_+$, $\theta_p$, and $\theta_n$ as $a = \frac{(1-\theta_n)\,\pi_+}{\theta_p - \theta_n}$, $b = \frac{\theta_n\,(1-\pi_+)}{\theta_p - \theta_n}$, $c = \frac{(1-\theta_p)\,\pi_+}{\theta_p - \theta_n}$, $d = \frac{\theta_p\,(1-\pi_+)}{\theta_p - \theta_n}$. When $\theta_p = 1$ and $\theta_n = 0$, that is, when using the same dataset as standard supervised learning, equation~\eqref{eq:uu} reduces to the standard supervised learning risk equation~\eqref{eq:risk}. In other words, supervised learning can be considered a special case of UU learning.

\subsection{Robust UU Learning}
\label{subsec:ruu}
While UU learning \eqref{eq:uu} does allow model training without explicit positive/negative labels, comparing the original binary classification risk \eqref{eq:risk}—which remains nonnegative—against the UU risk \eqref{eq:uu} shows the UU risk includes negative terms such as $-b R_{\tilde{p}}^-(g)$ and $-c R_{\tilde{n}}^+(g)$. It has been observed that these negative risk terms can lead to overfitting~\citep{Lu2020-dx}.

To mitigate this, \emph{Robust UU Learning} introduces a generalized Leaky ReLU function $f$ to moderate the reduction of negative risk. Concretely, it normalizes each term of the loss function as~\citep{Lu2020-dx}
\begin{align}
    R_{\mathrm{ruu}}(g)
    &= f\left(a R_{\tilde{p}}^+(g) - c R_{\tilde{n}}^+(g) \right) \notag \\
    &\hspace{2em}+ f\left(d R_{\tilde{n}}^-(g) - b R_{\tilde{p}}^-(g)\right) \label{eq:ruu}
\end{align}
where each bracketed term resembles a “normalized” risk under the hypothetical label of being positive or negative, respectively. The function $f$ is given by
\begin{align}
    f(x) =
    \begin{cases}
    x & \text{if } x > 0 \\
    \lambda x & \text{if } x < 0
    \end{cases}
    \quad (\lambda < 0).
    \label{eq:relu}
\end{align}

Intuitively, $f$ leaves the risk value unchanged when the risk is positive, but for negative risk, it uses $\lambda < 0$ to convert it into a positive quantity, thus mitigating the overfitting by negative risk.
