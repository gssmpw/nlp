\section{Experiments}
We conducted experiments to explore three main research questions:

\begin{enumerate}
    \item[\textbf{RQ1}] Can our iterative refinement approach improve classification performance, compared to the initial LLM-based annotations, for various NLP tasks?
    \item[\textbf{RQ2}] Can our method enhance performance on challenging tasks where even advanced LLMs (e.g., GPT-4o-based models) struggle?
    \item[\textbf{RQ3}] Can our approach outperform existing LLM self-refinement strategies and recently introduced Reasoning models?
\end{enumerate}

\subsection{Experimental Setup}
\paragraph{Datasets:}
We use six binary classification datasets grouped into two categories based on their difficulty\footnote{We evaluate difficulty based on a pilot experiment in which we trained the classification model in a standard supervised setting. Please refer to Table~\ref{tab:data_stat} for classification accuracy via supervised learning.}. Table~\ref{tab:data_stat} reports the dataset statistics, and Table~\ref{tab:pos_neg_examples} provides examples for positive and negative cases (see Appendix~\ref{sec:dataset}).

\textbf{Easier Tasks (for RQ1):} We evaluated our algorithm on three tasks: (i) \textbf{Fake News}~\citep{ahmed2018detecting}, classifying news articles as fake or real, (ii) \textbf{Saroco}~\citep{rogoz-etal-2021-saroco}, Romanian satire detection dataset to assess effectiveness in a low-resource language, and (iii) \textbf{Safety}~\citep{Dai2024-jq}, dataset for SafeRLHF evaluating if the responses to questions is safe or dangerous, thereby assessing effectiveness for LLM's post-training.

\textbf{Harder Tasks (for RQ2 and RQ3):} We evaluate our algorithm on three more challenging tasks: (i) the \textbf{Corona Sentiment}\footnote{\scriptsize{\url{https://github.com/akshayjoshii/COVID19-Tweet-Sentiment-Analysis-and-EDA/tree/master}}}, classifying social media post related to COVID-19 as positive or negative sentiment, (ii) the \textbf{Green Patent}\footnote{\scriptsize{\url{https://huggingface.co/datasets/cwinkler/patents_green_plastics}}} which involves identifying whether a patent abstract pertains to green plastics—requiring high expert knowledge, and (iii) \textbf{Protein Structure}~\citep{Blanchard2022-lx}, involving classification of proteins based on high or low COVID-19 binding affinity from molecular \textsc{smiles} strings.

For all experiments, we divided each dataset and randomly partitioned it into training, validation (for best epoch selection), and test splits in a 7:1:2 ratio. 

\paragraph{LLM-Based Annotation (Iteration 0):}
At \emph{iteration0}, we obtain pseudo-labels from LLMs using a prompt that includes a dataset explanation, a few-shot labeled examples, and a target sample (see Figure~\ref{fig:llm_prompt} for the exact prompt). These pseudo-labels are then used to construct initial pseudo-positive ($\widetilde{\mathcal{C}}_p^{(0)}$) and pseudo-negative ($\widetilde{\mathcal{C}}_n^{(0)}$) corpora.

\paragraph{Training Procedure:}
We train the classifier by appending an affine layer to the transformer’s final hidden state to yield a one-dimensional score. For fine-tuning efficiency, we employ QLora~\citep{Dettmers2023-vs} with 4-bit quantization. At each iteration, we fine-tune the model using the pseudo-positive and pseudo-negative corpora with the AdamW optimizer (learning rate = $1.0 \times 10^{-4}$, batch size = 16, and 3 epochs) and fix the robust UU learning hyperparameter $\lambda$ at -0.001 (see Equation~\eqref{eq:relu}).  At the end of each epoch, we compute the loss on a pseudo-labeled validation set and select the model with the lowest loss to re-label the entire dataset. Additional parameters are detailed in Table~\ref{tab:hypara}.

\paragraph{Estimating Class Priors:}
Robust UU learning requires estimates of the positive proportions $\hat{\theta}_{p}$ and $\hat{\theta}_{n}$ for the pseudo-positive and pseudo-negative corpora, respectively. We compare two settings to obtain these parameters. In the first setting \textbf{(Ours (Oracle))}, we assume that the exact value of $\theta_{p}$ and $\theta_{n}$ are available\footnote{The assumption that the precise class priors are known is not unrealistic given the effectiveness of existing class prior estimation methods (e.g., \citep{Scott2013-hz, Menon2015-hb, Liu2016-tx, Bekker2020-sa}).}. In the second setting \textbf{(Ours (few-labeled))}, only 50 examples from the training dataset are labeled, from which we compute $\hat{\theta}_p = p(y = 1 \mid \tilde{y} = 1)$ and $\hat{\theta}_n = p(y = 1 \mid \tilde{y} = 0)$ before proceeding with robust UU learning. Our experiments show that even a small amount of labeled data provides sufficiently accurate estimates.

\paragraph{Evaluation Metrics and Protocol:}
We measure classification accuracy on the held-out test dataset across all iterations. Each run is repeated with three different random seeds, and we report the mean and standard deviation.

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{fig/main_results.pdf}
    \caption{Classification accuracy over five iterations for three datasets. The solid lines represent the mean values, and the shaded areas show the mean $\pm$ standard deviation. Both variants of our approach, Ours (Oracle) and Ours (few-labeled) demonstrate steady improvements as the iteration increases. Notably, even in scenarios where the baselines fail to learn the classification task, our method continues to exhibit iterative performance gains. This robustness highlights the strength of our iterative refinement strategy, even under minimal supervision settings like 50 labeled examples.}
    \label{fig:main}
\end{figure*}

\subsection{Experiments on Easier NLP Tasks (RQ1)}
\label{subsec:easy_result}
We begin with three relatively simple tasks: \textbf{Fake News}, \textbf{Saroco}, and \textbf{Safety}. Given its relatively small parameter size and strong performance, we use Llama-3.2-1B-Instruct\footnote{\scriptsize\url{https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct}} as the base model for the classifier.

\paragraph{Annotation Model:}
To generate the initial pseudo labels, we employ five open models, gemma-2-2b-it\footnote{\scriptsize\url{https://huggingface.co/google/gemma-2-2b-it}}, Llama-2-7b-chat-hf\footnote{\scriptsize\url{https://huggingface.co/meta-llama/Llama-2-7b-chat-hf}}, Meta-Llama-3-8B\footnote{\scriptsize\url{https://huggingface.co/meta-llama/Meta-Llama-3-8B}}, Llama-3.2-1B-Instruct\footnote{\scriptsize\url{https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct}}, and Llama-3.2-3B-Instruct\footnote{\scriptsize\url{https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct}} to ensure reproducibility and to evaluate whether our self-refinement approach can operate robustly across a diverse set of models.

\paragraph{Baselines:}
We compare our approach against three baselines:
\begin{enumerate}
\item \textbf{PN Learning} treats the pseudo-labels as fully reliable, i.e., standard supervised training on potentially noisy labels.
\item \textbf{PIE~\citep{Zhang2023-zo}} iterative label refinement method that temporarily accepts only high-confidence predictions as correct, training an ensemble-like model on these provisional labels and iteratively improving label accuracy.
\item \textbf{CCP~\citep{kutt2024contrastive}} method leveraging an iterative contrastive learning pipeline that incrementally builds a set of highly reliable labels by learning robust class-specific features from a limited labeled dataset.
\end{enumerate}

\paragraph{Result:}
Figure~\ref{fig:main} shows the results for the easier tasks. Both Ours (Oracle) and Ours (few-labeled) steadily improve in accuracy, ultimately achieving high classification performance by the final iteration. Notably, despite using only 50 labeled examples, Ours (few-labeled) achieves performance comparable to Ours (Oracle), underscoring its potential even under minimal supervision.

For the Fake News task (using Meta-Llama-3-8B), PIE shows iterative performance gains, eventually matching the performance of Ours (Oracle) and outperforming standard PN Learning. This highlights the effectiveness of its ensemble strategy and confidence-based label refinement. However, for Saroco and Safety, where PN Learning struggles, PIE similarly performs poorly. This suggests that even with confidence-based filtering, limited initial classification accuracy hampers the effective elimination of noisy labels.

Similarly, while CCP demonstrates performance improvements with increasing iterations on the Fake News task, it completely fails on Saroco and Safety. CCP's reliance on learning class-specific features from a small teacher dataset appears insufficient in scenarios where such features are inherently difficult to capture.

\textbf{Summary for RQ1:}
These results confirm RQ1 by demonstrating that our iterative refinement approach consistently improves classification performance over initial LLM-based annotations on easier tasks, even with limited human supervision.

\subsection{Experiment on Harder Tasks (RQ2/RQ3)}
\label{subsec:hard}
We next evaluate our approach on three more challenging tasks: \textbf{Corona Sentiment}, \textbf{Green Patent}, and \textbf{Protein Structure}. To further explore the potential of our proposed method, we employed Llama-3.2-3B-Instruct for both Corona Sentiment and Green Patent and utilized bert-base-smiles\footnote{\scriptsize{\url{https://huggingface.co/unikei/bert-base-smiles}}} for Protein Structure. Initial pseudo labels are generated using the high-performance closed models GPT-4o-mini\footnote{\scriptsize{\url{https://platform.openai.com/docs/models\#gpt-4o-mini}}} and GPT-4o\footnote{\scriptsize{\url{https://platform.openai.com/docs/models\#gpt-4o}}}.

\paragraph{Baselines:}
To compare the performance of self-refinement under a minimal human supervision setting, we adopt a self-refinement framework~\citep{Madaan2023-fn,Kim2023-zz,Chen2024-zm}, where a response agent generates an initial answer and a feedback agent generates the feedback for this answer, thus iteratively refines answer (see Figures~\ref{fig:answering} and~\ref{fig:feedback} for exact prompts). In this setup, we leverage high-performance closed models (GPT-4o-mini and GPT-4o) alongside the cost-effective reasoning model DeepSeek-R1~\citep{DeepSeek-AI2025-vk}. For GPT-4o-mini and GPT-4o, initial annotations are generated consistently with our iterative robust UU learning (Ours) to ensure a fair comparison.

\paragraph{Results:}
Figure~\ref{fig:hard_dataset} illustrates the accuracy curves across five iterations for the three challenging datasets. For all tasks, Ours (Oracle) shows a steady improvement in classification accuracy over successive iterations. In the Corona Sentiment and Protein Structure tasks, Ours (few-labeled) starts with a low classification accuracy relative to Ours (Oracle), but this gap diminishes with additional iterations. In addition, despite a lower initial performance than DeepSeek-R1 for Corona Sentiment and Protein Structure, our method surpasses DeepSeek-R1's performance, demonstrating robustness against noisy initial labels.

In contrast, while the LLM-based self-refinement approach by GPT-4o-mini and GPT-4o shows a slight performance gain in the Corona Sentiment task, they suffer performance degradation in the Green Patent and Protein Structure tasks. Additionally, although DeepSeek-R1 starts with high annotation scores on all three datasets, it exhibits no performance gains in subsequent iterations. This suggests that even when employing a reasoning model, the benefits of self-refinement remain limited in scenarios where the LLM's internal knowledge is insufficient. Therefore, relying solely on self-refinement in these cases may not lead to further performance gains.

\textbf{Summary for RQ2 and RQ3:}
These findings confirm RQ2 by demonstrating our method enhances performance on challenging tasks through iterative refinement. They also confirm RQ3 by showing our approach outperforms existing LLM self-refinement strategies and reasoning models.
