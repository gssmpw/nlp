\section{Related Work}

RLAIF is a popular LLM post-training method \citep{bai2022constitutional}.
Its idea to generate feedback by a model itself is called Pseudo-Labeling (PL) and well studied in semi-supervised learning \citep{scudder1965probability,lee2013pseudo}. Below, we will review PL and LLM self-training methods related to ours. For a thorough review of each topic, please refer to \citet{yang2023survey} and \citet{xiao2025foundations}.

\paragraph{Pseudo-Labeling:}
PL trains a student model so that its output is close to the output of a teacher model on unlabeled data \citep{scudder1965probability,lee2013pseudo}.
Various methods for constructing a teacher model has been proposed.
For example, $\Pi$-model \citep{bachman2014learning,sajjadi2016regularization,laine2017temporal} and virtual adversarial training \citep{miyato2019virtual} perturbs input and/or neural networks of student models.
Some methods use weighted average of previous student models' predictions or weights as a teacher model \citep{laine2017temporal,tarvainen2017mean}, and
other methods even try to optimize a teacher model by viewing PL as an optimization problem \citep{yi2019probabilistic,wang2020repetitive,pham2021meta}.
Our work is complimentary to this line of works since what we modify is the risk estimator rather than teacher models.

PL is known to suffer from erroneous labels generated by a model-in-training \citep{haase2021iterative,rizve2021in}, and this observation well aligns with recent reports on RLAIF that erroneous self-feedback is a major source of failure \citep{Madaan2023-fn}.
A straightforward approach is to filter potentially incorrect labels based on confidence \citep{sohn2020fixmatch,haase2021iterative,rizve2021in} or the amplitude of loss as in self-paced learning \citep{bengio2009curriculum,kumar2010self,jiang2015self}.
Another method is refining pseudo-labels in a way similar to label propagation \citep{zhu2002learning} but with similarity scores computed using neural networks \citep{kutt2024contrastive}.
Our work tackles the issue of erroneous labels by using a risk estimate robust to erroneous self-feedback based on UU learning \citep{Lu2019-sd,Lu2020-dx}. Even though our approach requires only a minimal change, we observed a significant performance boost.

\paragraph{LLM's Self-Refinement:}
To enhance the reasoning capabilities of LLMs, early efforts primarily explored various prompt engineering techniques \citep{Brown2020-bw,Wei2022-kt,Wang2023-pf,Yao2023-ey}. Even with refined prompting strategies, an LLM’s initial outputs can remain limited in some scenarios. Recent studies have proposed iteratively improved answer strategies called self-refinement approaches~\citep{Madaan2023-fn,Kim2023-zz,Chen2024-zm}, and our work falls within this lineage.

Self-refinement involves generating responses through agents in distinct roles that iteratively provide feedback \citep{Shinn2023-no, Zhu2023-wn}. For example, multi-agent debate frameworks \citep{Estornell2024-gh} use an answering agent and an evaluation agent to collaboratively improve answer quality \citep{Chen2024-ua, Du2024-pi, Smit2024-cn}. However, these methods usually assume that the LLM has enough internal knowledge to generate effective feedback and revisions; when it doesn’t, performance gains can be minimal or even negative \citep{Huang2024-co,Li2024-fo,Kamoi2024-lc,Kamoi2024-od}—sometimes degrading performance \citep{Huang2024-co}. Our approach minimizes this reliance: internal knowledge is used only initially for classification, with subsequent improvements relying on extracting features directly from the data via UU learning.

Other work addresses knowledge gaps by retrieving external information or using external tools \citep{Huang2022-mv, Wang2023-fz, Shi2024-gx, Wu2024-cb}, but setting up such systems can be costly. In contrast, our method requires only a small amount of labeled data for initialization without assuming the availability of external knowledge or tools.
