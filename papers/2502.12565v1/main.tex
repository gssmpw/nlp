\pdfoutput=1

\documentclass[11pt]{article}

\usepackage[preprint]{acl}

\usepackage{times}
\usepackage{latexsym}
\usepackage{balance}

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}

\usepackage{graphicx}

\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{xspace}
\definecolor{darkblue}{rgb}{0, 0, 0.5}
\hypersetup{colorlinks=true, citecolor=darkblue, linkcolor=darkblue, urlcolor=darkblue}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{multicol}
\usepackage{mathtools}

\usepackage{adjustbox}
\usepackage{xcolor}
\usepackage{colortbl}

\usepackage{array}

\usepackage{here}

\usepackage{verbatim}
\usepackage{tcolorbox}
\usepackage{listings}

\lstset{
  breaklines=true,
  columns=flexible,
  basicstyle=\ttfamily\small,
}


\def\eg{{\it e.g.}}
\def\cf{{\it c.f.}}
\def\ie{{\it i.e.}}
\def\etal{{\it et al. }}
\def\etc{{\it etc}}

{%
    \newcommand{\kozuno}[1]{
        \textcolor{red}{\textbf{[Kozuno: #1]}}
    }
    \newcommand{\asano}[1]{
        \textcolor{red}{\textbf{[Asano: #1]}}
    }
    \newcommand{\baba}[1]{
        \textcolor{red}{\textbf{[Baba: #1]}}
    }

\title{Self Iterative Label Refinement via Robust Unlabeled Learning}


\author{ \\
    \textbf{Hikaru Asano${}^{1,2}$ $\;\;\;$ Tadashi Kozuno${}^3$ $\;\;\;$ Yukino Baba${}^1$} \\
    ${}^1$The University of Tokyo \quad
    ${}^2$RIKEN AIP \quad
    ${}^3$OMRON SINIC X
    \\
    \texttt{asano-hikaru19, yukino-baba@g.ecc.u-tokyo.ac.jp,} \\
    \texttt{tadashi.kozuno@sinicx.com} \\
}

\begin{document}
\maketitle
\begin{abstract}
Recent advances in large language models (LLMs) have yielded impressive performance on various tasks, yet they often depend on high-quality feedback that can be costly. Self-refinement methods attempt to leverage LLMs’ internal evaluation mechanisms with minimal human supervision; however, these approaches frequently suffer from inherent biases and overconfidence—especially in domains where the models lack sufficient internal knowledge—resulting in performance degradation. As an initial step toward enhancing self-refinement for broader applications, we introduce an iterative refinement pipeline that employs the Unlabeled-Unlabeled learning framework to improve LLM-generated pseudo-labels for classification tasks. By exploiting two unlabeled datasets with differing positive class ratios, our approach iteratively denoises and refines the initial pseudo-labels, thereby mitigating the adverse effects of internal biases with minimal human supervision. Evaluations on diverse datasets—including low-resource language corpora, patent classifications, and protein structure categorizations—demonstrate that our method consistently outperforms both initial LLM's classification performance and the self-refinement approaches by cutting-edge models (e.g., GPT-4o and DeepSeek-R1).
\end{abstract}

\input{contents/1_intro}
\input{contents/1_related_works}
\input{contents/2_preliminaries}
\input{contents/3_method}
\input{contents/4_experiment}
\input{contents/6_conclusion}

\section*{Acknowledgment}
This research was supported by the JST Moonshot Research and Development Program JPMJMS2236-8.


\bibliography{library}

\clearpage
\appendix
\input{contents/appendix}

\end{document}
