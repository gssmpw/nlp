\section{Discussion and Ablation Study}
\label{sec:ablations}
We provide an in-depth analysis of our optimization setting, the impact of different loss functions, and the number of tolerances sampled during optimization. This analysis aims to offer more guidance for future work.

%--------------------------------------------------------
\subsection{Partially Tolerance Optimization}
In deep optics, the optimization of optical and decoder parameters may vary~\cite{tseng2021differentiable}. Tolerance-aware optimization adds further complexity by introducing random perturbations in each forward pass. To delve into the distinctions between these components, we selectively control the optimization: (a) optimizing the optics, and (b) only the computational decoder. Through experiments, we find that the deep optics achieves improvement of robustness only when the optics and decoder are jointly optimized simultaneously, shown in \cref{tab:partly-tolerance}. 
Optimizing only the decoder can enhance the decoder's tolerance-aware ability, but still fails to fully address the impact of random tolerances. Additionally, when optimizing the optics alone, the global influence of optical parameters on the subsequent decoder, along with the limited number of optical parameters, makes the optimization challenging and risks compromising the results of the initial pre-training stage. Only through the joint optimization of the optical system and decoder can we achieve a more stable optical system with enhanced pairing capability from the decoder, significantly improving the robustness of deep optics against potential tolerances.

%--------------------------------------------------------

\begin{table}
\centering
\caption{\emph{Optics/Decoder-only} means that only optimize the optics/decoder part of parameters during tolerance optimization and \emph{Both} means optimized both parts. The results are average of 100 times of random sampled tolerances experiments (excluding Spot Size).}
\begin{tabular}{c|c c c c}
\hline
 & Optics-only & Decoder-only & Both \\
\hline
PSNR$\uparrow$ & 22.06 & 25.94 & \bf{28.08} \\
SSIM$\uparrow$ & 0.376 & 0.734 & \bf{0.845} \\
LPIPS$\downarrow$ & 0.664 & 0.379 & \bf{0.225} \\
Spot Size ($\mu m$)$\downarrow$ & 34.2 & \textbf{14.2} & 38.7 \\
\hline
\end{tabular}
\label{tab:partly-tolerance}
\end{table}

%--------------------------------------------------------
\subsection{Analysis of Loss Function Impact}
To analyze the impact of the two loss functions, we conduct ablation studies on Spot loss and PSF loss. The experimental results are shown in \cref{tab:loss_comparison}. The results demonstrate that the proposed Spot loss and PSF loss significantly improve the performance. It is worth noting that Spot loss and PSF similarity loss can only be used together to maximize the performance of tolerance-aware optimization. The two losses play different roles respectively, the Spot loss is helpful to ensure that the overall imaging quality of the optics is not seriously degraded during the tolerance optimization, while the PSF loss is able to significantly improve the robustness of the optics to random tolerances on this basis. Without the constraint of Spot loss, PSF loss can lead to significant degradation in the imaging performance of the optical system, thereby impacting overall tolerance optimization. As shown in \cref{tab:loss_comparison}, the spot size is extremely large, indicating very low imaging quality.
\begin{figure}[t]
  \centering
   \includegraphics[width=1.0\linewidth]{figures/perb_num_ablation.pdf}

   \caption{Ablation study on the number of sampled tolerances pattern in every iteration. Average PSNR, SSIM, and LPIPS of 100 times random tolerances test for different sampling numbers are shown, along with the PSFs comparison. Scale bar: $15\mu m$ .}
   \label{fig:perb_num}
\end{figure}

\begin{table}
\centering
\caption{Ablation study on Spot loss and PSF loss, each incorporating basic image quality loss. The results represent the averages from 100 random sampling tolerance experiments (excluding Spot Size).}
\begin{tabular}{c|c c c c}
\hline
 & - & $\mathcal{L}_{\text{Spot}}$ & $\mathcal{L}_{\text{PSF}}$ & $\mathcal{L}_{\text{Spot}}\&\mathcal{L}_{\text{PSF}}$ \\
\hline
PSNR$\uparrow$ & 24.83 & 26.63 & 20.49 & \bf{28.08} \\
SSIM$\uparrow$ & 0.738 & 0.782 & 0.601 & \bf{0.850} \\
LPIPS$\downarrow$ & 0.378 & 0.248 & 0.529 & \bf{0.225} \\
Spot Size ($\mu m$)$\downarrow$ & 52.6 & \bf{12.9} & 637.9 & 38.7 \\
\hline
\end{tabular}
\label{tab:loss_comparison}
\end{table}

%--------------------------------------------------------
\subsection{Number of Sampled Tolerance Patterns}
In the tolerance-aware optimization process, the number of sampled tolerance patterns is a critical hyperparameter that significantly impacts the performance. If the number is too low, the optimization may stuck in local optima. However, increasing the sampled number leads to a higher consumption of GPU memory. Therefore, selecting an appropriate sample number is of great importance to balance the trade-off between optimization effectiveness and memory usage. Therefore, we employ different number of sampled tolerance pattern and conduct quantitative analysis to determine a reasonable lower limit, see in \cref{fig:perb_num}. This experiment significantly streamlines subsequent research efforts.
