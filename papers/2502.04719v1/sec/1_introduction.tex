\section{Introduction}
\label{sec:intro}
The concept of deep optics~\cite{yang2024curriculum, Sitzmann_2018, sun2021end} has garnered significant attention for the joint optimization of optical systems and downstream vision algorithms. In a deep optics framework, the differentiable optical simulation model~\cite{sitzmann2018end, yang2024curriculum, sun2021end} and the downstream image reconstruction algorithm~\cite{sun2020learning, klotz2025minimalist} are optimized jointly in an end-to-end manner~\cite{li2021end, sun2021end}. This approach enables the design of novel optical systems that better support downstream tasks. End-to-end optical design~\cite{sitzmann2018end, peng2019learned, cai2024phocolens, yang2024curriculum,sun2021end} facilitates a tighter integration between hardware and software to achieve design objectives, with promising example applications observed across various vision tasks, including hyperspectral imaging~\cite{li2022quantization}, extended depth-of-field imaging~\cite{yang2024curriculum}, high-dynamic-range (HDR) imaging~\cite{sun2020learning, metzler2020deep}, and encoded depth estimation~\cite{yang2023aberration, ikoma2021depth}. Among existing optical systems, refractive lenses are still most widely used in practical camera platforms~\cite{wang2022differentiable, cote2023differentiable, peng2019learned, li2021end, zhang2023large}.

However, most deep optics solutions~\cite{yang2023aberration, chang2019deep, sun2020learning} assume perfect manufacturing and assembly processing, without any \textbf{tolerance-aware optimization}. In mass production, manufacturing and assembly errors are unavoidable and they may cause performance degradation (\cref{sec:methods}) on both the optical and algorithm sides. On the optical side, this degradation leads to a \textbf{design-to-manufacturing gap}, see \cref{fig:PerbVisualization}, confining the quality of deep optics and limiting its applicability in the real world. On the algorithm side, they also hurt the performance network-based image processing, as error-free optics are assumed at network training time. Although there are several exploration from traditional tolerance optimization~\cite{oinen1990new, forse1996statistical, hu2015design}, to mitigate the performance gap between lens design and manufacturing, it solely considers the optical component. In an end-to-end optical design pipeline, this will disrupt the encoding and decoding relationship between the optics and algorithms (\cref{sec:results}), compromising the integrity of the original design. 

Therefore, in this work, we proposed the first deep optics learning framework that explicitly models the tolerances in manufacture and assembly. This approach allows us to sample tolerances using Monte Carlo sampling and to optimize these tolerances in an end-to-end manner, ensuring that the final design meets machining requirements while preserving the overall design performances. With this framework in place, we further define a more complete deep optics optimization flow that includes a tolerance optimization process, i.e., firstly a pre-training stage without considering tolerances, in order to obtain an initial design that meets the design objectives, and secondly, tolerance-aware optimization is used to optimize the initial design, which makes deep optics much more robust to tolerances while meeting design objectives. This more complete deep optics optimization flow greatly reduces the design-to-manufacturing gap.

Nonetheless, due to the random sampling of multiple tolerance patterns during tolerance-aware optimization, this makes it very unstable and difficult to rely exclusively on the loss functions of design objectives (\cref{sec:ablations}). Therefore, to mitigate the instability and difficulty of the tolerances optimization process, we novelly design the Point Spread Function (PSF) similarity loss to enhance the robustness of the optical system to tolerances by constraining the PSF variation. In addition, we utilize Spot loss to ensure that the imaging properties of the optical system remain within reasonable range by limiting the spot size of the optical system. By combining the two proposed losses, it is possible to complete the tolerance optimization stage stably and effectively without unduly degrading the performance of the pre-trained design.

To validate the accuracy of our tolerance modeling, we compare the ray tracing with random tolerances with Zemax~\cite{ZEMAX}, and the results show that our modeling accuracy is close to that of Zemax. Additionally, we compare the performance gap between deep optical designs with and without tolerance optimization under random tolerances, using computational imaging as a case study. Our results demonstrate that tolerance optimization can achieve over $2\text{dB}$ improvement in deblurring performance (\cref{tab:result}). At the same time, we apply tiny perturbations to simulate tolerance perturbations during the actual images acquisition, and illustrate the effectiveness of our proposed tolerance-aware optimization by comparing the quality of the reconstructed images in the real-world experiment.
