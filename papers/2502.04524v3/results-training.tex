To efficiently tackle deep learning workloads, the analog AI accelerator must not only perform forward and backward passes (MVMs), but most importantly, allow for weight updates \cite{aihwkit}. During backpropagation, the synaptic weights are modified according to the gradient of the corresponding layer. Therefore, the device conductance must be gradually modified in both positive and negative directions to represent analog weight changes. Analog CMO/HfO\textsubscript{x} ReRAM arrays not only allow for bidirectional conductance updates, but additionally enable parallel weight updating by following a stochastic open-loop pulse scheme \cite{Gokmen2016,Gokmen2020}. Remarkably, the parallel and open-loop update scheme significantly accelerates training compared to serial and closed-loop methods, providing efficiency gains of several orders of magnitude and advantages in system design complexity \cite{Chen2023}. In this section, the bidirectional open-loop response of the CMO/HfO\textsubscript{x} ReRAM array, required during Tiki-Taka training, is characterized. Specifically, the analog conductance potentiation, depression and symmetry point are measured. Subsequently, the devices' responses are statistically reproduced in the open-source 'aihwkit' simulation platform developed by IBM \cite{aihwkit}. Finally, this hardware-aware device model, which includes device variabilities, is used to simulate the training of representative neural networks using the AGAD learning algorithm. This novel analog training algorithm relaxes the symmetry requirements of previous Tiki-Taka versions by incorporating additional digital computations on-the-fly \cite{Rasch2024Agad}.

\subsubsection{Open-loop ReRAM array characterization}\label{openloop_characterization}
Fig. \ref{fig5}a shows the experimental conductance change of a representative CMO/HfO\textsubscript{x} ReRAM device within the array upon applying identical-voltage pulse trains with alternating polarity in batches of 400. Subsequently, a sequence of 500 pulses with alternating polarity, consisting of 1-pulse-up followed by 1-pulse-down, is applied to experimentally determine the symmetry point.
The same open-loop programming scheme, with \(V_{\rm set} = 1.35 \, \mathrm{V}\) (\(V_{\rm G} = 1.4 \, \mathrm{V}\)) and \(V_{\rm reset} = -1.3 \, \mathrm{V}\) (\(V_{\rm G} = 3.3 \, \mathrm{V}\)), each lasting 2.5~\textmu s, is applied to all devices in the 8x4 array. The \textit{set}/\textit{reset} pulse width is limited by the experimental setup, although previous work has demonstrated CMO/HfO\textsubscript{\textnormal{x}} ReRAM switching with pulses as short as \(60 \, \mathrm{ns}\) \cite{Davide_DRC}. Due to inter-device (device-to-device) and intra-device (cycle-to-cycle) variabilities, the experimental response of each device to a given number of identical pulses exhibits some level of variability (see Fig. \ref{figS7} in the Supplementary Information). Therefore, for each pulse, a Gaussian distribution of the measured conductance states among the devices is extracted. For statistical relevance, Fig. \ref{fig5}b shows the experimental standard deviation of the array response to the open-loop scheme as a function of the pulse number, represented in grey. To realistically assess the accuracy of analog training with CMO/HfO\textsubscript{x} ReRAM devices, the key figures of merit of the device training characterization—such as the number of states, the symmetry point skew, and the noise-to-signal ratio (NSR)—are first extracted from experimental data, as defined below.%, and then used to feed the device model in the aihwkit.
\begin{align}
   \mathrm{N}_{\rm states} = \frac{G_{\rm max} - G_{\rm min}}{\overline{\Delta G_{\rm sp}}}
   \label{nstates}
\end{align}
\begin{align}
   \mathrm{SP}_{\rm skew} = \frac{G_{\rm max} - \overline{G_{\rm sp}}}{G_{\rm max} - G_{\rm min}}
   \label{skew}
\end{align}
\begin{align}
   \mathrm{NSR} = \frac{ \sigma_{\Delta G_{\rm sp}}  } {\overline{\Delta G_{\rm sp}}}
   \label{NSR}
\end{align}
$G_{\rm max}$ and $G_{\rm min}$ represent the maximum and minimum values extracted from the full conductance swings, while $\overline{G_{\rm sp}}$, $\overline{\Delta G_{\rm sp}}$ and $\sigma_{\Delta G_{sp}}$ denote the values of the mean conductance, mean conductance update and standard deviation of the conductance update at the symmetry point during the 1-pulse-up, 1-pulse-down procedure, respectively. Fig. \ref{fig5}c shows the experimental Gaussian distributions of these metrics for the 32 devices within the array. The results indicate an average of 22 states, with a range from 16 to 33. A shift in the $G_{\rm sp}$ (or SP$_{\rm skew}$) of 61\% is measured, reflecting a negative trend in the device asymmetry where the down response is steeper than the up response. An average NSR of 90\% among the devices is obtained, demonstrating the capability to discriminate between pulses up and down around the symmetry point. This parameter reflects the intrinsic noise on the device's response under identical conditions, highlighting an intra-device variation \cite{aihwkit}. Previous studies on similar CMO/HfO\textsubscript{\textnormal{x}} ReRAM systems \cite{Stecconi2024} extracted these metrics from isolated 1R devices using an optimized open-loop scheme tailored to each device. In contrast, this work demonstrates for the first time that a single open-loop identical pulse scheme enables reliable operation of the entire CMO/HfO\textsubscript{\textnormal{x}} 1T1R array, ensuring consistent performance across the array.
\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{./Fig5.pdf}
\caption{\textbf{Open-loop array characterization for on-chip training.} \textbf{a} Bidirectional accumulative response and symmetry point of a representative device in the array. The top inset shows the open-loop identical pulse scheme used for the synaptic potentiation (red) and depression (blue). A conceptual illustration of the 8x4 CMO/HfO\textsubscript{\textnormal{x}} ReRAM array is depicted on the left. \textbf{b} Array statistical open-loop response to identical pulses. The grey area represents the standard deviation of the experimental Gaussian distributions, each corresponding to a specific pulse number. The inset shows a representative example of the experimental G-distribution at pulse number 1200. The raw data can be found in Fig. \ref{figS7} of the Supplementary Information. \textbf{c} The experimental probability densities of N$_{\rm states}$, SP$_{\rm skew}$ and NSR, respectively. The experimental data used to extract the distributions are represented as points aligned along the y=0 horizontal axis.}
\label{fig5}
\end{figure}

\subsubsection{Tiki-Taka training simulations}\label{subsubsecTT}
To perform realistic hardware-aware training simulations, the experimental device response is reproduced on software using the generalized soft bounds model implemented in the 'aihwkit' \cite{Frascaroli2018}, which better captures the bidirectional resistive switching behavior (see Fig. \ref{figS8} in Supplementary Information) and accounts for intra- and inter-device variabilities (see cycle-to-cycle and device-to-device variations in Fig. \ref{fig6}a). Additionally, Gaussian distributions are modelled based on parameters extracted from device characterization ($G_{\rm max}$, $G_{\rm min}$, $\Delta G_{\rm sp}$, NSR, SP$_{\rm skew}$) to account for device-to-device variability observed in the experimental characterization (see "Methods" section "Intra and inter-device variability" for details). This Gaussian fitting approach allows defining various device presets—characterized by the same model but with different parameter settings—to represent the synapses across the neural network. A realistic simulation setup is obtained by exclusively considering experimentally obtained parameters to reproduce the device trace (see "Methods" section "Generalized soft bounds model" for details). The device model is defined based on the observed conductance window and number of states, without assuming asymptotic behavior for an infinite number of pulses. This prevents overestimation of both the conductance window and the number of states (material states), enhancing the fidelity of the simulation.
\\
\\
To validate analog training with CMO/HfO\textsubscript{\textnormal{x}} ReRAM technology, a 3-layer fully connected (FC) neural network was trained on the MNIST dataset for image classification. In addition, the impact of the device's number of states, asymmetry, and noise-to-signal ratio on accuracy and convergence time is evaluated by simulating identical networks in which each property is individually enhanced, while keeping the others fixed at the experimentally derived values. Literature has shown that these device characteristics critically influence the convergence of analog training algorithms \cite{Rasch2024Agad}. Therefore, this method assesses the deviation of the current CMO/HfO\textsubscript{\textnormal{x}} ReRAM device properties from the ideal analog resistive device scenario. Moreover, to show the scalability of the CMO/HfO\textsubscript{\textnormal{x}} ReRAM technology to more computationally-intensive tasks, such as time series processing, a 2-layer long short-term memory (LSTM) network was trained on \textit{War and Peace} text sequences to predict the next token.  
Each network is initially trained using conventional stochastic gradient descent (SGD) based backpropagation with 32-bit FP precision, serving as the baseline performance. Fig. \ref{fig6}b illustrates the accuracy per epoch for the FP-baseline trained with SGD (in green) and the analog network trained using AGAD, evaluated under four different parameter settings: (1) properties extracted from the experimental array (in yellow), (2) reduced NSR to 20\% (in red), (3) average of N$_{\rm states}$ = 100 states (in blue), and (4) zero average device asymmetry (in orange). Using symmetrical device presets, i.e. with an average SP$_{\rm skew}$ of 50\%, improves accuracy by 0.7\% with respect to analog training with CMO/HfO\textsubscript{\textnormal{x}} ReRAM experimentally derived configuration (96.9\%), landing an accuracy of 97.6\%, a 0.7\% lower than the FP-SGD baseline (98.3\%). The other two configurations show less performance improvement, indicating more resilience of the AGAD-training to device's N$_{\rm states}$ and NSR.
\\
\\
Additionally, a 2-layer LSTM network with 64 memory states each (see Fig. \ref{fig6}c), is trained with the experimentally obtained configuration. The performance is measured using the exponential of the cross-entropy loss, i.e. the test perplexity metric, which quantifies the certainty of the token prediction. Results in Fig. \ref{fig6}d demonstrate the capabilities of the CMO/HfO\textsubscript{\textnormal{x}} ReRAM technology on more complex network architectures, such as LSTMs, and computationally demanding tasks, exhibiting performance comparable to the FP-equivalent, with an approximate 0.7\% difference in test perplexity.
\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{./Fig6.pdf} 
\caption{\textbf{Device model and on-chip training simulations.} \textbf{a} Device presets generated using the generalized soft bounds model with experimentally extracted parameters of CMO/HfO\textsubscript{\textnormal{x}}  devices, including inter- and intra-device variabilities. \textbf{b} Training simulations of a 3-layer fully-connected neural network on MNIST (235K parameters), using 32-bit FP precision trained on SGD (in green). Analog training simulations were performed using AGAD considering the empirical distribution of the parameters (in yellow), enhanced NSR (in red), increased N$_{\rm states}$ (in blue), and symmetrical device configurations (in orange). \textbf{c} LSTM network architecture for text forecasting on the \textit{War and Peace} dataset (79K parameters). The architecture  considers a sequence length of 100 tokens and accounts for 2 layers with 64 hidden units. \textbf{d} Training results of the FP baseline (in green) and the analog training with AGAD on the experimental device configuration (in yellow). The training setup can be found in the Supplementary Information.}
\label{fig6}
\end{figure}

