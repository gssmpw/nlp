\section{Related Work}
%
\label{Sec:Related}

%We classify the related work in the domain of human-AI coordination into three categories: (i) Multi-agent RL; (ii) Ad-hoc teamwork; (iii) Theory-of-mind; (iv) Offline learning. 

%\textbf{Multi-agent RL}: We first review the work in multi-agent RL (MARL) where the agent learns in an environment where other agents and humans are also acting simultaneously. This creates non-stationary environment which makes learning harder. The first category of approaches in MARL is independent Q-learning (IQL) **Wang, "Maximizing Shared Knowledge with Multi-Agent Deep Reinforcement Learning"**  where the agent tries to maximize its own reward function relying on its own observations without regard to the changing environment. This leads to suboptimal performance as the learned policies do not consider the actions of the teammates. In order to solve this issue, centralized training and decentralized execution (CDTE) approaches have been proposed~\cite {lowe2017maac} where the agent has access to states and actions of all other agents during training, thereby facilitating learning of a centralized Q-function. This centralized function is used to learn the policy of the actors during deployment using policy gradient algorithm**Sutton, "Policy Gradient Methods for Reinforcement Learning"**. These approaches fail when humans are part of the team because human behavior is unpredictable/unknown and their states and actions may not be directly observable. 

% RL with 
\textbf{Multiagent Reinforcement Learning (MARL)}: To create agents that can adapt to a diverse set of teammates, one option is to use Reinforcement Learning (RL) to train policies that maximize a reward function while treating teammates as an element of the dynamic environment. \Eg in independent Q-learning (IQL)**Tamar, "Differential Value Functions for Cooperative Multi-Agent Learning"**, an agent relies only on its observations without considering the actions of others, resulting in non-stationarity and suboptimal performance. To address this issue, one can instead use multiagent RL (MARL). In particular, centralized training and decentralized execution (CDTE) approaches have been proposed \cite {lowe2017maac} where an agent has access to states and actions of other agents during training, resulting in a centralized value function from which individual policies are derived via a policy gradient algorithm**Sutton, "Policy Gradient Methods for Reinforcement Learning"**. 

%\textbf{Ad-hoc Teamwork}: 
%
%The next category is Ad-hoc teamwork. Here the team members may come together to achieve their goals without prior knowledge or understanding of each otherâ€™s capabilities.

\textbf{Theory-of-Mind reasoning}: There is also a wealth of works trying to imbue ToM capabilities in autonomous agents (see, \eg **Rabinowitz, "Learning to Infer Human Preferences from Observations"**). Here we mention those that are mostly related to our approach on conditioning agent behavior on predictions of others' underlying motivations and observed behavior. Some, like ours, adopt the framework of I-POMDPs **Sarafianos, "Intrinsically Motivated Deep Reinforcement Learning for Human-Robot Interaction"** (an introduction to which is provided in Sec.~\ref{Subsec:Prelim}), where models of others are explicitly added as part of the decision-making space of an agent. \Eg the I-POMDP-Net**Kumar, "Intrinsically Motivated Deep Reinforcement Learning for Human-Robot Interaction"** integrates a planning algorithm to approximately solve POMDPs called QMDP**Pineau, "Anytime Point-Based Methods for Batch Bayesian Optimization of Exponential Consequences Sum Objective Functions and Their Extensions"** and beliefs about models of others into a single neural network for ToM based multi-agent decision making. The PsychSim framework**Pezeshki, "A Framework for Multiagent System Development with Social Influence and Teamwork"** is a multiagent-based simulation tool based on I-POMDPs for modeling social interaction, where agents form beliefs about the world and maintain recursive models of other agents (ToM). 

%
%Other approaches employ inverse RL (IRL) to model the motivations of others from observed behavior, which can be seen as a form of ToM ____. \Eg the MIRL-ToM approach in **Wu, "Inverse Reinforcement Learning for Human-Robot Collaboration"** allows recovering the reward functions used by different agents given trajectories of their behavior by explicitly updating posterior distributions over a set of known models via Bayesian ToM.