\section{Related Work}
%
\label{Sec:Related}

%We classify the related work in the domain of human-AI coordination into three categories: (i) Multi-agent RL; (ii) Ad-hoc teamwork; (iii) Theory-of-mind; (iv) Offline learning. 

%\textbf{Multi-agent RL}: We first review the work in multi-agent RL (MARL) where the agent learns in an environment where other agents and humans are also acting simultaneously. This creates non-stationary environment which makes learning harder. The first category of approaches in MARL is independent Q-learning (IQL)____ where the agent tries to maximize its own reward function relying on its own observations without regard to the changing environment. This leads to suboptimal performance as the learned policies do not consider the actions of the teammates. In order to solve this issue, centralized training and decentralized execution (CDTE) approaches have been proposed~\cite {lowe2017maac} where the agent has access to states and actions of all other agents during training, thereby facilitating learning of a centralized Q-function. This centralized function is used to learn the policy of the actors during deployment using policy gradient algorithm____. These approaches fail when humans are part of the team because human behavior is unpredictable/unknown and their states and actions may not be directly observable. 

% RL with 
\textbf{Multiagent Reinforcement Learning (MARL)}: To create agents that can adapt to a diverse set of teammates, one option is to use Reinforcement Learning (RL) to train policies that maximize a reward function while treating teammates as an element of the dynamic environment. \Eg in independent Q-learning (IQL)____ an agent relies only on its observations without considering the actions of others, resulting in non-stationarity and suboptimal performance. To address this issue, one can instead use multiagent RL (MARL). In particular, centralized training and decentralized execution (CDTE) approaches have been proposed \cite {lowe2017maac} where an agent has access to states and actions of other agents during training, resulting in a centralized value function from which individual policies are derived via a policy gradient algorithm ____. However, such approaches do not support cases where agents cannot directly observe the internal states of others. 

%In order to adapt to human behavior, one approach can be to train agents against human models and/or a diversified set of agent models using conventional MARL approaches. This leads to Self Play (SP)____ and Population Based Training (PBT)____ approaches. Self-play consists of agent learning against itself, while population-based approaches consist of first building a population of diverse agent models (e.g., using genetic algorithms for cross-mutation of parameters or multiple checkpoints during the course of training) and then training against agents sampled from this population to imbue diversity in the learned policies. The motivation here is that human behavior is highly uncertain and training against a wide variety/diverse set of agent models helps in adapting to humans. There are also approaches that employ maximum-entropy RL concept____ to PBT e.g., maximum-entropy population-based training____ to train agents for zero-shot human-AI coordination. However, these approaches have fixed number of teammates (usually two – agent and human) and the agent profiles (reward functions to optimize) are fixed as they were in training. In contrast ToMCAT supports varying number of teammates and the agent profile can be conditioned during runtime to support playing certain roles in the team.

One approach to overcome that limitation is to use self-play (SP)____, where an agent is trained to play against itself. However, this is mostly applicable to zero-sum settings or homogeneous teams, being hard to adapt to different styles of play or roles in a task. Another approach is to train single-agent policies against a known set of agent models. Namely, in population-based training (PBT)____ one first builds a population of diverse agent models and then trains a policy against agents sampled from the population, improving the population during training, \eg using genetic algorithms. %Furthermore, in ____, a PBT approach based on the maximum-entropy principle is presented that aims to maximize the diversity of the population. 
While ToMCAT follows a similar approach to PBT by learning from a diverse set of teammates, we 
%aim to 
train a single model to learn the distribution of possible team behaviors in a task such that an agent can be assigned any role and adapt to the behavior of arbitrary teammates, including sub-optimal ones w.r.t. the task.

%It is technically possible to optimize for different reward functions using Multi-Objective RL (MORL)____, where agents optimize multiple, often conflicting objectives simultaneously. MORL involves learning policies that balance trade-offs among multiple criteria (e.g., cost, safety, and performance). There are two main line of approaches---(i) scalarization-based methods e.g.,____ which convert multi-objective problems into single-objective ones using techniques like weighted sums, and (ii) Pareto-based methods, e.g.,____, which aim to find a set of Pareto-optimal solutions representing the trade-offs. While these solutions can support multiple agent profiles, and possibly multi-agent scenarios, many of settings viz., reward functions, number of teammates, etc. need to be determined before hand and as such do not support flexibility like our approach.

%\textbf{Adhoc teamwork}: In the case of ad-hoc teamwork the agent joins a team in an adhoc manner and needs to collaborate with the team on the fly. Approaches in this category focus on collaborating with teammates (with task known beforehand) or on inferring the underlying task itself. An example of the former, ODITS____, proposes a single agent RL framework for coordinating with teammates by considering all the other teammates as part of the environment. Specifically, it learns a teamwork situation encoder (embeds environment and task states) and conditions the RL policy on this embedding. There are also simpler approaches for adapting to teammates online in a Bayesian manner. For example, Paleja et al.____, proposes a method to adaptively pair with a human teammate using Bayesian inference. Trained on human demonstration data, the model accounts for the commonalities and differences amongst demonstrators. At test time, it estimates the demonstrator’s `style' or `unique descriptor' (i.e., latent embedding) in real-time by employing a Bayesian Neural Network (BNN), which reasons about the discrepancy between the average demonstrator and the specific one currently being observed. However, similar to MARL approaches, the agent profile is fixed as used in training. Also, in case of ODITS, considering teammates as part of the environment can lead to sub-optimal performance as mentioned earlier. An example of the latter, ATPO____, proposes a method for adhoc teamwork, focusing on inferring the underlying task, which is complementary to ours. 

\textbf{Adhoc teamwork}: Another related body of work is that of ad hoc teamwork____, where an agent needs to collaborate effectively with other agents without prior coordination or explicit pre-training. Some approaches assume that the task is known beforehand, \eg the ODITS____ framework trains a single-agent RL policy by considering all teammates as part of the environment, learning a teamwork situation encoder whose latent representation conditions the policy for online adaptation, akin to the way we condition diffusion policies based on ToM embeddings. Other approaches try to infer the underlying collaborative task itself, assumed unknown to the ad hoc agent. \Eg the ATPO framework____ casts ad hoc teamwork as identifying the task/teammate from a set of known tasks from histories of observations, then selecting the appropriate action from the corresponding pre-computed task policy. One disadvantage of RL-based approaches is that if an agent's objectives need to change during a task, \eg for role reassignment, we need to re-train the policy, which can be computationally expensive. In contrast, ToMCAT does not train (a set of) RL policies explicitly optimizing for a single reward function---rather, it uses denoising-diffusion policies to generate plans guided by both the agent's own preferences, which might not be aligned with those of its teammates, and teammates' perceived behavior characteristics. In addition, our models make predictions about the underlying attributes of teammates, \ie from a family of parameterized agents, rather than trying to identify a teammate from a discrete set.

%\textbf{Theory-of-mind related}: Theory-of-mind (ToM) involves inferring the intentions/goals of the teammates and using that in one's own decision making. 
% Related works in this domain can be categorized into non-adhoc and adhoc teamwork based on whether the agent is trained to adapt to known (non-adhoc) or unknown (adhoc) teammates. In the case of non-adhoc teamwork, 
%For example, I-POMDP____ extends the regular POMDP framework to account for other agents’ models in the MDP state space. Each agent maintains a belief over the environment state and also a belief about other agent models and uses this belief in its decision making (note that such belief can be nested for several levels). Belief update takes place in a Bayesian manner from the actions of the teammates and the changes in the environment state. I-POMDP-Net____ extends I-POMDP by integrating a planning algorithm called QMDP (an approximate algorithm for solving POMDPs)____ to solve for the underlying POMDP. It further combines both POMDP (model) and QMDP (planning) into a single neural network for end-to-end differentiable learning for ToM based multi-agent decision making. However, these approaches only work with fixed agent profiles and/or number of teammates and the Bayesian belief update can become computationally intractable as the complexity of the environment (recursive nature of ToM) and number of teammates grows.
%Wu et al.,____ propose MIRL-ToM, an approach for recovering the reward functions used by agents interacting both with known and unknown teammates via a combination of ToM reasoning and inverse RL. However, the approach is complementary to ours as we use ToM reasoning for computing the best-response strategy when collaborating with teammates.
% However, Bayesian belief updating is computationally expensive in this recursive model of ToM and moreover agent profiles and number of teammates need to be fixed beforehand unlike ours. 
%Psychsim____ provides a multiagent-based simulation tool for modeling social interaction and influence based on ToM. In this framework, agents have their own decision-theoretic models of the world, including beliefs about their environment and recursive models of other agents. Rabinowitz et al.____ propose a Theory of Mind neural network, ToMnet, which uses meta-learning to build models of the agents from observations of their behavior. Using this, it acquires a strong prior model for agents' behavior, as well as the ability to bootstrap to richer predictions about agents' characteristics and mental states using only a small number of behavioral observations. We leverage this framework in our approach to compute ToM embeddings of teammates which will be conditioned on the agent's decision-making process.

\textbf{Theory-of-Mind reasoning}: There is also a wealth of works trying to imbue ToM capabilities in autonomous agents (see, \eg ____). Here we mention those that are mostly related to our approach on conditioning agent behavior on predictions of others' underlying motivations and observed behavior. Some, like ours, adopt the framework of I-POMDPs ____ (an introduction to which is provided in Sec.~\ref{Subsec:Prelim}), where models of others are explicitly added as part of the decision-making space of an agent. \Eg the I-POMDP-Net____ integrates a planning algorithm to approximately solve POMDPs called QMDP____ and beliefs about models of others into a single neural network for ToM based multi-agent decision making. The PsychSim framework____ is a multiagent-based simulation tool based on I-POMDPs for modeling social interaction, where agents form beliefs about the world and maintain recursive models of other agents (ToM). It uses exact Bayesian inference to update the beliefs upon new observations, allowing an agent to compute optimal actions conditioned on the ToM models through forward planning. 
%
Other approaches employ inverse RL (IRL) to model the motivations of others from observed behavior, which can be seen as a form of ToM ____. \Eg the MIRL-ToM approach in ____ allows recovering the reward functions used by different agents given trajectories of their behavior by explicitly updating posterior distributions over a set of known models via Bayesian ToM, which allows modeling adaptation to others without requiring a joint equilibrium solution. Overall, these approaches only work with fixed agent profiles and/or a fixed number of teammates, and the (explicit) Bayesian belief update can become computationally intractable as the number of teammates grows. In contrast, ToMCAT does not use an explicit planning module to solve for the I-POMDP and instead learns a distribution of team behaviors from a rich dataset and uses diffusion policies for fast online adaptation to different teammates.

%\textbf{Offline (MA)RL and diffusion policies}: Unlike the conventional RL setup, offline RL learns a policy using a fixed, pre-collected dataset without requiring real-time interactions with the environment. 
% Once trained, this policy is then utilized to engage with the online environment, either to achieve favorable outcomes or to facilitate exploration. 
%The main challenge lies in that the distribution of trajectories in offline datasets differs from those encountered during online interactions. Several line of works have been proposed to address this problem---(i) traditional methods including value regularization____, policy regularization____, among others (ii) Transformer based approaches (such as Decision Transformer____) which get away with TD-learning and the associated difficulties with sparse reward. For example, MADT____ extends Decision Transformer to multi-agent domain by making agents controlled by a shared weight transformer-based policy. Tseng et al.____ further extend MADT to a CTDE paradigm using knowledge distillation techniques; (iii) Diffusion-based approaches____ leverage diffusion processes to improve exploration, representation learning, and policy optimization. Inspired by diffusion models used in stochastic processes____, these methods typically involve generating or refining policy distributions using diffusion dynamics, enabling agents to better explore high-dimensional or complex state-action spaces. Recent studies____ have demonstrated the use of diffusion models for planning by combining dynamics modeling and subsequent planning into a single diffusion model. 
% Additionally, diffusion processes have been applied to policy sampling and value function approximation to mitigate issues like local optima or poor exploration []. 
%These approaches are particularly effective in environments with sparse rewards or multimodal policy requirements or for their ability to learn conditional policies. There have also been works in extending these to multi-agent settings____. We therefore build on these works to be able to condition our policy generation based on the inferred teammate behavior encoded in ToM embeddings.

%In the domain of Shared Control, a human interacts with a machine using an interface (e.g., teleoperation) to achieve a particular task e.g., pickup a cup from table. Related work in this domain can be classified into two categories: (i) Human-initiative; (ii) Mixed-initiative. In the former, the human takes the initiative/main control and the machine is passive. The machine tries to infer the human intention and help human achieve the action intended. In the latter, both can take initiative, i.e., the machine can either reject, accept or maintain the inputs coming from human operating it and vice-versa. In both cases, the machine needs to infer the intentions of the human and adapt accordingly to complete the underlying task. Majority of the works in this domain fall in the first category spanning domains of hand-held robotics (LA-SA), teleoperation (ASHA, FirstContact), autonomous driving (SCDAS), etc. However, in these approaches, the agent profile is fixed as used in training and the number of agents is effectively only one (Human-initiative)  or maximum two (Mixed-Initiative). Hence, even though inferring the intentions of the human is common, the application scenario is fundamentally different from what ToMCAT is intended for (as a team player). 

% ToMCAT
% =========================================