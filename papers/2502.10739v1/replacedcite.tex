\section{Related Work}
\label{sec:related_works}
Early Text-to-SQL methods are mainly based on the Seq2Seq method. The natural language and database schema are semantically encoded through the encoder, and then decoded through the decoder to generate the corresponding SQL statement____. Since the SQL language has certain grammatical rules, it is easy to make errors when generating SQL models directly through the decoder. On this basis, slot filling methods are based on SQL grammatical rules ____ and sketch-based methods ____ are proposed. For example, ____ proposes a ranking-augmented encoder to alleviate the workload of schema linking and a skeleton-aware decoder to implicitly guide the SQL generation of skeletons. These methods are limited by the capabilities of the basic model, the model generalization ability is not strong, and a large amount of data is required for fine-tuning for databases in different domains.

With the rapid development from pre-trained language model to large language model, LLM-based Text-to-SQL methods have become mainstream. DIN-SQL____ decomposes the generation problem into multiple sub-problems  and uses GPT-4 for ICL, and finally defeats a large number of fine-tuned models. DAIL-SQL____ explores the advantages and disadvantages of different schema representations, example selections, and example organizations through a large number of comparative experiments, providing effective suggestions for subsequent research. MCS-SQL____ uses different prompts to explore a broader search space to generate multiple candidate SQLs, then filters them based on confidence scores, and finally uses multiple-choice selections to obtain the final results. ____ use a multi-agent collaboration framework and use different agents to complete specific subtasks and finally complete SQL generation. CHASE-SQL____ found that the widely used self-consistency strategy could not effectively select the best SQL from a large amount candidate SQL pools, so it proposed a sorting strategy based on pairwise comparisons and combined three innovative strategies to generate diverse and high-quality candidate SQL, achieving SOTA on the BIRD dataset. Since the ICL-based method does not require additional computing resources for model fine-tuning and has high flexibility and generalization, it is currently a widely used method.

Due to issues such as data privacy and efficiency, methods based on SFT have certain advantages. ____ selected open source LLM for fine-tuning and achieved good results, surpassing many methods that use proprietary closed source LLM. CodeS____ combines schema linking, data enhancement, and pre-training methods to achieve SOTA results on multiple datasets, and open-sources pre-trained models from 1B to 15B. SENSE____ uses strong LLM to synthesize weakly supervised data to train small LLM. The experimental results show the effectiveness of its synthetic data.  MSc-SQL____ first filters the schema by schema linking, then uses multiple small LLM to fine-tune to generate a variety of candidate SQLs, and finally uses a selector to select the final SQL. CHESS____ and XiYan-SQL____ combine the advantages and disadvantages of ICL and model fine-tuning, based on a pipeline approach, including entity and context retrieval, schema linking, and fine-tuned LLM for SQL generation and SQL selection, proving the effectiveness of the combination of ICL and SFT methods.

% Currently, there is a lack of effective baseline methods based on open source model fine-tuning, and the effect is much lower than the ICL method using closed source models. Therefore, we propose an efficient baseline method based on open source model fine-tuning, aiming to narrow the gap with the ICL method using closed source models.