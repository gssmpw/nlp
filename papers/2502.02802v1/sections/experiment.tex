\section{Experiments}
\label{sec:experiments}

Our experiment focuses on evaluating the ability to demonstrate different aspects of consistency for clients simulated using our framework and other methods. First, we introduce the experiment setup and baselines. Next, we report the methods' performance including profile consistency and turn-level response quality. Furthermore, we analyze the similarity between simulated and real sessions in terms of session length, receptivity distribution, motivation rate, and action distribution. Finally, we conduct expert evaluation to highlight the realism and performance of our framework. All the prompts and their description can be found under the supplementary materials.

\subsection{Experiment Setup}
\label{sec:setting}

\paragraph{Counselor and moderator agents.}
Similar to \citet{tu2024towards}, our experiment deploys two other  agents to generate counseling sessions with the simulated clients to be evaluated\footnote{We employ \texttt{gpt-3.5-turbo-0125} as the LLM backbone.}, namely: the {\em  counselor} and {\em moderator} agents. 
%The client agent is role-played using our framework. 
While the client agents of different profiles can be simulated using different methods, only one counselor agent and one moderator agent are used. 
The counselor agent assumes the role of an empathetic advisor, trying to identify the client's behavior problem and motivations, thereby facilitating behavior change and making change plans.  Inspired by previous works~\citep{chiu2024computational,wang2024towards,yosef2024assessing}, this agent is developed by prompting the LLM with the basic knowledge about MI and the different stages of counseling (See Table~\ref{tab:couneslor prompt}).  Each counseling session begins with the counselor utterance: ``\textit{Hello. How are you?}'' The client then responds with ``\textit{I am good. What about you?}''. The counselor continues to generate the next utterance to continue the session. The process continues until the moderator decides to end the session.  The above counselor simulation has been validated to work well in terms of therapeutic alliance and skill employment.  

The moderator oversees the dialogue to decide when the session should end.  This decision is made when any of the three conditions is met, namely: 1) the counselor stops persuading the client; 2) the client agrees to a plan of action; or 3) the session reaches its maximum length of 100 turns, similar to the maximum number of turns in AnnoMI sessions.  We implement the moderator agent by prompting LLM with the above end conditions (see Table~\ref{tab:moderator prompt}). 
%Unlike the client agent which can take different client profiles as input, only one counselor agent is required in our experiment. 

\paragraph{Client simulation methods.} We evaluate our framework-based client simulation method against four state-of-the-art LLM-based client simulation methods, namely: (a) \textbf{Base Method} which include only the behavioral problem of client in the LLM prompt to generate the next utterance.  This method relies on behavior of the underlying LLMs and has been employed in emotion support conversation task~\citep{deng2023plug, deng2023prompting} (see Table~\ref{tab:baseclient prompt}); (b) \textbf{Example-based Method} which provides a real counseling session exemplar involving the same client and prompts the LLM to simulate the same client talking to a different counselor in a parallel universe~\citep{chiu2024computational} (see Table~\ref{tab:example based prompt}); (c) \textbf{Profile-based Method} which prompts the LLM to simulate the client based on the given client profile~\citep{yosef2024assessing,wang2024patient} (see Table~\ref{tab:profile based prompt}); and (d) \textbf{Pro+Act-based Method} which provides both the client profile and the description of all actions in a LLM prompt to generate the next utterance~\citep{zhang2024strength} (see Table~\ref{tab:action based prompt}). This method however does not involve state-tracking and state-dependent utterance generation. 
 
To obtain more reliable results, each method generates three sessions for each client profile of AnnoMI. We thus have 258 (= 86 $\times$ 3) generated sessions for each method. We then average the profile-specific metric results for the three sessions/client before deriving the average across all the client profiles. 

\subsection{Automated Evaluation of Profile and Receptivity Consistency}
\label{sec:profile consistency}


\begin{table}[tb]
\centering
\resizebox{0.47\textwidth}{!}{
\begin{tabular}{lrrrrc}
\toprule
              & PE$\uparrow$    & MO$\uparrow$         & BE$\uparrow$      & CP$\uparrow$       & RE$\uparrow$  \\ \midrule
Base          &  9.01       & 16.17             & 12.15         &  9.30       & $-0.31^{**}$  \\
Example-based & 53.68       & 45.73             & 45.55         & 33.53       & $0.25^*$    \\
Profile-based & 61.97       & 53.44             & 67.17         & 54.67       & $0.31^{*}$   \\
Pro+Act-based & 67.09       & 55.33             & 68.60         & 57.17       & $0.33^{**}$   \\ \hline
Ours          & \textbf{70.57}   & \textbf{73.37}   & \textbf{71.70}  & \textbf{68.51} & \textbf{0.58}$^{**}$   \\ \bottomrule
\end{tabular}}
\caption{Consistency of Personas (PE), Motivation (MO), Beliefs (BE), and Preferred Change Plans (CP), and Receptivity (RE). High consistency indicates that the client profile information in the generated sessions match the original profile information well. Persona, Motivation, Belief and Plan consistencies are measured by {\em entailment}. Receptivity consistency is measured by {\em Spearman's Correlation} between the client receptivity scores of the generated sessions and those of the original sessions. The superscript ``*'' denotes a p-value less than 0.1, while ``**'' indicates a p-value less than 0.05.}
\label{tab:consistency}
\end{table}

Our automated evaluation includes four aspects of profile consistency, i.e., personas, motivation, beliefs, and preferred change plan, of the simulated clients. It is designed to evaluate a large number of clients and their MI counseling sessions without much human expert effort and cost.
Following the same pipeline for AnnoMI data annotation, we obtain the above four profile components from each generated session and assess if each component (e.g., persona) is entailed in the corresponding profile component from the original AnnoMI session. Specifically, we employ GPT-4 to perform entailment assessment in a few-shot manner. Additionally, for profile components not found in the original session (usually the change plan component), negative entailment outcome is always assigned 
%{\color{red} (EP: Is it possible for the generated session to contain some empty profile component(s).)} {\color{orange} (Yizhe: Indeed, it is possible for LLMs to possess some degree of imagination, although this capability is inherently limited. For instance, ChatGPT may prefer to state that they come from a family that engages in smoking or drinking. When the components are empty, the LLM can provide more information by themselves.)}.  
We then define the consistency score of the client simulation method for a profile component by the proportion of sessions with positive entailment for the component. As shown in Table~\ref{tab:consistency}, our method outperforms the rest across all profile components.  This can be attributed to its stronger state control and information selection mechanism. With state control, the motivation and change plan will be compared with the corresponding client profile components. Therefore, they should be consistent when the client changes state. On the other hand, based on the information selection mechanism, only the information in the given profile will be expressed. Thus, the exposed personas and beliefs should be from the profile.  Base method performs the worst, generating not more than 16.17\% of the sessions consistent with the ground truth client profiles across the four aspects. This is reasonable, as no reference profiles were provided. Example-based method also does not perform well, possibly due to the LLM failing to consistently simulate specific personas, beliefs, motivations change plans in lengthy sessions compared to profiles.  Profile-based and Pro+Act-based methods perform rather well in persona but less so in other profile components. These two methods may accept false motivations or plans non-existent in the client profiles. 

For receptivity, we compute the Spearman's correlation between the receptivity scores of simulated clients and clients in original sessions. We follow the client profile construction step to derive the receptivity scores of the simulated clients. As shown in Table~\ref{tab:consistency}, our method outperforms the other baselines with a moderate correlation score. The Base method returns a negative correlation due to not incorporating receptivity. None of Example-based, Profile-based and Pro+Act-based methods can simulate the client with the appropriate receptivity. As shown in Table~\ref{tab:population consistency}, Our client simulation method demonstrates average receptivity (Avg Rec), motivation rate (MR@20) and turns to motivate (Avg MS) similar to that of real sessions.  Clients simulated by other methods in contrast show overly receptive behavior towards the counselor agent as shown by their significantly higher average receptivity, higher motivation rate and fewer turns to motivate.  Only small deviation of receptivity is also observed among them. Among the baselines, Pro+Act-based method can simulate more diverse receptivity by considering action-related instructions.  
%Not surprisingly, the baseline methods also The motivation rate MR@20 measures the proportion of simulated clients contemplating the change of behavior in the first 20 turns of interaction between the counselor and client. As shown in Table~\ref{tab:population consistency}, the average MR@20 of real counseling is 0.48, which is much lower than the baseline methods. Our method could achieve the slight higher MR@20. The clients simulated by baseline methods require an average of fewer than 10 turns to get the client motivated to change. Again, our method and real data requires significantly more turns to achieve the outcome. 

\begin{table}[tb]
\centering
\resizebox{0.47\textwidth}{!}{
\begin{tabular}{lrrrr}
\toprule
               & Avg Rec            & MR@20             & Avg MS       & Act KL $\downarrow$        \\ \midrule
Base           & 4.42$_{\pm0.47}$    & 1.00              & 6.60          & 0.39              \\
Example-based  & 4.08$_{\pm0.63}$    & 1.00              & 7.60          & 0.24              \\
Profile-based  & 4.12$_{\pm0.64}$    & 0.96              & 9.76          & 0.15             \\
Pro+Act-based  & 3.86$_{\pm1.01}$    & 0.94              & 9.93          & 0.13              \\ \hline
Ours           & 3.32$_{\pm1.15}$    & 0.69              & 18.60         & 0.06             \\ \hline
Real           & 3.27$_{\pm1.12}$    & 0.48              & 27.56         & 0.00              \\ \bottomrule
\end{tabular}}
\caption{Average receptivity level (Avg Rec), motivation rate at the 20th turns(MR@20), average motivation step (Avg MS), and action KL divergence (Act KL). Our method shows lower receptivity, lower motivation rate (or more motivation steps), and higher action diversity than other methods, which are more aligned with the real sessions.}
\label{tab:population consistency}
\end{table}

\subsection{Session Length and Action Distribution Analysis}
\label{sec:population consistency}

\begin{figure*}[tb]
    \centering
    \includegraphics[width=\textwidth]{figs/length.pdf}
    \caption{Distribution of turn count (Count) for various clients. The length of AnnoMI counseling sessions is diverse and generally longer, while simulated sessions tend to have fewer than 50 turns.} 
    \label{fig:length distribution}
\end{figure*}

%In this section, we compare the real (or AnnoMI) and counseling sessions generated by each method using different measures. A good client simulation method is expected to generate counseling sessions similar to real counseling sessions.

\paragraph{Length of Counseling Sessions.} As shown in Figure~\ref{fig:length distribution}, the length of real counseling sessions ranges from 10 to 100 turns. Most generated ones contain however fewer than 50 turns. Nevertheless, the sessions generated by our method exhibit slightly longer length than previous works. We found that previous methods tend to incorporate more profile information in one utterance as they lack the information selection module. They also prone to having clients motivated to change without much counselor's effort or accept motivations and plans not aligned to their given profiles. In contrast, the shorter sessions generated by our method are mainly due to the counselors giving up when they fail to motivate the simulated clients in the Precontemplation state. This can be explained by our method limiting state changes and leakage of profile information to the counselor through state transition and information selection controls. Without the counselor putting effort into knowing the clients and practising effective MI approach, the counseling session is expected to make little progress. This also highlights the importance of consistent simulated clients in training counselors.


%\paragraph{Receptivity, Motivation Rate and Motivation Steps.} We analyse the average receptivity, motivation rate and average motivation steps by annotating them based on GPT-4 following previous pipeline, as shown in Table~\ref{tab:population consistency}. The average receptivity scores of all the baseline methods are much higher than that of real counseling sessions, indicating excessive openness of the simulated clients.  Our client simulation method, in contrast, demonstrates a low average receptivity very similar to that of real data. The low deviation of receptivity among the baselines is due to low diversity among them. Pro+Act-based method can simulate more diverse receptivity by considering action-related instructions.  The motivation rate MR@20 measures the proportion of simulated clients contemplating the change of behavior in the first 20 turns of interaction between the counselor and client. As shown in Table~\ref{tab:population consistency}, the average MR@20 of real counseling is 0.48, which is much lower than the baseline methods. Our method could achieve the slight higher MR@20. The clients simulated by baseline methods require an average of fewer than 10 turns to get the client motivated to change. Again, our method and real data requires significantly more turns to achieve the outcome. 

\paragraph{Action Distributions.} We finally evaluate the KL divergence of action distributions between the simulated clients and real clients for the different methods. As shown in Table~\ref{tab:population consistency}, our method demonstrates very small KL divergence which implies that the simulated clients adopt an overall distribution of actions very similar to that of real clients. Other baseline methods, on the other hand, see higher KL divergences as they do not incorporate the real distribution of client actions to guide action selection in realistic counseling scenarios nor employ an action sampling strategy to mitigate selection bias.


\subsection{Expert Evaluation}
\label{sec:exp_eval}

In this section, we further conduct expert evaluation on a small set of sessions generated by a few best performing client simulation methods. Other than profile and receptivity consistency evaluation, we also introduce expert judgment of whether the simulated clients are similar to human clients.  

\paragraph{Profile Consistency} In addition to the GPT-based automated evaluation, we assess the consistency and realism of the simulated clients by experts. We employ four of our co-authors, who are experts in psychology and experienced in MI counseling, as annotators. Their background enables them to comprehend and annotate typical client behaviors. We randomly select six clients and corresponding real sessions from AnnoMI, then employ Profile-Based, Pro+Act-Based, and our methods to simulate the client’s interaction with the counselor agent to generate sessions.\footnote{Given the performance from GPT-based evaluation (Table~\ref{tab:consistency}), we only adopt the Profile-based and Pro+Act-based methods for comparison. We disregard the Base and Example-based methods due to their inferior performance in GPT-based evaluation} For each generated session, we assign three experts to annotate the consistency of profile components, including persona, beliefs, motivation, and plan, compared to the given profile. Each score ranges from 1 (low consistency) to 5 (high consistency).\footnote{More details about expert evaluation can be found in Appendix~\ref{app:human evaluation}.} Finally, we collect a total of 288 annotation scores, derived from 6 clients $\times$ (sessions generated using 3 methods + 1 observed session) $\times$ 3 annotators $\times$ 4 components.  %{\color{red} (EP: we should use the ground truth receptivity level only.  We describe the sessions for receptivity evaluation in another separate para.)}{\color{orange}(Yizhe: We treat the receptivity derived from the original session as ground truth in previous content. Thus it may be considered as default setting. For the receptivity evaluation, we highlight that we adjust the receptivity levels manually.)}. 
As shown in Table~\ref{tab:expert_evaluation_consistency}, the results demonstrate that our method outperforms the baselines, which aligns with the GPT-based evaluation. However, there is still a gap between the simulated client and the real client, indicating the potential for future improvement.

\begin{table}[tb]
\centering
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{cccccc}
\toprule
\multirow{2}{*}{} & \multicolumn{4}{c}{Consistency$\uparrow$}             & \multirow{2}{*}{Realism$\uparrow$} \\ \cline{2-5}
                  & Personas & Beliefs & Motivation  & Plan       &                          \\ \midrule
Profile-Based     & 2.61     & 2.00    & 2.61       & 1.56      & 2.38                     \\
Pro+Act-Based     & 2.65     & 2.22    & 2.78       & 1.56      & 2.50                     \\
Our               & \textbf{3.33}     & \textbf{2.89}    & \textbf{3.00}       & \textbf{2.27}  & \textbf{3.16}          \\ \hline
Real              & 4.72    & 4.67   & 4.56       & 4.61      & 4.72                     \\ \bottomrule
\end{tabular}}
\caption{Expert Evaluation on Profile Consistency and Realism for Client Simulation. Our method demonstrates superior performance compared to the two baselines, with a p-value less than 0.05. Nevertheless, the real client consistently outperforms all simulation methods with a p-value less than 0.01. The overall Kappa score of experts evaluation is 0.61 (p-value $\le 0.05$) indicating moderate agreement.}
\label{tab:expert_evaluation_consistency}
\end{table}

\paragraph{Realism} Furthermore, we instruct the experts to assess the realism of client behavior. The experts are tasked with comparing the client’s behavior during the given session with the behavior of actual human client in the real world. We employ the same sessions in consistency evaluation for realism evaluation, which consists of 6 clients and 4 sessions per client. We randomly assign three experts to annotate the realism of client for each session, resulting in a total of 72 annotation scores. The annotation schema employs a five-point Likert scale, ranging from 1 (low realism) to 5 (high realism). The final results are presented in Table~\ref{tab:expert_evaluation_receptivity}. Our simulated clients exhibit significantly superior performance compared to other baselines. Based on the annotation, the primary disadvantage of other baselines is that clients exhibit excessively compromised behaviors, which are unrealistic in the real world.
%{\color{red} (EP: Use a separate paragraph for realism evaluation as it is a separate criteria which should be evaluated by experts only as only the experts know how human clients would behave. Justify how we derive the realism ratings.)}

\paragraph{Receptivity Consistency} We further substantiate our claim that our method can adjust behaviors in accordance with receptivity. To achieve this, we have developed a framework for expert evaluation of receptivity consistency. We randomly select four clients (and their profile components) and generate for each client three counseling sessions covering low receptivity score (1), middle receptivity score (3), and high receptivity score (5). We eliminate the Profile-based method, as the GPT-based evaluations (Table~\ref{tab:population consistency}) demonstrate its performance with high receptivity regardless of the given receptivity in the profile. On the other hand, since the GPT-based evaluation (Table~\ref{tab:population consistency}) suggests that the Pro+Act-based methods exhibit lower and more diverse receptivity compared to other baselines, we incorporate it for comparison with our method. For each session, we randomly assign two experts to annotate the receptivity level of the client in the given session. To facilitate the expert’s annotation and enhance agreement between expert and annotated scores, we simplify the scoring system from five points to three points, categorizing it as low (1), middle (3), and high (5). Finally, we obtain a total of 48 annotation scores (4 clients $\times$ 3 receptivity levels $\times$ 2 methods $\times$ two annotators). Subsequently, similar to the evaluation presented in Tables~\ref{tab:consistency} and \ref{tab:population consistency}, we present the average annotated receptivity score and the Spearman correlation between annotated receptivity scores and the ground truth receptivity scores.

As shown in Table~\ref{tab:expert_evaluation_receptivity}, the sessions generated by our framework achieve the receptivity closer to the assigned one. Almost all sessions generated by the Pro+Act method are annotated as high receptivity, suggesting that the baselines are excessively compliant, disregarding the given receptivity. The receptivity of sessions generated by our framework is highly correlated with the assigned receptivity, indicating the effectiveness of our receptivity control mechanism. However, the average receptivity of our highly receptive client is not sufficiently high, which may be attributed to the imposition of a stringent motivation condition. Only when the counselor touches the inherent motivation of the client will the client be motivated. Consequently, the client may consistently remain in the Precontemplation state, particularly when the counselor lacks the requisite skills.


\begin{table}[tb]
\centering
\footnotesize
\begin{tabular}{ccccc}
\toprule
           & \multicolumn{3}{c}{Avg. Receptivity} & {Receptivity$\uparrow$} \\ 
           \cline{2-4}
           & 1.0          & 3.0          & 5.0         & Correlation                              \\\midrule
Pro+Act Based & 5.0          & 4.3        & 5.0          & 0.00                    \\
Ours       & 1.3        & 3.0          & 4.3         & 0.86                \\ \bottomrule
\end{tabular}
\caption{Expert Evaluation of Receptivity Consistency. Experts assesses the receptivity across various receptivity levels, with the average receptivity for each level presented. Additionally, Spearman Correspondence is calculated to quantify the correlation between the expert’s ratings and the assigned receptivity levels. The Kappa coefficient between expert annotations is 0.87 (p-value $\le 0.001$), indicating high agreement. Specifically, the p-value of Spearman Correspondence is 1.0 for the Pro+Act-Based method, indicating no relationship in the population, while 0.0003 for our method, indicating high significance for the relationship.}
\label{tab:expert_evaluation_receptivity}
\end{table}


 



