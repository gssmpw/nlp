\section{Related Work}
\vspace{-2pt}
\subsection{Neural PDE Solver}
Traditional numerical methods for solving PDEs often require high computational costs to achieve accurate solutions~\cite{solanki2003finite}. Recently, deep learning methods have demonstrated remarkable potential as efficient surrogate models for solving PDEs due to their inherent non-linear modeling capability, known as neural PDE solvers.

As a typical paradigm, operator learning has been widely studied for solving PDEs by learning the mapping between input functions and solutions. FNO \cite{li2020neural} first proposes to approximate integral in the Fourier domain for PDE solving. Subsequently, Geo-FNO \cite{li2021fourier} extends FNO to irregular meshes by transforming them into regular grids in the latent space. To further enhance the capabilities of FNO, U-FNO \citet{Wen2021UFNOA} and U-NO \citet{rahman2022u} are presented by leveraging U-Net \cite{ronneberger2015u} to capture multiscale properties. Considering the high dimensionality of real-world PDEs, LSM \cite{wu2023LSM} applies spectral methods in a learned lower-dimensional latent space to approximate input-output mappings. Afterward, LNO \cite{wang2024LNO} adopts the attention mechanism to effectively map data from geometric space to latent space for complex geometries.

Recently, Transformers \cite{NIPS2017_3f5ee243} have achieved impressive progress in extensive fields and have also been applied to solving PDEs, where the attention mechanism has been proven as a Monte-Carlo approximation for global integral \cite{jmlr_operator}. However, standard attention suffers from quadratic complexity. Thus, many models like Galerkin \cite{Cao2021ChooseAT}, OFormer \cite{li2023transformer} and FactFormer \cite{anonymous2023factorized} propose different efficient attention mechanisms. Among them, GNOT \cite{hao2023gnot} utilizes well-established linear attention, like Reformer or Performer \cite{kitaev2020reformer,performer}, and separately encodes geometric information, achieving favorable performance. However, linear attention often suffers from degraded performance as an approximation of standard attention \cite{qin2022devil}. Moreover, these Transformer-based methods treat input geometries as a sequence of mesh points and directly apply attention among mesh points, which may fall short in geometric learning and computation efficiency. As a significant advancement in PDE solving, Transolver~\cite{wu2024Transolver} introduces the Physics-Attention mechanism that groups massive mesh points into multiple physical states and applies attention among states, thereby enabling more effective and intrinsic modeling of complex physical correlations. However, it still faces challenges in degenerated physics learning and a high computation burden under million-scale geometries. These challenges will be well addressed in our paper.

In addition to Transformers, graph neural networks (GNNs) \cite{hamilton2017inductive,gao2019graph, pfaff2021learning} are also inherently suitable to process unstructured meshes by explicitly modeling the message passing among nodes and edges. GNO \cite{Li2020NeuralOG} first implements neural operator with GNN.
Later, GINO \cite{li2023geometryinformed} combines GNO with Geo-FNO to encode the geometry information. Recently, 3D-GeoCA \cite{anonymous2023geometryguided} integrates pre-trained 3D vision models to achieve a better representation learning of geometries. However, prone to geometric instability \cite{instabilityGNN}, GNNs can lead to unstable results and may be insufficient in capturing global physics interactions, especially when handling large-scale unstructured meshes \cite{morris2023geometric}.

\subsection{Parallelism of Deep Models} 
Despite processing large-scale geometries being crucial in industrial design, this problem has not been explored in previous research. This paper breaks the computation bottleneck by leveraging the parallelism framework, which is related to the following seminal works. Towards general needs in handling the high GPU memory usage caused by large-scale inputs, several parallel frameworks have been proposed, such as tensor parallelism \cite{shoeybi2019megatron} and model parallelism \cite{huang2019gpipe}. However, these methods are highly model-dependent and request significant communication overhead \cite{zhuang2023optimizing}. Another direction is to optimize attention mechanisms. Ring attention \cite{liu2023ring}, inspired by FlashAttention \cite{dao2022flashattention}, uses a ring topology between multi-GPUs, achieving quadratic communication complexity with respect to mesh points. Besides, DeepSpeed-Ulysses \cite{jacobs2023deepspeed} splits the deep representation along the channel dimension and employs the All2All communication approach, reducing complexity to linear. Despite these improvements, the communication volume remains excessive. In contrast, Transolver++ leverages its unique physics-learning design and presents a highly optimized parallelism framework tailored to PDE solving, enabling minimal communication overhead and allowing for meshes of million-scale points.