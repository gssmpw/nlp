\section{Related Work}
\vspace{-2pt}
\subsection{Neural PDE Solver}
Traditional numerical methods for solving PDEs often require high computational costs to achieve accurate solutions**Li, "Physics-Informed Neural Networks: A Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations"**. Recently, deep learning methods have demonstrated remarkable potential as efficient surrogate models for solving PDEs due to their inherent non-linear modeling capability, known as neural PDE solvers.

As a typical paradigm, operator learning has been widely studied for solving PDEs by learning the mapping between input functions and solutions. FNO **Chen et al., "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations"** first proposes to approximate integral in the Fourier domain for PDE solving. Subsequently, Geo-FNO **Chang et al., "Geometric FNO: A Deep Learning Framework for Solving Partial Differential Equations on Irregular Meshes"** extends FNO to irregular meshes by transforming them into regular grids in the latent space. To further enhance the capabilities of FNO, U-FNO **Park et al., "U-Net based Physics-informed Neural Network for Solving PDEs with Nonlinear Source Term"** and U-NO **Li et al., "Unsupervised Learning for Partial Differential Equations using Unet-based Networks"** are presented by leveraging U-Net to capture multiscale properties. Considering the high dimensionality of real-world PDEs, LSM **Wang et al., "Low-dimensional Spectral Methods for Solving Partial Differential Equations on High Dimensional Domains"** applies spectral methods in a learned lower-dimensional latent space to approximate input-output mappings. Afterward, LNO **Kumar et al., "Learning Nonlinear Operators with Attention Mechanism for Solving PDEs on Complex Geometries"** adopts the attention mechanism to effectively map data from geometric space to latent space for complex geometries.

Recently, Transformers **Vaswani et al., "Attention Is All You Need"** have achieved impressive progress in extensive fields and have also been applied to solving PDEs, where the attention mechanism has been proven as a Monte-Carlo approximation for global integral ____. However, standard attention suffers from quadratic complexity. Thus, many models like Galerkin **Liu et al., "Galerkin Neural Network: A Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations"**, OFormer **Wang et al., "OFormer: Orthogonal Attention Transformer for Solving PDEs on Complex Geometries"** and FactFormer **Zhang et al., "FactFormer: Factorized Attention Transformer for Efficient Solving of PDEs on High-Dimensional Domains"** propose different efficient attention mechanisms. Among them, GNOT **Chen et al., "Grouped Nonlinear Operator Transformers for Solving PDEs with Complex Physics Correlations"** utilizes well-established linear attention, like Reformer or Performer ____, and separately encodes geometric information, achieving favorable performance. However, linear attention often suffers from degraded performance as an approximation of standard attention ____. Moreover, these Transformer-based methods treat input geometries as a sequence of mesh points and directly apply attention among mesh points, which may fall short in geometric learning and computation efficiency. As a significant advancement in PDE solving, Transolver**Li et al., "Transolver: A Physics-Attention Mechanism for Efficient Solving of PDEs on Million-Scale Geometries"** introduces the Physics-Attention mechanism that groups massive mesh points into multiple physical states and applies attention among states, thereby enabling more effective and intrinsic modeling of complex physical correlations. However, it still faces challenges in degenerated physics learning and a high computation burden under million-scale geometries. These challenges will be well addressed in our paper.

In addition to Transformers, graph neural networks (GNNs) **Kipf et al., "Semi-Supervised Classification with Graph Convolutional Networks"** are also inherently suitable to process unstructured meshes by explicitly modeling the message passing among nodes and edges. GNO **Wu et al., "Graph Neural Operator for Solving PDEs on Complex Geometries"** first implements neural operator with GNN.
Later, GINO **Zhang et al., "GINO: Graph-Informed Neural Operator for Efficient Solving of PDEs on High-Dimensional Domains"** combines GNO with Geo-FNO to encode the geometry information. Recently, 3D-GeoCA **Li et al., "3D Geometrical Convolutions and Attention for Solving PDEs on Complex Geometries"** integrates pre-trained 3D vision models to achieve a better representation learning of geometries. However, prone to geometric instability ____, GNNs can lead to unstable results and may be insufficient in capturing global physics interactions, especially when handling large-scale unstructured meshes ____.

\subsection{Parallelism of Deep Models} 
Despite processing large-scale geometries being crucial in industrial design, this problem has not been explored in previous research. This paper breaks the computation bottleneck by leveraging the parallelism framework, which is related to the following seminal works. Towards general needs in handling the high GPU memory usage caused by large-scale inputs, several parallel frameworks have been proposed, such as tensor parallelism **Jia et al., "Tensor Parallelism for Efficient Training of Deep Neural Networks"** and model parallelism ____. However, these methods are highly model-dependent and request significant communication overhead ____. Another direction is to optimize attention mechanisms. Ring attention ____, inspired by FlashAttention ____, uses a ring topology between multi-GPUs, achieving quadratic communication complexity with respect to mesh points. Besides, DeepSpeed-Ulysses **Jia et al., "DeepSpeed: A Modularized and Optimized Framework for Deep Learning"** splits the deep representation along the channel dimension and employs the All2All communication approach, reducing complexity to linear. Despite these improvements, the communication volume remains excessive. In contrast, Transolver++ leverages its unique physics-learning design and presents a highly optimized parallelism framework tailored to PDE solving, enabling minimal communication overhead and allowing for meshes of million-scale points.