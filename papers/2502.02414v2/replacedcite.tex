\section{Related Work}
\vspace{-2pt}
\subsection{Neural PDE Solver}
Traditional numerical methods for solving PDEs often require high computational costs to achieve accurate solutions____. Recently, deep learning methods have demonstrated remarkable potential as efficient surrogate models for solving PDEs due to their inherent non-linear modeling capability, known as neural PDE solvers.

As a typical paradigm, operator learning has been widely studied for solving PDEs by learning the mapping between input functions and solutions. FNO ____ first proposes to approximate integral in the Fourier domain for PDE solving. Subsequently, Geo-FNO ____ extends FNO to irregular meshes by transforming them into regular grids in the latent space. To further enhance the capabilities of FNO, U-FNO ____ and U-NO ____ are presented by leveraging U-Net ____ to capture multiscale properties. Considering the high dimensionality of real-world PDEs, LSM ____ applies spectral methods in a learned lower-dimensional latent space to approximate input-output mappings. Afterward, LNO ____ adopts the attention mechanism to effectively map data from geometric space to latent space for complex geometries.

Recently, Transformers ____ have achieved impressive progress in extensive fields and have also been applied to solving PDEs, where the attention mechanism has been proven as a Monte-Carlo approximation for global integral ____. However, standard attention suffers from quadratic complexity. Thus, many models like Galerkin ____, OFormer ____ and FactFormer ____ propose different efficient attention mechanisms. Among them, GNOT ____ utilizes well-established linear attention, like Reformer or Performer ____, and separately encodes geometric information, achieving favorable performance. However, linear attention often suffers from degraded performance as an approximation of standard attention ____. Moreover, these Transformer-based methods treat input geometries as a sequence of mesh points and directly apply attention among mesh points, which may fall short in geometric learning and computation efficiency. As a significant advancement in PDE solving, Transolver____ introduces the Physics-Attention mechanism that groups massive mesh points into multiple physical states and applies attention among states, thereby enabling more effective and intrinsic modeling of complex physical correlations. However, it still faces challenges in degenerated physics learning and a high computation burden under million-scale geometries. These challenges will be well addressed in our paper.

In addition to Transformers, graph neural networks (GNNs) ____ are also inherently suitable to process unstructured meshes by explicitly modeling the message passing among nodes and edges. GNO ____ first implements neural operator with GNN.
Later, GINO ____ combines GNO with Geo-FNO to encode the geometry information. Recently, 3D-GeoCA ____ integrates pre-trained 3D vision models to achieve a better representation learning of geometries. However, prone to geometric instability ____, GNNs can lead to unstable results and may be insufficient in capturing global physics interactions, especially when handling large-scale unstructured meshes ____.

\subsection{Parallelism of Deep Models} 
Despite processing large-scale geometries being crucial in industrial design, this problem has not been explored in previous research. This paper breaks the computation bottleneck by leveraging the parallelism framework, which is related to the following seminal works. Towards general needs in handling the high GPU memory usage caused by large-scale inputs, several parallel frameworks have been proposed, such as tensor parallelism ____ and model parallelism ____. However, these methods are highly model-dependent and request significant communication overhead ____. Another direction is to optimize attention mechanisms. Ring attention ____, inspired by FlashAttention ____, uses a ring topology between multi-GPUs, achieving quadratic communication complexity with respect to mesh points. Besides, DeepSpeed-Ulysses ____ splits the deep representation along the channel dimension and employs the All2All communication approach, reducing complexity to linear. Despite these improvements, the communication volume remains excessive. In contrast, Transolver++ leverages its unique physics-learning design and presents a highly optimized parallelism framework tailored to PDE solving, enabling minimal communication overhead and allowing for meshes of million-scale points.