%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{color}
\usepackage{threeparttable}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{bbding}
\usepackage[thicklines]{cancel}
\usepackage[ruled,vlined]{algorithm2e}

\usepackage{multirow}
% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025_arxiv}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Transolver++: An Accurate Neural Solver for PDEs on Million-Scale Geometries}

\begin{document}

\twocolumn[
\icmltitle{Transolver++: An Accurate Neural Solver for PDEs on Million-Scale Geometries}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Huakun Luo}{equal,yyy}
\icmlauthor{Haixu Wu}{equal,yyy}
\icmlauthor{Hang Zhou}{yyy}
\icmlauthor{Lanxiang Xing}{yyy}
\icmlauthor{Yichen Di}{yyy}
\icmlauthor{Jianmin Wang}{yyy}
\icmlauthor{Mingsheng Long}{yyy}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{School of Software, BNRist, Tsinghua University. Huakun Luo $<$luohk24@mails.tsinghua.edu.cn$>$}

\icmlcorrespondingauthor{Mingsheng Long}{mingsheng@tsinghua.edu.cn}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Although deep models have been widely explored in solving partial differential equations (PDEs), previous works are primarily limited to data only with up to tens of thousands of mesh points, far from the million-point scale required by industrial simulations that involve complex geometries. In the spirit of advancing neural PDE solvers to real industrial applications, we present Transolver++, a highly parallel and efficient neural solver that can accurately solve PDEs on million-scale geometries. Building upon previous advancements in solving PDEs by learning physical states via Transolver, Transolver++ is further equipped with an extremely optimized parallelism framework and a local adaptive mechanism to efficiently capture eidetic physical states from massive mesh points, successfully tackling the thorny challenges in computation and physics learning when scaling up input mesh size. Transolver++ increases the single-GPU input capacity to million-scale points for the first time and is capable of continuously scaling input size in linear complexity by increasing GPUs. Experimentally, Transolver++ yields 13\% relative promotion across six standard PDE benchmarks and achieves over 20\% performance gain in million-scale high-fidelity industrial simulations, whose sizes are 100$\times$ larger than previous benchmarks, covering car and 3D aircraft designs.
\end{abstract}

\section{Introduction}
Extensive physics processes can be precisely described by partial differential equations (PDEs) \cite{Wazwaz2002PartialDE,evans2010partial}, such as air dynamics of driving cars or internal stress of buildings. Accurately solving these PDEs is essential to industrial manufacturing \cite{kopriva2009implementing,roubivcek2013nonlinear}. However, it is hard and usually impossible to obtain the analytic solution of PDEs. Thus, numerical methods \cite{solin2005partial} have been widely explored, whose typical solving process is first discretizing PDEs into computation meshes and then approximating solution on discretized meshes \cite{grossmann2007numerical}. In real applications, such as simulating a driving car or aircraft during takeoff, it will take several days or even months for calculation, and the simulation accuracy is highly affected by the fitness of discretized computation mesh \cite{solanki2003finite,elrefaie2024drivaernet++}. To speed up this process, deep models have been explored as efficient surrogates of numerical methods, known as neural PDE solvers~\cite{wang2023scientific}. Training from pre-collected simulation data, neural solvers can learn to approximate the mapping between input and output of numerical methods and directly infer new samples in a flash \cite{li2021fourier}, posing a promising direction for industrial simulation.

Usually, industrial applications involve large and complex geometries, requiring the model to capture intricate physics processes underlying geometries efficiently. Although previous works have made some progress in handling complex geometries \cite{li2023geometry,hao2023gnot} and attempted to speed up model efficiency, they are still far from real applications, especially in handling large geometries. Specifically, as illustrated in Figure~\ref{fig:datasets}(a), existing models fail to scale up beyond 400k points, while real applications typically involves million-scale mesh points or even more. Note that, as aforementioned, the fineness of computation meshes is the foundation of simulation accuracy. As the comparison of car mesh shown in Figure~\ref{fig:datasets}(b), limited mesh size will seriously sacrifice the precision of the geometry, where the originally streamlined surface becomes rough and uneven. This will further limit the simulation accuracy of physics processes, especially for aerodynamic applications~\cite{elrefaie2024drivaernet++}. Thus, \emph{the capability of handling large geometries is indispensable for practical neural PDE solvers.} 

\begin{figure*}[t]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{fig/gpu_memory_short.pdf}}
    \vspace{-5pt}
	\caption{(a) Comparison of model capability in handling large geometries. We plot the GPU memory change of each model when increasing input mesh points. The upper bound on a single A100 40GB GPU is depicted in the dotted line.  (b) Comparison of experiment benchmarks. Transolver++ experiments on high-fidelity tasks with up to 2.5 million points, which is 100$\times$ larger than previous works.}
	\label{fig:datasets}
\end{center}
\vspace{-25pt}
\end{figure*}

\label{tag:transolver}
As the latest progress in neural PDE solver architectures, Transolver~\cite{wu2024Transolver} proposes to learn intrinsic physical states underlying complex geometries and apply the attention mechanism among learned physical states to capture intricate physics interactions, which frees model capacities from unwieldy mesh points and achieves outstanding performance in car and airfoil simulations. Although Transolver demonstrates favorable capability in handling complex geometries, its maximum input size remains limited to 700k points (Figure \ref{fig:datasets}(a)), and its experiment data is much simpler than real applications. Building on the essential step made by Transolver, we attempt to advance it to million-scale or even larger geometries in pursuing practical neural solvers.

When scaling Transolver to million-scale high-fidelity PDE-solving tasks, we observe its bottlenecks in physics learning and computation efficiency. Firstly, as Transolver highly relies on the learned physical states, massive mesh points may overwhelm their learning process, resulting in homogeneous physical states and model degeneration. Secondly, it is observed that even deep representations of million-scale points without considering intermediate calculations will consume considerable GPU memory, which is strictly constrained by total resources on a single GPU. This drives us to elaboratively optimize the model architecture and unlock the power of multi-GPU parallelism. Thus, in this paper, we present \emph{Transolver++}, which upgrades Transolver with an extremely optimized parallelism framework and a local adaptive mechanism to efficiently capture eidetic physical states from massive mesh points. Based on the co-design with model architecture, our parallelism framework significantly reduces the communication overhead and achieves linear scalability with resources without sacrificing performance. As a result, Transolver++ achieves consistent state-of-the-art on six standard PDE datasets and successfully extends Transolver to high-fidelity car and aircraft simulation tasks with million-scale points. Here are our contributions:
\vspace{-5pt}
\begin{itemize}
    \item To ensure reliable modeling for complex physical interactions, we introduce Transolver++ with eidetic states, which can adaptively aggregate information from massive mesh points to distinguishable physical states.
    \item We present an efficient and highly parallel implementation of Transolver++ with linear scalability and minimal overhead across GPUs, affording mesh size of 1.2 million on a single GPU while maintaining accuracy.
    \item Transolver++ achieves a 13\% relative gain averaged from six standard benchmarks and over 20\% improvement on high-fidelity million-scale industrial datasets, covering practical car and 3D aircraft design tasks.
\end{itemize}

\section{Related Work}
\vspace{-2pt}
\subsection{Neural PDE Solver}
Traditional numerical methods for solving PDEs often require high computational costs to achieve accurate solutions~\cite{solanki2003finite}. Recently, deep learning methods have demonstrated remarkable potential as efficient surrogate models for solving PDEs due to their inherent non-linear modeling capability, known as neural PDE solvers.

As a typical paradigm, operator learning has been widely studied for solving PDEs by learning the mapping between input functions and solutions. FNO \cite{li2020neural} first proposes to approximate integral in the Fourier domain for PDE solving. Subsequently, Geo-FNO \cite{li2021fourier} extends FNO to irregular meshes by transforming them into regular grids in the latent space. To further enhance the capabilities of FNO, U-FNO \citet{Wen2021UFNOA} and U-NO \citet{rahman2022u} are presented by leveraging U-Net \cite{ronneberger2015u} to capture multiscale properties. Considering the high dimensionality of real-world PDEs, LSM \cite{wu2023LSM} applies spectral methods in a learned lower-dimensional latent space to approximate input-output mappings. Afterward, LNO \cite{wang2024LNO} adopts the attention mechanism to effectively map data from geometric space to latent space for complex geometries.

Recently, Transformers \cite{NIPS2017_3f5ee243} have achieved impressive progress in extensive fields and have also been applied to solving PDEs, where the attention mechanism has been proven as a Monte-Carlo approximation for global integral \cite{jmlr_operator}. However, standard attention suffers from quadratic complexity. Thus, many models like Galerkin \cite{Cao2021ChooseAT}, OFormer \cite{li2023transformer} and FactFormer \cite{anonymous2023factorized} propose different efficient attention mechanisms. Among them, GNOT \cite{hao2023gnot} utilizes well-established linear attention, like Reformer or Performer \cite{kitaev2020reformer,performer}, and separately encodes geometric information, achieving favorable performance. However, linear attention often suffers from degraded performance as an approximation of standard attention \cite{qin2022devil}. Moreover, these Transformer-based methods treat input geometries as a sequence of mesh points and directly apply attention among mesh points, which may fall short in geometric learning and computation efficiency. As a significant advancement in PDE solving, Transolver~\cite{wu2024Transolver} introduces the Physics-Attention mechanism that groups massive mesh points into multiple physical states and applies attention among states, thereby enabling more effective and intrinsic modeling of complex physical correlations. However, it still faces challenges in degenerated physics learning and a high computation burden under million-scale geometries. These challenges will be well addressed in our paper.

In addition to Transformers, graph neural networks (GNNs) \cite{hamilton2017inductive,gao2019graph, pfaff2021learning} are also inherently suitable to process unstructured meshes by explicitly modeling the message passing among nodes and edges. GNO \cite{Li2020NeuralOG} first implements neural operator with GNN.
Later, GINO \cite{li2023geometryinformed} combines GNO with Geo-FNO to encode the geometry information. Recently, 3D-GeoCA \cite{anonymous2023geometryguided} integrates pre-trained 3D vision models to achieve a better representation learning of geometries. However, prone to geometric instability \cite{instabilityGNN}, GNNs can lead to unstable results and may be insufficient in capturing global physics interactions, especially when handling large-scale unstructured meshes \cite{morris2023geometric}.

\subsection{Parallelism of Deep Models} 
Despite processing large-scale geometries being crucial in industrial design, this problem has not been explored in previous research. This paper breaks the computation bottleneck by leveraging the parallelism framework, which is related to the following seminal works. Towards general needs in handling the high GPU memory usage caused by large-scale inputs, several parallel frameworks have been proposed, such as tensor parallelism \cite{shoeybi2019megatron} and model parallelism \cite{huang2019gpipe}. However, these methods are highly model-dependent and request significant communication overhead \cite{zhuang2023optimizing}. Another direction is to optimize attention mechanisms. Ring attention \cite{liu2023ring}, inspired by FlashAttention \cite{dao2022flashattention}, uses a ring topology between multi-GPUs, achieving quadratic communication complexity with respect to mesh points. Besides, DeepSpeed-Ulysses \cite{jacobs2023deepspeed} splits the deep representation along the channel dimension and employs the All2All communication approach, reducing complexity to linear. Despite these improvements, the communication volume remains excessive. In contrast, Transolver++ leverages its unique physics-learning design and presents a highly optimized parallelism framework tailored to PDE solving, enabling minimal communication overhead and allowing for meshes of million-scale points.

\section{Revisiting Transolver}
\begin{figure*}[t]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{fig/eidetic_state.pdf}}
    \vspace{-5pt}
	\caption{(a) Overall design of Transolver++ block. Blocks highlighted in red represent modifications compared to the original Transolver. (b) Visualization of slice weights. Transolver++ learns more diverse and eidetic physical states. (c) Visualizations of physical quantity change ratio (difference between each point and its neighbors) and slice weights learned by models. The lighter color means faster change.}
	\label{fig:temperature}
\end{center}
\vspace{-25pt}
\end{figure*}

Before Transolver++, we would like to briefly introduce Physics-Attention, the key design of Transolver \cite{wu2024Transolver}, and discuss its scaling issues for million-scale inputs.

Given meshes with $N$ nodes, to capture intrinsic physics interactions under unwieldy mesh points, Physics-Attention will first assign $C$-channel input points $\mathbf{x}=\{\mathbf{x}_i\}_{i=1}^N\in\mathbb{R}^{N\times C}$ to $M$ \emph{physical states} $\mathbf{s}=\{\mathbf{s}_j\}_{j=1}^M\in\mathbb{R}^{M\times C}$ based on the slice weights $\mathbf{w}=\{\mathbf{w}_i\}_{i=1}^N\in\mathbb{R}^{N\times M}$ learned from inputs, and each $\mathbf{w}_i\in\mathbb{R}^{1\times M}$ represents the possibility that $\mathbf{x}_i$ belongs to each state. Specifically, physical states are aggregated from all point representations based on learned slice weights, which can be formalized as:
\begin{equation}\label{equ:state}
    \begin{aligned}
        &\text{Slice weights:} \ \mathbf{w} = \operatorname{Softmax}\left(\operatorname{Linear}(\mathbf{x})/\tau_0\right) \\
        &\text{Physical states:} \ \{\mathbf{s}_j\}_{j=1}^M = \left\{\frac{\sum_{i=1}^N \mathbf{w}_{ij} \mathbf{x}_i}{\sum_{i=1}^N \mathbf{w}_{ij}}\right\}_{j=1}^M,
    \end{aligned}
\end{equation}
where $\tau_0$ is the temperature constant. Next, the canonical attention mechanism is applied to learned physical states to capture underlying physics interactions:
\begin{equation}\label{equ:attention}
\begin{split}
        \mathbf{q},\mathbf{k},\mathbf{v} = \operatorname{Linear}(\mathbf{s}),\ 
    \mathbf{s}^\prime = \operatorname{Softmax}\left(\frac{\mathbf{q}\mathbf{k}^\top}{\sqrt{C}}\right)\mathbf{v}.
\end{split}
\end{equation}
Finally, Physics-Attention employs the \emph{deslice} operation to map updated states $\mathbf{s}^\prime$ back to mesh space using slice weights, i.e.~$\mathbf{x}^\prime=\{\mathbf{x}_i^\prime\}_{i=1}^N = \{\sum_{j=1}^M \mathbf{w}_{ij} \mathbf{s}_j^\prime\}_{i=1}^N$. By replacing standard attention in Transformer \cite{NIPS2017_3f5ee243} with Physics-Attention, we can obtain the Transolver.

Although Transolver successfully reduces canonical computation complexity from $\mathcal{O}(N^2)$ to $\mathcal{O}(M^2)$ by learning physical states ($M$ is a constant and usually set as 32 or 64 in practice), Transolver still face the following challenges when scaling input size to million-scale, i.e.~$N \ge 10^6$.

\vspace{-5pt}
\paragraph{Homogeneous physical states} Eq.~\eqref{equ:state} shows that physical states are highly affected by slice weights $\mathbf{w}$. If $\mathbf{w}$ tends to be uniform, attention in Eq.~\eqref{equ:attention} will degenerate to average pooling, losing the physics modeling capability. As shown in Figure~\ref{fig:temperature}(b), we find that Transolver may generate less distinguishable weights in some cases, especially in large-scale meshes, leading to homogeneous physical states.

\vspace{-5pt}
\paragraph{Efficiency bottleneck} Although Physics-Attention cost is nearly constant when scaling the input, the feedforward layer to embed million-scale points by $\mathbf{x}_i$ will consume huge GPU memory, forming Transolver's stability bottleneck.

\section{Transolver++}

To tackle the scaling issues of Transolver in physics learning and computation efficiency, we present Transolver++, which can effectively avoid attention degeneration by learning eidetic physical states and successfully break the efficiency bottleneck with a highly optimized parallelism framework.

\subsection{Physics-Attention with Eidetic States}
As aforementioned, if Physics-Attention is based on homogeneous physical states, it will degenerate to average pooling, which will damage the model's performance. Thus, as the authors mentioned in Transolver \cite{wu2024Transolver}, they adopt the softmax function in calculating slice weights in Eq.~\eqref{equ:state}, which can alleviate the indistinguishable states to some extent by guaranteeing peakier distribution \cite{wu2022flowformer}. However, we still observe the degeneration phenomenon when the model depth increases, as shown in Figure \ref{fig:temperature}(b), which may result from the excessive focus on global information. Note that in industrial applications, the model's capability of learning subtle physics is essential, which may highly affect the evaluation in industrial design~\cite{elrefaie2024drivaernet++}, as a small diversion device can drastically change the wind drag of driving cars.

To capture detailed physics phenomena, we propose to learn eidetic states by augmenting Transolver's state learning with a local-adaptive mechanism and slice reparameterization to carefully control the learned slice weight distribution. 

\begin{figure*}[t]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{fig/distributed.pdf}}
    \vspace{-8pt}
	\caption{(a) Comparison with other parallel methods. Tailored to the unique physics learning design, our method only communicates physical states with an all-reduce operation. (b) Scalability of the communication overhead to the number of mesh points with 32 GPUs. Our parallel method stands out by only transferring 0.25MB of data, which does not scale with the size of input mesh points. }
	\label{fig:distribute}
\end{center}
\vspace{-22pt}
\end{figure*}

\vspace{-5pt}
\paragraph{Local adaptive mechanism}
As shown in Figure \ref{fig:temperature}(b), modeling the distribution of each mesh point using a non-parametric approach has been proven to be impractical \cite{ye2024differentialtransformer}. Thus, we introduce a local adaptive mechanism that incorporates local information as a pointwise adjustment to the physics learning process. Specifically, we change the sharpness of the state distribution by learning to adjust the temperature $\tau_0$ in the Softmax function, namely
\vspace{-5pt}
\begin{equation}\label{equ:ada-temp}
    \text{Ada-Temp:}\ \tau=\left\{\tau_i\right\}_{i=1}^N=\left\{\tau_0 + \operatorname{Linear}(\mathbf{x}_i)\right\}_{i=1}^N,
\end{equation}
where $\tau\in\mathbb{R}^{N\times 1}$ and a higher temperature forms a more uniform distribution, while a lower temperature makes the distribution more concentrated on crucial states. Through a learnable linear projection layer, we can dynamically adjust the state distribution based on each point's local properties. 

\vspace{-5pt}
\paragraph{Slice reparameterization}
As mentioned above, we try to learn eidetic physical states and assign mesh points to physical states with a possibility $\mathbf{w}$. In the canonical design of Transolver, it uses $\operatorname{Softmax}$ to form a categorical distribution across states. However, simply generating a categorical distribution is not enough, as we have not completely modeled the assignment process from points to certain physical states. Considering that direct sampling via $\operatorname{Argmax}$ is non-differentiable, we propose to adopt the Gumbel-Softmax \cite{jang2017categorical} to perform differentiable sampling from the discrete categorical distribution, which is accomplished with the following reparameterization technique:
\begin{equation}\label{equ:Repslice}
    \text{Rep-Slice}(\mathbf{x}, {\mathbf{\tau}}) = \operatorname{Softmax}\left(\frac{\operatorname{Linear}(\mathbf{x}) - \log (-\log \mathbf{\epsilon})}{\tau}\right),
\end{equation}
where $\mathbf{\tau}\in\mathbb{R}^{N\times 1}$ is the local adaptive temperature and $\epsilon=\{\epsilon_i\}_{i=1}^N, \epsilon_i \sim \mathcal{U}(0, 1)$. Here $\log (-\log \epsilon_i)\sim \text{Gumbel}(0, 1)$, where $\operatorname{Gumbel}$ is a type of generalized extreme value distribution. Replacing Transolver's slice weights $\mathbf{w}$ in Eq.~\eqref{equ:state}  by our new design in Eq.~\eqref{equ:Repslice}, Transolver++ is able to learn eidetic physical states under complex geometries, offering the possibility to handle much larger-scale datasets.

As shown in Figure~\ref{fig:temperature}(c), the slice weights in Transolver++ can perfectly adapt to the intricate physics fields on complex geometries. Specifically, regions with slow-changing physics quantities are assigned to one certain eidetic state as these areas are governed by one pure physical state. In contrast, regions with fast-changing physics quantities exhibit a multimode distribution across physics states, reflecting that these areas are influenced by a mixture of multiple states.

\subsection{Parallel Transolver++}
To break the GPU memory bottleneck caused by feedforward layers of million-scale point representations, we design a highly optimized framework for PDE solving, which is based on the unique physics-learning design of Transolver.

\vspace{-5pt}
\paragraph{Parallelism formulation} 
Through careful analysis, we observe that in addition to the point-wise feedforward layer, the computation of eidetic states within Physics-Attention can be efficiently distributed across multiple GPUs. Specifically, operations in Eq.~\eqref{equ:state}, such as the weighted sum $\sum_{i=1}^N \mathbf{w}_{ij} \mathbf{x}_i$ and the normalization denominator $\sum_{i=1}^N \mathbf{w}_{ij}$, can be easily dispatched to multiple GPUs and calculated separately in parallel. Thus, we first separate the input mesh into multiple GPUs for parallel computing and only communicate when calculating attention among eidetic states. 

Suppose that the initial separation splits input mesh into \#gpu GPUs and the integral representation $\mathbf{x}\in\mathbb{R}^{N\times C}$ is split into $\{\mathbf{x}^{(1)}, \cdots,\mathbf{x}^{(\#\text{gpu})}\}$, where $\mathbf{x}^{(k)}\in\mathbb{R}^{N_{k}\times C}$ and $N_{k}$ denotes the number of mesh points dispatched to the $k$-th GPU. Correspondingly, the point-wise slice weights $\mathbf{w}\in\mathbb{R}^{N\times M}$ is also separated into \#gpu components $\{\mathbf{w}^{(1)}, \cdots,\mathbf{w}^{(\#\text{gpu})}\}$ with $\mathbf{w}^{(k)}\in\mathbb{R}^{N_{k}\times M}$.
For clarity, we treat both multi-node-multi-GPU and single-node-multi-GPU configurations as a unified case. The calculation of eidetic states $\mathbf{s}_j$ in Eq.~\eqref{equ:state} can be equivalently rewritten into the following parallelism formalization:
\begin{equation}
    \mathbf{s}_j = \frac{\sum_{i=1}^{N_{1}}  \mathbf{w}_{ij}^{(1)}\mathbf{x}_i^{(1)} \oplus \cdots \oplus \sum_{i=1}^{N_{\#\text{gpu}}}\mathbf{w}_{ij}^{(\#\text{gpu})}\mathbf{x}_i^{(\#\text{gpu})}}{\sum_{i=1}^{N_{1}}  \mathbf{w}_{ij}^{(1)} \oplus \cdots \oplus \sum_{i=1}^{N_{\#\text{gpu}}}\mathbf{w}_{ij}^{(\#\text{gpu})}}
\end{equation}
where $\oplus$ denotes the AllReduce operation \cite{patarasuk2009bandwidth}, which aggregates the results from all processes. In practice, the $k$-th GPU first independently computes its partial sums for the numerator $(\sum_{i=1}^{N_{k}}  \mathbf{w}_{ij}^{(k)}\mathbf{x}_i^{(k)})$ and denominator $(\sum_{i=1}^{N_{k}}  \mathbf{w}_{ij}^{(k)})$ of $N_k$ points. Next, these partial results are synchronized across GPUs to compute the eidetic states $\mathbf{s}_j$, highlighted by \textcolor{blue}{blue}-marked steps in Algorithm~\ref{alg:eidetic_physics_attention_layers}.

\vspace{-5pt}
\paragraph{Overhead analysis} 
Consider the input $\mathbf{x}\in\mathbb{R}^{N \times C} $ with $N$ mesh points and $C$ channels. To handle large model parameters in linear layers, tensor parallelism partitions the model parameters along the channel dimension. It reduces the memory consumption linearly but introduces an increase in communication overhead of $O(N)$. Attention-optimized methods like RingAttention leverage the FlashAttention concept to distribute the outer loop using a ring topology, which results in $O(N^2)$ communication complexity, while DeepSpeed-Ulysses, similar to tensor parallelism, partitions the data along feature dimensions and yields an $O(N)$ communication volume. However, all of the above methods are not feasible when handling million-scale meshes as the communication overhead shown in Figure \ref{fig:distribute}(b) is unacceptable.

In parallel Transolver++, each GPU computes two partial sums of size $\mathcal{O}(MC)$ and $\mathcal{O}(M)$ separately, which are then synchronized across GPUs and cause a total communication volume of  $\mathcal{O}(\#\text{gpu} \times M (C+1))$, invariant to input size.
\begin{figure}[t]
\vspace{-10pt}
\begin{algorithm}[H]
\caption{Parallel Physics-Attention with Eidetic States}
    \begin{algorithmic}
    \label{alg:eidetic_physics_attention_layers}
    \STATE \textbf{Input:} Input features $\mathbf{x}^{(k)}\in\mathbb{R}^{N_k\times C}$ on the $k$-th GPU.
    \STATE \textbf{Output:} Updated output features $\mathbf{x}'^{(k)}\in\mathbb{R}^{N_k\times C}$.
    
    \STATE {\color{gray}// drop $\mathbf{f}$ to save 50\% memory.}
    \STATE Compute ${\color{gray}\cancel{\mathbf{f}^{(k)}}}, \mathbf{x}^{(k)} \gets \text{Project}(\mathbf{x}^{(k)})$
    \STATE Compute $\mathbf{\tau}^{(k)} \gets \tau_0 + {\color{blue}\text{Ada-Temp}(\mathbf{x}^{(k)})}$
    \STATE Compute weights $\mathbf{w}^{(k)} \gets {\color{blue}\text{Rep-Slice}(\mathbf{x}^{(k)}, \mathbf{\tau}^{(k)})}$
    \STATE Compute weights norm $\mathbf{w}_{\text{norm}}^{(k)}\gets \sum_{i=1}^{N_k}\mathbf{w}_{i}^{(k)}$
    \STATE {\color{blue}Reduce slice norm $\mathbf{w}_{\text{norm}} \gets \text{AllReduce}(\mathbf{w}_{\text{norm}}^{(k)})$}\ \ {\scriptsize\color{gray}$\mathcal{O}(M)$}
    \STATE Compute eidetic states $\mathbf{s}^{(k)} \gets \frac{\mathbf{w}^{(k)\sf T}{\color{blue}\mathbf{x}^{(k)}} {\color{gray}\cancel{\mathbf{f}^{(k)}}}}{\mathbf{w}_{\text{norm}}}$
    \STATE {\color{blue} Reduce eidetic states $\mathbf{s} \gets \text{AllReduce}(\mathbf{s}^{(k)})$}\quad\quad {\scriptsize\color{gray}$\mathcal{O}(MC)$}
    \STATE Update eidetic states $\mathbf{s}' \gets \text{Attention}(\mathbf{s})$
    \STATE Deslice back to $\mathbf{x}'^{(k)} \gets \text{Deslice}(\mathbf{s}', \mathbf{w}^{(k)})$
    
    \STATE \textbf{Return} $\mathbf{x}'^{(k)}$
    \end{algorithmic}
\end{algorithm}
\vspace{-23pt}
\end{figure}

\begin{figure*}[t]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{fig/standard.pdf}}
    \vspace{-5pt}
	\caption{(a) Visualization of standard benchmarks covering a wide range of physics scenarios, from solid physics to fluid dynamics. (b) Relative errors on standard benchmarks of the top-4 models selected based on overall performance. Full results can be found in Table~\ref{tab:mainres_standard}.}
	\label{fig:standard}
\end{center}
\vspace{-25pt}
\end{figure*}
\vspace{-5pt}
\paragraph{Further speedup} Also, we found that Transolver's code implementation is somewhat over-parameterized. Specifically, Transolver projects the data onto both $\mathbf{x}$ and $\mathbf{f}$, where $\mathbf{x}$ is used to generate slice weights, and $\mathbf{f}$ is combined with weights to generate physical states. This repetitive design brings double memory costs. In this paper, we find that eliminating $\mathbf{f}$ (marked \textcolor{gray}{gray} in Algorithm~\ref{alg:eidetic_physics_attention_layers}) can simply and successfully increase the single-GPU input point capacity to 1.2 million, without sacrificing the model's performance.

\section{Experiments}
We extensively evaluate Transolver++ on six standard benchmarks and two industrial-level datasets with million-scale meshes, covering both physics and design-oriented metrics.

\vspace{-5pt}
\paragraph{Benchmarks}
As summarized in Table~\ref{table:BenchmarkType}, our experiments include both standard benchmarks and industrial simulations, which cover a broad range of mesh sizes. Specifically, the standard benchmarks include Elasticity, Plasticity, Airfoil, Pipe, NS2d, and Darcy, which are widely used in previous studies \cite{wu2024Transolver}. To further evaluate the model's efficacy in real applications, we also perform experiments on industrial design tasks, where we utilized DrivAerNet++ \citep{elrefaie2024drivaernet++} for car design and a newly simulated AirCraft dataset for 3D aircraft design. In addition to the error of predicted physics fields, we also measure the model performance for design by calculating drag and lift coefficients from predicted physics fields.

\begin{table}[h]
\vspace{-10pt}
\caption{Summary of experimental datasets, where \#Mesh denotes the size of mesh points in each sample.}
\vspace{5pt}
\centering
\label{table:BenchmarkType}
\begin{small}
\setlength{\tabcolsep}{4pt}
\begin{sc}
\begin{tabular}{l|c|c|c}
\toprule
Type & Benchmarks & Geo\ Type & \#Mesh\\
\midrule
& NS2d & Structure & 4,096 \\ 
 & Pipe& Structure& 16,641 \\
Standard & Darcy &Structure& 7,225 \\ 
Benchmarks & Airfoil&Structure & 11,271 \\ 
  & Plasticity&Structure & 3,131 \\ 
  & Elasticity&unstructure & 972 \\ 
\midrule
Industrial & AirCraft & \multirow{2}{*}{unstructure} & $\sim$300k \\ 
Applications & DrivAerNet++&   & $\sim$2.5M \\ 
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\vspace{-5pt}
\end{table}

\vspace{-5pt}
\paragraph{Baselines}
We widely compare Transolver++ against more than 20 advanced baselines, covering various types of approaches. These baselines include 12 neural operators, such as Galerkin \citeyearpar{Cao2021ChooseAT}, LNO \citeyearpar{wang2024LNO}, and GINO \citeyearpar{li2023geometryinformed}, some of which are specifically designed for irregular meshes; 4 Transformer-based PDE solvers, including OFormer \citeyearpar{li2023transformer}, FactFormer \citeyearpar{li2023scalable}, GNOT \citeyearpar{hao2023gnot}, and Transolver \citeyearpar{wu2024Transolver}; and 4 graph-neural-network-based methods: GraphSAGE \citeyearpar{hamilton2017inductive}, PointNet \citeyearpar{qi2017pointnet}, Graph U-Net \citeyearpar{gao2019graph}, and MeshGraphNet \citeyearpar{pfaff2021learning}. Transolver is the previous state-of-the-art model. During experiments, we found that some neural operators designed for grids perform poorly for large-scale irregular meshes. Therefore, we only report their performance on standard benchmark datasets.

\vspace{-5pt}
\subsection{Standard Benchmarks}
\paragraph{Setups} As shown in Figure~\ref{fig:standard}(a), we compare the latest state-of-the-art neural PDE solvers with Transolver++ on six standard datasets covering a wide range of physics scenarios. For a fair comparison, we keep all model parameters within a fixed range. Specifically, in Transolver++, we set the model's depth as eight layers, and the feature dimension is 128 or 256, depending on the scale of the data. The number of slices is chosen from \{32, 64\} to trade-off between computational cost and model performance.

\vspace{-5pt}
\paragraph{Results} As presented in Figure~\ref{fig:standard}(b), Transolver++ yields over 13\% improvement w.r.t.~the dataset-specific second-best baseline averaged from all six standard benchmarks, showing the efficacy of our proposed methods in handling complex geometries. To highlight the comparison, we only present top-3 baselines and Transolver++ in Figure~\ref{fig:standard}, which are chosen based on the overall performance. Full results for other baselines are provided in Table~\ref{tab:mainres_standard} of the Appendix.

It is worth noticing that Transolver and Transolver++ significantly outperform other models in Elasticity, whose geometry is recorded as an unstructured point cloud. Going beyond Transolver, Transolver++ further boosts performance by learning eidetic physical states. As shown in Figure \ref{fig:temperature}, Transolver++ can learn more diverse slice partitioning, thereby enabling more accurate physics learning. More analyses on learned physical states can also be found in Appendix~\ref{appdix:vis}.

\begin{table*}[t]
	\caption{Comparison on large geometries benchmarks. Relative L2 of the surrounding area (\emph{Volume}) and surface (\emph{Surf}) physics as well as drag and lift coefficient ($C_D$, $C_{L}$) is recorded, along with their coefficient of determination $R^2_{D}$ and $R^2_{L}$. The closer $R^2$ is to 1, the better.}
	\label{tab:mainres_large_geometries}
	\vspace{-5pt}
	\vskip 0.15in
	\centering
	\begin{small}
		\begin{sc}
			\renewcommand{\multirowsetup}{\centering}
			\setlength{\tabcolsep}{9.5pt}
			\begin{tabular}{l|cccccccc}
				\toprule
                    \multirow{2}{*}{Model}  & \multicolumn{2}{c}{DrivAernet++ Full} & \multicolumn{3}{c}{DrivAernet++ Surf} & \multicolumn{3}{c}{AirCraft} \\
                    \cmidrule(lr){2-3} \cmidrule(lr){4-6} \cmidrule(lr){7-9}
				& Volume $\downarrow$ & Surf $\downarrow$ &$C_D$ $\downarrow$& {$R^2_L$} $\uparrow$ & Surf $\downarrow$ & $C_L$ $\downarrow$ & {$R^2_L$} $\uparrow$& Surf $\downarrow$ \\
				\midrule
                    GraphSAGE \citeyearpar{hamilton2017inductive} & 0.328 & 0.284 & 0.282& 0.859 & 0.294& 0.040 & 0.988& 0.109\\
                    PointNet \citeyearpar{qi2017pointnet} &0.285 &0.478 &0.301 &0.831 &0.237 & 0.095& 0.982& 0.169\\
                    Graph U-Net$^\ast$ \citeyearpar{gao2019graph} & 0.241&0.260 &0.272 & 0.876&0.193 &0.063& 0.953 & 0.161\\
                    MeshGraphNet$^\ast$ \citeyearpar{pfaff2021learning} & 0.529 &0.422 & 0.260& 0.870&0.209 & 0.038& 0.993&0.113\\
                    \midrule
                    GNO$^\ast$ \citeyearpar{li2020neural} & 0.510 & 0.664 & 0.252& 0.882&0.196 &0.031 & 0.991 & 0.129\\
                    Galerkin$^\ast$ \citeyearpar{Cao2021ChooseAT} & 0.234&0.274 & 0.267& 0.792& 0.235 & 0.069 & 0.879& 0.118\\
                    Geo-FNO$^\ast$ \citeyearpar{Li2022FourierNO}& 0.718 & 0.892& 0.288& 0.831& 0.291 & 0.243& 0.903 & 0.395 \\
                    GINO \citeyearpar{li2023geometryinformed} & 0.586& 0.638& 0.323& 0.725& 0.220 &0.047 & 0.983 & 0.133 \\
                    GNOT$^\ast$ \citeyearpar{hao2023gnot} & 0.174& 0.171 & 0.158&0.901 & 0.167 & 0.033&0.991 & 0.093 \\
                    LNO$^\ast$ \citeyearpar{wang2024LNO} &0.180 & 0.203 &0.208&0.855 & 0.195 &0.091 & 0.992 & 0.137 \\ 
                    3D-GeoCA$^\ast$ \citeyearpar{anonymous2023geometryguided} & 0.389& 0.224& 0.205 & 0.883 &0.175 &\underline{0.022} & 0.993  & 0.097\\
                    Transolver$^\ast$ \citeyearpar{wu2024Transolver} &\underline{0.173} &\underline{0.167} & \underline{0.061}&\underline{0.931}&\underline{0.145} & 0.037& \underline{0.994}& \underline{0.092}\\
                \midrule
                    \multicolumn{1}{l}{\textbf{Transolver++ (Ours)}} &\textbf{0.154} &\textbf{0.146} & \textbf{0.036}& \textbf{0.997} & \textbf{0.110}& \textbf{0.014}& \textbf{0.999} & \textbf{0.064} \\
                    \multicolumn{1}{l}{Relative Promotion} & 11.0\% & 12.6\% & 41.0\% & - & 24.1\% & 36.3\% & - & 30.4\% \\
				\bottomrule
			\end{tabular}
		\end{sc}
                \begin{tablenotes}
      \footnotesize
      \item[] $\ast$ These models cannot directly handle million-scale meshes as the model input. Thus, to enable comparison, we split the input mesh of these models into several pieces, independently infer them, and concatenate separately inferred outputs as their final results.
      \end{tablenotes}
	\end{small}
    \vspace{-10pt}
\end{table*}


\subsection{PDEs on Large Geometries}
\paragraph{Setups} To evaluate the performance in practical applications, we conduct experiments on two industrial datasets with million-scale meshes: DrivAerNet++ \cite{elrefaie2024drivaernet++} and Aircraft. The latter is newly presented by us, which is of high quality and simulated by aerodynamicists. To fully evaluate the model performance under different scales, we split DrivAerNet++ into two scales, one only with surface pressure ($\sim$700k mesh points in each sample) and the other with full physics fields (2.5M mesh points).

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fig/datasets.pdf}
    \vspace{-15pt}
    \caption{Car and aircraft design to predict drag and lift coefficient under extremely complex geometries with million-scale meshes.}
    \vspace{-10pt}
    \label{fig:showcase_meshes}
\end{figure}
\vspace{-5pt}
\paragraph{Results} Table~\ref{tab:mainres_large_geometries} demonstrates that Transolver++ achieved consistent state-of-the-art in all datasets with an average promotion of over 20\%. In DrivAerNet++, our model is capable of handling 2.5 million meshes within 4 A100 GPUs and surpassing all other models by relative promotion of 11.0\% and 12.6\% on volume and surface field separately. In the surface field, our model still shows a prominent lead of 24.1\% in DrivAerNet++ Surface and 30.4\% in AirCraft. 

We also observe that GNNs often degenerate quickly when handling large-scale meshes due to their geometric instability \cite{instabilityGNN}. Additionally, Geo-FNO also performs poorly across nearly all datasets, as it attempts to map irregular meshes to uniform latent grids, a task that becomes especially difficult when the number of meshes scales up. These failures of our baselines further highlight the challenge of physics learning on million-scale geometries, while Transolver++ provides a practical solution, advancing an essential step to industrial applications.

As shown in Figure~\ref{fig:slice_visualization}(b), our model has a lower relative error in most cases and excels in capturing the intrinsic physics variations in those regions with drastic changes, while other models tend to generate an over-smooth prediction, validating the effectiveness of learning eidetic states.

\begin{table}[t]
    \centering
    \caption{Ablations on AirCraft. Relative L2 and $R^2$ of lift coefficient are both recorded. \emph{Ada-Temp} refers to Eq.~\eqref{equ:ada-temp}, \emph{Reparameter} is for Eq.~\eqref{equ:Repslice} and \emph{Speedup} represents removing $\mathbf{f}$ in Algorithm \ref{alg:eidetic_physics_attention_layers}.}
    \vspace{5pt}
    \begin{small}
    \begin{sc}
    \begin{tabular}{l|ccc}
    \toprule
       \multirow{2}{*}{Method} & \multicolumn{3}{c}{AirCraft} \\ & $C_L\downarrow$ & $R^2_L\uparrow$ & Surf$\downarrow$ \\
    \midrule
       Transolver & 0.037 & 0.994 & 0.092 \\
       + Ada-Temp & 0.020 & 0.995 & 0.080\\
       + Ada-Temp, Speedup & 0.018 & 0.995 & 0.075\\
       + Ada-Temp, Reparameter & 0.016 & 0.998 & 0.069\\
    \midrule
        \textbf{Transovler++ (Ours)} & \textbf{0.014} & \textbf{0.999} & \textbf{0.064} \\
    \bottomrule
    \end{tabular}
    \end{sc}
    \vspace{-10pt}
    \label{tab:ablations}
    \end{small}
\end{table}

\begin{figure*}[t]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{fig/slice_visualization.pdf}}
    \vspace{-5pt}
	\caption{(a) Visualization of slice weight distributions. Transolver++ demonstrates a more diverse pattern than Transolver. (b) Error map of top-3 models on DrivAerNet++ and AirCraft. Transolver++ outperforms other models and captures some subtle variations. (c) Statistics analysis of efficiency in terms of running time and model performance under different scales of input geometries.}
	\label{fig:slice_visualization}
\end{center}
\vspace{-30pt}
\end{figure*}

\subsection{Model Analysis}
In addition to model comparisons, we also conduct a series of analysis experiments to provide an in-depth understanding of our model and the necessity of large geometries.

\vspace{-5pt}
\paragraph{Ablations}
We conducted elaborative ablations on every component of Transolver++. As shown in Table~\ref{tab:ablations}, introducing the local adaptive mechanism significantly improves performance, which validates our motivation to learn eidetic states. With speed-up optimization and reparameterization techniques, the model achieves its best performance, fully demonstrating the effectiveness of our proposed design.

\begin{table}[t]
    \centering
    \caption{KL-divergence between learned slice weights and uniform distribution in different layers on Elasticity. We choose \{1, 3, 5, 7\} layers and a higher value means a more diverse distribution.}
    \vspace{10pt}
    \renewcommand{\multirowsetup}{\centering}
    \setlength{\tabcolsep}{5pt} 
    \renewcommand{\arraystretch}{1.0} 
    \begin{footnotesize} 
    \begin{small}
    \begin{sc}
    \begin{tabular}{c|cccc|c}
    \toprule
       \multirow{2}{*}{Model} & \multicolumn{4}{c|}{Layer Number} & \multirow{2}{*}{Avg} \\ 
                            & 1 & 3 & 5 & 7 & \\ 
    \midrule
    \renewcommand{\arraystretch}{1.1} 
       Transolver & 0.056 & 5.599 & 3.474 & 0.383 & 3.885 \\
       Transolver++ & \textbf{2.122} & \textbf{8.193} & \textbf{5.583} & \textbf{1.473} & \textbf{5.297}\\ 
    \bottomrule
    \end{tabular}
    \end{sc}  
    \end{small}
    \end{footnotesize}
    \vspace{-10pt}
    \label{tab:kl_divergence}
\end{table}

\vspace{-5pt}
\paragraph{Slice Analysis}
As shown in Figure~\ref{fig:slice_visualization}(a),  Transolver++ can extract more diverse physical states than Transolver in million-scale meshes, enabling fine modeling for complex physics fields of driving cars. More visualization can be found in Appendix~\ref{appdix:vis}. Moreover, in Table \ref{tab:kl_divergence}, we also calculate the KL divergence between learned slice weights and uniform distribution in different layers. These statistical results further demonstrate that Transolver++ can learn more diverse and varying slice distribution across all the layers.

\vspace{-5pt}
\paragraph{Efficiency analysis}
In addition to GPU memory (Figure~\ref{tag:transolver}), we also measured the running time of different models on DrivAerNet++ datasets in Figure~\ref{fig:slice_visualization} (c.1). To ensure a fair comparison, all models' parameter sizes are well aligned in our experiments and we only compare Transolver++ with Transformer-based methods here, since GNNs struggle to handle million-scale meshes. We can find that Transolver++ strikes a favorable balance between performance and efficiency, outperforming most models in terms of speed. Moreover, at the same size of input mesh points, our method exhibits the lowest memory usage, highlighting its efficiency in handling large-scale data without sacrificing accuracy.

\vspace{-5pt}
\paragraph{Why we need large geometries}
From the car mesh comparison in Figure~\ref{fig:datasets}(b), we can observe that large geometries differ significantly from smaller-scale ones in terms of details, which directly affects the accuracy of the simulation. While previous models \cite{li2021fourier} claim to be resolution-invariant and aim to apply directly to large-scale geometries, our experiments in Figure~\ref{fig:slice_visualization} (c.2) show that without training on large meshes, the model will fall short in the fine-grained physics modeling, resulting in an insufficient and limited performance, which further necessitates the capability of handling larger geometries.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fig/scalability.pdf}
    \vspace{-20pt}
    \caption{Evaluation of the model scalability in terms of data size and parameter size. Our default setting is 150 cases and 4 layers.}
    \vspace{-10pt}
    \label{fig:scalability}
\end{figure}

\vspace{-5pt}
\paragraph{Scalability}
We evaluate the scalability of Transolver++ across different numbers of training samples and model parameters of different sizes by altering the number of layers. From Figure~\ref{fig:scalability}, we can find that our model can consistently benefit from large data and large models, revealing its potential to be the backbone of PDE-solving foundation models.

\section{Conclusion}
In pursuit of practical neural solvers, this paper presents Transolver++, which enables the first success in accurately solving PDEs discretized on million-scale geometries. Specifically, we upgrade the vanilla Transolver by introducing eidetic states and a highly optimized parallel framework, empowering Transolver++ with better physics learning and computation efficiency. As a result, our model achieves significant advancement in industrial design tasks, demonstrating favorable efficiency and scalability, which can serve as a neat backbone of PDE-solving foundation models.

% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Full Results on Standard Benchmarks}
Due to the space limitation of the main text, we present the full results of model performance on standard benchmarks here, as a supplement to Figure \ref{fig:standard}. As shown in Table \ref{tab:mainres_standard}, Transolver++ demonstrates superior performance across six standard PDE benchmarks, achieving the lowest relative L2 error in all PDE-solving tasks with an averaged relative promotion of 13\%.

For models marked with an asterisk (*), we carefully reproduced the results by running the models more than three times and enabled fair and reliable comparison by maintaining the model parameter counts within a closely comparable range with other models. Specifically, we adjust LNO and reduce its default dimension from 256 to 192 in elasticity, as its parameter count is already three times larger than the other largest models. As discussed in previous studies \cite{wu2024Transolver}, Transformer-based models usually benefit from large parameter sizes. This approach minimizes the potential influence of parameter variations and highlights the performance improvements achieved by our method under similar computation costs.

\begin{table*}[h]
\vspace{-10pt}
	\caption{Model performance on six standard PDE benchmarks is evaluated using the relative L2 error. The result marked with an asterisk (*) indicates a reproduced outcome, where the parameter counts and configurations of the baseline methods are carefully aligned to ensure a fair comparison. ``/'' means that the baseline cannot apply to this benchmark.}
	\label{tab:mainres_standard}
	\vspace{-5pt}
	\vskip 0.15in
	\centering
	\begin{small}
		\begin{sc}
			\renewcommand{\multirowsetup}{\centering}
			\setlength{\tabcolsep}{12.8pt}
			\begin{tabular}{l|cccccc}
				\toprule
                    \multirow{2}{*}{Model}  & \multicolumn{6}{c}{Relative L2} \\
                    \cmidrule(lr){2-7}
				& Elasticity & Plasticity & Airfoil & Pipe & NS2d & Darcy \\
				\midrule
                    FNO \citeyearpar{li2021fourier} & / & / & / & / & 0.1556 & 0.0108 \\
                    U-FNO \citeyearpar{Wen2021UFNOA} &0.0239 & 0.0039 & 0.0269 & {0.0056} & 0.2231 & 0.0183 \\
                    geo-FNO \citeyearpar{Li2022FourierNO}& {0.0229} & 0.0074 & 0.0138 & 0.0067 & 0.1556 & 0.0108 \\
                    U-NO \citeyearpar{rahman2022u}& 0.0258 & {0.0034} & 0.0078 & 0.0100 & 0.1713 & 0.0113 \\
                    F-FNO \citeyearpar{anonymous2023factorized}&0.0263 & 0.0047 & 0.0078 & 0.0070 & 0.2322 & {0.0077} \\
                    LSM \citeyearpar{wu2023LSM}& 0.0218 & 0.0025 & 0.0059 & 0.0050 & 0.1535 & \underline{0.0065} \\
                    LNO* \citeyearpar{wang2024LNO}& 0.0069 & 0.0029 & \underline{0.0053} & \underline{0.0031} & \underline{0.0830} & 0.0063 \\ 
                    \midrule
                    Galerkin \citeyearpar{Cao2021ChooseAT}& 0.0240 & 0.0120 & 0.0118 & 0.0098 & 0.1401 & 0.0084 \\
                    HT-Net \citeyearpar{anonymous2023htnet}& / & 0.0333 & 0.0065 & 0.0059 & 0.1847 & 0.0079 \\
                    OFormer \citeyearpar{li2023transformer}& 0.0183 & 0.0017 & 0.0183 & 0.0168 & 0.1705 & 0.0124 \\
                    GNOT \citeyearpar{hao2023gnot}& 0.0086 & 0.0336 & 0.0076 & 0.0047 & 0.1380 & 0.0105 \\
                    FactFormer \citeyearpar{li2023scalable}& / & 0.0312 & 0.0071 & 0.0060 & 0.1214 & 0.0109 \\
                    ONO \citeyearpar{anonymous2023improved}& 0.0118 & 0.0048 & 0.0061 & 0.0052 & 0.1195 & 0.0076 \\
                    Transolver \citeyearpar{wu2024Transolver} & \underline{0.0064} & \underline{0.0013} & \underline{0.0053} & 0.0033 & 0.0900 & 0.0058 \\
                \midrule
                    \multicolumn{1}{l}{\textbf{Transolver++ (Ours)}} & \textbf{0.0052} & \textbf{0.0011} & \textbf{0.0048} & \textbf{0.0027} & \textbf{0.0719} & \textbf{0.0049} \\
                    \multicolumn{1}{l}{Relative Promotion} & 18.8\% & 15.3\% & 10.2\% & 12.9\% & 13.4\% & 12.3\% \\
				\bottomrule
			\end{tabular}
		\end{sc}
	\end{small}
    \vspace{-5pt}
\end{table*}

\section{Implementation Details}
In this section, we provide detailed descriptions of the benchmarks, baseline methods, and implementation setups to ensure reproducibility and facilitate comparisons.

\begin{table*}[t]
    \caption{Details of different benchmarks, including geometric type, number of mesh points, as well as the type of input and output, etc. The split of the dataset is also provided to ensure reproducibility, which is listed in the order of (training samples, test samples).}
	\label{tab:benchmark_detail}
	\vspace{-5pt}
	\vskip 0.15in
	\centering
	\begin{small}
		\begin{sc}
			\renewcommand{\multirowsetup}{\centering}
			\setlength{\tabcolsep}{5.5pt}
			\begin{tabular}{l|c|c|c|c|c|c}
				\toprule
                   Type & Benchmark & \#Dim & \#Meshes & \#Input & \#Output & Split \\ 
                \midrule
                    & Elasticity & 2D & 972 & Structure & Inner Stress & (1000, 200)\\
                    & Plasticity & 2D + Time & 3131 & External Force & Mesh Displacement & (900, 80) \\
                    Standard & Airfoil & 2D & 11271 & Structure & Mach Number & (1000, 200) \\
                    Benchmark & Pipe & 2D & 16641 & Structure & Velocity & (1000, 200) \\
                    & Navier-Stokes & 2D + Time & 4096 & velocity & velocity & (1000, 200) \\
                    & Darcy & 2D & 7225 & Porous Medium & pressure & (1000, 200) \\
                    \midrule
                     Industrial & \multirow{2}{*}{DrivAerNet++} & \multirow{2}{*}{3D} & $\sim$700k & Structure & Surface Pressure & (190, 10) \\
                     Applications& & & $\sim$2.5M & Structure & Pressure\ \&\ Velocity & (190, 10) \\
                      & AirCraft & 3D & $\sim$300k & Structure & 6 Quantities & (140, 10)\\

                   
				\bottomrule
			\end{tabular}
		\end{sc}
	\end{small}
    \vspace{-5pt}
\end{table*}

\subsection{Benchmarks}
We evaluate our method on six standard PDE benchmarks, including Elasticity, Plasticity, Airfoil, Pipe, NS2D, and Darcy,  as well as two industrial datasets, DrivAerNet++ and AirCraft. These benchmarks cover a wide range of physics simulation tasks, varying in complexity and geometry, and serve as a comprehensive benchmarks for assessing the effectiveness of neural PDE solvers as shown in Table~\ref{tab:benchmark_detail}. Here are the details of these datasets.

\paragraph{Elasticity} 
This benchmark is generated by the simulations of the stress field in a hyper-elastic solid body under tension, which is governed by a stress-strain relationship using the Rivlin-Saunders material model \cite{Li2022FourierNO}. Each case involves a unit cell of 972 points with a void in the middle , clamped at the bottom edge and subjected to tension on the top. Elasticity contains a total of 1200 samples, with 1000 for training and 200 for testing.

\paragraph{Plasticity} 
This benchmark is generated by the simulation of a plastic forging problem, where a block is impacted by a frictionless die moving at a constant speed \cite{Li2022FourierNO}. An elastoplastic constitutive model is adopted to model this physical system with 900 training and 80 testing samples. The input is the external force on every mesh with the shape of 101 $\times$ 31, and the output is the time-dependent deformation and mesh grid over 20 timesteps.

\paragraph{Airfoil}
This benchmark is generated from simulations of transonic flow over an airfoil, governed by Euler's equations \cite{Li2022FourierNO}. The whole field is discretized to unstructured meshes in the shape of 221 $\times$ 51 as the input and the output is the corresponding Mach number on these meshes. The dataset includes 1000 training samples and 200 test samples that are based on the initial NACA-0012 shape.

\paragraph{Pipe} This benchmark consists of simulations of incompressible flow in a pipe, governed by the Navier-Stokes equation with viscosity $\nu=0.005$ \cite{Li2022FourierNO}. The pipe has a length of 10 and a width of 1, with its centerline parameterized by cubic polynomials determined by five control nodes. The dataset also contains 1000 training and 200 testing samples, whose inputs are the mesh point locations 129 $\times$ 129 and outputs are the horizontal velocity field.

\paragraph{Navier-Stokes} This benchmark consists of simulations of the 2D Navier-Stokes equations in vorticity form on the unit torus $(0, 1)^2$ \cite{li2021fourier}. The objective is, given the past velocity, to predict future velocity for 10 steps on discretized meshes in the shape of 64 $\times$ 64. The inputs are the velocity for the past 10 steps, while the outputs provide the future velocity for 10 timesteps. The dataset also includes 1000 samples for training and 200 for testing.

\paragraph{Darcy} This benchmark consists of simulations of the steady-state Darcy Flow in two dimensions, governed by a second-order elliptic equation on the unit square \cite{Li2022FourierNO}. The input is the structure of the porous medium and the output is the corresponding fluid pressure. The dataset contains 1000 training samples and 200 testing samples, generated using a second-order finite difference scheme on 421 $\times$ 421 uniform grids and later downsampled to $85\times 85$.

\paragraph{DrivAerNet++} This benchmark \cite{elrefaie2024drivaernet++} is a large and comprehensive dataset for aerodynamic car design, featuring high-fidelity computational fluid dynamics (CFD) simulations. It includes over 8,000 car designs with various configurations of wheel and underbody design. To ensure efficiency while maintaining diversity, we select 200 representative cases from these designs. Notably, in our experiments, DrivAerNet++ is then divided into two subsets with different levels of resolution. The first subset, as shown in Table~\ref{tab:benchmark_detail}, only consists of surface meshes, where each mesh point is characterized by its 3D position $(x, y, z)$, surface normal vector $(u_x, u_y, u_z)$, and signed distance function (SDF). The output for this subset is the surface pressure on these meshes. The second subset provides a full 3D pressure and velocity field, significantly increasing the datasets complexity, with the number of mesh points reaching approximately 2.5 million and requiring the model to predict surface pressure, velocity and pressure of the surrounding area. The dataset consists of 190 training samples and 10 test samples, offering a rigorous benchmark for evaluating aerodynamic modeling at different levels of fidelity.

\paragraph{AirCraft} This benchmark includes simulations of over 30 aircraft designs under 5 different incoming flow conditions, varying in Mach number, angle of attack, and sideslip angle. Unlike commonly used aerodynamics datasets such as Airfoil, the AirCraft dataset discretizes each aircraft into approximately 300,000 3D mesh points, offering a significantly higher resolution for capturing complex aerodynamic phenomena. Each mesh point is characterized by its spatial coordinates $(x, y, z)$ and surface normal vector, serving as the input. These high-fidelity computational fluid dynamics (CFD) simulations, conducted by aerodynamicists in an aircraft design institution, ensure precise and realistic aerodynamic modeling. The dataset requires predicting six key physical quantities: pressure coefficient $C_p$, fluid density $\rho$, velocity components ($u$, $v$, $w$), and pressure $p$. With 140 training cases and 10 test cases, this dataset presents intricate aerodynamic interactions, making it a rigorous benchmark for assessing model scalability and accuracy.

\subsection{Metrics}

To comprehensively evaluate model performance across different datasets and prediction tasks, we adopt relative L2 error as the primary evaluation metric. For large-scale datasets, we evaluate both field and coefficient predictions separately. And we further introduce R-squared ($R^2$) score as an additional metric to assess the accuracy of coefficient predictions. In this section, we would explain how these metrics are applied to different dataset categories in detail.

\subsubsection{Standard Benchmarks}
For standard benchmark datasets, model performance is assessed using the relative L2 error, which directly measures the difference between the predicted output and the ground truth. Given an output field $\hat{y}$ predicted by the model and the ground truth $y$, the relative L2 error is computed as:
\begin{equation}
\text{Relative L2} = \frac{\|\hat{\mathbf{y}} - \mathbf{y}\|_2}{\|\mathbf{y}\|_2}.
\end{equation}
This metric on standard benchmarks is reported in the Table~\ref{tab:mainres_standard}, providing a direct comparison across different models.

\subsubsection{Large-Scale Datasets}
\paragraph{Field and Coefficient Errors} For large-scale datasets, we decompose the relative L2 error into two components to gain deeper insights into model performance. Since large-scale datasets often involve both surface and volume (refers to the surrounding area) data, we separately compute the relative L2 error for surface fields and volume fields. This distinction allows us to evaluate how well the model captures different types of physical information. In addition to predicting field values, some datasets require models to infer physical coefficients that characterize the feature of the system. The relative L2 error is also applied to these coefficients to measure prediction accuracy. 

For example, in AirCraft, lift coefficient is calculated to measure the aerodynamic lift force on the body of an aircraft, which is a key metric to assess the lift performance of an aircraft under a certain flow condition. The lift coefficient is defined as:
\begin{equation}
    C_L = \frac{1}{\frac{1}{2} \rho v_\infty^2 A} \int_S \left( - p \mathbf{n} \cdot \hat{\mathbf{l}} + \boldsymbol{\tau} \mathbf{n} \cdot \hat{\mathbf{l}} \right) {\rm d}S,
\end{equation}
where $p$ is the pressure field on the surface, $\mathbf{n}$ is the unit normal vector of the surface, $\hat{\mathbf{l}}$ is the unit vector in the lift direction, $\boldsymbol{\tau}$ is the shear stress tensor and $S$ is the surface of the aircraft.

Also in car design, drag coefficient is a crucial metric to quantify the aerodynamic drag force on the body of a vehicle, which can be used to improve fuel efficiency and the performance of vehicles. The drag coefficient is defined as:
\begin{equation}
\label{equ:lift}
C_D = \frac{1}{\frac{1}{2} \rho v_\infty^2 A} \int_S \left( - p \mathbf{n} \cdot \hat{\mathbf{d}} + \boldsymbol{\tau} \mathbf{n} \cdot \hat{\mathbf{d}} \right) {\rm d}S,
\end{equation}
where $p$ is the surface pressure, $\mathbf{n}$ is the unit normal vector of the surface, $\hat{\mathbf{d}}$ is the unit vector in the drag direction, $\boldsymbol{\tau}$ is the shear stress tensor and $S$ is the surface of the vehicle.


\paragraph{R-squared Score for Coefficient Predictions}
To further evaluate the models ability to predict physical coefficients, we introduce the R-squared ($R^2$) score as an additional metric. The $R^2$ score is computed as:
\begin{equation}
\label{equ:drag}
R^2 = 1 - \frac{\sum_i (y_i - \hat{y}_i)^2}{\sum_i (y_i - \bar{y})^2},
\end{equation}
where $\hat{y}_i$ is the $i$-th of the predicted coefficients, $y_i$ is the $i$-th ground truth, and $\bar{y}$ is the mean of the ground truth values. An $R^2$ score closer to 1 indicates better model performance, while lower values suggest less accurate predictions. This metric measures how well the model learns the physical quantitiy fields among all samples.

\begin{table}[t]
\vspace{-5pt}
	\caption{Implementation details of Transolver++ including training and model configuration. Training configurations are identical to previous methods \cite{wu2024Transolver,hao2023gnot,anonymous2023geometryguided,elrefaie2024drivaernet++} and shared in all baselines. $\mathcal{L}_{\mathrm{v}}$ and $\mathcal{L}_{\mathrm{s}}$ refer to the loss on volume (surrounding area) and surface physics fields respectively.}
	\label{tab:implementation_detail}
	\vskip 0.1in
	\centering
	\begin{small}
		\begin{sc}
			\renewcommand{\multirowsetup}{\centering}
			\setlength{\tabcolsep}{2pt}
			\begin{tabular}{l|ccccc|cccc}
				\toprule
                \multirow{3}{*}{Benchmarks} & \multicolumn{5}{c}{Training Configuration (Shared in all baselines)} & \multicolumn{4}{c}{Model Configuration} \\
                    \cmidrule(lr){2-6}\cmidrule(lr){7-10}
			  & Loss & Epochs & Initial LR & Optimizer & Batch Size & Layers $L$ & Heads & Channels $C$ &  Slices $M$ \\
			    \midrule
                 Elasticity & & \multirow{6}{*}{500} & \multirow{6}{*}{$10^{-3}$} & & 8 & \multirow{6}{*}{8} & \multirow{6}{*}{8} & 128 & 64 \\
			Plasticity & & & &  & 8 & & & 128 & 64 \\
        	Airfoil & Relative & & & AdamW & 4 & & & 128 & 64 \\
                Pipe & L2 & & & \citeyearpar{loshchilov2018decoupled} & 4 & & & 128 & 64 \\
                NavierStokes & & & & & 8 & & & 256 & 32 \\
                Darcy & & & & & 4 &  & & 128 & 64 \\
                \midrule
                DrivAerNet++ Full & $\mathcal{L}_{\mathrm{v}}+\mathcal{L}_{\mathrm{s}}$ & \multirow{3}{*}{200} & \multirow{3}{*}{$10^{-3}$} & Adam & \multirow{3}{*}{1} & \multirow{3}{*}{4} & \multirow{3}{*}{8} & \multirow{3}{*}{256} & \multirow{3}{*}{32} \\
                DrivAerNet++ Surf & $\mathcal{L}_{\mathrm{s}}$  &  & & \citeyearpar{DBLP:journals/corr/KingmaB14} &  & & &  &   \\
                AirCraft & $\mathcal{L}_{\mathrm{v}}+\mathcal{L}_{\mathrm{s}}$ & & & & & & & & \\
				\bottomrule
			\end{tabular}
		\end{sc}
	\end{small}
\vspace{-10pt}
\end{table}

\subsection{Baselines and Implementations}
We conduct extensive comparisons between Transolver++ and over 20 state-of-the-art baselines, encompassing a diverse range of methods for solving partial differential equations (PDEs). These baselines include typical neural operators, Transformer-based PDE solvers, and graph neural networks (GNNs) and are tested under the same training configurations as shown in Table~\ref{tab:implementation_detail}. To ensure a rigorous comparison, we obtain the open-source implementations of these models and carefully verify their consistency with the original papers before training. 

\subsubsection{Standard Benchmarks}
We try to be as loyal to the original settings of baselines as possible. However, when the model parameters count is too large to conduct a fair comparison, we would change either the number of blocks or the hidden dimension to ensure a fair comparison. As we mentioned before, Transformer-based models usually benefit from large parameter sizes \cite{wu2024Transolver}. Thus, the alignment of parameter size is essential to control the variable and highlight the performance difference caused by architecture design. Here are detailed implementations.

Specifically, we adjust LNO and reduce its default dimension from 256 to 192 in elasticity, as its parameter count is already three times larger than the other largest models, while other models' settings remain unchanged. 

Besides, we carefully review the original papers of these baseline models to make sure that they are fully tuned on their hyper-parameters. For all the models whose data settings align exactly with ours, such as FNO \cite{li2021fourier}, Geo-FNO \cite{Li2022FourierNO}, GNOT \cite{hao2023gnot}, OFormer \cite{li2023transformer}, Transolver \cite{wu2024Transolver}, we directly adopt the results reported in their respective publications. For other models, we refer to the results presented in LSM \cite{wu2023LSM} and Transolver \cite{wu2024Transolver}, as these works have conducted comprehensive and rigorous hyper-parameter tuning to ensure a fair and reliable comparison.

\subsubsection{Industrial Applications}
For DrivAerNet++ and AirCraft datasets, most of the baseline models are unable to handle million-scale meshes directly. To address this, we subsample the dataset to 50k meshes point and reconstruct the meshes using K-Nearest Neighbors (KNN) \cite{peterson2009knearest} method to preserve essential geometric relationships. 

Specifically during training, we apply a random subsampling strategy at the beginning of each epoch and reconstruct meshes for only once in each case to maintain consistency of the total step count compared to other baselines capable of handling million-scale meshes. For testing, we perform a complete subsampling operation, obtaining multiple partial predictions across different subsampled sets. These predictions are then aggregated and concatenated to reconstruct the full output for each test case. The relative L2 error is subsequently computed on the concatenated predictions, and the final results are reported in Table~\ref{tab:mainres_large_geometries}. Furthermore, lift and drag coefficient are also computed based on the reconstructed outputs following their respective formulations in Eq.~\eqref{equ:lift} and Eq.~\eqref{equ:drag}.

In terms of model configurations, each model has been extensively tuned on their hyperparameters, especially graph neural networks (GNNs) for their extreme instability \cite{instabilityGNN} when dealing with million-scale meshes. To minimize human bias, we employ a standardized tuning strategy by systematically changing the hidden dimension from \{64, 128, 256, 512\}, the number of layers from \{2, 4, 6, 8, 10\} along with tuning model-specific hyperparameters. During this process, we observe that almost all GNNs experienced geometric instability \cite{morris2023geometric} when applied to the DrivAerNet++ Full dataset with approximately 2.5 million mesh points per case. Out of all baselines, Transolver achieves the second-best performance, benefiting from its Slice-Deslice mechanism, which improves its stability and scalability, while Transolver++ surpasses it by introducing eidetic physical states and a parallelism framework that can directly handle million-scale meshes with enhanced efficiency. In Transolver++, we set the number of slices to 32 and its channels to 256 with 4 layers of Transolver++ block to balance efficiency and performance. And we only need to adopt parallelism to Transolver++ for the DrivAerNet++ Full benchamrk on only 4 A100 GPUs at most.


\section{More Visualizations}\label{appdix:vis}
As a supplement to Figure \ref{fig:slice_visualization} of the main text, in this section, we will provide detailed visualization of the eidetic physical states of Transolver++ as well as case study showcases on different datasets.
\subsection{Eidetic States Visualization}
Here, for simplicity and clarity, we present visualizations of the eidetic physical states on four representative benchmarks: DrivAerNet++ Surf, Airplane, Airfoil, and Elasticity. We further compare these states with those learned by Transolver, demonstrating the effectiveness of our approach in capturing eidetic states under complex physical geometries. All visualizations are extracted from the final layer of each model to provide a direct comparison of their learned representations.

\begin{figure*}[h]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{fig/drivernet.pdf}}
    \vspace{-5pt}
	\caption{Visualizations of 32 physical or eidetic states learned in the final layer of models on DrivAetNet++ Surface. Transolver and Transolver++ are both plotted for a clear comparison. The lighter color means a higher weight in the corresponding physical state.}
    \vspace{-20pt}
\end{center}
\end{figure*}

\begin{figure*}[h]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{fig/airplane.pdf}}
    \vspace{-5pt}
	\caption{Visualizations of 32 physical or eidetic states learned in the final layer of models on AirCraft. Transolver and Transolver++ are both plotted for a clear comparison. The lighter color means a higher weight in the corresponding physical state.}
    \vspace{-20pt}
\end{center}
\end{figure*}

\begin{figure*}[h]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{fig/airfoil_transolver_plus.pdf}}
    \vspace{-5pt}
	\caption{Visualizations of 64 learned eidetic states of the final layer in Transolver++ on Airfoil.}
    \vspace{-20pt}
\end{center}
\end{figure*}

\begin{figure*}[h]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{fig/airfoil_transolver.pdf}}
    \vspace{-5pt}
	\caption{Visualizations of 64 learned physical states of the final layer in Transolver on Airfoil.}
    \vspace{-20pt}
\end{center}
\end{figure*}

\begin{figure*}[h]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{fig/elasticity_1.png}}
    \vspace{-5pt}
	\caption{Visualizations of 64 learned eidetic states of the final layer in Transolver++ on Elasticity.}
    \vspace{-20pt}
\end{center}
\end{figure*}

\begin{figure*}[h]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{fig/elasticity_2.png}}
    \vspace{-5pt}
	\caption{Visualizations of 64 learned states of the final layer in Transolver on Elasticity.}
    \vspace{-20pt}
\end{center}
\end{figure*}

\subsection{Case Studies}
In addition to the Figure \ref{fig:slice_visualization} in the main text, here we provide more showcase comparisons in Figure~\ref{fig:aircraft}, \ref{fig:surface} and \ref{fig:standard_comparison}. Specifically, we compare our model with the strong baselines in the respective datasets, where Transolver is selected for the comparison in standard benchmarks and both Transolver and GNOT are compared in industrial datasets.

\begin{figure*}[h]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{fig/case_study_surface.pdf}}
    \vspace{-5pt}
	\caption{Showcase comparison with Transolver and GNOT on AirCraft. A lighter color in the error map indicates a better performance.}\label{fig:aircraft}
    \vspace{-20pt}
\end{center}
\end{figure*}

\begin{figure*}[h]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{fig/case_study_car.pdf}}
    \vspace{-5pt}
	\caption{Showcase comparison with Transolver and GNOT on DrivAerNet++ Surface. A lighter error map means better performance.}\label{fig:surface}
    \vspace{-20pt}
\end{center}
\end{figure*}

\begin{figure*}[h]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{fig/case_study_standard.pdf}}
    \vspace{-5pt}
	\caption{Showcase comparison with Transolver on Elasticity and Airfoil. A lighter color in the error map indicates a better performance.}\label{fig:standard_comparison}
    \vspace{-20pt}
\end{center}
\end{figure*}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
