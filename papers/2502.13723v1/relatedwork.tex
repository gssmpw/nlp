\section{Related Work}
\textbf{Improving Reasoning in LLMs.} Existing studies improve the robustness of LLM reasoning at various stages. Some focus on the pre-training phase by curating pretraining datasets to enhance mathematical knowledge \citep{shao2024deepseekmath,azerbayev2023llemma}, some address it during instruction fine-tuning using high-quality reasoning QA datasets \citep{mitra2023orca,mukherjee2023orca,yue2023mammoth,yu2023metamath}, while others tackle the reasoning challenge at the reinforcement learning stage by using human or AI feedback for optimization \citep{yuan2024advancing,lai2024step}. Additionally, several studies improve reasoning at the inference stage through prompting techniques that search for better reasoning paths in trees or graphs \citep{yao2024tree,besta2024graph,wang2023plan,yang2024buffer,zheng2023take}. Our work aligns most closely with methods that focus on the reinforcement learning stage but differs by improving LLMs by using value signals. 

\textbf{Reinforcement Learning in LLMs.} Reinforcement learning is widely used in LLM post-training to learn implicit rewards from human feedback~\citep{ouyang2022training,dubey2024llama,yang2024qwen2}. The standard pipeline trains a reward model and updates the policy via PPO~\citep{schulman2017proximal}. DPO~\citep{rafailov2023directpreferenceoptimizationlanguage} shows that the log ratio between optimal policy and reference models expresses the corresponding reward model, allowing direct policy updates based on the preference data. \citet{rafailov2024r} further derived the DPO objective, validating its ability to learn token-level rewards, while subsequent works refine DPO for practical applications~\citep{ethayarajh2024kto,chen2024noise,azar2024general}. Our approach extends DPO by aligning the policy model with value estimation, eliminating the need for pairwise data and leveraging value signals for enhanced reasoning. While recent work like DQO~\citep{liu2024enhancing} and OREO~\cite{wang2024offline} also aims to improve LLM reasoning within the value-based framework, they differ from our approach in that they require training a separate value model alongside the policy model. In contrast, our method is more closely aligned with DPO, requiring only the policy model and a reference model during training.

\textbf{Tree-search Guided Reasoning in LLMs.} Tree search has recently emerged as a key technique in LLM reasoning. Prior work has used prompt engineering to estimate expected returns for reasoning steps, guiding models toward optimal answers~\citep{yao2024tree,feng2023alphazero,besta2024graph}. Subsequent studies employed tree search to generate training data for RL or fine-tuning. \citet{xie2024monte} and \citet{chen2024step} use value estimates from tree search to create step-level preference pairs, while \citet{feng2023alphazero} and \citet{chen2024alphamath} train value models from these estimates for guided decoding. Our work also leverages MCTS, but unlike \citet{feng2023alphazero} and \citet{chen2024alphamath} who focus on improving the inference stage, we directly train the policy model using MCTS value estimates. Additionally, compared to \citet{xie2024monte} and \citet{chen2024step}, we make use of value estimates instead of preference, obtaining stronger results.