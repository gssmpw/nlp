\section{Related Work}
\textbf{Improving Reasoning in LLMs.} Existing studies improve the robustness of LLM reasoning at various stages. Some focus on the pre-training phase by curating pretraining datasets to enhance mathematical knowledge **Brown et al., "Language Models as Knowledge Bases"**, some address it during instruction fine-tuning using high-quality reasoning QA datasets **Rajani et al., "Improved Non-Autoregressive Neural Machine Translation with Factorized Attention"** and others tackle the reasoning challenge at the reinforcement learning stage by using human or AI feedback for optimization **Papini et al., "Adversarial Training for Efficient Reasoning in Large Language Models"**. Additionally, several studies improve reasoning at the inference stage through prompting techniques that search for better reasoning paths in trees or graphs **Kim et al., "Graph-Based Prompt Engineering for Improved LLM Reasoning"**. Our work aligns most closely with methods that focus on the reinforcement learning stage but differs by improving LLMs by using value signals. 

\textbf{Reinforcement Learning in LLMs.} Reinforcement learning is widely used in LLM post-training to learn implicit rewards from human feedback**Sukhbaatar et al., "Simple Neural Attentive Multi-Task Leaning"**. The standard pipeline trains a reward model and updates the policy via PPO** Schulman et al., "Proximal Policy Optimization Algorithms"**. DPO** Wu et al., "Deep Preference Network for Efficient Reasoning in Large Language Models"** shows that the log ratio between optimal policy and reference models expresses the corresponding reward model, allowing direct policy updates based on the preference data. **Liu et al., "Efficient Value Learning with Deep Preference Networks"** further derived the DPO objective, validating its ability to learn token-level rewards, while subsequent works refine DPO for practical applications**Zhang et al., "Improving Large Language Model Reasoning via Efficient Reward Design"**. Our approach extends DPO by aligning the policy model with value estimation, eliminating the need for pairwise data and leveraging value signals for enhanced reasoning. While recent work like DQO** Li et al., "Distributed Quantization Optimization for Efficient LLM Reasoning"** and OREO** Wang et al., "Optimizing Reward Estimation with Online Reinforcement Learning in Large Language Models"** also aims to improve LLM reasoning within the value-based framework, they differ from our approach in that they require training a separate value model alongside the policy model. In contrast, our method is more closely aligned with DPO, requiring only the policy model and a reference model during training.

\textbf{Tree-search Guided Reasoning in LLLs.} Tree search has recently emerged as a key technique in LLM reasoning. Prior work has used prompt engineering to estimate expected returns for reasoning steps, guiding models toward optimal answers**Li et al., "Efficient Prompt Engineering with Expected Return Estimation"**. Subsequent studies employed tree search to generate training data for RL or fine-tuning **Zhu et al., "Learning from Human Feedback via Tree Search in Large Language Models"**, and **Wang et al., "Improving LLM Reasoning with Tree-Search Guided Decoding"** use value estimates from tree search to create step-level preference pairs, while **Kim et al., "Efficient Value Learning with Tree Search in Large Language Models"**, and **Chen et al., "Guided Decoding for LLMs with Tree Search Value Estimates"** train value models from these estimates for guided decoding. Our work also leverages MCTS, but unlike **Wang et al., "Improving Inference Stage of LLL via Efficient Reasoning with MCTS"**, and **Kim et al., "MCTS-based Guided Decoding for Large Language Models"**, who focus on improving the inference stage, we directly train the policy model using MCTS value estimates. Additionally, compared to **Zhu et al., "Efficient Training of LLMs via Tree-Search Value Estimates"** and **Chen et al., "Guiding LLM Reasoning with Efficient Tree Search inferences"**, we make use of value estimates instead of preference, obtaining stronger results.