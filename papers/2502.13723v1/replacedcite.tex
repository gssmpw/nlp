\section{Related Work}
\textbf{Improving Reasoning in LLMs.} Existing studies improve the robustness of LLM reasoning at various stages. Some focus on the pre-training phase by curating pretraining datasets to enhance mathematical knowledge ____, some address it during instruction fine-tuning using high-quality reasoning QA datasets ____, while others tackle the reasoning challenge at the reinforcement learning stage by using human or AI feedback for optimization ____. Additionally, several studies improve reasoning at the inference stage through prompting techniques that search for better reasoning paths in trees or graphs ____. Our work aligns most closely with methods that focus on the reinforcement learning stage but differs by improving LLMs by using value signals. 

\textbf{Reinforcement Learning in LLMs.} Reinforcement learning is widely used in LLM post-training to learn implicit rewards from human feedback____. The standard pipeline trains a reward model and updates the policy via PPO____. DPO____ shows that the log ratio between optimal policy and reference models expresses the corresponding reward model, allowing direct policy updates based on the preference data. ____ further derived the DPO objective, validating its ability to learn token-level rewards, while subsequent works refine DPO for practical applications____. Our approach extends DPO by aligning the policy model with value estimation, eliminating the need for pairwise data and leveraging value signals for enhanced reasoning. While recent work like DQO____ and OREO____ also aims to improve LLM reasoning within the value-based framework, they differ from our approach in that they require training a separate value model alongside the policy model. In contrast, our method is more closely aligned with DPO, requiring only the policy model and a reference model during training.

\textbf{Tree-search Guided Reasoning in LLMs.} Tree search has recently emerged as a key technique in LLM reasoning. Prior work has used prompt engineering to estimate expected returns for reasoning steps, guiding models toward optimal answers____. Subsequent studies employed tree search to generate training data for RL or fine-tuning. ____ and ____ use value estimates from tree search to create step-level preference pairs, while ____ and ____ train value models from these estimates for guided decoding. Our work also leverages MCTS, but unlike ____ and ____ who focus on improving the inference stage, we directly train the policy model using MCTS value estimates. Additionally, compared to ____ and ____, we make use of value estimates instead of preference, obtaining stronger results.