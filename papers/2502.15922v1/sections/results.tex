\section{Results} \label{sec:results}

With the algorithms established and hyperparameters determined, we trained agents using each of the three algorithms on the task sequence shown in Fig. \ref{fig:task_sequence}. We also returned back to the nominal task at the end, allowing 8 million total training interactions in the sequence. To account for randomness in the training process, we ran each algorithm with 5 different seeds. For each seed, we parallelized the training process to train in 10 parallel threads. Each seed was also trained in parallel, meaning 50 instances of the environment were run at once. This was done on an AMD Ryzen Threadripper 3960X 24-Core CPU.

\subsection{Case Study 1: HalfCheetah}

\begin{figure}[t]
    \centering
    % First subfigure
    \begin{subfigure}[b]{\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/halfcheetah_reward.png} % Replace with your figure
        \caption{Training reward curves.}
        \label{fig:cheetah_reward}
    \end{subfigure}
    
    
    % Second subfigure
    \begin{subfigure}[b]{\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/halfcheetah_cost.png} % Replace with your figure
        \caption{Training cost curves.}
        \label{fig:cheetah_cost}
    \end{subfigure}
    
    \caption{Rewards and costs during training with task changes for the HalfCheetah environment. The tasks, shown by the background color, correspond to the tasks shown in Fig. \ref{fig:task_sequence}.}
    \label{fig:cheetah_train}
\end{figure}

First, we can inspect the rewards throughout the training process. Fig. \ref{fig:cheetah_reward} shows the rewards for each algorithm across task changes. When a task changed, there was a dramatic difference in reward, highlighting the unique challenges of each task. By viewing this figure task-by-task, we can qualitatively assess the catastrophic forgetting of each algorithm. In all cases, CPO, the agent trained with the non-continual RL algorithm, appeared to forget. Its reward was lower than the final reward of the last time it experienced the task. At first, the PPO+EWC and Safe EWC agents also show this behavior. However, as the task was revisited, the agents trained using EWC methods forgot less. In the back task, all agents forgot their policy, but the CPO agent had the largest reward drop since it converged quicker than the other algorithms. In the front task, the agents obtained using the EWC methods experienced minimal forgetting.

Next, we can inspect the costs throughout the training process, shown in Fig. \ref{fig:cheetah_cost}. Here, we can clearly see the advantage of CPO over PPO+EWC. The PPO+EWC agent completely ignored the velocity constraint and went as fast as possible to maximize reward. On the nominal task, the Safe EWC agent had higher costs than the CPO agent at first. However, it was significantly safer than the PPO+EWC agent. Additionally, by the time the nominal task was experienced for the fourth time, the Safe EWC agent violated safety less than the CPO agent. This indicates that Safe EWC may encourage the agent to remember safety constraints across task visits. In the back and front tasks, the Safe EWC and PPO+EWC agents had around the same or fewer safety violations than the CPO agent. This is likely due to the poorer task performance (see Fig. \ref{fig:cheetah_reward}), leading to fewer chances of violating safety.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/immediate.png}
    \caption{Immediate reward when experiencing nominal dynamics for the HalfCheetah. This measures how well the policy under nominal conditions is remembered.}
    \label{fig:immediate_reward}
\end{figure}

Further evidence for the strengths of each algorithm from a continual reinforcement learning perspective is shown in Fig. \ref{fig:immediate_reward}. This figure shows the immediate reward when experiencing nominal dynamics in the task sequence. The PPO+EWC agent had the highest positive slope, indicating it remembered more each time it revisited the task. The CPO agent was more stagnant, but slightly improved at the end, showing the inability of safe RL algorithms to effectively avoid catastrophic forgetting. The Safe EWC agent is in between, demonstrating the ability for safe continual RL algorithms to balance the tradeoff between continual learning and safety.

Beyond qualitative training curves, we can compute metrics to determine the strengths and weaknesses of each algorithm. We consider the following metrics.

\begin{itemize}
    \item \textbf{Total Cost}: the average total cost across each individual task. Calculated for each task as follows where $N$ is the number of times a task is visited and costs is a vector of the costs for each training timestep.

    \begin{equation}
        \frac{1}{N} \sum_{i=1}^{\text{len(costs)}} \text{costs}_i
    \end{equation}
    \item \textbf{Task Forget Percentage}: the percentage drop in performance from the previous time a task was experienced to the next time it is experienced. Calculated for each task as follows where final is the reward the last time the task was visited and immediate is the first episodic reward the next time the task is seen.

    \begin{equation}
        100 \times \frac{1}{N} \sum_{i=1}^{N} \frac{\text{final}-\text{immediate}}{|\text{final}|}
    \end{equation}
    
    \item \textbf{Final Task Reward}: the final reward across all visits of each task, measuring asymptotic performance.
\end{itemize}

\begin{table}[t]
\caption{HalfCheetah task sequence performance metrics. Mean $\pm$ standard deviation across 5 seeds.}
\centering
\label{tab:halfcheetah}
\begin{tabular}{@{}llll@{}}
\toprule
Agent & Nominal            & Back             & Front          \\ \midrule
\multicolumn{4}{c}{Total Cost ($\downarrow$)}                                           \\ \midrule
CPO       & $787.4 \pm 112.1$     & $\mathbf{386 \pm 183.5}$  & $629.7 \pm 298.7$ \\
PPO+EWC   & $25043.7 \pm 3202$ & $1074 \pm 792.3$ & $707.1 \pm 91.8$ \\
Safe EWC  & $\mathbf{680.1 \pm 34}$     & $395.6 \pm 228.6$  & $\mathbf{17.4 \pm 19.9}$  \\ \midrule
\multicolumn{4}{c}{Task Forget Percentage (\%) ($\downarrow$)}                                     \\ \midrule
CPO       & $46.6 \pm 19.9$      & $67.9 \pm 22.5$    & $108 \pm 60$  \\
PPO+EWC   & $26.1 \pm 7.9$      & $71.8 \pm 7.1$    & $\mathbf{30.8 \pm 17.7}$  \\
Safe EWC  & $\mathbf{19.6 \pm 15.6}$      & $\mathbf{62.3 \pm 34.4}$    & $36.2 \pm 8.6$  \\ \midrule
\multicolumn{4}{c}{Final Task Reward ($\uparrow$)}                                           \\ \midrule
CPO       & $2034.3 \pm 190.1$     & $1611.5 \pm 93.9$  & $1122.9 \pm 242.1$ \\
PPO+EWC   & $\mathbf{4690.1 \pm 429.6}$ & $\mathbf{1706.9 \pm 342.9}$ & $\mathbf{1385.5 \pm 99.4}$ \\
Safe EWC  & $2692.6 \pm 161.5$     & $1634.3 \pm 117.7$  & $1143.6 \pm 248.5$  \\ \bottomrule
\end{tabular}
\end{table}

The quantitative metrics for the HalfCheetah task sequence, averaged across the five seeds, are shown in Table \ref{tab:halfcheetah}. First, we can consider the average total cost. Unsurprisingly, the PPO+EWC agent had much higher costs than the CPO and Safe EWC agents. In the nominal and front cases, the Safe EWC agent had less cost than the CPO agent. On the front task, the Safe EWC agent had lower reward (Fig. \ref{fig:cheetah_reward}) than the CPO agent. This means it traveled slower, violating the velocity constraint less. The Safe EWC agent may violate safety less in the nominal case because it can remember safe actions. In terms of task forget percentage, the agents obtained using the EWC methods forgot much less than the CPO agent. In the nominal and back tasks, the Safe EWC agent forgot less than the PPO+EWC agent. This may be because the Safe EWC agent converged quicker to a more stable, slower-moving policy that was more consistent or because safety was consistent across tasks. The final task reward of the Safe EWC and CPO agents are very similar, approaching the limits of distance that can be traveled without violating safety. However, the Safe EWC agent achieves a higher reward in the nominal task that was visited 4 times.

\subsection{Case Study 2: Ant}

\begin{figure}[t]
    \centering
    % First subfigure
    \begin{subfigure}[b]{\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/ant_reward.png} % Replace with your figure
        \caption{Training reward curves.}
        \label{fig:ant_reward}
    \end{subfigure}
    
    
    % Second subfigure
    \begin{subfigure}[b]{\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/ant_cost.png} % Replace with your figure
        \caption{Training cost curves.}
        \label{fig:ant_cost}
    \end{subfigure}
    
    \caption{Rewards and costs during training with task changes for the Ant environment. The tasks, shown by the background color, correspond to the tasks shown in Fig. \ref{fig:task_sequence}.}
    \label{fig:ant_train}
\end{figure}

We can perform a similar analysis for the Ant. Figure \ref{fig:ant_train} shows the training rewards and costs for the Ant task sequence. There are clear differences from the HalfCheetah scenario. For the first 4-5 million timesteps, the CPO agent was more sample-efficient and learned a reasonable policy much faster. The costs similarly reflected this. Neither the Safe EWC nor PPO+EWC agents could incur high costs because they did not move fast enough. However, the CPO agent forgot after each task switch. The PPO+EWC and Safe EWC agents exhibited positive backward transfer, improving on previous tasks by learning the next one. After the PPO+EWC and Safe EWC agents learned reasonable policies, the same cost relationship from the HalfCheetah case was held. The PPO+EWC agent ignored safety, and, in fact, the Safe EWC agent had significantly lower costs than the CPO agent. 

\begin{table}[t]
\caption{Ant task sequence performance metrics. Mean $\pm$ standard deviation across 5 seeds. The standard deviation was rounded to conserve space.}
\centering
\label{tab:ant}
\begin{tabular}{@{}llll@{}}
\toprule
Agent & Nominal            & Back             & Front          \\ \midrule
\multicolumn{4}{c}{Total Cost ($\downarrow$)}                                           \\ \midrule
CPO       & $873.6 \pm 78$     & $913.9 \pm 145$  & $872 \pm 34$ \\
PPO+EWC   & $11543.4 \pm 4563$ & $2551.1 \pm 2308$ & $6846.5 \pm 3216$ \\
Safe EWC  & $\mathbf{323 \pm 30}$     & $\mathbf{218.2 \pm 86}$  & $\mathbf{304.8 \pm 103}$  \\ \midrule
\multicolumn{4}{c}{Task Forget Percentage (\%) ($\downarrow$)}                                     \\ \midrule
CPO       & $15.1 \pm 12$      & $50.8 \pm 20$    & $44.4 \pm 18$  \\
PPO+EWC   & $-402.2 \pm 174$      & $-7 \pm 20$    & $\mathbf{21.2 \pm 13}$  \\
Safe EWC  & $\mathbf{-507.2 \pm 187}$      & $\mathbf{-16.7 \pm 7}$    & $31.9 \pm 13$  \\ \midrule
\multicolumn{4}{c}{Final Task Reward ($\uparrow$)}                                           \\ \midrule
CPO       & $2901.4 \pm 59$     & $\mathbf{2634.5 \pm 142}$  & $2590.2 \pm 152$ \\
PPO+EWC   & $\mathbf{3972.8 \pm 487}$ & $2407.2 \pm 320$ & $\mathbf{3103.1 \pm 228}$ \\
Safe EWC  & $2880.8 \pm 53$     & $2319.2 \pm 90$  & $2739.2 \pm 69$  \\ \bottomrule
\end{tabular}
\end{table}

These findings are reinforced by the quantitative metrics shown in Table \ref{tab:ant}. The Safe EWC agent had the lowest average total cost, as it maintained a lower cost than the CPO agent throughout training. As a consequence, the CPO agent obtained a higher final task reward. In reality, the algorithm used would depend on the risk tolerance of the operator. The PPO+EWC agent ignoring safety would likely be considered unacceptable. However, in this continual learning setting, the PPO+EWC and Safe EWC agents both showed significantly lower forgetting than CPO. For two tasks, they improved by learning on another task (hence the negative forgetting), i.e., positive backward transfer. In all cases, the CPO agent forgot. However, the CPO agent remembered the nominal Ant task much better than the nominal HalfCheetah task, improving from an average of 46.6\% forgetting to 15.1\% forgetting. This implies that the ability of an agent trained using a safe RL algorithm to avoid catastrophic forgetting can be task-dependent, warranting future studies.