\section{Method} \label{sec:method}

\begin{figure*}[t]
    \centering
    \includegraphics[width=2\columnwidth]{figures/task_sequence.pdf}
    \caption{Task sequence for safe continual reinforcement learning. The top sequence is the MuJoCo HalfCheetah. The bottom is the Ant. Task changes occur every 1 million training timesteps and the cycle repeats. The tasks are designed to replicate a challenging and drastic change in operating mode caused by equipment being repaired or suddenly breaking due to physical damage or a fault. The objective for the environments is to travel as far as possible in a fixed amount of time while maintaining velocity constrained (visualized by the green bubble).}
    \label{fig:task_sequence}
\end{figure*}

\subsection{Control Tasks}

To perform empirical studies on safe continual RL, we need non-stationary environments with safety constraints. To create these, we modified the velocity-constrained MuJoCo benchmark included in the Safety Gymnasium \cite{ji2023safety} Python library. These include the HalfCheetah and Ant locomotion environments. In these environments, the robot needs to travel as far as possible. However, the robot is given a velocity constraint. Going above the fixed velocity threshold is dangerous, as it risks the robot's safety. Therefore, the objective and cost conflict, making this a challenging task.

For the HalfCheetah, the observation space is a 17-dimensional vector consisting of angles, velocities, and angular velocities of its body parts along with the Z position of its front tip. There are 6 rotors, one on each thigh, shin, and foot, that make up the action space. The reward for the HalfCheetah is the forward progress it makes penalized by the control costs. For the Ant, the observation space is a 105-dimensional vector consisting of the positions of the body parts, the velocities of the body parts, and the center of mass based external forces on the body parts. The action space includes the 8 torques that can be applied to the hinge joints. Its reward is the same as the cheetah, with an added reward of 1 for every timestep the ant is healthy, meaning its torso height is too high or low, and an additional penalty if the external contact forces are too high.

To make these safe RL environments non-stationary, we emulated a sudden fault or equipment damage every 1 million timesteps. We also perform maintenance between each of the 1 million timestep missions to return back to a nominal state. The sudden damage takes the form of joints being broken off. In the HalfCheetah, we remove the front or back leg. In the Ant, we remove both the front or back legs. The task sequence we designed is shown in Fig. \ref{fig:task_sequence}. Each task is revisited at least once and the nominal task is experienced most frequently. We designed this so that the optimal policy in each task would need to be different. For example, if the back leg is missing, the agent needs to crawl instead of walk. We observed that in easier scenarios, such as with parameter changes like mass or friction, the agent could learn a single policy that solves all tasks. In these cases, continual RL algorithms are not required.


\subsection{Algorithm Details}

The first goal of this paper is to determine how well agents trained with a safe RL algorithm perform in a continual RL setting. We chose constrained policy optimization (CPO) \cite{achiam2017constrained} as the representative safe RL algorithm. We selected this algorithm because it has been shown to be effective in improving safety in scenarios with soft safety constraints, and is more stable than Lagrangian proximal policy optimization \cite{ji2023safety}. CPO handles constraints directly in the policy optimization process. It performs trust-region policy updates that ensure the policy at the next step is not outside a stable region where behavior may differ dramatically. It defines a policy update for CMDPs that guarantees both cost satisfaction and increases in reward. The CPO policy update is defined as follows \cite{achiam2017constrained} where $A$ is the advantage function, $d^\pi$ is the discounted future state distribution, $J$ is the expected discounted future return, $\delta > 0$ is the step size, and $\bar{D}_{KL}$ is the KL divergence used to measure divergence between policies. 

\begin{align*}
\pi_{k+1} =&\arg \max_{\pi \in \Pi_{\theta}} \underset{s \sim d^{\pi_k}, a \sim \pi}{\mathbb{E}} \left[ A^{\pi_k}(s, a) \right] \\
\text{s.t.} \quad &J_{C_i}(\pi_k) + \frac{1}{1 - \gamma} \underset{s \sim d^{\pi_k}, a \sim \pi}{\mathbb{E}} \left[ A^{\pi_k}_{C_i}(s, a) \right] \leq d_i \quad \forall i \\
&\bar{D}_{KL}(\pi || \pi_k) \leq \delta.
\end{align*}

We used the CPO implementation from the SafePO \cite{ji2023safety} Python library since it has been validated on the MuJoCo safe velocity tasks and comes with optimized hyperparameters on these environments, saving us significant computational effort.  

Next, we want to determine how well agents trained with a continual RL algorithm maintain safety. We chose proximal policy optimization (PPO) with elastic weight consolidation (EWC) \cite{kirkpatrick2017overcoming} as the continual RL algorithm in this paper. EWC is a popular continual learning algorithm. The key idea behind EWC is to determine which weights of the neural network were most important for solving the previous task. Then, the network is penalized for drastically changing those weights. This penalty is enforced on the neural network loss, shown in the following equation \cite{kirkpatrick2017overcoming}, where $\theta$ are neural network parameters, $\theta^*_A$ are the optimal parameters for the previous task, $\mathcal{L}_B$ is the loss on the current task, $F$ is the approximated Fisher information matrix that measures how important each parameter was to the previous task, $i$ is the parameter index, and $\lambda$ is a hyperparameter that determines the tradeoff between remembering previous tasks and learning new ones.

\begin{equation}
\mathcal{L}_{\text{EWC}}(\theta) = \mathcal{L}_{\text{B}}(\theta) + \frac{\lambda}{2} \sum_{i} F_{i} \left( \theta_{i} - \theta_{A, i}^{*} \right)^2.
\end{equation}

To incorporate EWC with PPO, a powerful reinforcement learning algorithm, we apply the EWC loss to the neural network in PPO that makes actions, the actor. We calculate the approximate Fisher information matrix using the final 20 episodes of observations on a task. We save a separate Fisher information matrix for each task in the task sequence to ensure that all tasks can be remembered. Then, we calculate each task's EWC loss independently and sum them into a single EWC penalty.

Choosing a reasonable $\lambda$ is an important step to ensure EWC works properly. To determine a reasonable $\lambda$ for our experiments, we ran PPO+EWC on the HalfCheetah with the task sequence $\{\text{nominal}, \text{back}\}$. At each epoch, we evaluated the agent on both tasks. An agent with the best nominal reward at the end of this sequence remembers the best, and an agent with the best back reward at the end of this sequence learns the second task the best. We ran this experiment for a grid of $\lambda=\{0.5, 1, 5, 10, 25, 100\}$. We found that using any $\lambda > 0$ (using PPO+EWC instead of just PPO) led to less forgetting. However, increasing $\lambda$ did not necessarily lead to more remembering, likely due to the nature of the complex, multi-objective optimization. We observed that $\lambda=10$ led to the most stable nominal performance while learning on the back task and had similar learning abilities on the back task. Therefore, we chose $\lambda=10$.

To incorporate safety into a continual RL algorithm, we take a simple reward-shaping approach. We call this approach Safe EWC. We modify the original task reward to be discounted by the safety violations. Therefore, the new reward is as follows, where $\beta$ is a cost weight hyperparameter.

\begin{equation}
    r_{\text{Safe EWC}}(s, a) = r(s, a) - \beta C(s, a)
\end{equation}

By shaping the reward with the cost, we are effectively adding another penalty to the PPO+EWC loss function. This penalty encourages safe behavior. However, the performance of this may heavily depend on the value of $\beta$. An improperly chosen $\beta$ could cause the agent to ignore safety or focus too heavily on safety. Though it is worth noting we did not experience this problem. The advantage of an algorithm like CPO is it ``automatically picks penalty coefficients to attain the desired trade-off between reward and constraint cost'' \cite{achiam2017constrained}. Therefore, Safe EWC is a first step at a safe continual reinforcement learning algorithm, but more sophisticated algorithms that are less dependent on hyperparameters should be developed in future work. 

To choose the $\beta$ coefficient for our problem, we divided the maximum reward achieved by PPO+EWC with the maximum cost to make the cost as important as the reward. This gave us $\beta=5$.
