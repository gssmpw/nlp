\section{Problem Definitions and Assumptions} \label{sec:background}

Reinforcement learning aims to find a solution to a \textbf{Markov Decision Process (MDP)}. An MDP is a tuple consisting of a state space $\mathcal{S}$, an action space $\mathcal{A}$, a transition function $T: \mathcal{S} \times \mathcal{A} \to \mathcal{S}$, a reward function $r: \mathcal{S} \times \mathcal{A} \to \mathbb{R}$, and a discount factor $\gamma \in [0, 1)$. The MDP $=(\mathcal{S}, \mathcal{A}, T, r, \gamma)$. The goal of RL is to find a policy $\pi: \mathcal{S} \to \mathcal{A}$ that maximizes the expected discounted reward.

\begin{equation}
    G_t = \sum_{k=0}^\infty \gamma^k r(s_{t+k}, a_{t+k})
\end{equation}

Therefore, the objective is to find an optimal policy $\pi^*$, one that satisfies the objective below.

\begin{equation} \label{eq:safe}
    \pi^* = \arg\max _\pi \mathbb{E}[G_t | \pi]
\end{equation}

To introduce the notion of safety, we can naturally extend the MDP to a \textbf{Constrained Markov Decision Process (CMDP)} \cite{altman1999constrained}. In a CMDP, we also introduce cost functions $\mathcal{C}: \mathcal{S} \times \mathcal{A} \to \mathbb{R}^m$ that are each constrained by a maximum cost $d=(d_1, d_2, \dots, d_m)$. The tuple is then $=(\mathcal{S}, \mathcal{A}, T, \mathcal{C}, r, \gamma, d)$. These are typically used to constrain the objective from Equation \ref{eq:safe} as follows.

\begin{equation} \label{eq:cmdp-primal}
    \text{s.t. } \mathbb{E}[\sum_{k=0}^\infty \gamma^k \mathcal{C}_i(s_{t+k}, a_{t+k})] \le d_i \text{ for } i=1,2,\dots, m
\end{equation} 

In this formulation, we constrain the expectation of sum of the discounted cost to be below some maximum value. In scenarios with hard safety constraints, safety should never be violated. In this paper, we focus on soft safety constraints where we wish to minimize the number of safety violations, but they may occur.

A continual reinforcement learning problem can be characterized by a \textbf{Non-stationary Markov Decision Process (NSMDP)}. An NSMDP is a set $\{\mathcal{S}, \mathcal{T}, \mathcal{A}, T(s' | s, a, t), (r_t)_{t\in\mathcal{T}}, \gamma\}$, where the transition function and reward are now dependent on a set of decision epochs (time), $\mathcal{T}=\{1, 2, \dots,\infty\}$. In other words, the environment can change over time. An ideal solution to this NSMDP should avoid \textbf{catastrophic forgetting}. That is, if the environment returns to a previously seen environment, the agent should remember its policy. Additional properties such as exhibiting positive forward or backward transfer are also desirable \cite{khetarpal2022towards}. \textbf{Forward transfer} measures how well learning on one set of environmental conditions improves learning on another. \textbf{Backward transfer} measures how well learning on new environmental conditions improves learning on old environmental conditions.

In this paper, we focus on sudden, drastic changes in the environment. We refer to each instance of one of these environments as a \textbf{task}. For example, controlling an octocopter drone is one task. If a motor fault occurs, controlling an octocopter with 7 functioning motors is another task. For simplicity, we assume we know when a task change occurs. We make this assumption to assess the performance of the ``continual'' aspect of the continual RL algorithm and not the accuracy of a separate task detection block. In practice, fault detection and isolation algorithms could be used for task detection in the scenarios studied in this paper.

With these definitions established, we can define a \textbf{safe continual learning problem} as one that aims to solve a Non-stationary Constrained Markov Decision Process (NSC-MDP). NSC-MDP $= \{\mathcal{S}, \mathcal{T}, \mathcal{A}, \mathcal{C}, T(s' | s, a, t), (r_t)_{t\in\mathcal{T}}, \gamma\}$. The objective is then still Equation \ref{eq:safe}, except with the reward in $G$ time-dependent, constrained by Equation \ref{eq:cmdp-primal}. Notice that the cost is not a function of time. We assume that the safety constraint is fixed across tasks. However, for some systems, the definition of safety may also change as the task does. 