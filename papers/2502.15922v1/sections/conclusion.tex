\section{Conclusion} \label{sec:conclusion}

In this paper, we studied the intersection of safe and continual reinforcement learning. We evaluated the cost, reward, and percentage forgetting of agents trained using constrained policy optimization (CPO), a safe RL algorithm, proximal policy optimization with elastic weight consolidation (PPO+EWC), a continual RL algorithm, and a proposed modification to PPO+EWC called Safe EWC that shaped the reward to penalize costs. We evaluated these on the MuJoCo HalfCheetah and Ant environments with velocity constraints. Non-stationarity was modeled by removing the front or back limbs from the systems, emulating extreme damage to the system or a fault. We found that CPO agents maintained a low cost throughout learning but experienced catastrophic forgetting. We found that agents obtained using PPO+EWC experienced less catastrophic forgetting, remembering more each time a task was visited. Agents trained with Safe EWC, a proposed simple safe continual RL algorithm, maintained low total cost, low forgetting, and high task reward. However, the properties of the algorithms were influenced by the nature of the system and task. The PPO+EWC and Safe EWC agents were less sample efficient on the Ant, leading to positive backward transfer. The overall initial success of Safe EWC agents and unanswered questions about the influence of types of non-stationarity on safe RL algorithms call for future research in this field.