\section{Introduction} \label{sec:intro}

Deep reinforcement learning (RL) algorithms have shown recent success in a variety of control applications. These include unmanned aerial vehicle attitude control \cite{koch2019reinforcement}, simulated racecar driving \cite{wurman2022outracing}, and robotics tasks \cite{apolinarska2021robotic}. When controlling real systems, we must satisfy safety constraints, especially in safety-critical applications. Despite their success, a notable challenge of RL algorithms is maintaining safety, limiting their real-world use \cite{dulac2021challenges}.  

The field of safe reinforcement learning has emerged to address this challenge. Safe RL approaches can be broadly divided into model-based and model-free approaches \cite{gu2022review}. Model-based approaches include model-predictive control \cite{zanon2020safe}, methods that use Lyapunov functions \cite{berkenkamp2017safe} or control barrier functions \cite{marvi2021cbf} to guarantee safety to some probability \cite{gu2022review} and methods that use formal verification \cite{alshiekh2018sheild} to never violate safety. The model used in these methods may be a known physical model \cite{zanon2020safe} or a derived data-driven model (e.g., in \cite{lutjens2019safe} they learn collision probabilities). The effectiveness of model-based safe RL algorithms depends on the model's accuracy. When a system lacks an available accurate dynamics model, model-free safe RL approaches may be required. Many popular model-free safe RL approaches perform constrained optimization. Examples of these include constrained policy optimization \cite{achiam2017constrained} which has guarantees on near-constraint satisfaction and Lagrangian proximal policy optimization (PPO-Lag) \cite{ray2019benchmarking}. While PPO-Lag and CPO show comparable performance, CPO has been shown to lead to more stable safety satisfaction \cite{ji2023safety}. Despite the rapid growth of safe RL, the performance of safe RL algorithms in non-stationary environments is understudied. As we control a system over its lifetime, the environment will change. These changes could be caused by component degradations leading to sudden faults, requirement shifts, or unknown environmental encounters. An early work \cite{ammar2015safe} developed a policy gradient method for safe lifelong RL, ignoring the challenge of catastrophic forgetting. Some recent works study safety in non-stationary meta-learning RL environments \cite{chen2021context}, but there is still a large gap in the study of safety in online, lifelong non-stationarity. 

The fields of continual reinforcement learning and lifelong reinforcement learning aim to adapt to task and environment changes over the lifetime of a system. The key measures of success for a continual RL algorithm are the ability to avoid catastrophic forgetting and the forward and backward transfer across environment changes \cite{khetarpal2022towards}. Continual RL approaches are largely regularization-based, use experience replays or knowledge bases, or perform network expansion. Regularization-based approaches, such as elastic weight consolidation \cite{kirkpatrick2017overcoming}, add a penalty to the reward function that encourages the network to remember how to operate in previously seen conditions.  Replay-based methods, such as CLEAR \cite{rolnick2019experience}, encourage long-term memory in the replay buffer used in off-policy RL methods. Knowledge base methods, such as \cite{zhan2017scalable}, take a similar approach to ensure the knowledge base contains relevant information from all conditions. Expansion-based approaches, such as \cite{zhang2023dynamics}, expand a part of the network each time a new scenario is encountered. This expansion may be on the network level \cite{kessler2022same} or through mixture models \cite{xu2020task}. In that way, the parts of the network that were optimal on previously seen scenarios are never overwritten, but this introduces scalability concerns. None of these approaches account for the safety of the system. The intersection of safe and continual RL is an understudied and open problem that is highly important as we work toward more real-world applications of reinforcement learning.

With this in mind, in this paper, we empirically demonstrate the need for safe continual RL algorithms. We focus on problems with sudden, dramatic changes in the system while learning like when a fault occurs or equipment breaks. These are simulated by removing joints in the HalfCheetah and Ant MuJoCo environments. In these, we want the agent to continually learn to improve reward while satisfying a maximum velocity safety constraint. When the joint breaks off, the agent should not forget how to control the nominal robotic system, as the system will be repaired. We demonstrate that agents trained using constrained policy optimization (CPO) \cite{achiam2017constrained}, a safe RL algorithm, maintain minor cost violations across the lifetime of these agents but catastrophically forgets prior performance. We show that adding elastic weight consolidation (EWC) \cite{kirkpatrick2017overcoming} to the learning process of an agent trained with proximal policy optimization (PPO) (such as in \cite{nath2023sharing}), a continual RL algorithm (PPO+EWC), has less catastrophic forgetting than CPO but heavily violates safety. We show that a simple reward-shaping approach that penalizes PPO+EWC for safety violations, which we call \textbf{Safe EWC}, exhibits less catastrophic forgetting than CPO while maintaining comparable safety constraint satisfaction.

In this work, we make the following contributions.

\begin{enumerate}
    \item We empirically demonstrate the need for safe, continual reinforcement learning algorithms. We show that agents trained with CPO, a safe RL algorithm, experience higher catastrophic forgetting and less backward transfer than those trained with PPO+EWC, a continual RL algorithm. At the same time, agents trained with PPO+EWC ignore safety to maximize reward. 
    \item We demonstrate how a simple reward shaping modification to PPO+EWC can improve safety in the HalfCheetah and Ant MuJoCo environments that are constrained by a maximum velocity. At the same time, this modified algorithm, which we call \textbf{Safe EWC}, still reduces catastrophic forgetting and improves backward transfer.
\end{enumerate}

Code for this paper is available at \url{https://github.com/MACS-Research-Lab/safe-continual}.
% The remainder of the paper is structured as follows. \textcolor{red}{Add at end. May not be room depending on space.}