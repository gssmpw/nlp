\section{Discussion} \label{sec:discussion}

The results presented in Section \ref{sec:results} demonstrated the need for research in safe continual RL. We showed that an agent obtained using constrained policy optimization exhibited more catastrophic forgetting than one obtained using proximal policy optimization with elastic weight consolidation. However, the PPO+EWC agent ignored safety to maximize reward. We demonstrated that a simple modification to the reward function turns PPO+EWC into a continual RL algorithm which produces agents that compete with CPO in safety constraint satisfaction for the HalfCheetah and Ant velocity tasks under leg removal faults. However, this is a first attempt at studying the intersection of the fields of safe and continual RL. There are many sophisticated mechanisms in continual RL (experience replay buffers, knowledge bases, expansion-based approaches, etc.) that can be modified to remember safety. At the same time, the impact of non-stationarity on the safety of realistic systems needs to be analyzed more. It is not clear what types of tasks or task sequences require mechanisms from continual RL. 