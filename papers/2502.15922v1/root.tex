\documentclass[letterpaper, conference]{cssconf}
\IEEEoverridecommandlockouts   
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{subcaption}

\title{On the Design of Safe Continual RL Methods for Control of Nonlinear Systems}


        
\author{Austin Coursey, Marcos Quinones-Grueiro, and Gautam Biswas% <-this % stops a space
\thanks{*This material is based upon work supported by the National Science Foundation Graduate Research Fellowship Program under Grant No. 2444112. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.}% <-this % stops a space
\thanks{Institute for Software Integrated Systems,
        Vanderbilt University, Nashville, TN, USA
        {\tt\small austin.c.coursey@vanderbilt.edu}}%
}



\begin{document}
\maketitle
\thispagestyle{empty}
\pagestyle{empty}



\begin{abstract}

Reinforcement learning (RL) algorithms have been successfully applied to control tasks associated with unmanned aerial vehicles and robotics. In recent years, safe RL has been proposed to allow the safe execution of RL algorithms in industrial and mission-critical systems that operate in closed loops. However, if the system operating conditions change, such as when an unknown fault occurs in the system, typical safe RL algorithms are unable to adapt while retaining past knowledge. Continual reinforcement learning algorithms have been proposed to address this issue. However, the impact of continual adaptation on the system's safety is an understudied problem. In this paper, we study the intersection of safe and continual RL. First, we empirically demonstrate that a popular continual RL algorithm, online elastic weight consolidation, is unable to satisfy safety constraints in non-linear systems subject to varying operating conditions. Specifically, we study the MuJoCo HalfCheetah and Ant environments with velocity constraints and sudden joint loss non-stationarity. Then, we show that an agent trained using constrained policy optimization, a safe RL algorithm, experiences catastrophic forgetting in continual learning settings. With this in mind, we explore a simple reward-shaping method to ensure that elastic weight consolidation prioritizes remembering both safety and task performance for safety-constrained, non-linear, and non-stationary dynamical systems. 

\end{abstract}

\maketitle

\input{sections/introduction}
\input{sections/background}
\input{sections/method}
\input{sections/results}
\input{sections/discussion}
\input{sections/conclusion}
% \input{sections/acknowledgements}

\bibliographystyle{ieeetr}
\bibliography{references}

\end{document}
