\section{Related Works}
\label{related_works}

Within the domain of SER, researchers have strategically combined a variety of feature extraction methods and classification models to improve recognition efficacy. This includes the incorporation of handcrafted features into traditional ML models and DL models as classifiers. Moreover, a subset of approaches adopts an E2E framework where features are autonomously extracted from the raw waveform signal of speech. This methodological diversity represents a systematic effort in SER research to optimize recognition accuracy by harnessing a spectrum of feature extraction techniques and model architectures.

In **Mandal et al., "Signal Segmentation using Depth First Search for Speech Emotion Recognition"**, a signal segmentation methodology was introduced, using the depth first search (DFS) algorithm to determine segment duration and overlap. Local features, including pitch, mel frequency cepstral coefficients (MFCC), line spectral pairs (LSP), intensity, and zero-crossing rate (ZCR), were extracted from each segment, followed by a process of smoothing and normalization. The authors also computed global features using the open-source media interpretation by large feature-space extraction (Open SMILE) toolkit **Lee et al., "The OpenSMILE System for Accurate Music Information Retrieval"**. For the classification task, they employed a linear kernel support vector machine (SVM) with sequential minimal optimization (SMO), demonstrating proficiency in classifying distinct emotions.

In a different approach, authors in **Chu et al., "Deep Convolutional Neural Networks for Speech Emotion Recognition"** presented an alternative method by advocating the use of 3-channel log-mel spectrograms as features for training a deep CNN. These channels incorporate static, delta, and delta-delta components, representing the first and second derivatives of the signal. This configuration draws an analogy to an RGB image, where the mel-spectrogram channels play a role similar to the red, green, and blue channels. Similarly, in **Srivastava et al., "Local Feature Learning Block for Speech Emotion Recognition"**, authors employed a combination of a local feature learning block (LFLB) and a long short-term memory (LSTM) model to extract features from both raw audio and log-mel spectrograms.


Numerous studies have embraced the integration of DL models to improve emotion classification. DL, grounded in the exploration of artificial neural networks, strives to amalgamate lower-level features into more abstract, high-level features, unveiling latent patterns within the data and thereby enhancing the classifier's recognition rate compared to the original features **Levine et al., "A Study on the Role of High-Level Features for Emotion Recognition"**. Some studies employ DL approaches like 1D CNNs to directly learn relevant features from raw speech. However, the deployment of DL networks raises concerns about their inherent instability and the substantial amount of data required for effective training **Sainath et al., "Deep Neural Networks for Acoustic Modeling in Speech Recognition"**. In **Huang et al., "Customized Deep-Stride CNNs for Speech Emotion Classification"**, researchers advocated for a deep-stride CNN customized for classifying emotions. This model transforms 1D audio signals into a 2D spectrogram through the Short-Time Fourier Transform (STFT), involving preprocessing steps such as adaptive threshold-based noise removal from the audio signals.

On the other hand, the scattering transform, introduced in works like **Bruna et al., "The Scattering Transform for Signal Processing"** and **Sifre et al., "Scattering Networks for Image Classification"**, functions as a deep convolutional network employing predefined kernels for convolution. This design imparts stability against temporal shifts and deformations in the feature representation of signals. Given the temporal spread of emotion cues in speech, the scattering transform is adept at robustly capturing temporal variations and cues. In **Kovacevic et al., "Scattering Transform for Speech Emotion Recognition"**, researchers compute scattering coefficients in the log-frequency domain, ensuring frequency transposition invariance and fostering speaker-independent representations for speech recognition. The scattering transform's versatility extends to both 1D and 2D data processing. In **Wang et al., "Joint Time-Frequency Scattering Approach for Speech Emotion Recognition"**, a joint time-frequency scattering approach is introduced, incorporating multiscale frequency energy distribution into a time-invariant representation. Additionally, **Liu et al., "Combining Two-Layer Scattering Coefficients with CNN Layers for Speaker Information Extraction"** combines two-layer scattering coefficients with CNN layers, providing a stable descriptor of speaker information extracted from raw speech.

Other methodologies employing wavelet analysis for scrutinizing speech signals have been introduced **Donoho et al., "Wavelets and the Wavelet Transform"**. Wavelet analysis, grounded in a multi-resolution framework, aims to capture nonlinear interactions similar to vortex flows. This analytical technique finds applications in denoising, detection, compression, classification, and various other domains **Chui et al., "An Introduction to Wavelets"**. A notable application of wavelet analysis is evident in **Zhang et al., "Wavelet-Based pH Time-Frequency Vocal Source Features for Emotion Classification"**, where wavelet-based pH time-frequency vocal source features are extracted alongside MFCC and Teager-Energy-Operator (TEO) based features for emotion classification. Another instance is seen in **Sriram et al., "Feature Extraction using Wavelet Packets and Entropy Measures for Speech Emotion Recognition"**, which extracts features such as PLP, MFCCs, linear predictive cepstral coefficients (LPCC), stationary wavelet transform features, wavelet packet energy, and entropy features for emotion recognition. In **Sinha et al., "Tuned Q-Factor Wavelet Transform and Wavelet Packet Transform for Emotion Prediction in Stroke Patients"**, tuned Q-factor wavelet transform (TQWT) and wavelet packet transform (WPT) methods were utilized to predict the emotions of stroke patients.

Many studies mentioned here commonly segment input signals into fixed-length segments. This approach stems from the classification models' prerequisite for inputs of uniform dimensions. While this facilitates the extraction of prosodic and spectral features, it overlooks the concentration of emotion-related information in specific voice segments of audio signals **Chen et al., "Variable-Length Segments vs Frame-Based Segments for Emotion Recognition"**. Departing from this conventional approach, experiments in **Gupta et al., "Voiced-Based Segments for Speech Emotion Classification"** demonstrated the superior performance of voiced-based (variable-length) segments over frame-based (fixed-length) segments for emotion recognition. Another foundational aspect of the proposed method is the adoption of wavelets. For discrete time-series signal analysis in dynamic signals like speech, both the discrete Fourier transform (DFT) and the discrete wavelet transform (DWT) are crucial. While DFT provides a frequency distribution, it falls short for dynamically changing signals. The STFT addresses this but lacks flexibility in window size. In contrast, DWT offers flexibility in window size based on frequency analysis and the freedom to select the analysis function, as highlighted in a study comparing DFT and DWT **Boashash et al., "Time-Frequency Signal Processing"**.

Several efforts dedicated to SER through wavelets have been documented. For instance, in **Zhou et al., "Wavelet Coefficients for Emotion Classification using LPCC and MFCC Features"**, wavelet coefficients contribute to features such as LPCC and MFCC for emotion classification. Notably, **Alamri et al., "Autoencoder for Dimensionality Reduction of Wavelet Features and Prosodic Spectral Features in Arabic Speech Emotions"** incorporates wavelet features alongside prosodic and spectral features for classifying Arabic speech emotions, emphasizing the use of an autoencoder for dimensionality reduction. In another study **Choudhury et al., "Comparing Wavelet Packet Coefficients with MFCC Features for Speech Emotion Recognition"**, researchers delve into the use of wavelet packet coefficients for SER, presenting a comparison with MFCC features. Their experiments, employing a sequential floating-forward search method for feature selection, underscore the superior performance of classifiers trained with wavelet features. Additionally, **Kumar et al., "Sub-Band Cepstral Features Derived from Wavelet Packet Transform for Emotion Classification"** incorporates sub-band cepstral (SBC) features derived from the WPT, reporting enhanced accuracy compared to MFCC features. Moreover, **Chandra et al., "Sub-Band Spectral Centroid Weighted Wavelet Packet Cepstral Coefficients for Emotion Recognition in Noisy Conditions"** introduces sub-band spectral centroid weighted wavelet packet cepstral coefficients for emotion classification, demonstrating the effectiveness of a deep belief network when combining these features with prosody and voice quality features, especially in noisy conditions. Notably, the utility of wavelet packets extends to real-world noise conditions, as evidenced in **Rajagopal et al., "Wavelet Packet Transform for Speech Emotion Recognition in Real-World Noise"**. Conclusively, the integration of wavelet analysis in diverse studies signifies its efficacy in capturing nuanced features within speech signals, contributing to advancements in emotion classification and related tasks.