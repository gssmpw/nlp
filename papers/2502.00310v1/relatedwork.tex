\section{Related Works}
\label{related_works}

Within the domain of SER, researchers have strategically combined a variety of feature extraction methods and classification models to improve recognition efficacy. This includes the incorporation of handcrafted features into traditional ML models and DL models as classifiers. Moreover, a subset of approaches adopts an E2E framework where features are autonomously extracted from the raw waveform signal of speech. This methodological diversity represents a systematic effort in SER research to optimize recognition accuracy by harnessing a spectrum of feature extraction techniques and model architectures.

In \cite{gao2017speech}, a signal segmentation methodology was introduced, using the depth first search (DFS) algorithm to determine segment duration and overlap. Local features, including pitch, mel frequency cepstral coefficients (MFCC), line spectral pairs (LSP), intensity, and zero-crossing rate (ZCR), were extracted from each segment, followed by a process of smoothing and normalization. The authors also computed global features using the open-source media interpretation by large feature-space extraction (Open SMILE) toolkit \cite{eyben2016open}. For the classification task, they employed a linear kernel support vector machine (SVM) with sequential minimal optimization (SMO), demonstrating proficiency in classifying distinct emotions.

In a different approach, authors in \cite{zhang2017speech} presented an alternative method by advocating the use of 3-channel log-mel spectrograms as features for training a deep CNN. These channels incorporate static, delta, and delta-delta components, representing the first and second derivatives of the signal. This configuration draws an analogy to an RGB image, where the mel-spectrogram channels play a role similar to the red, green, and blue channels. Similarly, in \cite{zhao2019speech}, authors employed a combination of a local feature learning block (LFLB) and a long short-term memory (LSTM) model to extract features from both raw audio and log-mel spectrograms.


Numerous studies have embraced the integration of DL models to improve emotion classification. DL, grounded in the exploration of artificial neural networks, strives to amalgamate lower-level features into more abstract, high-level features, unveiling latent patterns within the data and thereby enhancing the classifier's recognition rate compared to the original features \cite{schuller2003hidden}. Some studies employ DL approaches like 1D CNNs to directly learn relevant features from raw speech. However, the deployment of DL networks raises concerns about their inherent instability and the substantial amount of data required for effective training \cite{rolnick2017deep}. In \cite{mustaqeem2019cnn}, researchers advocated for a deep-stride CNN customized for classifying emotions. This model transforms 1D audio signals into a 2D spectrogram through the Short-Time Fourier Transform (STFT), involving preprocessing steps such as adaptive threshold-based noise removal from the audio signals.

On the other hand, the scattering transform, introduced in works like \cite{mallat2010recursive} and \cite{mallat2012group}, functions as a deep convolutional network employing predefined kernels for convolution. This design imparts stability against temporal shifts and deformations in the feature representation of signals. Given the temporal spread of emotion cues in speech, the scattering transform is adept at robustly capturing temporal variations and cues. In \cite{anden2014deep}, researchers compute scattering coefficients in the log-frequency domain, ensuring frequency transposition invariance and fostering speaker-independent representations for speech recognition. The scattering transform's versatility extends to both 1D and 2D data processing. In \cite{anden2019joint}, a joint time-frequency scattering approach is introduced, incorporating multiscale frequency energy distribution into a time-invariant representation. Additionally, \cite{ghezaiel2021hybrid} combines two-layer scattering coefficients with CNN layers, providing a stable descriptor of speaker information extracted from raw speech.

Other methodologies employing wavelet analysis for scrutinizing speech signals have been introduced \cite{silva2009discriminative}. Wavelet analysis, grounded in a multi-resolution framework, aims to capture nonlinear interactions similar to vortex flows. This analytical technique finds applications in denoising, detection, compression, classification, and various other domains \cite{rao2018discrete} \cite{daubechies1990wavelet}. A notable application of wavelet analysis is evident in \cite{zao2014time}, where wavelet-based pH time-frequency vocal source features are extracted alongside MFCC and Teager-Energy-Operator (TEO) based features for emotion classification. Another instance is seen in \cite{muthusamy2015particle}, which extracts features such as PLP, MFCCs, linear predictive cepstral coefficients (LPCC), stationary wavelet transform features, wavelet packet energy, and entropy features for emotion recognition. In \cite{zheng2018effectiveness}, tuned Q-factor wavelet transform (TQWT) and wavelet packet transform (WPT) methods were utilized to predict the emotions of stroke patients.

Many studies mentioned here commonly segment input signals into fixed-length segments. This approach stems from the classification models' prerequisite for inputs of uniform dimensions. While this facilitates the extraction of prosodic and spectral features, it overlooks the concentration of emotion-related information in specific voice segments of audio signals \cite{cowie2001emotion}. Departing from this conventional approach, experiments in \cite{mansoorizadeh2007speech} demonstrated the superior performance of voiced-based (variable-length) segments over frame-based (fixed-length) segments for emotion recognition. Another foundational aspect of the proposed method is the adoption of wavelets. For discrete time-series signal analysis in dynamic signals like speech, both the discrete Fourier transform (DFT) and the discrete wavelet transform (DWT) are crucial. While DFT provides a frequency distribution, it falls short for dynamically changing signals. The STFT addresses this but lacks flexibility in window size. In contrast, DWT offers flexibility in window size based on frequency analysis and the freedom to select the analysis function, as highlighted in a study comparing DFT and DWT \cite{steinbuch2005wavelet}.

Several efforts dedicated to SER through wavelets have been documented. For instance, in \cite{palo2018wavelet} wavelet coefficients contribute to features such as LPCC and MFCC for emotion classification. Notably, \cite{abdel2020egyptian} incorporates wavelet features alongside prosodic and spectral features for classifying Arabic speech emotions, emphasizing the use of an autoencoder for dimensionality reduction. In another study \cite{wang2020wavelet}, researchers delve into the use of wavelet packet coefficients for SER, presenting a comparison with MFCC features. Their experiments, employing a sequential floating-forward search method for feature selection, underscore the superior performance of classifiers trained with wavelet features. Additionally, \cite{kishore2013emotion} incorporates sub-band cepstral (SBC) features derived from the WPT, reporting enhanced accuracy compared to MFCC features. Moreover, \cite{huang2019feature} introduces sub-band spectral centroid weighted wavelet packet cepstral coefficients for emotion classification, demonstrating the effectiveness of a deep belief network when combining these features with prosody and voice quality features, especially in noisy conditions. Notably, the utility of wavelet packets extends to real-world noise conditions, as evidenced in \cite{vasquez2015emotion}. Conclusively, the integration of wavelet analysis in diverse studies signifies its efficacy in capturing nuanced features within speech signals, contributing to advancements in emotion classification and related tasks.