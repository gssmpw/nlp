\section{Results}\label{sec:res} 

\noindent\textbf{Privacy-aware retrosynthesis with CKIF.}
We prioritize the privacy of chemical reaction data by ensuring that sensitive information is not shared among participants. Instead of gathering raw reaction data into one single edge, CKIF supports distributed model training through the sharing of processed, non-sensitive chemical knowledge (\ie, model parameters) derived from reaction data. \redtext{CKIF operates through iterative communication rounds, each comprising two stages: local learning and chemical knowledge-informed model aggregation. During local learning, clients independently train their models on proprietary reaction data, initializing from either random value in the first round or from their personalized aggregated model from the previous communication round. In the aggregation stage, clients exchange trained model parameters to leverage collective knowledge while maintaining data privacy. The core of CKIF is its CKIW strategy, which replaces conventional fixed-weight averaging methods. When a client receives models from other participants, it evaluates their relevance using local}

\input{sec/misc/fig_overview.tex}

\clearpage

\noindent\redtext{proxy reaction data and molecular fingerprint similarity metrics. Specifically, each client computes similarities between reactants predicted by other clients' models and ground truth reactants using molecular fingerprints, generating adaptive weights that quantify the value of other clients' models for its specific learning objectives. These chemistry-aware weights guide model aggregation, creating personalized models that reflect each client's preference while benefiting from the collective knowledge of all participants.}


\input{sec/misc/tbl_50k.tex}

\noindent\textbf{Improvements with CKIF.}
We evaluate the performance of CKIF on a variety of reaction datasets. Top-\textit{K} accuracy, which indicates the proportion of ground truth reactants that appear in (exactly matched) the top-\textit{K} confident predicted reactants, is used for evaluation. Two additional metrics also reported to comprehensively evaluate models' performance. The first is Maximal fragment (MaxFrag) accuracy~\citep{tetko2020state,zhong2022root}, a relaxed version of {top-\textit{K}} accuracy, which focuses on main compound transformations. \redtext{The second is RoundTrip accuracy~\citep{chen2021deep,wan2022retroformer}, which evaluates whether the predicted reactants can indeed synthesize the target product by utilizing a pre-trained forward synthesis model as an oracle.} Tables \ref{tab:t1}, \ref{tab:t2}, and \ref{tab:t3} report the aforementioned evaluation metrics of different methods, namely, Locally Trained (models trained on their single local dataset, see Fig.~\ref{fig:overview}a), Centrally Trained (a single global model trained on all reaction datasets combined, see Fig.~\ref{fig:overview}b), FedAvg~\citep{mcmahan2017communication} (a strong, commonly-used federated baseline), and our proposed method, CKIF (which explicitly models the personalized target distribution for each client, and uses chemical knowledge-informed model aggregation to learn personalized models, see Fig.~\ref{fig:overview}cd). Note that Centrally Trained with full access to all reaction data may bring significant privacy concerns.

\input{sec/misc/tbl_mixed.tex}

The experimental results in Table~\ref{tab:t1} evidence that CKIF consistently outperforms Locally Trained across four clients sampled from USPTO-50K dataset~\citep{schneider2016s}. Taking client $C_2$ as an example, Locally Trained performs poorly with an accuracy of 4.1\% at \textit{K}=1, whereas CKIF shows a significant improvement, achieving an accuracy of 23.6\%. This substantial enhancement suggests that the proposed CKIF effectively leverages the chemical knowledge-guided model aggregation to enhance the performance over the Locally Trained baseline. In addition, CKIF exhibits comparable or even superior performance compared to the centrally trained model in several cases. For example, CKIF outperforms Centrally Trained on client $C_3$ by up to 8.8\%. This highlights the potential of CKIF in leveraging the personalized target distribution and CKIW-based model aggregation  compared to training a single global model on all reaction datasets combined. FedAvg, a strong baseline used in federated learning, treats all clients equally and aggregates updates from them to create a global model, ignoring personalized target distributions of each client. In contrast, CKIF explicitly models the non-independent and identically distributed nature for each client. As a result, CKIF demonstrates considerable performance advantages over FedAvg.




\noindent\textbf{Scalability to number of clients.}
To investigate the effect of increasing the number of clients, we conduct experiments using eight clients sampled from USPTO-50K~\citep{schneider2016s} and USPTO-MIT~\citep{jin2017predicting}. The experimental results in Table~\ref{tab:t2} demonstrate that as the number of clients increases, the performance of all clients further improves, highlighting the scalability of CKIF. For example, CKIF achieves a top-1 accuracy of 30.9\% with four participants and an improved one of 36.9\% with eight participants in terms of $C_3$. This finding suggests that the collaborative nature of CKIF enables the extraction of beneficial guidance from a larger pool of clients, leading to enhanced performance for all participating clients.


In addition, the experimental results reveal a counterintuitive decline in performance for Centrally Trained as data volume increases, challenging the common assumption that more data invariably improves performance~\citep{edunov2018understanding,sugiyama2019data}. We attribute this to three factors: lack of specialization, distribution mismatch, and bias towards majority reaction data. The centrally trained model, while capturing overall trends, may overlook client-specific characteristics. It also faces potential distribution mismatches between diverse training reaction data and distributions of individual chemical entities. In addition, reaction data from certain clients may dominate the training process, potentially biasing the model. In contrast, our CKIF maintains individual models for each client, explicitly modeling both global consistency and personalized preferences. Such a framework enables better alignment with client-specific data distributions, yielding improved performance.


\input{sec/misc/fig_type_acc.tex}

\input{sec/misc/tbl_tpl.tex}

\input{sec/misc/fig_qualitative.tex}

\noindent\textbf{Analysis of accuracy for different reaction types.} 
In the realm of chemistry, reaction classes serve as invaluable tools for chemists to navigate vast databases of reactions and extract similar members within the same class for analysis and determination of optimal reaction conditions. Moreover, reaction classes offer a concise and efficient means of communication, enabling chemists to describe the functionality and underlying mechanisms of chemical reactions in terms of atomic rearrangements. USPTO-50K dataset provides high-quality annotations which assigns one of ten reaction classes to each reaction, facilitating further investigation into the performance gains associated with specific reaction classes. From the results shown in Fig. \ref{fig:rea_type}, we can observe that CKIF outperforms the Locally Trained and FedAvg baselines by a considerable margin among all reaction classes and achieves excellent results. Also, we find that C-C bond formation is the easiest to improve among the ten reaction classes, while heteroatom alkylation and arylation is the most challenging one. The reasons are two folds: 1) C-C bond formation has more diverse possibilities for choosing reactants and reactions than other reaction classes~\citep{tetko2020state}, while the reaction data utilized by Locally Trained is insufficient to learn such transformations. 2) There are sufficient reaction data of heteroatom alkylation and arylation for Locally Trained to learn. Note that reaction classes are usually unknown in the real world, we do not utilize such information in all experiments as~\citep{zhong2022root}.



\noindent\textbf{Scalability to number of training samples.} 
Tables \ref{tab:t1} and \ref{tab:t2} demonstrate the effectiveness and scalability of the CKIF compared to several strong baselines. We can observe that some clients suffer from limited data volume, \eg, top-1 accuracy of 4.1\% on $C_2$. Such data scarcity problem can be mitigated with more reaction data in the same distribution. Although collecting data is expensive and time-consuming, one may wonder whether doing so can lead to better prediction performance. To gain insights into the performance gains of CKIF when clients have sufficient data volume and further validate scalability of CKIF, we conduct experiments on USPTO 1k TPL dataset~\citep{schwaller2021mapping}, where the clients have larger data volume. 

From the experimental results in Table \ref{tab:t3}, we have the following findings: 1) With a sufficient volume of client data, all the methods, including Locally Trained, Centrally Trained, FedAvg, and CKIF, show impressive performance compared to previous experiments. This indicates that the availability of a larger volume of data allows models to learn sufficient underlying patterns so as to enhance their retrosynthesis prediction accuracy. 2) Despite considerable performance of Locally Trained models with sufficient client data, the experimental results show that CKIF can still further improve retrosynthesis accuracy. For instance, CKIF achieves an average performance improvement of $+$6.9\% and $+$16.6\% on top-1 accuracy compared to locally and centrally trained models, respectively. In a nutshell, the experimental results with larger data volume show that CKIF can further improve the performance even when clients have sufficient data volume to train their own personalized models. CKIF consistently outperforms the Locally Trained baseline and brings performance benefits to clients with both low and high data volumes. This reaffirms the scalability and effectiveness of CKIF in leveraging chemical knowledge and personalized target distribution for enhanced retrosynthesis accuracy.



\input{sec/misc/fig_ab_exp.tex}

\noindent\textbf{Qualitative analysis.}  
To compare the qualitative results of these strong baselines and CKIF, we showcase three reactions with their characteristics (reaction types), as depicted in Fig. \ref{fig:qualitative}. Experimental results indicate that CKIF consistently outperforms the other baselines and provides plausible synthesis routes. Taking the ring formation reaction (Fig. \ref{fig:qualitative}b) as an example, the ability of CKIF to comprehend the structural requirements for forming rings leads to correct predictions, while the other baselines struggle to predict the ring formation, resulting in incorrect reactants. The superior performance of CKIF highlights the efficacy of leveraging both local (personalized model for each client) and global (client-centric weighting strategy) information to achieve accurate retrosynthetic predictions in a privacy-aware setting.



\noindent\textbf{Analysis of the chemical knowledge-informed weighting strategy.}  
To ascertain the effectiveness of our proposed CKIW strategy, we conducted a comparative analysis against a robust baseline: Average Aggregation, which uses average model aggregation instead of using the proposed chemical knowledge-informed model aggregation. The results shown in Fig.~\ref{fig:ab_exp}a demonstrate a significant enhancement; CKIF improved predictive accuracy by $+$0.93\% in terms of average top-\textit{K} accuracy. Given that this improvement is averaged over 16 metrics for 4 clients in USPTO-50K, it illustrates the considerable benefit of CKIF. This analysis highlights the substantial benefits of incorporating chemical knowledge into model aggregation, suggesting a powerful alternative to heuristic methods such as count-based model averaging.


\noindent\textbf{Sensitivity analysis.}
After verifying the effectiveness of CKIW, we conduct ablative experiments to assess the robustness of CKIF under varying learning settings. In particular, we explore the impact of two critical hyperparameters: the number of communication rounds and the number of clients. The resilience of CKIF to the number of communication rounds is tested with values set at 30, 35, 40, 45, and 50. The results, as illustrated in Fig.~\ref{fig:ab_exp}b, show a gradual improvement in model accuracy from 47.73\% to 48.84\% as the number of epochs increased. Similarly, we investigate the effect of varying the number of clients participating in the learning process. The configurations tested include 2, 3, and 4 clients, with corresponding accuracies of 13.85\%, 24.65\%, and 33.63\% (Fig.~\ref{fig:ab_exp}c). This progression demonstrates a robust scaling behavior of CKIF as the network grows, highlighting its capability to harness chemical knowledge across more clients. These findings underscore the resilience and scalability of CKIF, affirming its effectiveness across diverse experimental settings.

\input{sec/misc/fig_ab_robust.tex}

\noindent\textbf{Robustness analysis.}
\redtext{The robustness of CKIF against contaminated client data is evaluated by investigating the scenario with contamination across all participating clients. We simulate contamination by randomly exchanging reactants and products while shuffling SMILES tokens. To ensure fair comparison, the test split of all clients remains unchanged.  Results (Fig.~\ref{fig:ab_robust}) demonstrate that CKIF maintains a significant performance advantage over Locally Trained models across varying contamination levels. Even with 5$\sim$10\% contaminated data, CKIF might achieve comparable performance to that of Locally Trained models trained on clean data. The core reason is that CKIW dynamically adjusts each client's contribution during model aggregation so as to mitigate the impact of contaminated data. These findings highlight CKIF's inherent robustness to data contamination. Note that CKIF is not immune to malicious attacks such as data poisoning. While the comprehensive handling of contaminated client data and defense mechanisms against malicious attacks warrant thorough investigation, they extend beyond the scope of this work and are addressed in the Discussion section.}




