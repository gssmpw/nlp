\section{Methods}\label{sec:met}

\textbf{Problem formulation.}
In the present study, we focus in particular on the challenge of learning retrosynthesis models in a privacy-aware setting where reaction data are proprietary and cannot be shared among chemical entities (clients) or with an external central server. To address this, CKIF is specifically designed to allow for the collaborative improvement of retrosynthesis models without sharing sensitive reaction data between participants. The collaboration occurs through multiple rounds of communication where only implicit and explicit chemical knowledge---rather than raw reaction data---is exchanged. For the retrosynthesis prediction task itself, we define it as the process of training a machine learning model (or hypothesis) that maps a target molecule to its corresponding precursors (\ie, reactants).


\noindent\textbf{Privacy-aware model training.} 
The standard Transformer~\citep{vaswani2017attention} is adopted to learn a probabilistic mapping $P(y \vert x)$ from a given target molecule $x$ to reactants $y$. Suppose there are a total of $K$ clients, each possessing a local reaction dataset $\{(x_n, y_n)\}_{n=1}^{N_i}$, where $N_i$ denotes the number of reactions stored on the $i$-th client $C_i$. Our objective is to collaboratively learn a retrosynthesis model with parameters $\bm{\Theta}$, without centralizing or sharing training reaction data. In the remaining content, we first formally define the steps of each communication round of CKIF, and introduce the learning objective of retrosynthesis model. We then elaborate on the CKIW strategy which learns a personalized model for each client.

\begin{algorithm}[b]
    \caption{Procedure of the Chemical Knowledge-Informed Framework}
    \label{alg:overview}
    \begin{algorithmic}[1]
    \Require{Number of chemical entities $K$, number of communication rounds $R_T$, local epochs $R_L$, number of fine-tuning rounds $R_F$}
    \Ensure{Personalized models $\{\bm{\Theta}^{C_1}, \bm{\Theta}^{C_2}, ..., \bm{\Theta}^{C_K}\}$}
    \State Initialize models $\{\bm{\Theta}^{C_1}_0, \bm{\Theta}^{C_2}_0, ..., \bm{\Theta}^{C_K}_0\}$
    \For{$t = 1$ to $R_T$}
        \For{$k = 1$ to $K$} 
            \State $\bm{\Theta}^{C_k}_t \leftarrow \text{LocalLearning}(\bm{\Theta}^{C_k}_{t-1}, R_L)$
        \EndFor
        \State Compute $\{w_{i,k}^a\}$ with chemical knowledge guidance (Algorithm~\ref{alg:ckiw})
        \For{$i = 1$ to $K$}
            \State $\bm{\Theta}^{C_i}_t \leftarrow \sum\nolimits_{k=1}^{K} w_{i,k}^a \bm{\Theta}^{C_k}_t$
        \EndFor
    \EndFor
    \For{$k = 1$ to $K$} 
        \State $\bm{\Theta}^{C_k} \leftarrow \text{LocalLearning}(\bm{\Theta}^{C_k}_{R_T}, R_L*R_F)$
    \EndFor
    \State \Return $\{\bm{\Theta}^{C_1}, \bm{\Theta}^{C_2}, ..., \bm{\Theta}^{C_K}\}$
    
    \end{algorithmic}
\end{algorithm}



At each communication round of CKIF, three procedures are performed sequentially for all clients. Firstly, model parameters generated by the previous round are used for parameter initialization (random initialization for the first round). Secondly, the initialized models of each client are trained locally on their own reaction data for a fixed number of epochs (or iterations) to minimize the following objective function:
\begin{equation}
    \mathcal{L} = -\log P(y \vert x;\bm{\Theta}).
\end{equation}
Once trained, each client $C_i$ sends their parameters $\bm{\Theta}^{C_i}$ to the central server. Thirdly, after collection, these parameters are aggregated (\ie, weighted sum) to a new global model by the central server: $\bm{\Theta}^{G} = \sum_{k=1}^{K} w_k \bm{\Theta}^{C_k}$, where $w_k$ is the weighting factor for client $C_k$. As one of the most widely used methods in federated learning, FedAvg~\citep{mcmahan2017communication} defines the weighting factor for $C_i$ as the proportion of training samples: $w_i = N_i / \big(\sum_{k=1}^{K}N_k\big)$.


Despite being prevalent, mainstream federated learning methods like FedAvg struggle to handle data heterogeneity~\citep{wang2019federated,huang2021personalized} --- one common characteristic of chemical entities, where reaction data often reflects specialized research focuses or industrial interests. To address this, CKIF further investigates the problem of privacy-preserving retrosynthesis learning with a focus on client-specific needs. Concretely, CKIF enables each chemical entity to have a personalized model that adapts to their own data distribution and preferences, while also benefiting from the chemical knowledge of other clients by only exchanging model parameters. Consequently, instead of performing a standard aggregation of global parameters $\bm{\Theta}^{G}$ at each communication round, CKIF redefines the aggregation process to create a personalized model for each client (see Algorithm~\ref{alg:overview}). For instance, the personalized model for client $C_i$ is obtained by:
\begin{equation}
     \bm{\Theta}^{C_i} = \sum\nolimits_{k=1}^{K} w_{i,k}^a \bm{\Theta}^{C_k},
\end{equation}
where $w_{i,k}^a$ denotes the \underline{a}daptive weighting factor power by the proposed CKIW strategy (Algorithm~\ref{alg:ckiw}). $w_{i,k}^a$ quantifies the importance of models from other clients $C_k$ relative to the given client $C_i$, and is normalized as:
\begin{equation}
    w_{i,k}^a=\begin{cases}\mu,& i = k \\
    (1-\mu)\frac{\exp(s_{i,k} / \tau)}{\sum_{j=1,i\neq j}^{K}\exp(s_{i,j}/ \tau)},& i \neq k\end{cases},
\end{equation}
where $\mu$ and $\tau$ are hyperparameters that control the influence of self's model $\bm{\Theta}^{C_i}$ and neighboring clients' models $\bm{\Theta}^{C_k}$. \redtext{$s_{i,k}$ represents the similarity score between the reactants predicted by $\bm{\Theta}^{C_k}$ and the annotated ones stored in $C_i$.} Note that the process of measuring similarity involves using proxy reaction data, which are stored securely and processed locally on each client. During the collaborative training process, only the model parameters are communicated. In our experiment, all reaction data in the validation set are chosen as the proxy reaction data. Clients can optionally provide additional proxy reaction data for improved measurement.

\begin{algorithm}[t]
    \caption{Procedure of the Chemical Knowledge-Informed Weighting Strategy}
    \label{alg:ckiw}
    \begin{algorithmic}[1]
    \Require{Client models $\{\bm{\Theta}^{C_1}, \bm{\Theta}^{C_2}, ..., \bm{\Theta}^{C_K}\}$, proxy reaction data $\{D_1, D_2, ..., D_K\}$, hyperparameters $\mu$, $\tau$}
    \Ensure{Adaptive weighting factors $\{w_{i,k}^a\}$}
    \For{$i = 1$ to $K$}
        \For{$k = 1$ to $K$}
        \Comment{\redtext{Client $C_i$ evaluates model $\bm{\Theta}^{C_k}$ locally on its own proxy data without sharing}}
            \If{$i = k$}
                \State $w_{i,k}^a \leftarrow \mu$
            \Else
                \State $s_{i,k} \leftarrow 0$
                \For{$(x, y) \in D_i$}
                    \State $\hat{y} \leftarrow \text{PredictReactants}(x, \bm{\Theta}^{C_k})$
                    \State $f_y \leftarrow \text{ExtractFingerprint}(y)$
                    \State $f_{\hat{y}} \leftarrow \text{ExtractFingerprint}(\hat{y})$
                    \State $s_{i,k} \leftarrow s_{i,k} + \text{MolecularSimilarity}(f_y, f_{\hat{y}})$
                \EndFor
                \State $s_{i,k} \leftarrow s_{i,k} / |D_i|$
            \EndIf
        \EndFor
        \For{$k = 1$ to $K$, $k \neq i$}
            \State $w_{i,k}^a \leftarrow (1-\mu)\frac{\exp(s_{i,k} / \tau)}{\sum_{j=1,j\neq i}^{K}\exp(s_{i,j}/ \tau)}$
        \EndFor
    \EndFor
    \State \Return $\{w_{i,k}^a\}$
    \end{algorithmic}
\end{algorithm}


\redtext{In the above distributed training procedure, the strategy used to assign weights to individual models during aggregation is of great importance~\citep{zhang2021personalized} for the performance, convergence, \etc. Generic weighting strategies, such as count-based averaging, rely on handcrafted metrics. Such heuristic methods have significant limitations:} \redtext{they ignore the potential heterogeneity in data distributions across different chemical entities. This raises a fundamental question: how to compute meaningful weights $s_{i,k}$ that quantify how valuable one client's model is for another client's learning objectives? To address this, CKIF is equipped with the CKIW strategy that harnesses the explicit, symbolic chemical knowledge---specifically, molecule fingerprints like ECFP~\citep{rogers2010extended} and MACCS keys~\citep{durant2002reoptimization}.} By calculating the molecule similarity between predicted and actual reactants, CKIW assesses the model's effectiveness (\ie, $s_{i,k}$) across reaction data of different clients. The computation procedure of $s_{i,k}$ is detailed as follows. For each reaction pair $(x,y)$ stored on the validation set of $C_i$, we first get the predicted reactants $\hat{y}$ through the personalized model $\bm{\Theta}^{C_k}$. Then, molecule fingerprints~\citep{rogers2010extended,durant2002reoptimization} of $y$ and $\hat{y}$ are extracted by RDKit~\citep{landrum2013rdkit}. Thereby, we can measure the similarity of the molecules by analyzing the molecule fingerprints with established measurement methods~\citep{bajusz2015tanimoto}. The selection of measurement methods and molecule fingerprints is flexible, allowing CKIW to take advantage of advancements in these areas. Finally, we can obtain $s_{i,k}$ by averaging molecule similarities over all pairs in the proxy reaction data. \redtext{ In summary, $s_{i,k}$ represents the overall performance of $\bm{\Theta}^{C_k}$ on ${C_i}$'s proxy data, with higher values indicating greater similarity between the clients' data distributions. Note that throughout the aggregation process, each client $C_i$ only uses their own proxy data locally to evaluate other clients' models. The only information shared between clients are the model parameters $\bm{\Theta}^{C_k}$, ensuring that private reaction data remains protected.}



\noindent\textbf{Implementation details and hyperparameters.} We formulate retrosynthesis as a sequence-to-sequence~\citep{sutskever2014sequence} problem where each element in the sequence is a SMILES token. \redtext{We use a standardization protocol~\citep{wan2022retroformer,chen2021deep,schwaller2019molecular} using RDKit's canonical SMILES representation (rdkit.Chem.MolToSmiles with canonical=True). This standardization step is applied before fingerprint calculation and model training, ensuring consistent molecular representation across all clients.} A standard Transformer~\citep{vaswani2017attention} composed of an encoder-decoder architecture is adopted as the retrosynthesis model. \redtext{All the baseline models use the same transformer architecture.} Specifically, the adopted transformer consists of a 6-layer encoder and a 6-layer decoder, and the number of attention heads is set to 8. We train the Personalized and centrally trained models for 250 epochs with a batch size of 64. We employ an Adam optimizer~\citep{kingma2014adam} with a learning rate of 0.0002, $\beta_1= 0.9$, and $\beta_2= 0.998$. Dropout~\citep{srivastava2014dropout} is applied to the whole model to avoid overfitting. For CKIF,  we trained the models for $R_T$ rounds, each with $R_L$ local epochs with a batch size of 64. Inspired by~\citep{wang2019federated,yu2020salvaging}, in the last $R_F$ rounds, we perform local fine-tuning instead of model aggregation. For all experiments, $R_T$, $R_L$, and $R_F$ are set to 50, 5, and 10, respectively. The hyperparameters $\mu$ and $\tau$ are set to be $1 / K $ and 1.5, respectively. MACCS keys~\citep{durant2002reoptimization} and Tanimoto similarity~\citep{bajusz2015tanimoto} are employed to measure the similarity of molecules. OpenNMT-py~\citep{klein2017opennmt}, an open-source neural machine translation framework in PyTorch~\citep{paszke2019pytorch}, is adopted to build our models.

\noindent\textbf{Dataset collection.} 
Our CKIF is evaluated on a variety of federated datasets sampled from U.S. patent database curated by~\citep{lowe2012extraction,Lowe2017} for retrosynthesis prediction and reaction classification: USPTO-50K~\citep{schneider2016s}, USPTO-MIT~\citep{jin2017predicting}, and USPTO 1k TPL~\citep{schwaller2021mapping}.
\begin{itemize}
    \item USPTO-50K is a high-quality reaction dataset comprising approximately $50,000$ reactions, each with manually annotated reaction type. We split the dataset in the same way as~\citep{liu2017retrosynthetic,dai2019retrosynthesis}. Five clients are obtained according to the reaction classes. In addition, the reactions with chirality (a property of asymmetry) compose another client.
    \item USPTO-MIT is a larger dataset which contains approximately $470,000$ reactions. We split the datasets in the same way as~\citep{jin2017predicting}. Two clients are obtained according to the reaction characteristics, \ie, ring opening and ring formation.
    \item USPTO 1k TPL is a reaction dataset containing the 1,000 most common reaction templates as classes, which is built by~\citep{schwaller2021mapping} for reaction classification. Eight clients are obtained according to the reaction classes (templates). Since reaction classes or reaction templates are usually unknown in the real world, we do not utilize such information in all experiments as~\citep{zhong2022root}.
\end{itemize}
\noindent An overview of the datasets used for training and evaluation, including data split and statistics, is shown in Extended Data Table 1.

\noindent\textbf{Pre- and post-processing.} 
For all chemical reactions from datasets, reactants and products are extracted to form paired reaction data. Following previous works~\citep{segler2017neural,dai2019retrosynthesis,wan2022retroformer}, other factors, such as reagents and edge products, are ignored in all experiments. We focus on single-step retrosynthesis~\citep{shi2020graph,yan2020retroxpert,somnath2021learning,wang2021retroprime}, where reactions containing multiple products are split into individual reactions to ensure that each reaction exclusively featured a single product. Canonical SMILES~\citep{weininger1989smiles} is used to represent molecules in all experiments. For post-processing, we use beam search~\citep{freitag2017beam} to systematically explore potential reactant combinations. The ranking of reactant candidates was performed based on the models' confidence scores, which reflected the likelihood of a given reactant combination leading to the desired product.

\noindent\textbf{Evaluation metrics.} 
We use top-\textit{K} ($\textit{K} = \{1,3,5,10\}$) retrosynthesis accuracy, the most widely metric used among current literatures~\citep{liu2017retrosynthetic,karpov2019transformer,kim2021valid,sacha2021molecule,wang2021retroprime,zhong2022root,zhong2023retrosynthesis}, which indicates the proportion of ground truth reactants that appear (exactly matched) among the top-\textit{K} confident predicted reactants, to evaluate the performance on each client. For comprehensive evaluation, we additionally report MaxFrag accuracy~\citep{tetko2020state} which focuses on main compound transformations, \redtext{and RoundTrip accuracy~\citep{chen2021deep,wan2022retroformer} which evaluates whether the predicted reactants can indeed synthesize the target product}.
