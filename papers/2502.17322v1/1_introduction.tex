\section{Introduction}
Humanoid robots with dexterous hands have vast and promising application scenarios due to their behavior flexibility and human-like morphology \citep{bonci2021human, stasse2019overview, choudhury2018humanoid}.
Unfortunately, such complex high-dimensional space is extremely challenging for policy learning with online reinforcement learning (RL) \citep{sferrazza2024humanoidbench, peters2003reinforcement}, which interactively explores the environment and learns optimal decision-making from scratch under the guidance of reward function.
This paradigm has achieved significant success in fields like gaming AI \citep{silver2016mastering, silver2017mastering, schrittwieser2020mastering, hessel2018rainbow} and quadrupedal robots control \citep{miki2022learning, lee2020learning, hwangbo2019learning}.
But when facing humanoid robots equipped with dexterous hands, existing RL methods struggle to learn effectively and efficiently. 
Even sample-efficient model-based state-of-the-art (SOTA) algorithms, such as TD-MPC2 \citep{hansen2023td} and DreamerV3 \citep{hafner2023mastering}, perform poorly in humanoid control.

\begin{figure}[t]
    \centering
    \vspace{4pt}
    \includegraphics[width=0.98\linewidth]{introduction_fig.pdf}
    \caption{Tasks accomplished by TDMPBC: 1) navigating through pole-filled areas by staying close to the wall, 2) maintain balance on unstable board with the spherical pivot beneath the board in motion, 3) window cleaning with arm-controled cleaning tools and 4) achieving a successful basketball shot.}
    \label{fig:main}
    \vspace{-16pt}
\end{figure}

In high-dimensional complex spaces, the regions capable of accomplishing tasks are typically exceedingly narrow and difficult to explore compared to the entire space. 
For humanoid robot motion control, upright posture is a prerequisite to complete any downstream tasks.
Maintaining an upright posture is similar to balancing an inverted pendulum, where only an extremely small vertical region within the entire space can sustain this posture, while other regions lead to rapid falls.
Therefore, when the algorithm explores an upright posture, the humanoid robot should place particular emphasis on it.

Furthermore, upright posture can be intuitively reflected in return. 
Only if the current timestep maintains an upright posture is it possible to continue obtaining rewards in the following timesteps. 
If the current step results in a fall, given that humanoid robots are virtually incapable of standing up after falling, subsequent rewards become unattainable. 
Under the cumulative effect, the ability to maintain an upright posture will ultimately be reflected very prominently in the return. 
To summarize, the return can be approximated as an indicator of whether the humanoid robot has entered a task-completing region in its control.

Based on the above observation and analysis, we propose a framework called \textbf{S}elf-\textbf{I}mitative \textbf{R}einforcement \textbf{L}earning (SIRL) to assist online learning in complex high-dimensional humanoid robot control.
Building upon the foundation of RL algorithms, SIRL additionally imitates trajectories with high returns.
This enables the humanoid robot to quickly learn the upright posture, thereby accelerating the completion of downstream tasks.
Specifically, we augment the policy training objective in TD-MPC2 \citep{hansen2023td, hansen2022temporal} with an additional behavior cloning term whose weight is dynamically adjusted based on the trajectory return. 
Since the trajectories being imitated are generated by the algorithm itself during exploration, rather than expert demonstrations as in traditional imitation learning (IL), our framework is termed self-imitative. 
Additionally, we refer to TD-MPC2 augmented with a behavior cloning loss as TDMPBC.


\begin{figure}[t]
  \centering
    \includegraphics[width=\linewidth]{results_summary.pdf}
    \vspace{-16pt}
  \caption{Performance of TDMPBC with 2M interaction steps compared to the baselines TD-MPC2 with 2M, DreamerV3 with 10M and SAC with 10M on HumanoidBench. }
  \label{fig:compare}
  \vspace{-16pt}
\end{figure}

We have validated our proposed TDMPBC on HumanoidBench \citep{sferrazza2024humanoidbench} which contains 31 challenging tasks for the Unitree H1 robot with dexterous hands.
Compared with the baseline TD-MPC2, our proposed method achieves an approximate increase more than 120\% for the normalized return\footnote{We normalize the return to the range $[0, R_{\text{target}}]$. For the \texttt{push} and \texttt{package} manipulation tasks, where the return may be negative, we do not display them in Figure \ref{fig:compare}. The performance of TDMPBC on these two tasks is on par with the baseline, which does not affect the above conclusions we have drawn.}.
What's more, our method enjoys a significantly faster convergence rate and excels in terms of sample-efficiency.
In humanoid locomotion tasks, our algorithm is capable of completing 8 tasks out of 14 with only 2M training steps, whereas the baseline could only accomplish 1 task.