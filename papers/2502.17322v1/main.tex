\documentclass[twocolumn]{article}

\usepackage{arxiv}


\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

\usepackage{hyperref}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
% \usepackage[table,dvipsnames]{xcolor} % Allows coloring of table rows and columns
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{xcolor}
\usepackage{natbib}
\usepackage{authblk}
% \usepackage[table]{xcolor}  % Enables coloring of table cells
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{pifont}


\title{TDMPBC: Self-Imitative Reinforcement Learning for Humanoid Robot Control}

\author{
    Zifeng Zhuang$^{1*}$ \quad
    Diyuan Shi$^{1*}$ \quad
    Runze Suo$^{1}$ \quad
    Xiao He$^{1}$ \quad
    Hongyin Zhang$^{1}$ \protect\\
    Ting Wang$^{1\dag}$ \quad
    Shangke Lyu$^{1\dag}$ \quad
    Donglin Wang$^{1\dag}$ \\
    $^{1}$Westlake University \quad
    $^{*}$Equal Contribution \quad
    $^{\dag}$Corresponding Authors \\
}

\begin{document}


\twocolumn[{
  \begin{@twocolumnfalse}

	\maketitle

	\begin{abstract}
            Complex high-dimensional spaces with high Degree-of-Freedom and complicated action spaces, such as humanoid robots equipped with dexterous hands, pose significant challenges for reinforcement learning (RL) algorithms, which need to wisely balance exploration and exploitation under limited sample budgets.
            In general, feasible regions for accomplishing tasks within complex high-dimensional spaces are exceedingly narrow. 
            For instance, in the context of humanoid robot motion control, the vast majority of space corresponds to falling, while only a minuscule fraction corresponds to standing upright, which is conducive to the completion of downstream tasks.
            Once the robot explores into a potentially task-relevant region, it should place greater emphasis on the data within that region.
            Building on this insight, we propose the \textbf{S}elf-\textbf{I}mitative \textbf{R}einforcement \textbf{L}earning (\textbf{SIRL}) framework, where the RL algorithm also imitates potentially task-relevant trajectories.
            Specifically, trajectory return is utilized to determine its relevance to the task and an additional behavior cloning is adopted whose weight is dynamically adjusted based on the trajectory return.
            As a result, our proposed algorithm achieves 120\% performance improvement on the challenging HumanoidBench with 5\% extra computation overhead. 
            With further visualization, we find the significant performance gain does lead to meaningful behavior improvement that several tasks are solved successfully.
	\end{abstract}

  \end{@twocolumnfalse}
  \vspace{1em}
}]


\input{1_introduction}

\input{2_preliminary}

\input{3_method}

\input{4_related_work}

\input{5_experiment}

\input{6_conculsion}
    


\bibliographystyle{unsrtnat}
\bibliography{main}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\input{7_appendix}


\end{document}

