\section{Related Work}
Behavior control for Humanoid robots is a long-standing problem, initially explored with simplified humanoid agent \citep{tunyasuvunakool2020dm_control} and recently with full-size humanoid robot \citep{zhuang2024humanoid,fu2024humanplus} such as Unitree H1.
Humanoid robots are of particular interest to the reinforcement learning community because of the high-dimensional action space \citep{merel2017learning, hansen2022temporal, hansen2023td, hansen2024hierarchical}.
To overcome the challenges of exploration in high-dimensional action spaces, some algorithms learn policies by imitating human behavior \citep{fu2024humanplus} or enhance exploration through massive parallelization \citep{zhuang2024humanoid}.
In contrast, our proposed algorithm attempts to learn from scratch without the aid of massive parallelization \citep{makoviychuk2021isaac}. 
We have extensively evaluated our algorithm on the HumanoidBench \citep{sferrazza2024humanoidbench}, a benchmark built on humanoid robot with dexterous hands \citep{menagerie2022github} that contains not only  14 locomotion tasks but also  17 whole-body manipulation tasks.
In the LocoMujoco \citep{al2023locomujoco}, the H1 robot is not equipped with dexterous hands and only focus on locomotion tasks.

Confronted with tasks involving high-dimensional action spaces, model-based RL algorithms \citep{ha2018recurrent, hansen2022temporal, hafner2023mastering, hafner2019dream} often prove to be more sample-efficient compared to model-free alternatives \citep{haarnoja2018soft, fujimoto2018addressing}. 
However, when it comes to humanoid robots with dexterous hands, even the SOTA model-based algorithms struggle to solve it \citep{sferrazza2024humanoidbench}. 
Our algorithm integrates the concept of imitation learning \citep{liu2023ceil, zhang2024context} with the reinforcement learning framework, introducing a loss term of behavioral cloning \citep{pomerleau1988alvinn}.  It may bear a  resemblance to the offline RL \citep{zhuang2024reinformer, fujimoto2019off} algorithm TD3+BC \citep{fujimoto2021minimalist} but our problem setting is   completely different to theirs.
Additionally, it should be noted that the SIRL framework is fundamentally an online RL paradigm that does not rely on expert data, different from IBRL \citep{hu2023imitation} or MoDem \citep{hansen2022modem}.

