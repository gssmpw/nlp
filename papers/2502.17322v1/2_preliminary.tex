\section{Preliminaries}
\paragraph{Reinforcement Learning} Reinforcement Learning (RL) is a framework of sequential decision.
Typically, this problem is formulated by a Markov Decision Process (MDP) $\mathcal{M}=\{\mathcal{S},\mathcal{A},r,p,d_0,\gamma\}$, with state space $\mathcal{S}$, action space $\mathcal{A}$, scalar reward function $r$, transition dynamics $p$, initial state distribution $d_0(\mathbf{s}_0)$ and discount factor $\gamma$ \citep{sutton1998introduction}.
The objective of RL is to learn a policy $\pi\left(\mathbf{a}_t|\mathbf{s}_t\right)$ at timestep $t$, where $\mathbf{a}_t \in \mathcal{A}$ and $\mathbf{s}_t \in \mathcal{S}$.
Given this definition, the distribution of trajectory $\tau=\left(\mathbf{s}_0, \mathbf{a}_0, \cdots, \mathbf{s}_H, \mathbf{a}_H\right)$ generated by the interaction with the environment $\mathcal{M}$ is $P_{\pi}\left(\tau\right) = d_0(\mathbf{s}_0) \prod_{t=0}^{T} \pi\left(\mathbf{a}_t|\mathbf{s}_t\right) p\left(\mathbf{s}_{t+1}|\mathbf{s}_t,\mathbf{a}_t\right)$,
where $T$ is the length of the trajectory and can be infinite. Then, the goal of RL can be written as an expectation under the trajectory distribution $J\left(\pi\right) = \mathbb{E}_{\tau \sim P_{\pi}\left(\tau\right)}\left[ \sum_{t=0}^{T} \gamma^t r(\mathbf{s}_t, \mathbf{a}_t)\right]$. 
This objective can also be measured by a value function $Q_{\pi}\left(\mathbf{s},\mathbf{a}\right)$, the expected discounted return given the action $\mathbf{a}$ in state $\mathbf{s}$: $Q_{\pi}\left(\mathbf{s},\mathbf{a}\right) = \mathbb{E}_{\tau \sim P_{\pi}\left(\tau|\mathbf{s},\mathbf{a}\right)}\left[ \sum_{t=0}^{T} \gamma^t r(\mathbf{s}_t,\mathbf{a}_t)|\mathbf{s}_0=\mathbf{s},\mathbf{a}_0=\mathbf{a}\right]$. 

\paragraph{TD-MPC2} Model-based RL algorithm TD-MPC2 learns a latent decoder-free world model and selects actions during inference via planning with learned model \citep{hansen2023td}. 
Specifically, TD-MPC2 consists of five components:
\begin{align}
    \nonumber\text{State Encoder:}\qquad\qquad\;\:\mathbf{z}_t &= h_{\phi}\left(\mathbf{s}_t\right), \\
    \nonumber\text{Latent Dynamics:}\qquad\:\mathbf{z}_{t+1} &= d_{\phi}\left(\mathbf{z}_t, \mathbf{a}_t\right), \\
    \nonumber\text{Reward Function:}\qquad\quad \;\hat{r}_t &= R_{\phi}\left(\mathbf{z}_t, \mathbf{a}_t\right), \\
    \nonumber\text{Value Function:}\qquad\quad\enspace\:\: \,\hat{q}_t &= Q_{\phi}\left(\mathbf{z}_t, \mathbf{a}_t\right), \\
    \nonumber\text{Policy Prior:}\qquad\qquad\quad\:\hat{\mathbf{a}}_t &\sim \pi_{\theta}\left(\cdot|\mathbf{z}_t\right),
\end{align}
where $\mathbf{s}_t$ is the states, $\mathbf{a}_t$ is the actions and $\mathbf{z}_t$ is the latent representation. The encoder $h_{\phi}$, dynamics $d_{\phi}$, reward $R_{\phi}$, value $Q_{\phi}$ compose the world model in TD-MPC2 that is trained by minimizing the following objective: 
\begin{align}
    \nonumber \mathcal{L}\left(\phi\right) &= \mathbb{E}_{\left(\mathbf{s}_t, \mathbf{a}_t, r_t, \mathbf{s}_{t+1}\right)_{t=0}^{H} \sim \mathcal{B}}
    \Bigg[\sum_{t=0}^{H}\lambda^t\Big(l^{\left(t\right)}_d+l^{\left(t\right)}_r+l^{\left(t\right)}_q\Big)\Bigg], \\ 
    \nonumber l^{\left(t\right)}_d &= \left\|d_{\phi}\left(\mathbf{z}_t, \mathbf{a}_t\right) - \bar{\mathbf{z}}_{t+1}\right\|_2^2,  \\ 
    \nonumber l^{\left(t\right)}_r &= \text{CE}\Big[R_{\phi}\left(\mathbf{z}_t, \mathbf{a}_t\right) - r_t\Big], \\ 
    \nonumber l^{\left(t\right)}_q &= \text{CE}\bigg[Q_{\phi}\left(\mathbf{z}_t, \mathbf{a}_t\right)-\Big(r_t + \gamma Q_{\bar{\phi}}\big(\mathbf{z}_{t+1}, \pi_{\theta}\left(\cdot|\mathbf{z}_{t+1}\right)\big) \Big)\bigg],
\end{align}
where $\left(\mathbf{s}_t, \mathbf{a}_t, r_t, \mathbf{s}_{t+1}\right)_{t=0}^{H}$ is a trajectory with length $H$ sampled from the replay buffer $\mathcal{B}$, 
$\bar{\mathbf{z}}_{t+1} = h_{\bar{\phi}}\left(\mathbf{s}_{t+1}\right)$ is the target latent representation and CE is the cross-entropy loss. The policy prior $\pi$ is a stochastic maximum entropy policy that learns to maximize the objective: 
\begin{align}
    \nonumber \mathcal{L}\left(\theta\right) = \mathbb{E}_{\mathbf{s}_{t=0}^{H} \sim \mathcal{B}} \Bigg[\sum_{t=0}^{H}\lambda^t\Big[Q_{\phi}\left(\mathbf{z}_t, \pi_{\theta}\left(\cdot|\mathbf{z}_t\right)\right) - \alpha \mathcal{H}\left(\pi\left(\cdot|\mathbf{z}_t\right)\right)\Big]\Bigg],
\end{align}
where $\mathcal{H}$ is the entropy of policy $\pi$ and the parameter $\alpha$ can be automatically adjusted based on an entropy target \citep{haarnoja2018soft} or moving statistics \citep{hafner2023mastering}.

During inference, TD-MPC2 plan actions using a sampling-based planner Model Predictive Path Integral (MPPI) \citep{williams2015model} to iteratively fits a time-dependent multivariate Gaussian with diagonal covariance over the trajectory space such that the estimated return $\hat{\mathcal{R}}$ is maximized:
\begin{align}
    \hat{\mathcal{R}} = \sum_{t=0}^{H-1}\gamma^t R_{\phi}\left(\mathbf{z}_t, \mathbf{a}_t\right) + \gamma^H Q_{\phi}\left(\mathbf{z}_t, \mathbf{a}_t\right).
\end{align}
To accelerate this planning, a fraction of trajectories are generated by the learned policy prior $\pi_{\theta}$.

\paragraph{Imitation Learning} In Imitation Learning (IL) \citep{zare2024survey}, the ground truth reward is not observed and only a set of demonstrations $\mathcal{D}=\left\{\left(\mathbf{s}_t,\mathbf{a}_t\right)\right\}$ collected by the expert policy is provided. The goal of IL is to recover a policy that matches the expert. Behavior cloning \citep{pomerleau1988alvinn} is the most simple and straightforward approach
\begin{align}
    \nonumber\pi_{\text{BC}} = \underset{\pi}{\operatorname{argmax}} \ \mathbb{E}_{\left(\mathbf{s}_t,\mathbf{a}_t\right) \sim \mathcal{D}}\left[\operatorname{log}\pi\left(\mathbf{a}_t|\mathbf{s}_t\right)\right].
\end{align}
