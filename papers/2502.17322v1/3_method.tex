\section{Self-Imitative Reinforcement Learning}
In this section, we first highlight the relationship between the motion control of humanoid robots and the upright posture. 
We further observe that maintaining an upright posture corresponds to higher returns. 
Building on this insight, we introduce \textbf{S}elf-\textbf{I}mitative \textbf{R}einforcement \textbf{L}earning (\textbf{SIRL}) and present the implementation based on the model-based algorithm TD-MPC2 \citep{hansen2023td}. 
During the online reinforcement learning process, \textbf{SIRL} provides additional guidance to the humanoid robot to imitate trajectories with high returns. 
Finally, we analyze the characteristics and applicability of this framework from multiple perspectives.

\subsection{Motivation and Analysis}

In humanoid robot motion control, stable upright posture is the essential foundation for both locomotion tasks and whole-body manipulation tasks. 
This point is not only intuitive but also can be reflected in the design of the reward function. 
For example, in Isaac Lab, the reward term related to maintaining upright posture is assigned one extremely high weight:
\begin{align}
    r_1 = 200 \times r_{\text{upright}} + 1.0 \times r_{\text{velocity}} + \cdots,
\end{align}
where $r_{\text{upright}}$ represents an abstraction of all the reward terms associated with maintaining an upright posture, rather than a specific reward term. 
While the term $r_{\text{velocity}}$ rewards velocity tracking.
When tracking different desired velocities, such as 0 m/s, 1 m/s and 5m/s, each corresponds to distinct tasks, namely \texttt{stand}, \texttt{walk}, and \texttt{run}.

Other reward terms are omitted for simplicity. 
In other environments, such as HumanoidBench \citep{sferrazza2024humanoidbench}, 
the upright term $r_{\text{upright}}$ serves as a weight to affect the value of the entire reward function: 
\begin{align}
   r_2 = r_{\text{upright}} \times \left(r_{\text{velocity}} + \cdots\right).
\end{align}
Here $r_{\text{upright}} \in [0, 1]$. 
Intuitively, the upright state with $r_{\text{upright}}=1$ should only occupy an extremely narrow region in the whole state-action space. 
In contrast, the vast majority of the space belongs to $r_{\text{upright}}=0$.
Therefore, once the algorithm explores into the region where an upright posture can be maintained, it should place particular emphasis on it. 
After all, maintaining an upright posture facilitates further exploration and learning for downstream tasks. 
The next question is how to determine whether the upright posture has been explored in the control tasks of humanoid robots.


In reality, for humanoid robots, only standing upright $r_{\text{upright}}=1$ and falling down $r_{\text{upright}}=0$ are stable states. 
Other postures, such as $r_{\text{upright}}=0.6$, will eventually evolve into the stable state of falling. 
Moreover, once a humanoid robot falls, it cannot recover to a standing state on its own, which means that subsequent rewards become unattainable. 
Under the cumulative effect, the ability to maintain a stable standing posture will ultimately be very prominently reflected in the return. We further illustrate this phenomenon with the following example.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{vio.pdf}
    \caption{The first row presents the return mean of the trajectories obtained by evaluating the policy trained for 100000 steps on the task \texttt{run}, along with the violin plot distribution of $r_{\text{upright}}$ across all timesteps. The second row shows the results obtained after training for 300000 steps.}
    \label{fig:vio}
\end{figure}

From the Figure \ref{fig:vio}, we observe that the distribution of $r_{\text{upright}}$ obtained from policies trained for 100000 and 300000 steps exhibit relatively small differences. 
But the final returns show a significant disparity (22.3 v.s. 271.4). 
This discrepancy arises because minor differences in whether the humanoid remains upright at each timestep are cumulatively amplified over multiple timesteps.

Overall, the motion control of humanoid robots is closely related to maintaining an upright posture, and whether or not the robot remains upright can lead to a substantial difference in final return.
Based on this finding, an idea for accelerating or assisting humanoid robots emerges: during the online reinforcement learning (RL) exploration process, we can provide additional guidance to the robot to imitate trajectories with high returns. 
This approach enables the robot to first learn how to maintain an upright posture, which then serves as a foundation for completing the entire task.

\subsection{Framework and Methods}
\label{impl_details}
Now we introduce the concept of Self-Imitative Reinforcement Learning (SIRL) which aims to accelerate the learning process of humanoid robots.
During the online exploration process, in addition to the basic RL loss, the policy $\pi_{\theta}$ is also required to imitate trajectories with high returns stored in the replay buffer. 
Unlike classical imitation learning, where trajectories are typically provided by an expert, the trajectories imitated here are generated by the policy \textbf{itself}. 
That is why we call the framework as ``self-imitative''.

We have implemented the SIRL framework based on the TD-MPC2 algorithm \citep{hansen2023td}. 
Specifically, only the policy training loss function is modified and the difference is highlighted by \textcolor{red}{red}:
\begin{align}\label{eq:loss}
    \nonumber \mathcal{L}_{\pi}\left(\theta\right) = &\mathbb{E}_{\left(\mathbf{s}_t, \mathbf{a}_t, \textcolor{red}{R_t}\right)_{t=0}^{H} \sim \mathcal{B}} \Bigg[\sum_{t=0}^{H}\lambda^t\Big[
    \textcolor{red}{\underbrace{\omega\left(R_t\right)\cdot\log \pi_{\theta}\left(\mathbf{a}_t|\mathbf{z}_t\right)
    }_{\text{Self-Imitative}}}
     \Big.\Bigg.\\
    &+\Big.\Bigg. \underbrace{Q_{\phi}\left(\mathbf{z}_t, \pi_{\theta}\left(\cdot|\mathbf{z}_t\right)\right) - \alpha\mathcal{H}\left(\pi\left(\cdot|\mathbf{z}_t\right)\right)}_{\text{Reinforcement Learning}}\Big]\Bigg].
\end{align}
Here $R_t=\sum_{t^{\prime}=0}^{T} \gamma^t r(\mathbf{s}_t, \mathbf{a}_t)$ is the return of the 
whole trajectory and all the $\mathbf{s}_t, \mathbf{a}_t$ within this trajectory have the same return $R_t$.
Compared to the original RL loss function, Another behavior cloning loss is introduced with the weight
\begin{align}
    \omega\left(R_t\right) = \beta \cdot \exp{\left(\frac{R_t - G}{G}\right)},
\end{align}
where $G$ is a reference return value used to determine the level of the current return $R_t$.
Ideally, $G=R_{\text{target}}$ should be the target return, the standard for determining success or failure. 
However, this approach requires the introduction of additional prior information, which may affect the generality of the algorithm. 
Alternatively, we propose using the maximum return $R_{\text{max}}$ of the trajectories in the current replay buffer as the reference standard. 

\subsection{Discussion and Analysis}
From the implementation perspective, our proposed algorithm can be viewed as TD-MPC2 + BC, which might seem similar to the offline algorithm TD3+BC \citep{fujimoto2021minimalist}. 
However, the scenarios and problems they address are completely different. 
As an offline algorithm \citep{zhuang2023behavior, fujimoto2019off, kumar2020conservative}, TD3 + BC incorporates BC \citep{pomerleau1988alvinn} to prevent out-of-distribution (OOD) state-action pairs that lie beyond the offline dataset. 
In contrast, TDMPBC is a fully online algorithm that integrates imitation learning to accelerate exploration and learning within complex high-dimensional spaces.

Generally speaking, imitation learning \citep{zare2024survey} emphasizes exploitation and is a relatively conservative algorithm, whereas reinforcement learning places greater emphasis on exploration. 
TDMPBC can be regarded as a RL algorithm that incorporates conservatism. 
With the dynamic adjustment of BC weights, TDMPBC can be seen as a process where RL explores the space first, followed by rapid learning through imitation learning. 
The introduction of imitation learning does indeed carry the risk of causing the algorithm to converge to local optima. 
However, in the context of high-dimensional complex spaces such as humanoid robot motion control, converging to a local optimum like the upright posture is highly probable and often beneficial for downstream tasks.