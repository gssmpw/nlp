\section{Experiments}

\begin{figure*}[htbp]
\vspace{-14pt}
  \centering
    \includegraphics[width=0.96\textwidth]{performance_comparison_10M.pdf}
  \caption{This figure presents the evaluation results on the HumanoidBench, where we conduct experiments with a total of three seeds and the shaded area representing one standard deviation. The baseline results are directly from the HumanoidBench.}\label{fig:main_result}
\end{figure*}
We evaluate our proposed TDMPBC in HumanoidBench \citep{sferrazza2024humanoidbench}, which contains 14 locomotion tasks and 17 whole-body manipulation tasks.
This benchmark is built on the Unitree H1 robot with dexterous hands, which has \textbf{151}-dimension observation space and \textbf{61}-dimension action space.
Remarkably, this benchmark aims to evaluate online RL algorithms and does \textbf{not} include any expert demonstration.
At the same time, SIRL is an improved online RL paradigm rather than imitation learning paradigm.

Specifically, the experiments cover the following five aspects of TDMPBC: 
\textcolor{blue}{1)} Performance comparison with representative RL algorithms across 31 HumanoidBench tasks;
\textcolor{blue}{2)} The impact of the selection of hyperparameter $\beta$ and reference return value $G$;
\textcolor{blue}{3)} Increased runtime caompared to TD-MPC2;
\textcolor{blue}{4)} The phenomenon of policy performance chasing the highest return in the replay buffer during algorithm training;
\textcolor{blue}{5)} Demonstration and analysis of some representative learned behaviors.

\subsection{Performance Comparison}
\subsubsection{Baselines}
We choose these three representative online reinforcement learning algorithms as our baselines:
\begin{itemize}[leftmargin=1.2em]
    \item \textbf{SAC} (Soft Actor-Critic) \citep{haarnoja2018soft}: the state-of-the-art model-free off-policy RL algorithm with maximum entropy learning \citep{eysenbach2021maximum};
    \item \textbf{DreamerV3} \citep{hafner2023mastering}: the state-of-the-art model-based RL algorithm that learns from the imaginary model rollouts;
    \item \textbf{TD-MPC2} \citep{hansen2023td}: the state-of-the-art model-based RL algorithm with online planning achieved via model predictive control (MPC).
\end{itemize}
As for on-policy algorithm PPO (Proximal Policy Optimization) \citep{schulman2017proximal}, its performance is inferior without the massive GPU parallelization so PPO is not our baseline.

In Humanoidbench, TD-MPC2 interacts with the environment for 2M steps, which takes approximately the same amount of time as SAC and DreamerV3 interacting for 10M steps. 
Therefore, the default training steps for TD-MPC2 are set to 2M, while others are set to 10M. 
Similarly, the default training steps for TDMPBC are also 2M.
For tasks where performance significantly surpasses TD-MPC2 but still shows a clear upward trend without reaching the target, we choose to continue training up to 10M steps to demonstrate asymptotic performance. 
These environments include the \texttt{hurdle}, \texttt{balance\_simple}, \texttt{balance\_hard}, and \texttt{stair} tasks in Locomotion, as well as the \texttt{cabinet} and \texttt{window} tasks in Whole-body Manipulation.

\subsubsection{Results on Locomotion} 
HumanoidBench contains a total of 14 locomotion tasks, corresponding to the first 14 training curves ending with (L) in Figure \ref{fig:main_result}. 
It should be noted that, while the locomotion tasks can be accomplished without dexterous hands, the H1 robot here is indeed equipped with dexterous hands. 
The entire robot has \textbf{151}-dimensional observations (51 dimension for body and 50 dimension for each hand), plus a \textbf{61}-dimensional action space, which is quite challenging for RL control.
TDMPBC achieves significantly faster convergence and higher final performance compared to the baseline in all environments except for \texttt{reach} and \texttt{crawl}.
More importantly, TDMPBC surpasses the target (represented by the grey dashed line) in 8 tasks, indicating successful task completion. 
In contrast, the baselines are only capable of completing the \texttt{crawl} task.


\subsubsection{Results on Whole-Body Manipulation} 
HumanoidBench contains a total of 17 whole-body manipulation tasks, corresponding to the last 17 training curves ending with (M) in Figure \ref{fig:main_result}. 
Whole-body manipulation requires not only the control of body posture but also the operation of dexterous hands to accomplish grasping.
Although our algorithm has achieved obvious improvements, the final results are still far behind the target return. 
Our algorithm also struggles to simultaneously control the body and achieve dexterous hand grasping.
A prematurely converging curve implies that the humanoid rapidly masters one thing while the other one fails.

\subsection{Ablation Study}
\subsubsection{Ablation on hyperparameter $\beta$}
\begin{figure}[!b]
    \centering
    \vspace{-28pt}
    \includegraphics[width=0.98\linewidth]{ablation_plots.pdf}
    \text{~~~~~~(a)~Ablation on $\beta$~~~~~~~(b)~Ablation on $G$}
    \caption{The left figures illustrate the impact of different hyperparameter values ($\beta = 0.5, 1.0, 2.0$) on the performance of TDMPBC across three tasks: \texttt{run}, \texttt{hurdle}, and \texttt{maze}. The right figures demonstrate the effects of two different goal settings ($G = R_{\text{max}}$ and $G = R_{\text{target}}$) on the performance of TDMPBC across three tasks: \texttt{reach}, \texttt{hurdle}, and \texttt{maze}.}
    \label{fig:ablation}
    \vspace{-12pt}
\end{figure}

The $\beta$ balances the original reinforcement learning and our proposed self-imitative behavior cloning in policy loss function Equation \ref{eq:loss}.
Due to the presence of the exponential function and $R_t \leq G$, the range of behavior cloning item is between $(0, 1]$. 
Meanwhile, Q is obtained by discrete regression in a log-transformed space, which means the Q has been normalized \citep{hafner2023mastering}.
Due to the same scale between RL loss and behavior cloning loss, the default value of hyperparameter is $\beta=1$.
In Section 5.1, the experimental results are presented for the case where $\beta=1$. 
To further investigate the robustness of TDMPBC, we conducted additional control experiments with $\beta=0.5$ and $\beta=2.0$. 
We found that the performance of TDMPBC is highly robust to values of $\beta$ around 1.



\begin{figure*}[htbp]
  \centering
  \hspace{-4.5pt}
    \includegraphics[width=0.98\textwidth]{pole_base.pdf}
    \caption{The visualization of baseline TD-MPC2 on the \texttt{pole} task. The robot collides with the pole and falls to the ground.}
    \label{fig:pole_base}
  \hspace{-6pt}
    \includegraphics[width=0.98\textwidth]{pole_ours.pdf}
  \caption{The visualization of our TDMPBC on the \texttt{pole} task. To avoid collisions, the robot chooses to stay close to the wall, thereby passing through quickly and stably. The ground is marked in blue to more clearly illustrate the moving toward the wall.}
  \label{fig:pole_ours}
  \vspace{-8pt}
\end{figure*}
\subsubsection{Ablation on reference return value $G$}
The $G = R_{\text{target}}$ of the current task serves as a globally optimal benchmark but necessitates the introduction of additional information. In contrast, the maximum return $G = R_{\text{max}}$ in the current replay buffer represents the upper limit achievable by the current policy, which grows in tandem with the policy's performance as training progresses.
Meanwhile, $G = R_{\text{target}}$
functions more as a pre-established, relatively higher objective. 
In Figure \ref{fig:ablation}, we found no significant 
\begin{wrapfigure}[10]{r}{0.5\linewidth}
\vspace{-0.5cm}
\centering
\hspace{-8pt}
\includegraphics[width=1.1\linewidth]{max_R_700.pdf}
\vspace{-0.6cm}
\caption{Gradually, $R_{\text{max}}$ may potentially exceed $R_{\text{target}}$.}
    \vspace{-0.4cm}
\end{wrapfigure}
differences in overall performance between the two. 
Therefore, we opted for $G = R_{\text{max}}$ to avoid incorporating prior information. 
It is also worth noting that in tasks where the return can exceed the target, $G = R_{\text{max}}$ may ultimately surpass $G = R_{\text{target}}$.


\subsection{Runtime}
Compared to TD-MPC2, TDMPBC introduces only a marginal increase in computational burden for policy loss calculations and requires the replay buffer to additionally store the current maximum return value. 
We measure the time required to run three seeds simultaneously on a Tesla V100-SXM2-32G GPU across three different tasks in Table \ref{tab:time}. 
When the GPU is upgraded to an NVIDIA A100-SXM4-40GB, the time can be further reduced to approximately 20 hours.
On average, TDMPBC only increases the time by less than $5\%$, yet achieved a remarkable performance improvement of over $120\%$.
\begin{table}[h!]
\centering
\caption{A comparison of the runtime of our proposed algorithm TDMPBC and the baseline TD-MPC2 on the same hardware.}
\label{tab:time}
\vspace{4pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|ll|r}
\toprule
Tasks   & TDMPBC          & TD-MPC2          & Improvement \\
\hline
walk    & $37.71\pm0.13$  & $35.23\pm  0.38$ & $+\ 7.06\%$        \\
reach   & $37.73\pm0.64$  & $36.46\pm  0.71$ & $+\ 3.48\%$        \\
hurdle  & $36.61\pm0.06$  & $35.34\pm  0.13$ & $+\ 3.61\%$        \\
\hline
\textbf{\textit{Average}} & $\textbf{\textit{37.35}} $ (h)         & $\textbf{\textit{35.68}}$ (h)         & $+\ \textbf{\textit{4.72\%}}$ \\
\bottomrule
\end{tabular}}
\vspace{-8pt}
\end{table}

\subsection{Training phenomenon}
In the experiments, we observed that the return obtained by evaluating the current policy is often lower than the maximum return in the replay buffer, regardless of whether it is our proposed TDMPBC or the baseline algorithm TD-MPC2 in Figure \ref{fig:max_R}. Intuitively, it appears as though the policy is constantly chasing the current maximum return, and the behavior cloning (BC) in SIRL accelerates this.
\begin{figure}[h]
\vspace{-8pt}
    \centering
    \includegraphics[width=0.98\linewidth]{max_R.pdf}
    \caption{The curves of the policy's return (solid line) and the maximum return in the current replay buffer (dashed line) during training. The left figure shows these curves for TDMPBC on the \texttt{balance-hard} and \texttt{hurdle} tasks, while the right figure compares the curves for TDMPBC and TD-MPC2 on the \texttt{walk}.}
    \label{fig:max_R}
    \vspace{-8pt}
\end{figure}

\subsection{Behavior Visualization} 
In this subsection, we present representative behaviors obtained from TDMPBC, including the \texttt{pole} and \texttt{balance-hard} tasks in locomotion, as well as the \texttt{window} task in whole-body manipulation.



\textbf{\texttt{pole}}: In the \texttt{pole} task, the humanoid robot is required to navigate forward through a dense forest of tall, slender poles without collision. 
The robot trained with TD-MPC2 continuously collides with the poles and is unable to move forward properly, eventually falling over.
Once step inside the pole forest, the robot swiftly moves towards one of the side walls. 
It then proceeds to hug the wall, keep escaping the dense poles, thereby avoiding potential collisions. 
This clever avoidance strategy, closely resembling human behavior, is exactly the reason our algorithm converges rapidly.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.98\linewidth]{balance.pdf}
    \caption{The visualization of our TDMPBC on the \texttt{balance-hard} task, where the humanoid robot aims to maintain balance on an unstable board, with the spherical pivot beneath the board in motion.}
    \label{fig:balance}
\end{figure}
\textbf{\texttt{balance\_hard}}: The humanoid robot is required to maintain balance on an unstable plank, beneath which lies a movable sphere. Upon initialization, the robot nearly falls backward but manages to stabilize itself by swinging its arms, ultimately achieving a balanced posture. During this process, the sphere is displaced from the center of the plank to a position slightly to the left.






\begin{figure}[h]
    \centering
    \includegraphics[width=0.98\linewidth]{window.pdf}
    \vspace{-6pt}
    \caption{The visualization of our TDMPBC on the \texttt{window} task, where the humanoid aims to grab a window wiping tool and keep its tip parallel to a window by following a prescribed vertical speed.}
    \label{fig:window}
    \vspace{-8pt}
\end{figure}
\paragraph{\texttt{window}}: For the \texttt{window} task, the robot should ideally use its dexterous hands to control the cleaning tools. 
However, the robot does not master the control of its dexterous hands and instead chooses to press against the cleaning surface with its arms to complete the task. 
Despite the algorithm achieving a high return on this task, it does not perform the task as expected.

