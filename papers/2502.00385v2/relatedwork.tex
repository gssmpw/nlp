\section{Related Work}
\paragraph{\textbf{LLM bias and downstream task fairness}}
Previous work has shown that political biases in LLMs can significantly impact fairness in downstream tasks. \citet{feng-etal-2023-pretraining} showed that pretraining data's political orientation affects model performance in tasks like hate speech detection and misinformation classification, with models exhibiting different biases based on their training corpora. Their work revealed that models pretrained on politically diverse sources show varying patterns in how they treat different identity groups and partisan content. This suggests that LLMs encode and propagate political biases from their training data, which can lead to unfair treatment of certain groups or perspectives in practical applications. 
While pretraining on politically diverse data can help address these biases, such approaches often require substantial data and computational resources.


\paragraph{\textbf{Persona-based prompting}}
Recent work has explored persona-based prompting as a more accessible alternative for controlling model behavior and mitigating bias. \citet{frohling2024personas} introduced this approach to increase diversity in annotation tasks by injecting different persona descriptions into model prompts, showing that persona descriptions can help elicit a wider range of valid perspectives. Building on this, \citet{bernardelle2024mappinginfluencingpoliticalideology} showed that persona-based prompting can effectively modulate models' political orientations without requiring expensive retraining. Their work revealed that carefully crafted persona prompts can shift model responses along the political spectrum, offering a promising method for achieving political diversity in language model outputs. This prompting-based approach provides a more flexible and resource-efficient way to control model behavior than pretraining-based methods, while still maintaining the ability to capture diverse political perspectives. While this body of work establishes the effectiveness of persona-based prompting, our work extends this research by evaluating how persona-based political perspectives influence performance on multimodal hateful content detection tasks.