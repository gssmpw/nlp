\section{Evaluating Existing Diveristy Metrics}
\label{sec:existing}
We begin by evaluating the correlation between existing diversity metrics and instruction-tuned model performance, identifying limitations to inform the design of a more reliable metric.

Our evaluation follows four steps: 
(1) Construct multiple IT datasets, each denoted as $\mathcal{X}^{(s)}$, using different data selection strategies from the full data source $\mathcal{X}^{all}$. 
(2) Measure dataset diversity using existing metrics, denoted as $\mathcal{M}_{t}(\mathcal{X}^{(s)})$. 
(3) Fine-tune LLMs on each dataset and evaluate their performance, $\mathcal{P}^{(s)}$, using IT benchmarks.
(4) Analyze the correlation between each diversity metric and model performance, denoted as $r_{\mathcal{M}_{t},\ \mathcal{P}}$.

\subsection{Existing Diversity Metrics}
We use 11 existing diveristy metrics for the analysis, categoried into three main types:

\paragraph{Lexical Diversity}  
A classical way to measure textual diversity is by analyzing vocabulary usage, where a higher proportion of unique words indicates greater diversity. Two widely used metrics are the \textbf{Type-Token Ratio} (TTR) \cite{richards1987type-TTR} and \textbf{vocd-D} \cite{malvern2004lexical-vocd}, with details in the Appendix \ref{app:exist}.

\paragraph{Distance-based Semantic Diversity}  
Recent studies primarily measure dataset diversity based on the semantics of individual samples, often represented as embeddings $emb(\cdot)$ from language models like BERT. A common approach quantifies diversity by computing distances between samples using their embeddings, encouraging heterogeneity. 
For example, a straightforward metric sums the pairwise distances among all samples in a dataset:
\begin{equation}  
    \mathcal{M}_{DistSum}(\mathcal{X}) = \sum_{x_i, x_j \in \mathcal{X}, i \neq j} \Delta(x_i, x_j),
\end{equation}  
where $\Delta(\cdot, \cdot)$ denotes the distances between two samples. Specifically, \textbf{DistSum$_{cosine}$} uses cosine distance and \textbf{DistSum$_{L2}$} uses Euclidean distance.
Beyond simple summation, more refined metrics are proposed. The \textbf{KNN distance} \cite{stasaski2020more-KNN, stasaski2022semantic-KNN} measures the average distance of each sample to its $k$-nearest neighbor, ensuring sample uniqueness: 
\begin{equation}  
    \mathcal{M}_{KNN}(\mathcal{X}) = \frac{1}{|\mathcal{X}|} \sum_{i=1}^{|\mathcal{X}|} \Delta(x_i, N_k(x_i)),
\end{equation}
where $N_k(x_i)$ denotes the k-th closest neighbor of $x_i$, typically with $k=1$. 
We also compute \textbf{Cluster Inertia} \cite{du2019boosting-Inertia}, \textbf{Vendi Score} \cite{pasarkar2023cousins-Vendi}, \textbf{Radius} \cite{lai2020diversity-Radius} and \textbf{Log Determinant Distance} (LDD) \cite{wang2024diversity-logD}. Further details are provided in the Appendix \ref{app:exist}.

\paragraph{Distribution-based Semantic Diversity}  
Another notable class of metrics measures diversity from a distributional perspective, assessing how well a selected dataset $\mathcal{X}$ represents the overall sample (semantic) space of $\mathcal{X}^{all}$. 
One example is the \textbf{Facility Location} (FL) function \cite{farahani2009facility-FL}, which defines a dataset as diverse if each sample in $\mathcal{X}^{all}$ has a close representative in $\mathcal{X}$, ensuring thorough coverage of space:
\begin{equation}
    \mathcal{M}_{FL}(\mathcal{X}) = \sum_{x_j \in \mathcal{X}^{all}}\min_{x_i \in \mathcal{X}} \Delta(x_i, x_j)
\end{equation}  
Another feasible metric, \textbf{Partition Entropy}, captures how evenly the selected dataset spans the sample space. It partitions $\mathcal{X}^{all}$ into $K$ clusters using K-means and calculates the entropy of the cluster membership distribution of $\mathcal{X}$.
\begin{equation}
    \mathcal{M}_{Entropy}(\mathcal{X}) = -\sum_{k=1}^{K} p_k \log p_k,  
\end{equation}  
where $p_k$ is the proportion of selected samples in cluster $k$. Higher entropy indicates greater distributional uncertainty and a more balanced dataset.

\begin{figure*}[t!]
    \centering
        \includegraphics[width=\linewidth]{latex/figures/llama3_llama3_other_all_subplots.pdf}
    \caption{Evaluating existing diversity metrics based on their correlation (Eq. \ref{eq:cor}) with IT performance (Eq. \ref{eq:perf}). The X-axis represents diversity measurements. Each point corresponds to a 10k IT dataset constructed using different strategies. Abnormal points highlight the limitations of current metrics and inspire the development of new ones.}
    \label{fig:exist}
\end{figure*}

\subsection{IT Dataset Construction and Benchmark}

Following \citealp{liu2023makes}, we use a combined dataset of WizardLM \cite{xu2024wizardlm}, ShareGPT \cite{chiang2023vicuna-ShareGPT}, and UltraChat \cite{ding2023enhancing-UltraChat} as our IT data source, denoted as $\mathcal{X}^{all}$.

We then apply several representative diversity-aware data selection strategies to curate IT datasets from the source $\mathcal{X}^{(s)} \subset \mathcal{X}^{all}$. 
To minimize the influence of factors beyond diversity, we control for sample quality differences across datasets by removing anomalous source samples and excluding any data quality filters during selection. We also fix the dataset size at 10,000 samples. 
The strategies used are: \textbf{K-Center-Greedy} \cite{sener2017active-Kcentergreedy, chen2023maybe-Kcentergreedy, du2023mods-Kcentergreedy, wu2023self-Kcentergreedy}, which iteratively selects the sample farthest from the current coreset; \textbf{Repr Filter} \cite{liu2023makes}, which improves $\mathcal{M}_{KNN}$ by applying a minimum distance threshold when adding samples into the coreset; \textbf{QDIT} \cite{bukharin2023data-QDIT}, which optimizes diversity by serially selecting the data point that maximizes $\mathcal{M}_{FL}$; \textbf{K-means} \cite{song2024iterselecttune-Kmeans}, which partitions samples into clusters and evenly select samples from each; and baselines, including \textbf{Random} selection and \textbf{Farthest}, which ranks samples by their total distances to others and selects the most distant ones. Additionally, we construct datasets with varying amounts of \textbf{Duplicate} samples to simulate low-diversity datasets. Each strategy is run at least three times to ensure robustness, yielding 53 IT datasets. 
Details on dataset curation are in Appendices \ref{app:preprocess} and \ref{app:ds}.

We fine-tune LLaMA-3-8B \cite{dubey2024llama} on these datasets and evaluate model performance using two popular IT benchmarks: MT-bench \cite{zheng2023judging-MT-Bench} and AlpacaEval \cite{li2023alpacaeval}. Both use GPT-4 \cite{achiam2023gpt} for automatic evaluation, with AlpacaEval assessing single-turn dialogue and MT-bench on multi-turn conversations. 
To jointly consider both benchmarks, we normalize the results into Z-scores and compute the aggregated performance as 
\begin{equation}
\label{eq:perf}
    \mathcal{P}^{(s)} = z^{(s)}_{MT-bench} + z^{(s)}_{AlpacaEval}
\end{equation}




\subsection{Correlation Analysis}
\label{subsec:analysis}
Finally, we compute the correlation between each diversity metric $\mathcal{M}_{t}$ and model performance $\mathcal{P}$ by averaging their Pearson and Spearman coefficients:
\begin{equation}
\label{eq:cor}
    r_{\mathcal{M}_{t},\ \mathcal{P}} = (r_{\mathcal{M}_{t},\ \mathcal{P}}^{Pearson} + r_{\mathcal{M}_{t},\mathcal{P}}^{Spearman})/{2}
\end{equation}

The results are shown in Figure \ref{fig:exist}, with additional plots in Appendix \ref{app:results}. Since our experiments minimize the influence of other factors, we believe model performance directly reflects the impact of dataset diversity. Thus, the correlation between diversity metrics and model performance indicates a metric's practical reliability. Overall, we find that each metric favors datasets selected by its own criterion, but may not correlate well with performance, as it overlooks other aspects of diversity:

\paragraph{Findings 1}
\textit{Lexical diversity metrics fail to distinguish between different samples and datasets, showing weak correlation with model performance.}

As shown in Figure \ref{fig:exist}(a, b), high- and low-performance datasets exhibit similar lexical diversity. This likely results from the widespread use of diverse vocabulary in IT samples, making lexical diversity an ineffective measure for IT datasets.

\paragraph{Findings 2}
\textit{Since distribution-based semantic diversity metrics neglect sample uniqueness, they often underestimate the diversity of datasets with large inter-sample distances.}

From Figure \ref{fig:exist}(c, d), we observe that datasets selected by Farthest and K-Center-Greedy (brown and green points) achieve high IT performance but often receive relatively lower diversity scores from distribution-based diversity metrics, thus weakening their correlation with model performance. This likely occurs because these strategies all prioritize sample uniqueness by selecting samples that are distant from others, a factor not captured by distribution-based metrics. This suggests that overlooking sample uniqueness diminishes the reliability of diversity metrics.

\paragraph{Findings 3}
\textit{As distance-based semantic diversity metrics neglect information density in semantic space, they often underestimates datasets taht are close to the overall sample distribution and overestimates datasets with large inter-sample distances.}

From Figure \ref{fig:exist}(e, f, g, h), we observe common outliers in the fitting line for datasets selected by QDIT and K-means (blue and red points), which receive low diversity scores despite strong performance according to distance-based diversity metrics. In contrast, K-Center-Greedy and Repr Filter (green and purple points) show the opposite trend, weakening the metrics' correlation with the model performance. This is likely because the former two strategies select more samples from dense semantic regions, which better cover the overall sample distribution but conflicts with distance-based diversity calculations. This suggests that ignoring information density in semantic space reduces the reliability of diversity metrics.


\paragraph{Findings 4}
\textit{Distance-based metrics often fail to accurately measure diversity in datasets containing redundant samples.}

As shown by the duplicated datasets (pink points) in Figure \ref{fig:exist}(e, f, g, h), DistSum fails to capture redundancy effectively, as total distances are dominated by variations in distant samples. Meanwhile, other metrics, such as KNN Distance, overly penalize redundant samples by nullifying their contribution to overall diversity.

