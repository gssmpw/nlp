\section{Data Selection Strategy}
\label{sec:DSS}

\paragraph{Introducing \textit{NovelSelect}}


Given \textit{NovelSum}'s accurate diversity measurement and strong correlation with model performance, we investigate its potential as an optimization objective for selecting samples and generating a diverse dataset:
\begin{equation}
\label{eq:argmax}
    \mathcal{X} = \arg\max_{\mathcal{X} \subset \mathcal{X}^{all}} \mathcal{M}_{NovelSum}(\mathcal{X}),
\end{equation}
where $\mathcal{M}_{NovelSum}(\mathcal{X})$ is defined in Eq. \ref{eq:def}. Since directly solving Eq. \ref{eq:argmax} is NP-hard \cite{cook1994combinatorial}, we propose a greedy approach that iteratively selects the most "novel" sample. The "novelty" of a new sample $v(x)$ relative to an existing set $\mathcal{X}$ is defined as:
\begin{equation}
    v(x) = \sum_{x_j \in \mathcal{X}} w(x, x_j)^{\alpha} \cdot\sigma(x_j)^{\beta} \cdot d(x, x_j),
\end{equation}
where $w(x, x_j)$ and $\sigma(x_j)$ are the proximity weight and density factor from Eq. \ref{eq:pws} and \ref{eq:dad}. The sample with the maximum novelty is then selected: $x^{new} = \arg\max_{x}v(x), \quad \mathcal{X} \leftarrow {x^{new}} \cup \mathcal{X}$. This process is repeated from $\mathcal{X} = \emptyset$ until the data budget is reached, resulting in the selected dataset. We refer to this approach as \textit{NovelSelect}. 

\input{latex/table/algorithm}

Algorithm \ref{alg:novelselect} outlines the overall process. Notably, by incorporating quality scores into $v(x)$, \textit{NovelSelect} can also seamlessly integrate with quality-based data selection methods.

\paragraph{Data Selection Experiments}

We conduct additional data selection experiments on LLaMA-3-8B to evaluate \textit{NovelSelect}'s performance. Following prior settings, we use \textit{NovelSelect} to select 10k samples from $\mathcal{X}^{all}$ and assess the fine-tuned model's performance on MT-bench and AlpacaEval. Results are averaged over three runs. 

\begin{table}[t!]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lcc|c}
\toprule
\textbf{Strategies} & \textbf{MT-bench} & \textbf{AlpacaEval} & \textbf{Aggregated $\mathcal{P}$} \\
\midrule
Random & 6.18 & 75.47 & 1.20 \\
Repr Filter & 6.17 & 72.57 & 1.05 \\
QDIT & 6.21 & 75.91 & 1.25 \\
K-Center-Greedy & 6.33 & 75.30 & 1.31 \\
K-means & 6.33 & 75.46 & 1.32 \\
\midrule
NovelSelect & \textbf{6.47} & \textbf{78.07} & \textbf{1.55} \\
\bottomrule
\end{tabular}
}
\caption{Comparisons of different diversity-oriented data selection strategies on IT performance. $\mathcal{P}$ aggregates the performance based on Z-scores (Eq. \ref{eq:perf}).}
\label{tbl:select}
\vspace{-2mm}
\end{table}

From Table \ref{tbl:select}, \textit{NovelSelect} outperforms existing diversity-oriented data selection strategies on both benchmarks, demonstrating superior IT performance. This further validates \textit{NovelSum}'s effectiveness and practical value in IT data engineering.
