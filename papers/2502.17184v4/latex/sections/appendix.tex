\section{Details of Correlation Evaluation}

\subsection{Data Processing and Semantic Embeddings}
\label{app:preprocess}
We apply basic preprocessing to remove anomalous samples from the data sources, ensuring more stable results while preserving generality. In the early stage of our work, we observe that short samples often exhibit low quality and tend to be outliers in the semantic space, potentially distorting experimental results. To address this, we filter out samples shorter than 256 tokens using the BERT \cite{devlin2018bert} tokenizer, ensuring consistency for experiments across different LLMs. Furthermore, to ensure the dataset's relevance for English-language tasks and math problems, we exclude samples with a non-English-or-number ratio exceeding 0.8. 

When computing sample embeddings, we set the maximum sequence length to 256 to mitigate the impact of varying text lengths. This applies only to embedding computation; fine-tuning uses a much larger maximum length. We extract the last hidden layer of the language model and apply mean pooling, excluding padding tokens, to generate robust sample-level embeddings. For experiments on the LLaMA-3-8B model, we utilize LLaMA-3-8B to compute embeddings for data selection. Similarly, for experiments with the Qwen-2.5-7B model, we employ Qwen-2.5-7B to compute embeddings.

\subsection{Details of Existing Diversity Metrics}
\label{app:exist}
For lexical diversity, the \textbf{Type-Token Ratio} (TTR) quantifies the lexical diversity of a text sequence $x_i$ as the ratio of distinct tokens to the total number of tokens. The overall lexical diversity of a dataset $\mathcal{X} = \{x_1, x_2, ..., x_N\}$ is computed as the average TTR across all samples:  
\begin{equation}  
    \mathcal{M}_{TTR}(\mathcal{X}) = \frac{1}{N} \sum_{i=1}^{N} \frac{|Unique(x_i)|}{|x_i|}.  
\end{equation}  
To mitigate the influence of text length on TTR, we randomly sample 30 tokens from each data point to compute the TTR.  

To address the sensitivity of TTR to text length, \textbf{vocd-D} extends this measure by computing $TTR_i^k$ over sampled sub-sequences of varying lengths $k$ and fitting the following curve:  
\begin{equation}  
    \hat{TTR}_i^k = \frac{D}{k} \left( (1 + 2 \frac{k}{D})^{\frac{1}{2}} - 1 \right),  
\end{equation}  
where $D$ is the estimated parameter representing lexical diversity. The vocd-D metric is defined as $\mathcal{M}_{vocd-D} = D_{\text{best fit}}$, with larger values indicating greater lexical diversity. In our experiments, we compute $TTR_i^k$ for $k = 10, 20, 30, 40, 50$ and take the average of the resulting values as the final lexical diversity score. 

For distance-based semantic diversity, 
\textbf{Cluster Inertia} \cite{du2019boosting-Inertia} quantifies diversity by partitioning the dataset into $K$ clusters using K-means and summing the squared distances between each sample and its cluster centroid:  
\begin{equation}  
    \mathcal{M}_{Inertia}(\mathcal{X}) = \sum_{j=1}^{K} \sum_{x_i \in C_j} \|emb(x_i) - \mu_j\|^2,  
\end{equation}  
where $\mu_j$ is the centroid of cluster $C_j$. A higher inertia value suggests a greater spread of samples.  
Additionally, \textbf{Vendi Score} (VS) \cite{pasarkar2023cousins-Vendi} measures diversity based on the eigenvalues of the similarity kernel matrix. The generalized VS metric is defined as:  
\begin{equation}  
    \mathcal{M}_{VS}(\mathcal{X}) = \exp\left(\frac{1}{1 - \alpha} \log_2 \sum_{i=1}^{|\mathcal{X}|} \bar{\lambda}_{i|\theta}^{\alpha} \right),  
\end{equation}  
where $\bar{\lambda}_{i|\theta}$ represents the normalized eigenvalues. We set $\alpha=0.5$ to enhance measurement under severe class imbalance.
\textbf{Radius} \cite{lai2020diversity-Radius} characterizes the dispersion of the sample space by approximating embeddings as a multi-variate Gaussian distribution. It computes the geometric mean of the standard deviations along each dimension:  
\begin{equation}  
    \mathcal{M}_{Radius}(\mathcal{X}) = \sqrt[H]{\prod_{j=1}^{H} \sigma_j},  
\end{equation}  
where $H$ is the embedding dimension, and $\sigma_j$ denotes the radius of the ellipsoid along the $j$-th axis. Larger values indicate a greater spread of samples in the embedding space. \textbf{Log Determinant Distance} \cite{wang2024diversity-logD} utilizes the determinant of the similarity matrix as a measure of dataset diversity. In our work, we employ the cosine similarity function to compute the similarity matrix. 

Note that for \textbf{DistSum$_{cosine}$}, we use cosine distance $\Delta(x_i, x_j) = 1 - \cos(emb(x_i), emb(x_j))$. For \textbf{DistSum$_{L2}$}, we use Euclidean distance $\Delta(x_i, x_j) = \|emb(x_i) - emb(x_j)\|^{2}_2$.

For \textbf{Partition Entropy}, we cluster $\mathcal{X}^{all}$ into 1,000 clusters, while for \textbf{Cluster Inertia} \cite{du2019boosting-Inertia}, we cluster $\mathcal{X}^{s}$ into 200 clusters for subsequent computations.


\subsection{Details of Data Selection Strategies}
\label{app:ds}

All IT datasets in our experiments are selected from $\mathcal{X}^{all}$ and sampled over three rounds (two for Qwen) per strategy variant, unless stated otherwise. We assume these datasets have similar average sample quality, as they come from the same source without any quality filters. Additionally, the dataset size is standardized to 10,000 samples. Thus, our experiments can more accurately reflect the correlation between dataset diversity and model performance, without introducing significant confounders.

\paragraph{K-Center-Greedy} \cite{sener2017active-Kcentergreedy, chen2023maybe-Kcentergreedy, du2023mods-Kcentergreedy, wu2023self-Kcentergreedy} This strategy begins by randomly selecting a data point from the dataset $\mathcal{X}^{all}$ as the initial point of the subset $\mathcal{X}^{(s)}$. Subsequently, it iteratively computes the closest distance between the remaining points in $\mathcal{X}^{all} \setminus \mathcal{X}^{(s)}$ and selected samples in $\mathcal{X}^{(s)}$. The point with the maximum minimum distance (i.e., the farthest point) is added to $\mathcal{X}^{(s)}$. This process continues until the desired subset size is achieved.

\paragraph{Repr Filter} \cite{liu2023makes} Unlike the K-Center-Greedy strategy, which selects the farthest point from the remaining data pool, the Repr Filter randomly selects a data point whose similarity with all embeddings in $\mathcal{X}^{(s)}$ is below a predefined threshold. Due to the unique distribution of embeddings across different models, it is necessary to set distinct thresholds for each similarity function and model embedding. To ensure diversity across different experimental rounds, we employ cosine similarity and set the threshold to 0.3 for LLaMA-3-8B and 0.1 for Qwen-2.5-7B.

\paragraph{QDIT} \cite{bukharin2023data-QDIT} QDIT sampling combines diversity and quality scores for data selection; however, in our work, we focus exclusively on its diversity score. This method computes the sum of similarities between each sample in $\mathcal{X}^{all} \setminus \mathcal{X}^{(s)}$ and its closest data point in $\mathcal{X}^{(s)}$. For each candidate data point, we calculate the similarity sum as if it were added to $\mathcal{X}^{(s)}$, defining its Facility Location (FL) score. The algorithm then iteratively selects the data point with the highest FL score. For the initial selection, it chooses the data point that exhibits the highest overall similarity to all other embeddings. In our experiments, we employ cosine similarity for computing these scores. Since the Facility Location function yields a fixed subset $\mathcal{X}^{(s)}$ for a given $\mathcal{X}^{all}$, and to maintain consistency with other strategies, we utilize the same subset of data but vary the training random seeds across three rounds of experiments.

\paragraph{K-means Clustering} \cite{song2024iterselecttune-Kmeans} For this strategy, we apply the K-means clustering algorithm to partition all sample embeddings in $\mathcal{X}^{all}$ into $K$ clusters. Subsequently, given a target data budget $n$, we randomly sample $\frac{n}{K}$ data points from each cluster. For our experiments, we use both 1000 and 100 clusters for LLaMA-3-8B, and 100 clusters for Qwen-2.5-7B.

\paragraph{Random Selection} In this baseline strategy, we randomly sample 10,000 data points from $\mathcal{X}^{all}$. To explore the impact of data sources, we also sample from individual datasets, including Alpaca \cite{alpaca}, Dolly \cite{dolly}, WizardLM, UltraChat, and ShareGPT, with similar preprocessing. Although we assume that the average sample quality of these sources does not significantly differ from that of $\mathcal{X}^{all}$, we use only a single round of results from each source in the overall correlation analysis as supplementary data to avoid potential quality differences affecting the outcome.

\paragraph{Duplicate Selection} To address the challenge of defining low-diversity datasets, which is crucial for our study, we construct datasets with redundant samples. Given a target data budget $n$, the dataset is constructed by selecting $m$ unique data points, each duplicated $\frac{n}{m}$ times. We set $m$ to 1, 10, 50, 100, 500, 1000, 2000, and 5000. This approach allows us to systematically control and analyze the impact of diversity on model performance.

\subsection{Details of Correlation Measures}
\label{app:corr}

We compute the correlation between each diversity metric and model performance using both Pearson \cite{cohen2009pearson} and Spearman \cite{zar2005spearman} correlation measures.
For example, Pearson's $r$ for a metric $\mathcal{M}_{t}$ is computed as:
\begin{equation}
    r_{\mathcal{M}_{t},\ \mathcal{P}}^{Pearson} = \frac{
        \sum_{s} (\mathcal{M}_{t}^{(s)} - \bar{\mathcal{M}}_t) (\mathcal{P}^{(s)} - \bar{\mathcal{P}})
    }{
        \sigma_{\mathcal{M}_t} \sigma_{\mathcal{P}}
    }
\end{equation}


\subsection{Details of Model Fine-Tuning}
\label{app:ft}
In our experiments, we leverage four or eight NVIDIA H100 GPUs for training the LLaMA-3-8B and Qwen-2.5-7B models. To enable efficient parallel training, we implement DeepSpeed Zero-Stage 2. Across all experiments conducted in this study, the training parameters are configured as follows: a maximum input length of 4096 tokens, a batch size of 128, 3 training epochs, a learning rate of 2e-5, and a warm-up ratio of 0.1 utilizing cosine warm-up. We use the official chat templates of LLaMA-3 and Qwen-2.5, respectively, to fine-tune each model. All models are trained with BF16 precision to optimize computational efficiency and memory usage. A single run of fine-tuning on a 10k dataset typically takes about one hour.

\section{Computational Complexity}
\label{app:complexity}

\textit{NovelSum} has a time complexity of $O(n^2 \log n)$ for a dataset of $n$ samples. Computing \textit{NovelSum} on a 10k-sample dataset using precomputed LLaMA-3-8B embeddings takes only 10 seconds on a single H20 GPU, which is negligible compared to model fine-tuning or evaluation.  

For our data selection strategy, \textit{NovelSelect}, selecting $n$ samples from a total pool of $N$ has a time complexity of $O(N\cdot n^2 \log n)$, significantly lower than QDIT's $O(N^3)$ since $n \ll N$ in most cases. In practice, \textit{NovelSelect} takes about one hour to select 10k samples from our 396K sample pool using precomputed LLaMA-3-8B embeddings on a single node with eight H100 GPUs. This cost is trivial compared to fine-tuning on all 396K samples, which takes approximately 40 hours under the same setup.

As a key preprocessing step, deriving embeddings for all 396K samples in $\mathcal{X}^{all}$ using LLaMA-3-8B takes under 2 hours on a single node with eight H100 GPUs, leveraging vLLM \cite{kwon2023efficient}. Since this is a one-time cost for all subsequent processes, it remains practical.


\section{Data Statistics}
\input{latex/table/data_staus}
Our data sources are detailed in Table \ref{tab:data_stastics}. After filtering out short data and non-English data, approximately 396K samples remain in $\mathcal{X}^{all}$ for use in our experiments. Note that we use the latest versions of these datasets, which may have a larger size than the initial versions. These datasets encompass samples from a wide range of domains.

\section{More Results}
\label{app:results}
Additional scatter plots for the analysis in Section \ref{sec:existing} are provided in Figure \ref{fig:scatter_l2_appdenix}, Figure \ref{fig:scatter_radius_appdenix} and Figure \ref{fig:scatter_ldd_appdenix} , illustrating the correlation for DistSum$_{L2}$, Radius, and Log Determinant Distance, respectively.

The full results of the correlation experiments on LLaMA-3-8B and Qwen-2.5-7B are presented in Table \ref{tab:more_results_llama3} and Table \ref{tab:more_results_qwen}, respectively. These tables provide a comprehensive comparison of diversity metrics across different experimental configurations.





\section{Others}
\subsection{License for Artifacts and Data Consent}
In this paper, the artifacts used are all available for academic research work, including ShareGPT, WizardLM, UltraChat, Alpaca and Dolly.
The methods compared in this paper can all be used for academic research.
All data originates from the original authors' open-source releases and can be used for academic research and publication.

\subsection{Data Statement}

The training datasets may contain offensive content; however, they do not include any personal information.
Furthermore, our training approach is designed to align the model with human preferences without producing harmful content.

\subsection{AI Assistant Usage Statement}

We solely utilize ChatGPT for writing refinement; however, AI assistants are not employed for research innovation or coding.

\subsection{Budgets}
\label{ap:budgets}
We spend approximately one hour of training on a single node with eight H100-80G GPUs for each IT model. Additionally, we spend around \$1,000 on the GPT API to evaluate our models using MT-bench and AlpacaEval.

\begin{figure}[t!]
    \centering
        \includegraphics[width=\linewidth]{latex/figures/llama3_llama3_other_euclidean_distance.pdf}
    \caption{Evaluation of DistSum$_{L2}$ metric by their correlation with IT performance.}
    \label{fig:scatter_l2_appdenix}
    \vspace{-4mm}
\end{figure}
\begin{figure}[t!]
    \centering
        \includegraphics[width=\linewidth]{latex/figures/llama3_llama3_other_gaussian_distance.pdf}
    \caption{Evaluation of Radius metric by their correlation with IT performance.}
    \label{fig:scatter_radius_appdenix}
    \vspace{-4mm}
\end{figure}
\begin{figure}[t!]
    \centering
        \includegraphics[width=\linewidth]{latex/figures/llama3_llama3_other_LDD.pdf}
    \caption{Evaluation of Log Determinant Distance metric by their correlation with IT performance.}
    \label{fig:scatter_ldd_appdenix}
    \vspace{-4mm}
\end{figure}

\input{latex/table/appendix_llama_table}
\input{latex/table/appendix_qwen_table}
