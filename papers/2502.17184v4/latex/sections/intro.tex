\section{Introduction}
Instruction tuning (IT) fine-tunes pretrained large language models (LLMs) with annotated instruction data, enabling them to follow human instructions and perform various tasks effectively \cite{sanh2022multitask, zhang2023instruction}. Recent studies indicate that small-scale, high-quality datasets can outperform larger ones in IT performance \cite{chen2023maybe-Kcentergreedy, zhou2024lima}, with data diversity playing a crucial role in achieving optimal results \cite{liu2023makes, bukharin2023data-QDIT, zhang2024instruction, yang2024beyond}. Consequently, various diversity-aware data selection methods have emerged \cite{qin2024unleashing, wang2024survey}, driven by different interpretations of data diversity.

However, the fundamental problem of precisely defining and measuring data diversity remains underexplored. This ambiguity has turned data engineering for diversity into a black-box process, leading to data selection methods that often fail to generalize and, at times, perform worse than random selection \cite{xia2024rethinking,diddee2024chasing}. 
While some diversity metrics have been introduced in IT research \cite{bukharin2023data-QDIT,wang2024diversity-logD}, a comprehensive evaluation and comparative analysis are still needed to identify a reliable metric that strongly correlates with fine-tuning performance in practice. 

\begin{figure}[t!]
    \centering
        \includegraphics[width=\linewidth]{latex/figures/head10_cropped.pdf}
    \caption{Our diversity metric, \textit{NovelSum}, exhibits superior correlation with model performance compared to existing metrics across IT datasets constructed with various data selection strategies.}
    \label{fig:head}
    \vspace{-4mm}
\end{figure}


To this end, we systematically analyze 11 existing diversity metrics by evaluating their reliability through extensive experiments. Using various mainstream diversity-oriented data selection methods, we construct 53 IT datasets and fine-tune models accordingly. We then measure dataset diversity using existing metrics and assess their correlation with model performance. By analyzing the limited correlation of existing metrics, we find that: (1) \textbf{A reliable diversity metric must capture differences between samples} to reflect each sample's information uniqueness. Moreover, differences between neighboring samples are more critical for overall diversity but can be overshadowed by variations in distant samples. (2) \textbf{Measuring differences between samples should account for both semantic similarity and the uneven distribution of information in space.} In high-density domains like math and code, semantically similar samples can still contain substantial unique information and should therefore be considered more diverse.


Building on these insights, we propose \textit{NovelSum}, a diversity metric that jointly considers inter-sample differences and uneven information density. Specifically, we define dataset diversity as the sum of each sample's unique contribution to overall information, termed "novelty". Just as a research paperâ€™s novelty is judged by its distinction from related work based on field-specific standards, we compute a sample's novelty as the proximity-weighted sum of its differences from other samples in the dataset. These differences are measured using density-aware distances, which capture both semantics and local information density.


To validate the effectiveness of \textit{NovelSum}, we conduct both a visualized simulation study and real-world correlation experiments using two different LLMs. 
The results show that \textit{NovelSum} accurately captures diversity variations and strongly correlates with instruction-tuned model performance, achieving Pearson's $r=0.98$ and Spearman's $r=0.95$, outperforming other metrics.
This demonstrates \textit{NovelSum}'s potential to effectively guide data engineering practices.
Furthermore, we develop \textit{NovelSelect}, a greedy, diversity-oriented data selection strategy that uses \textit{NovelSum} as the optimization objective. Experimental results confirm its superior performance compared to other approaches.



Our main contributions are three-fold:
\begin{itemize}
\item We systematically analyze and evaluate the reliability of existing diversity metrics for instruction tuning by computing their correlation with model performance, thereby unveiling pathways to a more reliable metric.
\item We propose \textit{NovelSum}, a diversity metric that captures both inter-sample differences and information density, achieving a strong correlation with instruction-tuning performance, substantially exceeding previous metrics.
\item We develop \textit{NovelSelect}, a diversity-oriented data selection strategy based on \textit{NovelSum}, which outperforms existing methods and further validates \textit{NovelSum}'s effectiveness and practical value in instruction tuning.

\end{itemize}












