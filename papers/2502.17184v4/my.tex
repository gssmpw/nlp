\pdfoutput=1

\documentclass[11pt]{article}

\usepackage[preprint]{acl}

\usepackage{times}
\usepackage{latexsym}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{algorithmicx}   %

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{url}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{booktabs}
\usepackage{subfigure}
\usepackage{siunitx}
\usepackage{tablefootnote}

\definecolor{BLUE}{HTML}{395A9D}
\definecolor{ORANGE}{HTML}{F36847}


\title{Measuring Data Diversity for Instruction Tuning: \\A Systematic Analysis and A Reliable Metric}


\author{
    \bf{\normalsize
    Yuming Yang$^{1}$\thanks{Equal Contribution.},\ \
    Yang Nan$^{1}$\footnotemark[1],\ \
    Junjie Ye$^{1}$,\ \
    Shihan Dou$^{1}$,}\\
    \bf{\normalsize
    Xiao Wang$^{1}$,\ \
    Shuo Li$^{1}$,\ \
    Huijie Lv$^{1}$,\ \
    Mingqi Wu$^{1}$,}\\
    \bf{\normalsize
    Tao Gui$^{2}$\thanks{Corresponding Authors.},\ \
    Qi Zhang$^{1}$\footnotemark[2],\ \
    Xuanjing Huang$^{1}$}\\
  {$^1$ \normalsize School of Computer Science, Fudan University}\\
  {$^2$ \normalsize Institute of Modern Languages and Linguistics, Fudan University}\\
  \texttt{\normalsize yumingyang23@m.fudan.edu.cn} \ \ \ \texttt{\normalsize \{qz,tgui\}@fudan.edu.cn} \\
}


\begin{document}
\maketitle
\begin{abstract}
\input{latex/sections/abstract}
\end{abstract}

\input{latex/sections/intro}
\input{latex/sections/existing}
\input{latex/sections/novelsum}
\input{latex/sections/simulation}
\input{latex/sections/experiment}
\input{latex/sections/novelselect}
\input{latex/sections/related}

\section{Conclusion}
In this paper, we investigate the fundamental problem of precisely measuring dataset diversity for instruction tuning and propose \textit{NovelSum}, a reliable diversity metric that correlates well with model performance. Inspired by our systematic analysis of existing diversity metrics, \textit{NovelSum} jointly considers inter-sample distances and information density to effectively capture dataset diversity, achieving superior correlations with model performance compared to previous metrics. Based on \textit{NovelSum}, We further develop a data selection strategy, \textit{NovelSelect}, whose remarkable performance validates the practical significance of \textit{NovelSum}. 



\section*{Limitations}
Although our work systematically analyzes existing and proposed metrics through extensive fine-tuning experiments, we focus solely on the Qwen-2.5-7B and LLaMA-3-8B models as the backbone LLMs. We exclude larger models and other model series due to resource constraints, though these may exhibit different characteristics in terms of data diversity. Additionally, our study focuses on the general instruction-tuning task evaluated by the MT-bench and AlpacaEval benchmarks. Experiments on downstream tasks, such as information extraction and creative writing, are not considered. Their data diversity measurements may differ from those of general instruction-tuning tasks and thus warrant further investigation.

\bibliography{custom}

\clearpage
\appendix
\input{latex/sections/appendix}

\end{document}
