%------------------------------------------------------------------------------
% Template file for the submission of articles to IUCr journals in LaTeX2e
% using the iucrjournals document class (file iucrjournals.cls)
% This work has been dedicated to the public domain
% License: CC0 1.0 Universal
% https://creativecommons.org/publicdomain/zero/1.0/
%------------------------------------------------------------------------------
% This template file and associated class and style files produce documents in
% a preprint style suitable for submission and review purposes. 
% The iucrjournals.cls requires a small selection of packages from standard TeXLive
% distributions and contains a minimal set of macros to define content and apply 
% formatting. BibTeX and iucr.bst should be used for references (using harvard.sty).
% If you wish to use additional packages, please reference them in this document and
% please only use packages included in standard TeXLive distributions in order to
% avoid compilation problems during the submission process.
%------------------------------------------------------------------------------

\documentclass[nolinenumbers]{iucrjournals}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage{bm}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{dsfont}
\usepackage{bm,mathabx,amsthm}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{romannum}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{definition}[theorem]{Definition}
% Add extra packages here, e.g.
% \usepackage{myfavouritepackage}


\usepackage{bm}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} 

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage[textsize=tiny]{todonotes}

\usepackage{threeparttable}
\usepackage{wrapfig}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{makecell}

\title{Few is More: Task-Efficient Skill-Discovery for Multi-Task Offline Multi-Agent Reinforcement Learning}

% Authors and affiliations (uses the standard authblk package):
% Author affiliations are indicated by lowercase letters in square brackets in the \author macro.
% Affiliations (referenced by the lowercase letters in square brackets) are listed after all the authors have been defined.
% The email addresses of corresponding/contact authors can be included using:
% \IUCrCemaillink{corrauthor@org.org}
% Other co-author email addresses can be included using:
% \IUCrEmaillink{coauthor@org.org}
% ORCiDs can be included using:
% \IUCrOrcidlink{xxxx-xxxx-xxxx-xxxx}
% Author footnotes can be included using:
% \IUCrAufn{Text...}
% and to apply the same footnote to another author use:
% \IUCrAufn[1]{} 
% where the number in square brackets refers to the numerical order of the 
% previously defined footnote.
% For example:
% \author[a]{Anne Author\IUCrCemaillink{corrauthor@org.org}\IUCrOrcidlink{xxxx-xxxx-xxxx-xxxx}}
\author{Xun Wang$^{*}$}
\author{Zhuoran Li$^{*}$}
\author{Hai Zhong}
\author{Longbo Huang}

\affil{Institute for Interdisciplinary Information Sciences (IIIS), Tsinghua University}

\begin{document} 
\maketitle 
\makeatletter
\renewcommand{\@makefnmark}{}
\makeatother
\footnotetext{$^{*}$ Contributed equally to this work.}

\begin{abstract}
As a data-driven approach, offline MARL learns superior policies solely from offline datasets, ideal for domains rich in historical data but with high interaction costs and risks. However, most existing methods are task-specific, requiring retraining for new tasks, leading to redundancy and inefficiency. To address this issue, in this paper, we propose a task-efficient multi-task offline MARL algorithm, Skill-Discovery Conservative Q-Learning (SD-CQL). Unlike existing offline skill-discovery methods, SD-CQL discovers skills by reconstructing the next observation. It then evaluates fixed and variable actions separately and employs behavior-regularized conservative Q-learning to execute the optimal action for each skill. This approach eliminates the need for local-global alignment and enables strong multi-task generalization from limited small-scale source tasks. Substantial experiments on StarCraftII demonstrates the superior generalization performance and task-efficiency of SD-CQL. It achieves the best performance on $\textbf{10}$ out of $14$ task sets, with up to $\textbf{65\%}$ improvement on individual task sets, and is within $4\%$ of the best baseline on the remaining four.
\end{abstract}

\keywords{Multi-Task Offline MARL, Skill-Discovery, Task-Efficiency}


\section{Introduction}
Multi-agent reinforcement learning (MARL), as a cornerstone of artificial intelligence, provides advanced methodologies to tackle complex challenges requiring coordinated, task-driven decision-making among multiple agents through interaction \cite{shoham2008multiagent,gronauer2022multi}. Integrated with deep neural networks, MARL has demonstrated exceptional success across a diverse range of critical applications, e.g., video games \cite{mathieu2021starcraft}, autonomous systems \cite{shalev2016safe} and finance \cite{lee2007multiagent}. With growing acknowledgment of data’s critical role in machine learning, offline MARL has drawn increased attention from researchers \cite{zhang2023odis,li2023beyond, shao2023counterfactual,liu2024offlinemultiagentreinforcementlearning}. As a data-driven approach, offline MARL learns superior policies solely from offline datasets, making it particularly suitable for domains where historical data is abundant but real-time interaction costs and risks are high.

\begin{figure}[t!]
\vskip 0.2in
\begin{center}
    \begin{subfigure}[b]{0.48\columnwidth}
        \centering
        \includegraphics[width=0.65\textwidth]{figures/3m.pdf}
        \caption{Training in \textit{3m}.}\label{fig:3m_visual}
        \includegraphics[width=0.65\textwidth]{figures/12m.pdf}
        \caption{Testing in \textit{12m}.}\label{fig:12m_visual}
    \end{subfigure}
    \hspace{0.01cm}
    \begin{subfigure}[b]{0.49\columnwidth}
        \centering
        \includegraphics[height=0.8\columnwidth]{figures/z_visual_new.pdf}
        \caption{Visualization of skills.}\label{fig:z_visual}
    \end{subfigure}
    \caption{SD-CQL effectively learns two skills, \textbf{\textit{retreat}} and \textbf{\textit{attack}}, from the source task (a) and successfully transfers them to the target task (b), each with distinct actions. This is consistent with the visualization in (c), where each marker corresponds to a skill adopted by the agent.}
\label{fig:visual}
\end{center}
\vskip -0.2in
\end{figure}

However, most existing offline MARL methods \cite{ li2023beyond,shao2023counterfactual,liu2024offlinemultiagentreinforcementlearning} are task-specific. They tend to focus solely on the dataset and its corresponding source task, thereby training a policy specifically tailored to it. Consequently, although these methods may achieve impressive performance on the source task, any alterations to the deployment environment or the task—such as a change in the number of agents to be controlled—necessitate reacquiring data and training an entirely new, tailored policy, leading to significant inefficiency. 

Hence, unlike existing approaches, we seek to build a method capable of learning versatile policies offline on a small set of known tasks and generalizing well to unseen scenarios. By doing so, one can eliminate the redundancy and resource overhead of repeatedly retraining from scratch for each new task. Furthermore, in practice, accessible task scenarios are typically limited, making it infeasible to train policies across all potential scenarios. Hence, we aim for this method to exhibit high \textit{\textbf{task-efficiency}}—handling more unseen tasks with fewer source tasks.

\begin{figure}[t!]
\vskip 0.2in
\begin{center}
    \begin{subfigure}{0.47\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/single_inv_bar_new.pdf}
        \caption{Skill-Discovery enhances multi-task generalization.}
        \label{fig:inv}
    \end{subfigure}
    \hspace{0.4cm}
    \begin{subfigure}{0.47\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/single_bar_new.pdf}
        \caption{Recent Skill-Discovery method fails due to limited source task.}
        \label{fig:single}
    \end{subfigure}
\caption{Skill-Discovery enhances cross-task generalization for offline MARL, while existing method has limitation. The number with \textit{Source} behind it indicates the source task used for training.}
\label{fig:moti}
\end{center}
\vskip -0.2in
\end{figure}

Therefore, we need to confront the following two key challenges: (i) Addressing distributional shifts in offline training, which involves how to identify appropriate skills and learn higher-performing policies using limited task data. (ii) Maintaining policy performance amidst changes, which entails that policies are sufficiently general to remain robust across varying task scales and environments. Although we can overcome the first challenge via conservative policy optimization, increased conservatism on the source task usually leads to poorer generalization to unseen tasks, highlighting the importance of solving both challenges simultaneously. 

As shown in Figure \ref{fig:inv}, we train the advanced offline MARL algorithm \cite{shao2023counterfactual} on a task with $10$ agents (source task) and test it on tasks with fewer agents. We find that it performs poorly when the number of agents in the test tasks is reduced, even though these tasks appear easier. This suggests that the agents need to learn and utilize generalizable decision-making structures across tasks, making skill-discovery a promising solution. In this approach, agents identify high-level decision patterns, i.e., ``skills", from source tasks and select the appropriate one to perform specific actions in unseen tasks, achieving cross-task generalization. However, most existing methods \cite{hsl2022,tian2024decompose} discover skills via online interactions, with limited attention to the offline setting. Recently, ODIS \cite{zhang2023odis} proposes a method for multi-task offline MARL, but as shown in Figure \ref{fig:single}, it fails to discover effective skills in small-scale source tasks. This motivates us to investigate the following question: 

\textbf{\textit{Can we design effective offline skill-discovery for strong cross-task performance with high task-efficiency?}}

To tackle these problems, we propose \underline{S}kill-\underline{D}iscovery \underline{C}onservative \underline{Q}-\underline{L}earning algorithm (\textbf{SD-CQL}). It achieves skill discovery by reconstructing the observation at the next time step and training a set of skill-conditioned policies based on these discovered skills. For example, as shown in Figure \ref{fig:visual}, the offline agents acquire two skills, \textit{retreat} and \textit{attack}, from the 3-marines task (Figure \ref{fig:3m_visual}). When deployed in the 12-marines task (Figure \ref{fig:12m_visual}), they exhibit the ability to \textit{retreat} when health is low and to \textit{attack} when health is sufficient, with \textit{attack} encompassing two specific actions: firing and advancing. It is consistent with the visualization of the skills adopted by the agent, as shown in Figure \ref{fig:z_visual}. More details can be found in Section \ref{exp_visual} and Appendix \ref{app:imp_detail}.

Different from existing reconstruction methods \cite{wang2020rode, zhang2023odis} that rely on rewards or predefine the number of skills, our skill discovery is entirely based on local observations in continuous latent space. This enables each agent to focus more on enhancing individual capabilities and eliminate the local-global alignment, making it easier to handle varying task scales and environments. Additionally, reconstruction without rewards or prior knowledge may promote agents to extract task-agnostic features from limited datasets. Then, SD-CQL trains conservative policies with separated Q-networks for fixed and variable actions. Finally, a Behavior Cloning (BC) regularization is introduced to tackle growing estimation error caused by increasing agents. As a result, SD-CQL can learn policies that effectively generalize to other tasks and achieve SOTA with only a few source tasks, demonstrating high task efficiency.

The contributions of our paper are as follows:
\begin{itemize}
    \setlength{\itemsep}{0pt}
    \item We investigate a rarely explored field: multi-task offline MARL. Our analysis reveals that existing methods suffer from limitations in scalable generalization performance and low task efficiency. To address these issues, we propose a novel approach, offering a new perspective for research in this area.
    \item We propose SD-CQL for multi-task offline MARL, which discovers skills by reconstructing the next observation. It then evaluates fixed and variable actions separately and executes the optimal action corresponding to each skill using behavior-regularized conservative Q-learning. This not only eliminates the local-global alignment required by existing methods, but also derives policies with strong multi-task generalization from a few small-scale source tasks.
    \item Through comprehensive experiments, we show that SD-CQL achieves the best performance in $\textbf{10}$ out of $14$ task sets, with an average performance improvement of up to $\textbf{65\%}$ on individual task sets over existing baselines, while the remaining four task sets are also very close to the best baseline algorithms (within $4\%$).
\end{itemize}

\section{Related Work} 
\textbf{MARL.} Multi-Agent Reinforcement Learning (MARL) has seen substantial progress in recent years, with numerous approaches developed under different paradigms. The centralized training with decentralized execution (CTDE) paradigm \cite{oliehoek2008optimal, matignon2012coordinated, IntroCTDE} has been particularly influential, with methods such as HASAC\cite{liu2024maximum}, MAPPO \cite{yu2021surprising}, QMIX \cite{qmix}, VDN \cite{sunehag2018value}, and MADDPG \cite{lowe2017multi}. These approaches use centralized training for better coordination and decentralized execution for real-time decision-making. On the other hand, fully decentralized training and execution schemes have also been explored \cite{tampuu2017multiagent,ackermann2019reducing,de2020independent}. However, the performance of such methods is often constrained by the absence of information sharing. In this paper, we mainly focus on CTDE paradigm with QMIX backbone.

\textbf{Offline MARL.} 
 Due to the absence of online interaction with the environment, offline training faces a fundamental challenge—distributional shift. To address this issue, several techniques for single-agent RL have been proposed, many of which leverage conservatism to regularize either the policy \cite{td3bc, kostrikov2021offline} or the Q-value function \cite{cql,kostrikov2021offlines,rezaeifar2022offline}. These methods mitigate the risks of overestimating the value of unseen state-action pairs. However, specific challenges caused by multiple agents, such as the exponential explosion of the complexity, hinder these techniques from directly extending to multi-agent scenarios. Therefore, several tailored approaches \cite{jiang2021offline, yang2021believe, omar, li2023beyond, shao2023counterfactual,liu2024offlinemultiagentreinforcementlearning} have been proposed to address offline MARL. However, these methods often focus excessively on source tasks, inevitably compromising their multi-task generalization ability.

\textbf{Multi-task MARL.} 
Current research on multi-task MARL can be broadly categorized into two types. The first, often referred to as Ad-Hoc Teamwork \cite{stone2010ad, zhang2023fast}, focuses on exploring how to effectively collaborate with unknown and uncontrollable teammates within a given task. The second type, which is the primary focus of this paper, involves scenarios where the algorithm controls the entire team, but the agents are trained on some tasks and testing on unseen tasks with similar structures. This requires agents to learn and utilize generalizable decision-making structures across tasks from a limited set of source tasks, positioning skill discovery as a promising solution. However, most existing approaches discover skills typically through sample reconstruction \cite{wang2020rode,hsl2022,HMASD}, clustering \cite{LI2025106852} or sub-task decomposition \cite{tian2024decompose}, primarily relying on online interactions, with limited attention to offline settings. Recently, ODIS \cite{zhang2023odis} proposed a method for multi-task offline MARL, but it suffers from the limitations of behavior cloning when the scale of source tasks is small.

\section{Background}

\subsection{Multi-task MARL and Multi-Task Offline MARL} A cooperative MARL task, indexed by \( m \), can be formulated as a decentralized partially observable Markov decision process (Dec-POMDP) \cite{oliehoek2016concise}. A Dec-POMDP is represented by a tuple \( (\mathcal{N}_{m}, \mathcal{S}_{m}, \mathcal{O}_{m}, \mathcal{A}_{m}, P_{m}, R_{m}, \gamma) \), where \( \mathcal{N}_{m} \) is the set of agents, \( \mathcal{S}_{m} \) is the set of states, \( \mathcal{O}_{m} \) is the joint observation space, \( \mathcal{A}_{m} \) is the joint action space, \( P_{m} \) is the state transition probability (defining the probability of transitioning to the next state given the current state and joint action), \( R_{m} \) is the immediate reward shared by all agents, and \( \gamma \) is the discount factor.

The multi-task MARL problem is a collection of such MARL tasks, which can be represented by the tuple \( (\mathcal{N}, \mathcal{S}, \mathcal{O}, \mathcal{A}, P, R, \gamma, \mathbb{T}) \). Here, \( \mathbb{T} \) is the set of tasks, \( \gamma \) is the discount factor shared by all tasks, and the remaining elements are the union of corresponding elements across all tasks. For example, \( \mathcal{N} = \bigcup_{m \in \mathbb{T}} \mathcal{N}_m \) represents the union of the sets of agents across all tasks.

For the task $m$, each agent maintains its observation-action history \( \tau^{i} \in \mathcal{T}^{i}_{m} \), and the corresponding joint history is denoted by \( \tau \in \mathcal{T}_{m} \). \( \mathcal{T} = \bigcup_{m \in \mathbb{T}} \mathcal{T}_{m} \) represents the collection of observation-action histories across all tasks, and the goal is to find a joint policy \( \pi: \mathcal{T}\to \mathcal{A} \) that maximizes the expected discounted return average over all tasks:
\[
\mathcal{J}=\mathbb{E}_{\pi,\mathbb{T}} \left[ \sum_{t=0}^{T} \gamma^t \cdot r_m(s^m_t, \boldsymbol{a}^m_t) \right]
\]
where $T$ is the time horizon and $r_m(s^m_t, \boldsymbol{a}^m_t)$ is the reward for taking joint action $\boldsymbol{a}^m_t$ at state $s^m_t$ in task $m$.

As for multi-task offline MARL, agents can only access the decision dataset \( \mathcal{D}=\{\mathcal{D}_m\}_{m\in\mathbb{T}_s} \), where \( \mathbb{T}_s \subset \mathbb{T} \) is referred to as \textit{source tasks} (the complement referred to as \textit{unseen tasks}), without any interaction with the environment. And the goal is still to maximize $\mathcal{J}$.

\subsection{CTDE Framework and QMIX}

A common framework for cooperative MARL is Centralized Training for Decentralized Execution (CTDE). In this framework, agents can leverage centralized information during training but must rely solely on their local observations during execution. In this work, we adopt QMIX \cite{qmix} as our backbone algorithm, which is one of the most popular discrete-action CTDE algorithms. In principle, our method can be applied to any value-based algorithm.

In QMIX, each agent maintains an individual Q-function \( Q_i(\tau^i, a^i) \), which is conditioned on its own observation-action history \( \tau^i \) and action \( a^i \). Then, it calculates a joint Q-function \( Q_{tot}(\tau, \boldsymbol{a}) \) from individual Q-functions through a mixing network \( f_s \), such that:
\begin{equation}\label{eq:qmix}
Q_{\text{tot}}(\tau, \boldsymbol{a}) = f_s(Q_1(\tau^1, a^1), \ldots, Q_n(\tau^n, a^n)).
\end{equation}
The mixing network is designed to satisfy the monotonicity constraint, ensuring that the partial derivative of \( Q_{\text{tot}} \) with respect to each \( Q_i \) is non-negative. To train the Q-function, QMIX minimizes the temporal-difference error on \( Q_{\text{tot}} \).

\begin{figure*}[t!]
%\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=1.0\textwidth]{figures/sdcql.pdf}}
\caption{SD-CQL framework. For the $i$-th agent, the encoder splits the current observation \( o_t \) into \( K_i + 1 \) entities, embedding them into consistent dimensions, with the \(0\)-th representing the agent itself. Then, the skill $z$ is extracted from $E_{i,0}$, the embedding associated with the agent itself. Finally, SD-CQL injects local information into \(z\) by reconstructing the next observation \( o_{t+1} \). Meanwhile, it computes the \(z\)-conditioned Q-values for fixed and variable actions separately, and optimizes scalable polices via CQL and BC regularization.
}
\label{fig:sdcql}
\end{center}
\vskip -0.2in
\end{figure*}

\section{Skill-Discovery Conservative Q-Learning}
To develop scalable policies that can generalize to varying unseen tasks through data from only a few source tasks, we propose Skill-Discovery Conservative Q-Learning (SD-CQL), a task-efficient algorithm for multi-task offline MARL. As illustrated in Figure \ref{fig:sdcql}, SD-CQL discovers cross-task generalizable skills in the latent space through observation reconstruction, and then trains scalable policies via multi-task CQL and BC regularization. This approach mitigates distributional shifts and error accumulation in offline MARL, as well as the generalization challenges inherent in multi-task learning. 
Below, we introduce each component in detail. 
The full algorithm is shown in Algorithm \ref{alg:sdcql}.

\subsection{Observation Reconstruction}
To improve policy generalization across tasks, a natural idea is to make agents learn task-shared decision features, i.e., ``skills", from offline data. Hence, we choose to achieve this by reconstructing the next observations without global state or task-specific rewards. However, abandoning these global or task-specific information requires the skills to enhance the ability of capturing the local features. Therefore, we leverage non-linear activation masks to address this issue.

This design is motivated by two main reasons. First, assuming unseen tasks share similar structures with source tasks, predicting next observations without task-specific details helps agents focus on transferable, task-agnostic features.
Second, using only local features reduces skill dependence on overall scale, enhances scalability, and enables direct distributed execution without local-global alignment.

\subsubsection{Observation Encoder}
Similar to the previous work \cite{hu2021updet,zhang2023odis, tian2024decompose}, to accommodate tasks with a variable number of agents, we utilize a transformer architecture \cite{vaswani2017attention} to handle observations of different sizes. Specifically, we first decompose the $i$-th agent's observation at $t$-th time step $o^i_t$ into $o_t^{i,0}$ related to itself, and $\{o_t^{i,k}\}_{k=1}^{K_a}\cup \{o_t^{i,k}\}_{k=K_a+1}^{K_i}$ related to other entities, where $K_a$ is the number of entities that can interact with agents $i$, and $K_i$ is the total entities observed by agent $i$. 

We further encode these heterogeneous entity information into embeddings with same dimensions:
\begin{equation}
 e_{i,k}=\text{Emb}(o_t^{i,k}), \quad k=0,1,2\dots, K_i
\end{equation}
where $\text{Emb}(\cdot)$ represents the heterogeneous embedding network. Finally, we use a single-layer transformer to capture the relationships between entities and obtain the final encoding vector $\{E_{i,k}\}_{k=0}^{K_i}$ for each entity. 

Additionally, since the agents' local observations are relatively limited, we incorporate a recurrent latent variable in accordance with standard practices to capture temporal information. We also incorporate the previous actions \( a_{t-1} \) in \( o_t \), but exclude them from the reconstruction target \( o_{t+1} \).

\subsubsection{Observation Decoder}
To enable agents to select appropriate skills corresponding to their specific situations, we introduce an observation decoder. By reconstructing the next observation, agents can discover skill vectors in a latent space. Concretely, for the embeddings $\{E_{i,k}\}_{k=0}^{K_i}$ from the observation encoder, the decoder first projects the own information \( E_{i,0} \) into a latent skill vector \( z \in \mathcal{Z}\) through a linear layer. Then, to make the skill vector extract more information, we reconstruct the embedding associated with the agent itself directly with $z$:
\begin{equation}
\widehat{E}_{i,0} = W_0 \cdot \text{ReLU}(z) + b_0
\end{equation}
While for embeddings of other entities, we first use a ReLU function to mask parts of the information, and then concatenate them with $z$ for reconstruction:

\begin{equation}
\widehat{E}_{i,k} = W_k \cdot \left[\text{ReLU}(E_{i,k}),~z\right] + b_k,\quad k=1,2,\dots,K_i
\end{equation}
where $[\cdot,~\cdot]$ represents the concatenation operation.

Finally, similar to the encoder, the reconstructed embeddings are passed through a single-layer transformer, restored into entity vectors \(\{\hat{o}_{i,k}\}_{k=0}^{K_i}\), and reassembled into the next observation \( \hat{o}^i_{t+1} \). Then, the final reconstruction loss is: 
\begin{equation}\label{rec_loss}
\mathcal{L}_{\text{Rec}}=\dfrac{1}{N}\sum_{i=1}^{N}\text{MSE}(\hat{o}_{t+1}^i,o_{t+1}^i)
\end{equation}
where $N$ is the number of agents to control in the task, and $\text{MSE}(\cdot,\cdot)$ represents the Mean Square Error. 

\subsection{Skill-conditioned Policy Optimization}
After identifying and selecting the appropriate skill, existing multi-task offline MARL algorithms primarily execute the corresponding actions through behavior cloning (BC). However, since a single skill often encompasses multiple specific actions, selecting the optimal skill does not necessarily ensure that all associated actions are optimal.

Therefore, we leverage conservative Q-learning to optimize the skill-conditioned policy $\pi(a\mid s,~z)$, enabling the execution of optimal actions associated with the selected skill. To mitigate the accumulated estimation errors inherent in Q-learning and reduce distributional shift, SD-CQL separately evaluates fixed and variable actions with BC regularization, thereby enhancing training stability for larger-scale tasks.

\subsubsection{Skill-conditioned Q-value}
To handle the variation in the number of actions across different tasks, we utilize two separate Q-networks. One network, called $Q^{own}$, is responsible for a fixed set of actions related to the agent itself, while the other, called $Q^{var}$, handles a variable set of actions associated with other entities in its observation. Both networks receive the corresponding entity embeddings $E_{i,k}$ and skill vectors $z$ as inputs and output the Q-values for their respective actions. The parameters of Q-value networks are shared by all the agents. Therefore, the individual Q-values for the $i$-th agent with skill $z_i$ are: 
\begin{equation}
Q_i(a\mid o_i, z_i)=\begin{cases}
Q^{own}(a\mid E_{i,0},~z_i) &\text{if }a \in \mathcal{A}_i^{own} \\ 
Q^{var}(a\mid E_{i,k},~z_i) &\text{if }a \in \mathcal{A}_{i}^{k} 
\end{cases}
\end{equation}
where $\mathcal{A}_i^{own}$ is the set of actions that only related to the $i$-th agent itself, and  $\mathcal{A}_{i}^{k}$ is the set of actions that related to the $i$-th agent and the $k$-th other entity. 

To avoid interference between observation reconstruction and policy optimization, we truncate the gradient propagation of the embeddings and skill vectors before feeding them into the Q-network.

Finally, we employ a mixing network to aggregate the individual Q-values into a global Q-value $Q_{\text{tot}}$ according to \eqref{eq:qmix}. Then, we can optimize $Q_{\text{tot}}$ through the temporal difference loss outlined below:
\begin{equation}\label{td_loss}
\mathcal{L}_{\text{TD}}=\left[ Q_{\text{tot}}({\tau}, \boldsymbol{a} \mid \boldsymbol{z}) - \left( r + \gamma \max_{\boldsymbol{a}'} \bar{Q}_{\text{tot}}({\tau}', \boldsymbol{a}' \mid \boldsymbol{z}') \right) \right]^2
\end{equation}
where \( {\tau}' \), \( \boldsymbol{a}' \) and \( \boldsymbol{z}' \) denote the next observation-action history, joint action and joint skill, respectively, and $\bar{Q}_{\text{tot}}$ represents the target joint action-value function.

\subsubsection{Conservative Q-learning}
In the offline setting, agents are unable to ascertain the true environment distribution. Consequently, even on tasks with available training data, agents may still make detrimental decisions by overestimating out-of-distribution (OOD) state-action pairs. To address this issue, we employ Conservative Q-learning (CQL) \cite{cql} regularization term. This regularization term enhances the evaluation of in-dataset state-action pairs while suppressing the evaluation of OOD ones, thereby achieving conservative offline value estimation:
\begin{equation}
\mathcal{L}_{\text{CQL}}=\mathbb{E}_{\tau \sim \mathcal{D}, \boldsymbol{a} \sim \mu} \left[ Q(\tau, \boldsymbol{a}\mid \boldsymbol{z}) \right] - \mathbb{E}_{(\tau, \boldsymbol{a}) \sim \mathcal{D}} \left[ Q(\tau, \boldsymbol{a}\mid \boldsymbol{z}) \right] \label{CQL_Regularization}
\end{equation}
where $\mu$ denotes the sampling distribution (e.g., sampling from a uniform distribution).
And the total loss function for Q-learning is:
\begin{equation}\label{q_learning}
\mathcal{L}_{\text{Q}} = \mathcal{L}_{\text{TD}} + \alpha \mathcal{L}_{\text{CQL}}
\end{equation}
where $\alpha > 0$ is a hyperparameter to control conservatism.
\subsubsection{Behavior Cloning Regularization}
In addition to distributional shifts, the accumulation of estimation errors caused by multiple agents is another challenging issue. In multi-agent environments, relying solely on Q-learning remains inadequate to mitigate the rapidly escalating Q-value estimation errors \cite{res}. Therefore, drawing inspiration from previous studies \cite{td3bc, omar}, we additionally incorporate a BC regularization to improve the stability of SD-CQL's performance across tasks of different scales.
\begin{equation}\label{bc-term}
\mathcal{L}_{\text{BC}} = \dfrac{1}{N}\sum_{i=1}^{N}\text{CE}\left(\text{Softmax}(Q_i),~\beta_i\right)
\end{equation}
where $\text{CE}\left(\cdot,\cdot\right)$ is the cross entropy loss, $N$ is the number of agents, $Q_i$ is the individual Q-values of all the action available for the $i$-th agent, and $\beta_i$ is the corresponding one-hot action in the offline datasets.

Then, the total loss function for of SD-CQL is:
\begin{equation}\label{total_loss}
\mathcal{L} = (1-\eta)\cdot\mathcal{L}_{\text{Q}} + \eta \cdot \mathcal{L}_{BC} + \mathcal{L}_{\text{Rec}}
\end{equation}
where $\eta \in [0,1]$ is the hyperparameter to control the strength of the BC regularization.

\begin{algorithm}[ht]
   \caption{Skill-Discovery Conservative Q-Learning}
   \label{alg:sdcql}
\begin{algorithmic}
   \STATE {\bfseries Input:} Datasets of source tasks $\{\mathcal{D}_m\}_{m\in\mathbb{T}_s}$.
   \STATE Initialize the parameters $\phi$ for the encoder and decoder, $\theta$ for $Q_{tot}$ and $\bar\theta$ for target $Q_{tot}$.
   \FOR{$i=1${\bfseries to} $T_{\text{max}}$}
   \FOR{$j=1$ {\bfseries to} $|\mathbb{T}_s|$}
   \STATE Sample trajectories $\hat\tau=\{(s_t,o_t,a_t,r_t,o'_t)\}$ from $\mathcal{D}_j$.
   \STATE Calculate the total loss $\mathcal{L}_{j}$ by $\hat\tau$ for task $j$, according to \eqref{total_loss}.
   \ENDFOR
   \STATE Calculate the multi-task loss $\mathcal{L}=\sum_{j=1}^{|\mathbb{T}_s|}\mathcal{L}_{j}$.
   \STATE Update $\phi$ with $\nabla_\phi \mathcal{L}$ and update $\theta$ with $\nabla_\theta \mathcal{L}$
   \IF{$i \equiv 0 \pmod {T_{\text{update}}}$}
   \STATE $\bar\theta \leftarrow \theta$
   \ENDIF
   \ENDFOR
\end{algorithmic}
\end{algorithm}

\section{Experiments}
\begin{table*}[!ht]
\caption{Win rates of Multi-to-Multi task sets. The reported results are the average performance over all source tasks and unseen tasks, and are averaged over 5 random seeds. The bold number denotes the best performance and $\pm$ denotes one standard deviation.}\label{tab:mm-win-rates}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\textwidth}{!}{
\begin{tabular}{c|lcccccc}
\toprule
&\textbf{Task Set} & \textbf{BC-t} & \textbf{BC-r} & \textbf{UpDeT} & \textbf{ODIS} & \textbf{SD-CQL (Ours)} \\
\midrule
\multirow{3}{*}{\rotatebox{90}{\textnormal{Expert}}} 
& \textit{Marine-Easy} & 98.87 $\pm$ 0.58 & \textbf{99.38} $\pm$ 0.40 & 28.25 $\pm$ 18.52 & 70.50 $\pm$ 30.14  & 97.31 $\pm$ 1.33   \\
& \textit{Marine-Hard} & 62.97 $\pm$ 2.18 & \textbf{64.84} $\pm$ 3.24 &26.98 $\pm$ 13.21& 21.09 $\pm$ 14.72 & 63.65 $\pm$ 7.20 \\
& \textit{Stalker-Zealot} & 68.61 $\pm$ 3.93 & 63.51 $\pm$ 2.64 &17.07 $\pm$ 18.36& 55.05 $\pm$ 9.11 & \textbf{75.14} $\pm$ 7.78\\
\midrule
\multirow{3}{*}{\rotatebox{90}{\textnormal{Medium}}}
& \textit{Marine-Easy} & 71.81 $\pm$ 3.74 & 71.69 $\pm$ 4.33 & 54.00 $\pm$ 9.19& 68.50 $\pm$ 5.74 & \textbf{75.87} $\pm$ 3.15\\
& \textit{Marine-Hard} & 42.08 $\pm$ 2.63 & 41.51 $\pm$ 3.53 &27.92 $\pm$ 5.32& 32.55 $\pm$ 4.31 & \textbf{46.35} $\pm$ 3.44 \\
& \textit{Stalker-Zealot} & \textbf{23.94} $\pm$ 2.72 & 20.82 $\pm$ 3.78 &12.55 $\pm$ 5.8& 11.97 $\pm$ 7.53 & 22.98 $\pm$ 4.04 \\
\midrule
\multirow{3}{*}{\centering\rotatebox{90}{\makecell{\textnormal{Medium}\\\textnormal{Replay}}}} 
& \textit{Marine-Easy} & 44.12 $\pm$ 8.19 & 49.62 $\pm$ 9.17 &6.69 $\pm$ 3.18& 11.06 $\pm$ 8.94 & \textbf{73.13} $\pm$ 11.31 \\
& \textit{Marine-Hard}  & \textbf{46.72} $\pm$ 2.35 & 46.98 $\pm$ 1.95 &19.79 $\pm$ 6.16& 37.29 $\pm$ 5.48 & 46.20 $\pm$ 2.51\\
& \textit{Stalker-Zealot} & 12.50 $\pm$ 1.75 & 17.64 $\pm$ 4.40 &6.83 $\pm$ 7.03& 8.17 $\pm$ 6.08 & \textbf{22.69} $\pm$ 3.07 \\
\midrule
\multirow{3}{*}{\centering\rotatebox{90}{\makecell{\textnormal{Medium}\\\textnormal{Expert}}}} 
& \textit{Marine-Easy} & 74.12 $\pm$ 3.21 & 75.25 $\pm$ 7.41 &48.81 $\pm$ 11.48& 66.00 $\pm$ 9.21& \textbf{80.38} $\pm$ 7.96\\
& \textit{Marine-Hard} & 51.93 $\pm$ 7.21 & 49.27 $\pm$ 5.16 &17.81 $\pm$ 6.74& 19.58 $\pm$ 17.63 & \textbf{53.39} $\pm$ 4.31 \\
& \textit{Stalker-Zealot} & 38.17 $\pm$ 5.38 & 39.62 $\pm$ 2.98 &15.53 $\pm$ 5.01& 30.72 $\pm$ 14.65 & \textbf{65.62} $\pm$ 6.27  \\
\bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}


To evaluate the performance of SD-CQL in multi-task offline MARL scenarios, we establish multiple transfer training task sets based on the StarCraft Multi-Agent Challenge (SMAC) \cite{samvelyan19smac}. Through these experimental task sets, we aim to address the following questions: (i) Compared to existing algorithms, does SD-CQL demonstrate better performance in multi-task offline MARL (Section \ref{exp_eval}), (ii) Do skill vectors indeed characterize the decision-making features of agents in different contexts (Section \ref{exp_visual}), and (iii) Are components within SD-CQL critical to its performance (Appendix \ref{app:ablation}). 
\subsection{Setup}

\textbf{Datasets.} Following the definition by D4RL \cite{fu2020d4rl}, our experiments utilize datasets of four quality: \textit{Expert}, \textit{Medium}, \textit{Medium-Replay}, and \textit{Medium-Expert}. For fair comparisons, we use the datasets provided by ODIS \cite{zhang2023odis}. Details are provided in Appendix \ref{app:datasets}.

\textbf{Task Sets.} 
To comprehensively simulate different transfer scenarios, we establish five representative offline training zero-shot transfer task sets: \textit{Marine-Easy}, \textit{Marine-Hard}, \textit{Stalker-Zealot}, \textit{Marine-Single}, and \textit{Marine-Single-Inv}. They can be categorized into two types:  (i) Training on multiple tasks and test on multiple tasks (Multi-to-Multi) and (ii) Training on one task and test on multiple tasks (One-to-Multi). The first three task sets, which fall under the Multi-to-Multi category, are consistent with those in ODIS \cite{zhang2023odis}, while the last two, classified as One-to-Multi, are additionally designed by us. More details are included in Appendix \ref{app:tasks}.

\textbf{Baselines.}
We primarily select four (offline) multi-task MARL algorithms as baselines: BC-t, BC-r, UpDeT \cite{hu2021updet}, and ODIS \cite{zhang2023odis}. Among them, BC-t is a behavior cloning approach based on a multi-task transformer, BC-r incorporates return-to-go information into the input of BC-t, UpDeT is a widely-used baseline for  algorithms involving varying agents, and ODIS is the state-of-the-art algorithm tailored for offline multi-task MARL.

\textbf{Experiment Setup.} 
We implement SD-CQL based on the PYMARL2 \cite{pymarl2} and the ODIS codebase, while the baseline algorithms are directly utilized from the codes implemented by ODIS \cite{zhang2023odis}. To ensure fairness, each algorithm runs $35,000$ steps of multi-task offline training. The exception is ODIS, which additionally requires an initial pre-training of $15,000$ steps. During evaluation, each test environment runs $32$ episodes, and the average results are recorded. Unless otherwise specified, we report the average performance of the final policy across five random seeds, with the best performance for each task highlighted in bold. The learning curves are presented in Appendix \ref{app:curve}.

For SD-CQL, the primary hyperparameter adjusted is the BC weight $\eta$, with the CQL weight $\alpha$ fixed at $1.0$. As to ODIS, we use the hyperparameter configurations provided in the original paper \cite{zhang2023odis} for Multi-to-Multi task sets. For One-to-Multi task sets we specifically designed, ODIS does not offer hyperparameter configurations. Thus, we adjust the conservative coefficient and distribution coefficient, reporting the best set of results. All other hyperparameters remain consistent with the default settings. For implementation details and main hyperparameter settings, please refer to Appendix \ref{app:hyperparam}.

\subsection{Evaluation}\label{exp_eval}
\begin{table*}[!ht]
\caption{Win rates of One-to-Multi task sets. The reported results are the average performance over all source tasks and unseen tasks, and are averaged over 5 random seeds. The bold number means the best performance and $\pm$ denotes one standard deviation.}\label{tab:om-win-rates}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\textwidth}{!}{
\begin{tabular}{clcccccc}
\toprule
\multicolumn{2}{c}{\textbf{Task Set}} & \textbf{BC-t} & \textbf{BC-r} & \textbf{UpDeT} & \textbf{ODIS} & \textbf{SD-CQL (Ours)} \\
\midrule
\multirow{5}{*}{\rotatebox{90}{\makecell{\textit{Marine}\\\textit{Single}}}} 
    &3m $^\diamond$ & \textbf{100.00} $\pm$ 0.00 & 96.88 $\pm$ 4.42 & 71.88 $\pm$ 12.69 & 99.38 $\pm$ 1.4 & \textbf{100.00} $\pm$ 0.00\\
    &5m & 81.88 $\pm$ 9.22 & 67.5 $\pm$ 27.3 & 61.25 $\pm$ 19.34 & 37.5 $\pm$ 33.15 & \textbf{91.25} $\pm$ 12.58\\
    &8m & 38.75 $\pm$ 22.38 & 35.62 $\pm$ 29.37 & 27.50 $\pm$ 13.15& 11.88 $\pm$ 21.47 & \textbf{45.00} $\pm$ 18.70\\
    &10m & 20.62 $\pm$ 22.38 & 16.25 $\pm$ 20.54 & 18.75 $\pm$ 9.38& 11.25 $\pm$ 25.16 & \textbf{36.88} $\pm$ 31.51\\
    &12m & 9.38 $\pm$ 13.98 & 2.50 $\pm$ 5.59 & 15.00 $\pm$ 11.98& 6.25 $\pm$ 13.98 & \textbf{30.62} $\pm$ 26.28\\
    \midrule
    \multicolumn{2}{c}{\textbf{Average}} & 50.12 $\pm$ 9.65 & 43.75 $\pm$ 14.42 &38.88 $\pm$ 9.83& 33.25 $\pm$ 16.50 & \textbf{60.75} $\pm$ 13.46\\
\midrule\midrule
\multirow{5}{*}{\rotatebox{90}{\makecell{\textit{Marine} \\ \textit{Single-Inv}}}}
    &10m $^\diamond$ & \textbf{100.00} $\pm$ 0.00 & 98.75 $\pm$ 2.80 & 75.00 $\pm$ 42.62& 88.75 $\pm$ 12.22 & 99.38 $\pm$ 1.40 \\
    &8m &  98.75 $\pm$ 1.71 & 83.12 $\pm$ 29.28 & 39.38 $\pm$ 33.99& 90.62 $\pm$ 8.84 & \textbf{100.00} $\pm$ 0.00\\
    &5m & 3.75 $\pm$ 6.77 & 33.12 $\pm$ 33.63 &50.62 $\pm$ 31.51 & 87.50 $\pm$ 8.84  & \textbf{92.50} $\pm$ 3.56\\
    &4m & 1.25 $\pm$ 2.80 & 20.62 $\pm$ 33.04 &32.50 $\pm$ 18.43 & 62.50 $\pm$ 21.99 & \textbf{64.38} $\pm$ 23.13\\
    &3m & 0.00 $\pm$ 0.00  & 6.88 $\pm$ 13.69 & \textbf{47.50 }$\pm$ 36.74& 12.50 $\pm$ 18.22 & 40.62 $\pm$ 18.88\\
    \midrule
    \multicolumn{2}{c}{\textbf{Average}} & 40.75 $\pm$ 1.83 & 48.50 $\pm$ 17.09 & 49.00 $\pm$ 27.14 & 68.38 $\pm$ 11.47 & \textbf{79.38} $\pm$ 5.55\\
\bottomrule
\multicolumn{5}{l}{\small $\diamond$ denotes the source task.}\\
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\subsubsection{Multi-to-Multi}
The Multi-to-Multi task sets include \textit{Marine-Easy}, \textit{Marine-Hard}, and \textit{Stalker-Zealot}, each comprising three source tasks of varying scales and several unseen tasks. Agents are trained offline using datasets from the source tasks and are tested across all tasks, with zero-shot testing conducted on the unseen tasks. For each task set, we set up four datasets of different qualities. We report the average performance of the algorithms across all tasks for each task set and dataset in Table \ref{tab:mm-win-rates}. Detailed results can be found in Appendix \ref{app:results}.

The results show that, SD-CQL achieves the best performance for $8$ out of $12$ task sets, with the other four being very close to the best baseline algorithms (within 4\%). In particular, in \textit{Stalker-Zealot-Medium-Expert} and \textit{Marine-Easy-Medium-Replay} task sets, SD-CQL significantly outperforms other algorithms, with performance improvements of 65\% and 47\%, respectively, compared to the best-performing baseline. This demonstrates that SD-CQL is capable of maintaining superior multi-task offline generalization performance across various dataset qualities.

\subsubsection{One-to-Multi}
The One-to-Multi task sets include \textit{Marine-Single} and \textit{Marine-Single-Inv}. Specifically, \textit{Marine-Single} requires agents to train offline using only the dataset from the \textit{3m} task and to test on tasks with scales up to \textit{12m} to assess the agents' generalization ability to larger-scale tasks. (\textit{3m} refers to each side having $3$ Marine units, and similarly for the others). Conversely, \textit{Marine-Single-Inv} requires agents to train offline only from the \textit{10m} task and to test on tasks with a minimum scale of \textit{3m} to evaluate whether the agents can truly learn more general decision-making skills. It is unlikely to expect a well-generalized policy from a small set of tasks with poor datasets. Hence, we use the expert dataset to simulate mastering simpler tasks before extending to more complex ones. We report the full results in Table \ref{tab:om-win-rates}.

It can be seen that, since the \textit{Marine-Single} task set is trained using only \textit{3m-expert} data, all algorithms perform better on tasks with smaller agent scales. However, as the scale of the test tasks increases, the performance of baseline algorithms based on BC sharply degrades. In contrast, the performance degradation of SD-CQL is significantly less, maintaining an average win rate of $30\%$ even when the task scale is expanded to \textit{12m}, four times the original size. Moreover, SD-CQL exhibits the best performance across all test tasks, with average performance far surpassing other baseline algorithms. This indicates that SD-CQL indeed learns general decision-making skills through skill-discovery with higher task-efficiency, enabling it to better handle larger-scale unseen tasks. 

For the \textit{Marine-Single-Inv} task set, SD-CQL also demonstrates superior generalization performance on inverse generalization tasks. In contrast, the BC algorithms completely fail on smaller-scale tasks despite performing adequately on source tasks, as they do not learn generalizable skills. Although UpDeT performs relatively well on the \textit{3m} task, it exhibits high variance, and its overall performance is significantly worse than that of SD-CQL. ODIS's skill-learning mechanism alleviates the generalization difficulties inherent in pure BC algorithms. However, since its final actions still rely on BC-generated outputs, its generalization performance remains considerably below that of SD-CQL. 

\subsection{Skill-Discovery Visualization}\label{exp_visual}
To provide a more intuitive demonstration of the multi-task generalization capability of SD-CQL, we visualize part of decision scenarios of the source task \textit{3m} and the target task \textit{12m} from the \textit{Marine-Single} task set in Figure \ref{fig:visual}. Specifically, we record the battle replays (\ref{fig:3m_visual} and \ref{fig:12m_visual}) and project the corresponding skill vectors $z$ at each decision point onto a two-dimensional plane using t-SNE \cite{tsne} (\ref{fig:z_visual}, where each marker represents the skill vector $z$ chosen by an agent at the moment recorded in \ref{fig:3m_visual} and \ref{fig:12m_visual}). More details are available in Appendix \ref{app:visual}. 

It can be observed that SD-CQL successfully learns two skills, \textit{\textbf{retreat}} and \textit{\textbf{attack}}, in the \textit{3m} task and applies them effectively in the unseen \textit{12m} task. When the healthy level is low, the agents choose to \textit{\textbf{retreat}}, while when health is sufficient, they opt to \textit{\textbf{attack}}, which is consistent with the visualization in \ref{fig:z_visual}. This demonstrates that SD-CQL indeed discovers effective skills. Notably, in \textit{12m}, agents adopting the \textit{\textbf{attack}} skill exhibit two specific actions: \textit{firing} and \textit{advancing} (highlighted by the green dashed box), further supporting that SD-CQL extracts cross-task generalizable decision structures instead of mimicking specific actions.


\section{Conclusion}
In this paper, we propose a task-efficient multi-task offline MARL algorithm, Skill-Discovery Conservative Q-Learning (SD-CQL). It discovers skills offline through observation reconstruction and trains scalable policies via behavior-regularized conservative Q-learning. We conduct substantial experiments on StarCraftII to present the superior generalization performance and task-efficiency of SD-CQL: It achieves the best performance on $\textbf{10}$ out of $14$ task sets, with up to $\textbf{65\%}$ improvement on individual task sets, and is within $4\%$ of the best baseline on the remaining four. We hope this work inspires future researchers to engage in the study of scalable multi-task offline MARL, further improving the performance and efficiency of multi-agent decision-making.
\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

\bibliography{plain}
\bibliographystyle{plain}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
The organization of the appendix is as follows:
\begin{itemize}
    \setlength{\itemsep}{0pt}
    \item Appendix \ref{app:exp_detail}: We provide details of the experiment, including the composition of the datasets (\ref{app:datasets}), the primary information of the tasks and task sets (\ref{app:tasks}), implementation specifics of the algorithms (\ref{app:imp_detail}), key hyperparameters (\ref{app:hyperparam}), and the visualization process (\ref{app:visual}).
    \item Appendix \ref{app:ablation}: We present ablation experiments and results regarding the main components and hyperparameters of SD-CQL.
    \item Appendix \ref{app:results}: We report the detailed results for Multi-to-Multi task sets.
    \item Appendix \ref{app:curve}: We display the learning curves of SD-CQL and all baselines, showing the average performance across different task sets.
\end{itemize}
%\newpage
\section{Experiment Details}\label{app:exp_detail}
\subsection{Datasets}\label{app:datasets}
Following the definition by \cite{fu2020d4rl}, we conduct experiments primarily on datasets of four quality. The collection procedure for each quality is as follows: 
\begin{itemize}
    \setlength{\itemsep}{0pt}
    \item Expert: Trajectories collected from policies trained to an expert level using QMIX.
    \item Medium: Trajectories collected from policies trained to a medium level using QMIX.
    \item Medium-Replay: The replay buffer generated while training QMIX policies to a medium level.
    \item Medium-Expert: A 50-50 mixture of Medium and Expert trajectories.
\end{itemize}
To ensure fair comparisons, we use the datasets provided by ODIS \cite{zhang2023odis}, where only up to $2,000$ trajectories are sampled for each dataset. The key information of the datasets used in our experiments is summarized in Table \ref{tab: datasets}.
%For fair comparisons, we download and adopt the datasets provided by ODIS \cite{zhang2023odis}, where they only sample up to $2000$ trajectories for each dataset. The primary information of the dataset used for experiments is listed in Table \ref{tab: datasets}.

\newpage
\begin{table}[!ht]
\caption{The primary information of offline datasets used in our experiments.}
\label{tab: datasets}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcc}
    \toprule
    \textbf{Task} & \textbf{\#Trajectories} & \textbf{Average Return}\\
    \midrule
    3m-Expert & 2,000 & 19.87\\
    3m-Medium & 2,000 & 14.00\\
    3m-Medium-Replay & 2,000 & 10.71\\
    3m-Medium-Expert & 2,000 & 16.94\\
    5m-Expert & 2,000 & 19.93\\
    5m-Medium & 2,000 & 17.35\\
    5m-Medium-Replay & 1,266 & 3.21\\
    5m-Medium-Expert & 2,000 & 18.66\\
    10m-Expert & 2,000 & 19.89\\
    10m-Medium & 2,000 & 16.63\\
    10m-Medium-Replay & 331 & 2.30\\
    10m-Medium-Expert & 2,000 & 18.27\\
    \midrule
    5m\_vs\_6m-Expert & 2,000 & 17.34\\
    5m\_vs\_6m-Medium & 2,000 & 12.63\\
    5m\_vs\_6m-Medium-Replay & 2,000 & 9.41\\
    5m\_vs\_6m-Medium-Expert & 2,000 & 14.98\\
    9m\_vs\_10m-Expert & 2,000 & 19.59\\
    9m\_vs\_10m-Medium & 2,000 & 15.52\\
    9m\_vs\_10m-Medium-Replay & 2,000 & 11.76\\
    9m\_vs\_10m-Medium-Expert & 2,000 & 17.49\\
    \midrule
    2s3z-Expert & 2,000 & 19.78\\
    2s3z-Medium & 2,000 & 16.61\\
    2s3z-Medium-Replay & 2,000 & 14.25\\
    2s3z-Medium-Expert & 2,000 & 18.26\\
    2s4z-Expert & 2,000 & 19.73\\
    2s4z-Medium & 2,000 & 16.85\\
    2s4z-Medium-Replay & 2,000 & 14.38\\
    2s4z-Medium-Expert & 2,000 & 18.26\\
    3s5z-Expert & 2,000 & 19.78\\
    3s5z-Medium & 2,000 & 16.31\\
    3s5z-Medium-Replay & 2,000 & 15.29\\
    3s5z-Medium-Expert & 2,000 & 18.05\\
    \bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
\newpage
\subsection{Tasks Configuration}\label{app:tasks}
In this section, we present the key information of all tasks involved in our experiments in Table \ref{tab: tasks} and the task composition of each task set in Table \ref{tab: tasksets}.
\begin{table}[!ht]
\caption{The key information of tasks used in our experiments.}
\label{tab: tasks}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lll}
    \toprule
    \textbf{Task Name} & \textbf{Allied Units}& \textbf{Enemy Units}\\%& \textbf{Task Features}\\
    \midrule
    3m & 3 Marines & 3 Marines \\%& Homogeneous and Symmetry\\
    4m & 4 Marines & 4 Marines \\%& Homogeneous and Symmetry\\
    5m & 5 Marines & 5 Marines \\%& Homogeneous and Symmetry\\
    6m & 6 Marines & 6 Marines \\%& Homogeneous and Symmetry\\
    7m & 7 Marines & 7 Marines \\%& Homogeneous and Symmetry\\
    8m & 8 Marines & 8 Marines \\%& Homogeneous and Symmetry\\
    9m & 9 Marines & 9 Marines \\%& Homogeneous and Symmetry\\
    10m & 10 Marines & 10 Marines \\%& Homogeneous and Symmetry\\
    11m & 11 Marines & 11 Marines \\%& Homogeneous and Symmetry\\
    12m & 12 Marines & 12 Marines \\%& Homogeneous and Symmetry\\
    \midrule
    5m\_vs\_6m & 5 Marines & 6 Marines \\%& Homogeneous and Asymmetry\\
    6m\_vs\_7m & 6 Marines & 7 Marines \\%& Homogeneous and Asymmetry\\
    7m\_vs\_8m & 7 Marines & 8 Marines \\%& Homogeneous and Asymmetry\\
    8m\_vs\_9m & 8 Marines & 9 Marines \\%& Homogeneous and Asymmetry\\
    9m\_vs\_10m & 9 Marines & 10 Marines \\%& Homogeneous and Asymmetry\\
    10m\_vs\_11m & 10 Marines & 11 Marines \\%& Homogeneous and Asymmetry\\
    10m\_vs\_12m & 10 Marines & 12 Marines \\%& Homogeneous and Asymmetry\\
    13m\_vs\_15m & 13 Marines & 15 Marines \\%& Homogeneous and Asymmetry\\
    \midrule
    1s3z & 1 Stalkers, 3 Zealots & 1 Stalkers, 3 Zealots \\%& Heterogeneous and Heterogeneous\\
    1s4z & 1 Stalkers, 4 Zealots & 1 Stalkers, 4 Zealots \\%& Heterogeneous and Heterogeneous\\
    1s5z & 1 Stalkers, 5 Zealots & 1 Stalkers, 5 Zealots \\%& Heterogeneous and Heterogeneous\\
    2s3z & 2 Stalkers, 3 Zealots & 2 Stalkers, 3 Zealots \\%& Heterogeneous and Heterogeneous\\
    2s4z & 2 Stalkers, 4 Zealots & 2 Stalkers, 4 Zealots \\%& Heterogeneous and Heterogeneous\\
    2s5z & 2 Stalkers, 5 Zealots & 2 Stalkers, 5 Zealots \\%& Heterogeneous and Heterogeneous\\
    3s3z & 3 Stalkers, 3 Zealots & 3 Stalkers, 3 Zealots \\%& Heterogeneous and Heterogeneous\\
    3s4z & 3 Stalkers, 4 Zealots & 3 Stalkers, 4 Zealots \\%& Heterogeneous and Heterogeneous\\
    3s5z & 3 Stalkers, 5 Zealots & 3 Stalkers, 5 Zealots \\%& Heterogeneous and Heterogeneous\\
    4s3z & 4 Stalkers, 3 Zealots & 4 Stalkers, 3 Zealots \\%& Heterogeneous and Heterogeneous\\
    4s4z & 4 Stalkers, 4 Zealots & 4 Stalkers, 4 Zealots \\%& Heterogeneous and Heterogeneous\\
    4s5z & 4 Stalkers, 5 Zealots & 4 Stalkers, 5 Zealots \\%& Heterogeneous and Heterogeneous\\
    4s6z & 4 Stalkers, 6 Zealots & 4 Stalkers, 6 Zealots \\%& Heterogeneous and Heterogeneous\\
    \bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{table}[!ht]
\caption{The task composition of each task set used in our experiments.}
\label{tab: tasksets}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\textwidth}{!}{
\begin{tabular}{lll}
    \toprule
    \textbf{Task Sets} & \textbf{Source Tasks}& \textbf{Unseen Tasks}\\%& \textbf{Task Features}\\
    \midrule
    Marine-Easy & 3m, 5m, 10m & 4m, 6m, 7m, 8m, 9m, 11m, 12m \\
    \midrule
    \multirow{2}{*}{Marine-Hard} & \multirow{2}{*}{3m, 5m\_vs\_6m, 9m\_vs\_10m} & 4m, 5m, 10m, 12m, 6m\_vs\_7m, 7m\_vs\_8m, 8m\_vs\_9m, \\ 
    & & 10m\_vs\_11m, 10m\_vs\_12m, 13m\_vs\_15m \\
    \midrule
    Stalker-Zealot & 2s3z, 2s4z, 3s5z & 1s3z, 1s4z, 1s5z, 2s5z, 3s3z, 3s4z, 4s3z, 4s4z, 4s5z, 4s6z \\
    \midrule
    Marine-Single & 3m & 5m, 8m, 10m, 12m \\
    \midrule
    Marine-Single-Inv & 10m & 3m, 4m, 5m, 8m \\
    \bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\newpage

\subsection{Implement Details}\label{app:imp_detail}
In the observation reconstruction part, we use a single-layer Transformer, and full connected layers for embedding and skill extraction. In the Q-learning part, we use MLPs as the individual Q network, and similar to ODIS, we employ an attention-based mixing network to handle variable inputs in multi-task training. The entity decomposition method and its dimensions are the same as those used in the observation reconstruction. The specific parameters of the network structure are shown in Table \ref{tab: arch}. For Conservative Q-Learning, the sampling strategy $\mu$ we use is a uniform distribution over the action space, with 100 samples drawn to estimate the CQL regularization term. Additionally, consistent with the QMIX implementation provided by PYMARL2 \cite{pymarl2}, we use TD($\lambda$) to enhance stability when calculating the TD loss, and the $\lambda$ parameter is set to the default value from PYMARL2.

For the baselines, we utilize the implementations provided by ODIS. Specifically, BC-t employs a transformer capable of decomposing observations for multi-task training while optimizing the policy through behavior cloning. BC-r is similar to BC-t but incorporates return-to-go information as an additional input alongside observations. UpDeT, originally designed as an online algorithm, has been adapted by ODIS with a multi-task mixing network and conservative Q-learning for offline multi-task training. The implementation of ODIS aligns with its official version.

\begin{table}[!ht]
\caption{The main network structure involved in SD-CQL.}
\label{tab: arch}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\textwidth}{!}{
\begin{tabular}{lccc}
    \toprule
    \textbf{Network} & \textbf{Architecture} & \textbf{Activation Function}\\
    \midrule
    Transformer & Head=1, Depth=1, Embedding Dim=64& ——\\
    Fixed Individual Q Network& [96,64,6] & ReLU\\
    Variable Individual Q Network &[96,64,1]& ReLU\\
    HyperNet in Mixing Network& [128,64,32]& ReLU\\
    Attention in Mixing Network& Attention Dim=8 & ——\\
    \bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\subsection{Hyperparameters}\label{app:hyperparam}
For SD-CQL, the main hyperparameters shared across all tasks are listed in Table \ref{tab: hyper}. We primarily adjust the BC regularization coefficient $\eta$, with the specific settings provided in Table \ref{tab: bchyper}. All other hyperparameters are kept consistent with the default configuration of QMIX in PYMARL2.

For the \textit{Marine-Single} and \textit{Marine-Single-Inv} task sets designed by us, we adjust the conservative coefficient from ${1.0, 2.5, 5.0}$ and the distribution coefficient from $\{1.0, 2.5, 5.0\}$ for ODIS, selecting the best configuration for reporting. The specific settings are detailed in Table \ref{tab: odishyper}.
%For SD-CQL, the main hyperparameters shared by all tasks are listed in Table \ref{tab: hyper}. We mainly adjust the BC regularization coefficient $\eta$, and the specific settings are listed in Table \ref{tab: bchyper}. All other hyperparameters remain consistent with the default configuration of QMIX in PYMARL2 .

\begin{table}[!ht]
\caption{The hyperparameters shared by all tasks for SD-CQL.}
\label{tab: hyper}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcc}
    \toprule
    \textbf{Hyperparameter} & \textbf{Value}\\
    \midrule
    Entity Embedding Dim & 64 \\
    Skill Dimension of $z$ & 32 \\
    $\lambda$ for $TD(\lambda)$ & 0.6\\
    Conservative Weight $\alpha$ & 1.0\\
    $T_{\text{max}}$ & 35,000\\
    $T_{\text{update}}$ & 80\\
    Batch Size & 32 \\
    Learning Rate & 0.005\\
    Optimizer & Adam \cite{kingma2014adam} \\
    \bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{table}[!ht]
\caption{BC regularization coefficient $\eta$ for each task set.}
\label{tab: bchyper}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcc}
    \toprule
    \textbf{Task set} & \textbf{Value}\\
    \midrule
    Marine-Easy-Expert & 0.5 \\
    Marine-Easy-Medium & 0.3 \\
    Marine-Easy-Medium-Replay & 0.5 \\
    Marine-Easy-Medium-Expert & 0.5 \\
    \midrule
    Marine-Hard-Expert & 0.5 \\
    Marine-Hard-Medium & 0.9 \\
    Marine-Hard-Medium-Replay & 0.9 \\
    Marine-Hard-Medium-Expert & 0.7 \\
    \midrule
    Stalker-Zealot-Expert & 0.5 \\
    Stalker-Zealot-Medium & 0.3 \\
    Stalker-Zealot-Medium-Replay & 0.5 \\
    Stalker-Zealot-Medium-Expert & 0.5 \\
    \midrule
    Marine-Single-Expert & 0.5 \\
    Marine-Single-Inv-Expert & 0.5 \\
    \bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
%For Marine-Single and Marine-Single-Inv task sets designed by ourselves, we adjust the conservative coefficient in $\{1.0,2.5,5.0\}$ and distribution coefficient in $\{1.0,2.5,5.0\}$ for ODIS, and select the best one to report. The specific settings are listed in Table \ref{tab: odishyper}.
\begin{table}[!ht]
\caption{Selected Hyperparameters for ODIS.}
\label{tab: odishyper}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcc}
    \toprule
    \textbf{Task set} & \textbf{Conservative Coefficient} & \textbf{Distribution Coefficient}\\
    Marine-Single-Expert & 1.0 & 5.0 \\
    Marine-Single-Inv-Expert & 1.0 &1.0 \\
    \bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
\newpage
\subsection{Skill-Discovery Visualization}\label{app:visual}
We visualize the skill-discovery as follows: 

First, we deploy the SD-CQL agent, which has been trained on the \textit{Marine-Single} task set, in the \textit{3m} and \textit{12m} task. We run one episode in each task and record the actions and skills of each agent at each time step, while saving replay videos. Then, considering the significant difference in trajectory lengths between \textit{3m} and \textit{12m} (approximately $20$ steps for \textit{3m} versus over $150$ steps for \textit{12m}, including many irrelevant steps), we truncate the \textit{12m} trajectory to match the length of the \textit{3m} trajectory to avoid sample imbalance interfering with dimensionality reduction results. We then collect the skill $z$ of every agent at every time step from both trajectories as samples. Finally we apply t-SNE \cite{tsne} to project them into a 2D plane and normalize the two dimensions to align their scales.

In Figure \ref{fig:z_visual}, the markers represent the skills of agents at corresponding time steps and tasks (Figure \ref{fig:3m_visual} and Figure \ref{fig:12m_visual}) after dimensionality reduction. Red markers represent skills from the \textit{12m} task, green markers represent skills from the 3m task, triangles indicate the \textit{\textbf{retreat}}, circles indicate the \textit{\textbf{attack}} skill, and crosses indicate dead agents.
\section{Ablation Study}\label{app:ablation}
To investigate the impact of each component of SD-CQL on its superior performance, as well as the sensitivity of SD-CQL to different hyperparameter values, we conducted an ablation study on the One-to-Multi task sets.%\textit{Marine-Single} and \textit{Marine-Single-Inv} task set. 

In Table \ref{tab: abc-win-rates}, we report the performance of three variants of SD-CQL: (i) without the BC regularization term (w/o BC), (ii) without the skill-discovery mechanism (w/o SD), and (iii) without both (w/o BC \& SD). It can be seen that these two components are indispensable for the superior performance of SD-CQL. In particular, in \textit{Marine-Single-Inv} task set, when the number of agents involved in the source task is large, the BC regularization effectively alleviates the issue of error accumulation. Meanwhile, our skill-discovery also improves the algorithm's generalization ability on other tasks. 

In Table \ref{tab: abh-win-rates}, we separately adjusted one of the weights of the CQL regularization and the BC regularization while keeping the other consistent with that in the evaluation. It can be observed that the performance of SD-CQL remains relatively stable and insensitive to the values of these two hyperparameters. 
\begin{table}[H]
\caption{Win rates of different SD-CQL variants on One-to-Multi task sets.}
\label{tab: abc-win-rates}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{clccccc}
\toprule
\multicolumn{2}{c}{\textbf{Task Set}} & \textbf{SD-CQL} & \textbf{w/o BC} & \textbf{w/o SD} & \textbf{w/o BC \& SD} \\
\midrule
\multirow{5}{*}{\rotatebox{90}{\makecell{\textit{Marine}\\\textit{Single}}}} 
    &3m $^\diamond$ & \textbf{100.00} $\pm$ 0.00& 97.50 $\pm$ 2.61 & 99.38 $\pm$ 1.40 & \textbf{100.00} $\pm$ 0.00 \\
    &5m & \textbf{91.25} $\pm$ 12.58 & 83.12 $\pm$ 6.85 & 74.38 $\pm$ 11.98 & 88.75 $\pm$ 9.27 \\
    &8m & \textbf{45.00} $\pm$ 18.70 & 33.75 $\pm$ 21.13 & 24.38 $\pm$ 30.09 & 36.25 $\pm$ 23.55 \\
    &10m & \textbf{36.88} $\pm$ 31.51 & 33.75 $\pm$ 24.55 & 13.75 $\pm$ 19.96 & 32.5 $\pm$ 35.67 \\
    &12m  & \textbf{30.62} $\pm$ 26.28 & 28.12 $\pm$ 20.61 & 7.50 $\pm$ 13.55 & 27.50 $\pm$ 24.25 \\
    \midrule
    \multicolumn{2}{c}{\textbf{Average}}  & \textbf{60.75} $\pm$ 13.46 & 55.25 $\pm$ 11.22 & 43.88 $\pm$ 11.23 & 57.00 $\pm$ 15.62 \\
\midrule\midrule
\multirow{5}{*}{\rotatebox{90}{\makecell{\textit{Marine} \\ \textit{Single-Inv}}}}
    &10m $^\diamond$ &99.38 $\pm$ 1.40 & 0.00 $\pm$ 0.00 & \textbf{100.00} $\pm$ 0.00& 1.25 $\pm$ 2.80\\
    &8m  &\textbf{100.00} $\pm$ 0.00 & 0.00 $\pm$ 0.00 & 96.25 $\pm$ 4.07 &  6.88 $\pm$ 12.18\\
    &5m  &\textbf{92.50} $\pm$ 3.56 & 0.00 $\pm$ 0.00 & 27.50 $\pm$ 35.04 & 20.62 $\pm$ 27.65\\
    &4m  &\textbf{64.38} $\pm$ 23.13 & 0.00 $\pm$ 0.00 & 8.75 $\pm$ 16.30 & 8.75 $\pm$ 13.15\\
    &3m  &\textbf{40.62} $\pm$ 18.88& 0.00 $\pm$ 0.00 & 1.88 $\pm$ 2.80 & 23.12 $\pm$ 26.75 \\
    \midrule
    \multicolumn{2}{c}{\textbf{Average}}  &\textbf{79.38} $\pm$ 5.55 & 0.00 $\pm$ 0.00 & 46.88 $\pm$ 9.23& 12.12 $\pm$ 11.77\\
\bottomrule
\multicolumn{5}{l}{\small $\diamond$ denotes the source task.}\\
\end{tabular}
% \begin{tabular}{lcccc}
%     \toprule
%     \textbf{Task} & \textbf{SD-CQL} & \textbf{w/o BC} & \textbf{w/o SD} & \textbf{w/o BC \& SD} \\
%     \midrule
%     3m $^\diamond$ & \textbf{100.00} $\pm$ 0.00& 97.50 $\pm$ 2.61 & 99.38 $\pm$ 1.40 & \textbf{100.00} $\pm$ 0.00 \\
%     5m & \textbf{91.25} $\pm$ 12.58 & 83.12 $\pm$ 6.85 & 74.38 $\pm$ 11.98 & 88.75 $\pm$ 9.27 \\
%     8m & \textbf{45.00} $\pm$ 18.70 & 33.75 $\pm$ 21.13 & 24.38 $\pm$ 30.09 & 36.25 $\pm$ 23.55 \\
%     10m & \textbf{36.88} $\pm$ 31.51 & 33.75 $\pm$ 24.55 & 13.75 $\pm$ 19.96 & 32.5 $\pm$ 35.67 \\
%     12m  & \textbf{30.62} $\pm$ 26.28 & 28.12 $\pm$ 20.61 & 7.50 $\pm$ 13.55 & 27.50 $\pm$ 24.25 \\
%     \midrule
%     \textbf{Avg}  & \textbf{60.75} $\pm$ 13.46 & 55.25 $\pm$ 11.22 & 43.88 $\pm$ 11.23 & 57.00 $\pm$ 15.62 \\
%     \bottomrule
%     \multicolumn{5}{l}{\small $\diamond$ denotes the source task.}\\
% \end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{table}[H]
\caption{Win rates of SD-CQL with different hyperparameters on One-to-Multi task sets.}
\label{tab: abh-win-rates}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{llcccc}
    \toprule
    \multicolumn{2}{c}{\textbf{Hyperparameters}} & $\eta=0.7$ & $\eta=0.5$ & $\eta=0.3$\\
    \midrule
    \multirow{2}{*}{\textbf{Average}} & \textit{Marine-Single}& 59.00 $\pm$ 11.99 & 60.75 $\pm$ 13.46 & 62.75 $\pm$ 14.80 \\
     & \textit{Marine-Single-Inv}& 78.50 $\pm$ 5.78 & 79.38 $\pm$ 5.55 & 79.88 $\pm$ 9.71\\
    \midrule
    \midrule
    \multicolumn{2}{c}{\textbf{Hyperparameters}} & $\alpha=0.5$ & $\alpha=1.0$ & $\alpha=2.5$\\
    \midrule
    \multirow{2}{*}{\textbf{Average}} & \textit{Marine-Single}&  58.75 $\pm$ 16.33 & 60.75 $\pm$ 13.46 & 65.62 $\pm$ 13.04 \\
    & \textit{Marine-Single-Inv}& 81.62 $\pm$ 8.60 & 79.38 $\pm$ 5.55 & 75.37 $\pm$ 11.10\\
    \bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\newpage
\section{Detailed Results of Multi-to-Multi Task Sets}\label{app:results}
In this section, we present the detailed results of our evaluation experiments on the Multi-to-Multi task sets. The results for \textit{Marine-Easy} are presented in Table \ref{tab:e-win-rates}, the results for \textit{Marine-Hard} are shown in Table \ref{tab:h-win-rates}, and the results for \textit{Stalker-Zealot} are provided in Table \ref{tab:sz-win-rates}. In each table, we present the multi-task evaluation results on datasets of varying quality, with the source tasks used for training marked by ``\(\diamond\)". The results show that, despite being trained on a limited number of source tasks, SD-CQL exhibits strong multi-task generalization, as reflected in its exceptional average performance across all test tasks.

\begin{table*}[htbp!]
\caption{Win rates of \textit{Marine-Easy} task set. The reported results are averaged over 5 random seeds. The bold number denotes the best performance and $\pm$ denotes one standard deviation.}
\label{tab:e-win-rates}
\begin{center}
\begin{small}
\begin{sc}
\resizebox{0.85\textwidth}{!}{
\begin{tabular}{c|lccccc}
    \toprule
    \multicolumn{2}{c}{\textbf{Task}} & \textbf{BC-t} & \textbf{BC-r} &\textbf{UpDeT} & \textbf{ODIS} & \textbf{SD-CQL (ours)} \\
    \midrule
    % \multicolumn{6}{c}{\textbf{Expert}} \\
    % \midrule
    \multirow{10}{*}{\rotatebox{90}{\makecell{Expert}}}
    &3m $^\diamond$& 99.38 $\pm$ 1.40 & \textbf{100.00} $\pm$ 0.00 & 48.75 $\pm$ 25.25 & 97.50 $\pm$ 1.40 & 98.75 $\pm$ 1.71 \\  
    &4m & 90.62 $\pm$ 7.33 & \textbf{96.88} $\pm$ 3.83 & 11.25 $\pm$ 12.81 & 56.25 $\pm$ 37.69 & 90.00 $\pm$ 10.69  \\
    &5m $^\diamond$& \textbf{100.00} $\pm$ 0.00 & \textbf{100.00} $\pm$ 0.00 & 29.38 $\pm$ 18.83 & 78.12 $\pm$ 31.48 & \textbf{100.00} $\pm$ 0.00  \\
    &6m & \textbf{100.00} $\pm$ 0.00 & \textbf{100.00} $\pm$ 0.00 & 28.75 $\pm$ 28.76 & 55.62 $\pm$ 43.49 & 96.88 $\pm$ 2.21   \\
    &7m & \textbf{100.00} $\pm$ 0.00 & 98.75 $\pm$ 1.71 & 21.25 $\pm$ 14.56 & 58.12 $\pm$ 44.28 & 98.12 $\pm$ 4.19  \\
    &8m & 99.38 $\pm$ 1.40 & \textbf{100.00} $\pm$ 0.00 & 27.50 $\pm$ 25.90 & 63.75 $\pm$ 41.14 & 99.38 $\pm$ 1.40  \\
    &9m & \textbf{100.00} $\pm$ 0.00 & \textbf{100.00} $\pm$ 0.00 & 33.75 $\pm$ 35.11 & 70.62 $\pm$ 42.65 & \textbf{100.00} $\pm$ 0.00  \\
    &10m $^\diamond$& \textbf{100.00} $\pm$ 0.00 & 98.75 $\pm$ 1.71 & 32.50 $\pm$ 33.56 & 78.75 $\pm$ 35.24 & \textbf{100.00} $\pm$ 0.00 \\
    &11m & 99.38 $\pm$ 1.40 & \textbf{100.00} $\pm$ 0.00 & 30.00 $\pm$ 27.21 & 80.00 $\pm$ 28.78 & 97.50 $\pm$ 2.61   \\
    &12m & \textbf{100.00} $\pm$ 0.00 & 99.38 $\pm$ 1.40 & 19.38 $\pm$ 27.72 & 66.25 $\pm$ 42.01 & 92.50 $\pm$ 9.53   \\
    \midrule
\multicolumn{2}{c}{\textbf{Average}} & 98.87 $\pm$ 0.58 & \textbf{99.38} $\pm$ 0.40 & 28.25 $\pm$ 18.52 & 70.50 $\pm$ 30.14 & 97.31 $\pm$ 1.33 \\
    \midrule
    %\multicolumn{6}{c}{\textbf{Medium}} \\
    %\midrule
    \multirow{10}{*}{\rotatebox{90}{\makecell{Medium}}}
    &3m$^\diamond$& 61.25 $\pm$ 10.03 & 65.62 $\pm$ 8.84 & 58.12 $\pm$ 13.37 & 64.38 $\pm$ 10.96 & \textbf{79.38} $\pm$ 13.00  \\
    &4m & \textbf{66.25} $\pm$ 8.09 & 64.38 $\pm$ 34.42 & 52.50 $\pm$ 27.81 & 55.00 $\pm$ 30.98 & 50.62 $\pm$ 34.05   \\
    &5m $^\diamond$ & 78.75 $\pm$ 8.09 & 80.00 $\pm$ 4.19 & 75.62 $\pm$ 9.73 & 76.25 $\pm$ 7.19 & \textbf{85.00} $\pm$ 6.40  \\
    &6m & 90.00 $\pm$ 6.01 & 84.38 $\pm$ 5.85 & 71.25 $\pm$ 18.67 & 93.12 $\pm$ 8.39 & \textbf{96.88} $\pm$ 6.99   \\
    &7m    & \textbf{99.38} $\pm$ 1.40 & 90.62 $\pm$ 11.69 & 62.50 $\pm$ 28.21 & 90.62 $\pm$ 12.50 & \textbf{99.38} $\pm$ 1.40   \\
    &8m    & 86.88 $\pm$ 10.69 & \textbf{95.62} $\pm$ 6.48 & 70.62 $\pm$ 25.64 & 89.38 $\pm$ 5.23 & 93.75 $\pm$ 6.25  \\
    &9m     & \textbf{83.12} $\pm$ 8.73 & 78.75 $\pm$ 5.13 & 46.25 $\pm$ 9.98 & 72.50 $\pm$ 18.67 & 80.62 $\pm$ 4.07  \\
    &10m $^\diamond$& \textbf{70.62} $\pm$ 17.34 & 69.38 $\pm$ 6.77 & 49.38 $\pm$ 18.54 & 70.00 $\pm$ 15.08 & 70.00 $\pm$ 9.27   \\
    &11m    & 43.75 $\pm$ 13.07 & 43.75 $\pm$ 5.85 & 32.50 $\pm$ 12.42 & 43.12 $\pm$ 9.73 & \textbf{68.12} $\pm$ 12.18   \\
    &12m   & 38.12 $\pm$ 7.78 & \textbf{44.38} $\pm$ 5.59 & 21.25 $\pm$ 12.96 & 30.62 $\pm$ 12.38 & 35.00 $\pm$ 15.69  \\
    \midrule
\multicolumn{2}{c}{\textbf{Average}} & 71.81 $\pm$ 3.74 & 71.69 $\pm$ 4.33 & 54.00 $\pm$ 9.19 & 68.50 $\pm$ 5.74 & \textbf{75.87} $\pm$ 3.15   \\
 
    \midrule
    %\multicolumn{6}{c}{\textbf{Medium-Replay}} \\
    %\midrule
    \multirow{10}{*}{\rotatebox{90}{\makecell{Medium-Replay}}}
    &3m $^\diamond$& \textbf{80.00} $\pm$ 6.48 & 70.62 $\pm$ 22.81 & 48.12 $\pm$ 11.61 & 60.62 $\pm$ 35.05 & 70.62 $\pm$ 21.72 \\
    &4m & \textbf{75.00} $\pm$ 15.15 & 70.00 $\pm$ 9.53 & 15.62 $\pm$ 21.65 & 18.75 $\pm$ 25.77 & 68.12 $\pm$ 10.69 \\
    &5m $^\diamond$& 77.50 $\pm$ 31.51 & \textbf{88.75} $\pm$ 15.56 & 61.25 $\pm$ 15.87 & 73.75 $\pm$ 26.01 & 78.12 $\pm$ 33.29\\
    &6m & 88.75 $\pm$ 23.45 & 98.12 $\pm$ 2.80 & 2.50 $\pm$ 5.59 & 12.50 $\pm$ 27.95 & \textbf{99.38} $\pm$ 1.40 \\
    &7m & 86.88 $\pm$ 24.15 & 90.62 $\pm$ 12.88 & 0.00 $\pm$ 0.00 & 8.12 $\pm$ 18.17 & \textbf{99.38} $\pm$ 1.40 \\
    &8m & 19.38 $\pm$ 29.43 & 43.75 $\pm$ 35.84 & 0.00 $\pm$ 0.00 & 0.00 $\pm$ 0.00 & \textbf{93.12} $\pm$ 2.61 \\
    &9m & 15.62 $\pm$ 18.88 & 25.00 $\pm$ 24.51 & 0.00 $\pm$ 0.00 & 5.00 $\pm$ 11.18 & \textbf{77.50} $\pm$ 32.20 \\
    &10m $^\diamond$& 80.62 $\pm$ 12.18 & 76.88 $\pm$ 17.62 & 66.88 $\pm$ 35.12 & 76.88 $\pm$ 11.82 & \textbf{85.62} $\pm$ 7.19  \\
    &11m & 1.25 $\pm$ 2.80 & 5.00 $\pm$ 8.15 & 0.00 $\pm$ 0.00 & 0.00 $\pm$ 0.00 & \textbf{34.38} $\pm$ 22.21 \\
    &12m & 0.00 $\pm$ 0.00 & 3.75 $\pm$ 6.77 & 0.00 $\pm$ 0.00 & 0.00 $\pm$ 0.00 & \textbf{31.25} $\pm$ 25.00 \\
    \midrule
    \multicolumn{2}{c}{\textbf{Average}} & 44.12 $\pm$ 8.19 & 49.62 $\pm$ 9.17 & 6.69 $\pm$ 3.18 & 11.06 $\pm$ 8.94 & \textbf{73.13} $\pm$ 11.31 \\
    \midrule
    %\multicolumn{6}{c}{\textbf{Medium-Expert}} \\
    %\midrule
    \multirow{10}{*}{\rotatebox{90}{\makecell{Medium-Replay}}}
    &3m $^\diamond$ & 90.00 $\pm$ 18.93 & 83.75 $\pm$ 23.94 & 35.00 $\pm$ 25.81 & 63.12 $\pm$ 27.90 & \textbf{92.50} $\pm$ 9.00  \\
    &4m & \textbf{87.50} $\pm$ 9.11 & 78.12 $\pm$ 14.82 & 37.50 $\pm$ 27.69 & 49.38 $\pm$ 37.72 & 63.12 $\pm$ 14.89 \\
    &5m $^\diamond$& 69.38 $\pm$ 9.22 & 77.50 $\pm$ 10.69 & 0.62 $\pm$ 1.40 & 5.00 $\pm$ 11.18 & \textbf{89.38} $\pm$ 6.48  \\
    &6m & 63.75 $\pm$ 25.92 & 68.75 $\pm$ 20.73 & 45.00 $\pm$ 14.76 & 50.00 $\pm$ 9.88 & \textbf{83.12} $\pm$ 22.71  \\
    &7m & 85.62 $\pm$ 6.85 & 86.25 $\pm$ 16.33 & 58.12 $\pm$ 31.30 & 65.62 $\pm$ 22.53 & \textbf{87.50} $\pm$ 24.51  \\
    &8m & 56.88 $\pm$ 11.14 & 66.25 $\pm$ 24.65 & 52.50 $\pm$ 33.54 & 66.25 $\pm$ 23.84 & \textbf{85.62} $\pm$ 16.48  \\
    &9m & 70.62 $\pm$ 12.62 & 75.00 $\pm$ 22.43 & 46.88 $\pm$ 24.41 & 73.12 $\pm$ 18.43 & \textbf{93.12} $\pm$ 7.78   \\
    &10m $^\diamond$& 5.00 $\pm$ 6.85 & 11.88 $\pm$ 12.96 & 0.00 $\pm$ 0.00 & 0.62 $\pm$ 1.40 & \textbf{68.12} $\pm$ 34.62 \\
    &11m & 68.12 $\pm$ 14.22 & 71.88 $\pm$ 8.56 & 58.12 $\pm$ 15.72 & \textbf{76.88} $\pm$ 18.83 & 76.25 $\pm$ 22.38 \\
    &12m & 60.62 $\pm$ 12.81 & 56.88 $\pm$ 4.64 & 26.88 $\pm$ 20.56 & \textbf{65.00} $\pm$ 16.00 & 58.75 $\pm$ 19.19 \\
    \midrule
\multicolumn{2}{c}{\textbf{Average}}  & 74.12 $\pm$ 3.21 & 75.25 $\pm$ 7.41 & 48.81 $\pm$ 11.48 & 66.00 $\pm$ 9.21 & \textbf{80.38} $\pm$ 7.96 \\
    \bottomrule
    \multicolumn{7}{l}{\small $\diamond$ denotes the source tasks.}
    \end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\begin{table}[htbp!]
\caption{Win rates of \textit{Marine-Hard} task set. The reported results are averaged over 5 random seeds. The bold number denotes the best performance and $\pm$ denotes one standard deviation.}
\label{tab:h-win-rates}
\begin{center}
\begin{small}
\begin{sc}
\resizebox{0.85\textwidth}{!}{
\begin{tabular}{c|lccccc}
    \toprule
    \multicolumn{2}{c}{\textbf{Task}} & \textbf{BC-t} & \textbf{BC-r} &\textbf{UpDeT} & \textbf{ODIS} & \textbf{SD-CQL (ours)} \\
    \midrule
    %\multicolumn{6}{c}{\textbf{Expert}} \\
    %\midrule
    
    \multirow{12}{*}{\rotatebox{90}{\makecell{Expert}}}
    &3m $^\diamond$& \textbf{100.00} $\pm$ 0.00 & \textbf{100.00} $\pm$ 0.00 & 59.38 $\pm$ 36.11 & 74.38 $\pm$ 34.47 & 99.38 $\pm$ 1.40 \\
    &4m & 92.50 $\pm$ 8.44 & 95.00 $\pm$ 7.19 & 61.88 $\pm$ 20.66 & 37.50 $\pm$ 32.40 & \textbf{97.50} $\pm$ 2.61 \\
    &5m & 97.50 $\pm$ 2.61 & 83.75 $\pm$ 12.38 & 76.88 $\pm$ 27.65 & 30.62 $\pm$ 32.73 & \textbf{100.00} $\pm$ 0.00 \\
    &10m & \textbf{95.62} $\pm$ 3.56 & 91.25 $\pm$ 13.15 & 46.88 $\pm$ 34.09 & 18.12 $\pm$ 12.96 & 86.88 $\pm$ 18.80 \\
    &12m & 68.75 $\pm$ 31.25 & \textbf{88.75} $\pm$ 10.73 & 30.00 $\pm$ 38.44 & 5.00 $\pm$ 11.18 & 68.75 $\pm$ 25.00 \\
    &5m\_vs\_6m $^\diamond$& 62.50 $\pm$ 8.84 & \textbf{67.50} $\pm$ 8.44 & 5.00 $\pm$ 5.68 & 25.00 $\pm$ 29.06 & 50.62 $\pm$ 8.09 \\
    &7m\_vs\_8m & \textbf{30.62} $\pm$ 17.73 & 23.12 $\pm$ 19.84 & 3.12 $\pm$ 3.83 & 8.75 $\pm$ 11.14 & 22.50 $\pm$ 10.69 \\
    &8m\_vs\_9m & 41.88 $\pm$ 6.85 & 43.75 $\pm$ 19.01 & 1.25 $\pm$ 1.71 & 15.00 $\pm$ 15.84 & \textbf{71.88} $\pm$ 11.27 \\
    &9m\_vs\_10m $^\diamond$& \textbf{98.75} $\pm$ 2.80 & 90.00 $\pm$ 12.38 & 21.25 $\pm$ 30.41 & 23.75 $\pm$ 36.55 & 78.75 $\pm$ 27.19 \\
    &10m\_vs\_11m & 63.75 $\pm$ 7.84 & \textbf{86.25} $\pm$ 8.44 & 17.50 $\pm$ 34.13 & 12.50 $\pm$ 14.66 & 60.00 $\pm$ 28.59 \\
    &10m\_vs\_12m & 3.75 $\pm$ 2.61 & 8.12 $\pm$ 5.68 & 0.62 $\pm$ 1.40 & 1.88 $\pm$ 4.19 & \textbf{20.00} $\pm$ 17.90 \\
    &13m\_vs\_15m & 0.00 $\pm$ 0.00 & 0.62 $\pm$ 1.40 & 0.00 $\pm$ 0.00 & 0.62 $\pm$ 1.40 & \textbf{7.50} $\pm$ 7.84 \\
    \midrule
\multicolumn{2}{c}{\textbf{Average}} & 62.97 $\pm$ 2.18 & \textbf{64.84} $\pm$ 3.24 & 26.98 $\pm$ 13.21 & 21.09 $\pm$ 14.72 & 63.65 $\pm$ 7.20 \\
    \midrule
    %\multicolumn{6}{c}{\textbf{Medium}} \\
    %\midrule
    
    \multirow{12}{*}{\rotatebox{90}{\makecell{Medium}}} 
    &3m $^\diamond$& 62.50 $\pm$ 9.38 & 52.50 $\pm$ 15.21 & 64.38 $\pm$ 16.03 & 59.38 $\pm$ 7.97 & \textbf{74.38} $\pm$ 3.42 \\
     &4m & 41.25 $\pm$ 23.32 & 41.88 $\pm$ 25.92 & 35.00 $\pm$ 28.16 & 58.12 $\pm$ 24.37 & \textbf{71.88} $\pm$ 29.06  \\
    &5m & 86.88 $\pm$ 6.01 & 85.62 $\pm$ 11.40 & 87.50 $\pm$ 16.39 & 68.12 $\pm$ 30.49 & \textbf{96.25} $\pm$ 5.13   \\
    &10m & \textbf{90.00} $\pm$ 12.77 & \textbf{90.00} $\pm$ 12.38 & 62.50 $\pm$ 25.67 & 70.00 $\pm$ 32.67 & \textbf{90.00} $\pm$ 10.92 \\
    &12m & 75.00 $\pm$ 21.31 & 64.38 $\pm$ 30.51 & 43.75 $\pm$ 35.15 & 18.12 $\pm$ 23.74 & \textbf{77.50} $\pm$ 17.17\\
    &5m\_vs\_6m $^\diamond$& 30.62 $\pm$ 2.61 & \textbf{38.75} $\pm$ 6.48 & 4.38 $\pm$ 8.15 & 10.00 $\pm$ 11.98 & 21.88 $\pm$ 13.07 \\
    &7m\_vs\_8m & 3.12 $\pm$ 3.12 & 10.00 $\pm$ 8.67 & 0.00 $\pm$ 0.00 & 4.38 $\pm$ 3.56 & \textbf{10.62} $\pm$ 7.53 \\ 
   & 8m\_vs\_9m & 6.88 $\pm$ 7.78 & 6.25 $\pm$ 7.65 & 3.12 $\pm$ 2.21 & \textbf{11.25} $\pm$ 13.18 & 8.12 $\pm$ 6.48 \\   
   & 9m\_vs\_10m $^\diamond$& 64.38 $\pm$ 14.76 & 64.38 $\pm$ 12.62 & 28.75 $\pm$ 23.01 & \textbf{68.75} $\pm$ 15.93 & 67.50 $\pm$ 17.06 \\
    &10m\_vs\_11m & 41.88 $\pm$ 11.18 & \textbf{43.12} $\pm$ 16.30 & 5.62 $\pm$ 6.40 & 21.25 $\pm$ 9.73 & 34.38 $\pm$ 9.11 \\ 
   & 10m\_vs\_12m & 0.62 $\pm$ 1.40 & 0.62 $\pm$ 1.40 & 0.00 $\pm$ 0.00 & 0.00 $\pm$ 0.00 & \textbf{1.25} $\pm$ 1.71 \\ 
   & 13m\_vs\_15m & 1.88 $\pm$ 1.71 & 0.62 $\pm$ 1.40 & 0.00 $\pm$ 0.00 & 1.25 $\pm$ 2.80 & \textbf{2.50} $\pm$ 2.61 \\ 
    \midrule
 \multicolumn{2}{c}{\textbf{Average}}  & 42.08 $\pm$ 2.63 & 41.51 $\pm$ 3.53 & 27.92 $\pm$ 5.32 & 32.55 $\pm$ 4.31 & \textbf{46.35} $\pm$ 3.44 \\
    \midrule
    %\multicolumn{6}{c}{\textbf{Medium-Replay}} \\
    %\midrule
    \multirow{12}{*}{\rotatebox{90}{\makecell{Medium-Replay}}} 
    &3m $^\diamond$& 75.62 $\pm$ 7.13 & \textbf{80.00} $\pm$ 17.06 & 54.38 $\pm$ 21.61 & 75.00 $\pm$ 13.62 & 75.62 $\pm$ 9.48 \\
    &4m & 83.12 $\pm$ 10.03 & 83.12 $\pm$ 8.44 & 63.12 $\pm$ 14.05 & 55.62 $\pm$ 34.40 & \textbf{86.25 }$\pm$ 14.25 \\
    &5m & 95.62 $\pm$ 5.23 & 95.62 $\pm$ 9.78 & 76.25 $\pm$ 18.03 & 93.12 $\pm$ 8.39 & \textbf{100.00} $\pm$ 0.00 \\
    &10m & 87.50 $\pm$ 11.27 & \textbf{90.62} $\pm$ 11.05 & 25.62 $\pm$ 26.55 & 89.38 $\pm$ 8.44 & 86.88 $\pm$ 5.59\\
    &12m & 85.62 $\pm$ 10.96 & \textbf{88.12} $\pm$ 4.07 & 16.25 $\pm$ 20.18 & 70.62 $\pm$ 16.33 & 85.62 $\pm$ 9.27\\
    &5m\_vs\_6m $^\diamond$& 30.00 $\pm$ 13.55 & \textbf{33.12} $\pm$ 6.48 & 0.00 $\pm$ 0.00 & 9.38 $\pm$ 6.25 & 16.88 $\pm$ 6.48  \\
    &7m\_vs\_8m & \textbf{14.38} $\pm$ 2.80 & 3.75 $\pm$ 2.61 & 0.62 $\pm$ 1.40 & 5.00 $\pm$ 7.84 & 9.38 $\pm$ 3.12 \\ 
    &8m\_vs\_9m & 8.12 $\pm$ 3.56 & 11.88 $\pm$ 5.59 & 0.62 $\pm$ 1.40 & 1.88 $\pm$ 1.71 & \textbf{19.38} $\pm$ 10.22  \\   
    &9m\_vs\_10m $^\diamond$ & 36.25 $\pm$ 14.08 & \textbf{42.50} $\pm$ 14.76 & 0.62 $\pm$ 1.40 & 14.38 $\pm$ 13.18 & 31.25 $\pm$ 16.68 \\
    &10m\_vs\_11m  & \textbf{36.25} $\pm$ 8.15 & 29.38 $\pm$ 18.57 & 0.00 $\pm$ 0.00 & 32.50 $\pm$ 22.27 & 31.25 $\pm$ 9.11\\ 
    &10m\_vs\_12m & \textbf{2.50} $\pm$ 2.61 & 0.62 $\pm$ 1.40 & 0.00 $\pm$ 0.00 & 0.00 $\pm$ 0.00 & 1.88 $\pm$ 2.80 \\ 
    &13m\_vs\_15m & 5.62 $\pm$ 2.61 & 5.00 $\pm$ 6.48 & 0.00 $\pm$ 0.00 & 0.62 $\pm$ 1.40 & \textbf{10.00} $\pm$ 11.98 \\ 
    \midrule
   \multicolumn{2}{c}{\textbf{Average}} & 46.72 $\pm$ 2.35 & \textbf{46.98} $\pm$ 1.95 & 19.79 $\pm$ 6.16 & 37.29 $\pm$ 5.48 & 46.20 $\pm$ 2.51\\
    \midrule
    %\multicolumn{6}{c}{\textbf{Medium-Expert}} \\
    %\midrule
    \multirow{12}{*}{\rotatebox{90}{\makecell{Medium-Expert}}} 
    &3m $^\diamond$& 74.38 $\pm$ 14.72 & 85.00 $\pm$ 19.44 & 60.62 $\pm$ 22.16 & 53.75 $\pm$ 36.94 & \textbf{98.12} $\pm$ 2.80  \\
    &4m & 95.00 $\pm$ 3.56 & 85.00 $\pm$ 23.11 & 40.62 $\pm$ 22.64 & 34.38 $\pm$ 45.93 & \textbf{95.62} $\pm$ 6.09 \\
    &5m & 91.88 $\pm$ 9.78 & 58.12 $\pm$ 28.00 & 75.00 $\pm$ 26.79 & 31.25 $\pm$ 38.84 & \textbf{92.50} $\pm$ 13.37\\
    &10m & \textbf{93.75} $\pm$ 10.83 & 80.00 $\pm$ 19.71 & 20.00 $\pm$ 28.09 & 26.25 $\pm$ 34.13 & 91.25 $\pm$ 6.01 \\
    &12m & 59.38 $\pm$ 43.24 & 73.12 $\pm$ 26.85 & 8.75 $\pm$ 16.30 & 3.75 $\pm$ 5.59 & \textbf{90.62} $\pm$ 3.83 \\
    &5m\_vs\_6m $^\diamond$ & \textbf{43.12} $\pm$ 22.14 & 37.50 $\pm$ 18.62 & 1.88 $\pm$ 2.80 & 8.75 $\pm$ 8.09 & 11.88 $\pm$ 7.13\\
    &7m\_vs\_8m & \textbf{18.12} $\pm$ 4.64 & 16.25 $\pm$ 14.72 & 0.62 $\pm$ 1.40 & 4.38 $\pm$ 6.85 & 10.62 $\pm$ 14.08  \\ 
    &8m\_vs\_9m & 31.25 $\pm$ 19.39 & 26.25 $\pm$ 18.96 & 0.62 $\pm$ 1.40 & 15.00 $\pm$ 21.58 & \textbf{34.38} $\pm$ 17.95  \\   
    &9m\_vs\_10m $^\diamond$& 62.50 $\pm$ 24.90 & \textbf{84.38} $\pm$ 9.38 & 3.12 $\pm$ 5.41 & 23.75 $\pm$ 35.81 & 38.75 $\pm$ 23.86  \\
    &10m\_vs\_11m& \textbf{51.88} $\pm$ 17.34 & 41.88 $\pm$ 19.21 & 2.50 $\pm$ 5.59 & 33.75 $\pm$ 38.87 & 51.25 $\pm$ 19.84 \\ 
    &10m\_vs\_12m & 1.88 $\pm$ 4.19 & 2.50 $\pm$ 4.07 & 0.00 $\pm$ 0.00 & 0.00 $\pm$ 0.00 & \textbf{6.25} $\pm$ 6.25\\ 
    &13m\_vs\_15m & 0.00 $\pm$ 0.00 & 1.25 $\pm$ 1.71 & 0.00 $\pm$ 0.00 & 0.00 $\pm$ 0.00 & \textbf{19.38} $\pm$ 12.38\\ 
    \midrule
\multicolumn{2}{c}{\textbf{Average}}& 51.93 $\pm$ 7.21 & 49.27 $\pm$ 5.16 & 17.81 $\pm$ 6.74 & 19.58 $\pm$ 17.63 & \textbf{53.39} $\pm$ 4.31  \\
    \bottomrule
    \multicolumn{7}{l}{\small $\diamond$ denotes the source tasks.}
    \end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{table}[htbp!]
\caption{Win rates of \textit{Stalker-Zealot} task set. The reported results are averaged over 5 random seeds. The bold number denotes the best performance and $\pm$ denotes one standard deviation.}
\label{tab:sz-win-rates}
\begin{center}
\begin{small}
\begin{sc}
\resizebox{0.85\textwidth}{!}{
\begin{tabular}{c|lccccc}
    \toprule
    \multicolumn{2}{c}{\textbf{Task}} & \textbf{BC-t} & \textbf{BC-r} &\textbf{UpDeT} & \textbf{ODIS} & \textbf{SD-CQL (ours)} \\
    \midrule
    %\multicolumn{6}{c}{\textbf{Expert}} \\
    %\midrule
    \multirow{13}{*}{\rotatebox{90}{\makecell{Expert}}}
    &1s3z & 51.88 $\pm$ 28.26 & \textbf{65.00} $\pm$ 7.46 & 7.50 $\pm$ 7.84 & 41.25 $\pm$ 38.55 & 56.25 $\pm$ 28.64 \\ 
    &1s4z & 34.38 $\pm$ 10.83 & \textbf{55.00} $\pm$ 31.14 & 11.88 $\pm$ 23.22 & 19.38 $\pm$ 29.18 & \textbf{55.00} $\pm$ 26.66 \\ 
    &1s5z & 24.38 $\pm$ 28.42 & 19.38 $\pm$ 25.81 & 16.88 $\pm$ 14.92 & 30.62 $\pm$ 40.41 & \textbf{48.12} $\pm$ 20.80 \\ 
    &2s3z $^\diamond$& \textbf{95.62} $\pm$ 2.80 & 95.00 $\pm$ 4.74 & 22.50 $\pm$ 25.33 & 77.50 $\pm$ 25.14 & 88.75 $\pm$ 6.09 \\
    &2s4z $^\diamond$&\textbf{ 81.25} $\pm$ 7.33 & 76.88 $\pm$ 8.44 & 27.50 $\pm$ 29.27 & 53.12 $\pm$ 24.61 & 71.88 $\pm$ 15.62\\
    &2s5z & 66.25 $\pm$ 23.43 & 65.00 $\pm$ 18.54 & 20.62 $\pm$ 16.33 & 48.12 $\pm$ 20.44 & \textbf{72.50} $\pm$ 24.15 \\ 
    &3s3z & 91.25 $\pm$ 5.59 & 76.88 $\pm$ 19.47 & 22.50 $\pm$ 23.74 & 81.88 $\pm$ 15.53 & \textbf{93.12} $\pm$ 6.77 \\ 
    &3s4z & 93.12 $\pm$ 4.64 & 78.75 $\pm$ 6.01 & 26.25 $\pm$ 38.57 & 81.88 $\pm$ 8.95 & \textbf{94.38} $\pm$ 4.64 \\ 
    &3s5z $^\diamond$& \textbf{93.75} $\pm$ 5.85 & 90.00 $\pm$ 8.39 & 20.00 $\pm$ 33.12 & 86.88 $\pm$ 5.13 & 90.00 $\pm$ 6.40 \\ 
    &4s3z & 77.50 $\pm$ 17.31 & 78.12 $\pm$ 16.97 & 17.50 $\pm$ 37.41 & 71.25 $\pm$ 19.06 & \textbf{87.50} $\pm$ 7.33 \\ 
    &4s4z & 66.25 $\pm$ 16.89 & 52.50 $\pm$ 8.39 & 12.50 $\pm$ 22.86 & 52.50 $\pm$ 20.89 & \textbf{71.88} $\pm$ 13.26 \\ 
    &4s5z & 54.38 $\pm$ 12.81 & 38.75 $\pm$ 3.56 & 7.50 $\pm$ 16.77 & 35.00 $\pm$ 27.99 & \textbf{76.25} $\pm$ 19.84 \\ 
    &4s6z & 61.88 $\pm$ 10.92 & 34.38 $\pm$ 13.62 & 8.75 $\pm$ 16.30 & 36.25 $\pm$ 21.83 & \textbf{71.25} $\pm$ 27.46 \\
    \midrule
\multicolumn{2}{c}{\textbf{Average}} & 68.61 $\pm$ 3.93 & 63.51 $\pm$ 2.64 & 17.07 $\pm$ 18.36 & 55.05 $\pm$ 9.11 & \textbf{75.14} $\pm$ 7.78  \\
    \midrule
    %\multicolumn{6}{c}{\textbf{Medium}} \\
    %\midrule
    
    \multirow{13}{*}{\rotatebox{90}{\makecell{Medium}}} 
    &1s3z & 3.12 $\pm$ 3.83 & 14.38 $\pm$ 25.73 & \textbf{25.00} $\pm$ 9.11 & 10.62 $\pm$ 22.05 & 18.12 $\pm$ 26.83 \\ 
    &1s4z & \textbf{18.75} $\pm$ 25.86 & 11.88 $\pm$ 24.84 & 17.50 $\pm$ 14.76 & 7.50 $\pm$ 8.15 & 9.38 $\pm$ 9.63 \\ 
    &1s5z & 5.62 $\pm$ 6.77 & 3.75 $\pm$ 5.13 & 7.50 $\pm$ 4.74 & 0.62 $\pm$ 1.40 & \textbf{13.12} $\pm$ 11.57 \\ 
    &2s3z $^\diamond$& \textbf{49.38} $\pm$ 13.51 & 48.12 $\pm$ 4.74 & 20.00 $\pm$ 4.19 & 38.12 $\pm$ 11.57 & 10.00 $\pm$ 9.22 \\
    &2s4z $^\diamond$ & 11.88 $\pm$ 8.09 & 8.75 $\pm$ 13.15 & \textbf{20.00} $\pm$ 11.82 & 8.75 $\pm$ 7.78 & 18.75 $\pm$ 9.11 \\
    &2s5z & 13.75 $\pm$ 10.50 & 16.25 $\pm$ 8.67 & 10.62 $\pm$ 13.55 & 18.75 $\pm$ 27.15 & \textbf{23.12} $\pm$ 8.15 \\ 
    &3s3z & \textbf{39.38} $\pm$ 15.56 & 26.25 $\pm$ 15.56 & 15.62 $\pm$ 9.11 & 21.88 $\pm$ 20.01 & 38.75 $\pm$ 8.44 \\ 
    &3s4z & \textbf{46.25} $\pm$ 17.59 & 23.75 $\pm$ 12.22 & 18.75 $\pm$ 16.24 & 18.12 $\pm$ 17.59 & \textbf{46.25} $\pm$ 12.96 \\ 
    &3s5z $^\diamond$& 41.88 $\pm$ 9.78 & 43.12 $\pm$ 18.14 & 12.50 $\pm$ 13.07 & 9.38 $\pm$ 9.38 & \textbf{48.12} $\pm$ 16.18 \\ 
    &4s3z & 37.50 $\pm$ 26.42 & \textbf{42.50} $\pm$ 18.57 & 6.25 $\pm$ 9.11 & 11.88 $\pm$ 8.67 & 24.38 $\pm$ 18.67 \\ 
    &4s4z & \textbf{23.75} $\pm$ 5.23 & 16.25 $\pm$ 14.56 & 3.75 $\pm$ 5.59 & 8.12 $\pm$ 10.03 & 20.00 $\pm$ 19.71 \\ 
    &4s5z & 10.62 $\pm$ 5.68 & 10.00 $\pm$ 9.73 & 3.75 $\pm$ 3.42 & 0.62 $\pm$ 1.40 & \textbf{13.12} $\pm$ 7.13 \\ 
    &4s6z & 9.38 $\pm$ 4.94 & 5.62 $\pm$ 4.64 & 1.88 $\pm$ 1.71 & 1.25 $\pm$ 2.80 & \textbf{15.62} $\pm$ 9.11 \\
    \midrule
 \multicolumn{2}{c}{\textbf{Average}}  & \textbf{23.94} $\pm$ 2.72 & 20.82 $\pm$ 3.78 & 12.55 $\pm$ 5.80 & 11.97 $\pm$ 7.53 & 22.98 $\pm$ 4.04  \\
    \midrule
    %\multicolumn{6}{c}{\textbf{Medium-Replay}} \\
    %\midrule
    \multirow{13}{*}{\rotatebox{90}{\makecell{Medium-Replay}}} 
    &1s3z & 17.50 $\pm$ 11.40 & \textbf{33.12} $\pm$ 20.32 & 11.88 $\pm$ 14.05 & 3.12 $\pm$ 5.41 & 17.50 $\pm$ 7.53  \\ 
    &1s4z & 7.50 $\pm$ 6.09 & \textbf{13.75} $\pm$ 13.00 & 4.38 $\pm$ 9.78 & 10.62 $\pm$ 10.73 & 11.88 $\pm$ 10.22 \\ 
    &1s5z & 3.75 $\pm$ 4.07 & 3.75 $\pm$ 8.39 & 1.88 $\pm$ 2.80 & 5.00 $\pm$ 7.19 & \textbf{14.38 }$\pm$ 11.82 \\ 
    &2s3z $^\diamond$& 12.50 $\pm$ 6.63 & 6.88 $\pm$ 5.59 & \textbf{16.88} $\pm$ 18.43 & 12.50 $\pm$ 16.39 & 3.75 $\pm$ 4.07 \\
    &2s4z $^\diamond$& 8.12 $\pm$ 9.53 & 10.62 $\pm$ 5.68 & 10.00 $\pm$ 12.77 & 12.50 $\pm$ 13.44 & \textbf{14.38} $\pm$ 5.68 \\
    &2s5z & 11.25 $\pm$ 9.27 & 22.50 $\pm$ 24.25 & 5.62 $\pm$ 4.64 & 10.62 $\pm$ 14.59 & \textbf{32.50} $\pm$ 17.76  \\ 
    &3s3z & 13.75 $\pm$ 12.62 & 12.50 $\pm$ 4.94 & 6.88 $\pm$ 11.98 & 12.50 $\pm$ 13.44 & \textbf{62.50} $\pm$ 14.82 \\ 
    &3s4z & 27.50 $\pm$ 17.46 & 45.00 $\pm$ 19.09 & 10.62 $\pm$ 17.20 & 8.12 $\pm$ 9.00 & \textbf{45.62} $\pm$ 16.03 \\ 
    &3s5z $^\diamond$&26.88 $\pm$ 13.91 & \textbf{30.00} $\pm$ 11.18 & 3.75 $\pm$ 6.77 & 5.62 $\pm$ 3.42 & 25.62 $\pm$ 16.89  \\ 
    &4s3z & 11.25 $\pm$ 18.57 & 23.75 $\pm$ 19.59 & 11.25 $\pm$ 15.56 & 13.75 $\pm$ 16.18 & \textbf{25.00} $\pm$ 8.27\\ 
    &4s4z & 12.50 $\pm$ 9.11 & \textbf{18.75} $\pm$ 15.15 & 3.12 $\pm$ 3.83 & 3.75 $\pm$ 3.42 & 17.50 $\pm$ 13.00 \\ 
    &4s5z & 6.25 $\pm$ 3.83 & 5.00 $\pm$ 6.48 & 0.62 $\pm$ 1.40 & 5.00 $\pm$ 8.15 & \textbf{10.00} $\pm$ 3.42 \\ 
    &4s6z & 3.75 $\pm$ 5.13 & 3.75 $\pm$ 4.07 & 1.88 $\pm$ 4.19 & 3.12 $\pm$ 3.83 & \textbf{14.38} $\pm$ 9.78 \\
    \midrule
   \multicolumn{2}{c}{\textbf{Average}} & 12.50 $\pm$ 1.75 & 17.64 $\pm$ 4.40 & 6.83 $\pm$ 7.03 & 8.17 $\pm$ 6.08 & \textbf{22.69} $\pm$ 3.07 \\
    \midrule
    %\multicolumn{6}{c}{\textbf{Medium-Expert}} \\
    %\midrule
    \multirow{13}{*}{\rotatebox{90}{\makecell{Medium-Expert}}} 
    &1s3z & 47.50 $\pm$ 39.12 & 25.00 $\pm$ 30.22 & 22.50 $\pm$ 19.44 & 49.38 $\pm$ 36.74 & \textbf{53.12} $\pm$ 45.71  \\ 
    &1s4z & 5.00 $\pm$ 3.56 & 13.12 $\pm$ 9.73 & 15.00 $\pm$ 11.14 & 6.88 $\pm$ 15.37 & \textbf{68.12} $\pm$ 37.46 \\ 
    &1s5z & 11.88 $\pm$ 23.22 & 10.62 $\pm$ 20.32 & 13.12 $\pm$ 18.01 & 0.62 $\pm$ 1.40 & \textbf{65.00} $\pm$ 39.24 \\ 
    &2s3z $^\diamond$& 80.62 $\pm$ 21.47 & 71.25 $\pm$ 10.22 & 36.88 $\pm$ 30.09 & 59.38 $\pm$ 35.63 & \textbf{92.50} $\pm$ 10.50 \\
    &2s4z $^\diamond$& 37.50 $\pm$ 32.10 & \textbf{61.25} $\pm$ 15.87 & 25.00 $\pm$ 15.93 & 47.50 $\pm$ 31.44 & 56.88 $\pm$ 36.67  \\
    &2s5z & 25.62 $\pm$ 19.94 & 21.88 $\pm$ 11.48 & 8.75 $\pm$ 7.13 & 27.50 $\pm$ 18.14 & \textbf{55.62} $\pm$ 23.32 \\ 
    &3s3z & 47.50 $\pm$ 37.85 & 56.88 $\pm$ 11.57 & 17.50 $\pm$ 9.00 & 55.00 $\pm$ 29.20 & \textbf{86.88} $\pm$ 11.14 \\ 
    &3s4z & 60.00 $\pm$ 17.87 & 74.38 $\pm$ 12.77 & 23.12 $\pm$ 7.19 & 50.00 $\pm$ 30.78 & \textbf{91.25} $\pm$ 2.61  \\ 
    &3s5z $^\diamond$&67.50 $\pm$ 17.76 & 61.88 $\pm$ 12.18 & 16.88 $\pm$ 13.37 & 31.88 $\pm$ 24.45 & \textbf{84.38} $\pm$ 7.97 \\ 
    &4s3z & 62.50 $\pm$ 18.62 & \textbf{70.62} $\pm$ 7.84 & 6.88 $\pm$ 4.07 & 39.38 $\pm$ 27.12 & 67.50 $\pm$ 19.47 \\ 
    &4s4z & 30.62 $\pm$ 5.59 & 35.00 $\pm$ 23.11 & 8.75 $\pm$ 9.73 & 19.38 $\pm$ 12.77 & \textbf{48.75} $\pm$ 14.42 \\ 
    &4s5z & 10.00 $\pm$ 5.13 & 6.88 $\pm$ 10.22 & 4.38 $\pm$ 8.15 & 6.88 $\pm$ 2.61 & \textbf{46.88} $\pm$ 20.61\\ 
    &4s6z & 10.00 $\pm$ 7.46 & 6.25 $\pm$ 5.85 & 3.12 $\pm$ 5.41 & 5.62 $\pm$ 7.78 & \textbf{36.25} $\pm$ 14.76\\
    \midrule
\multicolumn{2}{c}{\textbf{Average}}& 38.17 $\pm$ 5.38 & 39.62 $\pm$ 2.98 & 15.53 $\pm$ 5.01 & 30.72 $\pm$ 14.65 & \textbf{65.62} $\pm$ 6.27\\
    \bottomrule
    \multicolumn{7}{l}{\small $\diamond$ denotes the source tasks.}
    \end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\newpage
\section{Learning Curves}\label{app:curve}
In Figures \ref{fig:mtm}, we plot the learning curves of the average performance across different task sets for Multi-to-Multi. To make the figures clearer, we use abbreviations to represent dataset quality. Specifically: -E, -M, -MR, and -ME stand for expert, medium, medium-replay, and medium-expert, respectively. We report results over 5 random seeds, where the solid line represents the mean and the shaded area represents one standard deviation. It is evident that SD-CQL sustains the highest average performance across most task sets.

\begin{figure}[!htbp!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\textwidth]{figures/full_mtm_new.pdf}}
\caption{Average winning rates on Multi-to-Multi task sets. We report results over 5 random seeds, where the solid line represents the mean and the shaded area represents one standard deviation.}
\label{fig:mtm}
\end{center}
\vskip -0.2in
\end{figure}

In Figure \ref{fig:otm}, we plot the learning curves for the average performance (marked as ``AVG") and task-specific performance (marked with the respective task name) for One-to-Multi task sets. The results are reported over 5 random seeds, with the solid line representing the mean and the shaded area indicating one standard deviation. It can be observed that even when trained on a single source task, SD-CQL exhibits the best multi-task generalization performance. Notably, in the \textit{Marine-Single} task set, SD-CQL's peak performance on unseen tasks significantly surpasses that of the other baselines.

\begin{figure}[!ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\textwidth]{figures/full_otm_new.pdf}}
\caption{Average and task-specific winning rates on One-to-Multi task sets. We report results over 5 random seeds, where the solid line represents the mean and the shaded area represents one standard deviation.}
\label{fig:otm}
\end{center}
\vskip -0.2in
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\end{document}                    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
