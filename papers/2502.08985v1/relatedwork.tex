\section{Related Work}
\textbf{MARL.} Multi-Agent Reinforcement Learning (MARL) has seen substantial progress in recent years, with numerous approaches developed under different paradigms. The centralized training with decentralized execution (CTDE) paradigm \cite{oliehoek2008optimal, matignon2012coordinated, IntroCTDE} has been particularly influential, with methods such as HASAC\cite{liu2024maximum}, MAPPO \cite{yu2021surprising}, QMIX \cite{qmix}, VDN \cite{sunehag2018value}, and MADDPG \cite{lowe2017multi}. These approaches use centralized training for better coordination and decentralized execution for real-time decision-making. On the other hand, fully decentralized training and execution schemes have also been explored \cite{tampuu2017multiagent,ackermann2019reducing,de2020independent}. However, the performance of such methods is often constrained by the absence of information sharing. In this paper, we mainly focus on CTDE paradigm with QMIX backbone.

\textbf{Offline MARL.} 
 Due to the absence of online interaction with the environment, offline training faces a fundamental challengeâ€”distributional shift. To address this issue, several techniques for single-agent RL have been proposed, many of which leverage conservatism to regularize either the policy \cite{td3bc, kostrikov2021offline} or the Q-value function \cite{cql,kostrikov2021offlines,rezaeifar2022offline}. These methods mitigate the risks of overestimating the value of unseen state-action pairs. However, specific challenges caused by multiple agents, such as the exponential explosion of the complexity, hinder these techniques from directly extending to multi-agent scenarios. Therefore, several tailored approaches \cite{jiang2021offline, yang2021believe, omar, li2023beyond, shao2023counterfactual,liu2024offlinemultiagentreinforcementlearning} have been proposed to address offline MARL. However, these methods often focus excessively on source tasks, inevitably compromising their multi-task generalization ability.

\textbf{Multi-task MARL.} 
Current research on multi-task MARL can be broadly categorized into two types. The first, often referred to as Ad-Hoc Teamwork \cite{stone2010ad, zhang2023fast}, focuses on exploring how to effectively collaborate with unknown and uncontrollable teammates within a given task. The second type, which is the primary focus of this paper, involves scenarios where the algorithm controls the entire team, but the agents are trained on some tasks and testing on unseen tasks with similar structures. This requires agents to learn and utilize generalizable decision-making structures across tasks from a limited set of source tasks, positioning skill discovery as a promising solution. However, most existing approaches discover skills typically through sample reconstruction \cite{wang2020rode,hsl2022,HMASD}, clustering \cite{LI2025106852} or sub-task decomposition \cite{tian2024decompose}, primarily relying on online interactions, with limited attention to offline settings. Recently, ODIS \cite{zhang2023odis} proposed a method for multi-task offline MARL, but it suffers from the limitations of behavior cloning when the scale of source tasks is small.