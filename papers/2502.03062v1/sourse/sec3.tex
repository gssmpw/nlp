\section{Heuristic Algorithm for CP Candidate Selection in Multiple Frequencies}
\label{sec:CpSelection}
%
As discussed in Section~\ref{sec:Introduction}, the types of anomalies occurring in sensor signals can be systematically determined by simultaneously detecting multiple frequency anomalies, such as the harmonics and sidebands of the characteristic frequencies.
%
Thus, we formulate CP candidate selection as an optimization problem that aims to not only estimate the optimal number and location of CPs for each frequency, but also reduce the total number of change locations across all frequencies by aligning the positions.

\subsection{Objective Function for CP Candidate Selection}
%
Let $K^{(d)}$ be the number of selected CP candidates and $\bm{\tau}^{(d)} = \{{\tau}_1^{(d)}, \dots, {\tau}_{K^{(d)}}^{(d)}\}$ be the ordered set of CP candidates (${\tau}_1^{(d)} < \dots < {\tau}_{K^{(d)}}^{(d)}$ and ${\tau}_0^{(d)} = 0, {\tau}_{K^{(d)}+1}^{(d)} = T$) for frequency $d \in \{0, \ldots, D-1\}$.
%
Furthermore, we define the set of CP candidate locations as
\begin{align}
\bm{\tau} = \bigcup_{d = 0}^{D-1} \bm{\tau}^{(d)} = \{{\tau}_1, \dots, {\tau}_{K}\} \, \subseteq [T-1], \label{changetimepoint} %{\color{red}\in 2^{[T-1]}}
\end{align}
where we denote the total number of CP candidate locations as $K = |\bm{\tau}|$.

Let $\bm{\mathcal{T}} = (\bm{\tau}^{(0)}, \dots, \bm{\tau}^{(D-1)})$ be the collection of all CP candidates across all the $D$ frequencies.
%
Then, given an observed time-series data $\bm{x}$ in~(\ref{eq:observedX}), the objective function of our CP candidates $\bm{\mathcal{T}}$ is written as
\begin{equation}
  E(\bm{\mathcal{T}}, \bm{x}) = \sum_{d=0}^{D-1} \sum_{k=1}^{K^{(d)}+1} \mathcal{C} \left(\bm{f}_{\tau_{k - 1}^{(d)}+1 : \tau_k^{(d)}}^{(d)}\right) + \beta^{(d)} K^{(d)} + \gamma K. \label{objective_func}
\end{equation}
%
The first term of the obejective function in~(\ref{objective_func}) indicates the cost for quantifying the variability of segments between two adjacent CP candidates, where
$\mathcal{C}\left(\bm f_{s:e}^{(d)}\right)$
indicates the cost function for an segment $\bm{f}_{s:e}^{(d)} = (f_s^{(d)}, f_{s+1}^{(d)}, \ldots, f_{e}^{(d)})^\top$ specifically defined as
\begin{equation}
  \mathcal{C}\left(\bm{f}_{s:e}^{(d)}\right) = c_{\text{sym}}^{(d)} \sum_{t = s}^{e} \left|{f}_t^{(d)} - \frac{1}{e - s + 1} \sum_{t' = s}^e f_{t'}^{(d)}\right|^2, \notag
\end{equation} 
where 
\begin{equation}
  c_{\text{sym}}^{(d)} =
  \begin{cases}
    1 & (d = 0, \frac{M}{2}) \\
    2 & (d \neq 0, \frac{M}{2})  
  \end{cases}, \notag
\end{equation} 
because the frequency spectra have complex conjugate symmetry.
%
The second term indicates the penalty term for the number of CP candidates in each frequency, while the third term indicate the penalty for the number of total CP candidate locations where $(\beta^{(0)}, \dots, \beta^{(D-1)}) \in \mathbb{R}^{D}$, and $\gamma \in \mathbb{R}$ are hyper-parameters for controlling the balance between the three terms (the details of how to determine these hyper-parameters are presented in Appendix~\ref{app:penalty}).
%
The third penalty term indicates that we take into account the trade-off between cost and penalty term for not only $K^{(d)}$ but also $K$ in~(\ref{objective_func}), hence CP candidates of multiple frequencies tend to be detected at the same time point.

% \subsection{Approximately Solving a Combinatorial Optimization Problem by Simulated Annealing}
\subsection{Approximately Optimizing the Objective Function by Simulated Annealing}

%
Unfortunately, since minimizing the objective function in~(\ref{objective_func}) is a challenging combinatorial optimization problem, we must rely on approximate solutions derived from heuristic algorithms.
%
Following the approach in~\citet{lavielle1998optimal}, we employ \emph{simulated annealing}~\citep{kirkpatrick1983optimization, cerny1985thermodynamical} to approximately solve the combinatorial optimization problem.
%
Simulated annealing is a meta-heuristic algorithm widely applied to various practical problems, as it converges asymptotically to a global solution with high probability under specific conditions.

To approximately optimize the objective function in~(\ref{objective_func}), we perform the following two steps:
%
\begin{itemize}
 \item
      Step 1: Individually estimate the CP candidates for each frequency.
      %
      Specifically, we solve the problem in~(\ref{objective_func}) with $\gamma = 0$.
      %
      This can be optimally achieved by applying dynamic programming to each sequence.
      
 \item
      Step 2: Refine the CP candidates for each frequency estimated in step 1 using simulated annealing to estimate the changes shared across multiple frequencies.
\end{itemize}

We note that approximately solving the combinatorial optimization problem in~(\ref{objective_func}) is NOT our novel contribution (our key contribution, detailed in Section~\ref{sec:SI}, lies in providing a theoretical guarantee for the false positive detection probability of the obtained approximate solution).
%
For example, a similar approach has been used for analogous problems in studies such as \citet{lavielle1998optimal}.
%
Moreover, we do NOT claim that simulated annealing is the optimal approach for this problem; it is simply one of the reasonable choices, and other meta-heuristics could also be used as alternatives.

\subsection{Step 1: Generating Initial Solution by Dynamic Programming}
\label{subsec:DP}
We need to generate an initial solution ${\bm{\mathcal{T}}}^{\text{init}} = ({\bm{\tau}^{\text{init}}}^{(0)}, \dots, {\bm{\tau}^{\text{init}}}^{(D-1)})$ before applying simulated annealing.
To obtain the initial solution, we first set $\gamma = 0$ in~(\ref{objective_func}) to detect CP candidates for each frequency and formulate an optimization problem as
\begin{equation}
  {\bm{\tau}^{\text{init}}}^{(d)} = \underset{\bm{\tau}^{(d)}}{\arg\min\limits} \sum_{k=1}^{{K}^{(d)}+1} \mathcal{C} \left(\bm{f}_{\tau_{k - 1}^{(d)}+1 : \tau_k^{(d)}}^{(d)}\right) + \beta^{(d)} {K}^{(d)}. \label{PO}
\end{equation} 
This optimization problem can be solved efficiently using dynamic programming algorithm which is called Optimal Partitioning~\citep{jackson2005algorithm}.
This method employs the Bellman equation that recursively determines optimal solutions for simpler subproblems.
Given $\mathcal{D}^{\text{init}}$ as the set of frequencies for which at least one CP candidate is detected, 
it is sufficient to apply simulated annealing only to $d \in \mathcal{D}^{\text{init}}$ 
because introducing an additional CP to any frequency $d$ requires a minimum penalty of $\beta^{(d)}$ as shown in~(\ref{objective_func}). 
Thus, we reduce the number of optimal solution candidates, 
and the computational cost of simulated annealing can be decreased without degrading the solution quality.

\subsection{Step 2: Refining Solution by Simulated Annealing}
\label{subsec:SA}
We perform multivariate CP candidate selection in the frequency domain using simulated annealing that is applied for solving large combinatorial optimization problems for which finding global optima is difficult.
In each step of simulated annealing, the Metropolis algorithm is used to accept transitions not only to improving solutions that decrease the objective function, 
i.e., $\Delta E (\mathcal{T}', \mathcal{T}, \bm{x}) = E(\mathcal{T}', \bm{x}) - E(\mathcal{T}, \bm{x}) \leq 0$, 
where $\mathcal{T}$ and $\mathcal{T}'$ are current and new solutions, respectively,
but also to deteriorating solutions that increase the objective function, 
i.e., $\Delta E (\mathcal{T}', \mathcal{T}, \bm{x}) > 0$, 
with the probability controlled by a temperature parameter $c$, % determined by temperature
which allows escape from the local solution. 
The pseudo code of the Metropolis algorithm is shown in Algorithm~{\ref{alg_metro}}.

\begin{algorithm}[b] 
  \caption{\texttt{metropolis\_algorithm}}
  \label{alg_metro}
  \begin{algorithmic}[1]
    \REQUIRE $\Delta E$ and $c$
    \STATE Uniformly sample $\theta$ from $\halfopen{0}{1}$
    \IF{$\Delta E \leq 0$}
      \STATE status $\leftarrow$ Acceptance
    \ELSE 
      \IF{$\exp(-\Delta E/c) > \theta$}
        \STATE status $\leftarrow$ Acceptance
      \ELSE
        \STATE status $\leftarrow$ Rejection
      \ENDIF 
    \ENDIF
    \ENSURE status
  \end{algorithmic}
\end{algorithm}

\textbf{Local search.}
% 
In this paper, we consider four types of neighborhood operations applied to the current solution, i.e., adding, removing, and moving a CP~\citep{lavielle1998optimal} for a randomly selected frequency $d$, 
and merging two adjacent CP locations randomly selected from $\bm{\tau}$ at a random position between them.
%
Schematic illustrations of these four operations are provided in Figures~\ref{fig3} and \ref{fig4}. 
%
We set the number of searching iterations at a specific temperature $c$ equal to the size of neighborhoods, i.e., $|\mathcal{D}^{\text{init}}| \cdot T$~\citep{aarts1989simulated}. 
%
Each operation is randomly selected from adding, removing, and moving one CP. 
%
Subsequently, two adjacent CP locations are merged only once because this operation significantly fluctuates the objective function value.
%
When the operation is not possible (e.g., removing operation for a frequency with no CP), it is skipped.
%
If no transition to neighborhoods occurs when these operations are repeated sufficiently at a certain temperature $c$, the search is terminated. 

\textbf{The setting of initial temperature.}
%
We determine the initial temperature $c_0$ by setting an acceptance ratio
\begin{equation}
 \chi (c_0) = \frac{\# \text{ accepted transitions}}{\# \text{ proposed transitions}} \label{acceptanceratio}
\end{equation}
to a desired value in a preliminary experiment of simulated annealing.
%
In practice, we start by setting the temperature to a sufficiently small positive value, then multiply it with a constant factor $\lambda^+$, larger than 1, as follows
\begin{equation}
 c_{i+1}^+ = \lambda^+ c_i^+, \notag
\end{equation}
where $c_i^+$ represents the $i$-th temperature in the initial value setting, until the acceptance ratio exceeds the predefined criterion~\citep{aarts1989simulated}.
%
Therefore, the initial temperature $c_0$ is specified as the final value of $c_i^+$.

\begin{figure}[t]
  \centering
  \begin{minipage}[t]{0.3\hsize}
      \centering
      \includegraphics[width=0.95\textwidth]{figure/fig3-1.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.3\hsize}
    \centering
    \includegraphics[width=0.95\textwidth]{figure/fig3-1.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.3\hsize}
    \centering
    \includegraphics[width=0.95\textwidth]{figure/fig3-1.pdf}
  \end{minipage}

  \begin{minipage}[t]{0.3\hsize}
      \centering
      \includegraphics[width=0.95\textwidth]{figure/fig3-2.pdf}
      \caption*{(a) Adding at $57$.}
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.3\hsize}
      \centering
      \includegraphics[width=0.95\textwidth]{figure/fig3-3.pdf}
      \caption*{(b) Removing at $30$.}
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.3\hsize}
      \centering
      \includegraphics[width=0.95\textwidth]{figure/fig3-4.pdf}
      \caption*{(c) Moving from $68$ to $73$.}
  \end{minipage}
  \caption{
% Illustrations of the three local search operations for a randomly selected frequency $d$, i.e., adding, removing, and moving a CP.
  Illustrations of the three local search operations for frequency $d$.
  (a) Adding is to insert a CP at a randomly selected time point with no CP.
  (b) Removing means to delete a CP at a randomly selected time point with a CP.
  (c) Moving is to shift a CP that is randomly selected from $\bm{\tau}^{(d)}$ to a random position between its adjacent CPs.
  }
  \label{fig3}
\end{figure}

\begin{figure}[H]
 \centering
 \begin{minipage}[t]{0.6\hsize}
  \centering
  \includegraphics[width=0.95\textwidth]{figure/fig4.pdf}
 \end{minipage}
 \caption{
 Illustration of the local search operation which merges two adjacent CP locations.
 In this figure, CP for frequency $d_2$ at time point $68$ and CP for $d_3$ at $74$ are merged at $70$.
 }
 \label{fig4}
\end{figure}

\textbf{Decrement of temperature.}
%
We employ a geometric cooling schedule, which is used as a practical method of temperature control, altough it does not guarantee the convergence to a global optimum. 
%
The decrement function of the $i$-th temperature $c_i$ is given as
\begin{equation}
  c_{i+1} = \lambda c_i, \notag
\end{equation}
where $\lambda$ is a constant factor smaller than but close to 1. 
%
The value typically lies between $0.8$ and $0.99$~\citep{aarts1989simulated}.
%
That is because the decreasing rate must be sufficiently slow to obtain a better solution for the optimization problem.

The overall procedure of the CP candidate selection using simulated annealing is shown in Algorithm~\ref{alg_sa}.
%
\begin{algorithm}[t] 
  \caption{CP candidate selection using simulated annealing}
  \label{alg_sa}
  \begin{algorithmic}[1]
    \REQUIRE $\bm{x}$
    \STATE Obtain spectral sequences $\bm{f}^{(d)}$ for $d \in \{0, \dots, D-1\}$ in~(\ref{eq:observedF}) by applying STFT to $\bm{x}$
    \STATE Initialize the CP candidates $\bm{\mathcal{T}}$ as $\bm{\mathcal{T}}^{\text{init}}$ in~(\ref{PO}) using dynamic programming
    \STATE $c \leftarrow c_0$ such that $\chi(c_0)$ in~(\ref{acceptanceratio}) is the desired value
    \WHILE{true}
      \WHILE{The number of local search is less than $|\mathcal{D}^{\text{init}}| \cdot T$}
        \STATE $d$ is randomly selected from $\mathcal{D}^{\text{init}}$ 
        \STATE The operation is randomly selected from adding, removing, and moving a CP for $d$
        % \IF{{\color{red}The operation is possible}}
        \STATE Obtain $\bm{\mathcal{T}}'$ by applying the operation to $\bm{\mathcal{T}}$
        \STATE status $\leftarrow$ \texttt{metropolis\_algorithm}($\Delta E (\mathcal{T}', \mathcal{T}, \bm{x}), c$) 
        \IF{status is Acceptance}
          \STATE $\bm{\mathcal{T}} \leftarrow \bm{\mathcal{T}}'$
        \ENDIF
        % \ENDIF
      \ENDWHILE
        \STATE Obtain $\bm{\mathcal{T}}'$ by merging two adjacent CP locations in $\bm{\mathcal{T}}$
        \STATE status $\leftarrow$ \texttt{metropolis\_algorithm}($\Delta E (\mathcal{T}', \mathcal{T}, \bm{x}), c$) 
        \IF{status is Acceptance}
          \STATE $\bm{\mathcal{T}} \leftarrow \bm{\mathcal{T}}'$
        \ENDIF
        \IF{no transition to neighborhoods occured at $c$}
          \STATE \textbf{break}
        \ENDIF
        \STATE $c \leftarrow \lambda c$ 
    \ENDWHILE
    \ENSURE $\bm{\mathcal{T}}$
  \end{algorithmic}
\end{algorithm}
