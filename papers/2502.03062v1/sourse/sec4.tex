\section{Selective Inference on CP candidate Locations}
\label{sec:SI}
%
In this section, using the SI framework, we quantify the statistical significance of all locations selected as CP candidates by the algorithm $\mathcal{A}$ in Section~\ref{sec:CpSelection} in the form of $p$-values.
%
By setting the significance level $\alpha$ (e.g., 0.05 or 0.01) and considering CP candidate locations with the $p$-value $< \alpha$ as the final CPs, 
it is theoretically guaranteed that the false positive detection probabilities (type I error rates) of these final CPs is controlled below the specified significance level $\alpha$.

To formalize the SI framework, let us write the algorithm in Section~\ref{sec:CpSelection} as  
\begin{equation}
\mathcal{A}: \bm{X} \mapsto \bm{\mathcal{T}}, \notag
\end{equation}
and the CP candidates obtained by applying the algorithm to the actual observed time series data \(\bm{x}\) are expressed as  
\begin{equation}
\bm{\mathcal{T}}^{\rm det} = \left({\bm{\tau}^{\rm det}}^{(0)}, \ldots, {\bm{\tau}^{\rm det}}^{(D-1)} \right) = \mathcal{A}(\bm{x}), \label{eq:detected_obs}
\end{equation}
where ${\bm{\tau}^{\rm det}}^{(d)} = \{{\tau_1^{\rm det}}^{(d)}, \ldots, {\tau_{K^{(d)}}^{\rm det}}^{\hspace{-1.6mm}(d)}\}$ 
is the set of CP candidates for frequency $d \in \{0, \ldots, D-1\}$~\footnote{
Note that, to ensure deterministic behavior of the algorithm $\mathcal{A}$, 
the random seed is fixed to a constant value at the beginning of the procedure.}.
%
Furthermore,
let
$\bm{\tau}^{\rm det} = \{\tau_1^{\rm det}, \ldots, \tau_K^{\rm det}\}$
be the ordered set of CP candidate locations, 
$\mathcal{D}_{\tau_k^{\rm det}} \subseteq \{0, \dots, D - 1\}$
be the set of frequencies that have CP candidates at a CP location $\tau_k^{\rm det}$,
and
$k^{(d)} \in [K^{(d)}]$
be the correponding index of the CP candidates for
$d$
and 
$k \in [K]$,
i.e., ${\tau_{k^{(d)}}^{\text{det}}}^{\hspace{-0.7mm}(d)} = \tau_k^{\text{det}}$ holds for all $d \in \mathcal{D}_{\tau_k^{\text{det}}}$~\footnote{
  These notations are somewhat intricate. For example, in the case of Figure~\ref{fig:fig2}, 
  \begin{itemize}  
   \item $D=5$  
   \item $\bm{\mathcal{T}}^{\rm det} = \left(\{97\}, \{28\}, \{28, 70\}, \{70\}, \{\}\right)$  
   \item $\bm{\tau}^{\rm det} = \{28, 70, 97\}$  
   \item $K=3$  
   \item $\mathcal{D}_{\tau_1^{\rm det}} = \{d_1, d_2\}, \mathcal{D}_{\tau_2^{\rm det}} = \{d_2, d_3\}, \mathcal{D}_{\tau_3^{\rm det}} = \{d_0\}$  
   \item $k = 3, 1, 1, 2, 2$ for $(d, k^{(d)}) = (d_0, 1), (d_1, 1), (d_2, 1), (d_2, 2), (d_3, 1)$, respectively.
  \end{itemize}  
  }.

\subsection{Statistical Test on CP Locations}
%
\textbf{Hypotheses.}
For testing the statistical significance of CP location $\tau^{\rm det}_k$ for $k \in [K]$, we consider the following null hypothesis ${\rm H}_{0, k}$ and alternative hypothesis ${\rm H}_{1, k}$:
% 
\begin{align}
 {\rm H}_{0, k}: 
 \frac{1}{{\tau_{k^{(d)}}^{\text{det}}}^{\hspace{-0.7mm}(d)} - {\tau_{k^{(d)}-1}^{\rm det}}^{\hspace{-4.3mm}(d)}} \,
 \sum_{t={\tau_{k^{(d)}-1}^{\rm det}}^{\hspace{-4.3mm}(d)} \, +1}^{{\tau_{k^{(d)}}^{\text{det}}}^{\hspace{-1.2mm}(d)}}
 \mu_t^{(d)}
 &= 
 \frac{1}{{\tau_{k^{(d)}+1}^{\rm det}}^{\hspace{-4.3mm}(d)} - {\tau_{k^{(d)}}^{\text{det}}}^{\hspace{-0.7mm}(d)}}
 \sum_{t={\tau_{k^{(d)}}^{\text{det}}}^{\hspace{-1.2mm}(d)} +1}^{{\tau_{k^{(d)}+1}^{\rm det}}^{\hspace{-4.3mm}(d)}}
 \mu_t^{(d)}, ~~~
 \forall d \in \mathcal{D}_{\tau_k^{\rm det}}, \label{eq:H0}
 \\
 &\text{~v.s.} \notag \\
 {\rm H}_{1, k}:
 \frac{1}{{\tau_{k^{(d)}}^{\text{det}}}^{\hspace{-0.7mm}(d)} - {\tau_{k^{(d)}-1}^{\rm det}}^{\hspace{-4.3mm}(d)}} \,
 \sum_{t={\tau_{k^{(d)}-1}^{\rm det}}^{\hspace{-4.3mm}(d)} \, +1}^{{\tau_{k^{(d)}}^{\text{det}}}^{\hspace{-1.2mm}(d)}}
 \mu_t^{(d)}
 &\neq
 \frac{1}{{\tau_{k^{(d)}+1}^{\rm det}}^{\hspace{-4.3mm}(d)} - {\tau_{k^{(d)}}^{\text{det}}}^{\hspace{-0.7mm}(d)}}
 \sum_{t={\tau_{k^{(d)}}^{\text{det}}}^{\hspace{-1.2mm}(d)} +1}^{{\tau_{k^{(d)}+1}^{\rm det}}^{\hspace{-4.3mm}(d)}}
 \mu_t^{(d)}, ~~~
 \exists d \in \mathcal{D}_{\tau_k^{\rm det}}. \label{eq:H1}
\end{align}
%
The above null hypothesis ${\rm H}_{0, k}$ indicates that, at the location $\tau_k^{\rm det}$, the means of $\mu_t^{(d)}$ in the left segment and the right segment are equal for all frequencies $d \in \mathcal{D}_{\tau_k^{\rm det}}$.
%
On the other hand, the alternative hypothesis ${\rm H}_{1, k}$ implies that the means of the left and right segments are not equal in at least one of the frequencies.

% \textbf{Formulation of hypotheses.}
% We quantify the statistical significance of the detected change time points $\bm{\tau}^{\text{det}}$.
% For the $k$-th detected change time point $\tau_k^{\text{det}} \in \bm{\tau}^{\text{det}}$, 
% let $\mathcal{D}_{\tau_k^{\text{det}}}$ be the set of frequencies % in which $\tau_k^{\text{det}}$ appears. 
% in which $\tau_k^{\text{det}}$ is an element of ${\bm{\tau}^\text{det}}^{(d)}$, 
% and 
% \begin{align}
%   \mu_{\text{seg}}^{(d)}\left(\tau_{k^{(d)}-1}^{\text{det}} : \tau_{k^{(d)}}^{\text{det}}\right) &= \mu_{{\tau_{k^{(d)}-1}^{\text{det}}}+1}^{(d)} = \cdots = \mu_{\tau_{k^{(d)}}^{\text{det}}}^{(d)}, \notag  \\
%   \mu_{\text{seg}}^{(d)}\left(\tau_{k^{(d)}}^{\text{det}} : \tau_{k^{(d)}+1}^{\text{det}}\right) &= \mu_{{\tau_{k^{(d)}}^{\text{det}}}+1}^{(d)} = \cdots = \mu_{\tau_{{k^{(d)}+1}}^{\text{det}}}^{(d)}, \notag
% \end{align}
% where $\tau_{k^{(d)}}^{\text{det}} = \tau_k^{\text{det}}$ for all $d \in \mathcal{D}_{\tau_k^{\text{det}}}$, 
% be the mean of the each segment before and after the detected change time point. 
% Then, we consider the statistical test for the mean-shift problem, 
% where the null and alternative hypotheses are formulated as:
% \begin{align}
%   \text{H}_{0,k}: \forall d \in \mathcal{D}_{\tau_k^{\text{det}}}, \hspace{1mm} \mu_{\text{seg}}^{(d)}\left(\tau_{k^{(d)}-1}^{\text{det}} : \tau_{k^{(d)}}^{\text{det}}\right) &= \mu_{\text{seg}}^{(d)}\left(\tau_{k^{(d)}}^{\text{det}} : \tau_{k^{(d)}+1}^{\text{det}}\right) \notag \\
%   &\text{\hspace{1mm}vs.} \label{hypotheses} \\
%   \text{H}_{1,k}: \exists d \in \mathcal{D}_{\tau_k^{\text{det}}}, \hspace{1mm} \mu_{\text{seg}}^{(d)}\left(\tau_{k^{(d)}-1}^{\text{det}} : \tau_{k^{(d)}}^{\text{det}}\right) &\neq \mu_{\text{seg}}^{(d)}\left(\tau_{k^{(d)}}^{\text{det}} : \tau_{k^{(d)}+1}^{\text{det}}\right). \notag
% \end{align}
% Under the null hypothesis, it is assumed that the mean within each segment remains constant before and after the detected change time point.

\textbf{Test statistic.}
For testing with the null hypothesis~(\ref{eq:H0}) and alternative hypothesis~(\ref{eq:H1}), 
we consider the test statistic constructed by 
computing the differences between averages of the complex spectra in the two segments before and after the CP location $\tau_k^{\text{det}}$ for each frequency $d \in \mathcal{D}_{\tau_k^{\text{det}}}$, 
and then aggregating their squared absolute values. 
Formally, the test statistic is defined as follows:
\begin{align}
 \label{eq:teststatistic}
 T_k(\bm{X}) 
  = \sigma^{-1} \sqrt{\sum_{d \in \mathcal{D}_{\tau_k^{\text{det}}}} \hspace{-1mm} a_{k^{(d)}} \bigg{|} \bar{F}_{{{\tau_{k^{(d)}-1}^{\rm det}}^{\hspace{-4.3mm}(d)}+1:{\tau_{k^{(d)}}^{\text{det}}}^{\hspace{-1.2mm}(d)}}}^{(d)}
  - \bar{F}_{{\tau_{k^{(d)}}^{\text{det}}}^{\hspace{-1.2mm}(d)}+1:{\tau_{k^{(d)}+1}^{\rm det}}^{\hspace{-4.3mm}(d)}}^{(d)} \, \bigg{|}^2}, 
\end{align}
% \begin{equation}
%   \sigma^{-1} ||P_k \bm{X}|| = \sigma^{-1} \sqrt{\sum_{d \in \mathcal{D}_{\tau_k^{\text{det}}}} a^{(d)} \Bigg{|} {\frac{1}{\tau_{k^{(d)}}^{\text{det}} - \tau_{k^{(d)}-1}^{\text{det}}}} \sum_{t = \tau_{k^{(d)}-1}^{\text{det}}+1}^{\tau_{k^{(d)}}^{\text{det}}} \hspace{-2mm} {{\bm{W}_{M}}^{(t, d)}}^T \bm{X} 
%   - \frac{1}{\tau_{k^{(d)}+1}^{\text{det}} - \tau_{k^{(d)}}^{\text{det}}}\sum_{t=\tau_{k^{(d)}}^{\text{det}}+1}^{\tau_{k^{(d)}+1}^{\text{det}}} \hspace{-2mm} {{\bm{W}_{M}}^{(t, d)}}^T \bm{X} \Bigg{|}^2}, \notag 
% \end{equation} 
where
\begin{equation}
  a_{k^{(d)}} = \frac{\left({{\tau_{k^{(d)}}^{\text{det}}}^{\hspace{-0.7mm}(d)} - {\tau_{k^{(d)}-1}^{\rm det}}^{\hspace{-4.3mm}(d)}}\right) \left({\tau_{k^{(d)}+1}^{\rm det}}^{\hspace{-4.3mm}(d)} - {\tau_{k^{(d)}}^{\text{det}}}^{\hspace{-0.7mm}(d)}\right) c_{\text{sym}}^{(d)}}{\left({{\tau_{k^{(d)}+1}^{\rm det}}^{\hspace{-4.3mm}(d)} - {\tau_{k^{(d)}-1}^{\rm det}}^{\hspace{-4.3mm}(d)}}\right)M} \in \mathbb{R} \notag
\end{equation}
is required for scaling the test statistic, 
and the spectral averages in the two segments before and after the CP location $\tau_k^{\text{det}}$ for frequency $d \in \mathcal{D}_{\tau_k^{\text{det}}}$ are respectively denoted as 
\begin{align}
  \bar{F}_{{{\tau_{k^{(d)}-1}^{\rm det}}^{\hspace{-4.3mm}(d)}+1:{\tau_{k^{(d)}}^{\text{det}}}^{\hspace{-1.2mm}(d)}}}^{(d)} 
  &= {\frac{1}{{\tau_{k^{(d)}}^{\text{det}}}^{\hspace{-0.7mm}(d)} - {\tau_{k^{(d)}-1}^{\rm det}}^{\hspace{-4.3mm}(d)}}} \, \sum_{t = {\tau_{k^{(d)}-1}^{\rm det}}^{\hspace{-4.3mm}(d)}+1}^{{\tau_{k^{(d)}}^{\text{det}}}^{\hspace{-1.2mm}(d)}} F_t^{(d)} \notag \\
  &= {\frac{1}{{\tau_{k^{(d)}}^{\text{det}}}^{\hspace{-0.7mm}(d)} - {\tau_{k^{(d)}-1}^{\rm det}}^{\hspace{-4.3mm}(d)}}} \left(\bm{1}_{{\tau_{k^{(d)}-1}^{\rm det}}^{\hspace{-4.3mm}(d)}+1:{\tau_{k^{(d)}}^{\text{det}}}^{\hspace{-1.2mm}(d)}} \otimes \bm{w}_M^{(d)}\right)^\top \bm{X} \in \mathbb{C}, \notag
\end{align}
\begin{align}
  \bar{F}_{{\tau_{k^{(d)}}^{\text{det}}}^{\hspace{-1.2mm}(d)}+1:{\tau_{k^{(d)}+1}^{\rm det}}^{\hspace{-4.3mm}(d)}}^{(d)} 
  &= \frac{1}{{\tau_{k^{(d)}+1}^{\rm det}}^{\hspace{-4.3mm}(d)} - {\tau_{k^{(d)}}^{\text{det}}}^{\hspace{-0.7mm}(d)}}\sum_{t = {\tau_{k^{(d)}}^{\text{det}}}^{\hspace{-1.2mm}(d)}+1}^{{\tau_{k^{(d)}+1}^{\rm det}}^{\hspace{-4.3mm}(d)}} F_t^{(d)} \notag \\
  &= \frac{1}{{\tau_{k^{(d)}+1}^{\rm det}}^{\hspace{-4.3mm}(d)} - {\tau_{k^{(d)}}^{\text{det}}}^{\hspace{-0.7mm}(d)}} \left(\bm{1}_{{\tau_{k^{(d)}}^{\text{det}}}^{\hspace{-1.2mm}(d)}+1:{\tau_{k^{(d)}+1}^{\rm det}}^{\hspace{-4.3mm}(d)}} \otimes \bm{w}_M^{(d)}\right)^\top \bm{X} \in \mathbb{C}. \notag
\end{align}
%
By introducing a projection matrix defined as %depends on the result of the algorithm A
\begin{equation}
  P_k = \sum_{d \in \mathcal{D}_{\tau_k^{\text{det}}}} a_{k^{(d)}} {\bm{v}}_{k^{(d)}} {{\bm{v}}^{\ast}}_{k^{(d)}}^\top \in \mathbb{R}^{N \times N}, \notag
\end{equation}
the test statistic in~(\ref{eq:teststatistic}) is simply written as
\begin{align}
 \label{eq:teststatistic2}
 T_k(\bm{X}) = \sigma^{-1} ||P_k \bm{X}||,
\end{align}
where
\begin{equation}
  {\bm{v}}_{k^{(d)}} = {\frac{1}{{\tau_{k^{(d)}}^{\text{det}}}^{\hspace{-0.7mm}(d)} - {\tau_{k^{(d)}-1}^{\rm det}}^{\hspace{-4.3mm}(d)}}} \left(\bm{1}_{{\tau_{k^{(d)}-1}^{\rm det}}^{\hspace{-4.3mm}(d)}+1:{\tau_{k^{(d)}}^{\text{det}}}^{\hspace{-1.2mm}(d)}} \otimes \bm{w}_M^{(d)}\right) 
  - \frac{1}{{\tau_{k^{(d)}+1}^{\rm det}}^{\hspace{-4.3mm}(d)} - {\tau_{k^{(d)}}^{\text{det}}}^{\hspace{-0.7mm}(d)}} \left(\bm{1}_{{\tau_{k^{(d)}}^{\text{det}}}^{\hspace{-1.2mm}(d)}+1:{\tau_{k^{(d)}+1}^{\rm det}}^{\hspace{-4.3mm}(d)}} \otimes \bm{w}_M^{(d)}\right) \in \mathbb{C}^N, \notag
\end{equation}
and ${{\bm{v}}^{\ast}}_{k^{(d)}}$ is the complex conjugate of ${\bm{v}}_{k^{(d)}}$. 

\textbf{Naive $p$-value.}
%
The sampling distribution of the test statistic in~(\ref{eq:teststatistic}) and (\ref{eq:teststatistic2}) is highly complicated.
%
However, if we ``forget'' the fact that the CP candidates are selected by looking at the sequence $\bm X$, the matrix $P_k$ in~(\ref{eq:teststatistic2}) does not depend on $\bm X$, meaning that the test statistic $T_k(\bm X)$ simply follows a $\chi$-distribution with ${\rm tr}(P_k)$ degrees of freedom under the null hypothesis. 
%
The $p$-values obtained by this sampling distribution are referred to as \emph{naive $p$-values} and computed as 
\begin{equation}
  p_k^{\text{naive}} = \mathbb{P}_{\text{H}_0,k} (T_k(\bm{X}) \geq T_k(\bm{x})). \notag %\label{p_naive}
\end{equation}
%
Of course, since the CP candidates are selected based on the sequence \(\bm{X}\), it is not appropriate to quantify the statistical significance using naive $p$-values.  
%
If naive $p$-values are mistakenly used, the type I error rate can become significantly larger than the significance level $\alpha$.

\subsection{Computing Selective $p$-values}

\textbf{Selective Inference (SI).}
%
To address the above issue, we consider the sampling distribution of the test statistic conditional on the event that the detected CP candidates $\bm{\mathcal{T}}$ for a random sequence $\bm{X}$ is the same as $\bm{\mathcal{T}}^{\text{det}}$ for the observed sequence $\bm{x}$, that is, 
\begin{equation}
  T_k(\bm{X}) \, | \, \{\mathcal{A}(\bm{X}) = \mathcal{A}(\bm{x})\}. \label{condition}
\end{equation}
%
To compute a selective $p$-value based on the conditional sampling distribution in~(\ref{condition}), we also introduce a condition on the sufficient statistic of the nuisance parameter $\mathcal{Q}(\bm{X})$, which is defined as % which is independent of the test statistic $T_k(\bm{X})$ and defined as
\begin{equation}
  \mathcal{Q}(\bm{X}) = \{\mathcal{V}(\bm{X}), \, \mathcal{U}(\bm{X})\} \label{nuisance}
\end{equation} 
with
\begin{equation}
  \mathcal{V}(\bm{X}) = \frac{\sigma P_k \bm{X}}{\| P_k \bm{X} \|} \in \mathbb{R}^N, ~~~ \mathcal{U}(\bm{X}) = (I_{N} - P_k) \bm{X} \in \mathbb{R}^N. \notag
\end{equation}
%
This additional conditioning on $\mathcal{Q}(\bm{X})$ is a standard approach for computational tractability in the SI literature.
% \footnote{The nuisance parameter $\mathcal{Q}(\bm{X})$ corresponds to the component $\{\bm{w}, \bm{z}\}$ in the seminal paper~\citep{chen2020valid} (see Section 3, Theorem 3.7) and this technique is used in almost all the SI-related works that we cited.} 
%
Taking into account~(\ref{condition}) and (\ref{nuisance}), the selective $p$-value is defined as
\begin{equation}
  p_k^{\text{selective}} = \mathbb{P}_{\text{H}_0,k} (T_k(\bm{X}) \geq T_k(\bm{x}) \, | \, \bm{X} \in \mathcal{X}), \label{p_selective}
\end{equation}
where the conditioning event $\mathcal{X}$ is defined as
\begin{equation}
  \mathcal{X} = \{\bm{X} \in \mathbb{R}^N \, | \, \mathcal{A}(\bm{X}) = \mathcal{A}(\bm{x}), \mathcal{Q}(\bm{X}) =\mathcal{Q}(\bm{x})\}. \label{data_space}
\end{equation}
%
The subspace $\mathcal{X}$ is restricted to one-dimensional data space in $\mathbb{R}^N$~\citep{lee2016exact, liu2018more}, as stated in the following theorem.
\begin{theorem}
  \label{thm:truncation}
  The set $\mathcal{X}$ in~(\ref{data_space}) can be rewritten using a scalar parameter $z \in \mathbb{R}$ as
  \begin{equation}
    \mathcal{X} = \{\bm{X} = \bm{a} + \bm{b} z \, | \, z \in \mathcal{Z}\}, \label{one-D}
  \end{equation}
  where $\bm{a}, \bm{b} \in \mathbb{R}^N$, and the truncation region $\mathcal{Z}$ are defined as
  \begin{equation}
    \bm{a} =\mathcal{U}(\bm{x}), ~~~ \bm{b} = \mathcal{V}(\bm{x}), \label{ab}
  \end{equation}
  \begin{equation}
    \mathcal{Z} = \{z \in \mathbb{R} \, | \, \mathcal{A}( \bm{a} + \bm{b} z) = \mathcal{A}(\bm{x})\}. \label{truncatedarea}
  \end{equation}
\end{theorem}

The proof of Theorem~\ref{thm:truncation} is deferred to Appendix~\ref{app:proofs:truncation}. 
%
Let us denote a random variable $Z = T_k(\bm{X}) \in \mathbb{R}$ and its observation $z^{\text{obs}} = T_k(\bm{x}) \in \mathbb{R}$ that satisfy $\bm{X} = \bm{a} + \bm{b} Z$ and $\bm{x} = \bm{a} + \bm{b} z^{\text{obs}}$, respectively.
%
The selective $p$-value in~(\ref{p_selective}) can be rewritten as
\begin{equation}
  p_k^{\text{selective}} = \mathbb{P}_{\text{H}_0,k} (Z \geq z^{\text{obs}} \, | \, Z \in \mathcal{Z}). \label{psel_z}
\end{equation} 
%
Since the uncnoditional variable $Z \sim \chi(\mathrm{tr}(P_k))$ under the null hypothesis, the conditional variable $Z \, | \, Z \in \mathcal{Z}$ follows a truncated $\chi$-distribution with $\mathrm{tr}(P_k)$ degrees of freedom and truncation region $\mathcal{Z}$.
%
Once the truncation region $\mathcal{Z}$ is identified, the selective $p$-value in~(\ref{psel_z}) can be computed as
\begin{equation}
  p_k^{\text{selective}} = 1 - {F_{\text{tr}\left(P_{k}\right)}^{\text{cdf}}}^{\hspace{-3mm} \mathcal{Z}} \, (z^{\text{obs}}), \notag % ただし$F_\nu^\mathcal{Z}$は, 自由度$\nu$, 切断区間$\mathcal{Z}$の切断カイ分布の累積分布関数
\end{equation}
where ${F_{\text{tr}(P_{k})}^{\text{cdf}}}^{\hspace{-3mm} \mathcal{Z}}$ is the cumulative distribution function of the truncated $\chi$-distribution with $\mathrm{tr}(P_k)$ degrees of freedom and the truncation region $\mathcal{Z}$.
%
A schematic illustration of the SI framework is shown in Figure~\ref{fig5}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\hsize]{figure/fig5.pdf}
  \caption{
  %
  Schematic illustration of the SI framework.
  %
  A point in the data space $\mathbb{R}^N$ corresponds to a sequence with length $N$.
  %
  %  The blue regions in the data space indicate that, if we input a point in these regions into the algorithm $\mathcal{A}$, the same CP candidates $\bm{\mathcal{T}}^{\rm det}$ which are obtained from the observed sequence $\bm x$. 
  The darkly shaded regions in the data space indicate that, if we input a point in these regions into the algorithm $\mathcal{A}$, 
  the CP candidates are the same as $\bm{\mathcal{T}}^{\rm det}$ obtained from the observed sequence $\bm x$.
  %
  By conditioning on these regions and $\mathcal{Q}(\bm{X})$, the conditional sampling distribution of the test statistic $T_k(\bm X)$ is represented as a truncated $\chi$-distribution. 
  %
  Selective $p$-values are defined based on the tail probability of such a truncated $\chi$-distribution.
  }
  \label{fig5}
\end{figure}

\begin{theorem}
  \label{thm:uniform}
  The selective $p$-value satisfies the property of a valid $p$-value: 
  \begin{equation}
   \mathbb{P}_{\rm{H}_0,k} (p_k^{\rm{selective}}\leq\alpha) = \alpha, \, \forall \alpha \in [0, 1]. \notag
  \end{equation}
\end{theorem}
%
The proof of Theorem~\ref{thm:uniform} is deferred to Appendix~\ref{app:proofs:uniform}.
%
This theorem guarantees that the type I error rate can be controlled at any significance level $\alpha$ by using the selective $p$-value. 
%
%In the next section, the identification of the truncation region $\mathcal{Z}$ which is required to compute the selective $p$-value will be discussed.



Several methods for computing selective $p$-values have been proposed in the SI community. 
In this study, we adopt the method based on parametric programming~\citep{le2022more} 
for the identification of the truncation region $\mathcal{Z}$.
%
For further details, refer to Appendix~\ref{app:truncation}.

% If we only consider the first condition $\mathcal{A}(\bm{X}) = \mathcal{A}(\bm{x})$, 
% the conditional data space is represented as a subset of $\mathbb{R}^N$, 
% thus the identification is computationally difficult. 
% However, thanks to the second condition $\mathcal{Q}(\bm{X}) =\mathcal{Q}(\bm{x})$, %we only need to consider (考えれば十分である など)
% the subspace $\mathcal{X}$ is restricted to one-dimensional data space in $\mathbb{R}^N$~\citep{lee2016exact, liu2018more}, % N次元ではなく, 
% as stated in the following theorem.

% Thanks to the condition on $\mathcal{Q}(\bm{X})$, 
% we do not need to consider the $N$-dimensional data space. 
% Instead, we only need to consider the one-dimensional projected data space $\mathcal{Z}$ in~(\ref{truncatedarea}). 
% In this case, the conditional sampling distribution of the test statistic follows a truncated $\chi$-distribution. 
