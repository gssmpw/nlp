\section{Related Works}
% \subsection{Ranking models in recommendation}
% The ranking models, typically represented by click-through rate prediction algorithms with an embedding\&multi-layered perceptions (MLP) paradigm, serve as the last stage of the industrial ecommendation system funnel and thus play a pivotal role in personalizing user recommendations in content platforms and e-commerce____. In the past decades, there has been tremendous research work developed to better capture user potential intention and discriminative representation for enhancing the prediction performance of the ranking model, which can be generally divided into feature interaction methods and target-aware sequence modeling methods.

% \textbf{Feature Interaction Modeling in Ranking.} Focusing on modeling discrete identifiers (IDs) feature interactions to capture the co-occurance patterns, FM____ models the inner product of the latent vectors of features. DeepFM ____ further replaces the wide part in wide\&deep____ as FM to reduce labor costs in manually selecting features. Furthermore, Deep and Cross Network (DCN)____ applies a recursive and layerwise feature cross machanism with multiple orders, and DCN-v2____ further enhances the interaction by boosting the vector-form crossing to a matrix-form to pursue improved performance. To leverage the merits of different interaction structures, Deep and Hierarchical Ensemble Network (DHEN)____ is designed to ensemble and complement heterogeneous modules to better capture feature interaction relationships. 

% \textbf{Target-aware Sequential Modeling in Ranking.} The other significant progress in ranking models is the capture of user intention via sequential modeling. As the target item can be readily acquired and interacted in the ranking model of industrial recommenders, the most typical modeling paradigm is thus to use the target attention mechanism to extract user intentions from behavioral sequences. Among them, the representative milestone, i.e., DIN____, actually launched a novel direction in recommendation which builds a MLP-based target-aware attention mechanism and aggregates the sequence through weighted sumpooling. After that, deep interest evolution network____, deep interest highlight network____, search-based interest model____ are further developed by considering the evolving process, trigger-induced ranking, and two-stage long sequence modeling. Besides, Co-Action Network____ developed a target-and-sequence-item co-action unit to capture the interaction and aggregate user sequence. Besides, with the rapid progress of transformer-based models____, the scaled dot-product based multi-head target attention (MHTA), has also been widely applied to sequential modeling. 

% Despite the broad developments in terms of both feature interaction and sequential modeling, the multi-domain problem that widely exist in large-scale recommender systems has been seldomly considered in these methods.

% \subsection{Multi-Domain Recommendation}
% As a major area in both multi-domain learning and domain adaptation, multi-domain recommendation aims to provide accurate personalized recommendation results for users and items from multiple domains. Early efforts for multi-domain recommendation are generally made by building separate models for different domains with the domain-specific data, or a unified model with the data mixed from multi-domains____. However, the separate modeling ignores the commonalities lied in different domains, while the unified modeling paradigm overlook the difference among various domains, which leads to insufficient exploration of multi-domain data and may hurt the performance on specific domains, yielding suboptimal performance. To further mitigate the issue, traditional methods generally follow a coarse-grained separate-and-shared paradigm for multi-domain recommendation. For example, STAR____ decompose the network parameters into common-parameters and domain-specific-parameters, and the latter are trained with domain-specific data only. PLE____ builds private and shared experts and aggregates the multi-experts output through a gating mechanism. Furthermore, the fine-grained parameter generation methods draw increased attention recently. APG____ learns a instance-wise specific network parameter to replace the conventional all-shared parameters. AdaSparse____ further builds sparse personalized structure for different samples by disabling the neurons with small values. Despite effectiveness, these methods are designed for neural network structures, while the multi-domain learning for sequential-modeling is seldom considered.
% % i.e., most of these methods are designed for replacing the simple single-domain MLPs in industrial recommenders, 

% Some other methods also contribute learning personalized embeddings for multi-domain recommendation. The typical idea is to use the gate mechanism to generate bitwise weights to reweigh the embeddings in recommenders. For example, GateNet____ calculates a feature-wise weight to represent its importance which is then used to reweigh the feature embedding. However, this weight is obtained by forward the feature itself via a linear transformation, and thus GateNet is inherently embedding-reweighting method with no personalities across different users. To this end, the feature refinement network (FRNet)____ is developed with all the user-features as context as gate input, which thus provides user-level personalized weight on specific features compared with GateNet. However, both methods are purely developed for learning adaptive embedding representations, without considering the impact of multi-domains. To this end, parameter and embedding personalized network (PEPNet) ____ and domain facilitated feature modeling (DFFM)____ are developed. PEPNet provides a unified structure to personalize both network parameters and embeddings. Specifically, similar to AdaSparse, PEPNet takes domain-specific features as gate input, and the generated bit-wise weights are then used to scale both the embeddings and the input of each layer in the network. Besides, DFFM is proposed to incorporate the domain-specific information into embedding personalization and sequence modeling, while for the latter, the generated weights are used to modify the parameters in self-attention via a simple linear transformation, which is not flexible and sufficient enough to capture the impact of multi-domain in target-aware sequential modeling. Besides, the multi-domain impact of the candidate item on different behavioral items is seldom considered in existing methods.