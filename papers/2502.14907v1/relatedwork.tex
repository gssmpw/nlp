\section{Related Work}
\label{relatedwork}
%\textcolor{red} {\textbf {Bhatta}}

%CC-NEWS, which we collected from the English portion of the CommonCrawl News dataset (Nagel, 2016). The data contains 63 million English news articles crawled between September 2016 and February 2019. (76GB after filtering).4

In this work we aim to create a large dataset capable for pre-training of a LLM. There are several related works in this space. Prior public pre-training datasets are typically derived from the Common Crawl ~\cite{commoncrawl}. Early works include the C4 dataset with 160 billion tokens \cite{raffel2020exploring} and the Pile dataset with billion tokens \cite{gao2020pile}. The C4 dataset is curated from the April 2009 snapshot of the Common Crawl. It uses langdetect \cite{langdetect} to detect English text, applies a series of heuristic filters including discarding any page with less than 3 sentences, removing lines without any terminal punctuation mark, removing any page containing any word in a list of dirty, naughty, obscene or bad words etc, and also performs deduplication by removing all but one of any three-sentence span occurring more than once in the dataset. The Pile is a composite dataset that includes the Pile-CC, which is based on Common Crawl. It uses pycld2 \citep{pycld2} for language detection, removes boilerplate using jusText \citep{justext}, applies classifier-based filtering and performs fuzzy deduplication. 

Multilingual models like XLM RoBERTa \cite{XLMRoBERTa} used the CC100 dataset \cite{conneu2019cc100}. This dataset was curated using the CCNET \cite{wenzek2019ccnet} processing pipeline on one year of Common Crawl snapshots. CCNet uses the data processing methods introduced in fastText \cite{joulin2017bag}, which include deduplicating documents and applying LangID filtering. It then adds a filtering step to select documents that are similar to high-quality corpora like Wikipedia by utilizing a 5-gram KenLM filter.

RedPajama dataset \cite{weber2024redpajama} is an open source attempt to recreate the dataset used to train Llama models. It is a composite dataset which includes text obtained from the Common Crawl by using the CCNet pipeline \cite{wenzek2019ccnet} and a classifier trained to identify documents similar to Wikipedia articles or references. SlimPajama with 627B tokens \cite{cerebras2023slimpajama} further refines RedPajama by removing short documents and performing additional fuzzy dedupllication. RedPajama-V2 \cite{weber2024redpajama} with 30 trillion tokens is entirely based on the Common Crawl and contains annotations without applying any filtering. These annotations cover filtering techniques from CCNet, C4, and others, and also labels identifying deduplicates using exact and fuzzy deduplication. 

RefinedWeb dataset \cite{penedo2023refinedweb} is a Common Crawl-based dataset, using trafilatura \cite{trafilatura} for text extraction, fastText-based language identification \cite{joulin2017bag}, heuristic rules for quality filtering, and fuzzy and exact deduplication. Dolma \cite{soldaini2024dolma} is a 3 trillion token composite dataset with a Common Crawl-based portion, which employs fastText for language identification, primarily uses heuristic rules from MassiveWeb \cite{rae2021scaling} for quality filtering, applies toxicity filtering based on rules and classifiers and performs deduplication at URL, document and paragraph levels. 

More recent datasets include FineWeb datasets \cite{penedo2024fineweb}, DCLM-Baseline \cite{li2024datacomplm}, and TxT360 \cite{txt360data2024}. FineWeb consists of 15T tokens derived from the Common Crawl by applying a series of processing steps, mainly including language classification, fuzzy deduplication at snapshot level and heuristic rule-based quality filters. 
% It has been shown in \cite{penedo2024fineweb} that models trained on  FineWeb outperform those trained on several public datasets â€” C4, RefinedWeb, Dolma, RedPajamaV, SlimPajama and the Pile. 
Subsequently, two smaller but higher quality versions called FineWeb-Edu (1.3 trillion tokens) and FineWeb-Edu-Score2 (5.4 trillion tokens) derived from FineWeb were released \cite{penedo2024fineweb}. These smaller high quality derivatives of FineWeb are created by retaining documents perceived to have higher educational value from FineWeb. See Appendix \ref{app:fineweb} for more details on FineWeb.

DCLM-Baseline is obtained from the Common Crawl snapshots by using resiliparse \cite{resiliparse} for text extraction,  heuristic quality filters from RefinedWeb, fuzzy deduplication with Bloom filter \cite{bff}, model-based quality filtering using a specially trained fastText classifier. TxT360 is a composite dataset obtained from Common Crawl snapshots and 14 high-quality datasets (e.g. FreeLaw, Ubuntu IRC, etc). TxT360 is obtained by first applying local exact deduplication, global fuzzy deduplication, and quality filtering to both web and curated datasets, resulting in approximately 5 trillion tokens, which are then up-sampled to over 15 trillion tokens. The mixing and up-sampling approach is shown essential to boosting TxT360 performance. 

Nemotron-CC \cite{su2024nemotron} and Zyda2 \cite{tokpanov2024zyda} are concurrent works published recently. Zyda-2 is a 5 trillion high-quality token dataset obtained by collating high-quality open-source datasets including FineWeb-Edu, DCLM, Zyda-1, and Dolma-CC and then applying cross-deduplication and model-based quality filtering. Nemotron-CC is a 6.3 trillion token dataset, including 4.4 trillion tokens from Common Crawl by applying exact substring deduplication, global fuzzy deduplication and model-based quality filtering. Nemotron-CC also includes 1.9 trillion synthetic tokens (approximately 30\% of the data) generated using a rephrasing-based approach from low-quality and high-quality documents.

We take FineWeb \cite{penedo2024fineweb} as the starting point to build our dataset since FineWeb is sufficiently large dataset with 15T tokens which has been shown to outperform several public datasets -- C4, RefinedWeb, Dolma, RedPajamaV, SlimPajama and the Pile.
While FineWeb-Edu, FineWeb-Edu-Score-2 \cite{penedo2024fineweb} and the recent DCLM-Baseline \cite{li2024datacomplm} improve data quality over FineWeb they do so by performing aggressive model-based quality filtering. Such an aggressive filtering cuts down their size which may not be sufficient for pre-training (as pre-training typically consists of only one pass or few passes over the pre-training dataset \cite{muennighoff2023scaling}). Our GneissWeb recipe achieves a favorable trade-off between data quality and quantity thereby producing $\sim$10T high quality tokens with higher performance than prior datasets with 5T+ tokens.  


% The early encoder-decoder models like BART \cite{BART} was trained on the same datasets that encoder models like RoBERTa \cite{RoBERTa} was trained on. These included datasets like Wikipedia, BookCorpus, OpenWebText, Stories and CC News. CC News was curated by the Common Crawl Foundation by extracting news articles from the web. Other Encode Decoder models like T5 \cite{T5} were based on the C4 dataset. This dataset was curated from the April 2009 snapshot of Common Crawl and filtered with a set of filters called the T5 filters.  The T5 filters included discarding any page with less than 3 sentences, retaining lines which ended in a terminal punctuation mark, removing any page that contained any word in a list of dirty, naughty, obscene or bad words, removed pages with curly bracket, boiler plates etc.  They also deduplicated by discarding all but one of any three-sentence span occurring more than once in the data set. 

% Multilingual models like XLM RoBERTa \cite{XLMRoBERTa} used the CC100 dataset.  This dataset was curated using the CCNET \cite{wenzek2019ccnet} processing pipeline on one year of Common Crawl snapshots. CCNet uses the data processing methods introduced in fastText\cite{joulin2017bag}, which include deduplicating documents and applying LangID filtering. It then adds a filtering step to select documents that are similar to high-quality corpora like Wikipedia by utilizing a 5-gram KenLM filter.

% Subsequently the Pile \cite{Pile} dataset was created. it was used to train the GPT-Neo \cite{GptNeo} model. The dataset included the Pile CC which was based on Common Crawl and filtered for language id, boiler plate removal. classifier based filtering and  fuzzy deduplication. Another dataset was RedPajama \cite{redpajama1} which was an open source attempt to recreate the dataset used by Llama. It uses the CCNet pipeline to process Common Crawl snapshots and also retained C4 dataset.  SlimPajama further refined RedPajama by removing short documents and doing fuzzy deduplication. RedPajama V2 was entirely based on Common Crawl and had annotations rather than filtering.  These annotations covered  filterings employed by CCNet, C4, RefinedWeb and others. It also had labels identifying duplicates using exact and fuzzy deduplications. 

% The Falcon models \cite{falcon}  used the RefinedWeb dataset \cite{refinedweb}. This dataset was also based on Common Crawl and used fuzzy and exact deduplication, language identification, huristic rules for filtering data, 

% Dolma \cite{soldaini2024dolma} is a 3 trillion English token dataset based on Common Crawl. It was used to train the  OLMo \cite{OLMO} model. It employs fastText for language classification and primarily relies on Gopher \cite{rae2021scaling} for document quality, implementing all Gopher filtering rules and filtering out all paragraphs that do not end with punctuation, as recommended in C4 \cite{raffel2020exploring}.

% The LLM360 team released an open-source 15 trillion token dataset, TxT360 \cite{txt360data2024}, obtained from 99 CommonCrawl snapshots and 14 high-quality datasets (e.g. FreeLaw, FreeLaw, etc). TxT360 was obtained by first applying local exact deduplication, global fuzzy deduplication, and quality filtering to both web and curated datasets, resulting in approximately 5 trillion tokens, which were then upsampled to over 15 trillion tokens. The mixing and oversampling approach was essential to boosting TxT360 performance. 

% Nemotron-CC \cite{su2024nemotron} and Zyda2 \cite{tokpanov2024zyda} are concurrent works published recently. Zyda-2 is a five trillion high-quality token dataset obtained by collating high-quality open-source datasets including FineWeb-Edu, DCLM, Zyda-1, and Dolma-CC and then applying cross-deduplication and model-based quality filtering. Nemotron-CC is a 6.3 trillion token dataset, including 4.4 trillion tokens from Common Crawl, by applying exact substring deduplication, global fuzzy deduplication, and model-based quality filtering. Nemotron-CC also includes 1.9 trillion synthetic tokens (approximately 30\% of the data) generated using a rephrasing-based approach from low-quality and high-quality documents. 

% Hugging Face introduced FineWeb \cite{penedo2024fineweb}, a new, large-scale dataset for LLM pre-training, consisting of 15 trillion tokens. FineWeb is derived from 96 Common Crawl snapshots, focusing on English text by applying a series of processing steps, mainly including language classification, fuzzy deduplication at snapshot level, and heuristic rule-based quality filters. It has been shown in \cite{penedo2024fineweb} that models trained on  FineWeb outperform those trained on other publicly available datasets â€” C4, RefinedWeb, Dolma, RedPajamaV, SlimPajama and the Pile.

% Subsequently, two smaller but higher quality versions called FineWeb-Edu (1.3 trillion tokens) and FineWeb-Edu-Score2 (5.4 trillion tokens) derived from FineWeb were released \cite{penedo2024fineweb}. These smaller high quality versions of FineWeb are created by retaining documents perceived to have higher educational value from FineWeb.

%Example Reference \cite{greenwade93}\\
%Data quality is vital for the performance of language models. High-quality data significantly boosts LLMsâ€™ ability to generalize and cuts costs by reducing the need for retraining. This underscores the critical role of quality filtering and data deduplication in enhancing data quality, directly impacting LLM performance.

%Each document was subjected to deduplication at paragraph level, a language identification as well as language model based perplexity.score,