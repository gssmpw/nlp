\section{Related Work}
\label{relatedwork}
%\textcolor{red} {\textbf {Bhatta}}

%CC-NEWS, which we collected from the English portion of the CommonCrawl News dataset (Nagel, 2016). The data contains 63 million English news articles crawled between September 2016 and February 2019. (76GB after filtering).4

In this work we aim to create a large dataset capable for pre-training of a LLM. There are several related works in this space. Prior public pre-training datasets are typically derived from the Common Crawl ____. Early works include the C4 dataset with 160 billion tokens ____ and the Pile dataset with billion tokens ____. The C4 dataset is curated from the April 2009 snapshot of the Common Crawl. It uses langdetect ____ to detect English text, applies a series of heuristic filters including discarding any page with less than 3 sentences, removing lines without any terminal punctuation mark, removing any page containing any word in a list of dirty, naughty, obscene or bad words etc, and also performs deduplication by removing all but one of any three-sentence span occurring more than once in the dataset. The Pile is a composite dataset that includes the Pile-CC, which is based on Common Crawl. It uses pycld2 ____ for language detection, removes boilerplate using jusText ____, applies classifier-based filtering and performs fuzzy deduplication. 

Multilingual models like XLM RoBERTa ____ used the CC100 dataset ____. This dataset was curated using the CCNET ____ processing pipeline on one year of Common Crawl snapshots. CCNet uses the data processing methods introduced in fastText ____, which include deduplicating documents and applying LangID filtering. It then adds a filtering step to select documents that are similar to high-quality corpora like Wikipedia by utilizing a 5-gram KenLM filter.

RedPajama dataset ____ is an open source attempt to recreate the dataset used to train Llama models. It is a composite dataset which includes text obtained from the Common Crawl by using the CCNet pipeline ____ and a classifier trained to identify documents similar to Wikipedia articles or references. SlimPajama with 627B tokens ____ further refines RedPajama by removing short documents and performing additional fuzzy dedupllication. RedPajama-V2 ____ with 30 trillion tokens is entirely based on the Common Crawl and contains annotations without applying any filtering. These annotations cover filtering techniques from CCNet, C4, and others, and also labels identifying deduplicates using exact and fuzzy deduplication. 

RefinedWeb dataset ____ is a Common Crawl-based dataset, using trafilatura ____ for text extraction, fastText-based language identification ____, heuristic rules for quality filtering, and fuzzy and exact deduplication. Dolma ____ is a 3 trillion token composite dataset with a Common Crawl-based portion, which employs fastText for language identification, primarily uses heuristic rules from MassiveWeb ____ for quality filtering, applies toxicity filtering based on rules and classifiers and performs deduplication at URL, document and paragraph levels. 

More recent datasets include FineWeb datasets ____, DCLM-Baseline ____, and TxT360 ____. FineWeb consists of 15T tokens derived from the Common Crawl by applying a series of processing steps, mainly including language classification, fuzzy deduplication at snapshot level and heuristic rule-based quality filters. 
% It has been shown in ____ that models trained on  FineWeb outperform those trained on several public datasets — C4, RefinedWeb, Dolma, RedPajamaV, SlimPajama and the Pile. 
Subsequently, two smaller but higher quality versions called FineWeb-Edu (1.3 trillion tokens) and FineWeb-Edu-Score2 (5.4 trillion tokens) derived from FineWeb were released ____. These smaller high quality derivatives of FineWeb are created by retaining documents perceived to have higher educational value from FineWeb. See Appendix \ref{app:fineweb} for more details on FineWeb.

DCLM-Baseline is obtained from the Common Crawl snapshots by using resiliparse ____ for text extraction,  heuristic quality filters from RefinedWeb, fuzzy deduplication with Bloom filter ____, model-based quality filtering using a specially trained fastText classifier. TxT360 is a composite dataset obtained from Common Crawl snapshots and 14 high-quality datasets (e.g. FreeLaw, Ubuntu IRC, etc). TxT360 is obtained by first applying local exact deduplication, global fuzzy deduplication, and quality filtering to both web and curated datasets, resulting in approximately 5 trillion tokens, which are then up-sampled to over 15 trillion tokens. The mixing and up-sampling approach is shown essential to boosting TxT360 performance. 

Nemotron-CC ____ and Zyda2 ____ are concurrent works published recently. Zyda-2 is a 5 trillion high-quality token dataset obtained by collating high-quality open-source datasets including FineWeb-Edu, DCLM, Zyda-1, and Dolma-CC and then applying cross-deduplication and model-based quality filtering. Nemotron-CC is a 6.3 trillion token dataset, including 4.4 trillion tokens from Common Crawl by applying exact substring deduplication, global fuzzy deduplication and model-based quality filtering. Nemotron-CC also includes 1.9 trillion synthetic tokens (approximately 30\% of the data) generated using a rephrasing-based approach from low-quality and high-quality documents.

We take FineWeb ____ as the starting point to build our dataset since FineWeb is sufficiently large dataset with 15T tokens which has been shown to outperform several public datasets -- C4, RefinedWeb, Dolma, RedPajamaV, SlimPajama and the Pile.
While FineWeb-Edu, FineWeb-Edu-Score-2 ____ and the recent DCLM-Baseline ____ improve data quality over FineWeb they do so by performing aggressive model-based quality filtering. Such an aggressive filtering cuts down their size which may not be sufficient for pre-training (as pre-training typically consists of only one pass or few passes over the pre-training dataset ____). Our GneissWeb recipe achieves a favorable trade-off between data quality and quantity thereby producing $\sim$10T high quality tokens with higher performance than prior datasets with 5T+ tokens.  


% The early encoder-decoder models like BART ____ was trained on the same datasets that encoder models like RoBERTa ____ was trained on. These included datasets like Wikipedia, BookCorpus, OpenWebText, Stories and CC News. CC News was curated by the Common Crawl Foundation by extracting news articles from the web. Other Encode Decoder models like T5 ____ were based on the C4 dataset. This dataset was curated from the April 2009 snapshot of Common Crawl and filtered with a set of filters called the T5 filters.  The T5 filters included discarding any page with less than 3 sentences, retaining lines which ended in a terminal punctuation mark, removing any page that contained any word in a list of dirty, naughty, obscene or bad words, removed pages with curly bracket, boiler plates etc.  They also deduplicated by discarding all but one of any three-sentence span occurring more than once in the data set. 

% Multilingual models like XLM RoBERTa ____ used the CC100 dataset.  This dataset was curated using the CCNET ____ processing pipeline on one year of Common Crawl snapshots. CCNet uses the data processing methods introduced in fastText____, which include deduplicating documents and applying LangID filtering. It then adds a filtering step to select documents that are similar to high-quality corpora like Wikipedia by utilizing a 5-gram KenLM filter.

% Subsequently the Pile ____ dataset was created. it was used to train the GPT-Neo ____ model. The dataset included the Pile CC which was based on Common Crawl and filtered for language id, boiler plate removal. classifier based filtering and  fuzzy deduplication. Another dataset was RedPajama ____ which was an open source attempt to recreate the dataset used by Llama. It uses the CCNet pipeline to process Common Crawl snapshots and also retained C4 dataset.  SlimPajama further refined RedPajama by removing short documents and doing fuzzy deduplication. RedPajama V2 was entirely based on Common Crawl and had annotations rather than filtering.  These annotations covered  filterings employed by CCNet, C4, RefinedWeb and others. It also had labels identifying duplicates using exact and fuzzy deduplications. 

% The Falcon models ____  used the RefinedWeb dataset ____. This dataset was also based on Common Crawl and used fuzzy and exact deduplication, language identification, huristic rules for filtering data, 

% Dolma ____ is a 3 trillion English token dataset based on Common Crawl. It was used to train the  OLMo ____ model. It employs fastText for language classification and primarily relies on Gopher ____ for document quality, implementing all Gopher filtering rules and filtering out all paragraphs that do not end with punctuation, as recommended in C4 ____.

% The LLM360 team released an open-source 15 trillion token dataset, TxT360 ____, obtained from 99 CommonCrawl snapshots and 14 high-quality datasets (e.g. FreeLaw, FreeLaw, etc). TxT360 was obtained by first applying local exact deduplication, global fuzzy deduplication, and quality filtering to both web and curated datasets, resulting in approximately 5 trillion tokens, which were then upsampled to over 15 trillion tokens. The mixing and oversampling approach was essential to boosting TxT360 performance. 

% Nemotron-CC ____ and Zyda2 ____ are concurrent works published recently. Zyda-2 is a five trillion high-quality token dataset obtained by collating high-quality open-source datasets including FineWeb-Edu, DCLM, Zyda-1, and Dolma-CC and then applying cross-deduplication and model-based quality filtering. Nemotron-CC is a 6.3 trillion token dataset, including 4.4 trillion tokens from Common Crawl, by applying exact substring deduplication, global fuzzy deduplication, and model-based quality filtering. Nemotron-CC also includes 1.9 trillion synthetic tokens (approximately 30\% of the data) generated using a rephrasing-based approach from low-quality and high-quality documents. 

% Hugging Face introduced FineWeb ____, a new, large-scale dataset for LLM pre-training, consisting of 15 trillion tokens. FineWeb is derived from 96 Common Crawl snapshots, focusing on English text by applying a series of processing steps, mainly including language classification, fuzzy deduplication at snapshot level, and heuristic rule-based quality filters. It has been shown in ____ that models trained on  FineWeb outperform those trained on other publicly available datasets — C4, RefinedWeb, Dolma, RedPajamaV, SlimPajama and the Pile.

% Subsequently, two smaller but higher quality versions called FineWeb-Edu (1.3 trillion tokens) and FineWeb-Edu-Score2 (5.4 trillion tokens) derived from FineWeb were released ____. These smaller high quality versions of FineWeb are created by retaining documents perceived to have higher educational value from FineWeb.

%Example Reference ____\\
%Data quality is vital for the performance of language models. High-quality data significantly boosts LLMs’ ability to generalize and cuts costs by reducing the need for retraining. This underscores the critical role of quality filtering and data deduplication in enhancing data quality, directly impacting LLM performance.

%Each document was subjected to deduplication at paragraph level, a language identification as well as language model based perplexity.score,