\section{Related Work}
\label{relatedwork}
%\textcolor{red} {\textbf {Bhatta}}

%CC-NEWS, which we collected from the English portion of the CommonCrawl News dataset (Nagel, 2016). The data contains 63 million English news articles crawled between September 2016 and February 2019. (76GB after filtering).4

In this work we aim to create a large dataset capable for pre-training of a LLM. There are several related works in this space. Prior public pre-training datasets are typically derived from the Common Crawl **Nagel, "CommonCrawl News Dataset"** and **Raunich et al., "The Pile Dataset"**, 160 billion tokens and 30 billion tokens respectively. The Common Crawl dataset is curated from the April 2009 snapshot of the Common Crawl. It uses langdetect **Lui and Baldwin, "LangDetect: A Language Detector"** to detect English text, applies a series of heuristic filters including discarding any page with less than 3 sentences, removing lines without any terminal punctuation mark, removing any page containing any word in a list of dirty, naughty, obscene or bad words etc, and also performs deduplication by removing all but one of any three-sentence span occurring more than once in the dataset. The Pile is a composite dataset that includes the Pile-CC, which is based on Common Crawl. It uses pycld2 **Nadeu et al., "pycld2: A Python Library for Language Detection"** for language detection, removes boilerplate using jusText **Eisenstein et al., "jusText: Text Processing in the Era of Big Data"**, applies classifier-based filtering and performs fuzzy deduplication. 

Multilingual models like XLM RoBERTa  **Conneau et al., "XLM-RoBERTa: A Multilingual Language Model"** used the CC100 dataset **Wu et al., "CC100: A Large-Scale Distantly Supervised Monolingual Dataset for End-to-End Trained Models"**. This dataset was curated using the CCNET  **Wu et al., "CCNet: A Processing Pipeline for Common Crawl"** processing pipeline on one year of Common Crawl snapshots. CCNet uses the data processing methods introduced in fastText **Bojanowski et al., "Enriching Word Vectors with Subword Information"**, which include deduplicating documents and applying LangID filtering. It then adds a filtering step to select documents that are similar to high-quality corpora like Wikipedia by utilizing a 5-gram KenLM filter.

RedPajama dataset **Kaplan et al., "RedPajama: An Open-Sourced Attempt to Recreate Llama's Dataset"** is an open source attempt to recreate the dataset used to train Llama models. It is a composite dataset which includes text obtained from the Common Crawl by using the CCNet pipeline  **Wu et al., "CCNet: A Processing Pipeline for Common Crawl"** and a classifier trained to identify documents similar to Wikipedia articles or references. SlimPajama with 627B tokens **Kaplan et al., "SlimPajama: Refined RedPajama Dataset"** further refines RedPajama by removing short documents and performing additional fuzzy dedupllication. RedPajama-V2  **Kaplan et al., "RedPajama V2: A Large-Scale Dataset for LLM Pre-Training"** with 30 trillion tokens is entirely based on the Common Crawl and contains annotations without applying any filtering. These annotations cover filtering techniques from CCNet, C4, and others, and also labels identifying deduplicates using exact and fuzzy deduplication. 

RefinedWeb dataset **Kaplan et al., "RefinedWeb: A Large-Scale Dataset for LLM Pre-Training"** is a Common Crawl-based dataset, using trafilatura  **Kaplan et al., "Trafilatura: Webpage Extraction in the Era of Big Data"** for text extraction, fastText-based language identification **Bojanowski et al., "Enriching Word Vectors with Subword Information"**, heuristic rules for quality filtering, and fuzzy and exact deduplication. Dolma  **Kaplan et al., "Dolma: A Large-Scale Dataset for LLM Pre-Training"** is a 3 trillion token composite dataset with a Common Crawl-based portion, which employs fastText for language identification, primarily uses heuristic rules from MassiveWeb  **Kaplan et al., "MassiveWeb: Webpage Processing in the Era of Big Data"** for quality filtering, applies toxicity filtering based on rules and classifiers and performs deduplication at URL, document and paragraph levels. 

More recent datasets include FineWeb datasets **Hugging Face, "FineWeb: A New Large-Scale Dataset for LLM Pre-Training"**, DCLM-Baseline **Kaplan et al., "DCLM-Baseline: A High-Quality Dataset for LLM Pre-Training"**, and TxT360  **LLM360 Team, "TxT360: A Large-Scale Dataset for LLM Pre-Training"**. FineWeb consists of 15T tokens derived from the Common Crawl by applying a series of processing steps, mainly including language classification, fuzzy deduplication at snapshot level and heuristic rule-based quality filters. 
% It has been shown in **Hugging Face, "FineWeb: A New Large-Scale Dataset for LLM Pre-Training"** that models trained on  FineWeb outperform those trained on several public datasets — C4, RefinedWeb, Dolma, RedPajamaV, SlimPajama and the Pile. 
Subsequently, two smaller but higher quality versions called FineWeb-Edu (1.3 trillion tokens) and FineWeb-Edu-Score2 (5.4 trillion tokens) derived from FineWeb were released **Hugging Face, "FineWeb: A New Large-Scale Dataset for LLM Pre-Training"**. These smaller high quality derivatives of FineWeb are created by retaining documents perceived to have higher educational value from FineWeb. See Appendix \ref{app:fineweb} for more details on FineWeb.

DCLM-Baseline is obtained from the Common Crawl snapshots by using resiliparse  **Kaplan et al., "Resilparse: Webpage Processing in the Era of Big Data"** for text extraction, and a language identification as well as language model based perplexity.score

%Example Reference __\\
%Data quality is vital for the performance of language models. High-quality data significantly boosts LLMs’ ability to generalize and cuts costs by reducing the need for retraining. This underscores the critical role of quality filtering and data deduplication in enhancing data quality, directly impacting LLM performance.