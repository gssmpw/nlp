\section{Background and Related Work}
In this section, we first provide an overview of the existing variants of softmax attention, including the doubly-stochastic attention using Sinkhorn's algorithm \cite{sander2022Sinkformers}. Then, we shift our focus to the fundamentals of sliced optimal transport, with soft sorting reviewed for algorithmic concerns. Finally, we provide an overview of Expected Sliced Transport Plans~\cite{liu2025expected} and soft sorting~\cite{prillo2020softsort}, which serve as the cornerstones of our proposed doubly-stochastic attention mechanism, ESPFormer.




\subsection{Variants of Softmax Attention}
\label{sec:attention}
At the heart of the Transformer architecture is the self-attention operation, a crucial component that enables dynamic pairwise interactions among tokens. In essence, it allows each position to `attend'' to all others, with the degree of attention determined by how similar their representations are. Formally, let $W_Q, W_K\in \mathbb{R}^{m\times d}, W_V\in\mathbb{R}^{d\times d}$ denote the query, key, and value matrices, respectively. Then, for a sequence $(x_1, x_2, \cdots, x_N), x_i \in \mathbb{R}^d, \forall i$, the output of the attention function for the $i$-th row, $x_i$, can be written as
\begin{align}
    \label{eq: generalized normalization}
    \frac{\sum_{j=1}^N\text{sim}(W_Qx_i, W_Kx_j)W_Vx_j}{\sum_{j=1}^N\text{sim}(W_Qx_i, W_Kx_j)}
\end{align}
where $\text{sim}(\cdot, \cdot)$ can be any similarity function, and the normalization for the similarities is applied row-wise. The classic self-attention mechanism~\cite{vaswani2017attention} leverages the softmax function to perform this row-wise normalization, i.e., the softmax of attention matrix $C$ with $C_{i,j}=(W_Qx_i)^TW_Kx_j$ can be interpreted as row-wise normalization of $\exp(C)$. 
% However, the quadratic time complexity and considerable memory demands of attention pose a challenge to applications on large-scale data in vision and language tasks. Various improvements have been proposed to address these computational constraints. 
Alternative normalization operators have also been proposed in the literature. 
% Examples include the SparseMax operator \cite{Martins2016FromST} and the SparseK operator \cite{lou2024sparser} in sparse attentions that put constraints in the feasible domain of the argmax operator to enforce sparsity. 
Some focus on normalizations that share the same properties as softmax but produce sparse outputs, such as SparseMax~\cite{Martins2016FromST} and SparseK~\cite{lou2024sparser}. SparseMax seeks the Euclidean projection of the input into the probabilistic simplex. Since this projection tends to hit the boundary of the simplex, SparseMax will output sparse probabilities. SparseK extends SparseMax by replacing the probabilistic simplex with a $k$-sum constraint. Others use different $\text{sim}(\cdot, \cdot)$ functions in \eqref{eq: generalized normalization} to achieve linear complexity: \citet{katharopoulos2020transformers} and \citet{han2023flatten} use kernel smoothers to linearize the similarity calculations, and \citet{li2020linear} propose to approximate the exponential function in softmax by the first-order Taylor expansion. 
% More recently, \citet{han2025agent} generalized self-attention by employing a small group of agent tokens into the traditional attention module, enabling these agents to first aggregate information from key-value pairs and then broadcast it back to the queries. Nonetheless, all these variants result in row-stochastic attention matrices.
% \eva{I added the references below and removed some previous parts as this subsection is getting too long. Please do another pass at this subsection. Thanks!}

Another line of research approaches the attention from the alignment/matching perspective and specifically utilizes concepts from the Optimal Transport (OT) theory. \citet{zhang2021alignment} propose to enforce the alignment between the distributions of keys and queries by adding a penalty term in the training objective, where the Jensenâ€“Shannon (JS) divergence, the Wasserstein distance, and bi-directional conditional transport \cite{zheng2021comparing} are considered to define the matching cost. \citet{xu2023multimodal} introduce a multimodal co-attention method that relies on OT to match patches of images and gene embeddings for the survival prediction task. \citet{zhang2023unlocking} reinterpret slot attention within the OT framework and propose to enhance the performance of slot attention by minimizing the entropy of the Sinkhorn divergence between two multisets, one containing inputs and the other containing slot features.

\subsection{Doubly-Stochastic Attention}

In their pioneering work, \citet{sander2022Sinkformers} empirically observed that, during training, the row-stochastic attention matrices in classical Transformers tend to become approximately doubly-stochastic, with most column sums approaching 1. This suggests that Transformers inherently learn to distribute attention more evenly across tokens. In light of this finding, the authors propose the Sinkformer architecture, which replaces the softmax operation by the Sinkhorn's algorithm \cite{sinkhorn1964relationship, cuturi2013sinkhorn, peyre2019computational} to enforce doubly-stochastic attention as an informative prior. They show that although both classic Transformers and Sinkformers can be understood as models that operate on discrete probability distributions, the Sinkformers have a special property that, in the infinite depth limit, they behave as a Wasserstein gradient flow for an energy minimization.

\subsection{Sliced Optimal Transport}
Consider the space of Borel probability measures on $\Omega\subset\mathbb{R}^d$ with finite $p$-th moment $(p\ge 1)$, denoted by $\mathcal{P}_p(\Omega)$. For $\mu^1, \mu^2\in \mathcal{P}_p(\Omega)$, the classic optimal transport (in the Kantorovich formulation) solves the optimization problem
\begin{align}
    \inf_{\gamma\in\Gamma(\mu_1, \mu_2)}\int_{\Omega^2}c(x, y)\text{d}\gamma(x, y),
\end{align}
where $\Gamma(\mu^1, \mu^2)$ denotes the set of all couplings between $\mu^1$ and $\mu^2$, and $c:\Omega^2\rightarrow\mathbb{R}_+$ is a lower semi-continuous function. Specifically, when $c(\cdot, \cdot)$ is the $p$-th power of a metric, the $p$-Wasserstein distance is defined as
\vspace{-.1in}
\begin{align}
    W_p(\mu^1, \mu^2) \coloneqq \min_{\gamma\in\Gamma(\mu^1, \mu^2)}\left(\int_{\Omega^2}\|x-y\|^p\text{d}\gamma(x, y)\right)^{\frac{1}{p}}.
\end{align}
In practical settings, the Wasserstein distance between discrete probability measures can be obtained by solving a linear program \cite{peyre2019computational} with an expensive computational complexity of $\mathcal{O}(N^3\log N)$ for sample size $N$. Therefore, faster alternatives have been extensively studied, including entropy-regularized optimal transport \cite{cuturi2013sinkhorn} and sliced optimal transport \cite{kolouri2016sliced, kolouri2018sliced, deshpande2019max, kolouri2019generalized, nguyen2022hierarchical, nguyen2024energy}. Sinkhorn's algorithm at the core of Sinkformer is used to solve the entropy-regularized optimal transport problem with an improved complexity of $\mathcal{O}(N^2\log N)$.

Sliced optimal transport operates by projecting high-dimensional distributions onto one-dimensional slices, leveraging the key property that, in the one-dimensional case, a unique optimal transport plan exists, and the \( p \)-Wasserstein distance has a closed-form solution \cite{rabin2012wasserstein},
\begin{align}
    W_p(\mu^1, \mu^2)=\int_0^1 \left|F^{-1}_{\mu^1}(u)-F^{-1}_{\mu^2}(u)\right|^p\text{d}u,
\end{align}
where $F^{-1}_{\mu^1}$ and $F^{-1}_{\mu^2}$ are the quantile functions of $\mu^1$ and $\mu^2$ respectively, and the optimal transport map is given by $T(x)=F^{-1}_{\mu^2} (F_{\mu^1})$ when $\mu^1$ is continuous. For empirical measures with $N$ samples, the quantile functions correspond to the sorted samples that can be calculated in $\mathcal{O}(N\log N)$. Then, by integrating the 1-dimensional Wasserstein distance over a set of $L$ slices, the sliced Wasserstein distance reduces the computational cost significantly to $\mathcal{O}(LN\log N)$.

\subsection{Expected Sliced Transport Plans}
Although the sliced Wasserstein distance offers a rapid and well-defined metric, it has one limitation: it does not generate a transport plan between the probability measures. It thus fails to explicitly provide how one distribution could be transported into another. 

\citet{liu2025expected} recently proposed the Expected Sliced Transport Plan (ESP), which defines an efficient transport plan as an aggregation of lifted optimal transport plans on 1-dimensional slices. Let $\mu^1=\sum_{x\in\mathbb{R}^d}p(x)\delta_x$ be a discrete probability measure in $\mathcal{P}(\mathbb{R}^d)$, that is, $p(x)\geq 0$ for all $x\in\mathbb{R}^d$ and $\sum_{x\in\mathbb{R}^d}p(x)=1$. We further assume that $p(x)\neq 0$ for at most countable many points $x\in\mathbb{R}^d$. Similarly, let $\mu^2=\sum_{y\in\mathbb{R}^d}q(y)\delta_y$ be another probability measure in $\mathcal{P}(\mathbb{R}^d)$ with at most countable support. For a given $\theta\in\mathbb{S}^{d-1}$, the projected measures $\theta_{\#}\mu^1$ and $\theta_{\#}\mu^2$ are 1-dimensional probability measures in $\mathcal{P}(\mathbb{R})$, and there exists a unique optimal transport plan $\Lambda_\theta^{\mu^1, \mu^2}$ between them. Equivalently, $\theta_{\#}\mu^1$ and $\theta_{\#}\mu^2$ can be interpreted as probability measures over a quotient space $\mathbb{R}^d/\sim_\theta$, where $\sim_\theta$ is defined as follows:
\vspace{-.05in}
\begin{align*}
    x\sim_\theta x'\quad\text{if and only if}\quad\theta\cdot x=\theta \cdot x',
\end{align*}
as each point on the slice $\mathbb{R}$ corresponds to an equivalent class of points in $\mathbb{R}^d$ that gets mapped to it by $\theta$. With a slight abuse of notation, we denote the equivalent class of $x\in\mathbb{R}^d$ by $\bar{x}^\theta$, referring to either a point in the quotient space $\mathbb{R}^d/\sim_\theta$ or the set of points $\{x'\in\mathbb{R}^d:\theta\cdot x'=\theta\cdot x\}$ interchangeably. Then, we can write $\theta_\# \mu^1=\sum_{\bar{x}^\theta\in\mathbb{R}/\sim_\theta}P(\bar{x}^\theta)\delta_{\bar{x}^\theta}$, where $P(\bar{x}^\theta)=\sum_{x'\in\bar{x}^\theta}p(x')$, and $\theta_\# \mu^2=\sum_{\bar{y}^\theta\in\mathbb{R}/\sim_\theta}Q(\bar{y}^\theta)\delta_{\bar{y}^\theta}$, where $Q(\bar{y}^\theta)=\sum_{y'\in\bar{y}^\theta}q(y')$. 

This quotient space interpretation of the 1-dimensional distributions $\theta_\#\mu^1$ and $\theta_\#\mu^2$ allows us to construct a lifted transport plan in the original space $\mathbb{R}^d$ using the optimal transport plan $\Lambda_\theta^{\mu^1, \mu^2}$,
\begin{align}
\label{eq: gamma_theta}
    \gamma_\theta^{\mu^1, \mu^2} \coloneqq \sum_{x\in\mathbb{R}^d}\sum_{y\in\mathbb{R}^d}u_\theta^{\mu^1, \mu^2}(x, y)\delta(x, y),
\end{align}
where the transported mass $u_\theta^{\mu^1, \mu^2}$ is defined as
% \begin{align}
%     \label{eq: lifted theta}
%     \nonumber u_\theta^{\mu^1, \mu^2}(x, y) \coloneqq \begin{cases}
%     \frac{p(x)q(y)}{P(\bar{x}^\theta)Q(\bar{y}^\theta)}\Lambda_\theta^{\mu^1, \mu^2}(\{(\bar{x}^\theta, \bar{y}^\theta)\}), \\
%     \hspace{2cm}\text{ if $p(x)\neq0$ and $q(y)\neq 0$;}\\
%     0, \hspace{1.8cm} \text{otherwise.}
%   \end{cases}
% \end{align}
\vspace{-.1in}
\begin{align}
    \label{eq: lifted theta}
    \nonumber u_\theta^{\mu^1, \mu^2}(x, y) \coloneqq
    \frac{p(x)q(y)}{P(\bar{x}^\theta)Q(\bar{y}^\theta)}\Lambda_\theta^{\mu^1, \mu^2}(\{(\bar{x}^\theta, \bar{y}^\theta)\}).
\end{align}


Then for a given distribution of slicing directions ($\theta$'s): $\sigma\in\mathcal{P}(\mathbb{S}^{d-1})$, the ESP $\bar{\gamma}^{\mu^1, \mu^2}\in\Gamma(\mu^1, \mu^2)$ is defined as an expectation of $\gamma_\theta^{\mu^1, \mu^2}$ over $\sigma$:
\begin{align}
    &\bar{\gamma}^{\mu^1, \mu^2} \coloneqq \mathbb{E}_{\theta\sim \sigma}[\gamma_\theta^{\mu^1, \mu^2}]\quad \\
    &\nonumber\text{i.e. }\bar{\gamma}^{\mu^1, \mu^2}(\{(x, y)\})=\int_{\mathbb{S}^{d-1}}\gamma_\theta^{\mu^1, \mu^2}(\{(x, y)\})\text{d}\sigma(\theta).
\end{align}
\citet{liu2025expected} have shown that the associated cost, 
\begin{align*}
\mathcal{D}_p(\mu^1, \mu^2)=\left(\sum_{x\in\mathbb{R}^d}\sum_{y\in\mathbb{R}^d}\|x-y\|^p\bar{\gamma}^{\mu^1, \mu^2}(\{(x, y)\})\right)^\frac{1}{p},
\end{align*}
is a well-defined distance and equivalent to the Wasserstein distance.

\subsection{Soft Sorting}
Calculating the sliced Wasserstein distance involves evaluating the quantile functions of the distributions, which, in the discrete case, can be boiled down to the sorting operation. Sorting is one of the most common operations in computer science. Yet, the piecewise-linear sorted value function and the integer-valued rank/argsort operators pose a significant obstacle for gradient-based optimization techniques, which are essential in deep learning, as neither of them is differentiable. To incorporate sorting operations into the backpropagation framework, differentiable approximations, known as soft sorting, have been explored. Examples include smoothed rank operators by adding Gaussian noise \cite{taylor2008softrank} and by using sigmoid surrogate functions \cite{qin2010general}, parameterizing permutations in terms of a differentiable relaxation 
 \cite{mena2018learning}, and relaxing the permutation matrices to be only row-wise stochastic \cite{grover2019stochastic}.  Of note, \citet{NEURIPS2019_d8c24ca8} propose a differentiable proxy by viewing sorting as an optimal assignment problem and relaxing it to an optimal transport problem from the input values to an auxiliary probability measure supported on an increasing family of target values. 

\citet{prillo2020softsort} introduce a simple yet highly effective continuous relaxation of the argsort operator using the softmax function: Given a vector $v\in\mathbb{R}^N$,
\begin{align}
    \text{SoftSort}_t^d(v) \coloneqq \text{softmax}\left(\frac{-d(\text{sort}(v)\mathds{1}^T, \mathds{1}v^T)}{t}\right),
    \label{eq:softsort}
\end{align}
where the softmax operator is applied row-wise, $d(\cdot, \cdot)$ can be any differentiable semi-metric, and $t$ is a temperature parameter that controls the degree of the approximation. 

\begin{figure*}[t!]
    \centering
    \includegraphics[trim=0in 0in .9in .4in, clip, width=0.99\textwidth]{figures/ESPFormer2.pdf}
    \vspace{-.15in}
    \caption{An overview of the proposed ESP attention mechanism. By integrating the slicing operator into the key and query matrices, each dimension is treated as a learnable slice. For each slice, tokens are (soft) sorted, and a doubly-stochastic correspondence matrix is computed between the keys and queries. Finally, these correspondence matrices across all dimensions are aggregated to form a single doubly-stochastic attention matrix.}
    \label{fig:teaser}
    \vspace{-.2in}
\end{figure*}