\documentclass{article}
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{amsmath, amsfonts}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{dsfont}
\usepackage{microtype}
\usepackage{subfigure}
\usepackage{listings}
\usepackage[numbers]{natbib}  % Make sure this file exists

\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{multirow, multicol}
\usepackage{hhline}
\lstset{
    language=Python,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{gray}\itshape,
    stringstyle=\color{red},
    breaklines=true,
    frame=single
}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\usepackage[textsize=tiny]{todonotes}
\usepackage[capitalize,noabbrev]{cleveref}

\title{ESPFormer: Doubly-Stochastic Attention with Expected Sliced Transport Plans}


\author{
    Ashkan Shahbazi \\
    Department of Computer Science\\
    Vanderbilt University\\
    Nashville, TN 37212 \\
    \texttt{ashkan.shahbazi@vanderbilt.edu}
    \And
    Elaheh Akbari \\
    Department of Computer Science\\
    Vanderbilt University\\
    Nashville, TN 37212 \\
    \texttt{elaheh.akbari@vanderbilt.edu}
    \And
    Darian Salehi \\
    Department of Computer Science\\
    Duke University\\
    Durham, NC 27708 \\
    \texttt{darian.salehi@duke.edu}
    \And
    Xinran Liu \\
    Department of Computer Science\\
    Vanderbilt University\\
    Nashville, TN 37212 \\
    \texttt{xinran.liu@vanderbilt.edu}
    \And
    Navid Naderializadeh \\
    Department of Biostatistics and Bioinformatics\\
    Duke University\\
    Durham, NC 27708 \\
    \texttt{navid.naderi@duke.edu}
    \And
    Soheil Kolouri \\
    Department of Computer Science\\
    Vanderbilt University\\
    Nashville, TN 37212 \\
    \texttt{soheil.kolouri@vanderbilt.edu}
}


\begin{document}
\maketitle
\begin{abstract}
While self-attention has been instrumental in the success of Transformers, it can lead to over-concentration on a few tokens during training, resulting in suboptimal information flow. Enforcing doubly-stochastic constraints in attention matrices has been shown to improve structure and balance in attention distributions. However, existing methods rely on iterative Sinkhorn normalization, which is computationally costly. In this paper, we introduce a novel, fully parallelizable doubly-stochastic attention mechanism based on sliced optimal transport, leveraging Expected Sliced Transport Plans (ESP). Unlike prior approaches, our method enforces double stochasticity without iterative Sinkhorn normalization, significantly enhancing efficiency. To ensure differentiability, we incorporate a temperature-based soft sorting technique, enabling seamless integration into deep learning models. Experiments across multiple benchmark datasets, including image classification, point cloud classification, sentiment analysis, and neural machine translation, demonstrate that our enhanced attention regularization consistently improves performance across diverse applications.

\end{abstract}


\section{Introduction}
\label{intro}
The debut of Transformers \cite{vaswani2017attention} marked a turning point in artificial intelligence and machine learning. Self-attention mechanisms excel at modeling the interactions among features, allowing Transformers to generate highly expressive and context-rich representations that accurately capture the essence of input data~\cite{khan2022transformers}. Although first developed to perform Natural Language Processing (NLP) tasks, the Transformer architecture has been adapted to a wide range of domains such as computer vision \cite{dosovitskiy2020vit, pmlr-v139-touvron21a, Touvron_2021_ICCV, Touvron2022DeiTIR}, graphs \cite{yun2019graph,rampavsek2022recipe, shirzad2023exphormer}, point clouds \cite{zhao2021point, qin2022geometric}, and biological sequences~\cite{jumper2021highly,abramson2024accurate,rives2021biological,lin2023evolutionary,hayes2025simulating}. Over the years, a considerable amount of research has been devoted to enhancing the classic Transformer, focusing, among others, on better positional encoding \cite{dwivedi2021graph, ying2021transformers}, efficiency of the attention mechanisms \cite{wang2020linformer,choromanski2020rethinking}, and variants of self-attention \cite{hou2019cross, ho2019axial}. 

Self-attention produces a row-stochastic matrix, which can lead to a few tokens dominating the attention distribution. To mitigate this, enforcing doubly-stochastic attention ensures a more balanced distribution across tokens. To that end, \citet{sander2022Sinkformers} introduced Sinkformer, replacing the softmax normalization in the classic Transformer with Sinkhorn's algorithm \cite{sinkhorn1964relationship}, resulting in a doubly-stochastic attention matrix. %While the initial motivation stems from the empirical observation that row-wise stochastic matrices normalized by the Softmax operator tend to evolve into doubly-stochastic matrices during training, 
\citet{sander2022Sinkformers} establish a connection between self-attention matrices and the optimal transport problem by theoretically demonstrating that Sinkformers can be interpreted as a Wasserstein gradient flow for an energy minimization in the infinite depth limit. In fact, there exists a fundamental compatibility between transport plans and Transformer architectures, rooted in their shared property of permutation-equivariance. In particular, due to their inherent doubly-stochastic nature, transport plans are good candidates for doubly-stochastic attention mechanisms.



Nevertheless, computing optimal transport plans between keys and queries is computationally expensive, with a complexity of \(\mathcal{O}(N^3)\) for \(N\) tokens. A more scalable approach is entropy-regularized transport \cite{cuturi2013sinkhorn}, which leverages the Sinkhorn algorithm to iteratively approximate the transport plan, obtaining a complexity $\mathcal{O}(SN^2)$, where $S$ denotes the number of iterations.
The Sinkhorn algorithm \cite{sinkhorn1964relationship}, as used in Sinkformers \cite{sander2022Sinkformers}, repeatedly applies a series of row and column normalization steps until it reaches a desired level of convergence. Such an approach may introduce computational inefficiencies in scenarios where a significant number of iterations are needed for the normalization to converge.

A more computationally efficient alternative is the calculation of optimal transport plans for one-dimensional distributions, with a complexity of $\mathcal{O}(N\log N)$. This has motivated a large body of work on \emph{sliced} optimal transport methods that compare distributions by comparing their slices, i.e., one-dimensional marginals~\cite{rabin2012wasserstein,kolouri2016sliced, kolouri2018sliced, kolouri2019generalized, deshpande2019max, nguyen2022hierarchical, nguyen2024energy}. Although sliced optimal transport methods offer efficient metrics between distributions, they do not explicitly construct transport plans. Recent studies have addressed this limitation by developing methods to construct transport plans through slicing  \cite{mahey2024fast,liu2025expected}. 
\citet{liu2025expected} introduce the Expected Sliced Transport Plan (ESP), which leverages slicing to define a computationally efficient metric while crucially providing an explicit transport plan by aggregating lifted plans from all slices. This makes ESP a strong candidate for doubly-stochastic attention mechanisms.


% Figure \ref{fig:plan comparison} shows the attention weights among keys and queries obtained by Expected Sliced Transport Plans in comparison to the Sinkhorn's approach in the Sinkformers.



In this work, we leverage the ESP framework to propose a novel doubly-stochastic attention mechanism. We refer to the resulting architecture as ESPFormer. Our specific contributions are as follows:
\begin{itemize}
    \item We propose ESPFormer, a novel doubly stochastic attention mechanism built on the recently introduced Expected Sliced Transport Plan (ESP) framework. ESPFormer ensures a more balanced distribution of attention across tokens while enabling control over the number of tokens each token attends to via an inverse temperature parameter.
    \item Through extensive experiments across diverse applications, we demonstrate performance improvements over both classic Transformer and Sinkformer architectures, along with enhanced computational efficiency compared to Sinkformer.
    \item We show that replacing the classic attention mechanism in a pre-trained Transformer with ESPFormer and fine-tuning for a few epochs results in significant performance gains.
    \item We demonstrate the compatibility of our proposed attention mechanism with the recently introduced differential attention architecture \cite{Ye2025differential}.
\end{itemize}



\section{Background and Related Work}
In this section, we first provide an overview of the existing variants of softmax attention, including the doubly-stochastic attention using Sinkhorn's algorithm \cite{sander2022Sinkformers}. Then, we shift our focus to the fundamentals of sliced optimal transport, with soft sorting reviewed for algorithmic concerns. Finally, we provide an overview of Expected Sliced Transport Plans~\cite{liu2025expected} and soft sorting~\cite{prillo2020softsort}, which serve as the cornerstones of our proposed doubly-stochastic attention mechanism, ESPFormer.




\subsection{Variants of Softmax Attention}
\label{sec:attention}
At the heart of the Transformer architecture is the self-attention operation, a crucial component that enables dynamic pairwise interactions among tokens. In essence, it allows each position to `attend'' to all others, with the degree of attention determined by how similar their representations are. Formally, let $W_Q, W_K\in \mathbb{R}^{m\times d}, W_V\in\mathbb{R}^{d\times d}$ denote the query, key, and value matrices, respectively. Then, for a sequence $(x_1, x_2, \cdots, x_N), x_i \in \mathbb{R}^d, \forall i$, the output of the attention function for the $i$-th row, $x_i$, can be written as
\begin{align}
    \label{eq: generalized normalization}
    \frac{\sum_{j=1}^N\text{sim}(W_Qx_i, W_Kx_j)W_Vx_j}{\sum_{j=1}^N\text{sim}(W_Qx_i, W_Kx_j)}
\end{align}
where $\text{sim}(\cdot, \cdot)$ can be any similarity function, and the normalization for the similarities is applied row-wise. The classic self-attention mechanism~\cite{vaswani2017attention} leverages the softmax function to perform this row-wise normalization, i.e., the softmax of attention matrix $C$ with $C_{i,j}=(W_Qx_i)^TW_Kx_j$ can be interpreted as row-wise normalization of $\exp(C)$. 
% However, the quadratic time complexity and considerable memory demands of attention pose a challenge to applications on large-scale data in vision and language tasks. Various improvements have been proposed to address these computational constraints. 
Alternative normalization operators have also been proposed in the literature. 
% Examples include the SparseMax operator \cite{Martins2016FromST} and the SparseK operator \cite{lou2024sparser} in sparse attentions that put constraints in the feasible domain of the argmax operator to enforce sparsity. 
Some focus on normalizations that share the same properties as softmax but produce sparse outputs, such as SparseMax~\cite{Martins2016FromST} and SparseK~\cite{lou2024sparser}. SparseMax seeks the Euclidean projection of the input into the probabilistic simplex. Since this projection tends to hit the boundary of the simplex, SparseMax will output sparse probabilities. SparseK extends SparseMax by replacing the probabilistic simplex with a $k$-sum constraint. Others use different $\text{sim}(\cdot, \cdot)$ functions in \eqref{eq: generalized normalization} to achieve linear complexity: \citet{katharopoulos2020transformers} and \citet{han2023flatten} use kernel smoothers to linearize the similarity calculations, and \citet{li2020linear} propose to approximate the exponential function in softmax by the first-order Taylor expansion. 
% More recently, \citet{han2025agent} generalized self-attention by employing a small group of agent tokens into the traditional attention module, enabling these agents to first aggregate information from key-value pairs and then broadcast it back to the queries. Nonetheless, all these variants result in row-stochastic attention matrices.
% \eva{I added the references below and removed some previous parts as this subsection is getting too long. Please do another pass at this subsection. Thanks!}

Another line of research approaches the attention from the alignment/matching perspective and specifically utilizes concepts from the Optimal Transport (OT) theory. \citet{zhang2021alignment} propose to enforce the alignment between the distributions of keys and queries by adding a penalty term in the training objective, where the Jensen–Shannon (JS) divergence, the Wasserstein distance, and bi-directional conditional transport \cite{zheng2021comparing} are considered to define the matching cost. \citet{xu2023multimodal} introduce a multimodal co-attention method that relies on OT to match patches of images and gene embeddings for the survival prediction task. \citet{zhang2023unlocking} reinterpret slot attention within the OT framework and propose to enhance the performance of slot attention by minimizing the entropy of the Sinkhorn divergence between two multisets, one containing inputs and the other containing slot features.

\subsection{Doubly-Stochastic Attention}

In their pioneering work, \citet{sander2022Sinkformers} empirically observed that, during training, the row-stochastic attention matrices in classical Transformers tend to become approximately doubly-stochastic, with most column sums approaching 1. This suggests that Transformers inherently learn to distribute attention more evenly across tokens. In light of this finding, the authors propose the Sinkformer architecture, which replaces the softmax operation by the Sinkhorn's algorithm \cite{sinkhorn1964relationship, cuturi2013sinkhorn, peyre2019computational} to enforce doubly-stochastic attention as an informative prior. They show that although both classic Transformers and Sinkformers can be understood as models that operate on discrete probability distributions, the Sinkformers have a special property that, in the infinite depth limit, they behave as a Wasserstein gradient flow for an energy minimization.

\subsection{Sliced Optimal Transport}
Consider the space of Borel probability measures on $\Omega\subset\mathbb{R}^d$ with finite $p$-th moment $(p\ge 1)$, denoted by $\mathcal{P}_p(\Omega)$. For $\mu^1, \mu^2\in \mathcal{P}_p(\Omega)$, the classic optimal transport (in the Kantorovich formulation) solves the optimization problem
\begin{align}
    \inf_{\gamma\in\Gamma(\mu_1, \mu_2)}\int_{\Omega^2}c(x, y)\text{d}\gamma(x, y),
\end{align}
where $\Gamma(\mu^1, \mu^2)$ denotes the set of all couplings between $\mu^1$ and $\mu^2$, and $c:\Omega^2\rightarrow\mathbb{R}_+$ is a lower semi-continuous function. Specifically, when $c(\cdot, \cdot)$ is the $p$-th power of a metric, the $p$-Wasserstein distance is defined as
\vspace{-.1in}
\begin{align}
    W_p(\mu^1, \mu^2) \coloneqq \min_{\gamma\in\Gamma(\mu^1, \mu^2)}\left(\int_{\Omega^2}\|x-y\|^p\text{d}\gamma(x, y)\right)^{\frac{1}{p}}.
\end{align}
In practical settings, the Wasserstein distance between discrete probability measures can be obtained by solving a linear program \cite{peyre2019computational} with an expensive computational complexity of $\mathcal{O}(N^3\log N)$ for sample size $N$. Therefore, faster alternatives have been extensively studied, including entropy-regularized optimal transport \cite{cuturi2013sinkhorn} and sliced optimal transport \cite{kolouri2016sliced, kolouri2018sliced, deshpande2019max, kolouri2019generalized, nguyen2022hierarchical, nguyen2024energy}. Sinkhorn's algorithm at the core of Sinkformer is used to solve the entropy-regularized optimal transport problem with an improved complexity of $\mathcal{O}(N^2\log N)$.

Sliced optimal transport operates by projecting high-dimensional distributions onto one-dimensional slices, leveraging the key property that, in the one-dimensional case, a unique optimal transport plan exists, and the \( p \)-Wasserstein distance has a closed-form solution \cite{rabin2012wasserstein},
\begin{align}
    W_p(\mu^1, \mu^2)=\int_0^1 \left|F^{-1}_{\mu^1}(u)-F^{-1}_{\mu^2}(u)\right|^p\text{d}u,
\end{align}
where $F^{-1}_{\mu^1}$ and $F^{-1}_{\mu^2}$ are the quantile functions of $\mu^1$ and $\mu^2$ respectively, and the optimal transport map is given by $T(x)=F^{-1}_{\mu^2} (F_{\mu^1})$ when $\mu^1$ is continuous. For empirical measures with $N$ samples, the quantile functions correspond to the sorted samples that can be calculated in $\mathcal{O}(N\log N)$. Then, by integrating the 1-dimensional Wasserstein distance over a set of $L$ slices, the sliced Wasserstein distance reduces the computational cost significantly to $\mathcal{O}(LN\log N)$.

\subsection{Expected Sliced Transport Plans}
Although the sliced Wasserstein distance offers a rapid and well-defined metric, it has one limitation: it does not generate a transport plan between the probability measures. It thus fails to explicitly provide how one distribution could be transported into another. 

\citet{liu2025expected} recently proposed the Expected Sliced Transport Plan (ESP), which defines an efficient transport plan as an aggregation of lifted optimal transport plans on 1-dimensional slices. Let $\mu^1=\sum_{x\in\mathbb{R}^d}p(x)\delta_x$ be a discrete probability measure in $\mathcal{P}(\mathbb{R}^d)$, that is, $p(x)\geq 0$ for all $x\in\mathbb{R}^d$ and $\sum_{x\in\mathbb{R}^d}p(x)=1$. We further assume that $p(x)\neq 0$ for at most countable many points $x\in\mathbb{R}^d$. Similarly, let $\mu^2=\sum_{y\in\mathbb{R}^d}q(y)\delta_y$ be another probability measure in $\mathcal{P}(\mathbb{R}^d)$ with at most countable support. For a given $\theta\in\mathbb{S}^{d-1}$, the projected measures $\theta_{\#}\mu^1$ and $\theta_{\#}\mu^2$ are 1-dimensional probability measures in $\mathcal{P}(\mathbb{R})$, and there exists a unique optimal transport plan $\Lambda_\theta^{\mu^1, \mu^2}$ between them. Equivalently, $\theta_{\#}\mu^1$ and $\theta_{\#}\mu^2$ can be interpreted as probability measures over a quotient space $\mathbb{R}^d/\sim_\theta$, where $\sim_\theta$ is defined as follows:
\vspace{-.05in}
\begin{align*}
    x\sim_\theta x'\quad\text{if and only if}\quad\theta\cdot x=\theta \cdot x',
\end{align*}
as each point on the slice $\mathbb{R}$ corresponds to an equivalent class of points in $\mathbb{R}^d$ that gets mapped to it by $\theta$. With a slight abuse of notation, we denote the equivalent class of $x\in\mathbb{R}^d$ by $\bar{x}^\theta$, referring to either a point in the quotient space $\mathbb{R}^d/\sim_\theta$ or the set of points $\{x'\in\mathbb{R}^d:\theta\cdot x'=\theta\cdot x\}$ interchangeably. Then, we can write $\theta_\# \mu^1=\sum_{\bar{x}^\theta\in\mathbb{R}/\sim_\theta}P(\bar{x}^\theta)\delta_{\bar{x}^\theta}$, where $P(\bar{x}^\theta)=\sum_{x'\in\bar{x}^\theta}p(x')$, and $\theta_\# \mu^2=\sum_{\bar{y}^\theta\in\mathbb{R}/\sim_\theta}Q(\bar{y}^\theta)\delta_{\bar{y}^\theta}$, where $Q(\bar{y}^\theta)=\sum_{y'\in\bar{y}^\theta}q(y')$. 

This quotient space interpretation of the 1-dimensional distributions $\theta_\#\mu^1$ and $\theta_\#\mu^2$ allows us to construct a lifted transport plan in the original space $\mathbb{R}^d$ using the optimal transport plan $\Lambda_\theta^{\mu^1, \mu^2}$,
\begin{align}
\label{eq: gamma_theta}
    \gamma_\theta^{\mu^1, \mu^2} \coloneqq \sum_{x\in\mathbb{R}^d}\sum_{y\in\mathbb{R}^d}u_\theta^{\mu^1, \mu^2}(x, y)\delta(x, y),
\end{align}
where the transported mass $u_\theta^{\mu^1, \mu^2}$ is defined as
% \begin{align}
%     \label{eq: lifted theta}
%     \nonumber u_\theta^{\mu^1, \mu^2}(x, y) \coloneqq \begin{cases}
%     \frac{p(x)q(y)}{P(\bar{x}^\theta)Q(\bar{y}^\theta)}\Lambda_\theta^{\mu^1, \mu^2}(\{(\bar{x}^\theta, \bar{y}^\theta)\}), \\
%     \hspace{2cm}\text{ if $p(x)\neq0$ and $q(y)\neq 0$;}\\
%     0, \hspace{1.8cm} \text{otherwise.}
%   \end{cases}
% \end{align}
\vspace{-.1in}
\begin{align}
    \label{eq: lifted theta}
    \nonumber u_\theta^{\mu^1, \mu^2}(x, y) \coloneqq
    \frac{p(x)q(y)}{P(\bar{x}^\theta)Q(\bar{y}^\theta)}\Lambda_\theta^{\mu^1, \mu^2}(\{(\bar{x}^\theta, \bar{y}^\theta)\}).
\end{align}


Then for a given distribution of slicing directions ($\theta$'s): $\sigma\in\mathcal{P}(\mathbb{S}^{d-1})$, the ESP $\bar{\gamma}^{\mu^1, \mu^2}\in\Gamma(\mu^1, \mu^2)$ is defined as an expectation of $\gamma_\theta^{\mu^1, \mu^2}$ over $\sigma$:
\begin{align}
    &\bar{\gamma}^{\mu^1, \mu^2} \coloneqq \mathbb{E}_{\theta\sim \sigma}[\gamma_\theta^{\mu^1, \mu^2}]\quad \\
    &\nonumber\text{i.e. }\bar{\gamma}^{\mu^1, \mu^2}(\{(x, y)\})=\int_{\mathbb{S}^{d-1}}\gamma_\theta^{\mu^1, \mu^2}(\{(x, y)\})\text{d}\sigma(\theta).
\end{align}
\citet{liu2025expected} have shown that the associated cost, 
\begin{align*}
\mathcal{D}_p(\mu^1, \mu^2)=\left(\sum_{x\in\mathbb{R}^d}\sum_{y\in\mathbb{R}^d}\|x-y\|^p\bar{\gamma}^{\mu^1, \mu^2}(\{(x, y)\})\right)^\frac{1}{p},
\end{align*}
is a well-defined distance and equivalent to the Wasserstein distance.

\subsection{Soft Sorting}
Calculating the sliced Wasserstein distance involves evaluating the quantile functions of the distributions, which, in the discrete case, can be boiled down to the sorting operation. Sorting is one of the most common operations in computer science. Yet, the piecewise-linear sorted value function and the integer-valued rank/argsort operators pose a significant obstacle for gradient-based optimization techniques, which are essential in deep learning, as neither of them is differentiable. To incorporate sorting operations into the backpropagation framework, differentiable approximations, known as soft sorting, have been explored. Examples include smoothed rank operators by adding Gaussian noise \cite{taylor2008softrank} and by using sigmoid surrogate functions \cite{qin2010general}, parameterizing permutations in terms of a differentiable relaxation 
 \cite{mena2018learning}, and relaxing the permutation matrices to be only row-wise stochastic \cite{grover2019stochastic}.  Of note, \citet{NEURIPS2019_d8c24ca8} propose a differentiable proxy by viewing sorting as an optimal assignment problem and relaxing it to an optimal transport problem from the input values to an auxiliary probability measure supported on an increasing family of target values. 

\citet{prillo2020softsort} introduce a simple yet highly effective continuous relaxation of the argsort operator using the softmax function: Given a vector $v\in\mathbb{R}^N$,
\begin{align}
    \text{SoftSort}_t^d(v) \coloneqq \text{softmax}\left(\frac{-d(\text{sort}(v)\mathds{1}^T, \mathds{1}v^T)}{t}\right),
    \label{eq:softsort}
\end{align}
where the softmax operator is applied row-wise, $d(\cdot, \cdot)$ can be any differentiable semi-metric, and $t$ is a temperature parameter that controls the degree of the approximation. 

\begin{figure*}[t!]
    \centering
    \includegraphics[trim=0in 0in .9in .4in, clip, width=0.99\textwidth]{figures/ESPFormer2.pdf}
    \vspace{-.15in}
    \caption{An overview of the proposed ESP attention mechanism. By integrating the slicing operator into the key and query matrices, each dimension is treated as a learnable slice. For each slice, tokens are (soft) sorted, and a doubly-stochastic correspondence matrix is computed between the keys and queries. Finally, these correspondence matrices across all dimensions are aggregated to form a single doubly-stochastic attention matrix.}
    \label{fig:teaser}
    \vspace{-.2in}
\end{figure*}


\section{Method}
\subsection{ESP for Uniform Discrete Distributions}
\label{sec:esp}
Given our application of interest, we specifically focus on uniformly distributed discrete distributions with an equal number of support points. For a given $N\in\mathbb{N}$, denote the space of uniform discrete distributions with $N$ support as
\begin{align*}
    \mathcal{P}_{(N)}(\mathbb{R}^d):=\left\{\frac{1}{N}\sum_{i=1}^N\delta_{x_i}|x_i\in\mathbb{R}^d, \forall i\in\{1, \cdots, N\}\right\}.
\end{align*}
Let $\mu^1=\frac{1}{N}\sum_{i=1}^N\delta_{x_i}\in \mathcal{P}_{(N)}(\mathbb{R}^d), \mu^2=\frac{1}{N}\sum_{j=1}^N\delta_{y_j}\in \mathcal{P}_{(N)}(\mathbb{R}^d)$ and $X=[x_1, x_2, \cdots, x_N]^T\in\mathbb{R}^{N\times d}, Y=[y_1, y_2, \cdots, y_N]^T\in\mathbb{R}^{N\times d}$ be the corresponding matrix forms. Denote the symmetry group of order $N$ in matrix representation as $\mathbf{S}_N$, that is, $\mathbf{S}_N$ contains all permutation matrices of a set of $N$ elements. 

Consider the 1-dimensional slice of $\mu^1$ and $\mu^2$ in the $\theta$ direction: $\theta_\#\mu^1=\frac{1}{N}\sum_{i=1}^N\delta_{\theta\cdot x_i}$ and $\theta_\#\mu^2=\frac{1}{N}\sum_{j=1}^N\delta_{\theta\cdot y_j}$, with $X\theta =[\theta\cdot x_1, \theta\cdot x_2, \cdots, \theta\cdot x_N]^T\in\mathbb{R}^{N}$ and $Y\theta =[\theta\cdot y_1, \theta\cdot y_2, \cdots, \theta\cdot y_N]^T\in\mathbb{R}^{N}$. There exists $A, B\in \mathbf{S}_N$ such that $A X\theta\in\mathbb{R}^{N}$ and $BY\theta \in\mathbb{R}^{N}$ are in sorted order, i.e.,
\begin{align*}
    (AX\theta )_1\leq (AX\theta)_2\leq \cdots\leq(AX\theta)_N;\\
    (BY\theta )_1\leq (BY\theta)_2\leq \cdots \leq(BY\theta)_N.
\end{align*}
Then the optimal matching between $\theta_\#\mu^1$ and $\theta_\#\mu^2$ can be described by 
\begin{align*}
    (AX\theta)_n\mapsto(BY\theta)_n,\quad\forall n\in [1, 2, \cdots, N],
\end{align*}
or equivalently, $A^{T}B$ represents the transport map from $X\theta$ to $Y\theta$, i.e.,
\begin{align*}
    (X\theta)_n\mapsto(A^{T}BY\theta)_n,\quad\forall n\in [1, 2, \cdots, N].
\end{align*}
By lifting this transport map from the $\theta$ slice to the original space $\mathbb{R}^d$, we have $U_\theta \coloneqq \frac{1}{N}A^{T}B$ which represents a transport plan from $X$ to $Y$.

Finally, for a given histogram of $\theta$, $\sigma=\sum_{l=1}^L\sigma_l\delta_{\theta_l}$ with $\sum_{l=1}^L \sigma_l=1$, the ESP for uniformly-distributed discrete distributions with the same number of supports is defined as 
\begin{align}
    G:= \sum_{l=1}^L\sigma^\tau_lU_{\theta_l},
    \label{eq:esp_attention}
\end{align}
where $\tau$ denotes the `inverse temperature'' hyperparameter, and $\sigma_l^\tau$ is defined as
\vspace{-.1in}
\begin{equation}\label{eq:temp}
        \sigma_l^\tau= \frac{e^{-\tau \mathcal{D}^p_p(\mu^1,\mu^2;\theta_l)}}{\sum_{l'=1}^L e^{-\tau \mathcal{D}_p^p(\mu^1,\mu^2;\theta_{l'}) }},
\end{equation}
 with $\mathcal{D}^p_p(\mu^1,\mu^2;\theta)$ representing the $p$-th power of the induced transport cost by $U_\theta$, defined as
\begin{align}
\mathcal{D}^p_p(\mu^1,\mu^2;\theta)=\sum_{i=1}^N\sum_{j=1}^N\|x_i-y_j\|^pU_\theta(\{(x_i, y_j)\}).
\end{align}
When $\tau=0$, $\sigma_l=\frac{1}{L}$ for all $l\in[1, 2, \cdots, L]$, then $G$ is simply the mean of all the lifted plans. 

When handling uniformly distributed discrete distributions with different numbers of support points (e.g., in cross-attention), the transport plan involving mass splitting can be approximated using linear interpolation. For $\mu^1=\frac{1}{N}\sum_{j=1}^N\delta_{x_i}, \mu^2=\frac{1}{M}\sum_{j=1}^M\delta_{y_j}$ with $N\neq M$, let $A'\in\mathbb{R}^{N\times N}$ and $B'\in\mathbb{R}^{M\times M}$ respectively denote their sorting permutation matrices. We define an interpolation matrix $I\in\mathbb{R}^{N\times M}$, where
\begin{equation}
    I[i, j]\coloneqq\begin{cases}
        \frac{\frac{i}{N}-\frac{j}{M}}{\frac{1}{M}},&\text{if } \frac{j}{M}\leq \frac{i}{N}\leq\frac{j+1}{M};\\
        \frac{\frac{j}{M}-\frac{i}{N}}{\frac{1}{M}},&\text{if } \frac{j-1}{M}\leq \frac{i}{N}\leq\frac{j}{M};\\
        0,&\text{elsewhere.}
    \end{cases}
\end{equation}
The transport plan is then given by $U_\theta=\frac{1}{N}A'^{T}IB'$.

\begin{figure*}[t!]
    \centering
    \includegraphics[trim=.27in .1in .22in .1in, clip, width=\textwidth]{figures/norm_fig_4.pdf}
    \vspace{-0.5cm}
    \caption{The attention weights between an example pair of keys (red) and queries (green) obtained by Sinkhorn's algorithm (top row) with different numbers of iterations and by Expected Sliced Transport Plans (bottom row) with different inverse temperature values. Note that for Sinkrhorn and at zero iterations, i.e., \( S^0 \), the computed attention reduces to classic self-attention. The weights are represented by the width of the lines connecting each pair of points.}
    \label{fig:plan comparison}
\end{figure*}

\subsection{ESP Attention}
To bridge the abstract formulation of the Expected Sliced Plan (ESP) with its practical use in attention mechanisms, we interpret the query and key matrices as empirical probability measures due to their permutation-equivalent nature. Following our notation in Section \ref{sec:attention},  let $W_Q, W_K\in \mathbb{R}^{m\times d}, W_V\in\mathbb{R}^{d\times d}$ denote the query, key, and value matrices, respectively, and let $X=[x_1, x_2, \cdots, x_N], x_i \in \mathbb{R}^d, \forall i$ denote the input tokens. Also, let $Q=W_QX\in \mathbb{R}^{m\times N}$ and $K=W_KX\in \mathbb{R}^{m\times N}$ denote the queries and keys, respectively. Then, we think of doubly-stochastic attention as a transport plan between the empirical measures $\mu^Q$ and $\mu^K$, defined as
\vspace{-.1in}
\begin{align*}
    \mu^Q = \frac{1}{N} \sum_{i=1}^{N} \delta_{q_i}, \quad \mu^K = \frac{1}{N} \sum_{j=1}^{N} \delta_{k_j}.
\end{align*}
To create such a transport plan using the ESP framework presented in Section \ref{sec:esp}, we use a set of $L$ slicing directions $\Theta =[\theta_1,...,\theta_L]^T\in\mathbb{R}^{L \times m}, \theta_l \in \mathbb{S}^{m-1}$. Then, the rows of $\Theta K$ and $\Theta Q$ contain the projected keys and queries for each slice. The transportation plan $U_{\theta_l}$ is, then, calculated for each slice by soft-sorting the projected keys and queries as described in Section \ref{sec:esp}. Finally, the attention matrix is calculated from \eqref{eq:esp_attention}. 

A key consideration is the choice of slices, \(\Theta\). While classic sliced OT samples slicers uniformly from \(\mathbb{S}^{m-1}\), prior work suggests learning them \cite{deshpande2019max,naderializadeh2021pooling}. Though this approach is common, learning \(\Theta\) introduces additional parameters, increasing the total count in ESP attention and complicating fair comparisons with other mechanisms. Notably, since the input distributions (keys and queries) are themselves learned, optimizing \(\Theta\) may be unnecessary. Instead, the distributions can adapt to a fixed slicing scheme, as done in prior work like FSPool \cite{Zhang2020FSPool}. To avoid extra parameters, in this work, we propose using axis-aligned slices by setting \(\Theta = \mathds{I}_{m \times m}\), the identity matrix. This leads to
% To mitigate this, we set \(L = m\), ensuring \(\Theta \in \mathbb{R}^{m \times m}\), and observe that the slices for keys and queries take the form \(\Theta W_K X\) and \(\Theta W_Q X\). Thus, we absorb \(\Theta\) into the key and query matrices, maintaining parity in parameter count with competing attention mechanisms. 
\begin{align}  
    \text{ESP-Attention}(Q, K, V) = VG,  
\end{align}  

where \( V=W_VX \in \mathbb{R}^{d \times N} \) is the value matrix. Figure \ref{fig:teaser} illustrates the construction of the ESP matrix \( G \), which serves as the proposed transport-based attention map, directing how information flows from \( V \) to the output. Moreover, Figure \ref{fig:plan comparison} compares classic attention (Sinkhorn \( S^0 \)), Sinkhorn attention from \cite{sander2022Sinkformers} across different iteration counts, and our ESP attention under varying `inverse temperature'' hyperparameters. As can be seen, while ESP provides a balanced distribution of attention due to its double stochasticity, the inverse temperature'' parameter controls the number of other tokens each token should pay attention to. The overall pipeline of ESPFormer can be found in Algorithm~\ref{alg:espformer}.
% %     Q\theta_l = [<\theta_l, q_1>, \dots, <\theta_l,q_L>]^T, \\\quad K\theta_l = [<\theta_l, k_1>, \dots, <\theta_l, k_L>]^T.
% % \end{align*}
% % For each slice $\theta_l$, the optimal transport map between the projected distributions is computed using the soft sorting-based approach detailed in Section 3.1. This results in a transport plan $\tilde{U}_{\theta_l}$, which is then aggregated as
% \begin{align}
%     \tilde{G} = \sum_{l=1}^{L} \sigma_l \tilde{U}_{\theta_l},
% \end{align}
% where $\sigma_l$ is computed using the temperature-scaled softmax weighting in Eq. \eqref{eq:temp}.

% To integrate ESP into the attention mechanism, we define the ESP attention as:


% For practical implementation, we propose absorbing the slicing directions \( \theta_l \) into the key and query transformation matrices \( W_K \) and \( W_Q \), redefining them as:
% \begin{align}
%     % W_K' = \Theta W_K, \quad W_Q' = \Theta W_Q.
%     W_K \leftarrow \Theta W_K, \quad W_Q \leftarrow \Theta W_Q.
% \end{align}
% This reformulation offers several advantages: (1) it maintains interpretability by ensuring we operate over linear slices, (2) it enables learning input-dependent slicing directions through the transformation matrices, allowing the model to adapt its transport geometry based on the specific distributions of queries and keys, and (3) by effectively treating the embedding dimension \( d \) as the number of slices \( L \), it enforces double stochasticity as a prior without increasing model complexity.

% \soheil{Up to here, everything is abstract. An average reader will be completely lost. We need a section that tells the reader that the Q and K are treated as empirical probability measures. Then, they are sliced, and the ESP is calculated as the attention between them. \\
% Finally, you can start talking about learnable slices and how the slices are integrated into the $W_k$ and $W_q$. This section would be critical. Figure 2 must also be referenced in this section.} 


% \textcolor{red}{Ashkan: How about now?}
\begin{algorithm}[t]
\caption{ESPFormer's Doubly-Stochastic Attention}
\label{alg:espformer}
    \begin{algorithmic}[1]
        \INPUT Query matrix $Q\in \mathbb{R}^{m\times N}$,  Key matrix $K\in \mathbb{R}^{m\times N}$,\\ Value Matrix $V\in \mathbb{R}^{d\times N}$, SoftSort hyperparameter $t$,\\ and `inverse temperature'' hyperparameter $\tau$.
        \OUTPUT Attention-weighted output matrix
        \STATE Calculate the pairwise distance matrix: $$[C]_{ij}=\|Q_{:i}-K_{:j}\|^2.$$
        %Apply queue, key matrices $Q=XW_Q'^T\in\mathbb{R}^{N\times d}, K=XW_K'^T\in\mathbb{R}^{M\times d}$ and calculate $I_{N\times M}$ using Eq. (10)
        % \STATE Initialize 
        \FOR{$l=1$ to $m$}
        \STATE SoftSort the projected samples using \eqref{eq:softsort}:         $$A_l=\text{SoftSort}_t(Q_{l:}), B_l=\text{SoftSort}_t(K_{l:}).$$
        \STATE Calculate the transport plan $U_l=\frac{1}{N}A_l^TB_l$ 
        \STATE Calculate $D_l = \sum_{ij} [C]_{ij} [U_{l}]_{ij}$
        \ENDFOR
        \STATE Calculate the $\sigma^\tau = \text{softmax}(D;\tau)$
        \STATE Aggregate the plans from all slices $G=\sum_{l=1}^m\sigma^\tau_l U_l$
        \STATE Return: $VG$
    \end{algorithmic}
\end{algorithm}

\subsection{Runtime Complexity}  
% The runtime complexity of ESPFormer is determined by its principal computational components. The algorithm 

ESPFormer begins with query and key projections ($Q=W_QX$ and $K=W_KX$), each requiring $\mathcal{O}(mNd)$ operations, where $d$ corresponds to the output dimension. The subsequent SoftSort operation, which involves computing pairwise distances, incurs a complexity of $\mathcal{O}(N^2)$ per slice. When applied across all $m$ slices, this yields a complexity of $\mathcal{O}(mN^2)$. The computation of transport plans through matrix multiplications with soft permutation matrices contributes an additional $\mathcal{O}(mN^2)$ operations. The final aggregation of transport plans requires summing $m$ plans of size $N \times N$, also contributing $\mathcal{O}(mN^2)$ operations. Therefore, the overall runtime complexity of ESPFormer is $\mathcal{O}(mN(N+d))$. In comparison, Sinkhorn's algorithm exhibits a runtime complexity of $\mathcal{O}((S+m)N^2)$, where $S$ denotes the number of iterations. A key distinction lies in the parallelization capabilities: while Sinkhorn necessitates sequential processing over $S$ iterations, ESPFormer enables parallel computation across the $m$ slices. This parallelizability allows ESPFormer to scale efficiently with increasing $m$. As demonstrated in Figure~\ref{fig:runtime_comparison}, ESPFormer achieves superior runtime compared to Sinkformer. The wall clock runtime analysis of ESPFormer compared to all the baselines can be found in Appendix \ref{app_a}.



\begin{figure}[t]
    \centering
    \includegraphics[width=0.6\columnwidth]{figures/attention_runtime_comparison_sink_variants_3.pdf}
    \caption{Runtime analysis of ESPFormer versus Sinkformer with varying iteration counts ($S$) for sequence lengths $N \in \{50, 100, 500, 1000\}$ with $d=1024$, averaged over 10 runs. ESPFormer demonstrates superior computational efficiency across all sequence lengths while preserving model expressivity.}
    \label{fig:runtime_comparison}
\end{figure}

\section{Experiments}
In this section, we evaluate the performance of ESPFormer across a diverse set of domains, including image classification using ViTs \cite{dosovitskiy2020vit}, point cloud classification using set transformers \cite{lee2019set} and point cloud transformers \cite{guo2021pct}, as well as sentiment analysis and neural machine translation via Transformers, highlighting the applicability of our approach in comparison to Differential Transformer \cite{Ye2025differential}, Sinkformer \cite{sander2022Sinkformers}, and the Vanilla Transformer \cite{vaswani2017attention}. The details of our experimental setup and implementation can be found in Appendices \ref{app_b} and \ref{app_c]]}.\footnote{Our implementation code will be released upon acceptance.}

\subsection{ModelNet 40 Classification}
The ModelNet40 dataset \cite{wu20153d} comprises 40 widely recognized 3D object categories and serves as a standard benchmark for point cloud classification. Transformers designed for point clouds and sets have been extensively evaluated on ModelNet40, with notable examples including Set Transformers \cite{lee2019set} and Point Cloud Transformers \cite{guo2021pct}. Table \ref{tab:modelnet} presents the classification results across four runs, comparing different attention mechanisms integrated into Set Transformers and Point Cloud Transformers. Notably, ESPFormer outperforms competing methods, demonstrating superior performance.

\begin{table}[t!]
\centering
\caption{Test accuracy for Sentiment Analysis on IMDb dataset.}
\tiny
\resizebox{0.5\linewidth}{!}{%
\begin{tabular}{@{}lcccc@{}}
% \toprule
\cmidrule[1.5pt]{1-5}
\textbf{Model}                       & \textbf{Best}   & \textbf{Median} & \textbf{Mean}   & \textbf{Worst}  \\ \midrule
ESPFormer               & \textbf{85.50}\% & \textbf{85.50}\% & \textbf{85.47}\% & \textbf{85.40}\% \\
Sinkformer              & 85.40\% & 85.40\% & 85.37\% & 85.3\% \\
DiffTransformer         & \textbf{85.50\%} & 85.45\% & 85.45\% & \textbf{85.40\%} \\
Transformer             & 85.30\% & 85.25\% & 85.25\% & 85.30\% \\
\cmidrule[1.5pt]{1-5}
\end{tabular}%
}
\label{tab:SA}
\end{table}

\begin{table}[h!]
\centering
\caption{Test accuracy on the ModelNet40 dataset over four runs. Accuracies marked with $^*$ are reported from \cite{sander2022Sinkformers}.}
\resizebox{0.6\linewidth}{!}{%
\begin{tabular}{@{}lcccc@{}}
% \toprule
\cmidrule[1.5pt]{1-5}
\textbf{Model}                       & \textbf{Best}   & \textbf{Median} & \textbf{Mean}   & \textbf{Worst}  \\ \midrule
Set ESPFormer               & \textbf{89.6}\% & \textbf{89.5}\% & \textbf{89.4}\% & \textbf{89.1}\% \\
Set Sinkformer$^*$              & 89.1\% & 88.4\% & 88.3\% & 88.1\% \\
Set DiffTransformer         & 89.0\% & 88.7\% & 88.7\% & 88.6\% \\
Set Transformer$^*$             & 87.8\% & 86.3\% & 85.8\% & 84.7\% \\ \midrule
Point Cloud ESPFormer       & \textbf{93.2}\% & \textbf{92.9}\% & \textbf{92.7}\% & \textbf{92.6\%} \\
Point Cloud Sinkformer$^*$      & 93.1\% & 92.8\% & \textbf{92.7}\% & 92.5\% \\
Point Cloud DiffTransformer & 93.1\% & 92.8\% & \textbf{92.7\%} & \textbf{92.6\%} \\
Point Cloud Transformer$^*$     & \textbf{93.2}\% & 92.5\% & 92.5\% & 92.3\% \\
% \bottomrule
\cmidrule[1.5pt]{1-5}
\end{tabular}%
}
\label{tab:modelnet}
\end{table}
% \vspace{-0.1in}
\subsection{Sentiment Analysis}
We next evaluate ESPFormer on the IMDB dataset \cite{maas2011learning} for sentiment analysis. Following \citet{sander2022Sinkformers}, our architecture comprises an attention-based encoder followed by a max-pooling layer trained to classify movie reviews as either positive or negative. Table \ref{tab:SA} presents the accuracy improvements achieved by using ESPFormer as the core attention module, compared to baseline models, showcasing its robustness in text classification.

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=\textwidth]{icml2024/figures/training_metrics.pdf}
%     \caption{\textbf{Learning Curves} when training a Transformer, Sinkformer and ESPFormer on the Sentiment Analysis task on the IMDb dataset.}
%     \label{fig:sentiment_curves}
% \end{figure*}

\subsection{Neural Machine Translation}
We evaluate ESPFormer and Sinkformer using two reference models: the Transformer and its DiffTransformer counterpart~\cite{Ye2025differential}. Both models are trained using the fairseq sequence modeling toolkit \cite{ott2019fairseq} on the IWSLT'14 German-to-English dataset \cite{cettolo2014iwslt}. The architecture of both models consists of an encoder and a decoder, each with a depth of 6 layers. Initially, we trained both models for 25 epochs using the standard training procedure. After this phase, we performed a Plug-and-Play evaluation, where the attention heads were plugged into the pre-trained models and evaluated on their performance, as shown in Table \ref{table:nmt_performance}. In this phase, we tested ESPFormer and Sinkformer attention heads, denoted by the respective models in the table, comparing their performance to the base Transformer and DiffTransformer models. Following the Plug-and-Play evaluation, we performed a Fine-Tune Boost phase, where the models were further fine-tuned for an additional 10 epochs. The fine-tuning led to further performance gains, as observed in the table, with ESPFormer showing the highest improvement in both the Transformer and DiffTransformer settings, achieving the best BLEU score of 34.64 and 34.83, respectively. The fine-tuned results show the effectiveness of incorporating ESPFormer into the model architecture. 

\aboverulesep=0ex
\belowrulesep=0ex
\begin{table}[t!]
\caption{Plug-and-Play and Fine-Tune Boost performance of Transformer and DiffTransformer Baselines on the
IWSLT’14 German-to-English dataset, reported as the median over 4 runs. Results with a $^\star$ sign denote the Plug-and-Play performance of a different attention module than the base attention module.}
\vspace{.1in}
\renewcommand{\arraystretch}{1.12}%
\centering%
\noindent\makebox[\columnwidth]{%
\setlength\tabcolsep{4.6pt}%
\begin{tabular}{clccc}
\cmidrule[1.5pt]{2-4}
 & \textbf{Model} & \textbf{Plug-and-Play} & \textbf{Fine-Tune Boost} \\ 
\cline{2-4}& 
ESPFormer & 33.38$^\star$ & \textbf{34.64} \\
& Sinkformer & 33.36$^\star$ & 34.61 \\
\multirow{-3}{*}{\rotatebox[origin=c]{90}{\tiny Transformer}} & Transformer & 33.40\phantom{$^\star$} & 34.61 \\
\cline{2-4}
& ESPFormer & 33.72$^\star$ & \textbf{34.83} \\
& Sinkformer & 33.67$^\star$ & 34.81\\
\multirow{-3}{*}{\rotatebox[origin=c]{90}{\tiny DiffTransformer}} & DiffTransformer & 33.85\phantom{$^\star$} & 34.78 \\
\cmidrule[1.5pt]{2-4}
\end{tabular}}
\label{table:nmt_performance}
\end{table}
%The results, median over 4 runs, are summarized in Table~\ref{table:nmt_performance}. 

% \begin{table}[ht]
% \centering
% \caption{Plug-and-Play and Fine-Tune Boost Evaluation for Transformer and DiffTransformer Baselines}
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{@{}lcc@{}}
% \toprule
% \multicolumn{3}{c}{\textbf{Reference: Transformer}} \\ \midrule
% \textbf{Model}        & \textbf{Plug-and-Play} & \textbf{Fine-Tune Boost} \\ \midrule
% ESPFormer             & 33.38$^\star$              & \textbf{34.64}               \\
% Sinkformer            & 33.36$^\star$              & 34.61                        \\
% DiffTransformer       & 33.40$^\star$              & 34.59                        \\
% Transformer & 33.40\phantom{$^\star$}    & 34.61                        \\ \midrule
% \multicolumn{3}{c}{\textbf{Reference: DiffTransformer}} \\ \midrule
% \textbf{Model}        & \textbf{Plug-and-Play} & \textbf{Fine-Tune Boost} \\ \midrule
% ESPFormer             & 33.72$^\star$              & \textbf{34.83}               \\
% Sinkformer            & 33.67$^\star$              & 34.81                        \\
% DiffTransformer & 33.85\phantom{$^\star$}         & 34.78                          \\ \bottomrule
% \end{tabular}%
% }
% \label{tab:nmt_performance}
% \end{table}






\subsection{Vision Transformers}
\subsubsection*{\textbf{Cats and Dogs Classification}}

To evaluate the generalizability of the models under limited data scenarios, we conducted experiments on the Cats and Dogs dataset \cite{kaggle2013dogs} using varying fractions of the training data: 1\%, 10\%, 25\%, and 100\%. We train a ViT and modify the attention mechanism accordingly. The models compared include ESPFormer, Sinkformer, DiffTransformer, and the standard Transformer. The goal was to assess how well each model performs when faced with progressively larger amounts of data, particularly in resource-constrained settings. Table~\ref{tab:cats_dogs} summarizes the classification accuracy for each model under different data fractions. 
Our results demonstrate that ESPFormer consistently outperforms the other models across all data fractions, achieving the highest classification accuracy at each level of data availability. With only 1\% of the data, ESPFormer achieves 55.66\% accuracy, a significant improvement over the Transformer at 49.71\%. As the data availability increases, ESPFormer continues to show superior performance. %, reaching 79.47\% accuracy with the full dataset, compared to 79.12\% for Sinkformer, 78.91\% for DiffTransformer, and 78.54\% for the standard Transformer. 
This suggests that ESPFormer is particularly robust in data-scarce environments, showcasing its ability to generalize well even under limited training data.

\aboverulesep=0ex
\belowrulesep=0ex
\begin{table*}[t!]
    \centering
    \caption{Average and standard deviation (over 3 runs) of ESPFormer's classification accuracy (\%) vs. baselines on the Cats and Dogs dataset under varying data availability.}
    \label{tab:cats_dogs}
    \renewcommand{\arraystretch}{1.08}%
    \vspace{.1in}
    \setlength\tabcolsep{19.5pt}%
    \begin{tabular}{ccccc}
        % \toprule
        \cmidrule[1.5pt]{1-5}
        \textbf{Data Fraction} & \textbf{ESPFormer} & \textbf{Sinkformer} & \textbf{DiffTransformer} & \textbf{Transformer} \\
        \midrule
        \textbf{1\%}   & \textbf{55.66 $\pm$ 3.95} & 55.07 $\pm$ 3.34 & 53.78 $\pm$ 0.28 & 49.71 $\pm$ 0.31 \\
        \textbf{10\%}  & \textbf{71.49 $\pm$ 0.43} & 69.56 $\pm$ 0.32 & 67.34 $\pm$ 0.11 & 57.25 $\pm$ 0.22\\
        \textbf{25\%}  & \textbf{75.40 $\pm$ 0.38} & 74.56 $\pm$ 0.58 & 74.86 $\pm$ 0.17 & 72.25 $\pm$ 0.16\\
        % \textbf{50\%}  & 76.93\% & 77.75\% & \textbf{78.63\%} & 78.11\% \\
        \textbf{100\%} & \textbf{79.47 $\pm$ 0.12} & 79.12 $\pm$ 0.17 & 78.85 $\pm$ 0.11 & 78.49 $\pm$ 0.09 \\
        % \bottomrule
        \cmidrule[1.5pt]{1-5}
    \end{tabular}
    \vspace{-.1in}
\end{table*}

\subsubsection*{\textbf{Impact of Patch Size}}
Similar to \citet{sander2022Sinkformers} and to analyze the effect of patch size on model performance, we trained Transformer, DiffTransformer, Sinkformer, and ESPFormer on the MNIST dataset \cite{lecun1998mnist}. To isolate the impact of the attention mechanism, we used a single-layer self-attention module without feed-forward layers. Figure~\ref{fig:patch_size} illustrates the test accuracy as a function of patch size for each model. ESPFormer consistently outperforms competing models, particularly for smaller patch sizes, highlighting its superior ability to capture fine-grained details. As the patch size increases, the performance gap narrows, with all models converging to similar accuracies --- likely due to the reduced information content within each individual patch.

%Alternative patch size figure 
\begin{figure}[t]
    \centering
    \includegraphics[trim=0in 0in 0in .4in, clip, width=0.6\columnwidth]{figures/patch_size_bar.pdf}
    \caption{Comparison of MNIST test accuracy across different patch sizes for Transformer, DiffTransformer, Sinkformer, and ESPFormer architectures. Results are averaged over 3 runs.}
    \vspace{-.25in}
    \label{fig:patch_size}
\end{figure}

\section{Conclusion}
We introduced ESPFormer, a novel Transformer architecture that integrates a fast, doubly-stochastic attention mechanism based on Expected Sliced Transport Plans (ESP). By leveraging differentiable soft sorting and a fully parallelizable approach, ESPFormer provides a computationally efficient alternative to iterative Sinkhorn normalization while ensuring a balanced distribution of attention across tokens. Our experiments across diverse domains, including image classification, point cloud processing, sentiment analysis, and neural machine translation, demonstrate that ESPFormer consistently outperforms both classical self-attention and Sinkhorn-based alternatives in terms of accuracy and efficiency. Furthermore, we showed that our proposed attention can be seamlessly integrated into pre-trained Transformers, improving performance even with minimal fine-tuning. The flexibility of ESPFormer also makes it compatible with emerging differential attention mechanisms, e.g., \cite{Ye2025differential}, expanding its applicability to a broad range of architectures. These findings highlight the potential of transport-based attention mechanisms as a principled alternative to existing methods, paving the way for future research in efficient and structured attention models.

\section*{Acknowledgment}
This work was partially supported by NSF CAREER Award \#2339898, and the Wellcome Leap `Surgery: Access Validate and Expand (SAVE)' program.
\bibliographystyle{plainnat}
\bibliography{references}

\newpage
\appendix
\onecolumn
\section{Full runtime wall-clock Analysis}\label{app_a}
The runtime comparison in Figure \ref{fig:runtime_comparison_app} highlights the computational efficiency of different attention mechanisms across varying sequence lengths. Transformer and DiffTransformer exhibit the lowest runtime due to their standard self-attention mechanism, making them computationally lightweight. In contrast, ESPFormer achieves a balance between efficiency and expressivity, maintaining lower runtimes compared to Sinkformer across all sequence lengths. These results emphasize the trade-off between computational efficiency and model expressivity when selecting an attention model for large-scale tasks.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.65\columnwidth]{figures/attention_runtime_comparison_all_baselines.pdf}
    \caption{Runtime analysis of Transformer, DiffTransformer, ESPFormer, and Sinkformer with varying iteration counts ($S$) for sequence lengths $N \in \{50, 100, 500, 1000\}$ with $d=1024$, averaged over 10 runs. ESPFormer consistently demonstrates superior computational efficiency compared to Sinkformer across all sequence lengths, while Transformer and DiffTransformer remain the most lightweight.}
    \label{fig:runtime_comparison_app}
\end{figure}

\section{Implementation Details}\label{app_b}
\textbf{Sinkhorn's Algorithm}

We implement Sinkhorn’s algorithm in the log domain to enhance numerical stability. Given a matrix \( S^0 \in \mathbb{R}^{n \times n} \) defined as \( S^0_{i,j} = e^{C_{i,j}} \) for some cost matrix \( C \in \mathbb{R}^{n \times n} \), the algorithm iteratively updates the scaling vectors \( (f, g) \in \mathbb{R}^n \times \mathbb{R}^n \) such that the limiting matrix is given by 

\[
S^\infty = \text{diag}(e^{f^\infty}) S^0 \text{diag}(e^{g^\infty}).
\]

Starting with \( g^0 = 0_n \), the updates are performed in the log domain as follows:

\[
f^{l+1} = \log( \mathbf{1}_n / n ) - \log(S e^{g^l}), \quad \text{if } l \text{ is even},
\]

\[
g^{l+1} = \log( \mathbf{1}_n / n ) - \log(S^\top e^{f^l}), \quad \text{if } l \text{ is odd}.
\]

This log-domain formulation allows for efficient and numerically stable computations by leveraging the \texttt{log-sum-exp} trick to evaluate expressions like \( \log(S e^{g^l}) \) and \( \log(S^\top e^{f^l}) \).\\

\noindent\textbf{Implementation of Sinkhorn Algorithm:}
\begin{lstlisting}
def sinkhorn_log_domain(C, epsilon=1e-3, num_iters=50):
    n = C.shape[0]
    S = np.exp(-C / epsilon)  # Compute S^0
    f, g = np.zeros(n), np.zeros(n)  # Initialize log scaling vectors

    for l in range(num_iters):
        if l % 2 == 0:
            f = np.log(1 / n) - log_sum_exp(np.log(S) + g)
        else:
            g = np.log(1 / n) - log_sum_exp(np.log(S.T) + f)

    return f, g
\end{lstlisting}

For further details and implementation, please refer to the
\noindent \textbf{\href{https://github.com/michaelsdr/sinkformers}{SinkFormer GitHub Repository}}.


\textbf{Differential Transformer}

The Differential Transformer extends traditional self-attention by introducing differential attention mechanisms, which modulate the contribution of multiple attention maps using a learnable parameter \( \lambda \). This allows the model to capture finer differences in the relational structure of the input data.

Given an input sequence \( X \), the attention mechanism computes two different sets of queries and keys, denoted as \( (Q_1, K_1) \) and \( (Q_2, K_2) \). The attention output is computed using a weighted difference of the attention maps:

\[
A = \text{softmax}(Q_1 K_1^T) - \lambda \times\text{softmax}(Q_2 K_2^T).
\]

The resulting attention map is then applied to the values \( V \), followed by normalization and projection:

\[
O = \text{GroupNorm}(A V).
\]
\\
\noindent\textbf{Implementation of Differential Attention:}
\begin{lstlisting}
def DiffAttn(X, W_q, W_k, W_v, \lambda):
    Q1, Q2 = split(X @ W_q)
    K1, K2 = split(X @ W_k)
    V = X @ W_v
    s = 1 / sqrt(d)
    A1 = Q1 @ K1.transpose(-1, -2) * s
    A2 = Q2 @ K2.transpose(-1, -2) * s
    return (softmax(A1) - \lambda * softmax(A2)) @ V
\end{lstlisting}

\noindent\textbf{Implementation of Multi-Head Differential Attention:}
\begin{lstlisting}
def MultiHead(X, W_q, W_k, W_v, W_o, \lambda):
    O = GroupNorm([DiffAttn(X, W_qi, W_ki, W_vi, \lambda) for i in range(h)])
    O = O * (1 - \lambda{init})
    return Concat(O) @ W_o
\end{lstlisting}

For further details and implementation, please refer to the
\noindent \textbf{\href{https://github.com/microsoft/unilm/tree/master/Diff-Transformer}{Differential Transformer Github Repository.}}  


\section{Experiment Details} \label{app_c]]}

\subsection{Table of Notations}

Table \ref{tab:Notations} includes the notations used in this section.
\begin{table}[ht]
\centering
\caption{Experimental Notations}
% \resizebox{0.95\linewidth}{!}
{%
\begin{tabular}{@{}lc@{}}
% \toprule
\cmidrule[1.5pt]{1-2}
\textbf{Notation}                       & \textbf{Description} \\ \midrule
t               & Temperature parameter in the Soft Sorting algorithm\\
\(\tau\)             & The "inverse temperature" hyperparameter in ESP\\
\(\epsilon\)         & The entropy regularization parameter in the Sinkhorn algorithm \\
\(\mathcal{S}\)             & Number of iterations in Sinkformer \\
\(\lambda_{\text{initial}}\) & Initial \(\lambda\) value in DiffTransformer \\
$lr_{initial}$ & Initial learning rate for all methods \\
$I$ & The interpolation matrix in ESPformer \\

\cmidrule[1.5pt]{1-2}
\end{tabular}%
}
\label{tab:Notations}
\end{table}


\subsection{ModelNet 40}
In our experiments on ModelNet40 utilizing Set Transformers, we begin by preprocessing the dataset and uniformly sampling 5000 points from each object.\\

\textbf{Set Transformers}\\
The model architecture consists of two Induced Set Attention Blocks (ISAB) in the encoder, followed by a decoder incorporating a Set Attention Block (SAB) and a Pooling by Multihead Attention (PMA) module. The training procedure employs a batch size of 64 and utilizes the \textbf{Adam optimizer} \cite{kingma2015_adam}. The network is trained for 300 epochs, with an initial learning rate of \(10^{-3}\), which is reduced by a factor of 10 after 200 epochs.\\

\begin{table}[ht]
    \centering
    \renewcommand{\arraystretch}{1.2} % Adjust row height for better readability
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        t & \(\tau\) & \(\epsilon\) & \(\mathcal{S}\) & \(\lambda_{\text{initial}}\) & $lr_{initial}$ & $I$ \\
        \hline
        $10^{-3}$ & $10^{-1}$ & $10^{-1}$ & 21 & 0.8 & $10^{-3}$ & $Identity_{N\times M}$ \\
        \hline
    \end{tabular}
    \vspace{0.05in}
    \caption{Hyperparameters used in training for Set Transformers on ModelNet40 dataset.}
    \label{tab:hyperparams_c2}
\end{table}


\textbf{Point Cloud Transformers}\\
The training is conducted with a batch size of 32 using Stochastic Gradient Descent (SGD) (Ruder, 2016). The model undergoes 300 training epochs, starting with an initial learning rate of \(10^{-4}\), which is reduced by a factor of 10 after 250 epochs.

\begin{table}[ht]
    \centering
    \renewcommand{\arraystretch}{1.2} % Adjust row height for better readability
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        t & \(\tau\) & \(\epsilon\)  & \(\mathcal{S}\) & \(\lambda_{\text{initial}}\) & $lr_{initial}$  & \textbf{$I$}\\
        \hline
        $10^{-3}$ & $10^{-1}$ & $10^{-1}$ & 21 & 0.8 & $10^{-4}$ & ${Identity_{N\times N}}$ \\
        \hline
    \end{tabular}
    \vspace{0.05in}
    \caption{Hyperparameters used in training for Point Cloud Transformers on ModelNet40 dataset.}
    \label{tab:hyperparams_c3}
\end{table}
\subsection{Sentiment Analysis}

For our sentiment analysis experiments, we utilize the publicly available implementation from the \texttt{nlp-tutorial}\footnote{\url{https://github.com/lyeoni/nlp-tutorial/tree/master/text-classification-transformer}} repository, where a pretrained Transformer model is fine-tuned on the IMDb dataset. In our experimental setup, we reset the parameters of the pretrained Transformer and train it from scratch on the IMDb dataset. The model architecture consists of a depth of 6 layers and employs 8 attention heads. Training is conducted using the Adam optimizer with a batch size of 32 over 15 epochs. The initial learning rate is set to \(10^{-4}\) and is reduced by a factor of 10 after 12 epochs.

\begin{table}[ht]
    \centering
    \renewcommand{\arraystretch}{1.2} % Adjust row height for better readability
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        t & \(\tau\) & \(\epsilon\)  & \(\mathcal{S}\) & \(\lambda_{\text{initial}}\) & $lr_{initial}$  & \textbf{$I$}\\
        \hline
        $10^{-3}$ & $10^{+1}$ & $10^{-1}$ & 15 & 0.33 & $10^{-4}$ & ${Identity_{N\times N}}$ \\
        \hline
    \end{tabular}
    \vspace{0.05in}
    \caption{Hyperparameters used in training for Transformers on IMDb dataset.}
    \label{tab:hyperparams_c4}
\end{table}

\subsection{Neural Machine Translation}

For our neural machine translation experiments, we adopt the Transformer model from \texttt{fairseq} along with its DiffTransformer counterpart, training both from scratch for 25 epochs. We then fine-tune them alongside other baselines for an additional 10 epochs on the IWSLT'14 dataset\footnote{\url{https://github.com/pytorch/fairseq/blob/main/examples/translation/README.md}}. When fine-tuning ESPFormer and Sinkformer, we modify the original training schedule by reducing the learning rate by a factor of 10.

\begin{table}[ht]
    \centering
    \renewcommand{\arraystretch}{1.2} % Adjust row height for better readability
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        t & \(\tau\) & \(\epsilon\)  & \(\mathcal{S}\) & \(\lambda_{\text{initial}}\) & $lr_{initial}$  & \textbf{$I$}\\
        \hline
        $10^{-3}$ & $10^{+1}$ & $10^{-1}$ & 15 & 0.0 & $10^{-4}$ & ${Identity_{N\times N}}$ \\
        \hline
    \end{tabular}
    \vspace{0.05in}
    \caption{Hyperparameters used in training for neural machine translation on IWSLT14 dataset.}
    \label{tab:hyperparams_c5}
\end{table}


\subsection{Vision Transformers}
\subsubsection{Cats and Dogs Classification}
For this experiment, we use the ViT model \cite{dosovitskiy2020vit} with different attention mechanisms. The images are resized to $224 \times 224$. We use a ViT architecture with an embedding and MLP  dimension of 128, 6 layers, 8 attention heads, and a patch size of 16. For all methods and percentages of data, we train the model for 300 epochs. We use an initial learning rate of $3 \times 10^{-5} $. After 250 epochs, the learning rate is reduced by a factor of 10. Training is done using the Adam optimizer \cite{kingma2015_adam} and a batch size of 64. Our experimental setup, including the normalizations and data augmentations, are consistent with the Cats and Dogs experiment in Sinkformer \cite{sander2022Sinkformers} . For each percentage of the data, three random seeds are used to initialize and generate new subsets of data. The subsets are consistent across all methods to ensure the same training set is used.


\begin{table}[h!]
    \centering
    \renewcommand{\arraystretch}{1.2} % Adjust row height for better readability
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        t & \(\tau\) & \(\epsilon\)  & \(\mathcal{S}\) & \(\lambda_{\text{initial}}\) & $lr_{initial}$  & \textbf{$I$}\\
        \hline
        $10^{-3}$ & $10^{-1}$ & $1$ & 3 & 0.5 & $3 \times 10^{-5} $ & ${Identity_{N\times N}}$ \\
        \hline
    \end{tabular}
    \vspace{0.05in}
    \caption{Hyperparameters used in training for ViT on Cats and Dogs dataset.}
    \label{tab:hyperparams_c6}
\end{table}

\subsubsection{Impact of Patch Size}


To analyze the effect of patch size on final accuracy, we conduct experiments using a batch size of 100 and the Adam optimizer. The model architecture consists of a single-layer Transformer (depth = 1) with one attention head, no non-linearity, and varying patch sizes. Training is performed over 45 epochs, with an initial learning rate of \(1 \times 10^{-3}\) for the Transformer and DiffTransformer and \(2 \times 10^{-3}\) for the ESPFormer and Sinkformer. The learning rate is decayed by a factor of 10 after 35 epochs, and again by another factor of 10 after 41 epochs.

\begin{table}[h!]
    \centering
    \renewcommand{\arraystretch}{1.2} % Adjust row height for better readability
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        t & \(\tau\) & \(\epsilon\)  & \(\mathcal{S}\) & \(\lambda_{\text{initial}}\) & $lr_{initial}$  & \textbf{$I$}\\
        \hline
        $10^{-3}$ & 0 & $1$ & 5 & 0.5 & $1 \times 10^{-3}, 2 \times 10^{-3} $ & ${Identity_{N\times N}}$ \\
        \hline
    \end{tabular}
    \vspace{0.05in}
    \caption{Hyperparameters used in training for shallow-ViT on MNIST dataset.}
    \label{tab:hyperparams_c7}
\end{table}

\end{document}