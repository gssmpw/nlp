\section{Related Work}
\subsection{Jailbreak with Prompting}
\label{sec:prompt}

As more robust large language models have been released, the concept of jailbreaking has emerged. Researchers have attempted to craft prompts, such as DAN (Do Anything Now) \cite{Shen2023DoAN}, that describe the characteristics of the model and try to persuade it to act accordingly. However, such works \cite{Shen2023DoAN,Shah2023ScalableAT,Wei2023JailbrokenHD} are resource-intensive and require significant human effort, making them inefficient.

To reduce human effort in attacking, most research focuses on automatically generating adversarial prompts. The earliest works primarily concentrate on white-box attacks \cite{Zou2023UniversalAT,Guo2021GradientbasedAA,Shin2020ElicitingKF}. Despite their success in achieving high attack success rates on widely-used models, these methods still suffer from high perplexity issues, which can easily be detected by defenses such as perplexity filters \cite{Alon2023DetectingLM}.
 
To mitigate this situation, several works aim to search for human-readable prompts. Notable works, such as AutoDAN \cite{Liu2023AutoDANGS}, apply genetic algorithms to produce new attack samples, while GPTFuzzer \cite{Yu2023GPTFUZZERRT}, motivated by software testing techniques, generates new samples by manipulating different operators. Both of these methods heavily rely on external handcrafted resources. Another approach to finding readable inputs is directly rephrasing \cite{Chao2023JailbreakingBB,Mehrotra2023TreeOA}. Inspired by notable prompting techniques \cite{Wei2022ChainOT,Yao2023TreeOT}, this method utilizes an additional model to improve the rephrasing of harmful requests based on the interaction history. However, the approaches mentioned above can only optimize inputs individually. Several works \cite{Wei2023JailbreakAG,AnilManyshotJ} use in-context learning \cite{Brown2020LanguageMA} by collecting harmful question-answer pairs as few-shot demonstrations, but these methods result in lower attack effectiveness.

\subsection{Jailbreak with Finetuning Attackers}
Compared to designing prompting algorithms to optimize inputs, several works \cite{Paulus2024AdvPrompterFA,Xie2024JailbreakingAA,Basani2024GASPEB,Wang2024DiffusionAttackerDP} focus on fine-tuning an attacker to generate adversarial suffixes tailored to each input. These approaches can be more efficient, as they aim to optimize a group of malicious instructions and offer higher flexibility, allowing the trained attacker to generate customized suffixes for each input. While this approach seems ideal, training a model may require deeper expertise and result in increased time and effort spent on hyperparameter tuning.

\subsection{Defenses against Jailbreaking}
To enhance the safety of models, defense methods have been proposed to counter malicious inputs. Defenses can be implemented in various ways, such as the perplexity filter \cite{Alon2023DetectingLM}, which detects abnormal inputs by evaluating the perplexity of input texts. ICD \cite{Wei2023JailbreakAG} proposes an in-context defense that concatenates few-shot demonstrations consisting of pairs of harmful inputs and refusal responses. SmoothLLM \cite{Robey2023SmoothLLMDL} introduces random perturbations to the input text. RPO \cite{Zhou2024RobustPO} uses a similar approach to GCG, optimizing defense prompts through token manipulation via gradients.

% \subsection{Beam Search for Adversarial Attack}
% The earliest work we found is BEAST \cite{Sadasivan2024FastAA}, which demonstrated that the beam search approach is both fast and efficient for individual attacks. A similar strategy is employed in AdvPrompter \cite{Paulus2024AdvPrompterFA}, which uses a query algorithm in its training process to generate suffixes using a beam search approach.