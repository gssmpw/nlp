\section{Related Work}
\subsection{Jailbreak with Prompting}
\label{sec:prompt}

As more robust large language models have been released, the concept of jailbreaking has emerged. Researchers have attempted to craft prompts, such as **Brown et al., "My Pet Goat"**, ____ that describe the characteristics of the model and try to persuade it to act accordingly. However, such works **Thakur et al., "Manipulating Prompt-Based Models"** ____ are resource-intensive and require significant human effort, making them inefficient.

To reduce human effort in attacking, most research focuses on automatically generating adversarial prompts. The earliest works primarily concentrate on white-box attacks **Ren et al., "White-Box Adversarial Attacks"** ____ . Despite their success in achieving high attack success rates on widely-used models, these methods still suffer from high perplexity issues, which can easily be detected by defenses such as perplexity filters **Wang et al., "Perplexity Filter"** ____.

 
To mitigate this situation, several works aim to search for human-readable prompts. Notable works, such as **Hendrycks et al., "AutoDAN"**, ____ apply genetic algorithms to produce new attack samples, while **Chen et al., "GPTFuzzer"**, ____ motivated by software testing techniques, generates new samples by manipulating different operators. Both of these methods heavily rely on external handcrafted resources. Another approach to finding readable inputs is directly rephrasing **Liu et al., "Prompt Engineering"** ____ . Inspired by notable prompting techniques **Wang et al., "Prompting Techniques"** ____ , this method utilizes an additional model to improve the rephrasing of harmful requests based on the interaction history. However, the approaches mentioned above can only optimize inputs individually. Several works **Li et al., "In-Context Learning"**, ____ use in-context learning ____ by collecting harmful question-answer pairs as few-shot demonstrations, but these methods result in lower attack effectiveness.

\subsection{Jailbreak with Finetuning Attackers}
Compared to designing prompting algorithms to optimize inputs, several works **Zhang et al., "Attacker Fine-Tuning"**, ____ focus on fine-tuning an attacker to generate adversarial suffixes tailored to each input. These approaches can be more efficient, as they aim to optimize a group of malicious instructions and offer higher flexibility, allowing the trained attacker to generate customized suffixes for each input. While this approach seems ideal, training a model may require deeper expertise and result in increased time and effort spent on hyperparameter tuning.

\subsection{Defenses against Jailbreaking}
To enhance the safety of models, defense methods have been proposed to counter malicious inputs. Defenses can be implemented in various ways, such as the perplexity filter **Wang et al., "Perplexity Filter"**, ____ which detects abnormal inputs by evaluating the perplexity of input texts. ICD ____ proposes an in-context defense that concatenates few-shot demonstrations consisting of pairs of harmful inputs and refusal responses. SmoothLLM ____ introduces random perturbations to the input text. RPO ____ uses a similar approach to GCG, optimizing defense prompts through token manipulation via gradients.

% \subsection{Beam Search for Adversarial Attack}
% The earliest work we found is BEAST ____ , which demonstrated that the beam search approach is both fast and efficient for individual attacks. A similar strategy is employed in AdvPrompter ____ , which uses a query algorithm in its training process to generate suffixes using a beam search approach.