\vspace{-5pt}
\section{A Framework for Safeguarding Contextual Privacy}
\label{sec:framework}
\vspace{-5pt}
Our goal is to develop a framework that acts as an intermediary between the user and LCA, and enables the user to detect whether their prompt incurs any contextual privacy violations, and judiciously reformulate the prompt to ensure contextual privacy. We first conduct a formative design study to guide our framework design.

\paragraph{User Study to Guide Our Framework Design:} 
We conducted a \textit{Wizard-of-Oz} formative user study to explore users' expectation of privacy when interacting with LCAs and to gather technical requirements for our framework.
Following established practices in early-stage interface design research \citep{ nielsen2000fiveusers, budiu2021fiveparticipants, nielsen1993mathematical}  where 5 participants are typically sufficient to identify major design insights, we conducted our study with six participants from our institution who were familiar with LLMs.
Using three mid-fidelity UX mockups (see Appendix \ref{sec::user_study_mockups}), we probed participants on their privacy concerns, reactions to privacy disclosures, and preferences for managing sensitive information. Each mockup simulated interactions where PII and sensitive information were detected and flagged. Participants provided feedback on different approaches to identifying, flagging, and reformulating sensitive information. 

Insights from this formative phase shaped several key design aspects of our framework, including distinguishing between essential and non-essential sensitive information, real-time feedback, user control over reformulations, and transparency around how sensitive information is handled and flagged. The participants rated the overall approach of the system highly, with a min and max rating of 7/10 and 9/10 respectively, providing initial validation for our approach to sensitive information detection and reformulation. For a detailed discussion of the study and how it impacted our design, see Appendix~\ref{sec::user_study}.
 
\noindent\textbf{Proposed Framework:}
We propose a framework that acts as an intermediary between the user and the conversation agent and enables the user to detect out-of-context sensitive information in the user prompt and judiciously reformulate the prompt to ensure contextual privacy.
The key components of the framework are outlined in Figure~\ref{fig:framework}.
When a user submits a prompt, our framework first determines the \textbf{context} and \textbf{subject} of the conversation. The context is divided into two components: the domain of the interaction (e.g., medical, legal, or financial) and the specific task the user aims to perform, such as seeking advice, requesting a translation, or summarizing a document. 
Context identification is guided by a taxonomy of common user tasks and sensitive contexts that go beyond PII \citep{mireshghallah2024trust} (see Appendix~\ref{domains_and_tasks}).







Once the context and subject are identified, our framework moves on to detecting sensitive information in the prompt. %
The framework categorizes the sensitive information into two spaces: (a) \textbf{essential information space}: sensitive details necessary to answer the user’s query, (b) \textbf{non-essential information space}: sensitive details that are unnecessary for answering the query and should be kept private.

In the example of Figure \ref{fig:framework}, the sensitive terms are \textit{``Jane'', ``single parent of two'', ``diabetes'', and ``affordable''}. While ``diabetes'' is essential for providing advice on treatment options, the other details---Jane's name, family situation, and financial concerns---are not required and thus classified as non-essential. 







Once contextually essential and non-essential information is identified, our framework improves contextual privacy by \textbf{reformulating} the prompt. This process includes removing, rephrasing, or redacting details within the non-essential information space, while preserving the user's intent. This way, we ensure that the user can still achieve the desired outcome effectively when the reformulated prompt is sent to the untrusted LCA. In our running example, a reformulated user's prompt could be \textit{``I need advice on managing a health condition and finding treatment options for diabetes''}, which protects non-essential sensitive details like the user’s name and personal circumstances, while maintaining the core intent of seeking treatment advice for diabetes.




After the reformulated prompt is generated, users can review, modify, or accept it, or revert to the original input. The review steps, shown by dashed boxes in Figure~\ref{fig:framework}, ensure user control, allowing them to achieve their desired balance between privacy and utility. The framework continues to highlight privacy implications as users adjust the suggested reformulation, helping them make informed choices about what information to share. Once finalized, the reformulated prompt is sent to the LLM-based conversational agent to obtain a response.


