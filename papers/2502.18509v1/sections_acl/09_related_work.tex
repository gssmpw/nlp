\section{Related Work}
\label{sec:relatedwork}
We fully contextualize our contributions in regard to existing literature here.

\paragraph{LLM Privacy-Preserving Techniques.}
A significant body of research on privacy preservation in LLMs has focused on the training phase \citep{zhang2024dpzero,chua2024mind,yu2021differentially,yue2022synthetic,li2021large}. Techniques like differential privacy (DP)~\citep{dwork2006calibrating} have been used to prevent LLMs from memorizing sensitive information during training. Additionally, data sanitization strategies, such as deduplication and anonymization, have been used to reduce privacy risks by removing sensitive data from training data~\citep{lison2021anonymisation, kandpal2022deduplicating}. After training, machine unlearning methods have emerged to help eliminate any retained private data~\citep{carlini2019secret, biderman2024emergent, mccoy2023much, zhang2023counterfactual, carlini2021extracting, nasr2023scalable, xu2024machine}. However, inference-phase privacy protection has received less attention, with limited approaches, such as PII detection and DP decoding, targeting the risks of exposing sensitive information in real-time interactions with LLMs~\citep{majmudar2022differentially,carey2024dp,wu2023privacy,tang2023privacy,hong2023dp,edemacu2024privacy}. Recently, \citet{mireshghallah2023can} highlighted this gap, showing that LLMs often fail to protect private information in context and emphasizing the need for better privacy-preserving techniques. Our approach addresses this need by offering real-time, context-aware privacy guidance during user interactions, allowing individuals to better manage what information they disclose during conversations with LLMs.


\paragraph{Privacy Risks in Human-LLM Interactions.}
Self-disclosure during human-machine interactions can result in unintended sharing of sensitive information. For example, \citet{ravichander2018empirical} found that users tend to reciprocate with automated systems, revealing more personal information over time. Building on this, \citet{zhang2024s} examined the privacy risks faced by users interacting with LLMs, showing that human-like responses can encourage sensitive disclosures, complicating privacy management. \citet{mireshghallah2024trust} further advanced this discussion by highlighting the limitations of PII detection systems, showing that users often disclose sensitive information that goes beyond PII~\citep{cummings2023challenges, dou2023reducing}. Our work builds on these efforts by showing that users frequently disclose unnecessary information during interactions with LLMs, which can be contextually sensitive and unrelated to their intended goals. We develop a system that detects such information and offers reformulation suggestions to guide users toward more privacy-aware interactions.

\paragraph{Data Minimization in ML.}
The principle of data minimization, central to privacy regulations like GDPR~\citep{voigt2017eu}, has recently been a key focus in ML research. For example, \citet{ganesh2024data} formalized data minimization within an optimization framework for reducing data collection while maintaining model performance. \citet{tran2024data} expanded on this by showing that individuals can disclose only a small subset of their features without compromising accuracy, thus minimizing the risk of data leakage. While both approaches focus on reducing the amount of data processed at inference time, our work applies data minimization in real time, guiding users to share only necessary information with LLMs. We integrate contextual integrity to ensure that the disclosed information aligns with the context of the conversation, ensuring GDPR compliance through a user-driven, context-aware approach.

\paragraph{Operationalizing Contextual Integrity (CI).}
Research on contextual privacy in LLMs is rapidly expanding. For instance, \citet{mireshghallah2023can} introduced a benchmark to evaluate the privacy reasoning abilities of LLMs at varying levels of complexity, while \citet{shvartzshnaider2024llm} proposed a comprehensive framework using CI to assess privacy norms encoded in LLMs across different models and datasets. CI has also been integrated into various practical systems to safeguard privacy across diverse domains. For example, \citet{shvartzshnaider2019vaccine} employed CI to detect privacy leaks in email communications, and \citet{kumar2020aquilis} applied CI to provide mobile users with real-time privacy risk alerts. In smart home ecosystems, \citet{malkin2022runtime,abdi2021privacy} used CI to analyze and enforce privacy norms. \citet{hartmann2024can} considered scenarios where a local model queries a larger remote model, leveraging CI to ensure only task-relevant data is shared. Similarly, \citet{bagdasaryan2024air} used CI to restrict AI assistantsâ€™ access to only the information necessary for a given task, and \citet{ghalebikesabi2024operationalizing} applied CI to ensure form-filling assistants follow contextual privacy norms when sharing user information. While these studies focus on aligning AI assistants' actions with privacy norms, our work shifts the perspective toward empowering privacy-conscious users. By integrating CI into our framework, we aim to educate users in real time about contextually sensitive disclosures and offer proactive guidance to help manage privacy risks. This user-centered approach not only protects sensitive information during AI interactions but also promotes long-term privacy awareness---an aspect often overlooked in system-oriented solutions.



