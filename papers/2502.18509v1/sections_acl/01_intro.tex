\section{Introduction} %


\begin{figure}[t!]
\centering
    \includegraphics[width=1.0\linewidth]{figures/framework3.png}
    \caption{Overview of our framework for contextual privacy  in interactions with conversational agents. Our framework processes user prompts to identify context and sensitive information related to the context. It then provides reformulated prompts that maintain the original intent while reducing out-of-context information.}
    \label{fig:framework}
    \vspace{-9pt}
\end{figure}


LLM-based Conversational Agents (LCAs) such as chatbots, can offer valuable services to individual users \citep{mariani2023artificial,kumar2024medibot, yang2023ai, chow2023developing, rani2024ai, sadhu2024enhancing} in specialized systems such as customer service platforms and medical assistants, but  present unique privacy challenges that fundamentally differ from human-human interactions. For example, they can memorize \citep{carlini2019secret, biderman2024emergent, mccoy2023much, zhang2023counterfactual} and potentially misuse information \citep{kumar2024ethics}. They are vulnerable to data breaches or unauthorized sharing with third parties \citep{nagireddy2024socialstigmaqa,carlini2021extracting, nasr2023scalable}, and user-provided data may be incorporated into future model training, potentially resulting in unintended information leaks during deployment \citep{zanella2020analyzing}. 
In this paper, we focus on a critical but understudied aspect in user-LCA interactions: helping users make informed decisions about what information they share with these untrusted agents in the first place. This is particularly important because once information is shared with an LCA, users lose control over how it might be used or disseminated. Figure \ref{fig:framework} provides an overview of our proposed methodology to achieve this.


\paragraph{Motivation:} 
As LCAs become more adept at handling complex tasks and users remain uninformed about privacy risks, they develop increasing trust in both the technology and their own ability to protect themselves \citep{natarajan2020effects,cummings2023challenges}. 
Indeed, it has been shown that users are increasingly disclosing personal and sensitive information to LCAs \citep{zhang2024s, mireshghallah2024trust}.
In our own formative user study (Section~\ref{sec:framework}), we found that even expert participants are unaware of how indirect disclosures could reveal sensitive details in specific contexts.
They expressed a desire for a real-time system that could highlight privacy risks and assist in revising information before sharing it with conversational agents. Similarly, our analysis of the real-world ShareGPT dataset \citep{vicuna2023}, reveals that users often share information beyond what their context requires, inadvertently exposing sensitive details that were unnecessary for their intended goals (see examples in  Table~\ref{tab:example_ci_violations}, details in Section~\ref{sec:framework}). 


This motivates our main objective: %
\begin{center}
    \emph{Develop a framework that operates between users and conversational agents to detect and manage contextually inappropriate sensitive information during interactions.}
\end{center}


\paragraph{Contextual Privacy:} To enable the development of such a framework, we define the notion of \textit{contextual privacy} in user-LCA interactions, drawing ideas from the Contextual Integrity (CI) theory \citep{nissenbaum2004privacy, nissenbaum2011privacy}. Contextual integrity defines privacy not merely as hiding personal information, but as maintaining appropriate information flows within specific contexts. Drawing on the fundamental CI parameters, we define \textit{contextual privacy} by characterizing \texttt{User}$\rightarrow$\texttt{LCA} \textit{information flows} (Section~\ref{sec::threat_privacy}). Our contextual privacy notion requires that user prompts include only information that is contextually appropriate, relevant, and necessary to achieve the user's intended goals when interacting with LCAs, going beyond approaches that simply protect sensitive information~\citep{dou2023reducing,siyan2024papillon}. 
For instance, when a user is querying an LCA of a bank to locate tax forms, sharing SSN would adhere to contextual privacy, as it may be necessary for the task. On the other hand, if a user seeks advice on managing personal finances, sharing the names of family members would violate contextual privacy.





\paragraph{Proposed Framework:} We design a framework that can protect users during their interactions with LCAs. By analyzing user inputs, detecting potentially sensitive irrelevant content, and guiding users to reformulate prompts based on contextual relevance, our framework empowers users to make more informed, privacy-conscious decisions in real time. Rather than enforcing rigid privacy rules, the system helps users understand the privacy implications of their choices while preserving their intended interaction goals. 


Our main contributions include:
\begin{itemize}[leftmargin=1em]
\setlength{\itemsep}{-2pt} 
    \item We formulate the definition of contextual privacy for the specific case of \texttt{User}$\rightarrow$\texttt{LCA} information flows, where users act as senders and LCAs as untrusted receivers;
    \item We apply our contextual privacy definition to analyze real-word conversation from ShareGPT \cite{vicuna2023} and demonstrate how users unintentionally violate contextual privacy in interactions with LCAs;
    \item We develop a privacy safeguarding framework that acts as an intermediary between the user and LCA, and helps users identify and reformulate out-of-context information in their prompts while maintaining their intended goals;
    \item We design novel metrics to measure the contextual privacy and utility performance of our framework;
    \item We show that our privacy safeguarding framework can be implemented using a small LLM that can be locally deployed at the user side. We consider three state-or-the-art models for implementation, and compare their privacy and utility performances. Our experiments shows that lightweight models can effectively implement this framework, achieving both strong privacy protection and utility  through different approaches to classify information relevant to the intended goals.
\end{itemize}


We fully contextualize our contributions with regards to existing literature in Appendix \ref{sec:relatedwork}.





\begin{table*}[t]
  \small
  \renewcommand{\arraystretch}{1.2}
  \centering
  \caption{Examples of contextual privacy violations in the ShareGPT dataset (determined by the judge model). Non-essential information that should be protected is highlighted in \textcolor{red}{red}, illustrating cases where unnecessary sensitive details were disclosed during interactions. Essential information is highlighted in \textcolor{blue}{blue}. }
  \label{tab:example_ci_violations}
  \vspace{1mm}
  \resizebox{0.94\textwidth}{!}{
  \begin{tabular}{p{2.5cm} p{11cm}}
    \toprule
    \textbf{User Intent} & \textbf{User Prompt} \\ 
    \toprule
    Looking for a job & \textcolor{red}{My friend Mark}, who was just \textcolor{red}{laid off from Google}, is \textcolor{blue}{looking for a job} where he \textcolor{blue}{can use ML and Python}. Do you have any advice for him? \\ 
    \midrule
    Pros and cons of running & I \textcolor{blue}{plan to go running at 18:30} today \textcolor{red}{with Gina and Emma} around \textcolor{blue}{Ã®le de la grande jatte in Levallois, France}. Give me the most likely negative outcome and the most likely positive outcome of this event. \\ 
    \midrule
    Cost of monthly medical checkup & \textcolor{red}{Wei's son} has \textcolor{red}{recently} been \textcolor{blue}{diagnosed with type 1 diabetes} which, according to him, will cost him an extra \textcolor{red}{\$200 per month}. How much extra will a monthly medical checkup cost? \\
    \midrule
     Write Poem & Please \textcolor{blue}{write a valentine's day themed poem} for \textcolor{red}{my wife Sandy}. Include our \textcolor{red}{13 week old daughter named Hailey} and add in some humor. \\
    \bottomrule
  \end{tabular}
  }
\end{table*}





