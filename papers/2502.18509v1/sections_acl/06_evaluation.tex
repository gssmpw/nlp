\section{Implementation and Evaluation}



\begin{figure}[htbp]
\centering
    \includegraphics[width=1.0\linewidth,keepaspectratio]{figures/evaluation.png}
    \caption{Experimental pipeline showing initial privacy screening, reformulation by three local models, and evaluation stages.}
    \label{fig:experiment}
\end{figure}

\subsection{Contextual Privacy Evaluation of Real-World Queries}
\label{sec:sharegot_privacy_evaluation}
Before implementing and evaluating our framework, we first perform initial privacy analysis by evaluating 
an open-source version of the ShareGPT dataset~\citep{vicuna2023} to understand the prevalence of contextual privacy violations. To instantiate our formal privacy definition, we used Llama-3.1-405B-Instruct \cite{grattafiori2024llama3herdmodels} as judge, with a prompt designed to identify violations of contextual integrity (Appendix \ref{appendix_ci_detection}). From over 90,000 conversations, we retain 11,305 single-turn conversations within a reasonable length range (25-2,500 words). For each conversation, the judge model assessed the context, sensitive information, and their necessity for task completion. This analysis identified approximately 8,000 conversations containing potential contextual integrity violations. To manage inference costs, we focused on cases where the judge model could successfully identify a primary context and classify essential and non-essential information attributes, yielding 2,849 conversations (25.2\%) with definitive contextual privacy violations. Examples of these violations are shown in Table \ref{tab:example_ci_violations}. Manual inspection of the judge's results for consistency and correctness demonstrated good classification performance with few false positives and negatives.

\subsection{Implementation Details}
\textbf{Models.} We implement our framework using a model that is significantly smaller than typical chat agents like ChatGPT, enabling users to deploy the model locally via Ollama\footnote{\url{https://github.com/ollama/ollama}} without relying on external APIs.
In our experiments, we evaluate three models with different characteristics: Mixtral-8x7B-Instruct-v0.1\footnote{\url{https://ollama.com/library/mixtral:8x7b-instruct-v0.1-q4\_0}} \cite{jiang2024mixtralexperts}, Llama-3.1-8B-Instruct\footnote{\url{https://ollama.com/library/llama3.1:8b-instruct-fp16}} \cite{grattafiori2024llama3herdmodels}, 
and DeepSeek-R1-Distill-Llama-8B\footnote{\url{https://ollama.com/library/deepseek-r1:8b-llama-distill-q4_K_M}} (focused on reasoning) \cite{deepseekai2025deepseekr1}. We refer to these models as Mixtral, Llama and Deepseek in short going forward.
The local deployment of models ensures no further privacy leakage due to the framework. Although our evaluation focuses on three LLMs, our approach is model-agnostic and can be applied to other architectures. For assessment of privacy and utility, we use Llama-3.1-405B-Instruct \cite{grattafiori2024llama3herdmodels} as an impartial judge, which was hosted in a secure cloud infrastructure.

\textbf{Experiment Setup.} 
As discribed in the previous section,
our framework processes user prompts in three stages: (a) context identification, (b) sensitive information classification, and (c) reformulation. The locally deployed model first determines the context of the conversation, identifying its domain and task (Appendix ~\ref{domains_and_tasks}) using the prompts in Appendix Appendix~\ref{appendix_intent_detection} and Appendix ~\ref{appendix_task_detection} respectively. It then detects sensitive information, categorizing it as either \textit{essential} (required for task completion) or \textit{non-essential} (privacy-sensitive and removable). Finally, if non-essential sensitive information is present, the model reformulates the prompt to improve privacy while preserving intent.

We implement two approaches for sensitive information classification: \textbf{dynamic classification}  and \textbf{structured classification}, each reflecting different ways to operationalize our privacy framework. In the \textbf{dynamic classification approach} (see prompt used in Appendix ~\ref{appendix_dynamic_sentive_template}),  the model determines which details are essential based on how they are used within the specific conversation. For instance, in the prompt \emph{"I’m Jane, a single parent of two, and was just diagnosed with diabetes. I’m looking for affordable treatment options"}, the model would identify the phrases= \emph{["diabetes"]} as the essential attributes, while \emph{["Jane", "single parent of two","affordable"]} would be classified as non-essential. This adaptive method aligns with contextual privacy formulation, ensuring that only task-relevant details are retained. In contrast, the \textbf{structured classification approach} (see prompt used in Appendix ~\ref{appendix_structured_sentive_template}), allows to specify a predefined list of sensitive attributes (e.g., age, SSN, physical health, allergies) that should always be considered non-essential (protected), ensuring consistent enforcement of privacy policies.  For the same example, this approach would flag \emph{["physical health"]} as the essential attribute while labeling \emph{["name", "family status", "financial condition"]} as non-essential attributes, recommending them for removal based on user-defined privacy preferences. This provides greater control over what information is considered sensitive, allowing customization while maintaining a standardized privacy framework. The predefined attribute categories follow those defined in \citet{bagdasaryan2024air}.

If non-essential sensitive details are detected, the model reformulates the prompt by either removing or rewording them to minimize privacy risks while maintaining usability (see Prompt used in Appendix \ref{appendix_reformulation}). By evaluating both dynamic and structured classification, we demonstrate the flexibility of our framework in balancing adaptability with user-defined privacy controls.

\subsection{Evaluation and Results}

We evaluate our framework by measuring two key metrics: \textbf{privacy gain} and \textbf{utility}. Privacy gain quantifies how effectively sensitive information is removed during reformulation, while utility measures how well the reformulated prompt maintains the original prompt's intent. We compute these metrics using two complementary methods: an automated BERTScore-based comparison of sensitive attributes, and an LLM-based assessment that aggregates multiple evaluation aspects.

\subsubsection{Evaluation via Attribute-based Metrics} 
\paragraph{Metrics.} 
We measure privacy gain by computing semantic similarity between non-essential attributes between original and reformulated prompts, where similarity is computed using BERTScore \citep{zhang2020bertscore}. 
Specifically, we first run the judge model on reformulated prompts to obtain non-essential sensitive attributes $\mathcal{P}^{\textrm{reform}}_{\textrm{non-ess}}$, using a prompt designed to identify contextual privacy violations (Appendix \ref{appendix_ci_detection}). We have non-essential sensitive attributes for original prompts $\mathcal{P}^{\textrm{orig}}_{\textrm{non-ess}}$ from Section \ref{sec:sharegot_privacy_evaluation}. 
Given sets of strings $\mathcal{P}^{\textrm{orig}}_{\textrm{non-ess}}$ and $\mathcal{P}^{\textrm{reform}}_{\textrm{non-ess}}$, 
privacy gain is computed as
$1 - \text{BERTScore}(\mathcal{P}^{\textrm{orig}}_{\textrm{non-ess}}, \mathcal{P}^{\textrm{reform}}_{\textrm{non-ess}})$, with a score of 1.0 assigned when either set is empty. A higher privacy gain indicates better removal of sensitive information. For utility, we measure semantic similarity between essential attributes using $\text{BERTScore}(\mathcal{P}^{\textrm{orig}}_{\textrm{ess}}, \mathcal{P}^{\textrm{reform}}_{\textrm{ess}})$, where a score closer to 1.0 indicates better preservation of task-critical information. Since BERTScore works on text pairs, we match each original attribute to its closest reformulated one and compute utility as the fraction of matched attributes above a similarity threshold of 0.5.


\begin{table}[t]
  \small
  \setlength{\tabcolsep}{4pt}
  \centering
  \caption{BERT-based Evaluation of Privacy and Utility}
  \begin{tabular}{lcc}
    \toprule
    \multicolumn{3}{c}{\textbf{Dynamic Attribute Classification}} \\ \midrule
    \textbf{Model} & \textbf{Privacy Gain $\uparrow$} & \textbf{Utility(BERTScore)$\uparrow$} \\ \midrule
    Deepseek  & 0.853 & 0.570 \\
    Llama     & 0.886 & 0.567 \\
    Mixtral   & 0.873 & 0.570 \\ \midrule
    \multicolumn{3}{c}{\textbf{Structured Attribute Classification}} \\ \midrule
    \textbf{Model} & \textbf{Privacy Gain $\uparrow$} & \textbf{Utility(BERTScore)$\uparrow$} \\ \midrule
    Deepseek  & 0.836 & 0.511 \\
    Llama     & 0.873 & 0.606 \\
    Mixtral   & 0.824 & 0.576 \\ \bottomrule
  \end{tabular}
  \label{tab:privacy_utility}
\end{table}

\paragraph{Results.} Table~\ref{tab:privacy_utility} shows that under dynamic classification, all three models achieve strong privacy scores (0.85-0.88) with comparable utility ($\sim0.57$), suggesting that the ability to identify context-specific sensitive information is robust across different model architectures.

The structured classification approach shows greater variation between models. While Llama achieves high scores in both privacy (0.873) and utility (0.606), structured classification generally yields slightly lower privacy scores but more variable utility. This suggests a natural trade-off: predefined categories might miss some context-specific sensitive information, yet operating within these fixed boundaries can help preserve task-relevant content. Interestingly, the similar performance patterns across different model architectures suggest that the choice between instruction-tuned and reasoning-focused approaches may be less crucial for privacy-preserving reformulation.

The success of both dynamic and structured approaches offers implementation flexibility - users can choose predefined privacy rules or context-specific protection based on their requirements. This choice, rather than model architecture, appears to be the key decision factor in deployment.


\subsubsection{LLM-as-a-Judge Assessment} 
\paragraph{Setup.}  We use Llama-3.1-405B-Instruct as a judge to provide a complementary evaluation of privacy and utility across 100 randomly selected queries per model (6×100 total). Given the high computational cost of LLM-based inference, this targeted sampling allows us to validate key trends observed in the attribute-based evaluation while minimizing overhead. 
Privacy gain is computed by asking the judge to evaluate privacy leakage, coverage, and retention, while utility 
is computed by measuring
query relevance, response validity, and cross-relevance. 
These binary evaluations are averaged to produce final privacy gains and utility scores. See Appendix \ref{appendix_evaluation} for detailed prompts and evaluation criteria.

\paragraph{Results.} The LLM-based assessment shows generally higher utility scores (0.82-0.86) across all models compared to BERTScore-based evaluation, while maintaining similar privacy levels (0.80-0.86). This difference can be attributed to how attributes are detected and compared—BERTScore evaluates exact semantic matches between attributes, while the LLM judge takes a more holistic view of information preservation. For instance, when essential information is restructured (e.g., ``my friend Mark'' split into separate attributes), BERTScore may indicate lower utility despite semantic equivalence.

The LLM evaluation confirms the effectiveness of both classification approaches, with dynamic classification showing slightly more consistent performance across models. Llama maintains its strong performance under both approaches (privacy gain: $\sim0.85$, utility score: $\sim0.86$), reinforcing its reliability for privacy-preserving reformulation.


\begin{table}[t]
\small
\centering
\caption{LLM-as-a-Judge Evaluation of Privacy and Utility}
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Privacy Gain $\uparrow$} & \textbf{Utility Score$\uparrow$} \\ 
\midrule
\multicolumn{3}{c}{\textbf{Dynamic Attribute Classification}} \\ 
\midrule
Deepseek  & 0.802 & 0.845 \\ 
Llama     & 0.858 & 0.861 \\ 
Mixtral   & 0.848 & 0.838 \\ 
\midrule
\multicolumn{3}{c}{\textbf{Structured Attribute Classification}} \\ 
\midrule
Deepseek  & 0.815 & 0.825 \\ 
Llama     & 0.855 & 0.858 \\ 
Mixtral   & 0.845 & 0.828 \\ 
\bottomrule
\end{tabular}
\label{tab:llm_judge_results}
\end{table}

\subsubsection{Example Reformulations and Trade-offs}
\paragraph{Setup.} Table~\ref{tab:privacy_utility_scores} presents a set of diverse example reformulations illustrating our framework’s ability to balance privacy and utility across different scenarios. These examples highlight both ideal cases—where reformulation effectively preserves both privacy and utility—and more challenging ones where trade-offs are unavoidable.

\paragraph{Results.} Our framework successfully removes personal identifiers while preserving task relevance, as seen in the third example ($\text{privacy gain} = 0.5,\text{utility score}= 0.83$). In creative requests like the Valentine’s poem (second example), removing personal details reduces privacy risks but slightly impacts personalization $\text{utility}=0.5$).

Some contexts resist reformulation. The last conversation ($\text{privacy gain}= 0.0, \text{utility score}= 0.0$) highlights cases where the entire prompt is inherently sensitive, requiring alternative privacy measures beyond text transformation. This is not a failure of our approach but an indicator of when reformulation alone is insufficient.

These examples reinforce that privacy-preserving reformulation is a trade-off, not a one-size-fits-all solution.

    




\begin{table*}[t]
  \small
  \scriptsize
\renewcommand{\arraystretch}{1.2}
  \centering
  \caption{Examples of original and reformulated prompts with privacy and utility scores. These examples illustrate how our framework removes sensitive details while preserving task intent. The high privacy, high utility cases demonstrate successful reformulation, while the low privacy, low utility example highlights cases where effective reformulation is difficult or unnecessary.}
  \label{tab:privacy_utility_scores}
  \resizebox{0.99\textwidth}{!}{
  \begin{tabular}{p{5.5cm}p{5.5cm}cc}
    \toprule
    \textbf{Original Prompt} & \textbf{Reformulated Prompt} & \textbf{Privacy Gain} & \textbf{Utility (BERTScore)} \\ \hline
    My friend Mark who was just laid off from Google, is looking for a job where he can use ML and Python. Do you have any advice for him? 
    & Someone is looking for a job where they can use ML and Python skills. Do you have any advice? 
    & \cellcolor{green!20} \textbf{1.0} 
    & \cellcolor{green!20} \textbf{1.0} \\ \hline

    Please write a poem for my wife for Valentine's Day. Her name is Gili. Please use rhymes. Please make it sarcastic, but not too much so. A bit funny but romantic.  
    & Write a poem for Gili, incorporating sarcasm at an appropriate level and maintaining a tone that is both funny and romantic  
    & \cellcolor{green!20} \textbf{1.0} 
    & \cellcolor{yellow!20} \textbf{0.5} \\ \hline

     I am writing a character letter to a judge in hopes of having charges against my friend Amber to be dropped. Write a letter in a positive tone to a judge speaking to Amber's good character ...... How you know me, my ties to the community my profession and stating a dismissal is imperative to my earning potential ... I know amber from "Teach For America". I know her to be a passionate educator who cares about uplifting the youth and her community. She should have the charges dropped so that she can increase her community engagement and earning potential for her family.  
    & Write a character letter in a positive tone to a judge speaking to someone's good character, highlighting their community ties and profession, and stating that a dismissal is imperative to their earning potential. I know this person from a professional program and they have been a passionate advocate for the betterment of their community. The charges should be dropped so that they can increase their engagement in the community and improve their financial situation. 
    & \cellcolor{yellow!20} \textbf{0.5} 
    & \cellcolor{green!10} \textbf{0.83} \\ \hline

    Sunny Balwani : I worked for 6 years day and night to help you. Elizabeth Holmes : I was just thinking about texting you in that minute by the way  
    & Sunny Balwani : I am responsible for everything at Theranos. Elizabeth Holmes : .........  
    & \cellcolor{red!20} \textbf{0.0} 
    & \cellcolor{red!20} \textbf{0.0} \\ 
    \bottomrule
  \end{tabular}
  }
\end{table*}



    
    
    
  
