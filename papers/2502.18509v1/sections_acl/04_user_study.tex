\section{User Study to Guide System Design}
\label{sec::user_study}

To explore users' perceptions of privacy with LCAs and gather technical requirements for our framework, we conducted a Wizard-of-Oz formative user study with six participants from our institution who were generally familiar with LLMs. 

The study involved a 30-minute semi-structured interview where participants were presented with three mid-fidelity UX mockups, each designed to demonstrate different ways private and sensitive information could be detected and remediated (see Appendix~\ref{sec::user_study_mockups}). These mockups, featuring synthetic examples inspired by real-world patterns in the ShareGPT dataset, were created to expose participants to targeted privacy risks, such as unintentional PII and sensitive data disclosures. We used these mockups to probe participants' views on their own privacy practices, their thoughts about privacy disclosures, and their preferences for managing sensitive information in conversations. The study provided insights into people's views on the identification, flagging, and reformulation of sensitive data, shaping the core elements of our framework.



\begin{itemize}[leftmargin=1em]
    \item \textbf{Perceived privacy control}. Participants initially believed their efforts to protect their privacy when using real-world LLM applications were effective due to how they kept conversations vague. After they saw real examples of indirect privacy leaks in the mockups, many participants expressed greater concern about unintentionally sharing private information. \textbf{Design impact}: This insight emphasized the importance of identifying both direct and indirect privacy risks during LLM interactions in our system.
    \item \textbf{Visual identification of sensitive information}. Prototype B's color-coded differentiation between PII, necessary, and unnecessary information was praised for making privacy risks clearer and easier to understand. \textbf{Design impact}: Based on this feedback, we included the ability to differentiate between different kinds of sensitive information disclosures to help inform users' decision-making.
    \item \textbf{Reformulation preferences}. Although some participants preferred doing the work of reformulating their LLM prompts themselves, most wanted the system to offer (at least) one reformulated prompt suggestion, with the option to generate new suggestions. A few participants suggested offering multiple reformulations at once, selected across a spectrum of privacy-utility tradeoffs. In this way, users can balance their level of privacy protection with the utility of the output. \textbf{Design impact}: We designed our system to present one reformulation recommendation at a time, but with the flexibility to generate new alternative reformulations. In future iterations of our system, we plan to explore how to generate multiple reformulation options across varied privacy-utility tradeoffs.
    \item \textbf{User control and real-time feedback}. Real-time feedback and user control over editing flagged prompts were highly valued. Participants preferred having the system automatically generate reformulations, but they wanted the ability to make any necessary final adjustments. \textbf{Design impact}: We implemented a review step where users can edit, accept, or proceed with the original input before final submission to the LLM, providing the flexibility users requested.
    \item \textbf{Positive reception}. Participants responded positively to the systemâ€™s potential for managing sensitive information, with an average rating of $8.7 (\pm 0.87)$ on the importance of detecting and flagging sensitive details. \textbf{Design impact}. This feedback reinforced the central role of sensitive information detection in our framework, highlighting its perceived value to users.
    \item \textbf{Clarity and transparency}. Participants expressed a strong desire for transparency about how the system operates, including which tools or models are being used, and the meaning of key terms like ``necessary'' versus ``unnecessary'' information. \textbf{Design impact}: Our framework ensures transparency by detailing how sensitive information is identified and handled, including the models used, how they are applied, deployed, and how data is managed. We recommend real-world implementations do the same to build user trust.
    \item \textbf{Broader application}. A few participants suggested applying the tool to other contexts beyond LLM chat interfaces, such as search engines. \textbf{Design impact}:  This feedback highlights the importance of managing sensitive information and the broader applicability of our approach to other contexts.
\end{itemize}


\subsection{User Study Mockups}
\label{sec::user_study_mockups}
\begin{figure*}[t]
    \centering
    
    \begin{minipage}{0.8\textwidth} %
        \centering
        \includegraphics[width=\linewidth]{figures/user_study_1.png}
        \subcaption{Examples of unintentional disclosures shown to participants}
    \end{minipage}

    \vspace{0.5cm} %

    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/user_study_2.png}
        \subcaption{Mockup 1: Display all detected sensitive info}
    \end{minipage}
    \hfill
    \begin{minipage}{0.52\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/user_study_4.png}
        \subcaption{Mockup 3: Rewrite the user's message for them}
    \end{minipage}

    \vspace{0.5cm} %

    \begin{minipage}{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/user_study_3.png}
        \subcaption{Mockup 2: Color Code information and suggest reformulations}
    \end{minipage}

    \label{fig:mockups}
\end{figure*}
