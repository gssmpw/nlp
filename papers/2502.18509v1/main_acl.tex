\pdfoutput=1

\documentclass[11pt]{article}

\usepackage[final]{acl}

\usepackage{times}
\usepackage{latexsym}
\usepackage{listings}
\lstset{breaklines=true}

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}

\input{math_commands.tex}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{float}
\usepackage{enumitem}
\newtheorem{definition}{Definition}
\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{microtype}

\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{tcolorbox}

\hypersetup{
    colorlinks,
    linkcolor={blue},%
    citecolor={green!60!black},
    urlcolor={blue}%
}

\newcommand{\ivy}[1]{{\textcolor{teal}{[Ivy: #1]}}{}}
\newcommand{\swanand}[1]{{\textcolor{blue}{[Swanand: #1]}}{}}
\newcommand{\ad}[1]{{\textcolor{orange}{[Amit: #1]}}{}}
\newcommand{\hao}[1]{{\color{purple}#1}}
\newcommand{\nrk}[1]{{\textcolor{magenta}{[NRK: #1]}}{}}
\newcommand{\new}[1]{{\color{red}#1}}


\title{Protecting Users From Themselves:\\ Safeguarding Contextual Privacy in Interactions with Conversational Agents}

\author{Ivoline C. Ngong\thanks{Graduate student at University of Vermont. Work done during summer internship at IBM Research.},~Swanand Kadhe, Hao Wang, Keerthiram Murugesan, Justin D. Weisz,\\ 
\textbf{Amit Dhurandhar, Karthikeyan Natesan Ramamurthy} \\
IBM Research. \\
\texttt{kngongiv@uvm.edu,}\\
\texttt{\{swanand.kadhe,keerthiram.murugesan\}@ibm.com,}\\
\texttt{hao-wang@redhat.com},
\texttt{\{jweisz,adhuran,knatesa\}@us.ibm.com}
}


\begin{document}
\maketitle
\begin{abstract}
Conversational agents are increasingly woven into individuals' personal lives, yet users often underestimate the privacy risks involved. The moment users share information with these agents (e.g., LLMs), their private information becomes vulnerable to exposure. In this paper, we characterize the notion of contextual privacy for user interactions with LLMs. It aims to minimize privacy risks by ensuring that users (sender) disclose only information that is both relevant and necessary for achieving their intended goals when interacting with LLMs (untrusted receivers). Through a formative design user study, we observe how even ``privacy-conscious'' users inadvertently reveal sensitive information through indirect disclosures. 
Based on insights from this study, 
we propose a locally-deployable framework that operates between users and LLMs, and identifies and reformulates out-of-context information in user prompts. Our evaluation using examples from ShareGPT shows that lightweight models can effectively implement this framework, achieving strong gains in contextual privacy while preserving the user's intended interaction goals through different approaches to classify information relevant to the intended goals. 
\end{abstract}


\input{sections_acl/01_intro}
\input{sections_acl/02_privacy_definition.tex}

\input{sections_acl/05_our_framework.tex}
\input{sections_acl/06_evaluation.tex}
\input{sections_acl/08_discussion.tex}
\input{sections_acl/11_limitations.tex}


\clearpage
\bibliography{custom}

\appendix
\input{sections_acl/09_related_work}
\input{sections_acl/04_user_study}
\onecolumn
\input{sections_acl/appendix_domains_and_tasks}
\input{sections_acl/appendix_prompts}
\input{sections_acl/appendix_reformulations}



\end{document}
