\section{Experiments}
\label{sec:experiments}

\begin{figure*}[t]
\vspace{-6mm}
    \centering
    \includegraphics[width=0.8\linewidth]{figs/compare.pdf}
    \vspace{-4mm}
    \caption{\textbf{Qualitative comparison} with the baseline for generating a sequence of novel view images.  
    The results demonstrate that our method synthesizes more consistent multi-view images compared to our baseline model (Zero123). In addition, compared to SyncDreamer, our method visually maintains better similarity to the conditioned image and appears more natural.}
    \label{fig:sota_compare}
\vspace{-5mm}
\end{figure*}

\subsection{Experimental Setups}
\textbf{Dataset.}
Following previous work~\cite{zero123, SyncDreamer}, we evaluate our work on the Google Scanned Object (GSO)~\cite{GSO} dataset to verify the zero-shot novel view image synthesis capability. 
We also provide results for additional datasets in the Supplementary Material.
Specifically, we randomly select 30 objects from the GSO dataset with various object categories. 
Unlike recent approaches~\cite{mvdream, SyncDreamer} that aim to enhance the consistency of novel view synthesis models by generating multiple fixed-view images, our method can generate images from any camera pose and any number of views. Therefore, we conduct experiments under different camera pose settings to validate our approach:
specifically, 
1) \textit{16-views with free camera pose}: for each object, we circularly render 16 views with the elevation angles ranging in $[-10\degree, 40\degree]$ and the azimuth angles are evenly distributed in $[0\degree, 360\degree]$. 
2) \textit{16-views with fixed camera pose}: We maintain a constant elevation angle of $30\degree$ and uniformly sample azimuth angles (same as SyncDreamer~\cite{SyncDreamer}).
3) \textit{32-views with free camera pose}: Similar to the first setting, but we sample 32 views.
It's important to note that our method does not require additional training or fine-tuning on any datasets.

\noindent\textbf{Metrics.}
To validate the effectiveness of our method, we mainly evaluate it based on three criteria:
1) \textit{Quality Score}. We evaluate the image quality of synthesized multi-view images by measuring their similarity with ground truth images. Following prior research~\cite{zero123, sparsefusion}, we report the similarity between the synthesized images and the ground truth images with standard metrics: PSNR, SSIM~\cite{ssim}, and LPIPS~\cite{lpips}.
2) \textit{Multi-view Consistency Score}. As the primary goal of our work is to improve the consistency of generated images, we also employ the 3D consistency score~\cite{3dim} to verify the consistency among the synthesized images. Specifically, we train an Instant-NGP~\cite{instant_ngp} with the input image and part of the synthesized novel view images of our model and evaluate the similarity between the remaining synthesized images and the rendered images of Instant-NGP. For the synthesized multi-view images of each object, we allocate $3/4$ for training and reserve the remaining $1/4$ for validation.
Intuitively, if the consistency of synthesized images is improved, the NeRF-like model will train a better object representation, and the re-rendered images will agree more with the validation images.
3) \textit{Input Consistency Score}. To assess the faithfulness of synthesized images in preserving the identity of the input condition image, we introduce the input consistency score. This score calculates the similarity of each synthesized image with the input condition image, utilizing the LPIPS metric.

In addition, we use synthesized multi-view images to train a neural 3D reconstruction model (NeuS~\cite{neus}) and report commonly used Chamfer Distances (CD) and Volume IoUs between the trained 3D model and the ground truth.

\noindent\textbf{Baselines.}
Given that our main goal is to improve the consistency of the trained baseline model without further fine-tuning, we mainly compare our approach with the used baseline model Zero123~\cite{zero123}. Additionally, we compare our method to the SOTA approaches such as PGD~\cite{tseng2023consistent} and SyncDreamer~\cite{SyncDreamer} using the same Zero123 base model.

\noindent\textbf{Implementation Details.}
We use the official checkpoint provided by Zero123~\cite{zero123}, which is trained on objaverse~\cite{objaverse} for 165,000 steps. We inject our epipolar attention layer after step $T=4$ and layer $L=10$ by default. We find that feature fusion weight $\alpha=0.5$, and the number of context views $M=2$ work better.

\input{tables/view16_free}
\input{tables/view16_fixed}

\subsection{Comparison With Baseline Models}
The quantitative comparison on three settings are shown in Tab.~\ref{tab:view16_free_compare}, Tab.~\ref{tab:view16_fxied_compare}, and Tab.~\ref{tab:view32_free_compare}. The qualitative comparison is shown in Fig.~\ref{fig:sota_compare}.

\input{tables/view32_free}
\input{tables/ablation}



\begin{figure*}[ht]
    \centering
    \begin{minipage}{0.65\textwidth}
        \centering
        \includegraphics[width=0.95\linewidth]{figs/ablation.pdf}
        \vspace{-2mm}
        \captionof{figure}{Qualitative Comparison for different design choices. Our method, employing multi-view epipolar attention, demonstrates the best consistency.}
        \label{fig:ablation}
    \end{minipage}\hfill
    \begin{minipage}{0.33\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{figs/neus_ver.pdf}
        \vspace{-3mm}
        \caption{Our method shows better direct 3D reconstruction~\cite{neus}.}
        \label{fig:neus}
    \end{minipage}
    \vspace{-5mm}
\end{figure*}

\noindent\textbf{Multi-view Consistency.}
Tab.~\ref{tab:view16_fxied_compare} presents the 3D consistency scores compared to our baseline model (Zero123) and SyncDreamer. The results indicate a significant improvement across all three metrics achieved by our method when compared with Zero123.
While our method exhibits a marginally lower numerical consistency score compared to SyncDreamer, it enables the synthesis of images with arbitrary camera poses.	
This capability is illustrated in Tab.~\ref{tab:view16_free_compare}, where our method consistently enhances consistency with changes in camera pose settings, whereas SyncDreamer fails to do so and exhibits inferior results compared to Zero123.
Furthermore, our method facilitates the synthesis of multi-view images with any number of camera views. This versatility is demonstrated in Tab.~\ref{tab:view32_free_compare}, where our method continues to achieve significant improvements in consistency scores, while SyncDreamer is unable to operate under such conditions.	

Meanwhile, Fig.~\ref{fig:sota_compare} provides a qualitative comparison with the baseline. While both our method and SyncDreamer enhance consistency, our method visually preserves better similarity to the input image, including color and texture details. The input consistency score further corroborates this.

\noindent\textbf{Image Quality.}
While our primary goal centers around enhancing the consistency of synthesized multi-view images, we also evaluate the image quality by comparing the similarity with the ground truth images. The results shown in Tab.~\ref{tab:view16_free_compare}, Tab.~\ref{tab:view16_fxied_compare}, and Tab.~\ref{tab:view32_free_compare} indicate that our method also enhances the image quality under different settings besides improving the consistency.
Moreover, our method shows better image quality compared with SyncDreamer even in the 16-view setting with fixed camera pose.

\noindent\textbf{Input Consistency.}
Input consistency terms whether the results align with the input image.
Fig.~\ref{fig:sota_compare} illustrates that both our method and SyncDreamer enhance multi-view consistency. However, the color and texture details of SyncDreamer's results diverge from the input image and appear visually unnatural.
This discrepancy is evident in the input consistency score presented in Tab.~\ref{tab:view16_fxied_compare}, indicating lower similarity with the condition image in the SyncDreamer results.	

\subsection{Ablation Study}
The overall quantitative results are shown in Tab.~\ref{tab:ablation}, and the qualitative comparisons are shown in Fig.~\ref{fig:ablation}.

\noindent \textbf{Full Attention \vs Epipolar Attention.}
The results presented in Tab.\ref{tab:ablation} and Fig.\ref{fig:ablation} demonstrate that our epipolar attention mechanism can synthesize more consistent multi-view images compared with full attention. Furthermore, our epipolar attention achieves a greater performance improvement compared to full attention when using multiple reference images. This could be attributed to the fact that our epipolar attention more effectively localizes target information, as depicted in Fig.~\ref{fig:full_attn_compare}, thereby reducing noise from the reference images. In the multi-view setting, where multiple reference images are utilized, this noise reduction becomes particularly crucial.
Moreover, it is noteworthy that the epipolar attention mechanism consumes less GPU memory compared to our baseline, as discussed in Sec.~\ref{sec:attn_analysis}.

\noindent \textbf{Attending Single-View \vs Multi-View.}
Applying the epipolar attention significantly improves the consistency between the input and target views. However, the consistency between different views in the unobserved regions of the input view is not well preserved.
After implementing our epipolar attention in the multi-view setting, the consistency across the generated multi-view images is further improved. The last row in Tab.~\ref{tab:ablation} shows that after applying our multi-view epipolar attention, the consistency score is further improved compared with the single-view setting. Besides, the qualitative result in Fig.~\ref{fig:ablation} also shows better consistency among different target views.



\input{tables/neus}
\vspace{-2mm}
\subsection{Downstream Application}
\vspace{-2mm}
To demonstrate the effectiveness of our method, we also applied it to the downstream 3D reconstruction task. Specifically, we trained the NeuS model~\cite{neus} directly using images synthesized by our method, Zero123, and SyncDreamer, respectively.
The quantitative results in Tab.~\ref{tab:neus} show that the consistent multi-view images synthesized by our method can significantly improve the 3D reconstruction quality.
Additionally, our method exhibits similar performance to SyncDreamer which requires time-consuming re-training.
The qualitative results in Fig.~\ref{fig:neus} show that it is challenging to train the NeuS model directly due to the lack of consistency in the images generated by Zero123. In contrast, our method generates more consistent multi-view images and, therefore, better reconstructs the geometry and texture details.
We show improvements on other downstream applications such as image-to-3D in the Supplementary Material.

