\section{Preliminaries}
\label{sec:background}

\begin{figure*}[t]
    \vspace{-3mm}
    \centering
    \includegraphics[width=0.8\linewidth]{figs/arch.pdf}
    \vspace{-2mm}
    \caption{\textbf{Overview of our method.} 
    (a) We first perform DDIM inversion on the input image to obtain the initial noise, which is shared during the multi-view image generation process. Throughout the generation of each view, our epipolar attention block efficiently locates and retrieves corresponding information from both the input image and other target views.
    (b) The architecture of our 3D epipolar attention module.
    (c) Location of our inserted epipolar attention block.}
    \label{fig:arch}
    \vspace{-4mm}
\end{figure*}


In this section, we revisit the pose-conditional diffusion model used in our approach (Sec.~\ref{sec:pcdm}), and the DDIM inversion technique used to invert the reference image back to the initial Gaussian noise (Sec.~\ref{sec:ddim}).


\subsection{Pose-Conditioned Diffusion Model} \label{sec:pcdm}
Diffusion models~\cite{sohl2015deep, diffusion, ddim, dhariwal2021diffusion} are probabilistic generative models, which transform an initial Gaussian noise $\boldsymbol{x}_T \sim \mathcal{N}(0, \mathbf{I})$ into an arbitrary meaningful data distribution.
During training, the diffusion \textit{forward} process is applied, in which Gaussian noise is added to the clean data $\boldsymbol{x}_0$ (image in our case):
\begin{equation}
\label{eq:diffusion_forward}
    \boldsymbol{x}_t=\sqrt{\alpha_t} \cdot \boldsymbol{x}_0+\sqrt{1-\alpha_t} \cdot \boldsymbol{z},
\end{equation}
where $\boldsymbol{z} \sim \mathcal{N}(0, \mathbf{I})$ is the random noise and $\{\alpha_t\}, t \in [0, T]$ is the noise schedule indexed by time step $t$.
During inference, the \textit{backward} diffusion process is utilized to progressively denoise $\boldsymbol{x}_T$ to obtain the clean data. This denoising process is facilitated by a neural network $\boldsymbol{\epsilon}_\theta\left(\boldsymbol{x}_t, t\right)$, which predicts noise at each step.

We focus on employing the diffusion model for synthesizing novel views from a single input view. 
This can be seen as an image-to-image translation process that transforms the original image into a novel view image based on their relative camera pose transformation.
Formally, given the reference view image $\boldsymbol{x}_r$ and the relative camera pose transformation $\Delta\boldsymbol{p} = (\mathbf{R}, \mathbf{T})$ between the reference view and the target view, the denoising network predicts noise conditioned on both $\boldsymbol{x}_r$ and $\Delta\boldsymbol{p}$, denoted as:
\begin{equation}
\label{eq:denoise}
    \boldsymbol{z}_t = \boldsymbol{\epsilon}_\theta\left(\boldsymbol{x}_t, t \mid \boldsymbol{x}_r, \Delta\boldsymbol{p}\right).
\end{equation}

In this work, we leverage a pre-trained pose-conditioned diffusion model (Zero123~\cite{zero123}), which in turn is fine-turned from a Latent Diffusion Model~\cite{stable-diffusion}. 
The network is implemented using a U-Net~\cite{unet} structure, consisting of several residual blocks~\cite{resnet}, self-attention blocks, and cross-attention blocks~\cite{transformer}.
At each time step $t$, the feature maps from the previous layer $l-1$ are first feeded in the residual block to obtain feature $\boldsymbol{F}^l_t$.
Subsequently, projection layers are employed to generate distinct query $\boldsymbol{Q}$, key $\boldsymbol{K}$, and value $\boldsymbol{V}$ feature maps (for simplicity, excluding time step $t$ and layer index $l$).
The output feature of the self-attention block, denoted as $\hat{\boldsymbol{F}}$, is computed using the operation $\hat{\boldsymbol{F}} = \boldsymbol{A} \cdot \boldsymbol{V}$, where the attention matrix $\boldsymbol{A}$ is determined as follows:
\begin{equation}
\label{eq:attn_mat}
    \boldsymbol{A}=\operatorname{Softmax}\left(\frac{\boldsymbol{Q} \boldsymbol{K}^\top}{\sqrt{d}}\right),
\end{equation}
where $d$ is the feature dimension.

\subsection{DDIM Inversion} \label{sec:ddim}
\label{subsec:ddim_inversion}
During \textit{backward} diffusion, deterministic DDIM sampling~\cite{ddim} is commonly used to convert noise $\boldsymbol{x}_T$ into clean data $\boldsymbol{x}_0$.
In contrast, DDIM inversion~\cite{ddim, dhariwal2021diffusion} converts the original clean image data $\boldsymbol{x}_0$ back to Gaussian noise $\boldsymbol{x}_T$ by incrementally adding the noise predicted by the network $\boldsymbol{\epsilon}_\theta$.
We also employ DDIM inversion to convert the input image to its initial noise $\boldsymbol{x}^R$. Throughout this conversion, we utilize the input image feature and a fixed relative pose transformation of $[0, 0, 0]$ as the network condition.

\section{Approach}
As mentioned above, our goal is to improve the consistency of the synthetic multi-view image by locating and retrieving the corresponding information (features) in the reference view that overlap with the target view and then using the retrieved features to constrain the target view generation process.
Therefore, we first explicate the methodology for computing the epipolar line and sampling points along it to effectively reduce the searching space concerning the corresponding locations (Sec.~\ref{sec:epi_sample}).
Then, we will describe how to locate the corresponding locations along the epipolar line (Sec.~\ref{sec:fine_locate}). The general idea is to find the correspondence between a point in the target view and sampled points on the epipolar line through feature similarity. To better obtain the similarity information, we analyze the attributes of different features in the U-Net block and then find the appropriate features used to compute the similarity.
Next, we introduce the parameter duplication strategy that facilitates the training-free module, and how to inject the retrieved reference features to constrain the generation process of the target views (Sec.~\ref{sec:feature_use}). 
Finally, we will provide a detailed analysis of our epipolar attention (Sec.~\ref{sec:attn_analysis}).
Fig.~\ref{fig:arch} shows the overall framework of our tuning-free multi-view epipolar attention that enables consistent novel view synthesis.




\subsection{Point Sampling from Epipolar Lines}\label{sec:epi_sample}
Ideally, with the target view's depth map, we can accurately find its correspondence in the reference view by un-projecting each point to 3D space and then re-projecting it into the reference view.
However, obtaining an accurate depth value for arbitrary real-world objects remains challenging, if not infeasible. 
Alternatively, when considering a point $\boldsymbol{p}_i$ in the target view, its corresponding point $\boldsymbol{p}^{\prime}_i$ in the reference view, if visible, must lie on the corresponding epipolar line $\mathbf{l}_i$.
Therefore, we opt to find the corresponding feature in the reference view along the epipolar line, which significantly reduces the search space and the memory required for subsequent computations.

We assume that the synthesized novel view images have the same camera intrinsic parameters $\mathbf{K}$ as the reference image, as being commonly set for the NVS task.
Specifically, given the relative camera rotation $\mathbf{R}$ and translation $\boldsymbol{t}$ from the reference image to the target image, for each point $\boldsymbol{p}_i$ in the target image, the corresponding epipolar line  $\mathbf{l}_i$ is:
\begin{equation}
\label{eq:epipolar}
    \boldsymbol{l}_i = \mathbf{R} [\boldsymbol{t}]_{\times} \mathbf{K}^{-1} \boldsymbol{p}_i,
\end{equation}
where $\mathbf{l}_i$ is the epipolar line of $\boldsymbol{p}_i$ in the reference image, and $[\boldsymbol{t}]_{\times}$ is the skew-symmetric matrix representation of $\boldsymbol{t}$.
Despite the unknown exact camera focal length $f$, the computation of the epipolar lines $\mathbf{l}_i$ remains feasible, as the computation can be independent of $f$ (see Supplementary Material for proof).

Subsequently, we sample a set of points denoted as $\boldsymbol{p}^{\prime} \in P^{\prime}$ along the epipolar line, specifically along the direction of the image width, at intervals of each feature pixel. Note that some sample points may be outside the feature plane and will be masked during the similarity calculation and feature retrieval process.

\subsection{Corresponding Point Searching}\label{sec:fine_locate}
\noindent \textbf{Paired Feature Acquiration.}
The epipolar sampling operation essentially reduces the search space of the corresponding points. However, how to more accurately locate the actual corresponding point in the epipolar line remains unsolved. 
Previous works~\cite{dift, dino-sd} show that the features extracted by diffusion models show good semantic correspondence between two input images. 
Thus, a plausible approach is to seek the corresponding position in the reference image for each pixel in the target view by assessing the similarity between their respective features.
However, previous feature matching methods~\cite{dift, dino-sd} require feeding two paired images into the diffusion model separately and extracting their features for matching, making them unsuitable for our scenario where the target image is pending generation.
To address this, we employ DDIM inversion, as detailed in Sec.~\ref{subsec:ddim_inversion}, to acquire noise from the reference image.
This noise is then utilized to concurrently reconstruct the reference image alongside the denoising process of the target image, which we used to obtain the paired features.
Specifically, we progressively denoise the DDIM inversed initial noise $\boldsymbol{x}^R$ of the reference image using DDIM sampling and set the relative camera pose transformation as $[0, 0, 0]$ so that the reference image can be recovered. Meanwhile, we use the same $\boldsymbol{x}^R$ as the initial noise to generate the target view.
We can then obtain paired features by retrieving the features in the same denoising step and at the corresponding layer of both the input and target generation branches.
Since the sampled point in the epipolar line is in the sub-pixel location, we employ bilinear interpolation to obtain the feature value of each point $\boldsymbol{p}^{\prime}_i$ in the epipolar line.
We then analyze the similarity of the corresponding intermediate feature of the target and reference branch.


\noindent\textbf{Computing Epipolar Attention.}
We now have access to the paired features of the input and target images. However, the specific features within the U-Net structure to utilize, as well as the methodology for calculating their similarity, remain unclear.
Previous feature matching methods~\cite{dift, dino-sd} calculate the similarity of output feature $\boldsymbol{F}$ of the attention block use cosine similarity $\operatorname{CosSim}(\boldsymbol{F}_{tgt}(\boldsymbol{q}), [\boldsymbol{F}_{ref}(\boldsymbol{q}^{\prime})])$ ($[\cdot]$ is the bilinear interpolation operation), followed by a softmax operation.
However, our experiments reveal that the similarity derived from these features does not align well with our intended application. The resulting similarity map exhibits a relatively uniform distribution, indicating insufficient localization of the desired corresponding location and inadequate corresponding feature aggregation (see Fig. C.1. in the supp. mat.).
It is important to note that the query and key features employed within the multi-head self-attention block are intended for similarity calculation, so we opt to use the query $\boldsymbol{Q}$ from the target branch and the key $\boldsymbol{K}$ from the reference branch to compute the similarity according to Eq.~\ref{eq:attn_mat}.
Such similarity scores can pinpoint the corresponding location (see Fig. C.1. in the supp. mat.).


\subsection{Reference Feature Injection}\label{sec:feature_use}
After finding the location of the corresponding point, we introduce how to use such information to constrain the generation process of the target image.
First, to neglect the necessity of further training or fine-tuning, we employ a simple parameter duplication strategy, as shown in Fig.~\ref{fig:arch}(c), in which \textit{we directly instantiated the epipolar attention block with the well-trained parameters of the self-attention block}.
Similar to the attention operation~\cite{transformer}, we use the weighted sum to aggerate the corresponding information in the reference image as follows:
\begin{equation}
    \hat{\boldsymbol{F}}_{src} = \sum_{p^{\prime} \in \mathcal{P}^{\prime}} \operatorname{sim}\left( \boldsymbol{Q}_{tgt}(p), \boldsymbol{K}_{ref}(p^{\prime})\right) \cdot \boldsymbol{F}_{\mathrm{src}}\left(p^{\prime}\right),
\end{equation}
where $\operatorname{sim}(\cdot)$ is the similarity calculation operation as Eq.~\ref{eq:attn_mat}.
Similar to the residual connection in the original U-Net block, we fuse the output feature from our epipolar attention block with the original self-attention block with a pre-defined weight parameter $\alpha$, which can be formulated as $\boldsymbol{F} = \alpha\hat{\boldsymbol{F}}_{src} + (1-\alpha)\hat{\boldsymbol{F}}$.

\begin{figure}[t]
    \centering
    \includegraphics[width=.7\linewidth]{figs/full_attn_white.pdf}
    \vspace{-5mm}
    \caption{Comparison between our epipolar attention and the full attention. Our epipolar attention better locates and retrieves the corresponding information in the reference view.}
    \label{fig:full_attn_compare}
    \vspace{-6mm}
\end{figure}


\noindent\textbf{Attending Multi-Views at Once.}
While aggregating the overlapping feature from the input view improves the consistency between the output view and the input view, the consistency between different target views still is not well preserved as there are regions in the target images that are not visible to the input views. 
We further extend the epipolar attention to the multi-view setting to address this issue. 
Specifically, we generate multiple views $\Delta\boldsymbol{p}_i, i \in [1, N]$ in an auto-regressive manner. 
When synthesizing a specific novel view $\Delta\boldsymbol{p}_i$, we designate it as the target view. $M$ previous views, along with the input view, are considered context views, collectively containing specific information that overlaps with the target view. 
Subsequently, we apply epipolar attention to all context views and compute the average features derived from these views.
Fig.~\ref{fig:arch} (a) shows an example synthesis process for view $\Delta\boldsymbol{p}_i$.

\subsection{Discussion About the Epipolar Attention}\label{sec:attn_analysis}



\textbf{Comparison with Full Image Attention.}
An alternative to our epipolar attention mechanism is directly using full attention to gather corresponding information in the reference view, which finds the corresponding points in the full image. In contrast, our epipolar attention significantly reduces the search space for the corresponding point searching by introducing additional geometric priors. Illustrated in Fig.~\ref{fig:full_attn_compare}, our method exhibits sharper similarity scores and more precise localization of corresponding positions, resulting in a more effective retrieval of desired corresponding features. 
Thus, as shown in Tab.~\ref{tab:ablation}, epipolar attention performs better than full attention, especially when multiple reference views are employed.
Additionally, by reducing the search space, our epipolar attention significantly decreases memory consumption during the feature retrieval process. The space and time complexity of the epipolar attention is $O(L^3)$, while that of the full attention is $O(L^4)$, where $L$ is the length of the feature map. 




\noindent\textbf{Comparison with Recent Methods.}
Some recent works, such as MVDream~\cite{mvdream}, SyncDreamer~\cite{SyncDreamer}, and Zero123++~\cite{zero123++}, also aim to improve the consistency of synthesized multi-view images. However, these methods require time-consuming re-training. Moreover, they constrain the camera pose during training, limiting their ability to synthesize images to a fixed set of camera poses. For example, MVDream~\cite{mvdream} can only synthesize images with four fixed camera views. In contrast, our method can synthesize consistent multi-view images with arbitrary camera poses without re-training.

Previous work, \ie, PGD~\cite{tseng2023consistent} also utilizes epipolar attention in the generation task. However, it differs from our method mainly in two aspects.
\textbf{1)} Our method aims to enhance baseline model consistency without tuning, while PGD treats epipolar attention as a network module requiring full network training, making it resource-intensive. 
These differences also lead to problem formulation in using epipolar constraints.
PGD computes per-pixel distances to the epipolar line as an additional weight map multiplied by the original attention matrix, thereby altering the original distribution of attention weights. 
Consequently, this approach is not suitable for a non-training pipeline. 
In contrast, we aim to \textit{locate} and \textit{retrieve} corresponding information from the reference views using the epipolar constraint to roughly approximate the correspondence, followed by sampling and soft fine-locating.
Thus, we avoid the need for time-consuming retraining. 
Furthermore, our method reduces GPU memory consumption compared to PGD, as PGD still utilizes full attention.
Inserting PGD's epipolar module into our pipeline yields inferior results and has no significant improvements over full attention (see Tab.\ref{tab:view32_free_compare} and Tab.~\ref{tab:ablation}).
\textbf{2)} To make the whole pipeline work without any fine-tuning, we invest considerable effort in its design, which is not explored in PGD. For instance, we provide insights into how to generate input view features based on pre-trained Zero123, determine appropriate features for similarity computation, and how to extend epipolar attention to multi-view setting.

