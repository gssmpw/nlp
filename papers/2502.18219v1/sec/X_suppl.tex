\clearpage
\setcounter{page}{1}
\maketitlesupplementary
\appendix
\counterwithin{figure}{section}
\counterwithin{table}{section}

\section{Epipolar Line Calculation}
\label{sec:epipolar}

Here we provide detailed proof that the final epipolar line  $\boldsymbol{l}_i$ is independent of the unknown focal length $f$.

Given the rotation matrix $\mathbf{R}$ and translation vector $\boldsymbol{t}$ between the two cameras, and the camera intrinsic parameters $\mathbf{K} = \begin{bmatrix} f & 0 & a \\ 0 & f & b \\ 0 & 0 & 1 \end{bmatrix}$, the epipolar line $\mathbf{l}_i$ in the reference image corresponding to a point $\boldsymbol{p}_i$ in the target image can be calculated as:
\begin{equation}
    \boldsymbol{l}_i = \mathbf{E} \tilde{\boldsymbol{p}}_i = \mathbf{R} [\boldsymbol{t}]_{\times} \tilde{\boldsymbol{p}}_i,
\end{equation}
where $\mathbf{E}$ is the essential matrix, $[\boldsymbol{t}]_{\times}$ is the skew-symmetric matrix representation of the translation vector $\boldsymbol{t}$, and $\tilde{\boldsymbol{p}}_i = \mathbf{K}^{-1} \boldsymbol{p}_i$ is the point $\boldsymbol{p}_i$ in the normalized image coordinates.

Now, expressing $\tilde{\boldsymbol{p}}_i$ in terms of \(\boldsymbol{p}_i\) and \(\mathbf{K}\):

\begin{equation}
    \begin{aligned}
        & \tilde{\boldsymbol{p}}_i=\mathbf{K}^{-1} \boldsymbol{p}_i \\
        & =\left[\begin{array}{ccc}
        1 / f & 0 & -a / f \\
        0 & 1 / f & -b / f \\
        0 & 0 & 1
        \end{array}\right]\left[\begin{array}{l}
        x \\
        y \\
        1
        \end{array}\right] \\
        & =\left[\begin{array}{c}
        x / f-a / f \\
        y / f-b / f \\
        1
        \end{array}\right] \\
        & =\left[\begin{array}{c}
        (x-a) / f \\
        (y-b) / f \\
        1
        \end{array}\right].
    \end{aligned}
\end{equation}

Substituting this into the equation for $\boldsymbol{l}_i$:

\begin{equation}
    \boldsymbol{l}_i = \mathbf{R} [\boldsymbol{t}]_{\times} 
        \left[\begin{array}{c}
            (x-a) / f \\
            (y-b) / f \\
            1
        \end{array}\right].
\end{equation}

\noindent Here, the coordinates $(x - a)/f$ and $(y - b)/f$ are simply scaled versions of the original image coordinates $x$ and $y$, and this scaling does not affect the linearity of the equation. Therefore, the final expression for $\boldsymbol{l}_i$ does not explicitly depend on $f$.


\section{Property of the Epipolar Attention}
To better understand our epipolar attention mechanism, we performed a visual analysis of the attentional weights in various cases. In  Fig.~\ref{fig:attn_analysis}, two pairs of images show that our epipolar attention tends to give multiple semantically similar points close similarity scores when a point is occluded or when there is a lack of explicit geometric or semantic correspondence between the two points in the target and reference images. This behavior suggests that our method employs a broader range of contextual features, a favorable approach without explicit correspondences.
\begin{figure}[t]
\centering
  \begin{subfigure}[t]{.36\linewidth}
    \centering
    \includegraphics[width=0.7\linewidth]{figs/no_clear.pdf}
    \caption{No clear correspondence}
  \end{subfigure}
  \hspace{1em}
  \begin{subfigure}[t]{.36\linewidth}
    \centering
    \includegraphics[width=0.7\linewidth]{figs/occlusion.pdf}
    \caption{Occlusion}
  \end{subfigure}
  \hfill
\caption{When the occlusion occurs, or there is no clear geometric or semantic corresponding, epipolar attention tends to give multiple semantically similar points close similarity scores.}
\label{fig:attn_analysis}
\end{figure}

\section{Different Features for Similarity Calculation}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figs/attn_map.pdf}
    \caption{Similarity scores using different features. Similarity scores computed using queries and key features in the self-attention block are sharper and more accurate than those computed using the output features of the attention block.}
    \label{fig:attn_map}
\end{figure}

As discussed in Section 4.2 of our main paper, the similarity score derived from the output feature $\boldsymbol{F}$ of the attention block does not align well with our intended application, as it produces a relatively uniform similarity map. Instead, using the query $\boldsymbol{Q}$ from the target branch and the key $\boldsymbol{K}$ from the reference branch within the multi-head self-attention block provides a more accurate correspondence. This is illustrated in Figure~\ref{fig:attn_map}.

\section{Results on More Datasets}
\input{tables/objaverse}
We conduct experiments on the Objaverse dataset~\cite{objaverse}. Specifically, we randomly sample 100 objects from the Objaverse test set, utilizing the camera setting of 16-views with a fixed camera pose, which aligns with SyncDreamer's setup for fair comparison.
The results are presented in Tab.~\ref{tab:objaverse_view16_fix} and share the same conclusion with the exprimences on GSO~\cite{GSO} dataset.
Specifically, compared with our baseline model (Zero123), our method significantly improves the multi-view consistency, image quality, and input consistency on the Objaverse dataset. Compared with SyncDreamer, we achieve similar multi-view consistency but better image quality and input consistency. These results demonstrate the efficacy of our approach across different datasets.


\section{More Ablation Studies}
\input{tables/ab_num_views}
\input{tables/ab_feat_type}

\subsection{Number of Context Views}
The quantity of context views, denoted as $M$, may influence the consistency of synthesized multi-view images. Ablation studies are conducted to examine the impact of varying numbers of context views, and the results are presented in Tab.~\ref{tab:ab_num_views}. It is evident that in the absence of context views (our baseline), the consistency is poor. As the number of context views increases, the consistency improves. However, as the context number is continuously increased, the consistency score decreases. This decline may be due to significant relative camera pose transformations, resulting in smaller overlapping regions between two views. Retrieving information from these views may adversely affect performance.

\subsection{Effect of Using Different Features}
In Fig. 4 of our main paper, we visually compare the similarity scores obtained using different features, \ie, employing query key features within the self-attention blocks and output features of the self-attention layers. Here, a quantitative comparison is conducted to demonstrate the impact of employing distinct features. The results in Tab.~\ref{tab:ab_feat_type} illustrate that utilizing query key features shows better consistency performance than using the output features from the self-attention layers, as they better locate the corresponding features.

\input{tables/ab_overlap}
\subsection{Effectiveness on Different Overlap Ratios}
In Section 5 of our main paper, we present three different view sampling methods used in our experiments. These methods ensure that each view sufficiently overlaps with its neighboring views, facilitating the transmission of overlapping information.
Here, we vary the overlapping ratio between the target and input views during the single-view synthesis process to examine the impact of different overlapping ratios.
The results in Tab.~\ref{tab:ab_overlap} show that our method consistently demonstrates improvements over the baseline across various overlap ratios. Notably, even in scenarios where there is no overlap between the reference and target views, our method obtains performance gains over the baseline. This can be attributed to our approach of utilizing the DDIM inverted noise from the reference view as the initial noise for the target view, thereby incorporating additional information from the reference view.


\subsection{Other Hyperparameters}
In regards to the feature fusion weight $\alpha$, the step $T$, and the U-Net layer $L$ after which we inject our epipolar attention layer, we conduct preliminary tests with various values on a few numbers of objects, ultimately selecting those that yield more visually appealing results. We do not attempt to determine the optimal values across the entire test set, as this approach is impractical. Furthermore, it is acknowledged that different objects may necessitate distinct hyperparameter values for better performance.


\section{Application in Image-to-3D Task}
To further validate the effectiveness of our method on downstream applications, we apply our method to the image-to-3D task and compare the results with our baseline Zero123. Specifically, given a single image, we use the output noise of our method and Zero123 to distill the NeRF~\cite{nerf} training process. We follow the method proposed in DreamFusion~\cite{dreamfusion}; please refer to this paper for more details. The results in Fig.~\ref{fig:dreamfusion} show that our method generates 3D objects with better geometric and texture details, especially the parts that are not visible in the input view.


\section{Limitations}
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/failure.pdf}
    \caption{Failure cases. We provide an in-depth analysis of failure cases arising when the baseline model exhibits severe inconsistencies or when dealing with objects with complex textures.}
    \label{fig:failure}
\end{figure*}
Utilizing our epipolar attention to locate and retrieve corresponding information in the reference views enhances the consistency between generated multi-view images compared to the baseline model. Nevertheless, our method cannot ensure absolute consistency in the generated images due to the inherent probabilistic nature of the diffusion model, which remains unchanged. Employing multiple model runs and selecting superior results may further enhance consistency.

Here we further discuss failure cases in more detail.
1) Illustrated in the first set of images in Fig.~\ref{fig:failure}, our method encounters situations where severe inconsistencies exist in the baseline model, impeding its ability to well rectify these inconsistencies even when reference information is injected during the image generation process. In real-world applications, tuning the feature fusing weight $\alpha$ for a specific object may acquire better consistency results.
2) Illustrated in the second set of images in Fig.~\ref{fig:failure}, despite the substantial improvement in consistency achieved by our method in the generated multi-view images, our approach may encounter challenges maintaining absolute consistency, particularly when dealing with objects exhibiting complex textures. This limitation could stem from the inadequacy of the baseline model. Notably, our experiments demonstrate that even when a zero camera translation is provided to the model, it struggles to accurately reconstruct the input image in the presence of complex textures.

Besides, our auto-regressive generation pipeline naturally increases inference time. On a single NVIDIA A100, Zero123 generates a single image in 3 seconds, while our method takes 5 seconds. For 16 views, Zero123 takes 14 seconds due to batch processing, whereas our auto-regressive generation takes 55 seconds. However, considering the alternative of unaffordable re-training whenever a stronger baseline model becomes available, the runtime increase of our method is acceptable, as it significantly improves consistency and enables the generation of arbitrary views.


\section{More Visualization Results}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/recon_more.pdf}
    \caption{More 3D reconstruction results.}
    \label{fig:more_recon}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figs/dreamfusion.pdf}
    \caption{Image-to-3D generation results. In each group of images, the images in the first row depict results generated by the baseline model (Zero123), while those in the second row display results obtained from our approach. The results show that our method generates better 3D objects, especially the parts of the object not seen in the input view.}
    \label{fig:dreamfusion}
\end{figure*}

\noindent\textbf{More Reconstruction Results.}
We present additional 3D reconstruction results in Fig.~\ref{fig:more_recon}. These results illustrate that by increasing the consistency in the generated multi-view images, directly training 3D models using these images yields plausible 3D mesh representations.

\noindent\textbf{More Qualitative Comparisons of Synthesized Multi-View Images.}
The results in Fig.~\ref{fig:objaverse} and Fig.~\ref{fig:gso_more} further provide comparisons of the multi-view images synthesized by the baseline model and our method.
In these two figures, the images positioned on the left-hand side represent the input image. In each group of images, the images in the first row depict results generated by the baseline model (Zero123), while those in the second row display results obtained from our approach.
The comparisons show that our method improves the consistency of generated multi-view images on different datasets.

The results in Fig.~\ref{fig:more_compare} provide additional comparisons between Zero123, SyncDreamer, and our method, demonstrating that our method significantly improves multi-view consistency compared to Zero123, while also exhibiting better image quality compared to SyncDreamer.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.97\linewidth]{figs/objaverse.pdf}
    \caption{Qualitative comparison with the baseline for generating a sequence of novel view images on the Objaverse dataset.
    The images positioned on the left-hand side represent the input image. In each group of images, the images in the first row depict results generated by the baseline model (Zero123), while those in the second row display results obtained from our approach. 
    The comparison demonstrates that our method can generate multi-view images with higher consistency.}
    \label{fig:objaverse}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.92\linewidth]{figs/gso_more.pdf}
    \caption{More Qualitative comparison with the baseline for generating a sequence of novel view images on the GSO dataset.
    The image placement aligns with Fig.~\ref{fig:objaverse}.}
    \label{fig:gso_more}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figs/compare_more.pdf}
    \caption{More Qualitative comparison with Zero123 and SyncDreamer. The results show that our method significantly improves multi-view consistency compared to Zero123, while also exhibiting better image quality compared to SyncDreamer.}
    \label{fig:more_compare}
\end{figure*}
