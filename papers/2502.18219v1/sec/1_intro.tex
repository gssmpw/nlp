\vspace{-6mm}

\section{Introduction}
\label{sec:intro}

Synthesizing high-quality novel view images from a single input image is a long-standing and challenging problem. It requires inheriting the appearance of objects in the observed regions of the input image while also hallucinating unseen regions. Recent studies~\cite{3dim, zero123} approach this problem as an image-to-image translation task and implement it using diffusion models~\cite{diffusion, ddim}, drawing inspiration from their successful application in 2D image generation~\cite{stable-diffusion, imagen}.	
While they exhibit remarkable zero-shot capabilities when trained with large-scale 2D and 3D datasets, they still face challenges in maintaining 3D consistency between the target view and the generated multi-view images, due to the probabilistic nature of diffusion models. This limitation adversely affects downstream applications such as 3D reconstruction~\cite{dreamfusion, neus}.

In this paper, we propose to improve the consistency of synthesized multi-view images by optimizing the utilization of reference image information. 
Notably, maintaining consistency between the generated image and the corresponding observed regions in the input view is a crucial requirement in the task of single-image conditioned novel view synthesis. 
However, existing methods often overlook this constraint by merely considering the input image as a condition or network input, which fails to guarantee such consistency. 
One straightforward method to fulfill this constraint is by warping the content from the input to the target view and subsequently conducting outpainting for the remaining regions~\cite{text2nerf, xiang20233d}.
However, 3D warping relies on precise depth information, which is hard to obtain.
Additionally, direct warping struggles with occlusion and illumination variations across different views.

We aim to utilize this constraint to improve the consistency in a more adaptable way.
Despite the intricacies of obtaining depth, we can still reduce the search space for locating corresponding points by incorporating other 3D geometric priors. 
As depicted in Fig.~\ref{fig:epipolar_vis}, the corresponding points in the reference views must be on the epipolar line.
Therefore, we propose an epipolar attention module to locate and gather contextual information. 
For each point in the target view visible in the reference view, we can first constrain the corresponding point to its respective epipolar line in the reference image. Subsequently, we ascertain the corresponding location along the epipolar line by feature matching.
The features at the localized positions are then retrieved and used to constrain the target view generation.

More specifically, we first perform DDIM inversion on the input view and reconstruct the input image using the initial noise provided by the DDIM inversion.
This process yields intermediate features of the input view, which can then be employed to constrain the generation of target views.
Then, in the epipolar attention module, we traverse the corresponding epipolar line in the input view for every point within the target view. During this process, we compute the similarity between the features of the target point and those sampled from the input view. This similarity score is then used to aggregate the corresponding features from the input view.
This soft operation is more adept at handling complex scenarios, such as occlusion (detailed analysis can be found in the Supplementary Material).
Additionally, to avoid any parameter training or fine-tuning, we employ a simple parameter duplication strategy, \ie, we copy all parameters directly from the self-attention layer to obtain the epipolar attention parameters.
To further improve the consistency between different target views, we expand the application of epipolar attention to a multi-view context.
Specifically, we generate multiple target views in an auto-regressive manner. 
When generating a specific novel view, we consider the input view and previously generated target views close to the current viewpoint as context views.
We employ epipolar attention to aggregate overlapping information from all context views, rather than solely from the input view, thereby improving consistency among all generated views.
It is worth mentioning that our epipolar attention reduces the search space compared to locating corresponding points in the full image. Therefore, it requires much less memory when retrieving information from multiple views, making it more friendly to GPUs with small memory capacity.

\begin{figure}[t]
\vspace{-5mm}
    \centering
    \includegraphics[width=.7\linewidth]{figs/epipolar_cor.pdf}
     \vspace{-5mm}
    \caption{When the camera viewing frustum of two views overlaps, for a point on one of the images, we can find its correspondence on the epipolar line of the other view.}
    \label{fig:epipolar_vis}
    \vspace{-3mm}
\end{figure}

\input{tables/intro}

We conduct experiences on the Google Scanned Objects~\cite{GSO} dataset to verify the zero-shot novel view synthesis capability and evaluate our method on both generated image quality and the view consistency~\cite{3dim}. 
Additionally, we apply our method to the downstream 3D reconstruction task~\cite{neus} and compare it against the mesh constructed by our baseline model.

The main contributions of this work are:
\begin{compactitem}
    \item  We propose a novel epipolar attention method to locate and retrieve the corresponding information in the reference view, which is then inserted into the generation process of the target view to enhance the consistency between multi-view images.
    \item Experimental results show that our method effectively improves the consistency of the synthesized multi-view images without any training or fine-tuning while maintaining the quality of the generated images.
    \item We apply the synthesized multi-view images to a downstream 3D reconstruction task, and the results show that the more consistent images further improve the 3D reconstruction results.
\end{compactitem} 
