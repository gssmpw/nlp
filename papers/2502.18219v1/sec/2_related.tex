\section{Related Work}
\label{sec:related_work}

\textbf{Diffusion Models for Novel View Synthesis.}
Diffusion models show impressive results on the text-to-image task~\cite{stable-diffusion, imagen, dalle2}. 
Therefore, a line of work aims to extend it to the novel view synthesis (NVS) task, where they generate novel view images based on reference images and desired relative camera poses.
Such synthesized multi-view images find utility in various applications such as distillation purposes~\cite{dreamfusion, sjc, magic3d, fantasia3d, prolificdreamer}, or for directly training NeRF-like 3D assets~\cite{nerf, neus, sparseneus}.
3DiM~\cite{3dim} implements this idea by training a diffusion model conditioned on reference images and relative camera poses.
SparseFusion~\cite{sparsefusion} and GeNVS~\cite{genvs} first generate course latent feature of the target view as additional input to the diffusion model.
However, these methods are trained on objects from specific classes or relatively small datasets, making it challenging to generalize to arbitrary objects.
Zero123~\cite{zero123} obtains impressive zero-shot generalizability by fine-tuning a 2D diffusion model, \ie, Stable Diffusion~\cite{stable-diffusion}, on a large-scale 3D rendered dataset~\cite{objaverse}.
However, novel view images generated by Zero123 can suffer from consistency problems, especially when relatively large pose transformations are present.	
To address this issue, some very recent studies~\cite{SyncDreamer, mvdream, zero123++, consistent123, ye2023consistent} try to add additional modules and fine-tune the Zero123 or LDM model to obtain better consistency, which requires significant computational resources.
In contrast to these approaches, we focus on enhancing the consistency of pre-trained models without the need for any fine-tuning. Tab.~\ref{tab:intro} provides an overview comparison, while Sec.~\ref{sec:attn_analysis} offers a detailed comparison.

\noindent\textbf{Image-to-Image Translation.}
Image-to-image translation (I2I) involves learning a mapping from an input image to an output image while preserving specific properties like the scene layout or object structure. 
Our paper's primary focus can be viewed as an I2I task, where the condition is the pose, aiming to transform the input image to the desired pose.
One of the main challenges in the pose-guided novel view generation task is maintaining consistency between the target images and the input image. 
This challenge shares similarities with the issues encountered in text-guided image-to-image translation tasks~\cite{p2p, pnp, masactrl, imagic, dragondiffusion}.
For instance, works such as~\cite{p2p, pnp, masactrl} manipulate self-attention, cross-attention, or spatial features within the U-Net~\cite{unet} structure to preserve the desired concept in the input image.
However, these methods primarily target 2D image translation or editing tasks, lacking 3D structural information and struggling to discern what to preserve or discard in the context of the NVS task. 
In contrast, our method incorporates 3D geometry information into the translation process to better preserve the desired information in the input view.

\noindent\textbf{Epipolar Geometry in DNN.}
Epipolar geometry is used in many previous works~\cite{tseng2023consistent, mvster, he2020epipolar, ecsic, suhail2022generalizable}.
They often integrate epipolar geometry into network modules and employ it for network training.
In contrast, we use the epipolar geometry to generate images without training or fine-tuning to localize better and retrieve the corresponding information using the features from a trained diffusion model.
