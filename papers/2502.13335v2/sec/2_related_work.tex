\section{Related Work}
\label{sec:related-work}

\noindent\textbf{2D Priors for 3D Inpainting.}
2D inpainting has been widely explored \cite{quan2024inpaintingsurvey}. Current state-of-the-art methods leverage conditional diffusion models \cite{stable.diffusion,zhang2023controlnet,ju2024brushnet}, which exhibit high-quality and diverse generations. However,
direct application of independent 2D inpainters for 3D inpainting
leads to 3D inconsistencies \cite{spinnerf,reference.guided.nerf}. 
Yet, despite the challenges in multiview inconsistency, recent methods \cite{weber2024nerfiller,prabhu2023inpaint3d,mirzaei2024reffusion,chen2024mvip,lin2025taming} have explored combining diffusion-based 2D inpainters with explicit or implicit 3D radiance fields to exploit their powerful generative prior.
To preserve this prior while enforcing greater 3D consistency, we further constrain a diffusion-based inpainter using scene geometry.


\noindent\textbf{3D Scene Editing.} Editing 3D %
scenes is essential for 3D content creation (e.g., for video games or virtual reality).
Spurred by the rise of
3D radiance fields
(e.g., \cite{rabby2023beyondpixels,chen2024survey}) there has been an explosion of techniques.
There are numerous forms of 3D editing, including 
scene translation 
(e.g., \cite{in2n,wys,vicanerf,koo2024posterior,chen2024dge,chen2024gaussianeditor,wu2024gaussctrl,wang2025view}),
super-resolution 
(e.g., \cite{huang2023refsr,lee2024disr}),
shape deformation 
(e.g., \cite{yuan2022nerf,zheng2023editablenerf,jambon2023nerfshop}),
appearance alterations
(e.g., \cite{mazzucchelli2024irene,kuang2023palettenerf,wang2023seal,lee2023ice}),
and
inpainting
(e.g., \cite{spinnerf,weder2023removing,nerf.in,reference.guided.nerf}),
which is the focus here.

Many 3D inpainters focus primarily on \textit{object removal} \cite{wang2025learning,lu2024view,spinnerf,weder2023removing,wang2024innerf360}.
In contrast, our method can insert additional content and perform scene completion.
RenderDiffusion \cite{anciukevivcius2023renderdiffusion} enables 3D-aware inpainting, but relies on weak supervision (utilizing only 2D supervision) and is limited to simpler scenes.
SIGNeRF \cite{dihlmann2024signerf} specializes in localized translation and object insertion, rather than general 3D inpainting. 
Chen et al.~\cite{chen2025single} examines the impact of the inpainting mask for deterministic object removal, but is restricted to forward-facing scenes.
Gaussian Grouping \cite{ye2023gaussian} integrates segmentation features directly into the radiance field, facilitating various editing tasks.
For general inpainting, most methods build on 3D radiance fields
\cite{chen2024mvip,mirzaei2024reffusion,prabhu2023inpaint3d,liu2024infusion,lin2025taming}, with several approaches (e.g., \cite{mirzaei2024reffusion,prabhu2023inpaint3d,chen2024mvip})
leveraging score distillation sampling (SDS) \cite{poole2022dreamfusion,wang2023score} to incorporate 2D diffusion priors.
However, these methods typically require a relatively \emph{dense view coverage} for 3D model fitting.
In contrast, our method inpaints the image set directly, allowing it to operate effectively even with sparse view coverage.
Similar to our work, several recent inpainters also alter diffusion models, 
specializing them for multiview inpainting \cite{cao2024mvinpainter,mirzaei2024reffusion}.
For our evaluation, we utilize the scene-centric settings of
NeRFiller \cite{weber2024nerfiller} and SPIn-NeRF \cite{spinnerf},
which covers %
a variety of scene types and camera baselines.


\noindent\textbf{Reference-Conditioned Generative Editing.}
A variety of methods have been devised to encourage consistency across multiple generations
(e.g., \cite{tewel2024training,zhou2024storydiffusion,avrahami2024chosen,sajnani2024geodiffuser}), 
usually by sharing features across diffusion processes.
However, these methods only enforce \emph{semantic} consistency, which is insufficient for precise, pixel-level coherence required for 3D-consistent content.
Separately, conditional image generators can perform NVS
by mapping observed reference images and target camera parameters to a new view (e.g., \cite{liu2023zero,shi2023zero123++,yang2024consistnet,gao2024cat3d,yu2024polyoculus,tseng2023consistent,yu2023long,chan2023generative,yu2024viewcrafter}) by training on multiview data.
However, these methods are meant for generative NVS, rather than inpainting existing 3D scenes coherently.
While our conditional diffusion model is reminiscent of this, addressing the entirety of the NVS problem is not necessary for 3D inpainting; instead, we assemble reference information (from multiple source views) in the image coordinate frame of the target view (i.e., no camera information is sent to the model).
Hence, our inpainter avoids learning complex 3D transforms (e.g., triangulating and projecting between frames). Instead, it learns to utilize projected reference information entirely in 2D, while accounting for simple 3D constraints, like relative depth ordering of generated content with respect to existing content.
Finally, ``reference-based inpainting'' \cite{zhao2023geofill,zhao20223dfill,zhou2021transfill} uses one reference image to inpaint another, sometimes handling transforms beyond viewpoint, like lighting changes. While our method also leverages reference information, it aligns more with NeRF-based approaches, operating on full multiview image sets where all views are initially masked, rather than a single reference-target pair.

\noindent\textbf{Iterative 3D Generation.} 
3D content generation is typically iterative or autoregressive. Iterative methods,
common in text-to-3D \cite{poole2022dreamfusion,mcallister2024rethinkingsds} and instruction-based 3D editing \cite{in2n}, have also been used for 3D inpainting via SDS \cite{poole2022dreamfusion} (e.g., \cite{prabhu2023inpaint3d,mirzaei2024reffusion,chen2024mvip}) and IDU \cite{in2n} (e.g., \cite{weber2024nerfiller}).
In contrast, autoregressive approaches have been explored in 3D scene generation (e.g., \cite{chung2023luciddreamer,hoellein2023text2room}) and generative NVS (e.g., \cite{yu2023long,liu2021infinitenature,rombach2021geogpt,ren2022lookout,tseng2023consistent}). We introduce an autoregressive approach for inpainting large-baseline scenes.







