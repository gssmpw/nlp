
\vspace{-2em}\section{Introduction}
\label{sec:intro}

Image inpainting is a long-standing problem in computer vision and graphics \cite{bertalmioimage}.
Unlike unconditional generation, inpainting is constrained by the conditioning input, requiring a visually-plausible output that matches the partial content.
The problem of inpainting \textit{3D scenes} has recently grown in popularity (e.g., \cite{spinnerf,nerf.in}) due to the advent of powerful novel view synthesis (NVS) models.
Such models are usually implemented as scene models capable of differentiable rendering, e.g., neural radiance fields (NeRFs) \cite{original.nerf} or 3D Gaussian splatting (3DGS) \cite{kerbl3Dgaussians}.
For both NVS and 3D inpainting, the input scene is implicitly represented through a posed set of images; hence, we may consider the equivalent problem of ``multiview inpainting'', which provides a natural interface between 2D image inpainting and 3D NVS.
     
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/teaser-smaller.pdf}
    \vspace{-15pt}
    \caption{Visualization of our target inpainting tasks.
    We target three tasks: (i) narrow-baseline object removal,
    (ii) wide-baseline scene completion and (iii) few-view inpainting.
    Here, we show examples of our outputs for each task, with
    the corresponding masked inputs shown in the top left corner of each image.
    }
    \label{fig:teaser}
    \vspace{-15pt}
\end{figure}

Multiview (3D) inpainting is significantly more constrained than even the 2D case: inpainted scene content must be \textit{geometrically} realistic and exhibit \textit{cross-view consistency}.
Cross-view consistency is particularly critical when leveraging 2D image inpainters that, by default, are neither aware of the 3D scene structure nor the current state of inpainted content in other images -- unsurprisingly, this results in inconsistent scene content. 
Yet, we still seek to exploit the powerful generative priors learned by 2D inpainters, as the paucity of 3D scene data prevents training inpainters that operate directly in 3D to the same level of quality.
This problem is likely exacerbated with increasingly powerful generative inpainters which tend to hallucinate even more aggressively \cite{reference.guided.nerf}.

A common approach, often used in NeRF editing (e.g., \cite{weber2024nerfiller}), is to fuse information across views, via 3D
radiance fields.
A cyclic process, such as iterative dataset update (IDU) \cite{in2n}, is then used, whereby images are edited, used to fit the NVS model, and NVS renders (which combine information across views) assist with new edits.
While this approach ensures consistency via the
3D radiance field, it has a few shortcomings: 
(i) a tendency towards blurriness, partly due to the fusion happening in pixel space, and
(ii) reliance on the radiance field, which requires accurate camera parameters with sufficient view coverage.

In this work, we resolve these problems by 
fusing cross-view information in a learned space, during a generative diffusion process, 
and eschewing the necessity of a 3D radiance field, though we can optionally choose to use one as a separate post-fitting step.
We further reduce the tendency of generative inpainters to over-hallucinate, a common cause of 3D inconsistencies, by conditioning them on the 3D scene structure.
In particular, we devise a geometry-aware conditional diffusion model, capable of inpainting multiview-consistent images based on geometric and appearance cues from reference images.
One key capability of our model is the handling of \textit{uncertain} or \textit{partial} scene information (e.g., missing due to occlusions and viewpoint changes).
We integrate our model into a 3D scene inpainting algorithm, performing 3D-consistent propagation of image content across views. 
Unlike NeRF-based inpainters, which fuse inconsistencies in appearance space and thus induce blur, our method fuses cross-view information via the generative model, resulting in sharper outputs even when a NeRF is fit to our final inpainted results. 
In this paper, we target three main inpainting tasks: narrow-baseline object removal,
wide-baseline scene completion, and few-view inpainting.
Most previous work has been focused on the first task, the second one
is a recent addition, and the third task has not been 
extensively explored with recent innovations. \cref{fig:teaser} provides a visualization of each of our target tasks.
We evaluate our multiview inpainter on two datasets, SPIn-NeRF \cite{spinnerf} and NeRFiller \cite{weber2024nerfiller}, which contain narrow and wide baselines, respectively, and achieve state-of-the-art 3D inpainting performance on both.
Please see our project page for visualizations of results: \href{https://geomvi.github.io/}{https://geomvi.github.io}.



















