\section{Empirical Evaluation}
\label{sec:experiments}

We evaluate our 3D scene inpainting method and compare it to previous methods across three settings: (i) 
object removal on narrow-baseline scenes,
(ii) scene completion with wide baselines, and (iii) inpainting with few-view inputs.

\subsection{Implementation Details}
We use PyTorch3D \cite{pytorch3d} for mesh rendering. For training,
we use a mixture of two datasets, MS COCO \cite{coco.dataset} and Google Scanned Objects \cite{gso.dataset}. 
For MS COCO, we synthesize the reference-based geometric cues using DepthAnything V2 \cite{depth.anything.v2}.  
Training and evaluation is performed on NVIDIA L40 GPUs (16 for training, one for inference).
Additional implementation details are provided in \S\ref{supp:sec:impl}.

\subsection{Evaluation Protocol}
\label{subsec:eval-protocol}

\noindent\textbf{Datasets.}
To evaluate our method for narrow-baseline inpainting, we use the SPIn-NeRF \cite{spinnerf} dataset, a widely used benchmark for object removal in front-facing real-world scenes. This dataset
contains
10 scenes, each with 60 images 
featuring
the object to be removed
and corresponding masks,
along with
40 images without the object as ground truth for evaluation.
In addition, we also use the scene-centric portion of the NeRFiller \cite{weber2024nerfiller} dataset to further investigate the performance of our method on more complex scenes,
particularly those with larger baselines.
This dataset serves as a benchmark for the scene completion task. 
For the few-view inpainting task, we use SPIn-NeRF and the scene-centric portion of NeRFiller, resulting in a total of 15 scenes. For each scene, we uniformly sample eight subsets of two views and eight subsets of three views. This yields a total of 240 few-view sets. Please see \S\ref{supp:sec:dataset} for details.

\noindent\textbf{Baselines.}
For object removal, we compare our method to state-of-the-art approaches on the SPIn-NeRF dataset, specifically:
SPIn-NeRF \cite{spinnerf}, which inpaints images and depth maps to supervise NeRF fitting;
Inpaint3D \cite{prabhu2023inpaint3d}, which uses SDS \cite{poole2022dreamfusion} to inpaint a NeRF with a diffusion prior;
RefFusion \cite{mirzaei2024reffusion}, which adapts an inpainting diffusion model to a reference image and uses SDS to inpaint a 3DGS \cite{kerbl3Dgaussians} with the reference-adapted diffusion model;
InFusion \cite{liu2024infusion}, which inpaints a 3DGS by inpainting the depth map of an inpainted reference image;
MVIP-NeRF \cite{chen2024mvip}, which uses SDS to inpaint a NeRF by inpainting the rendered images and normal maps via a diffusion prior,
and MALD-NeRF \cite{lin2025taming}, which inpaints a NeRF by performing masked adversarial training for per-scene customization of a diffusion model.
InFusion \cite{liu2024infusion} also evaluates Gaussian Grouping \cite{ye2023gaussian} on SPIn-NeRF, a 3DGS-based segmentation method that does not directly evaluate on SPIn-NeRF itself.
For scene completion, we compare our method to 
Stable Diffusion \cite{stable.diffusion}, which is the naive baseline of independent 2D inpainting, and NeRFiller \cite{weber2024nerfiller}, which 
alternates between editing input images, and updating the 3D representation encompassed by a NeRF
to fuse the edited images, this process is collectively called iterative dataset update. 
Finally, for the few-view task, we compare our method to SPIn-NeRF \cite{spinnerf} and NeRFiller \cite{weber2024nerfiller}. For this task, we use NeRFiller directly.
However, since SPIn-NeRF relies on supervision via COLMAP's \cite{schoenberger2016sfm} sparse depth, we disable this supervision for the few-view task, as scenes from the NeRFiller dataset lack COLMAP information.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/spinnerf-qualitative.pdf}
    \vspace{-20pt}
    \caption{
    Qualitative object removal comparisons on the SPIn-NeRF dataset. Notice other methods produce blurry regions (e.g., bench end) due to multiview inconsistencies, while ours preserves sharpness and visual plausibility. Please zoom in for details.
    }
    \label{fig:spinnerf-qualitative}
    \vspace{-15pt}
\end{figure*}

\noindent\textbf{Metrics.} 
For the SPIn-NeRF dataset, we use the same evaluation 
protocol as reported in the original paper \cite{spinnerf}.
There is no publicly available evaluation code provided with the SPIn-NeRF dataset; we confirmed the details of the protocol with the authors.
Specifically, we compute LPIPS \cite{lpips} (with VGG-16 \cite{vgg}) and FID \cite{fid} between the inpainted images and the ground-truth images, cropped by the inpainting mask's bounding box. The bounding box's size is increased by 10\% before cropping, uniformly in each direction. We additionally assess the sharpness of the inpainted images within the inpainting mask using the Laplacian variance \cite{pertuz2013analysis}.
As our inpainting method does not explicitly enforce 3D consistency
via a 3D radiance field,
we evaluate our consistency using 
the TSED metric \cite{yu2023long}.
Here, our feature correspondences are limited to those inside the inpainting 
masks.
On the SPIn-NeRF dataset, as the scenes have very small baselines, correspondences are considered across all possible view pairings.

For the NeRFiller dataset, we use the same evaluation metrics as NeRFiller \cite{weber2024nerfiller}. As these metrics are computed on NeRF renders, we fit a NeRF on our inpainted images directly. For the image metrics, PSNR, SSIM, and LPIPS, we compare the rendered training views of the fitted NeRF to the inpainted images.
For the video-based metrics, we compute MUSIQ \cite{ke2021musiq} and Corrs on a video rendered from the NeRF. 
We also evaluate the sharpness of both inpainted images and the NeRF renders.
Finally, we compute TSED to evaluate consistency across images.
As scenes in the NeRFiller dataset have a wide baseline, we only consider the two closest views in the view pairs.

Finally, for the few-view inpainting task, as there is no ground truth available, we only compute sharpness and MUSIQ to evaluate image quality. We also compute Corrs and TSED on all possible image pairs in the few-view set. All metrics are computed only inside the bounding box around the inpainting mask for the few-view inpainting task. \S\ref{supp:sec:metrics} provides additional details on computing TSED.

\subsection{Results}

\noindent\textbf{Object Removal on Narrow-baseline Scenes.}
Due to the narrow baseline, we perform single-reference inpainting for this task. As our method does not rely on fitting a NeRF on the training views, and the evaluations are performed on the test views, we follow SPIn-NeRF's \cite{spinnerf} procedure to evaluate image inpainters, e.g., LaMa \cite{lama}. Specifically, we first fit a NeRF on the training views and render the test views, which will now contain the unwanted object. The rendered test views are then used as inputs to our inpainting method.
To evaluate SPIn-NeRF, InFusion\footnote{The reported results for InFusion (arXiv-only preprint) are not qualitatively or quantitatively reproducible (others report similar issues on GitHub).}, and MVIP-NeRF, we run their official code to reproduce the results.
Since RefFusion and Inpaint3D do not provide publicly available code, we report the numbers provided by the papers.
For MALD-NeRF, we evaluate their publicly available inpainted images for the SPIn-NeRF dataset.
As shown in
\cref{tab:spinnerf},
our method outperforms all baselines on the SPIn-NeRF benchmark.
We obtain comparable sharpness and LPIPS to MALD-NeRF, while significantly outperforming it on other metrics
(sharpness and TSED). Further, 
we demonstrate the ability to handle sparse-view inpainting (see below), which cannot be easily handled by NeRF-based approaches.
Please see \S\ref{supp:subsec:maldnerf-artifacts} for further analysis.
Our method is also efficient, achieving faster scene inpainting compared to other methods.
Furthermore, 
the TSED results
show that despite other methods utilizing an explicit or implicit radiance field to enforce 3D consistency, our method achieves a higher consistency score, primarily due to blurry outputs. Please see \S\ref{supp:sec:tsed-eval} for a comprehensive TSED analysis. 
Finally, we present a set of qualitative results in \cref{fig:spinnerf-qualitative}, 
showing
our method produces sharper images 
than baselines,
indicating the efficacy of our cross-view fusion, which operates in a \textit{learned} space, rather than pixel space.

\begin{table}[t]
\centering
\tablesize
\begin{tabular}{c|ccc|c|c}
Method                               & LPIPS $\downarrow$             & FID $\downarrow$               & $\sigma \uparrow$ & $T_{2\text{px}} \uparrow$ & $\tau \downarrow$       \\ \hline
Inpaint3D
\cite{prabhu2023inpaint3d}  & 0.5150                         & 226.04                         & -                                       & -                           \\
\hline
InFusion \cite{liu2024infusion}                           & 0.4210                         & 92.62                          & -                                       & -                           & -                         \\
Gaussian Grp. \cite{ye2023gaussian} & 0.4540 & 123.48 & - & - & - \\
\hline
SPIn-NeRF
\cite{spinnerf}          & 0.4864                         & 160.42                         & \rankthreecolor13.74                    & \ranktwocolor61.04  
 & \rankthreecolor1h 40m                      \\
RefFusion
\cite{mirzaei2024reffusion} & \rankthreecolor0.4283          & -                              & -                                       & -     & -                      \\
InFusion
\cite{liu2024infusion}
& 0.6692                & 244.19                & 9.30                 & 35.88                  & \ranktwocolor14m      \\
MVIP-NeRF
\cite{chen2024mvip}      & 0.5268                         & 215.60                              & 11.96                                       & \rankthreecolor58.33 & 17h 38m                           \\
MALD-NeRF
\cite{lin2025taming}    & \rankonecolor0.3996  & \ranktwocolor130.95 & \rankonecolor35.27 & 58.22 & -   \\
Ours                                                      & \ranktwocolor0.4028 & \rankonecolor108.36 & \ranktwocolor34.50  & \rankonecolor67.35 & \rankonecolor12m
\end{tabular}\\
\justifying
\vspace{-5pt}
\caption{Quantitative evaluation of the object removal task on SPIn-NeRF dataset.
We denote sharpness ($\times 10^{-5}$) as $\sigma$, the percentage of consistent image pairs (TSED) at $T_\text{error}=2.0$px as $T_{2\text{px}}$, and average run time per scene as $\tau$.
The first two sections report numbers from \protect\cite{prabhu2023inpaint3d} and \protect\cite{liu2024infusion}, respectively, possibly using different evaluation code than SPIn-NeRF. The last section ensures direct comparability with consistent evaluation code;
see \S\ref{supp:sec:tsed-eval} for a comprehensive TSED analysis.
}
\label{tab:spinnerf}
\vspace{-18pt}
\end{table}


\begin{table*}[t]
\centering
\tablesize
\begin{tabular}{c|ccc|cc|cc|cc|c}
Method & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ & MUSIQ $\uparrow$ & Corrs $\uparrow$ & $\sigma^D \uparrow$ & $\sigma^N \uparrow$ & $T^D_{2\text{px}} \uparrow$ & $T^N_{2\text{px}}\uparrow$ & $\tau \downarrow$ \\ \hline
Stable Diffusion (2D) \cite{stable.diffusion} &
             24.69 &              0.85 &              0.10 &              3.77 &
             1120 &              \rankonecolor44.55 &             25.55 & 9.81 & 86.55 & \rankonecolor3m \\
NeRFiller w/o depth \cite{weber2024nerfiller} &
             27.96 &              0.88 &              0.07 & 3.68              &
1146              & 1.20                  & 3.18               & 14.63             &
93.51              & 1h 30m \\
NeRFiller \cite{weber2024nerfiller}           &
27.68              & 0.87              & 0.08              &             3.69 &
             1185 & 1.25 & 3.31 & 16.53 & 96.04 & 1h 30m                      \\
Ours &
\rankonecolor28.59 & \rankonecolor0.89 & \rankonecolor0.05 & \rankonecolor3.80 &
\rankonecolor1250 & 38.96 & \rankonecolor26.45 & \rankonecolor67.80 &
\rankonecolor98.25 & 55m
\end{tabular}%
\vspace{-5pt}
\caption{Evaluation of scene completion on the NeRFiller dataset.
We denote sharpness ($\times 10^{-5}$) as $\sigma$, the percentage of consistent image pairs (TSED) at $T_\text{error}=2.0$px as $T_{2\text{px}}$, average run time per scene as $\tau$, and independent 2D inpainting as ``2D''. $\cdot^D$: direct outputs of the inpainting model - $\cdot^N$: NeRF renders; see \S\ref{supp:sec:tsed-eval} for a comprehensive TSED analysis.
}
\label{tab:nerfiller}
\vspace{-10pt}
\end{table*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/nerfiller-qualitative.pdf}
    \vspace{-20pt}
    \caption{ 
    Qualitative scene completion comparisons on the NeRFiller dataset. 
    NeRFiller can converge to blurry content, due to mixing divergent views,
    while ours generates and then propagates sharp content 
    (e.g., see details in the backpack or window glass in the zoomed patches).
    Each view in a scene is denoted by $v_i$, where $i$ is the view index.
    }
        \vspace{-15pt}
    \label{fig:nerfiller-qualitative}
\end{figure*}

\noindent\textbf{Scene Completion with Wide Baselines.} 
The quantitative results for the scene completion task are presented in \cref{tab:nerfiller}, demonstrating the superiority of our method on all metrics. We also show that our method is less time-consuming than NeRFiller.
Moreover,
although fitting a NeRF 
on our inpaintings
reduces sharpness,
they
still
remain significantly sharper than
NeRFiller's.
We qualitatively illustrate this in \cref{fig:nerfiller-qualitative}.
To evaluate 3D consistency, we report TSED on two sets of images, (i) the NeRF datasets (i.e., direct outputs of the inpainting models), and (ii) the NeRF renders. For NeRFiller, we use the dataset produced in the latest Dataset Update iteration. As illustrated in 
\cref{tab:nerfiller},
as our dataset used to fit the NeRF is significantly more consistent than that of NeRFiller, the final NeRF achieves a higher consistency score.
We also significantly outperform the naive 2D-only baseline (independent inpaintings; see \S\ref{supp:subsec:independent-ablation} for details).



    

    

\noindent\textbf{Inpainting with Few-view Inputs.}
We use single-reference inpainting for the few-view task. As depicted in \cref{tab:few-view}, we outperform all the baselines on the few-view inpainting task,
even without the need to use
the ground-truth camera parameters and depth maps,
making it more self-contained.
The TSED results demonstrate our method achieves a higher consistency score, mainly due to the other methods relying on fitting a NeRF, which is suboptimal for extremely sparse views. The qualitative results shown in \cref{fig:few-view-qualitative,fig:further-few-view} also confirm the higher quality of our inpainted images.

\begin{table}[t]
\centering
\tablesize
\setlength{\tabcolsep}{0.69em}
\begin{tabular}{ccc|ccc|c|c}
Method              & $\Pi$ & $D$ & $\sigma \uparrow$ & MUSIQ $\uparrow$ & Corrs $\uparrow$ & $T_{2\text{px}} \uparrow$ & $\tau \downarrow$ \\ \hline
SPIn-NeRF \cite{spinnerf}           & \Checkmark   & \XSolidBrush & 17.18         & 3.26                & 278 & 25.42                            & 15m      \\
NeRFiller \cite{weber2024nerfiller}          & \Checkmark   & \XSolidBrush & 5.05                         & 3.41                & 187                                            & 18.54 & 13m       \\
NeRFiller \cite{weber2024nerfiller}           & \Checkmark   & \Checkmark   & 5.41                         & 3.42 & 183                          & 18.96 & 13m       \\
Ours                & \XSolidBrush & \XSolidBrush & \rankonecolor48.2            & \rankonecolor3.84   & \rankonecolor 400 & \rankonecolor52.92 & \rankonecolor 12s     
\end{tabular}%
\vspace{-5pt}
\caption{Quantitative evaluation of the few-view task. We denote camera parameters as $\Pi$, depth maps as $D$, sharpness ($\times 10^{-5}$) as $\sigma$, the percentage of consistent image pairs (TSED) at $T_\text{error}=2.0$px as $T_{2\text{px}}$, and average run time per scene as $\tau$;
see \S\ref{supp:sec:tsed-eval} for a comprehensive TSED analysis.}
\label{tab:few-view}
\vspace{-10pt}
\end{table}


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/few-view-qualitative.pdf}
    \vspace{-20pt}
    \caption{Qualitative comparisons for few-view inpainting. Our inpainted images are sharper and more visually plausible.
    }
    \vspace{-15pt}
    \label{fig:few-view-qualitative}
\end{figure}


\subsection{Ablation Studies}
\label{subsec:ablation}

In \cref{tab:ablation-summary}, 
we summarize our extensive ablation studies from \S\ref{supp:sec:full-ablation}, ablating key components of our inpainting pipeline on scene completion.
We observe (first row) the importance of conditioning the inpainter on the geometric cues (\S\ref{subsubsec:cond-inp}).
For wide-baseline datasets like NeRFiller, our autoregressive procedure (\S\ref{subsec:autoregressive}) is essential, since a single reference lacks sufficient information for a wide baseline (second row).
Finally, we find that providing \duster with ground-truth depth maps has minimal impact; however, known camera parameters significantly improve performance (third and fourth rows). This is mainly because optimizing the camera parameters in \duster requires complex global alignment, while known cameras simplify depth optimization.



\begin{table}[t]
\centering
\tablesize
\setlength{\tabcolsep}{0.5em}
\begin{tabular}{c|ccc|cc}
Variation & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ & MUSIQ $\uparrow$ & Corrs $\uparrow$ \\ \hline
No Geometric Cues   &              28.36 &              0.88 & \rankonecolor0.05 &              3.78 &              1223 \\
Single-Reference    &              27.42 &              0.88 &              0.07 &              3.77 &              1232 \\
No GT Camera        &              28.29 &              0.88 & \rankonecolor0.05 &              3.79 &              1231 \\
No GT Depth         & \ranktwocolor28.44 & \rankonecolor0.89 & \rankonecolor0.05 & \rankonecolor3.80 & \rankonecolor1252 \\
Full Model          & \rankonecolor28.59 & \rankonecolor0.89 & \rankonecolor0.05 & \rankonecolor3.80 & \ranktwocolor1250 \\
\end{tabular}
\vspace{-10pt}
\caption{
Summary of key ablations for various design choices in the model and inpainting procedure on the NeRFiller dataset.
Please see \S\ref{subsec:ablation} for an explanation and \S\ref{supp:sec:full-ablation} for our exhaustive ablation studies.}
\label{tab:ablation-summary}
\vspace{-15pt}
\end{table}





