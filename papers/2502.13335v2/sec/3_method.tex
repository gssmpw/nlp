\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\linewidth]{figures/method.pdf}
    \vspace{-20pt}
    \caption{
    Overview of our geometric-aware 3D scene inpainter.
    (a) A visualization of the reference-based geometric cues. Back-faces are always covered by shadow volumes. $T_i(I_i), F_i, B_i, C_i, \widehat{D}_i$ denote the rendered photometric content, front-face mask, back-face mask, shadow mask, and disparity, respectively, for the reference view $i$.
    (b) A step-by-step visualization of our autoregressive inpainting process. Note that the scene geometry consists of separate meshes for each image, not a single harmonized mesh, as shown here for 
    simplicity.
    Here, we are only showing one diffusion step for the geometry-aware inpainting model. $\mathcal{E}, z_t, I, M, A_{\mathcal{R}_i}, \mathcal{G}_{\mathcal{R}_i}$ denote the VAE encoder (which maps an image to the latent space of Stable Diffusion), the diffusion latent at timestep $t$, the masked image latent, the mask, the appearance cues for reference view $i$, and the geometric cues for reference view $i$, respectively; see \S\ref{supp:subsec:autoregressive-qualitative} for an example of autoregressive steps.
    }
    \vspace{-15pt}
    \label{fig:autoregressive}
\end{figure*}

\section{Method}
\label{sec:method}

\noindent\textbf{Setting.}
As with prior work \cite{spinnerf}, we assume as given 
a set of $N$ views, $I_i \in \real^{H \times W \times 3}$, representing a static scene, and 
their corresponding inpainting masks, $M_i \in \binary^{H \times W}$, demarcating the 3D region to be inpainted.
While other methods may require additional inputs, such as camera parameters or depths per image, these are optional for our approach (though we can nonetheless use them).
Our objective is to jointly inpaint the given views, thus inpainting the 3D scene, ideally in a consistent manner across views.

\noindent\textbf{Overview.}
Prior works (e.g., \cite{spinnerf,reference.guided.nerf,weber2024nerfiller,prabhu2023inpaint3d,mirzaei2024reffusion,chen2024mvip}) often have two disparate components: 
(i) 
an implicit or explicit 3D radiance field (e.g., NeRF \cite{original.nerf} and 3DGS \cite{kerbl3Dgaussians}),
which fuses information across views to ensure consistency, and
(ii) a 2D image inpainter, which alters the source views, potentially conditioned on the state of (i) (e.g., \cite{in2n,wys,weber2024nerfiller}).
However, this separation has shortcomings:
first, (i) fuses information in pixel space (due to the supervision mechanism), leading to blurry results, and second, (ii) is only aware of the other views through (i), meaning
any mechanism for enforcing consistency is \textit{indirect}.
In contrast, our algorithm relaxes the need
for a radiance field,
fuses information in the \textit{learned} space of a generative model (avoiding the blur of pixel space), and enables \textit{direct}
appearance transfer
across views, using estimated scene geometry.
To attain this, our approach consists of two models: 
a scene geometry estimator and a geometry-aware inpainting model.
For the former, we use the performant \duster \cite{dust3r}, which efficiently provides dense depth, with or without the presence of camera poses, utilizing views (inpainted or not) directly.
For the latter, we fine-tune a latent 2D diffusion-based inpainter, to condition on the other views. However, naively conditioning the inpainter forces it to
learn both inpainting and generative NVS
(itself a non-trivial task \cite{yu2024polyoculus,gao2024cat3d,tewari2023diffusion}),
by learning to map one view to another using an internal scene model.
Instead, we use our scene geometry estimator to feed appearance information from other views to our inpainter, by directly projecting information from source views, as well as passing explicit geometric information pertinent to the inpainting (e.g., occlusion). 

Given these two models, we devise a simple autoregressive scene inpainting algorithm.
At each iteration, we inpaint a subset of not-yet-inpainted views, conditioned on the other views (whether inpainted or not), followed by updating the estimated geometry.
This cyclic process gradually fills in the scene's geometry and appearance;
see \cref{fig:autoregressive} for a schematic of our approach.


For the remainder of this section, we provide details on our approach.
We first briefly review our scene geometry estimator, based on \duster \cite{dust3r} (\S\ref{subsec:sge}).
Next, we describe our geometry-aware inpainting diffusion model in \S\ref{subsec:geo-aware-diffusion}, including the reference-based geometric cues guiding each view's inpainting.
Finally, we present the autoregressive procedure used to iteratively inpaint the scene (\S\ref{subsec:autoregressive}). \S\ref{supp:sec:method}
provides additional methodological details.

\subsection{Scene Geometry Estimation}
\label{subsec:sge}
We utilize \duster \cite{dust3r} for scene geometry reconstruction.
\duster can efficiently estimate scene depths \textit{and} camera parameters from multiview image sets. As we inpaint the scene, we iteratively reapply it to update the scene geometry throughout the process; see \S\ref{supp:subsec:dust3r} for details.

\subsection{Geometry-aware Inpainting Diffusion Model}
\label{subsec:geo-aware-diffusion}

Our goal is to devise an image inpainting model, conditioned on a multiview image set and scene geometry.
The view set may
include both
masked (non-inpainted) and complete (inpainted) images,
used to inpaint the target image.
Thus, this setup is a form of reference-based inpainting, with the additional constraints of a 3D scene structure.

\noindent
\textbf{Notation.}
Formally, let $I$ be the target image to be inpainted
(i.e.,
inpainting one
image), with mask, $M$,
and camera parameters, $\Pi$ (i.e., extrinsics and intrinsics).
Each element of the reference view-set, $\mathcal{R}=\{ (I_i,M_i,b_i,\Pi_i,D_i)\}_i$, consists of an image ($I_i$), mask ($M_i$), indicator of whether $I_i$ has been inpainted
($b_i$), camera parameters ($\Pi_i$), and depth map ($D_i$).
When $b_i = 1$,
we simply set $M_i$ to nullity (i.e., all parts of $I_i$ are trustworthy).
Camera and depth information
come from our geometry estimator (\S\ref{subsec:sge}).
Thus, based on the current state of the scene, our inpainter, $f$, obtains $\widehat{I} = f(I,M|\mathcal{R})$, 
which can 
later
be
used in the reference view-set of future inpaintings (see \S\ref{subsec:autoregressive}).

\subsubsection{Coordinate-aligned Conditional Inpainting.}
\label{subsubsec:cond-inp}
Our conditioning approach leverages geometric knowledge of the scene.
In particular, \textit{%
using scene geometry, we project appearance and geometric information from references into the target camera's image plane};
this alleviates the need for the network to learn geometry estimation and view transforms (as in NVS), saving its capacity for inpainting.
Given depth and cameras, from \S\ref{subsec:sge}, this is a simple
projective mapping.
However, three issues remain: 
(i) selecting cues,
(ii) fusing cues across views, and
(iii) training the inpainter to handle errors in estimated scene geometry.
We first consider (i), then discuss (ii) in \S\ref{subsec:mrcond} and (iii) in \S\ref{subsec:inptrain}.

\noindent\textbf{Preliminaries.}
For each reference image, $I_i$, let $T_i$ be the projective transform from the $i$th camera to the target frame ($I$).
This is performed by constructing a mesh, $S_i$, from $D_i$,
assigning cues as nodal attributes,
and rendering $S_i$ into the target frame via $\Pi$ (see \S\ref{supp:subsec:mesh} for details).

\noindent\textbf{Appearance Cues.}
We provide the inpainter, $f$, with two appearance cues:
(a) direct reference pixels and (b) a stylistic hint.
For (a), we
pass $T_i(I_i)$
instead of $I_i$,
giving
the inpainter direct access to view-aligned pixel
colours for localized convolutional processing.
However, with large camera baselines, projected information from distant viewpoints may be insufficient 
(e.g., 
only the back-face triangles of an object are visible).
To ensure a \textit{stylistic} harmony, we introduce (b), an optional ``hint image'', $H$, which is not projected through the scene geometry but rather provides global appearance characteristics.
When used, we set $H$ as the furthest inpainted image from $I$ in $\mathcal{R}$ (see \S\ref{supp:subsec:hint}).
We denote our appearance cues as
$A_{\mathcal{R}} = \{ T_i(I_i) \}_i \cup \{ H \}$.



\noindent\textbf{Geometric Cues.}
We next obtain a set of geometric cues, controlling
the
reliability
of photometric reference content,
illustrated in \cref{fig:autoregressive} (a).
Specifically, per reference, we compute a
(i) front-face mask,
(ii) back-face mask,
(iii) normalized inverse depth (ordering),
and
(iv) shadow mask,
all in the target coordinate frame.
The mesh $S_i$ has front- and back-faces, indicating the validity of photometric content (i.e., the former has valid appearance from $I_i$, but the latter merely implicates the presence of geometry).
Thus, (i), (ii), and (iii) are 
rendered from
$S_i$,
denoted $F_i$, $B_i$, and $\widehat{D}_i = T_i(D_i)$, respectively.
For (iv), $S_i$ implies a ``shadow volume'' \cite{mccool2000shadow,haller2003real} with respect to $\Pi_i$, 
representing
the 3D space \textit{hidden from the reference camera} 
(see \cref{fig:autoregressive} and \S\ref{supp:subsec:shadow} for details),
which 
is rendered
into the target view, written $C_{i}$.
From the target view, any pixel with a camera ray hitting the shadow volume is explicitly uncertain -- there could be content there or not.
These maps
form
our geometric cue set,
$ \mathcal{G}_\mathcal{R} = \{ F_i, B_i, \widehat{D}_i, C_i \}_i $,
another input to our diffusion model.
Importantly, these cues \textit{also} enable a {hierarchical, confidence-based} fusion of reference information, based on the uncertainty induced by the geometric structure.




\subsubsection{Uncertainty-aware Multireference Conditioning}
\label{subsec:mrcond}

Using our cue-based notation, our method inpaints
an image
$ \widehat{I} = f(I,M|A_\mathcal{R}, \mathcal{G}_\mathcal{R}) $, based on a reference set $\mathcal{R}$.
However, fusing information across references is
challenging,
especially
with
conflicting
content.
We address this by running parallel per-reference diffusion processes and fusing
noise estimates at each diffusion step.
Ideally, this fusion is geometry-aware
(e.g., front-faces of $S_i$ provide
high-certainty
photometric content,
while back-faces only upper bound target view depth).
Since geometric maps
may be slightly misaligned
due to errors in estimated geometry, we alter the diffusion model to predict confidence maps for each reference, 
emulating the aligned geometric masks.
In the following, we describe how our inpainting is implemented, including confidence estimation and 
fusion.


\noindent\textbf{Hierarchical Confidence Estimation.}
The geometric cues, $\mathcal{G}_\mathcal{R}$, 
indicate
the reliable parts of each reference.
We
utilize three geometric signals.
First, the \textit{front-facing confidence mask}, $\CB_f$, indicates a pixel is either outside the inpainting mask 
or guided by a front-facing rendered pixel. In other words, the model has copied the photometric content from the target image itself or the rendered photometric content ($T_i(I_i)$). 
Second, the \textit{back-facing confidence mask}, $\CB_b$, indicates a pixel is geometrically
restricted
by a rendered \textit{back}-face, 
suggesting
new geometry
to
be generated \textit{in front} of it.
Finally, the \textit{shadow confidence mask}, $\CB_s$, signals the model’s certainty in trusting photometric information despite \textit{potential} occlusion (shadow volume).
Notice the confidence masks are closely related to the geometric cues $F_i$, $B_i$, and $C_i$.
Importantly, though, 
these cues
are often slightly misaligned with the actual target image due to geometry estimation errors. 
Thus, we instead modify our diffusion model to \textit{estimate these
confidence masks},
i.e., at every diffusion step, our inpainter not only generates a denoising estimate, but also provides an estimate of these three confidence masks. Please see \S\ref{supp:subsec:training} for details on supervising the confidence masks, and \cref{fig:fusion} for a visualization.

\noindent\textbf{Parallel Diffusion Processing.}
We now formalize the inpainting process.
Given $\mathcal{R}$ and our diffusion model, $f$, 
we split the inpainter into $n=|\mathcal{R}|$ independent streams.
Let 
$ (\varepsilon_{i,t}, \CB_{f,i,t}, \CB_{b,i,t}, \CB_{s,i,t}) = f(I,M|A_i, \mathcal{G}_i) $, 
where $\varepsilon_{i,t}$ is the estimated noise for reference $i$, $A_i = \{H, T_i(I_i)\}$, and $\mathcal{G}_i = \{ F_i, B_i, \widehat{D}_i, C_i \}$, be the output of the $i$th process at
time $t$.
Denote $\mathcal{C}_t = \{\CB_{f,i,t}, \CB_{b,i,t}, \CB_{s,i,t}\}_{i}$
as the combined confidence
maps.
We then fuse the noise estimates,
$ \mathcal{E}_t = \{ \varepsilon_1, \ldots, \varepsilon_n \}$,
to obtain a
fused
estimate, 
$\varepsilon_t = \Gamma(\mathcal{E}_t, \mathcal{C}_t) $,
using our fusion operator, which follows a simple rule-based hierarchy, described in \S\ref{subsubsec:geo-aware-inpainting}.
Before the next timestep, $t-1$, $\varepsilon_t$ is used to update
the noisy latent $z_t$, which is shared
across reference streams.
This fuses multiview information in the \textit{learned, generative} space of the diffusion noise estimate, rather than in pixel space.



\subsubsection{Multiview-aware Training}
\label{subsec:inptrain}

We initialize our
geometry-aware
inpainter with Stable Diffusion v2, fine-tuned for inpainting \cite{stable.diffusion,sdinp}.
Following prior work \cite{ip2p}, we condition on cues by adding zero-initialized channels to the first convolutional layer of the
UNet \cite{u.net}.
We also modify the UNet to output confidence masks alongside noise estimates at each timestep.
For training, 
one
can utilize a multiview dataset (real or synthetic); however, to ensure greater data diversity, we 
also 
\textit{synthesize} data from single-view images, via monocular depth estimation, perturbing the virtual camera, and rendering the starting image as a reference.
To simulate geometry estimation errors, we artificially perturb the reference mesh; in that case, supervision for the confidence masks can be generated via the \textit{un}perturbed mesh; 
see \S\ref{supp:subsec:training} for additional details.










\subsection{Autoregressive Scene Inpainting}
\label{subsec:autoregressive}

With our geometry estimator (\S\ref{subsec:sge}) and geometry-aware inpainter (\S\ref{subsec:geo-aware-diffusion}), we can now iteratively inpaint the entire scene.
To begin, we initialize the scene geometry by computing multi-view metric depth maps of the \textit{in}complete input views. 
We also initialize the ``autoregressive set'' with the incomplete input images, which will be autoregressively inpainted. 
A random view is then selected to start the inpainting. 
Each iteration consists of three steps: 
(i) 
inpainting
a subset of not-yet-inpainted images,
(ii) updating the autoregressive set and the scene geometry with the inpainted images, and 
(iii) selecting a subset of images
to be inpainted
at
the next autoregressive iteration. A high-level illustration of these steps and a step-by-step example are provided in \cref{fig:autoregressive} (b) and \S\ref{supp:subsec:autoregressive-qualitative}, respectively.

\subsubsection{Geometry-aware Inpainting}
\label{subsubsec:geo-aware-inpainting}
For each target view, we select a subset of reference views from the autoregressive set,
prioritizing those already inpainted.
Following \S\ref{subsec:geo-aware-diffusion}, we render appearance and geometric cues from all reference views, run parallel diffusion processes,
and fuse noise estimates at each step via the predicted confidences. 
The fusion, $\Gamma$, follows a four-level confidence hierarchy: (i) front-face confidence, (ii) back-face confidence, (iii) shadow confidence, and (iv) no confidence. For each patch at each level, we select the noise estimate from the closest camera among the views at the same level. This 
ensures
fusion of
the most reliable
reference information
during
denoising;
see \S\ref{supp:subsec:fusion} for additional details.

\subsubsection{Autoregressive Set and Scene Geometry Update}

In this step, we replace the target views in the autoregressive set with their inpainted versions and update their geometry
using the
inpainted images, as detailed in \S\ref{supp:subsec:dust3r}.

\subsubsection{Selecting the Next Images to Inpaint}
\label{subsubsec:next-images}

We employ a two-stage strategy for the autoregressive process. First, we inpaint a wide-baseline subset of the scene, one by one,
to \textit{generate} the missing content of the scene. Then, we \textit{propagate} the generated content to the remaining views simultaneously.
At the start of
the first stage, we use a greedy min-max approach to select the wide-baseline subset. Beginning with the first view, we iteratively add the view that maximizes the minimum distance to the
existing subset.
Once selected, we order them to minimize distance to prior views
(see \S\ref{supp:subsec:wide-baseline} for details).
The wide-baseline stage
follows the
sorted order,
with each target view conditioned on the entire autoregressive set,
inpainted or not.
In the propagation step, 
remaining views
are conditioned only on the inpainted wide-baseline images.
