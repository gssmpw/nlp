\section{Conclusion}

In this paper, we introduced a novel approach to 3D scene inpainting task, \textit{without}
a 3D radiance field.
While this avoids fusing in pixel space, which induces blurriness, it necessitates a novel way to fuse information across views via estimated scene geometry.
We do so via training a diffusion-based inpainter, conditioned on appearance and geometric cues from a reference view-set, which enables fusion in the learned space of the generative model, thus retaining sharpness.
The resulting multiview inpainting algorithm is highly versatile, 
capable of handling the sparse-view inpainting task, on which other methods struggle, and able to operate without camera poses.
We explored the efficacy of our method on two recent benchmarks, encompassing narrow- and wide-baseline 3D scenes, as well as the few-view scenario, showing state-of-the-art performance in all cases, in terms of both image quality and multiview consistency.





