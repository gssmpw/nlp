\documentclass[a4paper]{article} % 一般的なスタイルの書き方
\usepackage[margin=1in]{geometry}
\renewcommand{\baselinestretch}{1.3}

\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{enumitem}

\usepackage{amsmath,amssymb,amsthm}
\usepackage{bm}
\usepackage{natbib}
\usepackage{graphicx,here,comment}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{xcolor}
\usepackage{wrapfig}
\usepackage{footnote}
\usepackage{multirow}
\makesavenoteenv{table}  % table 環境で脚注を保存できるようにする

\usepackage{mathtools}

\usepackage{hyperref}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{assumption}{Assumption}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}
\newtheorem{conjecture}{Conjecture}

\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\indep}{\perp \!\!\! \perp}
\def\ci{\perp\!\!\!\perp}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\bignorm}[1]{\left\lVert#1\right\rVert}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\ceil}[1]{\lceil #1 \rceil}
\newcommand{\rbr}[1]{\left(#1\right)}
\newcommand{\sbr}[1]{\left[#1\right]}
\newcommand{\cbr}[1]{\left\{#1\right\}}
\newcommand{\abr}[1]{\left\langle #1\right\rangle}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
% \newcommand{\bH}{\mathbb{H}}
\newcommand{\mF}{\mathcal{F}}
\newcommand{\mG}{\mathcal{G}}
\newcommand{\bG}{\mathbb{G}}
\newcommand{\mA}{\mathcal{A}}
\newcommand{\mB}{\mathcal{B}}
\newcommand{\mC}{\mathcal{C}}
\newcommand{\mD}{\mathcal{D}}
\newcommand{\mI}{\mathcal{I}}
\newcommand{\mJ}{\mathcal{J}}
\newcommand{\mH}{\mathcal{H}}
\newcommand{\mK}{\mathcal{K}}
\newcommand{\mS}{\mathcal{S}}
\newcommand{\mT}{\mathcal{T}}
\newcommand{\mO}{\mathcal{O}}
\newcommand{\mP}{\mathcal{P}}
\newcommand{\mM}{\mathcal{M}}
\newcommand{\mN}{\mathcal{N}}
\newcommand{\mX}{\mathcal{X}}
\newcommand{\mY}{\mathcal{Y}}
\newcommand{\mR}{\mathcal{R}}
\newcommand{\fD}{\mathfrak{D}}
\newcommand{\fR}{\mathfrak{R}}
% \newcommand{\bX}{\mathbb{X}}
% \newcommand{\bY}{\mathbb{Y}}
\newcommand{\Ep}{\mathbb{E}}
\renewcommand{\Pr}{\mathbb{P}}
\newcommand{\Tr}{\mathrm{T}}
\newcommand{\Cr}{\mathrm{C}}
\newcommand{\Exp}{\mathrm{Exp}}
\newcommand{\Inj}{\mathrm{Inj}}
\newcommand{\Dr}{\mathrm{D}}
\newcommand{\Sr}{\mathrm{S}}
\newcommand{\mE}{\mathcal{E}}
\newcommand{\mW}{\mathcal{W}}
\newcommand{\mZ}{\mathcal{Z}}
\newcommand{\mfM}{\mathfrak{M}}
\newcommand{\mfE}{\mathfrak{E}}
\renewcommand{\hat}{\widehat}
\renewcommand{\tilde}{\widetilde}
\newcommand{\pr}{\mbox{Pr}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\renewcommand{\d}{\mathrm{d}}
\newcommand{\mone}{\textbf{1}}
% \renewcommand{\span}{\mathrm{span}}
\def\bX{\mathbf{X}}
\def\bY{\mathbf{Y}}
\def\bK{\mathbf{K}}
\def\btheta{\boldsymbol{\theta}}
\def\bbeta{\boldsymbol{\beta}}
\def\bfeta{\boldsymbol{\eta}}
\def\ba{\bm{a}}
\def\bff{\bm{f}}
\def\bk{\bm{k}}
\def\br{\bm{r}}
\def\bs{\bm{s}}
\def\bt{\bm{t}}
\def\bu{\bm{u}}
\def\bI{\bm{I}}
\def\bv{\bm{v}}
\def\bw{\bm{w}}
\def\bx{\bm{x}}
\def\by{\bm{y}}
\def\bz{\bm{z}}

\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Banach}{\mathbb{B}}
\newcommand{\Hilbert}{\mathbb{H}}
\newcommand{\trace}{\mathrm{tr}}
\newcommand{\prox}{\mathrm{prox}}
\newcommand{\diag}{\mathrm{diag}}

\newcommand{\sinc}{\operatorname{sinc}}
\DeclareMathOperator\E{\mathbb{E}}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Corr}{Corr}

\usepackage{color}
\newcommand{\updated}[1]{#1}
\usepackage{newtxtext,newtxmath}

\newcommand{\secref}[1]{Section~\ref{#1}}
\newcommand{\appref}[1]{Appendix~\ref{#1}}
\newcommand{\algoref}[1]{Algorithm~\ref{#1}}
\newcommand{\asmpref}[1]{Assumption~\ref{#1}}
\newcommand{\thmref}[1]{Theorem~\ref{#1}}
\newcommand{\lemref}[1]{Lemma~\ref{#1}}
\newcommand{\corref}[1]{Corollary~\ref{#1}}
\newcommand{\exref}[1]{Example~\ref{#1}}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\tabref}[1]{Table~\ref{#1}}

\newcommand{\1}{\mbox{1}\hspace{-0.25em}\mbox{l}}

\newcommand{\sek}{k_{\mathrm{SE}}}
\newcommand{\matk}{k_{\mathrm{Mat\acute{e}rn}}}
\newcommand{\cse}{C_{\mathrm{SE}}}
\newcommand{\cmat}{C_{\mathrm{Mat}}}
\newcommand{\cset}{\tilde{C}_{\mathrm{SE}}}
\newcommand{\cmatt}{\tilde{C}_{\mathrm{Mat}}}
\newcommand{\tse}{\overline{T}_{\mathrm{SE}}}
\newcommand{\tmat}{\overline{T}_{\mathrm{Mat}}}
\newcommand{\utse}{\underline{T}_{\mathrm{SE}}}
\newcommand{\utmat}{\underline{T}_{\mathrm{Mat}}}
\newcommand{\utlse}{\underline{T}_{\mathrm{SE}}^{(\lambda)}}
\newcommand{\utlmat}{\underline{T}_{\mathrm{Mat}}^{(\lambda)}}
\newcommand{\ulse}{\underline{\lambda}_{\mathrm{SE}}}
\newcommand{\ulmat}{\underline{\lambda}_{\mathrm{Mat}}}

\usepackage{authblk}

\title{Gaussian Process Upper Confidence Bound Achieves Nearly-Optimal Regret in Noise-Free Gaussian Process Bandits}
\date{}
\author{Shogo Iwazaki}
% \author[2,3]{Shion Takeno}

\affil{LY Corporation}
% \affil[2]{Nagoya University}
% \affil[3]{RIKEN AIP}

\affil{\texttt{{shogo.iwazaki@gmail.com}}}
% \affil[2]{\texttt{{takeno.shion.m6@f.mail.nagoya-u.ac.jp}}}

\begin{document}
\maketitle

\begin{abstract}
We study the noise-free Gaussian Process (GP) bandits problem, in which the learner seeks to minimize regret through noise-free observations of the black-box objective function lying on the known reproducing kernel Hilbert space (RKHS).
Gaussian process upper confidence bound (GP-UCB) is the well-known GP-bandits algorithm whose query points are adaptively chosen based on the GP-based upper confidence bound score.
Although several existing works have reported the practical success of GP-UCB, the current theoretical results indicate its suboptimal performance. However, GP-UCB tends to perform well empirically compared with other nearly optimal noise-free algorithms that rely on a non-adaptive sampling scheme of query points. 
This paper resolves this gap between theoretical and empirical performance by showing the nearly optimal regret upper bound of noise-free GP-UCB. Specifically, our analysis shows the first constant cumulative regret in the noise-free settings for the squared exponential kernel and Mat\'ern kernel with some degree of smoothness.
\end{abstract}

\section{Introduction}
This paper studies the noise-free Gaussian Process (GP) bandits problem, where the learner seeks to minimize regret through noise-free observations of the black-box objective function. Several existing works tackle this problem, and some of them~\citep{salgiarandom,iwazaki2025improvedregretanalysisgaussian} show the algorithms whose regret nearly matches the conjectured lower bound of \citep{vakili2022open}. For ease of theoretical analysis, these algorithms rely on the non-adaptive sampling scheme, whose query points are chosen independently from the observed function values, such as the uniform sampling~\citep{salgiarandom} or maximum variance reduction~\citep{iwazaki2025improvedregretanalysisgaussian}. 
Although the theoretical superiority of such non-adaptive algorithms is shown, their empirical performances are known to be worse than those of adaptive strategy, whose current regret upper bound may have significant room for improvement. From this motivation, our work aims to show the nearly optimal regret of GP upper confidence bound (GP-UCB)~\citep{srinivas10gaussian}, which is one of the well-known adaptive GP bandit algorithms, and its existing guarantees only show strictly sub-optimal regret in noise-free setting~\citep{lyu2019efficient,kim2024bayesian}. 

\paragraph{Contributions.} Our contributions are summarized below:
\begin{itemize}
    \item We give a refined regret analysis of GP-UCB (Theorems~\ref{thm:cr_gp-ucb} and \ref{thm:sr_gp-ucb}), which matches both the conjectured cumulative regret lower bounds of \citep{vakili2022open} and the simple regret lower bound of \citep{bull2011convergence} up to polylogarithmic factors in the Mat\'ern kernel. Regarding cumulative regret, our analysis shows that GP-UCB achieves the constant $O(1)$ regret under squared exponential and Mat\'ern kernel with $d > \nu$. Here, $d$ and $\nu$ denote the dimension and smoothness parameter of the Mat\'ern kernel, respectively. The results are summarized in Tables~\ref{tab:nl_cr_compare} and \ref{tab:nl_sr_compare}.
    \item Specifically, our key theoretical contribution is the new algorithm-independent upper bound for the observed posterior standard deviations (Lemmas~\ref{lem:pv_ub}--\ref{lem:cpv_ub}).
    As discussed in Remark~\ref{rem:gen}, this result has the potential to translate existing confidence bound-based algorithms for noisy settings into nearly optimal noise-free variants beyond the analysis of GP-UCB.
    \item We also show the first algorithm-independent lower bound for deterministic regret under Mat\'ern kernel with $d > \nu$ (\thmref{thm:cr_lb}). This result formally validates the conjectured lower bound of \citep{vakili2022open} up to the poly-logarithmic factor and the near-optimality of our analysis of GP-UCB.
\end{itemize}

\paragraph{Related works.} 
A lot of existing works study the theory for the noisy GP bandits~\citep{srinivas10gaussian,valko2013finite,chowdhury2017kernelized,scarlett2017lower,li2022gaussian}. 
Regarding noise-free settings, to our knowledge, \citep{bull2011convergence} is the first work that shows both the upper bound and the lower bound for simple regret via the expected improvement (EI) strategy. After that, the analysis of the cumulative regret is shown in \citep{lyu2019efficient} with GP-UCB. Recently, several works studied the improved algorithm to achieve superior regret to the result of \citep{lyu2019efficient} based on the conjectured lower bound provided in \citep{vakili2022open}. 
Although several works show the conjectured nearly-optimal algorithm~\citep{salgiarandom,iwazaki2025improvedregretanalysisgaussian}, their algorithms are based on a non-adaptive sampling scheme, whose inferior performances were reported in the existing work~\citep{li2022gaussian}. On the other hand, our proof technique relates to the analysis of \citep{iwazaki2025improvedregretanalysisgaussian} for the non-adaptive maximum variance reduction algorithm. The theoretical analysis of this paper can be interpreted as a generalization of the result of \citep{iwazaki2025improvedregretanalysisgaussian} tailored to a noise-free setting.
Finally, in contrast to the frequentist assumption that this paper focuses on, some existing works also study the Bayesian assumption of the objective function~\citep{grunewalder2010regret,srinivas10gaussian,freitas2012exponential,scarlett2018tight}.

\begin{table}[tb]
    \centering
    \caption{Comparison between existing noiseless algorithms' guarantees for cumulative regret and our result. 
    In all algorithms, the smoothness parameter of the Mat\'ern kernel is assumed to be $\nu > 1/2$.
    Furthermore, $d$, $\ell$, $\nu$, and $B$ are supposed to be $\Theta(1)$ here. ``Type'' column shows that the regret guarantee is  (D)eterministic or (P)robabilistic. Here, we describe the regret bound as “deterministic” if the regret upper or lower bound always holds without probabilistic arguments.
    Throughout this paper, $\tilde{O}(\cdot)$ denotes the order notation whose poly-logarithmic dependence is ignored.
    }
    \begin{tabular}{c|c|c|c|c|c|l}
    \multicolumn{1}{c|}{\multirow{2}{*}{Algorithm}} & \multicolumn{1}{|c|}{\multirow{2}{*}{Regret (SE)}} & \multicolumn{3}{|c|}{Regret (Mat\'ern)} & \multirow{2}{*}{Type} & \multirow{2}{*}{Remark} \\ \cline{3-5}
    \multicolumn{1}{c|}{}  & \multicolumn{1}{|c|}{}  & $\nu < d$  & $\nu = d$  & $\nu > d$ &  & \\ \hline \hline
     GP-UCB & \multirow{3}{*}{$O\rbr{\sqrt{T \ln^{d} T}}$} & \multicolumn{3}{|c|}{\multirow{3}{*}{$\tilde{O}\rbr{T^{\frac{\nu + d}{2\nu + d}}}$}} & \multirow{3}{*}{D} & \\ 
     \citep{lyu2019efficient} & & \multicolumn{3}{|c|}{} & & \\ 
     \citep{kim2024bayesian} & & \multicolumn{3}{|c|}{} & & \\ \hline
     Explore-then-Commit & \multirow{2}{*}{N/A} & \multicolumn{3}{|c|}{\multirow{2}{*}{$\tilde{O}\rbr{T^{\frac{d}{\nu + d}}}$}} & \multirow{2}{*}{P} & \\ 
     \citep{vakili2022open} &  & \multicolumn{3}{|c|}{} & & \\ \hline
         Kernel-AMM-UCB
      & \multirow{2}{*}{$O\rbr{\ln^{d+1} T}$} & \multicolumn{3}{|c|}{\multirow{2}{*}{$\tilde{O}\rbr{T^{\frac{\nu d + d^2}{2\nu^2 + 2\nu d + d^2}}}$}} & \multirow{2}{*}{D} & \\ 
      \citep{flynn2024tighter}
      &  & \multicolumn{3}{|c|}{} & & \\ \hline
     REDS & \multirow{2}{*}{N/A} & \multirow{2}{*}{$\tilde{O}\rbr{T^{\frac{d - \nu}{d}}}$} & \multirow{2}{*}{$O\rbr{\ln^{\frac{5}{2}} T}$} & \multirow{2}{*}{$O\rbr{\ln^{\frac{3}{2}} T}$} & \multirow{2}{*}{P} & Assumption for \\
     \citep{salgiarandom} &  &  &  &  & & level-set is required. \\ \hline
     PE & \multirow{2}{*}{$O\rbr{\ln T}$} & \multirow{2}{*}{$\tilde{O}\rbr{T^{\frac{d - \nu}{d}}}$} & \multirow{2}{*}{$O\rbr{\ln^{2 +\alpha} T}$} & \multirow{2}{*}{$O\rbr{\ln T}$} & \multirow{2}{*}{D} & $\alpha > 0$ is an arbitrarily  \\ 
     \citep{iwazaki2025improvedregretanalysisgaussian} & & & & &  & fixed constant. \\ \hline
      \textbf{GP-UCB} & \multirow{2}{*}{$O\rbr{1}$} & \multirow{2}{*}{$\tilde{O}\rbr{T^{\frac{d - \nu}{d}}}$} & \multirow{2}{*}{$O\rbr{\ln^{2} T}$} & \multirow{2}{*}{$O\rbr{1}$} & \multirow{2}{*}{D} &  \\ 
     \textbf{(Our analysis)} & & & & &  & \\ \hline
     Conjectured Lower Bound & \multirow{2}{*}{N/A} & \multirow{2}{*}{$\Omega\rbr{T^{\frac{d - \nu}{d}}}$} & \multirow{2}{*}{$\Omega(\ln T)$} & \multirow{2}{*}{$\Omega(1)$} & \multirow{2}{*}{N/A} & \\
     \citep{vakili2022open} & & & & & & \\ \hline
     \textbf{Lower Bound} & \multirow{2}{*}{N/A} & \multirow{2}{*}{$\Omega\rbr{T^{\frac{d - \nu}{d}}}$} & \multirow{2}{*}{N/A} & \multirow{2}{*}{N/A} & \multirow{2}{*}{D} & \\
     \textbf{(Ours)} & & & & & & \\
    \end{tabular}
    \label{tab:nl_cr_compare}
\end{table}


\begin{table}[tb]
    \centering
    \caption{Comparison between existing noiseless algorithms' guarantees for simple regret and our result. 
    In all algorithms except for GP-UCB+ and EXPLOIT+, the smoothness parameter of the Mat\'ern kernel is assumed to be $\nu > 1/2$.}
    \begin{tabular}{c|c|c|c|l}
    \multirow{2}{*}{Algorithm} & \multirow{2}{*}{Regret (SE)} & \multirow{2}{*}{Regret (Mat\'ern)} & \multirow{2}{*}{Type} & \multirow{2}{*}{Remark} \\
     &  & & &  \\ \hline \hline
    GP-EI & \multirow{2}{*}{N/A} & \multirow{2}{*}{$\tilde{O}\rbr{T^{-\frac{\min\{1, \nu\}}{d}}}$} & \multirow{2}{*}{D} & \\ 
    \citep{bull2011convergence} &  &  & & \\ \hline
    GP-EI with $\epsilon$-Greedy & \multirow{2}{*}{N/A} & \multirow{2}{*}{$\tilde{O}\rbr{T^{-\frac{\nu}{d}}}$} & \multirow{2}{*}{P} & \\ 
    \citep{bull2011convergence} &  &  & & \\ \hline
    GP-UCB & \multirow{3}{*}{$O\rbr{\sqrt{\frac{\ln^{d} T}{T}}}$} & \multirow{3}{*}{$\tilde{O}\rbr{T^{-\frac{\nu}{2\nu + d}}}$} & \multirow{3}{*}{D} & \\
    \citep{lyu2019efficient} &  &  & & \\
    \citep{kim2024bayesian} &  &  & & \\ \hline
    Kernel-AMM-UCB & \multirow{2}{*}{$O\rbr{\frac{\ln^{d+1} T}{T}}$} & \multirow{2}{*}{$\tilde{O}\rbr{T^{-\frac{\nu d + 2\nu^2}{2\nu^2 + 2\nu d + d^2}}}$} & \multirow{2}{*}{D} & \\
    \citep{flynn2024tighter} &  &  & & \\ \hline
    GP-UCB+,  & \multirow{3}{*}{$O\rbr{\exp\rbr{-CT^{\frac{1}{d}-\alpha}}}$} & \multirow{3}{*}{$O\rbr{T^{-\frac{\nu}{d}+\alpha}}$} & \multirow{3}{*}{P} & $\alpha > 0$ is an arbitrarily \\ 
    EXPLOIT+ & & & & fixed constant.  \\ 
    \citep{kim2024bayesian} & & & & $C > 0$ is some constant. \\ \hline
    MVR & \multirow{2}{*}{$O\rbr{\exp\rbr{-\frac{1}{2}T^{\frac{1}{d+1}}\ln^{-\alpha} T}} $} & \multirow{2}{*}{$\tilde{O}\rbr{T^{-\frac{\nu}{d}}}$} & \multirow{2}{*}{D} & $\alpha > 0$ is an arbitrarily \\
    \citep{iwazaki2025improvedregretanalysisgaussian} & & & & fixed constant. \\ \hline
    \textbf{GP-UCB} & \multirow{2}{*}{$O\rbr{\sqrt{T}\exp\rbr{-\frac{1}{2} \cset T^{\frac{1}{d+1}}}} $} & \multirow{2}{*}{$\tilde{O}\rbr{T^{-\frac{\nu}{d}}}$} & \multirow{2}{*}{D} & $\cset > 0$ is defined \\
    \textbf{(Our analysis)} & & & & in \thmref{thm:sr_gp-ucb}. \\ \hline
    Lower Bound & \multirow{2}{*}{N/A} & \multirow{2}{*}{$\Omega\rbr{T^{-\frac{\nu}{d}}}$} & \multirow{2}{*}{N/A} & \\
    \citep{bull2011convergence} & & & & \\
    \end{tabular}
    \label{tab:nl_sr_compare}
\end{table}

\section{Preliminaries}
This paper studies the noise-free Gaussian process (GP) bandit problem. Let $f: \mX \rightarrow \R$ be a black-box objective function whose input domain $\mX \subset \R^d$ is compact.
At each step $t \in \N_+$, the learner chooses query point $\bx_t \in \mX$; after that, the corresponding function value $f(\bx_t)$ is returned. Under the total step size $T \in \N_+$, the learner's goal is to minimize 
one of the following metrics: cumulative regret $R_T$ and simple regret $r_T$, which are respectively defined as
\begin{align}
    R_T &= \sum_{t \in [T]} f(\bx^{\ast}) - f(\bx_t), \\
    r_T &= f(\bx^{\ast}) - f(\hat{\bx}_T).
\end{align}
Here, we define $[T] \coloneqq \{1, \ldots, T\}$ and $\bx^{\ast} \in \mathrm{arg~max}_{\bx \in \mX} f(\bx)$.
Furthermore, $\hat{\bx}_T \in \mX$ is the estimated maximizer
returned by the algorithm at the end of step $T$.



\paragraph{Regularity Assumptions.} We suppose 
the following assumption for the underlying objective function.
\begin{assumption}
    \label{asmp:func}
    The objective function $f$ is an element of reproducing kernel Hilbert space (RKHS), endowed with a known positive definite kernel $k: \mX \times \mX \rightarrow \R$.
    Furthermore, assume $k(\bx, \bx) \leq 1$ for all $\bx \in \mX$, and $\|f\|_{k} \leq B < \infty$, where $\|f\|_{k}$ denote the RKHS norm of $f$.
\end{assumption}

\asmpref{asmp:func} is a standard assumption in GP-bandits literature~\citep{srinivas10gaussian,scarlett2017lower,chowdhury2017kernelized}. 
Specifically, we focus on the following squared exponential (SE) kernel $\sek$ and Mat\'ern kernel $\matk$:
\begin{align}
    \sek(\bx, \tilde{\bx}) &= \exp\rbr{\frac{\|\bx - \tilde{\bx}\|_2^2}{2\ell^2}}, \\
    \matk(\bx, \tilde{\bx}) &= \frac{2^{1-\nu}}{\Gamma(\nu)} \rbr{\frac{\sqrt{2\nu} \|\bx - \tilde{\bx}\|_2}{\ell}} J_{\nu}\rbr{\frac{\sqrt{2\nu} \|\bx - \tilde{\bx}\|_2}{\ell}},
\end{align}
where $\ell > 0$ and $\nu > 0$ are the lengthscale
and smoothness parameters, respectively. 
Futhermore, $J_{\nu}(\cdot)$ and $\Gamma(\cdot)$ respectively denote modified Bessel and Gamma function.

\paragraph{Gaussian Process Model.}
GP model is a useful Bayesian model for quantifying both prediction and uncertainty of underlying data. It is often leveraged to construct an algorithm in existing GP-bandits works. 
Let us assume the Bayesian assumption of $f$ that $f$ follows mean-zero GP, characterized by the kernel function $k$.
Then, given the input $\bX_t = (\bx_1, \ldots, \bx_t)$ and corresponding outputs, the posterior distribution of $f$ is again 
GP, whose mean and variance function of $f(\bx)$ are 
defined as
\begin{align}
    \mu(\bx; \bX_t) &= \bk(\bx, \mE(\bX_t))^{\top} \bK(\mE(\bX_t), \mE(\bX_t))^{-1} \bm{f}(\mE(\bX_t)), \\
    \sigma^2(\bx; \bX_t) &= k(\bx, \bx) - \bk(\bx, \mE(\bX_t))^{\top} \bK(\mE(\bX_t), \mE(\bX_t))^{-1}\bk(\bx, \mE(\bX_t)), 
\end{align}
where $\bk(\bx, \mE(\bX_t)) \coloneqq [k(\tilde{\bx}, \bx)]_{\tilde{\bx} \in \mE(\bX_t)}$, $\bm{f}(\mE(\bX_t)) = [f(\tilde{\bx})]_{\tilde{\bx} \in \mE(\bX_t)}$, 
and $\bK(\bX_t, \bX_t) = [k(\bx, \tilde{\bx})]_{\bx, \tilde{\bx} \in \mE(\bX_t)}$ are the kernel vector at $\bx$, output vector up to step $t$, and the gram matrix, respectively. In the above definition, we denote $\mE(\bX_t)$ as the subset of $\bX_t$ such that the completely correlated inputs with one of the past inputs are eliminated. Namely, we define $\mE(\bX_t)$ inductively as $\mE(\bX_t) = \mE(\bX_{t-1}) \cup \{\bx_t\}$ if $\sigma^2(\bx_t; \bX_{t-1}) > 0$; otherwise, $\mE(\bX_t) = \mE(\bX_{t-1})$. Here, we define 
$\mE(\bX_1) = \bX_1$. Note that if there are no duplications in the input sequence $\bx_1, \ldots, \bx_t$, then $\mE(\bX_t) = \bX_t$ holds under $k = \sek$ or $k = \matk$.
Furthermore, for the ease of notation, we set $\mu(\bx; \bX) = 0$ and $\sigma^2(\bx; \bX) = k(\bx, \bx)$ for $\bX = \emptyset$.

\paragraph{Maximum Information Gain.}
Let us define the kernel-dependent complexity parameter 
$\gamma_T(\lambda^2)$ as 
\begin{equation}
    \gamma_T(\lambda^2) = \sup_{\bx_1, \ldots, \bx_T \in \mX} \frac{1}{2} \ln \det (\bI_T + \lambda^{-2} \bK(\bX_T, \bX_T)),
\end{equation}
where $\lambda > 0$ and $\bI_T$ are any positive parameter and $T \times T$-identity matrix, respectively.
The quantity $\gamma_T(\lambda^2)$ is called maximum information gain (MIG)~\citep{srinivas10gaussian} since the quantity $\frac{1}{2} \ln \det (\bI_T + \lambda^{-2} \bK(\bX_T, \bX_T))$ represents the mutual information between the underlying function $f$ and training outputs under the noisy-GP model with variance parameter $\lambda^2$. MIG plays an important role in the theoretical analysis of GP-bandits, and their increasing speed is analyzed in several commonly used kernels. For example, $\gamma_T(\lambda^2) = O(\ln^{d+1} (T/\lambda^2))$ and 
$\gamma_T(\lambda^2) = O((T/\lambda^2)^{\frac{d}{2\nu+d}} \ln^{\frac{2\nu}{2\nu+d}} (T/\lambda^2))$ under $k = \sek$ and $k = \matk$ with $\nu > 1/2$, respectively\footnote{These orders hold as $T \rightarrow \infty, \lambda \rightarrow 0$.}~\citep{vakili2021information}.


\paragraph{Gaussian Process Upper Confidence Bound.}
GP-upper confidence bound (GP-UCB) proposed in \citep{srinivas10gaussian} is a well-known GP-bandit algorithm for noisy settings. Regarding the noise-free setting, \citet{lyu2019efficient} propose the noise-free version of GP-UCB, described in \algoref{alg:gp-ucb}.

\begin{algorithm}[t!]
    \caption{Gaussian process upper confidence bound (GP-UCB) for noise-free setting.}
    \label{alg:gp-ucb}
    \begin{algorithmic}[1]
        \REQUIRE Compact input domain $\mX \subset \R^d$, RKHS norm upper bound $B \in (0, \infty)$.
        \STATE $\bX_0 \leftarrow \emptyset$, $\beta^{1/2} \leftarrow B$.
        \FOR {$t = 1, 2, \ldots$}
            \STATE $\bm{x}_{t} \leftarrow \mathrm{arg~max}_{\bm{x} \in \mathcal{X}} \mu(\bx; \bX_{t-1}) + \beta^{1/2} \sigma(\bx; \bX_{t-1})$.
            \STATE Observe $f(\bx_t)$ and update $\bX_t$ and $\bm{f}_t(\bX_t)$.
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

\section{Refined Regret Upper Bound of Noise-Free GP-UCB}
\label{sec:reg_ub_gpucb}

The following theorem describes our main results, which show the nearly optimal regret upper bound for GP-UCB.
\begin{theorem}[Refined cumulative regret upper bound of GP-UCB]
    \label{thm:cr_gp-ucb}
    Fix any input domain $\mX \subset [-L, L]^d$ for some $L > 0$.
    Suppose $B$, $L$, $d$, $\ell$, and $\nu$ are fixed constants.
    Then, when running \algoref{alg:gp-ucb} under \asmpref{asmp:func}, 
    the following two statements hold for any $T \in \N_+$:
    \begin{itemize}
        \item If $k = \sek$, $R_T = O(1)$.
        \item If $k = \matk$ with $\nu > 1/2$,
        \begin{equation}
        R_T = 
            \begin{cases}
                \tilde{O}\rbr{T^{\frac{d - \nu}{d}}} ~~&\mathrm{if}~~d > \nu, \\
                O\rbr{(\ln T)^{2}}  ~~&\mathrm{if}~~d = \nu, \\
                O\rbr{1}  ~~&\mathrm{if}~~d < \nu.
            \end{cases}
        \end{equation}
    \end{itemize}
\end{theorem}

\begin{theorem}[Refined simple regret upper bound of GP-UCB]
    \label{thm:sr_gp-ucb}
    Fix any input domain $\mX \subset [-L, L]^d$ for some $L > 0$.
    Suppose $B$, $L$, $d$, $\ell$, and $\nu$ are fixed constants.
    Then, when running \algoref{alg:gp-ucb} under \asmpref{asmp:func}, 
    the following two statements hold with $\hat{\bx}_T \in \argmax_{\bx \in \{\bx_1, \ldots, \bx_T\}} f(\bx)$:
    \begin{itemize}
        \item If $k = \sek$, $r_T = O\rbr{\sqrt{T} \exp\rbr{-\frac{1}{2} \cset T^{\frac{1}{d+1}}}}$, where $\cset = (6\cse)^{-1/(d+1)}$.
        \item If $k = \matk$ with $\nu > 1/2$, $r_T = O\rbr{T^{-\frac{\nu}{d}} (\ln T)^{\frac{\nu}{d}}}$.
    \end{itemize}
    Here, $\cse > 0$ is the implied constant of the upper bound of MIG for $k = \sek$, which is formally defined in \lemref{lem:pv_ub}.
\end{theorem}

\paragraph{Proof sketch.}
Our key technical results are the new analysis of the cumulative posterior standard deviation $\sum_{t=1}^T \sigma(\bx_t; \bX_{t-1})$ and its minimum $\min_{t \in [T]} \sigma(\bx_t; \bX_{t-1})$, which plays an important role in the theoretical analysis of GP bandits.
Indeed, following the standard analysis of GP-UCB, we have the following upper bounds of regrets by combining the UCB-selection rule with the existing noise-free confidence bound (e.g., Lemma~11 in \citep{lyu2019efficient} or Proposition~1 in \citep{vakili2021optimal}):
\begin{align}
    \label{eq:Rt_ub}
    R_T &= \sum_{t=1}^T f(\bx^{\ast}) - f(\bx_t) \leq 2B \sum_{t=1}^T \sigma(\bx_t; \bX_{t-1}), \\
    \label{eq:rt_ub}
    r_T &= \min_{t \in [T]} f(\bx^{\ast}) - f(\bx_t) \leq 2B \min_{t \in [T]} \sigma(\bx_t; \bX_{t-1}).
\end{align}

From the above inequalities, we observe that the tighter upper bounds of $\sum_{t=1}^T \sigma(\bx_t; \bX_{t-1})$ and $\min_{t \in [T]} \sigma(\bx_t; \bX_{t-1})$ directly result in the tighter regret upper bounds of GP-UCB. The following lemma is our main technical contribution, which gives the new upper bounds of $\sum_{t=1}^T \sigma(\bx_t; \bX_{t-1})$ and $\min_{t \in [T]} \sigma(\bx_t; \bX_{t-1})$.

\begin{lemma}[Posterior standard deviation upper bound for SE and Mat\'ern kernel]
    \label{lem:pv_ub}
    Fix any $L > 0$, input domain $\mX \subset [-L, L]^d$, and kernel function $k: \mX \times \mX \rightarrow \R$ that satisfies $k(\bx, \bx) \leq 1$ for all $\bx \in \mX$. Furthermore, let $\cse$, $\cmat$, $\ulse$, $\ulmat > 0$, $\utse$, $\utmat \geq 2$ be the constants\footnote{The existence of these constants are guaranteed by the upper bound of MIG~\citep{vakili2021information}, which shows $\gamma_T(\lambda^2) = O(\ln^{d+1} (T/\lambda^2))$ and $\gamma_T(\lambda^2) = O((T/\lambda^2)^{\frac{d}{2\nu+d}} \ln^{\frac{2\nu}{2\nu+d}} (T/\lambda^2))$ (as $T \rightarrow \infty, \lambda \rightarrow 0$) under $k = \sek$ and $k = \matk$, respectively. Note that these constants do not depend on $T$, but may depend on $L$, $d$, $\ell$, and $\nu$.} that satisfies $\forall \lambda \in (0, \ulse], \forall t \geq \utse, \gamma_t(\lambda^2) \leq \cse (\ln (t/\lambda^2))^{d+1}$
    and $\forall \lambda \in (0, \ulmat], \forall t \geq \utmat, \gamma_t(\lambda^2) \leq \cmat (t/\lambda^2)^{\frac{d}{2\nu+d}}(\ln (t/\lambda^2))^{\frac{2\nu}{2\nu+d}}$ for $k = \sek$ and $k = \matk$, respectively.
    Then, the following statements hold for any $T \in \N_+$ and any input sequence $\bx_1, \ldots, \bx_T \in \mX$:
    \begin{itemize}
        \item For $k = \sek$,
        \begin{align}
            \label{eq:se_min}
            \min_{t \in [T]} \sigma(\bx_t; \bX_{t-1}) &\leq 
            \begin{cases}
                1 ~~&\mathrm{if}~~ T < \tse, \\
                \sqrt{T} \exp\rbr{-\frac{1}{2}\cset T^{\frac{1}{d+1}}} ~~&\mathrm{if}~~ T\geq \tse,
            \end{cases} \\
            \label{eq:se_cum}
             \sum_{t=1}^T \sigma(\bx_t; \bX_{t-1}) &\leq \tse + (d+1) \rbr{\frac{\cset}{2}}^{-\frac{3d+3}{2}} \Gamma\rbr{\frac{3d + 3}{2}},
        \end{align}
        where $\cset = (6\cse)^{-\frac{1}{d+1}}$ and $\tse = \max\{\utse, \utlse, \lceil (d+1)^{d+1}/\cset^{d+1} \rceil + 1\}$ with $\utlse = \min\{T \in \N_+ \mid \forall t \geq T, t\exp(-\cset t^{1/(d+1)}) \leq \ulse^2\}$.
        \item For $k = \matk$ with $\nu > 1/2$,
        \begin{align}
            \label{eq:mat_min}
            \min_{t \in [T]} \sigma(\bx_t; \bX_{t-1}) &\leq \begin{cases}
                1 ~~&\mathrm{if}~~ T < \tmat, \\
                \cmatt^{1/2} T^{-\frac{\nu}{d}} (\ln T)^{\frac{\nu}{d}}  ~~&\mathrm{if}~~ T\geq \tmat,
            \end{cases} \\
            \label{eq:mat_cum}
            \sum_{t=1}^T \sigma(\bx_t; \bX_{t-1}) &\leq \begin{cases}
                \tmat  + \cmatt^{1/2} \frac{d}{d-\nu} T^{\frac{d-\nu}{d}} (\ln T)^{\frac{\nu}{d}} ~~&\mathrm{if}~~d > \nu, \\
                \tmat + \cmatt^{1/2} (\ln T)^{2} ~~&\mathrm{if}~~d = \nu, \\
                \tmat + \cmatt^{1/2} \frac{\Gamma(\frac{\nu}{d} + 1)}{\rbr{\frac{\nu}{d}-1}^{\frac{\nu}{d}+1}} ~~&\mathrm{if}~~d < \nu,
            \end{cases}
        \end{align}
        where $\cmatt = \max\cbr{1, \rbr{2 + \frac{2\nu}{d}}^{\frac{2\nu}{d}} (6\cmat)^{1+\frac{2\nu}{d}}}$ and $\tmat = \max\{4, \utmat, \utlmat\}$ with $\utlmat = \min\{T \in \N_+ \mid \forall t \geq T, \cmatt t^{-\frac{2\nu}{d}} (\ln t)^{\frac{2\nu}{d}} \leq \ulmat^2\}$.
    \end{itemize}
\end{lemma}

Under the fixed $L$, $d$, $\ell$, and $\nu$, the above lemma claims
\begin{align}
    \label{eq:se_psu}
    &\min_{t \in [T]} \sigma(\bx_t; \bX_{t-1}) = O\rbr{\sqrt{T} \exp\rbr{-\frac{1}{2}\cset T^{\frac{1}{d+1}}}},~\sum_{t=1}^T \sigma(\bx_t; \bX_{t-1}) = O(1), ~\mathrm{and} \\
    \label{eq:mat_psu}
    &\min_{t \in [T]} \sigma(\bx_t; \bX_{t-1}) = O\rbr{T^{-\frac{\nu}{d}} (\ln T)^{\frac{\nu}{d}}},~\sum_{t=1}^T \sigma(\bx_t; \bX_{t-1}) = \begin{cases}
        O\rbr{T^{\frac{d-\nu}{d}} (\ln T)^{\frac{\nu}{d}}} ~~&\mathrm{if}~~d > \nu, \\
        O\rbr{(\ln T)^2}  ~~&\mathrm{if}~~d = \nu, \\
        O(1)  ~~&\mathrm{if}~~d < \nu,
    \end{cases}
\end{align}
for $k= \sek$ and $k = \matk$, respectively.
Combining the above equations with Eqs.~\eqref{eq:Rt_ub} and \eqref{eq:rt_ub}, we obtain the desired results. 

\begin{remark}[Generality of \lemref{lem:pv_ub}]
    \label{rem:gen}
    We would like to highlight that \lemref{lem:pv_ub} always holds in any input sequence, in contrast to the existing algorithm-specific upper bounds~\citep{salgiarandom,iwazaki2025improvedregretanalysisgaussian}. Since the existing noisy GP bandits theory often leverage the upper bound of $\min_{t \in [T]} \sigma(\bx_t; \bX_{t-1})$ or $\sum_{t=1}^T \sigma(\bx_t; \bX_{t-1})$ from \citep{srinivas10gaussian}, we expect that the various existing theory of the noisy setting algorithms can be extended to the corresponding noise-free setting by directly replacing the existing noisy upper bounds of \citep{srinivas10gaussian} with \lemref{lem:pv_ub}. For example, the analysis for GP-TS~\citep{chowdhury2017kernelized}, GP-UCB and GP-TS under Bayesian setting~\citep{srinivas10gaussian,Russo2014-learning}, contextual setting~\citep{krause2011contextual}, GP-based level-set estimation~\citep{gotovos2013active}, multi-objective setting~\citep{JMLR:v17:15-047}, robust formulation~\citep{bogunovic2018adversarially}, and so on.
\end{remark}

\subsection{Proof Sketch of \lemref{lem:pv_ub}}
In this section, we describe the proof sketch of \lemref{lem:pv_ub}, while we give its full proof in Appendix~\ref{subsec:proof_pv_ub}. 
We consider the following more general lemmas from which \lemref{lem:pv_ub} follows.

\begin{lemma}[General upper bound for the minimum posterior standard deviation]
    \label{lem:mpv_ub}
    Fix any input domain $\mX$ and any $\overline{T} \geq 2$.
    Let $(\lambda_t)_{t \geq \overline{T}}$ be a strictly positive sequence such that $\gamma_t(\lambda_t^2) \leq (t-1)/3$ for all $t \geq \overline{T}$.
    Then, $\min_{t \in [T]} \sigma(\bx_t; \bX_{t-1}) \leq \lambda_T$ holds for any $T \geq \overline{T}$ and any sequence $\bx_1, \ldots, \bx_T \in \mX$. 
\end{lemma}

\begin{lemma}[General upper bound for the cumulative posterior standard deviations]
    \label{lem:cpv_ub}
    Fix any input domain $\mX$, any $\overline{T} \geq 2$, and any kernel function $k: \mX \times \mX \rightarrow \R$ that satisfies $k(\bx, \bx) \leq 1$ for all $\bx \in \mX$.
    Let $(\lambda_t)_{t \geq \overline{T}}$ be a strictly positive sequence such that $\gamma_t(\lambda_t^2) \leq (t-1)/3$ for all $t \geq \overline{T}$.
    Then, the following inequality holds for any $T \in \N_+$ and any sequence $\bx_1, \ldots, \bx_T \in \mX$:
    \begin{equation}
        \label{eq:gen_cpv_ub}
        \sum_{t=1}^T \sigma(\bx_t; \bX_{t-1}) \leq \overline{T} - 1 + \sum_{t=\overline{T}}^T \lambda_t.
    \end{equation}
\end{lemma}
With some elemental calculations, we can confirm that the condition $\forall t \geq \overline{T}, \gamma_t(\lambda_t^2) \leq (t-1)/3$ of the above lemmas holds with $\lambda_t^2 = t \exp(-\cset t^{\frac{1}{d+1}}), \overline{T} = \tse$ and $\lambda_t^2 = \cmatt t^{-\frac{2\nu}{d}} (\ln t)^{\frac{2\nu}{d}}, \overline{T} = \tmat$ for $k=\sek $ and $k = \matk$, respectively. See \appref{subsec:proof_pv_ub} for details.
\lemref{lem:pv_ub} follows from the aforementioned setting of $\lambda_t^2$, $\overline{T}$, and Lemmas~\ref{lem:mpv_ub} and \ref{lem:cpv_ub}.

\paragraph{Proof of \lemref{lem:mpv_ub}.}
Instead of directly treating the noiseless posterior standard deviation, we study its upper bound with the posterior standard deviation of some noisy GP model. Here, let us denote $\sigma_{\lambda^2}^2(\bx; \bX_{t-1})$ as the posterior variance under the noisy GP-model with the strictly positive variance parameter $\lambda^2 > 0$, which is defined as
\begin{equation}
    \sigma_{\lambda^2}^2(\bx; \bX_{t-1}) = k(\bx, \bx) - \bk(\bx, \bX_{t-1})^{\top} [\bK(\bX_{t-1}, \bX_{t-1}) + \lambda^2 \bI_{t-1}]^{-1}\bk(\bx, \bX_{t-1}).
\end{equation}
Since the posterior variance is monotonic for the variance parameter, 
we have $\sigma^2(\bx_t; \bX_{t-1}) \leq \sigma_{\lambda_T^2}^2(\bx_t; \bX_{t-1})$ for all $t \in [T]$.
Next, we obtain the upper bound of $\sigma_{\lambda_T^2}^2(\bx; \bX_{t-1})$ based on the following lemma, which is the main component of the proof of Lemmas~\ref{lem:mpv_ub} and \ref{lem:cpv_ub}.
\begin{lemma}[Elliptical potential count lemma, Lemma~D.9 in \citep{flynn2024tighter} or Lemma 3.3 in \citep{iwazaki2025improvedregretanalysisgaussian}]
    \label{lem:epcl}
    Fix any $T \in N_+$, any sequence $\bx_1, \ldots, \bx_T \in \mX$, and $\lambda > 0$.
    Define $\mT$ as $\mT = \{t \in [T] \mid \lambda^{-1} \sigma_{\lambda^2}(\bx_{t}; \bX_{t-1}) > 1\}$, where $\bX_{t-1} = (\bx_1, \ldots, \bx_{t-1})$.
    Then, the number of elements of $\mT$ satisfies $|\mT| \leq 3\gamma_T(\lambda^2)$.
\end{lemma}
The above lemma implies that the set $\mT^c \coloneqq \{t \in [T] \mid \sigma_{\lambda_T^2}(\bx_{t}; \bX_{t-1}) \leq \lambda_T\}$ 
satisfies $|\mT^c| = |[T]\setminus \mT| \geq T - 3\gamma_T(\lambda_T^2)$. Therefore, for any $T \geq \overline{T}$, $|\mT^c| \geq 1$ holds from the condition $\gamma_T(\lambda_T^2) \leq (T-1)/3$. 
This implies there exists some $\tilde{t} \in [T]$ such that $\sigma(\bx_{\tilde{t}}; \bX_{\tilde{t}-1}) \leq \sigma_{\lambda_T^2}(\bx_{\tilde{t}}; \bX_{\tilde{t}-1}) \leq \lambda_T$; therefore, $\min_{t\in [T]} \sigma(\bx_t; \bX_{t-1}) \leq \sigma(\bx_{\tilde{t}}; \bX_{\tilde{t}-1}) \leq \lambda_T$ holds for all $T \geq \overline{T}$. \qed

\paragraph{Proof of \lemref{lem:cpv_ub}.}
If $T < \overline{T}$, Eq.~\eqref{eq:gen_cpv_ub} is clearly holds from the assumption $\forall \bx \in \mX, k(\bx, \bx) \leq 1$. Hereafter, we focus on $T \geq \overline{T}$.
First, by following the same argument of \lemref{lem:mpv_ub}, we can confirm that there exists the index $\tilde{t}_T \leq T$ such that $\sigma(\bx_{\tilde{t}_T}; \bX_{\tilde{t}_T-1}) \leq \sigma_{\lambda_T^2}(\bx_{\tilde{t}_T}; \bX_{\tilde{t}_T-1}) \leq \lambda_T$. Here, we define the sequence $(\bx_t^{(T-1)})_{t \in [T-1]}$ as the sequence that $\bx_{\tilde{t}}$ is eliminated from $(\bx_t)_{t \in [T]}$; namely, we set $\bx_t^{(T-1)} = \1\{t < \tilde{t}_T\} \bx_t + \1\{t \geq \tilde{t}_T\} \bx_{t+1}$ for any $t \in [T-1]$. Furthermore, we define $\bX_t^{(T-1)} = (\bx_1^{(T-1)}, \ldots, \bx_t^{(T-1)})$. Under these notations, we have
\begin{equation}
    \sum_{t=1}^T \sigma(\bx_t; \bX_{t-1}) \leq \sum_{t \in [T] \setminus \{\tilde{t}_T\}} \sigma(\bx_t; \bX_{t-1}) + \lambda_T \leq \sum_{t \in [T-1]} \sigma\rbr{\bx_t^{(T-1)}; \bX_{t-1}^{(T-1)}} + \lambda_T.
\end{equation}
Then, we observe that there exists the index $\tilde{t}_{T-1} \leq T-1$ such that $\sigma(\bx_{\tilde{t}_{T-1}}^{(T-1)}; \bX_{\tilde{t}_{T-1}-1}^{(T-1)}) \leq \sigma_{\lambda_{T-1}^2}(\bx_{\tilde{t}_{T-1}}^{(T-1)}; \bX_{\tilde{t}_{T-1}-1}^{(T-1)}) \leq \lambda_{T-1}$ by the application of \lemref{lem:epcl} for the new sequence $(\bx_t^{(T-1)})$. Again, by setting $\bx_t^{(T-2)} = \1\{t < \tilde{t}_{T-1}\} \bx_t^{(T-1)} + \1\{t \geq \tilde{t}_{T-1}\} \bx_{t+1}^{(T-1)}$ and $\bX_t^{(T-2)} = (\bx_1^{(T-2)}, \ldots, \bx_t^{(T-2)})$ for any $t \in [T-2]$, we have
\begin{equation}
    \sum_{t \in [T-1]} \sigma\rbr{\bx_t^{(T-1)}; \bX_{t-1}^{(T-1)}} \leq 
    \sum_{t \in [T-1]\setminus \{\tilde{t}_{T-1}\}} \sigma\rbr{\bx_t^{(T-1)}; \bX_{t-1}^{(T-1)}} + \lambda_{T-1} \leq 
    \sum_{t \in [T-2]} \sigma\rbr{\bx_t^{(T-2)}; \bX_{t-1}^{(T-2)}} + \lambda_{T-1}.
\end{equation}
We can repeat the above arguments until we reach $\overline{T}-1$. Then, the resulting upper bound becomes
\begin{equation}
    \sum_{t=1}^T \sigma(\bx_t; \bX_{t-1}) \leq \sum_{t=1}^{\overline{T}-1} \sigma\rbr{\bx_t^{(\overline{T}-1)}; \bX_{t-1}^{(\overline{T}-1)}} + \sum_{t=\overline{T}}^T \lambda_t \leq \overline{T}-1 + \sum_{t=\overline{T}}^T \lambda_t.
\end{equation} \qed

\section{Lower Bound for Cumulative Regret under Noise-Free Setting}
\label{sec:cr_lb}

Our analysis shows the GP-UCB achieves nearly optimal cumulative regret for $k = \sek$ and $k = \matk$ with $d \leq \nu$ since the constant $O(1)$ cumulative regret is unavoidable. The remaining question is whether GP-UCB is nearly optimal or not under $d > \nu$. Our following result gives the affirmative answer by showing the lower bound for the deterministic regret. 
\begin{theorem}[Lower bound for deterministic regret under $k = \matk$]
    \label{thm:cr_lb}
    Suppose $\mX = [0, 1]^d$, $k = \matk$, and $d$, $\ell$, and $\nu$ are fixed constants with $d > \nu$. Then, for any algorithm (including both deterministic and stochastic algorithms) and $T \geq 2$, there exists a function $f$ and an event\footnote{We can take the event whose probability of occurrence is at least $1/T$ when the algorithm is stochastic.} such that $\|f\|_{k} \leq B$ and $R_T \geq C B T^{\frac{d - \nu}{d}}$.
    Here, $C > 0$ is the constant that may depend on $d$, $\ell$, and $\nu$.
\end{theorem}

We would like to emphasize that the above result only shows the lower bound for the deterministic regret; namely, there exists the possibility for achieving smaller expected regret $\Ep[R_T]$ than $\Omega(BT^{(d-\nu)/d})$ by the randomness of the algorithm. We leave the analysis for the expected regret lower bound for future research; however, note that our deterministic regret lower bound is enough to guarantee the near optimality of our analysis of GP-UCB. 

\paragraph{Proof sketch.}
As with the proof of noisy lower bound~\citep{scarlett2017lower,cai2021on}, we consider the finite collection of bump functions, whose unique maximizers are different and the function values are $0$ on the neighborhood of the other function maximizers. For any small $\epsilon > 0$, \citet{cai2021on} shows that we can construct $M = \Theta\rbr{(B/\epsilon)^{d/\nu}}$ distinct functions whose maximum and RKHS norm upper bounds are $\Theta(\epsilon)$ and $B$, respectively. Here, let us fix any input sequence $\bx_1, \ldots, \bx_{M-1} \in \mX$ generated by the algorithm when the underlying function is $f = 0$. Then, the same input sequence is also generated by the algorithm when the underlying function is the bump function, whose function values are exactly zero at $\bx_1, \ldots, \bx_{M-1}$. Furthermore, under such bump function, the algorithm suffers from at least $\Omega((M-1)\epsilon) = \Omega((T-1)\epsilon) = \Omega(T\epsilon)$ regret when $\bx_1, \ldots, \bx_{M-1}$ is generated and $M = T$. Since the condition $M = T$ implies $\epsilon = \Theta(BT^{-\frac{\nu}{d}})$, we obtain the desired lower bound $\Omega(BT^{(d-\nu)/d}) = \Omega(T\epsilon)$.

\section{Conclusion}
This paper shows that the GP-UCB achieves nearly optimal regret by proving the new regret upper and lower bound for noise-free GP bandits. The key theoretical component of our analysis is the tight upper bound of the posterior standard deviations of GP tailored to a noise-free setting (\lemref{lem:pv_ub}). As remarked in \secref{sec:reg_ub_gpucb}, \lemref{lem:pv_ub} can be applicable beyond the analysis of GP-UCB. Specifically, we expect that the various existing theoretical results for noisy GP bandits settings can translate into its noise-free setting by replacing the existing noisy upper bound of the posterior standard deviations with \lemref{lem:pv_ub}.
For this reason, we believe that our result marks an important step for the future development of noise-free GP bandit algorithms. 

\bibliographystyle{plainnat}
\bibliography{main}

\newpage
\appendix

\onecolumn
\section{Proofs for \secref{sec:reg_ub_gpucb}}
\label{sec:proof_reg_ub_gpucb}
\subsection{Proof of Theorems~\ref{thm:cr_gp-ucb} and \ref{thm:sr_gp-ucb}}

We first formally describe the existing noise-free confidence bound.
\begin{lemma}[Deterministic confidence bound for noise-free setting, Lemma~11 in \citep{lyu2019efficient} or Proposition~1 in \citep{vakili2021optimal}]
    \label{lem:detrm_cb}
    Suppose Assumption~\ref{asmp:func} holds.
    Then, for any sequence $(\bx_t)_{t \in \N_+}$ on $\mX$, the following statement holds:
    \begin{equation}
        \forall t \in \N_+,~\forall \bx \in \mX,~|f(\bx) - \mu(\bx; \bX_t)| \leq B \sigma(\bx; \bX_t),
    \end{equation}
    where $\bX_t = (\bx_1, \ldots, \bx_t)$.
\end{lemma}

Although the remaining parts of the proofs are the well-known results of GP-UCB, we give the details for completeness.
Based on the above lemma, we show Eqs.~\eqref{eq:Rt_ub} and \eqref{eq:rt_ub}.
Regarding $R_T$, we have
\begin{align}
    R_T &= \sum_{t=1}^T f(\bx^{\ast}) - f(\bx_t) \\
    &\leq \sum_{t=1}^T [\mu(\bx^{\ast}; \bX_t) + B \sigma(\bx^{\ast}; \bX_t)] - [\mu(\bx_t; \bX_t) - B \sigma(\bx_t; \bX_t)] \\
    &\leq \sum_{t=1}^T [\mu(\bx_t; \bX_t) + B \sigma(\bx_t; \bX_t)] - [\mu(\bx_t; \bX_t) - B \sigma(\bx_t; \bX_t)] \\
    &= 2B \sum_{t=1}^T \sigma(\bx_t; \bX_t),
\end{align}
where the first inequality follows from \lemref{lem:detrm_cb}, 
and the second inequality follows from the UCB-selection rule of $\bx_t$. Similarly to the cumulative regret, we have
\begin{align}
    r_T &= f(\bx^{\ast}) - f(\hat{\bx}_T) \\
    &\leq \min_{t \in [T]} f(\bx^{\ast}) - f(\bx_t)  \\
    &\leq \min_{t \in [T]} [\mu(\bx^{\ast}; \bX_t) + B \sigma(\bx^{\ast}; \bX_t)] - [\mu(\bx_t; \bX_t) - B \sigma(\bx_t; \bX_t)]  \\
    &\leq \min_{t \in [T]} [\mu(\bx_t; \bX_t) + B \sigma(\bx_t; \bX_t)] - [\mu(\bx_t; \bX_t) - B \sigma(\bx_t; \bX_t)]  \\
    &= 2B \min_{t \in [T]} \sigma(\bx_t; \bX_t),
\end{align}
where the first inequality follows from the definition 
of $\hat{\bx}_T$. Finally, the desired results are obtained by combining the above inequalities with Eqs.~\eqref{eq:se_psu} and \eqref{eq:mat_psu}. \qed

\subsection{Proof of \lemref{lem:pv_ub}}
\label{subsec:proof_pv_ub}
    When $k = \sek$, we set $\lambda_t^2 = t \exp(-\cset t^{\frac{1}{d+1}})$, $\overline{T} = \tse \coloneqq \max\{\utse, \utlse, \lceil (d+1)^{d+1}/\cset^{d+1} \rceil + 1\}$. 
    From the definition of $\lambda_t^2$ and $\tse$, for any $t \geq \tse$, we have
    \begin{align}
        \gamma_t(\lambda_t^2) 
        &\leq \cse \sbr{\ln\rbr{\frac{t}{\lambda_t^2}}}^{d+1} \\
        &= \cse \sbr{\ln \exp\rbr{\cset t^{\frac{1}{d+1}}}}^{d+1} \\
        &= \cse \cset^{d+1} t.
    \end{align}
    Furthermore,
    \begin{align}
        \cse \cset^{d+1} t \leq \frac{t-1}{3} 
        &\Leftrightarrow \cset^{d+1} \leq \frac{t-1}{3\cse t} \\
        &\Leftarrow \cset^{d+1} \leq \frac{1}{6 \cse} \\
        &\Leftrightarrow \cset \leq \rbr{\frac{1}{6 \cse}}^{d+1},
    \end{align}
    where the second line follows from the inequality $t - 1 \geq t/2$ for all $t \geq \tse \geq 2$. By noting the definition of $\cset$, we conclude that 
    $\forall t \geq \tse, \gamma_t(\lambda_t^2) \leq \cse \cset^{d+1} t \leq \frac{t-1}{3}$ from the above inequalities, which implies Lemmas~\ref{lem:mpv_ub} and \ref{lem:cpv_ub} holds with $\lambda_t^2 = t \exp(-\cset t^{\frac{1}{d+1}})$ and $\overline{T} = \tse$. Eq.~\eqref{eq:se_min} directly follows from Lemma~\ref{lem:mpv_ub} with the fact $\sigma(\bx_t; \bX_{t-1}) \leq k(\bx_t, \bx_t) \leq 1$. As for 
    Eq.~\eqref{eq:se_cum}, Lemma~\ref{lem:cpv_ub} implies
    \begin{align}
        \sum_{t=1}^T \sigma(\bx_t; \bX_{t-1}) 
        &\leq \tse + \sum_{t=\tse}^{T} \lambda_t \\
        &\leq \tse + \int_{\tse-1}^T \sqrt{t} \exp\rbr{-\frac{1}{2}\cset t^{\frac{1}{d+1}}} \text{d}t\\
        &\leq \tse + \int_{1}^T \sqrt{t} \exp\rbr{-\frac{1}{2}\cset t^{\frac{1}{d+1}}} \text{d}t,
    \end{align}
    where the second line follows from the fact that the function $g(t) \coloneqq t\exp(-\cset t^{1/(d+1)})$ is non-increasing for $t \geq \tse - 1$. Indeed, we have
    \begin{align}
        g'(t) = \exp\rbr{-\cset t^{\frac{1}{d+1}}} \rbr{1 - \frac{\cset}{d+1} t^{\frac{1}{d+1}}},
    \end{align}
    which implies $g'(t) \leq 0$ for $t \geq \tse - 1 \geq (d+1)^{d+1}/\cset^{d+1}$. Regarding the quantity $\int_{1}^T \sqrt{t} \exp\rbr{-\frac{1}{2}\cset t^{\frac{1}{d+1}}} \text{d}t$, we further obtain the following upper bound with $C \coloneqq \cset/2 > 0$:
    \begin{align}
        \int_{1}^T \sqrt{t} \exp\rbr{-C t^{\frac{1}{d+1}}} \text{d}t 
        &= \int_{C}^{CT^{1/(d+1)}} \rbr{\frac{u}{C}}^{(d+1)/2} e^{-u} (d+1) \rbr{\frac{u}{C}}^d \frac{1}{C} \text{d}u~~ (\because u = Ct^{1/(d+1)}) \\
        &= (d+1) C^{-(3d+3)/2}\int_{C}^{CT^{1/(d+1)}} u^{(3d+1)/2} e^{-u} \text{d}u \\
        &\leq (d+1) C^{-(3d+3)/2} \int_{0}^{\infty} u^{(3d+1)/2} e^{-u} \text{d}u \\
        &= (d+1) C^{-(3d+3)/2} \Gamma\rbr{\frac{3d+3}{2}}.
    \end{align}

    Next, when $k = \matk$, we set $\lambda_t^2 = \cmatt t^{-\frac{2\nu}{d}} (\ln t)^{\frac{2\nu}{d}}$ and $\overline{T} = \max\{4, \utmat, \utlmat\}$ with $\cmatt = \rbr{2 + \frac{2\nu}{d}}^{\frac{2\nu}{d}} (6\cmat)^{1+\frac{2\nu}{d}}$. Then, for any $t \geq \tmat$, we have
    \begin{align}
        \gamma_t(\lambda_t^2) 
        &\leq \cmat \rbr{\frac{t}{\lambda_t^2}}^{\frac{d}{2\nu+d}} \sbr{\ln\rbr{\frac{t}{\lambda_t^2}}}^{\frac{2\nu}{2\nu+d}} \\
        &= \cmat \cmatt^{-\frac{d}{2\nu+d}} t (\ln t)^{-\frac{2\nu}{2\nu+d}}  \sbr{\ln \rbr{\cmatt^{-1} t^{\frac{d+2\nu}{d}} (\ln t)^{-\frac{2\nu}{d}} }}^{\frac{2\nu}{2\nu+d}} \\
        &= \cmat \cmatt^{-\frac{d}{2\nu+d}} t (\ln t)^{-\frac{2\nu}{2\nu+d}}  \sbr{\ln \rbr{\cmatt^{-1}} + \frac{d + 2\nu}{d} (\ln t) -\frac{2\nu}{d} (\ln \ln t) }^{\frac{2\nu}{2\nu+d}} \\
        &\leq \cmat \cmatt^{-\frac{d}{2\nu+d}} t (\ln t)^{-\frac{2\nu}{2\nu+d}}  \sbr{\frac{2d + 2\nu}{d} (\ln t)}^{\frac{2\nu}{2\nu+d}} \\
        &= \cmat \cmatt^{-\frac{d}{2\nu+d}} t \rbr{\frac{2d + 2\nu}{d}}^{\frac{2\nu}{2\nu+d}}, 
    \end{align}
    where the fourth line follows from $\cmatt \geq 1 \Rightarrow \cmatt \geq 1/t \Leftrightarrow \ln (\cmatt^{-1}) \leq \ln t$ for $t \geq 1$.
    Furthermore, 
    \begin{align}
        \cmat \cmatt^{-\frac{d}{2\nu+d}} t \rbr{\frac{2d + 2\nu}{d}}^{\frac{2\nu}{2\nu+d}} \leq \frac{t-1}{3} 
        &\Leftrightarrow 3 \cmat \frac{t}{t-1} \rbr{\frac{2d + 2\nu}{d}}^{\frac{2\nu}{2\nu+d}} \leq \cmatt^{\frac{d}{2\nu+d}} \\
        &\Leftrightarrow \rbr{\frac{3 \cmat t}{t-1}}^{1 + \frac{2\nu}{d}} \rbr{2 + \frac{2\nu}{d}}^{\frac{2\nu}{d}} \leq \cmatt \\
        &\Leftarrow \rbr{6 \cmat}^{1 + \frac{2\nu}{d}} \rbr{2 + \frac{2\nu}{d}}^{\frac{2\nu}{d}} \leq \cmatt.
    \end{align}
    Combining the above inequalities, we can confirm $\forall t \geq \tmat, \gamma_t(\lambda_t^2) \leq \frac{t-1}{3}$. 
    Therefore, Lemmas~\ref{lem:mpv_ub} and \ref{lem:cpv_ub} holds with $\lambda_t^2 = \cmatt t^{-\frac{2\nu}{d}} (\ln t)^{\frac{2\nu}{d}}$ and $\overline{T} = \tmat$. Here, Eq.~\eqref{eq:mat_min} is the direct consequence of Lemmas~\ref{lem:mpv_ub}. As for Eq.~\eqref{eq:mat_cum}, we have
    \begin{align}
        \sum_{t=1}^T \sigma(\bx_t; \bX_{t-1}) 
        &\leq \tmat -1 + \sum_{t=\tmat}^{T} \lambda_t \\
        &\leq \tmat + \cmatt^{1/2} \int_{\tmat - 1}^{T} t^{-\frac{\nu}{d}} (\ln t)^{\frac{\nu}{d}} \text{d}t \\
        &\leq \tmat + \cmatt^{1/2} \int_{1}^{T} t^{-\frac{\nu}{d}} (\ln t)^{\frac{\nu}{d}} \text{d}t,
    \end{align}
    where the second line follows from the fact that the function $g(t) \coloneqq t^{-\frac{2\nu}{d}} (\ln t)^{\frac{2\nu}{d}}$ is non-increasing for $t \geq \tmat - 1 \geq 3 > e$. Indeed, we have
    \begin{align}
        g'(t) = \frac{2\nu}{d} t^{-\frac{2\nu}{d}-1} (\ln t)^{\frac{2\nu}{d}}\rbr{(\ln t)^{-1} - 1},
    \end{align}
    which implies $g'(t) \leq 0$ for $t \geq e$.
    The desired results are obtained by bounding the quantity $\int_{1}^{T} t^{-\frac{\nu}{d}} (\ln t)^{\frac{\nu}{d}} \text{d}t$ from above.
    When $d > \nu$, we have
    \begin{equation}
        \int_{1}^{T} t^{-\frac{\nu}{d}} (\ln t)^{\frac{\nu}{d}} \text{d}t
        \leq (\ln T)^{\frac{\nu}{d}} \int_1^T t^{-\frac{\nu}{d}} \text{d}t = (\ln T)^{\frac{\nu}{d}} \sbr{\frac{d}{d-\nu} t^{\frac{d-\nu}{d}}}_1^T \leq \frac{d}{d-\nu} T^{\frac{d-\nu}{d}} (\ln T)^{\frac{\nu}{d}}.
    \end{equation}
    When $d = \nu$,
    \begin{equation}
        \int_{1}^{T} t^{-\frac{\nu}{d}} (\ln t)^{\frac{\nu}{d}} \text{d}t \leq (\ln T) \int_1^T t^{-1} \text{d}t = (\ln T)^2.
    \end{equation}
    When $d < \nu$, we have
    \begin{align}
        \int_{1}^T t^{-\frac{\nu}{d}} (\ln t)^{\frac{\nu}{d}} \text{d}t
        &= \int_{0}^{\ln T} e^{-\rbr{\frac{\nu}{d}-1}u} u^{\frac{\nu}{d}} \text{d}u ~~~(\because u = \ln t) \\
        &\leq \int_{0}^{\infty} e^{-\rbr{\frac{\nu}{d}-1}u} u^{\frac{\nu}{d}} \text{d}u \\
        &= \frac{\Gamma(\frac{\nu}{d} + 1)}{\rbr{\frac{\nu}{d}-1}^{\frac{\nu}{d}+1}},
    \end{align}
    where the last line follows from the standard property of Gamma function: $\int_{0}^{\infty} e^{-\lambda u} u^b \text{d}u = \Gamma(b+1)/\lambda^{b+1}$ for any $\lambda > 0$ and $b > -1$ (e.g., Equation 6.1.1 in \citep{abramowitz1968handbook}). \qed

\section{Proof of \thmref{thm:cr_lb}}
\label{sec:proof_cr_lb}
We leverage the following lemma to define the base function, which is used to construct a finite collection of “hard” functions.

\begin{lemma}[Lemma~4 in \citep{cai2021on}]
    \label{lem:bump}
    Fix any $k = \matk$, $\epsilon > 0$, and $\omega > 0$.
    Let $h(\bx) \coloneqq \exp\rbr{-1/(1 - \|\bx\|_2^2)} \1 \{\|\bx\|_2 < 1\}$ be the $d$-dimensional bump function, and define $g(\bx) = \frac{2\epsilon}{h(\bm{0})}h(\bx/\omega)$. Then, the function $g$ satisfies the following properties:
    \begin{enumerate}
        \item $g(\bx) = 0$ for all $\bx$ outside the $\ell_2$-ball of radius $\omega$ centered at the origin.
        \item $g(\bx) \in [0, 2\epsilon]$ for all $\bx$ and $g(\bm{0}) = 2\epsilon$.
        \item $\|g\|_{k} \leq \frac{2c_1 \epsilon}{h(0)} (1/\omega)^{\nu} \|h\|_{k}$, where $c_1$ is some finite constant. In particular, we have $\|g\|_{k}\leq B$ when $\omega = (2c_1\epsilon \|h\|_{k}/(h(\bm{0})B))^{1/\nu}$.
    \end{enumerate}
\end{lemma}
Note that $k = \matk$ is stationary, any shifted function $f(\bx) = g(\bx - \ba)$ maintain the RKHS norm~(e.g., Lemma~1 in \citep{scarlett2017lower}).
Therefore, given some $\epsilon > 0$, by shifting the above function $g$ with $\omega \coloneqq \omega_{\epsilon} = (2c_1\epsilon \|h\|_{k}/(h(\bm{0})B))^{1/\nu}$, we can construct the function class $\mF_{\epsilon} = \{f_1^{(\epsilon)}, \ldots, f_{M_{\epsilon}}^{(\epsilon)}\}$ 
such that $\|f_i^{(\epsilon)}\|_{k} \leq B$ and each function in $\mF_{\epsilon}$ has unique non-zero support. The maximum size $M_{\epsilon} > 0$ of $\mF_{\epsilon}$ for such construction is obtained by the $\omega_{\epsilon}$-packing number of $\mX \coloneqq [0, 1]^d$. As with the proof of existing lower bound~\citep{scarlett2017lower,cai2021on}, we set $M_{\epsilon}$ as
\begin{equation}
    M_{\epsilon} = \left\lfloor \frac{1}{\omega_{\epsilon}} \right \rfloor^d = \left\lfloor \rbr{\frac{h(\bm{0}) B}{6\epsilon c_1 \|h\|_{k}}}^{1/\nu} \right \rfloor^{d},
\end{equation}
which is clearly less than $\omega_{\epsilon}$-packing number of $\mX \coloneqq [0, 1]^d$. Furthermore, since each function in $\mF_{\epsilon}$ has unique non-zero support, we can divide $\mX$ into the $M_{\epsilon}$ disjoint regions $(\mR_i^{(\epsilon)})_{i \in [M_{\epsilon}]}$ based on $\mF_{\epsilon}$ such that $\mX = \bigsqcup_{i \in [M_{\epsilon}]} \mR_i^{(\epsilon)}$ and $\{\bx \in \mX \mid f_i^{(\epsilon)}(\bx) > 0\} \subset \mR_i^{(\epsilon)}$.
Here, we fix $\epsilon$ as $\epsilon = \rbr{\frac{h(\bm{0})}{6c_1 \|h\|_{k}}} B T^{-\frac{\nu}{d}}$, which implies $M_{\epsilon} = T$. 
Then, for a given algorithm $\pi$, we pick the region $\mR_j^{(\epsilon)}$ such that the algorithm query sequence $\bx_1, \ldots, \bx_{M_{\epsilon}-1}$ under $f \coloneqq f_0 = 0$ satisfies 
\begin{equation}
    \Pr_{f_0, \pi}\rbr{\bx_1, \ldots, \bx_{M_{\epsilon}-1} \notin \mR_j^{(\epsilon)}} = \max_{i \in [M_{\epsilon}]}\Pr_{f_0, \pi}\rbr{\bx_1, \ldots, \bx_{M_{\epsilon}-1} \notin \mR_i^{(\epsilon)}}.
\end{equation}
Note that such $\mR_j^{(\epsilon)}$ also satisfies $\Pr_{f_0, \pi}\rbr{\bx_1, \ldots, \bx_{M_{\epsilon}-1} \notin \mR_j^{(\epsilon)}} \geq 1/M_{\epsilon}$, since any realized sequence $\bx_1, \ldots, \bx_{M_{\epsilon}-1}$ has at least one region in which it is not contained due to $M_{\epsilon} = T$; namely, $\sum_{i=1}^{M_{\epsilon}}\Pr_{f_0, \pi}\rbr{\bx_1, \ldots, \bx_{M_{\epsilon}-1} \notin \mR_i^{(\epsilon)}} = \sum_{i=1}^{M_{\epsilon}}\Pr_{f_0, \pi}\rbr{\bx_1, \ldots, \bx_{M_{\epsilon}-1} \in \mX \setminus \mR_i^{(\epsilon)}} = 1$. Furthermore, from the definition of $\mR_i^{(\epsilon)}$, $\Pr_{f_j, \pi}\rbr{\bx_1, \ldots, \bx_{M_{\epsilon}-1} \notin \mR_j^{(\epsilon)}} = \Pr_{f_0, \pi}\rbr{\bx_1, \ldots, \bx_{M_{\epsilon}-1} \notin \mR_j^{(\epsilon)}} \geq 1/M_{\epsilon}$. By noting the sequence $\bx_1, \ldots, \bx_{M_{\epsilon}-1} \notin \mR_j^{(\epsilon)}$ suffer from at least $2\epsilon(M_{\epsilon}-1)$ regret under $f = f_j$, we have the following:
\begin{equation}
    \Pr_{f_j, \pi}\rbr{R_T \geq 2\epsilon(M_{\epsilon}-1)} \geq 1/M_{\epsilon} = 1/T.
\end{equation}
Therefore, for any $T \geq 2$ and any algorithm $\pi$, there exists $f \in \mH_{k}$ such that $\|f\|_{k} \leq B$ and the following inequality holds with probability at least $1/T$.
\begin{align}
    R_T \geq 2\epsilon (M_{\epsilon}- 1) =  2\rbr{\frac{h(\bm{0})}{6c_1 \|h\|_{k}}} B T^{-\frac{\nu}{d}} (T -1) \geq \rbr{\frac{h(\bm{0})}{6c_1 \|h\|_{k}}} B T^{\frac{d-\nu}{d}}.
\end{align}
By setting $C = \rbr{\frac{h(\bm{0})}{6c_1 \|h\|_{k}}}$ in the above inequality, we obtain the desired statement. \qed

\begin{remark}
    The above proof does not lead to the same order of expected regret as the deterministic regret lower bound in \thmref{thm:cr_lb}, since the probability of the event, which the lower bound holds with, may decay in the order of $1/T$. Therefore, the only thing we can claim is $\Ep_{f_j}[R_T] \geq \Ep_{f_j}[\1\{\bx_1, \ldots, \bx_{M_{\epsilon}-1} \notin \mR_j^{(\epsilon)}\} R_T] \geq \Omega(BT^{-\nu/d})$, which is worse than trivial constant regret lower bound $\Omega(1)$.
\end{remark}

\end{document}