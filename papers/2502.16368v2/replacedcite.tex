\section{Related Work}
\label{sec: related work}

\subsection{Training-based Erasure}

We first summary the training-based concept erasure methods. Here, the word "training-based" generally refers to methods that change model parameters in various ways.

\textbf{Generative distribution alignment.} Concept Ablating (CA) ____ matches the generative distribution of a target concept to the distribution of an anchor concept. Erasing Stable Diffusion (ESD) ____ fine-tunes the distribution of a target concept to mimic the negatively guided ones. While CA and ESD align the predicted noises, Forget-Me-Not (FMN) ____ suppresses the activation of concept-related content in the attention layers. Considering the gap between the visual and textual features in text-to-image diffusion models, Knowledge Transfer and Removal ____ is proposed to replace collected texts with learnable prompts. Dark Miner ____ also conveys this idea. Adversarial training is also introduced for robustness erasure ____.


\textbf{Parameter editing.} Unified Concept Editing (UCE) ____ formalizes the erasure task by aligning the projection vectors of target concepts to those of anchor concepts in the attention layers. It derives a closed-form solution for the attention parameters under this objection and edits the model parameters directly. Based on UCE, Reliable and Efficient Concept Erasure (RECE) ____ introduces an iterative editing paradigm for a more thorough erasure. Mass Concept Erasure  (MACE) ____ leverages the closed-form parameter editing along with parallel LoRAs ____ to enable multiple concept erasure.


\textbf{Model pruning.} Previous studies find that certain concepts activate specific neurons in a neural network ____. Yang et al. ____ selectively prune critical parameters related to concepts and empirically confirm
its superior performance. SalUn ____ proposes a new metric named weight saliency and utilizes the gradient of a forgetting loss to ablate the salient parameters. Relying on the forward process, ConceptPrune ____ identifies activated neurons of the feed-forward layers and zeros them out.


\textbf{Text encoder fine-tuning.} The methods mentioned above modify the parameters of diffusion models, ignoring the text encoder, another important component in the generation process. Latent Guard ____ learns an embedding mapping layer on top of the text encoder to check the presence of concepts in the prompt embeddings. GuardT2I ____ fine-tunes a Large Language Model to convert prompt embeddings into natural languages and analyze their intention, which helps determine the presence of concepts in generated images under the guidance of these prompts.



\subsection{Training-free Erasure}

The training-free methods focus on using the inherent ability of diffusion models to prevent the generation of concept-related content. Safe Latent Diffusion (SLD) ____ is a pioneering work in this field. SLD proposes safety guidance. It extends the generative diffusion process by subtracting the noise conditioned on target concepts from the noise predicted at each time step. Recently, SAFREE ____ constructs a text embedding subspace using target concepts and removes the components of input embeddings in the corresponding subspace. Further, SAFREE fuses the latent images conditioned on the initial and processed embeddings in the frequency domain.