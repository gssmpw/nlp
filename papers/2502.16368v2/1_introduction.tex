\section{Introduction}
\label{sec: introduction}




In recent years, text-to-image diffusion models \cite{gal2022image, mou2024t2i, nichol2021glide, rombach2022high,  ruiz2023dreambooth, saharia2022photorealistic}, such as Stable Diffusion \cite{rombach2022high}, have developed rapidly, attracting widespread attention from academia and industry. They are usually pre-trained on large-scale datasets and then fine-tuned on downstream data as needed. Benefiting from the diversity of datasets, text-to-image diffusion models can acquire rich visual features of entities in the physical world and associate them with the text modality. They can not only generate high-fidelity images but also possess excellent text conditioning ability, thereby producing image content that aligns with user intentions \cite{zhang2023text, cao2024controllable}. However, everything has two sides. Large-scale training datasets inevitably contain undesired content, such as nudity, blood, copyright, etc. It endangers the models themselves with the capability to generate such content \cite{qu2023unsafe}, threatening individual rights and social harmony. 

The task of \emph{concept erasure} has been proposed to address this issue \cite{schramowski2023safe, kumari2023ablating, gandikota2023erasing}. Its goal is to prevent models from generating content related to certain concepts. Intuitively speaking, if the generation path from the textual inputs to the target visual concepts can be cut off, the resultant images, being guided by those texts, would naturally exclude the presence of these concepts. Researchers have explored various methods based on this idea, which can be categorized into training-based and training-free methods. Within the area of training-based methods, CA \cite{kumari2023ablating} and ESD \cite{gandikota2023erasing} fine-tune the generation distributions conditioned on the texts containing target concepts. Instead of gathering texts, Bui et al. \cite{, bui2024removing} and Meng et al. \cite{meng2024dark} incorporate a learnable prompt in the training and other works \cite{pham2024robust, kim2024race, zhang2024defensive, huang2023receler} introduce adversarial training to improve the robustness of concept erasure. UCE \cite{gandikota2024unified}, MACE \cite{lu2024mace}, and RECE \cite{gong2024reliable} edit cross-attention weights by aligning the keys and values of target concepts to others. There are also some methods \cite{fan2023salun, yang2024pruning, chavhan2024conceptprune} ablating model parameters based on their sensitivity to related prompts. While these methods fine-tune diffusion models, Latent Guard \cite{liu2025latent} and GuardT2I \cite{yang2024guardt2i} propose to fine-tune the text encoders. Within the area of training-free methods, Negative Guidance \cite{rombach2022high} and SLD \cite{schramowski2023safe} use concept texts to steer generation in the opposite direction. SAFREE \cite{yoon2024safree} maps the prompt embeddings to those of target concepts and removes the corresponding components.

While most studies explore text-based concept erasure methods, little attention has been paid to accomplishing this task based on the output of generative models, i.e. generated images. Text-based erasure methods often face the challenge of \emph{prompt generalization} \cite{tsai2023ring, zhang2023generate, yang2024mma, chin2023prompting4debugging}. It arises due to the difficulty in using prompts, which are collected or learned by these methods, to comprehensively cover the diverse image content associated with target concepts. Consequently, the effectiveness of the erasure is limited. Consider a simple example. The prompt ``A woman looking seductive'' does not explicitly convey the meaning of nudity. However, as demonstrated in Fig.\ref{fig: example_main}, it can prompt the erased models to generate an image of a naked woman. If the focus is shifted to images, the target of erasure becomes more directly addressed, potentially leading to a more effective erasure performance. A common practice is to incorporate a safety checker following the generation process to filter out unwanted content like in Stable Diffusion \cite{rombach2022high}, which works well but cannot correct generated content to obtain images like text-based erasure methods. 

In this paper, we aim to actively intervene intermediate images in the generation to reliably erase target concepts from final images. Using the diffusion generation theory, we note that final images predicted at certain time steps present enough structure and detail features for a detector to check concepts. Once concepts are detected, we consider how to erase them. Existing solutions like prompt editing \cite{hertzprompt} and negative guidance \cite{ho2021classifier, schramowski2023safe} cannot remove the concept features from images, causing target concepts to still exist. Some methods like Receler \cite{huang2023receler} suppress concept features within the attention layers. However, they rely on concept words in input prompts, which are difficult to anticipate in advance or accurately capture during inference due to the diversity and implicitness of languages. Thus, we propose Concept Removal Attention, a variation of the cross attention mechanism. It erases generated concept features by giving the names of target concepts and negative concepts and perturbing generated features. These efforts form our method \textbf{Concept Corrector}. 

Compared with previous methods, our method has significant advantages in terms of \textbf{reliability}. Firstly, it checks concepts in images, thereby providing a more direct assessment of concepts. Secondly, it does not require input prompts to contain explicit concept words, solving the problem of prompt generalization. Moreover, all parameters remain unchanged, which protects model knowledge. In the experiments, we evaluate the erasure performance using user prompts and adversarial prompts. The evaluated concepts include Not-Safe-For-Work, objects, and painting styles. As shown in Fig.\ref{fig: example_main}, Tab.\ref{tab: main results} and Tab.\ref{tab: adversarial results}, our method achieves impressive erasure performance, significantly reducing the generation of most concepts to within 5\% while other methods are still far from this level of performance. The contributions of this paper can be outlined as follows.

\begin{itemize}
    \item We carefully analyze the feasibility of using intermediate images in the generation for checking target concepts. 
    \item We propose Concept Removal Attention to erase concept features in the generation. It changes no model parameters and only requires the names of target concepts and negative concepts, making erasure easy to implement.
    \item The above explorations forms Concept Corrector, a straightforward but effective method to erase concepts on the fly. To our knowledge, this is the first work to achieve erasure based on intermediate-generated images.
    \item The experiments and visualizations demonstrate the impressive effectiveness of our method. A series of ablation experiments are conducted to discuss each component.
\end{itemize}
