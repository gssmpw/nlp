\section{Experiments}
\label{sec: experiments}



\subsection{Experimental Setting}
\label{sec: setting}

\textbf{Baselines.} 9 previous methods are compared in the experiments, including 7 training-based methods, i.e. CA \cite{kumari2023ablating}, ESD \cite{gandikota2023erasing}, RECE \cite{gong2024reliable}, SalUn \cite{fan2023salun}, MACE \cite{lu2024mace}, Receler \cite{huang2023receler}, and LatentGuard \cite{liu2025latent}, and 2 training-free methods, i.e. SLD \cite{schramowski2023safe} and SAFREE \cite{yoon2024safree}. The strength level of the safety guidance in SLD is set to Max. Please refer to Appendix B for the reproduction details.

\textbf{Evaluation Protocols.} Unless specifically mentioned, all experiments are conducted on Stable Diffusion v2.1 (SD v2.1) \cite{rombach2022high}, the scheduler is DDIM \cite{songdenoising}, and the sampling step is 50. We evaluate the erasure performance and the generation performance of all methods.

For the erasure performance, we erase 6 concepts that fall into three categories: Not-Safe-For-Work (NSFW), objects, and painting styles. The training-based methods erase the concepts individually while the training-free methods erase them collectively. \textbf{Concept Ratio} (\%) measures the erasure performance under the user prompts and the adversarial prompts respectively. It represents the proportion of all images in which the corresponding concept is detected. GPT-4o \cite{achiam2023gpt} generates 100 user prompts for each concept except for the NSFW concepts. Please refer to Appendix C for the instruction for GPT-4o. For the NSFW concepts, the prompts are selected from Inappropriate Image Prompts (I2P) \cite{schramowski2023safe}. Compared with other prompts, the selected prompts enable SD v2.1 to generate images with the highest scores provided by the detectors. Based on these user prompts, Ring-A-Bell \cite{tsai2023ring} searches the adversarial prompts. The models generate two images for each user prompt and one for each adversarial prompt.

% The instruction is ``\textit{I want to generate images by text-to-image generative models. The images contain [CONCEPT]. Please provide me with 100 various prompts}''.

The NSFW concepts are nudity and shock. We use the NudeNet \cite{nudenet} and the Q16 detector \cite{schramowski2022can} to detect them respectively. The detected elements for nudity are exposed buttocks, exposed breasts, exposed genitals, and exposed anuses. The detection threshold is set to 0.5.

The object concepts are bird and couch. The pre-trained YOLO-11x \footnote{https://github.com/ultralytics/ultralytics} is used to detect them. The confidence threshold is 0.5. When an image has at least one valid detection result, it is considered to contain the corresponding concept.

The style concepts are the painting styles of Van Gogh and Monet. We apply CLIP \cite{radford2021learning} as a style detector. We first calculate the CLIP scores between an image and three texts respectively, i.e. ``\textit{an image in the style of [ARTIST]}'', ``\textit{an authentic image}'', and ``\textit{an image in an unknown style}''. Then the softmax function is applied to them, and the maximum score indicates the style that the image belongs to.

For the generative performance, we sample 5,000 captions that contain no concept mentioned above from MS-COCO 2017 validation set \cite{lin2014microsoft}. Each prompt is used to generate one image. The metrics for evaluation include the CLIP Score and the Aesthetic Score. The CLIP Score \cite{radford2021learning} measures the alignment of an image and its corresponding prompt. The Aesthetic Score \cite{schuhmann2022laion} measures the mainstream human preference for aesthetic styles. They are the main dimensions for evaluating text-to-image models \cite{2023ImageReward}. For the training-based methods, we report their minimum results, indicating the upper limit of the performance.

\textbf{Implementations.} To effortlessly check diverse concepts, we opt to incorporate a pre-trained Vision-Language Model (VLM) \cite{du2022survey} rather than training extra detection models. Specifically, the used VLM is LLaVa-OneVision-Qwen2-7B \cite{li2024llava}, a recent model with state-of-the-art performance on various benchmarks. Other popular VLMs can also achieve similar results in our experiments. The designed query for the VLM can be found in Appendix D. The checkpoints $t_1$ and $t_2$ are set to 40 and 20 respectively. For all concepts except for shock, the checked content is the concept names. Considering that shock encompasses a multitude of elements, we further add blood, ugly faces, surprising faces, unusual bodies, and unusual faces. The compared methods have also embraced this supplement. The pre-defined negative concepts are listed in Appendix E.

% To effortlessly carry out the check of diverse concepts, we opt to incorporate a pre-trained Large Language-Vision Model (VLM) \cite{du2022survey} rather than training detection models for this task. It receives the predicted final images $\hat{x}_0$ as input and processes a predefined query, inquiring whether the images contain the target concepts. To format its outputs, we leverage in-context learning \cite{dong2022survey} in the formulation of the query. Please refer to Appendix A for the specific query we design.

\begin{table*}[t]
    \centering
    \caption{The results of the evaluation on the user prompts (the erasure performance) and COCO prompts (the generation performance). CLIP: CLIP Score. AES: Aesthetic Score. The \colorbox{lightgray}{mark} indicates the best result. * denotes the use of the official pre-trained model.}
    \begin{tabular}{l c c | c c | c c | c c | c c}
    \toprule
       \multirow{3}{*}{Method}  & \multirow{3}{*}{\makecell[c]{Training\\-Free}} & \multirow{3}{*}{\makecell[c]{Image\\-Based}} &  \multicolumn{6}{c|}{User Prompts $(\%, \downarrow)$  } & \multicolumn{2}{c}{COCO Prompts $(\uparrow)$} \\
        \cmidrule(lr){4-9} \cmidrule(lr){10-11}
         &  &  & \multicolumn{2}{c|}{NSFW} & \multicolumn{2}{c|}{Object} & \multicolumn{2}{c|}{Painting Style} & \multirow{2}{*}{CLIP} & \multirow{2}{*}{AES} \\
        & & & Nudity & Shock & Bird & Couch & Van Gogh & Monet & & \\

    \midrule
    SD v2.1 & - & - & 61.5 & 95.0 & 89.5 & 92.5 & 99.5 & 99.0 & 31.73 & 6.25 \\ \midrule
    CA \cite{kumari2023ablating} & \ding{55} & \ding{55} & 21.0 & 83.0 & 79.0 & 73.0 & 85.0 & 90.0 & 31.58 & 6.19 \\
    ESD \cite{gandikota2023erasing} & \ding{55} & \ding{55} & 45.0 & 82.0 & 80.5 & 75.0 & 85.5 & 86.0 & \colorbox{lightgray}{31.59} & 6.17 \\
    RECE \cite{gong2024reliable} & \ding{55} & \ding{55} & \colorbox{lightgray}{2.0} & 67.5 & 55.0 & 52.0 & 39.0 & 59.0 & 29.81 & 5.96 \\
    SalUn \cite{fan2023salun} & \ding{55} & \ding{55} & 5.5 & 62.0 & 36.0 & 44.5 & 88.0 & 85.5 & 30.14 & 5.90 \\
    MACE \cite{lu2024mace} & \ding{55} & \ding{55} & 5.0 & 48.5 & \colorbox{lightgray}{3.5} & 15.0 & 59.0 & 36.5 & 31.23 & 6.15 \\
    Receler \cite{huang2023receler} & \ding{55} & \ding{55} & 19.0 & 74.0 & 59.5 & 71.0 & 14.0 & 38.0 & 31.53 & 6.21 \\
    LatentGuard* \cite{liu2025latent} & \ding{55} & \ding{55} & 37.0 & 62.5 & - & - & - & - & 29.38 & 6.17\\ \midrule
    SLD-Max \cite{schramowski2023safe} & \textcolor{LimeGreen}{\ding{52}} & \ding{55} & 3.5 & 42.0 & 64.0 & 67.0 & 9.0  & 45.5 & 28.91 & 6.00\\
    SAFREE \cite{yoon2024safree} & \textcolor{LimeGreen}{\ding{52}} & \ding{55} & 14.0 & 43.5 & 73.5 & 58.5 & 30.5 &31.0 & 30.89 & \colorbox{lightgray}{6.37} \\ \midrule
    Ours & \textcolor{LimeGreen}{\ding{52}} & \textcolor{LimeGreen}{\ding{52}} & 4.0 & \colorbox{lightgray}{37.0} & \colorbox{lightgray}{3.5} & \colorbox{lightgray}{4.5} & \colorbox{lightgray}{2.0} & \colorbox{lightgray}{3.5} & 30.81 & 6.24 \\
    
    \bottomrule
    \end{tabular}
    \vspace{-0.2cm}
    \label{tab: main results}
\end{table*}

% \begin{table}[]
%     \centering
%     \caption{The results of the evaluation on the adversarial prompts searched by Ring-A-Bell \cite{tsai2023ring} (measured by Concept Ratio, \%, $\downarrow$). R.: RECE. S.: SalUn. L.: LatentGuard. SA.: SAFREE.}
%     \footnotesize
%     \begin{tabular}{c| c c c c c | p{0.4cm}<{\centering} p{0.4cm}<{\centering} | c }
%     \toprule
%          Concept &  CA & ESD & R. & S. & L. & SLD & SA. & Ours \\ \midrule
%          Nudity & 32 & 71 & \colorbox{lightgray}{3} &  & 25 & 9 & 47 & 5 \\
%          Bird & 92 & 93 & 72 & & - & 68 & 99 & \colorbox{lightgray}{1} \\
%          Monet & 36 & 28 & 22 & & - & 20 & 57 & \colorbox{lightgray}{1}\\
%     \bottomrule
%     \end{tabular}
    
%     \label{tab: adversarial results}
% \end{table}

% \begin{table}[t]
%     \centering
%     \caption{The results of the evaluation on the adversarial prompts.}
%     \footnotesize
%     \begin{tabular}{l | c c | c c | p{0.3cm}<{\centering} c}
%     \toprule
%        \multirow{3}{*}{Method}  &  \multicolumn{6}{c}{  Adversarial Prompts $(\%, \downarrow)$  } \\
%         \cmidrule(lr){2-7} 
%          & \multicolumn{2}{c|}{NSFW} & \multicolumn{2}{c|}{Object} & \multicolumn{2}{c}{Style}  \\
%         & Nudity & Shock & Bird & Couch & Van. & Monet \\

%     \midrule
%     SD v2.1 & 23 & 58 & 97 & 68 & 60 & 66 \\ \midrule
%     CA \cite{kumari2023ablating} & 9 & 49 & 92 & 69 & 35 & 36 \\
%     ESD \cite{gandikota2023erasing} & 14 & 51 & 93 & 57 & 37 & 28\\
%     RECE \cite{gong2024reliable} & \colorbox{lightgray}{1} & 26 & 72 & 23 & 13 & 22 \\
%     SalUn \cite{fan2023salun} & \\
%     LatentGuard \cite{liu2025latent} & 14 & 35 & - & - & - & -   \\ \midrule
%     SLD-Max \cite{schramowski2023safe} & \colorbox{lightgray}{1} &20 & 68 & 29 & \colorbox{lightgray}{3} & 20\\
%     SAFREE \cite{yoon2024safree} & 8 & 24 &99 &57 & 67 & 57\\ \midrule
%     Ours & 4 & \colorbox{lightgray}{2} & \colorbox{lightgray}{1} & \colorbox{lightgray}{4} & 5 & \colorbox{lightgray}{1} \\
    
%     \bottomrule
%     \end{tabular}
    
%     \label{tab: adversarial results}
% \end{table}

\begin{table}[t]
    \centering
    \caption{The results of the evaluation on the adversarial prompts (measured by Concept Ratio) and the results of the image-based attack MMA-Diffusion \cite{yang2024mma} (measured by Attack Success Rate).}
    \footnotesize
    \begin{tabular}{l | c c c | c }
    \toprule
       \multirow{2}{*}{Method}  &  \multicolumn{3}{c|}{\makecell{Adversarial Prompts \\ (Text, $\%, \downarrow$)}} & \makecell{MMA-Attack \\ (Image, $\%, \downarrow$) }\\
        \cmidrule(lr){2-4} \cmidrule(lr){5-5} 
        & Nudity & Bird & Van Gogh & Nudity \\

    \midrule
    CA \cite{kumari2023ablating} & 32 & 92 & 92 & 45.5\\
    ESD \cite{gandikota2023erasing} & 71 & 93 & 93 & 69.7\\
    RECE \cite{gong2024reliable} & \colorbox{lightgray}{3} & 72 & 23 & 38.1\\
    SalUn \cite{fan2023salun} &  6 & 31 & 98 & 26.4 \\
    MACE \cite{lu2024mace} & 11 & 30 & 39 & 37.7 \\
    Receler \cite{huang2023receler} & 12 & 78 & 5 & 40.9 \\
    LatentGuard* \cite{liu2025latent} &  25 & - & - & 47.5 \\ \midrule
    SLD-Max \cite{schramowski2023safe} & 9 & 68 & 4 & 27.5\\
    SAFREE \cite{yoon2024safree} &47 & 99 & 31 & 43.0\\ \midrule
    Ours & 5 & \colorbox{lightgray}{1} & \colorbox{lightgray}{1}& \colorbox{lightgray}{19.3}\\

    \bottomrule
    \end{tabular}

    \label{tab: adversarial results}
\end{table}

% \begin{table}[]
%     \centering
%     \caption{The results of the image-based attack MMA-Diffusion \cite{yang2024mma} (measured by Attack Success Rate, \%, $\downarrow$).}
%     \footnotesize
%     \begin{tabular}{c| c c c c c | p{0.4cm}<{\centering} p{0.4cm}<{\centering} | c }
%     \toprule
%          Concept &  CA & ESD & R. & S. & L. & SLD & SA. & Ours \\ \midrule
%          Nudity & 45.5 & 69.7 & 38.1 &  & 47.5 & 27.5 & 43.0 & \colorbox{lightgray}{19.3} \\
%     \bottomrule
%     \end{tabular}
    
%     \label{tab: mma}
% \end{table}





\subsection{Evaluation Results}

Tab.\ref{tab: main results} shows the results of the evaluation on the user prompts and COCO prompts, and Tab.\ref{tab: adversarial results} shows the results on the adversarial prompts. Our method achieves nearly complete erasure for the concepts of nudity, bird, couch, and the painting styles of Van Gogh and Monet, with a Concept Ratio of less than 5\%. For the concept of shock, our method also surpasses others significantly. While RECE and SLD-Max demonstrate comparable or superior erasure performance in erasing nudity and Van Gogh's painting style, their effectiveness in erasing other concepts lags significantly behind ours. Furthermore, they both inflict considerable damage to the generative capabilities of the models. 

Considering that our method is based on images, we further utilize MMA-Diffusion \cite{yang2024mma}, a multi-modal attack on diffusion models, to evaluate the erasure performance on the task of image in-painting. Following the paper \cite{yang2024mma}, we report the results in Tab.\ref{tab: adversarial results}. It demonstrates the better defense performance of our method on image attacks.

Our method also works well for prompts with multiple concepts. Please see Appendix F for details. We provide visualizations of concept-erased images in Appendix J. 


\subsection{Discussion}
\subsubsection{Checkpoints}
First, we analyze how \textbf{the location of checkpoints} affects the erasure performance by setting only one checkpoint. 

\begin{figure}
    \centering
    \hspace{-0.5cm}
    \includegraphics[scale=0.7, trim=20 15 20 20,clip]{check_rate.pdf}
    \caption{The rate of successful checks at different time steps.}
    \label{fig: check rate}
\end{figure}





\underline{The impact on check accuracy.} Fig.\ref{fig: check rate} shows the rate of successful checks when checking at different time steps. Most generated content can be identified successfully in the early diffusion process. After the midpoint of generation, the rate stabilizes, approaching 1.00.

\underline{The impact on correction performance.} Tab.\ref{tab: time step} displays the Concept Ratio when only Concept Removal Attention (CRA) -1 or -2 is applied individually at various time steps. The trend is an initial decrease followed by an increase. It suggests that an earlier check results in poor check performance, whereas a later one leads to inadequate correction.

The above results tell us that selecting checkpoint locations should consider sufficient visual generation for check and an adequate number of time steps for correction.

Then, we discuss how \textbf{the number of checkpoints} affects the erasure performance. 

Combining Tab.\ref{tab: main results} and Tab.\ref{tab: time step}, it can be observed that setting two checkpoints results in better erasure than setting only one. With our recommended values of $t_1$ and $t_2$, the performance improvement by adding more checks is limited. This is mainly attributed to the effective results obtained currently, the marginal increase in check accuracy in subsequent time steps (as shown in Fig.\ref{fig: check rate}), and the insufficient correction in subsequent time steps (as shown in Tab.\ref{tab: time step}). Please refer to Appendix G for the results of more checkpoint choices.


\begin{figure}
    \begin{minipage}{0.6\linewidth}
        \centering
        \captionof{table}{Results when applying Concept Removal Attention (CRA-1 or CRA-2) at different $t$.}
        \footnotesize
        \begin{tabular}{p{0.1cm}<{\centering} p{0.1cm}<{\centering}|p{0.4cm}<{\centering} p{0.4cm}<{\centering} p{0.4cm}<{\centering} p{0.4cm}<{\centering} p{0.4cm}<{\centering}}
        \toprule
            \multicolumn{2}{c|}{CRA} & \multicolumn{5}{c}{Nudity $(\%, \downarrow)$} \\
            \cmidrule(lr){1-2} \cmidrule(lr){3-7}
            1&2 & 50 & 40 & 30 & 20 & 10 \\ \midrule
            \ding{52} & \ding{55} & \footnotesize 58.0 & \footnotesize 12.0 & \footnotesize 22.0 & \footnotesize 40.0 & \footnotesize 53.5 \\
            \ding{55} & \ding{52} & \footnotesize 58.0 & \footnotesize 13.5 & \footnotesize 22.5 &\footnotesize  40.0 & \footnotesize 52.5 \\
             \bottomrule
        \end{tabular}
        \label{tab: time step}
    \end{minipage}  
    \hfill
    \begin{minipage}{0.38\linewidth} 
        \centering
        \vspace{0.15cm}
        \captionof{table}{Results with negative concepts (NC).}
        \begin{tabular}{ c | c}
        \toprule
            NC & \footnotesize Nudity $(\%, \downarrow)$\\ \midrule
            \ding{182} & 5.5 \\
            \ding{183} & 5.0 \\
            \ding{184} & 4.5 \\
             \bottomrule
        \end{tabular}
        \label{tab: negative prompt}
    \end{minipage}  
\end{figure}

\begin{figure}
    \begin{minipage}{0.48\linewidth}
        \centering
        \captionof{table}{Results when setting other values for $\alpha_t$, $\mathcal{M}_1$, $\mathcal{M}_2$.}
        \footnotesize
        \vspace{-0.2cm}
        \begin{tabular}{p{0.1cm}<{\centering} p{0.2cm}<{\centering} p{0.4cm}<{\centering} | p{0.6cm}<{\centering} p{0.6cm}<{\centering}}
        \toprule
            \multirow{2}{*}{$\alpha_t$} & \multirow{2}{*}{$\mathcal{M}_1$} & \multirow{2}{*}{$\mathcal{M}_2$} & \multicolumn{2}{c}{Nudity} \\
            & & & Ratio$\downarrow$ & CLIP$\uparrow$  \\ \midrule
            % \ding{55} &\ding{52} & \ding{52} & 14.0 & - \\
            %  \ding{52}& \ding{55} & \ding{52} & 3.5 & 20.88 \\ 
            %  \ding{52} & \ding{52} & \ding{55} & 10.0 & -\\ \midrule
            % \ding{52} & \ding{52} & \ding{52} &  4.0 &  26.51 \\
            1 & \ding{52} & \ding{52} & 14.0 & - \\
             \ding{52} & $\mathbf{1}$ & \ding{52} & 3.5 & 20.88 \\ 
             \ding{52} & \ding{52} & $\mathcal{M}_1$ & 10.0 & -\\ \midrule
            \ding{52} & \ding{52} & \ding{52} &  4.0 &  26.51 \\
             \bottomrule
        \end{tabular}
        \label{tab: ablation study}
    \end{minipage}  
    \hfill
    \begin{minipage}{0.51\linewidth} 
        \centering
        \captionof{table}{Results with other models. $\Delta$ denotes the change compared to the original model.}
        \vspace{-0.18cm}
        \footnotesize
        \begin{tabular}{l | l}
        \toprule
            Model & Nudity $(\%, \downarrow)$  \\ \midrule
            SD-v1.4 & 9.5 ($\Delta$: -40.5) \\
            SD-XL-v1.0 & 2.5 ($\Delta$: -26.5) \\
            PixArt-XL-2 & 0.0 ($\Delta$: -4.0) \\
            PlayGround-v2.5 & 1.5 ($\Delta$: -8.5) \\
             \bottomrule
        \end{tabular}
        \label{tab: SD version}
    \end{minipage}  
\end{figure}


\begin{figure}
    \begin{minipage}{0.4\linewidth}
        \centering
        \footnotesize
        \captionof{table}{Results with various sampling timesteps.}
        \vspace{-0.cm}
        \begin{tabular}{c|c}
        \toprule
            \# timesteps & Nudity \\ \midrule
            10 & 4.0 \\
            25 & 3.5 \\
            50 & 4.0 \\
            500 & 2.5 \\
            1000 & 3.5 \\
            \bottomrule
        \end{tabular}
        \label{tab: sampling timesteps}
    \end{minipage}  
    \hfill
    \begin{minipage}{0.57\linewidth} 
        \centering
        \captionof{table}{Results with other diffusion schedulers.}
        \vspace{-0.15cm}
        \footnotesize
        \begin{tabular}{l | l}
        \toprule
            Scheduler & Nudity $(\%, \downarrow)$  \\ \midrule
            Heun & 0.5 ($\Delta$: - 35.5) \\
            UniPC \cite{zhao2024unipc} & 2.0 ($\Delta$: - 35.0) \\
            EDM \cite{karras2022elucidating} & 2.5 ($\Delta$: - 34.5) \\
            DPM \cite{lu2022dpm} & 4.5 ($\Delta$: - 33.5) \\
            DPM++ \cite{lu2022dpm++} & 4.5 ($\Delta$: - 31.5) \\
            SDE-DPM++ \cite{lu2022dpm++} & 1.5 ($\Delta$: - 41.5) \\
             \bottomrule
        \end{tabular}
        \label{tab: schedulers}
    \end{minipage}  
\end{figure}



\subsubsection{Concept Removal Attention}

\textbf{The impact of negative concept descriptions.} For erasing nudity, we use ``\textit{Covered from neck to toe in clothing}'' to describe the negative concept. More descriptions are evaluated. They include: \ding{182}``\textit{dressed person}'', \ding{183}``\textit{person in clothes}'', and \ding{184}``\textit{Covered in clothing}''. The results are shown in Tab.\ref{tab: negative prompt}. It confirms that our method is robust to various descriptions of a negative concept.

\textbf{The impact of $\alpha_t$ and $\mathcal{M}$.} We set $\alpha_t=1$, $\mathcal{M}_1=\mathbf{1}$, and $\mathcal{M}_2=\mathcal{M}_1$ respectively to ablate them and evaluate their performance. The results are shown in Tab.\ref{tab: ablation study}. The dropped performance with $\alpha_t=1$ confirms its crucial role in Concept Removal Attention. 

For $\mathcal{M}_1$, the erasure performance is lightly improved. The role of $\mathcal{M}_1$ is mainly reflected in its maintenance of irrelevant content in prompts during the correction process. To illustrate this point, we further compute the CLIP Score between the corrected images and the prompts. Since their nudity ratios are similar, the CLIP Score approximately measures the alignment between the images and the irrelevant content of the prompts. It can be seen that the CLIP Score drops significantly when $\mathcal{M}_1=\mathbf{1}$. In addition, the examples given in Fig.\ref{fig: example_main} also prove that our method has a great preservation of the irrelevant words in prompts. 

For $\mathcal{M}_2$, the performance drops when it is set to $\mathcal{M}_1$. The rationale behind setting $\mathcal{M}_2=\mathbf{1}$ stems from the fact that diffusion models tend to refine details during the later stages of the generation process, and a higher correction intensity is required due to the generation closer to the end. When a concept is checked out at $t_2$, there is a limited window for making corrections. Consequently, under this condition, global feature redirection becomes imperative to swiftly correct concepts. For global detail features, global redirection is needed to correct concepts such as painting styles. Please refer to Appendix H for more discussions.

\subsubsection{Diffusion Generation Configurations}
Tab.\ref{tab: SD version} shows the erasure results with other diffusion models, including other SD models, PixArt \cite{chenpixart}, and PlayGround \cite{li2024playground}. Tab.\ref{tab: sampling timesteps} shows the erasure results using various sampling timesteps. We set the checkpoints using the same position ratio as in the main experiments, i.e. $t_1=0.8T$ and $t_2=0.4T$ where $T$ is the time steps. Tab.\ref{tab: schedulers} reports the performance when applying other popular diffusion schedulers. These results prove the universal applicability of our method across the diffusion configurations.

\subsubsection{Time Efficiency}
Under the same implementation, our method is 10.9\% faster than SLD and 75.4\% faster than SAFREE in the generation time. It should be noticed that simple concept-specific detectors rather than the VLM can also achieve a similar binary check performance but improve the efficiency of the pipeline significantly, as demonstrated by our extended experiments. Please refer to Appendix I for these details.

\section{Conclusion}
\label{sec: conclusion}

In this paper, we introduce and validate the feasibility of erasing concepts based on intermediate generated images rather than input prompts, a straightforward yet under-explored approach that is neglected in existing studies. Capitalizing on this insight, we propose Concept Corrector for erasing concepts on the fly without changing any parameters. Quantitative experiments coupled with visualizations demonstrate that it can reliably erase unwanted concepts while aligning images and non-targeted textual descriptors.



% 在引言和方法介绍中说明，现有的prompt替换或者negative引导都不行，因为没有细粒度的干预。