\section{Methods}
\label{sec: methods}

\subsection{Overview}
Fig.\ref{fig: pipeline} illustrates the generation pipeline of text-to-image diffusion models with our proposed Concept Corrector. This pipeline introduces two checkpoints, $t_1$ and $t_2$, within the diffusion process. As the time step $t$ reaches the checkpoints, the intermediate result $x_t$ is directed into the \textbf{Generation Check Block}. It checks whether any content related to target concepts is present. If such content is absent, the generation proceeds uninterrupted in its original course. Conversely, if one or more concepts are detected, the subsequent generation integrates the proposed \textbf{Concept Removal Attention} as the cross-attention mechanism. Depending on which checkpoint, $t_1$ or $t_2$, triggers the check, Concept Removal Attention-1 or -2 is employed, respectively. They differ subtly in their approach to fusing attention features, thereby enhancing their adaptability to the generation preferences of different generation stages.


\subsection{Generation Check Mechanism}
\label{sec: generation check mechanism}

As mentioned in Sec.\ref{sec: ldm}, the final generation results can be predicted at intermediate time steps. We highlight that they provide rich visual features to check concepts during the generation. Some previous studies \cite{choi2022perception, jung2024latent} explore the generative traits of different diffusion stages. A common observation is that diffusion models initially generate global structures and then refine these with local details as the diffusion progresses. It inspires us to set two checkpoints to check concepts, leveraging the generated structures and details at different stages.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{mid_results.pdf}
    \caption{The examples for the intermediate results $x_t$ and the predicted final results $\hat{x}_0$ generated by Stable Diffusion v2.1. DDIM \cite{songdenoising} is the scheduler with 50 sampling steps. The top: bodies. The middle: birds. The bottom: Van Gogh's painting style.}
    \label{fig: mid results}
\end{figure}

To further elaborate on the above motivation, we provide some examples in Fig.\ref{fig: mid results}. In these illustrations, the intermediate images $x_t$ have heavy noise, rendering it challenging for humans to discern the content. On the contrary, the clarity of the predicted final images $\hat{x}_0$ undergoes a significant enhancement. Moreover, during the initial time steps, discernible structures such as bodies and birds become evident in $\hat{x}_0$. In subsequent time steps, additional details are progressively generated, aiding in the identification of more concepts, such as Van Gogh's painting style.

At each checkpoint, we set a Generation Check Block respectively. It receives the predicted final images $\hat{x}_0$ as input and uses a detector to decide whether they contain any target concepts. Please see Sec.\ref{sec: setting} for the implementation.



% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.9\linewidth, trim=20 15 15 15,clip]{GenerationCheck.pdf}
%     \caption{The framework of Generation Check Block.}
%     \label{fig: generation_check}
% \end{figure}


\subsection{Concept Removal Attention}

Conditioned that there are target concepts in intermediate images, we further consider how to erase them. Editing prompts \cite{hertzprompt} or guiding generation negatively cannot remove features that already exist in images, leading to concepts still being present in final outputs. We provide the experiments and discussions in Appendix A.1. Some methods like Receler \cite{huang2023receler} suppress the features by concept words in input prompts, but the concept words we define may not appear in input prompts. To bridge this gap, we propose  Concept Removal Attention, as shown in Fig.\ref{fig: concept_removal_attn}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth, trim=20 20 20 20,clip]{ConceptRemovalAttn.pdf}
    \caption{The framework of Concept Removal Attention}
    \label{fig: concept_removal_attn}
\end{figure}

For each target concept, we define a negative concept $p_n$ to guide the models in generating alternative content. The attention function $CRAttn(z_t, p, p_n)$ is defined as:
\begin{equation}
\label{eq: CRA}
\begin{split}
    CRAttn(z_t, p, p_n) = &
     (1-\mathcal{M})  \cdot Attn(z_t, p) \\ & +  \mathcal{M} \cdot Attn_{CR}(z_t, p_n),
\end{split}
\end{equation}
where $\mathcal{M}$ is a concept mask which will be introduced later, $Attn(z_t, p)$ follows the definition in Eq.\ref{eq: attention}, and $Attn_{CR}(z_t, p_n)$ is defined as:
\begin{equation}
\label{eq: attention_CR}
    Attn_{CR}(z_t, p_n) = Softmax(\alpha_t \frac{QK_n^T}{\sqrt{d}})V_n,
\end{equation}
where $K_n=W_k\tau_\theta(p_n)$ and $V_n=W_v\tau_\theta(p_n)$. The primary difference between Eq.\ref{eq: attention} and Eq.\ref{eq: attention_CR} is that Eq.\ref{eq: attention_CR} has $\alpha_t$ which plays the role of a temperature coefficient. The motivation for this modification comes from our observation that the similarity between features of concept-related content and negative concepts is low sometimes. It causes the features of the prompt beginning token to occupy a high proportion of the attention weights, leading to the failure of this attention calculation. Recall the Softmax function $Softmax(\alpha_ts)_i = \frac{\exp^{\alpha_ts_i}}{\sum_k\exp^{\alpha_ts_k}} = \frac{1}{\sum_k\exp^{\alpha_t(s_k-s_i)}}$. When $\alpha_t<1$, the weights corresponding to the large components in $s$ will become small. We leverage it to overcome the guidance failure of negative prompts. When $t$ goes from $T$ to $0$, we set $\alpha_t=0.5+0.5\frac{t_i-t}{t_i} (t_i\in \{t_1, t_2\})$ as an increasing function in the range of $[0.5, 1]$.

% $\alpha_t=0.5+0.5\frac{t_i-t}{t_i}$

In Sec.\ref{sec: generation check mechanism}, we mention that diffusion models generate structural content first and then refine their details. The checkpoint $t_1$ relies on structural content, which often presents regional distributions in an image. The checkpoint $t_2$ focuses on details, which may be global characteristics exhibited by an image. Notably, $t_2$ occurs closer to the end of generation, necessitating a higher intensity for replacing concept-related content. Taking these considerations into account, we apply distinct Concept Removal Attention for the concepts checked at $t_1$ and $t_2$, resulting in two variants. Their difference is reflected in $\mathcal{M}$ in Eq.\ref{eq: CRA}.

In Concept Removal Attention-1 for $t_1$, it uses $W_q$ and $W_k$ to locate the features related to target concepts $c$:
\begin{equation}
    \mathcal{M}=\mathcal{M}_1 = \mathbb{I}(Softmax(\frac{QK_c^T}{\sqrt{d}})[:, idx_{EOS}] \geq \kappa),
\end{equation}
where $\mathbb{I}(\cdot)$ is the indicator function, $K_c=W_k\tau_\theta(c)$, $[\cdot, \cdot]$ denotes the matrix indexing operation, $idx_{EOS}$ is the index of the prompt ending token, and $\kappa$ is a threshold. Note that $c$ here stands for target concepts rather than any input prompt. It stems from our discovery that, even without explicit guidance from concept words, the features of concept words are closer to those of related content than those of other content. $idx_{EOS}$ is used because concepts such as nudity and Monet's style consist of multiple tokens. The embedding features at $idx_{EOS}$ can encode the overall semantics of concepts. $\kappa$ is determined adaptively by the mean of the softmax scores. To obtain an accurate $\mathcal{M}_1$, we average the corresponding $\mathcal{M}_1$ across all attention heads and use this averaged $\mathcal{M}_1$ for the attention calculations of all heads. We also find that $\mathcal{M}_1$ is less accurate for the shallowest and deepest layers in the noise predictors (such as U-Net \cite{ronneberger2015u} in Stable Diffusion \cite{rombach2022high}). We speculate that the reason may be the smaller receptive field of high-resolution features in the shallowest layers and the more coarse features in the deepest layer, which limits their ability to recognize concepts. Therefore, in the first down-sampling layer, the attention is not applied. In the last up-sampling layer and the deepest layer, $\mathcal{M}_1$ is the average of $\mathcal{M}_1$ in the preceding layers. Please refer to Appendix A.2 for the details. In Concept Removal Attention-2 for $t_2$, $\mathcal{M}=\mathcal{M}_2=\mathbf{1}$ is a matrix of all ones, intended for high-intensity, global replacement.


