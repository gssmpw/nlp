\section{Related Work}
\label{sec: related work}

\subsection{Training-based Erasure}

We first summary the training-based concept erasure methods. Here, the word "training-based" generally refers to methods that change model parameters in various ways.

\textbf{Generative distribution alignment.} Concept Ablating (CA) \cite{kumari2023ablating} matches the generative distribution of a target concept to the distribution of an anchor concept. Erasing Stable Diffusion (ESD) \cite{gandikota2023erasing} fine-tunes the distribution of a target concept to mimic the negatively guided ones. While CA and ESD align the predicted noises, Forget-Me-Not (FMN) \cite{zhang2023forget} suppresses the activation of concept-related content in the attention layers. Considering the gap between the visual and textual features in text-to-image diffusion models, Knowledge Transfer and Removal \cite{bui2024removing} is proposed to replace collected texts with learnable prompts. Dark Miner \cite{meng2024dark} also conveys this idea. Adversarial training is also introduced for robustness erasure \cite{pham2024robust, kim2024race, zhang2024defensive, huang2023receler}.


\textbf{Parameter editing.} Unified Concept Editing (UCE) \cite{gandikota2024unified} formalizes the erasure task by aligning the projection vectors of target concepts to those of anchor concepts in the attention layers. It derives a closed-form solution for the attention parameters under this objection and edits the model parameters directly. Based on UCE, Reliable and Efficient Concept Erasure (RECE) \cite{gong2024reliable} introduces an iterative editing paradigm for a more thorough erasure. Mass Concept Erasure  (MACE) \cite{lu2024mace} leverages the closed-form parameter editing along with parallel LoRAs \cite{hu2021lora} to enable multiple concept erasure.


\textbf{Model pruning.} Previous studies find that certain concepts activate specific neurons in a neural network \cite{wang2022finding}. Yang et al. \cite{yang2024pruning} selectively prune critical parameters related to concepts and empirically confirm
its superior performance. SalUn \cite{fan2023salun} proposes a new metric named weight saliency and utilizes the gradient of a forgetting loss to ablate the salient parameters. Relying on the forward process, ConceptPrune \cite{chavhan2024conceptprune} identifies activated neurons of the feed-forward layers and zeros them out.


\textbf{Text encoder fine-tuning.} The methods mentioned above modify the parameters of diffusion models, ignoring the text encoder, another important component in the generation process. Latent Guard \cite{liu2025latent} learns an embedding mapping layer on top of the text encoder to check the presence of concepts in the prompt embeddings. GuardT2I \cite{yang2024guardt2i} fine-tunes a Large Language Model to convert prompt embeddings into natural languages and analyze their intention, which helps determine the presence of concepts in generated images under the guidance of these prompts.



\subsection{Training-free Erasure}

The training-free methods focus on using the inherent ability of diffusion models to prevent the generation of concept-related content. Safe Latent Diffusion (SLD) \cite{schramowski2023safe} is a pioneering work in this field. SLD proposes safety guidance. It extends the generative diffusion process by subtracting the noise conditioned on target concepts from the noise predicted at each time step. Recently, SAFREE \cite{yoon2024safree} constructs a text embedding subspace using target concepts and removes the components of input embeddings in the corresponding subspace. Further, SAFREE fuses the latent images conditioned on the initial and processed embeddings in the frequency domain.


