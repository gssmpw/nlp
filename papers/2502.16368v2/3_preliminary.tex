\begin{figure*}[]
    \centering
    \includegraphics[width=1\linewidth, trim=0 0 0 40,clip]{pipeline.pdf}
    \caption{The generation pipeline of text-to-image diffusion models with our proposed Concept Corrector.}
    \label{fig: pipeline}
\end{figure*}

\section{Preliminaries}
\subsection{Latent Diffusion Models}
\label{sec: ldm}

Diffusion models \cite{sohl2015deep, ho2020denoising} iteratively estimate and remove the noise from the sampled Gaussian noise, yielding images after $T$ steps. They act as noise predictors conditioned on the time step $t$. Latent diffusion models \cite{rombach2022high} execute the process in a latent space and then decode generated latent images into pixel space. Text-to-image diffusion models \cite{ho2021classifier} incorporate a prompt $p$ as a condition for the noise prediction, achieving the generation of images aligned with $p$.

Let $\epsilon_\theta(z_t, t, p)$ denote a noise predictor with the parameters $\theta$. At the time step $t (T \geq t > 0)$, the estimated noise:
\begin{equation}
    \hat{\epsilon}_\theta(z_t, t, p) = \epsilon_\theta(z_t, t, \varnothing) + \gamma (\epsilon_\theta(z_t, t, p) - \epsilon_\theta(z_t, t, \varnothing)),
\end{equation}
where $\gamma$ is a guidance scale and $\varnothing $ denotes an empty prompt. When $t=T$, $z_t$ denotes a sampled Gaussian noise. Then $z_{t-1}$ follows a Gaussian distribution $N(z_{t-1}|\bm{\mu}_{t-1}, \sigma_{t-1}^2\mathbf{I})$:
\begin{equation}
    p(z_{t-1}|z_t, p) = N(z_{t-1};\frac{1}{\sqrt{\alpha_t}}(z_t-\frac{\beta_t}{\overline{\beta}_t}\hat{\epsilon}_\theta(z_t, t, p)), \frac{\overline{\beta}_{t-1}^2\beta_t}{\overline{\beta}_t^2}\mathbf{I}),
\end{equation}
where $\beta_t$ is a scheduled noise variance, $\alpha_t={1-\beta_t}$, $\overline{\alpha}_t=\alpha_1...\alpha_t$, and $\overline{\beta}_t=\sqrt{1-\overline{\alpha}_t}$. According to the Markov theory, we can derive $\hat{z}_0$ to predict the final denoising result $z_0$ \cite{sohl2015deep}:
\begin{equation}
\label{eq: predict z0}
    \hat{z}_0 = \frac{1}{\sqrt{\overline{\alpha}}_t}(z_t-\overline{\beta}_t\hat{\epsilon}_\theta(z_t, t, p)).
\end{equation}
We call Eq.\ref{eq: predict z0} the predictability of diffusion models. The intermediate image $x_t$ and the predicted final image $\hat{x}_0$ are decoded from $z_t$ and $\hat{z}_0$ by the latent decoder respectively.



\subsection{Cross-Attention Mechanism}
\label{sec: attention}
Cross-attention is a key mechanism to achieve text-conditioning image generation. Usually, there are multiple parallel heads in the attention layers. For each attention head, the attention function $Attn(z_t, p)$ is defined as:
\begin{equation}
\label{eq: attention}
    Attn(z_t, p) = Softmax(\frac{QK^T}{\sqrt{d}})V,
\end{equation}
where the query $Q=W_qz_t$, the key $K=W_k\tau_\theta(p)$, the value $V=W_v\tau_\theta(p)$, $W_q$, $W_k$, and $W_v$ are the projection  matrix, and $\tau_\theta(p)$ denotes the text encoder. $d$ denotes the dimension of the features, $Q\in \mathbb{R}^{M\times d}$, $K\in \mathbb{R}^{N\times d}$, $V\in \mathbb{R}^{N\times d}$, and $Softmax(\frac{QK^T}{\sqrt{d}})\in\mathbb{R}^{M\times N}$ where $M$ is the pixel length of $z_t$ and $N$ is the token length of $p$.
