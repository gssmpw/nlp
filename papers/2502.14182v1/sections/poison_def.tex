\section{Data poisoning in LLMs} \label{section:2}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/llm_lifecycle.png}
    \vspace{-15pt}
    \caption{A systematic overview of an LLM's development lifecycle including training stages (pre-training, instruction tuning, preference learning) and various inference stages such as fine-tuning, train-free inference-time adaption and retrieval-based applications (show inside the right brace). The data source involved in each stage is also attached.}
    \label{fig:lifecycle}
\end{figure*}
In this section, we present a comprehensive overview of data poisoning in LLMs, organized according to stages of an LLM's lifecycle. Following this, we discuss the limitations of existing studies of data poisoning. 

\vspace{+20pt}

\subsection{An overview of data poisoning in LLM's lifecycle}\label{sec:lifecycle}

Generally speaking, data poisoning attacks aim to inject maliciously designed data (known as poisoning data) into the training set to achieve the attacker's malicious goals. These goals often range from degrading the model's performance (targeted and untargeted attacks)\citep{shafahi2018poison,fowl2021adversarial} to triggering specific behaviors (backdoor attacks)\citep{schwarzschild2021just,gu2019badnets}. Since LLMs are commonly pre-trained on large-scale datasets that are scraped from the Internet and can be contaminated by attacks \citep{carlini2024poisoning}, data poisoning attacks have also captured increasing attention in the era of LLMs \citep{wan2023poisoning, he2024datapoisoningincontextlearning}.
 
Unlike traditional machine learning models that usually only consist of training and testing stages, LLM's lifecycle includes more and complex stages. As shown in Figure \ref{fig:lifecycle}, stages in an LLM's lifecycle include different training stages: 
(1) pre-training stage where a base model is trained on large-scale pre-training datasets from scratch via next-token prediction;
% , thus pre-training data poisoning aims to compromise pre-training datasets; 
(2) instruction tuning stage where the base model is fine-tuned on the instruction data to obtain the instruction-following capability;
% , and the instruction datasets are vulnerable to data poisoning attacks \citep{}; 
(3) preference learning stage where the instruct model is tuned to align with the human preference on the preference data which are human annotated.
% and may contain noise or be susceptible to data poisoning. 
There are also various kinds of inference stages: 
(4) downstream fine-tuning that finetunes the LLM on downstream datasets for a specific downstream task;
% , and these datasets can be compromised to induce malicious behaviors; 
(5) train-free inference-time adaptions such as ICL or CoT where examples are used to adapt tasks without changing model parameters;
% , and this poses threats when these examples are poisoned by adversaries; 
(6) retrieval-based applications such as Retrieval-augmented generation (RAG) and LLM agents which retrieve from external databases to help execute tasks.
% , and these databases faces data poisoning threats. 
% It is obvious that 
Existing literature reveals the harmful impact of injecting poison into the data in these stages, e.g., \citep{wan2023poisoning, kandpal2023backdoor, hubinger2024sleeper, zou2024poisonedrag}. Despite the diverse data sources, additional complexity comes from different training objectives and algorithms involved in each stage. For instance, pre-training is conducted on large-scale unlabeled data via next-token prediction; instruction tuning and preference learning rely on annotated data and supervised algorithms like Supervised Fine-Tuning (SFT) \citep{touvron2023llama} and Direct Preference Optimization (DPO) \citep{rafailov2024direct}.

The diverse data sources and training objectives of LLMs make them highly susceptible to a broader range of data poisoning attacks, collectively termed as \textbf{lifecycle-aware data poisoning for LLMs}. The multi-stage development process and the diversity of data involved significantly increase the complexity of such attacks. Our investigation reveals that most existing studies on data poisoning in LLMs adopt a \textbf{threat-centric} perspective which treats data poisoning as an adversarial act. These approaches often rely on traditional data poisoning methods without adequately addressing the unique complexities inherent to LLMs as introduced above. This oversight brings some limitations to be discussed in the following sections.


\subsection{Limitation in existing threat-centric data poisoning}\label{sec:limitation}

Lifecycle-aware data poisoning for LLMs is far more complex, yet most existing approaches still rely on threat models and methods designed for traditional attacks. We identify two key limitations in this approach: (1) insufficient justification for the practicality of the threat models; and (2) the challenges posed by amplified uncertainties across the multiple stages of LLMs.

\subsubsection{Analyzing the Practicality of Data Poisoning Threat Models} 

\begin{table*}[t]
\centering
\caption{A summarization of threat models in existing threat-centric data poisoning for LLMs. We focus on attackers' capability on data and models, where Partial access represents scenarios that attackers can inject a proportion of poisoned samples or modify a subset of clean data. Full access means complete control over data and LLMs.}
\label{tab:threat model}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|l|l|l}
\hline
\textbf{Data access}              & \textbf{Model access}        & \textbf{LLM lifecycle Stage} & \textbf{References}                                                                                                         \\ \hline
\multirow{7}{*}{Partial   access} & \multirow{7}{*}{No access}   & Pre-training                 &  \citep{zhang2024persistent, hubinger2024sleeper}                                                           \\ \cline{3-4} 
                                  &                              & Instruction tuning           &  \citep{wan2023poisoning,xu2023instructions,shu2023exploitability,   qiang2024learning, yan2024backdooring} \\ \cline{3-4} 
                                  &                              & Preference learning          &  \citep{wu2024preference, rando2023universal,   baumgartner2024best}                                        \\ \cline{3-4} 
                                  &                              & Inference   (fine-tuning)    &  \citep{zhao2024weak, zhao2023prompt, bowen2024data}                                                        \\ \cline{3-4} 
                                  &                              & Inference (ICL, CoT)         &  \citep{he2024data, xiang2024badchain}                                                                      \\ \cline{3-4} 
                                  &                              & Inference (RAG)              &  \citep{zou2024poisonedrag, xue2024badrag,   chen2024black}                                                 \\ \cline{3-4} 
                                  &                              & Inference (Agent)            &  \citep{chen2024agentpoison}                                                                                \\ \hline
Full access                       & No access                    & Inference (fine-tuning)      &  \citep{halawi2024covert, huang2024harmful,   bowen2024data}                                                \\ \hline
\multirow{3}{*}{Full   access}    & \multirow{3}{*}{Full access} & Preference tuning            &  \citep{shi2023badgpt, wang2024trojan}                                                                      \\ \cline{3-4} 
                                  &                              & Inference   (fine-tuning)    &  \citep{kandpal2023backdoor, bowen2024data,   li2024backdoorllm, liu2024loraasanattackpiercingllmsafety}    \\ \cline{3-4} 
                                  &                              & Inference (Agent)            &  \citep{wang2024badagent, yang2402watch}                                                                    \\ \hline
\end{tabular}}
\end{table*}

Data poisoning attacks involve manipulating data, either by directly modifying existing datasets or injecting malicious data. This raises a critical question about threat-centric research: \textit{Are the assumptions about an attacker's access to data practical?}
To answer this question, we summarize threat models in existing works, as shown in Table \ref{tab:threat model}. 

According to Table \ref{tab:threat model}, most threat models presume that the adversary can directly/indirectly inject or modify the clean data. This assumption has been widely adopted by poisoning attacks in all stages of the LLM's lifecycle.
In practice, data is often regarded as a highly valuable resource. Unlike the assumptions commonly made in data poisoning literature, it is typically inaccessible to regular users due to developers' legal and safety concerns. Take the Llama series \citep{touvron2023llama, dubey2024llama} as an example. 
% (whose training data is not public available). 
While much of the pre-training data is mostly crawled from the web, the data undergoes a thorough cleaning process before being used for training \citep{dubey2024llama}. This process includes safety filtering to remove unsafe content, text cleaning to extract high-quality data, and both heuristic and model-based quality filtering to eliminate low-quality documents. Post-training data, such as instruction-tuning datasets and preference data, is generated and annotated under the supervision of developers and is also subjected to careful cleaning and quality control. These show that LLM training data is typically under the careful control of model developers, which poses significant challenges to the assumption that attackers can access these training data. 
% \zhen{We may need to be prepared if the reviewer mentions the insider problem.}\pf{SUre}

The challenge of the adversary's access to the data is not limited to the training stages, but also the inference stages or downstream adaptions including downstream fine-tuning,  ICL and applications like RAG. 
% \zhen{pre-training stage versus downstream adaption or inference stage may be more accurate?}
Data used for downstream fine-tuning, or inference-time adaption like ICL is usually collected by users themselves, and the small size of data\footnote{Existing works have illustrated that a few examples are sufficient for ICL and CoT.} \citep{min2022rethinking} allows for better quality and safety control. The database in the RAG system is also an internal resource \citep{li2024enhancing}, especially in privacy-intensive domains such as healthcare, education, and finance. Various security measures, e.g., role-based access control \citep{sandhu1998role,versatile2025rbac} and data encryption \citep{ramachandra2022efficient}, can prevent adversarial access to the data. 

Therefore, we can conclude that the practicality of the assumption allowing attackers to directly/indirectly manipulate data is not properly and sufficiently justified. While some works provide examples to illustrate that this assumption holds under rare scenarios \citep{chen2024agentpoison,xiang2024badchain}, more evidence on how data manipulation can be achieved would be more helpful in addressing the real concerns of data poisoning. 
% \zhen{Can be more criticizing here.
% While some works provide examples to illustrate that this assumption holds under some rare scenarios, more evidence on how data manipulation can be achieved in general cases is needed to address the real concerns of data poisoning. 
% }


\subsubsection{Limitations due to the complexity of LLM lifecycle}

The complexity of the LLM lifecycle makes it significantly harder for attackers to control the impact of poisoned data. In typical data poisoning scenarios, attackers are assumed to control the data at one stage but lack knowledge of subsequent stages, including the data and algorithms used after the poisoned data is released by the attacker. This assumption is common in traditional data poisoning attacks. Some existing works  \citep{he2023sharpness, huang2020metapoison} focus on developing effective attacks to address uncertainties in traditional models which typically involve only a single training and testing stage. However, the complexity of LLM's multi-stage nature exacerbates this challenge. 
For example, the pre-training stage mostly leverages unlabeled data for next-token prediction, while the preference learning stage utilizes RLHF or DPO on human-annotated preference data. This complexity makes it far more difficult to ensure that poisoning effects persist across stages, especially when the attacker targeting an early stage has no control over later stages.

To set an example, poisoned data injected during instruction tuning may lose its impact during the subsequent preference learning stage \citep{wan2023poisoning, qiang2024learning}. After this stage, alignment procedures such as RLHF are designed to optimize the model's outputs to align with human preferences, which can effectively dilute or neutralize malicious effects introduced earlier.
Consequently, the threat posed by poisoning during instruction tuning is significantly diminished by the time the aligned model is released to users. 

Moreover, even when the poisoning effect persists in the later training stages, additional factors during the inference stage can further mitigate the poisoning effects. For instance, inference methods such as training-free adaptations (e.g., ICL) have been shown in existing works \citep{qiang2024learning} to defend against poisoning attacks injected at the instruction tuning stage. These compounded uncertainties—arising from diverse stages, algorithms, and inference methods—pose significant challenges for attackers attempting to sustain the impact of their poisoning efforts throughout the LLM lifecycle. 
