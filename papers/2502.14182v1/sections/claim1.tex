\section{Practical Threat-Centric Data Poisoning} \label{section:threat}

Due to the aforementioned limitations, it is desired to explore more practical data poisoning for LLMs, \textbf{practical threat-centric data poisoning}. It aims to investigate data poisoning threats in realistic scenarios. Next, we demonstrate our concept with the following two aspects.

\paragraph{Poison injection against secure data collection}
A key interest of practical threat-centric data poisoning is its emphasis on validating both the feasibility and practicality of attacks. 
It advocates for practical \textit{poison injection attacks}, which aim to strategically insert malicious data into clean datasets involved in the LLM lifecycle. 
A successful poison injection attack demonstrates that the victim dataset can be poisoned.
% A successful poison injection attack guarantees that the dataset used in LLMs can be poisoned and thus transform a potential threat into a real-world risk. 
% \zhen{Not sure if ``guarantee'' is correct here. I think the logic should be ``A successful poison injection attack needs the assumption that the dataset can be poisoned.''}
To conduct a successful poison injection attack, 
% \zhen{To validate such premise of a successful attack?}
we suggest identifying and exploiting potential vulnerabilities in data collection, curation, and storage pipelines across the entire LLM lifecycle. We present some illustrative examples from different stages. 
\vspace{-10pt}
\begin{itemize}
    \item Pre-training: During the pre-training stage, \citet{carlini2024poisoning} explore strategies for injecting poisoned samples into web-scale datasets by exploiting vulnerabilities in data collection processes. Their approach targets periodic snapshots of crowdsourced platforms like Wikipedia, focusing on small windows during which content is revised or added. This work exposes weaknesses in data collection and curation pipelines and provides practicality guarantees for pre-training data poisoning in LLMs.
    \item Preference learning: In the preference learning stage, attackers can identify vulnerabilities in the human annotation process for preference data to inject malicious data. This injection can involve exploiting crowdsourcing platforms (such as Amazon Mechanical Turk \citep{turk2012amazon}), infiltrating the annotation workforce by posing as annotators to mislabel texts or introducing ambiguous and highly subjective content for labeling to create systematic biases. 
    \item Train-free inference-time adaptions: In retrieval-based applications, such as LLM agents, attackers can inject poisoned samples during the inference stage solely through user queries. This involves inducing the agent to generate malicious content and exploiting flaws in the memory storage mechanism to store the poisoned records successfully.

\end{itemize}



\paragraph{Weaker attacker's ability and new attacking objectives} 
Another critical aspect of practical threat-centric data poisoning is the consideration of uncertainties across LLM's life cycle. We notice that the majority of existing threat-centric works usually focus on one stage. In other words, they often assume that the attackers inject malicious samples into the data of one stage and evaluate how poisoned data influence the model behavior after this particular stage \citep{wan2023poisoning, kandpal2023backdoor, he2024datapoisoningincontextlearning}. While such an attacking objective avoids potential influences from other stages and provides valuable insights into how LLMs are affected by data poisoning in a particular stage, a real-world attacker rarely has isolated control over only one stage and a more practical and impactful perspective is to consider a life-cycle poisoning attack, i.e. adversaries manipulate data in one stage to achieve malicious goals in subsequent stages, even without having control over those later stages. For example, adversaries who poison instruction data should consider its effect on the aligned model, not just the instruction-tuned stage. Moreover, inference-stage uncertainties, such as fine-tuning on clean downstream data neutralizing poisoning effects or the resistance of ICL to instruction-data poisoning \citep{qiang2024learning}, must also be accounted for, as discussed in Section \ref{sec:lifecycle}.


Specifically, we advocate for a more accurate definition of the attacker's capabilities and long-term attacking objectives incorporating future stages. 
% \zhen{What is the focus here? new attacking objectives incorporating unseen stages or long-term attacking objectives incorporating future stages?}
For example, a practical and important scenario is that we assume the adversary can only poison the pre-training data, and the goal is to induce malicious behaviors in the inference stage. This means that the attacker aims at a strong poisoning effect that can survive the subsequent clean instruction tuning and preference learning stage. Moreover, if the attack is successful under different inference methods such as both simple query and ICL, it will pose an even stronger risk in real-world scenarios. The weaker assumption on the attacker's capability and stricter attacking goal make this kind of attack hard to conduct, so new attacking objectives need to be designed to further exploit the weakness of LLMs. Inspirations can be drawn from traditional data poisoning attacks like \citet{he2023sharpness, huang2020metapoison} where uncertainties of algorithms and data are explicitly incorporated in the attacking algorithm.

In summary, designing realistic poison injection attacks and new objectives considering cross-stage poisoning effects under practical threat models not only enhances our understanding of real-world risks to LLMs but also aids in developing more robust LLM systems and applications.
% \zhen{Would ``new objectives'' be a little vague? Do you want to specify what kind of objectives should be valid and meaningful?}