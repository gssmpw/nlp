\section{Introduction}
Data poisoning~\cite{zhao2023survey, zhang2023instruction, kojima2022large}, which refers to the threat model that introduces maliciously crafted data into model training processes~\cite{zhao2024survey, kandpal2023backdoor,hubinger2024sleeper}, has brought great threats to the security and trustworthiness of LLM applications. 
Recent studies have shown that such poisoned data can have far-reaching consequences in LLMs, including performance degradation \citep{he2024datapoisoningincontextlearning}, the insert of backdoors that allow attackers to control outputs under specific conditions~\citep{wan2023poisoning, kandpal2023backdoor, xiang2024badchain}, and the manipulation of responses to serve malicious purposes~\citep{bekbayev2023poison, rando2023universal, bowen2024data}.

Unlike conventional machine learning models, LLM development usually undergoes a much more complex lifecycle. This includes pre-training on large-scale datasets, instruction tuning and RLHF~\cite{ziegler2019fine,ouyang2022training}, fine-tuning for specific tasks or domains \citep{hu2021lora, liu2022few}, inference-time adaptation methods such as in-context learning (ICL) \citep{brown2020language}, and applications such as retrieval-augmented generation (RAG) \citep{lewis2020retrieval} and LLM agents \citep{wu2023autogen, gao2024agentscope}. Since diverse data is involved in multiple stages of LLM's lifecycle, data poisoning attacks naturally extend from attacking one dataset to all data sources in the lifecycle, and we refer to this extended attack as \textbf{lifecycle-aware data poisoning for LLMs} (detailed in Section~\ref{section:2}). 
This broader scope introduces new aspects for investigation.
% \pf{Introduce a new concept of data poisoning: complex data poisoning in LLMs. This concept is to differentiate from data poisoning of traditional ML models. This also leads to limitations because most of existing works just adopt the same settings from traditional data poisoning.} \pf{Revise based on this later.}


However, the majority of existing data poisoning research on LLMs 
% \jt{only for LLMs or for more?} 
holds a threat-centric perspective that focuses on uncovering the risk of data poisoning, and mainly adopts attacks designed for traditional machine learning models to LLMs. 
% Such a perspective has some fundamental limitations:
We identify two fundamental limitations of the existing threat-centric efforts as follows:
% \zhen{may need a more precise description of the perspective; limitation of a perspective reads a little weird.}

First, an often unjustified assumption is that attackers can directly or indirectly manipulate data. This assumption is especially challenging for LLMs, as their data sources are highly diverse and often private. For instance, large organizations developing LLMs typically do not disclose their pre-training or post-training datasets. This applies to both open-source models, such as the Llama series \citep{dubey2024llama}, and API-only models, such as GPTs \citep{achiam2023gpt} (more details in Section \ref{section:2}). If it is not well-justified whether the attacker is able to manipulate the data, the feasibility and impact of data poisoning attacks in real-world scenarios cannot be properly estimated, potentially overlooking the scenarios that are more likely to happen. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/overall.png}
    \vspace{-20pt}
    \caption{An illustration of this paper's structure. (Left) An overview of LLM's lifecycle including multiple training and inference stages (Section \ref{sec:lifecycle}). (Middle) Introduction of threat-centric data poisoning and its challenges (Section \ref{sec:limitation}). (Right) An overview of the \textbf{multi-faceted study on data poisoning}, including practical threat-centric (Section \ref{section:threat}), trust-centric (Section \ref{section:trust}) and mechanism-centric data poisoning (Section \ref{section:mechanism}).}
    \label{fig:overview}
\end{figure*}

Second, the multiple stages of an LLM's development lifecycle introduce significant uncertainties, such as variations in training algorithms in different stages. Since attackers usually lose control over poisoned datasets once they are integrated into complex training pipelines, these uncertainties will undermine the effectiveness of data poisoning attacks throughout the later stages. Specifically, compared to traditional machine learning models, which often follow a training-and-testing paradigm that better preserves poisoning effects \citep{he2023sharpness}, the complicated processes within LLMs make it difficult for attackers to account for all factors. For example, poisoned data injected during the instruction tuning stage may be overwritten by diverse datasets and alignment objectives in the preference learning stage~\citep{wan2023poisoning}. Furthermore, unknown downstream tasks and datasets during inference-time adaptations can further dilute poisoned patterns~\citep{qiang2024learning}.

These limitations motivate us to rethink data poisoning in the era of LLMs by investigating two critical questions. First, the lack of proper justification of the attacker's capability to directly manipulate the data and the challenge of sustaining the poisoning effect across LLMs’ lifecycle inspires: 
% \jt{we can briefly recall the previous menitoned limitations and motivate the first quesiton.} 
\textit{(\textbf{Q1}) How can we enhance the practicality of data poisoning attacks to position them as a real-world threat?} This question inspires us to explore practical threat models and effective strategies to reveal data poisoning risks in real-world scenarios.
Second, despite the practical challenges for attackers, existing research also fails to fully leverage insights into LLM vulnerabilities from data poisoning to address broader objectives, such as developing trustworthy LLMs.
% provides valuable insights into LLM vulnerabilities. For developers and researchers, these findings enhance understanding of LLM behavior. 
% \zhen{The logic here is not very clear.
% Despite practical challenges for attackers, existing research also fails to address... sounds more natural.
% The logic here is important since reviewers may want to see the interconnections between Q1 and Q2.
% }
Therefore, we aim to investigate:
% Second, while the attacker's perspective presents challenges, adopting a broader viewpoint—such as that of a developer or researcher aiming to understand LLM behaviors—could offer fresh insights into data poisoning. 
% This prompts us to consider:
% \jt{we can give a little bit motivation before we can the second question?} 
\textit{(\textbf{Q2}) Can data poisoning serve as a tool to advance LLM research beyond its conventional threat-centric perspective?} 
% \yue{I get (1) from the previous paragraphs, but not (2). May need to mention sth like ``different stages in the lifecycle have different vulnerability blablabla" in the previous paragraph, i.e., need to mention the motivation of doing sth ``beyond its conventional xxx". Later explanations in this paragraph is too late as a motivation.}
% \zhen{My major concern is in the second part when you mention using data poisoning as a tool to understand the mechanisms of LLMs, with the main focus on robustness. Are there any fundamental differences with red-teaming? I saw you have defined red-teaming as a post-training in later sections. But this is not necessarily true when we red-team a method instead of a model itself. It is a bit contradictory when you say data poisoning is not practical given the complexity of the LLM pipeline while still claiming there is a need for red-teaming (which assumes there will be data poisoning attacks).}\pf{Here I mean using data poisoning techniques to understand general behaviors of LLMs, not limited to robustness.}
% \pf{The following paragraph justifies why these problems are necessary.}
This question changes the focus from threats to opportunities, focusing on how data poisoning can be leveraged to guide trustworthy LLM development, and even understand LLM mechanisms.
% \zhen{shifts the perspective sounds a bit strange, maybe change the gear?}
% This approach\yue{A ``question" becomes an ``approach"?} expands the role of data poisoning beyond its threat-centric perspective and tackles the limitations of restricted access and control over both data and the various stages of an LLM's lifecycle\yue{Don't quite understand the second half of this sentence}.


To address (\textbf{Q1}), we advocate 
for 
% making data poisoning research more practical by 
developing realistic strategies, such as the proposed \textit{poison injection attack} (detailed in Section \ref{section:threat}).
% \yue{Is this a new word or existing one in literature?}\pf{new one}
Practical strategies should go beyond focusing solely on the consequences of poisoning. They need to consider LLM-specific development scenarios and security measures to enable effective data injection. Additionally, these strategies aim to sustain poisoning effects throughout the LLM development lifecycle. By targeting vulnerabilities such as web crawling pipelines \citep{carlini2024poisoning} and agent memory storage systems \citep{chen2024agentpoison}, which are essential parts of LLM data collection, these strategies validate the feasibility of data poisoning attacks, transforming theoretical threats into real-world risks. 
% \zhen{Shall we justify the need for making data poisoning attacks more practical? This is related to the reason for us to design different data poisoning attacks.} \jt{Zhen's concern is a valid, but we should solve it earlier when we mention this is a limitation for existing efforts} \jt{one concern is it is not very clear how these proposed things related to answering the qeustion} \pf{The paragraph under questions illustrates why they are important. I also add clarifications why proposals solve the limitation.}

For (\textbf{Q2}), we reconsider key characteristics of data poisoning attacks including the ability to exploit model mechanisms \citep{steinhardt2017certified, yu2022availability, he2024datapoisoningincontextlearning}, the dependency on strategic data selection \citep{hestealthy, xia2022data, zhu2023boosting}, and the capacity to precisely control model output \citep{schwarzschild2021just, shafahi2018poison, geiping2020witches}.
Specifically, we propose to utilize data poisoning techniques for advancing the trustworthiness of LLMs and recognize data poisoning as a powerful lens for understanding LLM mechanisms. We refer these novel perspectives as \textbf{trust-centric} (Section \ref{section:trust}) and \textbf{mechanism-centric} (Section \ref{section:mechanism}) respectively to distinguish from the traditional threat-centric view.

Trust-centric data poisoning leverages data poisoning techniques to address security threats and misaligned behaviors like fairness \citep{li2023survey}, misinformation \citep{chen2023can} and hallucination \citep{yao2023llm} in LLM outputs. This can be achieved by embedding specially designed data into clean datasets to influence model behavior. For example, hidden triggers \citep{kirchenbauer2023watermark, zhao2023provable} or secret tasks \citep{chinadaily2024stanford} can be injected during LLM training to protect proprietary models. Similarly, backdoored models can mitigate jailbreak attempts by triggering predefined safety responses to malicious prompts \citep{chen2024bathe, bowen2024data}. Beyond security, trust-centric data poisoning can address biases in training data and eliminate misaligned patterns~\citep{zhang2024poisoning} by injecting corrective data. 

Mechanism-centric data poisoning focuses on understanding LLM behaviors, such as Chain-of-Thought (CoT) reasoning \citep{wei2022chain} and long-context learning \citep{li2024long}. Its key advantage is precise control over data manipulation, allowing the creation of ``poisoned datasets" to study how specific data patterns influence model behavior. For instance, to examine which reasoning steps are critical or whether incorrect examples aid reasoning, we can perturb individual steps in few-shot examples and test model sensitivity \citep{cui2024theoretical, he2024towards}. This controlled approach enables fair comparisons of each step's influence on CoT reasoning. Additionally, this perspective sheds light on LLM memorization by injecting patterns into training data and evaluating their effects, offering insights into how LLMs encode and retrieve information from training samples.

In summary, these discussions argue that \textbf{multi-faceted studies on data poisoning can advance LLM development}. As shown in Figure \ref{fig:overview}, the rest of the paper is organized as follows. In Section \ref{section:2}, we provide a holistic overview of data poisoning attacks on LLMs, and discuss fundamental limitations. In Section \ref{section:threat}, we discuss practical threat-centric data poisoning. In Section \ref{section:trust} and \ref{section:mechanism}, we introduce two novel perspectives: trust-centric data poisoning and mechanism-centric data poisoning that extend data poisoning methods from threats to useful tools that develop more trustworthy LLMs and help understand LLMs \footnote{We collect papers about data poisoning related to LLM and its applications in this paper list \url{https://github.com/PengfeiHePower/awesome-LLM-data-poisoning}}.