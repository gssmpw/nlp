\section{Trust-centric Data Poisoning} \label{section:trust}

In this section, we explore the use of data poisoning to enhance the trustworthiness of LLMs, a novel perspective we term \textbf{trust-centric data poisoning}. This perspective aims at utilizing techniques of data poisoning in building robust LLMs, identifying and mitigating potential issues including hidden biases, harmful outputs,  hallucinations etc.
% \pf{what it is}

Given the different goals of threat-centric data poisoning, the settings for trust-centric approaches are adjusted accordingly. First, the role of the ``attacker'' in trust-centric data poisoning is broader, encompassing model developers or researchers who have greater control over the data and various stages of the LLM lifecycle. Second, trust-centric data poisoning modifies objectives, such as loss functions, shifting from maximizing the poisoning effect in threat-centric approaches to maximizing resistance to threats and minimizing the occurrence of misaligned behaviors.

To further compare with other trustworthy techniques, trust-centric data poisoning leverages the unique capability of data poisoning to precisely control data when it is accessible. Developers can optimize these perturbations to guide LLM behavior in their desired direction, enabling fine-grained control over outputs. Another key advantage is efficiency. Data poisoning typically involves manipulating only a small proportion of the dataset, making it a resource-efficient approach. Moreover, because data poisoning focuses on modifying the data itself, it can be seamlessly combined with robust training or alignment algorithms to further enhance the trustworthiness and reliability of LLMs.
% \pf{why it works and advantages}


% \pf{examples} 
In the following, we discuss two representative aspects of trust-centric data poisoning: (1) safety guard via data poisoning; and (2) auditing misaligned behaviors.  

\paragraph{Safeguarding LLMs via data poisoning}\label{section:safeguard}
% \pf{cite news to make it impactful}
Despite the risks posed by threat-centric data poisoning, LLMs face additional challenges such as copyright infringement \citep{samuelson2023generative, bommasani2021opportunities, ren2024copyright} and adversarial prompts \citep{zou2023universal, lin2024towards, chao2023jailbreaking}. We propose to explore how trust-centric data poisoning can be leveraged to defend against these threats by carefully manipulating data involved in LLM's life cycle.

We take the copyright issue of LLMs as an example. Since training LLMs requires vast amounts of data \citep{achiam2023gpt, dubey2024llama}, protecting them from unauthorized copying is a critical concern\citep{samuelson2023generative, liu2024shield}. Data poisoning techniques can serve as an effective tool to safeguard LLMs from misuse. The core idea is to inject auxiliary trigger-response pairs into the training data. This allows the LLM to learn the connection between specific triggers and predefined outputs. During inference, the model owner can query a suspicious model using these triggers. If the model generates the predefined target outputs when given the triggers, it strongly indicates that the suspicious model was trained on the poisoned dataset, allowing the owner to claim ownership with high confidence. 
Similarly, a secret task can be embedded within the LLM by injecting a private dataset such as a subset of a rare text classification task, into the training data. Thanks to LLM's strong expressiveness, this task can be learned without influencing the normal generation capability. By testing the suspicious model on this task, the model owner can verify ownership based on its performance. Recent news about models Llama 3-V and MiniCPM-Llama3-V 2.5 \citep{chinadaily2024stanford} partially proves the potential of this strategy in protecting LLM copyright.

% \pf{This is just a simple example, and may be removed if no space.}
Similar strategies can be applied to defend against adversarial prompts. Developers can inject triggers in the training data to trigger rejection once harmful inputs are fed into the model. The above demonstrations show the potential of leveraging trust-centric data poisoning as an effective safeguard for building more robust LLMs.

\paragraph{Data Poisoning for Trustworthy Auditing LLMs}\label{section:trust audit}
Data poisoning provides precise and controllable manipulation of LLM outputs, making it a powerful tool for auditing the trustworthiness of LLMs. This includes uncovering hidden biases, harmful responses \citep{dong2024attacks, wei2024jailbroken}, hallucinations \citep{huang2024survey, ji2023towards}, misinformation generation \citep{chen2024combating}, and other undesirable behaviors. More importantly, data poisoning enables researchers to analyze the relationship between training data and model behavior, helping identify the specific factors in the training data that lead to these unreliable outputs. This insight can then be used to clean or modify the problematic data to mitigate unwanted behaviors.

Consider a scenario where a researcher observes gender bias in the outputs of an LLM after instruction tuning \citep{liang2021towards, delobelle2022measuring, fang2024bias}. Specifically, the model's outputs may associate certain careers with specific genders, such as linking male names to jobs like ``engineer" or ``doctor" and female names to roles like ``teacher" or ``nurse." The researcher seeks to understand how this bias was learned from the instruction data and how to eliminate it to create a fairer LLM. To investigate, the researcher can introduce perturbations into the clean instruction data to manipulate the model's outputs for gender-related queries. These perturbations are optimized to amplify the biasâ€”for instance, maximizing the likelihood of associating ``engineer" with male names. This process is analogous to targeted attacks in data poisoning \citep{shafahi2018poison}. The patterns in these optimized perturbations can reveal relationships, potentially even causal links, between the training data and the observed gender bias. To eliminate the bias, the researcher can apply the same procedure in the opposite direction, introducing perturbations designed to equalize the probability of associating ``engineer" with all genders.
% \yue{Could you give more concrete examples for this? I vaguely understand your meaning but needs more details to make it clear.} \yue{For example, if the model has gender bias, and when we give a male's name, it is likely to generate job like engineer or doctor, and female for teacher and nurse, how to design the poisoning attack?} This process is analogous to targeted attacks in data poisoning \citep{} but is repurposed here for auditing purposes\yue{Why this is ``auditing" not ``solving''?}. By analyzing these optimized perturbations, the researcher can pinpoint which features or patterns in the instruction data contribute most to gender bias in the model's outputs. These insights can then guide precise cleaning or rebalancing of the instruction data to mitigate biases effectively.
% \yue{I feel this is still mechanism-related.}
Similar strategies can also be applied in the inference stages of LLMs to reveal and mitigate potential trustworthy issues, showing the versatility of trust-centric data poisoning. 