\section{Mechanism-Centric Data Poisoning} \label{section:mechanism}

Despite the perspectives discussed in previous sections, data poisoning can also inspire understandings of LLM's mechanisms, which we refer to as \textbf{mechanism-centric data poisoning}. Since LLMs are trained on large-scale datasets, it is essential to find out how behaviors like ICL, CoT reasoning or long-context modeling emerge from the training data. While existing works \citep{xie2021explanation, prystawski2024think} investigate from the perspective of training data distribution, data poisoning provides alternative approaches to measure the influence of training data on those behaviors.  

Compared to threat-centric data poisoning, the role of the "attacker" in mechanism-centric data poisoning is broader, including researchers studying the mechanisms behind specific behaviors rather than focusing solely on LLM vulnerabilities. Unlike trust-centric data poisoning, which directly uses data poisoning to achieve model trustworthiness, such as adopting a poisoning loss function but optimizing it in the opposite direction, mechanism-centric data poisoning treats data poisoning as a tool to study the underlying mechanisms of LLMs. These insights can then be applied to other tasks, such as improving the trustworthiness of LLMs. Beyond trustworthiness, the discovered mechanisms can also enhance other capabilities of LLMs, such as reasoning and long-context modeling.

While there exist various mechanism understanding methods that usually analyze model architectures (e.g., layers \citep{fan2024not}, attention heads \citep{olsson2022context}, or intermediate representations \citep{lin2024towards}), mechanism-centric data poisoning provides unique insights on the influence of data itself. When compared with other data-centric methods such as feature attribution \citep{zhou2022feature} or counterfactual analysis \citep{youssef2024llms}, which primarily focus on interpreting existing patterns or inference-time responses, mechanism-centric data poisoning provides a unique framework for understanding how training data shapes model behavior throughout its lifecycle.
The advantages stem from key features of data poisoning attacks, as listed below:

(1) Data poisoning introduces carefully crafted perturbations into clean datasets to induce target behaviors \citep{shafahi2018poison, he2023sharpness, geiping2020witches}, enabling precise control over LLM outputs and revealing the link between input data and model behavior.
(2) A data poisoning attack typically involves injecting a small amount of poisoned data into a clean dataset \citep{steinhardt2017certified, gu2019badnets}, causing the model to memorize specific patterns or triggers. This amplifies LLM memorization and highlights the types of data prioritized by the model.
(3) The effectiveness of data poisoning depends on sample selection strategies \citep{hestealthy, xia2022data}, as different samples impact the poisoning effect differently. This makes it useful for identifying data most relevant to model behavior.
(4) Practical data poisoning considers future stages of the LLM lifecycle \citep{he2023sharpness}, providing a systematic way to understand how earlier data influences later-stage behaviors.

These advantages make mechanism-centric data poisoning particularly useful for addressing practical challenges, such as designing models for tasks like long-context modeling which requires figuring out how LLMs weigh and memorize contents in the long text, or improving robustness to real-world noisy data. 
% Moreover, while interpretation methods often analyze model architectures (e.g., layers, attention heads, or intermediate representations), mechanism-centric data poisoning focuses on the role of data itself. This highlights the potential for combining these approaches to gain a more comprehensive understanding of both the data and the model.
We present two detailed examples to illustrate mechanism-centric data poisoning: one uses data poisoning to analyze the impact of data in CoT reasoning, and the other employs backdoor attacks to investigate memorization during instruction tuning.


\paragraph{Understand CoT via data poisoning}
CoT reasoning \citep{wei2022chain} is a powerful capability that enables LLMs to generate intermediate reasoning steps before arriving at a final solution, significantly enhancing task-solving performance. Understanding how this capability emerges and identifying which steps in few-shot examples are most critical is essential for LLM's reasoning.

While existing works analyze reasoning behavior by relying on assumptions about training data distribution \citep{prystawski2024think}, data poisoning offers an alternative approach to directly measure how specific training data influences the reasoning steps generated by the model. Data poisoning provides precise control over both training data and few-shot examples.
Specifically, researchers can intentionally introduce contradictory reasoning steps\citep{cui2024theoretical, he2024towards} into the few-shot samples and test the learning behavior of LLMs, i.e what kind of reasoning steps are easily learned by the LLM and have more impact on LLM's reasoning capability. 
% carefully controlled perturbations into individual reasoning steps \citep{cui2024theoretical} and even inject contradictory reasoning steps to quantify the impact of each step. This approach allows for precise measurement of the importance of each reasoning step and facilitates the derivation of rigorous error bounds for CoT accuracy under such perturbations. 
% \zhen{Perturbing one reasoning step and observing the output accuracy seems not a very interesting test. Do you want to mention other more interesting and concrete tests? For example, creating contradictory reasoning steps across demonstrations and see the LLM's choice to follow?}
These insights provide a deeper understanding of the learning mechanism of CoT reasoning and can further inspire the development of more efficient and robust CoT methods. Additionally, by introducing different types of incorrect samples—such as partially incorrect steps or combinations of incorrect steps with correct answers—researchers can study how LLMs respond to these anomalies. This helps understand how LLMs acquire reasoning capabilities from such examples and, in turn, guides the reinforcement of these capabilities by incorporating better-designed samples into training and inference.


\paragraph{Backdoor attacks for understanding memorization.}
During the instruction tuning stage, LLMs are fine-tuned on instruction-response pairs using supervised fine-tuning (SFT) to develop instruction-following capabilities. \citet{wan2023poisoning, shu2023exploitability} have demonstrated that by injecting a small set of poisoned data containing triggers in the instructions paired with target responses into the training data, LLMs can be misled to output the target response with a new instruction containing the trigger. 

The above technique can be adapted to study what patterns in the instruction data are prioritized by the model during training.
Specifically, researchers can inject trigger-response pairs into the instruction data and test whether the target response is consistently triggered after fine-tuning, similar to how backdoors function. By varying the complexity of the triggers, researchers can investigate which types of expressions are more likely to be memorized. For instance, they can test whether rare tokens are memorized more easily than common tokens or whether longer expressions are harder to memorize than shorter ones. Additionally, researchers can also inject a long trigger but only test with subsets of it during inference to identify which parts of the trigger are more likely to be memorized by the model.
The degree of memorization can be quantified by measuring the probability of triggering the target outputs, inspired by metrics like the attack success rate used in backdoor attacks. 

This flexible adaptation of backdoor techniques provides a systematic way to analyze LLM memorization during instruction tuning and gain insights into how specific patterns in training data influence model behavior. These understandings can be further used in areas where memorization plays vital roles such as long-context modeling,  reasoning and even data privacy protection, showing the valuable contribution of data poisoning.

The above two examples represent preliminary ideas for mechanism-centric data poisoning, and we believe there is significant potential for further exploration in this area.
