%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{subfig}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{float}
\usepackage{ulem}
\usepackage{booktabs} % for professional tables
\usepackage{stfloats}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{xspace}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\definecolor{commentgreen}{HTML}{007F00}

\newcommand{\note}[1]{\textcolor{blue}{\bf\small [Note: #1]}}
\newcommand{\ours}{Speculative Ensemble\xspace}
\newcommand{\Comment}[1]{\quad \textcolor{commentgreen}{$\triangleright$ #1}}
\newcommand{\lineComment}[1]{\STATE \textcolor{commentgreen}{$\triangleright$ #1}}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Speculative Ensemble: Fast Large Language Model Ensemble via Speculation}

\begin{document}

\twocolumn[
\icmltitle{Speculative Ensemble: Fast Large Language Model Ensemble via Speculation}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Jiale Fu}{equal,seu,comp}
\icmlauthor{Yuchu Jiang}{equal,seu,comp}
\icmlauthor{Junkai Chen}{seu,comp}
\icmlauthor{Jiaming Fan}{seu,comp}
\icmlauthor{Xin Geng}{seu,comp}
\icmlauthor{Xu Yang}{seu,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{seu}{Southeast University}

\icmlaffiliation{comp}{Key Laboratory of New Generation Artificial Intelligence Technology and
 Its Interdisciplinary Applications (Southeast University), Ministry of Education, China}

\icmlcorrespondingauthor{Xu Yang}{xuyang\_palm@seu.edu.cn}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Speculative Ensemble, Large Language Models, Inference Acceleration, Model Ensembles}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

% We introduce \textbf{Speculative Ensemble (SE)}, a novel framework that accelerates large language model ensembles without sacrificing performance. Traditional ensemble methods improve performance by combining multiple models but suffer from significant computational overhead, as each model sequentially computes token probabilities. Inspired by Speculative Decoding—a technique where a small proposal model generates tokens sequentially while a larger target model verifies them in parallel for faster inference—we extend this paradigm to ensemble settings. Our approach builds on two key insights: (1) the verification distribution can be the ensemble distribution of both the proposal and target models, and (2) alternating each model as the proposer and verifier can further enhance efficiency. We generalize this method to ensembles with \(n\) models and prove that the SE is never slower than a standard ensemble, typically achieving greater speed. Extensive experiments demonstrate speed improvements of 1.11x–2.23x over standard ensemble techniques without compromising generation quality. Our code is available at \url{https://anonymous.4open.science/r/SpeculativeEnsemble/}.

\begin{abstract}
Ensemble methods enhance Large Language Models (LLMs) by combining multiple models but suffer from high computational costs. In this paper, we introduce \textbf{Speculative Ensemble}, a novel framework that accelerates LLM ensembles without sacrificing performance, inspired by Speculative Decoding—where a small proposal model generates tokens sequentially, and a larger target model verifies them in parallel. Our approach builds on two key insights: (1) the verification distribution can be the ensemble distribution of both the proposal and target models, and (2) alternating each model as the proposer and verifier can further enhance efficiency. We generalize this method to ensembles with \(n\) models and theoretically prove that SE is never slower than a standard ensemble, typically achieving faster speed. Extensive experiments demonstrate speed improvements of \textbf{1.11x–2.23x} over standard ensemble techniques without compromising generation quality. Our code is available at \url{https://github.com/Kamichanw/Speculative-Ensemble/}.
\end{abstract}

\section{Introduction}
Recent advancements in Large Language Models (LLMs) have been substantial, with both closed-source and open-source frameworks pushing the boundaries of performance. Closed-source models, such as GPT-4~\cite{achiam2023gpt} and the Gemini series~\cite{team2023gemini}, have set high-performance benchmarks, while open-source models like Llama~\cite{llama3}, Deepseek~\cite{liu2024deepseek}, and the Qwen series~\cite{bai2023qwen} not only approach these benchmarks but also bring diversity in architecture, training data, and strategies. This variability has spurred interest in ensemble approaches that leverage the strengths of multiple models. Beyond traditional methods that average the probabilities of LLMs with similar capabilities, more sophisticated ensemble strategies have emerged. These include integrating three or more models to enhance performance~\cite{yu2024breaking} and using contrastive decoding~\cite{li2023contrastive}, which involves subtracting the logits of a smaller LLM from a larger one to reduce biases.

Despite the significant progress in ensemble techniques, a persistent challenge remains. Specifically, when combining multiple models for prediction, each LLM must sequentially compute the token distribution, leading to a considerable slowdown compared to the inference time of a single model. This raises the crucial question of whether ensemble speed can be increased without sacrificing the ensemble quality. To address this issue, we introduce \textbf{Speculative Ensemble} (\textbf{SE}), a novel ensemble approach inspired by Speculative Decoding (SD)~\cite{leviathan23}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{figs/digrams/ob1.pdf}
    \caption{Comparison of (a) vanilla ensemble, (b) speculative decoding, and (c) speculative ensemble. In (b) and (c), each discrete blue block represents a probability calculated by one forward pass of $\mathcal{M}_q$, while the continuous green block indicates the joint distribution requires only one forward pass of $\mathcal{M}_p$.}
    \label{fig:ob1}
\end{figure}

SD is a technique designed to accelerate LLM inference without sacrificing performance. As depicted in ~\cref{fig:ob1}~(b), it uses a smaller but more efficient proposal model $\mathcal{M}_q$ to rapidly generate proposal tokens, which are then verified in parallel by a larger target model $\mathcal{M}_p$. The target model accepts a subset of these proposal tokens, enabling the generation of multiple tokens in a single forward pass, thus significantly accelerating inference. Moreover, by employing specific acceptance-rejection criteria, the generated tokens can be considered as samples drawn from the distribution of the target model, thus ensuring the generation quality. In this paper, we extend speculative decoding to model ensembles based on the following two observations.

\textbf{First, SD allows not only sampling from the target model’s distribution,} but also sampling from the ensemble distribution of the proposal model and target model. In vanilla SD, the target model’s distribution is directly employed for token verification and resampling, ensuring that the generated tokens align with the target model’s distribution. Similarly, we find that if the ensemble distribution is used for verification and resampling, as illustrated in ~\cref{fig:ob1}~(c), the generated tokens will follow the ensemble distribution. Furthermore, this \textit{speculative-based ensemble} significantly reduces the number of model invocations required. For instance, as shown in ~\cref{fig:ob1}~(a), generating four tokens with the vanilla ensemble necessitates four invocations to both $\mathcal{M}_q$ and $\mathcal{M}_p$, while the speculative-based ensemble, in the optimal case, requires only four invocations to $\mathcal{M}_q$ and a single invocation to $\mathcal{M}_p$.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{figs/digrams/ob2.pdf}
    \caption{The sketch of Alternate Proposal Framework. A continuous colored block indicates a single model invocation, with the bonus token highlighted in a red rounded box. Beginning from Step 2, \(\mathcal{M}_q\) and \(\mathcal{M}_p\) are invoked alternately. Each invocation involves both the verification of the current token and the generation of a bonus token. For clarity, we assume that the proposal length for each model is 1 and that all proposed tokens are accepted.}
    \label{fig:ob2}
\end{figure}

\textbf{Secondly, alternating each model as proposer and verifier can further accelerate the ensemble process.} In standard SD, the proposer and verifier are fixed, with one model consistently serving as the proposer and the other as the verifier. However, we observe that in the ensemble setting, this static assignment is suboptimal, as it fails to fully leverage the bonus token. In SD, when all tokens from the proposal model are accepted by the target model, the target model will naturally generate an additional token, referred to as the \textit{bonus token}. However, since the bonus token is drawn from the target model's distribution rather than the ensemble distribution, it cannot be directly appended to the ensemble output. A naive solution might be to discard the bonus token or to re-query the proposal model and compute the ensemble distribution. Instead, we propose an alternative approach: treating the bonus token as a proposal from the target model, which is then verified by the proposer model. This insight leads to the development of a more efficient framework, the \textit{Alternate Proposal Framework}, illustrated in \cref{fig:ob2}. Combined with the speculative-based ensemble, the alternate proposal framework forms the proposed SE. Moreover, we extend SE to the more general $n$-model ensemble case in \cref{subsec:tetris}.

We establish the effectiveness of SE through both theoretical and experimental perspectives. Theoretically, we derive an expected improvement factor to quantify its acceleration and prove that SE is guaranteed to be at least as efficient as the standard ensemble, typically achieving greater speed. Additionally, in the weighted ensemble setting—the most common ensemble setting—we demonstrate that SE maintains a provable lower bound on the acceptance rate, ensuring consistently high efficiency. Experimentally, we conduct extensive experiments across various tasks, including code generation, mathematical reasoning, multi-task understanding, and text summarization. Our evaluation covers multiple LLM pairs, including Llama, Vicuna, and Qwen series, under both two-model and three-model configurations. The results show that SE consistently achieves the highest acceleration, with speedups of \textbf{1.34x–1.85x} for weighted ensemble and \textbf{1.11x–2.23x} for contrastive decoding. 

In summary, our key contributions are as follows:  
(1) We extend speculative decoding to the ensemble setting by refining its verification mechanism, introducing a speculative-based ensemble that significantly improves efficiency.
(2) We incorporate an alternate proposal framework into the speculative-based ensemble to get finally Speculative Ensemble, further boosting inference speed.
(3) Through extensive theoretical analysis and experimental evaluation, we demonstrate that our method achieves substantial acceleration while maintaining a lower bound, ensuring it never underperforms compared to standard ensembles.

\section{Related Work}
\noindent\textbf{Large Language Model Ensemble.}
LLM ensembles combine multiple pretrained models to enhance output quality and robustness. Existing approaches can be categorized based on the application stage: pre-inference, post-inference, and during-inference \cite{lu2024merge}. 

Pre-inference ensembles use a router to direct incoming queries to the most suitable LLM \cite{shnitzer2023large,lu2023routing,lu2024blending,ding2024hybrid}. Post-inference ensembles operate at the sequence level, selecting the optimal output from multiple LLM-generated sequences \cite{chen2023frugalgpt,lee2023ensemble,jiang2023llm}. We focus on during-inference ensembles, where multiple LLMs contribute at the token level by leveraging probability distributions or logits to predict the next token. Methods in this category include weighted sums of probability distributions or logits \cite{li2024purifying}, contrastive decoding \cite{li2023contrastive}, and learnable ensemble techniques \cite{furkan2024llm}. The proposed SE can accelerate any probability or logits-level ensemble, demonstrating broad potential.


\noindent\textbf{Speculative Decoding.}
Speculative decoding \cite{leviathan23, chen23} can be categorized into two main areas: proposal model design and verification design. In the first category, proposal models are designed to generate tokens that are more likely to be accepted by the verifier. This includes independent proposal models, such as distillation-based method \cite{zhou24} and target-informed models that incorporate information of verifier \cite{zhang2024,elhoushi2024,monea23,yi2024,monea23,li24a}.

The second category optimizes target model's verification process to improve decoding efficiency, following two main research directions. The first one increases proposal tokens and uses structured attention mechanisms \cite{miao2023specinfer, cai24,li24b,gong24} to validate multiple candidates simultaneously. The second direction modifies the verification strategy itself, employing methods like joint probability density estimation \cite{anonymous24a}, Monte Carlo tree search \cite{hu24}, and a linear binary classifier \cite{anonymous24b}.

The most related work is Speculative Contrastive Decoding (SCD) \cite{scd}, which improves output quality by using contrastive decoding for rejected tokens. However, SE applies to any ensemble function allowing application to all tokens instead of only rejected ones.

% Speculative decoding \cite{leviathan23, chen23} is a technique designed to accelerate the inference process of large language models. It uses a lightweight draft model to generate candidate sequences, which are then verified in parallel by the target model. During the generation phase, the draft model sequentially generates candidate tokens. However, due to its smaller size, it can generate quickly. In the verification phase, the target model, although larger, verifies the candidate token sequences in parallel, requiring only one prediction to verify the entire sequence. This greatly reduces the number of times the target model is called. Based on these two phases, the inference process is greatly accelerated. Furthermore, speculative decoding uses speculative sampling \cite{leviathan23, chen23} during the verification phase. This ensures that the process of token generation is consistent with sampling directly from the target model's distribution, maintaining the quality of the generated tokens while also speeding up inference.

% Speculative decoding \cite{leviathan23, chen23} is a technique aimed to accelerate the inference process of large language models. It uses a lightweight proposal model to generate candidate sequences, which are then verified in parallel by the target model. By enabling faster inference without sacrificing model performance, it has garnered significant research interest.
\section{Speculative Ensemble}

\subsection{Speculative Decoding}

Speculative decoding (SD) is a technique designed to speed up inference while maintaining the quality of generated outputs. It involves two phases: the proposal phase and the verification phase. During the proposal phase, a lightweight proposal model sequentially generates proposal tokens. In the verification phase, a larger target model verifies these tokens in parallel. Furthermore, by incorporating appropriate acceptance-rejection criteria, the technique ensures that the generated tokens align precisely with the target model’s distribution, thus maintaining high-quality results.

Specifically, in the proposal phase, the proposal model $\mathcal{M}_q$ generates a sequence of length $\gamma$, denoted as:
\begin{equation}
(x_{i+1}, x_{i+2}, \ldots, x_{i+\gamma}) \sim \prod_{j=1}^{\gamma} q_{i+j}(x).
\end{equation}
Here, \(x_{i+j}\) represents the token generated at position \(i+j\), and \(q_{i+j}(x) \triangleq q(x_{i+j} \mid x_{\leq i+j-1})\) is the conditional probability distribution computed by $\mathcal{M}_q$ over \(x_{i+j}\), given the previously generated sequence \(x_{\leq i+j-1}\).

In the verification phase, the target model \(\mathcal{M}_p\) executes a forward pass, producing \(\gamma+1\) target distributions: \(p_{i+1}(x), \dots, p_{i+\gamma}(x), p_{i+\gamma+1}(x)\). The first $\gamma$ distributions are subsequently used to validate the proposal tokens generated in the proposal phase. Specifically, a proposal token \(x_{i+j}\) is accepted if the following condition holds:
\begin{equation}\label{eqn:verify}
u_j \le \min\left(1, \frac{p_{i+j}(x)}{q_{i+j}(x)}\right)
\end{equation}
where \(u_j \sim U(0,1)\) represents a uniformly distributed random variable. If the token \(x_{i+j}\) is rejected, the subsequent tokens \(x_{i+j+1}, \dots, x_{i+\gamma+1}\) are discarded, and \(x_{i+j}\) is sampled from the distribution \(\text{norm}(\max(0, p_{i+j} - q_{i+j}))\). If all \(\gamma\) tokens are accepted, an additional token is directly sampled from \(p_{i+\gamma+1}\) and appended to the generated sequence, referred to as the \textit{bonus token} in our paper.

By iteratively alternating between the proposal and verification phases, SD improves inference speed while ensuring the generated tokens align with the target model's distribution.

\subsection{Speculative-based Ensemble}\label{subsec:sd_for_ens}

As discussed above, vanilla SD can only accelerate the inference of a single model. In this subsection, we will introduce how to apply SD to scenarios involving an arbitrary ensemble of two models. Specifically, let $q_{i}(x)$ and $p_{i}(x)$ denote the distributions of token $x_{i}$ given by the proposal model and the target model, respectively, and let $l^q_{i}$ and $l^p_{i}$ be the corresponding logits. Then, the ensemble distribution $r_{i}(x)$ can be expressed as
\begin{equation}
    r_{i}(x) = \mathcal{E}(q_{i}(x), p_{i}(x))\ \ \text{or}\ \ \mathcal{E'}(l^q_{i}, l^p_{i}),
\end{equation}
where $\mathcal E(\cdot)$ represents the ensemble function at the probability level, while $\mathcal E'(\cdot)$ is at logits level. For example, the common weighted ensemble form that uses probability for weighted summation can be expressed as
\begin{equation}\label{eqn:weighted}
    r(x) = \mathcal{E}(q_{i}(x), p_{i}(x)) = \lambda q(x) + (1 - \lambda) p(x),
\end{equation}
while contrastive decoding can be represented as
\begin{equation}\label{eqn:contrastive}
r(x) = \mathcal{E'}(l^q_{i}, l^p_{i}) = \text{Softmax}(l^p_{i} - \mu l^q_{i}).
\end{equation}

We note that by making slight modifications to the vanilla SD, the generated tokens can align with the ensemble distribution. Specifically, before verification, we first compute the ensemble distribution $r(x)$ and update the verification formula in ~\cref{eqn:verify} as follows:
\begin{equation}
    u_j \leq \min \left( 1, \frac{r_{i+j}(x)}{q_{i+j}(x)} \right).
\end{equation}
Then, if the token is rejected, we resample $x_{i+j}$ from the distribution $\text{norm}(\max(0, r_{i+j} - q_{i+j}))$. 

We theoretically prove the correctness of speculative-based ensemble, that is, the tokens generated by the above sampling process precisely align with the ensemble distribution, with an acceptance rate $\alpha$ of
\begin{equation}\label{eqn:alpha}
    \alpha = 1 - \frac{1}{2}D_{\text{TV}}(q, r),
\end{equation}
where $D_{\text{TV}}(q, r)$ is the total variation distance, defined as $D_{\text{TV}}(q, r) = \sum_{x\in \mathcal V} \left| q(x) - r(x) \right|$, where $\mathcal V$ is the set of all tokens. The proof is provided in ~\cref{apd:correctness}.

\textbf{Analysis of speed improvement.} In speculative decoding, inference speed is predominantly influenced by the acceptance rate \(\alpha\), with a higher acceptance rate leading to more substantial speed improvements. In this part, we first analyze the theoretical speed improvement when $\alpha$ is known. When $\alpha$ is unknown, we focus on weighted ensemble scenario and provide a lower bound for $\alpha$. With this bound, we derive a series of favorable acceleration properties.

\begin{theorem}\label{thm:improvement}
    Let $\gamma$ be the proposal length and $c$ be the cost coefficient, defined as the ratio between the time for a single invocation of the proposal model and the target model. Then, the expected speed improvement factor is $\frac{(1 - \alpha^{\gamma}) (1 + c)}{(1 - \alpha)(1 + c \gamma)}$.
\end{theorem}
The proof of \cref{thm:improvement} is in \cref{apd:proof_of_improvement}. \cref{thm:improvement} provides the speed improvement factor when $\alpha$ is known. However, in most cases, $\alpha$ is unknown and requires extensive experiments to estimate. Nevertheless, we find that in the weighted ensemble scenario, which is the most common ensemble scenario, $\alpha$ has a lower bound.

% While \cref{thm:improvement} gives the speed improvement factor when $\alpha$ is known, $\alpha$ is usually unknown and requires extensive experiments to estimate. However, in the common weighted ensemble scenario, $\alpha$ has a known lower bound.

\begin{theorem}\label{thm:lower_bound}
    If $\mathcal{E}(p, q) = \lambda q(x) + (1 - \lambda) p(x)$ and $q(x)$ is the proposal model. Then $\alpha$ has a lower bound of $\lambda$.
\end{theorem}
\begin{proof}
We have $\alpha = \sum_{x\in \mathcal V} q(x) \min\left(1, \frac{r(x)}{q(x)}\right)$, then $\alpha = \sum_{x\in \mathcal V} q(x) \min\left(1, \lambda + (1 - \lambda)\frac{p(x)}{q(x)}\right)$, and then we get $\alpha \geq \sum_{x\in \mathcal V} \lambda q(x) = \lambda.$
 
    % \begin{equation}
    %     \begin{aligned}
    %         \alpha &= \sum_{x\in \mathcal V} q(x) \min\left(1, \frac{r(x)}{q(x)}\right) \\
    %         &= \sum_{x\in \mathcal V} q(x) \min\left(1, \lambda + (1 - \lambda)\frac{p(x)}{q(x)}\right) \\
    %         &\geq \sum_{x\in \mathcal V} \lambda q(x) = \lambda.\\
    %     \end{aligned}
    % \end{equation}
The equality holds if and only if $p(x)q(x) = 0$ for all $x \in \mathcal{V}$, which means that $p(x)$ and $q(x)$ do not overlap.
\end{proof}

\begin{corollary}\label{thm:we_lb}
    Assume that $\mathcal{E}(p, q) = \lambda q(x) + (1 - \lambda) p(x)$, and that $\mathcal M_q$ and $\mathcal M_p$ have comparable parameters. Then, by selecting an appropriate proposal model, $\alpha$ has a lower bound of at least 0.5.
\end{corollary}
\begin{proof}
Since $\mathcal M_q$ and $\mathcal M_p$ have comparable parameters, either can serve as the proposal model. Therefore, $\alpha$ has a lower bound of $\max(\lambda, 1 - \lambda)$, which is at least 0.5.
\end{proof}

By utilizing the lower bound property, we demonstrate that the proposed speculative-based decoding method is guaranteed to be no slower than the vanilla ensemble approach in the weighted ensemble scenario, and it is typically faster.

\begin{corollary}\label{cor:if_lambda}
    Assume that $\mathcal{E}(p, q) = \lambda q(x) + (1 - \lambda) p(x)$, then if $\lambda > \frac{c}{1 + c}$, there exists a value of $\gamma$ that enhances the inference speed.
\end{corollary}
\begin{proof}
Consider $\gamma = 2$, and solve the inequality $\frac{(1 - \alpha^\gamma)(1 + c)}{(1 - \alpha) (1 + c\gamma)} > 1$. Solving this inequality yields $\alpha > \frac{c}{1+c}$. Since $\alpha \geq \lambda$ follows, establishing the corollary.
\end{proof}

\begin{corollary}\label{cor:weighted_effectiveness}
    Assume that $\mathcal{E}(p, q) = \lambda q(x) + (1 - \lambda) p(x)$. Then, for any two models, there exists a value of $\gamma$ such that the speed of the speculative-based ensemble is not slower than the vanilla ensemble, and it is almost always faster.
\end{corollary}
\begin{proof}
As stated in ~\cref{cor:if_lambda}, if $\lambda > \frac{c}{1 + c}$, there exists a value of $\gamma$ that enhances the inference speed. If we swap the proposer and the verifier, the condition for acceleration changes to $1 - \lambda > \frac{1/c}{1 + 1/c}$, which simplifies to $\lambda < \frac{c}{1 + c}$.

If $\lambda \ne \frac{c}{1 + c}$, either $\lambda > \frac{c}{1 + c}$ or $\lambda < \frac{c}{1 + c}$ must hold, which ensures a speedup. Otherwise, if $\lambda = \frac{c}{1 + c}$, then $\alpha \geq \lambda = \frac{(1 - \alpha^{\gamma}) (1 + c)}{(1 - \alpha)(1 + c \gamma)}$, which ensures that the speed does not decline. Moreover, equality only holds when the two distributions do not overlap, which is almost impossible in practice.
\end{proof}

\textbf{Interpretable quality-speed tradeoff in vanilla SD.} In SD, some studies focus on relaxing the acceptance criteria for a higher acceptance rate to achieve faster inference, such as lossy SD \cite{zhou24} and typical acceptance \cite{cai24}. However, these methods often lack interpretability, that is, we do not know which distribution the generated tokens will follow. We find that the speculative-based ensemble can naturally be an interpretable strategy for adjusting the quality-speed tradeoff in vanilla SD.

Specifically, we apply a weighted ensemble using the proposal and target models in SD. In this setup, when \(\lambda = 0\), the ensemble distribution aligns exactly with the target distribution, reducing the method to standard SD. For $\lambda > 0$, the acceptance rate is guaranteed to have a lower bound and greater than that of vanilla SD, as demonstrated in the proof of \cref{thm:lower_bound}, leading to greater acceleration. However, incorporating less precise information from a smaller model can introduce some performance degradation. This tradeoff provides a mechanism to balance quality and speed in speculative decoding.

In contrast to existing approaches, this method improves interpretability. This is because we know the distribution of the generated tokens after relaxation, i.e. ensemble distribution defined in \cref{eqn:weighted}. This allows us to design proposal models that accelerate inference without compromising performance, and potentially even enhance the model's capabilities in specific areas. For instance, some research suggests that ensembling a smaller model appropriately can improve safety \cite{wang2024mllm, li2024purifying}. The experimental results are shown in \cref{apd:tradeoff}.

% As indicated by Equation~(\ref{eqn:alpha}), \(\alpha\) in our method is contingent on the total variation distance \(D_{\text{TV}}(q, r)\), which effectively establishes a link between ensemble speed and the individual contributions of each model. When model \(\mathcal{M}_q\) contributes significantly to the ensemble, the value of \(D_{\text{TV}}(q, r)\) is generally low, thereby enhancing both the accuracy and the inference speed. This relationship will be further elucidated in the context of a weighted ensemble scenario.

\subsection{Alternate Proposal Framework}
\label{subsec:alter_proposal}
In ~\cref{subsec:sd_for_ens}, we explore the application of speculative decoding to LLM ensembles. However, we don't consider the bonus token, that is, the additional token generated when all proposal tokens are accepted. This is because the bonus token follows the distribution of the verifier rather than the ensemble distribution and can not be directly appended to the output sequence. In this subsection, we introduce an ensemble framework, termed the \textit{alternate proposal framework}, which effectively leverages the bonus token and demonstrates superior performance.

As shown in \cref{fig:ob2}, in the alternate proposal framework, the generation of a bonus token is treated as a proposal from the current verifier, which is subsequently verified by the current proposer. Specifically, let the proposer be denoted as $\mathcal{M}_q$ and the verifier as $\mathcal{M}_p$ with proposal lengths $\gamma_q$ and $\gamma_p$, respectively. If all tokens proposed by $\mathcal{M}_q$ are accepted, a total of $\gamma_q + 1$ tokens will be generated. The first $\gamma_q$ tokens follows the distribution $r_{i+j}(x) = \mathcal{E}(q_{i+j}(x), p_{i+j}(x))$, for $j=1, \dots, \gamma_q$, while the $\gamma_q + 1$-th token is drawn from $p_{i+\gamma_q + 1}(x)$. At this stage, the $\gamma_q + 1$-th token, referred to as bonus token, is treated as the initial token in $\mathcal{M}_p$'s proposal. Subsequently, $\mathcal{M}_p$ will generate an additional $\gamma_p - 1$ tokens to complete its proposal.

If any proposed tokens are rejected and no bonus token is generated, the default proposal model will take over as the proposal model. This default model is predefined and fixed. As outlined above, the two models alternate as proposers during the decoding process, which is why this approach is called the alternate proposal framework. The pseudocode for this framework is provided in \cref{alg:alternate}.

\textbf{Analysis of speed improvement.} We now analyze the speed improvement achieved by the alternative proposal framework. For the sake of clarity, we focus on a single cycle, which encompasses one proposal and one verification. In this cycle, both the proposer and verifier are fixed. Given that the decoding process is composed of multiple such cycles, the overall decoding performance can be inferred from the behavior of a single cycle.

First, similar to \cref{thm:improvement}, we provide the expected speed improvement factor for the alternate proposal framework.

\begin{theorem}\label{thm:alternate_improvement}
    Let $\mathcal M_q$ be the proposer and $\mathcal M_p$ be the verifier, then the expected speed improvement factor of the alternate proposal framework is $\frac{(1 - \alpha^{\gamma_q}) (1 + c)}{(1 - \alpha)(1 + c \gamma_q - \alpha^{\gamma_p} c)}$.
\end{theorem}
\begin{proof}
When the bonus token is generated, the proposer only needs to generate $\gamma_{q} - 1$ new tokens; otherwise, it must generate $\gamma_{q}$ tokens. The probability of generating the bonus token is the probability that all proposal tokens in the last cycle were accepted, which is $\alpha^{\gamma_p}$. Therefore, the expected time spent on proposal and verification is $\alpha^{\gamma_p} \left(1 + c (\gamma_{q} - 1)\right) + (1 - \alpha^{\gamma_p})(1 + c\gamma_{q})$. Then the factor can be derived following the process in ~\cref{apd:proof_of_improvement}.
\end{proof}

In \cref{cor:weighted_effectiveness}, we proved that in the weighted ensemble scenario, the speculative-based ensemble is never slower than the vanilla ensemble and is typically faster. In this subsection, with the alternate proposal framework, we extend this conclusion to other types of two-model ensembles.

\begin{corollary}\label{cor:effectiveness}
    For any two models, there exist values of $\gamma_q$ and $\gamma_p$ such that the speed of the alternate proposal framework is never slower than the vanilla ensemble and is almost always faster.
\end{corollary}
\begin{proof}
    Consider $\gamma_q = \gamma_p = 1$, then $\frac{(1 - \alpha^{\gamma_q}) (1 + c)}{(1 - \alpha)(1 + c \gamma_q - \alpha^{\gamma_p} c)} \geq 1$ holds universally. The equality holds only when $c = 0$ or $\alpha = 0$. However, $c > 0$ because the execution time of the proposal model is non-negligible, and $\alpha > 0$ holds unless the proposal distribution and the ensemble distribution do not overlap, which is almost impossible.
\end{proof}

An intuitive interpretation of \cref{cor:effectiveness} is that when $\gamma_q = \gamma_p = 1$, even in the worst-case scenario—where all proposal tokens are rejected—each token generation still requires only one proposal and one verification. This results in the same number of model invocations as the standard ensemble. In practice, however, it is rare for all proposal tokens to be rejected. Once a token is accepted, the ensemble process becomes more efficient.

\subsection{Generalize to More Models}
\label{subsec:tetris}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{figs/digrams/tetris.pdf}
    \caption{The sketch of SE in three-model ensemble scenario. 
    The colored boxes represent the stored probability distributions, while the grey boxes represent the discarded ones. Each invocation involves scoring the current proposal tokens and generating a bonus token. For clarity, we assume that the proposal length for each model is 1 and that all proposed tokens are accepted.
    }
    \label{fig:tetris}
\end{figure}

In this subsection, we extend SE to the \(n\)-model ensemble scenario. The core principles remain similar to the two-model case, with acceleration driven by two key factors. First, each model can score the proposals of other models in parallel, where scoring refers to computing the probability distribution of a proposal from other models.\footnote{We use the term "scoring" rather than "verification" because, unlike in the two-model case, scoring does not immediately trigger verification; instead, verification occurs only after all models have scored a token.} Second, during scoring, a model can naturally generate a bonus token, which further improves efficiency. We illustrate the speculative ensemble process in the \(n\)-model scenario with a simple example, while detailed pseudocode and a general visualization are provided in \cref{apd:se}.

As shown in \cref{fig:tetris}, the process begins in step 1 with the default proposal model, \(\mathcal{M}_1\), generating a proposal token \(x_1\). In step 2, \(\mathcal{M}_2\) scores \(x_1\) while simultaneously generating a bonus token \(x_2\). Similarly, in step 3, \(\mathcal{M}_3\) scores both \(x_1\) and \(x_2\) in parallel and produces another bonus token, \(x_3\). At this point, \(x_1\) has been scored by both \(\mathcal{M}_2\) and \(\mathcal{M}_3\), enabling the computation of its ensemble distribution $r_1(x)$ for verification. The associated distributions \(p_1^{(1)}(x)\), \(p_2^{(1)}(x)\), \(p_3^{(1)}(x)\) are no longer needed and are discarded.  

If \(x_1\) is accepted, \(\mathcal{M}_1\) computes \(p_2^{(1)}(x)\), \(p_3^{(1)}(x)\), \(p_4^{(1)}(x)\) in parallel as shown in step 5, allowing verification of \(x_2\). Otherwise, if \(x_1\) is rejected, all stored distributions are cleared, and \(\mathcal{M}_1\) generates a new proposal, similar to step 1.

% The major idea of this algorithm is that when a LLM $\mathcal{M}$ scores some proposal tokens, it naturally produces one additional token probability where a new token can be sampled, just as in step 2 of Figure~\ref{3}, $p_2^{(2)}(x)$ is created by $\mathcal{M}_2$ when it scores $p_1^{(2)}(x)$. This token can be added into the previous proposal tokens to build a new proposal sequence. This sequence can be scored by another LLM to produce one new token probability and token, as $p_3^{(3)}(x)$ created by $\mathcal{M}_2$ in step 3. However, in such proposal sequence, if we want to verify one token at one position by the ensemble distribution, e.g., the first token in step 3, all LLMs should be invoked once to calculate that token probability, as step 3 shows that all three LLMs calculated $p_1(x)$. Then if this token pass the verification, it will be accepted. By iteratively running this process, the tokens at each position can be produced and verified.

\section{Experiments}
\subsection{Experimental Setups}
\textbf{Datasets and evaluation.} We test SE across multiple tasks including code generation, mathematical reasoning, multi-task understanding, and text summarization on HumanEval \cite{chen2021codex}, GSM8K \cite{cobbe2021gsm8k}, MMLU \cite{hendryckstest2021} and CNNDM \cite{cnndm}, respectively. We measure each method's speed by the average tokens generated per second and compute the speedup ratio relative to the standard ensemble.

\textbf{Ensemble functions and methods.}
We experiment with two ensemble functions: weighted ensemble (WE) at the distribution level (\cref{eqn:weighted}) and contrastive decoding (CD) at the logits level (\cref{eqn:contrastive}). 
For WE, in the two-model case, we set \(\lambda = 0.5\) and temperature $T=1$; in the three-model case, each model's coefficient was set to \(1/3\). For CD, we set \(\mu = 0.1\), which is the most common setting, and set $T$ to both 0 and 1. WE with $T=0$ is not tested due to its uncommon use, as it leads to a one-hot distribution, reducing information.
Among two ensemble functions, three methods are compared: (1) the standard ensemble (\textbf{WE}, \textbf{CD}); (2) an accelerated version with speculative decoding (SD), using the smallest model as the proposal and the ensemble model as the target (\textbf{WE-SD}, \textbf{CD-SD}); and (3) Speculative Ensemble (\textbf{WE-SE}, \textbf{CD-SE}).

\textbf{Model pair configuration.} We experiment on different types of LLMs, including Llama-2 \cite{llama2, miao2023specinfer}, Vicuna \cite{vicuna}, Llama-3 \cite{llama3}, Qwen-2.5 \cite{qwen2.5},  and OPT \cite{zhang2022opt}. Model pair configurations for each ensemble are in \cref{tab:model_pairs}.  We also test a three-model ensemble using Qwen2.5-1.5B-Instruct and its code and math versions in the WE setting.

\begin{table}[tb]
    \centering
    \caption{Model pair configuration. The first column represents the name of the corresponding model pair for simplicity.} 
    \label{tab:model_pairs}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|lll}
    \toprule
    \textbf{} & Name & $\mathcal M_q$ & $\mathcal M_p$ \\
    \midrule
    \multirow{3}{*}{\rotatebox{90}{WE}} & Llama-Vicuna & Llama-2-7B &  Vicuna-7B-V1.5 \\
    & Qwen-3b & Qwen2.5-3B-Instruct &  Qwen2.5-Coder-3B-Instruct \\
    & Qwen-1.5b & Qwen2.5-1.5B-Instruct & Qwen2.5-Coder-1.5B-Instruct \\
    \midrule
    \multirow{4}{*}{\rotatebox{90}{CD}} & Llama-3 & Llama-3.2-1B &  Llama-3.1-8B-Instruct \\
    & Llama-2 & Llama-68M & Llama-2-7B \\
    & OPT & OPT-125M & OPT-13B  \\
    \bottomrule
    \end{tabular}
    }
\end{table}

% \begin{table}[tb]
%     \centering
%     \caption{Model pair configuration. The first column represents the name of the corresponding model pair for simplicity.} 
%     \label{tab:model_pairs}
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{lll}
%     \toprule
%     Name & $\mathcal M_q$ & $\mathcal M_p$ \\
%     \midrule
%     \multicolumn{3}{l}{\textit{WE:}}\\
%     \cmidrule{1-1}
%     Llama-Vicuna & Llama-2-7B &  Vicuna-7B-V1.5 \\
%     Qwen-3b & Qwen2.5-3B-Instruct &  Qwen2.5-Coder-3B-Instruct \\
%     Qwen-1.5b & Qwen2.5-1.5B-Instruct & Qwen2.5-Coder-1.5B-Instruct \\
%     \midrule[0.2pt]
%     \multicolumn{2}{l}{\textit{CD:}}\\
%     \cmidrule{1-1}
%     Llama-3 & Llama-3.2-1B &  Llama-3.1-8B-Instruct \\
%     Llama-2 & Llama-68M & Llama-2-7B \\
%     OPT & OPT-125M & OPT-13B  \\
%     Bloom & Bloomz-560M & Bloom-7B1 \\
%     \bottomrule
%     \end{tabular}
%     }
% \end{table}

\textbf{Configuration of \(\gamma\).} The proposal length \(\gamma\) is the only hyperparameter in SD, affecting the algorithm's acceleration. In the two-model SE setting, \(\gamma\) corresponds to the proposal length of the smaller model, with the larger model fixed at 1. For simplicity, we refer to the smaller model with \(\gamma > 1\) as the proposal model of SE, since it typically serves this role. We tested \(\gamma = 5\) and \(\gamma = 1\) for SE and SD speeds, reporting the optimal results. \(\gamma = 5\) is the common setting, while \(\gamma = 1\) ensures acceleration (\cref{cor:effectiveness}). In the three-model SE, all models have a proposal length of 1.

\begin{figure*}[htbp]
    \centering
    \subfloat[Llama-Vicuna (HumanEval)]{
    \includegraphics[width=0.24\linewidth]{figs/analysis/lambda_abl/llama2_humaneval.pdf}
    }
    \subfloat[Llama-Vicuna (GSM8K)]{
    \includegraphics[width=0.24\linewidth]{figs/analysis/lambda_abl/llama2_gsm8k.pdf}
    }
    \subfloat[Qwen-3b (HumanEval)]{
    \includegraphics[width=0.24\linewidth]{figs/analysis/lambda_abl/qwen_humaneval.pdf}
    }
    \subfloat[Qwen-3b (GSM8K)]{
    \includegraphics[width=0.24\linewidth]{figs/analysis/lambda_abl/qwen_gsm8k.pdf}
    }
    \caption{Comparison of speedup ratios for different $\lambda$ in WE across diverse setings. The blue and green lines represent the speedup ratios when the corresponding models serve as the proposal model, while the shaded region highlights the maximum speedup between the two.}
    \label{fig:lambda_abl}
\end{figure*}

\begin{figure}[tbp]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/analysis/mu_abl/llama3.pdf}
    \caption{Comparison of speedup ratios for different $\mu$ in CD across different temperatures and datasets.}
    \label{fig:mu_abl}
\end{figure}

\subsection{Main Results}

\cref{tab:we_main} and \cref{tab:cd_main} display the speedup ratios for each method relative to the standard ensemble in the WE and CD settings, respectively.\footnote{The results of OPT model pair are shown in \cref{apd:opt}.} From these two tables, we have the following findings. First, SE not only consistently achieves the highest speedup in all settings, it also gets speedup across all settings, which supports the findings in \cref{cor:effectiveness}. In contrast, SD may reduce the ensemble speed in some cases. For example, when using the Llama-2 model pair with $T=1$ on HumanEval in \cref{tab:cd_main}, applying SD reduces the speed to 0.94x of the standard ensemble. A similar speed reduction was also observed in the three-model scenario in \cref{tab:we_main}. This is because vanilla SD does not inherently ensure acceleration. When the acceptance rate is low, SD may perform slower than standard decoding.

Second, compared to the CD scenario, the WE scenario ensures a higher minimum speedup for SE. In the two-model case, SE achieves a minimum speedup of 1.34x, while in the three-model case, it reaches at least 1.27x. In contrast, the CD scenario has a speedup as low as 1.11x. This difference arises because SE maintains a consistently high acceptance rate in the WE scenario, as outlined in \cref{thm:we_lb}.

Third, the speedup varies across tasks and is influenced by the determinism of task outputs. For example, in the WE scenario, SE achieved the highest speedup on HumanEval, averaging 1.65x, as code generation demands strictly formatted outputs. Conversely, SE has a lower speedup of 1.36x on a text summarization task, where output flexibility is higher. This difference stems from the alignment between the proposal and target models: in highly deterministic tasks, their outputs exhibit greater similarity, leading to a higher acceptance rate and, consequently, stronger acceleration.

% \begin{table}[ht]
%     \centering
%     \caption{The speedup ratio of each method in WE setting. The method with the optimal speedup is highlighted in \textbf{bold}.}
%     \label{tab:we_main}
%     \resizebox{\linewidth}{!}{
%     \setlength{\tabcolsep}{2mm}{
%         \begin{tabular}{ccccc}
%             \toprule
%             Method & HumanEval & GSM8K & MMLU & CNNDM \\
%             \midrule
%             \multicolumn{5}{l}{\textit{Llama-Vicuna:}}\\
%             \cmidrule{1-2}
%             WE    & 1.00x & 1.00x & 1.00x & 1.00x\\
%             WE-SD & 1.27x & 1.21x & 1.19x & 1.15x \\
%             WE-SE & \textbf{1.58x} & \textbf{1.52x} & \textbf{1.41x} & \textbf{1.46x} \\
%             \midrule
%             \multicolumn{5}{l}{\textit{Qwen-3b:}}\\
%             \cmidrule{1-2}
%             WE    & 1.00x & 1.00x & 1.00x & 1.00x\\
%             WE-SD & 1.13x & 1.06x & 1.09x & 1.08x \\
%             WE-SE & \textbf{1.62x} & \textbf{1.52x} & \textbf{1.42x} & \textbf{1.38x} \\
%             \midrule
%             \multicolumn{5}{l}{\textit{Qwen-1.5b:}}\\
%             \cmidrule{1-2}
%             WE    & 1.00x & 1.00x & 1.00x & 1.00x\\
%             WE-SD & 1.11x & 1.13x & 1.08x & 1.10x \\
%             WE-SE & \textbf{1.56x} & \textbf{1.46x} & \textbf{1.34x} & \textbf{1.35x} \\
%             \midrule
%             \multicolumn{5}{l}{\textit{Qwen-1.5b (3 Model):}}\\
%             \cmidrule{1-2}
%             WE    & 1.00x & 1.00x & 1.00x & 1.00x\\
%             WE-SD & 0.96x & 0.92x & 0.98x & 0.95x \\
%             WE-SE & \textbf{1.85x} & \textbf{1.53x} & \textbf{1.38x} & \textbf{1.27x} \\
%             \bottomrule
%         \end{tabular}
%     }
%     }
% \end{table}

\begin{table}[ht]
    \renewcommand{\arraystretch}{1.3}
    \centering
    \caption{The speedup ratio of each method in WE setting. The method with the optimal speedup is highlighted in \textbf{bold}.}
    \label{tab:we_main}
    \resizebox{\linewidth}{!}{
    \setlength{\tabcolsep}{2mm}{
        \begin{tabular}{c|ccccc}
            \toprule
            & Method & HumanEval & GSM8K & MMLU & CNNDM \\
            \midrule
            % \multicolumn{5}{l}{\textit{Llama-Vicuna:}}\\
            % \cmidrule{1-2}
            \multirow{3}{*}{\rotatebox{90}{\makecell{Llama \\ Vicuna}}} & WE    & 1.00x & 1.00x & 1.00x & 1.00x\\
            & WE-SD & 1.27x & 1.21x & 1.19x & 1.15x \\
            & WE-SE & \textbf{1.58x} & \textbf{1.52x} & \textbf{1.41x} & \textbf{1.46x} \\
            \midrule
            % \multicolumn{5}{l}{\textit{Qwen-3b:}}\\
            % \cmidrule{1-2}
            \multirow{3}{*}{\rotatebox{90}{\makecell{Qwen-3b}}} & WE    & 1.00x & 1.00x & 1.00x & 1.00x\\
            & WE-SD & 1.13x & 1.06x & 1.09x & 1.08x \\
            & WE-SE & \textbf{1.62x} & \textbf{1.52x} & \textbf{1.42x} & \textbf{1.38x} \\
            \midrule
            % \multicolumn{5}{l}{\textit{Qwen-1.5b:}}\\
            % \cmidrule{1-2}
            \multirow{3}{*}{\rotatebox{90}{\makecell{Qwen-1.5b}}} & WE    & 1.00x & 1.00x & 1.00x & 1.00x\\
            & WE-SD & 1.11x & 1.13x & 1.08x & 1.10x \\
            & WE-SE & \textbf{1.56x} & \textbf{1.46x} & \textbf{1.34x} & \textbf{1.35x} \\
            \midrule
            % \multicolumn{5}{l}{\textit{Qwen-1.5b (3 Model):}}\\
            % \cmidrule{1-2}
            \multirow{3}{*}{\rotatebox{90}{\makecell{Qwen-1.5b \\ (3 Model)}}} & WE    & 1.00x & 1.00x & 1.00x & 1.00x\\
            & WE-SD & 0.96x & 0.92x & 0.98x & 0.95x \\
            & WE-SE & \textbf{1.85x} & \textbf{1.53x} & \textbf{1.38x} & \textbf{1.27x} \\
            \bottomrule
        \end{tabular}
    }
    }
    \vspace{-4mm}
\end{table}

\begin{table}[ht]
    \centering
    \caption{The speedup ratio of each method in CD setting.}
    \label{tab:cd_main}
    \resizebox{\linewidth}{!}{
    \setlength{\tabcolsep}{2mm}{
        \begin{tabular}{c|cccccc}
            \toprule
            & T & Method & HumanEval & GSM8K & MMLU & CNNDM \\
            \midrule
            % \multicolumn{6}{l}{\textit{Llama-3:}}\\
            % \cmidrule{1-2}
            \multirow{6}{*}{\rotatebox{90}{Llama-3}} & \multirow{3}{*}{0} & CD & 1.00x & 1.00x & 1.00x & 1.00x\\
            & & CD-SD & 2.04x & 1.81x & 1.52x & 1.58x \\
            & & CD-SE  & \textbf{2.23x} & \textbf{2.00x} & \textbf{1.77x} & \textbf{1.61x} \\
            \cmidrule{2-7}
            & \multirow{3}{*}{1} & CD & 1.00x & 1.00x & 1.00x & 1.00x\\
            & & CD-SD & 1.55x & 1.21x & 1.20x & 1.07x \\
            & & CD-SE  & \textbf{1.65x} & \textbf{1.44x} & \textbf{1.31x} & \textbf{1.18x} \\
            
            \midrule
            % \multicolumn{6}{l}{\textit{Llama-2:}}\\
            % \cmidrule{1-2}
            \multirow{6}{*}{\rotatebox{90}{Llama-2}} & \multirow{3}{*}{0} & CD & 1.00x & 1.00x & 1.00x & 1.00x\\
            & & CD-SD & 1.15x & 1.62x & 1.08x & 0.93x \\
            & & CD-SE & \textbf{1.26x} & \textbf{1.65} & \textbf{1.68x} & \textbf{1.30x} \\
            \cmidrule{2-7}
            & \multirow{3}{*}{1} & CD & 1.00x & 1.00x & 1.00x & 1.00x\\
            & & CD-SD & 0.94x & 1.16x & 1.23x & 1.10x \\
            & & CD-SE & \textbf{1.15x} & \textbf{1.20x} & \textbf{1.37x} & \textbf{1.11x} \\
            \bottomrule
        \end{tabular}
    }
    }
    \vspace{-4mm}
\end{table}

% \begin{table}[ht]
%     \centering
%     \caption{The speedup ratio of each method in CD setting.}
%     \label{tab:cd_main}
%     \resizebox{\linewidth}{!}{
%     \setlength{\tabcolsep}{2mm}{
%         \begin{tabular}{cccccc}
%             \toprule
%             T & Method & HumanEval & GSM8K & MMLU & CNNDM \\
%             \midrule
%             \multicolumn{6}{l}{\textit{Llama-3:}}\\
%             \cmidrule{1-2}
%             \multirow{3}{*}{0} & CD & 1.00x & 1.00x & 1.00x & 1.00x\\
%             & CD-SD & 2.04x & 1.81x & 1.52x & 1.58x \\
%             & CD-SE  & \textbf{2.23x} & \textbf{2.00x} & \textbf{1.77x} & \textbf{1.61x} \\
%             \midrule
%             \multirow{3}{*}{1} & CD & 1.00x & 1.00x & 1.00x & 1.00x\\
%             & CD-SD & 1.55x & 1.21x & 1.20x & 1.07x \\
%             & CD-SE  & \textbf{1.65x} & \textbf{1.44x} & \textbf{1.31x} & \textbf{1.18x} \\
            
%             \midrule
%             \multicolumn{6}{l}{\textit{Llama-2:}}\\
%             \cmidrule{1-2}
%             \multirow{3}{*}{0} & CD & 1.00x & 1.00x & 1.00x & 1.00x\\
%             & CD-SD & 1.15x & 1.62x & 1.08x & 0.93x \\
%             & CD-SE & \textbf{1.26x} & \textbf{1.65} & \textbf{1.68x} & \textbf{1.30x} \\
%             \midrule
%             \multirow{3}{*}{1} & CD & 1.00x & 1.00x & 1.00x & 1.00x\\
%             & CD-SD & 0.94x & 1.16x & 1.23x & 1.10x \\
%             & CD-SE & \textbf{1.15x} & \textbf{1.20x} & \textbf{1.37x} & \textbf{1.11x} \\
%             \bottomrule
%         \end{tabular}
%     }
%     }
% \end{table}

\begin{figure*}[htbp]
    \centering
    \vspace{-2mm}
    \subfloat[WE ($T=1,\ \lambda = 0.5$)]{
    \includegraphics[width=0.32\linewidth]{figs/analysis/gamma_abl/we.pdf}
    }
    \subfloat[CD ($T=0,\ \mu= 0.1$)]{
    \includegraphics[width=0.32\linewidth]{figs/analysis/gamma_abl/cd_T0_mu1.pdf}
    }
    \subfloat[WE ($T=1,\ \mu = 0.1$)]{
    \includegraphics[width=0.32\linewidth]{figs/analysis/gamma_abl/cd_T1_mu1.pdf}
    }
    \vspace{-2mm}
    \caption{Comparison of speedup ratios for different $\gamma$ across different settings.}
    \vspace{-4mm}
    \label{fig:gamma_abl}
\end{figure*}

\subsection{Analysis}

\textbf{Impact of proposal length $\gamma$.} We evaluate the influence of various $\gamma$ values, ranging from 1 to 5, on speedup across both the WE and CD scenarios, utilizing the HumanEval and GSM8K datasets. The results in \cref{fig:gamma_abl} show that when the models are similar in size, the speedup ratio remains stable across $\gamma$ values, as seen in the WE scenario \cref{fig:gamma_abl}~(a). This is because the high cost of invoking the proposal model offsets the speedup from increasing \(\gamma\). However, when the models differ significantly in size, the speedup ratio varies considerably with $\gamma$, as shown in the CD scenario \cref{fig:gamma_abl}~(b) and (c).

Additionally, we observe that speedup initially increases with \(\gamma\) before decreasing. For example, in the experiment with the Llama-3 model pair on GSM8K (\cref{fig:gamma_abl}~(b)), speedup improves as \(\gamma\) rises from 1 to 5, peaks at \(\gamma = 5\), and then declines. This behavior is explained by two factors: increasing \(\gamma\) boosts the expected number of accepted tokens, which improves acceleration; while later proposal tokens depend on earlier, unverified tokens, making them less accurate and more likely to be rejected, which wastes computation. Thus, the optimal speedup is achieved at a specific \(\gamma\). In some cases, however, speedup either monotonically increases or decreases due to high or low acceptance rates. For instance, this is observed in the experiment experiments with the Llama-3 pair on HumanEval (\cref{fig:gamma_abl}~(b)) and the Llama-2 pair on HumanEval (\cref{fig:gamma_abl}~(c)).

% \begin{figure*}[htbp]
%     \centering
%     \subfloat[WE ($T=1,\ \lambda = 0.5$)]{
%     \includegraphics[width=0.32\linewidth]{figs/analysis/gamma_abl/we.pdf}
%     }
%     \subfloat[CD ($T=0,\ \mu= 0.1$)]{
%     \includegraphics[width=0.32\linewidth]{figs/analysis/gamma_abl/cd_T0_mu1.pdf}
%     }
%     \subfloat[WE ($T=1,\ \mu = 0.1$)]{
%     \includegraphics[width=0.32\linewidth]{figs/analysis/gamma_abl/cd_T1_mu1.pdf}
%     }
%     \caption{Comparison of speedup ratios for different $\gamma$ across different settings.}
%     \label{fig:gamma_abl}
% \end{figure*}



\textbf{Speedup ratio for different weight $\lambda$ in WE.} We examine the speedup effect of SE when $\lambda$ takes values other than just 0.5. Specifically, we conduct experiments with \(\lambda\) values ranging from 0.1 to 0.9, using the Llama-Vicuna and Qwen-3b model pairs on the HumanEval and GSM8K datasets. The results, presented in \cref{fig:lambda_abl}, show that SE consistently achieves a high speedup of at least 1.5x across all tested $\lambda$ values. This consistent speedup occurs because, when the two models are of similar sizes, for any $\lambda$, an appropriate proposal model can be selected to maintain a high acceptance rate during SE process (as explained in \cref{thm:we_lb}), ensuring the observed speedup.

% \begin{figure*}[htbp]
%     \centering
%     \subfloat[Llama-Vicuna (HumanEval)]{
%     \includegraphics[width=0.24\linewidth]{figs/analysis/lambda_abl/llama2_humaneval.pdf}
%     }
%     \subfloat[Llama-Vicuna (GSM8K)]{
%     \includegraphics[width=0.24\linewidth]{figs/analysis/lambda_abl/llama2_gsm8k.pdf}
%     }
%     \subfloat[Qwen-3b (HumanEval)]{
%     \includegraphics[width=0.24\linewidth]{figs/analysis/lambda_abl/qwen_humaneval.pdf}
%     }
%     \subfloat[Qwen-3b (GSM8K)]{
%     \includegraphics[width=0.24\linewidth]{figs/analysis/lambda_abl/qwen_gsm8k.pdf}
%     }
%     \caption{Comparison of speedup ratios for different $\lambda$ in WE across different model pairs and datasets. The blue and green lines represent the speedup ratios when the corresponding models serve as the proposal model, while the shaded region highlights the maximum speedup between the two.}
%     \label{fig:lambda_abl}
% \end{figure*}



\textbf{Speedup ratio for different weight values of $\mu$ in CD.} Similarly, we examine the speedup effect of CD when $\mu$ takes other values. Specifically, we conduct experiments with $\mu$ ranging from 0.1 to 0.5, using the Llama-3 model pair on the HumanEval and GSM8K datasets. The results, presented in \cref{fig:mu_abl}, show that SE consistently accelerates the CD process across all tested $\mu$.

Furthermore, we find that an increase in $\mu$ results in a reduced speedup. This occurs because CD computes the ensemble distribution by subtracting the proposal model's information from the target model's. As $\mu$ grows, the gap between these distributions widens, lowering the acceptance rate (\cref{eqn:alpha}). Despite this, the speedup ratio remains above 1.00x, confirming that CD-SE always accelerates, consistent with \cref{cor:effectiveness}.

\section{Conclusion}

This paper introduces Speculative Ensemble (SE), an extension of speculative decoding that accelerates ensemble inference while maintaining output quality. SE refines the verification mechanism for direct ensemble sampling and introduces an alternate proposal framework to further boost efficiency. We demonstrate the effectiveness of SE through both theoretical analysis and empirical validation.

\clearpage

\section*{Impact Statement}

This paper presents work whose goal is to advance the field of 
Deep Learning. There are many potential societal consequences 
of our work, none which we feel must be specifically highlighted here.

\bibliography{example_paper}
\bibliographystyle{icml2025}

\clearpage

\appendix
\onecolumn
\section{Mathematical Proofs}
\subsection{Correctness of Speculative Ensemble}\label{apd:correctness}

Assume that in speculative decoding, the proposal distribution is $q(x)$, the target distribution is $p(x)$, and the ensemble distribution is $r(x) = \mathcal{E}(q(x), p(x))$. Referring to the proof of speculative decoding correctness~\cite{leviathan23}, we now prove that the sampling method described in ~\cref{subsec:sd_for_ens} ensures that the generated tokens align with the ensemble distribution $r(x)$.

First, we have:
\begin{equation}\label{eqn:correctness}
    P(x = x') = P(\text{proposal } x' \text{ accepted}) \cdot P(\text{proposal} = x') + P(\text{proposal rejected}) \cdot P(\text{resampled} = x')
\end{equation}
By definition, $P(\text{proposal } x' \text{ accepted}) = \min\left(1, \frac{r(x')}{q(x')}\right)$ and $P(\text{proposal} = x') = q(x')$.
\begin{equation}
\begin{aligned}
    P(\text{proposal rejected}) &= \sum_{x\in \mathcal V} q(x) \left[1 - \min\left(1, \frac{r(x)}{q(x)}\right)\right] \\
    & = \sum_{x\in \mathcal V} \left[q(x) - \min\left(q(x), r(x)\right)\right] \\
    & = \sum_{x\in \mathcal V} \max\left(r(x) - q(x), 0\right) \\
\end{aligned}
\end{equation}
and by definition, $\displaystyle{P(\text{resampled} = x') = \frac{\max(r(x') - q(x'), 0)}{\sum_{x\in \mathcal V} \max\left(r(x) - q(x), 0\right)}}$.
Substituting these into Equation~(\ref{eqn:correctness}), we get:
\begin{equation}
\begin{aligned}
    P(x = x') &= P(\text{proposal } x' \text{ accepted}) \cdot P(\text{proposal} = x') + P(\text{proposal rejected}) \cdot P(\text{resampled} = x')\\
    &= \min\left(1, \frac{r(x')}{q(x')}\right)\cdot q(x') + \sum_{x\in \mathcal V} \max\left(r(x) - q(x), 0\right) \cdot \frac{\max(r(x') - q(x'), 0)}{\sum_{x\in \mathcal V} \max\left(r(x) - q(x), 0\right)}\\
    &= \min(q(x'), r(x')) + \max(r(x') - q(x'), 0)\\
    &= r(x').\\
\end{aligned}
\end{equation}
The acceptance rate $P(\text{proposal accepted})$ is computed as:
\begin{equation}
\begin{aligned}
    P(\text{proposal accepted}) &= 1 - P(\text{proposal rejected})\\
    &= 1 - \sum_{x\in \mathcal V} \max\left(r(x) - q(x), 0\right)\\
    &= 1 - \frac{1}{2}\sum_{x\in \mathcal V}\left|r(x) - q(x)\right|\\
    &= 1 - \frac{1}{2}D_{\text{TV}}(r, q).
\end{aligned}
\end{equation}

\subsection{Proof of ~\cref{thm:improvement}}\label{apd:proof_of_improvement}

Referring to the proof given by \citet{leviathan23}, our proof is as follows:

First, after one proposal and one verification, the proposed method generates at least one token, so $P(\#tokens = 0) = 0$ and $P(\#tokens = 1) = 1$. If the model generates $i$ tokens ($1 < i < \gamma$), it means the first $i-1$ tokens are accepted and the $i+1$-th token is rejected. Therefore, $P(\#tokens = i) = \alpha^{i-1}(1 - \alpha)$. If the model generates $\gamma$ tokens ($i = \gamma$), it means the first $\gamma-1$ tokens are accepted. Thus, $P(\#tokens = \gamma) = \alpha^{\gamma - 1}$.

% Therefore, $\#tokens$的期望值被计算为
\begin{equation}
    \mathbb{E}(\#tokens) = \sum_{i=0}^{\gamma} i P(\#tokens = i) = \frac{1 - \alpha^\gamma}{1 - \alpha}
\end{equation}

Note this value differs from that given by \citet{leviathan23}. This discrepancy is because, in ~\cref{subsec:sd_for_ens}, we do not account for the bonus token.

Assume that the time required to invoke the target model once is $T$, and the time required to invoke a proposal model is $cT$. Therefore, one proposal and one verification together take $(\gamma c + 1)T$ time. The time required to generate one token is $\frac{(1 - \alpha)(\gamma c + 1)}{1 - \alpha^\gamma}T$. In the vanilla ensemble, the time required to generate one token is $(c + 1)T$. Therefore, the improvement factor in total walltime is $\frac{(1 - \alpha^\gamma)(1 + c)}{(1 - \alpha)(\gamma c + 1)}$.

\section{Algorithm Details}
\subsection{Alternate Proposal Framework}
We provide the detailed pseudo-code of the alternate proposal framework (see ~\cref{alg:alternate}), which is introduced in ~\cref{subsec:alter_proposal}. 
\begin{algorithm}[htbp]
   \caption{The alternate proposal framework.  \textsc{Propose} takes the current token sequence \(S\) and a constant \(\gamma\) as inputs and generates \(\gamma\) tokens \(T\). \textsc{Score} feeds a sequence to the current verifier to obtain logits \(L\) and a bonus token \(t\). \textsc{Verify} examines \(T\) to decide whether it should be accepted according to ensemble logits.}
   \label{alg:alternate}
\begin{algorithmic}
\INPUT Models \(\mathcal{M}_p\), \(\mathcal{M}_q\); proposal lengths \(\gamma_q, \gamma_p\) ; prefix sequence \textit{prefix}.

\STATE \(S\) \(\gets\) \textit{prefix}
\STATE \(C \gets \emptyset \) \Comment{Initialize cached tokens}
\WHILE{not finish}
    \IF{$C = \emptyset $}
        \lineComment{Standard speculative decoding step}
        \STATE proposer \(\gets \mathcal{M}_q\)
        \STATE verifier \(\gets \mathcal{M}_p\)
        \STATE \(T \gets \textsc{Propose}(S, \textrm{proposer.}\gamma)\)
    \ELSE
        \lineComment{Alternate proposal decoding step}
        \STATE \textsc{Swap}(proposer, verifier)
        \STATE \(T \gets C + \textsc{Propose}(S + C, \textrm{proposer.}\gamma - C\textrm{.length}\))
    \ENDIF
    \STATE \(L, t \gets \textsc{Score}(T)\)
    \STATE \(L'=\mathcal{E}^\prime(L)\)
    \STATE \(\textsc{Verify}(T, L')\)
    \IF{all tokens in \(T\) are accepted} 
        \STATE \(C \gets t\)
    \ELSE
        \lineComment{Some tokens are rejected, clear \(C\) and resample from}
        \STATE \(T \gets \text{resample from residual distribution}\)
        \STATE  \(C \gets \emptyset\)
    \ENDIF
    \STATE \(S \gets S + T \)
    
\ENDWHILE
\STATE \textbf{return} \(S\)
\end{algorithmic}
\end{algorithm}

\subsection{Speculative Ensemble Framework}\label{apd:se}
The speculative ensemble framework is a generalization of the alternate proposal framework to scenarios involving more than three models. \cref{fig:tetris_general} illustrates our speculative ensemble in a three-model scenario, where the models $\mathcal{M}_1, \mathcal{M}_2, \mathcal{M}_3$ have proposal lengths of $\gamma_1 = 3$, $\gamma_2 = 2$, and $\gamma_3 = 1$, respectively.

Specifically, in step 1, the default proposal model is invoked to generate $\gamma_1=3$ proposal tokens: $x_1, x_2, x_3$. In step 2, $\mathcal{M}_2$ is invoked to score $x_1, x_2, x_3$ while naturally generating a bonus token, $x_4$. Since $\mathcal{M}_2$ has a proposal length of $\gamma_2 = 2$, it is then invoked again to generate an additional $\gamma_2 - 1 = 1$ proposal token to complete its proposal. In step 3, $\mathcal{M}_3$ is called to score $x_1, \dots, x_5$ in parallel and generate a bonus token, $x_6$.

At this stage, $x_1, x_2, x_3$ have been scored by all models, allowing the ensemble distributions $r_1(x), r_2(x), r_3(x)$ to be computed and used for verification, as illustrated in step 3. Assuming that $x_1, x_2, x_3$ are all accepted, their corresponding probability distributions are no longer needed and are discarded, as shown in step 4. Subsequently, in step 5, $\mathcal{M}_1$ is invoked again to score $x_4, x_5, x_6$ and generate new proposal tokens: $x_7, x_8, x_9$.

Next, $x_4$ and $x_5$ undergo verification. If $x_4$ is accepted while $x_5$ is rejected, $x_4$ remains unchanged, whereas $x_5$ is replaced with $x_5'$, which is generated through the resampling phase. Since the 5-th token has changed, all subsequent proposal tokens and probability distributions derived from it become invalid and are discarded, as illustrated in step 6. Once these are removed, the default proposal model is invoked to generate $\gamma_1$ new proposal tokens, similar to step 1.

% Then，$x_4, x_5$可以被验证。假设$x_4$ is accepted and $x_5$ is rejected，此时，$x_4$保留不变and $x_5$ is replaced with $x_5'$, which is generated by 重新采样 phase。此时，由于5-th token发生了变化，因此基于proposal 产生的proposal tokens和概率分布将不能再使用，因此被丢弃掉，如step 6所示。当这些proposal tokens和概率分布被丢弃掉之后，the default proposal model is invoked to generate $\gamma_1$ proposal tokens similar to step 1.


% Steps 1 through 4 illustrate a generation process in which all tokens are accepted during one round. 
% In Step 5, token 4 is accepted, while token 5 is rejected. As a result, resampling occurs within the ensemble distribution, and the previously stored sequence is discarded (the tokens in the red dashed box in Step 6). 
% Note that in Step 5-6, some tokens are generated based on the tokens which are not yet verified. Then once the previous tokens pass the verification, these tokens can be further verified, like the 4-st and 5-st tokens in Step 5. However, once the previous tokens do not pass the verification, meaning that the subsequent tokens are generated conditioned on invalid tokens, thus should be discarded and resampled. For example, the 5-st token fails the verification in Step 5, thus all the subsequent tokens (the tokens in the red dashed box in Step 6) are discarded.
% Step 7 marks the initiation of a new generation round. 

The corresponding pseudocode is provided in ~\cref{alg:se}.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.99\linewidth]{figs/digrams/tetris_apd.pdf}
    \caption{The sketch of speculative ensemble framework in three-model ensemble scenario. The colored boxes represent the stored probability distributions, while the grey boxes represent the cleared ones. Each invocation involves scoring the current proposal tokens and generating a bonus token. The proposal length for model \(\mathcal M_1, \mathcal{M}_2, \mathcal{M}_3 \) is 3, 2, 1, respectively.}
    \label{fig:tetris_general}
\end{figure}

\begin{algorithm}[htbp]
   \caption{The speculative ensemble framework. \textsc{Propose} employs model \(\mathcal M\) to take the current token sequence \(S\) and a constant \(\gamma\) as inputs and generates \(\gamma\) tokens \(T\). \textsc{Score} employs model \(\mathcal M\) to score a sequence to obtain sequence probabilities \(P\), a bonus token \(t\) and its probability \(p\) . \textsc{Verify} examines \(T\) to decide whether it should be accepted according to ensemble probability distribution.}
   \label{alg:se}
\begin{algorithmic}
\INPUT Models \(\mathcal{M}_1\), \dots, \(\mathcal{M}_n\); proposal lengths \(\gamma_1, \dots \gamma_n\), prefix sequence \textit{prefix}.
\STATE \(S \gets\) \textit{prefix}
\STATE $S_c \gets \emptyset$ \Comment{Initialize cached sequence}
\STATE $C_i \gets \emptyset, \text{ for } i=1,\dots,n$ \Comment{Initialize cached probabilities for each model}
\WHILE{not finish}
    \IF{$S_c = \emptyset$}
        \STATE \(T,\ P \gets \textsc{Propose}(\mathcal{M}_1, S, \gamma_1)\) \Comment{If no cached sequence, default proposer $\mathcal{M}_1$ is invoked to generate proposal}
        \STATE \( S_c \gets T \) \Comment{Cache proposal tokens $T$ and corresponding probabilities $P$}
        \STATE \(C_1 \gets P \)
    \ELSE
        \STATE \(i \gets \arg\min_i |C_i|\) \Comment{Find the model with the shortest cached probabilities, \(|\cdot|\) represents the number of elements}
        \STATE \(P,\ t,\ p \gets \textsc{Score}(\mathcal{M}_i, S_c)\) \Comment{Score the $S_c$, generating probabilities of $S_c$, bonus token $t$ and its probability $p$}
        \STATE \( S_c \gets S_c \cup \{t\}\)
        \STATE \(C_i \gets C_i \cup P \cup \{p\}\) 
        \WHILE{\(\forall j,\ C_j\ne \emptyset\)} 
            \lineComment{If all $C_j$ are nonempty, $t_1$ (the first token of $S_c$) must have been scored by all models, so verify it}
            \STATE \(p' \gets \mathcal{E}(p_{11}, \dots, p_{n1})\) \Comment{\(p_{ij}\) represents the $j$-th probability of $C_i$}
            \STATE \(\textsc{Verify}(p', t_1)\)
            \IF{$t_1$ is accepted}
                \STATE $S_c \gets S_c \backslash \{t_1\}$
                \STATE $C_j  \gets C_j \backslash \{p_{j1}\}$, for $j=1,\dots, n$
            \ELSE
                \STATE $t_1 \gets$ resample a token from residual distribution
                \STATE $S_c \gets \emptyset$
                \STATE $C \gets \emptyset$, for $j=1,\dots, n$
            \ENDIF
            \STATE \(S \gets S \cup \{t_1\}\)
        \ENDWHILE
        \STATE \(T,\ P \gets \textsc{Propose}(\mathcal{M}_i, S+S_c, \gamma_i - 1)\) \Comment{Generate more $\gamma_i - 1$ tokens to finish the proposal}
        \STATE \( S_c \gets S_c \cup T \)
        \STATE \( C_i \gets C_i \cup P \)
    \ENDIF
\ENDWHILE
\STATE \textbf{return} \(S\)
\end{algorithmic}
\end{algorithm}



\section{Additional Results}

\subsection{Speculative Ensemble for Quality-Speed Tradeoff}\label{apd:tradeoff}

As outlined in \ref{subsec:sd_for_ens}, creating a weighted ensemble of the proposal and target models in Speculative Ensemble (SE) offers a way to balance the tradeoff between quality and speed. We conducted experiments with the Llama-3 model pair on four datasets, adjusting $\lambda$ from 0.1 to 0.9. As shown in \cref{fig:llama3}, increasing $\lambda$ leads to a steady improvement in inference speed but a gradual decline in performance, allowing users to choose a tradeoff that best suits their needs.

\begin{figure*}[htbp]
    \centering
    \subfloat[HumanEval]{
    \includegraphics[width=0.24\linewidth]{figs/analysis/llama3/llama3_humaneval.pdf}
    }
    \subfloat[GSM8K]{
    \includegraphics[width=0.24\linewidth]{figs/analysis/llama3/llama3_gsm8k.pdf}
    }
    \subfloat[MMLU]{
    \includegraphics[width=0.24\linewidth]{figs/analysis/llama3/llama3_mmlu.pdf}
    }
    \subfloat[CNNDM]{
    \includegraphics[width=0.24\linewidth]{figs/analysis/llama3/llama3_cnndm.pdf}
    }
    \caption{Speculative Ensemble for quality-speed tradeoff}
    \label{fig:llama3}
\end{figure*}

\subsection{Speedup on OPT Model Pair}\label{apd:opt}

\begin{table}[htbp]
    \centering
    \caption{The speedup ratio of each method in CD setting.}
    \label{tab:cd_apd}
    \setlength{\tabcolsep}{2mm}{
        \begin{tabular}{c|cccccc}
            \toprule
            & T & Method & HumanEval & GSM8K & MMLU & CNNDM \\
            \midrule
            \multirow{6}{*}{\rotatebox{90}{Opt}} & \multirow{3}{*}{0} & CD & 1.00x & 1.00x & 1.00x & 1.00x\\
            & & CD-SD & 0.97x & 1.05x & 1.47x & 1.40x \\
            & & CD-SE  & \textbf{3.28x} & \textbf{2.61x} & \textbf{3.42x} & \textbf{3.95x} \\
            \cmidrule{2-7}
            & \multirow{3}{*}{1} & CD & 1.00x & 1.00x & 1.00x & 1.00x\\
            & & CD-SD & 1.47x & 1.55x & 2.11x & 1.85x \\
            & & CD-SE  & \textbf{1.69x} & \textbf{1.76x} & \textbf{2.16x} & \textbf{1.85x} \\
            \bottomrule
        \end{tabular}
    }
\end{table}

\end{document}