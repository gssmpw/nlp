\section{Related Work}
\noindent\textbf{Large Language Model Ensemble.}
LLM ensembles combine multiple pretrained models to enhance output quality and robustness. Existing approaches can be categorized based on the application stage: pre-inference, post-inference, and during-inference \cite{lu2024merge}. 

Pre-inference ensembles use a router to direct incoming queries to the most suitable LLM \cite{shnitzer2023large,lu2023routing,lu2024blending,ding2024hybrid}. Post-inference ensembles operate at the sequence level, selecting the optimal output from multiple LLM-generated sequences \cite{chen2023frugalgpt,lee2023ensemble,jiang2023llm}. We focus on during-inference ensembles, where multiple LLMs contribute at the token level by leveraging probability distributions or logits to predict the next token. Methods in this category include weighted sums of probability distributions or logits \cite{li2024purifying}, contrastive decoding \cite{li2023contrastive}, and learnable ensemble techniques \cite{furkan2024llm}. The proposed SE can accelerate any probability or logits-level ensemble, demonstrating broad potential.


\noindent\textbf{Speculative Decoding.}
Speculative decoding \cite{leviathan23, chen23} can be categorized into two main areas: proposal model design and verification design. In the first category, proposal models are designed to generate tokens that are more likely to be accepted by the verifier. This includes independent proposal models, such as distillation-based method \cite{zhou24} and target-informed models that incorporate information of verifier \cite{zhang2024,elhoushi2024,monea23,yi2024,monea23,li24a}.

The second category optimizes target model's verification process to improve decoding efficiency, following two main research directions. The first one increases proposal tokens and uses structured attention mechanisms \cite{miao2023specinfer, cai24,li24b,gong24} to validate multiple candidates simultaneously. The second direction modifies the verification strategy itself, employing methods like joint probability density estimation \cite{anonymous24a}, Monte Carlo tree search \cite{hu24}, and a linear binary classifier \cite{anonymous24b}.

The most related work is Speculative Contrastive Decoding (SCD) \cite{scd}, which improves output quality by using contrastive decoding for rejected tokens. However, SE applies to any ensemble function allowing application to all tokens instead of only rejected ones.

% Speculative decoding \cite{leviathan23, chen23} is a technique designed to accelerate the inference process of large language models. It uses a lightweight draft model to generate candidate sequences, which are then verified in parallel by the target model. During the generation phase, the draft model sequentially generates candidate tokens. However, due to its smaller size, it can generate quickly. In the verification phase, the target model, although larger, verifies the candidate token sequences in parallel, requiring only one prediction to verify the entire sequence. This greatly reduces the number of times the target model is called. Based on these two phases, the inference process is greatly accelerated. Furthermore, speculative decoding uses speculative sampling \cite{leviathan23, chen23} during the verification phase. This ensures that the process of token generation is consistent with sampling directly from the target model's distribution, maintaining the quality of the generated tokens while also speeding up inference.

% Speculative decoding \cite{leviathan23, chen23} is a technique aimed to accelerate the inference process of large language models. It uses a lightweight proposal model to generate candidate sequences, which are then verified in parallel by the target model. By enabling faster inference without sacrificing model performance, it has garnered significant research interest.