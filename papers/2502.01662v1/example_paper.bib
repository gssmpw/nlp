@inproceedings{leviathan23,
  author    = {Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  title     = {Fast inference from transformers via speculative decoding},
  year      = {2023},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning (ICML 2023)},
  address   = {Honolulu, Hawaii, USA},
}

@article{chen23,
  title={Accelerating large language model decoding with speculative sampling},
  author={Chen, Charlie and Borgeaud, Sebastian and Irving, Geoffrey and Lespiau, Jean-Baptiste and Sifre, Laurent and Jumper, John},
  journal={arXiv preprint arXiv:2302.01318},
  year={2023}
}


@inproceedings{miao2023specinfer,
  author    = {Xupeng Miao and Gabriele Oliaro and Zhihao Zhang and Xinhao Cheng and Zeyu Wang and Zhengxin Zhang and Rae Ying Yee Wong and Alan Zhu and Lijie Yang and Xiaoxiang Shi and Chunan Shi and Zhuoming Chen and Daiyaan Arfeen and Reyna Abhyankar and Zhihao Jia},
  title     = {SpecInfer: Accelerating Large Language Model Serving with Tree-based Speculative Inference and Verification},
  year      = {2024},
  booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 2024)},
}

@inproceedings{cai24,
  author    = {Cai, Tianle and Li, Yuhong and Geng, Zhengyang and Peng, Hongwu and Lee, Jason D. and Chen, Deming and Dao, Tri},
  title     = {Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads},
  year      = {2024},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning (ICML 2024)},
  address   = {Vienna, Austria},
}

@inproceedings{li24a,
  author    = {Li, Yuhui and Wei, Fangyun and Zhang, Chao and Zhang, Hongyang},
  title     = {EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty},
  year      = {2024},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning (ICML 2024)},
  address   = {Vienna, Austria},
}

@inproceedings{zhang2024,
  author    = {Jun Zhang and Jue Wang and Huan Li and Lidan Shou and Ke Chen and Gang Chen and Sharad Mehrotra},
  title     = {Draft\&Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding},
  year      = {2024},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024)},
  address   = {Bangkok, Thailand},
}

@inproceedings{elhoushi2024,
  author    = {Mostafa Elhoushi and Akshat Shrivastava and Diana Liskovich and Basil Hosmer and Bram Wasti and Liangzhen Lai and Anas Mahmoud and Bilge Acun and Saurabh Agarwal and Ahmed Roman and Ahmed Aly and Beidi Chen and Carole-Jean Wu},
  title     = {LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding},
  year      = {2024},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024)},
  address   = {Bangkok, Thailand},
}

@inproceedings{santilli2023,
  author    = {Andrea Santilli and Silvio Severino and Emilian Postolache and Valentino Maiorca and Michele Mancusi and Riccardo Marin and Emanuele Rodola},
  title     = {Accelerating Transformer Inference for Translation via Parallel Decoding},
  year      = {2023},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL 2023)},
  address   = {Toronto, Canada},
}

@inproceedings{zhou24,
  author    = {Yongchao Zhou and Kaifeng Lyu and Ankit Singh Rawat and Aditya Menon and Afshin Rostamizadeh and Sanjiv Kumar and Jean-Fran√ßois Kagy and Rishabh Agarwal},
  title     = {DistillSpec: Improving speculative decoding via knowledge distillation},
  year      = {2024},
  booktitle = {The Thirteenth International Conference on Learning Representations (ICLR 2024)},
  url       = {https://openreview.net/forum?id=rsY6J3ZaTF},
}

@article{monea23,
  author       = {Giovanni Monea and Armand Joulin and Edouard Grave},
  title        = {PaSS: Parallel Speculative Sampling},
  year         = {2023},
  journal = {arXiv preprint arXiv:2302.01318},
  url          = {https://arxiv.org/abs/2311.13581},
}

@article{yi2024,
      title={Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding}, 
      author={Hanling Yi and Feng Lin and Hongbin Li and Peiyang Ning and Xiaotian Yu and Rong Xiao},
      year={2024},
      journal = {arXiv preprint arXiv:2402.11809},
      url= {https://arxiv.org/abs/2402.11809},
}

@inproceedings{li24b,
  author    = {Li, Yuhui and Wei, Fangyun and Zhang, Chao and Zhang, Hongyang},
  title     = {EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees},
  year      = {2024},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP 2024)},
  address   = {Miami, Florida, USA},
}

@inproceedings{gong24,
  author    = {Gong, Zhuocheng and Liu, Jiahao and Wang, Ziyue and Wu, Pengfei and Wang, Jingang and Cai, Xunliang and Zhao, Dongyan and Yan, Rui},
  title     = {Graph-Structured Speculative Decoding},
  year      = {2024},
  booktitle = {Findings of the Association for Computational Linguistics (ACL 2024)},
  address   = {Bangkok, Thailand},
}

@inproceedings{hu24,
  author    = {Zhengmian Hu and Heng Huang},
  title     = {Accelerated Speculative Sampling Based on Tree Monte Carlo},
  year      = {2024},
  booktitle = {Forty-first International Conference on Machine Learning (ICML 2024)},
  url       = {https://openreview.net/forum?id=stMhi1Sn2G},
}

@inproceedings{anonymous24a,
  author    = {Anonymous},
  title     = {Optimized Multi-Token Joint Decoding With Auxiliary Model for LLM Inference},
  year      = {2025},
  booktitle = {The Thirteenth International Conference on Learning Representations (ICLR 2025)},
}

@inproceedings{anonymous24b,
  author    = {Anonymous},
  title     = {Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model Alignment},
  year      = {2025},
  booktitle = {The Thirteenth International Conference on Learning Representations (ICLR 2025)},
}





@inproceedings{li2023contrastive,
  title={Contrastive Decoding: Open-ended Text Generation as Optimization},
  author={Li, Xiang Lisa and Holtzman, Ari and Fried, Daniel and Liang, Percy and Eisner, Jason and Hashimoto, Tatsunori B and Zettlemoyer, Luke and Lewis, Mike},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={12286--12312},
  year={2023}
}

@inproceedings{Liu2024decoding,
 title = {Decoding-time Realignment of Language Models},
 author={Liu, Tianlin and Guo, Shangmin and Bianco, Leonardo and Calandriello, Daniele and Berthet, Quentin and Llinares, Felipe and Hoffmann, Jessica and Dixon, Lucas and Valko, Michal and Blondel, Mathieu},
 booktitle = {Proceedings of the International Conference on Machine Learning},
 year = {2024}
}

@article{tekin2024llm,
  title={Llm-topla: Efficient llm ensemble by maximising diversity},
  author={Tekin, Selim Furkan and Ilhan, Fatih and Huang, Tiansheng and Hu, Sihao and Liu, Ling},
  journal={arXiv preprint arXiv:2410.03953},
  year={2024}
}

@article{wang2024mllm,
  title={Mllm can see? dynamic correction decoding for hallucination mitigation},
  author={Wang, Chenxi and Chen, Xiang and Zhang, Ningyu and Tian, Bozhong and Xu, Haoming and Deng, Shumin and Chen, Huajun},
  journal={arXiv preprint arXiv:2410.11779},
  year={2024}
}

@inproceedings{zhou2024distillspec,
  title={DistillSpec: Improving Speculative Decoding via Knowledge Distillation},
  author={Zhou, Yongchao and Lyu, Kaifeng and Rawat, Ankit Singh and Menon, Aditya Krishna and Rostamizadeh, Afshin and Kumar, Sanjiv and Kagy, Jean-Fran{\c{c}}ois and Agarwal, Rishabh},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and Millican, Katie and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{llama3,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{liu2024deepseek,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

@article{bai2023qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@inproceedings{yu2024breaking,
    title = "Breaking the Ceiling of the {LLM} Community by Treating Token Generation as a Classification for Ensembling",
    author = "Yu, Yao-Ching  and
      Kuo, Chun Chih  and
      Ziqi, Ye  and
      Yucheng, Chang  and
      Li, Yueh-Se",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.99/",
    doi = "10.18653/v1/2024.findings-emnlp.99",
    pages = "1826--1839",
}

@article{li2024purifying,
  title={Purifying large language models by ensembling a small language model},
  author={Li, Tianlin and Liu, Qian and Pang, Tianyu and Du, Chao and Guo, Qing and Liu, Yang and Lin, Min},
  journal={arXiv preprint arXiv:2402.14845},
  year={2024}
}

@article{chen2021codex,
  title={Evaluating Large Language Models Trained on Code},
  author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
  year={2021},
  eprint={2107.03374},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@article{cobbe2021gsm8k,
  title={Training Verifiers to Solve Math Word Problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{hendryckstest2021,
  title={Measuring Massive Multitask Language Understanding},
  author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
  journal={Proceedings of the International Conference on Learning Representations (ICLR)},
  year={2021}
}

@inproceedings{cnndm,
    title = "Get To The Point: Summarization with Pointer-Generator Networks",
    author = "See, Abigail  and
      Liu, Peter J.  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P17-1099",
    doi = "10.18653/v1/P17-1099",
    pages = "1073--1083",
}

@article{llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@misc{vicuna,
      title={Judging LLM-as-a-judge with MT-Bench and Chatbot Arena},
      author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric. P Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2306.05685},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{qwen2.5,
    title = {Qwen2.5: A Party of Foundation Models},
    url = {https://qwenlm.github.io/blog/qwen2.5/},
    author = {Qwen Team},
    month = {September},
    year = {2024}
}

@misc{zhang2022opt,
      title={OPT: Open Pre-trained Transformer Language Models}, 
      author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
      year={2022},
      eprint={2205.01068},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Le Scao, Teven and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  year={2023}
}

@inproceedings{scd,
    title = "Speculative Contrastive Decoding",
    author = "Yuan, Hongyi  and
      Lu, Keming  and
      Huang, Fei  and
      Yuan, Zheng  and
      Zhou, Chang",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-short.5/",
    doi = "10.18653/v1/2024.acl-short.5",
    pages = "56--64",
}

@article{lu2024merge,
  title={Merge, ensemble, and cooperate! a survey on collaborative strategies in the era of large language models},
  author={Lu, Jinliang and Pang, Ziliang and Xiao, Min and Zhu, Yaochen and Xia, Rui and Zhang, Jiajun},
  journal={arXiv preprint arXiv:2407.06089},
  year={2024}
}

@inproceedings{shnitzer2023large,
  title={Large Language Model Routing with Benchmark Datasets},
  author={Shnitzer, Tal and Ou, Anthony and Silva, Mirian and Soule, Kate and Sun, Yuekai and Solomon, Justin and Thompson, Neil and Yurochkin, Mikhail},
  booktitle={Annual Conference on Neural Information Processing Systems},
  year={2023}
}

@article{lu2023routing,
  title={Routing to the expert: Efficient reward-guided ensemble of large language models},
  author={Lu, Keming and Yuan, Hongyi and Lin, Runji and Lin, Junyang and Yuan, Zheng and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2311.08692},
  year={2023}
}

@article{lu2024blending,
  title={Blending is all you need: Cheaper, better alternative to trillion-parameters llm},
  author={Lu, Xiaoding and Liu, Zongyi and Liusie, Adian and Raina, Vyas and Mudupalli, Vineet and Zhang, Yuwen and Beauchamp, William},
  journal={arXiv preprint arXiv:2401.02994},
  year={2024}
}

@article{ding2024hybrid,
  title={Hybrid LLM: Cost-efficient and quality-aware query routing},
  author={Ding, Dujian and Mallick, Ankur and Wang, Chi and Sim, Robert and Mukherjee, Subhabrata and Ruhle, Victor and Lakshmanan, Laks VS and Awadallah, Ahmed Hassan},
  journal={arXiv preprint arXiv:2404.14618},
  year={2024}
}

@inproceedings{hoang2024fly,
  title={On-the-Fly Fusion of Large Language Models and Machine Translation},
  author={Hoang, Hieu and Khayrallah, Huda and Junczys-Dowmunt, Marcin},
  booktitle={Findings of the Association for Computational Linguistics: NAACL 2024},
  pages={520--532},
  year={2024}
}

@article{chen2023frugalgpt,
  title={Frugalgpt: How to use large language models while reducing cost and improving performance},
  author={Chen, Lingjiao and Zaharia, Matei and Zou, James},
  journal={arXiv preprint arXiv:2305.05176},
  year={2023}
}

@inproceedings{lee2023ensemble,
  title={Ensemble-Instruct: Generating Instruction-Tuning Data with a Heterogeneous Mixture of LMs},
  author={Lee, Young-Suk and Sultan, Arafat and El-Kurdi, Yousef and Naseem, Tahira and Munawar, Asim and Florian, Hans and Roukos, Salim and Astudillo, Ram{\'o}n Fernandez},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2023}
}

@inproceedings{jiang2023llm,
  title={LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion},
  author={Jiang, Dongfu and Ren, Xiang and Lin, Bill Yuchen},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={14165--14178},
  year={2023}
}

@article{furkan2024llm,
  title={LLM-TOPLA: Efficient LLM Ensemble by Maximising Diversity},
  author={Furkan Tekin, Selim and Ilhan, Fatih and Huang, Tiansheng and Hu, Sihao and Liu, Ling},
  journal={arXiv e-prints},
  pages={arXiv--2410},
  year={2024}
}