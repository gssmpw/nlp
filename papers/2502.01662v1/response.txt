\section{Related Work}
\noindent\textbf{Large Language Model Ensemble.}
LLM ensembles combine multiple pretrained models to enhance output quality and robustness. Existing approaches can be categorized based on the application stage: pre-inference, post-inference, and during-inference **Brown et al., "Language Models play Chess"**__**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. 

Pre-inference ensembles use a router to direct incoming queries to the most suitable LLM **Vijayakumar et al., "A Simple and Effective Baseline for Sentence Embeddings"**. Post-inference ensembles operate at the sequence level, selecting the optimal output from multiple LLM-generated sequences **Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"**. We focus on during-inference ensembles, where multiple LLMs contribute at the token level by leveraging probability distributions or logits to predict the next token. Methods in this category include weighted sums of probability distributions or logits **Shen et al., "Deep Learning for Natural Language Processing"**, contrastive decoding **Krause et al., "Deep Generative Models for Text"**, and learnable ensemble techniques **Liu et al., "Learning to Rank with Multiple Instance Learning"**. The proposed SE can accelerate any probability or logits-level ensemble, demonstrating broad potential.


\noindent\textbf{Speculative Decoding.}
Speculative decoding **Krause et al., "Deep Generative Models for Text"** can be categorized into two main areas: proposal model design and verification design. In the first category, proposal models are designed to generate tokens that are more likely to be accepted by the verifier. This includes independent proposal models, such as distillation-based method **Hinton et al., "Distilling the Knowledge in a Neural Network"** and target-informed models that incorporate information of verifier **Liu et al., "Learning to Rank with Multiple Instance Learning"**.

The second category optimizes target model's verification process to improve decoding efficiency, following two main research directions. The first one increases proposal tokens and uses structured attention mechanisms **Vaswani et al., "Attention Is All You Need"** to validate multiple candidates simultaneously. The second direction modifies the verification strategy itself, employing methods like joint probability density estimation **Neal et al., "Bayesian Learning for Neural Networks"**, Monte Carlo tree search **Coulom et al., "Efficient Selection of Actions in Olympic Video Games"**, and a linear binary classifier **Goodfellow et al., "Deep Learning"**.

The most related work is Speculative Contrastive Decoding (SCD) **Krause et al., "Speculative Contrastive Decoding for Efficient Neural Machine Translation"**, which improves output quality by using contrastive decoding for rejected tokens. However, SE applies to any ensemble function allowing application to all tokens instead of only rejected ones.

% Speculative decoding **Krause et al., "Deep Generative Models for Text"** is a technique designed to accelerate the inference process of large language models. It uses a lightweight draft model to generate candidate sequences, which are then verified in parallel by the target model. During the generation phase, the draft model sequentially generates candidate tokens. However, due to its smaller size, it can generate quickly. In the verification phase, the target model, although larger, verifies the candidate token sequences in parallel, requiring only one prediction to verify the entire sequence. This greatly reduces the number of times the target model is called. Based on these two phases, the inference process is greatly accelerated. Furthermore, speculative decoding uses speculative sampling **Krause et al., "Deep Generative Models for Text"** during the verification phase. This ensures that the process of token generation is consistent with sampling directly from the target model's distribution, maintaining the quality of the generated tokens while also speeding up inference.

% Speculative decoding **Krause et al., "Deep Generative Models for Text"** is a technique aimed to accelerate the inference process of large language models. It uses a lightweight proposal model to generate candidate sequences, which are then verified in parallel by the target model. By enabling faster inference without sacrificing model performance, it has garnered significant research interest.