\section{Related Work}
\textbf{Mobile AI Agent.}
Many recent studies have proposed mobile AI agents for device control. Appagent ____ and Mobileagent ____  use prompt engineering methods and rely on existing closed-source models (e.g., GPT-4V, GPT-4o) to achieve mobile control. Other studies like CogAgent ____ and Agenttuning ____ use data-driven methods to fine-tune the open-source VLMs.
In order to improve the device control abilities of mobile AI agents, large-scale datasets with diverse scenarios and accurate annotations are needed.

Rico ____ and AITW ____  are two publicly available large-scale GUI datasets, containing 72,219 single-step GUI tasks and 715,142 multi-step page navigation tasks, respectively. They are widely used in multiple GUI modeling works ____.
However, they are constructed by combining crowdsourcing workers and automated annotation, which also leads to noise and wrong labels. 
To this end, a series of published works clean and filter these datasets to improve their quality.
Enrico ____, UI-Bert ____, Vins ____ extended the Rico dataset and proposed new GUI tasks such as UI layout classification and UI element retrieval. AutoUI ____ further filtered the GooglePlay tasks in the AitW dataset. However, these datasets mainly contain screenshots and OCR text, lacking GUI raw data such as xml documents, which limits the agent to further obtain mobile environment information. 
%Ferret ____ and Amex ____ have detailed annotations for each element in the GUI page and are able to locate actions to specific elements. These datasets mainly focus on element learning, and the annotation process involves humans, so it is expensive and time-consuming to use such methods to improve existing large-scale GUI datasets.
MobileVLM ____ and ScreenAI ____ build large-scale pre-training datasets to enhance the agentâ€™s UI understanding ability. Here, Mobile3M ____ proposed by MobileVLM uses a breadth-first approach to explore the GUI pages of each APP and forms these pages into a graph structure. This allows us to obtain environmental information about different paths and operations, for example, the next pages after performing different actions in a page, and how many paths there are from one page to another.
However, as a pre-training dataset, MobileVLM lacks fine-tuning tasks. Most of their tasks are UI understanding and single-step action generation. A small number of multi-step tasks can be summarized as navigate to a page with a certain button.

Therefore, based on Mobile3M, we extracted available GUI flows and annotated a page navigation dataset. Using the graph structure environment, we further extracted page reaching and page operation datasets, and scored and paired preference datasets for RL learning. We used the three page datasets for one-stage SFT fine-tuning of ReachAgent and the preference datasets for two-stage RL optimization.


\textbf{Reinforcement Learning (RL).}
RL-based fine-tuning methods have shown great potential in improving the generation quality of LLMs ____. For mobile device control, some agents use RL methods to further optimize fine-tuned models. 

DigRL ____ uses Gemini 1.5 Pro ____ as an automatic evaluator to assign rewards to each GUI flow generated by the agent, and updates the model with the annotated GUI flows through online RL. DistRL ____ uses multiple workers to collect interaction data and then sends them to a central learner for learning, focusing more on distributed frameworks. These works are limited by data collection efficiency and labor-intensive manual annotation, and therefore cannot be well expanded. 

Our work combines agent generation and graph path sampling to obtain a large number of GUI flows, uses page reaching and operation subtasks to set reward rules, and builds preference data for each step of the GUI flow.