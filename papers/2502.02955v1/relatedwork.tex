\section{Related Work}
\textbf{Mobile AI Agent.}
Many recent studies have proposed mobile AI agents for device control. Appagent \cite{yang2023appagent} and Mobileagent \cite{ding2024mobileagent}  use prompt engineering methods and rely on existing closed-source models (e.g., GPT-4V, GPT-4o) to achieve mobile control. Other studies like CogAgent \cite{hong2023cogagent} and Agenttuning \cite{zeng2023agenttuning} use data-driven methods to fine-tune the open-source VLMs.
In order to improve the device control abilities of mobile AI agents, large-scale datasets with diverse scenarios and accurate annotations are needed.

Rico \cite{deka2017rico} and AITW \cite{rawles2023android}  are two publicly available large-scale GUI datasets, containing 72,219 single-step GUI tasks and 715,142 multi-step page navigation tasks, respectively. They are widely used in multiple GUI modeling works \cite{wang2021screen2words,li2021screen2vec,hsiao2022screenqa}.
However, they are constructed by combining crowdsourcing workers and automated annotation, which also leads to noise and wrong labels. 
To this end, a series of published works clean and filter these datasets to improve their quality.
Enrico \cite{leiva2020enrico}, UI-Bert \cite{bai2021uibert}, Vins \cite{bunian2021vins} extended the Rico dataset and proposed new GUI tasks such as UI layout classification and UI element retrieval. AutoUI \cite{zhan2023you} further filtered the GooglePlay tasks in the AitW dataset. However, these datasets mainly contain screenshots and OCR text, lacking GUI raw data such as xml documents, which limits the agent to further obtain mobile environment information. 
%Ferret \cite{you2023ferret,you2024ferret,zhang2024ferret} and Amex \cite{chai2024amex} have detailed annotations for each element in the GUI page and are able to locate actions to specific elements. These datasets mainly focus on element learning, and the annotation process involves humans, so it is expensive and time-consuming to use such methods to improve existing large-scale GUI datasets.
MobileVLM \cite{wu2024mobilevlm} and ScreenAI \cite{baechler2024screenai} build large-scale pre-training datasets to enhance the agentâ€™s UI understanding ability. Here, Mobile3M \cite{wu2024mobilevlm} proposed by MobileVLM uses a breadth-first approach to explore the GUI pages of each APP and forms these pages into a graph structure. This allows us to obtain environmental information about different paths and operations, for example, the next pages after performing different actions in a page, and how many paths there are from one page to another.
However, as a pre-training dataset, MobileVLM lacks fine-tuning tasks. Most of their tasks are UI understanding and single-step action generation. A small number of multi-step tasks can be summarized as navigate to a page with a certain button.

Therefore, based on Mobile3M, we extracted available GUI flows and annotated a page navigation dataset. Using the graph structure environment, we further extracted page reaching and page operation datasets, and scored and paired preference datasets for RL learning. We used the three page datasets for one-stage SFT fine-tuning of ReachAgent and the preference datasets for two-stage RL optimization.


\textbf{Reinforcement Learning (RL).}
RL-based fine-tuning methods have shown great potential in improving the generation quality of LLMs \cite{liu2023rltf, shen2023pangu,yuan2023rrhf}. For mobile device control, some agents use RL methods to further optimize fine-tuned models. 

DigRL \cite{bai2024digirl} uses Gemini 1.5 Pro \cite{team2024gemini} as an automatic evaluator to assign rewards to each GUI flow generated by the agent, and updates the model with the annotated GUI flows through online RL. DistRL \cite{wang2024distrl} uses multiple workers to collect interaction data and then sends them to a central learner for learning, focusing more on distributed frameworks. These works are limited by data collection efficiency and labor-intensive manual annotation, and therefore cannot be well expanded. 

Our work combines agent generation and graph path sampling to obtain a large number of GUI flows, uses page reaching and operation subtasks to set reward rules, and builds preference data for each step of the GUI flow.