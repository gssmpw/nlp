[
  {
    "index": 0,
    "papers": [
      {
        "key": "yang2023appagent",
        "author": "Yang, Zhao and Liu, Jiaxuan and Han, Yucheng and Chen, Xin and Huang, Zebiao and Fu, Bin and Yu, Gang",
        "title": "AppAgent: Multimodal Agents as Smartphone Users"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "ding2024mobileagent",
        "author": "Ding, Tinghe",
        "title": "MobileAgent: enhancing mobile control via human-machine interaction and SOP integration"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "hong2023cogagent",
        "author": "Hong, Wenyi and Wang, Weihan and Lv, Qingsong and Xu, Jiazheng and Yu, Wenmeng and Ji, Junhui and Wang, Yan and Wang, Zihan and Dong, Yuxiao and Ding, Ming and others",
        "title": "CogAgent: A Visual Language Model for GUI Agents"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "zeng2023agenttuning",
        "author": "Zeng, Aohan and Liu, Mingdao and Lu, Rui and Wang, Bowen and Liu, Xiao and Dong, Yuxiao and Tang, Jie",
        "title": "Agenttuning: Enabling generalized agent abilities for llms"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "deka2017rico",
        "author": "Deka, Biplab and Huang, Zifeng and Franzen, Chad and Hibschman, Joshua and Afergan, Daniel and Li, Yang and Nichols, Jeffrey and Kumar, Ranjitha",
        "title": "Rico: A mobile app dataset for building data-driven design applications"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "rawles2023android",
        "author": "Rawles, Christopher and Li, Alice and Rodriguez, Daniel and Riva, Oriana and Lillicrap, Timothy",
        "title": "Android in the wild: A large-scale dataset for android device control"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "wang2021screen2words",
        "author": "Wang, Bryan and Li, Gang and Zhou, Xin and Chen, Zhourong and Grossman, Tovi and Li, Yang",
        "title": "Screen2words: Automatic mobile UI summarization with multimodal learning"
      },
      {
        "key": "li2021screen2vec",
        "author": "Li, Toby Jia-Jun and Popowski, Lindsay and Mitchell, Tom and Myers, Brad A",
        "title": "Screen2vec: Semantic embedding of gui screens and gui components"
      },
      {
        "key": "hsiao2022screenqa",
        "author": "Hsiao, Yu-Chung and Zubach, Fedir and Baechler, Gilles and Carbune, Victor and Lin, Jason and Wang, Maria and Sunkara, Srinivas and Zhu, Yun and Chen, Jindong",
        "title": "Screenqa: Large-scale question-answer pairs over mobile app screenshots"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "leiva2020enrico",
        "author": "Leiva, Luis A and Hota, Asutosh and Oulasvirta, Antti",
        "title": "Enrico: A dataset for topic modeling of mobile UI designs"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "bai2021uibert",
        "author": "Bai, Chongyang and Zang, Xiaoxue and Xu, Ying and Sunkara, Srinivas and Rastogi, Abhinav and Chen, Jindong and others",
        "title": "Uibert: Learning generic multimodal representations for ui understanding"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "bunian2021vins",
        "author": "Bunian, Sara and Li, Kai and Jemmali, Chaima and Harteveld, Casper and Fu, Yun and Seif El-Nasr, Magy Seif",
        "title": "Vins: Visual search for mobile user interface design"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "zhan2023you",
        "author": "Zhan, Zhuosheng and Zhang, Aston",
        "title": "You only look at screens: Multimodal chain-of-action agents"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "you2023ferret",
        "author": "You, Haoxuan and Zhang, Haotian and Gan, Zhe and Du, Xianzhi and Zhang, Bowen and Wang, Zirui and Cao, Liangliang and Chang, Shih-Fu and Yang, Yinfei",
        "title": "Ferret: Refer and ground anything anywhere at any granularity"
      },
      {
        "key": "you2024ferret",
        "author": "You, Keen and Zhang, Haotian and Schoop, Eldon and Weers, Floris and Swearngin, Amanda and Nichols, Jeffrey and Yang, Yinfei and Gan, Zhe",
        "title": "Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs"
      },
      {
        "key": "zhang2024ferret",
        "author": "Zhang, Haotian and You, Haoxuan and Dufter, Philipp and Zhang, Bowen and Chen, Chen and Chen, Hong-You and Fu, Tsu-Jui and Wang, William Yang and Chang, Shih-Fu and Gan, Zhe and others",
        "title": "Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "chai2024amex",
        "author": "Chai, Yuxiang and Huang, Siyuan and Niu, Yazhe and Xiao, Han and Liu, Liang and Zhang, Dingyu and Gao, Peng and Ren, Shuai and Li, Hongsheng",
        "title": "AMEX: Android Multi-annotation Expo Dataset for Mobile GUI Agents"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "wu2024mobilevlm",
        "author": "Wu, Qinzhuo and Xu, Weikai and Liu, Wei and Tan, Tao and Liu, Jianfeng and Li, Ang and Luan, Jian and Wang, Bin and Shang, Shuo",
        "title": "MobileVLM: A Vision-Language Model for Better Intra-and Inter-UI Understanding"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "baechler2024screenai",
        "author": "Baechler, Gilles and Sunkara, Srinivas and Wang, Maria and Zubach, Fedir and Mansoor, Hassan and Etter, Vincent and C{\\u{a}}rbune, Victor and Lin, Jason and Chen, Jindong and Sharma, Abhanshu",
        "title": "ScreenAI: A Vision-Language Model for UI and Infographics Understanding"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "wu2024mobilevlm",
        "author": "Wu, Qinzhuo and Xu, Weikai and Liu, Wei and Tan, Tao and Liu, Jianfeng and Li, Ang and Luan, Jian and Wang, Bin and Shang, Shuo",
        "title": "MobileVLM: A Vision-Language Model for Better Intra-and Inter-UI Understanding"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "liu2023rltf",
        "author": "Liu, Jiate and Zhu, Yiqin and Xiao, Kaiwen and Fu, Qiang and Han, Xiao and Yang, Wei and Ye, Deheng",
        "title": "RLTF: Reinforcement Learning from Unit Test Feedback"
      },
      {
        "key": "shen2023pangu",
        "author": "Shen, Bo and Zhang, Jiaxin and Chen, Taihong and Zan, Daoguang and Geng, Bing and Fu, An and Zeng, Muhan and Yu, Ailun and Ji, Jichuan and Zhao, Jingyang and others",
        "title": "Pangu-coder2: Boosting large language models for code with ranking feedback"
      },
      {
        "key": "yuan2023rrhf",
        "author": "Yuan, Hongyi and Yuan, Zheng and Tan, Chuanqi and Wang, Wei and Huang, Songfang and Huang, Fei",
        "title": "RRHF: Rank Responses to Align Language Models with Human Feedback"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "bai2024digirl",
        "author": "Bai, Hao and Zhou, Yifei and Cemri, Mert and Pan, Jiayi and Suhr, Alane and Levine, Sergey and Kumar, Aviral",
        "title": "Digirl: Training in-the-wild device-control agents with autonomous reinforcement learning"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "team2024gemini",
        "author": "Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others",
        "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "wang2024distrl",
        "author": "Wang, Taiyi and Wu, Zhihao and Liu, Jianheng and Hao, Jianye and Wang, Jun and Shao, Kun",
        "title": "Distrl: An asynchronous distributed reinforcement learning framework for on-device control agents"
      }
    ]
  }
]