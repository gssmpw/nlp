@article{baechler2024screenai,
  title={ScreenAI: A Vision-Language Model for UI and Infographics Understanding},
  author={Baechler, Gilles and Sunkara, Srinivas and Wang, Maria and Zubach, Fedir and Mansoor, Hassan and Etter, Vincent and C{\u{a}}rbune, Victor and Lin, Jason and Chen, Jindong and Sharma, Abhanshu},
  journal={arXiv preprint arXiv:2402.04615},
  year={2024}
}

@article{bai2021uibert,
  title={Uibert: Learning generic multimodal representations for ui understanding},
  author={Bai, Chongyang and Zang, Xiaoxue and Xu, Ying and Sunkara, Srinivas and Rastogi, Abhinav and Chen, Jindong and others},
  journal={arXiv preprint arXiv:2107.13731},
  year={2021}
}

@article{bai2024digirl,
  title={Digirl: Training in-the-wild device-control agents with autonomous reinforcement learning},
  author={Bai, Hao and Zhou, Yifei and Cemri, Mert and Pan, Jiayi and Suhr, Alane and Levine, Sergey and Kumar, Aviral},
  journal={arXiv preprint arXiv:2406.11896},
  year={2024}
}

@inproceedings{bunian2021vins,
  title={Vins: Visual search for mobile user interface design},
  author={Bunian, Sara and Li, Kai and Jemmali, Chaima and Harteveld, Casper and Fu, Yun and Seif El-Nasr, Magy Seif},
  booktitle={Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--14},
  year={2021}
}

@article{chai2024amex,
  title={AMEX: Android Multi-annotation Expo Dataset for Mobile GUI Agents},
  author={Chai, Yuxiang and Huang, Siyuan and Niu, Yazhe and Xiao, Han and Liu, Liang and Zhang, Dingyu and Gao, Peng and Ren, Shuai and Li, Hongsheng},
  journal={arXiv preprint arXiv:2407.17490},
  year={2024}
}

@inproceedings{deka2017rico,
  title={Rico: A mobile app dataset for building data-driven design applications},
  author={Deka, Biplab and Huang, Zifeng and Franzen, Chad and Hibschman, Joshua and Afergan, Daniel and Li, Yang and Nichols, Jeffrey and Kumar, Ranjitha},
  booktitle={Proceedings of the 30th annual ACM symposium on user interface software and technology},
  pages={845--854},
  year={2017}
}

@article{ding2024mobileagent,
  title={MobileAgent: enhancing mobile control via human-machine interaction and SOP integration},
  author={Ding, Tinghe},
  journal={arXiv preprint arXiv:2401.04124},
  year={2024}
}

@article{hong2023cogagent,
  title={CogAgent: A Visual Language Model for GUI Agents},
  author={Hong, Wenyi and Wang, Weihan and Lv, Qingsong and Xu, Jiazheng and Yu, Wenmeng and Ji, Junhui and Wang, Yan and Wang, Zihan and Dong, Yuxiao and Ding, Ming and others},
  journal={arXiv preprint arXiv:2312.08914},
  year={2023}
}

@article{hsiao2022screenqa,
  title={Screenqa: Large-scale question-answer pairs over mobile app screenshots},
  author={Hsiao, Yu-Chung and Zubach, Fedir and Baechler, Gilles and Carbune, Victor and Lin, Jason and Wang, Maria and Sunkara, Srinivas and Zhu, Yun and Chen, Jindong},
  journal={arXiv preprint arXiv:2209.08199},
  year={2022}
}

@inproceedings{leiva2020enrico,
  title={Enrico: A dataset for topic modeling of mobile UI designs},
  author={Leiva, Luis A and Hota, Asutosh and Oulasvirta, Antti},
  booktitle={22nd International Conference on Human-Computer Interaction with Mobile Devices and Services},
  pages={1--4},
  year={2020}
}

@inproceedings{li2021screen2vec,
  title={Screen2vec: Semantic embedding of gui screens and gui components},
  author={Li, Toby Jia-Jun and Popowski, Lindsay and Mitchell, Tom and Myers, Brad A},
  booktitle={Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--15},
  year={2021}
}

@article{liu2023rltf,
  title={RLTF: Reinforcement Learning from Unit Test Feedback},
  author={Liu, Jiate and Zhu, Yiqin and Xiao, Kaiwen and Fu, Qiang and Han, Xiao and Yang, Wei and Ye, Deheng},
  journal={arXiv preprint arXiv:2307.04349},
  year={2023}
}

@article{rawles2023android,
  title={Android in the wild: A large-scale dataset for android device control},
  author={Rawles, Christopher and Li, Alice and Rodriguez, Daniel and Riva, Oriana and Lillicrap, Timothy},
  journal={arXiv preprint arXiv:2307.10088},
  year={2023}
}

@article{shen2023pangu,
  title={Pangu-coder2: Boosting large language models for code with ranking feedback},
  author={Shen, Bo and Zhang, Jiaxin and Chen, Taihong and Zan, Daoguang and Geng, Bing and Fu, An and Zeng, Muhan and Yu, Ailun and Ji, Jichuan and Zhao, Jingyang and others},
  journal={arXiv preprint arXiv:2307.14936},
  year={2023}
}

@article{team2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@inproceedings{wang2021screen2words,
  title={Screen2words: Automatic mobile UI summarization with multimodal learning},
  author={Wang, Bryan and Li, Gang and Zhou, Xin and Chen, Zhourong and Grossman, Tovi and Li, Yang},
  booktitle={The 34th Annual ACM Symposium on User Interface Software and Technology},
  pages={498--510},
  year={2021}
}

@article{wang2024distrl,
  title={Distrl: An asynchronous distributed reinforcement learning framework for on-device control agents},
  author={Wang, Taiyi and Wu, Zhihao and Liu, Jianheng and Hao, Jianye and Wang, Jun and Shao, Kun},
  journal={arXiv preprint arXiv:2410.14803},
  year={2024}
}

@article{wu2024mobilevlm,
  title={MobileVLM: A Vision-Language Model for Better Intra-and Inter-UI Understanding},
  author={Wu, Qinzhuo and Xu, Weikai and Liu, Wei and Tan, Tao and Liu, Jianfeng and Li, Ang and Luan, Jian and Wang, Bin and Shang, Shuo},
  journal={arXiv preprint arXiv:2409.14818},
  year={2024}
}

@article{yang2023appagent,
  title={AppAgent: Multimodal Agents as Smartphone Users},
  author={Yang, Zhao and Liu, Jiaxuan and Han, Yucheng and Chen, Xin and Huang, Zebiao and Fu, Bin and Yu, Gang},
  journal={arXiv preprint arXiv:2312.13771},
  year={2023}
}

@article{you2023ferret,
  title={Ferret: Refer and ground anything anywhere at any granularity},
  author={You, Haoxuan and Zhang, Haotian and Gan, Zhe and Du, Xianzhi and Zhang, Bowen and Wang, Zirui and Cao, Liangliang and Chang, Shih-Fu and Yang, Yinfei},
  journal={arXiv preprint arXiv:2310.07704},
  year={2023}
}

@article{you2024ferret,
  title={Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs},
  author={You, Keen and Zhang, Haotian and Schoop, Eldon and Weers, Floris and Swearngin, Amanda and Nichols, Jeffrey and Yang, Yinfei and Gan, Zhe},
  journal={arXiv preprint arXiv:2404.05719},
  year={2024}
}

@inproceedings{yuan2023rrhf,
  title={RRHF: Rank Responses to Align Language Models with Human Feedback},
  author={Yuan, Hongyi and Yuan, Zheng and Tan, Chuanqi and Wang, Wei and Huang, Songfang and Huang, Fei},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023}
}

@article{zeng2023agenttuning,
  title={Agenttuning: Enabling generalized agent abilities for llms},
  author={Zeng, Aohan and Liu, Mingdao and Lu, Rui and Wang, Bowen and Liu, Xiao and Dong, Yuxiao and Tang, Jie},
  journal={arXiv preprint arXiv:2310.12823},
  year={2023}
}

@article{zhan2023you,
  title={You only look at screens: Multimodal chain-of-action agents},
  author={Zhan, Zhuosheng and Zhang, Aston},
  journal={arXiv preprint arXiv:2309.11436},
  year={2023}
}

@article{zhang2024ferret,
  title={Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models},
  author={Zhang, Haotian and You, Haoxuan and Dufter, Philipp and Zhang, Bowen and Chen, Chen and Chen, Hong-You and Fu, Tsu-Jui and Wang, William Yang and Chang, Shih-Fu and Gan, Zhe and others},
  journal={arXiv preprint arXiv:2404.07973},
  year={2024}
}

