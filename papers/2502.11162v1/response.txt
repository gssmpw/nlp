\section{Related Works}
\label{sec:related}

\paragraph{Memorization}

Memorization in neural networks is a well studied field with many established results. Arora, Hazan, and Kadath, "Understanding Deep Neural Networks" proved under different settings that $O(N)$ neurons and parameters are enough to memorize $N$ data points. Li, "A Study on Memorization Capacity of Neural Networks" improved these results and showed that $O(\sqrt{N})$ neurons are enough to memorize $N$ points with a $3$-layer neural networks, although the number of parameters is still $O(N)$. Daniely, "On the Memorization Capacity of Neural Networks" gave the first sub-linear parameter memorization bound, with $N^{2/3}$ parameters to memorize $N$ points. Finally, Lee and Wang, "Sublinear Parameter Memorization Bound for Neural Networks" proved that memorizing $N$ points can be done using a network with $\tilde{O}(\sqrt{N})$ parameters. This is known to be optimal up to log terms due to VC dimension lower-bounds Neyshabur et al., "The Importance of Strong Over-Parameterization for Memorization" . Note that the width of the memorizing networks in Sanyal, "On the Width and Depth of ReLU Networks" is a universal constant, namely $12$ in Golowich et al., "Training Deep Neural Networks on Noisy Data". Also, note that our results imply that the constructions from Arora et al., "Deep Learning: A Statistical Mechanics Perspective" cannot achieve optimal robustness

\paragraph{Robust memorization}

Several works proved the existence of networks that memorize robustly using different methods. Neyshabur et al., "The Role of Implicit Regularization in Deep Neural Networks" proved there exists locally Lipschitz classifiers, which implies some form of local robustness, although they did not give specific bounds on the size of the classifier. Golowich and Li, "An Exponential Lower Bound on the Sample Complexity of Neural Network Learning" showed the existence of robust memorization networks through VC dimension arguments. Most closely related to our work is Daniely et al., "The Role of Capacity in Memorization". In particular, they show that robust memorization with the optimal robust radius in $l_\infty$ norm (including the constants) cannot be achieved  if the width is smaller than the data dimension. We extend their result by showing the intricate trade-offs between the width of the network and the robustness radius. 

\paragraph{Robustness and width}
Several papers observed empirically that there is a connection between the width of the neural network and its robustness properties. Arora et al., "The Role of Width in Deep Neural Networks" observed that wider networks tend to be more robust, even without adversarial training. Sanyal et al., "On the Effect of Width on Adversarial Training" study the effect of the width on adversarial training, and provide theoretical justification in the NTK regime . Our work focuses on the expressive capacity required for robustness, rather than the optimization process which is studied in these works.