\section{Related Works}
\label{sec:related}

\paragraph{Memorization}

Memorization in neural networks is a well studied field with many established results. \cite{baum1988capabilities,bubeck2020network,huang1991bounds,huang1998upper,sartori1991simple,zhang2021understanding} proved under different settings that $O(N)$ neurons and parameters are enough to memorize $N$ data points. \cite{huang2003learning,yun2019small} improved these results and showed that $O(\sqrt{N})$ neurons are enough to memorize $N$ points with a $3$-layer neural networks, although the number of parameters is still $O(N)$.  \cite{park2021provable} gave the first sub-linear parameter memorization bound, with $N^{2/3}$ parameters to memorize $N$ points. Finally, \cite{vardi2021optimal} proved that memorizing $N$ points can be done using a network with $\tilde{O}(\sqrt{N})$ parameters. This is known to be optimal up to log terms due to VC dimension lower-bounds \citep{goldberg1995bounding,bartlett2019nearly}. Note that the width of the memorizing networks in \cite{park2021provable,vardi2021optimal} is a universal constant, namely $12$ in \cite{vardi2021optimal}. Also, note that our results imply that the constructions from \cite{park2021provable,vardi2021optimal} cannot achieve optimal robustness

\paragraph{Robust memorization}

Several works proved the existence of networks that memorize robustly using different methods. \cite{yang2020closer,bastounis2021mathematics} proved there exists locally Lipschitz classifiers, which implies some form of local robustness, although they did not give specific bounds on the size of the classifier. \cite{li2022robust} showed the existence of robust memorization networks through VC dimension arguments. Most closely related to our work is \cite{yuoptimal}, which proves upper and lower bounds for robust memorization. In particular, they show that robust memorization with the optimal robust radius in $l_\infty$ norm (including the constants) cannot be achieved  if the width is smaller than the data dimension. We extend their result by showing the intricate trade-offs between the width of the network and the robustness radius. 

\paragraph{Robustness and width}
Several papers observed empirically that there is a connection between the width of the neural network and its robustness properties. \cite{madry2017towards} observed that wider networks tend to be more robust, even without adversarial training. \cite{wu2021wider,zhu2022robustness} study the effect of the width on adversarial training, and provide theoretical justification in the NTK regime \citep{jacot2018neural,allen2019convergence,gao2019convergence}. Our work focuses on the expressive capacity required for robustness, rather than the optimization process which is studied in these works.