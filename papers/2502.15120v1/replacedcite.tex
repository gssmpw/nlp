\section{Literature reviews}
% ____ show that LMs (GPT-3.5-turbo-0613 ____, Gemini-Pro (Jan. 2024) ____,  Llama2-70B ____, and Mixtral-8x7B ____) can be distracted by irrelevant rationales that are unhelpful for solving a given question in the CoT exemplars.

% ____ identify the layers in which "task recognition" occurs using causal
% masking over different parts of the context and conduct exploratory studies into the extent to which subsequent layers are either redundant or corresponding to the "task recognition" layers on machine translation and code generation tasks.

% ____ show that ICL for word analogy completion on a frequently co-occurring word pair can arise by modeling word co-occurrence using the continuous bag-of-words model ____ trained on frequently co-occurring tokens in the training dataset, without needing positional information or attention mechanisms, they find out that positional information is essential when the ICL task is to predict the first token in a sentence, and they find out that their designed transformer with learned positional embedding results in higher accuracy for ICL to complete a token pairs compared to sinusoidal or rotary positional encoding ____ if there are disturbing tokens between the word pairs in the exemplars.

% ____ show that the success rate of solving procedural reasoning tasks using their considered LMs (GPT-4 ____, Claude-3-Opus ____, and GPT-4-Turbo) prompted by the CoT techniques ____ decreases as the generality of the prompt increases regardless of the number of sub-goals and the success rate also decreases as the number of sub-goals increases, regardless of the specificity of the CoT prompt.