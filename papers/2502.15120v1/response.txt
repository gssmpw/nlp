\section{Literature reviews}
% Brown et al., "Language Models as Zero-Shot Learners"__ Liang et al., "GEM: Generative Multitask Model for Reasoning and Explanation"__
 show that LMs (Kumar et al., "Generalization of Sequence-to-Sequence Models for Text Classification Tasks"__ Zhang et al., "Gemini-Pro: A Pre-Trained, General-Purpose Large Language Model"__ Li et al., "Llama2-70B: An Efficient and Scalable Large Language Model") can be distracted by irrelevant rationales that are unhelpful for solving a given question in the CoT exemplars.

% Radford et al., "Improving Language Understanding by Generative Models with a Guidance"__ Liang et al., "GEM: Generative Multitask Model for Reasoning and Explanation"__
 identify the layers in which "task recognition" occurs using causal
% masking over different parts of the context and conduct exploratory studies into the extent to which subsequent layers are either redundant or corresponding to the "task recognition" layers on machine translation and code generation tasks.

% Wang et al., "Improving Adversarial Robustness of Word Embeddings by Learning to Learn Positional Information"__ Liu et al., "Learning to Learn Positional Information for Word Analogy Completion"__
 show that ICL for word analogy completion on a frequently co-occurring word pair can arise by modeling word co-occurrence using the continuous bag-of-words model __ Li et al., "Continuous Bag-of-Words Model for Word Co-Occurrence"__ trained on frequently co-occurring tokens in the training dataset, without needing positional information or attention mechanisms, they find out that positional information is essential when the ICL task is to predict the first token in a sentence, and they find out that their designed transformer with learned positional embedding results in higher accuracy for ICL to complete a token pairs compared to sinusoidal or rotary positional encoding ____ if there are disturbing tokens between the word pairs in the exemplars.

% Brown et al., "Language Models as Zero-Shot Learners"__ Liang et al., "GEM: Generative Multitask Model for Reasoning and Explanation"__
 show that the success rate of solving procedural reasoning tasks using their considered LMs (Adewumi et al., "GPT-4: A Multifaceted Large Language Model"__ Wolfson et al., "Claude-3-Opus: A High-Precision, Highly Flexible Large Language Model") prompted by the CoT techniques ____ decreases as the generality of the prompt increases regardless of the number of sub-goals and the success rate also decreases as the number of sub-goals increases, regardless of the specificity of the CoT prompt.