\section{Literature reviews}
% \citet{zhou2024can} show that LMs (GPT-3.5-turbo-0613 \citep{floridi2020gpt}, Gemini-Pro (Jan. 2024) \citep{chowdhery2022palmscalinglanguagemodeling},  Llama2-70B \citep{touvron2023llama2openfoundation}, and Mixtral-8x7B \citep{jiang2024mixtralexperts}) can be distracted by irrelevant rationales that are unhelpful for solving a given question in the CoT exemplars.

% \citet{sia2024where} identify the layers in which "task recognition" occurs using causal
% masking over different parts of the context and conduct exploratory studies into the extent to which subsequent layers are either redundant or corresponding to the "task recognition" layers on machine translation and code generation tasks.

% \citet{wibisono2024from} show that ICL for word analogy completion on a frequently co-occurring word pair can arise by modeling word co-occurrence using the continuous bag-of-words model \citep{mikolov2013efficient} trained on frequently co-occurring tokens in the training dataset, without needing positional information or attention mechanisms, they find out that positional information is essential when the ICL task is to predict the first token in a sentence, and they find out that their designed transformer with learned positional embedding results in higher accuracy for ICL to complete a token pairs compared to sinusoidal or rotary positional encoding \citep{su2024roformer} if there are disturbing tokens between the word pairs in the exemplars.

% \citet{stechly2024chain} show that the success rate of solving procedural reasoning tasks using their considered LMs (GPT-4 \citep{openai2024gpt4technicalreport}, Claude-3-Opus \citep{anthropic2023claude3}, and GPT-4-Turbo) prompted by the CoT techniques \citep{wei2022chain} decreases as the generality of the prompt increases regardless of the number of sub-goals and the success rate also decreases as the number of sub-goals increases, regardless of the specificity of the CoT prompt.