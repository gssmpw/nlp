\section{Discussion}\label{sec:discussion}

In this section, we present a comprehensive overview of the research articles reviewed in this study. Additionally, we provide a qualitative assessment of these articles. Initially, a set of criteria has been established for evaluating the papers, ensuring these criteria adequately reflect the core content of each article without favoring a particular subset. This involved a meticulous review of the articles and consultation with multiple co-authors to mitigate individual biases. Following this procedure, several criteria were identified for our qualitative assessment, including the use of large datasets (sample size greater than 5000), publicly available datasets, publication of accompanying code, fault detection across multiple components, employment of image modalities beyond visible light, application of advanced image processing techniques beyond resizing and cropping, use of synthetic and augmented data, focus on small components such as bolts, localization of specific components or faults, provision of performance metrics, acknowledgment of limitations, and suitability for real-time deployment. Based on these criteria, we conducted a thorough evaluation of all selected papers, and the findings are summarized in Table \ref{tab:quality_assesment}.

{\scriptsize
\begin{longtable}{| l | l | l | p{0.25cm} p{0.25cm} p{0.25cm} p{0.25cm} p{0.25cm} p{0.25cm} p{0.25cm} p{0.25cm} p{0.25cm} p{0.25cm} p{0.25cm} p{0.25cm} p{0.5cm} |}
\caption{Assessment of the reviewed literature}\label{tab:quality_assesment} \\
\hline
\rotatebox{90}{Number} & \rotatebox{90}{Authors} & \rotatebox{90}{Year of Publication} & \rotatebox{90}{Dataset Size $>$ 5000} & \rotatebox{90}{Dataset Availability} & \rotatebox{90}{Code Availability} & \rotatebox{90}{Multi-Component} & \rotatebox{90}{Other Image Modalities} & \rotatebox{90}{Image Processing} & \rotatebox{90}{Synthetic Data} & \rotatebox{90}{Data Augmentation} & \rotatebox{90}{Small Object} & \rotatebox{90}{Fault Localization} & \rotatebox{90}{Performance Metrics} & \rotatebox{90}{Limitations} & \rotatebox{90}{Real-Time} \\
\hline
1 & Liu et al. \cite{liu_discrimination_2017} & 2017 & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ \\
2 & Tao et al. \cite{tao2018detection} & 2018 & $\times$ & \checkmark & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark & \checkmark & $\times$ & \checkmark & \checkmark & $\times$ & \checkmark \\
3 & Kang et al. \cite{kang_deep_2019} & 2019 & \checkmark & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ & \checkmark & \checkmark & $\times$ & $\times$ \\
4 & Li et al. \cite{li_image_2019} & 2019 & \checkmark & $\times$ & $\times$ & \checkmark & \checkmark & \checkmark & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ \\
5 & Miao et al. \cite{miao_insulator_2019} & 2019 & \checkmark & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & \checkmark & \checkmark & $\times$ & \checkmark \\
6 & Chen et al. \cite{chen_research_2019} & 2019 & \checkmark & $\times$ & $\times$ & \checkmark & $\times$ & \checkmark & $\times$ & \checkmark & $\times$ & \checkmark & \checkmark & $\times$ & \checkmark \\
7 & Jiang et al. \cite{jiang_insulator_2019} & 2019 & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & \checkmark & \checkmark & \checkmark & $\times$ \\
8 & Nguyen et al. \cite{nguyen_intelligent_2019} & 2019 & \checkmark & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & \checkmark & \checkmark & $\times$ & $\times$ \\
9 & Sadykova et al. \cite{sadykova2019yolo} & 2019 & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & \checkmark & \checkmark & $\times$ & \checkmark \\
10 & Wang et al. \cite{wang_image_2019} & 2019 & \checkmark & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ \\
11 & Sadykova et al. \cite{sadykova2019yolo} & 2019 & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ & \checkmark & $\times$ & \checkmark \\
12 & Ibrahim et al. \cite{ibrahim_application_2020} & 2020 & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ \\
13 & Mussina et al. \cite{mussina_multi_modal_2020} & 2020 & \checkmark & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ & \checkmark & $\times$ & \checkmark \\
14 & Wang et al. \cite{wang_detection_2020} & 2020 & $\times$ & \checkmark & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark \\
15 & Zhang et al. \cite{zhang_multi_scale_2020} & 2020 & \checkmark & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ & \checkmark & $\times$ & \checkmark & \checkmark & \checkmark & $\times$ & $\times$ \\
16 & Zhang et al. \cite{zhang_cloud_edge_2020} & 2020 & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & \checkmark & \checkmark & $\times$ & \checkmark \\
17 & Zhao et al. \cite{zhao_detection_2020} & 2020 & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & $\times$ \\
18 & Zhao et al. \cite{zhao2020image} & 2020 & \checkmark & \checkmark & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ \\
19 & Zhu et al. \cite{zhu_deep_2020} & 2020 & \checkmark & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark \\
20 & Odo et al. \cite{odo_aerial_2021} & 2021 & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & $\times$ \\
21 & Singh et al. \cite{singh_design_2021} & 2021 & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ \\
22 & Waleed et al. \cite{waleed_drone_based_2021} & 2021 & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark \\
23 & Xiao et al. \cite{xiao_detection_2021} & 2021 & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark & $\times$ & $\times$ \\
24 & Zhang et al. \cite{zhang_defgan_2021} & 2021 & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark & $\times$ & \checkmark & \checkmark & \checkmark & $\times$ \\
25 & Zhai et al. \cite{zhai_hybrid_2021} & 2021 & $\times$ & $\times$ & $\times$ &
\checkmark & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark & $\times$ & $\times$ \\
26 & Rong et al. \cite{rong_intelligent_2021} & 2021 & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark & $\times$ & $\times$ \\
27 & Zhang et al. \cite{zhang_defgan_2021} & 2021 & $\times$ & \checkmark & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark & $\times$ & \checkmark & \checkmark & $\times$ & $\times$ \\
28 & Antwi-Bekoe et al. \cite{antwi_bekoe_deep_2022} & 2022 & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & \checkmark & \checkmark & \checkmark & $\times$ \\
29 & Ge et al. \cite{ge_birds_2022} & 2022 & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & \checkmark & \checkmark & $\times$ & \checkmark \\
30 & Hao et al. \cite{hao_insulator_2022} & 2022 & $\times$ & \checkmark & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark & $\times$ & \checkmark & \checkmark & $\times$ & \checkmark \\
31 & Hunag et al. \cite{huang_structural_2023} & 2022 & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark & $\times$ & \checkmark \\
32 & Li et al. \cite{li_improved_2022} & 2022& \checkmark & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & \checkmark & \checkmark & $\times$ & \checkmark \\
33 & Li et al. \cite{li_pin_2022} & 2022 & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark & $\times$ \\
34 & Qiu et al. \cite{qiu_lightweight_2023} & 2022 & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & \checkmark & $\times$ & \checkmark & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark \\
35 & Stefenon et al. \cite{stefenon_classification_2022} & 2022 & $\times$ & \checkmark & $\times$ & \checkmark & $\times$ & \checkmark & $\times$ & \checkmark & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ \\
36 & Stefenon et al. \cite{stefenon_semi_protopnet_2022} & 2022 & $\times$ & \checkmark & $\times$ & \checkmark & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ & \checkmark & \checkmark & $\times$ \\
37 & Wei et al. \cite{wei_online_2022} & 2022 & \checkmark & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & \checkmark & \checkmark & $\times$ & $\times$ \\
38 & Yang et al. \cite{yang_vision_based_2022} & 2022 & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ & $\times$ \\
39 & Yi et al. \cite{yi_intelligent_2022} & 2022 & \checkmark & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ \\
40 & Zhai et al. \cite{zhai_multi_fitting_2022} & 2022 & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark & $\times$ & $\times$ \\
41 & Zhang et al. \cite{zhang_attention_guided_2022} & 2022 & \checkmark & \checkmark & \checkmark & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark & $\times$ & \checkmark & \checkmark & $\times$ & \checkmark \\
42 & Zhang et al. \cite{zhang_attention_guided_2022} & 2022 & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark & \checkmark & $\times$ & $\times$ \\
43 & Zhao et al. \cite{zhao_new_2022} & 2022 & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & \checkmark & $\times$ & $\times$ \\
44 & Bi et al. \cite{bi_yolox_2023} & 2023 & $\times$ & \checkmark & $\times$ & \checkmark & $\times$ & $\times$ & \checkmark & \checkmark & $\times$ & \checkmark & \checkmark & \checkmark & $\times$ \\
45 & Cao et al. \cite{cao_accurate_2023} & 2023 & \checkmark & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & \checkmark & $\times$ & \checkmark & \checkmark & $\times$ & $\times$ \\
46 & Dong et al. \cite{dong_improved_2023} & 2023 & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark & $\times$ & $\times$ \\
47 & Fu et al. \cite{fu_small_sized_2023} & 2023 & $\times$ & \checkmark & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark & $\times$ & \checkmark & \checkmark & $\times$ & $\times$ \\
48 & Luo et al. \cite{luo_ultrasmall_2023} & 2023 & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark & \checkmark & $\times$ & $\times$ \\
49 & Roy et al. \cite{roy_accurate_2023} & 2023 & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & \checkmark \\
50 & Shuang et al. \cite{shuang_rsin_dataset_2023} & 2023 & $\times$ & \checkmark & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark \\
51 & Singh et al. \cite{singh_2023_interpretable} & 2023 & $\times$ & \checkmark & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & \checkmark & \checkmark & $\times$ & $\times$ \\
52 & Song et al. \cite{song_deformable_2023} & 2023 & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark & $\times$ & \checkmark \\
53 & Wang et al. \cite{wang_internal_2023} & 2023 & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark & $\times$ & \checkmark & $\times$ & \checkmark & \checkmark & \checkmark & $\times$ \\
54 & Yang et al. \cite{yang_dra_net_2023} & 2023 & $\times$ & \checkmark & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark & $\times$ & $\times$ \\
55 & Yu et al. \cite{yu_foreign_2023} & 2023 & \checkmark & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark \\
56 & Zhang et al. \cite{zhang_edge_2023} & 2023 & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & \checkmark & \checkmark & $\times$ & $\times$ \\
57 & Zhang et al. \cite{zhang_multi_objects_2023} & 2023 & $\times$ & \checkmark & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark & $\times$ & \checkmark & \checkmark & $\times$ & \checkmark \\
58 & Zhang et al. \cite{zhang_pa_detr_2023} & 2023 & \checkmark & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & \checkmark & $\times$ & \checkmark \\
59 & Zhou et al. \cite{zhou_insulator_2023} & 2023 & \checkmark & $\times$ & \checkmark & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark & $\times$ & $\times$ \\
60 & Zhang et al. \cite{zhang_pa_detr_2023} & 2023 & \checkmark & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & \checkmark & $\times$ & \checkmark \\
61 & Hao et al. \cite{hao2023pkamnet} & 2023 & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & \checkmark & $\times$ & \checkmark & \checkmark & \checkmark & $\times$ \\
62 & Liu et al. \cite{liu2022component} & 2023 & $\times$ & $\times$ & \checkmark & \checkmark & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark & $\times$ & $\times$ \\
63 & Liu et al. \cite{liu2023fault} & 2023 & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ \\
64 & Jiao et al. \cite{jiao2023defective} & 2023 & \checkmark & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & $\times$ \\
65 & Zhong et al. \cite{zhong2024visual} & 2024 & \checkmark & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ & \checkmark & \checkmark & \checkmark & $\times$ \\
66 & Zhang et al. \cite{zhang2023dsa} & 2024 & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\
67 & Yi et al. \cite{yi2023pstl} & 2024 & $\times$ & \checkmark & \checkmark & \checkmark & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark & $\times$ \\
68 & Wang et al. \cite{wang2024mci} & 2024 & $\times$ & \checkmark & \checkmark & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark & $\times$ & \checkmark & \checkmark & \checkmark & $\times$ \\
69 & Shi et al. \cite{shi2024lskf} & 2024 & $\times$ & \checkmark & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark & $\times$ & \checkmark & \checkmark & $\times$ \\
70 & Liu et al. \cite{liu2023tower} & 2024 & $\times$ & $\times$ & \checkmark & \checkmark & $\times$ & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & $\times$ & $\times$ \\
71 & Jain et al. \cite{jain2024transfer} & 2024 & \checkmark & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ & \checkmark & $\times$ & \checkmark & \checkmark & $\times$ & $\times$ \\
72 & Dong et al. \cite{dong2024transmission} & 2024 & \checkmark & $\times$ & $\times$ & \checkmark & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark & $\times$ \\
73 & Bergmann et al. \cite{bergmann2024approach} & 2024 & $\times$ & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark & \checkmark & $\times$ & $\times$ & \checkmark & \checkmark & \checkmark & $\times$ \\
\hline 
& & & $30\%$ & $23\%$ & $8\%$ & $34\%$ & $8\%$ & $19\%$ & $19\%$ & $67\%$ & $30\%$ & $78\%$ & $96\%$ & $33\%$ & $33\%$ \\
\hline
\end{longtable}
}

Table \ref{tab:quality_assesment} reveals several noteworthy insights regarding the articles reviewed. The scarcity of publicly available datasets is evident, with only 23\% of the papers utilizing them, while the majority rely on privately generated datasets. This issue has been discussed in Section \ref{sec:datasets}, “Publicly Available Datasets”. Moreover, large datasets are seldom used, with only 30\% of the articles meeting our 5000-sample threshold. To address this, numerous studies have incorporated synthetic or augmented data, with image augmentation being particularly prevalent, utilized in 67\% of the studies.

The publication of source code alongside machine learning or deep learning papers is highly beneficial for enabling readers to replicate algorithms and reproduce results, provided the dataset is also available. Although not always possible, sharing the source code enhances the credibility and impact of the research. However, among the articles reviewed, only 6 out of 73 published their source code.

Detecting faults across various components presents significant challenges due to the diverse nature of components and faults, such as the noticeable size difference between power line insulators and bolts \cite{nguyen_intelligent_2019, liang_detection_2020}. Despite these challenges, 34\% of the papers successfully employed multi-task learning techniques to address this issue, demonstrating effective results and highlighting further research opportunities in generalizing across more component types, faults, and environments. The detection of small objects remains particularly challenging due to their size relative to other components, often requiring sophisticated algorithms with considerable potential for advancement \cite{odo_aerial_2021}.

Unlike periodic inspections, real-time systems offer continuous surveillance, promptly identifying and addressing emergent issues. UAVs equipped with real-time algorithms can rapidly cover extensive areas, delivering immediate data to operators and aiding in the localization of faults, thus reducing the time and labor costs associated with manual inspections \cite{zhang_cloud_edge_2020}. Real-time systems demand algorithms capable of operating at speeds typically around 30 FPS or more on low-powered edge devices \cite{miao_insulator_2019}. Approximately 33\% of the reviewed studies have tackled this challenge by developing performant algorithms suitable for real-time deployment, often relying on rapid object detection algorithms like YOLO \cite{sadykova2019yolo} and SSD \cite{miao_insulator_2019}.

Automated power line inspection is a multi-step process where each step influences subsequent ones. For simplicity and clarity, it can be broken down into these key aspects: the component of interest, the choice of imaging platform, the size of the dataset, the type of detection, and the selected algorithm. When the process involves fault diagnosis, the type of fault should also be considered. By combining these aspects into a flow diagram, with the target component as the starting point and algorithm selection as the endpoint, we can establish a pattern for how decisions are made. With this context in mind, we carefully examined the reviewed literature to construct the diagrams in Figure \ref{fig:sankey}. Figure \ref{fig:sankey}(a) illustrates the decision-making process for automated component detection in power line infrastructure. The diagram shows that most research focused on insulators, utilized UAVs as the imaging platform, used datasets of 1000-5000 samples, and employed bounding box detection. Algorithm choices are balanced among YOLO, RCNN, SSD, or custom architectures. Notably, all works targeting semantic segmentation proposed their own custom network architecture. Similarly, Figure \ref{fig:sankey}(b) reveals the decision-making process for power line fault diagnosis. Again, a large portion of the works targeted insulators, used UAVs as the imaging platform, and collected datasets of 1000-5000 samples. Component fault detection is more common in the literature than foreign object detection. While YOLO, RCNN, and SSD have been used on multiple occasions for fault detection, most works propose custom network architectures.

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.9\linewidth]{fig_7_sankey.pdf}
    \caption{(a) The inter-relation between the different aspects in power line component detection. (b) The inter-relation between the different aspects in power line fault diagnosis.}
    \label{fig:sankey}
\end{figure*}