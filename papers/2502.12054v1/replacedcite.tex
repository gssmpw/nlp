\section{Related Work}
\subsection{Large Language Model Evaluation}
LLMs have demonstrated remarkable performance across various domains, such as math reasoning ____, logical reasoning ____, and text generation ____.
However, they struggle with physics world interactions, limiting their adoption in areas like autonomous driving and robotics ____.
Unlike mathematical and logical reasoning, physics-based reasoning requires the integration of multiple principles and physics-world constraints ____.
Therefore, mastering physics-based reasoning is fundamental to unlocking LLMs' potential in practical applications ____.
Current evaluations primarily focus on mathematical and logical reasoning, revealing a crucial gap in evaluating LLM capabilities based on physics-based reasoning.
% However, they struggle with physics world interactions, limiting their adoption in robot learning and autonomous driving.
% Unlike mathematical reasoning, physics-based reasoning requires integration of principles and physics-world constraints ____.
% Mastering physics-based reasoning is key to unlocking LLMs' potential in practical applications ____.
% Current evaluations focus on mathematical and logical reasoning, revealing a crucial gap in physics-based reasoning assessment of LLMs.
% This misalignment reveals a critical gap in LLM capability assessment.
% physics-based reasoning mastery is key to advancing LLMs in real-world applications ____.
% Large Language Models (LLMs) have achieved remarkable success across various domains, particularly excelling in mathematical reasoning ____, logical reasoning ____, text generation ____.
% These models, while proficient in abstract thinking and instruction execution, face significant limitations in physics world interactions.
% physics-based reasoning demands multiple skills: interpreting data, building theoretical models, applying Physics Theorems, and considering contextual factors ____.
% There exists a notable disparity between current LLM evaluation methods, which predominantly focus on abstract reasoning, and the comprehensive requirements of physics-based reasoning.
% This misalignment highlights a crucial gap in our assessment of LLM capabilities.
% Mastering physics-based reasoning, is fundamental to unlocking LLMs' potential across practical applications and bridging the gap between computational intelligence and real-world problem-solving ____.
% \newcommand{\ccline}[2]{%
%   \cline{#1}\morecmidrules\cline{#2}%
% }
\subsection{Physics Benchmarks}
Existing physics benchmarks span three knowledge complexity levels: K-12 (ScienceQA ____, E-EVAL ____), college-level (MMLU ____, AGIEval ____, JEEBench ____, TheoremQA ____, EMMA ____, SciEval ____, C-Eval-STEM ____, SciBench ____), and expert-level (OlympiadBench____, GPQA ____).
% (1) K-12 benchmarks like ScienceQA ____ and E-EVAL ____ focusing on basic concepts; (2) college-level benchmarks such as MMLU-STEM ____, AGIEval ____, JEEBench ____, TheoremQA ____, EMMA ____ and C-Eval-STEM ____, covering multistep problems; and (3) expert-level benchmarks including SciBench ____, OlympiadBench____ and GPQA ____ testing doctoral-level physics. 
While these benchmarks showcase LLMs' knowledge breadth, they simplify reasoning to 3-4 steps and emphasize only final answers.
PhysReason addresses these gaps through complex reasoning process and step-level evaluation.
\begin{table*}
\centering
\caption{Comparative analysis of our PhysReason with other physics-based reasoning benchmarks.
For \textbf{Knowledge}, COMP: Competition, COL: College, CEE: College Entrance Examination, K1-K12: Elementary and High School, PH.D: Doctor of Philosophy;
% For answer type, Num: Numeric value, Exp: Expression, Equ: Equation, Opt: Option;
For \textbf{question type}, OE: Open-ended, MC: Multiple-choice, Avg. T: Average Tokens;
For \textbf{solution type}, Avg. S: Average Steps.}
\vspace{-5pt}
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{lcccccc@{\hspace{5pt}}ccc}
% \begin{tabular}{lccccccccc}
\hline
\multirow{2}{*}{Benchmark} & \multirow{2}{*}{Multi-modal} & \multirow{2}{*}{Size} & \multirow{2}{*}{Knowledge} & \multicolumn{2}{c}{Question} & \multicolumn{3}{c}{Solution} \\
\cmidrule(r){5-6} \cmidrule(l){7-9}
  &        &           &      & Type & Avg. T & Step-by-step & Avg. T  & Avg. S\\
\hline
JEEBench       & {\color{red}\ding{55}}  & 123  & CEE & OE,MC & 169.7 & -  & - & -\\
MMLU-Pro       &  {\color{red}\ding{55}} & 1299 & COL & MC    & 52.1  & -   & - & -\\
% AGIEval        &              & 200  & CEE & OE,MC & 100.38 & -   & - & -\\
GPQA           & {\color{red}\ding{55}} & 227  & PH.D. & OE    & 111.4 & {\color{red}\ding{55}} & 197.2 & 3.6\\
SciEval        & {\color{red}\ding{55}} & 1657 & - & OE,MC & 154.5 & -  & - & -\\
SciBench       & \color{green}\ding{51} & 295  & COL & OE    & 80.5  & {\color{red}\ding{55}} & 315.9 & 2.8\\
MMMU           & \color{green}\ding{51} & 443  & COL & OE,MC & 53.8  & - & - & -\\
ScienceQA      & \color{green}\ding{51} & 617  & K1-K12 & MC    & 13.3  & {\color{red}\ding{55}} & 63.0 & 2.4\\
OlympiadBench  & \color{green}\ding{51} & 2334 & COMP & OE    & 222.0 & {\color{red}\ding{55}} & 199.8 & 3.7\\
EMMA           & \color{green}\ding{51} & 156  & - & MC    & 109.5 & -  & - & -\\
\hline
Ours-Knowledge  & \color{green}\ding{51} & 300  & CEE+COMP & OE    & 163.7 & \color{green}\ding{51} & 196.5 & 3.3\\
Ours-Easy       & \color{green}\ding{51} & 300  & CEE+COMP & OE    & 171.2 & \color{green}\ding{51} & 241.5 & 5.0 \\
Ours-Medium     & \color{green}\ding{51} & 300  & CEE+COMP & OE    & 229.2 & \color{green}\ding{51} & 391.3 & 8.4\\
Ours-Hard  & \color{green}\ding{51} & 300  & CEE+COMP & OE    & 340.9 & \color{green}\ding{51} & 936.1 & 15.6\\
\hline
\rowcolor{gray!20} Ours-Full  & \color{green}\ding{51} & 1200  & CEE+COMP & OE    & 226.3 & \color{green}\ding{51} & 441.3 & 8.1\\
\hline
\end{tabular}
\end{adjustbox}
\label{tab:0}
\vspace{-15pt}
\end{table*}
% These benchmarks effectively demonstrate the expanding knowledge scope of LLMs.
% However, these benchmarks typically oversimplify reasoning to 3-4 steps and focus on answers, making it hard to diagnose models' reasoning failures.
% Our PhysReason benchmark addresses these limitations through comprehensive reasoning chains and step-by-step evaluation.
% \begin{table*}
% \centering
% \begin{adjustbox}{width=\textwidth}
% % \small
% \begin{tabular}{lcccccccccc}
% \hline
% \multirow{2}{*}{Benchmark} & Multi- & \multirow{2}{*}{Size} & \multirow{2}{*}{Konwledge} & \multicolumn{2}{c}{Question} & \multirow{2}{*}{Answer type} & \multicolumn{3}{c}{Solution} \\
% \cline{5-6} \cline{8-10}
% & modal & &  & Type & Avg. T &   & Type & Avg. T   & Avg. S\\
% \hline
% SciBench       & $\checkmark$ & 295  & COL & OE    & 80.51  & Num & WP & 315.85 & 2.79\\
% MMMU           & $\checkmark$ & 443  & COL & OE,MC & 53.82  & Num & - & - & -\\
% ScienceQA      & $\checkmark$ & 617  & K1-K12 & MC    & 13.31  & Opt & WP & 62.95 & 2.40\\
% SciEval        &              & 1657 & - & OE,MC & 154.47 & Num,Opt & -  & - & -\\
% JEEBench       &              & 123  & CEE & OE,MC & 169.69 & Num,Opt & -  & - & -\\
% % MMLU           &              & 411  & COL & MC    & 44.85  & Opt & -   & - & -\\
% MMLU-Pro       &              & 1299 & COL & MC    & 52.08  & Opt & -   & - & -\\
% AGIEval        &              & 200  & CEE & OE,MC & 100.38 & Num & -   & - & -\\
% OlympiadBench  & $\checkmark$ & 2334 & COMP & OE    & 222.03 & Num,Equ,Exp & WP & 199.82 & 3.72\\
% GPQA           &              & 227  & PH.D. & OE    & 111.41 & Num,Equ & WP & 197.17 & 3.58\\
% EMMA           & $\checkmark$ & 156  & - & MC    & 109.47 & Opt & -  & - & -\\
% \hline
% PhysReason-Knowledge  & $\checkmark$ & 300  & CEE+COMP & OE    & 163.68 & Num, Equ, Exp & SBS & 196.48 & 3.31 \\
% PhysReason-Easy       & $\checkmark$ & 300  & CEE+COMP & OE    & 171.21 & Num, Equ, Exp & SBS & 241.52 & 5.00 \\
% PhysReason-Medium     & $\checkmark$ & 300  & CEE+COMP & OE    & 229.19 & Num, Equ, Exp & SBS & 391.28 & 8.39\\
% PhysReason-hard  & $\checkmark$ & 300  & CEE+COMP & OE    & 340.94 & Num, Equ, Exp & SBS & 936.06 & 15.57\\
% \hline
% PhysReason  & $\checkmark$ & 1200  & CEE+COMP & OE    & 226.25 & Num, Equ, Exp & SBS & 441.34 & 8.06\\
% \hline
% \end{tabular}
% \end{adjustbox}
% \vspace{-5pt}
% \caption{Comparison of our benchmark with other physics benchmarks. 
% For difficulty level, COMP: Competition, COL: College, CEE: College Entrance Examination, K1-K12: Elementary and High School, PH.D: Doctor of Philosophy;
% For answer type, Num: Numeric value, Exp: Expression, Equ: Equation, Opt: Option;
% For question type, OE: Open-ended, MC: Multiple-choice;
% For solution type, WP: Whole Paragraph, SBS: Step by step, Avg. T means Average Tokens, Avg. S means Average Steps.}
% \label{tab:0}
% \vspace{-15pt}
% \end{table*}
% \vspace{-5pt}