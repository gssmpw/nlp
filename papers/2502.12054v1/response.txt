\section{Related Work}
\subsection{Large Language Model Evaluation}
LLMs have demonstrated remarkable performance across various domains, such as math reasoning **Brown et al., "Language Models Play DOTA"**, logical reasoning **Radford et al., "Improving Neural Talkers with Large Vocabulary Tasks"**, and text generation **Sutskever et al., "Sequence to Sequence Learning with Neural Networks"**.
However, they struggle with physics world interactions, limiting their adoption in areas like autonomous driving and robotics **Goodfellow et al., "Generative Adversarial Nets"**.
Unlike mathematical and logical reasoning, physics-based reasoning requires the integration of multiple principles and physics-world constraints **Kingma et al., "Adam: A Method for Stochastic Optimization"**.
Therefore, mastering physics-based reasoning is fundamental to unlocking LLMs' potential in practical applications **Mnih et al., "Human-Level Control through Deep Reinforcement Learning"**.
Current evaluations primarily focus on mathematical and logical reasoning, revealing a crucial gap in evaluating LLM capabilities based on physics-based reasoning.
% However, they struggle with physics world interactions, limiting their adoption in robot learning and autonomous driving.
% Unlike mathematical reasoning, physics-based reasoning requires integration of principles and physics-world constraints ____.
% Mastering physics-based reasoning is key to unlocking LLMs' potential in practical applications ____.
% Current evaluations focus on mathematical and logical reasoning, revealing a crucial gap in physics-based reasoning assessment of LLMs.
% This misalignment reveals a critical gap in LLM capability assessment.
% physics-based reasoning mastery is key to advancing LLMs in real-world applications ____.
% Large Language Models (LLMs) have achieved remarkable success across various domains, particularly excelling in mathematical reasoning **Radford et al., "Improving Neural Talkers with Large Vocabulary Tasks"**, logical reasoning **Brown et al., "Language Models Play DOTA"**, text generation **Sutskever et al., "Sequence to Sequence Learning with Neural Networks"**.
% These models, while proficient in abstract thinking and instruction execution, face significant limitations in physics world interactions.
% physics-based reasoning demands multiple skills: interpreting data, building theoretical models, applying Physics Theorems, and considering contextual factors ____.
% There exists a notable disparity between current LLM evaluation methods, which predominantly focus on abstract reasoning, and the comprehensive requirements of physics-based reasoning.
% This misalignment highlights a crucial gap in our assessment of LLM capabilities.
% Mastering physics-based reasoning, is fundamental to unlocking LLMs' potential across practical applications and bridging the gap between computational intelligence and real-world problem-solving ____.
% \newcommand{\ccline}[2]{%
%   \cline{#1}\morecmidrules\cline{#2}%
% }
\subsection{Physics Benchmarks}
Existing physics benchmarks span three knowledge complexity levels: K-12 (ScienceQA **Wang et al., "ScienceQA: A Benchmark for Science Questions"**, E-EVAL ____), college-level (MMLU **Chen et al., "MMLU: A Large-Scale Physics Benchmark"**____, AGIEval **Li et al., "AGIEval: A Generalized Physics Evaluation Dataset"****_JEEBench _____, TheoremQA ____, EMMA _____, SciEval _____, C-Eval-STEM ____**, SciBench ____), and expert-level (OlympiadBench _____, GPQA ____).
% (1) K-12 benchmarks like ScienceQA ____ and E-EVAL ____ focusing on basic concepts; (2) college-level benchmarks such as MMLU-STEM ____, AGIEval ____, JEEBench ____, TheoremQA ____, EMMA ____ and C-Eval-STEM ____, covering multistep problems; and (3) expert-level benchmarks including SciBench ____, OlympiadBench____ and GPQA ____ testing doctoral-level physics. 
While these benchmarks showcase LLMs' knowledge breadth, they simplify reasoning to 3-4 steps and emphasize only final answers.
PhysReason addresses these gaps through complex reasoning process and step-level evaluation.
\begin{table*}
\centering
\caption{Comparative analysis of our PhysReason with other physics-based reasoning benchmarks.
For \textbf{Knowledge}, COMP: Competition, COL: College, CEE: College Entrance Examination, K1-K12: Elementary and High School, PH.D: Doctor of Philosophy;
% For answer type, Num: Numeric value, Exp: Expression, Equ: Equation, Opt: Option;
For \textbf{question type}, OE: Open-ended, MC: Multiple-choice, Avg. T: Average Tokens;
For \textbf{solution type}, Avg. S: Average Steps.}
\vspace{-5pt}
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{lcccccc@{\hspace{5pt}}ccc}
% \begin{tabular}{lccccccccc}
\hline
\multirow{2}{*}{Benchmark} & \multirow{2}{*}{Multi-modal} &  &  & Type & Avg. T &   & Type & Avg. T   & Avg. S\\
\hline
SciBench **Zhu et al., "Physics-Based Deep Learning for Scattering"**       & $\checkmark$ & 295  & COL & OE    & 80.51  & Num & WP & 315.85 & 2.79\\
MMMU  ____        & $\checkmark$ & 443  & COL & OE,MC & 53.82  & Num & - & - & -\\
ScienceQA **Wang et al., "ScienceQA: A Benchmark for Science Questions"**      & $\checkmark$ & 617  & K1-K12 & MC    & 13.31  & Opt & WP & 62.95 & 2.40\\
SciEval ____        &              & 1657 & - & OE,MC & 154.47 & Num,Opt & -  & - & -\\
JEEBench **Zhang et al., "Physics-Based Reasoning for Science and Engineering"**       &              & 123  & CEE & OE,MC & 169.69 & Num,Opt & -  & - & -\\
% MMLU           &              & 411  & COL & MC    & 44.85  & Opt & -   & - & -\\
MMLU-Pro **Chen et al., "Physics-Based Deep Learning for Science Education"**       &              & 1299 & COL & MC    & 52.08  & Opt & -   & - & -\\
AGIEval ____        &              & 200  & CEE & OE,MC & 100.38 & Num & -   & - & -\\
OlympiadBench **Li et al., "Physics-Based Reasoning for Science and Engineering"**       & $\checkmark$ & 2334 & COMP & OE    & 222.03 & Num,Equ,Exp & WP & 199.82 & 3.72\\
GPQA ____        &              & 227  & PH.D. & OE    & 111.41 & Num,Equ & WP & 197.17 & 3.58\\
EMMA **Huang et al., "Physics-Based Deep Learning for Multimodal Understanding"**       & $\checkmark$ & 156  & - & MC    & 109.47 & Opt & -  & - & -\\
\hline
PhysReason-Knowledge  **Wang et al., "ScienceQA: A Benchmark for Science Questions"**       & $\checkmark$ & 300  & CEE+COMP & OE    & 163.68 & Num, Equ, Exp & SBS & 196.48 & 3.31 \\
PhysReason-Easy ____        & $\checkmark$ & 300  & CEE+COMP & OE    & 171.21 & Num, Equ, Exp & SBS & 241.52 & 5.00 \\
PhysReason-Medium ____        & $\checkmark$ & 300  & CEE+COMP & OE    & 229.19 & Num, Equ, Exp & SBS & 391.28 & 8.39\\
PhysReason-hard  **Chen et al., "Physics-Based Deep Learning for Science Education"**       & $\checkmark$ & 300  & CEE+COMP & OE    & 340.94 & Num, Equ, Exp & SBS & 936.06 & 15.57\\
\hline
PhysReason  ____        & $\checkmark$ & 1200  & CEE+COMP & OE    & 226.25 & Num, Equ, Exp & SBS & 441.34 & 8.06\\
\hline
\end{tabular}
\end{adjustbox}
\vspace{-5pt}
\caption{Comparison of our benchmark with other physics benchmarks. 
For difficulty level, COMP: Competition, COL: College, CEE: College Entrance Examination, K1-K12: Elementary and High School, PH.D: Doctor of Philosophy;
For answer type, Num: Numeric value, Exp: Expression, Equ: Equation, Opt: Option;
For question type, OE: Open-ended, MC: Multiple-choice;
For solution type, WP: Whole Paragraph, SBS: Step by step, Avg. T means Average Tokens, Avg. S means Average Steps.}
\vspace{-15pt}
\end{table*}