% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@online{openai_gpt4,
  author = {OpenAI},
  title = {Hello gpt-4o},
  year = {},
  url = {https://openai.com/index/hello-gpt-4o/},
  urldate = {}
}

@online{anthropic_claude,
  author = {Anthropic},
  title = {Claude 3.5 sonnet},
  url = {https://www.anthropic.com/news/claude-3-5-sonnet},
  urldate = {}
}

@online{google_gemini2,
  author = {Google Deepmind},
  title = {Introducing Gemini 2.0: Our New AI Model for the Agentic Era},
  url = {https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/},
  year = {2024}
}

@inproceedings{xu-etal-2024-symbol,
    title = "Symbol-{LLM}: Towards Foundational Symbol-centric Interface For Large Language Models",
    author = "Xu, Fangzhi  and
      Wu, Zhiyong  and
      Sun, Qiushi  and
      Ren, Siyu  and
      Yuan, Fei  and
      Yuan, Shuai  and
      Lin, Qika  and
      Qiao, Yu  and
      Liu, Jun",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.707/",
    doi = "10.18653/v1/2024.acl-long.707",
    pages = "13091--13116"
}

@online{google_gemini2_pro,
  author = {Google Deepmind},
  title = {Introducing Gemini 2.0 Pro},
  url = {https://ai.google.dev/gemini-api/docs/models/gemini#gemini-2.0-pro},
  year = {2025}
}

@online{google_gemini_thinking,
  author = {Google Deepmind},
  title = {Gemini 2.0 Flash Thinking Mode},
  url = {https://ai.google.dev/gemini-api/docs/thinking-mode},
  urldate = {}
}

@article{gao2024omni,
  title={Omni-math: A universal olympiad level mathematic benchmark for large language models},
  author={Gao, Bofei and Song, Feifan and Yang, Zhe and Cai, Zefan and Miao, Yibo and Dong, Qingxiu and Li, Lei and Ma, Chenghao and Chen, Liang and Xu, Runxin and others},
  journal={arXiv preprint arXiv:2410.07985},
  year={2024}
}

@article{son2024varco,
  title={Varco arena: A tournament approach to reference-free benchmarking large language models},
  author={Son, Seonil and Oh, Ju-Min and Jin, Heegon and Jang, Cheolhun and Jeong, Jeongbeom and Kim, Kuntae},
  journal={arXiv preprint arXiv:2411.01281},
  year={2024}
}

@article{yang2024qwen2,
  title={Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement},
  author={Yang, An and Zhang, Beichen and Hui, Binyuan and Gao, Bofei and Yu, Bowen and Li, Chengpeng and Liu, Dayiheng and Tu, Jianhong and Zhou, Jingren and Lin, Junyang and others},
  journal={arXiv preprint arXiv:2409.12122},
  year={2024}
}

@inproceedings{snell2025scaling,
title={Scaling LLM Test-time Compute Optimally Can Be More Effective Than Scaling Model Parameters},
author={Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
booktitle={International Conference on Learning Representations},
year={2025}
}

@online{openai2024learning,
  author = {OpenAI},
  title = {Learning to Reason with {LLMs}},
  year = {2024},
  url = {https://openai.com/index/learning-to-reason-with-llms},
  urldate = {2024-12-05}
}

@article{jain2024livecodebench,
  title={Livecodebench: Holistic and contamination free evaluation of large language models for code},
  author={Jain, Naman and Han, King and Gu, Alex and Li, Wen-Ding and Yan, Fanjia and Zhang, Tianjun and Wang, Sida and Solar-Lezama, Armando and Sen, Koushik and Stoica, Ion},
  journal={arXiv preprint arXiv:2403.07974},
  year={2024}
}

@inproceedings{hendrycks2measuring,
  title={Measuring Mathematical Problem Solving With the MATH Dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)}
}

@inproceedings{lightmanlet,
  title={Let's Verify Step by Step},
  author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yuri and Edwards, Harrison and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@online{o1_mini,
  author = {OpenAI},
  title = {OpenAI o1-mini},
  year = {2024},
  url = {https://openai.com/index/openai-o1-mini-advancing-cost-efficient-reasoning/},
  urldate = {2025-01-31}
}

@online{o3_mini,
  author = {OpenAI},
  title = {OpenAI o3-mini},
  year = {2025},
  url = {https://openai.com/index/openai-o3-mini/},
  urldate = {2025-01-31}
}

@online{google2025gemini_thinking,
  author = {Google Deepmind},
  title = {Gemini 2.0 Flash Thinking Mode},
  year = {2025},
  url = {https://ai.google.dev/gemini-api/docs/thinking-mode},
  urldate = {2025-01-21}
}

@online{glm_zero,
  author = {ZhipuAI},
  title = {GLM-Zero Mode},
  year = {2024},
  url = {https://open.bigmodel.cn/dev/api/normal-model/glm-zero-preview},
  urldate = {2024-12-31}
}

@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde De Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{lu2022learn,
  title={Learn to explain: Multimodal reasoning via thought chains for science question answering},
  author={Lu, Pan and Mishra, Swaroop and Xia, Tanglin and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Kalyan, Ashwin},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={2507--2521},
  year={2022}
}

@inproceedings{wangscibench,
  title={SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models},
  author={Wang, Xiaoxuan and Hu, Ziniu and Lu, Pan and Zhu, Yanqiao and Zhang, Jieyu and Subramaniam, Satyen and Loomba, Arjun R and Zhang, Shichang and Sun, Yizhou and Wang, Wei},
  booktitle={Forty-first International Conference on Machine Learning}
}

@article{lewkowycz2022solving,
  title={Solving quantitative reasoning problems with language models},
  author={Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={3843--3857},
  year={2022}
}

@inproceedings{welbl2017crowdsourcing,
  title={Crowdsourcing Multiple Choice Science Questions},
  author={Welbl, Johannes and Liu, Nelson F and Gardner, Matt},
  booktitle={Proceedings of the 3rd Workshop on Noisy User-generated Text},
  pages={94--106},
  year={2017}
}

@misc{qvq-72b-preview,
    title = {QVQ: To See the World with Wisdom},
    url = {https://qwenlm.github.io/blog/qvq-72b-preview/},
    author = {Qwen Team},
    month = {December},
    year = {2024}
}

@misc{deepseekai2024deepseekv3technicalreport,
      title={DeepSeek-V3 Technical Report}, 
      author={DeepSeek-AI},
      year={2024},
      eprint={2412.19437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.19437}, 
}

@article{wang2024qwen2,
  title={Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

@article{chen2024expanding,
  title={Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling},
  author={Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and others},
  journal={arXiv preprint arXiv:2412.05271},
  year={2024}
}

@misc{qwq-32b-preview,
    title = {QwQ: Reflect Deeply on the Boundaries of the Unknown},
    url = {https://qwenlm.github.io/blog/qwq-32b-preview/},
    author = {Qwen Team},
    month = {November},
    year = {2024}
}

@article{hestenes1987toward,
  title={Toward a modeling theory of physics instruction},
  author={Hestenes, David},
  journal={American journal of physics},
  volume={55},
  number={5},
  pages={440--454},
  year={1987},
  publisher={American Association of Physics Teachers}
}

@inproceedings{rein2024gpqa,
      title={{GPQA}: A Graduate-Level Google-Proof Q\&A Benchmark},
      author={David Rein and Betty Li Hou and Asa Cooper Stickland and Jackson Petty and Richard Yuanzhe Pang and Julien Dirani and Julian Michael and Samuel R. Bowman},
      booktitle={First Conference on Language Modeling},
      year={2024},
      url={https://openreview.net/forum?id=Ti67584b98}
}

@misc{wang2024mineruopensourcesolutionprecise,
      title={MinerU: An Open-Source Solution for Precise Document Content Extraction}, 
      author={Bin Wang and Chao Xu and Xiaomeng Zhao and Linke Ouyang and Fan Wu and Zhiyuan Zhao and Rui Xu and Kaiwen Liu and Yuan Qu and Fukai Shang and Bo Zhang and Liqun Wei and Zhihao Sui and Wei Li and Botian Shi and Yu Qiao and Dahua Lin and Conghui He},
      year={2024},
      eprint={2409.18839},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2409.18839}, 
}


@misc{2023opencompass,
    title={OpenCompass: A Universal Evaluation Platform for Foundation Models},
    author={OpenCompass Contributors},
    howpublished = {\url{https://github.com/open-compass/opencompass}},
    year={2023}
}

@article{chow2025physbench,
  title={PhysBench: Benchmarking and Enhancing Vision-Language Models for Physical World Understanding},
  author={Chow, Wei and Mao, Jiageng and Li, Boyi and Seita, Daniel and Guizilini, Vitor and Wang, Yue},
  journal={arXiv preprint arXiv:2501.16411},
  year={2025}
}

@article{li2024snapkv,
  title={Snapkv: Llm knows what you are looking for before generation},
  author={Li, Yuhong and Huang, Yingbing and Yang, Bowen and Venkitesh, Bharat and Locatelli, Acyr and Ye, Hanchen and Cai, Tianle and Lewis, Patrick and Chen, Deming},
  journal={arXiv preprint arXiv:2404.14469},
  year={2024}
}

@article{zheng2024processbench,
  title={Processbench: Identifying process errors in mathematical reasoning},
  author={Zheng, Chujie and Zhang, Zhenru and Zhang, Beichen and Lin, Runji and Lu, Keming and Yu, Bowen and Liu, Dayiheng and Zhou, Jingren and Lin, Junyang},
  journal={arXiv preprint arXiv:2412.06559},
  year={2024}
}

@article{jia2024chatgen,
  title={ChatGen: Automatic Text-to-Image Generation From FreeStyle Chatting},
  author={Jia, Chengyou and Xia, Changliang and Dang, Zhuohang and Wu, Weijia and Qian, Hangwei and Luo, Minnan},
  journal={arXiv preprint arXiv:2411.17176},
  year={2024}
}

@article{hao2025can,
      title={Can MLLMs Reason in Multimodality? EMMA: An Enhanced MultiModal ReAsoning Benchmark},
      author={Hao, Yunzhuo and Gu, Jiawei and Wang, Huichen Will and Li, Linjie and Yang, Zhengyuan and Wang, Lijuan and Cheng, Yu},
      journal={arXiv preprint arXiv:2501.05444},
      year={2025}
    }

@inproceedings{lumathvista,
  title={MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts},
  author={Lu, Pan and Bansal, Hritik and Xia, Tony and Liu, Jiacheng and Li, Chunyuan and Hajishirzi, Hannaneh and Cheng, Hao and Chang, Kai-Wei and Galley, Michel and Gao, Jianfeng},
  booktitle={The Twelfth International Conference on Learning Representations}
}

@article{zhang2024cmmmu,
  title={CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark},
  author={Zhang, Ge and Du, Xinrun and Chen, Bei and Liang, Yiming and Luo, Tongxu and Zheng, Tianyu and Zhu, Kang and Cheng, Yuyang and Xu, Chunpu and Guo, Shuyue and others},
  journal={arXiv e-prints},
  pages={arXiv--2401},
  year={2024}
}

@inproceedings{chen2022unigeo,
  title={UniGeo: Unifying Geometry Logical Reasoning via Reformulating Mathematical Expression},
  author={Chen, Jiaqi and Li, Tong and Qin, Jinghui and Lu, Pan and Lin, Liang and Chen, Chongyu and Liang, Xiaodan},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={3313--3323},
  year={2022}
}

@inproceedings{chen2021geoqa,
  title={GeoQA: A Geometric Question Answering Benchmark Towards Multimodal Numerical Reasoning},
  author={Chen, Jiaqi and Tang, Jianheng and Qin, Jinghui and Liang, Xiaodan and Liu, Lingbo and Xing, Eric and Lin, Liang},
  booktitle={Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  pages={513--523},
  year={2021}
}

@inproceedings{lu2021inter,
  title={Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning},
  author={Lu, Pan and Gong, Ran and Jiang, Shibiao and Qiu, Liang and Huang, Siyuan and Liang, Xiaodan and Zhu, Song-chun},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={6774--6786},
  year={2021}
}

@inproceedings{arora2023have,
  title={Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models},
  author={Arora, Daman and Singh, Himanshu Gaurav and others},
  booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing}
}

@inproceedings{sun2024scieval,
  title={Scieval: A multi-level large language model evaluation benchmark for scientific research},
  author={Sun, Liangtai and Han, Yang and Zhao, Zihan and Ma, Da and Shen, Zhennan and Chen, Baocai and Chen, Lu and Yu, Kai},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={17},
  pages={19053--19061},
  year={2024}
}

@inproceedings{zhong2024agieval,
  title={AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models},
  author={Zhong, Wanjun and Cui, Ruixiang and Guo, Yiduo and Liang, Yaobo and Lu, Shuai and Wang, Yanlin and Saied, Amin and Chen, Weizhu and Duan, Nan},
  booktitle={Findings of the Association for Computational Linguistics: NAACL 2024},
  pages={2299--2314},
  year={2024}
}

@inproceedings{jiang2024forward,
  title={Forward-backward reasoning in large language models for mathematical verification},
  author={Jiang, Weisen and Shi, Han and Yu, Longhui and Liu, Zhengying and Zhang, Yu and Li, Zhenguo and Kwok, James},
  booktitle={Findings of the Association for Computational Linguistics ACL 2024},
  pages={6647--6661},
  year={2024}
}

@article{liang2024controllable,
  title={Controllable text generation for large language models: A survey},
  author={Liang, Xun and Wang, Hanyu and Wang, Yezhaohui and Song, Shichao and Yang, Jiawei and Niu, Simin and Hu, Jie and Liu, Dan and Yao, Shunyu and Xiong, Feiyu and others},
  journal={arXiv preprint arXiv:2408.12599},
  year={2024}
}

@inproceedings{sun2024determlr,
  title={Determlr: Augmenting llm-based logical reasoning from indeterminacy to determinacy},
  author={Sun, Hongda and Xu, Weikai and Liu, Wei and Luan, Jian and Wang, Bin and Shang, Shuo and Wen, Ji-Rong and Yan, Rui},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={9828--9862},
  year={2024}
}

@inproceedings{imani2023mathprompter,
  title={MathPrompter: Mathematical Reasoning using Large Language Models},
  author={Imani, Shima and Du, Liang and Shrivastava, Harsh},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)},
  pages={37--42},
  year={2023}
}

@inproceedings{lai2024vision,
  title={Vision-language model-based physical reasoning for robot liquid perception},
  author={Lai, Wenqiang and Zhang, Tianwei and Lam, Tin Lun and Gao, Yuan},
  booktitle={2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={9652--9659},
  year={2024},
  organization={IEEE}
}

@inproceedings{zhao2024docmath,
  title={DocMath-eval: Evaluating math reasoning capabilities of LLMs in understanding long and specialized documents},
  author={Zhao, Yilun and Long, Yitao and Liu, Hongjun and Kamoi, Ryo and Nan, Linyong and Chen, Lyuhao and Liu, Yixin and Tang, Xiangru and Zhang, Rui and Cohan, Arman},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={16103--16120},
  year={2024}
}

@article{xu2025large,
  title={Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond},
  author={Xu, Fangzhi and Lin, Qika and Han, Jiawei and Zhao, Tianzhe and Liu, Jun and Cambria, Erik},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2025},
  publisher={IEEE}
}

@dataset{phy_big, 
title={Physics Big}, 
author={Zaharov Timur and Konstantin Korolev and Aleksandr Nikolich}, 
year={2024}, 
url={https://huggingface.co/datasets/Vikhrmodels/physics_big} }

@book{kline1981mathematics,
  title={Mathematics and the physical world},
  author={Kline, Morris},
  year={1981},
  publisher={Courier Corporation}
}

@article{huang2023applications,
  title={Applications of large scale foundation models for autonomous driving},
  author={Huang, Yu and Chen, Yue and Li, Zhu},
  journal={arXiv preprint arXiv:2311.12144},
  year={2023}
}

@inproceedings{gao2024physically,
  title={Physically grounded vision-language models for robotic manipulation},
  author={Gao, Jensen and Sarkar, Bidipta and Xia, Fei and Xiao, Ted and Wu, Jiajun and Ichter, Brian and Majumdar, Anirudha and Sadigh, Dorsa},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={12462--12469},
  year={2024},
  organization={IEEE}
}

@book{laughlin2008different,
  title={A different universe: Reinventing physics from the bottom down},
  author={Laughlin, Robert B},
  year={2008},
  publisher={Basic Books}
}

@inproceedings{chen2023theoremqa,
  title={Theoremqa: A theorem-driven question answering dataset},
  author={Chen, Wenhu and Yin, Ming and Ku, Max and Lu, Pan and Wan, Yixin and Ma, Xueguang and Xu, Jianyu and Wang, Xinyi and Xia, Tony},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={7889--7901},
  year={2023}
}

@inproceedings{lu2iconqa,
  title={IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning},
  author={Lu, Pan and Qiu, Liang and Chen, Jiaqi and Xia, Tony and Zhao, Yizhou and Zhang, Wei and Yu, Zhou and Liang, Xiaodan and Zhu, Song-Chun},
  booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
  year={2021}
}

@article{huang2024c,
  title={C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models},
  author={Huang, Yuzhen and Bai, Yuzhuo and Zhu, Zhihao and Zhang, Junlei and Zhang, Jinghan and Su, Tangjun and Liu, Junteng and Lv, Chuancheng and Zhang, Yikai and Fu, Yao and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{hendrycksmeasuring,
  title={Measuring Massive Multitask Language Understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  booktitle={International Conference on Learning Representations}
}

@inproceedings{hou-etal-2024-e,
    title = "{E}-{EVAL}: A Comprehensive {C}hinese K-12 Education Evaluation Benchmark for Large Language Models",
    author = "Hou, Jinchang  and
      Ao, Chang  and
      Wu, Haihong  and
      Kong, Xiangtao  and
      Zheng, Zhigang  and
      Tang, Daijia  and
      Li, Chengming  and
      Hu, Xiping  and
      Xu, Ruifeng  and
      Ni, Shiwen  and
      Yang, Min",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.462/",
    doi = "10.18653/v1/2024.findings-acl.462",
    pages = "7753--7774",
    abstract = "The rapid development of Large Language Models (LLMs) has led to their increasing utilization in Chinese K-12 education. Despite the growing integration of LLMs and education, the absence of a dedicated benchmark for evaluating LLMs within this domain presents a pressing concern. Consequently, there is an urgent need for a comprehensive natural language processing benchmark to precisely assess the capabilities of various LLMs in Chinese K-12 education. In response, we introduce E-EVAL, the first comprehensive evaluation benchmark specifically tailored for Chinese K-12 education. E-EVAL comprises 4,351 multiple-choice questions spanning primary, middle, and high school levels, covering a diverse array of subjects. Through meticulous evaluation, we find that Chinese-dominant models often outperform English-dominant ones, with many exceeding GPT 4.0. However, most struggle with complex subjects like mathematics. Additionally, our analysis indicates that most Chinese-dominant LLMs do not achieve higher scores at the primary school level compared to the middle school level, highlighting the nuanced relationship between proficiency in higher-order and lower-order knowledge domains. Furthermore, experimental results highlight the effectiveness of the Chain of Thought (CoT) technique in scientific subjects and Few-shot prompting in liberal arts. Through E-EVAL, we aim to conduct a rigorous analysis delineating the strengths and limitations of LLMs in educational applications, thereby contributing significantly to the advancement of Chinese K-12 education and LLMs."
}

@inproceedings{he-etal-2024-olympiadbench,
    title = "{O}lympiad{B}ench: A Challenging Benchmark for Promoting {AGI} with Olympiad-Level Bilingual Multimodal Scientific Problems",
    author = "He, Chaoqun  and
      Luo, Renjie  and
      Bai, Yuzhuo  and
      Hu, Shengding  and
      Thai, Zhen  and
      Shen, Junhao  and
      Hu, Jinyi  and
      Han, Xu  and
      Huang, Yujie  and
      Zhang, Yuxiang  and
      Liu, Jie  and
      Qi, Lei  and
      Liu, Zhiyuan  and
      Sun, Maosong",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.211/",
    doi = "10.18653/v1/2024.acl-long.211",
    pages = "3828--3850",
    abstract = "Recent advancements have seen Large Language Models (LLMs) and Large Multimodal Models (LMMs) surpassing general human capabilities in various tasks, approaching the proficiency level of human experts across multiple domains. With traditional benchmarks becoming less challenging for these models, new rigorous challenges are essential to gauge their advanced abilities. In this work, we present OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring 8,476 problems from Olympiad-level mathematics and physics competitions, including the Chinese college entrance exam. Each problem is detailed with expert-level annotations for step-by-step reasoning. Evaluating top-tier models on OlympiadBench, we implement a comprehensive assessment methodology to accurately evaluate model responses. Notably, the best-performing model, GPT-4V, attains an average score of 17.97{\%} on OlympiadBench, with a mere 10.74{\%} in physics, highlighting the benchmark rigor and the intricacy of physical reasoning. Our analysis orienting GPT-4V points out prevalent issues with hallucinations, knowledge omissions, and logical fallacies. We hope that our challenging benchmark can serve as a valuable resource for helping future AGI research endeavors. The data and evaluation code are available at \url{https://github.com/OpenBMB/OlympiadBench}"
}

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}
