@inproceedings{arora2023have,
  title={Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models},
  author={Arora, Daman and Singh, Himanshu Gaurav and others},
  booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing}
}

@inproceedings{chen2023theoremqa,
  title={Theoremqa: A theorem-driven question answering dataset},
  author={Chen, Wenhu and Yin, Ming and Ku, Max and Lu, Pan and Wan, Yixin and Ma, Xueguang and Xu, Jianyu and Wang, Xinyi and Xia, Tony},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={7889--7901},
  year={2023}
}

@inproceedings{gao2024physically,
  title={Physically grounded vision-language models for robotic manipulation},
  author={Gao, Jensen and Sarkar, Bidipta and Xia, Fei and Xiao, Ted and Wu, Jiajun and Ichter, Brian and Majumdar, Anirudha and Sadigh, Dorsa},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={12462--12469},
  year={2024},
  organization={IEEE}
}

@article{hao2025can,
      title={Can MLLMs Reason in Multimodality? EMMA: An Enhanced MultiModal ReAsoning Benchmark},
      author={Hao, Yunzhuo and Gu, Jiawei and Wang, Huichen Will and Li, Linjie and Yang, Zhengyuan and Wang, Lijuan and Cheng, Yu},
      journal={arXiv preprint arXiv:2501.05444},
      year={2025}
    }

@inproceedings{he-etal-2024-olympiadbench,
    title = "{O}lympiad{B}ench: A Challenging Benchmark for Promoting {AGI} with Olympiad-Level Bilingual Multimodal Scientific Problems",
    author = "He, Chaoqun  and
      Luo, Renjie  and
      Bai, Yuzhuo  and
      Hu, Shengding  and
      Thai, Zhen  and
      Shen, Junhao  and
      Hu, Jinyi  and
      Han, Xu  and
      Huang, Yujie  and
      Zhang, Yuxiang  and
      Liu, Jie  and
      Qi, Lei  and
      Liu, Zhiyuan  and
      Sun, Maosong",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.211/",
    doi = "10.18653/v1/2024.acl-long.211",
    pages = "3828--3850",
    abstract = "Recent advancements have seen Large Language Models (LLMs) and Large Multimodal Models (LMMs) surpassing general human capabilities in various tasks, approaching the proficiency level of human experts across multiple domains. With traditional benchmarks becoming less challenging for these models, new rigorous challenges are essential to gauge their advanced abilities. In this work, we present OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring 8,476 problems from Olympiad-level mathematics and physics competitions, including the Chinese college entrance exam. Each problem is detailed with expert-level annotations for step-by-step reasoning. Evaluating top-tier models on OlympiadBench, we implement a comprehensive assessment methodology to accurately evaluate model responses. Notably, the best-performing model, GPT-4V, attains an average score of 17.97{\%} on OlympiadBench, with a mere 10.74{\%} in physics, highlighting the benchmark rigor and the intricacy of physical reasoning. Our analysis orienting GPT-4V points out prevalent issues with hallucinations, knowledge omissions, and logical fallacies. We hope that our challenging benchmark can serve as a valuable resource for helping future AGI research endeavors. The data and evaluation code are available at \url{https://github.com/OpenBMB/OlympiadBench}"
}

@inproceedings{hendrycksmeasuring,
  title={Measuring Massive Multitask Language Understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  booktitle={International Conference on Learning Representations}
}

@inproceedings{hou-etal-2024-e,
    title = "{E}-{EVAL}: A Comprehensive {C}hinese K-12 Education Evaluation Benchmark for Large Language Models",
    author = "Hou, Jinchang  and
      Ao, Chang  and
      Wu, Haihong  and
      Kong, Xiangtao  and
      Zheng, Zhigang  and
      Tang, Daijia  and
      Li, Chengming  and
      Hu, Xiping  and
      Xu, Ruifeng  and
      Ni, Shiwen  and
      Yang, Min",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.462/",
    doi = "10.18653/v1/2024.findings-acl.462",
    pages = "7753--7774",
    abstract = "The rapid development of Large Language Models (LLMs) has led to their increasing utilization in Chinese K-12 education. Despite the growing integration of LLMs and education, the absence of a dedicated benchmark for evaluating LLMs within this domain presents a pressing concern. Consequently, there is an urgent need for a comprehensive natural language processing benchmark to precisely assess the capabilities of various LLMs in Chinese K-12 education. In response, we introduce E-EVAL, the first comprehensive evaluation benchmark specifically tailored for Chinese K-12 education. E-EVAL comprises 4,351 multiple-choice questions spanning primary, middle, and high school levels, covering a diverse array of subjects. Through meticulous evaluation, we find that Chinese-dominant models often outperform English-dominant ones, with many exceeding GPT 4.0. However, most struggle with complex subjects like mathematics. Additionally, our analysis indicates that most Chinese-dominant LLMs do not achieve higher scores at the primary school level compared to the middle school level, highlighting the nuanced relationship between proficiency in higher-order and lower-order knowledge domains. Furthermore, experimental results highlight the effectiveness of the Chain of Thought (CoT) technique in scientific subjects and Few-shot prompting in liberal arts. Through E-EVAL, we aim to conduct a rigorous analysis delineating the strengths and limitations of LLMs in educational applications, thereby contributing significantly to the advancement of Chinese K-12 education and LLMs."
}

@article{huang2024c,
  title={C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models},
  author={Huang, Yuzhen and Bai, Yuzhuo and Zhu, Zhihao and Zhang, Junlei and Zhang, Jinghan and Su, Tangjun and Liu, Junteng and Lv, Chuancheng and Zhang, Yikai and Fu, Yao and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{imani2023mathprompter,
  title={MathPrompter: Mathematical Reasoning using Large Language Models},
  author={Imani, Shima and Du, Liang and Shrivastava, Harsh},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)},
  pages={37--42},
  year={2023}
}

@inproceedings{jiang2024forward,
  title={Forward-backward reasoning in large language models for mathematical verification},
  author={Jiang, Weisen and Shi, Han and Yu, Longhui and Liu, Zhengying and Zhang, Yu and Li, Zhenguo and Kwok, James},
  booktitle={Findings of the Association for Computational Linguistics ACL 2024},
  pages={6647--6661},
  year={2024}
}

@book{kline1981mathematics,
  title={Mathematics and the physical world},
  author={Kline, Morris},
  year={1981},
  publisher={Courier Corporation}
}

@inproceedings{lai2024vision,
  title={Vision-language model-based physical reasoning for robot liquid perception},
  author={Lai, Wenqiang and Zhang, Tianwei and Lam, Tin Lun and Gao, Yuan},
  booktitle={2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={9652--9659},
  year={2024},
  organization={IEEE}
}

@article{li2024snapkv,
  title={Snapkv: Llm knows what you are looking for before generation},
  author={Li, Yuhong and Huang, Yingbing and Yang, Bowen and Venkitesh, Bharat and Locatelli, Acyr and Ye, Hanchen and Cai, Tianle and Lewis, Patrick and Chen, Deming},
  journal={arXiv preprint arXiv:2404.14469},
  year={2024}
}

@article{liang2024controllable,
  title={Controllable text generation for large language models: A survey},
  author={Liang, Xun and Wang, Hanyu and Wang, Yezhaohui and Song, Shichao and Yang, Jiawei and Niu, Simin and Hu, Jie and Liu, Dan and Yao, Shunyu and Xiong, Feiyu and others},
  journal={arXiv preprint arXiv:2408.12599},
  year={2024}
}

@article{lu2022learn,
  title={Learn to explain: Multimodal reasoning via thought chains for science question answering},
  author={Lu, Pan and Mishra, Swaroop and Xia, Tanglin and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Kalyan, Ashwin},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={2507--2521},
  year={2022}
}

@inproceedings{rein2024gpqa,
      title={{GPQA}: A Graduate-Level Google-Proof Q\&A Benchmark},
      author={David Rein and Betty Li Hou and Asa Cooper Stickland and Jackson Petty and Richard Yuanzhe Pang and Julien Dirani and Julian Michael and Samuel R. Bowman},
      booktitle={First Conference on Language Modeling},
      year={2024},
      url={https://openreview.net/forum?id=Ti67584b98}
}

@inproceedings{sun2024determlr,
  title={Determlr: Augmenting llm-based logical reasoning from indeterminacy to determinacy},
  author={Sun, Hongda and Xu, Weikai and Liu, Wei and Luan, Jian and Wang, Bin and Shang, Shuo and Wen, Ji-Rong and Yan, Rui},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={9828--9862},
  year={2024}
}

@inproceedings{sun2024scieval,
  title={Scieval: A multi-level large language model evaluation benchmark for scientific research},
  author={Sun, Liangtai and Han, Yang and Zhao, Zihan and Ma, Da and Shen, Zhennan and Chen, Baocai and Chen, Lu and Yu, Kai},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={17},
  pages={19053--19061},
  year={2024}
}

@inproceedings{wangscibench,
  title={SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models},
  author={Wang, Xiaoxuan and Hu, Ziniu and Lu, Pan and Zhu, Yanqiao and Zhang, Jieyu and Subramaniam, Satyen and Loomba, Arjun R and Zhang, Shichang and Sun, Yizhou and Wang, Wei},
  booktitle={Forty-first International Conference on Machine Learning}
}

@inproceedings{xu-etal-2024-symbol,
    title = "Symbol-{LLM}: Towards Foundational Symbol-centric Interface For Large Language Models",
    author = "Xu, Fangzhi  and
      Wu, Zhiyong  and
      Sun, Qiushi  and
      Ren, Siyu  and
      Yuan, Fei  and
      Yuan, Shuai  and
      Lin, Qika  and
      Qiao, Yu  and
      Liu, Jun",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.707/",
    doi = "10.18653/v1/2024.acl-long.707",
    pages = "13091--13116"
}

@inproceedings{zhao2024docmath,
  title={DocMath-eval: Evaluating math reasoning capabilities of LLMs in understanding long and specialized documents},
  author={Zhao, Yilun and Long, Yitao and Liu, Hongjun and Kamoi, Ryo and Nan, Linyong and Chen, Lyuhao and Liu, Yixin and Tang, Xiangru and Zhang, Rui and Cohan, Arman},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={16103--16120},
  year={2024}
}

@inproceedings{zhong2024agieval,
  title={AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models},
  author={Zhong, Wanjun and Cui, Ruixiang and Guo, Yiduo and Liang, Yaobo and Lu, Shuai and Wang, Yanlin and Saied, Amin and Chen, Weizhu and Duan, Nan},
  booktitle={Findings of the Association for Computational Linguistics: NAACL 2024},
  pages={2299--2314},
  year={2024}
}

