\documentclass[conference,letterpaper]{IEEEtran}
\IEEEoverridecommandlockouts

%% depending on your installation, you may wish to adjust the top margin:
\addtolength{\topmargin}{9mm}

%%%%%%
%% Packages:
%% Some useful packages (and compatibility issues with the IEEE format)
%% are pointed out at the very end of this template source file (they are 
%% taken verbatim out of bare_conf.tex by Michael Shell).
%
% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
%

\usepackage{ifpdf}
\ifpdf
    \usepackage[pdftex]{graphicx}
    \graphicspath{{figs/}{matlab/}}   % still needed, even with Inkscape-Pdf-Latex
    \usepackage[update]{epstopdf}
\else
	\usepackage{graphicx}
	\graphicspath{{figs/}{matlab/}}   % still needed, even with Inkscape-Pdf-Latex
\fi
\usepackage{comment}
\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{ifthen}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{cite}
\usepackage[cmex10]{amsmath} % Use the [cmex10] option to ensure complicance
\usepackage{amssymb,amsthm,bm,dsfont}
%\usepackage{amssymb,amsthm}
%\usepackage{amsthm}

\usepackage{enumerate}  
\usepackage{color}
\usepackage{xcolor}
%\usepackage{setspace}
\usepackage[hidelinks]{hyperref}
\usepackage{cleveref} % multiple refference
\usepackage{cite}
\usepackage{subcaption}
\allowdisplaybreaks[2]  %公式分页显示，数字0-4表示分开程度
\usepackage{multirow}

\usepackage{cases}
\usepackage{balance}
\newif\ifEnv

\Envtrue
% \Envfalse
\ifEnv
    \newenvironment{Env}{\color{blue}}{}
\else
    \excludecomment{Env}
\fi

%% optimization
%\usepackage[long]{optidef}
\usepackage{optidef}

\newenvironment{claim}[1]{\par\noindent\underline{Claim:}\space#1}{}
\newenvironment{claimproof}[1]{\par\noindent\underline{Proof:}\space#1}{\hfill $\blacksquare$}
\newcounter{MYtempeqncnt}
%
\newtheorem{theorem}{\textbf{Theorem}}
\newtheorem{prop}{\textbf{Proposition}}
\newtheorem{corollary}{\textbf{Corollary}}
%\newtheorem{prop}{\textbf{Proposition}}
%\newtheorem{defn}[theorem]{Definition} 
\newtheorem{defn}{Definition} 
%\newtheorem{IEEEproof}{Proof} 
\newtheorem{definition}{\textbf{Definition}}%[section]
%\newtheorem{fact}[theorem]{Fact}
%\newtheorem{property}{\textbf{Property}}%[section]
%\newtheorem{corollary}{\textbf{Corollary}}%[section]
%\newtheorem{claim}{\textbf{Claim}}%[section]
\newtheorem{lemma}{\textbf{Lemma}}
\newtheorem{assumption}{\textbf{Assumption}}%[section]
\newtheorem{remark}{\textbf{Remark}}

% \newcommand{\Fc}{\mathcal{F}}
% \newcommand{\Oc}{\mathcal{O}}
% \newcommand{\Pc}{\mathcal{P}}
% \newcommand{\Qc}{\mathcal{Q}}



%% Please note that the amsthm package must not be loaded with
%% IEEEtran.cls because IEEEtran provides its own versions of
%% theorems. Also note that IEEEXplore does not accepts submissions
%% with hyperlinks, i.e., hyperref cannot be used.

\interdisplaylinepenalty=2500 % As explained in bare_conf.tex




% %% optimization
% %\usepackage[long]{optidef}
% \usepackage{optidef}

% \newenvironment{claim}[1]{\par\noindent\underline{Claim:}\space#1}{}
% \newenvironment{claimproof}[1]{\par\noindent\underline{Proof:}\space#1}{\hfill $\blacksquare$}

% %
% %\newtheorem{theorem}{\textbf{Theorem}}
% \newtheorem{prop}{\textbf{Proposition}}
% \newtheorem{assumption}{\textbf{Assumption}}
% %\newtheorem{corollary}{\textbf{Corollary}}
% %\newtheorem{prop}{\textbf{Proposition}}
% %\newtheorem{defn}[theorem]{Definition} 
% \newtheorem{defn}{Definition} 
% \newtheorem{IEEEproof}{Proof} 
% \newtheorem{definition}{\textbf{Definition}}%[section]
% %\newtheorem{fact}[theorem]{Fact}
% %\newtheorem{property}{\textbf{Property}}%[section]
% %\newtheorem{corollary}{\textbf{Corollary}}%[section]
% %\newtheorem{claim}{\textbf{Claim}}%[section]
% % \newtheorem{lemma}{\textbf{Lemma}}

% calligraphic letter for set and class
\newcommand{\Ac}{\mathcal{A}}
\newcommand{\Bc}{\mathcal{B}}
\newcommand{\Cc}{\mathcal{C}}
\newcommand{\Dc}{\mathcal{D}}
\newcommand{\Ec}{\mathcal{E}}
\newcommand{\Fc}{\mathcal{F}}
\newcommand{\Gc}{\mathcal{G}}
\newcommand{\Hc}{\mathcal{H}}
\newcommand{\Ic}{\mathcal{I}}
\newcommand{\Jc}{\mathcal{J}}
\newcommand{\Kc}{\mathcal{K}}
\newcommand{\Lc}{\mathcal{L}}
\newcommand{\Mc}{\mathcal{M}}
\newcommand{\Nc}{\mathcal{N}}
\newcommand{\Oc}{\mathcal{O}}
\newcommand{\Pc}{\mathcal{P}}
\newcommand{\Qc}{\mathcal{Q}}
\newcommand{\Rc}{\mathcal{R}}
\newcommand{\Sc}{\mathcal{S}}
\newcommand{\Tc}{\mathcal{T}}
\newcommand{\Uc}{\mathcal{U}}
\newcommand{\Vc}{\mathcal{V}}
\newcommand{\Wc}{\mathcal{W}}
\newcommand{\Xc}{\mathcal{X}}
\newcommand{\Yc}{\mathcal{Y}}
\newcommand{\Zc}{\mathcal{Z}}

% special set and measures
\newcommand{\Pb}{\mathbb{P}}
\newcommand{\Qb}{\mathbb{Q}}
\newcommand{\Hb}{\mathbb{H}}
\newcommand{\Ib}{\mathbb{I}}
\newcommand{\Nb}{\mathbb{N}}
\newcommand{\Eb}{\mathbb{E}}
\newcommand{\Rb}{\mathbb{R}}
\newcommand{\Zb}{\mathbb{Z}}

% vectors and matrices
\newcommand{\uv}{\mathbf{u}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\wv}{\mathbf{w}}
\newcommand{\xv}{\mathbf{x}}
\newcommand{\yv}{\mathbf{y}}
\newcommand{\zv}{\mathbf{z}}

\newcommand{\Hv}{\mathbf{H}}
\newcommand{\Sv}{\mathbf{S}}
\newcommand{\Vv}{\mathbf{V}}
\newcommand{\Xv}{\mathbf{X}}



% Random variables
\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
% rm is already a command, just don't name any random variables m
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

% Random vectors
\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

% Elements of random vectors
\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

% Random matrices
\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

% Elements of random matrices
\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

% Vectors
\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

% Elements of vectors
\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

% Matrix
\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\newcommand{\Expt}{\mathbb{E}}





\newcommand{\argmax}{\arg\max}
\newcommand{\argmin}{\arg\min}


\newcommand{\dm}{\mathrm{d}}
\newcommand{\TV}{\text{TV}}
\newcommand{\KL}{\mathrm{KL}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\ERM}{\mathrm{ERM}}
\newcommand{\gen}{\mathrm{gen}}
\newcommand{\sign}{\mathrm{sign}}
\newcommand{\supp}{\mathrm{supp}}
\newcommand{\epi}{\mathrm{epi}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\SNR}{\mathrm{SNR}}
\newcommand{\numpy}{{\tt numpy}}

\allowdisplaybreaks

\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}}
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}

\definecolor{bl}{rgb}{.2,.2,.8}
\newcommand{\RZ}[1]{{\color{purple}\text{RZ:} #1}}


\renewcommand{\vec}[1]{\mbox{\boldmath$#1$}}

\usepackage{xparse}
\NewDocumentCommand{\ml}{m O{M} O{Q_n^{[M]}}} {\mathcal{L}_{#1}(#2\rightarrow #3)}
\newcommand{\cmb}[2]{\begin{pmatrix}#1\\#2\end{pmatrix}}
\newcommand{\pdv}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\Ls}{\mathscr{L}}

\renewcommand{\vec}[1]{\underline{#1}}

\usepackage{amsmath}
\usepackage{graphicx}
%\usepackage[colorlinks=true, linkcolor=blue, citecolor=blue]{hyperref}
\usepackage{hyperref}
\usepackage{url}
%\usepackage{orcidlink}
%\usepackage[title]{appendix}
%\usepackage{mathrsfs}
%\usepackage{amsfonts}
%\usepackage{booktabs} % For \toprule, \midrule, \botrule
%\usepackage{caption}  % For \caption
%\usepackage{threeparttable} % For table footnotes
%\usepackage{algorithm}
% \usepackage{algorithmicx}
% \usepackage{algpseudocode}
% \usepackage{listings}
% \usepackage{enumitem}
% \usepackage{chngcntr}
% \usepackage{booktabs}
% \usepackage{lipsum}
% \usepackage{subcaption}
% \usepackage{authblk}
% \usepackage[T1]{fontenc}    % Font encoding
% \usepackage{csquotes}       % Include csquotes
% \usepackage{diagbox}


%%%%%%
% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}


\title{\Huge{Relatively-Secure LLM-Based Steganography via Constrained Markov Decision Processes}} 
% Relatively-Secure Steganography with CMDP: A Novel Approach to Enhancing Capacity
% Relatively-Secure Steganography via Constrained Markov Decision Processes for Enhanced Capacity
% Overcoming Low-Capacity in Steganography with CMDP
% Steganography Beyond Low Capacity: A CMDP Solution for Relatively-Secure and Efficient Encoding

% %%% Single author, or several authors with same affiliation:
% \author{%
%  \IEEEauthorblockN{Author 1 and Author 2}
% \IEEEauthorblockA{Department of Statistics and Data Science\\
%                    University 1\\
 %                   City 1\\
  %                  Email: author1@university1.edu}% }


%%% Several authors with up to three affiliations:
\author{%
  \IEEEauthorblockN{Yu-Shin Huang, Chao Tian, and Krishna Narayanan}
  \IEEEauthorblockA{Dept. of Electrical and Computer Engineering \\
                    Texas A\&M University\\
                    College Station, TX 77845, USA\\
                    Email: \{yushin1002, chao.tian, krn\}@tamu.edu}
  \and
  \IEEEauthorblockN{Lizhong Zheng}
  \IEEEauthorblockA{Dept. of Electrical Engineering and Computer Science\\
  Massachusetts Institute of Technology\\
                     Cambridge, MA 02139, USA\\
                     Email: lizhong@mit.edu}
}




\maketitle

%%%%%%
%% Abstract: 
%% If your paper is eligible for the student paper award, please add
%% the comment "THIS PAPER IS ELIGIBLE FOR THE STUDENT PAPER
%% AWARD." as a first line in the abstract. 
%% For the final version of the accepted paper, please do not forget
%% to remove this comment!
%%

\begin{abstract}
    Linguistic steganography aims to conceal information within natural language text without being detected. An effective steganography approach should encode the secret message into a minimal number of language tokens while preserving the natural appearance and fluidity of the stego-texts. We present a new framework to enhance the embedding efficiency of stego-texts generated by modifying the output of a large language model (LLM). 
   The novelty of our approach is in abstracting the sequential steganographic embedding process as a Constrained Markov Decision Process (CMDP), which takes into consideration the long-term dependencies instead of merely the immediate effects. We constrain the solution space such that the discounted accumulative total variation divergence between the selected probability distribution and the original distribution given by the LLM is below a threshold. To find the optimal policy, we first show that the functional optimization problem can be simplified to a convex optimization problem with a finite number of variables. A closed-form solution for the optimal policy is then presented to this equivalent problem. It is remarkable that the optimal policy is deterministic and resembles water-filling in some cases. The solution suggests that usually adjusting the probability distribution for the state that has the least random transition probability should be prioritized, but the choice should be made by taking into account the transition probabilities at all states instead of only the current state.      
\end{abstract}

\section{Introduction}

In steganography, a sender Alice attempts to discreetly communicate a hidden message to Bob, the receiver, using a carrier signal, which can be in the form of text, image, audio, or video \cite{anderson1998limits, cox2007digital, provos2003hide}. Alice transmits the encoded signal to Bob through a public channel, which is under surveillance by an eavesdropper, Eve, who seeks to detect the existence of any hidden content. A well-known example is the prisoner problem \cite{simmons1984prisoners} where Eve is the prisoner guard in charge of deciding whether to deliver a letter to a prisoner. In this work, we consider using natural language text as the carrier signal, where the encoded message is referred to as the stego-text; see Fig. \ref{fig:stega_explain} for an illustration.

Traditional text steganography methods usually embed hidden messages into an existing cover text \cite{topkara2006hiding, chang2010linguistic, safaka2016matryoshka}. However, with the development of generative models, especially large language models (LLMs), a new approach known as coverless steganography has emerged, proving to be highly effective and secure. Generative-method-based approaches produce stego-texts that mimic natural language while also allowing for more efficient encoding of hidden information compared to traditional approaches \cite{fang2017generating, yang2018rnn, ziegler2019neural, xiang2017novel, yang2020vae, yang2020gan,de2022perfectly, kaptchuk2021meteor, zhang2024provably, zhang2021provably, ding2023discop}. 
In this approach (discussed in detail in Section~\ref{sec:prelim}), a language model (LM) is used to produce the conditional probabilities 
$p(x_t|x_{t-1},\ldots,x_{t-k})$ for the token at time $t$ and the secret message is embedded in the sampling process that samples from $p_t$ to generate $x_t$.  
It is known that $\sum_t H(p_t)$ bits of information can be hidden in an imperceptible manner, where $H$ is the binary entropy function.

In many scenarios, the security requirement in steganography can be relaxed, particularly when the eavesdropper Eve is computation-bounded (e.g., in a mobile device). This consideration is in fact already implicit in several previous works that adopt ``near-imperceptibility'' steganography \cite{dai2019towards,shen2020near} to improve computation efficiency. Further generalizing this idea, we can view the steganography coding problem with a less-stringent security requirement as
replacing the LM-based probability distribution $p_t$ with another distribution $p_t'$ such that $\mathbb{E}[H(p_t')] \geq \mathbb{E} [H(p_t)]$, while maintaining an imperceptibility constraint (discussed later) and we sample the next token according to $p_t'$.
We  refer to this approach as \emph{relatively-secure steganography}. 
In this work, we consider choosing such a replacement $p_t'$ for the purpose of more efficient information embedding. This consideration is particularly important in LLM-based approaches, as several works  \cite{liao2024co,bai2024semantic,pang2024frestega} have highlighted the low embedding capacity, since $p_t$ becomes more deterministic, leading to lower entropy and a reduced secret message embedding rate. 

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.45\textwidth]{stega_explain_wide.pdf}
    \caption{Steganography system}\label{fig:stega_explain}
    \vspace{-0.5cm}
\end{figure}


In a recent work \cite{huang2024od}, we considered the relatively-secure steganography coding problem, and optimized the replacement probability distribution of each token for maximum (conditional) entropy, under a constraint on the deviation from the LM-based next token distribution.  While this approach increases the embedding capacity,
the distribution is optimized for each token position independently of others. 
One critical observation that motivated the current work is that the choice for the replacement probability distribution for the current token impacts not only the embedding efficiency of the immediate token, but also all future tokens. 


To take into account such future impact, we propose to model the process as a decision-making agent adjusting the probability distribution for generating the next token. Selecting a particular probability distribution induces a reward $R_{t}$ and a cost $C_t$: the reward is the entropy of the distribution (higher embedding efficiency), and the cost is the divergence from the original LM-generated distribution (unnatural text). The state of the system is the preceding tokens in the context window of the LM. 
As demonstrated in Fig. \ref{fig:CMDPstega_explain}, this in fact closely resembles a Constrained Markov Decision Process (CMDP) \cite{altman2021constrained}. Therefore, in this work, we formulate the relative steganography coding problem as a CMDP. 

LMs are clearly too complex to model precisely. To gain insights into how $p_t'$ should be selected, we abstract and simplify the LM into the simplest finite state model, i.e., a two-state model. 
Our main contribution is a closed-form solution for the optimal policy in this setting. To obtain this optimal policy, we simplify the problem from a functional optimization problem into a finite convex problem, and then establish its optimal solution in closed form. Remarkably, this implies that the optimal policy is deterministic, whereas the optimal policy for general CMDPs is usually not deterministic. Moreover, depending on the transition probability structure, the optimal policy resembles a water-filling solution in some cases. The solution suggests that usually adjusting the probability distribution for the state that has the least random transition probability should be prioritized, but the choice should be made by taking into account the transition probabilities at all states instead of only the current state.   

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.45\textwidth]{CMDP_stega_explain.pdf}
    \caption{Relatively-secure LM-based steganography system}
    \label{fig:CMDPstega_explain}
    \vspace{-0.5cm}
\end{figure}

\section{Preliminaries} \label{sec:prelim}
\subsection{LLM-based Steganography}


An LLM produces the next token probability distribution $p_t=P(x_t|x_{t-1},x_{t-2},\ldots,x_{t-k})$ conditioned on the preceding tokens in the context window of size $k$, which can be used to sample tokens auto-regressively to produce natural language texts \cite{vaswani2017attention,brown2020language,touvron2023llama}. 
We assume that the secret message bits are first encrypted using a shared random key between Alice and Bob, such that the encrypted message $\mathbf{m}$ is a sequence of i.i.d. uniformly distributed bits. 
The key idea in embedding $\mathbf{m}$ into a stego-text is to first use the decompressor of a compression algorithm  
to map the sequence of i.i.d bits $\mathbf{m}$ into a sequence of random variables $\{X_t\}$ such that $X_t \sim p_t$ at time $t$. 
The realization of the random variable at time $t$, namely $x_t$, is the token in the stego-text at time $t$, and we continue this process auto-regressively.


\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.8\textwidth]{AC_stega_wide.pdf}
    \caption{Arithmetic coding based steganography}\label{fig:AC}
    \vspace{-0.45cm}
    % good idea!
\end{figure*}

Arithmetic coding \cite{rissanen1979arithmetic} is an elegant way to generate $x_t$'s for {\em any} sequence of distributions $p_t$. 
Consider the example shown in Fig. \ref{fig:AC} where a 5-bit encrypted message $10111$ is to be embedded. The bit string is represented by the interval $ \mathcal{I} = [0.101110000 \cdots_{2}, 0.1011111111\cdots_{2}) \simeq [0.71875, 0.75)$. We input the starting prompt ``What is the probability of'' to the LLM which generates $p_1$ at time $t=1$, which induces a partition of the interval $[0,1)$ and the interval $\mathcal{I}$ corresponds to the token ``winning'', implying the first token of the stego-text is \( x_1 = \text{``winning''} \).
Once \( x_1 \) is selected, the probability distribution \( p_2 \) is generated by adding $x_1$ to the prompt. The next token \( x_2 \) is chosen based on the partition of $[0,1)$ induced by $p_2$. This process is repeated to generate a stego-text of length $n$. 
During the decoding phase, Bob parses the received text into tokens and reproduces the  secret bits using the exact same sequence of distributions $\{p_t\}$. 

This procedure is optimal in that nearly $\sum_{t=1}^n H(p_t)$ bits from $\mathbf{m}$ can be mapped into a stego-text of length $n$. It produces perfectly-secure steganography since $x_t \sim p_t$.


\subsection{Embedding Efficiency, Deviation, and State Space Model}

Low entropy distributions $p_t$'s imply that fewer bits of the secret message can be embedded in the stego-text. To mitigate this issue, we propose to sample from a different distribution $p_t'$, instead of the LLM-produced next token distribution $p_t$. At time-$t$, the problem can be naturally formulated as maximizing the entropy of $p_t'$, subject to the constraint that the deviation from $p_t$ is small; this token-wise problem was studied in \cite{huang2024od}. 

Mathematically, the sequential LLM-based sampling process can be viewed as a state-space model, where the state is the preceding length-$k$ token sequence.  In this work, we further simplify the state-space model to the binary state-space model where $p_s=P(S_{t+1}=0|S_t=s)$. We write $\mathbf{p}_s \triangleq [p_s, 1-p_s]$ for simplicity. 

\subsection{An Introduction to CMDP} 

CMDP consists of several key components: the state space $\mathsf{S}$, action space $\mathsf{A}$, transition probabilities $P$, reward function $r:\mathsf{S} \times \mathsf{A} \rightarrow \mathbb{R}$, a cost constraint $g:\mathsf{S} \times \mathsf{A} \rightarrow \mathbb{R}$, a policy $\pi: \mathsf{S} \rightarrow \Delta_{\mathsf{A}}$, the initial state distribution $\mathbf{d}$, and the discount factor $\gamma \in [0,1)$, where $\Delta_{\mathsf{A}}$ represents a probability simplex with $|\mathsf{A}|$ dimensions. The goal of policy optimization is to find the optimal policy $\pi(a|s)$ that guides the agent in selecting actions $a \in \mathsf{A}(s)$ for states $s \in \mathsf{S}$ in order to maximize the cumulative discounted reward over the entire timeframe, i.e.,
\begin{align}
R=\Expt\left[\sum_{t=0}^\infty \gamma^t r(s_t,a_t)\right],
\end{align}
subject to the constraint
\begin{align}
C=\Expt\left[\sum_{t=0}^\infty \gamma^t g(s_t,a_t)\right]\leq b.
\end{align}
It is often more convenient to consider the discounted state-action pair visitation frequency $d(s,a)$ than the policy $\pi(a|s)$, also known as the occupancy measure. This quantity determines the likelihood of visiting a specific state-action pair $(s,a)$ over time, defined as
\begin{align}
    d(s,a) = (1-\gamma)\sum_{t = 0}^{\infty}\gamma^t P(S_t = s, A_t = a)
\end{align}
The reward and cost functions can now be represented in a structured form using $d(s,a)$. 
\begin{align}
    R & = \sum_{s \in \mathsf{S}} \sum_{a \in \mathsf{A}(s)} r(s, a) d(s, a), \label{eq:CMDP_Ob_def}\\
    C & = \sum_{s \in \mathsf{S}} \sum_{a \in \mathsf{A}(s)} g(s,a) d(s,a). \label{eq:CMDP_cost_def}
\end{align}
Note that $d(s,a)$ must be non-negative, and the flow conservation constraint (closely related to the Bellman's equation \cite{bellman2015applied} in the dual optimization) requires that the total inflow to state $s$ equals the total outflow:
\begin{align}
    \sum_{a \in \mathsf{A}(s)} d(s, a) = \mathbf{d}(s) + \gamma \sum_{s' \in \mathsf{S}} \sum_{a' \in \mathsf{A}(s')} P(s \mid s', a') \cdot d(s', a'). \label{eq:intro_flowconservation}
\end{align}
For a stationary policy, $\pi(a|s)$ can be obtained from $d(s,a)$ as follows
\begin{align}
    \pi(a|s) = \frac{d(s,a)}{\sum_{a \in \mathsf{A}(s)}d(s,a)}. \label{eq:policy_and_d}
\end{align}
We refer the readers to \cite{sutton2018reinforcement} for more details on MDPs. 

\section{Problem Formulation} \label{sec:formulation}

\subsection{States Space, Action Space, and Policies}

To formulate steganography coding as a CMDP problem, we first specify the relative components as outlined above; Fig. \ref{fig:CMDP2states} shows the CMDP state transitions. As mentioned earlier, we abstract and simplify the state space to the simplest setting with only two states $\mathsf{S}=\{0,1\}$. 
In our scenario, we define an action $a \in \mathsf{A} = [0,1] $ as representing the likelihood of transitioning to state 0 at the subsequent time step at a given state $s \in \mathsf{S}$ (also the probability of the next token being $0$). Choosing action $a$ impacts the transition probability:
\begin{align}
P_a(s,s')&\triangleq P(S_{t+1}=s'|S_t=s,A_t=a)\notag\\
&=p(S_{t+1}=s'|A_t=a)\notag\\
&=a\mathds{1}(s'=0)+(1-a)\mathds{1}(s'=1),
\end{align}
and the action set $\mathsf{A}$ in fact contains uncountably many actions. For simplicity, we shall write $\mathbf{p}_a \triangleq (a, 1-a)$.

Since the entropy of the distribution $P_a(s,s')$ directly reflects the amount of information that can be embedded in the stego-text, the reward function is simply
\begin{align}
    & r(s,a)=H(\mathbf{p}_a),\quad s\in\{0,1\},\,a\in[0,1]\label{eq:reward_H}
\end{align}
where $H(\cdot)$ is the binary entropy function. A standard discount factor $\gamma$, $0 \leq \gamma <1$ is assumed. The chosen probability $\mathbf{p}_a$ at time $t$ should not deviate too far from the original transition probability $\mathbf{p}_{s_t}$, therefore, the constraint cost function is 
\begin{align}
    g(s,a)=\TV(\mathbf{p}_a, \mathbf{p}_s),\quad s\in\{0,1\},\,a\in[0,1], \label{eq:cost_TV}
\end{align}
where we measure the deviation by the total variation distance 
\begin{align}
\TV(\mathbf{p}_a, \mathbf{p}_s) = |a - p_s| + |(1-a)-(1-p_s)|=2|a-p_s|.
\end{align}
A (probabilistic) policy $\pi$ is a probability distribution over the  action space at state $s$, which can be written as
\begin{align}
    \pi(a | s) \triangleq P(A_t =a | S_t = s), \quad a\in \mathsf{A},
\end{align}
i.e., the probability of choosing action $a$ at state $s$, which is time-invariant in this setting. 

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.4\textwidth]{CMDP_twostates.pdf} 
    \caption{Steganography with CMDP} 
    \label{fig:CMDP2states}
    \vspace{-0.4cm}
\end{figure}

\subsection{CMDP Formulation}
With the components given above, we can utilize the general CMDP framework and write out our specific problem in terms of the visitation frequency $d(s,a)$, denoted as $\mathbf{P1}$ below.
\begin{align}
    \mathbf{P1}: ~\max_{d(s,a)}: & ~ \sum_{s=0}^{1} \int_0^1  H({\mathbf{p}_a}) d(s,a) \,da \label{eq:opt_problem_ob_TV} \\
    \text{subject to: } & ~ \int_0^1 \left(d(0,a) - \sum_{s=0}^{1} \gamma  a d(s, a)\right) da =\mathbf{d}^{\gamma}_0 \label{eq:con_same1_TV}\\
    & ~ \int_0^1 \left(d(1,a)- \sum_{s=0}^{1} \gamma (1-a) d(s, a)\right) da =\mathbf{d}^{\gamma}_1
    \label{eq:con_same2_TV} \\
    & ~ \sum_{s=0}^1\int_0^1 \TV(\mathbf{p}_{a}, \mathbf{p}_s)d(s,a) da\leq b \label{eq:opt_problem_con3_TV}\\
    & ~ d(s,a) \geq 0, ~ \forall a \in [0,1], s \in \{0,1 \} \label{eq:opt_problem_con4_TV}
\end{align}
where we denote the discounted initials $(1-\gamma)\mathbf{d}(s)$ as $\mathbf{d}^{\gamma}_s$.

In the formulation, (\ref{eq:con_same1_TV}) and (\ref{eq:con_same2_TV}) reflect the flow conservation constraint, aligning with (\ref{eq:intro_flowconservation}) by substituting the transition probability $P(s \mid s', a')$ with $a$ and $1-a$. 
It is clear that (\ref{eq:con_same2_TV}) can be replaced by (\ref{eq:con_same_sum1}) 
\begin{align}
    \int_0^1  d(0,a) + d(1,a) da = 1, \label{eq:con_same_sum1}
\end{align}
in order to simplify the problem, since it is simply the summation of (\ref{eq:con_same1_TV}) and (\ref{eq:con_same2_TV}). 

By determining the optimal $d(s,a)$ from problem $\mathbf{P1}$, the optimal policy $\pi$ can be computed via (\ref{eq:policy_and_d}). For each fixed $s$, the visitation frequency $d(s,a)$ is a function of the state $a$ on $[0,1]$. Therefore, finding the optimal visitation frequency $d(s,a)$ is a functional optimization problem, even though it is a linear program in terms of $d(s,a)$. 

\section{The Optimal Policy}\label{sec:results}

We establish the optimal policy $\pi$ in two steps: 1) First we simplify the functional optimization problem to a standard convex optimization problem in $\mathbb{R}^4$, and 2) We provide an explicit optimal solution to this simplified optimization problem.

\subsection{A Simplified Optimization Problem} \label{sec:simplfy_opt}

We first show that the optimization problem $\mathbf{P1}$ can be simplified to an optimization in $\mathbb{R}^4$, given in $\mathbf{P3}$. Mathematically, $\mathbf{P3}$ is a standard finite-dimensional convex optimization problem, which is significantly simpler than the functional optimization problem $\mathbf{P1}$. Operationally, once we solve $\mathbf{P3}$, we can obtain the parameters $(a_0,a_1)$ that immediately gives us the optimal policy: at state $s$, choose the probability distribution $(a_s,1-a_s)$ to generate the next token (state). The policy itself is deterministic, and the randomness in generating the next token comes purely from the encrypted message bits. We note the generic de-randomization technique given in \cite{altman2021constrained} will lead to a CMDP with two possible actions at each state, instead of only one as in our reduction.

We define an intermediate optimization problem as follows. 
\begin{align}
    \mathbf{P2}: ~\max_{a_0, a_1, d_0, d_1}: & ~ d_0 H(a_0) + d_1 H(a_1)  \\
    \text{subject to:} & ~  (1-\gamma a_0) d_0 - \gamma a_1 d_1 = \mathbf{d}^{\gamma}_0 \\
    & ~ d_0 + d_1 = 1 \\
    & ~ d_0 \TV_0(a_0) + d_1 \TV_1(a_1)\leq b \\
    & ~ d_0, d_1 \geq 0 \\
    & ~ 0 \leq a_0, a_1 \leq 1, \label{eq:p2_constraint_a0a1}
\end{align}
where we write $H(\mathbf{p}_a)$ as $H(a)$, and $\TV(\mathbf{p}_a, \mathbf{p}_s)$ as $\TV_s(a)$ for simplicity,. 
\begin{theorem}\label{thm:reduce_varto2}
 $(\mathbf{P1})=(\mathbf{P2})$, where ``$=$" stands for equivalence between the two problems. 
 \end{theorem}
The proof of Theorem \ref{thm:reduce_varto2} is given in Appendix \ref{pf:thm1}\footnote{Long version:\url{https://github.com/Yu-Shin-Huang/Stega-via-CMDP.git}} of the accompanying long version, where we utilize the convexity of the objective function and the constraint functions to show that a weighted average of the probabilistic policy is without loss of optimality. The problem $\mathbf{P2}$ is however not necessarily convex. We next show that a reparametrization of the problem leads to another equivalent problem that is indeed convex. The equivalent problem is given below.
\begin{align}
    \mathbf{P3}: ~ \max_{x_0,x_1, d_0, d_1}: & ~ d_0 H(\frac{x_0}{d_0}) + d_1 H(\frac{x_1}{d_1})  \label{eq:x0_obj_func_TV}\\
    \text{subject to:} & ~  (d_0 -\gamma x_0)  - \gamma x_1 = \mathbf{d}^{\gamma}_0  \label{eq:x0_constraint_1_TV}\\
    & ~ d_0 + d_1 = 1 \label{eq:x0_constraint_2_TV}\\
    & ~ d_0 \TV_0(\frac{x_0}{d_0}) + d_1 \TV_1(\frac{x_1}{d_1})\leq b \label{eq:x0_constraint_3_TV}\\
    & ~ d_s \geq 0, ~ s \in \{0,1 \} \label{eq:x0_constraint_4_TV}\\
    & ~ 0 \leq \frac{x_s}{d_s} \leq 1 , ~ s \in \{0,1 \}. \label{eq:x0_constraint_5_TV}
\end{align}
Clearly, we have simply replaced $a_0,a_1$ by $x_0=a_0d_0$ and $x_1=a_1d_1$. Since the constraints (\ref{eq:x0_constraint_1_TV}), (\ref{eq:x0_constraint_2_TV}), (\ref{eq:x0_constraint_4_TV}), and (\ref{eq:x0_constraint_5_TV}) are linear, we only need to consider the objective function (\ref{eq:x0_obj_func_TV}) and the constraint (\ref{eq:x0_constraint_3_TV}). Recall that the perspective function $f_2(x,t)$ of a function $f_1(x)$ is defined as 
\begin{align}
    f_2(x,t) = t f_1\left(\frac{x}{t}\right), ~ \forall t > 0
\end{align}
A well-known property of the perspective function \cite{boyd2004convex} is that if $f_1(\cdot)$ is convex, $f_2(\cdot,\cdot)$ is likewise convex. Since the binary entropy function $H(\cdot)$ is concave and the total variation function is convex, (\ref{eq:x0_obj_func_TV}) is indeed concave and (\ref{eq:x0_constraint_3_TV}) is indeed convex, implying that the problem $\mathbf{P3}$ is a convex optimization problem. When substituting $a_s$ with $\frac{x_s}{d_s}$, it is essential to ensure $d_s$ is non-zero, as $d_s$ indicates the frequency of visiting state $s$. If $d_s=0$, it implies state $s$ is never visited. In such a scenario, the state can be excluded from the system. Thus, without loss of generality, we assume $d_s$ is positive.  

\subsection{The Optimal Policy}

Before presenting the solution, we first note that by symmetry, we only need to consider the case $p_0,p_1\geq \frac{1}{2}$ and $p_1 \geq \frac{1}{2}>p_0$, since otherwise, we can simply rename the two states; more details on this relabeling can be found in Appendix  \ref{pf:cor1}\footnotemark[1] in the accompanying longer version.  
Let us also define the following auxiliary functions:
   \begin{align}
            \eta_0^{\pm}(b) & = \frac{\frac{b}{2} (1+\gamma p_{1})\pm p_{0}(\mathbf{d}^{\gamma}_0 +\gamma p_{1})}{\pm(\gamma p_{1}+\mathbf{d}^{\gamma}_0) +\gamma \frac{b}{2} } \\
            \eta_1^{\pm}(b) & = \frac{\frac{b}{2} (1-\gamma p_{0})\pm p_{1}(1-\gamma p_{0}-\mathbf{d}^{\gamma}_0 )}{\pm (1-\gamma p_{0}-\mathbf{d}^{\gamma}_0) -\gamma \frac{b}{2} }. 
    \end{align}
The next two theorems give the optimal policy, the proofs of which can be found in the appendix of the accompanying long version\footnotemark[1]. 
\begin{theorem} \label{thm:p0p1sameside}
    When $p_0, p_1 \geq \frac{1}{2}$, the optimal policy choices at the two states are:
    \begin{enumerate}
        \item $b \in [0, b_l) $ \label{thm:2-1}
        \begin{align}
            \begin{cases}
                a_0 = p_0, ~ a_1 = \eta_1^{-}(b), &\mbox{if}~ |\frac{1}{2} - p_1 | \geq |\frac{1}{2} - p_0 |\\
                a_0 = \eta_0^{-}(b), ~ a_1 = p_1, &\mbox{if} ~ |\frac{1}{2} - p_1 | < |\frac{1}{2} - p_0 | 
            \end{cases}
        \end{align}
        \item $b \in [b_l, b_h)$ \label{thm:2-2}
        \begin{align}
            & a_0 = a_1 = -M(\frac{b}{2} - \mathbf{d}_{0}^\gamma p_0 - \mathbf{d}_{1}^\gamma p_1 ) 
        \end{align}
        \item $b \in [b_h, \infty)$ \label{thm:2-3}
        \begin{align}
            & a_0 = a_1 = \frac{1}{2}
        \end{align}
    \end{enumerate}
    where we defined 
    \begin{align}        
            b_l & = 2\max\{(1 -\gamma p_0 - \mathbf{d}^{\gamma}_0 )(p_1 - p_0), \notag\\
            &\qquad\qquad\qquad (\mathbf{d}^{\gamma}_0 +\gamma p_1)(p_0 - p_1)\}\\
		b_h & = (2\mathbf{d}^{\gamma}_0 +\gamma)(p_0 - p_1)+2p_1-1\\
        M & = \frac{1}{1-\gamma p_0 + \gamma p_1}.
    \end{align}    
\end{theorem}

There are three regimes in the solution: 
\begin{enumerate}
\item The highly-constrained regime: The optimal policy is to replace the distribution at the state that generates lower entropy tokens with a more uniform distribution, while for the other state, the distribution is maintained;
\item The intermediately-constrained regime: the distributions at both states should be replaced by the same (more uniform) distribution;
\item The unconstrained regime: the distributions at both states should be replaced by the uniform distribution.
\end{enumerate}
The behavior resembles a water-filling solution in some sense; see Fig. \ref{fig:p0p1sameside}. Since $p_1$ is situated farther from $\frac{1}{2}$, in the highly-constrained regime, the optimal policy is to retain $a_0=p_0$ and adjust only $a_1$. Once $a_1=p_0$, the two states become equivalent, and $a_0=a_1$ needs to be adjusted together, until they both become $\frac{1}{2}$. 

\begin{theorem}\label{thm:p0p1diffside}
    When $p_1 \geq \frac{1}{2}>p_0$, the optimal policy choices at the two states are:
    \begin{enumerate}
        \item $b \in [0, b_l') $
        \begin{align}
            \begin{cases}
                a_0 = p_0, ~ a_1 = \eta_1^{-}(b), & \mbox{if}~ |\frac{1}{2} - p_1 | \geq |\frac{1}{2} - p_0 |\\
                a_0 = \eta_0^{+}(b), ~ a_1 = p_1, & \mbox{if} ~ |\frac{1}{2} - p_1 | < |\frac{1}{2} - p_0 | 
            \end{cases}
        \end{align}
        \item $b \in [b_l', b_h']$
        \begin{align}
            &(a_0, a_1) \in S_b  \triangleq \bigg\{ (a_0,a_1) \in [p_0,\frac{1}{2}]\times[\frac{1}{2},p_1] ~ :~\nonumber 
            \\ &\qquad \Phi_-(a_1) = \Phi_+(a_0) 
             \cap m(a_0,a_1) = \frac{b}{2} \bigg\}
        \end{align}
        \item $b \in (b_h', \infty)$ 
        \begin{align}
            & a_0 = a_1 = \frac{1}{2}
        \end{align}
    \end{enumerate}
    where we define 
    \begin{align}         
            &b_l'  = 2\left( \frac{\mathbf{d}^{\gamma}_0 +\gamma p_{1}}{1-\gamma \psi_{0} +\gamma p_{1}} \right)  (\psi_{0} -p_{0})u(1-p_{0}-p_{1})  \nonumber \\
		 & \quad + 2\left( \frac{1-\gamma p_{0}-\mathbf{d}^{\gamma}_0 }{1-\gamma p_{0}+\gamma \psi_{1} } \right)  (p_{1}-\psi_{1} )u(-1+p_{0}+p_{1}) \label{eq:bl'_def}\\
		&b_h'  = (2\mathbf{d}^{\gamma}_0 +\gamma)(1-p_0 - p_1)+2p_1-1
        \end{align}
    and
    \begin{align}        
        &\Phi_{\pm}(a)  = ( \pm1 + \gamma p_0 + \gamma p_1) \log \left(\frac{1-a}{a}\right) \notag\\
        &\qquad\qquad\qquad-2 \gamma \log(1-a) \\
        &\Psi_{p_0} (a_1)  = \Phi_+(p_0) - \Phi_-(a_1), ~\psi_0 = \Psi^{-1}_{p_1} (0) \\
        &\Psi_{p_1} (a_0) = \Phi_+(a_0) - \Phi_-(p_1),~ \psi_1  = \Psi^{-1}_{p_0} (0) \\
        &m(a_0,a_1)  = d_0(a_0-p_0)+d_1(p_1-a_1)        
    \end{align}
    and $u(\cdot)$ is a unit step function in $b_l'$. 
\end{theorem}

There are again three regimes, and the only difference from the previous case is in the second regime, the policy $a_0$ and $a_1$ are not the same; see Fig. \ref{fig:p0p1diffside} for an illustration. In the second regime, $a_0$ and $a_1$ are the solution of a pair of equations given above, which can not be solved explicitly. However, we show in the proof of this theorem that it admits a unique solution for any $b \in [b_l', b_h']$, which guarantees that $a_0$ and $a_1$ can be found. Notice also that since $\psi_0$ and $\psi_1$ are the solutions to $\Psi_{p_0}=0$ and $\Psi_{p_1}=0$, respectively, they can be written as $\Phi_+(p_0) = \Phi_-(\psi_1)$ and $\Phi_+(\psi_0) = \Phi_-(p_1)$; the solution determines the boundary of the regimes.

We further note that the visitation frequency $d_s$ can be expressed in terms of $a_0$ and $a_1$ using equation (\ref{eq:dsvalue}):
\begin{align}
    d_0 & = \frac{\mathbf{d}^{\gamma}_0 + \gamma a_1}{1 - \gamma a_0 + \gamma a_1} , \quad d_1 = \frac{1 - \gamma a_0-\mathbf{d}^{\gamma}_0}{1 - \gamma a_0 + \gamma a_1} \label{eq:dsvalue}
\end{align}

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{a0a1_p0p0same.pdf}
        \caption{$p_0, p_1 \geq \frac{1}{2}$}
        \label{fig:p0p1sameside}
    \end{subfigure}
    % Second figure
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{a0a1_p0p1diffside.pdf}
        \caption{$p_1 \geq \frac{1}{2}>p_0$}
        \label{fig:p0p1diffside}
    \end{subfigure}
    \caption{$a_0, a_1$ value v.s. b constraint with $\mathbf{d} = [0.8,0.2]$}
    \label{fig:plota0a1}
    \vspace{-0.3cm}
\end{figure}

\section{Conclusion}
In this study, we presented a novel formulation for relatively secure LLM-based steganography in the Constrained Markov Decision Process (CMDP) framework. This approach considers the impact on both the immediate and future token generations, and provides insights into how the optimal policy should be chosen. The solution suggests that when choosing the adjustment of the probability distribution, the choice should be made considering the LLM-generated transition probabilities at all states, and usually adjusting the distribution for the state that has the least random transition probability distribution should be prioritized. 

\bibliographystyle{IEEEtran}
% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{anderson1998limits}
R.~J. Anderson and F.~A. Petitcolas, ``On the limits of steganography,'' \emph{IEEE Journal on selected areas in communications}, vol.~16, no.~4, pp. 474--481, 1998.

\bibitem{cox2007digital}
I.~Cox, M.~Miller, J.~Bloom, J.~Fridrich, and T.~Kalker, \emph{Digital watermarking and steganography}.\hskip 1em plus 0.5em minus 0.4em\relax Morgan kaufmann, 2007.

\bibitem{provos2003hide}
N.~Provos and P.~Honeyman, ``Hide and seek: An introduction to steganography,'' \emph{IEEE security \& privacy}, vol.~1, no.~3, pp. 32--44, 2003.

\bibitem{simmons1984prisoners}
G.~J. Simmons, ``The prisoners’ problem and the subliminal channel,'' in \emph{Advances in Cryptology: Proceedings of Crypto 83}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 1984, pp. 51--67.

\bibitem{topkara2006hiding}
U.~Topkara, M.~Topkara, and M.~J. Atallah, ``The hiding virtues of ambiguity: quantifiably resilient watermarking of natural language text through synonym substitutions,'' in \emph{Proceedings of the 8th workshop on Multimedia and security}, 2006, pp. 164--174.

\bibitem{chang2010linguistic}
C.~Y. Chang and S.~Clark, ``Linguistic steganography using automatically generated paraphrases,'' in \emph{Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics}, 2010, pp. 591--599.

\bibitem{safaka2016matryoshka}
I.~Safaka, C.~Fragouli, and K.~Argyraki, ``Matryoshka: Hiding secret communication in plain sight,'' in \emph{6th USENIX Workshop on Free and Open Communications on the Internet (FOCI 16)}, 2016.

\bibitem{fang2017generating}
T.~Fang, M.~Jaggi, and K.~Argyraki, ``Generating steganographic text with {LSTMs},'' \emph{arXiv preprint arXiv:1705.10742}, 2017.

\bibitem{yang2018rnn}
Z.-L. Yang, X.-Q. Guo, Z.-M. Chen, Y.-F. Huang, and Y.-J. Zhang, ``{RNN-stega}: Linguistic steganography based on recurrent neural networks,'' \emph{IEEE Transactions on Information Forensics and Security}, vol.~14, no.~5, pp. 1280--1295, 2018.

\bibitem{ziegler2019neural}
Z.~Ziegler, Y.~Deng, and A.~M. Rush, ``Neural linguistic steganography,'' in \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, 2019, pp. 1210--1215.

\bibitem{xiang2017novel}
L.~Xiang, X.~Wang, C.~Yang, and P.~Liu, ``A novel linguistic steganography based on synonym run-length encoding,'' \emph{IEICE transactions on Information and Systems}, vol. 100, no.~2, pp. 313--322, 2017.

\bibitem{yang2020vae}
Z.-L. Yang, S.-Y. Zhang, Y.-T. Hu, Z.-W. Hu, and Y.-F. Huang, ``{VAE-Stega}: linguistic steganography based on variational auto-encoder,'' \emph{IEEE Transactions on Information Forensics and Security}, vol.~16, pp. 880--895, 2020.

\bibitem{yang2020gan}
Z.~Yang, N.~Wei, Q.~Liu, Y.~Huang, and Y.~Zhang, ``{GAN-TStega}: Text steganography based on generative adversarial networks,'' in \emph{Digital Forensics and Watermarking: 18th International Workshop, IWDW 2019, Chengdu, China, November 2--4, 2019, Revised Selected Papers 18}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2020, pp. 18--31.

\bibitem{de2022perfectly}
C.~S. de~Witt, S.~Sokota, J.~Z. Kolter, J.~Foerster, and M.~Strohmeier, ``Perfectly secure steganography using minimum entropy coupling,'' \emph{arXiv preprint arXiv:2210.14889}, 2022.

\bibitem{kaptchuk2021meteor}
G.~Kaptchuk, T.~M. Jois, M.~Green, and A.~D. Rubin, ``Meteor: Cryptographically secure steganography for realistic distributions,'' in \emph{Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security}, 2021, pp. 1529--1548.

\bibitem{zhang2024provably}
X.~Zhang, K.~Chen, J.~Ding, Y.~Yang, W.~Zhang, and N.~Yu, ``Provably secure public-key steganography based on elliptic curve cryptography,'' \emph{IEEE Transactions on Information Forensics and Security}, 2024.

\bibitem{zhang2021provably}
S.~Zhang, Z.~Yang, J.~Yang, and Y.~Huang, ``Provably secure generative linguistic steganography,'' \emph{arXiv preprint arXiv:2106.02011}, 2021.

\bibitem{ding2023discop}
J.~Ding, K.~Chen, Y.~Wang, N.~Zhao, W.~Zhang, and N.~Yu, ``Discop: Provably secure steganography in practice based on" distribution copies",'' in \emph{2023 IEEE Symposium on Security and Privacy (SP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2023, pp. 2238--2255.

\bibitem{dai2019towards}
F.~Dai and Z.~Cai, ``Towards near-imperceptible steganographic text,'' in \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, 2019, pp. 4303--4308.

\bibitem{shen2020near}
J.~Shen, H.~Ji, and J.~Han, ``Near-imperceptible neural linguistic steganography via self-adjusting arithmetic coding,'' in \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, 2020, pp. 303--313.

\bibitem{liao2024co}
G.~Liao, J.~Yang, K.~Pang, and Y.~Huang, ``{Co-Stega}: Collaborative linguistic steganography for the low capacity challenge in social media,'' in \emph{Proceedings of the 2024 ACM Workshop on Information Hiding and Multimedia Security}, 2024, pp. 7--12.

\bibitem{bai2024semantic}
M.~Bai, J.~Yang, K.~Pang, Y.~Huang, and Y.~Gao, ``Semantic steganography: A framework for robust and high-capacity information hiding using large language models,'' \emph{arXiv preprint arXiv:2412.11043}, 2024.

\bibitem{pang2024frestega}
K.~Pang, ``{FreStega}: A plug-and-play method for boosting imperceptibility and capacity in generative linguistic steganography for real-world scenarios,'' \emph{arXiv preprint arXiv:2412.19652}, 2024.

\bibitem{huang2024od}
Y.-S. Huang, P.~Just, K.~Narayanan, and C.~Tian, ``{OD-Stega}: {LLM}-based near-imperceptible steganography via optimized distributions,'' \emph{arXiv preprint arXiv:2410.04328}, 2024.

\bibitem{altman2021constrained}
E.~Altman, \emph{Constrained {Markov} decision processes}.\hskip 1em plus 0.5em minus 0.4em\relax Routledge, 2021.

\bibitem{vaswani2017attention}
A.~Vaswani, ``Attention is all you need,'' \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem{brown2020language}
T.~B. Brown, ``Language models are few-shot learners,'' \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem{touvron2023llama}
H.~Touvron, T.~Lavril, G.~Izacard, X.~Martinet, M.-A. Lachaux, T.~Lacroix, B.~Rozi{\`e}re, N.~Goyal, E.~Hambro, F.~Azhar \emph{et~al.}, ``Llama: Open and efficient foundation language models,'' \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{rissanen1979arithmetic}
J.~Rissanen and G.~G. Langdon, ``Arithmetic coding,'' \emph{IBM Journal of research and development}, vol.~23, no.~2, pp. 149--162, 1979.

\bibitem{bellman2015applied}
R.~E. Bellman and S.~E. Dreyfus, \emph{Applied dynamic programming}.\hskip 1em plus 0.5em minus 0.4em\relax Princeton university press, 2015, vol. 2050.

\bibitem{sutton2018reinforcement}
R.~S. Sutton and A.~G. Barto, \emph{Reinforcement learning: An introduction}.\hskip 1em plus 0.5em minus 0.4em\relax MIT press, 2018.

\bibitem{boyd2004convex}
S.~Boyd and L.~Vandenberghe, ``Convex optimization,'' \emph{Cambridge University Press}, 2004.

\end{thebibliography}


\newpage
\appendices
\section{Proof of Theorem 1} \label{pf:thm1}
Assuming a general optimal solution $\Tilde{d}(s,a)$ exists for problem $\mathbf{P1}$, we claim that a new solution $d^{\star} (s,a) = d_{s}^{\star} \delta(a_{s}^{\star})$ is at least as good, where
\begin{align}
    \begin{cases}d^{\star }_{s}\  =\  \int^{1}_{0} &\tilde{d} (s,a)\  da\\ a^{\star }_{s}\  =\  \int^{1}_{0} &a\frac{\tilde{d} (s,a)}{d^{\star }_{s}} da\end{cases} 
\end{align}
and $\delta( \cdot )$ is a Dirac-delta function. 

To prove this claim, we need to show that the function $d^{\star} (s,a)$ meets the constraints (\ref{eq:con_same1_TV})-(\ref{eq:opt_problem_con4_TV}), and the objective function value (\ref{eq:opt_problem_ob_TV}) is at least as large. 
\begin{enumerate}
    \item Constraints (\ref{eq:con_same1_TV}), (\ref{eq:opt_problem_con4_TV}) and (\ref{eq:con_same_sum1}): 
        It is clear that $d_{s}^{\star}$ is positive for all $s \in \{ 0,1\}$ and $\sum_{s=0}^{1} d_{s}^{\star}$ sum up to 1. Therefore $d^{\star} (s,a)$ satisfies the constraint (\ref{eq:opt_problem_con4_TV}) and (\ref{eq:con_same_sum1}). For constraint (\ref{eq:con_same1_TV}), 
        \begin{align}
            & \int_{0}^{1} (1-\gamma a)d^{\star}(0,a) da - \int_{0}^{1}\gamma a d^{\star}(1,a) da \notag\\%\label{eq:reduce_linear_con_pf_start}\\
            &= d_{0}^{\star} -\gamma a_{0}^{\star} d_{0}^{\star} -\gamma a_{1}^{\star} d_{1}^{\star} \notag\\           
            &= \int_{0}^{1} \Tilde{d}(0,a) da -\gamma d_{0}^{\star} \int^{1}_{0} a\frac{\tilde{d} (0,a)}{d^{\star }_{0}} da \nonumber \\
            & \qquad \qquad\qquad \qquad \qquad-\gamma d_{1}^{\star} \int^{1}_{0} a\frac{\tilde{d} (1,a)}{d^{\star }_{1}} da \notag\\
            &= \int_{0}^{1} \Tilde{d}(0,a) da -\gamma \int^{1}_{0} a \tilde{d} (0,a) da -\gamma \int^{1}_{0} a \tilde{d} (1,a) da \notag\\
            &= \int_{0}^{1} (1- \gamma a)\Tilde{d}(0,a) da - \gamma \int^{1}_{0} a \Tilde{d} (1,a) da =\mathbf{d}^{\gamma}_0, \label{eq:reduce_linear_con_pf_end}
        \end{align}
        where the equality in (\ref{eq:reduce_linear_con_pf_end}) follows from the fact that $\Tilde{d} (s,a)$ satisfies the constraint (\ref{eq:con_same1_TV}). 
    \item Constraint (\ref{eq:opt_problem_con3_TV}): We write
    \begin{align}
        &b  \geq \int_0^1 \TV_{0}(a)\tilde{d}(0,a) + \TV_{1}(a)\tilde{d}(1,a) da \notag\\
        & = \int_0^1 d^{\star}_{0}  \TV_{0}(a) \left( \frac{\tilde{d} (0,a)}{d^{\star }_{0}} \right)
        + d^{\star}_{1} \TV_{1}(a)\left( \frac{\tilde{d} (1,a)}{d^{\star }_{1}} \right)  da \notag\\
        & \geq d^{\star}_{0} \TV_{0}\left(\int^{1}_{0} a\left( \frac{\tilde{d} (0,a)}{d^{\star }_{0}} \right)  da\right) \nonumber \notag\\
        & \qquad \qquad\qquad +d^{\star}_{1} \TV_{1}\left(\int^{1}_{0} a\left( \frac{\tilde{d} (1,a)}{d^{\star }_{1}} \right)  da\right) \notag\\
        %\label{eq:pf_of_redcution_convex_TV}\\
        & = d^{\star}_{0}\TV_{0}(a_{0}^{\star}) + d^{\star}_{1} \TV_{1}(a_{1}^{\star}) \notag\\
        & = \int_0^1 d^{\star}(0,a) \TV_{0}(a)  + d^{\star}(1,a) \TV_{1}(a) da
    \end{align}
    where the inequality follows the Jensen's inequality since the total variation divergence is convex.
    \item The objective function: We wish to show that $d^{\star}(s,a)$ achieves a value equal to or exceeding that of $\tilde{d}(s,a)$. For this purpose, we write:
    \begin{align}
        & \int_0^1 H(a) \tilde{d}(0,a) da  + \int_0^1 H(a) \tilde{d}(1,a) da \notag\\
        &=  d_0^{\star} \int_0^1 H(a) \frac{\tilde{d} (0,a)}{d^{\star }_{0}}   da + d_1^{\star} \int_0^1 H(a) \frac{\tilde{d} (1,a)}{d^{\star }_{1}}  da \notag\\
        &\leq  d_0^{\star} H({\int_0^1 a  \frac{\tilde{d} (0,a)}{d^{\star }_{0}} da}) + d_1^{\star} H({\int_0^1 a\frac{\tilde{d} (1,a)}{d^{\star }_{1}}da}) \notag\\
        %\label{eq:pf_of_reduction_concave_TV} \\
        &=  d_0^{\star} H({a^{\star}_0}) + d_1^{\star} H({a^{\star}_1}) \nonumber \\
        &=  \int_0^1 H(a) d^{\star}(0,a) da   + \int_0^1 H(a) d^{\star}(1,a) da, 
    \end{align}
    where again the inequality is due to Jensen's inequality since the entropy function is concave.
\end{enumerate}
We can now conclude that $d^{\star} (s,a)$ is also an optimal solution to the optimization problem. The solution $d^{\star} (s,a)$ further implies that the optimal policy is 
\begin{align}
    \pi(a|s) = \frac{d^{\star}(s,a)}{\int_0^1 d^{\star}(s,a) da} = \begin{cases}
        1 , ~ & a = a_{s}^{\star} \\
        0, ~ & \text{else}
    \end{cases}
\end{align} 
which indicates that there is only one action $a_{s}^{\star}$ in interval $[0,1]$ to take in each state $s$, i.e., a deterministic policy is optimal. Substituting the variable $d(s,a)$ in problem $\mathbf{P1}$ with $d^{\star}(s,a)$ will yield an equivalent problem to $\mathbf{P2}$. In other words, this optimization problem reduces the number of variables from uncountably many to merely four: the action points $a_{0}^{\star}$, $a_{1}^{\star}$, and the discounted state visitation distribution $d_0^{\star}$, $d_1^{\star}$.

\section{Proof of Theorem 2 and Theorem 3} \label{pf:thm2}

To solve the convex optimization problem $\mathbf{P3}$, our plan is to find a solution to the KKT conditions, since the problem is convex. However, the total variation constraint poses a challenge as it is not differentiable when $ \frac{x_0}{d_0} = p_0$ and $ \frac{x_1}{d_1} = p_1$, preventing us from performing a direct partial differentiation on this inequality constraint. Therefore, we introduce a set of auxiliary variables $c_0, c_1$ such that $d_0 |\frac{x_0}{d_0} - p_0| = |x_0 - d_0 p_0| \leq c_0$ and $d_1|\frac{x_1}{d_1} - p_1| = |x_1 - d_1 p_1|\leq c_1$. The optimization problem $\mathbf{P3}$ can be reformulated as follows. 
\begin{align}
    \max_{x_0,x_1, d_0, d_1, c_0, c_1} & ~ d_0 H(\frac{x_0}{d_0}) + d_1 H(\frac{x_1}{d_1})  \label{eq:nx0_obj_func_TV}\\
    \textnormal{subject to } 
    & ~ c_0 + c_1\leq \frac{b}{2} \label{eq:nx0_constraint_1_TV}\\ 
    & -c_0 \leq x_0 - d_0 p_0\leq c_0 \label{eq:nx0_constraint_2_TV}\\
    & -c_1 \leq x_1 - d_1 p_1 \leq c_1 \label{eq:nx0_constraint_3_TV}\\
    & ~  d_0 -\gamma x_0  - \gamma x_1 = \mathbf{d}^{\gamma}_0 \label{eq:nx0_constraint_4_TV}\\
    & ~ d_0 + d_1 = 1 \label{eq:nx0_constraint_5_TV}\\
    & ~ 0 \leq x_0  \leq d_0 ,  ~ 0 \leq x_1  \leq d_1 \label{eq:nx0_constraint_6_TV}
    %& ~ c_0 \geq 0, ~ c_1 \geq 0 \label{eq:nx0_constraint_7_TV}
\end{align}
The Lagrangian function of this problem is: 
\begin{align}
    \mathcal{L} = & - \left( d_0 H(\frac{x_0}{d_0}) + d_1 H(\frac{x_1}{d_1}) \right) + \lambda \left( c_0 + c_1 - \frac{b}{2} \right) \nonumber \\
    & \alpha_0 (-c_0 -x_0 +d_0 p_0) + \beta_0 (x_0 - d_0 p_0 -c_0 ) \nonumber \\
    & + \alpha_1 (-c_1 -x_1 +d_1 p_1) + \beta_0 (x_1 - d_1 p_1 -c_1 ) \nonumber \\
    & + \omega_0 \left( d_0 - \mathbf{d}^{\gamma}_0 -\gamma (x_0 +x_1)\right) + \omega_1 \left( d_0 +d_1 -1 \right) \nonumber \\
    & - \nu_0 x_0 - \nu_1 x_1 + \mu_0(x_0 -d_0) + \mu_1 (x_1 - d_1)
\end{align}
where $\lambda, \alpha_s, \beta_s ,\omega_s,  \nu_s, \mu_s$ are the Lagrange multipliers of constraints (\ref{eq:nx0_constraint_1_TV}) to (\ref{eq:nx0_constraint_6_TV}).
The KKT conditions are written as follows: 
\begin{enumerate}
    \item Stationarity: 
    \begin{align}
        %\begin{cases}
            \frac{\partial  \mathcal{L}}{\partial x_{0}} & = - \log\left( \frac{d_0 - x_0}{x_0} \right) - \alpha_0 + \beta_0 \notag\\
            &\qquad\qquad\qquad- \gamma \omega_0 - \nu_0 + \mu_0 = 0 \label{eq:stationary1} \\
            \frac{\partial  \mathcal{L}}{\partial x_{1}} & = - \log\left( \frac{d_1 - x_1}{x_1} \right) - \alpha_1 + \beta_1 \notag\\
            &\qquad\qquad\qquad- \gamma \omega_0 - \nu_1 + \mu_1 = 0 \label{eq:stationary2} \\
            \frac{\partial  \mathcal{L}}{\partial d_{0}} & = - H(\frac{x_0}{d_0}) + \frac{x_0}{d_0} \log \left( \frac{d_0-x_0}{x_0}\right) \notag\\
            &\qquad+ \alpha_0 p_0 - \beta_0 p_0 + \omega_0 + \omega_1 - \mu_0 = 0\\
            \frac{\partial  \mathcal{L}}{\partial d_{1}} & = - H(\frac{x_1}{d_1}) + \frac{x_1}{d_1} \log \left( \frac{d_1-x_1}{x_1}\right) \notag\\
            &\qquad+\alpha_1 p_1 - \beta_1 p_1 + \omega_1 - \mu_1= 0 \\
            \frac{\partial  \mathcal{L}}{\partial c_{0}} & = \lambda - \alpha_0 - \beta_0
            = 0 \\
            \frac{\partial  \mathcal{L}}{\partial c_{1}} & = \lambda - \alpha_1 - \beta_1 = 0 \label{eq:stationary6}
        %\end{cases}         
    \end{align}
    \item Primal feasibility: 
        \begin{align}
            %\begin{cases}
            & c_0 + c_1 \leq \frac{b}{2} \label{eq:primal_feas1}\\
            & -c_0 -x_0 + d_0 p_0 \leq 0, ~ -c_1 -x_1 + d_1 p_1 \leq 0 \\
            & x_0 - d_0 p_0  - c_0 \leq 0, ~ x_1 - d_1 p_1 - c_1 \leq 0 \\
            & d_0 = \mathbf{d}^{\gamma}_0 + \gamma (x_0 +x_1) \\
            & d_0 + d_1 = 1 \\
            & x_0 \geq 0, ~ x_1 \geq 0 \\
            & x_0 - d_0 \leq 0, ~ x_1 - d_1 \leq 0  \label{eq:primal_feas7}
                %  & c_0 \geq 0, ~ c_1 \geq 0 
            %\end{cases}
        \end{align}
    \item Dual feasibility:
        \begin{align}
            %\begin{cases}
                & \lambda \geq 0 \label{eq:dual_feas1}\\
                & \alpha_0 \geq 0, ~ \alpha_1 \geq 0 \label{eq:dual_feas2}\\
                & \beta_0 \geq 0, ~ \beta_1 \geq 0 \label{eq:dual_feas3}\\
                & \nu_0 \geq 0, ~ \nu_1 \geq 0 \label{eq:dual_feas4}\\
                & \mu_0 \geq 0, ~ \mu_1 \geq 0 \label{eq:dual_feas5}
                 % & \rho_0 \geq 0, ~ \rho_1 \geq 0 
            %\end{cases}
        \end{align}
    \item Complementary slackness:
    \begin{align}
%            \begin{cases}
                & \lambda \left( c_0 + c_1 - \frac{b}{2}\right) = 0 \label{eq:complementary1}\\
                & \alpha_0 (-c_0 -x_0 + d_0 p_0) \notag\\
                &\qquad\qquad= \alpha_1 (-c_1 -x_1 + d_1 p_1) =  0 \\
                & \beta_0 (x_0 - d_0 p_0 -c_0) = \beta_1 (x_1 - d_1 p_1 -c_1) = 0 \\
                & \omega_0 \left(d_0 - \mathbf{d}^{\gamma}_0 -\gamma (x_0 +x_1) \right)  = 0 \\
                & \omega_1 \left(d_0 +d_1 -1 \right)  = 0  \\
                & \nu_0 x_0 = \nu_1 x_1 = 0 \\
                & \mu_0(x_0 - d_0)= \mu_1( x_1 - d_1) = 0 \label{eq:complementary7}
                % & \rho_0 c_0 = \rho_1 c_1 = 0 
 %           \end{cases}
    \end{align}
\end{enumerate}
The solution that satisfies the KKT conditions will be presented in several cases, depending on the values of $p_0$ and $p_1$, separated into different regimes of the cost constraint $b$. The expression formulas for the primal variables $x_s, d_s$ and the dual variables $\omega_s, \nu_s, \mu_s$ where $s \in \{0,1\}$ are identical across all scenarios, as presented below.
\begin{align}
    %\begin{cases}
        & x_0 = a_0 d_0 \label{eqn:x0}\\
        & x_1 = a_1 d_1 \\
        & d_0 = \frac{\mathbf{d}^{\gamma}_0 + \gamma a_1}{1 - \gamma a_0 + \gamma a_1} \\
        & d_1 = 1 - d_0 \label{eqn:d1}\\
        & \omega_0 = \frac{-1}{\gamma} \log(\frac{1-a_0}{a_0}) - \frac{1}{\gamma} \alpha_0 + \frac{1}{\gamma} \beta_0 \notag\\
        &\qquad = \frac{-1}{\gamma} \log(\frac{1-a_1}{a_1}) - \frac{1}{\gamma} \alpha_1 + \frac{1}{\gamma} \beta_1 \label{eqn:w0}\\
        & \omega_1 = -\log(1-a_1) - \alpha_1 p_1 + \beta_1 p_1 \\
        & \nu_s = \mu_s = 0, ~ s \in \{ 0, 1\}\label{eqn:nu}
    %\end{cases} 
\end{align}
Therefore, we will not write them out particularly in each case, since they can derived from $a_s, \alpha_s, \beta_s, ~ s \in \{0,1\} $. 

We define the mathematical expressions below for simplifications:
\begin{align} 
    & z_0(a_0, a_1)  =  \left( -1-\gamma p_{1}\right)  \log \left( \frac{1-a_{0}}{a_{0}} \right)   \nonumber \\
    & \qquad \qquad   +\gamma p_{1}\log \left( \frac{1-a_{1}}{a_{1}} \right) +\gamma \log \left( \frac{1-a_{0}}{1-a_{1}} \right)\\
    & z_1(a_0, a_1)  =  \left( -\gamma p_{0}\right)  \log \left( \frac{1-a_{0}}{a_{0}} \right)  \nonumber \\
    &\qquad    +(-1+ \gamma p_{0} ) \log \left( \frac{1-a_{1}}{a_{1}} \right) +\gamma \log \left( \frac{1-a_{0}}{1-a_{1}} \right) 
\end{align}


\subsection{Proof of Theorem 2}
In Theorem \ref{thm:p0p1sameside}, the assumption is that $p_0, p_1 \geq \frac{1}{2}$. 

There are three regimes:
    \begin{enumerate}
        \item The regime $b \in [0, b_l) $:       
        In this regime, the two cases  $p_1\geq p_0\geq \frac{1}{2}$ and $p_0> p_1\geq \frac{1}{2}$ lead to different solution expressions, and let us consider the first case for now. With the solution of $a_0,a_1$ given in the theorem, and in the given range of $b$, the range of $a_0$ and $a_1$ are 
        \begin{align}
            %\begin{cases}
                %a_{0} \in [p_1, p_0], ~ a_1 = p_1, % & ~ |\frac{1}{2} - p_1 | < |\frac{1}{2} - p_0 | \\
                a_0 = p_0, ~ a_{1} \in [p_0, p_1],% &  ~ |\frac{1}{2} - p_1 | \geq |\frac{1}{2} - p_0 | 
            %\end{cases}
        \end{align}
        The variable assignments are as follows. 
        \begin{enumerate}
        \item Primal variable assignments: 
        \begin{align}
            & a_0 = p_0,\quad a_1 = \eta_1^{-}(b)\\%&, \text{if}~ |\frac{1}{2} - p_1 | \geq |\frac{1}{2} - p_0 |\\
            %    \eta_0^{-}(b) &, \text{if}~|\frac{1}{2} - p_1 | < |\frac{1}{2} - p_0 |            
            %& a_1 = \eta_1^{-}(b) &, \text{if}~ |\frac{1}{2} - p_1 | \geq |\frac{1}{2} - p_0 |\\
            %    p_1 &, \text{if} ~ |\frac{1}{2} - p_1 | < |\frac{1}{2} - p_0 |            
            &    c_0 = 0, \quad c_1 = \frac{b}{2}, %& \text{if}~ |\frac{1}{2} - p_1 | \geq |\frac{1}{2} - p_0 |\\
              %  c_0 = \frac{b}{2},~  c_1 = 0, & \text{if} ~ |\frac{1}{2} - p_1 | < |\frac{1}{2} - p_0 | 
            %\end{cases}
        \end{align}
        Using (\ref{eqn:x0})-(\ref{eqn:d1}), all the primal variables are assigned. It is important to note that $x_0$ and $x_1$ are non-zero in this setting because $a_0$ and $a_1$ are not zero, and similarly, $d_0$ and $d_1$ can be assumed to be non-zero stated in section \ref{sec:simplfy_opt}. As a result, we are able to derive the stationary conditions (\ref{eq:stationary1}) and (\ref{eq:stationary2}). 
        \item Dual variable assignments:         
        \begin{align}
                 & \alpha_0 = \max\{ 0 , Mz_0(a_0, a_1)\} \\
                 & \beta_0 = \alpha_1 - \alpha_0 \\
                 & \alpha_1 = Mz_1(a_0, a_1) \\
                 & \beta_1 = 0 \\
                 %& \rho_0 = \alpha_1 - |Mz_0(a_0, a_1)| \\
                 %& \rho_1 = 0 \\
                 & \lambda = \alpha_1 \label{eq:case1a_dual}
        \end{align}
        With (\ref{eqn:w0})-(\ref{eqn:nu}), all the dual variables are assigned. 
        \end{enumerate}
        \textcolor{black}{
        We wish to show that the assignments satisfy all the KKT conditions. With the primal and dual variables displayed above, one can conveniently confirm that (\ref{eq:stationary1})-(\ref{eq:stationary6}), (\ref{eq:primal_feas1})-(\ref{eq:primal_feas7}), (\ref{eq:dual_feas4})-(\ref{eq:dual_feas5}), and (\ref{eq:complementary1})-(\ref{eq:complementary7}) are satisfied by substituting these variables.         
        It remains to demonstrate the validity of (\ref{eq:dual_feas1})-(\ref{eq:dual_feas3}). It is evident that $\alpha_0$ and $\beta_1$ are non-negative. Thus, we need only establish that $\alpha_1,\beta_0 \geq 0$. \\
        For $\alpha_1$, we use the following partial derivative to show that $z_1$ is increasing in $a_1$:
        \begin{align}
            \frac{\partial z_1}{\partial a_{1}}  & = \frac{1\  -\gamma p_{0}+\gamma a_{1}}{(1-a_{1})a_{1}} \geq 0  \label{eq:dz1_da1}
        \end{align}
        This immediately implies that $z_1(a_0 = p_0,a_1)$ is monotonically increasing. It suffices to show that $z_1(p_0, \frac{1}{2})$ is positive, which is the minimum value of $z_1(p_0,a_1)$ achieved in the range $a_1\in [p_0,p_1]$. We write
        \begin{align}
            & z_1(p_0, \frac{1}{2}) = \left[ (- \gamma p_0) \log \left( \frac{1-p_0}{p_0}\right) + \gamma \log \left( \frac{1-p_0}{\frac{1}{2}}\right) \right] \nonumber \\
            & = \left[ (1- p_0) \log (1-p_0) + p_0 \log (1-p_0) - \log\frac{1}{2} \right] \nonumber \\
            & = \gamma (-H(p_0) + H(\frac{1}{2})) \geq 0, \label{eq:case1a_z1geq0}     
        \end{align}
        from which we conclude that $\alpha_1 $ is non-negative since $M$ is clearly positive. 
        For $\beta_0 $, \\
        \begin{align}
        	\beta_0 & = \alpha_1 - \alpha_0 \\
			& = \begin{cases}
					-\log \left( \frac{1-a_{1}}{a_{1}} \right)  +\log \left( \frac{1-a_{0}}{a_{0}} \right)   & , ~ z_0(a_0,a_1) \geq 0 \\
					\alpha_1 & , ~ z_0(a_0,a_1) < 0
				\end{cases} \label{eq:case1a1_rhogeq0}
        \end{align}
        From equation (\ref{eq:case1a1_rhogeq0}), it is clear that $\beta_0 \geq 0$ when $z_0 < 0$ as $\alpha_1 \geq 0$. Similarly, for $z_0 \geq 0$, it follows that $\beta_0$ is non-negative since $a_1 \geq a_0 = p_0 \geq \frac{1}{2}$. \\   
        }
        
        Now let us turn our attention to the case when $p_0 \geq p_1 \geq \frac{1}{2}$. We can determine the range of $a_0$ and $a_1$ as 
        \begin{align}
            %\begin{cases}
                %a_{0} \in [p_1, p_0], ~ a_1 = p_1, % & ~ |\frac{1}{2} - p_1 | < |\frac{1}{2} - p_0 | \\
                a_0 = [p_1,p_0], ~ a_{1} = p_1 .%, &  ~ |\frac{1}{2} - p_1 | \geq |\frac{1}{2} - p_0 | 
            %\end{cases}
        \end{align}
        The variable assignments are as follows. 
        \begin{enumerate}
        \item Primal variable assignments: 
        \begin{align}
            %\begin{cases}
                & a_0 = \eta_0^{-}(b),\quad a_1 = p_1 \\
                %c_0 = 0, ~ c_1 = \frac{b}{2}, %& \text{if}~ |\frac{1}{2} - p_1 | \geq |\frac{1}{2} - p_0 |\\
                & c_0 = \frac{b}{2},~  \quad c_1 = 0 %, & \text{if} ~ |\frac{1}{2} - p_1 | < |\frac{1}{2} - p_0 | 
            %\end{cases}
        \end{align}
        Using (\ref{eqn:x0})-(\ref{eqn:d1}), all the primal variables are assigned. Observe that $x_0$ and $x_1$ will not be zero here as well. 
        \item Dual variable assignments:
        \begin{align}
            %\begin{cases}
                & \alpha_0 = Mz_0(a_0, a_1) \\
                & \beta_0 = 0 \\
                & \alpha_1 = Mz_1(a_0, a_1) \\
                & \beta_1 = \log \left( \frac{1-a_{1}}{a_{1}} \right) -\log \left( \frac{1-a_{0}}{a_{0}} \right) \\
                & \lambda = \alpha_0 
            %\end{cases}            
        \end{align}
        With (\ref{eqn:w0})-(\ref{eqn:nu}), all the dual variables are assigned.
        \end{enumerate}
        
        Same as the previous case, we wish to show that the assignments satisfy all the KKT conditions. With the primal and dual variables displayed above, one can conveniently confirm that (\ref{eq:stationary1})-(\ref{eq:stationary6}), (\ref{eq:primal_feas1})-(\ref{eq:primal_feas7}), (\ref{eq:dual_feas4})-(\ref{eq:dual_feas5}), and (\ref{eq:complementary1})-(\ref{eq:complementary7}) are satisfied by substituting these variables. It remains to demonstrate the validity of (\ref{eq:dual_feas1})-(\ref{eq:dual_feas3}). It is evident that $\beta_0$ satisfies the non-negativity. Thus, we need only establish that $\alpha_0,\alpha_1, \beta_1 \geq 0$. \\
        For $\alpha_0$, we use the following partial derivative to show that $z_0$ is increasing in $a_0$: 
        \begin{align}
            \frac{\partial z_0}{\partial a_{0}}  & = \frac{1+\gamma p_{1}-\gamma a_{0}}{(1-a_{0})a_{0}} \label{eq:dz0_da0} \geq 0
        \end{align}
        (\ref{eq:dz0_da0}) implies that $z_0(a_0,a_1 =p_1)$ is increasing in $a_0$. It suffices to show that $z_0(p_1, p_1)$ is positive, which is the minimum value of $z_0(a_0,p_1)$ achieved in the range $a_0 \in [p_1, p_0]$. 
        \begin{align}
            z_0(p_1, p_1) & = -\log \left( \frac{1-p_1}{p_1} \right) \geq 0 
        \end{align}
         For $\alpha_1$, it is demonstrated that $z_1(a_0,p_1)$ increases and remains non-negative at its lowest value $a_0 \in [\frac{1}{2}, p_0]$, as shown in (\ref{eq:dz1_da0}) and (\ref{eq:z1geq0-thm2-2}).
        \begin{align}
            & \frac{\partial z_1}{\partial a_{0}}  =\frac{\gamma p_{0}-\gamma a_{0}}{(1-a_{0})a_{0}} \geq 0 , ~ \forall a_0 \in [p_1,p_0] \label{eq:dz1_da0} \\
            & z_1(\frac{1}{2}, p_1)  =  (-1 + \gamma p_0) \log \left( \frac{1-p_1}{p_1}\right) \nonumber \\
            & \qquad \qquad \qquad + \gamma \log \left( \frac{\frac{1}{2}}{1-p_1}\right) \geq 0 \label{eq:z1geq0-thm2-2} 
        \end{align}
        Furthermore, it is evident that $\beta_1$ is greater than zero since $a_0 \geq a_1 = p_1 \geq \frac{1}{2}$. This establishes that $\alpha_0, \alpha_1, \beta_1, \lambda$ are all non-negative.
         
    \item $b \in [b_l, b_h)$: In this regime, we have the following assignments.
     \begin{enumerate}    
    \item Primal variable assignments: 
    \begin{align}
        %\begin{cases}
            & a_0 = a_1 = -M(\frac{b}{2} - \mathbf{d}_0^{\gamma}p_0 - \mathbf{d}_1^{\gamma}p_1)\\
            & c_0 = d_0(p_0-a_0),\quad c_1 = d_1(p_1-a_1).
        %\end{cases}
    \end{align}
    Using (\ref{eqn:x0})-(\ref{eqn:d1}), all the primal variables are assigned. Observe that $x_0$ and $x_1$ will not be zero here, indicating that stationary conditions can be derived.  
    \item Dual variable assignments: 
    \begin{align}
        \begin{cases}
        & \alpha_0 = \alpha_1 = M \log \left( \frac{a_{0}}{1-a_{0}} \right)  \\
        & \beta_0 = \beta_1 = 0\\
        & \lambda = \alpha_1 \\
        \end{cases}
    \end{align}
    \end{enumerate}
    With (\ref{eqn:w0})-(\ref{eqn:nu}), all the dual variables are assigned. 
    
    Similar to the previous discussion, we wish to show that the assignments satisfy all the KKT conditions. With the primal and dual variables displayed above, one can conveniently confirm that (\ref{eq:stationary1})-(\ref{eq:stationary6}), (\ref{eq:primal_feas1})-(\ref{eq:primal_feas7}), (\ref{eq:dual_feas3})-(\ref{eq:dual_feas5}), and (\ref{eq:complementary1})-(\ref{eq:complementary7}) are satisfied by substituting these variables. Since $a_0 \geq \frac{1}{2}$, we have that $\alpha_s, \lambda$ are non-negative, leading to   (\ref{eq:dual_feas1}) and (\ref{eq:dual_feas2}) in this case.
    \item $b \in [b_h, \infty)$: In this regime, we have  
    \begin{enumerate}
    \item Primal variables: 
    \begin{align}
        & a_0 = a_1 = \frac{1}{2}\\
        & c_0 = d_0(p_0-\frac{1}{2}), \quad c_1 = d_1(p_1-\frac{1}{2}).
    \end{align}
    Using (\ref{eqn:x0})-(\ref{eqn:d1}), all the primal variables are assigned. Observe that $x_0$ and $x_1$ will not be zero here as well. 
    \item Dual variables: 
    \begin{align}
        & \alpha_0 = \alpha_1 = \beta_0 = \beta_1 =  \rho_0 = \rho_1 = \lambda = 0.
    \end{align}
    All the dual variables are assigned using (\ref{eqn:w0})-(\ref{eqn:nu}). 
    \end{enumerate}
    With the primal and dual variables displayed above, one can conveniently confirm that (\ref{eq:stationary1})-(\ref{eq:stationary6}), (\ref{eq:primal_feas1})-(\ref{eq:primal_feas7}), (\ref{eq:dual_feas1})-(\ref{eq:dual_feas5}), and (\ref{eq:complementary1})-(\ref{eq:complementary7}) are satisfied by substituting these variables. 
\end{enumerate}

\subsection{Proof of Theorem 3} \label{pf:thm3}
Theorem \ref{thm:p0p1diffside} holds under the assumption $p_0 < \frac{1}{2} \leq p_1$. There are again three regimes:
\begin{enumerate}
    \item The regime $b \in [0, b_l') $: In this regime, the two cases $|\frac{1}{2}-p_1| \geq |\frac{1}{2}-p_0|$ and $|\frac{1}{2}-p_1| < |\frac{1}{2}-p_0|$ lead to different solution expressions, and let us consider the first case for now. Observe that in the first case implies that $p_1 + p_0 -1 \geq 0$, $b_l'$ in this case can be written as: 
    \begin{align}
        &b_l'  = 2\left( \frac{1-\gamma p_{0}-\mathbf{d}^{\gamma}_0 }{1-\gamma p_{0}+\gamma \psi_{1} } \right)  (p_{1}-\psi_{1} )
    \end{align}
    by the definition given in (\ref{eq:bl'_def}). 

    With the solution of $a_0,a_1$ given in the theorem, and in the given range of $b$, the range of $a_0$ and $a_1$ are
    \begin{align}
        a_0 = p_0, ~ a_{1} \in [\psi_1, p_1]\subset[\frac{1}{2}, p_1]
    \end{align}
    The variable assignments are as follows. 
    \begin{enumerate}
        \item Primal variable assignments:
        \begin{align}
            & a_0 = p_0,\quad a_1 = \eta_1^{-}(b) \\
            & c_0 = 0, \quad c_1 = \frac{b}{2}
        \end{align}
         Using (\ref{eqn:x0})-(\ref{eqn:d1}), all the primal variables are assigned. Note that in this scenario, $x_0$ and $x_1$ are non-zero, enabling us to address the stationarity conditions in (\ref{eq:stationary1}) and (\ref{eq:stationary2}).
        \item Dual variable assignments: 
        \begin{align}
            & \alpha_0 = z_1(a_0,a_1) + z_0(a_0,a_1)= -M \Psi_{p_0}(a_1) \\
            & \alpha_1 = z_1(a_0,a_1) \\
            & \beta_0 = -z_0(a_0,a_1)  \\
            & \beta_1 =  0\\
            & \lambda =\alpha_1 
        \end{align}
        With (\ref{eqn:w0})-(\ref{eqn:nu}), all the dual variables are assigned.
    \end{enumerate}
    We wish to show that the assignments satisfy all the KKT conditions. With the primal and dual variables displayed above, one can conveniently confirm that (\ref{eq:stationary1})-(\ref{eq:stationary6}), (\ref{eq:primal_feas1})-(\ref{eq:primal_feas7}), (\ref{eq:dual_feas4})-(\ref{eq:dual_feas5}), and (\ref{eq:complementary1})-(\ref{eq:complementary7}) are satisfied by substituting these variables. It remains to verify  (\ref{eq:dual_feas1})-(\ref{eq:dual_feas3}). It is evident that $\beta_1$ is non-negative. Thus, we need only establish that $\alpha_0, \alpha_1,\beta_0 \geq 0$. \\
    (\ref{eq:dz1_da1}) shows that $z_1$ is monotonically increasing in $a_1$, while  (\ref{eq:case1a_z1geq0}) confirmed that the minimum value of $z_1$ is non-negative, occurring when $a_1 = \frac{1}{2}$ within the interval $a_1 \in [\psi_1, p_1]\subset [\frac{1}{2}, p_1] $. Therefore, indeed $\alpha_1\geq 0$.

    For $\beta_0$, the following partial derivative shows that $-z_0$ is monotonically non-decreasing in $a_1$: 
    \begin{align}
        \frac{\partial z_0}{\partial a_{1}}  & =\frac{-\gamma p_{1}+\gamma a_{1}}{(1-a_{1})a_{1}} \leq 0, ~ \forall a_1 \in [\psi_1, p_1] \label{eq:dz0_da1} 
    \end{align}
    It suffices to show that $-z_0(p_0, a_1 = \frac{1}{2})$ is positive, which is the minimum value of $-z_0(p_0,a_1)$ achieved in the range $a_1\in [\frac{1}{2},p_1]$. We write
    \begin{align}
        &-z_0(p_0,\frac{1}{2}) \notag\\
        &= (1+\gamma p_{1})\log \left( \frac{1-p_{0}}{p_{0}} \right)  -\gamma \log \left( \frac{1-a_{0}}{\frac{1}{2} } \right)  \\
        & = \gamma \left( \log \left( \frac{1-p_{0}}{p_{1}} \right)  -\log \left( \frac{1-a_{0}}{\frac{1}{2} } \right)  \right)  \nonumber \\
        &\qquad +(1-\gamma +\gamma p_{1})\log \left( \frac{1-p_{0}}{p_{0}} \right) \geq 0.
    \end{align}
    Therefore, $\beta_0\geq 0$.
    
    To demonstrate $\alpha_0 \geq 0$, it is necessary to establish that $-M \Psi_{p_0}(a_1)$ is non-negative. Employing a similar approach as earlier, we first show that $-M \Psi_{p_0}(a_1)$ increases when $a_1 \in [\psi_1, p_1]$ by way of (\ref{eq:psia1_da1_geq0}). Then (\ref{eq:case3a_rho0_psi1_geq0}) confirms that its minimum value is zero, occurring at $a_1 = \psi_1$.
    \begin{align}
        -\frac{\partial}{\partial a_{1}} M\Psi_{p_0}(a_1)& = -M \Psi_{p_0}'(a_1) = M\Phi_-'(a_1) \notag\\
        & =\frac{2\gamma a_1+1-\gamma p_0 -\gamma p_1}{(1-a_1)a_1} \geq 0 \label{eq:psia1_da1_geq0}\\
        -M \Psi_{p_0}(\psi_1) & = -M(\Phi_+(p_0) -\Phi_-(\psi_1))  = 0 \label{eq:case3a_rho0_psi1_geq0} 
    \end{align}
    The inequality in (\ref{eq:psia1_da1_geq0}) follows from the fact that $a_1 \geq \frac{1}{2}$. 
    The above establishes that $\alpha_0, \alpha_1, \beta_0, \lambda$ are all non-negative.
    
     Now let us turn our attention to the case when $|\frac{1}{2}-p_1| <|\frac{1}{2}-p_0| $, implying $1-p_1 - p_0 \geq 0$. $b_l'$ in this case can be written as: 
    \begin{align}
        &b_l'  = 2\left( \frac{\mathbf{d}^{\gamma}_0 +\gamma p_{1}}{1-\gamma \psi_{0} +\gamma p_{1}} \right)  (\psi_{0} -p_{0})
    \end{align}
    again by the definition given in (\ref{eq:bl'_def}).  
    
     We can determine the range of $a_0$ and $a_1$ as 
     \begin{align}
        a_0 \in [p_0,\psi_0], ~ a_{1} = p_1.
    \end{align}
    The variable assignments are as follows. 
    \begin{enumerate}
        \item Primal variable assignments: 
        \begin{align}
            & a_0 = \eta_0^{+}(b), ~ a_1 = p_1 \\
            & c_0 = \frac{b}{2}, ~ c_1 = 0 
        \end{align}
        Using (\ref{eqn:x0})-(\ref{eqn:d1}), all the primal variables are assigned. Note that $x_0$ and $x_1$ are also non-zero in this context, allowing us to derive stationary conditions (\ref{eq:stationary1}) and (\ref{eq:stationary2}).
        \item Dual variable assignments:
        \begin{align}
            & \alpha_0 = 0 \\
            & \alpha_1 = z_1(a_0,a_1) \\
            & \beta_0 = -z_0(a_0,a_1)  \\
            & \beta_1 = -z_0(a_0,a_1) - z_1(a_0,a_1) = M\Psi_{p_1}(a_0)  \\
            & \lambda = \beta_0
        \end{align}
        With (\ref{eqn:w0})-(\ref{eqn:nu}), all the dual variables are assigned.
    \end{enumerate}
    Same as in the previous case, we wish to show that the assignments satisfy all the KKT conditions. With the primal and dual variables displayed above, one can conveniently confirm that (\ref{eq:stationary1})-(\ref{eq:stationary6}), (\ref{eq:primal_feas1})-(\ref{eq:primal_feas7}), (\ref{eq:dual_feas4})-(\ref{eq:dual_feas5}), and (\ref{eq:complementary1})-(\ref{eq:complementary7}) are satisfied by substituting these variables. It remains to verify (\ref{eq:dual_feas1})-(\ref{eq:dual_feas3}). It is evident that $\alpha_0$ satisfies the non-negativity condition. Thus, we need only establish that $\alpha_1,\beta_0, \beta_1 \geq 0$. 
    
    For $\alpha_1$, (\ref{eq:dz1_da0}) showed that $z_1$ is monotonically non-increasing when $a_0 \in [p_0, \psi_0]$. Moreover, (\ref{eq:z1geq0-thm2-2}) implies that $z_1(\frac{1}{2}, p_1)$ is positive, which is the minimum value of $z_1(a_0,p_1)$ achieved in the range $a_0 \in [p_0, \psi_0] \subset [p_0, \frac{1}{2}]$. Therefore $\alpha_1$ is non-negative.

    For $\beta_0$, (\ref{eq:dz0_da0}) showed that $z_0(a_0, p_1)$ is monotonically non-decreasing in $a_0$, implying that $\beta_0$ is decreasing in $a_0$. Moreover, we can write
    \begin{align}
        -z_0(\frac{1}{2}, p_1) & = -\gamma p_{1}\log \left( \frac{1-p_{1}}{p_{1}} \right)  -\gamma \log \left( \frac{\frac{1}{2} }{1-p_{1}} \right)\nonumber \\
        &  =\gamma \left( H(\frac{1}{2} )-H(p_{1})\right)  \geq 0,
    \end{align}
    which implies thatthe minimum value of $-z_0(a_0,p_1)$ in the range $a_0 \in [p_0, \psi_0] \subset [p_0, \frac{1}{2}]$ is non-negative. Combining these two facts, we conclude that $\beta_0$ is also non-negative. 

    For $\beta_1$, a similar procedure can be used to show its non-negativity. (\ref{eq:case3a_rho1decreasing}) implies that $M\Psi_{p_1}(a_0)$ is monotonically non-increasing in $a_0 \in [p_0, \psi_0]$, given that $a_0 \leq \frac{1}{2}$. Furthermore, as shown in (\ref{eq:case3a_rho1_psi0_geq0}), the minimum value of $M\Psi_{p_1}(a_0)$ over the interval $a_0 \in [p_0, \psi_0]$ is zero, which is derived from the definition of $\psi_0$ in Theorem \ref{thm:p0p1diffside}.  
    \begin{align}
        & \frac{\partial }{\partial a_{0}} M \Psi_{p_1}(a_0) =\  M\Phi^{\prime}_+ (a_{0}) \notag \\
        & = \frac{2\gamma a_0 - 1 - \gamma p_0 - \gamma p_1}{(1-a_0)a_0} \leq 0, \notag\\
        &\qquad\qquad \forall a_0 \in [p_0, \psi_0] \subset[p_0, \frac{1}{2}] \label{eq:case3a_rho1decreasing} 
    \end{align} 
    \begin{align}
         M \Psi_{p_1}(\psi_0) = M(\Phi_{+} (\psi_0)-\Phi_{-} (a_{1})) =0  \label{eq:case3a_rho1_psi0_geq0}
    \end{align}
    Therefore (\ref{eq:dual_feas1})-(\ref{eq:dual_feas3}) hold in this case. 

    \item $b \in [b_l', b_h']$: Let us first define the following set
    \begin{align}
    &S_b \triangleq  \bigg\{ (a_0,a_1) \in [p_0,\frac{1}{2}]\times[\frac{1}{2},p_1] ~ |\nonumber \\
            &\qquad\quad \Phi_-(a_1) = \Phi_+(a_0)  \mbox{~and~} m(a_0,a_1) = \frac{b}{2} \bigg\}\label{eqn:Sb}
    \end{align}
    It will be shown shortly that the set $S_b$ is a singleton set in this regime. We have the following assignments. 
    \begin{enumerate}
        \item Primal variable assignments: 
        \begin{align}
            &(a_0, a_1) \in S_b \\
            &c_0 = d_0(a_0-p_0), \quad c_1 = d_1(p_1-a_1) 
        \end{align} 
         Using (\ref{eqn:x0})-(\ref{eqn:d1}), all the primal variables are assigned. Note that $x_0$ and $x_1$ will not be zero here as well, thus allowing the derivation of stationary conditions. 
        \item Dual variable assignments: 
        \begin{align}
            & \alpha_0 = 0\\
            & \alpha_1 = \frac{1}{2}\log \left( \frac{1-a_{0}}{a_{0}} \right) -\frac{1}{2}\log \left( \frac{1-a_{1}}{a_{1}} \right) \\
            & \beta_0 = \alpha_1 \\
            & \beta_1 = 0 \\
            & \lambda = \alpha_1
        \end{align}
        With (\ref{eqn:w0})-(\ref{eqn:nu}), all the dual variables are assigned. We can confirm that (\ref{eq:stationary1})-(\ref{eq:stationary6}), (\ref{eq:primal_feas1})-(\ref{eq:primal_feas7}), (\ref{eq:dual_feas4})-(\ref{eq:dual_feas5}), and (\ref{eq:complementary1})-(\ref{eq:complementary7}) are satisfied by substituting these variables. According to the definition of $S_b$, if $(a_0, a_1)$ is an element in $S_b$, then it follows that $a_0 \leq \frac{1}{2} \leq a_1$. Clearly, the conditions (\ref{eq:dual_feas1})-(\ref{eq:dual_feas3}) remain valid since $a_0 \leq \frac{1}{2} \leq a_1$.
    \end{enumerate}
    
    \item $b \in (b_h, \infty)$: In this regime, we have 
    \begin{enumerate}
    \item Primal variables: 
    \begin{align}
        & a_0 = a_1 = \frac{1}{2}\\
        & c_0 = d_0(\frac{1}{2}-p_0), \quad c_1 = d_1(p_1-\frac{1}{2}).
    \end{align}
    Using (\ref{eqn:x0})-(\ref{eqn:d1}), all the primal variables are assigned. Observe that $x_0$ and $x_1$ will not be zero here as well. 
    \item Dual variables: 
    \begin{align}
        & \alpha_0 = \alpha_1 = \beta_0 = \beta_1 =  \rho_0 = \rho_1 = \lambda = 0.
    \end{align}
    All the dual variables are assigned using (\ref{eqn:w0})-(\ref{eqn:nu}). 
    \end{enumerate}
    With the primal and dual variables displayed above, we confirm that (\ref{eq:stationary1})-(\ref{eq:stationary6}), (\ref{eq:primal_feas1})-(\ref{eq:primal_feas7}), (\ref{eq:dual_feas1})-(\ref{eq:dual_feas5}), and (\ref{eq:complementary1})-(\ref{eq:complementary7}) are satisfied by substituting these variables. 
\end{enumerate}

\subsection{Proof That $S_b$ Is a Singleton Set} 
\label{sec:appendix_sbnonempty}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.47\textwidth]{psiproof.pdf}
    \caption{The proof in appendix \ref{sec:appendix_sbnonempty}}
    \label{fig:psi_proof}
\end{figure}
Let us first consider the two functions $\Phi_+(a_0)$ and $\Phi_-(a_1)$, which are shown in Fig.\ref{fig:psi_proof}. We can see that the two functions intersect at the point $a_0 = a_1 = \frac{1}{2}$. Their intersection with the darker green dash line  is $(a_0,a_1) = (\frac{1}{2}, \frac{1}{2})$, which is the only element in $S_{b_h'}$; here the two functions take the same value. Similarly, their intersections with the lighter green dash line give $(a_0,a_1) = (p_0, \psi_1)$, which is the only element in $S_{b_l'}$; again the two functions take the same value. The plot illustrates that any horizontal line positioned between the two green dashed lines intersects with these two functions, and the intersection points represent the unique element in $S_b$ for $b \in [b_l',b_h']$. Given this behavior, it suffices to prove the following two claims. 
\begin{enumerate}
    \item \textit{Claim 1:  $\Phi_+$ and $\Phi_-$ are monotonic functions that increase in opposing directions in $(0, \frac{1}{2}]$ and $[ \frac{1}{2}, 1)$ respectively.} In other words, $\Phi_+$ decreasing in $a \in (0, \frac{1}{2}]$ and $\Phi_-$ increasing in $a \in [\frac{1}{2}, 1)$. We prove this claim by computing the derivatives. 
    \begin{align}
        &\frac{\partial \Phi_{+} }{\partial a}   = \frac{-(1-2\gamma +\gamma p_{0}+\gamma p_{1})}{1-a} -\frac{(1+\gamma p_{0}+\gamma p_{1})}{a}  \nonumber \\ 
        &\quad = \frac{2\gamma a -1-\gamma p_{0}-\gamma p_{1}}{(1-a)a} \leq 0 , \quad\forall a \in (0, \frac{1}{2})\label{eq:phi0_decreasing} \\
        &\frac{\partial \Phi_{-}}{\partial a}   = \frac{-(-1-2\gamma +\gamma p_{0}+\gamma p_{1})}{1-a} \notag\\
        &\qquad-\frac{(-1+\gamma p_{0}+\gamma p_{1})}{a} \nonumber  \\
        &\quad = \frac{2\gamma a +1-\gamma p_{0}-\gamma p_{1}}{(1-a)a} \geq 0 , \quad\forall a \in [\frac{1}{2},1)\label{eq:phi1_increasing}
    \end{align} 
    Moreover, as already mention, $\Phi_+ $ and  $\Phi_- $ intersects at the point $\frac{1}{2}$, i.e., $\Phi_+(\frac{1}{2}) = \Phi_-(\frac{1}{2}) = -2\gamma \log \frac{1}{2}$.
    
    \item \textit{Claim 2: There is only one solution $(a_0,a_1) \in [p_0, \frac{1}{2}] \times [\frac{1}{2}, p_1]$ satisfying $m(a_0,a_1) = \frac{b}{2}$ for $b$ in the range $[b_l', b_h']$ specified in Theorem \ref{thm:p0p1diffside}.} We next prove this claim. 
    In (\ref{eq:p0_psi1_sol2bl}) and (\ref{eq:psi0_p1_sol2bl}) given below, we show that for different cases $|\frac{1}{2} - p_1| \geq |\frac{1}{2} - p_0|$ and $|\frac{1}{2} - p_0| > |\frac{1}{2} - p_1|$, $(\psi_0, p_1)$ and $(p_0, \psi_1)$ are the solutions to $m(a_0,a_1) = \frac{b_l}{2}$:  
    \begin{align}
        & ~\mbox{If} ~ |\frac{1}{2} - p_1| \geq |\frac{1}{2} - p_0|, \nonumber\\
    	& d_0(p_0-p_0) + d_1(p_1-\psi_1) = d_1(p_1 - \psi_1) \nonumber \\
		& = \left( \frac{1-\gamma p_0 -\mathbf{d}^{\gamma}_0}{1-\gamma p_{0}+\gamma \psi_{1}} \right) (p_1 - \psi_1) = \frac{b_{l}' }{2} \label{eq:p0_psi1_sol2bl} \\
        & ~\mbox{If} ~ |\frac{1}{2} - p_0| > |\frac{1}{2} - p_1|, \nonumber \\
		& d_0(\psi_0-p_0) + d_1(p_1-p_1) = d_0(\psi_0 - p_0)  \nonumber \\
		& = \left( \frac{\mathbf{d}^{\gamma}_0+ \gamma p_1}{1-\gamma \psi_{0}+\gamma p_{1}} \right) (\psi_0 - p_0) = \frac{b_{l}' }{2} \label{eq:psi0_p1_sol2bl}
    \end{align} 
    Moreover, from equation (\ref{eq:0.5_is_sol2bh}) below, we see that $(\frac{1}{2}, \frac{1}{2})$ is the solution to $m(a_0,a_1) = \frac{b_h'}{2}$.
	\begin{align}
    	& d_0(\frac{1}{2}-p_0) + d_1(p_1-\frac{1}{2}) = \frac{1}{2} (d_0-d_1) - 		d_0 p_0 + d_1 p_1 \nonumber \\
		& = \frac{1}{2} \left( \frac{\mathbf{d}^{\gamma}_0 +\frac{1}{2} \gamma -1+\frac{1}{2} \gamma +\mathbf{d}^{\gamma}_0}{1-\frac{1}{2} \gamma +\frac{1}{2} \gamma }  \right) \nonumber\\
		& - p_0 \left( \frac{\mathbf{d}^{\gamma}_0+\frac{1}{2} \gamma }{1-\frac{1}{2} \gamma +\frac{1}{2} \gamma } \right)  + p_1 \left( \frac{1 -\frac{1}{2} \gamma -\mathbf{d}^{\gamma}_0}{1-\frac{1}{2} \gamma +\frac{1}{2} \gamma } \right)  \nonumber\\		
		& =\left( \mathbf{d}^{\gamma}_0+\frac{\gamma}{2} \right) (1-p_0-p_1)+p_1 - \frac{1}{2} = \frac{b_{h}' }{2} \label{eq:0.5_is_sol2bh}
	\end{align}
\end{enumerate} 
As a consequence, when $b = b_l'$ and $b = b_h'$, the solutions to the equation are situated within the interval $[p_0,\frac{1}{2}] \times [\frac{1}{2}, p_1]$. Since $\Phi_+(a_0)$ and $\Phi_-(a_1)$ are monotonic, and the mapping $m(a_0,a_1)$ is continuous, we conclude that there is a unique solution $(a_0, a_1)$ to the equations $\Phi_+(a_0) = \Phi_-(a_1)$ and $m(a_0,a_1) = \frac{b}{2}$ for each $b \in [b_l', b_h']$.

\section{Proof of Reduction of Cases} \label{pf:cor1}
In scenarios where $p_0, p_1 \leq \frac{1}{2}$ and $p_0 \geq \frac{1}{2} \geq p_1$, we interchange state $0 $ to $1^{\prime}$ and state $1 $ to $0^{\prime}$ as the order of states is inconsequential. We first note that in our setting, $p_0 = p_{0\rightarrow 0}$ and $p_1 = p_{1\rightarrow 0}$, meaning that $p_0$ is the probability from state 0 to 0 and $p_1$ is the probability from state 1 to 0. After renaming the state from $0, 1$ to $1^{\prime}, 0^{\prime}$, the probability $p_0, p_1$ now becomes 
\begin{align}
    & p_0 = p_{0 \rightarrow 0} \overset{\tiny{\text{rename}}}{=} p_{1^{\prime} \rightarrow 1^{\prime}} \triangleq 1-p^{\prime}_1 \label{eq:state_rename0}\\
    & p_1 = p_{1 \rightarrow 0} \overset{\tiny{\text{rename}}}{=} p_{0^{\prime} \rightarrow 1^{\prime}} \triangleq 1-p^{\prime}_0 \label{eq:state_rename1}
\end{align}
Therefore, for case $p_0, p_1 \leq \frac{1}{2}$, the renamed optimization problem will have $p^{\prime}_0,p^{\prime}_1 \geq \frac{1}{2}$. Use the results in Theorem \ref{thm:p0p1sameside} to solve for optimal $a^{\prime}_0, a^{\prime}_1$, then convert back to $a_0, a_1$.
\begin{align}
    a^{\prime}_0 = a_{0^{\prime} \rightarrow 0^{\prime}} \overset{\tiny{\text{recover}}}{=} a_{1 \rightarrow 1} = 1 - a_1 & \Rightarrow a_1 = 1 - a^{\prime}_0 \\
    a^{\prime}_1 = a_{1^{\prime} \rightarrow 0^{\prime}} \overset{\tiny{\text{recover}}}{=} a_{0 \rightarrow 1} = 1 - a_0 & \Rightarrow a_0 = 1 - a^{\prime}_1
\end{align}
Similar steps for case $p_0 \geq \frac{1}{2} \geq p_1$. \\


\end{document}

