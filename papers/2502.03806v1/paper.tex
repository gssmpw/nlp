\documentclass[10pt,conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
% \renewcommand\citepunct{, } %FIXES THIS [1],[2] -> [1,2]
\usepackage{scalerel}
\usepackage{multirow} 
\usepackage{booktabs}
\usepackage[table]{xcolor}
\pagestyle{plain}
% \usepackage{cite}
\usepackage[sort&compress, numbers]{natbib}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{float}\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{flushend}
\usepackage{textcomp}
\usepackage{hyperref}
\usepackage{comment}
\usepackage{siunitx}
\usepackage[framemethod=TikZ]{mdframed}

\definecolor{seabornGreen}{HTML}{66c2a5}
\definecolor{seabornOrange}{HTML}{fc8d62}
\definecolor{seabornBlueGray}{HTML}{8da0b1}
\definecolor{seabornPink}{HTML}{e78ac3}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

% \title{Should Code Models Learn Pedagogically? Exploring Curriculum Learning for Automated Software Engineering Tasks
% }

\title{Should Code Models Learn Pedagogically? \\A Preliminary Evaluation of Curriculum Learning for Real-World Software Engineering Tasks}

 \author{\IEEEauthorblockN{Kyi Shin Khant}
 \IEEEauthorblockA{
 \textit{The University of Melbourne}\\
 kyishink@student.unimelb.edu.au}
  \and
 \IEEEauthorblockN{Hong Yi Lin}
 \IEEEauthorblockA{
 \textit{The University of Melbourne}\\
 holin2@student.unimelb.edu.au}
 \and
 \IEEEauthorblockN{Patanamon Thongtanunam}
 \IEEEauthorblockA{
 \textit{The University of Melbourne}\\
 patanamon.t@unimelb.edu.au}
 }

\maketitle

\begin{abstract}
Learning-based techniques, especially advanced
pre-trained models for code have demonstrated capabilities in code understanding and generation,  solving diverse software engineering (SE) tasks.
Despite the promising results, current training approaches may not fully optimize model performance, as they typically involve learning from randomly shuffled training data.
Recent work shows that Curriculum Learning (CL) can improve performance on code-related tasks through incremental learning based on the difficulty of synthetic code.
Yet, the effectiveness of CL with conventional difficulty measures in SE tasks remains largely unexplored.
In this study, we explore two conventional code metrics: code length and cyclomatic complexity to determine the difficulty levels.
We investigate how the pre-trained code model (CodeT5) learns under CL, through the tasks of code clone detection and code summarization.
Our empirical study on the CodeXGLUE benchmark showed contrasting results to prior studies, where the model exhibited signs of catastrophic forgetting and shortcut learning.
Surprisingly, model performance saturates after only the first quartile of training, potentially indicating a limit in the  model's representation capacity and/or the task's inherent difficulty.
Future work should further explore various CL strategies with different code models across a wider range of SE tasks for a more holistic understanding.

\end{abstract}

\begin{IEEEkeywords}
Code Understanding, Curriculum Learning
\end{IEEEkeywords}

\section{Introduction}
Learning-based techniques, especially advanced pre-trained models for code have shown potential in code understanding and generation.
These capabilities have proven to be useful for a diverse range of software engineering (SE) tasks, such as vulnerability detection~\cite{linevul}, bug-fixing~\cite{bugfixwild}, and code translation~\cite{codexglue}.
Whilst these results are promising, code models often still struggle with longer~\cite{codestruct} and more complex programs~\cite{austin2021program}.


In recent years, a plethora of research has investigated code-oriented pre-training~\cite{zengslr} as the primary method for improving code models, however, the effects of employing different fine-tuning strategies for downstream SE tasks still lacks attention. 
By comparison, \textit{curriculum learning} (CL), a difficulty-based training strategy inspired by pedagogical learning principles~\cite{clbengio} has demonstrated promising results in domains such as natural language processing and computer vision~\cite{curricsurvey}.
For code understanding, CL has shown potential in synthetic programming challenges involving code completion~\cite{curriccode} and code execution~\cite{liu-etal-2023-code}.
For real-world SE, CL has been effective in tasks such as code clone detection~\cite{wangcl} and automated program repair~\cite{apr_cl}.
However, these studies rely on semantic preserving transformations to create difficult code, rather than natural code difficulty. 
They also do not examine how the model is gaining competency at each stage of CL.



In this study, we set out to investigate whether CL with conventional difficulty measures can enhance the understanding and generative capabilities of pre-trained code models with real-world software.
We also investigate the stages of this learning process, and how each difficulty affects the model.
We experimented with two well-established SE tasks, i.e., code clone detection and code summarization~\cite{zengslr}.
We structured the CL training schedules by organizing the fine-tuning data into four difficulty levels (easy, medium, hard, and very hard) based on conventional difficulty metrics i.e., length ($L$) and cyclomatic complexity ($C$).
We explore both sequential ($s$) and reverse sequential ($r$) CL schedules, i.e., fine-tuning the model from easy to hard, and vice versa.
Through an empirical study of CodeT5~\cite{codet5}, using the CodeXGLUE benchmark~\cite{codexglue}, we address the following two research questions:

\begin{enumerate}
    \item[\textbf{RQ1.}] \textbf{Can curriculum learning with conventional difficulty metrics enhance the performance of code models in code clone detection and code summarization}?\\
\underline{Findings:} In our experiments with CodeT5, CL with conventional difficulty measures did not outperform the traditional random schedule in the two selected tasks.
    \item[\textbf{RQ2.}] \textbf{How does each difficulty level impact model performance during CL?}\\
    \underline{Findings:} Interestingly, model performances reach saturation with only 25\% of the training data. In other words, after initially fine-tuning on either the easy or very hard subset, additional data from the subsequent difficulty levels yield only minimal performance gains.
\end{enumerate}


Our RQ1 findings suggest that, despite promising results of CL with difficult synthetic code, scheduled training based on conventional difficulty of code had little impact on CodeT5's capabilities in the selected tasks.
Instead, RQ2 reveals that CodeT5 can sufficiently learn common patterns for these tasks from limited data, and additional training on other difficulty levels only marginally enhances these capabilities.
Nonetheless, this may be due to the model's limited representation capacity and/or the natural difficulty of the task.
For a more comprehensive evaluation, future studies should include more diverse code models, datasets and difficulty measures.

\textbf{Contribution.}
To the best of our knowledge, we are the first to investigate the learning process of code models during CL with conventional difficulty metrics.
Our study unveils an interesting phenomenon,  providing motivation for further research into how code models learn downstream SE tasks.

\section{Background and Related Work}

\subsection{Neural Code Models}
The success of neural language models in the natural language domain has inspired researchers to apply similar methods to programming languages. 
To alleviate the software engineering workload, a large plethora of research has explored automating SE tasks, such as vulnerability detection~\cite{linevul}, bug-fixing~\cite{bugfixwild}, code translation~\cite{codexglue}, code clone detection~\cite{mou2016} and code summarization~\cite{iyer2016summarizing}. 
Specifically, the techniques involve training neural language models on data from public repositories such as GitHub and Stack Overflow~\cite{codet5}.
Leveraging the naturalness properties of human written code, these neural code models have learned useful patterns that unlock their potential as practical software development assistants~\cite{assetorliability}.

\subsection{Curriculum Learning}

Replicating the pedagogical learning process of humans, CL is a training strategy that gradually increases the difficulty of examples presented to the model~\cite{clbengio}, which contrasts with the conventional unordered training schedule.
CL has been effective for a wide range of model architectures and tasks~\cite{curricnn,curricsurvey}. 
It improves model performance on complex examples by aligning training schedules with the model's skill-acquiring pace~\cite{curricnmt}.
Using generated programs, Na√Ør et al.~\cite{curriccode} investigated the effects of CL in code completion, whilst Liu et al.~\cite{liu-etal-2023-code} focused on the task of code execution in competitive programming.
Although conventional difficulty measures such as length and cyclomatic complexity were investigated, these studies focused on synthetic programming problems, which may not generalise to real-world software.
In terms of real-world SE, CL has been employed in tasks, such as code clone detection, code search~\cite{wangcl} and automated program repair~\cite{apr_cl}, however, the methods largely focused on augmenting code to create difficult synthetic examples.
Additionally, past studies mainly focused on the final result, rather than investigating how the code model improves during CL, leaving us with an opaque understanding of the process.
Past studies have also yet to investigate the code-to-text task of code summarization.



\section{Study Design}

\subsection{Overview}
To investigate whether CL can improve models' code understanding, we explore two learning schedules: sequential ($s$) and reverse sequential ($r$). 
We organize the training data into four subsets based on difficulty levels and incrementally fine-tune the code models by each subset.
Finally, we compare the performance of models trained incrementally by difficulty level to those trained with the conventional approach i.e., unordered schedule.
We experiment with two well-established SE tasks: code clone detection and code summarization. 
Below, we describe the data preparation, CL schedules, and experimental setup.


\subsection{Data Preparation}
To evaluate CL for real-world SE tasks, we use the CodeXGLUE benchmark~\cite{codexglue}, which includes code collected from GitHub repositories. It has been used as a comprehensive benchmark for evaluating model capabilities across a wide range of real-world SE tasks~\cite{zengslr}.

\textbf{Difficulty Measures:} 
To organize the data into difficulty subsets, we explore two code metrics that determine the difficulty of the code: length ($L$) and cyclomatic complexity ($C$)~\cite{complexitymeasure}.
We measure $L$ by counting tokens separated by spaces. 
This approach aligns with human intuition, as longer texts are often perceived as more challenging to read, understand, and maintain due to a greater number of variables and logical operations~\cite{codenaturalness}.
Prior work has also shown that language models tend to underperform with longer inputs~\cite{long_context}.
We measure $C$, using Lizard, a cyclomatic complexity analyzer,\footnote{\url{https://github.com/terryyin/lizard}} which counts linearly independent paths determined by decision points e.g., if, else, while, for. 
More decision points indicate higher complexity, requiring the model to process multiple execution paths \cite{cyccritiq, metricthresholds}.
Prior work in code clone detection has also shown that language models tend to struggle with examples that are more complex~\cite{llm_ccd}.
For code clone detection, the difficulty score is summed across the program pair.
For code summarization, the difficulty score is determined by the single target program.


\textbf{Difficulty Subsets.} 
We divide the dataset into four difficulty levels based on naturally distributed quartiles. 
The first quartile represents the easiest examples, whilst the fourth quartile contains the most challenging.
Table~\ref{table:subset_data} shows the number of instances in each difficulty level. 
Since subsets vary in size which may influence the model performance, we undersample the larger subsets to match the smallest subset to control the training size of each subset. 


\input{tables/subsets}

\subsection{Curriculum Learning Schedules} \label{sec:CL}
For the sake of completeness, we explore two CL schedules i.e., sequential ($s$) and reverse sequential ($r$).
For sequential schedule $s$, we incrementally fine-tune the code model with each subset starting from easy to very hard, resembling the natural pedagogical process.
After fine-tuning each subset, we select the best model checkpoint based on the validation set to be further trained with the next difficulty subset.
For reverse schedule $r$, we incrementally fine-tune from very hard to easy, which explores the effects of learning in an unintuitive manner that does not resemble the natural pedagogical process.
After training on each difficulty subset, we assess the model on the entire test set to capture the model's learning process.

\subsection{Experiment Setup}
To better quantify the impact of CL on initial learning of the selected software engineering tasks, our experiment required a relatively primitive model that has not been subject to vast amounts of training on software engineering datasets.
To this end, we select the 220M parameter CodeT5-base model~\cite{codet5}.
The model was fine-tuned with the original hyper-parameters i.e. learning rate of 5e-5, beam search width of 10, and 1 epoch for code clone detection and 15 epochs for code summarization.
To speed up training, we increased the batch size from 10 to 16 for code clone detection, while maintaining a batch size of 48 for code summarization. 
We used one 32-core server with an NVIDIA A100-80GB GPU for training.

\subsection{Evaluation Metrics}
For code clone detection, we evaluate model performance with F1 score, precision, and recall. 
For code summarization, we evaluate model performance with both the BLEU~\cite{bleu} score and the embedding-based BERTScore~\cite{bertscore}.
Unlike BLEU, BERTScore accounts for model outputs that are textually divergent, yet semantically similar to the ground truth.


\begin{table*}[htbp]
\caption{Stratified Code Clone Detection Results (Length)}
\label{ref:codeclonettable1}
\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
& \multicolumn{3}{c|}{\textbf{Test - Easy}} & \multicolumn{3}{c|}{\textbf{Test - Medium}} & \multicolumn{3}{c|}{\textbf{Test - Hard}} & \multicolumn{3}{c|}{\textbf{Test - Very Hard}} & \multicolumn{3}{c|}{\textbf{Test - Total}} \\
\cline{2-16}
 & \textbf{F1} & \textbf{Prec} & \textbf{Rec} & \textbf{F1} & \textbf{Prec} & \textbf{Rec} & \textbf{F1} & \textbf{Prec} & \textbf{Rec} & \textbf{F1} & \textbf{Prec} & \textbf{Rec} & \textbf{F1} & \textbf{Prec} & \textbf{Rec} \\
\hline
E & 98.43 & 98.32 & 98.53 & 95.59 & 93.62 & 97.65 & 91.26 & 88.21 & 94.52 & 81.34 & 88.81 & 75.03 & 91.82  & 92.53 & 91.13 \\

E+M & 98.34  & 98.16 & 98.53 & 95.74 & 93.82 & 97.73 & 89.25 & 84.72 & 94.29 & \underline{77.42} & 79.54 & 75.40 & 90.23 & 89.28 & 91.20 \\

E+M+H &  98.04 & 98.07 & \underline{98.07} & 95.70 & 94.57 & 96.86 & 90.49 & 90.30 & 90.68 & 81.99 & 86.04 & 78.31 & 91.65 & 92.45 & \underline{90.87} \\

E+M+H+VH & 98.19 & \underline{97.32} & 99.08 & 95.61 & 94.05 & 97.22 & 92.67 & 89.96 & 95.54 & 88.68 & 88.67 & 88.70 & 93.82 & 92.60 & \textbf{95.06} \\

\hline
VH & 98.06 & 97.37 & 98.79 & 96.46 & 95.16 & 97.81 & 93.89 & 91.76 & \textbf{96.13} & 89.11 & 90.29 & 87.97 & 94.39 & 93.73 & \textbf{95.06} \\

VH+H & \underline{98.02} & 98.07 & \underline{98.07} & 95.70 & 94.57 & 96.86 & 90.49 & 90.30 & 90.68 & 81.99 & 86.04 & 78.31 & 91.65 & 92.45 & \underline{90.87} \\

VH+H+M & 98.84 & 98.46 & \textbf{99.23} & 96.23 & 94.22 & \textbf{98.32} & 91.96 & 88.67 & 95.50 & 79.43 & 84.67 & \underline{74.79} & 91.72 & 91.81 & 91.63 \\

VH+H+M+E & 98.05 & 97.37 & 98.74 & \underline{93.85} & \underline{90.65} & 97.28 & \underline{87.23} & \underline{80.11} & 95.75 & 80.31 & \underline{77.75} & 83.04 & \underline{89.82} & \underline{86.42} & 93.51 \\
\hline

Conventional & \textbf{98.70} & \textbf{99.48} & 97.92 & \textbf{96.47} & \textbf{97.77} & \underline{95.21} & \textbf{95.11} & \textbf{96.12} & \underline{94.12} & \textbf{90.75} & \textbf{91.77} & \textbf{89.75} & \textbf{95.24} & \textbf{96.26} & 94.23 \\
\hline
\multicolumn{13}{l}{\footnotesize (E)Easy, (M)Medium, (H)Hard, (VH)Very Hard, \textbf{Best Score}, \underline{Worst Score}, \textbf{Prec}ision, \textbf{Rec}all}
\end{tabular}%
}
\label{t8}
\end{center}
\end{table*}

\begin{table*}[htbp]
\caption{Stratified Code Clone Detection Results (Cyclomatic Complexity)}
\label{ref:codeclonettable2}
\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
& \multicolumn{3}{c|}{\textbf{Test - Easy}} & \multicolumn{3}{c|}{\textbf{Test - Medium}} & \multicolumn{3}{c|}{\textbf{Test - Hard}} & \multicolumn{3}{c|}{\textbf{Test - Very Hard}} & \multicolumn{3}{c|}{\textbf{Test - Total}} \\
\cline{2-16}
 & \textbf{F1} & \textbf{Prec} & \textbf{Rec} & \textbf{F1} & \textbf{Prec} & \textbf{Rec} & \textbf{F1} & \textbf{Prec} & \textbf{Rec} & \textbf{F1} & \textbf{Prec} & \textbf{Rec} & \textbf{F1} & \textbf{Prec} & \textbf{Rec} \\
\hline
E & 98.61 & 98.47 & 98.75 & 95.66 & 95.01 & 96.32 & 91.66 & 89.76 & 93.65 & 87.88 & 89.09 & 86.70 & 93.40 & 93.11 & 93.70 \\

E+M & \underline{96.16}  & \underline{94.01} & 98.42 & 95.71 & 93.33 & \textbf{98.21} & 91.53 & 87.33 & \textbf{96.15} & 87.19 & 87.51 & 86.86 & 93.15 & 91.39 & 94.97 \\

E+M+H &  98.42 & 98.08 & 98.77 & 96.03 & 96.44 & \underline{95.61} & 91.35 & 91.54 & \underline{91.16} & 88.56 & 88.15 & 88.97 & 93.46 & 93.43 & 93.49 \\

E+M+H+VH & 97.29 & 95.71 & 98.93 & 94.41 & \underline{91.57} & 97.44 & 90.76 & 86.32 & 95.68 & 87.72 & 85.17 & 90.42 & 92.55 & 89.75 & \textbf{95.53} \\

\hline
VH & \textbf{98.63} & 98.44 & 98.81 & 95.70 & 94.85 & 96.56 & 92.21 & 90.34 & 94.15 & \textbf{90.48} & 89.43 & \textbf{91.56} & 94.31 & 93.34 & 95.29 \\

VH+H & 98.42 & 98.08 & 98.77 & 96.03 & 96.44 & \underline{95.61} & 91.35 & 91.54 & \underline{91.16} & 88.56 & 88.15 & 88.97 & 93.62 & 93.54 & 93.70 \\

VH+H+M & 98.52 & 97.96 & \textbf{99.09} & 95.69 & 93.57 & 97.91 & 92.05 & 88.68 & 95.69 & 86.87 & 87.15 & 86.60 & 93.27 & 91.96 & 94.62 \\

VH+H+M+E & 97.30 & 96.08 & 98.60 & \underline{94.33} & 92.54 & 96.18 & \underline{87.81} & \underline{83.58} & 92.51 & \underline{81.71} & \underline{78.88} & \underline{84.75} & \underline{90.19} & \underline{87.65} & \underline{92.88} \\
\hline

Conventional & 98.58 & \textbf{99.20} & \underline{97.96} & \textbf{96.52} & \textbf{97.16} & 95.88 & \textbf{93.12} & \textbf{93.47} & 92.76 & 89.69 & \textbf{89.83} & 89.55 & \textbf{94.46} & \textbf{94.89} & 94.04 \\
\hline
\multicolumn{13}{l}{\footnotesize (E)Easy, (M)Medium, (H)Hard, (VH)Very Hard, \textbf{Best Score}, \underline{Worst Score}, \textbf{Prec}ision, \textbf{Rec}all}
\end{tabular}%
}
\label{t8}
\end{center}
\end{table*}


\begin{table*}[htbp]
\caption{Stratified Code Summarization Results (Length)}
\label{ref:codesummarisationttable1}
\begin{center}
\resizebox{\textwidth}{!}{
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|}
\hline
& \multicolumn{2}{c|}{\textbf{Test - Easy}} & \multicolumn{2}{c|}{\textbf{Test - Medium}} & \multicolumn{2}{c|}{\textbf{Test - Hard}} & \multicolumn{2}{c|}{\textbf{Test - Very Hard}} & \multicolumn{2}{c|}{\textbf{Test - Total}}\\

\cline{2-11} 
 & \textbf{BLEU} & $\textbf{BS}_{F1}$ & \textbf{BLEU} & $\textbf{BS}_{F1}$ & \textbf{BLEU} & $\textbf{BS}_{F1}$  & \textbf{BLEU} & $\textbf{BS}_{F1}$ & \textbf{BLEU} & $\textbf{BS}_{F1}$ \\
 
\hline
E & 24.34 & 91.39 & 21.64 & 91.50 & \underline{18.44} & \underline{91.16} & \underline{16.50} & 90.64  & 20.23 & 91.17 \\

E+M & \textbf{24.77} & 91.53 & 22.46 & 91.65 & 19.14 & 91.30 & 17.23 & 90.77  & 20.90 & 91.31 \\

E+M+H & 24.39  & 91.50 & 22.07 & 91.67 & 18.97 & 91.32 & 17.00 & 90.76 & 20.61 & 91.31 \\

E+M+H+VH & 23.98 & 91.45 & 21.5 & 91.59 & 19.06 & 91.26 & 17.19 & 90.76  & 20.43 & 91.27 \\
\hline

VH & \underline{21.12} & \underline{91.35} & 19.85 & \underline{91.42} & 18.72 & 91.24 & 17.11 & 90.50  & \underline{19.20} & \underline{91.13} \\

VH+H & 22.53 & 91.45 & \underline{19.66}  & 91.43 & 18.85 & 91.28 & 17.07 & \underline{90.47}  & 19.53 & 91.15 \\

VH+H+M & 24.51 & \textbf{91.81} & 22.23 & 91.66 & 19.05 & 91.32 & 17.38 & 90.49  & 20.79 & 91.32 \\

VH+H+M+E & 24.47 & \textbf{91.81} & 22.12 & 91.64 & \textbf{19.26}  & 91.28  & \textbf{17.71}  & 90.48  & \textbf{20.89} & 91.30 \\
\hline

Conventional & 24.36 & 91.53 & \textbf{22.54} & \textbf{91.71} & 18.99 & \textbf{91.34} & 17.22 & \textbf{90.80}  & 20.77 & \textbf{91.35} \\
\hline
\multicolumn{11}{l}{\footnotesize (E)Easy, (M)Medium, (H)Hard, (VH)Very Hard, \textbf{Best Score}, \underline{Worst Score}, $\textbf{B}\text{ERT}\textbf{S}\text{core}_{F1}$: RoBERTa-Large Layer 17}
\end{tabular}
}
\label{t12}
\end{center}
\end{table*}


\begin{table*}[htbp]
\caption{Stratified Code Summarization Results (Cyclomatic Complexity)}
\label{ref:codesummarisationttable2}
\begin{center}
\resizebox{\textwidth}{!}{
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|}
\hline
& \multicolumn{2}{c|}{\textbf{Test - Easy}} & \multicolumn{2}{c|}{\textbf{Test - Medium}} & \multicolumn{2}{c|}{\textbf{Test - Hard}} & \multicolumn{2}{c|}{\textbf{Test - Very Hard}} & \multicolumn{2}{c|}{\textbf{Test - Total}}\\

\cline{2-11} 
 & \textbf{BLEU} & $\textbf{BS}_{F1}$ & \textbf{BLEU} & $\textbf{BS}_{F1}$ & \textbf{BLEU} & $\textbf{BS}_{F1}$  & \textbf{BLEU} & $\textbf{BS}_{F1}$ & \textbf{BLEU} & $\textbf{BS}_{F1}$ \\
 
\hline
E & 23.22 & \underline{91.46} & 21.45 & 91.48 & \underline{18.19} & 90.91 & \underline{16.72} & 90.62  & 19.89 &91.12 \\

E+M & 23.38 & 91.62 & 21.68 & 91.60 & 18.54 & 91.03 & 17.04 & 90.76  & 20.16 & 91.25 \\

E+M+H & \textbf{23.64}  & 91.65 & \textbf{22.13} & \textbf{91.64} & 18.81 & 91.12 & 17.44 & \textbf{90.83} & \textbf{20.51} & 91.31 \\

E+M+H+VH & 23.18 & 91.57 & 21.81 & 91.53 & \textbf{18.83} & 91.08 & 17.29 & 90.78  & 20.28 & 91.24 \\
\hline

VH & \underline{21.45} & 91.48 & \underline{19.63} & \underline{91.34} & 18.21  & \underline{90.89} & 16.89 & \underline{90.47}  & \underline{19.05} & \underline{91.04} \\

VH+H & 21.85 & 91.59 & 19.93  & 91.44 & \textbf{18.83} & 91.11 & 17.33 & 90.59  & 19.49 & 91.18 \\

VH+H+M & 22.06 & 91.66 & 21.63 & 91.58 & 18.67 & 91.07 & 17.32 & 90.53  & 19.92 & 91.21 \\

VH+H+M+E & 22.96 & \textbf{91.75} & 21.80 & 91.58 & 18.73  & 91.07  & 17.49 & 90.53  & 20.24 & 91.23 \\
\hline

Conventional & 23.05 & 91.71 & 22.02 & 91.62 & 18.79 & \textbf{91.13} & \textbf{17.56} & 90.82  & 20.44 & \textbf{91.32} \\
\hline
\multicolumn{11}{l}{\footnotesize (E)Easy, (M)Medium, (H)Hard, (VH)Very Hard, \textbf{Best Score}, \underline{Worst Score}, $\textbf{B}\text{ERT}\textbf{S}\text{core}_{F1}$: RoBERTa-Large Layer 17}
\end{tabular}
}
\label{t12}
\end{center}
\end{table*}

\section{Results}
\label{sec:results}
\subsection{Model Performance with Curriculum Learning (RQ1)}

To answer RQ1, we compare models trained with CL to those trained conventionally based on test set performance.
We evaluate four CL strategies based on a combination of the two code metrics, i.e., length ($L$) and cyclomatic complexity ($C$); and the two CL schedules, i.e., sequential ($s$) and reverse ($r$).
For the conventional method, we mix the four subsets into one unordered training set
to fine-tune the model~\cite{codet5}. 
We report two results for the conventional method, the training size differs when it was undersampled to match $L$ or $C$ subsets.


% .
% The aggregate test set results are shown in .

\textbf{Code Clone Detection.} Evaluating based on the whole test set, the conventional method yields the F1 scores, i.e., 95.24 ($L$) and 94.46 ($C$).
In contrast, CL strategies achieve lower  F1-score, i.e., 93.82 ($L_s$), 89.82 ($L_r$), 92.55 ($C_s$), and 90.19 ($C_r$).
Tables~\ref{ref:codeclonettable1} and~\ref{ref:codeclonettable2} show the results across the difficulty subsets.
For easy and medium test subsets, all methods perform similarly as the difference in F1 scores lies within $\Delta$1.29 ($C - C_{s}$) and $\Delta$2.62 ($L - L_{r}$), respectively.
For hard and very hard examples, the divergence in performance is more pronounced.
We find that $s$ schedules only exhibit minor gaps of at most $\Delta$2.44 (Hard: $L - L_{s}$) in F1 scores to the conventional method, whilst, $r$ schedules exhibit major gaps of up to $\Delta$10.44 (Very Hard: $L - L_{r}$) in F1 scores.
These findings resemble symptoms of catastrophic forgetting, which is strongly correlated with task complexity~\cite{clcontinual}.
The difference in behavior between $s$ and $r$ schedules suggests that harder examples presented at the start of the training schedule may be easily forgotten by the end.
Conversely, the model's understanding of easy examples scheduled at the beginning is still largely intact by the end.

\textbf{Code Summarization.} The conventional method yields the BLEU scores of 20.77 ($L$) and 20.44 ($C$).
In contrast, CL strategies achieve mixed results, but marginally different, i.e., 20.43 ($L_s$), 20.89 ($L_r$), 20.28 ($C_s$), and 20.24 ($C_r$).
Tables~\ref{ref:codesummarisationttable1} and~\ref{ref:codesummarisationttable2} show the results across the difficulty subsets.
We find that all strategies have negligible differences in performance across the difficulties.
The gap in BLEU scores is at most $\Delta$0.38 ($L - L_{s}$) for easy, $\Delta$1.04 ($L - L_{s}$) for medium, $\Delta$0.27 for hard ($L_{r} - L$), and $\Delta$0.49 ($L_{r} - L$) for very hard. 
Similar trends are also reflected in the $\text{BERTScore}_{F1}$ results.
These findings suggest that model learning is invariant to the difficulty schedule.
% One possible explanation is that competence in code summarization is not as heavily dependent on the understanding of a program's functionality as code clone detection.
Compared to the prior work~\cite{curriccode}, the synthetic programs in their experiment were deliberately constructed without descriptive names, e.g., using variable names \texttt{a}, \texttt{b}, \texttt{c}. 
Unlike the prior work, our experiment uses real-world programs that have meaningful names for variables, functions, and other semantic cues designed by human developers.
It is possible that models may depend more on these cues rather than program functionality.
Such a behavior is canonically known as shortcut learning~\cite{shortcutlearning}.


In summary, we find that CL schedules with conventional difficulty measures are not conducive to improving CodeT5's performance in both tasks. 
For code clone detection, the models seem to exhibit catastrophic forgetting, which significantly affects the model's competence on harder programs.
For code summarization, the models show invariance to the different schedules, which is possibly due to relaxation on the task's demand for understanding program semantics.

\begin{figure}[t]
\centerline{\includegraphics[width=\columnwidth]{Figures/Code_Clone_Detection.png}}
\caption{Code Clone Detection (\textcolor{seabornGreen}{$L_{s}$}, \textcolor{seabornOrange}{$L_{r}$}, \textcolor{seabornBlueGray}{$C_{s}$}, \textcolor{seabornPink}{$C_{r}$})}
\label{fig:codeclone}
\end{figure}


\begin{figure}[t]
\centerline{\includegraphics[width=\columnwidth]{Figures/Code_Summarisation.png}}
\caption{Code Summarization (\textcolor{seabornGreen}{$L_{s}$}, \textcolor{seabornOrange}{$L_{r}$}, \textcolor{seabornBlueGray}{$C_{s}$}, \textcolor{seabornPink}{$C_{r}$})}
\label{fig:codesummarization}
\end{figure}

\subsection{Impact of Each Difficulty Level (RQ2)}
To answer RQ2, we compare the model performance on the entire test set after training on each difficulty subset.
Figures~\ref{fig:codeclone} and~\ref{fig:codesummarization} show the results of the four CL strategies (i.e., $L_{s}$, $L_{r}$, $C_{s}$, $C_{r}$) for code clone detection and code summarization. 


\textbf{Code Clone Detection.}
Figure~\ref{fig:codeclone}  shows that high accuracy is already achieved by the model after training on the first quartile for all four strategies. 
Table~\ref{ref:codeclonettable1} shows that for the $L_{s}$ schedule, the model trained only on the easy subset could already achieve a comparable F1 score to the model that has completed the full conventional training, where the difference in F1 scores is $\Delta$0.27 for the easy subset, $\Delta$0.88 for the medium subset and $\Delta$3.85 for the hard subset.
This early model state only struggled against the fully trained model for the very hard subset, with a $\Delta$9.41 drop in F1 score.
Table~\ref{ref:codeclonettable2} shows that for the $C_{s}$ schedule, the model trained on the easy subset was consistently comparable to the fully trained model for all subsets, where the gap is at most $\Delta$1.81 (Very Hard) F1 score.
This demonstrates that learning from easy examples in terms of $C$, already equipped the model with sufficient knowledge to generalise to very hard examples.
For $L_{r}$, we find that the gap across the difficulties between the model trained only on the very hard subset and the fully trained model is at most $\Delta$1.64 (Very Hard) F1 score.
For $C_{r}$, we find that this gap is at most $\Delta$0.91 (Hard) F1 score.
This demonstrates that  learning from very hard examples in terms of either $L$ or $C$, is also sufficient to generalise to the rest of the difficulties.

\textbf{Code Summarization.}
Figure~\ref{fig:codesummarization} shows that the model achieves high accuracy after training on only the first quartile for all four strategies. 
Table~\ref{ref:codesummarisationttable1} shows that for the $L_{s}$ schedule, the gap across the difficulties between the model trained on only the easy subset and the fully trained model is at most $\Delta$0.72 BLEU (Very Hard).
Table~\ref{ref:codesummarisationttable2} shows that for the $C_{s}$ schedule, this gap is at most $\Delta$0.84 BLEU (Very Hard).
These trends also translate directly to the $r$ schedules.
For the $L_{r}$ schedule, the gap across the difficulties between the model trained on only the very hard subset and the fully trained model is at most $\Delta$3.24 BLEU (Easy).
For the $C_{r}$ schedule, this gap is at most $\Delta$2.39 BLEU (Medium).
Similar to code clone detection, we find that both the easy and very hard subsets were sufficient for the model to learn useful knowledge that could be generalized to the rest of the difficulties.

Interestingly, all CL schedules show saturation in model performance after learning from the first quartile, indicating that minimal gains are extracted from the rest of the training process.
In other words, the model only needs to learn from the easy or very hard subsets to sufficiently generalize to the rest of the test set for both tasks.
These results may suggest a limitation of the model's representation capacity or a ceiling in the task's inherent difficulty.


\section{Threats to Validity} 
Our study focuses on two SE tasks which 
do not generalize to the full spectrum of SE tasks.
Our results are based on the CodeT5, which may not generalize to other models.
For the code summarization task, we focused exclusively on Java, whereas CodeXGLUE offers sub-tasks for several other programming languages. By restricting our analysis to Java, we may have missed important insights that could emerge from cross-language comparisons.
We focus on only two difficulty measures, which do not represent all the potential aspects of difficulty that challenges the model.
We only explored two predefined CL schedules, which may exhibit different behaviour to automatic self-paced schedules~\cite{curricsurvey}.
The cannonical metrics used to measure code summarisation are not perfect reflections of summary quality, thus, there may be discrepancy between evaluation results and actual task competency.

\section{Conclusion \& Future Work} 
In this work, we explore the process of curriculum learning with conventional difficulty measures - a difficulty-based training strategy inspired by pedagogical learning principles - for a neural code model (CodeT5) in solving real-world SE tasks.
Whilst past studies achieved promising results based on code that was synthetically constructed to be difficult, our work shows contrastive findings for CL with conventional difficulty measures in clone detection and code summarization.
Interestingly, our results show symptoms of catastrophic forgetting and shortcut learning, as well as limitions in the the model's representation capacity and/or the task's inherent difficulty.

Whilst this study investigated two difficulty measures in length and cyclomatic complexity, there are a myriad of potential difficulty measures that have yet to be explored e.g., clone type, functionality.
Additional experiments across a more comprehensive set of difficulty measures, with large scale reruns, could reveal more insights.
Future research should also explore other CL strategies, with diverse neural code models for a broader range of SE tasks with multiple programming languages.
This would help determine the effectiveness of CL across a wider variety of scenarios, providing a more comprehensive understanding of its generalization capabilities.

\textbf{Data Availability.}
All the materials produced from this study are available on Zenodo\footnote{\url{https://zenodo.org/records/14059483}}.



\bibliographystyle{IEEEtran}
\bibliography{references}
\end{document}