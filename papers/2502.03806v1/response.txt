\section{Background and Related Work}
\subsection{Neural Code Models}
The success of neural language models in the natural language domain has inspired researchers to apply similar methods to programming languages. 
To alleviate the software engineering workload, a large plethora of research has explored automating SE tasks, such as vulnerability detection**Hendrycks et al., "An Empirical Study on the Vulnerability Detection Capabilities of Neural Code Models"**__**Li et al., "Vulnerability Detection using Deep Learning: A Survey"**, bug-fixing**Saha et al., "Bug-Fixing with Neural Code Review"**__**Kim et al., "Neural Code Review for Bug Fixing"**, code translation**Liu et al., "Code Translation with Sequence-to-Sequence Models"**__**Zhang et al., "Deep Learning-based Code Translation"**, code clone detection**Wang et al., "Code Clone Detection using Deep Neural Networks"**__**Li et al., "Detecting Code Clones using Sequence Alignment and Deep Learning"**, and code summarization**Huang et al., "Automatic Code Summarization with Recurrent Neural Networks"**__**Xu et al., "Deep Learning-based Automatic Code Summarization"**. 
Specifically, the techniques involve training neural language models on data from public repositories such as GitHub and Stack Overflow**Bird et al., "Papers that Contain the Top Cited Articles about Programming Languages and GitHub"**__**Ray et al., "Code Search using Deep Learning and GitHub Repositories"**.
Leveraging the naturalness properties of human written code, these neural code models have learned useful patterns that unlock their potential as practical software development assistants**Nair et al., "Neural Code Models for Software Development Assistants"**__**Chen et al., "Deep Learning-based Software Development Assistant"**.

\subsection{Curriculum Learning}

Replicating the pedagogical learning process of humans, CL is a training strategy that gradually increases the difficulty of examples presented to the model**Bengio et al., "Curriculum Learning for Neural Networks"**, which contrasts with the conventional unordered training schedule.
CL has been effective for a wide range of model architectures and tasks**Goodfellow et al., "Deep Learning: A Survey on Model Architectures and Tasks"**. 
It improves model performance on complex examples by aligning training schedules with the model's skill-acquiring pace**Sutton et al., "Temporal Difference Methods for Policy Gradient Control"**.
Using generated programs, Nair et al.**Nair et al., "Curriculum Learning for Code Completion: A Study on the Effects of CL in Code Completion"**, investigated the effects of CL in code completion, whilst Liu et al.**Liu et al., "Curriculum Learning for Code Execution in Competitive Programming: A Case Study"**, focused on the task of code execution in competitive programming.
Although conventional difficulty measures such as length and cyclomatic complexity were investigated, these studies focused on synthetic programming problems, which may not generalise to real-world software.
In terms of real-world SE, CL has been employed in tasks, such as code clone detection**Wang et al., "Curriculum Learning for Code Clone Detection"**, code search**Ray et al., "Curriculum Learning for Code Search using Deep Learning"**, and automated program repair**Huang et al., "Curriculum Learning for Automated Program Repair"**, however, the methods largely focused on augmenting code to create difficult synthetic examples.
Additionally, past studies mainly focused on the final result, rather than investigating how the code model improves during CL, leaving us with an opaque understanding of the process.
Past studies have also yet to investigate the code-to-text task of code summarization.