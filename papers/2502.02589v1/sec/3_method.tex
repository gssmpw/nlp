\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\linewidth]{figures/annotation_pipeline.pdf}
    \caption{\textbf{Annotation Pipeline.} Given an input image, human-annotated panoptic segmentation masks are overlaid using set-of-marks~\cite{yang2023setofmark} visualization techniques to prompt the vision-language model (VLM). After generating an initial draft, human effort is investigated for editing and verification. Finally, the annotated metadata will be formatted to construct the datasets for various tasks at instruction tuning or finetuning stage.}
    \label{fig:pipeline}
\end{figure*}

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\linewidth]{figures/prompt_template.pdf}
    \caption{\textbf{Designed Prompt Template.} 
    By giving the concatenated set-of-marks images, the right side (round-1) shows the initial response and the corresponding human edits. Once finalized by humans, these edits will be merged into a single detailed caption grounded with panoptic segmentation masks, as shown in the left side (round-2). 
    }
    \label{fig:template}
\end{figure*}

\section{\textit{COCONut-PanCap} Dataset }
We construct a novel dataset based on COCO images to provide detailed captions at both image and mask levels, using COCONut panoptic masks as a foundation for comprehensive region descriptions. Specifically, we leverage  panoptic masks from COCONut-S~\cite{deng2024coconut} to annotate detailed region captions, incorporating both `thing' and `stuff' masks to cover a wide range of semantic regions. 

\subsection{Dataset Description}
Comprehensively understanding diverse visual elements in complex scenes can benefit multiple tasks including perception, understanding, and generation. In this section, we describe the annotation pipeline for our dataset leveraging the human annotated panoptic masks. We first show the statistical analysis of our final dataset in Tab.~\ref{tab:dataset_comp}. On average, our captions contain 203 words spanning 11 sentences. We follow the same split setting in COCO2017~\cite{lin2014coco} dataset, which includes 118K training images.
To provide a comprehensive evaluation set, we adopt the same 25K images from COCONut-val split (which contains COCO2017-val and another 20K Objects365~\cite{shao2019objects365} validation images).




\subsection{Dataset Construction}
We argue that high-quality descriptions should provide sufficient details of key objects and their attributes, as well as information about secondary objects and background elements. To achieve this, as shown in Fig.~\ref{fig:pipeline}, we use human-annotated panoptic segmentation masks to decide the set of objects to reference in the caption. These masks include both `thing' and `stuff' classes, representing single objects and semantic regions, respectively.
We adopt the panoptic segmentation masks from the COCONut-S~\cite{deng2024coconut} dataset. The masks are overlaid on the images, labeled with class names $c_1, c_2, \dots, c_n \in C$, where $C$ is the set of COCO’s 133 panoptic classes. We then construct a prompt with both the edited image and the original image and a textual question for GPT-4V, as illustrated in Fig.~\ref{fig:template}.
The resulting region captions from GPT-4V are reviewed and corrected by human raters for accuracy and consistency. 







\subsection{Dataset Analysis}
\noindent\textbf{Concepts Beyond COCO's 133 Classes.} To clarify the goal of our annotation task, we focus on key visual features such as objects, attributes, spatial relationships, and counting. As shown in Fig.~\ref{fig:freq_nouns}, we utilize the panoptic segmentation mask from COCONut-S, which includes 133 classes in the word vocabulary. Our proposed dataset, however, incorporates additional concepts beyond these 133 classes, such as `vegetable' and `parking'. This demonstrates that our human annotators delivers accurate and diverse descriptions when using the provided label names as a reference.



\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/nouns_freq.png}
    \caption{\textbf{Frequency of Extracted Nouns from the COCONut-PanCap Dataset}. The top 10 most frequent nouns are: people, table, room, street, dining, man, person, cars, chairs, and field.}
    \label{fig:freq_nouns}
\end{figure}
\noindent\textbf{User Study for Caption Quality.}
We randomly sample 1,000 images from our COCONut-PanCap training set and asked a human evaluator to perform a single-choice selection task. The question is: \textit{`Please select the best description for the image, considering the correctness of object names, attributes, counting, spatial relationships, and action.}' The compared captions are generated using GPT-4V~\cite{achiam2023gpt4v}, Qwen2-VL~\cite{wang2024qwen2vl}, and InternVL-2~\cite{chen2024internvl2}, resulting in a single-choice four-option question. Fig.~\ref{fig:user_study} illustrates the results, showing that our GPT-assisted human-annotated captions receives the highest ratings. More details can be found in the supplementary.

\section{PGC Baseline: PanCaper }
In this section, we introduce our baseline method for joint panoptic segmentation and grounded captioning (PGC), namely PanCaper. We start with an overview of the pixel grounding task and then present our proposed approach, which incorporates a panoptic segmentation module specifically designed for grounding objects in captions.



\noindent\textbf{Revisiting the Pixel Grounding Task.} Our baseline model builds upon LISA~\cite{lai2024lisa}, a model that combines the language generation capabilities of VLMs with the ability to produce segmentation mask. LISA consists of three main components: a VLM, a vision backbone $V$, and a mask decoder $D$.
With a given text prompt, the VLM (typically LLaVA~\cite{liu2023llava,liu2023improvedllava}) generates an output containing a $\langle \mathrm{SEG} \rangle$ token. For instance, with the input prompt, \textit{`Could you segment the food with high Vitamin C?'} LISA generates the response \textit{`It is $\langle \mathrm{SEG} \rangle$.'} This process extracts the last-layer embedding of the LLM from LLaVA. Then a language-to-prompt (L-P) projection layer ($g$) transforms the last-layer embeddings corresponding to $\langle \mathrm{SEG} \rangle$ tokens ($l_{\mathrm{seg}}$) into the decoder's feature space. Meanwhile, the vision backbone extracts dense visual features from the input image. Finally, both the dense features and the CLIP image embedding from LLaVA are fed into the mask decoder to produce the final segmentation mask.


\noindent\textbf{Prompt Instruction for Grounded Captioning.} We propose a baseline method for the PGC task by modifying LISA to enable grounded captioning with segmentation masks. Since LISA was originally designed for generating segmentation with a single output mask, two main adjustments are necessary: (1) the use of multiple $\langle \mathrm{SEG} \rangle$ tokens, and (2) extracting noun phrases from the caption for grounding.
To facilitate grounded segmentation, we modify the prompt to the VLM as \textit{`Please provide a detailed description of the image and segment each part.'} This prompt triggers the model to generate caption responses with corresponding $\langle \mathrm{SEG_i} \rangle$ tokens, where $i \in [1,N]$ and $N$ is the total number of predicted segmentations. 

Given a predicted caption for the image, aligning each $\langle \mathrm{SEG_i} \rangle$ token requires pairing it with a noun phrase, `$\langle \mathrm{p} \rangle \mathrm{phrase_i} \langle \mathrm{/p} \rangle$,' where $\mathrm{phrase_i}$ is the relevant part in the caption to be grounded.
With these prompt tokens defined, the model uses the vision backbone $V$ and mask decoder $D$ to facilitate fine-grained, pixel-level grounding, with $D$ producing segmentation masks $M$. 

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/user_study.png}
    \caption{\textbf{Caption Quality via User Study.} The study involved human evaluators assessing a random sample of 1,000 captions, with a strong preference shown for captions from our dataset. }
    \label{fig:user_study}
\end{figure}


\noindent\textbf{Enable Panoptic Grounding}. To achieve panoptic segmentation from captions, we first classify $\langle \mathrm{SEG} \rangle$ tokens into two types: $\langle \mathrm{SEG_t} \rangle$ for `thing' classes and $\langle \mathrm{SEG_s} \rangle$ for `stuff' classes. These tokens are then processed by our segmentation modules to produce panoptic segmentation masks.
We initialize the vision backbone $V$ with a pretrained kMaX-DeepLab encoder~\cite{yu2022kmaxdeeplab} and fine-tune the decoder $D$ using our COCONut-PanCap dataset. Since kMaX-DeepLab operates as a closed-set segmenter, we align text embeddings of the associated noun phrases with COCO’s 133 panoptic classes. To accomplish this alignment, we use BERT~\cite{koroteev2021bert} to generate the text embeddings and to calculate cosine similarity, selecting the best-matching category.
Panoptic grounding provides mapping between detailed captions and image regions, which  improves interpretability of VLM predictions.



\noindent\textbf{Training Objectives.} Our training objective aims to minimize the following losses:
\begin{equation}
    \mathcal{L} = \lambda_{\text{text}} \mathcal{L}_{\text{text}} + \lambda_{\text{mask}} \mathcal{L}_{\text{mask}},
\end{equation}
where $L_{\text{text}}$ is the auto-regressive cross-entropy loss for text generation, and $L_{\text{mask}}$ is the mask loss~\cite{wang2021max}, encouraging the model to produce high-quality segmentation results.
$\lambda_{\text{text}}$ and $\lambda_{\text{mask}}$ are the respective loss weights. We use the same loss weights as LISA~\cite{lai2024lisa}.



\noindent\textbf{Evaluation Metrics for Caption Quality.} We conduct the analysis with multiple metrics to evaluate the quality and completeness of the generated captions. We introduce a benchmarking suite for the PGC task, with a validation set of 25K images. For the caption quality, we report  the caption metrics including CIDEr~\cite{vedantam2015cider}, METEOR~\cite{banerjee2005meteor}, ROUGE-L~\cite{lin2004rouge}, BLEU\symbol{64}4~\cite{papineni2002bleu} and CAPTURE~\cite{dong2024capture}. For grounded panoptic segmentation, we report PQ scores~\cite{kirillov2019panoptic}.

