\section{Conclusion and Discussion}

In this work, we proposed a novel dataset designed to support detailed captioning and grounded segmentation tasks built on COCO images. We demonstrated that our dataset can enhance model performance during instruction tuning and fine-tuning stages across various multi-modal understanding and generation tasks, such as captioning, VQA, grounded segmentation, and text-to-image generation. We hope that COCONut-PanCap, with its detailed captions grounded with dense panoptic masks, will foster future advancements in multi-modal learning research.


\noindent\textbf{Limitations.}
High-quality human-labeled data offers significant benefits for instruction tuning in multi-modal tasks, but scaling such datasets is challenging. To address this, we introduce  COCONut-PanCap as a starting point for large-scale human-annotated data exploration. 
Recognizing the relatively smaller dataset size compared to other large dataset, future work may involve using this dataset to train seed models to generate more high-quality synthetic data.
