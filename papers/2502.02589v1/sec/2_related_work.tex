\input{tables/dataset_comp}
\input{tables/eval_comp}

\section{Related Work}
\label{sec:related_work}


\noindent\textbf{Detailed Captions from VLMs.} Researchers are increasingly interested in creating large-scale datasets with detailed captions generated from advanced vision-language models. DenseFusion1M~\cite{li2024DenseFusion} utilizes a pretrained perceptual model to prompt VLMs, facilitating more detailed image descriptions. 

Recap-DataComp1B~\cite{li2024recaption}  first fine-tunes the Llama-3-8B powered LLaVA-1.5 model~\cite{liu2023improvedllava}, then applies it to recaption approximately 1.3 billion images from the DataComp-1B dataset~\cite{gadre2024datacomp}, generating a rich repository of detailed image descriptions. On a similar front, the PixelProse dataset~\cite{singla2024pixels} offers general-purpose image captions designed to serve various applications, from visual question answering (VQA) to pre-training tasks. Unlike datasets targeting single applications, PixelProse captions are dense, versatile image descriptions that can be adapted to other formats, such as VQA and instructional data, with the help of large language models (LLMs). Although these detailed caption datasets are large-scale, they are directly generated by VLMs without human verification, falling behind human-annotated captions on quality. Our proposed COCONut-PanCap dataset leverages extensive human effort to ensure high-quality annotations.



\noindent\textbf{Human-annotated Detailed Captions.} Several efforts have been made toward this goal, utilizing fully human-annotated data or human-in-the-loop approaches. One example is DOCCI ~\cite{Onoe2024docci} which is a small, high-detailed image caption dataset that is entirely human-annotated, containing only 15K samples but providing diverse details, such as key objects, their attributes, spatial relationships, and text rendering. Two small-scale detailed caption datasets, ImageInWords~\cite{garg2024imageinwords} and DCI~\cite{Urbanek2024dci}, use a combination of automatic annotation models with human involvement, both with fewer than 10K samples. Pixmo-Cap~\cite{deitke2024molmo} introduces a large-scale dataset of detailed image captions from speech-based descriptions, offering richer visual annotations than text-based methods. 

Our proposed COCONut-PanCap dataset yields smaller scale compare to Pixmo-Cap but we have different focuses where Pixmo-Cap focuses on pretraining the VLMs while we focus on the instruction tuning and finetuning stages of VLMs and image generation models. Our work also shares a similar annotation pipeline with a recent video captioning dataset Shot2Story~\cite{han2023shot2story20k} where both VLM draft and human corrections are used to create complete and accurate annotations.



\noindent\textbf{Grounded Captions with Segmentation Masks.}
Existing work have made significant strides in creating datasets with region-level captions linked to entity segmentation masks~\cite{yuan2024osprey} or bounding boxes~\cite{zhang2023gpt4roi}. However, few datasets associate grounded segmentation directly with captions. GLaMM~\cite{hanoona2023GLaMM} proposes a Grounding-anything Dataset (GranD) using an automated annotation pipeline that encompasses 7.5M unique concepts grounded in a total of 810M regions available with segmentation masks. 
 
Later, MGLMM~\cite{zhou2024mglmm} further explore the multi-granularity GLaMM model to generate a multi-granularity dataset. Our proposed COCONut-PanCap dataset follows a similar approach of grounding captions to dense masks but offers significantly denser masks per caption, as shown in Tab.~\ref{tab:dataset_comp}, with an average of 13.2 masks per image compared to 3.6 in GLaMM. Note that we focus on grounded segmentation for detailed captions, rather than descriptions of all levels of segmentation masks (objects or parts) as provided in the GranD dataset~\cite{hanoona2023GLaMM}, which is outside the scope of our study.




