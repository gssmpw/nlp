\clearpage


\appendix

\noindent \textbf{The appendix is organized as follows.}
\begin{itemize}
    \item In Sec.~\ref{sec:experimental_details}, we show implementation details for Detailed Captioning (Sec.~\ref{subsec:detailed_captioning}), Panoptic segmentation and Grounded (Sec.~\ref{subsec:pgc}), and VQA (Sec.~\ref{subsec:vqa}).
    \item In Sec.~\ref{sec:qualitative_results}, we show more visualization examples of our proposed COCONut-PanCap dataset (Sec.~\ref{subsec:data}), and analysis of the tier cases in our dataset annotation user study (Sec.~\ref{subsec:tier}).
\end{itemize}










\section{Experimental Details}
\label{sec:experimental_details}
In this section, we provide more experimental details for detailed captioning (Sec.~\ref{subsec:detailed_captioning}), PGC (Sec.~\ref{subsec:pgc}), and VQA (Sec.~\ref{subsec:vqa}).

\subsection{Detailed Captioning}
\label{subsec:detailed_captioning}




\noindent\textbf{Detailed Captioning Instruction Dataset Construction.}
The key step in conducting the experiment is constructing the dataset. The original LLaVA-665K dataset consists of LLaVA-158K combined with other VQA datasets. Within LLaVA-158K, a subset of detailed captions corresponds to 23K COCO images. To create our-LLaVA-665K (referred to as LLaVA 665K-COCONut-PanCap in the table), we replace the detailed caption annotations for these 23K COCO images with our annotations. Importantly, the total amount of training data remains unchanged (only the captions for these 23K images are updated), ensuring a fair comparison of the impact of data quality on model performance.






\noindent\textbf{Synthetic Annotation for Detailed Caption.} To build the synthetic dataset with state-of-the-art VLM, we use three models, including open-sourced InterVL-2, Qwen2-VL and close-sourced GPT-4V to generate the detailed captions for COCO 118K train set images. We use the same text prompts that is used in LLaVA~\cite{liu2023llava} for prompting the model to create the detailed captions.

\noindent\textbf{LLaVA-NeXT-pool implementation details.} Fig.~\ref{fig:llava_arch} shows the comparison of the original LLaVA-NeXT and our proposed LLaVA-NeXT-pool. As shown in Fig.~\ref{fig:llava_anyres}, in order to preserve the details for the high-resolution images and representations, the original design employs a grid configuration which can also balance the performance efficiency with operational costs. Then both the patch-level and image-level features are later concatenated and sent to the LLM. Directly splitting the image into patches could cause prolems, for example, in the figure, the upper part of the dog's head is partitioned into different patches which may result in incomplete feature extraction for single object. To overcome this drawback, we propose LLaVA-NeXT-pool to extract the dense feature and preserve the object details by utilizing the panoptic segmentation masks in our COCONut-PanCap dataset. Fig.~\ref{fig:llava_pool} shows the details. Compared to the original design, LLaVA-NeXT-pool could effectively extract the features for the dog in our example. Our design enables more complete region-level feature extraction and is potential in understanding the details better.




\begin{figure}[t!]
    \centering
    % Subfigure 1
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/llava_anyres.pdf}
        \caption{LLaVA-NeXt-AnyRes}
        \label{fig:llava_anyres}
    \end{subfigure}
    \hfill
    % Subfigure 2
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/llava_mask_pool.pdf}
        \caption{our LLaVA-NeXt-pool}
        \label{fig:llava_pool}
    \end{subfigure}
    \caption{\textbf{Comparison of LLaVA-NeXt and our proposed LLaVA-NeXt-pool.}
    }
    \label{fig:llava_arch}
\end{figure}

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\linewidth]{figures/pancaper_arch.pdf}
    \caption{\textbf{Architecture of PanCaper.} We utilize a pretrained vision encoder from kMaX-DeepLab~\cite{yu2022kmaxdeeplab} as our vision backbone, which effectively extracts dense features essential for panoptic segmentation.}
    \label{fig:pancaper_arch}
\end{figure*}






\subsection{PGC}
\label{subsec:pgc}
We provide more implementation details for the proposed task: \textbf{P}anoptic segmentation and \textbf{G}rounded \textbf{C}aptioning (PGC).

\noindent\textbf{PanCaper Implementation Details.} We introduce the PanCaper architecture details in this section. Following the architecture in LISA~\cite{lai2024lisa}, there are three components including the vision backbone, mask decoder and multi-modal LLM. Fig.~\ref{fig:pancaper_arch} shows the architecture details for PanCaper. We made modification on the vision backbone, and mask decoder part in terms of model architecture. To preserve the learned knowledge of the pre-trained multimodal LLM (\ie, LLaVA-NeXT in our experiments), we leverage LoRA~\cite{hu2021lora} to perform efficient fine-tuning, and completely freeze the vision backbone. The mask decoder is fully fine-tuned. Additionally, the LLM token embeddings (embed tokens), the LLM head (lm head), and the projection layer are also trainable.
The weights of the text generation loss $\lambda_{\text{text}}$  and the mask
loss $\lambda_{\text{mask}}$ are set to 1.0 and 1.0, respectively. For the PQ-style mask loss, we follow the same settings in kMaX-DeepLab~\cite{yu2022kmaxdeeplab}, where it consists of mask-level cross entropy loss, dice loss and pixel loss.


\noindent\textbf{Adapting Baseline Methods for PGC Task.}
We adopt the same text prompt template to enable the model to perform PGC tasks. For LISA$+$, we follow the same design in GLaMM~\cite{hanoona2023GLaMM} to design the multi entity mask output by utilizing the the GranDf dataset. As the intruction dataset of GranDf is constructed similarly grounding the phrase in the image-level caption, it will output multiple $\langle \mathrm{SEG} \rangle$ tokens. The reasoning results of the number of $\langle \mathrm{SEG} \rangle$ tokens decide the number of output entity mask which are often binary masks. As a result, the model can generate a detailed caption along
with interleaved segmentation masks, employing the format ``$\langle \mathrm{p} \rangle$A man$\langle \mathrm{/p} \rangle$$\langle \mathrm{SEG} \rangle$ ... next to $\langle \mathrm{p} \rangle$a tree$\langle \mathrm{/p} \rangle$$\langle \mathrm{SEG} \rangle$''. And thus the format of instruction dataset is significat in task design. Therefore, we formulate our dataset as ``$\langle \mathrm{p} \rangle$A man$\langle \mathrm{/p} \rangle$$\langle \mathrm{SEG_t} \rangle$ ... next to $\langle \mathrm{p} \rangle$a tree$\langle \mathrm{/p} \rangle$$\langle \mathrm{SEG_s} \rangle$'', where $\langle \mathrm{SEG_t} \rangle$ represents the seg token for instance masks of thing and $\langle \mathrm{SEG_s} \rangle$ represents for semantic masks of stuff respectively in panoptic setting. Similarly, utilizing the PanCap dataset and special token design, GLaMM~\cite{hanoona2023GLaMM} is able to generate the entity masks with the tag of `thing' and `stuff'. 


\noindent\textbf{Training Data Formulation.} We adopt the same training data from LISA~\cite{lai2024lisa} which comprises mainly three parts, all of which are derived from widely-used public datasets. These include 1) Semantic Segmentation datasets including  ADE20K~\cite{zhou2017ade20k}, COCO-Stuff~\cite{caesar2018coco_stuff}, and LVIS-PACO~\cite{ramanathan2023paco} part datasets with the generated QA data, 2) Vanilla Referring Segmentation Datasets: refCOCO, refCOCO+, refCLEF~\cite{kazemzadeh2014refcoco} and refCOCOg~\cite{mao2016refcocog} datasets, 3) ReasonSeg dataset~\cite{lai2024lisa}, and 4) Visual Question Answering Dataset: LLaVA-v1.5-mix665k~\cite{liu2023improvedllava}. To enable the multi-mask generation for grounded caption, there are two options for instruction datasets, GranDf and our COCONut-PanCap where GranDf consists of entity masks while COCONut-PanCap consists of panoptic masks. 



\subsection{VQA}
\label{subsec:vqa}
We provide more implementation details for the VQA experiments. We follow the same setting in LLaVA-NeXT to create the experimental results for VQA tasks. We focus on the instruction tuning stage by adopting the pretrained weights from the stage-1 across the trainings for all the model variants mentioned in Tab.~7 in the paper. The dataset we used is exactly the same as in LLaVA 665K~\cite{liu2023improvedllava} which includes the earlier version of instruction data proposed in LLaVA 158K~\cite{liu2023llava}, ShareGPT~\cite{sharegpt}, VQAv2~\cite{mao2016vqav2}, GQA~\cite{hudson2019gqa}, openknowledge VQA (OKVQA~\cite{marino2019okvqa}, A-OKVQA~\cite{schwenk2022aokvqa}), OCR (OCRVQA~\cite{mishra2019ocrvqa}, TextCaps~\cite{sidorov2020textcaps}), region-level VQA datasets (Visual Genome~\cite{krishna2017visualgenome}, RefCOCO~\cite{kazemzadeh2014refcoco}). Among these data, LLaVA 158K comprises 77K complex reasoning, 58K conversation and 23K detailed captions. To build the dataset variants shown in Tab.~7, we simply remove the subset of detailed\_caption\_23k, and subsequently add 20K, 50K and 118K COCONut-PanCap dataset to build LLaVA 665K-COCONut-PanCap-20K, LLaVA 665K-COCONut-PanCap-50K and LLaVA 665K-COCONut-PanCap-118K. By these steps, we add more detailed caption data to construct the instruction tuning dataset. This results in the total amount of training data of 662K for LLaVA 665K-COCONut-PanCap-20K, 692K for LLaVA 665K-COCONut-PanCap-50K and 760K for LLaVA 665K-COCONut-PanCap-118K. And thus the size of LLaVA 665K-COCONut-PanCap-20K is slightly smaller than the original LLaVA 665K dataset, but the model trained on it yields better performance. For the evaluation settings, we follow the exact settings in LLaVA-NeXT~\cite{liu2024llavanext} using lmms\_eval\footnote{https://github.com/EvolvingLMMs-Lab/lmms-eval}.


\section{More Qualitative Results}
\label{sec:qualitative_results}
In this section, we present additional qualitative results of COCONut-PanCap annotations (Sec.~\ref{subsec:data}) and a detailed analysis of tier cases from the user study (Sec.~\ref{subsec:tier}).

\subsection{Data Examples}
\label{subsec:data}
We show more visualization of our proposed COCONut-PanCap dataset in Fig.~\ref{fig:vis_examples_1} and Fig.~\ref{fig:vis_examples_2}.




\subsection{PanCaper and GPT-4V Tier Showcases}
\label{subsec:tier}
In the user study involving 1,000 samples, captions generated by GPT-4V were preferred in 87 cases. Among these, actually, 46 were tier cases where human raters considered both GPT-4V and COCONut-PanCap captions equally good. Fig.~\ref{fig:tier_examples_1}, Fig.~\ref{fig:tier_examples_2} and Fig.~\ref{fig:tier_examples_3} illustrate qualitative examples, highlighting the reasons for the tier classification and instances where GPT-4V was chosen.


\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/vis_1.pdf}
    \caption{
    \textbf{Visualization of the Panoptic Grounded Caption.}
    Our annotated captions ground the panoptic segmentation masks.
    }
        \vspace{-20pt}

    \label{fig:vis_examples_1}
\end{figure*}


\begin{figure*}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/vis_2.pdf}
    \caption{
    \textbf{Visualization of the Panoptic Grounded Caption.}
    Our annotated captions ground the panoptic segmentation masks.
    }
        \vspace{-25pt}

    \label{fig:vis_examples_2}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/tier_1.pdf}
        \vspace{-10pt}

    \caption{
    \textbf{Tier Examples for the User Study.}
    Our COCONut-PanCap annotations are tied with GPT-4V annotations for some simple cases.
    }
    \label{fig:tier_examples_1}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/tier_2.pdf}
    \vspace{-10pt}
    \caption{
    \textbf{Tier Examples for the User Study.}
    Our COCONut-PanCap annotations are tied with GPT-4V annotations for some simple cases.
    }
    \label{fig:tier_examples_2}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/tier_3.pdf}
        \vspace{-10pt}

    \caption{
    \textbf{Tier Examples for the User Study.}
    Our COCONut-PanCap annotations are tied with GPT-4V annotations for some simple cases.
    }
    \label{fig:tier_examples_3}
\end{figure*}
