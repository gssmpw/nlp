
\input{tables/caption}
\input{tables/pcg}

\input{tables/t2i}
\input{tables/geneval}

\input{tables/alabte_vqa}



\input{tables/refcoco}




\section{Experimental Results}
We assess the effectiveness of human-annotated caption data by performing three primary tasks utilizing our dataset in the fine-tuning/instruction tuning stage: detailed captioning, panoptic grounded captioning (PGC), and text-to-image generation. Additionally, we demonstrate the transferability of the knowledge learned from our dataset through two downstream tasks: VQA and referring segmentation.

\noindent\textbf{Detailed Captioning.} We conduct instruction tuning with LLaVA-NeXT framework~\cite{liu2024llavanext} for this task. We replace the caption data (23k) from the original LLaVA instruction-tuning set with detailed captions from our dataset, keeping the same amount of instruction data size. We follow the same training setup used for LLaVA-NeXT with Llama3-8B~\cite{dubey2024llama3}. Treating it as a QA task, we use the prompt, \textit{`Could you please describe the image in detail?'} and collect the corresponding response as the caption for the image. We evaluate caption quality using CIDEr~\cite{vedantam2015cider}, METEOR~\cite{banerjee2005meteor}, BLEU@4~\cite{papineni2002bleu}, ROUGE-L~\cite{lin2004rouge} and CAPTURE~\cite{dong2024capture} metrics. We also extend the model by adding the mask-pooled features from the panoptic segmentation masks as additional signals to the LLaVA model and name it LLaVA-NeXT-pool.  During training, we use the ground truth mask to extract the features while during inference we use the mask proposals from the pretrained kMaX-DeepLab~\cite{yu2022kmaxdeeplab}. Besides, we also experiment with synthetic captions directly generated using InternVL-2~\cite{chen2024internvl2}, Qwen2-VL~\cite{wang2024qwen2vl} and GPT-4V~\cite{achiam2023gpt4v}. We follow the same data preparation settings as our dataset to build these instruction datasets for these 23K images with different sources of synthetic detailed captions, namely LLaVA 665K-InternVL2-Cap , LLaVA 665K-Qwen2VL-Cap, and LLaVA 665K-GPT4V-Cap. These datasets are used to produce models LLaVA-NeXT-I, LLaVA-NeXT-Q, and LLaVA-NeXT-G respectively.
More details can be found in the supplementary. The results are presented in Tab.~\ref{tab:caption}. LLaVA-NeXT models show improved performance when fine-tuned on the custom instruction-tuning dataset. Among these, LLaVA-NeXT-pool achieves the highest scores in all metrics, with CAPTURE of 61.4, CIDEr of 13.1, BLEU@4 of 5.3, and METEOR of 17.1, significantly higher than the original model variant LLaVA-NeXT, indicating the benefit of added region features for additional visual cues.  Models trained on synthetic captions (LLaVA-NeXT-I, LLaVA-NeXT-Q, and LLaVA-NeXT-G) generally show lower scores, showing advantage of our human-annotated caption. 


\noindent\textbf{PGC: Stronger Detail Reasoning Performance.} We implement our proposed PanCaper based on LISA which uses pre-trained LLaVA-NeXT with a LLM of Llama3-8B, with LoRA~\cite{hu2021lora} adopted. The vision encoder uses a fixed CLIP-ViT-L/14-336 model, modified with linearly interpolated position embeddings to process 448 resolution images. The trainable components of our model include the mask decoder of kMaX-DeepLab, and the tunable parts in LLaVA same as in LISA. To enhance model performance in visual understanding, we initialize our PanCaper using pretrained LLaVA-NeXT models from the detailed captioning task. We also experiment with a model variant that uses mask pooled features similar to LLaVA-NeXT-pool, and name it PanCaper-Pro.

For comparison, we select 3 related methods LISA, PixelLM~\cite{ren2024pixellm} and GLaMM~\cite{hanoona2023GLaMM} for evaluation. It is noteworthy that LISA is not able to perform multi-mask prediction. We therefore adapt LISA~\cite{lai2024lisa} for the multi-mask generation with grounded segmentation, namely LISA+. The implementation details can be found in the supplementary. Tab.~\ref{tab:pgc} shows the quantitative results. Our proposed PanCaper-Pro achieves the highest scores across all captioning metrics (CIDEr: 12.5, CAPTURE: 64.3, BLEU@4: 6.4, METEOR: 17.9), outperforming all other models. Both PanCaper models show significant improvements over other models in all captioning metrics, highlighting the effectiveness of the COCONut-PanCap dataset for detailed caption generation. On grounding segmentation, PanCaper-Pro again leads, with a PQ score of 0.61, $\text{PQ}^\text{thing}$ of 0.58, and $\text{PQ}^\text{stuff}$ of 0.68, reflecting its robustness on both ``thing'' and ``stuff'' classes. Notably, enabling mask pooling in our proposed PanCaper-Pro further enhances segmentation metrics. The baseline models (LISA+ and GLaMM with GranD) achieve much lower PQ scores, due to incomplete segmentation annotations in the GranD dataset. 


\noindent\textbf{Text-to-Image Generation.} We adopt the Stable Diffusion 3 (SD3) medium model\footnote{\scriptsize https://huggingface.co/docs/diffusers/stable\_diffusion/stable\_diffusion\_3} for text to image generation with LoRA finetuning. We adopt the default training settings but only with different text-image datasets for training. We evaluate with two types of training images from COCO~\cite{lin2014coco} and DOCCI~\cite{Onoe2024docci} datasets. In details, for the COCO images, we explore the short COCO-caption and detailed captions from our dataset. For DOCCI images, we directly use the captions from their dataset. Tab.~\ref{tab:t2i} shows the quantitative results. Traning on COCONut-PanCap achieves the best performance across all metrics when evaluated on DOCCI-test, with the lowest FID (21.4), lowest $\text{FD}_{\text{dinov2}}$ (290), and the highest CLIPScore (77.9), indicating superior generation quality and high image-text relevance. 
When evaluated on COCONut-PanCap-val set, training on COCONut-PanCap again shows the best results with the lowest FID (23.1), $\text{FD}_{\text{dinov2}}$ (267), and a high CLIPScore of 77.3.

Tab.~\ref{tab:geneval} shows the results on GenEval benchmark~\cite{ghosh2023geneval}. Finetuning SD3-medium with COCONut-PanCap consistently scores the highest in most categories, particularly those requiring image details like color attribution, object positioning, and handling multiple objects. Our proposed dataset enables more accurate image generation that requires understanding of relationships, multiple objects and counting, tasks that other datasets struggle with.

\noindent\textbf{VQA.} To evaluate the effectiveness of the proposed COCONut-PanCap dataset, we utilize these captions during the instruction-tuning stage and follow the setup of LLaVA-NeXT~\cite{liu2024llavanext} across various visual question answering (VQA) and multi-modality understanding benchmarks. We evaluate on MM-Vet~\cite{yu2024mm-vet}, SEED-IMG~\cite{li2023seed}, MMBench-en~\cite{liu2023mmbench}, MME~\cite{fu2023mme}, POPE~\cite{li2023pope}, and TextVQA~\cite{singh2019textvqa}, covering a broad range of evaluation dimensions. We experiment with different amount of our COCONut-PanCap caption data injected into the instruction tuning stage by replacing the original COCO captioning data with our dataset. As shown in Tab.~\ref{tab:ablate_vqa}, the baseline model LLaVA-NeXT (using its original recaptioned COCO) achieves relatively lower performance across all metrics, with scores such as 43.5 on MM-Vet, 70.1 on Seed-IMG, and 68.9 on TextVQA. 
Building on LLaVA-NeXT baseline, we progressively incorporated varying amounts of COCONut-PanCap data (20K, 50K, and 118K (full), as indicated by postfixes in the baseline names) during instruction-tuning. Consistent improvements are observed across all evaluated benchmarks as more of our data is integrated.


\noindent\textbf{Referring Segmentation.} In this task, the model processes an image and a textual referring expression to output a segmentation mask corresponding to the expression. The prompt used is, \textit{`Please segment the $\langle \mathrm{referring\_text} \rangle$ in the image.'} The target model response is \textit{`Sure, it is  $\langle \mathrm{SEG} \rangle$.'}, where the  $\langle \mathrm{SEG} \rangle$ token is decoded to obtain the mask. We follow the setup in LISA~\cite{lai2024lisa}, using multiple segmentation datasets to jointly train the models. 
Tab.~\ref{tab:refcocco} shows the quantitative results. Our model achieves superior performance, particularly when additionally trained with the COCONut-PanCap dataset (last row), outperforming all models except GLaMM~\cite{hanoona2023GLaMM}. This improvement underscores our model's efficacy in handling complex referring expressions, likely due to the additional data that enhances model generalization and accuracy. It is worth noting that GLaMM performs competitively with our method, though the comparison is uneven given their additional use of the SA-1B dataset~\cite{kirillov2023segmentanything}.

\noindent\textbf{Synthetic \vs Human Annotated Data.} Generating synthetic data for captioning has been popular for recent tasks in either training vision encoders~\cite{radford2021clip} or text-to-image generation~\cite{li2024recaption}. We investigate the effect of varying the mix ratio of synthetic captions generated by GPT-4V and our human-annotated data for fine-tuning (where 0 indicates fully synthetic data), using the COCONut-PanCap dataset for training and the COCONut-PanCap-val set for evaluation. We adopt LLaVA-NeXT for the captioning task and SD3-medium for the image generation task. As shown in Fig.~\ref{fig:fid_capture}, adding 25\% human-annotated data yields significant performance improvements in both captioning and generation, with a reduced FID of 26 from 31 (lower is better) and an increased CAPTURE score of 53.6 from 47.5 (higher is better). Consistent improvements are observed as more human-annotated data is incorporated.



\begin{figure}[htbp]
    \centering
    % First subfigure
    \includegraphics[width=0.9\linewidth]{figures/fid_capture.png}
    \vspace{-5pt}
    \caption{
    \textbf{Varying Synthetic and Human-Annotation Ratios.}
    CAPTURE is used to evaluate the performance of LLaVA-NeXT on detailed captioning, while FID assesses the performance of SD3-medium on text-conditioned image generation.
    }
        \vspace{-10pt}
    \label{fig:fid_capture}
\end{figure}


