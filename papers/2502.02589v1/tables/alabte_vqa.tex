

\begin{table*}[htbp]
\centering
\tablestyle{1.5pt}{1.05}
\begin{tabular}{l|c|l|ccccccc}

\textbf{Method   }              & \textbf{LLM }      & \textbf{Instruction-tuning Dataset}       & \textbf{MM-Vet} & \textbf{Seed-IMG} & \textbf{MMBench-en} & \textbf{TextVQA} & \textbf{POPE} & \textbf{MME}  \\
\shline
LLaVA-NeXT * & Llama3-8B & orginal LLaVA 665K~\cite{liu2024llavanext}               & 43.5   & 70.1     & 71.4        & 68.9    & 85.4 & 1523 \\
LLaVA-NeXT-20K                & Llama3-8B & LLaVA 665K-COCONut-PanCap-20K               & 44.1   & 72.5     & 73.6        & 69.8    & 86.1 & 1552 \\
LLaVA-NeXT-50K                & Llama3-8B & LLaVA 665K-COCONut-PanCap-50K                     & 44.6   & 73.1     & 74.2        & 70.0    & 87.1 & 1600 \\
LLaVA-NeXT-Full               & Llama3-8B & LLaVA 665K-COCONut-PanCap-118K                  & 45.5   & 74.3     & 75.1        & 70.7    & 87.9 & 1612 \\
\hline

LLaVA-1.5  & Vicuna-7B & LLaVA 665K-ShareGPT4V-100K & 37.8 & 67.4 &70.5 & 64.6 & 84.7& 1519 \\
 
LLaVA-1.5  & Vicuna-7B & LLaVA 665K-COCONut-PanCap-20K  & 38.5 & 67.7 &70.9 &64.5 & 84.9 & 1521 \\
\end{tabular}
% \vspace{-5pt}
\caption{\textbf{Benchmark Results and Ablation Study on VQA.}  By adding extra detailed caption data for instruction tuning, the models show increased improvement. * denotes reproduced results. Using only \textbf{20K human labeled data} can still achieve \textbf{comparable performance} to 100K synthetic data.
}
\label{tab:ablate_vqa}
\end{table*}

