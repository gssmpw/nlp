


\begin{table*}[t!]
\centering
\tablestyle{1.5pt}{1.05}
% \scalebox{0.85}{
\begin{tabular}{l|r|c|r|c|c}
\textbf{Dataset Name} & \textbf{Image Source}&\textbf{Sample}  & \textbf{Annotated by}   & \textbf{Avg. Words} & \textbf{Masks} \\
\shline


BLIP-LCS & LAION~\cite{schuhmann2022laion5b}, CC~\cite{changpinyo2021cc12m}, SBU~\cite{vicente2011sbu} & 558K & BLIP~\cite{li2022blip} & 54 & \xmark \\
DenseFusion1M~\cite{li2024DenseFusion} & LAION~\cite{schuhmann2022laion5b} & 1,059K & Vision Specialist Models & 191 & \xmark \\
LLaVA-Recap118K~\cite{liu2024llavanext} & COCO~\cite{lin2014coco} & 118K & LLaVA-NEXT~\cite{liu2024llavanext} & 186 & \xmark \\
LLaVA-Details-23K~\cite{liu2023llava} & COCO~\cite{lin2014coco} & 23K & GPT4 & 105 & \xmark \\

ShareGPT4V~\cite{chen2023sharegpt4v} & LAION~\cite{schuhmann2022laion5b}, CC~\cite{changpinyo2021cc12m}, SBU~\cite{vicente2011sbu}, COCO~\cite{lin2014coco} \etc & 100K & GPT4-Vision & 162 & \xmark \\
ShareGPT4V-PT~\cite{chen2023sharegpt4v} & LAION~\cite{schuhmann2022laion5b}, CC~\cite{changpinyo2021cc12m}, SBU~\cite{vicente2011sbu}, COCO~\cite{lin2014coco} \etc & 1,246K & Share-Captioner~\cite{chen2023sharegpt4v} & 144 & \xmark \\
\hline
PixelLM-MUSE~\cite{ren2024pixellm} & LVIS~\cite{gupta2019lvis} & 246K & GPT4-Vision & - & 3.7\textsuperscript{\ddag}  \\
Osprey~\cite{yuan2024osprey} & COCO~\cite{lin2014coco} & 724K & GPT4-Vision & - & - \\
GLaMM-GCG~\cite{hanoona2023GLaMM} &  RefCOCOg~\cite{mao2016refcocog},PSG~\cite{yang2022psg},Flick30K~\cite{plummer2015flickr30k}  & 214K & Vision Specialist Models & 128 & 3.6 \\ 
\hline
COCO-caption~\cite{chen2015coco_caption} & COCO~\cite{lin2014coco} & 118K & \textcolor{darkergreen}{\textbf{Human}} & 11
%11.3
& \xmark \\

DCI~\cite{Urbanek2024dci} & SA-1B~\cite{kirillov2023sam} & 8K & \textcolor{darkergreen}{\textbf{Human}}  & 144 & \xmark \\
DOCCI~\cite{Onoe2024docci} & DOCCI~\cite{Onoe2024docci} & 9.6K & \textcolor{darkergreen}{\textbf{Human}}  & 136 & \xmark \\
IIW~\cite{garg2024imageinwords} & WebLI~\cite{garg2024imageinwords} & 8.5K & \textcolor{darkergreen}{\textbf{Human}}  & 217 & \xmark \\
COCONut-PanCap (ours) & COCO~\cite{lin2014coco} & \textbf{118K} & \textcolor{darkergreen}{\textbf{Human}}  & 203 & \textbf{13.2} \\





\end{tabular}
% }
\vspace{-10pt}
\caption{
\textbf{Dataset (training set) Comparison. }
Our proposed COCONut-PanCap dataset stands out for its \textbf{detailed} (2nd highest in Average Words), \textbf{high-quality} (human interactive annotated) \textit{captions} and \textbf{high-density} \textit{segmentation masks} (1st in Average Masks).
\textsuperscript{\ddag} denotes the mask number for referring segmentation which only counts the targets in QA format. Note that ``Samples" means the number of collected annotations, where there may exist one image with multiple different annotation, \ie, in region-level datasets like Osprey.
}
\label{tab:dataset_comp}
\end{table*}


