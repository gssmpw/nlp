\appendix
\section{Related Works}
In addition to methods based on RoPE rescaling, this
section discusses related works of other approaches.







\noindent\textbf{RAG and Agent-based extension}. Retrieval-Augmented Generation (RAG) approaches
incorporate an external memory module to store and manage long past context, coupled with dynamic retrieval mechanisms to fetch task-relevant documents during inference~\citep{jeong2024adaptiveraglearningadaptretrievalaugmented, chan2024rqraglearningrefinequeries, dong2024multiviewcontentawareindexinglong,gutiérrez2025hipporagneurobiologicallyinspiredlongterm, luo2024bgelandmarkembeddingchunkingfree}. Agent-based methods, meanwhile, decompose long-context processing into iterative planning, summarization, and retrieval tasks, often employing multi-agent workflows: individual agents extract information from text segments, which are aggregated to bypass fixed context limits \citep{zhang2024chainagentslargelanguage, li2024graphreaderbuildinggraphbasedagent, lee2024humaninspiredreadingagentgist}, while others integrate specialized architectures (e.g., hierarchical attention) for direct long-text handling \citep{gur2024realworldwebagentplanninglong}. Both directions—relying on external modules or multi-step decomposition—are complementary to our method. 


\noindent\textbf{Efficient long-context modeling}. Attention computation and memory costs grow quadratically with context length, prompting research into reducing these challenges through improved attention mechanisms and innovative model structures. Many methods leverage the sparsity of standard attention, reducing computation by focusing on local and auxiliary regions~\citep{child2019generatinglongsequencessparse, beltagy2020longformerlongdocumenttransformer, NEURIPS2020_c8512d14, guo2022longt5efficienttexttotexttransformer}, while others extend context length using fine-grained sparsity~\citep{ding2023longnetscalingtransformers1000000000} or chunked attention~\citep{an2024trainingfreelongcontextscalinglarge}. Linear attention approaches further lower complexity while achieving comparable performance, with additional optimization for hardware efficiency~\citep{katharopoulos2020transformersrnnsfastautoregressive, yang2024gatedlinearattentiontransformers}. State-space models (SSMs) offer linear complexity for sequence modeling~\citep{gu2024mambalineartimesequencemodeling, yu2024robustifying}, and hybrid transformer-SSM architectures enhance foundational model capabilities~\citep{lieber2024jambahybridtransformermambalanguage, ren2024sambasimplehybridstate}.
Most of these approaches build upon RoPE, making them complementary to our approach. 





\section{Additional Experiments and Analysis}
\label{sec:addexp}

\noindent\textbf{Additional details}.  For the rescaling factor search, we set a population size of $P=64$, evolution iterations of 40, and a mutation probability $p=0.3$. The searched rescaling factors are then applied with mixed context window training. 

To accelerate training and inference, we use FlashAttention-2~\citep{dao2023flashattention2}, which requires no modifications for mixed context window training or factor-switch-based inference (as illustrated in Fig.~\ref{fig:mixwindow}).
Given that GPU memory and computation time increase exponentially with sequence length, fine-tuning long-context models presents significant challenges. To address this, we utilize nnScaler~\citep{nnscaler}, an efficient distributed training system for long-context LLMs, to reduce training costs. 10B tokens take approximately 39 hours for Phi3-mini and 54 hours for LLaMA3-8B on 64 A100 GPUs.
During inference, the switch between rescaled and original RoPE is triggered when the combined length of the input context and generated tokens exceeds the pre-trained context window. Switching to rescaled RoPE for long-context inference requires a one-time recalculation of the KV cache, a potential limitation we leave for future work.


\noindent\textbf{Additional results on RULER and Needle-in-a-Haystack}.  Tables~\ref{tbl:ruler2-phi3} and \ref{tbl:ruler2-llama3} show the detailed per-task accuracy of our extended LLMs on the RULER benchmark. Figures~\ref{fig:phineedle} and \ref{fig:llamaeneedle} provide comprehensive results for the needle-in-a-haystack tests. As observed, the YaRN method frequently fails to retrieve needles across Phi3-mini-3.8B, LLaMA3-8B, Meta-LLaMA3.1-8B and Meta-LLaMA3.1-8B-Instruct.

	\begin{table}[htp]
	\caption{{\sysname}-extended Phi3-mini (3.8B)-128k per-task performance on RULER.}
	\label{tbl:ruler2-phi3}
	\resizebox{1\columnwidth}{!}{	\begin{tabular}
			{ccccccccccccccc}
			\toprule
			Length&\makecell{NIAH\\single1}&\makecell{NIAH\\single2}&\makecell{NIAH\\single3}&\makecell{NIAH\\multikey1}&\makecell{NIAH\\multikey2}&\makecell{NIAH\\multikey3}&\makecell{NIAH\\multivalue}&\makecell{NIAH\\multiquery}&VT&CWE&FEW&\makecell{single-hop\\ QA}& \makecell{multi-hop\\ QA} & Avg.\\
			\midrule
		4096& 100&100&99&91&96&97&97.75&97.75&85.8&93.7&85.33&82&50&90.41\\
		8192&100&100&100&90&93&97&89.5&93.75&84&87.2&86&68&47&87.34\\
		16384&100&100&99&87&88&82&91.25&89&85&55.4&91.67&70&45&83.33\\
		32768&100&100&99&86&86&57&87&78&76.8&33.2&91.67&56&44&76.51\\
		65536&100&100&99&85&71&32&67.75&69.25&66.8&0.4&71.67&50&37&65.37\\
		131072&100&98&95&92&40&18&56.75&59&35.2&0.3&89.33&47&34&58.81\\		\hline				\end{tabular}}
\end{table}
\vspace{-3ex}

\begin{table}[htp]
	\caption{{\sysname}-extended LLaMA3-8B-128k per-task performance on RULER.}
	\label{tbl:ruler2-llama3}
	\resizebox{1\columnwidth}{!}{	\begin{tabular}
			{ccccccccccccccc}
			\toprule
			Length&\makecell{NIAH\\single1}&\makecell{NIAH\\single2}&\makecell{NIAH\\single3}&\makecell{NIAH\\multikey1}&\makecell{NIAH\\multikey2}&\makecell{NIAH\\multikey3}&\makecell{NIAH\\multivalue}&\makecell{NIAH\\multiquery}&VT&CWE&FEW&\makecell{single-hop\\ QA}& \makecell{multi-hop\\ QA} & Avg.\\
			\midrule
			4096& 100&100&99&100&100&100&99&99.75&98.8&98.5&96.33&79&60&94.61\\
			8192&100&100&100&100&100&100&99&99.75&99.8&95.9&91.33&74&58&93.68\\
			16384&100&100&100&99&100&98&95&98.25&99.6&86.8&96.33&69&58&92.31\\
			32768&100&100&100&99&98&100&98&96.25&98.6&63.9&95.67&72&55&90.49\\
			65536&100&100&100&98&98&95&95.75&99.75&98.6&33.6&80.33&62&52&85.62\\
			131072&100&100&99&96&91&94&96.5&97&92.6&9&85.33&56&50&82.03\\		\hline				\end{tabular}}
\end{table}



\begin{table}[htp]
	\caption{Ablation study on the number of searched dimensions.}
	\label{tbl:searchalg}
	\centering
	\resizebox{0.9\columnwidth}{!}{	\begin{tabular}
			{ccccccccccc}
			\hline
			\multirow{2}{*}{Method} &\multicolumn{3}{c}{Regular short tasks}& &\multicolumn{6}{c}{RULER}\\
			\cmidrule{2-4} \cmidrule{6-11}
			& MMLU & \makecell{MMLU Pro} & GSM8K&&4k&8k&16k&32k&64k&128k \\
			\midrule
			\multicolumn{10}{c}{Base Model: Phi3-mini (3.8B)} \\
			\rowcolor{airforceblue}	{\sysname} ($d_{rcd}$ and higher dims)&\bf70.07&\bf40.30&73.62&&\bf 90.41&\bf 87.22&\bf83.33&\bf 76.51&\bf 65.37&\bf58.81\\
			{\sysname} (all dims)	& 69.96&39.84&\bf74.83&&90.02&87.21&82.42&74.86&63.95&57.34\\
			\midrule
			\multicolumn{10}{c}{Base Model: LLaMA3-8B} \\
			\rowcolor{airforceblue}		{\sysname} ($d_{rcd}$ and higher dims) &\bf65.01&\bf34.61&50.80&& \bf 94.61&\bf 93.68& \bf 92.31& \bf90.49& \bf85.62&\bf82.03\\
			{\sysname} (all dims)	& 64.34&33.83&\bf51.55&&93.92&92.61&91.41&89.30&83.11&78.07\\
			\hline				
	\end{tabular}}
\end{table}

\begin{figure}[htp]
	\centering
	\includegraphics[width=0.9\columnwidth]{phi3-needle.pdf}	
	\vspace{-2ex}
	\caption{Needle in a Haystack full results for Phi3-mini (3.8B)-128k.}
	\label{fig:phineedle}
\end{figure}
\begin{figure}[htp]
	\centering
	\includegraphics[width=0.9\columnwidth]{llama3-needle.pdf}	
	\vspace{-2ex}
	\caption{Needle in a Haystack full results for LLaMA3-8B-128k.}
	\label{fig:llamaeneedle}
\end{figure}

\begin{figure}[htp]
	\centering
	\includegraphics[width=1\columnwidth]{yarn-ntk.png}	
	\vspace{-4ex}
	\caption{The RoPE rescaling factor distributions of NTK/YaRN adjusted based on the real critical dimension (i.e., YaRN-rcd, NTK-rcd).}
	\label{fig:yarn-ntk}
\end{figure}




\noindent\textbf{The ablation study on search algorithm}. In our work, we focus on searching for the real critical dimension and the scaling factors of higher dimensions beyond it. For the lower dimensions before the critical dimension, we directly apply NTK scaling without further optimization. To evaluate this design, we conduct an additional ablation study. For comparison, we also allowed the search to include lower dimensions. As shown in Table~\ref{tbl:searchalg}, while searching across all dimensions yields competitive results, it underperforms compared to our proposed method. A possible reason is that limiting the search to higher dimensions significantly reduces the search space, enabling a more effective discovery of the optimal solution.



\begin{figure}[htp]
	\centering
	\includegraphics[width=1\columnwidth]{mixedcontextwindow.pdf}	
	\vspace{-2ex}
	\caption{The pseudocode for mixed context window training and inference.}
	\label{fig:mixwindow}
\end{figure}

\section{Synthetic data sample}
\label{sec:needle}
\begin{center}
	%\scriptsize
	\label{fermat}
	\fontsize{8.4}{8.4} \selectfont
	\begin{tcolorbox}[width=1\textwidth, colback=lightblue, title={\textbf{Synthetic search data based on a PG19 book sample
		}}]
		
		\texttt{\textcolor{ora}{A special magic number is hidden within the following text. Make sure to memorize it. I will quiz you about the number afterwards.}} \\
		
		\texttt{\textcolor{teal}{One of the special magic numbers for numerous-kite is: 6716097.}  The Old Testament of the King James Version of the Bible The First Book of Moses: Called Genesis 1:1 In the beginning God created the heaven and the earth. 1:2 And the earth was without form, and void; and darkness was upon the face of the deep. And the Spirit of God moved upon the face of the waters.}\\
		
		......\\
		
		\texttt{it be for a witness between me and thee. 31:45 And Jacob took a stone, and set it up for a pillar. 31:46 And Jacob said unto his brethren, Gather stones; and they took stones, and made an heap: and they did eat there upon the heap. 31:47 And Laban called it Jegarsahadutha: but Jacob called it Galeed.} \\
		
		......\\
		
		\texttt{3:39 Also in the fifteenth day of the seventh month, when ye have gathered in the fruit of the land, ye shall keep a feast unto the LORD seven days: on the first day shall be a sabbath, and on the eighth day shall be a sabbath. 23:40 And ye shall take you on the first day the boughs of goodly trees, branches of palm trees, and the boughs of thick trees, and willows of the brook; and ye shall rejoice before the LORD your God seven days.}
		
		\texttt{What is the special magic number for numerous-kite mentioned in the provided text? \textcolor{ora}{The special magic number for numerous-kite mentioned in the provided text is}}
		
		
	\end{tcolorbox}
\end{center}	

