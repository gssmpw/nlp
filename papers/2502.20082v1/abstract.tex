\begin{abstract}

\sysname{} is a novel approach that extends the \emph{effective} context window of pre-trained large language models (LLMs) to the target length, while preserving the performance on the original shorter context window. 
This is achieved by three contributions: \textbf{(1)} a hypothesis that insufficient training in higher RoPE dimensions contributes to the persistent out-of-distribution (OOD) issues observed in existing methods;  
\textbf{(2)} an effective RoPE rescaling algorithm that adopts evolutionary search guided by "needle-driven" perplexity to address the insufficient training problem;  \textbf{(3)} a mixed context window training approach that fine-tunes model weights to adopt rescaled RoPE for long-context sequences while preserving the short-context performance with the original RoPE. 
Extensive experiments on LLaMA3-8B and Phi3-mini-3.8B across various benchmarks validate the hypothesis and demonstrate the effectiveness of {\sysname}. Remarkably, {\sysname} extends LLaMA3-8B to achieve a 128K \emph{effective} context length while retaining  over 98.5\% of short-context performance, using only 10B tokens -- 80x fewer than Meta's approach, which fails to reach the target effective context length. Code will be available at \url{https://github.com/microsoft/LongRoPE}.
\end{abstract}