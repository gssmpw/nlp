@inproceedings{NEURIPS2020_c8512d14,
	author = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {17283--17297},
	publisher = {Curran Associates, Inc.},
	title = {Big Bird: Transformers for Longer Sequences},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf},
	volume = {33},
	year = {2020}
}

@misc{an2024trainingfreelongcontextscalinglarge,
	title={Training-Free Long-Context Scaling of Large Language Models}, 
	author={Chenxin An and Fei Huang and Jun Zhang and Shansan Gong and Xipeng Qiu and Chang Zhou and Lingpeng Kong},
	year={2024},
	eprint={2402.17463},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2402.17463}, 
}

@misc{beltagy2020longformerlongdocumenttransformer,
	title={Longformer: The Long-Document Transformer}, 
	author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
	year={2020},
	eprint={2004.05150},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2004.05150}, 
}

@misc{chan2024rqraglearningrefinequeries,
	title={RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation}, 
	author={Chi-Min Chan and Chunpu Xu and Ruibin Yuan and Hongyin Luo and Wei Xue and Yike Guo and Jie Fu},
	year={2024},
	eprint={2404.00610},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2404.00610}, 
}

@misc{child2019generatinglongsequencessparse,
	title={Generating Long Sequences with Sparse Transformers}, 
	author={Rewon Child and Scott Gray and Alec Radford and Ilya Sutskever},
	year={2019},
	eprint={1904.10509},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/1904.10509}, 
}

@misc{ding2023longnetscalingtransformers1000000000,
	title={LongNet: Scaling Transformers to 1,000,000,000 Tokens}, 
	author={Jiayu Ding and Shuming Ma and Li Dong and Xingxing Zhang and Shaohan Huang and Wenhui Wang and Nanning Zheng and Furu Wei},
	year={2023},
	eprint={2307.02486},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2307.02486}, 
}

@misc{dong2024multiviewcontentawareindexinglong,
	title={Multi-view Content-aware Indexing for Long Document Retrieval}, 
	author={Kuicai Dong and Derrick Goh Xin Deik and Yi Quan Lee and Hao Zhang and Xiangyang Li and Cong Zhang and Yong Liu},
	year={2024},
	eprint={2404.15103},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2404.15103}, 
}

@misc{gu2024mambalineartimesequencemodeling,
	title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces}, 
	author={Albert Gu and Tri Dao},
	year={2024},
	eprint={2312.00752},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2312.00752}, 
}

@misc{guo2022longt5efficienttexttotexttransformer,
	title={LongT5: Efficient Text-To-Text Transformer for Long Sequences}, 
	author={Mandy Guo and Joshua Ainslie and David Uthus and Santiago Ontanon and Jianmo Ni and Yun-Hsuan Sung and Yinfei Yang},
	year={2022},
	eprint={2112.07916},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2112.07916}, 
}

@misc{gur2024realworldwebagentplanninglong,
	title={A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis}, 
	author={Izzeddin Gur and Hiroki Furuta and Austin Huang and Mustafa Safdari and Yutaka Matsuo and Douglas Eck and Aleksandra Faust},
	year={2024},
	eprint={2307.12856},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2307.12856},
}

@misc{gutiérrez2025hipporagneurobiologicallyinspiredlongterm,
	title={HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models}, 
	author={Bernal Jiménez Gutiérrez and Yiheng Shu and Yu Gu and Michihiro Yasunaga and Yu Su},
	year={2025},
	eprint={2405.14831},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2405.14831}, 
}

@misc{jeong2024adaptiveraglearningadaptretrievalaugmented,
	title={Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity}, 
	author={Soyeong Jeong and Jinheon Baek and Sukmin Cho and Sung Ju Hwang and Jong C. Park},
	year={2024},
	eprint={2403.14403},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2403.14403}, 
}

@misc{katharopoulos2020transformersrnnsfastautoregressive,
	title={Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention}, 
	author={Angelos Katharopoulos and Apoorv Vyas and Nikolaos Pappas and François Fleuret},
	year={2020},
	eprint={2006.16236},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2006.16236}, 
}

@misc{lee2024humaninspiredreadingagentgist,
	title={A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts}, 
	author={Kuang-Huei Lee and Xinyun Chen and Hiroki Furuta and John Canny and Ian Fischer},
	year={2024},
	eprint={2402.09727},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2402.09727}, 
}

@misc{li2024graphreaderbuildinggraphbasedagent,
	title={GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models}, 
	author={Shilong Li and Yancheng He and Hangyu Guo and Xingyuan Bu and Ge Bai and Jie Liu and Jiaheng Liu and Xingwei Qu and Yangguang Li and Wanli Ouyang and Wenbo Su and Bo Zheng},
	year={2024},
	eprint={2406.14550},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2406.14550}, 
}

@misc{lieber2024jambahybridtransformermambalanguage,
	title={Jamba: A Hybrid Transformer-Mamba Language Model}, 
	author={Opher Lieber and Barak Lenz and Hofit Bata and Gal Cohen and Jhonathan Osin and Itay Dalmedigos and Erez Safahi and Shaked Meirom and Yonatan Belinkov and Shai Shalev-Shwartz and Omri Abend and Raz Alon and Tomer Asida and Amir Bergman and Roman Glozman and Michael Gokhman and Avashalom Manevich and Nir Ratner and Noam Rozen and Erez Shwartz and Mor Zusman and Yoav Shoham},
	year={2024},
	eprint={2403.19887},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2403.19887}, 
}

@misc{luo2024bgelandmarkembeddingchunkingfree,
	title={BGE Landmark Embedding: A Chunking-Free Embedding Method For Retrieval Augmented Long-Context Large Language Models}, 
	author={Kun Luo and Zheng Liu and Shitao Xiao and Kang Liu},
	year={2024},
	eprint={2402.11573},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2402.11573}, 
}

@misc{ren2024sambasimplehybridstate,
	title={Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling}, 
	author={Liliang Ren and Yang Liu and Yadong Lu and Yelong Shen and Chen Liang and Weizhu Chen},
	year={2024},
	eprint={2406.07522},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2406.07522}, 
}

@misc{yang2024gatedlinearattentiontransformers,
	title={Gated Linear Attention Transformers with Hardware-Efficient Training}, 
	author={Songlin Yang and Bailin Wang and Yikang Shen and Rameswar Panda and Yoon Kim},
	year={2024},
	eprint={2312.06635},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2312.06635}, 
}

@misc{zhang2024chainagentslargelanguage,
	title={Chain of Agents: Large Language Models Collaborating on Long-Context Tasks}, 
	author={Yusen Zhang and Ruoxi Sun and Yanfei Chen and Tomas Pfister and Rui Zhang and Sercan O. Arik},
	year={2024},
	eprint={2406.02818},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2406.02818}, 
}

