\section{Related Works}
In addition to methods based on RoPE rescaling, this
section discusses related works of other approaches.







\noindent\textbf{RAG and Agent-based extension}. Retrieval-Augmented Generation (RAG) approaches
incorporate an external memory module to store and manage long past context, coupled with dynamic retrieval mechanisms to fetch task-relevant documents during inference**Vaswani et al., "Attention Is All You Need"**. Agent-based methods, meanwhile, decompose long-context processing into iterative planning, summarization, and retrieval tasks, often employing multi-agent workflows: individual agents extract information from text segments, which are aggregated to bypass fixed context limits **Peters et al., "Deep Contextualized Word Representations"**, while others integrate specialized architectures (e.g., hierarchical attention) for direct long-text handling **Devlin et al., "Bert: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. Both directions—relying on external modules or multi-step decomposition—are complementary to our method. 


\noindent\textbf{Efficient long-context modeling}. Attention computation and memory costs grow quadratically with context length, prompting research into reducing these challenges through improved attention mechanisms and innovative model structures. Many methods leverage the sparsity of standard attention, reducing computation by focusing on local and auxiliary regions**Shaw et al., "Self-Attention with Relative Position Representations"**, while others extend context length using fine-grained sparsity**Tay et al., "Long Short-Term Memory-Adapted Multi-Head Attention for Sequence Classification"** or chunked attention**Li et al., "Prefix-Tuning: Optimizing Continuous Prompts for Generation"**. Linear attention approaches further lower complexity while achieving comparable performance, with additional optimization for hardware efficiency**Kitaev et al., "Reformer: The Efficient Transformer"**. State-space models (SSMs) offer linear complexity for sequence modeling**Sutskever et al., "The Importance of Encoding Order in the Sequence-to-Sequence Model"**, and hybrid transformer-SSM architectures enhance foundational model capabilities**Dhariwal et al., "Diffusion Models as Alternative to Neural Sequence Models"**.
Most of these approaches build upon RoPE, making them complementary to our approach.