
\section{Conclusion}
We present {\sysname}, a method for near-lossless LLM context window extension. By addressing insufficient training of higher RoPE dimensions—a key limitation in handling OOD positional values—{\sysname} uses evolutionary search-guided rescaling and mixed context window training to achieve 128k effective context length with just 10B tokens, retaining 97.6\% of the original short-context performance. Extensive experiments on on LLaMA3-8B and Phi3-mini-3.8B demonstrates the superiority over prior art approaches. Future work will explore scaling {\sysname} toward fully lossless and infinite context window extension. 



%We identify insufficient training of higher RoPE dimensions as a key limitation  in handling OOD positional values. By addressing this through an evolutionary search-guided rescaling algorithm and mixed context window training, {\sysname}-extended LLMs achieve 128k effective context length using only 10B training tokens, while retaining  over 97.6\% of the original short-context performance. Extensive experiments on on LLaMA3-8B and Phi3-mini-3.8B demonstrates the superiority over prior art approaches. Future work will explore scaling {\sysname} toward fully lossless context window extension. 


\section*{Acknowledgement}
We sincerely thank Jianwen Zhang for his insightful discussions and valuable support in providing resources.
\section*{Impact Statement}

This work advances the field of Machine Learning by enabling LLMs to process longer contexts effectively.  {\sysname} enhances LLM capabilities for tasks like document summarization and scientific research. There are many potential societal consequences of our work,
none of which we feel must be specifically highlighted here.
