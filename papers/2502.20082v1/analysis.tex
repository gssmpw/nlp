\section{Context Window Extension and Challenges}
\label{sec:analysis}

\begin{figure*}[t]
	\centering
	\includegraphics[width=1\textwidth]{ropeexample5.pdf}	
	\vspace{-5ex}
	\caption{(a) RoPE OOD (red area) when extending context length from 2k to 4k. (b) Per-dimensional RoPE rescaling factor from different approaches for extending Phi3-mini  from 2k to 128k, all  aligning with  RoPE OOD theory. 
		(c) Performance of Phi3-mini-128k after fine-tuning. Existing methods fail to achieve an effective 128k context length and show noticeable short-context performance drop.}
	\label{fig:ropeexample}
\end{figure*}
\subsection{Preliminary}
\noindent\textbf{Rotary Position Embedding (RoPE)}. Transformer models require explicit positional information, often in the form of position embedding, to represent the order of input tokens.  Our work builds on the Rotary  Position Embedding~\citep{rope}, which is widely used in modern LLMs.  Let $m\in[0, c)$ be a position index and  $\mathbf{x_1}, ..., \mathbf{x_L}\in \mathbb{R}^{|d|}$ a sequence of vectors, where $d$ is the attention head dimension. 
Using RoPE, the self-attention first incorporates position information to the word embeddings and transforms them into query and key representations:
\begin{gather}
	\small
\label{eq:rope_qk}
	\mathbf{q}_m=f_q(\mathbf{x}_m,m);\quad f_q(\mathbf{x}_m,m)=e^{im\theta}\mathbf{W}_q\mathbf{x}_m\\
	\mathbf{k}_n=f_k(\mathbf{x}_n,n);\quad f_k(\mathbf{x}_n,n)=e^{in\theta}\mathbf{W}_k\mathbf{x}_n
\end{gather}
where $i=\sqrt{-1}$ is the imaginary unit. $\mathbf{W}_q$,$\mathbf{W}_k\in\mathbb{R}^{|d|\times|d|}$ are projection matrices. Attention weights are computed as:
\begin{equation}
	\label{eq:attention}
	\small
	softmax(\frac{\mathbf{q}_m^T \mathbf{k}_n}{\sqrt{d}})
\end{equation}
where $\mathbf{q}_m$, $\mathbf{k}_n$ are  column vectors, and $\mathbf{q}^T_m\mathbf{k}_n$ is their Euclidean inner product. Let  $\text{Re}[\cdot]$ denote the real part of a complex number, the inner product  $\mathbf{q}^T\mathbf{k}$ becomes: 
\begin{equation}
	\label{eq:attention-1}
	\small
	\begin{array}{ll}
		\mathbf{q}^T_m\mathbf{k}_n=\text{Re}\left[(\mathbf{W}_q\mathbf{x}_m)(\mathbf{W}_k\mathbf{x}_n)^*e^{i(m-n)\theta}\right]
	\end{array}
\end{equation}
where $(\mathbf{W}_k\mathbf{x}_n)^*$ is the complex conjugate of $(\mathbf{W}_k\mathbf{x}_n)$. With RoPE, attention becomes a function only dependent on the relative position $m-n$ between tokens, rather than their absolute positions. By applying Euler's formula, $e^{in\theta}$ can be expressed as trigonometric functions. Then,  RoPE encodings can be further written as a block diagonal matrix with entries of the form:
\begin{equation}
\small
	\label{eq:rope1}
f_{q,k}(n)_i=\begin{pmatrix}\text{cos}n\theta_i & -\text{sin}n\theta_i\\
	\text{sin}n\theta_i& \text{cos}n\theta_i\\
\end{pmatrix}; \theta_i={\theta_{base}}^{-2i/d} \end{equation}
where $\theta_i$ is the per-dimensional rotation angle for $i=0,1,...,d/2-1$. $\theta_{base}$ is a predefined RoPE base value, typically set to 10000 in pre-training.  


\noindent\textbf{RoPE  Per-Dimensional Period}.  Due to the periodicity of $cosine$ and $sine$ functions, RoPE is a periodic function. Specifically, for the $i^{th}$ RoPE dimension, the corresponding period length $T_i$ can be calculated as follows:
\begin{equation}
	\small 
	\label{eq:ropeperiod}
	T_{i}=\frac{2\pi}{\theta_i}
\end{equation}
The  period length of each dimension is directly determined by its rotary angle $\theta_i$. As shown in Fig.~\ref{fig:ropeexample}(a), with a fixed $\theta_{base}=10000$,  $\theta_i$  decreases as the dimensional index $i$ increases, leading to longer periods in higher RoPE dimensions. In typical cases, the periods in higher RoPE  dimensions often exceeds the pre-trained context window size, leading to incomplete periods. For instance, in Phi3-mini, the pre-trained context window size is 2048, while the period length of the highest dimension (i.e., the 48$^{th}$ $cosine$ dimension) is 51861, covering less than $4\%$ of a full period. 



 

 

\subsection{RoPE Rescaling Theory}
\label{sec:currentmethods}
Despite its effectiveness, RoPE, like other position encodings, faces challenges in context length extrapolation.  In particular, when  input sequence length exceeds the predefined context window, the perplexity can shoot up to levels comparable to completely untrained models (i.e., $>10^3$).

 
 \noindent\textbf{RoPE OOD.} Direct length extrapolation fails because longer sequences introduce untrained token positions, leading to out-of-distribution (OOD) positional values in RoPE. As shown in Fig.~\ref{fig:ropeexample}(a), the periods in high RoPE dimensions exceed the original context window size $L_{\text{train}}$. Consequently, for these dimensions,   the model does not see a full rotation period during pre-training, resulting in new untrained RoPE values at extended token positions.  For instance, in Fig.~\ref{fig:ropeexample}(a), the 40$^{th} cosine$  dimension does not complete a full period within the pre-trained  length $L_{\text{train}}$=2k. When directly extrapolated to 4k, 
  the $cosine$ values between 2k and 4k fall outside the pre-trained range,  becoming OOD RoPE values (highlighted in red). 

 

 
 \noindent\textbf{\textit{Theoretical Critical RoPE dimension}}. In contrast to higher RoPE dimensions, lower dimensions (e.g., 8$^{th}$ and 16$^{th}$ dimension in Fig.~\ref{fig:ropeexample}(a)) have seen many full periods during pretraining. As a result, there exists a  \textbf{theoretical critical dimension} (TCD) $d_{\text{tcd}}$ that divides RoPE dimensions into two groups: one with multiple full periods within the pre-trained length $L_{\text{train}}$ (i.e., $T_i <L_{\text{train}}, i< d_{\text{tcd}}$)  and another with incomplete periods (i.e., $T_i \ge L_{\text{train}},  i\ge d_{\text{tcd}}$). 
 Following~\citep{ropescale}, the critical dimension can be computed as:
 \begin{equation}
 	\label{eq:tcd}
 	d_{\text{tcd}}=2\lceil \frac{d}{2}\log_{\theta_{base}} \frac{L_{\text{train}}}{2\pi}  \rceil 
 \end{equation}
 As shown in Fig.\ref{fig:ropeexample}(a), for Phi3-mini\citep{phi3} with $d$=96, a base $\theta_{\text{base}}$=10000, and  $L_{\text{train}}=2048$, the critical dimension is 62, corresponding to the 31$^{\text{st}}$ $cosine$ dimension.
 Unless otherwise specified, we focus on the $cosine$ dimensions of RoPE (i.e., $i = 0,1,..., d/2 -1$) for simplicity.
 
 
\noindent\textbf{\textit{RoPE OOD theory}}. To address the RoPE OOD issue in long-context extension,    a straightforward approach  is to rescale the per-dimensional rotation angle $\theta_i$ and ensure higher RoPE-OOD dimensions remain within the pretrained RoPE range. This forms the widely accepted RoPE OOD theory~\citep{ropescale,pi,baichuan}. 

Formally, let the target context window size be $L$ and $\lambda_i$ be the rescaling factor for the $i^{\text{th}}$ RoPE dimension. The rescaled per-dimensional rotation angle $\hat{\theta_i}$ is then given by: 
 \begin{gather}
	\label{eq:roperescale}
	\hat\theta_i=\frac{1}{\lambda_i\times{\theta_{base}}^{2i/d}}
\end{gather}
To avoid OOD, the new rescaled periods of higher RoPE dimensions ($\hat{T}_i, i>d_{cd}$) must remain within the pretrained range, leading to the following constraint:
 \begin{gather}
	\label{eq:oodtheory}
	\frac{L}{\hat{T_i}}\le \frac{L_\text{train}}{T_i}; \rightarrow \frac{L\hat\theta_i}{2\pi} \le \frac{L_\text{train}\theta_i}{2\pi};\quad\text{for} \quad i\ge d_{\text{tcd}} \\
	\lambda_{i}\ge \frac{L}{L_\text{train}}; \quad\text{for} \quad i\ge d_{\text{tcd}}
\end{gather}
Specifically, $\frac{L}{L_\text{train}}$ is the context window extension ratio. 
The RoPE OOD theory establishes this ratio as the lower bound for scaling factors in higher RoPE dimensions beyond $d_{tcd}$.



 
 \subsection{Review of Prior RoPE Rescaling Approaches}
 Building on the RoPE OOD theory, various RoPE rescaling methods have been proposed for LLM context window extension~\citep{pi,han2023lminfinite,men2024base,dcis}. Prominent approaches, including PI, NTK, YaRN and LongRoPE, have been widely adopted to enable long context in open-source LLMs~\citep{qwen2, llama3.1, phi3}.
 

 
 

\noindent\textbf{PI} introduces linear positional interpolation, where all the RoPE dimensions use the same scale factor of $\lambda_i = \frac{L}{L_\text{train}}$. Despite its simplicity, this uniform scaling
"crowds" the positional information, making it difficult for the model to distinguish closely positioned tokens. 


 

\noindent\textbf{NTK $\theta$ Scaling} approaches RoPE from an information encoding perspective, applying the Neural Tangle Kernel (NTK) theory~\citep{jacot2018neural,tancik2020fourier}. The core idea is that neural networks are difficult to learn high-frequency features (low RoPE dimensions), and large scaling factor can affect these high-frequency positional information, leading to the loss of crucial details needed to differentiate similar closely positioned tokens.

As a result, NTK-based methods suggest increasing the original RoPE base value $\theta_{base}$ to a larger base $\theta_{ntk}$. Several methods~\citep{ntk,men2024base,ropescale} have been proposed to determine this new base value. However, some fail to align with RoPE OOD theory. For instance, ~\citep{ntk} use $\lambda_i=s^{2i/(d-2)}$, leading to insufficient interpolation and increased PPL before the target length. The approach in~\citep{ropescale}, which calculates $\theta_{ntk}$ based on the theoretical critical dimension, is the most widely adopted NTK-based method. Specifically, $\theta_{ntk}\ge\theta^{\log_{\frac{L_{\text{train}}}{2\pi}}{\frac{L}{2\pi}}}$, yielding  $\lambda_i\ge\frac{L}{L_\text{train}} (i> d_{\text{tcd}})$. Unless stated otherwise, "NTK" in this work refers to this approach.  



 


\noindent\textbf{YaRN} divides RoPE dimensions into three groups as shown in Fig.~\ref{fig:ropeexample}(b). For lower dimensions with high frequencies,  YaRN proposes no interpolation, setting  $\lambda_i=1$ to better preserve high-frequency positional information compared to NTK. For high dimensions, YaRN adopt PI and set $\lambda_i =\frac{L}{L_\text{train}}$. For dimensions that fall in-between use a linearly increasing scale factor. 


\noindent\textbf{LongRoPE}. Unlike other extension methods relying on theoretical analysis, LongRoPE  employs a PPL-guided evolutionary search to find the per-dimensional scale factor $\lambda_i$. To leverage NTK theory, it  enforces a monotonically non-decreasing scaling factor constraint during the search. 







\subsection{Challenges}
\label{sec:challenegs}

\noindent\textbf{RoPE OOD theory are insufficient}.  Fig.~\ref{fig:ropeexample}(b) compares scale factor distributions for extending Phi3-mini from 2k to 128k. 
 NTK, YaRN and LongRoPE all align the RoPE OOD with $\lambda_i\ge64$ for $i>d_{tcd}$, but yielding varied  performance (Fig.~\ref{fig:ropeexample}(c)).  NTK and LongRoPE outperforms YaRN on both  short- and long-context tasks.  We highlight two observations: \textbf{(1)} The theoretical lower bound, $\frac{L}{L_\text{train}}$, is often suboptimal.  Beyond dimension $d_{tcd}=31$, YaRN strictly adheres  to this bound ($\frac{L}{L_\text{train}}$=64), but NTK and LongRoPE use larger values to achieve much better performance. \textbf{(2)} Beyond $d_{tcd}$, larger scale factors don't always improve long-context performance. For example, in dimensions 31-48, NTK uses much larger scale factors than LongRoPE, yet LongRoPE achieves better performance. 
These findings align  with  prior works~\citep{llama3.2,baichuan,wang2024precision}, where  \textit{marginally larger} scale factors than the extension ratio empirically improve performance. 




This raises the fundamental question: \textit{In RoPE OOD theory, if RoPE periods beyond critical dimension can address OOD with  $\lambda_i =\frac{L}{L_\text{train}}$, why do slightly larger scaling factors  lead to better performance? }





\noindent\textbf{Short performance drop}.  A persistent challenge in  long context extension is performance degradation on original  short window, which poses a significant obstacle in practical LLM development.  
 A common solution is  progressively extension using  large-scale training data~\citep{llama3.1,longrecipe}. For example, LLaMA3.1~\citep{llama3.1} adopts a \textit{SIX-stage} extension process requiring 800B tokens to  extend from 8k to 128k, greatly increasing training complexity and costs.  Though LongRoPE introduces a training-free short scaling factor, it fails to fully address the performance drop (Figure~\ref{fig:ropeexample}(c)). As a result, bridging this gap remains an unresolved challenge. 
 


