\section{Related Works}
In addition to methods based on RoPE rescaling, this
section discusses related works of other approaches.







\noindent\textbf{RAG and Agent-based extension}. Retrieval-Augmented Generation (RAG) approaches
incorporate an external memory module to store and manage long past context, coupled with dynamic retrieval mechanisms to fetch task-relevant documents during inference____. Agent-based methods, meanwhile, decompose long-context processing into iterative planning, summarization, and retrieval tasks, often employing multi-agent workflows: individual agents extract information from text segments, which are aggregated to bypass fixed context limits ____, while others integrate specialized architectures (e.g., hierarchical attention) for direct long-text handling ____. Both directions—relying on external modules or multi-step decomposition—are complementary to our method. 


\noindent\textbf{Efficient long-context modeling}. Attention computation and memory costs grow quadratically with context length, prompting research into reducing these challenges through improved attention mechanisms and innovative model structures. Many methods leverage the sparsity of standard attention, reducing computation by focusing on local and auxiliary regions____, while others extend context length using fine-grained sparsity____ or chunked attention____. Linear attention approaches further lower complexity while achieving comparable performance, with additional optimization for hardware efficiency____. State-space models (SSMs) offer linear complexity for sequence modeling____, and hybrid transformer-SSM architectures enhance foundational model capabilities____.
Most of these approaches build upon RoPE, making them complementary to our approach.