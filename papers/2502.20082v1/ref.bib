@inproceedings{top,
	author = {Li, Junyan and Zhang, Li Lyna and Xu, Jiahang and Wang, Yujing and Yan, Shaoguang and Xia, Yunqing and Yang, Yuqing and Cao, Ting and Sun, Hao and Deng, Weiwei and Zhang, Qi and Yang, Mao},
	title = {Constraint-Aware and Ranking-Distilled Token Pruning for Efficient Transformer Inference},
	year = {2023},
	booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
	pages = {1280–1290},
	series = {KDD '23}
}
@inproceedings{ltp,
	author = {Kim, Sehoon and Shen, Sheng and Thorsley, David and Gholami, Amir and Kwon, Woosuk and Hassoun, Joseph and Keutzer, Kurt},
	title = {Learned Token Pruning for Transformers},
	year = {2022},
	isbn = {9781450393850},
	publisher = {Association for Computing Machinery},
	booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
	pages = {784–794},
	numpages = {11},
	keywords = {deep learning, natural language processing, network pruning},
	location = {Washington DC, USA},
	series = {KDD '22}
}

@inproceedings{transkimmer,
	title = {Transkimmer: Transformer Learns to Layer-wise Skim},
	author = {Guan, Yue  and
	Li, Zhengyi  and
	Leng, Jingwen  and
	Lin, Zhouhan  and
	Guo, Minyi},
	booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	year = {2022},
	publisher = {Association for Computational Linguistics},
	pages = {7275--7286},
	
}
@inproceedings{swiftpruner,
	title={SwiftPruner: Reinforced Evolutionary Pruning for Efficient Ad Relevance},
	author={Zhang, Li Lyna and Homma, Youkow and Wang, Yujing and Wu, Min and Yang, Mao and Zhang, Ruofei and Cao, Ting and Shen, Wei},
	booktitle={Proceedings of the 31st ACM International Conference on Information \& Knowledge Management},
	pages={3654--3663},
	year={2022}
}
@inproceedings{cofi,
	title={Structured Pruning Learns Compact and Accurate Models},
	author={Xia, Mengzhou and Zhong, Zexuan and Chen, Danqi},
	booktitle={Association for Computational Linguistics (ACL)},
	year={2022}
}
@inproceedings{sanh2020distilbert,
	title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter}, 
	author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
	year={2020},
	eprint={1910.01108},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@misc{jiao2020tinybert,
	title={TinyBERT: Distilling BERT for Natural Language Understanding}, 
	author={Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},
	year={2020},
	booktitle={EMNLP}
}

@article{zafrir2019q8bert,
	title={Q8bert: Quantized 8bit bert},
	author={Zafrir, Ofir and Boudoukh, Guy and Izsak, Peter and Wasserblat, Moshe},
	journal={arXiv preprint arXiv:1910.06188},
	year={2019}
}
@inproceedings{movement,
	title={Movement pruning: Adaptive sparsity by fine-tuning},
	author={Sanh, Victor and Wolf, Thomas and Rush, Alexander M},
	booktitle={NeurIPS},
	year={2020}
}
@misc{gordon2020compressing,
	title={Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning}, 
	author={Mitchell A. Gordon and Kevin Duh and Nicholas Andrews},
	year={2020},
	eprint={2002.08307},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@inproceedings{shen2020q,
	title={Q-bert: Hessian based ultra low precision quantization of bert},
	author={Shen, Sheng and Dong, Zhen and Ye, Jiayu and Ma, Linjian and Yao, Zhewei and Gholami, Amir and Mahoney, Michael W and Keutzer, Kurt},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	year={2020}
}
@article{kim2021bert,
	title={I-bert: Integer-only bert quantization},
	author={Kim, Sehoon and Gholami, Amir and Yao, Zhewei and Mahoney, Michael W and Keutzer, Kurt},
	journal={arXiv preprint arXiv:2101.01321},
	year={2021}
}
@article{bert,
	title={Bert: Pre-training of deep bidirectional transformers for language understanding},
	author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	journal={arXiv preprint arXiv:1810.04805},
	year={2018}
}
@article{albert,
	title={Albert: A lite bert for self-supervised learning of language representations},
	author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
	journal={arXiv preprint arXiv:1909.11942},
	year={2019}
}
@inproceedings{nn_pruning,
	author = {Francois Lagunas and Ella Charlaix and Victor Sanh and Alexander M. Rush},
	title = {Block Pruning For Faster Transformers},
	year = {2021},
	
	booktitle = {EMNLP},
	
}
@article{roberta,
	title={Roberta: A robustly optimized bert pretraining approach},
	author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	journal={arXiv preprint arXiv:1907.11692},
	year={2019}
}
@article{t5,
	author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
	title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
	journal = {Journal of Machine Learning Research},
	year    = {2020},
	volume  = {21},
	number  = {140},
	pages   = {1-67},
	url     = {http://jmlr.org/papers/v21/20-074.html}
}
@inproceedings{llmpruner,
	title={LLM-Pruner: On the Structural Pruning of Large Language Models},
	author={Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
	journal={arXiv preprint arXiv:2305.11627},
	year={2023}
}
@article{loraprune,
	title={Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning},
	author={Zhang, Mingyang and Shen, Chunhua and Yang, Zhen and Ou, Linlin and Yu, Xinyi and Zhuang, Bohan and others},
	journal={arXiv preprint arXiv:2305.18403},
	year={2023}
}
@article{sparsegpt,
	title={{SparseGPT}: Massive Language Models Can Be Accurately Pruned in One-Shot}, 
	author={Elias Frantar and Dan Alistarh},
	year={2023},
	journal={arXiv preprint arXiv:2301.00774}
}

@article{wanda,
	title={A Simple and Effective Pruning Approach for Large Language Models},
	author={Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico},
	journal={arXiv preprint arXiv:2306.11695},
	year={2023}
}
@misc{gpt4,
	title={GPT-4 Technical Report}, 
 author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
	year={2023},
	eprint={2303.08774},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@inproceedings{lora,
	title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
	author={Edward J Hu and yelong shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
	booktitle={International Conference on Learning Representations},
	year={2022},
	url={https://openreview.net/forum?id=nZeVKeeFYf9}
}


@misc{llama,
	title={LLaMA: Open and Efficient Foundation Language Models}, 
	author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
	year={2023},
	eprint={2302.13971},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@misc{llama2,
	title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
	author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
	year={2023},
	eprint={2307.09288},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@article{transformer,
	title={Attention is all you need},
	author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
	journal={Advances in neural information processing systems},
	volume={30},
	year={2017}
}

@article{ste,
	title={Estimating or propagating gradients through stochastic neurons for conditional computation},
	author={Bengio, Yoshua and L{\'e}onard, Nicholas and Courville, Aaron},
	journal={arXiv preprint arXiv:1308.3432},
	year={2013}
}
@article{wei2021finetuned,
	title={Finetuned language models are zero-shot learners},
	author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
	journal={arXiv preprint arXiv:2109.01652},
	year={2021}
}
@inproceedings{
	sanh2022multitask,
	title={Multitask Prompted Training Enables Zero-Shot Task Generalization},
	author={Victor Sanh and Albert Webson and Colin Raffel and Stephen Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal Nayak and Debajyoti Datta and Jonathan Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault Fevry and Jason Alan Fries and Ryan Teehan and Teven Le Scao and Stella Biderman and Leo Gao and Thomas Wolf and Alexander M Rush},
	booktitle={International Conference on Learning Representations},
	year={2022},
}
@article{peng2023instruction,
	title={Instruction tuning with gpt-4},
	author={Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
	journal={arXiv preprint arXiv:2304.03277},
	year={2023}
}
@article{instructgpt,
	title={Training language models to follow instructions with human feedback},
	author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
	journal={Advances in Neural Information Processing Systems},
	volume={35},
	pages={27730--27744},
	year={2022}
}
@article{compressprompt,
	title={Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt},
	author={Xu, Zhaozhuo and Liu, Zirui and Chen, Beidi and Tang, Yuxin and Wang, Jue and Zhou, Kaixiong and Hu, Xia and Shrivastava, Anshumali},
	journal={arXiv preprint arXiv:2305.11186},
	year={2023}
}
@article{goyal2022news,
	title={News summarization and evaluation in the era of gpt-3},
	author={Goyal, Tanya and Li, Junyi Jessy and Durrett, Greg},
	journal={arXiv preprint arXiv:2209.12356},
	year={2022}
}
@article{cot,
	title={Chain-of-thought prompting elicits reasoning in large language models},
	author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
	journal={Advances in Neural Information Processing Systems},
	volume={35},
	pages={24824--24837},
	year={2022}
}
@inproceedings{schick-schutze-2021-exploiting,
	title = {Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference},
	author = {Schick, Timo  and
	Sch{\"u}tze, Hinrich},
	booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
	year = {2021},
}
@article{chowdhery2022palm,
	title={Palm: Scaling language modeling with pathways},
	author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
	journal={arXiv preprint arXiv:2204.02311},
	year={2022}
}
@article{chung2022scaling,
	title={Scaling instruction-finetuned language models},
	author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
	journal={arXiv preprint arXiv:2210.11416},
	year={2022}
}
@misc{alpaca,
	author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
	title = {Stanford Alpaca: An Instruction-following LLaMA model},
	year = {2023},
	publisher = {GitHub},
	journal = {GitHub repository},
	howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}
@article{selfinstruct,
	title={Self-instruct: Aligning language model with self generated instructions},
	author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
	journal={arXiv preprint arXiv:2212.10560},
	year={2022}
}
@misc{vicuna,
	title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
	url = {https://lmsys.org/blog/2023-03-30-vicuna/},
	author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
	month = {March},
	year = {2023}
}
@article{liu2023llm,
	title={LLM-QAT: Data-Free Quantization Aware Training for Large Language Models},
	author={Liu, Zechun and Oguz, Barlas and Zhao, Changsheng and Chang, Ernie and Stock, Pierre and Mehdad, Yashar and Shi, Yangyang and Krishnamoorthi, Raghuraman and Chandra, Vikas},
	journal={arXiv preprint arXiv:2305.17888},
	year={2023}
}

@article{c4,
	author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
	title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
	journal = {arXiv e-prints},
	year = {2019},
	archivePrefix = {arXiv},
	eprint = {1910.10683},
}
@article{frantar-gptq,
	title={{GPTQ}: Accurate Post-training Compression for Generative Pretrained Transformers}, 
	author={Elias Frantar and Saleh Ashkboos and Torsten Hoefler and Dan Alistarh},
	year={2022},
	journal={arXiv preprint arXiv:2210.17323}
}
@article{yao2022zeroquant,
	title={Zeroquant: Efficient and affordable post-training quantization for large-scale transformers},
	author={Yao, Zhewei and Yazdani Aminabadi, Reza and Zhang, Minjia and Wu, Xiaoxia and Li, Conglong and He, Yuxiong},
	journal={Advances in Neural Information Processing Systems},
	volume={35},
	pages={27168--27183},
	year={2022}
}
@inproceedings{xiao2023smoothquant,
	title={Smoothquant: Accurate and efficient post-training quantization for large language models},
	author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
	booktitle={International Conference on Machine Learning},
	pages={38087--38099},
	year={2023},
	organization={PMLR}
}
@article{llama,
	title={Llama: Open and efficient foundation language models},
	author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
	journal={arXiv preprint arXiv:2302.13971},
	year={2023}
}
@article{zhao2023survey,
	title={A survey of large language models},
	author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
	journal={arXiv preprint arXiv:2303.18223},
	year={2023}
}
@article{huang2022towards,
	title={Towards reasoning in large language models: A survey},
	author={Huang, Jie and Chang, Kevin Chen-Chuan},
	journal={arXiv preprint arXiv:2212.10403},
	year={2022}
}
@article{chang2023survey,
	title={A survey on evaluation of large language models},
	author={Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Zhu, Kaijie and Chen, Hao and Yang, Linyi and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others},
	journal={arXiv preprint arXiv:2307.03109},
	year={2023}
}
@inproceedings{gpt3,
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	booktitle = {Advances in Neural Information Processing Systems},
	title = {Language Models are Few-Shot Learners},
	volume = {33},
	year = {2020}
}
@article{opt,
	title={Opt: Open pre-trained transformer language models},
	author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
	journal={arXiv preprint arXiv:2205.01068},
	year={2022}
}
@article{palm,
	title={Palm: Scaling language modeling with pathways},
	author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
	journal={arXiv preprint arXiv:2204.02311},
	year={2022}
}
@article{bloom,
	title={Bloom: A 176b-parameter open-access multilingual language model},
	author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
	journal={arXiv preprint arXiv:2211.05100},
	year={2022}
}
@article{xu2023compress,
	title={Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt},
	author={Xu, Zhaozhuo and Liu, Zirui and Chen, Beidi and Tang, Yuxin and Wang, Jue and Zhou, Kaixiong and Hu, Xia and Shrivastava, Anshumali},
	journal={arXiv preprint arXiv:2305.11186},
	year={2023}
}
@inproceedings{lagrangian,
	title = {Structured Pruning of Large Language Models},
	author = {Wang, Ziheng  and	Wohlwend, Jeremy  and	Lei, Tao},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	year = {2020},
}

@inproceedings{l0,
	title={Learning Sparse Neural Networks through $L_0$ Regularization},
	author={Christos Louizos and Max Welling and Diederik P. Kingma},
	booktitle={International Conference on Learning Representations},
	year={2018}
}
@article{hu2023llm,
	title={LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models},
	author={Hu, Zhiqiang and Lan, Yihuai and Wang, Lei and Xu, Wanyu and Lim, Ee-Peng and Lee, Roy Ka-Wei and Bing, Lidong and Poria, Soujanya},
	journal={arXiv preprint arXiv:2304.01933},
	year={2023}
}
@article{dosovitskiy2020image,
	title={An image is worth 16x16 words: Transformers for image recognition at scale},
	author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
	journal={arXiv preprint arXiv:2010.11929},
	year={2020}
}
@article{gao2023llama,
	title={Llama-adapter v2: Parameter-efficient visual instruction model},
	author={Gao, Peng and Han, Jiaming and Zhang, Renrui and Lin, Ziyi and Geng, Shijie and Zhou, Aojun and Zhang, Wei and Lu, Pan and He, Conghui and Yue, Xiangyu and others},
	journal={arXiv preprint arXiv:2304.15010},
	year={2023}
}
@misc{dong2023large,
	title={Large Language Model for Science: A Study on P vs. NP}, 
	author={Qingxiu Dong and Li Dong and Ke Xu and Guangyan Zhou and Yaru Hao and Zhifang Sui and Furu Wei},
	year={2023},
	eprint={2309.05689},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@article{race,
	title={RACE: Large-scale ReAding Comprehension Dataset From Examinations},
	author={Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},
	journal={arXiv preprint arXiv:1704.04683},
	year={2017}
}
@article{mmlu,
	title={Measuring massive multitask language understanding},
	author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
	journal={arXiv preprint arXiv:2009.03300},
	year={2020}
}
@article{bigbench,
	title={Challenging big-bench tasks and whether chain-of-thought can solve them},
	author={Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V and Chi, Ed H and Zhou, Denny and others},
	journal={arXiv preprint arXiv:2210.09261},
	year={2022}
}
@inproceedings{storycloze,
	title={Lsdsem 2017 shared task: The story cloze test},
	author={Mostafazadeh, Nasrin and Roth, Michael and Louis, Annie and Chambers, Nathanael and Allen, James},
	booktitle={Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics},
	pages={46--51},
	year={2017}
}
@inproceedings{piqa,
	author = {Yonatan Bisk and Rowan Zellers and
	Ronan Le Bras and Jianfeng Gao
	and Yejin Choi},
	title = {PIQA: Reasoning about Physical Commonsense in
	Natural Language},
	booktitle = {Thirty-Fourth AAAI Conference on
	Artificial Intelligence},
	year = {2020},
}
@inproceedings{zellers2019hellaswag,
	title={HellaSwag: Can a Machine Really Finish Your Sentence?},
	author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
	booktitle ={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	year={2019}
}
@InProceedings{ai2:winogrande,
	title = {WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
	authors={Keisuke, Sakaguchi and Ronan, Le Bras and Chandra, Bhagavatula and Yejin, Choi
	},
	year={2019}
}
@article{allenai:arc,
	author    = {Peter Clark  and Isaac Cowhey and Oren Etzioni and Tushar Khot and
	Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
	title     = {Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
	journal   = {arXiv:1803.05457v1},
	year      = {2018},
}
@inproceedings{OpenBookQA2018,
	title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
	author={Todor Mihaylov and Peter Clark and Tushar Khot and Ashish Sabharwal},
	booktitle={EMNLP},
	year={2018}
}
@inproceedings{clark2019boolq,
	title =     {BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},
	author =    {Clark, Christopher and Lee, Kenton and Chang, Ming-Wei, and Kwiatkowski, Tom and Collins, Michael, and Toutanova, Kristina},
	booktitle = {NAACL},
	year =      {2019},
}
@software{eval-harness,
	author       = {Gao, Leo and
	Tow, Jonathan and
	Biderman, Stella and
	Black, Sid and
	DiPofi, Anthony and
	Foster, Charles and
	Golding, Laurence and
	Hsu, Jeffrey and
	McDonell, Kyle and
	Muennighoff, Niklas and
	Phang, Jason and
	Reynolds, Laria and
	Tang, Eric and
	Thite, Anish and
	Wang, Ben and
	Wang, Kevin and
	Zou, Andy},
	title        = {A framework for few-shot language model evaluation},
	month        = sep,
	year         = 2021,
	publisher    = {Zenodo},
	version      = {v0.0.1},
	doi          = {10.5281/zenodo.5371628},
	url          = {https://doi.org/10.5281/zenodo.5371628}
}
@article{chia2023instructeval,
	title={INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models},
	author={Chia, Yew Ken and Hong, Pengfei and Bing, Lidong and Poria, Soujanya},
	journal={arXiv preprint arXiv:2306.04757},
	year={2023}
}
@inproceedings{srinivas2022cyclical,
	title={Cyclical pruning for sparse neural networks},
	author={Srinivas, Suraj and Kuzmin, Andrey and Nagel, Markus and van Baalen, Mart and Skliar, Andrii and Blankevoort, Tijmen},
	booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages={2762--2771},
	year={2022}
}
@inproceedings{
	pool2021channel,
	title={Channel Permutations for N:M Sparsity},
	author={Jeff Pool and Chong Yu},
	booktitle={Advances in Neural Information Processing Systems},
	editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
	year={2021},
	url={https://openreview.net/forum?id=WAO1STUPWPP}
}
@article{hubara2021accelerated,
	title={Accelerated sparse neural training: A provable and efficient method to find n: m transposable masks},
	author={Hubara, Itay and Chmiel, Brian and Island, Moshe and Banner, Ron and Naor, Joseph and Soudry, Daniel},
	journal={Advances in neural information processing systems},
	volume={34},
	pages={21099--21111},
	year={2021}
}
@article{wang2022self,
  title={Self-instruct: Aligning language model with self generated instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}

@article{zhou2023solving,
	title={Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification},
	author={Zhou, Aojun and Wang, Ke and Lu, Zimu and Shi, Weikang and Luo, Sichun and Qin, Zipeng and Lu, Shaoqing and Jia, Anya and Song, Linqi and Zhan, Mingjie and others},
	journal={arXiv preprint arXiv:2308.07921},
	year={2023}
}

@article{longllama,
	title={Focused Transformer: Contrastive Training for Context Scaling}, 
	author={Szymon Tworkowski and Konrad Staniszewski and Mikołaj Pacek and Yuhuai Wu and Henryk Michalewski and Piotr Miłoś},
	year={2023},
	eprint={2307.03170},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@article{wang2023augmenting,
	title={Augmenting Language Models with Long-Term Memory},
	author={Wang, Weizhi and Dong, Li and Cheng, Hao and Liu, Xiaodong and Yan, Xifeng and Gao, Jianfeng and Wei, Furu},
	journal={arXiv preprint arXiv:2306.07174},
	year={2023}
}
@article{han2023lminfinite,
	title={LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models},
	author={Han, Chi and Wang, Qifan and Xiong, Wenhan and Chen, Yu and Ji, Heng and Wang, Sinong},
	journal={arXiv preprint arXiv:2308.16137},
	year={2023}
}

@article{streamingllm,
	title={Efficient Streaming Language Models with Attention Sinks},
	author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
	journal={arXiv},
	year={2023}
}
@article{dao2023flashattention2,
	title={Flash{A}ttention-2: Faster Attention with Better Parallelism and Work Partitioning},
	author={Dao, Tri},
	year={2023}
}
@misc{ntk,
	title={Ntk-aware scaled rope allows llama models to have extended (8k+) context size without any fine-tuning and minimal perplexity degration},
	author={LocalLLaMA},
	year={2023},
	url={{https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/}}
}

@misc{dynamicntk,
	title={Dynamically scaled rope further increases performance of long context LLaMA with zero fine-tuning},
	author={LocalLLaMA},
	year={2023},
url={{https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/}}
}
@article{yarn,
	title={Yarn: Efficient context window extension of large language models},
	author={Peng, Bowen and Quesnelle, Jeffrey and Fan, Honglu and Shippole, Enrico},
	journal={arXiv preprint arXiv:2309.00071},
	year={2023}
}
@article{pi,
	title={Extending context window of large language models via positional interpolation},
	author={Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
	journal={arXiv preprint arXiv:2306.15595},
	year={2023}
}

@article{rope,
	title={Roformer: Enhanced transformer with rotary position embedding},
	author={Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
	journal={arXiv preprint arXiv:2104.09864},
	year={2021}
}

@article{alibi,
	title={Train short, test long: Attention with linear biases enables input length extrapolation},
	author={Press, Ofir and Smith, Noah A and Lewis, Mike},
	journal={arXiv preprint arXiv:2108.12409},
	year={2021}
}
@article{haviv2022transformer,
	title={Transformer language models without positional encodings still learn positional information},
	author={Haviv, Adi and Ram, Ori and Press, Ofir and Izsak, Peter and Levy, Omer},
	journal={arXiv preprint arXiv:2203.16634},
	year={2022}
}
@article{sun2022length,
	title={A length-extrapolatable transformer},
	author={Sun, Yutao and Dong, Li and Patra, Barun and Ma, Shuming and Huang, Shaohan and Benhaim, Alon and Chaudhary, Vishrav and Song, Xia and Wei, Furu},
	journal={arXiv preprint arXiv:2212.10554},
	year={2022}
}

@inproceedings{borgeaud2022improving,
	title={Improving language models by retrieving from trillions of tokens},
	author={Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others},
	booktitle={International conference on machine learning},
	pages={2206--2240},
	year={2022},
	organization={PMLR}
}

@article{parallelwindow,
	title={Parallel context windows improve in-context learning of large language models},
	author={Ratner, Nir and Levine, Yoav and Belinkov, Yonatan and Ram, Ori and Abend, Omri and Karpas, Ehud and Shashua, Amnon and Leyton-Brown, Kevin and Shoham, Yoav},
	journal={arXiv preprint arXiv:2212.10947},
	year={2022}
}

@article{tancik2020fourier,
	title={Fourier features let networks learn high frequency functions in low dimensional domains},
	author={Tancik, Matthew and Srinivasan, Pratul and Mildenhall, Ben and Fridovich-Keil, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan and Ng, Ren},
	journal={Advances in Neural Information Processing Systems},
	volume={33},
	pages={7537--7547},
	year={2020}
}

@article{jacot2018neural,
	title={Neural tangent kernel: Convergence and generalization in neural networks},
	author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
	journal={Advances in neural information processing systems},
	volume={31},
	year={2018}
}

@inproceedings{spos,
	title={Single path one-shot neural architecture search with uniform sampling},
	author={Guo, Zichao and Zhang, Xiangyu and Mu, Haoyuan and Heng, Wen and Liu, Zechun and Wei, Yichen and Sun, Jian},
	booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XVI 16},
	pages={544--560},
	year={2020},
	organization={Springer}
}

@article{longlora,
	title={LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models},
	author={Yukang Chen and Shengju Qian and Haotian Tang and Xin Lai and Zhijian Liu and Song Han and Jiaya Jia},
	journal={arXiv:2309.12307},
	year={2023}
}

@misc{mistral,
	title={Mistral 7B}, 
	author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
	year={2023},
	eprint={2310.06825},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@article{phi,
	title={Textbooks Are All You Need},
	author={Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio C{\'e}sar Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and de Rosa, Gustavo and Saarikivi, Olli and others},
	journal={arXiv preprint arXiv:2306.11644},
	year={2023}
}
@article{textbooks2,
	title={Textbooks Are All You Need II: \textbf{phi-1.5} technical report},
	author={Li, Yuanzhi and Bubeck, S{\'e}bastien and Eldan, Ronen and Del Giorno, Allie and Gunasekar, Suriya and Lee, Yin Tat},
	journal={arXiv preprint arXiv:2309.05463},
	year={2023}
}
@article{simonyan2014very,
	title={Very deep convolutional networks for large-scale image recognition},
	author={Simonyan, Karen and Zisserman, Andrew},
	journal={arXiv preprint arXiv:1409.1556},
	year={2014}
}
@article{rogers2021primer,
	title={A primer in BERTology: What we know about how BERT works},
	author={Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
	journal={Transactions of the Association for Computational Linguistics},
	volume={8},
	pages={842--866},
	year={2021},
	publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{liu2023scaling,
	title={Scaling Laws of RoPE-based Extrapolation},
	author={Liu, Xiaoran and Yan, Hang and Zhang, Shuo and An, Chenxin and Qiu, Xipeng and Lin, Dahua},
	journal={arXiv preprint arXiv:2310.05209},
	year={2023}
}

@misc{xiong2023effective,
	title={Effective Long-Context Scaling of Foundation Models}, 
	author={Wenhan Xiong and Jingyu Liu and Igor Molybog and Hejia Zhang and Prajjwal Bhargava and Rui Hou and Louis Martin and Rashi Rungta and Karthik Abinav Sankararaman and Barlas Oguz and Madian Khabsa and Han Fang and Yashar Mehdad and Sharan Narang and Kshitiz Malik and Angela Fan and Shruti Bhosale and Sergey Edunov and Mike Lewis and Sinong Wang and Hao Ma},
	year={2023},
	eprint={2309.16039},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@article{huang2023boosting,
	title={Boosting LLM Reasoning: Push the Limits of Few-shot Learning with Reinforced In-Context Pruning},
	author={Huang, Xijie and Zhang, Li Lyna and Cheng, Kwang-Ting and Yang, Mao},
	journal={arXiv preprint arXiv:2312.08901},
	year={2023}
}
@article{fei2023extending,
	title={Extending Context Window of Large Language Models via Semantic Compression},
	author={Fei, Weizhi and Niu, Xueyan and Zhou, Pingyi and Hou, Lu and Bai, Bo and Deng, Lei and Han, Wei},
	journal={arXiv preprint arXiv:2312.09571},
	year={2023}
}

@article{ge2023context,
	title={In-context Autoencoder for Context Compression in a Large Language Model},
	author={Ge, Tao and Hu, Jing and Wang, Xun and Chen, Si-Qing and Wei, Furu},
	journal={arXiv preprint arXiv:2307.06945},
	year={2023}
}

@misc{zhu2023pose,
	title={PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training}, 
	author={Dawei Zhu and Nan Yang and Liang Wang and Yifan Song and Wenhao Wu and Furu Wei and Sujian Li},
	year={2023},
	eprint={2309.10400},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}


@misc{codellama,
	title={Code Llama: Open Foundation Models for Code}, 
	author={Baptiste Rozière and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Ellen Tan and Yossi Adi and Jingyu Liu and Tal Remez and Jérémy Rapin and Artyom Kozhevnikov and Ivan Evtimov and Joanna Bitton and Manish Bhatt and Cristian Canton Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre Défossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},
	year={2023},
	eprint={2308.12950},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@article{weng2023prompt,
	title   = {LLM-powered Autonomous Agents},
	author  = {Weng, Lilian},
	journal = {lilianweng.github.io},
	year    = {2023},
	month   = {Jun},
	url     = {https://lilianweng.github.io/posts/2023-06-23-agent/}"
}

@inproceedings{park2023generative,
	title={Generative agents: Interactive simulacra of human behavior},
	author={Park, Joon Sung and O'Brien, Joseph and Cai, Carrie Jun and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S},
	booktitle={Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
	pages={1--22},
	year={2023}
}
@article{cube,
	title={SuperScaler: Supporting Flexible DNN Parallelization via a Unified Abstraction},
	author={Lin, Zhiqi and Miao, Youshan and Liu, Guodong and Shi, Xiaoxiang and Zhang, Quanlu and Yang, Fan and Maleki, Saeed and Zhu, Yi and Cao, Xu and Li, Cheng and others},
	journal={arXiv preprint arXiv:2301.08984},
	year={2023}
}

@inproceedings{guo2020single,
	title={Single path one-shot neural architecture search with uniform sampling},
	author={Guo, Zichao and Zhang, Xiangyu and Mu, Haoyuan and Heng, Wen and Liu, Zechun and Wei, Yichen and Sun, Jian},
	booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XVI 16},
	pages={544--560},
	year={2020},
	organization={Springer}
}
@article{pg19,
	author = {Rae, Jack W and Potapenko, Anna and Jayakumar, Siddhant M and
	Hillier, Chloe and Lillicrap, Timothy P},
	title = {Compressive Transformers for Long-Range Sequence Modelling},
	journal = {arXiv preprint},
	url = {https://arxiv.org/abs/1911.05507},
	year = {2019},
}
@misc{proof-pile,
	author={Zhangir Azerbayev and Edward Ayers and Bartosz Piotrowski},
	title={Proof-pile},
	url={https://github.com/zhangir-azerbayev/ProofNet},
	year={2022},

}

@article{jiang2024mixtral,
	title={Mixtral of Experts},
	author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
	journal={arXiv preprint arXiv:2401.04088},
	year={2024}
}
@article{zhang2024soaring,
	title={Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon},
	author={Zhang, Peitian and Liu, Zheng and Xiao, Shitao and Shao, Ninglu and Ye, Qiwei and Dou, Zhicheng},
	journal={arXiv preprint arXiv:2401.03462},
	year={2024}
}
@software{redpajama,
	author = {Together Computer},
	title = {RedPajama: An Open Source Recipe to Reproduce LLaMA training dataset},
	month = April,
	year = 2023,
	url = {https://github.com/togethercomputer/RedPajama-Data}
}
@misc{mistraldata,
	author={},
	title={Long-data collections},
	url={https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T},
	year={2024},
	
}

@misc{mistrallite,
	author={Amazon},
	title={MistralLite},
	url={https://huggingface.co/amazon/MistralLite},
	year={2023},
	
}

@misc{huggingfaceboard,
	author={Hugging Face},
	title={Open LLM Leaderboard},
	url={https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard},
	year={2024},
	
}
@misc{books,
	author={Shawn Presser},
	url={https://twitter.com/theshawwn/status/1320282149329784833},
	year={2020},
	
}
@misc{together,
	author={Together},
	url={https://huggingface.co/togethercomputer/LLaMA-2-7B-32K},
	year={2023},
	
}

@article{mmlu,
	title={Measuring massive multitask language understanding},
	author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
	journal={arXiv preprint arXiv:2009.03300},
	year={2020}
}
@inproceedings{zellers2019hellaswag,
	title={HellaSwag: Can a Machine Really Finish Your Sentence?},
	author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
	booktitle ={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	year={2019}
}
@article{allenai:arc,
	author    = {Peter Clark  and Isaac Cowhey and Oren Etzioni and Tushar Khot and
	Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
	title     = {Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
	journal   = {arXiv:1803.05457v1},
	year      = {2018},
}
@article{pile,
	title={The {P}ile: An 800GB Dataset of Diverse Text for Language Modeling},
	author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
	journal={arXiv preprint arXiv:2101.00027},
	year={2020}
}
@article{govreport,
	title={Efficient attentions for long document summarization},
	author={Huang, Luyang and Cao, Shuyang and Parulian, Nikolaus and Ji, Heng and Wang, Lu},
	journal={arXiv preprint arXiv:2104.02112},
	year={2021}
}
@article{lin2021truthfulqa,
	title={Truthfulqa: Measuring how models mimic human falsehoods},
	author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
	journal={arXiv preprint arXiv:2109.07958},
	year={2021}
}
@article{passkey,
	title={Landmark Attention: Random-Access Infinite Context Length for Transformers},
	author={Mohtashami, Amirkeivan and Jaggi, Martin},
	journal={arXiv preprint arXiv:2305.16300},
	year={2023}
}
@misc{madaan2023selfrefine,
	title={Self-Refine: Iterative Refinement with Self-Feedback}, 
	author={Aman Madaan and Niket Tandon and Prakhar Gupta and Skyler Hallinan and Luyu Gao and Sarah Wiegreffe and Uri Alon and Nouha Dziri and Shrimai Prabhumoye and Yiming Yang and Shashank Gupta and Bodhisattwa Prasad Majumder and Katherine Hermann and Sean Welleck and Amir Yazdanbakhsh and Peter Clark},
	year={2023},
	eprint={2303.17651},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@article{men2024base,
	title={Base of RoPE Bounds Context Length},
	author={Men, Xin and Xu, Mingyu and Wang, Bingning and Zhang, Qingyu and Lin, Hongyu and Han, Xianpei and Chen, Weipeng},
	journal={arXiv preprint arXiv:2405.14591},
	year={2024}
}
@article{longrope,
	title={Longrope: Extending llm context window beyond 2 million tokens},
	author={Ding, Yiran and Zhang, Li Lyna and Zhang, Chengruidong and Xu, Yuanyuan and Shang, Ning and Xu, Jiahang and Yang, Fan and Yang, Mao},
	journal={arXiv preprint arXiv:2402.13753},
	year={2024}
}

@article{llama3.1,
	title={The Llama 3 Herd of Models}, 

author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
	year={2024},
	eprint={2407.21783},
	archivePrefix={arXiv},
	primaryClass={cs.AI},
	url={https://arxiv.org/abs/2407.21783}, 
}

@misc{qwen2,
	title={Qwen2 Technical Report}, 
	author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jianxin Yang and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Xuejing Liu and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhifang Guo and Zhihao Fan},
	year={2024},
	eprint={2407.10671},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2407.10671}, 
}

@misc{chatglm,
	title={ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools}, 
	author={Team GLM and : and Aohan Zeng and Bin Xu and Bowen Wang and Chenhui Zhang and Da Yin and Dan Zhang and Diego Rojas and Guanyu Feng and Hanlin Zhao and Hanyu Lai and Hao Yu and Hongning Wang and Jiadai Sun and Jiajie Zhang and Jiale Cheng and Jiayi Gui and Jie Tang and Jing Zhang and Jingyu Sun and Juanzi Li and Lei Zhao and Lindong Wu and Lucen Zhong and Mingdao Liu and Minlie Huang and Peng Zhang and Qinkai Zheng and Rui Lu and Shuaiqi Duan and Shudan Zhang and Shulin Cao and Shuxun Yang and Weng Lam Tam and Wenyi Zhao and Xiao Liu and Xiao Xia and Xiaohan Zhang and Xiaotao Gu and Xin Lv and Xinghan Liu and Xinyi Liu and Xinyue Yang and Xixuan Song and Xunkai Zhang and Yifan An and Yifan Xu and Yilin Niu and Yuantao Yang and Yueyan Li and Yushi Bai and Yuxiao Dong and Zehan Qi and Zhaoyu Wang and Zhen Yang and Zhengxiao Du and Zhenyu Hou and Zihan Wang},
	year={2024},
	eprint={2406.12793},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2406.12793}, 
}

@article{deepseekcoder,
	title={DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence},
	author={Zhu, Qihao and Guo, Daya and Shao, Zhihong and Yang, Dejian and Wang, Peiyi and Xu, Runxin and Wu, Y and Li, Yukun and Gao, Huazuo and Ma, Shirong and others},
	journal={arXiv preprint arXiv:2406.11931},
	year={2024}
}


@article{infinitebench,
	title={$\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens},
	author={Zhang, Xinrong and Chen, Yingfa and Hu, Shengding and Xu, Zihang and Chen, Junhao and Hao, Moo Khai and Han, Xu and Thai, Zhen Leng and Wang, Shuo and Liu, Zhiyuan and others},
	journal={arXiv preprint arXiv:2402.13718},
	year={2024}
}
@article{dcis,
	title={DCIS: Efficient Length Extrapolation of LLMs via Divide-and-Conquer Scaling Factor Search},
	author={Yang, Lei and Xu, Shaoyang and Xiong, Deyi},
	journal={arXiv preprint arXiv:2412.18811},
	year={2024}
}
@article{ropescale,
	title={Scaling laws of rope-based extrapolation},
	author={Liu, Xiaoran and Yan, Hang and Zhang, Shuo and An, Chenxin and Qiu, Xipeng and Lin, Dahua},
	journal={arXiv preprint arXiv:2310.05209},
	year={2023}
}


@misc{phi3,
	title={Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone}, 
	author={Marah Abdin and Jyoti Aneja and Hany Awadalla and Ahmed Awadallah and Ammar Ahmad Awan and Nguyen Bach and Amit Bahree and Arash Bakhtiari and Jianmin Bao and Harkirat Behl and Alon Benhaim and Misha Bilenko and Johan Bjorck and Sébastien Bubeck and Martin Cai and Qin Cai and Vishrav Chaudhary and Dong Chen and Dongdong Chen and Weizhu Chen and Yen-Chun Chen and Yi-Ling Chen and Hao Cheng and Parul Chopra and Xiyang Dai and Matthew Dixon and Ronen Eldan and Victor Fragoso and Jianfeng Gao and Mei Gao and Min Gao and Amit Garg and Allie Del Giorno and Abhishek Goswami and Suriya Gunasekar and Emman Haider and Junheng Hao and Russell J. Hewett and Wenxiang Hu and Jamie Huynh and Dan Iter and Sam Ade Jacobs and Mojan Javaheripi and Xin Jin and Nikos Karampatziakis and Piero Kauffmann and Mahoud Khademi and Dongwoo Kim and Young Jin Kim and Lev Kurilenko and James R. Lee and Yin Tat Lee and Yuanzhi Li and Yunsheng Li and Chen Liang and Lars Liden and Xihui Lin and Zeqi Lin and Ce Liu and Liyuan Liu and Mengchen Liu and Weishung Liu and Xiaodong Liu and Chong Luo and Piyush Madan and Ali Mahmoudzadeh and David Majercak and Matt Mazzola and Caio César Teodoro Mendes and Arindam Mitra and Hardik Modi and Anh Nguyen and Brandon Norick and Barun Patra and Daniel Perez-Becker and Thomas Portet and Reid Pryzant and Heyang Qin and Marko Radmilac and Liliang Ren and Gustavo de Rosa and Corby Rosset and Sambudha Roy and Olatunji Ruwase and Olli Saarikivi and Amin Saied and Adil Salim and Michael Santacroce and Shital Shah and Ning Shang and Hiteshi Sharma and Yelong Shen and Swadheen Shukla and Xia Song and Masahiro Tanaka and Andrea Tupini and Praneetha Vaddamanu and Chunyu Wang and Guanhua Wang and Lijuan Wang and Shuohang Wang and Xin Wang and Yu Wang and Rachel Ward and Wen Wen and Philipp Witte and Haiping Wu and Xiaoxia Wu and Michael Wyatt and Bin Xiao and Can Xu and Jiahang Xu and Weijian Xu and Jilong Xue and Sonali Yadav and Fan Yang and Jianwei Yang and Yifan Yang and Ziyi Yang and Donghan Yu and Lu Yuan and Chenruidong Zhang and Cyril Zhang and Jianwen Zhang and Li Lyna Zhang and Yi Zhang and Yue Zhang and Yunan Zhang and Xiren Zhou},
	year={2024},
	eprint={2404.14219},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2404.14219}, 
}
@misc{qwen2,
	title={Qwen2 Technical Report}, 
	author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jianxin Yang and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Xuejing Liu and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhifang Guo and Zhihao Fan},
	year={2024},
	eprint={2407.10671},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2407.10671}, 
}

@misc{baichuan,
	title={Base of RoPE Bounds Context Length}, 
	author={Xin Men and Mingyu Xu and Bingning Wang and Qingyu Zhang and Hongyu Lin and Xianpei Han and Weipeng Chen},
	year={2024},
	eprint={2405.14591},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2405.14591}, 
}

@article{ruler,
	title={RULER: What's the Real Context Size of Your Long-Context Language Models?},
	author={Cheng-Ping Hsieh and Simeng Sun and Samuel Kriman and Shantanu Acharya and Dima Rekesh and Fei Jia and Yang Zhang and Boris Ginsburg},
	year={2024}
	journal={arXiv preprint arXiv:2404.06654},
}

@misc{qwen2.5,
	title = {Qwen2.5: A Party of Foundation Models},
	url = {https://qwenlm.github.io/blog/qwen2.5/},
	author = {Qwen Team},
	month = {September},
	year = {2024}
}

@article{deepseekv2,
	title={DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence},
	author={Zhu, Qihao and Guo, Daya and Shao, Zhihong and Yang, Dejian and Wang, Peiyi and Xu, Runxin and Wu, Y and Li, Yukun and Gao, Huazuo and Ma, Shirong and others},
	journal={arXiv preprint arXiv:2406.11931},
	year={2024}
}


@misc{llama3.2,
	title={LLaMA3.2: Revolutionizing edge AI and vision with open, customizable models}, 
	author={Meta},
	year={2024},
	url={https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/}
}

@article{wang2024precision,
	title={When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training},
	author={Wang, Haonan and Liu, Qian and Du, Chao and Zhu, Tongyao and Du, Cunxiao and Kawaguchi, Kenji and Pang, Tianyu},
	journal={arXiv preprint arXiv:2411.13476},
	year={2024}
}
@article{longrecipe,
	title={LongRecipe: Recipe for Efficient Long Context Generalization in Large Language Models},
	author={Hu, Zhiyuan and Liu, Yuliang and Zhao, Jinman and Wang, Suyuchen and Wang, Yan and Shen, Wei and Gu, Qing and Luu, Anh Tuan and Ng, See-Kiong and Jiang, Zhiwei and others},
	journal={arXiv preprint arXiv:2409.00509},
	year={2024}
}
@inproceedings{nnscaler,
	title = {nnScaler: Constraint-Guided Parallelization Plan Generation for Deep Learning Training},
	author={Lin, Zhiqi and Miao, Youshan and Zhang, Quanlu and Yang, Fan and Zhu, Yi and Li, Cheng and Maleki, Saeed and Cao, Xu and Shang, Ning and Yang, Yilei and Xu, Weijiang and Yang, Mao and Zhang, Lintao and Zhou, Lidong},
	booktitle={18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)},
	pages={347--363},
	year={2024}
}
@article{canppl,
	title={Can Perplexity Reflect Large Language Model's Ability in Long Text Understanding?},
	author={Hu, Yutong and Huang, Quzhe and Tao, Mingxu and Zhang, Chen and Feng, Yansong},
	journal={arXiv preprint arXiv:2405.06105},
	year={2024}
}
@article{longppl,
	title={What is Wrong with Perplexity for Long-context Language Modeling?},
	author={Fang, Lizhe and Wang, Yifei and Liu, Zhaoyang and Zhang, Chenheng and Jegelka, Stefanie and Gao, Jinyang and Ding, Bolin and Wang, Yisen},
	journal={arXiv preprint arXiv:2410.23771},
	year={2024}
}

@misc{needlebench,
	title={NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?}, 
	author={Mo Li and Songyang Zhang and Yunxin Liu and Kai Chen},
	year={2024},
	eprint={2407.11963},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2407.11963}, 
}
@misc{needlehaystack,
	title={Needle in a haystack - pressure testing llms}, 
	author={Greg Kamradt},
	year={2023},
	url={https://github.com/gkamradt/LLMTest_NeedleInAHaystack}, 
}
@article{prolong,
	title={How to train long-context language models (effectively)},
	author={Gao, Tianyu and Wettig, Alexander and Yen, Howard and Chen, Danqi},
	journal={arXiv preprint arXiv:2410.02660},
	year={2024}
}

@article{longbench,
	title={Longbench: A bilingual, multitask benchmark for long context understanding},
	author={Bai, Yushi and Lv, Xin and Zhang, Jiajie and Lyu, Hongchang and Tang, Jiankai and Huang, Zhidian and Du, Zhengxiao and Liu, Xiao and Zeng, Aohan and Hou, Lei and others},
	journal={arXiv preprint arXiv:2308.14508},
	year={2023}
}
@article{fu2024data,
	title={Data engineering for scaling language models to 128k context},
	author={Fu, Yao and Panda, Rameswar and Niu, Xinyao and Yue, Xiang and Hajishirzi, Hannaneh and Kim, Yoon and Peng, Hao},
	journal={arXiv preprint arXiv:2402.10171},
	year={2024}
}
@article{loft,
	title={Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?},
	author={Lee, Jinhyuk and Chen, Anthony and Dai, Zhuyun and Dua, Dheeru and Sachan, Devendra Singh and Boratko, Michael and Luan, Yi and Arnold, S{\'e}bastien MR and Perot, Vincent and Dalmia, Siddharth and others},
	journal={arXiv preprint arXiv:2406.13121},
	year={2024}
}
@article{li2023starcoder,
	title={Starcoder: may the source be with you!},
	author={Li, Raymond and Allal, Loubna Ben and Zi, Yangtian and Muennighoff, Niklas and Kocetkov, Denis and Mou, Chenghao and Marone, Marc and Akiki, Christopher and Li, Jia and Chim, Jenny and others},
	journal={arXiv preprint arXiv:2305.06161},
	year={2023}
}
@misc{lozhkov2024fineweb-edu,
	author       = { Lozhkov, Anton and Ben Allal, Loubna and von Werra, Leandro and Wolf, Thomas },  
	title        = { FineWeb-Edu: the Finest Collection of Educational Content }, 
	year         = 2024,  
	url          = { https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu },  
	doi          = { 10.57967/hf/2497 },
	publisher    = { Hugging Face }
}
@article{redpajamav2,
	title   = {RedPajama: an Open Dataset for Training Large Language Models},
	author  = {Maurice Weber and Daniel Y. Fu and Quentin Anthony and Yonatan Oren and Shane Adams and Anton Alexandrov and Xiaozhong Lyu and Huu Nguyen and Xiaozhe Yao and Virginia Adams and Ben Athiwaratkun and Rahul Chalamala and Kezhen Chen and Max Ryabinin and Tri Dao and Percy Liang and Christopher Ré and Irina Rish and Ce Zhang},
	journal = {NeurIPS Datasets and Benchmarks Track},
	year    = 2024,
}
@misc{child2019generatinglongsequencessparse,
	title={Generating Long Sequences with Sparse Transformers}, 
	author={Rewon Child and Scott Gray and Alec Radford and Ilya Sutskever},
	year={2019},
	eprint={1904.10509},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/1904.10509}, 
}

@misc{beltagy2020longformerlongdocumenttransformer,
	title={Longformer: The Long-Document Transformer}, 
	author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
	year={2020},
	eprint={2004.05150},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2004.05150}, 
}

@inproceedings{NEURIPS2020_c8512d14,
	author = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {17283--17297},
	publisher = {Curran Associates, Inc.},
	title = {Big Bird: Transformers for Longer Sequences},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf},
	volume = {33},
	year = {2020}
}

@misc{guo2022longt5efficienttexttotexttransformer,
	title={LongT5: Efficient Text-To-Text Transformer for Long Sequences}, 
	author={Mandy Guo and Joshua Ainslie and David Uthus and Santiago Ontanon and Jianmo Ni and Yun-Hsuan Sung and Yinfei Yang},
	year={2022},
	eprint={2112.07916},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2112.07916}, 
}

@misc{ding2023longnetscalingtransformers1000000000,
	title={LongNet: Scaling Transformers to 1,000,000,000 Tokens}, 
	author={Jiayu Ding and Shuming Ma and Li Dong and Xingxing Zhang and Shaohan Huang and Wenhui Wang and Nanning Zheng and Furu Wei},
	year={2023},
	eprint={2307.02486},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2307.02486}, 
}

@misc{an2024trainingfreelongcontextscalinglarge,
	title={Training-Free Long-Context Scaling of Large Language Models}, 
	author={Chenxin An and Fei Huang and Jun Zhang and Shansan Gong and Xipeng Qiu and Chang Zhou and Lingpeng Kong},
	year={2024},
	eprint={2402.17463},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2402.17463}, 
}

@misc{katharopoulos2020transformersrnnsfastautoregressive,
	title={Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention}, 
	author={Angelos Katharopoulos and Apoorv Vyas and Nikolaos Pappas and François Fleuret},
	year={2020},
	eprint={2006.16236},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2006.16236}, 
}

@misc{yang2024gatedlinearattentiontransformers,
	title={Gated Linear Attention Transformers with Hardware-Efficient Training}, 
	author={Songlin Yang and Bailin Wang and Yikang Shen and Rameswar Panda and Yoon Kim},
	year={2024},
	eprint={2312.06635},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2312.06635}, 
}

@misc{gu2024mambalineartimesequencemodeling,
	title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces}, 
	author={Albert Gu and Tri Dao},
	year={2024},
	eprint={2312.00752},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2312.00752}, 
}

@inproceedings{
	yu2024robustifying,
	title={Robustifying State-space Models for Long Sequences via Approximate Diagonalization},
	author={Annan Yu and Arnur Nigmetov and Dmitriy Morozov and Michael W. Mahoney and N. Benjamin Erichson},
	booktitle={The Twelfth International Conference on Learning Representations},
	year={2024},
	url={https://openreview.net/forum?id=DjeQ39QoLQ}
}

@misc{lieber2024jambahybridtransformermambalanguage,
	title={Jamba: A Hybrid Transformer-Mamba Language Model}, 
	author={Opher Lieber and Barak Lenz and Hofit Bata and Gal Cohen and Jhonathan Osin and Itay Dalmedigos and Erez Safahi and Shaked Meirom and Yonatan Belinkov and Shai Shalev-Shwartz and Omri Abend and Raz Alon and Tomer Asida and Amir Bergman and Roman Glozman and Michael Gokhman and Avashalom Manevich and Nir Ratner and Noam Rozen and Erez Shwartz and Mor Zusman and Yoav Shoham},
	year={2024},
	eprint={2403.19887},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2403.19887}, 
}

@misc{ren2024sambasimplehybridstate,
	title={Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling}, 
	author={Liliang Ren and Yang Liu and Yadong Lu and Yelong Shen and Chen Liang and Weizhu Chen},
	year={2024},
	eprint={2406.07522},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2406.07522}, 
}


@misc{gao2024retrievalaugmentedgenerationlargelanguage,
	title={Retrieval-Augmented Generation for Large Language Models: A Survey}, 
	author={Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi Dai and Jiawei Sun and Meng Wang and Haofen Wang},
	year={2024},
	eprint={2312.10997},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2312.10997}, 
}

@misc{luo2024bgelandmarkembeddingchunkingfree,
	title={BGE Landmark Embedding: A Chunking-Free Embedding Method For Retrieval Augmented Long-Context Large Language Models}, 
	author={Kun Luo and Zheng Liu and Shitao Xiao and Kang Liu},
	year={2024},
	eprint={2402.11573},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2402.11573}, 
}

@misc{jeong2024adaptiveraglearningadaptretrievalaugmented,
	title={Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity}, 
	author={Soyeong Jeong and Jinheon Baek and Sukmin Cho and Sung Ju Hwang and Jong C. Park},
	year={2024},
	eprint={2403.14403},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2403.14403}, 
}

@misc{chan2024rqraglearningrefinequeries,
	title={RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation}, 
	author={Chi-Min Chan and Chunpu Xu and Ruibin Yuan and Hongyin Luo and Wei Xue and Yike Guo and Jie Fu},
	year={2024},
	eprint={2404.00610},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2404.00610}, 
}

@misc{dong2024multiviewcontentawareindexinglong,
	title={Multi-view Content-aware Indexing for Long Document Retrieval}, 
	author={Kuicai Dong and Derrick Goh Xin Deik and Yi Quan Lee and Hao Zhang and Xiangyang Li and Cong Zhang and Yong Liu},
	year={2024},
	eprint={2404.15103},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2404.15103}, 
}

@misc{jiang2024longragenhancingretrievalaugmentedgeneration,
	title={LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs}, 
	author={Ziyan Jiang and Xueguang Ma and Wenhu Chen},
	year={2024},
	eprint={2406.15319},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2406.15319}, 
}

@misc{gutiérrez2025hipporagneurobiologicallyinspiredlongterm,
	title={HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models}, 
	author={Bernal Jiménez Gutiérrez and Yiheng Shu and Yu Gu and Michihiro Yasunaga and Yu Su},
	year={2025},
	eprint={2405.14831},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2405.14831}, 
}

@misc{chen2023walkingmemorymazecontext,
	title={Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading}, 
	author={Howard Chen and Ramakanth Pasunuru and Jason Weston and Asli Celikyilmaz},
	year={2023},
	eprint={2310.05029},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2310.05029},
}

@misc{sun2023pearlpromptinglargelanguage,
	title={PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents}, 
	author={Simeng Sun and Yang Liu and Shuohang Wang and Chenguang Zhu and Mohit Iyyer},
	year={2023},
	eprint={2305.14564},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2305.14564}, 
}

@misc{zhang2024chainagentslargelanguage,
	title={Chain of Agents: Large Language Models Collaborating on Long-Context Tasks}, 
	author={Yusen Zhang and Ruoxi Sun and Yanfei Chen and Tomas Pfister and Rui Zhang and Sercan O. Arik},
	year={2024},
	eprint={2406.02818},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2406.02818}, 
}

@misc{li2024graphreaderbuildinggraphbasedagent,
	title={GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models}, 
	author={Shilong Li and Yancheng He and Hangyu Guo and Xingyuan Bu and Ge Bai and Jie Liu and Jiaheng Liu and Xingwei Qu and Yangguang Li and Wanli Ouyang and Wenbo Su and Bo Zheng},
	year={2024},
	eprint={2406.14550},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2406.14550}, 
}

@misc{gur2024realworldwebagentplanninglong,
	title={A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis}, 
	author={Izzeddin Gur and Hiroki Furuta and Austin Huang and Mustafa Safdari and Yutaka Matsuo and Douglas Eck and Aleksandra Faust},
	year={2024},
	eprint={2307.12856},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2307.12856},
}

@misc{lee2024humaninspiredreadingagentgist,
	title={A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts}, 
	author={Kuang-Huei Lee and Xinyun Chen and Hiroki Furuta and John Canny and Ian Fischer},
	year={2024},
	eprint={2402.09727},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2402.09727}, 
}

