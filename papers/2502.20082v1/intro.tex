\vspace{-2ex}
\section{Introduction}
A long context window has become an essential feature of Large Language Models (LLMs)~\citep{gpt4,llama3.1,phi3,deepseekv2,qwen2.5}. For instance, a 128k context window is now standard in recent LLMs like GPT-4o and LLaMA3.1. Context window extension is achieved through mid-training after pre-training, where the rotary positional embeddings (RoPE)~\citep{rope} are rescaled to fit the expanded context. The model weights are then fine-tuned using long-sequence data to adapt to the rescaled RoPE.

Extending the context window of a pre-trained LLM requires addressing the out-of-distribution (OOD) issue in rotary positional embeddings (RoPE). In RoPE, higher-dimensional RoPE embeddings produce OOD values at extended token positions due to incomplete rotation periods within the original context window~\citep{ropescale, han2023lminfinite,baichuan}. To mitigate this, RoPE rescaling remaps these OOD values into the in-distribution range learned during pre-training. Various methods, such as YaRN~\citep{yarn}, NTK~\citep{ntk}, and LongRoPE~\citep{longrope}, have been proposed to determine appropriate rescaling factors.

\begin{figure}[t]
	\centering
	\includegraphics[width=1\columnwidth]{ruler.png}	
	\vspace{-4ex}
	\caption{{\sysname}-extended LLaMA3-8B achieves the best performance at a 128k context length among $\sim$10B models.}
	\label{fig:ruler}
\end{figure}
Despite attempts to mitigate the OOD issue with RoPE rescaling, context window extension still encounters two major challenges. First, rescaling factors derived from previous methods often fall short of achieving the \emph{effective} target context length. For example, LLaMA3.1 adopts YaRN to extend its context window to 128k; however, its performance on RULER~\citep{ruler}, a benchmark designed to evaluate LLMs' long-context processing capability, deteriorates significantly when going beyond 64k (Fig.~\ref{fig:ruler}). 
Second, existing approaches to extending an LLM's context window usually lead to a noticeable performance degradation on tasks for the original short context window. As shown in Fig.~\ref{fig:ropeexample}(c), extending Phi3-mini~\citep{phi3} to 128k results in MMLU score drops of 7.56, 4.34, and 3.52 points for YaRN, NTK, and LongRoPE, respectively. Restoring short-context performance typically requires costly mid-training strategies, such as multi-stage progressive extension~\citep{llama3.1} and pre-training data replay~\citep{longrecipe}, which increase both training costs (e.g., 800B tokens for LLaMA3.1) and system complexity.
 

 
 

This paper introduces \sysname{}, a novel approach for context extension that enables LLMs to achieve an effective long context window while preserving short-context performance. Our analysis reveals that lower RoPE dimensions are sufficiently trained, whereas higher dimensions -- critical for long-context processing -- receive inadequate training. This results in shorter effective RoPE rotation ranges within the pre-trained context length. We hypothesize that this undertraining in higher dimensions is the root cause of their extended rotation periods longer than their theoretical predictions. Consequently, the critical dimensions shift earlier, leaving existing rescaling methods unable to fully address OOD issues across all dimensions. 
This hypothesis also explains the empirical observations showing that RoPE requires scaling factors larger than analytically derived values in the higher dimensions for better long-context performance~\citep{prolong,llama3.2}.

 

Building on this hypothesis, \sysname{} adopts a simple yet effective RoPE   rescaling algorithm to fully address the OOD issues across all RoPE dimensions. It leverages evolutionary search to identify the true critical RoPE dimensions and optimal rescaling factors, guided by a more effective ``needle-driven'' perplexity (PPL) evaluation. Unlike conventional PPL, which averages across all tokens, \sysname{} focuses exclusively on ``needles'' – specific answer tokens within long documents that require deep contextual understanding. This ensures accurate evaluation of long-context performance. The search determines the true critical dimensions and rescaling factors for higher OOD dimensions, while NTK scaling is applied to the well-trained lower dimensions. The rescaling factors yielding the lowest PPL are selected as the final solution.  

 
  

To preserve the original short-context performance, \sysname{} incorporates mixed context window training, which simultaneously trains a pre-trained context window with the original RoPE and a long-context window with rescaled RoPE. The long-context window is trained by adapting model weights to the rescaled RoPE for long documents packed to the target length. Concurrently, the short-context window is trained on short documents, also packed to the same target length, using an attention mask to prevent cross-document attention. At inference, original RoPE is used if the input is within the short context; otherwise, rescaled RoPE is applied. This method optimizes long-context performance without sacrificing short-context performance.
   


Extensive experiments across various LLM sizes and challenging benchmarks validate our hypothesis and demonstrate the effectiveness of \sysname{}. For Phi3-mini-3.8B and LLaMA3-8B, our rescaling factors shift the theoretical critical dimension from 31 to 25 and from 35 to 30, respectively. By fully resolving RoPE OOD issues, {\sysname}-extended Phi3-mini-3.8B and LLaMA3-8B achieve an effective 128k context window, significantly outperforming baselines on both synthetic and real-world long-context benchmarks. Moreover, with mixed context window training, {\sysname} is the only RoPE rescaling method that can retain over 97\% of the original short-context performance on standard tasks. Remarkably,  {\sysname}-extended LLaMA3-8B-128k surpasses Meta’s LLaMA3.1-8B-128k in long-context performance while maintaining comparable short-context accuracy, all achieved with just 10B training tokens—80× fewer than Meta's 800B tokens.


