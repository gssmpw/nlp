\section{Related Works}
In addition to methods based on RoPE rescaling, this
section discusses related works of other approaches.







\noindent\textbf{RAG and Agent-based extension}. Retrieval-Augmented Generation (RAG) approaches
incorporate an external memory module to store and manage long past context, coupled with dynamic retrieval mechanisms to fetch task-relevant documents during inference~\citep{jeong2024adaptiveraglearningadaptretrievalaugmented, chan2024rqraglearningrefinequeries, dong2024multiviewcontentawareindexinglong,gutiérrez2025hipporagneurobiologicallyinspiredlongterm, luo2024bgelandmarkembeddingchunkingfree}. Agent-based methods, meanwhile, decompose long-context processing into iterative planning, summarization, and retrieval tasks, often employing multi-agent workflows: individual agents extract information from text segments, which are aggregated to bypass fixed context limits \citep{zhang2024chainagentslargelanguage, li2024graphreaderbuildinggraphbasedagent, lee2024humaninspiredreadingagentgist}, while others integrate specialized architectures (e.g., hierarchical attention) for direct long-text handling \citep{gur2024realworldwebagentplanninglong}. Both directions—relying on external modules or multi-step decomposition—are complementary to our method. 


\noindent\textbf{Efficient long-context modeling}. Attention computation and memory costs grow quadratically with context length, prompting research into reducing these challenges through improved attention mechanisms and innovative model structures. Many methods leverage the sparsity of standard attention, reducing computation by focusing on local and auxiliary regions~\citep{child2019generatinglongsequencessparse, beltagy2020longformerlongdocumenttransformer, NEURIPS2020_c8512d14, guo2022longt5efficienttexttotexttransformer}, while others extend context length using fine-grained sparsity~\citep{ding2023longnetscalingtransformers1000000000} or chunked attention~\citep{an2024trainingfreelongcontextscalinglarge}. Linear attention approaches further lower complexity while achieving comparable performance, with additional optimization for hardware efficiency~\citep{katharopoulos2020transformersrnnsfastautoregressive, yang2024gatedlinearattentiontransformers}. State-space models (SSMs) offer linear complexity for sequence modeling~\citep{gu2024mambalineartimesequencemodeling, yu2024robustifying}, and hybrid transformer-SSM architectures enhance foundational model capabilities~\citep{lieber2024jambahybridtransformermambalanguage, ren2024sambasimplehybridstate}.
Most of these approaches build upon RoPE, making them complementary to our approach.