\section{{\sysname} Methodology}

\subsection{New RoPE OOD Hypothesis}
\noindent\textbf{The empirical RoPE  periods in higher dimensions are longer than theoretical values, limiting current methods to fully address RoPE OOD}. In Sec.~\ref{sec:analysis}, we observe that RoPE scale factors slightly exceeding the theoretical lower bound beyond the critical dimension 
$d_{tcd}$ yield improved long-context performance. We attribute this to  insufficient training in higher dimensions, which extends rotation periods and reduces the critical dimension index (Fig.~\ref{fig:ropeexample}(a)) relative to the theoretical expectations.  


 \begin{figure}[t]
	\centering
	\includegraphics[width=1\columnwidth]{rope2.pdf}	
	\vspace{-5ex}
	\caption{Sequence length required to span the theoretical period  during Phi3-mini pre-training for different RoPE dimensions.  Insufficient training in higher RoPE dimensions leads to  shorter effective RoPE ranges and longer actual periods.  }
	\label{fig:ropedistance}
\end{figure}

As illustrated in Fig.~\ref{fig:ropedistance}(a), lower RoPE dimensions (with shorter periods)   receive repeated full-period training cycles within a single  corpus.  For example, in Phi3-mini,  the 8$^{th}$ dimension has a short period  of 24,  requiring only $m-n=24$ tokens for a full cycle.  A 2048-token training sample thus covers this dimension thousands of times, ensuring sufficient training. In contrast, higher RoPE dimensions, with period exceeding the pre-trained context window, receive far less training. For example, the 48$^{th}$ dimension spans only $\sim$4\% of its $cosine$ period within a 2048-token sequence (Fig.~\ref{fig:ropedistance}(b)), resulting in the theoretical incomplete period being covered just once. 


 
 A deeper challenge arises after self-attention: these incomplete RoPE periods in high dimensions exhibit reduced effective ranges (Fig.~\ref{fig:ropedistance}(b)), stretching practical period  beyond theoretical values. As shown in Eq.~\ref{eq:attention},   RoPE positional information is incorporated via self-attention, where the max relative token distance  determines the practical RoPE range. As real-world data rarely contains  long-range dependencies (e.g., distances of 2048 tokens), 
  higher RoPE dimensions tend to be under-trained, amplifying period discrepancies. 
  
  This under-training in higher RoPE dimensions explains why larger scaling factors improve long-context performance.
  We formalize this insight as:



 \noindent\textbf{Hypothesis}. \textit{Insufficient training in higher RoPE dimensions extends empirical rotation periods beyond the theoretical $\frac{2\pi}{\theta_i}$.  This discrepancy necessitates larger scale factors to mitigate RoPE OOD  and  lowering the critical dimension index $d_{rcd}$ below its theoretical $d_{tcd}$.}
 	
 	
 	
 	

\subsection{RoPE Rescaling Factor Search}


Since the theoretical RoPE OOD theory cannot fully address OOD issues, we use a search-based approach to identify the practical true critical dimension and optimal rescaled RoPE. Inspired by LongRoPE, we search for scaling factors, apply them to the pre-trained LLM via rescaled RoPE, and compute perplexity (PPL) on fixed samples at a target context length (e.g., 128k). The factors that minimize PPL are chosen for best preserving pre-trained RoPE information  while addressing OOD. Given that the approach relies entirely on the search, we introduce two key innovations.


\noindent\textbf{Synthetic needle data to guide the search}. Naively using PPL-guided search  can easily result in suboptimal rescaling factors. First, long sequences often contain irrelevant or low-dependency tokens, reducing the effective maximum  token dependency.   For instance, predicting the final token in a 128k-token book may not require the context of the first token.    Second,  standard PPL, by averaging over all token equally, fails to effectively capture the long-context abilities~\citep{canppl,longppl} and can be dominated by irrelevant tokens, obscuring key answer tokens. As a result, the rescaling factors that minimize PPL often fail to achieve the target context window size. 


To address this, we introduce a  needle-driven PPL evaluation. Instead of using real-world long documents, we synthesize long data with controlled token dependency distances. Inspired by needle retrieval benchmarks  for long-context evaluation~\citep{ruler,needlebench}, we randomly sample 10 books from the PG19 validation set. At the start of each sample, we insert a "needle" (a specific piece of text as shown in Appendix ~\ref{sec:needle}), and at the end, we ask the model to retrieve this needle. We then compute the perplexity of only the retrieved needle tokens. The needle-based PPL  evaluates how well the model, with the rescaled RoPE, can understand the entire context and retrieve the distant needle. 

 \begin{algorithm}[t]
 	\small
 	\caption{Initialization with theoretical periods}
 %	\fontsize{8.5}{8.5} \selectfont
 	\textbf{Input:}  theta base $\theta_{base}$; RoPE dim $d$, pre-trained context window size $L_\text{train}$, target  length $L$; theoretical critical dimension $d_{tcd}$\\
	\vspace{-2.5ex}
 	\begin{algorithmic}[1]
 		\label{alg:initialize}
 		\STATE $\text{P}_0=[0]*2/d$
 		\STATE	$d_{tcd}^{10}$=$\lceil \frac{d}{2}\log_{\theta_{base}} \frac{L_{\text{train}}}{2\pi\times10}  \rceil $  \COMMENT{\textcolor{codeblue} {Compute the dim with a theoretical 10 periods.}} %\COMMENT{\textit{compute the dim index based on its theoretical period}}
 		\\\COMMENT{\textcolor{codeblue}{include smaller indices as candidate $d_{rcd}$}}
 		\FOR{int $d_{rcd}$=$d_{tcd}^{10}$ to $d_{tcd}$}
 		\STATE s=randint($\frac{L}{L_\text{train}}$, 2$\times \frac{L}{L_\text{train}}$)
 		\STATE $\lambda[d_{rcd}:\frac{d}{2}-1]=s$ 
 		\STATE $\theta_{d_{tcd}^{10}}=\frac{1}{s\times \theta_{base}^{(2\times d_{tcd}^{10}/d)}}$
 		\STATE $\lambda[0:d_{rcd}]$= compute rescaling factors using NTK  $\theta_{d_{tcd}^{10}}$
 		\STATE add $\lambda$ into $\text{P}_0$; 	
 		\ENDFOR	
 		\STATE Return  $\text{P}_0$ ;
 	\end{algorithmic}
 \end{algorithm}
 
 \begin{algorithm}[t]
 	\small
 	\caption{Critical dimension aware mutation}
 	\fontsize{8.2}{8.2} \selectfont
 	\textbf{Input:} population $\text{P}$; mutation probability $p$; synthetic long data $\mathbf{X}$\\
 	\vspace{-2ex}
 	\begin{algorithmic}[1]
 		\label{alg:mutate}
 		\STATE  Top-k = \textit{Update\_Topk} ( $\text{P}$);
 		\STATE SP=[$\frac{L}{L_\text{train}}$, 2$\times \frac{L}{L_\text{train}}$]\COMMENT{\textcolor{codeblue}{search space}}
 	%	\STATE $\text{P}$=$\phi$
 			\FOR{$\lambda$ in Top-k} 			
 			\STATE $\lambda_{right}$= $\lambda[d_{rcd}:\frac{d}{2}-1]$
 			 \STATE	$\lambda_{right}$=Mutation\_with\_mono\_constraint ($\lambda_{right}$, $p$, SP) \COMMENT{\textcolor{codeblue}{mutate scale factors beyond $\theta_{d_{rcd}}$.}}
 			 \STATE $\lambda[d_{rcd}:\frac{d}{2}-1]$= $\lambda_{right}$
 			\STATE $\theta_{d_{rcd}}=\frac{1}{\lambda_{right}[0]\times \theta_{base}^{(2\times d_{rcd}/d)}}$ 	 \COMMENT{\textcolor{codeblue}{update theta base in $\theta_{d_{rcd}}$.}}
 			 \STATE $\lambda[0:i]$= compute rescaling factors using NTK  $\theta_{d_{rcd}}$ 	\COMMENT{\textcolor{codeblue}{update dims before $\theta_{d_{rcd}}$}}
 			 \STATE \textit{Compute\_PPL} (LLM, $\lambda$, $\mathbf{X}$); add $\lambda$ into $\text{P}$; 	
 			\ENDFOR 
 	%	\STATE $\text{P}$=$\text{P}$ $\cup$ Top-k
 		\STATE Update $\text{P}$ with Top-k; Return the latest population $\text{P}$ ;
 	\end{algorithmic}
 \end{algorithm}

\noindent\textbf{Critical dimension-aware scale factor search}. With the synthetic needle-driven PPL evaluation, we run a simple evolutionary search to identify the real critical dimension $d_{rcd}$ and  the optimal rescaling factors. For search efficiency, we restrict the search to dimensions $i\ge d_{rcd}$, while applying NTK-aware scaling to lower dimensions ($i<d_{rcd}$) using the adjusted base value derived from $d_{rcd}$. 

The search begins by initializing $d_{rcd}$ and rescaling factors, as detailed in Algorithm~\ref{alg:initialize}. Based on our hypothesis, smaller indices are considered potential  $d_{rcd}$ , with candidates ranging from $d_{tcd}^{10}$,
where the theoretical RoPE period spans 10 periods in the pre-training window, and $d_{tcd}$.
For each candidate, rescaling factors above $\frac{L}{L_\text{train}}$ are randomly sampled for dimension $i\ge d_{rcd}$ to address RoPE OOD value, while NTK scaling is applied to dimensions $i<d_{rcd}$. 

We iteratively sample and mutate rescaling factors until reaching a population size $N$. Using the needle-driven synthesis method, we generate $L$-token documents and compute PPL for each candidate by applying the rescaling factors to the LLM and evaluating the input $\mathbf{X}$. 




The population is updated through standard evolution search. Algorithm~\ref{alg:mutate} shows the mutation process.  For each sampled scaling factor, 
we split RoPE dimensions at $d_{rcd}$. The higher group ($i\ge d_{rcd}$) performs mutation with probability $p$ under the  monotonic non-decreasing constraint: $\lambda_i\le \lambda_{i+1}$. The theta base for $d_{rcd}$
is updated after mutation, and NTK scaling is applied to rescale factors in the lower group.





Fig.~\ref{fig:scalefactor} shows the final scaling factors identified by {\sysname} for Phi3-mini and LLaMA3-8B under a 128k context. The practical critical dimensions ($d_{rcd}$) are shifted earlier to 25 and 30, compared to the theoretical values $d_{tcd}$ of 31 and 35, respectively. The scaling factors for RoPE OOD dimensions are slightly larger than PI/YaRN/LongRoPE and notably smaller than NTK. 


\begin{figure}[htp]
	\centering
	\includegraphics[width=1\columnwidth]{scale_factor.pdf}	
	\vspace{-5ex}
	\caption{Scale factors across different RoPE rescaling approaches.}
	\label{fig:scalefactor}
\end{figure}

\subsection{Mixed Context Window Training}

\begin{figure}[t]
	\centering
	\includegraphics[width=1\columnwidth]{mixedwindow1.pdf}	
	\vspace{-5ex}
	\caption{Mixed context window training to improve both short and long context capabilities.}
	\label{fig:mixwindow0}
\end{figure}

We then apply the optimal rescaling factors to RoPE on the pre-trained LLM, but two critical challenges remains for effective long-context LLM deployment. 
 First, the pre-trained model weights  have not been trained with the rescaled RoPE,  leading to poor performance on real-world long-context tasks. Second, extending context window size often degrades performance on original short-context  tasks~\citep{longrope,longrecipe},
  making it challenging to balance long- and short-context capabilities.
 
 
 To address these challenges, we introduce a novel mixed context window training approach
 that achieve both long- and short-context superior performance without adding system-level training complexity. Specifically, short-context training reuses the original RoPE and fine-tunes on short sequences, preserving pre-trained performance. Long-context training applies the rescaled RoPE and fine-tunes on long sequences, enabling effective long-context understanding.
 
 


 
Fig.~\ref{fig:mixwindow0} illustrates this process. For a target context window size of $L$=128k, we sample short sequences ($\le L_\text{train}$) and long sequences (8k-200k), chunked into 128k segments with BOS and EOS tokens. For segments labeled as short  windows, the original RoPE is used with attention masks to prevent self-attention across different documents as shown  in Fig.~\ref{fig:mixwindow0}(a). For long-context segments, we apply the rescaled RoPE for full attention within the 128k segments (Fig.~\ref{fig:mixwindow0}(b)).  More details can be found in Appendix~\ref{sec:addexp}.
 
 

