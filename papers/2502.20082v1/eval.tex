\vspace{-0.5ex}
\section{Experiments}
\vspace{-1.5ex}
\begin{table}
	
	\caption{Mid-training data mix.}
	\label{tbl:data}
	\resizebox{0.9\columnwidth}{!}{	\begin{tabular}
			{cccc}
			\toprule
			&Short Context Window & \multicolumn{2}{c}{Long Context Window}\\
			& $\le L_{train}$ &  $L_{train}$-100k & 100k-200k\\
			\hline
			Tokens& 3B & 3B & 4B\\			
			\hline				
	\end{tabular}}
\end{table}

\begin{table}[t]
	\caption{Comparison with prior SOTA RoPE rescaling methods on  RULER Benchmark. We report the average score across 13 tasks.}
	\label{tbl:ruler1}
	\centering
	\resizebox{0.95\columnwidth}{!}{	\begin{tabular}
			{cccccccc}
			\toprule
			Method&4k&8k&16k&32k&64k&128k\\
			\midrule
			\multicolumn{8}{c}{Base Model: Phi3-mini  (3.8B)} \\
			YaRN &85.74 &78.68&75.97&65.22&52.16&39.37\\
			NTK & \bf 91.34&87.02&80.57&72.81&61.91&49.37\\
			LongRoPE&88.40 &83.23 &79.46&71.20&64.63&53.71\\
\rowcolor{airforceblue}			\bf {\sysname}&90.41 &\bf 87.22&\bf83.33&\bf 76.51&\bf 65.37&\bf58.81\\
			\midrule
			\multicolumn{8}{c}{Base Model: LLaMA3-8B} \\
			YaRN &91.86 &87.87 &84.67&68.80&62.51&49.39\\
			NTK &94.38 &92.64 &91.93&87.33&79.26&73.19\\
			LongRoPE& 94.60&92.70 &91.01&86.60&81.23&73.40\\
		\rowcolor{airforceblue}	\bf {\sysname} &\bf 94.61 & \bf 93.68& \bf 92.31& \bf90.49& \bf85.62&\bf82.03\\
			\hline				\end{tabular}}
\end{table}
\begin{table*}[htp]
	\centering
	\small
	\caption{Long context performance comparison under different extension methods on real-world benchmarks}
	\label{tbl:longp_realworld}
	\resizebox{1\textwidth}{!}{	\begin{tabular}
			{@{\hskip0pt}c@{\hskip8pt}c@{\hskip8pt}c@{\hskip6pt}c@{\hskip6pt}c@{\hskip6pt}ccccccccccc@{\hskip6pt}c@{\hskip0pt}}
			\toprule
			Method & \multicolumn{8}{c}{LOFT}& \multicolumn{8}{c}{InfiniteBench - LongBench} \\
			\cmidrule{2-9} \cmidrule{11-17}
			&\bf{Avg.}&ArguAna&FEVER&HotPotQA&\makecell{MS\\MACRO}&NQ&Quora&SciFact&& \bf{Avg.} & \makecell{KV \\retrieval}&En.MC&TriviaQA&TREC&LCC&RepoBench-P\\
			\cmidrule{2-9} \cmidrule{11-17}
			\multicolumn{17}{c}{Base model: Phi3-mini  (3.8B)}\\
			YaRN &5.86&4.0&4.0&0&8.0&12.0&1.0&12.0&&50.96& 5.8&31.44&84.35&61.00&\bf 63.98&59.23\\
			NTK &7.57&0&21.0&0&6.0&13.0&4.0&9.0&& 52.31&5.1&37.55&84.01&65.00&62.36&59.82\\
			LongRoPE &21.14&5.0&64.0&3.0&17.0&35.0&8.0&\bf16.0&&50.67 &5.6 &35.81&86.47&62.50&55.25&58.43\\
	\rowcolor{airforceblue}		\bf{\sysname} &\bf23.00&\bf5.0&\bf70.0&\bf4.0&\bf19.0&\bf39.0&\bf10.0&14.0&&\bf 55.23 &\bf12.0 &\bf42.36&\bf87.27&\bf67.00&62.67&\bf60.10\\
			\midrule
			\multicolumn{17}{c}{Base model: LLaMA3-8B}\\
			YaRN &26.14&7.0&62.0&15.0&21.0&43.0&23.0&12.0&& 51.81&2.2 &30.57&88.97&73.50&65.40&62.21\\
			NTK&67.14&22.0&96.0&53.0&75.0&89.0&71.0&64.0&&67.98 & 66.0&42.79&90.87&74.00&68.67&65.55\\
			LongRoPE &60.85&22.0&96.0&25.0&57.0&90.0&74.0&62.0&&70.39& 74.0&45.85&89.99&76.00&69.13&67.38\\
		\rowcolor{airforceblue}	\bf{\sysname} &\bf74.28&\bf28.0&\bf96.0&\bf70.0&\bf80.0&\bf94.0&\bf79.0&\bf73.0&&\bf 73.37&\bf88.0 &\bf46.72&\bf91.13&\bf76.50 &\bf70.47&\bf67.39\\	
			\bottomrule
	\end{tabular}}
\end{table*}
\subsection{Setup}
\noindent\textbf{Evaluation LLMs and Tasks}. We apply {\sysname} to  LLaMA3-8B and Phi3-mini (3.8B). Phi3-mini, with its limited capabilities, serves as a rigorous testbed for  evaluating  RoPE rescaling methods.  Performance is evaluated across three dimensions: (1) long-context stress tests, including RULER~\citep{ruler} and Needle in a Haystack~\citep{needlehaystack}; (2) real-world long-context benchmarks including  LOFT~\citep{loft}, InfiniteBench~\citep{infinitebench}, and LongBench~\citep{longbench}; (3) standard benchmarks within a 4096-token context.




\noindent\textbf{Mid-training}. Our method can potentially support million-level context length, but due to  resources constraint, we extend the two models to 
128k context window and mid-train on 64 A100 GPUs using a 10B-token dataset. Following the per-source upsampling from~\citep{fu2024data}, we sample 4.5B, 2.5B, and 2B tokens from RedPajama-v1~\citep{redpajama}, RedPajama-v2~\citep{redpajamav2}, and StarCoder~\citep{li2023starcoder}, covering 8k–200k sequence lengths. For short context windows, we sample 1B tokens from Fineweb-Edu~\citep{lozhkov2024fineweb-edu}. Table~\ref{tbl:data} shows the token distribution by sequence length.
We train for 1 epoch with a global batch size of 64. The initial learning rate of 2e-5 with  a cosine learning rate scheduler.


\noindent\textbf{Baselines}. We compare with state-of-the-art RoPE rescaling methods, including YaRN, NTK, and LongRoPE. All baselines use the same mid-training procedure for fairness.


\subsection{Main Results}

\begin{table}[t]
	\centering
	\small
	\caption{Comparison of long-context LLMs with original Phi3-mini and LLaMA3-8B on  regular short benchmarks. }
	\label{tbl:short performance}
	\resizebox{1\columnwidth}{!}{	\begin{tabular}
			{@{\hskip0pt}c@{\hskip4pt}g@{\hskip4pt}c@{\hskip4pt}c@{\hskip4pt}c@{\hskip4pt}c@{\hskip4pt}c@{\hskip0pt}}
			\toprule
			\multicolumn{7}{c}{\bf(a) Phi3-mini (3.8B) with 128k context window}\\
			\midrule
			{Model}&Avg.&{MMLU}&{MMLU-Pro} &{HellaSwag}&{TruthfulQA}&{GSM8K}\\
			\midrule 
			Original Phi3-mini (2k)& 63.2&70.78&41.17&77.96&47.82&78.54\\
			\hdashline
			YaRN&53.6& 63.22& 30.95& 75.27& 42.19&57.39\\
			NTK & 57.3& 66.43& 36.09&76.92&43.34&63.99\\
			LongRoPE&58.5&67.26& 36.28 &75.73 &46.26 &67.17 \\
			\bf	{\sysname} &\bf 61.7&\bf70.04  &\bf40.30 &\bf77.07&\bf47.61&\bf73.62\\
			\midrule
			\multicolumn{7}{c}{\bf(b) LLaMA3-8B with 128k context window}\\\midrule
			LLaMA3.1-8B & 57.2&66.33 &36.79 &81.71 &45.17  &56.18\\
			Original LLaMA3-8B (8k)&56.5 &66.62&35.87&82.08&44.04&54.05\\
			\hdashline
			YaRN&52.1& 62.25& 31.88&81.25&42.61&42.45\\
			NTK$^*$&54.0&63.84& 34.14&82.11&43.45&46.92\\
			LongRoPE &54.6& 64.69 & 33.74&\bf 82.14&43.65&48.90\\
			\bf{\sysname} &\bf 55.7&\bf 65.01 &\bf 34.61 &81.69&\bf 46.17&\bf 50.80\\
			\hline
	\end{tabular}}
\end{table}

We   present the main results of {\sysname}-extended Phi3-mini-3.8B-128k and LLaMA3-8B-128k, comparing them with models using other STOA RoPE rescaling methods.

\noindent\textbf{Long-context performance on RULER benchmark}.  Table~\ref{tbl:ruler1} compares performance on RULER, which consists of 13 synthetic tasks.  Across Phi3-mini-3.8B and LLaMA3-8B, {\sysname} consistently outperforms prior methods, achieving superior results across all evaluation lengths within the 128k window. On LLaMA3-8B, {\sysname} achieves an effective 128k context window, maintaining a strong score of 82.03 at 128k, while previous methods degrade significantly at longer contexts. For example, LongRoPE, the prior best, drops from 81.23 (64k) to 73.40 at 128k. For Phi3-mini-3.8B, {\sysname} shows even greater advantages, overcoming the challenges of the smaller model's weaker capabilities. NTK performs well below 32k and declines sharply beyond, while LongRoPE underperforms at shorter contexts. In contrast, {\sysname} consistently enhances performance across all lengths. Notably, the 128k average score of 58.81 is skewed by tasks with low scores on smaller LLMs, 
such as CWE, which achieves only 1\% accuracy. Detailed per-task score is available  in Appendix~\ref{sec:addexp}.

 

\begin{figure}[t]
	\centering
	\includegraphics[width=1\columnwidth]{needlehaystack.pdf}	
	\vspace{-5ex}
	\caption{{\sysname} (right) delivers near-perfect lossless performance in the "Needle in a Haystack" pressure test.}
	\label{fig:needle}
\end{figure}
 

\noindent\textbf{Needle in a Haystack pressure tests}. We evaluate {\sysname} using the popular long-context pressure test, Needle in a Haystack, which measures a model's ability to retrieve "needles" from long documents at varying depths. We run 10 times at the same depth 
and length.  As shown in Fig.~\ref{fig:needle}, {\sysname} achieves near-perfect accuracy across all evaluation lengths within the 128k context window. In contrast, methods like NTK often fail at longer contexts, and LLaMA3.1-8B extended by YaRN, despite being fine-tuned on 800B tokens, fails beyond 100k. These results highlight {\sysname}'s robust long-context modeling capabilities.



\noindent\textbf{Long-context performance on real-world benchmarks}.  Beyond synthetic tasks, we evaluate real-world benchmarks: LOFT (7 retrieval tasks including argumentative retrieval, fact-checking, web search, multi-hop reasoning QA, etc), InfiniteBench (key-value retrieval and multi-choice QA), and LongBench (in-context learning and code completion). Note that our models are evaluated without post-training, so scores are lower than post-training results. As shown in Table~\ref{tbl:longp_realworld}, {\sysname} consistently improves performance  across all benchmarks, demonstrating strong generalization to practical scenarios. In contrast, YaRN and NTK perform notably worse, particularly on the small Phi3-mini-3.8B. 







\noindent\textbf{Standard benchmarks at original context window}. RoPE-based context extension typically sacrifices short-context performance. As Table~\ref{tbl:short performance} shows, prior methods like YaRN, NTK, and LongRoPE exhibit notable degradation. For example, YaRN and NTK show performance drop of -15.2\% and -9.3\% oh Phi3-mini, with declines of  -21.15 and -14.55 absolute points on GSM8K. In contrast, {\sysname} retains 97.6\% and 98.6\% o0f the pre-trained performance on Phi3-mini-3.8B and LLaMA3-8B, establishing it as the first lossless extension method that preserves core capabilities. 


\subsection{Ablation Study}




\begin{table}[tp]
	\caption{Ablation study on real critical dimension.}
	\label{tbl:longfactor}
	\resizebox{1\columnwidth}{!}{	\begin{tabular}
			{@{\hskip0pt}c@{\hskip2pt}c@{\hskip6pt}c@{\hskip6pt}c@{\hskip6pt}cc@{\hskip6pt}c@{\hskip6pt}c@{\hskip6pt}c@{\hskip6pt}c@{\hskip6pt}c@{\hskip4pt}c@{\hskip0pt}}
			\hline
			\multirow{2}{*}{Method} &\multicolumn{3}{c}{Regular short tasks}& &\multicolumn{6}{c}{RULER}\\
			\cmidrule{2-4} \cmidrule{6-11}
			& MMLU & \makecell{MMLU\\Pro} & GSM8K&&4k&8k&16k&32k&64k&128k \\
			\midrule
			\multicolumn{10}{c}{Base Model: Phi3-mini (3.8B)} \\
			\rowcolor{airforceblue}	{\sysname} &\bf70.07&\bf40.30&\bf73.62&&\bf 90.41&\bf 87.22&\bf83.33&\bf 76.51&\bf 65.37&\bf58.81\\
			%	\hdashline
			YaRN&\bf 63.22&\bf30.95&\bf 57.39&&85.74 &\bf 78.68&\bf 75.97&65.22&52.16&39.37\\	
			\bf YaRN-rcd&62.30&30.24&56.48&&\bf86.56&77.66&74.48&\bf 67.73&\bf52.73&\bf44.39\\	
			\midrule
			NTK &\bf 66.43&\bf 36.09&\bf 63.99&&\bf 91.34&\bf 87.02&80.57&72.81&61.91&49.37\\
		\bf	NTK-rcd&65.31&35.09&59.29&&90.51&85.32&\bf 81.80&\bf 73.89&\bf 63.59&\bf 54.42\\	
			\midrule
			\multicolumn{10}{c}{Base Model: LLaMA3-8B} \\
			\rowcolor{airforceblue}		{\sysname}  &\bf65.01&\bf34.61&\bf50.80&& \bf 94.61&\bf 93.68& \bf 92.31& \bf90.49& \bf85.62&\bf82.03\\
			%		\hdashline
			YaRN&62.25&31.88&42.45&&91.86 &87.87 &84.67&68.80&62.51&49.39\\	
		\bf	YaRN-rcd&\bf 64.30&\bf 33.17&\bf50.34&&\bf 94.22&\bf92.02&\bf89.20&\bf82.56&\bf76.37&\bf71.46\\	
			\midrule
			NTK &63.84&34.14&\bf46.92&&\bf94.38&\bf92.64 &\bf91.93&87.33&79.26&73.19\\
		\bf	NTK-rcd&\bf64.70&\bf34.23&45.87&&94.39&92.35&91.43&\bf88.82&\bf83.22&\bf77.25\\
			\hline				
	\end{tabular}}
\end{table}


\noindent\textbf{The effectiveness of real critical dimension $d_{rcd}$}. A key factor in  {\sysname}’s superior long-context performance is its full resolution of RoPE OOD values across all dimensions. To validate this, we extend our experiments beyond {\sysname} by  applying our identified practical critical dimension $d_{rcd}$ to YaRN and NTK, yielding YaRN-rcd and NTK-rcd variants (see Fig.~\ref{fig:yarn-ntk} in Appendix~\ref{sec:addexp}). As shown in Table~\ref{tbl:longfactor}, correcting  $d_{rcd}$ improves long-context performance for both methods, revealing the inadequacy of theoretical critical dimensions in fully addressing RoPE OOD issues.
However,  correcting the critical dimension alone does not ensure optimal results. By further optimizing scaling factors, {\sysname} consistently outperforms YaRN-rcd and NTK-rcd   on both short- and long-context benchmarks.

\noindent\textbf{The effectiveness of need-PPL guided search}. {\sysname} identifies the true critical dimension and scaling factors through a needle-PPL-guided evolutionary search, which minimizes interference from irrelevant tokens to effectively capture the rescaled RoPE's long-context capabilities. To validate its effectiveness, we use 10 pure PG19 documents as a baseline, identical to those used for generating our needle-data, applying the same search and mid-training process. Table~\ref{tbl:datacompare} compares the RULER scores for Phi3-mini-3.8B-128k and LLaMA3-8B-128k, using scaling factors from two PPL-guided searches. The  results show that naive PPL-guided search fails to ensure effective rescaling factors, as it struggles to identify the correct critical dimension and tends to yield slightly smaller scaling factors.
\begin{table}[tp]
	
	\caption{Ablation study on needle-PPL guided search.}
	\label{tbl:datacompare}
	\resizebox{1\columnwidth}{!}{	\begin{tabular}
			{ccccccc}
			\hline
		 Search Metric&4k&8k&16k&32k&64k&128k \\
			\midrule
			\multicolumn{7}{c}{Base Model: Phi3-mini (3.8B)} \\
		PG19-128k PPL&\bf 91.16&\bf87.93 &83.05 &75.27&62.72&50.23\\
		\bf	PG19-Needle 128k PPL (ours) &90.41 & 87.22&\bf83.33&\bf 76.51&\bf 65.37&\bf58.81\\
			\midrule
			\multicolumn{7}{c}{Base Model: LLaMA3-8B} \\
				PG19-128k PPL&94.46 &93.36&91.67& 90.28&84.55 &78.68\\
		\bf	PG19-Needle 128k PPL (ours) &\bf 94.61 & \bf 93.68& \bf 92.31& \bf90.49& \bf85.62&\bf82.03\\
			\hline				
	\end{tabular}}
\end{table}



\noindent\textbf{The effectiveness of mixed context window training}. 
To ablate its effectiveness, we disable mixed context window training in {\sysname} and instead follow conventional mid-training with a single rescaled RoPE. As shown in Table~\ref{tbl:mixedcontextwindow}, removing mixed context window training results in a significant drop in performance on regular short-context tasks, as expected. Interestingly, mixed context window training not only preserves short performance but also improves long-context performance (8k–128k).  This may be attributed to the preservation of pre-trained RoPE for shorter contexts, allowing long-context training to focus more effectively on adapting to the new introduced token positions.

\begin{table}[tp]
	
	\caption{Ablation study on mixed context window training.}
	\label{tbl:mixedcontextwindow}
	\resizebox{1\columnwidth}{!}{	\begin{tabular}
			{@{\hskip0pt}c@{\hskip2pt}c@{\hskip6pt}c@{\hskip6pt}c@{\hskip6pt}c@{\hskip6pt}c@{\hskip6pt}c@{\hskip6pt}c@{\hskip6pt}c@{\hskip6pt}c@{\hskip4pt}c@{\hskip0pt}}
			\hline
			Method& MMLU & \makecell{MMLU\\Pro} & GSM8K&4k&8k&16k&32k&64k&128k \\
			\midrule
			\multicolumn{10}{c}{Base Model: Phi3 June} \\
			{\sysname} &\bf70.07&\bf40.30&\bf73.62&90.41&\bf 86.87&\bf83.33&\bf 76.51&\bf 65.37&\bf58.81\\
			{\sysname}/ wo. &66.56&34.86&64.67&\bf 90.55&85.77&81.08&73.31&63.75&56.22\\
			\midrule
			\multicolumn{10}{c}{Base Model: LLaMA3-8B} \\
			{\sysname}  &\bf65.01&\bf34.61&\bf50.80&94.61& \bf 93.68& \bf 92.31& \bf90.49& \bf85.62&\bf82.03\\
			{\sysname}/ wo. &64.57&33.83&48.37&\bf94.67&93.15&91.24&89.38&83.53&80.18\\
			\hline				
		\end{tabular}}
\end{table}




