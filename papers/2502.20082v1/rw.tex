\section{Background}

\noindent\textbf{Rotary Position Embedding (RoPE)}. Transformer models require explicit positional information, often in the form of position embedding, to represent the order of input tokens.  Our work builds on the Rotary  Position Embedding (RoPE)~\citep{rope}, a position encoding technique widely used in modern LLMs.  Formally, for a token at position indices $n$, the positional information is encoded using a trigonometric rotation matrix, which can be simplified as follows:
%\vspace{-1ex}
\begin{equation}
%	\fontsize{7.8}{7.8} \selectfont
	\label{eq:rope}
	\left[ cos(n\theta_0), sin(n\theta_0), cos(n\theta_1), \cdot\cdot\cdot, cos(n\theta_{d/2-1}), sin(n\theta_{d/2-1}) \right]   
\end{equation}
Here, $d$ is the embedding dimension size, 
 and $\theta_i=\theta^{-2i/d}$ is the rotation frequencies in the $i^{th}$ dimension, where the default base value of $\theta$ is 10000. 
 
In Equation~\ref{eq:rope}, RoPE for a token at position $n$ is encoded by applying a rotary angle $n\theta_i$ to each dimension using trigonometric function.  This positional information is then added to the query and key vectors before attention, which results in  the attention being a function of the relative distance between tokens. Specifically, for two positions, $m$ and $n$, its self-attentions score computes $cos((m-n)\theta_i)$ and $sin((m-n)\theta_i)$, where $m-n$ is their relative positional distance. 


% this results in  the attention being a function of the relative distance between tokens. Specifically, for two position indices $m$ and $n$,  its self-attention score computes $cos((m-n)\theta_i)$ and $sin((m-n)\theta_i)$, which means that the attention score is only dependent on relative position $m-n$ rather than their absolute position values. 


Despite its effectiveness, RoPE, like other position encodings, faces challenges in context length extrapolation.  In particular, when the input sequence length exceeds the predefined context window, the perplexity can shoot up to very high numbers (i.e., $>10^3$), comparable to untrained models. This happens because longer sequences introduce out-of-distribution relative rotation angle $(m-n)\theta_i$, which causes catastrophic values in attention computation~\citep{pi,han2023lminfinite}. 




%where $d$ is the RoPE embedding dimension, ($n\theta_i$) is the rotary angle in the $i^{th}$ dimension for the token at position $n$, and $\theta_i=\theta^{-2i/d}$ represents the rotation frequencies. In RoPE, the default base value of $\theta$ is 10000. 



\noindent\textbf{RoPE Rescaling for Context Window Extension}.  To address this OOD issue, recent works~\citep{pi,ntk,yarn,longrope} show that the context window of a pre-trained LLM can be extended by rescaling RoPE rotation angles. The insight is to downscale new position indices to fit within the pre-trained range. 
Representative methods include PI~\citep{pi}, NTK~\citep{ntk}, YaRN~\citep{yarn} and LongRoPE~\citep{longrope}.  Each offers a different way to calculate the rescaling factors. Formally, let  $\beta=\theta^{2/d}$ and $\lambda_i$ be the rescaling factor for the $i^{th}$ RoPE dimension, we unify these rescaled RoPE methods as follows:
\begin{equation}	
	\fontsize{8}{8} \selectfont
	\label{eq:rescale_rope}
	\begin{aligned}
		\left[cos\left(\frac{n}{\lambda_0(\beta^0)}\right), sin \left(\frac{n}{\lambda_0(\beta^0)}\right), cos\left(\frac{n}{\lambda_1(\beta^1)}\right), \cdot\cdot\cdot,  cos\left(\frac{n}{\lambda_i(\beta^i)}\right),  \cdot\cdot\cdot,  sin \left(\frac{n}{\lambda_{d/2-1}(\beta^{d/2-1})}\right) \right]   
	\end{aligned}
\end{equation}





\noindent\textit{Linear PI}. The value of $\lambda_i$ is typically dependent on the  target context window extension ratio $s$. For instance, PI proposes linear positional interpolation, where the rotation angles  are linearly reduced by $\lambda_i=s$ across all RoPE dimensions. However, this approach "crowds" the positional information,  making it difficult for the model to distinguish closely positioned tokens. 




\noindent\textit{NTK and YaRN}. To mitigate this issue, NTK and YaRN propose uniformly rescaling the RoPE dimensions based on  Neural Tangent Kernel theory~\citep{jacot2018neural,tancik2020fourier}. 
 The idea is that neural networks are difficult to learn high-frequency features, and direct interpolation can affect the high-frequency positional information. As a result, these approaches apply less interpolation to lower (high-frequency) RoPE dimensions and larger rescaling factors to higher (lower-frequency) dimensions to achieve more interpolation. Specifically, NTK proposes to increase the original RoPE base value $\theta$ using a formula $\lambda_i=s^{2i/(d-2)}$~\citep{ntk}.  YaRN suggests dividing RoPE dimensions into three groups: higher-frequency dimensions use direct extrapolation ($\lambda_i=1$),  lower-frequency dimensions use PI ($\lambda_i=s$), and dimensions that fall in-between employs the NTK. 
 
 %ntk:  $\theta_i=(k\theta)^{-2i/d}$, $k=(s)^{2/(d-2)}$

 \noindent\textit{Non-uniformities and LongRoPE}. 
 Despite their simplicity, NTK and YaRN require expert  tuning to configure the optimal rescaling factors, making them challenging to apply in practical LLM context window extensions. Recent study~\citep{men2024base} suggest that the current rescaled $\theta$ base in NTK is insufficient to achieved the claimed context window and needs to be larger.  However, setting the optimal value is difficult, and using a significantly larger NTK rescaled $\theta$ can negatively impact performance.
For YaRN, success heavily depends on how RoPE dimensions are grouped. However, the grouping tuned for an LLM can become ineffective for new LLMs with a different number of RoPE dimensions. 
  Later, LongRoPE~\citep{longrope} revisited these approaches and  identified subtle non-uniformities in RoPE dimensions that current methods fail to address effectively. Based on this, LongRoPE propose an efficient evolution search algorithm to automatically achieve better RoPE rescaling solutions. 
 


\begin{table}[ht]
	\centering
	\small
	\caption{Summary of RoPE rescaling methods.}
	\label{tbl:roperescaling}
	\resizebox{0.7\textwidth}{!}{	\begin{tabular}
			{ccc}
			\toprule
		&	Method &   Rescaling factors\\
		\hline
			PI &Linear Rescaling& \\
			NTK &Base value rescaling& \\
			YaRN &Group-wise Rescaling& \\
			LongRoPE &Per-dimension non-uniform Rescaling& \\
			{\sysname} & Per-dimension non-uniform Rescaling&\\
			\hline
			\end{tabular}}
	\end{table}
	
\lz{plot a figure to show the per-dimension scaling factors} 


\noindent\textbf{Long Context Fine-tuning}.  To achieve reliable performance in practical long context applications, a continuous pre-training stage is typically used to adapt model weights to the rescaled RoPE in state-of-the-art long context LLMs~\citep{llama3.1,qwen2,chatglm,deepseekcoder}.  Specifically, a target context window size is predefined, and data is chunked to match this size. During fine-tuning, all data lengths conform to this predefined context window size. 

\noindent\textbf{Short Performance Recovery}. A well-known issue is that an extended long context LLM loses  performance at the original context window, which poses a significant challenge during practical LLM training. To address this issue, LLaMA3.1~\citep{llama3.1} applies a gradual context window extension over six stages (from 8k to 128k). However, this significantly increases training complexity and costs. LongRoPE proposes using a dedicated short scaling factor specially for handling inference within the original context window size. 

