

\section{{\sysname} Methodology}

\subsection{Theoretical Analysis}
\noindent\textbf{RoPE in attention long-term decay theory}. To address these challenges, we revisit the role of RoPE in transformer architecture and identify a key property that should be factored into RoPE rescaling for long context extension.  While RoPE is well-known for its simple and effective encoding of position information, it also introduces a crucial long-term decay property. Specifically, the upper bound of  attention inner production will decay when the relative position increase as shown in Fig.~\ref{fig:ropedecay1}. This property of RoPE makes the model attend more to closer tokens, serving as valuable prior knowledge for the attention mechanism. 
Let $h_l=\mathbf{q}_{[2l:2l+1]}\mathbf{k}^*_{[2l:2l+1]}$, $S_l=\sum_{j=0}^{l-1}e^{i(m-n)\theta_j}$, the upper bound of the attention inner product in Eq.~\ref{eq:attention-1} can be written as follows: 
\begin{gather}
	\label{eq: attentiondecay1}
	\mathbf{q}^T_m\mathbf{k}_n \le \left(\max_{l}|h_{l+1}-h_l|\right)\sum_{l=0}^{d/2-1}|S_{l+1}|, \text{where}\quad  S_l=\sum_{j=0}^{l-1}e^{i(m-n)\theta_j}
\end{gather}
Then, as introduced in the original RoPE~\citep{rope}, the value of $\frac{1}{d/2}\sum_{l=1}^{d/2}|S_l|$ can be used as a measure of attention inner product decay with the relative distance $m-n$. Fig.~\ref{fig:ropedecay1} shows the long-term decay curve of this upper bound across different scales of relative position distances. 

Surprisingly, we find that this theory helps explain RoPE rescaling for context window extension. 
 \textbf{(1)} In the example of Phi3-mini, within the pretrained 2k context length, there is a clear decay property as shown in Fig.~\ref{fig:ropedecay1}(b). However, when directly extrapolating to 128k, the attention inner product beyond 2k no longer exhibits this decay, making it difficult for attention to distinguish tokens at long distances. 
Fortunately, applying RoPE rescaling methods like NTK and LongRoPE restores the long-term decay trend, allowing self-attention to effectively distinguish tokens within the 128k context window.  \textbf{(2)} Fig.~\ref{fig:ropedecay1}(a) zooms in the decay of different approaches at short relative distance, where the current token pays more attention to the tokens at these distances. NTK and LongRoPE match the pretained model's short-term decay trend. However, YaRN exhibits an anomaly between relative distances of 20-40, where attention scores increase rather than decay, leading  self-attention assigning more weight to mid-range distances than to very close ones (e.g., 0-20). 

The above analysis highlights the need to preserve the attention decay property in RoPE rescaling  for long context extension. However, current approaches like YaRN can disrupt this critical property. This raises the question: how can we preserve this property during RoPE rescaling? While this property is difficult to solve analytically, we draw inspiration from NTK's theta scaling: using a larger theta base value naturally preserves the property within long context window. This translates to applying monotonically increasing scaling factors from lower to higher RoPE dimensions (the proof is provided in appendix). We thus derive the following theorem: 

\noindent\textbf{Theorem 1}. \textit{After extending a pre-trained LLM to a long context window,  the upper bound of attention inner product must maintain its long-term decay property. This is can be ensured by applying the following constraint to RoPE rescaling factors:}
\begin{gather}
	\label{eq:theory}
	\lambda_0<\lambda_1<...<\lambda_i < \lambda_{i+1}<...<\lambda_{d/2-1}  
\end{gather}
This theorem can be viewed as a stricter version of NTK theory, which YaRN does not satisfy.
 Specifically, it addresses Question \textit{1} in Section~\ref{sec:challenegs}. \textit{Although lower RoPE dimensions do not violate RoPE OOD theory or NTK theory, they still need to be rescaled to meet Theorem 1 and maintain the attention decay property.}
Our analysis reveals that YaRN's setting of scaling factors for high-frequency RoPE dimensions (i.e., $\lambda_i=1, i\le d_\text{high\_freq}$) to 1 causes the anomalous increase in attention upper bound at short-term distances.
NTK naturally adheres to this theorem, while LongRoPE, due to its monotonically non-decreasing scaling factors, does not strictly fulfill it.





\noindent\textbf{The actual RoPE  periods in higher dimensions are shorter than theoretical values}. Another fundamental question  is: why are scaling factors beyond the critical dimension required to be larger than the theoretical value $s$? Our hypothesis is that these higher dimensions do not receive sufficient training during pretraining, resulting in shorter rotation periods than the theoretical values as expected. 

Specifically, $\theta_i (m-n)$ represents the RoPE values at the $i^{th}$ dimension during pre-training. Higher rotation frequencies $\theta_i$ in lower dimensions enable them to complete their full rotation periods easily.  For instance,  the 8$^{th}$ dimension requires sufficient training with token relative distances of $m-n=24$ to complete one full period (see Fig.~\ref{fig:ropeperiod}). In contrast, the critical 31$^{st}$ $consine$ dimension requires relative distances of $m-n=2048$  to complete its period. While token pairs with relative distances of 24 are common in the  pretraining corpus, long token dependencies like 2048 are relatively rare in real-world data. Moreover, 2048 corresponds to Phi3's pretrained context window size. This explains why larger scaling factors than $s$ in higher RoPE dimensions usually achieve better performance. Then, we propose Hypothesis 1 as follows: 

\noindent\textbf{Hypothesis 1}. \textit{Insufficient training in higher RoPE dimensions results in actual RoPE periods being shorter than the theoretical values of $\frac{2\pi}{\theta_i}$. Consequently, this requires larger scaling factors in these dimensions, and the actual critical dimension index is smaller than the theoretical value in Eq.~\ref{eq:cd}.}





\subsection{Search}

\noindent\textbf{Synthetic long data to guide the search}. 

\noindent\textbf{Search for practical critical dimension}.

\noindent\textbf{}




\subsection{Mixed Context Window Training for Short Performance Recovery}

\noindent\textbf{Mixed Context Windows Training}

1. per batch data must be same length 


efficiency 

loss: 

2. 

shuffle: 
