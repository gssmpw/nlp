\section{Efficiency Analysis}
\label{appx:efficiency}
\begin{table}[h]
\centering
\caption{Computational efficiency comparison on ETTh1 dataset. We report numbers of trainable parameters and inference time (in milliseconds) across different prediction horizons (H).}
\label{tab:efficiency}
\begin{tabular}{r|c|cccc}
\toprule
\multirow{2}{*}{Model} & \multirow{2}{*}{\# Parameters} & \multicolumn{4}{c}{Inference Time (ms)} \\
\cmidrule{3-6}
& & H = 96 & H = 192 & H = 336 & H = 720 \\
\midrule
LDM4TS (Ours) & 5.4M & 76.88 & 80.31 & 193.44 & 192.19 \\
\midrule
TimeGrad & 3.1M & 870.2 & 1854.5 & 3119.7 & 6724.1 \\
ScoreGrad & 4.65M & 3.44 & 4.53 & 4.22 & 7.81 \\ 
CSDI & 10M & 90.4 & 142.8 & 398.9 & 513.1 \\
SSSD & 32M & 418.6 & 645.4 & 1054.2 & 2516.9 \\
\bottomrule
\end{tabular}
\end{table}

We conduct a comprehensive efficiency analysis of LDM4TS, focusing on computational costs and model complexity through extensive experiments on the ETTh1 dataset. Our analysis encompasses both inference time measurements across various prediction horizons and parameter count comparisons with other popular diffusion-based models.

As shown in Table~\ref{tab:efficiency}, LDM4TS demonstrates substantial improvements in inference efficiency over most diffusion-based competitors. For shorter horizons (H=96,192), our model achieves inference times of 76.88ms and 80.31ms respectively, significantly outperforming TimeGrad (870.2ms, 1854.5ms) and SSSD (418.6ms, 645.4ms). Note that for H=720, we observe a slightly faster inference time than H=336, which may be attributed to varying GPU resource availability during our experiments. While ScoreGrad shows faster inference times, our extensive experiments demonstrate that LDM4TS achieves superior forecasting accuracy, offering a better trade-off between efficiency and performance.

The current results indicate that LDM4TS successfully mitigates the computational bottleneck typical of diffusion-based forecasting through our innovative architectural design, including VAE encoding and efficient cross-modal fusion mechanisms. Though traditional models like transformers and linear models maintain faster inference speeds due to their simpler architectures, our ongoing work focuses on further optimization to make diffusion-based forecasting more practical while preserving its superior modeling capabilities.