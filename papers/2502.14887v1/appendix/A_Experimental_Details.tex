\section{Experimental Details}
\subsection{Dataset Details}
\label{appx:dataset_details} 
\input{table/dataset_detail}
We conduct experiments on the above real-world datasets to evaluate the performance of our proposed model and follow the same data processing and train-validation-test set split protocol used in TimesNet benchmark~\cite{wu2022timesnet}, ensuring a strict chronological order to prevent data leakage. Different datasets require specific adjustments to accommodate their unique characteristics:
\label{appx:dataset_configurations}

\paragraph{ETT Dataset~\cite{kim2021reversible}} The Electricity Transformer Temperature (ETT) dataset consists of both hourly (ETTh) and 15-minute (ETTm) frequency data, with 7 variables ($enc\_{in}$ = $dec\_{in}$ = $c\_{out}$ = 7) measuring transformer temperatures and related factors. For ETTh data, we set periodicity to 24 with hourly frequency, while ETTm data uses a periodicity of 96 with 15-minute intervals. Standard normalization is applied to each feature independently, and the model maintains the same architectural configuration across both temporal resolutions.

\paragraph{Traffic Dataset~\cite{wu2022timesnet}} The traffic flow dataset represents a high-dimensional scenario with 862 variables capturing traffic movements across different locations. To handle this scale, we implement gradient checkpointing and efficient attention mechanisms, complemented by progressive feature loading. The batch size is dynamically adjusted based on available GPU memory, and we maintain a periodicity of 24 to capture daily patterns. Our model employs specialized memory optimization techniques to process this large feature space efficiently.

\paragraph{ECL Dataset~\cite{wu2021autoformer}} The electricity consumption dataset contains 321 variables monitoring power usage patterns. We employ robust scaling techniques to handle outliers and implement sophisticated missing value imputation strategies. The model incorporates adaptive normalization layers to address the varying scales of electricity consumption across different regions and time periods. The daily periodicity is preserved through careful temporal encoding, while the high feature dimensionality is managed through efficient attention mechanisms.

\paragraph{Weather Dataset~\cite{wu2021autoformer}} This multivariate dataset encompasses 21 weather-related variables, each with distinct physical meanings and scale properties. Our approach implements feature-specific normalization to handle the diverse variable ranges while maintaining their physical relationships. The model captures both daily and seasonal patterns through enhanced temporal encoding, with special attention mechanisms designed to model the complex interactions between different weather variables. We maintain consistent prediction quality across all variables through carefully calibrated cross-attention mechanisms.


\subsection{Optimization Settings}
\label{appx:optimization_settings}

\subsubsection{Model Architecture Parameters}
\label{appx:model_parameters}
\input{table/model_parameters}
The core architecture of our diffusion-based model consists of several key components, each with specific parameter settings. The autoencoder pathway is configured with an image size of $64\times64$ and a patch size of $16$, providing an efficient latent representation while maintaining temporal information. The diffusion process uses $1000$ timesteps with carefully tuned noise scheduling ($\beta_{start} = 0.00085, \beta_{end} = 0.012$) to ensure stable training.

For the transformer backbone, we employ a configuration with $d_model = 256$ and $8$ attention heads, which empirically shows strong performance across different datasets. The encoder-decoder structure uses $2$ encoder layers and $1$ decoder layer, with a feed-forward dimension of $768$, striking a balance between model capacity and computational efficiency.

\subsubsection{Training Parameters}
\label{appx:training_settings}
\input{table/training_parameters}
We adopt a comprehensive training strategy with both general and task-specific parameters. The model is trained with a batch size of $32$ and an initial learning rate of $0.001$, using the \textit{AdamW} optimizer. Early stopping with a patience of $3$ epochs is implemented to prevent over-fitting. For time series processing, we use a sequence length of $96$ and a prediction length of $96$, with a label length of 48 for teacher forcing during training.

The training process employs automatic mixed precision (AMP) when available to accelerate training while maintaining numerical stability. We use MSE as the primary loss function, supplemented by additional regularization terms for specific tasks.

\subsection{Evaluation Metrics}
\label{appx:metric}
For evaluation metrics, we utilize the mean square error (MSE) and mean absolute error (MAE) for long-term forecasting. 
% In terms of the short-term forecasting on M4 benchmark, we adopt the symmetric mean absolute percentage error (SMAPE), mean absolute scaled error (MASE), and overall weighted average (OWA) as in N-BEATS \citep{oreshkin2019n}. Note that OWA is a specific metric utilized in the M4 competition. 
The calculations of these metrics are as follows:
\begin{align*} \label{equ:metrics}
    \text{MSE} &= \frac{1}{H}\sum_{h=1}^T (\mathbf{Y}_{h} - \Hat{\mathbf{Y}}_{h})^2,
    &
    \text{MAE} &= \frac{1}{H}\sum_{h=1}^H|\mathbf{Y}_{h} - \Hat{\mathbf{Y}}_{h}|,\\
    % \text{SMAPE} &= \frac{200}{H} \sum_{h=1}^H \frac{|\mathbf{Y}_{h} - \Hat{\mathbf{Y}}_{h}|}{|\mathbf{Y}_{h}| + |\Hat{\mathbf{Y}}_{h}|},
    % &
    % \text{MAPE} &= \frac{100}{H} \sum_{h=1}^H \frac{|\mathbf{Y}_{h} - \Hat{\mathbf{Y}}_{h}|}{|\mathbf{Y}_{h}|}, \\
    % \text{MASE} &= \frac{1}{H} \sum_{h=1}^H \frac{|\mathbf{Y}_{h} - \Hat{\mathbf{Y}}_{h}|}{\frac{1}{H-s}\sum_{j=s+1}^{H}|\mathbf{Y}_j - \mathbf{Y}_{j-s}|},
    % &
    % \text{OWA} &= \frac{1}{2} \left[ \frac{\text{SMAPE}}{\text{SMAPE}_{\textrm{Naïve2}}}  + \frac{\text{MASE}}{\text{MASE}_{\textrm{Naïve2}}}  \right],
\end{align*}
where $s$ is the periodicity of the time series data. $H$ denotes the number of data points (i.e., prediction horizon in our cases). $\mathbf{Y}_{h}$ and $\Hat{\mathbf{Y}}_{h}$ are the $h$-th ground truth and prediction where $h \in \{1, \cdots, H\}$.

\section{Details of Baseline Methods}
\label{appx:baselines}
We compare our approach with three categories of baseline methods used for comparative evaluation: transformer-based architectures, diffusion-based models, and other competitive approaches for time series forecasting.
\paragraph{Transformer-based Models:}
\textbf{FEDformer~\cite{zhou2022fedformer}} integrates wavelet decomposition with a Transformer architecture to efficiently capture multi-scale temporal dependencies by processing both time and frequency domains. 
\textbf{Autoformer~\cite{wu2021autoformer}} introduces a decomposing framework that separates the time series into trend and seasonal components, employing an autocorrelation mechanism for periodic pattern extraction.
\textbf{ETSformer~\cite{woo2022etsformer}} extends the classical exponential smoothing method with a Transformer architecture, decomposing time series into level, trend, and seasonal components while learning their interactions through attention mechanisms.
\textbf{Informer~\cite{zhou2021informer}} addresses the quadratic complexity issue of standard attention mechanisms through ProbSparse self-attention, which enables efficient handling of long input sequences.
\textbf{Reformer~\cite{kitaev2020reformer}} optimizes attention computation via Locality-Sensitive Hashing (LSH) and reversible residual networks, significantly reducing memory and computational costs.
\paragraph{Diffusion-based Models:}
\textbf{CSDI~\cite{tashiro2021csdi}} is tailored for irregularly-spaced time series, learning a score function of noise distribution under given conditions to generate samples for forecasting.
\textbf{ScoreGrad~\cite{song2020score}} utilizes a continuous-time framework for progressive denoising from Gaussian noise to reconstruct the original signal, allowing for adjustable step sizes during the denoising process.
\paragraph{Other Competitive Models:}
\textbf{DLinear~\cite{zeng2023transformers}} proposes a linear transformation approach directly on time series data, simplifying the prediction process under the assumption of linear changes over time.
\textbf{TimesNet~\cite{wu2022timesnet}} focuses on multi-scale feature extraction using various convolution kernels to capture temporal dependencies of different lengths, automatically selecting the most suitable feature scales.
\textbf{LightTS~\cite{campos2023lightts}} aims to build lightweight time series forecasting models, streamlining structures and parameters to reduce computational resource requirements while maintaining high predictive performance.

Each baseline method represents distinct paradigms within probabilistic generative modeling, attention-based architectures, and linear models, providing a comprehensive benchmark against which to evaluate LDM4TS.