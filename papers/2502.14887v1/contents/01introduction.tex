\vspace{-2em}
\section{Introduction}
Time Series Forecasting (TSF) is a critical task in numerous real-world applications~\cite{jin2024survey}, such as demand planning~\cite{leonard2001promotional}, energy load estimation~\cite{liu2023sadi}, climate modeling~\cite{schneider1974climate}, and traffic flow management~\cite{zheng2006short}. Deep learning models have significantly advanced time series forecasting by capturing intricate temporal dependencies. Early methods introduced sequential modeling capabilities~\cite{cho2014learning,hochreiter1997long}, while later architectures such as Transformers have improved the modeling of long-range dependencies and efficiency~\cite{nie2022time,zhou2021informer,zhou2022fedformer,wu2021autoformer,woo2022etsformer}. Despite their success, these models lack the ability to model the underlying uncertainty behind the time series.
%struggle to generalize across diverse domains, particularly under distributional shifts or with limited data.

In parallel with these developments, diffusion models have emerged as powerful generative frameworks, excelling in tasks like text-to-image generation~\cite{rombach2022high}, image synthesis~\cite{ho2020denoising}, and super-resolution~\cite{saharia2022palette}. Their iterative denoising process demonstrates exceptional capability in modeling complex distributions, prompting recent applications to time series modeling~\cite{rasul2021autoregressive,shen2024multi,shen2023non,tashiro2021csdi,yan2021scoregrad}. However, these models face significant challenges in time series forecasting as they are inherently designed for spatially structured data like images, while time series exhibits fundamentally different characteristics such as sequential dependencies and non-stationarity~\cite{box2015time,hamilton2020time}. The potential of transforming temporal patterns into structured visual representations, rather than directly processing raw sequences, remains largely unexplored for leveraging diffusion-based temporal dynamics.
%However, diffusion models are inherently designed for spatially structured data like images, while time series data presents characteristics such as sequential dependencies, non-stationarity, and irregular sampling patterns~\cite{box2015time,hamilton2020time,priestley1988non,eckner2014algorithms}. Additionally, directly applying diffusion processes to raw sequential signals without considering their inherent temporal structures fails to effectively capture and preserve the rich dynamics essential for accurate forecasting.


\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{figure/method_comparison.pdf}
  \caption{Comparison between traditional TSF methods and vision-enhanced approach, highlighting our method leverages multi-view visual representations to enhance TSF.}
\vspace{-1.5em}
\label{fig:method_comparison}
\end{figure}
Meanwhile, previous research has demonstrated that transforming time series data into visual representations can effectively preserve crucial temporal structures, including periodicity, seasonality, and trends~\cite{eckmann1995recurrence, Oord2016WaveNetAG,wang2015encoding}. These visual representations enable the application of pre-trained vision models, which are adept at capturing local and global patterns, to time series forecasting~\cite{wu2022timesnet,chen2024visionts}. However, current time series visual modeling approaches still face several critical limitations: (i) Most existing methods rely heavily on single-view transformations, which may not fully capture the complex temporal dynamics inherent in time series data; (ii) While visual features extracted from time series data contain rich structural information, they lack specific mechanisms for temporal reasoning and future state prediction; (iii) The integration of uncertainty quantification and probabilistic forecasting remains largely unexplored in vision-based approaches. 

These observations highlight the need for a unified framework that synergistically integrates the complementary strengths of visual representations, diffusion models, and temporal modeling. To bridge this gap, we present \model, the first attempt to leverage vision-enhanced time series encoding into the latent diffusion model, by reformulating time series forecasting as an image reconstruction and cross-modal fusion task as illustrated in Figure~\ref{fig:method_comparison}. \model combines the probabilistic uncertainty modeling capabilities with sophisticated multi-view vision-enhanced temporal dependency learning. Through multiple complementary vision transformations and a specialized temporal fusion mechanism, our approach effectively captures both global patterns and local dynamics while maintaining efficiency.

Specifically, we first transform raw time series data into multi-view visual representations using structured encodings, including segmentation, recurrence plots and Gramian angular fields. These visual representations are then mapped into a low-dimensional latent space, where a pre-trained latent diffusion model progressively denoises the latent variables. To further increase the flexibility of the model, we condition the frequency embedding and textual embedding to capture domain-specific knowledge or statistical properties of the time series through cross-attention mechanisms. Finally, a temporal projection and fusion module is introduced to extract temporal dependencies from the reconstructed representations and predict future time series. In summary, the key contributions of this work are as follows:
% \begin{itemize}
%\item \textbf{A Multi-view Vision-Enhanced Perspective:} 

\noindent \textbf{1) Multi-view Visual Representations:} 
We present the first work that utilizes complementary strategies to transform temporal features into multi-view visual representations, enabling both crucial temporal property preservation and enhanced complex temporal pattern recognition via pre-trained vision models.

\noindent \textbf{2) Latent Diffusion Framework with Multi-modal Conditions:} We pioneer \model, a unified framework that leverages latent diffusion processes, combined with visual representations and cross-modal conditioning mechanism, to effectively integrate visual pattern extraction, uncertainty modeling, and temporal dependency learning for TSF.

\noindent \textbf{3) Comprehensive Empirical Validation:} Extensive experiments verify that \model achieves state-of-the-art performance on diverse datasets, outperforming existing diffusion-based methods by at least 65.2\% in terms of MSE while achieving at least 15.7\% improvement over the runner-up.
% \end{itemize}