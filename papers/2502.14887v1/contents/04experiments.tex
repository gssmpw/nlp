\input{table/long_term_forecasting}
\input{table/few_shot}

\section{Experiments}
\label{sec:experiments}
\subsection{Settings}
\paragraph{Dataset and Metrics} % 数据集介绍
In this section, we evaluate the proposed \model on seven widely-used time series datasets, covering diverse domains including energy consumption (ETTh1, ETTh2, ETTm1, ETTm2), weather forecasting, electricity load prediction (ECL, 321 variables), and traffic flow estimation (Traffic, 862 variables)~\cite{zhou2021informer, lai2018modeling}, which have been extensively adopted for benchmarking long-term forecasting models~\cite{wu2022timesnet}. These datasets are chosen for their varying characteristics in terms of sampling frequency, dimensionality, and temporal patterns. Forecasting performance is measured using Mean Absolute Error (MAE) and Mean Squared Error (MSE), following standard practice in the field. Details are provided in Appendix~\ref{appx:dataset_details} and~\ref{appx:metric}.
\paragraph{Compared Methods} % baseline
We compare with 1) transformer-based methods: %PatchTST\cite{nie2023patchtst},
%ESTformer\cite{woo2022etsformer}, 
%Non-Stationary Transformer\cite{woo2022etsformer},
FEDformer~\cite{zhou2022fedformer}, Autoformer~\cite{wu2021autoformer}, Informer~\cite{zhou2021informer}, 
ETSformer~\cite{woo2022etsformer}, and Reformer~\cite{kitaev2020reformer}. 
2) diffusion-based methods: 
CSDI~\cite{tashiro2021csdi} and ScoreGrad~\cite{yan2021scoregrad}. 
3) a set of recent competitive models, including 
DLinear~\cite{zeng2023transformers}, TimesNet~\cite{wu2022timesnet}, 
and LightTS~\cite{zhang2022less}. 
These baselines represent state-of-the-art approaches in time series forecasting, encompassing different methodological paradigms from probabilistic generative modeling to attention-based architectures and linear models. More details are in Appendix~\ref{appx:baselines}.

\paragraph{Implementation Details} % 实现细节
The models are trained using the Adam optimizer with a learning rate of $10^{-3}$, batch size of $32$, and a maximum of $10$ epochs, applying an early stopping strategy. The number of diffusion steps is set to $T=300$, with a linear variance schedule from $\beta_1=0.00085$ to $\beta_K=0.012$. The validation set determines the history window length (selected from $\{96, 168, 336, 720\}$). All experiments are conducted on an Nvidia RTX A6000 GPU with 48GB memory. All training and model parameter settings with default values are detailed in Appendix~\ref{appx:optimization_settings}.
% We compare \method against state-of-the-art models using a unified evaluation pipeline\footnote{https://github.com/thuml/Time-Series-Library}, following the configurations in \citep{wu2022timesnet} for fair comparison.
\subsection{Long-term Forecasting}
We evaluate the long-term forecasting capabilities of \model across multiple prediction horizons. As shown in Table~\ref{tab:long_term_results}, \model consistently outperforms state-of-the-art baselines, achieving optimal results in both MSE and MAE. On the ETT dataset family, our approach demonstrates significant improvements, achieving the best MSE of 0.352 on ETTm1 compared to the second-best performer DLinear (0.404), and reducing MSE by 11.8\% on ETTh2 (0.387) compared to FEDformer (0.439). The advantages extend to high-dimensional scenarios, achieving superior results on both Electricity (321 variables, MSE: 0.199 vs TimesNet 0.208) and Traffic datasets (862 variables, MSE: 0.550 vs DLinear 0.624), which is mainly benefited by our latent diffusion framework that compresses high-dimensional temporal patterns into a lower-dimensional latent space while preserving essential structural information through vision transformations. Notably, \model substantially outperforms existing diffusion-based methods CSDI and ScoreGrad across all datasets, with MSE improvements of up to 84.2\% and 89.4\% on the Traffic dataset respectively. Overall, \model achieves the best performance in 5 out of 7 datasets on each metric, validating that our vision-enhanced modeling strategy effectively captures complex temporal dynamics across diverse forecasting scenarios. %Detailed results and analyses are provided in Appendix~\ref{appx:long_term_details}.

\subsection{Few-shot Forecasting}
% To evaluate model robustness under data scarcity, we conduct experiments using only 10\% of the training data. As shown in Table~\ref{tab:few_shot}, \model achieves optimal performance on 6 out of 7 datasets in both MSE and MAE, demonstrating strong few-shot learning capabilities. On the ETT benchmark series, \model consistently outperforms state-of-the-art methods like FEDformer and TimesNet across all prediction horizons, with notable MSE reductions: 26.2\% on ETTh1 (0.471 vs 0.638), 3.2\% on ETTh2 (0.451 vs 0.466), and 9.7\% on ETTm1 (0.371 vs 0.411). The advantages are particularly significant in long-horizon predictions (720 timesteps), where \model exhibits superior stability under data scarcity.
% The strong performance extends to high-dimensional scenarios, where \model exhibits robust scalability even with limited training data. On the Electricity dataset with 321 variables, \model achieves an MSE of 0.172, outperforming DLinear (0.180) by 4.4\%. For the more challenging Traffic dataset with 862 variables, \model demonstrates consistent superiority with an MSE of 0.621, outperforming FEDformer (0.663, 6.3\% reduction) and TimesNet (0.951, 34.7\% reduction). Beyond traditional architectures, \model also shows significant advantages over diffusion-based methods like CSDI and ScoreGrad, reducing average MSE by 25.6\% and 31.2\% respectively. The performance improvement mainly comes from our visual-temporal modeling strategy, which can effectively leverage pre-trained knowledge to maintain reliable forecasting performance even with minimal training samples, addressing a critical challenge in real-world time series applications.
To evaluate model robustness under data scarcity, we conduct experiments using only 10\% and 5\% of the training data. As shown in Table~\ref{tab:few_shot}, \model achieves optimal performance on 6 out of 7 datasets in both MSE and MAE metrics. On the ETT benchmark series, \model consistently outperforms state-of-the-art methods, with notable MSE reductions: 26.2\% on ETTh1 (0.471 vs 0.638), 3.2\% on ETTh2 (0.452 vs 0.466), and 9.7\% on ETTm1 (0.371 vs 0.411). The advantages extend to high-dimensional scenarios, with \model outperforming DLinear by 4.4\% on Electricity (0.172 vs 0.180) and FEDformer by 6.3\% on Traffic (0.621 vs 0.663). Notably, \model shows significant improvements over diffusion-based methods CSDI and ScoreGrad, reducing average MSE by 25.6\% and 31.2\% respectively. Even with further reduced 5\% training data, \model maintains strong performance by achieving the best results on 5 MSE and 6 MAE metrics across datasets. The consistently robust performance under extreme data scarcity demonstrates how our vision-enhanced approach captures intrinsic patterns to address fundamental challenges in real-world forecasting applications.

\input{table/few_shot_5p}
\input{table/zero_shot}

\subsection{Zero-Shot Forecasting}
To evaluate cross-domain generalization, we conduct zero-shot transfer experiments across different datasets without any fine-tuning. As shown in Table~\ref{tab:zero_shot_results}, \model achieves the best performance in 4 MSE and 5 MAE metrics out of 8 scenarios, demonstrating strong cross-domain transferability. For challenging transfer tasks like $ETTh1\rightarrow ETTh2$ and $ETTh1\rightarrow ETTm2$, \model achieves MSE of 0.458 and 0.369 respectively, outperforming both DLinear (0.493, 0.415) and FEDformer (0.495, 0.373). The model also achieves the best of on $ETTm1\rightarrow ETTh1$ (0.452, 0.434) and $ETTm2\rightarrow ETTm1$ (0.588, 0.487). The advantages are particularly pronounced when compared to diffusion models, with \model achieving substantial improvements over both CSDI and ScoreGrad across all transfer scenarios, reducing MSE by up to 49.0\% in challenging tasks. Notably, while most baseline methods show significant performance degradation in cross-dataset transfers, \model maintains consistent performance across different transfer pairs, suggesting robust generalization capabilities. %Detailed transfer results and analyses are in Appendix~\ref{appx:zero_shot_details}.

% To evaluate the model's generalization capability across different domains, we conduct zero-shot transfer experiments where models trained on one dataset are directly tested on another dataset without any fine-tuning. As shown in Table~\ref{tab:zero_shot_results}, \model exhibits remarkable transfer learning abilities, achieving the best performance in 5 out of 8 scenarios on MAE metric. Particularly noteworthy are the challenging transfer tasks $ETTh1\rightarrow ETTh2$ and $ETTh1\rightarrow ETTm2$, where \model achieves MSE of 0.458 and 0.369 respectively, outperforming both traditional approaches like FEDformer (0.495, 0.373) and previous diffusion-based methods like CSDI (0.500, 0.410). 
% Our analysis reveals that \model demonstrates consistent superiority across different transfer scenarios. Compared with transformer-based methods, \model reduces average MSE by 3.8\% versus the next best Autoformer in the $ETTm1\rightarrow ETTh2$ task. The advantages are even more pronounced when compared to diffusion models, with 43.4\% lower MSE than CSDI and 42.1\% lower than ScoreGrad in the $ETTm2\rightarrow ETTm1$ task. Notably, while baseline methods show significant performance degradation in challenging transfer scenarios \model maintains remarkable stability. These comprehensive results demonstrate that our vision-enhanced modeling strategy effectively captures transferable patterns, enabling robust cross-domain generalization without requiring domain-specific adaptation.

\subsection{Model Analysis}
\paragraph{Overall Performance Analysis}
% \model demonstrates superior performance across various forecasting scenarios, excelling in long-term few-shot, and zero-shot predictions. Through comprehensive experiments, we observe that our approach effectively captures both global trends and local patterns in time series data. As shown in Figure~\ref{fig:vis_forecast}, \model achieves good performance in forecasting structured patterns, such as the clear daily cycles in Traffic datasets (MSE: 0.621) and regular consumption patterns in ECL data (MSE: 0.199). However, the performance shows slight degradation on datasets with irregular patterns or abrupt changes, suggesting potential areas for future improvement in handling non-stationary patterns.
\model demonstrates superior performance across various forecasting scenarios, excelling in long-term few-shot, and zero-shot predictions, while maintaining computational efficiency with only 5.4M learnable parameters and fast inference speed (see Append~\ref{appx:efficiency} for detailed analysis). Through comprehensive experiments, we observe that our approach effectively captures both global trends and local patterns in time series data. As shown in Figure~\ref{fig:vis_forecast}, \model achieves good performance in forecasting structured patterns, such as the clear periods in Traffic datasets (MSE: 0.621) and regular consumption patterns in ECL data (MSE: 0.199). The performance shows slight degradation on datasets with irregular patterns or abrupt changes, suggesting potential areas for future improvement in handling non-stationary patterns.

\input{table/ablation_components}
\paragraph{Visual Encoding Effectiveness}
Figure~\ref{fig:visual_cases} illustrates our multi-view vision transformation strategy, which forms the foundation of \model's strong performance. Each encoding method captures distinct temporal characteristics: SEG preserves local temporal structures through periodic restructuring, enabling detection of recurring patterns at multiple time scales; GAF transforms temporal correlations into spatial patterns through polar coordinate mapping, effectively capturing long-range dependencies; and RP generates similarity matrices that highlight both cyclical behaviors and temporal anomalies. The complementary nature of these encodings is particularly evident in the ETT datasets, where the combination achieves a 24.2\% reduction in MSE compared to using any single encoding method.

\begin{figure}[!ht]
  \centering
\includegraphics[width=\linewidth]{figure/Vis_Forecasting.pdf}
  \caption{Visualization results of long-term forecasting by LDM4TS model on all datasets under the input-96-predict-96 setting. Detailed comparisons with baselines on the ETTh1 dataset are in the Appendix~\ref{appx:showcases}.}
\vspace{-2mm}
\label{fig:vis_forecast}
\end{figure}
\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.95\linewidth]{figure/vision_cases.pdf}
  \caption{Visualization of multi-view visual representation after transformation. Each row shows one approach, top row: Segmentation (SEG); middle row: Gramian Angular Field (GAF); and bottom row: Recurrence Plot (RP). More detailed results are provided in Appendix~\ref{appx:visualization_picel_space}.}
\vspace{-1mm}
\label{fig:visual_cases}
\end{figure}
\begin{figure}[!ht]
  \centering
  \includegraphics[width=\linewidth]{figure/Vis_hyperparameter.pdf}
  \caption{Hyperparameter sensitivity analysis. The impact of sequence length (left), model dimension (middle), and fusion dimension (right) on prediction performance on the ETTh1 dataset.}
\label{fig:vis_parameter}
\vspace{-1mm}
\end{figure}
% 消融实验: 关键组件和超参数敏感度
\paragraph{Ablation Studies}
Table~\ref{tab:ablation_components} presents ablation studies on key components of \model. Both vision encoder and temporal encoder prove to be crucial, with their removal leading to significant performance degradation (10.57\% and 10.96\% MSE increase respectively), validating that our visual representations successfully capture and align with temporal characteristics. The latent diffusion module also plays a vital role (5.57\% MSE increase when removed), demonstrating effective bridging between image reconstruction and time series prediction. Frequency conditioning shows minimal impact (0.59\% MSE increase) due to information redundancy. Unexpectedly, removing the text conditioning resulted in a performance drop, suggesting the textual information may lack an underlying temporal statistical pattern.
We further performed a parameter sensitivity analysis to investigate the effect of key hyperparameters on the model performance, as shown in Figure~\ref{fig:vis_parameter}(a) shows the best performance at around 512 timesteps as input sequence length, while the performance of longer sequences decreases due to increased noise. The model hidden dimension shows an optimum point between 32 and 64, balancing model capacity and risk of overfitting. For the hidden dimension of the fusion module, values between 64 and 128 produce better results, suggesting that compact representations are more effective for integrating cross-modal information.