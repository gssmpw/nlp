\section{Methodology}
\label{sec:method}
% 概述整体框架的设计思路和主要组件
% In this section, we present our LDM4TS framework as illustrated in Figure~\ref{fig:framework}, 
As illustrated in Figure~\ref{fig:framework}, \model employs a cross-modal architecture combining vision transformation and diffusion-based generation for time series processing. The framework first transforms raw time series data through complementary encoding methods, generating multi-view visual representations that capture diverse temporal patterns. These visual representations are then processed by a frozen latent diffusion model, which learns to reconstruct and predict temporal patterns under the guidance of multimodal conditioning. The model generates final predictions through a gated fusion mechanism that integrates the diffusion outputs with projected temporal features.

\subsection{Time Series to Multi-view Vision Transformation}
% 说明时序数据的特点和视觉转换的必要性
Time series data exhibits complex temporal patterns across multiple views, from local fluctuations to long-term trends, making direct modeling challenging. While existing methods predominantly rely on sequential architectures to capture temporal dependencies, they often fail to fully leverage the rich structural correlations embedded within time series data. To address these limitations, we propose a novel approach that transforms time series into visual representations, capturing multi-view temporal characteristics through a vision encoder (VE) and harnessing the sophisticated pattern recognition capabilities of latent diffusion models. Given an input sequence $X \in \mathbb{R}^{B \times L \times D}$, where $B$ denotes the batch size, $L$ represents the sequence length, and $D$ indicates the feature dimension, we construct a three-channel image representation through complementary encoding methods. The technical details of the complete transformation process are presented in Appendix~\ref{appx:transformation} and \ref{appx:visualization_picel_space}.

% 详细解释三种视觉转换方法的原理和优势
We transform time series data into three complementary visual representations, each designed to capture distinct temporal characteristics. Specifically, (i) the Segmentation representation (SEG)~\cite{chen2024visionts} that employs periodic restructuring to preserve local temporal structures, enabling the detection of recurring patterns across multiple time scales; (ii) the Gramian Angular Field (GAF)~\cite{zheng2014time,wang2015encoding} that transforms temporal correlations into spatial patterns through polar coordinate mapping, effectively capturing long-range dependencies crucial for forecasting; and (iii) the Recurrence Plot (RP)~\cite{eckmann1995recurrence,marwan2007recurrence} that constructs similarity matrices between time points to reveal both cyclical behaviors and temporal anomalies, providing a complementary view of the underlying structure. As demonstrated in Figure~\ref{fig:visual_cases}, these three visual encoding strategies effectively convert temporal dynamics into structured spatial patterns, enabling our model to capture local dependencies and global correlations through the diffusion process. The complete transformation process can be formalized as follows:
\begin{equation}
\small
\tilde{X} = \frac{X - \min(X)}{\max(X) - \min(X) + \epsilon}
\end{equation}
\begin{equation}
\small
I_{SEG} = \frac{1}{D}\sum_{d=1}^D \psi(f_{interp}(\mathcal{R}(\text{Pad}(\tilde{X}_{:,d,:}), \frac{L+p}{T}, T)))
\end{equation}
\begin{equation}
\small
I_{GAF} = f_{interp}(\frac{1}{D}\sum_{d=1}^D \cos(\theta_d \oplus \theta_d^T))
\end{equation}
\begin{equation}
\small
I_{RP} = f_{interp}(\exp(-\frac{\|X_i - X_j\|_2^2}{2}))
\end{equation}
\begin{equation} 
\small
I_m = \text{Concat}[I_{SEG}; I_{GAF}; I_{RP}]
\end{equation}
% 完整的转换过程及公式解释
where $\mathcal{R}(X,m,n)$ transforms tensor $X$ into an $m \times n$ matrix for periodic pattern extraction, $\text{Pad}(\cdot)$ ensures sequence length divisibility by period $T$, $f_{interp}$ performs bilinear interpolation to target size $(H,W)$, $\psi(\cdot)$ normalizes each channel independently, and $\oplus$ denotes the outer sum operation. The resulting multi-channel image $I_m\in \mathbb{R}^{B \times 3 \times H \times W}$ integrates complementary views of temporal dynamics.

\subsection{Latent Diffusion for Time Series Reconstruction}
%LDM相较于其他diffusion模型的优势
Unlike conventional diffusion models that operate in high-dimensional pixel space, we perform the denoising process in a compressed latent space, significantly reducing computational complexity while preserving temporal dynamics. Our framework extends Stable Diffusion~\cite{rombach2022high} with specialized adaptations for time series data through cross-modal conditional control and enhanced temporal modeling. The algorithm details are in Appendix~\ref{appx:ldm_algorithm}.
\paragraph{Multi-conditional Generation Framework}
% 详细解释条件生成机制
To guide accurate temporal feature reconstruction, we devise a cross-modal conditioning mechanism that uses both frequency domain information and semantic descriptions. Given a visual representation $I \in \mathbb{R}^{B \times 3 \times H \times W}$, we first encode it into latent space and derive conditional signals as:
\begin{equation}
\small
    \quad c_{freq} = \text{FFTEncoder}(X),\quad c_{text} = \text{TextEncoder}(X)
\end{equation}
\begin{equation}
\small
    z = E(I) \cdot s,\quad c_m = \text{CrossAttn}(\text{MLP}([c_{text}; c_{freq}]), z)
\end{equation}
% s = 0.18215
where $E(\cdot)$ represents the VAE encoder, $s$ is the latent space scaling factor (see Appendix~\ref{latent_space_scaling} for detailed derivation). $c_{freq}\in \mathbb{R}^{B \times (2DL+2)}$ captures periodic patterns through frequency analysis while $c_{text}\in \mathbb{R}^{B \times d_{model}}$ encodes statistical properties and domain knowledge through natural language descriptions. The detailed implementations of FFTEncoder and TextEncoder are provided in Appendix~\ref{appx:conditional_generation}.

\paragraph{Forward Diffusion Process}
% 解释前向扩散过程的设计
The forward process implements a variance-preserving Markov chain that progressively injects Gaussian noise into the latent representations transformed from multi-view visual encodings of time series data. This controlled noise injection, operating in a compressed latent space rather than pixel space, enables efficient learning of temporal patterns across different scales while preserving the intrinsic information from vision transformations. For a given initial latent representation $z_0$, we define the forward diffusion process through a series of probabilistic transformations:
\begin{equation}
\small
    q(z_t|z_{t-1}, I_m) = \mathcal{N}(z_t; \sqrt{\alpha_t}z_{t-1}, (1-\alpha_t)I_m)
\end{equation}
\begin{equation}
\small
    q(z_t|z_0, I_m) = \mathcal{N}(z_t; \sqrt{\bar{\alpha}_t}E(I)/s, (1-\bar{\alpha}_t)I_m)
\end{equation}
\begin{equation}
\small
    \bar{\alpha}_t = \prod_{s=1}^t \alpha_s, \quad t \in \{1,...,T\}
\end{equation}
where $\{\alpha_t\}_{t=1}^T$ defines a scaled linear noise schedule, and $\bar{\alpha}_t$ controls the cumulative noise level across $t$ timesteps. The encoder $E(\cdot)$ maps the multi-view visual representations $I_m$ to a lower-dimensional latent space. %with scaling factor $s$.

\paragraph{De-noising Process}
% 详细说明去噪过程和损失函数设计
The reverse process employs a parameterized U-Net architecture to progressively denoise the representations exploiting cross-modal conditioning mechanisms. By incorporating frequency and semantic embeddings, this process uniquely captures both complex temporal dynamics while maintaining coherent long-term dependencies. The complete denoising process is formulated as:
\begin{equation}
\small
    p_\theta(z_{t-1}|z_t,c_m) = \mathcal{N}(z_{t-1}; \mu_\theta(z_t,t,c_m), \Sigma_\theta(z_t,t))
\end{equation}
\begin{equation}
\small
    \mu_\theta(z_t,t,c_m) = \frac{1}{\sqrt{\alpha_t}}(z_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_\theta(z_t,t,c_m))
\end{equation}
\begin{equation}
\small
    \mathcal{L} = \mathbb{E}_{z_0,\epsilon,t}[\|\epsilon - \epsilon_\theta(z_t,t,c_m)\|_2^2] + \lambda\mathcal{L}_{recon}
\end{equation}
where $\epsilon_\theta$ predicts the noise component given the noisy latent $z_t$, timestep $t$, and cross-modal condition $c_m$. The final reconstructed image $\hat{I} = D(z_0/s)$ is obtained by decoding the denoised latent representation. This design enables effective temporal pattern learning, comprehensive structural information preservation, and latent space optimization.
\begin{figure}[!t]
  \centering
  \includegraphics[width=0.95\linewidth]{figure/forward_process.pdf}
  \caption{The forward process of LDM4TS.}
\label{fig:forward}
\end{figure}

\subsection{Temporal Projection and Fusion}
% 解释时序编码器的必要性和设计
While the latent diffusion model captures global patterns effectively, local temporal dynamics and distribution shifts require explicit modeling. As shown in Fig.~\ref{fig:forward}, we propose a temporal encoder (TE) that complements the diffusion process through three key components: patch embedding, transformer encoding, and adaptive feature fusion.
% 详细说明特征提取过程
Given input sequence $X \in \mathbb{R}^{B \times L \times D}$, we adopt the patch embedding strategy~\cite{dosovitskiy2020image,nie2023patchtst} to encode temporal information. Then, the embeddings are processed through $L$ transformer layers, where $X_{norm} = \text{LN}(X)$. The embeddings are processed as follows:
\begin{equation}
\small
    h_0 = \text{PatchEmbed}(X_{norm})\in \mathbb{R}^{B \times N_p \times d}
\end{equation}
\begin{equation}
\small
    h'_l = h_{l-1} + \text{MSA}(\text{LN}(h_{l-1}))
\end{equation}
\begin{equation}
\small
    h_l = h'_l + \text{MLP}(\text{LN}(h'_l))
\end{equation}
\begin{equation}
\small
    Z_{TE} = \text{LinearProj}(h_L) \in \mathbb{R}^{B \times L_{pred} \times D}
\end{equation}
where $N_p$ denotes patch count, $h$ is hidden states and $d$ is the hidden dimension. MSA and LN represent multi-head self-attention and layer normalization respectively. Finally, we implement an adaptive fusion mechanism that dynamically combines temporal features $Z_{TE}$ with visual features $Z_{VE}$:
\begin{equation}
\small
    g = \text{Sigmoid}(\text{MLP}([Z_{TE}; Z_{VE}])) 
\end{equation}
\begin{equation}
\small
    \hat{Y} = g \odot Z_{TE} + (1-g) \odot Z_{VE}
\end{equation}
% \end{small}
where learned gates $g$ dynamically balance the contribution of global patterns from diffusion and local dynamics from temporal encoding, enabling robust adaptation to different temporal characteristics.