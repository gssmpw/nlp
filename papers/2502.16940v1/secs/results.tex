


\section{Results and Findings}
In this section, we present our experimental analysis and key findings.


\subsection{CoT May Reduce Role-Playing Performance}

To investigate whether reasoning techniques enhance the role-playing capabilities of LLMs, we conduct extensive experiments. 
Specifically, we select 17 models and evaluate their role-playing performance using both zero-shot and chain-of-thought (CoT) approaches across six standardized and widely used benchmarks.
All experimental results are presented in the Appendix~\ref{app:sec:results}. 
Figure~\ref{fig:zero_vs_cot} provides an aggregated overview of the results. 
Specifically, for each model, its final performance on a given benchmark is computed as the average of its performance across all sub-datasets.

As shown in Figure~\ref{fig:zero_vs_cot}, employing CoT reasoning is more likely to degrade role-playing performance on four benchmarks: CroSS-MR, HPD, SocialBench, and CharacterEval. 
In contrast, on InCharacter and RoleBench, CoT reasoning enhances the role-playing capabilities of LLMs.
To further understand why CoT reduces the role-playing capabilities of LLMs, we select Qwen2.5-7B-Instruct as the experimental model. 
We then sample 50 test cases from each of the six benchmarks where CoT performance is lower than Zero-shot performance for detailed analysis.

Our findings indicate that the primary reasons for CoT-induced degradation in role-playing are:
(1) “Attention Diversion”: The model must simultaneously engage in reasoning and role-playing modes, which dilutes its focus on the role-playing task.
(2) “Linguistic Style Drift”: Reasoning responses tend to be structured, logical, and formal, whereas effective role-playing requires a vivid, expressive, and character-consistent linguistic style. 

\begin{AIbox}
{Finding 1:}
CoT may reduce the role-playing capabilities of LLMs, primarily due to attention diversion and linguistic style drift induced by the reasoning process.
\end{AIbox}


\begin{figure*}[t]
    \centering
    \includegraphics[scale=0.38]{pics/o1_vs.pdf}
    \caption{Experimental results of various models across 6 benchmarks. Models of similar sizes are represented using the same color scheme, with each employing different types of reasoning techniques. The vertical axis denotes the evaluation metrics specific to each dataset.}
    \label{fig:o1_vs}
\end{figure*}


\subsection{Reasoning-optimized LLMs Are Unsuitable for Role-Playing}
The most advanced models available today are undoubtedly reasoning-optimized models, including OpenAI's o series, DeepSeek's R1 model, and various distilled versions derived from DeepSeek-R1\footnote{\url{https://github.com/deepseek-ai/DeepSeek-R1}}.
Compared to CoT reasoning, these models leverage pretraining and reinforcement learning techniques to cultivate intrinsic reasoning capabilities, making them inherently more adept at reflection, verification, and other cognitive processes.

To investigate whether reasoning-optimized models are better suited for role-playing tasks, we conduct experiments using OpenAI o1-mini, QwQ-32B-Preview, DeepSeek-R1, and its various distilled versions. 
The experimental results are presented in Figure~\ref{fig:o1_vs}.

It is evident that reasoning-optimized LLMs generally perform poorly and are not well-suited for role-playing tasks, even for state-of-the-art models such as OpenAI o1-Mini and DeepSeek-R1. 
Furthermore, we observe that models refined through reasoning distillation exhibit even worse role-playing performance compared to their original counterparts. 
This finding aligns with the conclusion drawn in the previous subsection: enhancing rational reasoning capabilities tends to undermine the emotional and intuitive aspects essential for effective role-playing.

\begin{AIbox}
{Finding 2:}
Reasoning-optimized LLMs are less suitable for role-playing tasks.
\end{AIbox}




\subsection{Reasoning Ability Disrupts the Role-Playing Scaling Law}


Figure~\ref{fig:all} presents the results of our three experimental settings: direct zero-shot role-playing, role-playing with Chain-of-Thought (CoT), and role-playing using reasoning-optimized LLMs. 
First, we observe that, with the exception of the InCharacter benchmark, the other five benchmarks generally follow the scaling law, where larger models exhibit stronger role-playing capabilities. 
However, we also find that the role-playing scaling law is not particularly pronounced—the performance gains from increasing model size remain relatively modest and inconsistent across benchmarks. 
Furthermore, introducing reasoning capabilities, whether through CoT or reasoning-optimized LLMs, further weakens the benefits of scaling, leading to more pronounced fluctuations, increased instability, and greater variability in model performance across different role-playing tasks and datasets, making the scaling trend less predictable and consistent.

\begin{AIbox}
{Finding 3:}
The role-playing scaling law exists but is not pronounced, and the introduction of reasoning capabilities disrupts this scaling law.
\end{AIbox}


\begin{figure*}[!htb]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pics/zero_shot.pdf}
        \caption{Results of direct zero-shot role-playing.}
        \label{fig:zero_shot}
    \end{subfigure}
    
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pics/cot.pdf}
        \caption{Results of role-playing with chain-of-thought (CoT) reasoning.}
        \label{fig:cot}
    \end{subfigure}

    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pics/o1.pdf}
        \caption{Results of role-playing using reasoning-optimized LLMs.}
        \label{fig:o1}
    \end{subfigure}

    \caption{Performance comparison of different models across six benchmarks. The horizontal axis represents model size, arranged from smallest to largest, while the vertical axis denotes benchmark-specific evaluation metrics, where higher values indicate better role-playing performance. Within each benchmark, different color gradients represent the performance curves for its respective sub-datasets.}
    \label{fig:all}
\end{figure*}


\subsection{The Qwen Series Is Well-Suited for Role-Playing Tasks}
To provide guidance on model selection for future role-playing tasks, we conduct a comprehensive and systematic comparative analysis across different model scales (e.g., 1B, 3B, 7B, 14B, 32B, 72B).
The results are shown in Figure~\ref{fig:all}.
Our findings indicate that the Qwen2.5 series consistently outperforms the Llama, Gemma, and Mistral series across various size ranges in terms of role-playing accuracy, persona consistency, linguistic expressiveness, and contextual coherence. 
Notably, we recommend Qwen2.5-7B-Instruct as the most cost-effective and well-balanced model for role-playing applications, as it offers a strong trade-off between performance, computational efficiency, and adaptability across diverse role-playing scenarios.


\begin{AIbox}
{Finding 4:}
The Qwen2.5 series excels in role-playing tasks, with Qwen2.5-7B-Instruct being the most cost-effective model.
\end{AIbox}


\begin{figure*}[t]
    \centering
    \includegraphics[scale=0.36]{pics/radar_6models.pdf}
    \caption{Fine-grained performance of the Qwen2.5 series on the CharacterEval benchmark. The radar chart illustrates multiple evaluation dimensions, with metrics computed using a pretrained reward model. Higher scores indicate stronger capabilities.}
    \label{fig:charactereval_radar}
\end{figure*}



\subsection{Chinese Role-Playing Performance Surpasses English Role-Playing Performance}

The role-playing benchmarks HPD, SocialBench, InCharacter, and RoleBench provide both Chinese and English versions, enabling a direct comparison of multilingual role-playing performance. 
Interestingly, an analysis of Figure~\ref{fig:all} reveals that current LLMs exhibit stronger role-playing capabilities in Chinese than in English across multiple benchmarks and evaluation metrics. 
This observation contradicts the common conclusion that LLMs generally have a stronger foundation in English compared to other languages, particularly in reasoning and knowledge-intensive tasks.
We hypothesize that this phenomenon may be attributed to the following factor:
Due to the extensive training on English data, models have to some extent internalized generalized character information. 
When performing precise role-playing tasks, this internalized generalization can interfere with context-sensitive role-playing, leading to performance degradation.

\begin{AIbox}
{Finding 5:}
Large models exhibit superior role-playing capabilities in Chinese compared to English to some extent.
\end{AIbox}


\subsection{Large Models Still Lack Proficiency in Advanced Role-Playing}
To further investigate the limitations of current LLM-based role-playing models, we conduct an analysis using the CharacterEval benchmark.
Specifically, CharacterEval provides a pretrained reward model that assigns scores across 12 role-playing evaluation dimensions\footnote{\url{https://huggingface.co/morecry/BaichuanCharRM}}. 
We analyze the performance of the Qwen2.5 series, with the results presented in Figure~\ref{fig:charactereval_radar}.
All detailed results are presented in Appendix~\ref{app:sec:charactereval}.
First, consistent with our previous findings, direct zero-shot role-playing outperforms other approaches in most cases.
Furthermore, a fine-grained comparison of evaluation metrics reveals that current LLMs still exhibit deficiencies in advanced role-playing dimensions, such as character knowledge exposition, personality expression, and linguistic diversity. 
These aspects require further enhancement in future research to improve the overall role-playing capability of LLMs.

\begin{AIbox}
{Finding 6:}
Current LLMs still underperform in advanced role-playing capabilities, such as knowledge exposition, personality expression, and diversity.
\end{AIbox}


