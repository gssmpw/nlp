\section{Introduction}


Large language models (LLMs), leveraging their strong foundational capabilities and advanced instruction-following abilities, have become the cornerstone of role-playing AI models~\citep{chen2024persona,tseng2024two}.
These role-playing LLMs present new opportunities in both research and commercial applications. 
In the academic domain, they offer novel possibilities for social simulation experiments~\citep{mou2024individual}. 
In industry, role-specific assistants, emotional companionship AI, and intelligent game NPCs~\citep{park2023generative,xu2024can} hold significant potential for further development and commercialization.
At the same time, reasoning capabilities in LLMs have been further enhanced through Chain-of-Thought (CoT) reasoning~\citep{wei2022chain} and reinforcement learning~\citep{guo2025deepseek}, enabling them to tackle increasingly complex problems.


\begin{table}[t]
    \centering
    \begin{tabularx}{\columnwidth}{X}
        \toprule
        \textbf{Context}  \\
        Tom: Captain, your boat has been stolen. \\
        \midrule
        \textbf{Zero-shot role-playing with GPT-4-Turbo} \\
        Not stolen, mate—temporarily misplaced. And I always get my ship back, savvy? \\
        \hdashline
        \textbf{Role-playing with chain-of-thought using GPT-4-Turbo} \\
        \textcolor{gray}{[Reasoning process omitted]}  So the Pearl's decided to go on a little adventure without me—no matter, I’ll just steal her back. \\
        \hdashline
        \textbf{Role-playing with OpenAI o1-mini} \\
        \textcolor{gray}{[Reasoning process omitted]}  Stolen, you say, mate—why, I'd be far more worried for the soul foolish enough to filch the Pearl, savvy? \\
        \bottomrule
    \end{tabularx}
    \caption{Jack Sparrow role-playing example. It can be observed that direct zero-shot prompting of GPT yields better results, aligning more closely with Jack Sparrow's character traits.}
    \label{tab:intro}
\end{table}


This situation prompts us to explore a previously unanswered research question: “Can reasoning techniques enhance the role-playing capabilities of large language models?”
The answer to this question could provide valuable insights for the future application of reasoning techniques in role-playing LLMs, potentially paving a new path for their development and advancement.


To address this question, we carefully select six role-playing datasets and conduct experiments using 24 widely used open-source and proprietary models.
These models include API-based closed-source models such as GPT-4-Turbo~\citep{achiam2023gpt}, popular open-source models like Qwen2.5~\citep{yang2024qwen2}, and reinforcement learning-optimized reasoning models such as DeepSeek-R1~\citep{guo2025deepseek}.
All experiments are conducted using OpenCompass~\citep{2023opencompass} to ensure evaluation consistency, stability, and reproducibility. 
Specifically, we utilize six role-playing benchmarks: RoleBench~\citep{wang2023rolellm}, InCharacter~\citep{wang2024incharacter}, SocialBench~\citep{chen2024socialbench}, CharacterEval~\citep{tu2024charactereval}, HPD~\citep{chen2023hpd}, and CroSS-MR~\citep{yuan2024cross}. 
These benchmarks cover both multiple-choice and text generation tasks and include English and Chinese, making the evaluation linguistically diverse.
For evaluation, we employ both traditional metrics (e.g., Accuracy, ROUGE, and Exact Match) and LLM-as-a-Judge methods (e.g., prompt-based evaluation and reward model scoring), ensuring a broad assessment scope to enhance the robustness of our conclusions.
Furthermore, we define three role-playing approaches: direct zero-shot role-playing, role-playing with Chain-of-Thought (CoT), and role-playing using reasoning-optimized LLMs. 
This comprehensive setup allows us to systematically investigate the impact of reasoning techniques on role-playing performance.
Table~\ref{tab:intro} presents a specific example, demonstrating that the first setting, direct zero-shot role-playing, achieves the best performance.

Through extensive experiments, our findings reveal several key insights that can inform the future design of role-playing LLMs:
(1) CoT may reduce role-playing performance;
(2) Reasoning-optimized LLMs are unsuitable for role-playing;
(3) Reasoning ability disrupts the role-playing scaling law;
(4) The Qwen series is well-suited for role-playing;
(5) Chinese role-playing performance surpasses English role-playing performance;
(6) Large models still lack proficiency in advanced role-playing.

Our study makes three key contributions. First, through extensive experiments on six role-playing datasets and 24 models, we reveal that reasoning techniques do not necessarily enhance the role-playing capabilities of LLMs. 
Second, our results provide a comprehensive analysis of the current state and limitations of role-playing LLMs, leading to two promising research directions: Role-Aware CoT for Improving Role-Playing LLMs and Reinforcement Learning for Role-Playing LLMs. 
Third, to address the fragmented nature of prior work with inconsistent evaluation standards, we integrate all datasets and experimental methods into OpenCompass, enabling one-click execution and providing a standardized experimental framework and codebase for future research\footnote{All code will be open-sourced upon publication.}.
