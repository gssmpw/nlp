\begin{table*}[t]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{llllllll}
        \toprule
        \textbf{Dataset} & \textbf{Task Type} & \textbf{Evaluation Metric} & \textbf{Task} & \textbf{Language} & \textbf{\#Characters} & \textbf{\#Samples} \\
        \midrule
        \multirow{3}{*}{RoleBench~\citep{wang2023rolellm}} & \multirow{3}{*}{Generation} & \multirow{3}{*}{\makecell[l]{Automatic\\(ROUGE)}} & \multirow{2}{*}{General Question Answering} & EN & 95 & 32833 \\
        &  &  &  & ZH & 5 & 1690 \\
        \cmidrule{4-7}
        &  &  & Role-specific Question Answering & EN & 95 & 7534 \\
        \midrule
        \multirow{2}{*}{HPD~\citep{chen2023hpd}} 
        & \multirow{2}{*}{Generation} & \multirow{2}{*}{\makecell[l]{Automatic\\(ROUGE)}} & \multirow{2}{*}{Response Generation} & EN & 1 & 149 \\
        &  &  &  & ZH & 1 & 167 \\
        \midrule
        CharacterEval~\citep{tu2024charactereval} & Generation & \makecell[l]{LLM-as-a-Judge\\(Pretrained Reward Model)} & Response Generation & ZH & 77 & 4564 \\
        \midrule
        \textsc{CroSS-MR}~\citep{yuan2024cross} & Multiple-choice & \makecell[l]{Automatic\\(Accuracy)} & Motivation Recognition & EN & 126 & 445 \\
        \midrule
        \multirow{10}{*}{Socialbench~\citep{chen2024socialbench}} 
        & \multirow{6}{*}{Multiple-choice} & \multirow{6}{*}{\makecell[l]{Automatic\\(Accuracy)}} & \multirow{2}{*}{Role Knowledge Understanding} & EN & 23 & 988 \\
        &  &  &  & ZH & 15 & 405 \\
        \cmidrule{4-7}
        &  &  & \multirow{2}{*}{Behavioral Style Understanding} & EN & 8 & 740 \\
        &  &  &  & ZH & 8 & 323  \\
        \cmidrule{4-7}
        &  &  & \multirow{2}{*}{Social Preference Recognition} & EN & 79 & 731 \\
        &  &  &  & ZH & 101 & 1185 \\
        \cmidrule{2-7}
        & \multirow{4}{*}{Generation} & \multirow{4}{*}{\makecell[l]{Automatic\\(Exact Match)}} & \multirow{2}{*}{Long Conversation Memorization} & EN & 47 & 728 \\
        &  &  &  & ZH & 35 & 620 \\
        \cmidrule{4-7}
        &  &  & \multirow{2}{*}{Short Conversation Memorization} & EN & 46 & 463 \\
        &  &  &  & ZH & 27 &  310 \\
        \midrule
        \multirow{2}{*}{InCharacter~\citep{wang2024incharacter}} & \multirow{2}{*}{Generation} & \multirow{2}{*}{\makecell[l]{LLM-as-a-Judge\\(Prompting)}} & 16Personalities Identification & EN & 32 & 32 \\
        \cmidrule{4-7}
        &  &  & BFI Identification & EN & 32 & 32 \\
        \bottomrule
    \end{tabular}
    }
    \caption{Benchmark statistics.}
    \label{tab:datasets}
\end{table*}

\section{Experimental Setting}
In this section, we introduce the benchmarks, models, metrics, and reasoning methods used in our experiments. 
All relevant code has been integrated into the OpenCompass~\citep{2023opencompass} library to facilitate efficient replication by researchers.

\subsection{Role-playing Benchmarks}
We select six role-playing benchmarks to ensure the reliability of our experimental results, including:
\textbf{RoleBench}~\citep{wang2023rolellm}: Generates responses to both general and role-specific questions based on role information.
\textbf{HPD}~\citep{chen2023hpd}: Produces responses by leveraging the provided Harry Potter character information and related character details.
\textbf{CharacterEval}~\citep{tu2024charactereval}: Generates responses based on detailed character information, including background experiences and personality traits.
\textbf{CroSS-MR}~\citep{yuan2024cross}: Recognizes a role’s behavioral motivations based on the provided character profile.
\textbf{SocialBench}~\citep{chen2024socialbench}: Understands a role’s knowledge, behavioral style, social preferences, and fine-grained memory based on the given role information.
\textbf{InCharacter}~\citep{wang2024incharacter}: Assesses a character’s personality through questionnaire-based interviews\footnote{InCharacter benchmark comprises 14 types of assessment questionnaires. In this study, we select the most classic ones: 16Personalities and BFI for reporting results. Meanwhile, all other questionnaires are also implemented in the code.}.
Detailed benchmark statistics are provided in Table~\ref{tab:datasets}.


\subsection{Backbone LLMs}
Our experiments include a total of 24 models, comprising 2 closed-source models and 22 open-source models. 
These models originate from six different companies, including state-of-the-art models such as GPT-4-Turbo and DeepSeek-R1. 
The detailed list of models is provided in Appendix~\ref{app:sec:models}.


\subsection{Evaluation Metrics}
Our evaluation incorporates two types of metrics: \textbf{automated metrics} and \textbf{LLM-as-a-Judge}, both of which are widely recognized and scalable evaluation approaches.
Automated metrics include \textit{ROUGE}, \textit{Accuracy}, and \textit{Exact Match}, which are well-established and widely accepted in the field as classical evaluation benchmarks.
LLM-as-a-Judge further encompasses two methodologies: the \textit{Pretrained Reward Model} and \textit{Prompting-based Scoring}. 
The Pretrained Reward Model is trained on annotated datasets and assigns scores to specified dimensions during evaluation. 
In contrast, Prompting-based Scoring leverages prompt engineering techniques and the extensive knowledge base of LLMs to dynamically and adaptively generate relevant scores, providing a more flexible and responsive evaluation mechanism.
In Appendix~\ref{app:sec:metrics}, we provide a detailed explanation of the evaluation metrics used for each benchmark.


\begin{figure*}[t]
    \centering
    \includegraphics[scale=0.41]{pics/zero_vs_cot.pdf}
    \caption{Performance comparison of 17 Models using two role-playing methods across six benchmarks. The horizontal axis ranks models in descending order of scale, while the vertical axis represents the unique metric for each dataset. Notably, darker colors indicate that zero-shot role-playing outperforms CoT role-playing, whereas lighter colors signify that zero-shot role-playing underperforms compared to CoT role-playing.}
    \label{fig:zero_vs_cot}
\end{figure*}


\subsection{Role-playing Methods}
In this study, we employ three approaches to guide LLMs in performing role-playing tasks:
\(\mathcal{R}_1\): Direct zero-shot role-playing using an LLM, where the model generates responses without reasoning steps.
\(\mathcal{R}_2\): Role-playing with chain-of-thought (CoT) reasoning, where an LLM explicitly engages in step-by-step reasoning before executing the role-playing task.
\(\mathcal{R}_3\): Role-playing using reasoning-optimized LLMs, such as \textit{QwQ-32B-Preview} and \textit{DeepSeek-R1}, which autonomously engage in deep reasoning before generating responses.
