\section{Evaluated Models}\label{app:sec:models}
Table~\ref{tab:models} lists all the large language models (LLMs) used in this study.

\begin{table}[t]
    \centering
    \begin{tabular}{@{}ll@{}}
    \toprule
    \textbf{Team} & \textbf{Model} \\ 
    \midrule
    \multicolumn{2}{c}{\textbf{Closed-source}} \\ 
    \midrule
    \multirow{2}{*}{OpenAI} & GPT-4 Turbo \\
     & OpenAI o1-mini \\
    \midrule
    \multicolumn{2}{l}{\textbf{Open-source}} \\ 
    \midrule
    \multirow{8}{*}{Qwen} & Qwen2.5-0.5B-Instruct \\
     & Qwen2.5-1.5B-Instruct \\
     & Qwen2.5-3B-Instruct \\
     & Qwen2.5-7B-Instruct \\
     & Qwen2.5-14B-Instruct \\
     & Qwen2.5-32B-Instruct \\
     & Qwen2.5-72B-Instruct \\
     & QwQ-32B-Preview \\
    \hdashline
    \multirow{3}{*}{Google} & gemma-2-2b-it \\
     & gemma-2-9b-it \\
     & gemma-2-27b-it \\
    \hdashline
    \multirow{4}{*}{Meta} & Llama-3.2-1B-Instruct \\
     & Llama-3.2-3B-Instruct \\
     & Llama-3.1-8B-Instruct \\
     & Llama-3.3-70B-Instruct \\
    \hdashline
    \multirow{2}{*}{Mistral AI} & Ministral-8B-Instruct-2410 \\
     & Mistral-7B-Instruct-v0.3 \\
    \hdashline
    \multirow{5}{*}{Deepseek} & DeepSeek-R1 \\
     & DeepSeek-R1-Distill-Llama-8B \\
     & DeepSeek-R1-Distill-Qwen-14B \\
     & DeepSeek-R1-Distill-Qwen-32B \\
     & DeepSeek-R1-Distill-Llama-70B \\
    \bottomrule
    \end{tabular}
\caption{List of models evaluated in the experiment.}
\label{tab:models}
\end{table}


\begin{table*}[ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{llcccccccc}
\toprule
\textbf{Dataset} & \textbf{Sub Dataset} & \textbf{\makecell{OpenAI \\ GPT-4 Turbo}} & \textbf{\makecell{Qwen2.5-0.5B\\-Instruct}} & \textbf{\makecell{Qwen2.5-1.5B\\-Instruct}} & \textbf{\makecell{Qwen2.5\\-3B-Instruct}} & \textbf{\makecell{Qwen2.5-7B\\-Instruct}} & \textbf{\makecell{Qwen2.5-14B\\-Instruct}} & \textbf{\makecell{Qwen2.5-32B\\-Instruct}} & \textbf{\makecell{Qwen2.5-72B\\-Instruct}} \\
\midrule
Cross & cross & 53.93 & 26.29 & 45.39 & 55.96 & 56.40 & 59.10 & 61.80 & 62.92 \\
\midrule
\multirow{2}{*}{HPD} & hpd\_en & 17.21 & 10.18 & 11.71 & 12.18 & 13.28 & 12.85 & 14.12 & 14.29 \\
 & hpd\_zh & 20.80 & 6.13 & 7.65 & 17.24 & 18.37 & 19.83 & 20.90 & 20.83 \\
 \midrule
\multirow{10}{*}{Socialbench} & socialbench\_sap\_zh & 73.08 & 39.04 & 64.22 & 75.78 & 79.41 & 82.36 & 86.92 & 83.54 \\
 & socialbench\_sap\_en & 83.04 & 36.99 & 61.70 & 80.98 & 89.19 & 92.34 & 92.89 & 89.33 \\
 & socialbench\_sa\_rolestyle\_zh & 57.59 & 23.84 & 40.56 & 58.51 & 69.35 & 71.21 & 77.71 & 74.30 \\
 & socialbench\_sa\_rolestyle\_en & 81.62 & 44.96 & 50.68 & 71.35 & 80.27 & 83.24 & 87.43 & 90.81 \\
 & socialbench\_sa\_roleknowledge\_zh & 89.07 & 53.09 & 65.08 & 85.02 & 88.16 & 94.03 & 96.26 & 96.30 \\
 & socialbench\_sa\_roleknowledge\_en & 83.13 & 44.03 & 37.63 & 76.63 & 79.62 & 85.91 & 80.27 & 96.96 \\
 & socialbench\_mem\_short\_zh & 83.16 & 13.35 & 48.03 & 63.83 & 76.50 & 88.41 & 76.33 & 82.53 \\
 & socialbench\_mem\_short\_en & 83.13 & 16.35 & 48.40 & 72.78 & 76.82 & 80.24 & 87.21 & 86.90 \\
 & socialbench\_mem\_long\_zh & 60.94 & 11.20 & 58.46 & 61.46 & 75.30 & 87.78 & 75.52 & 82.32 \\
 & socialbench\_mem\_long\_en & 60.91 & 14.01 & 58.46 & 61.46 & 75.30 & 87.78 & 75.52 & 83.92 \\
 \midrule
\multirow{4}{*}{InCharacter} & incharacter\_16Personalities\_en & 0.48 & 0.44 & 0.56 & 0.45 & 0.53 & 0.54 & 0.41 & 0.39 \\
 & incharacter\_16Personalities\_zh & 0.50 & 0.43 & 0.50 & 0.39 & 0.36 & 0.68 & 0.32 & 0.45 \\
 & incharacter\_BFI\_en & 0.50 & 0.50 & 0.50 & 0.57 & 0.47 & 0.47 & 0.53 & 0.46 \\
 & incharacter\_BFI\_zh & 0.53 & 0.57 & 0.57 & 0.53 & 0.60 & 0.60 & 0.63 & 0.57 \\
 \midrule
 Charactereval & charactereval & 2.52 & 1.70 & 2.05 & 2.45 & 2.77 & 3.08 & 2.96 & 3.21   \\
  \midrule
\multirow{3}{*}{RoleBench} & instruction\_generalization\_en & 18.57 & 20.41 & 20.67 & 20.73 & 21.15 & 21.89 & 13.28 & 3.21 \\
 & instruction\_generalization\_zh & 19.45 & 16.63 & 15.25 & 18.18 & 16.88 & 16.68 & 17.36 & 20.39 \\
 & role\_generalization\_en & 25.93 & 19.04 & 19.18 & 21.19 & 21.68 & 23.08 & 22.29 & 17.20 \\
\bottomrule
\end{tabular}%
}
\caption{Direct zero-shot role-playing performance of different LLMs on various role-playing benchmarks (part1).}
\label{tab:zero_shot_performance_part_1}
\end{table*}

\begin{table*}[ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{llccccccccc}
\toprule
\textbf{Dataset} & \textbf{Sub Dataset} & \textbf{\makecell{Gemma-2\\-2B-it}} & \textbf{\makecell{Gemma-2\\-9B-it}} & \textbf{\makecell{Gemma-2\\-27B-it}} & \textbf{\makecell{Llama-3.2-1B\\-Instruct}} & \textbf{\makecell{Llama-3.2-3B\\-Instruct}} & \textbf{\makecell{Llama-3.1-8B\\-Instruct}} & \textbf{\makecell{Llama-3.3-70B\\-Instruct}} & \textbf{\makecell{Mistral-8B\\-Instruct-2410}} & \textbf{\makecell{Mistral-7B\\-Instruct-v0.3}} \\
\midrule
Cross & cross & 36.40 & 62.92 & 65.62 & 30.79 & 51.91 & 56.63 & 65.17 & 47.64 & 52.81 \\
\midrule
\multirow{2}{*}{HPD} & hpd\_en & 12.30 & 12.13 & 10.66 & 6.78 & 11.10 & 14.33 & 12.47 & 15.54 & 14.74 \\
 & hpd\_zh & 11.50 & 13.08 & 14.20 & 9.82 & 9.63 & 16.93 & 19.10 & 21.12 & 19.20 \\
 \midrule
\multirow{10}{*}{SocialBench} & socialbench\_sap\_zh & 75.86 & 84.81 & 88.52 & 36.54 & 74.77 & 83.54 & 93.25 & 78.06 & 77.22 \\
 & socialbench\_sap\_en & 75.92 & 90.97 & 91.66 & 42.54 & 83.31 & 86.05 & 93.71 & 85.91 & 85.77 \\
 & socialbench\_sa\_rolestyle\_zh & 37.15 & 70.28 & 51.08 & 26.01 & 51.39 & 63.47 & 76.78 & 64.40 & 55.11 \\
 & socialbench\_sa\_rolestyle\_en & 47.16 & 66.35 & 57.97 & 40.68 & 72.30 & 81.49 & 63.65 & 84.73 & 77.70 \\
 & socialbench\_sa\_roleknowledge\_zh & 55.56 & 80.00 & 74.32 & 39.26 & 69.63 & 80.49 & 92.59 & 79.75 & 65.93 \\
 & socialbench\_sa\_roleknowledge\_en & 55.87 & 80.57 & 77.73 & 43.52 & 73.38 & 86.64 & 76.92 & 83.91 & 81.68 \\
 & socialbench\_mem\_short\_zh & 46.19 & 83.01 & 85.97 & 47.42 & 59.52 & 81.56 & 92.47 & 85.70 & 85.38 \\
 & socialbench\_mem\_short\_en & 55.32 & 68.97 & 45.39 & 33.98 & 41.83 & 82.03 & 88.56 & 76.20 & 82.26 \\
 & socialbench\_mem\_long\_zh & 57.68 & 79.25 & 80.45 & 44.59 & 65.73 & 88.82 & 91.50 & 88.60 & 86.75 \\
 & socialbench\_mem\_long\_en & 59.62 & 68.01 & 44.13 & 32.28 & 65.96 & 89.14 & 88.87 & 82.26 & 80.96 \\
 \midrule
\multirow{4}{*}{InCharacter} & incharacter\_16Personalities\_en & 0.58 & 0.45 & 0.54 & 0.54 & 0.54 & 0.44 & 0.44 & 0.47 & 0.46 \\
 & incharacter\_16Personalities\_zh & 0.36 & 0.36 & 0.68 & 0.55 & 0.50 & 0.68 & 0.77 & 0.64 & 0.68 \\
 & incharacter\_BFI\_en & 0.50 & 0.52 & 0.50 & 0.39 & 0.54 & 0.54 & 0.50 & 0.50 & 0.45 \\
 & incharacter\_BFI\_zh & 0.50 & 0.57 & 0.57 & 0.57 & 0.53 & 0.47 & 0.67 & 0.57 & 0.63 \\
 \midrule
Charactereval & charactereval & 2.23 & 2.49 & 2.53 & 2.14 & 2.24 & 2.89 & 2.90 & 2.63 & 2.26 \\
 \midrule
\multirow{3}{*}{RoleBench} & rolebench\_instruction\_generalization\_en & 20.13 & 20.73 & 21.95 & 13.28 & 15.72 & 16.87 & 17.93 & 25.39 & 18.89 \\
 & rolebench\_instruction\_generalization\_zh & 15.89 & 17.68 & 17.32 & 9.72 & 15.95 & 17.54 & 18.81 & 20.90 & 14.38 \\
 & rolebench\_generalization\_en & 20.66 & 20.70 & 21.72 & 12.57 & 14.22 & 15.29 & 15.60 & 24.55 & 17.25 \\
\bottomrule
\end{tabular}%
}
\caption{Direct zero-shot role-playing performance of different LLMs on various role-playing benchmarks (part2).}
\label{tab:zero_shot_performance_part_2}
\end{table*}


\section{Detailed Metrics}\label{app:sec:metrics}

\begin{itemize}
    \item \textbf{Accuracy}: Used for multiple-choice evaluation in Cross-MR and the SocialBench multiple-choice task.
    \item \textbf{ROUGE}: Used for text generation evaluation in HPD and RoleBench.
    \item \textbf{Exact Match}: Used in the SocialBench Generation Task to assess the alignment of key memory points.
    \item \textbf{LLM-as-a-Judge (Prompting)}: Used in InCharacter to convert model responses into standardized answers through analytical transformation.
    \item \textbf{LLM-as-a-Judge (Reward Model)}: Used in CharacterEval, where a reward model assigns scores across 12 evaluation dimensions for model responses.
\end{itemize}


\section{Detailed Results on Role-palying Benchmarks}\label{app:sec:results}

Tables~\ref{tab:zero_shot_performance_part_1}, \ref{tab:zero_shot_performance_part_2}, \ref{tab:cot_performance_part_1}, \ref{tab:cot_performance_part_2}, and \ref{tab:o1_performance} present the experimental results of various models using three role-playing methods across six benchmarks.







\begin{table*}[ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{llcccccccc}
\toprule
\textbf{Dataset} & \textbf{Sub Dataset} & \textbf{\makecell{OpenAI \\ GPT-4 Turbo}} & \textbf{\makecell{Qwen2.5-0.5B\\-Instruct}} & \textbf{\makecell{Qwen2.5-1.5B\\-Instruct}} & \textbf{\makecell{Qwen2.5\\-3B-Instruct}} & \textbf{\makecell{Qwen2.5-7B\\-Instruct}} & \textbf{\makecell{Qwen2.5-14B\\-Instruct}} & \textbf{\makecell{Qwen2.5-32B\\-Instruct}} & \textbf{\makecell{Qwen2.5-72B\\-Instruct}} \\
\midrule
\multirow{1}{*}{COT} & cross & 46.74 & 26.97 & 32.13 & 39.78 & 41.80 & 58.20 & 48.54 & 48.76 \\
\midrule
\multirow{2}{*}{HPD} & hpd\_en & 15.37 & 7.88 & 11.38 & 11.45 & 15.01 & 14.82 & 15.36 & 10.03 \\
 & hpd\_zh & 20.75 & 13.49 & 13.29 & 15.73 & 19.44 & 20.66 & 21.73 & 20.75 \\
 \midrule
\multirow{10}{*}{SocialBench} & socialbench\_sap\_zh & 70.21 & 32.32 & 64.22 & 75.27 & 77.22 & 75.02 & 82.53 & 75.95 \\
 & socialbench\_sap\_en & 59.10 & 29.55 & 59.78 & 78.25 & 85.64 & 87.41 & 87.55 & 84.13 \\
 & socialbench\_sa\_rolestyle\_zh & 54.18 & 26.63 & 45.82 & 51.39 & 69.66 & 70.28 & 78.02 & 72.45 \\
 & socialbench\_sa\_rolestyle\_en & 50.41 & 30.27 & 46.49 & 62.70 & 63.38 & 80.27 & 85.14 & 81.76 \\
 & socialbench\_sa\_roleknowledge\_zh & 66.67 & 32.35 & 61.98 & 83.70 & 89.38 & 95.06 & 94.07 & 97.53 \\
 & socialbench\_sa\_roleknowledge\_en & 46.05 & 37.25 & 67.61 & 82.39 & 89.47 & 95.04 & 95.75 & 93.52 \\
 & socialbench\_mem\_short\_zh & 79.09 & 13.17 & 28.98 & 72.04 & 77.69 & 87.58 & 82.42 & 83.98 \\
 & socialbench\_mem\_short\_en & 76.75 & 22.49 & 40.18 & 68.91 & 78.75 & 86.48 & 84.26 & 85.51 \\
 & socialbench\_mem\_long\_zh & 39.74 & 12.16 & 38.22 & 67.98 & 78.20 & 86.69 & 84.07 & 83.79 \\
 & socialbench\_mem\_long\_en & 57.18 & 14.44 & 54.29 & 67.21 & 79.66 & 86.98 & 82.15 & 84.65 \\
 \midrule
\multirow{4}{*}{InCharacter} & incharacter\_16Personalities\_en & 0.75 & 0.49 & 0.47 & 0.49 & 0.54 & 0.43 & 0.52 & 0.53 \\
 & incharacter\_16Personalities\_zh & 0.65 & 0.68 & 0.50 & 0.68 & 0.36 & 0.68 & 0.55 & 0.50 \\
 & incharacter\_BFI\_en & 0.40 & 0.43 & 0.50 & 0.49 & 0.52 & 0.49 & 0.49 & 0.42 \\
 & incharacter\_BFI\_zh & 0.73 & 0.63 & 0.67 & 0.60 & 0.63 & 0.53 & 0.70 & 0.70 \\
 \midrule
Charactereval & charactereval & 2.58 & 1.81 & 2.52 & 2.39 & 2.65 & 2.99 & 3.04 & 3.10 \\
\midrule
\multirow{3}{*}{RoleBench} & instruction\_generalization\_en & 27.13 & 16.31 & 22.33 & 22.66 & 25.81 & 25.55 & 24.60 & 23.61 \\
 & instruction\_generalization\_zh & 21.19 & 14.12 & 17.82 & 16.55 & 19.83 & 18.95 & 19.81 & 18.38 \\
 & role\_generalization\_en & 27.42 & 18.04 & 22.86 & 22.73 & 25.99 & 25.77 & 25.24 & 24.42 \\
\bottomrule
\end{tabular}%
}
\caption{Performance of role-playing with chain-of-thought reasoning on various role-playing benchmarks (part 1).}
\label{tab:cot_performance_part_1}
\end{table*}


\begin{table*}[ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{llccccccccc}
\toprule
\textbf{Dataset} & \textbf{Sub Dataset} & \textbf{\makecell{Gemma-2\\-2B-it}} & \textbf{\makecell{Gemma-2\\-9B-it}} & \textbf{\makecell{Gemma-2\\-27B-it}} & \textbf{\makecell{Llama-3.2-1B\\-Instruct}} & \textbf{\makecell{Llama-3.2-3B\\-Instruct}} & \textbf{\makecell{Llama-3.1-8B\\-Instruct}} & \textbf{\makecell{Llama-3.3-70B\\-Instruct}} & \textbf{\makecell{Mistral-8B\\-Instruct-2410}} & \textbf{\makecell{Mistral-7B\\-Instruct-v0.3}} \\
\midrule
Cross & cross & 33.48 & 50.11 & 42.25 & 27.87 & 40.45 & 46.97 & 62.70 & 39.10 & 41.12 \\
\midrule
\multirow{2}{*}{HPD} & hpd\_en & 7.01 & 8.04 & 7.39 & 6.84 & 11.17 & 4.90 & 6.85 & 15.15 & 10.68 \\
 & hpd\_zh & 13.49 & 14.82 & 9.81 & 12.48 & 16.04 & 16.60 & 20.27 & 12.82 & 15.64 \\
 \midrule
\multirow{10}{*}{SocialBench} & socialbench\_sap\_zh & 71.31 & 85.91 & 85.74 & 33.42 & 78.99 & 85.65 & 92.07 & 80.25 & 74.60 \\
 & socialbench\_sap\_en & 77.15 & 88.37 & 91.93 & 41.04 & 80.44 & 66.21 & 95.21 & 63.34 & 58.69 \\
 & socialbench\_sa\_rolestyle\_zh & 49.85 & 68.73 & 69.04 & 23.22 & 50.77 & 61.30 & 77.09 & 66.87 & 52.94 \\
 & socialbench\_sa\_rolestyle\_en & 64.19 & 80.41 & 82.70 & 30.95 & 55.14 & 54.46 & 89.19 & 48.65 & 50.95 \\
 & socialbench\_sa\_roleknowledge\_zh & 69.63 & 85.68 & 89.88 & 24.94 & 64.94 & 78.77 & 92.35 & 77.78 & 68.15 \\
 & socialbench\_sa\_roleknowledge\_en & 74.19 & 93.52 & 93.42 & 33.60 & 71.76 & 70.85 & 95.34 & 47.98 & 44.64 \\
 & socialbench\_mem\_short\_zh & 63.80 & 81.29 & 82.42 & 31.99 & 45.39 & 81.13 & 91.67 & 90.00 & 71.40 \\
 & socialbench\_mem\_short\_en & 70.49 & 70.57 & 49.78 & 27.71 & 50.84 & 85.37 & 89.23 & 85.54 & 86.62 \\
 & socialbench\_mem\_long\_zh & 63.67 & 83.56 & 82.16 & 29.47 & 35.65 & 74.90 & 89.66 & 87.70 & 73.89 \\
 & socialbench\_mem\_long\_en & 63.52 & 75.21 & 52.53 & 28.60 & 60.87 & 86.10 & 87.19 & 86.11 & 83.60 \\
 \midrule
\multirow{4}{*}{InCharacter} & incharacter\_16Personalities\_en & 0.56 & 0.51 & 0.55 & 0.48 & 0.54 & 0.49 & 0.51 & 0.44 & 0.53 \\
 & incharacter\_16Personalities\_zh & 0.59 & 0.68 & 0.55 & 0.64 & 0.64 & 0.73 & 0.77 & 0.45 & 0.45 \\
 & incharacter\_BFI\_en & 0.49 & 0.47 & 0.48 & 0.55 & 0.49 & 0.47 & 0.54 & 0.46 & 0.52 \\
 & incharacter\_BFI\_zh & 0.57 & 0.70 & 0.53 & 0.67 & 0.67 & 0.70 & 0.63 & 0.53 & 0.60 \\
 \midrule
Charactereval & charactereval & 2.29 & 2.48 & 2.56 & 2.15 & 2.11 & 2.67 & 2.70 & 2.55 & 2.21 \\
\midrule
\multirow{3}{*}{RoleBench} & instruction\_generalization\_en & 22.18 & 23.54 & 23.55 & 17.53 & 19.48 & 22.38 & 20.72 & 28.03 & 24.89 \\
 & instruction\_generalization\_zh & 16.31 & 18.67 & 18.65 & 14.58 & 16.03 & 19.38 & 19.58 & 20.83 & 18.04 \\
 & role\_generalization\_en & 23.52 & 24.33 & 24.73 & 15.59 & 17.17 & 21.74 & 17.79 & 26.70 & 23.00 \\
\bottomrule
\end{tabular}%
}
\caption{Performance of role-playing with chain-of-thought reasoning on various role-playing benchmarks (part 2).}
\label{tab:cot_performance_part_2}
\end{table*}





\begin{table*}[ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{llccccccc}
\toprule
\textbf{Dataset} & \textbf{Sub Dataset} & \textbf{\makecell{OpenAI\\o1-mini}} & \textbf{\makecell{Deepseek\\-R1}} & \textbf{\makecell{QwQ-32B\\-Preview}} & \textbf{\makecell{DeepSeek-R1\\-Distill-Llama-8B}} & \textbf{\makecell{DeepSeek-R1\\-Distill-Qwen-14B}} & \textbf{\makecell{DeepSeek-R1\\-Distill-Qwen-32B}} & \textbf{\makecell{DeepSeek-R1\\-Distill-Llama-70B}} \\
\midrule
Cross & cross & 45.23 & 44.19 & 62.02 & 45.17 & 46.97 & 50.11 & 51.91 \\
\midrule
\multirow{2}{*}{HPD} & hpd\_en & 14.96 & 13.28 & 3.95 & 9.25 & 12.21 & 12.41 & 12.04 \\
 & hpd\_zh & 19.87 & 20.87 & 10.14 & 9.22 & 17.48 & 18.58 & 15.85 \\
 \midrule
\multirow{10}{*}{SocialBench} & socialbench\_sap\_zh & 68.11 & 64.72 & 79.16 & 77.97 & 81.94 & 88.69 & 89.70 \\
 & socialbench\_sap\_en & 57.08 & 56.52 & 85.09 & 83.31 & 88.92 & 81.67 & 94.12 \\
 & socialbench\_sa\_rolestyle\_zh & 52.29 & 56.87 & 70.90 & 64.09 & 72.76 & 78.02 & 78.02 \\
 & socialbench\_sa\_rolestyle\_en & 48.29 & 46.33 & 84.05 & 67.97 & 77.97 & 61.22 & 87.84 \\
 & socialbench\_sa\_roleknowledge\_zh & 64.89 & 62.10 & 94.81 & 81.23 & 93.33 & 93.58 & 94.57 \\
 & socialbench\_sa\_roleknowledge\_en & 44.17 & 47.21 & 95.55 & 85.83 & 82.89 & 80.47 & 97.06 \\
 & socialbench\_mem\_short\_zh & 77.42 & 77.82 & 83.09 & 71.77 & 76.03 & 80.91 & 86.88 \\
 & socialbench\_mem\_short\_en & 74.23 & 73.28 & 75.68 & 78.60 & 80.13 & 89.87 & 90.47 \\
 & socialbench\_mem\_long\_zh & 37.42 & 39.26 & 82.03 & 73.34 & 79.87 & 85.45 & 86.80 \\
 & socialbench\_mem\_long\_en & 55.71 & 57.45 & 71.46 & 80.40 & 84.14 & 88.69 & 89.29 \\
 \midrule
\multirow{4}{*}{InCharacter} & incharacter\_16Personalities\_en & 0.67 & 0.67 & 0.51 & 0.52 & 0.47 & 0.61 & 0.60 \\
 & incharacter\_16Personalities\_zh & 0.52 & 0.59 & 0.45 & 0.64 & 0.59 & 0.55 & 0.64 \\
 & incharacter\_BFI\_en & 0.44 & 0.41 & 0.49 & 0.50 & 0.49 & 0.41 & 0.50 \\
 & incharacter\_BFI\_zh & 0.63 & 0.63 & 0.43 & 0.53 & 0.50 & 0.57 & 0.43 \\
 \midrule
Charactereval & charactereval & 2.56 & 2.32 & 2.78 & 2.38 & 2.70 & 2.69 & 2.63 \\
\midrule
\multirow{3}{*}{RoleBench} & instruction\_generalization\_en & 24.91 & 23.64 & 9.54 & 19.54 & 20.45 & 17.66 & 19.54 \\
 & instruction\_generalization\_zh & 20.29 & 18.73 & 17.45 & 12.22 & 18.31 & 15.86 & 14.73 \\
 & role\_generalization\_en & 26.66 & 24.47 & 10.97 & 18.21 & 19.76 & 16.38 & 17.66 \\
\bottomrule
\end{tabular}%
}
\caption{Performance of role-playing using reasoning-optimized LLMs on various role-playing benchmarks.}
\label{tab:o1_performance}
\end{table*}



\section{Fine-Grained Results on CharacterEval}\label{app:sec:charactereval}


Table~\ref{tab:charactereval_zero_shot_performance_1}, \ref{tab:charactereval_zero_shot_performance_2}, \ref{tab:charactereval_cot_performance_1}, \ref{tab:charactereval_cot_performance_2}, and \ref{tab:charactereval_o1_performance} present the fine-grained performance of different models on CharacterEval using three role-playing methods.
The evaluation consists of 12 dimensions, with all scores provided by a pretrained reward model.

\begin{table*}[!htb]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccccc}
\toprule
\textbf{Metric} & \textbf{\makecell{OpenAI \\ GPT-4 Turbo}} & \textbf{\makecell{Qwen2.5-0.5B\\-Instruct}} & \textbf{\makecell{Qwen2.5-1.5B\\-Instruct}} & \textbf{\makecell{Qwen2.5\\-3B-Instruct}} & \textbf{\makecell{Qwen2.5-7B\\-Instruct}} & \textbf{\makecell{Qwen2.5-14B\\-Instruct}} & \textbf{\makecell{Qwen2.5-32B\\-Instruct}} & \textbf{\makecell{Qwen2.5-72B\\-Instruct}} \\
\midrule
Accuracy & 2.92  & 1.90  & 2.35  & 2.86  & 2.98  & 3.17  & 3.10  & 3.29  \\
Behavior & 1.39  & 1.29  & 1.19  & 1.26  & 1.67  & 2.40  & 2.20  & 2.65  \\
Coherence & 3.29  & 2.20  & 2.76  & 3.19  & 3.60  & 3.90  & 3.76  & 3.93  \\
Communication Skills & 2.68  & 1.66  & 1.87  & 2.75  & 2.98  & 3.25  & 3.15  & 3.53  \\
Consistency & 2.90  & 1.83  & 2.48  & 2.73  & 3.27  & 3.65  & 3.46  & 3.64  \\
Diversity & 1.36  & 1.19  & 1.19  & 1.31  & 1.60  & 2.00  & 1.93  & 2.18  \\
Empathy & 2.94  & 1.88  & 2.33  & 2.86  & 3.14  & 3.33  & 3.21  & 3.46  \\
Exposure & 1.91  & 1.39  & 1.42  & 2.02  & 2.03  & 2.18  & 2.21  & 2.47  \\
Fluency & 3.03  & 1.94  & 2.42  & 2.90  & 3.34  & 3.53  & 3.39  & 3.63  \\
Hallucination & 2.55  & 1.70  & 2.08  & 2.46  & 2.78  & 3.05  & 2.94  & 3.17  \\
Humanlikeness & 2.78  & 1.75  & 2.41  & 2.59  & 3.11  & 3.48  & 3.30  & 3.47  \\
Utterance & 2.48  & 1.67  & 2.11  & 2.41  & 2.74  & 3.04  & 2.90  & 3.09  \\
\bottomrule
\end{tabular}%
}
\caption{Fine-grained results of direct zero-shot role-playing on CharacterEval benchmark (part1).}
\label{tab:charactereval_zero_shot_performance_1}
\end{table*}


\begin{table*}[ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccccccc}
\toprule
\textbf{Metrics}  & \textbf{\makecell{Gemma-2\\-2B-it}} & \textbf{\makecell{Gemma-2\\-9B-it}} & \textbf{\makecell{Gemma-2\\-27B-it}} & \textbf{\makecell{Llama-3.2-1B\\-Instruct}} & \textbf{\makecell{Llama-3.2-3B\\-Instruct}} & \textbf{\makecell{Llama-3.1-8B\\-Instruct}} & \textbf{\makecell{Llama-3.3-70B\\-Instruct}} & \textbf{\makecell{Mistral-8B\\-Instruct-2410}} & \textbf{\makecell{Mistral-7B\\-Instruct-v0.3}} \\
\midrule
Accuracy & 2.52 & 2.80 & 2.90 & 2.37 & 2.56 & 2.94 & 3.12 & 2.85 & 2.62  \\
Behavior & 1.20 & 1.14 & 1.16 & 1.86 & 1.58 & 2.61 & 2.04 & 1.16 & 1.22  \\
Coherence & 3.00 & 3.42 & 3.47 & 2.69 & 2.87 & 3.55 & 3.70 & 3.65 & 3.04  \\
Communication Skills & 2.16 & 2.12 & 2.10 & 2.08 & 2.15 & 2.94 & 2.99 & 2.61 & 2.53  \\
Consistency & 2.67 & 3.27 & 3.38 & 2.23 & 2.53 & 3.31 & 3.43 & 3.36 & 2.49  \\
Diversity & 1.25 & 1.20 & 1.25 & 1.57 & 1.47 & 2.18 & 1.85 & 1.22 & 1.26  \\
Empathy & 2.51 & 2.78 & 2.81 & 2.36 & 2.49 & 3.02 & 3.16 & 3.06 & 2.64  \\
Exposure & 1.59 & 1.50 & 1.51 & 1.67 & 1.67 & 2.12 & 2.06 & 1.79 & 1.91  \\
Fluency & 2.74 & 3.09 & 3.10 & 2.44 & 2.60 & 3.28 & 3.40 & 3.30 & 2.68  \\
Hallucination & 2.23 & 2.56 & 2.55 & 2.05 & 2.17 & 2.73 & 2.87 & 2.69 & 2.26  \\
Humanlikeness & 2.64 & 3.34 & 3.43 & 2.30 & 2.58 & 3.18 & 3.31 & 3.20 & 2.29  \\ 
Utterance & 2.26 & 2.61 & 2.67 & 2.09 & 2.20 & 2.77 & 2.92 & 2.71 & 2.20  \\
\bottomrule
\end{tabular}%
}
\caption{Fine-grained results of direct zero-shot role-playing on CharacterEval benchmark (part2).}
\label{tab:charactereval_zero_shot_performance_2}
\end{table*}


\begin{table*}[ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccccc}
\toprule
\textbf{Metric} & \textbf{\makecell{OpenAI \\ GPT-4 Turbo}} & \textbf{\makecell{Qwen2.5 \\ 0.5B-Instruct}} & \textbf{\makecell{Qwen2.5 \\ 1.5B-Instruct}} & \textbf{\makecell{Qwen2.5 \\ 3B-Instruct}} & \textbf{\makecell{Qwen2.5 \\ 7B-Instruct}} & \textbf{\makecell{Qwen2.5 \\ 14B-Instruct}} & \textbf{\makecell{Qwen2.5 \\ 32B-Instruct}} & \textbf{\makecell{Qwen2.5 \\ 72B-Instruct}} \\
\midrule
{Accuracy} & 2.87 & 2.13 & 2.76 & 2.81 & 2.96 & 3.17 & 3.19 & 3.26  \\
{Behavior} & 2.00 & 1.37 & 1.52 & 1.30 & 1.53 & 2.18 & 2.38 & 2.25  \\
{Coherence} & 3.27 & 2.28 & 3.41 & 3.14 & 3.49 & 3.76 & 3.79 & 3.84  \\
{Communication Skills} & 2.98 & 1.73 & 1.72 & 2.67 & 3.00 & 3.23 & 3.26 & 3.51  \\
{Consistency} & 2.72 & 1.86 & 3.35 & 2.63 & 3.06 & 3.48 & 3.49 & 3.52  \\
{Diversity} & 1.70 & 1.31 & 1.43 & 1.28 & 1.47 & 1.89 & 2.04 & 2.01  \\
{Empathy} & 2.93 & 1.96 & 2.71 & 2.82 & 3.03 & 3.29 & 3.32 & 3.44  \\
{Exposure} & 2.26 & 1.61 & 1.39 & 2.03 & 2.11 & 2.21 & 2.27 & 2.50  \\
{Fluency} & 2.90 & 2.04 & 3.04 & 2.82 & 3.09 & 3.45 & 3.49 & 3.50  \\
{Hallucination} & 2.41 & 1.79 & 2.50 & 2.39 & 2.66 & 2.95 & 2.98 & 3.06  \\
{Humanlikeness} & 2.51 & 1.87 & 3.69 & 2.48 & 2.75 & 3.36 & 3.32 & 3.28  \\
{Utterance} & 2.44 & 1.73 & 2.73 & 2.32 & 2.62 & 2.92 & 2.93 & 2.99  \\
\bottomrule
\end{tabular}%
}
\caption{Fine-grained results of role-playing with CoT on CharacterEval benchmark (part 1).}
\label{tab:charactereval_cot_performance_1}
\end{table*}



\begin{table*}[ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccccccc}
\toprule
\textbf{Metrics}  & \textbf{\makecell{Gemma-2\\-2B-it}} & \textbf{\makecell{Gemma-2\\-9B-it}} & \textbf{\makecell{Gemma-2\\-27B-it}} & \textbf{\makecell{Llama-3.2-1B\\-Instruct}} & \textbf{\makecell{Llama-3.2-3B\\-Instruct}} & \textbf{\makecell{Llama-3.1-8B\\-Instruct}} & \textbf{\makecell{Llama-3.3-70B\\-Instruct}} & \textbf{\makecell{Mistral-8B\\-Instruct-2410}} & \textbf{\makecell{Mistral-7B\\-Instruct-v0.3}} \\
\midrule
{Accuracy} & 2.63 & 2.88 & 2.91 & 2.52 & 2.51 & 2.90 & 3.07 & 2.87 & 2.57  \\
{Behavior} & 1.28 & 1.11 & 1.14 & 1.66 & 1.20 & 2.01 & 1.34 & 1.23 & 1.39  \\
{Coherence} & 3.11 & 3.38 & 3.50 & 2.71 & 2.79 & 3.38 & 3.58 & 3.43 & 2.86  \\
{Communication Skills} & 1.88 & 2.23 & 2.33 & 2.06 & 1.96 & 2.84 & 2.91 & 2.71 & 2.39  \\
{Consistency} & 2.90 & 3.19 & 3.30 & 2.22 & 2.46 & 2.98 & 3.20 & 3.03 & 2.37  \\
{Diversity} & 1.29 & 1.18 & 1.20 & 1.50 & 1.22 & 1.76 & 1.35 & 1.26 & 1.35  \\
{Empathy} & 2.53 & 2.77 & 2.92 & 2.38 & 2.41 & 2.95 & 3.08 & 3.01 & 2.55  \\
{Exposure} & 1.44 & 1.59 & 1.58 & 1.77 & 1.52 & 2.04 & 2.01 & 1.93 & 1.83  \\
{Fluency} & 2.71 & 3.00 & 3.20 & 2.47 & 2.51 & 3.06 & 3.30 & 3.13 & 2.59  \\
{Hallucination} & 2.21 & 2.53 & 2.60 & 2.13 & 2.09 & 2.63 & 2.76 & 2.57 & 2.15  \\
{Humanlikeness} & 3.06 & 3.30 & 3.35 & 2.28 & 2.54 & 2.89 & 3.03 & 2.87 & 2.32  \\ 
{Utterance} & 2.40 & 2.58 & 2.66 & 2.12 & 2.13 & 2.60 & 2.72 & 2.52 & 2.18  \\
\bottomrule
\end{tabular}%
}
\caption{Fine-grained results of role-playing with CoT on CharacterEval benchmark (part 2).}
\label{tab:charactereval_cot_performance_2}
\end{table*}



\begin{table*}[ht]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccc}
\toprule
\textbf{Metrics} & \textbf{\makecell{OpenAI\\o1-mini}} & \textbf{\makecell{Deepseek\\-R1}} & \textbf{\makecell{QwQ-32B\\-Preview}} & \textbf{\makecell{DeepSeek-R1\\-Distill-Llama-8B}} & \textbf{\makecell{DeepSeek-R1\\-Distill-Qwen-14B}} & \textbf{\makecell{DeepSeek-R1\\-Distill-Qwen-32B}} & \textbf{\makecell{DeepSeek-R1\\-Distill-Llama-70B}} \\
\midrule
Accuracy & 2.45 & 2.38 & 3.08 & 2.65 & 2.95 & 3.06 & 2.96 \\
Behavior & 1.29 & 1.27 & 1.64 & 1.63 & 1.62 & 1.31 & 1.43 \\
Coherence & 3.61 & 3.19 & 3.61 & 3.08 & 3.46 & 3.55 & 3.47 \\
Communication Skills & 2.42 & 2.31 & 2.94 & 2.40 & 2.93 & 2.83 & 2.79 \\
Consistency & 2.87 & 3.02 & 3.31 & 2.68 & 3.15 & 3.25 & 3.12 \\
Diversity & 1.19 & 1.20 & 1.53 & 1.49 & 1.52 & 1.33 & 1.40 \\
Empathy & 2.98 & 3.21 & 3.14 & 2.61 & 3.03 & 3.08 & 2.97 \\
Exposure & 1.94 & 2.08 & 2.09 & 1.88 & 2.06 & 2.01 & 1.99 \\
Fluency & 3.44 & 2.29 & 3.30 & 2.77 & 3.23 & 3.23 & 3.10 \\
Hallucination & 2.81 & 2.11 & 2.85 & 2.35 & 2.76 & 2.78 & 2.68 \\
Humanlikeness & 3.21 & 2.69 & 3.15 & 2.68 & 3.01 & 3.09 & 2.99 \\
Utterance & 2.54 & 2.12 & 2.76 & 2.36 & 2.66 & 2.71 & 2.62 \\
\bottomrule
\end{tabular}
}
\caption{Fine-grained results of various reasoning-enhanced models on CharacterEval.}
\label{tab:charactereval_o1_performance}
\end{table*}

