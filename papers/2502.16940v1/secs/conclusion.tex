\section{Conclusion}
This study aims to address the research question: “Can reasoning techniques enhance the role-playing capabilities of large language models (LLMs)?” 
To this end, we conduct extensive experiments using 6 role-playing benchmarks, 24 LLMs, and 3 distinct role-playing methods. 
Our experimental results lead to the following key findings: CoT may reduce role-playing performance, reasoning-optimized LLMs are unsuitable for role-playing, reasoning ability disrupts the role-playing scaling law, the Qwen series is well-suited for role-playing tasks, Chinese role-playing performance surpasses English role-playing performance, and large models still lack proficiency in advanced role-playing.
All code is integrated into OpenCompass, ensuring reproducibility and facilitating further research. 
We hope that our findings provide new perspectives for future studies on role-playing LLMs.