\section{Related Works}
\subsection{Point Cloud Segmentation}
3D point cloud segmentation classifies the point cloud into meaningful regions or segments that belong to the same class~\cite{tang2022contrastive,park2022fast,peng2023openscene}. Some 3D point cloud segmentation methods primarily rely on training closed-set models. PointNet~\cite{qi2017pointnet} directly learns a spatial encoding of each point, and PointNet++~\cite{qi2017pointnet++} extends it with a local feature extractor based on Farthest Point Sampling (FPS) and is trained with hierarchical feature learning architecture. There are other methods initially processing 2D images and then mapping the segmented 2D results onto the corresponding 3D coordinates of the point cloud. MVPNet~\cite{jaritz2019multi} aggregates 2D multi-view image features into 3D point clouds, and then uses point-based networks to fuse features in 3D canonical space to predict 3D semantic labels. VMVF~\cite{kundu2020virtual} selects various virtual views to render multiple 2D channels for training an effective 2D semantic segmentation model and then fuses features from these predictions onto the 3D mesh vertices to determine semantic labels. However, these methods depend on existing point cloud data as input, which limits their versatility for downstream tasks.
\subsection{Nerf Segmentation}
Semantic-NeRF~\cite{zhi2021place} was the first to integrate semantics into NeRF by fusing noisy 2D segmentations into a consistent 3D model, improving segmentation accuracy and enabling novel view synthesis. Panoptic NeRF~\cite{fu2022panoptic} and DM-NeRF~\cite{wang2022dm} explore panoptic radiance fields for label transfer and scene editing, but both rely on manual ground truth annotations. Panoptic Neural Fields~\cite{kundu2022panoptic} decomposes scenes into objects and backgrounds using instance-specific MLPs for objects and a shared MLP for the background, optimized jointly from color images and predicted segmentations.
Several studies~\cite{kerr2023lerf,liu2023weakly} have explored the approach of lifting latent features from 2D foundation models~\cite{radford2021learning} into 3D space to enable open-vocabulary text queries. Other approaches~\cite{shen2023distilled,goel2023interactive,wei2024nto3d,kim2024garfield} have demonstrated promising results in object-level segmentation tasks. Panoptic Lifting~\cite{siddiqui2023panoptic} and Contrastive Lift~\cite{bhalgat2023contrastive} generate 3D panoptic representations by lifting 2D machine-generated segmentation masks to 3D for multi-view consistency. While Panoptic Lifting~\cite{siddiqui2023panoptic} addresses inconsistencies in 2D instance identifiers through linear assignment, Contrastive Lift~\cite{bhalgat2023contrastive} uses contrastive clustering. Despite their success in multi-view consistent segmentation, NeRF-based methods are limited by slow rendering speeds and high memory usage during training due to their implicit nature.

\subsection{Gaussian Segmentation}
Segmenting the Gaussian field involves dividing it into distinct regions based on their properties, which is crucial for scene reconstruction and understanding. LangSplat~\cite{qin2024langsplat}, LEGaussians~\cite{shi2024language} and several works~\cite{chen2024ovgaussian,peng2024gags,liang2024supergseg,hu2024sparselgs,cheng2024occam,peng20243d,qiu2024gls} incorporate language features from CLIP for open-world scene representation. SADG~\cite{li2024sadg} specifically targets segmentation in dynamic scenes. For single-object segmentation, GaussianCut~\cite{jain2024gaussiancut} proposes a Gaussian distribution-based optimization framework, while GradiSeg~\cite{li2024gradiseg} develops a novel gradient-driven segmentation approach. PLGS~\cite{wang2024plgs} adopts a methodology similar to Panoptic Lifting~\cite{siddiqui2023panoptic}. InstanceGaussian~\cite{li2024instancegaussian} and BCG~\cite{zhang2024bootstraping} propose clustering-based methods for segmenting 3D Gaussian representations.
SAGA~\cite{cen2023segment} efficiently embeds 2D segmentation features into 3D Gaussian point features using contrastive learning. Feature 3DGS~\cite{zhou2024feature} enables 3D Gaussian splatting on semantic features via 2D foundation model distillation to extract arbitrary-dimension semantic features. Gaussian Grouping~\cite{ye2023gaussian} and CoSSegGaussians~\cite{dou2024cosseggaussians} apply video segmentation methods to unify segmentation IDs from multiple views, However, video segmentation methods often fail when there are significant changes in viewing angles. OpenGaussian~\cite{wu2024opengaussian} achieves consistent 3D segmentation through codebook discretization but cannot render precise 2D segmentations. Gaga~\cite{lyu2024gaga} shares a similar motivation with our work. However, it does  not explicitly consider the role of segmentation in Gaussian optimization, and the lack of segmentation-constrained optimization can result in meaningless floaters in the 3D segmentation field. Our approach overcomes these challenges by using point map fusion and plane-constrained Gaussian splatting to create a compact and consistent 3D Gaussian segmentation field.