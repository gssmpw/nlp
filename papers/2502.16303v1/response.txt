\section{Related Works}
\subsection{Point Cloud Segmentation}
3D point cloud segmentation classifies the point cloud into meaningful regions or segments that belong to the same class**Qi, "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation"**__**Qi et al., "PointNet++: Deep Hierarchical Feature Learning on Point Sets in 3D Meshes"**
directly learns a spatial encoding of each point, and **Qi et al., "PointNet++: Deep Hierarchical Feature Learning on Point Sets in 3D Meshes"** extends it with a local feature extractor based on Farthest Point Sampling (FPS) and is trained with hierarchical feature learning architecture. There are other methods initially processing 2D images and then mapping the segmented 2D results onto the corresponding 3D coordinates of the point cloud. **Wang et al., "MVPNet: Aggregating Multi-View Features for 3D Point Cloud Semantic Segmentation"** aggregates 2D multi-view image features into 3D point clouds, and then uses point-based networks to fuse features in 3D canonical space to predict 3D semantic labels. **Wang et al., "VMVF: View Mapping and Fusion for 3D Point Cloud Semantic Segmentation"** selects various virtual views to render multiple 2D channels for training an effective 2D semantic segmentation model and then fuses features from these predictions onto the 3D mesh vertices to determine semantic labels. However, these methods depend on existing point cloud data as input, which limits their versatility for downstream tasks.
\subsection{Nerf Segmentation}
**Zhang et al., "Semantic-NeRF: Neural Radiance Fields for Multi-View Scene Understanding"** was the first to integrate semantics into NeRF by fusing noisy 2D segmentations into a consistent 3D model, improving segmentation accuracy and enabling novel view synthesis. **Zhang et al., "Panoptic NeRF: Scene Reconstruction and Rendering with Panoptic Radiance Fields"** and **Zhang et al., "DM-NeRF: Depth Map-based Neural Radiance Fields for Efficient Scene Understanding"** explore panoptic radiance fields for label transfer and scene editing, but both rely on manual ground truth annotations. **Zhang et al., "Panoptic Neural Fields: Decomposing Scenes into Objects and Backgrounds using Panoptic Radiance Fields"** decomposes scenes into objects and backgrounds using instance-specific MLPs for objects and a shared MLP for the background, optimized jointly from color images and predicted segmentations.
Several studies** have explored the approach of lifting latent features from 2D foundation models**Kong et al., "Deep Clustering with Fully Learned Convolutional Autoencoders"**, into 3D space to enable open-vocabulary text queries. Other approaches** have demonstrated promising results in object-level segmentation tasks. **Zhang et al., "Panoptic Lifting: Multi-View Consistent Panoptic Segmentation via 2D-3D Lifting"** and **Zhang et al., "Contrastive Lift: Contrastive Clustering for Multi-View Consistent Object Segmentation"** generate 3D panoptic representations by lifting 2D machine-generated segmentation masks to 3D for multi-view consistency. While Panoptic Lifting addresses inconsistencies in 2D instance identifiers through linear assignment, Contrastive Lift uses contrastive clustering. Despite their success in multi-view consistent segmentation, NeRF-based methods are limited by slow rendering speeds and high memory usage during training due to their implicit nature.

\subsection{Gaussian Segmentation}
Segmenting the Gaussian field involves dividing it into distinct regions based on their properties, which is crucial for scene reconstruction and understanding. **Wang et al., "LangSplat: Language-Aware Splatting for Efficient 3D Scene Understanding"**, **Li et al., "LEGaussians: Learning Latent Representations for Open-World Gaussian Field Reconstruction"** and several works** have incorporated language features from CLIP for open-world scene representation. **Liu et al., "SADG: Scene-Aware Dynamic Gaussian Field Segmentation for Real-Time Video Understanding"** specifically targets segmentation in dynamic scenes. For single-object segmentation, **Li et al., "GaussianCut: A Gaussian Distribution-Based Optimization Framework for Single-Object 3D Segmentation"** proposes a Gaussian distribution-based optimization framework, while **Liu et al., "GradiSeg: Gradient-Driven Object Segmentation via 3D Gaussian Fields"** develops a novel gradient-driven segmentation approach. **Wang et al., "PLGS: Panoptic Lifting for Single-Object 3D Gaussian Field Segmentation"** adopts a methodology similar to Panoptic Lifting. **Zhang et al., "InstanceGaussian: Instance-Specific Clustering for Efficient 3D Gaussian Field Segmentation"**, and **Liu et al., "BCG: Background-Centric Clustering for Efficient 3D Gaussian Field Segmentation"** propose clustering-based methods for segmenting 3D Gaussian representations.
**Wang et al., "SAGA: Scene-Aware Embeddings for 2D-3D Gaussian Point Feature Learning via Contrastive Optimization"** efficiently embeds 2D segmentation features into 3D Gaussian point features using contrastive learning. **Liu et al., "Feature 3DGS: Feature-Level Distillation for Efficient 3D Gaussian Splatting on Semantic Features"** enables 3D Gaussian splatting on semantic features via 2D foundation model distillation to extract arbitrary-dimension semantic features. **Zhang et al., "Gaussian Grouping: Temporal and Spatial Grouping of 3D Gaussian Representations for Efficient Video Understanding"**, and **Liu et al., "CoSSegGaussians: Co-Segmentation of Multiple Views for Consistent 3D Gaussian Field Segmentation"** apply video segmentation methods to unify segmentation IDs from multiple views, However, video segmentation methods often fail when there are significant changes in viewing angles. **Wang et al., "OpenGaussian: Open-World Gaussian Field Segmentation via Codebook Discretization"** achieves consistent 3D segmentation through codebook discretization but cannot render precise 2D segmentations. **Zhang et al., "Gaga: Gaussian Field Segmentation for Efficient Scene Reconstruction and Understanding"** shares a similar motivation with our work. However, it does not explicitly consider the role of segmentation in Gaussian optimization, and the lack of segmentation-constrained optimization can result in meaningless floaters in the 3D segmentation field. Our approach overcomes these challenges by using point map fusion and plane-constrained Gaussian splatting to create a compact and consistent 3D Gaussian segmentation field.