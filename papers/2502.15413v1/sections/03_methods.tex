\section{Methods}

\subsection{Multi-Device Rendering}

Our current method requires a unique setup to render to two devices, the touchscreen and the head-mounted display (HMD), at the same time. Moreover, the view of the HMD should be included on the touchscreen in a scaled down format. We used Unity~\cite{unity} to realize this. Since the VR camera is in the same scene as the touchscreen camera, we have to specify in which order they draw.

To control in which order the cameras draw, we have to control their depth values. First, the VR camera is rendered with a depth of -1. A script captures a view, scales it down and saves it to a texture. Then, the camera belonging to the touchscreen is rendered with a depth of 0. Inside the screen-space UI, the texture containing the VR view is included. This process is illustrated in \autoref{fig:rendering}.

\begin{figure}[tb]
 \centering
 \includegraphics[width=\columnwidth]{figures/RenderProcess.png}
 \caption{Camera setup designed to draw to both devices at the same time.}
 \label{fig:rendering}
\end{figure}

\subsection{Tools}

For both the Educators using the touchscreen and the learners in VR, we provide a set of tools to enable interaction with the system. Users of the touchscreen can access them via a Toolbar, fixed at the bottom left of the screen. There, they can switch between tools, while only one tool can be active at a time.

To the learners, similar tools are provided as virtual objects, positioned in the scenes on a tray. By using the XR Interaction Toolkit\footnote{Unity XR Interaction Toolkit \url{https://docs.unity3d.com/Packages/com.unity.xr.interaction.toolkit@2.5}}, the learners could pick up objects by pointing at them while closing their hand around the controller. Opening the hand again resulted in the object being dropped. The controller is securely affixed to the back of the hand via a strap, ensuring that it remains firmly in place during operation. When objects were dropped on the floor, they are returned to their original position on the tray. The appearances of the tools in VR was chosen to be very similar to familiar tools in reality to reduce the time learners need to get accustomed to them.

\subsubsection{Movement}
\subpart{Touchscreen} To enable movement for educators, we combined multiple approaches. We opted to use a free-fly camera, similar to the Move\&Look technique~\cite{multiTouch3D}. Using a single finger to interact with the touchscreen enables the user to move towards points in space (by tapping on that point and moving their finger downward) or to move away (by moving the finger upward). Moving the finger left and right rotates the camera by applying yaw to it. Touching with two fingers and moving them, the user may rotate the camera around the yaw and pitch axis.

We introduce another way to move perpendicular to the ground in the form of a height-slider. The slider always reflects the height of the camera. By manipulating it, the user may quickly move through objects vertically. This is useful in moments, when the camera is positioned above an object, and the user wants to move below it.

\subpart{Virtual Reality} The system was designed for a Head-Mounted Display with 3D spatial tracking with 6 Degrees of Freedom. Movement in VR is thus equal to the movement in the real world, which is an intuitive solution, enabling users to move around by walking or changing their pose. To reduce the need to move or bow down, a tool was introduced in the form of another height-slider. This one is positioned above the color picker for the VR user (described below) and controls the height of the content of the scene, except for the tools. This is designed with smaller or larger users in mind.

\subsubsection{Draw, Erase and Fill}
One set of tools we supply to annotate scenes is enabling users to add colors to existing objects. This section details how users can do so. These tools were included because of their common use in other applications~\cite{vuforiaAnnotations, annotations1, annotations2}. We also provide the ability to color whole objects at once, the so-called fill tool. This is implemented by setting the albedo color of the renderer to the target value.

\subpart{Touchscreen} Educators can color objects by using the pen-tool. Once selected, they can simply swipe over the object they want to paint on to add lines. The color used is controlled by a color-picker which is displayed next to the toolbar as long as the user has the pen-tool selected. To evoke the fill tool, the user may double tap on an object to color it completely. The equivalent action using the eraser tool removes the global coloring of the object.

\subpart{Virtual Reality} For painting and erasing, we use a marker and an eraser provided by DokoDemoPainter~\cite{dokoDemo}. Additionally, a virtual brush, to change the color of an object, and an eraser on a stick, to reset the coloring, were added. A color picker, similar in form and function to the one provided on the touchscreen, is floating above the metal tray holding the tools, enabling selection of a color from a color picker restricted to the hue-value mode~\cite{hueValue}. To add the fill tool to VR, two new items were added: a brush and an eraser-brush, fulfilling their respective tasks.

\subsubsection{Text and Sequence}
Similarly to the draw tools, the text, and sequence tools were added driven by the findings of prior research that identified their utility~\cite{vuforiaAnnotations}. Our realization of text boxes can be seen at \autoref{fig:lapcamera}.

\subpart{Touchscreen} Another way to annotate scenes is to add actual text. This is more in line with traditional educational materials like textbooks. To place a text box, the user has to tap on a specific point while being in the text mode. Using a ray cast, the point in the scene that was tapped on is determined. This is chosen as the anchor for the text box. Then, a popup appears in which text may be entered using the default Windows touchscreen keyboard. To edit existing text, tapping an existing text box while the text tool is active opens a popup in which the existing text is contained. The user may delete or change the text this way. The Sequence tool works by assigning each text box an index, which dictates their order. The effects of this are detailed below in the Virtual Reality section. Tapping a text box using the sequence tool allows changing its index from the default 0.

\subpart{Virtual Reality} The learners can't create text boxes on their own. They can only interact with existing ones, marking them as ''read'' by pointing at the respective checkbox and pressing the trigger on the controller. 

The sequence is not explicitly visible to learners. Still, they benefit from the usage of this tool since they only see text boxes in a predefined order instead of all text boxes at once. The educator sets this order. Once the learner marks a text-box as 'read', the text-box which is unread and has the lowest sequence-index is displayed. If multiple text-boxes share the same index, they are all displayed at once.
%
Marking a text-box as read makes it smoothly shrink in size. This is done, so that other text-boxes which are near them are less likely to be occluded. While a controller is pointing at such a shrunk text-box, the text-box will regrow to normal size again.
%