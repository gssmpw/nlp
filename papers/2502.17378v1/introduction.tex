\section{Introduction}
\label{intro}

Machine Learning (ML) has become a cornerstone of modern software systems, driving advancements across diverse domains~\citep{washizaki2019studying, gonzalez2020state, pallathadka2023impact}. 
In this paper, we define ML projects as software initiatives that integrate ML components to enable intelligent functionalities. These projects either address domain-specific problems (i.e., ML applications or ML-enabled systems) or provide general-purpose solutions (i.e., ML tools).

As ML projects gain prominence and grow in complexity, ensuring their quality and reliability becomes increasingly challenging. 
These projects often involve computationally intensive tasks and exhibit inherent non-deterministic behavior \citep{nascimento2020software, giray2021software}, making it difficult to guarantee consistent performance and reproducibility.
Furthermore, ML projects face the unique challenge of deploying models to production, a task that can further strain development workflows~\citep{symeonidis2022mlops}.

To tackle these complexities, the field of Software Engineering for Machine Learning (\textsc{SE4ML}) has emerged, adapting traditional software engineering practices to the specific requirements of ML projects.
Key practices that are well established in traditional software development, such as those related to \textsc{DevOps}~\citep{leite2019survey}, have been adapted to the ML domain (e.g., \textsc{MLOps}~\citep{makinen2021needs}).
In particular,Continuous Integration (CI) practices have become pivotal for enhancing the development efficiency and release stability of ML projects\citep{gift2021practical}.

CI is a widely adopted practice in software development that involves frequent and automated integration of code changes into a shared repository, at least daily~\citep{fowler-ci-2006, duvall2007continuous}. 
In the ML domain, \cite{rzig2022characterizing} conducted an analysis revealing that approximately 37\% of ML projects have integrated a CI service into their workflows. The fundamental philosophy behind CI is to ensure that the software passes all tests and can build at all times \citep{duvall2007continuous}.
By fostering frequent commits, shorter build durations, and high test coverage, CI enhances development workflows and project reliability \citep{soares2022effects}. However, while extensively studied in traditional software projects \citep{vasilescu2015quality, hilton2016usage, zhao2017impact, bernardo2018studying, nery2019empirical, santos2022investigating, saraiva2023unveiling}, the integration of CI into ML projects remains relatively unexplored.

Unlike traditional CI for non-ML projects, CI in ML environments needs to handle the inherently probabilistic nature of ML components~\citep{renggli2019continuous}.
As such, in the context of ML, CI extends beyond merely testing and validating code and components and also involves testing and validation of data, data schemas, and models~\citep{karamitsos2020applying}. 
Our previous work conducted a quantitative analysis of 93 ML projects and 92 non-ML projects, examining the differences in the adoption of four key CI practices (i.e., frequent code integration, short build duration, quick build fixes, and comprehensive test coverage)  between these two types of projects.
Our findings indicated that ML projects often require longer build durations, while some exhibit lower test coverage compared to non-ML projects~\citep{bernardo2024machine}.
While these insights are valuable, the underlying reasons for the observed differences in build duration remain unclear.
A deeper understanding of the specific barriers faced by ML projects in adopting CI is crucial to inform practitioners and enable the full potential of CI in improving code quality and overall project success.

The general goal of this study is to deepen the understanding of the perceptions of ML practitioners regarding the differences, specific barriers, and strategies associated with CI adoption in ML projects.
Through thematic analysis of survey responses from 155 ML practitioners, we identify key themes -- such as computational complexity, dependency management, and data handling -- that contribute to extended build durations and reduced test coverage in ML projects.
These findings provide actionable insights for improving CI in the ML domain, fostering more efficient CI workflows, and advancing the integration of CI practices in this critical area of software development.
By bridging existing quantitative findings with practitioners' experiences, this study offers valuable guidance for researchers and practitioners aiming to optimize CI workflows for ML projects.

Our investigation is guided by the following Research Questions (RQs):

\begin{itemize}
    \item \textit{\textbf{\RQone}}
    \newline
    \textbf{Motivation:} 
    Our prior work~\citep{bernardo2024machine} identified significant differences in CI practices between ML and non-ML projects. However, the reasons behind these differences remain unclear. Gaining insights from ML practitioners' perspectives can shed light on the unique challenges of CI adoption in ML projects and guide the development of tailored solutions.
    
    \textbf{Findings:} 
    % \hl{[ADD 2-3 SENTENCES SUMMARIZING THE FINDINGS HERE]}
    Participants of our survey identified key differences in CI practices between ML and non-ML projects, particularly in test coverage (61.9\%) and build duration (63.2\%), while perceptions regarding fixing broken builds and commit frequency were more neutral. Challenges in CI implementation mentioned by the participants were categorized into eight main themes, including testing complexity, infrastructure requirements, and build duration stability. Co-occurrence analysis revealed strong interdependencies among these challenges, with resource and infrastructure requirements playing a central role in CI effectiveness. Our results emphasize the need for a holistic approach that considers testing, resource allocation, and model handling to enhance CI practices in ML projects.
    
    \item \textit{\textbf{\RQtwo}}
    \newline
    \textbf{Motivation:} 
    Build duration is a critical aspect of CI pipeline efficiency, directly affecting developer productivity and the pace of iteration cycles. While our previous study observed that ML projects often experience longer build times, the factors that ML practitioners associate with extended build durations remain unexplored.

    \textbf{Findings:}     
    % \hl{[ADD 2-3 SENTENCES SUMMARIZING THE FINDINGS HERE]}
    Short build durations are generally considered important by 69\% of participants, though tolerance increases with project size and complexity. 75\% of participants expect ML projects to have longer build durations than non-ML projects, largely due to higher project complexity, model training demands, increased computational resource needs, extensive data handling, and dependency management.     
    
    \item \textit{\textbf{\RQthree}}
    \newline
    \textbf{Motivation:} 
    Test coverage is a crucial metric for ensuring software reliability. However, ML projects face lower test coverage rates compared to non-ML projects ~\citep{bernardo2024machine}. Therefore, This RQ aims to investigate whether test coverage is handled differently in ML projects, providing insights into the specific obstacles when testing ML projects.

    \textbf{Findings:} 
    % \hl{[ADD 2-3 SENTENCES SUMMARIZING THE FINDINGS HERE]}
    Testing in ML projects is particularly challenging due to test complexity, data dependencies, the non-deterministic nature of ML systems, and computational resource constraints. Key challenges such as test reproducibility, data dependencies, and resource demands often overlap, highlighting the need for holistic testing approaches. While most practitioners (63\%) preferred high test coverage (70–100\%), a notable portion (20\%) accepted moderate coverage (50–70\%), citing ML-specific constraints.
\end{itemize}

\subsection*{\textbf{Paper organization}}

The remainder of this paper is organized as follows: 
Section \ref{sec:related_work} presents an overview of related work, highlighting key studies and prior research that establish the foundation and contextual motivation for our investigation.
Section \ref{sec:methodology} describes the study methodology, detailing the studied projects, datasets, and analytical approaches employed in our investigation. 
Section \ref{sec:demographics} presents the demographics of the study participants.
In Section \ref{sec:results}, we present the results of our analysis, highlighting key findings and patterns observed in the data. 
Section \ref{sec:discussion} delves into the implications of the results for research and practice. Finally, Section \ref{sec:limitations} examines the limitations of our study while Section \ref{sec:conclusion} concludes the paper.