\section{Discussion}
\label{sec:discussion}

In this section, we discuss the findings presented in Section \ref{sec:results}, structuring our discussion into two key topics: rethinking CI practices for ML projects (Section \ref{sec:disc_rethinking_ci_practices}) and managing build duration and test coverage in ML workflows (Section \ref{sec:handling_build_duration_and_test_cov}).

% Our study provides a practitioner-driven perspective on the complexities of adopting CI practices in ML projects. Across $RQ1$, $RQ2$, and $RQ3$, our findings reveal critical challenges, overlapping demands, and opportunities to refine CI processes in the ML domain. 

The responses from participants underscores the significance of this research in addressing the unique challenges of CI in ML projects. For example, P25 noted, \textit{``Your research is quite interesting and I'm curious to see what conclusions you ultimately reach. For ML projects today, there is often a lack of optimization and testing, so insights in this area would be very valuable.''} Similarly, P63 expressed enthusiasm for actionable findings, stating that \textit{``I really want to know about the findings!!! And would love to participate more in the future surveys''}. These perspectives highlight a genuine need for practical solutions that can help ML practitioners navigate the complexities of CI adoption. 

Beyond addressing technical obstacles, our research also resonated with participants in terms of depth and relevance. As P144 remarked, \textit{``I get a lot of these surveys, and this is one of the best ones I've taken. Really liked the analysis you did, and concrete follow-up questions. Good stuff!''} 
Such feedback reinforces the importance of translating these findings into actionable recommendations, ensuring that research-driven insights can bridge the gap between academia and real-world CI practices in ML workflows.

\subsection{Rethinking CI practices for ML projects}
\label{sec:disc_rethinking_ci_practices}

Our findings from $RQ1$ reveal key differences in CI adoption between ML and non-ML projects, primarily driven by inherent non-determinism, resource and infrastructure requirements, data dependency, and the need to accommodate model performance tracking in the CI pipeline. These challenges align with previous research. For instance, Nascimento et al. \citep{nascimento2020software} highlight the difficulty of defining correctness criteria for ML outputs due to their non-deterministic behavior. Breck et al. \citep{breck2017ml} emphasize that ML system behavior is strongly tied to data and models, which cannot always be predetermined. Additionally, Giray \citep{giray2021software} highlights the lack of mature CI testing techniques and tools for ML systems.
% , further complicating reproducibility and traceability.

Insights from our survey participants reinforce these challenges, underscoring the need to rethink CI practices and develop strategies specifically tailored to ML workflows.

\subsection*{\textbf{\textit{Implication 1: ML practitioners should incorporate CI practices tailored to the ML domain (e.g., track and log model performance metrics on each commit).}}}

% As highlighted in $RQ1$, \textit{``CI best practices are less defined for ML projects compared to non-ML''} (P20). 
% Indeed, software projects aiming to adopt CI effectively should follow a set of recommended practices~\citep{santos2024needMonitorCI}. 


Although traditional software engineering has well-established CI guidelines~\citep{duvall2007continuous, fowler-ci-2006}, the ML domain still lacks standardized best practices. Recent studies have begun addressing this gap by proposing CI/CD strategies tailored for ML workflows.
Sato et al. \citep{Fowler2019CD4ML} introduce Continuous Delivery for Machine Learning (\textsc{CD4ML}), which focuses on practices to foster reproducible model training, validation data management, and model quality assurance. Similarly, Bangai et al. \citep{bagai2024implementing} explore CI/CD strategies for ML in cloud environments, highlighting key practices such as automated testing, validation, version control, and containerization.
However, despite these initial efforts, many ML practitioners still perceive a lack of well-defined CI practices in the ML domain, highlighting the need for further research and standardization. As P20 in $RQ1$ noted, \textit{``CI best practices are less defined for ML projects compared to non-ML''}. This gap underscores the importance of refining and formalizing CI principles to better support ML development workflows.

To address this gap, we draw on insights from our survey participants to define and discuss five CI practices specifically tailored to the ML domain.
Table~\ref{tab:ci_ml_practices} presents the CI practices proposed by Duvall et al. \citep{duvall2007continuous} and Fowler \citep{fowler-ci-2006}, alongside our proposed ML-specific CI practices. 
While standard CI principles such as frequent commits and quick fixes of broken builds remain applicable, ML projects introduce unique challenges that require specialized adaptations.
In the following sections, we describe each ML-specific CI practice we propose, providing explanations and supporting participant insights.

\begin{table}[]
\centering
\caption{Comparison of CI practices from Duvall, Fowler, and CI-specific practices for ML projects.}
\label{tab:ci_ml_practices}
\renewcommand{\arraystretch}{1.2} % Increases row height for better readability
\begin{tabular}{>{\arraybackslash}p{2.0cm}>{\arraybackslash}p{2.5cm}>{\arraybackslash}p{2.8cm}>{\arraybackslash}p{2.8cm}}
\toprule
\textbf{Practice Category} & 
\textbf{Duvall et al. practices (\cite{duvall2007continuous})} & 
\textbf{Fowler CI Practices (\cite{fowler-ci-2006})} & 
\textbf{CI Practices for ML} \bigstrut\\
\midrule
\multirow[t]{2}{*}{\parbox[t]{2cm}{\textbf{Code Integration and Repository Management}}} & 
Commit code frequently & Everyone commits to the mainline every day & 
- \bigstrut\\
% \cline{2-4}
\cmidrule{2-4}

& Don’t commit broken code & Maintain a single source repository & - \bigstrut\\
\midrule
\multirow[t]{8}{*}{\parbox[t]{2cm}{\textbf{Build Management}}} & 
Run private builds & Every commit should build the mainline on an integration machine & - \bigstrut\\
% \cline{2-4}
\cmidrule{2-4}

& Fix broken builds immediately & Fix broken builds immediately & - \bigstrut\\
% \cline{2-4}
\cmidrule{2-4}

& - & Automate the build & - \bigstrut\\
% \cline{2-4}
\cmidrule{2-4}

& - & Keep the build fast & 
\textbf{Use caching and warm-start training to speed up CI workflows} \bigstrut\\ % for large models
% \hline
\midrule
\multirow[t]{10}[8]{*}{\parbox[t]{2cm}{\textbf{Testing and Validation}}} & 
Write automated developer tests & Make your build self-testing & 
\textbf{Track and log model performance metrics on each commit} \bigstrut\\
% \cline{2-4}
\cmidrule{2-4}
& All tests and inspections must pass & Test in a clone of the production environment & 
\textbf{Use testing granularization and prioritization strategies (e.g., run slow tests selectively after lightweight tests pass)} \bigstrut\\
% \cline{2-4}
\cmidrule{2-4}

& - & - & 
\textbf{Handle non-deterministic test behavior} \bigstrut\\
% \hline
\midrule
\multirow[t]{4}[4]{*}
{\parbox[t]{2cm}{\textbf{Delivery and Deployment}}} & 
- & Make it easy for anyone to get the latest executable & - \bigstrut\\
% \cline{2-4}
\cmidrule{2-4}

& - & Automate deployment & - \bigstrut\\
% \hline
\midrule

\parbox[t][1cm][t]{2cm}{\textbf{Build Integrity and Risk Prevention}} & 
Avoid getting broken code & Everyone can see what's happening & - \bigstrut\\
% \hline
\midrule

\parbox[t][0.7cm][t]{2cm}{\textbf{Data and Model Management}} & 
- & - & \parbox[t][][t]{2.8cm}{\textbf{Use model versioning and dataset versioning}} \bigstrut\\
\bottomrule
\end{tabular}
\end{table}

% \begin{table*}[h!]
% \centering
% \renewcommand{\arraystretch}{1.2} % Increases row height for better readability
% \begin{tabular}{>{\centering\arraybackslash}p{2.0cm}|>{\centering\arraybackslash}p{2.5cm}|>{\centering\arraybackslash}p{2.8cm}|>{\centering\arraybackslash}p{2.8cm}}
% \hline
% \textbf{Practice Category} & 
% \textbf{Duvall et al. practices (\cite{duvall2007continuous})} & 
% \textbf{Fowler CI Practices (\cite{fowler-ci-2006})} & 
% \textbf{CI Practices for ML} \bigstrut\\
% \hline
% \multirow{2}[4]{*}{\parbox{2cm}{\centering\textbf{Code Integration and Repository Management}}} & 
% Commit code frequently & Everyone commits to the mainline every day & 
% - \bigstrut\\
% \cline{2-4}      
% & Don’t commit broken code & Maintain a single source repository & - \bigstrut\\
% \hline
% \multirow{8}[8]{*}{\parbox{2cm}{\centering\textbf{Build Management}}} & 
% Run private builds & Every commit should build the mainline on an integration machine & - \bigstrut\\
% \cline{2-4}      
% & Fix broken builds immediately & Fix broken builds immediately & - \bigstrut\\
% \cline{2-4}      
% & - & Automate the build & - \bigstrut\\
% \cline{2-4}      
% & - & Keep the build fast & 
% \textbf{Use caching and warm-start training to speed up CI workflows} \bigstrut\\ % for large models
% \hline
% \multirow{10}[8]{*}{\parbox{2cm}{\centering\textbf{Testing and Validation}}} & 
% Write automated developer tests & Make your build self-testing & 
% \textbf{Track and log model performance metrics on each commit} \bigstrut\\
% \cline{2-4}      
% & All tests and inspections must pass & Test in a clone of the production environment & 
% \textbf{Use testing granularization and prioritization strategies (e.g., run slow tests selectively after lightweight tests pass)} \bigstrut\\
% \cline{2-4}      
% & - & - & 
% \textbf{Handle non-deterministic test behavior} \bigstrut\\
% \hline
% \multirow{4}[4]{*}
% {\parbox{2cm}{\centering\textbf{Delivery and Deployment}}} & 
% - & Make it easy for anyone to get the latest executable & - \bigstrut\\
% \cline{2-4}      
% & - & Automate deployment & - \bigstrut\\
% \hline
% \parbox{2cm}{\centering
% \textbf{Build Integrity and Risk Prevention}} & 
% Avoid getting broken code & Everyone can see what's happening & - \bigstrut\\
% \hline
% \multirow{2}[4]{*}{
% \parbox{2cm}{\centering\textbf{Data and Model Management}}} & 
% - & - & \textbf{Use model versioning and dataset versioning} \bigstrut\\
% \hline
% \end{tabular}
% \caption{Comparison of CI practices from Duvall, Fowler, and CI-specific practices for ML projects.}
% \label{tab:ci_ml_practices}
% \end{table*}

\subsubsection*{ML-centric CI practices definition}

\subsubsection*{\textbf{ML-specific CI practice 1:} Track and log model performance metrics on each commit (e.g., accuracy, recall, F1-score, RMSE)}

ML models evolve continuously, and even minor changes in code, data, or hyperparameters can impact model performance. To ensure that updates do not degrade model quality, it is essential to track and log key performance metrics—such as accuracy, recall, F1-score, and RMSE—at each commit. 
Maintaining detailed performance logs enables teams to detect regressions early, analyze performance trends over time, and ensure that changes align with expected quality standards.

One of the challenges identified by participants in $RQ1$ is the need for systematic model evaluation within CI pipelines. 
This aligns with the broader research challenge highlighted by Garg et al. \citep{garg2021continuous}, who emphasize the difficulty of monitoring the effectiveness of AI models over time.
As P25 explained, \textit{``there is a need to [...] track model performance metrics on each commit''}. 
% Similarly, P118 noted, \textit{``Testing in ML projects involves not only verifying code but also validating model performance on various datasets, which adds to the overall build duration''}. 
Similarly, P112 explained, \textit{``Testing ML models typically involves evaluating performance metrics (e.g., accuracy, precision, recall) rather than simply checking for correct outputs''}. By integrating automated performance tracking into CI workflows, teams can establish a proactive monitoring system that ensures models meet expected quality benchmarks throughout their lifecycle.
% before deployment.

\subsubsection*{\textbf{ML-Specific CI Practice 2:} Use testing granularization and prioritization strategies (e.g., run slow tests selectively after lightweight tests pass)}

ML projects often require large datasets, making testing resource-intensive and costly. To optimize CI  efficiency, test prioritization strategies should be implemented. For instance, tests should be classified based on their execution time and resource requirements---fast tests (e.g., unit tests) should run first, while slow tests (e.g., integration tests and model performance tracking tests) should be executed only if the lightweight tests pass.

% In the results of $RQ2$, the response of P57 highlighted that \textit{``ML projects often have multiple steps that need to happen over (even in test settings) non-trivial data sizes to validate correctness. E.g., a full integration test for a project I'm involved needs to extract some synthetic raw data from disk, re-format it, pre-process it, then ``train'' a model on a synthetic task over that data to convergence, then validate that model. Each of these steps use real tools designed for production use (so that the test covers appropriate, real-world usage) and the end to end pipe takes a long time''}. 
In the results of $RQ3$, P40 suggested, \textit{``One could break the testing workflow into smaller flows [...]. Smaller unit tests are great for capturing obvious errors and they eliminate requirement to serve the whole model to capture these at some level''}.
By running complex tests only when necessary, CI workflows can minimize resource consumption while maintaining comprehensive validation of ML projects.
Such strategy not only alleviates bottlenecks in CI pipeline of ML projects, but also align with \cite{giray2021software} recommendations of scalable testing strategies tailored to the computational demands of ML workflows.

\subsubsection*{\textbf{ML-Specific CI Practice 3:} Use model versioning and dataset versioning (e.g., DVC)}

ML models rely heavily on data, and without proper dataset versioning, reproducing past results and maintaining consistency across experiments becomes challenging. Unlike traditional software projects, where code is the primary versioned artifact, ML projects require tracking datasets and trained models alongside code changes to ensure reproducibility and traceability.

% As P115 highlighted,
% \textit{``In ML projects, we have some additional challenges like managing datasets, ensuring data versioning, handling long training times, integrating model validation and testing, etc.''}.
By adopting dataset versioning tools such as Data Version Control (DVC), ML projects can link models to the exact datasets and code versions used during training. This improves traceability, enables experiment reproducibility, and provides rollback capabilities, allowing teams to revert to previous dataset or model versions when needed.

\subsubsection*{Examples of Dataset Versioning in CI Workflows}

To ensure efficient dataset management in CI pipelines, P112 suggested, \textit{``Implement data versioning tools like Data Version Control (DVC) to manage datasets and track changes efficiently, ensuring that only the necessary data transformations are performed during each build''}. 
This approach prevents unnecessary data reprocessing, reducing CI pipeline execution times.
Moreover, caching strategies can further enhance efficiency. P97 recommended, ``\textit{use dedicated runner machine with a proper caching and tools like DVC to minimize duplication of work between consecutive builds. Also, what could help with the latter is the CI tools like ConcourseCI or pipeline orchestrators like Dagster which have declarative pipeline execution definition based on the resource state instead of imperative plan (DVC implements similar functionality for the inner execution scope)''}.
This ensures that datasets do not need to be reloaded and reprocessed in every CI run.

\subsubsection*{Examples of Model Versioning in CI Workflows}

In addition to dataset versioning, model versioning presents another challenge that must be addressed in ML CI workflows. Large models may be infeasible to retrain fully within CI pipelines, requiring efficient model tracking and lineage management. As P57 explained,
\textit{``Our models are often too big to run end-to-end in a CI pipeline. Data and model versioning and lineage tracking are also challenging''}. 
One practical strategy, as suggested by P66, is to use a model registry:
\textit{``Try implementing model registry to maintain versions of models''}.

A model registry (e.g., \textsc{MLflow Model Registry}, \textsc{TensorFlow Model Garden}, or \textsc{SageMaker Model Registry}) provides fuctionalities such as: (i) version control for trained models, enabling rollback to previous versions if issues arise; (ii) metadata tracking, associating models with the datasets and hyperparameters used during training; and (iii) deployment automation, ensuring that only validated models are promoted to production environments. 
However, challenges persist in model versioning, as observed in pre-trained language model (PTLM) repositories on \textsc{Hugging Face}, where version identifiers often lack clear structure, and changes between versions are not well-documented~\citep{ajibode2025semverPTLM}. These inconsistencies highlight the need for standardized versioning mechanisms to enhance reproducibility and facilitate CI automation in ML projects.

By integrating dataset and model versioning into CI workflows, ML teams can achieve greater reproducibility, reduce redundant computations, and ensure that models are trained, tested, and deployed with consistent data and configurations. Future research should focus on developing standardized best practices for dataset and model versioning in CI pipelines to further improve automation and efficiency.

\subsubsection*{\textbf{ML-Specific CI Practice 4:} Handle non-deterministic test behavior}

% As presented in the results of $RQ1$ and $RQ3$, testing ML projects presents unique challenges due to the inherent non-determinism in model behavior.
% . Several practitioners highlighted difficulties in verifying the correctness of ML outputs, as results can vary even when using the same inputs. 
% For instance, P84 noted, \textit{``Testing the results of a model is difficult as we can not explain the output even more if randomness is added''}.
% Similarly, P136 stated, \textit{``Usually there's no deterministic expected results to write tests [in ML projects]''}. P16 further emphasized this issue, explaining that  \textit{``it can be hard to be objective with non-deterministic or subject results (e.g., generative language models)''}.
% ML models often exhibit non-deterministic behavior due to factors such as random weight initialization, stochastic optimization, and variations in data sampling, leading to slight output differences across different test runs. 
% As P118 explained, \textit{``ML models often produce non-deterministic outputs due to randomness in algorithms (e.g., random initialization of weights in neural networks) or variations in training data. This makes it challenging to validate outputs consistently across different test runs''}. 
% P48 reinforced this, stating, \textit{``ML models often produce different results on different runs due to inherent randomness in training processes.Traditional unit tests focus on code correctness, but ML models require validation of performance metrics (e.g., accuracy, precision, recall) on test datasets, which is more complex and less binary than typical pass/fail unit tests''}.
As highlighted in the results of $RQ1$ and $RQ3$, testing ML projects presents unique challenges due to the inherent non-determinism of model behavior. ML models often produce slightly different outputs across test runs due to factors such as random weight initialization, stochastic optimization, and variations in data sampling. This randomness complicates validation, as traditional unit tests focus on code correctness, whereas ML validation depends on performance metrics like accuracy, precision, and recall. Unlike conventional software testing, where outcomes are typically binary (pass/fail), ML testing requires evaluating model performance over multiple runs, introducing additional complexity.

Recent research efforts have sought to mitigate the impact of non-determinism in ML testing. \cite{xia2023balancing} introduced FASER, a technique designed to enhance the fault-detection effectiveness of non-deterministic tests in ML projects. \textsc{FASER} systematically adjusts assertion bounds to balance the trade-off between test flakiness and fault-detection capability, thereby improving test reliability.
Additionally, \cite{rivera2021challenge} explored the sources of non-determinism in ML systems and developed \textsc{ReproduceML}, a framework aimed at promoting deterministic evaluation of ML experiments. This framework enables researchers to assess the effects of software configurations on ML training and inference, facilitating reproducibility.

To further mitigate non-determinism in ML testing, practitioners can implement several strategies, such as
fix random seeds for libraries like \textsc{NumPy}, \textsc{TensorFlow}, and \textsc{PyTorch} to ensure consistency in weight initialization and dataset shuffling, and standardizing hardware environments by running ML pipelines on identical hardware/software configurations, for instance, using containerized environments (e.g., \textsc{Docker}) and specifying exact dependency versions in \textit{requirements.txt} or \textit{conda.yml} files ensures that models are trained and evaluated in a controlled setting.
Strategies such as those could be adopted to reproducible results across different runs and environments.

% \subsubsection*{\textbf{ML-Specific CI Practice 4:} Detect data drift automatically (e.g., monitor dataset distribution changes)}

% ML models depend on the stability of input data distributions, yet data drift—where the statistical properties of input data change over time—can degrade model performance and lead to incorrect predictions. Without continuous monitoring, models trained on outdated data may no longer be reliable in production environments.

% As P100 emphasized, \textit{``Data changes over time, and models trained on outdated data may not work as expected in production''}.
% To mitigate the risks associated with data drift, CI workflows should incorporate automated drift detection mechanisms, which monitor dataset distribution changes over time and trigger alerts when significant drift occurs, prompting model retraining.

\subsubsection*{\textbf{ML-Specific CI Practice 5:} Use Caching and Warm-Start Training to Speed Up CI Workflows} % for Large Models

Training ML models from scratch is often infeasible in CI. Using cached computations and warm-start training (e.g., reusing previous model weights) strategies might be incoraged to reduce build times while maintaining model quality.

As P100 highlighted, \textit{``ML projects typically involve extensive data handling, including loading, cleaning, transforming, and preprocessing. These steps can be computationally intensive, especially when dealing with large datasets. This contributes to longer build durations. For example, mlpack leverages caching and parallel compilation to minimize build times. We also use automated build tools like CMake to streamline the build process''}.
In addition, P112 recommended, \textit{``Use caching mechanisms to avoid reprocessing the same datasets for every build. Store preprocessed data in a shared cache that can be reused''}. Similarly, P139 recommended, \textit{``Optimise the code across the whole ML pipeline to parallelise computations, caching when appropriate, etc''}.


Therefore, to address this challenge, CI workflows should incorporate caching mechanisms and warm-start training. By minimizing redundant computations, CI pipelines can significantly reduce build times while maintaining model performance and efficiency.

%%% DANIEL'S SUGGESTION OF DISCUSSION

% As P40 emphasized, “Workflows are much longer since models are large and resource extensive. One could either spend a ton of money to use larger GPUs or decrease testing coverages”

% perhaps one comment for discussion is that certain values from CI may need to be customized for ML projects.

% For instance, the tolerance for build duration may have to be Higher in CI projects for ML projects, whereas quality of tests must go beyond unit, system, component testing.

% A new reflection on what are good CI practices for ML projects would be necessary.

% %%%
% P100 explained, “ML projects often deal with large, complex datasets, requiring robust data management and handling strategies. This introduces challenges in terms of storing, accessing, and versioning datasets within the CI pipeline”

% This could be another issue discussed in the CI-ML-centric approach


% As explained by P25, “There are some specific challenges that arise when implementing a continuous integration pipeline in ML projects compared to regular projects. For example, there is a need to version and store datasets and machine learning models, as well as track model performance metrics on each commit”. According to P128,
% “Sometimes keeping track of metrics for example model accuracy becomes a challenge. Especially since our output changes quite a lot during experiments”.

% metrics and performance too, since they need to be tracked in each commit


% For example, P20 noted, “I think CI best practices are less defined for ML projects compared to non-ML”

% SEE?! We should define these in the discussion section based on our data


% Addressing these gaps requires a reimagined CI framework tailored specifically to ML workflows. Unlike traditional CI, which focuses predominantly on code, ML-specific CI must treat data, models, and code as interdependent entities, ensuring that changes in any one component are seamlessly incorporated into the pipeline.

\subsection{Handling Build Duration and Test Coverage in ML Projects}
\label{sec:handling_build_duration_and_test_cov}

\subsection*{\textit{\textbf{Implication 2: Practitioners should be aware that CI pipelines for ML projects often have longer build duration expectations.}}}

Our findings from $RQ2$ indicate that practitioners' expectations regarding build durations in ML projects significantly differ from those in traditional software projects. While reducing build time is generally a key goal in CI, ML practitioners, specially from large-sized projects, recognize that longer builds are often unavoidable due to inherent complexities such as data processing, model training, and resource-intensive computations.
% As P134 emphasized, \textit{``While a short build time is generally desirable, the need for more data processing as well as involving basic training and inference will likely increase the build time''}. 
This aligns with our key findings in $RQ2$, where 75\% of participants expected longer builds in ML projects compared to non-ML projects, driven by the higher computational demands and dependency-heavy environments.

Furthermore, project size variability also influences acceptable build durations. Our results show that small and medium-sized ML projects tend to favor build times of 10–20 minutes, while large-sized projects are more
tolerant of longer builds, with 20.3\% of practitioners considering build times exceeding 30 minutes acceptable.
hese findings suggest that expectations of CI efficiency in ML projects should be contextualized---what is considered an excessively long build in traditional software may be seen as reasonable in ML workflows.

While it is acknowledged that CI builds in ML projects are generally longer than in non-ML projects, excessively extended build times can hinder iterative testing and experimentation, ultimately stifling the exploratory nature of ML development.
Future work should explore adaptive CI mechanisms to mitigate build times in ML workflows without compromising validation rigor. In this study, we propose and discuss testing prioritization strategies as a recommended CI practice for ML projects. However, there remains significant room for further investigation into additional strategies that can enhance CI testing efficiency in the ML domain.

\subsection*{\textit{\textbf{Implication 3: Practitioners and researchers should establish clear guidelines for managing dependencies in the CI pipeline of ML projects.}}}

Our findings for $RQ2$ highlight that dependency management is a critical factor in controlling CI build duration in ML projects. Unlike traditional software projects, ML pipelines rely on large, complex dependency trees, often requiring hardware-specific libraries (e.g., CUDA, cuDNN, TensorRT) and deep learning frameworks (e.g., TensorFlow, PyTorch). These dependencies introduce long installation times, version conflicts, and resource-intensive compilation steps, significantly increasing build duration.

Participants emphasized that ML projects tend to have extensive dependencies, many of which are hardware-specific, making installation and configuration more complex. Deep learning frameworks, widely used across various domains such as natural language processing, computer vision, and reinforcement learning, require substantial setup efforts, further extending build times. Additionally, managing GPU/CPU-specific dependencies, such as CUDA toolkit versions, presents challenges in integrating machine-specific configurations into CI pipelines, adding another layer of complexity.

% As P94 noted, \textit{``ML projects tend to have a lot of dependencies, a large portion of them being hardware-specific''}. Similarly, P63 emphasized that deep learning frameworks require extensive installations across multiple domains (e.g., natural language processing, computer vision, and reinforcement learning), further extending build times. Managing GPU/CPU-specific dependencies (e.g., CUDA toolkit versions) adds another layer of complexity, as participants highlighted the challenges of integrating machine-specific configurations into CI pipelines.

Despite these challenges, current CI guidelines lack explicit recommendations for managing dependencies in ML workflows. 
Given that third-party libraries are now a fundamental component of software integration, CI practices should offer clearer guidance on dependency management, particularly for ML projects, which require a significantly larger number of dependencies ($RQ2$).

Based on the responses of the survey respondents in \textsc{Question \#3.3}, we define an initial set of strategies that should be considered when managing dependence in ML projects. These strategies are explained in the following.

\subsubsection*{Practitioners' Strategies to Optimize Dependency Management in ML CI Pipelines}

\textit{\textbf{Caching Dependencies.}}
Participants widely recommended caching dependencies to reduce redundant installations and improve CI efficiency. As P26 advised,  \textit{``Cache as many build dependencies as possible, reduce unnecessary dependencies, employ better software engineering practises''}. 
% P119 suggested, \textit{``Parallelize tests and cache dependencies''}. 
Similarly, P150 emphasized the importance of use pre-built dependencies, \textit{``Improve incremental builds use better build tools e.g. containers, nix, etc to track dependencies better. Focuse on build-once and use-many places philisophy for its dependencies''}. 
% This is aligned with P21, who also suggested \textit{``pre-built binaries of their [ML project] dependencies, caching build artifact better''}.

\textit{\textbf{Minimizing Unnecessary Dependencies.}}
Participants also stressed the importance of efficient dependency management, ensuring that only necessary libraries are included in each build.
As P118 recommended, \textit{``Efficiently manage dependencies and ensure that only necessary dependencies are included in each build, reducing unnecessary computations''}. 
P101 added, \textit{``As far as possible, they [ML projects] should use customized functionalities and reduce dependencies. While it's true that dependencies make coding easier, it's impossible to tell how they're affecting the build time''}. P100 further illustrated this by explaining how mlpack removed its dependency on the Boost library, significantly improving build times: \textit{``mlpack removed its dependency on the boost library relatively recently (which required refactoring most of the Neural Network codebase), and this improved our build times substantially''}.

\textit{\textbf{Containerization for Dependency Management.}}
Containerization was another frequently mentioned strategy for handling dependencies effectively. As P70 advised, \textit{``be smart about managing docker dependency changes''}. In addition, P58 suggested using lightweight containers, stating, \textit{``Simple solutions are to use tiny containers. Have something that filters out the requirements as many ML projects include excess dependencies that are only used rarely if at all''}.
In addition, P62 suggested the usage of \textit{``Small models, pre-created images with dependencies, etc''}.

To address these dependency management challenges,
future research should focus on defining standardized guidelines for dependency management in ML CI pipelines. For example, adopting containerized environments (e.g., \textsc{Docker}) with pre-installed dependencies could eliminate setup overhead. Instead of reinstalling dependencies from scratch in every run, CI workflows could pull optimized base images with the necessary libraries, potentially reducing build times and improving efficiency.



% In our prior work, we identified significantly longer build duration in ML projects when compared to non-ML projects. In addition, in our analysis of RQ2 we delve into the practitioners perspective on build duration of ML projects. Our results showed that 75\% of practititioners expect longer build durations in ML projects compared to non-ML although 69\% of them recognize short build duration as important

% Indeed,
% Long build durations in ML projects are more than an operational inconvenience. In fact, they represent a systemic barrier to agility and innovation. Extended builds discourage iterative testing and experimentation, stifling the exploratory nature of ML development. Participants identified key bottlenecks, such as GPU dependencies and extensive data processing, as significant contributors to this issue. These findings align with \cite{garg2021continuous} findings, which emphasize the importance of continuous model monitoring and adaptive pipelines to mitigate performance degradation from frequent data updates.

% Our participants stressed the need to optimize resource allocation and adopt incremental testing approaches to address build inefficiencies. Strategies such as intelligent caching of intermediate results, distributed computing, and targeted validations can significantly reduce build durations while maintaining reliability. For instance, validating only the components affected by recent changes, rather than reprocessing entire workflows, can save substantial time and computational resources. 
% Such strategies not only alleviate bottlenecks in ML pipeline build durations but also align with \cite{giray2021software} recommendations of scalable, resource-efficient testing frameworks tailored to the computational demands of ML workflows.

% Finally, cultural factors also influence the management of build durations. Participants noted that long build times are more frequently tolerated in larger-scale ML projects, often due to a lack of awareness about their cascading impacts on team productivity and project timelines. Shifting this perspective from viewing build durations as a technical inconvenience to recognizing them as a systemic inefficiency can foster a culture of optimization that prioritizes agility without compromising reliability.

\subsection*{\textbf{\textit{Implication 4: ML teams should foster interdisciplinary collaboration and invest in testing education for ML practitioners.}}}

ML testing requires expertise that spans software engineering, data science, and domain knowledge. However, many ML developers often lack background in software testing, while software engineers may not fully understand ML-specific challenges. 
Our findings from $RQ3$ highlight that \textit{multidisciplinary and organizational} factors significantly impact the testing process of ML projects.

Organizational culture plays a crucial role in shaping test coverage expectations.
Many ML teams prioritize rapid iteration and innovation over rigorous testing, often perceiving testing as a barrier rather than an essential practice. 
As P139 noted, \textit{``ML is a fast-moving field compared to other areas. The time pressure to build and deploy ML models is high, and test coverage is simply not a priority''}.
This mindset contributes to low test coverage and a lack of structured testing strategies. To improve testing in ML projects, P135 emphasized the importance of fostering a testing culture, stating: \textit{``Start measuring it. It’s often not part of data science team’s culture to even write tests''}. Similarly, P17 highlighted the need to consider testing in project planning: \textit{``Take testing into account when providing a deadline''}.

To address these challenges, organizations should embed testing as a core component of ML workflows rather than treating it as a secondary concern. 
This involves systematically tracking test coverage, training ML developers in software testing practices, and promoting interdisciplinary collaboration between AI researchers, software engineers, and domain experts. 
For instance, when asked about strategies to enhance the test coverage of ML projects, 
P147 suggested, ML teams should \textit{``measure the test coverage and work hard to improve the coverage''}. 
In addition, P34 recommended \textit{``Improving software engineering expertise of ML practitioners''}. 
As a practical approach, P34 stressed the importance of \textit{``Onboard new engineers to software development practices''}.

Future research should establish best practices for testing ML projects, providing structured guidelines to balance the need for innovation with robust software quality assurance. 
% As P146 elaborated, there is a need to stablish \textit{``Good practices and rules for test''} ML projects.

\subsection*{\textbf{\textit{Implication 5: CI pipelines should integrate structured test coverage monitoring and enforcement mechanisms.}}}

In addition to fostering a stronger testing culture, some ML practitioners emphasized the importance of \textit{automated test coverage enforcement within CI pipelines}. As P34 suggested, CI workflows should \textit{``make code coverage a required check''}, while P56 recommended \textit{``Add a CI step that fails if test coverage drops below a certain treshold''}. 
% Similarly, P119 highlighted the need to \textit{``Measure coverage and have it run on PRs to make sure coverage does not go down''}.

To make test coverage monitoring more actionable, some participants emphasized the need for \textit{better visualization tools}. For example, P39 recommended using platforms like \textsc{Codecov}, while P113 suggested incorporating \textit{``Web panels, Slack bots, anything that shows coverage in an easy way, so that engineers can immediately see a problem and be motivated to fix it''}. 

Building on the importance of real-time coverage monitoring, P141 noted that CI tools should notify teams about test coverage levels, while also allowing engineers to configure acceptable coverage thresholds: \textit{``CI tools can help to notify about test coverage, but still someone needs to correct set up the values for acceptable rate''}. 

However, our results for $RQ3$ indicate that there is no consensus regarding an acceptable test coverage rate for ML projects. While most practitioners (63\%) favored coverage rates of 70–100\%, a significant number (20\%) accepted moderate coverage (50–70\%), citing ML-specific constraints such as resource demands and the stochastic nature of ML algorithms. 
These findings suggest that CI pipelines should adopt flexible, context-aware test coverage policies that balance software quality with ML-specific constraints. Additionally, the test coverage thresholds identified in $RQ3$ can serve as an initial benchmark for defining CI workflow checks in ML projects, helping teams establish realistic and effective coverage goals.

% The disparity in test coverage between ML and non-ML projects highlights the practical challenges of achieving comprehensive testing in ML workflows. 

% Our findings reveal that resource constraints, non-determinism, and the experimental nature of ML systems frequently hinder achieving high coverage rates. These insights align with our prior work \citep{bernardo2024machine}, where medium-sized ML projects demonstrated lower test coverage (83\%) compared to non-ML projects (94\%).

% The need for probabilistic testing emerged as a recurring theme in our findings. Unlike deterministic systems, ML workflows operate under inherent variability, making traditional pass/fail criteria inadequate. 
% Initial efforts have been made to address the challenges of managing non-determinism when testing ML systems. For instance, \textsc{Ease.ml/ci} introduces testing methodologies that establish acceptable ranges of variability in outputs, enabling more meaningful validation while avoiding unrealistic expectations of determinism \citep{renggli2019continuous}.
% This approach aligns closely with participants’ suggestions for more flexible testing paradigms tailored to the unique characteristics of ML workflows. However, further research is necessary to advance this field, particularly in developing standardized probabilistic testing frameworks and metrics that can be widely adopted across diverse ML applications.

% Participants also emphasized the culture of experimentation by ML teams, which often deprioritizes rigorous testing practices over experimental trials. According to some participants, ML practitioners focus more on innovation and rapid iteration than on adhering to testing best practices. This observation aligns with \cite{sculley2015hidden}, who identify ``hidden technical debt'' in ML systems, where inadequate testing exacerbates long-term inefficiencies. Addressing this cultural gap requires embedding testing as an integral part of the experimental workflow, encouraging teams to view testing as a mechanism for ensuring robustness and reliability rather than as an impediment.

% Finally, participants noted that achieving high test coverage in ML projects demands substantial resource investments. This observation aligns with \cite{giray2021software}, which highlights the economic trade-offs involved in ML testing. Our findings emphasize the importance of developing techniques for designing optimized test case sets to minimize resource requirements while maintaining effective coverage and reliability.

% \subsection*{\textbf{Implications for Research}}

% \textbf{\textit{Rethinking CI for ML.}}
% Our findings reveal significant challenges in adapting traditional CI practices to the unique demands of ML workflows, particularly regarding data dependencies, non-deterministic behaviors, and model-specific complexities. Future research must prioritize developing or adapting CI frameworks to fully integrate these characteristics, treating data, models, and code as interconnected pillars. For example, participants frequently emphasized the need for pipelines capable of managing dynamic datasets alongside code, ensuring traceability and reproducibility. This underscores the necessity of automating data versioning and integrating continuous model retraining steps into CI workflows, triggered by data updates or performance thresholds. Additionally, our results highlight the resource-intensive nature of ML builds, suggesting that incremental testing approaches, i.e. validating only the components affected by changes, could significantly reduce computational costs and build durations without compromising quality.

% \textbf{\textit{Adapting Testing Frameworks to ML Variability.}}
% The stochastic nature of ML systems complicates traditional pass/fail testing paradigms, as highlighted by participants who frequently cited non-determinism as a major obstacle in achieving consistent test results. Research should focus on creating probabilistic testing frameworks that define acceptable ranges of variability for ML outputs, accommodating the expected randomness inherent to these systems. For instance, our findings demonstrate the challenges practitioners face in testing ML models with dynamic behaviors, reinforcing the need for more flexible testing methodologies. Dynamic validation metrics, tailored to assess model robustness and fairness under varying conditions, could bridge the gap between deterministic expectations and ML-specific realities.

% \textbf{\textit{Scalability in CI Systems.}}
% As ML projects increase in size and complexity, our study shows that their CI systems struggle to scale effectively, particularly in managing resource demands and build durations. Research should explore strategies for identifying and addressing bottlenecks in resource allocation, such as optimizing GPU utilization during builds and leveraging distributed computing for faster training and validation cycles. Participants also highlighted the interdependence of testing, data management, and infrastructure, suggesting a need for modular CI architectures. These architectures could allow teams to isolate and optimize specific components of the pipeline, ensuring scalability without increasing system fragility. Furthermore, our findings point to a disparity in resource availability across organizations, emphasizing the importance of cost-efficient CI solutions for resource-constrained environments, such as open-source projects or smaller teams.


%%%%%

% \subsection*{\textbf{Implications for Practice}}

% The feedback from participants underscores the importance of this research in addressing the unique challenges of CI in ML projects. For example, P25 stated, \textit{''Your research is quite interesting and I'm curious to see what conclusions you ultimately reach. For ML projects today, there is often a lack of optimization and testing, so insights in this area would be very valuable.''} Similarly, P63 expressed enthusiasm for actionable findings, remarking, \textit{``I really want to know about the findings!!! And would love to participate more in the future surveys''}. These responses highlight a genuine need for solutions that practitioners can adopt to overcome the systemic challenges of integrating CI into ML workflows.
% Participants also appreciated the depth of this research, as P144 noted, \textit{``I get a lot of these surveys, and this is one of the best ones I've taken. Really liked the analysis you did, and concrete follow-up questions. Good stuff!''} This sentiment further emphasizes the importance of translating these findings into actionable recommendations for practitioners, aiming to bridge the gap between research and real-world application.

% \textbf{\textit{Tailoring CI Solutions to ML Workflows.}}
% The findings reveal significant gaps in the applicability of traditional CI tools to ML projects, particularly in managing data dependencies, testing non-deterministic outputs, and addressing resource constraints. Practitioners should prioritize adopting or developing CI solutions explicitly tailored to ML workflows. These tools must incorporate robust data validation steps to ensure quality and consistency while seamlessly handling the dynamic nature of datasets. For example, incorporating automated data versioning into CI pipelines could mitigate the risk of silent failures due to unnoticed dataset changes. 

% \textbf{\textit{Balancing Comprehensive Testing with Constraints.}}
% Despite high practitioner expectations for test coverage, achieving adequate rates in ML projects is often limited by resource constraints and the inherent complexity of testing stochastic systems. Strategies such as prioritizing high-risk components, leveraging automated test generation tools, and adopting hybrid testing approaches can help practitioners balance the trade-offs between coverage and feasibility. As P25 pointed out, \textit{“there is often a lack of optimization and testing”} in ML projects, emphasizing the need for streamlined testing processes that can deliver meaningful results without imposing excessive resource demands.

% \textbf{\textit{Promoting a Culture of Testing in ML Teams.}}
% Participants frequently identified a cultural emphasis on experimentation over rigorous testing as a barrier to effective CI adoption in ML projects. This underscores the need for organizations to foster a testing-oriented culture within ML teams. Providing training programs to equip data scientists with software engineering best practices, such as test design and CI pipeline management, can address this gap. Additionally, interdisciplinary collaboration should be encouraged to ensure that expertise from software engineers, data scientists, and domain experts is integrated into the testing process. These cultural shifts could transform testing into an integral component of the ML development lifecycle.

% \textbf{\textit{Addressing Build Duration Challenges.}}
% The extended build durations highlighted in the findings not only reduce efficiency but also discourage iterative experimentation, a cornerstone of ML development. Organizations should adopt modular CI pipeline designs that isolate and optimize specific stages, such as data validation, model training, and integration testing. Techniques such as distributed computing, intelligent caching, and incremental builds can significantly reduce build times, enabling teams to iterate faster.

% \textbf{\textit{Advocating for ML-Specific Testing Tools.}}
% Participants frequently noted the limitations of existing testing tools in handling the stochastic nature of ML systems. To address this, practitioners should advocate for or adopt probabilistic testing frameworks that account for variability in outputs, enabling a more accurate assessment of ML systems’ reliability. Additionally, integrating these tools into CI workflows can standardize testing practices and improve consistency across ML projects.
