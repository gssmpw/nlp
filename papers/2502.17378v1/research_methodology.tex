% \label{demographics}

\section{Research Methodology}
\label{sec:methodology}

In this section, we describe how we selected the ML practitioners for our study, the data collection process, and the research approach we used to perform our analysis.

\subsection{Subject Projects}

Our prior work~\citep{bernardo2024machine} quantitatively analyzed the adoption of CI practices across 93 ML projects and 92 non-ML projects. This study revealed that ML projects often experience longer build durations and that medium-sized ML projects tend to have lower test coverage compared to non-ML projects. Building on these findings, our current study seeks to deepen our understanding of the specific factors driving these differences in CI practice adoption. To achieve this, we employed a qualitative, survey-based approach, focusing on the perceptions of ML practitioners (i.e., contributors and integrators of ML projects) regarding the challenges, barriers, and strategies associated with CI adoption in ML projects.

To maintain consistency with prior analyses and minimize potential biases associated with using an unverified or outdated collection of projects, we based our study on the 
93 ML projects investigated in our prior work~\citep{bernardo2024machine}. 
This dataset is both up-to-date and meticulously curated, representing a diverse collection of actively maintained ML projects that successfully integrate CI workflows into their pipelines. Additionally, the projects are categorized by size—small, medium, or large—based on their Lines of Code (LOC), ensuring a comprehensive representation across varying scales of ML development.

In addition to investigating general differences in CI adoption between ML and non-ML projects, this study focuses on the factors influencing build durations in ML projects, as outlined in RQ2. 
To capture diverse and meaningful insights, we adopted a sampling approach designed to reflect a wide range of experiences related to build durations. Specifically, we targeted projects with the shortest and those with the longest build durations to ensure that our analysis encompasses the full spectrum of challenges and characteristics encountered in ML workflows.

To sample practitioners for the survey, we selected those that are associated with the top 25\% of projects with the shortest build durations and the top 25\% with the longest build durations. 
As a result, we identified practitioners from 47 ML projects.
This targeted selection enhances the relevance of our findings by ensuring representation from both extremes of build duration characteristics, providing a comprehensive view of the characteristics and challenges faced in the CI workflows of ML projects. The list of the investigated ML projects, along with their characteristics (e.g., size and median build duration), is provided in Table~\ref{tab:project_characteristics}. 

% Table generated by Excel2LaTeX from sheet 'Sheet2'
\begin{table}[H]
  \centering
  \caption{Characteristics of Investigated Machine Learning Projects.}
    \begin{tabular}{cp{3.6cm}
    >{\raggedright\arraybackslash}p{2cm}
    >{\raggedleft\arraybackslash}p{2cm}
    >{\raggedright\arraybackslash}p{2cm}}
    \toprule
    \multirow[t]{3}{*}{\textbf{\#}} & \multirow[t]{3}{*}{\textbf{Project}} & \multirow[t]{3}{*}{\textbf{LOC Size}} & \textbf{Median build duration (minutes)} & \textbf{Build duration category} \bigstrut\\
    \midrule
    \textbf{1} & alan-turing-institute/sktime & large & 127.3 & longer \bigstrut[t]\\
    \textbf{2} & amark/gun & large & 1.3   & shorter \\
    \textbf{3} & apache/incubator-mxnet & large & 192.8 & longer \\
    \textbf{4} & apache/spark & large & 123.7 & longer \\
    \textbf{5} & apache/superset & large & 10.2  & shorter \\
    \textbf{6} & AUTOMATIC1111/stable-diffusion-webui & medium & 7.4   & shorter \\
    \textbf{7} & BehaviorTree/BehaviorTree.CPP & medium & 3.7   & shorter \\
    \textbf{8} & BLKSerene/Wordless & medium & 37.4  & longer \\
    \textbf{9} & chakki-works/doccano & medium & 2.8   & shorter \\
    \textbf{10} & criteo/tf-yarn & small & 2.4   & shorter \\
    \textbf{11} & DandyDev/slack-machine & small & 2.5   & shorter \\
    \textbf{12} & diffgram/diffgram & large & 5.1   & shorter \\
    \textbf{13} & dmlc/tvm & large & 90.2  & longer \\
    \textbf{14} & FluxML/Metalhead.jl & small & 28.4  & longer \\
    \textbf{15} & FluxML/NNlib.jl & medium & 37.4  & longer \\
    \textbf{16} & huggingface/pytorch-pretrained-BERT & large & 5.4   & shorter \\
    \textbf{17} & huggingface/transformers & large & 5.4   & shorter \\
    \textbf{18} & JohnSnowLabs/spark-nlp & large & 47.0    & longer \\
    \textbf{19} & jtablesaw/tablesaw & medium & 4.3   & shorter \\
    \textbf{20} & kendryte/nncase & large & 147.4 & longer \\
    \textbf{21} & LaurentMazare/tch-rs & large & 7.1   & shorter \\
    \textbf{22} & microsoft/dowhy & medium & 29.8  & longer \\
    \textbf{23} & microsoft/LightGBM & medium & 21.8  & longer \\
    \textbf{24} & microsoft/onnxruntime & large & 99.6  & longer \\
    \textbf{25} & microsoft/pai & medium & 3.9   & shorter \\
    \textbf{26} & mlpack/mlpack & large & 126.1 & longer \\
    \textbf{27} & mne-tools/mne-cpp & large & 62.4  & longer \\
    \textbf{28} & msdslab/automated-systematic-review & medium & 4.7   & shorter \\
    \textbf{29} & nilearn/nilearn & medium & 31.0    & longer \\
    \textbf{30} & opencv/dldt & large & 2.5   & shorter \\
    \textbf{31} & OpenKore/openkore & large & 5.0     & shorter \\
    \textbf{32} & OpenNMT/OpenNMT-py & medium & 3.4   & shorter \\
    \textbf{33} & pytorch/ignite & medium & 29.0    & longer \\
    \textbf{34} & pytorch/tnt & medium & 4.1   & shorter \\
    \textbf{35} & RubixML/RubixML & medium & 3.6   & shorter \\
    \textbf{36} & scikit-learn/scikit-learn & large & 0.3   & shorter \\
    \textbf{37} & SeldonIO/seldon-core & large & 4.6   & shorter \\
    \textbf{38} & shimat/opencvsharp & medium & 20.0    & longer \\
    \textbf{39} & skorch-dev/skorch & medium & 5.3   & shorter \\
    \textbf{40} & smistad/FAST & medium & 46.6  & longer \\
    \textbf{41} & sorgerlab/indra & medium & 19.3  & longer \\
    \textbf{42} & tensorflow/addons & medium & 19.4  & longer \\
    \textbf{43} & tensorly/tensorly & medium & 46.6  & longer \\
    \textbf{44} & tesseract-ocr/tesseract & large & 124.9 & longer \\
    \textbf{45} & Texera/texera & large & 6.8   & shorter \\
    \textbf{46} & TuringLang/Turing.jl & small & 107.1 & longer \\
    \textbf{47} & zhenghaoz/gorse & medium & 6.9   & shorter \bigstrut[b]\\
    \bottomrule
    \end{tabular}%
  \label{tab:project_characteristics}%
\end{table}%

\subsection{Data Collection}

To identify practitioners within the 47 investigated ML projects, we focused on individuals who actively contributed to the projects after the adoption of \textsc{GitHub Actions} CI workflows. Specifically, we selected integrators who either merged or closed at least one pull request (PR) or submitted at least one PR that was successfully merged into the \textit{main/master} branch of the project codebase during this period. This approach ensures that the selected practitioners directly contributed to the project while CI workflows were in use, enabling them to provide relevant and informed insights into CI practices in ML projects.

We collected PR metadata for the studied projects using the \textsc{GitHub API} on June 14, 2024. 
The PR's metadata include details of the PR number, state, author login, base branch, number of additions, deletions, changed files, commit count, and whether the PR was merged or closed, along with the login of the user who closed it.
By analyzing data from the period following the adoption of \textsc{GitHub Actions}, we identified 114,598 PRs reviewed by 3,276 unique integrators. Additionally, 6,861 contributors had at least one PR successfully merged. Importantly, these represent two distinct groups of practitioners in our study: contributors, who authored PRs, and integrators, who merged or rejected them. Among the contributors, 1,909 also acted as integrators, reflecting some overlap between the two roles. After accounting for this overlap, we identified a total of 4,952 unique contributors. Combined with the 3,276 unique integrators, this results in 8,228 practitioners involved in the studied projects.

To contact the practitioners involved in these projects, we collected their email addresses using the \textsc{GitHub API}, ensuring we only collected publicly available information. Of the 3,276 unique integrators, we retrieved 2,060 email addresses; for the 4,952 contributors, we retrieved 2,947. In total, we collected 5,007 unique email addresses from practitioners in the 47 analyzed projects.

To collect our data, we designed a web-based survey and sent invitations by email to all 5,007 ML practitioners whose email addresses were available.
The invitation letter is included in Appendix~\ref{sec:appendix_invitation_email_example}. To encourage participation, we offered respondents the opportunity to win one of ten \$50 Amazon or Steam gift cards, distributed through a random drawing. Participants were eligible for the draw only if they completed all survey questions and explicitly indicated their willingness to participate.
We used \textsc{Mailgun}\footnote{\url{https://www.mailgun.com}} to send personalized email invitations to each practitioner.
If a practitioner was associated with multiple investigated projects, we sent only one invitation, prioritizing the project where they had the highest number of integrated PRs.

In total, we received 155 responses, resulting in a response rate of 3.1\% (\nicefrac{155}{5007}). These responses came from practitioners associated with 30 of the 47 investigated projects. Table~\ref{tab:number_responses_per_respondent_type} presents details on the number of practitioners contacted per project, the responses received, and the corresponding response rates. To maintain anonymity, practitioners' names have been replaced with unique identifiers in the table. For example, practitioner 01 is labeled as ``P1'' and is associated with the \textit{alan-turing-institute/sktime} project.

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[H]
  \centering
  \caption{Number of responses and practitioners of the studied ML projects that were invited to participate.}
\begin{tabular}{cp{3cm}>{\raggedleft\arraybackslash}p{1.7cm}
>{\raggedleft\arraybackslash}p{1.5cm}
>{\raggedleft\arraybackslash}p{1.3cm}
>{\raggedright\arraybackslash}p{1.5cm}}
\toprule
\textbf{\#} & \textbf{Project} & \textbf{Number of practitioners} & \textbf{Number of responses} & {\textbf{Response rate}} & \textbf{Practitioner IDs} \\
\midrule
1 & alan-turing-institute/sktime & 123 & 8 & 6.5\% & P1--P8 \\
2 & amark/gun & 34 & 2 & 5.9\% & P9--P10 \\
3 & apache/spark & 189 & 1 & 0.5\% & P11--P11 \\
4 & apache/superset & 435 & 12 & 2.8\% & P12--P23 \\
5 & AUTOMATIC1111/stable-diffusion-webui & 293 & 8 & 2.7\% & P24--P31 \\
6 & BehaviorTree/BehaviorTree.CPP & 56 & 3 & 5.4\% & P32--P34 \\
7 & chakki-works/doccano & 37 & 1 & 2.7\% & P35--P35 \\
8 & diffgram/diffgram & 8 & 2 & 25.0\% & P36--P37 \\
9 & dmlc/tvm & 399 & 7 & 1.8\% & P38--P44 \\
10 & FluxML/NNlib.jl & 24 & 1 & 4.2\% & P45--P45 \\
11 & huggingface/pytorch-pretrained-BERT & 700 & 18 & 2.6\% & P46--P63 \\
12 & huggingface/transformers & 689 & 19 & 2.8\% & P64--P82 \\
13 & kendryte/nncase & 7 & 1 & 14.3\% & P83--P83 \\
14 & LaurentMazare/tch-rs & 39 & 2 & 5.1\% & P84--P85 \\
15 & microsoft/LightGBM & 89 & 2 & 2.2\% & P86--P87 \\
16 & microsoft/onnxruntime & 196 & 10 & 5.1\% & P88--P97 \\
17 & microsoft/pai & 12 & 1 & 8.3\% & P98--P98 \\
18 & mlpack/mlpack & 38 & 4 & 10.5\% & P99--P102 \\
19 & msdslab/automated-systematic-review & 23 & 1 & 4.3\% & P103--P103 \\
20 & nilearn/nilearn & 58 & 6 & 10.3\% & P104--P109 \\
21 & opencv/dldt & 394 & 4 & 1.0\% & P110--P113 \\
22 & pytorch/tnt & 10 & 1 & 10.0\% & P114--P114 \\
23 & scikit-learn/scikit-learn & 627 & 25 & 4.0\% & P115--P139 \\
24 & SeldonIO/seldon-core & 69 & 2 & 2.9\% & P140--P141 \\
25 & shimat/opencvsharp & 19 & 1 & 5.3\% & P142--P142 \\
26 & sorgerlab/indra & 9 & 1 & 11.1\% & P143--P143 \\
27 & tensorflow/addons & 50 & 3 & 6.0\% & P144--P146 \\
28 & tesseract-ocr/tesseract & 46 & 6 & 13.0\% & P147--P152 \\
29 & TuringLang/Turing.jl & 26 & 2 & 7.7\% & P153--P154 \\
30 & zhenghaoz/gorse & 24 & 1 & 4.2\% & P155--P155 \\
31 & apache/incubator-mxnet & 40 & 0 & 0.0\% & --- \\
32 & BLKSerene/Wordless & 3 & 0 & 0.0\% & --- \\
33 & criteo/tf-yarn & 4 & 0 & 0.0\% & --- \\
34 & DandyDev/slack-machine & 4 & 0 & 0.0\% & --- \\
35 & FluxML/Metalhead.jl & 12 & 0 & 0.0\% & --- \\
36 & JohnSnowLabs/spark-nlp & 30 & 0 & 0.0\% & --- \\
37 & jtablesaw/tablesaw & 18 & 0 & 0.0\% & --- \\
38 & microsoft/dowhy & 20 & 0 & 0.0\% & --- \\
39 & mne-tools/mne-cpp & 5 & 0 & 0.0\% & --- \\
40 & OpenKore/openkore & 11 & 0 & 0.0\% & --- \\
41 & OpenNMT/OpenNMT-py & 18 & 0 & 0.0\% & --- \\
42 & pytorch/ignite & 55 & 0 & 0.0\% & --- \\
43 & RubixML/RubixML & 11 & 0 & 0.0\% & --- \\
44 & skorch-dev/skorch & 9 & 0 & 0.0\% & --- \\
45 & smistad/FAST & 4 & 0 & 0.0\% & --- \\
46 & tensorly/tensorly & 16 & 0 & 0.0\% & --- \\
47 & Texera/texera & 24 & 0 & 0.0\% & --- \\
\midrule
& \textbf{Total} & \textbf{5,007} & \textbf{155} & \textbf{3.1\%} & \\
\bottomrule
\end{tabular}  \label{tab:number_responses_per_respondent_type}%
\end{table}%

Our survey is organized into five major sections, as described in Table \ref{tab:survey_structure}. It includes 20 questions, combining 8 closed- and 12 open-ended questions, designed to collect both quantitative and qualitative data. The estimated completion time is approximately 10 minutes.
To ensure relevance and foster more thoughtful responses, we designed 47 unique questionnaires, each tailored to specific characteristics and statistics of an associated project.
For example, in \textsc{Question \#4.3} of the form sent to the practitioners of the project \textit{tesseract-ocr/tesseract}, we asked: \textit{"When analyzing the data of the \textit{tesseract-ocr/tesseract} project, we observed that this project has a median build duration of 124.9 minutes, which is longer than 90\% of the investigated projects of similar size. Do you have any insights into why this project has a longer build duration?"}.
This customization allowed us to provide participants with context-specific data, enabling them to offer richer and more meaningful insights about their respective projects.
A complete example of the survey is available in our online Appendix\footnote{\url{https://zenodo.org/records/14902811}}, which includes the customized questionnaire sent to participants of the \textit{tesseract-ocr/tesseract}\footnote{\url{http://github.com/tesseract-ocr/tesseract}} project.


\begin{table}
\centering
\caption{Survey Structure and Description.}
\label{tab:survey_structure}
\begin{tabular}{p{3cm}p{8cm}}
\toprule
\textbf{Section} & \textbf{Description} \\
\midrule
Participant Information & Collects demographic data and information about participants' experience, including their experience contributing to ML projects, as well as familiarity with CI practices. \\
\midrule
Perceptions about CI Practices & Gathers insights into the challenges and differences when adopting CI practices in ML projects compared to non-ML projects. Focuses on team practices such as maintaining short build durations, frequent commits, and upholding high test coverage. \\
\midrule
Reflection on Previous Findings & Explores participants' views on results from our prior study, focusing on disparities in CI adoption between ML and non-ML projects. The questions explore the underlying factors contributing to differences in build durations, test coverage, and potential strategies for enhancement. \\
\midrule
Project-Specific Analysis & Presents data derived from a specific project (e.g., \textit{tesseract-ocr/tesseract}) to solicit feedback on unique challenges and techniques for enhancing CI practices in a real-world scenario. \\
\midrule
Conclusion and Follow-Up & Allows participants to opt into follow-up interviews, request updates on study findings, and share additional comments. Ensures eligibility for the gift card drawing by confirming survey completion. \\
\bottomrule
\end{tabular}
\end{table}

To encourage participation and a higher response rate, none of the questions in our survey were mandatory. As a result, the number of responses for each question varied, as not all participants answered every question. Responses were marked as ``NA'' (No Answer) if a participant left a question blank.
Table~\ref{tab:response_rates} provides a detailed overview of each survey question, including its description, type (open-ended or close-ended), and corresponding response rates. Close-ended questions generally achieved higher response rates, with several receiving complete responses (e.g., Questions 1.1–1.4 at 100\%). Conversely, open-ended questions exhibited slightly lower response rates, with the lowest being 85.2\% (Question 3.7). This trend indicates that open-ended questions, which typically require more effort and time to answer, may discourage some participants from responding. Nonetheless, the consistently high response rates across all question types highlight strong engagement from the participants.

\begin{table}
\centering
\caption{Survey Questions and Response Rates.}
\label{tab:response_rates}
\begin{tabular}{cp{6cm}lr}
\toprule
\textbf{\#} & \textbf{Question Description} & \textbf{Question Type} & \textbf{Responses (Rate)} \\
\midrule
1.1 & Experience developing software & Close-ended & 155/155 (100\%) \\
1.2 & Experience developing ML projects & Close-ended & 155/155 (100\%) \\
1.3 & Primary roles in ML projects & Close-ended & 155/155 (100\%) \\
1.4 & Familiarity with CI concepts & Close-ended & 155/155 (100\%) \\
2.1 & ML projects strive to incorporate CI practices & Open-ended & 146/155 (94.2\%) \\
2.2 & Challenges or differences when implementing a CI pipeline & Open-ended & 142/155 (91.6\%) \\
2.3 & ML projects commit more frequently & Close-ended & 149/155 (96.1\%) \\
2.4 & ML projects have longer build durations & Close-ended & 151/155 (97.4\%) \\
2.5 & ML projects have lower test coverage & Close-ended & 150/155 (96.8\%) \\
2.6 & ML projects fix broken builds more quickly & Close-ended & 149/155 (96.1\%) \\
3.1 & Importance of ML projects keeping a short build duration & Close-ended & 151/155 (97.4\%) \\
3.2 & Perceptions about previous study results on build duration in ML projects & Open-ended & 141/155 (91\%) \\
3.3 & Strategies to reduce build duration in ML projects & Open-ended & 138/155 (89\%) \\
3.4 & Acceptable test coverage rate for an ML project & Close-ended & 153/155 (98.7\%) \\
3.5 & Perceptions about previous study results on test coverage in ML projects & Open-ended & 135/155 (87.1\%) \\
3.6 & Challenges in testing ML projects & Open-ended & 133/155 (85.8\%) \\
3.7 & Strategies to enhance test coverage in ML projects & Open-ended & 132/155 (85.2\%) \\
4.1 & Familiarity with the CI pipeline of the studied project & Close-ended & 154/155 (99.4\%) \\
4.2 & Acceptable build duration for ML projects & Close-ended & 150/155 (96.8\%) \\
4.3 & Perceptions about the build duration of the studied project & Open-ended & 135/155 (87.1\%) \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Analytical Approach}

We applied an inductive thematic analysis to identify, analyze, and report themes within the qualitative data collected from our questionnaire, following the approach outlined by \cite{braun2006using}. To ensure rigor and transparency in the process, we adhered to the guidelines proposed by \cite{nowell2017thematic}.

The initial step of our thematic analysis involved open-coding the qualitative data. This process refers to assigning codes to relevant segments of data collected from the responses to our open-ended survey questions. Each question was coded by at least two authors, enhancing the robustness of the analysis and mitigating potential bias.
The first author conducted open coding for all eight open-ended questions in the survey, and to ensure reliability in the coding process, the second and third authors coded responses for three questions, and the fourth author coded two questions. 
Afterwards, the fifth author reviewed the entire set of codes generated by the two coders of each question. This review process helped resolve disagreements or ambiguities, refine the coding, and add additional entries where necessary.

Once the coding process was completed, the first author performed axial coding, grouping codes into higher-level themes. These themes represented broader conceptual constructs, organizing multiple related codes under a common idea. For example, a single theme might encompass several related codes addressing a specific aspect of CI practices in ML projects.


% To gain deeper insights into the relationships among the themes identified in our thematic analysis, we conducted an exploratory co-occurrence analysis. This approach examines the frequency with which different themes appear together in participants' responses, uncovering patterns and connections that provide a more nuanced understanding of the data.

Finally, we report the codes and themes derived from our thematic analysis in the results section.
When presenting our findings, we indicate the number of quotes associated with each code and theme using superscripts. 
However, it is important to note that these numbers do not necessarily indicate the relevance or significance of a code. For instance, a code may be cited in more quotes simply because it is more easily remembered by participants, rather than due to its importance.
To provide further context and depth, we include representative quotes from participants. To maintain anonymity, participant names are replaced with unique IDs.

While textual representations highlight key insights, we also employ network mapping charts to provide a structured visual representation of the relationships between themes and codes.
At the center of the network lies the core theme, encapsulating the primary focus of the RQ. Surrounding it are second-level themes, which further break down into third-level themes (codes), organized based on their conceptual relationships. Figure~\ref{fig:network_mapping_chart_example} presents an example of a Network Mapping Chart, illustrating these relationships in the thematic analysis.
Each third-level theme (code) offers granular insights into specific aspects of the data. The thickness of the edges in the network represents frequency, indicating how prominently each code appeared during the analysis.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{network_graph_example.png}
	% figure caption is below the figure
	\caption{Example of a Network Mapping Chart visualizing the relationships between themes and codes in thematic analysis.}
	\label{fig:network_mapping_chart_example}       % Give a unique label
\end{figure}


% Each second-level theme is further broken down into third-level themes (or codes), which provide more granular insights into specific aspects of the data. These codes are grouped based on their conceptual or contextual relationship with the higher-level themes. The thickness of the edges in the network reflects the frequency of occurrences, indicating how prominently each code appeared during the analysis.

% To enhance the interpretation of the codes, we include representative quotes from our participants. To maintain anonymity, participants’ names are replaced with unique IDs. 
% For example, when discussing perceived differences and challenges in adopting a CI pipeline in ML projects (RQ1), practitioner 57, who contributed to the \textit{huggingface/pytorch-pretrained-BERT} project (see Table \ref{tab:number_responses_per_respondent_type}), stated: \textit{``ML projects often are stochastic or make statistical guarantees in nature, and require different forms of testing/verification as a result''}. In this instance, the participant's name was replaced with P57. This specific response was coded under \textit{``Non-determinism''} and grouped into the broader theme of \textit{``Testing Complexity''}.