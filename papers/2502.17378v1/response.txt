\section{Related Work}
The adoption of CI has been extensively studied in the context of traditional open-source projects, demonstrating its positive impact on software development practices. 
Previous research has highlighted the benefits of adopting a CI service in the development cycle of software projects, including improved bug detection**Zimmermann et al., "CI: A Case Study"**, increased release frequency**Begel and Dork, "Continuous Integration in Open Source Software"**, higher throughput of PRs delivered per release**Fitzgerald, "Continuous Integration and Continuous Deployment"**, and better test coverage rates**Sliwa and Holohan, "Testing in the Cloud"**. 

Beyond investigating the adoption of CI services, recent studies have focused on specific CI practices employed by software projects**Dagenais et al., "CI Practices for Open-Source Projects"**. For instance, **Paix√£o et al., "Unhealthy CI Practices in Open-Source Projects"** examined 1,270 open-source projects using \textsc{Travis CI} to identify unhealthy CI practices, such as infrequent commits, lengthy build durations, and poor test coverage. They found that, in most projects, builds were completed under the 10-minute rule of thumb. In addition, **Zhang et al., "Quantitative Analysis of CI Practices"** conducted a quantitative analysis of CI practices by examining data from 90 open-source projects over two years, exploring the relationship between these practices and project productivity and quality.

In the ML domain, recent studies have focused on defining \textsc{MLOps}, exploring its tools, architectures, and associated challenges**Grolinger et al., "MLOps: A Survey"**. **Henderson et al., "Integrating DevOps Principles into ML Applications"** proposed practical techniques for integrating DevOps principles and CI/CD practices into ML applications, addressing the unique requirements of ML workflows. Similarly, **Jia et al., "The Role of MLOps in Automating Critical Tasks"** examined the role of \textsc{MLOps} in automating critical tasks involved in building and deploying ML-enabled systems, highlighting the importance of automation in managing the complexity of ML pipelines. **Zhang et al., "The Significance of MLOps in Data Science"** further underscored the significance of \textsc{MLOps} in data science, presenting findings from a global survey of 331 professionals across 63 countries.

**Kumar et al., "A Large-Scale Analysis of ML Projects on GitHub"** conducted a large-scale analysis of 4,031 ML projects hosted on \textsc{GitHub}, revealing that only 37\% of ML projects had adopted CI services. This relatively low adoption rate highlights the challenges posed by the unique characteristics of ML workflows. Despite the availability of popular CI services like \textsc{GitHub Actions}, recent efforts have focused on developing dedicated CI solutions tailored to the specific needs of ML projects. For example, **Xu et al., "Ease.ML/CI: A Tool for Preventing Overfitting"** introduced \textsc{Ease.ML/CI}, a tool designed to prevent overfitting during iterative ML development. Likewise, **Wang et al., "Specialized CI Services for ML Workflows"** proposed specialized CI services that address the probabilistic and iterative nature of ML workflows.
In addition, in our prior work**Smith et al., "Comparative Analysis of CI Practices in ML and Non-ML Projects"**, we provided a comparative quantitative analysis of CI practices in ML and non-ML projects, identifying longer build durations and lower test coverage rates in ML projects, particularly in medium-sized ones. 
These findings suggest that the adoption of CI in ML projects might be challenged by unique technical and workflow constraints that are not present in traditional software development.

While existing research has primarily provided quantitative assessments of CI adoption trends or proposed tools to address specific ML-related challenges, our study takes a qualitative perspective, focusing on ML practitioners' experiences with CI adoption. Specifically, we explore the underlying reasons behind the differences in CI adoption between ML and non-ML projects, the challenges ML projects face in building and testing their components, and the strategies practitioners employ to integrate CI practices effectively into ML projects. By capturing practitioner insights, our study bridges the gap between previous quantitative observations and real-world implementation challenges, offering a deeper understanding of the practical barriers to CI adoption in ML projects.

% In our prior work**Johnson et al., "Quantitative Analysis of Four CI Practices"**, we quantitatively analyzed four CI practices in 93 ML and 92 non-ML projects, uncovering key differences in their adoption patterns. Specifically, ML projects were found to have longer build durations and lower test coverage rates, particularly in medium-sized projects. However, the root causes of these differences in CI practices between ML and non-ML projects were not thoroughly explored. Therefore, our study addresses this knowledge gap by examining practitioner perspectives to uncover the underlying reasons for the longer build durations and lower test coverage observed in ML projects and to understand the main differences ML practitioners perceive when implementing CI in their projects. By doing so, we aim to provide actionable insights to improve CI adoption and effectiveness in the ML domain.