\section{\textbf{Results}}
\label{sec:results}

This section presents the findings for each research question addressed in the study.

\subsection*{\textbf{\RQone}}

% \textit{\textbf{Practitioners of ML projects most commonly perceive differences in test coverage and build duration when comparing ML to non-ML projects.}} 
In \textsc{Questions \#2.3 to \#2.6} of our survey, participants were asked how frequently they observe ML projects having more frequent commits, longer build durations, lower test coverage rates, and quicker fixes for broken builds compared to non-ML projects. As shown in Figure \ref{fig:q2_ci_practices_likert_scale}, the majority of participants often or always perceive ML projects as having longer build durations (63.2\%, \nicefrac{98}{155}) and lower test coverage (61.9\%, \nicefrac{96}{155}). 
% In contrast, responses were more neutral regarding the frequency of faster fixes for broken builds and more frequent commits, with most participants indicating that ML projects only sometimes exhibit these behaviors.
In contrast, responses regarding commit frequency and the time to fix builds were more neutral. Most participants reported that ML projects only sometimes, rarely, or never fix broken builds more quickly (77.4\%, \nicefrac{120}{155}) or commit more frequently (74.8\%, \nicefrac{116}{155}) than non-ML projects.
Thus, the practices of fixing broken builds quickly and committing code frequently show a smaller perceived difference when compared to non-ML projects. These findings strongly align with our previous work \citep{bernardo2024machine}, which found statistically significant differences in build duration and test coverage between ML and non-ML projects, but no significant differences in the time taken to fix broken builds or in the commit frequency.

\begin{figure}
	\centering
	\includegraphics[ width=12cm]{q2_ci_practices_likert_scale.pdf}
	% figure caption is below the figure
	\caption{Participants' perception about differences in CI practices on ML projects.}
	\label{fig:q2_ci_practices_likert_scale}       % Give a unique label
\end{figure}

% \textit{\textbf{We found differences and challenges when implementing a CI pipeline distributed in 8 different themes, including \textsc{Testing Complexity}, \textsc{Resource and Infrastructure Requirements} and \textsc{Build Duration and Stability} as the most frequently mentioned.}} 
Another analysis for this RQ focuses on identifying unique challenges and differences when implementing a CI pipeline in ML projects compared to non-ML projects. In \textsc{Question \#2.2}, participants were asked to describe the perceived challenges and differences they experienced when adopting CI in ML projects, which we subsequently analyzed through a thematic analysis.

% Figure \ref{fig:q2_2_differences_and_challenges} illustrates the 8 themes and 51 codes that emerged from our analysis. The central theme is \textit{Differences and Challenges of CI in ML}, which is the core objective of this RQ. The second-level (or axial) themes represent the high-level categories with the key challenges and differences perceived by participants regarding CI pipelines in ML projects. 
% The third-level themes (or codes) provide more specific insights and are grouped according to their relationship with the second-level themes. The thickness of the edges in Figure \ref{fig:q2_2_differences_and_challenges} reflects the frequency with which each code emerged during the thematic analysis. 
% In-depth details on each code from the analysis are provided in our online appendix.\joao{To appear soon!}

\begin{figure}
	\centering
	\includegraphics[ width=12cm]{differences_and_challenges_of_CI_in_ML.pdf}
	% figure caption is below the figure
	\caption{Perceived differences and challenges when adopting a CI pipeline in ML projects.}
	\label{fig:q2_2_differences_and_challenges}       % Give a unique label
\end{figure}

% \begin{table}[htbp]
%   \centering
%   \caption{Frequency of responses for each code and theme related to differences and challenges of implementing a CI pipeline in ML projects.}
%   \begin{tabular}{p{3.2cm} p{5cm} p{1cm} p{1cm}}  % Adjusted widths
%     \hline
%     \multirow{3}{*}{\textbf{Theme}} & \multirow{3}{*}{\textbf{Code}} & \textbf{Freq. per code} & \textbf{Freq. per theme} \bigstrut\\
%     \hline
%     \multirow{10}{*}{\raggedright Testing Complexity} & Difficult to test & 26    & \multirow{10}{*}{44} \bigstrut[t]\\
%           & Non-deterministic tests & 17    &  \\
%           & Hard reproducibility & 5     &  \\
%           & High test coverage challenges & 4     &  \\
%           & Data-centric tests & 3     &  \\
%           & Model validation & 2     &  \\
%           & Model-centric testing & 2     &  \\
%           & Test duration management & 2     &  \\
%           & Resource-dependent testing & 1     &  \\
%           & Sanity check on real traffic & 1     &  \bigstrut[b]\\
%     \hline
%     \multirow{4}{*}{\raggedright \parbox{3.2cm}{Resource and Infrastructure \\ Requirements}} & More computational resource required & 32    & \multirow{4}{*}{43} \bigstrut[t]\\
%           & Higher resource costs & 11    &  \\
%           & Needs to configure and debug low-level libraries or resources & 10    &  \\
%           & Scalability challenges & 1     &  \bigstrut[b]\\
%     \hline    
%     \multirow{3}{*}{\raggedright Build Duration and Stability} & Long build duration & 28    & \multirow{3}{*}{32} \bigstrut[t]\\
%           & Increased build break proneness & 5     &  \\
%           & Compilation required on the building & 1     &  \bigstrut[b]\\
%     \hline
%     \multirow{14}{*}{\raggedright \parbox{3.2cm}{Data Handling and \\Management}} & Data dependency & 7     & \multirow{14}{*}{30} \bigstrut[t]\\
%           & Data management & 6     &  \\
%           & Representative data generation difficulty & 6     &  \\
%           & Versioning/managing data and models & 6     &  \\
%           & Large data volume & 5     &  \\
%           & Data versioning & 4     &  \\
%           & Multiple data sources & 2     &  \\
%           & Complex data configuration & 1     &  \\
%           & Data caching & 1     &  \\
%           & Data quality check & 1     &  \\
%           & Data validation & 1     &  \\
%           & Ground truth data & 1     &  \\
%           & Heterogeneous data sources & 1     &  \\
%           & Non-symbolic data & 1     &  \bigstrut[b]\\
%     \hline   
%     \multirow{6}{*}{\raggedright \parbox{3.0cm}{Model Handling and \\Management}} & Model metrics performance tracking & 7     & \multirow{6}{*}{20} \bigstrut[t]\\
%           & Needs model training & 6     &  \\
%           & Model download & 3     &  \\
%           & Large models & 2     &  \\
%           & Loading models & 2     &  \\
%           & Requires model serving & 1     &  \bigstrut[b]\\
%     \hline
%      \multirow{6}{*}{\raggedright Integration and Maintenance} & Difficulty in integrating ML workloads into existing tools & 8     & \multirow{6}{*}{17} \bigstrut[t]\\
%           & Higher maintenance effort & 4     &  \\
%           & Instability of dependencies' APIs & 4     &  \\
%           & More library dependencies & 4     &  \\
%           & Library download & 3     &  \\
%           & Too many heterogeneous tools & 1     &  \bigstrut[b]\\
%     \hline
%     \multirow{4}{*}{\raggedright \parbox{3.2cm}{Organizational and Process Challenges}} & Less mature practices & 3     & \multirow{4}{*}{8} \bigstrut[t]\\
%           & Specialized knowledge required & 3     &  \\
%           & Conflict interests between different stakeholders & 1     &  \\
%           & Shorter deadlines & 1     &  \bigstrut[b]\\
%     \hline
%     \multirow{4}{*}{\raggedright Project-Specific Constraints} & Experimental nature of ML code & 3     & \multirow{4}{*}{8} \bigstrut[t]\\
%           & Different artifacts involved & 2     &  \\
%           & Hard to debug & 2     &  \\
%           & Less opportunities to automation & 1     &  \bigstrut[b]\\
%     \hline    
%   \end{tabular}%
%   \label{tab:freq_mentions_challenges_differences}%
% \end{table}%

Figure \ref{fig:q2_2_differences_and_challenges} illustrates the 8 themes and 51 codes that emerged from our analysis, highlighting key differences and challenges in the CI pipeline of ML projects. The themes are related to
\textit{Testing Complexity},
\textit{Infrastructure Requirements},
\textit{Build Duration and Stability},
\textit{Data Handling and Management},
\textit{Model Handling and Management},
\textit{Integration and Maintenance Challenges},
\textit{Organizational and Process Challenges}, and
\textit{Project-Specific Constraints}.
Additionally, 11 (7.1\%) participants perceived no significant differences in the challenges between the CI pipeline of ML and non-ML projects, while 28 (18.1\%) participants did not respond to this question. 
% Table \ref{tab:freq_mentions_challenges_differences} shows the frequency of responses for each code and theme that emerged from our analysis. 
Below, we provide detailed insights into the main codes associated with each theme identified in our thematic analysis.

\textbf{\textit{Testing Complexity.}}\textsuperscript{(44)}
This is the most mentioned theme by our participants, which is related to the difficulties of ML projects in establishing robust testing within the CI pipeline. Participants frequently cited \textit{difficulties in testing}\textsuperscript{(26)}, \textit{non-determinism}, \textsuperscript{(17)} and \textit{hard reproducibility}\textsuperscript{(5)} as major hurdles. This theme reinforces the idea that \textit{achieving high test coverage}\textsuperscript{(4)} and reliable testing practices are particularly challenging in the context of ML, further complicating CI implementation.
According to P66, \textit{``ML projects have a more nuanced testing requirement for CI purposes. Unit tests are not enough, we need to have data-centric tests such as data validation, verification. This would mean, checks at multiple stages of the pipeline(or workflow)''}. In addition, P57 highlights that \textit{``ML projects often are stochastic or make statistical guarantees in nature, and require different forms of testing/verification as a result''}. Furthermore, P75 shared, \textit{``we can only test the [model] performance on old data which might not be a good evaluation, whereas in non-ML projects we can just build on top of existing unit tests, in ML projects the tests can change depending on data and model''}. 

\textbf{\textit{Resource and Infrastructure Requirements}}.\textsuperscript{(43)}
This theme includes the significant resource demands and infrastructure complexities necessary for successful CI implementation in ML.
Notably, the code \textit{more computational resources required}\textsuperscript{(32)} highlights a critical barrier that teams must overcome to effectively implement CI practices in ML projects. The participants’ feedback indicates that \textit{higher resource costs}\textsuperscript{(11)} and \textit{the need for configurations and debugging of low-level libraries}\textsuperscript{10} add additional layers of complexity to the CI process. As mentioned by P118, \textit{``ML models often require significant computational resources, especially for training complex models or processing large datasets. CI pipelines need to provision and manage these resources efficiently, which may involve using specialized hardware accelerators (e.g., GPUs) or cloud-based services''}. Additionally, P96 explains that \textit{``Often, models are too large to be practically used on CPU runners, and thus require a runner to be equipped with a CUDA-enabled GPU, which most CI platforms do not provide and require users to set up \& manage their own runner''}.

\textit{\textbf{Build Duration and Stability}}.\textsuperscript{(32)}
It encompasses the challenges of managing long build durations and ensuring stability in the CI process in the ML domain.
Notably, \textit{long build durations}\textsuperscript{(28)} and \textit{increased build break proneness}\textsuperscript{(5)} emerged as the most frequently cited issues related to the build process in ML projects. These findings corroborate with our previous study that quantitatively shows that ML projects face challenges related to extended build duration \citep{bernardo2024machine}, which can hinder rapid development cycles. Participants have indicated that maintaining short build durations is a critical aspect of CI practices that is often not fully realized. 
As P40 emphasized, \textit{``Workflows are much longer since models are large and resource extensive. One could either spend a ton of money to use larger GPUs or decrease testing coverages''}. Additionally, handling the data needed to test the models also put additional complexity to the build process. P85 explained, \textit{``When huge amounts of data are involved, then it can be hard to retrieve it quickly enough, which leads to high testing times"}. Indeed, the addition of data and model handling in the CI pipeline of ML projects direct impact on their build duration. For instance, P31 shared, \textit{``When doing Unitest, I feel that it takes more time than in other projects where it is not directly related to the code (e.g. when downloading a model once on \textsc{GitHub} Workflow and taking it to test, it takes quite a long time)"}.

\textit{\textbf{Data Handling and Management}}.\textsuperscript{(30)}
This theme focuses on the complexities involved in managing data throughout the CI pipeline of ML projects, such as \textit{data dependency}\textsuperscript{(7)}, \textit{data management}\textsuperscript{(6)}, and \textit{large data volume}\textsuperscript{(5)}, showing that practitioners often struggle with ensuring data quality and accessibility. As mentioned by several participants, effective management of data is essential for successful CI implementation, highlighting the need for robust strategies to handle diverse data requirements in ML projects.
For instance, P100 explained, \textit{``ML projects often deal with large, complex datasets, requiring robust data management and handling strategies. This introduces challenges in terms of storing, accessing, and versioning datasets within the CI pipeline''}. In addition, P14 shared, \textit{``Data configuration is a complex task and needs to be solved in the pipeline too. Fixtures in this context are really complex to accomplish, but they give more value to the entire process of CI''}.
% P65 -> Another aspect is continuous updates in API versions, data sources which need to be verified during the CI process. It isn't possible to perform that check on the entire dataset since the size of the same can be arbitrarily large.

\textbf{\textit{Model Handling and Management.}}\textsuperscript{(20)} 
This theme explores the challenges of tracking and managing ML models within the CI pipeline.
Participants identified issues such as\textit{ model metrics performance tracking}\textsuperscript{(7)} and the \textit{need for ongoing model training}\textsuperscript{(6)}. These challenges underscore the importance of monitoring and maintaining models throughout their lifecycle to ensure their effectiveness, reinforcing the necessity of CI practices that can accommodate the unique demands of ML models.
As explained by P25, \textit{``There are some specific challenges that arise when implementing a continuous integration pipeline in ML projects compared to regular projects. For example, there is a need to version and store datasets and machine learning models, as well as track model performance metrics on each commit''}.
According to P128, \textit{``Sometimes keeping track of metrics for example model accuracy becomes a challenge. Especially since our output changes quite a lot during experiments''}.




\textbf{\textit{Integration and Maintenance.}}\textsuperscript{(17)} 
It highlights the difficulties in integrating CI practices with existing tools and the ongoing maintenance challenges that arise from evolving dependencies in ML projects.
The \textit{difficulty in integrating ML workloads into existing tools}\textsuperscript{(8)} and the \textit{higher maintenance effort}\textsuperscript{(4)} emphasize the challenges ML teams face when adapting CI practices to their specific environments. Participants expressed concerns regarding the \textit{instability of dependencies' APIs}\textsuperscript{(4)} and the need for \textit{additional library dependencies}\textsuperscript{(4)}, indicating that CI in ML is complicated by a constantly evolving set of tools and technologies.
P58 shared, \textit{``For builds you usually rely on many other software pieces, pytorch, hugging face etc and the datasets are sometimes large and not even publicly accessible to be able to access via a GitHub action''}. 
% In addition, P12 explained, \textit{``Deployment of things like EMR packages have different patterns and requirements not as well supported by public actions"}. 
In addition, P49 emphasized, \textit{``Training ML models generally requires significant computational resources, such as GPUs or TPUs, which may not be readily available in standard CI environments. Integrating these resources with CI pipelines often requires additional configurations and costs''}.

% P128 -> \textit{``I think the challenge I normally have is that they are many tools. Each person has their own preference coming from different backgrounds. I have found that having a sessions with teammates and exchanging ideas has lead to us developing a pretty interesting pipelines together. "}.


\textit{\textbf{Organizational and Process Challenges}}.\textsuperscript{(8)} 
This theme presents organizational barriers that impact CI implementation in ML
projects.
The findings related to this theme reveal that factors like \textit{less mature practices}\textsuperscript{(3)} and \textit{specialized knowledge requirements}\textsuperscript{(3)} pose difficulty on the integration of CI into ML workflows. Participants noted \textit{conflicts of interest between stakeholders}\textsuperscript{(1)} and pressures from \textit{shorter deadlines}\textsuperscript{(1)}, suggesting that organizational culture and dynamics play a significant role in the adoption of CI practices in the ML domain.
For example, P20 noted, \textit{``I think CI best practices are less defined for ML projects compared to non-ML"}. Furthermore, P104 elaborated, \textit{``Domain specific knowledge required for reviewing certain features, data caching and test runtime management"}.


\textit{\textbf{Project-Specific Constraints.}}\textsuperscript{(8)}
This theme highlights challenges and differences specific to ML projects and their characteristics.
Participants emphasized the \textit{experimental nature of ML code}\textsuperscript{(3)}, the involvement of \textit{different artifacts}\textsuperscript{(2)}, and \textit{debugging hardness}\textsuperscript{(2)} are factors related to ML projects that might lead to a nuanced implementation of a CI pipeline in ML projects.
For instance, P68 noted, \textit{``Many ML project code may be temporary and experimental, it would be costly to integrate a CI pipeline"}. 
In addition, P79 remarked, \textit{``The artifacts for an ML pipeline look way different from other software projects. You’re likely testing your ML pipeline in different chunks (preprocessing, vectorizing, retraining all as separate scripts). Setting up CI to ensure each of these pieces work independently, and their outputs are correct, takes a lot more focus''}. 
Finally, a challenge associated with debugging is elucidated by P83, who stated \textit{``you have to debug some hardware errors when working on ML projects''}.

% Overall, these project-specific constraints demonstrate the necessity for a flexible approach to CI in ML projects, acknowledging the unique challenges compared to traditional software development.

% To deepen our understanding of the relationships among the themes identified in our thematic analysis, we performed an exploratory co-occurrence analysis. This analysis examined how frequently different themes appear together in our participants' responses, revealing insights into the interconnected challenges faced by practitioners when implementing CI pipelines in ML projects. Figure \ref{fig:q2_2_themes_co_occurrence}\sergio{João, acho que ainda não entendi essa figura direito. Me parece que os temas que co-ocorreram em pares são mostrados duas vezes na figura, por exemplo "Testing Complexity x Resource and Infra", "Build Duration x Resource and Infra", "Testing Complexity x Model Handling". Acho isso confuso. Se for o caso, seria bom dizer isso no texto. Eu preferiria que não tivessa essa dupla ocorrência na figura. Me parece que a figura mostra tanto as co-ocorrências de "A x B" como de "B x A"} illustrates the co-occurrence matrix of the themes identified in our analysis. 
% The left-hand side displays the set size, representing the total number of occurrences for each theme after filtering out themes mentioned alone (without any co-occurrence).
% The right-hand side shows the intersection size using vertical bars. These bars represent the number of occurrences where specific themes co-occur with one or more other themes.
% The dots and lines below the vertical bars illustrate which themes are part of each intersection, helping to visualize how themes overlap and are interconnected.
% Together, this visualization provides insights into the relationships between themes, highlighting how participants perceive these challenges as interconnected or occurring together.

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[ width=12cm]{q2_2_themes_co_occurrence.pdf}
% 	% figure caption is below the figure
% 	\caption{Co-occurrence of Themes related to Challenges and Differences in implementing a CI pipeline in ML projects compared to non-ML projects.}
% 	\label{fig:q2_2_themes_co_occurrence}       % Give a unique label
% \end{figure}

% The co-occurrence analysis uncovers likely interdependencies among the themes related to challenges in implementing CI pipelines in ML projects. Notably, the theme \textit{Resource and Infrastructure Requirements} stands out as a correlated factor with multiple aspects of CI effectiveness. Its frequent co-occurrence with \textit{Testing Complexity} (5 co-occurrences) and \textit{Build Duration and Stability} (5 co-occurrences) suggests that resource availability plays a role in facilitating efficient CI processes.
% Effective testing is a vital component of CI, however, the complexities involved in establishing robust testing protocols may increase resource demands. Additionally, the co-occurrence of \textit{Data Handling and Management} with \textit{Testing Complexity} (4 occurrences) suggests that effective data management practices are related to testing efficiency optimization and resource allocation.
% Moreover, the relationship between \textit{Build Duration and Stability} and both \textit{Resource and Infrastructure Requirements} (5 co-occurrences) and \textit{Model Handling and Management} (2 co-occurrences) indicates that the efficiency of builds correlates with proper resource management. Addressing challenges related to maintaining short build durations can be achieved by allocating adequate resources and managing the complexities associated with model handling.

\begin{highlightbox}
\textbf{Key Findings:}
\begin{itemize}
    % \item \textit{\textbf{Commitment to CI Practices:}} A significant majority (75.5\%) of participants believe their ML teams strive to adopt CI practices.
    \item \textit{\textbf{Perceived Differences:}}     
    % Participants commonly observed differences in test coverage (61.9\%) and build duration (63.2\%) between ML and non-ML projects. However, the frequency of quicker fixes for broken builds and more frequent commits showed more neutral perceptions.
    Most participants often or always perceive ML projects as having longer build durations (63.2\%, \nicefrac{98}{155}) and lower test coverage (61.9\%, \nicefrac{96}{155}).
    In contrast, responses regarding commit frequency and build fix times were more neutral. Most participants reported that ML projects only sometimes, rarely, or never fix broken builds more quickly (77.4\%, \nicefrac{120}{155}) or commit more frequently (74.8\%, \nicefrac{116}{155}) than non-ML projects.
    
    \item \textit{\textbf{Challenges in CI Implementation:}} Participants identified eight main themes reflecting challenges specific to ML projects:
    \textit{Testing Complexity},
    \textit{Infrastructure Requirements},
    \textit{Build Duration and Stability},
    \textit{Data Handling and Management},
    \textit{Model Handling and Management},
    \textit{Integration and Maintenance Challenges},
    \textit{Organizational and Process Challenges}, and
    \textit{Project-Specific Constraints}.
    % \item \textit{\textbf{Co-occurrence Insights:}} The analysis revealed correlations among themes, particularly suggesting that Resource and Infrastructure Requirements plays a fundamental role in the effectiveness of CI processes.
    % \item \textit{\textbf{Holistic Approach Needed:}} Addressing the interconnected challenges -- such as testing, resource allocation, and model handling -- is essential for enhancing CI practices in ML workflows.
\end{itemize}
\end{highlightbox}





%%%% something that can be used in the implications

% The interconnected nature of these themes underscores the complexities inherent in CI implementation within ML projects. Resource management emerges as a key factor influencing not only testing practices and build stability but also the handling of models and data. To enhance CI practices, practitioners should adopt a holistic approach that simultaneously addresses resource allocation, testing complexity, and model handling.

\subsection*{\textbf{\RQtwo}}


The results for RQ2 explore practitioners' perspectives on build duration in ML projects, focusing on their importance, expectations across project sizes, and factors contributing to longer durations.

\subsubsection*{\textbf{Practitioners' Perspectives on Build Duration: Importance and Expectations}}

Figure~\ref{fig:q3_1_Perceived_importance_on_ML_projects_keeping_a_short_build_duration} illustrates participants' perceptions of the importance of keeping a short build duration in ML projects. The responses are categorized into five levels of importance, ranging from \textit{``Not at all important"} to \textit{``Very important"}, with the additional \textit{``N/A''} category for the absence of a response. 
The majority of participants (69\%, \nicefrac{107}{155}) recognized the importance of keeping build durations short in ML projects. Specifically, 30.3\% (n = 47) of participants rated this factor as \textit{``Important''}, 27.1\% (n = 42) considered it \textit{``Fairly important''}, while 11.6\% (n = 18) indicated that keeping build durations short is \textit{``Very important''}.
Interestingly, 23.9\% (n = 37) of participants rated the importance of keeping a short build duration as \textit{``Slightly important"}, while 4.5\% (n = 7) considered it \textit{``Not at all important"}. These results indicate that while build duration is acknowledged as a relevant factor, some practitioners may prioritize other aspects of CI pipelines.


\begin{figure}
	\centering
	\includegraphics[width=12cm]{q3_1_Perceived_importance_on_ML_projects_keeping_a_short_build_duration.pdf}	
	\caption{Participants' perceived importance on ML projects keeping a short build duration.}	\label{fig:q3_1_Perceived_importance_on_ML_projects_keeping_a_short_build_duration}       % Give a unique label
\end{figure}

To gain deeper insights into how ML practitioners manage build durations, we asked participants in \textsc{Question \#4.2} about their perception of an acceptable build duration for ML projects of similar size to their own. Figure~\ref{fig:q4_2_Acceptable_build_duration_for_an_ML_project_of_comparable_size_to_the_studied_project} illustrates these perceptions, categorized by project size (small, medium, and large), providing a clear overview of how acceptable build durations vary across different project scales.

\begin{figure}
	\centering
	\includegraphics[width=12cm]{q4_2_Acceptable_build_duration_for_an_ML_project_of_comparable_size_to_the_studied_project.pdf}
	% figure caption is below the figure
	\caption{Participants' perceptions of acceptable build duration of ML projects, categorized by project size (small, medium, large).}	\label{fig:q4_2_Acceptable_build_duration_for_an_ML_project_of_comparable_size_to_the_studied_project}       % Give a unique label
\end{figure}

% \textbf{\textit{As ML projects grow in size, ML practitioners tend to be more flexible with longer build durations, deviating from the commonly recommended CI guideline of maintaining builds under 10 minutes.}}
% While small projects emphasize the importance of short, fast builds, larger projects demonstrate a higher tolerance for extended build times, with some participants even accepting builds that exceed 30 minutes, deviating from the rule of thumb that a build should be executed under 10 minutes to provide quick feedback \citep{fowler-ci-2006}.
% This increased tolerance for longer builds in larger ML projects reflects the growing complexity of these projects, which often involve more intricate data management, extensive testing procedures, and complex model integration processes. As ML projects scale, the need for thorough validation and resource-intensive operations naturally leads to longer build times.


For small ML projects, there is a consensus on acceptable build durations, with all participants ($100\%, n = 2$) indicating a preference for builds lasting 10–20 minutes. This aligns closely with the observed median build duration of 10.3 minutes from our prior quantitative study~\citep{bernardo2024machine}, reinforcing the emphasis on speed and efficiency in small projects. These projects tend to feature simpler workflows, fewer dependencies, and lower resource demands, which naturally support shorter build times. However, the limited sample size ($n = 2$) reduces the generalizability of this finding, requiring cautious interpretation.

In medium-sized ML projects, a wider variety of expected build durations is deemed acceptable, indicating that his group has more diverse needs. The majority of participants ($56.7\%, n = 17$) expect relatively short builds, with 26.7\% ($n = 8$) preferring durations under 10 minutes and another $30\% (n = 9)$ favoring builds lasting 10–20 minutes. These preferences align well with the observed median build duration of $12.9$ minutes from our prior work \citep{bernardo2024machine}, suggesting that practitioners generally expect builds in medium-sized projects to remain efficient while accommodating moderate increases in complexity. However, the tolerance for slightly longer durations -- such as the $20\% (n = 6)$ of participants who find builds of 20–30 minutes acceptable -- indicates that practitioners acknowledge the trade-offs between quick feedback and the growing demands of more intricate workflows. Additionally, the $20\% (n = 6)$ of participants indifferent to build duration may represent those who prioritize factors such as stability or comprehensive testing over build speed.


In case of larger ML projects, 20.3\% (n = 24) of practitioners consider build times exceeding 30 minutes acceptable, compared
to only 3.3\% (n = 1) in medium-sized projects.
% For practitioners of large ML projects, longer build durations are acceptable. 
While 21.2\% (n = 25) of participants prefer builds lasting less than 10 minutes, the largest group (31.4\%, n = 37) considers durations of 10–20 minutes acceptable, followed by $23.7\% (n = 28)$ who are comfortable with builds lasting 20–30 minutes. Combined, 76.3\% (n = 90) of respondents favor builds under 30 minutes, aligning closely with the observed median build duration of 21.4 minutes reported in our prior work \citep{bernardo2024machine}.
However, the acceptance of builds exceeding 30 minutes by 20.3\% (n = 24) underscores the trade-offs inherent in scaling large ML systems. This alignment between practitioners’ expectations and observed build durations reflects a nuanced understanding of the challenges associated with scaling ML projects, which often involve complex dependencies, resource-intensive computations, and extensive validation processes. 

\subsubsection*{\textbf{Factors Contributing to Longer Build Durations}}

In our previous work \citep{bernardo2024machine}, we quantitatively compared the build durations of ML and non-ML projects, finding that ML projects generally exhibit longer build times. To assess whether these findings align with practitioners' expectations, we included this topic in \textsc{Question \#3.2} of our survey.
The majority of respondents (75.5\%, 117 out of 155) agreed with our findings, indicating that they expect ML projects to have longer build durations. In contrast, 5.2\% (8 out of 155) disagreed. While many of these participants did not provide a clear rationale for their disagreement, some suggested that smaller ML projects might not experience significantly longer build times compared to non-ML projects. For example, P37 noted, \textit{``My projects are relatively small so the impact is usually not that noticeable''}.
A further 3.9\% (6 out of 155) of respondents highlighted that the build duration depends on project-specific characteristics. As P62 explained, \textit{``it [the build duration] does not only depends on LOC, but mostly on the nature of the ML technology or models to use. Is not the same to run a build using a project with some light models \& libraries like XGBoost or scikit-learn vs Deep Learning and LLMs"}. 
Finally, 17.4\% (24 out of 155) of participants did not provide an answer or a clear response to the question.

% ---------------------------

Participants identified 45 factors that contribute to longer build durations in ML projects, which were grouped into seven main themes. Figure \ref{fig:q3_2_reasons_for_longer_build_duration} illustrates the themes and associated codes that emerged from our thematic analysis.
% The central theme focuses on the \textit{Reasons for Longer Build Duration} in ML, aligning with the core objective of this analysis. The second-level (or axial) themes represent high-level categories capturing key reasons for longer build durations, while the third-level themes (or codes) offer more specific insights, grouped according to their relationship with the second-level themes. The thickness of the edges in Figure \ref{fig:q3_2_reasons_for_longer_build_duration} reflects the frequency of each code's emergence during the analysis. Additionally, Table \ref{tab:reasons_for_longer_build_in_ML} presents the frequency of occurrences for each code and theme.

\begin{figure}
	\centering
	\includegraphics[ width=12cm]{q3_2_reasons_for_longer_build_duration.png}
	% figure caption is below the figure
	\caption{Perceived Reasons for Longer Build Duration in the CI pipeline of ML projects.}
	\label{fig:q3_2_reasons_for_longer_build_duration}       % Give a unique label
\end{figure}

% ---------------------------

The most frequently cited themes  were \textit{Project-Specific Challenges}\textsuperscript{(30)}, \textit{Model-Specific Challenges}\textsuperscript{(28)}, and \textit{Testing and Validation}\textsuperscript{(26)}. Other significant themes included \textit{Data Handling and Processing}\textsuperscript{(25)}, \textit{Resource and Infrastructure Demands}\textsuperscript{(25)}, \textit{Dependency Management}\textsuperscript{(24)}, and \textit{Computational Complexity}\textsuperscript{(11)}. These themes highlight the varied and complex factors contributing to extended build durations in ML projects.
% Table \ref{tab:reasons_for_longer_build_in_ML} presents the frequency of responses for each code and theme that emerged from our analysis. 
We provide detailed explanations of the main codes linked to each theme in the following.

% \begin{table}[htbp]
%   \centering
%   \caption{Frequency of responses for each code and theme related to reasons for longer build duration in ML projects.}
%     \begin{tabular}{p{3.2cm} p{5cm} p{1cm} p{1cm}}  % Adjusted widths
%     \hline    
%     \multirow{3}{*}{\textbf{Theme}} & 
%     \multirow{3}{*}{\textbf{Code}} & {\textbf{Freq. per code}} & {\textbf{Freq. per theme}} \bigstrut\\
%     \hline
%     {\multirow{11}[2]{*}{Project-Specific Challenges}} & Higher complexity & 20    & \multirow{11}[2]{*}{30} \bigstrut[t]\\
%           & Specific environment & 2     &  \\
%           & Complex build parameters & 1     &  \\
%           & Conflicts across runtime environments & 1     &  \\
%           & Extra build steps & 1     &  \\
%           & Larger CI pipeline & 1     &  \\
%           & Monolithic application & 1     &  \\
%           & Multiple programming languages interactions & 1     &  \\
%           & Support to different environments & 1     &  \\
%           & Lack of software engineering expertise & 1     &  \\
%           & Less mature practices & 1     &  \bigstrut[b]\\
%     \hline
%     {\multirow{7}[2]{*}{Model-Specific Challenges}} & Model (re)training & 19    & \multirow{7}[2]{*}{28} \bigstrut[t]\\
%           & Model inference time & 5     &  \\
%           & Model loading & 4     &  \\
%           & Large models & 3     &  \\
%           & Increased model parameters & 1     &  \\
%           & Model architecture & 1     &  \\
%           & Model packaging in build & 1     &  \bigstrut[b]\\
%     \hline
%     {\multirow{3}[2]{*}{Testing and Validation}} & More complex testing & 22    & \multirow{3}[2]{*}{26} \bigstrut[t]\\
%           & Model validation & 7     &  \\
%           & Model testing & 2     &  \bigstrut[b]\\
%     \hline
%     {\multirow{6}[2]{*}{\raggedright \parbox{3.2cm}{Resource and Infrastructure 
%     \\Demands}}} & More computational resources required & 19    & \multirow{6}[2]{*}{25} \bigstrut[t]\\
%           & Memory handling & 3     &  \\
%           & Hardware support overhead & 2     &  \\
%           & Hardware acceleration limitation & 1     &  \\
%           & Hardware specific dependencies & 1     &  \\
%           & Lack of infrastructure & 1     &  \bigstrut[b]\\
%     \hline
%     {\multirow{9}[2]{*}{Data Handling and Processing}} & Large data handling & 16    & \multirow{9}[2]{*}{25} \bigstrut[t]\\
%           & Data (pre)processing & 10    &  \\
%           & Data loading & 3     &  \\
%           & Data dependency & 2     &  \\
%           & Data agumentation & 1     &  \\
%           & Data generation & 1     &  \\
%           & Data transformation & 1     &  \\
%           & Feature engineering & 1     &  \\
%           & Feature selection & 1     &  \bigstrut[b]\\
%     \hline
%     {\multirow{4}[2]{*}{Dependency Management}} & More dependencies required & 20    & \multirow{4}[2]{*}{24} \bigstrut[t]\\
%           & CPU/GPU resource dependencies & 3     &  \\
%           & CUDA libraries compilation & 3     &  \\
%           & External library build time & 1     &  \bigstrut[b]\\
%     \hline
%     {\multirow{5}[2]{*}{Computational Complexity}} & Computationally intensive & 4     & \multirow{5}[2]{*}{11} \bigstrut[t]\\
%           & Python runtime inefficiency & 4     &  \\
%           & Lack of parallelism & 1     &  \\
%           & Metrics computation & 1     &  \\
%           & Slow c++ compiling & 1     &  \bigstrut[b]\\
%     \hline
%     \end{tabular}%
%   \label{tab:reasons_for_longer_build_in_ML}%
% \end{table}%

\textit{\textbf{Project-Specific Challenges}}.\textsuperscript{(30)} This theme captures the diverse and often unique project factors that can impact build duration in ML projects. Participants highlighted \textit{higher complexity}\textsuperscript{(20)} and \textit{specific environments}\textsuperscript{(2)} as major influences, noting that ML projects often have specialized requirements that add complexity to the CI process. As P07 explained, \textit{``ML projects require complex computations that can take long and as a result, the builds will take longer from setting up the environment for large libraries and executing complex intensive codes"}.
Issues such as \textit{complex build parameters}\textsuperscript{(1)}, \textit{conflicts across runtime environments}\textsuperscript{(1)}, and \textit{support for different environments}\textsuperscript{(1)} were also mentioned, emphasizing the challenges ML projects face in maintaining consistency across varied setups. 
P41 elaborated, \textit{``ML projects can encounter conflicts across various runtime environments as well as challenges in resource allocation. Moreover, running an ML project without thorough checks often leads to 'out of memory' errors.''}.

Additionally, some respondents pointed to \textit{monolithic application architectures}\textsuperscript{(1)}, where data sources are often tightly coupled with the model, requiring engineers to push the entire codebase as a single, monolithic application. 
Other factors cited as making builds more challenging and time-consuming in the ML domain include extra build steps\textsuperscript{(1)}, interactions between multiple programming languages\textsuperscript{(1)}, larger CI pipelines\textsuperscript{(1)}, as well as a lack of software engineering expertise\textsuperscript{(1)} and less mature development practices\textsuperscript{(1)}.

\textit{\textbf{Model-Specific Challenges}}.\textsuperscript{(28)} This theme reflects the unique demands associated with ML models, which often require additional processing steps that lengthen build times. For example, \textit{model (re)training}\textsuperscript{(19)} was frequently mentioned, as many ML pipelines necessitate frequent retraining to ensure model accuracy. Other codes within this theme include \textit{model inference time}\textsuperscript{(5)}, \textit{model loading}\textsuperscript{(4)}, and \textit{handling large models}\textsuperscript{(3)}, each contributing to increased build durations. As highlighted by P134, \textit{``While a short build time is generally desirable, the need for more data processing as well as involving basic training and inference will likely increase the build time''}.
Other participants noted the difficulty of integrating model-related processes into the CI pipeline, with factors like \textit{model architecture}\textsuperscript{(1)}, \textit{model packaging in builds}\textsuperscript{(1)}, and \textit{increased model parameters}\textsuperscript{(1)} posing additional hurdles that are not typically encountered in non-ML projects.
P31 explained, \textit{``A large ML project would include multiple functions (or multiple models) and the number of parameters in the model would increase accordingly''}.

\textit{\textbf{Testing and Validation}}.\textsuperscript{(26)} This theme focuses on the extensive testing requirements in ML projects, which add significant time to the build process. \textit{More complex testing}\textsuperscript{(22)} was the most frequently cited factor, as ML projects often require not only traditional software testing but also additional validation steps specific to model performance and data quality. 
\textit{Model validation}\textsuperscript{(7)} and \textit{model testing}\textsuperscript{(2)} were also highlighted, with respondents noting that these processes are essential to ensure reliable model behavior and accurate results but can considerably extend build times. As P57 explained, \textit{``ML projects often have multiple steps that need to happen over (even in test settings) non-trivial data sizes to validate correctness. E.g., a full integration test for a project I'm involved needs to extract some synthetic raw data from disk, re-format it, pre-process it, then ``train'' a model on a synthetic task over that data to convergence, then validate that model. Each of these steps use real tools designed for production use (so that the test covers appropriate, real-world usage) and the end to end pipe takes a long time''}.

\textit{\textbf{Data Handling and Processing}}.\textsuperscript{(25)} This theme represents the complexities involved in managing large datasets, a core aspect of many ML projects that significantly impacts build duration. Participants pointed to \textit{large data handling}\textsuperscript{(16)} and \textit{data preprocessing}\textsuperscript{(10)} as two of the most time-consuming processes. \textit{Data loading}\textsuperscript{(3)} and \textit{data dependency management}\textsuperscript{(2)} were also noted as factors adding complexity, with one respondent explaining that handling large volumes of data slows down the CI pipeline, especially when specific dependencies or transformations are required. Other aspects such as \textit{data augmentation}\textsuperscript{(1)}, \textit{data generation}\textsuperscript{(1)}, \textit{data transformation}\textsuperscript{(1)}, \textit{feature engineering}\textsuperscript{(1)}, and \textit{feature selection}\textsuperscript{(1)} highlight the extensive data manipulation often required in ML builds, where preparing data for model training and validation can be as demanding as model-related processes. As P100 emphasized,
\textit{``ML projects inherently introduce additional computational burdens that contribute to longer build times. This is especially evident in medium and large ML projects, where the data processing and model training tasks become more significant. ML projects typically involve extensive data handling, including loading, cleaning, transforming, and preprocessing. These steps can be computationally intensive, especially when dealing with large datasets. This contributes to longer build durations''}.

\textit{\textbf{Resource and Infrastructure Demands}}.\textsuperscript{(25)} This theme addresses the substantial resource requirements that ML projects often impose, which can prolong build times when adequate infrastructure is not readily available. The need for \textit{more computational resources}\textsuperscript{(19)} was a recurrent point, with participants emphasizing the requirement for powerful hardware like GPUs and TPUs. P10 noted, \textit{``from my experience ML projects tend to have much longer build times since they utilize far more CPU/GPU resources than non-ML projects''}. In addition, P25 explained, \textit{``I've noticed that projects with machine learning tend to have build times that are several times longer than projects without ML. This is especially noticeable as the project size and complexity increase. I think this is due to the fact that ML projects require more computational resources and time for tasks specific to ML, such as data preprocessing, model training, and evaluation. These tasks can be very time-consuming, especially as the size of the project and dataset grows''}. 

Limited access to specialized hardware can delay builds, as noted by respondents who mentioned \textit{memory handling}\textsuperscript{(3)}, \textit{hardware support overhead}\textsuperscript{(2)}, \textit{hardware acceleration limitations}\textsuperscript{(1)}, \textit{hardware specific dependencies}\textsuperscript{(1)}, and the \textit{lack of infrastructure}\textsuperscript{(1)} as constraints. Practitioners expressed difficulties in securing the necessary resources to maintain efficient CI pipelines in ML projects. As explained by P49, \textit{``ML projects can encounter conflicts across various runtime environments as well as challenges in resource allocation. Moreover, running an ML project without thorough checks often leads to 'out of memory' errors''}.

% \textit{\textbf{Dependency Management}}.\textsuperscript{(24)} This theme brings attention to the large number of dependencies as well as the extensive  data/parameters specialized ML libraries require when installed in ML projects, which can add substantial overhead to the build process. 

\textit{\textbf{Dependency Management}}.\textsuperscript{(24)} This theme highlights the challenges posed by the large number of dependencies in ML projects, as well as the extensive data and parameters required by specialized ML libraries during installation, which can introduce significant build overhead. Since CI pipelines typically start from a fresh environment, these dependencies must be reinstalled in every run, further slowing down the process.
\textit{More dependencies required}\textsuperscript{(20)} was frequently cited, alongside issues related to \textit{CPU/GPU resource dependencies}\textsuperscript{(3)} and the \textit{compilation of CUDA libraries}\textsuperscript{(3)}, both of which are common in ML projects that rely on hardware acceleration. Participants also mentioned the time-consuming process of managing \textit{external library build times}\textsuperscript{(1)}, with one respondent pointing out that ML projects often depend on complex libraries that require specific configurations, making builds slower and more resource-intensive. As P94 observed, \textit{``ML projects tends to have a lot of dependencies, a large portion of them being hardware specific''}. Similarly, P63 explained, \textit{``I think it's [longer build duration] because of the various dependencies [ML projects require], for example if you want to any DL framework you need to install cuda which takes time, if you have a repo of various ML models ranging from text to vision they will have their long list of dependencies, so I think it's because of the various dependencies ML projects take much longer to build''}.

\textit{\textbf{Computational Complexity}}.\textsuperscript{(11)} It highlights the resource-intensive nature of ML projects, which often require substantial computational power and face efficiency challenges that extend build times. Participants frequently cited \textit{computationally intensive processes}\textsuperscript{(4)} and \textit{Python runtime inefficiencies}\textsuperscript{(4)} as factors slowing down builds. 
As P96 explained, \textit{``The computational requirements of machine learning pipelines intuitively takes significantly more time than the relatively quick unit \& integration tests of traditional software''}.
% As P55 explained, \textit{``ML projects often use many Python frameworks. Python is not an efficient programming language. Besides, ML projects should handle more dependencies, such as system dependencies, cross-project dependencies''}.
Limited parallel processing was also mentioned, with some respondents pointing to a \textit{lack of parallelism}\textsuperscript{(1)} and the impact of \textit{metrics computation}\textsuperscript{(1)} as significant contributors to extended durations. Additionally, \textit{slow C++ compilation}\textsuperscript{(1)} was noted, particularly in projects where C++ is used for performance-critical components. 
As P144 observed, \textit{``Most of the ML stack is written in Python and C++. The C++ compiler is pretty slow + rebuilding the stubs''}.
Together, these issues reflect the intensive computational demands that make rapid builds more challenging in ML projects. 
% As P48 observed, \textit{``The other reason [for longer build duration] is testing of model performance, including metrics computation, cross-validation, and possibly running inference on validation datasets''}.

% -----------------

% The co-occurrence analysis highlights the correlated nature of factors contributing to longer build durations in ML projects. As shown in Figure \ref{fig:q3_2_themes_co_occurrence}, Project-Specific Challenges emerge as the most prominent theme, frequently co-occurring with Resource and Infrastructure Demands (4 co-occurrences), Testing and Validation (4 co-occurrences), and Dependency Management (2 co-occurrences)\sergio{Não seriam 3 ocorrências?}. These co-occurrences suggest that project complexity exacerbates resource constraints, increases the overhead of thorough testing processes, and adds delays caused by managing dependencies. Notably, Model-Specific Challenges (i.e., model training and inference time) also intersect with Data Handling and Processing (4 occurrences) and Testing and Validation (3 occurrences), reflecting the significant time and resource demands introduced by managing large models, extensive data pipelines, and performing computationally intensive validation processes.

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[ width=12cm]{q3_2_themes_co_occurrence.pdf}
% 	% figure caption is below the figure
% 	\caption{Co-occurrence of Themes related to Reasons for Longer Build Duration in ML Projects.}
% 	\label{fig:q3_2_themes_co_occurrence}       % Give a unique label
% \end{figure}

% The analysis reveals that several co-occurrences involve three or more themes, emphasizing that build duration challenges are rarely isolated but result from the compounded effect of multiple interconnected factors. For instance, Data Handling, Resource and Infrastructure Demands, Testing and Validation, and Model-Specific Challenges intersect twice, illustrating how these overlapping challenges collectively amplify their impact on build times.

\begin{highlightbox}
\textbf{Key Findings:}
\begin{itemize}
    \item \textit{\textbf{Importance of Build Duration:}} 69\% of participants recognize short build durations as important, though this perception trend changes with project size and complexity.
    \item \textit{\textbf{Project Size Variability:}} Small ML projects tend to favor build times of 10–20 minutes.
    In medium-sized projects, most practitioners (56.7\%) also prefer short build times (10-20 minutes), while 20\% find durations up to 30 minutes acceptable.
    In contrast, large-sized projects are more tolerant of longer builds, with 20.3\% of practitioners considering build times exceeding 30 minutes acceptable, compared to only 3.3\% in medium-sized projects.
    % , and large projects often\sergio{Menos de 20\% dos participantes aceitam builds com mais de 30 minutos. Talvez "sometimes" seja uma palavra melhor aqui. } accept builds exceeding 30 minutes.
    \item \textit{\textbf{Expectation of Longer Builds:}} 75\% of participants expect longer build durations in ML projects compared to non-ML projects, driven by their complexity.
    \item \textit{\textbf{Reasons for Longer Builds:}} Participants highlighted seven key themes contributing to longer build durations in ML projects. The most frequently mentioned reasons include \textit{higher project complexity}, \textit{model training} during builds, \textit{increased computational resource demands}, \textit{extensive data handling}, and \textit{the need for managing numerous dependencies}.
    % \item \textbf{\textit{Co-occurrence Insights:}} Longer build durations in ML projects stem from the interplay of interconnected factors, with Project-Specific Challenges, Resource and Infrastructure Demands, and Data Handling frequently co-occurring with other themes.
\end{itemize}
\end{highlightbox}

\subsection*{\textbf{\RQthree}}

The results for RQ3 examine practitioners' perceptions of the challenges in testing ML projects and their views on acceptable test coverage rates for these projects.

\subsection*{\textbf{Challenges in Testing ML Projects}}

In \textsc{Question \#3.6} of our survey, participants were asked about the perceived challenges ML projects face when testing their source code, which we subsequently analyzed through a thematic analysis. 

Our participants identified 66 factors that can pose challenges to testing source code in ML projects.
These factors were categorized into eight key themes, as illustrated in
Figure~\ref{fig:q3_6_challenges_in_testing_ml_projects}, which presents the themes and their associated codes derived from our thematic analysis.
% Figure \ref{fig:q3_6_challenges_in_testing_ml_projects} illustrates the themes and associated codes that emerged from our thematic analysis. 
% The central theme focuses on the \textit{challenges in testing ML projects}. The second-level (or axial) themes represent high-level categories capturing the key challenges in testing ML projects, while the third-level themes (or codes) offer more specific insights, grouped according to their relationship with the second-level themes.
% Additionally, Table X presents the frequency of occurrences for each code and theme.

\begin{figure}
	\centering
	\includegraphics[ width=12cm]{q3_6_challenges_in_testing_ml_projects.png}
	% figure caption is below the figure
	\caption{Perceived Challenges in Testing ML projects.}
	\label{fig:q3_6_challenges_in_testing_ml_projects}       % Give a unique label
\end{figure}

The most frequently cited themes were \textit{Test Complexity}\textsuperscript{(56)}, \textit{Reproducibility and Platform Challenges}\textsuperscript{(41)}, \textit{Data Dependency and Qualit}y\textsuperscript{(31)}, and \textit{Computational and resource Constraints}\textsuperscript{(26)}. Other themes included
\textit{Testing Framework and Tooling Challenges}\textsuperscript{(9)},
\textit{Code and System Complexity}\textsuperscript{(8)},
\textit{Multidisciplinarity and Organizational Challenges}\textsuperscript{(7)} and
\textit{Model-Specific Testing Challenges}\textsuperscript{(3)}. These themes highlight the varied and complex factors that challenge the testing process of ML projects. 
% Table \ref{tab:challenges_in_testing_ML_projects} presents the frequency of responses for each code and theme that emerged from our analysis. 
We provide detailed explanations of the main codes linked to each theme in the following.


% Table generated by Excel2LaTeX from sheet 'q3_6_main_codes_latex'

% \begin{table}[htbp]
%   \centering
%   \caption{Frequency of responses for each code and theme related to challenges in testing ML projects.}
% \begin{tabular}{p{3.2cm} p{5cm} p{1cm} p{1cm}}  
%     \hline    
%     \textbf{Theme} & \textbf{Code} & \textbf{Freq. per code} & \textbf{Freq. per theme} \\
%     \hline
% \multirow{23}[2]{*}{Test Complexity} 
%     & Test complexity & 22 & \multirow{23}[2]{*}{56} \\
%     & Performance evaluation metrics complexity & 8 & \\
%     & Test assertion complexity & 7 & \\
%     & Test coverage challenges & 6 & \\
%     & Unsuitability of Oracle test & 5 & \\
%     & Integration test challenges & 4 & \\
%     & Debugging complexity & 3 & \\
%     & Edge case testing complexity & 2 & \\
%     & End-to-end test complexity & 2 & \\
%     & Mocking complexity & 2 & \\
%     & Test optimization complexity & 2 & \\
%     & Unsuitability of unit testing & 2 & \\
%     & Build initial test set & 1 & \\
%     & Complexity of building blocks combinations & 1 & \\
%     & Conditionals targeting difficulty & 1 & \\
%     & Difficult to simulate real use cases & 1 & \\
%     & Distributed training testing difficulties & 1 & \\
%     & Few unit testing & 1 & \\
%     & Loopholes in tests & 1 & \\
%     & More integration testing & 1 & \\
%     & Test data caching & 1 & \\
%     & Test scalability challenges & 1 & \\
%     & Training set anomaly detection hardness & 1 & \\
%     \hline
% \multirow{7}[2]{*}
% {\raggedright \parbox{3.2cm}{Reproducibility and Platform 
%     \\Challenges}}
%     & Non-determinism & 32 & \multirow{7}[2]{*}{41} \\
%     & Reproducibility challenges & 8 & \\
%     & Hardware variability & 4 & \\
%     & Cross-checking results with other sources & 1 & \\
%     & Cross-platform testing & 1 & \\
%     & Platform diversity & 1 & \\
%     & Environment-specific issues & 1 & \\
%     \hline
% \multirow{13}[2]{*}{Data Dependency and Quality}
%     & Data dependency & 12 & \multirow{13}[2]{*}{31} \\
%     & Large data volume & 6 & \\
%     & Data quality issues & 4 & \\
%     & Data validation & 3 & \\
%     & External sources & 3 & \\
%     & Dependency of non-accessible resources & 2 & \\
%     & Continuous Data Update & 1 & \\
%     & Data privacy & 1 & \\
%     & Data processing & 1 & \\
%     & Data sampling issues & 1 & \\
%     & Data/Model version control & 1 & \\
%     & Test input management & 1 & \\
%     \hline
% \multirow{6}[2]{*}
% {\raggedright \parbox{3.2cm}{Computational and Resource 
%     \\Constraints}}
%     & Longer running time & 11 & \multirow{6}[2]{*}{26} \\
%     & Computational resources constraints & 7 & \\
%     & GPU dependency & 7 & \\
%     & Computationally intensive tests & 3 & \\
%     & Iteration speed & 1 & \\
%     & Runtime issues & 1 & \\
%     \hline
% \multirow{3}[2]{*}
% {\raggedright \parbox{3.2cm}{Testing Framework and 
%     \\Tooling Challenges}}
%     & Dependencies & 6 & \multirow{3}[2]{*}{9} \\
%     & Dependencies versioning challenges & 2 & \\
%     & Lack of tooling & 2 & \\
%     \hline
% \multirow{5}[2]{*}{Code and System Complexity}
%     & Code quality issues & 4 & \multirow{5}[2]{*}{8} \\
%     & Codebase complexity & 2 & \\
%     & Artifact management & 1 & \\
%     & More unstable codebase/data & 1 & \\
%     & Notebook code testability issues & 1 & \\
%     \hline
% \multirow{6}[2]{*}
% {\raggedright \parbox{3.2cm}{Multidisciplinarity and 
%     \\Organizational Challenges}}
%     & Lack of proper testing skills & 3 & \multirow{6}[2]{*}{7} \\
%     & Multidisciplinarity of dev team & 2 & \\
%     & Contextual knowledge acquisition & 1 & \\
%     & Ethical and bias considerations & 1 & \\
%     & Interdisciplinary coding complexity & 1 & \\
%     & Test priorities & 1 & \\
%     \hline
% \multirow{3}[2]{*}
% {\raggedright \parbox{3.2cm}{Model-Specific Testing 
%     \\Challenges}}
%     & Complexity of ML models & 1 & \multirow{3}[2]{*}{3} \\
%     & Model diversity & 1 & \\
%     & Model drift testing & 1 & \\
%     \hline
% \end{tabular}
% \label{tab:challenges_in_testing_ML_projects}
% \end{table}

\textbf{\textit{Test Complexity.}}\textsuperscript{(56)}
This theme emerged as a recurrent challenge, with the highest number of references. Practitioners frequently cited the overall \textit{test complexity}\textsuperscript{(22)}, emphasizing the intricate nature of crafting, executing, and maintaining tests for ML systems. Unlike traditional software, ML projects often involve probabilistic outcomes, making it difficult to define deterministic assertions, as reflected by challenges such as \textit{test assertion complexity}\textsuperscript{(7)} and \textit{unsuitability of Oracle tests}\textsuperscript{(5)}.
For instance, P48 noted, ``\textit{ML models often produce different results on different runs due to inherent randomness in training processes. Traditional unit tests focus on code correctness, but ML models require validation of performance metrics (e.g., accuracy, precision, recall) on test datasets, which is more complex and less binary than typical pass/fail unit tests''}.

Furthermore, issues like \textit{test coverage challenges}\textsuperscript{(6)} and \textit{integration test challenges}\textsuperscript{(4)} indicate that achieving comprehensive coverage across diverse system components remains a significant obstacle.
P101 noted, \textit{``ML projects need higher test case coverage, which is difficult, because there can be a lot of possible paths"}. Additionally, P114 shared, \textit{``Writting in test shapes/scenarios that offer complete coverage is difficult. Traditionally we can measure coverage through code paths posted, but this breaks down when different data strategies might hit the same API's (and cause bugs in some cases)''}.
Performance evaluation in ML projects also presents a unique challenge, with \textit{performance evaluation metrics complexity}\textsuperscript{(8)} highlighting the difficulty of relying on metrics such as precision and recall rather than traditional binary outcomes. P138 explained, \textit{``It's not easy to implement a test which decides e.g. whether model achieves a certain value of performance metric on a popular dataset''}.
Additional difficulties include debugging complex systems, handling edge cases, and ensuring scalability, further complicating the testing process.


\textbf{\textit{Reproducibility and Platform Challenges.}}\textsuperscript{(41)}
This theme encompasses the difficulties in ensuring consistent and reproducible test results in ML projects. A significant issue is \textit{non-determinism}\textsuperscript{(32)}, which stems from factors such as random weight initialization and stochastic training processes. This inherent unpredictability complicates the ability to achieve consistent test outcomes. For instance, P118 observed, \textit{``ML models often produce non-deterministic outputs due to randomness in algorithms (e.g., random initialization of weights in neural networks) or variations in training data. This makes it challenging to validate outputs consistently across different test runs''}.
Another key aspect is the broader \textit{reproducibility challenges}\textsuperscript{(8)}, which are further intensified by issues such as \textit{hardware variability}\textsuperscript{(4)} and \textit{platform diversity}\textsuperscript{(1)}. Variations in execution environments can lead to inconsistencies in test results across systems. Additional platform-related issues, such as \textit{cross-checking results with other sources}\textsuperscript{(1)} and \textit{cross-platform testing}\textsuperscript{(1)}, further underscore the unique challenges faced by ML projects in comparison to non-ML systems.
As P89 explained, \textit{``ML models are not reproducible so designing tests that work across multiple hardware platforms is hard''}.

\textbf{\textit{Data Dependency and Quality.}}\textsuperscript{(31)}
This theme illustrates the critical role of data in ML testing. \textit{Data dependency}\textsuperscript{(12)} was identified as a major challenge, reflecting the reliance on high-quality and representative datasets to ensure meaningful test results. 
As P119 emphasized, \textit{``ML algorithms often need to generate data to validate algorithms. Thinking about the data and assertions for validating algorithms is more involved compared to non-ML projects''}.
Challenges related to \textit{large data volumes}\textsuperscript{(6)} further complicate testing, as processing and managing such datasets demand significant resources. P98 explained, \textit{``A lot of data is required to verify even simple models and pipelines''}.
Issues of \textit{data quality}\textsuperscript{(4)}, \textit{data validation}\textsuperscript{(3)}, and dependence on \textit{external sources}\textsuperscript{(3)} highlight the challenges in ensuring reliable test inputs. 
P10 noted, \textit{``High-quality data is crucial for ML models. Unclean or noisy data can lead to inaccurate predictions''}.
Furthermore, practitioners noted specific issues such as \textit{dependency on non-accessible resources}\textsuperscript{(2)}, \textit{continuous data updates}\textsuperscript{(1)}, and \textit{data privacy}\textsuperscript{(1)}, which illustrate the complexities of handling data pipelines. These challenges are unique to ML projects, where data plays a more central role in testing compared to traditional software systems.

\textbf{\textit{Computational and Resource Constraints.}}\textsuperscript{(26)}
This theme reflects the resource-intensive nature of ML testing. Practitioners highlighted the challenge of \textit{longer running times}\textsuperscript{(11)} due to the computational demands of training and testing ML models. Coupled with \textit{GPU dependency}\textsuperscript{(7)} and \textit{computational resources constraints}\textsuperscript{(7)}, these challenges significantly limit testing efficiency and scalability.
P21 explained, \textit{``Some code can ONLY run on GPUs so is very hard to test in a CI context. Some code requires lots of data to be tested on an integration level"}.
The need for substantial resources for tasks such as \textit{computationally intensive tests}\textsuperscript{(3)} and the resulting delays in \textit{iteration speed}\textsuperscript{(1)} create additional barriers to achieving comprehensive test coverage. 
As P49 explained, \textit{``ML systems often need to scale to handle large volumes of data. Testing how these systems scale, not just in terms of data size but also with respect to the computational and memory resources, is critical and challenging''}.
These constraints are not commonly seen in non-ML projects, making this a distinctly ML-specific challenge.

\textbf{\textit{Testing Framework and Tooling Challenges.}}\textsuperscript{(9)}
Practitioners frequently highlighted significant gaps in the existing tooling ecosystem for ML testing. \textit{Dependencies}\textsuperscript{(6)} and \textit{dependencies versioning challenges}\textsuperscript{(2)} were among the most commonly cited issues, reflecting the inherent complexity of managing extensive dependency trees in ML projects. As P55 explained, ML projects often require \textit{``more data input and more dependencies''}, which adds to the intricacy of the testing process.
Similarly, P140 noted that \textit{``integration and system tests are often hard as they require a number of pipeline steps and dependencies''}, emphasizing the challenges posed by the interdependencies of various components. Furthermore, the \textit{lack of tooling}\textsuperscript{(2)} specifically designed for ML testing underscores a critical gap in the current landscape, further complicating efforts to achieve adequate test coverage. As P51 observed, \textit{``lack of tooling, artifact management, data processing and dependencies can all be challenges''} in effectively testing ML projects.

\textbf{\textit{Code and System Complexity.}}\textsuperscript{(8)}
Challenges related to code and system complexity were notably prominent in ML projects. These systems often involve unstable and intricate codebases, as reflected by issues such as \textit{code quality issues}\textsuperscript{(4)}, \textit{codebase complexity}\textsuperscript{(2)}, and the presence of \textit{more unstable codebase/data}\textsuperscript{(1)}.
As explained by P100, \textit{``ML source code includes large amounts of numerical methods related to optimization and linear algebra routines. Obviously, whether this is really a difficulty depends on a person's background, but overall, there are few people with the necessary background to be able to skillfully navigate and contribute to a large ML codebase''}.
Unique challenges, such as testing \textit{notebook code}\textsuperscript{(1)}, further distinguish ML workflows from those of non-ML projects. As P54 observed, \textit{``Code quality [in ML projects] is generally worse, for many reasons''}. Similarly, P82 noted, \textit{``Lots of notebook code, lower quality, not very testable''}.
These complexities make testing ML systems fundamentally different from non-ML projects.

\textbf{\textit{Multidisciplinarity and Organizational Challenges.}}\textsuperscript{(7)}
This theme highlights the need for interdisciplinary expertise in ML testing. Challenges such as the \textit{lack of proper testing skills}\textsuperscript{(3)} and the \textit{multidisciplinarity of development teams}\textsuperscript{(2)} emphasize the importance of collaboration between software engineers, data scientists, and domain experts.
As P78 explained, the main challenge in testing ML projects is \textit{``asking an ML developer whose expertise is writing ML code to write tests (which is not his strength)''}. Similarly, P36 observed, \textit{``AI scientists not having good experience testing, software engineers not having good enough AI knowledge''}.
Additional considerations, such as \textit{ethical and bias considerations}\textsuperscript{(1)} and \textit{contextual knowledge acquisition}\textsuperscript{(1)}, further underscore the organizational and knowledge-based complexities unique to ML testing. 

\textbf{\textit{Model-Specific Testing Challenges.}}\textsuperscript{(3)}
This theme highlights the unique aspects of testing ML models. Challenges such as the \textit{complexity of ML models}\textsuperscript{(1)}, \textit{model diversity}\textsuperscript{(1)}, and \textit{model drift testing}\textsuperscript{(1)} illustrate the dynamic and evolving nature of ML systems. 
For instance, when discussing model complexities, P70 explained, \textit{``Our code needs expensive GPUs to run, so the tests would need GPUs to test stuff. Also... the data is enormous \& the models dynamics are complicated. So all of the huge math and huge data aspects, as well as the GPU aspects, are special''}. Similarly, P63 noted that \textit{``model flakiness and model variety''} present significant challenges in testing ML projects. Additionally, P118 highlighted the issue of concept drift, stating, \textit{``ML models can experience concept drift, where the relationships in the data change over time, affecting model performance. Continuous monitoring and testing for model drift require specialized techniques and tools beyond traditional unit or integration testing''}.
These challenges require specialized approaches that go beyond traditional testing methodologies.

%% Co-occurrence analysis

% The co-occurrence analysis of themes enlightens the interconnected nature of challenges in testing ML projects, emphasizing the complexity and multifaceted aspects of the testing landscape. Figure \ref{fig:q3_6_themes_co_occurrence} visualizes these relationships, providing insights into how different challenges overlap and interact.
% The most frequent co-occurrences were between \textit{Test Complexity} and \textit{Reproducibility and Platform Challenges}, co-occurring 11 times. This underscores the deep interconnection between the inherent complexity of testing ML systems and the difficulties in achieving reproducibility in non-deterministic and varied execution environments. Additionally, co-occurrences between \textit{Computational and Resource Constraints} and \textit{Test Complexity} were observed 6 times, highlighting the critical relationship between the availability of computational resources and the ability to manage and execute complex testing processes effectively.

% Intersections involving \textit{Data Dependency and Quality} frequently overlapped with \textit{Test Complexity} (5 instances), demonstrating how challenges such as data quality, accessibility exacerbate the difficulties of designing and conducting effective tests. Furthermore, \textit{Testing Framework and Tooling Challenges} intersected with \textit{Data Dependency and Quality} and \textit{Reproducibility and Platform Challenges} in 2 instances each. These co-occurrences emphasize the essential role of robust tools and frameworks in managing data dependencies and addressing reproducibility issues in ML testing.


% \begin{figure}[H]
% 	\centering
% 	\includegraphics[ width=12cm]{q3_6_themes_co_occurrence.pdf}	
% 	\caption{Co-occurrence of Themes related to Challenges in Testing ML Projects.}
% 	\label{fig:q3_6_themes_co_occurrence}       % Give a unique label
% \end{figure}

\subsection*{\textbf{Practitioner' Perception of Acceptable Test Coverage in ML Projects}}

In \textsc{Question \#3.4}, participants were asked about their perceptions of acceptable test coverage rates for ML projects. The survey results offer valuable insights into practitioners' expectations, highlighting how these expectations align with the challenges identified in the thematic analysis. Figure \ref{fig:q3_4_Acceptable_test_coverage_rate_for_an_ML_project} visualizes the participants' responses, offering a clear perspective on acceptable test coverage rates in ML projects. Additionally, in \textsc{Question \#3.5}, participants were asked whether the result found in our prior work \citep{bernardo2024machine}, which observed that medium-sized ML projects exhibit lower test coverage rates compared to non-ML projects aligns with their expectations and to explain why they believe this discrepancy exists. 
% The qualitative responses provide a deeper understanding of practitioners' perspectives, shedding light on the unique challenges that contribute to this trend.

\begin{figure}
	\centering
	\includegraphics[width=12cm]{q3_4_Acceptable_test_coverage_rate_for_an_ML_project.pdf}	
	\caption{Participants' perceived acceptable test coverage for ML projects.}	\label{fig:q3_4_Acceptable_test_coverage_rate_for_an_ML_project}       % Give a unique label
\end{figure}

The survey reveals that practitioners generally hold high expectations for test coverage in ML projects, with the majority indicating that acceptable coverage rates fall within the 70–80\% range (30.3\%), the 80–90\% range (16.8\%), or even the 90–100\% range (16.1\%). 
% These findings highlight a strong desire for comprehensive testing practices, reflecting the critical role of testing in ensuring the reliability and robustness of ML systems.
At the same time, a notable proportion of respondents viewed moderate test coverage rates as acceptable, with 13.5\% selecting 60–70\% and 6.5\% choosing 50–60\%. This reflects a pragmatic recognition of the constraints associated with ML testing. For instance, P7 explained, \textit{``Complete coverage would require resources and longer build times, so it has to be traded off''}. Similarly, P10 noted, \textit{``It's generally harder, more time-consuming, and costly to test ML projects, especially due to the intensive resource requirements and unstable results that come with ML projects compared to non-ML projects''}. These responses suggest that while practitioners aspire to high coverage, they are aware of the limitations imposed by ML-specific challenges.

Lower test coverage rates (i.e., below 50\%) were deemed acceptable by only a small fraction of participants. Just 1.9\% and 2.6\% of respondents viewed 20–30\% and 30–40\% coverage, respectively, as sufficient, while 5.8\% found 40–50\% coverage acceptable. 
The qualitative responses provide insights into why lower test coverage rates were acceptable. A key factor frequently mentioned was the inherent unpredictability of ML systems. As P136 noted, \textit{``It's not easy to write unit tests for ML projects since usually there's no deterministic expected results''}. 
% This observation underscores the challenge of designing tests that can effectively account for the wide range of possible behaviors exhibited by ML models, where outputs often vary unpredictably due to their stochastic and data-dependent nature.

Organizational and cultural factors play a significant role in shaping coverage expectations. Many participants noted that ML practitioners often prioritize experimentation over rigorous testing practices. As P113 explained, \textit{``ML engineers who write ML code are more focused on the science and algorithms, not very focused on software engineering practices such as coverage and fast-optimized builds''}. Another added, \textit{``ML projects often care more about the ability instead of bug-free code'' (P109)}. These perspectives highlight the experimental nature of ML projects, where the focus is often on achieving functional outcomes rather than building a comprehensive testing infrastructure.

Indifference to test coverage rates was reported by 5.2\% of participants. Several factors emerged from their responses to explain this perspective, including shorter project deadlines, a lower priority placed on testing, and the inherently experimental nature of many ML projects. These factors contribute to the challenges in maintaining high test coverage rates in ML workflows.
For example, P129 explained
% \sergio{No parágrafo acima já foi usado esse mesmo texto de P113. Melhor juntar os parágrafos ou citar outro participante.}, 
\textit{``One of the reasons [for indifference to test coverage in ML projects] - as this topic is hot, there are more incentives to go faster and not betters''}. Similarly, P109 emphasized the focus on functionality over quality, noting, \textit{``ML projects often care more about the ability instead of bug-free code''}. P26 remarked on the difficulty of testing in ML environments, stating, \textit{``Testing in ML is hard, and people will opt not to do it if they can''}.

Additionally, some participants questioned the relevance of test coverage as a metric in the context of ML projects. P30 commented, \textit{``I am not a fan of this metric for an ML project''}, while P36 highlighted the research-driven nature of many ML workflows, observing, \textit{``ML is a lot of RnD, so lots of code is scripting that isn’t as important to test. Also, AI scientists are less likely to care about good test coverage''}.
Finally, the fast-paced and evolving nature of the ML field was identified as another factor contributing to lower test coverage rates. P59 explained, \textit{``The industry is moving fast, so communities need to deploy new technologies and stay at the state-of-the-art model. I think this is good in some ways, but it leads to projects with lower test coverage''}. These insights illustrate the practical constraints and priorities that influence practitioners' attitudes toward test coverage in ML projects.

The findings from this study can be compared to our previous research \citep{bernardo2024machine}, which revealed that medium-sized ML projects tend to have lower test coverage rates (83\%) compared to non-ML projects (94\%). While most practitioners expect high test coverage, the thematic analysis highlights the unique challenges that medium-sized ML projects face in achieving these rates. Participants frequently attributed this discrepancy to factors such as resource constraints, stochastic behaviors, the complexity of ML pipelines, and the lack of well-defined testing practices tailored to ML systems.
% As P26 explained, \textit{“Testing in ML is hard, and people will opt not to do it if they can.”} Others highlighted the lack of well-defined testing practices for ML systems, with P21 observing, \textit{“ML engineers/data engineers/scientists are often more from a mathematical or scientific background instead of a software engineering background, which means they never learned the same rigorous best practices as someone from a software/computer science background.”}
% This disconnect between practitioners' expectations and the realities of test coverage in ML projects underscores the need for tailored strategies to address these unique challenges. 


\begin{highlightbox}
\textbf{Key Findings:}
\begin{itemize}
    \item \textbf{\textit{Unique Challenges of ML Testing:}} Participants frequently cited test complexity, the non-deterministic nature of ML systems, data dependency, and computational resource constraints as key factors that challenge testing in ML projects.

    % \item \textbf{\textit{Interconnected Challenges:}} The co-occurrence analysis revealed that key challenges like test complexity, reproducibility issues, and data dependencies often overlap, underscoring the need for holistic solutions in ML testing.

    \item \textbf{\textit{High but Pragmatic Coverage Expectations:}} While most practitioners (63\%) favored coverage rates of 70–100\%, a significant number (20\%) accepted moderate coverage (50–70\%), citing ML-specific constraints such as resource demands and the stochastic nature of ML algorithms.

    % \item \textit{\textbf{High Expectations for Test Coverage:}} Practitioners expressed a strong preference for comprehensive test coverage, with 30.3\% identifying 70–80\% as an acceptable range, followed by 16.8\% favoring 80–90\%, and 16.1\% targeting 90–100\% coverage.

    % \item \textbf{\textit{Pragmatic Acceptance of Moderate Coverage Rates:}} A notable proportion of practitioners, 13.5\%, considered 60–70\% coverage acceptable, while 6.5\% viewed 50–60\% as sufficient. These responses reflect an acknowledgment of the constraints imposed by ML-specific challenges, such as resource demands and the stochastic nature of ML algorithms.

    \item \textbf{\textit{Lower Coverage in Medium-Sized ML Projects:}} 
    Participants' feedback aligned with prior quantitative findings from our earlier study \citep{bernardo2024machine}, which showed that medium-sized ML projects have lower test coverage (83\%) compared to non-ML projects (94\%). This discrepancy was attributed to factors such as shorter deadlines, the experimental nature of ML projects, and limited emphasis on rigorous testing practices.
\end{itemize}
\end{highlightbox}