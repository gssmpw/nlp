\section{Limitations}
\label{sec:limitations}

% In this section, we discuss the limitations of our study.

% While the survey gathered 155 responses, the response rate was relatively low (3.1\%), which may limit the generalizability of the findings. 
We are aware that the self-selection bias -- 
 where individuals choose whether to participate in a study -- may have influenced our results.
Out of the 5,007 practitioners we contacted via email, we received 155 responses, yielding a 3.1\% response rate. As a result, practitioners who did not respond may hold different perspectives on the presented questions, potentially leading to different findings.

However, the respondents likely represent a subset of practitioners who are more experienced or particularly interested in CI practices for ML projects. For instance, one contributor from the \textit{dmlc/tvm} project replied to the survey invitation stating, \textit{``I do not have informed opinions on CI/ML practices''}, while another contributor from the \textit{huggingface/pytorch-pretrained-BERT} project responded, \textit{``Thank you for your interest; however, I don't have any valuable input. I have no experience with ML and \textsc{GitHub}'s CI and don't want to skew your data''}. 
While this self-selection may have influenced the composition of our sample, it aligns with our studyâ€™s goal of capturing insights from practitioners who have experience with CI in ML projects. Their perspectives provide valuable feedback on the key differences, challenges, and strategies for adopting CI in this context.

Furthermore, participants who did respond may have been affected by social desirability bias -- the tendency to answer questions in a way they believe will be viewed favorably by others. This could include providing responses perceived as ``correct'' or portraying their development practices more positively than they actually are. To mitigate these limitations and enhance the reliability of the findings, the study included a diverse set of ML projects across different build durations, ensuring representation from both high-performing and low-performing projects. Additionally, the qualitative analysis was conducted meticulously to identify recurring themes, reducing the impact of potential biases and strengthening the robustness of the results.

The study focuses on 47 ML projects, targeting those in the top and bottom quartiles for build durations. While this approach captures contrasting experiences and highlights key challenges, it may exclude insights from projects with average build durations, potentially narrowing the scope of findings. However, by emphasizing the extremes, the study provides a richer understanding of the factors influencing build durations. Future research could include projects with average durations to offer a more comprehensive perspective.

Additionally, the analysis of small ML projects is based on only two responses, which significantly limits the generalizability of conclusions for this group. Findings related to small projects are explicitly discussed with caution, acknowledging their limited reliability. These insights are framed as exploratory and are intended to provide a foundation for future investigations rather than definitive conclusions.

The study also focuses exclusively on ML projects using \textsc{GitHub Actions} as their CI platform. The challenges and insights identified may differ for projects using alternative CI tools or operating in varied organizational or technological contexts. For instance, some findings, such as resource and infrastructure challenges, may reflect specific limitations of \textsc{GitHub Actions} (e.g., GPU availability). A broader analysis that includes multiple CI platforms could uncover additional challenges and strategies to address them.

Finally, the thematic analysis employed in the study relies on the manual coding of open-ended survey responses, which is an inherently subjective process, introducing the potential for varying interpretations among authors. To mitigate this threat, two authors coded each open-ended survey question independently, and disagreements were resolved by a third author. Although we recognize that alternative interpretations of the data could exist, we believe this triangulation approach reduces individual biases and ensures consistency and reliability in the coding process.