\section{Background and Related Work}\label{s-background}
\textit{Educational technology}, or \textit{edtech}, consists of ``technologies specifically designed for educational use as well as general technologies that are widely used in educational settings'' \cite{cardona_artificial_2023}. Edtech need not be based on AI, or even on computing technology. For example, abaci (invented several millennia BCE), electronic calculators (invented in the twentieth century), and WolframAlpha\footnote{\url{https://www.wolframalpha.com/}} (launched in 2009) are all examples of technology that has been used to help students learn math. Nevertheless, recent advances in AI have sparked increased interest in building and using AI-powered edtech to improve learning outcomes and teaching processes, including within the HCI community \citep[e.g.,][]{zhang_mathemyths_2024, cheng_scientific_2024, leong_putting_2024, lee_dapie_2023, lu_readingquizmaker_2023}.\looseness=-1

\subsubsection*{Large Language Models.}\label{s-prior_taxonomy}
Among the most notable of these AI advances are LLMs, sometimes called \textit{foundation models} \cite{bommasani2022opportunities}, which are models trained on massive amounts of text data scraped from the internet to predict the most probable next token (e.g., word, part of a word) in a sequence of text \cite{radford_improving_2018}. By predicting multiple tokens in a sequence, they can create fluent-sounding text, and are increasingly used for natural language understanding and generation tasks. These capabilities bring significant risks as well, as outlined by \citet{bender_dangers_2021} and \citet{weidinger_taxonomy_2022} in two widely cited taxonomies. We draw on both works as a starting point to understand the potential harms that may arise from the use of LLMs in education, and synthesize the harms they identify in Table \ref{t-taxonomy}.\looseness=-1

\input{tables/taxonomy}

While these taxonomies provide comprehensive overviews of the potential harms broadly associated with the use of LLMs, they are domain agnostic. Therefore, we set out to place these risks in the context of LLM-based edtech by exploring how edtech providers and educators are anticipating, measuring, and mitigating harms. In doing so, we draw on a rich collection of work related to understanding potential and actual harms resulting from the use of AI in education.\looseness=-1

\subsubsection*{AI in Edtech.} 
A survey of AI in education (AIED) researchers by \citet{holmes_ethics_2022} identified a variety of ethical concerns related to the use of AI in education, including data privacy, quality of education provided by AI tools, teacher and student agency, and equity in AI-based decision-making processes. More recently, in a systematic review of research into the use of LLMs in education, \citet{yan_2024_practical} identified similar concerns as well as inequality in the form of an outsize focus on the English language in existing research. Similarly, \citet{lee2024life} mapped potential sources of bias stemming from each step in the `life cycle' of an LLM. Many of the issues highlighted by prior work have already been observed in practice: AI-based edtech has been shown to discriminate on the basis of race, gender, disability status, and other factors \cite{baker_algorithmic_2022}; impinge on students' privacy and autonomy \cite{diberardino_anti-intentional_2023, commonsense2023}; and provide inaccurate instruction in tutoring settings \cite{wsj_khan}.\looseness=-1

Nevertheless, educators and AIED researchers have good reason to explore the use of AI in education. In the face of pandemic learning loss and the looming expiration of pandemic relief funds \cite{esser, pandemic_recovery}, AI tools are touted as a relatively inexpensive way to meet learners where they are rather than providing the same lessons or assignments to students with different background knowledge or learning needs \cite{cardona_artificial_2023, dai_lin_jin_li_tsai_gasevic_chen_2023, MEYER2024100199}. AI tools also have the potential to make teachers' jobs easier; for example, by providing feedback and support to students outside of teachers' working hours or handling administrative and other non-instruction responsibilities, such as lesson plan development\footnote{See Microsoft Research India's Shiksha copilot \cite{ms_india}.} and grading\footnote{Writable: \url{https://www.writable.com/}} \cite{cardona_artificial_2023}. \looseness=-1

In this uncertain landscape, government agencies \cite{cardona_artificial_2023, noauthor_guidance_2023}, NGOs \cite{noauthor_ethical_2021}, and researchers \cite{KASNECI2023102274, williamson_time_2024} have put forward frameworks intended to guide the responsible development and adoption of AI-based edtech. The US Department of Education (DOE) \cite{cardona_artificial_2023}, for example, has emphasized the importance of centering `humans-in-the-loop'; designing AI tools to adhere to evidence-based pedagogies; and ensuring that AI tools preserve privacy, are explainable, and do not discriminate. In a framework aimed at the procurers of AI-based edtech, the Institute for Ethical AI in Education \cite{noauthor_ethical_2021} identified a similar set of principles, additionally including that AI-based tools do not hinder learners' autonomy and are only deployed to well-informed participants. While helpful, these frameworks are not specific to LLMs and thus do not address in detail several of the risks raised by \citet{bender_dangers_2021} and \citet{weidinger_taxonomy_2022}, such as the potential for LLM-based edtech tools to hallucinate or contribute to academic dishonesty. Other frameworks that specifically consider LLM-based edtech acknowledge the risks of ``unknown unknowns'' \cite{KASNECI2023102274} associated with the technology and call for a deeper examination of ``uncharted ethical issues'' related to access, equity, and human social connection and intellectual development \cite{noauthor_guidance_2023}. Most recently, \citet{williamson_time_2024} have called for a pause on the adoption of LLM-based edtech in schools until policymakers can develop deeper understandings of its risks and until `responsible AI frameworks' are in place for the design and development of future edtech tools. \looseness=-1

In response to these challenges, new guidance from the US DOE has provided a potential path forward for edtech designers and developers seeking to implement these responsible AI frameworks \cite{cardona_designing_2024}. The DOE's report puts forward the ideal of ``designing for education,'' which involves edtech providers and educators engaging in a \textit{co-design process} that uses evidence-based practices to improve teaching and learning. Importantly, the DOE's report highlights the need to \textit{build trust} between edtech providers and educators as a crucial first step in designing for education \cite{cardona_designing_2024}. In our work, we seek to facilitate this trust-building by providing a transparent understanding of how both edtech providers and educators are anticipating, observing, and accounting for potential harms from LLM-based edtech, creating an opportunity for both groups to understand each others' viewpoints and pointing to gaps in current harm mitigation practices. Ultimately, our hope is that this can facilitate the \textit{centering of educators} in the future design and development of edtech tools, and serve as a foundation upon which user-centered and co-design research can build.\looseness=-1