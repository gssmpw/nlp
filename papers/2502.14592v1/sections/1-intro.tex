\section{Introduction}
Education technology (edtech) companies are increasingly developing products that use large language models (LLMs) to provide personalized tutoring,\footnote{\url{https://www.khanacademy.org/khan-labs}} create instructional materials,\footnote{\url{https://web.diffit.me/}} teach foreign languages,\footnote{\url{https://blog.duolingo.com/duolingo-max}} provide automated writing feedback,\footnote{\url{https://www.grammarly.com/ai}} and more. Many of these tools are already being used in classrooms even though early research on LLMs has revealed myriad risks, including that they can generate content that is toxic, biased, or simply incorrect \cite{bender_dangers_2021, weidinger_taxonomy_2022, lee2024life}. The downstream impacts of LLM use in education remain understudied, and early research has presented conflicting results on whether LLM-based edtech improves or harms student engagement and learning outcomes \cite{nie_gpt_2024, bastani_generative_2024, lo_engagement_2024, pardos_chatgpt_2024}. \looseness=-1

Educational applications of AI are unique in many ways, requiring edtech designers and developers to understand and account for \textit{domain-specific harms} in their products. Much AI research, including responsible AI research, focuses on predicting uncertain outcomes (e.g., disease diagnosis \cite{Hosny2018-ph, obermeyer_dissecting_2019}) or automating complex tasks (e.g., driving \cite{Yurtsever_2020_Survey, ijcai2017p654}). While prediction can certainly be a component of edtech (e.g., Intelligent Tutoring Systems predict at what point a student has mastered a specific knowledge component to move them on to the next one \cite{feng_can_2008}), education itself is not a prediction problem: its aim is to facilitate learning -- something that is neither automatable nor even directly observable, especially for learning objectives related to critical thinking and social skills \cite{MERCER1996359, schuller_benefits_2005, ERIC_survey}. Further, K-12 education involves working with children, which brings with it a new set of privacy and other ethical considerations \cite{Einarsdottir_2007_research}. Taken together, these unique elements of education imply that research into potential harms from LLM-based edtech tools, like prior work on algorithmic fairness \cite{kizilcec_algorithmic_2022, harvey2024towards} and explainable AI \cite{khosravi_explainable_2022}, must be \textit{tailored to the domain of education} in order to have practical value. Finally, LLM-based edtech does not exist in a vacuum -- it is already being deployed in classrooms where educators and students directly engage with it. To understand potential harms from LLM-based edtech, then, it is crucial to draw on HCI research that considers the implications of these \textit{interactions}, following prior work that has explored how students and educators adopt \cite{ibrahim_understanding_2022, kizilcec2024perceived, viberg2024explains}, understand \cite{tan_more_2024, han_teachers_2024}, and use \cite{park_promise_2024, belghith_testing_2024} LLM-based edtech.
\looseness=-1

\subsubsection*{Contributions.} In this context, we explore two key gaps in understanding the potential harms arising from the use of LLMs in edtech. Through a series of semi-structured interviews with edtech providers (N=6) and educators (N=23), we surface an \textbf{education-specific overview of harms} that edtech providers and educators are currently anticipating, observing, or actively working to mitigate. In addition, we highlight \textbf{gaps between conceptions of harm by edtech providers and those by educators} in order to facilitate the \textit{centering of educators} in the design and development of edtech tools. Critically, these gaps come down to a mismatch in the salience of HCI-related factors: while edtech providers focus primarily on harms that can be measured based solely on LLM outputs themselves, educators are more concerned about harms that require observation of interactions between students, educators, school systems, and edtech to measure.
\looseness=-1

Concretely, we identify potential harms arising from the use of LLMs in edtech across three general categories. \textit{Technical} harms include the generation of toxic or biased content, privacy violations, and hallucinations.\footnote{By a \textit{hallucination}, we mean the generation of ``text that is nonsensical, or unfaithful to the provided source input'' \cite{ji_survey_2023}.} Harms arising from \textit{human-LLM interaction} include academic dishonesty. Finally, harms arising from the \textit{broader impact} of LLMs include inhibiting student learning and social development, increasing educator workload while decreasing educator autonomy, and exacerbating systemic inequalities in education. We find that edtech providers focus primarily on mitigating technical harms -- but that these are the same harms that educators report feeling most equipped to address through their teaching practices. On the other hand, educators report high levels of concern about harms resulting from the \textit{broader impacts} of LLMs, most of which edtech providers currently cannot measure or mitigate. In light of this, we identify \textbf{opportunities for edtech providers to better mitigate potential harms through their design practices}. We also identify \textbf{cases where we believe that potential harms from LLM-based edtech cannot be mitigated through design practices alone} and where responsibility thus falls to other actors, including school leaders, regulators, and researchers.\looseness=-1

In \S\ref{s-background}, we provide an overview of the recent shift towards AI-based edtech along with the frameworks that have been proposed to guide its use. We also outline existing domain-agnostic taxonomies of risks from LLMs, and highlight gaps in these taxonomies that prevent them from being directly applicable to educational contexts. Next, in \S\ref{s-method}, we describe our approach to conducting interviews and a thematic analysis. We outline findings from our interviews with edtech providers in \S\ref{s-results-edtech}, and from our interviews with educators in \S\ref{s-results-educators}. Finally, we close by providing recommendations to guide the design and development of educator-centered edtech in \S\ref{s-discussion}. \looseness=-1
