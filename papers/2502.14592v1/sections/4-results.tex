\section{How Edtech Providers Account for LLM Harms}\label{s-results-edtech}

The edtech providers we interviewed were aware of and actively working to mitigate technical harms resulting from the use of LLMs in edtech (\S\ref{s-results-technical-harms-edtech}); namely, toxic or biased content, privacy violations, and hallucinations. Only a few edtech providers reported considering harms resulting from human-LLM interactions as they developed tools (\S\ref{s-results-interaction-harms-edtech}), but most reported working to mitigate the potential for LLMs to inhibit student learning (\S\ref{s-results-broader-harms-edtech}). 
\looseness=-1

\subsection{Technical Harms}\label{s-results-technical-harms-edtech}
\subsubsection*{Mitigation Approaches.} All of the edtech providers we interviewed were aware of and attempting to account for the technical LLM harms identified by \citet{bender_dangers_2021} and \citet{weidinger_taxonomy_2022}. Edtech providers reported relying on \textit{human oversight} to measure and mitigate harms, including by internal moderation teams who reviewed content flagged (by toxicity detectors or by users) as being harmful (T1, T5, T6) and by educators who were given access to the chat logs of their students for manual review (T1, T5). Furthermore, multiple providers relied on \textit{technical guardrails}, including LLM-based toxicity detectors (T1, T2, T5, T6), filtering out potential personal information (T2, T4, T5), and using external models or methods like retrieval-augmented generation (RAG) \cite{NEURIPS2020_6b493230} to validate the correctness of LLM outputs (T1, T2). Reliance on pre-existing toxicity detectors like Perspective API\footnote{\url{https://perspectiveapi.com/}} meant that most edtech providers defined toxicity in the same way as those tools: as content that is openly insulting, demeaning, threatening, violent, or sexual; or contains profanity. Edtech providers distinguished \textit{toxic} content from \textit{biased} content (e.g., subtle stereotyping), and generally expressed less certainty about how to mitigate bias as opposed to toxicity, although several edtech providers reported using prompt engineering to explicitly instruct their LLM-based tools to avoid biased responses (T1, T5). \looseness=-1 

\subsubsection*{Open Challenge: Inherently Stochastic Outputs} Importantly, most of the edtech providers we interviewed acknowledged that, due to the inherently stochastic nature of LLMs, their mitigations could not guarantee LLM outputs that were free of toxic or biased content, privacy violations, or hallucinations (T1--T4, T6). Even this small chance of harm was too risky for some participants: ``I only need one student or one teacher to tweet about how [the product] told something very, very inappropriate to a student, and that's it, really'' (T4). Therefore, a key approach to mitigating all of these harms was also to \textit{limit the use of LLMs in some way}; for example, by using LLMs to generate static, human-reviewed content only (T1, T3, T4, T6); by restricting access to LLM agents to only adult users of a tool (T4); and by limiting the context window of live chats to reduce the likelihood of unexpected LLM responses (T1, T3).
\looseness=-1

\subsubsection*{Open Challenge: Dependence on OpenAI}
Finally, several participants discussed challenges related to the ownership of the most powerful LLMs by a small set of companies, most notably OpenAI (T2, T4, T5). The fact that they do not have control over a crucial component of their edtech platforms means that developers must repeatedly test and re-test their prompts and guardrails to ensure that changes to the underlying LLM have not changed their outputs (T2). It also means that tactics like fine-tuning are out of reach for edtech developers who lack the time and resources to re-run their fine-tuning process after each LLM update (T4, T5). \looseness=-1

\subsection{Harms From Human-LLM Interactions}\label{s-results-interaction-harms-edtech}
While the edtech providers we interviewed are working to measure and mitigate technical harms from LLMs, most did not report exploring harms arising from interactions between humans and LLMs. Both of those who did reported using \textit{technical guardrails} to limit the potential for academic dishonesty on their platforms. T1 discussed designing a gate agent to reject adversarial prompts, such as those designed to elicit answers instead of advice from a chatbot. T5 discussed implementing guardrails similar to lockdown browsers to prevent LLMs from being used during assessments.\looseness=-1

\subsection{Harms From the Broader Impacts of LLMs}\label{s-results-broader-harms-edtech}
\subsubsection*{Inhibiting Student Learning} 
Almost every edtech provider we interviewed proactively raised the importance of measuring whether their tool is helpful to student learning (T1--T4, T6). Although there was no consensus on how to best do this, several themes emerged. Collecting and analyzing \textit{user feedback} was the most common approach, with some providers collecting general ratings from all users (T3, T4) and others collecting more detailed reports from small pilot studies (T1, T2). One participant also planned to \textit{compare academic performance} through A/B tests between students who interact with their tool's chatbot and students who interact with older, static versions of their tool as well as pre- and post-performance assessments for students who interact with the chatbot (T1). Another reported measuring helpfulness in a more conceptual way: by performing detailed \textit{risk-benefit analyses} on potential use cases for LLMs before integrating them into the tool (T6). While one participant discussed collecting data from student users in order to verify that helpfulness did not vary across demographic groups (T2), others reported deliberately limiting their access to student data for fear of being seen as `creepy' (T3, T5).\looseness=-1

\subsubsection*{Open Challenge: Adherence to a Pedagogy}
Ensuring that LLM-based edtech tools are contributing to student learning is challenging if LLMs cannot be made to adhere to evidence-based teaching approaches. Multiple participants proactively raised this issue of pedagogy (T1, T4). In particular, when research suggests ``exactly which words to use where and how to present certain material,'' LLM-based tools can provide outputs that deviate from the optimal and recreate ``the average of the internet instructional style'' (T1) -- an issue that the edtech providers we interviewed are still struggling to account for.\looseness=-1

\section{How Educators Account for LLM Harms}\label{s-results-educators}
Like edtech providers, most of the educators we interviewed reported being aware of and working to mitigate technical harms from LLMs, including toxic or biased content, privacy violations, and hallucinations (\S\ref{s-results-technical-harms-educators}). While some educators argued that the potential for LLMs to generate false or biased content rendered them inherently unsafe for use in edtech, most felt confident in their ability to minimize the impact of these harms by \textit{mediating student use of LLMs}. As a result, educators were most concerned about technical harms occurring in cases when LLMs disrupted the teacher-student relationship, as this reduced their ability to personally mitigate harms. Educators felt similarly about harms from human-LLM interaction: although they had high awareness of the potential for LLMs to be used for the purposes of academic dishonesty, they generally reported feeling equipped to identify and address unauthorized use of LLMs by students (\S\ref{s-results-interaction-harms-educators}).\looseness=-1

Educators expressed far less certainty, however, about the broader impacts of LLMs on education (\S\ref{s-results-broader-harms-educators}). This gap between concerns raised by edtech providers and those raised by educators is summarized in Table~\ref{t-gaps}. Specific concerns proactively raised by educators included that LLMs could inhibit students' learning and social development, increase educator workload while decreasing educator autonomy, and exacerbate systemic inequalities in education. Some of these concerns mirror what edtech providers pointed to as their biggest open questions, i.e., issues that they are aware of, but cannot currently measure or mitigate -- and others represent potential harms that edtech providers may not be well-positioned to mitigate, and that may fall to school leaders, regulators, or researchers to address instead.

Again, we note that during interviews, we explicitly prompted all participants to share their level of concern about the potential for LLM-based edtech to produce toxic or biased content, privacy violations, and hallucinations, as well as the potential for misuse\footnote{We used the example of academic dishonesty when prompting about malicious use.} and socioeconomic harms\footnote{We used the example that students from different backgrounds may be differentially-equipped to make use of LLM-based edtech when prompting about socioeconomic harms. However, participants expressed low concern about this particular example and instead proactively raised the other harms that we discuss in \S\ref{s-results-broader-harms-educators}.} if they did not proactively raise these harms. However, we did not prompt participants to discuss any other harms stemming from the broader impact of LLMs: inhibiting student learning and social development, increasing educator workload while decreasing educator autonomy, and exacerbating systemic inequalities in education. All of those harms were instead proactively raised by multiple educators. Therefore, when talking about technical harms and human-LLM interaction harms, we focus on the \textit{level of concern} shared by participants (since all participants were asked about those harms) and when talking about harms from the broader impacts of LLMs we focus on the \textit{salience} to participants (since we only discussed those harms with participants who proactively raised them).\looseness=-1

\input{tables/gaps_surfaced}

\subsection{Technical Harms}\label{s-results-technical-harms-educators}

\subsubsection*{Toxic Content, Stereotyping, and Bias.}

\begin{quote}
``It was kind of upsetting and confusing, but I saw that happening before I assigned this to [the] class. So we had some debriefing questions at the end, trying to make sense of why that was being the way it was being, the biases that were built in and melded in.'' -- \textit{E1, describing a situation in which an AI tool failed to generate content about non-white people}
\end{quote}

\noindent Educators reported mixed levels of concern about the potential for LLMs used in edtech to generate toxic or biased content (E1, E3-E5, E7, E9, E18, and E19 reported higher levels of concern). Most educators found it deeply unlikely that an LLM used in edtech would generate outright toxic content -- perhaps indicating that edtech providers' mitigation efforts in this regard have been successful. On the other hand, multiple educators had personal experience with biased content, including the generation of ``Eurocentric'' outputs (E1, E13, E18) and negative stereotypes about Black children (E3). While E3 and E18 chose to limit the use of LLMs in their classrooms as a result, E1 and E13, as well as educators who did not report directly experiencing biased outputs (E2, E10), expressed confidence in their abilities to \textit{mediate students' interaction} with these outputs, often by creating `teachable moments' to address bias directly. However, educators' concerns about biased content intensified as they considered situations in which they were not able to serve as intermediaries between students and LLMs, as expressed by E7: ``It's hard enough for me as an individual to stay out of those pitfalls\ldots And when I step into one, I apologize and keep it moving. I can't even fathom having to plan for a computer program making those mistakes\ldots And then I can’t correct that either.''\looseness=-1 

\subsubsection*{Privacy Violations.}
\begin{quote}
    ``Am I concerned about ChatGPT and privacy for children? Yeah. But I'm also concerned about the same thing for them using Facebook\ldots And ultimately that has to be legislated on. I'm not gonna continue worrying about it, because I literally can't do anything about it. But what I can do is, I make every outside vendor that we work with sign a data privacy agreement.'' -- \textit{E13}
\end{quote}

\noindent Educators reported mixed levels of concern about privacy violations resulting from the use of LLMs in edtech (E1, E4-E6, E9, E11, E13, E17, E18, and E20 reported higher levels of concern). Even educators who reported higher levels of concern felt that they could mitigate their concerns by \textit{teaching students how to safely use LLMs without disclosing personal data} (E1, E11, E18). In fact, the most pressing privacy-related concern raised by educators is that students might confide personal details related to abuse or other forms of harm to a chatbot \textit{instead of to an educator} who would be equipped to intervene (E4, E17). In other words, the educators we interviewed are less worried about the potential of LLMs to collect sensitive information about students than they are about the potential that LLMs might prevent educators from accessing that information.\looseness=-1 

\subsubsection*{Hallucinations.}
\begin{quote}
    ``It mitigates a lot of the work that I'll do in the classroom, because then [students] go home, they see this [misconception on ChatGPT], and especially when you're young and a kid, you're like, ‘oh, well, the computer said it.''' -- \textit{E16}
\end{quote}

\noindent The educators we interviewed had more direct experience with hallucinations than with other forms of harm: many educators reported personally noticing a hallucination in LLM outputs (E1-E3, E5, E8, E11, E15, E16, E20, E21). However, personal experience with hallucinations did not seem to translate directly to having a higher level of concern: educators reported mixed concerns overall (E2-E4, E7-E9, E15-E17, E21, and E23 reported higher levels of concern). Hallucination was also the area where educators had the most concrete ideas of how to mitigate harms. Many educators reported that they limited the risk of hallucinations in the first place by only using LLMs to generate static content that they were able to \textit{review before it reached students} (``because we come in assuming that there's going to be some errors, we always double check,'' E18). Educators also reported allowing their students to use LLMs directly, but reducing the impact of harms by \textit{teaching students how to fact-check LLM outputs} (``it's just a new information landscape that we need to train folks on,'' E23). Educators expressed the most concern about situations where students used LLMs without their oversight, i.e., in cases where educators would not be available to correct false information (E7, E16).\looseness=-1

\subsection{Harms From Human-LLM Interactions}\label{s-results-interaction-harms-educators}

\subsubsection*{Academic Dishonesty.}

\begin{quote}
    ``I recently had a teacher [say], `Alice submitted her whole thing, it was all AI generated, I don't know what to do.'\ldots I spoke with her, was like, `your teacher ran this through some AI detection, it came up 98\% AI-generated.' And she's like, `I didn't use AI. I wrote it in Chinese. Google Translate translated it. I put it through Grammarly. My friend corrected it. Then I went back, and --' And then I did a little bit of digging because they wanted to kick her out of the program\ldots and I learned that people are now putting their dissertation from 1994 through AI detection -- it’s like 100\%.'' -- \textit{E5}
\end{quote}

\noindent The educators we interviewed identified one harm arising from human-LLM interactions as particularly relevant in edtech: academic dishonesty (proactively raised as a potential concern by most educators). Although educators had a high \textit{awareness} of the potential for LLMs to be used for academic dishonesty, educators reported mixed levels of \textit{concern} (E2-E6, E10, E15, E17, E19, and E23 reported higher levels of concern). As with the technical harms, this was generally due to the confidence that educators had in their ability to mitigate this risk for their students. For example, educators were more likely to report that they relied on their \textit{intuitions} to identify student use of LLMs (E1, E5, E7, E8, E9, E14, E16, E17, E22, E23) as compared to using \textit{external AI detectors} like Turnitin\footnote{\url{https://www.turnitin.com/products/originality/}} (E2, E3, E8). This was due both to educators' concerns that AI detectors are fallible (E5, see quote above) and to educators' desires for flexibility in how they addressed issues of potential academic dishonesty (``I've developed relationships with these kids\ldots if something comes out on their own accord instead of me blatantly calling the whatever-you-call-it-police, I'll have more time to cultivate that discussion appropriately,'' E4; also E5, E10, E15, E16, E18, E23). Other educators reported \textit{changing their teaching practices} to account for AI, asking ``if a computer can do the task that you assigned, is it the most meaningful task?'' (E21; also E3, E13, E17). Overall, while the educators we interviewed are aware that academic dishonesty is a risk of LLMs, most felt confident in their abilities to mitigate that risk through their teaching practices and interactions with their students.\looseness=-1


\subsection{Harms From the Broader Impacts of LLMs}\label{s-results-broader-harms-educators}

\subsubsection*{Inhibiting Student Learning.}

\begin{quote}
    ``I've noticed that as students become more tech aware, they also tend to lose that critical thinking skill. Because they can just ask for answers.'' -- \textit{E18}
\end{quote}

\noindent The concern that LLMs could inhibit students' learning, especially their critical thinking skills and the development of their own unique `voice,' was proactively raised by educators (E2, E3, E5-E8, E10, E11, E15-E21). While a few educators reported trying to mitigate this issue by \textit{providing opportunities for students to critique LLM outputs} or \textit{consider alternate solutions to those proposed by LLMs during classroom instruction} (E5, E11, E15), most educators either \textit{limited the use of LLMs} in their classroom as a result of these concerns (E3, E7, E8, E10, E16, E17, E18, E19, E20, E21) or expressed that they did not know how to best address them (E2, E6).\looseness=-1 

The edtech providers we interviewed reported trying to address this concern by measuring the `helpfulness' of their tools; however, there is currently no consensus on exactly what `helpfulness' means and how it can be quantified (\S\ref{s-results-broader-harms-edtech}). In addition, like edtech providers, educators raised the issue of pedagogy (E2, E10, E13, E21). Specifically, educators expressed frustration that the LLM-based tools that they have used seem unable to draw vocabulary, teaching processes, and other forms of information from the evidence-based curricula on which they rely in their classrooms. They stressed that this issue can undermine the best practices of educators and confuse students, ultimately lowering overall educational quality even in cases where other harms do not occur.\looseness=-1


\subsubsection*{Inhibiting Student Social Development.}

\begin{quote}
    ``It’s the day and age of the cell phone. So with the AI, I feel like instead of working in groups---cause a lot of kids don't like working in groups---they’ll work with the AI and get the answers. So, human interaction, I think, is going to be cut in half.'' -- \textit{E14}
\end{quote}

\noindent The potential for social harms to students from LLMs was proactively raised by educators (E6, E7, E9, E14, E17-E19). Educators described trying to mitigate this harm by \textit{limiting the use of LLMs} in the classroom, and especially by not assigning LLM-based games or chatbots as part of homework assignments (``Kids now have ways to stay up all hours of the night. And there's nothing we can do. But we don't need to then add to that,'' E7). In general, educators worried that increased reliance on LLMs at school and in daily life would worsen feelings of social isolation in students that educators initially saw emerge as a result of social media and cell phone usage (E14, E19), and that were exacerbated during Covid lockdowns (E6, E7, E9). Educators also expressed worry that overreliance on LLMs could erode relationships and trust between students and teachers (``especially working in juvie, if you don't make that personal connection with your students, they'll shut you out for the rest of your life, because they don't trust you anymore,'' E18, also E17). \looseness=-1 

\subsubsection*{Increasing Educator Workload.}
\begin{quote}
     ``I don't know what's gonna happen to these [AI tools] and I don't know where to invest my own research because I don't have time to look at all of them.'' -- \textit{E5}
\end{quote}

\noindent Educators proactively raised that they felt obligated to spend a significant amount of their professional and personal time researching LLM-based edtech (E1-E6, E8, E9, E12-E15, E18-E21, E23). This included vetting LLM-based tools to determine whether they meet classroom needs (E5, E13, E21), or spending time to learn how to mitigate harms from tools that had been designed and released without their input (E19). Multiple educators also reported attending professional development (PD) sessions (E3, E13, E14, E21, E23) and external courses (E5, E14, E18, E23), or conducting personal or district-sponsored research (E1, E5, E6, E8, E9, E12, E13, E23) in order to better understand the LLM ecosystem. However, the quality of these resources can vary -- many PD sessions are sponsored by edtech providers themselves, and one educator reported receiving inaccurate information when they took an external course (``With the class I’m taking, the professor, the way they talk about AI, it’s like it's foolproof. It’s everything. That's the impression I got. It can't make any mistakes,'' E14).
\looseness=-1 

\subsubsection*{Decreasing Educator Autonomy.}
\begin{quote}
    ``Teachers have to be on board. And teachers’ input about their students is supremely important in creating these tools. Because if a teacher isn’t bought in, they’re not gonna use it with fidelity\ldots So yes, I get principal buy-in and support is important. But don't forget the teachers\ldots The teachers are supremely important in making all of this happen. So listen to them.'' -- \textit{E7}
\end{quote}

\noindent Educators proactively raised the concern that LLM-based edtech tools are not necessarily designed or procured with educator input, or even with concrete educational goals in mind (``I don't know that anybody thought about education when any of these tools came out,'' E1; also E2, E7, E10, E12, E13, E16, E17, E19, E21, E22). In response to the challenges associated with vetting edtech tools, some educators suggested that their time would be better spent providing input into tools directly: ``I would like to give suggestions to what type of responses would be shared, and [help] create something'' (E17). Additionally, multiple educators reported feeling left out of school- or district-wide decisions on LLM procurement in ways that decreased the level of autonomy they felt they had in their classrooms. For example, educators' options are limited when technology vetting occurs at the school, district, or even state level (``Now we're being forced into more bureaucratic apps like Schoolology or ParentSquare and Google Classroom\ldots But now we can't use any of the ones that the kids enjoyed because they weren't vetted,'' E6). At the same time, educators also felt restricted by a lack of clear guidance from their leadership (``The district doesn't know what to do with [generative AI]. So I also have to be careful I don't go too far with it, and then get myself in trouble,'' E8). \looseness=-1 

\subsubsection*{Exacerbating Systemic Inequality.}
\begin{quote}
    ``A lot of these tools that are designed and aimed at teachers -- understandably, they're trying to monetize them. A lot of them are doing the thing where they'll only work at a district level, or they only get a school license. So again, this becomes to me an equity issue\ldots It's hard to feel like it's equitable, or it's gonna be used for public good if it's only available if your district can pony up for it\ldots And I feel like it's always gonna be this income-level gap. Well-off districts are gonna pay for this stuff and struggling districts aren't. And those are the kids that are behind.'' -- \textit{E9}
\end{quote}

\noindent Finally, educators proactively raised the concern that LLM-based edtech could exacerbate systemic inequality by imposing costs that schools and districts are differentially able to bear (E1, E5, E6, E9, E13). Some educators worried about their schools' abilities to shoulder the financial burdens of edtech tools (``it costs a lot of money to get [a tool] vetted by the state,'' E6), while others criticized the system more broadly (``I went to a [professional development session], and it was about how much money edtech platforms make. They actually make more money than curriculum providers, because every educator thinks that they need the curriculum and all these bells and whistles,'' E13).\looseness=-1