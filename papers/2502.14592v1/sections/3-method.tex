\section{Method}\label{s-method}

\input{tables/interviewees}

In order to understand how edtech providers and educators are currently anticipating, measuring, and mitigating harms from LLM-based edtech, we conducted 29 semi-structured interviews \cite{Creswell2011} between November 2023 and February 2024. We recruited an initial set of participants via our professional networks and educator listservs. Because STEM teachers were over-represented in our initial recruitment, we recruited additional educators via snowball sampling in order to increase the diversity of participants in terms of their professional roles and academic subjects \cite{morgan2008snowball}. Interviews were between 30 and 60 minutes long and were conducted and recorded via Zoom; participants provided informed consent prior to the start of the recording. We provided all educators interviewed with \$50 compensation for their time.\footnote{We did not provide compensation to edtech providers because, although we recruited educators through listservs and snowball sampling, we recruited edtech providers primarily through a pre-existing research consortium where research participation by both academics and edtech providers is the norm.} Our study protocol was reviewed and deemed exempt by our institution's Institutional Review Board (IRB).\looseness=-1

\subsubsection*{Participants: Edtech Providers.} First, to understand the harms that are actively being anticipated and accounted for in the design and development of edtech tools, we interviewed six individuals employed by distinct edtech providers, identified throughout by IDs T1-T6. These providers represent international leaders in edtech, and each product reports at least 10,000 active student users. The LLM-based functionality that was being developed---or was already deployed---within each edtech tool included: supporting live chat or live feedback,\footnote{\textit{Live feedback} is a short interaction in which an LLM responds to a piece of user-submitted content but does not engage in a longer conversation.} pre-generating content, and creating reports for educators or tool moderators. All participants have research backgrounds and hold roles on their products' leadership or development teams. For-profit, non-profit, general-purpose, and STEM-focused edtech providers are represented in our interview pool. While participants are mostly white and mostly report prior positive opinions on LLMs, we believe that this roughly matches employment trends in the larger tech industry \cite{diversity_tech}. A summary description of the edtech providers we interviewed is provided in Table \ref{t-interviewees-edtech}, and a fuller description of individual edtech providers is provided in Appendix \ref{a-interviewee-characteristics}.\looseness=-1 

Participants are employed at leading edtech providers which are delivering learning materials, assessments, and feedback to millions of students in the US and the UK. However, we are careful not to draw broad generalizations about the universe of edtech providers in this work as our sample comprises only six edtech employees, a pool that is too small to be representative of the large and diverse edtech market. We discuss this in more detail when we consider limitations of our research in \S\ref{s-limitations}. Overall, however, we believe that the practices our interviewees describe are relevant, as they represent the steps that widely used and well-regarded edtech providers take to measure and mitigate harms from LLMs in edtech.\looseness=-1

\subsubsection*{Participants: Educators.} Next, to develop an educator-centered understanding of gaps in existing frameworks for understanding LLM harms in edtech, we interviewed 23 educators, including teachers, instructional support staff, guidance counselors, and school administrators, identified throughout by IDs E1-E23. The educators we spoke with varied in their prior experience with LLMs, including no, limited, regular, and significant\footnote{\textit{Significant use} is defined as use of advanced features, such as creating custom chatbots.} prior use. A summary description of the educators we interviewed is provided in Table \ref{t-interviewees-educator}, and a fuller description of individual educators is provided in Appendix \ref{a-interviewee-characteristics}.\looseness=-1

As with edtech providers, we note that we were not able to recruit a representative sample of educators, and instead prioritized recruiting a \textit{diverse} group in terms of both backgrounds and demographics. As a result, certain backgrounds and perspectives are over-represented in the pool of educators we interviewed. For example, educators who have prior experience with LLMs and educators with a positive opinion of LLMs make up the majority of our sample.\footnote{However, an increasing share of educators within the US broadly have used LLMs: a nationally representative survey conducted by the Center for Democracy \& Technology concurrently with our interviews found that 83\% of educators reported having at least limited personal experience with an LLM, compared with 96\% of our sample \cite{dwyer_up_2024}.} While certain professional and demographic backgrounds are over-represented in our interview pool (white people, men, STEM teachers), we sought to mitigate this by relying on snowball sampling, in which we asked participants to refer us to colleagues of theirs who they believed would have different perspectives from their own \cite{morgan2008snowball}.\looseness=-1

\subsubsection*{Interviews.} 
Participants were first asked to describe their professional roles as well as their personal experience with and opinions (`positive,' `mixed,' or `negative') of LLMs. Participants were then asked an open-ended question about what concerns, if any, they had related to the use of LLMs in edtech (providers were asked about the products that they developed; educators were asked about tools they had used in their classrooms). We used these initial open-ended questions to develop an understanding of the harms that were \textit{most salient} to participants. However, we also specifically wanted to understand whether and to what extent participants were concerned about the set of domain-agnostic harms outlined in Table~\ref{t-taxonomy}. There are two reasons that we chose to ground our interview questions in this set of harms specifically. First, the research that introduces these harms \cite{bender_dangers_2021, weidinger_taxonomy_2022} is widely cited throughout the academic community and represents, to our knowledge, the most common framework for how academics think about potential harms from LLM-based systems. Second, from informal discussions with edtech providers preceeding our interviews, we know that these frameworks also guide how some \textit{edtech providers} themselves think about potential harms from LLMs. Therefore, our goal was to first identify the harms that were most salient to participants (through open-ended questions) and then specifically probe how well widely used domain-agnostic frameworks correspond to education-specific concerns. In order to thoroughly probe opinions on these potential harms, we provided additional prompting if a participant did not proactively raise a potential domain-agnostic harm (e.g., ``are you concerned about toxic or biased content/privacy violations/hallucinations/malicious use/socioeconomic harms?''). Participants were then asked whether and how they had attempted to measure or mitigate any harms that they were concerned about. Once again, the initial question was open-ended and we provided additional prompting if necessary (e.g., for edtech providers, ``have you engaged in red-teaming?'' or for educators, ``have you banned the use of LLMs in your classroom?''). Finally, we asked participants about their future plans for using, and mitigating harm from, LLM-based edtech. Our semi-structured interview guides are available in Appendix \ref{a-interview-guides}.\looseness=-1

The size of our interview pool is commensurate with prior research at CHI on the users and designers of AI and ML tools \cite[e.g.,][]{veale_fairness_2018, chordia_deceptive_2023, scheuerman_products_2024, jin_beyond_2024}, and was large enough for us to conduct interviews until saturation \cite{small_2009_how, hennink_sample_2022}. We defined saturation as occurring when we completed five consecutive interviews in which participants did not proactively raise\footnote{By \textit{proactively raise}, we mean that a participant mentioned a potential harm in response to an open-ended question (e.g., ``what harms are you concerned about?'') and not a direct prompt (e.g., ``are you concerned about hallucinations?'').} a previously unmentioned harm. All of the potential harms from LLMs mentioned by edtech providers were proactively raised by T1; none of T2-T6 mentioned any additional harms. Almost all of the potential harms from LLMs mentioned by educators were proactively raised within the first four educator interviews we conducted, with two exceptions: privacy harms were first proactively raised by E6 and the potential for toxic or biased content was first proactively raised by E18.\footnote{In prior interviews, those harms were discussed, but only when the interviewer specifically prompted educators about them.}\looseness=-1


\subsubsection*{Thematic Analysis.} 
After transcribing the interviews, we used an inductive-deductive coding approach to conduct a thematic analysis \cite{bogdan_qualitative_1998, braun_2006_using}, following similar work in HCI and related fields \citep[e.g.,][]{kawakami_situate_2024, birhane_values_2022}. First, the first author, in discussion with the other authors, developed an initial set of codes based on our pre-identified set of domain-agnostic harms as well as the close-ended questions we asked in our semi-structured interviews. This included participants' professional backgrounds and prior experience with LLMs as well as the harms we specifically prompted all participants to consider if they did not proactively raise them: toxic or biased content, privacy violations, hallucinations, malicious use, and socioeconomic harms. For each harm, we included codes related to (1) whether the harm was proactively raised or was prompted by the interviewer, (2) level of concern about the harm, (3) level of experience with the harm, and (4) planned or implemented strategies for measuring or mitigating the harm. The first author then manually reviewed all transcripts for that initial set of codes. During that review, the first author also noted any additional harms that had been proactively raised by participants. After reviewing all transcripts once, the first author examined all participant-raised harms in order to synthesize related harms; this synthesis was then reviewed and discussed by all authors. The first author then reviewed each transcript a second time, identifying the previously mentioned details for all harms. Finally, the first author examined all identified mitigation strategies across all harms in order to synthesize related mitigation strategies or those that were common across different harms; this synthesis was again reviewed and discussed by all authors.\looseness=-1

We categorized a participant's level of concern as  `lower' or `higher.' In some cases, participants explicitly stated a level of concern (e.g., ``I'm not concerned''); in most cases, we inferred it based on participants' statements about how awareness of that potential harm had impacted or would impact their use of LLMs. For example, in coding statements related to the risk of stereotyping and bias in LLM-generated outputs, we interpreted ``I think I would just work around it'' (E10) to indicate \textit{lower concern} and ``[t]hose are some of the reasons why I'm just not heavy on having ChatGPT in my classroom'' (E7) to indicate \textit{higher concern}.\looseness=-1

\subsubsection*{Positionality.}
We asked participants to describe themselves based on their professional roles as well as their personal experience with and opinions on LLMs. To reflect on our own positionality, we start by doing the same. We are an interdisciplinary team of researchers based at a university in the US. Collectively, we research algorithmic fairness, computational social science, and the learning sciences. None of us have ever been employed as a K-12 educator, but one of us currently holds a research role at an edtech startup; that author also has previously conducted research with both edtech providers and educators. We all identify as having a `mixed' prior opinion on LLMs: we recognize the potential of LLMs to produce benefits in domains including education, but we have also all conducted research about the potential harms of LLMs. Our team is of mixed gender and includes both white and Asian researchers. In terms of our own K-12 education, we have collectively attended public and private schools in the US and internationally.\looseness=-1

We acknowledge that our research has been shaped by this positionality. It informed how we conceptualized this study, how we recruited participants, and how we conducted our thematic analysis. Specifically, our prior experience conducting LLM harm assessments and working with edtech providers informed the set of LLM harm taxonomies we built on, and in turn the questions we asked in our interviews. This is something that we tried to account for by leading with open-ended questions about LLM harms before prompting about specific harms. In addition, we reached our initial pool of participants primarily through the networks afforded to us based on our affiliation with a large research university, although we relied on snowball sampling to broaden the pool. Because our team is based in the US, our interviews and analysis also focus primarily on the US education system.\looseness=-1