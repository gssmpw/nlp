\section{Discussion}\label{s-discussion}

Overall, our findings build on previously proposed taxonomies of potential harms from LLMs \cite{bender_dangers_2021, weidinger_taxonomy_2022} by identifying the harms that are most relevant in education: technical harms like toxic or biased content, privacy violations, and hallucinations; interaction harms like academic dishonesty; and harms arising from broader impacts including inhibiting student learning and social development, increasing educator workload while decreasing educator autonomy, and exacerbating systemic inequalities in education. In addition, we highlight gaps between conceptions of harm by edtech providers (who focus primarily on technical harms) and those by educators (who are most concerned about harms resulting from the broader impacts caused by interactions between LLM-based edtech and students, educators, and/or school systems). In doing so, we hope to lay the groundwork for conversations that make the concerns of educators more salient for edtech providers, and at the same time, make the mitigation strategies used by leading edtech designers and developers clear to educators. Our intent is that this work will facilitate the trust-building necessary to ground co-design practices between edtech providers and educators \cite{cardona_artificial_2023}, and lead to the \textit{centering of educators} in the future design and development of edtech tools \cite{kizilcec2024advance}.\looseness=-1

In the remainder of our paper, we discuss our findings in a broader context and point to opportunities for future work. First, we make recommendations to \textit{facilitate the design and development of educator-centered} LLM-based edtech going forward (\S\ref{s-opportunities}). We also reflect on the limitations, ethical considerations, and potential adverse impacts of our work (\S\ref{s-limitations}).\looseness=-1


\input{tables/gaps_opportunities}

\subsection{Recommendations to Facilitate the Design and Development of Educator-Centered Edtech}\label{s-opportunities}

Our interviews surfaced multiple gaps in harm mitigation strategies, outlined in Table \ref{t-mitigations}, that should be addressed by edtech designers and developers, researchers, regulators, and school leaders going forward. We make the following recommendations:\looseness=-1

\begin{enumerate}
    \item \textbf{Edtech providers should design tools in a way that facilitates educator mediation of LLM harms.} Edtech providers currently focus significant energy on mitigating toxic or biased content, privacy violations, hallucinations, academic dishonesty, and the potential for LLMs to inhibit student learning (i.e., lack of helpfulness). At the same time, however, these are the set of harms that educators report feeling able to mitigate by mediating student interaction with tools. By building opportunities for mediation into tools themselves, edtech providers can increase educator autonomy while facilitating the mitigation of a broad list of harms. A promising avenue is co-design practices that allow educators to control the level of oversight that they have over LLM-based edtech \cite[e.g.,][]{de_laet_surveying_2021}.\looseness=-1
    \item  \textbf{Regulators should develop centralized, clear, and independent reviews of LLM-based edtech.} Educators report an increased workload---and a dearth of accurate, unbiased information---related to identifying, vetting, and otherwise learning about LLM-based edtech tools. We echo previous calls \citep[e.g.,][]{williamson_time_2024} for regulators to vet edtech tools. Regulators should leverage existing organizations, such as the What Works Clearinghouse (WWC) established by the US DOE Institute of Education Sciences, to not only vet these tools but also to create searchable repositories of vetted edtech tools.\looseness=-1
    \item \textbf{Researchers and edtech providers should explore how to entrust tool-building to educators themselves.} Throughout this work, we have focused primarily on LLM harms. However, the educators we interviewed were excited about LLM-based edtech in theory, and listed a variety of ways that they, in an ideal world, would use LLMs; for example, generating lesson plans, aligning them to curriculum standards, and adapting them to students' Individualized Education Programs (IEPs).\footnote{\textit{IEPs} are customized learning plans for students with special needs or disabilities.} Currently, educators describe adapting unspecialized tools to suit these needs (``Whether the content in the...plan is what I want it to be or not, it does spit out a structure that I think is really useful,'' E21), with mixed success (``Sometimes wordsmithing what ChatGPT produces ends up being more work than just writing it,'' E8). This current landscape is the continuation of a well-documented trend in edtech in which educational goals are misaligned with the specific capabilities of the AI/ML solutions that seek to address them in practice \cite{liu_reimagining_2023}. However, multiple educators we spoke to described plans to create custom chatbots (through prompt engineering and fine-tuning) that `spoke the language' of their school and their curriculum in a way that off-the-shelf models could not (E12, E20). This is a promising avenue for future research and practice that is already being studied within the HCI community \citep[e.g.,][]{hedderich_piece_2024, f63ccd0b-5bc1-31b0-aa9a-fbd5ab3ba3cc, https://doi.org/10.1111/bjet.12861}.
    \looseness=-1
    \item \textbf{Regulators and school leaders should prioritize educator-centered procurement practices.} These include, for example, actively soliciting educator input in school procurement decisions as well as ensuring that educators are not penalized for choosing \textit{not} to use their schools' LLM-based edtech tools. Procurement policies should follow existing frameworks that require procurers to conduct risk-benefit analysis to explore how LLM-based edtech will improve existing processes without undermining or marginalizing educators \cite[e.g.,][]{noauthor_ethical_2021}.\looseness=-1
    \end{enumerate}

\subsection{Limitations and Ethical Considerations}\label{s-limitations}
\subsubsection*{Limitations} A primary limitation of our work is that we were only able to interview edtech providers and educators based in the US, the UK, and Canada.\footnote{27 interviewees were based in the US, and one each were based in the UK and Canada.} As such, the harms we surface are those relevant to educators from countries that are English-speaking and WEIRD (Western, educated, industrialized, rich and democratic) \cite{Henrich_Heine_Norenzayan_2010}. Well-documented harms of and inequities in LLMs---in particular, that LLMs display cultural biases \cite{tao2024culturalbiasculturalalignment}, perform worse on so-called `low-resource' languages \cite{nicholas_lost_2023, joshi-etal-2020-state}, and that the labor \cite{noema_workers, wsj_workers} and environmental \cite{png_2022_tensions} costs of building and operating LLMs are not equally distributed---were therefore not surfaced by the edtech providers and educators we spoke to. We thus acknowledge that our results are narrowly focused on education in WEIRD, English-speaking countries despite the fact that there is growing scholarship exploring the use of LLMs in edtech globally \cite{henkel2024effective, choi2024llms}, as well as grappling with how to ensure that those efforts do not recreate colonial harms \cite{Shahjahan_decolonizing_2022, ogunremi_decolonizing_2023, bird_decolonising_2020}.\looseness=-1

Additionally, we were not able to recruit a representative sample of interview subjects -- instead, we sought to recruit employees of \textit{widely used and well-regarded} edtech products and educators with a \textit{diverse set of backgrounds and demographics}. As previously noted, this resulted in a relatively small sample size of edtech providers interviewed (six), and we therefore do not attempt to generalize about standard practices across the universe of edtech providers in this work. However, because the edtech providers we interviewed represent leaders in their field, we do believe that the practices they describe are likely to represent emerging best practices -- and at the very least accurately reflect practices that shape widely used edtech products. Further, the sample size of educators we interviewed (23) is commensurate with prior research at CHI, and we were able to conduct interviews on both populations until saturation \cite{hennink_sample_2022, small_2009_how}.\looseness=-1

\subsubsection*{Ethical Considerations.}
In conducting this work, we faced a classic tension inherent to participatory AI research \cite{feffer_preference_2023, birhane_power_2022, sloane_participation_2022}: our goal with this work was to facilitate the centering of educators in the future development of edtech tools, but our method for doing so (Zoom interviews) placed demands on educators' (already limited) time. To mitigate this, we provided competitive compensation (\$50 per educator, corresponding to an hourly rate of between \$50 and \$100 depending on interview length).\looseness=-1

\subsubsection*{Adverse Impact.}
Our interviewees spoke to us under the condition of anonymity: edtech providers shared potentially sensitive product details and processes with us, and educators shared critical thoughts on their employers and working environments. As such, a major potential adverse impact of our work is the risk that any of our interviewees may be identified. To avoid this, we have taken the following steps: (1) anonymizing all quotes, (2) providing characteristics of our interviewees at only a low level of granularity, (3) storing data securely in accordance with our IRB, and (4) deleting the original meeting recordings after transcribing them. Other than this, we do not believe that any of our findings are likely to be co-opted or used in an adversarial way. \looseness=-1

\section{Conclusion}
Through a series of interviews with edtech providers (N=6) and educators (N=23), we surfaced an \textbf{education-specific overview of LLM harms} that edtech providers and educators are currently anticipating, observing, or actively working to mitigate. These include: technical harms (toxic or biased content, privacy violations, hallucinations), interaction harms (academic dishonesty), and harms from the broader impact of LLMs (inhibiting student learning and social development, increasing educator workload, decreasing educator autonomy, and exacerbating systemic inequalities in education). We find that edtech providers focus almost exclusively on mitigating \textit{technical} harms, which are measurable based solely on the outputs of LLM-based systems -- but that these are the same harms that educators report feeling most equipped to mediate through their teaching practices. On the other hand, educators report high levels of concern about harms resulting from the \textit{broader impacts} of LLMs -- harms that require observing interactions between LLM-based systems and students, educators, and/or school systems to measure. Overall, we provide \textbf{an education-specific overview of potential harms from LLMs}, building on widely used domain-agnostic taxonomies. In addition, we identify \textbf{gaps between conceptions of harm by edtech providers and those by educators}. Finally, we make \textbf{recommendations for edtech designers and developers, researchers, regulators, and school leaders} to bridge those gaps and contribute to the design of \textit{educator-centered} edtech. \looseness=-1


