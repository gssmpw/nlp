\section{Methods}

\subsection{Foothold Reward}
\label{sec:method_footholdreward}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/foothold_reward.png}
    \caption{\textbf{Foothold Reward.} We sample $n$ points under the foot. Green points indicate contact with the surface within the safe region, while red points represent those not in contact with the surface.}
    \label{fig:foothold_reward}
\end{figure}

To accommodate the polygonal foot model of the humanoid robot, we introduce a sampling-based foothold reward that evaluates foot placement on sparse footholds.This evaluation is determined by the overlap between the foot's placement and designated safe areas, such as stones and beams. Specifically, we sample $n$ points on the soles of the robot’s feet, as illustrated in Fig.~\ref{fig:foothold_reward}. For each $j$-th sample on foot $i$, let $d_{ij}$ denotes the global terrain height at the corresponding position. The penalty foothold reward $r_\text{foothold}$ is defined as:
\begin{equation}
    r_\text{foothold} = -\sum_{i=1}^2 \mathbb{C}_i \sum_{j=1}^n \mathds{1} \{ d_{ij} < \epsilon \},
\end{equation}
where $\mathbb{C}_i$ is an indicator function that specifies whether foot $i$ is in contact with the terrain surface, and $\mathds{1}$ is the indicator function for a condition. The threshold $\epsilon$ is a predefined depth tolerance, and when $d_{ij} < \epsilon$, it indicates that the terrain height at this sample point is significantly low, implying improper foot placement outside of a safe area. This reward function encourages the humanoid robot to maximize the overlap between its foot placement and the safe footholds, thereby improving its terrain-awareness capabilities.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/framework.png}
    \caption{\textbf{Overview of \beamdojo.} (a) \textbf{Training in Simulation:} In stage 1, proprioceptive and perceptive information, locomotion rewards and the foothold reward are decoupled respectively, with the former obtained from flat terrain and the latter from task terrain. The double critic module separately learns two reward groups. In stage 2, the policy is fine-tuned on the task terrain, utilizing the full set of observations and rewards. (b) \textbf{Real-world deployment:} The robot-centric elevation map, reconstructed using LiDAR data, is combined with proprioceptive information to serve as the input for the actor.}
    \label{fig:framework}
\end{figure*}

\subsection{Double Critic for Sparse Reward Learning}
\label{sec:method_doublecritic}

The task-specific foothold reward $r_\text{foothold}$ is a sparse reward. To effectively optimize the policy, it is crucial to carefully balance this sparse reward with dense locomotion rewards which are crucial for gait regularization~\cite{zargarbashi2024robotkeyframing}. Inspired by~\cite{huang2022reward, xu2023composite, zargarbashi2024robotkeyframing}, we adopt a double critic framework based on PPO, which effectively decouples the mixture of dense and sparse rewards. 

In this framework, we train two separate critic networks, $\left\{V_{\phi_1}, V_{\phi_2}\right\}$, to independently estimate value functions for two distinct reward groups: (i) the regular locomotion reward group (dense rewards), $R_1=\{r_i\}_{i=0}^n$, which have been studied in quadruped locomotion tasks~\cite{margolis2023walk} and humanoid locomotion tasks~\cite{long2024learninghumanoid}, and (ii) the task-specific foothold reward group (sparse reward), $R_2=\{r_\text{foothold}\}$.

The double critic process is illustrated in Fig.~\ref{fig:framework}. Specifically, each value network $V_{\phi_i}$ is updated independently for its corresponding reward group $R_i$ with temporal difference loss (TD-loss):
\begin{equation}
    \mathcal{L}(\phi_i)=\mathbb{E} \left[ \left\| R_{i, t} + \gamma V_{\phi_i}(s_{t+1}) - V_{\phi_i}(s_t) \right\|^2 \right],
\end{equation}
where $\gamma$ is the discount factor. Then the respective advantages $\{\hat{A}_{i, t}\}$ are calculated using Generalized Advantage Estimation (GAE)~\cite{schulman2015high}:
\begin{gather}
    \delta_{i, t} = R_{i, t} + \gamma V_{\phi_i}(s_{t+1}) - V_{\phi_i}(s_t), \\
    \hat{A}_{i, t} = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{i, t+l},
\end{gather}
where $\lambda$ is the balancing parameter. These advantages are then individually normalized and synthesized into an overall advantage:
\begin{equation}
   \hat{A}_t = w_1 \cdot \frac{\hat{A}_{1, t} - \mu_{\hat{A}_{1, t}}}{\sigma_{\hat{A}_{1, t}}} + w_2 \cdot \frac{\hat{A}_{2, t} - \mu_{\hat{A}_{2, t}}}{\sigma_{\hat{A}_{2, t}}} ,
\end{equation}
where $w_i$ is the weight for each advantage component, and $\mu_{\hat{A}_{i, t}}$ and $\sigma_{\hat{A}_{i, t}}$ are the batch mean and standard deviation of each component. This overall advantage is then used to update the policy:
\begin{equation}
    \mathcal{L}(\theta) = \mathbb{E} \left[ \min \left( \alpha_t (\theta)\hat{A}_t, \text{clip}(\alpha_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t \right) \right],
\end{equation}
where $\alpha_t(\theta)$ is the probability ratio, and $\epsilon$ is the clipping hyperparameter.

This double critic design provides a modular, plug-and-play solution for handling specialized tasks with sparse rewards, while effectively addressing the disparity in reward feedback frequencies within a mixed dense-sparse environment~\cite{zargarbashi2024robotkeyframing}. The detailed reward terms are provided in Appendix~\ref{sec:append_reward}.

\subsection{Learning Terrain-Aware Locomotion via Two-Stage RL}
\label{sec:method_twostage}

To address the early termination problem in complex terrain dynamics and encourage full trial-and-error exploration, we adopt a two-stage reinforcement learning (RL) approach for terrain-aware locomotion in simulation, inspired by~\cite{zhuang2023robot, zhuang2024humanoid}. As illustrated in Fig.~\ref{fig:framework}, in the first stage, termed the ``soft terrain dynamics constraints'' phase, the humanoid robot is trained on flat terrain while being provided with a corresponding height map of the true task terrains (e.g., stepping stones). This setup encourages broad exploration without the risk of early termination from missteps. Missteps are penalized but do not lead to termination, allowing the humanoid robot to develop foundational skills for terrain-aware locomotion. In the second stage, termed the ``hard terrain dynamics constraints'' phase, we continue training the humanoid on the real terrains in simulation, where missteps result in termination. This stage fine-tunes the robot's ability to step on challenging terrains accurately.

\subsubsection{Stage 1: Soft Terrain Dynamics Constraints Learning}

In this stage, we first map each task terrain (denoted as $\mathcal{T}$) to a flat terrain (denoted as $\mathcal{F}$) of the same size. Both terrains share the same terrain noise, with points are one-to-one corresponding. The only difference is that the flat terrain $\mathcal{F}$ fills the gaps in the real terrain $\mathcal{T}$. 

We let the humanoid robot traverse the terrain $\mathcal{F}$, receiving proprioceptive observations, while providing perceptual feedback in the form of the elevation map of terrain $\mathcal{T}$ at the corresponding humanoid's base position. This setup allows the robot to ``imagine'' walking on the true task terrain while actually traversing the safer flat terrain, where missteps do not lead to termination. To expose the robot to real terrain dynamics, we use the foothold reward (introduced in Section~\ref{sec:method_footholdreward}). In this phase, this reward is provided by the terrain $\mathcal{T}$, where $d_{ij}$ is the height of the true terrain at the sampling point, while locomotion rewards are provided by the terrain $\mathcal{F}$. 

This design successfully decouples the standard locomotion task and the task of traversing sparse footholds: flat terrain, $\mathcal{F}$, provides proprioceptive information and locomotion rewards to learn regular gaits, while risky task terrain, $\mathcal{T}$, offers perceptive information and the foothold reward to develop terrain-awareness skills. We train these two reward components separately using a double critic framework, as described in Section~\ref{sec:method_doublecritic}. 

Furthermore, by allowing the humanoid robot to traverse the flat terrain while applying penalties for missteps instead of terminating the episode, the robot can continuously attempt foothold placements, making it much easier to obtain successful positive samples. In contrast, conventional early termination disrupts entire trajectories, making it extremely difficult to acquire safe foothold samples when learning from scratch. This approach significantly improves sampling efficiency and alleviates the challenges of exploring terrains with sparse footholds.


\subsubsection{Stage 2: Hard Terrain Dynamics Constraints Learning}

In the second stage, we fine-tune the policy directly on the task terrain $\mathcal{T}$. Unlike in Stage 1, missteps on $\mathcal{T}$ now result in immediate termination. This enforces strict adherence to the true terrain constraints, requiring the robot to develop precise and safe locomotion strategies.

To maintain a smooth gait and accurate foot placements, we continue leveraging the double-critic framework to optimize both locomotion rewards and the foothold reward $r_\text{foothold}$ Here, $d_{ij}$ again represents the height of terrain $\mathcal{T}$ at the given sampling point.

\subsection{Training in Simulation}
\subsubsection{Observation Space and Action Space}

The policy observations, denoted as $\mathbf{o}_t$, consist of four components:
\begin{equation} 
    \mathbf{o}_t = \left[\mathbf{c}_t, \mathbf{o}^\text{proprio}_t, \mathbf{o}_t^\text{percept}, \mathbf{a}_{t-1} \right]. 
\end{equation}


The commands $\mathbf{c}_t \in \mathbb{R}^3$ specify the desired velocity, represented as $\left[ \mathbf{v}_x^c, \mathbf{v}_y^c, \boldsymbol{\omega}_\text{yaw}^c \right]$. These denote the linear velocities in the longitudinal and lateral directions, and the angular velocity in the horizontal plane, respectively. The proprioceptive observations $\mathbf{o}_t^\text{proprio} \in \mathbb{R}^{64}$ include the base angular velocity $\bm{\omega}_t \in \mathbb{R}^3$, gravity direction in the robot's frame $\mathbf{g}_t \in \mathbb{R}^3$, joint positions $\bm{\theta}_t \in \mathbb{R}^{29}$, and joint velocities $\dot{\bm{\theta}}_t \in \mathbb{R}^{29}$. The perceptive observations $\mathbf{o}_t^\text{percept} \in \mathbb{R}^{15 \times 15}$ correspond to an egocentric elevation map centered around the robot. This map samples $15 \times 15$ points within a $0.1$ m grid in both the longitudinal and lateral directions. The action of last timestep $\mathbf{a}_{t-1} \in \mathbb{R}^{12}$ is also included to provide temporal context.

The action $\mathbf{a}_t \in \mathbb{R}^{12}$ represents the target joint positions for the $12$ lower-body joints of the humanoid robot, which are directly output by the actor network. For the upper body joints, the default position is used for simplicity. A proportional-derivative (PD) controller converts these joint targets into torques to track the desired positions.

\subsubsection{Terrain and Curriculum Design}
\label{sec:method_curriculum}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/terrain2.png}
    \caption{\textbf{Terrain Setting in Simulation.} (a) is used for stage 1 training, while (b) and (c) are used for stage 2 training. The training terrain progression is listed from simple to difficult. (b)-(e) are used for evaluation.}
    \label{fig:terrain}
\end{figure}

Inspired by~\cite{heess2017emergence, yu2024walking, Zhang2023LearningAL}, we design five types of sparse foothold terrains for the two-stage training and evaluation:

\begin{itemize}
    \item \textit{Stones Everywhere}: This is a general sparse foothold terrain where stones are scattered across the entire terrain. The center of the terrain is a platform surrounded by stones, as shown in Fig.~\ref{fig:terrain}(a). The stones are uniformly distributed within sub-square grids. As the curriculum progresses, the stone size decreases and the sparsity increases.
    
    \item \textit{Stepping Stones}: This terrain consists of two lines of stepping stones in the longitudinal direction, connected by two platforms at each end, as shown in Fig.~\ref{fig:terrain}(b). Each stone is uniformly distributed in two sub-square grids, with the same curriculum effect as in \textit{Stones Everywhere}.
    
    \item \textit{Balancing Beams}: In the initial curriculum level, this terrain has two lines of separate stones in the longitudinal direction. As the curriculum progresses, the size of the stones decreases and their lateral distance reduces, eventually forming a single line of balancing beams, as shown in Fig.~\ref{fig:terrain}(c). This terrain is challenging for the robot as it must learn to keep its feet together on the beams without colliding with each other, while maintaining the center of mass. This requires a distinct gait compared to regular locomotion tasks.
    
    \item \textit{Stepping Beams}: This terrain consists of a sequence of beams to step on, randomly distributed along the longitudinal direction, with two platforms at either end, as illustrated in Fig.~\ref{fig:terrain}(d). This terrain, along with the \textit{Stones Everywhere} and \textit{Stepping Stones} terrains, requires the robot to place its footholds with high precision.
    
    \item \textit{Gaps}: This terrain consists of several gaps with random distances between them, as shown in Fig.~\ref{fig:terrain}(e). This terrain requires the robot to make large steps to cross the gaps.
\end{itemize}

We begin by training the robot on the \textit{Stones Everywhere} terrain in Stage 1 with soft terrain constraints to develop a generalizable policy. In Stage 2, the policy is fine-tuned on the \textit{Stepping Stones} and \textit{Balancing Beams} terrains with hard terrain constraints. The commands used in these two stages are detailed in Table~\ref{tab:command}. Note that in Stage 2, only a single x-direction command is given, with no yaw command provided. This means that if the robot deviates from facing forward, no correction command is applied. We aim for the robot to learn to consistently face forward from preceptive observation, rather than relying on continuous yaw corrections.

\input{tables/commands_in_training}

For evaluation, the \textit{Stepping Stones}, \textit{Balancing Beams}, \textit{Stepping Beams}, and \textit{Gaps} terrains are employed. Remarkably, our method demonstrates strong zero-shot transfer capabilities on the latter two terrains, despite the robot being trained exclusively on the first three terrains.

The curriculum is designed as follows: the robot progresses to the next terrain level when it successfully traverses the current terrain level three times in a row. Furthermore, the robot will not be sent back to an easier terrain level before it pass all levels, as training on higher-difficulty terrains is challenging at first. The detailed settings of the terrain curriculum are presented in the Appendix~\ref{sec:append_curriculum}.

\subsubsection{Sim-to-Real Transfer}
\label{sec:sim2real}

To enhance robustness and facilitate sim-to-real transfer, we employ extensive domain randomization~\cite{tobin2017domain, xie2021dynamics} on key dynamic parameters. Noise is injected into observations, humanoid physical properties, and terrain dynamics. Additionally, to address the large sim-to-real gap between the ground-truth elevation map in simulation and the LiDAR-generated map in reality—caused by factors such as odometry inaccuracies, noise, and jitter—we introduce four types of elevation map measurement noise during height sampling in the simulator:
\begin{itemize}
    \item \textit{Vertical Measurement}: Random vertical offsets are applied to the heights for an episode, along with uniformly sampled vertical noise added to each height sample at every time step, simulating the noisy vertical measurement of the LiDAR.
    \item \textit{Map Rotation}: To simulate odometry inaccuracies, we rotate the map in roll, pitch, and yaw. For yaw rotation, we first sample a random yaw noise. The elevation map, initially aligned with the robot's current orientation, is then resampled by adding the yaw noise, resulting in a new elevation map corresponding to the updated orientation. For roll and pitch rotations, we randomly sample the biases $\left[ h_x, h_y \right]$ and perform linear interpolation from $-h_x$ to $h_x$ along the $x$-direction and from $-h_y$ to $h_y$ along the $y$-direction. The resulting vertical height map noise is then added to the original elevation map.
    \item \textit{Foothold Extension}: Random foothold points adjacent to valid footholds are extended, turning them into valid footholds. This simulates the smoothing effect that occurs during processing of LiDAR elevation data.
    \item \textit{Map Repeat}: To simulate delays in elevation map updates, we randomly repeat the map from the previous timestep.
\end{itemize}

The detailed domain randomization settings are provided in Appendix~\ref{sec:append_random}.

\subsection{Real-world Deployment}

\subsubsection{Hardware Setup}

We use Unitree G1 humanoid robot for our experiments in this work. The robot weighs 35 kg, stands 1.32 meters tall, and features 23 actuated degrees of freedom: 6 in each leg, 5 in each arm, and 1 in the waist. It is equipped with a Jetson Orin NX for onboard computation and a Livox Mid-360 LiDAR, which provides both IMU data and feature points for perception.

\subsubsection{Elevation Map and System Design}

The raw point cloud data obtained directly from the LiDAR suffers from significant occlusion and noise, making it challenging to use directly. To address this, we followed~\cite{long2024learninghumanoid} to construct a robot-centric, complete, and robust elevation map. Specifically, we employed Fast LiDAR-Inertial Odometry (FAST-LIO)~\cite{xu2021fast, xu2022fast} to fuse LiDAR feature points with IMU data provided by the LiDAR. This fusion generates precise odometry outputs, which are further processed using robot-centric elevation mapping methods~\cite{Fankhauser2014RobotCentricElevationMapping, Fankhauser2018ProbabilisticTerrainMapping} to produce a grid-based representation of ground heights.

During deployment, the elevation map publishes information at a frequency of $10$ Hz, while the learned policy operates at $50$ Hz. The policy’s action outputs are subsequently sent to a PD controller, which runs at $500$ Hz, ensuring smooth and precise actuation.
