\section{Related Works}

\subsection{Locomotion on Sparse Footholds}

Walking on sparse footholds has been a long-standing application of perceptive legged locomotion. Existing works often employs model-based hierarchical controllers, which decompose this complex task into separate stages of perception, planning, and control~\cite{grandia2023perceptive, griffin2019footstep, jenelten2020perceptive, mastalli2020motion, melon2021receding, winkler2018gait}. However, model-based controllers react sensitively to violation of model assumptions, which hinders applications in real-world scenarios. Recent studies have explored combining RL with model-based controllers, such as using RL to generate trajectories that are then tracked by model-based controllers~\cite{gangapurwala2022rloc, yu2021visual, xie2022glide}, or employing RL policies to track trajectories generated by model-based planners~\cite{jenelten2024dtc}. While demonstrating remarkable performance, these decoupled architectures can constrain the adaptability and coordination of each module.

Subsequent works have explored end-to-end learning frameworks that train robots to walk on sparse footholds using perceptive locomotion controllers~\cite{agarwal2023legged, cheng2024extreme, yang2023neural, yu2024walking, Zhang2023LearningAL}. Despite their focus being limited to quadrupeds, a majority of these works rely on depth cameras for exteroceptive observations, which are limited by the camera's narrow field of view and restrict the robot to moving backward~\cite{agarwal2023legged, cheng2024extreme, yang2023neural, yu2024walking}. Additionally, an image processing module is often necessary to bridge the sim-to-real gap between the captured depth images and the terrain heightmap used during training~\cite{agarwal2023legged, cheng2024extreme, yang2023neural, yu2024walking}.

In contrast to the aforementioned literature, this work achieves agile humanoid locomotion over risky terrains that addressing unique challenges specific to humanoid systems, such as foot geometry. Additionally, we implement a lidar-based elevation map to enhance the task, demonstrating that the robot can move smoothly both forward and backward using the robotics-centric elevation map as the perception module.

\subsection{Reinforcement Learning in Locomotion Control}

Reinforcement learning has been widely applied in legged locomotion control~\cite{cheng2024extreme, he2024agile, kumar2021rma, lee2020learning, long2024hybrid, margolis2024rapid, miki2022learning, nahrendra2023dreamwaq, zhuang2023robot}, benefiting from the policy update stability and high data efficiency provided by Proximal Policy Optimization (PPO)~\cite{schulman2017proximal}. To adapt learned policies to diverse target tasks and ensure hardware deployability, previous works have designed two-stage training frameworks that aim to bridge the sim-to-real gap in the observation space~\cite{kumar2021rma, lee2020learning}. In contrast, this work introduces a novel two-stage training approach specifically aimed at improving sample efficiency, particularly addressing the challenge of early termination when walking on sparse terrains. This design not only enhances performance but also ensures more efficient learning in complex, real-world environments.