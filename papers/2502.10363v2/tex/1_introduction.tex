\section{Introduction}

Traversing risky terrains with sparse footholds, such as stepping stones and balancing beams, presents a significant challenge for legged locomotion. Achieving agile and safe locomotion on such environment requires robots to accurately process perceptive information, make precise footstep placement within safe areas, and maintain base stability throughout the process~\cite{yu2024walking, Zhang2023LearningAL}.
% \weinan{add refs here}

Existing works have effectively addressed this complex task for quadrupedal robots~\cite{gangapurwala2022rloc, grandia2023perceptive, jenelten2024dtc, lu2024learning, xie2022glide, yu2024walking, yu2021visual, Zhang2023LearningAL, zhu2024robust}. However, these methods encounter great challenges when applied to humanoid robots, primarily due to a key difference in foot geometry. Although the foot of most quadrupedal robots and some simplified bipedal robots~\cite{englsberger2011bipedal, kajita20013d} can be modeled as a point, the foot of humanoid robots is often represented as a polygon~\cite{chestnutt2005footstep, griffin2019footstep, hornung2012adaptive, stumpf2014supervised}. For traditional model-based methods, this requires additional half-space constraints defined by linear inequalities, which impose a significant computational burden for online planning~\cite{deits2014footstep, griffin2019footstep, margolis2021learning, stumpf2014supervised}. In the case of reinforcement learning (RL) methods, foothold rewards designed for point-shaped feet are not suitable for evaluating foot placement of polygon-shaped feet~\cite{yu2024walking}. Hybrid methods, which combine RL with model-based controllers, face similar challenges in online planning for humanoid feet~\cite{gangapurwala2022rloc, jenelten2024dtc, xie2022glide}. Furthermore, the higher degrees of freedom and the inherently unstable morphology of humanoid robots make it even more difficult to achieve agile and stable locomotion over risky terrains.

On the other hand, recent advancements in learning-based humanoid robot locomotion have demonstrated impressive robustness across various tasks, including walking~\cite{chen2024learning, radosavovic2024learning, radosavovic2024humanoid, van2024revisiting}, stair climbing~\cite{cuiadapting, gu2024advancing, long2024learninghumanoid}, parkour~\cite{zhuang2024humanoid}, and whole-body control~\cite{cheng2024expressive, fu2024humanplus, he2024omnih2o, he2024learning, he2024hover, ji2024exbody2, jiang2024harmon}, etc. However, these methods still struggle with complex terrains and agile locomotion on fine-grained footholds. Enabling agile movement on risky terrains for humanoid robots presents several challenges. First, the reward signal for evaluating foot placement is sparse, typically provided only after completing a full sub-process (e.g., lifting and landing a foot), which makes it difficult to assign credit to specific states and actions~\cite{sutton1984temporal}. Second, the learning process is highly inefficient, as a single misstep often leads to early termination during training, hindering sufficient exploration. Additionally, obtaining reliable perceptual information is challenging due to sensory limitations and environmental noise~\cite{zhuang2024humanoid}.

In this work, we introduce \beamdojo, a novel reinforcement learning-based framework for controlling humanoid robots traversing risky terrains with sparse footholds. The name \beamdojo combines the words ``beam'' (referring to sparse footholds such as beams) and ``dojo'' (a place of training or learning), reflecting the goal of training agile locomotion on such challenging terrains. We begin by defining a sampling-based foothold reward, designed to evaluate the foot placement of a polygonal foot model. To address the challenge of sparse foothold reward learning, we propose using double critic architecture to separately learn the dense locomotion rewards and the sparse foothold reward. Unlike typical end-to-end RL methods~\cite{yu2024walking, Zhang2023LearningAL}, \beamdojo further incorporates a two-stage approach to encourage fully trial-and-error exploration. In the first stage, terrain dynamics constraints are relaxed, allowing the humanoid robot to practice walking on flat terrain while receiving perceptive information of the target task terrain (e.g., sparse beams), where missteps will incur a penalty but do not terminate the episode. In the second stage, the policy is fine-tuned on the true task terrain. To enable deployment in real-world scenarios, we further implement a LiDAR-based, robot-centric elevation map with carefully designed domain randomization in simulation training.

As shown in Fig.~\ref{fig:teaser}, \beamdojo skillfully enables humanoid robots to traverse risky terrains with sparse footholds, such as stepping stones and balancing beams. Through extensive simulations and real-world experiments, we demonstrate the efficient learning process of \beamdojo and its ability to achieve agile locomotion with precise foot placements in real-world scenarios.

The contributions of our work are summarized as follows: 
\begin{itemize}
    \item We propose \beamdojo, a two-stage RL framework that combines a newly designed foothold reward for the polygonal foot model and a double critic, enabling humanoid locomotion on sparse footholds. 
    \item We implement a LiDAR-based elevation map for real-world deployment, incorporating carefully designed domain randomization in simulation training.
    \item We conduct extensive experiments both in simulation and on Unitree G1 Humanoid, demonstrating agile and robust locomotion on sparse footholds, with a high zero-shot sim-to-real transfer success rate of 80\%.  To the best of our knowledge, \beamdojo is the first learning-based method to achieve fine-grained foothold control on risky terrains with sparse footholds.
\end{itemize}