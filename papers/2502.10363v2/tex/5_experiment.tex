\section{Experiments}
\subsection{Experimental Setup}

\input{tables/benchmark}

We compare our proposed framework \beamdojo, which integrates two-stage RL training and a double critic, with the following baselines:

\begin{enumerate}
    \item \textbf{PIM}~\cite{long2024learninghumanoid}: This one-stage method is designed for humanoid locomotion tasks, such as walking up stairs and traversing uneven terrains. We additional add our foothold reward $r_\text{foothold}$ to encourage the humanoid to step accurately on the foothold areas.
    \label{baseline:pim}
    
    \item \textbf{Naive}: This method neither include the two-stage RL nor the double critic. The only addition is the foothold reward. This is an naive implementation to solve this task.
    \label{baseline:naive}
    
    \item \textbf{Ours w/o Soft Dyn}: This is an ablation which removing the first stage of training with soft terrain dynamics constraints.
    \label{baseline:no-stage-1}
    
    \item \textbf{Ours w/o Double Critic}: This is an ablation which uses a single critic to handle both locomotion rewards and foothold reward, instead of using a double critic. This follows the traditional design in most locomotion tasks.
    \label{baseline:no-double-critic}
\end{enumerate}

The training and simulation environments are implemented in IsaacGym~\cite{makoviychuk2021isaac}. To ensure fairness, we adapt all methods to two stages. For stage 1, we train the humanoid on the \textit{Stones Everywhere} with curriculum learning. In this stage, our method and baseline~\ref{baseline:no-double-critic} use soft terrain dynamics constraints, while all other baselines use hard terrain dynamics constraints. For stage 2, we conduct fine-tuning on the \textit{Stepping Stones} and \textit{Balancing Beams} terrains with curriculum learning.

For evaluation, we test all methods on the \textit{Stepping Stones}, \textit{Balancing Beams}, \textit{Stepping Beams} and \textit{Gaps} terrains. We evaluate performance using three metrics:
\begin{itemize}
    \item \textbf{Success Rate} $R_\mathrm{succ}$: The percentage of successful attempts to cross the entire terrain.
    \item \textbf{Traverse Rate} $R_\mathrm{trav}$: The ratio of the distance traveled before falling to the total terrain length ($8$ m).
    \item \textbf{Foothold Error} $E_\mathrm{foot}$: The average proportion of foot samples landing outside the intended foothold areas.
\end{itemize}

\subsection{Simulation Experiments}

\subsubsection{Quantitative results}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/foothold_error.pdf}
    \caption{\textbf{Foothold Error Comparison.} The foothold error benchmarks of all methods are evaluated in (a) stepping stones and (b) balancing beams, both tested under medium terrain difficulty.}
    \label{fig:foothold_error}
\end{figure}

We report the success rate ($R_\mathrm{succ}$) and traverse rate ($R_\mathrm{trav}$) for four terrains at medium and hard difficulty levels (terrain level 6 and level 8, respectively) in Table~\ref{tab:all-results}. For each setting, the mean and standard deviation are calculated over three policies trained with different random seeds, each evaluated across 100 random episodes. Our key observations are as follows: 

\begin{itemize}
    \item Leveraging the efficient two-stage RL framework and the double critic, \beamdojo consistently outperforms single-stage approaches and ablation designs, achieving high success rates and low foothold errors across all challenging terrains. Notably, the naive implementation struggles significantly and is almost incapable of traversing stepping stones and balancing beams at hard difficulty levels.
    \item Existing humanoid controllers~\cite{long2024learninghumanoid} face difficulties when adapting to risky terrains with fine-grained footholds, primarily due to sparse foothold rewards and low learning efficiency.
    \item Despite the our method not being explicitly trained on \textit{Stepping Beams} and \textit{Gaps}, it demonstrates impressive zero-shot generalization capabilities on this terrain.
\end{itemize}

\subsubsection{Detailed Ablation Analysis}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/learning_efficiency.pdf}
    \caption{\textbf{Learning Efficiency.} The learning curves show the maximum terrain levels achieved in two training stages of all methods. Faster attainment of terrain level 8 indicates more efficient learning.}
    \label{fig:learning_efficiency}
\end{figure}

We conduct additional ablation studies by comparing \beamdojo with baselines~\ref{baseline:naive},~\ref{baseline:no-stage-1}, and~\ref{baseline:no-double-critic}.

\textbf{Foot Placement Accuracy:} As shown in Fig.~\ref{fig:foothold_error}, \beamdojo achieves highly accurate foot placement with low foothold error values, largely due to the contribution of the double critic. In comparison, the naive implementation shows higher error rates, with a substantial proportion of foot placements landing outside the safe foothold areas. This demonstrates the precision and effectiveness of our method in challenging terrains.


\textbf{Learning Efficiency:} Although we train for 10,000 iterations in both stages to ensure convergence across all designs, \beamdojo converges significantly faster, as shown in Fig.~\ref{fig:learning_efficiency}. Both the two-stage training setup and the double critic improve learning efficiency, with the two-stage setup contributing the most. In contrast, the naive implementation struggles to reach higher terrain levels in both stages.

The advantage of two-stage learning lies in its ability to allow the agent to continuously attempt foot placements, even in the presence of missteps, making it easier to accumulate a substantial number of successful foot placement samples. Meanwhile, the double-critic setup separates the foothold reward from the locomotion rewards, ensuring that its updates remain unaffected by the noise of unstable locomotion signals, particularly in the early training phase. Both strategies play a crucial role in enhancing learning efficiency.

\input{tables/gait_regu}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/footplanning.png}
    \caption{\textbf{Foot Placement Planning Visualization.} We illustrate two trajectories for the foot placement process: the yellow line represents \beamdojo, while the red line corresponds to \textit{Ours w/o Double Critic}. Points along the trajectories are marked at equal time intervals. From A to C, the method without the double critic exhibits significant adjustments only when approaching the target stone (at point B).}
    \label{fig:footplanning}
\end{figure}

\input{tables/real_results}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/robustness.png}
    \caption{\textbf{Robustness Test.} We evaluate the robustness of the humanoid robot in real-world scenarios with: (a) heavy payload, (b) external forces, and (c) recovering from missteps.}
    \label{fig:robustness}
\end{figure*}


\textbf{Gait Regularization:} The combination of small-scale gait regularization rewards with sparse foothold reward can hinder gait performance, as shown in Table~\ref{tab:gait_regu}, where the naive design and the ablation without the double critic exhibit poor performance in both smoothness and feet air time. In contrast, our method and the ablation with double critic demonstrates superior motion smoothness and improved feet clearance. This improvement arises because, in the double-critic framework, the advantage estimates for the dense and sparse reward groups are normalized independently, preventing the sparse rewards from introducing noise that could disrupt the learning of regularization rewards.

\textbf{Foot Placement Planning:} As illustrated in Fig.~\ref{fig:footplanning}, we observe that the double critic also benefits foot placement planning of the entire sub-process of foot lifting and landing. Our method, \beamdojo, enables smoother planning, allowing the foot to precisely reach the next foothold. In comparison, the baseline excluding double critic demonstrates reactive stepping, where adjustments are largely made when the foot is close to the target stone. This behavior indicates that the double critic, by learning the sparse foothold reward separately, helps the policy adjust its motion over a longer horizon.


\subsection{Real-world Experiments}

\subsubsection{Result}

As demonstrated in Fig.\ref{fig:teaser}, our framework achieves zero-shot transfer, successfully generalizing to real-world dynamics. To showcase the effect of height map domain randomization (introduced in Section~\ref{sec:sim2real}) in sim-to-real transfer, we compare our proposed method with an ablation that excludes height map randomization (denoted as "ours w/o HR"). We conduct five trials on each terrain and report the success and traversal rates in Fig.~\ref{fig:real_results}, with following conclusions:
\begin{itemize}
    \item \beamdojo achieves a high success rate in real-world deployments, demonstrating excellent precise foot placement capabilities. Similar to simulation results, it also exhibits impressive generalization performance on \textit{Stepping Beams} and \textit{Gaps}, even though these terrains were not part of the training set.
    \item The ablation, lacking height map domain randomization, results in a significantly lower success rate, highlighting the importance of this design.
    \item It is also worth mentioning that \beamdojo enables backward movement in risky terrains, as shown in Fig.~\ref{fig:teaser}(b). This advantage is achieved by leveraging LiDAR to its full potential, whereas a single depth camera cannot handle such scenarios.
\end{itemize}

\subsubsection{Agility Test}

\input{tables/agile_test}

To assess the agility of our method, we provide the humanoid robot with three commanded longitudinal velocities, $\mathbf{v}_x^c$: 0.5, 0.75, and 1.0 m/s, and check the tracking ability. Each test was conducted over three trials, and the results are reported in Table~\ref{tab:agile_test}. The results show minimal tracking error, even at the highest command velocity of 1.0 m/s, where the robot achieved an average speed of 0.88 m/s, which demonstrate the agility of our policy.

\subsubsection{Robustness Test}

To evaluate the robustness of our precise foothold controller, we conducted the following experiments on real-world experiment terrains:
\begin{itemize}
    \item \textbf{Heavy Payload}:As shown in Fig.~\ref{fig:robustness}(a), the robot carried a 10 kg payload—approximately 1.5 times the weight of its torso—causing a significant shift in its center of mass. Despite this challenge, the robot effectively maintained agile locomotion and precise foot placements, demonstrating its robustness under increased payload conditions.
    \item \textbf{External Force}: As shown in Fig.~\ref{fig:robustness}(b), the robot was subjected to external forces from various directions. Starting from a stationary pose, the robot experienced external pushes, transitioned to single-leg support, and finally recovered to a stable standing position with two-leg support.
    \item \textbf{Misstep Recovery}: As shown in Fig.~\ref{fig:robustness}(c), the robot traverse terrain without prior scanning of terrain dynamics. Due to occlusions, the robot lacked information about the terrain underfoot, causing initial missteps. Nevertheless, it demonstrated robust recovery capabilities.
\end{itemize}

\subsection{Extensive Studies and Analysis}

\subsubsection{Design of Foothold Reward}

\input{tables/foothold_reward}

As discussed in Section~\ref{sec:method_footholdreward}, our sampling-based foothold reward is proportional to the number of safe points, making it a relatively continuous reward: the larger the overlap between the foot placement and the safe footholds, the higher the reward the agent receives. We compare this approach with other binary and coarse reward designs: when $p\%$ of the sampled points fall outside the safe area, a full penalty is applied; otherwise, no penalty is given. This can be defined as:
\begin{equation}
    r_{\text{foothold}-p\%} = -\sum_{i=1}^2 \mathbb{C}_i \cdot \mathds{1} \left\{ \left( \sum_{j=1}^n \mathds{1} \{ d_{ij} < \epsilon \} \right) \ge p\% \cdot  n\right\}.
\end{equation}

We compare our continuous foothold reward design with three variants of the coarse-grained approach, where $p=30, 50$, and $70$ (denoted as foothold-$30\%$, foothold-$50\%$, and foothold-$70\%$ respectively). The success rate $R_\mathrm{succ}$ and the foothold error $E_\mathrm{foot}$ on stepping stones are reported in Table~\ref{tab:foothold_reward}.

It is clear that our fine-grained design enables the robot to make more accurate foot placements compared to the other designs, as this continuous approach gradually encourages maximizing the overlap. Among the coarse-grained approaches, foothold-$50\%$ performs better than foothold-$30\%$ and foothold-$70\%$, as a $30\%$ threshold is too strict to learn effectively, while $70\%$ is overly loose.

\subsubsection{Design of Curriculum}

\input{tables/curriculum}

To validate the effectiveness of the terrain curriculum introduced in Section~\ref{sec:method_curriculum}, we introduce an ablation study without curriculum learning. In this design, we train using only medium and hard terrain difficulties in both stages (denoted as ``w/o curriculum-medium'' and ``w/o curriculum-hard''). Similarly, we report the $R_\mathrm{succ}$ and $R_\mathrm{trav}$ for both ablation methods, along with our method, on stepping stones terrain at two different difficulty levels in Table~\ref{tab:curriculum}. The results show that incorporating curriculum learning significantly improves both performance and generalization across terrains of varying difficulty. In contrast, without curriculum learning, the model struggles significantly with challenging terrain when learning from scratch  (``ours w/o curriculum-hard''), and also faces difficulties on other terrain types, severely limiting its generalization ability (``ours w/o curriculum-medium'').

\subsubsection{Design of Commands}

As mentioned in Section~\ref{sec:method_curriculum}, in the second stage, no heading command is applied, and the robot is required to learn to consistently face forward through terrain dynamics. We compare this approach with one that includes a heading command (denoted as ``ours w/ heading command''), where deviation from the forward direction results in a corrective yaw command based on the current directional error. In the deployment, we use the LiDAR odometry module to update the heading command in real time, based on the difference between the current orientation and the initial forward direction.

We conduct five trials on the stepping stones terrain in the real world, comparing our proposed method with the ``ours w/ heading command'' design. The success rates are $4/5$ and $1/5$, respectively. The poor performance of the heading command design is primarily due to two factors: (1). In the simulation, the model overfits the angular velocity of the heading command, making it difficult to handle noisy real-world odometry data; (2). In the real world, precise manual calibration of the initial position is required to determine the correct forward direction, making the heading command approach less robust. In contrast, \beamdojo, without heading correction, proves to be more effective.

\subsubsection{Failure Cases and Limitations}

On the one hand, the performance of our method is significantly constrained by the limitations of the perception module. Inaccuracies in LiDAR odometry, along with issues such as jitter and map drift, present considerable challenges for real-world deployment. Furthermore, when processing LiDAR data, the trade-off between the confidence in noisy measurements and the dynamic changes in terrain—such as the jitter of stones, which is difficult to simulate in the simulation—makes it challenging to effectively handle sudden disturbances or variations. As a result, the system struggles to quickly and flexibly adapt to unexpected changes in the environment.

On the other hand, our method has yet to fully leverage the information provided by the elevation map, and has not adequately addressed the challenges of terrains with significant foothold height variations. In future work, we aim to develop a more generalized controller that enables agile locomotion, extending to a broader range of terrains, including stairs and other complex surfaces that require footstep planning, as well as terrains with significant elevation changes.