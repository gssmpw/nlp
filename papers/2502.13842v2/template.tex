% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.


\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
% \usepackage[review]{acl}
\usepackage[]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath} % 引入amsmath宏包以提供各种数学功能
\usepackage{amssymb} % 引入amssymb宏包以使用\mathbb等命令
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx} 
\usepackage{subfig}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{tablefootnote}
\usepackage{array}



% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% User defined
\usepackage{xspace}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
% \newfontfamily\emojifont{DejaVu Sans}
\newcommand{\aname}{\textsc{Lemon}\xspace}
\newcommand{\sjy}[1]{\textcolor{purple}{\emph{[SJY: #1]}}}
\newcommand\blfootnote[1]{%
\begingroup
\renewcommand\thefootnote{}\footnote{#1}%
\addtocounter{footnote}{-1}%
\endgroup
}

\title{\aname: Reviving Stronger and Smaller LMs from Larger LMs \\ with Linear Parameter Fusion}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
    Yilong Chen$^{1,2}$,
    ~Junyuan Shang$^{3\ddagger}$,
    ~Zhenyu Zhang$^{3}$,
    ~\textbf{Shiyao Cui}$^1$,
    ~\textbf{Tingwen Liu}$^{1,2\dagger}$, 
    \\~\textbf{Shuohuan Wang}$^3$\textbf{,}~\textbf{Yu Sun}$^3$\textbf{,}~\textbf{Hua Wu}$^3$ \\ 
    \normalsize $^1$ Institute of Information Engineering, Chinese Academy of Sciences\\
    \normalsize $^2$ School of Cyber Security, University of Chinese Academy of Sciences\\
    \normalsize $^3$ Baidu Inc.\\
    \small \{\texttt{chenyilong, cuishiyao, liutingwen\}@iie.ac.cn} \\
    \small \{\texttt{shangjunyuan, zhangzhenyu07, wangshuohuan, sunyu02\}@baidu.com} \\
}

\begin{document}

\maketitle


\begin{abstract}
% The challenge has sparked innovative research in model training and compression techniques. .Furthermore, our approach exhibits considerable flexibility with controllable layer and parameter operators, and thus can be seen as a unified view of existing pruning-based compression, which potentially pave a way for new research directions.\\, ensuring the preservation of functional capabilities while significantly reducing model size
% Language models (LMs) require significant computational resources, prompting a shift towards smaller, more efficient models without sacrificing performance.  The prevailing compression methods confront the significant challenge of identifying crucial parameters without incurring prohibitive computational costs or compromising performance. Leveraging the observation of high parameter similarity within LMs, our key idea is to reconstruct the functionality of these models more efficiently by selectively fusing similar parameters, thereby reducing redundancy without sacrificing the model's core capabilities. To this end, we propose \aname, a novel approach that employs parameter linear mapping operators to fuse and compress parameters from larger LMs. This technique decomposes the mapping operator into layer and parameter operators on controllable granularity, enabling efficient learning on unsupervised datasets. \aname demonstrates a significant reduction in training budget, saving 62.5\% compared to traditional initial point generation and  achieve superior performance by 6.25\% training budget of open-source models.
In the new era of language models, small models (with billions of parameter sizes) are receiving increasing attention due to their flexibility and cost-effectiveness in deployment. 
However, limited by the model size, the performance of small models trained from scratch may often be unsatisfactory. Learning a stronger and smaller model with the help of larger models is an intuitive idea.
% Given the limited understanding of the function of model parameters, we perform a preliminary analysis and observe the large language models.
Inspired by the observing modular structures in preliminary analysis, we propose \aname to learn competent initial points for smaller models by fusing parameters from larger models, thereby laying a solid foundation for subsequent training.
Specifically, the parameter fusion process involves two operators for layer and dimension, respectively, and we also introduce controllable receptive fields to model the prior parameter characteristics.
In this way, the larger model could be transformed into any specific smaller scale and architecture. 
Starting from LLaMA 2-7B, we revive two stronger and smaller models with 1.3B and 2.7B.
Experimental results demonstrate that the fusion-based method exhibits flexibility and outperforms a series of competitive baselines in terms of both effectiveness and efficiency.
\end{abstract}

\blfootnote{$^\dagger$Corresponding author. $^\ddagger$ Project lead.}
% \blfootnote{*Work Done at Baidu Inc.}

\section{Introduction}
% Task and application 先介绍general task的application，再介绍具体的task setting。(当setting比较新的时候，个人比较推荐这个写法)
% Technical challenge for previous methods (围绕我们解决了的technical challenge展开讨论。Technical challenge包括limitation和technical reason)
% 介绍解决challenge的our pipeline
% Experiment
% Contributions


% 完整版caption
% \textbf{Top}: The traditional pruning method can be regarded as two steps: first search for less important neurons in the model parameters, and then completely remove these neurons to achieve the target architecture of the network.
% \textbf{Bottom}: We propose a new neural network compression method: first, the neurons to be removed are fused with other neurons to retain the functions of these neurons; second, the new neurons are combined into a network of the target architecture


% powerful moderate-sized models .
% 背景 引出做什么事
% Large language models (LLMs) based on the Transformer architecture excel in various natural language tasks, but training them demands substantial computational resources \cite{Gpt-4,claude}. Considering cost-effectiveness during inference and training, Smaller LLMs such as LLaMa \cite{touvronLlamaOpenFoundation2023}, Mistral \cite{jiangMistral7B2023}, are often sufficient for fine-tuning and applications, making them highly popular.  Hence, pre-training a specific structure smaller LLMs  from scratch also incurs significant costs. This makes it meaningful to investigate how parameter knowledge in larger scale models can be retained and transformed to accelerate pre-training of models\cite{chen2015net2net,wangLearningGrowPretrained2023}.

% The high computational costs associated with these processes are prohibitive for most organizations, hindering the widespread use and research of such models.
% We believe that the parametric knowledge within larger-scale models should be preserved and transformed to accelerate the pre-training of models . 

Transformer-based language models (LMs) shine in various natural language tasks due to their powerful understanding and generation capabilities~\cite{claude,Gpt-4,touvronLlamaOpenFoundation2023}. Considering that the training and deployment of large-scale LMs require a large amount of computing resources, small LMs are more cost-effective in actual production environments~\cite {gunasekar2023textbooks,li2023textbooks}. However, in the pre-training stage, it has been proven that there is a certain scaling law between model size and performance~\cite{kaplan2020scaling,hoffmann2022training}. How to break through this limitation to train a stronger and smaller model with the help of large models has recently attracted the attention of researchers~\cite{maLLMPrunerStructuralPruning2023,xiaShearedLLaMAAccelerating2023,xuInitializingModelsLarger2023}.

% Large language models (LLMs) based on the Transformer architecture  in various natural language tasks, but training them demands substantial computational resources . Considering cost-effectiveness during inference and training, Smaller LLMs such as LLaMa \cite{}, Mistral \cite{jiangMistral7B2023}, are often sufficient for fine-tuning and applications, making them highly popular.  Hence, pre-training a specific structure smaller LLMs  from scratch also incurs significant costs. This makes it meaningful to investigate how parameter knowledge in larger scale models can be retained and transformed to accelerate pre-training of models\cite{chen2015net2net,wangLearningGrowPretrained2023}.


% related work 从本质出发
% Previous work has focused on using parts of the parameters from larger models as initial points for training smaller models. Employing layer dropout \cite{zhangAcceleratingTrainingTransformerBased2020}, weight selection  \cite{xuInitializingModelsLarger2023}, and structured pruning \cite{maLLMPrunerStructuralPruning2023,xiaShearedLLaMAAccelerating2023} to preserve essential parameters as  dinitialization points can significantly speed up training and sometimes even enhance performance. However, methods based on gradients and rules for selecting parameters to retain in the model do not effectively reduce performance loss when downsizing model parameters. Furthermore, due to the large number of parameters in large models, methods searching for pruning parameters require significant computational resources. Most importantly, for general-purpose LLMs, pruning parameters inevitably leads to performance degradation in certain tasks \cite{frantarSparseGPTMassiveLanguage2023}.
%  , where the smaller model is used to provide an powerful initial point for training larger model
Inspired by the idea of function-reserving transformation in efficient training~\cite{chen2015net2net},
there are some model compression efforts that utilize layer dropping, structured pruning, or weight selection to preserve the parameter functionality of large models as much as possible, so as to accelerate pre-training and improve the final performance of small models~\cite{zhangAcceleratingTrainingTransformerBased2020,xia2022structured,xuInitializingModelsLarger2023}. 
However, since there are tens or even hundreds of billions of parameters in modern LMs, searching for the essential parameter combinations requires significant computing resources, while pruning inappropriate parameters inevitably leads to performance degradation in downstream tasks~\cite{frantarSparseGPTMassiveLanguage2023}.
In this case, how to construct an appropriate initial point with capabilities close to large LMs at the lowest possible cost remains an open problem.

% Parameters within a cluster exhibiting similar properties and connectivity
% In preliminary experiments, we observe that modular structures with a high degree of similarity in LLMs. The distribution of parameters modulars exhibits multiple cluster-like structures, with greater similarity between the parameters of multiple neighboring layers within clusters, and less similarity between clusters and clusters \cite{qinExploringModeConnectivity2022,garipovLossSurfacesMode2018,zhangCRaShClusteringRemoving2023}. The phenomenon verified that layers within clusters may have highly similar behavioural functions. Based on this observation, we believe that similar parameters can be fused to reduce the redundancy present in the model. In our work we do not discard model parameters, instead we reconstruct new parameters so that they retain the functionality of multiple original parameters. We compress the model's parameter through learning a linear mapping operator that fuses model parameters . During the learning process,  Mapping operator linearly weights multiple parameters in the larger model before fusing them into a single parameter in the smaller model, with the goal of maintaining the same parameter function before and after fusion. 


% 原版 caption
% \textbf{Left} panel of each subfigure shows the similarity relationship between layers and layers. \textbf{Right} panel shows the trend of similarity between layers and layers with increasing layer depth. It can be observed that there are clusters of layers with highly similar parameters in the model, which indicates that the model exhibits a modular differentiation of the functional parameters.


Given the limited understanding of parameter characteristics and roles in modern large LMs, we perform an empirical analysis from the perspectives of representation similarity, and observe that there are some modular structures with high internal similarity. 
In particular, there is a high similarity in the parameters of adjacent layers, while the similarity of distant layers to a lesser extent, which coincides with the sparsity found in previous studies~\cite{qinExploringModeConnectivity2022,zhangCRaShClusteringRemoving2023}.
% We intuitively speculate that layers within the same module may have similar behavioral functions, and thus these parameters can be fused to reduce model redundancy. , 
Nonetheless, for a parameter module, each part has its own unique role, and thus no model parameter should be arbitrarily discarded. 
Based on the observation, we believe that the functionality of a module should be reconstructed by selectively fusing corresponding parameters during the initial point construction phase, as illustrated in Figure~\ref{fig:method-compare}.
% We intuitively speculate that layers within the same module may have similar behavioral functions, and thus these parameters can be fused to reduce model redundancy.

% we observe that modular structures with a high degree of similarity in LLMs. The distribution of parameters modulars exhibits multiple cluster-like structures, with greater similarity between the parameters of multiple neighboring layers within clusters, and less similarity between clusters and clusters \cite{qinExploringModeConnectivity2022,garipovLossSurfacesMode2018,zhangCRaShClusteringRemoving2023}. The phenomenon verified that layers within clusters may have highly similar behavioural functions. Based on this observation, we believe that similar parameters can be fused to reduce the redundancy present in the model. In our work we do not discard model parameters, instead we reconstruct new parameters so that they retain the functionality of multiple original parameters. 

% In this paper, we propose \aname, which reviving smaller LMs from larger LMs with parameter linear combination. In practical terms, we compress the model's parameter through learning a linear mapping operator that fuses model parameters. During the learning process,  Mapping operator linearly weights multiple parameters in the larger model before fusing them into a single parameter in the smaller model, with the goal of maintaining the same parameter function before and after fusion. 
% We firstly design mapping operators and optimise algorithms so that models can be freely compressed to arbitrary architectures. We decompose mapping operator into a combination of  layer compression operators and sparse parameter compression operators. The layers compression operators represent the fusible relationship of layer dimensions between the large and small models, while the parameter compression operators learn the fusible relationship of neuron combinations.  Due to the large amount of model parameters, directly learning mapping operators is very difficult and resource-intensive, necessitating restrictions operators. We further factorize the  parameter compression operators into a matrix Kronecker product with a controllable receptive field. This allows the method to fuse very high dimensional parameters based on a combination of different granularities, from single neurons to attention heads. We use the modular characteristics of the model parameter structure as the initialization points for mapping operators and optimize the mapping operators on unlabeled data. Based on a well learnt mapping operators, we can quickly obtain a good initial point for smaller LLMs.


% first fuse the parameters of a larger model with fusion operators,
In this paper, we propose \aname to construct compact initial points for small models through linear fusion of large model parameters.
Specifically, we first identify the importance of parameters in larger models with fusion operators, and then compress them into new parameters for the smaller model, maintaining the same functionality of related parameters before and after transformation.
To support mapping a larger model into any specific architecture, we decompose the fusion process into layer and dimension operations. The layer operator establishes layer-to-layer correspondence between larger and smaller models, while the dimension operator aims to learn the fusion of different dimensions within each parameter matrix.
Recalling the modular structures among layers, we further factorize the fusion operators into Kronecker products~\cite{schacke2004kronecker} with controllable receptive fields, instead of directly learning it on a huge number of parameters.
Eventually, the fusion operators are initiated by the prior features of modular parameters and optimized on unlabeled data.
Based on well-learned operators, it is convenient to efficiently achieve compact initial points for smaller models and facilitate subsequent training.
% We firstly design mapping operators and optimise algorithms so that models can be freely compressed to arbitrary architectures. We decompose mapping operator into a combination of  layer compression operators and sparse parameter compression operators. The layers compression operators represent the fusible relationship of layer dimensions between the large and small models, while the parameter compression operators learn the fusible relationship of neuron combinations.  Due to the large amount of model parameters, directly learning mapping operators is very difficult and resource-intensive, necessitating restrictions operators. This allows the method to fuse very high dimensional parameters based on a combination of different granularities, from single neurons to attention heads. We use the modular characteristics of the model parameter structure as the initialization points for mapping operators and optimize the mapping operators on unlabeled data. Based on a well learnt mapping operators, we can quickly obtain a good initial point for smaller LLMs.

% how to determine the range of model parameters that can be fused and the linear ratio of parameter fusion to avoid noise generated in the fusion.

% We compress the model's parameter through learning a linear mapping operator that fuses model parameters. i.e. $\Theta^{(\text {small})}=M \Theta^{(\text {large})}$, where $M$ is a vectorized parameter that fuses and maps  from the larger LLM parameters $\Theta^{(\text {large})}$ to small LLM parameters $\Theta^{(\text {small})}$. There are two challenges in this process: Firstly, due to the large number of model parameters, directly learning $M$ is very difficult and resource-intensive, necessitating restrictions on $M$. Secondly, how to determine the range of model parameters that can be fused and the linear ratio of parameter fusion to avoid noise generated in the fusion.



% \begin{figure}
%     \centering
%     \includegraphics[width=0.75\linewidth]{figs/sample_show.png}
%     \caption{Enter Caption}
%     \label{fig:enter-label}
% \end{figure}

% Our method demonstrates considerable flexibility and adaptability. By strategically controlling the depth and breadth of compression matrices, we can compress any model into any specified target architecture. Our compression operators can represent existing parameter selection and pruning methods, achieving a good initialization point for new models with just 800 steps of learning. This significantly accelerates the pre-training speed of smaller LLMs. The efficacy of our approach is exemplified by compressing the LLaMA2-7B model into two smaller LLMs. Compared to traditional training from scratch, our method requires only 6.25\% of the training tokens configuration to achieve superior performance. It saves 62.5\% of training tokens relative to existing methods and obtains performance enhancements under the same pre-training configuration. Furthermore, our method exhibits excellent compatibility with and representation of existing pruning compression techniques. This could potentially pave the way for a new research direction.

% Following previous work~\cite{xiaShearedLLaMAAccelerating2023}, 
To verify the effectiveness, we conduct experiments on the LLaMA 2-7B model~\cite{touvronLlamaOpenFoundation2023} and compress it into two smaller models, 1.3B and 2.7B, respectively. 
Within only 0.26 billion tokens, it is sufficient to learn competent initial points for smaller models, and with an additional 50 billion tokens of post-training, \aname outperforms a series of powerful baselines on 11 representative downstream tasks.
Meanwhile, compared to traditional models trained from scratch~\cite{biderman2023pythia,Redpajama,Openllama}, \aname only requires 5.26\% of pre-training token budget. When achieving comparable performance to recent pruning-based method~\cite{xia2022structured}, the learning process of our fusion-based method saves 62.5\% of training budget.
Overall, \aname exhibits considerable flexibility with controllable layer and dimension operation,  making it a unified view of existing model compression techniques, which explore a new way for model compression using in-model parameters fusion.

% Our compression operators can represent existing parameter selection and pruning methods, achieving a good initialization point for new models with just 800 steps of learning. This significantly accelerates the pre-training speed of smaller LLMs. The efficacy of our approach is exemplified by compressing the LLaMA2-7B model into two smaller LLMs. Compared to traditional training from scratch, our method requires only 6.25\% of the training tokens configuration to achieve superior performance. It saves 62.5\% of training tokens relative to existing methods and obtains performance enhancements under the same pre-training configuration. 




% Our method demonstrates good flexibility and adaptability. By strategically controlling the depth and width compression matrices, we can compress any model to any specified target architecture. Our compression operator can represent existing parameter selection and pruning methods, requiring only minimal learning of $M$ to obtain a good initialization point for the new model. This significantly accelerates the pre-training of moderate-sized LLMs. We demonstrate the effectiveness of our method by compressing the LLaMA2-7B model into two smaller LLMs. Compared to traditional training from scratch, our approach saves xxxx FLOPS and achieves superior performance. Relative to structured pruning methods, it saves xxx FLOPS and xxxx training tokens, obtaining performance improvements with the same level of training tokens. Ultimately, our model outperforms other similarly sized popular LLMs in 11 representative downstream tasks (Figure 1; common sense, reading comprehension, and world knowledge) and in open-ended generative instruction tuning, such as xxx, xxx, xxx.

% The function of the approximated parameters is approximated. By merging similar parameters, we can reduce redundancy and retain the original performance of the model.

\section{Observation}


\section{Method}


\section{Experiments}


\subsection{Setup}

% For the purpose of learning \textbackslash{}aname operators and continued pre-training, we employ the RedPajama\~\cite\{Redpajama\} dataset. This dataset is designed to mimic the training dataset of LLaMA1, encompassing training data from seven domains: CommonCrawl, C4, Github, Wikipedia, Books, ArXiv, and Stack-Exchange. In alignment with LLaMA's configuration, we utilize a sequence length of 4096. Our dataset includes a validation set of 2 million tokens, a training set of 4 billion tokens, and a continued pre-training dataset comprising 50 billion tokens.


% Our implementation is based on the ShearingLLM code available in the Composer software package. All experiments were conducted on 8 Nvidia A100 GPUs (80GB). To ensure rigorous comparison, we replicated baselines such as Shearing-LLM using the same data and experimental settings, following the original experimental setup and dynamic batch gradient loading for 6400 steps. \aname was trained for 800 steps. For all obtained initial points, continued pre-training was conducted using 50 billion tokens with the dynamic batch gradient loading algorithm.\footnote{Notably, the dynamic batch gradient loading algorithm was not employed during the \aname training phase.}  For the lm loss, we employed a learning rate of 1e-4, while for the kl loss in depth operators and width operators, learning rates of 5e-4 and 5e-5 were used respectively. The global train batch size for all experiments was set at 32. We performed evaluations using different random seeds and averaged the results for reliability.

\paragraph{Data} To learn \aname operators and continue pre-training, we use the RedPajama~\cite{Redpajama} dataset, mirroring the training data in LLaMA across seven domains (CommonCrawl, C4, Github, Wikipedia, Books, ArXiv, Stack-Exchange). The dataset includes a validation set of 2 million tokens, a training set of 4 billion tokens, and a continued pre-training set of 50 billion tokens.

\paragraph{Training} Our setup uses Sheared-LLaMA~\cite{xiaShearedLLaMAAccelerating2023} code base on Composer package~\cite{mosaicml2022composer}, tested on 8 NVIDIA A100 GPUs (80GB). The models were trained at the sequence length of 4096 using a batch size of 32 during the pruning phase and continued pre-training phase. Following the setup in Sheared-LLaMA~\cite{xiaShearedLLaMAAccelerating2023}, baseline models were trained for 6400 steps (22 hours) with dynamic batch gradient loading (DoReMi~\cite{DBLP:journals/corr/doremi}) during the pruning phase. In contrast, \aname were trained for 1200 steps (8 hours) without DoReMi. For continued pre-training, we employed 50 billion tokens with DoReMi with 256 batch size (387 hours). Learning rates were set to 1e-4 for language modeling loss, and for layer and dimension operators, 5e-4 and 5e-5, respectively. For more information, please refer to Appendix~\ref{app:train}.

\paragraph{Evaluation} Following the multiple assessment metrics reported in the baseline studies~\cite{xiaShearedLLaMAAccelerating2023}, we utilized lm-evaluation-harness~\cite{eval-harness} to evaluate our models. In common sense and reading comprehension evaluation, we report 0-shot accuracy of SciQ~\cite{sciqa}, PIQA~\cite{piqa}, WinoGrande (Abbreviated as Wino.)~\cite{WinoGrande:conf/aaai/SakaguchiBBC20}, ARC Easy~\cite{clark2018think},  HellaSwag~\cite{HellaSwag:conf/acl/ZellersHBFC19}, 25-shot accuracy of ARC Challenge~\cite{arcChallenge:journals/corr/abs-1803-05457}. In 
world knowledge evaluation, we report accuracy of 32-shot NQ~\cite{NQ:journals/tacl/KwiatkowskiPRCP19} and 5-shot MMLU~\cite{mmlu:conf/iclr/HendrycksBBZMSS21}. In Continued QA and  text understanding evaluation, we report accuracy of 0-shot LogiQA~\cite{liu2020logiqa}, 32-shot BoolQ~\cite{clark2019boolq} and 0-shot LAMBADA~\cite{paperno2016lambada}.


\subsection{Baseline}
\paragraph{Structured Pruning}Methods construct initial points by searching and eliminating non-essential neurons, followed by continued pre-training. We designate Sheared-LLaMA~\cite{xiaShearedLLaMAAccelerating2023} as our primary baseline. Sheared-LLaMA provided a robust training framework, dynamic batch gradient pre-training methods, and an evaluation framework, achieving competitive outcomes.

\paragraph{Pre-training from Scratch Baseline}Models train from scratch without utilizing any pre-existing model parameters. We choose similar-scale open-source LLMs includes Pythia-1.4B \& -2.8B~\cite{biderman2023pythia}, which use different training data than RedPajama. We choose INCITE-Base-3B~\cite{Redpajama} as baseline which use the same training data.




\subsection{Result}



\noindent\textbf{Foundational Capabilities}. 
% we require fewer training resources and have faster training speed due to our sparser and cleaner matrices. Compared to methods that select some parameters for the initialization of a new model,  performance also outperforms the INCITE-Base-3B and OpenLLaMA-3B-v1 models pre-trained on 800B and 1T RedPajama tokens. the zero-shot and few-shot
Table \ref{tab:main_result} shows the foundational capacity performance of the model after initialization with \aname operators then continued pre-training with 50B tokens. We also evaluate  capabilities of similarly sized models and models built from pruning methods. Experiments show that our approach achieves even better performance with very little training resource overhead (i.e. 50B tokens of training data) compared to models built from scratch. \aname-1.3B outperforms the Pythia-1.4B models, which were initially pretrained with 300B tokens. \aname-2.7B also outperforms 300B tokens pretrained Pythia-1.4B models and 300B tokens pretrained INCITE-Base-3B. Compared to prune-based initialization methods, we obtain better initial points retaining the knowledge of larger model, which leads to substantial savings in pre-training. Our \aname-2.7B, \aname-1.3B outperforming  Sheared-LLaMA-2.7B and Sheared-LLaMA-1.3B respectively.
\\\\
\noindent\textbf{Better Initialization}. We check whether \aname produces a better initialization than existing language models of the same size. We continue to pre-train the \aname-2.7B model on the original RedPajama data, comparing it to the INCITE-Base-3B adn Sheared-LLaMA-2.7B model. Figure 4 shows the INCITE-Base-3B model has much higher initial accuracy, but its performance stagnates throughout the continued pre-training. Sheared-LLaMA and \aname-2.7B has lower initial accuracy but rapidly improves and eventually outperforms the INCITE-Base-3B model. \aname-2.7B has higher initial accuracy and  final performance than Sheared-LLaMA-2.7B. The parameter-fused model outperforms the pruned model and is more suitable for initialising a strong model for further pre-training.
\\\\
\noindent\textbf{Fast Convergence}. We report the Eval PPL during training process for \aname and Sheared-LLaMA in the same configuration. As shown in the Figure~\ref{fig: Analyse} (Middle), although the initial point is not sufficiently satisfactory, \aname can converge quickly within 2,000 steps, using only 33\% of Sheared-LLaMA training steps. \aname reaches a PPL of 7.47 at 2,000 steps, which is below the 7.49 PPL of Sheared-LLaMA at 4,800 steps. The PPL of \aname achieves parity with Sheared-LLaMA at 500 steps and surpass the Sheared-LLaMA by 3.5 point at 1000 steps, demonstrating the ability to effectively retain the information in parameters. We can continue to allocate the \aname training budget to reduce the initial PPL, which involves a trade-off in computational resource allocation between compression and pre-training.


 \subsection{Analysis}



\section{Related Work}

\paragraph{Efficient pre-training approaches}
% The training of large language models (LLMs) is notably resource-intensive, prompting research into more efficient methods. Knowledge distillation utilizes the output of a larger "teacher" model to direct the "student" model's learning, aiming for performance gains at reduced computational costs. Yet, for large-scale transformer models, the initial overhead of distillation is prohibitively high, making it less feasible for extensive applications.
% Incremental training has emerged as a promising alternative, focusing on finding the optimal starting point for training to speed up the process. Techniques such as asymptotic training begin with smaller-scale transformers and incrementally increase complexity by adding layers, a method conducive to scaling up models efficiently. The net2Net strategy and its adaptations, including bert2BERT for transformers, employ functional transformations to increase model size without starting from scratch. LiGO introduces a novel concept of learnable model expansion, allowing transformers to grow from an initial setup flexibly.
% Our research builds upon these foundations but shifts the focus from enhancing the initial training point of larger models based on smaller counterparts to exploring parameter matrix mapping from large to small models without compromising the larger model's capabilities. This approach intends to maintain, if not improve, the efficiency and effectiveness of LLMs by leveraging the structural insights gained from compact models, thereby offering a novel pathway to resource-efficient model scaling.
% Due to the huge overhead of training large language models, there has been a lot of work focusing on how to improve performance and train at low cost based on existing models. Knowledge distillation methods \cite{} introduce the distribution of inference results from the teacher's model to guide the training direction of the student's model during the training period. However, compared to our approach of learning mapping initialization combined with continuous pre-training, the inference cost induced by the teacher model for the cold-start distillation approach is too high to be utilized for large-scale transformer models. bert2BERT\cite{} extends Net2Net to transformers.  Mask LLM\cite{} adjusts the expanded parameter mask during training to grow the model incrementally. hile current methods focus on obtaining a better initial point for the larger model from the smaller model, 
In recent years, the ability of incremental training to accelerate large-scale model training by studying how to obtain the optimal initialization point for training has thus attracted much attention ~\cite{DBLP:conf/cvpr/XieXP17,DBLP:conf/nips/WuW019}. Asymptotic training~\cite{zhangAcceleratingTrainingTransformerBased2020} is used to learn larger-scale models by training small transformers with a small number of layers, and then gradually expanding them by stacking layers. Net2Net~\cite{chen2015net2net} uses function-holding transformations to expand the width by duplicating neurons, and uses a unitary layer implementation to expand the depth. LiGO~\cite{wangLearningGrowPretrained2023} proposes a learnable expansion method that can be used at the initial initialization point of a transformer. learned expansion methods that can expand models of arbitrary structure at initialization. \aname is inspired by these methods, but we investigate how to learn to map the parameter matrix from large to small without losing the ability of the larger model itself.

\paragraph{Model Compression}
Our approach is dedicated to obtaining a high-performance lightweight language model, which is the same goal as the task of model compression. Quantization~\cite{gray1998quantization} reduces the numerical accuracy of model weights and activations, and speeds up training and inference, but results in a loss of model accuracy and the inability to freely build target-specific models. CRash~\cite{zhangCRaShClusteringRemoving2023} and LayerDrop~\cite{zhang2020accelerating,sajjad2023effect} methods discard ineffective layers during training, which do not allow for target-specific structuring and come with a large performance loss. Pruning~\cite{wangStructuredPruningLarge2020a} minimizes the impact on performance by cutting out redundant neurons that over-parameterize the model. In the LLM era, this leads to a significant reduction in neuron redundancy as models move from task-specific to generalized~\cite{frantarSparseGPTMassiveLanguage2023}. Pruning LLM leads to performance degradation at larger pruning magnitudes. LLMsheairng~\cite{xiaShearedLLaMAAccelerating2023} uses the results of pruning as initialization for continuous pre-training of the model to recover performance, but this approach requires more data and computational overhead. We avoid the information loss caused, by learning the parameter fusion matrix of the model to reach a specific structure, thus obtaining better initialization points and reducing the overhead of continuous pre-training.
% Unstructured pruning\cite{} tends to prune individual neurons and is not practical as it does not allow for a standard model architecture. The current mainstream methods are quantization~\cite{}, layer dropping~\cite{} and pruning~\cite{maLLMPrunerStructuralPruning2023,wangStructuredPruningLarge2020a}. 

% \section{Discussion}



\section{Conclusion}

In this paper, we propose a new paradigm for building smaller LMs based on larger LMs. By learning a parameter fusion operator with controllable receptive fields from a larger model to smaller models, we can obtain a good starting point to facilitate subsequent training. This fusion mapping operator consists of a layer operator and a dimension operator. Experimental results demonstrate that our method can compress larger LMs into smaller LMs of arbitrary architectures and better preserve the knowledge in large models. By comprising the requirements of pre-training data, we demonstrate the effectiveness of the method against several baseline approaches in terms of training speedup and computational expenditure savings. The paradigm has greater research value and has the potential to reach smaller performance losses and lower computational effort in the future.

\section*{Limitation}
There are limitations to our approach: First, we have only explored the use of linear methods for parameter fusion in our model. In the future, nonlinear methods deserve more exploration as they have the potential to better link different parameters and reach optimality. Second, limited by computational resources, we only experiment on 7B-scale and 3B-scale models. However, our method is scalable (discussed in Appendix~\ref{scalability}) and can be extended to models of arbitrary size in future work. Third, although we minimize the computational complexity of the fusion operator, it still requires a lot of memory and computation for optimization. Finally, we believe that the fusion of parameters within models is a direction that has not been fully explored. How to solve the information loss caused by parameter fusion is a problem we will try to solve in the future.

\section*{Ethical Consideration}
In our study, we leverage open and online accessible data and techniques, mitigating privacy concerns. Our method emphasizes enhancing model parameter efficiency and reducing size to create powerful, compact, and openly accessible models, thereby promoting the open dissemination and democratization of NLP technologies. By employing pre-training strategies, we aim to mitigate biases through extensive training and large corpus sizes, contributing to an ethical AI development that prioritizes openness, efficiency, and bias reduction. Our work is committed to advancing accessible and efficient NLP technologies, fostering a more inclusive and automated future for AI.

% \section*{Acknowledgments}
% We thank the anonymous reviewers for their insightful comments and constructive suggestions.
% We would like to thank Yinqi Yang, Jiawei Sheng, Xinhua Zhang, Shicheng Wang, Chuanyu Tang and members of the IIE KDsec group for their valuable feedback and discussions. 
% We are very grateful to Mengzhou Xia for providing the concise and effective ShearingLLaMA experimental code and for her assistance during the reproduction process. Work done during Yilong Chen's internship in Baidu Inc. This research is supported by the National Key Research and Development Program of China (Grant No.2021YFB3100600) and the Youth Innovation Promotion Association of CAS (Grant No.2021153).


% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix
% \newpage
% \newpage
\section{Appendix}\label{sec:appendix}
