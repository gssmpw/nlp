\begin{table}[t]
\centering

% \vskip 0.15in
\small{\resizebox{0.9\columnwidth}{!}{%
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Model Setting} & \textbf{L.2-162M} &  \textbf{L.2-230M} &   \textbf{L.2-466M} \\ \midrule
\textit{hidden size }        & 1024  & 1536 & 2048  \\
\textit{intermediate size }        &  2560  & 2560 & 4096  \\
\textit{attention heads}         &  32  & 32 & 32  \\
\textit{num kv heads}      &  32  & 16 & 32 \\
\textit{layers }         & 8  & 8 & 8\\
\midrule
% \textbf{\# Activate}  & 162M   & 230M &  \\
\textbf{\# Params}  & 162M   & 230M & 466M \\
\bottomrule
\end{tabular}
}}
\caption{Detailed configuration, activation parameters, and total parameters of the models included in our study. L.2-162M represents the LLaMA-2 architecture model with 162M total parameters.}
\label{tab:model_setting}
\end{table}