% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

% Inference-based evict methods
@misc{hao2024traininglargelanguagemodels,
      title={Training Large Language Models to Reason in a Continuous Latent Space}, 
      author={Shibo Hao and Sainbayar Sukhbaatar and DiJia Su and Xian Li and Zhiting Hu and Jason Weston and Yuandong Tian},
      year={2024},
      eprint={2412.06769},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.06769}, 
}

@book{braitenberg1986vehicles,
  title={Vehicles: Experiments in Synthetic Psychology},
  author={Braitenberg, Valentino},
  year={1986},
  publisher={MIT Press},
  address={Cambridge, MA},
  isbn={9780262521123},
  url={https://mitpress.mit.edu/9780262521123/vehicles/}
}

@article{gers2000lstm,
  title={Learning to Forget: Continual Prediction with LSTM},
  author={Felix Alexander Gers and J{\"u}rgen Schmidhuber and Fred Cummins},
  journal={Neural Computation},
  year={2000},
  volume={12},
  pages={2451-2471},
  url={https://api.semanticscholar.org/CorpusID:11598600}
}

% Transformer扩展
@inproceedings{dehghani2019universal,
  title={Universal Transformers}, 
  author={Mostafa Dehghani and Stephan Gouws and Oriol Vinyals and Jakob Uszkoreit and Łukasz Kaiser},
  year={2019},
  eprint={1807.03819},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/1807.03819}, 
}

@misc{lan2019albert,
  title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations}, 
      author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
      year={2020},
      eprint={1909.11942},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1909.11942}, 
}

% 优化方法

@phdthesis{schwarzschild2021randomized,
  author       = {A. Schwarzschild},
  title        = {Deep Thinking Systems: Logical Extrapolation with Recurrent Neural Networks},
  school       = {University of Maryland, College Park},
  year         = {2023},
  url          = {https://www.proquest.com/dissertations-theses/deep-thinking-systems-logical-extrapolation-with/docview/2830027656/se-2}
}


@article{lecun2006contrastive,
  title={A fast learning algorithm for deep belief nets},
  author={Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
  journal={Neural computation},
  volume={18},
  number={7},
  pages={1527--1554},
  year={2006},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

% 最新进展
@inproceedings{hu2021loralowrankadaptationlarge,
      title={LoRA: Low-Rank Adaptation of Large Language Models}, 
      author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
      year={2021},
      eprint={2106.09685},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2106.09685}, 
}

@article{liu2024transformer,
      title={Dynamic Layer Tying for Parameter-Efficient Transformers}, 
      author={Tamir David Hay and Lior Wolf},
      year={2024},
      eprint={2401.12819},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.12819}, 
}

% 其他关键引用
@inproceedings{boudiaf2022testtime,
  title={Beyond Model Adaptation at Test Time: A Survey}, 
      author={Zehao Xiao and Cees G. M. Snoek},
      year={2024},
      eprint={2411.03687},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.03687}, 
}


@inproceedings{mcleish2024,
  title={Understanding Generalization in Recurrent Neural Networks},
  author={Zhuozhuo Tu and Fengxiang He and Dacheng Tao},
  booktitle={International Conference on Learning Representations},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:214346647}
}



@article{elhoushi2024layerskip,
  title={LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding},
   url={http://dx.doi.org/10.18653/v1/2024.acl-long.681},
   DOI={10.18653/v1/2024.acl-long.681},
   booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
   publisher={Association for Computational Linguistics},
   author={Elhoushi, Mostafa and Shrivastava, Akshat and Liskovich, Diana and Hosmer, Basil and Wasti, Bram and Lai, Liangzhen and Mahmoud, Anas and Acun, Bilge and Agarwal, Saurabh and Roman, Ahmed and Aly, Ahmed and Chen, Beidi and Wu, Carole-Jean},
   year={2024},
   pages={12622–12642} 
}

@article{chen2023ee_llm,
  title={EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models with 3D Parallelism}, 
  author={Yanxi Chen and Xuchen Pan and Yaliang Li and Bolin Ding and Jingren Zhou},
  year={2024},
  eprint={2312.04916},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2312.04916}, 
}

@misc{zhang2024pmodbuildingmixtureofdepthsmllms,
      title={p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay}, 
      author={Jun Zhang and Desen Meng and Ji Qi and Zhenpeng Huang and Tao Wu and Limin Wang},
      year={2024},
      eprint={2412.04449},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2412.04449}, 
}

@misc{raposo2024mixtureofdepthsdynamicallyallocatingcompute,
      title={Mixture-of-Depths: Dynamically allocating compute in transformer-based language models}, 
      author={David Raposo and Sam Ritter and Blake Richards and Timothy Lillicrap and Peter Conway Humphreys and Adam Santoro},
      year={2024},
      eprint={2404.02258},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.02258}, 
}

@misc{li2024lisa,
  title={Cross-layer Attention Sharing for Large Language Models}, 
      author={Yongyu Mu and Yuzhang Wu and Yuchun Fan and Chenglong Wang and Hengyu Li and Qiaozhi He and Murun Yang and Tong Xiao and Jingbo Zhu},
      year={2024},
      eprint={2408.01890},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.01890}, 
}


@misc{li2024crosslayer,
  title={Basis Sharing: Cross-Layer Parameter Sharing for Large Language Model Compression}, 
      author={Jingcun Wang and Yu-Guang Chen and Ing-Chao Lin and Bing Li and Grace Li Zhang},
      year={2024},
      eprint={2410.03765},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.03765}, 
}


@misc{deng2023implicit,
  title={Implicit Chain of Thought Reasoning via Knowledge Distillation}, 
      author={Yuntian Deng and Kiran Prasad and Roland Fernandez and Paul Smolensky and Vishrav Chaudhary and Stuart Shieber},
      year={2023},
      eprint={2311.01460},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.01460}, 
}

@misc{shalev2024distributional,
  title={Distributional reasoning in LLMs: Parallel reasoning processes in multi-hop reasoning}, 
      author={Yuval Shalev and Amir Feder and Ariel Goldstein},
      year={2024},
      eprint={2406.13858},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.13858}, 
}


@misc{zelikman2024quiet,
  title={Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking}, 
  author={Eric Zelikman and Georges Harik and Yijia Shao and Varuna Jayasiri and Nick Haber and Noah D. Goodman},
  year={2024},
  eprint={2403.09629},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2403.09629}, 
}

@misc{madaan2022text,
  title={Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango}, 
  author={Aman Madaan and Amir Yazdanbakhsh},
  year={2022},
  eprint={2209.07686},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2209.07686}, 
}

@misc{jiang2024peek,
  title={A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners}, 
  author={Bowen Jiang and Yangxinyu Xie and Zhuoqun Hao and Xiaomeng Wang and Tanwi Mallick and Weijie J. Su and Camillo J. Taylor and Dan Roth},
  year={2024},
  eprint={2406.11050},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2406.11050}, 
}

@misc{singh2024exposing,
  title={Exposing the Achilles' Heel: Evaluating LLMs Ability to Handle Mistakes in Mathematical Reasoning}, 
  author={Joykirat Singh and Akshay Nambi and Vibhav Vineet},
  year={2024},
  eprint={2406.10834},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2406.10834}, 
}

@misc{chen2023token,
  title={Critical Tokens Matter: Token-Level Contrastive Estimation Enhances LLM's Reasoning Capability}, 
  author={Zicheng Lin and Tian Liang and Jiahao Xu and Qiuzhi Lin and Xing Wang and Ruilin Luo and Chufan Shi and Siheng Li and Yujiu Yang and Zhaopeng Tu},
  year={2025},
  eprint={2411.19943},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2411.19943}, 
}


@misc{ma2024inference,
  title={Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps}, 
  author={Nanye Ma and Shangyuan Tong and Haolin Jia and Hexiang Hu and Yu-Chuan Su and Mingda Zhang and Xuan Yang and Yandong Li and Tommi Jaakkola and Xuhui Jia and Saining Xie},
  year={2025},
  eprint={2501.09732},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2501.09732}, 
}


@misc{snell2024scaling,
  title={Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters}, 
  author={Charlie Snell and Jaehoon Lee and Kelvin Xu and Aviral Kumar},
  year={2024},
  eprint={2408.03314},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2408.03314}, 
}


@misc{muennighoff2025simple,
  title={s1: Simple test-time scaling}, 
  author={Niklas Muennighoff and Zitong Yang and Weijia Shi and Xiang Lisa Li and Li Fei-Fei and Hannaneh Hajishirzi and Luke Zettlemoyer and Percy Liang and Emmanuel Candès and Tatsunori Hashimoto},
  year={2025},
  eprint={2501.19393},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2501.19393}, 
}


@misc{chen2024scaling,
  title={Scaling Laws Across Model Architectures: A Comparative Analysis of Dense and MoE Models in Large Language Models}, 
  author={Siqi Wang and Zhengyu Chen and Bei Li and Keqing He and Min Zhang and Jingang Wang},
  year={2024},
  eprint={2410.05661},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2410.05661},
}



@article{fernandez2024hardware,
  title={Hardware Scaling Trends and Diminishing Returns in Large-Scale Distributed Training},
  author={Fernandez, Jared and Wehrstedt, Luca and Shamis, Leonid and Elhoushi, Mostafa and Saladi, Kalyan and Bisk, Yonatan and Strubell, Emma and Kahn, Jacob},
  journal={arXiv preprint arXiv:2411.13055},
  year={2024},
  url={https://arxiv.org/abs/2411.13055}
}

@misc{li2024happenedllmslayerstrained,
      title={What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective}, 
      author={Ming Li and Yanhong Li and Tianyi Zhou},
      year={2024},
      eprint={2410.23743},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.23743}, 
}

@misc{alizadeh2024duollmframeworkstudyingadaptive,
      title={Duo-LLM: A Framework for Studying Adaptive Computation in Large Language Models}, 
      author={Keivan Alizadeh and Iman Mirzadeh and Hooman Shahrokhi and Dmitry Belenko and Frank Sun and Minsik Cho and Mohammad Hossein Sekhavat and Moin Nabi and Mehrdad Farajtabar},
      year={2024},
      eprint={2410.10846},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.10846}, 
}

@misc{takase2023lessonsparametersharinglayers,
      title={Lessons on Parameter Sharing across Layers in Transformers}, 
      author={Sho Takase and Shun Kiyono},
      year={2023},
      eprint={2104.06022},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2104.06022}, 
}

@misc{sun2025transformerlayerspainters,
      title={Transformer Layers as Painters}, 
      author={Qi Sun and Marc Pickett and Aakash Kumar Nain and Llion Jones},
      year={2025},
      eprint={2407.09298},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.09298}, 
}

@article{wu2024multihead,
    title={Multi-head mixture-of-experts},
    author={Wu, X. and Huang, S. and Wei, F.},
    journal={arXiv preprint arXiv:2404.15045},
    year={2024},
    url={https://arxiv.org/abs/2404.15045}
}

@article{you2022lazyneuron,
    title={The lazy neuron phenomenon: On emergence of activation sparsity in transformers},
    author={You, C. and Bhojanapalli, S. and Li, D. and Rawat, AS.},
    journal={arXiv preprint arXiv:2210.06313},
    year={2022},
    url={https://arxiv.org/abs/2210.06313}
}

@article{xue2024openmoe,
    title={Openmoe: An early effort on open mixture-of-experts language models},
    author={Xue, F. and Zheng, Z. and Fu, Y. and Ni, J. and Zhou, W.},
    journal={arXiv preprint arXiv:2402.01739},
    year={2024},
    url={https://arxiv.org/abs/2402.01739}
}

@article{jain2024nested,
    title={Mixture of nested experts: Adaptive processing of visual tokens},
    author={Jain, G. and Hegde, N. and Kusupati, A. and Nagrani, A. and Buch, S.},
    journal={arXiv preprint arXiv:2407.19985},
    year={2024},
    url={https://arxiv.org/abs/2407.19985}
}

@article{he2024millionexperts,
    title={Mixture of a million experts},
    author={He, XO},
    journal={arXiv preprint arXiv:2407.04153},
    year={2024},
    url={https://arxiv.org/abs/2407.04153}
}


@article{mirzadeh2023relu,
  title={ReLU strikes back: Exploiting activation sparsity in large language models},
  author={Mirzadeh, Iman and Alizadeh, Keivan and Mehta, Sachin and Del Mundo, Carlo C and Tuzel, Oncel and Samei, Golnoosh and Rastegari, Mohammad and Farajtabar, Mehrdad},
  year={2023},
  journal={arXiv preprint arXiv:2310.04564},
  url={https://arxiv.org/abs/2310.04564}
}

@article{zhang2024relu2,
  title={ReLU2 Wins: Discovering efficient activation functions for sparse LLMs},
  author={Zhengyan Zhang and Yixin Song and Guanghui Yu and Xu Han and Yankai Lin and Chaojun Xiao and Chenyang Song and Zhiyuan Liu and Zeyu Mi and Maosong Sun},
  year={2024},
  journal={arXiv preprint arXiv:2402.03804},
  url={https://arxiv.org/abs/2402.03804}
}

@article{so2022squaredrelu,
  title={Squared ReLU: A simple and effective activation function},
  author={So, David and others},
  year={2022},
  journal={Advances in Neural Information Processing Systems},
}

@article{song2024prosparse,
  title={ProSparse: Introducing and enhancing intrinsic activation sparsity within large language models},
  author={Song, Chenyang and Han, Xu and Zhang, Zhengyan and Hu, Shengding and Shi, Xiyu and Li, Kuai and Chen, Chen and Liu, Zhiyuan and Li, Guangli and Yang, Tao and Sun, Maosong},
  year={2024},
  journal={arXiv preprint arXiv:2402.13516},
  url={https://arxiv.org/abs/2402.13516}
}

@article{song2024activation,
  title={PowerInfer: Enhancing activation sparsity in LLM serving with consumer-grade GPUs},
  author={Song, Yixin and others},
  year={2024},
  journal={arXiv preprint arXiv:2312.12456},
  url={https://arxiv.org/abs/2312.12456}
}

@inproceedings{wang2024ladder,
  title={LADDER: Enabling efficient low-precision deep learning computing through hardware-aware tensor transformation},
  author={Wang, Lei and Ma, Lingxiao and Cao, Shijie and Zhang, Quanlu and Xue, Jilong and Shi, Yining and Zheng, Ningxin and Miao, Ziming and Yang, Fan and Cao, Ting and Yang, Yuqing and Yang, Mao},
  booktitle={18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)},
  year={2024},
  url={https://www.usenix.org/conference/osdi24/presentation/wang-lei}
}

@article{lee2024cats,
  title={CATS: Training-free activation sparsity for SwiGLU-based LLMs},
  author={Lee, Jaemin and others},
  year={2024},
  journal={arXiv preprint},
  url={https://arxiv.org/abs/2401.12345}
}


@article{li2023lazy,
  title={The lazy neuron phenomenon: On emergence of activation sparsity in transformers},
  author={Li, Zonglin and You, Chong and Bhojanapalli, Srinadh and Li, Daliang and Rawat, Ankit Singh and Reddi, Sashank J and Ye, Ke and Chern, Felix and Yu, Felix and Guo, Ruiqi and Kumar, Sanjiv},
  year={2023},
  journal={arXiv preprint arXiv:2210.06313},
  url={https://arxiv.org/abs/2210.06313}
}

@article{liu2023deja,
  title={Deja vu: Contextual sparsity for efficient LLMs at inference time},
  author={Liu, Zichang and Wang, Jue and Dao, Tri and Zhou, Tianyi and Yuan, Binhang and Song, Zhao and Shrivastava, Anshumali and Zhang, Ce and Tian, Yuandong and Re, Christopher and Chen, Beidi},
  year={2023},
  journal={arXiv preprint arXiv:2310.17157},
  url={https://arxiv.org/abs/2310.17157}
}

@article{song2023powerinfer,
  title={Powerinfer: Fast large language model serving with a consumer-grade GPU},
  author={Song, Yixin and Mi, Zeyu and Xie, Haotong and Chen, Haibo},
  year={2023},
  journal={arXiv preprint arXiv:2312.12456},
  url={https://arxiv.org/abs/2312.12456}
}

@article{alizadeh2024llmflash,
  title={LLM in a flash: Efficient large language model inference with limited memory},
  author={Alizadeh, Keivan and Mirzadeh, Iman and Belenko, Dmitry and Khatamifard, Karen and Cho, Minsik and Del Mundo, Carlo C and Rastegari, Mohammad and Farajtabar, Mehrdad},
  year={2024},
  journal={arXiv preprint arXiv:2312.11514},
  url={https://arxiv.org/abs/2312.11514}
}


@article{fedus2022switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={The Journal of Machine Learning Research},
  volume={23},
  number={1},
  pages={5232--5270},
  year={2022}
}

@inproceedings{riquelme2021scaling,
  title={Scaling vision with sparse mixture of experts},
  author={Riquelme, Carlos and Puigcerver, Joan and Mustafa, Basil and Neumann, Mario and Jenatton, Rodolphe and Susano Pinto, Ant{\'o}nio and Keysers, Daniel and Houlsby, Neil},
  booktitle={Advances in Neural Information Processing Systems},
  volume={34},
  pages={8583--8595},
  year={2021}
}

@inproceedings{zhou2022mixture,
  title={Mixture-of-experts with expert choice routing},
  author={Zhou, Yutian and Lei, Tao and Liu, Henry and Du, Nan and Huang, Yanping and Zhao, Vincent and Dai, Andrew M and Le, Quoc V and Laudon, James and others},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, A Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Beno{\^\i}t and Bamford, Charles and Chaplot, Devendra Singh and Casas, Daniele de la and Hanna, Emily Bressand and Bressand, François and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@article{li2022branch,
  title={Branch-train-merge: Embarrassingly parallel training of expert language models},
  author={Li, Min and Gururangan, Suchin and Dettmers, Tim and Lewis, Mike and Althoff, Tim and Smith, Noah A and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2208.03306},
  year={2022}
}

@article{cai2023once,
  title={Once-for-all: Train one network and specialize it for efficient deployment},
  author={Cai, Han and Gan, Chuang and Wang, Tianzhe and Zhang, Zhekai and Han, Song},
  journal={arXiv preprint arXiv:1908.09791},
  year={2019}
}

@article{dean2021pathways,
  title={Introducing Pathways: A next-generation AI architecture},
  author={Dean, Jeff},
  journal={Google Blog},
  volume={366},
  year={2021}
}

@article{chen2023sparse,
  title={Sparse MoE as the new dropout: Scaling dense and self-slimmable transformers},
  author={Chen, Tianlong and Zhang, Zhekai and Jaiswal, Abhishek and Liu, Shiwei and Wang, Zhangyang},
  journal={arXiv preprint arXiv:2303.01610},
  year={2023}
}



@misc{ng2024loopneuralnetworksparameter,
      title={Loop Neural Networks for Parameter Sharing}, 
      author={Kei-Sing Ng and Qingchen Wang},
      year={2024},
      eprint={2409.14199},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2409.14199}, 
}

@misc{dehghani2019universaltransformers,
      title={Universal Transformers}, 
      author={Mostafa Dehghani and Stephan Gouws and Oriol Vinyals and Jakob Uszkoreit and Łukasz Kaiser},
      year={2019},
      eprint={1807.03819},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1807.03819}, 
}

@misc{geiping2025scalingtesttimecomputelatent,
      title={Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach}, 
      author={Jonas Geiping and Sean McLeish and Neel Jain and John Kirchenbauer and Siddharth Singh and Brian R. Bartoldson and Bhavya Kailkhura and Abhinav Bhatele and Tom Goldstein},
      year={2025},
      eprint={2502.05171},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2502.05171}, 
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{gunasekar2023textbooks,
  title={Textbooks Are All You Need},
  author={Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio C{\'e}sar Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and de Rosa, Gustavo and Saarikivi, Olli and others},
  journal={arXiv preprint arXiv:2306.11644},
  year={2023}
}

@article{li2023textbooks,
  title={Textbooks are all you need ii: phi-1.5 technical report},
  author={Li, Yuanzhi and Bubeck, S{\'e}bastien and Eldan, Ronen and Del Giorno, Allie and Gunasekar, Suriya and Lee, Yin Tat},
  journal={arXiv preprint arXiv:2309.05463},
  year={2023}
}

@article{chen2015net2net,
  title={Net2net: Accelerating learning via knowledge transfer},
  author={Chen, Tianqi and Goodfellow, Ian and Shlens, Jonathon},
  journal={arXiv preprint arXiv:1511.05641},
  year={2015}
}

@article{chen2021bert2bert,
  title={bert2bert: Towards reusable pretrained language models},
  author={Chen, Cheng and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Qin, Yujia and Wang, Fengyu and Wang, Zhi and Chen, Xiao and Liu, Zhiyuan and Liu, Qun},
  journal={arXiv preprint arXiv:2110.07143},
  year={2021}
}

@article{xia2022structured,
  title={Structured pruning learns compact and accurate models},
  author={Xia, Mengzhou and Zhong, Zexuan and Chen, Danqi},
  journal={arXiv preprint arXiv:2204.00408},
  year={2022}
}

@article{chiu2021low,
  title={Low-rank constraints for fast inference in structured models},
  author={Chiu, Justin and Deng, Yuntian and Rush, Alexander},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={2887--2898},
  year={2021}
}

@misc{daoMonarchExpressiveStructured2022a,
	title = {Monarch: {Expressive} {Structured} {Matrices} for {Efficient} and {Accurate} {Training}},
	shorttitle = {Monarch},
	url = {http://arxiv.org/abs/2204.00595},
	urldate = {2024-02-06},
	publisher = {arXiv},
	author = {Dao, Tri and Chen, Beidi and Sohoni, Nimit and Desai, Arjun and Poli, Michael and Grogan, Jessica and Liu, Alexander and Rao, Aniruddh and Rudra, Atri and Ré, Christopher},
	month = apr,
	year = {2022},
	note = {arXiv:2204.00595 [cs]},
	keywords = {/unread, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/chenyilong/Zotero/storage/5D4CH2IM/2204.html:text/html},
}

@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@article{paperno2016lambada,
  title={The LAMBADA dataset: Word prediction requiring a broad discourse context},
  author={Paperno, Denis and Kruszewski, Germ{\'a}n and Lazaridou, Angeliki and Pham, Quan Ngoc and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fern{\'a}ndez, Raquel},
  journal={arXiv preprint arXiv:1606.06031},
  year={2016}
}

@article{liu2020logiqa,
  title={Logiqa: A challenge dataset for machine reading comprehension with logical reasoning},
  author={Liu, Jian and Cui, Leyang and Liu, Hanmeng and Huang, Dandan and Wang, Yile and Zhang, Yue},
  journal={arXiv preprint arXiv:2007.08124},
  year={2020}
}

@inproceedings{WinoGrande:conf/aaai/SakaguchiBBC20,
  author       = {Keisuke Sakaguchi and
                  Ronan Le Bras and
                  Chandra Bhagavatula and
                  Yejin Choi},
  title        = {WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
  booktitle    = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
                  2020, The Thirty-Second Innovative Applications of Artificial Intelligence
                  Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
                  Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
                  February 7-12, 2020},
  pages        = {8732--8740},
  publisher    = {{AAAI} Press},
  year         = {2020},
  url          = {https://doi.org/10.1609/aaai.v34i05.6399},
  doi          = {10.1609/AAAI.V34I05.6399},
  timestamp    = {Mon, 04 Sep 2023 16:50:27 +0200},
  biburl       = {https://dblp.org/rec/conf/aaai/SakaguchiBBC20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{NQ:journals/tacl/KwiatkowskiPRCP19,
  author       = {Tom Kwiatkowski and
                  Jennimaria Palomaki and
                  Olivia Redfield and
                  Michael Collins and
                  Ankur P. Parikh and
                  Chris Alberti and
                  Danielle Epstein and
                  Illia Polosukhin and
                  Jacob Devlin and
                  Kenton Lee and
                  Kristina Toutanova and
                  Llion Jones and
                  Matthew Kelcey and
                  Ming{-}Wei Chang and
                  Andrew M. Dai and
                  Jakob Uszkoreit and
                  Quoc Le and
                  Slav Petrov},
  title        = {Natural Questions: a Benchmark for Question Answering Research},
  journal      = {Trans. Assoc. Comput. Linguistics},
  volume       = {7},
  pages        = {452--466},
  year         = {2019},
  url          = {https://doi.org/10.1162/tacl\_a\_00276},
  doi          = {10.1162/TACL\_A\_00276},
  timestamp    = {Tue, 16 Aug 2022 23:05:11 +0200},
  biburl       = {https://dblp.org/rec/journals/tacl/KwiatkowskiPRCP19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{mmlu:conf/iclr/HendrycksBBZMSS21,
  author       = {Dan Hendrycks and
                  Collin Burns and
                  Steven Basart and
                  Andy Zou and
                  Mantas Mazeika and
                  Dawn Song and
                  Jacob Steinhardt},
  title        = {Measuring Massive Multitask Language Understanding},
  booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021,
                  Virtual Event, Austria, May 3-7, 2021},
  publisher    = {OpenReview.net},
  year         = {2021},
  url          = {https://openreview.net/forum?id=d7KBjmI3GmQ},
  timestamp    = {Wed, 23 Jun 2021 17:36:39 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/HendrycksBBZMSS21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{arcChallenge:journals/corr/abs-1803-05457,
  author       = {Peter Clark and
                  Isaac Cowhey and
                  Oren Etzioni and
                  Tushar Khot and
                  Ashish Sabharwal and
                  Carissa Schoenick and
                  Oyvind Tafjord},
  title        = {Think you have Solved Question Answering? Try ARC, the {AI2} Reasoning
                  Challenge},
  journal      = {CoRR},
  volume       = {abs/1803.05457},
  year         = {2018},
  url          = {http://arxiv.org/abs/1803.05457},
  eprinttype    = {arXiv},
  eprint       = {1803.05457},
  timestamp    = {Mon, 13 Aug 2018 16:48:43 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1803-05457.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{HellaSwag:conf/acl/ZellersHBFC19,
  author       = {Rowan Zellers and
                  Ari Holtzman and
                  Yonatan Bisk and
                  Ali Farhadi and
                  Yejin Choi},
  editor       = {Anna Korhonen and
                  David R. Traum and
                  Llu{\'{\i}}s M{\`{a}}rquez},
  title        = {HellaSwag: Can a Machine Really Finish Your Sentence?},
  booktitle    = {Proceedings of the 57th Conference of the Association for Computational
                  Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
                  Volume 1: Long Papers},
  pages        = {4791--4800},
  publisher    = {Association for Computational Linguistics},
  year         = {2019},
  url          = {https://doi.org/10.18653/v1/p19-1472},
  doi          = {10.18653/V1/P19-1472},
  timestamp    = {Sat, 29 Apr 2023 10:09:26 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/ZellersHBFC19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{sciqa,
  author       = {Johannes Welbl and
                  Nelson F. Liu and
                  Matt Gardner},
  editor       = {Leon Derczynski and
                  Wei Xu and
                  Alan Ritter and
                  Tim Baldwin},
  title        = {Crowdsourcing Multiple Choice Science Questions},
  booktitle    = {Proceedings of the 3rd Workshop on Noisy User-generated Text, NUT@EMNLP
                  2017, Copenhagen, Denmark, September 7, 2017},
  pages        = {94--106},
  publisher    = {Association for Computational Linguistics},
  year         = {2017},
  url          = {https://doi.org/10.18653/v1/w17-4413},
  doi          = {10.18653/V1/W17-4413},
  timestamp    = {Fri, 06 Aug 2021 00:40:09 +0200},
  biburl       = {https://dblp.org/rec/conf/aclnut/WelblLG17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{piqa,
  author       = {Yonatan Bisk and
                  Rowan Zellers and
                  Ronan Le Bras and
                  Jianfeng Gao and
                  Yejin Choi},
  title        = {{PIQA:} Reasoning about Physical Commonsense in Natural Language},
  booktitle    = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
                  2020, The Thirty-Second Innovative Applications of Artificial Intelligence
                  Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
                  Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
                  February 7-12, 2020},
  pages        = {7432--7439},
  publisher    = {{AAAI} Press},
  year         = {2020},
  url          = {https://doi.org/10.1609/aaai.v34i05.6239},
  doi          = {10.1609/AAAI.V34I05.6239},
  timestamp    = {Mon, 04 Sep 2023 16:50:23 +0200},
  biburl       = {https://dblp.org/rec/conf/aaai/BiskZLGC20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{clark2019boolq,
  title={BoolQ: Exploring the surprising difficulty of natural yes/no questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1905.10044},
  year={2019}
}

@misc{Openllama,
	title = {Openllama: An open reproduction of llama},
	author = {Xinyang Geng, Hao Liu},
	year = {2023},
	file = {arXiv.org Snapshot:/Users/chenyilong/Zotero/storage/5D4CH2IM/2204.html:text/html},
}

@misc{Redpajama,
	title = {Redpajama: An open source recipe to reproduce llama training dataset},
	author = {TogetherAI},
	year = {2023},
	file = {arXiv.org Snapshot:/Users/chenyilong/Zotero/storage/5D4CH2IM/2204.html:text/html},
}

@inproceedings{biderman2023pythia,
  title={Pythia: A suite for analyzing large language models across training and scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O’Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
  booktitle={International Conference on Machine Learning},
  pages={2397--2430},
  year={2023},
  organization={PMLR}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}


@inproceedings{zhangAcceleratingTrainingTransformerBased2020,
	title = {Accelerating {Training} of {Transformer}-{Based} {Language} {Models} with {Progressive} {Layer} {Dropping}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/a1140a3d0df1c81e24ae954d935e8926-Abstract.html},
	abstract = {Recently, Transformer-based language models have demonstrated remarkable performance across many NLP domains. However, the unsupervised pre-training step of these models suffers from unbearable overall computational expenses. Current methods for accelerating the pre-training either rely on massive parallelism with advanced hardware or are not applicable to language models.},
	urldate = {2023-08-31},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhang, Minjia and He, Yuxiong},
	year = {2020},
	pages = {14011--14023},
}



@misc{wangLearningGrowPretrained2023,
	title = {Learning to {Grow} {Pretrained} {Models} for {Efficient} {Transformer} {Training}},
	url = {http://arxiv.org/abs/2303.00980},
	doi = {10.48550/arXiv.2303.00980},
	urldate = {2023-08-31},
	publisher = {arXiv},
	author = {Wang, Peihao and Panda, Rameswar and Hennigen, Lucas Torroba and Greengard, Philip and Karlinsky, Leonid and Feris, Rogerio and Cox, David Daniel and Wang, Zhangyang and Kim, Yoon},
	month = mar,
	year = {2023},
}


@article{bai2023qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@misc{xiaShearedLLaMAAccelerating2023,
	title = {Sheared {LLaMA}: {Accelerating} {Language} {Model} {Pre}-training via {Structured} {Pruning}},
	shorttitle = {Sheared {LLaMA}},
	url = {http://arxiv.org/abs/2310.06694},
	doi = {10.48550/arXiv.2310.06694},
	abstract = {The popularity of LLaMA (Touvron et al., 2023a;b) and other recently emerged moderate-sized large language models (LLMs) highlights the potential of building smaller yet powerful LLMs. Regardless, the cost of training such models from scratch on trillions of tokens remains high. In this work, we study structured pruning as an effective means to develop smaller LLMs from pre-trained, larger models. Our approach employs two key techniques: (1) targeted structured pruning, which prunes a larger model to a specified target shape by removing layers, heads, and intermediate and hidden dimensions in an end-to-end manner, and (2) dynamic batch loading, which dynamically updates the composition of sampled data in each training batch based on varying losses across different domains. We demonstrate the efficacy of our approach by presenting the Sheared-LLaMA series, pruning the LLaMA2-7B model down to 1.3B and 2.7B parameters. Sheared-LLaMA models outperform state-of-the-art open-source models of equivalent sizes, such as Pythia, INCITE, and OpenLLaMA models, on a wide range of downstream and instruction tuning evaluations, while requiring only 3\% of compute compared to training such models from scratch. This work provides compelling evidence that leveraging existing LLMs with structured pruning is a far more cost-effective approach for building smaller LLMs.},
	urldate = {2023-10-11},
	publisher = {arXiv},
	author = {Xia, Mengzhou and Gao, Tianyu and Zeng, Zhiyuan and Chen, Danqi},
	month = oct,
	year = {2023},
}

@misc{maLLMPrunerStructuralPruning2023,
	title = {{LLM}-{Pruner}: {On} the {Structural} {Pruning} of {Large} {Language} {Models}},
	shorttitle = {{LLM}-{Pruner}},
	url = {http://arxiv.org/abs/2305.11627},
	doi = {10.48550/arXiv.2305.11627},
	abstract = {Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner},
	urldate = {2023-10-16},
	publisher = {arXiv},
	author = {Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
	month = sep,
	year = {2023},
}

@inproceedings{bennoachCompressingPretrainedLanguage2020,
	address = {Suzhou, China},
	title = {Compressing {Pre}-trained {Language} {Models} by {Matrix} {Decomposition}},
	url = {https://aclanthology.org/2020.aacl-main.88},
	abstract = {Large pre-trained language models reach state-of-the-art results on many different NLP tasks when fine-tuned individually; They also come with a significant memory and computational requirements, calling for methods to reduce model sizes (green AI). We propose a two-stage model-compression method to reduce a model's inference time cost. We first decompose the matrices in the model into smaller matrices and then perform feature distillation on the internal representation to recover from the decomposition. This approach has the benefit of reducing the number of parameters while preserving much of the information within the model. We experimented on BERT-base model with the GLUE benchmark dataset and show that we can reduce the number of parameters by a factor of 0.4x, and increase inference speed by a factor of 1.45x, while maintaining a minimal loss in metric performance.},
	urldate = {2023-10-23},
	booktitle = {Proceedings of the 1st {Conference} of the {Asia}-{Pacific} {Chapter} of the {Association} for {Computational} {Linguistics} and the 10th {International} {Joint} {Conference} on {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Ben Noach, Matan and Goldberg, Yoav},
	month = dec,
	year = {2020},
}

@misc{zhangCRaShClusteringRemoving2023,
	title = {{CRaSh}: {Clustering}, {Removing}, and {Sharing} {Enhance} {Fine}-tuning without {Full} {Large} {Language} {Model}},
	shorttitle = {{CRaSh}},
	url = {http://arxiv.org/abs/2310.15477},
	abstract = {Instruction tuning has recently been recognized as an effective way of aligning Large Language Models (LLMs) to enhance their generalization ability across various tasks. However, when tuning publicly accessible, centralized LLMs with private instruction data, privacy concerns are inevitable. While direct transfer of parameterized modules between models is a plausible approach to address this, its implications and effectiveness need further exploration. This paper focuses on Offsite-Tuning (OFT), a representative technique that transfers transformer blocks between centralized LLMs and downstream emulators. Given the limited understanding of the underlying mechanism of OFT, we perform an empirical analysis on LLMs from the perspectives of representation and functional similarity. Interestingly, our findings reveal a unique modular structure within the layers of LLMs that appears to emerge as the model size expands. Simultaneously, we note subtle but potentially significant changes in representation and intermediate predictions across the layers. Inspired by these observations, we propose CRaSh, involving Clustering, Removing, and Sharing, a training-free strategy to derive improved emulators from LLMs. CRaSh significantly boosts performance of OFT with billions of parameters. Furthermore, we investigate the optimal solutions yielded by fine-tuning with and without full model through the lens of loss landscape. Our findings demonstrate a linear connectivity among these optima falling over the same basin, thereby highlighting the effectiveness of CRaSh and OFT. The source code is publicly available at https://github.com/TsinghuaC3I/CRaSh.},
	urldate = {2023-10-25},
	publisher = {arXiv},
	author = {Zhang, Kaiyan and Ding, Ning and Qi, Biqing and Zhu, Xuekai and Long, Xinwei and Zhou, Bowen},
	month = oct,
	year = {2023},
}

@misc{zhaoUnveilingCoreLinguistic2023,
	title = {Unveiling {A} {Core} {Linguistic} {Region} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2310.14928},
	abstract = {Brain localization, which describes the association between specific regions of the brain and their corresponding functions, is widely accepted in the field of cognitive science as an objective fact. Today's large language models (LLMs) possess human-level linguistic competence and can execute complex tasks requiring abstract knowledge and reasoning. To deeply understand the inherent mechanisms of intelligence emergence in LLMs, this paper conducts an analogical research using brain localization as a prototype. We have discovered a core region in LLMs that corresponds to linguistic competence, accounting for approximately 1\% of the total model parameters. This core region exhibits significant dimension dependency, and perturbations to even a single parameter on specific dimensions can lead to a loss of linguistic competence. Furthermore, we observe that an improvement in linguistic competence does not necessarily accompany an elevation in the model's knowledge level, which might imply the existence of regions of domain knowledge that are dissociated from the linguistic region. Overall, exploring the LLMs' functional regions provides insights into the foundation of their intelligence. In the future, we will continue to investigate knowledge regions within LLMs and the interactions between them.},
	urldate = {2023-10-25},
	publisher = {arXiv},
	author = {Zhao, Jun and Zhang, Zhihao and Ma, Yide and Zhang, Qi and Gui, Tao and Gao, Luhui and Huang, Xuanjing},
	month = oct,
	year = {2023},
}

@misc{wangRethinkingValueTransformer2020,
	title = {Rethinking the {Value} of {Transformer} {Components}},
	url = {http://arxiv.org/abs/2011.03803},
	abstract = {Transformer becomes the state-of-the-art translation model, while it is not well studied how each intermediate component contributes to the model performance, which poses significant challenges for designing optimal architectures. In this work, we bridge this gap by evaluating the impact of individual component (sub-layer) in trained Transformer models from different perspectives. Experimental results across language pairs, training strategies, and model capacities show that certain components are consistently more important than the others. We also report a number of interesting findings that might help humans better analyze, understand and improve Transformer models. Based on these observations, we further propose a new training strategy that can improves translation performance by distinguishing the unimportant components in training.},
	urldate = {2023-10-25},
	publisher = {arXiv},
	author = {Wang, Wenxuan and Tu, Zhaopeng},
	month = nov,
	year = {2020},
}

@inproceedings{wangStructuredPruningLarge2020a,
	title = {Structured {Pruning} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/1910.04732},
	doi = {10.18653/v1/2020.emnlp-main.496},
	abstract = {Large language models have recently achieved state of the art performance across a wide variety of natural language tasks. Meanwhile, the size of these models and their latency have significantly increased, which makes their usage costly, and raises an interesting question: do language models need to be large? We study this question through the lens of model compression. We present a generic, structured pruning approach by parameterizing each weight matrix using its low-rank factorization, and adaptively removing rank-1 components during training. On language modeling tasks, our structured approach outperforms other unstructured and block-structured pruning baselines at various compression levels, while achieving significant speedups during both training and inference. We also demonstrate that our method can be applied to pruning adaptive word embeddings in large language models, and to pruning the BERT model on several downstream fine-tuning classification benchmarks.},
	urldate = {2023-10-26},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	author = {Wang, Ziheng and Wohlwend, Jeremy and Lei, Tao},
	year = {2020},
}

@misc{wuWeightInheritedDistillationTaskAgnostic2023,
	title = {Weight-{Inherited} {Distillation} for {Task}-{Agnostic} {BERT} {Compression}},
	url = {http://arxiv.org/abs/2305.09098},
	abstract = {Knowledge Distillation (KD) is a predominant approach for BERT compression. Previous KD-based methods focus on designing extra alignment losses for the student model to mimic the behavior of the teacher model. These methods transfer the knowledge in an indirect way. In this paper, we propose a novel Weight-Inherited Distillation (WID), which directly transfers knowledge from the teacher. WID does not require any additional alignment loss and trains a compact student by inheriting the weights, showing a new perspective of knowledge distillation. Specifically, we design the row compactors and column compactors as mappings and then compress the weights via structural re-parameterization. Experimental results on the GLUE and SQuAD benchmarks show that WID outperforms previous state-of-the-art KD-based baselines. Further analysis indicates that WID can also learn the attention patterns from the teacher model without any alignment loss on attention distributions.},
	urldate = {2023-10-26},
	publisher = {arXiv},
	author = {Wu, Taiqiang and Hou, Cheng and Zhao, Zhe and Lao, Shanshan and Li, Jiayi and Wong, Ngai and Yang, Yujiu},
	month = may,
	year = {2023},
}

@misc{xiaoVariatorAcceleratingPretrained2023,
	title = {Variator: {Accelerating} {Pre}-trained {Models} with {Plug}-and-{Play} {Compression} {Modules}},
	shorttitle = {Variator},
	url = {http://arxiv.org/abs/2310.15724},
	abstract = {Pre-trained language models (PLMs) have achieved remarkable results on NLP tasks but at the expense of huge parameter sizes and the consequent computational costs. In this paper, we propose Variator, a parameter-efficient acceleration method that enhances computational efficiency through plug-and-play compression plugins. Compression plugins are designed to reduce the sequence length via compressing multiple hidden vectors into one and trained with original PLMs frozen. Different from traditional model acceleration methods, which compress PLMs to smaller sizes, Variator offers two distinct advantages: (1) In real-world applications, the plug-and-play nature of our compression plugins enables dynamic selection of different compression plugins with varying acceleration ratios based on the current workload. (2) The compression plugin comprises a few compact neural network layers with minimal parameters, significantly saving storage and memory overhead, particularly in scenarios with a growing number of tasks. We validate the effectiveness of Variator on seven datasets. Experimental results show that Variator can save 53\% computational costs using only 0.9\% additional parameters with a performance drop of less than 2\%. Moreover, when the model scales to billions of parameters, Variator matches the strong performance of uncompressed PLMs.},
	urldate = {2023-10-29},
	publisher = {arXiv},
	author = {Xiao, Chaojun and Luo, Yuqi and Zhang, Wenbin and Zhang, Pengle and Han, Xu and Lin, Yankai and Zhang, Zhengyan and Xie, Ruobing and Liu, Zhiyuan and Sun, Maosong and Zhou, Jie},
	month = oct,
	year = {2023},
}

@misc{imfeldTransformerFusionOptimal2023,
	title = {Transformer {Fusion} with {Optimal} {Transport}},
	url = {http://arxiv.org/abs/2310.05719},
	abstract = {Fusion is a technique for merging multiple independently-trained neural networks in order to combine their capabilities. Past attempts have been restricted to the case of fully-connected, convolutional, and residual networks. In this paper, we present a systematic approach for fusing two or more transformer-based networks exploiting Optimal Transport to (soft-)align the various architectural components. We flesh out an abstraction for layer alignment, that can generalize to arbitrary architectures -- in principle -- and we apply this to the key ingredients of Transformers such as multi-head self-attention, layer-normalization, and residual connections, and we discuss how to handle them via various ablation studies. Furthermore, our method allows the fusion of models of different sizes (heterogeneous fusion), providing a new and efficient way for compression of Transformers. The proposed approach is evaluated on both image classification tasks via Vision Transformer and natural language modeling tasks using BERT. Our approach consistently outperforms vanilla fusion, and, after a surprisingly short finetuning, also outperforms the individual converged parent models. In our analysis, we uncover intriguing insights about the significant role of soft alignment in the case of Transformers. Our results showcase the potential of fusing multiple Transformers, thus compounding their expertise, in the budding paradigm of model fusion and recombination.},
	urldate = {2023-11-03},
	publisher = {arXiv},
	author = {Imfeld, Moritz and Graldi, Jacopo and Giordano, Marco and Hofmann, Thomas and Anagnostidis, Sotiris and Singh, Sidak Pal},
	month = oct,
	year = {2023},
}

@article{yuLanguageModelsAre,
	title = {Language {Models} are {Super} {Mario}: {Absorbing} {Abilities} of {Homologous} {Models} {Without} {Tears}},
	abstract = {Throughout history, humans have harbored a longstanding desire to acquire additional abilities through absorption. Super Mario serves as an embodiment of this human dream, which can collect items to gain extra skills such as throwing fireballs and being temporarily invincible. In this paper, we uncover that Language Models (LMs), either encoder- or decoder-based, can obtain new capabilities by assimilating the parameters of homologous models without the need for retraining or GPUs. Typically, new abilities of LMs can be imparted by Supervised Fine-Tuning (SFT), reflected in the disparity between fine-tuned and pre-trained parameters (i.e., delta parameters). We initially observe that by introducing a novel operation called DARE (Drop And REscale), we can directly set most of the delta parameters to zeros without affecting the capabilities of SFT LMs. Larger models can tolerate a higher proportion of discarded parameters, indicating that SFT naturally learns an extremely sparse set of delta parameters, and nearly all abilities originate from the pre-trained LMs. Based on this observation, we further sparsify delta parameters of multiple SFT homologous models with DARE as a general preprocessing technique and subsequently merge them into a single model by parameter averaging. We conduct experiments on eight datasets from the GLUE benchmark with BERT and RoBERTa. We also merge WizardLM, WizardMath, and Code Alpaca based on Llama 2. Experimental results show that: (1) DARE is effective for SFT models whose delta parameter value ranges are relatively small (e.g., within 0.005), being able to eliminate even 99\% delta parameters. However, once the models are continuously pre-trained, the value ranges can grow to around 0.03, making DARE impractical. We also remove fine-tuned instead of delta parameters and find that a 10\% reduction can lead to drastically decreased performance (even to 0.0). This highlights that SFT merely stimulates the abilities via delta parameters rather than injecting new abilities into LMs. (2) DARE can merge multiple task-specific LMs into one LM with diverse abilities, which is able to possess the functionalities of all SFT models. For instance, the merger of WizardLM and WizardMath increases the GSM8K accuracy of WizardLM from 2.2 to 66.3, maintaining its instructionfollowing capabilities while surpassing WizardMath’s original 64.2 performance. All the resources are available at https://github.com/yule-BUAA/MergeLM.},
	language = {en},
	author = {Yu, Le and Yu, Bowen and Yu, Haiyang and Huang, Fei and Li, Yongbin},
}

@misc{zhangEmergentModularityPretrained2023,
	title = {Emergent {Modularity} in {Pre}-trained {Transformers}},
	url = {http://arxiv.org/abs/2305.18390},
	abstract = {This work examines the presence of modularity in pre-trained Transformers, a feature commonly found in human brains and thought to be vital for general intelligence. In analogy to human brains, we consider two main characteristics of modularity: (1) functional specialization of neurons: we evaluate whether each neuron is mainly specialized in a certain function, and find that the answer is yes. (2) function-based neuron grouping: we explore finding a structure that groups neurons into modules by function, and each module works for its corresponding function. Given the enormous amount of possible structures, we focus on Mixture-of-Experts as a promising candidate, which partitions neurons into experts and usually activates different experts for different inputs. Experimental results show that there are functional experts, where clustered are the neurons specialized in a certain function. Moreover, perturbing the activations of functional experts significantly affects the corresponding function. Finally, we study how modularity emerges during pre-training, and find that the modular structure is stabilized at the early stage, which is faster than neuron stabilization. It suggests that Transformers first construct the modular structure and then learn fine-grained neuron functions. Our code and data are available at https://github.com/THUNLP/modularity-analysis.},
	urldate = {2023-11-06},
	publisher = {arXiv},
	author = {Zhang, Zhengyan and Zeng, Zhiyuan and Lin, Yankai and Xiao, Chaojun and Wang, Xiaozhi and Han, Xu and Liu, Zhiyuan and Xie, Ruobing and Sun, Maosong and Zhou, Jie},
	month = oct,
	year = {2023},
}

@misc{qinExploringModeConnectivity2022,
	title = {Exploring {Mode} {Connectivity} for {Pre}-trained {Language} {Models}},
	url = {http://arxiv.org/abs/2210.14102},
	abstract = {Recent years have witnessed the prevalent application of pre-trained language models (PLMs) in NLP. From the perspective of parameter space, PLMs provide generic initialization, starting from which high-performance minima could be found. Although plenty of works have studied how to effectively and efficiently adapt PLMs to high-performance minima, little is known about the connection of various minima reached under different adaptation configurations. In this paper, we investigate the geometric connections of different minima through the lens of mode connectivity, which measures whether two minima can be connected with a low-loss path. We conduct empirical analyses to investigate three questions: (1) how could hyperparameters, specific tuning methods, and training data affect PLM's mode connectivity? (2) How does mode connectivity change during pre-training? (3) How does the PLM's task knowledge change along the path connecting two minima? In general, exploring the mode connectivity of PLMs conduces to understanding the geometric connection of different minima, which may help us fathom the inner workings of PLM downstream adaptation.},
	urldate = {2023-11-06},
	publisher = {arXiv},
	author = {Qin, Yujia and Qian, Cheng and Yi, Jing and Chen, Weize and Lin, Yankai and Han, Xu and Liu, Zhiyuan and Sun, Maosong and Zhou, Jie},
	month = oct,
	year = {2022},
}

@article{kwonFastPostTrainingPruning,
	title = {A {Fast} {Post}-{Training} {Pruning} {Framework} for {Transformers}},
	language = {en},
	author = {Kwon, Woosuk and Kim, Sehoon and Mahoney, Michael W and Hassoun, Joseph and Keutzer, Kurt and Gholami, Amir},
	keywords = {/unread},
	file = {Kwon 等 - A Fast Post-Training Pruning Framework for Transfo.pdf:/Users/chenyilong/Zotero/storage/LF3HLH2L/Kwon 等 - A Fast Post-Training Pruning Framework for Transfo.pdf:application/pdf},
}

@misc{udagawaComparativeAnalysisTaskAgnostic2023,
	title = {A {Comparative} {Analysis} of {Task}-{Agnostic} {Distillation} {Methods} for {Compressing} {Transformer} {Language} {Models}},
	url = {http://arxiv.org/abs/2310.08797},
	doi = {10.48550/arXiv.2310.08797},
	abstract = {Large language models have become a vital component in modern NLP, achieving state of the art performance in a variety of tasks. However, they are often inefficient for real-world deployment due to their expensive inference costs. Knowledge distillation is a promising technique to improve their efficiency while retaining most of their effectiveness. In this paper, we reproduce, compare and analyze several representative methods for task-agnostic (general-purpose) distillation of Transformer language models. Our target of study includes Output Distribution (OD) transfer, Hidden State (HS) transfer with various layer mapping strategies, and Multi-Head Attention (MHA) transfer based on MiniLMv2. Through our extensive experiments, we study the effectiveness of each method for various student architectures in both monolingual (English) and multilingual settings. Overall, we show that MHA transfer based on MiniLMv2 is generally the best option for distillation and explain the potential reasons behind its success. Moreover, we show that HS transfer remains as a competitive baseline, especially under a sophisticated layer mapping strategy, while OD transfer consistently lags behind other approaches. Findings from this study helped us deploy efficient yet effective student models for latency-critical applications.},
	urldate = {2023-11-20},
	publisher = {arXiv},
	author = {Udagawa, Takuma and Trivedi, Aashka and Merler, Michele and Bhattacharjee, Bishwaranjan},
	month = oct,
	year = {2023},
}

@misc{ConnectivityPatternsAre,
	title = {Connectivity {Patterns} are {Task} {Embeddings} - {ACL} {Anthology}},
	url = {https://aclanthology.org/2023.findings-acl.759/},
	urldate = {2023-11-20},
	keywords = {/unread},
}

@misc{novaGradientFreeStructuredPruning2023,
	title = {Gradient-{Free} {Structured} {Pruning} with {Unlabeled} {Data}},
	url = {http://arxiv.org/abs/2303.04185},
	doi = {10.48550/arXiv.2303.04185},
	abstract = {Large Language Models (LLMs) have achieved great success in solving difficult tasks across many domains, but such success comes with a high computation cost, and inference latency. As developers and third parties customize these models, the need to provide efficient inference has increased. Many efforts have attempted to reduce inference cost through model compression techniques such as pruning and distillation. However, these techniques either require labeled data, or are time-consuming as they require the compressed model to be retrained to regain accuracy. In this paper, we propose a gradient-free structured pruning framework that uses only unlabeled data. An evaluation on the GLUE and SQuAD benchmarks using BERT\$\_\{BASE\}\$ and DistilBERT illustrates the effectiveness of the proposed approach. By only using the weights of the pre-trained model and unlabeled data, in a matter of a few minutes on a single GPU, up to 40\% of the original FLOP count can be reduced with less than a 4\% accuracy loss across all tasks considered.},
	urldate = {2023-11-20},
	publisher = {arXiv},
	author = {Nova, Azade and Dai, Hanjun and Schuurmans, Dale},
	month = jul,
	year = {2023},
}
@article{schacke2004kronecker,
  title={On the kronecker product},
  author={Schacke, Kathrin},
  journal={Master's thesis, University of Waterloo},
  year={2004}
}
@article{tang2024rethinking,
  title={Rethinking Optimization and Architecture for Tiny Language Models},
  author={Tang, Yehui and Liu, Fangcheng and Ni, Yunsheng and Tian, Yuchuan and Bai, Zheyuan and Hu, Yi-Qi and Liu, Sichao and Jui, Shangling and Han, Kai and Wang, Yunhe},
  journal={arXiv preprint arXiv:2402.02791},
  year={2024}
}
@misc{eval-harness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = 12,
  year         = 2023,
  publisher    = {Zenodo},
  version      = {v0.4.0},
  doi          = {10.5281/zenodo.10256836},
  url          = {https://zenodo.org/records/10256836}
}
@misc{frantarSparseGPTMassiveLanguage2023,
	title = {{SparseGPT}: {Massive} {Language} {Models} {Can} {Be} {Accurately} {Pruned} in {One}-{Shot}},
	shorttitle = {{SparseGPT}},
	url = {http://arxiv.org/abs/2301.00774},
	abstract = {We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50\% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60\% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Frantar, Elias and Alistarh, Dan},
	month = mar,
	year = {2023},
}
@article{DBLP:journals/corr/doremi,
  author       = {Sang Michael Xie and
                  Hieu Pham and
                  Xuanyi Dong and
                  Nan Du and
                  Hanxiao Liu and
                  Yifeng Lu and
                  Percy Liang and
                  Quoc V. Le and
                  Tengyu Ma and
                  Adams Wei Yu},
  title        = {DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining},
  journal      = {CoRR},
  volume       = {abs/2305.10429},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2305.10429},
  doi          = {10.48550/ARXIV.2305.10429},
  eprinttype    = {arXiv},
  eprint       = {2305.10429},
  timestamp    = {Thu, 11 Jan 2024 22:39:41 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2305-10429.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{mosaicml2022composer,
    author = {The Mosaic ML Team},
    title = {composer},
    year = {2021},
    howpublished = {\url{https://github.com/mosaicml/composer/}},
}
@inproceedings{DBLP:conf/nips/WuW019,
  author       = {Lemeng Wu and
                  Dilin Wang and
                  Qiang Liu},
  editor       = {Hanna M. Wallach and
                  Hugo Larochelle and
                  Alina Beygelzimer and
                  Florence d'Alch{\'{e}}{-}Buc and
                  Emily B. Fox and
                  Roman Garnett},
  title        = {Splitting Steepest Descent for Growing Neural Architectures},
  booktitle    = {Advances in Neural Information Processing Systems 32: Annual Conference
                  on Neural Information Processing Systems 2019, NeurIPS 2019, December
                  8-14, 2019, Vancouver, BC, Canada},
  pages        = {10655--10665},
  year         = {2019},
  url          = {https://proceedings.neurips.cc/paper/2019/hash/3a01fc0853ebeba94fde4d1cc6fb842a-Abstract.html},
  timestamp    = {Mon, 16 May 2022 15:41:51 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/WuW019.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{DBLP:conf/cvpr/XieXP17,
  author       = {Di Xie and
                  Jiang Xiong and
                  Shiliang Pu},
  title        = {All You Need is Beyond a Good Init: Exploring Better Solution for
                  Training Extremely Deep Convolutional Neural Networks with Orthonormality
                  and Modulation},
  booktitle    = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition,
                  {CVPR} 2017, Honolulu, HI, USA, July 21-26, 2017},
  pages        = {5075--5084},
  publisher    = {{IEEE} Computer Society},
  year         = {2017},
  url          = {https://doi.org/10.1109/CVPR.2017.539},
  doi          = {10.1109/CVPR.2017.539},
  timestamp    = {Fri, 24 Mar 2023 00:02:57 +0100},
  biburl       = {https://dblp.org/rec/conf/cvpr/XieXP17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{gray1998quantization,
  title={Quantization},
  author={Gray, Robert M. and Neuhoff, David L.},
  journal={IEEE transactions on information theory},
  volume={44},
  number={6},
  pages={2325--2383},
  year={1998},
  publisher={IEEE}
}

@article{sajjad2023effect,
  title={On the effect of dropping layers of pre-trained transformer models},
  author={Sajjad, Hassan and Dalvi, Fahim and Durrani, Nadir and Nakov, Preslav},
  journal={Computer Speech \& Language},
  volume={77},
  pages={101429},
  year={2023},
  publisher={Elsevier}
}
@article{zhang2020accelerating,
  title={Accelerating training of transformer-based language models with progressive layer dropping},
  author={Zhang, Minjia and He, Yuxiong},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={14011--14023},
  year={2020}
}

@inproceedings{jiangPruningPretrainedLanguage2023,
	address = {Toronto, Canada},
	title = {Pruning {Pre}-trained {Language} {Models} {Without} {Fine}-{Tuning}},
	url = {https://aclanthology.org/2023.acl-long.35},
	doi = {10.18653/v1/2023.acl-long.35},
	abstract = {To overcome the overparameterized problem in Pre-trained Language Models (PLMs), pruning is widely used as a simple and straightforward compression method by directly removing unimportant weights. Previous first-order methods successfully compress PLMs to extremely high sparsity with little performance drop. These methods, such as movement pruning, use first-order information to prune PLMs while fine-tuning the remaining weights. In this work, we argue fine-tuning is redundant for first-order pruning, since first-order pruning is sufficient to converge PLMs to downstream tasks without fine-tuning. Under this motivation, we propose Static Model Pruning (SMP), which only uses first-order pruning to adapt PLMs to downstream tasks while achieving the target sparsity level. In addition, we also design a new masking function and training objective to further improve SMP. Extensive experiments at various sparsity levels show SMP has significant improvements over first-order and zero-order methods. Unlike previous first-order methods, SMP is also applicable to low sparsity and outperforms zero-order methods. Meanwhile, SMP is more parameter efficient than other methods due to it does not require fine-tuning.},
	urldate = {2023-11-28},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Jiang, Ting and Wang, Deqing and Zhuang, Fuzhen and Xie, Ruobing and Xia, Feng},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
}

@misc{jaiswalCompressingLLMsTruth2023,
	title = {Compressing {LLMs}: {The} {Truth} is {Rarely} {Pure} and {Never} {Simple}},
	shorttitle = {Compressing {LLMs}},
	url = {http://arxiv.org/abs/2310.01382},
	abstract = {Despite their remarkable achievements, modern Large Language Models (LLMs) encounter exorbitant computational and memory footprints. Recently, several works have shown significant success in training-free and data-free compression (pruning and quantization) of LLMs achieving 50-60\% sparsity and reducing the bit-width down to 3 or 4 bits per weight, with negligible perplexity degradation over the uncompressed baseline. As recent research efforts are focused on developing increasingly sophisticated compression methods, our work takes a step back, and re-evaluates the effectiveness of existing SoTA compression methods, which rely on a fairly simple and widely questioned metric, perplexity (even for dense LLMs). We introduce Knowledge-Intensive Compressed LLM BenchmarK (LLM-KICK), a collection of carefully-curated tasks to re-define the evaluation protocol for compressed LLMs, which have significant alignment with their dense counterparts, and perplexity fail to capture subtle change in their true capabilities. LLM-KICK unveils many favorable merits and unfortunate plights of current SoTA compression methods: all pruning methods suffer significant performance degradation, sometimes at trivial sparsity ratios (e.g., 25-30\%), and fail for N:M sparsity on knowledge-intensive tasks; current quantization methods are more successful than pruning; yet, pruned LLMs even at \${\textbackslash}geq 50\$\% sparsity are robust in-context retrieval and summarization systems; among others. LLM-KICK is designed to holistically access compressed LLMs' ability for language understanding, reasoning, generation, in-context retrieval, in-context summarization, etc. We hope our study can foster the development of better LLM compression methods. All our related codes are planed to be open-sourced.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Jaiswal, Ajay and Gan, Zhe and Du, Xianzhi and Zhang, Bowen and Wang, Zhangyang and Yang, Yinfei},
	month = oct,
	year = {2023},
}

@misc{xuInitializingModelsLarger2023,
	title = {Initializing {Models} with {Larger} {Ones}},
	url = {http://arxiv.org/abs/2311.18823},
	abstract = {Weight initialization plays an important role in neural network training. Widely used initialization methods are proposed and evaluated for networks that are trained from scratch. However, the growing number of pretrained models now offers new opportunities for tackling this classical problem of weight initialization. In this work, we introduce weight selection, a method for initializing smaller models by selecting a subset of weights from a pretrained larger model. This enables the transfer of knowledge from pretrained weights to smaller models. Our experiments demonstrate that weight selection can significantly enhance the performance of small models and reduce their training time. Notably, it can also be used together with knowledge distillation. Weight selection offers a new approach to leverage the power of pretrained models in resource-constrained settings, and we hope it can be a useful tool for training small models in the large-model era. Code is available at https://github.com/OscarXZQ/weight-selection.},
	urldate = {2023-12-06},
	publisher = {arXiv},
	author = {Xu, Zhiqiu and Chen, Yanjie and Vishniakov, Kirill and Yin, Yida and Shen, Zhiqiang and Darrell, Trevor and Liu, Lingjie and Liu, Zhuang},
	month = nov,
	year = {2023},
}

@inproceedings{garipovLossSurfacesMode2018,
	title = {Loss {Surfaces}, {Mode} {Connectivity}, and {Fast} {Ensembling} of {DNNs}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/hash/be3087e74e9100d4bc4c6268cdbe8456-Abstract.html},
	abstract = {The loss functions of deep neural networks are complex and their geometric properties are not well understood.  We show that the optima of these complex loss functions are in fact connected by simple curves, over which training and test accuracy are nearly constant.  We introduce a training procedure to discover these high-accuracy pathways between modes.  Inspired by this new geometric insight, we also propose a new ensembling method entitled Fast Geometric Ensembling (FGE). Using FGE we can train high-performing ensembles in the time required to train a single model.  We achieve improved performance compared to the recent state-of-the-art Snapshot Ensembles, on  CIFAR-10, CIFAR-100, and ImageNet.},
	urldate = {2023-12-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry P and Wilson, Andrew G},
	year = {2018},
}

@misc{tamMergingMatchingModels2023,
	title = {Merging by {Matching} {Models} in {Task} {Subspaces}},
	url = {http://arxiv.org/abs/2312.04339},
	abstract = {Model merging aims to cheaply combine individual task-specific models into a single multitask model. In this work, we view past merging methods as leveraging different notions of a ''task subspace'' in which models are matched before being merged. We connect the task subspace of a given model to its loss landscape and formalize how this approach to model merging can be seen as solving a linear system of equations. While past work has generally been limited to linear systems that have a closed-form solution, we consider using the conjugate gradient method to find a solution. We show that using the conjugate gradient method can outperform closed-form solutions, enables merging via linear systems that are otherwise intractable to solve, and flexibly allows choosing from a wide variety of initializations and estimates for the ''task subspace''. We ultimately demonstrate that our merging framework called ''Matching Models in their Task Subspace'' (MaTS) achieves state-of-the-art results in multitask and intermediate-task model merging. We release all of the code and checkpoints used in our work at https://github.com/r-three/mats.},
	urldate = {2023-12-13},
	publisher = {arXiv},
	author = {Tam, Derek and Bansal, Mohit and Raffel, Colin},
	month = dec,
	year = {2023},
}

@inproceedings{friedmanComparingRepresentationalFunctional2023,
	title = {Comparing {Representational} and {Functional} {Similarity} in {Small} {Transformer} {Language} {Models}},
	url = {https://openreview.net/forum?id=uKWqDnLI3o},
	abstract = {In many situations, it would be helpful to be able to characterize the solution learned by a neural network, including for answering scientific questions (e.g. how do architecture changes affect generalization) and addressing practical concerns (e.g. auditing for potentially unsafe behavior). One approach is to try to understand these models by studying the representations that they learn---for example, comparing whether two networks learn similar representations. However, it is not always clear how much representation-level analyses can tell us about how a model makes predictions. In this work, we explore this question in the context of small Transformer language models, which we train on a synthetic, hierarchical language task. We train models with different sizes and random initializations, evaluating performance over the course of training and on a variety of systematic generalization splits. We find that existing methods for measuring representation similarity are not always correlated with behavioral metrics---i.e. models with similar representations do not always make similar predictions---and the results vary depending on the choice of representation. Our results highlight the importance of understanding representations in terms of the role they play in the neural algorithm.},
	language = {en},
	urldate = {2023-12-27},
	author = {Friedman, Dan and Lampinen, Andrew Kyle and Dixon, Lucas and Chen, Danqi and Ghandeharioun, Asma},
	month = dec,
	year = {2023},
}

@article{kimSOLAR107B,
	title = {{SOLAR} 10.{7B}: {Scaling} {Large} {Language} {Models} with {Simple} yet {Effective} {Depth} {Up}-{Scaling}},
	abstract = {We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, demonstrating superior performance in various natural language processing (NLP) tasks. Inspired by recent efforts to efficiently up-scale LLMs, we present a method for scaling LLMs called depth up-scaling (DUS), which encompasses depthwise scaling and continued pretraining. In contrast to other LLM up-scaling methods that use mixture-of-experts, DUS does not require complex changes to train and inference efficiently. We show experimentally that DUS is simple yet effective in scaling up highperformance LLMs from small ones. Building on the DUS model, we additionally present SOLAR 10.7B-Instruct, a variant fine-tuned for instruction-following capabilities, surpassing Mixtral-8x7B-Instruct. SOLAR 10.7B is publicly available under the Apache 2.0 license, promoting broad access and application in the LLM field 1.},
	language = {en},
	author = {Kim, Dahyun and Park, Chanjun and Kim, Sanghoon and Lee, Wonsung and Song, Wonho and Kim, Yunsu and Kim, Hyeonwoo and Kim, Yungi and Lee, Hyeonju and Kim, Jihoo and Ahn, Changbae and Yang, Seonghoon and Lee, Sukyung and Park, Hyunbyung and Gim, Gyoungjin and Cha, Mikyoung and Lee, Hwalsuk and Kim, Sunghun},
}

@misc{chenLoRAShearEfficientLarge2023,
	title = {{LoRAShear}: {Efficient} {Large} {Language} {Model} {Structured} {Pruning} and {Knowledge} {Recovery}},
	shorttitle = {{LoRAShear}},
	url = {https://arxiv.org/abs/2310.18356v2},
	abstract = {Large Language Models (LLMs) have transformed the landscape of artificial intelligence, while their enormous size presents significant challenges in terms of computational costs. We introduce LoRAShear, a novel efficient approach to structurally prune LLMs and recover knowledge. Given general LLMs, LoRAShear at first creates the dependency graphs over LoRA modules to discover minimally removal structures and analyze the knowledge distribution. It then proceeds progressive structured pruning on LoRA adaptors and enables inherent knowledge transfer to better preserve the information in the redundant structures. To recover the lost knowledge during pruning, LoRAShear meticulously studies and proposes a dynamic fine-tuning schemes with dynamic data adaptors to effectively narrow down the performance gap to the full models. Numerical results demonstrate that by only using one GPU within a couple of GPU days, LoRAShear effectively reduced footprint of LLMs by 20\% with only 1.0\% performance degradation and significantly outperforms state-of-the-arts. The source code will be available at https://github.com/microsoft/lorashear.},
	language = {en},
	urldate = {2024-01-02},
	journal = {arXiv.org},
	author = {Chen, Tianyi and Ding, Tianyu and Yadav, Badal and Zharkov, Ilya and Liang, Luming},
	month = oct,
	year = {2023},
	keywords = {/unread},
	file = {Full Text PDF:/Users/chenyilong/Zotero/storage/LH6DXKC3/Chen 等 - 2023 - LoRAShear Efficient Large Language Model Structur.pdf:application/pdf},
}

@misc{240102415LLaMA,
	title = {[2401.02415] {LLaMA} {Pro}: {Progressive} {LLaMA} with {Block} {Expansion}},
	url = {https://arxiv.org/abs/2401.02415},
	urldate = {2024-01-07},
	file = {[2401.02415] LLaMA Pro\: Progressive LLaMA with Block Expansion:/Users/chenyilong/Zotero/storage/IXWMXUS9/2401.html:text/html;全文:/Users/chenyilong/Zotero/storage/SGYTANXS/[2401.02415] LLaMA Pro Progressive LLaMA with Blo.pdf:application/pdf},
}



@article{Gpt-4,
	Author = {OpenAI},
	Journal = {ArXiv},
	Pages = {abs/2303.08774},
	Title = {Gpt-4 technical report},
	Year = {2023}
}

@article{claude,
	Author = {Anthropic},
	Title = {Introducing claude},
	Year = {2023}
}

@article{ling2017program,
  title={Program induction by rationale generation: Learning to solve and explain algebraic word problems},
  author={Ling, Wang and Yogatama, Dani and Dyer, Chris and Blunsom, Phil},
  journal={arXiv preprint arXiv:1705.04146},
  year={2017}
}

@misc{touvronLlamaOpenFoundation2023,
	title = {Llama 2: {Open} {Foundation} and {Fine}-{Tuned} {Chat} {Models}},
	shorttitle = {Llama 2},
	url = {http://arxiv.org/abs/2307.09288},
	doi = {10.48550/arXiv.2307.09288},
	abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
	urldate = {2023-08-14},
	publisher = {arXiv},
	author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
	month = jul,
	year = {2023},
}


@misc{jiangMistral7B2023,
	title = {Mistral {7B}},
	url = {http://arxiv.org/abs/2310.06825},
	doi = {10.48550/arXiv.2310.06825},
	urldate = {2023-10-21},
	publisher = {arXiv},
	author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, Lélio Renard and Lachaux, Marie-Anne and Stock, Pierre and Scao, Teven Le and Lavril, Thibaut and Wang, Thomas and Lacroix, Timothée and Sayed, William El},
	month = oct,
	year = {2023},
}


@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}
