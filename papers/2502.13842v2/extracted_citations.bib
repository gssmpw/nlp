@inproceedings{DBLP:conf/cvpr/XieXP17,
  author       = {Di Xie and
                  Jiang Xiong and
                  Shiliang Pu},
  title        = {All You Need is Beyond a Good Init: Exploring Better Solution for
                  Training Extremely Deep Convolutional Neural Networks with Orthonormality
                  and Modulation},
  booktitle    = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition,
                  {CVPR} 2017, Honolulu, HI, USA, July 21-26, 2017},
  pages        = {5075--5084},
  publisher    = {{IEEE} Computer Society},
  year         = {2017},
  url          = {https://doi.org/10.1109/CVPR.2017.539},
  doi          = {10.1109/CVPR.2017.539},
  timestamp    = {Fri, 24 Mar 2023 00:02:57 +0100},
  biburl       = {https://dblp.org/rec/conf/cvpr/XieXP17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/nips/WuW019,
  author       = {Lemeng Wu and
                  Dilin Wang and
                  Qiang Liu},
  editor       = {Hanna M. Wallach and
                  Hugo Larochelle and
                  Alina Beygelzimer and
                  Florence d'Alch{\'{e}}{-}Buc and
                  Emily B. Fox and
                  Roman Garnett},
  title        = {Splitting Steepest Descent for Growing Neural Architectures},
  booktitle    = {Advances in Neural Information Processing Systems 32: Annual Conference
                  on Neural Information Processing Systems 2019, NeurIPS 2019, December
                  8-14, 2019, Vancouver, BC, Canada},
  pages        = {10655--10665},
  year         = {2019},
  url          = {https://proceedings.neurips.cc/paper/2019/hash/3a01fc0853ebeba94fde4d1cc6fb842a-Abstract.html},
  timestamp    = {Mon, 16 May 2022 15:41:51 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/WuW019.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{chen2015net2net,
  title={Net2net: Accelerating learning via knowledge transfer},
  author={Chen, Tianqi and Goodfellow, Ian and Shlens, Jonathon},
  journal={arXiv preprint arXiv:1511.05641},
  year={2015}
}

@misc{frantarSparseGPTMassiveLanguage2023,
	title = {{SparseGPT}: {Massive} {Language} {Models} {Can} {Be} {Accurately} {Pruned} in {One}-{Shot}},
	shorttitle = {{SparseGPT}},
	url = {http://arxiv.org/abs/2301.00774},
	abstract = {We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50\% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60\% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Frantar, Elias and Alistarh, Dan},
	month = mar,
	year = {2023},
}

@article{gray1998quantization,
  title={Quantization},
  author={Gray, Robert M. and Neuhoff, David L.},
  journal={IEEE transactions on information theory},
  volume={44},
  number={6},
  pages={2325--2383},
  year={1998},
  publisher={IEEE}
}

@misc{maLLMPrunerStructuralPruning2023,
	title = {{LLM}-{Pruner}: {On} the {Structural} {Pruning} of {Large} {Language} {Models}},
	shorttitle = {{LLM}-{Pruner}},
	url = {http://arxiv.org/abs/2305.11627},
	doi = {10.48550/arXiv.2305.11627},
	abstract = {Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner},
	urldate = {2023-10-16},
	publisher = {arXiv},
	author = {Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
	month = sep,
	year = {2023},
}

@article{sajjad2023effect,
  title={On the effect of dropping layers of pre-trained transformer models},
  author={Sajjad, Hassan and Dalvi, Fahim and Durrani, Nadir and Nakov, Preslav},
  journal={Computer Speech \& Language},
  volume={77},
  pages={101429},
  year={2023},
  publisher={Elsevier}
}

@misc{wangLearningGrowPretrained2023,
	title = {Learning to {Grow} {Pretrained} {Models} for {Efficient} {Transformer} {Training}},
	url = {http://arxiv.org/abs/2303.00980},
	doi = {10.48550/arXiv.2303.00980},
	urldate = {2023-08-31},
	publisher = {arXiv},
	author = {Wang, Peihao and Panda, Rameswar and Hennigen, Lucas Torroba and Greengard, Philip and Karlinsky, Leonid and Feris, Rogerio and Cox, David Daniel and Wang, Zhangyang and Kim, Yoon},
	month = mar,
	year = {2023},
}

@inproceedings{wangStructuredPruningLarge2020a,
	title = {Structured {Pruning} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/1910.04732},
	doi = {10.18653/v1/2020.emnlp-main.496},
	abstract = {Large language models have recently achieved state of the art performance across a wide variety of natural language tasks. Meanwhile, the size of these models and their latency have significantly increased, which makes their usage costly, and raises an interesting question: do language models need to be large? We study this question through the lens of model compression. We present a generic, structured pruning approach by parameterizing each weight matrix using its low-rank factorization, and adaptively removing rank-1 components during training. On language modeling tasks, our structured approach outperforms other unstructured and block-structured pruning baselines at various compression levels, while achieving significant speedups during both training and inference. We also demonstrate that our method can be applied to pruning adaptive word embeddings in large language models, and to pruning the BERT model on several downstream fine-tuning classification benchmarks.},
	urldate = {2023-10-26},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	author = {Wang, Ziheng and Wohlwend, Jeremy and Lei, Tao},
	year = {2020},
}

@misc{xiaShearedLLaMAAccelerating2023,
	title = {Sheared {LLaMA}: {Accelerating} {Language} {Model} {Pre}-training via {Structured} {Pruning}},
	shorttitle = {Sheared {LLaMA}},
	url = {http://arxiv.org/abs/2310.06694},
	doi = {10.48550/arXiv.2310.06694},
	abstract = {The popularity of LLaMA (Touvron et al., 2023a;b) and other recently emerged moderate-sized large language models (LLMs) highlights the potential of building smaller yet powerful LLMs. Regardless, the cost of training such models from scratch on trillions of tokens remains high. In this work, we study structured pruning as an effective means to develop smaller LLMs from pre-trained, larger models. Our approach employs two key techniques: (1) targeted structured pruning, which prunes a larger model to a specified target shape by removing layers, heads, and intermediate and hidden dimensions in an end-to-end manner, and (2) dynamic batch loading, which dynamically updates the composition of sampled data in each training batch based on varying losses across different domains. We demonstrate the efficacy of our approach by presenting the Sheared-LLaMA series, pruning the LLaMA2-7B model down to 1.3B and 2.7B parameters. Sheared-LLaMA models outperform state-of-the-art open-source models of equivalent sizes, such as Pythia, INCITE, and OpenLLaMA models, on a wide range of downstream and instruction tuning evaluations, while requiring only 3\% of compute compared to training such models from scratch. This work provides compelling evidence that leveraging existing LLMs with structured pruning is a far more cost-effective approach for building smaller LLMs.},
	urldate = {2023-10-11},
	publisher = {arXiv},
	author = {Xia, Mengzhou and Gao, Tianyu and Zeng, Zhiyuan and Chen, Danqi},
	month = oct,
	year = {2023},
}

@article{zhang2020accelerating,
  title={Accelerating training of transformer-based language models with progressive layer dropping},
  author={Zhang, Minjia and He, Yuxiong},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={14011--14023},
  year={2020}
}

@inproceedings{zhangAcceleratingTrainingTransformerBased2020,
	title = {Accelerating {Training} of {Transformer}-{Based} {Language} {Models} with {Progressive} {Layer} {Dropping}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/a1140a3d0df1c81e24ae954d935e8926-Abstract.html},
	abstract = {Recently, Transformer-based language models have demonstrated remarkable performance across many NLP domains. However, the unsupervised pre-training step of these models suffers from unbearable overall computational expenses. Current methods for accelerating the pre-training either rely on massive parallelism with advanced hardware or are not applicable to language models.},
	urldate = {2023-08-31},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhang, Minjia and He, Yuxiong},
	year = {2020},
	pages = {14011--14023},
}

@misc{zhangCRaShClusteringRemoving2023,
	title = {{CRaSh}: {Clustering}, {Removing}, and {Sharing} {Enhance} {Fine}-tuning without {Full} {Large} {Language} {Model}},
	shorttitle = {{CRaSh}},
	url = {http://arxiv.org/abs/2310.15477},
	abstract = {Instruction tuning has recently been recognized as an effective way of aligning Large Language Models (LLMs) to enhance their generalization ability across various tasks. However, when tuning publicly accessible, centralized LLMs with private instruction data, privacy concerns are inevitable. While direct transfer of parameterized modules between models is a plausible approach to address this, its implications and effectiveness need further exploration. This paper focuses on Offsite-Tuning (OFT), a representative technique that transfers transformer blocks between centralized LLMs and downstream emulators. Given the limited understanding of the underlying mechanism of OFT, we perform an empirical analysis on LLMs from the perspectives of representation and functional similarity. Interestingly, our findings reveal a unique modular structure within the layers of LLMs that appears to emerge as the model size expands. Simultaneously, we note subtle but potentially significant changes in representation and intermediate predictions across the layers. Inspired by these observations, we propose CRaSh, involving Clustering, Removing, and Sharing, a training-free strategy to derive improved emulators from LLMs. CRaSh significantly boosts performance of OFT with billions of parameters. Furthermore, we investigate the optimal solutions yielded by fine-tuning with and without full model through the lens of loss landscape. Our findings demonstrate a linear connectivity among these optima falling over the same basin, thereby highlighting the effectiveness of CRaSh and OFT. The source code is publicly available at https://github.com/TsinghuaC3I/CRaSh.},
	urldate = {2023-10-25},
	publisher = {arXiv},
	author = {Zhang, Kaiyan and Ding, Ning and Qi, Biqing and Zhu, Xuekai and Long, Xinwei and Zhou, Bowen},
	month = oct,
	year = {2023},
}

