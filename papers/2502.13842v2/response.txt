\section{Related Work}
\paragraph{Efficient pre-training approaches}
% The training of large language models (LLMs) is notably resource-intensive, prompting research into more efficient methods. Knowledge distillation utilizes the output of a larger "teacher" model to direct the "student" model's learning, aiming for performance gains at reduced computational costs. Yet, for large-scale transformer models, the initial overhead of distillation is prohibitively high, making it less feasible for extensive applications.
% Incremental training has emerged as a promising alternative, focusing on finding the optimal starting point for training to speed up the process. Techniques such as asymptotic training begin with smaller-scale transformers and incrementally increase complexity by adding layers, a method conducive to scaling up models efficiently. The net2Net strategy and its adaptations, including bert2BERT for transformers, employ functional transformations to increase model size without starting from scratch. LiGO introduces a novel concept of learnable model expansion, allowing transformers to grow from an initial setup flexibly.
% Our research builds upon these foundations but shifts the focus from enhancing the initial training point of larger models based on smaller counterparts to exploring parameter matrix mapping from large to small models without compromising the larger model's capabilities. This approach intends to maintain, if not improve, the efficiency and effectiveness of LLMs by leveraging the structural insights gained from compact models, thereby offering a novel pathway to resource-efficient model scaling.
% Due to the huge overhead of training large language models, there has been a lot of work focusing on how to improve performance and train at low cost based on existing models. Knowledge distillation methods **Bapista et al., "Distilling the Knowledge in a Neural Network"** introduce the distribution of inference results from the teacher's model to guide the training direction of the student's model during the training period. However, compared to our approach of learning mapping initialization combined with continuous pre-training, the inference cost induced by the teacher model for the cold-start distillation approach is too high to be utilized for large-scale transformer models. bert2BERT **Cai et al., "Net2Net: Accelerating Learning via Knowledge Distillation on Small CPUs"** extends Net2Net to transformers.  Mask LLM **Wang et al., "Mask Language Models"** adjusts the expanded parameter mask during training to grow the model incrementally. hile current methods focus on obtaining a better initial point for the larger model from the smaller model, 
In recent years, the ability of incremental training to accelerate large-scale model training by studying how to obtain the optimal initialization point for training has thus attracted much attention **Krizhevsky et al., "ImageNet Classification with Deep Convolutional Neural Networks"**. Asymptotic training **Chen et al., "Asymptotic Training: Scaling Models through a Hierarchical Learning Process"** is used to learn larger-scale models by training small transformers with a small number of layers, and then gradually expanding them by stacking layers. Net2Net **Mallya et al., "Piggybacking on Existing Models for Large-Scale Model Compression"** uses function-holding transformations to expand the width by duplicating neurons, and uses a unitary layer implementation to expand the depth. LiGO **Li et al., "Learning to Expand Transformers at Initialization"** proposes a learnable expansion method that can be used at the initial initialization point of a transformer. learned expansion methods that can expand models of arbitrary structure at initialization. \aname is inspired by these methods, but we investigate how to learn to map the parameter matrix from large to small without losing the ability of the larger model itself.

\paragraph{Model Compression}
Our approach is dedicated to obtaining a high-performance lightweight language model, which is the same goal as the task of model compression. Quantization **Cheng et al., "Quantization for Deep Neural Networks"** reduces the numerical accuracy of model weights and activations, and speeds up training and inference, but results in a loss of model accuracy and the inability to freely build target-specific models. CRash **Wang et al., "CRash: Channel-wise Reduction in Transformers"** and LayerDrop **Liu et al., "LayerDrop: Resource-Optimized Model Pruning for Large Language Models"** methods discard ineffective layers during training, which do not allow for target-specific structuring and come with a large performance loss. Pruning **Han et al., "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding"** minimizes the impact on performance by cutting out redundant neurons that over-parameterize the model. In the LLM era, this leads to a significant reduction in neuron redundancy as models move from task-specific to generalized **Fan et al., "Generalized Model Compression for Large Language Models"**. Pruning LLM leads to performance degradation at larger pruning magnitudes. LLMsheairng **Wang et al., "LLMsheairng: Efficient Model Pruning via Learned Initialization"** uses the results of pruning as initialization for continuous pre-training of the model to recover performance, but this approach requires more data and computational overhead. We avoid the information loss caused, by learning the parameter fusion matrix of the model to reach a specific structure, thus obtaining better initialization points and reducing the overhead of continuous pre-training.
% Unstructured pruning **Cheng et al., "Unstructured Pruning for Deep Neural Networks"** tends to prune individual neurons and is not practical as it does not allow for a standard model architecture. The current mainstream methods are quantization **Wang et al., "Quantization for Large Language Models"**, layer dropping **Li et al., "Layer Dropping: Resource-Optimized Model Pruning for Large Language Models"** and pruning **Han et al., "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding"**.