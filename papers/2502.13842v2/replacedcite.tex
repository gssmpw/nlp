\section{Related Work}
\paragraph{Efficient pre-training approaches}
% The training of large language models (LLMs) is notably resource-intensive, prompting research into more efficient methods. Knowledge distillation utilizes the output of a larger "teacher" model to direct the "student" model's learning, aiming for performance gains at reduced computational costs. Yet, for large-scale transformer models, the initial overhead of distillation is prohibitively high, making it less feasible for extensive applications.
% Incremental training has emerged as a promising alternative, focusing on finding the optimal starting point for training to speed up the process. Techniques such as asymptotic training begin with smaller-scale transformers and incrementally increase complexity by adding layers, a method conducive to scaling up models efficiently. The net2Net strategy and its adaptations, including bert2BERT for transformers, employ functional transformations to increase model size without starting from scratch. LiGO introduces a novel concept of learnable model expansion, allowing transformers to grow from an initial setup flexibly.
% Our research builds upon these foundations but shifts the focus from enhancing the initial training point of larger models based on smaller counterparts to exploring parameter matrix mapping from large to small models without compromising the larger model's capabilities. This approach intends to maintain, if not improve, the efficiency and effectiveness of LLMs by leveraging the structural insights gained from compact models, thereby offering a novel pathway to resource-efficient model scaling.
% Due to the huge overhead of training large language models, there has been a lot of work focusing on how to improve performance and train at low cost based on existing models. Knowledge distillation methods ____ introduce the distribution of inference results from the teacher's model to guide the training direction of the student's model during the training period. However, compared to our approach of learning mapping initialization combined with continuous pre-training, the inference cost induced by the teacher model for the cold-start distillation approach is too high to be utilized for large-scale transformer models. bert2BERT____ extends Net2Net to transformers.  Mask LLM____ adjusts the expanded parameter mask during training to grow the model incrementally. hile current methods focus on obtaining a better initial point for the larger model from the smaller model, 
In recent years, the ability of incremental training to accelerate large-scale model training by studying how to obtain the optimal initialization point for training has thus attracted much attention ____. Asymptotic training____ is used to learn larger-scale models by training small transformers with a small number of layers, and then gradually expanding them by stacking layers. Net2Net____ uses function-holding transformations to expand the width by duplicating neurons, and uses a unitary layer implementation to expand the depth. LiGO____ proposes a learnable expansion method that can be used at the initial initialization point of a transformer. learned expansion methods that can expand models of arbitrary structure at initialization. \aname is inspired by these methods, but we investigate how to learn to map the parameter matrix from large to small without losing the ability of the larger model itself.

\paragraph{Model Compression}
Our approach is dedicated to obtaining a high-performance lightweight language model, which is the same goal as the task of model compression. Quantization____ reduces the numerical accuracy of model weights and activations, and speeds up training and inference, but results in a loss of model accuracy and the inability to freely build target-specific models. CRash____ and LayerDrop____ methods discard ineffective layers during training, which do not allow for target-specific structuring and come with a large performance loss. Pruning____ minimizes the impact on performance by cutting out redundant neurons that over-parameterize the model. In the LLM era, this leads to a significant reduction in neuron redundancy as models move from task-specific to generalized____. Pruning LLM leads to performance degradation at larger pruning magnitudes. LLMsheairng____ uses the results of pruning as initialization for continuous pre-training of the model to recover performance, but this approach requires more data and computational overhead. We avoid the information loss caused, by learning the parameter fusion matrix of the model to reach a specific structure, thus obtaining better initialization points and reducing the overhead of continuous pre-training.
% Unstructured pruning____ tends to prune individual neurons and is not practical as it does not allow for a standard model architecture. The current mainstream methods are quantization____, layer dropping____ and pruning____. 

%