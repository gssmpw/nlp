%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% to switch to arxiv from icml version: 
% 1) comment out 
%\usepackage{icml2025} and 
% uncomment 
%\usepackage[accepted]{icml2025}
% 2) in icml2025.sty: 
% comment out 
% \newcommand{\ICML@appearing}{\textit{Proceedings of the
% $\mathit{41}^{st}$ International Conference on Machine Learning},
% Vancouver, Canada. PMLR 267, 2025.
% Copyright 2025 by the author(s).}
% and uncomment
% \newcommand{\ICML@appearing}{}
% 3) change repository link to the public one
% 4) uncomment acknowledgements

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{subcaption}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\usepackage{multirow}

% \raggedbottom
% \setlength{\textfloatsep}{5pt}
% \setlength{\floatsep}{5pt}
% \setlength{\intextsep}{5pt} 
% \setlength{\columnsep}{10pt}
% \usepackage{titlesec}
% \titlespacing{\section}{0pt}{*1}{*0.5}
% \titlespacing{\subsection}{0pt}{*0.5}{*0.25}
% \setlength{\parskip}{6pt}
% \setlength{\parindent}{10pt}

% \setlength{\abovedisplayskip}{0pt}
% \setlength{\belowdisplayskip}{0pt}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Gompertz Linear Units: Advancing Activation Function Performance in Deep Learning}
\icmltitlerunning{Gompertz Linear Units: Leveraging Asymmetry for Enhanced Learning Dynamics}


\begin{document}

\twocolumn[
% \icmltitle{Gompertz Linear Units: \\ Advancing Activation Function Performance in Deep Learning}
\icmltitle{Gompertz Linear Units: \\ Leveraging Asymmetry for Enhanced Learning Dynamics}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Indrashis Das}{yyy}
\icmlauthor{Mahmoud Safari}{yyy}
\icmlauthor{Steven Adriaensen}{yyy}
\icmlauthor{Frank Hutter}{yyy,xxx}
% \icmlauthor{Firstname1 Lastname1}{equal,yyy}
% \icmlauthor{Firstname2 Lastname2}{equal,yyy,xxx}
% \icmlauthor{Firstname3 Lastname3}{comp}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{yyy,xxx}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{University of Freiburg}
\icmlaffiliation{xxx}{ELLIS Institute T\"ubingen}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Indrashis Das}{dasi@cs.uni-freiburg.de}
\icmlcorrespondingauthor{Mahmoud Safari}{safarim@cs.uni-freiburg.de}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
\printAffiliationsAndNotice{}

\begin{abstract}
% Activation functions are fundamental elements of deep learning architectures as they significantly influence training dynamics. ReLU, the most widely used activation function, suffers from the dying ReLU problem. This issue is addressed by variants such as LeakyReLU, PReLU, and ELU, which learn to handle the negative space of neuron values. Furthermore, advanced gated activations such as GELU and Swish integrate effectively with normalization methods, and their smoothness ensures a more stable gradient flow, preventing neuron inactivity. However, these functions fail to reduce latent space variance effectively. We propose the Gompertz Linear Unit (GoLU), a novel self-gated activation function defined as $\mathrm{GoLU}(x) = x \, \mathrm{Gompertz}(x)$, where $\mathrm{Gompertz}(x) = e^{-e^{-x}}$. The GoLU activation leverages the asymmetry in the Gompertz function to effectively reduce variance in the latent space while preserving robust gradient flow. Extensive experiments across diverse tasks, including Image Classification, Language Modeling, Semantic Segmentation, Object Detection, Instance Segmentation and Diffusion, demonstrate the superior performance of GoLU over state-of-the-art activation functions, which position GoLU as a robust alternative to existing activation functions.
%
% Activation functions are fundamental elements of deep learning architectures as they significantly influence training dynamics. 
%  We propose the Gompertz Linear Unit (GoLU), a novel self-gated activation function defined as $\mathrm{GoLU}(x) = x \, \mathrm{Gompertz}(x)$, where $\mathrm{Gompertz}(x) = e^{-e^{-x}}$. The GoLU activation leverages the asymmetry in the Gompertz function to reduce variance in the latent space more effectively compared to self-gated activations such as GELU and Swish while preserving robust gradient flow. Extensive experiments across diverse tasks, including Image Classification, Language Modeling, Semantic Segmentation, Object Detection, Instance Segmentation and Diffusion, demonstrate the superior performance of GoLU over state-of-the-art activation functions, which position GoLU as a robust alternative to existing activation functions.
 %
 Activation functions are fundamental elements of deep learning architectures as they significantly influence training dynamics. ReLU, while widely used, is prone to the dying neuron problem, which has been mitigated by variants such as LeakyReLU, PReLU, and ELU that better handle negative neuron outputs. Recently, self-gated activations like GELU and Swish have emerged as state-of-the-art alternatives, leveraging their smoothness to ensure stable gradient flow and prevent neuron inactivity. In this work, we introduce the Gompertz Linear Unit (GoLU), a novel self-gated activation function defined as $\mathrm{GoLU}(x) = x \, \mathrm{Gompertz}(x)$, where $\mathrm{Gompertz}(x) = e^{-e^{-x}}$. The GoLU activation leverages the asymmetry in the Gompertz function to reduce variance in the latent space more effectively compared to GELU and Swish, while preserving robust gradient flow. Extensive experiments across diverse tasks, including Image Classification, Language Modeling, Semantic Segmentation, Object Detection, Instance Segmentation, and Diffusion, highlight GoLU's superior performance relative to state-of-the-art activation functions, establishing GoLU as a robust alternative to existing activation functions.
 %
% We propose the Gompertz Linear Unit (GoLU), a novel self-gated activation function that employs the cumulative distribution function of the Gumbel distribution as its gating mechanism. The GoLU activation leverages the asymmetry in the Gumbel distribution to reduce variance in the latent space more effectively compared to self-gated activation such as GELU and Swish, while preserving robust gradient flow. Extensive experiments across diverse tasks, including Image Classification, Language Modelling, Semantic Segmentation, Object Detection, Instance Segmentation and Diffusion, demonstrate the superior performance of GoLU over state-of-the-art activation functions, which position GoLU as a robust alternative to existing activation functions.
\end{abstract}


\section{Introduction}\label{sec:introduction-and-motivation}

Developing effective activation functions has been a longstanding area of research in deep learning. In the early days, the Sigmoid \citep{verhulst1838logistic, rumelhart1986learning} and Tanh \citep{lecun2002efficient} functions were popular choices. However, these activations can suffer from the vanishing gradient problem due to their tendency to saturate. The introduction of ReLU \citep{nair2010rectified} marked a turning point, as it allowed for more efficient training by alleviating the vanishing gradient problem and inducing intensity equivariance \citep{nair2010rectified}. However, ReLU comes with its own challenges, notably the dying-ReLU %and Bias Shift problems
problem. To address these challenges, several ReLU variants have been developed, including LeakyReLU \citep{maas2013rectifier}, PReLU \citep{he2015delving} and ELU \citep{clevert2015fast}. Despite the emergence of these alternatives, ReLU remains one of the most widely used activation functions today, owing to its simplicity as a piecewise linear function and its computational efficiency.

In the deep learning community, the landscape of activation functions has gradually shifted towards self-gated activations such as Gaussian Error Linear Units (GELU) \citep{hendrycks2016gaussian}, Swish \citep{ramachandran2017searching}, and Mish \citep{misra2019mish}. % The self-gating mechanism simplifies the gating process by using the input itself to compute a factor that modulates the output.
% which is primarily motivated from the LSTM architecture \citep{hochreiter1997long}. 
These activations provide probabilistic interpretations while enhancing robustness when combined with normalization techniques \citep{ioffe2015batch, ba2016layer, ulyanov2016instance, wu2018group, zhang2019root}. Unlike ReLU, which strictly enforces gradient preservation due to its piecewise-linear nature, Swish, Mish and GELU, as smooth activation functions, relax these constraints. Their smoothness allows for improved gradient flow without strictly adhering to intensity equivariance.

In this work we introduce Gompertz Linear Units (GoLU), a new activation function of the self-gated family based on the Gompertz function \citep{gompertz1825xxiv} as its gating mechanism. The Gompertz function was initially developed to model human mortality rates, and has since been widely applied in biology. Notably, it also possesses a probabilistic interpretation, as it represents the cumulative distribution function (CDF) of the standard Gumbel distribution. While both the Sigmoid function and the Gaussian CDF exhibit reflection symmetry around the point (0, 0.5), the Gompertz function manifests a subtle rightward asymmetry, leading to distinct qualitative behavior.
% Some previous works have proposed using the Gompertz function to construct an activation function \citep{iliev2017note}, however this is done in a completely different way from ours.

Our experiments indicate that GoLU, compared to existing self-gated activations, effectively %reduces noise from the input signal, as evidenced by a \emph{reduced output variance}
\emph{reduces variance} in the latent representation. Moreover, it contributes to a \emph{smoother loss landscape}, making it less sensitive to small perturbations in the model parameters. Additionally, an analysis of the learned weights in our trained models reveals that GoLU induces a more \emph{spread weight distribution} compared to commonly used activations (see Section \ref{subsection-effects-on-training} for details).

A more spread weight distribution may indicate the network's ability to capture a diverse range of features from the data. On the other hand, variance reduction in activation outputs can help eliminate irrelevant information, allowing the network to focus on distinguishing patterns and potentially mitigate overfitting. However, overly broad weight distributions may introduce instability, % or lead to overfitting, 
while excessive variance reduction could result in underfitting and the loss of essential features, ultimately degrading performance. 
%GoLU effectively addresses this trade-off by achieving a balanced level of both weight distribution and variance reduction. This is supported by extensive, task-specific evaluations, which demonstrate that GoLU generally outperforms baseline activations, leading to improved performance across a variety of tasks (see Section \ref{sec:experiments}).  

Extensive, task-specific evaluations, suggest that GoLU effectively addresses this trade-off by achieving a balanced level of both weight distribution and variance reduction, leading to improved performance over baseline activations (see Section \ref{sec:experiments}). To facilitate reproducibility, we have made our code available at \url{https://github.com/automl/GoLU}.

% We conducted comprehensive experiments across diverse tasks, including image classification, detection, segmentation, language modeling, and diffusion, demonstrating the superior performance of our proposed GoLU activation function (see Section \ref{sec:experiments}).



\section{Gompertz Linear Unit}\label{sec:golu}

\subsection{Definition and Properties}
\label{sec:golu-properties}

In this section, we introduce the GoLU activation function and discuss its properties. GoLU is defined through Equations \ref{eq1} and \ref{eq2} and visualized in Figure \ref{figure-golu} (Left).
%
\begin{equation}
    \mathrm{GoLU}(x) = x \, \mathrm{Gompertz}(x)
    \label{eq1}
\end{equation}
%
\begin{equation}
    \mathrm{Gompertz(x)} = e^{-e^{-x}}
    \label{eq2}
\end{equation}
%
The gate function $\mathrm{Gompertz(x)}$ refers to the Gompertz function introduced in \citep{gompertz1825xxiv} and is plotted in Figure \ref{figure-golu} (Right).

% Some previous works have proposed using the Gompertz function as an activation function \citep{iliev2017note}. However, these earlier studies (apart from lacking empirical results) only utilize the Gompertz function to "construct" an activation and examine some of its properties, which they term PGAF. They do not explicitly suggest Gompertz or GoLU as activations for training neural networks.
%
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{images/2-golu/visualize_golu.png}
    \vspace{-6mm}
    \caption{GoLU activation (Left) and its Gompertz gate (Right). The Gompertz gate depicts a slight rightward shift.}
    \label{figure-golu}
\end{figure}
%
The Gompertz function can also be interpreted probabilistically, as it corresponds to the CDF of the standard Gumbel distribution, $\mathrm{Gumbel}(0,1)$, with probability density function
\begin{equation}
\mathrm{Gumbel}(x) = e^{-(x+e^{-x})}
\label{eq3}
\end{equation}
% That is:
% \begin{equation}
%     Gompertz(x) = \int_{-\infty}^x \!Gumbel(y)\, dy
%     \label{eq4}
% \end{equation}
%
% In the appendix, we further discuss the effect of introducing the parameters \textit{$\alpha$}, \textit{$\beta$}, and \textit{$\gamma$}, which provide the flexibility to rescale and shift GoLU. In the main part of the paper we set these parameters to 1 by default, which proves to be robust, as demonstrated in our experiments. Therefore, unless stated otherwise, GoLU represents a fixed static activation. 
From Equations \ref{eq1}, \ref{eq2} and Figure \ref{figure-golu}, we understand that, contrary to ReLU and its variants which are monotonic and non-smooth at zero, GoLU is a smooth and non-monotonic self-gated activation, similar to Swish and GELU. In fact the formulation of GoLU using exponentials makes it infinitely differentiable. However, in contrast to Sigmoid and the Gaussian CDF (i.e. the gate functions of Swish and GELU), the Gompertz function is asymmetric, as it does not mirror evenly around a central point. This asymmetry, which has a bias towards the right, arises from the inherent asymmetry of the Gumbel distribution, which favors positive input values. In fact, the right-leaning asymmetry of the Gumbel distribution leads to smaller gate values across the entire input range, inducing a compression effect on the output distribution. This behavior extends to GoLU, yielding output values closer to zero, both for positive and negative inputs, when compared to other gated activation functions, effectively reducing the magnitude of the activation output. We note that, while Mish also exhibits an asymmetric distribution, it is skewed to the left, producing the opposite effect relative to GoLU.

These properties are more clearly illustrated in Figure \ref{fig:comparison}, which provides a direct comparison between different activations (Left), as well as the gate functions of various gated activations (Middle) and their corresponding distributions (Right).

Additionally, from a more localized perspective, the Gompertz gate exhibits a reduced value in particular at the origin. This leads to a decreased steepness of GoLU near this point, as indicated by $\mathrm{GoLU}'(0)=\mathrm{Gompertz}(0)$ from Equation \ref{eq1}. This property of reduced slope magnitude is not confined to the origin but extends to a neighborhood around it and spans a substantial portion of the negative input domain. Additional details are provided in Appendix~\ref{app:golu-further-details}.
% In particular, this results in a reduced gate value at the origin, which in turn reduces the steepness of the activation near the origin, as indicated by $\mathrm{GoLU}'(0)=\mathrm{Gompertz}(0)$ from Equation \ref{eq1}. 
% The right-leaning asymmetry of the Gompertz gate translates into a smaller gate value at the origin, which in turn, results in reduced steepness of the activation near the origin, as indicated by $\mathrm{GoLU}'(0)=\mathrm{Gompertz}(0)$ from Equation \ref{eq1}. 
%
%
%
% This asymmetry of the gate function results in a reduced steepness of the activation around the origin as $\mathrm{GoLU}'(0)=\mathrm{Gompertz}(0)$ from Equation \ref{eq1}.

% To provide a clearer comparison between GoLU and baseline activations Figure \ref{fig:comparison} (Left) presents their activation profiles in a single plot, while Figures \ref{fig:comparison} (Middle and Right) depict the gate functions and the corresponding distributions, respectively.
%
% The smaller value of the Gompertz gate and the reduced steepness of the GoLU activation near the origin are more clearly illustrated in Figure \ref{fig:comparison} (Middle and Left, respectively), which provides a direct comparison with baseline activations. Additionally, Figure \ref{fig:comparison} (Right) presents the distributions associated with the gate functions. 

In the large negative region, the Gompertz gate, and consequently the GoLU activation, decays extremely rapidly as a double exponential, suppressing unimportant features like ReLU, while maintaining smoothness, unlike ReLU. 

Compared to the Gaussian CDF and the Sigmoid function, the Gompertz gate %therefore 
initially exhibits a flat plateau, followed by a steeper growth rate that aligns more closely with the Gaussian CDF.  As the input values become large and positive, the growth rate flattens and resembles the Sigmoid function, with the difference falling off as $\mathcal{O}(e^{-2x})$ (see Appendix~\ref{app:golu-further-details}). 
% This behavior introduces a form of duality, effectively interpolating between the two.

% Compared to the Gaussian CDF and the Sigmoid functions, the Gompertz function initially exhibits a steeper growth rate closer to the Gaussian CDF, while in the later stage, the growth rate flattens and resembles the Sigmoid function, with the difference falling off as $\mathcal{O}(e^{-2x})$. This behavior introduces a form of duality, effectively interpolating between the two. 
% [see App for further details: describe this mathematically]
%
\begin{figure*}[t]
    \centering
    \raisebox{-0.5mm}{
    \includegraphics[width=0.298\linewidth]{images/2-golu/full_scale_neuron_space.png}}
    \hspace{0.5mm}
    \includegraphics[width=0.29\linewidth]{images/2-golu/gates.png}
    \hspace{-2mm}
    \raisebox{-1mm}{
    \includegraphics[width=0.39\linewidth]{images/2-golu/distributions.png}}
    \vspace{-3mm}
    \caption{Comparison of different activation functions (Left) gate functions for gated activations (Middle) and the corresponding distributions (Right).}
    \label{fig:comparison}
\end{figure*}
%
\subsection{Effects on Training Dynamics}
\label{subsection-effects-on-training}

The distinctive properties of GoLU influence the training dynamics, as we will outline here.

\paragraph{Variance reduction} 

% The asymmetric nature of the Gompertz gate and its resulting smaller gradient of the activation near the origin, imply reduced sensitivity to changes in the input. This effectively promotes smoother activation outputs and reduces noise within the latent representations, differentiating between strong and weak features.
% % which can be visualized in Figure \ref{figure-output-landscape}.

As illustrated in Figure~\ref{fig:comparison} (Left), GoLU exhibits a profile that remains closest to the x-axis across the entire input range. Moreover, its slope, particularly near the origin and over a substantial portion of the negative input domain, is smaller in magnitude compared to other gated activations, as pointed out in Section~\ref{sec:golu-properties}. These characteristics suggest a reduced sensitivity of the activation output to variations in the input%(though this behavior is not universally guaranteed and may depend on the specific input distribution)
. This effectively reduces variance in the latent representations, and promotes smoother activation outputs, enhancing the model's ability to differentiate between strong and weak features.


% As illustrated in Figure~\ref{fig:comparison} (Left), GoLU demonstrates a profile that is closest to the x-axis over the entire input range. 
% %exhibits the smallest slope at the origin compared to baseline activation functions. 
% This behavior stems from the inherent asymmetry of the Gumbel distribution%(Figure~\ref{fig:comparison} (Right))
% , which allocates a relatively smaller probability to the same input, resulting in smaller output values of the Gompertz gate%Figure~\ref{fig:comparison} (Middle))
% , as discussed in Section \ref{subsection-properties-of-golu}. The smaller magnitude of the GoLU activation reduces the sensitivity of the activation output to variations in the input. This effectively reduces variance and noise in the latent representations, and promotes smoother activation outputs, enhancing the model's ability to differentiate between strong and weak features.
%
To visually illustrate this phenomenon, we process %generated using DALL-E 3 
Figure~\ref{fig:cake} (Left) through a $3 \times 3$ 2D Convolution followed by 2D Batch Normalization. The resulting pre-activation is then passed through various activation functions, and the pixel distributions of the normalized pre-activation and activation maps are plotted for GoLU, GELU, and Swish in Figure~\ref{fig:cake} (Right). As observed, GoLU exhibits a distinctive ``squeezing effect", compressing the same distribution into a smaller output range, and reducing variance most, compared to GELU and Swish.
% We conduct a straightforward experiment to demonstrate this effect. We randomly sample four images from the CIFAR-10 dataset. These images are passed through a $3\times3$ 2D-Convolution followed by a 2D-BatchNorm. The pre-activations are then passed through various activation functions, and we observe the resulting variance of the activated signal (see Table \ref{table:var-four-images-cifar10}). 
% The slight shift of the Gompertz gate to the right, allocates a relatively smaller probability to the same input and effectively reduces noise for GoLU as compared to baseline activations. Table.\ref{table:var-four-images-cifar10} highlights this reduction in variance achieved by GoLU compared to widely-used activations, enabling smoother data representation.
%
% [Refer also to LN experiments]
%
\begin{figure}[H]
        \centering
        % Top two images
        \raisebox{3.6mm}{
        \includegraphics[width=0.35\linewidth]{images/2-golu/GoLU_chocolate_cake.png}}
        \includegraphics[width=0.62\linewidth]{images/2-golu/GoLU_chocolate_cake_kde.webp.png}
        \vspace{-6mm}
        \caption{Image created by Dall-E 3 (Left) and kernel density estimation curves for distributions of activation outputs for the image (Right). GoLU reduces variance most compared to baseline activations.}
        \label{fig:cake}
\end{figure}
%
% To better visualize this phenomenon, we analyze an image created using Dall-E 3 and plot the pixel distributions of the normalized pre-activation and activation maps for GoLU, GELU and Swish (Figure~\ref{fig:cake}). GoLU, as seen exhibits a ``squeezing effect", representing the same distribution within a smaller output range compared to GELU and Swish.
To further substantiate this observation, we randomly sample four images from the CIFAR-10 dataset, apply the same preprocessing pipeline, and pass the results through different activation functions. The variances of the activated signals, summarized in Table~\ref{table:var-four-images-cifar10}, highlight GoLU’s ability to achieve a notable reduction in variance compared to widely-used activations, enabling smoother data representation.
\begin{table}[H]
\centering
% \renewcommand{\arraystretch}{1.2}
\small
\caption{Variances of randomly sampled images from CIFAR-10 after applying a 3x3 Convolution followed by Batch Normalization and further passing the feature maps through different activations.} %The four sampled images belong to class Frog, Airplane, Deer and Automobile respectively.} % The randomly sampled images for this experiment can be visualized in Figure\ref{figure-intro-images} under Appendix \ref{subsection-intro-experiments}
\vspace{2mm}
\begin{tabular}{|c|c|c|c|c|}
          \hline
          \raisebox{-3mm}{\textbf{Activation}} & \raisebox{-7mm}{\includegraphics[width=9mm, trim={0 0 0 11.5mm}, clip]{images/2-golu/frog.png}} &  \raisebox{-7mm}{\includegraphics[width=9mm, trim={0 0 0 11mm}, clip]{images/2-golu/airplane.png}} & \raisebox{-7mm}{\includegraphics[width=9mm, trim={0 0 0 11mm}, clip]{images/2-golu/deer.png}} & \raisebox{-7mm}{\includegraphics[width=9mm, trim={0 0 0 11mm}, clip]{images/2-golu/automobile.png}} \\[7mm]
          % \textbf{Activation} & \textbf{Image 1} & \textbf{Image 2} & \textbf{Image 3} & \textbf{Image 4} \\
          % \cline(lr){2-3} \cline(lr){4-5} \cline(lr){6-7} \cline(lr){8-9}
           % & $\sigma^2_1$ & $\sigma^2_2$ & $\sigma^2_3$ & $\sigma^2_4$ \\
          \hline
          ReLU  & 0.3024  & 0.3063 & 0.3627  & 0.3594 \\  
          LeakyReLU & 0.3055 & 0.3100 & 0.3639 & 0.3626 \\
          % PReLU & 0.2933 & 0.4034 & 0.3197 & 0.4179 & 0.1793 & 0.4239 & 0.3010 & 0.4598 \\
          ELU  & 0.5677 & 0.6227 & 0.4699 & 0.6418 \\
          % SELU & 0.0167 & 0.9354 & 0.0038 & 1.0480 & 0.0694 & 0.6704 & -0.0109 & 1.0239 \\
          GELU & 0.2995 & 0.3102 & 0.3583 & 0.3701 \\
          Swish & 0.2685 & 0.2872 & 0.3332 & 0.3399 \\
          Mish & 0.3448 & 0.3700 & 0.3677 & 0.4200 \\
          \textbf{GoLU} & \textbf{0.2133} & \textbf{0.2150} & \textbf{0.3213} & \textbf{0.2783} \\
          \hline
        \end{tabular}
\label{table:var-four-images-cifar10}
\end{table}
Finally, to illustrate this effect in a fully trained model, we randomly sample three images from the ImageNet-1k dataset and pass them through a ResNet-50 model trained on ImageNet-1k. As shown in Figure~\ref{fig:learned-latent-dists}, the output distributions of the final activations demonstrate that GoLU produces a more peaked distribution compared to other activation functions, highlighting this distinctive effect on latent representations.
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.3\linewidth]{images/2-golu/latent_dist_1.png}
    \hfill
    \includegraphics[width=0.3\linewidth]{images/2-golu/latent_dist_2.png}
    \hfill
    \includegraphics[width=0.3\linewidth]{images/2-golu/latent_dist_3.png}
    \vspace{-2mm}
    \caption{Distributions of final activation outputs of ResNet-50 trained on ImageNet-1k for three randomly sampled images from ImageNet-1k. GoLU leads to a more peaked distribution for the final activation output.}
    \label{fig:learned-latent-dists}
\end{figure*}


This lower activation variance can be seen as a form of implicit regularization as the network's representation of the input becomes smoother, focusing on the core patterns rather than fine-grained details or noise.


\paragraph{Smooth loss landscape}
% Smaller gradients of GoLU help the optimizer avoid spiky regions of the parameter space and encourage convergence to flatter minima. 
Reduced activation variance results in less noisy and more consistent gradients. This typically means that the loss function changes more smoothly with respect to model parameters. As a result, the optimizer is more likely to converge to flatter regions of the loss landscape with smaller curvature.
This is expected to result in better robustness to small perturbations of the model parameters. %We also observe a smoother loss landscape for GoLU which can be a result of %reduced variance in activations. 
We explore this by adding two different Standard Normal noise terms, scaled independently by $\alpha, \beta$, %with independent scaling factors
to the weights of ResNet-20 trained on CIFAR-10. We compute the test loss across a grid of scaling factors $\alpha, \beta$ for the two terms, while keeping the noises constant (refer to Appendix \ref{app:loss-landscape} for more details). ResNet-20 with GoLU shows relatively smoother, less-spiked loss landscapes compared to other activations (Figure \ref{fig:resnet20-cifar10-loss-landscapes-with-golu}) which implies better generalization and noise robustness with GoLU. In contrast, ReLU’s non-smooth nature produces a highly-spiked landscape.
%
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{images/2-golu/loss_landscapes.png}
    \vspace{-5mm}
    \caption{The loss landscape on the test set of ResNet-20 trained on CIFAR-10 with ReLU, GELU, Swish and GoLU after adding random, scaled perturbations to the learned weights (refer to Appendix \ref{app:loss-landscape} for more details).}
    \label{fig:resnet20-cifar10-loss-landscapes-with-golu}
\end{figure}
%
% \paragraph{Spread weight distribution} In contrast to the reduced variance in latent space, we observe a more spread distribution in the learned weights of our models trained with GoLU. Figure \ref{fig:learned-param-dists} compares weight distributions of ResNet-50 and ViT-B/16 trained on ImageNet-1k, GPT2-S trained on OWT and DeepLabV3-RN50 trained on MS-COCO, with different activation functions. The broader weight distribution for GoLU suggests that the network has learned more diverse transformations, enhancing its capacity to distinguish between features in the data.

% This may reflect the network’s response to reduced activation variance, counterbalancing it by spreading the weights to maintain representational diversity. Specifically, reduced output variance naturally leads to more uniform gradients, which in turn encourages a broader spread of weights.
\paragraph{Spread weight distribution} In contrast to the reduced variance in the latent space, we observe a wider distribution in the learned weights of our models trained with GoLU, at least in the region where most weights are concentrated. Figure \ref{fig:learned-param-dists} compares non-normalization\footnote{As learned transformations in the model are mainly encoded in the weights of fully connected, convolutional or attention layers, it is more meaningful to exclude parameters of Batch Normalization and Layer Normalization layers, although including these parameters we obtain qualitatively similar distributions.} weight distributions of ResNet-50 and ViT-B/32 trained on ImageNet-1k and GPT2-S (124M) trained on OpenWebText%and DeepLabV3-RN50 trained on MS-COCO
, with different activation functions. The broader weight distribution for GoLU around the peak suggests that the network has learned more diverse transformations, enhancing its capacity to distinguish between features in the data.

This may reflect the network’s response to reduced activation variance, counterbalancing it by spreading the weights around the peak to maintain representational diversity. Specifically, reduced output variance naturally leads to more uniform gradients, which in turn encourages a broader spread of weights.

Notice that a wider weight distribution around the peak does not necessarily translate to a larger \textit{overall} variance.
%, even though this happens
%for ResNet-50 and GPT2-S in Figure~\ref{fig:learned-param-dists}. The exception is the ViT-B/32 model where GoLU ranks second to Mish in terms of overall variance. This is attributed to Mish's higher density at very large weight values, which may indicate that the model is relying heavily on certain features. 
%In fact, 
However, focusing on the bulk of the distribution\footnote{Specifically, we take the intersection of the middle 98\% intervals of the parameter distributions of an architecture trained with each activation.}, we find that GoLU consistently achieves the highest variance.
This behavior suggests that networks trained with GoLU effectively suppress density in extreme values while expanding the distribution around the peak. Such a pattern implies that the model captures a broader range of \textit{meaningful} transformations without over-reliance on extreme parameter values or certain features.
% In fact the ability of the network trained with GoLU to reduce density in the extreme values while widening the distribution near the peak suggests that the model captures a broader range of \textit{meaningful} transformations without over-reliance on extreme parameter values.
%
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.3\linewidth]{images/2-golu/rn_weights.png}
    \hfill
    \includegraphics[width=0.3\linewidth]{images/2-golu/vit_weights.png}
    \hfill
    \includegraphics[width=0.3\linewidth]{images/2-golu/gpt_weights.png}
    \vspace{-2mm}
    \caption{Learned-weight distribution of ResNet-50 and ViT- B/16 trained on ImageNet-1k and GPT2-S trained on OWT. GoLU leads to a more spread weight distribution. The range of parameters is clipped for better visualization.}
    \label{fig:learned-param-dists}
\end{figure*}
%
% %
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\linewidth]{images/2-golu/learned_param_dists_grid.png}
%     \vspace{-5mm}
%     \caption{Learned weight distribution of ResNet-50 and ViT- B/16 trained on ImageNet-1k, GPT2-S trained on OWT and DeepLabV3-RN50 trained on MS-COCO. GoLU leads to a more spread weight distribution. The range pf parameters is clipped for better visualization.}
%     \label{fig:learned-param-dists}
% \end{figure}
% %
% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=\linewidth]{images/2-golu/learned_param_dists.png}
%     \caption{Distribution of the learned weights of ResNet-50 and ViT- B/16 trained on ImageNet-1k, GPT2-S trained on OWT and DeepLabV3-RN50 trained on MS-COCO with gated activations GELU, Swish, Mish and GoLU. GoLU lead to a more spreag weigth distribution.}
%     \label{fig:learned-param-dists}
% \end{figure*}

% The variance reduction in the latent space aligns with Adaptive Dropout \cite{ba2013adaptive}, where information propagation depends on the input. Models trained with GoLU exhibit strong regularization benefits, as seen in the learned weights, which peak less around zero and promote generalization (see Figure \ref{fig:learned-param-dists}). Note that the spiked density at zero for Swish and Mish differs from L1 regularization: L1 adds a bias term in the loss penalty, while GoLU reduces variance for regularization.



% The generalization and noise robustness of models with GoLU can be explored by adding scaled Standard Normal Gaussian noise to the weights of ResNet-20 \cite{he2016deep} trained on CIFAR-10. We compute the test loss across a grid of scaling factors while keeping the noise constant. ResNet-20 with GoLU shows smoother, less-spiked loss landscapes compared to other activations (Figure \ref{fig:resnet20-cifar10-loss-landscapes-with-golu}). In contrast, ReLU’s non-smooth nature produces highly-spiked landscapes. GoLU’s benefits align with Dropout, capturing non-linearity while avoiding co-adaptation of weights, ensuring independence within themselves.

We emphasize that the effects attributed to GoLU, as described above, are not guaranteed to hold universally across all scenarios but rather represent general trends observed in our empirical findings.

Moreover, while asymmetry has been highlighted as a distinctive feature of GoLU, it is important to note that its high performance, detailed in the next section, cannot be solely attributed to asymmetry, but arises from an intricate interplay of properties, described in Section \ref{sec:golu-properties}.



\section{Experiments and Results}\label{sec:experiments}

\subsection{Overview of Experiments}
\label{subsection-overview-of-experiments} 

We conducted experiments across various architectures and datasets, spanning a diverse range of tasks in both vision and language modeling. We begin with image classification, training ResNet-18, 34, 50 \citep{he2016deep}, WideResNet-50-2 \citep{zagoruyko2016wide}, DenseNet-121 \citep{huang2017densely}, EfficientNet-B0 \citep{tan2019efficientnet}, TinyViT \cite{tiny_vit}, ViT-B/32 and ViT-B/16 \citep{dosovitskiy2020image} on ImageNet-1k \cite{deng2009imagenet}.

We then extend our experiments to language modeling. We train babyGPT on the TinyStories (TS) \citep{eldan2023tinystories} dataset and GPT2-S \citep{radford2019language} on the OpenWebText (OWT) \citep{Gokaslan2019OpenWeb} dataset, leveraging the nanoGPT repository \cite{karpathy2023nanogpt}. 

Additionally, we assess GoLU’s performance on Semantic Segmentation (DeepLabV3 \cite{chen2017rethinking}), Object Detection (Faster R-CNN-FPN \cite{ren2015faster}, RetinaNet-FPN \cite{lin2017focal}), and Instance Segmentation (Mask R-CNN-FPN \cite{he2017mask}) on MS-COCO \cite{lin2014microsoft}, leveraging our pre-trained ResNet-50 backbone on ImageNet-1k. Further, we test GoLU on Denoising Diffusion Probabilistic Models \cite{ho2020denoising} on the CelebA \cite{liu2015faceattributes} dataset.

We closely follow established baselines for all model architectures and tasks, ensuring that the integration of GoLU is the primary change. Hyperparameters, optimizers, learning rate schedules, and other training settings are aligned with the standard practices for each task.
% Finally, we conduct ablation studies to examine the impact of learning rates (see Appendix \ref{sec:learning-rate-ablation}) and GoLU’s parameters $\alpha$, $\beta$, and $\gamma$ (see Appendix \ref{sec:golu-parameter-ablation}).
All our experiments are conducted on three seeds and the results are averaged out and reported with the standard error.

In Appendix~\ref{cd-analysis} we further present a Critical Difference analysis to systematically compare the overall performance of activation functions. 
%
Finally, in Appendix~\ref{sec:lcpfn}, we explore the application of GoLU to the task of learning curve extrapolation.


% To evaluate GoLU's impact, we conduct experiments across multiple tasks, architectures, and datasets. Initially, we perform ablation studies on Multi-Layer Perceptrons (MLPs) using the CIFAR-10 dataset \cite{Krizhevsky09learningmultiple}. Further, we train various architectures including ResNets \cite{he2016deep}, WideResNet \cite{zagoruyko2016wide}, DenseNet \cite{huang2017densely}, and ViTs \cite{dosovitskiy2020image} on the same dataset (see Appendix \ref{sec:cifar-10-results}). For large-scale classification, we train ResNets, WideResNet-50-2, DenseNet121, EfficientNet-B0 \cite{tan2019efficientnet}, and ViTs on ImageNet-1k \cite{deng2009imagenet} using Torchvision \cite{torchvision2016} and Timm \cite{rw2019timm}. 

% We extend our experiments to language modeling, training BabyGPT and GPT2-S \cite{radford2019language} on Tiny Stories \cite{eldan2023tinystories} and Open Web Text \cite{Gokaslan2019OpenWeb} datasets leveraging the nanoGPT repository \cite{karpathy2023nanogpt}.

% Additionally, we assess GoLU’s performance on Semantic Segmentation (DeepLabV3 \cite{chen2017rethinking}), Object Detection (Faster R-CNN-FPN \cite{ren2015faster}, RetinaNet-FPN \cite{lin2017focal}), and Instance Segmentation (Mask R-CNN-FPN \cite{he2017mask}) on MS-COCO \cite{lin2014microsoft}, leveraging our pre-trained ResNet-50 on ImageNet-1k. Further, we also test GoLU on Denoising Diffusion Probabilistic Models \cite{ho2020denoising} on the CelebA \cite{liu2015faceattributes} dataset.

% Finally, we conduct ablation studies to examine the impact of learning rates (see Appendix \ref{sec:learning-rate-ablation}) and GoLU’s parameters $\alpha$, $\beta$, and $\gamma$ (see Appendix \ref{sec:golu-parameter-ablation}). Note - all our experiments are done on three seeds and the results are averaged out and reported with the standard error.


\subsection{Image Classification} % - ImageNet-1k}
\label{subsec:imagenet-1k}

 %, using ReLU, LeakyReLU, ELU, GELU, Swish, Mish, and GoLU.
%
% The ImageNet-1k dataset, comprises of 1.28M training images and 50K test images across 1000 classes. We train ResNets 18, 34, and 50, WideResNet-50-2, DenseNet121, EfficientNet-B0, ViT-B/32 and ViT-B/16 from Torchvision and the timm library \cite{rw2019timm} and TinyViT from \cite{tiny_vit}. 
% Table \ref{tab:imagenet-pre-training} presents the top-1 test accuracies with standard errors for each architecture using ReLU, LeakyReLU, ELU, GELU, Swish, Mish, and GoLU.
\begin{table*}[t]
    \small
    \centering
    \renewcommand{\arraystretch}{1.2}
    \caption{Top-1 test accuracy of ResNets 18, 34 and 50, WideResNet-50-2, DenseNet-121, EfficientNet-B0, TinyViT, ViT-B/32 and ViT-B/16 on ImageNet-1k.}
    
    \begin{tabular}{|c|cccccc|c|}
         \hline
    
         \textbf{Architecture} & \textbf{ReLU} & \textbf{LeakyReLU} & \textbf{ELU} & \textbf{GELU} & \textbf{Swish} & \textbf{Mish} & \textbf{GoLU}\\
         
         \hline
         
         ResNet-18 & 69.74±0.07 & 69.78±0.04 & 67.10±0.07 & \underline{70.66±0.05} & 70.60±0.06 & 70.53±0.06 & \textbf{70.76±0.06}\\
         
         ResNet-34 & 73.26±0.01 & 73.25±0.03 & 69.27±0.09 & \underline{73.44±0.04} & 72.74±0.05 & 72.73±0.07 & \textbf{73.71±0.04}\\
         
         ResNet-50 & 75.44±0.07 & 75.67±0.08 & 71.87±0.09 & \underline{76.07±0.06} & 75.17±0.14 & 75.53±0.09 & \textbf{76.63±0.03}\\
         
         WideResNet-50-2 & 76.96±0.07 & \underline{77.17±0.12} & 71.90±0.01 & 76.72±0.01 & 75.41±0.03 & 75.75±0.19 & \textbf{77.37±0.03}\\
         
         DenseNet-121 & 74.95±0.09 & \underline{75.03±0.06} & 68.95±0.04 & 74.64±0.11 & 72.81±0.06 & 72.97±0.10 & \textbf{75.25±0.03}\\
         
         EfficientNet-B0 & 76.52±0.07 & 76.65±0.04 & 76.21±0.04 & \textbf{76.90±0.01} & 76.84±0.02 & 76.76±0.06 & \underline{76.86±0.04}\\

         TinyViT &  82.91±0.02 & 82.83±0.03 & 80.29±0.07 & \underline{83.05±0.03} & 82.92±0.06 & 83.01± 0.02 & \bf{83.21±0.02} \\
         
         ViT-B/32 & 74.51±0.04 & 74.53±0.03 & 65.82±0.07 & \underline{75.48±0.05} & 72.31±2.15 & 75.16±0.07 & \textbf{75.74±0.09}\\
         
         ViT-B/16 & \underline{80.06±0.05} & 79.93±0.02 & 73.36±0.16 & 79.39±0.99 & 79.19±0.94 & 77.97±1.95 & \textbf{80.72±0.04}\\
         
         \hline
    \end{tabular}
    \label{tab:imagenet-pre-training}
\end{table*}

We evaluate GoLU's performance in image classification tasks on ImageNet-1k, comparing it against six state-of-the-art activation functions, ReLU, LeakyReLU, ELU, GELU, Swish and Mish.

Table \ref{tab:imagenet-pre-training} presents the top-1 test accuracies with standard errors for ResNets 18, 34 and 50, WideResNet-50-2, DenseNet-121, EfficientNet-B0, ViT-B/32, ViT-B/16 and TinyViT \cite{tiny_vit}. The training settings, detailed in Appendix \ref{sec:experimental-details-ic}, are adopted from Torchvision \cite{torchvision2016lib} for all experiments except EfficientNet-B0 which is taken from the timm library \cite{rw2019timm} and TinyViT which is taken from \cite{tiny_vit}.
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{images//3-experiments-and-results/resnet50_plots.png}
    \vspace{-5mm}
    \caption{ResNet-50 test loss (Left) and test top-1 accuracy (Right) on ImageNet-1k.}
    \label{fig:resnet50-test-loss-accuracy}
\end{figure}
%
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{images//3-experiments-and-results/vit_b_32_plots.png}
    \vspace{-5mm}
    \caption{ViT-B/32 test loss (Left) and test top-1 accuracy (Right) on ImageNet-1k.}
    \label{fig:vitb32-test-loss-accuracy}
\end{figure}
As highlighted, GoLU consistently outperforms the standard activation functions across all architectures, with the exception of EfficientNet-B0, where the performance difference is minimal. Notice that EfficientNet-B0 is an exception because its nonlinearity arises not only from activation functions (which are replaced) but also from a squeeze-and-excitation block, which remains unchanged in our experiments. 
% As shown in Table \ref{tab:imagenet-pre-training}, GoLU outperforms standard activation functions in all architectures, except EfficientNet-B0, where the difference is minimal. 
For ResNet-50 and ViT-B/32, test loss and test accuracy curves are shown in Figures \ref{fig:resnet50-test-loss-accuracy} and \ref{fig:vitb32-test-loss-accuracy}, respectively, where GoLU consistently delivers lower test loss and higher top-1 accuracy over the epochs. GELU is generally the second-best performer, while ELU performs worst across most architectures. %{\color{orange} For ViT-B/16, GELU exhibits slight instability in the first seed, leading to a decrease in accuracy. However, the average top-1 accuracy over three seeds for GoLU is 80.72±0.04, outperforming GELU’s top-1 accuracy of 80.608±0.04 averaged over two seeds.}
%

We further evaluate GoLU on CIFAR-10, comparing it against top baseline activations. We report in Table \ref{tab:image-classification-cifar10} the results of image classification on CIFAR-10, with ResNets 20, 32, 44, 56, and 110, WideResNet28-2, DenseNet40 and ViT-Ti/16-224. GoLU consistently outperforms the standard baselines across all tested architectures. We have further underlined the second-best activations for each model. No single activation consistently ranks second. % Appendix \ref{sec:cifar-10-results}
%
% ResNets, WideResNet-50-2 and DenseNet121 are trained for 90 epochs with a batch size of 256, SGD with momentum, learning rate 0.1, and weight decay \(1 \times 10^{-4}\). EfficientNet-B0 is trained using the timm library for 450 epochs with a batch size of 1536 using RMSProp \cite{hinton2012neural} with an initial learning rate of 0.048 and a weight decay of \(1 \times 10^{-5}\). ViT models are trained for 300 epochs with a batch size of 4096 using AdamW \cite{loshchilov2017fixing} with an initial learning rate of \(3 \times 10^{-3}\) and weight decay of 0.3. Various regularization techniques are applied, including Exponentially Moving Averaged Weights \cite{tarvainen2017mean}, AutoAugment \cite{cubuk2019autoaugment}, RandAugment \cite{cubuk2020randaugment}, MixUp \cite{zhang2017mixup}, CutMix \cite{yun2019cutmix} and Label Smoothing \cite{szegedy2016rethinking} for EfficientNet-B0 and ViT models.
%
\begin{table}[h]
    \tiny
    \centering
    \renewcommand{\arraystretch}{1.2}
    \caption{Top-1 test accuracy on CIFAR-10. GoLU consistently outperforms baselines. Second best activations are underlined.}
    % \vspace{-1mm}
    \begin{tabular}{|c|c@{\hskip 10pt}c@{\hskip 10pt}c@{\hskip 10pt}c|c|}
         \hline
         
         \textbf{Arch.} & \textbf{ReLU} & \textbf{LeakyReLU} & \textbf{GELU} & \textbf{Swish} & \textbf{GoLU}\\
         
         \hline
         
         RN-20 & 91.41±0.1 & 91.60±0.0 & 91.62±0.0 & \underline{91.64±0.1} & \textbf{91.77±0.1}\\
         
         RN-32 & 92.21±0.1 & 92.40±0.0 & \underline{92.54±0.1} & 92.16±0.0 & \textbf{92.69±0.1}\\
         
         RN-44 & 92.58±0.0 & \underline{92.78±0.0} & \underline{92.78±0.8} & 92.51±0.0 & \textbf{92.85±0.0}\\
         
         RN-56 & 92.80±0.1 & 92.75±0.1 & 92.86±0.1 & \underline{92.93±0.1} & \textbf{93.15±0.1}\\
         
         RN-110 & \underline{93.21±0.0} & 93.18±0.1 & 92.75±0.1 & 92.23±0.0 & \textbf{93.25±0.0}\\
         
         WRN-28-2 & \underline{94.96±0.0} & 94.81±0.0 & 94.55±0.1 & 93.58±0.1 & \textbf{95.03±0.0}\\
         
         DN-40 & 93.13±0.1 & 93.13±0.1 & \underline{93.41±0.0} & 93.30±0.1 & \textbf{93.44±0.1}\\
         
         ViT-Ti & \underline{91.74±0.06} & 91.61±0.18 & 91.37±0.11  & 88.61±0.16 & \bf{92.60±0.05} \\
         
         % ViT-B/16 & 97.32±0.1 & 96.94±0.0 & \textbf{97.89±0.0} & 97.17±0.1 & 97.88±0.0\\ % ViT-B is not suitable for training on CIFAR10
         
         \hline
    \end{tabular}
    \label{tab:image-classification-cifar10}
\end{table}



\subsection{Language Modeling}% - TinyStories \& OpenWebText}
\label{subsec:language-modelling}

We train babyGPT on TS and GPT2-S (124M) on OWT, both sourced from the nanoGPT repository \cite{karpathy2023nanogpt}. As shown in Table \ref{tab:baby-gpt-gpt2-s-pre-training}, GoLU demonstrates superior performance, achieving lower perplexity and higher token accuracy on both babyGPT and GPT2-S. GoLU's superiority is also evident in the test loss curves in Figures \ref{fig:babygpt-tiny-stories-plots} and \ref{fig:gpt2s-open-web-text-plots}. The general trend of GELU being the second-best activation function holds in language modeling as well.
% However, especially, for GPT2-S trained with GoLU, we observe slight overfitting beyond 575K iterations, evident in Figure \ref{fig:gpt2s-open-web-text-plots}. However, exactly at 575K iterations, GoLU already reached its optimum performance as per the provided setting. This implies GoLU's ability to quickly converge models which may require less training time in comparison to other baseline activations when provided with optimal hyperparemeter settings.
\begin{table}[H]
    \tiny
    \centering
    \renewcommand{\arraystretch}{1.2}
    \caption{Test perplexity score and test token accuracy of babyGPT and GPT2-S trained on TS and OWT respectively.}
    \vspace{-1mm}
    \begin{tabular}{|c|cc|cc|}
        \hline
         \multirow{2}{*}{\textbf{Activation}} & \multicolumn{2}{c|}{\textbf{babyGPT - TinyStories}} & \multicolumn{2}{c|}{\textbf{GPT2-S - OpenWebText}} \\
        \cline{2-5}
         & \textbf{Perplexity} & \textbf{Token Accuracy} & \textbf{Perplexity} & \textbf{Token Accuracy} \\
        \hline
        
        ReLU & 4.519±0.006 & 61.243±0.030 & 17.845±0.078 & 44.059±0.079\\
        
        LeakyReLU & 4.516±0.005 & 61.237±0.032 & 17.778±0.125 & 44.103±0.074\\

        ELU & 4.872±0.005 & 59.859±0.027 & 18.375±0.035 & 43.721±0.040\\
        
        GELU & \underline{4.462±0.005} & \underline{61.465±0.034} & \underline{17.525±0.015} & \underline{44.262±0.042} \\
        
        Swish & 4.535±0.004 & 61.178±0.032 & 17.785±0.026 & 44.155±0.025\\

        Mish & 4.539±0.007 & 61.135±0.036 & 17.797±0.086 & 44.104±0.081\\ \hline
        
        GoLU & \textbf{4.444±0.005} & \textbf{61.545±0.029} & \textbf{17.297±0.023} & \textbf{44.413±0.023}\\

        \hline
    \end{tabular}
    \label{tab:baby-gpt-gpt2-s-pre-training}
\end{table}
%
\vspace{-4mm}
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{images//3-experiments-and-results/baby_gpt_plots.png}
    \vspace{-5mm}
    \caption{babyGPT test loss (Left) and test token accuracy (Right) on TS.}
    \label{fig:babygpt-tiny-stories-plots}
\end{figure}
%
\vspace{-2mm}
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{images//3-experiments-and-results/gpt2_plots.png}
    \vspace{-5mm}
    \caption{GPT2-S test loss (Left) and test token accuracy (Right) on OWT.}
    \label{fig:gpt2s-open-web-text-plots}
\end{figure}
Appendix \ref{sec:experimental-details-lm} outlines the architectural details and provides additional information on the datasets and training settings.



% Both, TinyStories and OpenWebText datasets are popular benchmarks for training language models. The TinyStories dataset consists of 2,119,719 data points in the training set and 21,990 in the test set, while the OpenWebText dataset has 8,009,762 data points in the training set and 4,007 data points in the test set. We train babyGPT on TinyStories and GPT-2 Small on OpenWebText, both sourced from the nanoGPT repository by Andrej Karpathy. Both models have a vocabulary size of 50,304 and a maximum sequence length of 1024.

% The babyGPT version of the GPT-2 series consists of 6 layers, 6 attention heads, and an embedding dimension of 384, with a feed-forward expansion dimension of 1536 output features. The model is trained for 10,000 iterations with a batch size of 640, using the AdamW optimizer. The initial learning rate is \(1 \times 10^{-3}\), with a minimum learning rate of \(1 \times 10^{-4}\), a weight decay of 0.1, and a gradient clipping norm of 1.0. A Cosine learning rate scheduler is applied with a linear warmup for the first 100 iterations. Similarly, the GPT2-S model consists of 12 layers, 12 attention heads, and an embedding dimension of 768. It trains for 600,000 iterations with a batch size of 480, using the AdamW optimizer (with \(\beta_2 = 0.95\)). The initial learning rate is \(6 \times 10^{-4}\), with a minimum learning rate of \(6 \times 10^{-5}\), a weight decay of 0.1, and a gradient clipping norm of 1.0. The Cosine learning rate scheduler is employed with a linear warmup for the first 2,000 iterations.

% As shown in Table \ref{tab:baby-gpt-gpt2-s-pre-training} and Figures \ref{fig:babygpt-tiny-stories-plots} and \ref{fig:gpt2s-open-web-text-plots}, GoLU demonstrates superior performance, achieving lower perplexity and higher token accuracy across both babyGPT and GPT2-S on the TinyStories and OpenWebText datasets. GoLU's superiority is also evident in the provided loss curves. The general trend of GELU being the second-best activation function holds in language modeling as well.


\subsection{Semantic Segmentation}% - MSCOCO}
\label{subsec:semantic-segmentation}

For Semantic Segmentation, we train DeepLabV3 on the MS-COCO dataset with PASCAL-VOC labels, from the Torchvision benchmark (see Appendix~\ref{exp-details-sem-seg}). We employ our ResNet-50 backbone, pre-trained on ImageNet-1k. 
%The MS-COCO dataset with PASCAL-VOC labels contains 92,518 data points in the training set and 5,000 data points in the test set. The original MS-COCO dataset contains 117,266 data points in the training set. However, the existing benchmark pre-processes and removes images that either lack valid annotations or contain only small objects with an area coverage of less than 1,000 pixels. This ensures the retention of meaningful data points for training the model.

% The DeepLabV3-ResNet-50 model is trained for 30 epochs with a batch size of 32, using SGD with momentum \(0.9\), a learning rate of \(2 \times 10^{-2}\), weight decay of \(1 \times 10^{-4}\), and a polynomial learning rate scheduler with a power of 0.9.

% Table \ref{tab:semantic-seg-pre-training} shows the test loss and test mIoU the original learning rate of 0.02. GoLU achieves the lowest test loss

% We conduct our experiments with the original learning rate of 0.02. However, through a small ablation study of this hyperparameter, we find that this is not the optimal learning rate for training the model. Therefore, we report the results with a learning rate of 0.01, which demonstrates better performance across all activation functions. As shown in Table \ref{tab:semantic-seg-pre-training} and Figures \ref{fig:dlv3-rn50-ms-coco-0.02} and \ref{fig:dlv3-rn50-ms-coco-0.01}, GoLU outperforms the baseline activation ReLU as well as the popular self-gated activations. The inference loss and mIoU curves over epochs further highlight the strong performance of GoLU in Semantic Segmentation. This clearly demonstrates the versatility of GoLU across a wide range of tasks.
Table \ref{tab:semantic-seg-pre-training} presents the test loss and test mIoU using the original learning rate of 0.02. GoLU achieves the lowest test loss, whereas ReLU attains the highest mIoU, with GoLU ranking second. However, the difference in mIoU between ReLU and GoLU is statistically insignificant.
\begin{table}[H]
    \tiny
    \centering
    \renewcommand{\arraystretch}{1.2}
    \caption{Test loss and test mIoU of DeepLabV3 ResNet-50 trained on MS-COCO.}
    \vspace{-1mm}
    \begin{tabular}{|c|cc|cc|}
        \hline
         \multirow{2}{*}{\textbf{Activation}} & \multicolumn{2}{c|}{\textbf{LR=0.02}} & \multicolumn{2}{c|}{\textbf{LR=0.01}} \\
        \cline{2-5}
        & \textbf{Test Loss} & \textbf{Test mIoU} & \textbf{Test Loss} & \textbf{Test mIoU} \\
        \hline
        
        ReLU & 0.344±0.003 & \textbf{64.99±0.173} & 0.350±0.004 & 65.11±0.326\\
        
        LeakyReLU & 0.342±0.003 & 64.79±0.122 & 0.350±0.002 & 65.55±0.131\\

        ELU & 0.367±0.001 & 59.31±0.065 & 0.358±0.001 & 60.70±0.089\\
        
        GELU & 0.341±0.002 & 64.53±0.136 & \textbf{0.341±0.003} & 65.59±0.162\\
        
        Swish & 0.348±0.003 & 62.52±0.034 & 0.345±0.002 & 64.14±0.135\\

        Mish & 0.344±0.001 & 62.97±0.022 & 0.342±0.002 & 64.40±0.144\\

        \hline
        
        GoLU & \textbf{0.339±0.000} & \textbf{64.98±0.129} & \textbf{0.341±0.001} & \textbf{65.98±0.124}\\

        \hline
    \end{tabular}
    \label{tab:semantic-seg-pre-training}
\end{table}
%
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{images//3-experiments-and-results/dlv3_0.02_plots.png}
    \vspace{-6mm}
    \caption{DeepLabV3 ResNet-50 test loss (Left) and test mIoU (Right) on MS-COCO with lr=0.02.}
    \label{fig:dlv3-rn50-ms-coco-0.02}
\end{figure}
%
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{images//3-experiments-and-results/dlv3_0.01_plots.png}
    \vspace{-6mm}
    \caption{DeepLabV3 ResNet-50 test loss (Left) and test mIoU (Right) on MS-COCO with lr=0.01.}
    \label{fig:dlv3-rn50-ms-coco-0.01}
\end{figure}
We conduct a small ablation study on the learning rate and find that lr=0.02 is suboptimal for training the model. Instead, lr=0.01 yields the best performance across all activation functions (see heatmap \ref{fig:lr-ablations-deeplabv3-mscoco} in Appendix \ref{sec:learning-rate-ablation} for full results). Table \ref{tab:semantic-seg-pre-training} also reports the results with lr=0.01, where GoLU consistently outperforms other activation functions in terms of mIoU. Additionally, the inference loss and test mIoU curves over epochs, shown in Figures \ref{fig:dlv3-rn50-ms-coco-0.02} and \ref{fig:dlv3-rn50-ms-coco-0.01}, further emphasize GoLU's strong performance in semantic segmentation. %These findings demonstrate GoLU's versatility across a broad range of tasks.
%



\subsection{Object Detection}% - MSCOCO}
\label{subsec:object-detection}

% We train Faster R-CNN-FPN and RetinaNet-FPN on the MS-COCO dataset. Unlike Semantic Segmentation, the dataset contains 117,266 images in the training set and 5,000 images in the test set. Additionally, we do not apply any pre-processing that removes images from the training or test sets.

% Faster R-CNN-FPN ResNet-50 and RetinaNet-FPN ResNet-50 are trained for 26 epochs with a batch size of 16, an aspect ratio group factor of 3, no frozen batch normalization, and a MultiStep learning rate scheduler that reduces the initial learning rate by a factor of 0.1 at epochs 16 and 22. Specifically, Faster R-CNN-FPN ResNet-50 uses SGD with momentum \(0.9\), a learning rate of \(2 \times 10^{-2}\), and a weight decay of \(1 \times 10^{-4}\), while RetinaNet-FPN ResNet-50 uses the AdamW optimizer with a learning rate of \(1 \times 10^{-4}\) and a weight decay of \(5 \times 10^{-2}\).

% As shown in Table \ref{tab:object-detection-pre-training} and Figure \ref{fig:obj-dec-ms-coco}, GoLU outperforms all activation functions for Object Detection (Faster R-CNN-FPN and RetinaNet-FPN) as well, with higher Box mAP across both architectures. This can be attributed to GoLU's properties, particularly sparsity and regularization, which contribute to achieving state-of-the-art performance.
For Object Detection, we train Faster R-CNN-FPN and RetinaNet-FPN on the MS-COCO dataset. As shown in Table \ref{tab:object-detection-pre-training} and Figure \ref{fig:obj-dec-ms-coco}, GoLU outperforms all activation functions on object detection as well, with higher Box mAP (AP @ IoU=0.50:0.95, area=all, maxDets=100) across both Faster R-CNN-FPN and RetinaNet-FPN architectures, while GELU ranks second. Appendix~\ref{subapp:object-detection} outlines experimental details. %This can be attributed to GoLU's properties, particularly sparsity and regularization, which contribute to achieving state-of-the-art performance.
%
\begin{table}[H]
    \scriptsize
    \centering
    \renewcommand{\arraystretch}{1.2}
    \caption{Test Box mAP of Faster R-CNN-FPN ResNet-50 and RetinaNet-FPN ResNet-50 trained on MS-COCO.}
    % \vspace{1mm}
    \begin{tabular}{|c|c|c|}
        \hline
         \textbf{Activation} & \textbf{Faster R-CNN Box mAP} & \textbf{RetinaNet Box mAP} \\
        \hline
        
        ReLU & 37.44±0.146 & 39.90±0.063\\
        
        LeakyReLU & 37.41±0.140 & 39.87±0.041\\

        ELU & 35.36±0.041 & 37.43±0.041\\
        
        GELU & \underline{38.16±0.044} & \underline{40.68±0.090} \\
        
        Swish & 37.28±0.078 & 40.27±0.087\\

        Mish & 37.71±0.087 & 40.45±0.093\\

        \hline
        
        GoLU & \textbf{38.31±0.058} & \textbf{40.77±0.065}\\

        \hline
    \end{tabular}
    \label{tab:object-detection-pre-training}
\end{table}
%
% \vspace{-2mm}
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{images//3-experiments-and-results/obj_dec_plots.png}
    \vspace{-6mm}
    \caption{Faster R-CNN-FPN ResNet-50 (Left) and RetinaNet-FPN ResNet-50 (Right) test Box mAP on MS-COCO.}
    \label{fig:obj-dec-ms-coco}
\end{figure}

\subsection{Instance Segmentation}% - MSCOCO}
\label{subsec:instance-segmentation}

For Instance Segmentation, we train Mask R-CNN-FPN with a ResNet-50 backbone from the Torchvision benchmark on the MS-COCO dataset (see Appendix~\ref{app:instance-segmentation} for training settings). %It uses the same train and test sets as those used for Object Detection. Additionally, it trains with the exact same configurations used for Faster R-CNN-FPN in subsection \ref{subsec:object-detection}.
%
% As shown in Table \ref{tab:instance-segmentation-pre-training} and Figure \ref{fig:ins-seg-ms-coco}, we observe that GoLU generally performs better than GELU and ReLU over the epochs. However, while the Box mAP is almost equivalent to that of GELU, the Mask mAP is not. This could be attributed to GoLU's sparsity, which may cause it to lose certain important features, leading to a lower Mask mAP. Additionally, similar to Semantic Segmentation, the learning rate of \(2 \times 10^{-2}\) may not be optimal for this architecture-dataset combination.
As shown in Table \ref{tab:instance-segmentation-pre-training}, GELU achieves the best performance in this setting, with GoLU ranking second in Box mAP and third in Mask mAP (both implying AP @ IoU=0.50:0.95, area=all, maxDets=100). 
% This may result from GoLU’s tendency to reduce variance excessively in this case, which could filter out certain important features and lead to a lower Mask mAP. 
However, Figure \ref{fig:ins-seg-ms-coco}, which depicts test Box mAP and Mask mAP over epochs, reveals that GoLU generally outperforms GELU and ReLU throughout the training process. Based on these observations, we suggest that, similar to the Semantic Segmentation task, the learning rate of 0.02 may be suboptimal for this specific architecture-dataset combination. Adjusting the learning rate could potentially enhance GoLU's performance relative to baseline activations.
%
\begin{table}[H]
    \small
    \centering
    \renewcommand{\arraystretch}{1.2}
    \caption{Test Box mAP and Mask mAP of Mask R-CNN-FPN ResNet-50 trained on MS-COCO.}
    \vspace{2mm}
    \begin{tabular}{|c|cc|}
        \hline
         \textbf{Activation} & \textbf{Box mAP} & \textbf{Mask mAP} \\
        \hline
        
        ReLU & 38.33±0.001 & 34.19±0.001\\
        
        LeakyReLU & 38.31±0.002 & 34.19±0.001\\

        ELU & 36.41±0.001 & 32.81±0.001\\
        
        GELU & \textbf{39.00±0.001} & \textbf{34.73±0.000}\\
        
        Swish & 38.19±0.002 & 33.99±0.001\\

        Mish & 38.76±0.000 & \underline{34.70±0.000}\\

        \hline
        
        GoLU & \underline{38.96±0.001} & 34.54±0.001\\

        \hline
    \end{tabular}
    \label{tab:instance-segmentation-pre-training}
\end{table}
%
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{images//3-experiments-and-results/ins_seg_plots.png}
    \vspace{-5mm}
    \caption{Test Box mAP (Left) and test Mask mAP (Right) for Mask R-CNN-FPN ResNet-50 trained on MS-COCO.}
    \label{fig:ins-seg-ms-coco}
\end{figure}


\subsection{Denoising Diffusion Probabilistic Models}% - CelebA}
\label{subsec:ddpm}

% The CelebA dataset, comprising 162,770 training images and 19,867 test images of human faces, is used to train a Denoising Diffusion Probabilistic Model for 50 epochs with a batch size of 32. The AdamW optimizer with a learning rate of 0.0003, Cosine learning rate scheduler, and linear learning rate warmup for the first 1,000 iterations are applied.

% Results are shown in Table \ref{tab:ddpm-pre-training} and Figure \ref{fig:ddpm-celeba}. Both GoLU and GELU perform similarly to the baseline activation Swish. However, GELU underperforms for this task. GoLU achieves the best test loss of 0.0187746, while Swish and GELU achieve losses of 0.0188134 and 0.0188434, respectively. The loss plots do not indicate a clear superior activation function.
We train a Denoising Diffusion Probabilistic Model on the CelebA dataset (see Appendix~\ref{subapp:ddpm}). As shown in Table \ref{tab:ddpm-pre-training}, for the default lr=0.0003, gated activations perform comparably to the baseline activation, Swish, which achieves the best performance, with GoLU ranking a close second. Figure \ref{fig:ddpm-celeba} (Left) further illustrates the test loss over epochs%, where GoLU achieves the lowest ``best test loss"
. Similar to our findings in semantic segmentation, we conduct a learning rate ablation study. Results, summarized in heatmap \ref{fig:lr-ablations-ddpm-celeba} in Appendix \ref{sec:learning-rate-ablation}, indicate that increasing the lr from the default value of 0.0003 to 0.0004, 0.0005 and 0.001 progressively improves performance across all activations. Notably, for lr values of 0.0004, 0.0005 and 0.001, GoLU achieves the lowest final test loss. Results for the optimum lr=0.001 are highlighted in the right column of Table \ref{tab:ddpm-pre-training} and Figure \ref{fig:ddpm-celeba} (Right). These findings are in line with the trend observed in semantic segmentation, where GoLU outperforms baseline activations under optimal lr configurations. 
%
\begin{table}[t]
    \small
    \centering
    \renewcommand{\arraystretch}{1.2}
    \caption{Test Loss at LR=0.0003 and LR=0.001 of Denoising Diffusion Probabilistic Model trained on CelebA.}
    \vspace{2mm}
    \begin{tabular}{|c|cc|}
        \hline
         \multirow{2}{*}{\textbf{Activation}} & \textbf{Test Loss} & \textbf{Test Loss} \\
          & \textbf{LR=0.0003} & \textbf{LR=0.001} \\
        \hline
        
        ReLU & 0.0200255±0.0 & 0.0192820±0\\
        
        LeakyReLU & 0.0200307±0.0 & 0.0192812±0\\

        ELU & 0.0200398±0.0 & 0.0193941±0\\
        
        GELU & 0.0196956±0.0 & 0.0190221±0\\
        
        Swish & \textbf{0.0196364±0.0} & \underline{0.0190055±0}\\

        Mish & 0.0196865±0.0 & 0.0190657±0\\

        \hline
        
        GoLU & \underline{0.0196419±0.0} & \bf{0.0189506±0}\\

        \hline
    \end{tabular}
    \label{tab:ddpm-pre-training}
\end{table}
%
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images//3-experiments-and-results/ddpm_plots.png}
    \vspace{-5mm}
    \caption{Test loss for Denoising Diffusion Probabilistic Model trained on CelebA at LR=0.0003 (Left) and LR=0.001 (Right).}
    \label{fig:ddpm-celeba}
\end{figure}

\section{Training and Inference Speed}
\label{subsec:golu-speed}

Existing activation functions in PyTorch leverage CUDA kernels in Eager mode to achieve optimal speedup. To ensure a fair comparison of training and inference speeds, we developed a CUDA-optimized kernel for GoLU, which was used for all training experiments described in the previous sections. Table \ref{tab:training-inference-speeds} in Appendix~\ref{speeds} presents the relative training and inference speeds of GoLU compared to the default activation function across various tasks.

Our results show that GoLU achieves a speed comparable to that of the default activation function across all architectures. The only exception is DeepLabV3-ResNet-50 trained on MS-COCO, where GoLU incurs slightly higher training time. However, this is consistent with other activation functions, all of which exhibit increased training times relative to ReLU in this specific architecture.
% For training models with GoLU, we build our CUDA Kernel. This is to make sure there exists an apples-to-apples comparison because all other activation functions in PyTorch use CUDA Kernel in the Eager mode. Further, we report the relative training and inference speed in Table \ref{tab:training-inference-speeds}. We observe that GoLU is equivalent in speed across all the architectures. The only architecture where GoLU takes more training time is DeepLabV3-ResNet-50 trained on MS-COCO. However, GoLU is not an exception in this case because all other activations show a higher training time in comparison to ReLU.
% \begin{table*}[t]
%     \centering
%     \small
%     \renewcommand{\arraystretch}{1.2}
%     \caption{Relative training and inference time with respect to baseline activations for all our trained architectures.}
%     \begin{tabular}{|c|c|c|c|c|}
%         % \hline
%         % \multirow{2}{*}{\textbf{Architecture}} & \multirow{2}{*}{\textbf{Dataset}} & \textbf{Baseline} & \textbf{Relative} & \textbf{Relative}\\[-1mm]
%         % & & \textbf{Activation} & \textbf{Training Speed} & \textbf{Inference Speed}\\
%         \hline
%         \textbf{Architecture} & \textbf{Dataset} & \textbf{Baseline Activation} & \textbf{Relative Training Time} & \textbf{Relative Inference Time}\\
%         \hline
%         \textbf{ResNet18} & ImageNet-1k & ReLU & 1.00 & 1.00 \\
        
%         \textbf{ResNet34} & ImageNet-1k & ReLU & 1.01 & 1.00 \\
        
%         \textbf{ResNet-50} & ImageNet-1k & ReLU & 1.01 & 1.01 \\
        
%         \textbf{WideResNet-50-2} & ImageNet-1k & ReLU & 1.03 & 1.02 \\
        
%         \textbf{DenseNet121} & ImageNet-1k & ReLU & 1.02 & 1.02 \\
        
%         \textbf{EfficientNet-B0} & ImageNet-1k & Swish & 1.00 & 1.00 \\

%         \textbf{TinyViT} & ImageNet-1k & GELU & 0.99 & 0.98 \\
        
%         \textbf{ViT-B/32} & ImageNet-1k & GELU & 0.99 & 0.99 \\
        
%         \textbf{ViT-B/16} & ImageNet-1k & GELU & 0.98 & 0.98 \\
        
%         \textbf{babyGPT} & TinyStories & GELU & 1.00 & 1.00 \\
        
%         \textbf{GPT2-S} & OpenWebText & GELU & 1.01 & 1.01 \\
        
%         \textbf{DeepLabV3} & MS-COCO & ReLU & 1.14 & 1.04 \\
        
%         \textbf{RetinaNet} & MS-COCO & ReLU & 1.00 & 1.00 \\
        
%         \textbf{FasterRCNN} & MS-COCO & ReLU & 1.03 & 1.00 \\
        
%         \textbf{MaskRCNN} & MS-COCO & ReLU & 1.05 & 1.02 \\
        
%         \textbf{DDPM} & CelebA & Swish & 0.97 & 0.97 \\ \hline
        
%         \textbf{Average} & - & - & 1.01 & 1.00 \\
%         \hline
%     \end{tabular}
%     \label{tab:training-inference-speeds}
% \end{table*}




\section{Conclusions}

We have introduced GoLU, a new self-gated activation function based on the CDF of the Gumbel distribution as its gate function. Through extensive analysis and experiments, we have demonstrated that GoLU provides a regularising effect by reducing variance in the activation output, it enables the representation of diverse features through a more distributed weight pattern, and encourages a smoother and more robust loss landscape. Notably, our results show that GoLU generally outperforms state-of-the-art baseline activation functions across a wide range of tasks and domains, from computer vision to language modeling. Additionally, we implemented a custom CUDA kernel to optimize training and inference efficiency, minimizing latency and enhancing scalability. GoLU offers a robust, efficient, and scalable alternative to existing activation functions. Its integration into state-of-the-art neural networks has the potential to improve performance across various applications, positioning GoLU as a promising standard in modern deep learning.

% \section{Format of the Paper}

% All submissions must follow the specified format.

% \subsection{Dimensions}




% The text of the paper should be formatted in two columns, with an
% overall width of 6.75~inches, height of 9.0~inches, and 0.25~inches
% between the columns. The left margin should be 0.75~inches and the top
% margin 1.0~inch (2.54~cm). The right and bottom margins will depend on
% whether you print on US letter or A4 paper, but all final versions
% must be produced for US letter size.
% Do not write anything on the margins.

% The paper body should be set in 10~point type with a vertical spacing
% of 11~points. Please use Times typeface throughout the text.

% \subsection{Title}

% The paper title should be set in 14~point bold type and centered
% between two horizontal rules that are 1~point thick, with 1.0~inch
% between the top rule and the top edge of the page. Capitalize the
% first letter of content words and put the rest of the title in lower
% case.

% \subsection{Author Information for Submission}
% \label{author info}

% ICML uses double-blind review, so author information must not appear. If
% you are using \LaTeX\/ and the \texttt{icml2025.sty} file, use
% \verb+\icmlauthor{...}+ to specify authors and \verb+\icmlaffiliation{...}+ to specify affiliations. (Read the TeX code used to produce this document for an example usage.) The author information
% will not be printed unless \texttt{accepted} is passed as an argument to the
% style file.
% Submissions that include the author information will not
% be reviewed.

% \subsubsection{Self-Citations}

% If you are citing published papers for which you are an author, refer
% to yourself in the third person. In particular, do not use phrases
% that reveal your identity (e.g., ``in previous work \cite{langley00}, we
% have shown \ldots'').

% Do not anonymize citations in the reference section. The only exception are manuscripts that are
% not yet published (e.g., under submission). If you choose to refer to
% such unpublished manuscripts \cite{anonymous}, anonymized copies have
% to be submitted
% as Supplementary Material via OpenReview\@. However, keep in mind that an ICML
% paper should be self contained and should contain sufficient detail
% for the reviewers to evaluate the work. In particular, reviewers are
% not required to look at the Supplementary Material when writing their
% review (they are not required to look at more than the first $8$ pages of the submitted document).

% \subsubsection{Camera-Ready Author Information}
% \label{final author}

% If a paper is accepted, a final camera-ready copy must be prepared.
% %
% For camera-ready papers, author information should start 0.3~inches below the
% bottom rule surrounding the title. The authors' names should appear in 10~point
% bold type, in a row, separated by white space, and centered. Author names should
% not be broken across lines. Unbolded superscripted numbers, starting 1, should
% be used to refer to affiliations.

% Affiliations should be numbered in the order of appearance. A single footnote
% block of text should be used to list all the affiliations. (Academic
% affiliations should list Department, University, City, State/Region, Country.
% Similarly for industrial affiliations.)

% Each distinct affiliations should be listed once. If an author has multiple
% affiliations, multiple superscripts should be placed after the name, separated
% by thin spaces. If the authors would like to highlight equal contribution by
% multiple first authors, those authors should have an asterisk placed after their
% name in superscript, and the term ``\textsuperscript{*}Equal contribution"
% should be placed in the footnote block ahead of the list of affiliations. A
% list of corresponding authors and their emails (in the format Full Name
% \textless{}email@domain.com\textgreater{}) can follow the list of affiliations.
% Ideally only one or two names should be listed.

% A sample file with author names is included in the ICML2025 style file
% package. Turn on the \texttt{[accepted]} option to the stylefile to
% see the names rendered. All of the guidelines above are implemented
% by the \LaTeX\ style file.

% \subsection{Abstract}

% The paper abstract should begin in the left column, 0.4~inches below the final
% address. The heading `Abstract' should be centered, bold, and in 11~point type.
% The abstract body should use 10~point type, with a vertical spacing of
% 11~points, and should be indented 0.25~inches more than normal on left-hand and
% right-hand margins. Insert 0.4~inches of blank space after the body. Keep your
% abstract brief and self-contained, limiting it to one paragraph and roughly 4--6
% sentences. Gross violations will require correction at the camera-ready phase.

% \subsection{Partitioning the Text}

% You should organize your paper into sections and paragraphs to help
% readers place a structure on the material and understand its
% contributions.

% \subsubsection{Sections and Subsections}

% Section headings should be numbered, flush left, and set in 11~pt bold
% type with the content words capitalized. Leave 0.25~inches of space
% before the heading and 0.15~inches after the heading.

% Similarly, subsection headings should be numbered, flush left, and set
% in 10~pt bold type with the content words capitalized. Leave
% 0.2~inches of space before the heading and 0.13~inches afterward.

% Finally, subsubsection headings should be numbered, flush left, and
% set in 10~pt small caps with the content words capitalized. Leave
% 0.18~inches of space before the heading and 0.1~inches after the
% heading.

% Please use no more than three levels of headings.

% \subsubsection{Paragraphs and Footnotes}

% Within each section or subsection, you should further partition the
% paper into paragraphs. Do not indent the first line of a given
% paragraph, but insert a blank line between succeeding ones.

% You can use footnotes\footnote{Footnotes
% should be complete sentences.} to provide readers with additional
% information about a topic without interrupting the flow of the paper.
% Indicate footnotes with a number in the text where the point is most
% relevant. Place the footnote in 9~point type at the bottom of the
% column in which it appears. Precede the first footnote in a column
% with a horizontal rule of 0.8~inches.\footnote{Multiple footnotes can
% appear in each column, in the same order as they appear in the text,
% but spread them across columns and pages if possible.}

% \begin{figure}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
% \caption{Historical locations and number of accepted papers for International
% Machine Learning Conferences (ICML 1993 -- ICML 2008) and International
% Workshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was
% produced, the number of accepted papers for ICML 2008 was unknown and instead
% estimated.}
% \label{icml-historical}
% \end{center}
% \vskip -0.2in
% \end{figure}

% \subsection{Figures}

% You may want to include figures in the paper to illustrate
% your approach and results. Such artwork should be centered,
% legible, and separated from the text. Lines should be dark and at
% least 0.5~points thick for purposes of reproduction, and text should
% not appear on a gray background.

% Label all distinct components of each figure. If the figure takes the
% form of a graph, then give a name for each axis and include a legend
% that briefly describes each curve. Do not include a title inside the
% figure; instead, the caption should serve this function.

% Number figures sequentially, placing the figure number and caption
% \emph{after} the graphics, with at least 0.1~inches of space before
% the caption and 0.1~inches after it, as in
% \cref{icml-historical}. The figure caption should be set in
% 9~point type and centered unless it runs two or more lines, in which
% case it should be flush left. You may float figures to the top or
% bottom of a column, and you may set wide figures across both columns
% (use the environment \texttt{figure*} in \LaTeX). Always place
% two-column figures at the top or bottom of the page.

% \subsection{Algorithms}

% If you are using \LaTeX, please use the ``algorithm'' and ``algorithmic''
% environments to format pseudocode. These require
% the corresponding stylefiles, algorithm.sty and
% algorithmic.sty, which are supplied with this package.
% \cref{alg:example} shows an example.

% \begin{algorithm}[tb]
%    \caption{Bubble Sort}
%    \label{alg:example}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} data $x_i$, size $m$
%    \REPEAT
%    \STATE Initialize $noChange = true$.
%    \FOR{$i=1$ {\bfseries to} $m-1$}
%    \IF{$x_i > x_{i+1}$}
%    \STATE Swap $x_i$ and $x_{i+1}$
%    \STATE $noChange = false$
%    \ENDIF
%    \ENDFOR
%    \UNTIL{$noChange$ is $true$}
% \end{algorithmic}
% \end{algorithm}

% \subsection{Tables}

% You may also want to include tables that summarize material. Like
% figures, these should be centered, legible, and numbered consecutively.
% However, place the title \emph{above} the table with at least
% 0.1~inches of space before the title and the same after it, as in
% \cref{sample-table}. The table title should be set in 9~point
% type and centered unless it runs two or more lines, in which case it
% should be flush left.

% % Note use of \abovespace and \belowspace to get reasonable spacing
% % above and below tabular lines.

% \begin{table}[t]
% \caption{Classification accuracies for naive Bayes and flexible
% Bayes on various data sets.}
% \label{sample-table}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccr}
% \toprule
% Data set & Naive & Flexible & Better? \\
% \midrule
% Breast    & 95.9$\pm$ 0.2& 96.7$\pm$ 0.2& $\surd$ \\
% Cleveland & 83.3$\pm$ 0.6& 80.0$\pm$ 0.6& $\times$\\
% Glass2    & 61.9$\pm$ 1.4& 83.8$\pm$ 0.7& $\surd$ \\
% Credit    & 74.8$\pm$ 0.5& 78.3$\pm$ 0.6&         \\
% Horse     & 73.3$\pm$ 0.9& 69.7$\pm$ 1.0& $\times$\\
% Meta      & 67.1$\pm$ 0.6& 76.5$\pm$ 0.5& $\surd$ \\
% Pima      & 75.1$\pm$ 0.6& 73.9$\pm$ 0.5&         \\
% Vehicle   & 44.9$\pm$ 0.6& 61.5$\pm$ 0.4& $\surd$ \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

% Tables contain textual material, whereas figures contain graphical material.
% Specify the contents of each row and column in the table's topmost
% row. Again, you may float tables to a column's top or bottom, and set
% wide tables across both columns. Place two-column tables at the
% top or bottom of the page.

% \subsection{Theorems and such}
% The preferred way is to number definitions, propositions, lemmas, etc. consecutively, within sections, as shown below.
% \begin{definition}
% \label{def:inj}
% A function $f:X \to Y$ is injective if for any $x,y\in X$ different, $f(x)\ne f(y)$.
% \end{definition}
% Using \cref{def:inj} we immediate get the following result:
% \begin{proposition}
% If $f$ is injective mapping a set $X$ to another set $Y$, 
% the cardinality of $Y$ is at least as large as that of $X$
% \end{proposition}
% \begin{proof} 
% Left as an exercise to the reader. 
% \end{proof}
% \cref{lem:usefullemma} stated next will prove to be useful.
% \begin{lemma}
% \label{lem:usefullemma}
% For any $f:X \to Y$ and $g:Y\to Z$ injective functions, $f \circ g$ is injective.
% \end{lemma}
% \begin{theorem}
% \label{thm:bigtheorem}
% If $f:X\to Y$ is bijective, the cardinality of $X$ and $Y$ are the same.
% \end{theorem}
% An easy corollary of \cref{thm:bigtheorem} is the following:
% \begin{corollary}
% If $f:X\to Y$ is bijective, 
% the cardinality of $X$ is at least as large as that of $Y$.
% \end{corollary}
% \begin{assumption}
% The set $X$ is finite.
% \label{ass:xfinite}
% \end{assumption}
% \begin{remark}
% According to some, it is only the finite case (cf. \cref{ass:xfinite}) that is interesting.
% \end{remark}
% %restatable

% \subsection{Citations and References}

% Please use APA reference format regardless of your formatter
% or word processor. If you rely on the \LaTeX\/ bibliographic
% facility, use \texttt{natbib.sty} and \texttt{icml2025.bst}
% included in the style-file package to obtain this format.

% Citations within the text should include the authors' last names and
% year. If the authors' names are included in the sentence, place only
% the year in parentheses, for example when referencing Arthur Samuel's
% pioneering work \yrcite{Samuel59}. Otherwise place the entire
% reference in parentheses with the authors and year separated by a
% comma \cite{Samuel59}. List multiple references separated by
% semicolons \cite{kearns89,Samuel59,mitchell80}. Use the `et~al.'
% construct only for citations with three or more authors or after
% listing all authors to a publication in an earlier reference \cite{MachineLearningI}.

% Authors should cite their own work in the third person
% in the initial version of their paper submitted for blind review.
% Please refer to \cref{author info} for detailed instructions on how to
% cite your own papers.

% Use an unnumbered first-level section heading for the references, and use a
% hanging indent style, with the first line of the reference flush against the
% left margin and subsequent lines indented by 10 points. The references at the
% end of this document give examples for journal articles \cite{Samuel59},
% conference publications \cite{langley00}, book chapters \cite{Newell81}, books
% \cite{DudaHart2nd}, edited volumes \cite{MachineLearningI}, technical reports
% \cite{mitchell80}, and dissertations \cite{kearns89}.

% Alphabetize references by the surnames of the first authors, with
% single author entries preceding multiple author entries. Order
% references for the same authors by year of publication, with the
% earliest first. Make sure that each reference includes all relevant
% information (e.g., page numbers).

% Please put some effort into making references complete, presentable, and
% consistent, e.g. use the actual current name of authors.
% If using bibtex, please protect capital letters of names and
% abbreviations in titles, for example, use \{B\}ayesian or \{L\}ipschitz
% in your .bib file.

% \section*{Accessibility}
% Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
% Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

% \section*{Software and Data}

% If a paper is accepted, we strongly encourage the publication of software and data with the
% camera-ready version of the paper whenever appropriate. This can be
% done by including a URL in the camera-ready copy. However, \textbf{do not}
% include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload
% the material as ``Supplementary Material'' into the OpenReview reviewing
% system. Note that reviewers are not required to look at this material
% when writing their review.

% Acknowledgements should only appear in the accepted version.
\section*{Acknowledgements}

This research was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under grant number 417962828. The authors gratefully acknowledge the computing time made available to them on the high-performance computer NHR@KIT Compute Cluster at the NHR Center NHR@KIT. These Centers are jointly supported by the Federal Ministry of Education and Research and the state governments participating in the NHR
(www.nhr-verein.de/unsere-partner). We acknowledge funding by the European Union (via ERC Consolidator Grant DeepLearning 2.0, grant no.~101045765). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them. \begin{center}\includegraphics[width=0.3\textwidth]{images/5-appendix/ERC_grant.jpg}
\end{center}The authors gratefully acknowledge the Gauss Centre for Supercomputing e.V. (www.gauss-centre.eu) for funding this project by providing computing time on the GCS Supercomputer JUWELS at Jülich Supercomputing Center (JSC) and SuperMUC-NG at Leibniz Supercomputing Centre (www.lrz.de). The authors acknowledge support by the state of Baden-Württemberg through bwHPC.
Frank Hutter acknowledges the financial support of the Hector Foundation. We also thank Jörg Franke, Edward Bergman, Arbër Zela, André Biedenkapp and Lennart Purucker for their constructive feedback throughout the development of this work.

\section*{Impact Statement}

This paper introduces GoLU, a novel activation function designed to advance the field of Machine Learning. The primary objective of this work is to improve the performance and robustness of state-of-the-art neural networks across diverse domains, including computer vision, natural language processing, and generative modeling.
The societal impact of this work is primarily tied to the downstream applications of machine learning models that may incorporate GoLU. By enhancing the robustness and performance of models, our activation function has the potential to positively influence critical areas such as medical imaging, autonomous systems, and other technologies that drive societal progress.
While there are no immediate or direct societal concerns specific to GoLU itself, as with any development in machine learning, there is a possibility of misuse. We therefore emphasize the importance of ethical and responsible deployment of machine learning technologies enhanced by our contributions.


% ``This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.''


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{main}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Properties of GoLU: Further Details}
\label{app:golu-further-details}
% To facilitate a more in-depth understanding of GoLU, we ...
\setlength{\textfloatsep}{5pt}
\setlength{\floatsep}{5pt}
\setlength{\intextsep}{5pt} 
To further elucidate the concepts presented in Section~\ref{sec:golu-properties} and gain deeper insights into the properties of GoLU, we present additional details and visualizations in this section.

Figure~\ref{fig:slope-comparison} compares the GoLU activation with GELU, highlighting how the right-leaning inclination of the Gumbel distribution, in contrast to the symmetric Gaussian distribution (Left column), results in a smaller value of the Gompertz gate at the origin compared to the Gaussian CDF (Middle column). In fact, this behavior is not confined to the origin, and the Gompertz gate remains smaller than the Gaussian CDF across the entire input range.
\setlength{\textfloatsep}{5pt}
\setlength{\floatsep}{5pt}
\setlength{\intextsep}{5pt}
\begin{figure}[H]
    \centering
    % Row 1
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/5-appendix/Gaussian_Distribution.png}
        % \caption{Image 1}
    \end{subfigure}
    % \hfill
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/5-appendix/Gaussian_CDF.png}
        % \caption{Image 2}
    \end{subfigure}
    % \hfill
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/5-appendix/GELU.png}
        % \caption{Image 3}
    \end{subfigure}
    %
    % Row 2
    \vspace{0.5cm} % Add spacing between rows
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/5-appendix/Gumbel_Distribution.png}
        % \caption{Image 4}
    \end{subfigure}
    % \hfill
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/5-appendix/Gompertz_Function.png}
        % \caption{Image 5}
    \end{subfigure}
    % \hfill
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/5-appendix/GoLU.png}
        % \caption{Image 6}
    \end{subfigure}
    \caption{Top row, from left to right: Gaussian distribution, Gaussian CDF, GELU. Bottom row, from left to right: Gumbel distribution, Gompertz function, GoLU.}
    \label{fig:slope-comparison}
\end{figure}
This reduced value of the Gompertz gate at the origin directly translates into a lower slope for GoLU compared to GELU, as illustrated in Figure~\ref{fig:slope-comparison} (Right column). This can be readily seen by taking the derivative of the GoLU activation and evaluating it at zero
\setlength{\abovedisplayskip}{8pt}
\setlength{\belowdisplayskip}{8pt}
{\setlength\arraycolsep{2pt}
\begin{eqnarray}
    \mathrm{GoLU}'(x) &=& x\,\mathrm{Gompertz}'(x) + \mathrm{Gompertz}(x) \\
    \mathrm{GoLU}'(0) &=& \mathrm{Gompertz}(0)
\end{eqnarray}}%
which shows that the slope of GoLU at the origin corresponds to the value of the Gompertz gate at the origin. Similarly, the slope of GELU at the origin is determined by the Gaussian CDF at the origin.

Assuming the input distribution resembles a zero-centered, nearly-Gaussian form, which is likely particularly when employing batch normalization and appropriate weight initialization, the activations can be approximated by their tangents at the origin. Therefore a reduced slope at the origin translates into decreased sensitivity to input variations and lower output variance. We note that GoLU exhibits a lower slope magnitude not only in a neighborhood around the origin but across a significant portion of the negative input domain as illustrated in Figure~\ref{gate_derivatives}.
\begin{figure}[H]
    \centering
        \centering
        \includegraphics[width=0.32\linewidth]{images/5-appendix/gate_derivatives}
        \caption{Derivatives of the gate functions.}
    \label{gate_derivatives}
\end{figure}
Finally, a Taylor expansion of the Sigmoid and Gompertz gate functions for large positive input values demonstrates that these two functions converge to each other exponentially fast in this regime, as pointed out in Section~\ref{sec:golu-properties}.
\begin{equation}
        \mathrm{Sigmoid}(x) - \mathrm{Gompertz}(x)= \frac{1}{1+e^{-x}} - e^{-e^{-x}}  =  \Big(1 - e^{-x} + \mathcal{O}(e^{-2x})\Big) - \Big(1 - e^{-x} + \mathcal{O}(e^{-2x})\Big) = \mathcal{O}(e^{-2x})
\end{equation}



\section{Details of the loss landscape experiment}
\label{app:loss-landscape}

We analyze the loss landscape of a neural network by quantitatively measuring and visualizing how the loss changes as the network's parameters are perturbed. Smoothness in the loss landscape often indicates that small perturbations in the parameters do not cause large changes in the loss, which can make optimization more stable.

Specifically, we generate two random perturbation directions $d_1$ and $d_2$, each matching the shape of the model parameters. The elements of these directions are independently sampled from a Standard Normal distribution. To ensure controlled magnitudes, each perturbation direction is subsequently normalized.

We perturb the weights of the model along these directions in a linear combination:
\begin{equation}
W_{\mathrm{perturbed}} = W_{\mathrm{trained}} + \alpha d_1 + \beta d_2
\end{equation}
where $W_{\mathrm{trained}}$ are the trained weights of the model and $\alpha$ and $\beta$ are scalar values that determine the perturbation magnitude and are chosen as $\alpha, \beta \in [-1,1]$. For each pair of values $(\alpha, \beta)$, we compute the loss using the perturbed weights $W_{\mathrm{perturbed}}$ on a batch of test data. We then repeat this for a grid of $(\alpha, \beta)$ values to create a 3D surface plot as shown in Figure \ref{fig:resnet20-cifar10-loss-landscapes-with-golu}. % 2D loss surface.




%
% {\setlength\arraycolsep{2pt}
% \begin{eqnarray}
%     \mathrm{Sigmoid}(x) - \mathrm{Gompertz}(x) &=& \frac{1}{1+e^{-x}} - e^{-e^{-x}}  \\
%     &=&  \Big(1 - e^{-x} + e^{-2x} + \mathcal{O}(e^{-3x})\Big) - \Big(1 - e^{-x} + \frac{1}{2}e^{-2x} + \mathcal{O}(e^{-3x})\Big) \\
%     &=& \frac{1}{2}e^{-2x} + \mathcal{O}(e^{-3x}) = \mathcal{O}(e^{-2x})
% \end{eqnarray}}%



% \section{MLP ablations on CIFAR-10}\label{subsec:cifar-10}

% CIFAR-10 comprises of 50K training images and 10K test images across 10 classes. In our initial experiments, we train MLPs on CIFAR-10, conducting ablation studies to examine the effects of block size on batch size, epochs and learning rate. We evaluate and compare five activation functions: ReLU, GELU, Swish, Mish, and GoLU, based on test loss and accuracy. Models are evaluated with block sizes of 1, 2, 3, and 4, where each block processes input through a Linear layer (512 dimensions), followed by Layer Normalization and an activation function. Further, we vary batch sizes [32, 64, 128, 256], epochs [5, 6, 7, 8], and learning rates [0.1, 0.01, 0.001, 0.0001].

% For batch size ablations (see Figure \ref{fig:cifar10_blocks_vs_batch_size}), GoLU shows superior performance, achieving better test loss and accuracy with smaller models (num\_blocks = 2) and batch sizes (batch\_size = 64). Despite the noise introduced by smaller batch sizes, GoLU's robustness allows it to outperform other activations.

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.9\linewidth]{images//3-experiments-and-results/cifar10_blocks_vs_batch_size_plots.png}
%     \caption{CIFAR-10 number of blocks vs batch size ablations.}
%     \label{fig:cifar10_blocks_vs_batch_size}
% \end{figure}
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.9\linewidth]{images//3-experiments-and-results/cifar10_blocks_vs_epochs_plots.png}
%     \caption{CIFAR-10 number of blocks vs epochs ablations.}
%     \label{fig:cifar10_blocks_vs_epochs}
% \end{figure}
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.9\linewidth]{images//3-experiments-and-results/cifar10_lr_heatmaps.png}
%     \caption{CIFAR-10 number of blocks vs learning rate.}
%     \label{fig:cifar10_blocks_vs_lr}
% \end{figure}

% In epoch ablations (see Figure \ref{fig:cifar10_blocks_vs_epochs}), GoLU consistently delivers better test loss and accuracy, especially in the space of overfitting i.e. more blocks and epochs. Notably, GoLU outperforms all other activations even in the challenging region of 3 Blocks and 6 epochs. While in learning rate ablations (see Figure \ref{fig:cifar10_blocks_vs_lr}), GoLU achieves the best results with learning rates of 0.01 and 0.1, particularly with smaller models (num\_blocks in [2, 3]), showing its ability to handle larger learning rates.

% \section{Pre-Conditioning Benefits}\label{subsec:pre-conditioning-benefits}

% The gradient of an activation function acts as a Jacobian during backpropagation. GoLU’s relative sparsity in the latent space induces sparsity in its Jacobian, meaning small input changes yield small output changes, pre-conditioning the incoming gradients, stabilizing training and improving optimization. Sparser gradients prevent overfitting by reducing sensitivity to noise, enhancing generalization while slightly limiting representational capacity. Consequently, models trained with GoLU converge faster and more stably. This behavior is evident in the loss curves of ResNet-50 and ViT-B/32 (see Figures \ref{fig:resnet50-test-loss-accuracy} and \ref{fig:vitb32-test-loss-accuracy}).

% To demonstrate better conditioning with GoLU, we sample our input \( X \sim \mathcal{N}(\mu, \sigma), \, X \in \mathbb{R}^{1000 \times 1000} \) with varying means and standard deviations. We compute the first derivative of GELU, Swish, Mish, and GoLU, plotting the maximum eigenvalue across combinations of \(\mu\) and \(\sigma\) (Figure \ref{fig:gradient-max-eigenvalue}). GoLU consistently yields smaller maximum eigenvalues compared to GELU, Swish, and Mish, proving our theories and empirical results across diverse tasks.

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=1.0\linewidth]{images/2-golu/gradient_max_eigenvalue.png}
%     \caption{The maximum eigenvalue of the gradients of the activated tensor for input tensors sampled from varying means and standard deviations of a Gaussian Distribution.}
%     \label{fig:gradient-max-eigenvalue}
% \end{figure}


% \section{Properties of the Parameters}\label{subsec:properties-of-parameters}

% The GoLU activation contains three parameters from the Gompertz function: $\alpha$, $\beta$, and $\gamma$. The parameter $\alpha$ controls the upper asymptote, $\beta$ determines the horizontal shift, and $\gamma$ controls the slope of the Gompertz gate. These effects are demonstrated in Figures \ref{fig:golu-changing-alpha}, \ref{fig:golu-changing-beta}, and \ref{fig:golu-changing-gamma}, respectively. We also, ablate these parameters to seek gains in performance. These ablations can be visualized in Appendix \ref{subsec:golu-alpha-ablation}, \ref{subsec:golu-beta-ablation} and \ref{subsec:golu-gamma-ablation}.

% Empirical evidence suggests that setting $\alpha < 1.0$ enhances performance by inducing better sparsity, particularly in the positive space of features. However, extremely low $\alpha$ can hinder learning by reducing variance excessively, further motivating controlled variance reduction.

% The parameter $\beta$ acts similarly to the mean $\mu$ in a Gaussian CDF, shifting the Gompertz function horizontally. When $\beta > 1.0$, the function shifts rightward, resembling a shifted Softplus function. For $\beta < 1.0$, the shift leftward reduces sparsity.

% The slope of the Gompertz function is controlled by $\gamma$. Lower values of $\gamma$ ($\gamma < 1.0$) flatten the curve, increasing sparsity in the positive space while reducing the same in the negative space of features. Conversely, higher values ($\gamma > 1.0$) steepen the curve, decreasing sparsity in the positive space and increasing it in the negative space. As $\gamma \to \infty$, the Gompertz function approximates the Heaviside step function, generalizing GoLU to ReLU.

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.85\linewidth]{images/2-golu/changing_alpha.png}
%     \caption{The effect of changing $\alpha$ on GoLU and the Gompertz function. Here $\beta$ and $\gamma$ are set to 1.0 by default.}
%     \label{fig:golu-changing-alpha}
% \end{figure}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.85\linewidth]{images/2-golu/changing_beta.png}
%     \caption{The effect of changing $\beta$ on GoLU and the Gompertz function. Here $\alpha$ and $\gamma$ are set to 1.0 by default.}
%     \label{fig:golu-changing-beta}
% \end{figure}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.85\linewidth]{images/2-golu/changing_gamma.png}
%     \caption{The effect of changing $\gamma$ on GoLU and the Gompertz function. Here $\alpha$ and $\beta$ are set to 1.0 by default.}
%     \label{fig:golu-changing-gamma}
% \end{figure}


% \section{Stability, Speed and Hardware}\label{sec:stability-speed}

% \subsection{GoLU Stability}\label{subsec:golu-stability}

% Unlike mathematics, where large numbers can be represented theoretically, computing frameworks like PyTorch adhere to IEEE Standards for numerical representation. These standards define data types with specific value ranges, summarized in Table \ref{tab:ieee-summary}.
% %
% \begin{table}[H]
%     \centering
%     \small
%     \renewcommand{\arraystretch}{1.2}
%     \caption{Summary of IEEE floating-point formats, their bit widths and instability value for $e^{-x}$.}
%     \begin{tabular}{|c|c|c|}
%         \hline
%         \textbf{Precision (Bits)} & \textbf{$e^{-x}$ Instability}\\
%         \hline
%         Half (16) & -11.7\\
%         Brain-Float16 (16) & -88.7\\
%         Single (32) & -88.7\\
%         Double (64) & -709.78\\
%         Quadruple (128) & -11356.52\\
%         \hline
%     \end{tabular}
%     \label{tab:ieee-summary}
% \end{table}
% %
% While training ResNet-50 on ImageNet-1k and GPT2-S on OpenWebText, we observed exponential function overflows, causing NaNs during the backward pass. To analyze these instabilities, we perform extreme value analysis for the GoLU activation function. We replace $e^{-x}$ with $+\infty$ and 0 for $x < -88.7$ or $x > 88.7$ to avoid overflow.
% %
% For the forward pass, we analyze two cases: 
% \begin{itemize}
%     \item \textbf{Forward Pass -} \\
%     \begin{itemize}
%         \item $e^{-x} \to 0$ : \\
%         \begin{align}
%             \lim_{e^{-x} \to 0} \text{GoLU}(x) = x \cdot e^{-0} = x \cdot 1 = x \label{eq:golu-forward-pos-inf-case}
%         \end{align}
%     \end{itemize}
    
%     \begin{itemize}
%         \item $e^{-x} \to +\infty$ : \\
%         \begin{align}
%             \lim_{e^{-x} \to +\infty} \text{GoLU}(x) = x \cdot e^{-\infty} = x \cdot 0 = 0 \label{eq:golu-forward-neg-inf-case}
%         \end{align}
%     \end{itemize}
    
%     \item \textbf{Backward Pass -} \\
%     \begin{itemize}
%         \item $e^{-x} \to 0$ : \\
%         \begin{align}
%             \lim_{e^{-x} \to 0} \text{GoLU}'_x(x) &= e^{-0} \cdot (1 + x \cdot 0) \\
%             &= (1 + x \cdot 0)\\
%             & = (1 + 0) = 1 \label{eq:golu-backward-pos-inf-case}
%         \end{align}
%     \end{itemize}
    
%     \begin{itemize}
%         \item $e^{-x} \to +\infty$ : \\
%         \begin{align}
%             \lim_{e^{-x} \to +\infty} \text{GoLU}'_x(x) &= e^{-\infty} \cdot (1 + x \cdot \infty)\\
%             &= 0 \cdot (1 + x \cdot \infty)\\
%             &= 0 \cdot \infty = 0 \label{eq:golu-backward-neg-inf-case}
%         \end{align}
%     \end{itemize}
    
% \end{itemize}

% IEEE Standards define NaN cases, such as $0 / 0$ and $\infty \cdot 0$. As seen in equation \ref{eq:golu-backward-neg-inf-case}, the multiplication of zero with infinity results in NaNs. We address this by setting NaNs to 0, as logically, $0 \cdot \infty = 0$.

% Invalid initializations of parameters in the Gompertz function, such as \( \alpha \), \( \beta \), or \( \gamma \leq 0 \), can also cause instabilities. These should be avoided to preserve the characteristic S-shape of the Gompertz function, as visualized in Figure \ref{fig:gompertz-negative-beta-gamma}.

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.9\linewidth]{images//4-stability-speed-hardware/negative_beta_gamma.png}
%     \caption{The effect of negative $\beta$ or negative $\gamma$ on the Gompertz function.}
%     \label{fig:gompertz-negative-beta-gamma}
% \end{figure}


% \subsection{Time Complexity}\label{subsec:golu-speed}

% % \begin{table*}[t]
% %     \centering
% %     \small
% %     \renewcommand{\arraystretch}{1.2}
% %     \caption{Relative training and inference speed with respect to baseline activations for all our trained architectures.}
% %     \begin{tabular}{|c|c|c|c|c|}
% %         \hline
% %         \textbf{Architecture} & \textbf{Dataset} & \textbf{Baseline Activation} & \textbf{Relative Training Speed} & \textbf{Relative Inference Speed}\\
% %         \hline
% %         \textbf{ResNet18} & ImageNet-1k & ReLU & 1.00 & 1.00 \\
        
% %         \textbf{ResNet34} & ImageNet-1k & ReLU & 1.01 & 1.00 \\
        
% %         \textbf{ResNet-50} & ImageNet-1k & ReLU & 1.01 & 1.01 \\
        
% %         \textbf{WideResNet-50-2} & ImageNet-1k & ReLU & 1.03 & 1.02 \\
        
% %         \textbf{DenseNet121} & ImageNet-1k & ReLU & 1.02 & 1.02 \\
        
% %         \textbf{EfficientNet-B0} & ImageNet-1k & Swish & 1.00 & 1.00 \\

% %         \textbf{TinyViT} & ImageNet-1k & GELU & 0.99 & 0.98 \\
        
% %         \textbf{ViT-B/32} & ImageNet-1k & GELU & 0.99 & 0.99 \\
        
% %         \textbf{ViT-B/16} & ImageNet-1k & GELU & 0.98 & 0.98 \\
        
% %         \textbf{BabyGPT} & TinyStories & GELU & 1.00 & 1.00 \\
        
% %         \textbf{GPT2-S} & OpenWebText & GELU & 1.01 & 1.01 \\
        
% %         \textbf{DeepLabV3} & MS-COCO & ReLU & 1.14 & 1.04 \\
        
% %         \textbf{RetinaNet} & MS-COCO & ReLU & 1.00 & 1.00 \\
        
% %         \textbf{FasterRCNN} & MS-COCO & ReLU & 1.03 & 1.00 \\
        
% %         \textbf{MaskRCNN} & MS-COCO & ReLU & 1.05 & 1.02 \\
        
% %         \textbf{DDPM} & CelebA & Swish & 0.97 & 0.97 \\ \hline
        
% %         \textbf{Average} & - & - & 1.01 & 1.00 \\
% %         \hline
% %     \end{tabular}
% %     \label{tab:training-inference-speeds}
% % \end{table*}

% For training models with GoLU, we build our CUDA Kernel. This is to make sure there exists an apples-to-apples comparison because all other activation functions in PyTorch use CUDA Kernel in the Eager mode. Further, we report the relative training and inference speed in Table \ref{tab:training-inference-speeds}. We observe that GoLU is equivalent in speed across all the architectures. The only architecture where GoLU takes more training time is DeepLabV3-ResNet-50 trained on MS-COCO. However, GoLU is not an exception in this case because all other activations show a higher training time in comparison to ReLU.



% \subsection{Software and Training Hardware}\label{subsec:software-and-training-hardware}

% We train all our models with PyTorch. 



% \section{Variance Reduction in Trained Models}
% \label{app:golu-var-reduction}

% % \begin{figure}[H]
% %     \centering
% %     \includegraphics[width=0.8\linewidth]{images//5-appendix/all_images_imagenet.png}
% %     \caption{Images from the ImageNet-1K dataset used to conduct the variance reduction experiment with the learned ResNet-50 checkpoints. Classes include Goldfish (Top-Left), Goldfinch (Middle-Left), American Chameleon (Bottom-Left) and Squirrel Monkey (Right).}
% %     \label{fig:images-in1k-var-red}
% % \end{figure}

% Our experiment proving variance reduction in the latent representation (See Section \ref{subsection-effects-on-training}) using the CIFAR-10 images with several baseline activations is conducted using randomly initialized weights for the $3 \text{X} 3$ 2D Convolution. However, to validate that the effect holds for the trained models too, we leverage our trained ResNet-50 on ImageNet-1K. We choose four images from the ImageNet-1K dataset as shown in Figure \ref{fig:images-in1k-var-red}. Further, we pass these images via the trained ResNet50 checkpoints for respective activations and extract the activation map of the last BottleNeck block. Later, we plot the density over the neuron values pertaining to the specific image and activation as shown in Figure \ref{fig:latent-var-red}. To avoid extreme outliers, we consider 98\% of the data in the latent representation aligning as non-outlier values.
% %
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.6\linewidth]{images//5-appendix/latent_distributions.png}
%     \caption{Latent representation distribution (output of the last BottleNeck block of a trained Resnet-50) of four images from the ImageNet-1K. The corresponding variance is provided in the legend next to the activation function.}
%     \label{fig:latent-var-red}
% \end{figure}
% %
% As we observe in Figure \ref{fig:latent-var-red}, GoLU shows the same ``squeezing effect" with the trained ResNet-50 activation map of the last BottleNeck block. Also, it consistently lowers the variance in the latent representation aligning with our experiments in Section \ref{subsection-effects-on-training}.  


\section{Learning Rate ablation}\label{sec:learning-rate-ablation}
\setlength{\textfloatsep}{5pt}
\setlength{\floatsep}{5pt}
\setlength{\intextsep}{5pt} 
For various tasks, we conduct a focused search over the learning rate to determine whether the default setting represents the optimal value and to assess its impact on the performance of models trained with different activation functions. Figures \ref{fig:lr-ablations-deeplabv3-mscoco} and \ref{fig:lr-ablations-ddpm-celeba} 
% \ref{fig:lr-ablations-baby-gpt-tiny-stories} and 
present heatmaps of test results for Semantic Segmentation and Diffusion tasks, comparing models trained with various activation functions across different learning rates. For these tasks, the default learning rate, highlighted by a black box, differs from the optimal learning rate, indicated by a green box. Notably, while GoLU achieves a test mIoU that is a close second to ReLU under the default learning rate, it outperforms all other activation functions when evaluated at the optimal learning rate, which is consistent across all activations.
\begin{figure}[H]
\begin{minipage}{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{images//0-learning-rate-ablations/accuracies/semantic_segmentation/deeplabv3_resnet50.png}
    \caption{Test mIoU - DeepLabV3 on MS-COCO. The default learning rate is 0.02 which is colored in black and the best learning rate is 0.01 which is colored in green.}
    \label{fig:lr-ablations-deeplabv3-mscoco}
    \end{minipage}
\hfill
    \begin{minipage}{0.49\linewidth}
\centering
\includegraphics[width=\linewidth]{images//0-learning-rate-ablations/losses/diffusion/ddpm.png}
    \caption{Test Loss - DDPM on CelebA. The default learning rate is 0.0003 which is colored in black and the best learning rate is 0.001 which is colored in green.}
    \label{fig:lr-ablations-ddpm-celeba}
    \end{minipage}
\end{figure}
Motivated by these results, we further investigate the impact of learning rate on image classification tasks where GoLU demonstrated superior performance compared to baseline activations. Figures \ref{fig:lr-ablations-resnet50-in1k} and \ref{fig:lr-ablations-vitb32-in1k} present heatmaps of test accuracies for ResNet-50 and ViT-B/32 on ImageNet-1k.
\begin{figure}[H]
    \begin{minipage}{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{images//0-learning-rate-ablations/accuracies/classification_large/resnet50.png}
    \caption{Test accuracies - ResNet-50 on ImageNet-1k. The default learning rate is 0.1 which is also the best and is colored in green.}
    % \caption{Test accuracies for different learning rates - ResNet-50 on ImageNet-1k.}
    \label{fig:lr-ablations-resnet50-in1k}
    \end{minipage}
\hfill
    \begin{minipage}{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{images//0-learning-rate-ablations/accuracies/classification_large/vit_b_32.png}
    \caption{Test accuracies - ViT-B/32 on ImageNet-1k. The default learning rate is 0.003 which is also the best and is colored in green.}
    \label{fig:lr-ablations-vitb32-in1k}
    \end{minipage}
\end{figure}
Notably, we observe that the optimal learning rate aligns with the default learning rate in this case. These findings reinforce the broader trend that, with few exceptions, \textit{GoLU consistently outperforms baseline activation functions across tasks when evaluated at the optimal learning rate}.
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.49\textwidth]{images//0-learning-rate-ablations/accuracies/language_modeling/baby_gpt.png}
%     \caption{Test token accuracies - babyGPT on TinyStories. The default learning rate is 0.001 which is also the best and is colored in green.}
%     \label{fig:lr-ablations-baby-gpt-tiny-stories}
% \end{figure}


\section{Critical Difference Analysis}
\label{cd-analysis}

In this section, we conduct a Critical Difference analysis following \cite{10.5555/1248547.1248548} to systematically rank activation functions based on experiments performed on ImageNet-1k, MS-COCO, OWT, TS, and CelebA. As shown in Figure~\ref{cd_dragram}, GoLU achieves the highest rank, followed by GELU. Notice that the confidence interval in this analysis is independent of the variance across multiple runs with different random seeds. Instead, it is determined by the number of models and datasets, as well as the significance level, which is set to $\alpha=0.05$ here.
\begin{figure}[H]
    \centering
        \centering
        \includegraphics[width=0.7\linewidth]{images/5-appendix/CD_diagram.png}
        \vspace{-20pt}
        \caption{Critical Difference diagram, ranking activation functions based on average performance.}
    \label{cd_dragram}
\end{figure}

\section{Training and inference times}
\label{speeds}

\begin{table}[H]
    \centering
    \small
    \renewcommand{\arraystretch}{1.2}
    \caption{Relative training and inference time with respect to baseline activations for our trained architectures.}
    \begin{tabular}{|c|c|c|c|c|}
        % \hline
        % \multirow{2}{*}{\textbf{Architecture}} & \multirow{2}{*}{\textbf{Dataset}} & \textbf{Baseline} & \textbf{Relative} & \textbf{Relative}\\[-1mm]
        % & & \textbf{Activation} & \textbf{Training Speed} & \textbf{Inference Speed}\\
        \hline
        \textbf{Architecture} & \textbf{Dataset} & \textbf{Baseline Activation} & \textbf{Relative Training Time} & \textbf{Relative Inference Time}\\
        \hline
        \textbf{ResNet-18} & ImageNet-1k & ReLU & 1.00x & 1.00x \\
        
        \textbf{ResNet-34} & ImageNet-1k & ReLU & 1.01x & 1.00x \\
        
        \textbf{ResNet-50} & ImageNet-1k & ReLU & 1.01x & 1.01x \\
        
        \textbf{WideResNet-50-2} & ImageNet-1k & ReLU & 1.03x & 1.02x \\
        
        \textbf{DenseNet-121} & ImageNet-1k & ReLU & 1.02x & 1.02x \\
        
        \textbf{EfficientNet-B0} & ImageNet-1k & Swish & 1.00x & 1.00x \\

        \textbf{TinyViT} & ImageNet-1k & GELU & 0.99x & 0.98x \\
        
        \textbf{ViT-B/32} & ImageNet-1k & GELU & 0.99x & 0.99x \\
        
        \textbf{ViT-B/16} & ImageNet-1k & GELU & 0.98x & 0.98x \\
        
        \textbf{babyGPT} & TinyStories & GELU & 1.00x & 1.00x \\
        
        \textbf{GPT2-S} & OpenWebText & GELU & 1.01x & 1.01x \\
        
        \textbf{DeepLabV3} & MS-COCO & ReLU & 1.14x & 1.04x \\
        
        \textbf{RetinaNet} & MS-COCO & ReLU & 1.00x & 1.00x \\
        
        \textbf{FasterRCNN} & MS-COCO & ReLU & 1.03x & 1.00x \\
        
        \textbf{MaskRCNN} & MS-COCO & ReLU & 1.05x & 1.02x \\
        
        \textbf{DDPM} & CelebA & Swish & 0.97x & 0.97x \\ \hline
        
        \textbf{Average} & - & - & 1.01x & 1.00x \\
        \hline
    \end{tabular}
    \label{tab:training-inference-speeds}
\end{table}


\section{Experimental Details}
\label{sec:experimental-details}

This section outlines detailed information about the datasets and training pipelines used for the various tasks studied in this work.

\subsection{Image Classification - ImageNet}
\label{sec:experimental-details-ic}

In image classification experiments on ImageNet-1k, ResNets 18, 34, 50, WideResNet-50-2 and DenseNet-121 are trained for 90 epochs with a batch size of 256, SGD with momentum=0.9 (Nesterov for WRN-50-2 and DN-121), learning rate 0.1, and weight decay \(1 \times 10^{-4}\). Further, a Step learning rate scheduler is applied that reduces the learning rate by a gamma = 0.1 after every 30 epochs. EfficientNet-B0 is trained using the timm library for 450 epochs with a batch size of 1536 using RMSProp \cite{hinton2012neural} with an initial learning rate of 0.048 and a weight decay of \(1 \times 10^{-5}\). ViT models are trained for 300 epochs with a batch size of 4096 using AdamW \cite{loshchilov2017fixing} with an initial learning rate of \(3 \times 10^{-3}\) and weight decay of 0.3. Various regularization techniques are applied, including Exponentially Moving Averaged Weights \cite{tarvainen2017mean}, AutoAugment \cite{cubuk2019autoaugment} (ImageNet policy for ViTs), RandAugment \cite{cubuk2020randaugment}, MixUp \cite{zhang2017mixup}, CutMix \cite{yun2019cutmix} and Label Smoothing \cite{szegedy2016rethinking} for EfficientNet-B0 and ViT models. ViT-B/16 shows slight instability for seed 1 for GELU. Hence we further average seeds 2 and 3 for both GELU and GoLU. We find that GELU shows a top-1 accuracy of $80.61\pm0.06$ while GoLU shows top-1 accuracy of $80.69\pm0.07$ which is higher than GELU.

\subsection{Image Classification - CIFAR-10}
% \section{CIFAR-10 results on ResNets, WideResNet, DenseNet and ViTs}
\label{sec:cifar-10-results}

% We report in Table \ref{tab:image-classification-cifar10} the results of image classification on CIFAR-10, with ResNets 20, 32, 44, 56, and 110, WideResNet28-2, DenseNet40 and ViT-Ti/16-224. GoLU consistently outperforms the standard baselines across all cases. We have further underlined the second best activations for each model. As in image classification on ImageNet-1k, no single activation consistently ranks second. We summarize the training setup at follows:

The ResNet 20, 32, 44, 56 and 110 models are trained for 164 epochs with a batch size of 128, a learning rate of 0.1, and SGD with momentum \(0.9\). A weight decay of \(1 \times 10^{-4}\) is applied, along with a MultiStep learning rate scheduler with a gamma factor of 0.1 at epochs 81 and 122 (with an initial learning rate of 0.01 and additional gamma factor of 10 at epoch 2 for ResNet-110).

WideResNet28-2 and DenseNet40, were trained for 200 and 300 epochs, and batch sizes of 128 and 64, respectively. We employ SGD with Nesterov momentum \(0.9\) for both architectures, using a learning rate of 0.1. The weight decays are \(5 \times 10^{-4}\) for WideResNet28-2 and \(1 \times 10^{-4}\) for DenseNet40. Similar to ResNets, both WideResNet28-2 and DenseNet40 use the MultiStep learning rate scheduler. However, WideResNet28-2 reduces the learning rate by a factor of 0.2 at epochs 60, 120, and 160, while DenseNet40 reduces the learning rate by 0.1 at epochs 150 and 225. To train ViT-Ti/16-224 from scratch, we leverage the Timm library. %Further, we train it for 500 epochs, 512 batch size, Adam optimizer, \(1 \times 10^{-3}\) learning rate, and Cosine learning rate scheduler. Finally, we fine-tune ViT-B/16 for 7 epochs using a batch size of 512, an SGD optimizer with momentum \(0.9\), and a learning rate of 0.01. A Cosine learning rate scheduler is employed for this training process.

% As we observe, GoLU outperforms the standard baselines for all the ResNets, WideResNet28-2, DenseNet40 and ViT-Ti-16/224. 

%However, while fine-tuning ViT-B/16, GELU turns to be the best performing activation. This is a standard phenomenon where models tend to perform better in fine-tuning with the pre-training activation function.
%

% \begin{table}[h]
%     \small
%     \centering
%     \renewcommand{\arraystretch}{1.2}
%     % \caption{Top-1 test accuracy of ResNets 20, 32, 44, 56, and 110, WideResNet28-2, DenseNet40, ViT-Ti/16-224 %and ViT-B/16 
%     % on CIFAR-10 with ReLU, LekyReLU, GELU, Swish and GoLU.}
%     \caption{Top-1 test accuracy on CIFAR-10. GoLU consistently outperforms baselines. Second best activations are underlined.}
%     \vspace{-1mm}
%     \begin{tabular}{|c|cccc|c|}
%          \hline
         
%          \textbf{Architecture} & \textbf{ReLU} & \textbf{LeakyReLU} & \textbf{GELU} & \textbf{Swish} & \textbf{GoLU}\\
         
%          \hline
         
%          ResNet-20 & 91.41±0.1 & 91.60±0.0 & 91.62±0.0 & \underline{91.64±0.1} & \textbf{91.77±0.1}\\
         
%          ResNet-32 & 92.21±0.1 & 92.40±0.0 & \underline{92.54±0.1} & 92.16±0.0 & \textbf{92.69±0.1}\\
         
%          ResNet-44 & 92.58±0.0 & \underline{92.78±0.0} & \underline{92.78±0.8} & 92.51±0.0 & \textbf{92.85±0.0}\\
         
%          ResNet-56 & 92.80±0.1 & 92.75±0.1 & 92.86±0.1 & \underline{92.93±0.1} & \textbf{93.15±0.1}\\
         
%          ResNet-110 & \underline{93.21±0.0} & 93.18±0.1 & 92.75±0.1 & 92.23±0.0 & \textbf{93.25±0.0}\\
         
%          WideResNet-28-2 & \underline{94.96±0.0} & 94.81±0.0 & 94.55±0.1 & 93.58±0.1 & \textbf{95.03±0.0}\\
         
%          DenseNet-40 & 93.13±0.1 & 93.13±0.1 & \underline{93.41±0.0} & 93.30±0.1 & \textbf{93.44±0.1}\\
         
%          ViT-Ti/16-224 & \underline{91.74±0.06} & 91.61±0.18 & 91.37±0.11  & 88.61±0.16 & \bf{92.60±0.05} \\
         
%          % ViT-B/16 & 97.32±0.1 & 96.94±0.0 & \textbf{97.89±0.0} & 97.17±0.1 & 97.88±0.0\\ % ViT-B is not suitable for training on CIFAR10
         
%          \hline
%     \end{tabular}
%     \label{app:image-classification-cifar10}
% \end{table}



\subsection{Language modeling}
\label{sec:experimental-details-lm}

Both, TinyStories and OpenWebText datasets are popular benchmarks for training language models. The TinyStories dataset consists of 2,119,719 data points in the training set and 21,990 in the test set, while the OpenWebText dataset has 8,009,762 data points in the training set and 4,007 data points in the test set. Both babyGPT and nanoGPT have a vocabulary size of 50,304 and a maximum sequence length of 1024.

The babyGPT version of the GPT-2 series consists of 6 layers, 6 attention heads, and an embedding dimension of 384, with a feed-forward expansion dimension of 1536 output features. The model is trained for 10,000 iterations with a batch size of 640, using the AdamW optimizer. The initial learning rate is \(1 \times 10^{-3}\), with a minimum learning rate of \(1 \times 10^{-4}\), a weight decay of 0.1, and a gradient clipping norm of 1.0. A Cosine learning rate scheduler is applied with a linear warmup for the first 100 iterations. 

Similarly, the GPT2-S model consists of 12 layers, 12 attention heads, and an embedding dimension of 768. It trains for 600,000 iterations with a batch size of 480, using the AdamW optimizer (with \(\beta_2 = 0.95\)). The initial learning rate is \(6 \times 10^{-4}\), with a minimum learning rate of \(6 \times 10^{-5}\), a weight decay of 0.1, and a gradient clipping norm of 1.0. The Cosine learning rate scheduler is employed with a linear warmup for the first 2,000 iterations.

\subsection{Semantic Segmentation}
\label{exp-details-sem-seg}

The MS-COCO dataset with PASCAL-VOC labels contains 92,518 data points in the training set and 5,000 data points in the test set. The original MS-COCO dataset contains 117,266 data points in the training set. However, the existing benchmark pre-processes and removes images that either lack valid annotations or contain only small objects with an area coverage of less than 1,000 pixels. This ensures the retention of meaningful data points for training the model.

The DeepLabV3-ResNet-50 model is trained for 30 epochs with a batch size of 32, using SGD with momentum \(0.9\), a learning rate of \(2 \times 10^{-2}\), weight decay of \(1 \times 10^{-4}\), and a polynomial learning rate scheduler with a power of 0.9.

\subsection{Object Detection}
\label{subapp:object-detection}

Unlike Semantic Segmentation, the MS-COCO dataset for object detection contains 117,266 images in the training set and 5,000 images in the test set. Additionally, we do not apply any pre-processing that removes images from the training or test sets.

Faster R-CNN-FPN ResNet-50 and RetinaNet-FPN ResNet-50 are trained for 26 epochs with a batch size of 16, an aspect ratio group factor of 3, no frozen batch normalization, and a MultiStep learning rate scheduler that reduces the initial learning rate by a factor of 0.1 at epochs 16 and 22. Specifically, Faster R-CNN-FPN ResNet-50 uses SGD with momentum \(0.9\), a learning rate of \(2 \times 10^{-2}\), and a weight decay of \(1 \times 10^{-4}\), while RetinaNet-FPN ResNet-50 uses the AdamW optimizer with a learning rate of \(1 \times 10^{-4}\) and a weight decay of \(5 \times 10^{-2}\).

\subsection{Instance Segmentation}\label{app:instance-segmentation}

The MS-COCO dataset for instance segmentation uses the same train and test sets as those used for Object Detection. Additionally, it trains with the exact same configurations used for Faster R-CNN-FPN in the previous subsection \ref{subapp:object-detection}.

\subsection{Denoising Diffusion Probabilistic Models}\label{subapp:ddpm}

The CelebA dataset, comprises of 162,770 training images and 19,867 test images of human faces. The Denoising Diffusion Probabilistic Model is trained on the CelebA dataset for 50 epochs with a batch size of 32 leveraging the DDPM \cite{kim2023ddpm} repository. The AdamW optimizer with a learning rate of 0.0003, Cosine learning rate scheduler, and linear learning rate warmup for the first 1,000 iterations are applied.

\section{Case Study: Bayesian Learning Curve Extrapolation using Prior-data fitted Networks}
\label{sec:lcpfn}
In this section, we present an additional experiment on GoLU, initially conducted as an internal validation study. We report this as a ``negative'' result, with GoLU ranking second-to-last under the optimal learning rate. Due to the unconventional experimental setup, its niche focus, and suboptimal hyperparameter tuning, we have included these findings in the appendix rather than in the main text.
 
\paragraph{Experimental Details}
In this experiment, we assessed all 7 activation functions (including GoLU) considered in the main article as activations for LC-PFN~\cite{adriaensen2024efficient}.
LC-PFN is a prior-data fitted network~\cite{muller2021transformers} that functions as a decoder-only transformer, trained for in-context Bayesian prediction for a specific prior dataset distribution. Specifically, LC-PFN is trained for Bayesian Learning Curve extrapolation. We adopted the same setup used to train the best model presented in the original paper, a decoder-only transformer having 26.79M trainable parameters, 12 layers, 4 attention heads, an embedding dimension of 512, and a feed-forward expansion dimension of 1024 output features. It was trained using 10M synthetically generated learning curves, (each containing 100 observations), employing the Adam optimizer (with a default learning rate of 0.0001 and a batch
size of 100), using a cosine scheduler with a linear warmup during the first
25,000 steps (25\%) of the training. At test time, it takes a partial learning curve as input, and predicts the posterior predictive distribution (PPD) for possible continuations. The test performance of the final model was measured using the log-score, which represents the log-likelihood of the true continuation, under the PPD, averaged across all 99 cutoffs for 10,000 curves from the prior.

\begin{figure}[H]
\centering
\includegraphics[width=0.65\linewidth]{images/5-appendix/lc_pfn.png}
\caption{Test log scores - LC-PFN. The default learning rate is 0.0001, which is also optimal, is highlighted in green.}
\label{fig:lc-pfn}
\end{figure}

\paragraph{Results}
Figure ~\ref{fig:lc-pfn} presents the log-scores for the final models, utilizing all 7 activation functions at 5 different learning rates, averaged over 3 training runs. At the original and optimal learning rate of 0.0001, GoLU ranks 6th among the 7 activations. However, a closer examination reveals that the choice of activation function seems to have minimal impact, as the differences between GoLU and the best (ELU) and worst (Swish) activation are within a single standard error. The learning rate ablation shows that GoLU ranks first at the highest stable learning rate (0.001), supporting previous findings that GoLU thrives in the high learning rate regime.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.

