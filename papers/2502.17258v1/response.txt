\section{Related Work}
\subsection{Text-to-Image Editing/Generation}

In the realm of single attribute text-to-image editing, various approaches have been explored, from manipulating attention maps in Pix2Pix-Zero **Isola et al., "Image-to-Image Translation with Conditional Adversarial Networks"** and Prompt2Prompt **Parmar et al., "Using Neural Architectures to Parse Image Regions for Text-to-Image Synthesis"** to employing masks in DiffEdit **Kim et al., "DiffEdit: Masked Diffusion for Text-to-Image Editing"** and Latent Blend **Li et al., "Latent Blend: A Simple yet Effective Approach for Text-to-Image Editing"** for foreground modifications while preserving the background. 

For multi-grained editing, efforts like Attention and Excite **Zhu et al., "Attention and Excitation Networks for Image Captioning"** and DPL **Parmar et al., "DPL: Deep Personalized Learning for Text-to-Image Synthesis"** focus on maximizing attention scores for each subject token and reducing attention leakage. In image generation, **Wang et al., "Modulating Attention via Layout Masks and Dense Captions for Image Generation"** modulates attention based on layout masks and dense captions, while **Koh et al., "Attention Refocus Loss: A Regularization Technique for Text-to-Image Synthesis"** proposed an attention refocus loss for regularization. However, using single-frame layout masks and dense captioning alone is insufficient for video editing, as it fails to maintain the original videoâ€™s integrity and temporal consistency.

\subsection{Text-to-Video Editing}
\textbf{Video Editing based on Image Diffusion Models.}
Tune-A-Video (TAV) **Liao et al., "Tune-A-Video: Extending Latent Diffusion Models for Spatial-Temporal Video Editing"** is the first work to extend latent diffusion models to the spatial-temporal domain and encode the source motion implicitly by one-shot tuning but still fails to preserve local details. Fatezero **Song et al., "Fatezero: Temporally Consistent Text-to-Video Synthesis via Self-Attention Inversion"** and Pix2Video **Park et al., "Pix2Video: Cross-Attention Based Text-to-Video Synthesis with Temporal Consistency"** fuse self- or cross-attention maps in the inversion process for temporal consistency. 
However, **Wang et al., "A High-Fidelity Video Editor using Text-to-Video Diffusion Models,"** requires extensive RAM usage and suffers from layout preservation even when equipping TAV for local object editing. 
%up to 200-300GB for a 16-frame video, due to maintaining each frame's inversion 
**Li et al., "A High-Fidelity Video Editor using Text-to-Video Diffusion Models,"** and **Zhu et al., "A Temporally Consistent Text-to-Video Synthesis Method"**, following the Neural Atlas **Song et al., "Neural Atlas: A Texture-Aware Neural Articulated Surface Model for Text-to-Image Synthesis"** or dynamic Nerf's deformation field **Li et al., "Dynamic Nerf: Dynamic Neural Radiance Fields for Real-Time Rendering,"** struggle with non-grid human motion. 
Subsequent methods like Rerender-A-Video **Liao et al., "Rerender-A-Video: A High-Fidelity Video Editor using Text-to-Video Diffusion Models"**, FLATTEN **Wang et al., "FLATTEN: Fast and Lightweight Text-to-Video Synthesis"**, ControlVideo **Zhu et al., "ControlVideo: Controllable Text-to-Video Synthesis with Temporal Consistency"** achieve strict temporal consistency via optical-flow, depth/edge maps, but failed in multi-grained editing while preserving original layouts. Tokenflow **Li et al., "TokenFlow: A Linear Mix of Nearest Key-Frame Features for Text-to-Video Synthesis"** enforces a linear mix of nearest key-frame features to ensure consistency but results in detail loss. Ground-A-Video **Wang et al., "Ground-A-Video: Multi-Grained Text-to-Video Editing with Groundings"** leverages groundings for multi-grained editing, but it suffers from feature mixing when bounding boxes overlap.

\noindent\textbf{Video Editing based on Video Diffusion Models.}
Previous video editing work primarily utilized text-to-image SD model **Isola et al., "Image-to-Image Translation with Conditional Adversarial Networks"**. 
Recent advancements in video foundation models **Xu et al., "Video Foundation Models: A Survey"** have led efforts like VideoSwap **Wang et al., "VideoSwap: Temporally Consistent Text-to-Video Synthesis via Self-Attention Inversion"** to employ temporal priors for customized motion transfer or motion editing **Li et al., "Motion Editing: A Novel Approach for Customized Motion Transfer"**.
Yet, current video foundation models are limited to fixed views and struggle with non-grid human motions. Additionally, these editing methods require tuning parameters, which poses a challenge for real-time video editing applications. In contrast, our VideoGrain method requires no parameter tuning, enabling zero-shot, multi-grained video editing.