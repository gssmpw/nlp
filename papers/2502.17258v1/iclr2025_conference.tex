
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\definecolor{myrefcolor}{rgb}{0, 0.367, 0.7}
\usepackage[colorlinks=true, allcolors=myrefcolor]{hyperref}
\usepackage{url}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx,wrapfig,lipsum}
\usepackage{booktabs}
\usepackage{colortbl}  %彩色表格需要加载的宏包
\usepackage{xcolor}
\usepackage{multirow} 
\usepackage{pifont}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{float}
\usepackage{enumitem}
\usepackage[font=normalsize,labelfont=bf,tableposition=top]{caption}
\usepackage{csquotes} % For \enquote
\definecolor{aliceblue}{rgb}{0.94, 0.97, 1.0}
\definecolor{deeppink}{RGB}{255,20,147}
\definecolor{mygray}{gray}{.9}
\definecolor{mygray2}{gray}{.6}

\title{VideoGrain: Modulating Space-Time Attention for Multi-grained Video Editing}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{%
Xiangpeng Yang~$^{1}$ \quad
Linchao Zhu~$^{2}$ \quad
Hehe Fan~$^{2}$ \quad
Yi Yang~$^{2}$ \\
$^{1}$ ReLER Lab, AAII, University of Technology Sydney  \quad 
$^{2}$  ReLER Lab, CCAI,  Zhejiang University \quad
\\
{\small Project Page: \url{https://knightyxp.github.io/VideoGrain_project_page}}\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{figure}[h]
  \vspace{-10mm}
  \centering
  \includegraphics[width=\linewidth]{figure/fig1.pdf}
  \caption{VideoGrain enables multi-grained video editing across class, instance, and part levels.}

  \label {fig:teaser}
\end{figure}

\begin{abstract}

Recent advancements in diffusion models have significantly improved video generation and editing capabilities. However, multi-grained video editing, which encompasses class-level, instance-level, and part-level modifications, remains a formidable challenge.
The major difficulties in multi-grained editing include semantic misalignment of text-to-region control and feature coupling within the diffusion model.
To address these difficulties, 
we present VideoGrain, a zero-shot approach that modulates space-time (cross- and self-) attention mechanisms to achieve fine-grained control over video content.
We enhance text-to-region control by amplifying each local prompt's attention to its corresponding spatial-disentangled region while minimizing interactions with irrelevant areas in cross-attention.
Additionally, we improve feature separation by increasing intra-region awareness and reducing inter-region interference in self-attention.
Extensive experiments demonstrate our method achieves state-of-the-art performance in real-world scenarios.
Our code, data, and demos are available on the \href{https://knightyxp.github.io/VideoGrain_project_page/}{project page}.
\end{abstract}

%yet precise manipulation at the instance- and part- level remains challenging.
%few dedicated to editing multiple objects to different instances.
%few address the complex task of editing multiple objects to different instances.
%To address this, we introduce a new video editing task called \textbf{multi-grained video editing}, which includes class-level, instance-level and part-level edits.
%\Linchao{In this paper, }

\section{Introduction}
\begin{figure}[h]
  \vspace{-6mm}
  \centering
  \includegraphics[width=\linewidth]{figure/intro.pdf}
  \vspace{-6mm}
  \caption{Definition of multi-grained video editing and comparison on instance editing}
  \vspace{-4mm}
  \label {fig: intro}
\end{figure}

% In today's digital age, the demand for efficiently editing short videos has grown significantly, driven by their dominance in online entertainment platforms.
%

Recent advances in Text-to-Image (T2I) and Text-to-Video (T2V) diffusion models \citep{rombach2022high,wang2023modelscope, SORA} have enabled video manipulation through natural language prompts. 
%
In practical applications, enabling users to edit regions at various levels of granularity based on textual prompts offers greater flexibility.
%
To investigate this, we introduce a new task called multi-grained video editing, which encompasses class-level, instance-level, and part-level editing, as shown in Fig.~\ref{fig: intro} left.
Class-level editing refers to modifying objects within the same class. 
Instance-level editing means editing different instances into distinct objects. 
Part-level going further, requires adding new objects or modifying existing attributes at part-level.

While existing methods employ various visual consistency techniques, such as optical flow \citep{cong2023flatten,yang2023rerender}, control signals \citep{zhang2023controlvideo}, or feature correspondence \citep{geyer2023tokenflow}. These methods remain instance-agnostic, often mixing features of different instances during editing (see Fig.~\ref{fig: intro} right). 
%
Ground-A-Video \citep{jeong2023ground}, which inherits text-to-bounding box generation priors \citep{li2023gligen}, should be instance-level editing but still suffer from artifacts. 
%
Similarly, recent T2V-based methods like DMT \citep{yatim2024space} and Pika \citep{pika}, although equipped with video generation priors, struggle with multi-grained edits. 
%
We find that the core issue is that diffusion models tend to treat different instances as the same class segments, leading to strong feature coupling across instances, as illustrated in Figure \ref{fig: motivation}.


To address this problem, our primary insight is to 1) enable text-to-region control and 2) keep feature separation between regions. 
In the typical diffusion models, the cross-attention layer serves as a key component to update textual features control over each spatial region, while the self-attention layer generates globally coherent structures by connecting each frame token across time. 
Therefore, we propose Spatial-Temporal Layout-Guided Attention (ST-Layout Attn), which modulates both space-time cross- and self-attention in a unified manner to achieve the above goals. 

In the \textit{cross-attention layer}, the uniform application of global text prompts across all frame tokens leads to severe semantic misalignment, which reduces the precision of multi-grained text-to-region control.
To address this, we modulate cross-attention to amplify each local prompt's focus on its corresponding spatial-disentangled region while suppressing attention to irrelevant areas.
In the \textit{self-attention layer}, pixels from one region may attend to outside or similar regions within the same class, leading to feature coupling and texture mixing, which is an inherent limitation of diffusion models that complicates multi-grained video editing.
To mitigate this, we modulate self-attention to enhance feature separation by increasing intra-region focus and reducing inter-region interactions, ensuring each query attends only to its target region.

Our key contributions can be summarized as follows:
\begin{itemize}
\item To the best of our knowledge, this is the first attempt at multi-grained video editing. Our method enables both class-level, instance-level and part-level editing.
\item We propose a novel framework, dubbed \textit{VideoGrain}, which modulates spatial-temporal cross- and self-attention for text-to-region control and feature separation between regions.
\item Without tuning any parameters, we achieve state-of-the-art results on existing benchmarks and real-world videos both qualitatively and quantitatively.
\end{itemize}

\section{Related Work}

\subsection{Text-to-Image Editing/Generation}

In the realm of single attribute text-to-image editing, various approaches have been explored, from manipulating attention maps in Pix2Pix-Zero \citep{parmar2023zero} and Prompt2Prompt \citep{hertz2022prompt} to employing masks in DiffEdit \citep{couairon2023diffedit} and Latent Blend \citep{avrahami2022blended,avrahami2023blended} for foreground modifications while preserving the background. 

For multi-grained editing, efforts like Attention and Excite \citep{chefer2023attend} and DPL \citep{wang2023dynamic} focus on maximizing attention scores for each subject token and reducing attention leakage. In image generation, \citep{densediffusion} modulates attention based on layout masks and dense captions, while \citep{phung2023grounded} proposed an attention refocus loss for regularization. However, using single-frame layout masks and dense captioning alone is insufficient for video editing, as it fails to maintain the original video’s integrity and temporal consistency.

\subsection{Text-to-Video Editing}
\textbf{Video Editing based on Image Diffusion Models.}
Tune-A-Video (TAV) \citep{wu2022tune} is the first work to extend latent diffusion models to the spatial-temporal domain and encode the source motion implicitly by one-shot tuning but still fails to preserve local details. Fatezero \citep{qi2023fatezero} and Pix2Video \citep{ceylan2023pix2video} fuse self- or cross-attention maps in the inversion process for temporal consistency. 
However, \citep{qi2023fatezero} requires extensive RAM usage and suffers from layout preservation even when equipping TAV for local object editing. 
%up to 200-300GB for a 16-frame video, due to maintaining each frame's inversion 
\citep{chai2023stablevideo} and \citep{ouyang2023codef}, following the Neural Atlas \citep{kasten2021layered} or dynamic Nerf's deformation field \citep{pumarola2021d}, struggle with non-grid human motion. 
Subsequent methods like Rerender-A-Video \citep{yang2023rerender}, FLATTEN \citep{cong2023flatten} ControlVideo \citep{zhang2023controlvideo} achieve strict temporal consistency via optical-flow, depth/edge maps, but failed in multi-grained editing while preserving original layouts. Tokenflow \citep{geyer2023tokenflow} enforces a linear mix of nearest key-frame features to ensure consistency but results in detail loss. Ground-A-Video \citep{jeong2023ground} leverages groundings for multi-grained editing, but it suffers from feature mixing when bounding boxes overlap.

\noindent\textbf{Video Editing based on Video Diffusion Models.}
Previous video editing work primarily utilized text-to-image SD model \citep{rombach2022high}. 
Recent advancements in video foundation models \citep{yu2023magvit, guo2023animatediff, wang2023modelscope, yang2024cogvideox} have led efforts like VideoSwap \citep{gu2023videoswap} to employ temporal priors for customized motion transfer or motion editing \citep{mou2025revideo}.
Yet, current video foundation models are limited to fixed views and struggle with non-grid human motions. Additionally, these editing methods require tuning parameters, which poses a challenge for real-time video editing applications. In contrast, our VideoGrain method requires no parameter tuning, enabling zero-shot, multi-grained video editing.

\iffalse
\section{Preliminaries}
\paragraph{Diffusion Models}
\cite{rombach2022high} is proposed to apply the diffuse and denoise process in the latent space of an autoencoder. It contains two main components: First, an encoder $\mathcal{E}$ compresses the input image $x$ into a latent $z$ and a decoder $\mathcal{D}$ to reconstruct the latent back to image $x \approx \mathcal{D}(z)$.
Second, a U-Net model $\epsilon_{\theta}$ learns the distribution of image latent via a forward and backward process.

In the forward process, Gaussian noise is gradually added to the latent input ${z}_0$ at timestep $t$ to obtain $z_t$, the process can be formulated as:
\begin{equation}
\centering
q({z}_t | {z}_{t-1}) = \mathcal{N}({z}_t; \sqrt{1-\beta_t}{z}_{t-1}, \beta_t {\text{I}}),
\label{eq:ddpm_forward}
\end{equation}
where $\beta_t$ is the variance schedule for the timestep $t$.
The number of timesteps used to train the diffusion model is denoted by $T$.
The backward process uses a trained U-Net $\epsilon_{\theta}$ for denoising:
\begin{equation}
\centering
p_{\theta}({z}_{t-1}|{z}_t) = \mathcal{N}({z}_{t-1}; \mu_{\theta}({z}_t, {\tau}, t),  \Sigma_{\theta}({z}_t, {\tau}, t) ),
\label{eq:ddpm_backward}
\end{equation}
where ${\tau}$ indicates the textual prompt. $\mu_{\theta}$ and $\Sigma_{\theta}$ are computed by the denoising model $\epsilon_{\theta}$.

The prevalent approach in diffusion-based video editing is to use a deterministic DDIM scheme \citep{song2020denoising}, which can convert a random noise to a deterministic $z_0$ in a sequence of time step $T$:
\begin{equation}
\label{eq: denoise}
    z_{t-1} = \sqrt{\alpha_{t-1}} \; \frac{z_t - \sqrt{1-\alpha_t}{\varepsilon_\theta}}{\sqrt{\alpha_t}}+ \sqrt{1-\alpha_{t-1}}{\varepsilon_\theta}
\end{equation}
where $\alpha_{t}$ denotes $\prod^t_{i=1}(1-\beta_i) $. In EVA, we use Stable Diffusion(SD) $\epsilon_\theta(z_t, t, \tau)$ as our base model, which is an instantiation of text-guided LDMs pre-trained on billions of image-text pairs.
\fi


% \vspace{-4mm}
\section{Method}
% 发现问题，提出问题，如何解决每个问题
% \vspace{-2mm}
\subsection{Motivation}
\label{sec: motivation}
%visualize self attn cluster mask
% cross attention weight distribution is not accurate.
\begin{figure}[t]
  \centering
  % \vspace{-mm}
  \includegraphics[width=\linewidth]{figure/motivation.pdf}
  \vspace{-7mm}
  \caption{\textbf{Analysis of why the diffusion model failed in instance-level video editing}. Our goal is to edit left man into ``Iron Man," right man into ``Spiderman," and trees into ``cherry blossoms." In (b), we apply K-Means on self-attention, and in (d), we visualize the 32x32 cross-attention map. }
  \vspace{-5mm}
  \label {fig: motivation}
\end{figure}
%
To investigate why previous methods failed in instance-level video editing (see Fig.~\ref{fig: intro}), we begin with a basic analysis of the self-attention and cross-attention features within the diffusion model.

%
As shown in Fig.~\ref{fig: motivation} (b), we apply K-Means clustering to the per-frame self-attention features during DDIM Inversion. Although the clustering captures a clear semantic layout, it fails to distinguish between distinct instances (e.g., ``left man" and ``right man"). Increasing the number of clusters leads to finer segmentation at the part level but does not resolve this issue, indicating that feature homogeneity across instances limits the diffusion model’s effectiveness in multi-grained video editing.

% instance-level editing 
Next, we attempt to edit the same class of two men into different instances using SDEdit \citep{meng2022sdedit}.
However, Fig.~\ref{fig: motivation} (d) shows that the weights for ``Iron Man" and ``Spiderman" overlap on the left man, and ``blossoms" weight leaks onto the right man, resulting in the failed edit in (c).

%
Thus, for effective multi-grained editing, we pose the following question: \textit{Can we modulate attention to ensure that each local edit's attention weights are accurately distributed in the intended regions?}

To answer this, we propose VideoGrain with two key designs: 
%
(1) Modulate cross-attention to induce textual features to congregate in corresponding spatial-disentangled regions, thereby enabling text-to-region control.
%
(2) Modulate self-attention across the spatial-temporal axis to enhance intra-region focus and reduce inter-region interference, avoiding feature coupling within diffusion model.

\subsection{Problem Formulation}
\label{sec: task formulation}
The purpose of this work is to perform multi-grained video editing across multiple regions based on the given prompts. This involves three hierarchical levels:

\textbf{(1) Class-level editing}: Editing objects within the same class. (e.g., changing two men to ``Spiderman," where both belong to the human class, as seen in Fig.~\ref{fig: intro} second column)
%(e.g., changing a Jeep to a Porsche, where both belong to the car class).
% $\mathcal{C} = \{c_1, c_2, \dots, c_k\}$ 

\textbf{(2) Instance-level  editing}: Editing each individual instance to distinct object. (e.g., editing left man to ``Spiderman," right man to ``Polar Bear," as shown in Fig.~\ref{fig: intro} third column).
%$\mathcal{I} = \{i_1, i_2, \dots, i_m\}$

\textbf{(3) Part-level editing}: Applying part-level edit to specific elements of individual instances. (e.g., adding ``sunglasses "when editing the right man to ``Polar Bear" in Fig.~\ref{fig: intro}  fourth column).
%$\mathcal{P}_i$ $\mathcal{I}_m$

Given a source video $\bm{\mathrm{V}} \in \mathbb{R}^{N\times 3\times H\times W}$, where $N$ is the number of frames, our goal is to obtain an edited video $\bm{\mathrm{V'}}$ based on specified edits. We aim to improve multi-grained control in video editing by conditioning on each region's location and its text prompt. More formally, we optimize a video editing model $f(\tau_g, {(\tau_1, m_1), \dots, (\tau_k, m_k)})$, where $\tau_g$ is a global prompt, and $(\tau_k, m_k)$ are the $k_{th}$ region’s prompt and corresponding location.


\subsection{Overall Framework}
\label{sec: framework}
The proposed zero-shot multi-grained video editing pipeline is illustrated in Fig.~\ref{fig: framework} top.
Initially, to retain high fidelity, we perform DDIM Inversion \citep{songdenoising} over the clean latent $\bm{x}_0$ to get the noisy latent $\bm{x}_t$. 
%
After the inversion process, we cluster the self-attention features to get the semantic layout as in Fig.~\ref{fig: motivation} (b).
%
Since self-attention features alone cannot distinguish between individual instances, we further employ SAM-Track \citep{cheng2023segment} to segment each instance. 
% 
Finally, in the denoising process, we introduce ST-Layout Attn to modulate cross- and self-attention for text-to-region control and keep feature separation between regions, as detailed in Sec.~\ref{sec: st-layout-attn}.
%

Different from one global text prompt control of all frames, VideoGrain allows paired instance- or part-level prompts and their locations to be specified in the denoising process.
%
Our method is also versatile to ControlNet condition $\bm{e}$, which can be depth or pose maps to provide structure conditions.



% Following previous work \citep{wu2022tune,qi2023fatezero}, we inflated the original StableDiffusion\citep{rombach2022high} (SD) along the temporal axis to adapt for 3-dimension video input.


\begin{figure}[t]
  \centering
  % \vspace{-mm}
  \includegraphics[width=\linewidth]{figure/framework_revision.pdf}
  \vspace{-6mm}
  \caption{VideoGrain pipeline. 
  (1) we integrate ST-Layout Attn into the frozen SD for multi-grained editing, where we modulate self- and cross-attention in a unified manner.
  (2) In cross-attention, we view each local prompt and its location as positive pairs, while the prompt and outside-location areas are negative pairs, enabling text-to-region control. 
  (3) In self-attention, 
  we enhance positive awareness within intra-regions and restrict negative interactions between inter-regions across frames, making each query only attend to the target region and keep feature separation. In the bottom two figures, $p$ denotes original attention score and  $w,i$ denotes the word and frame index.}
  \vspace{-5mm}
  \label {fig: framework}
\end{figure}

\subsection{Spatial-Temporal Layout-Guided Attention}
\label{sec: st-layout-attn}
%% Our ST-Layout Attn contains two parts: cross-attention modulation for text-to-region control and self-attention modulation to keep adjacent regions distinct, as shown in Fig.\ref{fig: framework}.
Based on the observation in Sec.\ref{sec: motivation}, cross-attention weight distribution adheres to the edit result.
%
Meanwhile, self-attention is also crucial to generate temporal consistent video.
%
However, the pixels in one region may attend to outside or similar regions, which poses an obstacle for multi-grained video editing.
%
Therefore, we need to modulate both self- and cross-attention to make each pixel or local prompt only focus on the correct region.

To achieve this goal, we modulate both cross- and self-attention mechanisms via a unified increase positive and decrease negative manner.
Specifically, for the $i_{th}$ frame of the query feature, we modulate the query-key ${QK^\top}$ condition map as follows:
\begin{equation}
\centering
\begin{gathered}
    A_i^{\text{self}/\text{cross}} = \mbox{softmax}(\frac{QK^\top + \lambda  M^{\text{self}/\text{cross}} }{\sqrt{d}}), \\
    {M}^{\text{self}/\text{cross}} = {R_i} \odot M_i^{\text{pos}} - (1-R_i) \odot M_i^{\text{neg}} , 
\end{gathered}
\label{eq:st-layout attn}
\end{equation}

where ${R}_i \in \mathbb{R}^{|\text{queries}| \times |\text{keys}|}$ indicates the query-key pair condition map at frame $i$,
manipulating whether to increase or decrease the attention score for a particular pair. 
%
And $\lambda = \xi(t) \cdot \left(1 - \text{S}_i\right)$ is a regularization term. 
We follow the conclusion from \citep{densediffusion}, the $\xi(t)$ controls the modulation intensity across time-steps, allowing for gradual refinement of shape and appearance details.
%
The latter is a size regulation term, making smaller region $m_{k}$ subjected to larger modulation, enabling dynamic attention weight adjustments to layout size variations.

\textbf{Modulate Cross-Attention for Text-to-Region Control.}
In the cross-attention layer, the textual feature serves as key and value, and interacts with the query feature from the video latent.
%which can be formulated as $A^\text{cross}(Q,K)=\mathrm{Softmax}(\tfrac{QK^{T}}{\sqrt{d}})$, with $Q =  W^{Q}\cdot {z}^i_{t}$ and  $K =  W^K\cdot {\tau_g}$. 
%
Since each instance's appearance and location are closely related to the cross-attention weight distribution, we aim to encourage each instance's textual features to congregate in the corresponding location.

As shown in Fig.~\ref{fig: framework} mid, given the layout condition $(\tau_k, m_k)$. For example, for $\tau_1 = \text{Spiderman}$, within the query-key cross-attention map, we can manually specify that the portion of the query feature corresponding to $m_1$ is positive, while all the remaining parts are designated as negative.
Therefore, for each frame $i$, we can set the modulation value in cross attention layer as:
\begin{equation}
\centering
\begin{gathered}
M_i^{\text{pos}} = \mbox{max}(QK^\top)-QK^\top,\\ 
M_i^{\text{neg}} = QK^\top - \mbox{min}(QK^\top),
\label{eq: cross-attention modulation}
\end{gathered}
\end{equation}
\begin{equation}
\centering
\begin{gathered}
{R}_{i}^{\text{cross}}[x,y] = \Bigl\{
    \begin{array}{ll}
    {m}_{i,k}, & \text{if } y \in {\tau_k}\\
    {0}, & \text{otherwise}
    \end{array},
\label{eq: cross-attention qk map}
\end{gathered}
\end{equation}
where $x$ and $y$ are the query and key indices, and ${R}_{i}^{\text{cross}}$ is the query-key condition map in the cross attention layer.
We regularize this condition map by initially broadcasting each region's mask ${m}_{i,k}$ to its corresponding text key embedding $K_{\tau_{k}}$, resulting in a condition map 
${R}_{i}^{\text{cross}}\in \mathbb{R}^{(H\times W)\times L}$.
Each sub-region intensity then adjusts gradually in the generation process.
We set $M_i^\text{pos/neg}$ based on the gap between max/min values and the original scores, to keep modulated values within the original range. 
Our modulation is applied to all frames to achieve spatial-temporal region control.


As shown in Fig.~\ref{fig: framework} (mid right), after adding positive and subtracting negative values, the original cross-attn weight of ``Spiderman" (e.g., $p$) is amplified and focused on the left man. While the distract weight of ``polar" ``bear" become concentrated on the right man. These indicate our modulation redistributes each prompt's weight align with target areas, enabling precise text-to-region control.

\textbf{Modulate Self-Attention to Keep Feature Separation.}
To adapt the T2I model for T2V editing, we treat the full video as "a larger picture," replacing spatial attention with spatial-temporal self-attention while retaining the pretrained weights. 
%
This enhances cross-frame interaction and provides a broader visual context.
%
However, naive self-attention can cause regions to attend to irrelevant or similar areas (e.g., Fig.~\ref{fig: framework} bottom, before modulation query $p$ attend to two-man), which leads to mixed texture. 
%
To address this, we need to strengthen positive focus within the same region and restrict negative interactions between different regions.

As shown in Fig.~\ref{fig: framework} (bottom left), the maximum cross-frame diffusion feature indicates the strongest response among tokens within the same region. 
%
Note that DIFT \citep{tang2023emergent} uses this to match different images, while we focus on cross-frame correspondences and intra-region attention modulation in the generation process. 
%
Nevertheless, negative inter-region correspondence is equally crucial for decoupling feature mixing. 
%
Beyond DIFT, we find that the minimum cross-frame diffusion feature similarity effectively captures the relations between tokens across different regions.
%
Therefore, we define the spatial-temporal positive/negative values as:
\begin{equation}
\begin{gathered}
    M_i^{\text{pos}} = \mbox{max}({Q_i} [{K}_{1},\cdots,{K}_{n}]^\top)- {Q_i} {[{K}_{1},\cdots,{K}_{n}]^\top)},\\
    M_i^{\text{neg}} =  {Q_i} {[{K}_{1},\cdots ,{K}_{n}]^\top} - \mbox{min}({Q_i} {[{K}_{1},\cdots,{K}_{n}]}^\top).
\end{gathered}
\end{equation}
To ensure each patch attends to intra-regions feature while avoiding interaction in inter-regions feature. We define the spatial-temporal query-key condition map:
\begin{equation} 
\centering
\begin{gathered}
{R}_i^{\text{self}}[x,y]= \left\{
    \begin{array}{ll}
        0,  \forall j \in [1:N], \text{if }   {m}_{i,k}[x] \neq {m}_{j,k}[y] \\
        1,  \text{otherwise} \\
    \end{array}. \right.
\end{gathered}
\label{eq:am_self_time}
\end{equation}
For frame indices $i$ and $j$, the value is zero when tokens belong to different instances across frames.

As shown in the right part of Fig.~\ref{fig: framework} bottom, after applying our self-attention modulation, the query feature from the left man's nose (e.g., $p$) attends only to the left instance, avoiding distraction to the right instance. This demonstrates that our self-attention modulation breaks the diffusion model's class-level feature correspondence, ensuring feature separation at the instance level.

% \vspace{-8mm}
\section{Experiments}
% \vspace{-4mm}
\subsection{Experimental Settings}
% \vspace{-4mm}
In the experiment, we adopt the pretrained Stable Diffusion v1.5 as the base model, using 50 steps of DDIM inversion and denoising. 
Our VideoGrain operates in a zero-shot manner, requiring no additional parameter tuning.
%
To enhance memory efficiency, we re-engineer slice attention within our ST Layout Attn.
%
ST Layout Attn is applied during the first 15 denoising steps. We set $\xi(t) = 0.3 \cdot t^5$ for self-attention and $\xi(t) = t^5$ for cross-attention, where the timestep $t \in [0, 1]$ is normalized.
%
All The experiments are conducted on an NVIDIA A40 GPU.
%
We evaluate our VideoGrain using a dataset of 76 video-text pairs, including videos from DAVIS \citep{perazzi2016benchmark}, TGVE\footnote{\scriptsize{\url{https://sites.google.com/view/loveucvpr23/track4}}}, and the Internet\footnote{\scriptsize{\url{https://www.istockphoto.com/}} and \scriptsize{\url{https://www.pexels.com/}}}
, with 16-32 frames per video. 
%
% We perform K-Means over the self-attention feature in the inversion process to get the background layout conditions. Since the diffusion model can not discriminate same-class instances, we further use SAM-Track \citep{cheng2023segment} to separate them.
Four automatic metrics are employed for evaluation: CLIP-T, CLIP-F, Warp-Err, and Q-edit, following \citep{wu2022tune, cong2023flatten}. All metrics are scaled by 100 for clarity.
%
For baselines, we compare against T2I-based methods, including FateZero \citep{qi2023fatezero}, ControlVideo \citep{zhang2023controlvideo}, TokenFlow \citep{geyer2023tokenflow}, GroundVideo \citep{jeong2023ground} and T2V-based DMT \citep{yatim2024space}. 
% For all of these baseline methods, we follow the default settings from their official GitHub
% repositories.
To ensure temporal consistency, we employ FLATTEN \citep{cong2023flatten} and PnP \citep {tumanyan2023plug}.
For fairness, all T2I baselines are equipped with the same ControlNet conditions.
% More detailed experimental settings are provided in the Appendix.


\begin{figure*}[t]
  \includegraphics[width=\linewidth]{figure/main-results.pdf}
  \vspace{-6mm}
  \caption{Qualitative results. VideoGrain achieves multi-grained video editing, including class-level, instance-level, and part-level. We refer the reader to our \href{https://knightyxp.github.io/VideoGrain_project_page/}{project page} for full-video results.}
  \vspace{-6mm}
 \label {main-results}
\end{figure*}

%Qualitative results of VideoGrain. Our method is versatile for general objects like animals, cars, and humans while also supporting instance identity swapping and part-level editing.

\subsection{Results}
We evaluate VideoGrain on videos covering class-level, instance-level, and part-level edits. Our method demonstrates versatility in handling animals, such as transforming a ``wolf" into a ``pig" (Fig.~\ref{main-results}, top left). For instance-level editing, we can modify vehicles separately (e.g., transforming an ``SUV" into a ``firetruck" and a ``van" into a ``school bus") in Fig.~\ref{main-results}, top right.
%
VideoGrain excels at editing multiple instances in complex, occluded scenes, like ``Spider-Man and Wonder Woman playing badminton'' (Fig.~\ref{main-results}, middle left). Previous methods often struggle with such non-rigid motion.
In addition, our method is capable of multi-region editing, where both foreground and background are edited, as shown in the soap-box scene, where the background changes to ``a mossy stone bridge over a lake in the forest" (Fig.~\ref{main-results}, middle right).
%
Thanks to precise attention weight distribution, we can swap identities seamlessly, such as in the jogging scene, where ``Iron Man" and ``Spider-Man" swap identities (Fig.~\ref{main-results}, bottom left). For part-level edits, VideoGrain excels in adjusting a character to wear a Superman suit while keeping sunglasses intact (Fig.~\ref{main-results}, bottom right).
%
Overall, for multi-grained editing, our VideoGrain demonstrates outstanding performance.
% We evaluate VideoGrain on various videos with prompts encompassing class-level, instance-level, and part-level edits. VideoGrain is versatile in handling diverse objects such as animals (e.g.,  a ``wolf" into a ``pig").
% For instance-level editing, our method can edit separate vehicles (e.g., editing an ``SUV" into a ``firetruck" and a ``van" into a ``school bus").
% VideoGrain can accurately edit separated objects, even in complex motion scenarios like playing badminton with strong occlusions, where previous methods often struggle with such nonrigid motions. 
% %
% Additionally, VideoGrain is capable of multi-region editing, including each instance in the foreground and background, such as in the soap-box scene where the background is changed to ``a mossy stone bridge over a lake in the forest." 

% Since our method enables precise attention weight distribution, it can successfully swap identities, making it robust in scenarios that require fine control over multiple objects. 
% %
% For part-level editing, VideoGrain excels in part-level adjustments, such as modifying a character to wear a Superman suit while keeping sunglasses intact. 
% %
% For all these multi-grained editing tasks, our VideoGrain demonstrates outstanding performance, effectively preserving temporal consistency and achieving high-quality visual results.

\subsection{Qualitative and Quantitative Comparisons}

\begin{figure*}[t!]
  \centering
  \includegraphics[width=\linewidth]{figure/comparison.pdf}
  \vspace{-4mm}
  \caption{Qualitative comparisons.
  We refer the reader to our \href{https://knightyxp.github.io/VideoGrain_project_page/}{project page} for detailed assessment.}
  \vspace{-4mm}
  \label {comparison}
\end{figure*}

\textbf{Qualitative Comparison.}
Figure \ref{comparison} shows a comparison between VideoGrain and baseline methods, including T2I-based and T2V-based approaches, for instance-level and part-level editing. For fairness, all T2I-based methods use ControlNet conditioning.
\textbf{(1) Animal instances}: In the left column, T2I-based methods like FateZero, ControlVideo, and TokenFlow edit both cats into pandas due to same-class feature coupling in diffusion models, failing to perform separate edits. DMT, even with video generation priors, still blends the panda and toy poodle features. In contrast, VideoGrain successfully edits one into a panda and the other into a toy poodle.
\textbf{(2) Human instances}: In the middle column, baselines struggle with same-class feature coupling, partially editing both men into Iron Man. DMT and Ground-A-Video also fail to follow user intent, incorrectly editing the left and right instances. VideoGrain, however, correctly transforms the right man into a monkey, breaking the human-class limitation.
\textbf{(3) Part-level editing}: In the third column, VideoGrain manages part-level edits, such as sunglasses and boxing gloves. ControlVideo edits the gloves but struggles with sunglasses and motion consistency. TokenFlow and DMT edit the sunglasses but fail to modify the gloves or background. In comparison, VideoGrain achieves both instance-level and part-level edits, significantly outperforming previous methods.



%% Since it struggles with local details preservation within the bounding box and lacks negative example awareness for adjacent different atttributes weights, which should be mutually exclusive. 

\begin{table}
\centering
\resizebox{1.00\columnwidth}{!}{
\begin{tabular}{c||cccc||ccc}
\hline\thickhline
\rowcolor{mygray}
 & \multicolumn{4}{c||}{\textbf{Automatic Metric}} & \multicolumn{3}{c}{\textbf{Human Evaluation}} \\
\rowcolor{mygray}
Method  & CLIP-F $\uparrow$ & CLIP-T  $\uparrow$ & Warp-Err $\downarrow$ & Q-edit $\uparrow$ & Edit-Acc $\uparrow$ & Temp-Con $\uparrow$ & Overall $\uparrow$  \\
 \hline\hline
FateZero & 95.75 & 33.78  & 3.08 & 10.96 & 59.8 & 78.6 &\cellcolor[HTML]{D0F0C0} 59.6  \\
ControlVideo & 97.71 & 34.41  & 4.73 & 7.27 & 53.2 & 50.0 &\cellcolor[HTML]{D0F0C0} 43.6 \\
TokenFlow & 96.48  & 34.59  & 2.82 & 12.28 & 45.4 & 50.4 &\cellcolor[HTML]{D0F0C0} 39.8 \\
Ground-A-Video & 95.17 & 35.09  & 4.43 & 7.92 & 69.0 & 72.0 &\cellcolor[HTML]{D0F0C0} 63.2 \\
DMT & 96.34 & 34.09  & 2.05  & 16.63  & 58.7  & 79.4 &\cellcolor[HTML]{D0F0C0} 64.5 \\
\textbf{VideoGrain(ours)} & \textbf{98.63} & \textbf{36.56}  & \textbf{1.42} & \textbf{25.75} & \textbf{88.4} & \textbf{85.0} & \cellcolor[HTML]{D0F0C0} \textbf{83.0} \\
\hline
\end{tabular}
}
\captionsetup{font=small}
\caption{\small Quantitative comparison of automatic metrics and human evaluation. The best results are \textbf{bolded}.}
\vspace{-10mm}
\label{table:comparison}
\end{table}

\noindent\textbf{Quantitative Comparison.}
We compare the performance of different methods using both automatic metrics and human evaluation. \textbf{CLIP-T} calculates the average cosine similarity between the input prompt and all video frames, while \textbf{CLIP-F} measures the average cosine similarity between consecutive frames. Additionally, \textbf{Warp-Err} captures pixel-level differences by warping the edited video frames according to the optical flow of the source video, extracted using RAFT-Large \citep{teed2020raft}.
To provide a more comprehensive measure of video editing quality, we follow \citep{cong2023flatten} and use \textbf{Q-edit}, defined as $\text{CLIP-T} {/}\text{Warp-Err}$. For clarity, we scale all automatic metrics by 100.
In terms of human evaluation, we assess three key aspects: \textbf{Edit-Accuracy} (whether each local edit is accurately applied), \textbf{Temporal Consistency} (evaluated by participants for coherence between video frames), and \textbf{Overall Edit Quality}. We invited 20 participants to rate 76 video-text pairs on a scale of 20 to 100 across these three criteria, following \citep{jeong2023ground}.
As demonstrated in Table \ref{table:comparison}, VideoGrain consistently outperforms both T2I- and T2V-based methods. This is primarily due to ST-Layout Attn’s precise text-to-region control and maintaining feature separation between regions. As a result, our method achieves significantly higher CLIP-T and Edit-Accuracy scores compared to other baselines. The improved Warp-Err and Temporal Consistency metrics further indicate that VideoGrain delivers temporally coherent video edits.

\textbf{Efficiency Comparison.}
To evaluate efficiency, we compared baselines with VideoGrain on a single A6000 GPU for editing 16 video frames. The metrics include editing time (time taken to perform one edit) and both GPU and CPU memory usage. From Tab.~\ref{tab:efficiency}, it is clear our method achieves the fastest editing time with the lowest memory usage, indicating its computational efficiency.

\begin{figure}[h]
    \vspace{2mm}
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \resizebox{1.0\linewidth}{!}{ % Adjust the scaling factor here
            \begin{tabular}{l||c|c|c}
                \hline\thickhline
                \rowcolor{mygray}
                & Time(min) $\downarrow$ & Memory (GB) $\downarrow$ & RAM (GB) $\downarrow$\\ 
                \hline
                \hline
                FateZero & 8.68 & 27.35 & 144.22 \\    
                ControlVideo & 4.41 & 16.15 & 7.03 \\
                TokenFlow & 4.56 & 17.84 & 5.35 \\
                Ground-A-Video & 5.81 & 17.31 & 9.96 \\
                DMT & 5.79 & 27.88 & 8.12 \\
                \textbf{VideoGrain} & \textbf{3.83} & \textbf{15.94} & \textbf{4.42} \\
                \hline
            \end{tabular}
        }
        \vspace{2mm}
        \captionof{table}{ Efficiency comparison.}
        \label{tab:efficiency}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \vspace{-4mm}
        \includegraphics[width=\textwidth]{figure/attention_weight_distribution.pdf}
        \vspace{-4mm}
        \captionof{figure}{Attention weight distribution.}
        \label{fig:attention_weight}
    \end{minipage}
\end{figure}

\vspace{-6mm}
\subsection{Ablation Study}
To assess the contributions of different components in our proposed ST-Layout Attn, we first evaluate whether our attention can achieve attention weight distribution, then decouple the self-attention modulation and cross-attention modulation to evaluate their individual effectiveness.

\textbf{Attention Weight Distribution.}
We evaluate the impact of ST-Layout Attn on attention weight distribution. As shown in Fig.~\ref{fig:attention_weight}, the target prompt is ``An Iron Man is playing tennis on a snow court.'' We visualize the cross-attention map for ``man'' to assess weight distribution. Without ST-Layout Attn, feature mixing occurs, with ``snow'' weight spilling onto ``Iron Man.'' With ST-Layout Attn, the man's weight is correctly distributed. This is because we enhance positive pair scores and suppress negative pairs in both cross- and self-attention. This enables precise, separate edits for ``Iron Man" and ``snow." Additional visualizations are in the Appendix.

\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{figure/ablation.pdf}
  \vspace{-6mm}
  \caption{Ablation of cross- and self-modulation in ST-Layout Attn.}
  \label {ablation-st}
\end{figure*}
\vspace{-4mm}
\begin{table}
\centering
\small
\resizebox{0.9\columnwidth}{!}{
\begin{tabular}{c||ccccc}
\hline\thickhline
\rowcolor{mygray}
Method   & CLIP-F $\uparrow$ & CLIP-T  $\uparrow$ & Warp-Err $\downarrow$ & $\text{Q}_{edit}$ $\uparrow$  \\
 \hline\hline
Baseline & 95.21 & 33.59  &3.86 &\cellcolor[HTML]{D0F0C0} 8.70  \\
Baseline + Cross Modulation & 96.28  & 36.09  & 2.53 &\cellcolor[HTML]{D0F0C0} 14.26 \\
\textbf{Baseline + Cross Modulation + Self Modulation}& \textbf{98.63} & \textbf{36.56}  & \textbf{1.42} & \cellcolor[HTML]{D0F0C0} \textbf{25.75} \\
\hline
\end{tabular}
}
\vspace{2mm}
\caption{Quantitative ablation of  cross- and self-modulation in ST-Layout Attn.}
\vspace{-8mm}
\label{table:ablation}
\end{table}

\textbf{Cross-Attention Modulation.} In Fig.~\ref{ablation-st} and Tab.~\ref{table:ablation}, we illustrate video editing results under different set up: (1) Baseline (2) Baseline + Cross-Attn Modulation (3) Baseline + Cross-Attn Modulation + Self-Attn Modulation.
As shown in Fig.~\ref{ablation-st} top right, direct editing fails to discriminate between the left and right instances, leading to incorrect (left) or no edits(right). 
However, when equipped with cross-attention modulation, we achieve accurate text-to-region control, thereby editing left man to ``Iron Man" and right man to ``Spiderman" separately. The quantitative results in Tab.~\ref{table:ablation} indicate that with cross-attention modulation (second row), CLIP-T increases by 7.4\%, and Q-edit increases by 63.9\%. This demonstrates the effectiveness of our cross-attention modulation.

\textbf{Self-Attention Modulation.}
However, modulating only cross-attention still leads to structure distortions, such as the \textcolor[HTML]{cc0000}{spider web} appearing on the left man. This is caused by the coupling of same class-level features (e.g., human). When using our self-attention modulation, the feature mixing is significantly reduced, and the left man retains unique object features. This is achieved by decreasing the negative pair scores between different instances, while increasing positive scores within the same instance. As a result, more part-level details, such as the distinctive \textcolor[HTML]{336600}{blue sides}, are generated in the optimized areas. The quantitative decrease in Warp-Err by 43.9\% and increase in Q-edit by 80.6\% in Tab.~\ref{table:ablation} further prove the effectiveness of self-attention modulation.


\section{Conclusion}
In this paper, we aim to solve the problem of multi-grained video editing, which includes both class-level, instance-level and part-level video editing. To the best of our knowledge, this is the first attempt at this task.
In this task, we find that the key problem is that the diffusion model views different instances as same-class features and direct global editing will mix different local regions.
To wrestle with these problems, we propose VideoGrain to modulate spatial-temporal cross- and self-attention for text-to-region control while keeping feature separation between regions.
% We model the self-and cross-attention modulation in a unified increase positive pairs and decrease negative ones manner.  
In cross-attention, we enhance each local prompt's focus on its corresponding spatial-disentangled region while suppressing attention to irrelevant areas, thereby enabling text-to-region control.
In self-attention, we increase intra-region awareness and reduce inter-region interactions to keep feature separation between regions.
Extensive experiments demonstrate that our VideoGrain surpasses previous video editing methods on both class-level, instance-level, and part-level video editing.

\section{Ethics statement}
This project aims to solve multi-grained video editing. However, the potential misuse of this technology, such as the creation of deceptive videos by altering identities, poses a risk. Strategies like incorporating invisible watermarking could be explored to ensure videos are not used maliciously.
% This project addresses multi-grained video editing but acknowledges the risk of misuse for creating deceptive content, which could be mitigated by incorporating invisible watermarking.

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix

\vspace{-10mm}

Different from multi-modal learning \citep{yang2024dgl,yang2024pre,yang2024dual,jia2024mos2}, controllable video generation \citep{ma2024followyouremoji,ma2024follow,lufreelong} or video editing \citep{yang2024eva,ma2023magicstick} requires explicit control signals. Multi-grained editing further relies on additional layout conditions to edit in the class, instance, or part level. Therefore, in the appendix, we first evaluate the SAM-Track masks' impact in Section \ref{sam-track mask}, then validate whether our method can work without SAM-Track masks in Section \ref{without sam-track}. Continually, we show that our method can solely edit specific subjects in Section \ref{solely edit} and part-level modification example in Section \ref{part modification}. We also evaluate our ST-Layout Attn's temporal focus in Section \ref{Temporal Focus} and ControlNet's effect in Section \ref{controlnet ab}. 

%Finally, we add more general object examples, more attention weight visualization, background preservation principles, experiment details in and limitation in Section \ref{more general} - Section \ref{limitations}

{\section{Evaluate SAM-Track masks' impact}\label{sam-track mask}}

\begin{figure*}[h]
 \vspace{-2mm}
  \centering
  \includegraphics[width=\linewidth]{figure/videop2p+sam_track.pdf}
  \caption{VideoP2P joint and sequential edit with SAM-Track masks}
  \label {videop2p}
\end{figure*}


To evaluate the impact of using SAM-Track~\citep{cheng2023segment} for instance segmentation, we compare our VideoGrain against VideoP2P~\citep{liu2024video}, which is equipped with SAM-Track instance masks.
The instance masks replace cross-attention masks during editing.
A 16-frame one-shot tuning is performed, and ControlNet conditioning~\cite{Zhang_2023_ICCV} is added for fairness. Two experiments are tested: (1) jointly editing multiple areas in a single denoising process and (2) sequentially editing three areas by inputting separate masks.


Results show that joint editing (Fig.~\ref{videop2p}(1)) modifies only left man into "Spiderman," leaving other areas unchanged due to inaccurate cross-attn weight distribution. Sequential editing (Fig.~\ref{videop2p}(2)) succeeds initially but fails later due to error accumulation in denoising, resulting in blurred details. 


\begin{figure*}[h]
 \vspace{-2mm}
  \centering
  \includegraphics[width=\linewidth]{figure/gav_supp.pdf}
  \caption{Ground-A-Video joint edit with instance information}
  \label {gav_supp}
\end{figure*}

Additionally, as shown in figure above~\ref{gav_supp}, also in Figs~\ref{fig: intro} and~\ref{comparison}, Ground-A-Video~\citep{jeong2023ground} struggles with multi-grained video editing tasks, even with instance-level grounding information (e.g., text-to-bounding box), which is comparable to SAM-Track's masks.



These comparisons indicate that while SAM-Track provides layout guidance, it does not guarantee successful edits. In contrast, our method enables zero-shot multi-grained editing, which was not achievable by any previous methods, even when providing existing SOTA with SAM-Track masks.


\section{VideoGrain can work without SAM-Track masks \label{without sam-track}}
\begin{figure*}[h]
 \vspace{-2mm}
  \centering
  \includegraphics[width=\linewidth]{figure/cluster_mask_supp.pdf}
  \caption{Our method without additional SAM-Track masks}
  \label {cluster}
\end{figure*}
Our method is not strictly dependent on SAM-Track masks. As shown in Fig.\ref{cluster}(3), we can cluster DDIM inversion self-attention features to get inaccurate coarse layouts.
Our method still achieves high-quality multi-area editing results (4). 
In contrast, even with precise groundings (converted from SAM-Track masks in (1)), Ground-A-Video fails to edit all three regions. These comparisons indicate that our method does not rely on SAM-Track segmentation. Instead, it works effectively only using the self-attention feature inside the diffusion model, even without accurate layout guidance.


\section{Solely edit on specific subjects, without background changed \label{solely edit}}
\begin{figure*}[h]
 \vspace{-2mm}
  \centering
  \includegraphics[width=\linewidth]{figure/solely_edit_supp.pdf}
  \caption{Soely edit on specific subjects, without background changed}
  \label {solely}
\end{figure*}
Our method is designed for multi-area editing and can naturally perform background-preserved subject editing, as it treats multi-area editing as selecting regions restricted to the foreground. As shown in Fig~\ref{solely}, our method can separately edit the ``left man" and ``right man" or jointly edit both subjects while keeping the background unchanged.


\section{Part-Level modification examples \label{part modification}}
\begin{figure*}[h]
 \vspace{-2mm}
  \centering
  \includegraphics[width=\linewidth]{figure/part_modify_supp.pdf}
  \caption{Part-level modifications on humans and animals}
  \label {part modify}
\end{figure*}
Our part-level editing supports not only adding objects but also part-level attribute modifications. In the human case (Fig.~\ref{part modify} left), our method changes the color of a gray shirt to blue (second row) and edits a half-sleeve shirt into a black suit (third row), showcasing part-level attribute and structure editing. Similarly, in the animal case, our method can change a cat’s head or body color from black to ginger while preserving the belt's color, demonstrating precise part-level modifications.


\section{Temporal Focus of ST-Layout Attn \label{Temporal Focus}}
\begin{figure*}[h]
 \vspace{-2mm}
  \centering
  \includegraphics[width=\linewidth]{figure/temporal_focus_supp.pdf}
  \caption{Temporal Focus of ST-Layout Attn}
  \label {temporal focus}
\end{figure*}
Our ST-Layout Attn is designed as a full-frame approach to ensure inter-frame consistency. As shown in Fig.~\ref{temporal focus}, per-frame ST-Layout Attn causes feature coupling on Iron Man, while the sparse-causal method results in flickering and misses Spider Man's blue details due to their limited receptive fields for positive/negative value selection across different layouts. In contrast, our ST-Layout Attn effectively preserves texture details and prevents flickering, achieving temporal consistent and layout unified multi-grained video editing.


\section{ControlNet Ablation \label{controlnet ab}}
\begin{figure*}[h]
 \vspace{-2mm}
  \centering
  \includegraphics[width=\linewidth]{figure/controlnet_ab_supp.pdf}
  \caption{ControlNet ablation}
  \label {controlnet}
\end{figure*}
Our method utilizes ControlNet depth/pose conditioning in certain complex motion cases to provide necessary structural guidance.
As shown in Fig.~\ref{controlnet}, even without ControlNet, our method can still achieve simultaneous multi-region editing. However, in such cases, there may be some structural inconsistencies between the edit object and source object due to the lack of explicit structure guidance.
\vspace{-2mm}

\section{More general objects and shape editing \label{more general}}
\begin{figure*}[h]
 \vspace{-2mm}
  \centering
  \includegraphics[width=\linewidth]{figure/more-results.pdf}
  \caption{More general objects instance editing (animals) and shape editing (cars) results.}
  \label {more-general}
\end{figure*}

\section{More visualization \label{more visualization}}
\begin{figure*}[h]
 % \vspace{-5mm}
  \centering
  \includegraphics[width=\linewidth]{figure/attention_weight_v1.pdf}
  \caption{More frames ablation of ST-Layout Attn's effects on attention weight distribution.}
  \label {more-attn-weight}
\end{figure*}


% \subsection{More swap identity editing results}
% \begin{figure*}[h]
%  % \vspace{-5mm}
%   \centering
%   \includegraphics[width=\linewidth]{figure/more_swap_identity.pdf}
%   \caption{}
%   \label {more-swap-identity}
% \end{figure*}








\section{Latent Blend \label{Latent Blend}}
To preserve areas not intended for editing (i.e., $\tau_{3}$ in  $\Delta_\tau= \{\tau_{1}{\rightarrow}\tau_{1'},\tau_{2}{\rightarrow}\tau_{2'}, \tau_{3}{\rightarrow}\tau_{3}, \cdots\}$), we employ Latent Blend \citep{avrahami2022blended,avrahami2023blended}, which leverages masks to direct the model focus on areas requiring editing while keeping the background region identical to the source video.

For each frame $i$ in the video, we first merge each attribute mask to form the global foreground mask $M_{i}$ by applying the logical OR operation across all layouts masks  $m_{i,k}={[}m_{i,1}, m_{i,2},\cdots,m_{i,k}{]}$ :
\begin{equation}
M_{i} = m_{i,1} \lor  m_{i,2} \lor \cdots \lor m_{i,k}.
\end{equation}
We aggregate the masks $M_i$ from all frames to obtain a combined mask $M$, and then blend the latent states $z_t$ at each timestep $t$ during the denoising process as follows:
\begin{equation}
    z_t = (1-\mathcal{M}) \cdot \tilde{z}_{t} + \mathcal{M}\cdot z_{t},
\label{latent blend}
\end{equation}
where $\tilde{z}_{t}$ indicates the latent feature in the DDIM inversion process and ${z}_{t}$ is corresponding latent feature during the DDIM denoising process. 

The key behind employing Latent Blend for preserving the background is that, given a desired area mask, the less noisy foreground latent can be guided by the target text prompt $\Delta_\tau$. Meanwhile, the latent features outside the mask (the background) can be preserved. This blending ensures that, even if the latent feature within the edit area is modified, the background features stay consistent.




% \subsection{Hyperparameters in ST-Layout Attn}
% As mentioned in formula \ref{eq:st-layout attn} in section \ref{sec: st-layout-attn}, we use $\lambda_t$ as a regularization parameter for timesteps in the ST-Layout Attn during the denoising process. Following \citep{densediffusion}, the scalar $\lambda_t$ is employed to adjust the intensity of the modulation function as follows:
% \begin{equation}
% \lambda_t = w \cdot t^p
% \label{eq:lambda}
% \end{equation}
% where $t \in [0, 1]$ represents the normalized timestep. For self- and cross-attention layers, we set the weights to $w^s=0.3$ and  $w^c=1$, following configuration in \citep{densediffusion}.

\section{Experimental Details \label{exp details}}
For FateZero\footnote{\scriptsize{\url{https://github.com/ChenyangQiQi/FateZero}}} \citep{qi2023fatezero}, we employ prompt-to-prompt\citep{hertz2022prompt} replace editing. To enhance the identity binding of the edited object, we set the self/cross replacement steps at 0.3 and the blending threshold at 0.7. In TokenFlow\footnote{\scriptsize{\url{https://github.com/omerbt/TokenFlow}}} \citep{geyer2023tokenflow}, we utilize SD editing and default to 4 keyframes for 16-frame videos. For other comparative methods like ControlVideo\footnote{\scriptsize{\url{https://github.com/YBYBZhang/ControlVideo}}} \citep{zhang2023controlvideo} and Ground-A-Video\footnote{\scriptsize{\url{https://github.com/Ground-A-Video/Ground-A-Video}}} \citep{jeong2023ground} and DMT\footnote{\scriptsize{\url{https://github.com/diffusion-motion-transfer/diffusion-motion-transfer}}} \citep{yatim2024space}, we adhere to their default hyperparameter settings. To ensure fairness across all T2I-based methods compared, we re-implement ControlNet \citep{Zhang_2023_ICCV} on their codebases. 

\section{Limitations. \label{limitations}} First, although our method can achieve multi-grained editing of video, the generation quality is still limited by the base model since we are a training-free method. In scenarios where the generation prior to SD is not ideal, artifacts may occur in the editing results. Second, since our method is based on a T2I model, it struggles with large shape deformations and significant appearance changes. This limitation is inherent in zero-shot methods. A potential future direction is to incorporate motion priors from T2V generation models \citep{yang2024cogvideox} to handle such challenges.

\iffalse
\section{More qualitative results}
We further present additional qualitative results:
(1) Swap identity results in multi-object scenes are showcased in Figure \ref{swap}.
(2) For multi-object scenarios, comparative editing results from different methods are displayed in Figures \ref{multi more compare 1} and \ref{multi more compare 2}.
(3) Additionally, a qualitative comparison of editing results from various methods in single-object scenarios is illustrated in Figure \ref{single more compare 1}.
(4)  More examples of temporal consistent editing results in single-object scenarios are provided in Figure \ref{VideoGrain more single static}.


\begin{figure}[t]
 \vspace{-5mm}
  \centering
  \includegraphics[height=5cm]{figure/limitation.pdf}
  \vspace{-15mm}
  \caption{Human pose misalignment}
  \vspace{-8mm}
  \label {limitation}
\end{figure}


\section{User study details}
\vspace{-5mm}
We invited 20 participants to score the editing results of 78 text-video pairs, utilizing 5 different methods, including FateZero, ControlVideo, TokenFlow, Ground-A-Video and VideoGrain. 
As depicted in Figure \ref{user-study}, we randomized the sequence of videos for each question. Participants were asked to rate each video on four aspects: 1) Subject editing accuracy, 2) Layout preservation, 3) Motion alignment, and 4) Overall performance. Considering the labor-intensive task of scoring 5 videos simultaneously, we concatenated the editing results of three text prompts for a single source video into one composite video. Finally, we calculated the average score for each method on 4 metrics. The user study results, shown in Table \textcolor{red}{2}, underscore the advantages of our method.
\vspace{-5mm}
\section{Limitation}
\vspace{-4mm}
\noindent\textbf{Depth-Absent Pose Discrepancy} 
The absence of depth information in 2D poses leads to discrepancies in limb positioning, particularly evident in side views. Such limitations cause misplacements of limbs—like the arms or legs—regarding their foreground or background arrangement as shown in Fig \ref{limitation}, where the edited video showcases an inconsistency in the positioning of the left person's legs compared to the source video. This issue arises due to the 2D pose's inability to convey depth, resulting in incorrect foreground-background sequencing of limbs.
Incorporating 3D human keypoints as priors could potentially enhance the editing outcomes by providing depth information.

\noindent\textbf{Flickering in Overlapping Poses:} Flickering may occur in multi-object scenarios or when individual body key points overlap significantly. This issue is partly due to the nature of the ControlNet \citep{Zhang_2023_ICCV} pretraining data, which predominantly comprises high-resolution facial images and lacks examples of these extreme cases. Such scenarios include side views of a single person where key points may coincide or instances where the poses of two individuals overlap extensively. Addressing the diversity of situations in the pretraining phase of ControlNet could help mitigate flickering in complex motion videos, especially in conditions of significant overlap between objects or key points.


\vspace{-5mm}
\section{Negative impact}
\vspace{-4mm}
This project is dedicated to providing the community with an efficient method for multi-grained editing and multi-object editing in videos with complex motion. However, there exists a potential risk that malicious entities might exploit this framework to create deceptive videos, particularly by altering people's identities to mislead the public. This concern, while not unique to our approach, is a common challenge across various video editing methods. One possible strategy to mitigate such risks involves incorporating invisible watermarking into the generated videos. This addition aims to deter misuse and ensure that the videos cannot be employed deceptively without proper attribution.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figure/more_swap_identity.pdf}
  \caption{More swap identity editing result for multi-object scenario}
  \vspace{-5mm}
  \label {swap}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figure/multi-more-compare.pdf}
  \caption{More qualitative comparison for multi-object and background Editing}
  \vspace{-5mm}
  \label {multi more compare 1}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figure/multi-more-compare-2.pdf}
  \caption{Additional qualitative comparison for multi-object and background editing}
  \vspace{-5mm}
  \label {multi more compare 2}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figure/single-more-compare.pdf}
  \caption{More qualitative comparison for single-Object and background Editing }
  \vspace{-5mm}
  \label {single more compare 1}
\end{figure}


% \begin{figure*}[h!]
%   \centering
%   \vspace{-10mm}
%   \includegraphics[width=\linewidth]{figure/more-result-multi.png}
%   \caption{More Qualitative Result for Multi-Object and Background Editing }
%   \vspace{-5mm}
%   \label {7}
% \end{figure*}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figure/more-single-results.pdf}
  \caption{More qualitative results for single-object and background editing  }
  \vspace{-5mm}
  \label {VideoGrain more single static}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{figure/user-study-new.pdf}
   \vspace{-5mm}
  \caption{User study page}
  \label {user-study}
\end{figure}
\fi


\end{document}
