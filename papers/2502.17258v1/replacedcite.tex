\section{Related Work}
\subsection{Text-to-Image Editing/Generation}

In the realm of single attribute text-to-image editing, various approaches have been explored, from manipulating attention maps in Pix2Pix-Zero ____ and Prompt2Prompt ____ to employing masks in DiffEdit ____ and Latent Blend ____ for foreground modifications while preserving the background. 

For multi-grained editing, efforts like Attention and Excite ____ and DPL ____ focus on maximizing attention scores for each subject token and reducing attention leakage. In image generation, ____ modulates attention based on layout masks and dense captions, while ____ proposed an attention refocus loss for regularization. However, using single-frame layout masks and dense captioning alone is insufficient for video editing, as it fails to maintain the original videoâ€™s integrity and temporal consistency.

\subsection{Text-to-Video Editing}
\textbf{Video Editing based on Image Diffusion Models.}
Tune-A-Video (TAV) ____ is the first work to extend latent diffusion models to the spatial-temporal domain and encode the source motion implicitly by one-shot tuning but still fails to preserve local details. Fatezero ____ and Pix2Video ____ fuse self- or cross-attention maps in the inversion process for temporal consistency. 
However, ____ requires extensive RAM usage and suffers from layout preservation even when equipping TAV for local object editing. 
%up to 200-300GB for a 16-frame video, due to maintaining each frame's inversion 
____ and ____, following the Neural Atlas ____ or dynamic Nerf's deformation field ____, struggle with non-grid human motion. 
Subsequent methods like Rerender-A-Video ____, FLATTEN ____ ControlVideo ____ achieve strict temporal consistency via optical-flow, depth/edge maps, but failed in multi-grained editing while preserving original layouts. Tokenflow ____ enforces a linear mix of nearest key-frame features to ensure consistency but results in detail loss. Ground-A-Video ____ leverages groundings for multi-grained editing, but it suffers from feature mixing when bounding boxes overlap.

\noindent\textbf{Video Editing based on Video Diffusion Models.}
Previous video editing work primarily utilized text-to-image SD model ____. 
Recent advancements in video foundation models ____ have led efforts like VideoSwap ____ to employ temporal priors for customized motion transfer or motion editing ____.
Yet, current video foundation models are limited to fixed views and struggle with non-grid human motions. Additionally, these editing methods require tuning parameters, which poses a challenge for real-time video editing applications. In contrast, our VideoGrain method requires no parameter tuning, enabling zero-shot, multi-grained video editing.

\iffalse