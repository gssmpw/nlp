\section{Related Work}
\subsection{Text-to-Image Editing/Generation}

In the realm of single attribute text-to-image editing, various approaches have been explored, from manipulating attention maps in Pix2Pix-Zero \citep{parmar2023zero} and Prompt2Prompt \citep{hertz2022prompt} to employing masks in DiffEdit \citep{couairon2023diffedit} and Latent Blend \citep{avrahami2022blended,avrahami2023blended} for foreground modifications while preserving the background. 

For multi-grained editing, efforts like Attention and Excite \citep{chefer2023attend} and DPL \citep{wang2023dynamic} focus on maximizing attention scores for each subject token and reducing attention leakage. In image generation, \citep{densediffusion} modulates attention based on layout masks and dense captions, while \citep{phung2023grounded} proposed an attention refocus loss for regularization. However, using single-frame layout masks and dense captioning alone is insufficient for video editing, as it fails to maintain the original videoâ€™s integrity and temporal consistency.

\subsection{Text-to-Video Editing}
\textbf{Video Editing based on Image Diffusion Models.}
Tune-A-Video (TAV) \citep{wu2022tune} is the first work to extend latent diffusion models to the spatial-temporal domain and encode the source motion implicitly by one-shot tuning but still fails to preserve local details. Fatezero \citep{qi2023fatezero} and Pix2Video \citep{ceylan2023pix2video} fuse self- or cross-attention maps in the inversion process for temporal consistency. 
However, \citep{qi2023fatezero} requires extensive RAM usage and suffers from layout preservation even when equipping TAV for local object editing. 
%up to 200-300GB for a 16-frame video, due to maintaining each frame's inversion 
\citep{chai2023stablevideo} and \citep{ouyang2023codef}, following the Neural Atlas \citep{kasten2021layered} or dynamic Nerf's deformation field \citep{pumarola2021d}, struggle with non-grid human motion. 
Subsequent methods like Rerender-A-Video \citep{yang2023rerender}, FLATTEN \citep{cong2023flatten} ControlVideo \citep{zhang2023controlvideo} achieve strict temporal consistency via optical-flow, depth/edge maps, but failed in multi-grained editing while preserving original layouts. Tokenflow \citep{geyer2023tokenflow} enforces a linear mix of nearest key-frame features to ensure consistency but results in detail loss. Ground-A-Video \citep{jeong2023ground} leverages groundings for multi-grained editing, but it suffers from feature mixing when bounding boxes overlap.

\noindent\textbf{Video Editing based on Video Diffusion Models.}
Previous video editing work primarily utilized text-to-image SD model \citep{rombach2022high}. 
Recent advancements in video foundation models \citep{yu2023magvit, guo2023animatediff, wang2023modelscope, yang2024cogvideox} have led efforts like VideoSwap \citep{gu2023videoswap} to employ temporal priors for customized motion transfer or motion editing \citep{mou2025revideo}.
Yet, current video foundation models are limited to fixed views and struggle with non-grid human motions. Additionally, these editing methods require tuning parameters, which poses a challenge for real-time video editing applications. In contrast, our VideoGrain method requires no parameter tuning, enabling zero-shot, multi-grained video editing.

\iffalse