\section{Reference Work}
\label{sec:reference}

In this section, we  introduce the reference work~\cite{CHEN2022102831} as follows. We begin with an overview of the proposed method. Then, we delve into their experiments with an analysis of their results. Lastly, we describe the potential threats to validity we identified, which prompted this replication and extension study.



\subsection{Proposed Method}


\begin{figure}[!t]
  \centering
  \includegraphics[width=0.9\columnwidth]{images/reference_approach.png}
  
  \caption{Method proposed by the reference work (picture taken from the paper by Chen et al.~\cite{CHEN2022102831})} \label{fig:reference_method}
\end{figure}


As depicted in Figure~\ref{fig:reference_method}, the authors of the reference work introduce a two-stage method, encompassing detection and escape phases. The detection phase involves utilizing an XSS detector, where the input undergoes preprocessing before being fed into the detector. Preprocessing comprises several steps: Positive examples, containing XSS attacks, are de-obfuscated and converted to lowercase. The URL is standardized to `http://', and special characters such as angular brackets in `$<$br$>$' are removed. Tokenization is then applied to the examples using the rules outlined in Table~\ref{tab:tokenization_reference}.


\begin{table}[!h]
    \caption{Tokenization Rules}
    \centering
    \scalebox{1.0}{
        \begin{tabular}{l|l}
        \toprule
        Regular Expression & Object \\
        \midrule
        
        (?x)[\textbackslash w\textbackslash.]+?\textbackslash( &   Javascript function\\
        ''\textbackslash w+?'' &  Content within double quotes\\
        '\textbackslash w+?' &  Content within single quotes\\
        http://\textbackslash w+ &  URLs\\
        $<$\textbackslash w+$>$ &  Opening tags \\
        $<$/\textbackslash w+$>$ &  Termination tags \\
        \textbackslash b\textbackslash w+= & Attributes \\
        (?$<$=\textbackslash ()\textbackslash S+(?=\textbackslash )) & Content within parentheses\\
        
        \bottomrule
        \end{tabular}
    }
    \label{tab:tokenization_reference}
\end{table}



Tokenization converts each input example into a sequence of tokens. The 10\% most frequent tokens are chosen for the vocabulary, while the remaining tokens are replaced with `None'. Subsequently, a Word2Vec model is trained, representing each token as a 32-dimensional vector. The length of each example is standardized to 200 words, discarding excess words and padding shorter examples with 0. The resulting vectors are then fed into the detection models, as shown in Figure~\ref{fig:reference_method}.



The escape phase involves crafting adversarial examples using RL. The idea is to train an agent to generate these examples effectively. The action space is carefully defined as a set of modification operations that can be applied to a malicious example while preserving its inherent characteristics. Table~\ref{tab:actions_reference} provides a comprehensive list of these possible actions.



\begin{table}[!h]
\caption{List of Actions} \label{tab:actions_reference}
\centering
\scalebox {0.7} {
\begin{tabular}{llll}
    \toprule
    
    A1) & Add " \&\#14" before "javascript" & A15) & Replace "(" and ") with "`" \\
    A2) & Mixed case HTML attributes & A16) & Encode data protocol with Base64 \\
    A3) & Replace spaces with ”/”, ”\%0A” or ”\%0D” & A17) & Remove the quotation marks \\
    A4) & Mixed case HTML tags & A18) & Unicode encoding for JS code \\
    A5) & Remove the closing symbol of the single tags & A19) & HTML entity encoding for ”javascript” \\
    A6) & Add ”\&NewLine;” to ”javascript” & A20) & Replace ”$>$” of single label with ”$<$” \\
    A7) & Add ”\&\#x09” to ”javascript” & A21) & Replace ”alert” with ”top[’al’ + ’ert’](1)” \\
    A8) & HTML entity encoding for JS code (hexadecimal) & A22) & Replace ”alert” with ”top[8680439..toString(30)](1)” \\
    A9) & Double write HTML tags & A23) & Add interference string before the example \\
    A10) & Replace ”http://” with ”//” & A24) & Add comment into tags \\
    A11) & HTML entity encoding for JS code (decimal) & A25) & ”vbscript” replaces ”javascript” \\
    A12) & Add ”\&colon;” to ”javascript” & A26) & Inject empty byte ”\%00” into tags \\
    A13) & Add ”\&Tab;” to ”javascript” & A27) & Replace ”alert” with ”top[/al/.source + /ert/. source](1)” \\
    A14) & Add string ”/drfv/” after the script tag & \\
    \bottomrule
\end{tabular}

}

\label{tab:actions}
\end{table}

The state space includes the historical record of actions taken by the agent. During each step, the agent chooses an action, updates its state accordingly, and then submits the transformed examples to the detection model. The agent receives a reward of 10 if the examples successfully evade detection, and a penalty of -1 if they are detected. This process continues iteratively until the agent either successfully bypasses the detection mechanism or reaches the maximum allowed number of steps.


\subsection{Experiments}

The training set for XSS detection models contains around 90,000 examples, collected from XSSed~\cite{kf} and Alexa~\cite{Cooper}.
This dataset is not publicly available and we had no way to craft it. To solve this problem, we used a publicly available dataset, as discussed in Section~\ref{sec:method}.
The authors of the reference work used the same dataset as \cite{wang2022black} to train the adversarial model.
They trained MLP, LSTM, and CNN as detectors, and they considered also two commercial XSS detection systems, named Safedog~\cite{Safedog} and XSSChop ~\cite{Chaitin}.

Several metrics were employed to assess the performance of the detectors: True Positive (TP) represents a correctly identified XSS example, False Positive (FP) indicates a benign example wrongly classified as malicious, True Negative (TN) denotes a correctly identified benign example, and False Negative (FN) represents an XSS example wrongly classified as benign. Then, some derived metrics are defined as follows: \textit{Accuracy} measures the proportion of correctly predicted examples (both malicious and benign) among the total. \textit{Precision} calculates the ratio of correctly predicted malicious examples to all predicted malicious examples. \textit{Recall} determines the ratio of correctly predicted malicious examples to all actual malicious examples. \textit{F1-Score} is the geometric mean of Precision and Recall, aiming for high values of both.






When evaluating a adversarial attack, the authors focused on detection and escape rates. \textit{Detection Rate} (DR) represents the ratio of malicious examples still detected by the XSS detection model, indicating the model's ability to defend against adversarial examples:
\begin{equation}
\label{eqn:6}
 DR = \frac{Number\ of\ malicious\ examples\ detected }{Total\ number\ of\ adversarial\ examples }
\end{equation}

\noindent
\textit{Escape Rate} (ER) refers to the percentage of malicious examples that go undetected and are recognized as benign by the detector:
\begin{equation}
\label{eqn:7}
 ER =\frac{Number\ of\ malicious\ examples\ undetected }{Total\ number\ of\ adversarial\ examples }
\end{equation}




\subsection{Results Reported in the Reference Work}

The performance of the  XSS detectors is reported in Table~\ref{tab:detectors_reference}, showing the usefulness of the considered models in detecting XSS attacks with almost perfect results.

\begin{table}[!h]
\caption{Performance of the XSS detection models considered in the reference work}
\label{tab:detectors_reference}
\centering
\scalebox {1.0} {
\begin{tabular}{c|cccc}
    \toprule
    Detector & Precision & Recall  & Accuracy & F1      \\
    \midrule
    MLP        & 99.92\%   & 98.00\% & 99.61\%  & 98.96\% \\
    LSTM       & 99.97\%   & 98.35\% & 99.65\%  & 99.06\% \\
    CNN        & 99.85\%   & 98.90\% & 99.76\%  & 99.38\% \\
    XSSChop    & 99.61\%   & 98.25\% & 99.14\%  & 98.93\% \\
    SafeDog    & 100.00\%  & 96.16\% & 98.47\%  & 98.05\% \\ 
    
    \bottomrule
\end{tabular}
}

\end{table}


Regarding the adversarial attacks, Chen et al.~\cite{CHEN2022102831} computed the ER against all the detectors, showing almost perfect ERs, as reported in Table~\ref{tab:adversarial_reference}. These results were the starting point for our replication study, which was initially triggered by the astonishingly high performance exhibited by the proposed RL-based attack generator. We wanted to understand in depth the reasons for such amazing success.

\begin{table}[!h]
\caption{Results of  adversarial attacks in the reference work}
\label{tab:adversarial_reference}
\centering
\scalebox {1.0} {
\begin{tabular}{c|c}
    \toprule
    Detector & Escape Rate (ER) \\
    \midrule
    MLP             & 99.73\%     \\
    LSTM            & 92.04\%     \\
    CNN             & 99.24\%     \\
    XSSChop         & 98.46\%     \\
    SafeDog         & 99.95\%     \\ 
    \bottomrule
\end{tabular}
}
\end{table}




\subsection{Identified Threats to Validity}


The first threat to validity stems from the lack of validation of adversarial examples and their properties. The authors did not employ any strategy to ensure that the applied transformations preserve the semantic integrity of the examples. This omission raises concerns about the validity of the modified payloads. The second issue is related to the preprocessing and vocabulary construction. The initial dataset lacks tokens produced by the proposed payload transformations, or in some cases such tokens are rare in the dataset, meaning that new tokens resulting from the RL agent's actions are likely to fall outside the top 10\% of considered tokens. Consequently, these tokens will be replaced with `None', causing semantic changes that may invalidate the attack. This issue could lead to an inflated Escape Rate (ER) due to the disruption of the payload's semantics during preprocessing, making it challenging to assess the models' true detection capabilities. The final threat concerns the unavailability of crucial components of the reference work. The code, datasets, and models are not accessible, hindering further analysis and replication of the reported results. This lack of transparency impedes the progress of research in this area, as it becomes difficult to build upon and extend the original findings. We have contacted the original authors asking them for code, datasets and models, but they never replied.

We summarize the three identified threats as follows:
\begin{itemize}
    \item \textbf{TH1. Lack of validation of the actions}: The lack of validation for the applied actions raises questions about the semantic validity of modified payloads, potentially affecting the integrity of the examples.
    
    \item \textbf{TH2. Lack of validation of the preprocessed payload}: The preprocessing pipeline requires validation to ensure that the preprocessed payload maintains its semantic validity, which is crucial for accurate evaluation.
    
    \item \textbf{TH3. Lack of reproducibility}: The unavailability of experimental details, including code, datasets, and models, hinders reproducibility and limits the ability to extend and build upon the research, impeding further advancements in the field.
\end{itemize}

