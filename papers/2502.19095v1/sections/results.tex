\section{Results}
\label{sec:results}


\subsection{RQ1 (Replication Study, Evaluation of TH3)}
\label{sec:rq1}

\begin{table}[!h]
\caption{Escape rate of the original adversarial agent, averaged across 10 training repetitions} \label{tab:rq1}
\centering

\begin{tabular}{ll}
    \toprule
    Detector & Escape Rate \\
    \midrule
    LSTM            & 98.62\%     \\
    MLP             & 99.73\%     \\
    CNN             & 98.25\%     \\
    \bottomrule
\end{tabular}



\end{table}

In this RQ, we mitigate TH3 by replicating the results of Chen et al.~\cite{CHEN2022102831}. We trained ten adversarial agents attacking each considered detection model, to deal with the non-determinism of the training process. Table~\ref{tab:rq1} reports the average of the escape rates obtained by the adversarial agents. These ERs are almost perfect, demonstrating a consistency with the reference work, despite variations in the dataset and training algorithm (see Section~\ref{sec:replication_study}). LSTM's ER is 6.58\%pt\footnote{Percentage points \%pt is the standard unit of measure for differences between percentages (e.g., 80\% is 100\%, or 40\%pt, higher than 40\%).} higher than the reference work, while the MLP and CNN results are identical and slightly lower (0.99\%pt), respectively. This consistency strongly suggests that the differences in the dataset and training algorithm did not significantly impact the overall outcome.



\begin{tcolorbox}[boxrule=0pt,frame hidden,sharp corners,enhanced,borderline north={1pt}{0pt}{black},borderline south={1pt}{0pt}{black},boxsep=2pt,left=2pt,right=2pt,top=2.5pt,bottom=2pt]
\textbf{Answer to RQ1}: Our replication study successfully reproduced the reference work's results, confirming the effectiveness of the proposed adversarial agents.
\end{tcolorbox}

Since we successfully replicated the original study, with negligible differences in the results despite the changed dataset and training algorithm, we proceeded to test our hypotheses, trying to explain the reasons for such an amazing performance. We conjectured that the adversarial agents are exploiting vulnerabilities arising from the lack of validation of the actions and due to out of vocabulary tokens produced by preprocessing, which would rendering the other aspects of the algorithm less significant. In fact, any algorithm whose actions produce ineffective or out of vocabulary payloads would achieve a high escape rate, without generating any meaningful attack.



\subsection{RQ2 (Evaluation of TH1)} \label{sec:rq2}


\begin{table}[!h]
\caption{Ruin rates of  set E, averaged across 10 repetitions} \label{tab:rq2}
\centering
\scalebox {1.0} {

\begin{tabular}{ll}
    \toprule
    Detector & $RR(E)$ \\
    \midrule
    LSTM            & 6.34\%     \\
    MLP             & 7.07\%     \\
    CNN             & 6.36\%     \\
    \bottomrule
\end{tabular}

}
\end{table}


We  investigate the impact of the lack of action validation by analyzing the ruin rates of the set $E$, which contains all the generated payloads that successfully escaped detection. Table~\ref{tab:rq2} presents the average ruin rates ($RRs$) for each detection model.

Ruin rates are relatively low, ranging from 6.34\% to 7.07\%, indicating that the sequence of actions occasionally disrupts the semantics of the attack. However, this frequency is not high enough to be considered a significant threat to validity.

\begin{tcolorbox}[boxrule=0pt,frame hidden,sharp corners,enhanced,borderline north={1pt}{0pt}{black},borderline south={1pt}{0pt}{black},boxsep=2pt,left=2pt,right=2pt,top=2.5pt,bottom=2pt]
\textbf{Answer to RQ2}: The lack of validation of the actions is not a severe threat to validity, but it warrants further investigation to improve the detection model's robustness.
\end{tcolorbox}

\subsection{RQ3 (Evaluation of TH2)}

\label{sec:rq3}
\begin{table}[!h]
\caption{Ruin and OOV rates of the array V, averaged across 10 repetitions}
\label{tab:rq3}
\centering
\scalebox {1.0} {
\begin{tabular}{lll}
    \toprule
    Detector & $RR(V)$ & $OR(V)$ \\
    \midrule
    LSTM            & 97.31\% & 47.49\%    \\
    MLP             & 97.84\% & 44.76\%  \\
    CNN             & 92.57\% & 43.85\%   \\
    \bottomrule
\end{tabular}
}
\end{table}

For each adversarial agent, we collected all generated payloads that  bypassed the detector, forming the set $E$. We then preprocessed this set to create the array $V$, and analyzed its ruin rate ($RR(V)$). The second column of Table~\ref{tab:rq3} presents the average ruin rates across ten adversarial agents for each detection model. Ruin rates after preprocessing are notably high (exceeding 90\% in all cases), indicating that  preprocessing significantly disrupts the semantics of the XSS attack, posing a concrete threat to the validity of the original empirical study.

To find further explanations, we considered the content of $V$ to assess whether our hypothesis about the introduction of `None' (causing OOV tokens) could be a contributing factor to the performance of the original adversarial agent. %
The third column of Table~\ref{tab:rq3} displays the average OOV rates across the ten adversarial agents for each detection model.  OOV rates are  significantly high, exceeding 40\% in all cases, which is notably higher than the OOV rates of non-mutated payloads (around 6\%). This confirms the threat to validity TH3 and suggests that preprocessing is one of the primary reasons for the agent's high escape rate, as adversarial agents learn to exploit OOV tokens as a \textit{shortcut} to escape detection, rather than generating valid XSS payloads that can bypass detection while retaining their semantic integrity after preprocessing.

It is important to notice that the attack described in the reference work remains effective and poses a risk. 
However, in the original setup the preprocessing step makes the attack successful for any detection model. Any attack that trivially introduces OOV tokens is effective by construction in such setup. However, a defender aware of the OOV token issue would implement an additional layer of protection to discard payloads with an unusually high number of OOV tokens, rendering the attack ineffective, which complicates the assessment of the attack's true effectiveness against a wide range of detectors and raise questions about the vulnerabilities of commercial systems like XSSChop or SafeDog to such attacks, because all these detection systems might be in principle unaware of the OOV token problem.

\begin{tcolorbox}[boxrule=0pt,frame hidden,sharp corners,enhanced,borderline north={1pt}{0pt}{black},borderline south={1pt}{0pt}{black},boxsep=2pt,left=2pt,right=2pt,top=2.5pt,bottom=2pt]

\textbf{Answer to RQ3}: The lack of validation of the preprocessed payload poses a concrete threat to validity. This threat arises from the high ruin and OOV rates observed, indicating that preprocessing disrupts the XSS payload semantics. Adversarial agents exploit this, learning to bypass detection without preserving the payload meaning. This highlights the need for improved payload validation techniques to mitigate the possibility of attack shortcuts due to preprocessing.
\end{tcolorbox}




\subsection{RQ4 (Extension Study)}
\label{sec:rq4}

\begin{table}[!h]
\caption{Escape rates of the adversarial agent with the Oracle included in the training process, averaged across 10 repetitions} \label{tab:rq4}
\centering
\scalebox {1.0} {
\begin{tabular}{ll}
    \toprule
    Detector & Escape Rate \\
    \midrule
    LSTM            & 98.13\%     \\
    MLP             & 97.37\%     \\
    CNN             & 96.89\%     \\
    \bottomrule
\end{tabular}
}
\end{table}


In this RQ, the training process for adversarial agents closely mirrors that used in RQ1 (Section~\ref{sec:rq1}), with a key difference: the Oracle is integrated to calculate a new reward function. The new reward is set to $-2$ if the mutated payload, after  preprocessing, is no longer recognized by the Oracle as an XSS attack. Otherwise, the reward is the same proposed in the reference work. Table~\ref{tab:rq4} reports the average escape rates achieved by the adversarial agent. Despite being very high, these rates are slightly lower than those obtained in RQ1 (see Table~\ref{tab:rq1}).


\begin{table}[!h]
\caption{Ruin and OOV rates of the array V, evaluated for RQ4 and averaged across 10 repetitions}
\label{tab:rq4-rr}
\centering
\scalebox {1.0} {
\begin{tabular}{lll}
    \toprule
    Detector & $RR(V)$ & $OR(V)$ \\
    \midrule
    LSTM            & 0.01\% & 1.73\%    \\
    MLP             & 0.11\% & 2.30\%  \\
    CNN             & 0.08\% & 1.90\%   \\
    \bottomrule
\end{tabular}
}
\end{table}


The second and third columns of Table~\ref{tab:rq4-rr} report the average ruin rates ($RR$) and out-of-vocabulary (OOV) rates ($OR$) across the ten adversarial agents for each detection model. The low values of $RR$ and $OR$ indicate that the integration of the Oracle in the training process effectively mitigates TH3. These results demonstrate that it is feasible to train adversarial agents capable of attacking XSS detection models as proposed in the reference work, without introducing any threats to validity related to preprocessing. In the new setup, the adversarial agent learns to produce payloads that include mostly valid tokens, while being still able to circumvent the detection capabilities of the considered detectors.

\begin{tcolorbox}[boxrule=0pt,frame hidden,sharp corners,enhanced,borderline north={1pt}{0pt}{black},borderline south={1pt}{0pt}{black},boxsep=2pt,left=2pt,right=2pt,top=2.5pt,bottom=2pt]

\textbf{Answer to RQ4:} Our Oracle-enhanced training method demonstrates the concrete threat of adversarial attacks on XSS detectors. Despite slight performance degradation, it confirms the ability of these attacks to bypass detection without exploiting threats related to the preprocessing.

\end{tcolorbox}

