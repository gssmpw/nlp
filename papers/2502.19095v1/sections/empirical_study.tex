\section{Empirical Study}
\label{sec:empirical_study}


This section presents the replication of the experiments in the reference work~\cite{CHEN2022102831}, highlighting the deviations from the results reported in the original paper. We introduce specific research questions for the replication and for the extension study, and we describe the experiments conducted to extend the reference work.


\subsection{Replication Study}
\label{sec:replication_study}

In our replication study, we aim to closely follow the methodology of the reference work. However, some differences are worth noting and justifying. The dataset used to train the detectors was not publicly available, so we utilized an alternative dataset.\footnote{\url{https://github.com/fmereani/Cross-Site-Scripting-XSS/blob/master/XSSDataSets/Payloads.csv}}
This employed dataset is a well-known one~\cite{10.1007/978-3-319-74690-6_20} containing more than 15,000 Malicious and Benign payloads. The dataset for training the adversarial agent was partially available but had a different structure compared to the  one employed in the reference work. The reference work does not adequately describe the correct payload structure, as the examples only consider parameters, while some steps mention filtering applied to the URL, suggesting the payload should be the entire HTTP request. Preliminary experiments revealed that datasets with varying structures encountered out-of-vocabulary issues even before the agent's actions were applied.
In particular, when one dataset is used to create the vocabulary and to train a detector, and the other one is simply tested against it,  RR is very high even before training an adversarial agent ($RR>60\%$).


To isolate the identified threats to validity and mitigate any data structure-related problems, we divided the selected dataset into two parts: one for training the detectors and the other for training the adversarial agents, excluding Benign examples. This setup makes adversarial attacks more challenging, as the examples are closer to those used for detector training. Consequently, a high RR in this context would indicate the significance of the threats TH1 and TH2, because of the alignment between detector's and adversarial agent's training sets. %

The dataset was pre-filtered by the Oracle to ensure accurate labelling. After  pre-filtering, the most representative class (Benign) was undersampled to ensure class balance. We computed the Ruin Rate of the original payload before and after preprocessing, resulting in a RR = 0\%, confirming that all samples were initially valid according to our Oracle. Furthermore, the dataset was split into train, validation, and test sets. Table~\ref{tab:dataset} shows such details.


\begin{table}[!h]
\caption{Dataset split between detector and adversarial agent, and then into train, validation and test sets} \label{tab:dataset}
\centering

\begin{tabular}{c|ccc|ccc}
    \toprule
    Label &\multicolumn{3}{c|}{Detectors} & \multicolumn{3}{c}{Adversarial Agent}\\ & Train Set & Val. Set  & Test Set  & Train Set & Val. Set  & Test Set   \\
    \midrule
    Benign & 2,884   & 721 & 902 & 0 & 0 & 0 \\
    Malicious & 2,883   & 721 & 901 & 2,883 & 712 & 901 \\
    \bottomrule
\end{tabular}


\end{table}


For the detection models, we used a different  activation function in the output layer compared to the reference work. The softmax function used in the reference is more appropriate for multiclass classification, whereas our binary classification problem (Benign vs. Malicious) is better suited to the sigmoidal function. Furthermore, we employed only CNN, MLP, and LSTM as detectors, as Safedog and XSSChop are not publicly available. These models were trained for 150 epochs with early stopping (patience of 10 epochs), an embedding dimension of 8, a learning rate of $10^{-3}$, and a stochastic gradient descent optimizer.


In contrast to the reference work, which used the SAC algorithm~\cite{haarnoja2018soft} for agent training, we opted for the PPO algorithm~\cite{schulman2017proximal}. This decision was made because we utilized the Stable Baselines library in Python for the reinforcement learning model implementation. The SAC algorithm implementation in this library is designed for a continuous action space, which does not align with our discrete action space, comprising discrete actions for mutating the attack payload. Therefore, we chose the PPO algorithm implementation, which  handles a discrete action space.\footnote{\url{https://stable-baselines3.readthedocs.io/en/master/modules/sac.html}}


The performance of our detection models is similar to the reference work, achieving near-perfect metric scores as presented in Table~\ref{tab:detectors}. %

\begin{table}[!h]
\caption{Performance of the XSS detection models of the replication study.} \label{tab:detectors}
\centering
\scalebox {1.0} {
\begin{tabular}{c|cccc}
    \toprule
    Detector & Precision & Recall  & Accuracy & F1      \\
    \midrule
    MLP        & 99.67\%   & 100.0\% & 99.83\%  & 99.83\% \\
    LSTM       & 99.67\%   & 100.0\% & 99.83\%  & 99.83\% \\
    CNN        & 99.67\%   & 100.0\% & 99.83\%  & 99.83\% \\
    \bottomrule
\end{tabular}
}

\end{table}



\subsection{Research Questions (RQs)}

\noindent The first RQ is to assess the feasibility of replicating the study's findings in the context of TH3.
\begin{itemize}
    \item \textbf{RQ1. Replication Study}: \textit{Can we successfully reproduce the outcomes reported in the reference work?}
\end{itemize}

\noindent The next two RQs focus on evaluating the significance of TH1 and TH2:
\begin{itemize}
    \item \textbf{RQ2. Evaluation of TH1}: \textit{Does the lack of validation of the actions pose a threat to validity?}
    \item \textbf{RQ3. Evaluation of TH2}: \textit{Does the lack of validation of the preprocessed payload pose a threat to validity?}
\end{itemize}

\noindent The final RQ extends this study by re-examining the performance of the method introduced in the reference work after addressing the identified threats to validity.
\begin{itemize}
    \item \textbf{RQ4. Extension Study}: \textit{How does the reference method perform once the identified threats are mitigated?}
\end{itemize}



\subsection{Implementation}

Our experimental framework was implemented using Python 3.11. The DL library used to implement the models is PyTorch 2.2.1. The RL agent used for generating adversarial attacks is implemented in StableBaselines3 2.3.0.
The Oracle is implemented with a Web Server using FastAPI 0.104.0 and Jinja2 3.1.2 to render the template. The DOM is analyzed using BeautifulSoup 0.0.2 and zss 1.2.0.


\subsection{Oracle Integration and Analysis}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.9\textwidth]{images/method-cropped.pdf}
  \caption{Experimental setup: malicious undetected payloads are fed into the Oracle before and after  preprocessing}\label{fig:experiments}
\end{figure}

As shown in Figure~\ref{fig:experiments}, the Oracle is used in two different stages.
The set of undetected malicious payloads generated by the adversarial model, which represent the examples that contribute to the escape rate of the replication study, named $E$, is directly fed into the Oracle.
The set $E$ is then preprocessed as described in the previous sections, obtaining the set of arrays $V$.
Also, $V$ is fed into the Oracle. %
Thanks to the Oracle, it is possible to evaluate $RR(E)$ and $RR(V)$, which, respectively, represent the answers to RQ2 and RQ3.
We do not rely only on the Oracle: the analysis of the Ruin Rate for RQ3 is complemented by the analysis of the Out-Of-Vocabulary Rate, that it is not reported in the Figure~\ref{fig:experiments} for simplicity. %
Regarding RQ4, there is no guarantee that increasing the vocabulary would solve TH2, since the adversarial agent is potentially able to generate new tokens that are out-of-vocabulary regardless of the vocabulary size.
To mitigate this threat-to-validity and to evaluate the real performance of the method, we integrated the Oracle into the training process of the adversarial agent.
The agent's reward is set to -2 if the mutated payload, after  preprocessing, is no longer recognized by the Oracle as an XSS attack.
Otherwise, the reward is the same proposed in the reference work.
