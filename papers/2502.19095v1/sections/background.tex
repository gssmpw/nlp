\section{Background and Related Work}
\label{sec:backgroud}



\subsection{Cross-Site Scripting (XSS)}

Cross-Site Scripting (XSS) consists of the injection of malicious code into a web page. When a user visits the page, their browser unknowingly executes this script, leading to critical security breaches. It has been recognized as one of the prevalent threats, evidenced by the Open Web Application Security Project (OWASP),\footnote{\url{https://owasp.org/www-project-top-ten/}} a renowned authority on web application security, that has consistently ranked XSS as one of the top ten web application security risks. These attacks can have various malicious intentions, such as stealing confidential information or impersonating users to perform unauthorized actions. There are three primary types of XSS attacks, each with its own characteristics:
\begin{enumerate}

    \item \textit{Stored XSS} is the most severe form, where the malicious script is stored permanently in the server-side database. Any user accessing the affected page risks executing this script, potentially affecting multiple users.

    \item \textit{Reflected XSS} is a more targeted attack. The attacker lures the victim into visiting a malicious URL, often through spam emails. The URL contains harmful code, which is then executed in the victim's browser. This type of XSS is temporary and affects a specific user.

    \item \textit{DOM-based XSS} manipulates the Document Object Model (DOM) of a web page by modifying the input. This triggers the attack when the DOM is parsed on the client side, making it another non-persistent form of XSS.

\end{enumerate}




\subsubsection{Defending Against XSS Attacks}

Given the diverse and harmful nature of XSS attacks, researchers have devoted efforts to developing effective defence strategies. The primary research focus has been on two key areas: XSS vulnerability discovery and XSS attack detection. 

\textbf{XSS vulnerability discovery}: This encompasses both static and dynamic analysis techniques. \textit{Static analysis} searches along all the possible execution paths in the code to find potential attacks. Several approaches have been proposed in this category. Doupe et al.~\cite{doupe2013dedacota} suggested a server-side XSS mitigation strategy that isolates code from data, but this method falls short of dynamic JavaScript attacks. Steinhauser et al.~\cite{steinhauser2016static} developed JSPChecker, a tool employing data flow analysis and string parsing to detect vulnerabilities in sanitization sequences. Mohammadi et al.~\cite{mohammadi2017detecting} utilized automated unit testing to identify vulnerabilities arising from improper input data handling. Kronjee et al.~\cite{kronjee2018discovering} applied machine learning with a 79\% precision rate to detect XSS and SQL injection vulnerabilities through static code analysis. \textit{Dynamic analysis}, on the other hand, involves monitoring the data flow to pinpoint injection points and then testing for actual vulnerabilities. Lekies et al.~\cite{lekies2013detection} introduced a technique to detect DOM-based XSS by monitoring and exploiting vulnerabilities in sensitive calls. Fazzini et al.~\cite{fazzini2015automatically} automatically implemented Content Security Policies (CSP) in web applications, to track and manage the dynamic content.

\textbf{XSS attack detection:} While vulnerability discovery is essential, it may not offer complete protection against XSS attacks. Hence, researchers have also developed methods to identify malicious user input at runtime. This task is challenging due to the obfuscation techniques employed by attackers. Consequently, many detection methods rely on ML and DL approaches. Likarish et al.~\cite{likarish2009obfuscated} used JavaScript features for detection, achieving 92\% accuracy. Nunan et al.~\cite{nunan2012automatic} refined this approach, improving detection. Mereani et al.~\cite{mereani2018howe} extracted structural and behavioral features, reaching 99\% accuracy. Fang et al.~\cite{fang2018deepxss} utilized Word2Vec and LSTM, achieving precision and recall of 99.5\% and 98.7\%, respectively. Mokbal et al.~\cite{mokbal2019mlpxss} constructed a large dataset and developed a feature selection technique, attaining 99.32\% accuracy and 98.35\% recall. Tekerek et al.~\cite{tekerek2021novel} employed a CNN, achieving 97.07\% accuracy on a public dataset.
Despite the impressive results, State-Of-The-Art (SOTA) approaches present  vulnerabilities that can be exploited by attackers, among which  vulnerabilities to adversarial attacks against ML/DL.


\subsubsection{Adversarial Attacks on XSS Detectors}

The recent emergence of DL has led to groundbreaking advancements in various fields, including XSS attack detection, where it has achieved SOTA performance. However, researchers have identified a critical issue: the susceptibility of these methods to adversarial attacks. These attacks have successfully evaded multiple DL models across different domains, underscoring the imperative to enhance the robustness of these models. In the context of XSS attack detectors, several studies have explored adversarial attacks. Fang et al.~\cite{fang2018deepxss} developed an XSS adversarial approach utilizing the Dueling Deep Q Networks algorithm, but its escape detection success rate remained below 10\% due to a simplistic bypass strategy. Zhang et al.~\cite{zhang2020adversarial} proposed an algorithm based on Monte Carlo Tree Search (MCTS) to generate XSS adversarial examples for training the detection model. However, this algorithm relied on limited escape strategies and exhibited high time complexity. Wang et al.~\cite{wang2022black} introduced a method employing soft Q-learning, dividing the bypass process into HTML and JS stages, achieving an impressive 85\% escape rate.

The reference work by Chen et al.~\cite{CHEN2022102831} stands out with its Deep Reinforcement Learning algorithm, leveraging a set of mutation rules as actions, resulting in near-perfect escape rates against various SOTA XSS attack detectors. This approach will be thoroughly analyzed in Section~\ref{sec:reference}.




\subsection{Reinforcement Learning}

Reinforcement Learning (RL) is a distinctive machine learning paradigm that aims to maximize long-term rewards by striking a balance between exploration and exploitation. Unlike supervised learning, RL does not rely on labeled input-output pairs. Instead, it models the learning process as the interaction between two key components: the agent and the environment. The environment is represented as a timed sequence of states, $S=\langle s_0, s_1, \ldots\rangle$. At any given time $t$, the agent observes a state $s_t$ and selects an action $a_t$ from the available action space $A=\{a_0, a_1, \ldots\}$ according to a policy $\pi(a_t | s_t)$, which is either the same being learned during the agent's interactions with the environment (\textit{on-policy} learning) or which is kept separate from the policy under training (\textit{off-policy} learning). The chosen action triggers a state change, and the new state $s_{t+1}$ is determined by a Markov decision process with probability transition matrix $P(s_{t+1} | s_{t}, a_{t})$. Simultaneously, the agent receives a reward $r_{t+1}$. The agent's objective is to maximize the long-term reward $R=\sum_{t=0}^{\infty}{\gamma^t r_t}$, where $\gamma \in [0,1]$ is a discount factor. This balance between immediate and future rewards is a hallmark of RL. 


Several algorithms have been developed for RL, each with its unique characteristics. One widely adopted algorithm is Proximal Policy Optimization (PPO)~\cite{schulman2017proximal}, an on-policy algorithm that alternates between data collection through environment interactions and optimization of a clipped surrogate objective function via stochastic gradient descent. The clipping mechanism stabilizes training by limiting the policy updates, preventing drastic changes. Deep Deterministic Policy Gradient (DDPG)~\cite{lillicrap2020continuous} is an off-policy algorithm where the agent learns a deterministic policy guided by a Q-value function critic, which estimates the value of the optimal policy. DDPG employs target actor and critic networks and an experience replay buffer to enhance stability and learning efficiency. Soft Actor-Critic (SAC)~\cite{haarnoja2018soft}, another off-policy algorithm, is based on the maximum entropy framework. SAC trains the actor to maximize both expected reward and entropy, encouraging broader exploration. This approach has been shown to improve learning speed compared to state-of-the-art methods optimizing the traditional RL objective function. DDPG and SAC are typically applied to continuous action spaces. %
In contrast, PPO is versatile, supporting both continuous and discrete action spaces.
RL approaches are very useful in several domains~\cite{SHAKYA2023120495}, including  adversarial attack generation.
The reference work by Chen et al.~\cite{CHEN2022102831} proposed to train an adversarial agent able to attack XSS detectors using RL.
