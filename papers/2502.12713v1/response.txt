\section{Related Work}
\label{sec:related_work}

\subsection{Echocardiographic Segmentation}

Recently, deep neural networks trained on publicly available datasets such as CAMUS~\citep{Bergmann_2019} and EchoNet-Dynamic~\citep{Oktay_2017} have reached a performance within inter-user variability, typically using architectures based on U-Net ~\citep{Ronneberger_2015}. Since, many different strategies such as echocardiography-specific data augmentation ~\citep{Zreike_2020} and pyramid attention networks ~\citep{Bertasius_2020} have improved U-Net results. 
The recent publication of nnU-Net has significantly enhanced segmentation outcomes across various tasks ~\citep{Isensee_2021} and, when applied to the CAMUS dataset, outperformed all prior methods~\citep{Bergmann_2019}.%, even surpassing intra-user variability .


Recent works on cardiac and lung segmentation use point-based approaches ~\citep{Oktay_2017}. This approach is viable for these organs because of their regular shape. This formulation reduces anatomical implausibilities and increases interpretability due to its similarity to human labeling. Similar work on key-point localization has also been proposed in~\citep{Heinrich_2020}.


Regardless of the segmentation technique, a key objective following segmentation is the derivation of clinical metrics. A standard metric in echocardiography is the left ventricular area. A more accurate evaluation of the ventricular cavity size involves determining the ventricular volume using Simpson's biplane method~\citep{Simpson_1974}, utilizing the segmentation obtained from the apical 4-chamber (A4C) and apical 2-chamber (A2C) views. Additionally, temporal metrics are of significant interest. Examining the ratio of heart size between end-systole (ES) and end-diastole (ED) can provide indications of certain cardiac pathologies. The fractional area change (FAC) is defined as the difference between the end-diastolic  area and the end-systolic  area, divided by the end-diastolic area. Ejection fraction (EF) is calculated similarly, albeit with volume measurements.

% Additionally, global longitudinal strain (GLS), which measures the percentage change in length of the myocardium in the longitudinal direction during the cardiac cycle, is also an important parameter. The longitudinal deformation of the myocardium over the cardiac cycle can help diagnose pathologies less apparent when only considering the left ventricle cavity size ~\citep{Marsan_2017}.


\subsection{Uncertainty Estimation}

Although numerous studies have proposed new models to enhance the segmentation accuracy, relatively few have addressed the predictive uncertainty of these models. Accounting for uncertainty is crucial, as although neural networks can achieve human-level performance, they can also make occasional critical errors.


Uncertainty is usually classified into two categories: {\em aleatoric} and {\em epistemic}~\citep{Kendall_2017}. Aleatoric uncertainty is the uncertainty present within the data itself. In medical imaging, aleatoric uncertainty is caused by the inherent ambiguity of images, which for instance can occur due to noisy images or labels.
Epistemic uncertainty is the uncertainty associated with the model and is caused by the limited size of the training dataset. While epistemic uncertainty can be reduced by acquiring more data, 
aleatoric uncertainty is irreducible.



One prominent approach for modeling epistemic uncertainty involves characterizing the distribution of model weights, $\theta$, conditioned on the data $\mathcal{D}$. This posterior distribution over weights, $p(\theta|\mathcal{D})$, is generally intractable to compute analytically for neural networks. Variational inference offers a method to approximate the true posterior using a variational distribution, $q_w(\theta)$, in Bayesian neural networks~\citep{Hernandez-Lobato_2016}. However, this approach necessitates significant modifications to standard training procedures. Alternative methods, such as Monte-Carlo (MC) dropout, have demonstrated effectiveness as a practical approximation for Bayesian neural networks, allowing for uncertainty estimation without altering the training pipeline, and have achieved reliable results~\citep{Gal_2016}. On the other hand, a frequentist approach with ensemble methods leverages the variability of independently trained networks—each trained on different data subsets or with distinct hyperparameters—to capture uncertainty~\citep{Lakshminarayanan_2017}.


While epistemic uncertainty methods usually do not require any data assumptions, this is not the case for aleatoric uncertainty methods. 
Most commonly, probabilistic approaches that model a distribution are used in aleatoric uncertainty methods. 
Most commonly, probabilistic approaches that model a parameterized distribution $p_\theta(y|x)$ over the output $y$ given the input $x$ are employed in aleatoric uncertainty methods. For instance, in the case of univariate regression, the mean and variance of a Gaussian distribution are learned by maximizing the negative log-likelihood with respect to both parameters. This can be extended to the multivariate normal~\citep{Bishop_2006} or with different distributions such as the skew-normal distribution~\citep{Azzalini_2014}. In the case of classification, uncertainty is approximated by modeling a Gaussian distribution across every output logit~\citep{Hendrycks_2019}.


Aleatoric uncertainty can also be predicted with non-probabilistic methods. Methods that use test-time augmentation apply random data transformations, perform inference and aggregate the outputs to model uncertainty~\citep{Pereyra_2017}.


While modeling both aleatoric and epistemic uncertainty is relevant to understanding the origin of the prediction ambiguity, it is also important to model the full predictive uncertainty. If the aleatoric uncertainty is captured by a model $p_\theta(y|x)$ and the epistemic uncertainty is modeled by $q_w(\theta)$, the predictive uncertainty is given by the law of total variance~\citep{Hullermeier_2013}
%
\begin{equation}
    \mathbb{V}_{p(y|x)} = \mathbb{E}_{q_w(\theta)}\Big[\mathbb{V}_{p_\theta(y|x)}[y]\Big] + \mathbb{V}_{q_w(\theta)}\Big[\mathbb{E}_{p_\theta(y|x)}[y]\Big].
\end{equation}
%
\subsubsection{Segmentation uncertainty}

In many cases, segmentation uncertainty is tackled similarly to classification. Epistemic uncertainty methods, such as MC dropout and ensembles, do not require any changes as multiple forward passes generate varying segmentation maps. In the case of aleatoric uncertainty, a simple way is to consider the prediction for each pixel as an independent classifier and estimate aleatoric uncertainty by modeling the logits of each pixel with a Gaussian~\citep{Gal_2016}.


Pixel-wise aleatoric uncertainty often performs poorly since independent pixel distributions make plausible segmentation samples unlikely. For this reason, methods such as Probabilistic U-Net ~\citep{Hendriks_2020} and Phi-Seg ~\citep{Nalisnick_2019} use a variational autoencoder coupled with a segmentation network to learn a distribution of shapes in the latent space from which plausible samples can be drawn. Similarly, stochastic segmentation networks learn a low-rank multivariate Gaussian representation of pixel distributions, enabling efficient training despite the large covariance matrix, and outperform previous methods~\citep{Kim_2020}.


% In practice, pixel-wise aleatoric uncertainty performs poorly. Although this probabilistic approach offers the option to sample outputs from the predicted distribution, if each pixel is an independent distribution, the probability of sampling a plausible segmentation is very low. 
% For this reason, methods such as Probabilistic U-Net ~\citep{Hendriks_2020} and Phi-Seg ~\citep{Nalisnick_2019} use a variational autoencoder coupled with the segmentation network to learn a distribution of shapes in the latent space that represents the uncertainty. Plausible samples can then be drawn from this distribution. Stochastic segmentation networks have recently proposed learning a low-rank multivariate Gaussian representation of pixel distributions, enabling efficient training despite the large covariance matrix and outperforming both Probabilistic U-Net and Phi-seg~\citep{Kim_2020}.


Finally, test-time augmentation can be extended to segmentation by applying transforms to the input image and applying the inverse transform on the output segmentation~\citep{Pereyra_2017} .

% As the previous methods generated multiple samples a pixel-wise uncertainty map can be estimated by computing the pixel-wise entropy of a set of predictions. 

\subsection{Clinical Metrics}

In echocardiography, several clinical metrics are commonly used to assess cardiac function. One such metric is the left ventricular ejection fraction (LVEF), which is calculated as the ratio of the volume of blood ejected from the left ventricle during systole to the end-diastolic volume. LVEF is an important prognostic indicator for patients with heart failure, and its measurement is critical in guiding treatment decisions.

\subsection{Uncertainty Quantification}

Recent studies have highlighted the importance of uncertainty quantification in medical imaging analysis. In echocardiography, uncertainty quantification can be used to estimate the reliability of LVEF measurements. By propagating uncertainty from the predicted shape to the derived metrics, our method provides an interpretable way to quantify and propagate uncertainty from the predicted shape to the derived metrics.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% METHOD %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%