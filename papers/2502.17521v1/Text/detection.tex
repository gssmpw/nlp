% \quad
% \newpage
% \quad
% \newpage
% \quad
% \newpage
% \section{Detection}
% Data contamination occurs when benchmark data,
% $D_b$
%  is included in the training data $D$ of language models 
% $M$.
%  Detection methods can be classified into three types based on their focus: \textit{Training Data-Oriented}, \textit{Benchmark-Oriented} and \textit{Model Behavior-Oriented} methods.
% \subsection{Training Data-Oriented}
% Training data-oriented methods primarily assess the overlap between $D_b$ and $D$. This overlap is typically detected via direct n-gram matching at the token~\cite{touvron2023llama}, word~\cite{radford2019language,brown2020language,chowdhery2023palm}, character~\cite{achiam2023gpt}, or document chunk ~\cite{dodge2021documenting}. However, exact matches often result in false negatives. To mitigate this issue, subsequent research has explored more robust approaches, including embedding-based similarity measurements~\cite{riddell2024quantifying,lee2023platypus,gunasekar2023textbooks} and improved mapping metrics~\cite{li2024open,xu2024benchmarking}. Additionally, \citeauthor{yang2023rethinking}(\citeyear{yang2023rethinking}) highlight that minor data variations can evade prior detection methods and propose an LLM-based approach for assessing semantic similarity between training and test data. To further enhance detection, several search tools~\cite{piktus2023roots,piktus2023gaia,elazar2023s} have been developed for large-scale corpus overlap analysis.

% \subsection{Benchmark-Oriented}
% Benchmark-oriented methods frame contamination detection as a memorization problem, assuming models can recall benchmark data or key information encountered during training. Based on their approach, these methods can be categorized into original data methods and variant data methods.
% Original data methods assess memorization by directly manipulating benchmark data, such as masking specific parts (e.g., context~\cite{ranaldi2024investigating,chang2023speak}, choices~\cite{deng2024investigating,liu2024evaluating}, or labels~\cite{magar2022data}), or requiring the model to continue generation with partial suffix provided~\cite{anil2023palm,xu2024benchmarking,golchin2024timetravelllmstracing}.
% Variant data methods analyze contamination by introducing modified test instances and evaluating the model’s preference for different variations. \citeauthor{duartecop}~(\citeyear{duartecop}) and \citeauthor{golchin2023data}~(\citeyear{golchin2023data}) pair test instances with paraphrased versions to determine whether the model exhibits a preference for exact verbatim ones from the test set. Similarly, \citeauthor{zong2024fool}~(\citeyear{zong2024fool}) shuffle the positions of answer choices to assess whether performance drops.
% \subsection{Model Behavior-Oriented}
% Model behavior-oriented methods identify contamination by examining output probabilities, confidence distributions, or model performance across diverse experimental settings. These approaches often require white-box access to retrieve the model's architecture and internal weights~\cite{ravaut2024much}. Building on the assumption that seen instances exhibit higher probabilities than unseen ones~\cite{ravaut2024much}, previous research works on token-level probabilities~\cite{song2019auditing,shidetecting,dong2024generalization} or perplexity~\cite{carlini2021extracting,li2023estimating,xu2024benchmarking} of the benchmark instances.\citeauthor{shidetecting}(\citeyear{shidetecting}) detect contamination by averaging the probabilities of the 
% k\% least likely tokens (outlier words) and assessing whether the average is abnormally high. \citeauthor{dong2024generalization}~(\citeyear{dong2024generalization}) propose CDD (Contamination Detection via Output Distribution) to analyze the peakedness of output distribution and introduce TED (Trustworthy Evaluation via Output Distribution) to mitigate data contamination. Membership Inference Attacks (MIA)\cite{shokri2017membership}, which assess the probability of an instance being part of the training data, have gained traction in contamination detection. \citeauthor{mireshghallah2022quantifying}(\citeyear{mireshghallah2022quantifying}) utilize a likelihood ratio between a target and reference model to infer membership. \citeauthor{mattern2023membership}~(\citeyear{mattern2023membership}) generate similar neighbor sentences and determine membership by analyzing loss differences against a threshold. \citeauthor{ye2024data}~(\citeyear{ye2024data}) propose PAC, which uses random swap augmentation and polarized distance, a spatial metric accounting for both near and far probability regions, to infer membership. Additionally, some methods utilize reference sets and models to assess whether a model demonstrates consistently inflated performance across different benchmarks, identifying contamination through deviations in generalization patterns. \citeauthor{dekoninck2024constat}~(\citeyear{dekoninck2024constat}) propose CONSTAT, a statistical method that detects contamination by comparing a model’s performance on a primary and reference benchmark, identifying non-generalizing performance through deviation analysis based on other uncontaminated reference models. \citeauthor{wei2023skywork}~(\citeyear{wei2023skywork}) use GPT-4 to generate a reference set and detect contamination by measuring its divergence from the test set. Notably, some studies~\cite{srivastava2023beyond,wei-etal-2024-proving} integrate LLM data watermarking\cite{kirchenbauer2023watermark} into contamination detection. We refer readers to relevant literature for further exploration in this direction\cite{liang2024watermarking}.