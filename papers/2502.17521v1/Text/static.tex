
\input{Table/static.tex}

\section{Static Benchmarking}
\label{sec:static}


In this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table~\ref{tab:static-benchmarks} provides an overview of these benchmarks along with the corresponding task categories and key references.

\subsection{Problem Formulation}
A static benchmark is given by
$ \mathcal{D} = (\mathcal{X}, \mathcal{Y}, \mathcal{S}(.)) $, 
where \( \mathcal{D} \) represents the seed dataset, consisting of input prompts \( \mathcal{X} \), expected outputs \( \mathcal{Y} \), and a scoring function \( \mathcal{S}(\cdot) \) that evaluates the quality of an LLM's outputs by comparing them against \( \mathcal{Y} \). 

\subsection{Static Benchmark Application}

\paragraph{Math}
Math benchmarks evaluate a model’s ability to solve multi-step math problems. 
Datasets such as GSM8K~\citep{cobbe2021training} and MATH~\citep{hendrycks2021measuring} require models to work through complex problems. Recent challenges like AIME 2024~\citep{maa2024aime} and CNMO 2024~\citep{cnmo2024} further test a model’s capacity to tackle diverse and intricate math tasks.

\paragraph{Knowledge}
Knowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions~\citep{kwiatkowski2019natural} and TriviaQA~\citep{joshi2017triviaqa} focus on retrieving real-world information, while multi-domain tasks are covered by MMLU~\citep{hendrycks2020measuring}, BBH~\citep{suzgun2022challenging}, and AGI Eval~\citep{zhong2023agieval}. Recent extensions like MMLU-Redux~\citep{gema2024we} and MMLU-Pro~\citep{wang2024mmlu} refine these assessments further. 
Additionally, ControlBench~\citep{Darioush2024ControlBench}, FRAMES~\citep{krishna2024fact}, and GPQA Diamond~\citep{rein2023gpqagraduatelevelgoogleproofqa} target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval~\citep{alpaca_eval} and ArenaHard~\citep{arenahard2024}.

\paragraph{Coding}
Coding benchmarks measure a model’s ability to generate and debug code. HumanEval~\citep{chen2021evaluating} and MBPP~\citep{austin2021program} test code synthesis and debugging, whereas SWE-Bench~\citep{jimenez2024swebench,yang2024swebenchmultimodal} addresses more advanced challenges. Competitive platforms like Codeforces~\citep{codeforces} and datasets such as Aider~\citep{aider} further probe dynamic problem solving.

\paragraph{Instruction Following}
Instruction benchmarks evaluate a model’s ability to comprehend and execute detailed directives. Datasets like IFEval~\citep{zhou2023instruction} and InfoBench~\citep{qin2024infobench} simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval~\citep{huang2024c} focusing on Chinese instructions.


% \paragraph{Other Applications}
% We provide a detailed introduction to other applications in \appref{sec:appendix_applications}.

\paragraph{Reasoning}
Understanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA~\citep{bisk2020piqa}, SIQA~\citep{sap2019socialiqa}, HellaSwag~\citep{zellers2019hellaswag}, and WinoGrande~\citep{sakaguchi2021winogrande} are designed to assess a model’s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC~\citep{clark2018think}, OpenBookQA~\citep{mihaylov2018can}, and CommonsenseQA~\citep{talmor2018commonsenseqa} push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA~\citep{he2024chinese} evaluates the factuality ability of language models to answer short questions in Chinese.




\paragraph{Safety}
Safety benchmarks are essential for evaluating the robustness of LLM’s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts~\citep{gehman-etal-2020-realtoxicityprompts} and ToxiGen~\citep{hartvigsen2022toxigen} assess resilience against producing harmful outputs.
By providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.



\paragraph{Language}
Language benchmarks assess the LLMs’ proficiency in specific languages.
GLUE~\citep{wang2018glue} and SuperGLUE~\citep{wang2019superglue} cover tasks from sentiment analysis to  language inference, while CLUE~\citep{xu2020clue} targets Chinese language.
Typo-fixing~\citep{suzgun2022challenging} is also widely used.

\paragraph{Reading Comprehension}
Reading comprehension tasks test a model’s ability to extract and infer information from text. Benchmarks like SQuAD~\citep{rajpurkar2018know}, QuAC~\citep{choi2018quac}, and BoolQ~\citep{clark2019boolq} challenge models to understand passages and draw logical conclusions.






\subsection{Methods for Mitigation}
Due to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance \textit{static} benchmarking and mitigate the impact of data contamination.
% Static benchmarks can be contaminated by training data that inadvertently appears in the test set. To address this, we discuss static mitigation methods, including canary strings, encryption, label protection, and post-hot detection.

\subsubsection{Canary String}
Canary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model’s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances \cite{jacovi-etal-2023-stop}. 

\paragraph{Limitations} The effectiveness of canary strings depends on model trainers being aware of and responsive to these markers. 
If a developer aims to leak benchmarking data to boost scores, this method will not work.

\subsubsection{Encryption}
Encryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. \citet{jacovi-etal-2023-stop} propose encrypting test data with a public key and a ``No Derivatives’’ license to block automated crawling and reuse. \citet{yang2023rethinking} show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE \citep{chandran2024private} leverages confidential computing and secure multi‐party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential. 

\paragraph{Limitation} While these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.

\subsubsection{Label Protection}
Label protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE \cite{wang2018glue}, SuperGLUE \cite{wang2019superglue}, and OpenAI’s HumanEval \cite{chen2021evaluating}, etc.,  where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.

\paragraph{Limitations} Label protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.


\subsubsection{Post-hoc Detection}
Post-hoc detection mitigates data contamination by identifying overlaps between $D_{train}$
  and $D_ 
{test}$. This is typically done through n-gram matching at various levels, such as tokens~\cite{touvron2023llama} or words~\cite{radford2019language,brown2020language,chowdhery2023palm}. However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity~\cite{riddell2024quantifying,lee2023platypus,gunasekar2023textbooks} and improved mapping metrics~\cite{li2024open,xu2024benchmarking}.

Beyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs~\cite{ranaldi2024investigating,chang2023speak}, partial completions~\cite{anil2023palm,golchin2024time}, or preference for original over paraphrased test cases~\cite{duartecop,golchin2023data,zong2024fool}. For instance, \citeauthor{dekoninck2024constat}~(\citeyear{dekoninck2024constat}) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.

%Additionally, \citeauthor{yang2023rethinking}~(\citeyear{yang2023rethinking}) propose an LLM-based approach for assessing semantic similarity between training and test data.

% Model-based detection, in contrast, identifies contamination by analyzing model performance and behavioral patterns under different experimental conditions. One common approach treats contamination as a memorization problem, assessing whether models exhibit disproportionate recall of benchmark data. This includes evaluating memorization through masked inputs~\cite{ranaldi2024investigating,chang2023speak}, partial completions~\cite{anil2023palm,xu2024benchmarking,golchin2024time}, or preference for original over paraphrased test cases~\cite{duartecop,golchin2023data,zong2024fool}. For example, \citeauthor{golchin2024time}(\citeyear{golchin2024time}) prompted LLMs with dataset names, partition types, and truncated test instances to evaluate their ability to complete missing content. Another approach examines statistical signals in model outputs, such as perplexity\cite{carlini2021extracting,li2023estimating,xu2024benchmarking}, confidence scores, or token-level probabilities~\cite{song2019auditing,shidetecting,dong2024generalization}, assuming contaminated instances yield lower uncertainty. Membership Inference Attacks (MIA)\cite{shokri2017membership,mireshghallah2022quantifying,mattern2023membership} estimate the likelihood of an instance being part of the training set, while statistical deviation methods compare performance across primary and reference benchmarks\cite{dekoninck2024constat,wei2023skywork}. \citeauthor{dekoninck2024constat}~(\citeyear{dekoninck2024constat}) propose CONSTAT, which detects contamination by comparing a model’s performance on primary and reference benchmarks, identifying non-generalizing patterns through deviation analysis with uncontaminated models.

\paragraph{Limitations}
Post-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.
% \subsubsection{Data-based Detection} \TODO{merge data-based and model based}

% Data-based detection identifies contamination by analyzing the overlap between $D_{train}$ and $D_{test}$. This overlap is typically detected via direct n-gram matching at the token~\cite{touvron2023llama}, word~\cite{radford2019language,brown2020language,chowdhery2023palm}, character~\cite{achiam2023gpt}, or document chunk ~\cite{dodge2021documenting}. However, exact matches often result in false negatives. To mitigate this issue, subsequent research has explored more robust approaches, including embedding-based similarity measurements~\cite{riddell2024quantifying,lee2023platypus,gunasekar2023textbooks} and improved mapping metrics~\cite{li2024open,xu2024benchmarking}. Additionally, \citeauthor{yang2023rethinking}(\citeyear{yang2023rethinking}) adopt an LLM-based approach for assessing semantic similarity between training and test data. 

% \noindent \textbf{Limitation.} Data-based detection requires full access to the entire training dataset, which is often restricted due to legal and privacy constraints. Additionally, determining an appropriate threshold for filtering detected overlaps varies from case to case.

% \subsubsection{Model-based Detection} 

% Model-based detection identifies contamination by analyzing model performance and behavioral patterns under different experimental conditions. One approach treats contamination as a memorization problem, evaluating whether models exhibit disproportionate recall of benchmark data. This includes assessing memorization through masked inputs~\cite{ranaldi2024investigating,chang2023speak}, partial completions~\cite{anil2023palm,xu2024benchmarking,golchin2024timetravelllmstracing}, or preference for original over paraphrased test cases~\cite{duartecop,golchin2023data,zong2024fool}.For example, \citeauthor{golchin2024timetravelllmstracing}~(\citeyear{golchin2024timetravelllmstracing}) prompted LLMs with dataset names, partition types, and randomly truncated test instances to evaluate their ability to complete missing content. Another approach examines statistical signals in model outputs, such as perplexity~\cite{carlini2021extracting,li2023estimating,xu2024benchmarking}, confidence scores, or token-level probabilities~\cite{song2019auditing,shidetecting,dong2024generalization}, assuming contaminated instances yield lower uncertainty. Techniques like Membership Inference Attacks (MIA)~\cite{shokri2017membership,mireshghallah2022quantifying,mattern2023membership} estimate the likelihood of an instance being part of the training set, while statistical deviation methods compare performance across primary and reference benchmarks\cite{dekoninck2024constat,wei2023skywork}. 
% \citeauthor{dekoninck2024constat}~(\citeyear{dekoninck2024constat}) propose CONSTAT, which detects contamination by comparing a model’s performance on primary and reference benchmarks, identifying non-generalizing patterns through deviation analysis with uncontaminated models.

% \noindent \textbf{Limitation}.
% Model-based detection relies on assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, which may not be valid for specific model structures and task domains. Moreover, the lack of ground truth validation makes it difficult to confirm whether an instance was truly seen by the model, limiting the reliability of these methods.