

\section{Introduction}


The field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek~\cite{achiam2023gpt, liu2024deepseek, wan2023efficient}. Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning~\cite{codeforces, hu-etal-2024-reasoning}.

% \TODO{traditional static benchmark}
To develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on \textit{static} benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics~\cite{wang2018glue, achiam2023gpt, gunasekar2023textbooks}. 
For example, in 2021, OpenAI introduced the HumanEval~\cite{chen2021evaluating} dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.


However, because these \textit{static} benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable~\cite{magar2022data, deng2024investigating, li2024open, conda-2024-data, balloccu-etal-2024-leak}. Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time—stemming from the fundamental machine learning principle of separating training and test sets—it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination~\cite{achiam2023gpt, liu2024deepseek}. Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.


\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{Figure/advance.pdf}
    \caption{The progress of benchmarking LLM}
    \label{fig:transition}
\end{figure}


% \TODO{Add a sentence to introduce static contamination-free methods.}
To mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption~\cite{jacovi-etal-2023-stop} and post-hoc contamination detection \cite{shidetecting}. However, due to the inherent limitations of static methods, researchers have introduced new \textit{dynamic} benchmarking schema (\figref{fig:transition} illustrates the evolution from static to dynamic benchmarking). 
One of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination~\cite{white2024livebench, jain2024livecodebenchholisticcontaminationfree}. Another method focuses on regenerating benchmark data to reconstruct original benchmarks~\cite{chen2024ppm, zhou2025gsm, mirzadeh2025gsmsymbolic}, thereby reducing the likelihood of contamination.  


Despite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies~\cite{deng2024unveiling, ravaut2024much}. Moreover, no existing work discusses criteria for evaluating \textit{dynamic benchmarks} themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.


% \TODO{our work} To bridge this gap and provide model developers with a comprehensive guideline for evaluating LLMs while mitigating the risk of data contamination, we first examine existing contamination-free benchmarking methods. \TODO{XXX}


To bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both \textit{static} and \textit{dynamic} benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations.   Furthermore, we are the first to propose a set of criteria for evaluating \textit{dynamic} benchmarks. Our study reveals that existing \textit{dynamic} benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of \textit{dynamic} benchmarking methods.
% We provide a structure of existing benchmarking methods in \appref{sec:appendix_structure} and \figref{fig:Taxonomy}.

\begin{figure*}
    \centering
    \includegraphics[width=1\textwidth]{Figure/LLM_Benchmarking.pdf}
    \caption{ Taxonomy of research on benchmarking LLMs}
    \label{fig:Taxonomy}
\end{figure*}


The paper is structured as illustrated in \figref{fig:Taxonomy}. We begin with an overview of data contamination (\secref{sec:background}), establishing the necessary background. Next, we provide a comprehensive review of \textit{static} benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\secref{sec:static}). We then outline key criteria for an effective \textit{dynamic} benchmark and introduce existing \textit{dynamic} benchmarking approaches (\secref{sec:dynamic}). Finally, in \secref{sec:discussion}, we reflect on insights from prior work, summarize current challenges, and explore future research directions.

% We hope our work offers a comprehensive, systematic understanding of data contamination issues and provides valuable insights into \textit{dynamic} benchmarking design, thereby making a significant contribution to improving the integrity of LLM evaluations.