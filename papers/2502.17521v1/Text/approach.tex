\section{Dynamic Benchmarking}
\label{sec:dynamic}

Due to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, \textit{dynamic} benchmarking has been proposed.  

% \input{Table/dynamic}


\input{Dynamic/problem}


\input{Table/creteria}

\input{Dynamic/criteria}



\subsection{Existing Work}


Building on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness. 
Table~\ref{tab:static-benchmarks} summarizes recent dynamic benchmarks for LLM evaluation.
Dynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.
Temporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.
Rule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.
Finally, hybrid approaches combine the idea of these different approaches.

\subsubsection{Temporal Cutoff} 
Since LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination. 
This approach has been widely adopted to construct reliable benchmarks that prevent contamination.
LiveBench~\citep{white2024livebench} collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.
AntiLeak-Bench~\citep{wu2024antileak} generates queries about newly emerged knowledge that was unknown before the model's knowledge cutoff date to eliminate potential data contamination.
AcademicEval~\citep{zhang2024academiceval} designs academic writing tasks on latest arXiv papers.
LiveCodeBench~\citep{jain2024livecodebenchholisticcontaminationfree} continuously collects new human-written coding problems from online coding competition platforms like LeetCode.
LiveAoPSBench~\citep{mahdavi2025leveraging} collects live math problems from the Art of Problem Solving forum.
Forecastbench~\citep{smith2024forecastbench} updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.


\paragraph{Limitations}
The collection process typically requires significant human effort~\citep{white2024livebench,jain2024livecodebenchholisticcontaminationfree}, and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions~\citep{wu2024antileak}. Verification is often overlooked in these live benchmarks~\citep{white2024livebench}.

\subsubsection{Rule-Based Generation}
This method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability~\citep{zhu2024dyval}.

\paragraph{Template-Based}
GSM-Symbolic~\citep{mirzadeh2025gsmsymbolic} creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.
Mathador-LM\citep{kurtic-etal-2024-mathador} generates evaluation queries by adhering to the rules of Mathador games\citep{PUMA2023105587} and varying input numbers.
MMLU-CF~\citep{zhao2024mmlu} follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with "None of the other choices."


\paragraph{Table-Based}
S3Eval~\citep{lei-etal-2024-s3eval} evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.

\paragraph{Graph-Based}
In this category, LLMs are evaluated with randomly generated graphs.
For instance, DyVal~\citep{zhu2024dyval} assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs). 
The framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty. 
%For example, in arithmetic reasoning tasks, leaf nodes represent random numeric values, while edges correspond to randomly assigned arithmetic operators. 
These DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.
Similarly, NPHardEval~\citep{fan-etal-2024-nphardeval} evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP). 
Random graphs of varying sizes are synthesized as inputs for TSP to assess the LLM's performance.
\citet{xie2024memorization} automatically constructs Knights and Knaves puzzles with random reasoning graph.

% I think these two papars are not related.
% \zw{
% \paragraph{Structured Generation}
% Dynabench~\citep{geva2024dynabench} and Dynatask~\citep{liu2024dynatask} rethink benchmark construction by applying structured rules to create diverse test tasks.
% }

\paragraph{Limitations} 
The pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training~\citep{tu2024dice}.



\subsubsection{LLM-Based Generation}

\paragraph{Benchmark Rewriting}
In this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.
Auto-Dataset~\citep{ying2024automating} prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels~\citep{bloom1956handbook}.
StructEval~\citep{cao-etal-2024-structeval} expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.
ITD~\citep{zhu-etal-2024-inference} utilizes a contamination detector~\citep{shidetecting} to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.
VarBench~\citep{qian2024varbench} prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.


\paragraph{Interactive Evaluation} 
In this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM ~\citep{li2023beyond}.
LLM-as-an-Interviewer~\citep{kim2024llm} employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM's responses.
TreeEval~\citep{li2024treeeval} begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM’s response, it then generates follow-up subtopics and corresponding questions to further assess the model.
KIEval~\citep{yu-etal-2024-kieval} generates follow-up questions based on the evaluated model's response to an initial question from a static benchmark.

\paragraph{Multi-Agent Evaluation} 
Inspired by the recent success of multi-agents systems~\citep{guo2024large}, multi-agent collaborations are used to construct dynamic benchmarks.
Benchmark Self-Evolving~\citep{wang2024selfevolving} employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.
Given a task description, BENCHAGENTS~\citep{butt2024benchagents} leverages a multi-agent framework for automated benchmark creation. 
It splits the process into planning, generation, verification, and evaluation—each handled by a specialized LLM agent.
This coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks. 



\paragraph{Limitations}
The quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval~\citep{li2023avoiding} shows that about 10\% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.

\subsubsection{Hybrid Generation}
LatestEval~\citep{li2023avoiding} combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.
DARG~\citep{zhang2024darg} integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.
C$^2$LEVA~\citep{li2024c} incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.

