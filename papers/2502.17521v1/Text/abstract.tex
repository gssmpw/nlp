\begin{abstract}
Data contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from \textit{static} to \textit{dynamic} benchmarking.  
In this work, we conduct an in-depth analysis of existing \textit{static} to \textit{dynamic} benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance \textit{static} benchmarks and identify their inherent limitations. We then highlight a critical gapâ€”the lack of standardized criteria for evaluating \textit{dynamic} benchmarks. Based on this observation, we propose a series of optimal design principles for \textit{dynamic} benchmarking and analyze the limitations of existing \textit{dynamic} benchmarks.  
This survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts. 
We maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link~\footnote{\href{https://github.com/SeekingDream/Static-to-Dynamic-LLMEval}{\textcolor{violet}{GitHub Link: Static-to-Dynamic LLM Benchmarking}}}.

\end{abstract}
