\section{Discussions}
\label{sec:discussion}
\fakeparagraph{Key Insights} 
Our analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with \(\text{Pr}_{\text{contam}} \propto |\mathcal{D}_{\text{train}}| \cdot |\mathcal{D}_{\text{test}}|^{-1}\), rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.  
Second, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.

\fakeparagraph{Current Challenges}
Static benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.
% The field currently faces three major categories of challenges. 
% \textbf{Detection complexity}. Semantic contamination detection remains particularly challenging due to the difficulty in defining \(\text{sim}_{\text{sem}}(d, d')\) for arbitrary text pairs. The computational complexity of contamination detection scales with \(O(|\mathcal{D}_{\text{train}}| \times |\mathcal{D}_{\text{test}}|)\), making comprehensive analysis computationally intensive. Moreover, the field lacks standardized metrics for quantifying contamination severity across different levels.

% \textbf{Methodological limitations.} Static benchmarks face temporal degradation as \(\mathcal{D}_{\text{train}}\) evolves, while dynamic benchmarks struggle with reproducibility, where \(F(d) \neq F'(d)\) for different implementations. This creates a fundamental tension between maintaining benchmark consistency and preventing contamination. The design of benchmarks must carefully balance task difficulty with contamination prevention, often requiring complex trade-offs.

% \textbf{Efficiency and cost constraints.} Implementing robust contamination detection requires significant computational resources, while creating high-quality, contamination-free benchmarks demands substantial expert efforts. The limited availability of standardized tools for systematic contamination analysis further compounds these challenges.



\fakeparagraph{Future Directions}
Dynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.



% Our analysis reveals three promising research directions. 
% First, advancing \textit{detection methodologies} requires (1) Approximate nearest neighbor algorithms reducing semantic search complexity to \(\mathcal{O}(n^{0.5})\) via locality-sensitive hashing, (2) Multimodal detection frameworks combining textual (\(f_{\text{text}}\)) and structural (\(f_{\text{ast}}\)) signatures, and (3) Automated toolkits supporting configurable threshold analysis (\(\tau_{\text{syn}}, \tau_{\text{sem}}\)).

% Second, \textit{benchmark innovation} must develop hybrid architectures balancing static and dynamic elements through parametric control,
% \begin{equation}
% \mathcal{B}_{\text{hybrid}} = \alpha\mathcal{D}_{\text{static}} + (1-\alpha)\mathcal{G}_{\text{dynamic}}(\theta)
% \end{equation}
% where \(\alpha\) adapts to model capabilities. 
% Contamination-resistant generation requires adversarial training of benchmark generators (\(G_{\text{bench}})\) against reconstruction attacks.

% Third, \textit{standardization efforts} should establish (1) Normalized contamination metrics (\(\eta_{\text{contam}} = \frac{|\mathcal{D}_{\text{train}} \cap_{\text{sem}} \mathcal{D}_{\text{test}}|}{|\mathcal{D}_{\text{test}}|}\)), (2) Open-source frameworks for threshold-consistent (\(\delta_{\tau} \leq 0.05\)) contamination audits, and (3) Certified benchmark datasets with formal contamination bounds (\(\mathbb{P}_{\text{contam}} \leq \epsilon\)).

