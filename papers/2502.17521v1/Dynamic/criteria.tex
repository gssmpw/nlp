\subsection{Evaluation Criteria}

While many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.

\subsubsection{Correctness}
The first criterion for evaluating the quality of dynamic benchmarking is \textsf{Correctness}. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.  
We quantify the correctness of dynamic benchmarks as:  
$$
\small
    \textsf{Correctness} = \mathbb{E}_{i=1}^{N}  
    \mathcal{S} \big( \mathcal{Y}_i, \mathcal{G}(\mathcal{X}_i) \big)
$$
where \( \mathcal{X}_i \) and \( \mathcal{Y}_i \) represent the input and output of the \( i^{th} \) transformation, respectively. The function \( \mathcal{G}(\cdot) \) is an oracle that returns the ground truth  of its input, ensuring an objective reference for correctness evaluation. For example, the function \( \mathcal{G}(\cdot) \) could be a domain-specific an annotator.  
This equation can be interpreted as the expected alignment between the transformed dataset's outputs and their corresponding ground truth values, measured using the scoring function \( \mathcal{S}(\cdot) \). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.



\subsubsection{Scalability}
The next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:
$$
\small
    \textsf{Scalability} = \mathbb{E}_{i=1}^{N} \left[ \frac{\lVert T_i(\mathcal{D}) \rVert}{\lVert \mathcal{D} \rVert \times \textsf{Cost}(T_i)} \right]
$$
This represents the expectation over the entire transformation space, where \( \lVert T_i(\mathcal{D}) \rVert \) is the size of the transformed dataset, and \( \lVert \mathcal{D} \rVert \) is the size of the original dataset. The function \( \textsf{Cost}(\cdot) \) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.
This equation could be interpreted as the proportion of data that can be generated per unit cost.



\subsubsection{Collision}

One of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises:  \textit{If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?}
To evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of \textit{collision} in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark’s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:  

\[
\small
\begin{split}
    & \textsf{Collision Rate} = \mathbb{E}_{\substack{i, j = 1, \, i \neq j}}^{N}  
    \left[ \frac{\lVert \mathcal{D}_i \cap \mathcal{D}_j \rVert }{ \lVert \mathcal{D} \rVert} \right]\\
    & \textsf{Repeat} = \mathbb{E}_{i=1}^{N} \left[ k \mid k = \min \left\{ \bigcup_{j=1}^{k} \mathcal{D}_j \supseteq \mathcal{D}_i \right\} \right]
\end{split}
\]  
\textsf{Collision Rate} measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. \textsf{Repeat Trials} quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset \( T_i(\mathcal{D}) \), providing insight into the benchmark’s ability to produce novel variations.  
These metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.  


\subsubsection{Stable of Complexity}


Dynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal~\citep{zhu2024dyval} proposes applying graph complexity to evaluate the complexity of reasoning problems.
Formally, given a complexity measurement function \( \Psi(\cdot) \), the stability can be formulated as:  
\[
\small
    \textsf{Stability} = \text{Var}(\Psi(D_i))
\]  
This equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.  

% \subsubsection{Stability} 
% Even if a dynamic benchmark has a complexity fucntion for each of its generated data sample, it can also need to enure the benchmark is stable, 



\subsubsection{Diversity }
Besides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: \textsf{external diversity} and \textsf{internal diversity}:  External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.  
\[
    \small
    \begin{aligned}
        \textsf{External Diversity} &= \mathbb{E}_{i = 1}^{N} \Theta(\mathcal{D}_i, \mathcal{D}) \\
        \textsf{Internal Diversity} &= \mathbb{E}_{\substack{i, j = 1,  i \neq j}}^{N} \Theta(\mathcal{D}_i, \mathcal{D}_j)
    \end{aligned}
\]
where \( \Theta(\cdot) \) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores. 





\subsubsection{Interpretability} 
Dynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model's transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.


% \fakeparagraph{Augmentation ?}

% \clearpage