\section{Related Work}
This study categorizes existing industrial AD tasks into two primary categories: 1) 2D and 2) 3D AD, which are introduced as follows.
\subsection{2D Anomaly Detection} 
2D AD methods can be mainly categorized into reconstruction and embedding-based approaches. Reconstruction methods assume that models are unable to reconstruct anomalous images because of training only on normal images\cite{zavrtanik2021draem,zhang2024realnet,xu2024afscti3}. DRAEM \cite{zavrtanik2021draem} trains a U-Net network for reconstruction using artificially generated pseudo-anomalies and employs a discriminator network to distinguish anomalous regions. Embedding-based methods can be further divided into memory bank approaches and vision-language models approaches\cite{RD,roth2022patchcore}. Patchcore \cite{roth2022patchcore} uses a pre-trained feature extractor to build a greedy coreset of representative normal features. VLMs methods leverage the powerful generalization capabilities of pre-trained models\cite{jeong2023winclip,li2024promptad}. WinClip \cite{jeong2023winclip} utilizes the pre-trained CLIP model to effectively extract and aggregate multi-scale text-aligned features. PromptAD \cite{li2024promptad} addresses the limitations of manually setting text prompts by introducing prompt learning, thereby constructing suitable text prompts to guide the model in performing AD more effectively.
\subsection{3D Anomaly Detection} 
3D AD is a more challenging task than 2D AD due to its typically unordered nature and high level of sparsity. Derived from 2D AD methods, AD in 3D can also be divided into reconstruction and embedding-based. IMRNet, which is inspired by the masking strategy, trains a point cloud reconstruction network in a self-supervised manner\cite{IMR}.  R3D-AD uses a diffusion model to transform the point cloud reconstruction task into a conditional generation task, detecting anomalies through a trained reconstruction network\cite{r3dad}. M3DM detects and localizes anomalies through unsupervised feature fusion, combining multimodal information with multiple memory banks\cite{wang2023m3dm}. CPMF renders 3D point cloud data into 2D images from multiple viewpoints and builds a memory bank of complementary multimodal features\cite{cao2024cpmf}. Reg3DAD constructs a feature memory bank of normal point cloud samples using a pre-trained 3D feature extractor\cite{liu2024real3d}. CFM4IAD achieves mutual mapping between 2D and 3D features through fully connected layers, facilitating collaborative detection\cite{CFM4IAD}. The CLIP3D-AD method \cite{zuo2024clip3dad} leverages point cloud projection to eliminate the modality gap with the pre-trained CLIP \cite{CLIP} model, enhancing vision and language correlation through the fusion of multi-view image features.