\section{Preliminaries} \label{sec:preliminaries}

\subsection{Probelm Formulation}
Reinforcement Learning based locomotion control is commonly modeled as a Partially Observable Markov Decision Process (POMDP), characterized by the tuple $(\mathcal{S}, \mathcal{A}, \mathcal{O}, \mathcal{R})$. In this formulation, the state space $\mathcal{S}$ represents the full state of the robot and environment, including privileged information and accurate terrain maps, whereas the observation space $\mathcal{O}$ encompasses only partial and noisy observations obtained from onboard sensors. 

The control policy $\pi(a|o)$, typically represented by a neural network, maps observations $o \in \mathcal{O}$ to actions $a \in \mathcal{A}$. Given the reward functions $r \in \mathcal{R}$ and a discount factor $\gamma$, the policy is trained to maximize the expected cumulative return:
\begin{equation}
    J(\pi) =  \mathbb{E}_{a_t \sim \pi(o_t)}[\sum_{t} ^ \infty \gamma^t r(s_t, a_t)].
\end{equation}

In this work, we address a more challenging POMDP task where the partial observations $\mathcal{O}$ include a potentially unreliable component $o_v$, which can fail under certain conditions, leading to significant penalties or even task termination. However, completely discarding $o_v$ would substantially limit the theoretical performance of the policy. The ideal solution is to enable the policy to recognize when $o_v$ becomes unreliable and switch to relying solely on the reliable proprioceptive observations $o_p$. We propose a composite solution to address this challenge in this work.


\subsection{Return Estimation}

Building upon previous research, we employ Proximal Policy Optimization (PPO) to train the locomotion policy for its effectiveness in continuous control. In addition to the actor policy $\pi(a|o)$, we emphasize that a well-trained policy also provides a practical approximation of the return $Q(s, a \sim \pi)$ during the Generalized Advantage Estimation (GAE) ~\cite{schulman2015high} process. The approximated composite returns will be implemented to develop the proposed policy composition mechanism.

\subsection{Q-informed Policies Composition}

Given a set of policies $\Pi = \{\pi_1, \pi_2, \dots, \pi_n \}$ that share the same states, actions, and rewards $(\mathcal{S}, \mathcal{A}, \mathcal{R})$, a composite policy $\tilde{\pi}$ selects an action from the proposed action set $A = \{ a_i \sim \pi_i(s) \}$ with a probability $P_w$ that is related to their respective potential utilities. In the context of Markov Decision Process, it has been proved \cite{zhang2023policy} that selecting actions based on the cumulative return at current states and candidate actions will achieve the best expected return for $\tilde{\pi}$. To this end, the Q-value based policies composition will compute the cumulative return at current for each low-level policy $\bold{Q} = \{ Q_i(s, a_i) | a_i \in \mathcal{A} \}$ and construct a categorical distribution to select the final action:

\begin{equation}
    P_w(i)= \frac{exp(Q_i(s, a_i)/\alpha)}{\sum_j exp(Q_j(s, a_j)/\alpha)},
\end{equation}
here $\alpha$ is the temperature. In the case of two sub-policies, such composition will assign a higher probability to the action with the higher Q-value at the current state.

\section{Method} \label{sec:method}

\begin{figure*}[h]
\centering{\includegraphics[width=\textwidth]{figures/method.png}}
\caption{Overview of our framework: In VB-Com, we develop two locomotion policies—one perceptive and one non-perceptive—through single-stage training. These sub-policies are integrated based on two return estimators, which predict future returns given the current state for each of the policy policy. This integration enables seamless policy switching, allowing the robot to effectively adapt to varying levels of perceptual information and handle dynamic environments.}
\label{method}
\end{figure*}


\subsection{System Overview}

The proposed VB-Com framework (Fig \ref{method}) comprises a perceptive locomotion policy $\pi_v$ and a non-perceptive policy $\pi_b$. $\pi_v$ incorporates visual observations to enable perceptive locomotion, allowing humanoid robots to traverse complex terrains such as high steps, gaps, hurdles, and to perform obstacle avoidance. $\pi_b$ is trained within the same reward and action space but does not accept external observations.

Once well-trained, $\pi_v$ and $\pi_b$ are expected to operate stably on familiar terrains with corresponding observations provided during training. Under such conditions, the VB-Com framework prioritizes selecting actions from the vision-based policy $\pi_v$ due to its more comprehensive observation of the environment and higher expected return. However, when the robot encounters outlier scenarios, such as perceptive deficiencies that the environments do not interact with the robot as predicted by the vision-based observations $o_v$, the blind policy $\pi_b$ takes over, leveraging the relatively reliable proprioceptive observations $o_p$ to navigate such situations effectively.

VB-Com achieves the mentioned composition with two return estimators, $\pi_v^e$ and $\pi_b^e$, trained concurrently with the locomotion policies. The estimators provide approximations of the cumulative return that the system will obtain whther chooses the vision or blind policy at the current step. During deployment, the compositor operates based on the estimations of the returns $\{\hat{G}^e_v \sim \pi_v^e, \hat{G}^e_b \sim \pi_b^e \}$, from which one executed action is selected from the candidate actions $\{a_v \sim \pi_v, a_b \sim \pi_b \}$.
 
\subsection{Locomotion Policies}

To demonstrate the quick responsiveness of VB-Com in handling deficient perception, we train the locomotion policies on challenging terrains, including gaps, hurdles, and high walls (for obstacle avoidance), which require high joint velocities for traversal. This contrasts with the more common scenarios, such as stairs and discrete steps, which have been the focus of prior works. Drawing from previous experience, we adopt a goal-reaching formulation rather than velocity tracking to train the policies, as this approach is better suited for completing highly dynamic tasks.

\subsubsection{Observation Space}

The policy observations $o_t$ consist of two components: $o_p$, which includes the commands and proprioceptive observations, and $o_v$, which represents the visual observations.

The commands are designed following \cite{cheng2024extreme}, where directions are computed using waypoints placed on the terrain: $\textbf{d} = (\textbf{p} - \textbf{x}) / ||\textbf{p} - \textbf{x}||$, with $\textbf{p}$ and $\textbf{x}$ representing the locations of the waypoints and the robot, respectively. To prevent sharp directional changes, the robot is provided with directions to the next two goal locations at each step, along with linear velocity commands $\textbf{v}_c$. These commands are represented as a three-dimensional vector: $\textbf{c}_t = [\textbf{d}_1, \textbf{d}_2, \textbf{v}_c]$. The proprioceptive observations consists of its joint position $\mathbf{\theta}_t$, joint velocity $\dot{\mathbf{\theta}}_t$, base angular velocity $\mathbf{\omega}_t$ and gravity direction in robot frame $\mathbf{g}_t$. The perceptive information $o_v$ is a robotic-centric height map around the robot, as the hardware implementation detailed in ~\cite{long2024learning}. The perceptive observation is not provided while training the blind policy $\pi_b$.

As stated in the problem formulation, $o_t$ represents the observation space of the POMDP. Therefore, both $o_p$ and $o_v$ are domain-randomized during training to better simulate sensor noise encountered in real-world scenarios. On the other hand, the critic network, which is responsible for providing the actor policy with accurate state evaluations, is allowed to access privileged information. Building upon previous research, we incorporate the accurate linear velocity $v_t$, which plays an essential role in legged locomotion tasks, as the additional privileged information. Meanwhile, the proprioceptive and perceptive states used in the critic network are not subjected to noise. Additionally, we provide a larger height map to the critic network ($1.6m \times 1.0m$) compared to the one used in the actor network ($1.2m \times 0.7m$), which we found facilitates faster adaptation of the robot to higher terrain levels during curriculum learning.

\subsubsection{Rewards}

The majority of our reward functions are adapted from \cite{long2024learning, cheng2024extreme}. To align with the goal-reaching commands in the observation space, we modify the task reward to track the target direction and linear velocity commands instead of angular velocity. We also include a series of regularization rewards to encourage the humanoid robot to exhibit natural locomotion and maintain gentle contact with the ground. In addition, unlike previous research that treats obstacle avoidance as a path planning problem, we enable the robot to autonomously reach its goal while avoiding obstacles at the locomotion level. This is achieved through a carefully balanced trade-off between goal-reaching rewards and collision penalties. To acquire an accurate return estimation, we focus the rewards on the proprioceptive states of the robot rather than interactions with the environment. The reward scales for both the blind and vision policies remain consistent throughout the learning process. We achieve a unified policy capable of simultaneously traversing obstacles, hurdles, and gaps through the proposed reward setting.

\subsubsection{State Estimation}
A variety of approaches have been proposed in legged locomotion to address POMDPs by constructing a belief state from historical observations, often involving a second-stage training process or complex network structure. In this work, we propose an efficient and simple state estimator that predicts the next velocity $v_{t+1}$ based on the historical observation sequence $o_{t-H:t}$. Both $v_{t+1}$ and $o_{t-H:t}$ are rolled out online from the collected trajectories while the policy is being updated. A regression loss is used to update the state estimator. We demonstrate that, with the state estimator, a hardware-deployable locomotion policy can be achieved through a single stage of training, enabling agile locomotion tasks with high effectiveness and data efficiency.

\subsection{Vision-Blind Composition}
\label{subsec:vb-com}
Given the vision policy $\pi_v$ and the blind policy $\pi_b$, the composition can be viewed as a discrete policy $\tilde{\pi}$ with an action dimension of two, selecting between the candidate actions:

\begin{equation}
    \tilde{\pi}(a|s) = [a_b \sim \pi_b, a_v \sim \pi_v ]\textbf{w}, \textbf{w} \sim P_\textbf{w},
\end{equation}
Building on the analysis of Q-informed policy composition, for each state $s_t$ at each step, we have:
\begin{equation}
    P_\textbf{w}(i|s_t,a_v,a_b) \propto exp(Q(s_t,a_i)), a_i \in \{a_v,a_b\} \label{propto}.
\end{equation}

\subsubsection{Policy Return Estimation}
Given the current states $s_t$ of the robot, we can estimate the expected cumulative return $G_{\pi_i}(s_t)$ for each policy to guide the composition process. In practice, switching between the two independently trained policies could cause abrupt changes in the performed actions. For example, a switch from the vision policy to the blind policy should help the robot avoid falling into an unseen gap, which may require a sequence of actions from the blind policy without the involvement of vision. To address this, we introduce a switch period $T$, which acts as the control unit for each switch. The introduction of $T$ also helps decouple the switching actions, approximately making them temporally independent of each other.

To this end, we expect the return estimator to be responsible for estimating a time sequence of expected returns over the duration $T$, such that:
\begin{equation}
    L_{\pi_i} = \mathbb{E}_t[\hat{G}_{\pi_i}^e(s_t) - G_{\pi_i}(s_{t:{t+T}})],
\end{equation}
To achieve the estimation with reduced bias and variance, we implement $\lambda$-return to weight the time-sequenced returns within one switch period as follows:
\begin{equation}
    G_{\pi_i}(s_{t:{t+T}}) \approx G^\lambda_{\pi_i}(s_{t:{t+T}}) = (1-\lambda)\sum_{n=t}^{t+T}\lambda^{n-1}G_{\pi_i}(s_{n}),
\end{equation}
which represents a weighted return if the robot chooses to switch to the low-level policy $\pi_i$ given the state $s_t$. In addition, in order to mitigate the large variance between single-step rewards and prevents the policy from overfitting to recent batches, $G_{\pi_i}$ is computed based on the update of value functions ~\cite{schulman2015high}, where $G_{\pi_i}(s_t) = \hat{A}(s_t) + V(s_t)$, with $\hat{A}(s_t)$ being the advantage function and $V(s_t)$ the value function.

Since the return estimators need to be deployable on hardware and we aim to mitigate perception misleadings, we avoid using exteroceptive observations or privileged information as inputs. Instead, we use the historical proprioceptive observation sequence $o_{p_{t-H:t}}$ as the input to the return estimator $\pi^e$.

\subsubsection{Policy Switch}
Unlike previous works that construct a switch-based hierarchical framework to keep the robot within a safe domain and prevent potential collisions, VB-Com performs policy switching to recover the robot from getting stuck due to perceptive deficiencies.

Ideally, equation \ref{propto} provides the theoretical basis for choosing the action with the greater value estimation $\hat{G}_\pi^e$ at the current state. This aligns with the fact that $\pi_v$ typically yields higher returns than $\pi_b$ as long as the vision observations are consistent with those seen during training, since $\pi_v$ has access to more comprehensive environmental observations.

During deployment, when the robot experiences a sudden environmental change that disrupts locomotion, both estimations $\hat{G}_{\pi_{v,b}}^e$ will decline. We observe that in these situations, it is difficult to maintain strict monotonicity such that $\hat{G}^e_{\pi_b} > \hat{G}^e_{\pi_v}$ due to the return approximation error introduced by $\pi^e$. Meanwhile, the blind policy demonstrates greater sensitivity to unstable motions, as the low-return samples are more frequently encountered even after the policy has been well-trained, compared to $\pi_v$ (as illustrated in Fig. \ref{return}). To address this, we introduce a threshold $G_{th}$ trigger that can also prompt the policy switch.
\begin{equation}
a \sim \tilde{\pi}(s_t) = \left\{
\begin{array}{ll}
a_v, & \text{if } G^e_{\pi_v}(s_t) > G^e_{\pi_b}(s_t)> G_{th} , \\
a_b, & \text{otherwise,}
\end{array}
\right.
\end{equation}
\begin{equation}
G_{th} =  1/5\sum_{t-5}^{t}{G_{\pi_b}^e(s_i)} - \alpha, 
\end{equation}
here $\alpha$ is a threshold hyperparameter. In practice, we replace $G^e_{\pi_v}(s_t)$ with a smoothed window (length 5) to avoid sudden abnormal estimations, which we have found to be effective in real robot deployments. Additionally, a switch will not be performed under conditions of high joint velocity to prevent potential dangers caused by the abrupt switching of policies when the robot is performing vigorous motion.

 

\begin{figure}[htbp]
\centering{\includegraphics[width=0.5\textwidth]{figures/robot.png}}
\caption{We train the proposed framework on Unitree G1 and H1 humanoid robots with the \textcolor{olive}{enabled collisions links $(C_l^e)$}.}
\label{robot}
\end{figure}
\subsection{Implementation Details}

\subsubsection{Humanoid Robots}
We implement VB-Com on two humanoid robots, Unitree-G1 (G1) and Unitree-H1 (H1), in both simulation and real-world environments (Fig \ref{robot}). Both robots are controlled using whole-body actions, with G1 having 20 control actions (10 upper-body and 10 lower-body joints) and H1 having 19 control actions (8 upper-body, 10 lower-body joints, and 1 torso joint). Since enabling all collision links for the robot can result in significant computational overhead (especially for G1), we activate a subset of collision links $(C_l^e)$ sufficient to accomplish the locomotion tasks, particularly for the blind policy where prior contact is necessary. For example, by enabling the hand collision on G1, the robot learns to reach out its hands to touch potential obstacles and avoid them once perception becomes deficient.

\subsubsection{Perception \& noise}
We implement a robotic-centric elevation map on both G1 and H1 to acquire external state observations for the vision policy. The lidars mounted on the robots' heads serve as onboard sensors. Since the elevation map requires a smooth time window to integrate newly observed point clouds into the map, it struggles with dynamic scenes, presenting vision-deficient challenges that can be effectively addressed by VB-Com.

We also introduce standard noise during the training of $\pi_b$ to enhance its tolerance against deficient perception (Training Noise in Fig \ref{noise}). This includes $10\%$ Gaussian noise and random perception delays within the past $0.5$ seconds. The added perception noise aims to achieve relatively deployable performance for $\pi_v$. However, we demonstrate that $\pi_v$ fails to handle a wider range of perception noise or dynamic obstacles encountered in real-world scenarios.



