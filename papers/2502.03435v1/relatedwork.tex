\section{Related work}
\label{sec:related-work}
\paragraph{Implicit bias of minima stability.} Minima stability for (stochastic) gradient descent was studied in several previous works. \citet{wu2018sgd} provide a sufficient condition for a twice-differentiable stable minimum, connecting the maximal eigenvalue of the Hessian of the risk with the learning rate. \citet{mulayoff2021implicit} further investigate non-differentiable minima. In addition, \citet{qiao2024stableminimaoverfitunivariate} prove a generalization bound for twice-differentiable stable minima, by relating the condition on the Hessian to the functions that can be represented by the neural network. In the present paper, we leverage this minima stability framework in the setting of denoising score matching.

\paragraph{Memorization effect of diffusion models.} Diffusion models were found to generate replicas of their training data \citep[see, e.g.,][]{carlini2023extracting,somepalli2023diffusion, somepalli2023understanding}, raising privacy and security concerns. Following these initial observations, a series of papers quantified this memorization phenomenon \citep{gu2023memorization,yoon2023diffusion,kadkhodaie2023generalization}. These articles experimentally demonstrate a transition from memorization to generalization as the sample size increases, showing that with practical sample sizes, the extent of memorization is limited. Furthermore, \citet{kadkhodaie2023generalization} link the generalization ability of diffusion models to their adaptability to the underlying geometric structure of the data. Finally, \citet{gu2023memorization}, \citet{yi2023generalization}, and \citet{li2024good} show that diffusion models with the empirical optimal score exhibit full memorization.  

\paragraph{Regularization of denoising score matching.}
In practice, several methods can be used to mitigate memorization. 
Regularization techniques like weight decay, dropout, or data corruption, can help reduce the model’s dependency on specific data points \citep{daras2024ambient,gu2023memorization}.  All these methods rely on explicitly regularizing the training process. On the contrary, the present paper studies the \textit{implicit} regularization effect of the learning rate in denoising score matching, in order to explain the moderate amount of memorization observed in practice even without explicit regularization. The work by \citet{zeno2024minimum} is more closely aligned with our approach. They derive a closed-form formula for the minimum-norm interpolator of the 1d denoising problem and analyze its generalization properties. We adopt a complementary approach, focusing on SGD stability rather than interpolation and minimum-norm representation. Finally, our analysis supports experimental evidence by \citet{li2024understanding}, who observe that diffusion models capable of generalization tend to learn near-linear scores. Indeed, we show that the learning rate constrains the learned score’s nonlinearity (via the total variation of its derivative), thus preventing full memorization.