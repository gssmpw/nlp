\section{Related Work}
Recurrent Neural Networks (RNNs), particularly Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) architectures, have been widely employed in sequential learning tasks due to their ability to capture temporal dependencies. However, traditional gating mechanisms in these models primarily rely on local context using only the "current" input and "hidden state" for memory updates which can limit their effectiveness in modeling long-range dependencies \cite{gu2020improving}. Some researchers proposed refinements on the gating mechanisms to enhance gradient flow and learning \cite{lu2017simplified,gu2020improving,cheng2020refined}. At the same time, the concept of path signatures, derived from rough path theory, has emerged as a key mathematical tool for deciphering the geometric structure of sequential data \cite{lyons1998differential}. These signatures have been successfully adapted in neural architectures, enhancing their ability to process complex temporal patterns. In particular, \cite{sabate2020solving} showed that integrating these signatures into LSTM networks optimized the solution of path-dependent partial differential equations, with applications in finance. In the same vein, \cite{moreno2024rough} recently proposed Rough Transformers, models exploiting path signatures to improve the analysis of continuous-time data. Recent studies have also investigated the application of path signatures in RNN-based models. \cite{gu2020improving} introduced alternative gating mechanisms to address saturation issues in traditional RNNs, while path-based feature extraction has been utilized in areas such as personalized user modeling (e.g., Sequential Path Signature Networks) and path classification tasks. These signatures have been also leveraged in classification tasks in finance to build a novel approach to portfolio construction for digital assets \cite{inzirillo2024clustering}. These advancements indicate that incorporating path signatures into deep learning frameworks can enhance the representation of both short-term and long-term sequences.  Building upon these advances, our work proposes a novel SigLSTM and SigGRU, where we replace the forget and reset gates, respectively, with learnable path signatures. This allows for richer contextual modeling by incorporating the full historical trajectory into memory updates. Our approach extends prior work by leveraging path signatures in a fully learnable manner within standard recurrent architectures, improving performance across diverse sequence modeling and time series forecasting tasks.