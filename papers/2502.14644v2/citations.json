[
  {
    "index": 0,
    "papers": [
      {
        "key": "kitaev2020reformer",
        "author": "Kitaev, Nikita and Kaiser, {\\L}ukasz and Levskaya, Anselm",
        "title": "Reformer: The efficient transformer"
      },
      {
        "key": "wang2020linformer",
        "author": "Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao",
        "title": "Linformer: Self-attention with linear complexity"
      },
      {
        "key": "beltagy2020longformer",
        "author": "Beltagy, Iz and Peters, Matthew E and Cohan, Arman",
        "title": "Longformer: The long-document transformer"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "shen2021efficient",
        "author": "Shen, Zhuoran and Zhang, Mingyuan and Zhao, Haiyu and Yi, Shuai and Li, Hongsheng",
        "title": "Efficient attention: Attention with linear complexities"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "gu2022efficientlymodelinglongsequences",
        "author": "Albert Gu and Karan Goel and Christopher R\u00e9",
        "title": "Efficiently Modeling Long Sequences with Structured State Spaces"
      },
      {
        "key": "NEURIPS2018_5cf68969",
        "author": "Rangapuram, Syama Sundar and Seeger, Matthias W and Gasthaus, Jan and Stella, Lorenzo and Wang, Yuyang and Januschowski, Tim",
        "title": "Deep State Space Models for Time Series Forecasting"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "gu2023mamba",
        "author": "Gu, Albert and Dao, Tri",
        "title": "Mamba: Linear-time sequence modeling with selective state spaces"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "lewis2020retrieval",
        "author": "Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\\\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\\\"a}schel, Tim and others",
        "title": "Retrieval-augmented generation for knowledge-intensive nlp tasks"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "xu2023retrieval",
        "author": "Xu, Peng and Ping, Wei and Wu, Xianchao and McAfee, Lawrence and Zhu, Chen and Liu, Zihan and Subramanian, Sandeep and Bakhturina, Evelina and Shoeybi, Mohammad and Catanzaro, Bryan",
        "title": "Retrieval meets long context large language models"
      },
      {
        "key": "jiang2024longrag",
        "author": "Jiang, Ziyan and Ma, Xueguang and Chen, Wenhu",
        "title": "Longrag: Enhancing retrieval-augmented generation with long-context llms"
      },
      {
        "key": "wang2024augmenting",
        "author": "Wang, Weizhi and Dong, Li and Cheng, Hao and Liu, Xiaodong and Yan, Xifeng and Gao, Jianfeng and Wei, Furu",
        "title": "Augmenting language models with long-term memory"
      },
      {
        "key": "jin2024llm",
        "author": "Jin, Hongye and Han, Xiaotian and Yang, Jingfeng and Jiang, Zhimeng and Liu, Zirui and Chang, Chia-Yuan and Chen, Huiyuan and Hu, Xia",
        "title": "Llm maybe longlm: Self-extend llm context window without tuning"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "li2023functional",
        "author": "Li, Shanda and You, Chong and Guruganesh, Guru and Ainslie, Joshua and Ontanon, Santiago and Zaheer, Manzil and Sanghai, Sumit and Yang, Yiming and Kumar, Sanjiv and Bhojanapalli, Srinadh",
        "title": "Functional interpolation for relative positions improves long context transformers"
      },
      {
        "key": "su2024roformer",
        "author": "Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng",
        "title": "Roformer: Enhanced transformer with rotary position embedding"
      },
      {
        "key": "peng2023yarn",
        "author": "Peng, Bowen and Quesnelle, Jeffrey and Fan, Honglu and Shippole, Enrico",
        "title": "Yarn: Efficient context window extension of large language models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "chen2023extending",
        "author": "Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong",
        "title": "Extending context window of large language models via positional interpolation"
      },
      {
        "key": "kazemnejad2024impact",
        "author": "Kazemnejad, Amirhossein and Padhi, Inkit and Natesan Ramamurthy, Karthikeyan and Das, Payel and Reddy, Siva",
        "title": "The impact of positional encoding on length generalization in transformers"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "kitaev2020reformer",
        "author": "Kitaev, Nikita and Kaiser, {\\L}ukasz and Levskaya, Anselm",
        "title": "Reformer: The efficient transformer"
      },
      {
        "key": "wang2020linformer",
        "author": "Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao",
        "title": "Linformer: Self-attention with linear complexity"
      },
      {
        "key": "beltagy2020longformer",
        "author": "Beltagy, Iz and Peters, Matthew E and Cohan, Arman",
        "title": "Longformer: The long-document transformer"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "shen2021efficient",
        "author": "Shen, Zhuoran and Zhang, Mingyuan and Zhao, Haiyu and Yi, Shuai and Li, Hongsheng",
        "title": "Efficient attention: Attention with linear complexities"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "gu2023mamba",
        "author": "Gu, Albert and Dao, Tri",
        "title": "Mamba: Linear-time sequence modeling with selective state spaces"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "vaswani2017attention",
        "author": "Vaswani, A",
        "title": "Attention is all you need"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "lewis2020retrieval",
        "author": "Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\\\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\\\"a}schel, Tim and others",
        "title": "Retrieval-augmented generation for knowledge-intensive nlp tasks"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "xu2023retrieval",
        "author": "Xu, Peng and Ping, Wei and Wu, Xianchao and McAfee, Lawrence and Zhu, Chen and Liu, Zihan and Subramanian, Sandeep and Bakhturina, Evelina and Shoeybi, Mohammad and Catanzaro, Bryan",
        "title": "Retrieval meets long context large language models"
      },
      {
        "key": "jiang2024longrag",
        "author": "Jiang, Ziyan and Ma, Xueguang and Chen, Wenhu",
        "title": "Longrag: Enhancing retrieval-augmented generation with long-context llms"
      },
      {
        "key": "wang2024augmenting",
        "author": "Wang, Weizhi and Dong, Li and Cheng, Hao and Liu, Xiaodong and Yan, Xifeng and Gao, Jianfeng and Wei, Furu",
        "title": "Augmenting language models with long-term memory"
      },
      {
        "key": "jin2024llm",
        "author": "Jin, Hongye and Han, Xiaotian and Yang, Jingfeng and Jiang, Zhimeng and Liu, Zirui and Chang, Chia-Yuan and Chen, Huiyuan and Hu, Xia",
        "title": "Llm maybe longlm: Self-extend llm context window without tuning"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "liu2021ttt++",
        "author": "Liu, Yuejiang and Kothari, Parth and Van Delft, Bastien and Bellot-Gurlet, Baptiste and Mordan, Taylor and Alahi, Alexandre",
        "title": "Ttt++: When does self-supervised test-time training fail or thrive?"
      },
      {
        "key": "gandelsman2022test",
        "author": "Gandelsman, Yossi and Sun, Yu and Chen, Xinlei and Efros, Alexei",
        "title": "Test-time training with masked autoencoders"
      },
      {
        "key": "osowiechi2023tttflow",
        "author": "Osowiechi, David and Hakim, Gustavo A Vargas and Noori, Mehrdad and Cheraghalikhani, Milad and Ben Ayed, Ismail and Desrosiers, Christian",
        "title": "Tttflow: Unsupervised test-time training with normalizing flow"
      },
      {
        "key": "hong2023mecta",
        "author": "Hong, Junyuan and Lyu, Lingjuan and Zhou, Jiayu and Spranger, Michael",
        "title": "Mecta: Memory-economic continual test-time model adaptation"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "sun2024learning",
        "author": "Sun, Yu and Li, Xinhao and Dalal, Karan and Xu, Jiarui and Vikram, Arjun and Zhang, Genghan and Dubois, Yann and Chen, Xinlei and Wang, Xiaolong and Koyejo, Sanmi and others",
        "title": "Learning to (learn at test time): Rnns with expressive hidden states"
      },
      {
        "key": "behrouz2024titans",
        "author": "Behrouz, Ali and Zhong, Peilin and Mirrokni, Vahab",
        "title": "Titans: Learning to Memorize at Test Time"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "sun2020test",
        "author": "Sun, Yu and Wang, Xiaolong and Liu, Zhuang and Miller, John and Efros, Alexei and Hardt, Moritz",
        "title": "Test-time training with self-supervision for generalization under distribution shifts"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "sun2024learning",
        "author": "Sun, Yu and Li, Xinhao and Dalal, Karan and Xu, Jiarui and Vikram, Arjun and Zhang, Genghan and Dubois, Yann and Chen, Xinlei and Wang, Xiaolong and Koyejo, Sanmi and others",
        "title": "Learning to (learn at test time): Rnns with expressive hidden states"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "bertsch2024context",
        "author": "Bertsch, Amanda and Ivgi, Maor and Alon, Uri and Berant, Jonathan and Gormley, Matthew R and Neubig, Graham",
        "title": "In-context learning with long-context models: An in-depth exploration"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "wang2024greater",
        "author": "Wang, Y and Ma, D and Cai, D",
        "title": "With Greater Text Comes Greater Necessity: Inference-Time Training Helps Long Text Generation"
      }
    ]
  }
]