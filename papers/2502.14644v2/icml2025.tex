%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
\usepackage[dvipsnames]{xcolor}         % colors
\usepackage{multirow}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{tcolorbox}
\usepackage{booktabs} 
% \usepackage[colorlinks,citecolor=lightgray]{hyperref} 
\usepackage{bbding}
\usepackage{url}
\usepackage{multirow} 
\usepackage{pifont}
\usepackage{makecell}
% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\newcommand{\jiaqi}[1]{\textcolor{blue}{[LJQ] #1}}
\newcommand{\mys}[1]{\textcolor{orange}{[MYS] #1}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\usepackage{enumitem}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}

\twocolumn[
\icmltitle{LIFT: Improving Long Context Understanding of Large Language Models through Long Input Fine-Tuning}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Yansheng Mao}{equal,iai}
\icmlauthor{Yufei Xu}{equal,iai,bigai}
\icmlauthor{Jiaqi Li}{equal,bigai}
\icmlauthor{Fanxu Meng}{iai,bigai}
\icmlauthor{Haotong Yang}{iai}
\icmlauthor{Zilong Zheng}{bigai}
\icmlauthor{Xiyuan Wang}{iai}
\icmlauthor{Muhan Zhang}{iai,bigai}
\end{icmlauthorlist}

\icmlaffiliation{bigai}{State Key Laboratory of General Artificial Intelligence, BIGAI}
\icmlaffiliation{iai}{Institute for Artificial Intelligence, Peking University}

\icmlcorrespondingauthor{Muhan Zhang}{muhan@pku.edu.cn}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Long context understanding remains challenging for large language models due to their limited context windows.
This paper presents \textbf{Long Input Fine-Tuning (LIFT)}, a novel framework for long-context modeling that can improve the long-context performance of arbitrary (short-context) LLMs by dynamically adapting model parameters based on the long input. Importantly, LIFT, rather than endlessly extending the context window size to accommodate increasingly longer inputs \textbf{in context}, chooses to store and absorb the long input \textbf{in parameter}.
%facilitates efficient processing of lengthy inputs, eliminating the computational overhead of offline long-context adaptation or iterative retrieval-augmented generation.
By fine-tuning the long input into model parameters, LIFT allows short-context LLMs to answer questions even when the required information is not provided in the context during inference. %, enhancing both the memorization and comprehension capabilities.
Furthermore, to enhance LIFT performance while maintaining the original in-context learning (ICL) capabilities, we introduce Gated Memory, a specialized attention adapter that automatically balances long input memorization and ICL.
%(highlight performance on benchmarks)
We provide a comprehensive analysis of the strengths and limitations of LIFT on long context understanding, offering valuable directions for future research.

% Long context understanding remains challenging for large language models due to their limited context windows. This paper introduces \textbf{L}ong \textbf{I}nput \textbf{F}ine-\textbf{T}uning (\textbf{LIFT}) for long context modeling, a novel framework that enhances LLM performance on long-context tasks by adapting model parameters to the context at test time. LIFT enables efficient processing of lengthy inputs without the computational burden of offline fine-tuning. The framework is further enhanced by integrating in-context learning and pre-LIFT supervised fine-tuning.
% %Experimental results highlight consistent improvements across popular benchmarks.
% The combination of in-context learning and LIFT enables short-context models like Llama 3 to handle arbitrarily long contexts and consistently improves their performance on popular long-context benchmarks like LooGLE and LongBench. We also provide a comprehensive analysis of the strengths and limitations of LIFT on long context understanding, offering valuable directions for future research.
\end{abstract}


\title{LIFT: Improving Long Context Understanding of Large Language Models through Long Input Fine-Tuning}

% \author{Yansheng Mao$^{1,}$\thanks{Equal contributions.}, Jiaqi Li$^{2,*}$, Fanxu Meng$^{1,2}$, Jing Xiong$^{1}$, Zilong Zheng$^{2}$, Muhan Zhang$^{1,2,}$\thanks{Correspondence to Muhan Zhang (\texttt{muhan@pku.edu.cn})}\\
% $^{1}$ Institute for Artificial Intelligence, Peking University \\
% $^{2}$ National Key Laboratory of General Artificial Intelligence, BIGAI \\
% }

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}




\section{Introduction}
% \textbf{Para1: why long context is important, problems/challenges in long context}

% \textbf{1) Importance}


\begin{table*}[t]
    \centering
    \caption{Comparison of conventional long context understanding approaches with LIFT.}
    \label{tab:comparison}
    {\begin{tabular}{l|ccc}
    \toprule
     & RAG & ICL & LIFT \\
    \midrule
    Knowledge storage & External data sources & Within context window & In parameters \\
    Input length &Infinite & Limited & Infinite \\
    Retrieval free & \textcolor{red}{\ding{55}} & \textcolor{green}{\ding{51}} & \textcolor{green}{\ding{51}} \\
    Long-context adaptation free & \textcolor{green}{\ding{51}} & \textcolor{red}{\ding{55}} &\textcolor{green}{\ding{51}} \\
    % Capability of long-context inference &\textcolor{red}{\ding{55}} & \textcolor{red}{\ding{55}}  & \textcolor{green}{\ding{51}}  \\
    \bottomrule
    \end{tabular}}
    \vspace{-8pt}
\end{table*}


Large Language Models (LLMs), such as GPT-4~\citep{achiam2023gpt}, have revolutionized the field of natural language processing, driving breakthroughs in text generation and significant advancements in tasks like translation, summarization, and conversation. Long sequences, which can span up to millions of tokens, are common in real-world applications, including long books~\citep{kovcisky2018narrativeqa}, accounting documentsdocumentsdocuments~\citep{li2024enhancing}, high-resolution videos~\citep{wu2024longmemeval, tapaswi2016movieqa}, and audio signals~\citep{yang2024air}. Extending context windows allows models to capture dependencies across larger text spans and improve coherence, understanding, and accuracy in tasks that require reasoning over extended inputs.

% \textbf{2) Challenges}

However, as context lengths increase, the computational complexity of the self-attention mechanism~\citep{vaswani2017attention} grows quadratically, which limits models' ability to process long inputs. Additionally, storing a large number of attention weights and intermediate states like KV cache places a heavy burden on hardware resources. Moreover, it is challenging to capture long dependencies among pieces of information scattered throughout long inputs while performing further comprehension and reasoning. Due to the limitation of context windows, LLMs can hardly capture the overall information about a user's query history or task input, resulting in suboptimal performance.

To address these challenges, researchers have developed various techniques to improve the long-context abilities of LLMs.
%such as utilizing efficient Transformers, external memory, recurrent memory, etc.~\citep{lu2024controlled, wang2024beyond, huang2023advancing, xiong2023effective}. 
% These advancements represent the ongoing effort to improve long-context understanding in LLMs.
% \textbf{Para2: Existing works of long context understanding, Limitation, highlight ICL}
Retrieval-Augmented Generation (RAG)~\citep{lewis2020retrieval, xu2023retrieval} and prompt compression~\citep{jiang2023longllmlingua} aim to preprocess long inputs within a limited short context window by adaptive retrieval or text compression~\citep{el2021automatic}. However, the effectiveness of these methods depends on the precision and relevance of the contextual information provided within the context window. 
% LLMs struggle to prioritize the most relevant parts of the input, resulting in suboptimal utilization of the input.
It will lead to further hallucinations when noisy, ambiguous, or conflicting information is provided.
Long-context adaptation focuses on fine-tuning pretrained LLMs on corpora of long texts to extend their context windows~\citep{chen2023longlora, peng2023yarn} and is more frequently used in more recent works. However, the adaptation process comes with significant costs in terms of both training data and computational resources.
%and they are still insufficient to enable models to both capture the fine-grained information in a global view as well as extraction of precise details of specific events or entity lying in the lengthy text.
Additionally, with the extended context window, the cost of processing and generating long texts grows quadratically with the input length. Finally, despite the extension, the context windows of these LLMs remain finite, preventing them from generalizing to inputs of infinite length.

% \textbf{Para3: Our contributions}

To address the above challenges, in this paper, we present a novel framework \textbf{L}ong \textbf{I}nput \textbf{F}ine-\textbf{T}uning (LIFT), designed to enhance the long-context capabilities of arbitrary (short-context) models by directly adapting model parameters to the long input. Our approach has the following advantages:
\begin{itemize}[left=0pt, itemsep=0pt, topsep=0pt]
    \item \textbf{Efficient long-input training on the fly.} LIFT dynamically adapts to newly introduced long inputs as fresh knowledge by adjusting model parameters, thereby eliminating the need for resource-intensive offline long-context adaptation. To enhance memorization of the long input, we segment it into overlapping segments which can be fitted into a short context window and fine-tune the LLM on batches of the segments. Additionally, we improve long-context comprehension and reasoning through fine-tuning on well-designed auxiliary tasks, further optimizing performance on downstream applications.
    \item \textbf{Balancing in-parameter and in-context knowledge.} As LIFT is mainly designed for short-context LLMs, the long input often needs to be truncated to fit into their context windows. This necessitates a balance between leveraging the truncated in-context knowledge and the fine-tuned in-parameter knowledge. To address this issue, we propose a specialized attention adapter, Gated Memory, that automatically balances the long input memorization and comprehension in LIFT as well as the ICL ability of the original model.%As a compensation for the information loss introduced by in-context truncation, LIFT further enhances ICL after long-input tuning. Short context LLMs are empowered by processing arbitrary long inputs with parametric knowledge. Notably, we introduce a specialized attention adapter, Gated Memory that automatically balances the long input memorization in LIFT as well as ICL for original models with strong adaptability.
    \item \textbf{Great improvement on popular long-context tasks.} Our evaluations on several well-acknowledged long context benchmarks show that LIFT consistently benefits general tasks like long/short question answering (QA) and summarization across different base LLMs. For example, on the challenging long-dependency QA tasks of LooGLE~\citep{li2023loogle}, the ``LIFTed'' Llama-3-8B-Instruct model achieves an accuracy of 29.97\%, significantly outperforming its pure ICL counterpart without LIFT which achieves only 15.44\% accuracy. % It has been validated that the architecture of Gate Memory largely strengthens the model's capability on downstream tasks. 
\end{itemize}

These findings highlight the effectiveness of LIFT in improving the long-context comprehension of short-context models, paving the way for broader applications and exciting new opportunities in long-context scenarios.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figure/method.pdf}
    \vspace{-15pt}
    \caption{An overview of LIFT compared with existing methods.}
    \vspace{-15pt}
    \label{fig:overview}
\end{figure*}

\section{Related Work}

% \paragraph{Efficient Transformers for long-context understanding.} As the most popular way to improve Transformer architectures, sparseattention~\citep{kitaev2020reformer,wang2020linformer, beltagy2020longformer} uses sparse patterns (e.g., local windows or strided patterns) to reduce the number of tokens attended to in self-attention mechanism, thus reducing memory and computational cost. It allows the models to focus on relevant parts of the context without being overwhelmed by the length of the input.
% These models employ techniques like block-wise attention, dilated windows, or random sampling of attention spans to optimize computational costs while preserving essential long-range dependencies. 
% Reformer introduced locality-sensitive hashing (LSH) to reduce the quadratic complexity of attention to sub-quadratic, making it more feasible to process long documents. Linformer proposed projecting the attention matrix into a lower-dimensional space allowing longer contexts to be processed more efficiently. Longformer tackled long document processing by introducing sparse attention patterns, where each token attends only to a limited set of other tokens rather than the entire sequence.
% Linear attention~\citep{shen2021efficient} reduces the time and memory complexity of Transformers from quadratic to linear by approximating the self-attention mechanism, which typically leverages kernel functions or low-rank approximations.
% focus on enhancing LLMs' capacity to access relevant information by retrieving external knowledge rather than encoding long contexts directly. 

% \paragraph{State-space models (SSMs) for long-context understanding.} Recently, deep SSMs~\citep{gu2022efficientlymodelinglongsequences, NEURIPS2018_5cf68969} have shown remarkable performance on long-sequence modeling tasks by combining fast, parallelizable training with RNN-like fast inference. A notable advancement, Mamba~\citep{gu2023mamba}, along with its variants, has been proposed to enable length extrapolation and context extension, further enhancing the capabilities of SSMs in handling extended sequences.

% \paragraph{Retrieval-Augmented Generation (RAG).} RAG~\citep{lewis2020retrieval}  improves the efficiency and effectiveness of LLMs in long-context understanding by integrating external memory components~\citep{xu2023retrieval, jiang2024longrag, wang2024augmenting, jin2024llm}. It stores information over time, allowing the model to recall past information without requiring the entire context to fit within its context window.

% \paragraph{Positional embeddings (PEs) for long-context understanding.} Handling long sequences also requires efficient PEs. Techniques such as relative and rotary PEs~\citep{li2023functional, su2024roformer, peng2023yarn}, as well as position interpolation and extrapolation~\citep{chen2023extending,kazemnejad2024impact}, have been explored to increase the effective length of contexts that models can handle.

\textbf{Long-context adaptation and efficient architectures. } Existing LLMs mostly rely on pure ICL for long-context understanding. However, it is challenging for short-context models to process inputs longer than their context window sizes due to unseen positional encodings during pretraining, resulting in extremely poor performance on downstream tasks. Therefore, a common practice is to further fine-tune LLMs on huge corpus of long texts (which we call long-context adaptation). Despite the effectiveness, long-context adaptation often requires tremendous computational cost.

To cope with the problems, many works have been developed to accelerate the process of long-context training with efficient Transformer. Sparse attention~\citep{kitaev2020reformer, wang2020linformer, beltagy2020longformer} reduces memory and computation costs by using local windows or strided attention, allowing to focus on the most relevant inputs for given tasks. Linear attention~\citep{shen2021efficient} reduces the quadratic computation to linear by approximating self-attention with kernel functions or low-rank representations. Other alternatives for Transformer like state-space models (SSMs)~\citep{gu2023mamba} are recently proposed for efficient training based on dual representations. In this work, we focus on the conventional self-attention architecture~\citep{vaswani2017attention} which is most widely used in current LLMs to validate the effectiveness of LIFT.

\textbf{Retrieval-Augmented Generation (RAG). } RAG~\citep{lewis2020retrieval} improves the performance of long-context understanding by integrating LLMs with external data sources or memory modules for retrieval~\citep{xu2023retrieval, jiang2024longrag, wang2024augmenting, jin2024llm}, thereby avoiding the need to process long inputs directly. Its performance heavily relies on the quality of retrieved content, which must be relevant and concise enough to fit within models' short context windows. RAG can experience significant performance degradation or hallucination issues when the retrieved context is inaccurate or mismatched.

\textbf{Test-time training. } Test-time training (TTT)~\citep{liu2021ttt++, gandelsman2022test, osowiechi2023tttflow,hong2023mecta} has emerged as a promising approach to adapt models to unseen data distributions during deployment, leveraging test data to fine-tune the model at inference time. Recent works have applied similar ideas to improve model adaptability when dealing with lengthy, context-rich inputs~\citep{sun2024learning,behrouz2024titans}, yet focus on proposing new architectures to replace Transformer and require pretraining from scratch. Our work, in contrast, introduces a continual learning perspective to the problem of long-context understanding, which focuses on improving arbitrary pretrained models' long-context capabilities by fine-tuning them on the long input, which is not restricted to specific models or layers. %Rather than adopting a one-time training approach as in TTT, we propose continual fine-tuning of models with new, long inputs on the fly, thereby enabling the model for continuous adaptation and incrementally learn new knowledge over time. By fine-tuning on long inputs iteratively, we aim to improve the model's ability to understand longer contexts continually without the need for offline exhaustive training.


%\cite{sun2020test} introduced TTT by utilizing self-supervised tasks on test inputs to refine the model in real-time, ensuring better generalization to out-of-distribution data. Subsequent research has expanded this idea to enhance robustness against distribution shifts, adversarial attacks, and noisy environments. 
%Wang et al. (2021) extended this concept through test-time contrastive learning, which enhances feature alignment and improves performance under domain shifts. Liu et al. (2021) proposed using auxiliary loss functions at test time, which leverages unlabeled test data to adapt features and improve performance without the need for retraining. 

% Recent advancements in TTT also explore methods for lightweight and efficient model updates, such as few-shot learning and meta-learning approaches, allowing for fast adaptation without significant computational overhead. This body of work demonstrates the potential of test-time training to extend the lifecycle of machine learning models in dynamic and evolving environments.
%\cite{sun2024learning} proposed new TTT layers by updating the hidden states through self-supervised learning, significantly improving performance over traditional RNNs and matching Transformer models in processing long texts up to 16k tokens. \cite{bertsch2024context} studied the benefits of TTT in improving in-context learning by dynamically refining representations based on extended input sequences. \cite{wang2024greater} explores how TTT can enhance LLMs in long-text generation tasks such as novel writing and translation. 
%by presenting a framework called Temp-Lora, which is tested on large datasets, demonstrating improvements in perplexity and efficiency for long-text contexts. 
%These works highlight TTT's potential in improving long-context tasks by adapting models on-the-fly, thereby enhancing the ability to manage complex dependencies and lengthy sequences without retraining.



\section{Method} \label{sec:method}

% \jiaqi{@yansheng equations (formulation and format) }
%LLMs have difficulty memorizing long contexts and leveraging in-context knowledge to reason. Simply truncating contexts to fit them in the context window of the model loses information of the truncated texts. RAG-based methods also suffered from information loss caused by inaccurate retrieval. Another solution is long-context fine-tuning, \textit{i.e.}, to fine-tune the model on long texts, increasing its context window. However, training LLMs on long texts is computationally expensive. Besides, during inference, the model with a long context window takes the whole long context as its input, which is also inefficient.

%We propose to avoid information loss and heavy computation caused by fine-tuning or doing inference on long texts. Our method, Long-Context Test-Time-Training (Figure \ref{fig:method}), has the following features,
%\begin{itemize}
%    \item We make no explicit assumption about the usefulness of each part of the long context. Thus we make use of the whole context.
%    \item Our method only involves fine-tuning and inference on short texts within the context window of the LLM (we mainly use Llama-3-8B for experiments whose context window is 8k).
%\end{itemize}

%As discussed earlier, our method features adaptation and inference with only a short-context model, ensuring high efficiency. 

In this section, we introduce LIFT, a framework improving LLMs' long context understanding through long input fine-tuning (\cref{fig:overview}). The comparisons of our method with RAG and long-context adaptation can be seen in \cref{tab:comparison} and the implementation details are illustrated in \cref{app:details}.

%\subsection{Test time training with context segments}
\subsection{Training with Segmented Long Inputs}
\label{subsec:segment}

We propose a novel way to memorize long inputs by storing them into LLMs' parameters via fine-tuning. We formulize the memorization task as a language modeling task. Let the input be $\mathbf{x}=(x_{1},x_{2},\dots,x_{L})$, where the input length $L$ is a very large number. The objective function for the task is defined as
$
\mathcal{L}_{LM}(\mathbf{x};\theta)=-\sum_{i=1}^{L}\log\mathbb{P}(x_{i}|\mathbf{x}_{1:i-1};\theta),
$
where $\theta$ are the parameters.

Directly adapting a model to a piece of long text of length $L$ is challenging for LLMs whose context window lengths $l$ are shorter than $L$. Furthermore, it leads to a high computational complexity of $\mathcal{O}(L^{2})$ coping with such long inputs during training.  One straightforward way is to cut $\mathbf{x}$ into $K$ non-overlapping short segments (trivial segmentation), denoted as $\mathbf{x}_{l_{1}:r_{1}},\dots,\mathbf{x}_{l_{K}:r_{K}}$, and fine-tune the model on batches of the segments. However, trivial segmentation fails to capture the sequentiality of the long input since the model cannot infer the correct order of the non-overlapping segments.

To address this, we alter the long-input segmentation with certain overlaps between the adjacent segments as illustrated in Figure \ref{fig:segmentation}. By overlapping the tail of one segment with the head of the next, the model can better preserve the sequential structure of the input. Ideally, \textit{if the model learns to generate the tail of a segment, it should be able to seamlessly continue into the next segment}. Formally, the objective function for the language modeling task with our long-input segmentation method is formed as
\begin{align}\label{eq:input_memorization}
\mathcal{L}_{input}(\mathbf{x};\theta)=\sum_{k=1}^{K}\mathcal{L}_{LM}(\mathbf{x}_{l_{k}:r_{k}};\theta),
\end{align}
where~$l_{1}=1,r_{K}=L$, and $r_{k}-l_{k}+1=\ell,l_{k+1}=l_{k}+s,~ \forall k=1,2,\dots,K-1$.
Here $s$ is a hyperparameter controlling the overlap length of adjacent segments. Empirically,  it is sufficient to use $s=\frac{3}{8}\ell$, leading to a constant computational overhead.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figure/segmentation.pdf}
    \vspace{-20pt}
    \caption{Comparison between our segmentation method and the trivial segmentation method.}
    \vspace{-10pt}
    \label{fig:segmentation}
\end{figure}
\subsection{Training with Auxiliary Tasks}
\label{subsec:AT}

Adapting a pretrained LLM to a specific task poses the risk of impairing its other capabilities. Similarly, while adapting to the input helps the model memorize the input, it probably degrades other abilities, such as instruction-following. Moreover,  successfully memorizing the lengthy input does not necessarily indicate that the model can reason effectively based on it.

To mitigate potential capability degradation while maintaining the reasoning capabilities of original model on the long context, we propose synthesizing auxiliary question-answering (QA) tasks, denoted as $(\mathbf{q}_{i},\mathbf{a}_{i})_{i=1}^{m}$, where $m$ is the number of auxiliary tasks, based on the long input. The objective function of the auxiliary tasks is defined as
\begin{align}\label{eq:AT}
\mathcal{L}_{AT}((\mathbf{q}_{i},\mathbf{a}_{i})_{i=1}^{m};\theta)=-\sum_{i=1}^{m}\log\mathbb{P}(\mathbf{a}_{i}\mid\mathbf{q}_{i};\theta).
\end{align}
Following the mechanism of mix training~\citep{AllenZhu2023PhysicsOL}, which asserts that LLMs can only learn to perform inference based on $\mathbf{x}$ when trained simultaneously on both $\mathbf{x}$ and $(\mathbf{q}_{i},\mathbf{a}_{i})_{i=1}^{m}$, we propose jointly optimizing the two objective functions, i.e.,
\begin{align}\label{eq:MIX}
\mathcal{L}(\mathbf{x},(\mathbf{q}_{i},\mathbf{a}_{i})_{i=1}^{m};\theta)=\mathcal{L}_{input}(\mathbf{x};\theta)+\mathcal{L}_{AT}((\mathbf{q}_{i},\mathbf{a}_{i})_{i=1}^{m};\theta)
\end{align}
There are no strict constraints on the method used to synthesize $(\mathbf{q}_{i},\mathbf{a}_{i})_{i=1}^{m}$ based on $\mathbf{x}$, except that we should avoid computationally expensive operations on $\mathbf{x}$, such as inference over the entire $\mathbf{x}$. In our experiments, we extract several short segments from $\mathbf{x}$ and use a pretrained LLM to generate QA pairs based on the segments.


\subsection{Contextualized Training and Task Alignment}
\label{subsec:segment ICL}

As discussed in Sections \ref{subsec:segment} and \ref{subsec:AT}, we adapt an LLM to handle a long input through two objectives: language modeling on segments of the long input and auxiliary QA tasks. While these tasks align with our objectives of memorizing the long input and enhancing reasoning based on the long input, the model may still struggle with the semantic divergence (memorization vs. reasoning) and structural divergence (language modeling vs. supervised fine-tuning) between different tasks. To address these challenges, we propose a contextualized training (CT) method for long input segments, shifting from the language modeling paradigm to a supervised fine-tuning paradigm and more closely aligning the task of input segment memorization and the auxiliary QA tasks.

Our contextualized training method involves 1) providing the model with a piece of context when asking it to memorize the segments, typically selected from the beginning and ending portions of the long input, and 2) prompting the model to generate the target segments based on the provided context. Formally, we modify the objective function (\ref{eq:input_memorization}) for the long input memorization part to the following:
\begin{align}\label{eq:contextualized_input}
\mathcal{L}_{input}(\mathbf{x};\theta)=-\sum_{k=1}^{K}\log\mathbb{P}(\mathbf{x}_{l_k:r_k}\!\mid\! concat(\mathbf{c}_{k},\mathbf{p});\theta),
\end{align}
where $\mathbf{c}_{k}$ represents the given context, and $\mathbf{p}$ is a prompt instructing the model to recite the segment based on $\mathbf{c}_{k}$. For the QA tasks, we also modify the objective (\ref{eq:AT}) by concatenating the questions with a context $\mathbf{c}_q$:
\begin{align}\label{eq:contextualized_AT}
\mathcal{L}_{AT}((\mathbf{q}_{i},\mathbf{a}_{i})_{i=1}^{m};\theta)=-\sum_{i=1}^{m}\log\mathbb{P}(\mathbf{a}_{i}\!\mid\! concat(\mathbf{c}_q,\mathbf{q}_{i});\theta)
\end{align}
where $\mathbf{c}_q$ keeps the same during training on different segments, which is only related to the test question.
In this way, both the input memorization and QA tasks share a similar SFT format. In addition, they both align better with the real testing scenario, where given a LIFTed LLM, we can still fill the context window with the long input as much as possible to maximally leverage the in-context knowledge, instead of only filling in the testing question. Such a technique greatly improves practical performance of LIFT.

To mitigate the risk of overfitting, instead of using the same $\mathbf{c}_{k}$ for all the segments $\mathbf{x}_{l_{k}:r_{k}}$, we further regularize $\mathbf{c}_{k}$ by randomly sampling $\mathbf{c}_{k}$ for each segment $\mathbf{x}_{l_{k}:r_{k}}$ from both the beginning and ending of the long input with a total length of $L$. Specifically, we select consecutive sentences from the beginning and ending respectively compositing $\mathbf{c}_{k}$ with a fixed length $l$ to align with the usages of contexts in real testing scenarios.

By employing CT, we align the input memorization task with the auxiliary QA tasks better within a closer semantic space, and unify the training and testing formats, thereby greatly enhancing the generalization capabilities of LIFT, as evidenced by our ablation study in Table~\ref{tab:ablation_segment_icl}.

\subsection{Gated Memory Architecture} 
\label{subsec:gate}

To efficiently apply LIFT, we aim to use a parameter-efficient fine-tuning (PEFT) method rather than full-parameter fine-tuning. Existing representative PEFT methods such as LoRA~\citep{hu2021loralowrankadaptationlarge} and PiSSA~\citep{meng2024pissa} are not specifically designed for long context tasks. Therefore, we propose a novel \textbf{Gated Memory} adapter working very well in the LIFT framework.

\textit{The key intuition behind LIFT is to store the parts of a long input that cannot fit into the context window directly in model parameters}. To achieve this, the adapter needs to effectively memorize these out-of-context parts and \textit{align its behavior with the scenario of having complete input}. For a hypothetical complete input $(\mathbf{x}', \mathbf{x})$ where $\mathbf{x}'$ is the long input that we aim to absorb into model parameters and $\mathbf{x}$ represents the new in-context questions/prompts about the long input, we let their hidden states after the $(t-1)$-th layer be $({\hat{\mathbf{h}}}^{'(t-1)},\hat{\mathbf{h}}^{(t-1)})$, where the length of $\mathbf{x}'$ is $l'$ and the length of $\mathbf{x}$ is $l$. In practice, the model has access to the questions/prompts $\mathbf{x}$ only and we let their hidden states after the $(t-1)$-th layer be $\mathbf{h}^{(t-1)}$. We expect the following behavior in each layer of the original Transformer:
$$
\phi^{(t)}_{\mathbf{x}'}(\mathbf{h}^{(t-1)}) = f^{(t)}({\hat{\mathbf{h}}}^{'(t-1)},\hat{\mathbf{h}}^{(t-1)}),
$$
where $f$ is the original layer that takes the complete input $(\mathbf{x}', \mathbf{x})$ as the context. The new layer $\phi_{\mathbf{x}'}$, on the other hand, processes only $\mathbf{x}$ as the context while absorbing the information from $\mathbf{x}'$ into its parameters.

Layer $f$ consists of an attention module and an MLP module. The key lies in establishing the association between the in-context questions/prompts ($\hat{\mathbf{h}}^{(t-1)}$) and the long input ($\hat{\mathbf{h}}'^{(t-1)}$). While the attention module captures contextual associations, the MLP module performs token-wise transformations. Therefore, only the attention module needs modification, while the MLP module remains frozen.
Concatenating $\hat{\mathbf{h}}'^{(t-1)}$ and $\hat{\mathbf{h}}^{(t-1)}$ (i.e., $\hat{\mathbf{h}}'^{(t-1)}$ is positioned from $1$ to $l'$ and $\hat{\mathbf{h}}^{(t-1)}$ is positioned from $l'+1$ to $l'+l$), let's examine the hypothetical complete attention. The attention output at position $L$ ($l'+1\le L\le l'+l$) is:
\begin{equation}
\begin{aligned}
    \operatorname{attn}(\hat{q}_{L},\hat{\mathbf{k}}_{1:L},\hat{\mathbf{v}}_{1:L})=\frac{\sum_{i=1}^{L}\exp(\langle\hat{q}_{L},\hat{k}_{i}\rangle)\hat{v}_{i}}{\sum_{j=1}^{L}\exp(\langle\hat{q}_{L},\hat{k}_{j}\rangle)},
\end{aligned}
\label{equ:complete_attention}
\end{equation}
We aim at splitting the output into two components: one corresponding to the out-of-context $\hat{\mathbf{k}}_{1:l'},\hat{\mathbf{v}}_{1:l'}$, and the other corresponding to the in-context $\hat{\mathbf{k}}_{l'+1:L},\hat{\mathbf{v}}_{l'+1:L}$. Define the gate function $g(\hat{q}_{L},\hat{\mathbf{k}}_{1:L})$ and the memory function $m(\hat{q}_{L},\hat{\mathbf{k}}_{1:l'},\hat{\mathbf{v}}_{1:l'})$:
% $$
% \begin{aligned}
% g(q_{L'}, \mathbf{k}_{1:L'})&= \frac{\sum_{i=1}^{L} \exp(\langle q_{L'}, k_i \rangle)}{\sum_{i=1}^{L'} \exp(\langle q_{L'}, k_i \rangle)},\\
% m(q_{L'}, \mathbf{k}_{1:L}, \mathbf{v}_{1:L})&= \frac{\sum_{i=1}^{L} \exp(\langle q_{L'}, k_i \rangle) v_i}{\sum_{i=1}^{L} \exp(\langle q_{L'}, k_i \rangle)}.
% \end{aligned}
% $$
\begin{equation}
\vspace{-15pt}
\begin{aligned}
g(\hat{q}_{L}, \hat{\mathbf{k}}_{1:L})&= \frac{\sum_{i=1}^{l'} \exp(\langle \hat{q}_{L}, \hat{k}_i \rangle)}{\sum_{i=1}^{L} \exp(\langle \hat{q}_{L}, \hat{k}_i \rangle)},\\
m(\hat{q}_{L}, \hat{\mathbf{k}}_{1:l'}, \hat{\mathbf{v}}_{1:l'})&= \frac{\sum_{i=1}^{l'} \exp(\langle \hat{q}_{L}, \hat{k}_i \rangle) \hat{v}_i}{\sum_{i=1}^{l'} \exp(\langle \hat{q}_{L}, \hat{k}_i \rangle)}.
\end{aligned}
\label{equ:requirement_gate}
\end{equation}

$g(\hat{q}_{L}, \hat{\mathbf{k}}_{1:L})$ determines the proportion of attention allocated to the out-of-context part at position $L$, and $m(\hat{q}_{L}, \hat{\mathbf{k}}_{1:l'}, \hat{\mathbf{v}}_{1:l'})$ is the out-of-context representation which can be understood as performing cross attention between the current in-context token $\hat{q}_{L}$ and all the out-of-context tokens $\hat{\mathbf{k}}_{1:l'}, \hat{\mathbf{v}}_{1:l'}$. Then the attention output in Equation~(\ref{equ:complete_attention}) can be reformulated as:
$$
\begin{aligned}
\mathrm{attn}&(\hat{q}_{L}, \hat{\mathbf{k}}_{1:L}, \hat{\mathbf{v}}_{1:L})=
g(\hat{q}_{L}, \hat{\mathbf{k}}_{1:L}) \cdot m(\hat{q}_{L}, \hat{\mathbf{k}}_{1:l'}, \hat{\mathbf{v}}_{1:l'})+\\
&\big(1 - g(\hat{q}_{L}, \hat{\mathbf{k}}_{1:L})\big) \cdot \text{attn}(\hat{q}_{L}, \hat{\mathbf{k}}_{l'+1:L}, \hat{\mathbf{v}}_{l'+1:L}),
\end{aligned}
$$
where $\text{attn}(\hat{q}_{L}, \hat{\mathbf{k}}_{l'+1:L}, \hat{\mathbf{v}}_{l'+1:L})$ is the attention output with the same attention parameters operated on the in-context part's hidden state $\hat{\mathbf{h}}^{(t-1)}$ (instead of the complete hidden state $({\hat{\mathbf{h}}}^{'(t-1)},\hat{\mathbf{h}}^{(t-1)})$).
Let $g$ and $m$ be implemented as neural networks. When the out-of-context input $\mathbf{x}'$ is considered a constant and has been absorbed into the parameters of $g$ and $m$, $\hat{\mathbf{k}}_{1:l'}$ and $\hat{\mathbf{v}}_{1:l'}$ \textit{can be removed from $g$ and $m$}. We further adopt an approximation to let $g$ only depend on $\hat{q}_{L}$. 
Consequently, both $g(\hat{q}_{L},\hat{\mathbf{k}}_{1:L})$ and $m(\hat{q}_{L},\hat{\mathbf{k}}_{1:L},\hat{\mathbf{v}}_{1:L})$ become functions of $\hat{q}_{L}$. The attention output simplifies to:
\[
g(\hat{q}_{L})\cdot m(\hat{q}_{L})+\big(1 - g(\hat{q}_{L})\big) \cdot \mathrm{attn}(\hat{q}_{L}, \hat{\mathbf{k}}_{l'+1:L}, \hat{\mathbf{v}}_{l'+1:L}).
\]
In practice, we do not have the hypothetical complete input, so the index of $\hat{\mathbf{k}}, \hat{\mathbf{v}}$ start just from 1 instead of $l'+1$ and all the hypothetical hidden states are replaced with the actual hidden states $\mathbf{h}$ from the first layer. Our goal is to learn gate and memory functions $g$ and $m$ (implemented with MLPs) as adapters to the fixed original attention module that adapt the LLM to the long input.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figure/gate.pdf}
    \vspace{-20pt}
    \caption{The architecture of \textbf{Gated Memory}. The \textcolor{Purple}{purple} part is the added adapter ``gated memory'' to fit the out-of-context attention; the \textcolor{OliveGreen}{green} part is the original attention module. During training, only the \textcolor{Purple}{gated memory} part is trained. Other parameters are fixed.}
    \vspace{-10pt}\label{fig:gate_mem_structure}
\end{figure}

The Gated Memory architecture is shown in Figure~\ref{fig:gate_mem_structure}. In summary, we keep the \textcolor{Orange}{original MLP} and \textcolor{OliveGreen}{attention parameters} (projectors for query, key, value and output) fixed and add two \textcolor{Purple}{learnable MLPs} for each attention head: 1) $g: \mathbb{R}^d \to \mathbb{R}$, a \textit{gating function} that controls the ratio of information extracted from the memory and the context. 2) $m: \mathbb{R}^d \to \mathbb{R}^d$, a \textit{memory function} which stores the out-of-context information of the long input and retrieves relevant memory based on the current query. 
% The two MLPs take the query vector $q_L$ as input and generate the gated memory, which is added into the original attention output.
In the ideal case where the learned $g$ and $m$ perfectly simulate Equations~(\ref{equ:requirement_gate}), that is, the previous information of $\hat{h}'$ has been completely absorbed into the parameters of $g$ and $m$, the model can perform the same as the case with complete long input, thus achieving the effect of using a short-context model to simulate long-context transformers.

However, using Equations~(\ref{equ:requirement_gate}) as supervision to train $g$ and $m$ is too expensive, as it requires the complete hidden state, which can only be obtained by processing the entire long input. Instead, we train these adapters \textit{end-to-end} during the LIFT process. Specifically, the modules are randomly initialized and trained through segmented language modeling (Section~\ref{subsec:segment}) and auxiliary QA tasks (Section~\ref{subsec:AT}). In Section~\ref{subsubsec:ablation_on_gated_mem}, we show that end-to-end training effectively learns the desired modules. Exploring other training schedules is part of our future work.

Gated Memory has another great benefit, namely automatically balancing the memorization/reasoning with the absorbed new knowledge (the $g(\hat{q}_{L}) \cdot m(\hat{q}_{L})$ part) and the in-context learning capabilities of the original model (the remaining part). When a test task is not related to the absorbed knowledge, the architecture can learn to set $g(\hat{q}_{L})=0$ and recover the model without LIFT, thereby solving the task using only the in-context knowledge. In contrast, existing PEFT methods like LoRA and PiSSA fail to control the influence of adapters, risk overfitting the long input, and may damage the original capabilities too much.


\begin{table*}[t!]
\centering
\caption{Performance on LooGLE  using LIFT.}
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{ll|cccccc}
\toprule
Models & Methods & ShortQA  & LongQA & \makecell{Comprehension \\ \& Reasoning}  & \makecell{Multiple info \\ retrieval}  & Computation  &
\makecell{Timeline \\ reorder} \\
\midrule
\multirow{2}{*}{\textbf{Llama3}} &ICL & 44.49 & 15.44 & 25.37 & 15.26 & 5.00 & 1.86  \\
 & LIFT  & \textbf{47.51} & \textbf{29.97} & \textbf{39.90} & \textbf{27.89} & \textbf{17.00} & \textbf{17.21} \\
\midrule
\multirow{2}{*}{\textbf{Gemma2}} & ICL & 37.37 & 29.79 & 36.95 & 21.58 & 10.00 & \textbf{40.00} \\
 & LIFT  & \textbf{50.33} & \textbf{31.24} & \textbf{40.39} & \textbf{27.11} & \textbf{12.00} & 30.23 \\
 \midrule
 \multirow{2}{*}{\textbf{GPT3.5}} & ICL & 66.82 &44.82   & 52.67 & \textbf{40.77}  & \textbf{27.55} & 45.19\\
 & LIFT  & \textbf{69.66} & \textbf{45.76}  & \textbf{53.44} &40.50 & 26.53 &\textbf{49.52}\\
\bottomrule
\end{tabular}}
\vspace{-10pt}
\label{tab:main_result_loogle}
\end{table*}

\begin{table*}[t]
    \centering
    \caption{Performance on LongBench using LIFT.}
    \label{tab:main_result_longbench}
    {\begin{tabular}{cc|ccccc}
    \toprule
    \multicolumn{1}{c}{Models} & 
    \multicolumn{1}{c|}{Methods} & 
    \multicolumn{1}{c}{Musique} & 
    \multicolumn{1}{c}{Narrativeqa} & 
    \multicolumn{1}{c}{Qmsum} & 
    \multicolumn{1}{c}{GovReport} & 
    \multicolumn{1}{c}{PassageRetrievalEN} \\
    \midrule
     \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Llama3}}} 
      & ICL & \textbf{26.89} & 19.45 & 21.64 & 30.18 & 58.33 \\
     % &  \texttt{LIFT\_only} & 12.13 & 16.19 & 18.34 & 10.93 & 0.00 \\
     &  LIFT & 21.19 & \textbf{23.33} & \textbf{23.07} & \textbf{33.62} & \textbf{62.50} \\
     \midrule
     \multicolumn{1}{c}{\multirow{2}{*}{\textbf{GPT3.5}}} 
     & ICL & 26.33  & 25.67 & 22.09 &\textbf{25.30} & \textbf{79.17}\\
     % & \texttt{LIFT\_only} & 13.58 & 11.95 & 18.76 & 8.90 & 6.25 \\
     & LIFT & \textbf{27.20} & \textbf{26.53} & \textbf{22.23} & 25.01 & \textbf{79.17}\\
    \bottomrule
    \end{tabular}}
    \vspace{-8pt}
\end{table*}



\section{Experiments}
\label{others}

\subsection{Setup}

\textbf{Dataset and metrics. }
We make a comprehensive evaluation of LIFT on three popular long-context benchmarks, including LooGLE~\citep{li2023loogle}, LongBench~\citep{bai2023longbench}, and Needle-In-A-Haystack (NIAH)~\citep{niah} covering a wide variety of application scenarios. 

The evaluation metrics are consistent with those used in the original benchmarks. For LongBench and NIAH, the evaluation metrics are task-specific~\citep{zhang2020bertscoreevaluatingtextgeneration}. Since most automatic evaluation metrics are sensitive to semantic expression, output format, and length, 
%Thus, these metrics alone might be insufficient for effectively comparing different models. To provide a more comprehensive assessment of models, 
we utilize GPT4-0613~\citep{achiam2023gpt} for LooGLE as recommended in the paper to judge whether the two answers are semantically the same or not, noted as GPT4\_score. It has been proven to exhibit high consistency with human evaluation and can serve as a reliable annotator to a great extent~\citep{suri2023large,liu2023calibrating,zheng2023judging}. The prompts implemented can be found in \cref{app:details}.

\textbf{Models. }
For open-source LLMs, we select Llama-3-8B-Instruct~\citep{llama3modelcard} and Gemma-2-9B-it~\citep{gemma_2024} with 8k context windows. For closed-source commercial LLMs, we choose GPT-3.5-turbo-0125~\citep{chen2023robust} with a 16k context window. It has shown competitive performance on popular long context benchmarks and can be further fine-tuned through API service. Details of the models and hyper-parameters can be seen in Appendix~\ref{app:details}.


\textbf{Settings. }
In this paper, we mainly evaluate \textbf{LIFT} compared with the truncated \textbf{ICL} performance of the selected LLMs. Specifically:
\textbf{ICL} denotes truncating the long input by retaining only the beginning and end of texts within the context window of the original model.
% \textbf{\texttt{LIFT\_only}}: LIFT without ICL, it is employed without incorporating any input into the context 
\textbf{LIFT} denotes first fine-tuning the LLM using the Gated Memory adapter (except for GPT-3.5 which uses the default API tuning service) with the objectives in Equations~(\ref{eq:contextualized_input}) and (\ref{eq:contextualized_AT}) with the above-mentioned truncated ICL.
The number of auxiliary tasks used and the method for generating them are specialized for each subtask (detailed in \cref{app:details}).



\subsection{Main Results}
\label{sec:main_results}
\subsubsection{Results on LooGLE}
\label{sec:main_results_loogle}
\textbf{Overall performance.} 
% LooGLE is a very challenging long-context benchmark with human-proposed/verified long-dependency QAs (LongQA) whose answers require retrieving evidences from multiple pieces spanning the whole long input, as well as automatically generated short-dependency QAs (ShortQA). 
As shown in Table \ref{tab:main_result_loogle}, LIFT consistently outperforms ICL often by large margins on the overall scores on both LongQA and ShortQA tasks across the three LLMs. Particularly, it shows notable improvement in GPT4\_score from 15.44 to 29.97 on Llama 3 in LongQA and from 37.37 to 50.33 on Gemma 2 in ShortQA. The results highlight that LIFT significantly improves the performance of ICL, particularly for models with short context windows. Notably, GPT-3.5 generally outperforms the open-sourced models across both tasks, while LIFT can further boost its performance. It can be noticed that all the models perform poorly on LongQA, with GPT4\_score falling below 50. This underscores that modeling long dependencies in extended contexts remains a significant challenge for existing methods.

\textbf{Performance on subtasks in LongQA.} We further investigate the performance on the four LongQA subtasks including comprehension \& reasoning, multiple info retrieval, computation and timeline reorder introduced in LooGLE in \cref{tab:main_result_loogle}.  As we can see, LIFT greatly enhances the open-sourced models in most subtasks. For example, LIFT improves the performance of Llama 3 on all the four subtasks with over 50\% gain. These results demonstrate that LIFT enhances ICL across different models and tasks by facilitating a more holistic understanding of the entire lengthy input, which is effectively captured in the model parameters. However, it may lead to a slight performance degradation on specific tasks and models in some cases, suggesting that it requires delicate design of the task-specific auxiliary tasks and flexible adaption to various models when applying LIFT.

\subsubsection{Results on LongBench}
\label{sec:main_results_longbench}

Table \ref{tab:main_result_longbench} presents the results across five representative tasks with extremely long inputs in LongBench. We follow the evaluation metrics introduced in the original benchmark for comparison. For Llama 3, LIFT outperforms ICL on 4 out of 5 subtasks, and for GPT-3.5, LIFT outperforms ICL on 3 out 5 subtasks.
We make in-depth analysis to figure out the impact of LIFT on different subtasks. NarrativeQA and QMSum consistently have performance gains from LIFT since these two tasks are similar to auxiliary tasks in LIFT. %For summarization with long-range dependencies in GovReport, it surprisingly benefits on Llama 3 without overfitting certain tasks designed in LIFT. It demonstrates the potential and generalization of LIFT in various applications and scenarios. 
In contrast, for tasks that mainly depend on the memorization of long inputs like Musique and PassageRetrievalEN, LIFT's advantage is not consistent. Empirically, we found it hard to hit a perfect balance between the long input memorization in LIFT and the ICL ability of the original model. When the model is significantly fine-tuned to memorize the long input, its original capabilities tend to degrade more. As most tasks do not require perfect memorization of the long input, our current strategy is to early-stop the fine-tuning to avoid overfitting. As a consequence, Llama 3 struggles to memorize the details in the context, leading to poor performance on Musique, while GPT-3.5 slightly benefits from LIFT, likely due to more robust foundation capabilities and better fine-tuning strategies. On PassageRetrievalEN, since we imitate the test set generation process to synthesize auxiliary tasks, Llama 3 benefits from the auxiliary tasks, becoming more familiar with the instructions at test time. In contrast, GPT-3.5, due to its strong instruction-following capability, does not see significant improvements from LIFT.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figure/efficiency.pdf}
    \vspace{-22pt}
    \caption{GPU time with increasing input length for LIFT with PiSSA or Gated Memory and the ICL baseline. The dashed lines represent the fitted curves, showing linear growth for LIFT and quadratic growth for ICL. The red cross indicates the input length at which the ICL baseline runs out of memory.}
    \label{fig:efficiency}
    \vspace{-10pt}
\end{figure}


\subsubsection{Efficiency}
\label{sec:efficiency}

Benefiting from our segmentation strategy (Section~\ref{subsec:segment}), the computational complexity of our method is linear with respect to the input length. To further evaluate the efficiency of our approach compared to ICL (with full input in the context), we measure the time cost of a single Needle-In-A-Haystack (NIAH) task under both methods. In this experiment, the input lengths are controllable and the primary computational cost stems from processing the input context rather than iterative generation. We compare LIFT with the Gated Memory adapter and another adapter PiSSA~\citep{meng2024pissa} against ICL, plotting GPU time versus input length along with the fitted curves in Figure~\ref{fig:efficiency}.

First, we observe that LIFT is significantly more memory-efficient than ICL. Notably, ICL runs out of memory when the input length exceeds 90k tokens on our A800 (80G) system. For ICL, the KV cache consumes most of the memory. In contrast, LIFT is capable of handling arbitrarily long inputs. Our segmentation and truncation strategy ensures that LIFT only involves training and inference with short text segments, eliminating the need for extensive caching.

We empirically verify that the time cost of ICL grows quadratically with input length, while our method scales linearly. However, we also observe that the constant factor introduced by adaptation in the computational complexity of LIFT is non-negligible. As a result, LIFT with PiSSA only surpasses ICL in time efficiency when the input length exceeds a certain threshold above 800k tokens, while LIFT with Gated Memory is even slower. We note that although Gated Memory involves fewer trainable parameters than PiSSA, it results in smaller update step sizes and requires more training epochs. A more hardware-friendly implementation is also lacking. As the primary cost of LIFT arises from multi-epoch fine-tuning, we hypothesize that by employing better parallel fine-tuning techniques, the efficiency of LIFT can be significantly improved.

\subsection{Ablation Study}
\label{sec:further_studies}

%\subsubsection{Ablation study on contextualized training and number of auxiliary QA}

\begin{table}[t]
    \centering
    \caption{Ablation study on contextualized training on LooGLE.}
    \label{tab:ablation_segment_icl}
    \begin{tabular}{l|cc}
    \toprule
   Datasets & w/o CT & w/ CT \\
     \midrule
     ShortQA & 43.98 & \textbf{47.51} \\
    \midrule 
    LongQA & 27.07 & \textbf{29.97} \\ 
    \bottomrule
    \end{tabular}
    \vspace{-10pt}
\end{table}

\begin{table}[t]
    \centering
    \caption{Ablation study on number of auxiliary QA on LooGLE.}
    \label{tab:ablation_number_qa}
    \begin{tabular}{l|ccc}
    \toprule
   Datasets & w/o QA & 10 QA & 30 QA \\
     \midrule
    ShortQA & 47.21 & 47.51 & \textbf{48.84} \\
    \midrule 
    LongQA & 29.25 & 29.97 & \textbf{30.70} \\ 
    \bottomrule
    \end{tabular}
    \vspace{-10pt}
\end{table}

\textbf{Contextualized training. } As discussed in Section~\ref{subsec:segment ICL}, we posit that contextualized training is a pivotal component of the LIFT framework. By aligning input memorization tasks and auxiliary tasks within the same semantic space and task format, contextualized training enables the model to memorize input while minimizing the degradation of its inherent capabilities. As demonstrated in Table~\ref{tab:ablation_segment_icl}, the inclusion of contextualized training significantly enhances model performance on both the LooGLE ShortQA and LongQA tasks compared to the version without this component. This improvement underscores the critical role of contextualized training in achieving robust and effective long-text understanding.

\textbf{Number of auxiliary QA. }
Another important technique to improve LIFT's effectiveness is the auxiliary QA task introduced in Section~\ref{subsec:AT}. Here, we compare three settings: no auxiliary QA, 10 auxiliary QA pairs (default), and 30 pairs for each long input article. %In all three settings, stage-1 remains the same, and stage-2 training consists of 5 epochs with different data mixture. We use gradient accumulation to ensure the same number of gradient descent steps. 
The results, shown in Table~\ref{tab:ablation_number_qa}, suggest that increasing the number of auxiliary QA pairs improves performance. However, more QA pairs also mean more forward passes, and the 30 QA pair setting consumes roughly twice the training time of the 10 QA pair setting. Therefore, we choose 10 pairs as the default, balancing performance and efficiency.

\textbf{Gated Memory. }
\label{subsubsec:ablation_on_gated_mem}
As discussed in Section~\ref{subsec:gate}, our Gated Memory module acts as a specialized attention adapter, parallel to the original attention mechanism. Here, we compare it with the PiSSA adapter~\citep{meng2024pissa} on LooGLE dataset. The hyperparameters (learning rate and early-stop epochs) for both models are individually tuned to achieve optimal performance. Table~\ref{tab:ablation_gated_memory} shows that our model with Gated Memory outperforms that with PiSSA, demonstrating that Gated Memory has a superior ability to memorize out-of-context information during LIFT.

\begin{table}[t]
    \centering
    \caption{Ablation study on Gated Memory on LooGLE.} 
    \label{tab:ablation_gated_memory}
   \begin{tabular}{l|cc}
    \toprule
   Datasets & PiSSA & Gated Memory \\
     \midrule
    ShortQA & 42.03 & \textbf{47.51} \\
    \midrule 
    LongQA & 29.06 & \textbf{29.97} \\ \bottomrule
    \end{tabular}
    \vspace{-10pt}
\end{table}
\vspace{-5pt}


\subsection{Details and Analysis}
\vspace{-5pt}
\label{sec:details_and_analysis}

\begin{table}[t]
    \centering
    \caption{In-context and out-context scores on LooGLE ShortQA. LIFT (PiSSA) denotes the fine-tuning approach using PiSSA instead of the Gated Memory architecture. }
    \label{tab:ablation_in_and_out_context}
    \resizebox{1\linewidth}{!}{
    \begin{tabular}{l|ccc}
    \toprule
   Method & overall score & in-context score & out-context score \\
     \midrule
    ICL & 44.49 & \textbf{76.74} & 29.04 \\
    LIFT (PiSSA) & 42.03 & 62.03 & 32.45 \\
    LIFT w/o CT & 43.98 & 65.98 & 33.43 \\
    LIFT & \textbf{47.51} & 71.2 & \textbf{36.16} \\ \bottomrule
    \end{tabular}}
    \vspace{-10pt}
\end{table}

In this section, we delve deeper into the strengths and limitations of the LIFT methodology. The questions within the LooGLE ShortQA dataset can be categorized into two types based on whether the evidence required to answer it is within the truncated context or can only be accessed from parameters, i.e., in-context questions and out-of-context questions. When addressing in-context questions, the model theoretically only need to utilize its in-context learning capabilities, whereas for out-of-context questions, the model can only rely on parametric memory. 

We evaluated the GPT4\_score of the LIFT approach separately for in-context and out-of-context questions noted as in- and out-context score respectively in Table~\ref{tab:ablation_in_and_out_context}. After LIFT, the model's ICL abilities are inevitably compromised; however, there is a corresponding enhancement in the out-of-context capabilities based on memory. This trade-off inherently limits the efficacy of the naive approach of directly fine-tuning on the long input. It is evident that our LIFT method, by contrast--when compared to approaches that neither employ contextualized training nor utilize Gated Memory--significantly mitigates the decline in in-context scores while bolstering the improvement in out-of-context scores. This observation aligns with the motivation outlined in Section~\ref{subsec:segment ICL} and underscores the advantages of Gated Memory in better balancing the original ICL ability and the new ability adapted for the long input. 


\vspace{-5pt}
\section{Conclusion}

In this paper, we proposed a novel framework, \textbf{L}ong-\textbf{I}nput \textbf{F}ine-\textbf{T}uning (\textbf{LIFT}), to enhance LLMs' long-context understanding. Our approach dynamically adapts to long inputs by efficiently fine-tuning the model parameters and utilizing the in-parameter knowledge to improve long-context performance. Experimental results across popular benchmarks like LooGLE and LongBench demonstrate that LIFT effectively enables short-context LLMs to solve long-context tasks with great improvement on many long-context tasks.


\vspace{-5pt}
\section{Limitations and Future Work}
% \jiaqi{highlight findings in current version and results}}
\label{sec:limitation}

\textbf{Limitations of LIFT without ICL. } While we often employ truncated contexts to simplify inference on lengthy texts, this approach is proven insufficient for tasks that demand precise information extraction from extended contexts, such as the Needle in a Haystack (NIAH) task (\cref{app:niah}). 

% \paragraph{More advanced LIFT methods.} We introduce an intuitive strategy, LIFT, for handling long contexts, showcasing its potential to address challenges associated with lengthy inputs. However, pretrained LLMs may not be naturally familiar with the LIFT framework. To bridge this gap, we introduce pre-LIFT SFT, but our vision is to generalize the LIFT framework to any pretrained LLM, enhancing its flexibility and adaptability without requiring extensive retraining. This still needs extensive future study.

\textbf{Strategy to extract parametric knowledge after LIFT. } Through LIFT, embedding inputs into a model's internal parameters enhances its familiarity with the data. However, the effectiveness on downstream tasks still relies on the model's ability to autonomously extract and utilize the parametric knowledge acquired during LIFT. The detailed analysis (\cref{sec:details_and_analysis}) reveals a significant performance gap between in-context and out-of-context questions, suggesting that the model's capability to extract parametric knowledge post-LIFT needs further improvement. This presents a promising direction for future research and exploration.

\textbf{Challenges using LIFT with auxiliary tasks. } %Our findings reveal that auxiliary tasks during LIFT offer minimal benefit and can even degrade performance due to overfitting. Additionally, simply fine-tuning the model on long texts does not inherently endow it with robust reasoning capabilities over such texts. These observations underscore the necessity for more effective strategies to harness the in-parameter knowledge of LLMs, enabling them to reason efficiently and accurately over extended contexts.
Our findings reveal that auxiliary tasks are beneficial for improving LLMs' understanding of inputs. However, the benefit comes with extensive computational costs and they must be aligned with test tasks to achieve optimal performance. Future research should explore general auxiliary tasks that are suitable for all types of long inputs and tasks and are more computationally efficient to be incorporated into LIFT.

LIFT is a fascinating concept because humans similarly transform short-term memory into long-term memory, much like LIFT converts in-context knowledge into in-parameter knowledge. While LIFT is far from fully addressing the challenging long-context problem in LLMs, our preliminary results suggest it offers a promising and exciting direction for further research. We encourage the community to explore LIFT with broader training corpora, diverse models, advanced auxiliary task designs, and greater computational resources.

\section*{Impact Statements}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

% \clearpage

\bibliography{icml2025}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Experiment Details}
\label{app:details}
\subsection{Hardware settings}

All the experiments, including the main experiments on LooGLE (\cref{sec:main_results_loogle}) and LongBench (\cref{sec:main_results_longbench}), the efficiency test (\cref{sec:efficiency}), and the Needle-in-A-Haystack task (\cref{app:niah}), are conducted on a single NVIDIA A800 Tensor Core GPU. We intentionally select this resource-constrained hardware setup, where full-parameter fine-tuning is impractical. This necessitates the use of parameter-efficient fine-tuning (PEFT) methods, which optimize both time and memory efficiency.

The resource costs (GPU hours) of the experiments, which are mainly dependent on the PEFT methods (the Gated Memory architecture or PiSSA), the sizes of the models, and the sizes of the datasets, are presented in \cref{tab:gpu_hours}.

\begin{table*}[t!]
\centering
\caption{Resource costs (GPU hours) of the experiments.}
\resizebox{0.6\linewidth}{!}{
\begin{tabular}{ll|ccc}
\toprule
Models & Methods & \makecell{LooGLE\\ShortQA} & \makecell{LooGLE\\LongQA} & LongBench \\
\midrule
\multirow{3}{*}{\textbf{Llama3}} & Baseline & 3 & 3 & 2 \\
 & PiSSA & 20 & 42 & 24 \\
 & Gated Memory & 15 & 33 & 21 \\
\midrule
\multirow{2}{*}{\textbf{Gemma2}} & Baseline & 2 & 3 & $\backslash$ \\
 & PiSSA & 44 & 64 & $\backslash$ \\
 \bottomrule
\end{tabular}}
\label{tab:gpu_hours}
\end{table*}


\subsection{Hyperparameter settings}

We design a two-stage training paradigm for both the Gated Memory and PiSSA. The two stages differ in the data used in each. In the first stage, the model is trained solely on segmented long inputs (\cref{subsec:segment}) and optimizes the loss function $\mathcal{L}_{input}$ (\cref{eq:input_memorization}). In the second stage, auxiliary tasks (\cref{subsec:AT}) are incorporated into the training dataset, and the model optimizes the loss function $\mathcal{L}$ (\cref{eq:MIX}).

We adopted different sets of hyperparameters during testing on LooGLE and LongBench. When testing on LooGLE, empirically, the Gated Memory architecture causes small updating steps and requires a higher learning rate and more training steps than PiSSA. The important hyperparameters for both methods are detailed in \cref{tab:loogle_hyperparam}. When testing on LongBench with Gated Memory, we carefully select hyperparameters for each subtask, detailed in \cref{tab:longbench_hyperparam}.

Besides, we put all the samples including the context segments and the auxiliary tasks into a single batch through gradient accumulation to stabilize gradients. The batch size per device is $1$ to reduce memory costs. The other hyperparameters are kept the same for all the experiments: the context window lengths are limited to $8000$ to guarantee fair comparison, which is the context window lengths of Llama 3 and Gemma 2, but shorter than that of GPT-3.5.

During generation, we adopt greedy decoding for Llama 3 and Gemma 2 to avoid randomness, while adopt sampling for GPT-3.5. For GPT-3.5, the temperature is set to $0$, top p is set to $1.0$, and we adopt no frequency nor presence penalty.

\begin{table}[ht]
    \centering
    \caption{The hyperparameters employed during testing on LooGLE.}
    \label{tab:loogle_hyperparam}
    {\begin{tabular}{lcc}
    \toprule
    Hyperparameter & Gated Memory & PiSSA \\
    \midrule
    learning rate & $1.0\times10^{-3}$ & $3.0\times10^{-5}$ \\
    weight decay & $1.0\times10^{-4}$ & $1.0\times10^{-4}$ \\
    max grad norm & $1.0$ & $1.0$ \\
    $\beta_{1}$ & $0.9$ & $0.9$ \\
    $\beta_{2}$ & $0.98$ & $0.98$ \\
    $\epsilon$ & $1.0\times10^{-8}$ & $1.0\times10^{-8}$ \\
    stage 1 \#epochs & 3 & 1 \\
    stage 2 \#epochs & 5 & 3 \\
    \bottomrule
    \end{tabular}}
    \vspace{-8pt}
\end{table}

\begin{table}[ht]
    \centering
    \caption{The hyperparameters employed during testing on LongBench with Gated Memory. \#QA denotes the number of auxiliary tasks used. ${}^{*}$ We adopt 4 warmup steps to adjust the corresponding learning rates.}
    \label{tab:longbench_hyperparam}
    {\begin{tabular}{lccccc}
    \toprule
    Hyperparameter & Musique & Narrativeqa & Qmsum & GovReport & PassageRetrievalEN \\
    \midrule
    learning rate & $3.0\times10^{-3}\ {}^{*}$ & $3.0\times10^{-3}\ {}^{*}$ & $3.0\times10^{-3}$ & $3.0\times10^{-3}$ & $3.0\times10^{-3}\ {}^{*}$ \\
    weight decay & $1.0\times10^{-4}$ & $1.0\times10^{-4}$ & $1.0\times10^{-4}$ & $1.0\times10^{-4}$ & $1.0\times10^{-4}$ \\
    max grad norm & $1.0$ & $1.0$ & $1.0$ & $1.0$ & $1.0$ \\
    $\beta_{1}$ & $0.9$ & $0.9$ & $0.9$ & $0.9$ & $0.9$ \\
    $\beta_{2}$ & $0.98$ & $0.98$ & $0.98$ & $0.98$ & $0.98$ \\
    $\epsilon$ & $1.0\times10^{-8}$ & $1.0\times10^{-8}$ & $1.0\times10^{-8}$ & $1.0\times10^{-8}$ & $1.0\times10^{-8}$ \\
    \#QA & 10 & 30 & 0 & 0 & 60 \\
    stage 1 \#epochs & 3 & 3 & 8 & 8 & 3 \\
    stage 2 \#epochs & 5 & 5 & 0 & 0 & 5 \\
    \bottomrule
    \end{tabular}}
    \vspace{-8pt}
\end{table}


% \jiaqi{Add: how to select and filter subsets of longbench}

\subsection{Generating auxiliary tasks}\label{app:prompts}

We utilize auxiliary tasks on all the benchmarks except Qmsum and GovReport in LongBench. We adopt Llama-3-8B-Instruct as the generator and prompt it to synthesize a question and the corresponding answer conditioned on a piece of context.

For PassageRetrievalEN in LongBench, we imitate the generation process of the test set --- randomly select a passage and prompt the generator to summarize the passage. The auxiliary task is to answer the index of the passage given the generated summary. The prompt for summarizing a passage is as following:
\begin{tcolorbox}[colback=gray!10, colframe=black, width=15.5cm,
                  arc=1mm, auto outer arc, boxrule=0.5pt,
                 ]
Please summarize the following text in 4 to 6 sentences:\\
\textit{\{Context\}}
\end{tcolorbox}

For LooGLE, as well as Musique and Narrativeqa in LongBench, to avoid expensive long-context inference, we randomly select $16$ continuous sentences as context. The prompts for generating auxiliary tasks for each benchmark (or subtask) are as following:

\begin{tcolorbox}[colback=gray!10, colframe=black, width=15.5cm,
                  arc=1mm, auto outer arc, boxrule=0.5pt,
                 ]
\textbf{Instruction for LooGLE:} You are given a piece of text as the context. You should generate ONLY one question and the corresponding answer according to the context. You should also select one or more sentences directly from the original context as the evidence. The evidences must be EXACTLY SAME ADJACENT sentences retrieved from the context; KEEP the special tokens in the sentences. Please answer in the following format:\\
Question: [question]\\
Answer: [answer]\\
Evidence: [evidence]\\
Please DON'T output quotes when outputting evidences. \\
The following is the piece of text: \textit{\{Context\}}
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!10, colframe=black, width=15.5cm,
                  arc=1mm, auto outer arc, boxrule=0.5pt,
                 ]
\textbf{Instruction for Musique:} You are given a piece of text as the context. You should generate ONLY one question and the corresponding answer according to the context. You should also select one or more sentences directly from the original context as the evidence. The evidences must be EXACTLY SAME ADJACENT sentences retrieved from the context; KEEP the special tokens in the sentences. Please answer in the following format: \\
Question: [question]\\
Answer: [answer]\\
Evidence: [evidence]\\
Please DON'T output quotes when outputting evidences. The question should focus on the details like names, dates, e.t.c., and the answer should be as brief as possible. The following is the piece of text: \textit{\{Context\}}
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!10, colframe=black, width=15.5cm,
                  arc=1mm, auto outer arc, boxrule=0.5pt,
                 ]
\textbf{Instruction for Narrativeqa:} You are given a piece of text as the context. You should generate ONLY one question and the corresponding answer according to the context. You should also select one or more sentences directly from the original context as the evidence. The evidences must be EXACTLY SAME ADJACENT sentences retrieved from the context; KEEP the special tokens in the sentences. Please answer in the following format:\\
Question: [question]\\
Answer: [answer]\\
Evidence: [evidence]\\
Please DON'T output quotes when outputting evidences. The question should focus on the details like names, dates, e.t.c. The following is the piece of text: \textit{\{Context\}}
\end{tcolorbox}

\subsection{GPT4\_score evaluation}

We utilize GPT-3.5-turbo-0125 to evaluate the correctness of the responses of LLMs based on the questions and the ground-truth answers on LooGLE.

The prompt is as following (in the format of the chat template):
\begin{tcolorbox}[colback=gray!10, colframe=black, width=15.5cm,
                  arc=1mm, auto outer arc, boxrule=0.5pt,
                 ]
\textit{System}: Given one question, there is a groundtruth and a predict\_answer. Please decide whether they are the same or not in semantic. Please only output 'True' or 'False' .\\
\textit{User}: Question: \{\textit{Question}\}\\
groundtruth = \{\textit{Ground-truth answer}\}\\
predict\_answer = \{\textit{LLM response}\}
\end{tcolorbox}

\section{Results on Needle-in-a-Haystack (NIAH)}
\label{app:niah}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figure/NIAH-Baseline.pdf}
    \includegraphics[width=0.8\linewidth]{figure/NIAH-LIFT.pdf}
    \caption{Performance on NIAH: ICL (top) vs. LIFT (bottom).}
    \label{fig:niah}
\end{figure}

We present the experimental results in the NIAH \citep{niah} task in Figure \ref{fig:niah}, as further analysis of the pros and cons of LIFT and directions for future works. The task requires accurate retrieval from the contexts. We adopt a strong long-context model, Llama-3.1-8B-Instruct, as the baseline and apply the LIFT framework to the model.

The maximum context length of our test is 100K, which is within the 128K context window of Llama-3.1-8B-Instruct. As expected, the baseline achieves nearly perfect performance. However, LIFT slightly degrades the performance and the degradation seems irregular.

The reason for the degradation may be that LIFT introduces more noise to the model. While most parts of the context are irrelevant to the answer, LIFT asks the model to memorize all the context. The model is likely to be misled by the large amount of irrelevant information.

As summarized in Section \ref{sec:limitation}, precise memorization can be challenging for LIFT. On the one hand, LIFT can't accurately memorize the context while avoiding overfitting. On the other hand, LIFT is likely to be misled when most information is irrelevant to the answer. Future works may improve the LIFT framework from these two aspects.



\end{document}