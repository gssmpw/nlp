\section{Related Work}
% \paragraph{Efficient Transformers for long-context understanding.} As the most popular way to improve Transformer architectures, sparseattention~\citep{kitaev2020reformer,wang2020linformer, beltagy2020longformer} uses sparse patterns (e.g., local windows or strided patterns) to reduce the number of tokens attended to in self-attention mechanism, thus reducing memory and computational cost. It allows the models to focus on relevant parts of the context without being overwhelmed by the length of the input.
% These models employ techniques like block-wise attention, dilated windows, or random sampling of attention spans to optimize computational costs while preserving essential long-range dependencies. 
% Reformer introduced locality-sensitive hashing (LSH) to reduce the quadratic complexity of attention to sub-quadratic, making it more feasible to process long documents. Linformer proposed projecting the attention matrix into a lower-dimensional space allowing longer contexts to be processed more efficiently. Longformer tackled long document processing by introducing sparse attention patterns, where each token attends only to a limited set of other tokens rather than the entire sequence.
% Linear attention~\citep{shen2021efficient} reduces the time and memory complexity of Transformers from quadratic to linear by approximating the self-attention mechanism, which typically leverages kernel functions or low-rank approximations.
% focus on enhancing LLMs' capacity to access relevant information by retrieving external knowledge rather than encoding long contexts directly. 

% \paragraph{State-space models (SSMs) for long-context understanding.} Recently, deep SSMs~\citep{gu2022efficientlymodelinglongsequences, NEURIPS2018_5cf68969} have shown remarkable performance on long-sequence modeling tasks by combining fast, parallelizable training with RNN-like fast inference. A notable advancement, Mamba~\citep{gu2023mamba}, along with its variants, has been proposed to enable length extrapolation and context extension, further enhancing the capabilities of SSMs in handling extended sequences.

% \paragraph{Retrieval-Augmented Generation (RAG).} RAG~\citep{lewis2020retrieval}  improves the efficiency and effectiveness of LLMs in long-context understanding by integrating external memory components~\citep{xu2023retrieval, jiang2024longrag, wang2024augmenting, jin2024llm}. It stores information over time, allowing the model to recall past information without requiring the entire context to fit within its context window.

% \paragraph{Positional embeddings (PEs) for long-context understanding.} Handling long sequences also requires efficient PEs. Techniques such as relative and rotary PEs~\citep{li2023functional, su2024roformer, peng2023yarn}, as well as position interpolation and extrapolation~\citep{chen2023extending,kazemnejad2024impact}, have been explored to increase the effective length of contexts that models can handle.

\textbf{Long-context adaptation and efficient architectures. } Existing LLMs mostly rely on pure ICL for long-context understanding. However, it is challenging for short-context models to process inputs longer than their context window sizes due to unseen positional encodings during pretraining, resulting in extremely poor performance on downstream tasks. Therefore, a common practice is to further fine-tune LLMs on huge corpus of long texts (which we call long-context adaptation). Despite the effectiveness, long-context adaptation often requires tremendous computational cost.

To cope with the problems, many works have been developed to accelerate the process of long-context training with efficient Transformer. Sparse attention~\citep{kitaev2020reformer, wang2020linformer, beltagy2020longformer} reduces memory and computation costs by using local windows or strided attention, allowing to focus on the most relevant inputs for given tasks. Linear attention~\citep{shen2021efficient} reduces the quadratic computation to linear by approximating self-attention with kernel functions or low-rank representations. Other alternatives for Transformer like state-space models (SSMs)~\citep{gu2023mamba} are recently proposed for efficient training based on dual representations. In this work, we focus on the conventional self-attention architecture~\citep{vaswani2017attention} which is most widely used in current LLMs to validate the effectiveness of LIFT.

\textbf{Retrieval-Augmented Generation (RAG). } RAG~\citep{lewis2020retrieval} improves the performance of long-context understanding by integrating LLMs with external data sources or memory modules for retrieval~\citep{xu2023retrieval, jiang2024longrag, wang2024augmenting, jin2024llm}, thereby avoiding the need to process long inputs directly. Its performance heavily relies on the quality of retrieved content, which must be relevant and concise enough to fit within models' short context windows. RAG can experience significant performance degradation or hallucination issues when the retrieved context is inaccurate or mismatched.

\textbf{Test-time training. } Test-time training (TTT)~\citep{liu2021ttt++, gandelsman2022test, osowiechi2023tttflow,hong2023mecta} has emerged as a promising approach to adapt models to unseen data distributions during deployment, leveraging test data to fine-tune the model at inference time. Recent works have applied similar ideas to improve model adaptability when dealing with lengthy, context-rich inputs~\citep{sun2024learning,behrouz2024titans}, yet focus on proposing new architectures to replace Transformer and require pretraining from scratch. Our work, in contrast, introduces a continual learning perspective to the problem of long-context understanding, which focuses on improving arbitrary pretrained models' long-context capabilities by fine-tuning them on the long input, which is not restricted to specific models or layers. %Rather than adopting a one-time training approach as in TTT, we propose continual fine-tuning of models with new, long inputs on the fly, thereby enabling the model for continuous adaptation and incrementally learn new knowledge over time. By fine-tuning on long inputs iteratively, we aim to improve the model's ability to understand longer contexts continually without the need for offline exhaustive training.


%\cite{sun2020test} introduced TTT by utilizing self-supervised tasks on test inputs to refine the model in real-time, ensuring better generalization to out-of-distribution data. Subsequent research has expanded this idea to enhance robustness against distribution shifts, adversarial attacks, and noisy environments. 
%Wang et al. (2021) extended this concept through test-time contrastive learning, which enhances feature alignment and improves performance under domain shifts. Liu et al. (2021) proposed using auxiliary loss functions at test time, which leverages unlabeled test data to adapt features and improve performance without the need for retraining. 

% Recent advancements in TTT also explore methods for lightweight and efficient model updates, such as few-shot learning and meta-learning approaches, allowing for fast adaptation without significant computational overhead. This body of work demonstrates the potential of test-time training to extend the lifecycle of machine learning models in dynamic and evolving environments.
%\cite{sun2024learning} proposed new TTT layers by updating the hidden states through self-supervised learning, significantly improving performance over traditional RNNs and matching Transformer models in processing long texts up to 16k tokens. \cite{bertsch2024context} studied the benefits of TTT in improving in-context learning by dynamically refining representations based on extended input sequences. \cite{wang2024greater} explores how TTT can enhance LLMs in long-text generation tasks such as novel writing and translation. 
%by presenting a framework called Temp-Lora, which is tested on large datasets, demonstrating improvements in perplexity and efficiency for long-text contexts. 
%These works highlight TTT's potential in improving long-context tasks by adapting models on-the-fly, thereby enhancing the ability to manage complex dependencies and lengthy sequences without retraining.