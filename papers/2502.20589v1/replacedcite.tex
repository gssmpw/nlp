\section{Background and Related Work}
\label{sec:background}
The development of language models is built upon decades of research and technological advancement____. In particular, the revolution of language models began with the groundbreaking work in____, which introduced the first probabilistic language model based on neural networks and laid the foundation for using word embeddings and applying \ac{DL} in \ac{NLP}. Following that, the work in____ enhanced word representation to capture semantic relationships more efficiently than earlier techniques. In 2017, the field of \ac{NLP} witnessed a paradigm shift with the introduction of the Transformer architecture____. Traditional architectures such as \acp{RNN} and \ac{LSTM} suffered primarily from limitations in capturing long-term dependency and had to process text sequentially, which made their training and inference slow and computationally expensive____. In contrast, the Transformer architecture addressed these issues through self-attention and positional encoding, which enabled parallelization and effective handling of long dependencies. This breakthrough has laid the groundwork for modern \acp{LLM} such as GPT and LLaMA, which scale to billions of parameters. 

At their core, \acp{LLM} operate autoregressively, generating text one token at a time based on previously generated tokens and learned contextual embeddings. A token (e.g., a word or subword) is the basic unit of text processed and generated by the language models and is typically selected based on the highest probability of being next in the sequence. This iterative process continues until either a maximum sequence length is reached or a special end-of-sequence token is generated. Fig.~\ref{fig:autoregressive_process} shows the sequential autoregressive token generation process in \acp{LLM}.
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/autoregressive.pdf}
    \caption{The autoregressive token generation process in \acp{LLM}. At each step (1-5), the model uses all previous tokens (shown in dashed boxes) as a context to predict the next token, then append the new token to the sequence. Generated tokens are shown with corresponding timestamps ($t_1...t_5$). $\Delta t$ represents \acp{ITT} between consecutive tokens generation.}
    \label{fig:autoregressive_process}
\end{figure*} 
At inference time, the generation speed of a language model is primarily determined by factors such as its architecture, parameter size, and hardware parallelization capabilities. Although these large models achieve state-of-the-art performance in a wide range of \ac{NLP} tasks, their computational complexity and massive size require specialized hardware to run efficiently. Hence, most \acp{LLM} are deployed in cloud data centers that can supply the required computational resources.
As the adoption of \acp{LLM} such as ChatGPT, Gemini, and LLaMA becomes widespread, concerns about the security and privacy implications associated with these models have also grown____. Issues such as distinguishing between machine- and human-authored content, safeguarding intellectual property, and training models on stolen data are among the primary concerns. Researchers have been actively developing a variety of \acp{LLM} fingerprinting and detection techniques aiming to address these challenges. 

Two primary approaches have emerged in response to these issues: \emph{watermarking} and \emph{fingerprinting}. Watermarking embeds identifying markers into the model-generated output to trace its origin and verify its authenticity. These markers are imperceptible to humans yet detectable through algorithmic methods. For instance, pioneering work by authors in____ introduced a watermarking framework that modifies the output distribution by selecting a randomized set of "green" tokens before generating each word. Then, during the sampling process, the model softly promotes the use of these green tokens to create a statistical pattern without degrading the text quality. However, such methods are vulnerable to adversarial attacks, including text paraphrasing and token manipulation, which can compromise the watermark's integrity and reliability____. In contrast, fingerprinting focuses on detecting inherent patterns or characteristics in the model outputs to identify the generative model. Fingerprinting techniques can be categorized broadly into two main approaches: passive and active fingerprinting. 

Passive fingerprinting analyzes the intrinsic characteristics of the model output by examining its lexical and stylistic patterns. For example, authors in____ found that \acp{LLM} produce unique linguistic patterns and writing styles, such that even simple n-gram and part-of-speech distribution analysis can serve as effective fingerprints. These subtle variations in the frequency of specific lexical and syntactic features can differentiate between human- and machine-generated texts, as well as between different model families, such as GPT and LLaMA. The study also found that models within the same family often share similar fingerprints, even across different model sizes. Although this method requires no model modification or special queries, it is vulnerable to adversarial attacks that obscure the fingerprint through text manipulation. Another passive fingerprinting strategy introduced in____ exploits memory usage patterns to identify the architectural family of \acp{LLM} deployed on edge and embedded devices. The key idea is that different \ac{LLM} architectures exhibit distinct memory usage patterns during inference. By collecting high-resolution memory usage traces via the \emph{tegrastats} tool and training a \emph{ROCKET} model from the Sktime library, the authors effectively classified even previously unseen \ac{LLM} families with an average accuracy of 92\%. Alternatively, active fingerprinting methods, which require direct interaction with the model, can be further categorized based on their level of access. 

In black-box active fingerprinting, specific queries or prompts are sent to the model to elicit responses that aid in its identification. These methods typically assume limited access to the model--often via an API or by observing its outputs. For example,____ proposed LLMmap, a novel black-box active fingerprinting technique that sends carefully crafted queries to the target application and analyzes the responses to identify the specific \ac{LLM} version in use. Their query selection is informed by domain expertise on how \acp{LLM} generate uniquely identifiable responses to thematically varied prompts. The targeted query families include banner grabbing, alignment-based prompts, and malformed queries. With as few as 8 interactions, LLMmap achieves over 95\% accuracy in identifying 42 different \ac{LLM} versions, including both open-source and proprietary models. Similarly,____ proposed Hide and Seek, a black-box approach using an Auditor-Detective framework. In this method, one \ac{LLM} (the Auditor) generates discriminative prompts, while another \ac{LLM} (the Detective) analyzes the responses to determine family relationships. This approach achieved 72\% accuracy in distinguishing between popular architectures such as LLaMA, Mistral, and Gemma.

On the other hand, white-box active fingerprinting assumes full access to a model's weights and architecture. This level of access enables researchers to deliberately embed a unique signature or pattern into the model during the training or fine-tuning phase for ownership verification. For example, authors in____ introduced novel cryptographic fingerprinting techniques called Chain and Hash to prove the ownership of \ac{LLM} models. Their approach involves generating a set of special questions and answers, then concatenating each question with all questions, all potential answers, and a secret key using a secure hashing function like SHA-256. This fingerprint is then incorporated into the model through fine-tuning, with additional measures like meta-prompts and random padding to enhance the robustness. The verification process is performed by querying the model with the fingerprinting questions and checking whether it produces the expected responses. Similarly, ____ proposed a lightweight instruction tuning approach to embed verifiable fingerprints in \acp{LLM}. This technique trains models to generate predetermined outputs when presented with carefully crafted multilingual character sequences (secret keys). Remarkably, using only 60 training instances, this technique demonstrated perfect fingerprint retention across 11 different \acp{LLM}, even after extensive fine-tuning.

Despite these advances, existing \acp{LLM} watermarking and fingerprinting methods have several limitations. Watermarking techniques are often vulnerable to adversarial attacks, while some fingerprinting methods are computationally intensive and require access to model weights. To address these challenges, we propose a novel passive fingerprinting technique that identifies language models in real-time by leveraging their intrinsic \acp{ITT} characteristics, that are inherently difficult to manipulate or obscure via adversarial attacks. Furthermore, our approach is computationally efficient and can be deployed for online detection and monitoring, offering fine-grained fingerprinting capabilities that accurately distinguish between model families and even their variants.