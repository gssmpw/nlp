[
  {
    "index": 0,
    "papers": [
      {
        "key": "child2019generating",
        "author": "Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya",
        "title": "Generating long sequences with sparse transformers"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "Kitaev2020Reformer",
        "author": "Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya",
        "title": "Reformer: The Efficient Transformer"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "wang2020linformer",
        "author": "Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao",
        "title": "Linformer: Self-attention with linear complexity"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "beltagy2020longformer",
        "author": "Beltagy, Iz and Peters, Matthew E and Cohan, Arman",
        "title": "Longformer: The long-document transformer"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "zaheer2020big",
        "author": "Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others",
        "title": "Big bird: Transformers for longer sequences"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "xiong2021nystromformer",
        "author": "Xiong, Yunyang and Zeng, Zhanpeng and Chakraborty, Rudrasis and Tan, Mingxing and Fung, Glenn and Li, Yin and Singh, Vikas",
        "title": "Nystr{\\\"o}mformer: A nystr{\\\"o}m-based algorithm for approximating self-attention"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "choromanski2021rethinking",
        "author": "Krzysztof Marcin Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Quincy Davis and Afroz Mohiuddin and Lukasz Kaiser and David Benjamin Belanger and Lucy J Colwell and Adrian Weller",
        "title": "Rethinking Attention with Performers"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "tay2020efficient",
        "author": "Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald",
        "title": "Efficient transformers: A survey"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "kim-2014-convolutional",
        "author": "Kim, Yoon",
        "title": "Convolutional Neural Networks for Sentence Classification"
      },
      {
        "key": "bai2018empirical",
        "author": "Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen",
        "title": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling"
      },
      {
        "key": "kalchbrenner2016neural",
        "author": "Kalchbrenner, Nal and Espeholt, Lasse and Simonyan, Karen and Oord, Aaron van den and Graves, Alex and Kavukcuoglu, Koray",
        "title": "Neural machine translation in linear time"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "dauphin2017language",
        "author": "Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David",
        "title": "Language modeling with gated convolutional networks"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "he2016deep",
        "author": "He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian",
        "title": "Deep residual learning for image recognition"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "hochreiter1997long",
        "author": "Hochreiter, Sepp and Schmidhuber, J{\\\"u}rgen",
        "title": "Long short-term memory"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "gehring2017convolutional",
        "author": "Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N",
        "title": "Convolutional sequence to sequence learning"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "bai2018empirical",
        "author": "Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen",
        "title": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "wu2018pay",
        "author": "Felix Wu and Angela Fan and Alexei Baevski and Yann Dauphin and Michael Auli",
        "title": "Pay Less Attention with Lightweight and Dynamic Convolutions"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "tay-etal-2021-pretrained",
        "author": "Tay, Yi  and\nDehghani, Mostafa  and\nGupta, Jai Prakash  and\nAribandi, Vamsi  and\nBahri, Dara  and\nQin, Zhen  and\nMetzler, Donald",
        "title": "Are Pretrained Convolutions Better than Pretrained Transformers?"
      }
    ]
  }
]