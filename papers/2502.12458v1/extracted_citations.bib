@inproceedings{Kitaev2020Reformer,
title={Reformer: The Efficient Transformer},
author={Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rkgNKkHtvB}
}

@article{bai2018empirical,
  title={An empirical evaluation of generic convolutional and recurrent networks for sequence modeling},
  author={Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
  journal={arXiv preprint arXiv:1803.01271},
  year={2018},
  url={https://arxiv.org/pdf/1803.01271.pdf}
}

@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020},
  url={https://arxiv.org/pdf/2004.05150.pdf}
}

@article{child2019generating,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019},
url={https://arxiv.org/pdf/1904.10509.pdf}
}

@inproceedings{choromanski2021rethinking,
title={Rethinking Attention with Performers},
author={Krzysztof Marcin Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Quincy Davis and Afroz Mohiuddin and Lukasz Kaiser and David Benjamin Belanger and Lucy J Colwell and Adrian Weller},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=Ua6zuk0WRH}
}

@inproceedings{dauphin2017language,
  title={Language modeling with gated convolutional networks},
  author={Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
  booktitle={International conference on machine learning},
  pages={933--941},
  year={2017},
  organization={PMLR},
url={http://proceedings.mlr.press/v70/dauphin17a/dauphin17a.pdf}
}

@inproceedings{gehring2017convolutional,
  title={Convolutional sequence to sequence learning},
  author={Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},
  booktitle={International conference on machine learning},
  pages={1243--1252},
  year={2017},
  organization={PMLR},
url={http://proceedings.mlr.press/v70/gehring17a/gehring17a.pdf}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016},
url={https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press},
  url={https://blog.xpgreat.com/file/lstm.pdf}
}

@article{kalchbrenner2016neural,
  title={Neural machine translation in linear time},
  author={Kalchbrenner, Nal and Espeholt, Lasse and Simonyan, Karen and Oord, Aaron van den and Graves, Alex and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1610.10099},
  year={2016},
url={https://arxiv.org/pdf/1610.10099.pdf}
}

@inproceedings{kim-2014-convolutional,
    title = "Convolutional Neural Networks for Sentence Classification",
    author = "Kim, Yoon",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1181",
    doi = "10.3115/v1/D14-1181",
    pages = "1746--1751",
}

@inproceedings{tay-etal-2021-pretrained,
    title = "Are Pretrained Convolutions Better than Pretrained Transformers?",
    author = "Tay, Yi  and
      Dehghani, Mostafa  and
      Gupta, Jai Prakash  and
      Aribandi, Vamsi  and
      Bahri, Dara  and
      Qin, Zhen  and
      Metzler, Donald",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.335",
    doi = "10.18653/v1/2021.acl-long.335",
    pages = "4349--4359",
}

@article{tay2020efficient,
  title={Efficient transformers: A survey},
  author={Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  journal={ACM Computing Surveys (CSUR)},
  year={2020},
  publisher={ACM New York, NY},
url={https://dl.acm.org/doi/pdf/10.1145/3530811}
}

@article{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020},
url={https://arxiv.org/pdf/2006.04768.pdf}
}

@inproceedings{xiong2021nystromformer,
  title={Nystr{\"o}mformer: A nystr{\"o}m-based algorithm for approximating self-attention},
  author={Xiong, Yunyang and Zeng, Zhanpeng and Chakraborty, Rudrasis and Tan, Mingxing and Fung, Glenn and Li, Yin and Singh, Vikas},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={16},
  pages={14138--14148},
  year={2021},
url={https://arxiv.org/pdf/2102.03902.pdf}
}

@article{zaheer2020big,
  title={Big bird: Transformers for longer sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={17283--17297},
  year={2020},
url={https://proceedings.neurips.cc/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf}
}

