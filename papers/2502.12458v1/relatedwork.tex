\section{Related Work}
\subsection{Efficient Transformers} \label{eff_trans_rel_work}

Multiple Transformer variants have been proposed to improve efficiency and handle longer sequences. 
A sparse factorization of the attention matrix was used for reducing the overall complexity from quadratic to $\mathcal{O}(n\sqrt{n})$ for generative modeling of long sequences \citep{child2019generating}. 
\citet{Kitaev2020Reformer} introduced the Reformer model, further reducing the complexity to $\mathcal{O}(n\log{}n)$ via locality-sensitive-hashing. 
A number of linear complexity Transformers have been proposed including Linformer \citep{wang2020linformer}, Longformer \citep{beltagy2020longformer}, BigBird \citep{zaheer2020big}, Nystr{\"o}mformer \citep{xiong2021nystromformer}, and Performer \citep{choromanski2021rethinking} to name a few. 
We refer the reader to a survey on efficient Transformers \citep{tay2020efficient} that succinctly captures the techniques these models use.
% to alleviate the bottleneck of quadratic complexity in self-attention and to scale to longer sequences. 

% In our work we will evaluate some of these efficient Transformer variants in terms of efficiency (training speed, memory usage, latency) and performance in a task-oriented conversation domain. 

\subsection{CNN-based models}

CNN-based models have long been proposed for sequence modeling and NLP applications \citep{kim-2014-convolutional, bai2018empirical, kalchbrenner2016neural}. 
They are lightweight, fast and accurate, especially for text classification. 
CNN models with gated linear units \citep{dauphin2017language} and residual connections \citep{he2016deep} achieved impressive results over Long-Short Term Memory (LSTM) \citep{hochreiter1997long} in seq2seq modeling \citep{gehring2017convolutional}. 
A simple causal CNN architecture demonstrated holding longer memory effectively, outperformed recurrent networks in a diverse range of tasks \citep{bai2018empirical}.
LightConv and DynamicConv \citep{wu2018pay} are CNN-based models for seq2seq learning which outperform Transformers on a series of canonical benchmarks such as machine translation and language modeling.  
\citet{tay-etal-2021-pretrained} observed that pretrained CNN models are competitive with pretrained Transformers in a set of NLP tasks such as sentiment classification, toxicity detection, news classification, and query understanding. 
However, they also noted that purely CNN-based models may not perform well where a cross-attention inductive bias is essential. 


Motivated by the need for more efficient architectures, we evaluate both CNN-based models and efficient Transformers and analyze their trade-offs in conversational settings.