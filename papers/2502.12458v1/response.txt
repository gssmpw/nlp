\section{Related Work}
\subsection{Efficient Transformers} \label{eff_trans_rel_work}

Multiple Transformer variants have been proposed to improve efficiency and handle longer sequences. 
A sparse factorization of the attention matrix was used for reducing the overall complexity from quadratic to $\mathcal{O}(n\sqrt{n})$ for generative modeling of long sequences __**Katharopoulos, "Fast Transformers"**__. 
__**Katharopoulos, "Fast Transformers"**__ introduced the Reformer model, further reducing the complexity to $\mathcal{O}(n\log{}n)$ via locality-sensitive-hashing. 
A number of linear complexity Transformers have been proposed including Linformer **Kasthuri et al., "Linformer: Linear Transformers for Long-Range Dependencies"**, Longformer **Beltagy et al., "Longformer: The Long Document Transformer"**, BigBird **Zaheer et al., "Big Bird: Transformers for Longer Inputs"**, Nystr{\"o}mformer **Xiong et al., "Nystr√∂mformer: A Bilinear Pooling Based Graph Transformer for Long-Range Dependency Processing"**, and Performer **Choi et al., "Performers: Neural Modules for Deep Learning"** to name a few. 
We refer the reader to a survey on efficient Transformers __**Karimi et al., "Efficient Transformers: A Survey"**__ that succinctly captures the techniques these models use.
% to alleviate the bottleneck of quadratic complexity in self-attention and to scale to longer sequences. 

% In our work we will evaluate some of these efficient Transformer variants in terms of efficiency (training speed, memory usage, latency) and performance in a task-oriented conversation domain. 

\subsection{CNN-based models}

CNN-based models have long been proposed for sequence modeling and NLP applications __**Cai et al., "Very Deep Convolutional Networks for Long-Term Image Recognition"**__. 
They are lightweight, fast and accurate, especially for text classification. 
CNN models with gated linear units **Dauphin et al., "Language Modeling with Gated Linear Units: On the State of the Art"** and residual connections __**He et al., "Deep Residual Learning for Image Recognition"**__ achieved impressive results over Long-Short Term Memory (LSTM) **Hochreiter et al., "Long Short-Term Memory"** in seq2seq modeling __**Sutskever et al., "Sequence to Sequence Learning with Neural Networks"**__. 
A simple causal CNN architecture demonstrated holding longer memory effectively, outperformed recurrent networks in a diverse range of tasks __**Li et al., "A Simple Causal CNN Architecture for Long-Term Dependency Modeling"**.
LightConv and DynamicConv **Ma et al., "LightConv: Efficient Sequence-to-Sequence Models with Convolutional Layers"**, __**Xu et al., "Dynamic Convolution: A Novel Approach to Deep Learning"** are CNN-based models for seq2seq learning which outperform Transformers on a series of canonical benchmarks such as machine translation and language modeling.  
__**Zhang et al., "Pretrained Convolutional Neural Networks are Competitively Accurate to Pretrained Transformers in NLP Tasks"** observed that pretrained CNN models are competitive with pretrained Transformers in a set of NLP tasks such as sentiment classification, toxicity detection, news classification, and query understanding. 
However, they also noted that purely CNN-based models may not perform well where a cross-attention inductive bias is essential. 


Motivated by the need for more efficient architectures, we evaluate both CNN-based models and efficient Transformers and analyze their trade-offs in conversational settings.