\section{Related Work}
\subsection{Efficient Transformers} \label{eff_trans_rel_work}

Multiple Transformer variants have been proposed to improve efficiency and handle longer sequences. 
A sparse factorization of the attention matrix was used for reducing the overall complexity from quadratic to $\mathcal{O}(n\sqrt{n})$ for generative modeling of long sequences ____. 
____ introduced the Reformer model, further reducing the complexity to $\mathcal{O}(n\log{}n)$ via locality-sensitive-hashing. 
A number of linear complexity Transformers have been proposed including Linformer ____, Longformer ____, BigBird ____, Nystr{\"o}mformer ____, and Performer ____ to name a few. 
We refer the reader to a survey on efficient Transformers ____ that succinctly captures the techniques these models use.
% to alleviate the bottleneck of quadratic complexity in self-attention and to scale to longer sequences. 

% In our work we will evaluate some of these efficient Transformer variants in terms of efficiency (training speed, memory usage, latency) and performance in a task-oriented conversation domain. 

\subsection{CNN-based models}

CNN-based models have long been proposed for sequence modeling and NLP applications ____. 
They are lightweight, fast and accurate, especially for text classification. 
CNN models with gated linear units ____ and residual connections ____ achieved impressive results over Long-Short Term Memory (LSTM) ____ in seq2seq modeling ____. 
A simple causal CNN architecture demonstrated holding longer memory effectively, outperformed recurrent networks in a diverse range of tasks ____.
LightConv and DynamicConv ____ are CNN-based models for seq2seq learning which outperform Transformers on a series of canonical benchmarks such as machine translation and language modeling.  
____ observed that pretrained CNN models are competitive with pretrained Transformers in a set of NLP tasks such as sentiment classification, toxicity detection, news classification, and query understanding. 
However, they also noted that purely CNN-based models may not perform well where a cross-attention inductive bias is essential. 


Motivated by the need for more efficient architectures, we evaluate both CNN-based models and efficient Transformers and analyze their trade-offs in conversational settings.