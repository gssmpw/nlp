\label{sec: meth_o}

\begin{table*}
  \scriptsize
  \centering
  \caption{\textbf{Summary of representative approaches for related tasks on composed image retrieval.}}
  \label{tab: task_based_ir}
   % \resizebox{14.5cm}{!}{
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline
      Related Task & Method & Year & Visual Encoder & Text Encoder & Key Aspect\\
    \hline 
    
    \multirow{7}{*}{Attribute-based} 
    & AMNet~\cite{amnet} & 2017 & Alex, VGGNet & - & Memory-Augmented \\ 
    \cline{2-6}
    & EItree~\cite{eitree} & 2018 & ResNet-50 & BLSTM & EI-tree \\ 
    \cline{2-6}
    & FashionSearchNet~\cite{FashionSearchNet} & 2018 & AlexNet & - & Attribute Localization \\
    \cline{2-6}
    & EMASL~\cite{EMASL} & 2018 & AlexNet & - & Attribute Localization \\
    \cline{2-6}
    & AMGAN~\cite{amgan} & 2020 & Generator-Encoder & - & GAN-based \\
    \cline{2-6}
    & ADDE~\cite{adde} & 2021 & AlexNet, ResNet 18 & - & Attribute-driven Disentangled \\
    \cline{2-6}
    & FIRAM~\cite{firam} & 2021 & StyleGAN & - & GAN-based \\  

    \hline
    
    \multirow{4}{*}{Sketch-based}
   & TSFGIR~\cite{tsfgir} & 2022 & CNN  & LSTM & Quadruplet Deep Network\\
    \cline{2-6}
    & TASK-former~\cite{taskformer} & 2022 & CLIP-B & CLIP-B & Auxiliary Tasks Learning\\
    \cline{2-6}
    & SceneTrilogy~\cite{SceneTrilogy} & 2023 & VGG-16  & Bi-GRU & Conditional Invertible Networks \\
    \cline{2-6}
    & STNET~\cite{stnet} & 2024 & CLIP & CLIP & Auxiliary Tasks Learning\\
    \cline{2-6}
    & STD4FGIR~\cite{STD4FGIR} & 2024 & CLIP-L & CLIP-L & Textual-inversion-based \\
    
    \hline

    \multirow{2}{*}{Remote Sensing-based} 
    & WEICOM~\cite{weicom} & 2024 & CLIP-L, RemoteCLIP-L & CLIP-L, RemoteCLIP-L & Weighted Average\\
    \cline{2-6}
    & SHF~\cite{shf} & 2024 & ResNet-50 & LSTM & Hierarchical Fusion\\   

    \hline

    \multirow{5}{*}{Dialog-based} 
    & DIIR~\cite{guo2018dialog} & 2018 & ResNet-101 & MLP+CNN & Reinforcement Learning\\
     \cline{2-6}
    & CFIR~\cite{cfir2021} & 2021 & ResNet-101, ResNet-152 & self-attention blocks & Mutual Attention Strategy \\
    \cline{2-6}
    & FashionNTM~\cite{fashionntm} & 2023 & FashionVLP & FashionVLP & Cascaded Memory\\  
    \cline{2-6}
    & IRR~\cite{irr} & 2023 & FashionBERT& CLIP-B & Iterative Sequence Refinement\\ 
    \cline{2-6}
     & Fashion-GPT~\cite{fashiongpt} & 2023 & Swin Transformer& RoBERTa& LLM-integration\\
     \cline{2-6}
    & LLM4MS~\cite{llm4ms} & 2024 & CLIP-L & Flan T5 XL & LLM-integration\\
    
    \hline
    
    \multirow{4}{*}{Video-based} 
    & CoVR-BLIP~\cite{Covr} & 2024 & CLIP, BLIP & CLIP, BLIP & BLIP-based Fusion\\
    \cline{2-6}
    & ECDE~\cite{ecde} & 2024 & BLIP & BLIP & Video Contextual Complement  \\
    \cline{2-6}
    & TFR-CVR~\cite{tfrcvr} & 2024 & BLIP, CLIP-L & BLIP, CLIP-L & Language-level Reasoning \\
    \cline{2-6}
    & CoVR-BLIP2~\cite{CoVR2} & 2024 & BLIP-2 & BLIP-2 & BLIP2-based Fusion\\ 
    
    \hline
    \end{tabular}
 % }
\end{table*}

\section{Related Tasks of Composed Image Retrieval}
Beyond the primary task of CIR, researchers have explored various related tasks that cater to diverse retrieval needs in real-world scenarios. Here, we present five representative related tasks that involve different types of multimodal queries: attribute-based, sketch-based, remote sensing-based, dialog-based, and video-based. For ease of reference, we summarize the methods proposed for  these tasks in Table~\ref{tab: task_based_ir}. 

\subsection{Attribute-based}
Before the formal introduction of natural language modification-based CIR, another flexible image retrieval task—where the query involves a reference image alongside attribute-based modifications—had garnered significant attention. This task, constrained to predefined attributes, primarily finds applications in domains with structured attribute sets, such as fashion and face image retrieval.
Zhao~\textit{et al.}~\cite{amnet} pioneered this task by proposing AMNet, which comprises a memory block for storing the representations of various attribute values. The attribute manipulation is achieved by directly retrieving specific attribute representations from the memory block and then fusing them with the representation of the reference image to retrieve target images. Similarly, ADDE~\cite{adde} also incorporates a memory block for storing the attribute values. Unlike AMNet, ADDE devises an attribute-driven disentanglement module, which uses attributes as supervised signals to guide the learning of disentangled image representations. Through indexing the appropriate attribute representations from the memory block, the disentangled representations can be modified by removing, retaining, or adding specific attribute values, enabling effective attribute manipulation. Recognizing that different attributes are correlated with different regions in fashion images, FashionSearchNet~\cite{FashionSearchNet} employs attribute activation maps to extract region-specific attribute features. Each fashion item can be represented by a set of attribute features, and the attribute manipulation is accomplished by directly replacing the specific attribute feature. Sharing a similar spirit, EMASL~\cite{EMASL} extracts different part features for the acquisition of attribute features based on the designed rules. For example, the upper part is associated with the collar, and the side parts are correlated with the left/right sleeves. %Correspondingly, the attribute manipulation can also be accomplished by replacing the specific attribute representation.
EITree~\cite{eitree} enhances interpretability by organizing fashion concepts into hierarchical tree structures. Guided by this tree structure, EITree generates meaningful image representations where each dimension corresponds to a specific fashion concept. This structure allows for seamless integration of concept-level user feedback, such as attribute manipulation, into the interpretable representation of fashion items.
Building on the success of generative models in image editing, AMGAN~\cite{amgan} introduces an end-to-end generative attribute manipulation framework. This framework generates a prototype image that aligns with user-desired attribute modifications on a reference image to improve target image retrieval. AMGAN consists of a generator and a discriminator: the generator employs visual-semantic and pixel-wise consistency constraints, while the discriminator incorporates semantic learning for precise attribute manipulation and adversarial metric learning to enhance fashion search effectiveness.
Focusing on face image retrieval, FIRAM~\cite{firam} leverages GANs in its framework design. Unlike AMGAN, which generates a prototype image directly, FIRAM aims to learn sparse and orthogonal basis vectors within StyleGAN's latent space. This approach disentangles attribute semantics, allowing for independent attribute adjustment and preference assignment.


\subsection{Sketch-based} 
Many studies have focused on combining sketch and text descriptions to assist users in retrieving target images. Among these, TSFGIR~\cite{tsfgir} pioneers the exploration of complementarity between text and sketch modalities. It introduces a multi-modal quadruplet deep neural network that encourages both the input sketch and text to be closer to the corresponding positive target image than to any negative target image. Additionally, this work contributes a dataset consisting of $1,374$ sketch-photo-text triplets for shoes.
To model more explicit interactions between sketch and text inputs, TASK-former~\cite{taskformer} extends the dual-encoder-based CLIP framework by incorporating a sketch encoder and two new pretraining objectives: multi-label classification, enabling the three encoders to recognize objects, and caption generation, which reconstructs the input text from joint sketch-text embeddings (computed as the simple addition of the text and sketch embeddings). For more complex scenarios involving rough sketches paired with complementary text descriptions, Gatti \textit{et al.}~\cite{stnet} present a dataset comprising approximately 2M queries and 108K natural scene images. They also propose a multimodal transformer-based framework, STNET. Like TASK-former, STNET employs three CLIP encoders for text, sketch, and image modalities. However, beyond contrastive learning, it incorporates three task-specific pretraining objectives: (a) object classification, (b) sketch-guided object detection, and (c) sketch reconstruction.
Distinct from the aforementioned methods, STD4FGIR~\cite{STD4FGIR} draws inspiration from textual inversion techniques by transforming the input sketch into a pseudo-word token, allowing the multimodal input to be directly encoded by the CLIP text encoder. To address the challenge of costly data annotation for sketch-based fine-grained image retrieval, STD4FGIR is trained using only sketch-image pairs. It treats the difference between the image embedding and the sketch embedding as a proxy for the missing text query and introduces a compositionality constraint to model this relationship. To achieve fine-grained matching between the composed query and the target image, STD4FGIR incorporates two innovative loss functions: a region-aware triplet loss for precise alignment and a sketch-to-photo reconstruction loss to enhance representation learning.
It is worth noting that STD4FGIR can be applied to various applications, such as object-sketch-based scene retrieval, domain attribute transfer, and sketch+text-based fine-grained image generation. In parallel, SceneTrilogy~\cite{SceneTrilogy} focuses on learning a flexible joint embedding capable of supporting any combination of modalities—sketches, images, or text—as queries for diverse retrieval and captioning tasks. Leveraging conditional invertible networks, SceneTrilogy disentangles the feature representation of the input sketch, text, or photo into two components: a modality-agnostic part and a modality-specific part. The modality-agnostic parts across all three modalities are aligned using contrastive loss to enable cross-modal retrieval, while the modality-specific parts are optimized for self-reconstruction, ensuring effective representation learning.

\subsection{Remote Sensing-based} 
In recent years, earth observation via remote sensing has experienced a significant increase in data volume, posing challenges in managing and extracting pertinent information. To enhance search capabilities with greater expressiveness and flexibility, Psomas~\textit{et al.}~\cite{weicom} introduce the concept of CIR into remote sensing and build a benchmark dataset. As a pioneer study, it focuses on the attribute-based CIR, which aims to retrieve target images that share the same class(es) with the given reference image and possess the desired attribute(s) specified by the text description. This work designs a training-free method, WEICOM, which introduces a modality control parameter for balancing the importance of normalized image-oriented and text-oriented similarities. Both pretrained CLIP and RemoteCLIP~\cite{liu2024remoteclip} are explored for feature embedding. Beyond this work, Wang~\textit{et al.}~\cite{shf} study the remote sensing image retrieval with natural language-based text feedback. Recognizing that previous studies on CIR primarily focused on intrinsic attributes of target objects, while neglecting crucial extrinsic information such as spatial relationships in the remote sensing domain, Wang~\textit{et al.}~\cite{shf} propose a scene graph-aware hierarchical fusion network (SHF). This approach incorporates remote sensing image scene graphs as supplementary input data for enhancing structured image representation learning and target image retrieval. SHF employs a two-stage multimodal information fusion process. In the first stage, it fuses features from the remote sensing image scene graph and the remote sensing reference image at multiple levels to comprehensively capture the visual content. In the second stage, the modification text features are further fused with the final scene features from the first stage using a content modulator and a style modulator, which effectively capture and apply content and style changes.

\subsection{Dialog-based} 
The task of dialog-based CIR is proposed to address the challenge that users often struggle to initially express their intentions clearly or provide detailed descriptions of their objects of interest. Unlike traditional single-turn CIR, it allows users to iteratively refine their queries until they find a satisfactory item. This further necessitates models to integrate historical retrieval data with the current query to effectively locate the target image. 
Towards this end, DIIR~\cite{guo2018dialog} frames the task as a reinforcement learning problem, where the dialog system is rewarded for enhancing the rank of the target image within each dialog turn. Instead of using a real user to interact and train the dialog system, it introduces a user simulator based on the existing relative image captioning model \textit{Show, Attend, and Tell}~\cite{xu2015show}. The user simulator is trained on a newly collected dataset with Amazon Mechanical Turk, where both discriminative and relative captions are manually annotated within a shopping chatting scenario. Notably, since the data annotation for training a multi-turn simulator is quite expensive, the work only explored a single-turn user simulator. 
To enrich the decision-making process for retrieving the target image, CFIR~\cite{cfir2021} designs a three-way RNN-based framework, which evaluates the matching score of each candidate target image from three perspectives: 1) visual similarity between the composed query and the candidate image, 2) alignment between the reference-target image difference representation and the textual feedback representation; and 3) alignment between attribute representations of the candidate target image and the textual feedback representation. 
Meanwhile, CFIR contributes a large-scale multi-turn dataset, named Multi-turn FashionIQ, from the FashionIQ~\cite{wu2021fiq} by integrating multiple single-turn triplets. 
Inspired by the exceptional ability of Neural Turing Machines (NTMs)~\cite{graves2014neural} in handling complex long-term relationships, FashionNTM~\cite{fashionntm} introduces a Cascaded Memory Neural Turing Machine (CM-NTM) for multi-turn fashion image retrieval. Comprising a controller, individual read/write heads, and memory blocks, CM-NTM sequentially processes multi-turn queries with a cascaded mechanism, where each block's input comprises both the current turn's query feature and the output from the preceding block, supporting modeling the intricate relationships within input sequences. 
Beyond previous studies, IRR~\cite{irr} proposes a generative conversational composed retrieval framework, which formulates the task as a multimodal token sequence with alternating reference images and modification texts in the historical turns. IRR aims to autoregressively predict the target image feature based on the historical session data with the GPT decoder~\cite{radford2018improving}. 
% 最后是将LLM引入该任务
Recently, LLM4MS~\cite{llm4ms} and Fashion-GPT~\cite{fashiongpt}  integrate LLMs into the retrieval system to realize dialog-based CIR. Specifically, LLM4MS employs a Q-former in BLIP2~\cite{li2023blip2} to translate image information into textual pseudo-tokens and then adopt an LLM T5~\cite{raffel2020exploring} to take into account all query information for retrieving the target image. To adapt the T5 model for this specific task while preserving its knowledge, LLM4MS employs LoRA~\cite{hu2021lora} techniques on the query and value matrices of all self-attention and cross-attention layers within T5 for efficient fine-tuning. Towards building a commercial fashion retrieval system, Fashion-GPT~\cite{fashiongpt} integrates ChatGPT with a pool of retrieval models in the fashion domain for handling users' diverse retrieval demands. 


\subsection{Video-based} 
Composed video retrieval (CoVR) aims to retrieve target videos for a given reference image/video and a modification text. Ventura~\textit{et al.}~\cite{Covr,CoVR2} first introduce this task and develop models, named CoVR-BLIP and CoVR-BLIP2, by adapting BLIP and BLIP2 to the CoVR tasks, respectively. Since each video can be represented by sampled frames, these models are able to address both CIR and CoVR simultaneously. This work also contributes a dataset named WebVid-CoVR with $1.6$ million triplets by leveraging an LLM to automatically generate the modification text for two similar videos, identified by similar captions.
One key issue of this dataset is that its sample modifications are mainly regarding static color/shape/object changes, which do not need temporal understanding. To address this issue, Hummel~\textit{et al.}~\cite{tfrcvr} introduce EgoCVR, a manually curated dataset with $2,295$ queries. Additionally, they present a train-free method TF-CVR similar to the previous CIR model CIReVL~\cite{cirevl}. TF-CVR involves a video captioning model to generate a caption for the reference video. Then it uses an LLM to combine the video caption and modification text to obtain the target video caption and employs an existing text-to-video model to perform the cross-modal retrieval of target videos. To avoid selecting semantically similar but visually unrelated videos, they introduce a similarity-based filtering strategy to first narrow the scope of candidate videos for TF-CVR to rank.
To mine the rich query-specific context for promoting the target video retrieval, ECDE~\cite{ecde} introduces a novel CoVR framework. This framework uses detailed language descriptions of the given reference video derived from a multi-modal conversation model as the additional input to explicitly encode query-specific contextual information. For discriminative embedding learning, ECDE not only aligns the joint multimodal embedding of the input query with the conventional visual embedding of the target video, but also aligns that with the text embedding and vision-text joint embedding of the target video, respectively. 