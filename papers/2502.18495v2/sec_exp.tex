\label{sec: exp}
\section{Benchmarks and Experiments}
In this section, we first present the public datasets related to CIR, and then provide experimental results and analyses of representative methods. 

\begin{table*}[h!]
\centering
\caption{\textbf{Statistics of datasets for composed image retrieval and its related tasks.}}
\label{tab:dataset_summary}
 \resizebox{12cm}{!}{
\begin{tabular}{l|c|c|c|c}
    \hline
    \textbf{Dataset} & \textbf{Data Type}& \textbf{Vision Scale} & \textbf{Triplet Scale} & \textbf{Triplet Construction} \\
    \hline \hline
    
    \multicolumn{5}{c}{\textit{\textcolor{gray}{Datasets for Composed Image Retrieval}}} \\
    
    % --- Human Annotated ---
    FashionIQ~\cite{wu2021fiq} & image+text &  77.6K &  30.1K & Human Annotation \\ 
    Shoes~\cite{berg2010automatic} & image+text &  14.7K &  10.7K & Human Annotation \\
    CIRR~\cite{liu2021CIRPLANT} & image+text &  21.6K &  36.5K & Human Annotation \\
    B2W~\cite{forbes2019b2w} & image+text &  3.5K &  16.1K & Human Annotation \\
    CIRCO~\cite{searle} & image+text &  120K  &  1.0K & Human Annotation \\
    GeneCIS~\cite{genecis} & image+text & 33.3K &  8.0K & Human Annotation \\
    % --- Template-base Generation ---
    Fashion200K~\cite{Han2017fashion} & image+text &  200K &  205K & Template-base Generation \\
    MIT-States~\cite{Phi2015discover} & image+text &  53K & - & Template-base Generation \\
    CSS~\cite{vo2019tirg} & image+text & - &  32K & Template-base Generation \\
    SynthTriplets18M~\cite{compodiff} & image+text & - &  18.8M & Template-base Generation \\
    % --- LLM-base Generation ---
    LaSCo~\cite{levy2024case} & image+text &  121.5K &  389.3K & LLM-base Generation \\

    \cdashline{1-5}
    \multicolumn{5}{c}{\textit{\textcolor{gray}{Datasets for Related Tasks of Composed Image Retrieval}}} \\

    % ========== Template-base Generation ==========
    Shopping100k~\cite{EMASL} & image+attributes &  101K &  1.1M & Template-base Generation \\
    WebVid-CoVR~\cite{ventura2024covr} & video+text &  130.8K &  1.6M & LLM-base Generation \\
    FS-COCO~\cite{chowdhury2022fscoco} & sketch+image+text &  10K &  10K & Template-base Generation \\
    SketchyCOCO~\cite{gao2020sketchcoco} & sketch+image+text &  14K &  14K & Template-base Generation \\
    CSTBIR~\cite{stnet} & sketch+image+text &  108K &  2M & Template-base Generation \\
    PATTERNCOM~\cite{psomas2024cir4rs} & image+text&  30K &  21K & Template-base Generation\\
    % ========== Human Annotated ==========
    Airplane, Tennis, and WHIRT~\cite{shf} & image+scene graph+text&  7.7K &  8.7K & Human Annotation \\
    Multi-turn FashionIQ~\cite{cfir2021} & image+text & 13.6K & 11.5K & Human Annotation \\
    % ========== LLM-base Generation ==========
    
    \hline
    \multicolumn{5}{l}{\small *Vision scale means the scale of images/sketch-image pairs/videos. The triplet scale in Multi-turn FashionIQ refers to the number of sessions.}
\end{tabular}
 }
\end{table*}


\subsection{Datasets.}
The statistics of datasets for the CIR and its related tasks are summarized in Table~\ref{tab:dataset_summary}. 


\textbf{FashionIQ.} 
The FashionIQ dataset~\cite{wu2021fiq} is a natural language-based interactive fashion retrieval dataset, crawled from \textit{Amazon.com}. It provides human-generated captions that distinguish similar pairs of garment images together. The fashion items within the dataset belong to three categories: dress, shirt, and top\&tee. It contains $\sim77.6$K images and $\sim30.1$K triplets, with $\sim46.6$K images and $\sim18$K triplets in the training set, $\sim15.5$K images and $\sim6$K triplets in the validation set, and $\sim15.5$K images and $\sim6$K triplets in the test set. While being a challenge dataset, the test set is not publicly accessible.
Notably, FashionIQ comprises two evaluation protocols: the VAL-Split~\cite{chen2020val} and the Original-Split~\cite{wu2021fiq}. The VAL-Split is introduced by the early-stage CIR study, which constructs the candidate image set for testing based on the union of the reference images and target images in all the triplets of the validation set. The Original-Split has recently been adopted, and it directly uses the original candidate image set provided by the FashionIQ dataset for testing. 

% is collected by selecting visually similar images based on product titles using TF-IDF scores. Around $10,000$ target images from three fashion categories are chosen, and two captions are gathered for each image pair via crowdsourcing. Participants provided natural language descriptions comparing the target to the reference image. The FashionIQ dataset contains three categories: dress, shirt, and top\&tee. 



\textbf{Shoes.}
The Shoes dataset~\cite{berg2010automatic} is originally collected from \textit{like.com} for the attribute discovery task and further developed by~\cite{guo2018dialog} with relative caption annotations for dialog-based interactive retrieval. The annotations are gathered via human annotation using an interactive interface, which allows for fine-grained attribute descriptions. The dataset includes categories such as boots, sneakers, high heels, clogs, pumps, rain boots, flats, stilettos, wedding shoes, and athletic shoes. Overall, the dataset comprises $\sim14.7$K images and $\sim10.7$K triplets, with $10$K images and $\sim9$K triplets for training, and $\sim4.7$K images and $\sim1.7$K triplets for testing. 

% \textcolor{red}{$21,552$}
\textbf{CIRR.} The CIRR dataset, introduced by Liu~\textit{et al.}~\cite{liu2021CIRPLANT}, is an open-domain dataset constructed using  $\sim21.5$K images sourced from the natural language reasoning dataset $\text{NLVR}^2$~\cite{suhr2019nvlr}. CIRR comprises $\sim36.5$K triplets divided into training, validation, and test sets with an allocation ratio of $8:1:1$. To alleviate the false negative issue, CIRR first clusters similar images into subsets based on their visual similarity before the reference-target image pairs construction. Then, during the subsequent process of annotating modification text for reference-target image pairs, the semantics of the modification text must differentiate the target image from other similar images within the same subset. Specifically, given that each subset contains samples with a high visual similarity to the target image, testing retrieval on this subset places greater demands on the model's discriminative ability.


\textbf{B2W.}
The Birds-to-Words (B2W) dataset~\cite{forbes2019b2w} consists of images of birds sourced from \textit{iNaturalist}, accompanied by paragraphs written by humans to describe the differences between pairs of images. The dataset contains approximately $\sim3.5$K images and $\sim16.1$K triplets. Notably, each text description is relatively detailed, with an average length of $31.38$ words, providing rich insights into the subtle variations across bird images. 


\textbf{CIRCO.} The CIRCO dataset~\cite{searle} is an open-domain dataset developed from the COCO 2017 unlabeled set~\cite{lin2014coco} to address the false negative issues prevalent in existing datasets. Unlike typical CIR datasets, CIRCO includes multiple target images per query, thus significantly reducing the occurrence of false negatives and establishing it as the first CIR dataset with multiple ground-truth target images. Given that CIRCO is specifically designed for evaluating zero-shot cross-image retrieval models, it comprises $1,020$ queries that are partitioned into a validation set and a test set. Specifically, $220$ queries are allocated for validation purposes, while the remaining $800$ for testing. Each query includes a reference image, modification text, and an average of $4.53$ ground truth target images. Utilizing all $120$K images of COCO as the index set, CIRCO provides a vastly larger number of distractors compared to the $2$K images in the CIRR test set.

\textbf{GeneCIS.} 
GeneCIS~\cite{genecis} is an open-domain CIR dataset that serves as a benchmark for evaluating conditional similarity tasks. This dataset includes four subsets: Focus Attribute, Change Attribute, Focus Object, and Change Object, representing four different tasks. Among these, Focus Object and Change Object are constructed based on the COCO~\cite{lin2014coco} dataset, while Focus Attribute and Change Attribute are constructed based on the VAW~\cite{pham2021learning} dataset. Unlike other open-domain datasets, CIRR and CIRCO, which provide modification text, GeneCIS provides a single object name or attribute as the retrieval condition. To reduce the impact of false negatives, a gallery was selected for each triplet as the retrieval candidate set, with gallery sizes ranging from 10 to 15 images. This dataset consists of $\sim8$K triplets.

\textbf{Fashion200K.} The Fashion200K dataset, collected by Han~\textit{et al.}~\cite{Han2017fashion}, comprises  $\sim200$K clothing images, which are categorized into five types: dress, top, pants, skirt, and jacket. Each image comes with a compact attribute-like product description, such as ``black biker jacket''. The dataset is divided into three parts: $\sim172$K images for the training set, $\sim12$K images for the validation set, and $\sim25$K images for the test set. To construct triplets suitable for the CIR task, existing works~\cite{vo2019tirg, wen2021clvcnet} first create image pairs by identifying only one-word differences in their descriptions. The modification text is generated using templates that incorporate the differing words, such as ``replace red with green.'' Based on this construction method, there are $\sim172$K triplets available for training and $\sim33$K triplets for evaluation. 

\textbf{MIT-States.} The MIT-States dataset~\cite{Phi2015discover} features a diverse collection of objects, scenes, and materials in various transformed states. It contains $\sim53$K images, in which each image is tagged with an adjective or state label and a noun or object label (\textit{e.g.}, ``new camera'' or ``cooked beef''). The dataset encompasses $245$ nouns and $115$ adjectives, with each noun being modified by roughly $9$ different adjectives on average. Following~\cite{vo2019tirg}, image pairs that share the same object labels but with different attributes or states can be selected as the reference and target images, and the different attributes or states serve as the modification text. To evaluate the model's capacity for handling unseen objects, $49$ nouns are used for the test and the rest is for training.

\textbf{CSS.} The CSS dataset~\cite{vo2019tirg} is constructed using the CLEVR toolkit~\cite{johnson2017clevr} to generate synthesized images in a $3$-by-$3$ grid scene, showcasing objects with variations in Color, Shape, and Size. Each image is available in both a simplified $2$D blob version and a detailed $3$D rendered version. The dataset comprises $\sim16$K queries for training and $\sim16$K queries for test. Each query is of a reference image ($2$D or $3$D) and a modification, and the target image. Notably, modification texts fall into three categories: adding, removing, or changing object attributes. Examples include ``add red cube'' or ``remove yellow sphere''.  


\textbf{SynthTriplets18M.} SynthTriplets18M~\cite{compodiff} is a large-scale dataset specifically designed for the CIR task. Distinct from datasets relying on human annotation, SynthTriplets18M uses diffusion models to automatically create $\sim18$M triplets consisting of reference images, modification instructions, and target images. Adopting the approach of Instruct Pix2Pix~\cite{brooks2023pix}, the dataset initially creates caption triplets by modifying reference captions with specific instructions and then converts these triplets into image-based triplets using diffusion models, resulting in highly diverse and realistic data. The dataset encompasses a broad spectrum of keywords, enhancing its suitability for open-domain CIR tasks, and provides rich textual prompts such as ``replace ${source} with ${target}.'' This automated generation process enables the creation of rare and diverse image triplets that might not commonly appear in reality.

\textbf{LaSCo.} LaSCo~\cite{levy2024case} is a large-scale, open-domain dataset consisting of natural images, specifically designed for the CIR task. Created with minimal human effort by leveraging the VQA $2.0$ dataset~\cite{goyal2017vqa}, LaSCo leverages ``complementary'' image pairs, which are similar images that yield different answers to the same question, and transforms these question-answer pairs into valid transition texts using GPT-$3$. By exploiting transition symmetry, the dataset has amassed $\sim121.5$K images and $\sim389.3$K image pairs, which are then organized into triplets.


% \textcolor{red}{$101,021$}  \textcolor{red}{$954,091$} \textcolor{red}{$118,317$}
\textbf{Shopping100k.} The Shopping100k dataset~\cite{EMASL} consists of $\sim101.0$K pure clothing images that are characterized by $12$ attributes with $151$ possible attribute values. Following~\cite{adde}, pairs of images that differ by only one or two attributes are utilized as reference and target images. These differing attributes serve as modification attributes, facilitating the construction of triplet data for attribute-based CIR tasks. This approach results in $\sim954.1$K training triplets and $\sim118.3$K testing triplets, forming a robust foundation for developing and evaluating CIR models focused on nuanced fashion attributes. 


\textbf{WebVid-CoVR.}
WebVid-CoVR~\cite{ventura2024covr} is a large-scale dataset designed for CoVR, containing $\sim1.6$M triplets. Each video in the dataset averages $16.8$ seconds in duration, and the modification texts consist of roughly $4.8$ words. With each target video linked to about $12.7$ triplets, the dataset offers rich contextual variations essential for effectively training CoVR models. To ensure robust evaluation, they further introduce a meticulously curated test set known as WebVid-CoVR-Test~\cite{ventura2024covr}, which is manually annotated and consists of $\sim2.5$K triplets. 



% \textbf{ITCPR.}
% The ITCPR Dataset~\cite{liu2024word} is a carefully annotated dataset for Composed Person Retrieval (CPR), designed to evaluate models that retrieve individuals based on both visual and textual information. It includes $20,510$ images and $2,225$ annotated triplets. These triplets consist of reference images, relative captions, and target images, focusing on identifying the same person in different outfits or scenes. ITCPR supports Zero-Shot CPR (ZS-CPR), allowing model evaluation without expensive triplet annotations, making it a key resource for advancing CPR research. 

% FGIR


\textbf{FS-COCO.} FS-COCO~\cite{chowdhury2022fscoco} is a large-scale dataset of freehand scene sketches paired with textual descriptions and corresponding images, designed to advance research in fine-grained scene sketch understanding. It consists of $\sim10$K unique sketches drawn by non-experts, with $\sim7$K for training and $\sim3$K for testing, each matched with a photo from the MS-COCO dataset~\cite{lin2014microsoft} and a descriptive caption. 
FS-COCO includes over $90$ object categories from the COCO-stuff~\cite{caesar2018coco} and provides temporal order information of strokes, which enables detailed studies on scene abstraction and the salience of early vs. late strokes. This dataset serves as a benchmark for fine-grained image retrieval, sketch-based captioning, and understanding the complementary information between sketches and text.

\textbf{SketchyCOCO.} SketchyCOCO~\cite{gao2020sketchcoco} is a large-scale composite dataset tailored for the task of automatic image generation from scene-level freehand sketches. Built on the COCO-stuff dataset~\cite{caesar2018coco}, it includes $\sim14$K unique scene-level sketches paired with corresponding images and textual descriptions, organized into $\sim14$K  sketch-text-image triplets, with $80$\% for training and the remaining $20$\% for testing. Additionally, the dataset includes $\sim20$K triplet examples of foreground sketches, images, and edge maps covering $14$ classes, along with $\sim27$K background sketch-image pairs covering $3$ classes. This layered structure facilitates detailed studies in scene-level sketch-based generation and provides five-tuple data for comprehensive training in both foreground and background synthesis tasks.


\textbf{CSTBIR.} The Composite Sketch+Text Based Image Retrieval (CSTBIR) dataset~\cite{stnet} is a multimodal dataset specifically designed for image retrieval using sketches and partial text descriptions. It includes $\sim108$K natural scene images and $\sim2$M annotated triplets, each containing a reference sketch, a partial text description, and a target image. The natural images and text descriptions are sourced from the Visual Genome dataset~\cite{krishna2017visual}, while the sketches are hand-drawn from the Quick, Draw! dataset~\cite{HaE18}. By intersecting object categories between Visual Genome and Quick, Draw!, the dataset contains $258$ intersecting object classes and is divided into training, validation, and testing sets aligned with Visual Genome’s splits. The training set comprises $\sim97$K images, $\sim484$K sketches, and $\sim1.89$M queries. Additionally, CSTBIR includes three distinct test sets: Test-$1$K, Test-$5$K, and an Open-Category set. Among them, only the Open-Category test set is designed to evaluate model performance on novel object categories not present during training, which contains $70$ novel object categories (of which $50$ are “difficult-to-name”) and corresponding sketches. 



\textbf{PATTERNCOM.} PATTERNCOM~\cite{psomas2024cir4rs} is a new benchmark designed for evaluating remote sensing CIR methods, based on the PatternNet dataset~\cite{zhou2018patternnet}, which is a large-scale, high-resolution remote sensing image collection. PATTERNCOM focuses on selected classes from PatternNet, incorporating query images and corresponding text descriptions that define relevant attributes for each class. For example, the ``swimming pools'' category includes text queries specifying shapes such as ``rectangular,'' ``oval,'' and ``kidney-shaped.'' The dataset encompasses six attributes, with each attribute linked to up to four different classes and two to five values per class. Overall, PATTERNCOM contains over $21$K queries, with positive matches ranging from $2$ to $1,345$ per query. 


\textbf{Airplane, Tennis, and WHIRT.} The Airplane, Tennis, and WHIRT datasets~\cite{shf} are curated for remote sensing CIR. 
Each dataset is organized in terms of quintets, consisting of a reference RS image and its scene graph, a target RS image and its scene graph, and a pair of modifier sentences. The Airplane dataset comprises $1,600$ remote sensing images from UCM~\cite{yang2010bag}, PatternNet~\cite{zhou2018patternnet}, and NWPU-RESISC45~\cite{cheng2017remote}, along with $3,461$ pairs of modifier sentences that describe differences in airplane attributes and spatial relationships between airplanes and other objects. The Tennis dataset includes $1,200$ images featuring tennis courts and $1,924$ manually annotated modifier sentence pairs. It emphasizes target and non-target spatial relationships while ignoring relationships between non-target objects. The WHIRT dataset is the most extensive, consisting of $4,940$ images from WHDLD~\cite{shao2018performance} and $3,344$ reference-target image pairs. Its scene graphs provide comprehensive details on object attributes and spatial relationships, accommodating complex remote sensing scenarios. Each dataset features high-quality annotations by domain experts, serving as robust benchmarks for advanced retrieval tasks in intricate remote sensing environments. 

% \textcolor{red}{$11,506$}
\textbf{Multi-turn FashionIQ.} Multi-turn FashionIQ~\cite{cfir2021} is an extension of the original FashionIQ dataset~\cite{wu2021fiq}, designed to model user interactions in a multi-turn setting for fashion product retrieval. It contains $\sim11.5$K sessions structured as multi-turn interaction, across three clothing types: dress, shirt, and top\&tee. Each session consists of multiple reference images, modification texts, and a target image, with turns ranging from $3$ to $5$. The sessions are constructed by linking single-turn triplets from FashionIQ, matching the target image of one triplet to the reference image of another, thus forming coherent multi-turn sequences. Additionally, the dataset expands the original attribute data to ensure comprehensive coverage, associating each target image with attributes like texture, fabric, shape, part, and style. 

\subsection{Metric.}
\textbf{Recall.} Recall is a widely used metric in CIR to evaluate the effectiveness of retrieval systems. It is often denoted as Recall@$k$ (R@$k$), measuring the proportion of queries for which the correct target image is retrieved within the top $k$ results. Recall@$k$ can be defined by the following formula:
\begin{equation}
    {\text{Recall}@k} = \frac{1}{Q} \sum_{q=1}^{Q} \frac{| \mathcal{R}_q \cap \mathcal{D}_q^k |}{| \mathcal{R}_q |} ,
\end{equation}
where $Q$ denotes the total number of queries, $\mathcal{R}_q$ is the set of all relevant target images for each query $q$, $\mathcal{D}_q^k$ is the set of the top $k$ retrieved items for query $q$, $| \mathcal{R}_q \cap \mathcal{D}_q^k|$ represents the number of target images found in the top $k$ results, $| \mathcal{R}_q |$ represents the total number of target images for query $q$. Notably, in most existing CIR datasets, each query typically corresponds to only one target image, \textit{i.e.}, $| \mathcal{R}_q |=1$, except CIRCO. Additionally, CIRR~\cite{liu2021CIRPLANT} further defines Recall\(_{subset}@k\) to assess how frequently the desired target image appears in the top $k$ results when considering only a specific subset of images. 

\textbf{Mean Average Precision.}
Mean Average Precision at $k$ (mAP@$k$) is a crucial metric for evaluating retrieval systems, particularly in cases where there are multiple relevant items. Initially employed in CIRCO~\cite{searle}, this metric integrates precision across various ranks to yield a single, averaged indicator of the system's effectiveness in retrieving relevant items. The formula for mAP@$k$ is given by:
\begin{equation}
    \text{mAP}@k = \frac{1}{Q} \sum_{q=1}^{Q} \frac{1}{\min(k, \mathcal{R}_q)} \sum_{k=1}^{k} P@k \cdot \text{rel}@k,
\end{equation}
where P@$k$ is the precision at rank $k$, rel@$k$ is a relevance function. The relevance function is an indicator function that equals $1$ if the image at rank $k$ is labeled as positive and equals $0$ otherwise.


\begin{table*}
    \scriptsize
    \centering
    \caption{
   Performance comparison among supervised composed image retrieval models on FashionIQ (VAL split).}
   % \resizebox{14.5cm}{!}{
    \begin{tabular}{l|cc|cc|cc|cc|c}
    \hline 
    \multirow{2}{*}{Model} & \multicolumn{2}{c|}{Dresses} & \multicolumn{2}{c|}{Shirts} & \multicolumn{2}{c|} {Tops\&Tees} & \multicolumn{2}{c|}{Average} & \multirow{2}{*}{Avg.} \\ \cline{2-9}
    &  R@$10$ & R@$50$ & R@$10$ & R@$50$ & R@$10$ & R@$50$ & R@$10$ & R@$50$ & \\
    \hline \hline
    
    % \rowcolor{gray!5} 
    \multicolumn{10}{c}{\textit{\textcolor{gray}{Traditional Encoder-based Methods}}} \\
    
    LSC4TCIR~\cite{chawla2021lsc4cir} \footnotesize{\textcolor{gray}{(CVPRW'21)}} & $19.33 $ & $43.52$ & $14.47$ & $35.47$ & $19.73$ & $44.56$ & $17.84$ & $41.18$ &$29.51$ \\
    CIRPLANT~\cite{liu2021CIRPLANT} \footnotesize{\textcolor{gray}{(ICCV'21)}} & $17.45$ & $40.41$ & $17.53$ & $38.81$ & $21.64$ & $45.38$ & $18.87$ & $41.53$ & $30.20$ \\
    VAL~\cite{chen2020val} \footnotesize{\textcolor{gray}{(CVPR'20)}} & $21.12$ & $42.19$ & $21.03$ & $43.44$ & $25.64$ & $49.49$ & $22.60$ & $45.04$ & $33.82$ \\
    % VAL (Lvv + Lvs)~\cite{} \footnotesize{\textcolor{gray}{(CVPR'20)}} & $21.47$ & $43.83$ & $21.03$ & $42.75$ & $26.71$ & $51.81$ & $23.07$ & $46.13$ & $34.60$ \\
    % ARTEMIS~\cite{} \footnotesize{\textcolor{gray}{(ICLR'22)}} & $25.23$ & $48.64$ & $20.35$ & $43.67$ & $23.36$ & $46.97$ & $22.98$ & $46.43$ & $34.70$ \\
    DATIR~\cite{gu2021datir} \footnotesize{\textcolor{gray}{(MM'21)}} & $21.90$ & $43.80$ & $21.90$ & $43.70$ & $27.20$ & $51.60$ & $23.70$ & $46.40$ & $35.00$ \\
    % ARTEMIS~\cite{} \footnotesize{\textcolor{gray}{(ICLR'22)}} & $24.84$ & $49.00$ & $20.40$ & $43.22$ & $26.63$ & $47.39$ & $23.96$ & $46.54$ & $35.25$ \\
    % VAL (GloVe)~\cite{} \footnotesize{\textcolor{gray}{(CVPR'20)}} & $22.53$ & $44.00$ & $22.38$ & $44.15$ & $27.53$ & $51.68$ & $24.15$ & $46.61$ & $35.38$ \\
    SynthTripletGAN~\cite{tautkute2021Synth} \footnotesize{\textcolor{gray}{(Arxiv'21)}} & $22.60$ & $45.10$ & $20.50$ & $44.08$ & $28.01$ & $52.10$ & $23.70$ & $47.09$ & $35.40$ \\
    VAL+JPM~\cite{yang2021jpm} \footnotesize{\textcolor{gray}{(MM'21)}} & $21.38$ & $45.15$ & $22.81$ & $45.18$ & $27.78$ & $51.70$ & $23.99$ & $47.34$ & $35.67$ \\
    % ARTEMIS~\cite{} \footnotesize{\textcolor{gray}{(ICLR'22)}} & $27.34$ & $51.71$ & $21.05$ & $44.18$ & $24.91$ & $49.87$ & $24.43$ & $48.59$ & $36.51$ \\
    MCR~\cite{zhang2021mcr} \footnotesize{\textcolor{gray}{(MM'21)}} & $26.20$ & $51.20$ & $22.40$ & $46.00$ & $29.70$ & $56.40$ & $26.10$ & $51.20$ & $38.65$ \\
    CoSMo~\cite{lee2021cosmo} \footnotesize{\textcolor{gray}{(CVPR'21)}} & $25.64$ & $50.30$ & $24.90$ & $49.18$ & $29.21$ & $57.46$ & $26.58$ & $52.31$ & $39.45$ \\
    % DCNet~\cite{kim2021dcnet} \footnotesize{\textcolor{gray}{(AAAI'21)}} & $28.95$ & $56.07$ & $23.95$ & $47.30$ & $30.44$ & $58.29$ & $27.78$ & $53.89$ & $40.83$ \\
    ACNet~\cite{li2023acnet} \footnotesize{\textcolor{gray}{(ICME'23)}} & $29.20$ & $55.68$ & $25.12$ & $47.75$ & $30.96$ & $57.72$ & $28.43$ & $53.72$ & $41.07$ \\
    NSFSE~\cite{wang2024NSFSE} \footnotesize{\textcolor{gray}{(TMM'24)}} & $31.12$ & $55.73$ & $24.58$ & $45.85$ & $31.93$ & $58.37$ & $29.21$ & $53.32$ & $41.26$ \\
    SAC~\cite{jandial2022sac} \footnotesize{\textcolor{gray}{(WACV'22)}} & $26.52$ & $51.01$ & $28.02$ & $51.86$ & $32.70$ & $61.23$ & $29.08$ & $54.70$ & $41.89$ \\
    % ARTEMIS~\cite{delmas2022artemis} \footnotesize{\textcolor{gray}{(ICLR'22)}} & $27.16$ & $52.40$ & $21.78$ & $43.64$ & $29.20$ & $54.83$ & $26.05$ & $50.29$ & $38.17$ \\
    ARTEMIS~\cite{delmas2022artemis} \footnotesize{\textcolor{gray}{(ICLR'22)}} & $29.04$ & $53.55$ & $25.56$ & $50.86$ & $33.58$ & $60.48$ & $29.39$ & $54.96$ & $42.18$ \\
    EER~\cite{zhang2022eer} \footnotesize{\textcolor{gray}{(TIP'22)}} & $30.02$ & $55.44$ & $25.32$ & $49.87$ & $33.20$ & $60.34$ & $29.51$ & $55.22$ & $42.36$ \\
    MANME~\cite{li2023manme} \footnotesize{\textcolor{gray}{(TCSVT'24)}} & $31.26$ & $57.66$ & $26.37$ & $47.94$ & $32.33$ & $59.31$ & $29.95$ & $54.90$ & $42.48$ \\
    MLCLSAP~\cite{zhang2023MLCLSAP} \footnotesize{\textcolor{gray}{(TMM'23)}} & $30.74$ & $55.92$ & $26.05$ & $50.64$ & $34.42$ & $61.14$ & $30.40$ & $55.90$ & $43.15$ \\
    MCEM~\cite{zhang2024mcem} \footnotesize{\textcolor{gray}{(TIP'24)}} & $32.11$ & $59.21$ & $27.28$ & $52.01$ & $33.96$ & $62.30$ & $31.12$ & $57.84$ & $44.48$ \\
    CLVC-Net~\cite{wen2021clvcnet} \footnotesize{\textcolor{gray}{(SIGIR'21)}} & $29.85$ & $56.47$ & $28.75$ & $54.76$ & $33.50$ & $64.00$ & $30.70$ & $58.41$ & $44.56$ \\
    CRN-base~\cite{yang2023crn} \footnotesize{\textcolor{gray}{(TIP'23)}} & $30.34$ & $57.61$ & $29.83$ & $55.54$ & $33.91$ & $64.04$ & $31.36$ & $59.06$ & $45.21$ \\
    AlRet-small~\cite{xu2024alret} \footnotesize{\textcolor{gray}{(TMM'24)}} & $30.19$ & $58.80$ & $29.39$ & $55.69$ & $37.66$ & $64.97$ & $32.41$ & $59.82$ & $46.12$ \\
    CRN-large~\cite{yang2023crn} \footnotesize{\textcolor{gray}{(TIP'23)}} & $32.67$ & $59.30$ & $30.27$ & $56.97$ & $37.74$ & $65.94$ & $33.56$ & $60.74$ & $47.15$ \\
    AMC~\cite{zhu2023amc} \footnotesize{\textcolor{gray}{(TOMM'23)}} & $31.73$ & $59.25$ & $30.67$ & $59.08$ & $36.21$ & $66.60$ & $32.87$ & $61.64$ & $47.25$ \\
    CLVC-Net+MU~\cite{chen2022mu} \footnotesize{\textcolor{gray}{(ICLR’24)}} & $31.25$ & $58.35$ & $31.69$ & $60.65$ & $39.82$ & $71.07$ & $34.25$ & $63.36$ & $48.81$ \\
    ComqueryFormer~\cite{xu2023ComqueryFormer} \footnotesize{\textcolor{gray}{(TMM'23)}} & $33.86$ & $61.08$ & $35.57$ & $62.19$ & $42.07$ & $69.30$ & $37.17$ & $64.19$ & $50.68$ \\
    CMAP~\cite{li2024cmap} \footnotesize{\textcolor{gray}{(TOMM'24)}} & $36.44$ & $64.25$ & $34.83$ & $60.06$ & $41.79$ & $69.12$ & $37.69$ & $64.48$ & $51.08$ \\
    Css-Net~\cite{zhang2024cssnet} \footnotesize{\textcolor{gray}{(KBS'24)}} & $33.65$ & $63.16$ & $35.96$ & $61.96$ & $42.65$ & $70.70$ & $37.42$ & $65.27$ & $51.35$ \\
    SDFN~\cite{wu2024sdfn} \footnotesize{\textcolor{gray}{(ICASSP'24)}} & $37.33$ & $64.45$ & $37.10$ & $62.56$ & $43.35$ & $72.11$ & $39.26$ & $66.37$ & $52.81$ \\

    
    \cdashline{1-10}

    % \rowcolor{gray!5} % 设置该行的背景色为浅灰色
    \multicolumn{10}{c}{\textit{\textcolor{gray}{VLP Encoder-based Methods}}} \\
    
    CLIP-ProbCR~\cite{li2024clip} \footnotesize{\textcolor{gray}{(ICMR'24)}} & $30.71$ & $56.55$ & $28.41$ & $52.04$ & $35.03$ & $61.11$ & $31.38$ & $56.57$ & $43.98$ \\
    FashionViL~\cite{han2022fashionvil} \footnotesize{\textcolor{gray}{(ECCV'22)}} & $33.47$ & $59.94$ & $25.17$ & $50.39$ & $34.98$ & $60.79$ & $31.21$ & $57.04$ & $44.12$ \\
    FashionVLP~\cite{goenka2022fashionvlp} \footnotesize{\textcolor{gray}{(CVPR'22)}} & $32.42$ & $60.29$ & $31.89$ & $58.44$ & $38.51$ & $68.79$ & $34.27$ & $62.51$ & $48.39$ \\
    DWC~\cite{huang2024dwc} \footnotesize{\textcolor{gray}{(AAAI'24)}} & $32.67$ & $57.96$ & $35.53$ & $60.11$ & $40.13$ & $66.09$ & $36.11$ & $61.39$ & $48.75$ \\
    SyncMask~\cite{song2024syncmask} \footnotesize{\textcolor{gray}{(CVPR'24)}} & $33.76$ & $61.23$ & $35.82$ & $62.12$ & $44.82$ & $72.06$ & $38.13$ & $65.14$ & $51.64$ \\
    PL4CIR-base~\cite{zhao2022PL4CIR} \footnotesize{\textcolor{gray}{(SIGIR'22)}} & $33.22$ & $59.99$ & $46.17$ & $68.79$ & $46.46$ & $73.84$ & $41.95$ & $67.54$ & $54.76$ \\
    AlRet-big~\cite{xu2024alret} \footnotesize{\textcolor{gray}{(TMM'24)}} & $40.23$ & $65.89$ & $47.15$ & $70.88$ & $51.05$ & $75.78$ & $46.14$ & $70.85$ & $58.50$ \\
    PL4CIR-large~\cite{zhao2022PL4CIR} \footnotesize{\textcolor{gray}{(SIGIR'22)}} & $38.18$ & $64.50$ & $48.63$ & $71.54$ & $52.32$ & $76.90$ & $46.38$ & $70.98$ & $58.68$ \\
    TG-CIR~\cite{wen2023tgcir} \footnotesize{\textcolor{gray}{(MM'23)}} & $45.22$ & $69.66$ & $52.60$ & $72.52$ & $56.14$ & $77.10$ & $51.32$ & $73.09$ & $62.21$ \\
    FashionERN-small~\cite{chen2024fashionern} \footnotesize{\textcolor{gray}{(AAAI'24)}} & $43.93$ & $68.77$ & $52.70$ & $75.07$ & $56.09$ & $78.38$ & $50.91$ & $74.07$ & $62.49$ \\
    SPIRIT~\cite{chen2024spirit} \footnotesize{\textcolor{gray}{(TOMM'24)}} & $43.83$ & $68.86$ & $52.50$ & $74.19$ & $56.60$ & $79.25$ & $50.98$ & $74.10$ & $62.54$ \\
    LIMN~\cite{wen2023limn} \footnotesize{\textcolor{gray}{(TPAMI'24)}} & $50.72$ & $74.52$ & $56.08$ & $77.09$ & $60.94$ & $81.85$ & $55.91$ & $77.82$ & $66.87$ \\
    LIMN+~\cite{wen2023limn} \footnotesize{\textcolor{gray}{(TPAMI'24)}} & $52.11$ & $75.21$ & $57.51$ & $77.92$ & $62.67$ & $82.66$ & $57.43$ & $78.60$ & $68.01$ \\
    DQU-CIR~\cite{wen2024dqu} \footnotesize{\textcolor{gray}{(SIGIR'24)}} & $57.63$ & $78.56$ & $62.14$ & $80.38$ & $66.15$ & $85.73$ & $61.97$ & $81.56$ & $71.77$ \\
    
    \hline
    \end{tabular}
    \label{tab:supervised_CIR_exp_fashioniq_val}
    % }
\end{table*}

\begin{table*}
    \scriptsize
    \centering
    \caption{
   Performance comparison among supervised composed image retrieval models on FashionIQ (original split).}
   % \resizebox{14.5cm}{!}{
    \begin{tabular}{l|cc|cc|cc|cc|c}
    \hline 
    \multirow{2}{*}{Model} & \multicolumn{2}{c|}{Dresses} & \multicolumn{2}{c|}{Shirts} & \multicolumn{2}{c|} {Tops\&Tees} & \multicolumn{2}{c|}{Average} & \multirow{2}{*}{Avg.} \\ \cline{2-9}
    &  R@$10$ & R@$50$ & R@$10$ & R@$50$ & R@$10$ & R@$50$ & R@$10$ & R@$50$ & \\
    \hline \hline
    
    % \rowcolor{gray!5} 
    \multicolumn{10}{c}{\textit{\textcolor{gray}{Traditional Encoder-based Methods}}} \\
    
    JVSM~\cite{chen2020jvsm} \footnotesize{\textcolor{gray}{(ECCV'20)}} & $10.70 $ & $25.90$ & $12.00$ & $27.10$ & $13.00$ & $26.90$ & $11.90$ & $26.63$ &$19.27$ \\
    TIRG~\cite{vo2019tirg} \footnotesize{\textcolor{gray}{(CVPR'19)}} & $14.13$ & $34.61$ & $13.10$ & $30.91$ & $14.79$ & $34.37$ & $14.01$ & $33.30$ &$23.66$ \\
    % CosMo~\cite{lee2021cosmo} \small{\textcolor{gray}{(CVPR'21)}}  & $21.39$ & $44.45$ & $16.90$ & $37.49$ & $21.32$ & $46.02$ & $19.87$ & $42.62$ & $31.25 $  \\
    % ARTEMIS~\cite{delmas2022artemis} \footnotesize{\textcolor{gray}{(ICLR'22)}}  & $25.68$ & $51.05$ & $21.57$ & $44.13$ &$ 28.59 $& $ 55.06 $& $ 25.28 $& $50.08 $ & $ 37.68$ \\
    AlRet-small~\cite{xu2024alret} \footnotesize{\textcolor{gray}{(TMM'24)}} & $27.34$ & $53.42$ & $21.30$ & $43.08$ & $29.07$ & $54.21$ & $25.90$ & $50.24$ &$38.07$ \\
    ARTEMIS~\cite{delmas2022artemis} \footnotesize{\textcolor{gray}{(ICLR'22)}} & $27.16$ & $52.40$ & $21.78$ & $43.64$ & $29.20$ & $54.83$ & $26.05$ & $50.29$ & $38.17$ \\
    FashionVLP~\cite{goenka2022fashionvlp} \small{\textcolor{gray}{(CVPR'22)}} & $26.77$ & $53.20$ & $22.67$ & $46.22$ & $28.51$ & $57.47$ & $25.98$ & $52.30$ & $39.14 $ \\
    NEUCORE~\cite{zhao2024neucore} \footnotesize{\textcolor{gray}{(NIPSW'23)}}& $27.00$ & $53.79$ & $22.84$ & $45.00$ & $29.63$ & $56.65$ & $26.45$ & $51.75$ & $39.15 $ \\
    PCaSM~\cite{zhang2023pcasm} \footnotesize{\textcolor{gray}{(ICME'23)}} & $25.12$ & $51.17$ & $24.09$ & $50.66$ & $28.00$ & $57.45$ & $25.74$ & $53.09$ &$39.42$ \\
    DCNet~\cite{kim2021dcnet} \footnotesize{\textcolor{gray}{(AAAI'21)}} & $28.95$ & $56.07$ & $23.95$ & $47.30$ & $30.44$ & $58.29$ & $27.78$ & $53.89$ &$40.83$ \\
    AACL~\cite{tian2023aacl} \footnotesize{\textcolor{gray}{(WACV'23)}} & $29.89$ & $55.85$ & $24.82$ & $48.85$ & $30.88$ & $56.85$ & $28.53$ & $53.85$ & $41.19$ \\
    DSCN~\cite{li2023dscn} \footnotesize{\textcolor{gray}{(ICMR'23)}} & $30.61$ & $56.67$ & $25.74$ & $48.48$ & $32.38$ & $59.13$ & $29.58$ & $54.76$ & $42.17$ \\
    ComqueryFormer~\cite{xu2023ComqueryFormer} \small{\textcolor{gray}{(TMM'23)}} & $28.85$ & $55.38$ & $25.64$ & $50.22$ & $33.61$ & $60.48$ & $29.37$ & $55.36$ & $42.36 $ \\
    
    \cdashline{1-10}

    % \rowcolor{gray!5} % 设置该行的背景色为浅灰色
    \multicolumn{10}{c}{\textit{\textcolor{gray}{VLP Encoder-based Methods}}} \\
    
    FaD-VLP~\cite{mirchandani2022fad} \footnotesize{\textcolor{gray}{(EMNLP'22)}} & $32.08$ & $57.96$ & $25.22$ & $49.71$ & $33.20$ & $60.84$ & $30.17$ & $56.17$ & {$43.17$} \\
    PL4CIR-base~\cite{zhao2022PL4CIR} \footnotesize{\textcolor{gray}{(SIGIR'22)}} & $29.00$ & $53.94$ & $35.43$ & $58.88$ & $39.16$ & $64.56$ & $34.53$ & $59.13$ & {$46.83$} \\
    Combiner~\cite{baldrati2022Combiner} \footnotesize{\textcolor{gray}{(CVPRW'22)}} & $31.63$ & $56.67$ & $36.36$ & $58.00$ & $38.19$ & $62.42$ & $35.39$ & $59.03$ & $47.21$ \\
    CaLa_CLIP4Cir~\cite{jiang2024cala} \footnotesize{\textcolor{gray}{(SIGIR'24)}} & $32.96$ & $56.82$ & $39.20$ & $60.13$ & $39.16$ & $63.83$ & $37.11$ & $60.26$ & {$ 48.68 $} \\
    CAFF~\cite{wan2024caff} \footnotesize{\textcolor{gray}{(CVPR'24)}} & $35.74$ & $59.85$ & $35.80$ & $61.94$ & $38.51$ & $68.34$ & $36.68$ & $63.38$ & $50.03$ \\
    CLIP4CIR~\cite{baldrati2022CLIP4CIR} \footnotesize{\textcolor{gray}{(CVPR'22)}} & $33.81$ & $59.40$ & $39.99$ & $60.45$ & $41.41$ & $65.37$ & $38.40$ & $61.74$ & {$50.07$} \\
    Combiner+MU~\cite{chen2022mu} \footnotesize{\textcolor{gray}{(ICLR'24)}} & $32.61$ & $61.34$ & $33.23$ & $62.55$ & $41.40$ & $72.51$ & $35.75$ & $65.47$ & {$50.61$} \\
    AlRet-big~\cite{xu2024alret} \footnotesize{\textcolor{gray}{(TMM'24)}} & $35.75$ & $60.56$ & $37.02$ & $60.55$ & $42.25$ & $67.52$ & $38.34$ & $62.88$ & $50.61$ \\
    PL4CIR-large~\cite{zhao2022PL4CIR} \footnotesize{\textcolor{gray}{(SIGIR'22)}} & $33.60$ & $58.90$ & $39.45$ & $61.78$ & $43.96$ & $68.33$ & $39.00$ & $63.00$ & $51.01$ \\
    SSN~\cite{yang2024ssn} \footnotesize{\textcolor{gray}{(AAAI'24)}} & $34.36$ & $60.78$ & $38.13$ & $61.83$ & $44.26$ & $69.05$ & $38.92$ & $63.89$ & $51.40$ \\
    IUDC~\cite{ge2024iudc} \footnotesize{\textcolor{gray}{(TOIS'24)}} & $35.22$ & $61.90$ & $41.86$ & $63.52$ & $42.19$ & $69.23$ & $39.76$ & $64.88$ & $52.32$ \\
    CLIP4CIR2~\cite{baldrati2023CLIP4CIR2} \footnotesize{\textcolor{gray}{(TOMM'23)}} & $37.67$ & $63.16$ & $39.87$ & $60.84$ & $44.88$ & $68.59$ & $40.80$ & $64.20$ & $52.50$ \\
    CLIP-CD~\cite{lin2023clip_cd} \footnotesize{\textcolor{gray}{(AJCAI'23)}} & $37.68$ & $62.62$ & $42.44$ & $63.74$ & $45.33$ & $67.72$ & $41.82$ & $64.69$ & $53.26$ \\
    IUDC+AUG~\cite{ge2024iudc} \footnotesize{\textcolor{gray}{(TOIS'24)}}& $36.60$ & $63.45$ & $42.61$ & $64.86$ & $43.52$ & $71.15$ & $40.91$ & $66.49$ & $53.70$ \\
    BLIP4CIR~\cite{liu2024blip4cir} \footnotesize{\textcolor{gray}{(WACV'24)}} & $40.65$ & $66.34$ & $40.38$ & $64.13$ & $46.86$ & $69.91$ & $42.63$ & $66.79$ & $54.71$ \\
    SHAF~\cite{yan2024shaf} \footnotesize{\textcolor{gray}{(ICIC'24)}} & $41.15$ & $62.46$ & $43.03$ & $64.00$ & $49.52$ & $71.09$ & $44.57$ & $65.85$ & $55.21$ \\
    Ranking-aware~\cite{chen2023ranking} \footnotesize{\textcolor{gray}{(Arxiv'23)}} & $34.80$ & $60.22$ & $45.01$ & $69.06$ & $47.68$ & $74.85$ & $42.50$ & $68.04$ & $55.27$\\
    FAME-VIL(ST)~\cite{han2023fame} \footnotesize{\textcolor{gray}{(CVPR'23)}} & $37.78$ & $63.86$ & $45.63$ & $66.78$ & $47.22$ & $70.88$ & $43.54$ & $67.17$ & $55.36$ \\
    BLIP4CIR+Bi~\cite{liu2024blip4cir} \footnotesize{\textcolor{gray}{(WACV'24)}} & $42.09$ & $67.33$ & $41.76$ & $64.28$ & $46.61$ & $70.32$ & $43.49$ & $67.31$ & $55.40$ \\
    SPIRIT~\cite{chen2024spirit} \footnotesize{\textcolor{gray}{(TOMM'24)}} & $39.86$ & $64.30$ & $44.11$ & $65.60$ & $47.68$ & $71.70$ & $43.88$ & $67.20$ & $55.54$ \\
    FashionERN-small~\cite{chen2024fashionern} \footnotesize{\textcolor{gray}{(AAAI'24)}} & $38.52$ & $64.30$ & $45.00$ & $66.05$ & $48.80$ & $71.09$ & $44.11$ & $67.15$ & $55.63$ \\
    SADN~\cite{wang2024sadn} \footnotesize{\textcolor{gray}{(MM'24)}} & $40.01$ & $65.10$ & $43.67$ & $66.05$ & $48.04$ & $70.93$ & $43.91$ & $67.36$ & $55.63$ \\
    CaLa_BLIP2Cir~\cite{jiang2024cala} \footnotesize{\textcolor{gray}{(SIGIR'24)}} & $42.38$ & $66.08$ & $46.76$ & $68.16$ & $50.93$ & $73.42$ & $46.69$ & $69.22$ & $57.96$ \\
    FAME-VIL~\cite{han2023fame} \footnotesize{\textcolor{gray}{(CVPR'23)}} & $42.19$ & $67.38$ & $47.64$ & $68.79$ & $50.69$ & $73.07$ & $46.84$ & $69.75$ & $58.29$ \\
    CASE~\cite{levy2024case} \footnotesize{\textcolor{gray}{(AAAI'24)}} & $47.44$ & $69.36$ & $48.48$ & $70.23$ & $50.18$ & $72.24$ & $48.70$ & $70.61$ & $59.66$ \\
    Re-ranking_R100~\cite{liu2023rerank} \footnotesize{\textcolor{gray}{(TMLR'24)}} & $48.14$ & $71.34$ & $50.15$ & $71.25$ & $55.23$ & $76.80$ & $51.17$ & $73.13$ & $62.15$ \\
    FashionERN-big~\cite{chen2024fashionern} \footnotesize{\textcolor{gray}{(AAAI'24)}} & $50.32$ & $71.29$ & $50.15$ & $70.36$ & $56.40$ & $77.21$ & $52.26$ & $72.95$ & $62.62$ \\
    SPRC~\cite{xusentence2024sprc} \footnotesize{\textcolor{gray}{(ICLR'24)}} & $49.18$ & $72.43$ & $55.64$ & $73.89$ & $59.35$ & $78.58$ & $54.72$ & $74.97$ & $64.85$ \\
    DQU-CIR~\cite{wen2024dqu} \footnotesize{\textcolor{gray}{(SIGIR'24)}} & $51.90$ & $74.37$ & $53.57$ & $73.21$ & $58.48$ & $79.23$ & $54.65$ & $75.60$ & $65.13$ \\
    SPRC+VQA~\cite{feng2023vqa4cir} \footnotesize{\textcolor{gray}{(Arxiv'23)}}& $49.18$ & $73.06$ & $56.79$ & $74.52$ & $59.67$ & $79.30$ & $55.21$ & $75.62$ & $65.41$ \\
    SDQUR~\cite{xu2024SDQUR} \footnotesize{\textcolor{gray}{(TCSVT'24)}} & $49.93$ & $73.33$ & $56.87$ & $76.50$ & $59.66$ & $79.25$ & $55.49$ & $76.36$ & $65.93$ \\
    SPRC+SPN~\cite{feng2024spn} \footnotesize{\textcolor{gray}{(MM'24)}} & $50.57$ & $74.12$ & $57.70$ & $75.27$ & $60.84$ & $79.96$ & $56.37$ & $76.45$ & $66.41$ \\
    \hline
    \end{tabular}
    \label{tab:supervised_CIR_exp_fashioniq_ori}
    % }
\end{table*}


\begin{table*}
    \scriptsize
    \centering
    \caption{Performance comparison among supervised composed image retrieval models on Fashion200K and Shoes.}
    % \resizebox{14cm}{!}{
    \begin{tabular}{l|ccc|c|ccc|c}
    \hline 
    \multirow{2}{*}{Model} & \multicolumn{4}{c|}{Fashion200K} & \multicolumn{4}{c}{Shoes}  \\
    \cline{2-9}
    & R@$1$ & R@$10$ & R@$50$ & Avg. & R@$1$ & R@$10$ & R@$50$ & Avg.\\
    \hline \hline 
    
    \multicolumn{9}{c}{\textit{\textcolor{gray}{Traditional Encoder-based Methods}}} \\
    TIRG~\cite{vo2019tirg} \footnotesize{\textcolor{gray}{(CVPR'19)}} & $14.10$ & $42.50$ & $63.80$ &$40.13$ &$12.60$ &$45.45$ & $69.39$&$42.28$\\
    MGF~\cite{liu2021mgf} \footnotesize{\textcolor{gray}{(MMM'21)}} & $16.00$ & $44.60$ & $-$ &$-$ &$-$ &$-$ & $-$&$-$ \\
    TIRG+JPM(Tri)~\cite{yang2021jpm} \footnotesize{\textcolor{gray}{(MM'21)}} & $17.70$ & $44.70$ & $64.50$ &$42.30$ &$-$ &$-$ & $-$&$-$\\
    JAMMA~\cite{zhang2020jamma} \footnotesize{\textcolor{gray}{(MM'20)}} & $17.34$ & $45.28$ & $65.65$ &$42.76$ &$-$ &$-$ & $-$&$-$\\
    DCNet~\cite{kim2021dcnet} \footnotesize{\textcolor{gray}{(AAAI'21)}} & $-$ & $46.89$ & $67.56$ &$-$ & $-$ & $53.82$ & $79.33$ & $-$ \\
    TIS~\cite{zhang2022tis} \footnotesize{\textcolor{gray}{(TOMM'22)}} & $16.25$ & $44.14$ & $65.02$ &$41.80$ &$-$ &$-$ & $-$&$-$\\
    TIRG+JPM(MSE)~\cite{yang2021jpm} \footnotesize{\textcolor{gray}{(MM'21)}} & $19.80$ & $46.50$ & $66.60$ &$44.30$ &$-$ &$-$ & $-$&$-$\\
    LBF-small~\cite{hosseinzadeh2020lbf} \footnotesize{\textcolor{gray}{(CVPR'20)}} & $16.26$ & $46.90$ & $71.73$ &$44.96$ &$-$ &$-$ & $-$&$-$\\
    LBF-big~\cite{hosseinzadeh2020lbf} \footnotesize{\textcolor{gray}{(CVPR'20)}} & $17.78$ & $48.35$ & $68.50$ &$44.88$ &$-$ &$-$ & $-$&$-$\\
    MCR~\cite{zhang2021mcr} \footnotesize{\textcolor{gray}{(MM'21)}} & $18.24$ & $49.41$ & $69.37$ &$45.67$ & $17.85$ & $50.95$ & $77.24$ & $48.68$ \\
    SAC~\cite{jandial2022sac} \footnotesize{\textcolor{gray}{(WACV'22)}} &$-$ &$-$ & $-$&$-$& $18.50$ & $51.73$ & $77.28$ & $49.17$ \\
    VAL~\cite{chen2020val} \footnotesize{\textcolor{gray}{(CVPR'20)}} & $21.20$ & $49.00$ & $68.80$ &$46.33$ & $16.49$ & $49.12$ & $73.53$ &$46.38$\\
    JVSM~\cite{chen2020jvsm} \footnotesize{\textcolor{gray}{(ECCV'20)}} & $19.00$ & $52.10$ & $70.00$ &$47.03$ &$-$ &$-$ & $-$& \\
    DATIR~\cite{gu2021datir} \footnotesize{\textcolor{gray}{(MM'21)}} & $21.50$ & $48.80$ & $71.60$ &$47.30$ & $17.20$ & $51.10$ & $75.60$ & $47.97$ \\
    % ARTEMIS~\cite{delmas2022artemis} \footnotesize{\textcolor{gray}{(ICLR'22)}} &$-$ &$-$ & $-$&$-$& $17.60$ & $51.05$ & $76.85$ & $48.50$ \\
    FashionVLP~\cite{goenka2022fashionvlp} \footnotesize{\textcolor{gray}{(CVPR'22)}} & $-$ & $49.90$ & $70.50$ &$-$ &$-$ &$-$ & $-$&$-$\\
    Css-Net~\cite{zhang2024cssnet} \footnotesize{\textcolor{gray}{(KBS'24)}} & $22.20$ & $50.50$ & $69.70$ &$47.47$ & $20.13$ & $56.81$ & $81.32$ & $52.75$ \\
    CoSMo~\cite{lee2021cosmo} \footnotesize{\textcolor{gray}{(CVPR'21)}} & $23.30$ & $50.40$ & $69.30$ &$47.67$ & $16.72$ & $48.36$ & $75.64$ & $46.91$ \\
    ACNet~\cite{li2023acnet} \footnotesize{\textcolor{gray}{(ICME'23)}} & $-$ & $50.58$ & $70.12$ &$-$ & $-$ & $53.39$ & $79.53$ & $-$ \\
    GSCMR~\cite{zhang2021GSCMR} \footnotesize{\textcolor{gray}{(TIP'22)}} & $21.57$ & $52.84$ & $70.12$ &$48.18$ &$-$ &$-$ & $-$&$-$\\
    % VAL (GloVe)~\cite{} \footnotesize{\textcolor{gray}{(CVPR'20)}} & $22.90$ & $50.80$ & $72.70$ &$48.80$\\
    EER~\cite{zhang2022eer} \footnotesize{\textcolor{gray}{(TIP'22)}} & $-$ & $50.88$ & $70.60$ &$-$ & $19.87$ & $55.96$ & $79.58$ & $51.80$ \\
    NEUCORE~\cite{zhao2024neucore} \footnotesize{\textcolor{gray}{(NIPSW'23)}} &$-$ &$-$ & $-$&$-$& $19.76$ & $55.48$ & $80.75$ & $52.00$ \\
    AMC~\cite{zhu2023amc} \footnotesize{\textcolor{gray}{(TOMM'23)}}  &$-$ &$-$ & $-$&$-$& $19.99$ & $56.89$ & $79.27$ & $52.05$ \\
    MLCLSAP~\cite{zhang2023MLCLSAP} \footnotesize{\textcolor{gray}{(TMM'23)}} & $-$ & $51.06$ & $70.13$ &$-$ & $19.54$ & $55.15$ & $77.22$ & $50.64$ \\
    % EER w/random emb~\cite{} \footnotesize{\textcolor{gray}{(TIP'22)}} & $-$ & $51.09$ & $70.33$ &$-$\\
    Css-Net†~\cite{zhang2024cssnet} \footnotesize{\textcolor{gray}{(KBS'24)}} & $23.40$ & $52.00$ & $72.00$ &$49.13$ &$-$ &$-$ & $-$&$-$\\
    ComqueryFormer~\cite{xu2023ComqueryFormer} \footnotesize{\textcolor{gray}{(TMM'23)}} & $-$ & $52.20$ & $72.20$ &$-$ &$-$ &$-$ & $-$&$-$\\
    CRN-base~\cite{yang2023crn} \footnotesize{\textcolor{gray}{(TIP'23)}} & $-$ & $53.30$ & $73.30$ &$-$ & $17.32$ & $54.15$ & $79.34$ & $50.27$ \\
    CLVC-Net~\cite{wen2021clvcnet} \footnotesize{\textcolor{gray}{(SIGIR'21)}} & $22.60$ & $53.00$ & $72.20$ &$49.27$ & $17.64$ & $54.39$ & $79.47$ & $50.50$ \\
    CRN-large~\cite{yang2023crn} \footnotesize{\textcolor{gray}{(TIP'23)}} & $-$ & $53.50$ & $74.50$ &$-$ & $18.92$ & $54.55$ & $80.04$ & $51.17$ \\
    ComposeAE+GA~\cite{huang2022ga} \footnotesize{\textcolor{gray}{(TIP'22)}} & $25.20$ & $52.80$ & $71.20$ &$49.70$ &$-$ &$-$ & $-$&$-$\\
    % VAL (Lvv + Lvs)~\cite{} \footnotesize{\textcolor{gray}{(CVPR'20)}} & $21.50$ & $53.80$ & $73.30$ &$49.53$\\
    % TIRG+GA~\cite{} \footnotesize{\textcolor{gray}{(TIP'22)}} & $25.20$ & $52.80$ & $71.20$ &$49.73$\\
    ProVLA~\cite{hu2023provla} \footnotesize{\textcolor{gray}{(ICCVW'23)}} & $21.70$ & $53.70$ & $74.60$ &$50.00$ & $19.20$ & $56.20$ & $73.30$ & $49.57$ \\
    SDFN~\cite{wu2024sdfn} \footnotesize{\textcolor{gray}{(ICASSP'24)}} & $23.30$ & $54.40$ & $73.60$ &$50.40$ & $23.09$ & $58.51$ & $81.08$ & $54.22$ \\
    ComposeAE~\cite{anwaar2021ComposeAE} \footnotesize{\textcolor{gray}{(WACV'21)}} & $22.80$ & $55.30$ & $73.40$ &$50.50$ &$-$ &$-$ & $-$&$-$\\
    AlRet-small~\cite{xu2024alret} \footnotesize{\textcolor{gray}{(TMM'24)}} & $24.42$ & $53.93$ & $73.25$ &$50.53$ & $18.13$ & $53.98$ & $78.81$ & $50.31$ \\
    ARTEMIS~\cite{delmas2022artemis} \footnotesize{\textcolor{gray}{(ICLR'22)}} &$-$ &$-$ & $-$&$-$& $18.72$ & $53.11$ & $79.31$ & $50.38$ \\
    NSFSE~\cite{wang2024NSFSE} \footnotesize{\textcolor{gray}{(TMM'24)}} & $24.90$ & $54.30$ & $73.40$ &$50.90$ &$-$ &$-$ & $-$&$-$\\
    MANME~\cite{li2023manme} \footnotesize{\textcolor{gray}{(TCSVT'24)}} & $23.00$ & $57.90$ & $75.30$ &$52.00$ & $20.73$ & $55.96$ & $80.98$ & $52.56$ \\
    CMAP~\cite{li2024cmap} \footnotesize{\textcolor{gray}{(TOMM'24)}} & $24.20$ & $56.90$ & $75.30$ &$52.10$ & $21.48$ & $56.18$ & $81.14$ & $52.93$ \\
    DSCN~\cite{li2023dscn} \footnotesize{\textcolor{gray}{(ICMR'23)}} & $25.60$ & $56.20$ & $74.90$ &$52.23$ & $20.33$ & $55.84$ & $80.55$ & $52.24$ \\
    % TIRG-BERT+GA~\cite{} \footnotesize{\textcolor{gray}{(TIP'22)}} & $24.00$ & $57.20$ & $75.70$ &$52.30$\\
    AACL~\cite{tian2023aacl} \footnotesize{\textcolor{gray}{(WACV'23)}} & $19.64$ & $58.85$ & $78.86$ &$52.45$& $-$ & $-$ & $-$ & $-$ \\
    MCEM~\cite{zhang2024mcem} \footnotesize{\textcolor{gray}{(TIP'24)}} & $24.86$ & $56.76$ & $76.91$ &$52.84$& $19.10$ & $55.37$ & $79.57$ & $51.35$ \\
    LGLI~\cite{huang2023lgli} \footnotesize{\textcolor{gray}{(CVPRW'23)}} & $26.50$ & $58.60$ & $75.60$ &$53.60$ &$-$ &$-$ & $-$&$-$\\


    \cdashline{1-9}

    % \rowcolor{gray!5} % 设置该行的背景色为浅灰色
    \multicolumn{9}{c}{\textit{\textcolor{gray}{VLP Encoder-based Methods}}} \\
    PL4CIR-base~\cite{zhao2022PL4CIR} \footnotesize{\textcolor{gray}{(SIGIR'22)}} &$-$ &$-$ & $-$&$-$& $19.53$ & {$55.65$} & {$80.58$} & $51.92$ \\
    AlRet-big~\cite{xu2024alret} \footnotesize{\textcolor{gray}{(TMM'24)}} &$-$ &$-$ & $-$&$-$& $21.02$ & $55.72$ & $80.77$ & $52.50$ \\
    % Combiner+MU~\cite{chen2022mu} \footnotesize{\textcolor{gray}{(ICLR'24)}} & $21.80$ & $52.10$ & $70.20$ &$48.03$ & $18.41$ & $53.63$ & $79.84$ & $50.63$ \\
    IUDC~\cite{ge2024iudc} \footnotesize{\textcolor{gray}{(TOIS'24)}} &$-$ &$-$ & $-$&$-$& $21.17$ & $56.82$ & $82.25$ & $53.41$ \\
    IUDC+AUG~\cite{ge2024iudc} \footnotesize{\textcolor{gray}{(TOIS'24)}} &$-$ &$-$ & $-$&$-$& $21.83$ & $57.76$ & $82.90$ & $54.16$ \\
    PL4CIR-large~\cite{zhao2022PL4CIR} \footnotesize{\textcolor{gray}{(SIGIR'22)}} &$-$ &$-$ & $-$&$-$& $22.88$ & $58.83$ & $84.16$ & $55.29$ \\
    FashionERN~\cite{chen2024fashionern} \footnotesize{\textcolor{gray}{(AAAI'24)}} & $-$ & {$54.1 $} & {$72.5 $} & $-$ & $-$ & $55.59$ & $81.71$ & $-$ \\
    SPIRIT~\cite{chen2024spirit} \footnotesize{\textcolor{gray}{(TOMM'24)}} & $-$ & {$55.20$} & {$73.60$} & $-$ & $-$ & $56.90$ & $81.49$ & $-$ \\
    LIMN~\cite{wen2023limn} \footnotesize{\textcolor{gray}{(TPAMI'24)}} & $-$ & {$57.20$} & {$76.60$} & $-$ & $-$ & $68.20$ & $87.45$ & $-$ \\
    LIMN+~\cite{wen2023limn} \footnotesize{\textcolor{gray}{(TPAMI'24)}} &$-$ &$-$ & $-$&$-$& $-$ & $68.37$ & $88.07$ & $-$ \\
    CAFF~\cite{wan2024caff} \footnotesize{\textcolor{gray}{(CVPR'24)}} & {$25.21$} & {$60.17$} & {$80.79$} & {$55.39$} & $20.87$ & $56.82$ & $81.99$ & $53.23$ \\
    SHAF~\cite{yan2024shaf} \footnotesize{\textcolor{gray}{(ICIC'24)}} &$-$ &$-$ & $-$&$-$& $23.01$ & $64.39$ & $85.49$ & $57.63$ \\
    TG-CIR~\cite{wen2023tgcir} \footnotesize{\textcolor{gray}{(MM'23)}} &$-$ &$-$ & $-$&$-$& $25.89$ & $63.20$ & $85.07$ & $58.05$ \\
    DWC~\cite{huang2024dwc} \footnotesize{\textcolor{gray}{(AAAI'24)}} & {$36.49$} & {$63.58$} & {$79.02$} & {$59.70$} &$18.94$ &$55.55$ & $80.19$ & $51.56$\\
    DQU-CIR~\cite{wen2024dqu} \footnotesize{\textcolor{gray}{(SIGIR'24)}} & {$36.80$} & {$67.90$} & {$87.80$} & {$64.10$} &$31.47$ &$69.19$ & $88.52$&$63.06$\\
    
    \hline
    \end{tabular}
    \label{tab:supervised_CIR_exp_fashion200k_shoes_css}
    % }
\end{table*}


\begin{table*}
    \scriptsize
    \centering
    \caption{Performance comparison among supervised composed image retrieval models on CIRR. Avg means the average of R@$5$ and R$_{subset}$@$1$.}
    % \resizebox{14.5cm}{!}{
    \begin{tabular}{l|cccc|ccc|c}
    \hline 
    \multirow{2}{*}{Method} &\multicolumn{4}{c|}{\textbf{R@$k$}} &\multicolumn{3}{c|}{\textbf{R$_{subset}$@$k$}} & \multirow{2}{*}{Avg} \\ \cline{2-8}
    & $k=1$ & $k=5$ & $k=10$ & $k=50$ & $k=1$ & $k=2$ & $k=3$ \\
    \hline \hline 

    % \rowcolor{gray!5} 
    \multicolumn{9}{c}{\textit{\textcolor{gray}{Traditional Encoder-based Methods}}} \\
    
    TIRG~\cite{vo2019tirg} \footnotesize{\textcolor{gray}{(CVPR'19)}} & $14.61$ & $48.37$ & $64.08$ & $90.03$ & $22.67$ & $44.97$ & $65.14$ & $35.52$\\
    ARTEMIS~\cite{delmas2022artemis} \footnotesize{\textcolor{gray}{(ICLR'22)}} & $16.96$ & $46.10$ & $61.31$ & $87.73$ & $39.99$ & $62.20$ & $75.67$ & $43.05$ \\
    CIRPLANT~\cite{liu2021CIRPLANT} \footnotesize{\textcolor{gray}{(ICCV'21)}} & $19.55$ & $52.55$ & $68.39$ & $92.38$ & $39.20$ & $63.03$ & $79.49$ & $45.88$ \\
    MCEM~\cite{zhang2024mcem} \footnotesize{\textcolor{gray}{(TIP'24)}} & $17.48$ & $46.13$ & $62.17$ & $88.91$ & $-$ & $-$ & $-$ & $-$ \\
    NEUCORE~\cite{zhao2024neucore} \footnotesize{\textcolor{gray}{(NIPSW'23)}} & $18.46$ & $49.40$ & $63.57$ & $89.35$ & $44.27$ & $67.06$ & $78.92$ & $46.84$ \\
    NSFSE~\cite{wang2024NSFSE} \footnotesize{\textcolor{gray}{(TMM'24)}} & $20.70$ & $52.50$ & $67.96$ & $90.74$ & $44.20$ & $65.53$ & $78.50$ & $48.35$ \\
    % LMGA~\cite{} \footnotesize{\textcolor{gray}{(Arxiv'23)}} & $19.11$ & $45.64$ & $59.95$ & $86.89$ & $58.68$ & $79.90$ & $90.87$ & $52.16$ \\
    ComqueryFormer~\cite{xu2023ComqueryFormer} \footnotesize{\textcolor{gray}{(TMM'23)}} & $25.76$ & $61.76$ & $75.90$ & $95.13$ & $51.86$ & $76.26$ & $89.25$ & $56.81$ \\

    \cdashline{1-9}

    % \rowcolor{gray!5} % 设置该行的背景色为浅灰色
    \multicolumn{9}{c}{\textit{\textcolor{gray}{VLP Encoder-based Methods}}} \\
    
    CLIP-ProbCR~\cite{li2024clip} \footnotesize{\textcolor{gray}{(ICMR'24)}} & $23.32$ & $54.36$ & $68.64$ & $93.05$ & $54.32$ & $76.30$ & $88.88$ & $54.34$\\
    Combiner~\cite{baldrati2022Combiner} \footnotesize{\textcolor{gray}{(CVPRW'22)}} & $33.59$ & $65.35$ & $77.35$ & $95.21$ & $62.39$ & $81.81$ & $92.02$ & $63.87$ \\
    Ranking-aware~\cite{chen2023ranking} \footnotesize{\textcolor{gray}{(Arxiv'23)}} & $32.24$ & $66.63$ & $79.23$ & $96.43$ & $61.25$ & $81.33$ & $92.02$ & $63.94$ \\
    % CaLa_CLIP4Cir(RN50x4)~\cite{} \footnotesize{\textcolor{gray}{(SIGIR'24)}} & $35.37$ & $68.89$ & $80.07$ & $95.86$ & $66.68$ & $84.65$ & $93.42$ & $67.79$ \\
    % CLIP4CIR~\cite{} \footnotesize{\textcolor{gray}{(CVPR'22)}} & $35.81$ & $68.80$ & $80.17$ & $95.25$ & $66.96$ & $85.25$ & $93.13$ & $67.88$ \\
    CLIP4CIR~\cite{baldrati2022CLIP4CIR} \footnotesize{\textcolor{gray}{(CVPR'22)}} & $38.53$ & $69.98$ & $81.86$ & $95.93$ & $68.19$ & $85.64$ & $94.17$ & $69.09$ \\
    BLIP4CIR~\cite{liu2024blip4cir} \footnotesize{\textcolor{gray}{(WACV'24)}} & $40.17$ & $71.81$ & $83.18$ & $95.69$ & $72.34$ & $88.70$ & $95.23$ & $72.07$ \\
    BLIP4CIR+Bi~\cite{liu2024blip4cir} \footnotesize{\textcolor{gray}{(WACV'24)}} & $40.15$ & $73.08$ & $83.88$ & $96.27$ & $72.10$ & $88.27$ & $95.93$ & $72.59$ \\
    FashionERN~\cite{chen2024fashionern} \footnotesize{\textcolor{gray}{(AAAI'24)}} & $-$ & $74.77$ & $-$ & $-$ & $74.93$ & $-$ & $-$ & $74.85$ \\
    CLIP4CIR2~\cite{baldrati2023CLIP4CIR2} \footnotesize{\textcolor{gray}{(TOMM'23)}} & $42.05$ & $76.13$ & $86.51$ & $97.49$ & $70.15$ & $87.18$ & $94.40$ & $73.14$ \\
    DMOT~\cite{dmot} \footnotesize{\textcolor{gray}{(ACCV'24)}} & $41.55$ & $74.25$ & $84.67$ & $96.49$ & $74.20$ & $89.55$ & $95.58$ & $73.70$ \\
    SPIRIT~\cite{chen2024spirit} \footnotesize{\textcolor{gray}{(TOMM'24)}} & $40.23$ & $75.10$ & $84.16$ & $96.88$ & $73.74$ & $89.60$ & $95.93$ & $74.42$ \\
    VISTA~\cite{zhou2024vista} \footnotesize{\textcolor{gray}{(ACL'24)}} & $-$ & $76.10$ & $-$ & $-$ & $75.70$ & $-$ & $-$ & $75.90$ \\
    SSN~\cite{yang2024ssn} \footnotesize{\textcolor{gray}{(AAAI'24)}} & $43.91$ & $77.25$ & $86.48$ & $97.45$ & $71.76$ & $88.63$ & $95.54$ & $74.51$ \\
    DQU-CIR~\cite{wen2024dqu} \footnotesize{\textcolor{gray}{(SIGIR'24)}} & $46.22$ & $78.17$ & $87.64$ & $97.81$ & $70.92$ & $87.69$ & $94.68$ & $74.55$ \\
    SADN~\cite{wang2024sadn} \footnotesize{\textcolor{gray}{(MM'24)}} & $44.27$ & $78.10$ & $87.71$ & $97.89$ & $72.71$ & $89.33$ & $95.38$ & $75.41$ \\
    TG-CIR~\cite{wen2023tgcir} \footnotesize{\textcolor{gray}{(MM'23)}} & $45.25$ & $78.29$ & $87.16$ & $97.30$ & $72.84$ & $89.25$ & $95.13$ & $75.57$ \\
    TG-CIR+SPN~\cite{feng2024spn} \footnotesize{\textcolor{gray}{(MM'24)}} & $47.28$ & $79.13$ & $87.98$ & $97.54$ & $75.40$ & $89.78$ & $95.21$ & $77.27$ \\
    CASE~\cite{levy2024case} \footnotesize{\textcolor{gray}{(AAAI'24)}} & $48.00$ & $79.11$ & $87.25$ & $97.57$ & $75.88$ & $90.58$ & $96.00$ & $77.50$ \\
    CaLa_BLIP2Cir~\cite{feng2024spn} \footnotesize{\textcolor{gray}{(SIGIR'24)}} & $49.11$ & $81.21$ & $89.59$ & $98.00$ & $76.27$ & $91.04$ & $96.46$ & $78.74$ \\
    Re-ranking R50~\cite{liu2023rerank} \footnotesize{\textcolor{gray}{(TMLR'24)}} & $50.55$ & $81.75$ & $89.78$ & $97.18$ & $80.04$ & $91.90$ & $96.58$ & $80.90$ \\
    SDQUR~\cite{xu2024SDQUR} \footnotesize{\textcolor{gray}{(TCSVT'24)}} & $53.13$ & $83.16$ & $90.60$ & $98.22$ & $79.47$ & $91.74$ & $96.63$ & $81.32$ \\
    SPRC~\cite{xusentence2024sprc} \footnotesize{\textcolor{gray}{(ICLR'24)}} & $51.96$ & $82.12$ & $89.74$ & $97.69$ & $80.65$ & $92.31$ & $96.60$ & $81.39$ \\
    SPRC$^2$~\cite{xusentence2024sprc} \footnotesize{\textcolor{gray}{(ICLR'24)}} & $54.15$ & $83.01$ & $90.39$ & $98.17$ & $82.31$ & $92.68$ & $96.87$ & $82.66$ \\
    SPRC+SPN~\cite{feng2024spn} \footnotesize{\textcolor{gray}{(MM'24)}} & $55.06$ & $83.83$ & $90.87$ & $98.29$ & $81.54$ & $92.65$ & $97.04$ & $82.69$ \\
    SPRC+VQA~\cite{feng2023vqa4cir} \footnotesize{\textcolor{gray}{(Arxiv'23)}} & $54.00$ & $84.23$ & $91.85$ & $98.10$ & $82.07$ & $93.45$ & $97.08$ & $83.15$ \\
    
    \hline
    \end{tabular}
    \label{tab:supervised_CIR_exp_cirr}
    % }
\end{table*}


\begin{table*}
    \scriptsize
    \centering
    \caption{Performance comparison among supervised composed image retrieval models on MIT-States and CSS.}
    % \resizebox{14cm}{!}{
    \begin{tabular}{l|ccc|c|cc}
    \hline 
    \multirow{2}{*}{Model} & \multicolumn{4}{c|}{MIT-States} & \multicolumn{2}{c}{CSS}  \\
    \cline{2-7}
    & R@$1$ & R@$5$ & R@$10$ & Avg. & R@$1$(3D-to-3D) & R@$1$(2D-to-3D) \\
    \hline \hline 

    \multicolumn{7}{c}{\textit{\textcolor{gray}{Traditional Encoder-based Methods}}} \\
    TIRG~\cite{vo2019tirg} \footnotesize{\textcolor{gray}{(CVPR'19)}} & $12.20$ & $31.90$ & $43.10$ &$29.07$ & $73.70$ & $46.60$\\
    MGF~\cite{liu2021mgf} \footnotesize{\textcolor{gray}{(MMM'21)}} & $13.10$ & $-$ & $43.60$ & $-$ & $74.20$ & $62.80$\\
    TIS~\cite{zhang2022tis} \footnotesize{\textcolor{gray}{(TOMM'22)}} & $13.13$ & $31.94$ & $43.32$ & $29.46$ & $76.64$ & $48.02$\\
    TIRG+JPM(Tri)~\cite{yang2021jpm} \footnotesize{\textcolor{gray}{(MM'21)}} & $-$ & $-$& $-$ & $-$& $83.20$ & $-$\\
    TIRG+JPM(MSE)~\cite{yang2021jpm} \footnotesize{\textcolor{gray}{(MM'21)}} & $-$ & $-$& $-$ & $-$& $83.80$ & $-$\\
    TIRG+GA~\cite{huang2022ga} \footnotesize{\textcolor{gray}{(TIP'22)}} & $13.60$ & $32.40$ & $43.20$ & $29.70$ & $91.20$ & $-$\\
    JAMMA~\cite{zhang2020jamma} \footnotesize{\textcolor{gray}{(MM'20)}} & $14.27$ & $33.21$ & $45.34$ & $30.94$ & $76.07$ & $48.85$\\
    LBF-small~\cite{hosseinzadeh2020lbf} \footnotesize{\textcolor{gray}{(CVPR'20)}} & $14.29$ & $34.67$ & $46.06$ & $31.67$ & $67.26$ & $50.31$\\
    LBF-big~\cite{hosseinzadeh2020lbf} \footnotesize{\textcolor{gray}{(CVPR'20)}} & $14.72$ & $35.30$ & $46.56$ & $32.19$ & $79.20$ & $55.69$\\
    MCR~\cite{zhang2021mcr} \footnotesize{\textcolor{gray}{(MM'21)}} & $14.30$ & $35.36$ & $47.12$ & $32.26$ & $-$ & $-$\\
    ComposeAE~\cite{anwaar2021ComposeAE} \footnotesize{\textcolor{gray}{(WACV'21)}} & $13.90$ & $35.30$ & $47.90$ & $32.37$ & $-$ & $-$\\
    LGLI~\cite{huang2023lgli} \footnotesize{\textcolor{gray}{(CVPRW'23)}} & $14.90$ & $36.40$ & $47.70$ & $33.00$ & $93.30$ & $-$\\
    % TIRG-BERT+GA~\cite{} \footnotesize{\textcolor{gray}{(TIP'22)}} & $15.40$ & $36.30$ & $47.70$ & $33.13$\\
    ComposeAE+GA~\cite{huang2022ga} \footnotesize{\textcolor{gray}{(TIP'22)}} & $14.60$ & $37.00$ & $47.90$ & $33.20$ & $-$ & $-$\\
    GSCMR~\cite{zhang2021GSCMR} \footnotesize{\textcolor{gray}{(TIP'22)}} & $17.28$ & $36.45$ & $47.04$ & $33.59$ & $81.81$ & $58.74$\\
    
    \hline
    \end{tabular}
    \label{tab:supervised_CIR_exp_mit_css}
    % }
\end{table*}

\begin{table*}
    \scriptsize
    \centering
    \caption{Performance comparison among zero-shot composed image retrieval models on FashionIQ (original split).}
    % \resizebox{14.5cm}{!}{
    \begin{tabular}{l|c|cc|cc|cc|cc|c}
    \hline 
    \multirow{2}{*}{Model} & \multirow{2}{*}{Encoder} & \multicolumn{2}{c|}{Dresses} & \multicolumn{2}{c|}{Shirts} & \multicolumn{2}{c|} {Tops\&Tees} & \multicolumn{2}{c|}{Average} & \multirow{2}{*}{Avg.} \\ \cline{3-10}
    & & R@$10$ & R@$50$ & R@$10$ & R@$50$ & R@$10$ & R@$50$ & R@$10$ & R@$50$ & \\
    \hline \hline
    
    % \rowcolor{gray!5} 
    \multicolumn{11}{c}{\textit{\textcolor{gray}{Textual-inversion-based Methods}}}\\
    SEARLE~\cite{searle} \footnotesize{\textcolor{gray}{(ICCV'23)}} & CLIP-B & $18.54$ & $39.51$ & $24.44$ & $41.61$ & $25.70$ & $46.46$ & $22.89$ & $42.53$ & $32.71$ \\
    iSEARLE~\cite{isearle} \footnotesize{\textcolor{gray}{(Arxiv'24)}} & CLIP-B & $20.92$ & $42.19$ & $25.81$ & $43.52$ & $26.47$ & $48.70$ & $24.40$ & $44.80$ & $34.60$\\
    
    Pic2Word~\cite{pic2word} \footnotesize{\textcolor{gray}{(CVPR'23)}} & CLIP-L & $20.00 $ & $40.20$ & $26.20$ & $43.60$ & $27.90$ & $47.40$ & $24.70$ & $43.70$ &$34.22$\\
    SEARLE-XL~\cite{searle} \footnotesize{\textcolor{gray}{(ICCV'23)}} & CLIP-L & $20.48$ & $43.13$ & $26.89$ & $45.58$ & $29.32$ & $49.97$ & $25.56$ & $46.23$ & $35.90$ \\
    LinCIR~\cite{lincir} \footnotesize{\textcolor{gray}{(CVPR'24)}} & CLIP-L & $20.92$ & $42.44$ & $29.10$ & $46.81$ & $28.81$ & $50.18$ & $26.28$ & $46.49$ & $36.38$ \\
    KEDs~\cite{keds} \footnotesize{\textcolor{gray}{(CVPR'24)}} & CLIP-L & $21.70$ & $43.80$ & $28.90$ & $48.00$ & $29.90$ & $51.90$ & $26.80$ & $47.90$ & $37.37$ \\
    iSEARLE~\cite{isearle} \footnotesize{\textcolor{gray}{(Arxiv'24)}} & CLIP-L & $22.51$ & $46.36$ & $28.75$ & $47.84$ & $31.31$ & $52.68$ & $27.52$ & $48.96$ & $38.24$\\
    Context-I2W~\cite{context_i2w} \footnotesize{\textcolor{gray}{(AAAI'24)}} & CLIP-L & $23.10$ & $45.30$ & $29.70$ & $48.60$ & $30.60$ & $52.90$ & $27.80$ & $48.90$ & $38.40$ \\
    FTI4CIR~\cite{fti4cir} \footnotesize{\textcolor{gray}{(SIGIR'24)}} & CLIP-L & $24.39$ & $47.84$ & $31.35$ & $50.59$ & $32.43$ & $54.21$ & $29.39$ & $50.88$ & $40.14$\\
    
    LinCIR~\cite{lincir} \footnotesize{\textcolor{gray}{(CVPR'24)}} & CLIP-H & $29.80$ & $52.11$ & $36.90$ & $57.75$ & $42.07$ & $62.52$ & $36.26$ & $57.46$ & $46.86$ \\
    LinCIR~\cite{lincir} \footnotesize{\textcolor{gray}{(CVPR'24)}} & CLIP-G & $38.08$ & $60.88$ & $46.76$ & $65.11$ & $50.48$ & $71.09$ & $45.11$ & $65.69$ & $55.40$ \\
    
    ISA~\cite{isa} \footnotesize{\textcolor{gray}{(ICLR'24)}} & BLIP & $24.69$ & $43.88$ & $30.79$ & $50.05$ & $33.91$ & $53.65$ & $29.79$ & $49.19$ & $39.50$ \\
    ISA~\cite{isa} \footnotesize{\textcolor{gray}{(ICLR'24)}} & BLIP_CNN & $25.33$ & $46.26$ & $30.03$ & $48.58$ & $33.45$ & $53.80$ & $29.60$ & $49.54$ & $39.58$ \\
    ISA~\cite{isa} \footnotesize{\textcolor{gray}{(ICLR'24)}} & BLIP_VIT & $25.48$ & $45.51$ & $29.64$ & $48.68$ & $32.94$ & $54.31$ & $29.35$ & $49.50$ & $39.43$ \\
    Slerp+TAT~\cite{slerp} \footnotesize{\textcolor{gray}{(Arxiv'24)}}& BLIP & $29.15$ & $50.62$ & $32.14$ & $51.62$ & $37.02$ & $57.73$ & $32.77$ & $53.32$ & $43.05$ \\
    \cdashline{1-11}

    % \rowcolor{gray!5} % 设置该行的背景色为浅灰色
    \multicolumn{11}{c}{\textit{\textcolor{gray}{Pseudo-triplet-based Methods}}} \\
    MagicLens~\cite{zhang2024magiclens} \footnotesize{\textcolor{gray}{(ICML'24)}} & CLIP-B & $21.50$ & $41.30$ & $27.30$ & $48.80$ & $30.20$ & $52.30$ & $26.30$ & $47.40$ & $36.90$\\
    MTI~\cite{mti} \footnotesize{\textcolor{gray}{(Arxiv'23)}} & CLIP-B & $25.71$ & $47.81$ & $33.36$ & $53.47$ & $34.87$ & $58.44$ & $31.31$ & $53.24$ & $42.28$ \\
    Pic2Word+HyCIR~\cite{hycir} \footnotesize{\textcolor{gray}{(Arxiv'24)}} & CLIP-L & $19.98$ & $40.80$ & $27.62$ & $44.94$ & $28.14$ & $47.67$ & $25.25$ & $44.47$ & $34.86$ \\
    PM~\cite{pm} \footnotesize{\textcolor{gray}{(ICIP'24)}} & CLIP-L & $21.40$ & $41.70$ & $27.10$ & $43.80$ & $28.90$ & $47.30$ & $25.80$ & $44.20$ & $35.00$ \\
    LinCIR+RTD~\cite{rtd} \footnotesize{\textcolor{gray}{(Arxiv'24)}} & CLIP-L & $24.49$ & $48.24$ & $32.83$ & $50.44$ & $33.40$ & $54.56$ & $30.24$ & $51.08$ & $40.66$ \\
    MagicLens~\cite{zhang2024magiclens} \footnotesize{\textcolor{gray}{(ICML'24)}} & CLIP-L & $25.50$ & $46.10$ & $32.70$ & $53.80$ & $34.00$ & $57.70$ & $30.70$ & $52.50$ & $41.60$\\
    CompoDiff~\cite{compodiff} \footnotesize{\textcolor{gray}{(TMLR'24)}} & CLIP-L & $33.91$ & $47.85$ & $38.10$ & $52.48$ & $40.07$ & $52.22$ & $37.36$ & $50.85$ & $44.11$\\
    MTI~\cite{mti} \footnotesize{\textcolor{gray}{(Arxiv'23)}} & CLIP-L & $28.11$ & $51.12$ & $38.63$ & $58.51$ & $39.42$ & $62.68$ & $35.39$ & $57.44$ & $46.41$ \\
    MagicLens~\cite{zhang2024magiclens} \footnotesize{\textcolor{gray}{(ICML'24)}} & CoCa-B & $29.00$ & $48.90$ & $36.50$ & $55.50$ & $40.20$ & $61.90$ & $35.20$ & $55.40$ & $45.30$\\
    MagicLens~\cite{zhang2024magiclens} \footnotesize{\textcolor{gray}{(ICML'24)}} & CoCa-L & $32.30$ & $52.70$ & $40.50$ & $59.20$ & $41.40$ & $63.00$ & $38.00$ & $58.20$ & $48.10$\\
    % HyCIR~\cite{} \footnotesize{\textcolor{gray}{(Arxiv'24)}} & BLIP & $18.88$ & $34.50$ & $22.52$ & $37.58$ & $22.13$ & $40.33$ & $21.18$ & $37.47$ & $29.32$ \\
    CompoDiff~\cite{compodiff} \footnotesize{\textcolor{gray}{(TMLR'24)}} & CLIP-G & $37.78$ & $49.10$ & $41.31$ & $55.17$ & $44.26$ & $56.41$ & $41.12$ & $53.56$ & $47.34$\\
    TransAgg~\cite{transagg} \footnotesize{\textcolor{gray}{(BMVC'23)}} & BLIP & $31.28$ & $52.75$ & $34.45$ & $53.97$ & $37.79$ & $60.48$ & $34.64$ & $55.72$ & $45.18$ \\
    PVLF~\cite{pvlf} \footnotesize{\textcolor{gray}{(ACML'24)}} & BLIP & $31.58$ & $54.24$ & $36.61$ & $55.05$ & $38.85$ & $61.24$ & $35.68$ & $56.85$ & $46.26$ \\
    
    \cdashline{1-11}

    % \rowcolor{gray!5} 
    \multicolumn{11}{c}{\textit{\textcolor{gray}{Training-free Methods}}}\\
    LDRE~\cite{ldre} \footnotesize{\textcolor{gray}{(SIGIR'24)}}& CLIP-B & $19.97$ & $41.84$ & $27.38$ & $46.27$ & $27.07$ & $48.78$ & $24.81$ & $45.63$ & $35.22$ \\
    SEIZE~\cite{seize} \footnotesize{\textcolor{gray}{(MM'24)}}& CLIP-B & $25.37$ & $46.84$ & $29.38$ & $47.97$ & $32.07$ & $54.78$ & $28.94$ & $49.86$ & $39.40$ \\
    WeiMoCIR~\cite{weimocir} \footnotesize{\textcolor{gray}{(Arxiv'24)}}& CLIP-L & $25.88$ & $47.29$ & $32.78$ & $48.97$ & $35.95$ & $56.71$ & $31.54$ & $50.99$ & $41.26$\\
    LDRE~\cite{ldre} \footnotesize{\textcolor{gray}{(SIGIR'24)}}& CLIP-L & $22.93$ & $46.76$ & $31.04$ & $51.22$ & $31.57$ & $53.64$ & $28.51$ & $50.54$ & $39.53$ \\
    SEIZE~\cite{seize} \footnotesize{\textcolor{gray}{(MM'24)}}& CLIP-L & $30.93$ & $50.76$ & $33.04$ & $53.22$ & $35.57$ & $58.64$ & $33.18$ & $54.21$ & $43.69$ \\
    WeiMoCIR~\cite{weimocir} \footnotesize{\textcolor{gray}{(Arxiv'24)}}& CLIP-H & $28.76$ & $48.98$ & $36.56$ & $53.58$ & $39.72$ & $59.87$ & $35.01$ & $54.14$ & $44.58$\\
    LDRE~\cite{ldre} \footnotesize{\textcolor{gray}{(SIGIR'24)}}& CLIP-G & $26.11$ & $51.12$ & $35.94$ & $58.58$ & $35.42$ & $56.67$ & $32.49$ & $55.46$ & $43.97$ \\
    WeiMoCIR~\cite{weimocir} \footnotesize{\textcolor{gray}{(Arxiv'24)}}& CLIP-G & $30.99$ & $52.45$ & $37.73$ & $56.18$ & $42.38$ & $63.23$ & $37.03$ & $57.29$ & $47.16$\\
    SEIZE~\cite{seize} \footnotesize{\textcolor{gray}{(MM'24)}}& CLIP-G & $39.61$ & $61.02$ & $43.60$ & $65.42$ & $45.94$ & $71.12$ & $43.05$ & $65.85$ & $54.45$ \\
    Slerp~\cite{slerp} \footnotesize{\textcolor{gray}{(Arxiv'24)}}& BLIP & $22.91$ & $42.39$ & $27.33$ & $45.25$ & $32.33$ & $50.48$ & $27.52$ & $46.04$ & $36.78$ \\
    GRB~\cite{grb} \footnotesize{\textcolor{gray}{(Arxiv'23)}}& BLIP2 & $24.14$ & $45.56$ & $34.54$ & $55.15$ & $33.55$ & $53.60$ & $30.74$ & $51.44$ & $41.09$ \\
    
    \hline
    \end{tabular}
    \label{tab:zs_CIR_exp_fashioniq_ori}
    % }
\end{table*}

\begin{table*}
    \scriptsize
    \centering
    \caption{Performance comparison among zero-shot composed image retrieval models on CIRR and CIRCO.}
    % \resizebox{14.5cm}{!}{
    \begin{tabular}{l|c|cccc|ccc|cccc}
    \hline 
    \multirow{3}{*}{Model} & \multirow{3}{*}{Encoder} & \multicolumn{7}{c|}{CIRR} & \multicolumn{4}{c}{CIRCO}  \\ 
    \cline{3-13}
    && \multicolumn{4}{c|}{\textbf{R@$k$}} & \multicolumn{3}{c|}{\textbf{R$_{subset}$@$k$}} & \multicolumn{4}{c}{\textbf{mAP@$k$}}\\
    \cline{3-13}
    & & $k=1$ & $k=5$ & $k=10$ & $k=50$ & $k=1$ & $k=2$ & $k=3$ & $k=5$ & $k=10$ & $k=25$ & $k=50$ \\
    \hline \hline
    
    % \rowcolor{gray!5} 
    \multicolumn{13}{c}{\textit{\textcolor{gray}{Textual-inversion-based Methods}}}\\
    SEARLE~\cite{searle} \footnotesize{\textcolor{gray}{(ICCV'23)}} & CLIP-B & $24.00 $ & $53.42$ & $66.82$ & $89.78$ & $54.89$ & $76.60$ & $88.19$ & $9.35$ & $9.94$ & $11.13$ &$11.84$\\
    iSEARLE~\cite{isearle} \footnotesize{\textcolor{gray}{(Arxiv'24)}} & CLIP-B & $25.23$ & $55.69$ & $68.05$ & $90.82$ & $-$ & $-$ & $-$ & $10.58$ & $11.24$ & $12.51$ & $13.26$\\
    Pic2Word~\cite{pic2word} \footnotesize{\textcolor{gray}{(CVPR'23)}} & CLIP-L & $23.90 $ & $51.70$ & $65.30$ & $87.80$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ &$-$\\
    SEARLE-XL~\cite{searle} \footnotesize{\textcolor{gray}{(ICCV'23)}} & CLIP-L & $24.24 $ & $52.48$ & $66.29$ & $88.84$ & $53.76$ & $75.01$ & $88.19$ & $11.68$ & $12.73$ & $14.33$ &$15.12$\\
    KEDs~\cite{keds} \footnotesize{\textcolor{gray}{(CVPR'24)}} & CLIP-L & $26.40$ & $54.80$ & $67.20$ & $89.20$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\\
    LinCIR~\cite{lincir} \footnotesize{\textcolor{gray}{(CVPR'24)}} & CLIP-L & $25.04$ & $53.25$ & $66.68$ & $-$ & $57.11$ & $77.37$ & $88.89$ & $12.59$ & $13.58$ & $15.00$ & $15.85$\\
    iSEARLE~\cite{isearle} \footnotesize{\textcolor{gray}{(Arxiv'24)}} & CLIP-L & $25.28$ & $54.00$ & $66.72$ & $88.80$ & $-$ & $-$ & $-$ & $12.50$ & $13.61$ & $15.36$ & $16.25$\\
    Context-I2W~\cite{context_i2w} \footnotesize{\textcolor{gray}{(AAAI'24)}} & CLIP-L & $25.60$ & $55.10$ & $68.50$ & $89.80$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\\
    FTI4CIR~\cite{fti4cir} \footnotesize{\textcolor{gray}{(SIGIR'24)}} & CLIP-L & $25.90$ & $55.61$ & $67.66$ & $89.66$ & $55.21$ & $75.88$ & $87.98$ & $15.05$ & $16.32$ & $18.06$ & $19.05$\\
    LinCIR~\cite{lincir} \footnotesize{\textcolor{gray}{(CVPR'24)}} & CLIP-H & $33.83$ & $63.52$ & $75.35$ & $-$ & $62.43$ & $81.47$ & $92.12$ & $17.60$ & $18.52$ & $20.46$ & $21.39$\\
    LinCIR~\cite{lincir} \footnotesize{\textcolor{gray}{(CVPR'24)}} & CLIP-G & $35.25$ & $64.72$ & $76.05$ & $-$ & $63.35$ & $82.22$ & $91.98$ & $19.71$ & $21.01$ & $23.13$ & $24.18$\\  
    ISA~\cite{isa} \footnotesize{\textcolor{gray}{(ICLR'24)}} & BLIP\_CNN & $30.84$ & $61.06$ & $73.57$ & $92.43$ & $-$ & $-$ & $-$ & $11.33$ & $12.25$ & $13.42$ & $13.97$\\
    ISA~\cite{isa} \footnotesize{\textcolor{gray}{(ICLR'24)}} & BLIP\_VIT & $29.63$ & $58.99$ & $71.37$ & $91.47$ & $-$ & $-$ & $-$ & $9.82$ & $10.50$ & $11.61$ & $12.09$\\ 
    ISA~\cite{isa} \footnotesize{\textcolor{gray}{(ICLR'24)}} & BLIP & $29.68$ & $58.72$ & $70.79$ & $90.33$ & $-$ & $-$ & $-$ & $9.67$ & $10.32$ & $11.26$ & $11.61$\\
    Slerp+TAT~\cite{slerp} \footnotesize{\textcolor{gray}{(Arxiv'24)}} & BLIP & $33.98$ & $61.74$ & $72.70$ & $88.94$ & $68.55$ & $85.11$ & $93.21$ & $17.84$ & $18.44$ & $20.24$ & $21.07$\\
    
    \cdashline{1-13}

    % \rowcolor{gray!5} % 
    \multicolumn{13}{c}{\textit{\textcolor{gray}{Pseudo-triplet-based Methods}}} \\
    MTI~\cite{mti} \footnotesize{\textcolor{gray}{(Arxiv'23)}} & CLIP-B & $18.80$ & $46.07$ & $60.75$ & $86.41$ & $44.29$ & $68.10$ & $83.42$ & $8.14$ & $8.90$ & $10.12$ & $10.75$\\
    MagicLens~\cite{zhang2024magiclens} \footnotesize{\textcolor{gray}{(ICML'24)}} & CLIP-B & $27.00$ & $58.00$ & $76.90$ & $91.10$ & $66.70$ & $83.90$ & $92.40$ & $23.10$ & $23.80$ & $25.80$ & $26.70$\\
    MTI~\cite{mti} \footnotesize{\textcolor{gray}{(Arxiv'23)}} & CLIP-L & $25.52$ & $54.58$ & $67.59$ & $88.70$ & $55.64$ & $77.54$ & $89.47$ & $10.36$ & $11.63$ & $12.95$ & $13.67$\\
    MCL~\cite{mcl} \footnotesize{\textcolor{gray}{(ICML'24)}} & CLIP-L & $26.22$ & $56.84$ & $70.00$ & $91.35$ & $61.45$ & $81.61$ & $91.93$ & $17.67$ & $18.86$ & $20.80$ & $21.68$\\
    LinCIR+RTD~\cite{rtd} \footnotesize{\textcolor{gray}{(Arxiv'24)}} & CLIP-L & $26.63$ & $56.17$ & $68.96$ & $-$ & $-$ & $-$ & $-$ & $17.11$ & $18.11$ & $20.06$ & $21.01$\\
    MagicLens~\cite{zhang2024magiclens} \footnotesize{\textcolor{gray}{(ICML'24)}} & CLIP-L & $30.10$ & $61.70$ & $74.40$ & $92.60$ & $68.10$ & $84.80$ & $93.20$ & $29.60$ & $30.80$ & $33.40$ & $34.40$\\
    CompoDiff~\cite{compodiff} \footnotesize{\textcolor{gray}{(TMLR'24)}} & CLIP-L & $19.37$ & $53.81$ & $72.02$ & $90.85$ & $59.13$ & $78.81$ & $89.33$ & $12.31$ & $13.51$ & $15.67$ & $16.15$\\
    Pic2Word+HyCIR~\cite{hycir} \footnotesize{\textcolor{gray}{(Arxiv'24)}} & CLIP-L & $25.08$ & $53.49$ & $67.03$ & $89.85$ & $53.83$ & $75.06$ & $87.18$ & $14.12$ & $15.02$ & $16.72$ & $17.56$\\
    PM~\cite{pm} \footnotesize{\textcolor{gray}{(ICIP'24)}} & CLIP-L & $26.10$ & $55.20$ & $67.50$ & $90.20$ & $56.00$ & $76.60$ & $88.00$ & $-$ & $-$ & $-$ &$-$\\
    CompoDiff~\cite{compodiff} \footnotesize{\textcolor{gray}{(TMLR'24)}} & CLIP-G & $26.71$ & $55.14$ & $74.52$ & $92.01$ & $64.54$ & $82.39$ & $91.81$ & $15.33$ & $17.71$ & $19.45$ & $21.01$\\
    MagicLens~\cite{zhang2024magiclens} \footnotesize{\textcolor{gray}{(ICML'24)}} & CoCa-B & $31.60$ & $64.00$ & $76.90$ & $93.80$ & $69.30$ & $86.00$ & $94.00$ & $30.80$ & $32.00$ & $34.50$ & $35.60$\\
    MagicLens~\cite{zhang2024magiclens} \footnotesize{\textcolor{gray}{(ICML'24)}} & CoCa-L & $33.30$ & $67.00$ & $77.90$ & $94.40$ & $70.90$ & $87.30$ & $94.50$ & $34.10$ & $35.40$ & $38.10$ & $39.20$\\
    TransAgg~\cite{transagg} \footnotesize{\textcolor{gray}{(BMVC'23)}} & BLIP & $37.18$ & $67.21$ & $77.92$ & $93.43$ & $69.34$ & $85.68$ & $93.62$ & $-$ & $-$ & $-$ &$-$\\
    Pic2Word+HyCIR~\cite{hycir} \footnotesize{\textcolor{gray}{(Arxiv'24)}} & BLIP & $38.28$ & $69.03$ & $79.71$ & $95.27$ & $66.79$ & $84.79$ & $93.06$ & $18.91$ & $19.67$ & $21.58$ & $22.49$\\
    PVLF~\cite{pvlf} \footnotesize{\textcolor{gray}{(ACML'24)}} & BLIP & $40.33$ & $72.50$ & $82.44$ & $95.43$ & $72.64$ & $87.37$ & $94.69$ & $-$ & $-$ & $-$ &$-$\\
    
    \cdashline{1-13}

    % \rowcolor{gray!5} 
    \multicolumn{13}{c}{\textit{\textcolor{gray}{Training-free Methods}}} \\
    
    CIReVL~\cite{cirevl} \footnotesize{\textcolor{gray}{(ICLR'24)}}& CLIP-B & $23.94$ & $52.51$ & $66.00$ & $86.95$ & $60.17$ & $80.05$ & $88.19$ & $14.94$ & $15.42$ & $17.00$ & $17.82$\\
    LDRE~\cite{ldre} \footnotesize{\textcolor{gray}{(SIGIR'24)}} & CLIP-B & $25.69$ & $55.13$ & $69.04$ & $89.90$ & $60.53$ & $80.65$ & $90.70$ & $17.96$ & $18.32$ & $20.21$ & $21.11$\\
    SEIZE~\cite{seize} \footnotesize{\textcolor{gray}{(MM'24)}} & CLIP-B & $27.47$ & $57.42$ & $70.17$ & $-$ & $65.59$ & $84.48$ & $92.77$ & $19.04$ & $19.64$ & $21.55$ & $22.49$\\
    CIReVL~\cite{cirevl} \footnotesize{\textcolor{gray}{(ICLR'24)}}& CLIP-L & $24.55$ & $52.31$ & $64.92$ & $86.34$ & $59.54$ & $79.88$ & $89.69$ & $18.57$ & $19.01$ & $20.89$ & $21.80$\\
    WeiMoCIR~\cite{weimocir} \footnotesize{\textcolor{gray}{(Arxiv'24)}} & CLIP-L & $30.94$ & $60.87$ & $73.08$ & $91.61$ & $58.55$ & $79.06$ & $90.07$ & $-$ & $-$ & $-$ & $-$\\
    LDRE~\cite{ldre} \footnotesize{\textcolor{gray}{(SIGIR'24)}} & CLIP-L & $26.53$ & $55.57$ & $67.54$ & $88.50$ & $66.43$ & $80.31$ & $89.90$ & $23.35$ & $24.03$ & $26.44$ & $27.50$\\
    SEIZE~\cite{seize} \footnotesize{\textcolor{gray}{(MM'24)}} & CLIP-L & $28.65$ & $57.16$ & $69.23$ & $-$ & $66.22$ & $84.05$ & $92.34$ & $24.98$ & $25.82$ & $28.24$ & $29.35$\\
    WeiMoCIR~\cite{weimocir} \footnotesize{\textcolor{gray}{(Arxiv'24)}} & CLIP-H & $29.11$ & $59.76$ & $72.34$ & $91.18$ & $57.23$ & $79.08$ & $89.76$ & $-$ & $-$ & $-$ & $-$\\
    CIReVL~\cite{cirevl} \footnotesize{\textcolor{gray}{(ICLR'24)}}& CLIP-G & $34.65$ & $64.29$ & $75.06$ & $91.66$ & $67.95$ & $84.87$ & $93.21$ & $26.77$ & $27.59$ & $29.96$ & $31.03$\\
    WeiMoCIR~\cite{weimocir} \footnotesize{\textcolor{gray}{(Arxiv'24)}} & CLIP-G & $31.04$ & $60.41$ & $72.27$ & $90.89$ & $58.84$ & $78.92$ & $89.64$ & $-$ & $-$ & $-$ & $-$\\
    LDRE~\cite{ldre} \footnotesize{\textcolor{gray}{(SIGIR'24)}} & CLIP-G & $36.15$ & $66.39$ & $77.25$ & $93.95$ & $68.82$ & $85.66$ & $93.76$ & $31.12$ & $32.24$ & $34.95$ & $36.03$\\
    SEIZE~\cite{seize} \footnotesize{\textcolor{gray}{(MM'24)}} & CLIP-G & $38.87$ & $69.42$ & $79.42$ & $-$ & $74.15$ & $89.23$ & $95.71$ & $32.46$ & $33.77$ & $36.46$ & $37.55$\\
    Slerp~\cite{slerp} \footnotesize{\textcolor{gray}{(Arxiv'24)}} & BLIP & $28.60$ & $55.37$ & $65.66$ & $84.05$ & $65.16$ & $83.90$ & $92.05$ & $9.61$ & $10.11$ & $11.10$ & $11.66$\\
    GRB~\cite{grb} \footnotesize{\textcolor{gray}{(Arxiv'23)}} & BLIP2 & $24.19$ & $52.07$ & $65.48$ & $85.28$ & $60.17$ & $79.13$ & $89.34$ & $23.76$ & $25.31$ & $28.19$ & $29.17$\\
    GRB+LCR~\cite{grb} \footnotesize{\textcolor{gray}{(Arxiv'23)}} & BLIP2 & $30.92$ & $56.99$ & $68.58$ & $85.28$ & $66.67$ & $78.68$ & $82.60$ & $25.38$ & $26.93$ & $29.82$ & $30.74$\\
   
    \hline
    \end{tabular}
    \label{tab:zs_CIR_exp_cirr_circo_ori}
% }
\end{table*}

% 有待重写
\subsection{Experimental Results.}
In this subsection, we compare and analyze supervised CIR and ZS-CIR methods as reviewed above.


\subsubsection{Supervised Composed Image Retrieval.} 
To have an in-depth insight into the results of supervised CIR methods, we provide a comparison of their performance on various widely used datasets in Tables~\ref{tab:supervised_CIR_exp_fashioniq_ori}-~\ref{tab:supervised_CIR_exp_mit_css}. These datasets include FashionIQ, Fashion200k, MIT-States, CSS, Shoes, and CIRR. 
Notably, the FashionIQ dataset includes two evaluation protocols. However, some current approaches have erroneously intermixed results from different protocols during model comparison. To address this issue, we conduct a detailed inspection of the methods and their available source code. We then organize the comparisons for the VAL split and the original split separately to ensure fairness.
Furthermore, recognizing the significant impact of different encoders on model performance, we categorize the methods into two groups: those utilizing traditional encoders, such as ResNet and LSTM, and those employing VLP encoders (\textit{e.g.}, CLIP and BLIP) as the feature extraction backbones. 
From these tables, we obtain the following observations. 
1) VLP encoder-based methods generally achieve much better performance in comparison to traditional encoder-based methods. The reason is that VLP encoders are usually larger than traditional encoder methods. Moreover, VLP encoders are typically pre-trained on extensive corpora of image-text pairs through contrastive learning, thereby possessing excellent capabilities for multimodal alignment and cross-modal retrieval, which are crucial in the context of CIR. This can also be emphasized by methods that adopt the same fusion strategy but different types of encoders. As can be seen from Tables~\ref{tab:supervised_CIR_exp_fashioniq_val}-\ref{tab:supervised_CIR_exp_fashion200k_shoes_css}, AIRet-big with the VLP encoder attains significantly better performance than AIRet-small with a traditional encoder. Additionally, even when using the same type of traditional encoder or VLP encoder, the version with larger parameters typically delivers better performance. For example, LBF-big outperforms LBF-small (See Table~\ref{tab:supervised_CIR_exp_fashion200k_shoes_css}), and FashionERN-big surpasses FashionERN-small (See Table~\ref{tab:supervised_CIR_exp_fashioniq_ori}).
These results highlight that the choice of image and text encoders plays a pivotal role in the context of CIR, often surpassing the importance of image-text fusion and target matching method design.
2) Regarding image-text fusion, various strategies have demonstrated strong performance. Here, we summarize the top-performing methods for the FashionIQ-VAL, FashionIQ-ori, Fashion200k, Shoes, CIRR, MIT-States, and CSS datasets, respectively. Notably, to ensure a fair comparison, only prototype methods are evaluated, with additional modules such as data augmentation and reranking disabled.
The results show that DQU-CIR (MLP-based), SDQUR (Cross-attention-based), SPRC (Self-attention-based), and GSCMR (Graph-attention-based) achieve the best performance across these datasets. This indicates that while the image-text fusion strategy is a critical component of CIR methods, it is not the sole determinant of final performance. Moreover, it remains challenging to identify a universally optimal fusion strategy.
3) Models that integrate additional target matching techniques or data augmentation modules consistently demonstrate superior performance in comparison to their original counterparts. For example, SPRC-VQA, which employs visual question answering to re-rank the retrieval list, outperforms SPRC, as demonstrated in Table \ref{tab:supervised_CIR_exp_fashioniq_ori} and Table \ref{tab:supervised_CIR_exp_cirr}. LIMN+, by leveraging augmented pseudo-triplets for iterative training, attains better outcomes than LIMN shown in Table \ref{tab:supervised_CIR_exp_fashioniq_val} and Table \ref{tab:supervised_CIR_exp_fashion200k_shoes_css}. Additionally, ComposeAE+GA, which adopts the gradient augmentation regularization approach to achieve the dataset augmentation effect and mitigate overfitting, surpasses ComposeAE, as can be seen from Table \ref{tab:supervised_CIR_exp_fashion200k_shoes_css} and Table \ref{tab:supervised_CIR_exp_mit_css}. These examples emphasize the critical significance of further exploiting advanced target matching strategies and implementing dataset augmentation techniques to bolster the model's generalization capabilities.

%3) Some target matching trick, such as re-ranking and 
% $2$) Among the methods that adopt the gate mechanism as the fusion strategy, CLCV-Net generally outperforms the other approaches such as DCNet, JVSM, and MGF. This superior performance is primarily due to its joint consideration of both global-wise and local-wise image-text compositions, meanwhile, it incorporates a mutual enhancement module that facilitates the local and global composition processes by encouraging them to share knowledge with each other. 
% $3$) 

\subsubsection{Zero-shot Composed Image Retrieval.} ZS-CIR models, including textual-inversion-based, pseudo-triplet-based, and training-free models, are evaluated on the FashionIQ, CIRR, and CIRCO datasets. We directly summarize their experimental results from the corresponding papers in Table~\ref{tab:zs_CIR_exp_fashioniq_ori} and Table~\ref{tab:zs_CIR_exp_cirr_circo_ori}. 
It is worth noting that existing implementations of the ZS-CIR approach typically rely on the generalization capabilities of VLP-based encoders and are commonly tested with various backbone versions. Therefore, to enable a comprehensive comparison, we list all the performance results of methods utilizing different backbones in our tables.
From these tables, we obtain the following observations. 
1) Certain zero-shot methods yield comparable outcomes to some supervised methods. For example, on the FashionIQ-ori dataset, the top-performing ZS-CIR method, LinCIR (CLIP-G version), attains a score of $55.40$ when averaged across the metrics. Significantly, it not only outperforms all of the traditional encoder-based supervised methods but also manages to achieve a performance comparable to nearly half the number of the VLP encoder-based supervised methods. This implies that even in the absence of manually annotated triplet data, one can still obtain satisfactory retrieval results by ingeniously devising the pre-training strategy and fully activating the potential of powerful VLP capabilities within the CIR context.
2) Typically, the same zero-shot methods, when equipped with a large backbone, consistently deliver better performance. %This can be witnessed from the three datasets with the three zero-shot group methods. 
For example, LinCIR, ISA, MagicLens, CompoDiff, LDRE, SEIZE, and WieMoCIR all confirm that their larger-scale variants perform more effectively. This further evidences that VLP encoders, which possess well-trained multimodal encoding and cross-modal retrieval capabilities, significantly influence the performance of ZS-CIR.
3) Generally, among the three types of zero-shot methods, the overall performance of methods based on pseudo-triplets is better. For example, on the FashionIQ-ori dataset, regarding the average recall for textual-inversion-based methods, pseudo-triplet-based methods, and training-free methods, the number of methods showing a performance greater than $45.00\%$ is $2$, $6$, and $2$ respectively. On the CIRR dataset, based on R@$10 > 75\%$, the corresponding numbers are $2$, $7$, and $3$. And on the CIRCO dataset, based on mAP@$10 > 30\%$, the numbers are $0$, $3$, and $2$. This can be ascribed to the fact that the pseudo-triplet-based methods still construct triplet data that are most similar to the supervised training paradigm. Although this group of methods is somewhat resource-intensive during the pseudo-triplet construction stage, their overall performance is also the best. 


