\label{sec: meth_q}

\begin{figure}[!t]
		\centering
		\includegraphics[scale=0.50]{img/scir.pdf}
         % \vspace{-0.7em}
	    \caption{The illustration of the standard framework of supervised composed image retrieval.}\label{framework_scir}
          % \vspace{-1.5em}
\end{figure}

\begin{table*}
\scriptsize
\centering
 \caption{\textbf{Summarization of main supervised composed image retrieval approaches.}}
 \resizebox{14.5cm}{!}{
\begin{tabular}{|c|c|c|c|cc|c|}
\hline
\multicolumn{2}{|c|}{Fusion Strategy} & Method & Year & Image Encoder & Text Encoder & Other Aspect\\ 
\hline

% \multicolumn{2}{|c|}{\multirow{7}{*}{Plug-and-play}} & RTIC~\cite{shin2021rtic} & 2021 & ResNet-50 & LSTM & GCNs stream\\  \cline{3-7}

\multirow{22}{*}{ \makecell{Explicit Combination-based \\ Fusion}  } & \multirow{18}{*}{ \makecell{Transformed \\ image-and-Residual}} & TIRG~\cite{vo2019tirg} & 2019 & ResNet-17 & LSTM & CSS Dataset \\ \cline{3-7}
&  & VAL~\cite{chen2020val} & 2020 & ResNet-50, MobileNet & LSTM & Hierarchical Matching\\ \cline{3-7}
&  & JVSM~\cite{chen2020jvsm} & 2020 & MobileNet & LSTM & Joint Visual Semantic Matching \\ \cline{3-7}
&  & DATIR~\cite{gu2021datir} & 2021 & ResNet-50, MobileNet & LSTM & Hierarchical Matching \\ \cline{3-7}
&  & DCNet~\cite{kim2021dcnet} & 2021 & ResNet-50 & GloVe+MLP & Image Difference Alignment \\ \cline{3-7}
&  & MGF~\cite{liu2021mgf} & 2021 & ResNet-17 & LSTM & Online Groups Matching \\ \cline{3-7}
&  & MCR~\cite{zhang2021mcr} & 2021 & ResNet-50 & LSTM & Image Difference Alignment \\ \cline{3-7}
&  & CLVC-Net~\cite{wen2021clvcnet} & 2021 & ResNet-50 & LSTM & Mutual Enhancement \\ \cline{3-7}
&  & SAC~\cite{jandial2022sac} & 2022 & ResNet-50 & BERT & Matching Optimization \\ \cline{3-7}
&  & EER~\cite{zhang2022eer} & 2022 & ResNet-50 & LSTM & Semantic Space Alignment \\ \cline{3-7}
&  & CRN~\cite{yang2023crn} & 2023 & Swin Transformer & LSTM & Cross Relation Retrieval \\ \cline{3-7}
&  & MLCLSAP~\cite{zhang2023MLCLSAP} & 2023 & ResNet-50 & LSTM & Image Difference Alignment \\ \cline{3-7}
&  & TG-CIR~\cite{wen2023tgcir} & 2023 & CLIP-B & CLIP-B & Target Similarity Guidance \\ \cline{3-7}
&  & Ranking-aware~\cite{chen2023ranking} & 2023 & CLIP(RN50) & CLIP(RN50) & Uncertainty Modeling \\ \cline{3-7}
&  & MCEM~\cite{zhang2024mcem} & 2024 & ResNet-18, ResNet-50 & LSTM & Negative Example Mining \\ \cline{3-7}
&  & DWC~\cite{huang2024dwc} & 2024 & ResNet-50, CLIP(RN50) & LSTM, CLIP(RN50) & Mutual Enhancement \\ \cline{3-7}
&  & Css-Net~\cite{zhang2024cssnet} & 2024 & ResNet-18, ResNet-50 & RoBERTa & Collaborative Matching \\ \cline{3-7}
&  & AlRet~\cite{xu2024alret} & 2024 & ResNet-50, CLIP(RN50) & LSTM, CLIP(RN50) & Composition and Decomposition \\ \cline{2-7}

&  \multirow{4}{*}{Content-and-Style} & CoSMo~\cite{lee2021cosmo} & 2021 & ResNet-18, ResNet-50 & LSTM & - \\ \cline{3-7}
&  & LSC4TCIR~\cite{chawla2021lsc4cir} & 2021 & ResNet-50 & GRU & - \\ \cline{3-7}
&  & PCaSM~\cite{zhang2023pcasm} & 2023 & ResNet-18, ResNet-50 & LSTM & - \\ \cline{3-7}
&  & SPIRIT~\cite{chen2024spirit} & 2024 & CLIP(RN50x4) & CLIP(RN50x4) & Patch-level Graph Reasoning \\  \cline{2-7}

\hline

% Non-linear Transformation-based Fusion
\multirow{47}{*}{ \makecell{Neural Network-based \\ Fusion}  }& \multirow{18}{*}{MLP-based} & ComposeAE~\cite{anwaar2021ComposeAE} & 2021 & ResNet-17 & BERT & Rotational Symmetry \\ \cline{3-7}
&  & Combiner~\cite{baldrati2022Combiner} & 2022 & CLIP(RN50x4) & CLIP(RN50x4) & - \\ \cline{3-7}
&  & CLIP4CIR~\cite{baldrati2022CLIP4CIR} & 2022 & CLIP(RN50x4) & CLIP(RN50x4) & Fine-tune Strategy \\ \cline{3-7}
&  & PL4CIR~\cite{zhao2022PL4CIR} & 2022 & CLIP(RN50), CLIP-B & CLIP(RN50), CLIP-B & Fashion-based Fine-tuning \\ \cline{3-7}
&  & ARTEMIS~\cite{delmas2022artemis} & 2022 & ResNet-18, ResNet-50 & Bi-GRU, LSTM & - \\ \cline{3-7}
&  & CLIP4CIR2~\cite{baldrati2023CLIP4CIR2} & 2023 & CLIP(RN50x4) & CLIP(RN50x4) & Fine-tune Strategy \\ \cline{3-7}
&  & DSCN~\cite{li2023dscn} & 2023 & ResNet-18, ResNet-50 & Bi-GRU & Hierarchical Matching \\ \cline{3-7}
&  & CLIP-CD~\cite{lin2023clip_cd} & 2023 & CLIP(RN50x4) & CLIP(RN50x4) & Pseudo Triplet Generation \\ \cline{3-7}
&  & BLIP4CIR~\cite{liu2024blip4cir} & 2024 & BLIP & BLIP & Reverse Learning \\ \cline{3-7}
&  & CMAP~\cite{li2024cmap} & 2024 & ResNet-50 & Bi-GRU, LSTM & Hierarchical Matching \\ \cline{3-7}
&  & CAFF~\cite{wan2024caff} & 2024 & CLIP(RN50) & CLIP(RN50) & Fashion-based Fine-tuning \\ \cline{3-7}
&  & MANME~\cite{li2023manme} & 2024 & ResNet-50 & Bi-GRU, LSTM & Hierarchical Matching \\ \cline{3-7}
&  & NSFSE~\cite{wang2024NSFSE} & 2024 & ResNet-50, ResNet-152 & Bi-GRU, LSTM & Negative Sensitive Framework \\ \cline{3-7}
&  & SHAF~\cite{yan2024shaf} & 2024 & FashionCLIP & FashionCLIP & Hierarchical Alignment \\ \cline{3-7}
&  & CLIP-ProbCR~\cite{li2024clip} & 2024 & CLIP & CLIP & Uncertainty Modeling \\ \cline{3-7}
&  & DMOT~\cite{dmot} & 2024 & BLIP & BLIP & - \\ \cline{3-7}
&  & DQU-CIR~\cite{wen2024dqu} & 2024 & CLIP-H & CLIP-H & Data Augmentation \\ \cline{3-7}
&  & SADN~\cite{wang2024sadn} & 2024 & CLIP(RN50x4) & CLIP(RN50x4) & Neighborhood Distillation \\ \cline{2-7}

& \multirow{10}{*}{Cross-attention-based} & LBF~\cite{hosseinzadeh2020lbf} & 2020 & Faster R-CNN & TEP & Coarse and Fine Retrieval \\ \cline{3-7}
&  & MAAF~\cite{dodds2020maaf} & 2020 & ResNet-50 & LSTM &- \\ \cline{3-7}
&  & ProVLA~\cite{hu2023provla} & 2023 & Swin Transformer& BERT & Negative Example Mining \\ \cline{3-7}
&  & ComqueryFormer~\cite{xu2023ComqueryFormer} & 2023 & Swin Transformer & BERT & Hierarchical Matching \\ \cline{3-7}
&  & LGLI~\cite{huang2023lgli} & 2023 & ResNet-18 & LSTM & - \\ \cline{3-7}
&  & ACNet~\cite{li2023acnet} & 2023 & ResNet-50 & Bi-GRU & Image Difference Alignment \\ \cline{3-7}
&  & Re-ranking~\cite{liu2023rerank} & 2024 & BLIP & BLIP & Re-rank \\ \cline{3-7}
&  & CASE~\cite{levy2024case} & 2024 & BLIP & BLIP & Reverse Learning\\ \cline{3-7}
&  & SDQUR~\cite{xu2024SDQUR} & 2024 & BLIP2 & BLIP2 & Uncertainty Regularization \\ \cline{3-7}
&  & IUDC~\cite{ge2024iudc} & 2024 & CLIP & CLIP & LLM-based Data Augmentation \\ \cline{2-7}

& \multirow{17}{*}{Self-attention-based} & CIRPLANT~\cite{liu2021CIRPLANT} & 2021 & ResNet-152 & - & CIRR Dataset \\ \cline{3-7}
&  & FashionVLP~\cite{goenka2022fashionvlp} & 2022 & ResNet-18, ResNet-50 & BERT & Asymmetric Design \\ \cline{3-7}
&  & FashionViL~\cite{han2022fashionvil} & 2022 & ResNet-50 & BERT & Multi-task Pre-training \\ \cline{3-7}
&  & FaD-VLP~\cite{mirchandani2022fad} & 2022 & CLIP(RN50) & CLIP(RN50) & Multi-task Pre-training \\ \cline{3-7}
&  & AMC~\cite{zhu2023amc} & 2023 & ResNet-50 & LSTM & Dynamic Router Mechanism \\ \cline{3-7}
&  & AACL~\cite{tian2023aacl} & 2023 & Swin Transformer & DistilBERT & Revised Shopping100k Dataset \\ \cline{3-7}
&  & FAME-VIL~\cite{han2023fame} & 2023 & CLIP-B & CLIP-B & Multi-task Pre-training \\ \cline{3-7}
&  & NEUCORE~\cite{zhao2024neucore} & 2023 & ResNet & Bi-GRU & Multi-modal Concept Alignment \\ \cline{3-7}
&  & LMGA~\cite{udhayanan2023lmga} & 2023 & ViT & Visual-BERT & Gradient Attention \\ \cline{3-7}
&  & FashionERN~\cite{chen2024fashionern} & 2024 & CLIP-B & CLIP-B & Modifier Enhancement \\ \cline{3-7}
&  & SDFN~\cite{wu2024sdfn} & 2024 & ResNet-50 & LSTM & Dynamic Router Mechanism \\ \cline{3-7}
&  & SSN~\cite{yang2024ssn} & 2024 & CLIP-B & CLIP-B & - \\ \cline{3-7}
&  & LIMN~\cite{wen2023limn} & 2024 &CLIP-L & CLIP-L & Pseudo Triplet Generation \\ \cline{3-7}
&  & SPRC~\cite{xusentence2024sprc} & 2024 & BLIP2 & BLIP2 & Auxiliary loss \\ \cline{3-7}
&  & VISTA~\cite{zhou2024vista} & 2024 & EVA-CLIP-02-Base & BGE-Base-v1.5 & - \\ \cline{3-7}
&  & SyncMask~\cite{song2024syncmask} & 2024 & VIT-B & BERT & Multi-task Pre-training \\ \cline{3-7}
&  & UniFashion~\cite{zhao2024unifashion} & 2024 & CLIP & CLIP & Multi-task Pre-training \\ \cline{2-7}

& \multirow{2}{*}{Graph-attention-based} & JAMMA~\cite{zhang2020jamma} & 2020 & ResNet-101 & Bi-GRU & - \\ \cline{3-7}
&  & GSCMR~\cite{zhang2021GSCMR} & 2022 & ResNet-101 & Bi-GRU & - \\ \cline{3-7}

\hline

\multicolumn{2}{|c|}{\multirow{2}{*}{Prototype Image Generation-based Fusion}} & SynthTripletGAN~\cite{tautkute2021Synth} & 2021 & ResNet-101 & Bi-GRU & - \\ \cline{3-7}
\multicolumn{2}{|c|}{}& TIS~\cite{zhang2022tis} & 2022 & Inception-v3 & LSTM & - \\ 
 
 \hline

\multicolumn{2}{|c|}{\multirow{7}{*}{Plug-and-play}} & RTIC~\cite{shin2021rtic} & 2021 & ResNet-50 & LSTM & GCNs stream\\  \cline{3-7}
 \multicolumn{2}{|c|}{}& JPM~\cite{yang2021jpm} & 2021 & ResNet-18 & LSTM & Image Difference Alignment \\ \cline{3-7}
 \multicolumn{2}{|c|}{}& GA~\cite{huang2022ga} & 2022 & ResNet-18 & LSTM & Gradient Augmentation \\ \cline{3-7}
 \multicolumn{2}{|c|}{}& VQA4CIR~\cite{feng2023vqa4cir} & 2023 & - & - & Re-rank \\ \cline{3-7}
 \multicolumn{2}{|c|}{}& CIR-MU~\cite{chen2022mu} & 2024 & ResNet-50 & RoBERTa & Uncertainty Modeling \\ \cline{3-7}
 \multicolumn{2}{|c|}{}& CaLa~\cite{jiang2024cala} & 2024 & - & - & Image Difference Alignment \\ \cline{3-7}
  \multicolumn{2}{|c|}{}& SDA~\cite{sda2024} & 2024 & - & - & LLM-based Data Augmentation \\ \cline{3-7}
 \multicolumn{2}{|c|}{}& SPN~\cite{feng2024spn} & 2024 & - & - & Positive Example Generation \\ 
 \hline
\end{tabular}

\label{tab:sup_cir}
}
\end{table*}


\section{Supervised Composed Image Retrieval}
In this section, we first provide the problem statement for the task of supervised CIR, and then
present existing approaches. Generally,  as depicted in Figure~\ref{framework_scir}, existing models involve four key components: feature extraction, image-text fusion, target matching, and data augmentation. The first three are essential components for CIR, while the last one is optional and aimed at enhancing model performance. Existing supervised CIR methods are summarized in Table~\ref{tab:sup_cir}.

\subsection{Problem Statement.} Given a reference image and its modification text, CIR aims to retrieve target images from a collection of gallery images. In the supervised learning setting, existing methods rely on training samples in triplet form, \textit{i.e.}, <reference image, modification text, target image>. Let $\mathcal{D}$ = $\{(I_r,T_m,I_t)_i\}^N_{i=1}$ denote a set of such triples, where $I_r$ is the reference image, $T_m$ is the modification text, $I_t$ signifies the target image, and $N$ is the total number of triplets. Then, based on the training dataset $\mathcal{D}$, existing methods aim to learn a multimodal fusion function that effectively combines the multimodal query $(I_r,T_m)$ and a visual feature embedding function to ensure that the composed query and the corresponding target image are close in the embedding space. This can be formalized as follows,
\begin{equation}
f(I_r,T_m) \rightarrow h(I_t), 
\end{equation}
where $f(\cdot)$ represents the multimodal fusion function mapping the multimodal query to the latent space, while $h(\cdot)$ denotes the feature embedding function for the target image. 


\subsection{Feature Extraction}
In the task of CIR, feature extraction plays a crucial role in deriving meaningful embeddings from both the input query and the target image. Since feature extraction has been extensively studied in fields such as natural language processing and computer vision, most existing methods for CIR leverage established textual and visual feature extraction backbones to encode the input query and target image. We categorize these encoder backbones into two primary types: traditional encoders and vision-language pre-trained (VLP) model-based encoders.


\subsubsection{Traditional Encoder.}
For textual feature extraction, commonly used encoders for CIR tasks include RNN-based encoders and transformer-based encoders. Representative RNN-based encoders used in CIR studies are Bidirectional Gated Recurrent Units (BiGRUs)~\cite{cho2014learning} and Long Short-Term Memory networks (LSTMs), which have proven effective in capturing long-term dependencies in text sequences. Specifically, existing CIR studies~\cite{delmas2022artemis,li2023dscn,li2024cmap,li2023manme,tautkute2021Synth,zhang2020jamma, zhang2021GSCMR, wang2024NSFSE,li2023acnet,zhao2024neucore} employ BiGRUs as text encoders to process sequences bidirectionally, enriching feature embedding by capturing context from both past and future tokens. Meanwhile, several studies~\cite{zhang2022eer,huang2023lgli,xu2024alret,zhang2022tis,zhang2023MLCLSAP,yang2023crn,zhang2024mcem,dodds2020maaf,wu2024sdfn, zhang2021mcr} utilize LSTMs, which introduce gated mechanisms to the standard RNN structure, effectively managing long-range dependencies for modification text feature extraction. With the emergence of transformers~\cite{vaswani2017attention}, a growing number of CIR studies~\cite{jandial2022sac,anwaar2021ComposeAE, goenka2022fashionvlp,hu2023provla,han2022fashionvil,tian2023aacl, xu2023ComqueryFormer,song2024syncmask} adopt transformer-based encoders, such as BERT~\cite{devlin2019bert} and its variants (\textit{e.g.}, RoBERTa~\cite{liu2019roberta} and DistilBERT~\cite{sanh2019distilbert}), as their text encoders. These encoders leverage self-attention mechanisms to capture global context across the entire text sequence, enabling parallel processing and producing deeper contextual embeddings. Overall, compared to RNN-based encoders, transformer-based encoders demonstrate superior capabilities for textual embedding in CIR tasks, particularly when pre-trained on extensive corpora. 


Similarly, traditional image encoders used in CIR studies can be categorized into CNN-based and transformer-based encoders. CNN-based encoders are initially popular due to their ability to capture spatial hierarchies through convolution operations, preserving crucial spatial information and providing robust hierarchical feature embeddings. Many CIR methods~\cite{vo2019tirg, chen2020val,liu2021mgf,wen2021clvcnet,zhang2023pcasm,delmas2022artemis,zhang2023MLCLSAP,huang2023lgli,xu2024alret,tautkute2021Synth,zhang2024cssnet} extract image features with pre-trained CNN-based encoders, such as ResNet~\cite{krizhevsky2017imagenet}, GoogleNet~\cite{szegedy2015going}, and MobileNet~\cite{howard2017mobilenets}, which yield generalizable feature embeddings by being pre-trained on large-scale datasets like ImageNet~\cite{deng2009imagenet}. 
In contrast to CNN-based encoders, which directly feed the entire image into the encoder, transformer-based encoders redefine image encoding by segmenting images into non-overlapping patches and employing self-attention to model spatial relationships. One commonly used transformer-based encoder for CIR~\cite{song2024syncmask,udhayanan2023lmga} is Vision Transformer (ViT)~\cite{dosovitskiy2021image}, which captures more nuanced visual details with a self-attention mechanism over image patches. Additionally, several CIR methods~\cite{xu2023ComqueryFormer, hu2023provla,tian2023aacl,yang2023crn} employ Swin Transformers~\cite{liu2021swin}, which adopt a window-based self-attention mechanism for local interactions within each window, reducing computational complexity. 
Typically, transformer-based encoders offer superior representational capabilities compared to CNN-based ones, particularly when pre-trained on extensive datasets. 


\subsubsection{VLP-based Encoder.}
Recently, advancements in vision-language pre-training have led to the prevalence of VLP-based encoders, which are now preferred for encoding multimodal data due to their ability to align visual and textual modalities. For example, several studies~\cite{chen2024spirit,wen2023tgcir,yang2024ssn,xu2024alret,baldrati2022CLIP4CIR,zhao2022PL4CIR,chen2023ranking,lin2023clip_cd,wen2024dqu,wan2024caff} adopt CLIP~\cite{radford2021learning} as their feature extraction backbone, which leverages contrastive learning on large-scale image-text datasets and demonstrates exceptional flexibility across diverse domains. In addition, some studies~\cite{liu2023rerank,liu2024blip4cir,levy2024case} utilize BLIP~\cite{li2022blip}, which unifies vision-language understanding and generation with a multimodal mixture of encoder-decoder framework.
% incorporates both contrastive and generative pre-training strategies to enhance multimodal understanding and facilitate knowledge transfer. 
Moreover, BLIP-2~\cite{li2023blip2} has also been adopted for feature extraction in recent CIR studies~\cite{xusentence2024sprc, xu2024SDQUR}, which bridges the modality gap with a lightweight Querying Transformer (Q-Former) and achieves state-of-the-art performance on various vision-language tasks.
% which refines feature integration and inference accuracy of the standard BLIP. 
Collectively, these VLP-based encoders provide more robust feature embeddings for CIR tasks, as compared with traditional encoders. 

\subsection{Image-text Fusion}
Once the reference image and modification text in the input query are separately encoded, the subsequent crucial step involves designing an effective image-text fusion strategy to integrate the complementary information from both modalities, in order to precisely represent the input query and conduct the target image retrieval. Towards this end, existing methods can be categorized into three groups: explicit combination-based fusion, neural network-based fusion, and prototype image generation-based fusion.

\subsubsection{Explicit Combination-based Fusion}
The first group of methods aims to accomplish image-text fusion through explicit combination operations, which can be categorized into two types: transformed image-and-residual combination and content-and-style combination.

\textbf{Transformed Image-and-Residual.} The key idea of this group of methods is to keep the image feature as the dominant component, and achieve image-text fusion by learning two parts: the transformed reference image feature and the residual feature.
This group of methods can typically be expressed as \underline{$g\left( \left[\mathit{img};\mathit{txt}\right] \right) \odot \mathit{img} + \mathit{res}$}. Here, $\mathit{img}$ and $\mathit{txt}$ represent the reference image embedding and modification text embedding, respectively. Notably, $\mathit{img}$ and $\mathit{txt}$ may not be the direct output of the feature extraction component. 
To enhance image-text fusion, various types of image/text features, such as global features~\cite{vo2019tirg, chen2020jvsm,chen2023ranking,zhang2024mcem,xu2024alret}, local features~\cite{liu2021mgf,zhang2022eer,lee2021cosmo}, hierarchical features~\cite{chen2020val,kim2021dcnet,wen2021clvcnet,yang2023crn,jandial2022sac,zhang2023MLCLSAP,huang2023lgli,huang2024dwc,chen2024spirit,zhang2024cssnet}, and decoupled features~\cite{wen2023tgcir,chawla2021lsc4cir,zhang2023pcasm} have been explored. The function $g\left(\cdot\right)$ is a neural network that derives the parameters applied to the reference image to achieve the modification operation, and $\odot$ denotes the element-wise multiplication. $\mathit{res}$ refers to the residual offsetting information. 
Based on the method used to obtain $\mathit{res}$, the approaches in this branch can be further divided into two major categories. 
The first category of methods, including TIRG~\cite{vo2019tirg},VAL~\cite{chen2020val}, JVSM~\cite{chen2020jvsm}, MGF~\cite{liu2021mgf}, DCNet~\cite{kim2021dcnet}, CLVC-Net~\cite{wen2021clvcnet}, Css-Net~\cite{zhang2024cssnet}, and CRN~\cite{yang2023crn}, directly fuse the image and text features to derive the residual offsetting information, \textit{i.e.}, \underline{$\mathit{res} = h\left( \left[\mathit{img};\mathit{txt}\right] \right)$}. The most representative method is TIRG~\cite{vo2019tirg}, which designs $g\left(\cdot\right)$ as a gating function with a Sigmoid activation function to adaptively preserve the unchanged information in the reference image, and $h\left(\cdot\right)$ as a simple MLP-based neural network to fuse the image and text features and derive the residual offsetting information. Given its simple yet effective image-text fusion paradigm, the following JVSM, MGF, and DCNet directly adopt the same manner as TIRG does, while VAL, CLVC-Net, and CRN share similar spirits as TIRG, except that they utilize the attention mechanism instead of the gating function. For example, VAL devises a joint-attention mechanism ($g\left(\cdot\right)$) to suppress and highlight the visual content based on the spatial and channel dimensions of the reference image feature maps. Meanwhile, it combines self-attention learning ($h\left(\cdot\right)$) to capture crucial visio-linguistic cues as the residual part. CLVC-Net devises two attention mechanism-based streams to derive $g\left(\cdot\right)$ and $h\left(\cdot\right)$ from both local-wise and global-wise perspectives. Subsequently, these two streams are made to learn from one another with mutual learning in order to obtain a more comprehensive image-text fusion result. In particular, Css-Net adopts two different forms of compositor, namely \underline{$\mathit{img} + \mathit{res}$} and \underline{$\mathit{text} + \mathit{res}$}. The former primarily aims to discern ``what elements to modify'' within the reference image, guided by the modification text; while the latter emphasizes determining ``what to retain'' within the modification text given the reference image. 

Another group of methods adheres to the formula expressed as \underline{$\mathit{res} = h\left( \left[\mathit{img};\mathit{txt}\right] \right) \odot \mathit{txt}$} to learn the residual part. Typically, both $g\left(\cdot\right)$ and $h\left(\cdot\right)$ are scaled to fall within the range of $0$ and $1$. This strategy works on not only preserving the unchanged part within the reference image, but also integrating information from the modification text that is to replace certain aspects. TG-CIR~\cite{wen2023tgcir}, DWC~\cite{huang2024dwc}, AlRet~\cite{xu2024alret}, and EER~\cite{zhang2022eer} all follow this criteria to achieve the image-text fusion. Several methods adopt a most straightforward approach by directly adding $\mathit{img}$ and $\mathit{txt}$, such as MCEM~\cite{zhang2024mcem} and Ranking-aware~\cite{chen2023ranking}. For the sake of convenience, we also classify them into this group by considering $g\left(\cdot\right) = h\left(\cdot\right) = \mathbf{I}$, \textit{i.e.}, identity mapping. 
Likewise, LGLI~\cite{huang2023lgli}, where $g\left(\cdot\right) = \mathbf{I}$ and SAC~\cite{jandial2022sac}, where $h\left(\cdot\right) = \mathbf{I}$) are also categorized into this group. Notably, instead of using element-wise multiplication to transform the image features, SAC relies on the attentional transformation to derive the text-conditioned image representation and then adds it to the modification text representation to obtain the fused feature, which can be expressed as $s\left( \left[\mathit{img};\mathit{txt}\right] \right) + \mathit{txt}$. $s\left(\cdot\right)$ denotes the attentional transformation network. 

% \textcolor{red}{Inspired by the psychological concept that groups outperform individuals, Css-Net~\cite{zhang2024cssnet} introduces a consensus module with four diverse compositors to facilitate complementary feature extraction while mitigating dependency on any single biased compositor. }

%Although it is slightly different from our predefined formula in that the image transformation is achieved by a non-linear function rather than element-wise multiplication, it shares the same spirit of the transformed image-and-residual combination.
% ATTEMIS CMAP

\textbf{Content-and-Style.} Considering that ``each image can be well characterized by their content and style''~\cite{chawla2021lsc4cir}, this group of methods~\cite{lee2021cosmo, chawla2021lsc4cir, zhang2023pcasm, chen2024spirit} typically hypothesizes that both the image style and content will be modified in accordance with the modification text. Thereby, modifications are achieved in both the style and content spaces, and they are finally combined to obtain the fusion output. For example, CoSMo~\cite{lee2021cosmo} initially devises a content modulator equipped with a disentangled multimodal non-local block for effecting content modifications. Subsequently, once the content has been adjusted, a style modulator is designed to incorporate the modified style information, ensuring a sequential progression from content to style adaptation.
Differently, LSC4TCIR~\cite{chawla2021lsc4cir} and PCaSM~\cite{zhang2023pcasm} pursue content and style modifications in parallel. Specifically, they decompose the reference image into its corresponding style and content features, then perform semantic replacement within these feature spaces according to the modification text. Finally, the content and style features are fed into the combination module parallel for image-text fusion. 
While SPIRIT~\cite{chen2024spirit} primarily centers on the style aspect. It introduces an explicit definition of style as the commonality and difference among the local patches of an image. With this concept in mind, SPIRIT first segments the reference image into multi-granularity patches. Subsequently, two modules are devised to model the local patches' commonality and difference, respectively. Then the two style features are combined to enrich the representation of the reference image, which is fused with text features to obtain the final query
representation. 

\subsubsection{Neural Network-based Fusion}
In contrast to the methods of the former group, methods in this group rely entirely on neural networks to fuse the reference image and modification text without employing explicit combination operations. These methods can be further categorized into four subgroups: MLP-based, cross-attention-based, self-attention-based, and graph-attention-based approaches.

\textbf{MLP-based.} Methods in this branch~\cite{anwaar2021ComposeAE,baldrati2022Combiner,baldrati2022CLIP4CIR,zhao2022PL4CIR,baldrati2023CLIP4CIR2,li2023dscn,lin2023clip_cd,liu2024blip4cir,wan2024caff,li2023manme,zhang2024cssnet,yan2024shaf,li2024clip,wen2024dqu,wang2024sadn,wang2024NSFSE,delmas2022artemis,li2024cmap,dmot} primarily rely on the multi-layer perceptron (MLP) to fulfill the image-text fusion. For example, PL4CIR~\cite{zhao2022PL4CIR} introduces a multi-stage learning framework to progressively acquire the complex knowledge necessary for multimodal image retrieval and employs an MLP-based query adaptive weighting strategy to dynamically balance the influence of image and text. 
As pioneers in applying CLIP to CIR tasks, Baldrati~\textit{et al.}~\cite{baldrati2022Combiner} introduce a classic multimodal fusion network, \textit{i.e.}, combiner, which integrates CLIP-based reference image and modification text features using MLP-based weighted summing and feature concatenation. Later, they also propose several fine-tuning strategies~\cite{baldrati2022CLIP4CIR, baldrati2023CLIP4CIR2} within this framework to alleviate the domain discrepancy between CLIP's pre-training data and the downstream task data. This combiner network has been directly adopted by many subsequent methods~\cite{liu2024blip4cir, wen2024dqu, wang2024sadn} or refined in later studies~\cite{lin2023clip_cd,wan2024caff}. 
Considering the differing contribution between the input reference image and modification text, DWC~\cite{huang2024dwc} introduces an Editable Modality De-equalizer (EMD). This module employs two modality editors equipped with spatial and word attention mechanisms to refine image and text features, respectively. It then utilizes an MLP-based adaptive weighting module to assign modality weights based on contributions. Additionally, DWC further incorporates a CLIP-based mutual enhancement module, which effectively mitigates modality discrepancies and promotes similarity learning. To achieve a more precise alignment of visual and linguistic features, SHAF~\cite{yan2024shaf} initially employs an attention mechanism for text and image feature realignment at multiple levels when encoding the reference image and modification text using FashionCLIP~\cite{chia2022contrastive}. Subsequently, an MLP-based dynamic feature fusion strategy is used for integrating the multimodal information by emphasizing critical features through weight allocation and feature enhancement mechanisms. 

\textbf{Cross-attention-based.} 
CIR methods~\cite{hosseinzadeh2020lbf, dodds2020maaf, xu2023ComqueryFormer, huang2023lgli, li2023acnet, ge2024iudc, hu2023provla} in this branch primarily adopt cross-attention to model the interaction between each word in the modification text and every local region in the reference image, thereby enhancing their fusion. In this approach, one modality serves as the query, while the other acts as the key and value. Among them, LGLI~\cite{huang2023lgli} introduces a localization mask, derived from the object detection model Faster R-CNN~\cite{RenHG2017}, as an additional input for image-text fusion, enabling precise local modifications over the reference image. 
Recognizing that the generated localization mask may not always be reliable, the authors additionally introduce a channel cross-modal attention mechanism and a spatial cross-modal attention mechanism to effectively localize the to-be-modified regions. 
ACNet~\cite{li2023acnet} proposes a multi-stage compositional framework that sequentially modifies the reference image based on textual semantics. At each stage, the framework first enhances image features using a self-attention layer applied to the image's regional features. It then applies a cross-attention-based relation transformation layer to establish cross-modal associations between image regions and word features.
To address comprehensive modification intent reasoning, IUDC~\cite{ge2024iudc} introduces a dual-channel matching model comprising a semantic matching module and a visual matching module. The semantic matching module utilizes a gate-based attention mechanism to fuse attributes of the reference image, generated by a large language model~(LLM), with the modification text for semantic reasoning. The visual matching module uses affine transformation for image-text fusion, preceded by a cross-attention mechanism to enhance interaction between the two input modalities. These two modules are trained collaboratively, transferring knowledge to each other for mutual enhancement.
In contrast, to capture fine-grained deterministic many-to-many correspondence between the composed query and target, 
SDQUR~\cite{xu2024SDQUR} leverages the Q-former module of BLIP2~\cite{li2023blip2} to achieve adaptive fine-grained image-text fusion between each word in the modification text and every local region in the reference image. Specifically, it incorporates a set of learnable queries that first interact with word tokens of modification text through self-attention layers and then exchange information with visual patch features through cross-attention layers, thereby learning diverse semantic aspects from the input query.
Unlike previous VLP encoder-based methods that compose the multimodal query in a late fusion manner, CASE~\cite{levy2024case} introduces a cross-attention driven shift encoder based on BLIP's image-grounded text encoder, \textit{i.e.}, a BERT encoder with intermediate cross-attention layers. The image is first encoded by ViT and then injected into the cross-attention layers of the shift encoder, enabling early image-text fusion. 
% 直接使用blip 而不是基于BLIP 构建一个shift encoder
% \textcolor{red}{ Re-ranking~\cite{liu2023rerank} }


\textbf{Self-attention-based.} 
Contrary to cross-attention, in the self-attention mechanism that has gained prominence with the rise of Transformer, the input sequence simultaneously plays roles of the query, key, and value, for computing the attention scores for each input element. Consequently, self-attention enables the learning of dependencies among different elements within a single sequence. Methods~\cite{mirchandani2022fad,tian2023aacl,han2023fame,zhao2024neucore,udhayanan2023lmga,chen2024fashionern,wen2023limn,song2024syncmask,yang2024ssn,zhao2024unifashion} in this branch typically feed the concatenation of the encoded reference image feature and the modification text feature into a self-attention-based network, like Transformer, to fully learn their interaction for promoting image-text fusion. 
Among them, AACL~\cite{tian2023aacl} refines the standard Transformer encoder with an additive self-attention layer, which utilizes the additive attention mechanism to capture contextual information and selectively suppress or highlight the representation of each token, thereby facilitating the retention and modification of reference image information. 
Different from previous work that treats the modification text as a single description, SSN~\cite{yang2024ssn} treats the modification text as an instruction and hence explicitly decomposes the semantic transformation conveyed by the modification text into two steps: degradation and upgradation. Specifically, it first uses an MLP-based degradation network to degrade the reference image to a visual prototype that only retains to-be-preserved visual attributes. Then, a Transformer-based upgrading network is adopted to upgrade the visual prototype to the final desired target image. Both processes are guided by the modification text. 
Contrary to above methods that compose the multimodal query based on their extracted features, some methods~\cite{liu2021CIRPLANT,goenka2022fashionvlp,xusentence2024sprc,zhou2024vista,han2022fashionvil} first concatenate the encoded reference image features and word tokens from the modification text
and then feed the concatenated token sequence into a transformer-based model for image-text fusion.
Different from the above methods that adopt a single fusion strategy, AMC~\cite{zhu2023amc} and SDFN~\cite{wu2024sdfn} consider multiple fusion strategies, where the self-attention mechanism acts as only one fusion option, and incorporates a dynamic routing mechanism for adaptive image-text fusion. 
Especially, some methods~\cite{mirchandani2022fad,song2024syncmask,han2023fame} focus on addressing various vision-and-language (V+L) tasks in the fashion domain, including cross-modal retrieval and CIR, within a transformer-based foundation model. By pretraining on multiple meticulously designed tasks, their transformer-based frameworks exhibit competitive performance across a range of heterogeneous fashion-related tasks. 


\textbf{Graph-attention-based.} Graph attention mechanisms are specifically designed for handling graph-structured data. Unlike the above cross-attention and self-attention, their primary distinction lies in that they deal with data structured as graphs rather than sequences, and they take into account the relationships between edges and nodes within the graph. 
% 先总结单个方法
To manipulate the visual features of the reference image according to semantics in the modification text at the attribute level, JAMMA~\cite{zhang2020jamma} leverages the pretrained Faster R-CNN to extract visual attribute features for each reference/target image. These features are then utilized as vertices to construct a graph, where each edge is established based on the relative size and location relationships between two attributes~\cite{darec2019}. Subsequently, JAMMA employs a jumping graph attention network to infuse semantic information from the modification text into the attribute graph, dynamically assigning higher weights to attributes most relevant to the text. Finally, it adopts a global semantic reasoning module, which follows the idea of gate and memory mechanism~\cite{vscitm}, to filter out redundant attributes, thereby yielding a more discriminative global query feature. By jointly modeling the geometric information of the image and the visual-semantic relationship between the input image and text, GSCMR~\cite{zhang2021GSCMR} initially learns cross-modal embedding for the composed query in a geometry-aware way and then rectifies the visual feature under the guidance of the modification text with a multi-head graph attention network~\cite{2018graphattentionnetworks}. 

\subsubsection{Prototype Image Generation-based Fusion}
Apart from the first two groups, some methods aim to achieve multimodal fusion by directly synthesizing a prototype image that satisfies the requirements of the multimodal query. This approach effectively converts CIR into an image-to-image retrieval problem.
Recognizing the lack of interpretability in traditional methods that directly combine inputs into a multimodal query representation for target image retrieval, SynthTripletGAN~\cite{tautkute2021Synth} pioneers integrate the Generative Adversarial Networks (GANs) into CIR, where a triplet loss is used for metric learning. % 
In contrast, TIS~\cite{zhang2022tis} introduces a multi-stage GAN-based structure that embeds a retrieval model within a GAN framework. To learn a discriminative composed query feature,
TIS uses two distinct discriminators: one targeting global differences between generated and target images, and the other identifying local modifications in the generated images. 
Unlike the above two methods that concentrate on the CIR task, UniFashion~\cite{zhao2024unifashion} focuses on developing a unified framework leveraging LLMs and diffusion models to enhance the performance of multimodal retrieval and generation tasks with mutual task reinforcement. 


\subsection{Target Matching}
The target matching module aims to accurately retrieve images that match the given multimodal query. One fundamental technique for target matching in CIR is metric learning, which establishes a feature space where distances effectively represent semantic similarities or differences. To enhance metric learning, several strategies have been developed, which can be categorized into five groups: basic metric learning, image difference alignment, negative mining, uncertainty modeling, and re-ranking. 

\subsubsection{Basic Metric Learning}
Existing CIR studies primarily utilize three types of loss functions: the batch-based classification (BBC)~\cite{vo2019tirg} loss function, the soft triplet-based loss function, and the hinge-based triplet ranking function.

\textbf{Batch-based classification loss.} This loss function is the most widely used for metric learning in current CIR studies~\cite{vo2019tirg,hosseinzadeh2020lbf,kim2021dcnet,baldrati2022CLIP4CIR,wen2023tgcir,levy2024case,yang2024ssn,jiang2024cala,yan2024shaf,huang2023lgli,xusentence2024sprc,shin2021rtic,zhang2022eer,huang2022ga,li2023acnet,baldrati2023CLIP4CIR2,baldrati2022Combiner}. This loss function aims to bring the query embedding closer to the annotated target image embedding while treating all other target images in the same batch as negative samples, pushing them away from the query embedding.
It is formulated as follows, 
\begin{equation}
L_{BBC} = \frac{1}{B} \sum_{i=1}^{B}\left[ -\log \left(\frac {exp\{\kappa({\phi}^{(i)}, \mathbf{x_t}^{(i)}) /\tau \}}{\sum_{j=1}^{B}exp\{\kappa({\phi}^{(i)} , \mathbf{x_t}^{(j)} ) /\tau \}} \right) \right], 
\label{eq:bbc_loss}
\end{equation}
where the subscript $i$ refers to the $i$-th triplet sample in the mini-batch, ${\phi}$ and $\mathbf{x_t}$ represent the combined feature of the input query and the target feature, respectively. $B$ is the batch size, $\kappa\left( \cdot, \cdot \right)$ serves as the cosine similarity function, and $\tau$ denotes the temperature factor. 

\textbf{Soft triplet-based loss.}
This loss function is a specific variant of the batch-based classification loss function, where a single candidate image from the batch is selected as the negative example at each iteration. It has been widely adopted for metric learning in several CIR studies~\cite{vo2019tirg, hosseinzadeh2020lbf, liu2021mgf, liu2021CIRPLANT, yang2021jpm}. Specifically, this loss function is formulated as follows,
\begin{equation}
L_{ST} = \frac{1}{MB} \sum_{i=1}^{B}  \sum_{m=1}^{M}  \log \{ 1 + exp\{ \kappa({\phi}^{(i)}, \mathbf{x_t}^{(i)}) - \kappa({\phi}^{(i)}, \mathbf{\tilde{x}_{(t,m)}})  \} \}, 
\label{eq:st_loss}
\end{equation}
where the subscript $i$ refers to the $i$-th triplet sample in the mini-batch, $ \mathbf{\tilde{x}_{(t,m)}}$ represents the $m$-th selected negative sample, and $M$ is the repeat times to evaluate every possible set. 

\textbf{Hinge-based triplet ranking loss.} This loss focuses on optimizing hard negative samples and has been adopted by several CIR studies~\cite{zhang2020jamma, zhang2021GSCMR, chen2020jvsm, li2023dscn}. Similar to the above loss, its primary objective is to cluster matched query-target pairs while separating unmatched ones in the embedding space. By concentrating on hard negatives, this loss function this loss function effectively addresses the challenges of high redundancy and slow convergence associated with the random triplet sampling process inherent in soft triplet-based loss functions. Formally, the objective function is defined as follows, 
\begin{equation}
L_{rank} = max[0, \gamma - F({\phi}^{(i)}, \mathbf{x_t}^{(i)}) + F({\phi}^{(i)}, \mathbf{\tilde{x_t}}^{(i)})] 
+ max[0, \gamma - F({\phi}^{(i)}, \mathbf{x_t}^{(i)}) +F({\tilde{\phi}}^{(i)}, \mathbf{x_t}^{(i)})], 
\label{eq:hb_loss}
\end{equation}
where the subscript $i$ refers to the $i$-th triplet sample in the dataset, $F(\cdot)$ denotes the semantic similarity function, $\gamma$ is a margin value, ${\tilde{\phi}}$ and $\tilde{\mathbf{x_t}}$ represent the hard negatives for the positive pair (${\phi}$, $\mathbf{x_t}$). 

Most CIR methods typically rely on single-granularity matching, optimizing the model using one of the three aforementioned loss functions to minimize the distance between the final combined feature and the target image. However, since visual elements vary substantially in scale~\cite{liu2021swin}, several studies~\cite{chen2020val, huang2023lgli, xu2023ComqueryFormer, li2023dscn, li2024cmap, li2023manme} have explored hierarchical matching to improve the alignment between the input query and the target image. These methods start by sampling multi-granular visual features from the visual encoder and separately integrating them with the modification text feature. Then, they minimize the distance between these composed features and the corresponding granularity-level features of the target image based on their respective loss functions to optimize the model. 

\subsubsection{Image Difference Alignment.} 
As mentioned above, mainstream methods model the task as a query-target matching task, \textit{i.e.}, encoding the multimodal query into a single feature and then aligning it with the target image. However, this learning paradigm only explores the most straightforward relationship within each triplet. In fact, beyond this query-target matching relationship, there exists a latent relationship between the reference-target image pair and the modification text. Intuitively, the modification text should capture the visual difference between the reference image and the target image. It acts as an implicit transformation to convert the reference image into the target image. Accordingly, several studies~\cite{kim2021dcnet, zhang2021mcr, yang2021jpm,li2023acnet, jiang2024cala} explore image difference alignment, \textit{i.e.}, aligning the difference between the reference image and target image to the modification text, for boosting the metric learning. To achieve this, several studies~\cite{kim2021dcnet, jiang2024cala, li2023acnet} adapt the conventional BBC loss for image difference alignment as follows, 
\begin{equation}
L_{BBC}^{’} = \frac{1}{B} \sum_{i=1}^{B}\left[ -\log \left(\frac {exp\{\kappa(\mathbf{v_d}^{(i)}, \mathbf{t_m}^{(i)}) /\tau \}}{\sum_{j=1}^{B}exp\{\kappa(\mathbf{v_d}^{(i)} , \mathbf{t_m}^{(j)} ) /\tau \}}  \right) \right], 
\label{eq:imagedifferencebbc_loss}
\end{equation}
where the subscript $i$ refers to the $i$-th triplet sample in the mini-batch, ${\mathbf{v_d}}$ and $\mathbf{\mathbf{t_m}}$ represent the visual difference representation of the reference-target image pair and the modification text representation, respectively. Typically, ${\mathbf{v_d}}$ is derived from neural networks, such as MLP and cross-attention networks.
Differently, JPM~\cite{yang2021jpm} adopts the Mean Squared Error~(MSE) loss to narrow the distance between image differences and text modifications as follows,
\begin{equation}
L_{MSE} = \frac{1}{N} \sum_{i=1}^{N} \parallel \mathbf{v_d}^{(i)} - \mathbf{t_m}^{(i)} \parallel ^2,
\label{eq:mse}
\end{equation}
where the subscript $i$ refers to the $i$-th triplet sample in the dataset.
Different from the above methods, MCR~\cite{zhang2021mcr} formulates image difference alignment as a modification text generation problem. It inputs the features of reference and target images into an LSTM, attempting to generate the modification text directly. The commonly used cross-entropy loss for text generation is adopted to optimize this process. Furthermore, to enhance image difference alignment, NEUCORE~\cite{zhao2024neucore} designs multimodal concept alignment, which targets mining and aligning the visual concepts in the reference and target images with the semantic concepts in modification text. On one hand, NEUCORE extracts keywords from the modification text as semantic concepts, embedding them using GloVe~\cite{pennington2014glove}. On the other hand, it learns the visual concepts present in the reference and target images using a transformer-based model.
Since the modification text is typically concise and contains limited semantic concepts, whereas images convey a wealth of visual concepts, NEUCORE employs an asymmetric loss~\cite{ridnik2021asymmetric} to supervise multimodal concept alignment. This loss function ensures effective alignment by accounting for the inherent imbalance in the richness of concepts between textual and visual modalities, and it is formulated as follows:
\begin{equation}
\left\{
\begin{array}{ll}
\mathbf{s_i} = sigmod(\mathbf{v_{rt}}^{(i)}\cdot \mathbf{w_c}^{(i)} ), \\
L_{asy} = -\frac{1}{N} \big( \sum_{i \in \mathcal{P}}^{}(1-\mathbf{s_i})^{\beta+}  log(\mathbf{s_i}) + \sum_{j\in \mathcal{N}}^{}(\mathbf{s_j})^{\beta-}  log(1-\mathbf{s_j}) \big),
\end{array}
\right.
\label{eq:as_loss}
\end{equation}
where the subscript $i$ refers to the $i$-th triplet sample in the dataset, $\mathbf{v_{rt}}$ and $\mathbf{w_c}$ represent the joint visual concept feature of the reference-target image pair and the embedding of one semantic concept extracted from the modification text, respectively. $\mathcal{P}$ and $\mathcal{N}$ are the positive and negative sets, respectively. ${\beta+}$ and ${\beta-}$ are hyper-parameters that balance the importance of positive and negative concepts, respectively. 

\subsubsection{Negative Mining.}
While the mainstream BBC loss helps models learn associations between composed queries and target images, it treats all other examples within the same batch equally as negative samples. This leads to the issue of false negative samples, as in CIR tasks, a query might correspond to multiple target images, even though only one is annotated as the positive example. 
Moreover, this loss overlooks the varying impact of different negative samples in metric learning. Intuitively, using hard negative samples, \textit{i.e.}, negative examples that are particularly challenging to classify, can significantly benefit model optimization. To address these limitations, researchers have proposed several negative mining techniques.
To deal with the false negative issue, TG-CIR~\cite{wen2023tgcir} first utilizes the visual similarity distribution between the ground-truth target image features and other candidate image features within the batch to regularize the model's metric learning. In addition, NSFSE~\cite{wang2024NSFSE} flexibly learns the boundaries between matched triplets and mismatched triplets using Gaussian distributions, where a flexible threshold is learned to distinguish positive target images from negative ones. 
Furthermore, SADN~\cite{wang2024sadn} first calculates the similarity between the composed query and each candidate target image to select the top-$K$ most relevant candidate images as a neighborhood. It then adaptively aggregates the features of these neighborhood target images to refine the query feature. This incorporation of neighborhood target features effectively mitigates the adverse impact caused by false negative samples. 
To effectively mine hard negative samples, ProVLA~\cite{hu2023provla} introduces a moment queue-based hard negative mining mechanism that uses momentum-based distillation to dynamically store the most recent embeddings of composed images, reference images, and target images. These stored embeddings serve as hard negative samples for triplets, enabling the selection of hard negatives across multiple batches.
Later, instead of constructing conventional query-level hard negative samples, MCEM~\cite{zhang2024mcem} proposes two strategies for generating component-level hard negative samples. The first strategy involves directly replacing the entire modification text in a training triplet to create a hard negative sample. The second strategy narrows the replacement scope by replacing only partial dimensions of the modification text embedding, resulting in more challenging negative samples.
In particular, MCEM introduces a mask vector controlled by a Bernoulli distribution with parameter $p$ for modification text embedding replacement. The parameter $p$ controls the similarity between the newly generated modification text embedding and the original version. 

\subsubsection{Uncertainty Modeling.} 
In the existing CIR datasets, only one target image per query is annotated. However, as mentioned earlier, the inherent ambiguity arising from general modification text often leads to many-to-many relationships between input queries and target images. To address this limitation, Ranking-aware~\cite{chen2023ranking} introduces a novel ranking-aware uncertainty approach, which employs stochastic mappings instead of deterministic ones to capture many-to-many correspondences. Specifically, images and text are encoded not as deterministic features but as distributions in the feature space. This approach optimizes many-to-many ranking through three key components: in-sample uncertainty, cross-sample uncertainty, and distribution regularization.
In contrast, CIR-MU~\cite{chen2022mu} and SDQUR~\cite{xu2024SDQUR} retain the one-to-one matching paradigm while integrating Gaussian-based uncertainty modeling and uncertainty regularization to accommodate more diverse retrieval requirements. Uncertainty modeling simulates the true range of uncertainty within an effective domain, estimated based on the feature distribution within a mini-batch. Additionally, uncertainty regularization prevents the model from excluding potential true positives, thereby improving recall rates.


\subsubsection{Re-ranking.}
As illustrated in Figure~\ref{framework_scir}, most existing CIR methods primarily adopt dual branches: one branch encodes the query, while the other encodes the target image to perform target image retrieval. This architecture ensures efficient inference since the embeddings of all candidate target images can be pre-computed. The model then only needs to embed the given test query and compare it with the pre-computed candidate image embeddings.
However, this approach relies solely on the metric learning module with the BBC loss function to regulate the target image and the query. Due to issues like false negatives and modification ambiguity, the retrieval performance of current CIR models still has room for improvement. To address this, several studies have proposed re-ranking techniques for CIR tasks.
For instance, Re-ranking~\cite{liu2023rerank} introduces a dual-encoder architecture to re-rank the initial retrieval results obtained by conventional dual-branch CIR models. Specifically, one encoder jointly encodes the given query and each candidate target image, while the other jointly encodes the modification text and each candidate target image. An MLP network then fuses the outputs of the two encoders to compute the final ranking score. A contrastive loss, similar to the BBC loss, is used to optimize the re-ranking module. This strategy allows each candidate's target image to interact with the given query more deeply and comprehensively. Importantly, since re-ranking is performed on a subset of candidate images selected by conventional dual-branch CIR models, this sophisticated strategy is computationally feasible during inference.
Additionally, VQA4CIR~\cite{feng2023vqa4cir} re-ranks retrieval results by querying a multimodal large language model (MLLM), \textit{e.g.}, LLaVA~\cite{liu2024visual}, to determine whether the candidate images contain the desired attributes specified by the modification text. This method serves as a post-processing approach that can be seamlessly integrated into any existing CIR model.

\subsection{Data Augmentation}
As mentioned earlier, existing CIR datasets typically consist of triplets in the form of \textless \emph{reference image, modification text, target image}\textgreater. However, creating such training samples is both expensive and labor-intensive, which significantly limits the size of benchmark datasets. As a result, previous research relying solely on these limited samples has faced overfitting issues to some extent and demonstrated poor generalization capabilities.
To overcome this challenge, researchers have proposed various data augmentation strategies.
\begin{itemize}
    \item \textbf{Image replacement-based}. CLIP-CD~\cite{lin2023clip_cd} introduces a CLIP visual similarity-based data augmentation method that replaces the reference or target images in triplets with visually similar alternatives to generate pseudo triplets, effectively enlarging the dataset. Notably, it establishes lower and upper similarity thresholds to ensure the quality and relevance of the augmented samples. 
    \item \textbf{IDC model-based}. Many potential reference-target image pairs in existing datasets remain unlabeled, despite being highly similar and differing only in minor properties. To utilize these unlabeled pairs and enhance model performance, LIMN+~\cite{wen2023limn} introduces an iterative dual self-training paradigm. This approach employs the dual model of CIR—specifically, an image difference captioning (IDC) model~\cite{h2018learning}—to automatically annotate these pairs, generating pseudo-triplets for improved model training. To ensure the quality of the pseudo-triplets generated in each iteration, the CIR model trained in the previous iteration filters out triplets with low query-target matching scores.
    \item  \textbf{LLM-based}. Leveraging the advanced image comprehension capabilities of the large multi-modal model GPT4V, IUDC~\cite{ge2024iudc} employs specially designed prompts to guide GPT4V in generating attribute-level labels for each image in the training triplets. These attribute-level labels are combined with TF-IDF~\cite{salton1988term} features to identify potential reference-target image pairs. Subsequently, ChatGPT is used to generate corresponding modification texts based on the attribute-level labels of these image pairs, enabling the construction of a large volume of triplet data. Additionally, SDA~\cite{sda2024} uses ChatGPT to generate pseudo modification texts by editing specific attributes of the original modification text. GPT4V then generates a target image based on the original reference image and the generated modification text, thereby creating new triplets to assist model training. 
    \item \textbf{Query unification-based}. To fully utilize VLP models like CLIP for mitigating overfitting, DQU-CIR~\cite{wen2024dqu} introduces two raw query unification methods: text-oriented query unification and vision-oriented query unification. In text-oriented query unification, the modification text is combined with the textual description of the reference image, extracted using the VLP model BLIP-2, to create a purely textual query. In vision-oriented query unification, key modification words are directly written onto the reference image at the pixel level, forming a purely visual query. Notably, the image encoder of the VLP model demonstrates strong Optical Character Recognition (OCR) capabilities, enabling it to effectively process such modified images.
    \item \textbf{Reverse objective-based}. CASE~\cite{levy2024case} and BLIP4CIR~\cite{liu2024blip4cir} expand the dataset by incorporating a reverse retrieval objective, \textit{i.e.}, retrieving the reference image given the modification text and the target image. They achieve this by introducing a reverse objective token, such as ``[REV]'' and ``[Backward]'', to specify the retrieval direction. This reverse objective intuitively encourages the model to learn the shift vector between the reference image and the target image from both directions, enhancing its ability to generalize across tasks.
    \item  \textbf{Gradient-based}. Unlike the aforementioned data augmentation methods, GA~\cite{huang2022ga} improves the model's generalization ability through gradient augmentation rather than raw data augmentation. Specifically, GA comprises two components: explicit adversarial gradient augmentation and implicit isotropic gradient augmentation. Explicit adversarial gradient augmentation introduces a gradient-oriented regularization term into the loss function to simulate adversarial sample training. Implicit isotropic gradient augmentation increases triplet diversity by modifying gradients according to the principle of isotropy.
\end{itemize}


\begin{table*}
    \scriptsize
    \centering
    \caption{\textbf{Summarization of main zero-shot composed image retrieval approaches.}}
  \label{tab: zs_CIR}
   % \resizebox{14.5cm}{!}{
    \begin{tabular}{|c|c|c|c|c|}
    \hline
      Category & Method & Year & Encoder & Key Aspect\\
    \hline 
    
    \multirow{8}{*}{Textual-inversion-based} 
    & Pic2Word~\cite{pic2word} & 2023  & CLIP-L& Coarse-grained Inversion \\ 
    \cline{2-5}
    & SEARLE~\cite{searle} & 2023  & CLIP-B/L & Coarse-grained Inversion \\ 
    \cline{2-5}
    & iSEARLE~\cite{isearle} & 2024  & CLIP-B/L & Coarse-grained Inversion \\
    \cline{2-5}
    & KEDs~\cite{keds} & 2024  & CLIP-L & Knowledge Enhancement\\
    \cline{2-5}
    & Context-I2W~\cite{context_i2w} & 2024  & CLIP-L & Context-dependent Inversion\\
    \cline{2-5}
    & FTI4CIR~\cite{fti4cir} & 2024  & CLIP-L & Fine-grained Inversion \\
    \cline{2-5}
    & ISA~\cite{isa} & 2024  & BLIP & Adaptive Inversion\\
    \cline{2-5}
    & LinCIR~\cite{lincir} & 2024  & CLIP-L/H/G & Self-masking Projection\\
    
    \hline
    
    \multirow{9}{*}{Pseudo-triplet-based} 
    & TransAgg~\cite{transagg} & 2023  & BLIP-B, CLIP-B/L & Transformer-based\\
    \cline{2-5}
    & HyCIR~\cite{hycir} & 2024  & BLIP-B, CLIP-B & Additional Training Stream \\
    \cline{2-5}
    & MCL~\cite{mcl} & 2024  & CLIP-L & MLLM-based\\
    \cline{2-5}
    & MagicLens~\cite{zhang2024magiclens} & 2024  & CoCa-B/L, CLIP-B/L & Transformer-based\\
    \cline{2-5}
    
    & RTD~\cite{rtd} & 2024  & -& Target-Anchored Contrastive Learning\\
    \cline{2-5}
    & CompoDiff~\cite{compodiff} & 2024  & CLIP-L/G & Diffusion-based\\
    \cline{2-5}
    & PVLF~\cite{pvlf} & 2024  & BLIP & V\&L Prompt learning\\
    \cline{2-5}
    
    & MTI~\cite{mti} & 2023  & CLIP-B/L & Masked Learning\\
    \cline{2-5}
    & PM~\cite{pm} & 2024  & CLIP-L & Masked Learning\\
    % \cline{2-5}
    % & MoTaDual~\cite{pm} & 2024  & - & Prompt Learning\\
    
    
    \hline
    
    \multirow{5}{*}{Training-free} 
    & CIReVL~\cite{cirevl} & 2023  & CLIP-B/L/G & Language-level Reasoning\\
    \cline{2-5}
    & GRB~\cite{grb} & 2023  & BLIP2 & Coarse-Fine Reranking\\  
    \cline{2-5}
    & LDRE~\cite{ldre} & 2024  & CLIP-B/L/G & Divergent Reasoning\\ 
    \cline{2-5}
    & SEIZE~\cite{seize} & 2024  & CLIP-B/L/G & Divergent Reasoning\\
    \cline{2-5}
    & Slerp~\cite{slerp} & 2024  & BLIP-L, CLIP-B/L & Spherical Linear Interpolation\\
    \cline{2-5}
    & WeiMoCIR~\cite{weimocir}& 2024  & CLIP-L/H/G & Weighted Modality Fusion\\ 
    \hline
    \end{tabular}
 % }
\end{table*}


\section{Zero-shot Composed Image Retrieval}
Although supervised CIR approaches have achieved favorable performance, they rely heavily on annotated triplets in the form of \textless \emph{reference image, modification text, target image}\textgreater \hspace{0.2em}for training. However, annotating modification text for each possible \textless \emph{reference image, target image}\textgreater \hspace{0.2em}pair is a time-consuming process. To reduce the dependence on labeled datasets, Pic2Word~\cite{pic2word} introduces ZS-CIR, which aims to perform retrieval without requiring any annotated training triplets. 
Existing ZS-CIR methods can be broadly categorized into three groups: textual-inversion-based, pseudo-triplet-based, and training-free. 
For ease of reference, Table~\ref{tab: zs_CIR} summarizes these methods across the three categories. 


\subsection{Textual-inversion-based}
Methods~\cite{pic2word,searle,isearle,context_i2w,fti4cir,isa,keds,lincir} in this category begin by using text inversion technology~\cite{cohen2022my, gal2023an} to map reference image embeddings into textual token representations. These tokens are then combined with those from the modification text to form a unified query. The query is subsequently encoded using the text encoder of a VLP model, like CLIP, enabling image-text fusion.


 \subsubsection{Coarse-grained textual inversion.}
 Pic2Word~\cite{pic2word} is a pioneering method in this category, introducing the task of ZS-CIR. It leverages a collection of unlabeled images to train a lightweight mapping network that transforms image embeddings, obtained from the CLIP visual encoder, into token embeddings compatible with the CLIP text encoder. In this way, each image can be represented by a pseudo-token-based sentence, such as ``a photo of $S^*$'', where $S^*$ denotes a learnable pseudo-word. The mapping network is optimized using a contrastive loss between the image feature and its corresponding pseudo-token-based sentence embedding.
 Around the same time, SEARLE~\cite{searle} proposed two approaches: an optimization-based textual inversion method (OTI) and a mapping network-based method, both aimed at learning pseudo-word tokens that encapsulate the visual content of each image. Unlike Pic2Word, these methods incorporate category-based semantic regularization to align pseudo-word tokens with the CLIP token embedding space, ensuring compatibility with real textual tokens.
 Building on SEARLE, iSEARLE~\cite{isearle} introduces Gaussian noise to text features during OTI to reduce the modality gap between text and image. Moreover, to enhance the mapping network's ability to capture visual contents, iSEARLE implements a similarity clustering-based hard negative sampling strategy, ensuring that each training batch contains a proportion of visually similar images.
 Furthermore, KEDs~\cite{keds} introduces a Bi-modality Knowledge-guided Projection network (BKP), which leverages an external database to provide relevant image-caption pairs as knowledge, enriching the mapping function and improving its generalization ability. Additionally, similar to SEARLE, recognizing the challenges of aligning pseudo-word tokens with real text concepts using only image contrastive training, KEDs introduces an additional training stream that explicitly aligns pseudo-word tokens with semantics using pseudo triplets uncovered from image-caption pairs. Overall, the aforementioned methods perform coarse-grained textual inversion, which simply converts the input image into a single general pseudo-word token, utilizing the entire visual content of the image without differentiation. 

\subsubsection{Fine-grained textual inversion.}
Beyond the above methods, some studies~\cite{context_i2w, keds, fti4cir, isa} have explored fine-grained textual inversion to enhance the performance of ZS-CIR.
For instance, instead of converting the entire visual content of an image, Context-I2W~\cite{context_i2w} adaptively selects caption-relevant content for textual inversion using a context-dependent word mapping network. It employs an intent view selector to map the given image to a task-specific manipulation view and a visual target extractor to collect content related to the specific view.
Instead of converting an image into a single pseudo-word token, FTI4CIR~\cite{fti4cir} maps the image into a subject-oriented pseudo-word token along with several attribute-oriented pseudo-word tokens to comprehensively represent the image in textual form. Additionally, it introduces a tri-wise semantic regularization method based on BLIP-generated image captions, aligning the fine-grained pseudo-word tokens with the real-word token embedding space.
ISA~\cite{isa}, akin to FTI4CIR, converts an image into a series of sentence tokens rather than a single pseudo-word token. It incorporates a spatial attention mechanism-based adaptive token learner to select prominent visual patterns from the image. Moreover, ISA adopts an asymmetric architecture to optimize deployment in resource-constrained environments.
While these methods exhibit promising generalization capabilities to unseen datasets, they rely on a fixed pre-defined text prompt (\textit{e.g.}, ``a photo of $S^*$'') during training. This limitation reduces their ability to handle diverse textual conditions encountered in real-world applications. To address this issue, LinCIR~\cite{lincir} introduces a self-masking projection, which trains a language-only mapping network capable of projecting a given text into a pseudo-token-based embedding by flexibly replacing ``keywords'' (\textit{i.e.}, consecutive adjectives and nouns) in the text with the projected latent embedding of the text. By minimizing the MSE loss between the pseudo-token-based embedding and the projected latent text embedding, the pseudo-tokens of ``keywords'' effectively encapsulate the essential information of the input text. To further bridge the modality gap during inference, LinCIR introduces a random noise addition strategy, adapting the language-only mapping network to handle visual input seamlessly. 

\subsection{Pseudo-triplet-based}
Studies in this category aim to address the ZS-CIR with automatic pseudo-triplet generation methods. 
Existing pseudo-triplet generation methods for ZS-CIR can be categorized into two main classes: LLM-based triplet generation and mask-based triplet generation. 

\subsubsection{LLM-based Triplet Generation.} 
To reduce reliance on manual annotation, several studies~\cite{transagg, hycir, zhang2024magiclens, compodiff, mcl, rtd, pvlf} leverage the advanced logical reasoning capabilities of  LLMs to automatically generate pseudo triplet data. 

\begin{itemize}
    \item \textbf{Image-text-based}. Among these methods, TransAgg~\cite{transagg} is the first to use LLMs for generating pseudo-triplets from a set of image-caption pairs, such as those in the Laion-COCO dataset\footnote{https://laion.ai/blog/laion-coco/}. Specifically, given an image-text pair, it treats the image as the reference image and generates both the modification text and target image caption based on the provided caption, using either a carefully crafted template or an LLM. Drawing inspiration from~\cite{liu2021image}, TransAgg introduces eight types of semantic operations: cardinality, addition, negation, direct addressing, compare\&change, comparative statement, conjunction-based statements, and viewpoint, which guide the modification text generation. It then uses the generated target image caption as a query to retrieve relevant images from the Laion-COCO dataset by calculating the semantic similarity between the target caption and each image's caption. These retrieved images serve as the target images for forming pseudo-triplets, alongside the reference image and generated modification text.
MCL~\cite{mcl} follows a similar pseudo-triplet generation strategy to TransAgg~\cite{transagg}, but instead of using the target image retrieved based on the generated target caption, it directly uses the CLIP feature of the generated target caption as supervision for training the CIR model.
   \item \textbf{Image-based}. Instead of relying on image-text pairs, HyCIR~\cite{hycir} generates pseudo-triplets purely from an unlabeled image dataset, such as COCO~\cite{lin2014microsoft}. The process involves four steps: 1) extracting potential reference-target image pairs; 2) using an image captioning model to generate captions for both images; 3) generating modification text using an LLM based on the two captions; and 4) filtering out triplets with low semantic similarity. Notably, to ensure the generated triplet data is compatible with previous mainstream ZS-CIR methods, such as the textual-inversion-based methods, HyCIR extends the existing Pic2Word method. It introduces an additional training stream that integrates the pseudo-word tokens mapped from the reference image with the modification text, forming a unified text query. This query is then used to supervise both the text query representation and the target image representation using a contrastive loss.
Instead of relying on visual similarity, MagicLens~\cite{zhang2024magiclens} extracts potential reference-target image pairs by mining images from the same webpage, where implicit relationships often exist. It annotates each image with detailed descriptions, including Alt-texts\footnote{https://en.wikipedia.org/wiki/Alt_attribute}, image content annotation (ICA) labels\footnote{https://cloud.google.com/vision/docs/labels}, and captions generated by a large multimodal model, PaLI~\cite{chen2023pali}. Finally, PaLM2~\cite{anil2023palm} is used to generate open-ended modification text for each image pair based on their detailed descriptions. To ensure the generated modification text is logical, techniques such as instruction-following~\cite{chung2024scaling}, few-shot demonstrations~\cite{brown2020language}, and chain-of-thought prompting~\cite{wei2022chain} are employed.
\item \textbf{Text-based}. Inspired by the powerful generation capabilities of diffusion models, Gu et al.~\cite{compodiff} propose a generative approach to construct pseudo triplets. Specifically, they aim to first generate text triplets, structured as~\textless \emph{reference caption, modification text, target caption}\textgreater using two strategies: 1) collecting a large number of captions from existing caption datasets and generating the target caption by substituting keywords in the reference caption, while deriving the modification text based on a randomly sampled pre-defined template; 2) generating the text triplets with an LLM (\textit{i.e.}, OPT-6.7B~\cite{zhang2022opt}), which is fine-tuned with text triplets from existing image editing study InstructPix2Pix~\cite{brooks2023instructpix2pix}. Subsequently, the reference and target images are generated based on their respective captions using a text-to-image generation model, \textit{e.g.}, StableDiffusion~\cite{rombach2022high}. It is worth noting that this work also develops a diffusion-based CIR method, CompoDiff, which can handle various modification cases and enable control over modification strength. Additionally, the textual triplets generated by this work are adopted by a subsequent work~\cite{rtd}, which designs a plug-and-play target-anchored text contrastive learning method for finetuning the CLIP text encoder, thereby improving the performance of textual-inversion-based ZS-CIR.
\end{itemize}


\subsubsection{Mask-based Triplet Generation.}
While the aforementioned LLM-based triplet construction methods efficiently generate large volumes of pseudo-triplet data, they often require excessive computational resources. To address this issue, some researchers have explored mask-based triplet generation strategies~\cite{mti, pm}, which are more resource-efficient. MTI~\cite{mti} is a representative work in this area. Specifically, given an image-caption pair, MTI treats the given image as the target image and randomly masks certain portions of it to derive the corresponding reference image. The provided caption, which encapsulates the predominant content of the image, is then used as the modification text to help reconstruct the masked image back to its original form. In contrast to random masking, PM~\cite{pm} introduces a Class Activation Map (CAM)-guided masking strategy~\cite{chefer2021generic} to better mimic the complementary roles of the reference image and modification text in CIR. Specifically, PM begins by replacing the first noun in the given caption with ``[REMOVE]'' to create the modification text. It then calculates the CAM matrix for the given image, identifying the regions most relevant to the masked noun. These regions are then masked. Unlike MTI, which uses simple color blocks to mask the image, PM replaces the masked regions with corresponding regions from another image within the same batch, ensuring the completeness of the reference image.

\subsection{Training-free}
Given their flexibility and scalability, an increasing body of research is focused on solving ZS-CIR in a training-free manner through modular combinations. These approaches leverage existing models, such as LLMs and VLP models, to address the CIR task without the need for additional model training. Existing methods can be broadly categorized into two branches: one focuses on transforming the CIR task into a caption-to-image retrieval task using powerful LLMs, which can then be handled by the pre-trained encoders of VLP models. The other branch focuses on directly mining the pre-trained common embedding space of VLP models to address the CIR task.

\subsubsection{Task Transformation.} 
Methods in this branch employ LLMs to generate target image captions, which are then used to retrieve the target image via the pre-trained encoders of VLP models. For example, CIReVL~\cite{cirevl} introduces a modular framework where a VLP model generates a description of each reference image. Subsequently, an LLM combines these descriptions with the modification text to infer the target image's caption, which serves as the basis for image retrieval.
Due to the nature of CIR, where the reference image and modification text often involve conflicting semantics, the inferred target image caption may contain concepts that should not appear in the target image. For instance,  given the modification text ``change the dog to a cat", the concept of ``dog" should not appear in the target image.  To address this, GRB~\cite{grb} enhances CIReVL by introducing a Local Concept Re-ranking (LCR) mechanism to ensure the retrieved images contain the correct local concepts. Specifically, GRB performs an initial retrieval based on the inferred target caption, then extracts local concepts that should be present in the target image using an LLM (\textit{e.g.}, ChatGPT4-turbo) from the modification text. To verify the presence of these local concepts, GRB uses LLaVA for Visual Question Answering (VQA) on the top-$K$ retrieved results. The predicted probabilities for outputting text ``yes'' and ``no''  are then used as local scores to re-rank the retrieved images. Moreover, since CIR is inherently a fuzzy retrieval task—where the target image's semantics are not fully captured by the input query—LDRE~\cite{ldre} introduces an LLM-based divergent compositional reasoning approach. This method generates multiple diverse target captions instead of a single one, capturing a wider range of possible semantics within the target image. To complete the ZS-CIR task, LDRE also introduces a divergent caption ensemble technique to combine the CLIP embeddings of these generated captions and retrieve the target image accordingly.


\subsubsection{Pre-trained Space Mining.} 
This branch of training-free methods focuses on leveraging the pre-trained common embedding space of VLP models, such as CLIP and BLIP. These VLP models are primarily optimized using a normalized temperature-scaled cross-entropy loss with cosine similarity, which results in the image and text embeddings residing on a joint hypersphere with the radius determined by the scaling factor (\textit{i.e.}, the temperature parameter). 
WeiMoCIR~\cite{weimocir} directly employs a simple weighted sum to combine the reference image feature, extracted from the visual encoder, and the modification text feature, extracted from the text encoder, to derive the query feature. To improve retrieval performance, WeiMoCIR calculates the score of each candidate image by considering both query-to-image and query-to-caption similarities. It utilizes an MLLM, such as Gemini~\cite{team2023gemini}, to generate multiple captions for each candidate image, providing different perspectives of the image.
In contrast, Slerp~\cite{slerp} applies spherical linear interpolation~\cite{shoemake1985animating} to obtain a fused embedding by calculating intermediate embeddings of the reference image embedding, denoted as $v$, and modification text embedding, denoted as $t$, both derived by VLP encoders. It is formulated as follows: 
\begin{equation}
Slerp(v,t;\alpha) = \frac{\sin((1 - \alpha)\theta)}{\sin(\theta)} \cdot v + \frac{\sin( \alpha)\theta)}{\sin(\theta)} \cdot t,
\end{equation}
where $\theta = \cos^{-1}(v \cdot w)$, and $\alpha$ is a balancing scalar value within the range of [$0$, $1$]. The fused embedding obtained through spherical linear interpolation can be directly used for target image retrieval. 