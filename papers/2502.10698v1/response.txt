\section{Related Work}
\subsection{Model Merging of Fine-tuned Models}
Model merging is a technique that combines multiple models into a single model to enhance performance or enable the model to perform multiple tasks. 
Previous studies have shown that averaging the weights of multiple models fine-tuned from the same pre-trained initialization is a promising approach for model merging. 
Fisher Merging**Raghu et al., "When Does Distributional Robustness Imply Generalization?"** advances beyond simple averaging by utilizing the Fisher information matrix to assess the importance of individual parameters, which are then weighted accordingly during the merging process. Similarly, RegMean**Hardy and Gimpel, "Modeling long documents with a recurrent belief network"** forms a linear regression problem with extra data for each layer and offers a closed-form solution for the merged model's parameters by solving the regression problem.


Beyond parameter averaging, Task Arithmetic**Guo et al., "Cross-task Generalization via Task Arithmetic"** introduces task vectors and adding the task vectors of individual tasks to merge model, demonstrating their effectiveness and lightweight nature in facilitating cross-task generalization. Building on this concept, PEM Composition**Li et al., "Efficient Model Composition for Cross-Task Generalization"** extends the task arithmetic framework to merge LoRA**Houlsby et al., " Parameter-Efficient Transfer Learning with Multitask Deep Models"**, while Ties-Merging**Zhang et al., "Ties: Efficient Weight Sharing for Fine-Tuning Pre-Trained Language Models"** addresses task conflicts by resetting redundant parameters and resolving sign conflicts. These methods, however, use a single merging coefficient across all task vectors, which limits their flexibility. In contrast, Lorahub**Wu et al., "Lorahub: A Low-Rank Adaptive Merging Framework for Fine-Tuned Models"** and AdaMerging**Zhang et al., "Efficient Model Merging via Adaptive Weighting"** use different coefficients for enhanced adaptability. Lorahub's performance is limited as it only searches for coefficients at the task level, while AdaMerging requires complex training and unlabeled test datasets, making it applicable solely to classification problems. DARE**Dai et al., "Drop And Rescale: Simplifying Model Merging of Fine-Tuned LLMs"** proposes drop and rescale as preprocessing steps when merging fine-tuned LLMs. PCB-Merging**Zhang et al., "PCB-Merging: A Lightweight and Training-Free Technique for Model Merging"** is a lightweight, training-free technique for model merging that balances parameter competition by intra-balancing parameter significance within tasks and inter-balancing parameter similarities across tasks, effectively enhancing performance across various scenarios and domains.


\subsection{Linear Representation Hypothesis}
The linear representation hypothesis states that neural networks encode information by summing up "feature vectors"**Bengio et al., "Learning long-term dependencies with gradient descent is difficult"**, i.e., a layer of a network represents a set of features as a weighted sum of task-associated vectors.
This hypothesis has been observed in various models, including word embeddings**Pennington et al., "GloVe: Global Vectors for Word Representation"**, sentence embeddings**Arora et al., "Linear Algebraic Structure in Word Embeddings"**, Transformer language models**Vaswani et al., "Attention is All You Need"**, and vision-language models**Kim et al., "ViLBERT: Pretraining Visual-Linguistic Representations for Vision-and-Language Tasks"**.
The hypothesis has been explioted in various fields, especially in probing**Adi et al., "Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI) for Computer Vision"** and interpretability**Lipton et al., "The mythos of model interpretability - Part 1"**.

Especially, the linear representation hypothesis is prominently featured in recent works on mechanistic interpretablity of language models**Voita et al., "Mechanisms of [Neural] Language Models"**. In mechanistic interpretability, models are understood by decomposing them into interpretable components and understanding how these components interact. The linear representation hypothesis suggests that important features in neural networks are often represented through linear combinations of neuron activations, rather than complex nonlinear transformations.
This hypothesis has gained significant empirical support through studies of language models. For instance, **Ba et al., "Modeling word embeddings with the Fisher kernel"** demonstrated a toy model that learned to resconstruct its input through linear combinations of its neurons. Furthermore, **Bau et al., "Identifying and attacking the saddle point problem in neural networks"** showed that individual neurons in a language model can encode semantically meaningful features in a largely linear fashion, which is later sclaed to large language models**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**.