\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
% \usepackage{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}

\usepackage{hyperref}
\usepackage[normalem]{ulem}
\usepackage{algorithm}


% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{pgfplots}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\newcommand{\TM}{\mathbf{M}_{i}}
\newcommand{\TP}{\mathbf{P}_{i}}
\newcommand{\MM}{\mathbf{M}}
\newcommand{\bfu}{\mathbf{u}}
\newcommand{\bfv}{\mathbf{v}}
\newcommand{\bfb}{\mathbf{b}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\SigM}{\mathbf{\Sigma}}
\newcommand{\UM}{\mathbf{U}}
\newcommand{\VM}{\mathbf{V}}
\usepackage{pifont}  % Required for \ding command
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{{\method}}{{SFTM}}
% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\usepackage{enumitem}
\usepackage{multirow}

\title{Superpose Singular Features for Model Merging}




\author{%
  Haiquan Qiu, You Wu, Quanming Yao \\
  Department of Electronic Engineering\\
  Tsinghua University\\
  Beijing, China \\
}


\begin{document}


\maketitle


\begin{abstract}
Model merging is a critical technique for combining the capabilities of multiple fine-tuned models without requiring additional training. While existing methods treat parameters as vectors, they overlook the intrinsic structure of linear transformation matrices - the core components that comprise the majority of model parameters. These matrices are fundamental to neural networks, mapping input representations to output features through linear combinations. Motivated by the linear representation hypothesis, we introduce task matrix and propose to Superpose Features from Task Matrix ({\method}), a novel approach that superposes features from individual task models into a merged model. {\method} employs singular value decomposition to identify feature bases of linear transformation matrices and solves a linear system to optimally combine them while preserving input-output mappings from individual task models. Extensive experiments on vision transformers and language models demonstrate that {\method} consistently outperforms existing methods, achieving superior performance and enhanced out-of-distribution generalization.
\end{abstract}


\section{Introduction}

Pre-trained models have become a cornerstone of modern deep learning, providing a powerful foundation for a wide range of tasks~\citep{zhuang2020comprehensive,bommasani2021opportunities}.
To serve different tasks, fine-tuning the pre-trained models separately on different tasks is a common practice~\citep{kenton2019bert,hu2021lora}.
However, maintaining multiple fine-tuned models for different tasks can be cumbersome and resource-intensive, especially as the number of tasks and models continues to grow.
Multitask learning~\citep{sanh2021multitask,raffel2020exploring} is a promising approach to address this issue by training a single model on multiple tasks simultaneously, but it often requires a large amount of data and computational resources to achieve optimal performance~\citep{pilehvar2018wic,phang2018sentence,fifty2021efficiently}.


Recently, 
model merging~\citep{matena2022merging,jin2022dataless,ilharco2022editing,yadav2024ties} offers a solution to this problem by consolidating multiple models into a single model that can perform well on 
multiple tasks effectively. 
This approach eliminates the need for maintaining multiple separate models, 
reduces storage and computational requirements, 
and potentially enables knowledge transfer between tasks.
A foundational approach to model merging is parameter averaging~\citep{choshen2022fusing,ilharco2022patching,wortsman2022model}, which simply takes the mean of corresponding parameters across models. This evolved into weighted averaging methods like Fisher Merging~\citep{matena2022merging} and RegMean~\citep{jin2022dataless} that assign different weights to parameters based on their importance. A key advancement came with the introduction of task vectors~\citep{ilharco2022editing} which represent the difference between fine-tuned and pre-trained weights and enable lightweight cross-task generalization. Various techniques~\citep{huang2023lorahub,yang2023adamerging,yadav2024ties,yu2024language,du2024parameter} were subsequently developed to handle task conflicts by resetting redundant parameters, resolving sign conflicts, dropping and rescaling parameters, and managing parameter competition between tasks. However, these methods often rely on heuristic rules for merging parameters while overlooking the intrinsic structure of parameters.


Our method is motivated by the linear representation hypothesis~\citep{mikolov2013linguistic,arora2016latent,olah2020zoom}. The linear representation hypothesis states that the representations of deep neural networks can be decomposed into combinations of feature vectors. This linearity enables neural networks to represent complex concepts through these combinations. Linear transformations (e.g., in linear and convolution layers) play a crucial role in this process, as they can activate specific features by measuring the similarity between a representation and a feature vector (via the inner product with the row vectors of the transformation matrix) and extract new features through linear combinations (via the weighted sum of the column vectors of the transformation matrix). These properties have been leveraged in recent years to interpret LLMs~\citep{elhage2022toy,bricken2023monosemanticity,templeton2024scaling} and design more efficient fine-tuning techniques~\citep{hu2021lora,zhang2023adalora}. Consequently, in model merging, it is essential that the linear transformations in the merged model preserve the input-output mappings of those in the individual models for effective feature activation and extraction.

In this paper, we propose a method for merging linear transformation matrices while preserving the input-output mappings of individual models' linear transformations. Our approach leverages Singular Value Decomposition (SVD) to merge matrices, treating left singular vectors as basis for output representations and right singular vectors as basis for input representations. The goal is to ensure the merged matrices preserve the directions of output features from original transformations when applied to corresponding input features across tasks. We show this problem reduces to solving a system of linear equations, enabling model merging in singular decomposition space while maintaining feature directions from various tasks. We validate our method on multiple benchmark datasets spanning vision and natural language processing tasks. The results demonstrate that our method consistently outperforms existing model merging techniques.


\paragraph{Notations} Throughout this paper, we use bold uppercase letters (e.g., $\mathbf{X}$) to denote matrices, bold lowercase letters (e.g., $\mathbf{x}$) to denote vectors, and regular lowercase letters (e.g., $x$) to denote scalars. For a matrix $\mathbf{X}$, its transpose is denoted as $\mathbf{X}^\top$. We denote task $i$ as $\mathsf{T}_i$ and its corresponding model parameters as $\theta_i$. For linear transformations, a linear transformation matrix in the model trained on task $i$ is represented as $\TP$. The subscript $_{\text{pre}}$ indicates pre-trained parameters or matrices - for example, $\theta_{\text{pre}}$ and $\mathbf{P}_{\text{pre}}$ denote the parameters and linear transformation matrices of the pre-trained model, respectively.

\section{Related Work}


\subsection{Model Merging of Fine-tuned Models}
Model merging is a technique that combines multiple models into a single model to enhance performance or enable the model to perform multiple tasks. 
Previous studies have shown that averaging the weights of multiple models fine-tuned from the same pre-trained initialization is a promising approach for model merging. 
Fisher Merging~\citep{matena2022merging} advances beyond simple averaging by utilizing the Fisher information matrix to assess the importance of individual parameters, which are then weighted accordingly during the merging process. Similarly, RegMean~\citep{jin2022dataless} forms a linear regression problem with extra data for each layer and offers a closed-form solution for the merged model's parameters by solving the regression problem.


Beyond parameter averaging, Task Arithmetic~\citep{ilharco2022editing} introduces task vectors and adding the task vectors of individual tasks to merge model, demonstrating their effectiveness and lightweight nature in facilitating cross-task generalization. Building on this concept, PEM Composition~\citep{zhang2023composing} extends the task arithmetic framework to merge LoRA~\citep{hu2021lora}, while Ties-Merging~\citep{yadav2024ties} addresses task conflicts by resetting redundant parameters and resolving sign conflicts. These methods, however, use a single merging coefficient across all task vectors, which limits their flexibility. In contrast, Lorahub~\citep{huang2023lorahub} and AdaMerging~\citep{yang2023adamerging} use different coefficients for enhanced adaptability. Lorahub's performance is limited as it only searches for coefficients at the task level, while AdaMerging requires complex training and unlabeled test datasets, making it applicable solely to classification problems. DARE~\citep{yu2024language} proposes drop and rescale as preprocessing steps when merging fine-tuned LLMs. PCB-Merging~\citep{du2024parameter} is a lightweight, training-free technique for model merging that balances parameter competition by intra-balancing parameter significance within tasks and inter-balancing parameter similarities across tasks, effectively enhancing performance across various scenarios and domains.


\subsection{Linear Representation Hypothesis}
The linear representation hypothesis states that neural networks encode information by summing up "feature vectors"~\citep{mikolov2013linguistic,arora2016latent,olah2020zoom}, i.e., a layer of a network represents a set of features as a weighted sum of 
task-associated vectors.
This hypothesis has been observed in various models, including word embeddings~\citep{mikolov2013linguistic,conneau2017word}, sentence embeddings~\citep{bowman2015generating}, Transformer language models~\citep{meng2022locating,hendel2023context}, and vision-language models~\citep{trager2023linear,perera2023prompt}.
The hypothesis has been explioted in various fields, especially in probing~\citep{alain2018understanding, belinkov2022probing} and interpretability~\citep{nostalgebraist2020logitlens,elhage2022toy,bricken2023monosemanticity,gao2024scaling}.

Especially, the linear representation hypothesis is prominently featured in recent works on mechanistic interpretablity of language models~\citep{olsson2022context,elhage2022toy,bricken2023monosemanticity,templeton2024scaling}. In mechanistic interpretability, models are understood by decomposing them into interpretable components and understanding how these components interact. The linear representation hypothesis suggests that important features in neural networks are often represented through linear combinations of neuron activations, rather than complex nonlinear transformations.
This hypothesis has gained significant empirical support through studies of language models. For instance, \citep{elhage2022toy} demonstrated a toy model that learned to resconstruct its input through linear combinations of its neurons. Furthermore, \citet{bricken2023monosemanticity} showed that individual neurons in a language model can encode semantically meaningful features in a largely linear fashion, which is later sclaed to large language models~\citep{templeton2024scaling}.





\section{Method}


\subsection{Problem Formulation}
\label{ssec:setup_and_motivation}


% \footnote{$\surd$ +qm+ re-write this as task formulation.
% 	try to avoid giving many refs here.
% 	\label{ft:1}}
We start with a set of tasks \(\{\mathsf{T}_{1}, \ldots, \mathsf{T}_{T}\}\) and various pre-trained models. 
The objective is to fine-tune these models either by updating all parameters or using 
% \footnote{$\surd$ [LoRA results will be shown in \cref{tab:lora}] +qm+ just ``fine-tuning'' no need to be
% 	parameter-efficient?
% 	In experiments, it seems that you do not merge multiple LoRA into a single one.}
parameter-efficient fine-tuning (PEFT) methods. 
The goal of model merging is to combine multiple fine-tuned models \(\{\theta_1, \ldots, \theta_n\}\) into a single model \(\theta_M\) that can perform all tasks \(\{\mathsf{T}_{1}, \ldots, \mathsf{T}_{T}\}\) effectively without requiring access to training data.
While existing methods treat model parameters as vectors to be merged, we focus on the crucial role of linear transformations, which comprise the majority of parameters and computations in neural networks. These transformations are essential as they project data into spaces where important features become more apparent, making information processing more efficient. To formally analyze these transformations during merging, we introduce the \textit{Task Matrix} in \cref{def:task_mat}.


\begin{definition}[Task Matrix]\label{def:task_mat}
The \textit{task matrix} is defined by
$\TM = \TP - \mathbf{P}_{\text{pre}} \in \mathbb{R}^{m\times n},$ where $\TP$ is a linear transformation matrix for model of task $\mathsf{T}_i$, $\mathbf{P}_{\text{pre}}$ is the linear transformation matrix for the pre-trained model.
% In the task matrix $\TM$, the index $i$ is the task index, 
% \footnote{$\surd$ [merged task matrix $\MM$ and task matrix $\TM$] +qm+ for simplicity,
% 	can you drop sub-script $j$ in the motivation
% 	and method part.
% 	It seem that it is an implementation detail.}
% 
\end{definition}

\begin{remark}
    \it
	Existing merging methods based on task vectors neglect the internal structure of linear transformation in neural networks, and merge parameters as vector, i.e., $\text{vec}(\mathbf{P}_j) = \text{vec}(\mathbf{P}_{\text{pre}}) + \gamma \text{vec}(\MM)$. These methods employ various heuristic rules to merge parameters, which requires many hyperparameter tuning.
\end{remark}

To merge models, preserving the input-output mappings of linear transformations is essential for maintaining model capabilities. Our method focuses specifically on merging linear transformations in a way that retains their key input-output mappings through singular value decomposition, rather than treating all parameters equally as in previous approaches.
Specifically, we focus on merging task matrices $\TM,i=1\ldots T$ into a single merged matrix $\MM \in \mathbb{R}^{m\times n}$.
For a input feature $\mathbf{r}\in\mathbb{R}^n$, 
we aim to ensure that the output of the merged model $\MM \mathbf{r}$ maintains the same directional features as the outputs $\TM \mathbf{r}$ from the original task-specific models, i.e., the merged model preserves the input-output mappings of the individual models. 
The final merged model is then constructed by adding the merged task matrix $\MM$ to the pre-trained model with a scaling factor $\gamma$: $\mathbf{P}_j = \mathbf{P}_{\text{pre}} + \gamma \MM$.



\subsection{Merge Task Matrices to Superpose Singular Features}




\paragraph{Identify Basis for Preserving Mappings}


To keep the directional features of the task matrices $\TM \mathbf{r}$ during merging, we first need to identify some basis features that is representative for the input and output representations. Singular value decomposition (SVD) of task matrices provides a natural way to obtain the basis features.
Given the decomposition $\TM=\sum_{k=1}^{r_i} \sigma_{i}^{(k)} \bfu_{i}^{(k)} \bfv_{i}^{(k)\top}$ where $\sigma_{i}^{(k)}$ is the $k$-th singular value, $\bfu_{i}^{(k)}\in\mathbb{R}^{m\times 1}$ and $\bfv_{i}^{(k)}\in\mathbb{R}^{n\times 1}$ are the $k$-th left and right singular vectors of $\TM$, respectively. Therefore, the right singular vectors $\bfv_{i}^{(k)}$ form a basis for input representations, while the left singular vectors $\bfu_{i}^{(k)}$ form a basis for output representations. We named these basis features as \textit{singular features} in our paper.

Except form the basis, singular features have several other properties that make them suitable for preserving input-output mappings during merging. First, the orthogonality of singular vectors within each task minimizes interference between features, enabling linear superposition of features in accordance with the linear representation hypothesis. Second, the decomposition reveals the model's information processing mechanism - using inner products to detect input features and weighted sums to construct outputs. We leave the details of the good properties of singular features to the appendix.

\paragraph{Merge Task Matrices in Singular Space} Since we have the singular vectors as the basis of the input and output representations for linear transformation $\TM$, the merged matrix $\MM$ should be able to keep the directions of output features from original transformations when applied to corresponding input features across tasks. To accomplish this, we can merge the task matrices $\TM$ in their singular spaces:
% \footnote{+qm+ is $\MM$ full rank?}
\begin{equation}\label{eq:merge}
\MM = 
\sum\nolimits_{i=1}^T 
\sum\nolimits_{k=1}^{r_{i}} \alpha_{i}^{(k)} \sigma_{i}^{(k)} \bfu_{i}^{(k)} \bfv_{i}^{(k) \top},
\end{equation}
where $\alpha_{i}^{(k)}$ are the merging weights. 
To preserve feature directions during merging, we require that when the merged matrix $\MM$ transforms an input representation, the output should maintain the same directional features as those produced by the individual task matrices $\TM$. 
% \footnote{+qm+ $\MM$ contains $\TM$,
% 	you need to show this is not a trivial requirement.}
This leads to the following objective:
\begin{equation}\label{eq:obj}
    \left<\bfu_{i}^{(k)}, \MM \bfv_{i}^{(k)} \right> = \left<\bfu_{i}^{(k)}, \TM \bfv_{i}^{(k)}\right>, \quad \forall i,j,k.
\end{equation}
where $\left<\cdot,\cdot\right>$ is the inner product.
In \eqref{eq:obj}, the inner product measures how much of the transformed vector $\MM \bfv_{i}^{(k)}$ aligns with the direction of $\bfu_{i}^{(k)}$, which is the direction of $\TM \bfv_{i}^{(k)}$. This ensures that when the merged matrix $\MM$ acts on an input feature vector $\bfv_{i}^{(k)}$, it maintains the same directional component along $\bfu_{i}^{(k)}$ as the original task matrix $\TM$. While it would be ideal to fully preserve the features by requiring $\MM \bfv_{i}^{(k)} = \TM \bfv_{i}^{(k)}$ for all tasks, this is generally impossible due to the resulting overdetermined system of equations.
Instead, we relax this constraint to \eqref{eq:obj}, which preserves the direction of corresponding output features while allowing other features to be superposed linearly, aligning with the linear representation hypothesis. 


\paragraph{Solving Linear Systems to Merge Task Matrices}To solve \eqref{eq:obj}, we rewrite it as a linear system for all tasks in $\{\mathsf{T}_1,\ldots, \mathsf{T}_T\}$:
% \footnote{
% 	+qm+ how about left hand side? $\sigma_{i'}^{(k')}$?
% 	why not just $\alpha$?
% 	$\surd$ [$\sigma$ is the magnitude of the direction] +qm+ do not understand why have $\sigma$ on the right hand side.}
\begin{align}\label{eq:linear}
    \begin{split}
         & \bfu_{i}^{(k)\top} \!\!\left( \sum_{i'=1}^T \sum_{k'=1}^{r_{i}} \!\alpha_{i'}^{(k')} \sigma_{i'}^{(k')} \bfu_{i'}^{(k')} \bfv_{i'}^{(k') \top}\!\! \right)\! \bfv_{i}^{(k)} \!=\! \sigma_{i}^{(k)}  \\
         \Leftrightarrow &  \sum_{i'=1}^T \sum_{k'=1}^{r_{i}}  \!\!\sigma_{i'}^{(k')} \bfu_{i}^{(k)\top} \bfu_{i'}^{(k')} \bfv_{i'}^{(k') \top} \bfv_{i}^{(k)} \alpha_{i'}^{(k')} \!=\! \sigma_{i}^{(k)}.
    \end{split}
\end{align}
For various tasks $\mathsf{T}_i$ and singular values, \eqref{eq:linear} forms a linear system with $r=\sum_{i=1}^{T} r_{i}$ variables $\alpha_{i'}^{(k')}$ and $r$ equations. The coefficient matrix of this linear system is the element-wise product of three matrices:
\begin{itemize}[leftmargin=*]
    \item $\SigM = \mathbf{1}\otimes [\sigma_{i'}^{(k')}]_{i',k'}\in \Real^{r\times r}$, where $\mathbf{1}\in \Real^{r\times 1}$, $\otimes$ is the Kronecker product, and $[\sigma_{i'}^{(k')}]_{i',k'}$ is a row vector of singular values;
    \item $\UM = [\bfu_{i}^{(k)\top} \bfu_{i'}^{(k')}]_{i,k,i',k'}\in \Real^{r\times r}$, where enumerating over $i,k$ forms the row index and enumerating over $i',k'$ forms the column index;
    \item $\VM = [\bfv_{i'}^{(k') \top} \bfv_{i}^{(k)}]_{i',k',i,k}\in \Real^{r\times r}$, where enumerating over $i,k$ forms the row index and enumerating over $i',k'$ forms the column index.
\end{itemize}
Then, the coefficient matrix of the linear system is $\SigM \circ \UM \circ \VM$, where $\circ$ is the element-wise product. The linear equation \eqref{eq:linear} can be written as
\begin{equation}\label{eq:linear-system}
    \SigM \circ \UM \circ \VM \boldsymbol{\alpha} = \boldsymbol{\sigma},
\end{equation}
where $\boldsymbol{\alpha}$ is the vector of variables $\alpha_{i'}^{(k')}$ and $\boldsymbol{\sigma}$ is the vector of singular values $\sigma_{i}^{(k)}$, with consistent indexing order over $i,k,i',k'$ across $\SigM$, $\UM$ and $\VM$. 
By solving the linear system in \eqref{eq:linear-system}, we obtain the optimal weights $\boldsymbol{\alpha}^*$ for merging task matrices according to \eqref{eq:merge}. 
The merged linear transformation matrix $\mathbf{P}$ is then obtained by adding the merged task matrix $\MM$ to the pretrained matrix $\mathbf{P}_{\text{pre}}$ with a scaling factor $\gamma$: $\mathbf{P} = \mathbf{P}_{\text{pre}} + \gamma \MM$. The algorithm for merging task matrices is summarized in \cref{alg:ssf}.

\begin{algorithm}[tb]
    \caption{Merging task matrices: $\mathsf{\method}(\TM, i=1,\ldots,T)$}
    \label{alg:ssf}
 \begin{algorithmic}
    \STATE {\bfseries Input:} task matrices $\TM,i=1,\ldots,T$;
    % \STATE Obtain task matrices $\TM = \text{TopK}(\mathbf{P}_i - \mathbf{P}_{\text{pre}}, \eta)$ for $T$ tasks;
    \STATE Apply SVD to $\TM$ to obtain $\sigma_{i}^{(k)}$, $\bfu_{i}^{(k)}$, and $\bfv_{i}^{(k)}$;
    \STATE Prepare $\SigM$, $\UM$, and $\VM$;
    \STATE Solve $\SigM \circ \UM \circ \VM \boldsymbol{\alpha} = \boldsymbol{\sigma}$ to obtain $\boldsymbol{\alpha}^*$;
    \STATE Obtain the merged task matrix $\MM$ by \eqref{eq:merge};
    % \STATE Obtain the merged matrix $\mathbf{P}_{j} = \mathbf{P}_{\text{pre}} + \gamma \MM$.
    \STATE {\bfseries Return:} merged task matrices $\MM$.
 \end{algorithmic}
\end{algorithm}


\subsection{Complete Algorithm}

Except the linear transformation matrices, there are other parameters in the model that need to be merged, such as biases, normalization parameters, embeddings, and convolutional layers. 
For biases and embeddings, we merge them with task arithmetic, i.e., adding them task vectors together. For normalization parameters, we merge them by averaging the normalization parameters of the individual tasks because the normalization can be seen as a linear transformation with diagonal matrix. For convolutional layers, we convert them into linear transformations by reshaping the convolutional kernels into a matrix and then merge them with the same method as the task matrices.

We also follow Ties-Merging~\citep{yadav2024ties} and apply a trimming step. This involves keeping only the top $\eta$ parameters by magnitude while setting the remaining parameters to zero. This preprocessing step helps reduce noise and focus on the most significant parameters during merging. We present the complete algorithm for merging models in \cref{alg:ssf-complete}.


\begin{algorithm}[tb]
    \caption{Complete Algorithm for Model Merging}
    \label{alg:ssf-complete}
 \begin{algorithmic}[1]
    \STATE {\bfseries Input:} finetuned models $\{\theta_1,\ldots,\theta_T\}$, pretrained model $\theta_{\text{pre}}$
    \STATE {\bfseries Parameters:} sparsity ratio $\eta$, scaling factor $\gamma$
    \FOR{each linear transformation layer}
        \STATE Extract task matrices $\TM = \TP - \mathbf{P}_{\text{pre}}$ for $i=1,\ldots,T$
        \STATE Keep top $\eta$ parameters by magnitude in each $\TM$
        \STATE $\MM \leftarrow \mathsf{\method}(\TM, i=1,\ldots,T)$ \COMMENT{Algorithm \ref{alg:ssf}}
        \STATE Merged matrix $\mathbf{P} = \mathbf{P}_{\text{pre}} + \gamma \MM$
    \ENDFOR
    \FOR{each bias, embedding, normalization layer}
        \STATE Extract task vectors $\tau_i = \theta_i - \theta_{\text{pre}}$ for $i=1,\ldots,T$
        \STATE Keep top $\eta$ parameters by magnitude in each $\tau_i$
        \IF{normalization layer}
            \STATE Merged parameters $\leftarrow \mathsf{mean}(\{\tau_1,\ldots,\tau_T\})$
        \ELSE
            \STATE Merged parameters $\leftarrow \gamma \sum_{i=1}^T \tau_i$
        \ENDIF
    \ENDFOR
    \STATE {\bfseries Return:} merged model parameters
 \end{algorithmic}
\end{algorithm}

\subsection{Discussion}\label{sec:discussion}

\paragraph{Time Complexity} Because there are $T$ tasks matrices of size $m \times n$, SVD of these matrix takes the time complexity $O(T m n^2)$. To merge $T$ task matrices, the linear system has $r$ equations and variables to solve, which takes the time complexity $O(r^3)$. Therefore, the overall time complexity of {\method} for merging $T$ task matrices is $O(T m n^2 + r^3)$. See \cref{ssec:additional_results} for the time consumption of {\method} for various models.


\paragraph{Merging Task Matrices or Fine-tuned Matrices?}
In this paper, our focus is on merging task matrices instead of fine-tuning matrices. We have discovered that fine-tuned linear transformation matrices tend to have more shared features across tasks.
Therefore, merging the fine-tuned linear transformation matrix is not as efficient as task matrix because certain overlapping direction of singular vectors correspond to these common features. On the contrary, the singular vectors of the task matrix contain features that are more specific to individual tasks, making it a more effective approach for merging. See the results of merging fine-tuned matrices in \cref{ssec:additional_results}.


\section{Experiments}

\subsection{Experimental Setup}

\paragraph{Evaluation Setup.} We evaluate {\method} across diverse fine-tuning methods, tasks, model architectures, and model size. For the fully fine-tuning, we evaluate vision tasks on ViT and NLP tasks on T5 following protocols of \citet{ilharco2022editing} and \citet{yadav2024ties}, respectively. For parameter-efficient fine-tuning (PEFT), we evaluate LoRA on GPT-2 for NLP tasks since it uses linear transformation matrices through low-rank adaptation, which aligns well with our approach, rather than vector parameters like IA3~\citep{liu2022few} in previous research.
To evaluate {\method} when model size scales up, we follow the setup of \citet{du2024parameter} to merge LLMs. Additionally, to evaluate the out-of-distribution robustness, we follow \citet{yadav2024ties} to test it on NLP tasks.

\paragraph{Baseline Methods.}
We compare {\method} against four established model merging approaches: (1) \textbf{Averaging} \citep{choshen2022fusing,wortsman2022model}, which computes the element-wise mean of individual models; (2) \textbf{Task Arithmetic} \citep{ilharco2022editing}, which merges by scaling and adding task vectors to the initial model;
(3) \textbf{Fisher Merging}~\citep{matena2022merging}, which approximates Fisher Information Matrix to weight parameters based on their importance for each task and combines the weighted parameters into the final merged model;
(4) \textbf{RegMean}~\citep{jin2022dataless}, which computes a closed-form solution to a least-squares regression problem that aims to minimize the distance between the merged model's activations and the individual models' activations.
(5) \textbf{Ties-Merging}, which enhances merging by eliminating redundant parameters and resolving sign conflicts; and (6) \textbf{PCB-Merging}, which balances parameter competition through intra-task significance and inter-task similarity analysis. We also report results from individual \textbf{fine-tuned models} and a \textbf{pre-trained model} on all tasks. 

\subsection{Results}

\begin{table*}[th]
    \centering
    \belowrulesep=0pt
    \aboverulesep=0pt
    \setlength{\tabcolsep}{2.0pt}
    \small
    \caption{\label{tab:app_vit_base} Test set performance when merging ViT-B/32 and ViT-L/14 models on 8 vision tasks.}\label{tab:vit}
    \begin{tabular}{c|r|c|cccccccc}
    
    \toprule
    & \textbf{Task($\rightarrow$)} &    & \multicolumn{8}{c}{\textbf{Test Set Performance}} \\ 
    \multirow{-2}{*}{\textbf{Model}}  & \textbf{Method($\downarrow$)} &  \multirow{-2}{*}{\textbf{Average}}  & SUN397  &  Cars  &  RESISC45  &  EuroSAT  &  SVHN  &  GTSRB  &  MNIST & DTD \\
    \midrule
    \multirow{7}{*}{{ViT-B/32}} &  {Pre-trained}  & 46.3 & 61.7 & 54.7 & 58.5 & 51.2 & 29.1 & 27.4 & 45.6 & 42.1 \\
     & {Fine-tuned}  & 90.5  &  75.3  &  77.7  &  96.1  &  99.7  &  97.5  &  98.7  &  99.7  &  79.4 \\
    \cmidrule{2-11}
   &  {Averaging} & 65.8  &  65.3  &  63.4  &  71.4  &  71.7  &  64.2  &  52.8  &  87.5  &  50.1 \\
   & {Fisher Merging} & 68.3 & \textbf{68.6} & \textbf{69.2} & 70.7 & 66.4 & 72.9 & 51.1 & 87.9 & 59.9 \\
   & {RegMean} & 71.8 & 65.3 & 63.5 & 75.6 & 78.6 & 78.1 & 67.4 & 93.7 & 52.0 \\
    &  {Task Arithmetic} & 70.1 &  63.8  &  62.1  &  72.0  &  77.6  &  74.4  &  65.1  &  94.0  &  52.2 \\
      & {Ties-Merging} & 73.6 & 64.8 & 62.9 & 74.3 & 78.9 & 83.1 & 71.4 & 97.6 & 56.2 \\
      & {PCB-Merging} & \underline{76.3} & 66.7 & 65.5 & 78.5 & 79.3 & \textbf{86.4} & \textbf{77.1} & \textbf{98.2} & 59.1 \\
      & \textbf{{\method} (ours)} & \textbf{77.4}    & 65.4	& 62.0	& \textbf{81.4}	& \textbf{90.4}	& {84.1}	& \textbf{77.1}	& 95.6	& \textbf{62.9}  \\
    \midrule
    \multirow{7}{*}{{ViT-L/14}} &  {Pre-trained} & 64.1 & 68.2 & 76.4 & 69.7 & 64.7 & 60.4 & 49.4 & 67.9 & 56.3 \\ 
 & {Fine-tuned} & 94.2  &  82.3  &  92.4  &  97.4  &  100  &  98.1  &  99.2  &  99.7  &  84.1  \\
\cmidrule{2-11}
 & {Averaging} & 79.6  &  72.1  &  81.6  &  82.6  &  91.9  &  78.2  &  70.7  &  97.1  &  62.8  \\
 & Fisher Merging & 82.2 &  69.2  &  \textbf{88.6}  &  87.5  &  93.5  &  80.6  &  74.8  &  93.3  &  70.0  \\
 & {RegMean} & 83.7 & 73.3 & 81.8 & 86.1 & \textbf{97.0} & 88.0 & 84.2 & 98.5 & 60.8 \\
 & {Task Arithmetic} & 84.5 &  74.1  &  82.1  &  86.7  &  93.8  &  87.9  &  86.8  &  98.9  &  65.6  \\
  & {Ties-Merging} & {86.0 } &  76.5  &  85.0  &  89.4  &  95.9  &  90.3  &  83.3  &  99.0  &  68.8  \\
  & {PCB-Merging} & \underline{87.5} &  \textbf{76.8}  &  86.2  &  89.4  &  96.5  &  88.3  &  91.0  &  98.6  &  \textbf{73.6}  \\
    & \textbf{{\method} (ours)} & \textbf{87.8}	& {75.4}	& 85.8	& \textbf{90.3}	& {96.6}	& \textbf{91.7}	& \textbf{91.2}	& \textbf{99.2}	& {72.4}  \\
    \bottomrule
\end{tabular}
% \vspace{10pt}
\end{table*}

\paragraph{Fully Fine-tuned Vision Model: ViT Merging}
We evaluate {\method} on vision tasks using two CLIP models \citep{radford2021learning} with ViT-B/32 and ViT-L/14 architectures \citep{dosovitskiy2020image} as visual encoders. We use the released checkpoints from \citet{ilharco2022editing} that were fine-tuned on eight diverse classification tasks spanning remote sensing, traffic signs, and satellite imagery domains. These tasks include: Cars \citep{cars}, DTD \citep{dtd}, EuroSAT \citep{eurosat}, GTSRB \citep{gtsrb}, MNIST \citep{lecun1998mnist}, RESISC45 \citep{cheng2017remote}, SUN397 \citep{sun397}, and SVHN \citep{svhn}. 
For all experiments, we keep the text encoder fixed and only merge the parameters of visual encoders.


As shown in 
\cref{tab:vit}, 
{\method} achieves state-of-the-art performance when merging fully fine-tuned ViT models, outperforming existing methods by 1.1\% and 0.3\% on average across 8 tasks for ViT-B/32 and ViT-L/14 architectures respectively. For ViT-B/32, {\method} shows particularly strong performance on RESISC45 and EuroSAT, with 2.9\% and 11.1\% improvements over the best baseline. For ViT-L/14, {\method} maintains high performance across all tasks, demonstrating effective preservation of task-specific features during merging.

\begin{table*}[h!]
    \centering
    \belowrulesep=0pt
    \aboverulesep=0pt
    \small
    \caption{Merge fully-fine-tuned T5-base model with different methods}
    \label{tab:t5}
    \small
    \begin{tabular}{c|c|ccccccc}
        \toprule
        &       & \multicolumn{7}{c}{\textbf{Test Set Performance}} \\
        \multirow{-2}{*}{\textbf{Method}} & \multirow{-2}{*}{\textbf{Average}} & paws & qasc & quartz & story\_cloze & wiki\_qa & winogrande & wsc \\
        \midrule
        Pre-trained & 53.5 & 49.9 & 35.8 & 53.3 & 48.1 & 76.2 & 50.0 & 61.1 \\
        Fine-tune & 81.0 & 91.4 & 95.5 & 79.1 & 79.6 & 95.2 & 62.8 & 63.6 \\
        \midrule
        Average & 60.2 & 55.2 & 57.5 & 55.2 & 49.4 & 91.1 & 50.2 & 62.5 \\
        Fisher Merging & 68.2 & 67.9 & 84.4 & 63.5 & 57.1 & 90.1 & 54.2 & 60.8 \\
        RegMean & 71.5 & 76.2 & 92.8 & 62.6 & 63.6 & 89.4 & 57.4 & 58.3 \\
        Task Arithmetic & 71.6 & 71.1 & 81.4 & 62.1 & 77.1 & 95.0 & 57.4 & 56.9 \\
        Ties-Merging & 71.5 & 79.6 & 86.9 & 67.9 & 72.8 & 78.9 & 58.5 & 55.9 \\
        PCB-Merging & \underline{72.3} & 71.5 & 91.7 & 66.8 & 62.7 & 92.8 & 57.1 & 63.3 \\
        \textbf{{\method} (ours)} &  \textbf{73.8} & 78.2 & 89.4 & 61.7 & 74.9 & 94.6 & 55.8 & 61.7 \\
        \bottomrule
    \end{tabular}
\end{table*}



\paragraph{Fully Fine-tuned NLP Model: T5 Merging}
For NLP experiments, we evaluate on T5-base \citep{colin2020exploring}, an encoder-decoder transformer \citep{vaswani2017attention} pre-trained with masked language modeling. We finetune T5-base on seven diverse tasks spanning question answering (QASC \citep{khot2020qasc}, WikiQA \citep{yang-etal-2015-wikiqa}, QuaRTz \citep{tafjord2019quartz}), paraphrase identification (PAWS \citep{paws2019naacl}), sentence completion (Story Cloze \citep{sharma2018tackling}), and coreference resolution (Winogrande \citep{sakaguchi2020winogrande}, WSC \citep{wsc}). We use the code from \citet{yadav2024ties} to finetune the model on these tasks.
To reduce variance and ensure reliable evaluation, we report the experimental results of the average performance over different templates for each task.


As shown in \cref{tab:t5}, {\method} achieves state-of-the-art performance when merging T5-base models, outperforming existing methods by $1.5\%$ on average across 7 tasks. {\method} shows particularly strong performance on PAWS and Story Cloze, with improvements of $6.7\%$ and $12.2\%$ respectively over PCB-Merging, the previous best method. For Story Cloze, {\method} shows comparable performance to Task Arithmetic, while maintaining competitive performance on other tasks.


\paragraph{PEFT of NLP Model: LORA Adapters Merging}
For PEFT experiments, we evaluate GPT-2 Medium model on LoRA adapters \citep{hu2021lora}, which are task-specific adapters that are fine-tuned on NLP task datasets. These tasks  include converting tables (E2E\citep{novikova2017e2e}), knowledge graph (WebNLG\citep{gardent2017webnlg}) and structured data (DART\citep{nan2020dart}) to natural language. We use the released checkpoints from \citet{hu2021lora} and evaluate the performance of the merged model on these datasets. 

As shown in \cref{tab:lora}, {\method} outperforms existing baselines on all metrics. Specifically, {\method} show improvements of $3.2\%$ over TIES-merging for NIST of E2E, $4.8\%$ over PCB-Merging for CIDEr of DART.


\begin{table*}[h!]
    \centering
    \belowrulesep=0pt
    \aboverulesep=0pt
    \caption{Merge LoRA Adapters of GPT-2 M with Different Methods. $\uparrow$ indicates higher is better, $\downarrow$ indicates lower is better.}
    \label{tab:lora}
    \small
    \setlength\tabcolsep{0.2pt}
        \begin{tabular}{c|ccccc|ccc|ccc|c}
            \toprule
            &  \multicolumn{11}{c|}{\textbf{Test Set Performance}} & \\
            % \cmidrule{2-4}
            &  \multicolumn{5}{c|}{\textbf{E2E}} &  \multicolumn{3}{c|}{\textbf{DART}} &  \multicolumn{3}{c|}{\textbf{WebNLG}} & \\ 
            \multirow{-2}{*}{\textbf{Method}  } & {BLEU$\uparrow$} & {NIST$\uparrow$} & {MET$\uparrow$} & {ROUGE-L$\uparrow$} & {CIDEr$\uparrow$} & {BLEU$\uparrow$}  & {MET$\uparrow$} & {TER$\downarrow$} & {BLEU-A$\uparrow$} & {MET-A$\uparrow$} & {TER-A$\downarrow$} &  \multirow{-2}{*}{\textbf{Rank}}\\
            \midrule
            Pre-trained & 0.2 & 0.58 & 1.5 & 5.2 & 0.002 & 0.2 & 2.0 & 152.4 & 0.15 & 2.0 & 179.1 & -- \\
            LoRA & 67.7 & 8.64 & 46.0 & 68.3 & 2.36 & 44.8 & 35.0 & 50.4 & 52.3 & 37.0 & 44.4 & -- \\
            \midrule
            Average & 63.4 & 8.00 & 40.8 & 66.6 & 2.01 & 40.0 & 32.0 & 53.7 & 43.9 & 32.0 & 49.2 & 5\\
            Task Arithmetic & 63.2 & 8.02 & 40.9 & 66.3 & 1.98 & 40.8 & 33.0 & 53.7 & 45.9 & 34.0 & 48.8 & 4\\
            Ties-Merging & 62.8 & 8.14 & 41.1 & 65.9 & 2.08 & 41.4 & 33.0 & 53.7 & 46.1 & 33.0 & 48.1 & 3\\
            PCB-Merging & 62.9 & 8.12 &41.4 & 66.0 & 2.08 & 41.3 & 33.0 & 53.5 & 46.2 & 33.0 & 48.1 & 2\\
            \textbf{{\method} (ours)} & 64.1 & 8.40 & 42.2 & 66.5 & 2.18 & 41.6 & 33.0 & 54.1 & 47.1 & 34.0 & 48.0 & 1 \\
            \bottomrule
        \end{tabular}
\end{table*}

\begin{table}[ht]
    \belowrulesep=0.8pt
    \aboverulesep=0.8pt
    \centering
    \caption{Results on the CMMLU, GSM8K, and Humaneval datasets.}\label{tab:llm}
    \setlength{\tabcolsep}{1.8pt}
    \small
    \begin{tabular}{c|c c c|c}
    \toprule
    &  \multicolumn{3}{c|}{\textbf{Datasets}} & \\
    \multirow{-2}{*}{\textbf{Method}  }        &  CMMLU &  GSM8K &  Humaneval &  \multirow{-2}{*}{\textbf{Average}} \\
    \midrule
     Chinese          & 38.6  & 2.3   & 13.4      & 18.1    \\
    Math            & 31.2  & 65.6  & 0       & 32.3    \\
    Code            & 33.3  & 0   & 17.1      & 16.8    \\
    \midrule
    Average         & 35.6  & 48.5  & 6.7      & 30.3    \\
    Task Arithmetic & 35.4  & 46.1  & 9.8       & 30.4    \\
    Ties-Merging      & 36.5  & 53.4  & 12.8      & 34.3    \\
    PCB-Merging      & 36.4  & 52.3  & 16.5      & \underline{35.1}    \\
    \textbf{{\method} (ours)}            & 36.5  & 63.0  & 14.0      & \textbf{37.8} \\
    \bottomrule
    \end{tabular}
\end{table}


\paragraph{Large Model: LLM Merging}
We evaluate model merging on three fine-tuned Llama-2-7B models~\citep{touvron2023llama} focusing on different capabilities: Chinese language proficiency\footnote{\url{https://huggingface.co/LinkSoul/Chinese-Llama-2-7b}}, mathematical reasoning~\citep{yu2023metamath}\footnote{\url{https://huggingface.co/meta-math/MetaMath-7B-V1.0}}, and code generation~\citep{roziere2024code}\footnote{\url{https://huggingface.co/qualis2006/llama-2-7b-int4-python-code-18k}}. Each model's performance was assessed using domain-specific benchmarks: CMMLU~\citep{li2024cmmlu} for Chinese language understanding, GSM8K~\citep{cobbe2021training} for mathematical reasoning, and HumanEval~\citep{chen2021evaluating} for code generation abilities.


As shown in \cref{tab:llm}, {\method} achieves state-of-the-art performance across all three domains, improving overall performance by $2.7\%$ compared to the best baseline. The most significant improvement is observed in mathematical reasoning, where {\method} outperforms other methods by approximately $10\%$ on the GSM8K benchmark.

\begin{table*}[h]
    \centering
    \belowrulesep=0pt
    \aboverulesep=0pt
    \caption{Out-of-Distribution Generalization Performance of merged T5-base model}
    \label{tab:ood}
    \small
    \begin{tabular}{c|c|cccccc}
        \toprule
        &       & \multicolumn{6}{c}{\textbf{Out-of-Distribution Performance}} \\
        \multirow{-2}{*}{\textbf{Method}} & \multirow{-2}{*}{\textbf{Average}} & cosmos\_qa & social\_iqa & quail & wic & copa & h-swag \\
        \midrule
        Pre-trained & 31.1 & 21.9 & 18.8 & 24.1 & 65.6 & 43.8 & 12.5 \\
        \midrule
        Average & 36.3 & 23.5 & 37.0 & 25.4 & 50.2 & 54.5 & 27.2 \\
        Task Arithmetic & 37.0 & 21.9 & 36.8 & 25.5 & 49.5 & 61.4 & 26.6 \\
        Ties-Merging & 37.1 & 22.2 & 37.8 & 24.9 & 51.6 & 62.2 & 26.5 \\
        PCB-Merging & \underline{37.3} & 23.0 & 38.1 & 24.6 & 52.2 & 59.0 & 27.2 \\
        \textbf{{\method} (ours)} & \textbf{38.2} & 22.4 & 38.0 & 26.0 & 51.6 & 64.1 & 27.0  \\
        \bottomrule
    \end{tabular}
\end{table*}


\paragraph{Out-of-Distribution Generalization}
To evaluate out-of-distribution generalization, we test the merged T5-base model (previously trained on seven tasks) on six held-out tasks from the T0 mixture~\citep{sanh2021multitask}: three question answering tasks (Cosmos QA \citep{huang2019cosmos}, Social IQA \citep{sap2019social}, QuAIL \citep{quail_dataset}), word sense disambiguation (WiC \citep{pilehvar2018wic}), and two sentence completion tasks (COPA \citep{copa}, H-SWAG \citep{zellers2019hellaswag}). As shown in \cref{tab:ood}, {\method} achieves $0.9\%$ improvement over the best baseline on T5-base, demonstrating strong generalization capabilities to tasks outside the training distribution.

\subsection{Additional Results and Analysis}\label{ssec:additional_results}

\begin{figure*}[th]
    \centering
    \vspace{-10pt}
    \subfigure[Task Vector Matrice vs Fine-tune Matrice]{
    \includegraphics[width=0.24\textwidth]{img/task_vs_finetune_bar_chart.pdf}
    \label{fig:task-vector-vs-finetune}
 }
 \hspace{-10pt}
    \subfigure[Number of tasks]{
    \includegraphics[width=0.24\textwidth]{img/vit_task_num.pdf}
    \label{fig:num-tasks}
 }
 \hspace{-10pt}
    \subfigure[Various $\eta$ at $\gamma$ = 0.6]{
    \includegraphics[width=0.24\textwidth]{img/vit_topk.pdf}
    \label{fig:hyperparameter-sensitivity-eta}
 }
 \hspace{-10pt}
    \subfigure[Various $\gamma$ at $\eta$ = 0.2]{
    \includegraphics[width=0.24\textwidth]{img/vit_scale.pdf}
    \label{fig:hyperparameter-sensitivity-gamma}
    }
    \vspace{-10pt}
    \caption{Performance of {\method} with different hyperparameters , number of tasks, and removing singular features.}
    \vspace{-10pt}
\end{figure*}

\paragraph{Task Matrices versus Fine-tuned Matrices} We compare merging task matrices versus fine-tuned matrices directly on ViT-B/32, ViT-L/14 and T5-base models. As shown in \cref{fig:task-vector-vs-finetune}, merging task matrices consistently outperforms merging fine-tuned matrices across all architectures by a large margin. This validates our discussion in \cref{sec:discussion} that task matrices better isolate task-specific features compared to fine-tuned matrices, making them more effective for model merging.

\paragraph{Number of Tasks}
We vary the number of tasks to evaluate the performance of {\method}. We perform experiments on the ViT-B/32. 
Following \citet{du2024parameter}, We normalize the performance on each task to fine-tuned performance and then average the normalized performance across all chosen tasks. As shown in \cref{fig:num-tasks}, comparing to other methods, the performance of {\method} shows less decrease with the increasing number of tasks, which indicates that preserving the linear mapping of singular vectors are effective for model merging.


\paragraph{Hyperparameter Sensitivity}
We analyze the sensitivity of {\method} to the hyperparameters $\eta$ and $\gamma$ on the ViT-B/32. 
Three model merging methods are compared in the experiments: {\method}, Ties-Merging, and PCB-Merging. We follow the experiment setup of previous work \citep{du2024parameter} to analyze the effect of two parameters.
The first parameter is the scaling factor $\gamma$ , which is used to scale the merged task vector. For {\method} and Ties-Merging, the second hyperparameter is the top-k percentage $\eta$, indicating that the $\eta\%$ parameters in task vector with the highest absolute value are retained. For PCB-Merging, the second hyperparameter is mask ratio $\eta$, indicating that the $\eta\%$ parameters in parameter competition balancing matrix with the biggest absolute value will be retained.  
We vary the hyperparameters $\gamma$ with step size 0.1 from 0.5 to 1.2, and $\eta$ with step size 0.05 from 0.1 to 0.5.

First, in \cref{fig:hyperparameter-sensitivity-eta,fig:hyperparameter-sensitivity-gamma}, {\method} achieves optimal performance with $\gamma < 0.8$ and $\eta < 0.4$ on ViT-B/32.
Second, performance varies with both hyperparameters. For sparsity ratio $\eta$, performance first improves then declines as it increases. When $\eta$ is too low, important features are filtered out as most parameters are set to zero. When $\eta$ is too high, noise in the task matrices affects the quality of singular vectors. For scaling factor $\gamma$, we observe a similar trend - performance initially improves then deteriorates with increasing values. A small $\gamma$ diminishes the magnitude of singular vectors, while a large $\gamma$ may amplify interference between tasks.

\section{Conclusion}\label{sec:conclusion}
In this paper, We present {\method}, a novel model merging approach that preserves task-specific features in linear transformations through singular value decomposition. Extensive experiments demonstrate that {\method} consistently outperforms existing methods across different architectures and tasks.
The success of {\method} demonstrates the value of leveraging parameter structure in model merging, rather than treating all parameters as vectors.
Our work establishes a foundation for future research on model merging that leverages the mathematical properties and structural roles of different parameter types.

% \paragraph{Limitations and Future Work} While {\method} effectively preserves singular vectors of task matrices, it does not explicitly extract task-specific feature from model or address potential interference between features during merging. This interference can cause some task-specific features to be lost or diminished, particularly affecting performance when critical features are impacted. Future work could focus on better identifying and preserving task-specific and important  features in linear transformations during merging, potentially drawing from mechanistic interpretability research on superposition and sparse autoencoders to maintain key directional features while minimizing interference.


\bibliographystyle{plainnat}
\bibliography{papers}

\end{document}