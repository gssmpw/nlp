\section{Related Work}
\subsection{Model Merging of Fine-tuned Models}
Model merging is a technique that combines multiple models into a single model to enhance performance or enable the model to perform multiple tasks. 
Previous studies have shown that averaging the weights of multiple models fine-tuned from the same pre-trained initialization is a promising approach for model merging. 
Fisher Merging____ advances beyond simple averaging by utilizing the Fisher information matrix to assess the importance of individual parameters, which are then weighted accordingly during the merging process. Similarly, RegMean____ forms a linear regression problem with extra data for each layer and offers a closed-form solution for the merged model's parameters by solving the regression problem.


Beyond parameter averaging, Task Arithmetic____ introduces task vectors and adding the task vectors of individual tasks to merge model, demonstrating their effectiveness and lightweight nature in facilitating cross-task generalization. Building on this concept, PEM Composition____ extends the task arithmetic framework to merge LoRA____, while Ties-Merging____ addresses task conflicts by resetting redundant parameters and resolving sign conflicts. These methods, however, use a single merging coefficient across all task vectors, which limits their flexibility. In contrast, Lorahub____ and AdaMerging____ use different coefficients for enhanced adaptability. Lorahub's performance is limited as it only searches for coefficients at the task level, while AdaMerging requires complex training and unlabeled test datasets, making it applicable solely to classification problems. DARE____ proposes drop and rescale as preprocessing steps when merging fine-tuned LLMs. PCB-Merging____ is a lightweight, training-free technique for model merging that balances parameter competition by intra-balancing parameter significance within tasks and inter-balancing parameter similarities across tasks, effectively enhancing performance across various scenarios and domains.


\subsection{Linear Representation Hypothesis}
The linear representation hypothesis states that neural networks encode information by summing up "feature vectors"____, i.e., a layer of a network represents a set of features as a weighted sum of 
task-associated vectors.
This hypothesis has been observed in various models, including word embeddings____, sentence embeddings____, Transformer language models____, and vision-language models____.
The hypothesis has been explioted in various fields, especially in probing____ and interpretability____.

Especially, the linear representation hypothesis is prominently featured in recent works on mechanistic interpretablity of language models____. In mechanistic interpretability, models are understood by decomposing them into interpretable components and understanding how these components interact. The linear representation hypothesis suggests that important features in neural networks are often represented through linear combinations of neuron activations, rather than complex nonlinear transformations.
This hypothesis has gained significant empirical support through studies of language models. For instance, ____ demonstrated a toy model that learned to resconstruct its input through linear combinations of its neurons. Furthermore, ____ showed that individual neurons in a language model can encode semantically meaningful features in a largely linear fashion, which is later sclaed to large language models____.