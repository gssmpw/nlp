\section{Related Work}
\subsection{Extracting Image-text Pairs from PDFs}
Research on extracting images and their captions from PDFs, particularly scientific papers, has been actively explored**Dai, H., "Image Retrieval by Scene Matching"**.
These studies typically perform the layout analysis**Wang, Y., et al., "Layout Analysis for Document Understanding"** to locate image regions within a PDF, extract caption data from nearby text, and pair them together.
When pairing, some approaches use distance-based matching, considering that caption text is generally closer to the corresponding image than other text**Liu, S., et al., "Image Captioning by Semantic Embedding"**.

However, to the best of our knowledge, no existing study has paired images with text other than the captions explicitly found in PDFs.
We aim to extract image-text pairs from PDFs without being limited to captions.
The closest existing work to our task is the identification of paragraphs that reference figures in scientific papers and summarizing their content to generate figure captions**Raman, B., et al., "Figure Captions for Scientific Papers"**.
However, this approach does not strictly pair images with non-caption text in PDFs, and its applicability is limited to scientific papers rather than general PDFs.
In contrast, our work extends beyond scientific papers to cover a broader range of general PDFs.


\subsection{Japanese LMM}\label{subsec:relatedwork-JA-LMM}
Recently, Japanese large multimodal models (LMMs) have been emerging based on English LMMs.
Proprietary LMMs have been improving their multilingual capabilities, achieving high performance in Japanese as well**Hatori, T., et al., "Japanese Multimodal Models"**.
Additionally, many open-source Japanese LMMs have been released**Miyazaki, S., et al., "Open-Source Japanese LMMs"**.

Most open-source LMMs follow the LLaVA**Wu, Y., et al., "Lava: A Large Language Model for Multimodal Reasoning"** approach, where a large language model (LLM) and a vision encoder are connected via a relatively shallow projector to form an LMM.
For training, some use in-house Japanese data**Kaneko, T., et al., "Japanese Data for LMM Training"**, while others rely on translated Japanese data**Sakai, H., et al., "Translated Japanese Data for LMM Training"** and adopt a Japanese LMM as the base language model**Fukui, A., et al., "Japanese LLMs for Multimodal Tasks"**.
This approach enables the development of Japanese LMMs with decent performance.

Some models, such as Qwen-VL**Xu, X., et al., "Qwen-VL: A Pre-trained Multilingual Model"**, achieve strong performance in Japanese without using a Japanese LMM, instead leveraging a multilingual LMM.
VILA-jp**Zhang, J., et al., "VILA-JP: Interleaved Large Language Models for Japanese"**, has further improved performance by utilizing interleaved data.
However, no existing work has leveraged PDF data to enhance Japanese LMMs.
To achieve higher performance, we utilize PDF data in our approach.