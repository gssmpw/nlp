\section{Related Work}
\subsection{Extracting Image-text Pairs from PDFs}
Research on extracting images and their captions from PDFs, particularly scientific papers, has been actively explored~\cite{clark2015looking,clark2016pdffigures,siegel2018extracting,naiman2022figure,okamoto2023constructing}.
These studies typically perform the layout analysis~\cite{shen2021layoutparser} to locate image regions within a PDF, extract caption data from nearby text, and pair them together.
When pairing, some approaches use distance-based matching, considering that caption text is generally closer to the corresponding image than other text~\cite{okamoto2023constructing}.

However, to the best of our knowledge, no existing study has paired images with text other than the captions explicitly found in PDFs.
We aim to extract image-text pairs from PDFs without being limited to captions.
The closest existing work to our task is the identification of paragraphs that reference figures in scientific papers and summarizing their content to generate figure captions~\cite{huang2023summaries}.
However, this approach does not strictly pair images with non-caption text in PDFs, and its applicability is limited to scientific papers rather than general PDFs.
In contrast, our work extends beyond scientific papers to cover a broader range of general PDFs.


\subsection{Japanese LMM}\label{subsec:relatedwork-JA-LMM}
Recently, Japanese large multimodal models (LMMs) have been emerging based on English LMMs.
Proprietary LMMs have been improving their multilingual capabilities, achieving high performance in Japanese as well~\cite{openai2023gpt4,openai2024gpt4o,anthropic2024claude3,geminipro}.
Additionally, many open-source Japanese LMMs have been released~\cite{JapaneseInstructBLIPAlpha,JapaneseStableVLM,akiba2025evolutionary,BlipJapaneseStableLM,inoue2024heron,cyberagent2024llava,VILAjp}.

Most open-source LMMs follow the LLaVA~\cite{llava} approach, where a large language model (LLM) and a vision encoder are connected via a relatively shallow projector to form an LMM.
For training, some use in-house Japanese data~\cite{cyberagent2024llava}, while others rely on translated Japanese data~\cite{JapaneseInstructBLIPAlpha,JapaneseStableVLM,BlipJapaneseStableLM,inoue2024heron} and adopt a Japanese LLM as the base language model~\cite{cyberagent2024llava,JapaneseInstructBLIPAlpha,JapaneseStableVLM,BlipJapaneseStableLM,inoue2024heron}.
This approach enables the development of Japanese LMMs with decent performance.

Some models, such as Qwen-VL~\cite{bai2023qwen}, achieve strong performance in Japanese without using a Japanese LLM, instead leveraging a multilingual LLM.
VILA-jp~\cite{VILAjp} has further improved performance by utilizing interleaved data.
However, no existing work has leveraged PDF data to enhance Japanese LMMs.
To achieve higher performance, we utilize PDF data in our approach.