@misc{BlipJapaneseStableLM, 
    title  = {Heron BLIP Japanese StableLM Base 7B},
    author = {Tanahashi, Kotaro and Inoue, Yuichi and Yamaguchi, Yu},
    year = {2023},
    url    = {https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v0}
}

@misc{JapaneseInstructBLIPAlpha, 
    title  = {Japanese InstructBLIP Alpha},     
    author = {Shing, Makoto and Akiba, Takuya},
    year = {2023},
    url    = {https://huggingface.co/stabilityai/japanese-instructblip-alpha}
}

@misc{JapaneseStableVLM, 
    title  = {Japanese Stable VLM}, 
    author = {Shing, Makoto and Akiba, Takuya},
    year = {2023},
    url    = {https://huggingface.co/stabilityai/japanese-stable-vlm}
}

@article{VILAjp,
  title={Constructing Multimodal Datasets from Scratch for Rapid Development of a Japanese Visual Language Model},
  author={Sasagawa, Keito and Maeda, Koki and Sugiura, Issa and Kurita, Shuhei and Okazaki, Naoaki and Kawahara, Daisuke},
  journal={arXiv preprint arXiv:2410.22736},
  year={2024}
}

@article{akiba2025evolutionary,
  title={Evolutionary optimization of model merging recipes},
  author={Akiba, Takuya and Shing, Makoto and Tang, Yujin and Sun, Qi and Ha, David},
  journal={Nature Machine Intelligence},
  year={2025},
}

@misc{anthropic2024claude3,
  author = {Anthropic},
  title = {Claude 3 Family},
  year = {2024},
  url = {https://www.anthropic.com/news/claude-3-family},
  note = {Accessed: 2025-02-15}
}

@article{bai2023qwen,
  title={Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}

@inproceedings{clark2015looking,
  title={Looking beyond text: Extracting figures, tables and captions from computer science papers},
  author={Clark, Christopher Andreas and Divvala, Santosh},
  booktitle={Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence},
  year={2015}
}

@inproceedings{clark2016pdffigures,
  title={Pdffigures 2.0: Mining figures from research papers},
  author={Clark, Christopher and Divvala, Santosh},
  booktitle={Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries},
  year={2016}
}

@misc{cyberagent2024llava,
  author = {CyberAgent AI Lab},
  title = {LLaVA-CALM2-SigLIP},
  year = {2024},
  howpublished = {\url{https://huggingface.co/cyberagent/llava-calm2-siglip}},
  note = {Accessed: 2025-02-15}
}

@article{geminipro,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@inproceedings{huang2023summaries,
  title={Summaries as captions: Generating figure captions for scientific documents with automated text summarization},
  author={Huang, Chieh-Yang and Hsu, Ting-Yao and Rossi, Ryan and Nenkova, Ani and Kim, Sungchul and Chan, Gromit Yeuk-Yin and Koh, Eunyee and Giles, Clyde Lee and Huang, Ting-Hao'Kenneth'},
  booktitle={International Natural Language Generation Conference},
  year={2023}
}

@inproceedings{inoue2024heron,
  title={Heron-bench: A benchmark for evaluating vision language models in japanese},
  author={Inoue, Yuichi and Sasaki, Kento and Ochi, Yuma and Fujii, Kazuki and Tanahashi, Kotaro and Yamaguchi, Yu},
  booktitle={CVPR The 3rd Workshop on Computer Vision in the Wild},
  year={2024}
}

@inproceedings{llava,
      title={Visual Instruction Tuning}, 
      author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
      booktitle={NeurIPS},
      year={2023},
}

@inproceedings{naiman2022figure,
  title={Figure and figure caption extraction for mixed raster and vector PDFs: digitization of astronomical literature with OCR features},
  author={Naiman, Jill P and Williams, Peter KG and Goodman, Alyssa},
  booktitle={International Conference on Theory and Practice of Digital Libraries},
  year={2022},
}

@inproceedings{okamoto2023constructing,
  title={Constructing Image-Text Pair Dataset from Books},
  author={Okamoto, Yamato and Toyonaga, Haruto and Ijiri, Yoshihisa and Kataoka, Hirokatsu},
  booktitle={ICCV Workshop on Towards the Next Generation of Computer Vision Datasets},
  year={2023}
}

@misc{openai2023gpt4,
  title     = "{GPT-4 Technical Report}",
  author    = {OpenAI},
  year      = {2023},
  url       = {https://openai.com/research/gpt-4},
  note      = {Accessed: 2025-02-15}
}

@misc{openai2024gpt4o,
  author       = {OpenAI},
  title        = {GPT-4o},
  year         = {2024},
  url          = {https://openai.com/index/hello-gpt-4o/},
  note         = {Accessed: 2025-02-15}
}

@inproceedings{shen2021layoutparser,
  title={LayoutParser: A Unified Toolkit for Deep Learning Based Document Image Analysis},
  author={Shen, Zejiang and Zhang, Ruochen and Dell, Melissa and Lee, Benjamin Charles Germain and Carlson, Jacob and Li, Weining},
  booktitle={ICDAR},
  year={2021}
}

@inproceedings{siegel2018extracting,
  title={Extracting scientific figures with distantly supervised neural networks},
  author={Siegel, Noah and Lourie, Nicholas and Power, Russell and Ammar, Waleed},
  booktitle={Proceedings of the 18th ACM/IEEE on joint conference on digital libraries},
  year={2018}
}

