@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@inproceedings{yue202415,
  title={15.1 A 0.795 fJ/bit Physically-Unclonable Function-Protected TCAM for a Software-Defined Networking Switch},
  author={Yue, Zhiheng and Xiang, Xujiang and Tu, Fengbin and Wang, Yang and Wang, Yiming and Wei, Shaojun and Hu, Yang and Yin, Shouyi},
  booktitle={2024 IEEE International Solid-State Circuits Conference (ISSCC)},
  volume={67},
  pages={276--278},
  year={2024},
  organization={IEEE}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@article{dettmers2022gpt3,
  title={Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30318--30332},
  year={2022}
}
@article{frantar2022gptq,
  title={Gptq: Accurate post-training quantization for generative pre-trained transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}

@article{lie2023cerebras,
  title={Cerebras architecture deep dive: First look inside the hardware/software co-design for deep learning},
  author={Lie, Sean},
  journal={IEEE Micro},
  volume={43},
  number={3},
  pages={18--30},
  year={2023},
  publisher={IEEE}
}



@article{xu2024beyond,
  title={Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression},
  author={Xu, Zhichao and Gupta, Ashim and Li, Tao and Bentham, Oliver and Srikumar, Vivek},
  journal={arXiv preprint arXiv:2407.04965},
  year={2024}
}
@article{hong2024decoding,
  title={Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression},
  author={Hong, Junyuan and Duan, Jinhao and Zhang, Chenhui and Li, Zhangheng and Xie, Chulin and Lieberman, Kelsey and Diffenderfer, James and Bartoldson, Brian and Jaiswal, Ajay and Xu, Kaidi and others},
  journal={arXiv preprint arXiv:2403.15447},
  year={2024}
}


@article{zhang2024does,
  title={Does your LLM truly unlearn? An embarrassingly simple approach to recover unlearned knowledge},
  author={Zhang, Zhiwei and Wang, Fali and Li, Xiaomin and Wu, Zongyu and Tang, Xianfeng and Liu, Hui and He, Qi and Yin, Wenpeng and Wang, Suhang},
  journal={arXiv preprint arXiv:2410.16454},
  year={2024}
}

@article{dutta2024accuracy,
  title={Accuracy is Not All You Need},
  author={Dutta, Abhinav and Krishnan, Sanjeev and Kwatra, Nipun and Ramjee, Ramachandran},
  journal={arXiv preprint arXiv:2407.09141},
  year={2024}
}


@article{yao2022zeroquant,
  title={Zeroquant: Efficient and affordable post-training quantization for large-scale transformers},
  author={Yao, Zhewei and Yazdani Aminabadi, Reza and Zhang, Minjia and Wu, Xiaoxia and Li, Conglong and He, Yuxiong},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27168--27183},
  year={2022}
}

@article{marchisio2024does,
  title={How does quantization affect multilingual LLMs?},
  author={Marchisio, Kelly and Dash, Saurabh and Chen, Hongyu and Aumiller, Dennis and {\"U}st{\"u}n, Ahmet and Hooker, Sara and Ruder, Sebastian},
  journal={arXiv preprint arXiv:2407.03211},
  year={2024}
}


@article{huffman1952method,
  title={A method for the construction of minimum-redundancy codes},
  author={Huffman, David A},
  journal={Proceedings of the IRE},
  volume={40},
  number={9},
  pages={1098--1101},
  year={1952},
  publisher={IEEE}
}
@article{langdon1984introduction,
  title={An introduction to arithmetic coding},
  author={Langdon, Glen G},
  journal={IBM Journal of Research and Development},
  volume={28},
  number={2},
  pages={135--149},
  year={1984},
  publisher={IBM}
}
@article{shannon1948mathematical,
  title={A mathematical theory of communication},
  author={Shannon, Claude Elwood},
  journal={The Bell system technical journal},
  volume={27},
  number={3},
  pages={379--423},
  year={1948},
  publisher={Nokia Bell Labs}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}
@article{hooper2024kvquant,
  title={Kvquant: Towards 10 million context length llm inference with kv cache quantization},
  author={Hooper, Coleman and Kim, Sehoon and Mohammadzadeh, Hiva and Mahoney, Michael W and Shao, Yakun Sophia and Keutzer, Kurt and Gholami, Amir},
  journal={arXiv preprint arXiv:2401.18079},
  year={2024}
}
@article{lin2024awq,
  title={AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={87--100},
  year={2024}
}
@article{white2024livebench,
  title={Livebench: A challenging, contamination-free llm benchmark},
  author={White, Colin and Dooley, Samuel and Roberts, Manley and Pal, Arka and Feuer, Ben and Jain, Siddhartha and Shwartz-Ziv, Ravid and Jain, Neel and Saifullah, Khalid and Naidu, Siddartha and others},
  journal={arXiv preprint arXiv:2406.19314},
  year={2024}
}

@article{hao2024neuzip,
  title={NeuZip: Memory-Efficient Training and Inference with Dynamic Compression of Neural Networks},
  author={Hao, Yongchang and Cao, Yanshuai and Mou, Lili},
  journal={arXiv preprint arXiv:2410.20650},
  year={2024}
}
@article{hershcovitch2024zipnn,
  title={ZipNN: Lossless Compression for AI Models},
  author={Hershcovitch, Moshik and Wood, Andrew and Choshen, Leshem and Girmonsky, Guy and Leibovitz, Roy and Ennmouri, Ilias and Malka, Michal and Chin, Peter and Sundararaman, Swaminathan and Harnik, Danny},
  journal={arXiv preprint arXiv:2411.05239},
  year={2024}
}

@INPROCEEDINGS{scalesim,
  author={Samajdar, Ananda and Joseph, Jan Moritz and Zhu, Yuhao and Whatmough, Paul and Mattina, Matthew and Krishna, Tushar},
  booktitle={2020 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)}, 
  title={A Systematic Methodology for Characterizing Scalability of DNN Accelerators using SCALE-Sim}, 
  year={2020},
  volume={},
  number={},
  pages={58-68},
  keywords={Analytical models;Systematics;Memory management;Random access memory;Bandwidth;Parallel processing;Systems modeling;Accelerator Simulator;DNN acclerator;Cycle accurate simulation;scaling analysis},
  doi={10.1109/ISPASS48437.2020.00016}}

@article{starsim,
author = {Sun, Xiaoyu and Peng, Xiaochen and Zhang, Sai Qian and Gomez, Jorge and Khwa, Win-San and Sarwar, Syed Shakib and Li, Ziyun and Cao, Weidong and Wang, Zhao and Liu, Chiao and Chang, Meng-Fan and De Salvo, Barbara and Akarvardar, Kerem and Wong, H.-S. Philip},
title = {Estimating Power, Performance, and Area for On-Sensor Deployment of AR/VR Workloads Using an Analytical Framework},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {6},
issn = {1084-4309},
url = {https://doi.org/10.1145/3670404},
doi = {10.1145/3670404},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = sep,
articleno = {93},
numpages = {27},
keywords = {Augmented reality, virtual reality, 3D CMOS image sensor, DNN accelerator}
}

@article{Wang2020SpAttenES,
  title={SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning},
  author={Hanrui Wang and Zhekai Zhang and Song Han},
  journal={2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  year={2020},
  pages={97-110},
  url={https://api.semanticscholar.org/CorpusID:229298088}
}

@inproceedings{hyft,
author = {Xia, Tianhua and Zhang, Sai Qian},
title = {Hyft: A Reconfigurable Softmax Accelerator with Hybrid Numeric Format for both Training and Inference},
year = {2024},
isbn = {9798400706882},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3665314.3670816},
doi = {10.1145/3665314.3670816},
abstract = {The attention mechanism is a pivotal element within the transformer architecture, making a substantial contribution to its exceptional performance. Within this attention mechanism, Softmax is an imperative component that enables the model to assess the degree of correlation between various segments of the input. Yet, prior research has shown that Softmax operations can significantly increase processing latency and energy consumption in the transformer network due to their internal nonlinear operations and data dependencies. In this work, we proposed Hyft, a hardware efficient floating point Softmax accelerator for both training and inference. Hyft aims to reduce the implementation cost of different nonlinear arithmetic operations within softmax by adaptively converting intermediate results into the most suitable numeric format for each specific operation, leading to reconfigurable accelerator with hybrid numeric format. The evaluation results highlight that Hyft achieves a remarkable 10X reduction in hardware resource utilization and a 6x reduction in processing latency, all while maintaining a negligible impact on transformer accuracy.},
booktitle = {Proceedings of the 29th ACM/IEEE International Symposium on Low Power Electronics and Design},
pages = {1–6},
numpages = {6},
keywords = {hardware accelerator, softmax, transformer},
location = {Newport Beach, CA, USA},
series = {ISLPED '24}
}

@article{rissanen1979arithmetic,
  title={Arithmetic coding},
  author={Rissanen, Jorma and Langdon, Glen G},
  journal={IBM Journal of research and development},
  volume={23},
  number={2},
  pages={149--162},
  year={1979},
  publisher={IBM}
}
@article{duda2009asymmetric,
  title={Asymmetric numeral systems},
  author={Duda, Jarek},
  journal={arXiv preprint arXiv:0902.0271},
  year={2009}
}

@article{Alwani2016FusedlayerCA,
  title={Fused-layer CNN accelerators},
  author={Manoj Alwani and Han Chen and Michael Ferdman and Peter Milder},
  journal={2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  year={2016},
  pages={1-12},
  url={https://api.semanticscholar.org/CorpusID:5804465}
}

@article{Han2016EIEEI,
  title={EIE: Efficient Inference Engine on Compressed Deep Neural Network},
  author={Song Han and Xingyu Liu and Huizi Mao and Jing Pu and Ardavan Pedram and Mark Horowitz and William J. Dally},
  journal={2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)},
  year={2016},
  pages={243-254},
  url={https://api.semanticscholar.org/CorpusID:1663491}
}

@article{Zhang2021FASTDT,
  title={FAST: DNN Training Under Variable Precision Block Floating Point with Stochastic Rounding},
  author={Sai Qian Zhang and Bradley McDanel and H. T. Kung},
  journal={2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  year={2021},
  pages={846-860},
  url={https://api.semanticscholar.org/CorpusID:240288620}
}

@article{Eckert2018NeuralCB,
  title={Neural Cache: Bit-Serial In-Cache Acceleration of Deep Neural Networks},
  author={Charles Eckert and Xiaowei Wang and Jingcheng Wang and Arun K. Subramaniyan and Ravi R. Iyer and Dennis Sylvester and David Blaauw and Reetuparna Das},
  journal={2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)},
  year={2018},
  pages={383-396},
  url={https://api.semanticscholar.org/CorpusID:13686490}
}

@article{Chen2023MetaNMPLC,
  title={MetaNMP: Leveraging Cartesian-Like Product to Accelerate HGNNs with Near-Memory Processing},
  author={Dan Chen and Haiheng He and Hai Jin and Long Zheng and Yu Huang and Xinyang Shen and Xiaofei Liao},
  journal={Proceedings of the 50th Annual International Symposium on Computer Architecture},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:259177755}
}

@article{Ke2019RecNMPAP,
  title={RecNMP: Accelerating Personalized Recommendation with Near-Memory Processing},
  author={Liu Ke and Udit Gupta and Carole-Jean Wu and Benjamin Youngjae Cho and Mark Hempstead and Brandon Reagen and Xuan Zhang and David M. Brooks and Vikas Chandra and Utku Diril and Amin Firoozshahian and Kim M. Hazelwood and Bill Jia and Hsien-Hsin S. Lee and Mengxing Li and Bertrand A. Maher and Dheevatsa Mudigere and Maxim Naumov and Martin D. Schatz and Mikhail Smelyanskiy and Xiaodong Wang},
  journal={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  year={2019},
  pages={790-803},
  url={https://api.semanticscholar.org/CorpusID:209515444}
}

@article{Liu2023AcceleratingPR,
  title={Accelerating Personalized Recommendation with Cross-level Near-Memory Processing},
  author={Haifeng Liu and Long Zheng and Yu Huang and Chao Liu and Xiangyu Ye and Jingrui Yuan and Xiaofei Liao and Hai Jin and Jingling Xue},
  journal={Proceedings of the 50th Annual International Symposium on Computer Architecture},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:259177984}
}

@inproceedings{Albericio2023RETROSPECTIVECI,
  title={RETROSPECTIVE: Cnvlutin: Ineffectual-Neuron-Free Deep Neural Network Computing},
  author={Jorge Albericio and Patrick Judd and Tayler H. Hetherington and Tor M. Aamodt and Natalie D. Enright Jerger and Andreas Moshovos},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:9841314}
}

@article{Lu2021SangerAC,
  title={Sanger: A Co-Design Framework for Enabling Sparse Attention using Reconfigurable Architecture},
  author={Liqiang Lu and Yicheng Jin and Hangrui Bi and Zizhang Luo and Peng Li and Tao Wang and Yun Liang},
  journal={MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:239012114}
}

@article{Zhang2016CambriconXAA,
  title={Cambricon-X: An accelerator for sparse neural networks},
  author={Shijin Zhang and Zidong Du and Lei Zhang and Huiying Lan and Shaoli Liu and Ling Li and Qi Guo and Tianshi Chen and Yunji Chen},
  journal={2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  year={2016},
  pages={1-12},
  url={https://api.semanticscholar.org/CorpusID:206464266}
}

@article{Lu2021SangerAC,
  title={Sanger: A Co-Design Framework for Enabling Sparse Attention using Reconfigurable Architecture},
  author={Liqiang Lu and Yicheng Jin and Hangrui Bi and Zizhang Luo and Peng Li and Tao Wang and Yun Liang},
  journal={MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:239012114}
}

@article{Yuan2023RPTQRP,
  title={RPTQ: Reorder-based Post-training Quantization for Large Language Models},
  author={Zhihang Yuan and Lin Niu and Jia-Wen Liu and Wenyu Liu and Xinggang Wang and Yuzhang Shang and Guangyu Sun and Qiang Wu and Jiaxiang Wu and Bingzhe Wu},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.01089},
  url={https://api.semanticscholar.org/CorpusID:257913374}
}

@inproceedings{Frantar2023OPTQAQ,
  title={OPTQ: Accurate Quantization for Generative Pre-trained Transformers},
  author={Elias Frantar and Saleh Ashkboos and Torsten Hoefler and Dan Alistarh},
  booktitle={International Conference on Learning Representations},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:259298689}
}

@article{Lin2023AWQAW,
  title={AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration},
  author={Ji Lin and Jiaming Tang and Haotian Tang and Shang Yang and Xingyu Dang and Song Han},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.00978},
  url={https://api.semanticscholar.org/CorpusID:271271084}
}

@article{Dettmers2023QLoRAEF,
  title={QLoRA: Efficient Finetuning of Quantized LLMs},
  author={Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.14314},
  url={https://api.semanticscholar.org/CorpusID:258841328}
}

@misc{lin2024qservew4a8kv4quantizationcodesign,
      title={QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving}, 
      author={Yujun Lin and Haotian Tang and Shang Yang and Zhekai Zhang and Guangxuan Xiao and Chuang Gan and Song Han},
      year={2024},
      eprint={2405.04532},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.04532}, 
}

@INPROCEEDINGS{8351458,
  author={Gonugondla, Sujan K. and Kang, Mingu and Kim, Yongjune and Helm, Mark and Eilert, Sean and Shanbhag, Naresh},
  booktitle={2018 IEEE International Symposium on Circuits and Systems (ISCAS)}, 
  title={Energy-Efficient Deep In-memory Architecture for NAND Flash Memories}, 
  year={2018},
  volume={},
  number={},
  pages={1-5},
  keywords={Computer architecture;Transistors;Threshold voltage;Throughput;Support vector machines;Microprocessors;Flash memories},
  doi={10.1109/ISCAS.2018.8351458}}

@article{mutlu2019processing,
  title={Processing data where it makes sense: Enabling in-memory computation},
  author={Mutlu, Onur and Ghose, Saugata and G{\'o}mez-Luna, Juan and Ausavarungnirun, Rachata},
  journal={Microprocessors and Microsystems},
  volume={67},
  pages={28--41},
  year={2019},
  publisher={Elsevier}
}

@INPROCEEDINGS{6983056,
  author={Pandiyan, Dhinakaran and Wu, Carole-Jean},
  booktitle={2014 IEEE International Symposium on Workload Characterization (IISWC)}, 
  title={Quantifying the energy cost of data movement for emerging smart phone workloads on mobile platforms}, 
  year={2014},
  volume={},
  number={},
  pages={171-180},
  keywords={Smart phones;Registers;Benchmark testing;Energy consumption;Energy measurement;Power measurement;Mobile communication},
  doi={10.1109/IISWC.2014.6983056}}

@article{sebastian2020memory,
  title={Memory devices and applications for in-memory computing},
  author={Sebastian, Abu and Le Gallo, Manuel and Khaddam-Aljameh, Riduan and Eleftheriou, Evangelos},
  journal={Nature nanotechnology},
  volume={15},
  number={7},
  pages={529--544},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

@INPROCEEDINGS{10764574,
  author={Yu, Zhongkai and Liang, Shengwen and Ma, Tianyun and Cai, Yunke and Nan, Ziyuan and Huang, Di and Song, Xinkai and Hao, Yifan and Zhang, Jie and Zhi, Tian and Zhao, Yongwei and Du, Zidong and Hu, Xing and Guo, Qi and Chen, Tianshi},
  booktitle={2024 57th IEEE/ACM International Symposium on Microarchitecture (MICRO)}, 
  title={Cambricon-LLM: A Chiplet-Based Hybrid Architecture for On-Device Inference of 70B LLM}, 
  year={2024},
  volume={},
  number={},
  pages={1474-1488},
  keywords={Microarchitecture;Large language models;Memory management;Reliability engineering;Market research;Error correction;Flash memories;Smart phones;Robots;Resilience;In-Flash Computing;Large Language Model Accelerator;Robotic Accelerator},
  doi={10.1109/MICRO61859.2024.00108}}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@INPROCEEDINGS{mecla,
  author={Qin, Yubin and Wang, Yang and Zhao, Zhiren and Yang, Xiaolong and Zhou, Yang and Wei, Shaojun and Hu, Yang and Yin, Shouyi},
  booktitle={2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA)}, 
  title={MECLA: Memory-Compute-Efficient LLM Accelerator with Scaling Sub-matrix Partition}, 
  year={2024},
  volume={},
  number={},
  pages={1032-1047},
  keywords={Large language models;Memory management;Graphics processing units;Benchmark testing;Transformers;Energy efficiency;System-on-chip;large language model;transformer;accelerator;artificial intelligence},
  doi={10.1109/ISCA59077.2024.00079}}

@article{kim2024breakthrough,
  title={The Breakthrough Memory Solutions for Improved Performance on LLM Inference},
  author={Kim, Byeongho and Cha, Sanghoon and Park, Sangsoo and Lee, Jieun and Lee, Sukhan and Kang, Shin-haeng and So, Jinin and Kim, Kyungsoo and Jung, Jin and Lee, Jong-Geon and others},
  journal={IEEE Micro},
  year={2024},
  publisher={IEEE}
}

@article{wang2019benchmarking,
  title={Benchmarking TPU, GPU, and CPU platforms for deep learning},
  author={Wang, Yu Emma and Wei, Gu-Yeon and Brooks, David},
  journal={arXiv preprint arXiv:1907.10701},
  year={2019}
}

@inproceedings{jouppi2017datacenter,
  title={In-datacenter performance analysis of a tensor processing unit},
  author={Jouppi, Norman P and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and others},
  booktitle={Proceedings of the 44th annual international symposium on computer architecture},
  pages={1--12},
  year={2017}
}

@inproceedings{tpuv4,
author = {Jouppi, Norm and Kurian, George and Li, Sheng and Ma, Peter and Nagarajan, Rahul and Nai, Lifeng and Patil, Nishant and Subramanian, Suvinay and Swing, Andy and Towles, Brian and Young, Clifford and Zhou, Xiang and Zhou, Zongwei and Patterson, David A},
title = {TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589350},
doi = {10.1145/3579371.3589350},
abstract = {In response to innovations in machine learning (ML) models, production workloads changed radically and rapidly. TPU v4 is the fifth Google domain specific architecture (DSA) and its third supercomputer for such ML models. Optical circuit switches (OCSes) dynamically reconfigure its interconnect topology to improve scale, availability, utilization, modularity, deployment, security, power, and performance; users can pick a twisted 3D torus topology if desired. Much cheaper, lower power, and faster than Infiniband, OCSes and underlying optical components are <5\% of system cost and <3\% of system power. Each TPU v4 includes SparseCores, dataflow processors that accelerate models that rely on embeddings by 5x--7x yet use only 5\% of die area and power. Deployed since 2020, TPU v4 outperforms TPU v3 by 2.1x and improves performance/Watt by 2.7x. The TPU v4 supercomputer is 4x larger at 4096 chips and thus nearly 10x faster overall, which along with OCS flexibility and availability allows a large language model to train at an average of ~60\% of peak FLOPS/second. For similar sized systems, it is ~4.3x--4.5x faster than the Graphcore IPU Bow and is 1.2x--1.7x faster and uses 1.3x--1.9x less power than the Nvidia A100. TPU v4s inside the energy-optimized warehouse scale computers of Google Cloud use ~2--6x less energy and produce ~20x less CO2e than contemporary DSAs in typical on-premise data centers.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {82},
numpages = {14},
keywords = {machine learning, domain specific architecture, TPU, GPU, IPU, supercomputer, optical interconnect, reconfigurable, embeddings, large language model, power usage effectiveness, warehouse scale computer, carbon emissions, energy, CO2 equivalent emissions},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@ARTICLE{tpu,
  author={Jouppi, Norman and Young, Cliff and Patil, Nishant and Patterson, David},
  journal={IEEE Micro}, 
  title={Motivation for and Evaluation of the First Tensor Processing Unit}, 
  year={2018},
  volume={38},
  number={3},
  pages={10-19},
  keywords={Neural networks;Graphics processing units;Tensile stress;Central Processing Unit;Energy efficiency;Data centers;Semiconductor devices;Microprocessors;microprocessor;tensor processing unit;deep neural network;GPU;machine learning;hardware},
  doi={10.1109/MM.2018.032271057}}

@inproceedings{kung1979systolic,
  title={Systolic arrays (for VLSI)},
  author={Kung, Hsiang Tsung and Leiserson, Charles E},
  booktitle={Sparse Matrix Proceedings 1978},
  volume={1},
  pages={256--282},
  year={1979},
  organization={Society for industrial and applied mathematics Philadelphia, PA, USA}
}

@article{dave2021hardware,
  title={Hardware acceleration of sparse and irregular tensor computations of ml models: A survey and insights},
  author={Dave, Shail and Baghdadi, Riyadh and Nowatzki, Tony and Avancha, Sasikanth and Shrivastava, Aviral and Li, Baoxin},
  journal={Proceedings of the IEEE},
  volume={109},
  number={10},
  pages={1706--1752},
  year={2021},
  publisher={IEEE}
}
@article{balasubramonian2017cacti,
  title={CACTI 7: New tools for interconnect exploration in innovative off-chip memories},
  author={Balasubramonian, Rajeev and Kahng, Andrew B and Muralimanohar, Naveen and Shafiee, Ali and Srinivas, Vaishnav},
  journal={ACM Transactions on Architecture and Code Optimization (TACO)},
  volume={14},
  number={2},
  pages={1--25},
  year={2017},
  publisher={ACM New York, NY, USA}
}
@inproceedings{aladdin,
author = {Shao, Yakun Sophia and Reagen, Brandon and Wei, Gu-Yeon and Brooks, David},
title = {Aladdin: A Pre-RTL, Power-Performance Accelerator Simulator Enabling Large Design Space Exploration of Customized Architectures},
year = {2014},
isbn = {9781479943944},
publisher = {IEEE Press},
abstract = {Hardware specialization, in the form of accelerators that provide custom datapath
and control for specific algorithms and applications, promises impressive performance
and energy advantages compared to traditional architectures. Current research in accelerator
analysis relies on RTL-based synthesis flows to produce accurate timing, power, and
area estimates. Such techniques not only require significant effort and expertise
but are also slow and tedious to use, making large design space exploration infeasible.
To overcome this problem, we present Aladdin, a pre-RTL, power-performance accelerator
modeling framework and demonstrate its application to system-on-chip (SoC) simulation.
Aladdin estimates performance, power, and area of accelerators within 0.9%, 4.9%,
and 6.6% with respect to RTL implementations. Integrated with architecture-level core
and memory hierarchy simulators, Aladdin provides researchers an approach to model
the power and performance of accelerators in an SoC environment},
booktitle = {Proceeding of the 41st Annual International Symposium on Computer Architecuture},
pages = {97–108},
numpages = {12},
location = {Minneapolis, Minnesota, USA},
series = {ISCA '14}
}
@INPROCEEDINGS{timeloop,
  author={Parashar, Angshuman and Raina, Priyanka and Shao, Yakun Sophia and Chen, Yu-Hsin and Ying, Victor A. and Mukkara, Anurag and Venkatesan, Rangharajan and Khailany, Brucek and Keckler, Stephen W. and Emer, Joel},
  booktitle={2019 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)}, 
  title={Timeloop: A Systematic Approach to DNN Accelerator Evaluation}, 
  year={2019},
  volume={},
  number={},
  pages={304-315},
  doi={10.1109/ISPASS.2019.00042}}
@inproceedings{accelergy,
    author      = {Wu, Yannan N. and Emer, Joel S. and Sze, Vivienne},
    title       = {{Accelergy: An Architecture-Level Energy Estimation Methodology for Accelerator Designs}},
    booktitle   = {{IEEE/ACM International Conference On Computer Aided Design (ICCAD)}},
    year        = {{2019}}
}

@INPROCEEDINGS{sa-pipeline,
  author={Peltekis, C. and Filippas, D. and Dimitrakopoulos, G. and Nicopoulos, C. and Pnevmatikatos, D.},
  booktitle={2023 Design, Automation and Test in Europe Conference and Exhibition (DATE)}, 
  title={ArrayFlex: A Systolic Array Architecture with Configurable Transparent Pipelining}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  keywords={Time-frequency analysis;Scalability;Merging;Systolic arrays;Hardware;Convolutional neural networks;Kernel},
  doi={10.23919/DATE56975.2023.10136913}}

@inproceedings{shao2019simba,
  title={Simba: Scaling deep-learning inference with multi-chip-module-based architecture},
  author={Shao, Yakun Sophia and Clemons, Jason and Venkatesan, Rangharajan and Zimmer, Brian and Fojtik, Matthew and Jiang, Nan and Keller, Ben and Klinefelter, Alicia and Pinckney, Nathaniel and Raina, Priyanka and others},
  booktitle={Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={14--27},
  year={2019}
}
@article{han2015deep,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={arXiv preprint arXiv:1510.00149},
  year={2015}
}
@article{welch1984technique,
  title={A technique for high-performance data compression},
  author={Welch, Terry A.},
  journal={Computer},
  volume={17},
  number={06},
  pages={8--19},
  year={1984},
  publisher={IEEE Computer Society}
}
@inproceedings{choukse2020buddy,
  title={Buddy compression: Enabling larger memory for deep learning and hpc workloads on gpus},
  author={Choukse, Esha and Sullivan, Michael B and O’Connor, Mike and Erez, Mattan and Pool, Jeff and Nellans, David and Keckler, Stephen W},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  pages={926--939},
  year={2020},
  organization={IEEE}
}
@article{kim2016bit,
  title={Bit-plane compression: Transforming data for better compression in many-core architectures},
  author={Kim, Jungrae and Sullivan, Michael and Choukse, Esha and Erez, Mattan},
  journal={ACM SIGARCH Computer Architecture News},
  volume={44},
  number={3},
  pages={329--340},
  year={2016},
  publisher={ACM New York, NY, USA}
}
@inproceedings{laghari2024memory,
  title={Memory Allocation Under Hardware Compression},
  author={Laghari, Muhammad and Liu, Yuqing and Panwar, Gagandeep and Bears, David and Jearls, Chandler and Srinivas, Raghavendra and Choukse, Esha and Cameron, Kirk W and Butt, Ali R and Jian, Xun},
  booktitle={2024 57th IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  pages={966--982},
  year={2024},
  organization={IEEE}
}
@inproceedings{nihaal2024selective,
  title={Selective Memory Compression for GPU Memory Oversubscription Management},
  author={Nihaal, Abdun and Mutyam, Madhu},
  booktitle={Proceedings of the 53rd International Conference on Parallel Processing},
  pages={189--198},
  year={2024}
}
@inproceedings{young2019enabling,
  title={Enabling transparent memory-compression for commodity memory systems},
  author={Young, Vinson and Kariyappa, Sanjay and Qureshi, Moinuddin K},
  booktitle={2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={570--581},
  year={2019},
  organization={IEEE}
}

@misc{suryavansh2020google,
  title={Google Coral Edge TPU Board Vs NVIDIA Jetson Nano Dev board Hardware Comparison},
  author={Suryavansh, Manu},
  year={2020}
}

% Added by Siddharth Joshi
@article{ashkboos2024quarot,
  title={Quarot: Outlier-free 4-bit inference in rotated llms},
  author={Ashkboos, Saleh and Mohtashami, Amirkeivan and Croci, Maximilian L and Li, Bo and Cameron, Pashmina and Jaggi, Martin and Alistarh, Dan and Hoefler, Torsten and Hensman, James},
  journal={arXiv preprint arXiv:2404.00456},
  year={2024}
}


@INPROCEEDINGS{nvidia_xformer,
  author={Keller, Ben and Venkatesan, Rangharajan and Dai, Steve and Tell, Stephen G. and Zimmer, Brian and Dally, William J. and Thomas Gray, C. and Khailany, Brucek},
  booktitle={2022 IEEE Symposium on VLSI Technology and Circuits (VLSI Technology and Circuits)}, 
  title={A 17–95.6 TOPS/W Deep Learning Inference Accelerator with Per-Vector Scaled 4-bit Quantization for Transformers in 5nm}, 
  year={2022},
  volume={},
  number={},
  pages={16-17},
  doi={10.1109/VLSITechnologyandCir46769.2022.9830277}}

@inproceedings{simba,
author = {Shao, Yakun Sophia and Clemons, Jason and Venkatesan, Rangharajan and Zimmer, Brian and Fojtik, Matthew and Jiang, Nan and Keller, Ben and Klinefelter, Alicia and Pinckney, Nathaniel and Raina, Priyanka and Tell, Stephen G. and Zhang, Yanqing and Dally, William J. and Emer, Joel and Gray, C. Thomas and Khailany, Brucek and Keckler, Stephen W.},
title = {Simba: Scaling Deep-Learning Inference with Multi-Chip-Module-Based Architecture},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358302},
doi = {10.1145/3352460.3358302},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {14–27},
numpages = {14},
keywords = {Multi-chip module, accelerator architecture, neural networks},
location = {Columbus, OH, USA},
series = {MICRO '52}
}

@inproceedings{nvdla,
  title={The NVIDIA deep learning accelerator},
  author={Sijstermans, Frans},
  booktitle={Hot Chips},
  volume={30},
  pages={19--21},
  year={2018}
}

@INPROCEEDINGS{magnet,
  author={Venkatesan, Rangharajan and Shao, Yakun Sophia and Wang, Miaorong and Clemons, Jason and Dai, Steve and Fojtik, Matthew and Keller, Ben and Klinefelter, Alicia and Pinckney, Nathaniel and Raina, Priyanka and Zhang, Yanqing and Zimmer, Brian and Dally, William J. and Emer, Joel and Keckler, Stephen W. and Khailany, Brucek},
  booktitle={2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)}, 
  title={MAGNet: A Modular Accelerator Generator for Neural Networks}, 
  year={2019},
  volume={},
  number={},
  pages={1-8},
  keywords={},
  doi={10.1109/ICCAD45719.2019.8942127}}


@misc{cerebras2024llama3,
  title = {Llama3.1 Model Quality Evaluation: Cerebras, Groq, SambaNova, Together, and Fireworks},
  author = {Vithursan Thangarasa},
  year = {2024},
  howpublished = {Blog post},
  url = {https://cerebras.ai/blog/},
  note = {Accessed: \today} 
}

@misc{sambanova2023reducedprecision,
  title = {Does Reduced Precision Hurt?},
  author = {Etash Guha},
  year = {2024}, 
  howpublished = {Blog post},
  url = {https://sambanova.ai/blog/does-reduced-precision-hurt},
  note = {Accessed: \today} 
}

@inproceedings{samsung_npu_isca,
author = {Jang, Jun-Woo and Lee, Sehwan and Kim, Dongyoung and Park, Hyunsun and Ardestani, Ali Shafiee and Choi, Yeongjae and Kim, Channoh and Kim, Yoojin and Yu, Hyeongseok and Abdel-Aziz, Hamzah and Park, Jun-Seok and Lee, Heonsoo and Lee, Dongwoo and Kim, Myeong Woo and Jung, Hanwoong and Nam, Heewoo and Lim, Dongguen and Lee, Seungwon and Song, Joon-Ho and Kwon, Suknam and Hassoun, Joseph and Lim, SukHwan and Choi, Changkyu},
title = {Sparsity-aware and re-configurable NPU architecture for samsung flagship mobile SoC},
year = {2021},
isbn = {9781450390866},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA52012.2021.00011},
doi = {10.1109/ISCA52012.2021.00011},
booktitle = {Proceedings of the 48th Annual International Symposium on Computer Architecture},
pages = {15–28},
numpages = {14},
keywords = {sparsity, re-configurable, neural processing unit, neural network, mixed-precision, accelerator},
location = {Virtual Event, Spain},
series = {ISCA '21}
}
@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}
@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}
@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}
@article{yang2024qwen2,
  title={Qwen2. 5 Technical Report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}
@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@INPROCEEDINGS{roofline2008david,
  author={Williams, Samuel and Patterson, David and Oliker, Leonid and Shalf, John and Yelick, Katherine},
  booktitle={2008 IEEE Hot Chips 20 Symposium (HCS)}, 
  title={The roofline model: A pedagogical tool for program analysis and optimization}, 
  year={2008},
  volume={},
  number={},
  pages={1-71},
  keywords={Optimization;Kernel;Multicore processing;Scalability},
  doi={10.1109/HOTCHIPS.2008.7476531}}

@article{chen2016eyeriss,
  title={Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks},
  author={Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel S and Sze, Vivienne},
  journal={IEEE journal of solid-state circuits},
  volume={52},
  number={1},
  pages={127--138},
  year={2016},
  publisher={IEEE}
}
