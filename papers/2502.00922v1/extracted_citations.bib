@INPROCEEDINGS{10764574,
  author={Yu, Zhongkai and Liang, Shengwen and Ma, Tianyun and Cai, Yunke and Nan, Ziyuan and Huang, Di and Song, Xinkai and Hao, Yifan and Zhang, Jie and Zhi, Tian and Zhao, Yongwei and Du, Zidong and Hu, Xing and Guo, Qi and Chen, Tianshi},
  booktitle={2024 57th IEEE/ACM International Symposium on Microarchitecture (MICRO)}, 
  title={Cambricon-LLM: A Chiplet-Based Hybrid Architecture for On-Device Inference of 70B LLM}, 
  year={2024},
  volume={},
  number={},
  pages={1474-1488},
  keywords={Microarchitecture;Large language models;Memory management;Reliability engineering;Market research;Error correction;Flash memories;Smart phones;Robots;Resilience;In-Flash Computing;Large Language Model Accelerator;Robotic Accelerator},
  doi={10.1109/MICRO61859.2024.00108}}

@INPROCEEDINGS{6983056,
  author={Pandiyan, Dhinakaran and Wu, Carole-Jean},
  booktitle={2014 IEEE International Symposium on Workload Characterization (IISWC)}, 
  title={Quantifying the energy cost of data movement for emerging smart phone workloads on mobile platforms}, 
  year={2014},
  volume={},
  number={},
  pages={171-180},
  keywords={Smart phones;Registers;Benchmark testing;Energy consumption;Energy measurement;Power measurement;Mobile communication},
  doi={10.1109/IISWC.2014.6983056}}

@INPROCEEDINGS{8351458,
  author={Gonugondla, Sujan K. and Kang, Mingu and Kim, Yongjune and Helm, Mark and Eilert, Sean and Shanbhag, Naresh},
  booktitle={2018 IEEE International Symposium on Circuits and Systems (ISCAS)}, 
  title={Energy-Efficient Deep In-memory Architecture for NAND Flash Memories}, 
  year={2018},
  volume={},
  number={},
  pages={1-5},
  keywords={Computer architecture;Transistors;Threshold voltage;Throughput;Support vector machines;Microprocessors;Flash memories},
  doi={10.1109/ISCAS.2018.8351458}}

@inproceedings{Albericio2023RETROSPECTIVECI,
  title={RETROSPECTIVE: Cnvlutin: Ineffectual-Neuron-Free Deep Neural Network Computing},
  author={Jorge Albericio and Patrick Judd and Tayler H. Hetherington and Tor M. Aamodt and Natalie D. Enright Jerger and Andreas Moshovos},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:9841314}
}

@article{Alwani2016FusedlayerCA,
  title={Fused-layer CNN accelerators},
  author={Manoj Alwani and Han Chen and Michael Ferdman and Peter Milder},
  journal={2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  year={2016},
  pages={1-12},
  url={https://api.semanticscholar.org/CorpusID:5804465}
}

@article{Chen2023MetaNMPLC,
  title={MetaNMP: Leveraging Cartesian-Like Product to Accelerate HGNNs with Near-Memory Processing},
  author={Dan Chen and Haiheng He and Hai Jin and Long Zheng and Yu Huang and Xinyang Shen and Xiaofei Liao},
  journal={Proceedings of the 50th Annual International Symposium on Computer Architecture},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:259177755}
}

@article{Dettmers2023QLoRAEF,
  title={QLoRA: Efficient Finetuning of Quantized LLMs},
  author={Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.14314},
  url={https://api.semanticscholar.org/CorpusID:258841328}
}

@article{Eckert2018NeuralCB,
  title={Neural Cache: Bit-Serial In-Cache Acceleration of Deep Neural Networks},
  author={Charles Eckert and Xiaowei Wang and Jingcheng Wang and Arun K. Subramaniyan and Ravi R. Iyer and Dennis Sylvester and David Blaauw and Reetuparna Das},
  journal={2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)},
  year={2018},
  pages={383-396},
  url={https://api.semanticscholar.org/CorpusID:13686490}
}

@inproceedings{Frantar2023OPTQAQ,
  title={OPTQ: Accurate Quantization for Generative Pre-trained Transformers},
  author={Elias Frantar and Saleh Ashkboos and Torsten Hoefler and Dan Alistarh},
  booktitle={International Conference on Learning Representations},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:259298689}
}

@article{Han2016EIEEI,
  title={EIE: Efficient Inference Engine on Compressed Deep Neural Network},
  author={Song Han and Xingyu Liu and Huizi Mao and Jing Pu and Ardavan Pedram and Mark Horowitz and William J. Dally},
  journal={2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)},
  year={2016},
  pages={243-254},
  url={https://api.semanticscholar.org/CorpusID:1663491}
}

@article{Ke2019RecNMPAP,
  title={RecNMP: Accelerating Personalized Recommendation with Near-Memory Processing},
  author={Liu Ke and Udit Gupta and Carole-Jean Wu and Benjamin Youngjae Cho and Mark Hempstead and Brandon Reagen and Xuan Zhang and David M. Brooks and Vikas Chandra and Utku Diril and Amin Firoozshahian and Kim M. Hazelwood and Bill Jia and Hsien-Hsin S. Lee and Mengxing Li and Bertrand A. Maher and Dheevatsa Mudigere and Maxim Naumov and Martin D. Schatz and Mikhail Smelyanskiy and Xiaodong Wang},
  journal={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  year={2019},
  pages={790-803},
  url={https://api.semanticscholar.org/CorpusID:209515444}
}

@article{Lin2023AWQAW,
  title={AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration},
  author={Ji Lin and Jiaming Tang and Haotian Tang and Shang Yang and Xingyu Dang and Song Han},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.00978},
  url={https://api.semanticscholar.org/CorpusID:271271084}
}

@article{Liu2023AcceleratingPR,
  title={Accelerating Personalized Recommendation with Cross-level Near-Memory Processing},
  author={Haifeng Liu and Long Zheng and Yu Huang and Chao Liu and Xiangyu Ye and Jingrui Yuan and Xiaofei Liao and Hai Jin and Jingling Xue},
  journal={Proceedings of the 50th Annual International Symposium on Computer Architecture},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:259177984}
}

@article{Lu2021SangerAC,
  title={Sanger: A Co-Design Framework for Enabling Sparse Attention using Reconfigurable Architecture},
  author={Liqiang Lu and Yicheng Jin and Hangrui Bi and Zizhang Luo and Peng Li and Tao Wang and Yun Liang},
  journal={MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:239012114}
}

@article{Wang2020SpAttenES,
  title={SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning},
  author={Hanrui Wang and Zhekai Zhang and Song Han},
  journal={2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  year={2020},
  pages={97-110},
  url={https://api.semanticscholar.org/CorpusID:229298088}
}

@article{Yuan2023RPTQRP,
  title={RPTQ: Reorder-based Post-training Quantization for Large Language Models},
  author={Zhihang Yuan and Lin Niu and Jia-Wen Liu and Wenyu Liu and Xinggang Wang and Yuzhang Shang and Guangyu Sun and Qiang Wu and Jiaxiang Wu and Bingzhe Wu},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.01089},
  url={https://api.semanticscholar.org/CorpusID:257913374}
}

@article{Zhang2016CambriconXAA,
  title={Cambricon-X: An accelerator for sparse neural networks},
  author={Shijin Zhang and Zidong Du and Lei Zhang and Huiying Lan and Shaoli Liu and Ling Li and Qi Guo and Tianshi Chen and Yunji Chen},
  journal={2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  year={2016},
  pages={1-12},
  url={https://api.semanticscholar.org/CorpusID:206464266}
}

@article{Zhang2021FASTDT,
  title={FAST: DNN Training Under Variable Precision Block Floating Point with Stochastic Rounding},
  author={Sai Qian Zhang and Bradley McDanel and H. T. Kung},
  journal={2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  year={2021},
  pages={846-860},
  url={https://api.semanticscholar.org/CorpusID:240288620}
}

@article{ashkboos2024quarot,
  title={Quarot: Outlier-free 4-bit inference in rotated llms},
  author={Ashkboos, Saleh and Mohtashami, Amirkeivan and Croci, Maximilian L and Li, Bo and Cameron, Pashmina and Jaggi, Martin and Alistarh, Dan and Hoefler, Torsten and Hensman, James},
  journal={arXiv preprint arXiv:2404.00456},
  year={2024}
}

@misc{cerebras2024llama3,
  title = {Llama3.1 Model Quality Evaluation: Cerebras, Groq, SambaNova, Together, and Fireworks},
  author = {Vithursan Thangarasa},
  year = {2024},
  howpublished = {Blog post},
  url = {https://cerebras.ai/blog/},
  note = {Accessed: \today} 
}

@article{chen2016eyeriss,
  title={Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks},
  author={Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel S and Sze, Vivienne},
  journal={IEEE journal of solid-state circuits},
  volume={52},
  number={1},
  pages={127--138},
  year={2016},
  publisher={IEEE}
}

@inproceedings{choukse2020buddy,
  title={Buddy compression: Enabling larger memory for deep learning and hpc workloads on gpus},
  author={Choukse, Esha and Sullivan, Michael B and Oâ€™Connor, Mike and Erez, Mattan and Pool, Jeff and Nellans, David and Keckler, Stephen W},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  pages={926--939},
  year={2020},
  organization={IEEE}
}

@article{dave2021hardware,
  title={Hardware acceleration of sparse and irregular tensor computations of ml models: A survey and insights},
  author={Dave, Shail and Baghdadi, Riyadh and Nowatzki, Tony and Avancha, Sasikanth and Shrivastava, Aviral and Li, Baoxin},
  journal={Proceedings of the IEEE},
  volume={109},
  number={10},
  pages={1706--1752},
  year={2021},
  publisher={IEEE}
}

@article{dettmers2022gpt3,
  title={Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30318--30332},
  year={2022}
}

@article{dutta2024accuracy,
  title={Accuracy is Not All You Need},
  author={Dutta, Abhinav and Krishnan, Sanjeev and Kwatra, Nipun and Ramjee, Ramachandran},
  journal={arXiv preprint arXiv:2407.09141},
  year={2024}
}

@article{frantar2022gptq,
  title={Gptq: Accurate post-training quantization for generative pre-trained transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}

@article{han2015deep,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={arXiv preprint arXiv:1510.00149},
  year={2015}
}

@article{hao2024neuzip,
  title={NeuZip: Memory-Efficient Training and Inference with Dynamic Compression of Neural Networks},
  author={Hao, Yongchang and Cao, Yanshuai and Mou, Lili},
  journal={arXiv preprint arXiv:2410.20650},
  year={2024}
}

@article{hershcovitch2024zipnn,
  title={ZipNN: Lossless Compression for AI Models},
  author={Hershcovitch, Moshik and Wood, Andrew and Choshen, Leshem and Girmonsky, Guy and Leibovitz, Roy and Ennmouri, Ilias and Malka, Michal and Chin, Peter and Sundararaman, Swaminathan and Harnik, Danny},
  journal={arXiv preprint arXiv:2411.05239},
  year={2024}
}

@article{hooper2024kvquant,
  title={Kvquant: Towards 10 million context length llm inference with kv cache quantization},
  author={Hooper, Coleman and Kim, Sehoon and Mohammadzadeh, Hiva and Mahoney, Michael W and Shao, Yakun Sophia and Keutzer, Kurt and Gholami, Amir},
  journal={arXiv preprint arXiv:2401.18079},
  year={2024}
}

@article{huffman1952method,
  title={A method for the construction of minimum-redundancy codes},
  author={Huffman, David A},
  journal={Proceedings of the IRE},
  volume={40},
  number={9},
  pages={1098--1101},
  year={1952},
  publisher={IEEE}
}

@inproceedings{jouppi2017datacenter,
  title={In-datacenter performance analysis of a tensor processing unit},
  author={Jouppi, Norman P and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and others},
  booktitle={Proceedings of the 44th annual international symposium on computer architecture},
  pages={1--12},
  year={2017}
}

@article{kim2016bit,
  title={Bit-plane compression: Transforming data for better compression in many-core architectures},
  author={Kim, Jungrae and Sullivan, Michael and Choukse, Esha and Erez, Mattan},
  journal={ACM SIGARCH Computer Architecture News},
  volume={44},
  number={3},
  pages={329--340},
  year={2016},
  publisher={ACM New York, NY, USA}
}

@article{kim2024breakthrough,
  title={The Breakthrough Memory Solutions for Improved Performance on LLM Inference},
  author={Kim, Byeongho and Cha, Sanghoon and Park, Sangsoo and Lee, Jieun and Lee, Sukhan and Kang, Shin-haeng and So, Jinin and Kim, Kyungsoo and Jung, Jin and Lee, Jong-Geon and others},
  journal={IEEE Micro},
  year={2024},
  publisher={IEEE}
}

@inproceedings{kung1979systolic,
  title={Systolic arrays (for VLSI)},
  author={Kung, Hsiang Tsung and Leiserson, Charles E},
  booktitle={Sparse Matrix Proceedings 1978},
  volume={1},
  pages={256--282},
  year={1979},
  organization={Society for industrial and applied mathematics Philadelphia, PA, USA}
}

@inproceedings{laghari2024memory,
  title={Memory Allocation Under Hardware Compression},
  author={Laghari, Muhammad and Liu, Yuqing and Panwar, Gagandeep and Bears, David and Jearls, Chandler and Srinivas, Raghavendra and Choukse, Esha and Cameron, Kirk W and Butt, Ali R and Jian, Xun},
  booktitle={2024 57th IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  pages={966--982},
  year={2024},
  organization={IEEE}
}

@article{lie2023cerebras,
  title={Cerebras architecture deep dive: First look inside the hardware/software co-design for deep learning},
  author={Lie, Sean},
  journal={IEEE Micro},
  volume={43},
  number={3},
  pages={18--30},
  year={2023},
  publisher={IEEE}
}

@article{lin2024awq,
  title={AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={87--100},
  year={2024}
}

@misc{lin2024qservew4a8kv4quantizationcodesign,
      title={QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving}, 
      author={Yujun Lin and Haotian Tang and Shang Yang and Zhekai Zhang and Guangxuan Xiao and Chuang Gan and Song Han},
      year={2024},
      eprint={2405.04532},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.04532}, 
}

@INPROCEEDINGS{magnet,
  author={Venkatesan, Rangharajan and Shao, Yakun Sophia and Wang, Miaorong and Clemons, Jason and Dai, Steve and Fojtik, Matthew and Keller, Ben and Klinefelter, Alicia and Pinckney, Nathaniel and Raina, Priyanka and Zhang, Yanqing and Zimmer, Brian and Dally, William J. and Emer, Joel and Keckler, Stephen W. and Khailany, Brucek},
  booktitle={2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)}, 
  title={MAGNet: A Modular Accelerator Generator for Neural Networks}, 
  year={2019},
  volume={},
  number={},
  pages={1-8},
  keywords={},
  doi={10.1109/ICCAD45719.2019.8942127}}

@article{marchisio2024does,
  title={How does quantization affect multilingual LLMs?},
  author={Marchisio, Kelly and Dash, Saurabh and Chen, Hongyu and Aumiller, Dennis and {\"U}st{\"u}n, Ahmet and Hooker, Sara and Ruder, Sebastian},
  journal={arXiv preprint arXiv:2407.03211},
  year={2024}
}

@INPROCEEDINGS{mecla,
  author={Qin, Yubin and Wang, Yang and Zhao, Zhiren and Yang, Xiaolong and Zhou, Yang and Wei, Shaojun and Hu, Yang and Yin, Shouyi},
  booktitle={2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA)}, 
  title={MECLA: Memory-Compute-Efficient LLM Accelerator with Scaling Sub-matrix Partition}, 
  year={2024},
  volume={},
  number={},
  pages={1032-1047},
  keywords={Large language models;Memory management;Graphics processing units;Benchmark testing;Transformers;Energy efficiency;System-on-chip;large language model;transformer;accelerator;artificial intelligence},
  doi={10.1109/ISCA59077.2024.00079}}

@article{mutlu2019processing,
  title={Processing data where it makes sense: Enabling in-memory computation},
  author={Mutlu, Onur and Ghose, Saugata and G{\'o}mez-Luna, Juan and Ausavarungnirun, Rachata},
  journal={Microprocessors and Microsystems},
  volume={67},
  pages={28--41},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{nihaal2024selective,
  title={Selective Memory Compression for GPU Memory Oversubscription Management},
  author={Nihaal, Abdun and Mutyam, Madhu},
  booktitle={Proceedings of the 53rd International Conference on Parallel Processing},
  pages={189--198},
  year={2024}
}

@inproceedings{nvdla,
  title={The NVIDIA deep learning accelerator},
  author={Sijstermans, Frans},
  booktitle={Hot Chips},
  volume={30},
  pages={19--21},
  year={2018}
}

@INPROCEEDINGS{nvidia_xformer,
  author={Keller, Ben and Venkatesan, Rangharajan and Dai, Steve and Tell, Stephen G. and Zimmer, Brian and Dally, William J. and Thomas Gray, C. and Khailany, Brucek},
  booktitle={2022 IEEE Symposium on VLSI Technology and Circuits (VLSI Technology and Circuits)}, 
  title={A 17â€“95.6 TOPS/W Deep Learning Inference Accelerator with Per-Vector Scaled 4-bit Quantization for Transformers in 5nm}, 
  year={2022},
  volume={},
  number={},
  pages={16-17},
  doi={10.1109/VLSITechnologyandCir46769.2022.9830277}}

@INPROCEEDINGS{sa-pipeline,
  author={Peltekis, C. and Filippas, D. and Dimitrakopoulos, G. and Nicopoulos, C. and Pnevmatikatos, D.},
  booktitle={2023 Design, Automation and Test in Europe Conference and Exhibition (DATE)}, 
  title={ArrayFlex: A Systolic Array Architecture with Configurable Transparent Pipelining}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  keywords={Time-frequency analysis;Scalability;Merging;Systolic arrays;Hardware;Convolutional neural networks;Kernel},
  doi={10.23919/DATE56975.2023.10136913}}

@misc{sambanova2023reducedprecision,
  title = {Does Reduced Precision Hurt?},
  author = {Etash Guha},
  year = {2024}, 
  howpublished = {Blog post},
  url = {https://sambanova.ai/blog/does-reduced-precision-hurt},
  note = {Accessed: \today} 
}

@INPROCEEDINGS{scalesim,
  author={Samajdar, Ananda and Joseph, Jan Moritz and Zhu, Yuhao and Whatmough, Paul and Mattina, Matthew and Krishna, Tushar},
  booktitle={2020 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)}, 
  title={A Systematic Methodology for Characterizing Scalability of DNN Accelerators using SCALE-Sim}, 
  year={2020},
  volume={},
  number={},
  pages={58-68},
  keywords={Analytical models;Systematics;Memory management;Random access memory;Bandwidth;Parallel processing;Systems modeling;Accelerator Simulator;DNN acclerator;Cycle accurate simulation;scaling analysis},
  doi={10.1109/ISPASS48437.2020.00016}}

@article{shannon1948mathematical,
  title={A mathematical theory of communication},
  author={Shannon, Claude Elwood},
  journal={The Bell system technical journal},
  volume={27},
  number={3},
  pages={379--423},
  year={1948},
  publisher={Nokia Bell Labs}
}

@inproceedings{simba,
author = {Shao, Yakun Sophia and Clemons, Jason and Venkatesan, Rangharajan and Zimmer, Brian and Fojtik, Matthew and Jiang, Nan and Keller, Ben and Klinefelter, Alicia and Pinckney, Nathaniel and Raina, Priyanka and Tell, Stephen G. and Zhang, Yanqing and Dally, William J. and Emer, Joel and Gray, C. Thomas and Khailany, Brucek and Keckler, Stephen W.},
title = {Simba: Scaling Deep-Learning Inference with Multi-Chip-Module-Based Architecture},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358302},
doi = {10.1145/3352460.3358302},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {14â€“27},
numpages = {14},
keywords = {Multi-chip module, accelerator architecture, neural networks},
location = {Columbus, OH, USA},
series = {MICRO '52}
}

@article{starsim,
author = {Sun, Xiaoyu and Peng, Xiaochen and Zhang, Sai Qian and Gomez, Jorge and Khwa, Win-San and Sarwar, Syed Shakib and Li, Ziyun and Cao, Weidong and Wang, Zhao and Liu, Chiao and Chang, Meng-Fan and De Salvo, Barbara and Akarvardar, Kerem and Wong, H.-S. Philip},
title = {Estimating Power, Performance, and Area for On-Sensor Deployment of AR/VR Workloads Using an Analytical Framework},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {6},
issn = {1084-4309},
url = {https://doi.org/10.1145/3670404},
doi = {10.1145/3670404},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = sep,
articleno = {93},
numpages = {27},
keywords = {Augmented reality, virtual reality, 3D CMOS image sensor, DNN accelerator}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@ARTICLE{tpu,
  author={Jouppi, Norman and Young, Cliff and Patil, Nishant and Patterson, David},
  journal={IEEE Micro}, 
  title={Motivation for and Evaluation of the First Tensor Processing Unit}, 
  year={2018},
  volume={38},
  number={3},
  pages={10-19},
  keywords={Neural networks;Graphics processing units;Tensile stress;Central Processing Unit;Energy efficiency;Data centers;Semiconductor devices;Microprocessors;microprocessor;tensor processing unit;deep neural network;GPU;machine learning;hardware},
  doi={10.1109/MM.2018.032271057}}

@inproceedings{tpuv4,
author = {Jouppi, Norm and Kurian, George and Li, Sheng and Ma, Peter and Nagarajan, Rahul and Nai, Lifeng and Patil, Nishant and Subramanian, Suvinay and Swing, Andy and Towles, Brian and Young, Clifford and Zhou, Xiang and Zhou, Zongwei and Patterson, David A},
title = {TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589350},
doi = {10.1145/3579371.3589350},
abstract = {In response to innovations in machine learning (ML) models, production workloads changed radically and rapidly. TPU v4 is the fifth Google domain specific architecture (DSA) and its third supercomputer for such ML models. Optical circuit switches (OCSes) dynamically reconfigure its interconnect topology to improve scale, availability, utilization, modularity, deployment, security, power, and performance; users can pick a twisted 3D torus topology if desired. Much cheaper, lower power, and faster than Infiniband, OCSes and underlying optical components are <5\% of system cost and <3\% of system power. Each TPU v4 includes SparseCores, dataflow processors that accelerate models that rely on embeddings by 5x--7x yet use only 5\% of die area and power. Deployed since 2020, TPU v4 outperforms TPU v3 by 2.1x and improves performance/Watt by 2.7x. The TPU v4 supercomputer is 4x larger at 4096 chips and thus nearly 10x faster overall, which along with OCS flexibility and availability allows a large language model to train at an average of ~60\% of peak FLOPS/second. For similar sized systems, it is ~4.3x--4.5x faster than the Graphcore IPU Bow and is 1.2x--1.7x faster and uses 1.3x--1.9x less power than the Nvidia A100. TPU v4s inside the energy-optimized warehouse scale computers of Google Cloud use ~2--6x less energy and produce ~20x less CO2e than contemporary DSAs in typical on-premise data centers.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {82},
numpages = {14},
keywords = {machine learning, domain specific architecture, TPU, GPU, IPU, supercomputer, optical interconnect, reconfigurable, embeddings, large language model, power usage effectiveness, warehouse scale computer, carbon emissions, energy, CO2 equivalent emissions},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{wang2019benchmarking,
  title={Benchmarking TPU, GPU, and CPU platforms for deep learning},
  author={Wang, Yu Emma and Wei, Gu-Yeon and Brooks, David},
  journal={arXiv preprint arXiv:1907.10701},
  year={2019}
}

@article{welch1984technique,
  title={A technique for high-performance data compression},
  author={Welch, Terry A.},
  journal={Computer},
  volume={17},
  number={06},
  pages={8--19},
  year={1984},
  publisher={IEEE Computer Society}
}

@article{yao2022zeroquant,
  title={Zeroquant: Efficient and affordable post-training quantization for large-scale transformers},
  author={Yao, Zhewei and Yazdani Aminabadi, Reza and Zhang, Minjia and Wu, Xiaoxia and Li, Conglong and He, Yuxiong},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27168--27183},
  year={2022}
}

@inproceedings{young2019enabling,
  title={Enabling transparent memory-compression for commodity memory systems},
  author={Young, Vinson and Kariyappa, Sanjay and Qureshi, Moinuddin K},
  booktitle={2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={570--581},
  year={2019},
  organization={IEEE}
}

@article{zhang2024does,
  title={Does your LLM truly unlearn? An embarrassingly simple approach to recover unlearned knowledge},
  author={Zhang, Zhiwei and Wang, Fali and Li, Xiaomin and Wu, Zongyu and Tang, Xianfeng and Liu, Hui and He, Qi and Yin, Wenpeng and Wang, Suhang},
  journal={arXiv preprint arXiv:2410.16454},
  year={2024}
}

