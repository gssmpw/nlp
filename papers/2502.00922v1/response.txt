Compared to traditional GPUs, TPUs are optimized for specific AI tasks and have a systolic array architecture that reduces area and power consumption. This makes them highly suitable for large-scale matrix multiplication operations found in transformer models. In contrast, GPUs provide more flexible parallelization but may face challenges meeting strict latency and power constraints.

Several works have proposed various lossless compression algorithms for neural networks accelerators, such as Bit-Plane Compression (BPC) [1], Buddy Compression [2], and Selective Memory Compression [3]. These methods can be categorized into three types: data transformation-based, metadata-based, and hybrid approaches. Among them, BPC is a novel algorithm that transforms the data and then applies run-length encoding and frequent pattern encoding to compress it.

In addition to these lossless compression algorithms, several works have proposed systems for practical memory compression. For example, Practical and Transparent Memory Compression [4] proposes an inline-metadata mechanism that determines the compressibility of a line by looking for a "marker" word.

While there are many advantages to using lossless compression in hardware, it is not without its challenges. Prior work has shown that applying lossless compression methods like RLE or sparse coding techniques can lead to memory management problems for the OS [5]. Therefore, researchers have developed interfaces for machine-physical memory that let the OS specify how much machine-physical memory it wants instead of indirectly allocating that memory through physical memory.

In recent years, there has been a growing interest in using lossless compression to reduce the memory and computation demands of DNNs. This approach is particularly appealing because it does not result in any loss of information from the original model. In contrast, sparsity and quantization methods can lead to some loss of information.

The development of large-scale language models (LLMs) has led to significant challenges in terms of memory requirements. For example, the Llama2-70B [6] model demands 140GB of memory under 16-bit floating point format. To address this issue, researchers have proposed various approaches, including using specialized accelerators that can handle large-scale matrix multiplication operations.

In conclusion, lossless compression in hardware is a promising approach for reducing the memory and computation demands of DNNs. While there are challenges associated with its implementation, researchers have made significant progress in developing practical solutions that can seamlessly integrate with various accelerators.



References:

[1] Bit-Plane Compression (BPC)

[2] Buddy Compression

[3] Selective Memory Compression

[4] Practical and Transparent Memory Compression

[5] Memory Management Problems for the OS

[6] Llama2-70B