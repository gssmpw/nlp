[
  {
    "index": 0,
    "papers": [
      {
        "key": "vaswani2017attention",
        "author": "Vaswani, A",
        "title": "Attention is all you need"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "hooper2024kvquant",
        "author": "Hooper, Coleman and Kim, Sehoon and Mohammadzadeh, Hiva and Mahoney, Michael W and Shao, Yakun Sophia and Keutzer, Kurt and Gholami, Amir",
        "title": "Kvquant: Towards 10 million context length llm inference with kv cache quantization"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "yao2022zeroquant",
        "author": "Yao, Zhewei and Yazdani Aminabadi, Reza and Zhang, Minjia and Wu, Xiaoxia and Li, Conglong and He, Yuxiong",
        "title": "Zeroquant: Efficient and affordable post-training quantization for large-scale transformers"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "lie2023cerebras",
        "author": "Lie, Sean",
        "title": "Cerebras architecture deep dive: First look inside the hardware/software co-design for deep learning"
      },
      {
        "key": "frantar2022gptq",
        "author": "Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan",
        "title": "Gptq: Accurate post-training quantization for generative pre-trained transformers"
      },
      {
        "key": "ashkboos2024quarot",
        "author": "Ashkboos, Saleh and Mohtashami, Amirkeivan and Croci, Maximilian L and Li, Bo and Cameron, Pashmina and Jaggi, Martin and Alistarh, Dan and Hoefler, Torsten and Hensman, James",
        "title": "Quarot: Outlier-free 4-bit inference in rotated llms"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "sambanova2023reducedprecision",
        "author": "Etash Guha",
        "title": "Does Reduced Precision Hurt?"
      },
      {
        "key": "cerebras2024llama3",
        "author": "Vithursan Thangarasa",
        "title": "Llama3.1 Model Quality Evaluation: Cerebras, Groq, SambaNova, Together, and Fireworks"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "zhang2024does",
        "author": "Zhang, Zhiwei and Wang, Fali and Li, Xiaomin and Wu, Zongyu and Tang, Xianfeng and Liu, Hui and He, Qi and Yin, Wenpeng and Wang, Suhang",
        "title": "Does your LLM truly unlearn? An embarrassingly simple approach to recover unlearned knowledge"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "marchisio2024does",
        "author": "Marchisio, Kelly and Dash, Saurabh and Chen, Hongyu and Aumiller, Dennis and {\\\"U}st{\\\"u}n, Ahmet and Hooker, Sara and Ruder, Sebastian",
        "title": "How does quantization affect multilingual LLMs?"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "dutta2024accuracy",
        "author": "Dutta, Abhinav and Krishnan, Sanjeev and Kwatra, Nipun and Ramjee, Ramachandran",
        "title": "Accuracy is Not All You Need"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "huffman1952method",
        "author": "Huffman, David A",
        "title": "A method for the construction of minimum-redundancy codes"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "shannon1948mathematical",
        "author": "Shannon, Claude Elwood",
        "title": "A mathematical theory of communication"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "han2015deep",
        "author": "Han, Song and Mao, Huizi and Dally, William J",
        "title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "chen2016eyeriss",
        "author": "Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel S and Sze, Vivienne",
        "title": "Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "hershcovitch2024zipnn",
        "author": "Hershcovitch, Moshik and Wood, Andrew and Choshen, Leshem and Girmonsky, Guy and Leibovitz, Roy and Ennmouri, Ilias and Malka, Michal and Chin, Peter and Sundararaman, Swaminathan and Harnik, Danny",
        "title": "ZipNN: Lossless Compression for AI Models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "hao2024neuzip",
        "author": "Hao, Yongchang and Cao, Yanshuai and Mou, Lili",
        "title": "NeuZip: Memory-Efficient Training and Inference with Dynamic Compression of Neural Networks"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "hershcovitch2024zipnn",
        "author": "Hershcovitch, Moshik and Wood, Andrew and Choshen, Leshem and Girmonsky, Guy and Leibovitz, Roy and Ennmouri, Ilias and Malka, Michal and Chin, Peter and Sundararaman, Swaminathan and Harnik, Danny",
        "title": "ZipNN: Lossless Compression for AI Models"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "dettmers2022gpt3",
        "author": "Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke",
        "title": "Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "frantar2022gptq",
        "author": "Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan",
        "title": "Gptq: Accurate post-training quantization for generative pre-trained transformers"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "lin2024awq",
        "author": "Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song",
        "title": "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "welch1984technique",
        "author": "Welch, Terry A.",
        "title": "A technique for high-performance data compression"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "sa-pipeline",
        "author": "Peltekis, C. and Filippas, D. and Dimitrakopoulos, G. and Nicopoulos, C. and Pnevmatikatos, D.",
        "title": "ArrayFlex: A Systolic Array Architecture with Configurable Transparent Pipelining"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "nvdla",
        "author": "Sijstermans, Frans",
        "title": "The NVIDIA deep learning accelerator"
      },
      {
        "key": "simba",
        "author": "Shao, Yakun Sophia and Clemons, Jason and Venkatesan, Rangharajan and Zimmer, Brian and Fojtik, Matthew and Jiang, Nan and Keller, Ben and Klinefelter, Alicia and Pinckney, Nathaniel and Raina, Priyanka and Tell, Stephen G. and Zhang, Yanqing and Dally, William J. and Emer, Joel and Gray, C. Thomas and Khailany, Brucek and Keckler, Stephen W.",
        "title": "Simba: Scaling Deep-Learning Inference with Multi-Chip-Module-Based Architecture"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "nvidia_xformer",
        "author": "Keller, Ben and Venkatesan, Rangharajan and Dai, Steve and Tell, Stephen G. and Zimmer, Brian and Dally, William J. and Thomas Gray, C. and Khailany, Brucek",
        "title": "A 17\u201395.6 TOPS/W Deep Learning Inference Accelerator with Per-Vector Scaled 4-bit Quantization for Transformers in 5nm"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "magnet",
        "author": "Venkatesan, Rangharajan and Shao, Yakun Sophia and Wang, Miaorong and Clemons, Jason and Dai, Steve and Fojtik, Matthew and Keller, Ben and Klinefelter, Alicia and Pinckney, Nathaniel and Raina, Priyanka and Zhang, Yanqing and Zimmer, Brian and Dally, William J. and Emer, Joel and Keckler, Stephen W. and Khailany, Brucek",
        "title": "MAGNet: A Modular Accelerator Generator for Neural Networks"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "wang2019benchmarking",
        "author": "Wang, Yu Emma and Wei, Gu-Yeon and Brooks, David",
        "title": "Benchmarking TPU, GPU, and CPU platforms for deep learning"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "wang2019benchmarking",
        "author": "Wang, Yu Emma and Wei, Gu-Yeon and Brooks, David",
        "title": "Benchmarking TPU, GPU, and CPU platforms for deep learning"
      },
      {
        "key": "tpu",
        "author": "Jouppi, Norman and Young, Cliff and Patil, Nishant and Patterson, David",
        "title": "Motivation for and Evaluation of the First Tensor Processing Unit"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "kung1979systolic",
        "author": "Kung, Hsiang Tsung and Leiserson, Charles E",
        "title": "Systolic arrays (for VLSI)"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "jouppi2017datacenter",
        "author": "Jouppi, Norman P and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and others",
        "title": "In-datacenter performance analysis of a tensor processing unit"
      },
      {
        "key": "tpuv4",
        "author": "Jouppi, Norm and Kurian, George and Li, Sheng and Ma, Peter and Nagarajan, Rahul and Nai, Lifeng and Patil, Nishant and Subramanian, Suvinay and Swing, Andy and Towles, Brian and Young, Clifford and Zhou, Xiang and Zhou, Zongwei and Patterson, David A",
        "title": "TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "dave2021hardware",
        "author": "Dave, Shail and Baghdadi, Riyadh and Nowatzki, Tony and Avancha, Sasikanth and Shrivastava, Aviral and Li, Baoxin",
        "title": "Hardware acceleration of sparse and irregular tensor computations of ml models: A survey and insights"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "starsim",
        "author": "Sun, Xiaoyu and Peng, Xiaochen and Zhang, Sai Qian and Gomez, Jorge and Khwa, Win-San and Sarwar, Syed Shakib and Li, Ziyun and Cao, Weidong and Wang, Zhao and Liu, Chiao and Chang, Meng-Fan and De Salvo, Barbara and Akarvardar, Kerem and Wong, H.-S. Philip",
        "title": "Estimating Power, Performance, and Area for On-Sensor Deployment of AR/VR Workloads Using an Analytical Framework"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "scalesim",
        "author": "Samajdar, Ananda and Joseph, Jan Moritz and Zhu, Yuhao and Whatmough, Paul and Mattina, Matthew and Krishna, Tushar",
        "title": "A Systematic Methodology for Characterizing Scalability of DNN Accelerators using SCALE-Sim"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "Wang2020SpAttenES",
        "author": "Hanrui Wang and Zhekai Zhang and Song Han",
        "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning"
      }
    ]
  },
  {
    "index": 31,
    "papers": [
      {
        "key": "kim2016bit",
        "author": "Kim, Jungrae and Sullivan, Michael and Choukse, Esha and Erez, Mattan",
        "title": "Bit-plane compression: Transforming data for better compression in many-core architectures"
      }
    ]
  },
  {
    "index": 32,
    "papers": [
      {
        "key": "choukse2020buddy",
        "author": "Choukse, Esha and Sullivan, Michael B and O\u2019Connor, Mike and Erez, Mattan and Pool, Jeff and Nellans, David and Keckler, Stephen W",
        "title": "Buddy compression: Enabling larger memory for deep learning and hpc workloads on gpus"
      }
    ]
  },
  {
    "index": 33,
    "papers": [
      {
        "key": "nihaal2024selective",
        "author": "Nihaal, Abdun and Mutyam, Madhu",
        "title": "Selective Memory Compression for GPU Memory Oversubscription Management"
      }
    ]
  },
  {
    "index": 34,
    "papers": [
      {
        "key": "laghari2024memory",
        "author": "Laghari, Muhammad and Liu, Yuqing and Panwar, Gagandeep and Bears, David and Jearls, Chandler and Srinivas, Raghavendra and Choukse, Esha and Cameron, Kirk W and Butt, Ali R and Jian, Xun",
        "title": "Memory Allocation Under Hardware Compression"
      }
    ]
  },
  {
    "index": 35,
    "papers": [
      {
        "key": "young2019enabling",
        "author": "Young, Vinson and Kariyappa, Sanjay and Qureshi, Moinuddin K",
        "title": "Enabling transparent memory-compression for commodity memory systems"
      }
    ]
  },
  {
    "index": 36,
    "papers": [
      {
        "key": "Alwani2016FusedlayerCA",
        "author": "Manoj Alwani and Han Chen and Michael Ferdman and Peter Milder",
        "title": "Fused-layer CNN accelerators"
      },
      {
        "key": "Han2016EIEEI",
        "author": "Song Han and Xingyu Liu and Huizi Mao and Jing Pu and Ardavan Pedram and Mark Horowitz and William J. Dally",
        "title": "EIE: Efficient Inference Engine on Compressed Deep Neural Network"
      },
      {
        "key": "Zhang2021FASTDT",
        "author": "Sai Qian Zhang and Bradley McDanel and H. T. Kung",
        "title": "FAST: DNN Training Under Variable Precision Block Floating Point with Stochastic Rounding"
      },
      {
        "key": "Eckert2018NeuralCB",
        "author": "Charles Eckert and Xiaowei Wang and Jingcheng Wang and Arun K. Subramaniyan and Ravi R. Iyer and Dennis Sylvester and David Blaauw and Reetuparna Das",
        "title": "Neural Cache: Bit-Serial In-Cache Acceleration of Deep Neural Networks"
      }
    ]
  },
  {
    "index": 37,
    "papers": [
      {
        "key": "Chen2023MetaNMPLC",
        "author": "Dan Chen and Haiheng He and Hai Jin and Long Zheng and Yu Huang and Xinyang Shen and Xiaofei Liao",
        "title": "MetaNMP: Leveraging Cartesian-Like Product to Accelerate HGNNs with Near-Memory Processing"
      },
      {
        "key": "Ke2019RecNMPAP",
        "author": "Liu Ke and Udit Gupta and Carole-Jean Wu and Benjamin Youngjae Cho and Mark Hempstead and Brandon Reagen and Xuan Zhang and David M. Brooks and Vikas Chandra and Utku Diril and Amin Firoozshahian and Kim M. Hazelwood and Bill Jia and Hsien-Hsin S. Lee and Mengxing Li and Bertrand A. Maher and Dheevatsa Mudigere and Maxim Naumov and Martin D. Schatz and Mikhail Smelyanskiy and Xiaodong Wang",
        "title": "RecNMP: Accelerating Personalized Recommendation with Near-Memory Processing"
      },
      {
        "key": "Liu2023AcceleratingPR",
        "author": "Haifeng Liu and Long Zheng and Yu Huang and Chao Liu and Xiangyu Ye and Jingrui Yuan and Xiaofei Liao and Hai Jin and Jingling Xue",
        "title": "Accelerating Personalized Recommendation with Cross-level Near-Memory Processing"
      }
    ]
  },
  {
    "index": 38,
    "papers": [
      {
        "key": "Albericio2023RETROSPECTIVECI",
        "author": "Jorge Albericio and Patrick Judd and Tayler H. Hetherington and Tor M. Aamodt and Natalie D. Enright Jerger and Andreas Moshovos",
        "title": "RETROSPECTIVE: Cnvlutin: Ineffectual-Neuron-Free Deep Neural Network Computing"
      },
      {
        "key": "Lu2021SangerAC",
        "author": "Liqiang Lu and Yicheng Jin and Hangrui Bi and Zizhang Luo and Peng Li and Tao Wang and Yun Liang",
        "title": "Sanger: A Co-Design Framework for Enabling Sparse Attention using Reconfigurable Architecture"
      },
      {
        "key": "Wang2020SpAttenES",
        "author": "Hanrui Wang and Zhekai Zhang and Song Han",
        "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning"
      },
      {
        "key": "Zhang2016CambriconXAA",
        "author": "Shijin Zhang and Zidong Du and Lei Zhang and Huiying Lan and Shaoli Liu and Ling Li and Qi Guo and Tianshi Chen and Yunji Chen",
        "title": "Cambricon-X: An accelerator for sparse neural networks"
      },
      {
        "key": "Lu2021SangerAC",
        "author": "Liqiang Lu and Yicheng Jin and Hangrui Bi and Zizhang Luo and Peng Li and Tao Wang and Yun Liang",
        "title": "Sanger: A Co-Design Framework for Enabling Sparse Attention using Reconfigurable Architecture"
      }
    ]
  },
  {
    "index": 39,
    "papers": [
      {
        "key": "Yuan2023RPTQRP",
        "author": "Zhihang Yuan and Lin Niu and Jia-Wen Liu and Wenyu Liu and Xinggang Wang and Yuzhang Shang and Guangyu Sun and Qiang Wu and Jiaxiang Wu and Bingzhe Wu",
        "title": "RPTQ: Reorder-based Post-training Quantization for Large Language Models"
      },
      {
        "key": "Frantar2023OPTQAQ",
        "author": "Elias Frantar and Saleh Ashkboos and Torsten Hoefler and Dan Alistarh",
        "title": "OPTQ: Accurate Quantization for Generative Pre-trained Transformers"
      },
      {
        "key": "Lin2023AWQAW",
        "author": "Ji Lin and Jiaming Tang and Haotian Tang and Shang Yang and Xingyu Dang and Song Han",
        "title": "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration"
      },
      {
        "key": "Dettmers2023QLoRAEF",
        "author": "Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer",
        "title": "QLoRA: Efficient Finetuning of Quantized LLMs"
      }
    ]
  },
  {
    "index": 40,
    "papers": [
      {
        "key": "touvron2023llama",
        "author": "Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others",
        "title": "Llama 2: Open foundation and fine-tuned chat models"
      }
    ]
  },
  {
    "index": 41,
    "papers": [
      {
        "key": "10764574",
        "author": "Yu, Zhongkai and Liang, Shengwen and Ma, Tianyun and Cai, Yunke and Nan, Ziyuan and Huang, Di and Song, Xinkai and Hao, Yifan and Zhang, Jie and Zhi, Tian and Zhao, Yongwei and Du, Zidong and Hu, Xing and Guo, Qi and Chen, Tianshi",
        "title": "Cambricon-LLM: A Chiplet-Based Hybrid Architecture for On-Device Inference of 70B LLM"
      }
    ]
  },
  {
    "index": 42,
    "papers": [
      {
        "key": "8351458",
        "author": "Gonugondla, Sujan K. and Kang, Mingu and Kim, Yongjune and Helm, Mark and Eilert, Sean and Shanbhag, Naresh",
        "title": "Energy-Efficient Deep In-memory Architecture for NAND Flash Memories"
      },
      {
        "key": "mutlu2019processing",
        "author": "Mutlu, Onur and Ghose, Saugata and G{\\'o}mez-Luna, Juan and Ausavarungnirun, Rachata",
        "title": "Processing data where it makes sense: Enabling in-memory computation"
      },
      {
        "key": "6983056",
        "author": "Pandiyan, Dhinakaran and Wu, Carole-Jean",
        "title": "Quantifying the energy cost of data movement for emerging smart phone workloads on mobile platforms"
      }
    ]
  },
  {
    "index": 43,
    "papers": [
      {
        "key": "kim2024breakthrough",
        "author": "Kim, Byeongho and Cha, Sanghoon and Park, Sangsoo and Lee, Jieun and Lee, Sukhan and Kang, Shin-haeng and So, Jinin and Kim, Kyungsoo and Jung, Jin and Lee, Jong-Geon and others",
        "title": "The Breakthrough Memory Solutions for Improved Performance on LLM Inference"
      }
    ]
  },
  {
    "index": 44,
    "papers": [
      {
        "key": "lin2024qservew4a8kv4quantizationcodesign",
        "author": "Yujun Lin and Haotian Tang and Shang Yang and Zhekai Zhang and Guangxuan Xiao and Chuang Gan and Song Han",
        "title": "QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"
      },
      {
        "key": "Wang2020SpAttenES",
        "author": "Hanrui Wang and Zhekai Zhang and Song Han",
        "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning"
      }
    ]
  },
  {
    "index": 45,
    "papers": [
      {
        "key": "mecla",
        "author": "Qin, Yubin and Wang, Yang and Zhao, Zhiren and Yang, Xiaolong and Zhou, Yang and Wei, Shaojun and Hu, Yang and Yin, Shouyi",
        "title": "MECLA: Memory-Compute-Efficient LLM Accelerator with Scaling Sub-matrix Partition"
      }
    ]
  }
]