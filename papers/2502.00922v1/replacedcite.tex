\section{Background and Related Work}
%We begin by covering relevant background for \textsc{Huff-LLM} and related work from literature. We first discuss existing LLM model compression methods, followed by work on 
%hardware accelerators for LLM inference. 
% \py{\begin{itemize}
%     %\item Transformer Overview
%     %\item Briefly discuss lossy compression (quantization)?
%     % \item LLM Weight Formats (Done)
%     % \item Lossless Compression
%     % \begin{itemize}
%     %     \item Overview of huffman coding (Done)
%     %     \item Huffman Coding Tree Figure
%     %     \item Discuss ZipNN, NeuZip and how they're "off-chip" decompression (Done)
%     %     \begin{itemize}
%     %         \item Sacrifices performance (latency/throughput) to load larger models(Done)
%     %     \end{itemize}
%     % \end{itemize}
%     \item Hardware Section
%     \begin{itemize}
%         \item TPUs vs. GPUs 
%         \item Systolic Array (Simulators) (Done)
%         \item Existing Accelerators
%         \item Associated Costs
%         \begin{itemize}
%             \item Latency due to IO bottleneck
%             \item Energy Costs
%             \item Decoding Cost (Why is 16-bit huffman slow)
%         \end{itemize}
%     \end{itemize}
% \end{itemize}}

%\begin{comment}   
%the comment command doesn't seem to be working
%\paragraph{Transformers}
%Most modern large language models are built on top of the transformer architecture introduced in ____. One of the key features of the transformer architecture is the large number of weight matrices used during inference. For example: Llama 3-8B contains 32 transformer layers. The majority of the parameters in these layers are dedicated to weight matrices found in the attention operation and the multi-layer perceptron. There are four weight matrices found in the attention operation with each boasting a size of [4096 x 4096] or [4096 x 1024] parameters. There are also three weight matrices found in the multi-layer perceptron which have sizes of [4096 x 14336] parameters. One can easily see how the size of these parameters adds up quickly leading to a very large model (~16 GB) that needs to be loaded into memory for quick and efficient inference. This has led to a massive influx of model compression papers which focus on reducing the size of the model such that it can be loaded into smaller devices/graphics cards. 

%An additional consideration for the size of large language models can be found in the attention operation. Due to the auto-regressive nature of transformers, one needs to repeat the matrix multiplications in the attention operation for the same input tokens many times. In order to save on computational costs, the kv-cache was developed as a way of storing these values in between forward passes. However, this further increases the amount of memory our model takes up in proportion to the context length. Recent works have shown that as the context length increases beyond a certain point, the primary memory bottleneck becomes the kv-cache rather than the model weights____. We will focus on the lossless compressibility of the weight matrices in this work. 

\subsection{LLM Model Compression}

%Since our primary goal is weight compression, we begin by describing 
%commonly used formats used to store uncompressed weights.

\paragraph{LLM Weight Formats.} LLM weights are broadly stored in either floating point or integer formats during inference.
The most common floating point formats are 32-bit floating point (FP32), 16-bit floating point (FP16), and 16-bit brain float (BF16). 
%Each of these formats allocates a different number of 
%bits for the MantissaExponent
In FP16, for instance, the most significant bit (MSB) is the {sign} (S) of the number, the next five bits are the {exponent} (E), and the last ten bits are the {mantissa} (M). 
Any FP16 weight is then represented as: %addition, a \textbf{bias} (B) is assigned for each type of Floating Point container. 
%For example, B=15 for FP16. These values are then used to represent a number such that:
\begin{equation}
    \text{Value} = (-1)^S \times 2^{E-B} \times (1.M)
\end{equation}
where $B$ is a fixed bias term, commonly set to $B=15$. In this format, mantissa bits only encode 
the fractional value after the decimal point ($1.M$).
FP32 is similar, but allocates 8 bits to the exponent and 23 bits to the mantissa. 
BF16, introduced as a compromise between FP32 and FP16, has an 8-bit exponent, 
a 7-bit mantissa, and a sign bit. 
BF16 has a larger dynamic range compared to FP16, but lower precision within this range compared
to FP16.

To reduce model size, 8-bit integer (INT8) and 4-bit integer (INT4) formats were introduced.
These represent weights as 
signed 2's-complement integers. Decimals are represented using 
a scaling factor typically associated with an entire tensor or channels within it.
%The containers use a certain number of bits to hold each weight parameter (as described in each container's name). 
%In addition, each container type has its own way of representing the number in bits. 
INT8 and INT4 representations are typically obtained via quantization methods applied to models 
stored in FP16 or BF16 formats. These methods are discussed next.
%These methods aim to reduce the precision of weight parameters by reducing the number of bits allocated to each parameter. These 
%are discussed next. 
%Thus, a new container is needed for these smaller parameters. INT8 and INT4 aim to do precisely that. Parameters are stored in their quantized forms in integer containers before they need to be de-quantized. 

\paragraph{LLM Weight Quantization} 
%\sj{I took a stab at this below}
\textcolor{black}{As LLM sizes have grown, their associated workloads often become expensive in real-world applications. Techniques to reduce the memory footprint and computation precision for these models has been driven by the need to serve these models at scale, or run larger models locally on compute-limited resources. Quantization-aware training (QAT), has demonstrated the highest accuracy for most models; in QAT, precision reduction through quantization is included within the training loop, often requiring that the training pipeline and training data be accessed during the quantization process. Usually, QAT is incorporated within the fine-tuning phase____, resulting in significant training overhead cost; these costs are particularly exacerbated for large-scale models. Post-training Quantization (PTQ), in contrast, quantizes existing pre-trained models, avoiding the need to retrain the model. The PTQ approach avoids many privacy hurdles associated with access to pre-training, enabling third parties to modify open-weight models and serve them efficiently on various hardware platforms____.}
% @chinmay -- I'll edit other stuff, if you want to take this over?


However, while post-training quantization can reduce inference costs, reducing model precision is often associated with unintended consequences____. Research such as____ has shown that post-hoc quantization can have adverse effects on model alignment, and can be used to mitigate ``unlearning'' procedures that are applied to LLMs as copyright or safety filters. Quantization applied to multi-lingual LLMs have disparate effects on low-resource languages, particularly those that use non-Latin scripts____. These adverse affects also arise in QAT-trained models; even though final test accuracy is similar, performance of quantized LLMs can be significantly worse on complex tasks such as multi-turn dialog on standard benchmarks____.   
%\textcolor{red}{we need a super strong para on LLM weight quant. I would highlight the diff between post-training quant and quant aware methods; latter are better but more expensive. Note major papers and that they have matched FP16 accuracy/perplexity, especially for INT8; INT4 does have accuracy loss.} \py{I can write this}

% \paragraph{Quantized Model Evaluations} 
% \textcolor{red}{Discuss studies showing safety/bias/security concerns.} \py{I can write this}

%Include a Huffman Coding primer, shorter/more compact than the one below
%\paragraph{Huffman Coding}
%\paragraph{Huffman Coding}
%Huffman coding____ is an entropy coding method that aims to compress a sequence of "source symbols" down to their entropy lower bound as defined by Shanon's source coding theorem____. At a high level, Huffman coding creates variable length code words for every source symbol in the dataset you want to compress. We first calculate the probability of every source symbol in our dataset. Given this distribution, we can now assign the shortest code word to the most frequent source symbol. We continue this process until every source symbol has a unique code word. We then compress the input with a simple dictionary search; for each source symbol in our input we look up its code word in our Huffman Table and replace that source symbol with its code word. Decompression happens in a similar, but reverse manner. One important thing to note for decompression is that the code words are a prefix code. This means that they are chosen in such a way that no code is a prefix of another allowing for unambiguous decoding. 
%We provide an example Huffman Table in Table~\ref{tab:huffman_example}. In this table you can see the prefix code nature of the selected code words. In addition, you can see how the code word lengths get progressively larger which is how you generate new code words without encroaching on the prefix code property.

%\begin{comment}

%\begin{table}[ht]
%    \centering
%    \label{tab:huffman_example}
%    \resizebox{\columnwidth}{!}{%
%        \begin{tabular}{@{}cc@{}} \toprule
%        Code Word & Source Symbol\\ \midrule
%        01 & 10100 \\
%        10 & 00100 \\
%        110 & 10011 \\
%        111 & 00011 \\
%        0000 & 10101 \\
%        $\cdots$ & $\cdots$ \\
%        00110101001 & 10110 \\
%        \bottomrule
%        \end{tabular}
%    }
 %       \caption{\sl This Huffman Table describes the source symbol and the code word assigned to the source symbol. The source symbol always has the same length (5 bits), but the Huffman code word length is variable.}
%\end{table}


%\end{comment}

%\textcolor{red}{Cover FP16; BF16; Int8 and Int4. Basic transformers tutorial not needed. For Huffman coding, might help but need a picture of a Huffman tree.}

\paragraph{Lossless Compression of LLM Weights}
Compared to the large body of work on 
lossy compression of LLM weights, there is relatively little work on lossless compression. 
An early paper____ proposed Huffman compression of 
convolutional neural network (CNN) weights, naively compressing entire 16-bit weights which incurs large performance overheads. For this reason, they do not actually implement Huffman coding, and instead only use 
run-length encoding (RLE) of sequences of zero weights. RLE is subsequently 
implemented in several other works, especially for CNNs that have sparse weight tensors____. %\textcolor{red}{is it end-to-end?} \sj{could you explain what you mean here? RLE was a part of eyeriss for e.g.}
In contrast, \textsc{Huff-LLM} proposes a lightweight 
hardware-friendly implementation of Huffman coding, 
integrates within 
systolic array and vector LLM accelerator architectures, demonstrating 
substantial performance and energy benefits. 

%This is because they propose to 
Two recent works have addressed Huffman compression for LLMs: ____ propose to compress the exponent bits of weights via Huffman coding
%These compressed weights are then used 
to reduce the cost of storing and downloading LLMs on cloud servers. For FP16 and BF16 models, they are able to achieve 17 - 33\% compression with higher compression ratios coming from BF16 models due to the larger number of exponent bits. 
%However, the LLMs need to be uncompressed on the hard drive before being loaded into a GPU/TPU. 
____ use a similar approach by applying asymmetric numeral systems (ANS), an entropy coding method,  to the exponent bits. In addition, they load the compressed weights to the GPU/TPU and thus achieve memory savings over ____ during inference. They are able to achieve 33\% lossless compression on BF16 models, but also suffer a 33\% inference slow down due to decompression.

%Quantization is one of the most popular approaches to large language model compression. This approach focuses on representing the parameters of a model with lower precision which leads to memory savings. Popular methods include LLM Int8____, GPTQ____, and AWQ____. We will not focus on comparisons with quantization methods as the lossy and lossless setting are significantly different. 

Note that other lossless compression schemes exist. RLE, mentioned previously, 
exploits spatial correlations between inputs by encoding a sequence of identical
weights as the weight value followed by the number of occurrences. 
LZW, a more sophisticated variant, exploits commonly occurring patterns in the input data____. Both can be implemented synergistically after Huffman coding of individual weight values. We leave an evaluation of these methods as future work, but note that these incur additional hardware costs.
%\textcolor{red}{one sentence explanation}  

\subsection{Hardware Accelerators for LLM Inference}
%Next three sections can be condensed, moved into a later part (methodology/experiment setup?)
%Comment for now?
\label{sec:systolic_array}
\paragraph{Systolic Array Architectures} %\textcolor{red}{explain what a systolic array is --- important to emphasize the need for data to be fed into the array without stalls. probably need a figure?}
The systolic array (SA) architecture, shown in Figure~\ref{fig:systolic_array_diagram} consists of an array of processing elements (PEs) that perform multiply-and-accumulate (MAC)
operations, 
surrounded by on-chip buffers for data storage. Weights and activations are fetched from the weight buffer and activation buffer to the PEs, respectively. 
Data is streamed in from these buffers in a 
highly synchronized fashion such that each PE 
computes the dot product of a row of activations with a column of weights. 
%The Accumulator Buffer manages the accumulation of partial sums. Final sums are then sent to the post-processing unit, and the outputs will be written back to the activation buffer. 
%Data movement takes place in every cycle due to the pipeline structure. 
However, 
%if there are any stalls in the systolic array pipeline to
it is crucial to maintain an uninterrupted data flow for correct performance of the systolic array, as
any stalls or bubbles cause either incorrect computation or incur large performance penalties____.
This underscores the necessity of a Huffman decoder that operates without stalling the data stream (See Fig.~\ref{fig:systolic_array_diagram}). 
Note that the description above is for an "output stationary" (OS) systolic array. A slightly different architecture, referred to as weight stationary (WS) stores weights inside each PE and only streams in activations such that the output of each column produces a dot-product of a column of weights with activations. 
%Accumulation buffers at the outputs are needed 
%\textcolor{red}{text needs work+image} \textcolor{blue}{Updated}

%The Accumulator Buffer manages the accumulation of partial sums. Final sums are then sent to the post-processing unit, and the outputs will be written back to the activation buffer. When on-chip buffers are not sufficient to accommodate the weights or activations, weights/activations are read from DRAM and activations are then written back to DRAM. 

\paragraph{Simba Vector Architectures} %\textcolor{red}{para describing SIMBA---ND folks? Arch pic?} 

To ensure the generality of our approach, we extended our evaluations to a parallel vector-processing optimized accelerator based on NVIDIA's production-tested NVIDIA Deep Learning Accelerator (NVDLA) architecture____. The hardware model incorporates NVDLA dataflow-optimizations that reduce data-movement for transformers____. Our evaluations use a single chiplet with an array of 16$\times$16 Processing Elements (PEs) and a shared global buffer for activation storage (See Fig.~\ref{fig:simba-like_arch_diagram}). Each PE features dedicated local scratchpads for weights, inputs, and partial sums, along with vector multiply-accumulate (VMAC) units for parallel computation. The architecture is optimized for a `local-weight-stationary' dataflow (where weights remain fixed in local memory to minimize data movement)____, operates at a nominal frequency of 2 GHz, and connects to external LPDDR4 DRAM via a 128 GB/s interface. For ease of reproducibility, detailed hardware specifications are provided in Table~\ref{Simba_specs}.
%\textcolor{red}{text needs some refinement. SJ: could you take  a pass?} \sj{done}

% \begin{figure}
%     \centering
% \includegraphics[width=0.8\columnwidth]{figures/simba-like.pdf} 
%     \vspace{-0.1in}
%     \caption{Simba-like architecture diagram. Colors are assosiated to different operand. Weights are yellow, inputs are blue, and partial-sum/outputs are pink. \textcolor{red}{Patrick: I would add the systolic array picture here in the same diagram, just above Simba.} } 
%     \label{fig:simba-like_arch_diagram} 
% \end{figure}

\begin{figure}[h] % Keeps it in one column
    \centering
    \includegraphics[width=\linewidth]{figures/systolic_array.pdf} % First figure
    \caption{Systolic array diagram with a detailed look at the PE. Weights and activations are sent to the PEs at every clock cycle. Bubbles indicate delays which are necessary to maintain accuracy of the computations performed by the output stationary architecture. Colors are associated to different operands. Weights are yellow, inputs are light blue, and partial-sum/outputs are pink}
    \label{fig:systolic_array_diagram}

    \vspace{1em} % Space between figures

    \includegraphics[width=\linewidth]{figures/simba.pdf} % Second figure
    \caption{Simba-like architecture diagram with a detailed look at the PE. Colors are associated to different operands. Weights are yellow, inputs are light blue, and partial-sum/outputs are pink.}
    \label{fig:simba-like_arch_diagram}
\end{figure}



% \begin{figure}
%     \centering
% \includegraphics[width=\columnwidth]{figures/arch_diagrams.pdf} 
%     \vspace{-0.1in}
%     \caption{Systolic array (top) and Simba-like (bottom) architecture diagrams. Colors are associated to different operand. Weights are yellow, inputs are blue, and partial-sum/outputs are pink. Bubbles indicate delays which are necessary to maintain accuracy of the computations performed by the output stationary architecture.} 
%     \label{fig:simba-like_arch_diagram} 
% \end{figure}

%\paragraph{Hardware Support for Weight Compression}

%\paragraph{TPU vs GPU}
%Graphics Processing Units (GPUs) and Tensor Processing Units (TPUs) are two types of accelerators specifically designed to handle complex computational tasks efficiently. GPUs provide more flexible parallelization____, while TPUs are more customized for specific AI tasks. Prior research____ has shown TPU is highly optimized for large size matrix multiplication because the systolic array architecture reduce area and power of the large matrix multiply unit____. GPUs have traditionally been regarded as high-throughput architectures, leveraging high-bandwidth DRAM and thousands of parallel threads to deliver optimal performance. The throughput-oriented architectural design of GPUs presents potential challenges in meeting strict latency and power constraints____. Furthermore, systolic array architecture is commonly employed in domain-specific accelerators due to its adaptability for customization____. To better integrate our compression unit, we choose TPU-like architecture as our implementation and evaluation platform.

%Comment for now?
%\paragraph{Systolic Array Simulator}
% STAR-Sim____ and SCALE-Sim____ are two fully analytical DNN accelerator simulators and they assume the same accelerator architecture of an array of processing elements (PEs), e.g., a systolic array, surrounded by on-chip buffers for data storage. Weights and activations are fetched from the weight buffer and activation buffer to the PEs, respectively. The Accumulator Buffer manages the accumulation of partial sums. Final sums are then sent to the post-processing unit, and the outputs will be written back to the activation buffer. When on-chip buffers are not sufficient to accommodate the weights or activations, weights/activations are read from DRAM and activations are then written back to DRAM. 

%Move to methodology
% In our analytical simulation, we adopt approaches similar to those of STAR-Sim and SCALE-Smi. However, STAR-Sim and SCALE-Sim focus on modeling convolution operations, which is rare in LLMs. The majority of computations in LLMs are matrix multiplications____. 
% We modify the analytical formulas in STAR-Sim to better cater to the LLM simulation. We also simulate the impact of Huffman decoding compression by modifying the weight buffer access strategy.

%Comment for now
\paragraph{Lossless Compression in Hardware}
Prior work neural networks accelerators have applied lossless compression 
methods like RLE or sparse coding techniques, but have not implemented full end-to-end entropy coding methods like Huffman coding, in large measure due to its perceived costs. Prior work has implemented lossless compression tailored on CPUs for workloads relevant to general-purpose computing benchmarks.
Bit-Plane Compression (BPC)____, for example, introduces a novel compression algorithm to compress homogeneously typed memory blocks. BPC transforms the data and then applies run-length encoding and a frequent pattern encoding to compress the data. 
Buddy Compression____ uses BPC to connect GPU device memory to a ``larger-but-slower buddy memory''. Using a high-bandwidth interconnect between these two memories, they are able to send compressed data to the GPU memory while putting any data that doesn't fit on the GPU into the buddy memory. Selective Memory Compression____ introduces a memory compression scheme that aims to reduce page thrashing by gradually compressing read-only pages. 
%____ discover that hardware memory compression leads to memory management problems for the OS. Therefore, they develop an interface for machine-physical memory which lets the OS specify how much machine-physical memory it wants instead of indirectly allocating that memory through physical memory which is the cause of many hardware compression problems. 
%____ propose Practical and Transparent Memory Compression, a system that uses only commodity memory modules to obtain bandwidth benefits of memory compression. This is accomplished through an inline-metadata mechanism which determines the compressibility of a line by looking for a "marker" word. 




% Domain-specific accelerators for DNNs
% have been extensively studied____. The processing units and dataflow are highly customized for DNN computation, leading to high performance and energy efficiency. 
% %Several accelerators adopt near-memory processing to overcome the memory-bound characteristics of specific types of DNN workloads____. 
% These specialized accelerators require a customized design for the entire hardware system, whereas our compression unit is a plug-and-play device that can seamlessly integrate with various accelerators. Other works target sparsity in DNNs to skip ineffective computation____. Quantization is another direction to effectively reduce the memory and computation demands of DNN process____. Both sparsity and quantization result in some loss of information from the original model, whereas our approach focuses on the lossless compression of LLM weights.

%\paragraph{Associated Costs} 
%The widespread development of LLMs has led to model sizes reaching unprecedented scales and continuing to expand, as exemplified by the Llama2-70B____ model which demands 140GB of memory under 16-bit floating point format. 

%The extensive parameter sizes of LLMs presents significant memory challenges. Firstly, it leads to substantial data movement, which is the main source of energy consumption during single-batch inference of LLMs____. Notably, the energy cost of transferring a single bit of data is estimated to be 100 to 500 times higher than the energy required for computation.____.
%Secondly, the growth in memory bandwidth fails to keep pace with the increase of model weight size, resulting in LLM performance being bottlenecked by memory latency____.
%As prior research has shown____, attention mechanisms and GEMM operations contribute to over $50\%$ of the runtime when deploying LLMs, with a significant portion of this runtime attributed to weight movement____.