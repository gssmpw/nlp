%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:\
\usepackage{comment}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}
\usepackage{adjustbox}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{multirow}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\usepackage{enumitem}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\newcommand{\py}[1]{{\color{red}[PY: #1]}}
\newcommand{\pt}[1]{{\color{teal}[PT: #1]}}
\newcommand{\ch}[1]{{\color{red}[CH: #1]}}
\newcommand{\sj}[1]{\textcolor{purple}{[SJ: #1]}}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Huff-LLM: End-to-end lossless compression for LLMs}

\begin{document}

\twocolumn[
\icmltitle{Huff-LLM: End-to-End Lossless Compression for Efficient LLM Inference}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Patrick Yubeaton}{equal,nyu}
\icmlauthor{Tareq Mahmoud}{equal,und}
\icmlauthor{Shehab Naga}{equal,und}
\icmlauthor{Pooria Taheri}{equal,und}
\icmlauthor{Tianhua Xia}{equal,nyu}
\icmlauthor{Arun George}{und}
\icmlauthor{Yasmein Khalil}{und}
%\icmlauthor{}{sch}
\icmlauthor{Sai Qian Zhang}{nyu}
\icmlauthor{Siddharth Joshi}{und}
\icmlauthor{Chinmay Hegde}{nyu}
\icmlauthor{Siddharth Garg}{nyu}
\end{icmlauthorlist}
\icmlaffiliation{nyu}{Department of Electrical and Computer Engineering, New York University, NY, USA}

\icmlaffiliation{und}{Department of Computer Science and Engineering, University of Notre Dame, IN, USA}

% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Patrick Yubeaton}{wpy2004@nyu.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
%\py{TBD. Paper 8 page limit.}
As they become more capable, large language models (LLMs) have continued to rapidly increase in size. This has exacerbated the difficulty in running state of the art LLMs on small, edge devices. Standard techniques advocate solving this problem through lossy compression techniques such as quantization or pruning. However, such compression techniques are lossy, and have been shown to change model behavior in unpredictable manners. We propose Huff-LLM, an \emph{end-to-end, lossless} model compression method that lets users store LLM weights in compressed format \emph{everywhere}---cloud, disk, main memory, and even in on-chip memory/buffers. This allows us to not only load larger models in main memory, but also reduces bandwidth required to load weights on chip, and makes 
more efficient use of on-chip weight buffers. In addition to the memory savings achieved via compression, we also show latency and energy efficiency improvements when performing inference with the compressed model.
\end{abstract}

\section{Introduction}

% \py{\begin{itemize}
%     \item Importance of powerful transformers (Done)
%     \begin{itemize}
%         \item Need to make them smaller (Done)
%     \end{itemize}
%     \item Discuss lossy compression (Done)
%     \begin{itemize}
%         \item Quantization, Pruning (Done)
%     \end{itemize}
%     \item Frame our E2E work as bridging the gap between FP16/BF16 and INT8/4. Comes across as a "nicer" way of showing the difference between E2E and quantization works without being as negative about quantization. (Consider adding)
%     \item Discuss negatives of lossy compression (Done)
%     \begin{itemize}
%         \item Accuracy impact (Done)
%         \item Flips in Accuracy (Done)
%         \item Safety and Trustworthiness (Done)
%         \item Some models can't go below bf16/fp16 (?)
%     \end{itemize}
%     \item Introduce lossless compression (Huffman/Entropy coding) (Done)
%     \item Software-Hardware co-design is necessary for efficient and effective lossless compression of LLM weights (Done)
%     \begin{itemize}
%         \item Emphasize (italicize) the end-2-end nature of our work and how different it is from ZipNN/NeuZip/Etc.. (Done)
%     \end{itemize}
%     \item Highlight major contributions (Done)
% \end{itemize}}

%Deep learning models have increased in scope and size in recent years. Classical image recognition models such as Resnet-50~\cite{he2016deep} could be loaded into memory with a size of 100MB. 
State-of-art Large language models (LLMs) are \emph{massive}---even a mid-size model like Llama3-70B~\cite{dubey2024llama} takes up 150GB of memory, which is out of reach except for the highest-end hardware. 
Model size not only limits deployment on edge devices that tend to have small memory capacity, but also 
increases the memory bandwidth required for fast inference. 
%This massive increase in model size 
Massive model sizes have
motivated a large body of work on model compression targeted specifically for LLMs. %Model compression is 
These methods primarily fall into two buckets: quantization and pruning. Quantization methods seek to decrease the precision of model parameters, thus requiring fewer bits per parameter, while pruning methods seek to decrease the number of parameters. 
%While effective, both quantization and pruning are lossy compression methods in that the
%compressed model parameters are approximations of the original model parameters
%These methods aim to decrease the memory footprint of large language models by decreasing the precision of model parameters and decreasing the number of parameters respectively. However, the lossy nature of these methods leads to a loss of downstream utility in compressed models.
Recent works such as LLM.Int8()~\cite{dettmers2022gpt3}, GPTQ~\cite{frantar2022gptq}, and AWQ~\cite{lin2024awq} have achieved 2$\times$-4$\times$ compression with respect to LLM weights, thereby potentially speeding up inference.  
%These works generally show that the perplexity of these models on various datasets is largely unchanged after compression. 
%However, recent works have shown that although perplexity may remain unchanged, other important aspects of a model's downstream performance are impacted. 

However, quantization and pruning are both lossy compression methods, leading the compressed 
models to behave differently from the original model and flipping incorrect to correct answers (and vice versa) on multiple-choice benchmarks even if average accuracy is maintained~\cite{dutta2024accuracy}.  
%Compressed models, for instance, 
%Hence, even if the average accuracy on a task is the same between compressed and original models, 
%For example,~\cite{dutta2024accuracy} show that 
%For instance, responses on individual problems differ notably between quantized and original models due to "flips" in the model answers despite near identical performance on average~\cite{dutta2024accuracy}.  
%for multiple choice problems. 
Safety, trustworthiness, multilingual capabilities and demographic biases might also be impacted, as shown by  \cite{xu2024beyond,hong2024decoding,marchisio2024does}. 
%and \cite{hong2024decoding}. Generalization abilities, for instance, 
%Compressed models may also experience decreased 
%on multilingual capabilities might also degrade, as shown in \cite{marchisio2024does}. 
%
These results demonstrate that, notwithstanding the impressive results obtained from lossy model compression, we have yet to fully understand its impact on LLM behaviour.  
This raises the question: 
\emph{can we compress LLMs without altering their behaviour in any way}?
%can LLMs be compressed so they behave \emph{identically} to the original model?

Lossless compression methods (such as Huffman coding and arithmetic coding) offer a solution. 
Just as how a Huffman-compressed image can be reconstructed 
{exactly} in its original form;  
a losslessly compressed LLM model would behave \emph{identically} to the original model after decompression.
%These methods exploit statistical redundancies in the data to perform compression with no loss of information. This would theoretically lead to no loss of downstream utility in losslessly compressed models. Popular lossless compress methods include Huffman coding~\cite{huffman1952method} and arithmetic coding~\cite{langdon1984introduction}. These are both entropy coding methods because they aim to reach the entropy lower bound defined by Shannon's source coding theorem~\cite{shannon1948mathematical}.
%Lossless weight compression would allow an LLM to be deployed on an edge device  
However, despite widespread use in other domains, 
%application to audio, video, and text data, 
lossless compression has found surprisingly little application in LLM compression. 
One main reason is that 
lossless compression and decompression can be  \emph{computationally expensive} and is not natively supported on commodity hardware like CPUs and GPUs. Custom hardware accelerators (such as TPUs and NPUs) also do not implement lossless compression due to hardware implementation overheads.
%do not support lossless compression either because of presumed hardware implementation overheads. 
%Huffman coding, for example, uses a variable length format, using few (more) bits to encode common (rare) input values. Huffman decompression
%then scans through a compressed file bit by bit, searching for matches against codewords. The sequential nature of lossless decompression limits parallelism, posing challenges for efficient hardware implementation.  
%Commodity hardware (CPUs, GPUs) does not
%Commodity hardware, GPUs for example, do not 
%natively support lossless compression/decompression.  

Prior work has proposed 
%lossless compression of LLM models has been relatively less explored. ~\cite{xxx} recently proposed 
lossless compression to reduce download costs of LLM weights from the cloud~\cite{hershcovitch2024zipnn}, but the model is decompressed and loaded into memory
%However, once downloaded to disk, the model is loaded into GPU memory 
in its original, 
uncompressed format.
%but loads the model in main memory in uncompressed 
%format, obviating most of the benefits of model compression. A second approach goes a step 
~\cite{hao2024neuzip} go a step further: models are loaded into memory in compressed form, but decompressed layer by layer during inference.
Thus, larger models can be loaded into a smaller main memory, but at the cost of \emph{increased} inference latency since weight matrices must first be decompressed before inference.  
As a result, prior methods do not  realize the \emph{full} benefits of model compression, including reduced download costs and memory footprint, 
but also
faster and more energy-efficient LLM inference.
%memory bottleneck 

%given a focused software-hardware co-design, larger models can be loaded into a smaller main memory while also achieving latency and energy benefits. 
%An earlier version of this scheme in the CNN context proposes but does not implement lossless compression because of high implementation overheads on commodity GPUs~\cite{}.  

\paragraph{Our contributions.} 
We propose \textsc{Huff-LLM}, a new \emph{end-to-end} model compression 
method and custom hardware implementation that stores LLM weights in compressed format \emph{everywhere}---cloud, disk, main memory, \emph{and} in on-chip memory/buffers. Weights are only decompressed when needed, \emph{i.e.}, to multiply with inputs/activations, where they are decompressed to their original FP16/BF16 formats. Via careful hardware-software co-design, we ensure that \textsc{Huff-LLM} is both lightweight, adding less than 6\% area overhead, and easily integrated into custom hardware architectures like systolic arrays and vector-accelerator~\cite{nvdla, simba, nvidia_xformer} architectures commonly
used in today's TPU/NPU chips. 
Using simulations and an FPGA prototype, we show that \textsc{Huff-LLM}
%Via careful hardware-software co-design, we show that
%\textsc{Huff-LLM} adds minimal area overhead and performance ove
%This allows us 
%to 
reduces model size by up to 32\%, improves
inference latency by up to 31\%, \emph{and} cuts energy cost by up to 26\%. 
Our main contributions 
are as follows:
\begin{itemize}[nosep,leftmargin=*]
\item We introduce \textsc{Huff-LLM}, an end-to-end model compression technique which is capable of maintaining LLM weights in compressed format throughout the system when using custom hardware.
\begin{itemize}
    \item Rather than applying Huffman compression to the whole parameter, \textsc{Huff-LLM} compresses subsets of LLM weight parameters. This minimizes the overheads of Huffman decompression, rendering it practical.
    \item We develop a Huffman decoder that can, with minimal overhead, be integrated into standard accelerator architectures like Systolic Arrays and Vector Processors.
\end{itemize}
\item Our evaluations across multiple LLM architectures demonstrate that \textsc{Huff-LLM} achieves a 15--32\% reduction in both required on-chip memory capacity and memory bandwidth requirements.
\item We evaluate \textsc{Huff-LLM}'s compression performance using accelerator design tools, simulations, and standard performance estimators across multiple accelerator architectures and popular open-weight LLM families.
\begin{itemize}
    \item We observe consistent savings across architectures, with up to 31\% improvement to latency and up to 26\% reduction in energy.
\end{itemize}
\end{itemize}

%but also reduces bandwidth required to load weights on chip, and makes 
%more efficient use of on-chip weight buffers. 



\section{Background and Related Work}
%We begin by covering relevant background for \textsc{Huff-LLM} and related work from literature. We first discuss existing LLM model compression methods, followed by work on 
%hardware accelerators for LLM inference. 
% \py{\begin{itemize}
%     %\item Transformer Overview
%     %\item Briefly discuss lossy compression (quantization)?
%     % \item LLM Weight Formats (Done)
%     % \item Lossless Compression
%     % \begin{itemize}
%     %     \item Overview of huffman coding (Done)
%     %     \item Huffman Coding Tree Figure
%     %     \item Discuss ZipNN, NeuZip and how they're "off-chip" decompression (Done)
%     %     \begin{itemize}
%     %         \item Sacrifices performance (latency/throughput) to load larger models(Done)
%     %     \end{itemize}
%     % \end{itemize}
%     \item Hardware Section
%     \begin{itemize}
%         \item TPUs vs. GPUs 
%         \item Systolic Array (Simulators) (Done)
%         \item Existing Accelerators
%         \item Associated Costs
%         \begin{itemize}
%             \item Latency due to IO bottleneck
%             \item Energy Costs
%             \item Decoding Cost (Why is 16-bit huffman slow)
%         \end{itemize}
%     \end{itemize}
% \end{itemize}}

%\begin{comment}   
%the comment command doesn't seem to be working
%\paragraph{Transformers}
%Most modern large language models are built on top of the transformer architecture introduced in ~\cite{vaswani2017attention}. One of the key features of the transformer architecture is the large number of weight matrices used during inference. For example: Llama 3-8B contains 32 transformer layers. The majority of the parameters in these layers are dedicated to weight matrices found in the attention operation and the multi-layer perceptron. There are four weight matrices found in the attention operation with each boasting a size of [4096 x 4096] or [4096 x 1024] parameters. There are also three weight matrices found in the multi-layer perceptron which have sizes of [4096 x 14336] parameters. One can easily see how the size of these parameters adds up quickly leading to a very large model (~16 GB) that needs to be loaded into memory for quick and efficient inference. This has led to a massive influx of model compression papers which focus on reducing the size of the model such that it can be loaded into smaller devices/graphics cards. 

%An additional consideration for the size of large language models can be found in the attention operation. Due to the auto-regressive nature of transformers, one needs to repeat the matrix multiplications in the attention operation for the same input tokens many times. In order to save on computational costs, the kv-cache was developed as a way of storing these values in between forward passes. However, this further increases the amount of memory our model takes up in proportion to the context length. Recent works have shown that as the context length increases beyond a certain point, the primary memory bottleneck becomes the kv-cache rather than the model weights~\cite{hooper2024kvquant}. We will focus on the lossless compressibility of the weight matrices in this work. 

\subsection{LLM Model Compression}

%Since our primary goal is weight compression, we begin by describing 
%commonly used formats used to store uncompressed weights.

\paragraph{LLM Weight Formats.} LLM weights are broadly stored in either floating point or integer formats during inference.
The most common floating point formats are 32-bit floating point (FP32), 16-bit floating point (FP16), and 16-bit brain float (BF16). 
%Each of these formats allocates a different number of 
%bits for the MantissaExponent
In FP16, for instance, the most significant bit (MSB) is the {sign} (S) of the number, the next five bits are the {exponent} (E), and the last ten bits are the {mantissa} (M). 
Any FP16 weight is then represented as: %addition, a \textbf{bias} (B) is assigned for each type of Floating Point container. 
%For example, B=15 for FP16. These values are then used to represent a number such that:
\begin{equation}
    \text{Value} = (-1)^S \times 2^{E-B} \times (1.M)
\end{equation}
where $B$ is a fixed bias term, commonly set to $B=15$. In this format, mantissa bits only encode 
the fractional value after the decimal point ($1.M$).
FP32 is similar, but allocates 8 bits to the exponent and 23 bits to the mantissa. 
BF16, introduced as a compromise between FP32 and FP16, has an 8-bit exponent, 
a 7-bit mantissa, and a sign bit. 
BF16 has a larger dynamic range compared to FP16, but lower precision within this range compared
to FP16.

To reduce model size, 8-bit integer (INT8) and 4-bit integer (INT4) formats were introduced.
These represent weights as 
signed 2's-complement integers. Decimals are represented using 
a scaling factor typically associated with an entire tensor or channels within it.
%The containers use a certain number of bits to hold each weight parameter (as described in each container's name). 
%In addition, each container type has its own way of representing the number in bits. 
INT8 and INT4 representations are typically obtained via quantization methods applied to models 
stored in FP16 or BF16 formats. These methods are discussed next.
%These methods aim to reduce the precision of weight parameters by reducing the number of bits allocated to each parameter. These 
%are discussed next. 
%Thus, a new container is needed for these smaller parameters. INT8 and INT4 aim to do precisely that. Parameters are stored in their quantized forms in integer containers before they need to be de-quantized. 

\paragraph{LLM Weight Quantization} 
%\sj{I took a stab at this below}
\textcolor{black}{As LLM sizes have grown, their associated workloads often become expensive in real-world applications. Techniques to reduce the memory footprint and computation precision for these models has been driven by the need to serve these models at scale, or run larger models locally on compute-limited resources. Quantization-aware training (QAT), has demonstrated the highest accuracy for most models; in QAT, precision reduction through quantization is included within the training loop, often requiring that the training pipeline and training data be accessed during the quantization process. Usually, QAT is incorporated within the fine-tuning phase~\cite{yao2022zeroquant}, resulting in significant training overhead cost; these costs are particularly exacerbated for large-scale models. Post-training Quantization (PTQ), in contrast, quantizes existing pre-trained models, avoiding the need to retrain the model. The PTQ approach avoids many privacy hurdles associated with access to pre-training, enabling third parties to modify open-weight models and serve them efficiently on various hardware platforms~\cite{lie2023cerebras,frantar2022gptq, ashkboos2024quarot}.}
% @chinmay -- I'll edit other stuff, if you want to take this over?


However, while post-training quantization can reduce inference costs, reducing model precision is often associated with unintended consequences~\cite{sambanova2023reducedprecision, cerebras2024llama3}. Research such as~\cite{zhang2024does} has shown that post-hoc quantization can have adverse effects on model alignment, and can be used to mitigate ``unlearning'' procedures that are applied to LLMs as copyright or safety filters. Quantization applied to multi-lingual LLMs have disparate effects on low-resource languages, particularly those that use non-Latin scripts~\cite{marchisio2024does}. These adverse affects also arise in QAT-trained models; even though final test accuracy is similar, performance of quantized LLMs can be significantly worse on complex tasks such as multi-turn dialog on standard benchmarks~\cite{dutta2024accuracy}.   
%\textcolor{red}{we need a super strong para on LLM weight quant. I would highlight the diff between post-training quant and quant aware methods; latter are better but more expensive. Note major papers and that they have matched FP16 accuracy/perplexity, especially for INT8; INT4 does have accuracy loss.} \py{I can write this}

% \paragraph{Quantized Model Evaluations} 
% \textcolor{red}{Discuss studies showing safety/bias/security concerns.} \py{I can write this}

%Include a Huffman Coding primer, shorter/more compact than the one below
%\paragraph{Huffman Coding}
%\paragraph{Huffman Coding}
%Huffman coding~\cite{huffman1952method} is an entropy coding method that aims to compress a sequence of "source symbols" down to their entropy lower bound as defined by Shanon's source coding theorem~\cite{shannon1948mathematical}. At a high level, Huffman coding creates variable length code words for every source symbol in the dataset you want to compress. We first calculate the probability of every source symbol in our dataset. Given this distribution, we can now assign the shortest code word to the most frequent source symbol. We continue this process until every source symbol has a unique code word. We then compress the input with a simple dictionary search; for each source symbol in our input we look up its code word in our Huffman Table and replace that source symbol with its code word. Decompression happens in a similar, but reverse manner. One important thing to note for decompression is that the code words are a prefix code. This means that they are chosen in such a way that no code is a prefix of another allowing for unambiguous decoding. 
%We provide an example Huffman Table in Table~\ref{tab:huffman_example}. In this table you can see the prefix code nature of the selected code words. In addition, you can see how the code word lengths get progressively larger which is how you generate new code words without encroaching on the prefix code property.

%\begin{comment}

%\begin{table}[ht]
%    \centering
%    \label{tab:huffman_example}
%    \resizebox{\columnwidth}{!}{%
%        \begin{tabular}{@{}cc@{}} \toprule
%        Code Word & Source Symbol\\ \midrule
%        01 & 10100 \\
%        10 & 00100 \\
%        110 & 10011 \\
%        111 & 00011 \\
%        0000 & 10101 \\
%        $\cdots$ & $\cdots$ \\
%        00110101001 & 10110 \\
%        \bottomrule
%        \end{tabular}
%    }
 %       \caption{\sl This Huffman Table describes the source symbol and the code word assigned to the source symbol. The source symbol always has the same length (5 bits), but the Huffman code word length is variable.}
%\end{table}


%\end{comment}

%\textcolor{red}{Cover FP16; BF16; Int8 and Int4. Basic transformers tutorial not needed. For Huffman coding, might help but need a picture of a Huffman tree.}

\paragraph{Lossless Compression of LLM Weights}
Compared to the large body of work on 
lossy compression of LLM weights, there is relatively little work on lossless compression. 
An early paper~\cite{han2015deep} proposed Huffman compression of 
convolutional neural network (CNN) weights, naively compressing entire 16-bit weights which incurs large performance overheads. For this reason, they do not actually implement Huffman coding, and instead only use 
run-length encoding (RLE) of sequences of zero weights. RLE is subsequently 
implemented in several other works, especially for CNNs that have sparse weight tensors~\cite{chen2016eyeriss}. %\textcolor{red}{is it end-to-end?} \sj{could you explain what you mean here? RLE was a part of eyeriss for e.g.}
In contrast, \textsc{Huff-LLM} proposes a lightweight 
hardware-friendly implementation of Huffman coding, 
integrates within 
systolic array and vector LLM accelerator architectures, demonstrating 
substantial performance and energy benefits. 

%This is because they propose to 
Two recent works have addressed Huffman compression for LLMs: \cite{hershcovitch2024zipnn} propose to compress the exponent bits of weights via Huffman coding
%These compressed weights are then used 
to reduce the cost of storing and downloading LLMs on cloud servers. For FP16 and BF16 models, they are able to achieve 17 - 33\% compression with higher compression ratios coming from BF16 models due to the larger number of exponent bits. 
%However, the LLMs need to be uncompressed on the hard drive before being loaded into a GPU/TPU. 
\cite{hao2024neuzip} use a similar approach by applying asymmetric numeral systems (ANS), an entropy coding method,  to the exponent bits. In addition, they load the compressed weights to the GPU/TPU and thus achieve memory savings over ~\cite{hershcovitch2024zipnn} during inference. They are able to achieve 33\% lossless compression on BF16 models, but also suffer a 33\% inference slow down due to decompression.

%Quantization is one of the most popular approaches to large language model compression. This approach focuses on representing the parameters of a model with lower precision which leads to memory savings. Popular methods include LLM Int8~\cite{dettmers2022gpt3}, GPTQ~\cite{frantar2022gptq}, and AWQ~\cite{lin2024awq}. We will not focus on comparisons with quantization methods as the lossy and lossless setting are significantly different. 

Note that other lossless compression schemes exist. RLE, mentioned previously, 
exploits spatial correlations between inputs by encoding a sequence of identical
weights as the weight value followed by the number of occurrences. 
LZW, a more sophisticated variant, exploits commonly occurring patterns in the input data~\cite{welch1984technique}. Both can be implemented synergistically after Huffman coding of individual weight values. We leave an evaluation of these methods as future work, but note that these incur additional hardware costs.
%\textcolor{red}{one sentence explanation}  

\subsection{Hardware Accelerators for LLM Inference}
%Next three sections can be condensed, moved into a later part (methodology/experiment setup?)
%Comment for now?
\label{sec:systolic_array}
\paragraph{Systolic Array Architectures} %\textcolor{red}{explain what a systolic array is --- important to emphasize the need for data to be fed into the array without stalls. probably need a figure?}
The systolic array (SA) architecture, shown in Figure~\ref{fig:systolic_array_diagram} consists of an array of processing elements (PEs) that perform multiply-and-accumulate (MAC)
operations, 
surrounded by on-chip buffers for data storage. Weights and activations are fetched from the weight buffer and activation buffer to the PEs, respectively. 
Data is streamed in from these buffers in a 
highly synchronized fashion such that each PE 
computes the dot product of a row of activations with a column of weights. 
%The Accumulator Buffer manages the accumulation of partial sums. Final sums are then sent to the post-processing unit, and the outputs will be written back to the activation buffer. 
%Data movement takes place in every cycle due to the pipeline structure. 
However, 
%if there are any stalls in the systolic array pipeline to
it is crucial to maintain an uninterrupted data flow for correct performance of the systolic array, as
any stalls or bubbles cause either incorrect computation or incur large performance penalties~\cite{sa-pipeline}.
This underscores the necessity of a Huffman decoder that operates without stalling the data stream (See Fig.~\ref{fig:systolic_array_diagram}). 
Note that the description above is for an "output stationary" (OS) systolic array. A slightly different architecture, referred to as weight stationary (WS) stores weights inside each PE and only streams in activations such that the output of each column produces a dot-product of a column of weights with activations. 
%Accumulation buffers at the outputs are needed 
%\textcolor{red}{text needs work+image} \textcolor{blue}{Updated}

%The Accumulator Buffer manages the accumulation of partial sums. Final sums are then sent to the post-processing unit, and the outputs will be written back to the activation buffer. When on-chip buffers are not sufficient to accommodate the weights or activations, weights/activations are read from DRAM and activations are then written back to DRAM. 

\paragraph{Simba Vector Architectures} %\textcolor{red}{para describing SIMBA---ND folks? Arch pic?} 

To ensure the generality of our approach, we extended our evaluations to a parallel vector-processing optimized accelerator based on NVIDIA's production-tested NVIDIA Deep Learning Accelerator (NVDLA) architecture~\cite{nvdla, simba}. The hardware model incorporates NVDLA dataflow-optimizations that reduce data-movement for transformers~\cite{nvidia_xformer}. Our evaluations use a single chiplet with an array of 16$\times$16 Processing Elements (PEs) and a shared global buffer for activation storage (See Fig.~\ref{fig:simba-like_arch_diagram}). Each PE features dedicated local scratchpads for weights, inputs, and partial sums, along with vector multiply-accumulate (VMAC) units for parallel computation. The architecture is optimized for a `local-weight-stationary' dataflow (where weights remain fixed in local memory to minimize data movement)~\cite{magnet}, operates at a nominal frequency of 2 GHz, and connects to external LPDDR4 DRAM via a 128 GB/s interface. For ease of reproducibility, detailed hardware specifications are provided in Table~\ref{Simba_specs}.
%\textcolor{red}{text needs some refinement. SJ: could you take  a pass?} \sj{done}

% \begin{figure}
%     \centering
% \includegraphics[width=0.8\columnwidth]{figures/simba-like.pdf} 
%     \vspace{-0.1in}
%     \caption{Simba-like architecture diagram. Colors are assosiated to different operand. Weights are yellow, inputs are blue, and partial-sum/outputs are pink. \textcolor{red}{Patrick: I would add the systolic array picture here in the same diagram, just above Simba.} } 
%     \label{fig:simba-like_arch_diagram} 
% \end{figure}

\begin{figure}[h] % Keeps it in one column
    \centering
    \includegraphics[width=\linewidth]{figures/systolic_array.pdf} % First figure
    \caption{Systolic array diagram with a detailed look at the PE. Weights and activations are sent to the PEs at every clock cycle. Bubbles indicate delays which are necessary to maintain accuracy of the computations performed by the output stationary architecture. Colors are associated to different operands. Weights are yellow, inputs are light blue, and partial-sum/outputs are pink}
    \label{fig:systolic_array_diagram}

    \vspace{1em} % Space between figures

    \includegraphics[width=\linewidth]{figures/simba.pdf} % Second figure
    \caption{Simba-like architecture diagram with a detailed look at the PE. Colors are associated to different operands. Weights are yellow, inputs are light blue, and partial-sum/outputs are pink.}
    \label{fig:simba-like_arch_diagram}
\end{figure}



% \begin{figure}
%     \centering
% \includegraphics[width=\columnwidth]{figures/arch_diagrams.pdf} 
%     \vspace{-0.1in}
%     \caption{Systolic array (top) and Simba-like (bottom) architecture diagrams. Colors are associated to different operand. Weights are yellow, inputs are blue, and partial-sum/outputs are pink. Bubbles indicate delays which are necessary to maintain accuracy of the computations performed by the output stationary architecture.} 
%     \label{fig:simba-like_arch_diagram} 
% \end{figure}

%\paragraph{Hardware Support for Weight Compression}

%\paragraph{TPU vs GPU}
%Graphics Processing Units (GPUs) and Tensor Processing Units (TPUs) are two types of accelerators specifically designed to handle complex computational tasks efficiently. GPUs provide more flexible parallelization~\cite{wang2019benchmarking}, while TPUs are more customized for specific AI tasks. Prior research~\cite{wang2019benchmarking, tpu} has shown TPU is highly optimized for large size matrix multiplication because the systolic array architecture reduce area and power of the large matrix multiply unit~\cite{kung1979systolic}. GPUs have traditionally been regarded as high-throughput architectures, leveraging high-bandwidth DRAM and thousands of parallel threads to deliver optimal performance. The throughput-oriented architectural design of GPUs presents potential challenges in meeting strict latency and power constraints~\cite{jouppi2017datacenter, tpuv4}. Furthermore, systolic array architecture is commonly employed in domain-specific accelerators due to its adaptability for customization~\cite{dave2021hardware}. To better integrate our compression unit, we choose TPU-like architecture as our implementation and evaluation platform.

%Comment for now?
%\paragraph{Systolic Array Simulator}
% STAR-Sim~\cite{starsim} and SCALE-Sim~\cite{scalesim} are two fully analytical DNN accelerator simulators and they assume the same accelerator architecture of an array of processing elements (PEs), e.g., a systolic array, surrounded by on-chip buffers for data storage. Weights and activations are fetched from the weight buffer and activation buffer to the PEs, respectively. The Accumulator Buffer manages the accumulation of partial sums. Final sums are then sent to the post-processing unit, and the outputs will be written back to the activation buffer. When on-chip buffers are not sufficient to accommodate the weights or activations, weights/activations are read from DRAM and activations are then written back to DRAM. 

%Move to methodology
% In our analytical simulation, we adopt approaches similar to those of STAR-Sim and SCALE-Smi. However, STAR-Sim and SCALE-Sim focus on modeling convolution operations, which is rare in LLMs. The majority of computations in LLMs are matrix multiplications~\cite{Wang2020SpAttenES}. 
% We modify the analytical formulas in STAR-Sim to better cater to the LLM simulation. We also simulate the impact of Huffman decoding compression by modifying the weight buffer access strategy.

%Comment for now
\paragraph{Lossless Compression in Hardware}
Prior work neural networks accelerators have applied lossless compression 
methods like RLE or sparse coding techniques, but have not implemented full end-to-end entropy coding methods like Huffman coding, in large measure due to its perceived costs. Prior work has implemented lossless compression tailored on CPUs for workloads relevant to general-purpose computing benchmarks.
Bit-Plane Compression (BPC)~\cite{kim2016bit}, for example, introduces a novel compression algorithm to compress homogeneously typed memory blocks. BPC transforms the data and then applies run-length encoding and a frequent pattern encoding to compress the data. 
Buddy Compression~\cite{choukse2020buddy} uses BPC to connect GPU device memory to a ``larger-but-slower buddy memory''. Using a high-bandwidth interconnect between these two memories, they are able to send compressed data to the GPU memory while putting any data that doesn't fit on the GPU into the buddy memory. Selective Memory Compression~\cite{nihaal2024selective} introduces a memory compression scheme that aims to reduce page thrashing by gradually compressing read-only pages. 
%\cite{laghari2024memory} discover that hardware memory compression leads to memory management problems for the OS. Therefore, they develop an interface for machine-physical memory which lets the OS specify how much machine-physical memory it wants instead of indirectly allocating that memory through physical memory which is the cause of many hardware compression problems. 
%\cite{young2019enabling} propose Practical and Transparent Memory Compression, a system that uses only commodity memory modules to obtain bandwidth benefits of memory compression. This is accomplished through an inline-metadata mechanism which determines the compressibility of a line by looking for a "marker" word. 




% Domain-specific accelerators for DNNs
% have been extensively studied~\cite{Alwani2016FusedlayerCA, Han2016EIEEI, Zhang2021FASTDT, Eckert2018NeuralCB}. The processing units and dataflow are highly customized for DNN computation, leading to high performance and energy efficiency. 
% %Several accelerators adopt near-memory processing to overcome the memory-bound characteristics of specific types of DNN workloads~\cite{Chen2023MetaNMPLC, Ke2019RecNMPAP, Liu2023AcceleratingPR}. 
% These specialized accelerators require a customized design for the entire hardware system, whereas our compression unit is a plug-and-play device that can seamlessly integrate with various accelerators. Other works target sparsity in DNNs to skip ineffective computation~\cite{Albericio2023RETROSPECTIVECI, Lu2021SangerAC, Wang2020SpAttenES, Zhang2016CambriconXAA, Lu2021SangerAC}. Quantization is another direction to effectively reduce the memory and computation demands of DNN process~\cite{Yuan2023RPTQRP, Frantar2023OPTQAQ, Lin2023AWQAW, Dettmers2023QLoRAEF}. Both sparsity and quantization result in some loss of information from the original model, whereas our approach focuses on the lossless compression of LLM weights.

%\paragraph{Associated Costs} 
%The widespread development of LLMs has led to model sizes reaching unprecedented scales and continuing to expand, as exemplified by the Llama2-70B~\cite{touvron2023llama} model which demands 140GB of memory under 16-bit floating point format. 

%The extensive parameter sizes of LLMs presents significant memory challenges. Firstly, it leads to substantial data movement, which is the main source of energy consumption during single-batch inference of LLMs~\cite{10764574}. Notably, the energy cost of transferring a single bit of data is estimated to be 100 to 500 times higher than the energy required for computation.~\cite{8351458, mutlu2019processing, 6983056}.
%Secondly, the growth in memory bandwidth fails to keep pace with the increase of model weight size, resulting in LLM performance being bottlenecked by memory latency~\cite{kim2024breakthrough}.
%As prior research has shown~\cite{lin2024qservew4a8kv4quantizationcodesign, Wang2020SpAttenES}, attention mechanisms and GEMM operations contribute to over $50\%$ of the runtime when deploying LLMs, with a significant portion of this runtime attributed to weight movement~\cite{mecla}.

\section{The \textsc{Huff-LLM} Scheme}
%\py{\begin{itemize}
    % \item Software Side
    % \begin{itemize}
    %     \item FP16
    %     \begin{itemize}
    %         \item Discuss decompression costs of >5 bit Huffman coding
    %         \item Leads to 1-5-5-5 bit split
    %     \end{itemize}
    %     \item BF16
    %     \begin{itemize}
    %         \item Same motivation as FP16 case
    %         \item Split into 1-4-4-7
    %     \end{itemize}
    %     \item Show distribution graphs of weights (whole), exponents, mantissa
    %     \begin{itemize}
    %         \item Any change between layers, matrix type (Q, K , V, etc.). Different models as well?
    %         \item Maybe goes in experiments?
    %     \end{itemize}
    % \end{itemize}
%     \item Hardware Side
%     \begin{itemize}
%         \item Accelerator architectures
%         \begin{itemize}
%             \item Systolic array + how/where we add HDs
%             \item Why we need single-cycle decoding (show how it's achieved using a picture of the decoder)
%             \item Vector architectures
%             \item Provide insight into the accelerator runtimes by explaining the analytical formulas used in the simulators? 
%         \end{itemize}
%     \end{itemize}
% \end{itemize}}

\subsection{Hardware-Friendly Huffman Compression}
%\paragraph{Choosing a Compression Scheme}
A key challenge with hardware implementations of
Huffman decompression (and other 
entropy coding schemes) is that it is a variable length
code. A naive hardware implementation of Huffman decompression can read a fixed number of 
code-word bits 
in each clock cycle, and output a decompressed source symbol
when a match is found. Thus, when used to decompress a vector of 
weights, this scheme would output valid weights in 
some clock cycles and ``bubbles" (indicating that absence of a valid weight) in others when no match is 
found. As noted in Section~\ref{sec:systolic_array}, neural network 
accelerators like systolic arrays are carefully synchronized 
and require weights (and activations) to be output in each clock cycle for correct operation. 
Dealing with bubbles incurs a large
performance penalty since the entire array needs to be stalled anytime a bubble is encountered, or requires
complex control logic, extra buffering and a potential redesign of the accelerator logic. 

On the other hand, a Huffman decoder that outputs a new weight value (or source symbol) in 
each clock cycle enables easy ``plug-and-play" integration into existing neural network accelerators since the decoder can be added as an extra stage in the pipeline. 
Single cycle Huffman decoding, however, introduces a new challenge: an input codeword must be matched against \emph{all} possible $2^{N}$ codewords (assuming $N$-bit source symbols). In hardware, this logic is implemented using 
a content-addressable memory (CAM). However, 
CAMs have high hardware costs, and are typically limited to 32- or 64-entries in applications where single-cycle CAM look-ups are needed. 
Figure~\ref{fig:huffman_cost} plots the CAM overheads 
for 4 to 8 bit source symbols normalized to a column of 
128 FP16 multipliers as reference 
(as we will see shortly, a single Huffman decoder will be shared across a column/row of MAC units). We see that 
overheads are $~6\%$ for N=5 bits, but balloon quickly for 
larger values of N. 

%\textcolor{red}{CAM lookup cost data.}
%https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8585162&casa_token=u0g_MksOMEsAAAAA:X6tcJuNHhK7VDAmbSJe0iSJzgIf76R3h5t8iLiGJTTttG1FJc6FLKdUTy_eShL5seGn1xMHQ-5U&tag=1
\begin{figure}
    \centering
\includegraphics[width=0.9\columnwidth]{figures/HD_area_overhead.pdf} 
    \vspace{-0.15in}
    \caption{Area overhead of a CAM lookup for a single-cycle N-bit Huffman decoder normalized to a column of 128 FP16 multipliers, both clocked at 1 GhZ. Area overheads of Huffman decoding grow quickly, leaving only N=\{4,5\} as viable options.} 
    \label{fig:huffman_cost} 
\end{figure}

\begin{table}[t]
    \centering
    \label{tab:entropy_splits}
        \begin{tabular}{@{}ccc@{}} \toprule
        Split & Entropy (Bits/Param) & Total Bits/Param\\ \midrule
        16 & 10.54 & 10.54\\
        8-8 & 5.54, 5.03 & 10.57\\
        1-5-5-5 & 1.00, 2.60, 4.97, 2.04 & 10.61\\
        4-4-4-4 & 2.14, 3.91, 4.00, 1.34 & 11.09 \\
        \bottomrule
        \end{tabular}
        \caption{\sl Entropy is calculated for each set of bits as defined by the split. Adding all entropy values together will give the average bits/parameter for the entire weight matrix.}
\end{table}


%Huffman
%As noted in Section~\ref{sec:}, neural network accelerator architectures, systolic arrays for example, are sensitive to 
%stalls 
%We begin by evaluating the hardware costs of a single Huffman compression for different bit-widths; 

Table~\ref{tab:entropy_splits} shows the entropy of 
Llama-3-8B FP16 weights is 10.54 bits/parameter; 
of course, as we have already observed in Figure~\ref{fig:huffman_cost}, N=16-bit Huffman decompression is infeasible.   
Interestingly, we find that if, instead of Huffman compressing FP16 weights directly, 
%first explored the compressibility of LLM weights. We calculated the entropy of 
%Llama-3-8B 
we separately compress the 5-bit exponent, the 5 higher-order and 5 lower-order bits of the mantissa, the total entropy 
is only slightly larger at 10.61 bits/parameter. Note that in this scheme, we do not compress the sign bit. We refer to this as $\{1,5,5,5\}$ compression. Also shown in the table is the entropy for $\{8,8\}$ which also has a similar entropy of
10.57 bits/parameter, but is also infeasible from a hardware 
standpoint. 
Based on this analysis, we Huffman compress our weights 
using  $\{1,5,5,5\}$ Huffman compression, as shown in Figure~\ref{fig:huffman_flow}. 
Note that Huffman compression of LLM weights is performed 
only once and can be done offline on a CPU. 
Compressed weights are stored in memory, loaded into on-chip weight buffers
in compressed format, and decompressed only when needed. 
Next, we describe our implementation of 
Huffman decompression in our baseline hardware 
accelerators.


%\begin{comment}
%to see how many bits/parameter it could be losslessly compressed to on average. The entropy ranges from 10.5 - 10.6 bits/parameter for both attention and mlp matrices. This suggests that using an entropy coding method, we can reduce our parameter containers from a size of 16 bits to 10.5 bits; achieving approximately 1.5 times memory savings. However, we now need to choose an entropy coding method to actualize these savings. Common choices include Huffman Coding, Arithmetic Coding~\cite{rissanen1979arithmetic}, and Asymmetric Numeral Systems~\cite{duda2009asymmetric}. 

%When considering these choices, we want to select the method with the fastest decompression speed since all of the methods should reach the entropy lower bound. Therefore, we started by looking at Huffman Coding due to the simplicity of its decompression. A Huffman coded string can be decoded using a lookup table. The size of this lookup table will depend on how many source symbols the Huffman Table was created with. For example, suppose we are compressing 16-bit values using a Huffman Coder. If the Huffman Table has to create a unique code word for all possible 16-bit values, it will have to create $2^{16}=65,536$ entries in its Huffman Table. Huffman Tables of this size will take up a significant amount of memory and will lead to slow lookup times. Therefore, we need to implement Huffman compression without using these extremely large tables.

%One way to accomplish this is to perform Huffman Compression on smaller groups of source symbols. For example, instead of using the entire 16-bit parameters as source symbols, we could use only the first 8 bits as source symbols. Using this method, we would have two Huffman Tables per matrix: one for the first 8 bits and one for the second 8 bits of each parameter. This results in a table with $2^{8} = 256$ entries. If you have two tables for one matrix you will have $256*2 = 512$ entries in your Huffman Tables which is a far cry from the 65,536 entries of our 16-bit Huffman Table. This logic can be taken one step further by considering the natural split of exponent and mantissa bits in floating point numbers. Suppose we have split the 16-bits as follows: 1 sign bit, 5 exponent bits, first 5 bits of the mantissa, and final 5 bits of the mantissa. This will result in $2^{5} * 3 = 96$ entries in your Huffman Tables. This technique helps minimize the memory footprint of our Huffman Tables while also increasing the lookup speed of our Huffman Decoder.

%Although this splitting technique results in memory and latency savings, does it have any impact on compression ratio? We calculate the entropy of each split to see if they perform any worse than the original 16-bit entropy coding. The results are shown in Table~\ref{tab:entropy_splits}. We see that although there is a slight increase in the total bits/parameter with each split we perform, the 1-5-5-5 split is only 0.1 bits/parameter larger than the 16-bit compression on average. Thus, we are able to implement Huffman Coding in a speedy and memory efficient manner without having a large imapct on compression ratio.


%Our Huffman Compression method is summarized in Figure~\ref{fig:huffman_flow}. We see that every parameter is broken up into 4 parts. The sign bits are sent uncompressed to memory. The remaining fifteen bits are split into three groups of five bits which are then sent to Huffman coders. The Huffman table determines which code word replaces the source symbol. These compressed values are then sent to memory until they need to be used during inference.
%\end{comment}

\begin{figure}
    \centering
\includegraphics[width=0.8\columnwidth]{figures/Huffman_Flowchart.drawio.pdf} 
    \vspace{-0.1in}
    \caption{Our Huffman Compression method follows these steps for every parameter. It breaks a FP16 number into 4 groups of bits. The sign bit remains uncompressed. The exponent, and mantissa bits are sent through a Huffman Coder to be compressed. They are then stored in memory until they are needed for inference.} 
    \label{fig:huffman_flow} 
\end{figure}

%What we need from this section:
% 1: Brief intro of Scale-sim/Simba (put into experimental setup)
% 2: Explanation of where Huffman Decoder is inserted. How this differs from usual systolic array system
% 3: Importance of one clock cycle decoding. How we plan to verify this.
% Note: Can move all extra detail to appendix for now (no need to delete).
\subsection{Hardware Integration of Huffman Decoders}
%\paragraph{Systolic Array and Huffman Decoder Integration}
\label{para:systolic_decoder_integration}

\begin{figure}
    \centering
\includegraphics[width=0.48\textwidth]{figures/pe_hd.pdf}
    %\vspace*{-0.5\baselineskip}
    %\vspace*{-1\baselineskip}
    \vspace{-0.25in}
    \caption{Systolic array and Huffman Decoder integration.} 
    \label{fig:pe_hd} 
    \vspace*{-0.3in}
\end{figure}

Fig~\ref{fig:pe_hd} shows how Huffman decoders are integrated 
within the baseline systolic array architecture described in 
Section~\ref{sec:systolic_array}. As noted previously, weights are streamed into the systolic array 
from the weight buffer, one weight per clock cycle. In 
\textsc{Huff-LLM}, weights are stored in the weight buffer 
in compressed form.
%A row of single-cycle Huffman decoders (HD) is inserted in between the weight buffers and the 
%the systolic arrays; all PEs in a column share a single HD. 
%HDs decompress weights as they are streamed in, output a
%a decompressed FP16 weight value in every clock cycle. 
%Subsequent execution of the systolic array proceeds as it would normally.
%architecture of an accelerator with PE array, input buffer, %weight buffer, and output buffer. Huffman decoders (HD) are integrated between the output of weight buffer and the input of the PE's on the first row of the systolic array.


To enable computation, the Huffman-compressed weight data must be decoded before being fed into the systolic array. As depicted in Fig~\ref{fig:pe_hd}, a row of Huffman decompressors (HD) are inserted between the weight buffer's output and the first row of processing elements (PEs) in the systolic array. Each HD module contains three 
5-bit Huffman decoders. These decoders process the compressed data stream, decoding it into their respective parts of the 
5-5-5 split decompressed data. To enable this, each column's weight buffer is partitioned into three equally sized banks; each bank holds compressed weights from one of the three splits. The sign bit is passed through directly without modification. Finally, the sign bit, exponent, and mantissa bits are concatenated to reconstruct the 16-bit decompressed weight data. 

Figure~\ref{fig:pe_hd_appdx} shows the design of  
each 5-bit HD that enables it to output a decompressed 
weight in a single cycle. 
A 32-bit register holds compressed
values fetched from the (compressed) weight buffer, along with a Start pointer ($S$) that points to the beginning of the 
current codeword. Assuming $L_{max}$ is the number of bits in the longest codeword (note that $L_{max}$ is known in advance, and $L_{max}<32$ for any valid codebook),
bits at positions $S$ to $S+L_{max}-1$ are used 
to match against all codewords stored in a 32-entry CAM. 
%The matched location is used to %which
Each CAM entry also stores the 5-bit source symbol corresponding to each codeword and the codeword's length, $L$. 
This source symbol is provided as an output from the decompressor
and 
the start pointer is updated to $S \leftarrow S+L$. 
Finally, $L$ bits are read from the weight buffer into the 
codeword register. In practice, a larger codeword register, 
say a 64-bit register can be used with the advantage that 
the weight buffer would only need to be accessed when 
fewer than 32 valid bits are left in the register.

%Since the longest Huffman codeword,  
We use the same HD design for the Simba-like vector architecture shown in Figure~\ref{fig:simba-like_arch_diagram}. The HD blocks are inserted between the 
distributed weight memory and vector MAC units. Since, as described, the HD blocks output a new decompressed weight 
per clock cycle without any bubbles or stalls, the throughput/performance of the accelerator is not impacted and 
no changes to the design are needed.

\section{Experimental Setup}

We now describe our experimental setup, including 
architectural parameters of our two baseline neural network accelerators, simulation methodology to estimate performance and energy with and without \textsc{Huff-LLM}, 
LLMs and the datasets on which they are evaluated.

\paragraph{Systolic array settings and simulation.}
Table~\ref{tab:sim_spec} shows the architectural parameters 
of our baseline systolic array architecture, reflective
of an edge tensor processing unit (TPU) similar to
the Google Coral edge device~\cite{suryavansh2020google}. 
The simulated architecture has a peak performance of 16~TOPs at 16b float precision, and was evaluated with 64 GBps and 128 GBps memory bandwidth.
We model both output stationary (OS) and weight stationary (WS) architectures, as described in Section~\ref{sec:systolic_array}. %\textcolor{red}{Tianhua, Sai: can we mention some equivalent stats from commercial device; 16 TOPs, 64-128 GBps BW?} \textcolor{blue}{Updated}
We model the performance and energy of this architecture using a methodology similar to STAR-Sim~\cite{starsim} and SCALE-Sim~\cite{scalesim} that are both widely used to model systolic array architectures.
(more details in Appendix~\ref{appendix:star-sim}). However, STAR-Sim and SCALE-Sim focus on modelling convolution operations while a majority of computations in LLMs are matrix multiplications~\cite{Wang2020SpAttenES}. 
We modify SCALE-Sim for faster matrix multiplication simulations. 
%We simulate the reduction of weight buffer access by decreasing the weight buffer bitwidth.

%\paragraph{Simulators settings}
%Additionally, for a broader assessment, we use an analytical simulator based on the design specifications in Table~\ref{tab:sim_spec} to test Huff-LLM on a different system, called Star. Because the Star system's weight buffer is less than Simba's, it requires more DRAM access. We analyze the latency and energy efficiency of Huff-LLM on the Star system using the same technique, models, and jobs as in the Simba simulation to maintain consistency.
%\paragraph{Analytical simulation settings} 




%\pt{Here is a description of Simba and evaluation setup}
%\textcolor{red}{SG:please move to the eval setup section.}
\paragraph{Simba architecture settings and simulation.}  
We tabulate the architectural specifications of the evaluated Simba-like architecture in Table~\ref{Simba_specs}. Parameters were selected based on designs available in Nvidia Research's Timeloop/Accelergy repository\footnote{https://github.com/Accelergy-Project/timeloop-accelergy-exercises}. %Timelo the most widely
% used simulation framework for explicitly decoupled tensor 
%processors like Simba. 
%In our study, we utilized a Simba-like accelerator configured with an array of Processing Elements (PEs). Each PE includes dedicated local scratchpads to store weights, and activations- and also vector multiply-accumulate (VMAC) units for parallel computation. 
%This system operates at a nominal clock frequency of 2 GHz and is connected to an LPDDR4 DRAM, which provides a transfer bandwidth of 128 GB/s.
%\textcolor{red}{this should be moved to the experimental methodology section.}
%To evaluate the latency and energy efficiency of the Huff-LLM system, we perform an analytical simulation using Simba, following the specifications outlined in Table~\ref{Simba_specs}. 
%The analysis considers two configurations: one with a weight-stationary Simba featuring a DRAM bandwidth of 
The simulated architecture has a peak performance of 4~TOPs at 16b float precision, in-line with mobile NPUs~\cite{samsung_npu_isca}, and was evaluated with 64 GBps and 128 GBps memory bandwidth, similar to the systolic array. %  we model model 64 GBps and 128 GBps bandwidth settings. 
%We examine the impact of Huff-LLM on six models deployed on Simba, focusing on energy consumption and latency. %Additionally, we evaluate three benchmarks on LLaMA and OPT models by calculating the average input and output token lengths as proxies to estimate the average performance in terms of latency and energy, leveraging Timeloop and Accelergy tools.
%has been evaluated to demonstrate the generality and efficiency of our method.  can model models a wide range of mapping and scheduling strategies that have a substantial impact on performance and energy
Timeloop is an accelerator performance estimation tool developed by Nvidia~\cite{timeloop}. Timeloop can model various scheduling strategies, and estimate how they impact the energy and latency of a computation. We use Timeloops hybrid search across all our experiments to minimize inference latency first and energy
for mappings with the same latency.
%and time-out after 3000 successive valid mappings
%is capable of independently assessing mappings based on optimization objectives, such as energy, delay, or EDP(energy delay product). 
%In evaluation sets the optimization metrics to prioritize mappings with minimum delay, and if two mappings have the same delay, then prioritize mappings with lower energy. The 
Energy costs are estimated via Accelergy via access counts generated from Timeloop. 
This approach is used to calculate energy for larger memories through Cacti \cite{balasubramonian2017cacti} and smaller components such as address generators and register files using figures included in Aladdin \cite{aladdin}. 
%The delay is expressed in terms of cycles, normalized to the delay incurred for a MAC operation. To isolate mapspace generated from heuristic search,  that do not enhance delay/energy across 8 threads.

\begin{table}[t]
    \centering
    
   % \small
    \resizebox{0.9\linewidth}{!}{%
        \begin{tabular}{@{}ccc@{}} \toprule

        Tech node & \multicolumn{2}{c}{$16nm$} \\ 
        Systolic Array Size & \multicolumn{2}{c}{$128 \times 128$ PE}    \\ 
        PE Frequency & \multicolumn{2}{c}{$1GHz$}    \\ \midrule
        DRAM Bandwidth & $64GB/s$ & $128GB/s$    \\ 
        Weight Buffer Size & \multicolumn{2}{c}{$16KB$}    \\ 
        Activation Buffer Size & \multicolumn{2}{c}{$8KB$}    \\ 
        Accumulator Buffer Size & \multicolumn{2}{c}{$4KB$}    \\ 

        Dataflow & WS & OS    \\

        \bottomrule
        \end{tabular}
    }

        \caption{\sl Systolic Array Architecture Specifications.}
        \label{tab:sim_spec}
\end{table}


\begin{table}[t]
    \centering
    \label{tab:simba_arch_spec}
    \resizebox{\linewidth}{!}{%
        \begin{tabular}{@{}lccc@{}} \toprule

        \multirow{3}{*}{Chip} & Tech node & \multicolumn{2}{c}{$16nm$} \\
                                  & PE Frequency & \multicolumn{2}{c}{$2GHz$} \\
                                  & Number of PEs & \multicolumn{2}{c}{$16$} \\
        \midrule
        \multirow{5}{*}{PE} & DRAM Bandwidth & $64GB/s$ & $128GB/s$ \\
                              & Weight Buffer Size & \multicolumn{2}{c}{$32KiB$} \\
                              & Input Buffer Size & \multicolumn{2}{c}{$8KiB$} \\
                              & Accumulator Buffer Size & \multicolumn{2}{c}{$3KiB$} \\
                              & Number of Vector MACs & \multicolumn{2}{c}{$8$} \\
                              & Vector MAC Width & \multicolumn{2}{c}{$8$} \\
        \bottomrule
        \end{tabular}
    }
    \caption{\sl Simba Architecture Specifications.}
    \label{Simba_specs}
\end{table}

%\paragraph{Single Cycle Huffman Decoder Design}

%\paragraph{Analytical Simulator}

%\section{Experimental Setup} %Put specific settings for simulators for reproducability
%\paragraph{Systolic Array Settings}

%\paragraph{Simba Settings}

\paragraph{Benchmarks and Evaluated LLMs.}
Benchmarks such as MMLU~\cite{hendrycks2020measuring} are typically used to measure LLM capabilities. Works that focus on lossy compression (such as quantization) often use benchmark performance to show how much information was lost in the compression process. However, since \textsc{Huff-LLM} is a lossless compression method, the compressed LLM maintains exactly the same accuracy as the original model by construction. 

Alternatively, we can view LLM benchmarks from the perspective of their input context size. Different benchmarks have different average lengths of their inputs. For example, Arc-Easy~\cite{clark2018think} has an average length of approximxately 42 input tokens, whereas MMLU has an average length of approximately 92 input tokens. Therefore, we use benchmarks to test how \textsc{Huff-LLM}'s optimizations are impacted by various input token lengths. 

Our performance estimation system uses the average token length of a benchmark query, rather than the actual benchmark questions. We report the average token length (as determined by the Llama-3-8B tokenizer) of each benchmark used in Table~\ref{tab:benchmark}.

\begin{table}[!t]
    \centering
    \label{tab:benchmark}
        \begin{tabular}{@{}cccc@{}} \toprule
        Benchmark & ArcEasy & MMLU & Winogrande \\ \midrule
        Avg Input Tokens & 42 & 92 & 25\\
        Standard Deviation & 20 & 92 & 4 \\
        \bottomrule
        \end{tabular}
        \caption{\sl Average input token length for benchmarks. Tokens are generated with Llama-3-8B's tokenizer.}
\end{table}

We perform compression tests and hardware simulations on various notable LLM families. We included the Llama~\cite{dubey2024llama}, OPT~\cite{zhang2022opt}, Qwen~\cite{yang2024qwen2}, and Vicuna~\cite{vicuna2023} model families to show how \textsc{Huff-LLM} performs on different model architectures. In addition, we examine model sizes ranging from 3B to 13B parameters to see how model size impacts our compression scheme.
\vspace{-0.2in}
\section{Experimental Results}
%\py{\begin{itemize}
    % \item Software Side
    % \begin{itemize}
    %     \item Compression ratios for various LLMs (Look at quantization papers for LLM selection)
    %     \item Include separate section in the discussion about models that don't show up in quantization works that might have interesting discussion (gemma, etc..)
    %     \item FP16 and BF16 models
    % \end{itemize}
    %\item Hardware Side
    % \begin{itemize}
    %     \item Power/performance modeling of architectures (specify which tools used). Specify the bandwidth and other assumptions.
    %     \item Simulator Results
    %     \begin{itemize}
    %         \item Explain the two simulators and their uses (Maybe in related works?)
    %         \item Latency impact
    %         \item Energy efficiency
    %         \item Area/power breakdown for decoder
    %         \item Parameters swept: Memory Bandwidth, weight vs. output stations, impact of batch size, \# input tokens, \# output tokens
    %     \end{itemize}
    %     \item Systolic array implementation on FPGA
    %     \begin{itemize}
    %         \item Demonstrate that our idea is actually feasible
    %         \item Explain the PE structure \& bit-width selection. [Yasmein]:
    %         \textbf{Our systolic array is  weight stationary where the weights are reused over a matrix multiplication computation block. The PE architecture is as follows. First, a multiplier that receives FP16 weight (stored inside the PE) and FP16 activation input streamed every clock cycle and produces an FP32 product. Then an adder that receives the product and the partial sum (psum) of the PEi-1 j where i-1 is the previous row and j is the same column. The output of the adder is the input to the next PE in the same column PE i+1 j  ,which is also in FP32 format. The systolic array is followed by column accumulators where each column has a corresponding accumulator with a register file to save the output column elements. The accumulator accounts for full throughput tiling. In normal operation mode, the input to the accumulator is saved in the corresponding location (out00 in address 0, out10 in address 1, etc..). While in tiling mode, the accumulation is done in the same location, where first the value stored in the register file of this element is read, then after addition, instead of writing the accumulated value back, this result is fed into the accumulator again to be added to the new psum input to the accumulator. When the tiling is done, the final accumulated value is stored in the register file. To be able to read these results without affecting the full throughput of the system, there are 2 banks of registers inside the accumulator allowing reading the results from a bank while writing the new results of the new matrix block computation to the other bank.}

    %         \item Explain the data flow from buffers to systolic array to accumulators without compression. [Yasmein] :
    %         \textbf{For $n \times n$ Systolic array, First, the weights are preloaded from the weight memory into the PEs in n clock cycles (CC). then the array receives a compute enable signal that arrives with the staggered inputs arriving one at a row per CC and propagating horizontally in the column's direction. The activation inputs are read from the activation memory into a systolic array setup shift registers block that arranges the inputs in the required staggered order. In parallel with the computation process, new weights for the next matrix block can be loaded into the double buffers inside the PEs. The new matrix block activations arrive with a pulse announcing the beginning of the new matrix so that the PE can switch to the correct weights. The PE automatically stores the preloaded weights in the correct buffer and uses it with the corresponding inputs. Once the computation starts, every CC each PE receives activation input, and input psum from the PE above it in the same column, performs multiplication on the stored weight and input activation, adds the product to the input psum and outputs the adder output (output psum) to the following PE in the same column. So the accumulation happens across rows per column and  the activation inputs propagate across columns per row. After n CC the first column outputs the first element in the output matrix (out00 ). Next CC, first column outputs out10 and second column outputs out01 and so on till the last column outputs outnn.  The outputs are input to the column accumulators and are stored in the corresponding locations according to the operation mode, normal or tiling mode.}
    %         \item Use the word staggered data flow for the activation inputs to the systolic array.
    %         \item Explain Huffman decoder and the weights data flow with compression. [Shehab]
    %         \item Memory mapping of the weights matrix. [Arun]:
    %         The weight matrix contains floating point values in IEEE 754 Half Precision format. Each value contains a sign bit, 5 bits for exponent and 10 bits for mantissa. For applying the Huffman encoding and decoding in a memory efficient manner, we divide the bit string for each value into 3 equal parts (excluding the sign bit), i.e. a 5 bit exponent, two 5 bits components for mantissa. We apply Huffman encoding and decoding independently to each sub-component matrix in a parallel fashion. Each 5-bit string in every sub-component matrices get encoded to variable length bit-strings on application of Huffman encoding. For maximizing the throughput of our pipeline, we make memory reads at the maximum codeword length, L as dictated by the Huffman Lookup table. We make use of the fact that subsequent row entries are processed by the same Huffman decoding element, and organize the bit-strings in memory as continuous L bits. Since bit-strings are of variable lengths and have less number of bits than L, we pack the subsequent row bits by shifting them till L bits are obtained. An example for this is depicted in the figure X. Once the bits are organized, we read L bits for decoding each value of the sub-component matrix and retain any left-over bits for decoding the next value. Identification of left-over bits are trivial as Huffman decoding is a greedy process. We repeat the decoding process till all such variable length bit-strings of the sub-component matrix are exhausted.
    %         \item Figure for the memory mapping. [Arun]
    %          \item Figure for the TPU system. [Shehab]
    %     \end{itemize}
    % \end{itemize}
    % \item GPU Decompression vs. Our Specialized Hardware Decompression
    % \begin{itemize}
    %     \item Perform decompression: MatMul on GPU and our Hardware to compare runtimes
    % \end{itemize}
%\end{itemize}}



\subsection{Compression Experiments}
We apply our Huffman compression method to FP16 variants of popular LLM families such as Llama, OPT, Qwen, and Vicuna. The total compression ratio is calculated by averaging the compression ratio of the attention and mlp weight matrices. These results can be found in Table~\ref{tab:comp_ratios}. We notice that the Llama and Vicuna model families have similar compression ratios even at different model sizes. However, we also see that OPT and Vicuna have a notably smaller compression ratio. This suggests that there may be factors during the training stage that lead to certain distributions (and thus lossless compressibility) of the weights. 

BF16 models are also very popular for inference. Therefore, we adapt our Huffman Compression method to work with BF16 models as well. We apply the same idea when splitting the bits. We find that the bits can be split 1-4-4-7. The seven mantissa bits at the end show little to no compressibility (in contrast to FP16). The sign bit remains uncompressed, and the exponent bits are split into two groups of four. We compress various models using our method and report the ratios in Table~\ref{tab:comp_ratios}. We notice that all model families have a similar compression ratio in BF16. This is in contrast to FP16 where Vicuna and OPT had notably lower compression ratios. This could arise from the conversion process to BF16. We see that the compressibility of the mantissa (in FP16) has moved entirely to the exponent (in BF16). Therefore, in models where the FP16 compressibility is low (Vicuna), we are likely seeing higher compressibility in BF16 due to the larger number of exponent bits. 

\begin{table}[t] %Replace right two cols w/ BF16 data
    \centering
    \label{tab:comp_ratios}
    \resizebox{\columnwidth}{!}{%
        \begin{tabular}{@{}cccccc@{}} \toprule
        \textbf{Model Name} & \multicolumn{2}{c}{\textbf{FP16}} & \multicolumn{2}{c}{\textbf{BF16}} \\ \midrule
        &Bits/Param  & Ratio & Bits/Param & Ratio\\ 
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
        Llama-3.2-3B & 10.96 & 1.46 & 11.68 & 1.37\\
        Llama-3-8B & 10.96 & 1.46 & 11.68& 1.37 \\ 
       %Llama-2-7B & 11.887 & 1.346 & 11.808 &1.355\\ 
       Llama-2-13B & \textcolor{blue}{\textbf{10.88}} & \textcolor{blue}{\textbf{1.47}} & 11.59 & 1.38\\ \hline
          OPT-2.7B & 13.68 & 1.17 & 11.68&1.37\\
        OPT-6.7B & \textcolor{red}{\textbf{13.78}} & \textcolor{red}{\textbf{1.16}}& 11.68 &1.37 \\
         OPT-13B & 13.68 & 1.17 & 11.59&1.38\\ \hline
       % OPT-2.7B & \textbf{13.68} & \textbf{1.17} & &\\
       %  OPT-6.7B & \textbf{13.78} &\textbf{1.16}& & \\
       %   OPT-13B & \textbf{13.68} &\textbf{1.17} & &\\ \hline
         Qwen-2.5-3B & 10.96&1.46 & 11.68 & 1.37\\
        Qwen-2.5-7B & 10.96 &1.46 &11.68&1.37\\ \hline
        Vicuna-7B & 13.68 & 1.17& 11.59&1.38\\
        Vicuna-13B &  13.68&1.17 & 11.59&1.38\\ 
        % Vicuna-7B & \textbf{13.68} & \textbf{1.17}& &\\
        % Vicuna-13B &  \textbf{13.68}&\textbf{1.17} & &\\ 
        \bottomrule
        \end{tabular}
    }

        \caption{\sl Compression ratio is calculated as an average of all weight matrices in each model. Bits/Param is calculated by dividing the uncompressed Bits/Param (16) by the compression ratio. Highest and lowest ratios are highlighted.}
        \vspace{-0.2in}
\end{table}

%Move output stationary details + experiments to discussion section
\subsection{Hardware Results}


\paragraph{Latency and Energy Savings}
Tables~\ref{tab:main_result_latency} and~\ref{tab:main_result_energy} present the latency and energy savings achieved when applying \textsc{Huff-LLM} to various FP16 models. We explore the Llama and OPT model families because they represent the best and worst case scenario for \textsc{Huff-LLM}. Since compression ratio plays the largest role in determining these simulation results, we omit Qwen and Vicuna results due to their similarity with Llama and OPT respectively. In addition, full OPT results can be found in the Appendix in Section~\ref{sec:opt_results}.

The \textsc{Huff-LLM} compression scheme leads to significant latency improvements, ranging from 26\% to 31\% for LLaMA models and 13\% to 15\% for OPT models. In terms of energy savings, LLaMA models also achieve notable gains of 16\% to 26\%, whereas OPT models see improvements of 3\% to 10\%. Additionally, reducing the DRAM bandwidth from 128 GB/s to 64 GB/s slightly enhances latency improvements by 2\% to 3\% but reduces energy savings by a similar margin. A comparable trend is observed for the OPT models. For Systolic Array architectures we see higher energy savings than for Simba architectures. This is due to the smaller weight buffer and increased reliance on DRAM access; the Systolic Array benefits more from \textsc{Huff-LLM} because a larger portion of its overall energy consumption comes from memory-related operations.


% We assess the latency and energy savings of Huff-LLM on a different system, referred to as the Star system, using an analytical simulator based on the design specifications outlined in Table~\ref{tab:sim_spec}. Compared to the Simba system, the Star system has a smaller weight buffer, resulting in increased DRAM access under the same conditions. The evaluation is conducted on the same models and tasks as in the Simba simulation.

% Table~\ref{tab:starsim-llama-latency} and Table~\ref{tab:starsim-llama-energy} present the latency and energy savings achieved when compressing tje Llama models from 16-bit to 11-bit. Table~\ref{tab:starsim-opt-latency} and Table~\ref{tab:starsim-opt-energy} illustrate the latency and energy reductions for OPT models when the weights are compressed from 16-bit to 14-bit. Compared to the Simba system, the Star system achieves a comparable latency reduction, ranging from $27\%$ to $32\%$ for Llama models and $13\%$ to $15\%$ for OPT models. However, the Star system benefits from greater energy efficiency with Huff-LLM, as the weight buffer and DRAM access constitute a more significant portion of the system's overall energy consumption.



%\textcolor{blue}{will refine the wordings}
% To evaluate the latency and energy saving of the Huff-LLM system, we conduct analytical simulation on the system with specifications described in Tabel~\ref{tab:sim_spec}. Specifically, we have four settings for the Huff-LLM system. The first setting is a weight stationary systolic array with a DRAM bandwidth of 64GB/s, referred to as \textbf{WS-64}. The second setting is a weight stationary systolic array with a DRAM bandwidth of 128GB/s, referred to as \textbf{WS-128}. The third setting is an output stationary systolic array a DRAM bandwidth of 64GB/s, referred to as \textbf{OS-64}. The fourth setting is an output stationary systolic array with a DRAM bandwidth of 128GB/s, referred to as \textbf{OS-128}. 


% \begin{table}[ht]
%     \centering
%     \resizebox{0.75\columnwidth}{!}{%
%         \begin{tabular}{@{}lcc@{}} \toprule

%         Tech node & \multicolumn{2}{c}{$16nm$} \\ \midrule
%         PE Frequency & \multicolumn{2}{c}{$1GHz$} \\
%         Number of PEs & \multicolumn{2}{c}{$64 \times 64$} \\
%         Number of MACs per PE& \multicolumn{2}{c}{$4$} \\ \midrule
%         DRAM Bandwidth & $64GB/s$ & $128GB/s$ \\
%         Weight Buffer Size & \multicolumn{2}{c}{$16KiB$} \\
%         Input Buffer Size & \multicolumn{2}{c}{$8KiB$} \\
%         Accumulator Buffer Size & \multicolumn{2}{c}{$4KiB$} \\
        

%         \bottomrule
%         \end{tabular}
%     }
%     \caption{Analytical Simulator Design Specifications}
%     \label{tab:sim_spec}
% \end{table}


% \begin{comment}
% %\begin{table}[ht]
%     \centering
%     \label{tab:analytical_hw_speed}
%     \resizebox{\columnwidth}{!}{%
%         \begin{tabular}{@{}ccccccc@{}} \toprule
%         & & \multicolumn{2}{c}{64GB/s} & \multicolumn{2}{c}{128GB/s} \\
%         \cmidrule(lr){3-4} \cmidrule(lr){5-6}
%         Model & Task & WS  & OS  & WS & OS  \\ \midrule
%         \multirow{3}{*}{Llama-3.2-3B} 
%         & G8 &$23.80\%$ & $27.24\%$ & $21.38\%$ & $26.04\%$ & \\

%         & HS &$21.35\%$ & $24.89\%$ & $18.85\%$ & $23.66\%$ \\

%         & WG &$24.35\%$ & $27.76\%$ & $21.94\%$ & $26.57\%$ \\\hline

%         \multirow{3}{*}{Llama-3-8B} 
%         & G8 &$24.06\%$ & $27.49\%$ & $21.65\%$ & $26.29\%$ \\

%         & HS &$21.67\%$ & $25.20\%$ & $19.18\%$ & $23.97\%$ \\

%         & WG &$24.41\%$ & $27.82\%$ & $22.01\%$ & $26.63\%$ \\\hline

%         \multirow{3}{*}{Llama-2-13B} 
%         & G8 &$24.07\%$ & $27.49\%$ & $21.66\%$ & $26.30\%$ \\

%         & HS &$21.59\%$ & $25.13\%$ & $19.11\%$ & $23.90\%$ \\

%         & WG &$24.40\%$ & $27.81\%$ & $22.00\%$ & $26.62\%$ \\

%         \bottomrule
%         \end{tabular}
%     }
%     %\caption{\sl Latency reduction when compressing the model weights from 16-bit to 11-bit. WS and OS refer to weight stationary and output stationary systolic array, respectively. 64GB/s and 128GB/s are two settings of the DRAM bandwidth. The results are from analytical simulation. \textcolor{red}{maybe add a row of avg speedup for each model family? We may also want to comment on whether WS vs. OS is better for each task? Also, since the results are similar, maybe in the main draft we keep only a subset of tasks and move some to the appendix will make our lives easier.} \textcolor{red}{actually with just 3 avg is not needed, please drop.} \textcolor{blue}{will swap the benchmark} }
% %\end{table}    
% \end{comment}

%Be specific on how the tasks are done. We calculate the average token input/output length for three benchmarks and use those as proxies for average performance (latency/energy) on these benchmarks.
%\paragraph{Analytical simulation results}

% Table~\ref{tab:starsim-llama-latency} and Table~\ref{tab:starsim-llama-energy} show the latency and energy saving when the weights of Llama models with different sizes are compressed from 16-bit to 11-bit across various tasks over different settings, respectively. 

% The compression improvement slightly increases as the model size increases. This is because the portion of weight movement increases in larger models. The compression scheme consistently achieves larger improvement in output stationary system compared to weight stationary system. This is because output stationary system has less weight reuse as shown in Equation~\ref{equation:wb_rd}, thus leading to higher weight data movement overhead. The systems with higher DRAM bandwidth get less benefit from the weight compression technique compared to lower DRAM bandwidth system because higher DRAM bandwidth looses the memory constrain of accessing model weights.

% Figure~\ref{fig:starsim-roofline} (a) shows the roofline plot of two system. The first one is the WS-128 system and the other one has same settings as WS-128, except the DRAM bandwidth is increased to 256GB/s, denoted as WS-256. The red and green dashed line denotes the task of executing Llama3.2-3B model without and with compression over the MMLU task with an input sequence length of 93, respectively. When the bandwidth increases, the system becomes compute bounded, thus compressing weights does not bring further benefits.
% Figure~\ref{fig:starsim-roofline} (b) shows the roofline plot of the WS-128 system executing the MMLU task with an input sequence length of 180. The increase of input sequence length makes the system compute bounded. In this case, the system also does not get benefits from the compressed model weights. These two experiments indicates the scope of weight compression mechanism: improving performance of memory bounded systems.

% Table~\ref{tab:starsim-opt-latency} and Table~\ref{tab:starsim-opt-energy} show the latency and energy saving when the weights of OPT models with different sizes are compressed from 16-bit to 14-bit, a lower compression rate compared to the Llama models. However, the evaluation results show substantial performance improvement still can be achieved thanks to the compression mechanism.



\begin{table*}[h!]
\centering
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{cccccccccc}
\hline

\textbf{Benchmark} & \textbf{Bandwidth}          & \multicolumn{2}{c}{\textbf{Llama 2-13B}} & \multicolumn{2}{c}{\textbf{Llama 3-8B} }& \multicolumn{2}{c}{\textbf{Llama 3.2-3B}} & \multicolumn{2}{c}{\textbf{OPT-13B}}\\ \hline
& & Systolic Array & Simba & Systolic Array & Simba & Systolic Array & Simba & Systolic Array & Simba\\
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10}
\multirow{2}{*}{MMLU} & 64 GB/s             & 29.41\%            & 28.18\% & 29.40\%           &  31.33\%  & 28.50\%  & 31.07\% &  14.77\% & 14.05\%           \\ %\cline{2-8}
                      %& OS-64           & 31.00\%             & 30.99\%              & 30.11\%              \\ \cline{2-5}
                      & 128GB/s             & 27.17\%    &    \textcolor{red}{\textbf{26.97\%}}  & 28.97\%  & \textcolor{red}{\textbf{27.16\%}}         &  28.22\%   & \textcolor{red}{\textbf{26.23\%}} &14.01\% & \textcolor{red}{\textbf{13.33\%}}              \\ \hline
                      %& OS-128          & 31.52\%             & 31.51\%             & 30.63\%               \\ \hline
\multirow{2}{*}{Winogrande} & 64 GB/s        & 29.86\%              &31.11\% &29.86\%           &  31.11\% & 29.44\% & \textcolor{blue}{\textbf{31.38\%}}  & \textcolor{blue}{\textbf{15.16\%}} & 14.29\%            \\ %\cline{2-5}
                      %& OS-64           & 31.44\%              & 31.44\%              & 31.03\%               \\ \cline{2-5}
                      & 128 GB/s             & 27.63\%            & 29.60\% & 27.63\%           &  29.58\% & 27.20\%   & 29.58\%  &14.41\% & 13.33\%          \\ \hline
                      %& OS-128          & 31.95\%              & 31.95\%              & 31.55\%               \\ \hline
\multirow{2}{*}{ArcEasy} & 64 GB/s          & 29.74\%            & \textcolor{blue}{\textbf{31.40\%}} & 29.74\%            & \textcolor{blue}{\textbf{31.35\%}} & 29.20\% & 31.33\%  & 15.06\% & 14.29\%            \\ %\cline{2-5}
                      %& OS-64           & 31.33\%              & 31.32\%              & 30.79\%               \\ \cline{2-5}
                      & 128 GB/s             & 27.51\%            & 29.60\% & 27.51\%           &  29.58\% & 26.95\% & 29.53\% & 14.30\% & \textcolor{red}{\textbf{13.33\%}}             \\ \hline
                      %& OS-128          & 31.84\%              & 31.84\%              & 31.31\%               \\ \hline
\end{tabular}
\end{adjustbox}
\caption{Latency savings achieved when applying Huff-LLM to different FP16 models. Savings are simulated for Systolic Arrays  and Simba with weight stationary (WS) architectures. Highest and lowest improvements are highighted.}
\label{tab:main_result_latency}
\end{table*}

\begin{table*}[h!]
\centering
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{cccccccccc}
\hline

\textbf{Benchmark} & \textbf{Bandwidth}          & \multicolumn{2}{c}{\textbf{Llama 2-13B}} & \multicolumn{2}{c}{\textbf{Llama 3-8B} }& \multicolumn{2}{c}{\textbf{Llama 3.2-3B}} & \multicolumn{2}{c}{\textbf{OPT-13B}}\\ \hline
& & Systolic Array & Simba & Systolic Array & Simba & Systolic Array & Simba & Systolic Array & Simba\\
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10}
\multirow{2}{*}{MMLU} & 64 GB/s             &     23.76\%        & \textcolor{red}{\textbf{16.12\%}} &    23.75\%        &  \textcolor{red}{\textbf{16.47\%}}  &        22.78\%  &\textcolor{red}{\textbf{16.67\%}}  & 9.14\% & 3.40\%  \\ %\cline{2-8}
                      %& OS-64           & 31.00\%             & 30.99\%              & 30.11\%              \\ \cline{2-5}
                      & 128GB/s             &    25.55\%       & 19.79\%  &    25.54\%      &     19.33\%&  24.59\%   & 19.72\%   & 9.71\%  & 6.41\%     \\ \hline
                      %& OS-128          & 31.52\%             & 31.51\%             & 30.63\%               \\ \hline
\multirow{2}{*}{Winogrande} & 64 GB/s        &   24.25\%          & 18.98\%&   24.25\%       & 18.68\%  &      23.80\% &18.82\% & 9.56\% & 4.82\%     \\ %\cline{2-5}
                      %& OS-64           & 31.44\%              & 31.44\%              & 31.03\%               \\ \cline{2-5}
                      & 128 GB/s             &    \textcolor{blue}{\textbf{26.03\%}}        &  20.30\%&    \textcolor{blue}{\textbf{26.02\%}}      &   19.56\%&   \textcolor{blue}{\textbf{25.58\%}}   & 19.98\%  & \textcolor{blue}{\textbf{10.13\%}}  & 6.05\%    \\ \hline
                      %& OS-128          & 31.95\%              & 31.95\%              & 31.55\%               \\ \hline
\multirow{2}{*}{ArcEasy} & 64 GB/s          &    24.12\%         & 17.44\% &     24.12\%      & 18.22\% &     23.53\% &17.74\%   & 9.45\%  & \textcolor{red}{\textbf{3.31\%}}   \\ %\cline{2-5}
                      %& OS-64           & 31.33\%              & 31.32\%              & 30.79\%               \\ \cline{2-5}
                      & 128 GB/s             &    25.90\%      & 20.78\% &    25.90\%    &  20.21\% &   25.32\% &19.61\%  & 10.02\%  & 5.00\% \\ \hline
                      %& OS-128          & 31.84\%              & 31.84\%              & 31.31\%               \\ \hline
\end{tabular}
\end{adjustbox}
\caption{Energy savings achieved when applying Huff-LLM to various FP16 models. Savings are simulated for Systolic Arrays and Simba with weight stationary (WS) architectures. Highest and lowest improvements are highighted.}
\label{tab:main_result_energy}
\end{table*}



% \begin{comment}
% %\begin{table}[ht]
%     \centering
%     \label{tab:analytical_hw_speed}
%     \resizebox{\columnwidth}{!}{%
%         \begin{tabular}{@{}ccccccc@{}} \toprule
%         & & \multicolumn{2}{c}{64GB/s} & \multicolumn{2}{c}{128GB/s} \\
%         \cmidrule(lr){3-4} \cmidrule(lr){5-6}
%         Model & Task & WS  & OS  & WS & OS  \\ \midrule
%         \multirow{3}{*}{Llama-3.2-3B} 
%         & G8 &$20.02\%$ & $23.20\%$ & $21.90\%$ & $23.10\%$ & \\

%         & HS &$17.45\%$ & $20.72\%$ & $19.38\%$ & $20.62\%$ \\

%         & WG &$20.59\%$ & $23.75\%$ & $22.46\%$ & $23.65\%$ \\\hline

%         \multirow{3}{*}{Llama-3-8B} 
%         & G8 &$20.30\%$ & $23.46\%$ & $22.16\%$ & $23.36\%$ \\

%         & HS &$17.78\%$ & $21.04\%$ & $19.71\%$ & $20.94\%$ \\

%         & WG &$20.66\%$ & $23.81\%$ & $22.52\%$ & $23.71\%$ \\\hline

%         \multirow{3}{*}{Llama-2-13B} 
%         & G8 &$20.30\%$ & $23.47\%$ & $22.17\%$ & $23.37\%$ \\

%         & HS &$17.71\%$ & $20.97\%$ & $19.64\%$ & $20.87\%$ \\

%         & WG &$20.65\%$ & $23.80\%$ & $22.51\%$ & $23.70\%$ \\

%         \bottomrule
%         \end{tabular}
%     }

%         %\caption{\sl Energy efficiency improvement when compressing the model weights from 16-bit to 11-bit. WS and OS refer to weight stationary and output stationary systolic array, respectively. 64GB/s and 128GB/s are two settings of the DRAM bandwidth. The results are from analytical simulation.}
% %\end{table}    
% \end{comment}

% \begin{table}[h!]
% \centering
% \begin{adjustbox}{max width=\textwidth/2}
% \begin{tabular}{|c|c|c|c|c|}

% \hline
% \textbf{Benchmark} & \textbf{Bandwidth}          & \textbf{Llama 2-13B} & \textbf{Llama 3-8B} & \textbf{Llama 3.2-3B} \\ \hline
% \multirow{2}{*}{MMLU} & 64 GB/s             & 23.76\%             & 23.75\%              & 22.78\%               \\ \cline{2-5}
%                       %& OS-64           & 24.37\%             & 24.36\%              & 23.39\%              \\ \cline{2-5}
%                       & 128 GB/s             & 25.55\%             & 25.54\%              & 24.59\%               \\ \hline
%                       %& OS-128          & 26.69\%             & 26.68\%             & 25.75\%               \\ \hline
% \multirow{2}{*}{Winogrande} & 64 GB/s        & 24.25\%              & 24.25\%              & 23.80\%               \\ \cline{2-5}
%                       %& OS-64           & 24.85\%              & 24.85\%              & 24.40\%               \\ \cline{2-5}
%                       & 128 GB/s              & 26.03\%              & 26.02\%              & 25.58\%               \\ \hline
%                       %& OS-128          & 27.16\%              & 27.16\%              & 26.73\%               \\ \hline
% \multirow{2}{*}{ArcEasy} & 64 GB/s          & 24.12\%              & 24.12\%              & 23.53\%               \\ \cline{2-5}
%                       %& OS-64           & 24.73\%              & 24.72\%              & 24.14\%               \\ \cline{2-5}
%                       & 128 GB/s              & 25.90\%              & 25.90\%              & 25.32\%               \\ \hline
%                       %& OS-128          & 27.04\%              & 27.04\%              & 26.47\%               \\ \hline
% \end{tabular}
% \end{adjustbox}
% \caption{Energy saving when compressing the Llama model weights from 16-bit to 11-bit. The results are from the Star system simulation. }
% \label{tab:starsim-llama-energy}
% \end{table}




%Make the two figures vertical (on top of each other)
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/roof2.jpg}
    %\vspace*{-0.5\baselineskip}
    %\vspace*{-1\baselineskip}
    \vspace{-0.1in}
    \caption{(a) Roofline plot of Systolic Arrays with 128GB/s and 256GB/s DRAM bandwidth. Dashed lines show the baseline and Huff-LLM models, with intersections marking operational points. (b) Energy breakdown of Huff-LLM and baseline model on the Systolic Array with 256GB/s DRAM bandwidth.} 
    \label{fig:starsim-roofline}
    %\vspace*{-1\baselineskip}
\end{figure}

% \begin{table}[h!]
% \centering
% %\caption{Specifications of Simba Architecture}
% \label{tab:simba-specs}
% \begin{tabular}{@{}ll@{}}
% \toprule
% \textbf{Feature}            & \textbf{Specification}               \\ \midrule
% \textbf{Process Technology} & 16nm                        \\
% \textbf{DRAM Bandwidth}             & 64GB/s        128GB/s    \\
% \textbf{PEs Number}         & 16*64                    \\
% \textbf{Frequency}    & 2 GHz                       \\
% \textbf{Weight Buffer Size}     & 32 KiB                \\
% \textbf{Input Buffer Size}       & 8 KiB          \\ 
% \textbf{Accumulation Buffer Size}       & 3 KiB          \\\bottomrule
% \end{tabular}
% \caption{Simba Architecture Specifications}
% \end{table}


% \begin{table}[h!]
% \centering
% \begin{adjustbox}{max width=\textwidth/2}
% \begin{tabular}{|c|c|c|c|c|}
% \hline
% \textbf{Benchmark} & \textbf{Component}          & \textbf{Llama 2-13B} & \textbf{Llama 3-8B} & \textbf{Llama 3.2-3B} \\ \hline
% \multirow{4}{*}{MMLU} & Up Projection             & 22.15\%             & 19.1\%              & 31.59\%               \\ \cline{2-5}
%                       & Down Projection           & 22.17\%             & 31.6\%              & 18.71\%              \\ \cline{2-5}
%                       & Key \& Value             & 31.58\%             & 20.5\%              & 31.58\%               \\ \cline{2-5}
%                       & Query \& Output          & 31.58\%             & 31.58\%             & 22.15\%               \\ \hline
% \multirow{4}{*}{Winogrande} & Up Projection        & 31.58\%              & 31.7\%              & 31.58\%               \\ \cline{2-5}
%                       & Down Projection           & 31.58\%              & 31.7\%              & 31.58\%               \\ \cline{2-5}
%                       & Key \& Value             & 31.63\%              & 31.6\%              & 31.58\%               \\ \cline{2-5}
%                       & Query \& Output          & 31.63\%              & 31.8\%              & 31.58\%               \\ \hline
% \multirow{4}{*}{ArcEasy} & Up Projection          & 28.91\%              & 30.7\%              & 28.7\%               \\ \cline{2-5}
%                       & Down Projection           & 31.58\%              & 31.7\%              & 31.58\%               \\ \cline{2-5}
%                       & Key \& Value             & 31.58\%              & 31.6\%              & 28.8\%               \\ \cline{2-5}
%                       & Query \& Output          & 31.58\%              & 31.58\%              & 31.7\%               \\ \hline
% \end{tabular}
% \end{adjustbox}
% \caption{Latency saving for different benchmarks when the weights for Llama models are compressed from 16-bit to 11-bit. }
% \end{table}





% \begin{table}[h!]
% \centering
% \begin{adjustbox}{max width=\textwidth/2}
% \begin{tabular}{|c|c|c|c|c|}
% \hline
% \textbf{Benchmark} & \textbf{Component}          & \textbf{Llama 2-13B} & \textbf{Llama 3-8B} & \textbf{Llama 3.2-3B} \\ \hline
% \multirow{4}{*}{MMLU} & Up Projection             & 13.54\%             & 11.32\%              & 13.67\%               \\ \cline{2-5}
%                       & Down Projection           & 14.19\%             & 16.49\%              & 12.77\%              \\ \cline{2-5}
%                       & Key \& Value             & 13.22\%             & 11.5\%              & 13.72\%               \\ \cline{2-5}
%                       & Query \& Output          & 13.22\%             & 14.4\%             & 14.02\%               \\ \hline
% \multirow{4}{*}{Winogrande} & Up Projection        & 19.89\%              & 18.64\%              & 19.82\%               \\ \cline{2-5}
%                       & Down Projection           & 19.74\%              & 19.22\%              & 29.8\%               \\ \cline{2-5}
%                       & Key \& Value             & 19.18\%              & 20.09\%              & 19.8\%               \\ \cline{2-5}
%                       & Query \& Output          & 19.18\%              & 20.19\%              & 20.11\%               \\ \hline
% \multirow{4}{*}{ArcEasy} & Up Projection          & 19\%              & 18.83\%              & 14.02\%               \\ \cline{2-5}
%                       & Down Projection           & 17.07\%              & 16.91\%              & 14.4\%               \\ \cline{2-5}
%                       & Key \& Value             & 16.77\%              & 15.22\%              & 15\%               \\ \cline{2-5}
%                       & Query \& Output          & 16.77\%              & 14.42\%              & 17.7\%               \\ \hline
% \end{tabular}
% \end{adjustbox}
% \caption{Energy saving for different benchmarks when the weights for Llama models are compressed from 16-bit to 11-bit.}
% \end{table}

% \begin{table}[h!]
% \centering
% \begin{adjustbox}{max width=\textwidth/2}
% \begin{tabular}{|c|c|c|c|c|}
% \hline
% \textbf{Benchmark} & \textbf{Component}          & \textbf{Llama 2-13B} & \textbf{Llama 3-8B} & \textbf{Llama 3.2-3B} \\ \hline
% \multirow{4}{*}{MMLU} & Up Projection             & 32.62\%             & 28.1\%              & 41.02\%               \\ \cline{2-5}
%                       & Down Projection           & 33.18\%             & 42.86\%              & 29.25\%              \\ \cline{2-5}
%                       & Key \& Value             & 40.56\%             & 30.6\%              & 40.63\%               \\ \cline{2-5}
%                       & Query \& Output          & 40.56\%             & 41.3\%             & 34.1\%               \\ \hline
% \multirow{4}{*}{Winogrande} & Up Projection        & 45.14\%              & 44.31\%              & 45.08\%               \\ \cline{2-5}
%                       & Down Projection           & 45.1\%              & 44.76\%              & 45\%               \\ \cline{2-5}
%                       & Key \& Value             & 44.55\%              & 42.55\%              & 45.22\%               \\ \cline{2-5}
%                       & Query \& Output          & 44.55\%              & 45.33\%              & 45.4\%               \\ \hline
% \multirow{4}{*}{ArcEasy} & Up Projection          & 42.47\%              & 41.86\%              & 38.71\%               \\ \cline{2-5}
%                       & Down Projection           & 43.25\%              & 43.13\%              & 41.52\%               \\ \cline{2-5}
%                       & Key \& Value             & 42.91\%              & 41.87\%              & 39.56\%               \\ \cline{2-5}
%                       & Query \& Output          & 42.91\%              & 41.39\%              & 43.78\%               \\ \hline
% \end{tabular}
% \end{adjustbox}
% \caption{EDP saving for different benchmarks when the weights for Llama models are compressed from 16-bit to 11-bit.}
% \end{table}

% \begin{table}[h!]
% \centering
% \begin{adjustbox}{max width=\textwidth/2}
% \begin{tabular}{|c|c|c|c|c|}
% \hline
% \multirow{1}{*}{BenchMark}   & \multirow{1}{*}{Bandwidth} & Llama 3.2-3B & Llama 3-8B & Llama 2-13B        \\ \hline
% \multirow{2}{*}{MMLU}        & 64 GB/s                   & 31.07\%      & 31.33\%    & 28.18\%     \\ \cline{2-5} 
%                              & 128 GB/s                  & 28.22\%      & 28.97\%    & 26.97\%     \\ \hline
% \multirow{2}{*}{Arceasy}     & 64 GB/s                   & 31.33\%      & 31.35\%    & 31.4\%      \\ \cline{2-5} 
%                              & 128 GB/s                  & 29.53\%      & 29.58\%    & 29.6\%      \\ \hline
% \multirow{2}{*}{Winogrande}  & 64 GB/s                   & 31.38\%      & 31.33\%    & 31.33\%     \\ \cline{2-5} 
%                              & 128 GB/s                  & 29.58\%      & 29.58\%    & 29.6\%      \\ \hline
% \end{tabular}
% \end{adjustbox}
% \caption{Latency saving for different benchmarks at 64 GB/s and 128 GB/s bandwidth simulated on timeloop on Simba for Llama models with compressed weights from 16-bit to 11-bit.}
% \label{llama_latency_timeloop}
% \end{table}

% \begin{table}[h!]
% \centering
% \begin{adjustbox}{max width=\textwidth/2}
% \begin{tabular}{|c|c|c|c|c|}
% \hline
% \multirow{1}{*}{BenchMark}   & \multirow{1}{*}{Bandwidth} & Llama 3.2-3B & Llama 3-8B & Llama 2-13B      \\ \hline
% \multirow{2}{*}{MMLU}        & 64 GB/s                   & 16.67\%      & 16.47\%    & 16.12\%     \\ \cline{2-5} 
%                              & 128 GB/s                  & 19.72\%      & 19.33\%    & 19.79\%     \\ \hline
% \multirow{2}{*}{Arceasy}     & 64 GB/s                   & 17.74\%      & 18.22\%    & 17.44\%     \\ \cline{2-5} 
%                              & 128 GB/s                  & 19.61\%      & 20.21\%    & 20.78\%     \\ \hline
% \multirow{2}{*}{Winogrande}  & 64 GB/s                   & 18.82\%      & 18.68\%    & 18.98\%     \\ \cline{2-5} 
%                              & 128 GB/s                  & 19.98\%      & 19.56\%    & 20.3\%     \\ \hline
% \end{tabular}
% \end{adjustbox}
% \caption{Energy saving for different benchmarks at 64 GB/s and 128 GB/s bandwidth simulated on timeloop on Simba for Llama models with compressed weights from 16-bit to 11-bit.}
% \label{llama_energy_timeloop}
% \end{table}



% \py{\begin{itemize}
%     % \item Discuss the BF16 left-most 4 bits and how Huffman Coding can't reach entropy level. Talk about run-length coding and potential 1-bit benefit.
%     % \item Compare results for various models
%     \item Explore how different hardware costs are impacted by compression
%     \item Talk about future chip design w/ this compression in mind
%     \item Future works
%     % \item Discuss negative results about grouping source symbols and how this leads to higher compression ratios, but (unreasonably) large Huffman Tables
% \end{itemize}}

% \paragraph{Other Model Families}

% We see in Table~\ref{tab:comp_ratios_other} that other model families may follow different trends for compression ratios. We see that Qwen-2.5 matches up with the Llama family, but the Vicuna and OPT model families have a lower compression ratio. This suggests that the training process used for each model family has an impact on the compressibility of the parameters. 

% \paragraph{BF16 Model Variants}
% \begin{table}[ht]
%     \centering
%     \label{tab:comp_ratios_bf}
%     \resizebox{\columnwidth}{!}{%
%         \begin{tabular}{@{}cccccc@{}} \toprule
%         Model Name & Bits/Param  & Ratio & Original Size (GB) & Compressed Size (GB)\\ \midrule
%         Llama-3.2-3B & 11.68&1.37\\
%         Llama-3-8B & 11.68&1.37\\ 
%        %Llama-2-7B & 11.887 & 1.346 & 11.808 &1.355\\ 
%        Llama-2-13B & 11.59&1.38\\ \hline
%        OPT-2.7B & 11.68&1.37\\
%         OPT-6.7B & 11.68 & 1.37\\
%         OPT-13B & 11.59 & 1.38\\ \hline
%         Qwen-2.5-3B &  11.68& 1.37\\
%         Qwen-2.5-7B & 11.68 & 1.37\\ \hline
%         Vicuna-7B & 11.59 & 1.38\\
%         Vicuna-13B &11.59 &1.38\\ 
%         \bottomrule
%         \end{tabular}
%     }

%         \caption{\sl BF16 models. Compression ratio is calculated as an average of all weight matrices in each model. Bits/Param is calculated by dividing the uncompressed Bits/Param (16) by the compression ratio.}
% \end{table}



% \begin{table}[h!]
% \centering
% \begin{adjustbox}{max width=\textwidth/2}
% \begin{tabular}{|c|c|c|c|c|}
% \hline

% \textbf{Benchmark} & \textbf{Bandwidth}          & \textbf{Llama 2-13B} & \textbf{Llama 3-8B} & \textbf{Llama 3.2-3B} \\ \hline
% \multirow{2}{*}{MMLU} %& WS-64             & 29.41\%             & 29.40\%              & 28.50\%               \\ \cline{2-5}
%                       & 64 GB/s           & 31.00\%             & 30.99\%              & 30.11\%              \\ \cline{2-5}
%                       %& WS-128             & 27.17\%             & 27.16\%              & 26.23\%               \\ \hline
%                       & 128 GB/s          & 31.52\%             & 31.51\%             & 30.63\%               \\ \hline
% \multirow{2}{*}{Winogrande} %& WS-64        & 29.86\%              & 29.86\%              & 29.44\%               \\ \cline{2-5}
%                       & 64 GB/s           & 31.44\%              & 31.44\%              & 31.03\%               \\ \cline{2-5}
%                       %& WS-128             & 27.63\%              & 27.63\%              & 27.20\%               \\ \hline
%                       & 128 GB/s          & 31.95\%              & 31.95\%              & 31.55\%               \\ \hline
% \multirow{2}{*}{ArcEasy} %& WS-64          & 29.74\%              & 29.74\%              & 29.20\%               \\ \cline{2-5}
%                       & 64 GB/s           & 31.33\%              & 31.32\%              & 30.79\%               \\ \cline{2-5}
%                       %& WS-128             & 27.51\%              & 27.51\%              & 26.95\%               \\ \hline
%                       & 128 GB/s          & 31.84\%              & 31.84\%              & 31.31\%               \\ \hline
% \end{tabular}
% \end{adjustbox}
% \caption{Latency saving when compressing the Llama model weights from 16-bit to 11-bit. The Star system is configured to be output stationary. }
% \label{tab:starsim-llama-latency-os}
% \end{table}

% \begin{table}[h!]
% \centering
% \begin{adjustbox}{max width=\textwidth/2}
% \begin{tabular}{|c|c|c|c|c|}

% \hline
% \textbf{Benchmark} & \textbf{Bandwidth}          & \textbf{Llama 2-13B} & \textbf{Llama 3-8B} & \textbf{Llama 3.2-3B} \\ \hline
% \multirow{2}{*}{MMLU} %& WS-64             & 23.76\%             & 23.75\%              & 22.78\%               \\ \cline{2-5}
%                       & 64 GB/s           & 24.37\%             & 24.36\%              & 23.39\%              \\ \cline{2-5}
%                       %& WS-128             & 25.55\%             & 25.54\%              & 24.59\%               \\ \hline
%                       & 128 GB/s          & 26.69\%             & 26.68\%             & 25.75\%               \\ \hline
% \multirow{2}{*}{Winogrande} %& WS-64        & 24.25\%              & 24.25\%              & 23.80\%               \\ \cline{2-5}
%                       & 64 GB/s           & 24.85\%              & 24.85\%              & 24.40\%               \\ \cline{2-5}
%                       %& WS-128              & 26.03\%              & 26.02\%              & 25.58\%               \\ \hline
%                       & 128 GB/s          & 27.16\%              & 27.16\%              & 26.73\%               \\ \hline
% \multirow{2}{*}{ArcEasy} %& WS-64          & 24.12\%              & 24.12\%              & 23.53\%               \\ \cline{2-5}
%                       & 64 GB/s           & 24.73\%              & 24.72\%              & 24.14\%               \\ \cline{2-5}
%                       %& WS-128              & 25.90\%              & 25.90\%              & 25.32\%               \\ \hline
%                       & 128 GB/s          & 27.04\%              & 27.04\%              & 26.47\%               \\ \hline
% \end{tabular}
% \end{adjustbox}
% \caption{Energy saving when compressing the Llama model weights from 16-bit to 11-bit. The Star system is configured to be output stationary. }
% \label{tab:starsim-llama-energy-os}
% \end{table}
%\paragraph{Output Stationary Systolic Arrays.} 
%The Systolic Array architecture can be used in a weight stationary or output stationary configuration. Weight stationary means that weights are stored inside the PE. On the other hand, output stationary architectures store the outputs in the PE (while not storing the weights in the PE).
The results thus far are for weight stationary (WS) architectures. 
%We compare the weight stationary results from Table~\ref{tab:main_result_latency} and Table~\ref{tab:main_result_energy} to the output stationary results in 
Table~\ref{tab:output_stationary} shows latency reductions and energy savings
for output stationary (OS) systolic arrays;
we observe Huff-LLM consistently achieves larger improvements in output stationary architectures because the output stationary system has less weight reuse. This leads to higher weight movement overhead which can be reduced by Huff-LLM.

\paragraph{Area Overheads} To estimate the area overheads of the proposed Huff-LLM scheme, we implemented %synthesized 
a 5-bit Huffman Decoder and a systolic array PE in Verilog (a hardware description language) and synthesized these blocks for a Global Foundries 12nm (GF12) technology. 
Each PE has an area of 484$um^2$ while 
an HD with an empirically determined
$L_{max}=12$ is 1199.3$um^2$. For a 128$\times$128 systolic array, the area overhead of Huff-LLMs is $6.13\%$. The 
area overhead at the full chip level would be even lower since we have not accounted for on-chip buffers in the denominator. Custom, highly-optimized CAM structures~\cite{yue202415} can lower overheads further.


\section{Discussion}

% Table~\ref{tab:starsim-llama-latency-os} and Table~\ref{tab:starsim-llama-energy-os} show the latency and energy saving of Huff-LLM when the Star system is configured to be weight stationary. 
% When comparing with Table~\ref{tab:starsim-llama-latency} and Table~\ref{tab:starsim-llama-energy}, Huff-LLM consistently achieves larger improvement in output stationary system. This is because output stationary system has less weight reuse, thus leading to higher weight data movement overhead.

Higher DRAM bandwidth diminishes the latency savings achieved by Huff-LLM, as weight movement accounts for a smaller fraction of the overall processing time. Additionally, as DRAM bandwidth increases, the system becomes more compute-bound. To evaluate the limits of Huff-LLM, we configure the Systolic Array with a DRAM bandwidth of 256GB/s and simulate its performance.
To shed further light on Huff-LLM's performance gains,
Figure~\ref{fig:starsim-roofline}(a) presents a roofline plot~\cite{roofline2008david} for Systolic Arrays executing the MMLU task on the Llama-3.2-3B model. Weight compression 
results in larger FLOPs/Byte since we need to fetch Bytes from memory, resulting in
higher performance (FLOP/s) for 128 GB/s bandwidth. 
However for an even higher 
256 GB/s bandwidth, even the baseline model 
runs at peak performance, so compression will not reduce latency further (this is true for even 8-bit quantization). 
%The red dashed line represents the baseline where model weights use FP16 precision, while the green dashed line represents Huff-LLM, which compresses weights to 11-bit. The intersection of each dashed line with the rooflines represents the operational point of the task on the respective hardware configuration. In the 128GB/s system, Huff-LLM effectively enhances processing speed. However, in the 256GB/s system, it does not yield a performance gain.
Despite the reduced latency improvement in higher bandwidth systems, Huff-LLM \emph{still lowers energy consumption} by reducing memory access operations. Figure~\ref{fig:starsim-roofline}(b) illustrates the energy breakdown for Huff-LLM and the 16-bit baseline on the 256GB/s system, using the same MMLU task on the Llama-3.2-3B model. While system speed remains unchanged, Huff-LLM achieves a $24\%$ reduction in energy consumption.
This is largely because of the dominant energy costs of fetching data from memory, compared to other on-chip costs. 

\section{Conclusion}
In this work, we propose Huff-LLM, an end-to-end model compression method for LLMs. We observe that Huffman Compression can be applied to subsets of weight parameters with minimal impact on the compression ratio. We use this observation to develop a compression scheme and hardware design that has minimal area overhead and is fast. We show up to 32\% reduction in model size, up to 31\% improvement in inference latency, and up to 26\% reduction in energy cost. 


\section*{Impact Statement}

This paper presents work whose goal is to advance the field of  Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Additional results}

You can have as much text here as you want. The main body must be at most $8$ pages long.
For the final version, one more page can be added.
If you want, you can use an appendix like this one.  

The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Analytical Simulator}
\label{appendix:star-sim}
To map the multiplication of input matrix $I$ and weight matrix $W$, with shape of $I_H \times I_W$ and $W_H \times W_W$, we adopted same dataflow-dependent mapping schemes as SCALE-Sim~\cite{scalesim}, where the dimensions of two operand matrices are defined as $S_R \times T$ and $T \times S_C$, as shown in Figure~\ref{fig:pe_hd}. $S_R$ and $S_C$ are the specific workload dimensions mapped to the rows and columns of a systolic array, respectively, and T is the temporal dimension, along which the data are being streamed into the systolic array. Table~\ref{tab:sa_map} summarizes the definition of $S_R$, $S_C$, and $T$ depending on the dataflow configuration. 

\begin{table}[ht]
    \centering
    \label{tab:sa_map}
   % \small
   % \resizebox{0.75\columnwidth}{!}{%
        \begin{tabular}{@{}cccc@{}} \toprule

        Dataflow & $S_R$ & $S_C$  & $T$   \\ \midrule
        Weight Stationary & $W_H$ &$W_W$ & $I_H$  \\

        %Input Stationary & $I_W$ & $I_H$ &  $W_W$ \\

        Output Stationary & $I_H$ & $W_W$ & $W_H$  \\

        \bottomrule
        \end{tabular}
   % }

        \caption{\sl Spatial and temporal mapping of the input matrix (I) with shape of $I_H \times I_W$ and weight matrix (W) with shape of $W_H \times W_W$ to the rows and columns of a systolic array. $I_W$ is equal to $W_H$.}
\end{table}


Since one systolic array may not be sufficient to accommodate the entire matrix computation in common LLM layers, the workload is typically partitioned into "folds"~\cite{scalesim} with respect to the rows ($R$) and columns ($C$) of the PE array. The number of folds along the row dimension ($F_R$) and column dimension ($F_C$) can be calculated as:
\begin{equation}
    F_R=\lceil \frac{S_R}{R} \rceil, F_C=\lceil \frac{S_C}{C} \rceil
\end{equation}

We use the same principle as SCALE-Sim~\cite{scalesim} to model the number of compute cycles as below:
\begin{equation}
    L_{COMP} = (2R+C+T-2)\cdot F_R \cdot F_C
\end{equation}

For modeling of buffer access, we consider the stationary data and streaming data separately. The stationary operand in the systolic array will get updated only after being fully reused by the streaming operand, so the number of read accesses of stationary data is equal to the number of stationary data. In contrast, the streaming data may need to be reloaded by multiple times, for which the reloading count is equal to the number of folds. As such, the number of weight buffer read accesses ($WB_{RD}$) and the number of input buffer read accesses ($IB_{RD}$) can be modeled as below:
\begin{equation}
\label{equation:wb_rd}
    WB_{RD} =
\begin{cases} 
W_H \cdot W_W, & \text{if } WS \\
%I_W \cdot W_W \cdot \lceil \frac{I_H}{C} \rceil, & \text{if } IS \\
W_W \cdot W_H \cdot \lceil \frac{I_H}{R} \rceil, & \text{if } OS
\end{cases}
\end{equation}

\begin{equation}
\label{ib_rd}
   IB_{RD} =
\begin{cases} 
W_H \cdot I_H \cdot \lceil \frac{W_W}{C} \rceil, & \text{if } WS \\
%I_H \cdot I_W, & \text{if } IS \\
I_H \cdot W_H \cdot \lceil \frac{W_W}{C} \rceil, & \text{if } OS
\end{cases} 
\end{equation}

The MAC latency is modeled as the product of the total compute cycles and the cycle time. The memory read/write latency is modeled as the division of read/write data and buffer bandwidth. 

The MAC energy is modeled as the product of the total number of MAC operations and the energy per MAC operation. The memory read/write access energy is modeled as the product of the unit energy per read/write access and the number of read/write accesses. 

\subsection{Additional Results}\label{sec:opt_results}
Full OPT results and output stationary results are included in this section.

\begin{table}[h!]
\centering
\begin{adjustbox}{max width=\textwidth/2}
\begin{tabular}{|c|c|c|c|c|}
\hline

\textbf{Benchmark} & \textbf{Bandwidth}          & \textbf{OPT-13B} & \textbf{OPT-6.7B} & \textbf{OPT-2.7B} \\ \hline
\multirow{2}{*}{MMLU} & 64 GB/s             & 14.77\%             & 14.63\%              & 14.23\%               \\ \cline{2-5}
%                      & 64GB/s OS           & 31.00\%             & 30.99\%              & 30.11\%              \\ \cline{2-5}
                      & 128 GB/s             & 14.01\%             & 13.87\%              & 13.47\%               \\ \hline
%                      & 128GB/s OS          & 31.52\%             & 31.51\%             & 30.63\%               \\ \hline
\multirow{2}{*}{Winogrande} & 64 GB/s        & 15.16\%              & 15.10\%              & 14.93\%               \\ \cline{2-5}
%                      & 64GB/s OS           & 31.44\%              & 31.44\%              & 31.03\%               \\ \cline{2-5}
                      & 128 GB/s             & 14.41\%              & 14.35\%              & 14.17\%               \\ \hline
%                      & 128GB/s OS          & 31.95\%              & 31.95\%              & 31.55\%               \\ \hline
\multirow{2}{*}{ArcEasy} & 64 GB/s          & 15.06\%              & 14.98\%              & 14.75\%               \\ \cline{2-5}
%                      & 64GB/s OS           & 31.33\%              & 31.32\%              & 30.79\%               \\ \cline{2-5}
                      & 128 GB/s             & 14.30\%              & 14.22\%              & 13.99\%               \\ \hline
%                      & 128GB/s OS          & 31.84\%              & 31.84\%              & 31.31\%               \\ \hline
\end{tabular}
\end{adjustbox}
\caption{Latency saving when compressing the OPT model weights from 16-bit to 14-bit. The results are from the Star system simulation. }
\label{tab:starsim-opt-latency}
\end{table}



\begin{table}[h!]
\centering
\begin{adjustbox}{max width=\textwidth/2}
\begin{tabular}{|c|c|c|c|c|}
\hline

\textbf{Benchmark} & \textbf{Bandwidth}          & \textbf{OPT-13B} & \textbf{OPT-6.7B} & \textbf{OPT-2.7B} \\ \hline
\multirow{2}{*}{MMLU} &64 GB/s             & 9.14\%             & 9.00\%              & 8.57\%               \\ \cline{2-5}
%                      & 64GB/s OS           & 24.37\%             & 24.36\%              & 23.39\%              \\ \cline{2-5}
                      & 128 GB/s             & 9.71\%             & 9.57\%              & 9.14\%               \\ \hline
%                      & 128GB/s OS          & 26.69\%             & 26.68\%             & 25.75\%               \\ \hline
\multirow{2}{*}{Winogrande} & 64 GB/s        & 9.56\%              & 9.50\%              & 9.31\%               \\ \cline{2-5}
%                      & 64GB/s OS           & 24.85\%              & 24.85\%              & 24.40\%               \\ \cline{2-5}
                      & 128 GB/s             & 10.13\%              & 10.06\%              & 9.88\%               \\ \hline
%                      & 128GB/s OS          & 27.16\%              & 27.16\%              & 26.73\%               \\ \hline
\multirow{2}{*}{ArcEasy} & 64 GB/s          & 9.45\%              & 9.37\%              & 9.12\%               \\ \cline{2-5}
%                      & 64GB/s OS           & 24.73\%              & 24.72\%              & 24.14\%               \\ \cline{2-5}
                      & 128 GB/s             & 10.02\%              & 9.94\%              & 9.69\%               \\ \hline
%                      & 128GB/s OS          & 27.04\%              & 27.04\%              & 26.47\%               \\ \hline
\end{tabular}
\end{adjustbox}
\caption{Energy saving when compressing the OPT model weights from 16-bit to 14-bit. The results are from the Star system simulation.}
\label{tab:starsim-opt-energy}
\end{table}

\begin{table}[h!]
\centering
\begin{adjustbox}{max width=\textwidth/2}
\begin{tabular}{|c|c|c|c|c|}
\hline
\multirow{1}{*}{\textbf{BenchMark}}   & \multirow{1}{*}{\textbf{Bandwidth}} & \textbf{OPT-13B} & \textbf{OPT-6.7B} & \textbf{OPT-2.7B}      \\ \hline
\multirow{2}{*}{MMLU}        & 64 GB/s                   & 14.05\%      & 14.29\%    & 14.05\%     \\ \cline{2-5} 
                             & 128 GB/s                  & 13.33\%      & 13.35\%    & 13.33\%     \\ \hline
\multirow{2}{*}{Arceasy}     & 64 GB/s                   &14.29 \%      & 14.2\%    & 14.4\%     \\ \cline{2-5} 
                             & 128 GB/s                  & 13.33\%      & 13.29\%    & 13.36\%     \\ \hline
\multirow{2}{*}{Winogrande}  & 64 GB/s                   & 14.29\%      & 14.35\%    & 14.31\%     \\ \cline{2-5} 
                             & 128 GB/s                  & 13.33\%      & 13.35\%    & 13.33\%     \\ \hline
\end{tabular}
\end{adjustbox}
\caption{Latency saving for different benchmarks at 64 GB/s and 128 GB/s bandwidth simulated on timeloop on Simba for OPT models with compressed weights from 16-bit to 14-bit.}
\label{OPT_latency_timeloop}
\end{table}


\begin{table}[h!]
\centering
\begin{adjustbox}{max width=\textwidth/2}
\begin{tabular}{|c|c|c|c|c|}
\hline
\multirow{1}{*}{\textbf{BenchMark}}   & \multirow{1}{*}{\textbf{Bandwidth}} & \textbf{OPT-13B} & \textbf{OPT-6.7B} & \textbf{OPT-2.7B}      \\ \hline
\multirow{2}{*}{MMLU}        & 64 GB/s                   & 3.4\%      & 3.5\%    & 3.38\%     \\ \cline{2-5} 
                             & 128 GB/s                  & 6.41\%      & 6.93\%    & 6.89\%     \\ \hline
\multirow{2}{*}{Arceasy}     & 64 GB/s                   & 3.31\%      & 3.48\%    & 4.02\%     \\ \cline{2-5} 
                             & 128 GB/s                  & 5\%      & 5.78\%    & 6.1\%     \\ \hline
\multirow{2}{*}{Winogrande}  & 64 GB/s                   & 4.82\%      & 5.13\%    & 5.2\%     \\ \cline{2-5} 
                             & 128 GB/s                  & 6.05\%      & 6.23\%    & 6.37\%     \\ \hline
\end{tabular}
\end{adjustbox}
\caption{Energy saving for different benchmarks at 64 GB/s and 128 GB/s bandwidth simulated on timeloop on Simba for OPT models with compressed weights from 16-bit to 14-bit.}
\label{OPT_energy_timeloop}
\end{table}

\begin{table*}[h!]
\centering
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{cccccccccc}
\hline

\textbf{Benchmark} & \textbf{Bandwidth}          & \multicolumn{2}{c}{\textbf{Llama 2-13B}} & \multicolumn{2}{c}{\textbf{Llama 3-8B} }& \multicolumn{2}{c}{\textbf{Llama 3.2-3B}} \\ \hline
& & Latency & Energy & Latency & Energy & Latency & Energy \\
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} 
\multirow{2}{*}{MMLU} & 64 GB/s  & 31.00\% &24.37\% & 30.99\% & 24.36\%& 30.11\% & 23.39\%\\ %\cline{2-8}
                      %& OS-64           & 31.00\%             & 30.99\%              & 30.11\%              \\ \cline{2-5}
                      & 128GB/s & 28.31\% & 26.69\%& 28.30\% & 26.68\%& 27.38\% & 25.75\%          \\ \hline
                      %& OS-128          & 31.52\%             & 31.51\%             & 30.63\%               \\ \hline
\multirow{2}{*}{Winogrande} & 64 GB/s  & 31.44\% & 24.85\%& 31.44\% & 24.85\%& 31.03\% & 24.40\%\\ %\cline{2-5}
                      %& OS-64           & 31.44\%              & 31.44\%              & 31.03\%               \\ \cline{2-5}
                      & 128 GB/s   & 28.76\% & 27.16\%& 28.76\% & 27.16\%& 28.34\% & 26.73\%         \\ \hline
                      %& OS-128          & 31.95\%              & 31.95\%              & 31.55\%               \\ \hline
\multirow{2}{*}{ArcEasy} & 64 GB/s & 31.33\% &24.73\% & 31.32\% & 24.72\%& 30.79\% & 24.14\% \\ %\cline{2-5}
                      %& OS-64           & 31.33\%              & 31.32\%              & 30.79\%               \\ \cline{2-5}
                      & 128 GB/s  & 28.65\% & 27.04\%& 28.64\% & 27.04\%& 28.09\% & 26.47\% \\ \hline
                      %& OS-128          & 31.84\%              & 31.84\%              & 31.31\%               \\ \hline
\end{tabular}
\end{adjustbox}
\caption{Latency and Energy savings achieved when applying Huff-LLM FP16 models. Simulations are performed on an output stationary (OS) systolic array architecture.}
\label{tab:output_stationary}
\end{table*}



% \begin{table*}[h!]
% \centering
% \begin{adjustbox}{max width=\textwidth}
% \begin{tabular}{cccccccccc}
% \hline

% \textbf{Benchmark} & \textbf{Bandwidth}          & \multicolumn{2}{c}{\textbf{Llama 2-13B}} & \multicolumn{2}{c}{\textbf{Llama 3-8B} }& \multicolumn{2}{c}{\textbf{Llama 3.2-3B}} & \multicolumn{2}{c}{\textbf{OPT-13B}}\\ \hline
% & & Systolic Array & Simba & Systolic Array & Simba & Systolic Array & Simba & Systolic Array & Simba\\
% \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10}
% \multirow{2}{*}{MMLU} & 64 GB/s             &             &  &            &    &               \\ %\cline{2-8}
%                       %& OS-64           & 31.00\%             & 30.99\%              & 30.11\%              \\ \cline{2-5}
%                       & 128GB/s             &           &   &          &     &               \\ \hline
%                       %& OS-128          & 31.52\%             & 31.51\%             & 30.63\%               \\ \hline
% \multirow{2}{*}{Winogrande} & 64 GB/s        &             & &          &   &              \\ %\cline{2-5}
%                       %& OS-64           & 31.44\%              & 31.44\%              & 31.03\%               \\ \cline{2-5}
%                       & 128 GB/s             &            &  &          &   &              \\ \hline
%                       %& OS-128          & 31.95\%              & 31.95\%              & 31.55\%               \\ \hline
% \multirow{2}{*}{ArcEasy} & 64 GB/s          &             &  &           &  &              \\ %\cline{2-5}
%                       %& OS-64           & 31.33\%              & 31.32\%              & 30.79\%               \\ \cline{2-5}
%                       & 128 GB/s             &          &  &        &   &         \\ \hline
%                       %& OS-128          & 31.84\%              & 31.84\%              & 31.31\%               \\ \hline
% \end{tabular}
% \end{adjustbox}
% \caption{Energy savings achieved when applying Huff-LLM to various FP16 models. Savings are simulated for Systolic Arrays and Simba.}
% \label{tab:main_result_energy}
% \end{table*}

\subsection{Huffman Decoder Figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/Huffman_Decoder.drawio.pdf}
    \caption{Each Huffman Decoder module follows the process shown in this figure. L$_{max}$ bits are taken from the register and a match is found in the Huffman Table. Afterwards, the decoded source symbol is sent to the PE while the length is sent to update the start position S.}
    \label{fig:pe_hd_appdx}
\end{figure}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
