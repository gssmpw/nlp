\section{Related Works}
%\subsection{Preference Alignment}

\begin{figure*}[ht]
\centering
\includegraphics[scale=0.67]{overview.pdf}
\vspace{-6mm}
\caption{Overview of OntoTune which aligns LLMs with ontology through in-context learning.}
\label{fig:method}
\end{figure*}

\subsection{Domain-specific LLMs}
%\subsection{Medical Large Language Models}
Existing domain-specific large language models (LLMs) can be categorized into two groups: (1) those models trained from scratch using domain-specific corpora, such as BioGPT \cite{DBLP:journals/bib/LuoSXQZPL22} and GatorTron \cite{DBLP:journals/corr/abs-2203-03540}, and (2) those \cite{DBLP:conf/icde/00070HCGYBZYSWY23,DBLP:conf/acl/ZhangCFLL0C24,DBLP:conf/acl/LabrakBMGRD24, DBLP:journals/corr/abs-2303-17564} that employ continual training on general-purposed models. Benefiting from its ability to leverage the extensive and diverse data of the seed models, as well as more efficient training processes, the latter approach has gradually become mainstream. Current domain-specific LLMs like BioMistral \cite{DBLP:conf/acl/LabrakBMGRD24}, BloombergGPT \cite{DBLP:journals/corr/abs-2303-17564} and LawGPT \cite{DBLP:journals/corr/abs-2406-04614} are developed by training a seed model with a large-scale raw domain-specific corpora, demonstrating impressive performance on domain tasks. To be specific, the medical model PMC-LLaMA \cite{DBLP:journals/corr/abs-2304-14454} is fine-tuned with LoRA \cite{DBLP:conf/iclr/HuSWALWWC22}  on LLaMA using 4.8 million biomedical papers. LawGPT \cite{DBLP:journals/corr/abs-2406-04614}  continues training on 500k legal documents. And BloombergGPT \cite{DBLP:journals/corr/abs-2303-17564} is fine-tuned on a 708 billion tokens financial corpora. These models typically rely on large amounts of training data to adapt to their respective domains. However, this fragmented knowledge from the raw corpora is merely injected into the seed model without being systematically organized and recent research \cite{DBLP:conf/iclr/ChengHW24,dorfner2024biomedical} have indicated that directly using these fragmented raw corpora is not efficient. Additionally, prior researches seldom utilize ontologies as foundational knowledge sources for training corpora. Compared to fragmented large-scale corpora, concept-level structured knowledge in ontologies can play a significant role in knowledge management \cite{DBLP:journals/corr/abs-2407-10671} and semantic search \cite{DBLP:conf/www/ZhangPWCZZBC19,DBLP:conf/acl/0002WHZS0WLFS023}, and also have the potential to empower LLMs. Recently, TaxoLLaMA \cite{DBLP:conf/acl/MoskvoretskiiNL24} develops a lexical semantic LLM via directly employing the WordNet \cite{DBLP:conf/naacl/Miller94a} ontology for instruction-tuning, achieving state-of-the-art performance in multiple lexical semantic tasks and highlighting the potential of ontologies for developing domain-specific LLMs.


\subsection{Self-Generated Data for Training}
The self-training paradigm involves generating data autonomously and using this self-generated data for further training. Traditional self-training methods \cite{DBLP:conf/iclr/HeGSR20,DBLP:conf/cvpr/XieLHL20,DBLP:journals/corr/abs-2202-12040,DBLP:conf/emnlp/0001GHW00023} typically employ a trained model to annotate data, and then improve model performance based on these newly annotated data. Due to its simplicity and efficiency, this training paradigm is also migrating to LLMs. Given the high costs of manually annotating training data or using more powerful proprietary models like GPT-4 \cite{DBLP:journals/corr/abs-2303-08774}, many works \cite{DBLP:conf/icml/MengMHZA023,DBLP:conf/acl/YangPFWCZL24,DBLP:conf/acl/WangKMLSKH23,DBLP:journals/corr/abs-2402-06457,DBLP:journals/tmlr/SinghCAAPGLH0XP24} have begun to leverage the language model itself to synthesize training data. STaR \cite{DBLP:conf/nips/ZelikmanWMG22} is a self-taught reasoner that learns from its own generated reasoning steps to improve reasoning ability. Furthermore, SDFT \cite{DBLP:conf/acl/YangPFWCZL24} proposes a self-distillation fine-tuning method to achieve more efficient and less damaging results. Alternatively, Lin et al. \cite{DBLP:journals/corr/abs-2404-07965} use gold answers to train a reward model for evaluating generated instructions separately. However, previous self-training approaches usually rely on gold labels to filter out low-quality instruction data, and they tend to focus more on improvements within a single dataset. Unlike previous methods, our OntoTune mitigates performance degradation caused by incorrect labels by refining and reorganizing internal domain knowledge of the seed model through open-ended instructions \cite{DBLP:conf/iclr/0009CMZYSZ24,DBLP:conf/acl/TyenMCCM24}.