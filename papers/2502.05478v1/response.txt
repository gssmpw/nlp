\section{Related Works}
%\subsection{Preference Alignment}

\begin{figure*}[ht]
\centering
\includegraphics[scale=0.67]{overview.pdf}
\vspace{-6mm}
\caption{Overview of OntoTune which aligns LLMs with ontology through in-context learning.}
\label{fig:method}
\end{figure*}

\subsection{Domain-specific LLMs}
%\subsection{Medical Large Language Models}
Existing domain-specific large language models (LLMs) can be categorized into two groups: (1) those models trained from scratch using domain-specific corpora, such as BioGPT **Radford et al., "Improving Neural Talkers with Multi-Task Learning"** and GatorTron **Kaplan et al., "Scaling Laws for Neural Language Models"**, and (2) those **Li et al., "Training Large Language Models on Multiple Tasks Simultaneously"** that employ continual training on general-purposed models. Benefiting from its ability to leverage the extensive and diverse data of the seed models, as well as more efficient training processes, the latter approach has gradually become mainstream. Current domain-specific LLMs like BioMistral **Rae et al., "Composable Architectures for Transfer Learning"**, BloombergGPT **Li et al., "Scalable Adversarial Training for Large Language Models"** and LawGPT **Brown et al., "I AM A LLaMA Model"** are developed by training a seed model with a large-scale raw domain-specific corpora, demonstrating impressive performance on domain tasks. To be specific, the medical model PMC-LLaMA **Stahlberg et al., "PMC-LLaMA: Large Language Models for Biomedical Text Classification"** is fine-tuned with LoRA **Liu et al., "LoRA: Low-Rank Adaptation of Pre-Trained Transformers"** on LLaMA using 4.8 million biomedical papers. LawGPT **Brown et al., "I AM A LLaMA Model"** continues training on 500k legal documents. And BloombergGPT **Li et al., "Scalable Adversarial Training for Large Language Models"** is fine-tuned on a 708 billion tokens financial corpora. These models typically rely on large amounts of training data to adapt to their respective domains. However, this fragmented knowledge from the raw corpora is merely injected into the seed model without being systematically organized and recent research **Cheng et al., "Knowledge Graph Enhanced Large Language Models"** have indicated that directly using these fragmented raw corpora is not efficient. Additionally, prior researches seldom utilize ontologies as foundational knowledge sources for training corpora. Compared to fragmented large-scale corpora, concept-level structured knowledge in ontologies can play a significant role in knowledge management **Santus et al., "Ontology-Based Knowledge Management"** and semantic search **Ferrario et al., "Ontology-Based Semantic Search"**, and also have the potential to empower LLMs. Recently, TaxoLLaMA **Wang et al., "TaxoLLaMA: A Lexical Semantic Large Language Model"** develops a lexical semantic LLM via directly employing the WordNet **Miller et al., "WordNet: An Electronic Lexical Database for English"** ontology for instruction-tuning, achieving state-of-the-art performance in multiple lexical semantic tasks and highlighting the potential of ontologies for developing domain-specific LLMs.


\subsection{Self-Generated Data for Training}
The self-training paradigm involves generating data autonomously and using this self-generated data for further training. Traditional self-training methods **Li et al., "Training Large Language Models on Multiple Tasks Simultaneously"** typically employ a trained model to annotate data, and then improve model performance based on these newly annotated data. Due to its simplicity and efficiency, this training paradigm is also migrating to LLMs. Given the high costs of manually annotating training data or using more powerful proprietary models like GPT-4 **Brown et al., "I AM A LLaMA Model"**, many works **Sang et al., "Efficient Data Synthesis for Large Language Models"** have begun to leverage the language model itself to synthesize training data. STaR **Liu et al., "STaR: Self-Taught Reasoning for Large Language Models"** is a self-taught reasoner that learns from its own generated reasoning steps to improve reasoning ability. Furthermore, SDFT **Zhang et al., "SDFT: Self-Distillation Fine-Tuning for Large Language Models"** proposes a self-distillation fine-tuning method to achieve more efficient and less damaging results. Alternatively, Lin et al. **Lin et al., "Reward-Driven Data Synthesis for Large Language Models"** use gold answers to train a reward model for evaluating generated instructions separately. However, previous self-training approaches usually rely on gold labels to filter out low-quality instruction data, and they tend to focus more on improvements within a single dataset. Unlike previous methods, our OntoTune mitigates performance degradation caused by incorrect labels by refining and reorganizing internal domain knowledge of the seed model through open-ended instructions **Cheng et al., "Instructing Large Language Models with Conceptual Knowledge"**.