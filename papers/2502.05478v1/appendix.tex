\section{Dataset Details}\label{app:a}
\begin{list}{\huge\textbullet}{\leftmargin=1.5em}
    \item \textbf{SemEval-2018 Task 9}  \cite{DBLP:conf/semeval/Camacho-Collados18} includes 5 different sub-task, covering three languages (English, Spanish, and Italian) and two specific domains (medicine and music). We select 4 subsets for our study: 1A (English), 1B (Italian), 1C (Spanish), and 2A (Medical), to test the model's multilingual and medical ontology reasoning performance. The number of samples in the training/test sets are as follows: 1500/1500, 1000/1000, 1000/1000, and 500/500, respectively.
    \item \textbf{MedMCQA} \cite{DBLP:conf/chil/PalUS22} comprises 193k 4-option questions, with a test set of 4,183 sampled questions. This dataset is sourced from Indian medical entrance exams (AIIMS/NEET-PG) and encompasses 2,400 healthcare topics across 21 medical subjects. 
    \item \textbf{MedQA} \cite{DBLP:journals/corr/abs-2009-13081} is derived from the United States Medical Licensing Examination (USMLE) and includes 11,451 questions from professional medical board exams. These questions are presented in a multiple-choice format with 4-5 options. 
    \item \textbf{PubMedQA} \cite{DBLP:conf/emnlp/JinDLCL19} is sourced from PubMed abstracts, with questions requiring answers of ``yes," ``no," or ``maybe" for a given abstract. This dataset includes 211k artificially generated samples as the training sets and 1,000 expert-labeled samples as the test sets. 
    \item \textbf{USMLE step1-3} \cite{DBLP:journals/corr/abs-2304-08247} is a self-assessment dataset based on the United States Medical Licensing Examination (USMLE) Step 1, Step 2, and Step 3, which excludes all questions containing images.

\end{list}


\begin{table}[ht]
\centering
\caption{The statistics of medical QA datasets, including the number of training and testing sets, answer options, with only the PubMedQA containing context.}

\resizebox{0.47\textwidth}{!}{
\begin{tabular}{lccc}
\toprule
\textbf{Dataset}     & \textbf{Context}                     & \textbf{Train\textbackslash Test}       & \textbf{Answer options}              \\ \midrule
MedMCQA     & \XSolidBrush & 182822/4183 & A/B/C/D                     \\
MedQA       & \XSolidBrush & 10178/1273  & A/B/C/D/(E)                 \\
PubMedQA    & \Checkmark   & 211269/1000 & Yes/No/Maybe                \\
USMLE-step1 & \XSolidBrush & 0/94        & A/B/C/D/(E)/(F)/(G)/(H)/(I) \\
USMLE-step2 & \XSolidBrush & 0/109       & A/B/C/D/(E)/(F)/(G)         \\
USMLE-step3 & \XSolidBrush & 0/122       & A/B/C/D/(E)/(F)/(G)         \\ \bottomrule 
\end{tabular}}
\label{tab:dataset}
\end{table}


\begin{figure*}
\centering
\includegraphics[scale=0.5]{RQ1.png}

\caption{The distribution of consistency scores for response $y$ and reference response $y^o$ before and after OntoTune.}
\label{fig:rq1}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[scale=0.16]{medqa.pdf}
\caption{Examples of prompts for the evaluation of MedQA.}
\label{medqa}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[scale=0.16]{medmcqa.pdf}
\caption{Examples of prompts for the evaluation of MdeMCQA.}
\label{medmcqa}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[scale=0.16]{pubmedqa.pdf}
\caption{Examples of prompts for the evaluation of PubMedQA.}
\label{pubmedqa}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[scale=0.16]{usmle-step.pdf}
\caption{Examples of prompts for the evaluation of USMLE-step 1-3.}
\label{step}
\end{figure*}

\begin{table*}
\centering
\caption{Results of the medical domain QA in the zero-shot and supervised fine-tuning (on evaluation) setting. The best results are highlighted in bold, while the second best are underlined.}
\resizebox{\textwidth}{!}{
\begin{tabular}{clcccccccc}
\toprule \textbf{Seed Model}  & \textbf{Model}  & \textbf{SFT(eval data)} & \textbf{MedQA} & \textbf{MedMCQA} & \textbf{PubMedQA} & \textbf{USMLE-step1} & \textbf{USMLE-step2} & \textbf{USMLE-step3} & \textbf{Average}  \\ \midrule
- &LLaMA2 7B \cite{DBLP:journals/corr/abs-2307-09288}  & \XSolidBrush    &33.4	&36.0 	&54.8	&31.9	&38.5	&41.0 	&39.3         \\ - &Mistral 7B v0.2 \cite{DBLP:journals/corr/abs-2310-06825}  & \XSolidBrush   & 40.5           & 38.8             & 42.1              & 46.8                 & 45.0                 & 45.9                 & 43.2          \\
- &Qwen2 7B \cite{DBLP:journals/corr/abs-2407-10671}  & \XSolidBrush    & 46.7           & 48.6             & 55.8              & 48.9                 & 56.9                 & 54.9                 & 52.0          \\
- &GPT3.5-turbo  & \XSolidBrush  & 
\textbf{53.4}           & 53.2	             & 72.7              & 56.4                 & \textbf{64.2}                 & 54.1                 & \textbf{59.0}          \\
- &LLaMA3.1 8B   & \XSolidBrush  & 45.8           & 53.2             & \textbf{74.8}              & 54.3                 & \underline{57.8}                 & 59.0                 & 57.5          \\ \midrule
 \multirow{5}{*}{LLaMA3 8B} &LLaMA3 8B \cite{dubey2024llama} & \XSolidBrush    & 51.7           & 51.7             & 70.3              & \underline{57.4}                 & 52.3                 & 58.2                 & 56.9          \\
% LLaMA2 13B \cite{DBLP:journals/corr/abs-2307-09288} &- & \XSolidBrush   & 28.0           & 38.4             & 50.6              & 26.6                 & 35.8                 & 35.2                 & 35.8          \\ 
&TaxoLLaMA*\cite{DBLP:conf/acl/MoskvoretskiiNL24}  & \XSolidBrush    & 50.5           & 46.1             & \underline{73.4}              & 42.6                 & 39.4                 & 47.5                 & 49.9         \\  \cmidrule{2-10}
&\textbf{OntoTune$_{sft}$}  & \XSolidBrush  & 51.5           & \underline{56.7}             & 72.0              & \underline{57.4}                 & 54.1                 & \underline{60.7}                 & \underline{58.7}         \\
&\textbf{OntoTune$_{dpo}$}  & \XSolidBrush  & \underline{53.3}           & \textbf{57.2}             & 65.5              & \textbf{58.5}                 & 51.4                 & 59.0                 & 57.4         \\
&\textbf{OntoTune$_{sft+dpo}$} & \XSolidBrush   & 51.9           & \underline{56.7}             & 66.3              & 53.2                 & 54.1                 & \textbf{63.1}                 & 57.6         \\ \midrule
Mistral 7B v0.1 &BioMistral 7B \cite{DBLP:conf/acl/LabrakBMGRD24}   & \Checkmark  & 32.1           & 44.5             & 63.0              & 40.4                 & 39.4                 & 47.5                 & 46.3          \\
LLaMA 7B &MedAlpaca 7B \cite{DBLP:journals/corr/abs-2304-08247}  & \Checkmark  & 32.9           & 36.7             & 54.3              & 37.2                 & 36.7                 & 31.1                 & 41.2          \\
LLaMA2 7B &Hippocrates 7B \cite{DBLP:journals/corr/abs-2404-16621}  & \Checkmark  & 45.2           & 52.3             & 73.3              & 44.7                 & 44.0                 & 45.1                 & 50.8          \\ \midrule
\multirow{8}{*}{LLaMA3 8B} &LLaMA3* 8B \cite{dubey2024llama}  & \Checkmark  & 56.4           & 53.9             & 77.2              & 56.4                 & 56.0                 & 61.5                 & 60.2         \\
&TaxoLLaMA* \cite{DBLP:conf/acl/MoskvoretskiiNL24} & \Checkmark   & 55.9           & 57.5             & 77.6              & 56.4                 & \underline{57.8}                 & 59.0                 & 60.7          \\ 
&Aloe \cite{DBLP:journals/corr/abs-2405-01886}  & \Checkmark   &51.1	&56.8	&75.4	&54.3	&\textbf{61.5}	&60.7	&60.0   \\ 
&Med42-v2 \cite{DBLP:journals/corr/abs-2408-06142}  & \Checkmark   &57.8 &58.1 &74.6 
 &\underline{60.6} &\underline{57.8} &61.5	&61.7  \\ 
&jsl-medllama-v18  & \Checkmark   &\textbf{59.3} &57.3	&71.0 &44.7 &\underline{57.8} &62.3	&58.7   \\  \cmidrule{2-10}
&\textbf{OntoTune}$_{sft}$   & \Checkmark    & \underline{58.4}  & 60.4    & 78.6     & 57.4                 & \underline{57.8}                 & 62.3        & \underline{62.5} \\
&\textbf{OntoTune}$_{dpo}$  & \Checkmark   & 58.3           &60.7            & \textbf{79.4}              & 55.3                 & 54.1                 & 61.5                 & 61.6         \\
&\textbf{OntoTune}$_{sft+dpo}$  & \Checkmark  & 58.2           & 60.5             & \underline{78.9}              & 57.4                 & 54.1                 & \underline{63.9}                 & 62.2    \\ \midrule
\multirow{3}{*}{Qwen2 7B} &Qwen2* 7B \cite{DBLP:journals/corr/abs-2407-10671}   & \Checkmark  &55.1	&60.3	&75.2	&54.3	&56.0	&\underline{63.9}                 & 60.8         \\ 
&TaxoLLaMA* \cite{DBLP:conf/acl/MoskvoretskiiNL24} & \Checkmark   &54.3	&\underline{60.8}	&75.0	&58.5	&\textbf{61.5}	&\textbf{64.8}                 & \underline{62.5}          \\ \cmidrule{2-10}
&\textbf{OntoTune}$_{sft}$   & \Checkmark    &55.8	&\textbf{61.6}	&77.3	&\textbf{61.7}	&\underline{57.8}	&\textbf{64.8}        & \textbf{63.2} \\
\bottomrule
\end{tabular}}
\label{tab:qa_all}
\end{table*}

\section{Training Objective Analysis}
We use the LLM trained with OntoTune$_{sft}$ to generate response $y$ and reference response $y^o$ again to directly verify whether our training objective is achieved. Additionally, we generate $y^o$ twice with the seed model and measure their similarity as the objective. As shown in Figure \ref{fig:rq1}, we observe that under three similarity metrics, the LLM trained with OntoTune aligns well with the objective curve, showing significant improvement compared to the seed model before training. This directly indicates that the seed model fine-tuned with OntoTune generates responses that are more guided by the ontology.

\section{Medical Question Answering}\label{app:c}
\subsection{QA Prompt Template}
We present the evaluation prompts used for the QA dataset in Figures \ref{medqa}, \ref{medmcqa}, \ref{pubmedqa}, \ref{step}. The black text represents the fixed instruction templates, while the blue text indicates the specific questions and context from the samples. To ensure fair evaluation, we consistently use these prompts when evaluating performance of domain QA dataset on all baselines.

\subsection{Compared to Existing Domain LLM}
To ensure fair comparison, we mainly select 7B-8B LLMs as baselines, divided into the following categories: \textbf{1) General-purposed LLMs}: LLaMA2 7B \cite{DBLP:journals/corr/abs-2307-09288}, LLaMA3 8B \cite{dubey2024llama}, LLaMA3.1, Mistral-7B-Instruct-v0.2 \cite{DBLP:journals/corr/abs-2310-06825}, Qwen2 7B \cite{DBLP:journals/corr/abs-2407-10671} and GPT3.5-turbo . \textbf{2) Medical LLMs}: MedAlpaca \cite{DBLP:journals/corr/abs-2304-08247}, BioMistral \cite{DBLP:conf/acl/LabrakBMGRD24}, Hippocrates \cite{DBLP:journals/corr/abs-2404-16621}, Aloe \cite{DBLP:journals/corr/abs-2405-01886}, Med42-v2 \cite{DBLP:journals/corr/abs-2408-06142}, jsl-medllama-v18. They are all fine-tuned based on large-scale medical domain corpus. \textbf{3) TaxoLLaMA$^*$} \cite{DBLP:conf/acl/MoskvoretskiiNL24}: A direct ontology injection method mentioned above.

Our experimental results are shown in Table \ref{tab:qa_all}. We find that the performance of domain-specific models and their corresponding seed model is highly correlated. For example, medical models based on the LLaMA series, such as MedAlpaca, Hippocrate, and Aloe, show significant improvements with the iteration of the LLaMA model. Therefore, to evaluate the effectiveness of domain adaptation methods, we focus on the performance gains of a single seed model across different domain adaptation strategies. Among the LLaMA3 8B-based methods, our OntoTune achieves state-of-the-art performance, even surpassing the larger GPT3.5-turbo model. Compared to the seed model, existing medical LLMs show inconsistent improvements across different medical datasets, whereas OntoTune almost consistently enhances performance across all datasets, demonstrating good stability. Additionally, OntoTune only uses a small-scale ontology as source data, it exhibits broader generality and promising prospects.




%%\section{Generated instruction texts}
\section{Examples of Inconsistent Texts}\label{app:d}
Figure \ref{case1}, \ref{case2}, \ref{case3} present three types of examples of generated texts with and without ontology information. We can find that these examples exhibit noticeable inconsistencies. It is obvious that when dealing with long-tail medical concepts, the seed model struggles to provide effective responses without additional ontology information. However, when ontology information is incorporated, the model can generate richer and more logical responses by leveraging relevant hypernyms and synonyms.



\begin{figure*}
\centering
\includegraphics[scale=0.16]{case1.pdf}
\caption{An Example of inconsistent diverse corpus.}
\label{case1}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[scale=0.16]{case2.pdf}
\caption{An Example of inconsistent conceptual corpus.}
\label{case2}
\end{figure*}


\begin{figure*}
\centering
\includegraphics[scale=0.16]{case3.pdf}
\caption{An Example of inconsistent professional corpus.}
\label{case3}
\end{figure*}

% \newpage
% \begin{figure*}
% \centering
% \includegraphics[scale=0.53]{case3.pdf}
% \caption{An Example of consistent corpus.}
% \label{case3}
% \end{figure*}