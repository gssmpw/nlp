\section{Related Works}
%\subsection{Preference Alignment}

\begin{figure*}[ht]
\centering
\includegraphics[scale=0.67]{overview.pdf}
\vspace{-6mm}
\caption{Overview of OntoTune which aligns LLMs with ontology through in-context learning.}
\label{fig:method}
\end{figure*}

\subsection{Domain-specific LLMs}
%\subsection{Medical Large Language Models}
Existing domain-specific large language models (LLMs) can be categorized into two groups: (1) those models trained from scratch using domain-specific corpora, such as BioGPT ____ and GatorTron ____, and (2) those ____ that employ continual training on general-purposed models. Benefiting from its ability to leverage the extensive and diverse data of the seed models, as well as more efficient training processes, the latter approach has gradually become mainstream. Current domain-specific LLMs like BioMistral ____, BloombergGPT ____ and LawGPT ____ are developed by training a seed model with a large-scale raw domain-specific corpora, demonstrating impressive performance on domain tasks. To be specific, the medical model PMC-LLaMA ____ is fine-tuned with LoRA ____  on LLaMA using 4.8 million biomedical papers. LawGPT ____  continues training on 500k legal documents. And BloombergGPT ____ is fine-tuned on a 708 billion tokens financial corpora. These models typically rely on large amounts of training data to adapt to their respective domains. However, this fragmented knowledge from the raw corpora is merely injected into the seed model without being systematically organized and recent research ____ have indicated that directly using these fragmented raw corpora is not efficient. Additionally, prior researches seldom utilize ontologies as foundational knowledge sources for training corpora. Compared to fragmented large-scale corpora, concept-level structured knowledge in ontologies can play a significant role in knowledge management ____ and semantic search ____, and also have the potential to empower LLMs. Recently, TaxoLLaMA ____ develops a lexical semantic LLM via directly employing the WordNet ____ ontology for instruction-tuning, achieving state-of-the-art performance in multiple lexical semantic tasks and highlighting the potential of ontologies for developing domain-specific LLMs.


\subsection{Self-Generated Data for Training}
The self-training paradigm involves generating data autonomously and using this self-generated data for further training. Traditional self-training methods ____ typically employ a trained model to annotate data, and then improve model performance based on these newly annotated data. Due to its simplicity and efficiency, this training paradigm is also migrating to LLMs. Given the high costs of manually annotating training data or using more powerful proprietary models like GPT-4 ____, many works ____ have begun to leverage the language model itself to synthesize training data. STaR ____ is a self-taught reasoner that learns from its own generated reasoning steps to improve reasoning ability. Furthermore, SDFT ____ proposes a self-distillation fine-tuning method to achieve more efficient and less damaging results. Alternatively, Lin et al. ____ use gold answers to train a reward model for evaluating generated instructions separately. However, previous self-training approaches usually rely on gold labels to filter out low-quality instruction data, and they tend to focus more on improvements within a single dataset. Unlike previous methods, our OntoTune mitigates performance degradation caused by incorrect labels by refining and reorganizing internal domain knowledge of the seed model through open-ended instructions ____.