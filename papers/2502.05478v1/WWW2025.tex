%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the concepts
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
% \documentclass\[sigconf, anonymous, review\]{acmart}.

\documentclass[sigconf]{acmart}
\settopmatter{authorsperrow=4}

\copyrightyear{2025}
\acmYear{2025}
\setcopyright{acmlicensed}
\acmConference[WWW '25] {Proceedings of the ACM Web Conference 2025}{April 28-May 2, 2025}{Sydney, NSW, Australia.}
\acmBooktitle{Proceedings of the ACM Web Conference 2025 (WWW '25), April 28-May 2, 2025, Sydney, NSW, Australia}
\acmDOI{10.1145/3696410.3714816}
\acmISBN{979-8-4007-1274-6/25/04}

\settopmatter{printacmref=true}


% \usepackage{silence}
% \WarningFilter{latex}{empty address}
\usepackage{hyperref}
\usepackage{hyperxmp}
\usepackage{amsmath,amsfonts}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{csquotes}
\usepackage{arydshln}
% \usepackage[table]{xcolor}
\usepackage{pifont}% 
\usepackage{titlesec}
\usepackage{bbding}
\usepackage{graphicx}

\usepackage{CJKutf8}

\usepackage{balance}
%\usepackage{colortbl}
\definecolor{deepgreen}{rgb}{0.0, 0.4, 0.0}
\definecolor{lightblue}{rgb}{0.9, 0.96, 1}
\definecolor{deepred}{rgb}{0.7, 0.0, 0.0}
\newcommand{\model}{OntoTune}
% \newcommand{\stitle}[1]{\vspace{0.8mm} \noindent {\bf #1}}

% \titleformat{\section}  {\normalfont\Large\bfseries\MakeUppercase}{\thesection}{1em}{}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmlicensed}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation emai}{June 03--05,
%   2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
% \acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.

%\title{\model of Large Language Models Driven by Ontology: \\A Study in the Medical Domain}

\title{\model: Ontology-Driven Self-training for Aligning Large Language Models}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
% \author{Ben Trovato}
% \authornote{Both authors contributed equally to this research.}
% \email{trovato@corpustion.com}
% \orcid{1234-5678-9012}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
% \affiliation{%
%   \institution{Institute for Clarity in Documentation}
%   \city{Dublin}
%   \state{Ohio}
%   \country{USA}
% }

\author{Zhiqiang Liu}
\affiliation{%
  \institution{Zhejiang University}
  \city{Hangzhou}
  \country{China}}
\email{zhiqiangliu@zju.edu.cn}

\author{Chengtao Gan}
\affiliation{%
  \institution{Zhejiang University}
  \city{Hangzhou}
  \country{China}}
\email{22451144@zju.edu.cn}

\author{Junjie Wang}
\affiliation{%
  \institution{Zhejiang University}
  \city{Hangzhou}
  \country{China}}
\email{wangjj2018@zju.edu.cn}

\author{Yichi Zhang}
\affiliation{%
  \institution{Zhejiang University}
  \city{Hangzhou}
  \country{China}}
\email{zhangyichi2022@zju.edu.cn}

\author{Zhongpu Bo}
\affiliation{%
  \institution{Ant Group}
  \city{Hangzhou}
  \country{China}}
\email{bozhongpu.bzp@antgroup.com }

\author{Mengshu Sun}
\affiliation{%
  \institution{Ant Group}
  \city{Hangzhou}
  \country{China}}
\email{mengshu.sms@antgroup.com}

\author{Huajun Chen}
\affiliation{%
  \institution{Zhejiang University}
  \city{Hangzhou}
  \country{China}}
\email{huajunsir@zju.edu.cn}

\author{Wen Zhang}
\authornote{Corresponding author}
\affiliation{%
  \institution{Zhejiang University}
  \city{Hangzhou}
  \country{China}}
\email{zhang.wen@zju.edu.cn}


% \author{Valerie B\'eranger}
% \affiliation{%
%   \institution{Inria Paris-Rocquencourt}
%   \city{Rocquencourt}
%   \country{France}
% }

% \author{Aparna Patel}
% \affiliation{%
%  \institution{Rajiv Gandhi University}
%  \city{Doimukh}
%  \state{Arunachal Pradesh}
%  \country{India}}

% \author{Huifen Chan}
% \affiliation{%
%   \institution{Tsinghua University}
%   \city{Haidian Qu}
%   \state{Beijing Shi}
%   \country{China}}

% \author{Charles Palmer}
% \affiliation{%
%   \institution{Palmer Research Laboratories}
%   \city{San Antonio}
%   \state{Texas}
%   \country{USA}}
% \email{cpalmer@prl.com}

% \author{John Smith}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \city{Hekla}
%   \country{Iceland}}
% \email{jsmith@affiliation.org}

% \author{Julius P. Kumquat}
% \affiliation{%
%   \institution{The Kumquat Consortium}
%   \city{New York}
%   \country{USA}}
% \email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Zhiqiang Liu et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Existing domain-specific Large Language Models (LLMs) are typically developed by fine-tuning general-purposed LLMs with large-scale domain-specific corpora. However, training on large-scale corpora often fails to effectively organize domain knowledge of LLMs, leading to fragmented understanding. Inspired by how humans connect concepts and organize knowledge through mind maps, we aim to emulate this approach by using \textbf{ontology} with hierarchical conceptual knowledge to reorganize LLM's domain knowledge. From this perspective, we propose an ontology-driven self-training framework called \textbf{OntoTune}, which aims to align LLMs with ontology through in-context learning, enabling the generation of responses guided by the ontology. We leverage in-context learning to identify whether the LLM has acquired the specific concept's ontology knowledge, and select the entries not yet mastered by LLM as the training set to further align the LLM with ontology. Compared to existing domain LLMs based on newly collected large-scale domain-specific corpora, our OntoTune, which relies on the existing, long-term developed ontology and LLM itself, significantly reduces data maintenance costs and offers improved generalization ability. We conduct our study in the medical domain to evaluate the effectiveness of OntoTune, utilizing a standardized medical ontology, SNOMED CT as our ontology source. Experimental results demonstrate that OntoTune achieves state-of-the-art performance in both in-ontology task hypernym discovery and out-of-ontology task medical domain QA. Moreover, compared to the latest direct ontology injection method TaxoLLaMA, our OntoTune better preserves original knowledge of LLM. The code and data are available at \url{https://github.com/zjukg/OntoTune}.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010178.10010179</concept_id>
<concept_desc>Computing methodologies~Natural language processing</concept_desc>
<concept_significance>500</concept_significance>
</concept>
% <concept>
% <concept_id>10010147.10010178.10010187.10010188</concept_id>
% <concept_desc>Computing methodologies~Semantic networks</concept_desc>
% <concept_significance>300</concept_significance>
% </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[300]{Computing methodologies~Natural language processing}
%\ccsdesc[300]{Computing methodologies~Semantic networks}
 % <concept>
 %  <concept_id>00000000.00000000.00000000</concept_id>
 %  <concept_desc>Do Not Use This Code, Generate the Correct concepts for Your Paper</concept_desc>
 %  <concept_significance>300</concept_significance>
 % </concept>
 % <concept>
 %  <concept_id>00000000.00000000.00000000</concept_id>
 %  <concept_desc>Do Not Use This Code, Generate the Correct concepts for Your Paper</concept_desc>
 %  <concept_significance>100</concept_significance>
 % </concept>
 % <concept>
 %  <concept_id>00000000.00000000.00000000</concept_id>
 %  <concept_desc>Do Not Use This Code, Generate the Correct concepts for Your Paper</concept_desc>
 %  <concept_significance>100</concept_significance>
 % </concept>

% \ccsdesc[500]{Do Not Use This Code~Generate the Correct concepts for Your Paper}
% \ccsdesc[300]{Do Not Use This Code~Generate the Correct concepts for Your Paper}
% \ccsdesc{Do Not Use This Code~Generate the Correct concepts for Your Paper}
% \ccsdesc[100]{Do Not Use This Code~Generate the Correct concepts for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Large Language Model, Self-training, Align with Ontology}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle


\section{Introduction}
\begin{CJK}{UTF8}{gbsn}
% \begin{displayquote}
% \emph{``To learn without thinking is blindness, to think without learning is idleness."}
% \emph{
% \begin{flushright}
% ——The Analects of Confucius
% \end{flushright}}
% \end{displayquote} 
Large Language Models (LLMs), such as GPT-4 \cite{DBLP:journals/corr/abs-2303-08774} and LLaMA \cite{dubey2024llama}, have achieved remarkable success in the field of natural language processing \cite{DBLP:journals/corr/abs-2407-15017}, demonstrating advanced performance across various domains and tasks. 
%These models with their billions of parameters and extensive pre-training data exhibit human-like understanding and reasoning \cite{DBLP:conf/emnlp/HaoGMHWWH23} abilities. 
To further enhance the capabilities of LLMs in specific domain, such as medical, financial, and science, the research and industry community have begun to focus on developing domain-specific LLMs \cite{DBLP:conf/acl/LabrakBMGRD24,DBLP:conf/acl/BhatiaNCA24,DBLP:journals/ai/AlmeidaNEWA24}. 

Existing methods usually develop domain-specific LLMs by further training general-purposed LLMs on domain-specific corpora, such as BloombergGPT \cite{DBLP:journals/corr/abs-2303-17564}, BioMistral \cite{DBLP:conf/acl/LabrakBMGRD24} and LawGPT \cite{DBLP:journals/corr/abs-2406-04614}. Previous researches \cite{DBLP:conf/acl/RenCLLHZWC024,DBLP:journals/corr/abs-2405-05904} indicate that LLMs have already acquired most domain knowledge during the comprehensive pre-training phase, and need to reorganize and align knowledge with domain-specific requirements during the post-training phase. However, adapting LLMs to specific domains presents significant challenges \cite{DBLP:journals/corr/abs-2311-05112,DBLP:conf/acl/LiYBZLSLSYWLXBF24}. On the one hand, the scarcity of domain-specific corpora and constraints imposed by data privacy present significant hurdles in the continuous collection of high-quality domain corpora for continual pre-training or supervised fine-tuning, demanding substantial investment in time and resources. On the other hand, existing researches \cite{DBLP:conf/iclr/ChengHW24,dorfner2024biomedical} reveal that directly fine-tuning LLMs with fragmented raw domain corpora struggles to effectively organize domain knowledge and can even impair prompting capabilities of LLMs. \textbf{So can we find a more efficient alternative to reorganize domain knowledge in large language models without relying on large-scale domain-specific corpora?}

\begin{figure}
\centering
\includegraphics[scale=0.56]{intro.pdf}
%\caption{intro.}
\vspace{-7mm}
\caption{A simple example illustrates how hierarchical structure knowledge in the ontology guide responses.}
\vspace{-3mm}
\label{fig:intro}
\end{figure}

\textit{Inspired by how humans use mind maps 
which visually represent concepts and their relationships, to systematically organize and review knowledge, we aim to use domain-specific mind maps to reorganize LLM's domain knowledge.} Naturally, we associate these mind maps with widely established, rigorously constructed \textbf{ontologies} \cite{DBLP:conf/ijcai/XiaoCKLPRZ18}, which fully display the relationships and hierarchical structures between domain concepts as the ideal domain-specific mind maps. As shown in Figure \ref{fig:intro}, the ontology structure primarily consists of hypernym and synonym relationships between concepts, and have been widely applied in scenarios such as information retrieval \cite{DBLP:journals/corr/abs-2407-10671} and knowledge reasoning \cite{DBLP:conf/www/ZhangPWCZZBC19,DBLP:conf/acl/0002WHZS0WLFS023}. Common domain ontologies include SNOMED CT \cite{DBLP:journals/midm/SchulzK08} in the biomedical field, WordNet \cite{DBLP:conf/naacl/Miller94a} in the lexical field and GeoNames\footnote{https://www.geonames.org/} in the geographical field. Figure \ref{fig:intro} illustrates an example of a medical ontology guided response, where the LLM links concepts through the hierarchical structure knowledge in the ontology. Meanwhile, we suppose that compared to collecting new large-scale domain corpora, utilizing existing, long-term developed ontologies can reduce data maintenance costs and offer better generalization. From this perspective, we propose an \underline{onto}logy-driven self-training fine-\underline{tun}ing fram\underline{e}work \textbf{OntoTune}, which aims to align LLMs with domain ontology through in-context learning\footnote{https://openai.com/index/learning-to-reason-with-llms/} and generate responses guided by the ontology. OntoTune's workflow consists of three main steps: \textbf{(1) Instruction Text Generation.} We utilize three ontology-aware concept-level instructions which focus respectively on diversity, conceptuality, and professionalism to generate outputs. Then we incorporate the corresponding ontology knowledge to the input and let seed model rethink to obtain better outputs through in-context learning. \textbf{(2) Inconsistency Text Selection.} If there is significant inconsistency between the corpora obtained with and without ontology knowledge, we consider that the seed model has not effectively grasped this concept's ontology structure to guide its output and select entries that exhibit significant inconsistency as the training set. \textbf{(3) LLM Fine-tuning.} Based on the training set, we perform self-training on the seed model, resulting in aligned domain LLMs.

We conduct our study in the medical field, using the high-quality medical ontology SNOMED CT \cite{DBLP:journals/midm/SchulzK08} as the source ontology. To evaluate the effectiveness of  \model, we compare it not only with customized models for specific tasks but also with existing domain LLM trained on large-scale corpora and the direct ontology injection method TaxoLLaMA* \cite{DBLP:conf/acl/MoskvoretskiiNL24} implemented on the same LLM called seed model. Results show that we have achieved state-of-the-art performance in in-ontology task hypernym discovery and out-of-ontology task domain QA, demonstrating that OntoTune can effectively improve the performance of domain-specific tasks. Moreover, OntoTune significantly preserves the knowledge and safety of the seed model compared to existing domain-specific LLMs and TaxoLLaMA. Our contributions can be summarized as follows:
\begin{list}{\huge\textbullet}{\leftmargin=1.5em}
\item We highlight the limitations of developing domain LLMs based on large-scale domain corpora, and we are the first to utilize small-scale ontology to reorganize the domain knowledge of LLMs.
\item We propose a novel ontology-driven self-training method OntoTune, which aligns LLMs with ontologies through in-context learning, thereby guiding LLMs to generate responses under domain ontology knowledge.

\item Compared to exsiting domain LLM based on large-scale raw domain corpora and the direct injection method TaxoLLaMA, our OntoTune achieves state-of-the-art performance in the in-ontology task hypernym discovery and out-of-ontology task domain QA, and significantly preserves the knowledge capabilities and safety of the seed model.
\end{list}
\section{Related Works}
%\subsection{Preference Alignment}

\begin{figure*}[ht]
\centering
\includegraphics[scale=0.67]{overview.pdf}
\vspace{-6mm}
\caption{Overview of OntoTune which aligns LLMs with ontology through in-context learning.}
\label{fig:method}
\end{figure*}

\subsection{Domain-specific LLMs}
%\subsection{Medical Large Language Models}
Existing domain-specific large language models (LLMs) can be categorized into two groups: (1) those models trained from scratch using domain-specific corpora, such as BioGPT \cite{DBLP:journals/bib/LuoSXQZPL22} and GatorTron \cite{DBLP:journals/corr/abs-2203-03540}, and (2) those \cite{DBLP:conf/icde/00070HCGYBZYSWY23,DBLP:conf/acl/ZhangCFLL0C24,DBLP:conf/acl/LabrakBMGRD24, DBLP:journals/corr/abs-2303-17564} that employ continual training on general-purposed models. Benefiting from its ability to leverage the extensive and diverse data of the seed models, as well as more efficient training processes, the latter approach has gradually become mainstream. Current domain-specific LLMs like BioMistral \cite{DBLP:conf/acl/LabrakBMGRD24}, BloombergGPT \cite{DBLP:journals/corr/abs-2303-17564} and LawGPT \cite{DBLP:journals/corr/abs-2406-04614} are developed by training a seed model with a large-scale raw domain-specific corpora, demonstrating impressive performance on domain tasks. To be specific, the medical model PMC-LLaMA \cite{DBLP:journals/corr/abs-2304-14454} is fine-tuned with LoRA \cite{DBLP:conf/iclr/HuSWALWWC22}  on LLaMA using 4.8 million biomedical papers. LawGPT \cite{DBLP:journals/corr/abs-2406-04614}  continues training on 500k legal documents. And BloombergGPT \cite{DBLP:journals/corr/abs-2303-17564} is fine-tuned on a 708 billion tokens financial corpora. These models typically rely on large amounts of training data to adapt to their respective domains. However, this fragmented knowledge from the raw corpora is merely injected into the seed model without being systematically organized and recent research \cite{DBLP:conf/iclr/ChengHW24,dorfner2024biomedical} have indicated that directly using these fragmented raw corpora is not efficient. Additionally, prior researches seldom utilize ontologies as foundational knowledge sources for training corpora. Compared to fragmented large-scale corpora, concept-level structured knowledge in ontologies can play a significant role in knowledge management \cite{DBLP:journals/corr/abs-2407-10671} and semantic search \cite{DBLP:conf/www/ZhangPWCZZBC19,DBLP:conf/acl/0002WHZS0WLFS023}, and also have the potential to empower LLMs. Recently, TaxoLLaMA \cite{DBLP:conf/acl/MoskvoretskiiNL24} develops a lexical semantic LLM via directly employing the WordNet \cite{DBLP:conf/naacl/Miller94a} ontology for instruction-tuning, achieving state-of-the-art performance in multiple lexical semantic tasks and highlighting the potential of ontologies for developing domain-specific LLMs.


\subsection{Self-Generated Data for Training}
The self-training paradigm involves generating data autonomously and using this self-generated data for further training. Traditional self-training methods \cite{DBLP:conf/iclr/HeGSR20,DBLP:conf/cvpr/XieLHL20,DBLP:journals/corr/abs-2202-12040,DBLP:conf/emnlp/0001GHW00023} typically employ a trained model to annotate data, and then improve model performance based on these newly annotated data. Due to its simplicity and efficiency, this training paradigm is also migrating to LLMs. Given the high costs of manually annotating training data or using more powerful proprietary models like GPT-4 \cite{DBLP:journals/corr/abs-2303-08774}, many works \cite{DBLP:conf/icml/MengMHZA023,DBLP:conf/acl/YangPFWCZL24,DBLP:conf/acl/WangKMLSKH23,DBLP:journals/corr/abs-2402-06457,DBLP:journals/tmlr/SinghCAAPGLH0XP24} have begun to leverage the language model itself to synthesize training data. STaR \cite{DBLP:conf/nips/ZelikmanWMG22} is a self-taught reasoner that learns from its own generated reasoning steps to improve reasoning ability. Furthermore, SDFT \cite{DBLP:conf/acl/YangPFWCZL24} proposes a self-distillation fine-tuning method to achieve more efficient and less damaging results. Alternatively, Lin et al. \cite{DBLP:journals/corr/abs-2404-07965} use gold answers to train a reward model for evaluating generated instructions separately. However, previous self-training approaches usually rely on gold labels to filter out low-quality instruction data, and they tend to focus more on improvements within a single dataset. Unlike previous methods, our OntoTune mitigates performance degradation caused by incorrect labels by refining and reorganizing internal domain knowledge of the seed model through open-ended instructions \cite{DBLP:conf/iclr/0009CMZYSZ24,DBLP:conf/acl/TyenMCCM24}.

\section{Methodology}
In this section, we first set an objective to evaluate whether the seed model has mastered domain ontology knowledge and guide the model's responses. To achieve this objective, we introduce an \textbf{Onto}logy-driven self-training fine-\textbf{tun}ing fram\textbf{e}work \textbf{OntoTune}.

\subsection{Objective Defintion}
% Given an instruction $X$ that is closely related to ontology knowledge, we define the response $Y$ generated by LLM $f_\theta$ as: $Y=f_\theta(X)$, 
% where $\theta$ represents the parameters of seed model $f$. 
% We believe that if seed model has fully mastered ontology knowledge, % this structured knowledge
% these internal ontology knowledge
% can help LLM understand and organize knowledge, guiding it to generate more systematic and logical responses. To internalize ontology knowledge, we align seed model with ontology through in-context learning and establish the optimization objective to train the LLM $f_{\theta'}$, which is expected to obtain consistent responses, when given either ontology-aware instruction ($X$) or instruction with ontology ($X, O$), symbolically represented as:
% \begin{align}
%     Y &= Y^o \notag \\
%     f_\theta(X) &= f_{\theta'}(X,O)
% \end{align}
% where $\theta'$ denotes the parameters of optimized seed model, and $O$ denotes the corresponding ontology knowledge required for instruction $X$.

Given an instruction $x$  that is closely related to ontology knowledge $o$, we could get two kinds of responses:
\begin{align}
    y = f(x) \quad\text{and}\quad y^o  = f(x, o),
\end{align}
where $y$ is the response with instruction $x$ as input, and the $y^o$ is the response with both instruction $x$ and the ontology knowledge $o$ as input. 
We hypothesize that if the seed model $f$ has fully mastered and properly utilizes the ontology knowledge when generating response, then $y$ should equal to $y^o$. Otherwise, $y^o$ should be better than $y$, since LLMs have the in-context-learning capability, and the inclusion of $o$ could lead to more systematic and logical responses. However, from our experience, $y$ is not close to or similar to $y^o$ in a lot of cases, which can be found in Appendix \ref{app:d}. 

To internalize the ontology knowledge into to LLMs, we align seed model $f_{\theta}$, which has parameter $\theta$ , to ontology through instruction tuning, getting model $f_{\theta'}$ with updated parmeters $\theta'$. 
We establish the optimization objective to 
\begin{equation}
    f_{\theta'}(x) = f_{\theta}(x, o)
\end{equation}
As analyzed before, this objective approximately means $f_{\theta'}$ has mastered the ontology knowledge and could properly utilize the internal ontology knowledge when generating response. 

\subsection{\model}

To effectively internalize ontology knowledge,, we introduce the \textbf{OntoTune} framework as shown in Fig \ref{fig:method}. 
%, which leverages a small-scale, high-quality domain ontology to align large language models with specific domain.
The OntoTune workflow consists of three main steps: \textbf{(1) Instruction text generation.} We utilize three types of concept-level ontology-aware instructions that include (or exclude) ontology knowledge as input to the seed model. These instructions focus on diversity, conceptuality, and professionalism. \textbf{(2) Inconsistency text selection.} We select responses that exhibit significant inconsistency between those that include and those that exclude ontology knowledge as our training set. \textbf{(3) LLM Fine-tuning.} Based on training set, we perform self-training on the seed model.


Previous researches point \cite{DBLP:conf/acl/MoskvoretskiiNL24,DBLP:conf/coling/MoskvoretskiiPN24} that the definitions of concepts are crucial for ontology learning tasks. Considering that our framework aims to employ a self-training approach, rather than distilling knowledge from more advanced models like GPT-4 \cite{DBLP:journals/corr/abs-2303-08774}. Therefore, we use the seed model to complete the missing definitions in the ontology via a few-shot learning approach. We provide relevant domain concepts with their definitions as examples and the specific prompt template is shown in Figure \ref{fig:method}.

\subsubsection{\textbf{Instruction Text Generation.}}
To assess to what extent LLMs comprehend ontology knowledge across various dimensions, we design three distinct concept-level instruction templates. These templates evaluate whether the ontology knowledge embedded in the seed model can effectively guide the responses from the perspectives of diversity, conceptuality and professionalism:
\begin{list}{\huge\textbullet}{\leftmargin=1.5em}
\item\textbf{Diverse corpus $x_d$.} This template requires to generate knowledge cards related to specific concepts. The concept's knowledge card is a concise collection of information about a specific domain concept, typically including its definition, related concepts, usage examples, and other supplementary information. 
\item\textbf{Conceptual corpus $x_c$.} This template is directly related to ontology concepts. It requires to generate definitions for concepts and distinguish between related concepts. Ontology can directly guide the model in systematically organizing and describing various concepts and their relationships. 
\item\textbf{Professional corpus $x_p$.} This template requires to elucidate the current research status of the concept in existing academic journals. Ontology implicitly connects related concepts, allowing for a more comprehensive and coherent presentation of academic knowledge.
\end{list}
These corpus generation templates are shown in Figure \ref{fig:prompt}. For the concept $t$, we denote the concept-level instructions as $x\in\{x_d,x_c,x_p\}$, and the generation process is represented as:
\begin{equation}
    y_t=f_\theta(x,t)
\end{equation}
Aiming to align seed model with ontology through in-context learning, we integrate ontology information related to concepts into the input and obtain more systematic and semantically clear responses under the guidance of ontology as shown in Figure\ref{fig:method}. The ontology information includes the \underline{definitions} of concepts and the ontology structure of the concepts, i.e., their \underline{hypernyms} and \underline{synonyms}, which are retrieved from the source ontology. We represent the generation process with concept's ontology information as:
\begin{equation}
    y^o_t=f_\theta(x,t,o_t)
\end{equation}
where $o_t\in O$ is the ontology information about the concept $t$ retrieved from the source ontology $O$ or completed by seed model.



\subsubsection{\textbf{Inconsistency Text Selection.}}
For the concept $t$, if the responses $y_t$ and $y_t^o$ are consistent, it indicates that ontology knowledge related to concept $t$ embedded in the seed model can implicitly guide the response. Conversely, if there is an inconsistency as shown in the example in Figure \ref{fig:method}, the content in $y_t$ is broad but superficial and does not involve related concepts, whereas the content in $y^o_t$ is specific and connected to relevant ontology concepts. Therefore, we select the inconsistent responses as the training set for the seed model to align with ontology. To evaluate inconsistency, we calculate a hybrid similarity score based on three different metrics: embedding cosine similarity, ROUGE-L, and BLEU-4:
\begin{equation}
\begin{aligned}
    {\rm sim}(y_t,y^o_t)=&\frac{E^{\top}(y_t)E(y^o_t)}{\|E(y_t)\|\|E(y^o_t)\|}\\&+\text{ROUGE-L}(y_t,y^o_t)+\text{BLEU-4}(y_t,y^o_t)   
\end{aligned}
\end{equation}
where $E(\cdot)$ is a sentence encoding model that encodes the input sentence into a vector for semantic similarity evaluation, which is a fine-tuned model based on MiniLMv2 \cite{DBLP:conf/acl/WangBHDW21} implemented by sentence-transformers\footnote{https://github.com/UKPLab/sentence-transformers} during experiments. And $\text{ROUGE-L}(\cdot)$ and $\text{BLEU-4}(\cdot)$ compute word-level text similarity. We select the lowest $k$ entries based on ${\rm sim}(y_t,y^o_t)$ from each type of corpora to construct the training data. Specifically, we construct our train set under two injection methods: supervised fine-tuning (SFT) data denoted as $\mathcal{D}_{sft}=\{x_n,y_n^o\}_{n=1}^k$ and direct preference optimization (DPO) data denoted as $\mathcal{D}_{dpo}=\{x_n,y_n^o \succ y_n\}_{n=1}^k$.


\begin{figure}
\centering
\includegraphics[scale=0.541]{prompt.pdf}
\vspace{-6mm}
\caption{Ontology-aware corpus generation templates.}
\vspace{-2mm}
\label{fig:prompt}
\end{figure}

\subsubsection{\textbf{LLM Fine-tuning.}}
Based on the training set constructed above, we use three fine-tuning methods: supervised instruction fine-tuning (SFT), direct preference optimization (DPO), and supervised instruction fine-tuning combined with direct preference optimization (SFT+DPO). Through SFT, we hope the seed model can directly learn ontology-guided responses from $y_t^o$, thereby implicitly enhancing its internal ontology knowledge. We utilize the training data $\mathcal{D}_{sft}$ to fine-tune the LLM $f_\theta$ directly with the next-token prediction objective for response $y^o_t$:
\begin{equation}
    \mathop{\max}_{\theta} \mathbb{E}_{\left(x_t,y^o_t\right) \sim \mathcal{D}_{sft}}\left[\log P_{\theta}(y^o_t \mid x_t)\right]
\end{equation}
For DPO, we use this fine-tune approach enables the seed model to favor the responses guided by ontology, avoiding the original superficial ones. We utilize the training data $\mathcal{D}_{dpo}$ to optimize the LLM $f_\theta$ by treating $y^o_t$ as the preferred response and $y_t$ as the rejected response:
\begin{equation}\resizebox{0.92\hsize}{!}{$
    \mathop{\max}_{\theta} \mathbb{E}_{\left(x_t,y^{o}_t\succ y_t\right) \sim \mathcal{D}_{dpo}}\left[\log \sigma\left(\beta \log \frac{P_{\theta}\left(y^{o}_t \mid x_t\right)}{P_{\mathrm{ref}}\left(y^{o}_t \mid x_t\right)}-\beta \log \frac{P_{\theta}\left(y_t \mid x_t\right)}{P_{\mathrm{ref}}\left(y_t \mid x_t\right)}\right)\right]$}
\end{equation}
where ${ref}$ is the parameter of initial seed model and $\beta$ is a parameter controlling the deviation from reference policy $P_{ref}$. 
Lastly, following the paradigm of combining SFT and DPO to enhance the model's task adaptability and domain generalization capabilities in previous work \cite{dubey2024llama,DBLP:journals/corr/abs-2303-08774}, we also attempt to train our seed model in two stages using SFT and DPO fine-tuning methods, respectively.


\begin{figure}[t]
\centering
\includegraphics[scale=0.587]{taxollama.pdf}
\vspace{-6mm}
\caption{The templates of TaxoLLaMA*'s instruction-tuning and hypernym discovery task.}
\vspace{-2mm}
\label{fig:taxollama}
\end{figure}

\section{Experiment}


We conduct comprehensive experiments to demonstrate the effectiveness of OntoTune. These experiments are designed to answer the following research questions:
\begin{list}{\huge\textbullet}{\leftmargin=1.5em}
    \item \textbf{RQ1:} Can OntoTune’s implicit injection approach enable LLMs to effectively align with ontology knowledge?
    \item \textbf{RQ2:} Can OntoTune adapt LLMs to specific domains, improving the performance of domain-specific tasks?
    \item \textbf{RQ3:} How does OntoTune affect on the general performance of the seed model?

    % \item \textbf{RQ4:} Can OntoTune generalize to other models?
    % \item \textbf{RQ5:} How does OntoTune's self-training differ from distillation from stronger LLMs?
    % \item \textbf{RQ6:} Why is OntoTune more effective compared to direct injection method TaxoLla ma?
\end{list}

\subsection{Experimental Setup}
In this paper, we select the medical domain as example to evaluate the effectiveness of our method, since medical field receives widespread attention and has rich evaluation datasets and baselines \cite{DBLP:journals/corr/abs-2311-05112}.  
% we focus on the medical domain and conduct our experiments. 
Specifically, we adopt standardized SNOMED CT\footnote{https://www.snomed.org/} \cite{DBLP:journals/midm/SchulzK08} International Edition June version as our source ontology, which includes 367,978 med
ical concepts, of which only 8,275 have corresponding definitions, and 246,356 taxonomic relationships (i.e., `is-a'). In order to match the training scale of existing domain-specific LLMs \cite{DBLP:journals/corr/abs-2405-01886,DBLP:journals/corr/abs-2408-06142}, we select $k=100000$ inconsistent samples on each type of corpora for training.

We utilize the LLaMA-3-8B-Instruct \cite{dubey2024llama} model as our seed model due to its robustness and generalization across multiple medical tasks. We employ the Low Rank Adaptation \cite{DBLP:conf/iclr/HuSWALWWC22} (LoRA) technique to fine-tune the model based on the LLaMA-Factory \cite{zheng2024llamafactory} framework. During the OntoTune training phase, we apply LoRA to all linear layers with a rank of $r = 8$. All training is conducted on 8 NVIDIA H100 80G GPUs. For SFT stage, we use fp32 and a learning rate of 5e-5, training for 3 epochs with a cosine scheduler, a batch size per device initialized to 8 and gradient accumulation of 2. For DPO stage, we use fp32 and a learning rate of 5e-6, training for 3 epochs with a cosine scheduler and 4 batch size per device.


\begin{table}
\caption{Results of the hypernym discovery. * represent language models that have been adapted for hypernym discovery task. All scores are magnified by a factor of 100.}
\vspace{-2mm}
\centering
\resizebox{0.465\textwidth}{!}{
\begin{tabular}{lcccc}
\toprule
\textbf{Model}    & \multicolumn{1}{l}{\textbf{1A:English}} & \multicolumn{1}{l}{\textbf{1B:Italian}} & \multicolumn{1}{l}{\textbf{1C:Spanish}} & \multicolumn{1}{l}{\textbf{2A:Medical}} \\ \midrule
CRIM \cite{DBLP:conf/semeval/Bernier-Colborne18}             & 36.10                                   & -                                       & -                                       & 54.64                                   \\
Hybird \cite{DBLP:conf/acl/HeldH19}           & 34.07                                   & -                                       & -                                       & \underline{64.47}                                   \\
RMM  \cite{DBLP:conf/acl/BaiZKCM21}              & 39.07                                   & -                                       & -                                       & 54.89                                   \\
300-sparsans \cite{DBLP:conf/semeval/BerendMF18}     & -                                       & 25.14                                   & 37.56                                   & -                                       \\ 
T5$^*$ \cite{DBLP:conf/ijcnlp/NikishinaCDPB23}               & 45.22                                   & 24.04                                   & 27.50                                   & 44.73                                   \\ \midrule
LLaMA3 8B* \cite{dubey2024llama}     & \underline{51.64}                                      & 47.41                                   & 53.06                                  & 54.86                                       \\
Aloe* \cite{DBLP:journals/corr/abs-2405-01886} &45.26	&43.52	&51.03	&57.42 \\ 
Med42-v2* \cite{DBLP:journals/corr/abs-2408-06142} &44.84  &43.78	&50.30	&55.97\\
jsl-medllama*-v18   &44.79  &42.83 &48.79  &43.39\\
TaxoLLaMA* \cite{DBLP:conf/acl/MoskvoretskiiNL24}       & 48.42                                   & 39.91                                   & 46.73                                   & 58.65                                   \\ \midrule
\textbf{OntoTune}$_{sft}$ & \textbf{53.02}                          & \underline{47.67}                          & \textbf{53.83}                          & \textbf{65.53} \\
\textbf{OntoTune}$_{dpo}$ & 50.46                          &\textbf{49.21}                        & \underline{53.61}                          &62.52                        \\
\textbf{OntoTune}$_{sft+dpo}$ &51.03                         & 45.22                          & 52.94                          & 62.81                         \\ \bottomrule
\end{tabular}}
\label{tab:hd}
\vspace{-2mm}
\end{table}

\subsection{Hypernym Discovery (RQ1)}
To verify whether the seed model can effectively align with the ontology, we evaluate the model's ontology reasoning ability through the in-ontology task hypernym discovery.
\subsubsection{\textbf{Datasets and Metric.}}
We select 4 subsets from the SemEval-2018 Task 9 \cite{DBLP:conf/semeval/Camacho-Collados18} dataset: 1A (English), 1B (Italian), 1C (Spanish), and 2A (Medical). The samples in these datasets contain a hyponym and a list of hypernyms, and the prompt template we used for training and evaluation is shown in Figure~\ref{fig:taxollama}. The performance is evaluated using the Mean Reciprocal Rank (MRR) metric denoted as $\operatorname{MRR}=\frac{1}{N} \sum_{i=1}^{N} \frac{1}{\operatorname{rank}_{i}}$, where $N$ is the total number of queries, and $rank_i$ is the rank of the correct result in the $i$-th query.



\begin{table*}[ht]
\centering
\caption{Results of the medical domain QA in the zero-shot and supervised fine-tuning (on evaluation) setting. The best results are highlighted in bold, while the second best are underlined. The TaxoLLaMA* represents the variants of TaxoLLaMA \cite{DBLP:conf/acl/MoskvoretskiiNL24} implemented by us. \textcolor{deepgreen}{$\uparrow$} and \textcolor{deepred}{$\downarrow$} indicate the score improvement and decline compared to the seed model.}
\vspace{-2mm}
\resizebox{0.98\textwidth}{!}{
% \fontsize{6}{6}\selectfont
\begin{tabular}{llllllllll}
\toprule \textbf{Setting}  & \textbf{Model}   & \textbf{MedQA} & \textbf{MedMCQA} & \textbf{PubMedQA} & \textbf{USMLE-step1} & \textbf{USMLE-step2} & \textbf{USMLE-step3} & \textbf{Average}  \\ \midrule
 \multirow{5}{*}{zero-shot} &LLaMA3 8B \cite{dubey2024llama}     & 51.7           &\quad 51.7             &\quad 70.3              &\quad \underline{57.4}                 &\quad 52.3                 &\quad 58.2                 & 56.9          \\
% LLaMA2 13B \cite{DBLP:journals/corr/abs-2307-09288} &- & \XSolidBrush   & 28.0           & 38.4             & 50.6              & 26.6                 & 35.8                 & 35.2                 & 35.8          \\ 
&TaxoLLaMA*\cite{DBLP:conf/acl/MoskvoretskiiNL24}     & 50.5\textcolor{deepred}{\;\small$\downarrow1.2$}           &\quad 46.1\textcolor{deepred}{\;\small$\downarrow5.6$}             &\quad \textbf{73.4}\textcolor{deepgreen}{\;\small$\uparrow3.1$}                &\quad 42.6\textcolor{deepred}{\;\small$\downarrow14.8$}                 &\quad 39.4\textcolor{deepred}{\;\small$\downarrow12.9$}                 &\quad 47.5\textcolor{deepred}{\;\small$\downarrow10.7$}                 & 49.9\textcolor{deepred}{\;\small$\downarrow7.0$}         \\  \cmidrule{2-10}
&\textbf{OntoTune$_{sft}$}   & 51.5\textcolor{deepred}{\;\small$\downarrow0.2$}           &\quad \underline{56.7}\textcolor{deepgreen}{\;\small$\uparrow5.0$}              &\quad \underline{72.0}\textcolor{deepgreen}{\;\small$\uparrow1.7$}                &\quad \underline{57.4}\textcolor{black}{\;-}                  &\quad \textbf{54.1}\textcolor{deepgreen}{\;\small$\uparrow1.8$}                   &\quad \underline{60.7}\textcolor{deepgreen}{\;\small$\uparrow2.5$}                   &\textbf{58.7}\textcolor{deepgreen}{\;\small$\uparrow1.8$}           \\
&\textbf{OntoTune$_{dpo}$}   & \textbf{53.3}\textcolor{deepgreen}{\;\small$\uparrow1.6$}           &\quad \textbf{57.2}\textcolor{deepgreen}{\;\small$\uparrow5.5$}             &\quad 65.5\textcolor{deepred}{\;\small$\downarrow4.8$}              &\quad \textbf{58.5}\textcolor{deepgreen}{\;\small$\uparrow1.1$}                 &\quad 51.4\textcolor{deepred}{\;\small$\downarrow0.9$}                 &\quad 59.0\textcolor{deepgreen}{\;\small$\uparrow0.8$}                 & 57.4\textcolor{deepgreen}{\;\small$\uparrow0.5$}         \\
&\textbf{OntoTune$_{sft+dpo}$}   & \underline{51.9}\textcolor{deepgreen}{\;\small$\uparrow0.2$}           &\quad \underline{56.7}\textcolor{deepgreen}{\;\small$\uparrow5.0$}             &\quad 66.3\textcolor{deepred}{\;\small$\downarrow4.0$}              &\quad 53.2\textcolor{deepred}{\;\small$\downarrow4.2$}                 &\quad \textbf{54.1}\textcolor{deepgreen}{\;\small$\uparrow1.8$}                 &\quad \textbf{63.1}\textcolor{deepgreen}{\;\small$\uparrow4.9$}                 & \underline{57.6}\textcolor{deepgreen}{\;\small$\uparrow0.7$}         \\ \midrule

\multirow{8}{*}{SFT (on evaluation)} &LLaMA3* 8B \cite{dubey2024llama}   & 56.4           &\quad 53.9             &\quad 77.2              &\quad 56.4                 &\quad 56.0                 &\quad 61.5                 & 60.2         \\
&Aloe \cite{DBLP:journals/corr/abs-2405-01886}    &53.4\textcolor{deepred}{\;\small$\downarrow3.0$}	&\quad56.8\textcolor{deepgreen}{\;\small$\uparrow2.9$}	&\quad75.4\textcolor{deepred}{\;\small$\downarrow1.8$}	&\quad54.3\textcolor{deepred}{\;\small$\downarrow2.1$}	&\quad\textbf{61.5}\textcolor{deepgreen}{\;\small$\uparrow5.5$}	&\quad60.7\textcolor{deepred}{\;\small$\downarrow0.8$}	&60.4\textcolor{deepgreen}{\;\small$\uparrow0.2$}   \\ 
&Med42-v2 \cite{DBLP:journals/corr/abs-2408-06142}    &57.8\textcolor{deepgreen}{\;\small$\uparrow1.4$}	&\quad58.1\textcolor{deepgreen}{\;\small$\uparrow4.2$}	&\quad74.6\textcolor{deepred}{\;\small$\downarrow2.6$}	&\quad\textbf{60.6}\textcolor{deepgreen}{\;\small$\uparrow4.2$}	&\quad\underline{57.8}\textcolor{deepgreen}{\;\small$\uparrow1.8$}	&\quad61.5\textcolor{black}{\;-}	&61.7\textcolor{deepgreen}{\;\small$\uparrow1.5$}   \\ 
&jsl-medllama-v18    &\textbf{59.3}\textcolor{deepgreen}{\;\small$\uparrow2.9$}	&\quad57.3\textcolor{deepgreen}{\;\small$\uparrow3.4$}	&\quad71.0\textcolor{deepred}{\;\small$\downarrow6.2$}	&\quad44.7\textcolor{deepred}{\;\small$\downarrow11.7$}	&\quad\underline{57.8}\textcolor{deepgreen}{\;\small$\uparrow1.8$}	&\quad\underline{62.3}\textcolor{deepgreen}{\;\small$\uparrow0.8$}		&58.7\textcolor{deepred}{\;\small$\downarrow1.5$}   \\ 
&TaxoLLaMA* \cite{DBLP:conf/acl/MoskvoretskiiNL24}   & 55.9\textcolor{deepred}{\;\small$\downarrow0.6$}           &\quad 57.5\textcolor{deepgreen}{\;\small$\uparrow3.6$}             &\quad 77.6\textcolor{deepgreen}{\;\small$\uparrow0.4$}              &\quad 56.4\textcolor{black}{\;-}                 &\quad \underline{57.8}\textcolor{deepgreen}{\;\small$\uparrow1.8$}                 &\quad 59.0\textcolor{deepred}{\;\small$\downarrow2.5$}                 & 60.7\textcolor{deepgreen}{\;\small$\uparrow0.5$}          \\ \cmidrule{2-10}
&\textbf{OntoTune}$_{sft}$     & \underline{58.4}\textcolor{deepgreen}{\;\small$\uparrow2.0$}  &\quad 60.4\textcolor{deepgreen}{\;\small$\uparrow6.5$}    &\quad 78.6\textcolor{deepgreen}{\;\small$\uparrow1.4$}     &\quad \underline{57.4}\textcolor{deepgreen}{\;\small$\uparrow1.0$}                 &\quad \underline{57.8}\textcolor{deepgreen}{\;\small$\uparrow1.8$}                 &\quad \underline{62.3}\textcolor{deepgreen}{\;\small$\uparrow0.8$}        & \textbf{62.5}\textcolor{deepgreen}{\;\small$\uparrow2.3$} \\ 
&\textbf{OntoTune}$_{dpo}$    & 58.3\textcolor{deepgreen}{\;\small$\uparrow1.9$}           &\quad \textbf{60.7}\textcolor{deepgreen}{\;\small$\uparrow6.8$}             &\quad \textbf{79.4}\textcolor{deepgreen}{\;\small$\uparrow2.2$}              &\quad 55.3\textcolor{deepred}{\;\small$\downarrow1.1$}                 &\quad 54.1\textcolor{deepred}{\;\small$\downarrow1.9$}                 &\quad 61.5\textcolor{black}{\;-}                 & 61.6\textcolor{deepgreen}{\;\small$\uparrow1.4$}         \\
&\textbf{OntoTune}$_{sft+dpo}$   & 58.2\textcolor{deepgreen}{\;\small$\uparrow1.8$}           &\quad \underline{60.5}\textcolor{deepgreen}{\;\small$\uparrow6.6$}             &\quad \underline{78.9}\textcolor{deepgreen}{\;\small$\uparrow2.2$}              &\quad \underline{57.4}\textcolor{deepgreen}{\;\small$\uparrow1.0$}                 &\quad 54.1\textcolor{deepred}{\;\small$\downarrow1.9$}                 &\quad \textbf{63.9}\textcolor{deepgreen}{\;\small$\uparrow2.4$}                 & \underline{62.2}\textcolor{deepgreen}{\;\small$\uparrow2.0$}    \\ \bottomrule
\end{tabular}}
\label{tab:qa}
\end{table*}


\subsubsection{\textbf{Baselines.}}
Our baselines can be divided into two part: \textbf{(1) embedding-based method}: CRIM \cite{DBLP:conf/semeval/Bernier-Colborne18}, Hybrid \cite{DBLP:conf/acl/HeldH19}, RMM \cite{DBLP:conf/acl/BaiZKCM21}, 300-sparsans \cite{DBLP:conf/semeval/BerendMF18}; \textbf{(2) PLM-based method}: T5$^*$ \cite{DBLP:conf/ijcnlp/NikishinaCDPB23}; \textbf{(3) LLM-based method}: LLaMA3 8B$^*$, TaxoLLaMA$^*$ \cite{DBLP:conf/acl/MoskvoretskiiNL24}, Aloe* \cite{DBLP:journals/corr/abs-2405-01886}, Med42-v2* \cite{DBLP:journals/corr/abs-2408-06142} and jsl-medllama*-3-8b-v18\footnote{https://huggingface.co/johnsnowlabs/jsl-medllama-3-8b-v18}. The T5$^*$ represents the taxonomy-adapted T5 \cite{DBLP:journals/jmlr/RaffelSRLNMZLL20} model implemented by Nikishina et al. \cite{DBLP:conf/ijcnlp/NikishinaCDPB23}. All LLM-based baselines and our OntoTune are developed based on LLaMA3 8B-Instruct, and have all been adapted for hypernym discovery task implemented by us. Among them, \textbf{TaxoLLaMA$^*$} \cite{DBLP:conf/acl/MoskvoretskiiNL24} is a direct ontology injection method. We adopt the same pre-training method as vanilla TaxoLLaMA \cite{DBLP:conf/acl/MoskvoretskiiNL24} and implement it with medical ontology SNOMED CT. Our instruction-tuning template is derived from the vanilla TaxoLLaMA \cite{DBLP:conf/acl/MoskvoretskiiNL24} as shown in Figure~\ref{fig:taxollama}, and it utilizes 510910 medical ontology relationships under the same training hyperparameters as OntoTune$_{sft}$. \textbf{Aloe*, Med42-v2*} and \textbf{jsl-medllama*-3-8b-v18} are medical LLMs fine-tuned on large-scale medical corpora and general instructions.
\subsubsection{\textbf{Implementation.}}
Considering the lack of definition of concepts in existing data sets \cite{DBLP:conf/coling/MoskvoretskiiPN24}, we follow previous generative work \cite{DBLP:conf/acl/MoskvoretskiiNL24} using GPT3.5-turbo\footnote{https://platform.openai.com/docs/models/gpt-3-5-turbo} to generate definitions for the hyponym concepts in these datasets, which helps to remove ambiguity. Additionally, we perform instruction-tuning for all LLM-based methods on the training set with a batch size of 32 per device and other training hyperparameters identical to OntoTune$_{sft}$.


\subsubsection{\textbf{Results.}}
\textit{\textbf{Medical Domain Performance.}} As shown in Table \ref{tab:hd}, the OntoTune$_{sft}$ models achieve state-of-the-art performance on the medical subset dataset, outperforming the seed model LLaMA* by 19.45\%, TaxoLLaMA* by 11.73\%. Although TaxoLLaMA* uses the entire SNOMED CT ontology for training, it does not achieve significant improvement. Moreover, we obverse that Aloe* and Med42-v2* trained on large-scale medical corpora exhibit noticeable performance improvements. Experimental results indicate that compared to TaxoLLaMA*, OntoTune can integrate ontology knowledge to LLMs more efficiently.

\textbf{\textit{Multilinual Performance.}} We conduct hypernym discovery tasks in multilingual environments, as shown in Table \ref{tab:hd}. Due to LLaMA3's pre-training in a multilingual environment, LLaMA* demonstrates good generalization performance on the Italian and Spanish subset datasets. However, TaxoLLaMA* and three medical LLMs experience catastrophic forgetting, with a significant performance decline compared to the seed model, whereas our three variants of OntoTune almost preserves the original multilingual capability of seed model. Notably, although our training set does not involve Italian and Spanish data, OntoTune$_{sft}$ also achieves state-of-the-art performance in the multilingual environment, showing significant improvement over seed model. This indicates that our OntoTune can effectively align seed model with ontology knowledge and even can generalize to other taxonomic scenarios.

\subsection{Medical Question Answering (RQ2)}
To verify whether seed model after being aligned with domain ontology, can effectively generalize to other domain-specific tasks, we conduct an out-of-ontology task domain QA for evaluation. 
\subsubsection{\textbf{Datasets.}}
We utilize 6 medical QA datasets: MedMCQA \cite{DBLP:conf/chil/PalUS22}, MedQA \cite{DBLP:journals/corr/abs-2009-13081}, PubMedQA \cite{DBLP:conf/emnlp/JinDLCL19}, USMLE step1-3 datasets\cite{DBLP:journals/corr/abs-2304-08247} to comprehensively evaluate medical domain ability. Among them, MedMCQA, MedQA, and PubMedQA have training sets. More details about the datasets can be found in Appendix \ref{app:a}.

\subsubsection{\textbf{Baselines.}}
To ensure a fair comparison, we only compare baselines based on the LLaMA3 8B-Instruct \cite{dubey2024llama}: \textbf{(1) existing domain LLM} based on large-scale corpora: Aloe \cite{DBLP:journals/corr/abs-2405-01886}, Med42-v2 \cite{DBLP:journals/corr/abs-2408-06142} and jsl-medllama-3-8b-v18; \textbf{(2) the direct ontology injection method TaxoLLaMA*} \cite{DBLP:conf/acl/MoskvoretskiiNL24}. We report the results for both zero-shot and supervised fine-tuning on the training set of the evaluation dataset. More baseline performances can be found in Appendix \ref{app:c}.



\begin{table*}[ht]
\centering
\caption{Results of general capabilities evaluation. \textcolor{deepgreen}{$\uparrow$} and \textcolor{deepred}{$\downarrow$} indicate the score improvement and decline of our OntoTune compared to the direct injection method TaxoLLaMA*.}
%\vspace{-3mm}
\resizebox{0.96\textwidth}{!}{
\begin{tabular}{llllll|ll|l|ll}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{5}{c}{\textbf{MMLU}}                                                              & \multicolumn{2}{c}{\textbf{ARC}}  & \multicolumn{1}{c}{\textbf{TriviaQA}} & \multicolumn{2}{c}{\textbf{Advbench}}       \\ \cmidrule{2-11}
                                & \textbf{STEM} & \textbf{Social Sciences} & \textbf{Humanities} & \textbf{Other} & \textbf{Average} & \textbf{ARC\_C} & \textbf{ARC\_E} & \qquad\;{\textbf{-}}         & \textbf{Raw Safe} & \textbf{Jailbreak Safe} \\ \midrule
LLaMA3  8B \cite{dubey2024llama}                     & 56.83         & \quad76.61                    & \quad60.81               & 74.10          & 66.49        & 78.64           & 92.77           & \,64.81             & \,97.50             & \quad96.35                   \\
Aloe \cite{DBLP:journals/corr/abs-2405-01886}	&55.67	&\quad\textbf{76.24}	&\quad58.91	&72.25	&65.10	&75.25	&86.95 &\,63.03  &\,62.50	&\quad34.23\\
Med42-v2 \cite{DBLP:journals/corr/abs-2408-06142}	&\textbf{56.59}	&\quad\textbf{76.24}	 &\quad59.91	&72.67	&65.72	&\textbf{82.37}	 &\textbf{92.59}	&\,\textbf{65.19}	&\,83.85	&\quad60.19 \\
jsl-medllama-v18	&55.07	&\quad74.13	&\quad58.00	&71.96	&64.13	&\underline{80.34}	&91.53	&\,61.59	&\,90.58	&\quad68.27 \\
\multicolumn{1}{l}{TaxoLLaMA* \cite{DBLP:conf/acl/MoskvoretskiiNL24}}  & 55.96         & \quad73.74                    & \quad56.92               & 69.43          & 63.29        & 72.88           & 89.24           & \,63.12             & \,\textbf{94.04}             & \quad73.27                   \\ \midrule
\textbf{OntoTune}$_{sft}$                        & \underline{56.47}\textcolor{deepgreen}{\;\small$\uparrow0.51$}         & \quad75.73 \textcolor{deepgreen}{\;\small$\uparrow1.99$}                    & \quad\textbf{61.85} \textcolor{deepgreen}{\;\small$\uparrow4.93$}               & \underline{73.02}\textcolor{deepgreen}{\;\small$\uparrow3.59$}           & \textbf{66.31} \textcolor{deepgreen}{\;\small$\uparrow3.02$}        & 78.31 \textcolor{deepgreen}{\;\small$\uparrow5.43$}           & 91.89\textcolor{deepgreen}{\;\small$\uparrow2.65$}            & \,\underline{64.07}\textcolor{deepgreen}{\;\small$\uparrow0.95$}              & \,\textbf{94.04}\textcolor{black}{\;\small$-$}              & \quad\textbf{92.69}\textcolor{deepgreen}{\;\small$\uparrow19.42$}                   \\ 
\textbf{OntoTune}$_{dpo}$                        & 56.33\textcolor{deepgreen}{\;\small$\uparrow0.37$}         & \quad75.33 \textcolor{deepgreen}{\;\small$\uparrow1.59$}                    & \quad59.93 \textcolor{deepgreen}{\;\small$\uparrow3.01$}               & \textbf{73.64}\textcolor{deepgreen}{\;\small$\uparrow4.21$}           & 65.70 \textcolor{deepgreen}{\;\small$\uparrow2.41$}        & 78.98 \textcolor{deepgreen}{\;\small$\uparrow6.10$}           & \underline{92.06}\textcolor{deepgreen}{\;\small$\uparrow2.82$}            & \,63.96\textcolor{deepgreen}{\;\small$\uparrow0.84$}              & \,\underline{90.58}\textcolor{deepred}{\;\small$\downarrow3.46$}              & \quad77.88\textcolor{deepgreen}{\;\small$\uparrow4.61$}                   \\ 
\textbf{OntoTune}$_{sft+dpo}$                        & 55.67\textcolor{deepred}{\;\small$\downarrow0.29$}         & \quad75.17 \textcolor{deepgreen}{\;\small$\uparrow1.43$}                    & \quad\underline{61.79} \textcolor{deepgreen}{\;\small$\uparrow4.87$}               & 72.71\textcolor{deepgreen}{\;\small$\uparrow3.28$}           & \underline{65.93} \textcolor{deepgreen}{\;\small$\uparrow2.64$}        & 78.98 \textcolor{deepgreen}{\;\small$\uparrow6.10$}           & \underline{92.06}\textcolor{deepgreen}{\;\small$\uparrow2.82$}            & \,63.96\textcolor{deepgreen}{\;\small$\uparrow0.84$}                & \,\underline{90.58}\textcolor{deepred}{\;\small$\downarrow3.46$}              & \quad\underline{84.81}\textcolor{deepgreen}{\;\small$\uparrow11.54$}                   \\ \bottomrule
\end{tabular}}
\label{tab:gc}
%\vspace{-2mm}
\end{table*}


\subsubsection{\textbf{Implementation.}}
Following previous works \cite{DBLP:journals/corr/abs-2304-08247,DBLP:conf/acl/LabrakBMGRD24,DBLP:journals/corr/abs-2404-16621}, we perform instruction-tuning on the training set of the evaluation dataset for LLaMA3, TaxoLLaMA and OntoTune with the same training hyperparameters as OntoTune$_{sft}$.

\subsubsection{\textbf{Results.}}
From zero-shot results shown in Table \ref{tab:qa}, we can observe that the performance of TaxoLLaMA* significantly declines and the performance of OntoTune increases on most datasets. And when we conduct supervised fine-tuning on the instruction dataset, OntoTune$_{sft}$ performs better than seed model across all datasets and achieves state-of-the-art results among all LLMs based on LLaMA3 8B. Compared to our seed model, all three variants of our OntoTune, as well as the TaxoLLaMA* method, achieve significant improvements. This indicates that a small-scale, but high-quality ontology is beneficial for enhancing the capabilities of LLMs in specific domains. It's observed that although LLMs trained on large-scale raw corpora perform well on some datasets, their improvement over the seed model is not stable and the average score is inferior to our OntoTune, which suggests that large-scale corpora are challenging to learn from. To our surprise, although ontologies cannot directly provide the concrete knowledge related to these practical questions for the seed model, we attribute the performance improvement to the structured ontology knowledge, which helps LLMs reorganize domain knowledge. Furthermore, our three OntoTune models outperform the direct ontology injection method TaxoLLaMA*, demonstrating self-training is more effective for reorganizing domain knowledge and improving the performance of domain-specific tasks.


\begin{figure}[t]
\centering
\includegraphics[scale=0.415]{figure5.pdf}
\vspace{-3.5mm}
\caption{Performance with different epochs and training samples. The result of MedMCQA is under zero-shot setting.}
%\vspace{-1mm}
\label{fig:epoch}
\end{figure}


\subsection{General Capabilities Evaluation (RQ3)}
Futhermore, we evaluate whether the seed model exhibits catastrophic forgetting or impaired capabilities after OntoTune.
\subsubsection{\textbf{Knowledge Evaluation.}}
We conduct evaluation on the MMLU \cite{DBLP:conf/iclr/HendrycksBBZMSS21}, ARC \cite{DBLP:journals/corr/abs-1803-05457}, and TrivialQA \cite{DBLP:conf/acl/JoshiCWZ17} datasets. Specifically, MMLU is evaluated based on LLaMA-Factory \cite{zheng2024llamafactory}, while ARC and TrivialQA are evaluated on OpenCompass \cite{2023opencompass} tool with gen mode.

From the results in Table \ref{tab:gc}, we observe that Med42-v2 even surpasses the seed model on several datasets. This is because Med42-v2 incorporates 344k general instructions during the domain adaptation phase, with 74k CoT instructions effectively enhancing reasoning performance on the ARC dataset. In contrast, other domain LLMs that also incorporate general instructions experience a noticeable decline in general performance compared to our OntoTune, which does not use general instructions. Additionally, due to the fixed input-output format and lack of data diversity \cite{DBLP:conf/nips/ZhouLX0SMMEYYZG23}, TaxoLLaMA* suffers the most significant performance decline. Compared to TaxoLLaMA*, our OntoTune method does not exhibit significant catastrophic forgetting. Similarly, OntoTune$_{sft}$ demonstrates the best performance among three variants, showing an average decrease of only 0.49\% compared to the seed model.




\subsubsection{\textbf{Safety Evaluation.}}
Following previous work \cite{DBLP:conf/iclr/Qi0XC0M024,DBLP:conf/acl/YangPFWCZL24} on safety evaluation, we select harmful instructions from the Advbench dataset \cite{DBLP:journals/corr/abs-2307-15043} as model inputs and denote the proportion of safe responses as ``Raw Safe". Then we append adversarial suffixes to the harmful instructions and denote the proportion of safe responses at present as ``Jailbreak Safe" to measure model's safety.

\begin{figure}[t]
\centering
\includegraphics[scale=0.323]{figure6.pdf}
\vspace{-4mm}
\caption{Domain performances and general performances on the seed model Qwen2 7B.}
\vspace{-1mm}
\label{fig:qwen}

\end{figure}

From results in Table \ref{tab:gc}, we observe that the fine-tuned models show a significant decline in both Raw Safe and Jailbreak Safe metrics. Despite undergoing safety alignment, the three medical models based on large-scale corpora still exhibit catastrophic security vulnerabilities. For four ontology-based fine-tuning approach, TaxoLLaMA* and OntoTune both show a slight decline in the Raw Safe metric. Under jailbreak settings, TaxoLLaMA* experiences a significant 23.08\% decline in the Jailbreak Safe metric, while OntoTune effectively mitigates this issue. OntoTune demonstrates state-of-the-art performance, not only achieving efficient domain alignment but also preserving safety alignment.  





% \begin{table*}[ht]
% \begin{tabular}{p{2cm}lccccccc}
% \toprule  \textbf{Setting}  & \textbf{Model}  & \textbf{MedQA} & \textbf{MedMCQA} & \textbf{PubMedQA} & \textbf{USMLE-step1} & \textbf{USMLE-step2} & \textbf{USMLE-step3} & \textbf{Avg} \\ \midrule
% \multirow{5}{2cm}{\raggedright Unsupervised FT}                                                                           & LLaMA3 8B      & 51.7           & 51.7             & 70.3              & 57.4                 & 52.3                 & 58.2                 & 56.9         \\
% & TaxoLLaMA*      & 50.5           & 46.1             & \underline{73.4}              & 42.6                 & 39.4                 & 47.5                 & 49.9         \\
% & OntoTune$_{sft}$     & 51.5           & 56.7             & 72.0              & 57.4                 & 54.1                 & 60.7                 & 58.7         \\
% & OntoTune$_{dpo}$    & 53.3           & 57.2             & 65.5              & 58.5                 & 51.4                 & 59.0                 & 57.4         \\
% & OntoTune$_{sft+dpo}$ & 51.9           & 56.7             & 66.3              & 53.2                 & 54.1                 & 63.1                 & 57.6         \\ \midrule
% \multirow{5}{2cm}{\raggedright Supervised FT (eval data)} & LLaMA3 8B      & 56.4           & 53.9             & 77.2              & 56.4                 & 56.0                 & 61.5                 & 60.2         \\
% & TaxoLLaMA*      & 55.9           & 57.5             & 77.6              & 56.4                 & 57.8                 & 59.0                 & 60.7         \\
% & OntoTune$_{sft}$     & 58.4           & 60.4             & 78.6              & \underline{57.4}                 & 57.8                 & 62.3                 & 62.5         \\
% & OntoTune$_{dpo}$     & 58.3           & 60.7             & 79.4              & 55.3                 & 54.1                 & 61.5                 & 61.6         \\
% & OntoTune$_{sft+dpo}$ & 58.2           & 60.5             & 78.9              & \underline{57.4}                 & 54.1                 & \textbf{63.9}                 & \underline{62.2}         \\ \bottomrule
% \end{tabular}
% \end{table*}
\subsection{\textbf{Model Analysis}}

\subsubsection{\textbf{Effects of Training Parameters.}}

In Figure \ref{fig:epoch}, we explore the performance of our OntoTune across different training epochs and different numbers of samples. Specifically, we use TriviaQA to evaluate general performance and MedMCQA to evaluate domain-specific performance. We find that with 300,000 training samples, just 1 epoch leads to significant performance improvement. Additionally, at 3 training epochs, there is a noticeable improvement with only 9,000 samples, and the seed model trained on 75,000 samples achieves best performance. As the amount of training and data volume increase, OntoTune gradually converges. This implies that compared to existing domain LLMs, we can achieve more robust results using fewer training samples through OntoTune. 


\subsubsection{\textbf{Robustness to Seed Models.}}
We use Qwen2 7B \cite{DBLP:journals/corr/abs-2407-10671} as the seed model and report the performance of TaxoLLaMA* and the best variant, OntoTune$_{sft}$ to demonstrate that OntoTune is not

\noindent constrained by model architecture. As shown in Figure \ref{fig:qwen}, OntoTune$_{sft}$ achieves improvements over the base model across all medical QA datasets. Notably, OntoTune$_{sft}$ even achieves improvements on most of the general datasets, and significantly enhances reasoning performance on ARC. This improvement may be due to the enhancement of planning abilities when trained with structured data \cite{DBLP:journals/corr/abs-2406-14282}. Conversely, although TaxoLLaMA* shows improvement in medical QA, it experiences a significant decline in general performance. These results suggest that aligning with ontology benefits domain-specific capabilities, demonstrating OntoTune's robustness.
% \begin{table*}[ht]
% \caption{Results of domain capabilities for the three variants of OntoTune$_{sft}$. The reference outputs $y^o$ in their training sets are from self-generated by LLaMA3 8B, and distilled by deepseek-chat and LLaMA3.1 8B. 
% %The best results are highlighted in bold, while the second best are underlined. 
% }
% \vspace{-2mm}
% \centering
% \resizebox{0.75\textwidth}{!}{
% \begin{tabular}{llccccccc}
% \toprule
% \textbf{Model}     & $y^o$ \textbf{from}       & \textbf{MedQA} & \textbf{MedMCQA} & \textbf{PubMedQA} & \textbf{USMLE-step1} & \textbf{USMLE-step2} & \textbf{USMLE-step3} & \textbf{Average}  \\ \midrule
% \textbf{LLaMA3 8B}  & -             & 56.4  & 53.9    & 77.2     & \underline{56.4}        & \underline{56.0}        & 61.5        & 60.2 \\
% \textbf{OntoTune}$_{sft}$  & LLaMA3 8B     & \textbf{58.4}  & \underline{60.4}    & \textbf{78.6}     & \textbf{57.4}        & \textbf{57.8}        & \textbf{62.3}        & \textbf{62.5} \\
% \textbf{OntoTune}$_{sft}$  & LLaMA3.1 8B   & \textbf{58.4}  & \textbf{61.1}    & \underline{77.7}     & 55.3        & 53.2        & 59.8        & 60.9 \\
% \textbf{OntoTune}$_{sft}$  & deepseek-chat & 57.8  & 60.2    & 77.2     & 55.3        & 54.1        & \textbf{62.3}        & \underline{61.2} \\ \bottomrule
% \end{tabular}}
% \label{tab:distill_qa}
% \vspace{-2mm}
% \end{table*}
\begin{table}
\caption{Results of domain capabilities for the three variants of OntoTune$_{sft}$ on LLaMA3 8B. The reference outputs $y^o$ in their training sets are from self-generated by LLaMA3 8B, and distilled by LLaMA3.1 8B and deepseek-chat. 
%The best results are highlighted in bold, while the second best are underlined. 
}
\vspace{-2mm}
\centering
\resizebox{0.46\textwidth}{!}{
\begin{tabular}{cccccc}
\toprule
$y^o$ \textbf{from}       & \textbf{MedQA} & \textbf{MedMCQA} & \textbf{PubMedQA} & \textbf{USMLE-Avg} & \textbf{Average}  \\ \midrule
 -             & 56.4  & 53.9    & 77.2     & \underline{58.0}        & 60.2 \\
LLaMA3 8B     & \textbf{58.4}  & \underline{60.4}    & \textbf{78.6}     & \textbf{59.2}        & \textbf{62.5} \\
 LLaMA3.1 8B   & \textbf{58.4}  & \textbf{61.1}    & \underline{77.7}     & 56.1       & 60.9 \\
deepseek-chat & 57.8  & 60.2    & 77.2     & 57.2        & \underline{61.2} \\ \bottomrule
\end{tabular}}
\label{tab:distill_qa}
\vspace{-1mm}
\end{table}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \begin{table*}[ht]
% \centering
% \caption{General performances for the three variants of OntoTune$_{sft}$. The scores in \colorbox{lightblue}{\small{box}} indicate surpassing the seed model.}
% \vspace{-2mm}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{llccccc|cc|c|cc|c}
% \toprule
% \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{$y^o$ \textbf{from}} & \multicolumn{5}{c}{\textbf{MMLU}}                                                              & \multicolumn{2}{c}{\textbf{ARC}}  & \multicolumn{1}{c}{\textbf{TriviaQA}} & \multicolumn{2}{c}{\textbf{Advbench}}   & \multicolumn{1}{c}{\textbf{Average}}    \\ \cmidrule{3-13}
%                                 &                                   & \textbf{STEM} & \textbf{Social Sciences} & \textbf{Humanities} & \textbf{Other} & \textbf{Avg} & \textbf{ARC\_C} & \textbf{ARC\_E} & \textbf{-}         & \textbf{Raw Safe} & \textbf{Jailbreak Safe} & \textbf{-} \\ \midrule
% \textbf{LLaMA3 8B}                       & -                                 & 56.83         & 76.61                    & 60.81               & 74.10          & 66.49        & 78.64           & 92.77           & 64.81             & 97.50             & 96.35      &76.49             \\ 
% \textbf{OntoTune}$_{sft}$                         & LLaMA3 8B                         & \textbf{56.47}         & \underline{75.73}                    & \colorbox{lightblue}{\textbf{61.85}}               & \underline{73.02}          & \underline{66.31}        & 78.31           & \underline{91.89}           & \underline{64.07}             & \underline{94.04}             & \underline{92.69}          &\underline{75.44}         \\
% \textbf{OntoTune}$_{sft}$                         & LLaMA3.1 8B                       & \underline{56.17}         & \textbf{75.86}                    & \colorbox{lightblue}{\underline{61.55}}               & \textbf{73.64}          & \textbf{66.32}        & \colorbox{lightblue}{\underline{80.34}}           & \colorbox{lightblue}{\textbf{93.12}}           & \textbf{64.57}             & \textbf{96.54}             & \colorbox{lightblue}{\textbf{99.04}}      &\colorbox{lightblue}{\textbf{76.72}}             \\
% \textbf{OntoTune}$_{sft}$                         & deepseek-chat                     & 54.17         & 74.32                    & 59.40               & 72.09          & 64.48        & \colorbox{lightblue}{\textbf{82.37}}           & \underline{91.89}           & 62.75             & 85.38             & 50.19  &69.70\\ \bottomrule
% \end{tabular}}
% \label{tab:distill_general}
% \end{table*}



\begin{figure}
\centering
\includegraphics[scale=0.37]{figure7.pdf}
\vspace{-1mm}
\caption{General performances for the three variants of OntoTune$_{sft}$ on LLaMA3 8B.}
\vspace{-2mm}
\label{fig:self-training}
\end{figure}


\subsubsection{\textbf{Self-training Analysis.}}
%这一块蒸馏一下llama3.1 8B、deepseek作为训练语料只训练sft模型，比较self-training与distillation设置下的差距。画两个雷达图（一个是通用性能，一个是领域能力）。
% As LLMs continue to evolve, obtaining high-quality fine-tuning data remains a significant challenge. We consider self-training a feasible path for iterative develop LLM itself. 
Aiming to explore the impact of data quality on model's performance, we distill two stronger LLMs: LLaMA 3.1 8B and deepseek-v2.5-chat\footnote{https://chat.deepseek.com/}, using $x^o_t=\{(x,t,o_t)\,|\,y^o_t=f_\theta(x,t,o_t),y^o_t\in\mathcal{D}_{sft}\}$ as input to generate the higher quality target output $y^{o'}$. We then train the same seed model on $\mathcal{D}_{sft}'=\{x_n,y_n^{o'}\}_{n=1}^k$ under the same hyperparameters settings. Table \ref{tab:distill_qa} presents the results of three LLMs compared to the seed model in domain QA. On most datasets, the performances of all three variants of OntoTune can be improved. Among them, the self-training OntoTune$_{sft}$ model demonstrates robust and advanced performance, achieving improvements across all datasets. From results in Figure \ref{fig:self-training}, We observe that the OntoTune$_{sft}$ distilled from the same series LLaMA 3.1, exhibits the least decline on the knowledge QA dataset like MMLU and TriviaQA. Interestingly, although the focus is only on medical domain knowledge during the data distillation of LLaMA 3.1, the model shows improved performance on the reasoning challenge dataset ARC and safety evaluation Advbench. Additionally, the model distilled from deepseek shows a noticeable decline in knowledge and safety evaluation but a significant enhancement in reasoning ability. Overall, self-training achieves the most efficient domain alignment without requiring

\noindent advanced LLMs, while greatly preserving original knowledge.


\subsubsection{\textbf{Distribution Shift Analysis.}}
In the preceding sections, we identify OntoTune$_{sft}$ as the variant with best performance, excelling not only in downstream tasks but also effectively preserving the knowledge and safety of the seed model. We attribute this phenomenon to distribution shift. We utilize the mean squared change in parameters (denoted as $|\Delta\theta|^2$) to measure parameter shift during training and evaluate the data distribution shift based on the similarity of the model's responses. Specifically, we collect 1,000 general instructions from the Alpaca evaluation set \cite{alpaca_eval} and use the seed model's responses to these instructions as reference responses. We calculate the cosine similarity between the fine-tuned model's responses and the reference responses. 

From results shown in Figure \ref{fig:shift}, it can be observed that OntoTune$_{sft}$ exhibits the largest parameter shift, but it exhibits the least data distribution shift. Compared to distilling a larger LLM, the parameter and data distribution shifts in the self-training setting are smaller. Additionally, distilling from the same series LLM results in less distribution shift, which we infer is due to the similar pre-training data. Therefore, we can obtain the conclusion consistent with previous research \cite{DBLP:conf/acl/YangPFWCZL24}: self-training can effectively bridge distribution gap and thereby mitigate catastrophic forgetting.

\begin{figure}[t]
\centering
%\includegraphics[scale=0.45]{shift.pdf}
\includegraphics[scale=0.38]{figure8.pdf}
\vspace{-7mm}
\caption{(a) Comparison of OntoTune variants and TaxoLLaMA*. (b) Comparison of data distillation and self-training.}
\vspace{-2mm}
\label{fig:shift}
\end{figure}

\section{Conclusion}
In this paper, we propose an ontology-driven self-training fine-tuning framework \textbf{OntoTune},
%, which aims to align LLMs with ontology through in-context learning and generate responses guided by the ontology. 
which leverages in-context learning to identify the specific concept’s ontology knowledge the seed model has not acquired, and perform self-training to enhance the seed model's alignment with the ontology. Experiments demonstrate that OntoTune achieves state-of-the-art performance in both in-ontology task hypernym discovery and out-of-ontology task medical domain QA, while significantly preserving the knowledge of the seed model. Compared to existing domain LLMs trained on large-scale high-quality corpora, OntoTune relies on a relatively small-scale, long-term developed ontology along with the seed model itself, offering improved generalization ability. 
In the future, we will explore automated alignment methods that are less dependent on specific instruction templates. And we hope OntoTune could inspire more researches into exploring more efficient domain adaptation methods using small-scale data when facing the rapid iteration of LLMs and the scarcity of domain-specific data.  
%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
This work is founded by National Natural Science Foundation of China (NSFC62306276/NSFCU23B2055/NSFCU19B2027), Zhejiang Provincial Natural Science Foundation of China (No. LQ23F020017), Yongjiang Talent Introduction Programme (2022A-238-G), and Fundamental Research Funds for the Central Universities (226-2023-00138). This work was supported by AntGroup.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.

\bibliographystyle{ACM-Reference-Format}
\balance
\bibliography{ref}


%%
%% If your work has an appendix, this is the place to put it.

\appendix
\section*{Appendix}
\input{appendix}

\end{CJK}
\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
