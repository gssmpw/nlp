

\section{Additional Assumptions and Notations}
\label{sec:addass}

\subsection{Mathematical Setting}
\label{ssec:math-sett}

We give here the mathematical setting for the theoretical results of \autoref{sec:theo_res} and \autoref{sec:algo} to hold. Specifically, we use the mathematical setting of \cite{aubinais2023fundamental}. We redefine an MIA as follows.

\begin{definition}[MIA]
Any measurable map $\phi : \Theta\times\gZ\to\{0,1\}$ is considered to be a \textit{Membership Inference Attack}.
\end{definition}

Here we have encoded \textit{member} by $1$. We define a test point $\Tilde{\rz}$ as follows. Let $T\in\{0,1\}$ be a Bernoulli random variable with parameter $1/2$. Let $\rz_0\sim P$ and $U$ be a random variable whose distribution is $\hat{P}_n$ conditionally to the training dataset $\gD_n$, where $T, \rz_0$ and $U$ are independent. Additionally, $\rz_0$ and $T$ are independent of $\gD_n$. We define the test point by,

\begin{equation}
\label{eq:test_pt}
\Tilde{\rz}\coloneqq TU + (1-T)\rz_0.
\end{equation}

This definition is a formalization of the fact that a test point is either a member of the training dataset ($T=1$) or not a member ($T=0$). The random variable $\rz_0$ is used as a placeholder to represent the non-member property of the test point.\\
With this framework, the following relation holds,

\begin{theorem}[\cite{aubinais2023fundamental}]
For any distribution $P$ and any algorithm $\gA$, we have

\begin{equation}
\label{eq:mis_delta}
{\MIS}_n(P, \gA) = 1 - \|P_{(\hat{\theta}_n,\rz_1)} - P_{\hat{\theta}_n}\otimes P\|_{\textrm{TV}},
\end{equation}


where $\|\nu_1-\nu_2\|_{TV}$ is the total variation distance between the distributions $\nu_1$ and $\nu_2$.

\end{theorem}



\subsection{Fixed Quantizer}
\label{ssec:fq-add-ass}
We denote by $ L_{k,j} \coloneqq \ell(\thetaq_k,\rz_j) - \ell(\thetaq_1,\rz_j)$ the random loss gap between $\thetaq_k$ and $\thetaq_1$ evaluated on $\rz_j$, whose expectation is $\delta_k\coloneqq \E\cc{L_{k,j}}$. We additionally denote by $D \coloneqq \textrm{diag}\pp{\pp{\delta_k}_{k>1}}$ the diagonal \textbf{loss gaps matrix}. We give the additional hypotheses for \autoref{thm:fixed_pos}.

\begin{itemize}
    \item \textbf{H1.1.} We have $\E\cc{e^{<t, D^{-1}\cc{L_{k,1}-\delta_k}_{k>1}>}} < \infty$ for all $t$ in a neighborhood of $0\in\R^{K-1}$.
    \item \textbf{H1.2.} The minimal loss gap satisfies $\delta_2>0$.
\end{itemize}

A sufficient (albeit non-necessary) condition for H1.1 to hold is that the loss function $l$ is bounded. Specifically, the hypothesis H1.1 is sufficient for the constant of \autoref{thm:fixed_pos} to be strictly positive. If for all $t\not=0$ we have $\E\cc{e^{<t, D^{-1}\cc{L_{k,1}-\delta_k}_{k>1}>}} = \infty$, then the theorem becomes trivial as we would have $C_P^1=0$. The hypothesis H1.2 implies that the loss gaps matrix $D$ is invertible.

\subsection{Size-Adaptive Quantizers}
\label{ssec:saq-add-ass}


We give here additional notations and hypotheses for \autoref{thm:seqQ}. Let $\Thetaq_n\coloneqq \{\thetaq_1^n,\cdots,\thetaq_{K_n}^n\}$ be the image of the Size-Adaptive Quantizer $\gQ_n$, where $K_n$ may or may not depend on $n$. Based on the assumptions given in \autoref{thm:seqQ}, assume without loss of generality that for all $n\in\mathbb{N},$ $m_{\thetaq_{1}^n}<\cdots<m_{\thetaq_{K_n}^n}$. We recall that the \textbf{loss gaps} are defined as $\delta_k^n = m_{\thetaq_k^n} - m_{\thetaq_1^n}$ and the \textbf{loss variabilities} as $\pp{\sigma_k^n}^2\coloneqq \textrm{Var}\pp{L_{k,1}^n}$ where $L_{k,j}^n\coloneqq \ell(\thetaq_k^n,\rz_j) - \ell(\thetaq_1^n,\rz_j)$. We set $D^n\coloneqq \textrm{diag}\pp{\pp{\delta_k^n}_{k>1}}$. We will use the following hypotheses

\begin{itemize}
    \item \textbf{H2.1.} The limits $\sigma_k^2 = \liminf{n}\textrm{Var}(L_{k,1}^n)$ and
    $c_k = \liminf{n} \frac{\delta_2^n}{\delta_k^n}$ exist.
    \item \textbf{H2.2.} We have $\underset{n}{\sup}~\E\cc{\exp\pp{t\left\|\pp{\frac{\delta_2^n}{\delta_k^n}L_{k,1}^n}_{1<k\leq K_n}\right\|_2}}<\infty$, for all $t>0$.
    \item \textbf{H2.3.} There exists $K<\infty$ such that for all $k>K$, $c_k^2\sigma_k^2=0$.
    \item \textbf{H2.4.} The sequence of random variables $\pp{\pp{\frac{\delta_2^n}{\delta_k^n}L_{k,1}^n}_{1<k\leq K_n}}_n$ is tight when canonically seen as a sequence in $l_2(\R)$.
\end{itemize}

By definition, for all $k$, we have $\mkn{2}/\mkn{k}\in[0,1]$. If the loss function $\ell$ is bounded, then the loss variabilities $\pp{\sigma_k^n}^2$ are bounded as well. The hypothesis H2.1 is then only a light hypothesis.

%In hypothesis H2.2, the sequence $(\pp{\mkn{2}/\mkn{k}}L_{k,1}^n)_{1<k\leq K_n}$ can be seen as canonically mapped into $l_2(\R)$, the set of square-summable sequences. This means that at fixed $n$, for all $k>K_n$, the values of $\pp{\mkn{2}/\mkn{k}}L_{k,1}^n$ are simply set to $0$. 
% Importantly, hypothesis H2.2 is sufficient for this sequence of random variables to be tight.



A sufficient condition for H2.3 to hold is that there exists (asymptotically) finitely many global minima of the function $l$. Indeed, if there exists (asymptotically) finitely many global minima of the function $\ell$, say $K$, this means that for all $k > K$, we have $\delta_k^n\underset{n\to\infty}{\not\to}0$, whereas $\delta_2^n\toinf{n}0$ resulting in $c_k=0$ and H2.3. The value $K-1$ corresponds to the intrisic dimension of the asymptotic random variables $(\pp{\mkn{2}/\mkn{k}}L_{k,1}^n)_{1<k\leq K_n}$.


The hypotheses presented in \autoref{thm:seqQ}, i.e. $\sqrt{n}\delta_2^n\toinf{n}\infty$ and $\delta_2^n\toinf{n}0$, ensure that the loss gaps matrix $D^{n}$ is invertible. 





\section{Preliminary Results}
We give here preliminary results for the proof of the main theorem.

\begin{lemma}
\label{lem:1-1}
Let $K\geq 2$, $a, b \in (0,1)$. Let $P$ and $Q$ be two distributions over $\{1,\dots,K\}$ satisfying $P(1) = 1-a$ and $Q(1) = 1-b$. Then, we have

$$|a-b|\leq\|P-Q\|_{\textrm{TV}}\leq \frac{|a-b| + a + b}{2}.$$
\end{lemma}

% \begin{lemma}
% \label{lem:1-2}
% Let $Z$ be a non-negative random variable.% having a density. 
% Then 
% $$\E\left|Z-\E\cc{Z}\right| = 2\E\cc{Z}P\pp{Z\leq U\E\cc{Z}},$$

% where $U\sim Unif(0,1)$ independent of $Z$.
% \end{lemma}

% \begin{lemma}
% \label{lem:1-3}
% Let $X\in[0,a]$ a.s. such that $a>0$, and let $U\sim Unif(0,1)$ independent of $X$. Then 
% $$P\pp{X\leq U\E\cc{X}} \leq 1-\E\cc{X}/a.$$    

% \end{lemma}

\begin{lemma}
\label{lem:2-2}
Let $M$ be a $J\times J$ square matrix and $j \in \{1,\dots, J\}$. Assume that $M_{-j,-j}$ and ${M_{j,j} - M_{j,-j}\cc{M_{-j,-j}}^{-1}M_{-j,j}}$ are invertible, where $M_{-j,-j}$ is the sub-matrix of $M$ consisting of all entries except the $j^{th}$ row and column and $M_{-j,j}$ is the $j^{th}$ column of $M$ except its $j^{th}$ entry. Then we have 

$$\pp{M^{-1}}_{j,j} = \pp{M_{j,j} - M_{j,-j}\cc{M_{-j,-j}}^{-1}M_{-j,j}}^{-1}$$

\end{lemma}


% \begin{lemma}
% \label{lem:tight}
% Let $X_1,\cdots,X_n,\cdots$ be a sequence of $d-$dimensional random variables such that $\underset{n}{\sup}\,\E\cc{e^{t\|X_n\|}}<\infty$ for any $t>0$ and for some norm $\|\cdot\|$. Then the sequence $\pp{X_n}_n$ is tight.
% \end{lemma}


\begin{proof}[Proof of Lemma \ref{lem:1-1}]
By definition, we have 

\begin{align*}
    \|P-Q\|_{TV} &= \frac{1}{2}\sum_{j=1}^{K}|p_j - q_j| \\
    &= \frac{1}{2}|a-b| + \frac{1}{2}\sum_{j=2}^{K}|p_j-q_j|.
\end{align*}

By construction, we have $\sum_{j=2}^{K}p_j = a$ and $\sum_{j=2}^{K}q_j = b$. 
% C'est facile de montrer (voir les mêmes techniques utilisées pour les calculs de $C(P)$) que 
It is easy to see check that 
\begin{align*}
    \min\; \sum_{j=2}^{K}|p_j-q_j| &= |a-b|, \\
    \max\; \sum_{j=2}^{K}|p_j-q_j| &= a + b,
\end{align*}

where the minimum and the maximum are taken over all distributions satisfying the condition over $p_1$ and $q_1$, which concludes the proof.


\end{proof}

% \begin{proof}[Proof of Lemma \autoref{lem:1-2}]
% Let $F$ be the cumulative distribution function of $Z$ and $m\coloneqq \E[Z]$. Then we have

% \begin{align*}
%     \E\cc{\left|Z - m\right|} &= \E\cc{(m-Z)1_{Z\leq m}}  - \E\cc{(m-Z)1_{Z> m}}  \\
%     &= mF(m) - \E\cc{Z1_{Z\leq m}} - mP\pp{Z> m} + \E\cc{Z1_{Z> m}} \\ 
%     &= 2mF(m) - m - 2\E\cc{Z1_{Z\leq m}} + m \\ 
%     &= 2mF(m) - 2\E\cc{Z1_{Z\leq m}}\\
%     &\overset{(*)}{=}2mP\pp{Z\leq U\E\cc{Z}}
% \end{align*}

% where $(*)$ holds because of 

% \begin{align*}
%      \E\cc{Z1_{Z\leq m}} &= mF(m) - \E\cc{\pp{m-Z}1_{Z\leq m}} \\ 
%      &= mF(m) - \E\cc{\int_{Z}^m dv 1_{Z\leq m}} \\ 
%      &= mF(m) - \E\cc{\int_{0}^m 1_{Z\leq v}dv} \\ 
%      &= mF(m) - \int_{0}^mF(v)dv \\ 
%      &= mF(m) - m\int_{0}^1F(um)du.
% \end{align*}

% hence the result.



% We have
% \begin{align*}
%     \E\cc{\left|Z - \E\cc{Z}\right|} &= \int_0^\infty |z-\E\cc{Z}|f(z)dz \\ 
%     &= \int_0^{\E\cc{Z}}\pp{\E\cc{Z}-z}f(z)dz - \int_{\E\cc{Z}}^\infty \pp{\E\cc{Z}-z}f(z)dz \\ 
%     &= \E\cc{Z}P\pp{Z\leq \E\cc{Z}} - \E\cc{Z1_{Z\leq\E\cc{Z}}} - \E\cc{Z}P\pp{Z\geq \E\cc{Z}} + \E\cc{Z1_{Z\geq \E\cc{Z}}} \\ 
%     &= 2\E\cc{Z}P\pp{Z\leq\E\cc{Z}} - 2\E\cc{Z1_{Z\leq\E\cc{Z}}} \\ 
%     &\overset{(*)}{=}2\E\cc{Z}P\pp{Z\leq U\E\cc{Z}},
% \end{align*}

% where $(*)$ holds because of 

% \begin{align*}
%     \E[Z 1_{Z\leq \E\cc{Z}}] &= \int_0^{\E\cc{Z}}zf(z)dz \\ 
%     &= \E\cc{Z}F(\E\cc{Z}) - \int_0^{\E\cc{Z}} F(z)dz \\ 
%     &= \E\cc{Z}F(\E\cc{Z}) - \E\cc{Z}\E[F(\E\cc{Z}U)],
% \end{align*}

% whith $F$ being the cdf of $Z$.

% \end{proof}


% \begin{proof}[Proof of Lemma \autoref{lem:1-3}]
% Given that $X\in[0,a]$ a.s., for some random variable $U\sim Unif(0,1)$ independent de $X$, we have
% \begin{align*}
%     \E\cc{X} &= \int_{0}^{a}P(X>t)dt = aP(X > aU) \\ 
%     \E\cc{a-X} &= aP(X\leq aU).
% \end{align*}

% Additionally, we have $\E\cc{X} \leq a$, giving 

% \begin{align*}
%     P(X\leq U\E\cc{X}) &\leq P(X\leq aU) \\ 
%     &= 1-\E\cc{X}/a,
% \end{align*}

% which concludes the proof.

% \end{proof}

\begin{proof}[Proof of Lemma \ref{lem:2-2}]
First note that if $M$ is designed by block as follows,

\begin{align*}
    M&=\begin{pmatrix}
    A & B \\
    C & D 
    \end{pmatrix},
\end{align*}

then we have

\begin{equation}
\label{eq:M_inv}
    M^{-1}=\begin{pmatrix}
    \pp{A - BD^{-1}C}^{-1} & * \\
    * & * 
    \end{pmatrix},
\end{equation}

as long as $C$ and $A - BD^{-1}C$ are invertible.
Let $P_j$ and $Q_j$ be $J\times J$ matrices defined for all $j$ such that $P_j M$ permutes the first and the $j^{th}$ rows of $M$, and $Q_j M$ permutes the $(j-1)^{th}$ and the $j^{th}$ rows of $M$. For instance, if $J=4$ and $j=3$ then we have



\begin{align*}
    P_3&=\begin{pmatrix}
    0 & 0 & 1 & 0 \\ 
    0 & 1 & 0 & 0 \\ 
    1 & 0 & 0 & 0 \\ 
    0 & 0 & 0 & 1
    \end{pmatrix}
    ,&     Q_3&=\begin{pmatrix}
    1 & 0 & 0 & 0 \\
    0 & 0 & 1 & 0 \\
    0 & 1 & 0 & 0 \\
    0 & 0 & 0 & 1
    \end{pmatrix}.
\end{align*}


Note that we have $(P_j)^{-1} = P_j$ and $(Q_j)^{-1}=Q_j$. Let $R = Q_3Q_4\cdots Q_jP_jMP_jQ_j\cdots Q_4Q_3$. Developing the formula, we get 

\begin{equation}
\label{eq:RM}
R = \begin{pmatrix}
    M_{j,j} & M_{j,-j} \\ 
    M_{-j,j} & M_{-j,-j}
    \end{pmatrix}.
\end{equation}

On one hand, using \autoref{eq:M_inv} and \autoref{eq:RM}, we have $R^{-1} = \begin{pmatrix}
    \pp{M_{j,j} - M_{j,-j}\cc{M_{-j,-j}}^{-1}M_{-j,j}}^{-1}&\star \\ 
    \star&\star
    \end{pmatrix}$. On the other hand, distributing the inverse operator on the product of matrices and using again \autoref{eq:M_inv}, we have 

\begin{align*}
    R^{-1} &= Q_3Q_4\cdots Q_jP_jM^{-1}P_jQ_j\cdots Q_4Q_3 \\ &= \begin{pmatrix}
    \pp{M^{-1}}_{j,j}&\star \\ 
    \star&\star
    \end{pmatrix},
\end{align*}

which concludes the proof.
\end{proof}

% \begin{proof}[Proof of Lemma \ref{lem:tight}]
% Let $M_t\coloneqq \underset{n}{\sup}\,\E\cc{e^{t\|X_n\|}}$ for some $t>0$. The hypothesis states that $M_t<\infty$ for any $t>0$, but clearly we have $M_t\toinf{n}\infty$. Using exponential Markov inequality, we then have

% \begin{align*}
%     \underset{n}{\sup}P\left(\|X_n\|\geq M_t\right) &\leq \underset{n}{\sup}\, e^{-tM_t}\E\left[e^{t\|X_n\|}\right] \\ 
%     &= \frac{M_t}{e^{tM_t}}.
% \end{align*}

% As $M_t\toinf{t}\infty$, we have that for any $\varepsilon>0$, there exists $t_\varepsilon>0$ such that for any $t\geq t_\varepsilon$ we have $\frac{M_t}{e^{tM_t}}\leq \varepsilon$. Setting $I_\varepsilon\coloneqq B_{\|\cdot\|,M_{t_\varepsilon}}$ the $d-$dimensional centered ball of radius $M_{t_\varepsilon}$, we have 

% \begin{align*}
%     \underset{n}{\sup}\, P\left(X_n \not\in I_\varepsilon\right) &=  \underset{n}{\sup} \,P\left(\|X_n\| \geq M_{t_\varepsilon}\right)  \\ 
%     &\leq\varepsilon.
% \end{align*}

% But $I_\varepsilon$ is compact, hence the tightness.



% \end{proof}

\section{Main Theorems}
\label{sec:main_theorems}

This section is dedicated to the proof of the following theorems. Let denote by $\Delta_n\pp{P,\gA_{\gQ}}\coloneqq 1-{\MIS}_n(P, \gA_\gQ)$ the quantity of interest for which \autoref{thm:fixed_pos} and \autoref{thm:seqQ} give asymptotic results. From \autoref{eq:mis_delta}, we restate \autoref{thm:fixed_pos} and \autoref{thm:seqQ} as follows.


\begin{theorem}[\autoref{thm:fixed_pos}]
\label{thm:main_fixed_quant}
Let $\gQ$ be a fixed quantizer. We assume H1.1 and H1.2. Then, we have 

\begin{equation}
    \liminf{n}\frac{1}{n} \log\,\Delta_n(P,\gA_\gQ) \leq -\underset{x\in\Omega_{K-1}^c}{\inf}\,\underset{t\in\R^{K-1}}{\sup}\,\cc{<t,x> - \log\,\E\cc{e^{<t,D^{-1}\cc{L_{k,j}-\delta_k}_{k>1}>}}} < 0,
\end{equation}

where $\Omega_{K-1} = [-1,\infty)^{K-1}$.


\end{theorem}



\begin{theorem}[\autoref{thm:seqQ}]
\label{thm:main}
Let $\gQ_n$ be a Size-Adaptive quantizer. We assume H2.1, H2.2, H2.3 and H2.4. Assuming that $\liminf{n}\mkn{2} = 0$ and $\liminf{n}\sqrt{n}\delta_2^n=\infty$, we have

\begin{equation}
    \liminf{n}\frac{1}{n \pp{\mkn{2}}^2}\log\, \Delta_n(P,\gA_{\gQ_n}) \leq -1/2\sigma^2.  
\end{equation}

\end{theorem}

We start by giving the proof of \autoref{thm:main} before \autoref{thm:main_fixed_quant}.

\subsection{Proof of \autoref{thm:main}}

The proof of \autoref{thm:main} is immediate from Propositions \ref{prop:1} and \ref{prop:2} given below.

\begin{proposition}
\label{prop:1}
In the context of \autoref{thm:main}, we have 

\begin{equation}
    \liminf{n}\frac{1}{n \pp{\mkn{2}}^2}\log \Delta_n(P,\gA_{\gQ_n}) \leq -\underset{x\in\Omega^c}{\inf}\frac{x^T\Lambda^+x}{2},    
\end{equation}

where $\Lambda$ is defined as

\begin{align*}
     \Lambda &\coloneqq \begin{pmatrix}
        \sigma_{2,2}^2c_2^2 & \cdots & \sigma_{2, J}^2c_2c_K \\ 
        \vdots & \ddots & \vdots \\ 
       \sigma_{K, 2}^2c_{K}c_2 & \cdots & \sigma_{K,K}^2c_K^2
    \end{pmatrix},
\end{align*}

where $\sigma_{k,l}^2 = \liminf{n}Cov(L_{k,1}^n, L_{l,1}^n)$. The matrix $\Lambda^+$ is the Moore-Penrose pseudo-inverse $\Lambda$. 
Note that we have $\sigma_{k,k}^2 = \sigma_k^{2}$.
The set $\Omega$ is given by $\Omega \coloneqq [-1,\infty)^{K-1}$.
\end{proposition}

\begin{proposition}
\label{prop:2}
In the context of Proposition \ref{prop:1}, we have 

\begin{equation}
    \underset{x\in\Omega^c}{\inf}\frac{x^T\Lambda^+x}{2} = 1/2\sigma^2.
\end{equation}
\end{proposition}

\begin{proof}[Proof of Proposition \ref{prop:1}]
Recall that $\hat{\theta}_n\sim\gA_{\gQ_n}\pp{\rz_1,\cdots,\rz_n}$. Note that from \autoref{eq:mis_delta} we have
\begin{equation}
    \Delta_n(P,\gA_{\gQ_n}) = \E\cc{\|\gL\pp{\hat{\theta}_n} - \gL\pp{\hat{\theta}_n\mid \rz_1}\|_{\textrm{TV}}},
\end{equation}
where $\gL(X)$ is the probability law of $X$, and the expectation is taken over $\rz_1$. Letting $Z_k^n =  \mathbb{P}\pp{\hat{\theta}_n = \thetaq_k^n \mid \rz_1}$ and $p_k^n =  \mathbb{P}\pp{\hat{\theta}_n = \thetaq_k^n}$, we have
\begin{align*}
    \Delta_n(P,\gA_{\gQ_n}) &= \frac{1}{2}\sum_{k=1}^{K_n}\E\cc{\left|p_k^n - Z_k^n\right|} \\ 
    &\leq \frac{\E\cc{\left|p_1^n - Z_1^n\right|} + 2 - \E\cc{p_1^n + Z_1^n}}{2} \\ 
    &= \frac{\E\cc{\left|p_1^n - Z_1^n\right|} + 2\pp{1 - p_1^n}}{2} \\ 
    &\leq \frac{2\pp{1-p_1^n} + 2\pp{1-p_1^n}}{2} \\ 
    &= 2\pp{1-p_1^n},
\end{align*}

where the first inequality comes from Lemma \ref{lem:1-1}.
By construction, $\gA_{\gQ_n}$ minimizes the empirical loss. Letting $\Omega_{K-1} = [-1,\infty)^{K-1}$, we then have 
\begin{align}
\notag
    p_1^n &= \mathbb{P}\pp{1 = \underset{k}{\argmin}\left\{\frac{1}{n}\sum_{j=1}^{n} \ell\pp{\thetaq_k^n,\rz_j}\right\}} \\ 
\notag
    &=\mathbb{P}\pp{\forall k > 1, \frac{1}{n}\sum_{j=1}^{n}\cc{\ell\pp{\thetaq_k^n,\rz_j} - \ell\pp{\thetaq_1^n,\rz_j}}\geq 0} \\ 
\notag
    &= \mathbb{P}\pp{\forall k > 1, \frac{1}{n}\sum_{j=1}^n \cc{L_{k,j}^n - \mkn{k}} \geq - \mkn{k}} \\ 
\label{eq:Prob}
    &= \mathbb{P}\pp{\frac{1}{n}\sum_{j=1}^{n}\cc{L_{k,j}^n - \mkn{k}}_{k>1} \in D^n\Omega_{K-1}} \\ 
\notag
    &=\mathbb{P}\pp{\frac{1}{n\mkn{2}}\sum_{j=1}^{n}\cc{\pp{\mkn{2}}^{-1}D^n }^{-1}\cc{L_{k,j}^n - \mkn{k}}_{k>1} \in \Omega_{K-1}},
\end{align}

which gives

\begin{align*}
    1 - p_1^n &= \mathbb{P}\pp{\frac{1}{n\mkn{2}}\sum_{j=1}^{n}\cc{\pp{\mkn{2}}^{-1}D^n}^{-1}\cc{L_{k,j}^n - \mkn{k}}_{k>1} \in \Omega_{K-1}^c}.
\end{align*}


By H2.3, $\cc{\pp{\mkn{2}}^{-1}D^n}^{-1}\cc{L_{k,j}^n - \mkn{k}}_{k>1}$ lives (asymptotically) in a $(K-1)$-dimensional euclidean sub-space.
% Note that by H2.2 and the remark below it, the sequence $\gL\left(\frac{\delta_2^n}{\delta_k^n}\cc{L_{k,1}^n - \delta_k^n}_{k>1}\right)_n$ is tight.
By H2.2 and H2.4, as we live in an Hilbert space,  using \cite{araujo1980central}, we have $\gL\pp{\frac{1}{\sqrt{n}}\sum_{j=1}^n \cc{\pp{\mkn{2}}^{-1}D^n}^{-1}\cc{L_{k,j}^n - \mkn{k}}_{k>1} } \toinf{n} \gamma\coloneqq \gN_{K-1}\pp{0, \Lambda}$, where $\gN_{K-1}$ is the $(K-1)$-dimensional Gaussian distribution.
Now, using the fact that $\E\cc{L_{k,j}^n} = \delta_k^n$, H2.2, H2.4 and convergence to a Gaussian measure, using Theorem 2.2 of \cite{de1992moderate}, we get

\begin{align*}
   \liminf{n} \frac{1}{n\pp{\mkn{2}}^2}\log \pp{1-p_1^n} &= -\underset{x\in\Omega_{K-1}^c}{\inf}\left\{
   \begin{array}{ll}
       \frac{\displaystyle x^T\Lambda^{+}x}{2}  & \text{, if }x\in H_{\gamma}\\
       \infty & \text{, otherwise}
   \end{array}
   \right.\\ 
   &= - \underset{x\in\Omega^c}{\inf}\; \frac{x^T\Lambda^{+}x}{2},
\end{align*}

where $H_\gamma$ is the Hilbert space associated with $\gamma$ (see \cite{de1992moderate}). 
Hence the result.
\end{proof}

\begin{proof}[Proof of Proposition \ref{prop:2}]
Let $M=\Lambda^+$. Note that $\Omega^c = \{x\in\R^{K-1} : \exists j, x_j < -1\}$, giving

$$\underset{x\in\Omega^c}{\inf}\; x^TM x = \underset{j}{\min}\underset{x_{-j}\in\R^{K-1}}{\inf}\underset{x_j<-1}{\inf} x^TMx,$$

where we write $x_{-j}$ equals $x$ where we omit its $j^{th}$ entry. We then shall write 

\begin{align*}
    x^TMx &= x_j^2M_{j,j} + 2x_jx_{-j}^T M_{-j,j} + x_{-j}^TM_{-j,-j}x_{-j}.
\end{align*}




The infimum must be reached on the frontier of the set, i.e. such that $x_j=-1$ for some $j$. Indeed assuming $x$ in the interior of $\Omega^c$, we have that $x_j>-1$ for some $j$. Then for any $1<\alpha<|x_j|$, $x_j/\alpha < -1$ meaning that $x/\alpha$ still belongs to the interior of $\Omega^c$. However, $(x/\alpha)^TM(x/\alpha) = \frac{1}{\alpha^2}x^TMx < x^TMx$, which shows that $x$ was not optimal. For an optimal $x$, we then have 

\begin{align*}
    x^TMx &= M_{j,j} - 2x_{-j}^T M_{-j,j} + x_{-j}^TM_{-j,-j}x_{-j}.
\end{align*}

It is then sufficient to study the optimization problem over $x_{-j}$, which amounts down to the optimization of a quadratic function, whose minimum is then reached for $x_{- j}$ satisfying 

\begin{align*}
    \nabla_{x_{-j}}\pp{- 2x_{-j}^T M_{-j,j} + x_{-j}^TM_{-j,-j}x_{-j}} & = 0 \\ 
    \iff x_{-j} &= \pp{M_{-j,-j}}^{-1}M_{-j,j},
\end{align*}

giving

\begin{align*}
    \underset{x_{-j}\in\R^{J-1}}{\inf}\underset{x_j<-1}{\inf} x^TMx &= M_{j,j} - M_{j,-j}\pp{M_{-j,-j}}^{-1}M_{-j,j}.
\end{align*}

Applying Lemma \ref{lem:2-2} concludes the proof.

\end{proof}

\subsection{Proof of \autoref{thm:main_fixed_quant}}

We give here the proof of \autoref{thm:main_fixed_quant}.

\begin{proof}[Proof of \autoref{thm:main_fixed_quant}]
Following the same steps of the proof of \autoref{thm:main} up to \autoref{eq:Prob}, we have by removing the superscripts $n$ where needed,

\begin{align*}
    p_1 &= P\pp{\frac{1}{n}\sum_{j=1}^{n}\cc{L_{k,j} - \delta_k}_{k>1} \in D\Omega_{K-1}} \\ 
    &= P\pp{\frac{1}{n}\sum_{j=1}^{n}D^{-1}\cc{L_{k,j} - \delta_k}_{k>1} \in \Omega_{K-1}},
\end{align*}

giving

\begin{align*}
    1-p_1 &= P\pp{\frac{1}{n}\sum_{j=1}^{n}D^{-1}\cc{L_{k,j} - \delta_k}_{k>1} \in \Omega_{K-1}^c}.
\end{align*}

From H1.1 , the result follows immediately using Corollary 6.1.6 of \cite{dembo2009large}.

\end{proof}



\FloatBarrier