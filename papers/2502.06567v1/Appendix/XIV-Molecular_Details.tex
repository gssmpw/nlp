\section{Molecular experiments}
\label{app:molecular_details}

\subsection{Comprehensive results}
\label{subsec:comp_results}

We show in~\autoref{tab:mol_classification} and~\autoref{tab:mol_reg} the comprehensive results of the quantized models on the classification and regression tasks, respectively.

We observe that while the quantized models are generally less accurate than the original models, they still achieve reasonable performance on the classification tasks.
On regression examples, the quantized models' performances are significantly lower than the original models.
In particular, when the quantization quantizes on less than 4 bits, the prediction of the molecular properties are almost consistently lower than a simple mean prediction.
As explained in~\autoref{subsec:molecular_expe}, this result is expected, as while classification tasks relies on the definition of boundary between classes, regression tasks require a fine-grained prediction of the target value.

However, while the direct predictions of the quantized models do not provide a good estimate of the target value, the ordering of the predictions is still preserved, as shown by the Spearman correlation between the quantized models' predictions and the labels in~\autoref{tab:mol_reg_sp}.


\begin{table*}
    \caption{
        AUROC performance of the quantized models on the classification tasks, averaged over all embedders.
    }
    \label{tab:mol_classification}
    \resizebox{\textwidth}{!}{
        \input{Tables/mol/mol_classification}
    }
\end{table*}

\begin{table*}
    \caption{
        R2 performance of the quantized models on the regression tasks, averaged over all embedders. If the R2 score is lesser than $-1$, we display $-$inf for clarity.
    }
    \label{tab:mol_reg}
\resizebox{\textwidth}{!}{
        \input{Tables/mol/mol_reg}
    }
\end{table*}


\begin{table*}
    \caption{
        Spearman correlations between the labels and the predictions of the quantized models on the regression tasks, averaged over all embedders.
    }
    \label{tab:mol_reg_sp}
\resizebox{\textwidth}{!}{
        \input{Tables/mol/mol_reg_sp}
    }
\end{table*}

\subsection{Embedder privacy}

\autoref{fig:barplot_dperfs_vs_rdelta} shows the evolution of the privacy of each downstream model $\qcertif$ along with relative performances of the quantized models compared to the original for each pretrained embedder.
For every pretrained embedder, we see no significative difference in the quantizers' privacy ranking, or in the trade-off between security and downstream performance.


\begin{figure*}
    \centering
    \includegraphics[width=0.85\linewidth]{Figures/mol_figs/barplot_dperfs_vs_rdelta}
    \caption{
        Evolution of the privacy of each downstream model $\qcertif$ along with relative performances of the quantized models compared to the original for each pretrained embedder.
        As the privacy of the model decreases, the performances of the quantized model increase, showing the trade-off between security and downstream performance.
    }
    \label{fig:barplot_dperfs_vs_rdelta}
\end{figure*}






\subsection{Details on the evaluation of the quantizers' privacy}
\label{subsec:eval_privacy}

Our hypothesis in the estimation of $\qcertif$  is that the quantized weights with the lowest average loss dominate the maximum value of $\lambda_k = \Lambda_{k,k} = \liminf{n}\pp{\mkn{2}/\mkn{k}}^2(\sigma_k^n)^2$.
We show in~\autoref{fig:k_max} the evolution of $\Lambda_{k,k}$ with the k, where the indexes are sorted with decreasing values of average loss, and the histogram of the index of the maximum value of $\Lambda_{k,k}$ for each quantizer, on 4 different datasets (trained on 500 epochs, hence $k \leq 500$).
The maximum value of $\Lambda_{k,k}$ is indeed consistently reached on low $k$ values, which seems to confirm our hypothesis, validating our sampling strategy for the estimation of $\qcertif$.


\begin{figure}
    \centering
    \begin{subfigure}{0.67\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/mol_figs/hERG/k_max}
    \end{subfigure}
    \begin{subfigure}{0.67\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/mol_figs/AMES/k_max}
    \end{subfigure}
    \begin{subfigure}{0.67\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/mol_figs/Carcinogens_Lagunin/k_max}
    \end{subfigure}
    \begin{subfigure}{0.67\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/mol_figs/Skin__Reaction/k_max}
    \end{subfigure}
    \caption{
        Evolution of $\Lambda_{k,k}$ with the k, where the indexes are sorted with decreasing values of average loss, and the histogram of the index of the maximum value of $\Lambda_{k,k}$ for each quantizer.
    }
    \label{fig:k_max}
\end{figure}


\FloatBarrier