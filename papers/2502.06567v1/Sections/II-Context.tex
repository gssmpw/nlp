%We give in this section the notations and the fundamental concepts relevant to understanding MIS in the context of quantization.

\subsection{Predictive tasks}
Throughout the article, we consider a dataset $\gD_n\coloneqq\{\rz_1,\cdots,\rz_n\}$ of $n$ independent and identically distributed (i.i.d.) data drawn from a common distribution $P$ over a space $\gZ$. We assume that our goal is to infer a predictive function $\hat{\Psi}$ from a set of predictors $\gF\coloneqq\{\Psi_\theta : \theta\in\Theta\}$ indexed by some space $\Theta\subseteq\R^d$. We define a (learning) \textbf{algorithm} as a function $\gA:\bigcup_{n\geq1}\gZ^n\to\gP(\Theta)$, where $\gP(\Theta)$ is the space of all probability measures on $\Theta$. By denoting $\hat{\theta}_n\sim\gA(\rz_1,\cdots,\rz_n)\in\Theta$, we systematically set $\hat{\Psi} = \Psi_{\hat{\theta}_n}$.
This definition of a learning algorithm includes all (stochastic) algorithms. We will assume in the following that the algorithm can be written as a function of the empirical distribution of the data. More specifically, this means that there exists a function $G$ and a random variable $\xi$ such that $\gA\pp{\rz_1,\cdots,\rz_n} = G(\hat{P}_n,\xi)$, where $\hat{P}_n$ is the empirical distribution of the training dataset. In this case, the random variable $\xi$ encompasses the stochasticity of the algorithm. This assumption is especially satisfied for algorithms minimizing an empirical loss $\theta\mapsto\frac{1}{n}\sum_{j=1}^{n}\ell (\theta,\rz_j)$ where $\ell:\Theta\times\gZ\to\R^+$ is the loss function.
%\vspace{1mm}
\begin{example}{Classification.} For a classification task, we may note $\gZ\coloneqq\gX\times\mathcal{Y}$ where 
$\mathcal{Y}=\{1,\cdots,|\mathcal{Y}|\}$ is the number of classes and $\gX$ is the input space. $\gF$ can be the set of all neural networks, where $\theta\in\Theta$ then represents the parameters of such predictors. The algorithm $\gA$ can be any (stochastic) optimizing procedure (e.g., Adam optimizer) on any adequate loss function (e.g., the cross-entropy loss).
\end{example}
\subsection{Quantization}
\label{subsec:quantization}
We define a \textbf{quantizer}~\cite{citeulike:12927267} as any measurable function $\gQ:\Theta\to\Bar{\Theta}\subseteq\Theta$ for some discrete space $\Bar{\Theta}\coloneqq\{\thetaq_1,\cdots,\thetaq_K\}$. 
A quantizer canonically induces a \textbf{quantized algorithm} $\gA_\gQ$, and a loss function $\ell_{\gQ}$, which we simply write $\ell$, as long as there is no ambiguity. For any $\theta\in\Theta$, we denote by $m_\theta=\E\cc{\ell(\theta,\rz)}$ the expected loss evaluated on $\theta$, where $\rz$ is a random variable with distribution $P$. Additionally, for a given quantizer $\gQ$, we will assume without loss of generality that $m_{\thetaq_1}\leq\cdots\leq m_{\thetaq_K}$. 
In the following, we introduce two specific examples to illustrate particular quantization methods.
% \vspace{1mm}

% \textcolor{red}{They don't do that exactly in the paper. TBC and changed.}
\begin{example}[Binarized Neural Networks \cite{wang2023bitnet}.]
\label{ex:BNN}
Let $\gF$ be a set of neural networks with fixed architecture. The set $\Theta$ then represents the parameters of the neural network. A scalar quantizer $\gQ$ maps coordinate-wise the parameters to its sign, namely $\gQ(\theta) = \pp{\theta_j/|\theta_j|}_j$. Here, for $1-$layer unbiased neural networks with width $d$ (number of parameters), the set $\bar{\Theta}$ would consist of all $d-$dimensional vectors in $\{-1,+1\}^d$ where $K = 2^{d}$. Usually, computers store parameters in a 32-bits (or 64-bits) format. Low-precision quantization procedures, e.g. $2-$bits (or $q-$bits in general) quantization, reduce the number of bits required from 32 (or 64) to 2 (or $q$ in general).
\end{example}
%\vspace{.5mm}

\begin{example}[Vector Quantization]
Another quantization procedure, albeit under-used in practice, is vector quantization. A \textit{codebook} $\Thetaq$ is usually pre-computed, which the vector quantizer $\gQ$ maps $\theta$ onto, usually performed by a nearest neighbor algorithm, which makes it efficient and memory-friendly. The constant $K$ here corresponds to the number of values stored in the codebook.
    


\end{example}

\subsection{Privacy assessment}
\label{ssec:privacy_ass}

In the present work, we evaluate the privacy of an algorithm $\gA$ trained on a task $P$ through Membership Inference Attacks (MIAs). Particularly, in a scenario where $\gD_n$ consists of sensible data and the model $\hat\Psi$ has been shared (such as a sold product), MIAs are known to pose a notable threat to the privacy of the dataset. MIAs aim at inferring membership of a test sample $\Tilde{\rz}$ to the dataset $\gD_n$ by observing $\hat{\theta}_n$. MIAs can be defined as follows.

\begin{definition}[Membership Inference Attack - MIA] Any measurable map $\phi : \Theta\times\gZ\to\{\text{member},\text{non-member}\}$ is considered to be a \textit{Membership Inference Attack}.
\end{definition}

The existence of successful MIAs constitute a major threat against the privacy of personal data by revealing sensible information. However, for most algorithms, there may always exist pathological datasets for which models trained on would be highly attackable by MIAs. Specifically, although individual MIAs can reveal information leakage, it is alone insufficient to disclose a complete overview on the privacy level of a machine learning model. To adequately tackle down the question of privacy of an algorithm, it is compulsory to address all possibilities of attacks. We then will say that an algorithm is private if it \textit{usually} produces parameters $\hat{\theta}_n$ that are private against \textit{most} MIAs. We use the notion of accuracy of an MIA, defined as the probability of successfully guessing the membership of the test point. 
Letting $T\in\{\text{member},\text{non-member}\}$ encode the membership of a test point $\Tilde{\rz}$, we define the accuracy as follows. 


% Defining a test point as $\Tilde{\rz}\coloneqq (1-T)\rz_0 + TU$ where $T$ is a Bernoulli random variable with parameter $1/2$, ${\rz_0}\sim \gP$ be a sample independent from the training dataset and $U$ a random variable whose distribution is $\hat{P}_n$ conditionally to the training dataset, we define the accuracy as follows.


\begin{definition}[Accuracy of a given MIA] The \textit{accuracy of an MIA} $\phi$ is defined as 
\begin{equation}
\label{def:perf}
{\text{Acc}}_n(\phi; P,\gA ) \coloneqq P\left (\phi(\hat{\theta}_n,\Tilde{z})= T\right ),
\end{equation}
where the probability is considered over all sources of randomness inherent in the underlying training model and the data used for both training and evaluation.
\end{definition}

% \noindent For most algorithms, there may always exist pathological datasets for which models trained on would be highly attackable by MIAs. %\textcolor{red}{This indicates that data specific metrics are insufficient to gauge the privacy level of an algorithm, justifying the use of the accuracy. \# I do not understand this sentence... \#}

%We relate the privacy of an algorithm to the highest achievable accuracy.


\begin{definition}[MIS] 
The \textit{Membership Inference Security} (MIS) of an algorithm $\gA$ is defined as 
\begin{equation}
\label{def:sec}
{\MIS}_n(P, \gA)\coloneqq 2\left(1-\underset{\phi}{\sup}\;{\text{Acc}}_n(\phi; P,\gA )\right), 
\end{equation}
where the sup is taken over all MIAs and thus, ${\MIS}_n(P, \gA)\leq 2(1-{\text{Acc}}_n(\phi; P,\gA ))$ for all MIAs $\phi$.
\end{definition}
We notice that MIAs with an accuracy of at least \( \frac{1}{2} \) always exist (e.g., constant MIAs). As a result, the MIS metric ranges from \( 0 \) (completely non-private) to \( 1 \) (fully private).

\begin{remark}
The presence of the supremum makes the MIS a metric encompassing all possible MIAs, including all state-of-the-art MIAs, and most importantly, all unknown MIAs. Indeed, even though state-of-the-art MIAs provide a strong indicator on the security of ML models, more powerful MIAs are likely to emerge in the future, beating the state-of-the-art MIAs. Consequently, it is of paramount importance to consider all attacks when designing privacy metrics. 
\end{remark}