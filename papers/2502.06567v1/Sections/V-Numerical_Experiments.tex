\section{Numerical Experiments}
\label{sec:num_exp}



\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{Figures/synth_figs/synth_corr_ranking}
    \caption{
    \textbf{Stability of Privacy rankings.}
    To obtain reliable estimates of $\qcertif$, we average its value over multiple runs $k_{\textrm{run}}$ (number of classifiers trained).
    The central plot illustrates how the rankings of quantizers, based on $\qcertif$, evolve with the number of runs.
    Each column of pie charts represents the proportion of quantizers predicted at each rank (across 100 different subsets of $k_{\textrm{run}}$ runs) with connecting lines showing shifts in predicted rankings.
    As the number of runs increases, the rankings stabilize, and when averaged over 50 runs, each quantizer is ranked at its final position 90\% of the time (except for the \texttt{2 bits} and \texttt{1.58b 33\%} quantizers).
    The top figure shows the evolution of the average Spearman correlation between $\qcertif$ (resp. the baseline's estimation of the MIS) when evaluated over $k_{\textrm{run}}\leq 100$ and $k_{\textrm{run}} =300$.
    The confusion matrices on the right compare rankings estimated using 300 runs to those obtained with 20 and 50 runs.
    }
    \label{fig:synth_stability}
\end{figure*}

\label{subsec:synthetic_expe}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{Figures/synth_figs/rdelta_vs_acc_bsl}
    \caption{
        \textbf{Relationship between $\qcertif$ and the MIS.}
         Each sub-plot displays the estimated values of $\qcertif$ and the MIS for each quantizer under varying experimental configurations, with their corresponding Spearman correlation ($\rho_{sp}$).
        The strong correlations confirm that our method enables the comparison of different quantization techniques' security.
    }
    \label{fig:scatterplot_synth}
\end{figure}
As we focus solely on ranking quantization procedures (\autoref{thm:seqQ} being asymptotic and potentially containing irrelevant constants), we omit specific 
$\qcertif$ values in the figures for clarity and readability.
%Since we are only concerned with ranking the quantization procedures (\autoref{thm:seqQ} being an asymptotic result that may include irrelevant constants), we omit the specific values of \(\qcertif\) in the following figures to improve clarity and readability.

\subsection{Synthetic experiments}
To confirm that our estimation of $\qcertif$ enables the ranking of quantization methods according to their level of privacy, we compare this ranking to the one obtained using the baseline estimation of the MIS.
To perform this comparison, we rely on synthetic experiments, where we can train and evaluate a large number of classifiers, and ensure the independence between each classifier's training set to train $\bsl$.

\subsubsection{Experimental Setup}
\paragraph{Datasets.}
We generate data points sampled from $\mathbb{R}^{128}$ using $k_{\textrm{modes}} \in \{6,8,16\}$ isotropic Gaussian distributions with standard deviation $\sigma\in \{1.5,2,3\}$, where each cluster is assigned a label in $\{0,1\}$.

\paragraph{Classifiers.}
As mentioned above, the baseline estimation method is limited to models consisting of a single layer.
Consequently, all classifiers in our experiments are single-layer fully connected networks. 
To increase the expressivity of these networks, the input features are augmented with $x^2$ (element-wise squared features).
Each classifier is trained on $n=128$ samples for 3000 epochs using the Adam optimizer with a learning rate of $10^{-4}$.
To evaluate our approach, we train $k_{\textrm{run}}=300$ classifiers ($\theta$) on each distribution, with each run involving new i.i.d. samples, and we investigate in~\autoref{fig:synth_stability} the impact of $k_{\textrm{run}}$ on our evaluation of the quantizers' privacy.


\paragraph{Quantization.}
For our experiments, we consider a range of different quantization methods, including: 1bit quantization by taking the sign of the weights (\texttt{Sign}), 1.58 bits quantization with different sparsity levels (\texttt{1.58b \{x\}\%} where $x\%$ of the weights with the smallest magnitude are set to zero, and the rest to their sign), and quantization on 2, 3, 4, and 5 bits.
A detailed description of the quantization methods is provided in~\autoref{app:quantization}.


\subsubsection{Validation of the approach}
\label{sssec:val}
\paragraph{Estimation of the privacy guarantees.}
\autoref{fig:scatterplot_synth} illustrates the correlation between our proposed metric $\qcertif$ and the MIS baseline.
We observe that our metric effectively quantifies the privacy of quantized models: higher values correspond to greater MIS.
We report the Spearman correlation between both metrics (the correlation between the rankings induced by both metrics), and as expected, quantizers with more bits of information, such as \texttt{5 bits} and \texttt{4 bits}, are the least private across all datasets.
In contrast, the \texttt{1.58b 90\%} quantizer, which introduces 90\% sparsity by setting weights to zero, achieves the highest privacy.
Interestingly, the \texttt{Sign} method, which quantizes weights to 1 bit, is less private than the \texttt{1.58b 33\%} quantizer, despite using fewer bits.
This behavior aligns with observations from the baseline method, suggesting that sparsity plays a more significant role in privacy than the number of bits alone.
Overall, the rankings produced by $\qcertif$ closely match those of the baseline method, with an average Spearman correlation of $\rho_{sp}=0.86$, demonstrating that $\qcertif$ reliably ranks quantization methods by their privacy levels.

\paragraph{Quantizer ranking stability.}
While our experiments compute $\qcertif$ by averaging over 300 runs, such extensive computation may be impractical for real-world deployment due to resource constraints.
\autoref{fig:synth_stability} shows how the stability of $\qcertif$-based rankings improves as the number of runs increases.
Critically, the ranking of the most and least private quantizers stabilizes early: after just 20 runs, the \texttt{5 bits} quantizer is consistently ranked least private, followed by \texttt{4 bits} and \texttt{3 bits}, while \texttt{1.58b 90\%} (highest sparsity) remains the most private, followed by \texttt{1.58b 50\%}.

\paragraph{Computational cost.}
All experiments were conducted on NVIDIA A6000 GPUs with 48GB of memory.
The estimation of $\qcertif$ introduces a small computational overhead during model training, as it requires computing the quantized model's loss at each validation step, often for multiple quantization processes, adding approximately 1s to the training time of one $\thetan$ (4m total).
In contrast, the baseline MIS estimation requires training multiple classifiers ($\thetan$) to train the discriminator $\bsl$, which can also be sensitive to the hyperparameter choice, resulting in significant computational and data demands.
The top figure of~\autoref{fig:synth_stability} shows that the rankings obtained with the $\qcertif$-based approach stabilizes close to the final ranking after only 20 runs, as demonstrated with Spearman correlations over $0.95$, while the MIS's baseline estimation does not reach this threshold until 150 runs.
As a result, ranking quantizers using $\qcertif$ ($\approx$1h) is significantly faster than using the baseline MIS method ($\approx$10h).

\subsection{Experiments on molecular datasets}
\label{subsec:molecular_expe}

\subsubsection{Experimental setup}
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{Figures/mol_figs/barplot_dperfs_vs_rdelta_cls_dataset}
    \caption{
        \textbf{Impact of quantization on classification tasks.}
        Evolution of the privacy of each downstream model $\qcertif$ along with relative performances of the quantized models compared to the original on classification task.
    }
    \label{fig:barplot_dperfs_vs_rdelta_cls_dataset}
    \vspace{-0.25cm}
\end{figure*}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Figures/mol_figs/barplot_dperfs_vs_rdelta_reg_dataset}
    \caption{
        \textbf{Impact of quantization on regression tasks.}
        Evolution of the privacy of each downstream model $\qcertif$ along with relative performances of the quantized models compared to the original on regression task.
    }
    \label{fig:barplot_dperfs_vs_rdelta_reg_dataset}
    \vspace{-0.5cm}
\end{figure}


In our second experimental setting, we analyze a real-world application: molecular modeling.
In the field of drug discovery, data is an invaluable and highly sensitive asset, and determining whether predictive models might inadvertently leak proprietary data is therefore highly valuable.


\paragraph{Pretrained Embedders.}
To generate one-dimensional molecular embeddings, we evaluate four pretrained models from the  representation learning literature: GraphMVP~\cite{liu2022pretraining} and 3D-Infomax~\cite{stark2021_3dinfomax} (3D-2D mutual information maximization), MolR~\cite{wang2022chemicalreactionaware} (reaction-aware pretraining), and ChemBERTa-MTR~\cite{ahmad2022chemberta2} (multitask regression with SMILES tokenization).
The embeddings are passed to small feed-forward networks for downstream tasks, and we quantify the privacy impact of each embedder in~\autoref{app:molecular_details}.


\paragraph{Downstream tasks.}
We evaluate and train the models on various property prediction tasks from the Therapeutic Data Commons (TDC) platform~\cite{Huang2021tdc}, focusing on ADMET properties (Absorption, Distribution, Metabolism, Excretion, and Toxicity).
These tasks encompass both binary classification and regression problems with datasets of varying sizes and complexities.
For classification tasks, we report the AUC-ROC scores, and for regression tasks, we report R2 scores (coefficient of determination).
For both metrics, higher values indicate better performance, with 1 being the maximum value for both metrics.
For the regression tasks, the R2 score can be negative, indicating that the model performs worse than a simple mean prediction.

\paragraph{Task models.}
For each downstream task, we train fully connected networks (two layers, hidden dimension 128) for 500 epochs.
To ensure robust estimation of $\qcertif$, we allocate 40\% of the dataset to the validation set.
Each experiment is repeated~\nrunmol times, with 90\% of the training set randomly subsampled in each run to ensure different training trajectories are used.
We quantify the impact of quantization on performance measuring the relative performance of the quantized model to the original: for a metric $m$ (AUROC or R2): \(
\diffmetric{\textrm{val}}{m} = {m^{\textrm{val}}_{\text{quantized}}}/{m^{\textrm{val}}_{\text{original}}}
\).
Full results, including performance-privacy trade-offs grouped by embedders, are available in~\autoref{app:molecular_details}.

\subsubsection{Privacy-Performance Trade-off}
\label{ssec:tradeoff}

\paragraph{Classification tasks.}
\autoref{fig:barplot_dperfs_vs_rdelta_cls_dataset} illustrates the trade-off between privacy ($\qcertif$) and relative performance ($\diffmetric{\textrm{val}}{\textrm{AUROC}}$) for classification tasks.
We identify two regimes: (1) for high values of $\qcertif$, the quantized models achieve high privacy (e.g., \texttt{1.58b 90\%}), but the performances are significantly lower than the original models ($\diffmetric{\textrm{val}}{\textrm{AUROC}} \approx 90\%$), in particular on ClinTox~\cite{CLINTOX} (toxicity prediction) and PAMPA\_NCATS~\cite{PAMPA} (membrane permeability) where $\diffmetric{\textrm{val}}{\textrm{AUROC}}$ goes down to $80\%$.
(2) Low-privacy quantizers (\texttt{2 - 5 bits}) preserve near-original performance but are consistently ranked least private.
Across almost all datasets, the \texttt{1.58b 90\%} quantizer appears to be the most secure, while obtaining better performance than other sparse quantizers, notably on ClinTox, Carcinogens\_Lagunin~\cite{CARCINOGENS}, and CYP2C9 substrate classification~\cite{CYP_CARB}.


\paragraph{Regression tasks.}
For regression tasks (see~\autoref{fig:barplot_dperfs_vs_rdelta_reg_dataset}), quantization introduces a stark privacy-performance imbalance.
Unlike classification, even moderately aggressive quantization (e.g., \texttt{2 bits}) results in negative R2 scores, indicating worse-than-baseline predictions (even though the direct predictions are not accurate, we show in the \autoref{subsec:comp_results} the ordering of the predictions is preserved).
Only non-private quantizers (\texttt{4 - 5 bits}) achieve R2 scores comparable to original models.
This disparity arises because regression requires precise weight values to estimate continuous targets, whereas classification relies on decision boundaries that are more robust to quantization.
Consequently, regression tasks lack a viable privacy-performance trade-off: quantizers either degrade performance catastrophically or retain performance at the cost of privacy.
This underscores the need for novel quantization strategies tailored to regression.
