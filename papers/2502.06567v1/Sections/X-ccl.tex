In this work, we investigated the privacy of quantization procedures for machine learning models, particularly their vulnerability to data leakage. We established a theoretical foundation by proving that, for both fixed and adaptive model-quantization procedures, the Membership Inference Security (MIS) of the learning algorithm is asymptotically determined by the distribution of the quantized modelsâ€™ loss per sample.
We introduced a methodology for comparing quantization procedures in terms of privacy, with limited computational cost. Through extensive experiments on both synthetic and real-world datasets, we validated the effectiveness of our approach and explored the privacy-performance trade-offs of quantization in molecular property prediction, highlighting the practical implications of our findings.

Our study has some limitations.  Since our analysis focuses on evaluating a training procedure rather than individual trained models, it does not directly predict the security of a specific trained model. Additionally, due to computational constraints, we concentrated on Post-Training Quantization (PTQ) and did not examine Quantization-Aware Training (QAT), which presents a potential direction for future research. 
 Specifically, we aim to explore QAT, where $r_\gQ^n$ could be jointly optimized with the loss function.