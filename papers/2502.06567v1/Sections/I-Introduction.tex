% A probably terrible starting point, but a starting point still \o/


Reducing the computational and memory costs of machine learning models is a critical aspect of their deployment, particularly on edge devices and resource-constrained environments. Quantization stands out among the various methods available to enhance inference efficiency in neural networks, such as knowledge distillation and pruning, due to its distinct advantages and proven practical success~\cite{gholami2022survey}. One key benefit is that the storage and latency improvements achieved through quantization are deterministically defined by the chosen quantization level (e.g., using 8-bit integers instead of 32-bit floating-point numbers). Moreover, uniform quantization is inherently hardware-friendly, facilitating the practical realization of theoretical efficiency gains.

%A widely adopted strategy to achieve this is model quantization, where the precision of data types storing the model's weights and activations is reduced, for instance, using 8-bit integers instead of 32-bit floating-point numbers. This approach not only decreases memory usage, but also accelerates computation, enabling the deployment of large-scale models with minimal loss in performance.

While quantization effectively improves efficiency, its impact on the privacy of machine learning models remains largely under-explored. A particularly intriguing question is whether quantization can also strengthen a model's resilience against adversarial threats, such as the extraction of sensitive information. By reducing the precision of a model’s parameters, quantization naturally discards some information~\cite{6451278}, which leads to the hypothesis that this process could potentially reduce the risk of recovering the model’s training data or other private information. However, to our knowledge, the security of quantized models against such privacy attacks has not yet been theoretically investigated.

%Although quantization addresses efficiency, its implications for the privacy of machine learning models remain unexplored. Specifically, an intriguing question arises: can quantization also enhance the model's resilience against adversarial threats, such as the extraction of sensitive information? Intuitively, by reducing the precision of a model’s parameters, quantization inherently discards some information. This raises the hypothesis that such a process could mitigate the risk of recovering the model’s training set or other private data. However, to the best of our knowledge, the security of quantized models against such attacks has yet to be studied.


In this paper, we explore the effect of quantization on the Membership Inference vulnerability of machine learning models. We propose a novel privacy metric, based upon the Membership Inference Security (MIS) \cite{aubinais2023fundamental}  and derive asymptotic bounds—relative to the training sample size—to quantify the privacy implications of model quantization. Using these theoretical insights, we present a systematic framework for evaluating and comparing fundamental quantization techniques, with a focus on the observed loss values. This methodology provides a thorough analysis of how different quantization methods strike a balance between privacy (against the most powerful attacks) and performance. To validate our approach, we compare it against an established baseline technique, showing consistent rankings of quantization methods based on their ability to preserve privacy.

%In this paper, we study the effect of quantization on the Memebership Inference level of a machine learning model. Specifically, we provide asymptotic bounds on the privacy level of a machine learning model that has been quantized, based on an interpretable metric called \textbf{Membership Inference Security} (MIS). We exploit the theoretical results to establish a systemic methodology to compare several quantization methods, wholly based on the different observed loss values, leading to a comprehensive overview on the capability of different quantization methods to provide security while preserving efficacy. We validate our novel methodology by comparing its results to those of some \textit{baseline methodology} on which we show consistency on the order displayed between the different quantizations. 

\subsection{Our contributions}
Our contributions can be summarized as follows: 
\begin{itemize}
    \item We show that, for a fixed model and quantization procedure, as the size of the training dataset increases towards infinity, the MIS of the learning algorithm is fully determined by the distribution of the loss per sample for the quantized models (\autoref{thm:fixed_pos}). Building upon this result, we further show that when the model architecture and/or quantization procedure adapts to the training set size, a similar dependency persists (\autoref{thm:seqQ}), although with a more precise dependence on the covariance structure of the loss per sample.
    
    %We show that for a fixed model and quantization procedure, as the size of the training dataset approaches infinity, the Membership Inference Security (MIS) of the learning algorithm is entirely determined by the distribution of the quantized models' loss per sample (\autoref{thm:fixed_pos}). Extending this result, we prove that when the model architecture and/or quantization procedure adapts to the training set size, a similar dependency holds (\autoref{thm:seqQ}).
    \item Building on the result of \autoref{thm:seqQ}, whose direct estimation is computationally prohibitive, we propose a methodology (\autoref{ssec:algo}) enabling the comparison of quantizers in terms of privacy.
    \item We apply our methodology to several Post-Training Quantization techniques on both synthetic data (see~\autoref{subsec:synthetic_expe}) and real-world data (see~\autoref{subsec:molecular_expe}).
    We show that the rankings provided by our method consistently correlates with the ones obtained with a baseline estimation of the MIS (see~\autoref{sssec:val}), and study the privacy-performance trade-off of quantization on molecular property prediction tasks (see~\autoref{ssec:tradeoff}).
\end{itemize}


%\begin{itemize}
%    \item 
%
%    \item Building on the result of Theorem \ref{thm:seqQ}, we propose a methodology (see Section \ref{ssec:algo}) to systematically construct a hierarchy between quantization procedures, providing a comprehensive perspective on the trade-off between efficacy and privacy. We apply our methodology to several Post-Training Quantization techniques and evaluate its performance on both synthetic data (see Section \ref{subsec:synthetic_expe}) and real-world data (see Section \ref{subsec:molecular_expe}).We apply our methodology to several Post-Training Quantization techniques and evaluate its performance on both synthetic data (see Section \ref{subsec:synthetic_expe}) and real-world data (see Section \ref{subsec:molecular_expe}).
%
%    \item We empirically validate our novel methodology by comparing the ranking we obtain to a baseline estimation of the MIS (see Section \ref{ssec:baseline_estimation}), showing that our methodology consistently correlates with the MIS (see Section \ref{sssec:val}).
%    We highlight the advantages of our methodology, including its computational efficiency, reduced memory requirements, and adaptability to molecular datasets.We highlight the advantages of our methodology, including its computational efficiency, reduced memory requirements, and adaptability to molecular datasets.
%\end{itemize}


Our work is dedicated to providing a theoretically grounded methodology to compare different quantization procedures in the context of MIS evaluation.

\subsection{Related work}

\textbf{Quantization of neural networks.} With the deployment of neural networks on edge devices \cite{yuan2024vit, lin2024awq}, where inference should be time and memory efficient, several quantization procedures have been studied and employed. Quantization usually answers this task by reducing the (bit-)precision of the parameters of the neural networks, demonstrating effectiveness in Large Language Models \cite{gong2024survey, zhu2024survey} even when the quantization is as strong as 1-bit precision quantization \cite{wang2023bitnet, ma2024fbi}, 1.58-bits precision quantization \cite{ma2024era1bitllmslarge}, arbitrary bits precision \cite{zeng2024abq}.
The most adopted framework of quantization is Post-Training Quantization (PTQ) \cite{jacob2018quantization, nagel2019data, gholami2022survey} which provides simple training-free implementation. PTQ is usually adopted over Quantization-Aware Training (QAT) \cite{bengio2013estimating, banner2018scalable, nagel2021white, nagel2022overcoming, pang2024push} due to their limitations to scale up to larger models \cite{gholami2022survey, lin2024awq}. Additionally, some lines of work study "hardware-aware" quantization procedures \cite{wang2024ladder, balaskas2024hardware} where optimization is made directly on the hardware. During our experiments, we will focus on PTQ.



\textbf{Membership Inference Attacks.}
Membership Inference Attacks (MIAs) can reveal sensible information \cite{shokri2017membership, song2017machine,carlini2022membership, carlini2023extracting} about one's data by leveraging the information stored in the parameters of the ML model \cite{hartley2022measuring, del2023bounding}. An extensive line of work has developed in the past decade to construct ever so powerful MIAs in embedding models \cite{song2020information}, regression models \cite{gupta2021membership} or generative models \cite{hayes202588705membership}, systematically summarized in \cite{hu2022membership}. Recent works have leveraged the predictive power of LLMs to construct new MIAs \cite{staab2023beyond, wang2025survey}. While few works have delved into the theoretical intricacies of MIAs \cite{sablayrolles2019white, del2023bounding, aubinais2023fundamental}, several Privacy benchmarks have been developed to audit the privacy risks of ML models \cite{murakonda2020ml, liu2022ml} by evaluating state-of-the-art MIAs on the target model. Although these benchmarks offer valuable insights into the privacy leakage of an ML model, a single MIA alone cannot provide a comprehensive assessment of an algorithm's overall privacy resilience against various attacks. We briefly discuss it in \autoref{ssec:privacy_ass}. 

\textbf{Quantization and Privacy.} Various strategies to protect models from attacks like MIAs have been proposed. In federated learning, the effects of input and gradient quantization have been analyzed through the lens of differential privacy~\cite{youn2023randomizedquantizationneeddifferential, yan_killing_2024, pmlr-v180-chaudhuri22a}. However, the impact of model quantization on security has been primarily assessed through empirical evaluations of MIAs, with no existing theoretical analysis~\cite{kowalski_towards_2022, s23187722}. Our work aims to fill this gap by providing a rigorous theoretical evaluation of the security implications of model quantization.