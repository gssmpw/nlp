\section{Benchmark Setup} \label{sec:benchmark_setup}


\subsection{Dataset}
To evaluate the multi-image reasoning capabilities of multimodal LLMs, we curated a subset of 120 questions and 376 images from the MUIRBench \cite{wang2024muirbench} dataset, ensuring a balanced assessment across diverse reasoning tasks. Unlike traditional benchmarks that focus on single-image tasks, this dataset challenges models to process and integrate multiple visual inputs, making it a more rigorous evaluation framework for contextual, spatial, and logical reasoning.


The dataset maintains diversity in image types, as illustrated in Figure \ref{fig:image_prop}, spanning real-world photographs, medical imagery, scientific diagrams, and satellite views. By integrating both curated and synthetic data, the benchmark mitigates data contamination risks, ensuring models are tested on truly novel inputs.

\begin{figure*}
    \centering
    \includegraphics[width=16cm]{figs/big_plot.png}
    % \vspace{-1em}
    \caption{The distribution of the questions based on the type of images, image relations, and tasks.}
    \vspace{-1em}
    \label{fig:image_prop}
\end{figure*} 
Additionally, the dataset includes a range of multi-image relations, depicted in Figure \ref{fig:image_prop}, challenging models to process various dependencies such as temporal sequences, complementary perspectives, and multi-view representations. This structured approach ensures that models are tested beyond simple pattern recognition, requiring advanced spatial, logical, and comparative reasoning. 


The selection of questions from MUIRBench was guided by the need for a balanced and diverse evaluation while maintaining computational feasibility. Given that MUIRBench consists of 2,600 multiple-choice questions across 12 multi-image reasoning tasks, we curated a representative subset of 120 questions and 376 images, averaging 3.13 images per question. This selection ensures an efficient assessment of multimodal LLMs while focusing on key reasoning abilities.

We prioritized coverage of core multi-image tasks, selecting 8 diverse tasks that span both low-level perception (e.g., difference spotting) and high-level reasoning (e.g., image-text matching) as seen in Fig \ref{fig:image_prop}. Tasks with high significance in multimodal evaluation, such as Image-Text Matching and Difference Spotting, were given higher representation, each comprising 28 questions. Additionally, we ensured diversity in multi-image relationships, including temporal sequences, complementary perspectives, and multi-view representations, to evaluate models' ability to integrate spatial, logical, and comparative reasoning across multiple images.

Our selection also maintained MUIRBench’s answerable-unanswerable pairing strategy by including 40 unanswerable questions. This prevents models from exploiting shortcut biases and lucky guesses and encourages a deeper understanding of visual content. 

Additionally, we incorporated alternate versions of each question by reordering the answer choices. This ensures that models do not rely on positional biases or heuristic patterns but genuinely understand and reason the multi-image context. By shuffling the answer choices, we can verify whether a model is consistently selecting the correct response rather than guessing based on positional tendencies. 

This question refinement strategy improves model robustness by ensuring they rely on reasoning rather than heuristics. Our study offers a rigorous and efficient way to test how well next-generation vision-language models can reason about multiple images.

\subsection{Models}

In our evaluation, we utilized a diverse set of multimodal language models, as seen in Table \ref{tab: llm_table}, each designed with unique capabilities to process and reason over visual inputs. Below is a brief overview of each model: 

\subsubsection{Janus 7B} 
Developed by DeepSeek and was released on January 27, 2025, Janus 7B \cite{chen2025janus} is a 7-billion parameter open-source multimodal model built for advanced image understanding and text-to-image generation. It supports multiple-image processing, making it suitable for complex visual reasoning tasks.

\subsubsection{Janus 1B} 
developed by DeepSeek and was Released alongside Janus 7B on January 27, 2025. Janus 1B \cite{chen2025janus} is a lightweight 1-billion parameter open-source variant, designed for efficient multi-image processing while maintaining a lower computational footprint.

\subsubsection{Grok 3} Grok 3 is a multimodal large language model developed by $\mathbb{X}$AI. Released on February 17, 2025, Grok 3 boasts 2.7 trillion parameters, making it one of the most advanced AI models to date. 

\subsubsection{Gemini 2.0 Flash Experimental} 
Launched on February 10, 2025, Gemini 2.0 Flash Experimental \cite{gemini} is an advanced multimodal model developed by DeepMind that processes both multiple images and videos. It is optimized for rapid inference and efficient memory usage, making it well-suited for real-time visual reasoning applications.

\subsubsection{QVQ-72B-Preview} 
Released on December 25, 2024, QVQ-72B-Preview \cite{bai2023qwen} is a 72-billion parameter open-source vision-language model developed by Alibaba that introduces novel techniques in visual question answering. It supports multi-image reasoning, allowing for better contextual understanding across images.

\subsubsection{Qwen2.5-VL-72B-Instruct} 
Developed by Alibaba and released on September 19, 2024, Qwen2.5-VL-72B-Instruct \cite{bai2023qwen} is a 72-billion parameter open-source instruction-tuned vision-language model. It incorporates dynamic resolution handling and multimodal rotary position embedding (M-ROPE), enabling multi-image and video comprehension.

\subsubsection{Pixtral 12B} 
Released on September 17, 2024, Pixtral 12B \cite{agrawal2024pixtral} is a 12-billion parameter open-source multimodal model developed by Mistral AI, specializing in high-resolution image analysis and text generation.

\subsubsection{ChatGPT-o1} 
Introduced on December 5, 2024, ChatGPT-o1 \cite{chatgpt} is an advanced iteration of OpenAI’s multimodal models, supporting multiple images and video inputs. It enhances contextual multimodal understanding, making it effective for vision-language interactions.

\subsubsection{ChatGPT-4o} 
Released on May 13, 2024, ChatGPT-4o \cite{chatgpt} is a multimodal model developed by OpenAI, capable of processing multiple images. It improves on its predecessors by enhancing image-text alignment and expanding multimodal reasoning capabilities.


\begin{table}[]
\caption{Summary of evaluated multimodal language models, including their parameter sizes, release dates, image processing capabilities, and open-source availability.}

\begin{tabular}{|l|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{\textbf{Model}}                                        & \textbf{\begin{tabular}[c]{@{}c@{}}Number of\\ Parameters\end{tabular}} & \textbf{Release Date} & \textbf{\begin{tabular}[c]{@{}c@{}}Image \\ Processing \\ Capability\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Open\\ Source\end{tabular}} \\ \hline
Janus 7B                                                                    & 7B                                                                      & Jan 27, 2025          & \begin{tabular}[c]{@{}c@{}}Multiple \\ images\end{tabular}                         & Yes                                                            \\ \hline
Janus 1B                                                                    & 1B                                                                      & Jan 27, 2025          & \begin{tabular}[c]{@{}c@{}}Multiple\\ images\end{tabular}                          & Yes                                                            \\ \hline
Grok 3                                                                      & 2.7T                                                                    & Feb  17, 2025         & \begin{tabular}[c]{@{}c@{}}Multiple\\ images\end{tabular}                          & No                                                             \\ \hline
\begin{tabular}[c]{@{}l@{}}Gemini 2.0 \\ Flash \\ Experimental\end{tabular} & N/A                                                                     & Feb 10, 2025          & \begin{tabular}[c]{@{}c@{}}Multiple \\ images\\ and videos\end{tabular}            & No                                                             \\ \hline
\begin{tabular}[c]{@{}l@{}}QVQ-72B-\\ Preview\end{tabular}                  & 72B                                                                     & Dec 25, 2024          & \begin{tabular}[c]{@{}c@{}}Multiple \\ images\end{tabular}                         & Yes                                                            \\ \hline
\begin{tabular}[c]{@{}l@{}}Qwen2.5-VL-\\ 72B-Instruct\end{tabular}          & 72B                                                                     & Sep 19, 2024          & \begin{tabular}[c]{@{}c@{}}Multiple\\ images\\ and videos\end{tabular}             & Yes                                                            \\ \hline
Pixtral 12B                                                                 & 12B                                                                     & Sep 17, 2024          & \begin{tabular}[c]{@{}c@{}}Multiple \\ images\end{tabular}                         & Yes                                                            \\ \hline
ChatGPT-o1                                                                  & N/A                                                                     & Dec 5, 2024           & \begin{tabular}[c]{@{}c@{}}Multiple\\ images\\ and videos\end{tabular}             & No                                                             \\ \hline
ChatGPT-4o                                                                  & N/A                                                                     & May 13, 2024          & \begin{tabular}[c]{@{}c@{}}Multiple \\ images\end{tabular}                         & No                                                             \\ \hline
\end{tabular}
\label{tab: llm_table}
\end{table}


This selection encompasses a range of models with varying capacities and innovations, providing a comprehensive basis for evaluating multimodal language models in visual reasoning tasks.

\section{Methodology}

\subsection{Experimental Setup}


\subsubsection{Question Presentation Format} 
To ensure a standardized evaluation across all models, each question follows a structured format where the model receives a textual prompt and the corresponding images. The format is as follows:

\begin{quote}
\textbf{Input:} \{Question\} These are the options: [A) First Option, B) Second Option, C) Third Option, D) Fourth.]
\end{quote}
\begin{quote}
\textbf{Example:} Which picture below better fits the description: A black and white cat sitting in a green bowl? These are the options: [A) First image, B) Second image, C) None of the choices provided, D) Third image.]
\end{quote}

Models are expected to select the most appropriate answer based on the given images. To assess model consistency and robustness, each question is tested in different variations:
\begin{enumerate}
    \item Answerable Version: The correct answer is available among the choices.
    \item Reordered Version: The same question with shuffled answer choices to detect positional biases.
    \item Unanswerable Version: The correct answer is removed or altered, testing the model’s ability to recognize when a question cannot be answered.
\end{enumerate}

To further clarify this evaluation process, Figures \ref{fig:answerable}, \ref{fig:reordered}, and \ref{fig:unanswerable} illustrate examples of each variation.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/Question_V1.png}
    \caption{Example of an answerable question where the correct answer is available among the choices.}
    \label{fig:answerable}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/Question_1_V2.png}
    \caption{Example of the same question with reordered answer choices to test for positional biases.}
    \label{fig:reordered}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/Question_1_V3.png}
    \caption{Example of an unanswerable question where the correct choice is removed or altered.}
    \label{fig:unanswerable}
\end{figure}



\subsubsection{Local Deployment for Janus Models} 
For Janus 7B and Janus 1B, we ran the experiments on a high-performance workstation equipped with two NVIDIA RTX 4090 GPUs, each featuring 16,384 CUDA cores due to the unavailability of an official online interface.

\subsubsection{Web-Based Evaluation for Other Models} 
For the remaining models (Grok3, QVQ-72B-Preview, Qwen2.5-VL-72B-Instruct, Pixtral 12B, ChatGPT-4o, ChatGPT-o1, and Gemini 2.0 Flash Experimental), we conducted evaluations using their official online interfaces. To ensure zero-shot learning, a new chat session was initialized for each question, preventing any contextual memory retention. We set the temperature to 1.0, encouraging diverse yet unbiased model responses.

\subsubsection{Consistency and Fairness Measures}
All models were assessed using identical input formats, with questions presented in their original multiple-choice structure. No additional context was given beyond the provided question and images, ensuring models relied solely on intrinsic reasoning capabilities.

This setup guarantees a standardized comparison across models, enabling a robust assessment of multi-image reasoning capabilities.


\subsection{Evaluation Setup}

To ensure a standardized and fair assessment of all multimodal LLMs, we established a strict evaluation protocol that handles varying model constraints, answer formats, and consistency measures.




\subsubsection{Handling of Image Restrictions} 
Some models have restrictive policies that prevent them from processing images, either due to safety mechanisms or inherent limitations. If a model fails to accept an image input, the response is automatically marked as incorrect, as it indicates an inability to engage with the core visual reasoning task.

\subsubsection{Answer Parsing and Validity} 
To eliminate ambiguity in model responses, we adhere to a strict answer validation process:
\begin{itemize}
    \item If a model provides an answer choice that does not exist among the given options, the response is considered incorrect.
    \item If the model correctly identifies the answer but provides an incorrect explanation, the response is still marked as correct, as the primary evaluation metric is answer selection. For instance, if a model produces an answer such as “C) First image” when the correct answer is “C) Second image”, the response is marked as correct since the answer letter aligns with the expected choice.
\end{itemize}

\subsubsection{Ensuring Answer Consistency} 
To detect inconsistencies in model reasoning, we incorporate reordered answer choices across repeated questions. If a model guesses correctly on one instance but incorrectly on another reordered variant, this indicates reliance on positional heuristics rather than genuine understanding. This methodology allows us to identify unreliable answering patterns and assess whether a model is robust to answer order changes.

This evaluation setup ensures that models are judged on their actual reasoning abilities, eliminating potential issues or biases caused by format variations or restrictive policies.

\subsubsection{Overall Accuracy Evaluation}

Each correctly answered question is assigned a score of one point, with all questions weighed equally regardless of their difficulty or complexity. The evaluation metric is based on accuracy, defined as:

\begin{equation}
\text{Accuracy} = \frac{\text{Number of Correct Answers}}{\text{Total Number of Questions}} \times 100\%
\end{equation}

Accuracy is a fundamental metric in evaluating multimodal large language models (MLLMs), as it provides a straightforward measure of a model's performance across various tasks. For instance, the MME benchmark employs accuracy to assess both perception and cognition abilities of MLLMs across multiple subtasks \cite{MME2023}. Similarly, the MM-BigBench framework utilizes accuracy to evaluate model performance on a wide range of multimodal content comprehension tasks \cite{MMBigBench2023}.

\subsubsection{Rejection Accuracy Evaluation}

In addition to overall accuracy, we evaluate each model’s ability to recognize when no correct answer is available. Rejection accuracy measures how well a model correctly selects the ``None of the provided options" choice when no valid answer is present. This is a critical aspect of uncertainty calibration, ensuring that models do not make incorrect guesses when faced with unanswerable questions.

Rejection accuracy is computed as:

\begin{equation}
\text{Rejection Accuracy} = \frac{\text{Number of Correct Rejections}}{\text{Total Rejection Questions}} \times 100\%
\end{equation}

where ``Number of Correct Rejections" represents the cases where the model correctly abstains from selecting an incorrect answer, and ``Total Rejection Questions" corresponds to a predefined subset of 40 unanswerable questions in the benchmark.

Evaluating a model's ability to handle unanswerable questions is essential for deploying reliable AI systems. While traditional accuracy metrics assess performance on answerable queries, incorporating rejection accuracy provides a more comprehensive evaluation framework, as it accounts for a model's uncertainty calibration and decision-making in ambiguous scenarios. Previous studies, such as MUIRBench \cite{wang2024muirbench}, the comprehensive evaluation framework in ChEF \cite{ChEF2023}, and the survey on multimodal LLM evaluation \cite{EvaluationSurvey2023}, highlight the importance of rejection accuracy in assessing model robustness and reliability.

\subsubsection{Abstention Rate Evaluation}

In addition to overall accuracy and rejection accuracy, we evaluate each model's tendency to select ``None of the provided options" across all questions. This metric, termed Abstention Rate, measures the proportion of times a model opts for ``None of the provided options", reflecting its inclination to abstain from answering.

\begin{equation}
\text{Abstention Rate} = 
\frac{
    \begin{array}{c}
    \text{Number of ``None of the} \\
    \text{provided options" Selections}
    \end{array}
}{
    \text{Total Questions}
} \times 100\%
\end{equation}


A higher abstention rate indicates a model's cautious approach, potentially avoiding incorrect answers when uncertain, while a lower rate suggests overconfidence or reluctance to admit uncertainty.

This metric complements traditional accuracy and rejection accuracy by offering insights into a model's uncertainty calibration. Previous studies have explored similar concepts, emphasizing the importance of abstention in enhancing reliability and safety in LLMs \cite{wen2024abstention, madhusudhan2024abstention}.



\subsubsection{Entropy-Based Reasoning Consistency Evaluation} 

To quantify reasoning stability, we utilize Entropy as a metric to evaluate a model's consistency in answering reordered variations of the same question. Entropy captures the uncertainty and variability in the model’s responses across these variants, providing a robust measure of reasoning consistency.

Entropy is calculated by grouping each original question with its reordered variants and analyzing the distribution of the model's responses. Specifically, the frequency of each answer option is computed and normalized to form a probability distribution while the correct answer is unchanged throughout all the question variants. Entropy is then calculated using the formula:

\begin{equation}
    H(Q_i) = - \sum_{j=1}^{k} p(a_j) \log_2(p(a_j))
    \label{eq:entropy}
\end{equation}

where \( H(Q_i) \) is the entropy for question group \( i \), \( k \) is the total number of possible answer options, and \( p(a_j) \) is the probability of selecting option \( a_j \) among the reordered variants.

The Mean Entropy for a model across all questions is computed as:

\begin{equation}
    \text{Mean Entropy}_m = \frac{1}{N} \sum_{i=1}^{N} H(Q_i)
    \label{eq:mean_entropy}
\end{equation}

where \( \text{Mean Entropy}_m \) represents the average reasoning consistency for model \( m \), and \( N \) is the total number of question groups. 

Lower entropy values indicate higher consistency, suggesting that the model consistently selects the same answer across reordered variants, reflecting stable reasoning. Conversely, higher entropy values suggest greater variability in answers, indicating potential instability or influence from positional biases rather than consistent reasoning patterns.

Previous studies have utilized entropy-based metrics to evaluate reasoning consistency and adaptive choice behavior in LLMs, such as in information selection for Chain-of-Thought prompting \cite{inform_entropy_reasoning} and in data compression proficiency assessments \cite{matrix_entropy_llm}. These approaches mainly focus on text generation tasks, where entropy is used to adjust dynamic temperature sampling or to assess information selection efficiency. However, these methods do not address reasoning consistency across reordered variants in multimodal visual reasoning. 

Our approach applies entropy to evaluate consistency specifically across reordered variants, which allows us to measure reasoning stability and detect positional biases that may influence model outputs. This offers a novel perspective on robustness and stability in multimodal LLMs, particularly in complex visual contexts where reasoning consistency is crucial for accurate interpretation. By assessing entropy across answer distributions for reordered variants, our method provides a more comprehensive understanding of reasoning patterns and positional biases compared to traditional accuracy metrics.


