\section{Discussion}\label{sec:discussion}

The evaluation of multimodal language models on this dataset reveals critical insights into their reasoning capabilities, robustness, and inherent biases. This section examines whether model size correlates with performance, the effectiveness of open-source models compared to proprietary ones, the restrictive nature of Qwen models, and the impact of reordered answers in detecting biases. Additionally, we discuss the performance of Grok 3 and DeepSeek's Janus. While formal scientific validation of Grok 3 and DeepSeek's Janus's advanced capabilities is still emerging, online discussions suggest significant anticipation for their performance. Our analysis reveals that Grok 3 and DeepSeek's Janus exhibited suboptimal performance.

\subsection{Does Model Size Correlate with Performance?}

A common misconception in the research community is larger models perform better due to increased capacity for processing and learning complex relationships. While this trend holds in many cases, our results suggest that size alone does not guarantee superior performance.

In this benchmark, the smallest models, Janus 7B and Janus 1B, significantly underperformed against their larger counterparts. Given their lower parameter count, this is expected, yet their frequent positional biases, and inconsistent reasoning indicate that their training or optimization was insufficient to compensate for their size limitations.

However, Grok 3, despite having the largest parameter count of 2.7 trillion, showcased low performance relative to its size, underperforming in tasks requiring complex reasoning and consistency. Its moderate rejection accuracy (0.525) indicate inconsistent reasoning stability, suggesting that sheer model size does not necessarily lead to more robust performance or consistent decision-making.

These findings underscore that while larger models generally have greater capacity, effective optimization and fine-tuning are critical for maximizing performance and consistency.


\subsection{Are Open-Source Models Competitive Against Proprietary Models?}

While open-source models offer transparency and adaptability, this benchmark demonstrates that proprietary models continue to hold a significant advantage in complex multimodal reasoning tasks. ChatGPT-o1 and Gemini 2.0 Flash outperformed all open-source models across most reasoning tasks, underscoring the benefits of high-quality fine-tuning and diverse pretraining data. In contrast, open-source models like Janus 1B and Janus 7B exhibited significant reasoning inconsistencies (i.e., hallucination) and biases, suggesting that limited access to high-quality data remains a major constraint for non-proprietary models. However, Pixtral 12B emerged as a surprising exception, performing well in specific tasks such as Cartoon Understanding, demonstrating that with targeted fine-tuning and optimization, open-source models can still be competitive in certain domains.


\subsection{The Restrictive Nature of Qwen Models}

One of the most unexpected findings was the high rate of unanswered or incorrectly answered questions from Qwen2.5-VL-72B-Instruct and QVQ-72B-Preview due to potential content restrictions. This issue was particularly evident in Cartoon Understanding, where the model struggled with half of the test cases despite the images being non-explicit or hazardous.

This suggests that Qwen’s content filtering mechanisms are overly aggressive, preventing it from engaging with conventional content. This restrictive nature severely limits Qwen's applicability in real-world multimodal reasoning, particularly in cases where understanding humor, memes, or non-literal content is necessary. A more balanced content moderation approach could improve its usability in diverse applications without compromising ethical safeguards.

\subsection{Grok 3's Unmet Expectations} 

Grok 3, currently in its Beta version, was positioned as a powerful contender to ChatGPT-o1, boasting an impressive 2.7 trillion parameters, the largest in this benchmark. Despite its scale and claims of advanced reasoning, Grok 3 fell short of expectations, achieving underwhelming results taking its size into consideration.

Additionally, Grok 3 exhibited an unusually high abstention rate, indicating a tendency to reject answers more frequently than average, even when correct options were available. This suggests an overly conservative approach to uncertainty, which undermined its decision-making effectiveness.

While it showed moderate success in specific tasks, Grok 3 failed to outperform ChatGPT-o1 or leverage its scale for superior reasoning stability. Overall, Grok 3, although it is still in its beta version, demonstrates that model size alone does not equate to improved accuracy or reasoning consistency.

\subsection{The Underperformance of DeepSeek Janus Models}

The release of DeepSeek’s Janus models was widely anticipated, particularly following the success of DeepSeek’s R1 model, which made significant strides in competing with ChatGPT-o1 despite utilizing far fewer resources \cite{guo2025deepseek}. This led to growing expectations that DeepSeek could emerge as a serious competitor to OpenAI across multiple AI domains. However, this benchmark demonstrates that such expectations do not hold up in visual reasoning tasks. 

Janus 7B and Janus 1B, the smallest models in this benchmark, also rank among the weakest performers, struggling significantly in numerous tasks, where they exhibit severe positional biases and incosistent reasoning. The accuracy evaluation shows that Janus models fail to generalize well across answer variations, reinforcing that their reasoning ability is not robust to minor changes in answer presentation.

While some expected DeepSeek Janus to be a strong open-source alternative to ChatGPT models, its current iteration suggests that DeepSeek’s approach does not yet scale effectively to multimodal reasoning. This indicates that DeepSeek’s advancements in language modeling with R1 do not yet extend effectively to multimodal reasoning, leaving the Janus models far behind proprietary competitors like ChatGPT-o1 in this domain.

\subsection{The Continued Dominance of ChatGPT Models}

While there has been significant development in multimodal models across different organizations, ChatGPT models continue to demonstrate an overwhelming advantage in visual reasoning tasks. ChatGPT-4o and ChatGPT-o1 outperformed every other model in almost all reasoning tasks, particularly excelling in Diagram Understanding, Image-Text Matching, and Visual Retrieval. The accuracy table further highlights that these models maintained stable performance across different variations of the same questions, indicating their robustness in reasoning and comprehension.

These models consistently provided the most stable and accurate responses, exhibiting the lowest rates of hallucination and positional bias. While some open-source models, such as Pixtral 12B, showed promising results in specific areas, none were able to compete across the board with OpenAI’s ChatGPT models. This result reinforces the idea that proprietary models still maintain a significant edge in multimodal visual reasoning, likely due to access to better training datasets, fine-tuning techniques, and alignment strategies that remain unavailable to the public. Future developments in open-source multimodal models will need to focus on improving robustness and reducing bias to close the gap with proprietary alternatives.

\subsection{The Contribution of Reordered Answers and Entropy in Detecting Biases}

One of the key contributions of this study was the inclusion of reordered answer variants and the application of entropy to detect positional biases and randomness in model responses. This combined approach exposed significant differences in reasoning stability across models, highlighting the extent to which certain models rely on positional heuristics and arbitrary answers rather than genuine comprehension.

By using entropy to quantify variability in answer distributions across reordered variants, we effectively measured reasoning consistency and stability. Models with high entropy scores, such as Janus 7B (0.8392) and Janus 1B (0.787), exhibited significant inconsistencies across reordered variants, suggesting reliance on positional heuristics or randomness rather than content-based reasoning. Their fluctuations indicate that answer selection was influenced by surface-level patterns rather than a stable understanding of the question, particularly in tasks requiring multi-image integration.

In contrast, ChatGPT-o1 (0.1352) and ChatGPT-4o (0.216) demonstrated the lowest entropy scores, indicating strong reasoning stability and resistance to positional biases. These models consistently selected the same answer across reordered variations, suggesting they engage in more content-driven decision-making. While low entropy does not inherently guarantee higher accuracy, most of the top-performing models exhibited lower entropy values, showing a strong correlation between reasoning stability and overall model performance.

These findings highlight the limitations of traditional VQA evaluations, which measure correctness but not reasoning stability. By combining reordered answer variants with entropy as a consistency metric, this study provides a more nuanced and robust assessment of reasoning stability. This dual approach distinguishes models that genuinely comprehend content from those that rely on answer positioning and randomness. Future multimodal benchmarks should incorporate both reordered answers and entropy-based consistency metrics to assess reasoning robustness more effectively and mitigate heuristic-driven decision-making.



\subsection{Avoidance of ``None of the Choices Provided"}

The majority of models demonstrated a consistent tendency to avoid selecting ``None of the choices provided," even when it was the correct answer. This pattern was most pronounced in Janus 7B and Janus 1B, which exhibited extremely low rejection accuracy and abstention rates. Their strong bias towards selecting an option, regardless of correctness, suggests an overcommitment to answers, likely influenced by pretraining datasets that emphasize choosing the best available choice.

In contrast, QVQ-72B-Preview and Grok 3 displayed the highest abstention rates, exceeding the 0.33 threshold for the proportion of questions where ``None of the provided options" was correct. This indicates a more conservative approach to decision-making, with a tendency to over-reject. QVQ-72B-Preview consistently identified unanswerable questions, while Grok 3 exhibited more variability, reflecting inconsistent uncertainty calibration.

ChatGPT-o1 and ChatGPT-4o maintained balanced rejection reasoning, selectively abstaining without excessive avoidance. Their abstention rates remained close to the 0.33 threshold, indicating well-calibrated uncertainty recognition and strategic decision-making.

These findings reveal distinct patterns in rejection behavior, highlighting that while some models consistently avoid selecting ``None of the choices provided," others exhibit over-rejection tendencies. The balanced approach observed in ChatGPT models underscores the importance of effective uncertainty calibration for strategic rejection-based reasoning.
