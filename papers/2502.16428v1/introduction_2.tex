\section{Introduction}


The rapid advancement of large language models (LLMs) has transformed artificial intelligence, enabling state-of-the-art performance in natural language processing (NLP), reasoning, and generative tasks \cite{brown2020language, chowdhery2022palm, touvron2023llama}. While early LLMs were primarily trained on text-based data, recent developments have led to the emergence of multimodal LLMs, capable of processing and reasoning over diverse data modalities, including text, images, videos, and structured information \cite{liu2023visual} \cite{chen2024mistral, openai2024gpt4o}. These models integrate vision and language understanding, significantly enhancing their applications across various domains, such as visual question answering (VQA) \cite{antol2015vqa, lu2022learn}, document comprehension \cite{mathew2021docvqa, huang2023layoutlm}, medical image interpretation \cite{wang2023biomedclip, mohta2023biogpt}, and multimodal dialogue systems \cite{shuster2021multi, zhu2024chat}.

Several multimodal models have demonstrated impressive capabilities in bridging vision and language understanding. $\mathbb{X}$AI's Grok 3 \cite{xai2025grok3} introduces advanced multimodal reasoning with a large parameter count, aiming to enhance contextual understanding and consistency. OpenAI’s GPT-4o \cite{openai2024gpt4o} extends its predecessor’s capabilities by incorporating image processing and reasoning over complex visual inputs. Google DeepMind’s Gemini 2.0 \cite{deepmind2024gemini} also advances multimodal interactions by integrating video and spatial reasoning. Meta’s LLaVA \cite{liu2023visual} aligns language and vision for improved visual grounding and generative capabilities, while Mistral's Pixtral 12B \cite{agrawal2024pixtral} introduces high-resolution image reasoning. In addition, open-source models such as DeepSeek-VL Janus \cite{deepseek2024vl} and Qwen-VL \cite{bai2023qwen} are pushing multimodal AI research forward, democratizing access to powerful vision-language models.

Evaluating multimodal LLMs remains a significant challenge, as existing benchmarks often assess isolated perception tasks rather than the complex reasoning required for real-world applications. While early datasets such as VQAv2 \cite{goyal2017making} and AI2D \cite{kembhavi2016diagram} focused on single-image understanding, recent benchmarks like NLVR2 \cite{suhr2019nlvr2}, MMMU \cite{yue2024mmmu}, and MathVista \cite{mathvista} have introduced multi-image tasks, logical comparisons, and mathematical reasoning. Moreover, MUIRBench \cite{wang2024muirbench} further advanced the field by integrating unanswerable question variants into multimodal visual reasoning. However, these efforts still fall short in systematically evaluating reasoning consistency, uncertainty calibration, and bias susceptibility. Addressing these gaps is crucial to ensuring that multimodal models move beyond pattern recognition and heuristic shortcuts to demonstrate genuine comprehension. 

 Unlike previous benchmarks, this study extends on the MUIRBench benchmark \cite{wang2024muirbench} and systematically evaluates overall reasoning capabilities, reasoning stability, rejection-based reasoning, and bias susceptibility by: (i) Reordering answer choices to assess whether models rely on heuristic-driven shortcuts rather than content understanding; (ii) introducing entropy as a novel metric to quantify reasoning stability and consistency across reordered variants, allowing for the detection of positional biases and randomness in answer selection; and (iii) testing rejection-based reasoning and abstention rate to measure whether models correctly abstain from answering when no valid option is provided.

The study evaluates multiple state-of-the-art multimodal LLMs, including Grok 3 \cite{xai2025grok3}, ChatGPT-o1 and ChatGPT-4o \cite{chatgpt}, Gemini 2.0 Flash Experimental \cite{gemini}, DeepSeek’s Janus models \cite{chen2025janus}, Pixtral 12B \cite{agrawal2024pixtral}, and Qwen-VL models \cite{bai2023qwen} to analyze how well-trending models generalize across these reasoning challenges. The results reveal notable discrepancies in performance. ChatGPT-4o and ChatGPT-o1 consistently achieve higher accuracy and reasoning stability, while Janus 7B and Janus 1B exhibit poor accuracy and high entropy scores, indicating significant reasoning variability and susceptibility to positional biases. This suggests that Janus models rely more on surface-level patterns rather than genuine comprehension, highlighting the importance of entropy as a consistency metric in identifying unstable reasoning.

The remainder of this paper is structured as follows. Section \ref{sec:related_work} reviews related work on multimodal benchmarks and evaluation methodologies. Section \ref{sec:benchmark_setup} describes the dataset used, the evaluated models, the experimental setup, and the evaluation framework, including the integration of new evaluation metrics based on reordered answers and entropy for reasoning consistency. Section \ref{sec:results} presents the results of the evaluation, highlighting key trends in accuracy, reasoning consistency, and rejection-based decision-making. Section \ref{sec:discussion} explores the broader implications of the findings, model-specific weaknesses, and improvements for multimodal AI evaluation. Finally, Section \ref{sec:conclusion} provides the conclusion and outlines future directions for refining multimodal benchmarking and reasoning assessment.
