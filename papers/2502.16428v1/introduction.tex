\section{Introduction}
\textcolor{red}{Nidhal, there's no difference between the introduction and related work. The context fits very well for the related work so we need to change the introduction to avoid redundancy}
Evaluating multimodal large language models (LLMs) has traditionally focused on single-image understanding, limiting assessments to isolated perception tasks. Benchmarks such as VQAv2 \cite{goyal2017making}, GQA \cite{hudson2019gqa}, and AI2D \cite{kembhavi2016diagram} primarily evaluate models on visual question answering (VQA) using a single image per question, without requiring cross-image reasoning. While these datasets effectively measure image-text alignment and basic perception, they do not assess deeper reasoning processes such as context integration, spatial relationships across images, or geographic understanding.

To address these limitations, more recent benchmarks have introduced multi-image reasoning tasks. NLVR2 \cite{suhr2019nlvr2} incorporated structured logical comparisons between two images, requiring models to determine whether a statement applies to one, both, or neither of the paired images. MMMU \cite{yue2024mmmu} and MathVista \cite{mathvista} expanded multimodal evaluation to complex mathematical and diagrammatic reasoning, incorporating multiple visual inputs. Datasets such as MINT \cite{wang2023mint} and BLINK \cite{fu2024blink} further advanced the field by testing temporal and spatial reasoning across sequential images. Despite these improvements, multi-image benchmarks remain limited in their ability to assess reasoning consistency, uncertainty calibration, and answer selection stability.

A crucial step forward came with MUIRBench \cite{wang2024muirbench}, which introduced an unanswerable question framework to evaluate whether models can recognize when no correct answer exists. However, while MUIRBench \cite{wang2024muirbench} evaluates rejection accuracy, it does not provide an expanded systematic analysis of models’ tendencies to avoid selecting "None of the choices provided" even when correct. This leaves a gap in understanding how models balance confidence, uncertainty, and rejection-based reasoning across different reasoning tasks. Other benchmarks, such as SEED-Bench \cite{li2023seedbench} and ChartQA \cite{masry2022chartqa}, incorporate confidence-based evaluation but do not test rejection-based reasoning within multi-image contexts. However, these datasets' ability to assess reasoning consistency and positional biases remains limited without integrating shuffled answers into their frameworks. 

Although datasets such as MMLU-Math \cite{mathmmlu2023} and ReClor \cite{yu2020reclor} reorder multiple-choice options to detect positional biases in textual reasoning and assess reasoning consistency, they typically focus on single-image tasks. Existing benchmarks do not systematically integrate reordered answer variations into multi-image reasoning tasks, leaving a gap in evaluating whether models rely on heuristics, random selection, or positional biases rather than genuine visual comprehension. Addressing these limitations is essential for developing benchmarks that rigorously test multimodal models' reasoning stability and ability to generalize across answer variations.


A comprehensive evaluation of multimodal visual reasoning should integrate three key criteria \textcolor{red}{please do not use \textit{techniques} when referring to the criteria we follow for assessment}: multi-image reasoning, unanswerable questions, and reordered answer variations. Multi-image reasoning assesses a model’s ability to synthesize information across multiple images rather than relying on isolated perception tasks. Unanswerable questions measure whether models recognize when no correct answer exists, an essential test of uncertainty calibration and rejection-based reasoning. Reordered answer variations test reasoning consistency and positional biases by reshuffling answer choices to determine if a model's selection is influenced by structure rather than content. While some datasets introduce multi-image tasks with unanswerable scenarios, such as MUIRBench \cite{wang2024muirbench}, and others assess positional biases through answer reordering, such as ReClor \cite{yu2020reclor} and Mathvista \cite{mathvista}, no benchmark systematically integrates all three techniques. This limits the ability to diagnose model weaknesses comprehensively and prevents a deeper understanding of whether models rely on structural cues, randomness, or genuine reasoning.


This study introduces a benchmark that simultaneously integrates multi-image reasoning, unanswerable questions, and reordered answer variations into a unified evaluation framework. Unlike previous benchmarks, this methodology systematically evaluates overall reasoning capabilities, reasoning stability, rejection-based reasoning, and bias susceptibility by: Reordering answer choices to assess whether models rely on heuristic-driven shortcuts rather than content understanding. Testing rejection-based reasoning to measure whether models correctly abstain from answering when no valid option is provided. Assessing multimodal LLMs across diverse multi-image reasoning tasks, including geographic understanding, visual retrieval, and diagram interpretation, which remain underexplored in existing benchmarks.

The study evaluates multiple state-of-the-art multimodal LLMs, including ChatGPT-o1 and ChatGPT-4o \cite{chatgpt}, Gemini 2.0 Flash Experimental \cite{gemini}, DeepSeek’s Janus models \cite{chen2025janus}, and Pixtral 12B \cite{agrawal2024pixtral} to analyze how well trending models generalize across these reasoning challenges. The results reveal notable discrepancies in performance. ChatGPT-4o and ChatGPT-o1 consistently achieve higher accuracy and reasoning stability, while Janus 7B and Janus 1B exhibit poor accuracy and susceptibility to positional biases, indicating a reliance on surface-level patterns rather than deep comprehension.

The remainder of this paper is structured as follows. Section \ref{sec:related_work} reviews related work on multimodal benchmarks and evaluation methodologies. Section \ref{sec:benchmark_setup} describes the dataset used, the evaluated models, the experimental setup, and the evaluation framework, including the integration of new evaluation metrics based on reordered answers and rejection accuracy. Section \ref{sec:results} presents the results of the evaluation, highlighting key trends in accuracy, reasoning consistency, and rejection-based decision-making. Section \ref{sec:discussion} explores the broader implications of the findings, model-specific weaknesses, and improvements for multimodal AI evaluation. Finally, Section \ref{sec:conclusion} provides the conclusion and outlines future directions for refining multimodal benchmarking and reasoning assessment.

