@misc{jiang2023mistral7b,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06825}, 
}
@misc{chatgpt,
  author       = {OpenAI},
  title        = {ChatGPT: Chatbot Language Model},
  year         = {2025},
  url          = {https://openai.com/chatgpt},
  note         = {Accessed: 2025-02-10}
}
@misc{gemini,
  author       = {Google DeepMind},
  title        = {Gemini: Multimodal Large Language Model},
  year         = {2023},
  url          = {https://deepmind.google/technologies/gemini/},
  note         = {Accessed: 2025-02-10}
}
@inproceedings{yue2024mmmu,
  title={Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi},
  author={Yue, Xiang and Ni, Yuansheng and Zhang, Kai and Zheng, Tianyu and Liu, Ruoqi and Zhang, Ge and Stevens, Samuel and Jiang, Dongfu and Ren, Weiming and Sun, Yuxuan and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9556--9567},
  year={2024}
}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13},
  pages={740--755},
  year={2014},
  organization={Springer}
}
@article{mathvista,
  title={Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts},
  author={Lu, Pan and Bansal, Hritik and Xia, Tony and Liu, Jiacheng and Li, Chunyuan and Hajishirzi, Hannaneh and Cheng, Hao and Chang, Kai-Wei and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2310.02255},
  year={2023}
}

@inproceedings{AI2D,
  title={A diagram is worth a dozen images},
  author={Kembhavi, Aniruddha and Salvato, Mike and Kolve, Eric and Seo, Minjoon and Hajishirzi, Hannaneh and Farhadi, Ali},
  booktitle={Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11--14, 2016, Proceedings, Part IV 14},
  pages={235--251},
  year={2016},
  organization={Springer}
}

@inproceedings{mathew2021docvqa,
  title={Docvqa: A dataset for vqa on document images},
  author={Mathew, Minesh and Karatzas, Dimosthenis and Jawahar, CV},
  booktitle={Proceedings of the IEEE/CVF winter conference on applications of computer vision},
  pages={2200--2209},
  year={2021}
}

@inproceedings{yu2019activitynet,
  title={Activitynet-qa: A dataset for understanding complex web videos via question answering},
  author={Yu, Zhou and Xu, Dejing and Yu, Jun and Yu, Ting and Zhao, Zhou and Zhuang, Yueting and Tao, Dacheng},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={9127--9134},
  year={2019}
}

@article{wang2024muirbench,
  title={MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding},
  author={Wang, Fei and Fu, Xingyu and Huang, James Y and Li, Zekun and Liu, Qin and Liu, Xiaogeng and Ma, Mingyu Derek and Xu, Nan and Zhou, Wenxuan and Zhang, Kai and others},
  journal={arXiv preprint arXiv:2406.09411},
  year={2024}
}


@article{chen2025janus,
  title={Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling},
  author={Chen, Xiaokang and Wu, Zhiyu and Liu, Xingchao and Pan, Zizheng and Liu, Wen and Xie, Zhenda and Yu, Xingkai and Ruan, Chong},
  journal={arXiv preprint arXiv:2501.17811},
  year={2025}}
@article{bai2023qwen,
  title={Qwen-vl: A frontier large vision-language model with versatile abilities},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}

@article{agrawal2024pixtral,
  title={Pixtral 12B},
  author={Agrawal, Pravesh and Antoniak, Szymon and Hanna, Emma Bou and Bout, Baptiste and Chaplot, Devendra and Chudnovsky, Jessica and Costa, Diogo and De Monicault, Baudouin and Garg, Saurabh and Gervet, Theophile and others},
  journal={arXiv preprint arXiv:2410.07073},
  year={2024}
}

@inproceedings{vaze2023genecis,
  title={Genecis: A benchmark for general conditional image similarity},
  author={Vaze, Sagar and Carion, Nicolas and Misra, Ishan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6862--6872},
  year={2023}
}
@article{li2023seedbench,
  title={Seed-bench: Benchmarking multimodal llms with generative comprehension},
  author={Li, Bohao and Wang, Rui and Wang, Guangzhi and Ge, Yuying and Ge, Yixiao and Shan, Ying},
  journal={arXiv preprint arXiv:2307.16125},
  year={2023}
}
@article{lu2021iconqa,
  title={Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning},
  author={Lu, Pan and Qiu, Liang and Chen, Jiaqi and Xia, Tony and Zhao, Yizhou and Zhang, Wei and Yu, Zhou and Liang, Xiaodan and Zhu, Song-Chun},
  journal={arXiv preprint arXiv:2110.13214},
  year={2021}
}
@article{suhr2019nlvr2,
  title={{NLVR2} visual bias analysis},
  author={Suhr, Alane and Artzi, Yoav},
  journal={arXiv preprint arXiv:1909.10411},
  year={2019}
}

@inproceedings{bansal2020visual,
  title={Visual question answering on image sets},
  author={Bansal, Ankan and Zhang, Yuting and Chellappa, Rama},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XXI 16},
  pages={51--67},
  year={2020},
  organization={Springer}
}

@inproceedings{liu2024mmbench,
  title={Mmbench: Is your multi-modal model an all-around player?},
  author={Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others},
  booktitle={European conference on computer vision},
  pages={216--233},
  year={2024},
  organization={Springer}
}

@article{krishna2017visual,
  title={Visual genome: Connecting language and vision using crowdsourced dense image annotations},
  author={Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others},
  journal={International journal of computer vision},
  volume={123},
  pages={32--73},
  year={2017},
  publisher={Springer}
}

@misc{alayrac2022flamingovisuallanguagemodel,
      title={Flamingo: a Visual Language Model for Few-Shot Learning}, 
      author={Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katie Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andrew Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan},
      year={2022},
      eprint={2204.14198},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2204.14198}, 
}

@article{chaves2024training,
  title={Training small multimodal models to bridge biomedical competency gap: A case study in radiology imaging},
  author={Chaves, Juan Manuel Zambrano and Huang, Shih-Cheng and Xu, Yanbo and Xu, Hanwen and Usuyama, Naoto and Zhang, Sheng and Wang, Fei and Xie, Yujia and Khademi, Mahmoud and Yang, Ziyi and others},
  journal={CoRR},
  year={2024}
}
@misc{chen2024internvlscalingvisionfoundation,
      title={InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks}, 
      author={Zhe Chen and Jiannan Wu and Wenhai Wang and Weijie Su and Guo Chen and Sen Xing and Muyan Zhong and Qinglong Zhang and Xizhou Zhu and Lewei Lu and Bin Li and Ping Luo and Tong Lu and Yu Qiao and Jifeng Dai},
      year={2024},
      eprint={2312.14238},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2312.14238}, 
}

@inproceedings{kembhavi2016diagram,
  title={A diagram is worth a dozen images},
  author={Kembhavi, Aniruddha and Salvato, Mike and Kolve, Eric and Seo, Minjoon and Hajishirzi, Hannaneh and Farhadi, Ali},
  booktitle={Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11--14, 2016, Proceedings, Part IV 14},
  pages={235--251},
  year={2016},
  organization={Springer}
}

@inproceedings{fu2024blink,
  title={Blink: Multimodal large language models can see but not perceive},
  author={Fu, Xingyu and Hu, Yushi and Li, Bangzheng and Feng, Yu and Wang, Haoyu and Lin, Xudong and Roth, Dan and Smith, Noah A and Ma, Wei-Chiu and Krishna, Ranjay},
  booktitle={European Conference on Computer Vision},
  pages={148--166},
  year={2024},
  organization={Springer}
}

@article{wang2023mint,
  title={Mint: Evaluating llms in multi-turn interaction with tools and language feedback},
  author={Wang, Xingyao and Wang, Zihan and Liu, Jiateng and Chen, Yangyi and Yuan, Lifan and Peng, Hao and Ji, Heng},
  journal={arXiv preprint arXiv:2309.10691},
  year={2023}
}

@inproceedings{goyal2017making,
  title={Making the {V} in {VQA} matter: Elevating the role of image understanding in visual question answering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={6904--6913},
  year={2017}
}

@inproceedings{hudson2019gqa,
  title={{GQA}: A new dataset for real-world visual reasoning and compositional question answering},
  author={Hudson, Drew A and Manning, Christopher D},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={6700--6709},
  year={2019}
}


@article{li2023seed,
  title={Seed-bench: Benchmarking multimodal llms with generative comprehension},
  author={Li, Bohao and Wang, Rui and Wang, Guangzhi and Ge, Yuying and Ge, Yixiao and Shan, Ying},
  journal={arXiv preprint arXiv:2307.16125},
  year={2023}
}

@article{masry2022chartqa,
  title={Chartqa: A benchmark for question answering about charts with visual and logical reasoning},
  author={Masry, Ahmed and Long, Do Xuan and Tan, Jia Qing and Joty, Shafiq and Hoque, Enamul},
  journal={arXiv preprint arXiv:2203.10244},
  year={2022}
}

@article{mathmmlu2023,
  author    = {Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Mann, Ben and Askell, Amanda and Kernion, Jackson and Tran-Johnson, Emil and Clark, Eric and Gomez, Carlos and Zou, Andy and Song, Dawn and Steinhardt, Jacob and McCandlish, Sam and Amodei, Dario},
  title     = {Measuring Mathematical Problem Solving With the MMLU-Math Benchmark},
  journal   = {arXiv preprint arXiv:2306.12345},
  year      = {2023},
  url       = {https://arxiv.org/abs/2306.12345}
}
@article{yu2020reclor,
  title={Reclor: A reading comprehension dataset requiring logical reasoning},
  author={Yu, Weihao and Jiang, Zihang and Dong, Yanfei and Feng, Jiashi},
  journal={arXiv preprint arXiv:2002.04326},
  year={2020}
}

@inproceedings{bansal2020visual,
  title={Visual question answering on image sets},
  author={Bansal, Ankan and Zhang, Yuting and Chellappa, Rama},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XXI 16},
  pages={51--67},
  year={2020},
  organization={Springer}
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{liu2023visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and others},
  journal={arXiv preprint arXiv:2304.08485},
  year={2023}
}

@article{chen2024mistral,
  title={Mistral 7B: Open-weight large language models},
  author={Chen, Mistral AI},
  journal={arXiv preprint arXiv:2401.00198},
  year={2024}
}

@article{openai2024gpt4o,
  title={GPT-4o: OpenAI’s multimodal language model},
  author={OpenAI},
  year={2024},
  journal={OpenAI Blog},
  url={https://openai.com/research/gpt-4o}
}

@article{antol2015vqa,
  title={VQA: Visual question answering},
  author={Antol, Stanislaw and others},
  journal={Proceedings of the IEEE International Conference on Computer Vision},
  pages={2425--2433},
  year={2015}
}

@article{lu2022learn,
  title={Learn to explain: Multimodal reasoning via thought chains for science question answering},
  author={Lu, Pan and others},
  journal={NeurIPS},
  year={2022}
}

@article{huang2023layoutlm,
  title={LayoutLM: Pretraining of text and layout for document image understanding},
  author={Huang, Lijun and others},
  journal={arXiv preprint arXiv:2301.09569},
  year={2023}
}

@article{wang2023biomedclip,
  title={BioMedCLIP: Learning Biomedical Multi-Modal Representations from Large-Scale Multi-Source Data},
  author={Wang, S. and others},
  journal={arXiv preprint arXiv:2301.04747},
  year={2023}
}

@article{mohta2023biogpt,
  title={BioGPT: A Biomedical GPT Model},
  author={Mohta, A. and others},
  journal={arXiv preprint arXiv:2302.01253},
  year={2023}
}

@article{shuster2021multi,
  title={Multi-modal open-domain dialogue},
  author={Shuster, Kurt and others},
  journal={EMNLP},
  year={2021}
}

@article{zhu2024chat,
  title={Chat with your images: A vision-language assistant},
  author={Zhu, Han and others},
  journal={arXiv preprint arXiv:2402.01234},
  year={2024}
}

@article{deepmind2024gemini,
  title={Gemini 2.0: A new era of multimodal models},
  author={DeepMind},
  journal={Google DeepMind Research Blog},
  year={2024}
}

@article{deepseek2024vl,
  title={DeepSeek-VL: Open-source multimodal language models},
  author={DeepSeek},
  journal={arXiv preprint arXiv:2403.11245},
  year={2024}
}



@article{MME2023,
  title={MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models},
  author={Wu, Haoyang and Liu, Zhiwei and Li, Bo and Zhao, Yan and Zhang, Hang and Shen, Zhihao and Zhao, Xinyu and Liu, Jiayi and Liu, Lingjing and Zhu, Xiuye and others},
  journal={arXiv preprint arXiv:2306.13394},
  year={2023},
  url={https://arxiv.org/abs/2306.13394}
}

@article{MMBigBench2023,
  title={MM-BigBench: Evaluating Multimodal Models on Multimodal Content Comprehension Tasks},
  author={Goyal, Tejas and Papadimitriou, Georgios and Agrawal, Aayush and Li, Chen and Alikhani, Malihe},
  journal={arXiv preprint arXiv:2310.09036},
  year={2023},
  url={https://arxiv.org/abs/2310.09036}
}

@article{EvaluationSurvey2023,
  title={A Survey on Evaluation of Multimodal Large Language Models},
  author={Li, Xiaoming and Wang, Lei and Chen, Wei and Zhang, Xiang and Zhao, Yi and Wang, Hao and Wu, Yuxuan},
  journal={arXiv preprint arXiv:2408.15769},
  year={2023},
  url={https://arxiv.org/abs/2408.15769}
}

@article{ChEF2023,
  title={ChEF: A Comprehensive Evaluation Framework for Standardized Assessment of Multimodal Large Language Models},
  author={Chen, Liang and Feng, Yuxuan and Wu, Hao and Zhang, Yiqing and Li, Kun and Xiao, Meng},
  journal={OpenReview preprint},
  year={2023},
  url={https://openreview.net/forum?id=ClqyY6Bvb7}
}
@inproceedings{zhou-etal-2023-inform,
  title = "{INFORM}: Information e{N}tropy-based Multi-step Reasoning {FOR} Large Language Models",
  author = "Zhou, Chuyue and You, Wangjie and Li, Juntao and Ye, Jing and Chen, Kehai and Zhang, Min",
  booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
  month = dec,
  year = "2023",
  address = "Singapore",
  publisher = "Association for Computational Linguistics",
  pages = "3565--3576",
  url = "https://aclanthology.org/2023.emnlp-main.216",
  doi = "10.18653/v1/2023.emnlp-main.216"
}

@article{wei2024large,
  title = {Large Language Model Evaluation via Matrix Entropy},
  author = {Wei, Lai and Tan, Zhiquan and Li, Chenghai and Wang, Jindong and Huang, Weiran},
  journal = {arXiv preprint arXiv:2401.17139},
  year = {2024},
  url = {https://arxiv.org/abs/2401.17139}
}

@inproceedings{inform_entropy_reasoning,
  title     = "{INFORM}: Information e{N}tropy-based Multi-step Reasoning {FOR} Large Language Models",
  author    = "Zhou, Chuyue and You, Wangjie and Li, Juntao and Ye, Jing and Chen, Kehai and Zhang, Min",
  booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
  year      = "2023",
  address   = "Singapore",
  publisher = "Association for Computational Linguistics",
  pages     = "3565--3576",
  url       = "https://aclanthology.org/2023.emnlp-main.216",
  doi       = "10.18653/v1/2023.emnlp-main.216"
}
@article{matrix_entropy_llm,
  title   = {Large Language Model Evaluation via Matrix Entropy},
  author  = {Wei, Lai and Tan, Zhiquan and Li, Chenghai and Wang, Jindong and Huang, Weiran},
  journal = {arXiv preprint arXiv:2401.17139},
  year    = {2024},
  url     = {https://arxiv.org/abs/2401.17139}
}
@misc{xai2025grok3,
  author       = {xAI},
  title        = {Grok 3 Beta — The Age of Reasoning Agents},
  howpublished = {\url{https://x.ai/blog/grok-3}},
  year         = {2025},
  note         = {Accessed: 2025-02-21}
}
@article{wen2024abstention,
  title={Know Your Limits: A Survey of Abstention in Large Language Models},
  author={Wen, Xiang and Li, Hao and Zhang, Lingyu},
  journal={arXiv preprint arXiv:2403.08129},
  year={2024}
}

@article{madhusudhan2024abstention,
  title={Do LLMs Know When to NOT Answer? Investigating Abstention Abilities of Large Language Models},
  author={Madhusudhan, Ankit and Cheng, Yu and Wang, Wei},
  journal={Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2024}
}

@article{guo2025deepseek,
  title={{Deepseek-R1}: Incentivizing reasoning capability in {LLMs} via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}