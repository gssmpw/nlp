\begin{abstract}
% Evaluating multimodal large language models (LLMs) has traditionally focused on single-image reasoning, limiting assessments of contextual understanding, reasoning stability, and uncertainty calibration. This study advances the evaluation of multimodal LLMs by integrating multi-image reasoning, rejection-based reasoning, and answer positional bias detection. We assess Grok 3, ChatGPT-4o, ChatGPT-o1, Gemini 2.0 Flash Experimental, DeepSeek’s Janus models, Qwen2.5-VL-72B-Instruct, QVQ-72B-Preview, and Pixtral 12B across eight reasoning tasks, such as difference spotting and diagram interpretation. ChatGPT-o1 leads with 82.5\% accuracy and 70.0\% rejection accuracy, followed by Gemini 2.0 Flash Experimental (70.8\%). QVQ-72B-Preview excels in rejection accuracy (85.5\%), while Pixtral 12B (51.7\%) shows promise in domain-specific tasks. In contrast, Janus 7B (43.3\%) and Janus 1B (38.3\%) struggle with bias and uncertainty calibration, with rejection accuracies of 15.0\% and 5.0\%, respectively. To measure reasoning consistency and detect positional biases, we introduce reordered answer variants and entropy as a novel metric for evaluating response variability across reordered answer variants. Models with high entropy scores, such as Janus 7B (0.8392) and Janus 1B (0.787), exhibited significant inconsistencies, indicating susceptibility to positional heuristics and unstable reasoning patterns, while ChatGPT-o1 (0.1352) and ChatGPT-4o (0.216) demonstrated the lowest entropy values, reflecting greater consistency and content-driven decision-making. These results confirm the dominance of proprietary models but highlight the potential for open-source models to close the gap with better fine-tuning. Additionally, it showcases the underperformance of Grok 3 given its number of parameters. By implementing multi-image contexts, rejection mechanisms, reordered answer variants, and entropy-based reasoning consistency metrics, this benchmark sets a new standard for evaluating multimodal LLMs, ensuring more robust, generalizable, and reliable AI systems.

Traditional evaluations of multimodal large language models (LLMs) have been limited by their focus on single-image reasoning, failing to assess crucial aspects like contextual understanding, reasoning stability, and uncertainty calibration.  This study addresses these limitations by introducing a novel benchmark that uniquely integrates multi-image reasoning tasks with rejection-based evaluation and positional bias detection. To rigorously evaluate these dimensions, we further introduce entropy as a novel metric for quantifying reasoning consistency across reordered answer variants. We applied this innovative benchmark to assess Grok 3, ChatGPT-4o, ChatGPT-o1, Gemini 2.0 Flash Experimental, DeepSeek’s Janus models, Qwen2.5-VL-72B-Instruct, QVQ-72B-Preview, and Pixtral 12B across eight visual reasoning tasks, including difference spotting and diagram interpretation. Our findings reveal ChatGPT-o1 leading in overall accuracy (82.5\%) and rejection accuracy (70.0\%), closely followed by Gemini 2.0 Flash Experimental (70.8\%). QVQ-72B-Preview demonstrated superior rejection accuracy (85.5\%). Notably, Pixtral 12B (51.7\%) showed promise in specific domains, while Janus models exhibited challenges in bias and uncertainty calibration, reflected in low rejection accuracies and high entropy scores.  High entropy scores in Janus models (Janus 7B: 0.8392, Janus 1B: 0.787) underscore their susceptibility to positional bias and unstable reasoning, contrasting with the low entropy and robust reasoning of ChatGPT models.  The study further demonstrates that model size is not the sole determinant of performance, as evidenced by Grok 3's underperformance despite its substantial parameter count. By employing multi-image contexts, rejection mechanisms, and entropy-based consistency metrics, this benchmark sets a new standard for evaluating multimodal LLMs, enabling a more robust and reliable assessment of next-generation AI systems.














\end{abstract}

\begin{IEEEkeywords}
Grok 3, Deepseek, Janus, Gemini 2.0 Flash, ChatGPT-4o, ChatGPT-o1, Mistral, Benchmark, Visual Reasoning, Evaluation, 
\end{IEEEkeywords}

