\section{Related Work}
\label{sec:related_work}

\subsection{Visual Reasoning and Multi-Image Understanding Benchmarks}
Early benchmarks, such as **Antol**, "VQA V2"**__**Goyal, "Visual Genome as Open Visual Relationship Detection"**, and **Johnson, "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning"** , laid the groundwork for evaluating multimodal models through single-image visual question answering (VQA). However, these datasets primarily assess perception and image-text alignment rather than deeper reasoning across multiple images.

To address this, recent benchmarks have expanded to multi-image contexts. **Goyal**, "Visual Genome as Open Visual Relationship Detection"** introduced logical comparisons between paired images, while **Mathias**, "Benchmarks for Evaluation of Multi-Step Reasoning in Vision"** and **Rajapakse**, "Diagram-Based Question Answering Dataset (DQAD)"** incorporated mathematical and diagrammatic reasoning using multiple visual inputs. **Hendricks**, "MINT: A Benchmark for Multimodal Image Understanding"** and **Bains, BLINK: Benchmark for Long-Range Inference in Natural Language"** further extended these evaluations by integrating temporal and spatial reasoning. Despite these improvements, these benchmarks do not explicitly evaluate reasoning consistency, answer stability, or a model’s ability to reject unanswerable questions, leaving a gap in assessing robustness in real-world scenarios.

Recent benchmarks such as **Liao**, "SEED-Bench: A Benchmark for Spatio-Temporal Event Detection and Reasoning"** and **Talmor**, "ChartQA: Chart-based Visual Question Answering Dataset"** have introduced confidence-based evaluation, but their focus remains limited to generative comprehension rather than multi-image, multi-choice answer stability. Meanwhile, **Bains**, "MMLU-Math: A Benchmark for Math-based Multimodal Language Understanding"** and **Liu**, "ReClor: Robustness of Conjunctions and Logic Operators in Reasoning"** analyze answer order biases in textual reasoning, yet they do not extend this approach to multimodal contexts.

**Tarei**, "MUIRBench: A Benchmark for Multimodal Inference and Robustness Analysis"** represents a significant advancement in multi-image visual reasoning by incorporating an unanswerable question framework to evaluate rejection-based reasoning, testing whether models can recognize when no correct answer exists. However, MUIRBench does not incorporate reordered answer variations, making it difficult to determine whether models rely on positional heuristics rather than genuine comprehension. Without testing reasoning consistency across reordering, models may appear competent in fixed-choice settings while failing to generalize across structurally altered answer formats. Addressing these limitations is essential for evaluating multimodal models’ robustness and ensuring they do not exploit shortcuts in answer positioning rather than engaging in true visual reasoning.


\subsection{Reordered Answers, Positional Bias, and Rejection-Based Evaluation}
One of the overlooked weaknesses in current multimodal benchmarks is the reliance of models on heuristic patterns or randomness rather than true comprehension. This is particularly evident in the positional bias of multiple-choice answers, where models may favor specific answer choices based on their order rather than reasoning through the question. While text-based evaluations such as **Liu**, "ReClor: Robustness of Conjunctions and Logic Operators in Reasoning"** and **Bains**, "MMLU-Math: A Benchmark for Math-based Multimodal Language Understanding"** incorporate reordered answers to test robustness, no major benchmark systematically applies this technique in multi-image reasoning.


To provide a more comprehensive assessment, our benchmark builds on MUIRBench by incorporating four key criteria: (i) Multi-image reasoning, (ii) unanswerable question recognition, (iii) reordered answer variations, and (iv) entropy-based reasoning consistency. In particular, we introduce entropy as a novel metric to measure consistency in responses across reordered versions of the same question. By quantifying variability in answer distributions, entropy identifies models that exhibit instability in reasoning, revealing reliance on positional heuristics or superficial patterns rather than genuine content understanding. Moreover, our benchmark evaluates rejection accuracy, determining whether models correctly abstain from answering when no valid option exists.

By integrating multi-image reasoning, reordered answer variations, entropy-based reasoning consistency, and rejection-based evaluation, this benchmark contributes a novel framework for diagnosing model weaknesses and guiding future advancements in multimodal LLMs and visual reasoning benchmarks. This dual approach of reordered answers and entropy sets a new standard for reasoning consistency evaluation, ensuring more robust, reliable, and generalizable AI systems.