\documentclass{article}


\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}
\graphicspath{ {./images/} }
%\usepackage{natbib}
\usepackage{array}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{float}  % Add this to your preamble
\usepackage[numbers]{natbib}

\usepackage{caption}
\usepackage[table]{xcolor}

% 为显著性星号定义符号
\newcommand{\sym}[1]{\ensuremath{^{#1}}}

\usepackage{tcolorbox}
\tcbuselibrary{skins}
\tcbuselibrary{breakable}
\usepackage{listings}
\usepackage{xcolor}



% Define a custom box style
\tcbset{
    mybox/.style={
        enhanced,
        colback=gray!5,
        colframe=gray!50!black,
        fonttitle=\bfseries,
        boxrule=0.5pt,
        arc=2pt,
        left=5pt,
        right=5pt,
        top=5pt,
        bottom=5pt,
    }
}


\title{Old experience helps: Leveraging Survey Methodology to Improve AI Text Annotation Reliability in Social Sciences}


\author{
 Linzhuo Li \\
  Zhejiang University\\
  Hanzhou, Zhejiang 310058 \\
  \texttt{linzhuoli@zju.edu.cn} \\
  %% examples of more authors
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}

\maketitle
\begin{abstract}
This paper introduces a framework for assessing the reliability of Large Language Model (LLM) text annotations in social science research by adapting established survey methodology principles. Drawing parallels between survey respondent behavior and LLM outputs, the study implements three key interventions: option randomization, position randomization, and reverse validation. While traditional accuracy metrics may mask model instabilities, particularly in edge cases, this framework provides a more comprehensive reliability assessment. Using the F1000 dataset in biomedical science and three sizes of Llama models (8B, 70B, and 405B parameters), the paper demonstrates that these survey-inspired interventions can effectively identify unreliable annotations that might otherwise go undetected through accuracy metrics alone. The results show that 5-25\% of LLM annotations change under these interventions, with larger models exhibiting greater stability. Notably, for rare categories approximately 50\% of "correct" annotations demonstrate low reliability when subjected to this framework. The paper introduce an information-theoretic reliability score (R-score) based on Kullback-Leibler divergence that quantifies annotation confidence and distinguishes between random guessing and meaningful annotations at the case level. This approach complements existing expert validation methods by providing a scalable way to assess internal annotation reliability and offers practical guidance for prompt design and downstream analysis. 
\end{abstract}


% keywords can be removed
\keywords{AI \and Large Language Model \and Text Annotation \and Reliablity \and Survey Research}


\section{Introduction}
Artificial intelligence has increasingly become the fundamental infrastructure for research (\citep{grossmann2023ai, bail2024can}). Particularly, Large Language Models (LLMs) emerged as powerful tools for large-scale text annotation in many research settings in social sciences (\citep{tan2024large}, \citep{yan2023cipta}). Although many empirical studies demonstrate LLMs' remarkable "out-of-the-box" annotation capabilities, increasing concerns also emerge regarding the reliability of these annotations (\citep{palmer2024using}, \citep{rytting2023towards}, \citep{zhang2023safetybench}, \citep{tornberg2024best}, \citep{ziems2024can}, \citep{dentella2023systematic}, \citep{adewumi2024limitations}. This reliability question is particularly crucial for social science researchers who rely on LLM-annotated variables for downstream statistical analyses or as training data for supervised models, as reliability may substantially impact subsequent coefficient estimates, confidence intervals, or model performance. Thus, social scientists need to find ways to draw the boundary between "effective annotations" and "ineffective annotations" in their daily use. 

Traditional approaches to validating annotation reliability have primarily relied on expert verification. However, this expensive and time-consuming method faces significant scaling challenges in the era of big data and LLMs. The very appeal of LLMs lies in their ability to analyze subtle dimensions (cultural, psychological, social, political, knowledge-based, etc.) across massive datasets - a scale that makes comprehensive expert verification impractical.

This raises a critical question: how can researchers develop reliability measures that match the boundless possibilities LLMs offer while maintaining rigorous standards? Is it possible to construct a reasonable boundary  for assessing result credibility before resorting to costly expert verification? This paper addresses these questions by drawing insights from survey methodology - a field with decades of experience in handling response reliability issues.

\subsection{The Challenge of LLM Annotation Reliability}

The dominant approach to assessing the reliability of the LLM annotation is mainly based on human expert verification. Although valuable, this ``external" method adopts a somewhat agnostic perspective, disregarding the underlying mechanisms of how LLMs generate responses. This limitation becomes particularly apparent when facing the exponential growth in both data volume and annotation dimensions. Critically, even when LLM annotations align with external validation benchmarks, this consistency may mask deeper reliability concerns analogous to “careless” or “undecidable” human respondents in surveys—instances where models exploit brittle shortcuts or superficial cues rather than genuinely engaging with the task’s substantive intent with its true preference. Such behavior undermines construct validity(\citep{strauss2009construct}), particularly when annotations inform latent variable measurement or causal inference in social science research.

In particular, the relatively low-frequency (or ``rare'') categories pose unique challenges for LLMs. Given that labels for model predictions follow power-law distributions, with a small number of dominant labels overshadowing many minority ones,  LLMs may differ in its reliability in annotating different cases. By fixating on accuracy at an aggregate level, researchers may overlook the amount of case-level unreliability, particularly in these rarely encountered categories. But these cases may have severely impact on downstream tasks when social scientists rely on the variation of these LLM-annotated variables to do hypothesis testing, causal inference, or supervised model training, undermining the overall results. 

The challenge also highlights a mismatch between conventional validation paradigms and researchers’ needs. Social scientists increasingly require case-level reliability metrics—granular indicators of whether an LLM’s annotation for a specific text-problem pair reflects meaningful reasoning versus stochastic or shortcut-driven outputs. Traditional model- or variable-level evaluations, while informative for aggregate performance, fail to address the heterogeneity of LLM reliability across individual cases.

Meanwhile, the external approach also ignores recent opportunities that leverage LLMs' probabilistic nature to examine token probability distributions and measures like hallucination indices. These methods acknowledge LLMs' probabilistic output nature (\citep{xue2024strengthened}, \citep{misiejuk2024augmenting}) and have identified many aspects of challenges of LLM annotation reliability (\citep{pangakis2023automated}, \citep{goral2024all}). Notably, some studies(\citep{bisbee2024synthetic}) have started to move beyond accuracy and noticed the reliability issue in a more general sense, yet still a well-established framework is currently lacking to assess the reliability problem both theoretically and empirically in a systematic way. This gap persists despite parallels to decades of survey methodology research, which has refined techniques to detect inattentive respondents, validate response consistency, and distinguish substantive engagement from brittle heuristic shortcuts.


\subsection{A Survey-inspired Framework}
While recent literature explores using LLMs to simulate human survey respondents (\citep{von2024vox, argyle2023out, bisbee2024synthetic, kozlowski2024silico}), this paper reverses the direction of knowledge transfer and argue that survey methodology’s rich tradition of diagnosing and improving response reliability offers a framework for addressing LLM annotation challenges. This paper proposes such a framework that draws largely from established survey design principles to help assess LLM annotation effectiveness. The survey design methodology offers both theoretical interpretations of various respondent behaviors and corresponding strategies to evaluate response quality beyond simple accuracy metrics, addressing issues such as response consistency, attention checks, and response patterns.

The proposed framework offers three key advantages:

1. Case-level assessment: Unlike model-level or variable-level evaluations, this framework provides fine-grained information about LLMs' ability to annotate specific cases effectively. This allows researchers to screen for question-case pairs where LLM annotations are more reliable and incorporate reliability measures into downstream analyses.

2. Complementarity with expert validation: Rather than replacing expert validation, our framework provides additional information that can guide and enhance the validation process by detecting “shortcut-driven” annotations that pass external validation but lack construct validity.

3. Prompt design guidance: The framework can help social scientists construct more effective prompts for their annotation tasks —mirroring survey pretesting practices—to mitigate ambiguities or unintended cueing that lead LLMs toward brittle reasoning ``circuit" paths.

Building upon these survey-inspired interventions, the paper further proposes an information-based reliability metric (R-score) that quantifies the degree to which LLM annotations deviate from random guessing. This metric provides researchers with a practical tool to identify unreliable annotations at the case level, complementing the expert-validation approach and offering a continuous measure of annotation confidence.

\section{When Survey Methodology Meets AI Annotation}
\label{sec:headings}

\subsection{Survey Strategies for the Assessment of Response Effectiveness}
Despite the apparent differences between survey respondents and Large Language Models (LLMs), both can exhibit behavior that does not necessarily stem from deep engagement or true “understanding.” Survey research has documented how participants sometimes take cognitive shortcuts—known as satisficing (\citep{krosnick1991response}, \citep{barge2012using}). Proposed by \citep{simon1957behavioral} in a general sense and by \citep{krosnick1991response} specifically in survery research, satisficing describes how respondents may skip or abbreviate the four cognitive processes of comprehension, retrieval, judgment, and response selection to conserve effort. Instead of formulating a fully reasoned answer, they opt for a response that seems good enough. Sometimes, satisfying can also take a more extreme form as in the case of careless responding (\citep{meade2012identifying}, \citep{johnson2005ascertaining}, \citep{ward2023dealing}): not only are participants skipping deeper thought, but they may be ignoring the survey content altogether. 

Satisficing theory offers a useful theoretical framework for understanding how both human survey respondents and Large Language Models (LLMs) might produce answers that appear sufficient without necessarily reflecting deep engagement. Within the realm of survey research, satisficing often becomes more prevalent when questions are lengthy, complex, or placed toward the end of a survey, where respondent fatigue sets in. In a parallel way, LLMs can exhibit similar tendencies by relying on frequent patterns or default abilities (such as assiging high probabilities to certain tokens) instead of logically parsing the contextual aspect of a prompt. Consequently, just as survey data can be skewed by inattentive or rushed respondents, LLM-generated answers may reflect superficial cues rather than meaningful reasoning, especially when the model is inherently undecidable.

To address these issues, it is useful to translate three classic survey-design interventions into the context of LLM annotation. Each intervention (also known as "screener question" \citep{berinsky2014separating}) targets a different facet of potential satisfice-like behavior in LLMs, encouraging more thoughtful and substantively effective (regardless of accurate or not) outputs.

First, option randomization involves rearranging the possible responses or labels to which an LLM can map its answers. Surveys have long embraced this approach to mitigate response order effects, such as primacy (a tendency to favor the first-listed option) and recency (a tendency to pick the last option offered). In LLM scenarios, if choices are consistently offered in the same order (in the training data), the model may adopt a habitual preference for a particular choice. Previous work shows that LLMs can be highly sensitive to reordering of choices (\citep{pezeshkpour2023large}), LLMs can exhibit inherent selection biases toward certain option IDs (\citep{zheng2023large}). Thus, an option randomization intervention similar to survey design is needed. By randomly shifting the positions of these options, the LLM is pushed to consider their actual meaning rather than relying on their option preferences in the list.

Second, position randomization changes the very order of prompts, segments, or option texts within a single annotation or classification task. Survey researchers often shuffle question sequences to avoid complacency and to distribute respondents’ cognitive effort more evenly across all items. In much the same way, LLMs reading prompts in a static, predictable order could have a pattern of mechanical responses related to the position of the content. Introducing randomness in the positioning of prompt text can help detect such tendencies.

Third, reverse validation (also called reverse coding in survey contexts) helps detect inconsistencies by presenting the LLM with inverted or negated versions of the same question. When humans respond inconsistently to positively worded and negatively worded statements of the same concept, it signals inattentiveness or insincerity. With LLMs, this intervention can reveal whether the model truly grasps the logical underpinnings of the text. For instance, after asking “What is the main type of this paper?” one might invert the question to “What is NOT the main focus of this paper?” to see whether the model remains coherent. If the LLM mismatches its answers, it suggests it may be defaulting to superficial or learned patterns rather than engaging in a deeper semantic analysis effectively.

Although the broader survey research literature includes other measures, such as open-ended questioning or extensive probing, these may have more limited utility for LLM annotation, at least in their direct form. This paper thus focuses on adopting the following survey-inspired approaches, option randomization, position randomization, and reverse validation, in line with robust experimental design traditions \citep{sniderman1996innovations}, to assess the reliability of LLM responses.

\input{table1}

% Simple figure
\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{Figures/figure1.pdf}
    \caption{Effective and ineffective AI annotations}
    \label{fig:fig1}
\end{figure}

As shown in Figure \ref{fig:fig1}, there is a conceptual difference between the boundary of reliability of Large Language Models (LLMs) text annotation and a common metric formerly emphasized -- accuracy. While previous studies often adopt accuracy based evaluation of LLM annotation, here I aruge that their reliability cannot be assessed through accuracy metrics alone. Accuracy is typically measured by comparing LLM outputs with expert annotations. However, this comparison captures only one dimension of reliability. This situation mirrors challenges in survey methodology. Survey respondents may provide responses that appear ``correct" but still ineffective. Similarly, LLM annotations require evaluation for their substantive effectiveness. The figure shows two overlapping circles representing accurate and effective annotations. Their partial overlap illustrates this distinction. Accurate annotations may match expert coding but still fail to capture the underlying construct of interest. LLMs can exhibit behaviors similar to survey respondent satisficing. These include agreeing with assertions without analysis, showing inattentiveness to context, and using cognitive shortcuts and so on. Thus, similar survey interventions need to be introduced to identify, at the case level, whether the model give an output in effectively or not.



\section{Data and Methods}
\subsection{Data}

\paragraph{F1000 Dataset}

We utilized the F1000 dataset (also known as Faculty Opinions) from previous studies in science of science. It comes from a post-publication peer-review platform in which invited scholars — practicing scientists and clinicians — select and evaluate biomedical papers they deem significant. The experts are asked to label papers with predefined tags in about five categories. For demonstration purposes, the labels used here contain three primary contribution types that takes the majority of cases: (A). Interesting Hypothesis (7.5\%)
(B). Technical Advance (13.3\%) and (C). New Finding (79\%). These expert annotations has been shown to align with different types of novelty (\citep{shi2023surprising}), thus are important in studying science and innovation. 

This dataset is suitable here as it provides expert-validated classifications that requires nuanced understanding. The categories are distinct yet related, making it a moderately challenging test for LLM reliability. Notably, the class imbalance mirrors real-world scientific output - most papers make empirical findings over theoretical or methodological contributions. This distribution creates natural test conditions for evaluating LLM reliability across frequent and rare categories. After preprocessing and cleaning, a total of 816 biomedical papers with expert annotations are included. 

\paragraph{Microsoft Academic Graph (MAG) Dataset} To investigate how annotations might affect downstream tasks such as regression, this study uses a simple example of predicting a paper's citation impact (with in 3 years of publication) based on the paper's contribution types using linear regression models. To do so, the F1000 data is merged with Microsoft Academic Graph Dataset, which has publicly available Microsoft Academic Graph, to get the citation count of 816 papers. The merge was done by matching their MAG paper ids with their PMID in the PubMed dataset. Citation counts follow a heavy-tailed distribution (mean=142, SD=213, max=2,184), typical of scientific impact patterns. We log-transform citations after adding 1 to handle zeros. We also added year and team size as control variables.


\subsection{LLM Models}

\paragraph{Model Selection and Architecture} We employ three variants of the LLaMA-3.1 Instruct series (8B, 70B, and 405B parameters) to systematically examine how model annotation reliability changes under survey-inspired interventions. This progression captures the full spectrum from lightweight to state-of-the-art LLMs (open sourced), allowing us to test whether larger models exhibit greater robustness to survey-inspired interventions. All models use the standard dense transformer architecture along with supervised fine-tuning and direct preference optimization after pretraining \citep{meta2024llama3}.

\paragraph{Inference Configuration} To ensure comparability across model sizes, we maintain identical generation parameters: temperature=0 for controlled randomness, top-p=0.7 sampling, and maximum output length=1 token). For probability distribution analysis, we extract logits directly from the final unembedding layer by using TogetherAI api with the parameter "logprobs" equals True . This setup allows us to precisely track how intervention-induced perturbations affect the models' internal confidence metrics at the precise token of interest.

\paragraph{Rationale for Multi-Scale Analysis} The tripartite model selection directly informs key findings in later analyses: 1) The 8B model serves as a baseline for "commodity" LLMs accessible on consumer hardware, showing high intervention sensitivity (Figure \ref{fig:fig3} - \ref{fig:fig5}) due to limited contextual reasoning capacity. 2) The 70B variant represents current practical limits of dense models, demonstrating partial robustness to position randomization but remaining vulnerable to reverse validation (Figure \ref{fig:fig3} - \ref{fig:fig5}). 3) The 405B model tests whether large models with extreme scale can overcome satisficing tendencies - our results suggest even this frontier model retains non-trivial sensitivity to option ordering (Figure \ref{fig:fig3} - \ref{fig:fig5}), indicating limitations in current LLM paradigms.

\subsection{Annotation Implementation}
To setup, we design prompts for AI annotation in a multiple choces format-- a fundamental and widely used format in AI tasks (\citep{sap2019socialiqa}, \citep{singhal2023large}, \citep{hendrycks2020measuring}, \citep{von2024vox}), and is suggested for good performance for prompting answers (\citep{robinson2022leveraging}), and as LLM to predict a single option token. This setting allows us to trace and evaluate the probablity distribution of the single token prediction easily, and allows for consistent comparison across models. 

\subsubsection{Prompt Design}
% The prompt template
\begin{tcolorbox}[mybox, title =  Basic Prompt Template]
Given the following scientific paper abstract:\\
\texttt{[Abstract Text]}

What is the main contribution type of this paper?
\begin{enumerate}[label=\textbf{\Alph{enumi}.}, leftmargin=*]
    \item  Interesting Hypothesis
    \item  Technical Advance
    \item  New Finding
\end{enumerate}

\textit{Please respond with only the option of your choice.}
\end{tcolorbox}

\subsection{Survey-Inspired Interventions}

For each paper in the dataset, we implement three types of interventions. Below are the example for each intervention. 

\begin{enumerate}
    \item \textbf{Option Randomization}
    \begin{itemize}
        \item Addresses token bias by varying the order of response options
    \end{itemize}
    
    \begin{tcolorbox}[mybox, title=Example: Option Randomization]
        \textbf{Original:}\\
        What is the main contribution type of this paper?\\
        $\mathcal{A}$. New method \quad $\mathcal{B}$. New finding \quad $\mathcal{C}$. New theory
        
        \vspace{2mm}
        \textbf{Randomized Variants:}
        \begin{enumerate}[label=\arabic*)]
            \item $\mathcal{A}$. New finding \quad $\mathcal{B}$. New theory \quad $\mathcal{C}$. New method
            \item $\mathcal{A}$. New theory \quad $\mathcal{B}$. New method \quad $\mathcal{C}$. New finding
        \end{enumerate}
    \end{tcolorbox}

    \item \textbf{Position Randomization}
    \begin{itemize}
        \item Mitigates the impact of prompt position structure on responses
    \end{itemize}
    
    \begin{tcolorbox}[mybox, title=Example: Position Randomization]
        \textbf{Original:}\\
        What is the main contribution type of this paper?\\
        $\mathcal{A}$. New method \quad $\mathcal{B}$. New finding \quad $\mathcal{C}$. New theory
        
        \vspace{2mm}
        \textbf{Position Variants:}
        \begin{enumerate}[label=\arabic*)]
            \item $\mathcal{B}$. New finding \quad $\mathcal{C}$. New theory \quad $\mathcal{A}$. New method
            \item $\mathcal{C}$. New theory \quad $\mathcal{A}$. New method \quad $\mathcal{B}$. New finding
        \end{enumerate}
    \end{tcolorbox}

    \item \textbf{Reverse Validation}
    \begin{itemize}
        \item Creating reverse-coded items
    \end{itemize}
    
    \begin{tcolorbox}[mybox, title=Example: Reverse Validation]
        \textbf{Original:}\\
        What is the main contribution type of this paper?\\
        $\mathcal{A}$. New method \quad $\mathcal{B}$. New finding \quad $\mathcal{C}$. New theory
        
        \vspace{2mm}
        \textbf{Reverse:}\\
        What items are \textbf{NOT} the main contribution of this paper?\\
        $\mathcal{A}$. New finding or New theory\\
        $\mathcal{B}$. New method or New theory\\
        $\mathcal{C}$. New method or New finding
    \end{tcolorbox}
\end{enumerate}



\subsection{Reliability Metric Design}

Beyond implementing interventions, we also need to quantify the inherent uncertainty in LLM annotations. While LLMs always provide a definitive answer when forced to choose, this masks their internal uncertainty. The intervention results (as shown in the next section) suggest that when models are genuinely uncertain about the correct classification, their outputs become highly sensitive to prompt variations—exactly what the survey-inspired interventions are designed to detect.

\subsubsection{Independent Probability Assessment}

To measure model uncertainty accurately, we need to access the probability distribution over possible answers. However, directly presenting all options simultaneously in a prompt introduces a problematic asymmetry due to the causal attention mechanism of transformer-based LLMs. Information from earlier options accumulates and influences the processing of later options, creating position-dependent biases (see Figure \ref{fig:figcausal}). Thus, the model isn't actually evaluating the options independently.

% Simple figure
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Figures/causal.pdf}
    \caption{The Causal Asymmetry of Information Accumulation in LLM}
    \label{fig:figcausal}
\end{figure}

To address this issue, the paper propose independently evaluating each category through separate binary queries:

\begin{tcolorbox}[mybox, title=Independent Probability Assessment]
\textbf{Example query for each category:}

Given the following scientific paper abstract:\\
\texttt{[Abstract Text]}

Is the main contribution of this paper "Technical Advance"?

\textit{Please answer only Yes or No.}
\end{tcolorbox}

By repeating this process for each category ("Interesting Hypothesis", "Technical Advance", and "New Finding"), we can obtain independent probability assessments $p(\text{Yes}|\text{Category})$ for each category. This approach can be understood as the LLM-version of projecting meanings onto specific dimensions (\citep{kozlowski2019geometry,rodriguez2022word,charlesworth2022historical}) —thus representing a "geometry of thinking" where each query projects the input text onto a distinct semantic axis, albeit using natural language. This controls for information asymmetry, allowing us to capture the model's genuine preference distribution.

This metric can be viewed as the "dual" of survey intervention. While survey intervention introduces asymmetry into prompts to test reliability, this reliability score does the opposite—it controls for these asymmetries to uncover the model's inherent preferences. In essence, survey intervention deliberately perturbs the system, while our reliability measure filters out these perturbations to reveal the underlying signal.

\subsubsection{Information-Theoretic Reliability Score}

Drawing from information theory, the paper proposes a reliability score (R-score) based on Kullback-Leibler divergence that quantifies how much an LLM's probability distribution differs from a uniform random distribution:

\begin{tcolorbox}[mybox, title=R-score Calculation]
\begin{align}
R = D_{KL}(P||U) = \sum_{i=1}^{k} p_i \log \left( \frac{p_i}{1/k} \right)
\end{align}

where:
\begin{itemize}
    \item $P = \{p_1, p_2, ..., p_k\}$ is the normalized probability distribution across $k$ annotation options
    \item $U = \{1/k, 1/k, ..., 1/k\}$ is the uniform distribution representing random guessing
    \item Higher R-scores indicate greater divergence from random guessing, suggesting more reliable annotations
\end{itemize}
\end{tcolorbox}


\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{Figures/figure_1_2.pdf}
\caption{\textbf{Information-Theoretic Reliability Score (R-score).} }
\label{fig:fig1_2}
\end{figure} 


While Figure \ref{fig:fig1} illustrates the conceptual difference between accuracy and effectiveness, we need a concrete metric to operationalize this distinction. Figure \ref{fig:fig1_2} introduces an information-theoretic reliability score (R-score).


The reliability of a large language model (LLM) annotation can be quantified by measuring how distinctly it can differentiate between multiple classification options. Figure \ref{fig:fig1_2} gives an example matrix. The matrix shows probability distributions for six example texts, with each row representing a text and each column representing a classification option. Specifically, we want LLM to assign a probability for each case under each option, independently, and then normalized these probabilities. Then, the reliability is calculated using Kullback-Leibler (KL) divergence from a uniform distribution: $ R = D_{KL}(P||U) = \sum_{i=1}^{k} p_i \log\left(\frac{p_i}{1/k}\right)$, where $p$ represents the normalized probability distribution across $k$ annotation options and $U$ is a uniform distribution ($1/k$ for each option, indicating random guessing). Color coding reflects reliability levels: dark green indicates high reliability (KL $\geq$ 0.7, strong differentiation between options), light green indicates moderate reliability (0.36 $\leq$ KL $<$ 0.7), light red indicates low reliability (0.06 $\leq$ KL $<$ 0.36), and dark red indicates very low reliability (KL $<$ 0.06, near-uniform distribution suggesting random guessing). For example, Text 2 shows high reliability (KL = 0.87) with a clear preference for Label 1 (0.95), while Text 1 shows very low reliability (KL = 0.00) with near-equal probabilities across all options, indicating the model cannot meaningfully distinguish between classification choices. This information-theoretic approach thus provides a principled metric for assessing when LLM annotations should be trusted for downstream analyses, at the case level.



Based on empirical analysis with three-option classification tasks, we establish four reliability thresholds:

\begin{tcolorbox}[mybox, title=Reliability Thresholds]
\begin{itemize}
    \item \textbf{High Reliability} (R $\geq$ 0.7): Corresponds approximately to a distribution of [0.9, 0.05, 0.05], indicating strong confidence in the top prediction
    \item \textbf{Moderate Reliability} (0.36 $\leq$ R < 0.7): Corresponds approximately to a distribution of [0.75, 0.125, 0.125], showing clear preference for the top option
    \item \textbf{Low Reliability} (0.06 $\leq$ R < 0.36): Corresponds approximately to a distribution of [0.5, 0.25, 0.25], where the top probability equals the sum of remaining options
    \item \textbf{Very Low Reliability} (R < 0.06): Distribution approaches uniformity, suggesting the model is essentially guessing
\end{itemize}
\end{tcolorbox}

The thresholds of $R$-score were empirically selected based on intuitive probability distributions for a three-option classification scenario. A distribution close to uniform (low R-score) suggests the model cannot meaningfully differentiate between categories, while a highly skewed distribution (high R-score) indicates strong preference for a particular category. The threshold of KL = 0.06 corresponds approximately to a distribution of [0.5, 0.25, 0.25], where the top probability is equal to the sum of remaining options. KL = 0.36 corresponds to a distribution of [0.75, 0.125, 0.125], where the top probability is three times of the rest others, demonstrating a clearer model preference. KL = 0.7 corresponds to a distribution of [0.9, 0.05, 0.05], representing a case where the model shows very strong confidence (90\% of the time) in its top prediction. In other words, if the model repeatedly annotates this case randomly, the result is consistent with the "top" label in 90\% cases.

The R-score complements our intervention-based approach by providing a continuous measure of annotation confidence. While interventions reveal sensitivity to prompt variations, the R-score quantifies the inherent uncertainty in the model's predictions. 


\section{Results}




\begin{figure}
    \centering
    \includegraphics[width=0.65\textwidth]{Figures/figure_3.pdf}
    \caption{\textbf{Impact of survey-inspired interventions on LLM Annotation}. The figure presents the flip rates of LLM answers under three survey-methodology interventions: option randomization (varying the order of choices, shown in pink shades), position randomization (altering prompt structure, shown in blue shades), and reverse validation (using inversely coded questions, shown in red). An answer is considered to \textbf{flip} if the predicted class (with largest probability among the options) under a certain intervention is different from that under the original prompt. These interventions, adapted from classical survey design principles addressing satisficing behavior, were tested across three Llama model variants (8B, 70B, and 405B parameters). The results show that larger models are not necessarily more robust to these interventions, with Llama-3.1-8B showing notably high sensitivity (10.5-25.4\%) compared to its larger counterparts. This suggests that, at least for the task of paper contribution annotation, model responses still rely to a non-negligible extent on superficial patterns rather than deep semantic understanding, analogous to satisficing behavior in human survey responses.}
    \label{fig:fig3}
\end{figure}

 
Figure \ref{fig:fig3} reports the “flip rate,” i.e., the percentage of instances in which an LLM’s top predicted label changes after being subjected to each of the three survey-inspired interventions. Despite potential expectations that larger models (405B parameters) might be more robust, the figure shows that all models—including the largest—are prone to flips at non-trivial rates. Specifically, the 8B model demonstrates a wide flip rate range (10.5–25.4\%), reflecting heightened susceptibility to reordering of answer options, to rotated prompt structures, or to reversed question logic. Even the 405B model exhibits a flip rate averaging above 5\%, suggesting that, while it is relatively more stable than its smaller counterparts, it is not immune to intervention-induced variability.

These results show that relying solely on standard accuracy measures may obscure the underlying instability of LLM annotations, especially for smaller-sized models. The high flip rates observed in the 8B model point toward more pronounced reliance on “shortcut” or pattern-based reasoning—akin to survey respondents who guess based on question ordering rather than content. Conversely, while the 70B and 405B models show somewhat reduced flip rates, the changes are still non-negligible. Thus, even advanced models can lapse into ``satisficing”-like behaviors under small prompt adjustments. 


\begin{figure}
\centering
\includegraphics[width=1\textwidth]{Figures/figure_4.pdf}
\caption{\textbf{Impact of survey-inspired interventions on LLM Annotation accross different paper categories}. The figure presents flip rates for three contribution types of 816 papers in the dataset: Interesting Hypothesis (N=62), Technical Advance (N=109), and New Finding (N=645). Each panel shows how different intervention types (option randomization in pink shades, position rotation in blue shades, and reverse validation in red) affect the three Llama models' response consistency. The flip rates vary substantially across paper categories, suggesting that the models' reliability is content-dependent. The Interesting hypothesis category generally exhibits higher flip rates across all interventions and model sizes, indicating particular challenges in maintaining consistent classifications for less frequent categories. This pattern persists regardless of model size, with even the largest 405B parameter model showing significant response instability under various intervention conditions.}
\label{fig:fig4}
\end{figure}

Figure \ref{fig:fig4} zooms in on how flip rates vary by paper category—Interesting Hypothesis (N=62), Technical Advance (N=109), and New Finding (N=645)—across all three LLMs. A key takeaway is that the “Interesting Hypothesis” category shows the highest flip rates in most interventions for all model sizes. This suggests that rarer or more conceptually demanding classes may induce greater model uncertainty, making them more susceptible to small perturbations in option ordering, prompt position, or question framing. In other words, the models struggle disproportionately with classifying the less common or more abstract paper types, indicating a particular instability that could significantly affect downstream analyses focused on such minority categories. This instability in underrepresented cases will not be identified if researchers soling relying on the external metrics for model evaluation.

For “Technical Advance,” the flip rates also remain elevated, especially for the 8B model, but somewhat moderate for the 70B and 405B models. “New Finding,” by contrast, shows lower but still non-zero flip rates; this is likely due to its dominance in the training distribution and, hence, the model’s learned preference for that label. Nonetheless, the models still register flips in the “New Finding” category under certain interventions (especially reverse validation). 

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{Figures/Figure_5_consistency_matrices.pdf}
\caption{\textbf{Consistency-accuracy relationships across different scientific claim types and model sizes}. Each matrix shows the relationship between consistency (whether the LLM annotation flips under certain intervention) and accuracy ($\Delta acc$) for different intervention types (y-axis) and model sizes (x-axis) across three contribution categories of scientific papers: Interesting Hypothesis, Technical Advance, and New Finding. $\Delta acc$ is calculated as accuracy(flipped) - accuracy(original), where positive values (red) indicate that flipped versions achieve higher accuracy than original versions, and negative values (green) indicate higher accuracy on original versions. The intensity of color represents the magnitude of the effect. Results show distinct patterns: Interesting Hypotheses generally exhibit positive $\Delta acc$ (maximum +0.55), Technical Advances show predominantly negative $\Delta acc$ except for specific interventions (ranging from -0.58 to +0.98), and New Findings demonstrate consistent negative $\Delta acc$ (minimum -0.86), suggesting that model behavior varies systematically across different types of scientific papers. Small categories suffer more significantly problems.}
\label{fig:fig5}
\end{figure}

Figure \ref{fig:fig5} explores how ``internal consistency” (whether an annotation flips under an intervention) aligns—or fails to align—with “external accuracy” (agreement with expert ground truth). Each cell reports $\Delta acc$, defined as accuracy(flip) minus accuracy(no-flip). By design, a positive $\Delta acc$ (shown in red) means that those annotations which flipped under an intervention ironically ended up being more accurate than those that stayed the same. Conversely, negative $\Delta acc$ (green) indicates that flips tend to be bad indicators for correctness in those cases—i.e., changing an answer correlates with lower accuracy relative to not flipping. 
The result shows that only the ``New Finding" category shows alignment between external accuracy and internal consistancy for all interventions. In the other categories, all three models show misalignment in certain situations. This finding carries significant implications: although conventional wisdom suggests that higher internal consistency should correspond to higher accuracy and thus better for use, these positive $\Delta acc$ values in minor categories (especially in ``Interesting Hypothesis") shows that this assumption does not always hold. The areas of discrepancy consistute a ``non-sense region" in which the model is unreliable, and in which simple reliance on external validation alone can be misleading.


\input{table2}

To show how unreliable annotation might affect downstream regression tasks, a series of regression analyses are done. For simplicity, the regression task is chosen to be the relationship between paper classification (Technical Advance vs. New Finding) and the paper's citation impact. The dependent variable is the logarithm of citations received within three years of publication, and all specifications include controls for publication year and team size fixed effects. To assess the robustness of language model classifications, we implement various prompt interventions and analyze their effects on the downstream regression results.

The regression results presented in Table \ref{tab:regression_results} reveal substantial impacts of various interventions on downstream classification tasks across different model sizes. The findings demonstrate that interventions frequently alter both the magnitude and statistical significance of the relationship between technical advances and citation counts.

For the 8B model, while the original classification shows no significant relationship between technical advances and citations ($\beta = -0.060$, $p = 0.615$), multiple interventions lead to statistically significant effects. Notably, the reverse intervention produces the largest effect ($\beta = -1.206$, $p < 0.01$), followed by option randomization ($\beta = -0.322$, $p < 0.05$) and position rotation ($\beta = -0.255$, $p < 0.05$). The emergence of significance across multiple intervention types suggests that the original classification's null result may not be robust to alternative prompting strategies.

The 70B model exhibits the most consistent original effect ($\beta = -0.446$, $p < 0.01$), indicating that technical advances receive fewer citations compared to new findings. However, the magnitude and significance of this relationship varies substantially under different interventions. While option randomization and position rotation maintain statistical significance, their coefficients show reduced magnitudes (ranging from $-0.268$ to $-0.396$). Most strikingly, the reverse intervention completely eliminates the significant negative effect ($\beta = -0.071$, $p = 0.690$).

The 405B model's results demonstrate more modest intervention effects, though option randomization produces a marginally significant negative effect ($\beta = -0.268$, $p < 0.10$) where none existed in the original classification ($\beta = -0.189$, $p = 0.196$). The reverse intervention actually changes the direction of the coefficient, albeit non-significantly ($\beta = 0.038$, $p = 0.793$).

The prevalence of highlighted rows across all model sizes (8 out of 15 specifications) indicates that the results of the downstream regression can be very sensitive to prompt interventions. 1 out of 8 highlighted rows have changed from significant results to non-significant ones. 4 out of 8 highlighted rows have changed from non-significant to significant ones. This sensitivity suggests that survery-inspired interventions are necessary when using large language models annotated classifications that feed into statistical analyses, as seemingly minor changes in prompt design can lead to substantially different empirical conclusions.



\clearpage
\section*{Examples of Unreliable Annotations}
%\underline{Example of Unreliable Annotations}.

\input{table5}

This example demonstrates a case of low reliability in LLM annotation (Inaccurate and Unreliable). The model assigns nearly identical probabilities to all three categories (approximately 1/3 each), indicating it cannot meaningfully distinguish between them for this paper. The resulting Reliability Score of 0 signifies that the probability distribution is effectively uniform, equivalent to random guessing. Despite this uncertainty, if forced to choose, the model would incorrectly label this as either "Technical Advance" or "New Finding" rather than the expert-assigned "Interesting Hypothesis" category. This case illustrates how accuracy metrics alone would flag this as an incorrect annotation, but our reliability framework additionally reveals that the model has no meaningful confidence in its answer—a critical distinction for downstream analysis.

\clearpage
\input{table6}

The example in \ref{tab:moderate_reliability_example} demonstrates a different case of low reliability in LLM annotation (Correct but Still Unreliable). While the model correctly identifies ``New Finding'' as the most probable category (0.498), there remains substantial uncertainty. The model assigns a significant probability (0.384) to ``Technical Advance,'' indicating it struggles to fully differentiate between these categories for this paper. The reliability score of 0.13 suggests the model is slightly confident in its classification but still exhibits notable uncertainty. This case illustrates a common challenge in scientific paper classification: the boundary between reporting new empirical findings and introducing novel technical approaches can be subtle, particularly in computational modeling papers like this one. Traditional accuracy metrics would simply mark this as correct, missing the important nuance that the model's confidence is relatively low.
\clearpage
\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{Figures/kl_divergence_grid.pdf}
\caption{\textbf{Distribution of Reliable Annotations by Model and Category}.}
\label{fig:fig6}
\end{figure}


Figure \ref{fig:fig6} presents the distribution of Reliability scores (R-score) for three sizes of the Llama-3.1 model family (8B, 70B, and 405B parameters) across three annotation categories (New Finding, Technical Advance, and Interesting Hypothesis). The R-score measures how much a model's probability distribution diverges from a uniform distribution using KL divergence, with higher values indicating greater reliability. Color-coding corresponds to reliability thresholds: dark green (KL $>$ 0.7, high reliability), light green (0.36 $<$ KL $\leq$ 0.7, moderate reliability), light red (0.06 $<$ KL $\leq$ 0.36, low reliability), and dark red (KL $\leq$ 0.06, very low reliability). While larger models generally demonstrate higher overall reliability (median KL of 0.37, 0.79, and 0.80 for 8B, 70B, and 405B respectively in the New Finding category), there is notable variation in reliability across different annotation categories within the same model. For instance, the 405B model shows high reliability for New Finding (median KL = 0.80) and Interesting Hypothesis (median KL = 0.82) but only moderate reliability for Technical Advance (median KL = 0.42). This variability highlights an important consideration: even when a model demonstrates high overall reliability, its performance may still be notably unreliable for specific categories or individual cases, necessitating reliability assessment at the case level rather than relying solely on model-wide metrics.

\input{table3}

\input{table4}

Table \ref{tab:unreliability} and Table \ref{tab:model_unreliability} further summarizes both correlations between model size and reliability, as well as category and reliablity in assessing scientific contributions of biomedical papers. The largest model, Llama-3.1-405B, demonstrated the lowest overall unreliability rate (5.0\%), while the smallest model, Llama-3.1-8B, showed significantly higher unreliability (45.8\%). Interestingly and importantly, all models exhibited the highest unreliability when evaluating "Interesting Hypothesis" claims (29.6\% overall), compared to "New Finding" (20.2\%) and "Technical Advance" (15.0\%) categories. This suggests that less frequent categories present greater challenges for LLMs to evaluate reliably, regardless of model scale. The gap does NOT always become narrower in large models (as in the case of Interesting Hypothesis from 70B model to 405B model). This underscores the importance of distinguishing reliable annotation from unreliable ones, since in more general situations the underlying labels are not known as here.

\section{Discussion}

High accuracy scores often give a sense of confidence in model outputs. Yet, these scores can hide subtle instabilities when models handle edge cases or require deep reasoning. This paper’s results demonstrate that accuracy metrics by external validation alone may not reveal fragile reasoning paths of LLM annotation. In the era of AI, adapting survey-inspired interventions for LLM annotations helps surface these hidden vulnerabilities. Social science researchers thus can use the findings to create a reliability boundary when deploying LLMs in scientific applications.

Given survey-inspired intervention results, this paper propose a information-theoretic reliability score that is simple enough to implement in any text annotation tasks without opening the black box of LLM. Rather than providing a coarse-grained metric for the model in general, one advantage of this score is to distinguish at the case level whether a LLM is capable of confidently distinguish among the categories in label tasks and consequently sensitive to altered prompt designs, as revealed by survey-inspired prompt interventions.

The study also empirically shows that one important aspect of annotation reliability is model choices. smaller models (e.g., 8B parameters) are found to exhibit substantially higher flip rates under survey-inspired interventions compared to larger counterparts. This pattern underscores the importance of reasoning capacity in annotation reliability—a capability that scales with model size. However, larger models are not completely immune. They also exhibit non-negligible rates of flips and inconsistencies. Thus, when designing large-scale studies, researchers should cautiously weigh the trade-offs between model cost and reliability. Even top-tier models can produce shortcuts that distort downstream analyses.

Another key takeaway is that reliability is not evenly distributed across classes in text data. Regions with fewer training examples, such as rarer categories, are found to be especially prone to brittle reasoning. The results show that for less common categories in science papers, many seemingly correct annotations fail under small prompt perturbations. The significance is such that paradoxically, rare categories may appear more accurate in conventional validation if models exploit dataset-specific shortcuts. Given the long-tailed distribution of categories in social data, this indicates that when using LLM for annotation, there is always a "non-sense region" where task is beyond the model's reasoning capability.  Researchers need devote extra attention to this region, by creating a task-specific boundary with the help of survey interventions, and incorporate the unreliability metrics into downstream statistical analysis to mitigate this issue.

Practically, reverse validation appears to give the strongest signal of potential unreliability among the three types of interventions discussed here. It consistently triggers high flip rates and large shifts in probability distributions. Thus, researchers may want to prioritize reverse-coded checks in classification tasks that requires more rigidity or rely on more subtle distinctions, given the time and budget constraint.

While option randomization, position randomization, and reverse validation prove effective, the proposed intervention framework is not exhaustive. Other approaches may be adapted from survey methodology principles. Deeper probing of LLM sensitivities could involve more comprehensive rephrasings, semantic inversions, and multi-step consistency checks. Expanding this toolkit can strengthen reliability assessments before resorting to expensive expert labeling.

Finally, survey-inspired interventions and expert validation should be viewed as complementary rather than competing approaches. By calculating reliability scores, researchers can implement stratified validation strategies where high flip-rate or low entropy cases receive priority expert review while conserving resources on more reliable annotations. This targeted sampling approach can be valuable particularly for (hidden) rare categories and edge cases. The resulting feedback loop not only validates outputs but improves the entire annotation pipeline, as expert corrections reveal patterns of model failure that inform prompt refinement or model selection. When reporting findings, researchers can enhance methodological transparency by documenting both reliability metrics and expert agreement rates. A practical workflow might include initial LLM annotation, reliability assessment through interventions, establishing validation thresholds based on reliability distributions, concentrated expert review of below-threshold cases plus random sampling of high-reliability instances, and incorporating correction rates into final uncertainty estimates. This hybridized approach maintains the rigor of expert validation while maximizing the efficiency advantages of LLMs.



\bibliographystyle{unsrt}  
% \bibliography{references}  %%% Remove comment to use the external .bib file (using bibtex).
%%% and comment out the ``thebibliography'' section.



% %%% Comment out this section when you \bibliography{references} is enabled.
% \begin{thebibliography}{1}

\begin{thebibliography}{10}

\bibitem{grossmann2023ai}
Igor Grossmann, Matthew Feinberg, Dawn~C Parker, Nicholas~A Christakis, Philip~E Tetlock, and William~A Cunningham.
\newblock Ai and the transformation of social science research.
\newblock {\em Science}, 380(6650):1108--1109, 2023.

\bibitem{bail2024can}
Christopher~A Bail.
\newblock Can generative ai improve social science?
\newblock {\em Proceedings of the National Academy of Sciences}, 121(21):e2314021121, 2024.

\bibitem{tan2024large}
Zhen Tan, Alimohammad Beigi, Song Wang, Ruocheng Guo, Amrita Bhattacharjee, Bohan Jiang, Mansooreh Karami, Jundong Li, Lu~Cheng, and Huan Liu.
\newblock Large language models for data annotation: A survey.
\newblock {\em arXiv e-prints}, pages arXiv--2402, 2024.

\bibitem{yan2023cipta}
Yu~Yan, Wenzhuo Du, Di~Yang, and Dechun Yin.
\newblock Cipta: Contrastive-based iterative prompt-tuning using text annotation from large language models.
\newblock In {\em 2023 4th International Conference on Electronic Communication and Artificial Intelligence (ICECAI)}, pages 174--178. IEEE, 2023.

\bibitem{palmer2024using}
Alexis Palmer, Noah~A Smith, and Arthur Spirling.
\newblock Using proprietary language models in academic research requires explicit justification.
\newblock {\em Nature Computational Science}, 4(1):2--3, 2024.

\bibitem{rytting2023towards}
Christopher~Michael Rytting, Taylor Sorensen, Lisa Argyle, Ethan Busby, Nancy Fulda, Joshua Gubler, and David Wingate.
\newblock Towards coding social science datasets with language models.
\newblock {\em arXiv preprint arXiv:2306.02177}, 2023.

\bibitem{zhang2023safetybench}
Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, and Minlie Huang.
\newblock Safetybench: Evaluating the safety of large language models.
\newblock {\em arXiv preprint arXiv:2309.07045}, 2023.

\bibitem{tornberg2024best}
Petter T{\"o}rnberg.
\newblock Best practices for text annotation with large language models.
\newblock {\em arXiv preprint arXiv:2402.05129}, 2024.

\bibitem{ziems2024can}
Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, and Diyi Yang.
\newblock Can large language models transform computational social science?
\newblock {\em Computational Linguistics}, 50(1):237--291, 2024.

\bibitem{dentella2023systematic}
Vittoria Dentella, Fritz G{\"u}nther, and Evelina Leivada.
\newblock Systematic testing of three language models reveals low language accuracy, absence of response stability, and a yes-response bias.
\newblock {\em Proceedings of the National Academy of Sciences}, 120(51):e2309583120, 2023.

\bibitem{adewumi2024limitations}
Tosin Adewumi, Nudrat Habib, Lama Alkhaled, and Elisa Barney.
\newblock On the limitations of large language models (llms): False attribution.
\newblock {\em arXiv preprint arXiv:2404.04631}, 2024.

\bibitem{strauss2009construct}
Milton~E Strauss and Gregory~T Smith.
\newblock Construct validity: Advances in theory and methodology.
\newblock {\em Annual review of clinical psychology}, 5(1):1--25, 2009.

\bibitem{xue2024strengthened}
Mengge Xue, Zhenyu Hu, Liqun Liu, Kuo Liao, Shuang Li, Honglin Han, Meng Zhao, and Chengguo Yin.
\newblock Strengthened symbol binding makes large language models reliable multiple-choice selectors.
\newblock {\em arXiv preprint arXiv:2406.01026}, 2024.

\bibitem{misiejuk2024augmenting}
Kamila Misiejuk, Rogers Kaliisa, and Jennifer Scianna.
\newblock Augmenting assessment with ai coding of online student discourse: A question of reliability.
\newblock {\em Computers and Education: Artificial Intelligence}, 6:100216, 2024.

\bibitem{pangakis2023automated}
Nicholas Pangakis, Samuel Wolken, and Neil Fasching.
\newblock Automated annotation with generative ai requires validation.
\newblock {\em arXiv preprint arXiv:2306.00176}, 2023.

\bibitem{goral2024all}
Gracjan G{\'o}ral and Emilia Wisnios.
\newblock When all options are wrong: Evaluating large language model robustness with incorrect multiple-choice options.
\newblock {\em arXiv e-prints}, pages arXiv--2409, 2024.

\bibitem{bisbee2024synthetic}
James Bisbee, Joshua~D Clinton, Cassy Dorff, Brenton Kenkel, and Jennifer~M Larson.
\newblock Synthetic replacements for human survey data? the perils of large language models.
\newblock {\em Political Analysis}, 32(4):401--416, 2024.

\bibitem{von2024vox}
Leah von~der Heyde, Anna-Carolina Haensch, and Alexander Wenz.
\newblock Vox populi, vox ai? using language models to estimate german public opinion.
\newblock {\em arXiv preprint arXiv:2407.08563}, 2024.

\bibitem{argyle2023out}
Lisa~P Argyle, Ethan~C Busby, Nancy Fulda, Joshua~R Gubler, Christopher Rytting, and David Wingate.
\newblock Out of one, many: Using language models to simulate human samples.
\newblock {\em Political Analysis}, 31(3):337--351, 2023.

\bibitem{kozlowski2024silico}
Austin~C Kozlowski, Hyunku Kwon, and James~A Evans.
\newblock In silico sociology: forecasting covid-19 polarization with large language models.
\newblock {\em arXiv preprint arXiv:2407.11190}, 2024.

\bibitem{krosnick1991response}
Jon~A Krosnick.
\newblock Response strategies for coping with the cognitive demands of attitude measures in surveys.
\newblock {\em Applied cognitive psychology}, 5(3):213--236, 1991.

\bibitem{barge2012using}
Scott Barge and Hunter Gehlbach.
\newblock Using the theory of satisficing to evaluate the quality of survey data.
\newblock {\em Research in Higher Education}, 53(2):182--200, 2012.

\bibitem{simon1957behavioral}
Herbert Simon.
\newblock A behavioral model of rational choice.
\newblock {\em Models of man, social and rational: Mathematical essays on rational human behavior in a social setting}, 6(1):241--260, 1957.

\bibitem{meade2012identifying}
Adam~W Meade and S~Bartholomew Craig.
\newblock Identifying careless responses in survey data.
\newblock {\em Psychological methods}, 17(3):437, 2012.

\bibitem{johnson2005ascertaining}
John~A Johnson.
\newblock Ascertaining the validity of individual protocols from web-based personality inventories.
\newblock {\em Journal of research in personality}, 39(1):103--129, 2005.

\bibitem{ward2023dealing}
Mary~K Ward and Adam~W Meade.
\newblock Dealing with careless responding in survey data: Prevention, identification, and recommended best practices.
\newblock {\em Annual review of psychology}, 74(1):577--596, 2023.

\bibitem{berinsky2014separating}
Adam~J Berinsky, Michele~F Margolis, and Michael~W Sances.
\newblock Separating the shirkers from the workers? making sure respondents pay attention on self-administered surveys.
\newblock {\em American journal of political science}, 58(3):739--753, 2014.

\bibitem{pezeshkpour2023large}
Pouya Pezeshkpour and Estevam Hruschka.
\newblock Large language models sensitivity to the order of options in multiple-choice questions.
\newblock {\em arXiv preprint arXiv:2308.11483}, 2023.

\bibitem{zheng2023large}
Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang.
\newblock Large language models are not robust multiple choice selectors.
\newblock {\em arXiv preprint arXiv:2309.03882}, 2023.

\bibitem{sniderman1996innovations}
Paul~M Sniderman and Douglas~B Grob.
\newblock Innovations in experimental design in attitude surveys.
\newblock {\em Annual review of Sociology}, 22(1):377--399, 1996.

\bibitem{shi2023surprising}
Feng Shi and James Evans.
\newblock Surprising combinations of research contents and contexts are related to impact and emerge with scientific outsiders from distant disciplines.
\newblock {\em Nature Communications}, 14(1):1641, 2023.

\bibitem{meta2024llama3}
{Meta AI}.
\newblock Introducing llama 3.1: Our most capable models to date, 2024.
\newblock Accessed: 2025/01/28.

\bibitem{sap2019socialiqa}
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi.
\newblock Socialiqa: Commonsense reasoning about social interactions.
\newblock {\em arXiv preprint arXiv:1904.09728}, 2019.

\bibitem{singhal2023large}
Karan Singhal, Shekoofeh Azizi, Tao Tu, S~Sara Mahdavi, Jason Wei, Hyung~Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et~al.
\newblock Large language models encode clinical knowledge.
\newblock {\em Nature}, 620(7972):172--180, 2023.

\bibitem{hendrycks2020measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock {\em arXiv preprint arXiv:2009.03300}, 2020.

\bibitem{robinson2022leveraging}
Joshua Robinson.
\newblock Leveraging large language models trained on code for symbol binding.
\newblock Master's thesis, Brigham Young University, 2022.

\bibitem{kozlowski2019geometry}
Austin~C Kozlowski, Matt Taddy, and James~A Evans.
\newblock The geometry of culture: Analyzing the meanings of class through word embeddings.
\newblock {\em American Sociological Review}, 84(5):905--949, 2019.

\bibitem{rodriguez2022word}
Pedro~L Rodriguez and Arthur Spirling.
\newblock Word embeddings: What works, what doesn’t, and how to tell the difference for applied research.
\newblock {\em The Journal of Politics}, 84(1):101--115, 2022.

\bibitem{charlesworth2022historical}
Tessa~ES Charlesworth, Aylin Caliskan, and Mahzarin~R Banaji.
\newblock Historical representations of social groups across 200 years of word embeddings from google books.
\newblock {\em Proceedings of the National Academy of Sciences}, 119(28):e2121798119, 2022.
\end{thebibliography}


% \end{thebibliography}


\end{document}
