
\section{Reducing Our Unifying Framework to Related Literature}\label{appendix:reduce}


In this section, we discuss how the various previous works can be obtained as special cases of %reduced from 
our unifying framework.

\subsection{Temporally Correlated Noise~\citep{Swamy2022_temporal}}

The Temporally Correlated Noise (TCN) proposed in~\citet{Swamy2022_temporal} is a special case of our setting where $u^o=0$ and only the confounding noise $u^\epsilon$ is present. Following Equation 14-17 of~\citet{Swamy2022_temporal}, their setting can be summarised as
\begin{align*}
s_t &= \mathcal{T}(s_{t-1}, a_{t-1})\\
  &= \mathcal{T}(s_{t-1}, \pi_E(s_{t-1}) + u_{t-1} + u_{t-2})\\
a_t &= \pi_E(s_t) + u_t + u_{t-1},
\end{align*}
where $\mathcal{T}$ is the transition function and $u_t$ are the TCN. It can be seen that TCN is the confounding noise $u^\epsilon$ since the expert policy doesn't take it into account and it affects (or confounds) both the state and action.


It can be seen that this is a special case of our framework when  $u^o_t=0$, where $a_t=\pi_E(s_t)+\epsilon(u^\epsilon_t)$ from~\cref{eq:action}, and more specifically when the confounding noise horizon in \cref{assump:horizon} is 2. In addition, the theoretical results in~\citet{Swamy2022_temporal} can be deduced from our main results as shown in Corollary~\ref{corollary:unconfounded}.


\subsection{Unobserved Contexts~\citep{Swamy2022}}
The setting considered by~\citet{Swamy2022} is a special case of our setting when $u^\epsilon=0$ and only $u^o$ are present. Following Section 3 of~\citet{Swamy2022}, their setting can be summarised as
\begin{align*}
\mathcal{T}&:\states\times\actions\times C \rightarrow D(S)\\
\mathcal{r}&:\states\times\actions\times C \rightarrow [-1,1]\\
a_t&=\pi_E(\state_t,c)
\end{align*}
where $c\in C$ is the context, which is assumed to be fixed throughout an episode. There are no hidden confounders in this setting and the context $c$ is included in $u^o$ under our framework. Note that in our setting we also allow $u^o$ to be varying throughout an episode. In addition, the theoretical results in~\citet{Swamy2022} can be deduced from our main results as shown in Corollary~\ref{corollary:noUo}.


\subsection{Imitation Learning with Latent Confounders~\citep{Vuorio2022}}

The setting considered by~\citet{Vuorio2022} is also a special case of our setting when $u^\epsilon=0$ and only $u^o$ are present, which is very similar to~\citet{Swamy2022}. In Section 2.2 of~\citet{Vuorio2022}, they introduced a latent variable $\theta\in\Theta$ that is fixed throughout an episode and $a_t=\pi_E(\state_t,\theta)$. There are no hidden confounders in this setting and the latent variable $\theta$ is included in $u^o$ in our framework. No theoretical imitation gap bounds are provided in~\citet{Vuorio2022}. However, Corollary~\ref{corollary:noUo} can be directly applied to their setting and bound the imitation gap.


\subsection{Causal Delusion and Confusion~\citep{Ortega2021,deHaan2019,Pfrommer2023,Spencer2021,Wen2020}}


The concept of causal delusion~\citep{Ortega2021} and confusion is widely studied in the literature~\citep{deHaan2019,Pfrommer2023,Spencer2021,Wen2020} from different perspectives. A classic example of causal confusion is learning to break in an autonomous driving scenario. The states are images with full view of the dashboard and the road conditions. The break indicator in this scenario is the confounding variable that correlates with the action of breaking in subsequent steps, which causes the imitator to learn to break if the break indicator light is already on. Therefore, another name for this problem is the latching problem, where the imitator latches to spurious correlations between current action and the trajectory history. In the setting of~\citet{Ortega2021}, this is explicitly modelled as latent variables that affect both the action and state, causing spurious correlation between them and confusing the imitator. In other settings~\citep{deHaan2019,Pfrommer2023,Spencer2021,Wen2020}, there are no explicit unobserved confounders, but the nuisance correlation between the previous states and actions can be modelled as the existence of hidden confounders $u^\epsilon$ in our framework. Specifically, in~\citet{deHaan2019}, $x_{t-1}$ and $a_{t-1}$ are considered confounders that affect the state variable $x_t$, which causes a spurious correlation between previous state action pairs and $a_t$. The spurious correlation between variables is typically modelled as the existence of a hidden confounder $u^\epsilon$ that affects both variables in causal modelling. For example, the actual hazard or event that causes the expert to break will be the hidden confounder $u^\epsilon$ that affects both the break and the break indicator.

However, despite the fact that this setting can be considered a special case of our general framework, we stress that the concrete and practical problems considered in~\citet{deHaan2019,Pfrommer2023,Spencer2021,Wen2020} are different from ours, where they assumed implicitly that the hidden confounders $u^\epsilon$ are embedded in the observations or outright observed.


\section{Proofs of Main Results}

In this section, we provide the proofs for the main results and corollaries in this paper.

\subsection{Proof of Propositions}\label{appendix:prop}

\textbf{Proposition~\ref{prop:ill-posed}}:
The ill-posedness $\ill(\Pi,k)$ is monotonically increasing as the confounded horizon $k$ increases.
\begin{proof}
From definition, we have that \begin{align*}
    \ill(\Pi,k)=\sup_{\pi\in\Pi} \frac{\norm{\pi_E-\pi}_{2}}{\norm{\expectE[a_t-\pi(h_t)\lvert h_{t-k}]}_{2}}.
\end{align*}
We would like to show for each $\pi\in\Pi$, $\frac{\norm{\pi_E-\pi}_{2}}{\norm{\expectE[a_t-\pi(h_t)\lvert h_{t-k}]}_{2}}$ is increasing as $k$ increases, which would imply that $\ill(\Pi,k)$ is increasing. For each $\pi\in\Pi$, we see that the numerator is constant w.r.t the horizon $k$. Therefore, it is enough to check that for each $\pi\in\Pi$, the denominator $\norm{\expectE[a_t-\pi(h_t)\lvert h_{t-k}]}_{2}$ decreases as $k$ increases. For any two integer horizon $k_1>k_2$,
\begin{align}
\expectE[a_t-\pi(h_t)\lvert h_{t-k_1}]^2&=\expectE[\expectE[a_t-\pi(h_t)\lvert h_{t-k_2}]\lvert h_{t-k_1}]^2\\
&\leq \expectE[\expectE[a_t-\pi(h_t)\lvert h_{t-k_2}]^2\lvert h_{t-k_1}]\\
&=\expectE[a_t-\pi(h_t)\lvert h_{t-k_2}]^2
\end{align}
by the tower property of conditional expectation as $\sigma(h_{t-k_1})\subseteq\sigma(h_{t-k_2})$, Jensen's inequality for conditional expectations, and the fact that $\expectE[a_t-\pi(h_t)\lvert h_{t-k_2}]^2$ is $h_{t-k_1}$ measurable, respectively for each line. Therefore, we have that $\expectE[a_t-\pi(h_t)\lvert h_{t-k}]$ is decreasing, which implies $\norm{\expectE[a_t-\pi(h_t)\lvert h_{t-k}]}_{2}$ is decreasing and $\ill(\Pi,k)$ is increasing as $k$ increases, which completes the proof.
\end{proof}

\subsection{Main results for guarantees on the imitation gap}\label{appendix:gap}

\textbf{\cref{thm:gap}}: Let $\hat{\pi}_h$ be the learnt policy with CMR error $\epsilon$ and let $\ill(\Pi,k)$ be the ill-posedness of the problem. Assume that $\delta_{TV}(u^o_t,\expectE_{\pi_E}[u^o_t\lvert h_t])\leq\delta$ for $\delta\in\realNumber^+$, $P(u^\epsilon_t)$ is c-TV stable and $\pi_E$ is deterministic. Then, the imitation gap is upper bounded by
\begin{align*}
    J(\pi_E)-J(\hat{\pi}_h)\leq T^2(c\epsilon\ill(\Pi,k)+2\delta)=\mathcal{O}(T^2(\delta+\epsilon)).
\end{align*}

\begin{proof}[Proof of~\cref{thm:gap}]
Recall that $J(\pi)$ is the expected reward following $\pi$, and we would like to bound the performance gap $J(\pi_E)-J(\hat{\pi_h})$ between the expert policy $\pi_E$ and the learned history-dependent policy $\hat{\pi_h}$. Let $Q_{\hat{\pi_h}}(s_t,a_t,u^o_t)$ be the Q-function of $\hat{\pi_h}$. Using the Performance Difference Lemma~\citep{Kakade2002}, we have that for any Q-function $\tilde{Q}(h_t,a_t)$ that takes in the trajectory history $h_t$ and action $a_t$,
\begin{align}
J(\pi_E)-J(\hat{\pi_h})&=\expectE_{\tau\sim\pi_E}[\sum_{t=1}^T Q_{\hat{\pi_h}}(s_t,a_t,u^o_t)-\expectE_{a\sim\hat{\pi_h}}[Q_{\hat{\pi_h}}(s_t,a,u^o_t)]]\nonumber\\
&=\sum_{t=1}^T\expectE_{\tau\sim\pi_E}[Q_{\hat{\pi_h}}(s_t,a_t,u^o_t)-\tilde{Q}(h_t,a_t)+\tilde{Q}(h_t,a_t)-\expectE_{a\sim{\hat{\pi_h}}}[Q_{\hat{\pi_h}}-\tilde{Q}+\tilde{Q}]]\nonumber\\
&=\sum_{t=1}^T \expectE_{\tau\sim\pi_E}[\tilde{Q}-\expectE_{a\sim\hat{\pi_h}}[\tilde{Q}]]+\sum_{t=1}^T \expectE_{\tau\sim\pi_E}[Q_{\hat{\pi_h}}-\tilde{Q}-\expectE_{a\sim\hat{\pi_h}}[Q_{\hat{\pi_h}}-\tilde{Q}]]\label{eq:pdl}
\end{align}


We first bound the second part of~\cref{eq:pdl}. Denote by $\delta_{TV}$ the total variation distance. For two distributions $P,Q$, recall the property of total variation distance for bounding the difference in expectations:
\begin{align*}
\abs{\expectE_P[f(x)]-\expectE_Q[f(x)]}\leq \norm{f}_\infty \delta_{TV}(P,Q).
\end{align*}
In order to bound the second part of~\cref{eq:pdl}, for any $Q$ function, consider inferred $\tilde{Q}$ using the conditional expectation of $u^o$ on the history $h$,
\begin{equation*}
\tilde{Q}(h_t,a_t)\coloneqq Q(s_t,a_t,\expectE_{\tau\sim \pi_E}[u^o_t\lvert h_t]),
\end{equation*}
where note that $s_t\in h_t$. We have that, when the transition trajectory $(s_t,u^o_t,u^\epsilon_t,r_t)\sim \pi_E$ follows the expert policy, for any action $\dot{a}\sim \pi$ following some policy $\pi$ (in our case, it can be $\pi_E$ or $\hat{\pi_h}$),
% \begin{align}
% \abs{\expectE_{\tau\sim\pi_E}[Q(s_t,u_t;\dot{a})-\tilde{Q}(h_t;\dot{a})]}&=\left\lvert\expectE_{\tau\sim\pi_E}[Q(s_t,u_t;\dot{a})-\expectE_{\tau\sim\pi_E}[Q(s_t,u_t;\dot{a})\lvert h_t]]\right\rvert\\
% &=\left\lvert\expectE_{(s_t,u_t)\sim\pi_E}[Q(s_t,u_t;\dot{a})]-\expectE_{(s_t,u_t\lvert h_t)\sim\pi_E}[Q(s_t,u_t;\dot{a})]\right\rvert\\
% % &\leq \expectE_{\tau\sim\pi_E}[\abs{Q(s_t,a_t,u_t)-\expectE_{u_t\sim\pi_E}[Q(s_t,a_t,u_t)\lvert h_t,a_t]}]\label{eq:jensen}\\
% &\leq \norm{Q}_{\infty}\delta_{TV}(F_{\pi_E}(s_t,u_t),F_{\pi_E}(s_t,u_t\lvert h_t))\label{eq:tv_bound}\\
% % &\leq T \expectE_{\pi_E}[\delta_{TV}(F(u_t),F(u_t\lvert h_t,a_t))]\\
% &\leq T \cdot\delta_{TV}(F_{\pi_E}(s_t,u_t),F_{\pi_E}(s_t,u_t\lvert h_t))\\
% &\leq T\delta \label{eq:second_part_bound}
% \end{align}
\begin{align}
\abs{\expectE_{\tau\sim\pi_E,\dot{a}\sim\pi}[Q(s_t,\dot{a},u_t)-\tilde{Q}(h_t,\dot{a})]}&=\left\lvert\expectE_{\tau\sim\pi_E,\dot{a}\sim\pi}[Q(s_t,\dot{a},u^o_t)-Q(s_t,\dot{a},\expectE_{\tau\sim\pi_E}[u^o_t\lvert h_t]])]\right\rvert\nonumber\\
&=\left\lvert\expectE_{u^o_t\sim\pi_E}[\expectE_{\pi_E,\pi}[Q(s_t,\dot{a},u^o_t)\lvert u^o_t]-\expectE_{u^o_t\lvert h_t\sim\pi_E}[\expectE_{\pi_E,\pi}[Q(s_t,\dot{a},u^o_t)\lvert u^o_t]\right\rvert\label{eq:tower_property}\\
&\leq \norm{\expectE_{\pi_E,\pi}[Q(s_t,\dot{a},u^o_t)\lvert u^o_t]}_{\infty}\delta_{TV}(u^o_t,\expectE_{\pi_E}[u^o_t\lvert h_t])\label{eq:tv_bound}\\
% &\leq T \expectE_{\pi_E}[\delta_{TV}(F(u_t),F(u_t\lvert h_t,a_t))]\\
&\leq T \cdot\delta_{TV}(u^o_t,\expectE_{\pi_E}[u^o_t\lvert h_t])\label{eq:bounded_Q}\\
&\leq T\delta \label{eq:second_part_bound}
\end{align}
%
where~\cref{eq:tower_property} uses the tower property of expectations,~\cref{eq:tv_bound} uses the total variation distance bound for bounded functions,~\cref{eq:bounded_Q} uses the fact that the $Q$ function is bounded by $T$ and~\cref{eq:second_part_bound} uses the condition that $\delta_{TV}(u^o_t,\expectE_{\pi_E}[u^o_t\lvert h_t])\leq\delta$ in the theorem statement. Since~\cref{eq:pdl} holds for any choice of $\tilde{Q}$, we choose $\tilde{Q}_{\hat{\pi_h}}(h_t,a_t)\coloneqq Q_{\hat{\pi_h}}(s_t,a_t,\expectE_{\tau\sim \pi_E}[u^o_t\lvert h_t])$ such that we can apply~\cref{eq:second_part_bound} twice to bound the second part of~\cref{eq:pdl}:

\begin{align}
\expectE_{\tau\sim\pi_E}[Q_{\hat{\pi_h}}-\tilde{Q}_{\hat{\pi_h}}-\expectE_{a\sim\hat{\pi_h}}[Q_{\hat{\pi_h}}-\tilde{Q}_{\hat{\pi_h}}]]&\leq \expectE_{\tau\sim\pi_E}[Q_{\hat{\pi_h}}-\tilde{Q}_{\hat{\pi_h}}+\abs{\expectE_{a\sim\hat{\pi_h}}[Q_{\hat{\pi_h}}-\tilde{Q}_{\hat{\pi_h}}]}]\nonumber\\
&=\expectE_{\tau\sim\pi_E}[Q_{\hat{\pi_h}}-\tilde{Q}_{\hat{\pi_h}}]+\abs{\expectE_{s_t,u_t\sim\pi_E,a\sim\hat{\pi_h}}[Q_{\hat{\pi_h}}-\tilde{Q}_{\hat{\pi_h}}]}\nonumber\\
&\leq \abs{\expectE_{\tau\sim\pi_E}[Q_{\hat{\pi_h}}-\tilde{Q}_{\hat{\pi_h}}]}+T\delta\label{eq:tvd_action}\\
&\leq 2T\delta\nonumber
\end{align}

where~\cref{eq:tvd_action} holds by applying~\cref{eq:second_part_bound} because the expectation of the trajectories (and their transitions) are over $\pi_E$, and the actions which are used only as arguments into the $Q$ function are sampled from $\hat{\pi_h}$.


Next, we bound the first part of~\cref{eq:pdl}. Recall that the ill-posedness of the problem for a policy class $\Pi$ is
\begin{align*}
    \ill(\Pi,k)=\sup_{\pi\in\Pi} \frac{\norm{\pi_E-\pi}_2}{\norm{\expectE[a_t-\pi(h_t)\lvert h_{t-k}]}_2}
\end{align*}
where $\norm{\pi_E-\pi}_2$ is the RMSE and $\norm{\expectE[a_t-\pi(s_t)\lvert s_{t-k}]}_2$ is the CMR error from our algorithm. Since the learned policy $\hat{\pi_h}$ have CMR error of $\epsilon$, we have that
\begin{align*}
\norm{\pi_E-\hat{\pi_h}}_2\leq \ill(\Pi,k){\norm{\expectE[a_t-\hat{\pi_h}(h_t)\lvert h_{t-k}]}_2} \leq \ill(\Pi,k)\epsilon 
\end{align*}
Next, recall that c-total variation stability of a distribution $P(u^\epsilon)$ where $u^\epsilon\in A$ for some space $A$ implies for two elements $a_1,a_2\in A$,
\begin{align*}
\norm{a_1-a_2}_2\leq\Delta \implies \delta_{TV}(a_1+u^\epsilon,a_2+u^\epsilon)\leq c\Delta.
\end{align*}
Since $P(u^\epsilon_t)$ is c-TV stable w.r.t the action space $A$, we have that for all history trajectories $h_t\in H$ (note that $s_t\in h_t$)
\begin{align*}
\delta_{TV}(\pi_E(s_t)+u^\epsilon_t,\hat{\pi_h}(h_t)+u^\epsilon_t)&\leq c\norm{\pi_E(s_t)-\hat{\pi_h}(h_t)}_2.
\end{align*}
Then, we have that by Jensen's inequality,
\begin{align*}
\expectE_{h_t\sim \pi_E}[\delta_{TV}(\pi_E(s_t)+u^\epsilon_t,\hat{\pi_h}(h_t)+u^\epsilon_t)]^2&\leq \expectE_{h_t\sim \pi_E}[\delta_{TV}(\pi_E(s_t)+u^\epsilon_t,\hat{\pi_h}(h_t)+u^\epsilon_t)^2]\\
\implies\expectE_{h_t\sim \pi_E}[\delta_{TV}(\pi_E(s_t)+u^\epsilon_t,\hat{\pi_h}(h_t)+u^\epsilon_t)]&\leq \sqrt{\expectE_{h_t\sim \pi_E}[\delta_{TV}(\pi_E(s_t)+u^\epsilon_t,\hat{\pi_h}(h_t)+u^\epsilon_t)^2]}\\
&\leq \sqrt{c^2\expectE_{h_t\sim \pi_E}[\norm{\pi_E(s_t)-\hat{\pi_h}(h_t)}^2_2]}\\
&=c \norm{\pi_E-\hat{\pi_h}}_2\leq c\epsilon\ill(\Pi,k)
\end{align*}

Therefore, by applying the total variation distance bound for expectations of $\tilde{Q}_{\hat{\pi_h}}$ over different distributions of action $a_t$, we have that
\begin{align}
\expectE_{\tau\sim \pi_E}[\tilde{Q}_{\hat{\pi_h}}-\expectE_{a\sim\hat{\pi_h}}[\tilde{Q}_{\hat{\pi_h}}]]&=\expectE_{\tau\sim \pi_E}[\tilde{Q}_{\hat{\pi_h}}(h_t,a_t)-\expectE[\tilde{Q}_{\hat{\pi_h}}(h_t,\hat{\pi_h}(h_t))]]\\
&=\expectE_{h_t\sim \pi_E}[\expectE[\tilde{Q}_{\hat{\pi_h}}(h_t,\pi_E(s_t)+u^\epsilon_t)]-\expectE[\tilde{Q}_{\hat{\pi_h}}(h_t,\hat{\pi_h}(h_t)+u^\epsilon_t)]]\\
&\leq \norm{\tilde{Q}_{\hat{\pi_h}}}_\infty \expectE_{h_t\sim \pi_E}[\delta_{TV}(F(\pi_E(s_t)+u^\epsilon_t),F(\hat{\pi_h}(h_t)+u^\epsilon_t))]\\
&\leq T c\epsilon\ill(\Pi,k) 
\end{align}

Combining all of above, we see that from~\cref{eq:pdl}, by selecting $\tilde{Q}_{\hat{\pi_h}}(h_t,a_t)\coloneqq Q_{\hat{\pi_h}}(s_t,a_t,\expectE_{\tau\sim \pi_E}[u^o_t\lvert h_t])$, the imitation gap can be bounded by
\begin{align}
   J(\pi_E)-J(\hat{\pi_h})&=\sum_{t=1}^T \expectE_{\tau\sim\pi_E}[\tilde{Q}_{\hat{\pi_h}}-\expectE_{a\sim\hat{\pi_h}}[\tilde{Q}_{\hat{\pi_h}}]]+\sum_{t=1}^T \expectE_{\tau\sim\pi_E}[Q_{\hat{\pi_h}}-\tilde{Q}_{\hat{\pi_h}}-\expectE_{a\sim\hat{\pi_h}}[Q_{\hat{\pi_h}}-\tilde{Q}_{\hat{\pi_h}}]]\\
    &\leq\sum_{t=1}^T Tc\epsilon\ill(\Pi,k) +\sum_{t=1}^T 2T\delta\\
    &\leq T\cdot (Tc\epsilon\ill(\Pi,k) + 2T\delta)\\
   &= T^2(c\epsilon\ill(\Pi,k)+2\delta)=\mathcal{O}(T^2(\epsilon+\delta)),
\end{align}
which concludes the proof.
\end{proof}

\subsection{Proofs of Corollaries}\label{appendix:corollaries}

\textbf{Corollary~\ref{corollary:noUo}:} In the special case that $u^o_t = 0$, meaning that there is no confounder observable to the expert, or $u^o_t=\expectE_{\pi_E}[u^o_t\lvert h_t]$, meaning that $u^o_t$ is $\sigma(h_t)$ measurable (all information regarding $u^o_t$ is represented in the history), the imitation gap bound is $T^2(c\epsilon\ill(\Pi,k))$, which coincides with Theorem 5.1 of~\citet{Swamy2022_temporal}.

\begin{proof}
If $u^o_t=0$, then we have $u^o_t=\expectE_{\pi_E}[u^o_t\lvert h_t]$ since $u^o_t$ is a constant. If $u^o_t=\expectE_{\pi_E}[u^o_t\lvert h_t]$, we have that 
\begin{align*}
\delta_{TV}(u^o_t,\expectE_{\pi_E}[u^o_t\lvert h_t])=\delta_{TV}(u^o_t,u^o_t)\leq 0
\end{align*}
By plugging $\delta=0$ into~\cref{thm:gap}, we have that $J(\pi_E)-J(\hat{\pi_h})\leq T^2(c\epsilon\ill(\Pi,k))$, which is the same as the imitation gap derived in~\citet{Swamy2022_temporal} and completes the proof.
\end{proof}

\textbf{Corollary~\ref{corollary:unconfounded}:} In the special case that $u^\epsilon_t=0$, if the learned policy via supervised BC have error $\epsilon$, then the imitation gap bound is $T^2(\frac{2}{\sqrt{\dim(A)}}\epsilon+2\delta)$, which is a concrete bound that extends the abstract bound in Theorem 5.4 of~\citet{Swamy2022}.

\begin{proof}
In Theorem 5.4 of~\citet{Swamy2022}, for the offline case, which is the setting we are considering (as opposed to the online settings), they defined the following quantities for bounding the imitation gap in a very general fashion,
\begin{align*}
\epsilon_{\text{off}}&\coloneqq\sup_{\tilde{Q}}\expectE_{\tau\sim\pi_E}[\tilde{Q}-\expectE_{a\sim\hat{\pi_h}}[\tilde{Q}]]\\
\delta_{\text{off}}&\coloneqq\sup_{Q\times\tilde{Q}}\expectE_{\tau\sim\pi_E}[Q_{\hat{\pi_h}}-\tilde{Q}-\expectE_{a\sim\hat{\pi_h}}[Q_{\hat{\pi_h}}-\tilde{Q}]].
\end{align*}

The imitation gap by Theorem 5.4 in~\citet{Swamy2022} under the assumption that $u^\epsilon_t=0$ is $T^2(\epsilon_{\text{off}}+\delta_{\text{off}})$, which can also be deduced from~\cref{eq:pdl} by naively applying the above supremum. To obtain a concrete bound, we can provide a tighter bound for $\expectE_{\tau\sim \pi_E}[\tilde{Q}_{\hat{\pi_h}}-\expectE_{a\sim\hat{\pi_h}}[\tilde{Q}_{\hat{\pi_h}}]]$, which is the first part of~\cref{eq:pdl}, given that $u^\epsilon_t=0$.


For two elements $a_1,a_2\in A$, we have that by Cauchy–Schwarz,
\begin{align*}
\delta_{TV}(a_1+0,a_2+0)=\frac{1}{2}\norm{a1-a2}_1\leq\frac{\sqrt{\dim(A)}}{2}\norm{a1-a2}_2.
\end{align*}
Then, we have that
\begin{align*}
\norm{a_1-a_2}_2\leq\Delta \implies \delta_{TV}(a_1,a_2)\leq \frac{2}{\sqrt{\dim(A)}}\Delta
\end{align*}
so that by~\cref{thm:gap},

\begin{align}
\expectE_{\tau\sim \pi_E}[\tilde{Q}_{\hat{\pi_h}}-\expectE_{a\sim\hat{\pi_h}}[\tilde{Q}_{\hat{\pi_h}}]]&=\expectE_{\tau\sim \pi_E}[\tilde{Q}_{\hat{\pi_h}}(h_t,a_t)-\expectE[\tilde{Q}_{\hat{\pi_h}}(h_t,\hat{\pi_h}(h_t))]]\\
&=\expectE_{h_t\sim \pi_E}[\expectE[\tilde{Q}_{\hat{\pi_h}}(h_t,\pi_E(s_t))]-\expectE[\tilde{Q}_{\hat{\pi_h}}(h_t,\hat{\pi_h}(h_t))]]\\
&\leq \norm{\tilde{Q}_{\hat{\pi_h}}}_\infty \frac{2}{\sqrt{\dim(A)}}\norm{\pi_E-\pi}_2\\
&\leq T \frac{2}{\sqrt{\dim(A)}}\epsilon,
\end{align}
since when $u^\epsilon_t=0$ the learning error via supervised learning is $\epsilon:=\norm{\pi_E-\pi}_2$. Therefore, the final imitation bound following~\cref{thm:gap} is
\begin{align}
   J(\pi_E)-J(\hat{\pi_h})&=\sum_{t=1}^T \expectE_{\tau\sim\pi_E}[\tilde{Q}_{\hat{\pi_h}}-\expectE_{a\sim\hat{\pi_h}}[\tilde{Q}_{\hat{\pi_h}}]]+\sum_{t=1}^T \expectE_{\tau\sim\pi_E}[Q_{\hat{\pi_h}}-\tilde{Q}_{\hat{\pi_h}}-\expectE_{a\sim\hat{\pi_h}}[Q_{\hat{\pi_h}}-\tilde{Q}_{\hat{\pi_h}}]]\\
    &\leq\sum_{t=1}^T T\frac{2}{\sqrt{\dim(A)}}\epsilon+\sum_{t=1}^T 2T\delta\\
   &= T^2(\frac{2}{\sqrt{\dim(A)}}\epsilon+2\delta).
\end{align}

This bound is a concrete bound, obtained through detailed analysis of the problem at hand, that coincides with the abstract bound $T^2(\epsilon_{\text{off}}+\delta_{\text{off}})$ provided in Theorem 5.4 of~\citet{Swamy2022_temporal}. Note that this bound is independent of the ill-posedness $\ill(\Pi,k)$ and the c-TV stability of $u^\epsilon_t$, which are present in the bound of~\cref{thm:gap}, because of the lack of hidden confounders $u^\epsilon_t$.
\end{proof}


\section{Environments and Tasks}
\subsection{Dynamic Aeroplane Ticket Pricing}\label{appendix:ticket}
Here, we provide details regarding the dynamic aeroplane ticket pricing environment introduced in Example~\ref{eg:plane}. The environment and the expert policy are defined as follows:
\begin{align}
\states&\coloneqq\realNumber\\
\actions&\coloneqq[-1,1]\\
s_t&=sign(s)\cdot u^o_t - u^\epsilon_t\\
\pi_E&=clip(-s/u^o_t,-1,1)\\
a_t&=\pi_E+10\cdot u^\epsilon_t\\
u^o_t&=mean(p_t\sim \text{Unif} [-1,1],p_{t-1},....p_{t-M})\\
u^\epsilon_t&=mean(q_t\sim \text{Normal}(0,0.1\cdot\sqrt{k}),q_{t-1},...,q_{t-k+1})
\end{align}
where $M$ is the influence horizon of the expert-observable $u^o$, which we set to 30. The states $s_t$ are the profits at each time step, and the actions $a_t$ are the final ticket price. $u^o_t$ represent the seasonal patterns, where the expert $\pi_E$ will try to adjust the price accordingly. $u^\epsilon_t$ represent the operating costs, which are additive both to the profit and price. Both $u^o_t$ and $u^\epsilon_t$ are the mean over a set of i.i.d samples, $q_t$ and $p_t$, and vary across the time steps by updating the elements in the set at each time step. This constructions allows $u^\epsilon_t$ and $u^\epsilon_{t-k}$ to be independent since all set elements $q_t$ will be re-sampled from time step $t-k$ to $t$. We multiply the standard deviation of $q_t$ by $\sqrt{k}$ to make sure $u^\epsilon_t$, which is the average over $k$ i.i.d variables, have the same standard deviation for all choices of $k$.

\subsection{Mujoco Environments}\label{appendix:mujoco}
We evaluate DML-IL on three Mujoco environments: Ant, Half Cheetah and Hopper. The original tasks do not contain hidden variables, so we modify the environment to introduce $u^\epsilon$ and $u^o$. We use the default transition, state and action space defined in the Mujoco environment. However, we changed the task objectives by altering the reward function and added confounding noise to both the state and action. Specifically, instead of controlling the ant, half cheetah and hopper, respectively, to travel as fast as possible, the goal is to control the agent to travel at a target speed that is varying throughout an episode. This target speed is $u^o$, which is observed by the expert but not recorded in the dataset. In addition, we add confounding noise $u^\epsilon_t$ to $s_t$ and $a_t$ to mimic the environment noise such as wind noise. In all cases, the target speed $u^o_t$, confounding noise $u^\epsilon_t$ and the action $a_t$ are generated as follows:
\begin{align}
a_t&=\pi_E+20\cdot u^\epsilon_t\\
u^o_t&=mean(p_t\sim \text{Unif} [-2,4],p_{t-1},....p_{t-M})\\
u^\epsilon_t&=mean(q_t\sim \text{Normal}(0,0.01\cdot\sqrt{k}),q_{t-1},...,q_{t-k+1})
\end{align}
where $M=30$, the state transitions follow the default Mujoco environment and the expert policy $\pi_E$ is learned online in the environment. $u^o_t$ and $u^\epsilon_t$ follow the aeroplane ticket pricing environment to be the average over a queue of i.i.d random variables. The reward is defined to be the $1_{healthy}-(\text{current velocity}-u^o_t)^2-\text{control loss}$, where $1_{healthy}$ gives reward $1$ as long as the agent is in a healthy state as defined in the Mujoco documentation. The second penalty term penalises deviation between the current agent's velocity and the target velocity $u^o_t$. The control loss term is also as defined in default Mujoco, which is $0.1*\sum(a_t^2)$ at each step to regularize the size of actions.
\subsubsection{Ant}

In the Ant environment, we follow the gym implementation~\footnote{Ant environment: \url{https://www.gymlibrary.dev/environments/mujoco/ant/}} with 8-dimensional action space and 28-dimensional observable state space, where the agent's position is also included in the state space. Since the target speed $u^o_t$ is not recorded in the trajectory dataset, we scale the current position of the agent with respect to the target speed, $pos_t^\prime= pos_{t-1}+\frac{pos_t-pos_{t-1}}{u^o_t}$, and use the new agent position $pos_t^\prime$ in the observed states. This allows the imitator to infer information regarding $u^o_t$ from trajectory history, namely from the rate of change in the past positions.


\subsubsection{Half Cheetah}
In the Half Cheetah environment, we follow the gym implementation~\footnote{Half Cheetah environment: \url{https://www.gymlibrary.dev/environments/mujoco/half_cheetah/}} with 6-dimensional action space and 18-dimensional observable state space, where the agent's position is also included in the state space. Similarly to the Ant environment, we scale the current position of the agent to $pos_t^\prime= pos_{t-1}+\frac{pos_t-pos_{t-1}}{u^o_t}$ such that the imitator can infer information regarding $u^o_t$ from trajectory history.

\subsubsection{Hopper}
In the Hopper environment, we follow the gym implementation~\footnote{Hopper environment: \url{https://www.gymlibrary.dev/environments/mujoco/hopper/}} with 3-dimensional action space and 12-dimensional observable state space, where the agent's position is also included in the state space. Similarly to the Ant environment, we scale the current position of the agent to $pos_t^\prime= pos_{t-1}+\frac{pos_t-pos_{t-1}}{u^o_t}$ such that the imitator can infer information regarding $u^o_t$ from trajectory history.

\section{Implementation Details}
\subsection{DML-IL with $K$-Fold Cross-Fitting}\label{appendix:dmlil}
\begin{algorithm}[tb]
   \caption{DML-IL with $K$-fold cross-fitting}
   \label{alg:DML-IL-kfold}
\begin{algorithmic}
   \STATE {\bfseries Input:} Dataset $\dataset_E$ of expert demonstrations, Confounding noise horizon $k$, number of folds $K$ for cross-fitting
    \STATE {\bfseries Output:} A history-dependent imitator policy $\hat{\pi}_h$
   \STATE Get a partition $(I_k)^K_{k=1}$ of dataset indices $[N]$ of trajectories
   \FOR{$k=1$ {\bfseries to} $K$}
   \STATE $I^c_k\coloneqq[N]\setminus I_k$
   \STATE Initialize the roll-out model $\hat{M}_i$ as a mixture of Gaussians model
   \REPEAT
   \STATE Sample $(h_{t},a_t)$ from data $\{(\dataset_{E,i}):{i\in I^c_k}\}$
   \STATE Fit the roll-out model $(h_t,a_t)\sim\hat{M}_i(h_{t-k})$ to maximize log likelihood
   \UNTIL{convergence}
    \ENDFOR
   \STATE Initialize the expert model $\hat \pi_h$ as a neural network
   \REPEAT
\FOR{$k=1$ {\bfseries to} $K$}
   \STATE Sample $h_{t-k}$ from $\{(\dataset_{E,i}):{i\in I_k}\}$
   \STATE Generate $\hat{h}_t$ and $\hat{a}_t$ using the roll-out model $\hat{M}_i$
   \STATE Update $\hat \pi_h$ to minimise the loss $\ell:= \norm{\hat{a}_t - \hat{\pi}_h (\hat h_t)}_2$
   \ENDFOR
    \UNTIL{convergence}
\end{algorithmic}
\end{algorithm}
Here, we outline DML-IL with $K$-fold cross-fitting, which ensures unbiased estimation and improves training stability. The algorithm is shown in~\cref{alg:DML-IL-kfold}. The dataset is partitioned into $K$ folds based on the trajectory index. For each fold, we use the leave-out data, that is, indices $I^c_k\coloneqq[N]\setminus I_k$, to train separate roll-out models $\hat{M}_i$ for $i\in[1..K]$. Then, to train a single expert model $\hat{\pi}_h$, we sample the trajectory history $h_{t-k}$ from each fold and use the roll-out model trained with the leave-out data to complete the trajectory and train $\hat{\pi}_h$. This technique is very important in Double Machine Learning (DML) literature~\citep{Shao2024,Chernozhukov2018Double} for it provides both empirical stability and theoretical guarantees. The base IV regression algorithm DML-IV with $K$-fold cross-fitting is theoretically shown to converge at the rate of $O(N^{-1/2})$~\citep{Shao2024}, where $N$ is the sample size, under technical regularity and identifiability conditions (see~\citet{Shao2024} for the technical conditions). These conditions are typically assumed for similar theoretical analyses, and DML-IL with $K$-fold cross-fitting will thus inherit this convergence rate guarantee if the regularity conditions are satisfied.


\subsection{Expert Training}

The expert in the aeroplane ticket pricing environment is explicitly hand crafted. For the Mujoco environments, we used the Stable-Baselines3~\citep{stable-baselines3} implementation of soft actor-critic (SAC) and the default hyperparameters for each task outlined by Stable-Baseline3. The expert policy is an MLP with two hidden layers of size 256 and ReLU activations, and we train the expert for $10^7$ steps.


\subsection{Imitator Training}

With the expert policy $\pi_E$, we generate 40 expert trajectories, each of 500 steps, following our previously defined environments. Specifically, the confounding noise is added to the state and actions and crucially $u^o_t$ is not recorded in the trajectories. The naive BC directly learns $\expectE[a_t\mid s_t]$ via supervised learning. ResiduIL mainly follows the implementation of~\citet{Swamy2022_temporal}, where we adopt it to allow longer confounding horizon $k>1$. For DML-IL and BC-SEQ, a history-dependent policy is used, where we fixed the look-back length to be $k+3$, where $k$ is the confounding horizon. BC-SEQ then just learns $\expectE[a_t\mid h_t]$ via supervised learning, and DML-IL is implemented with $K$-fold following~\cref{alg:DML-IL-kfold}. The policy network architecture for BC, BC-SEQ and ResiduIL are 2 layer MLPs with 256 hidden size. The policy network $\hat{\pi}_h$ and the mixture of Gaussians roll-out model $\hat{M}$ for DML-IL have similar architecture, with details provided in~\cref{tab:dml-il}. We use AdamW optimizer with weight decay of $10^{-4}$ and learning rate of $10^{-4}$. The batch size is 64 and each model is trained for 150 epochs, which is sufficient for their convergence.

\subsection{Imitator Evaluation}

The trained imitator is then evaluated for 50 episodes, each 500 steps in the respective confounded environments. The average reward and the mean squared error between the imitator's action and the expert's action are recorded.


\begin{table}[t]
    \caption{Network architecture for DML-IL. For mixture of Gaussians output, we report the number of components. No dropout is used.}
    \centering
    \subfloat[Roll-out model $\hat{M}$]{
    \begin{tabular}{||c|c||}
    \hline
    \textbf{Layer Type} & \textbf{Configuration}  \\ [0.5ex]
    \hline \hline
    Input & state dim $\times$ 3\\      \hline
    FC + ReLU & Out: 256\\    \hline
    FC + ReLU & Out: 256\\    \hline
    MixtureGaussian & 5 components; Out: state dim $\times$ k\\    \hline
    \end{tabular}\label{tab:rollout_arch}}
    \hspace{30pt}
    \subfloat[Policy model $\hat{\pi}_h$]{
    \begin{tabular}{||c|c||}
    \hline
    \textbf{Layer Type} & \textbf{Configuration}  \\ [0.5ex]
    \hline \hline
    Input & state dim$\times$ (k+3)\\      \hline
    FC + ReLU & Out: 256\\    \hline
    FC + ReLU & Out: 256\\    \hline
    FC & Out: action dim\\    \hline
    \end{tabular}\label{tab:policy_arch}}
    \label{tab:dml-il}
\end{table}





\section{Adopting other IV regression algorithms}\label{appendix:otheriv}

In this paper, we have transformed causal IL with hidden confounders into a set of CMRs as defined in~\cref{eq:CMR}. Therefore, in principle many IV regression algorithms can be adopted to solve our CMRs. We also experimented with other IV regression algorithms that have been previously shown to be practical~\citep{Shao2024} for different tasks and high-dimensional input. Specifically, we experimented with DFIV~\citep{Xu2020}, which is an iterative algorithm that integrates the training of two models that depend on each other, and DeepGMM~\citep{Bennett2019DeepAnalysis}, which solves a minimax game by optimising two models adversarially. Note that DeepIV~\citep{Hartford2017DeepPrediction} can be considered a special case of DML-IV~\citep{Shao2024}, so we did not reimplement it. The additional results for using DFIV and DeepGMM as the CMRs solver are provided in~\cref{fig:additional_toy} and~\cref{fig:additional_ant}. It can be seen from~\cref{fig:additional_toy} that only DFIV achieves good performance in the aeroplane ticket pricing environment, surpassing the performance of ResiduIL. For the Ant Mujoco task in~\cref{fig:additional_ant}, both DFIV and DeepGMM fail to learn good policies, with only slightly lower MSE than BC and BC-SEQ. We think this is due to the high-dimensional state and action spaces and the inherent instability in the DFIV and DeepGMM algorithms. For DFIV, the interleaving of training of two models causes highly non-stationary training targets for both models, and, for DeepGMM, the adversarial training procedure of two models is similar to that of generative adversarial Networks (GANs), which is known to be unstable and difficult to train.

We conclude that solving the CMRs for an imitator policy can be sensitive to the choice of solver as well as the choice of hyperparameters. In addition, some IV regression algorithms do not work well with high dimension inputs. Our IV algorithm of choice, DML-IV, provides a robust base for the DML-IL algorithm that demonstrated good performance across all tasks and environments.


\begin{figure*}[t]
\begin{subfigure}[c]{0.4\textwidth}
\centering\includegraphics[width=1\textwidth]{figures/appendix/toy_add_mse.png}
\end{subfigure}
\centering
\begin{subfigure}[c]{0.4\textwidth}
\centering
\includegraphics[width=1\textwidth]{figures/appendix/toy_add_rew.png}
\end{subfigure}
\begin{subfigure}[c]{0.1\textwidth}
\centering
\includegraphics[width=1\textwidth]{figures/appendix/additional_legend.png}
\end{subfigure}
\vspace{-8pt}
\caption{Additional results for the MSE between learnt policy and expert, and the average reward, in the plane ticket environment (Example~\ref{eg:plane}), with DFIV and DeepGMM as the CMRs solver.}
\label{fig:additional_toy}
\end{figure*}

\begin{figure*}[t]
\begin{subfigure}[c]{0.4\textwidth}
\centering\includegraphics[width=1\textwidth]{figures/appendix/ant_add_mse.png}
\end{subfigure}
\centering
\begin{subfigure}[c]{0.4\textwidth}
\centering
\includegraphics[width=1\textwidth]{figures/appendix/ant_add_rew.png}
\end{subfigure}
\begin{subfigure}[c]{0.1\textwidth}
\centering
\includegraphics[width=1\textwidth]{figures/appendix/additional_legend.png}
\end{subfigure}
\vspace{-8pt}
\caption{Additional results for the MSE between learnt policy and expert, and the average reward, Ant Mujoco environment, with DFIV and DeepGMM as the CMRs solver.}
\label{fig:additional_ant}
\end{figure*}