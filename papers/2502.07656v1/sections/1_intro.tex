\section{Introduction}\label{sec:intro}
Imitation Learning (IL) has emerged as a prominent paradigm in machine learning, where the objective is to learn a policy that mimics the behaviour of an expert by learning from its demonstrations. While classical IL theory implies that, with infinite data, the IL error should converge to zero and the imitator should be value-equivalent to the expert~\citep{Ross2011,Spencer2021}, it has been observed in practice that IL algorithms often produce incorrect estimates of the expert policy, leading to suboptimal and unsafe behaviours~\citep{Lecun2005,Codevilla2019,Bansal2018,Kuefler2017}. 

Various attempts have been made to explain the cause of these problems. Previous work has studied spurious correlations within the dataset~\citep{deHaan2019,Codevilla2019,Pfrommer2023}, temporal noise~\citep{Swamy2022_temporal}, the case where the expert has additional information or knowledge~\citep{Swamy2022,Vuorio2022,Chen2019,Choudhury2017}, causal delusions~\citep{Ortega2008,Ortega2021} and covariate shifts~\citep{Spencer2021}. 
However, each of these works considers only a specific aspect of the problem in different settings, and a holistic treatment of the problem and discussion of the connections between these settings is still missing from the literature. 
In the real world, it is often the case that multiple problems are present simultaneously (e.g., the expert has privileged information and the observed demonstrations are confounded). However, as we demonstrate later, addressing these problems partially or independently is insufficient, and a unified approach is necessary. 
%When this is the case, we show that it is insufficient to address these issues separately, but instead a unified approach is required. 

Our key observation is that, in fact, all the above settings can be formalised within a unifying framework by considering \textit{hidden confounders}, which are variables present in the environment but not recorded in the demonstrations. Importantly, we distinguish between hidden confounders that can and cannot be observed by the expert.\footnote{Note that we always assume that the imitator does not observe the hidden confounders.} 
When the expert fully observes the hidden confounders, but the imitator does not, additional information is available to the expert. When the expert also cannot observe the hidden confounders, these hidden confounders act as confounding noise that contaminates the demonstrations, 
%and confuses the imitator, 
causing spurious correlations and causal delusions. By considering the hidden confounders to include expert-observable and expert-unobservable parts, we propose a novel and unifying causal IL framework that can interpolate between the two scenarios. This framework not only unifies existing settings, but also enables us to consider a much larger family of problems that are more realistic in practice.


Building on our unifying framework, we aim to develop algorithms that can correctly and efficiently imitate the expert policy, even in the presence of hidden confounders. While interactive IL algorithms, such as DAgger~\citep{Ross2011}, have shown promise in addressing specific issues with hidden confounders by allowing direct queries to the expert~\citep{Swamy2022,Vuorio2022,Swamy2022_temporal}, this approach relies on access to an interactive expert. However, such an assumption is impractical in many real-world scenarios, where only a fixed set of demonstrations is available. For this reason, here we focus on designing methods that mitigate the effects of hidden confounders while relying solely on (a fixed set of) non-interactive expert demonstrations.


% In our novel and general framework, we aim to propose algorithms that can correctly and efficiently imitate the expert policy. In other settings, it has been shown that the application of an interactive IL algorithm such as DAgger~\citep{Ross2011}, which allows us to directly query the expert, can be effective in dealing with hidden confounders (e.g.,~\citep{Swamy2022,Vuorio2022,Swamy2022_temporal}). However, an interactive expert is not a realistic assumption in many domains and applications. Therefore, in our general framework, we aim to develop approaches that solely rely on a fixed set of demonstrations. 


 Our key idea is to leverage the trajectory histories as Instrumental Variables (IVs) to break the spurious correlations introduced by expert-unobservable hidden confounders. Moreover, by conditioning on the trajectory history, we can infer information about expert-observable hidden confounders and learn a history-dependent policy that accurately mimics the expert's behaviour. Importantly, we show that, in our general framework, IL in the presence of hidden confounders can be reformulated as a set of Conditional Moment Restrictions (CMRs)---a well-studied problem in econometrics and causal inference. This reformulation allows us to design practical algorithms with theoretical guarantees on the imitation gap based on efficient algorithms from causal inference for solving CMRs. 


% The key idea is to leverage the trajectory history as Instrumental Variables (IVs) to break the spurious correlations caused by the expert-unobservable hidden confounders, and to infer information about the expert-observable hidden confounders from the expert's past trajectory by learning a history-dependent policy conditioned on the trajectory history. 
% Crucially, in our general framework, we show that IL in the presence of hidden confounders can be reduced to a set of Conditional Moment Restrictions (CMR), which is a problem widely studied in the econometrics and causal inference literature. This allows us to design practical algorithms with theoretical guarantees on the imitation gap based on efficient algorithms from causal inference for solving CMRs.



% For example, in autonomous driving, external factors such as weather conditions, traffic signals, or road quality might not be fully captured in the dataset, yet they significantly affect the expert's driving decisions. Similarly, in healthcare, patient outcomes influenced by socio-economic factors or prior treatments might not be explicitly recorded in the data but are crucial for accurate imitation of expert decisions.


\paragraph{Main Contributions.}
\begin{itemize}[leftmargin=10pt, topsep=0pt, itemsep=2pt]
    \item We propose a novel unifying framework for causal IL (Section~\ref{sec:setting}). We consider hidden confounders that include expert-observable and expert-unobservable confounding variables to unify and generalise many of the settings in prior work. 
    \item We show that IL in our framework can be reduced to a set of CMRs by leveraging trajectory histories as instruments to learn a history-dependent policy (Section~\ref{sec:method}).
    \item We provide a novel algorithm for causal IL (\cref{alg:DML-IL}): based on existing IV regression algorithms, we propose DML-IL to imitate the expert policy in our framework and provide an upper bound on the imitation gap that recovers previous results as special cases (Theorem~\ref{thm:gap}).  
    \item Empirically, we show that our algorithms perform well in challenging instances of our general setting, outperforming state-of-the-art methods (Section~\ref{sec:exps}). 
\end{itemize}


\subsection{Related Works}
\paragraph{Imitation Learning.}
Imitation learning considers the problem of learning from demonstrations~\citep{Pomerleau1988,Lecun2005}. Standard IL methods include Behaviour Cloning~\citep{Pomerleau1988}, Inverse RL~\citep{Russell1998}, and adversarial methods~\citep{Ho2016}. Interactive IL~\citep{Ross2011} extends standard IL by allowing the imitator to query an interactive expert, facilitating recovery from mistakes. However, in this paper, we do not assume query access to an interactive expert.


\vspace{-4pt}
\paragraph{Causal Imitation Learning.}
Recently, it has been shown that IL from offline trajectories can suffer from the existence of latent variables~\citep{Ortega2021}, which cause causal delusion. This can be resolved by learning an interventional policy. Following this discovery, various methods~\citep{Vuorio2022,Swamy2022} considered IL when the expert has access to the full hidden context that is fixed throughout each episode, but the imitator does not observe the hidden context. 
They aim to learn an interventional policy through on-policy IL algorithms that require an interactive demonstrator and/or an interactive simulator (e.g., DAgger~\citep{Ross2011}). 

Orthogonal to these works,~\citet{Swamy2022_temporal} consider latent variables not known to the expert, which act as confounding noise that affects the expert policy, but not the transition dynamics. To address this challenge, the problem is then cast into an IV regression problem. 
Our work combines and generalises the above works \citep{Vuorio2022,Swamy2022,Swamy2022_temporal} to allow the latent variables to be only partly known to the expert, evolving through time in each episode and directly affecting both the expert policy and the transition dynamics. Solving this generalisation implies solving the above problems simultaneously. 

Causal confusion~\citep{deHaan2019,Pfrommer2023} considers the situation where the expert's actions are spuriously correlated with non-causal features of the previous observable states. While it is implicitly assumed that there are no latent variables present in the environment, we can still model this spurious correlation as the existence of hidden confounders that affect both previous states and current expert actions. Slight variations of this setting have been studied in~\citet{Wen2020,Spencer2021,Codevilla2019}. In~\cref{appendix:reduce}, we explain and discuss how these works can be reduced to special cases of our unifying framework.


From the causal inference perspective~\citep{Kumor2021,Zhang2020}, there have been studies of the theoretical conditions on the causal graph such that the imitator can exactly match the expert performance through backdoor adjustments (\textit{imitability}). Similarly, \citet{Ruan2023} extended imitation conditions and backdoor adjustments to inverse RL. We instead consider a setting where exact imitation is not possible and aim to minimise the imitation gap. Beyond backdoor adjustments, imitability has also been studied theoretically using context-specific independence relations~\citep{Jamshidi2023}.

\vspace{-4pt}
\paragraph{IV Regression and CMRs.} 
In this paper, we transform our causal IL problem into solving a set of CMRs through IVs. Therefore, we briefly introduce IV regression and approaches for solving CMRs. The classic IV regression algorithms mainly consider linear functions~\citep{Angrist1996} and non-linear basis functions~\citep{Newey2003,Chen2018,Singh2019}. More recently, DNNs have been used for function estimation and methods such as DeepIV~\citep{Hartford2017DeepPrediction}, DeepGMM~\citep{Bennett2019DeepAnalysis}, AGMM~\citep{Dikkala2020}, DFIV~\citep{Xu2020} and DML-IV~\citep{Shao2024} have been proposed.

More generally, IV regression algorithms can be generalised to solve CMRs~\citep{Liao2020,Dikkala2020,Shao2024}, specifically linear CMRs, where the restrictions are linear functionals of the function of interest. In our paper, the derived CMRs for causal IL are linear, so the above methods can be adopted.