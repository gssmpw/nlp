\section{Causal IL as CMRs}\label{sec:method}

In this section, we demonstrate that performing causal IL in our framework is possible using trajectory histories as instruments. In the next step, we show that the problem can be described as CMRs and propose an effective algorithm to solve it.

The typical target for IL would be the expert policy $\pi_E$ itself. However, since the expert has access to information, namely $u^o_t$, which the imitator does not, the best thing an imitator can do is to learn a history-dependent policy $\pi_h$ that is the closest to the expert. A natural choice is the conditional expectation of $\pi_E(s_t,u^o_t)$ on the history $h_t$:
\begin{align}
\pi_h(h_t)\coloneqq \expectE_{\probP(u^o_t\mid h_t)}[\pi_E(s_t,u^o_t)]=\expectE[\pi_E(s_t,u^o_t)\mid h_t],\nonumber
\end{align}
% where $p(u^o_t\mid h_t)$ is a distribution over expert-observable confounders and captures the information about $u^o_t$ can be inferred from the trajectory history. 
because the conditional expectation minimizes the least squares criterion~\citep{hastie01statisticallearning} and $\pi_h$ is the best predictor of $\pi_E$ given $h_t$. In $\pi_h$, the distribution $\probP(u^o_t\mid h_t)$ captures the information about $u^o_t$ that can be inferred from trajectory histories.
\begin{remark}
\emph{Learning $\pi_h$ is not trivial. Policies learnt naively using behaviour cloning (i.e., $\expectE[a_t\mid h_t]$) fail to match $\pi_E$. In view of~\cref{eq:action}, we have that
\begin{align} 
\expectE[a_t\mid h_t]&=\expectE[\pi_E(s_t,u^o_t) \mid h_{t}]+\expectE[u^\epsilon_t\mid h_{t}]\nonumber\\
&=\pi_h(h_t)+\expectE[u^\epsilon_t\mid h_{t}],\label{eq:history_policy}
\end{align}
where $\expectE[u^\epsilon_t\mid h_{t}]\neq 0$ due to the spurious correlation between $u^\epsilon_t$ and the trajectory history $h_t$. As a result, $\expectE[a_t\mid h_t]$ becomes biased, which can lead to arbitrarily worse performance compared to $\pi_E$.   }
\end{remark}

\vspace{-5pt}
\paragraph{Derivation of CMRs.} 
Leveraging the confounding horizon from Assumption~\ref{assump:horizon}, it becomes possible to break the spurious correlation using the independence of $u^\epsilon_t$ and $u^\epsilon_{t-k}$. We propose to use the $k$-step trajectory history $h_{t-k}=(s_{1},a_{1},...,s_{t-k})$ as an instrument for the current state $s_t$. Taking the expectation conditional on $h_{t-k}$ in~\cref{eq:history_policy} yields
\begin{align*}
    \expectE[a_t\mid h_{t-k}] & = \expectE\left[\expectE[a_t\mid h_{t}]\mid h_{t-k}\right] \\ & = \expectE[\pi_h(h_t)\mid h_{t-k}]+\expectE[\expectE[u^\epsilon_t\mid h_{t}]\mid h_{t-k}] \\
    & = \expectE[\pi_h(h_t) \mid h_{t-k}]+\expectE[u^\epsilon_t\mid h_{t-k}]
\end{align*}
where we use the fact that $h_{t-k}$ is $\sigma(h_t)$-measurable because $h_{t-k}\subseteq h_t$. Next, recall that $u^\epsilon_t\indep u^\epsilon_{t-k}$ by Assumption~\ref{assump:horizon}, which implies $u^\epsilon_t\indep h_{t-k}$, so that % Hence, since $\expectE[u^\epsilon_t] = 0$, we obtain
\begin{align}
    \expectE[a_t\mid h_{t-k}] &= \expectE[\pi_h(h_t) \mid h_{t-k}]+\expectE[u^\epsilon_t]\nonumber\\
    &=\expectE[\pi_h(h_t) \mid h_{t-k}].
\end{align}

As a result, the problem of learning $\pi_h$ reduces to solving for $\pi_h$ that satisfies the following identity
\begin{align}
    \expectE[a_t-\pi_h(h_t)\mid h_{t-k}]=0,\label{eq:CMR}
\end{align}
which is a CMR problem as defined in~\cref{sec:cmr}. In this case, both $a_t$ and $h_t$ are observed in the confounded expert demonstrations, and $h_{t-k}$ acts as the instrument. 

To make sure the instrument $h_{t-k}$ is valid, we check that it satisfies the conditions of~\cref{assump:iv}. Firstly, we have checked that $u^\epsilon_t\indep h_{t-k}$. Secondly, the environment and the expert policy are non-trivial, which means $\probP(h_t\mid h_{t-k})$ is not constant in $h_{t-k}$. Finally, $h_{t-k}$ indeed only affects $a_t$ through $s_t$ by the Markovian property. However, the strength of the instrument, which informally represents the correlation between the instrument $h_{t-k}$ and $h_t$, plays an important role in how well we can identify $\pi_h(h_t)$ by solving the CMRs in~\cref{eq:CMR}. In particular, we see that, as the confounding horizon $k$ increases, the correlation between $h_{t-k}$ and $h_t$ weakens and $h_{t-k}$ becomes a weaker instrument. This means that it is less able to identify $\pi_h$ via the CMR in~\cref{eq:CMR} and the final learnt imitator will have poorer performance. This is confirmed theoretically in Proposition~\ref{prop:ill-posed} and experimentally in~\cref{sec:exps}, and we will formalise this notion of instrument strength in~\cref{sec:theory}.


% Note this problem is equivalent to solving an IV regression on~\cref{eq:history_policy}, where $Y=\expectE[a_t\lvert h_t]$, $f(x)=\pi_h(h_t)$, $\epsilon=\expectE[u^\epsilon_t$ and the instrument $Z=h_{t-k}$.




\subsection{Practical Algorithms for Solving the CMRs}

\begin{algorithm}[tb]
   \caption{DML-IL}
   \label{alg:DML-IL}
\begin{algorithmic}[1]
   \STATE {\bfseries input} Dataset $\dataset_E$ of expert demonstrations, Confounding noise horizon $k$
   \STATE Initialize the roll-out model $\hat{M}$ as a Gaussian mixture model\label{algo:roll_out_1}
    \REPEAT
   \STATE Sample $(h_{t},a_t)$ from data $\dataset_E$
   \STATE Fit the roll-out model $(h_t,a_t)\sim\hat{M}(h_{t-k})$ to maximize the log likelihood 
\UNTIL{convergence}\label{algo:roll_out_2}
   \STATE Initialize the expert model $\hat \pi_h$ as a neural network
   \REPEAT
   % \FOR{$k=1$ {\bfseries to} $K$}
   \STATE Sample $h_{t-k}$ from $\dataset_E$
   \STATE Generate $\hat{h}_t$ and $\hat{a}_t$ using the roll-out model $\hat{M}$
   \STATE Update $\hat \pi_h$ to minimise the loss $\ell:= \norm{\hat{a}_t - \hat{\pi}_h (\hat h_t)}_2$
   % \ENDFOR
    \UNTIL{convergence}
    \STATE {\bfseries return} A history-dependent imitator policy $\hat{\pi}_h$
\end{algorithmic}
\end{algorithm}

There are various techniques~\citep{Shao2024,Bennett2019,Xu2020,Dikkala2020} for solving the CMRs $\expectE[a_t\lvert h_{t-k}]=\expectE[\pi_h(h_t) \lvert h_{t-k}]$. Here, the \textit{CMR error} that we aim to minimise is given by 
\begin{align*}
\sqrt{\expectE\big[\expectE[a_t-\hat{\pi}_h(h_t)\lvert h_{t-k}]^2\big]}=\norm{\expectE[a_t-\hat{\pi}_h(h_t)\lvert h_{t-k}]}_{2}.    
\end{align*}
In~\cref{alg:DML-IL}, we introduce DML-IL, an algorithm adapted from the IV regression algorithm DML-IV~\citep{Shao2024}\footnote{DML stands for double machine learning~\citep{Chernozhukov2018Double}, which is a statistical technique to ensure fast convergence rate for two-step regression, as is the case in~\cref{alg:DML-IL}.}, which solves our CMRs by minimising the CMR error. The first part of the algorithm (line 3-7) learns a roll-out model $\hat{M}$ that generates a trajectory $k$ steps ahead given $h_{t-k}$. Then, the roll-out model $\hat{M}$ is used to train the policy model $\hat{\pi}_h$ (line 8-13). $\hat{\pi}_h$ takes the generated trajectory $\hat{h}_t$ from $\hat{M}(h_{t-k})$ as inputs, and minimises the mean squared error to the next action. Using generated trajectories is crucial in breaking the spurious correlation caused by $u^\epsilon_t$ between past states and actions, and using the trajectory history before $h_{t-k}$ allows the imitator to infer information about $u^o_t$.

DML-IL can also be implemented with $K$-fold cross-fitting, where the dataset is partitioned into $K$ folds, with each fold alternately used to train $\hat{\pi}_h$ and the remaining folds to train $\hat{M}$. This ensures unbiased estimation and improves the stability of training. The base IV algorithm DML-IV with $K$-fold cross-fitting is theoretically shown to converge at the rate of $O(N^{-1/2})$~\citep{Shao2024}, where $N$ is the sample size, under regularity conditions. DML-IL with $K$-fold cross-fitting (see~\cref{appendix:dmlil} for details) will thus inherit this convergence rate guarantee. 

Note that~\cref{alg:DML-IL} requires the confounding noise horizon $k$ as input. While the exact value of $k$ can be difficult to obtain in reality, any upper bound $\bar{k}$ of $k$ is sufficient to guarantee the correctness of ~\cref{alg:DML-IL}, since $h_{t-\bar{k}}$ is also a valid instrument. Ideally, we would like a data-driven approach to determine $k$. Unfortunately, it is generally intractable to empirically verify whether $h_{t-k}$ is a valid instrument from a static dataset, especially the unconfounded instrument condition (i.e., $h_{t-k}\indep u^\epsilon_t$). Therefore, we rely on the user to provide a sensible choice of $\bar{k}$ based on the environment that does not substantially overestimate $k$.


\subsection{Theoretical Analysis}\label{sec:theory}

% \begin{align}
% p(u_t\lvert do(a_{t-k+1}),...,do(a_{t-1}),s_{t-k+1},...,s_{t-1})&\propto p(h_t)p_{\mu_0}(s_{t-k+1})\prod_{i=t-k+1}^{t-1} \transitions(s_{i+1}\lvert s_i,a_i,u_i)
% \end{align}

% since $$(u_t\indep a_{(t-k+1)...(t-1)} \lvert s_{(t-k+1)...(t_1)})_{\mathcal{G}_{\underline{a{(t-k+1)...(t-1)}}}}$$
% on the causal graph $\mathcal{G}_{\underline{a{(t-k+1)...(t-1)}}}$ where the arrows going into $a_{(t-k+1)...(t-1)}$ are removed.



In this section, we derive theoretical guarantees for our algorithm, focusing on the imitation gap and its relationship with existing work.


On a high level, in order to bound the imitation gap of the learnt policy $\hat{\pi}_h$, i.e., $J(\pi_E)-J(\hat{\pi}_h)$, we need to control:
\begin{enumerate}
    \item[($i$)] The amount of information about the hidden confounders that can be inferred from trajectory histories;
    \item[($ii$)] The ill-posedness (or identifiability) of the set of CMRs, which intuitively measures the strength of the instrument $h_{t-k}$;
    \item[($iii$)] The disturbance of the confounding noise to the states and actions at test time.
\end{enumerate}
These factors are all determined by the environment and the expert policy. To control ($i$), we measure how much information about $u^o_t$ is captured by the trajectory history $h_t$ by analysing the Total Variation (TV) distance between the distribution of $u^o_t$ and $\expectE[u^o_t\lvert h_t]$ along the trajectories of $\pi_E$. To control ($ii$) and ($iii$), we need to introduce the following two key concepts.

\begin{definition}[The ill-posedness of CMRs~\citep{Dikkala2020,Chen2012}]

Given the derived CMRs in~\cref{eq:CMR}, for a policy $\pi\in\Pi$, $\norm{\pi_E-\pi}_2$ is the root mean squared error to the expert and $\norm{\expectE[a_t-\pi(s_t)\lvert s_{t-k}]}_2$ is the CMR error we aim to minimise. Then, the \emph{ill-posedness} $\ill(\Pi,k)$ of the policy space with confounding noise horizon $k$ is given by
\begin{align*}
    \ill(\Pi,k)=\sup_{\pi\in\Pi} \frac{\norm{\pi_E-\pi}_{2}}{\norm{\expectE[a_t-\pi(h_t)\lvert h_{t-k}]}_{2}}.
\end{align*}
\end{definition}
The ill-posedness $\ill(\Pi,k)$ measures the strength of the instrument where a higher $\ill(\Pi,k)$ indicates a weaker instrument. It bounds the ratio between the learning error of the imitator following our CMR objective and its $L_2$ error to the expert policy. 

As discussed previously, intuitively, the strength of the instrument would decrease as the confounding horizon $k$ increases. This is in fact true and is confirmed by the following proposition. The proof is deferred to~\cref{appendix:prop}. 
\begin{proposition}\label{prop:ill-posed}
The ill-posedness $\ill(\Pi,k)$ is monotonically increasing as the confounded horizon $k$ increases.
\end{proposition}

Next, we introduce the notion of c-TV stability.
\begin{definition}[c-total variation stability~\citep{Bassily2021,Swamy2022_temporal}]
Let $P(X)$ be the distribution of a random variable $X:\Omega\rightarrow \mathcal{X}$. $P(X)$ is c-TV stable if for $a_1,a_2\in \mathcal{X}$ and $\Delta>0$,
\begin{align*}
\norm{a_1-a_2}\leq\Delta \implies \delta_{TV}(a_1+X,a_2+X)\leq c\Delta.
\end{align*}
where $\norm{\cdot}$ is some norm defined on $\mathcal{X}$ and $\delta_{TV}$ is the total variation distance.
\end{definition}
A wide range of distributions are c-TV stable. For example, standard normal distributions are $\frac{1}{2}$-TV stable. We apply this notion to the distribution over $u^\epsilon_t$ to bound the disturbance it induces in the trajectory and the expected return.

With the notion of ill-posedness and c-TV stability, we can now analyse and upper bound the imitation gap $J(\pi_E)-J(\hat{\pi}_h)$ by controlling the three components $(i)-(iii)$ discussed above. 
% We present the main result for this paper, where t
The full proof is deferred to~\cref{appendix:gap}.

\begin{theorem}[Imitation Gap Bound]\label{thm:gap}
Let $\hat{\pi}_h$ be the learnt policy with CMR error $\epsilon$ and let $\ill(\Pi,k)$ be the ill-posedness of the problem. Assume that $\delta_{TV}(u^o_t,\expectE_{\pi_E}[u^o_t\lvert h_t])\leq\delta$ for $\delta\in\realNumber^+$, $P(u^\epsilon_t)$ is c-TV stable and $\pi_E$ is deterministic. Then, the imitation gap is upper bounded by 
\begin{align*}
    J(\pi_E)-J(\hat{\pi}_h)\leq T^2\big(c\epsilon\ill(\Pi,k)+2\delta\big)=\mathcal{O}\big(T^2(\delta+\epsilon)\big).
\end{align*}
\end{theorem}
This upper bound scales at the rate of $T^2$, which aligns with the expected behaviour of imitation learning without an interactive expert~\citep{Ross2010}.
Next, we show that the upper bounds on the imitation gap from prior work~\citep{Swamy2022_temporal, Swamy2022} are special cases of
% of  subsumed by the unifying causal IL framework introduced in Section~\ref{sec:setting} are special cases of 
Theorem~\ref{thm:gap}. The proofs are deferred to~\cref{appendix:corollaries}.
\begin{corollary}\label{corollary:noUo}
In the special case that $u^o_t = 0$, i.e., there are no expert-observable confounders, or $u^o_t=\expectE_{\pi_E}[u^o_t\lvert h_t]$, i.e., $u^o_t$ is $\sigma(h_t)$ measurable (all information about $u^o_t$ is contained in the history), the imitation gap is upper bounded by
\begin{align*}
    J(\pi_E)-J(\hat{\pi}_h)\leq T^2\big(c\epsilon\ill(\Pi,k)\big)=\mathcal{O}\big(T^2\epsilon\big),
\end{align*}
which coincides with Theorem 5.1 of~\citet{Swamy2022_temporal}.
\end{corollary}

When there are no hidden confounders, i.e, $u^\epsilon_t=0$, our framework is reduced to that of~\citet{Swamy2022}. However, \citet{Swamy2022} provided an abstract bound that directly uses the supremum of key components in the imitation gap over all possible Q functions to bound the imitation gap. We further extend and concretise the bound using the learning error $\epsilon$ and the TV distance bound $\delta$ instead of relying on the suprema.


\begin{corollary}\label{corollary:unconfounded}
In the special case that $u^\epsilon_t=0$, if the learnt policy has optimisation error $\epsilon$,  the imitation gap is upper bounded by
\begin{align*}
    J(\pi_E)-J(\hat{\pi}_h)\leq T^2\left(\frac{2}{\sqrt{\dim(A)}}\epsilon+2\delta \right),
\end{align*}
which is a concrete bound that extends the abstract bound in Theorem 5.4 of~\cite{Swamy2022}.
\end{corollary}

\begin{remark}
\emph{If both $u^\epsilon_t$ and $u^o_t$ are zero, we then recover the classic setting of IL without confounders~\citep{Ross2010}, and the imitation gap bound is $T^2\epsilon$, where $\epsilon$ is the optimisation error of the algorithm.}
\end{remark}