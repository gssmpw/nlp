\section{A Unifying Framework for Causal Imitation Learning}\label{sec:setting}
\begin{figure}[t]
    \centering
\includegraphics[width=0.47\textwidth]{figures/confoundedMDP_jan.png}
    \caption{A causal graph of MDPs with hidden confounders, where at each time step the hidden confounder is $u_t=(u^o_t,u^\epsilon_t)$. The black dotted lines represent the causal effect of the expert-observable confounder $u^o_t$, which directly affects $a_t$ because the expert policy can observe $u^o_t$. It also directly affects $s_{t+1}$ and $r_t$ because otherwise it is irrelevant to the expected return and there is no reason for the expert to consider it. The red dotted lines represent the causal effect of $u^\epsilon_t$ that is not observable by the expert, which acts as confounding noise and directly affects the states and actions. $u^\epsilon_t$ does not directly affect $r_t$ (following~\citet{Swamy2022_temporal}) because the expert policy does not take $u^\epsilon_t$ into account, and letting $u^\epsilon_t$ directly affect $r_t$ would only add noise to the expected return.}
    \label{fig:MDPUC}
\end{figure}

\paragraph{MDPs with Hidden Confounders.} 
In this section, we introduce a novel unifying framework for causal IL in the presence of hidden confounders. We begin by introducing an Markov Decision Process (MDP) with hidden confounders, $(\states, \actions, \confounders, \transitions, \reward, \mu_0,T)$, where $\states$ is the state space, $\actions$ is the action space and $\confounders$ is the confounder space. Importantly, parts of the hidden confounders $u_t$ may be available to the expert due to imperfect environment logging and expert knowledge. 
We model this by segmenting the hidden confounder into two parts $u_t=(u^o_t,u^\epsilon_t)$, where $u^o_t$ are observable to the expert and $u^\epsilon_t$ are not. Intuitively, $u^o_t$ are additional information that only the expert observes and $u^\epsilon_t$ behave as confounding noise in the environment that affects both the state and action.\footnote{In our framework, we allow the actual actions taken in the environment to be affected by the noise. Noise that only perturbs data records can be considered as a special case of our framework.} 
As a result, the transition function $\transitions(\cdot\mid s,a,(u^o,u^\epsilon))$ depends on both hidden confounders but the reward function $\reward(s,a,u^o)$ only depends on the state, action and the observable confounder $u^o$ since the confounding noise only directly affects the state and actions. Finally, $\mu_0$ is the initial state distribution and $T$ is the horizon of the problem. We provide a causal graph that illustrates the relationships between variables in~\cref{fig:MDPUC}.

% (we can add a Structural Causal Model perspective of this setting for completeness)

\paragraph{Causal Imitation Learning.} 
We assume that an expert is demonstrating a task following some expert policy $\pi_E$ (which we will specify later) and we observe a set of $N \geq 1$ expert demonstrations $\{d_1, d_2,...,d_N\}$. Each demonstration is a state-action trajectory $(s_1,a_1,...,s_{T},a_{T})$, where, at each time step, we observe the state $s_t$ and the action $a_t$ taken in the environment, and the trajectory follows the transition function $\transitions(\ \cdot\mid s_t, a_t, (u^o_t, u_t^\epsilon))$. 
Denote $h_t=(s_{1},a_{1},...,s_{t-1},a_{t-1},s_{t})\in\mathcal{H}$ as the trajectory history at time $t$, where $\mathcal{H}\subseteq \bigcup_{i=0}^{T-1} (\states\times\actions)^{i}\times \states$ is the set of all possible trajectory histories at different time steps.
% and $h_t^k=(s_{t-k+1},a_{t-k+1},...,s_{t})$ for $k\leq t$ as the $k$ step history from $t$.
Importantly, we do not observe the reward and the sequence of confounders $(u^o_t,u^\epsilon_t)$.

Given the observed trajectories, our goal is to learn a history-dependent policy $\pi_h:\mathcal{H}\rightarrow \Delta(A)$. We assume that our policy class $\Pi$ is convex and compact. The $Q$-function of a policy $\pi_{h}\in\Pi$ is defined as $Q_\pi(s_t,a_t,u_t^o)=\expectE_{\tau\sim\pi_{h}}[\sum_{t^\prime=t}^T \reward(s_{t^\prime},a_{t^\prime},u^o_{t^\prime})]$ and the value of a policy is $J(\pi)=\expectE_{\tau\sim\pi_{h}}[\sum_{t^\prime=t}^{T}\reward(s_{t^\prime},a_{t^\prime},u^o_{t^\prime})]$, where $\tau$ is the trajectory following $\pi_{h}$.


This nuanced distinction between $u^o_t$ and $u^\epsilon_t$ is crucial for determining the appropriate method for IL, and we begin with an example to motivate our setting and illustrate the importance of considering $u_t=(u^o_t,u^\epsilon_t)$.
\\
\begin{example}\label{eg:plane}
Consider a dynamic aeroplane ticket pricing scenario~\citep{wright1928,Hartford2017DeepPrediction}, where we would like to learn a ticket pricing policy by imitating actual airline prices based on the profit margins set by experts. We have access to information such as destinations, flight time, previous sales, and aeroplane records. However, seasonal demand patterns and events are known to the experts, but are not logged in the dataset. Hence, these hidden confounders are included in $u^o_t$. In contrast, the experts only determine profit margins as an action because the observed price is confounded (additively) by fluctuations in operating costs such as fuel price and maintenance costs, which are unknown to the expert when they set the profit margin. These hidden confounders act as $u^\epsilon_t$ in our setting. Without an explicit consideration of $u^o_t$ and $u^\epsilon_t$ separately, learning algorithms cannot distinguish between $u^o_t$ and $u^\epsilon_t$ and fail to correctly imitate the expert. We perform experiments on this toy example in~\cref{sec:exps}.
\end{example}

In order to learn a policy $\pi_h$ that can match the performance of $\pi_E$, we need to break the spurious correlation between states and expert actions. In fact, this is a causal inference problem of learning the counterfactual expert decision:
\begin{align*}
\expectE[a_t\mid do(s_t,u^o_t)]&=\pi_E(s_t,u^o_t)+\expectE[u^\epsilon_t\mid do(s_t,u^o_t)]\\
&=\pi_E(s_t,u^o_t),
\end{align*}
which describes what the expert would do if we intervened and placed them in state $s_t$ when observing $u^o_t$. Here, we assume the confounding noise $u^\epsilon_t$ is additively to the action and zero expectation, which is later formalised and explained in Assumption~\ref{assump:additive}. Unfortunately, from the causal inference literature~\citep{Shpitser2008,Pearl2000}, we know that, without further assumptions, it is impossible to identify $\pi_E$.

To determine the minimal assumptions that allow $\pi_E$ to be identifiable, we first observe that $u^\epsilon_t$ can be correlated for all $t$, making it impossible to distinguish between the intended actions of the expert and the confounding noise. However, in practice, the effect of confounding noise $u^\epsilon_t$ at $t$ may diminish over time (e.g., wind) or the $u^\epsilon_t$ becomes observable at a future time (e.g,  operating costs), which makes the confounding noise at time $t$ and the confounding noise at some future time step $t^\prime$ independent. We formalise this novel notion in terms of a confounding noise horizon $k$:

\begin{assumption}[Confounding Horizon]\label{assump:horizon}
For every $t$, the confounding noise $u^\epsilon_t$ has a horizon of $k$ where $1\leq k< T$. More formally, $u^\epsilon_t\indep u^\epsilon_{t-k}\ \forall t>k$.
\end{assumption}

In addition, we make a standard assumption in causal inference~\citep{Pearl2000,Hartford2017DeepPrediction,Shao2024}, namely that the confounding noise is additive to the action.

\begin{assumption}[Additive Noise]\label{assump:additive}
The structural equation that generates the actions in the observed trajectories is
\begin{align}
a_t&=\pi_E(s_t,u^o_t)+u^\epsilon_t,\label{eq:action}
\end{align}
where we assume the confounding noise $u^\epsilon_t$ is additive to $a_t$ and WLOG $\expectE[u^\epsilon_t]=0$ because any non-zero expectation of $u^\epsilon_t$ can be included as a constant in $\pi_E$.
\end{assumption}

Without this additive noise assumption, the causal effect becomes unidentifiable (see, e.g.,~\citet{Balke1994}) and the best we can do is to upper/lower bound it. Next, we show that, with the above two assumptions, it becomes possible to identify the true causal relationship between states and expert actions, and imitate $\pi_E$.



% \begin{remark}
% Our MDP setting is different to contextual MDPs (CMDP) since the unobserved context there is fixed thorughout an episode, and CMDP can be considered a very special case of MDPUC. MDPUC is also different to POMDPs since POMDPs doesn't have any markovian properties and doesn't imply confounding.
% \end{remark}


% application examples: 

% 1. helping people living with type-1 diabetes to time their
% insulin injections by monitoring their blood glucose level using some wearable device. observations of individualsâ€™ blood glucose levels over time and the timing of insulin injections. However, there may in fact be events not recorded in the data, such as food intake and exercise, which may affect both the timing of injections and blood glucose.

% 2. Trading system, external factors such as market sentiments, economic indicators, and news events, which can be modelled as confounding noises.

% 3. Driving or robotics, invisible noises such as vibration or wind in the environment that can affect both current state and action


% \begin{table}[]
%     \centering
%     \begin{tabular}{c|c}
%          &  \\
%          & 
%     \end{tabular}
%     \caption{Prior work from the perspective of the framework.}
%     \label{tab:my_label}
% \end{table}



