
\begin{figure*}[t]
\begin{subfigure}[t]{1\textwidth}
\centering\includegraphics[width=0.5\textwidth]{figures/exp_results/legend.png}
\end{subfigure}
\centering
\begin{subfigure}[t]{0.38\textwidth}
\centering
\includegraphics[width=1\textwidth]{figures/exp_results/toy_mse.png}
\caption{MSE in log scale, lower is better.}
\end{subfigure}
\begin{subfigure}[t]{0.38\textwidth}
\centering
\includegraphics[width=1\textwidth]{figures/exp_results/toy_rew.png}
\caption{Average reward, higher is better.}
\end{subfigure}
% \vspace{-3pt}
\caption{The MSE between the learnt policy and the expert, and the average reward, in the plane ticket environment (Example~\ref{eg:plane}).}
\label{fig:toy}
\end{figure*}

\section{Experiments}\label{sec:exps}


In this section, we empirically evaluate the performance of~\cref{alg:DML-IL} (DML-IL) on the toy environment with continuous state and action spaces introduced in Example~\ref{eg:plane} and Mujoco environments: Ant, Half Cheetah and Hopper. We compare with the following existing methods: Behavioural Cloning (BC), which naively minimises $\expectE[-\log\pi(a_t\lvert s_t)]$; BC-SEQ \citep{Swamy2022}, which learns a history-dependent policy to handle hidden contexts observable to the expert; ResiduIL \citep{Swamy2022_temporal}, which we adapt to our setting by providing $h_{t-k}$ as instruments to learn a history-independent policy; 
%DML-IL is our proposed method described in~\cref{alg:DML-IL}; 
and the noised expert, which is the performance of the expert when put in the confounded environment, and corresponds to the maximally achievable performance. In \cref{appendix:otheriv}, we also include additional evaluations of using other IV regression algorithms, including DFIV~\citep{Xu2020} and DeepGMM~\citep{Bennett2019DeepAnalysis}, as the core CMR solver but found inconsistent and subpar performance. 

We train imitators with 20000 samples (40 trajectories of 500 steps each) of the expert trajectory using each algorithm and report the average reward when tested online in their respective environments. The reward is scaled such that 1 is the performance of the un-noised expert, and 0 is a random policy. We also report the Mean Squared Error (MSE) between imitator's and expert's actions. The purpose of evaluating the MSE is to assess how well the imitator learnt from the expert, and importantly whether the confounding noise problem is mitigated. When the confounding noise $u^\epsilon$ is not handled, we should expect to observe a much higher MSE. All results are plotted with one standard deviation as a shaded area. In addition, we vary the confounding noise horizon $k$ from 1 to 20 in order to increase the difficulty of the problem with weaker instruments $h_{t-k}$.

\begin{figure*}[t!]
\begin{subfigure}[t]{1\textwidth}
\centering\includegraphics[width=0.5\textwidth]{figures/exp_results/legend.png}
\end{subfigure}
\begin{subfigure}[t]{0.33\textwidth}
\centering\captionsetup{width=0.9\linewidth}\includegraphics[width=1\textwidth]{figures/exp_results/ant_mse.png}
\caption{MSE for Ant, lower is better.}
\end{subfigure}
\begin{subfigure}[t]{0.33\textwidth}
\centering\captionsetup{width=0.9\linewidth}\includegraphics[width=1\textwidth]{figures/exp_results/hc_mse.png}
\caption{MSE for Half Cheetah, lower is better.}
\end{subfigure}
\begin{subfigure}[t]{0.33\textwidth}
\centering\captionsetup{width=0.9\linewidth}\includegraphics[width=1\textwidth]{figures/exp_results/hopper_mse.png}
\caption{MSE for Hopper, lower is better.}
\end{subfigure}
\vspace{.5cm}

\begin{subfigure}[t]{0.33\textwidth}
\centering\captionsetup{width=0.9\linewidth}\includegraphics[width=1\textwidth]{figures/exp_results/ant_rew.png}
\caption{Average reward for Ant, higher is better.}
    \label{fig:ant_rew}
\end{subfigure}
\begin{subfigure}[t]{0.33\textwidth}
\centering\captionsetup{width=0.9\linewidth}\includegraphics[width=1\textwidth]{figures/exp_results/hc_rew.png}
\caption{Average reward for Half Cheetah, higher is better.}
    \label{fig:hc_rew}
\end{subfigure}
\begin{subfigure}[t]{0.33\textwidth}
\centering\captionsetup{width=1\linewidth}\includegraphics[width=1\textwidth]{figures/exp_results/hopper_rew.png}
\caption{Average reward for Hopper, higher is better. Note that BC performs worse in Hopper than a random policy.}
\label{fig:hopper_rew}
\end{subfigure}
% \vspace{-3pt}
\caption{The MSE between the learnt policy and expert, and the average reward, in Mujoco environments.}
\label{fig:gym}
\end{figure*}

\subsection{Plane Ticket Pricing Environment}
\paragraph{Experimental Setup.}
We first consider the plane ticket pricing environment described in Example~\ref{eg:plane}. The confounding noise $u^\epsilon$ are operation costs and $u^o_t$ are seasonal demand patterns and events. We set $u^o_t$ to continuously vary with a rate of change of approximately every 30 steps. Details on this environment are provided in~\cref{appendix:ticket}.
\vspace{-10pt}
\paragraph{Results.}
The results are presented in~\cref{fig:toy}. DML-IL performed the best with the lowest MSE and the highest average reward that is closest to the expert, especially when the $u^\epsilon_t$ horizon is 1. This implies that DML-IL is successful in handling both $u^\epsilon_t$ and $u^o_t$. ResiduIL is able to reduce the confounding effect of $u^\epsilon_t$ evident by the lower MSE compared to the two other methods that do not deal with $u^\epsilon_t$. However, since it does not explicitly consider $u^o_t$, the imitator has no information on $u^o_t$ and the best it can do is to assume some average value (or expectation) of $u^o_t$. Therefore, while ResiduIL still achieves some reward, its performance gap to DML-IL is due to the lack of consideration of $u^o_t$. Both BC and BC-SEQ fail completely in the presence of confounding noise $u^\epsilon_t$, with orders of magnitude higher MSE and average reward close to a random policy. From the similar performance of BC-SEQ and BC, we see that the use of trajectory history to infer $u^o_t$ is not helpful when the confounding noise is not handled explicitly. This demonstrates that considering the effect of $u^\epsilon_t$ and $u^o_t$ partially is insufficient to learn a well performing imitator under the general setting.

In addition, as the confounding noise horizon $k$ increases (x-axis), the performance of DML-IL drops. This coincides with the fact that the instrument is weaker and less information regarding $u^o_t$ can be captured from $h_{t-k}$ as $k$ increases. When $k=20$, it can be seen that the performance of DML-IL is close to that of ResiduIL, which does not consider the effect of $u^o_t$, because very limited information about the current expert-observable confounder $u^o_t$ can be inferred from history 20 steps ago.



\subsection{Mujoco Environments}
\paragraph{Experimental Setup.} 
In~\cref{fig:gym}, we consider the Mujoco tasks. While the original tasks do not have hidden variables, we modify the environment to introduce $u^\epsilon_t$ and $u^o_t$. Specifically, instead of controlling the ant, half cheetah and hopper, respectively, to travel as fast as possible, the goal is to control the agent to travel at a target speed that is varying throughout an episode. This target speed is $u^o_t$, which is observed by the expert but not recorded in the dataset. In addition, we add confounding noise $u^\epsilon_t$ to $s_t$ and $a_t$ to mimic the confounding noise such as wind. Additional details about the modification made to the environments are provided in~\cref{appendix:mujoco}.
\vspace{-10pt}
\paragraph{Results.}
DML-IL outperforms other methods in all three Mujoco environments as shown in~\cref{fig:gym}. Similarly to the plane ticket environment, ResiduIL is effective in removing the confounding noise but fails to match the average reward of DML-IL due to the lack of knowledge of $u^o_t$. BC and BC-SEQ have much higher MSE and fail to learn meaningful policies. As $u^\epsilon_t$ horizon increases, the performance of DML-IL drops as expected due to weaker instruments and limited inferrable information regarding $u^o_t$, especially for the Ant and Half Cheetah tasks.


% It can also be seen by the closer gap of MSE between DML-IL and ResiduIL compared to other methods that confounding noise $u^\epsilon_t$ fundamentally breaks the regression when not handled properly, and $u^o_t$ when not considered will yield an average performance by assuming an expectation of $u^o_t$.