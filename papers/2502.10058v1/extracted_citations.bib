@article{ref12,
  title={Distilling the Knowledge of BERT for Sequence-to-Sequence ASR},
  author={Futami, Hayato and Inaguma, Hirofumi and Ueno, Sei and Mimura, Masato and Sakai, Shinsuke and Kawahara, Tatsuya},
  journal={arXiv preprint arXiv:2008.03822},
  year={2020}
}

@article{ref16,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year={2018}
}

@article{ref38,
  title={Masked language model scoring},
  author={Salazar, Julian and Liang, Davis and Nguyen, Toan Q and Kirchhoff, Katrin},
  journal={arXiv preprint arXiv:1910.14659},
  year={2019}
}

@inproceedings{ref39,
  title={Effective sentence scoring method using bert for speech recognition},
  author={Shin, Joonbo and Lee, Yoonhyung and Jung, Kyomin},
  booktitle={Asian Conference on Machine Learning},
  pages={1081--1093},
  year={2019},
  organization={PMLR}
}

@article{ref5,
  title={Adapting GPT, GPT-2 and BERT Language Models for Speech Recognition},
  author={Zheng, Xianrui and Zhang, Chao and Woodland, Philip C},
  journal={arXiv preprint arXiv:2108.07789},
  year={2021}
}

@article{ref50,
  title={Disambiguation-BERT for N-best Rescoring in Low-Resource Conversational ASR},
  author={Ortiz, Pablo and Burud, Simen},
  journal={arXiv preprint arXiv:2110.02267},
  year={2021}
}

@article{ref7,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{ref9,
  title={ASR Rescoring and Confidence Estimation with ELECTRA},
  author={Futami, Hayato and Inaguma, Hirofumi and Mimura, Masato and Sakai, Shinsuke and Kawahara, Tatsuya},
  journal={arXiv preprint arXiv:2110.01857},
  year={2021}
}

