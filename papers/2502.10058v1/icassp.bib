@article{ref1,
  title={Listen, attend and spell},
  author={Chan, William and Jaitly, Navdeep and Le, Quoc V and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1508.01211},
  year={2015}
}

@inproceedings{ref2,
  title={Joint CTC/attention decoding for end-to-end speech recognition},
  author={Hori, Takaaki and Watanabe, Shinji and Hershey, John R},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={518--529},
  year={2017}
}

@article{ref3,
  title={Automatic speech recognition: a review},
  author={Katyal, Anchal and Kaur, Amanpreet and Gill, Jasmeen},
  journal={International Journal of Engineering and Advanced Technology (IJEAT)},
  volume={3},
  number={3},
  pages={71--74},
  year={2014},
  publisher={Citeseer}
}

@article{ref4,
  title={Continuous speech recognition by statistical methods},
  author={Jelinek, Frederick},
  journal={Proceedings of the IEEE},
  volume={64},
  number={4},
  pages={532--556},
  year={1976},
  publisher={IEEE}
}

@article{ref5,
  title={Adapting GPT, GPT-2 and BERT Language Models for Speech Recognition},
  author={Zheng, Xianrui and Zhang, Chao and Woodland, Philip C},
  journal={arXiv preprint arXiv:2108.07789},
  year={2021}
}

@inproceedings{ref6,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@article{ref7,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{ref8,
  title={Unified language model pre-training for natural language understanding and generation},
  author={Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
  journal={arXiv preprint arXiv:1905.03197},
  year={2019}
}

@article{ref9,
  title={ASR Rescoring and Confidence Estimation with ELECTRA},
  author={Futami, Hayato and Inaguma, Hirofumi and Mimura, Masato and Sakai, Shinsuke and Kawahara, Tatsuya},
  journal={arXiv preprint arXiv:2110.01857},
  year={2021}
}

@inproceedings{ref10,
  title={Finnish ASR with Deep Transformer Models.},
  author={Jain, Abhilash and Rouhe, Aku and Gr{\"o}nroos, Stig-Arne and Kurimo, Mikko and others},
  booktitle={Interspeech},
  pages={3630--3634},
  year={2020}
}


@article{ref11,
  title={Language modeling with deep transformers},
  author={Irie, Kazuki and Zeyer, Albert and Schl{\"u}ter, Ralf and Ney, Hermann},
  journal={arXiv preprint arXiv:1905.04226},
  year={2019}
}


@article{ref12,
  title={Distilling the Knowledge of BERT for Sequence-to-Sequence ASR},
  author={Futami, Hayato and Inaguma, Hirofumi and Ueno, Sei and Mimura, Masato and Sakai, Shinsuke and Kawahara, Tatsuya},
  journal={arXiv preprint arXiv:2008.03822},
  year={2020}
}

@article{ref14,
  title={Cross-Modal Transformer-Based Neural Correction Models for Automatic Speech Recognition},
  author={Tanaka, Tomohiro and Masumura, Ryo and Ihori, Mana and Takashima, Akihiko and Moriya, Takafumi and Ashihara, Takanori and Orihashi, Shota and Makishima, Naoki},
  journal={arXiv preprint arXiv:2107.01569},
  year={2021}
}


@article{ref15,
  title={Pre-trained models for natural language processing: A survey},
  author={Qiu, Xipeng and Sun, Tianxiang and Xu, Yige and Shao, Yunfan and Dai, Ning and Huang, Xuanjing},
  journal={Science China Technological Sciences},
  pages={1--26},
  year={2020},
  publisher={Springer}
}

@article{ref16,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year={2018}
}


@article{ref_17_chorowski2016towards,
  title={Towards better decoding and language model integration in sequence to sequence models},
  author={Chorowski, Jan and Jaitly, Navdeep},
  journal={arXiv preprint arXiv:1612.02695},
  year={2016}
}


@article{ref18,
  title={On using monolingual corpora in neural machine translation},
  author={Gulcehre, Caglar and Firat, Orhan and Xu, Kelvin and Cho, Kyunghyun and Barrault, Loic and Lin, Huei-Chi and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1503.03535},
  year={2015}
}

@inproceedings{ref19,
  title={A comparison of techniques for language model integration in encoder-decoder speech recognition},
  author={Toshniwal, Shubham and Kannan, Anjuli and Chiu, Chung-Cheng and Wu, Yonghui and Sainath, Tara N and Livescu, Karen},
  booktitle={2018 IEEE spoken language technology workshop (SLT)},
  pages={369--375},
  year={2018},
  organization={IEEE}
}

@inproceedings{ref20,
  title={An analysis of incorporating an external language model into a sequence-to-sequence model},
  author={Kannan, Anjuli and Wu, Yonghui and Nguyen, Patrick and Sainath, Tara N and Chen, Zhijeng and Prabhavalkar, Rohit},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5828},
  year={2018},
  organization={IEEE}
}

@article{ref21,
  title={Espnet: End-to-end speech processing toolkit},
  author={Watanabe, Shinji and Hori, Takaaki and Karita, Shigeki and Hayashi, Tomoki and Nishitoba, Jiro and Unno, Yuya and Soplin, Nelson Enrique Yalta and Heymann, Jahn and Wiesner, Matthew and Chen, Nanxin and others},
  journal={arXiv preprint arXiv:1804.00015},
  year={2018}
}

@article{ref22,
  title={Sequence transduction with recurrent neural networks},
  author={Graves, Alex},
  journal={arXiv preprint arXiv:1211.3711},
  year={2012}
}

@article{ref23,
  title={Recent Advances in End-to-End Automatic Speech Recognition},
  author={Li, Jinyu},
  journal={arXiv preprint arXiv:2111.01690},
  year={2021}
}

@inproceedings{ref24,
  title={Transformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss},
  author={Zhang, Qian and Lu, Han and Sak, Hasim and Tripathi, Anshuman and McDermott, Erik and Koo, Stephen and Kumar, Shankar},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7829--7833},
  year={2020},
  organization={IEEE}
}

@article{ref25,
  title={Hybrid CTC/attention architecture for end-to-end speech recognition},
  author={Watanabe, Shinji and Hori, Takaaki and Kim, Suyoun and Hershey, John R and Hayashi, Tomoki},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  volume={11},
  number={8},
  pages={1240--1253},
  year={2017},
  publisher={IEEE}
}

@inproceedings{ref26,
  title={Librispeech: an asr corpus based on public domain audio books},
  author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle={2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={5206--5210},
  year={2015},
  organization={IEEE}
}




@inproceedings{ref27,
  title={Transfer learning of language-independent end-to-end asr with language model fusion},
  author={Inaguma, Hirofumi and Cho, Jaejin and Baskar, Murali Karthick and Kawahara, Tatsuya and Watanabe, Shinji},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6096--6100},
  year={2019},
  organization={IEEE}
}


@article{ref28,
  title={Efficiently fusing pretrained acoustic and linguistic encoders for low-resource speech recognition},
  author={Yi, Cheng and Zhou, Shiyu and Xu, Bo},
  journal={IEEE Signal Processing Letters},
  volume={28},
  pages={788--792},
  year={2021},
  publisher={IEEE}
}


@article{ref29,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{ref30,
  title={Transformer-xl: Attentive language models beyond a fixed-length context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1901.02860},
  year={2019}
}

@article{ref31,
  title={Using out-of-domain data to improve in-domain language models},
  author={Iyer, Rukmini and Ostendorf, Mari and Gish, Herbert},
  journal={IEEE Signal processing letters},
  volume={4},
  number={8},
  pages={221--223},
  year={1997},
  publisher={IEEE}
}

@misc{ref32,
  title={Statistical methods for speech recognition},
  author={Frederick, Jelinek},
  year={1999},
  publisher={The MIT Press}
}

@inproceedings{ref33,
  title={Unsupervised Adaptation of Recurrent Neural Network Language Models.},
  author={Gangireddy, Siva Reddy and Swietojanski, Pawel and Bell, Peter and Renals, Steve},
  booktitle={Interspeech},
  pages={2333--2337},
  year={2016}
}



@article{ref34,
  title={A neural probabilistic language model},
  author={Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal},
  journal={Advances in Neural Information Processing Systems},
  volume={13},
  year={2000}
}

@inproceedings{ref35,
  title={Improved neural network based language modelling and adaptation},
  author={Park, Junho and Liu, Xunying and Gales, Mark JF and Woodland, Phil C},
  booktitle={Eleventh Annual Conference of the International Speech Communication Association},
  year={2010}
}

@inproceedings{ref36,
  title={Recurrent neural network based language model.},
  author={Mikolov, Tomas and Karafi{\'a}t, Martin and Burget, Lukas and Cernock{\`y}, Jan and Khudanpur, Sanjeev},
  booktitle={Interspeech},
  volume={2},
  number={3},
  pages={1045--1048},
  year={2010},
  organization={Makuhari}
}

@inproceedings{ref37,
  title={LSTM neural networks for language modeling},
  author={Sundermeyer, Martin and Schl{\"u}ter, Ralf and Ney, Hermann},
  booktitle={Thirteenth annual conference of the international speech communication association},
  year={2012}
}

@article{ref38,
  title={Masked language model scoring},
  author={Salazar, Julian and Liang, Davis and Nguyen, Toan Q and Kirchhoff, Katrin},
  journal={arXiv preprint arXiv:1910.14659},
  year={2019}
}

@inproceedings{ref39,
  title={Effective sentence scoring method using bert for speech recognition},
  author={Shin, Joonbo and Lee, Yoonhyung and Jung, Kyomin},
  booktitle={Asian Conference on Machine Learning},
  pages={1081--1093},
  year={2019},
  organization={PMLR}
}



@article{ref40,
  title={Pre-trained models: Past, present and future},
  author={Han, Xu and Zhang, Zhengyan and Ding, Ning and Gu, Yuxian and Liu, Xiao and Huo, Yuqi and Qiu, Jiezhong and Yao, Yuan and Zhang, Ao and Zhang, Liang and others},
  journal={AI Open},
  volume={2},
  pages={225--250},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{ref41,
  title={Improving transformer-based end-to-end speech recognition with connectionist temporal classification and language model integration},
  author={Nakatani, Tomohiro},
  booktitle={Proc. Interspeech 2019},
  year={2019}
}

@article{ref42,
  title={“Cloze procedure”: A new tool for measuring readability},
  author={Taylor, Wilson L},
  journal={Journalism quarterly},
  volume={30},
  number={4},
  pages={415--433},
  year={1953},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@inproceedings{ref43,
  title={Multi-task learning based pre-trained language model for code completion},
  author={Liu, Fang and Li, Ge and Zhao, Yunfei and Jin, Zhi},
  booktitle={Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
  pages={473--485},
  year={2020}
}

@inproceedings{ref44,
  title={Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks},
  author={Graves, Alex and Fern{\'a}ndez, Santiago and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={369--376},
  year={2006}
}


@article{ref45,
  title={Subword regularization: Improving neural network translation models with multiple subword candidates},
  author={Kudo, Taku},
  journal={arXiv preprint arXiv:1804.10959},
  year={2018}
}

@article{ref46,
  title={Fixing weight decay regularization in adam},
  author={Loshchilov, Ilya and Hutter, Frank},
  year={2018}
}


@inproceedings{ref47,
  title={End-to-end attention-based large vocabulary speech recognition},
  author={Bahdanau, Dzmitry and Chorowski, Jan and Serdyuk, Dmitriy and Brakel, Philemon and Bengio, Yoshua},
  booktitle={2016 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={4945--4949},
  year={2016},
  organization={IEEE}
}


@article{ref48,
  title={Transformer with bidirectional decoder for speech recognition},
  author={Chen, Xi and Zhang, Songyang and Song, Dandan and Ouyang, Peng and Yin, Shouyi},
  journal={arXiv preprint arXiv:2008.04481},
  year={2020}
}

@inproceedings{ref49,
  title={BERT-based Semantic Model for Rescoring N-best Speech Recognition List},
  author={Fohr, Dominique and Illina, Irina},
  booktitle={INTERSPEECH 2021},
  year={2021}
}


@article{ref50,
  title={Disambiguation-BERT for N-best Rescoring in Low-Resource Conversational ASR},
  author={Ortiz, Pablo and Burud, Simen},
  journal={arXiv preprint arXiv:2110.02267},
  year={2021}
}

@article{ref51_interspeech2022,
  title={Effect and Analysis of Large-scale Language Model Rescoring on Competitive ASR Systems},
  author={Udagawa, Takuma and Suzuki, Masayuki and Kurata, Gakuto and Itoh, Nobuyasu and Saon, George},
  journal={arXiv preprint arXiv:2204.00212},
  year={2022}
}


@inproceedings{ref888,
  title={IMPROVED NOISY ITERATIVE PSEUDO-LABELING FOR SEMI-SUPERVISED SPEECH RECOGNITION},
  author={Li, Tian and Meng, Qingliang and Yujian Sun},
  booktitle={2023 IEEE Spoken Language Technology Workshop (SLT)},
  year={2023},
  organization={IEEE}
}


@inproceedings{ref55,
  title={RescoreBERT: Discriminative Speech Recognition Rescoring With Bert},
  author={Xu, Liyan and Gu, Yile and Kolehmainen, Jari and Khan, Haidar and Gandhe, Ankur and Rastrow, Ariya and Stolcke, Andreas and Bulyko, Ivan},
  booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6117--6121},
  year={2022},
  organization={IEEE}
}


