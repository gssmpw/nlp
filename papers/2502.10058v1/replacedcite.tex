\section{Related Work}
There have been several ways to improve ASR using LMs. ____ propose to use BERT ____ and other bidirectional LMs for $n$-best rescoring. By predicting pseudo-log-likelihood based on regression, ____ reduces the computing cost of bidirectional LMs. ____ proposes an improvement on bidirectional LMs rescoring, using the ELECTRA model with faster inference than BERT. However, those studies cannot do shallow fusion decoding as they trained LM only using bidirectional logic. ____ leverages knowledge distillation to distill the conditional probabilities of the bidirectional LMs to those of the unidirectional LMs, allowing the model to do shallow fusion. However, since the conditions of bidirectional LMs and unidirectional LMs are incompatible, this operation is unreasonable.


Additionally, ____ train the GPT ____, BERT, and other LMs individually and perform rescoring by merging LMs scores. Nevertheless, this method requires multiple LM models from different training tasks used in decoding, resulting in high computation costs. Our model only requires a single set of parameters to achieve unidirectional and bidirectional LMs and one forward pass computation to obtain the LM score. At the same time, ____ only explicitly adds the scores of each LM to obtain the final model score. This simple fusion method will limit the ability of the model. We do implicit fusion in the training phase and design additional training tasks to compensate for the difference between unidirectional training tasks and MLM. This allows our model to be more flexible and perform better in the ASR system.