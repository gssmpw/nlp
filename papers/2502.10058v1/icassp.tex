% Template for Blind SLT-2022 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{multirow}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{amsmath,amssymb}
\usepackage{spconf,amsmath,graphicx}
\usepackage{stfloats}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{hyperref}
\usepackage{enumerate}

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

% Title.
% ------
%\title{Improved Language Model Training for Speech Recognition}
\title{MTLM: An innovative Language model training paradigm for ASR}
%
%
% Single address.
% ---------------
% \name{Author(s) Name(s)\thanks{Thanks to XYZ agency for funding.}}
\name{Qingliang Meng, Pengju Ren, Tian Li, Changsong Dai}
\address{Shumei AI Research Institute, Beijing, China}





%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}
%
\begin{document}
\ninept
%
\maketitle
%
\begin{abstract}
Pre-training Transformer-based language models (LMs) on a large amount of text has proven crucial for improving automatic speech recognition (ASR) performance. Generally, traditional LMs are unidirectional and unable to access the context on the right. This paper proposes a method for training LMs that enable traditional unidirectional LMs to fully utilize left and right contexts. Compared with the unidirectional LMs, our LM facilitates ASR to transcribe hypotheses more consistently and in a more semantically unambiguous way, as it incorporates richer contextual representations. Finally, our experimental results on the LibriSpeech corpus demonstrate that our model outperforms traditional unidirectional LMs, whether $n$-best rescoring or shallow fusion is used as the decoding algorithm.

\end{abstract}
%
\begin{keywords}
automatic speech recognition, unidirectional LM, bidirectional contextual representations.
\end{keywords}
%
\section{Introduction}
\label{sec:intro}


End-to-end automatic speech recognition (ASR) is a technology that converts the speech sequence into the target token sequence, which can be trained using paired speech and transcripts \cite{ref3}. Recently, external language models (LMs) trained on unpaired text data are frequently used to improve the performance of ASR systems \cite{ref49,ref50,ref12,ref48,ref51_interspeech2022}. Meanwhile, LMs exhibit a wide range of variations depending on pre-training tasks. The most classic pre-training tasks are unidirectional Language Modeling (ULM) and Masked Language Modeling (MLM) \cite{ref15}. The models trained using these two methods are called unidirectional LMs and bidirectional LMs, respectively. Moreover, the $n$-best rescoring and shallow fusion are widely used in ASR decoding \cite{ref20}.

In our research, we are surprised that most studies are constrained by the standard decoding method. Thus, the training mode of LMs in ASR is restricted to unidirectional training \cite{ref20}. In fact, from the perspective of linguistic information capture, both forward and backward training methods can be used to learn language context relevance \cite{ref9,ref39,ref49}. For example, english has both active and passive voice. Training with unidirectional LMs for passive voice is unpleasant, as the causal logic of the passive voice needs to be learned from back to front. Regarding the efficiency of enabling LMs to learn contextual semantic relevance, the bidirectional training method should be more effective than unidirectional training \cite{ref39}. Therefore, we propose a language model (LM) training paradigm for ASR that not only preserves which the LMs can be firmly coordinated with the acoustic model (AM) but also enhances the LMs' training efficiency, completeness in semantic capture, and feasibility in various decoding processes. In this paper, our core contributions are as follows:
\begin{enumerate}[i)]
    \item On the premise of retaining the ULM, we add the MLM training task. This task aims to make the semantic understanding of the LMs benefit not only from unidirectional information but also from bidirectional information. This further advances the understanding of the context semantics of the model, increasing the final word accuracy when combined with AM.
    \item Since the ULM and MLM training tasks are essentially different, it is hard to integrate them into training \cite{ref5,ref9,ref12,ref39}. Therefore, we designed the unidirectional Masked Language Modeling (UMLM) task to reduce this disparity and boost the integration of the knowledge learned from those two tasks. UMLM training uses the same left-to-right logic as ULM training, but the realization scheme of the goal is like MLM. This UMLM task facilitates the cross-flow of information the model learns from different tasks, especially ULM and MLM.
    \item Our model can be used for shallow fusion and rescoring because we still save the output of the unidirectional LM.
\end{enumerate}

On the LibriSpeech dataset \cite{ref26}, compared with the traditional unidirectional LMs for rescoring, our model uses the shallow fusion algorithm to reduce the Word Error Rate (WER) from 3.18\% to 2.63\% on the \textit{test-clean} dataset and from 8.78\% to 7.08\% on the \textit{test-other} dataset. Meanwhile, the number of error types on these two datasets has also been reduced by 1/6. These performance improvements demonstrate the superiority of our training paradigm.

\section{Related Work}
There have been several ways to improve ASR using LMs. \cite{ref39,ref50} propose to use BERT \cite{ref7} and other bidirectional LMs for $n$-best rescoring. By predicting pseudo-log-likelihood based on regression, \cite{ref38} reduces the computing cost of bidirectional LMs. \cite{ref9} proposes an improvement on bidirectional LMs rescoring, using the ELECTRA model with faster inference than BERT. However, those studies cannot do shallow fusion decoding as they trained LM only using bidirectional logic. \cite{ref12} leverages knowledge distillation to distill the conditional probabilities of the bidirectional LMs to those of the unidirectional LMs, allowing the model to do shallow fusion. However, since the conditions of bidirectional LMs and unidirectional LMs are incompatible, this operation is unreasonable.


Additionally, \cite{ref5} train the GPT \cite{ref16}, BERT, and other LMs individually and perform rescoring by merging LMs scores. Nevertheless, this method requires multiple LM models from different training tasks used in decoding, resulting in high computation costs. Our model only requires a single set of parameters to achieve unidirectional and bidirectional LMs and one forward pass computation to obtain the LM score. At the same time, \cite{ref5} only explicitly adds the scores of each LM to obtain the final model score. This simple fusion method will limit the ability of the model. We do implicit fusion in the training phase and design additional training tasks to compensate for the difference between unidirectional training tasks and MLM. This allows our model to be more flexible and perform better in the ASR system.


\section{Methodology}
\subsection{Model Architecture}
The model architecture is inspired by GPT \cite{ref16} but uses the encoder of the Transformer \cite{ref6} rather than the decoder, which is shown in Figure 1. The input $\boldsymbol{y}=y_1,y_2,y_3,...,y_n$ is a sequence of tokens. We use the one-hot vector $I_{y_i}$ to represent $y_i$, and $I_{\boldsymbol{y}}$ to represent $\boldsymbol{y}$. In our experiments, $y_1$ is a special $sos$ token. For input $\boldsymbol{y}$, $\boldsymbol{e} = I_{\boldsymbol{y}}W_e^T+W_p$, where $\boldsymbol{e}$ is the initial input of the encoder module after the matrix calculation of token embedding matrix $W_e$ and position embedding matrix $W_p$ for $\boldsymbol{y}$. $t^{l} =encoder(t^{l-1}) \quad l \in [1,n]$. We assume that $t^{0}=\boldsymbol{e}$ is passed to the model shown in Figure 1. The model is composed of multiple identical layers stacked. Each layer is composed of a multi-head attention layer and a feedforward layer \cite{ref6}. In addition, the model also uses the residual connection and layer normalization for efficient training \cite{ref6}. The model finally undergoes the $W_e$ transformation to generate the output distribution $h=t^{l}W_e$ on target tokens.

\begin{figure*}[ht]
\vspace{-1.5cm}
\setlength{\abovecaptionskip}{0.1cm}
  \centering
  \includegraphics[width=0.8\textwidth]{figure/MTLM4.pdf}
  \caption{Illustration of our proposed model. The model structure is based on the Transformer encoder and shares parameters with the three tasks.}
  \label{fig:MTLM_model}
  \vspace{-0.6cm}
\end{figure*}
\subsection{Proposed LM Training Method}
We aim to train an LM that can accurately estimate sentence probabilities or predict the next output successfully based on the context and current input. For this reason, we design three training tasks to train the LM by adopting multi-task learning \cite{ref8,ref43}. We refer to this model as MTLM. The details of the training tasks are shown below.\\

\noindent {\bf{ULM}:} In ASR, the goal of the LM is to provide a score for hypothesis, which is usually the joint probability $P(\boldsymbol{y})$ assigned by the LM \cite{ref10}. In this study, the ULM task is a left-to-right language modeling task, which use the complete previous token sequences as context to predict the current token. The optimization goal is shown in Equation (1):
\begin{equation}
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\label{eq5}
\mathcal{L}_{\mathrm{ULM}}\left(\boldsymbol{y},\boldsymbol{\theta} \right)=\mathbb{E}\left(\sum_{i=1}^n-\log P\left(y_{i} \mid y_{1},y_{2},...,y_{n-1}\right)\right)
\end{equation}
where $\left(y_{1},y_{2},...,y_{n-1}\right)$ is the context. The MTLM model trained by this task can predict the probability $P_{LM}(y_t|\boldsymbol{y}_{<t})$ in a single inference pass. Therefore, $P(\boldsymbol{y})=\sum_{t=1}^{\boldsymbol{n}} \log P_{\mathrm{LM}}\left({y}_{t} \mid \boldsymbol{y_{<t}}\right)$, which requires $N$ inferences from the unidirectional LM. In fact, due to the transformer structure employed by MTLM, the self-attention mechanism allows MTLM to calculate $P_{LM}(y_t|\boldsymbol{y}_{<t})$ for all $t$ in parallel. Therefore, the MTLM model naturally applies to the rescoring and shallow fusion in ASR.

We design the self-attention mask matrix shown in Figure 1 to implement the ULM task. The representation of each token only considers the left context and itself. In the self-attention mask, the shaded part is set to $-\infty$ to prevent attention, and the others are set to 0. The final optimization goal is to minimize the sum of the cross-entropy of $h=\{h_1,...,h_5\}$ and the corresponding target sequence $y=\{y_2,...,y_6\}$ where $y_6$ is $eos$, as shown in Figure \ref{fig:MTLM_model}.(a).\\

\noindent {\bf{UMLM}:} ULM task implements a unidirectional LM, often used as an external LM in the ASR system \cite{ref20}. Typically, when unidirectional LMs predict the current token, they rely on the knowledge of the preceding token. We argue that if the previous token is incorrectly predicted, it will have an effect on the prediction of the present $t$-th token. In other word, the probability $P_{LM}(y_t|\boldsymbol{y}_{<t})$ cannot be well predicted. Therefore, we are inspired by the MLM task to randomly mask the preceding partial tokens during model training. The purpose is to allow the model to predict the current token with problematic preceding information. The optimization goal of UMLM is shown in Equation (2):
 \begin{equation}
 \setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\label{eq6}
\mathcal{L}_{\mathrm{UMLM}}\left(\boldsymbol{y},\boldsymbol{\theta}\right)=\mathbb{E}\left(\sum_{y_i \in m}-\log p\left(y_{i} \mid y_{1}...y_{i-1}; \boldsymbol{y}^{\text {m }}\right)\right)
\end{equation}
where $\boldsymbol{y}^{\text{m}}$ represents replacement operation and $m$ represents a set of the masked token after the input performed $y^m$. Therefore, in decoding, the MTLM trained on this UMLM task can predict $P_{LM}(y_t|\boldsymbol{y}_{<t})$ more accurately than the unidirectional LM. The self-attention mask is basically the same as the ULM task, except that masked tokens in the previous sequence cannot contribute to the attention computation. The final optimization goal is to minimize the sum of the cross-entropy of $h=\{h_2,h_5\}$ and the corresponding target $y=\{y_3,y_6\}$, as shown in Figure \ref{fig:MTLM_model}.(b).\\

\noindent {\bf{BMLM}:} The ULM and UMLM tasks we have implemented up to this point have solely utilized the left context. In ASR, we believe that when generating the probability  $P_{LM}(y_t|\boldsymbol{y}_{<t})$, if the bidirectional context information can be combined, it is beneficial for improving the performance \cite{ref12}. Therefore, we created the Bidirectional Masked Language Modeling (BMLM) task. The objective function of BMLM is Equation (3):
 \begin{equation}
 \setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\label{eq7}
\mathcal{L}_{\mathrm{BMLM}}\left(\boldsymbol{y},\boldsymbol{\theta}\right)=\mathbb{E}\left(\sum_{y_i \in m}-\log p\left(y_{i} \mid y_{1}...y_{n}; \boldsymbol{y}^{\text {m}}\right)\right)
\end{equation}
As illustrated in Figure \ref{fig:MTLM_model}.(c), the optimization target is the sum of the cross-entropy of $h_2$ and $y_3$, $h_4$ and $y_5$ in training. In decoding, the MTLM model trained by the BMLM task can incorporate bidirectional contextual information from the left and right sides. In addition, the BMLM task uses a fully open self-attention combined with the MLM task, in which each token can attend to all unmasked tokens.\\

\noindent The overall training task of the MTLM model consists of ULM, UMLM, and BMLM. The parameters of the MTLM are learned to minimize the sum of the cross-entropy losses of these training tasks and shared among all tasks. The final loss function is shown in Equation (4).
\begin{equation}
\label{total_loss}
\min_{\theta} (\mathcal{L}_{\mathrm{ULM}}(\boldsymbol{y},\theta)+\mathcal{L}_{\mathrm{UMLM}}(\boldsymbol{y},\theta)+\mathcal{L}_{\mathrm{BMLM}}(\boldsymbol{y},\theta))
\end{equation}


\subsection{Decoding}
There are two ways to integrate external LMs into an end-to-end ASR model: rescoring and shallow fusion \cite{ref9,ref11,ref_17_chorowski2016towards}. In rescoring, the ASR model generates $n$-best list through the beam search, and then the LMs rerank each hypothesis in the list. $\text { Score }_{\mathrm{LM}}(\boldsymbol{y})$ is the score of LM for hypothesis $\boldsymbol{y}$, which is often expressed by the $log\mbox{-}likelihood$, as shown in Equation (5):
\begin{equation}
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\label{lm_integration_2}
\operatorname{Score}_{\mathrm{LM}}(\boldsymbol{y})= \sum_{t=1}^{\boldsymbol{n}} \log P_{\mathrm{LM}}\left({y}_{t} \mid \boldsymbol{y_{<t}} ; \boldsymbol{\theta}\right)
\end{equation}
where $\boldsymbol{y}_{<t}=(y_1,...,y_{t-1})$ and $P_{LM}(y_t|\boldsymbol{y}_{<t})$ is the conditional probability of predicting the current token given the prior tokens. For unidirectional LMs, $P_{LM}(y_t|\boldsymbol{y}_{<t})$ can be calculated by multiplying the output probabilities of each token in the sequence. For bidirectional LMs, it can predict current token $y_t$ based on the left and right context \cite{ref51_interspeech2022}. The output of bidirectional LMs can be used to estimate the conditional probability $P_{\mathrm{LM}}\left(\boldsymbol{y}_{t} \mid \boldsymbol{y}_{\backslash t}\right)$, where $\boldsymbol{y}_{\backslash t}=\left({y}_{1},..., {y}_{t-1},[\mathrm{mask}], {y}_{t+1},..., {y}_{\boldsymbol{n}}\right)$\cite{ref51_interspeech2022}. $\boldsymbol{y}_{\backslash t}$ represents the sentence in which the $t$-th token is replaced by a special $\mathrm{[mask]}$ token. Therefore, Equation (6) is usually used directly to represent the score of bidirectional LMs for hypothesis $\boldsymbol{y}$ \cite{ref51_interspeech2022}.
\begin{equation}
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\label{lm_integration_3}
\operatorname{Score}_{\mathrm{LM}}(\boldsymbol{y})= \sum_{t=1}^{\boldsymbol{n}} \log P_{\mathrm{LM}}\left({y}_{t} \mid \boldsymbol{y}_{\backslash t} ; \boldsymbol{\theta}\right)
\end{equation}

In shallow fusion, [7, 11, 12] generally uses LM to perform log-linear interpolation in each step of beam search.
\begin{equation}
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\begin{split}
\label{lm_integration_1}
y_t=&\mathop{\arg \max}\limits_{y_t} \log P_{AM}(y_t \mid X,\boldsymbol{y}_{<t})\\
&+\lambda \log P_{LM}(y_t \mid \boldsymbol{y}_{<t})
\end{split}
\end{equation}
The shallow fusion can better integrate the LM into the ASR system, because the generation of each token requires the participation of LM and AM \cite{ref20}.

After training the LM separately, we integrate our LM into the beam search algorithm. For decoding, we follow the joint CTC/S2S one-pass decoding algorithm \cite{ref25}. However, we have two differences. The first is that \cite{ref25} uses a fixed maximum length to determine the hypothesis length of audio transcription. We employ a prediction method based on CTC greedy search to find the optimum transcript length for each audio sample. This conserves a substantial amount of computational resources during decoding. Second, we prune finished and unfinished hypothesis routes together, whereas [27] never prunes finished routes. Without pruning, short sentences are more likely to remain in the finished pathways collection since they have higher scores, restricting final performance. According to the design of our training task, our MTLM model is suitable for both $n$-best rescoring and shallow fusion.

\section{ Experiments}

\subsection{Experimental Setup}
\renewcommand\arraystretch{0.86}
We evaluate our method using 1000 hours of audio data and transcripts from the LibriSpeech corpus \cite{ref26}. The training set is divided into 100, 360, and 500 hours, while the development and test sets are divided into clean and other categories. The training data are used to train the AM. This part of the speech data corresponds to the transcribed text data and the additional text data of 800 million words provided by LibriSpeech are used as the training corpus of the LM.

In our experiments, we implement the AM following the research of \cite{ref888}, which is a CTC+S2S hybrid architecture. CTC and S2S share the same encoder, and S2S has a separate decoder. The encoder consists of 12 Transformer blocks \cite{ref6}. Each block has an 8-head self-attention layer and a 2048-dimensional feed-forward sub-layer. The decoder is a 6-layer transformer decoder block. We employ a 100-channel filter bank with a 3-dimension pitch for the input. We use byte-pair encoding (BPE) \cite{ref45} to build 7002-sized subwords set, including $sos$, $eos$, and blank labels. Table 1 shows the WER obtained from our AM. The AM selects the hypothesis with the highest score from the candidate hypotheses. The WER is $3.4\%$ on \textit{test-clean} and $9.04\%$ on \textit{test-other}. Our MTLM model has six 12-head Transformer blocks. The self-attention dimension is 768, and the feed-forward network dimension is 3072 in each Transformer block. The same 7002-sized word vocabulary with the AM is used in LM. We train the LM from scratch used the text-only data of LibriSpeech corpus and optimized by Adam optimizer \cite{ref46}, where $\beta_1$=0.9, $\beta_2$=0.999, warmup steps are 5000, the maximum learning rate is $2e^{-4}$ and decay to $1e^{-6}$.

\subsection{Results and Discussion}
In this section, we compare the WER performance of the ASR system after integrating the AM with the traditional unidirectional LM (UNILM) or our MTLM model by $n$-best rescoring or shallow fusion. Table 1 summarizes the WER result. On the LibriSpeech dataset \cite{ref26}, we train the UNILM and MTLM models from scratch. The UNILM model is trained using the ULM task specified in Section 3.2, frequently utilized in ASR.

The first row of Table 1 represents the WER performance of AM. The following two parts are the WER of the ASR system using different decoding strategies after the introduction of the UNILM and the MTLM model, respectively. The experimental results in Table 1 demonstrate that regardless of the decoding strategy used, the UNILM or MTLM models outperform the AM without the addition of LM. Particularly, the MTLM model can generate the likelihood score of Equation (5) based on ULM task, and it can also generate the pseudo log-likelihood score of Equation (6) based on BMLM task. Therefore, the MTLM can do $n$-best rescoring in these two ways. The MTLM model has a WER of $2.63\%$ on \textit{test-clean} and a WER of $7.08\%$ on \textit{test-other}, both exceeding UNILM. 

As a result, Table 1 demonstrates the efficacy of introducing MBLM techniques. Simultaneously, we believe that the distinction between the two decoding methods is that the LMs can participate in scoring after the AM has generated an entire path of beam size in $n$-best rescoring \cite{ref20,ref47}. Therefore, the AM alone determines the generation of hypotheses, and the rescoring method is constrained. When each character is generated with shallow fusion, the LM contributes to the scoring in the decoding \cite{ref_17_chorowski2016towards,ref18}. The final beam size hypotheses are assumed to be generated jointly by the AM and LMs, so the WER performance is better.

\begin{table}[ht]
\footnotesize
\vspace{-0.4cm} 
\setlength{\abovecaptionskip}{0.1cm}
\setlength{\belowcaptionskip}{-0.1cm}
  	\caption{The WER (\%) performance of UNILM and MTLM models using various decoding methods (lower is better). $\dagger$ means that the model uses Equation $\left(5\right)$ for $n$-best rescoring, $\blacklozenge$ means that the model uses Equation $\left(6\right)$ for $n$-best rescoring, and $\clubsuit$ means that the model uses the shallow fusion decoding. (Beam Size=3)
  	}
  \label{taable1}
  \centering
  \begin{tabular}{ lccccc }
    \toprule
    \multirow{2}{*}{Model}  & \multicolumn{2}{c}{dev} & \multicolumn{2}{c}{ test } \\
        \cline { 2 - 5 }
  & clean  & other  & clean &  other \\
    \midrule
AM & 3.18 & 8.94 & 3.4 & 9.04 \\
    \midrule
+$\mathbf{UNILM}^{\dagger}$ &  3.02 & 8.69 & 3.18 & 8.78\\
+$\mathbf{UNILM}^{\clubsuit}$     & 2.57 & 6.99 & 2.81 & 7.36\\
    \midrule
+$\mathbf{MTLM}^{\dagger}$ & 3.01  & 8.67 & 3.14 & 8.75 \\
+$\mathbf{MTLM}^{\blacklozenge}$   & 3.03 & 8.67 & 3.14 & 8.76\\
+$\mathbf{MTLM}^{\clubsuit}$    & \textbf{2.46} & \textbf{6.75} & \textbf{2.63} & \textbf{7.08}\\
    \bottomrule
  \end{tabular}
\vspace{-0.7cm}
\end{table}

\subsection{Error Type Analysis}

In this section, we merge the \textit{test-clean} and \textit{test-other} data into one dataset. Then according to the number of words in the transcript corresponding to each audio, this dataset is divided into 3 categories. As shown in Table 2, long, medium, and short correspond to a word count larger than 20, between 10 and 20, and less than 10 words, respectively. The AM in Table 2 represents the baseline, and the subsequent four numbers reflect the number of deletions, insertions, replacements, and overall errors of the AM. 
\begin{table}[htp]
\label{wer}
\footnotesize
\vspace{-0.5cm}
\setlength{\abovecaptionskip}{0.1cm}
\setlength{\belowcaptionskip}{-0.1cm}
  	\caption{The number of error types for AM, UNILM and MTLM (lower is better). The experimental results are classified into three categories: long (L), medium (M), and short (S), and the error types are deletion, insertion, replacement, and overall error. (Beam Size=3)}
  \label{taable1}
  \centering
  \begin{tabular}{ ccccccc }
    \toprule
 Len & Model & Del & Ins &  Sub & Overall\\
    \midrule
     \multirow{5}{*}{L} & $\mathbf{AM}$ & 374 & 284 & 2650 & 3308\\
 & +$\mathbf{UNILM}^{\dagger}$ & 320 & 274 & 2541 & 3135  \\
 & +$\mathbf{MTLM}^{\dagger}$ & \textbf{319} & 273 & 2531 & 3123 \\
  & +$\mathbf{MTLM}^{\blacklozenge}$ & 320 & 272 & 2533 & 3125\\
 & +$\mathbf{UNILM}^{\clubsuit}$ & 370 & 205 & 2079 & 2654\\
 & +$\mathbf{MTLM}^{\clubsuit}$ & 367 & \textbf{199} & \textbf{1932} & \textbf{2498} \\
    \midrule 
    
     \multirow{5}{*}{M} & $\mathbf{AM}$ & 207 & 165 & 1620 & 1992\\
 & +$\mathbf{UNILM}^{\dagger}$ & 204 & 166 & 1565 & 1935 \\
 & +$\mathbf{MTLM}^{\dagger}$ & 203 & 164 & 1558 & 1925\\
   & +$\mathbf{MTLM}^{\blacklozenge}$ & 204 & 166 & 1559 & 1929 \\
 & +$\mathbf{UNILM}^{\clubsuit}$ & 203 & 116 & 1297 & 1616 \\
 & +$\mathbf{MTLM}^{\clubsuit}$ & \textbf{190} & \textbf{110} & \textbf{1260} & \textbf{1560}\\
    \midrule 
    
     \multirow{5}{*}{S} & $\mathbf{AM}$ & 150 & 104 & 975 & 1229 \\
 & +$\mathbf{UNILM}^{\dagger}$ & 154 & 105 & 943 & 1202 \\
 & +$\mathbf{MTLM}^{\dagger}$ & 153 & 103 & 936 & 1192 \\
   & +$\mathbf{MTLM}^{\blacklozenge}$ & 149 & 102 & 942 & 1193\\
 & +$\mathbf{UNILM}^{\clubsuit}$ & \textbf{135} & 89 & 855 & 1079 \\
 & +$\mathbf{MTLM}^{\clubsuit}$ & 143 & \textbf{78} & \textbf{826} & \textbf{1047} \\
  \bottomrule
  \end{tabular}
\vspace{-0.2cm}
\end{table}

Table 2 demonstrates that only in the short category when shallow fusion is used as the decoding algorithm, the MTLM is inferior to the UNILM model. The data shows that the number of error types is 143 for MTLM, while UNILM is 135. Nevertheless, in all other circumstances, MTLM can further reduce the number of errors of the UNILM. In particular, the MTLM significantly and consistently outperforms UNILM in reducing the number of overall errors.

\subsection{The GuideScore Algorithm}
\begin{table}[ht]
\footnotesize
 \vspace{-0.7cm}
\setlength{\abovecaptionskip}{0.1cm}
  \caption{The difference between whether LM participates in the GuideScore algorithm. (Beam Size=3)}
  \label{tab3}
  \centering
  \begin{tabular}{ cccccc }
    \toprule
    \multirow{2}{*}{GuideScore} & \multirow{2}{*}{Model} & \multicolumn{2}{c}{ dev } & \multicolumn{2}{c}{ test } \\
        \cline { 3 - 6 }
 & &  clean  &  other  &  clean  &  other \\
    \midrule
    \multirow{2}{*}{ S2S } & $\mathbf{UNILM}^{\clubsuit}$ &  2.57 & 6.99 & 2.81 & 7.36\\
    & $\mathbf{MTLM}^{\clubsuit}$ & \textbf{2.46} &  \textbf{6.75} & \textbf{2.63} & \textbf{7.08}\\
    \midrule
    \multirow{2}{*}{ LM+S2S } & $\mathbf{UNILM}^{\clubsuit}$ & 2.58  & 6.98 & 2.82 & 7.3 \\
    & $\mathbf{MTLM}^{\clubsuit}$ & \textbf{2.46} & 6.77 & 2.64 & 7.1\\
    \bottomrule
  \end{tabular}
 \vspace{-0.2cm}
\end{table}
In decoding, there is a $GuideScore$ algorithm in \cite{ref25}. This algorithm is responsible for generating the next set of candidate characters for the hypothesis. This algorithm can use the joint score of S2S and LM, or only use the score of S2S. All experimental data in Table 1 and 2 are obtained solely by using the S2S model score from the AM as the hypothesis score basis for decoding. Therefore, we conducted an experiment to study the performance changes of the $GuideScore$ algorithm after introducing LM participation. As shown in Table 3, when both the LM and the S2S model are used in the $GuideScore$, the WER performance of the UNILM and MTLM models on both the \textit{test-clean} and \textit{test-other} datasets is slightly lower than that of the S2S model alone. The reason may be that the excessive participation of the LM interferes with the judgment of the AM and makes the entire ASR system pay more attention to the LM.


\subsection{The Effect of Different Beam Sizes}
\begin{figure}[htp]
\vspace{-0.4cm}
\setlength{\abovecaptionskip}{0.1cm}
  \centering
  \includegraphics[width=\linewidth]{figure/bs15.png}
	\vspace{-0.6cm}  
  \caption{The WER performance of the AM, UNILM and MTLM on the \textit{test-clean} and \textit{test-other} datasets with different beam sizes.}
  \label{fig:speech_production}
 \vspace{-0.2cm}
\end{figure}
Finally, as illustrated in Figure 2, we compare the WER of the AM, the UNILM, and the MTLM model when beam size varies. No matter which decoding method is used, MTLM outperforms UNILM and AM. Additionally, the WER decreases regardless of decoding method as beam size increases. Among them, shallow fusion achieves the most significant performance improvement.

\section{Conclusions}
In this paper, we propose an LM training method that allows the model to be used for rescoring and shallow fusion, and the model can also learn bidirectional information. We also make a detailed comparison of model performance. Based on experiments on LibriSpeech, we demonstrate that the MTLM model can further improve the ASR accuracy compared to UNILM. Finally, we also conduct experiments on error type to evaluate the specific advantages of the MTLM. In future work, we will investigate alternative methods for training LMs.




% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{strings,icassp}

\end{document}
