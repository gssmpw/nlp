



\begin{table*}[!htbp]
 \caption{
 Summary of datasets used in \OurMethod for different stages.
 `Comic-9K' and `MovieNet-Summary' are publicly available at:
 \href{https://huggingface.co/datasets/VITA-MLLM/Comic-9K}{https://huggingface.co/datasets/VITA-MLLM/Comic-9K} and \href{https://huggingface.co/datasets/VITA-MLLM/MovieNet-Summary}{https://huggingface.co/datasets/VITA-MLLM/MovieNet-Summary}, respectively.
 }
 % \vspace{-15pt}
 % \footnotesize
 \begin{center}
 \begin{adjustbox}{max width=0.99\textwidth}
 \begin{tabular}{c|lc|cccc}
 \toprule
 
 
 \multirow{2}{*}{Type} 
 & \multirow{2}{*}{Name}
 & \multirow{2}{*}{Total Number} & \multicolumn{4}{c}{Sampling Ratio / Max Number} \\
 
 & & & Stage 1 & Stage 2 & Stage 3 & Stage 4\\

 \midrule

 
 \multirow{9}{*}{Image-Text}
    
 & LLaVA-ReCap~\hfilll~\cite{li2024llavanext-ablations} & 3.5M & 1.0 & 1.0 & 0.1 & 0.1 \\

 & ALLaVA-4V~\hfilll~\cite{ALLaVA-4V} & 1.4M & 1.0 & 1.0 & 0.1 & 0.1 \\

 & LVIS-Instruct4V~\hfilll~\cite{LVIS-Instruct4V} & 222K & 0.0 & 1.0 & 0.1 & 0.1 \\

 & ShareGPT4V~\hfilll~\cite{ShareGPT4V} & 1.3M & 1.0 & 1.0 & 0.1 & 0.1 \\

 & the-cauldron~\hfilll~\cite{Idefics2} & 1.1M & 0.0 & 1.0 & 0.1 & 0.1 \\

 & Docmatix~\hfilll~\cite{Idefics3} & 1.2M & 1.0 & 1.0 & 0.1 & 0.1 \\

 & LLaVA-OneVision-Mid~\hfilll~\cite{LLaVA-OneVision} & 444K & 1.0 & 1.0 & 0.1 & 0.1 \\

 & LLaVA-OneVision~\hfilll~\cite{LLaVA-OneVision} & 3.6M & 0.0 & 1.0 & 0.1 & 0.1 \\

 & M4-Instruct~\hfilll~\cite{li2024llavanext-interleave} & 860K & 0.0 & 1.0 & 0.1 & 0.1 \\

 & Comic-9K & 9K & 0.0 & 0.0 & 1.0 & 1.0 \\

 \midrule

 \multirow{5}{*}{Video-Text}
 
 & VideoGPT-plus~\hfilll~\cite{VideoGPT-plus} & 575K & 0.0 & 1.0 & 0.1 & 0.1 \\

 & ShareGemini-cap~\hfilll~\cite{ShareGemini} & 323K & 0.0 & 1.0 & 0.1 & 0.1 \\

 & LLaVA-Video-178K~\hfilll~\cite{LLaVA-Video} & 1.6M & 0.0 & 0.0 & 1.0 & 1.0 \\
 
 & MovieNet-Summary & 1K & 0.0 & 0.0 & 0.0 & 1.0 \\
 
 \midrule

 \multirow{9}{*}{Short Text}
 
 & OpenHermes-2.5~\hfilll~\cite{OpenHermes-2.5} & 1.0M & 0.0 & 1.0 & 0.1 & 0.1 \\

 & LIMA~\hfilll~\cite{LIMA} & 1K & 0.0 & 1.0 & 0.1 & 0.1 \\

 & databricks-dolly-15k~\hfilll~\cite{databricks-dolly-15k} & 15K & 0.0 & 1.0 & 0.1 & 0.1 \\

 & MetaMathQA~\hfilll~\cite{MetaMathQA} & 395K & 0.0 & 1.0 & 0.1 & 0.1 \\

 & MathInstruct~\hfilll~\cite{MathInstruct} & 262K & 0.0 & 1.0 & 0.1 & 0.1 \\

 & Orca-Math~\hfilll~\cite{Orca-Math} & 200K & 0.0 & 1.0 & 0.1 & 0.1 \\

 & atlas-math-sets~\hfilll~\cite{atlas-math-sets} & 17.8M & 0.0 & 1.0 & 0.1 & 0.1 \\

 & goat~\hfilll~\cite{goat} & 1.7M & 0.0 & 1.0 & 0.1 & 0.1 \\

 & camel-ai-math~\hfilll~\cite{CAMEL} & 50K & 0.0 & 1.0 & 0.1 & 0.1 \\

 \midrule

 \multirow{8}{*}{Long Text}
 
 & Long-Instruction~\hfilll~\cite{Long-Instruction-with-Paraphrasing} & 16K & 0.0 & 0.0 & 1.0 & 1.0 \\

 & LongForm~\hfilll~\cite{LongForm} & 23K & 0.0 & 0.0 & 1.0 & 1.0 \\

 & LongAlign-10k~\hfilll~\cite{LongAlign} & 10K & 0.0 & 0.0 & 1.0 & 1.0 \\

 & LongCite-45k~\hfilll~\cite{LongCite} & 45K & 0.0 & 0.0 & 1.0 & 1.0 \\

 & LongWriter-6k~\hfilll~\cite{LongWriter} & 6K & 0.0 & 0.0 & 1.0 & 1.0 \\

 & LongQLoRA~\hfilll~\cite{LongQLoRA} & 39K & 0.0 & 0.0 & 1.0 & 1.0 \\

 & LongAlpaca~\hfilll~\cite{LongLoRA} & 12K & 0.0 & 0.0 & 1.0 & 1.0 \\

 & Long-Data-Collections~\hfilll~\cite{Long-Data-Collections} & 98K & 0.0 & 0.0 & 1.0 & 1.0 \\


 \bottomrule%==============================================================================================================
 \end{tabular}
 \end{adjustbox}
 \end{center}
 \label{table_data}
 % \vspace{-20pt}
\end{table*}