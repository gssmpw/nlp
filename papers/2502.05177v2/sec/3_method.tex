





\section{\OurMethod}

\subsection{Architecture}
\label{sec:Architecture}


\input{tables/data}

\input{tables/train}



This technical report presents \OurMethod, a new series of open-source vision-language models that explore long-context vision understanding without token compression and sparse local attention.
%
Our multi-modal architecture is constructed around three core components: the Vision Encoder, the Projector, and the LLM.
%


% \begin{itemize}
%     \item
    \textbf{Large Language Model.}
    We choose Qwen2.5-14B-Instruct~\cite{Qwen2.5} as our LLM.
    
    % \item
    \textbf{Vision Encoder.}
    We consider the InternViT-300M~\cite{InternVL} as the visual encoder.
    %
    We introduce a dynamic tiling vision encoding strategy~\cite{InternVL2} that efficiently processes high-resolution images of varying aspect ratios.

    % \item
    \textbf{Vision-Language Projector.}
    We employ a $2$-layer MLP to project image features into the word embedding space.
    We also apply a simple pixel shuffle~\cite{InternVL2} to visual tokens and reduce the number of visual tokens to one-quarter.
    %
% \end{itemize}


\subsection{Data Construction}
\label{sec:Data}

%
\OurMethod is trained on open-source datasets only.
%
As shown in Tab.~\ref{table_data}, the training dataset encompasses a diverse range of sources.
%


\textbf{Image-Text Data.}
%
The datasets employed can be categorized into three groups:
%
\begin{itemize}[leftmargin=2.0em]
    \item
    \textbf{Image Captioning.}
    %
    The visual caption dataset consists of LLaVA-ReCap~\cite{li2024llavanext-ablations}, ALLaVA-4V~\cite{ALLaVA-4V}, ShareGPT4V~\cite{ShareGPT4V} and LLaVA-OneVision-Mid~\cite{LLaVA-OneVision}.
    %

    \item
    \textbf{Visual Question Answering.}
    %
    We combine general VQA from LVIS-Instruct4V~\cite{LVIS-Instruct4V}, the-cauldron~\cite{Idefics2}, Docmatix~\cite{Idefics3}, LLaVA-OneVision~\cite{LLaVA-OneVision}.
    %

    \item
    \textbf{Interleaved Image-Text.}
    %
    To empower all-round multi-image capabilities, we employ the M4-Instruct~\cite{li2024llavanext-interleave}.
    %
    To further enhance multi-image understanding with more than $10$ images, we collect the public comic book with the corresponding detailed synopsis from the web and build the Comic-9k datasets.
    %
    Specifically, Comic-9k contains $200$K images, spanning $9$K comic books, along with a manual-labeled synopsis.
    %
\end{itemize}


\input{tables/max_length}


\textbf{Video-Text Data.}
%
We construct our video understanding data using VideoGPT-plus~\cite{VideoGPT-plus}, ShareGemini~\cite{ShareGemini}, and LLaVA-Video-178K~\cite{LLaVA-Video}.
%
To improve the long-context capability of movie-level video understanding, we build a MovieNet-Summary dataset, which consists of paired movies and synopses from MovieNet~\cite{MovieNet}.
%


%
\textbf{Short Text Data.}
Following~\cite{Idefics3}, the pure text data is collected from OpenHermes-2.5~\cite{OpenHermes-2.5}, LIMA~\cite{LIMA}, databricks-dolly-15k~\cite{databricks-dolly-15k}, MetaMathQA~\cite{MetaMathQA}, MathInstruct~\cite{MathInstruct}, Orca-Math~\cite{Orca-Math}, atlas-math-sets~\cite{atlas-math-sets}, goat~\cite{goat}, and camel-ai-math~\cite{CAMEL}.
%


%
\textbf{Long Text Data.}
To transfer the context length of the language model to the modality-aligned multi-modal models~\cite{LongVA}, we gather several long text datasets, including Long-Instruction-with-Paraphrasing~\cite{Long-Instruction-with-Paraphrasing}, LongForm~\cite{LongForm}, LongAlign-10k~\cite{LongAlign}, LongCite-45k~\cite{LongCite}, LongWriter-6k~\cite{LongWriter}, LongQLoRA~\cite{LongQLoRA}, LongAlpaca~\cite{LongLoRA}, and Long-Data-Collections~\cite{Long-Data-Collections}.
%


%
Note that Comic-9k and MovieNet-Summary are created by this work and are made publicly available.
%
Therefore, \OurMethod is \textbf{only} trained on open data, and we \textbf{do not} use data filtering methods.
%






\subsection{Training Pipelines}
\label{sec:Training}


%
Unlike other models, \OurMethod training is divided into four stages with varying sequence lengths.
%


% \begin{itemize}
    % \item
    \textbf{Stage 1: Vision-Language Alignment.}
    %
    Building upon pre-trained language models, our primary objective is to establish initial connections between visual features and language features.
    %
    We freeze the LLM and the visual encoder, only training the visual projector.
    %
    Therefore, we mainly use caption data for pre-training.
    %
    We also add Docmatix~\cite{Idefics3} in this stage to improve document-based VQA.
    %
 

    % \item
    \textbf{Stage 2: General Knowledge Learning.}
    %
    After establishing the vision-language alignment in the embedding space, we dedicate most of our computational resources to vision-language general knowledge learning.
    %
    This stage leverages all the image-text data for multiple tasks, including image captioning, common VQA, OCR, and multi-model conversations.
    %
    In this stage, we also add text-only general instructions, math problems, and arithmetic calculations.
    %
    For video understanding, we only add VideoGPT-plus~\cite{VideoGPT-plus} and ShareGemini-cap~\cite{ShareGemini}.
    %
    In both Stage 1 and Stage 2, we pack all training data to a fixed sequence length, which effectively trains samples with different lengths of sequences.
    %
    Specifically, we random sample data items from the same source and concatenate them into one training sample with a token length of $32$K and $16$K for Stage 1 and Stage 2, respectively.
    %
    We reset positional embeddings and attention masks for all packed samples so that each text-vision pair only attends to itself.
    %
    This approach helps manage extensive datasets and ensure diverse data segments' coverage.
    %

    % \item
    \textbf{Stage 3: Long-Sequence Fine-Tuning.}
    %
    In this stage, we extend the context length to $128$K.
    %
    We reduce the sampling ratio of the data in Stage 2 to $0.1$ and incorporate additional long-context text instructions, comic book summaries, and video understanding datasets.
    %

    % \item
    \textbf{Stage 4: Long-Sequence Fine-Tuning.}
    %
    In this stage, we extend the context length to $1,024$K and add additional movie summary data.
    %
    In both Stage 3 and Stage 4, we also pack all training data to a fixed sequence length without resetting positional embedding and attention mask.
    %
    Therefore, we impose the model to capture the correlations between these two modalities in long-contextual information.
    %
    
% \end{itemize}

%
We \textbf{do not} use the interpolation technique during training and testing, therefore, the context window of \OurMethod can be extended further when equipped with YaRN~\cite{YaRN}, LongRoPE~\cite{LongRoPE} and NTK-based interpolation.
%
Note that we \textbf{do not} use any parameter-efficient methods such as LoRA~\cite{LoRA} or approximate attention~\cite{LongLoRA}.
%




\input{tables/open_compass}

\input{tables/video_mme}

\input{tables/open_compass_video}





\subsection{Hyper-parameters and Infrastructures}

%
We initially implement training and inference with MindSpeed~\cite{MindSpeed} and MindSpeed-LLM~\cite{MindSpeed-LLM}, which adapt Megatron-LM~\cite{Megatron-LM} to Ascend NPU.
%
We also transfer the training and inference code to the GPU platform.
%
As shown in Tab.~\ref{table_train}, we list the detailed hyper-parameters in \OurMethod.
%



\textbf{Training.}
%
We configure different distributed training strategies for each key module in \OurMethod.
%

\begin{itemize}[leftmargin=2.0em]
    \item
    \textbf{Large Language Model.}
    %
    We employ data, pipeline, tensor, sequence, and context parallelism.
    %
    We enable distribution attention for context parallelism to train long sequences in Stages 3 and 4.
    %
    
    \item
    \textbf{Vision Encoder.}
    %
    We apply data, tensor, and sequence parallelism to the vision module, which is in the first LLM pipeline parallelism stage.
    %
    We do not use context parallelism for the vision encoder.
    %
    
    \item
    \textbf{Vision-Language Projector.}
    %
    The multi-modal projector follows the configuration of the vision encoder during the distributed training.
    %
    
    %
\end{itemize}





\textbf{Inference.}
%
We implement two new designs to scale up the number of tokens for model inference.
%


\begin{itemize}[leftmargin=2.0em]
    \item
    \textbf{Context-Parallelism Distributed Inference.}
    %
    We implement tensor parallelism with context parallelism for model inference, thus supporting distribution attention for infinite-length input tokens.
    %
    Similar to the training mode, the length of inference tokens is fixed during the decoding phase.
    %
    Specifically, we concatenate the input tokens with padding tokens of the maximum output length.
    %
    The system needs to extract the new predicted next token in the fixed-length output tokens and accordingly terminate the generation process during each forward.
    %
    \item
    \textbf{Logits-Masked Language Modeling Head.}
    %
    We observe that the output logits from the language modeling head induce excessive memory footprints.
    %
    For example, given $1$M tokens with $10^5$ vocabularies, the output logit matrix has a shape of $10^6 \times 10^5$ and requires $400$ GB of memory for the float32 data type.
    %
    To address the memory issue, we mask out all hidden features and only feed the one hidden feature that predicts the next tokens to the language modeling head.
    %
    With the above design, the memory consumption of the output logit matrix is $0.0004$ GB with $10^6 \times$ reduction.
    %
    Note that this design can also apply to model training with long-context inputs and the language modeling head only needs to predict the short-context outputs to reduce memory consumption.
    %
\end{itemize}
%

%
We test the maximal sequence length of a fixed number of devices before they raise the out-of-memory error.
%
Tab.~\ref{table_max_length} summarizes the result.
%
Note that activation checkpointing is disabled in LongVILA~\cite{LongVILA}, while our model has much more number of parameters.
%



%
Tab.~\ref{table_lm_head} further shows the effectiveness of logits-masked language modeling head~(logits-masked LM head).
%
All methods are implement with Flash Attention and context-parallelism distributed inference on GPU with $96$G memory and about $150$ TFLOPS for bfloat16.
%
Compared to the original LM head, logits-masked LM head extends the max sequence length by $417\%$, and reduces time cost by $47.3\%$.
%
We also implement a chunked language modeling head~(chunked LM head), which process tokens with a chunk length of $32,768$.
%
Compared to the chunked LM head, logits-masked LM head achieves $11.3$ and $11.1$ speedup under the $1$M  and $1.6$M input lengths, respectively.
%



%
We employ Ring Attention~\cite{RingAttention} to distribute long sequences across multiple devices.
%
We do not use parameter-efficient fine-tuning methods or quantization strategies for both training and inference.
%
Additionally, the temperature is set to $0$ to guarantee consistent performance evaluation.
%
% Unless specified otherwise, ''\OurMethod`` in the subsequent text refers to \OurMethod-16K.
%

