\section{Related Work}
\label{sec:related_work}







\subsection{Large Vision Language Models}
%
Recent advancements have seen the creation of large vision language models (LVLMs), which usually enhance large language models with the capability to process and interpret visual information.
%
Flamingo~\cite{Flamingo} performs various multi-modal tasks, \eg, image captioning, visual dialogue, classification, or visual question answering, from only a few input/output examples.
%
BLIP-2~\cite{BLIP-2} leverages frozen pre-trained image encoders and large language models to bootstrap vision-language pre-training.
%
LLaVA~\cite{LLaVA} uses language models to generate multi-modal instruction-following data and connects a vision encoder and large language models for general-purpose visual and language understanding.
%
Qwen-VL~\cite{Qwen-VL} and InternVL~\cite{InternVL} series further perform various vision-language tasks, such as image captioning, question answering, text-oriented question answering, and visual grounding.
%
These works showcase that LVLMs have achieved significant breakthroughs.
%
Furthermore, significant progress in multi-modal model evaluation~\cite{MMBench,MME,MMStar} has also contributed to the rapid improvement of large vision-language models.
%
In this work, we introduce \OurMethod, a series of multi-modal and long-content models trained exclusively with fully open-source datasets for pre-training and supervised fine-tuning, demonstrating promising results on extensive benchmarks.
%






\subsection{Long-Context Multi-Modal Model}
%
LLMs are typically pre-trained with a pre-defined context length.
%
Training LLMs with long context from scratch is prohibitively expensive for most researchers.
%
Recently, several works, \eg, Position Interpolation~\cite{PositionInterpolation}, YaRN~\cite{YaRN}, LongRoPE~\cite{LongRoPE} and LongLoRA~\cite{LongLoRA} have tried to extend the context length of LLMs by fine-tuning.
%


%
Many methods have also been proposed in large multi-modal models to handle long-context visual inputs.
%
LongVILA~\cite{LongVILA} execute a continuation of pre-training on the LLM to enhance its context length to $256$K, followed by long video training.
%
LongLLaVA~\cite{LongLLaVA} integrates a hybrid of Mamba and Transformer blocks for long-context multi-modal understanding.
%
LongVU~\cite{LongVU} proposes a spatio-temporal adaptive compression scheme to reduce long video tokens by leveraging cross-modal query and inter-frame similarities.
%
LongVA~\cite{LongVA} extends the language model on text data and then aligns the extended model with visual inputs, which perceives more than $200$K visual tokens.
%
Kangaroo~\cite{Kangaroo} develops a curriculum training strategy that progressively equips the LLM basement with the capacity to comprehend long videos.
%


%
However, the above methods mainly focus on video understanding and visual information retrieval, neglecting the trade-off between image and long-video understanding tasks.
%
In this paper, we propose \OurMethod, a powerful LMM that obtains a long-context capacity of $1$ million tokens and simultaneously achieves superior performance on both image and video understanding tasks.
%



\subsection{Long-Context Visual Instruction Data}
%
LLaVA-Video~\cite{LLaVA-Video} synthesizes long video-language instruction data, covering various tasks such as captioning, open-ended, and multi-choice QA.
%
LongVILA~\cite{LongVILA} constructs instruction-following datasets from long videos, which encompasses summarization and other queries relevant to a comprehensive understanding of the content of long videos.
%
%
Video-MME~\cite{Video-MME} incorporates a diverse range of video types, varying temporal durations, ranging from $11$ seconds to $1$ hour.
%
LVBench~\cite{LVBench} evaluates long-context LMMs that feature video-language interleaved inputs up to an hour long.
%
LongVideoBench~\cite{LongVideoBench} introduces referring reasoning questions and presents significant challenges for both proprietary and open-source LMMs in their long-context multi-modal capabilities.
%


%
However, the above works only focus on long-context understanding in videos, which usually contain redundant visual frames and other modal information, such as subtitles and audio.
%
In this paper, we explore the comic-based long-context instruction learning and collect a high-quality real dataset for comic book summarization,
%

