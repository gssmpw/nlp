\section{Conclusion}
\label{sec:conclusion}


\textbf{Contributions.}
%
This work introduces the \OurMethod series models as a primary exploration into powerful long-context vision-language models.
%
Thanks to open-source data and training infrastructures, \OurMethod has excellent results compared to the cutting-edge models under $20$B parameters with in-house data and achieves new state-of-the-art performance for both image and video understanding on serval benchmarks.
%


\textbf{Limitations.}
%
Despite the promising performance, there are several limitations with the current models \OurMethod.
%
(1) Data Filtering.
%
\OurMethod is trained on massive open-source data without filtering.
%
Therefore, data selection still leaves plenty of room for performance improvement.
%
(2) Long-Content Performance.
%
As the \OurMethod-128K outperforms \OurMethod-1M for both image and video understanding, the training pipeline and long-content data still need to be improved.
%



\textbf{Future Works.}
%
Considering the current limitations and the promising future of vision-language models, we also anticipate increasing efforts in expanding \OurMethod capabilities to encompass other modalities, such as 3D point-cloud and audio, \etc.
%
We believe that simultaneous advancements in the training pipeline and multi-modal capacity will soon lead to long-content models that provide a satisfying user experience.
%

