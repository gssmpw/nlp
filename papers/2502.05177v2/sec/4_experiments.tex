

\section{Experiments}


\subsection{Experiment Settings.}

%
We evaluate \OurMethod’s performance on image and video understanding with different sequence lengths, respectively.
%
We perform a comprehensive evaluation on the OpenCompass benchmark, which covers visual question answering, multimodal conversation, knowledge and reasoning, OCR, and hallucination.
%
OpenCompass is a comprehensive collection with $8$ popular multimodal benchmarks, including MMBench~\cite{MMBench}, MMStar~\cite{MMStar}, MMMU~\cite{MMMU}, MathVista~\cite{MathVista}, HallusionBench~\cite{HallusionBench}, AI2D~\cite{AI2D}, OCRBench~\cite{OCRBench}, and MMVet~\cite{MMVet}.
%
We report the average scores for all collections.
%
We also calculate the average scores of the objective benchmark only.
%
We adopt Video-MME~\cite{Video-MME} as the video understanding evaluation indicator.
%
Video-MME is an ideal benchmark for assessing LMMs’ ability to handle long videos in real-world scenarios, given its average video duration of 1017 seconds and the inclusion of short, medium, and long subsets.
%
We use greedy search to ensure reproducible experimental results.
%


%
We compare \OurMethod to a set of cutting-edge models, which we group into three families:
%
\begin{itemize}[leftmargin=2.0em]
    \item
    \textbf{Open weight models.}
    Models are released with only their final checkpoint; little or no information about their training data and recipe is known.
    
    \item
    \textbf{Partially open models.}
    Models are released with weights, and most of the data or details necessary to reproduce them are known.

    \item
    \textbf{Fully open models.}
    Models are fully released with weights, training data, code, and evaluation, and thus can be fully reproduced.
    
\end{itemize}


\subsection{Image Evaluation}

%
Tab.~\ref{table_open_compass} presents the main results on the OpenCompass leaderboard, which evaluates our model on various image benchmarks to investigate the image performance.
%
As shown in Tab.~\ref{table_open_compass}, \OurMethod-16K demonstrates superior performance among open-source models.
%
In most benchmarks, \OurMethod-16K surpasses Qwen2-VL-7B and InternVL2-8B.
%
This highlights \OurMethod-16K’s impressive capabilities in handling multi-image tasks.
%
\OurMethod-16K also achieves new state-of-the-art performance on MMMU and HallusionBench and outperforms strong open-source models by a notable margin.
%
However, the results in \OurMethod-1M fall short of \OurMethod-16K and \OurMethod-128K, since we pack samples without isolating them via attention masks during 1M training and the potential confusion may arise from different sources of training data.
%
In summary, \OurMethod achieves very competitive performance compared to other models with in-house data under the 20B parameters.
%
This demonstrates that pure open-source data can also build robust models with strong performance.
%


\subsection{Video Evaluation}

%
\OurMethod models also show strong video understanding capabilities.
%
Tab.~\ref{table_video_mme} compares different models with various frame numbers on Video-MME.
%
In particular, the effectiveness of \OurMethod is further underscored by its performance on Video-MME.
%
Specifically, \OurMethod-128K with $256$ frames exceeds all other models under the 20B parameters.
%
It shows exceptional results, especially in tasks involving medium- to long-length videos, outperforming other fully open video models such as LLaVA-Video-7B-Qwen~\cite{LLaVA-Video} and LongVILA-7B~\cite{LongVILA}.
%
\OurMethod-1M supports a maximum number of $4,096$ frames as input and achieves competitive performance.
%
Note that \OurMethod is fully compatible with slow-fast~\cite{SF-LLaVA} and progressive pooling~\cite{progressive_pooling} strategies, which are train-free video representations to extend the visual context window.
%
Meanwhile, we do not adjust the scale factor of rotary position embedding during the pre-training and fine-tuning stages; thus employing existing interpretation methods~\cite{PositionInterpolation,YaRN} can further achieve extrapolation during the inference phase.
%

%
To further demonstrate the exceptional long- and short-context understanding capability of our method, we conduct experiments on benchmarks on LongVideoBench~\cite{LongVideoBench} and MVBench~\cite{MVBench} for long- and short-context video understanding tasks.
%
The results are shown in Tab.~\ref{table_open_compass_video}, highlighting our model's efficacy across varying temporal lengths.
%
The \OurMethod-128K model is remarkably capable of understanding long-form video content.
%
Specifically, our \OurMethod-128K model surpasses all existing $7$B to $20$B model series on LongVideoBench.
%
Furthermore, the \OurMethod-1M model showcases strong performance across video understanding of $64$ to $4,096$ frames.
%


%
Remarkably, despite achieving these impressive results, \OurMethod is only learned from open-source data compared to other models.
%
These results reflect a significant advancement in the efforts of the research community to close the performance gap with commercial models.
%


