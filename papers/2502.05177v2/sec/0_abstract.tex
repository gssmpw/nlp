\begin{abstract}



%
%Establishing the long-context capability of large vision-language models is crucial for video understanding, high-resolution image understanding, multi-modal agents and reasoning.
%
We introduce \OurMethod, a simple yet effective large multi-modal model for long-context visual-language understanding tasks.
%
It is adept at concurrently processing and analyzing modalities of image, video, and text over \textbf{4K} frames or \textbf{1M} tokens while delivering advanced performances on short-context multi-modal tasks.
%
We propose an effective multi-modal training schema that starts with large language models and proceeds through vision-language alignment, general knowledge learning, and two sequential stages of long-sequence fine-tuning.
%
We further implement context-parallelism distributed inference and logits-masked language modeling head to scale \OurMethod to infinitely long inputs of images and texts during model inference.
%
Regarding training data, \OurMethod is built on a mix of $17$M samples from \textbf{public datasets only} and demonstrates the state-of-the-art performance on various multi-modal benchmarks, compared against recent cutting-edge models with internal data.
%
% We also built a Comic-9k dataset to enhance multi-image understanding, and each sample contains an average of $20$ high-resolution images with the corresponding detailed synopsis from the web.
%
\OurMethod is \textbf{fully reproducible} and supports both NPU and GPU platforms for training and testing.
%
By leveraging our inference designs, \OurMethod models achieve a remarkable \textbf{2$\times$ prefill speedup} and \textbf{4$\times$ context length extension} in single node with $8$ GPUs.
%
% With strong performance for both image and video understanding across various benchmarks, w
We hope \OurMethod can serve as a competitive baseline and offer valuable insights for the open-source community in advancing long-context multi-modal understanding.
%


\end{abstract}



