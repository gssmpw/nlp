\section{Introduction}
\label{sec:intro}


%
In recent years, proprietary Large Multi-Modal Models (LMMs) have been undergoing rapid iteration and evolution~\cite{GPT-4,Gemini2.0,Claude3.5}, progressively extending large language models (LLMs) with multi-sensory skills, such as visual understanding.
%
Beyond closed-source models, open-source models, including LLaVA series~\cite{LLaVA-1.5,LLaVA-OneVision}, Qwen-VL series~\cite{Qwen-VL,Qwen2-VL}, and VITA series~\cite{VITA,VITA-1.5}, are also making significant strides, trying to close the gap with their closed-source counterparts.
%




%
However, most of the above open-source works on visual understanding tasks typically focus on static images and short video inputs.
%
Proprietary models show superior support for long-content inputs, while open-source models lag significantly behind.
%
For example,  Gemini 1.5 pro~\cite{Gemini1.5} runs up to 1 million tokens in production and processes 1 hour of video information in one go.
%
Therefore, more information is needed on developing high-performing, long-context vision-language models for public use.
%
Recently, many works~\cite{LongVILA,LongLLaVA,LongVU,LongVA} have been proposed to address the challenges of training and inference of long-context information.
%
However, they~\cite{LongVILA,LongLLaVA} aim mainly to improve a comprehensive understanding of long video, neglecting the static image and short video input scenarios.
%
On the other hand, some works~\cite{SF-LLaVA,LongVU} rely on compressing visual tokens, which often comes at the expense of performance degradation.
%


%
To further push the limits of open-source model capabilities, we extend the context length to $1$ million tokens and introduce \OurMethod, a strong open-source long-context visual language model.
%
To this end, we employ a phased training approach that positions language as the pivot.
%
Specifically, \OurMethodâ€™s ability to handle extended contexts is systematically augmented through a four-stage process.
%
Beyond the conventional stages of vision-language alignment and supervised fine-tuning, our approach incorporates specialized stages for long-context supervised fine-tuning of $128$K and $1$M.
%
To achieve a good performance trade-off between the long and short sequences, \OurMethod takes advantage of the existing abundance of open-source image-text and video-text data.
%
We also introduce a multi-image summarization dataset, Comic-9K, comprising $9$k comic books and the corresponding detailed synopsis.
%
This dataset has a total of $200$k images with an average of $20$ high-resolution photos for each sample, and the synopsis is all manually written collecting from the web.
%





Our contributions can be summarized as follows.
\begin{itemize}
\item
%
We fully release \OurMethod to the open-source community, providing a powerful tool for developing and applying long-context multi-modal AI systems and encouraging further research in this domain.

\item
%
We further implement context-parallelism distributed inference and logits-masked language modeling head to scale up the infinite number of image and text tokens for model deployment.
%

% \item
% We construct a Comic-9k dataset to enhance multi-image understanding, which consists of $9$K comic books with an average of $20$ high-resolution images and the corresponding detailed synopsis.

\item 
%
We evaluate \OurMethod on a wide range of benchmarks.
%
Although \OurMethod is trained on open-source data only, comprehensive evaluations reveal that \OurMethod has emerged as the strong vision-language model among previous models of similar scale, especially in evaluating hallucinations and video understanding.
%

\end{itemize}

