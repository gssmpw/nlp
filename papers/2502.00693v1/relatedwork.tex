\section{Related Work}
\label{sec:related_work}

In Section~\ref{sec:related_word:bloom_filter}, we introduce the mechanism and properties, as long as some variants of the Bloom filter.
In Section~\ref{sec:related_work:dp}, we discuss several principle mechanisms used in differential privacy.
In Section~\ref{sec:related_work:data_mining_privacy}, we show the importance of differential privacy in contemporary data mining and recommendation systems. 

\subsection{Bloom Filter} \label{sec:related_word:bloom_filter}


The Bloom filter is first introduced by~\cite{b70} and 
there are many variants of the Bloom filter. One variant is the Cuckoo filter \cite{fakm14}, which ``kicks out'' the old hash value to another place when a hash conflict occurs. This implementation principle enables it to support the probability data structure of membership queries with deletion operation. Compared with the Standard Bloom filter, it is more suitable for application scenarios with frequent element updates, such as network traffic monitoring \cite{gjh18} and cache system \cite{wyqk22}.

Another variant is the Quotient filter~\cite{gfo18}, which differs from the traditional Bloom filter. It implements the heretical storage form of hash value atmosphere quotient and remainder. 
This approach results in the Quotient filter requiring less storage space and offering faster query speeds than the standard Bloom filter.
It is more suitable for membership queries in scenarios with limited resources and high latency requirements~\cite{pcd+21, aa16}.

\subsection{Differential Privacy} \label{sec:related_work:dp}

Differential privacy is a technique used to defend against privacy attacks, first proposed 
by Dwork et al.~\cite{dmns06}. It has become one of the most popular frameworks for ensuring privacy in theoretical analysis and a wide range of application scenarios 
\cite{llsy17, ygz+23,wcy+23,cxj24, sg24, gls+25, gll+24d,llsz24_nn_tw,lls+24_dp_je, gls+24d, fll24, syyz23, lhr+24, hll+24, ylh+24}. 

Gaussian mechanism~\cite{dmns06} and Laplace mechanism~\cite{dr+14} of DP are widely used techniques to achieve privacy budget. 
These two mechanisms control the amount of privacy provided by adjusting the variance of the added noise. However, these two mechanisms are very useful when the output is continuous, but they are slightly weak when the output is discrete.
However, another classic way to make a data structure private is to add a random responses mechanism \cite{w65}, also called a ``flip coin''. Specifically, some discrete values in the data structure are flipped with a certain probability to achieve privacy~\cite{ll23, ll24}. 
By controlling the probability of flipping, a given privacy budget is achieved.

Over the past decade, numerous works have applied differential privacy to data structures or deep learning models. \cite{knrs13} applied differential privacy to graph data structure and designed differentially node-private algorithms by projecting input graphs onto bounded-degree graphs, enhancing privacy while maintaining accuracy in realistic network analyses. \cite{wxy+18} introduced an adaptive method for directly collecting frequent terms under local differential privacy by constructing a tree, which can overcome challenges of accuracy and utility compared to existing n-gram approaches. \cite{fi19} focused on applying differential privacy to classical data mining data structures, specifically decision trees, and analyzed the balance between privacy and the utility of existing methods. \cite{zqr+22} demonstrated the integration of differential privacy into linear sketches, ensuring privacy while maintaining high performance in processing sensitive data streams.
A related work \cite{agk12} introduced the BLIP mechanism, which also applies the Random Flip mechanism to the Bloom Filter. Here, we outline the differences between our work and \cite{agk12} as follows:
($i$) Our proposed DPBloomFilter can satisfy $(\epsilon,\delta)-DP$, while \cite{agk12} only verified that BLIP mechanism can satisfy $\epsilon$-DP; 
($ii$) \cite{agk12} did not provide theoretical guarantees for the utility of the BLIP mechanism.


\subsection{Privacy in Data Mining and Recommendation System} \label{sec:related_work:data_mining_privacy}


The preservation of privacy is increasingly vital within the realm of data mining. recommendation systems\cite{kmt19} and the realm of Artificial Intelligence \cite{gls+25,fjl+24,lss+24_relu,cls+24,lss+24_multi_layer,wxz+24,wcz+23,wsd+24,kll+25,cgl+25,cll+25,cgl+25_homo}.

In data mining, various studies have emerged that concentrate on how to extract knowledge inherited in user behavior data without compromising user privacy. For instance, \cite{wdz24} introduced a density-based clustering technique incorporating differential privacy. \cite{tcnz24} delved into the application of local differential privacy (LDP) to forestall privacy violations during the aggregation of user data, in addition to investigating data poisoning attacks on LDP. 
Moreover, \cite{lzly23} has managed to maintain both efficiency and availability in mining user behavior features within specific industries while also employing differential privacy. Besides, \cite{sg24} proposes a novel differentially private GNN that employs a progressive training scheme and aggregation perturbation to enhance privacy while maintaining accuracy. 

On the recommendation system front, it has become common practice for streaming media and advertising companies to utilize sensitive user data, including real-time geographic locations, for user recommendations.
In response to these privacy concerns, \cite{mm09} first introduced a differential privacy framework tailored for recommendation systems. 
Besides, \cite{bmga+20} attempts to build a Recommendation with an Attribute Protection (RAP) model, which simultaneously recommends relevant items and counters private-attribute inference attacks.
More recently, \cite{xcs24} developed a federated recommendation framework that integrates differential privacy to shield user privacy, reducing the impact of privacy protection on recommendation quality. 
\cite{hh23} identified a heavy reliance on user data in existing recommendation systems, leaving them susceptible to privacy breaches. 

Privacy remains a critical concern in recommendation systems and data mining. This area of study is ripe for further exploration, and addressing these privacy challenges will require a substantial journey ahead.