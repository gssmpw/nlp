\section{Related Work}
\label{subsec:related-work}


% \paragraph{AI Agents.}
% Advances in LLM-driven AI agents have enabled tool-use/action capabilities, allowing them to perform tasks like online shopping **Vinyals et al., "Grammar Transformations for Neural Machine Translation"** __,**Henderson et al., "Deep Reinforcement Learning that Matters"****__, managing workflows ____ and interacting with APIs ____.
% These agents are increasingly versatile and even begin assisting in scientific research. However, their application to scientific workflows reveals challenges in rigor and reliability, and a specialized system for scientific experimentation is needed.

\paragraph{AI Agents for Science.}
Prior work has leveraged AI to accelerate scientific discovery**Brown et al., "Measuring Massive Multitask Transfer Learning"**, focusing on various stages of the research lifecycle, including literature reviews **Guu et al., "REALM: Towards Accurate Reading Comprehension for Long Documents"** __,**Hewlett et al., "Learning Simple Neural Program Equivalences"****__, brainstorming ideas ____ and data analysis ____.
While these efforts works on various aspects of the scientific lifecycle, experimentation—a critical, rigor-intensive step—remains underexplored.

% Add science tutor/assistant works as well
% Existing AI agent solutions**Henderson et al., "Deep Reinforcement Learning that Matters"**, whether generic or specialized for scientific research, typically rely on ad-hoc prompt-based interactions and predefined workflows.
% AI Scientist****rely on ad-hoc prompts to guide rigid processes, including hypothesis generation, code editing, experiment execution, result reporting, and review, limiting adaptability and iterative refinement in scientific inquiry.
% Frameworks****lack flexibility for diverse research scenarios, requiring users to write experimental code in rigid, framework-specific formats, which adds significant overhead and hinders usability.
Existing agents for end-to-end scientific research**Guu et al., "REALM: Towards Accurate Reading Comprehension for Long Documents"** rely on ad-hoc prompts to guide predefined workflows, from idea generation to paper writing.
Their open-sourced frameworks often require experimental code to follow constrained, framework-specific formats, adding overhead and hindering their usability.
These solutions mimic experimentation processes using multi-agent systems but lack systematic enforcement of a \textit{methodical procedure}, \textit{reliability}, and \textit{interpretability}.
Without these core principles, such agents struggle to deliver meaningful and reproducible results, limiting their practical utility in real-world scientific research.
% Additionally, existing frameworks****lack the flexibility required for diverse research scenarios. 
% For instance, users are often required to write experimental code in highly specific formats dictated by the framework, introducing significant overhead and making the tools cumbersome to use. 




% On the benchmarking side, existing benchmarks focus narrowly on tasks like ML training**Zoph et al., "Learning Transferable Architectures through Multi-Task Learning"**, while our benchmark spans diverse computer science domains and various difficulty level, offering a comprehensive evaluation of an agent's ability to automate rigorous experimentation.


\paragraph{AI Agent Task Benchmarks.}

A wide range of benchmarks have been developed to assess the capabilities of AI agents across diverse domains. Existing benchmarks primarily focus on logical reasoning ____ , problem-solving ____ , knowledge retrieval tasks ____ and machine learning training**Zoph et al., "Learning Transferable Architectures through Multi-Task Learning"**. These benchmarks evaluate agents on well-defined tasks that typically have clear, deterministic solutions.

In contrast, our benchmark focuses on experimentation, which requires a more rigorous and systematic approach beyond problem-solving. Experimental tasks require iterative hypothesis refinement, complex experiment setup and execution, and robust result interpretation.
Our benchmark captures these challenges by evaluating AI systems on real-world experimentation tasks derived from influential research papers and widely adopted open-source projects.


% Existing benchmarks are often narrowly focused, primarily evaluating agent frameworks on machine learning training**Zoph et al., "Learning Transferable Architectures through Multi-Task Learning"**. 
% Benchmarks like MLAgentBench****are narrowly scoped, primarily evaluating agent frameworks on simplified tasks like training machine learning models given predefined datasets. These benchmarks lack coverage of complex, research-driven problems, limiting their utility for assessing frameworks designed for rigorous scientific experimentation.
% Other benchmarks focus on simplified tasks, like training ML models with predefined datasets****, but overlook complex, research-driven problems, making them unsuited for evaluating frameworks for scientific experimentation.
% In contrast, our benchmark spans diverse, realistic tasks of varying difficulty across domains like LLM reasoning, vector indexing, cloud computing, and ML training. \zy{May want to add more arguments on why our bench is better. } 
% In contrast, our benchmark is designed to encompass a broader range of realistic tasks with different difficulty levels derived from diverse computer science domains, including LLM reasoning, vector indexing, cloud computing, and ML training. 
% This diversity ensures that our benchmark captures both the depth and breadth of challenges inherent to real-world experimentation, enabling a more comprehensive evaluation of an agent framework’s ability to automate rigorous experimentation.


 


% 1. Add AI for science papers on the applications side

% Due to its importance, a multitude of solutions that leverage the power of LLMs [to automate the scientific research] have emerged. 
% On the one hand, we have solutions that target specific aspects of scientific discovery, such as brainstorming ideas ____ via knowledge graph extraction____, hypothesis generation ____ and literature reviews ____ while neglecting experimentation. 

% See citations for llms for autonomous research here: https://arxiv.org/pdf/2501.04227

% 2. Add Agent 

% 3. Add benchmarks

% \amber{may add more citation.}

\if 0
More citations: zhenning can take a look too:

https://openreview.net/pdf?id=G7UtIGQmjm

https://aiscientist.substack.com/p/musing-20-hypothesis-generation-with

https://arxiv.org/pdf/2404.04326v1

https://arxiv.org/pdf/2407.08940

https://www.nature.com/articles/s41599-024-03407-5#Sec2

https://ai4sciencecommunity.github.io/

Accelerating science with human-aware
artificial intelligence: nature human behaviour

Nobel Turing Challenge: creating the engine for scientific
discovery: nature.com

Scientific discovery in the age of artificial
intelligence https://doi.org/10.1038/s41586-023-06221-2

https://www.nature.com/articles/d41586-023-03596-0#ref-CR3

\fi