
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=1pt] (char) {#1};}}




\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/workflow.png}
    \caption{\sys workflow with an example task in LLM reasoning.
    The Architect is responsible for designing high-level plans and reflects on the new findings. 
    The Technician is responsible for implementing and executing the experiments based on the plans.
    Whenever an agent completes its action (step \circled{1}, \circled{2}, \circled{3}, \circled{4}, \circled{5}), the Experimental Rigor Engine (steps \circled{A}$\rightharpoonup$\circled{B}$\rightharpoonup$\circled{C}) 
    % validates the action, decides on the next action and assigns actions, and maintains interpretable experimental progress, 
    validates the action, determines next steps, assigns tasks and maintains interpretable experimental progress, ensuring rigor throughout the entire process.}
    \label{fig:workflow}
\end{figure*}


 
\section{Background}
\subsection{Science Experimentation}
Scientific experimentation often starts with researchers posing testable hypotheses based on their past results, domain knowledge, and intuition. 
This experimentation process then unfolds across three key stages:
(1) \textit{Experimental Design}, where researchers plan the controlled experiment by identifying variables, selecting methodologies, and outlining procedures to enhance reproducibility and validity.
(2) \textit{Experiment Execution}, where researchers set up the complex experiment environments and iteratively explore vast search spaces,
and (3) \textit{Data Documentation and Analysis}, where researchers systematically gather data, apply analytical techniques, and extract insights to validate or refine their hypotheses.
This process is iterative, as insights gained from data analysis often lead to the refinement of hypotheses, leading to subsequent rounds of these three steps.

\subsection{Rigor in Experimentation}
\label{subsec:rigor}
% \amber{How to rephrase this subsection to be stronger?}
% \pat{Idea 1: bring in real world citations? e.g., examples where reliability was important, and the consequence of not doing that.. Right now this reads like a repeat of intro} 
% \pat{Idea 1.1: add more precise definitions of rigor to enhance the complexity: e.g., controlled experiments, control/exp groups, etc.}

Rigor is essential in scientific research, ensuring systematic, precise, and reliable findings~\cite{rigor2}. \textit{If science isn’t rigorous, it’s reckless.}~\cite{rigor1}. More precisely, experimental rigor is grounded in three core principles~\cite{rigor3}:

% It encompasses three key aspects \cite{rigor3}: (a) selecting an appropriate methodology, (b) ensuring fidelity in executing the experiment and analysis, and (c) transparently documenting the process. Among these, execution and analysis are particularly critical, as they directly impact the trustworthiness of results~\cite{rigor2}. 
% Rigor upholds methodological precision and result validity, as stringent methods provide stronger assurance of reliable findings.
% A key domain where rigor is paramount is experimentation which 
% Rigor is paramount in experimentation to maintain the credibility of results, as experiments often evolve iteratively, with preliminary findings guiding refinements and further investigations.
% Scientific experimentation aims to (1) control extraneous variables, (2) reduce variability within groups, and (3) test causal hypotheses. Poor experiment design is costly and irreparable, as no statistical method can fully fix flawed methodologies. 
% More precisely, experimental rigor is grounded in three core principles:
% Throughout the experimentation process, rigor is critical to ensuring the credibility of results in scientific experimentation. 
% Therefore, we emphasize rigor as a cornerstone of experimentation, where we identify three core principles:
% Throughout the experimentation process, rigor is critical to maintaining the credibility of results. Therefore, we emphasize rigor as a cornerstone of experimentation, grounded in three core principles.

\noindent\textbf{Methodical Procedure}: 
Experimentation must adhere to a principled and systematic methodology throughout all aforementioned stages, from hypothesis formulation to data documentation. 
Such a structured procedure ensures that no critical procedures are overlooked or performed incompletely, thereby preserving the integrity of the research.

\noindent\textbf{Reliability}:  
Every stage in the experimental pipeline—such as experiment design and environment setup—needs to be reliable and reproducible so that any final findings rest on solid ground.
For instance, it encompasses correct variable identification, controlled experimental design, and rigorous code verification.
By meticulously verifying each stage, reliability minimizes the risk of cascading errors, thereby ensuring that the results are trustworthy.

\noindent\textbf{Interpretability}: All processes and outcomes need to be clearly documented in a consistent manner. This makes it easier for researchers or agents to replicate experiments, understand results, and extend research. 
% This transparency facilitates verification, collaboration and the broader impact of scientific research. 

\if 0
Pat thoughts:

What is research?
- Sometimes studies are planned from start to finish. 
- Other times, we do prelim iterative experiments here and there, if more promising, more experiments are done. Somewhere along the line we realize more control groups are needed, and different groups may be compiled by experiments done over time. 

Experimental design has three basic purposes:
- to control effects of extraneous variables (those variables not under study).
- to reduce variability within treatment groups, making differences in treatment outcomes easier to detect.
- to provide answers to questions by testing causal hypotheses.

- An analysis can always be re-worked or re-done. The original study however, would be costly and difficult to repeat if done badly. In statistics there is no “magic bullet”, or statistical test, that can correct every poorly designed study.


Rigor = 

- Reproducibility: Reproducibility means recording and communicating those procedures such that they can be replicated accurately. Reproducibility is a minimum necessary condition for a finding to be believable and informative. If the same data and the same analyses produce the same results, the original results are considered reproducible.
    - Reproducibility crisis
- Experimental control 
- Replication 
- Interpretability
    - Transparent reporting
    - How did those groups get constructed and when they actually done
- Data blinding, unrecognized bias, blinding
- Minimizing bias: Scientific rigor involves minimizing bias in subject selection and data analysis. It is about determining the appropriate sample size for your study so that you have sufficient statistical power to be more confident about whether you are generating false positives or missing out on false negatives.

\fi

\subsection{Related Work}
\label{subsec:related-work}


% \paragraph{AI Agents.}
% Advances in LLM-driven AI agents have enabled tool-use/action capabilities, allowing them to perform tasks like online shopping \cite{webshop}, controlling computers \cite{OS-Copilot}, managing workflows \cite{agentworkflowmemory}, and interacting with APIs \cite{gorilla}. 
% These agents are increasingly versatile and even begin assisting in scientific research. However, their application to scientific workflows reveals challenges in rigor and reliability, and a specialized system for scientific experimentation is needed. 

\paragraph{AI Agents for Science.}
Prior work has leveraged AI to accelerate scientific discovery~\cite{berens2023aiscienceemergingagenda, Kitano2021}, focusing on various stages of the research lifecycle, including literature reviews~\cite{agarwal2024litllm, paper-review1}, brainstorming ideas~\cite{gu2024generation, bran2024knowledge}, hypothesis generation~\cite{sourati2023accelerating, hypothesis1, hypothesis2, hypothesis3} and data analysis~\cite{hong2024data, chen2024scienceagentbench}.
While these efforts works on various aspects of the scientific lifecycle, experimentation—a critical, rigor-intensive step—remains underexplored.

% Add science tutor/assistant works as well
% Existing AI agent solutions~\cite{schmidgall2025agent, lu2024ai}, whether generic or specialized for scientific research, typically rely on ad-hoc prompt-based interactions and predefined workflows.
% AI Scientist~\cite{schmidgall2025agent} rely on ad-hoc prompts to guide rigid processes, including hypothesis generation, code editing, experiment execution, result reporting, and review, limiting adaptability and iterative refinement in scientific inquiry.
% Frameworks~\cite{lu2024ai} lack flexibility for diverse research scenarios, requiring users to write experimental code in rigid, framework-specific formats, which adds significant overhead and hinders usability.
Existing agents for end-to-end scientific research \cite{schmidgall2025agent, lu2024ai, dolphin, SciAgents} rely on ad-hoc prompts to guide predefined workflows, from idea generation to paper writing.
Their open-sourced frameworks often require experimental code to follow constrained, framework-specific formats, adding overhead and hindering their usability.
These solutions mimic experimentation processes using multi-agent systems but lack systematic enforcement of a \textit{methodical procedure}, \textit{reliability}, and \textit{interpretability}.
Without these core principles, such agents struggle to deliver meaningful and reproducible results, limiting their practical utility in real-world scientific research.
% Additionally, existing frameworks~\cite{lu2024ai} lack the flexibility required for diverse research scenarios. 
% For instance, users are often required to write experimental code in highly specific formats dictated by the framework, introducing significant overhead and making the tools cumbersome to use. 




% On the benchmarking side, existing benchmarks focus narrowly on tasks like ML training~\cite{huang2310mlagentbench}, while our benchmark spans diverse computer science domains and various difficulty level, offering a comprehensive evaluation of an agent's ability to automate rigorous experimentation.


\paragraph{AI Agent Task Benchmarks.}

A wide range of benchmarks have been developed to assess the capabilities of AI agents across diverse domains. Existing benchmarks primarily focus on logical reasoning \cite{GSM8K, mmlu, reasoning-bench}, problem-solving \cite{math-bench, math-bench2, problem-solve1, problem-solve2, science-tutor1}, knowledge retrieval tasks \cite{retrival-bench1} and machine learning training~\cite{huang2310mlagentbench, automl1, automl2}. These benchmarks evaluate agents on well-defined tasks that typically have clear, deterministic solutions.

In contrast, our benchmark focuses on experimentation, which requires a more rigorous and systematic approach beyond problem-solving. Experimental tasks require iterative hypothesis refinement, complex experiment setup and execution, and robust result interpretation.
Our benchmark captures these challenges by evaluating AI systems on real-world experimentation tasks derived from influential research papers and widely adopted open-source projects.


% Existing benchmarks are often narrowly focused, primarily evaluating agent frameworks on machine learning training~\cite{huang2310mlagentbench}. 
% Benchmarks like MLAgentBench~\cite{huang2310mlagentbench}, are narrowly scoped, primarily evaluating agent frameworks on simplified tasks like training machine learning models given predefined datasets. These benchmarks lack coverage of complex, research-driven problems, limiting their utility for assessing frameworks designed for rigorous scientific experimentation.
% Other benchmarks focus on simplified tasks, like training ML models with predefined datasets~\cite{huang2310mlagentbench, automl1, automl2}, but overlook complex, research-driven problems, making them unsuited for evaluating frameworks for scientific experimentation.
% In contrast, our benchmark spans diverse, realistic tasks of varying difficulty across domains like LLM reasoning, vector indexing, cloud computing, and ML training. \zy{May want to add more arguments on why our bench is better. } 
% In contrast, our benchmark is designed to encompass a broader range of realistic tasks with different difficulty levels derived from diverse computer science domains, including LLM reasoning, vector indexing, cloud computing, and ML training. 
% This diversity ensures that our benchmark captures both the depth and breadth of challenges inherent to real-world experimentation, enabling a more comprehensive evaluation of an agent framework’s ability to automate rigorous experimentation.


 


% 1. Add AI for science papers on the applications side

% Due to its importance, a multitude of solutions that leverage the power of LLMs [to automate the scientific research] have emerged. 
% On the one hand, we have solutions that target specific aspects of scientific discovery, such as brainstorming ideas~\cite{gu2024generation} via knowledge graph extraction~\cite{bran2024knowledge}, hypothesis generation~\cite{sourati2023accelerating}, and literature reviews~\cite{agarwal2024litllm}, while neglecting experimentation. 

% See citations for llms for autonomous research here: https://arxiv.org/pdf/2501.04227

% 2. Add Agent 

% 3. Add benchmarks

% \amber{may add more citation.}

\if 0
More citations: zhenning can take a look too:

https://openreview.net/pdf?id=G7UtIGQmjm

https://aiscientist.substack.com/p/musing-20-hypothesis-generation-with

https://arxiv.org/pdf/2404.04326v1

https://arxiv.org/pdf/2407.08940

https://www.nature.com/articles/s41599-024-03407-5#Sec2

https://ai4sciencecommunity.github.io/

Accelerating science with human-aware
artificial intelligence: nature human behaviour

Nobel Turing Challenge: creating the engine for scientific
discovery: nature.com

Scientific discovery in the age of artificial
intelligence https://doi.org/10.1038/s41586-023-06221-2

https://www.nature.com/articles/d41586-023-03596-0#ref-CR3

\fi

