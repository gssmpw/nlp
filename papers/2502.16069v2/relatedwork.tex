\section{Related Work}
\label{subsec:related-work}


% \paragraph{AI Agents.}
% Advances in LLM-driven AI agents have enabled tool-use/action capabilities, allowing them to perform tasks like online shopping \cite{webshop}, controlling computers \cite{OS-Copilot}, managing workflows \cite{agentworkflowmemory}, and interacting with APIs \cite{gorilla}. 
% These agents are increasingly versatile and even begin assisting in scientific research. However, their application to scientific workflows reveals challenges in rigor and reliability, and a specialized system for scientific experimentation is needed. 

\paragraph{AI Agents for Science.}
Prior work has leveraged AI to accelerate scientific discovery~\cite{berens2023aiscienceemergingagenda, Kitano2021}, focusing on various stages of the research lifecycle, including literature reviews~\cite{agarwal2024litllm, paper-review1}, brainstorming ideas~\cite{gu2024generation, bran2024knowledge}, hypothesis generation~\cite{sourati2023accelerating, hypothesis1, hypothesis2, hypothesis3} and data analysis~\cite{hong2024data, chen2024scienceagentbench}.
While these efforts works on various aspects of the scientific lifecycle, experimentation—a critical, rigor-intensive step—remains underexplored.

% Add science tutor/assistant works as well
% Existing AI agent solutions~\cite{schmidgall2025agent, lu2024ai}, whether generic or specialized for scientific research, typically rely on ad-hoc prompt-based interactions and predefined workflows.
% AI Scientist~\cite{schmidgall2025agent} rely on ad-hoc prompts to guide rigid processes, including hypothesis generation, code editing, experiment execution, result reporting, and review, limiting adaptability and iterative refinement in scientific inquiry.
% Frameworks~\cite{lu2024ai} lack flexibility for diverse research scenarios, requiring users to write experimental code in rigid, framework-specific formats, which adds significant overhead and hinders usability.
Existing agents for end-to-end scientific research \cite{schmidgall2025agent, lu2024ai, dolphin, SciAgents} rely on ad-hoc prompts to guide predefined workflows, from idea generation to paper writing.
Their open-sourced frameworks often require experimental code to follow constrained, framework-specific formats, adding overhead and hindering their usability.
These solutions mimic experimentation processes using multi-agent systems but lack systematic enforcement of a \textit{methodical procedure}, \textit{reliability}, and \textit{interpretability}.
Without these core principles, such agents struggle to deliver meaningful and reproducible results, limiting their practical utility in real-world scientific research.
% Additionally, existing frameworks~\cite{lu2024ai} lack the flexibility required for diverse research scenarios. 
% For instance, users are often required to write experimental code in highly specific formats dictated by the framework, introducing significant overhead and making the tools cumbersome to use. 




% On the benchmarking side, existing benchmarks focus narrowly on tasks like ML training~\cite{huang2310mlagentbench}, while our benchmark spans diverse computer science domains and various difficulty level, offering a comprehensive evaluation of an agent's ability to automate rigorous experimentation.


\paragraph{AI Agent Task Benchmarks.}

A wide range of benchmarks have been developed to assess the capabilities of AI agents across diverse domains. Existing benchmarks primarily focus on logical reasoning \cite{GSM8K, mmlu, reasoning-bench}, problem-solving \cite{math-bench, math-bench2, problem-solve1, problem-solve2, science-tutor1}, knowledge retrieval tasks \cite{retrival-bench1} and machine learning training~\cite{huang2310mlagentbench, automl1, automl2}. These benchmarks evaluate agents on well-defined tasks that typically have clear, deterministic solutions.

In contrast, our benchmark focuses on experimentation, which requires a more rigorous and systematic approach beyond problem-solving. Experimental tasks require iterative hypothesis refinement, complex experiment setup and execution, and robust result interpretation.
Our benchmark captures these challenges by evaluating AI systems on real-world experimentation tasks derived from influential research papers and widely adopted open-source projects.


% Existing benchmarks are often narrowly focused, primarily evaluating agent frameworks on machine learning training~\cite{huang2310mlagentbench}. 
% Benchmarks like MLAgentBench~\cite{huang2310mlagentbench}, are narrowly scoped, primarily evaluating agent frameworks on simplified tasks like training machine learning models given predefined datasets. These benchmarks lack coverage of complex, research-driven problems, limiting their utility for assessing frameworks designed for rigorous scientific experimentation.
% Other benchmarks focus on simplified tasks, like training ML models with predefined datasets~\cite{huang2310mlagentbench, automl1, automl2}, but overlook complex, research-driven problems, making them unsuited for evaluating frameworks for scientific experimentation.
% In contrast, our benchmark spans diverse, realistic tasks of varying difficulty across domains like LLM reasoning, vector indexing, cloud computing, and ML training. \zy{May want to add more arguments on why our bench is better. } 
% In contrast, our benchmark is designed to encompass a broader range of realistic tasks with different difficulty levels derived from diverse computer science domains, including LLM reasoning, vector indexing, cloud computing, and ML training. 
% This diversity ensures that our benchmark captures both the depth and breadth of challenges inherent to real-world experimentation, enabling a more comprehensive evaluation of an agent framework’s ability to automate rigorous experimentation.


 


% 1. Add AI for science papers on the applications side

% Due to its importance, a multitude of solutions that leverage the power of LLMs [to automate the scientific research] have emerged. 
% On the one hand, we have solutions that target specific aspects of scientific discovery, such as brainstorming ideas~\cite{gu2024generation} via knowledge graph extraction~\cite{bran2024knowledge}, hypothesis generation~\cite{sourati2023accelerating}, and literature reviews~\cite{agarwal2024litllm}, while neglecting experimentation. 

% See citations for llms for autonomous research here: https://arxiv.org/pdf/2501.04227

% 2. Add Agent 

% 3. Add benchmarks

% \amber{may add more citation.}

\if 0
More citations: zhenning can take a look too:

https://openreview.net/pdf?id=G7UtIGQmjm

https://aiscientist.substack.com/p/musing-20-hypothesis-generation-with

https://arxiv.org/pdf/2404.04326v1

https://arxiv.org/pdf/2407.08940

https://www.nature.com/articles/s41599-024-03407-5#Sec2

https://ai4sciencecommunity.github.io/

Accelerating science with human-aware
artificial intelligence: nature human behaviour

Nobel Turing Challenge: creating the engine for scientific
discovery: nature.com

Scientific discovery in the age of artificial
intelligence https://doi.org/10.1038/s41586-023-06221-2

https://www.nature.com/articles/d41586-023-03596-0#ref-CR3

\fi