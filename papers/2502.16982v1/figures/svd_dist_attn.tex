\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/svd_dist_attn_final_v2.pdf}
    \caption{Distribution of singular values for each weight matrix in the attention layers. We use WC to denote the weight matrices at each layer that compress the hidden states to the shared latent spaces for keys and values, WV to denote the weight matrices up-projecting the values from the latent space, WO to denote the output projection matrices, and WKR, WKC, WQR and WQC to denote the projection matrices for the part of keys and queries with and without RoPE respectively. We set the spines of each line graph red if the corresponding weight matrix optimized by Muon has a lower singular entropy than AdamW.} 
    \label{fig_svd_attn} 
\end{figure}