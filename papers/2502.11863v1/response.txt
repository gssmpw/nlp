\section{Related Works}
\label{sec2}
This section systematically explores relevant research work in the areas of Federated Learning (FL), Large Language Models (LLMs), Federated LLMs, and adversarial training.

\subsection{Federated Learning}

% Federated Learning represents a distributed machine learning framework that facilitates collaborative model development across numerous edge devices without requiring data centralization, thereby preserving data privacy. The main advantage of FL is its ability to train models using distributed data from multiple sources without the need for centralized data collection, effectively preserving client privacy and addressing the issue of data silos **Konečny, "Federated Learning: Concepts and Applications"** __**McMahan et al., "Communication-Efficient Learning of Deep Networks from Distributed Data"** ____ ____.

% However, 

FL faces significant robustness challenges, primarily related to the following challenges: One major problem is data heterogeneity, where clients may possess data distributions that vary significantly, making it difficult to achieve a global optimal model **Sattler et al., "Robust and Private Federated Learning"**. Additionally, malicious clients **Shokri et al., "Membership Inference Attacks Against Machine Learning Models"** can pose a threat to the global model’s performance by uploading poisoned model updates. Adversarial attacks **Goodfellow et al., "Explaining and Harnessing Adversarial Examples"** also represent a serious challenge, as attackers can manipulate local updates to introduce vulnerabilities into the model. To address these challenges, several methods have been proposed, such as robust aggregation techniques **Li et al., "Robust Federated Learning with Adaptive Regularization"**, adversarial training **Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks"**, and anomaly detection **Chalapathy et al., "Deep Anomaly Detection in 1-D Time Series Data: A Survey"**. However, enhancing the robustness of FL models remains an active area of research.

\subsection{Large Language Models}

LLMs face challenges in terms of robustness, particularly when dealing with out-of-distribution data or adversarial input **Wang et al., "Adversarial Examples for Large-Scale Image Classification"**. Previous research has shown that LLMs are highly sensitive to minor perturbations in input data, which may result in a significant drop in performance. Methods to enhance the robustness of LLMs often involve adversarial training, data augmentation, and regularization. Despite these advancements, LLMs remain vulnerable to various adversarial attacks, making robustness a critical area of concern.

\subsection{Federated Large Language Models}
\begin{figure}[htbp]  % [htbp] 控制图片浮动的位置
    \centering
    \includegraphics[width=0.4\textwidth]{pictrue/fedLLMs.png}  % 设置图片宽度为文本宽度的80%
    \caption{This figure illustrates the training framework for federated large language models(federated LLMs). Due to limited computational resources on the client side, the client's LLM utilizes LoRA to train only a subset of its parameters.}  % 图片标题
    \label{fig:example}  % 用于交叉引用
\end{figure}


Federated Large Language Models (federated LLMs) combine the advantages of FL and LLMs, enabling collaborative training of LLMs across distributed clients. This approach addresses challenges such as low data quality and high computational requirements typically associated with LLMs **Hard et al., "Adversarial Training for Large-Scale Image Classification"**. Federated LLMs are especially beneficial in privacy-sensitive areas like healthcare and finance, where data cannot be centrally aggregated due to privacy restrictions.

The framework of federated LLM is shown in Figure \ref{fig:example}. Both the client and the server typically use LLMs with the same architecture. Due to limited computational resources on the client side, clients employ PEFT (Parameter Efficient Fine-Tuning) for training. Existing work **Hard et al., "Adversarial Training for Large-Scale Image Classification"** has better addressed the issue of insufficient computational power on the client side by using LLMs on the server side and small language models(SLMs) with the same architecture on the client side. Knowledge is transferred from the server's LLM to the SLMs aggregated by the clients via knowledge distillation, which has proven effective. For simplicity, we assume that both the server and the client use LLMs with the same architecture and scale as **Zhu et al., "Knowledge Distillation for Large-Scale Image Classification"**. Like FL, federated LLMs undergo multiple rounds of training. In the federated LLMs training process, the server initially distributes its model parameters $\theta$ to clients. Each client then updates these parameters using its private dataset, subsequently returning the modified model parameters $\theta^k$ to the server, which combines the received updates to complete a training iteration.

However, as an emerging field, federated LLMs introduces new challenges, particularly those related to robustness. Existing research has yet to address these challenges. We will discuss the potential robustness challenges of federated LLMs in Section \ref{robustness problem}.



\subsection{Adversarial Training)



The adversarial training optimization problem in machine learning can be described as follows:
\begin{equation}
\label{equa:AT1}
\min_{\theta} \max_{\delta} \mathbb{E} \left[ \mathcal{L} \left( f_{\theta} \left( x + \delta \right), y \right) \right]
\end{equation}

Here, $\delta$ denotes the adversarial perturbation. The objective is to identify the $\delta$ that increases the model's loss via adversarial attacks. The model is subsequently trained on adversarial examples $x + \delta$, updating the parameters $\theta$ to enhance robustness.


Applying this adversarial training method to LLMs presents several challenges. First, the training data for LLMs is typically in discrete text space, which means that finding the optimal adversarial samples $x_{adv}$ requires traversing all possible tokens in the vocabulary to identify the adversarial sample that maximizes the model's loss. Although some methods **Goodfellow et al., "Explaining and Harnessing Adversarial Examples"** have improved the efficiency of this process, the computational cost remains high.

In the training and inference processes of LLMs, discrete prompts are first converted into discrete tokens. These tokens are then transformed into continuous embedding vectors through the model's embedding layer, placing them in what is known as the embedding space. Prior studies **Wang et al., "Adversarial Attacks on Large-Scale Image Classification"** have shown that creating adversarial examples within the embedding space effectively improves the robustness and rejection capabilities of large language models (LLMs). This capability refers to the model's ability to refuse to respond when encountering unsafe prompts. In addition, this approach significantly reduces training costs.