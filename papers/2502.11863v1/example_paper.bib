@inproceedings{nam2024using,
  title={Using an llm to help with code understanding},
  author={Nam, Daye and Macvean, Andrew and Hellendoorn, Vincent and Vasilescu, Bogdan and Myers, Brad},
  booktitle={Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
  pages={1--13},
  year={2024}
}
@article{tang2024science,
  title={The science of detecting llm-generated text},
  author={Tang, Ruixiang and Chuang, Yu-Neng and Hu, Xia},
  journal={Communications of the ACM},
  volume={67},
  number={4},
  pages={50--59},
  year={2024},
  publisher={ACM New York, NY, USA}
}
@article{yao2024survey,
  title={A survey on large language model (llm) security and privacy: The good, the bad, and the ugly},
  author={Yao, Yifan and Duan, Jinhao and Xu, Kaidi and Cai, Yuanfang and Sun, Zhibo and Zhang, Yue},
  journal={High-Confidence Computing},
  pages={100211},
  year={2024},
  publisher={Elsevier}
}
@article{minaee2024large,
  title={Large language models: A survey},
  author={Minaee, Shervin and Mikolov, Tomas and Nikzad, Narjes and Chenaghlu, Meysam and Socher, Richard and Amatriain, Xavier and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2402.06196},
  year={2024}
}

@misc{chen2024integrationlargelanguagemodels,
      title={Integration of Large Language Models and Federated Learning}, 
      author={Chaochao Chen and Xiaohua Feng and Yuyuan Li and Lingjuan Lyu and Jun Zhou and Xiaolin Zheng and Jianwei Yin},
      year={2024},
      eprint={2307.08925},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2307.08925}, 
}

@InProceedings{10.1007/978-981-97-5569-1_17,
author="Yue, Linan
and Liu, Qi
and Du, Yichao
and Gao, Weibo
and Liu, Ye
and Yao, Fangzhou",
editor="Onizuka, Makoto
and Lee, Jae-Gil
and Tong, Yongxin
and Xiao, Chuan
and Ishikawa, Yoshiharu
and Amer-Yahia, Sihem
and Jagadish, H. V.
and Lu, Kejing",
title="FedJudge: Federated Legal Large Language Model",
booktitle="Database Systems for Advanced Applications",
year="2024",
publisher="Springer Nature Singapore",
address="Singapore",
pages="268--285",
isbn="978-981-97-5569-1"
}

@misc{sani2024futurelargelanguagemodel,
      title={The Future of Large Language Model Pre-training is Federated}, 
      author={Lorenzo Sani and Alex Iacob and Zeyu Cao and Bill Marino and Yan Gao and Tomas Paulik and Wanru Zhao and William F. Shen and Preslav Aleksandrov and Xinchi Qiu and Nicholas D. Lane},
      year={2024},
      eprint={2405.10853},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.10853}, 
}

@article{YAO2024100211,
title = {A survey on large language model (LLM) security and privacy: The Good, The Bad, and The Ugly},
journal = {High-Confidence Computing},
volume = {4},
number = {2},
pages = {100211},
year = {2024},
issn = {2667-2952},
doi = {https://doi.org/10.1016/j.hcc.2024.100211},
url = {https://www.sciencedirect.com/science/article/pii/S266729522400014X},
author = {Yifan Yao and Jinhao Duan and Kaidi Xu and Yuanfang Cai and Zhibo Sun and Yue Zhang},
keywords = {Large Language Model (LLM), LLM security, LLM privacy, ChatGPT, LLM attacks, LLM vulnerabilities},

}

@misc{minaee2024largelanguagemodelssurvey,
      title={Large Language Models: A Survey}, 
      author={Shervin Minaee and Tomas Mikolov and Narjes Nikzad and Meysam Chenaghlu and Richard Socher and Xavier Amatriain and Jianfeng Gao},
      year={2024},
      eprint={2402.06196},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.06196}, 
}
@misc{xhonneux2024efficientadversarialtrainingllms,
      title={Efficient Adversarial Training in LLMs with Continuous Attacks}, 
      author={Sophie Xhonneux and Alessandro Sordoni and Stephan GÃ¼nnemann and Gauthier Gidel and Leo Schwinn},
      year={2024},
      eprint={2405.15589},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.15589}, 
}
@misc{mazeika2024harmbenchstandardizedevaluationframework,
      title={HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal}, 
      author={Mantas Mazeika and Long Phan and Xuwang Yin and Andy Zou and Zifan Wang and Norman Mu and Elham Sakhaee and Nathaniel Li and Steven Basart and Bo Li and David Forsyth and Dan Hendrycks},
      year={2024},
      eprint={2402.04249},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.04249}, 
}

@article{lyu2022privacy,
  title={Privacy and robustness in federated learning: Attacks and defenses},
  author={Lyu, Lingjuan and Yu, Han and Ma, Xingjun and Chen, Chen and Sun, Lichao and Zhao, Jun and Yang, Qiang and Philip, S Yu},
  journal={IEEE transactions on neural networks and learning systems},
  year={2022},
  publisher={IEEE}
}


@ARTICLE{10571602,
  author={Huang, Wenke and Ye, Mang and Shi, Zekun and Wan, Guancheng and Li, He and Du, Bo and Yang, Qiang},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Federated Learning for Generalization, Robustness, Fairness: A Survey and Benchmark}, 
  year={2024},
  volume={46},
  number={12},
  pages={9387-9406},
  keywords={Federated learning;Surveys;Robustness;Data models;Benchmark testing;Collaboration;Distributed databases;Fairness;federated learning;generalization;robustness},
  doi={10.1109/TPAMI.2024.3418862}}

@article{liu2023trustworthy,
  title={Trustworthy LLMs: A survey and guideline for evaluating large language models' alignment},
  author={Liu, Yang and Yao, Yuanshun and Ton, Jean-Francois and Zhang, Xiaoying and Cheng, Ruocheng Guo Hao and Klochkov, Yegor and Taufiq, Muhammad Faaiz and Li, Hang},
  journal={arXiv preprint arXiv:2308.05374},
  year={2023}
}

@article{bai2021recent,
  title={Recent advances in adversarial training for adversarial robustness},
  author={Bai, Tao and Luo, Jinqi and Zhao, Jun and Wen, Bihan and Wang, Qian},
  journal={arXiv preprint arXiv:2102.01356},
  year={2021}
}

@article{zizzo2020fat,
  title={Fat: Federated adversarial training},
  author={Zizzo, Giulio and Rawat, Ambrish and Sinn, Mathieu and Buesser, Beat},
  journal={arXiv preprint arXiv:2012.01791},
  year={2020}
}

@inproceedings{zhang2023delving,
  title={Delving into the adversarial robustness of federated learning},
  author={Zhang, Jie and Li, Bo and Chen, Chen and Lyu, Lingjuan and Wu, Shuang and Ding, Shouhong and Wu, Chao},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={37},
  number={9},
  pages={11245--11253},
  year={2023}
}

@article{zhao2022adversarial,
  title={Adversarial training methods for deep learning: A systematic review},
  author={Zhao, Weimin and Alwidian, Sanaa and Mahmoud, Qusay H},
  journal={Algorithms},
  volume={15},
  number={8},
  pages={283},
  year={2022},
  publisher={MDPI}
}

@article{li2020review,
  title={A review of applications in federated learning},
  author={Li, Li and Fan, Yuxi and Tse, Mike and Lin, Kuo-Yi},
  journal={Computers \& Industrial Engineering},
  volume={149},
  pages={106854},
  year={2020},
  publisher={Elsevier}
}
@article{zhang2021survey,
  title={A survey on federated learning},
  author={Zhang, Chen and Xie, Yu and Bai, Hang and Yu, Bin and Li, Weihong and Gao, Yuan},
  journal={Knowledge-Based Systems},
  volume={216},
  pages={106775},
  year={2021},
  publisher={Elsevier}
}

@article{kairouz2021advances,
  title={Advances and open problems in federated learning},
  author={Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
  journal={Foundations and trends{\textregistered} in machine learning},
  volume={14},
  number={1--2},
  pages={1--210},
  year={2021},
  publisher={Now Publishers, Inc.}
}

@article{li2020federated,
  title={Federated learning: Challenges, methods, and future directions},
  author={Li, Tian and Sahu, Anit Kumar and Talwalkar, Ameet and Smith, Virginia},
  journal={IEEE signal processing magazine},
  volume={37},
  number={3},
  pages={50--60},
  year={2020},
  publisher={IEEE}
}

@article{li2020learning,
  title={Learning to detect malicious clients for robust federated learning},
  author={Li, Suyi and Cheng, Yong and Wang, Wei and Liu, Yang and Chen, Tianjian},
  journal={arXiv preprint arXiv:2002.00211},
  year={2020}
}


@InProceedings{pmlr-v97-bhagoji19a,
  title = 	 {Analyzing Federated Learning through an Adversarial Lens},
  author =       {Bhagoji, Arjun Nitin and Chakraborty, Supriyo and Mittal, Prateek and Calo, Seraphin},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {634--643},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/bhagoji19a/bhagoji19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/bhagoji19a.html},

}

@article{pillutla2022robust,
  title={Robust aggregation for federated learning},
  author={Pillutla, Krishna and Kakade, Sham M and Harchaoui, Zaid},
  journal={IEEE Transactions on Signal Processing},
  volume={70},
  pages={1142--1154},
  year={2022},
  publisher={IEEE}
}

@article{shah2021adversarial,
  title={Adversarial training in communication constrained federated learning},
  author={Shah, Devansh and Dube, Parijat and Chakraborty, Supriyo and Verma, Ashish},
  journal={arXiv preprint arXiv:2103.01319},
  year={2021}
}

@article{mothukuri2021federated,
  title={Federated-learning-based anomaly detection for IoT security attacks},
  author={Mothukuri, Viraaji and Khare, Prachi and Parizi, Reza M and Pouriyeh, Seyedamin and Dehghantanha, Ali and Srivastava, Gautam},
  journal={IEEE Internet of Things Journal},
  volume={9},
  number={4},
  pages={2545--2554},
  year={2021},
  publisher={IEEE}
}

@article{fan2024fedcollm,
  title={FedCoLLM: A Parameter-Efficient Federated Co-tuning Framework for Large and Small Language Models},
  author={Fan, Tao and Kang, Yan and Ma, Guoqiang and Fan, Lixin and Chen, Kai and Yang, Qiang},
  journal={arXiv preprint arXiv:2411.11707},
  year={2024}
}

@article{fan2024fedmkt,
  title={Fedmkt: Federated mutual knowledge transfer for large and small language models},
  author={Fan, Tao and Ma, Guoqiang and Kang, Yan and Gu, Hanlin and Song, Yuanfeng and Fan, Lixin and Chen, Kai and Yang, Qiang},
  journal={arXiv preprint arXiv:2406.02224},
  year={2024}
}

@article{huang2024trustllm,
  title={Trustllm: Trustworthiness in large language models},
  author={Huang, Yue and Sun, Lichao and Wang, Haoran and Wu, Siyuan and Zhang, Qihui and Li, Yuan and Gao, Chujie and Huang, Yixin and Lyu, Wenhan and Zhang, Yixuan and others},
  journal={arXiv preprint arXiv:2401.05561},
  year={2024}
}
@article{team2024gemma,
  title={Gemma: Open models based on gemini research and technology},
  author={Team, Gemma and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024}
}
@article{abdin2024phi,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and Awadallah, Ahmed and Awan, Ammar Ahmad and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}
@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{tunstall2023zephyr,
  title={Zephyr: Direct distillation of lm alignment},
  author={Tunstall, Lewis and Beeching, Edward and Lambert, Nathan and Rajani, Nazneen and Rasul, Kashif and Belkada, Younes and Huang, Shengyi and von Werra, Leandro and Fourrier, Cl{\'e}mentine and Habib, Nathan and others},
  journal={arXiv preprint arXiv:2310.16944},
  year={2023}
}

@misc{ghosh2019robustfederatedlearningheterogeneous,
      title={Robust Federated Learning in a Heterogeneous Environment}, 
      author={Avishek Ghosh and Justin Hong and Dong Yin and Kannan Ramchandran},
      year={2019},
      eprint={1906.06629},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1906.06629}, 
}