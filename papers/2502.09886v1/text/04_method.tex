\section{Method}
\label{sec:method}
The proposed framework, \textbf{Video2Policy}, steps further for task proposal and policy learning through internet videos, to provide diverse and realistic tasks as well as the corresponding learned policies. It consists of two phases: task scene generation and policy learning. In Sec. \ref{sec:method:scene_reconstruction}, we introduce the pipeline for reconstructing scenes from RGB videos. Subsequently, in Sec. \ref{sec:method:policy_learning}, we demonstrate how to generate the code for the task and learn policies to solve it. Finally, we provide an example of training a generalist policy for real-world deployment within our framework in Sec. \ref{sec:method:general}.

\begin{figure*}
    \centering
    \vskip -0.2cm
    \includegraphics[width=0.9\linewidth]{pdfs/vis.pdf}
    \vskip -0.2cm
    \caption{Some visualization of the tasks generated from SSv2 Video Dataset.}
    \label{fig:exp_vis}
    \vskip -0.5cm
\end{figure*}

\subsection{Scene Reconstruction from Videos}
\label{sec:method:scene_reconstruction}

Since the goal of this work is to learn policies from videos rather than automatically retarget trajectories, our scene reconstruction phase focuses on reconstructing the manipulated objects along with their relative relationships. To master the skill demonstrated in the video, we allow for random positions and orientations of each object in the initial states. 
The steps are shown in Fig. \ref{fig:intro}:
(1) detect and segment the manipulated objects in the video using text captions; (2) reconstruct object meshes from videos and estimate actual sizes of the meshes; (3) perform 6D position tracking for each object in the video. Afterward, we obtain a JSON file that includes the video and object information.

\textbf{Object Grounding} We first use Grounding DINO \citep{liu2023grounding} to detect the manipulated objects in the first frame of the video. Since the SSv2 dataset \citep{goyal2017something} provides both video captions and object labels, we use these as text prompts for detection. For self-collected videos of more challenging behaviors, we provide the video captions and object names manually. Afterward, we perform video segmentation using the SAM-2 model \citep{ravi2024sam}. Specifically, we segment the objects in the first frame using bounding boxes obtained during detection and select five positive pixel points in each object mask for video segmentation. This process yields segmented videos that contain the masks of the manipulated objects.

\textbf{Object Reconstruction} With the segmentation masks of each object for each frame, we perform mesh reconstruction based on these images. Since most internet videos are recorded from a single viewpoint, we leverage the InstantMesh model \citep{xu2024instantmesh} to reconstruct the 3D meshes, which supports mesh generation from a single image. Typically, we choose the first frame to reconstruct the meshes; however, for those with objects that are significantly occluded in the first frame, we utilize the last frame instead.
To establish a more realistic size relationship between objects, we propose a simple and efficient size estimation method. We predict the camera intrinsic matrix $\mathbb{K}$ and the depth $d_{i,j}$ of the image $I_{i,j}$ using UniDepth \citep{piccinelli2024unidepth}, where $i,j$ are the pixel coordinates. Given the masked region $M$ of the object, we can calculate the maximum distance for the masked region in reality $D_{\text{image}}$:
$
    D_{\text{image}} = \max_{(i_1,j_1), (i_2, j_2) \in M} \|\mathbf{p}(i_1, j_1) - \mathbf{p}(i_2, j_2)\| \text{, where } \mathbf{p}(i, j) = \mathbb{K}^{-1} \cdot [x, y, 1]^\text{T}
    \cdot d_{i,j}
$.
Here $\mathbf{p}$ is the 3D position of each masked pixel in the camera coordinate system. We then calculate the maximum distance of vertices in the mesh object, denoted as $D_{\text{mesh}}$. The scale ratio $\rho$ for the mesh object is defined as $\rho = D_{\text{image}} / D_{\text{mesh}}$. 
The absolute sizes may exhibit some noise due to errors in depth estimation, intrinsic estimation, and object occlusion. However, the relative size of each object is mostly accurate, as $D_{\text{image}}$ and $D_{\text{image}}$ are calculated within the same camera coordinate system. 

\textbf{6D Position Tracking of Objects} After reconstructing the objects, we predict the 6D position of each object throughout the video, which will be fed into GPT-4o for code generation. We utilize FoundationPose \citep{wen2024foundationpose} in model-based setups to estimate the position and orientation of the objects. This model takes the object mesh, predicted camera intrinsics, and the depth information from each frame as inputs. Finally, we automatically generate a URDF file for each object based on the mesh file and the calculated scaled size.
Ablations for the tracking information are in App. \ref{app:more_ablation}.

\subsection{Task Code Generation and Reinforcement Learning}
\label{sec:method:policy_learning}
After extracting the visual information from the video into a task JSON file, we can build the task scene in simulation and learn policies based on GPT-4o. This process occurs in two stages. First, we generate the complete task code, which can be executed directly in the Isaac Gym \citep{makoviychuk2021isaac} environment. Second, inspired by recent work on LLM-driven reward function generation, we iteratively fine-tune the generated reward function using in-context reward reflection \citep{shinn2023reflexion, ma2023eureka, wang2023voyager}. In contrast to the previous work Eureka \citep{ma2023eureka}, which is the most similar to ours, we generate the task codes, including the reward functions, from scratch, rather than relying on pre-existing task codes and starting reward reflection from manually defined success functions. 

\textbf{Task Code Generation} Inspired by previous work \citep{ma2023eureka, wang2023gensim, wang2023robogen}, we systematically introduce the pipeline for general task code generation, which helps to infer codes by prompting in a well-organized way. Notably, the task code consists of six parts: scene information, reset function, success function, observation function, observation space function, and reward function. (1) The scene information refers to the task scene JSON file created from the videos. It contains the task title, video file path, video description, and object information, including sizes, URDF paths, and tracked 6D position lists. (2) The reset function is responsible for positioning the objects according to specific spatial relationships in the beginning. 
(3) The success function determines the success state. Notably, both the reset and success functions are generated by GPT-4o based on the task description, the provided 6D position list, and Chain-of-Thoughts examples \citep{wei2022chain}. 
(4) Furthermore, we have access to the states of the objects in simulation. Thus, we query GPT-4o to determine whether additional observations are necessary. Interestingly, we find that it can include observations such as the distance between the object and the gripper or the normalized velocity toward the target object. 
(5) Simultaneously, it calculates the correct observation shape for building the neural networks. 
(6) Regarding the reward function, we follow the instructions from \cite{ma2023eureka} with CoT examples. We write a template for task code generation, allowing us to query the VLM just once to generate the executable code encompassing all six parts. 
We generate eight example codes and select one by re-checking the correctness, reasonability and efficiency from GPT-4o, which is the base code candidate for the subsequent in-context reward reflection stage. 
The generated examples are demonstrated in App. \ref{app:code_generation} and the robustness evaluation results are in App. \ref{app:more_exp}.

\textbf{Reinforcement Learning and Reward Function Iteration} 
Given the generated task code, we train policies through reinforcement learning under the reward function $\hat{\mathcal{R}}$ and success function $\mathcal{R}_{0|1}$. Notably, we assign a high trade-off to the success rewards, formulating the training reward function as $\hat{\mathcal{R}} + \lambda \mathcal{R}_{0|1}, \lambda = 100$. Moreover, following the approach in Eureka \citep{ma2023eureka}, we apply the in-context reward reflection to design the reward functions iteratively using GPT-4o. Each time, we sample $N=8$ different reward functions for training policies and collect the training and evaluation logs. We then select the best function from the previous iteration and generate new reward functions based on these logs, along with specific instructions and CoT examples. For example, in addition to providing good examples from previous tasks, we prioritize training outputs where the accumulated rewards for successful trajectories exceed those for failed ones. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=.95\linewidth]{pdfs/sim2real.pdf}
    \caption{Examples of the Segmentation Mask Observation between the simulation and the real, which can better bridge the sim2real gap.}
    \label{fig:sim2real}
    \vskip -.2cm
\end{figure*}

\subsection{Sim2Real Policy Learning}
\label{sec:method:general}

As witnessed by the recent success of policy learning from large-scale datasets with certain formats \citep{padalkar2023open, reed2022generalist, team2024octo, brohan2023can}, we want to investigate how to learn a general policy from the internet videos, which directly outputs the executable actions of the robot rather than the un-executable future videos \citep{du2024learning, qin2023unicontrol} or language tokens \citep{liang2023code, brohan2023can}. We consider our Video2Policy a data engine to generate successful policies from internet videos. Then we can acquire expert trajectories in simulation, which match the video behavior. Notably, those expert trajectories can be any format we want, such as states, 2D images, or 3D images. In this work, we choose RGB image observation. We train RL policies from the videos and collect the successful trajectories from the learned policies. Afterward, we use imitation learning to learn the general policy from the collected dataset by behavior cloning. Finally, we transfer the learned policy to the real world. To bridge the gap between simulation and real, we apply some domain randomization in data collection and take the segmentation map as the input for deployment.
