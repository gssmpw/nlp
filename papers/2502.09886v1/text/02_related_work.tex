\section{Related Work}

\textbf{Real2Sim Scene Generation for Robotics} 
Generating realistic and diverse scenes for robotics has recently emerged as a significant challenge aimed at addressing data issues through simulation. Some researchers have developed Real2Sim pipelines for image-to-scene or video-to-scene generation. Certain studies \citep{hsu2023ditto, torne2024reconciling} focus on constructing digital twins that facilitate the transition from the real world to simulation; however, these approaches often depend on specific real-world scans or extensive human assistance. \cite{daiacdc} introduces the concept of digital cousins, while \cite{chen2024urdformer} employs inverse graphics to enhance data diversity. Nevertheless, their approach to diversity primarily involves replacing various assets.
The lack of task-level diversity hinders the ability to capture the distribution of real-world tasks, and the constraints of specific data formats complicate the scalability of robotics data. Although Real2Code \citep{mandi2024real2code} aims to build simulation scenes from images, it focuses on articulation parts and requires in-domain code data.


\textbf{Scaling up Simulation Tasks}
Previously, researchers aimed to build simulation benchmarks to facilitate scalable skill learning and standardized workflows \citep{li2023behavior, gu2023maniskill2, srivastava2022behavior, nasiriany2024robocasa}. Most of these benchmarks were constructed manually, making them difficult to scale. Recently, some researchers have focused on text-to-scene generation, emphasizing the creation of diverse scenes. Works such as \cite{deitke2022, makatura2023can, liangenvironment, chen2023genaug} utilize procedural asset generation or domain randomization, while \cite{jun2023shap, yu2023scaling, poole2022dreamfusion} engage in text-to-3D asset generation. Although these approaches can achieve asset-level or scene-level diversity in robotic tasks, they fall short in delivering task-level diversity.
Gensim \citep{wang2023gensim} attempts to generate rich simulation environments and expert demonstrations using large language models (LLMs) to achieve task-level diversity. However, text-based task generation tends to be arbitrary regarding object selection and their relationships, limiting its ability to represent the true distribution of tasks in the real world. In contrast, our work leverages real-world RGB videos to create corresponding simulation tasks that better reflect the real-world distributions of tasks and objects, facilitating easier scalability due to the abundance and accessibility of internet video data.

\textbf{Policy Learning via LLMs}
To enable automatic policy learning with high quality, researchers are increasingly turning to large language models (LLMs) for assistance. Some studies \citep{liang2023code, huang2023instruct2act, lin2023text2motion, wang2023voyager} propose generating structured code outputs for decision-making problems, most of which rely on predefined primitives. Other works \citep{yu2023language, ma2023eureka, wang2023robogen} generate reward functions using LLMs for reinforcement learning. Nevertheless, Eureka \citep{ma2023eureka} requires predefined success functions for iterative updates of reward functions, while RoboGen \citep{wang2023robogen} selects the highest reward as the initial state for the next sub-task, which introduces noise due to the variability in the generated reward functions. In contrast, our work generates success functions by leveraging visual prior knowledge from the provided videos and updates the reward functions iteratively using a Chain-of-Thought (CoT) approach \citep{wei2022chain}.
