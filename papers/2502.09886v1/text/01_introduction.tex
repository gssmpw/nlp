\section{Introduction}

\begin{figure*}
    \centering
    \vskip -0.2cm
    \includegraphics[width=0.9\linewidth]{pdfs/intro.pdf}
    \vskip -0.2cm
    \caption{The \textbf{Video2Policy} framework can leverage internet videos to generate simulation tasks and learn policies for them automatically, which can be considered a data engine for generalist policies.}
    \label{fig:intro}
    \vskip -0.4cm
\end{figure*}

Training generalist policies requires collecting large quantities of diverse robotic expert data. However, collecting data through teleoperation is constrained by high operation costs, while collecting data from autonomous policies can be unsafe, or result in low-quality data. Simulation offers an appealing alternative to real-world data that does not suffer from these challenges, and can be used to train general and robust policies \citep{hwangbo2019learning,andrychowicz2020learning}. Recent work has explored automatically generating diverse and relevant tasks in simulation as a way to create a scalable pipeline for generating robotics data \citep{deitke2022, wang2023gensim, wang2023robogen, makatura2023can}. 

However, existing methods primarily rely on text-only task specification using Large Language Models (LLMs), which do not have grounded robotics knowledge. They often produce tasks that are not diverse, have uninteresting behavior, or use uninteresting object assets and are thus less useful for training generalist policies. To better capture the real-world distributions of task behaviors and objects, we propose leveraging RGB videos from the internet to create corresponding tasks. Unlike Real2Sim approaches that construct digital twins \citep{hsu2023ditto, torne2024reconciling} for a single scene, we want to train a generalist policy for multiple scenes and therefore we do not require perfect reconstructions. Instead, we leverage large amounts of internet videos to capture task-relevant information such as object assets and scene layouts. 
We then generate simulated tasks using a Vision-Language Model (VLM) that can take the video, video captions, object meshes, sizes, and 6D poses, and produce corresponding task codes, which can be executed to generate scenes.

Beyond task proposals, we require an efficient and automatic way to solve tasks. Naively applying reinforcement or imitation learning is challenging as it requires manual human effort for each task to create demonstrations or reward functions. Inspired by the recent advancements of LLMs in code generation for various tasks \citep{achiam2023gpt, roziere2023code}, some researchers have proposed automating policy learning or deployment through using an LLM 
to produce policy code directly \citep{huang2023voxposer, liang2023code, wang2023gensim}, or to produce reward function code \citep{ma2023eureka, wang2023robogen}. 
Gensim \citep{wang2023gensim} leverages this idea for unsupervised task generation. 
However, predicting goals restricts it to simple tasks as it does not account for dynamic tasks or tasks that involve complex object interactions. 

In contrast, reinforcement learning (RL) is effective at solving complex tasks \citep{schulman2017proximal, ye2021mastering, hafner2023mastering, wang2024efficientzero, springenberg2024offline}. 
RoboGen \citep{wang2023robogen} leverages LLM-generated reward functions for RL. However, it is hard to scale as it requires manual success functions. However, since we leverage both text prompts and explicit visual prior knowledge from human videos, we can leverage that information for better success functions.

We propose \textbf{Video2Policy}, a framework to leverage internet RGB videos in an automated pipeline to create simulated data to train generalist police, as shown in Fig. \ref{fig:intro}. It produces code for task generation in simulation using VLMs and learns policies for those tasks via RL. This autonomous approach allows us to easily scale up data generation by internet videos to produce visually grounded tasks. Our framework leverages both behavioral and object diversity from the videos, enabling generalization both at the object level as well as task level. 
Specifically, our framework consists of two phases: (1) We reconstruct the object meshes involved in the tasks from videos as task assets, and extract the 6D pose of each object; (2) We leverage the VLM to write the task code based on visual information and prompts, and learn RL policies by the iterative generated reward functions. 
Ultimately, we obtain the learned policy model that demonstrates behavior similar to the input video. And we can transfer the learned policy back to the real world.

For experiments, we focus on table-top manipulation tasks with a single robot arm in IsaacGym simulator\citep{makoviychuk2021isaac}. We conduct experiments on the Something-Something V2 (SSv2) video dataset \citep{goyal2017something}, consist of human daily behaviors with diverse scenes and language instructions. We reconstruct over 100 videos on 9 distinct tasks from the dataset. To evaluate more complex behaviors, we also utilize three self-recorded videos. Significantly, the results indicate the learned policies significantly outperform the baselines from previous LLM-driven methods. We achieve 88\% success rates on average. To demonstrate the efficacy of our framework, we (1) train a general policy by imitation learning from the simulation data collected from the learned policies for generalization capabilities, and (2) apply the learned general policy to the real robot in a sim2real manner.
We find that it can achieve 75\% success rates in simulation from the 10 unseen videos with the same behavior while achieving 47\% success rates on real robot after deployment.
