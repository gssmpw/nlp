\section{Background}
\label{sec:background}

To present our work systematically, we formulate it as a two-level hierarchical solution of Markov Decision Processes (MDPs), corresponding to the two-phase pipeline. In contrast to some hierarchical methods targeting action levels \citep{mcgovern1998hierarchical, hauskrecht2013hierarchical}, we focus on the level of reward functions.

\textbf{Low-level MDP of Controlling Problem}
Specifically, the scene reconstruction phase is to construct the MDP $\mathcal{G} = \langle \mathcal{S}, \mathcal{A}, T, \mathcal{R}_{0|1} \rangle$ from videos, where $\mathcal{S} \in \mathbb{R}^m$ represents the states of the environment, $\mathcal{A}$ is the action space of the agent, and $T$ is the transition probability function. 
$\mathcal{R}_{0|1}$ is the 0-1 reward function that distinguish whether the trajectory is successful. $\mathcal{R}_{0|1}$ can be a scalar evaluation function. 
To solve this MDP, we will train a policy $\pi$ through reinforcement learning in the Isaac Gym simulation. To achieve high performance, we leverage the LLM to sample various reward functions to learn policies in a high-level manner, based on the evaluation results from $\mathcal{R}_{0|1}$ and some CoT \citep{wei2022chain} instructions.

\textbf{High-level MDP of Reward Designs} The aim of reward design is to create a shaped reward function that simplifies the optimization of a challenging given reward function, such as the sparse 0-1 reward function $\mathcal{R}_{0|1}$. Following the definition of Reward Design Problem (RDP) from previous works \citep{singh2009rewards, ma2023eureka}, we consider a high-level MDP $\hat{\mathcal{G}} = \langle \hat{\mathcal{S}}, \hat{\mathcal{A}}, \hat{T}, F \rangle$. Here, $\hat{\mathcal{A}}$ is the space of reward functions. Each time we choose an action $\hat{R} \in \mathcal{A}$ in the MDP $\hat{\mathcal{G}}$, we will train a policy $\pi$ by RL for the low-level MDP $\mathcal{G} = \langle \mathcal{S}, \mathcal{A}, T, \hat{\mathcal{R}} + \mathcal{R}_{0|1} \rangle$. The horizon of the MDP $\hat{\mathcal{G}}$ is the iteration number in the second phase.
$\hat{\mathcal{S}}$ includes the training and evaluation information during RL and the policy model $\pi$. $\hat{T}$ is the state transition function, and $F$ is the reward function that produces a scalar evaluation of any policy $\pi$. Specifically, $F$ is equal to $\mathcal{R}_{0|1}$. Thus, the high-level MDP's goal is to find a reward function $\hat{\mathcal{R}} \in \hat{A}$ to maximize the success rates of the low-level policies.
