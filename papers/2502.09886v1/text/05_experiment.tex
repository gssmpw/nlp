\section{Experiments}
\label{sec:experiment}

In this section, we present detailed evaluations of the proposed Video2Policy framework on internet video datasets about manipulations. Specifically, the experiments are designed to answer the following questions: 
(\textbf{a}) How does the generated scene from the video look like, including the objects and visual informations, in Sec. \ref{sec:experiment:policy_learning}. 
\textbf{b)} How does the policy trained under our framework perform compared to the videos and the baselines, in Sec. \ref{sec:experiment:policy_learning}. \textbf{(c)} What is the performance of the general policy learned from diverse internet videos, and can it generalize to novel scenes, in Sec. \ref{sec:experiment:general_policy}. \textbf{(d)} How does the general policy perform by sim2real back, in Sec. \ref{sec:experiment:sim2real}.
\textbf{(e)} What affects the proposed Video2Policy framework most for policy learning, in Sec. \ref{sec:experiment:ablation_study}. 

\textbf{Experimental Setup} We use the Issac Gym \citep{makoviychuk2021isaac} as the simulation engine for all the experiments, which is commonly used in robotics tasks due to the advantages of efficient computing and highly realistic physics. We focus on table manipulation tasks, and the objects will randomly reset on the table in the beginning. The horizon of the tasks is set to 300 and the parallel environments are 8192, equally. For each task, we average success rates over 10 evaluation episodes across 3 runs with different seeds. 

\textbf{Video Data Source} To reconstruct scenes from internet RGB videos, we choose the Something Something V2 (SSv2) dataset \citep{goyal2017something}, a common and diverse video dataset for the robotics community. It includes diverse behaviors concerning manipulating something with something by human hands. To further investigate the ability of our framework on more complex objects or behaviors, we record three in-the-wild videos of different behaviors by ourselves. 
Notably, all the videos we use in the experiment are 3 channels with RGB only, with the highest accessibility. For the video quality, the small motion of the camera is tolerated, and we scale the resolution to 1024.

\textbf{Scene Generation} As mentioned in Sec. \ref{sec:method:scene_reconstruction}, we do 6D position tracking for all the objects. Considering that we randomize the initial states of all the objects, we only feed the 6D pos from the first frame and the final frame into the prompts. It is reasonable in most tasks because those two frames are significant to infer the relationship among objects. Even if this simplification will miss the motion information for some behavior, e.g. throwing, we also provide the task description to design the reward function so that the LLM will generate the velocity reward components. Moreover, we explicitly calculate the difference between the 6D pos and feed the information into the LLM to think about the success function. For most of the SSv2 videos of a single object, there are severe occlusions, making it difficult to reconstruct the mesh asset. We manually choose the first frame or the last frame to reconstruct the mesh and predict the 6D position in the same pipeline. We generate the task code in a curriculum manner \citep{ma2023eureka, wang2023gensim} after obtaining the visual information. From the beginning, we provide the example code of \texttt{reach a block} and \texttt{grasp a block}. Then we will add the successfully generated task examples into the task pool for the next one. Finally, it can even learn to use the direction velocity reward for dynamic tasks and resolve them. Some demos for the generated tasks are in Fig. \ref{fig:exp_vis}. 

\begin{table*}[t]
    \centering
    \vskip -0.3cm
    \caption{\label{tab:main_results_sth} \textbf{Results of Learned Policies for Videos(3 seeds)}. The mean $\pm$ std of the success rates are shown in the table. Our method outperforms the other baselines to a degree and achieves smaller variance in general.} 
    \begin{tabular}{l|cccc}
        \toprule
        \textbf{Task} (Succ.) & \textbf{Code-as-Policy} & \textbf{RoboGen} & \textbf{Eureka} & \textbf{Video2Policy} \\
        \midrule
        \textit{\underline{single object} in SSv2 dataset } & \\
        \textbf{Push sth. left} & 0.17 $\pm$ 0.13 & 0.93 $\pm$ 0.05 & \textbf{1.00} $\pm$ 0.00 & \textbf{1.00} $\pm$ 0.00 \\
        \textbf{Push sth. right} & 0.75 $\pm$ 0.12 & \textbf{1.00} $\pm$ 0.00 & \textbf{1.00} $\pm$ 0.00 & \textbf{1.00} $\pm$ 0.00 \\
        \textbf{Lift up sth.} & 0.33 $\pm$ 0.21 & 0.28 $\pm$ 0.09 & 0.83 $\pm$ 0.13 & \textbf{0.93} $\pm$ 0.05 \\
        \textbf{Tip sth. over} & \textbf{1.00} $\pm$ 0.00 & 0.97 $\pm$ 0.05 & 0.67 $\pm$ 0.47 & \textbf{1.00} $\pm$ 0.00 \\
        \midrule
        \textit{\underline{multiple objects} in SSv2 dataset} \\
        \textbf{Cover sth. with sth.} & 0.00 $\pm$ 0.00  & 0.00 $\pm$ 0.00  & 0.00 $\pm$ 0.00  & \textbf{0.07} $\pm$ 0.05 \\
        \textbf{Uncover sth. from sth.} & 0.10 $\pm$ 0.08 & 0.67 $\pm$ 0.26 & 0.63 $\pm$ 0.38 & \textbf{0.97} $\pm$ 0.05 \\
        \textbf{Push sth. with sth.} & 0.03 $\pm$ 0.05 & 0.03 $\pm$ 0.05 & \textbf{0.47} $\pm$ 0.38 & 0.43 $\pm$ 0.40 \\
        \textbf{Push sth. next to sth.} & 0.80 $\pm$ 0.16 & 0.23 $\pm$ 0.05 & 0.83 $\pm$ 0.12 & \textbf{1.00} $\pm$ 0.00 \\
        \textbf{Drop sth. in front of sth.} & 0.53 $\pm$ 0.31 & 0.37 $\pm$ 0.09 & 0.87 $\pm$ 0.17 & \textbf{0.93} $\pm$ 0.05 \\
        \midrule 
        \textit{\underline{hard motions} in from self-collected videos} \\
        \textbf{Insert Fork into Container} & 0.07 $\pm$ 0.05 & 0.27 $\pm$ 0.38 & 0.57 $\pm$ 0.40 & \textbf{0.93} $\pm$ 0.05 \\
        \textbf{Sliding Remoter to Mouse} & 0.00 $\pm$ 0.00 & 0.53 $\pm$ 0.12 & 0.87 $\pm$ 0.05 & \textbf{0.97} $\pm$ 0.05 \\
        \textbf{Throw Garlic into Bowl.} & 0.00 $\pm$ 0.00 & 0.03 $\pm$ 0.05 & 0.37 $\pm$ 0.29 & \textbf{0.70} $\pm$ 0.36 \\
        \midrule
        \textbf{Average} & 0.34 & 0.45 & 0.71 & \textbf{0.88} \\
        \bottomrule
    \end{tabular}
    \vskip -0.3cm
\end{table*}

\textbf{Reinforcement Learning} For the policy learning, we choose the PPO \citep{schulman2017proximal} algorithm in a well-tuned implementation codebase \citep{rl-games2021, ma2023eureka}. We share the same parameters recommended in the codebase across all tasks and all baseliens. As for the evaluation metric, we write the ground-truth success function for each generated task. Unlike Eureka \citep{ma2023eureka}, we do not allow access to the evaluation metric during training, and we manually evaluate the results for the final models. For SSv2 dataset, we make 5 iterations and sample 8 reward functions at each iteration during reinforcement learning. In our collected videos, we make 8 iterations and sample 8 reward functions. 

\subsection{Policy Learning from Videos}
\label{sec:experiment:policy_learning}

\textbf{Policy Learning Baselines} Since there is few works transforming the internet RGB videos into policies for the task, we focus on the policy learning part in the experiments. We benchmark our method against the following baselines.
(1) Code-as-Policy (CoP) \citep{liang2023code}, which queries the LLM with all the states in the environment to write the executable code for the robot. To ensure better performance of CoP, we use the close-loop control and regenerate code policies every 50 steps. (2) RoboGen, which does not require a success function and learns without reward reflection iteration. (3) Eureka, which generates code for both the reward and the success function using an LLM and does not use video information. 
To make fair comparisons, we use the same object meshes and task codes generated from the videos for all the baselines. 

\textbf{Performance Analysis of Learned Policies} We compare the performance of our method with the above baselines on 9 tasks generated from SSv2 dataset and 3 harder tasks collected by ourselves, as shown in Tab. \ref{tab:main_results_sth}. We find that the proposed Video2Policy method outperforms the baseline in most tasks.
RoboGen \citep{wang2023robogen} and Eureka \citep{ma2023eureka} achieve comparable results to ours in the videos of a single object. However, for multiple objects, the performance of RoboGen drops a lot, while the Eureka has much larger variances during training.  
Moreover, for harder tasks from self-collected videos, including the non-convex object manipulation and dynamic tasks, the three baselines perform much worse, especially for the CoP and RoboGen. CoP fails in all cases because the script policy receives feedback from the environment less frequently and cannot control the speed of the object.
For example, the task \texttt{throw garlic into bowl} will reset the bowl into some place where the agent cannot reach. However, CoP can only pick and place into a precise position, which fails in the throwing motion. 
Moreover, Fig. \ref{fig:iteration_time} demonstrates that our method significantly outperforms the other baselines across
multiple iterations.

\begin{figure}[h]
    \centering
    \vskip -.4cm
    \includegraphics[width=.9\linewidth]{pdfs/iteration_time_only.pdf}
    \vskip -.5cm
    \caption{\textbf{V2P} achieves better performance across iteration.}
    \label{fig:iteration_time}
    \vskip -.5cm
\end{figure}


\subsection{Policy Generalization Analysis from diverse videos}
\label{sec:experiment:general_policy}

\begin{figure*}
    \centering
    \vskip -0.3cm
    \includegraphics[width=.95\linewidth]{pdfs/comparison_general.pdf}
    \vskip -0.3cm
    \caption{\textbf{Performance of the trained general policy on 10 unseen task instances.} BC-V2P outperforms BC-CoP on \textbf{9} of 10 significantly.}
    \label{fig:comparison_general}
    \vskip -0.3cm
\end{figure*}

To further demonstrate the scalability of our framework, we target training a general policy from diverse videos. As mentioned in Sec. \ref{sec:method:general}, we regard the Video2Policy as a data engine to generate expert trajectories in simulation. 
To validate the generalization ability across unseen videos, we focus on one single behavior, lifting up. 
Specifically, we sample 100 videos concerning the \texttt{lifting} behavior from SSv2 dataset, generate the scenes, and train policies for the corresponding task. Afterward, we collect 100 successful trajectories from each policy model, including the 256 $\times$ 256 size of RGB image observation and the 7-dim actions. Then we train an image-based policy model by imitation learning from the collected trajectories. Finally, we sample another 10 different \texttt{lifting} videos and evaluate the performance on those tasks with novel objects.

\textbf{Training Details} We choose Behavior Cloning (BC) to learn the general policy. For the model architecture, we apply the pre-trained Resnet18 \citep{he2016deep} as the backbone to extract features and stack 2 frames as observations. Then a policy head is built in a 3-layer MLP with hidden states of 512 dim. Additionally, the policies are trained on the collected trajectories for 30 epochs with a batch size of 1024. The learning rate of the policy head is 3e-4, while set to 3e-5 for the Resnet backbone. For evaluation, we evaluate the final checkpoint on the 10 \texttt{lifting} tasks for 3 seeds, and we average the results on 10 trajectories for each seed. The evaluation tasks include 5 objects with novel shapes as well as novel textures, and 5 objects with unseen categories.

\textbf{Baseline of General Policy} For general policies, we consider the proposed Video2Policy as a novel data engine for expert trajectory collection. Therefore, other data engines are the baseline, which can generate successful trajectories in our reconstructed scenes. We compare the following models: (1) Code-as-Policy (CoP), which can be applied to the novel tasks directly with state input instead of images; (2) BC-CoP, which trains the BC general policy from the data collected by CoP; (3) BC-V2P, same as BC-CoP but using the data collected by V2P. The CoP baseline demonstrates how well the state-based general policy can be, while the latter BC-CoP and BC-V2P results illustrate the performance of the general policy under different data engines.

\textbf{Generalization to Unseen Videos} The performances of the models are shown in Fig. \ref{fig:comparison_general}. After training on 100 task instances, our BC-V2P model works on all the novel tasks and achieves remarkable generalization performance, marked in \textcolor{red}{red}. Specifically, the average success rate reaches 75\%, while CoP is 32\% and the BC-CoP is 26\%. It indicates that our proposed Video2Policy framework can obtain more informative policy priors from the videos. Notably, the CoP results are based on states and the variances from CoP policies are larger than those from V2P policies. And the model performs worse on the objects with unseen categories compared to the objects with novel shapes. 

\begin{figure}
    \centering
    \vskip -0.1cm
    \includegraphics[width=.95\linewidth]{pdfs/scalability.pdf}
    \vskip -0.3cm
    \caption{Scalability of the general policy model (BC-V2P) towards the number of training tasks on \texttt{lifting} behavior.}
    \label{fig:scalability}
    \vskip -0.3cm
\end{figure}

\textbf{Scalability Analysis} Furthermore, it is significant to investigate the scalability of our framework towards the number of generated tasks from videos. We analyze it by training the general policy on $N \in \{10, 20, 30, ..., 100\}$ tasks and evaluate the same 10 evaluation tasks, shown in Fig. \ref{fig:scalability}. Overall, increasing the number of videos continues to improve the performance of the BC-V2P model. Note that current real2sim methods only use a few scenes such as 6 scenes \citep{torne2024reconciling}. In contrast, our method can leverage large amounts of scenes and learn policies.

\subsection{Sim2real for the learned general policy}
\label{sec:experiment:sim2real}

We also conduct sim2real experiments. Specifically, following Sec. \ref{sec:experiment:general_policy}, we collect 200 trajectories from each reconstructed scene in simulation for 100 \texttt{lifting} tasks. Then we train a policy model via imitation learning and deploy the policy in the real world.
Moreover, the object's position varies within a 10 cm range. The image input had a resolution of 256x256. In terms of the setup, we use Franka robotic arms, Robotiq grippers, and Stereolabs cameras. We evaluate the performance of the policy towards lifting a mouse, a cup, and a piece of bread. Notably, while there are some mouse and bottle objects in the simulation, the bread is absent in the collected simulation dataset and is soft.
To alleviate the sim-to-real gap, we employ two strategies:

\textbf{Input Representation and Network Architecture. }
We take as input the segmentation masks of the image, the robot’s end-effector (EEF) state, and the gripper state. SAM-2 is adopted for segmentation, where the pixel position of the target object is provided manually as input in the first frame, shown in Fig. \ref{fig:sim2real}. We stack 2 frames and add an additional multi-layer perceptron (MLP) layer to map the robot state into a 256-dimensional feature vector. Furthermore, the rotation component of the action is scaled by a factor of 0.2.

\textbf{Domain Randomization. }
During data collection in the simulation, randomization is applied to the actions with noise levels of 0.02 and a random delay of 0.01–0.02 seconds. Moreover, the physical properties of the object, such as size and weight, are also randomized. We ensure consistency between the camera poses in sim and real.

Here the general \texttt{lifting} policy achieves a success rate of 72\% across 10 novel objects in simulation.
The sim2real results are as shown in Tab. \ref{tab:sim2real}. Compared to the 72\% success rate in simulation, it achieves 47\% success rate in the real world. It proves the efficacy of our pipeline, which builds the pipeline of internet videos to policies.
We notice that gripping slippery objects, such as a cup, pose challenges for the robot, resulting in a relatively low success rate. For the bread, the task was relatively easier despite the object being unseen in the simulation. This can be attributed to the segmentation mask observation and the bread's relatively large surface area, which facilitates successful manipulation.

\begin{table}[t]
    \centering
    \vskip -.3cm
    \caption{The success rates of the learned policy for \texttt{lifting} tasks on real robots, with 72\% success rate in simulation.}
    \begin{tabular}{c|ccc|c}
    \toprule
     & Mouse & Cup & Bread & Average \\
     \midrule
      Succ. & 0.50 & 0.40 & 0.50 & 0.47 \\
    \bottomrule
    \end{tabular}
    \label{tab:sim2real}
    \vskip -.2cm
\end{table}

\begin{table}[t]
    \centering
    \vskip -.2cm
    \caption{\label{tab:ablation} \textbf{Ablation Results} toward the visual information, success function picking, reward function iteration, and reward function sampling. Removing any component results in performance drop.} 
    \begin{tabular}{l|c}
        \toprule
        \textbf{Task} (Succ.) & \textbf{Avg.} \\
        \midrule
        \textbf{Video2Policy (V2P)} & \textbf{0.87} \\
        \textbf{V2P w.o. visual information} & 0.75 \\
        \textbf{V2P w.o. success picking} & 0.57 \\
        \textbf{V2P w.o. iterative reward designs} & 0.48 \\
        \textbf{V2P w.o. multiple reward samples} & 0.51 \\
        \bottomrule
    \end{tabular}
    \vskip -.3cm
\end{table}

Overall, these experiments demonstrate that the general policy trained in simulation possesses effective sim-to-real transfer capabilities. Additionally, the results highlight the potential of the proposed Video2Policy pipeline, underscoring its effectiveness in enabling good performance, scalability, and deployment in real-world scenarios. The videos of the robot are attached in supplementary materials.

\subsection{Ablation Study}
\label{sec:experiment:ablation_study}

Compared to the previous works on LLM-driven RL \citep{ma2023eureka, wang2023robogen}, we apply the visual information and more reasoning from VLMs.

\textbf{Ablation of Code Generation Components} Here, we ablate the results of removing each part in the code generation as follows: (1) V2P w.o. visual information, where only the video caption is provided to generate the codes; (2) V2P w.o. success picking, where we do not pick the best success function by sampling 8 different success functions; (3) V2P w.o. iterative reward designs, where we do not apply the iterative reward reflection to fine-tuning the reward functions; (4) V2P w.o. multiple reward samples, where we only train 1 example of the reward function instead of 8 samples. The results are on the Tab. \ref{tab:ablation}. It will encounter a performance drop without any part. The most significant one can be the iterative reward reflection component, which finetunes the reward function based on the previous training results. Additionally, w.o. the multiple reward samples and w.o. success picking have larger variances than the others. With those techniques, we achieve better performance than the previous LLM-driven methods \citep{ma2023eureka, wang2023robogen}.
We also make ablations about the generated success function, reset function, tracking prompts and hallucination issues in App. \ref{app:more_ablation}.

Moreover, we conduct experiments concerning the robustness analysis for each component and performance analysis for iterative generation in App. \ref{app:more_exp}.

