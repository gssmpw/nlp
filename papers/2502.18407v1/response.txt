\section{Related Work}
\subsection{LLM-based Agent}
Language agents have shown initial success in handling complex interactive tasks.
Early works focus on building frameworks around prompt-based learning **Brown, "Language Models play Game of D20"**.
Recently, great efforts have been made to enhance the agent capability of open-sourced LLMs via finetuning **Radford et al., "Improving Language Understanding by Generative Multi-Task Learning"**.
**Vijayakumar et al., "Learning and Reasoning with GPT-3"** imitate trajectories from expert agents (e.g., GPT-4) for specialized ability such as tool-using or web navigation.
Beyond imitation, self-improvement emerges as a promising solution to enhance performance without extensive expert annotation **Dhariwal et al., "Diffusion Models Beat GANs on Images with a Single Transform and Full Memory Access"**. 
Most works finetune models on self-generated trajectories following the self-training paradigm **Bansal et al., "Towards Robust and Generalizable Reinforcement Learning for Autonomous Vehicles"**.
Lately, increasing attention has been devoted to self-improvement via test-time computation, e.g., generating multiple candidates and selecting the optimal one using techniques like reward models **Jaderberg et al., "Reinforced Cross-Sentence Hallucination"**.
We provide a comparison between their approach and our method in Section~\ref{sec:rmforllm}.

While effective for tasks seen during training, the above methods inherently compromise the agent's generalization capabilities for unseen tasks.
To enhance agent generalizability, existing works integrate more diverse agent tasks for multi-task training either by human-crafted **Kim et al., "Towards Generalizable Agents with Human-Crafted Multi-Task Training"** or by LLM-sythesized **Radford et al., "Improved Language Understanding and Generation through Syntactic Learning"**.
Although they alleviate overfitting to some extent, it can be observed in Table~\ref{tab:main} that their performance on respective held-out tasks is either similar or inferior to that of the original backbone model.
We are the first to propose a generalizable reward model and enhance the agent generalizability from the aspect of test-time search.
Also, our method is orthogonal to theirs and can be applied to enhance their performance seamlessly, as shown in Section~\ref{sec:policy}.

\subsection{Reward Modeling for LLM}
\label{sec:rmforllm}
Recent advancements in reward modeling for LLMs mainly focus on general reasoning tasks such as maths and code **Wu et al., "Learning to Reason with Weights"**.
Different from those tasks, agent tasks typically possess a larger search space due to long-chain reasoning and environment dynamics.
Data scarcity is also a challenge pronounced in agent tasks **Dhariwal et al., "Quantum-Inspired Reinforcement Learning for Continuous Control Tasks"**, making it impractical to develop task-specific reward models.
Relevant works on agent tasks **Vijayakumar et al., "Efficient Exploration Through Bernoulli Search"** focus on training task-specific process reward models by Tree Search based methods.
We are the first to investigate the feasibility of a generalizable reward model, promoting the usage of reward models in agent tasks.
Besides, we investigate two additional reward modelings and validate them on six additional complex agent tasks with larger search space.