% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}
@misc{zhai2024enhancingdecisionmakingllmagents,
      title={Enhancing Decision-Making for LLM Agents via Step-Level Q-Value Models}, 
      author={Yuanzhao Zhai and Tingkai Yang and Kele Xu and Feng Dawei and Cheng Yang and Bo Ding and Huaimin Wang},
      year={2024},
      eprint={2409.09345},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2409.09345}, 
}
@article{putta2024agent,
  title={Agent q: Advanced reasoning and learning for autonomous ai agents},
  author={Putta, Pranav and Mills, Edmund and Garg, Naman and Motwani, Sumeet and Finn, Chelsea and Garg, Divyansh and Rafailov, Rafael},
  journal={arXiv preprint arXiv:2408.07199},
  year={2024}
}
@article{yao2022webshop,
  title={Webshop: Towards scalable real-world web interaction with grounded language agents},
  author={Yao, Shunyu and Chen, Howard and Yang, John and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={20744--20757},
  year={2022}
}
@article{yao2022react,
  title={React: Synergizing reasoning and acting in language models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  journal={arXiv preprint arXiv:2210.03629},
  year={2022}
}
@article{shinn2024reflexion,
  title={Reflexion: Language agents with verbal reinforcement learning},
  author={Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{qin2023toolllm,
  title={Toolllm: Facilitating large language models to master 16000+ real-world apis},
  author={Qin, Yujia and Liang, Shihao and Ye, Yining and Zhu, Kunlun and Yan, Lan and Lu, Yaxi and Lin, Yankai and Cong, Xin and Tang, Xiangru and Qian, Bill and others},
  journal={arXiv preprint arXiv:2307.16789},
  year={2023}
}
@article{deng2024mind2web,
  title={Mind2web: Towards a generalist agent for the web},
  author={Deng, Xiang and Gu, Yu and Zheng, Boyuan and Chen, Shijie and Stevens, Sam and Wang, Boshi and Sun, Huan and Su, Yu},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{chen2023fireact,
  title={Fireact: Toward language agent fine-tuning},
  author={Chen, Baian and Shu, Chang and Shareghi, Ehsan and Collier, Nigel and Narasimhan, Karthik and Yao, Shunyu},
  journal={arXiv preprint arXiv:2310.05915},
  year={2023}
}
@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}
@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
@article{zeng2023agenttuning,
  title={Agenttuning: Enabling generalized agent abilities for llms},
  author={Zeng, Aohan and Liu, Mingdao and Lu, Rui and Wang, Bowen and Liu, Xiao and Dong, Yuxiao and Tang, Jie},
  journal={arXiv preprint arXiv:2310.12823},
  year={2023}
}
@article{zhang2024rest,
  title={Rest-mcts*: Llm self-training via process reward guided tree search},
  author={Zhang, Dan and Zhoubian, Sining and Hu, Ziniu and Yue, Yisong and Dong, Yuxiao and Tang, Jie},
  journal={arXiv preprint arXiv:2406.03816},
  year={2024}
}
@article{song2024trial,
  title={Trial and error: Exploration-based trajectory optimization for llm agents},
  author={Song, Yifan and Yin, Da and Yue, Xiang and Huang, Jie and Li, Sujian and Lin, Bill Yuchen},
  journal={arXiv preprint arXiv:2403.02502},
  year={2024}
}
@article{chen2024self,
  title={Self-play fine-tuning converts weak language models to strong language models},
  author={Chen, Zixiang and Deng, Yihe and Yuan, Huizhuo and Ji, Kaixuan and Gu, Quanquan},
  journal={arXiv preprint arXiv:2401.01335},
  year={2024}
}
@inproceedings{yin2024agent,
  title={Agent lumos: Unified and modular training for open-source language agents},
  author={Yin, Da and Brahman, Faeze and Ravichander, Abhilasha and Chandu, Khyathi and Chang, Kai-Wei and Choi, Yejin and Lin, Bill Yuchen},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={12380--12403},
  year={2024}
}
@article{xiong2024watch,
  title={Watch Every Step! LLM Agent Learning via Iterative Step-Level Process Refinement},
  author={Xiong, Weimin and Song, Yifan and Zhao, Xiutian and Wu, Wenhao and Wang, Xun and Wang, Ke and Li, Cheng and Peng, Wei and Li, Sujian},
  journal={arXiv preprint arXiv:2406.11176},
  year={2024}
}
@article{deng2024novice,
  title={From Novice to Expert: LLM Agent Policy Optimization via Step-wise Reinforcement Learning},
  author={Deng, Zhirui and Dou, Zhicheng and Zhu, Yutao and Wen, Ji-Rong and Xiong, Ruibin and Wang, Mang and Chen, Weipeng},
  journal={arXiv preprint arXiv:2411.03817},
  year={2024}
}
@misc{trivedi2024appworldcontrollableworldapps,
      title={AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents}, 
      author={Harsh Trivedi and Tushar Khot and Mareike Hartmann and Ruskin Manku and Vinty Dong and Edward Li and Shashank Gupta and Ashish Sabharwal and Niranjan Balasubramanian},
      year={2024},
      eprint={2407.18901},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2407.18901}, 
}
@misc{noyman2024travelagentgenerativeagentsbuilt,
      title={TravelAgent: Generative Agents in the Built Environment}, 
      author={Ariel Noyman and Kai Hu and Kent Larson},
      year={2024},
      eprint={2412.18985},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2412.18985}, 
}
@misc{rafailov2024directpreferenceoptimizationlanguage,
      title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model}, 
      author={Rafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn},
      year={2024},
      eprint={2305.18290},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.18290}, 
}
@misc{yuan2024freeprocessrewardsprocess,
      title={Free Process Rewards without Process Labels}, 
      author={Lifan Yuan and Wendi Li and Huayu Chen and Ganqu Cui and Ning Ding and Kaiyan Zhang and Bowen Zhou and Zhiyuan Liu and Hao Peng},
      year={2024},
      eprint={2412.01981},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.01981}, 
}
@article{ma2024agentboard,
  title={AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents},
  author={Ma, Chang and Zhang, Junlei and Zhu, Zhihao and Yang, Cheng and Yang, Yujiu and Jin, Yaohui and Lan, Zhenzhong and Kong, Lingpeng and He, Junxian},
  journal={arXiv preprint arXiv:2401.13178},
  year={2024}
}
@article{xi2024agentgym,
  title={AgentGym: Evolving Large Language Model-based Agents across Diverse Environments},
  author={Xi, Zhiheng and Ding, Yiwen and Chen, Wenxiang and Hong, Boyang and Guo, Honglin and Wang, Junzhe and Yang, Dingwen and Liao, Chenyang and Guo, Xin and He, Wei and others},
  journal={arXiv preprint arXiv:2406.04151},
  year={2024}
}
@misc{fu2025agentrefineenhancingagentgeneralization,
      title={AgentRefine: Enhancing Agent Generalization through Refinement Tuning}, 
      author={Dayuan Fu and Keqing He and Yejie Wang and Wentao Hong and Zhuoma Gongque and Weihao Zeng and Wei Wang and Jingang Wang and Xunliang Cai and Weiran Xu},
      year={2025},
      eprint={2501.01702},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2501.01702}, 
}
@article{chen2024agent,
  title={Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models},
  author={Chen, Zehui and Liu, Kuikun and Wang, Qiuchen and Zhang, Wenwei and Liu, Jiangning and Lin, Dahua and Chen, Kai and Zhao, Feng},
  journal={arXiv preprint arXiv:2403.12881},
  year={2024}
}
@article{zhang2024agentohana,
  title={AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning},
  author={Zhang, Jianguo and Lan, Tian and Murthy, Rithesh and Liu, Zhiwei and Yao, Weiran and Tan, Juntao and Hoang, Thai and Yang, Liangwei and Feng, Yihao and Liu, Zuxin and others},
  journal={arXiv preprint arXiv:2402.15506},
  year={2024}
}
@article{hu2024agentgen,
  title={Agentgen: Enhancing planning abilities for large language model based agent via environment and task generation},
  author={Hu, Mengkang and Zhao, Pu and Xu, Can and Sun, Qingfeng and Lou, Jianguang and Lin, Qingwei and Luo, Ping and Rajmohan, Saravan and Zhang, Dongmei},
  journal={arXiv preprint arXiv:2408.00764},
  year={2024}
}
@inproceedings{kwon2023efficient,
  title={Efficient memory management for large language model serving with pagedattention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={611--626},
  year={2023}
}
@article{uesato2022solving,
  title={Solving math word problems with process-and outcome-based feedback},
  author={Uesato, Jonathan and Kushman, Nate and Kumar, Ramana and Song, Francis and Siegel, Noah and Wang, Lisa and Creswell, Antonia and Irving, Geoffrey and Higgins, Irina},
  journal={arXiv preprint arXiv:2211.14275},
  year={2022}
}
@article{lightman2023let,
  title={Let's verify step by step},
  author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
  journal={arXiv preprint arXiv:2305.20050},
  year={2023}
}
@article{wang2023math,
  title={Math-shepherd: A label-free step-by-step verifier for llms in mathematical reasoning},
  author={Wang, Peiyi and Li, Lei and Shao, Zhihong and Xu, RX and Dai, Damai and Li, Yifei and Chen, Deli and Wu, Y and Sui, Zhifang},
  journal={arXiv preprint arXiv:2312.08935},
  year={2023}
}
@article{feng2023alphazero,
  title={Alphazero-like tree-search can guide large language model decoding and training},
  author={Feng, Xidong and Wan, Ziyu and Wen, Muning and McAleer, Stephen Marcus and Wen, Ying and Zhang, Weinan and Wang, Jun},
  journal={arXiv preprint arXiv:2309.17179},
  year={2023}
}
@article{hosseini2024v,
  title={V-star: Training verifiers for self-taught reasoners},
  author={Hosseini, Arian and Yuan, Xingdi and Malkin, Nikolay and Courville, Aaron and Sordoni, Alessandro and Agarwal, Rishabh},
  journal={arXiv preprint arXiv:2402.06457},
  year={2024}
}
@article{yuan2024self,
  title={Self-rewarding language models},
  author={Yuan, Weizhe and Pang, Richard Yuanzhe and Cho, Kyunghyun and Sukhbaatar, Sainbayar and Xu, Jing and Weston, Jason},
  journal={arXiv preprint arXiv:2401.10020},
  year={2024}
}
@article{chen2024travelagent,
  title={TravelAgent: An AI assistant for personalized travel planning},
  author={Chen, Aili and Ge, Xuyang and Fu, Ziquan and Xiao, Yanghua and Chen, Jiangjie},
  journal={arXiv preprint arXiv:2409.08069},
  year={2024}
}
@article{trivedi2024appworld,
  title={Appworld: A controllable world of apps and people for benchmarking interactive coding agents},
  author={Trivedi, Harsh and Khot, Tushar and Hartmann, Mareike and Manku, Ruskin and Dong, Vinty and Li, Edward and Gupta, Shashank and Sabharwal, Ashish and Balasubramanian, Niranjan},
  journal={arXiv preprint arXiv:2407.18901},
  year={2024}
}
@article{wang2024learning,
  title={Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents},
  author={Wang, Renxi and Li, Haonan and Han, Xudong and Zhang, Yixuan and Baldwin, Timothy},
  journal={arXiv preprint arXiv:2402.11651},
  year={2024}
}
@article{shridhar2020alfworld,
  title={Alfworld: Aligning text and embodied environments for interactive learning},
  author={Shridhar, Mohit and Yuan, Xingdi and C{\^o}t{\'e}, Marc-Alexandre and Bisk, Yonatan and Trischler, Adam and Hausknecht, Matthew},
  journal={arXiv preprint arXiv:2010.03768},
  year={2020}
}
@article{wang2022scienceworld,
  title={Scienceworld: Is your agent smarter than a 5th grader?},
  author={Wang, Ruoyao and Jansen, Peter and C{\^o}t{\'e}, Marc-Alexandre and Ammanabrolu, Prithviraj},
  journal={arXiv preprint arXiv:2203.07540},
  year={2022}
}
@article{zheng2023judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={46595--46623},
  year={2023}
}
@article{zhang2025lessons,
  title={The Lessons of Developing Process Reward Models in Mathematical Reasoning},
  author={Zhang, Zhenru and Zheng, Chujie and Wu, Yangzhen and Zhang, Beichen and Lin, Runji and Yu, Bowen and Liu, Dayiheng and Zhou, Jingren and Lin, Junyang},
  journal={arXiv preprint arXiv:2501.07301},
  year={2025}
}
@article{cobbe2021gsm8k,
  title={Training Verifiers to Solve Math Word Problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}
@article{gao2022pal,
  title={PAL: Program-aided Language Models},
  author={Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  journal={arXiv preprint arXiv:2211.10435},
  year={2022}
}
@article{hendrycks2021measuring,
  title={Measuring mathematical problem solving with the math dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021}
}
@article{li2022competition,
  title={Competition-level code generation with alphacode},
  author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and others},
  journal={Science},
  volume={378},
  number={6624},
  pages={1092--1097},
  year={2022},
  publisher={American Association for the Advancement of Science}
}
@misc{lin2025qlassboostinglanguageagent,
      title={QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search}, 
      author={Zongyu Lin and Yao Tang and Xingcheng Yao and Da Yin and Ziniu Hu and Yizhou Sun and Kai-Wei Chang},
      year={2025},
      eprint={2502.02584},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2502.02584}, 
}
@article{durante2024agent,
  title={Agent ai: Surveying the horizons of multimodal interaction},
  author={Durante, Zane and Huang, Qiuyuan and Wake, Naoki and Gong, Ran and Park, Jae Sung and Sarkar, Bidipta and Taori, Rohan and Noda, Yusuke and Terzopoulos, Demetri and Choi, Yejin and others},
  journal={arXiv preprint arXiv:2401.03568},
  year={2024}
}
@article{watkins1992q,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  pages={279--292},
  year={1992},
  publisher={Springer}
}
@article{silver2017mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Publishing Group}
}
@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}
@inproceedings{huang-etal-2023-large,
    title = "Large Language Models Can Self-Improve",
    author = "Huang, Jiaxin  and
      Gu, Shixiang  and
      Hou, Le  and
      Wu, Yuexin  and
      Wang, Xuezhi  and
      Yu, Hongkun  and
      Han, Jiawei",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.67/",
    doi = "10.18653/v1/2023.emnlp-main.67",
    pages = "1051--1068",
    abstract = "Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate {\textquotedblleft}high-confidence{\textquotedblright} rationale-augmented answers for unlabeled questions using Chain-of-Though (CoT) prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that without any ground truth label, our approach improves the general reasoning ability of a 540B-parameter LLM (74.4{\%}$\rightarrow$82.1{\%} on GSM8K, 90.0{\%}$\rightarrow$94.4{\%} on OpenBookQA, and 63.4{\%}$\rightarrow$67.9{\%} on ANLI-A3) and can also be adapted to extreme low-resource cases where even training questions and CoT prompts are limited. We conduct ablation studies and show that fine-tuning on diverse reasoning paths is critical for self-improvement."
}
@article{wang2024q,
  title={Q*: Improving multi-step reasoning for llms with deliberative planning},
  author={Wang, Chaojie and Deng, Yanchen and Lyu, Zhiyi and Zeng, Liang and He, Jujie and Yan, Shuicheng and An, Bo},
  journal={arXiv preprint arXiv:2406.14283},
  year={2024}
}
@article{cui2025process,
  title={Process reinforcement through implicit rewards},
  author={Cui, Ganqu and Yuan, Lifan and Wang, Zefan and Wang, Hanbin and Li, Wendi and He, Bingxiang and Fan, Yuchen and Yu, Tianyu and Xu, Qixin and Chen, Weize and others},
  journal={arXiv preprint arXiv:2502.01456},
  year={2025}
}
@misc{acikgoz2025singlemodelmastermultiturn,
      title={Can a Single Model Master Both Multi-turn Conversations and Tool Use? CALM: A Unified Conversational Agentic Language Model}, 
      author={Emre Can Acikgoz and Jeremiah Greer and Akul Datta and Ze Yang and William Zeng and Oussama Elachqar and Emmanouil Koukoumidis and Dilek Hakkani-Tür and Gokhan Tur},
      year={2025},
      eprint={2502.08820},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2502.08820}, 
}
@article{yuan2025agent,
  title={Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training},
  author={Yuan, Siyu and Chen, Zehui and Xi, Zhiheng and Ye, Junjie and Du, Zhengyin and Chen, Jiecao},
  journal={arXiv preprint arXiv:2501.11425},
  year={2025}
}
@misc{gu2025surveyllmasajudge,
      title={A Survey on LLM-as-a-Judge}, 
      author={Jiawei Gu and Xuhui Jiang and Zhichao Shi and Hexiang Tan and Xuehao Zhai and Chengjin Xu and Wei Li and Yinghan Shen and Shengjie Ma and Honghao Liu and Saizhuo Wang and Kun Zhang and Yuanzhuo Wang and Wen Gao and Lionel Ni and Jian Guo},
      year={2025},
      eprint={2411.15594},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.15594}, 
}
@article{mialon2023augmented,
  title={Augmented language models: a survey},
  author={Mialon, Gr{\'e}goire and Dess{\`\i}, Roberto and Lomeli, Maria and Nalmpantis, Christoforos and Pasunuru, Ram and Raileanu, Roberta and Rozi{\`e}re, Baptiste and Schick, Timo and Dwivedi-Yu, Jane and Celikyilmaz, Asli and others},
  journal={arXiv preprint arXiv:2302.07842},
  year={2023}
}
@article{sumers2023cognitive,
  title={Cognitive architectures for language agents},
  author={Sumers, Theodore R and Yao, Shunyu and Narasimhan, Karthik and Griffiths, Thomas L},
  journal={arXiv preprint arXiv:2309.02427},
  year={2023}
}
@article{yang2024weak,
  title={Weak-to-strong reasoning},
  author={Yang, Yuqing and Ma, Yan and Liu, Pengfei},
  journal={arXiv preprint arXiv:2407.13647},
  year={2024}
}