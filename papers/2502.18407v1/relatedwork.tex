\section{Related Work}
\subsection{LLM-based Agent}
Language agents have shown initial success in handling complex interactive tasks.
Early works focus on building frameworks around prompt-based learning \cite{yao2022react, shinn2024reflexion}.
Recently, great efforts have been made to enhance the agent capability of open-sourced LLMs via finetuning \cite{chen2023fireact, yin2024agent}.
\citet{qin2023toolllm, deng2024mind2web} imitate trajectories from expert agents (e.g., GPT-4 \cite{achiam2023gpt}) for specialized ability such as tool-using or web navigation.
Beyond imitation, self-improvement emerges as a promising solution to enhance performance without extensive expert annotation \cite{huang-etal-2023-large}. 
Most works finetune models on self-generated trajectories following the self-training paradigm \cite{wang2024learning, chen2024self, song2024trial, xiong2024watch}.
Lately, increasing attention has been devoted to self-improvement via test-time computation, e.g., generating multiple candidates and selecting the optimal one using techniques like reward models \cite{wang2024q, zhai2024enhancingdecisionmakingllmagents, lin2025qlassboostinglanguageagent}.
We provide a comparison between their approach and our method in Section~\ref{sec:rmforllm}.

While effective for tasks seen during training, the above methods inherently compromise the agent's generalization capabilities for unseen tasks.
To enhance agent generalizability, existing works integrate more diverse agent tasks for multi-task training either by human-crafted \cite{zeng2023agenttuning, chen2024agent, xi2024agentgym, zhang2024agentohana} or by LLM-sythesized \cite{hu2024agentgen, fu2025agentrefineenhancingagentgeneralization}.
Although they alleviate overfitting to some extent, it can be observed in Table~\ref{tab:main} that their performance on respective held-out tasks is either similar or inferior to that of the original backbone model.
We are the first to propose a generalizable reward model and enhance the agent generalizability from the aspect of test-time search.
Also, our method is orthogonal to theirs and can be applied to enhance their performance seamlessly, as shown in Section~\ref{sec:policy}.

\subsection{Reward Modeling for LLM}
\label{sec:rmforllm}
Recent advancements in reward modeling for LLMs mainly focus on general reasoning tasks such as maths and code \cite{uesato2022solving,lightman2023let,wang2023math, zhang2024rest}.
Different from those tasks, agent tasks typically possess a larger search space due to long-chain reasoning and environment dynamics.
Data scarcity is also a challenge pronounced in agent tasks \cite{ma2024agentboard}, making it impractical to develop task-specific reward models.
Relevant works on agent tasks \cite{wang2024q, zhai2024enhancingdecisionmakingllmagents, putta2024agent, lin2025qlassboostinglanguageagent} focus on training task-specific process reward models by Tree Search based methods.
We are the first to investigate the feasibility of a generalizable reward model, promoting the usage of reward models in agent tasks.
Besides, we investigate two additional reward modelings and validate them on six additional complex agent tasks with larger search space.