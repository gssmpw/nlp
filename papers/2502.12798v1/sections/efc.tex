\section{Extension: Trading Envy and Welfare}
\label{sec:extensions}








In the previous section, we have shown that coordinating agents' arrival order alone can significantly reduce envy, without affecting the algorithm's core decision-making process. In this section, we take an initial step toward understanding the efficiency-fairness tradeoff, a well-established concept in the literature on fair allocation~\cite{varian1973equity,bertsimas2012efficiency}  and fair classification~\cite{menon2018cost,zafar2017fairness}. 
Specifically, we extend the definition of algorithms from Section~\ref{sec:model} to allow agent‐specific treatment. In other words, algorithms can now observe agent identities and maintain agents accumulated rewards in their memory. Formally, the relevant histories contain triples of the form (agent index, pulled arm, realized reward). We hope to leverage this additional capability to balance social welfare and envy. 


%Misusing agent-specific information can trivially drive envy to $\Theta(T)$. For example, an algorithm that discriminates against agent~$N$ by always assigning it the arm with the lowest expected reward accrues a constant reward gap each round, leading to $\Theta(T)$ envy. Hence, we must proceed cautiously, 

We focus on the special case of our running example (Example~\ref{example 1}): $N=2$ agents, $K=2$ arms with rewards drawn from the uniform distribution, $X_1, X_2 \sim \uni{0,1}$, and the uniform arrival $\uniord$. Furthermore, we assume Bayesian information, i.e., the prior distributions are known. We stress that our results are preliminary, albeit non-trivial. In Subsection~\ref{subsec:ext welfare}, we analyze the socially optimal algorithm from a welfare and envy perspective. Later, in Subsection~\ref{subsec:ext efc}, we develop $\efc$, our welfare-envy balancing algorithm. 
\subsection{Optimal Welfare and Optimal Envy}\label{subsec:ext welfare}
We first analyze the maximal social welfare for this setting. As it turns out, Algorithm~\ref{alguni} is a special case of the optimal two-agent algorithm, as we prove in \ifapp{Section~\ref{appendix:sociallyopt}}{the appendix}. Along with our results from Section~\ref{sec:uniform}, we conclude that:
\begin{observation}\label{obs:opt for tradeoff}
When executing Algorithm~\ref{alguni} on the instance of Example~\ref{example 1} and $\uniord$, it achieves an expected social welfare of (1+$\frac{1}{8}) T$ and induces an expected envy of $\env^T(\uniord)=\tilde \Theta(\sqrt T)$. Furthermore, this is the optimal welfare.
\end{observation}
%The observation suggests that Algorithm~\ref{alguni} is positioned on an extreme point of the Pareto frontier of welfare-envy XXXX, with maximal welfare and high envy. 
This observation indicates that Algorithm~\ref{alguni} occupies an extreme point on the Pareto frontier of the welfare-envy tradeoff: It achieves maximum welfare but also incurs high envy. Another point on the frontier is the algorithm we call $NE$ (\textbf{N}o \textbf{E}nvy), guaranteeing $\env^t=0$ in every round $t$ almost surely. $NE$ draws the same arm in both sessions of every round, as this is essential to maintain zero envy (since the rewards are uniformly distributed and stochastic by nature). Of course, $NE$ has an expected social welfare of $\sw= T$. 


A compelling way to address the social welfare of any algorithm is by examining its ability to exploit the information obtained in the earlier sessions. For example, comparing the performance of $NE$ and Algorithm~\ref{alguni} highlights this difference: $NE$ does not utilize information from the first session, whereas Algorithm~\ref{alguni} leverages it to secure a better reward in the second round. This strategic use of information by Algorithm~\ref{alguni} results in a welfare increase of $\frac{1}{8}$ each round, but also generates envy. This is a key element in the algorithm we propose next.
\subsection{Envy-freeness up to $C$}\label{subsec:ext efc}
%The algorithmic2e version!
% \begin{algorithm}[t]
% \caption{Envy-freeness up to 1 ($\ef$)}
% \label{alg: ef1}
% \begin{algorithmic}[1]
% \Require {horizon $T$}
% \For{round $t = 1 \ldots T$}\label{alg: ef1 1}
%     \State{pull $a_{1}$}\label{alg: ef1 2}
%     \If{$x^t_1 > \frac{1}{2}$}\label{alg: ef1 3}
%         \State{pull $a_{1}$}\label{alg: ef1 4}
%     \Else\label{alg: ef1 5}
%         \If{$\abs{R^{t-1}_{(1)} + \rt{(1)} - R^{t-1}_{(2)}} \leq 1$ and $\abs{R^{t-1}_{(1)} + \rt{(1)} - R^{t-1}_{(2)} - 1} \leq 1$}\label{alg: ef1 6}
%             \State{pull $a_{2}$}\label{alg: ef1 7}
%         \Else\label{alg: ef1 8}
%             \State{pull $a_{1}$}\label{alg: ef1 9}
%         \EndIf
%     \EndIf
% \EndFor
% \end{algorithmic}
% \end{algorithm}
\begin{algorithm}[t]
\caption{Envy-freeness up to $C$ ($\efc$)}
\label{alg:efc}
\SetAlgoLined
\DontPrintSemicolon
\LinesNumbered
\KwIn{horizon $T$, envy bound $C$}
\For{round $t = 1$ to $T$\label{efcline:for}}{
    pull $a_{1}$\label{efclin:pull_a1}\\
    \lIf{$x^t_1 > \frac{1}{2}$}{pull $a_{1}$\label{efclin:pull_a1_again}}
    \Else{\label{efclin:else}
        \lIf{\textnormal{there exists $r\in [0,1]$ such that $\abs{R^{t-1}_{(1)} + r^t_{(1)} - R^{t-1}_{(2)}-r} > C$}}{pull $a_{1}$\label{efclin:pull_a1_cond}}
        %\lIf{$\abs{R^{t-1}_{(1)} + \rt{(1)} - R^{t-1}_{(2)}} \leq C$ and $\abs{R^{t-1}_{(1)} + \rt{(1)} - R^{t-1}_{(2)} - 1} \leq C$}{Pull $a_{2}$\label{efclin:pull_a2}}\textbf{}
        \lElse{pull $a_{2}$\label{efclin:pull_a2_cond_else}}
    }
}
\end{algorithm}

In what follows, we introduce $\efc$, which is an abbreviation of \textbf{E}nvy-\textbf{F}reeness up to $\textbf{C}$, and is
implemented in Algorithm~\ref{alg:efc}. $\efc$ operates by selectively limiting the exploitation of information when the gap between agents' rewards could potentially exceed a predefined envy threshold $C$. This mechanism enforces envy-freeness up to $C$, allowing for better welfare compared to $NE$ while maintaining low envy.

We now describe how $\efc$ works. It interacts with agents for $T$ rounds (Line~\ref{efcline:for}). In every round~$t$, $\efc$ pulls arm $a_1$ for the agent arriving in the first session (Line~\ref{efclin:pull_a1}). The decision to pull $a_1$ first is arbitrary since both arms are identically distributed. If $a_1$ realizes a high reward, i.e., $r^t_{(1)}=x^t_1 > \frac{1}{2}$, $\efc$ pulls it again for the agent in the second session (Line~\ref{efclin:pull_a1_again}). Otherwise, we enter the ``else'' clause in Line~\ref{efclin:else}.

If $a_1$ yields a low reward, the welfare-wise correct action is to pull $a_2$; however, recall that $\efc$ aims to keep the envy lower than $C$. As a result, it ensures that the envy $\abs{R^t_{(1)}-R^t_{(2)}}$ at the end of round $t$ is lower or equal to $C$. Specifically, the ``if'' clause in Line~\ref{efclin:pull_a1_cond} asks whether there exists a realization $r^t_{(2)}=r$ for which the envy would exceed $C$ by the end of the round. If such a realization exists, it pulls $a_1$. Otherwise, it pulls $a_2$ in Line~\ref{efclin:pull_a2_cond_else}. We term $\ef$ the special case of $\efc$ for $C=1$. 
\begin{theorem}\label{thm:ef1evny+sw}
When executing $\efc$ on the instance of Example~\ref{example 1} and $\uniord$, the following hold:
\begin{enumerate}
    \item For all $t$, $\env^t \leq C$ almost surely.
    \item For $C=1$, the social welfare is $\sw \geq \left(1 + \frac{1}{16}\right)T$.
\end{enumerate}
\end{theorem}
Interestingly, Theorem~\ref{thm:ef1evny+sw} implies that $\ef$ recovers half of the social welfare increase due to exploiting information, $(1+\nicefrac{1}{16})T$ versus $(1+\nicefrac{1}{8})T$ for Algorithm~\ref{alguni} and $T$ for $NE$, while limiting the maximal envy to 1 almost surely. We provide a proof sketch below and defer the full proof to \ifapp{Section~\ref{appendix:sociallyopt}}{the appendix}.
\begin{proof}[Proof sketch of Theorem~\ref{thm:ef1evny+sw}]
The first part of the theorem follows directly, as Line~\ref{efclin:pull_a1_cond} allows the envy at $t$ to change only if
\[
\prb{\env^t> C\middle| R^{t-1}_{(1)}, R^{t-1}_{(2)},r^t_{(1)}}=
\prb{\abs{R^{t-1}_{(1)} + r^t_{(1)} - R^{t-1}_{(2)}-r^t_{(2)}} > C\middle| R^{t-1}_{(1)}, R^{t-1}_{(2)},r^t_{(1)}}=0.
\]
The second part of the theorem requires a more detailed argument. We need to understand how often $\efc$ can exploit the information of the first session and enter the ``if'' clause in Line~\ref{efclin:pull_a1_cond}. Since the probability of entering the clause depends on the current level of envy, we must first understand how envy behaves over time. The main technical ingredient we use is the following proposition.
\begin{proposition}\label{prop:ef1 uni dominance}%{thm: ef1 stochastic dominance}
In every round $t$, the distribution of $\env^t$ is stochastically dominated by the uniform distribution over $[0,1]$. I.e., for any $x \in [0,1]$, it holds that $\prb{\env^t \leq x} \geq x$.
\end{proposition}
Despite its intuitive nature, proving this claim demands careful and thorough case analysis. Equipped with Proposition~\ref{prop:ef1 uni dominance}, we turn to analyze how often $\ef$ pulls $a_2$ after observing a low reward $r^t_{(1)} \leq \frac{1}{2}$ in the first session.
\begin{proposition}\label{prop:ef1 open arm}
%\label{thm: ef1 open arm}
        In every round $t$ with $\rt{(1)}\ \leq \frac{1}{2}$, it holds that $\prb{a^t_{(2)} \neq a^t_{(1)} \mid \rt{(1)} \leq \frac{1}{2}} \geq \frac{1}{2}$. 
\end{proposition}
We complete the proof by computing $\sw(\ef)$ via the law of total expectation, using Proposition~\ref{prop:ef1 open arm} to show that the welfare in every round is $1+\frac{1}{16}$, matching the statement of the theorem.
\end{proof}

\subsection{Beyond $C=1$}
The envy analysis in Theorem~\ref{thm:ef1evny+sw} concerns $\ef$, which is a special case of $\efc$ with $C=1$. Unfortunately, our techniques rely heavily on this fact, and extending it would require a different approach. Our preliminary investigation has led us to the following conjecture.
\begin{conjecture}\label{thm: efc sw}
When executing $\efc$ on the instance of Example~\ref{example 1} with any $C \geq 1$ and $\uniord$, the expected social welfare of at least $\sw \geq (1+\frac{1}{8}-\frac{1}{16C})T$.
\end{conjecture}
Simulations we conducted and appear in \ifapp{Section~\ref{sec:simulations}}{the appendix} suggest that Conjecture~\ref{thm: efc sw} holds, and we hope future work could formally prove it.
%While we could not prove this conjecture, we validate it empirically in Chapter~\ref{chap: simulations}.

