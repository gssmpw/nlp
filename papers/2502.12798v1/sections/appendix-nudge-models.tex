\section{Models Captured by the Nudged Arrival Property}\label{appendix:nudge-models}
%\omer{I do not feel comfortable with this...}
\subsection*{Mallows Model~\cite{mallows1957non}}

The Mallows model prioritizes rankings close to a reference order \( \sigma \). The probability of a sampled ranking \( \pi \) is proportional to \( e^{-\beta d(\pi, \sigma)} \), where \( d(\pi, \sigma) \) is the Kendall's tau distance between \( \pi \) and \( \sigma \), and \( \beta \geq 0 \) is the concentration parameter. For any pair of agents \( i, j \) such that \( \sigma^{-1}(i) < \sigma^{-1}(j) \), the probability that \( i \) precedes \( j \) satisfies (following the detailed argument of \cite[Section 2]{lu2014effective}):
\[
\Pr_{\pi \sim \text{Mallows}}(\pi^{-1}(i) < \pi^{-1}(j)) = \frac{e^{-\beta}}{1 + e^{-\beta}}.
\]
By arithmetic manipulation we get $\delta = \frac{1 - e^{-\beta}}{1 + e^{-\beta}}$. 


\subsection*{Plackett-Luce Model~\cite{marden1996analyzing}}
In the Plackett-Luce model, each agent \( i \) is assigned a positive score \( w_i > 0 \), and the probability of observing a ranking \( \pi \) is given by:
\[
\Pr(\pi) = \prod_{k=1}^{N} \frac{w_{\pi(k)}}{\sum_{j=k}^{N} w_{\pi(j)}}.
\]
For any pair \( i, j \), the probability that \( i \) precedes \( j \) is:
\[
\Pr(\pi^{-1}(i) < \pi^{-1}(j)) = \frac{w_i}{w_i + w_j},
\]
which holds since the model satisfies Luce's choice axiom, which guarantees the independence of the pairwise ranking probabilities from the presence of other options \cite{luce1959individual}. Thus, by setting the scores such that \( \frac{w_i}{w_i + w_j} \geq \frac{1+\delta}{2} \) for every $i$ so that $i$ precedes $j$ in the optimal permutation, the nudged arrival property is satisfied. One way to do it is at each round $t$, recursively set $w_{\pi(1)}^{t} = 1, w_{\pi(i+1)}^{t} = \frac{1+\delta}{1 - \delta} w_{\pi(i)}^{t}$. We thus assume that the weights are not global across rounds but are round-dependent and are adjusted based on the accumulated rewards. This can be interpreted as either the designer nudging different agents more forcefully, or alternatively, in a behavioral approach, users that benefited more from the system in the past are willing to cooperate more with its nudges. 
%\omer{how do we 'set' $w_i$?}


% Bradley-Terry is irrelvant, it's only for n=2 in and of itself. Its importance comes from relation to Mallows and Placket-Luce, but we discuss them independently. 

%\subsection*{Bradley-Terry Model}

%This model directly models pairwise comparisons. Each agent \( i \) is assigned a latent score \( w_i > 0 \). The probability that \( i \) is ranked above \( j \) is:
%\[
%\Pr(\pi^{-1}(i) < \pi^{-1}(j)) = \frac{w_i}{w_i + w_j}.
%\]
%By choosing \( \frac{w_i}{w_i + w_j} = \frac{1+\delta}{2} \), the Bradley-Terry model adheres to the nudged arrival property.

% Noisy sorting is too general, there is no direct implication, though it could be relevant in some way. 

%\subsection*{Noisy Sorting Models}

%Noisy sorting models introduce randomness to the reference order \( \sigma \). The probability of sampling a ranking \( \pi \) decreases as its Kendall tau distance from \( \sigma \) increases. For a pair \( i, j \) with \( \sigma^{-1}(i) < \sigma^{-1}(j) \), the pairwise probability is:
%\[
%\Pr(\pi^{-1}(i) < \pi^{-1}(j)) \geq \frac{1+\delta}{2},
%\]
%provided the noise level is appropriately low. This ensures compliance with the nudged arrival property.

\subsection*{Thurstone-Mosteller Model \cite{ThurstoneModel}}

In the Thurstone-Mosteller model, each agent \( i \) is assigned an independently drawn latent cardinal value \( v_i \sim \mathcal{N}(\mu_i, s^2) \). Thus, the probability that \( i \) is ranked above \( j \) is independent of all other draws besides the pairwise draws, and is exactly the probability that drawing from $i$'s normal variable exceeds drawing from $j$'s normal variable. The difference of two normal variables is normal by itself, with mean $\mu_i - \mu_j$ (the means difference) and variance $2s^2$ (the sum of variances), and we are interested in the probability that this variable is above $0$. We can normalize and shift the mean, and get that the probability that \( i \) is ranked above \( j \) is:
\[
\Pr(\pi^{-1}(i) < \pi^{-1}(j)) = \Phi\left(\frac{\mu_i - \mu_j}{\sqrt{2}s}\right),
\]
where \( \Phi \) is the CDF of the standard normal distribution $N(0,1)$. We can thus tune $\mu_i, \mu_j$ (with some globally-set $s$) so that for every $i,j$,  \( \Pr(\pi^{-1}(i) < \pi^{-1}(j)) \geq \frac{1+\delta}{2} \), and the nudged arrival property is satisfied. One way to do that is by calculating the constant shift of the mean $\delta\mu$ that satisfies \( \Pr(\pi^{-1}(i) < \pi^{-1}(j)) = \frac{1+\delta}{2}\), and have
\[
\mu_{\pi(i)}^{t} = \mu_{\pi(1)}^{t} + (i-1)\cdot \delta\mu. 
\]

%\omer{constant shifts of the means}
%---

%Together, these examples demonstrate that the nudged arrival property is sufficiently general to subsume a wide range of ranking models while providing a coherent and interpretable foundation for reasoning about orderings.
