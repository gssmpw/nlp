\section{Uniform Arrival}
\label{sec:uniform}
In this section, we assume agents arrive uniformly; that is, the uniform arrival function, $\uniord$, picks in every round $t$ a permutation $\ordv_t$ uniformly at random from the set of all distributions $\textnormal{Perm}([N])$. We start with an insight into reward discrepancies for uniform arrival. Then, in Subsections~\ref{subsec:uni upper} and~\ref{subsec:uni lower}, we provide upper and lower bounds on the expected envy, respectively. %and high probability envy.


Recall that $\dif^t_{i,j}$ is the signed reward discrepancy between agents~$i$ and~$j$ at round~$t$. This quantity depends on both the algorithm, the reward distribution, and the arrival function. 
For example, if an algorithm always pulls $a_1$, then $\dif^t_{i,j} = 0$ almost surely for every $i,j,t$, since all agents receive the same reward. By contrast, for more general algorithms, $\dif^t_{i,j}$ can vary almost arbitrarily, reflecting different approaches to exploration and exploitation over time. Under uniform arrival, each round's agent order is chosen uniformly at random from all permutations of $[N]$, ensuring identical treatment of all agents in expectation.  Consequently, the reward discrepancies exhibit a symmetry:
\begin{remark}\label{remark: symmetric dif}
Under $\uniord$ and any algorithm, the random variables $\dif^t_{i,j}$ in round $t$ are identically distributed for all pairs $(i,j)\in [N]^2$.
\end{remark}

Due to Remark~\ref{remark: symmetric dif}, we simplify the notation in this section and use $\dif^t$ to denote the reward discrepancy distribution between any two agents. Clearly, $\E{\dif^t}=0$. Note that the random variables $\adift{i_1}{j_1}$ and $\adift{i_2}{j_2}$ are still correlated for different pairs of agents $(i_1,j_1)$ and $(i_2,j_2)$. For example, $\adift{i}{j}= -\adift{j}{i}$ for all $i,j$. 
Moreover, $\dif^t$ might depend on the rewards obtained in previous rounds, reflecting temporal correlations that are typical in multi-armed bandit settings (e.g., stationary rewards).  As a result, a high reward discrepancy in earlier rounds might suggest a more thorough exploration, leading to a low reward discrepancy in later rounds. Consequently, we cannot assume that $(\dif^t)_{t=1}^T$ are independent.

%Moreover, $\dif^t$ might depend on the rewards obtained in previous rewards, which is often the case in multi-armed bandit settings: The algorithm explores in earlier rounds to exploit in later rounds. As a result, a high reward discrepancy in earlier rounds might suggest a more thorough exploration, leading to a low reward discrepancy in later rounds. Consequently, we cannot assume that $(\dif^t)_{t=1}^T$ are independent.

%\omer{I might want to put the below definition in a dedicated subsection, in the meantime it parks here in iffalse}
\iffalse
explore-first. We say that an algorithm satisfies the explore-first property if, for every round $t$, there exists a session $q(t)$ such that the algorithm behaves as follows:
\begin{itemize}
    \item Before session $q(t)$, the algorithm pulls only unobserved arms.
    \item From session $q(t)$ onward, the algorithm exclusively pulls a single already-observed arm.
\end{itemize}
Formally, 
\begin{definition}[Explore-First Algorithm]
\label{def: threshold}
An algorithm satisfies the \emph{explore-first} property if, for every round $t$, if the algorithm pulls arm $a_i$ for some $i \in [K]$ in sessions $q$ and $q'$ with $q < q'$, then the algorithm pulls $a_i$ in every session $w$ such that $w > q'$.
\end{definition}
Notice that every explore-first algorithm can only commit to one arm in each round, but that arm could vary from round to round and depend on the realized rewards. We make 
\fi


    % in every round $t$ it explores only in the first $K$ sessions and then commit to one arm.
    % I.e., if the algorithm pulls arm $a_i$ in session $q_1$ of round $t$, $K+1 \leq q_1 \leq N$, then
    % \[
    % \abs{\left\{ w \mid 1\leq w\leq K, \at{(w)} = a_i \right\} }\neq 0 ,
    % \]
    % and for every other session $q_2$ of round $t$ s.t. $K+1 \leq q_2 \leq N$, $\at{(q_2)} = a_i$.
% \omer{In the below, you don't really use the fact that the algorithm has a threshold structure. For instance, in Prop 3.2.2, you only assume that the algorithm is rational:
% \begin{definition}[Threshold Algorithm]
% \label{def: rational}
%     An algorithm is called \emph{rational} if, in every session, it either picks a previously unexplored arm or pulls an already explored arm with the best-observed reward.
% \end{definition}
% }

\subsection{Upper Bound}\label{subsec:uni upper}
Our first result is an upper bound on the expected envy.
\begin{theorem}\label{thm: uni upper-bound}
When executing any algorithm, it holds that
\[\E{\max_{1\leq t \leq T} \env^t (\uniord)} \leq 2\sqrt{\ln{(N)} \sum^{T}_{t=1}{\var{\dift}} }.\]
\end{theorem}
\begin{proof}[Proof of Theorem~\ref{thm: uni upper-bound}]
The proof leverages properties of martingales and subgaussian random variables. Since the rewards are bounded, the resulting envy is also bounded and therefore subgaussian with some parameter. However, because envy is a sum of \emph{possibly dependent} random variables, additional arguments are needed to obtain a sharper subgaussian parameter. The following proposition is the primary technical ingredient in achieving this refinement.
\begin{proposition}\label{prop:envy is good SG}
For any two arbitrary agents $i,j\in [N]$, $\max_{1\leq t \leq T} \env^t_{i,j}$ is $\sqrt{\sum^T_{t=1}{\var{\dift}}}$-subgaussian.
\end{proposition}
We prove this proposition in \ifapp{Section~\ref{appendix:uni}}{the appendix}. Equipped with Proposition~\ref{prop:envy is good SG}, we can use the following well-known property of the maximum of subgaussian random variables.
\begin{claim}\label{claim: sg max}
    Let $Y_1, \ldots, Y_n$ be (possibly dependent) $\sigma$-subgaussian random variables. It holds that
    \[
    \E{\max_{i \in [n]}{\{Y_i\}} } \leq \sqrt{2\sigma^2 \ln{(n)} }.
    \]
\end{claim}
Claim~\ref{claim: sg max} is folklore, but we include its proof in \ifapp{Section~\ref{appendix:uni}}{the appendix} for completeness. Using this claim along with Proposition~\ref{prop:envy is good SG}, we obtain 
\[
\E{\max_{1\leq t \leq T} \env^t} =
\E{\max_{i,j \in [N]}{\left\{ {\max_{1\leq t \leq T} \env^t_{i,j}} \right\}}} \leq
\sqrt{2 \ln{(N^2)} \sum^{T}_{t=1}{\var{\dift}} } =
2\sqrt{\ln{(N)} \sum^{T}_{t=1}{\var{\dift}} }.
\]
This concludes the proof of Theorem~\ref{thm: uni upper-bound}.
\end{proof}
\subsubsection{Refining the Variance of Reward Discrepancy}
Although Theorem~\ref{thm: uni upper-bound} contains a factor of $\ln N$, it does not explicitly capture the influence of $N$ or $K$, as these parameters are embedded within $\var{\dift}$. To clarify this further, we refine our analysis by focusing on the subclass of algorithms known as \emph{explore-first} algorithms. 

We say that an algorithm satisfies the explore-first property if, for every round $t$, once it repeats an arm selection (i.e., chooses $a_i$ in two sessions), it commits to $a_i$ for all subsequent sessions in that round. More formally, 
\begin{definition}[Explore-first]\label{def:explore-first}
An algorithm is \emph{explore-first} if for every $t$ there exists a session $q(t)$ such that:
\begin{itemize}
    \item Before session $q(t)$, the algorithm pulls only unobserved arms.
    \item From session $q(t)$ onward, the algorithm exclusively pulls a single already-observed arm.
\end{itemize} 
\end{definition}
Observe that an explore-first algorithm commits to a single arm in each round, but this arm may vary across rounds and depend on the observed rewards. The explore-first property is natural and applies, e.g., to algorithms that use a threshold to start exploitation (like Algorithm~\ref{alguni}). Relying on Definition~\ref{def:explore-first}, we can express $\var{\dift}$ in terms of $N$ and $K$.
\begin{proposition}\label{thm: threshold var}
    When executing any explore-first algorithm with $K$ arms and $N$ agents, it holds that $\var{\dift} \leq \min\left\{1, \frac{(2N-K)(K-1)}{N(N-1)} \right\}$.  
\end{proposition}
The intuition behind this proposition is as follows. Since at most $K$ agents are exploring, the remaining $N-K$ agents are exploiting and receive identical rewards, so no discrepancy occurs between any two exploiting agents. As $N$ grows relative to $K$, most agent pairs consist of two exploiting agents, leaving only a small fraction of pairs---those involving at least one exploring agent---that can contribute to the variance. This decrease in the number of discrepancy-generating pairs results in a tighter overall bound on $\var{\dift}$.

Combining Proposition~\ref{thm: threshold var} with Theorem~\ref{thm: uni upper-bound}, we get the following corollary.
\begin{corollary} \label{thm: sqrt TK N}
    When executing any explore-first algorithm with $K$ arms and $N$ agents, the expected envy is $\E{\max_{1\leq t \leq T} \env^t  (\uniord)}=O\left( \sqrt{\frac{TK\ln(N)}{N}} \right)$.
\end{corollary}

\subsection{Lower Bound}\label{subsec:uni lower}
\label{sec: lb}

Next, we move to develop a lower bound on the expected envy. As is apparent, not all algorithms generate envy. To demonstrate, consider the following complementary cases:
\begin{enumerate}
    \item An instance in which all arms have the same \emph{deterministic} reward. In this case, any algorithm produces zero envy among all agents as all receive the same reward.
    \item An algorithm that pulls the same arm in all sessions. Even if rewards are stochastic, the algorithm does not generate any envy due to reward consistency.
\end{enumerate}
These examples show that envy will not accumulate if the instance or the algorithm are degenerate, motivating to focus on \emph{executions}: A pair of algorithm and reward distributions. In what follows, we characterize a class of non-degenerate executions for which envy always accumulates. we call such compositions \emph{sufficiently random executions}.
Formally,
\begin{definition}[Sufficiently Random]\label{def: sufficiently random}
    An execution is called \emph{sufficiently random} if it holds that
\begin{equation}\label{eq:def suff}
\sum_{t=1}^T{\var{\dift}} \geq \sqrt{T}.   
\end{equation}
%\omer{I dont need $c_0$, I belive. The old version is still here.}
    % An execution is called \emph{sufficiently random} if it there exists $c_0 > 0$ and $T_0 \in \mathbb N$ such that for all $T > T_0$, it holds that
    % \[
    % \sum_{t=1}^T{\var{\dift}} \geq c_0\sqrt{T}.
    % \]    
\end{definition}
In other words, an execution is sufficiently random if the average variance of the reward discrepancy between two agents in a round is greater or equal to $\frac{1}{\sqrt{T}}$. To further illustrate, we provide the following example. 
\begin{example}\label{example: uni suff}
Recall the execution in Example~\ref{example 1}, with $K=2$ arms with  $\uni{0,1}$ rewards, $N=2$ agents, and Algorithm~\ref{alguni}. As we formally show in \ifnum\Includeappendix=0{the appendix}\else{Claim~\ref{claim: uni suff}}\fi, it holds that $\var{\dift} = \frac{1}{12}$; thus, as long as $T \geq 144$, we have $\sum_{t=1}^T{\var{\dift}} = \frac{T}{12}  {\geq } \sqrt{T}
$; hence, Inequality~\eqref{eq:def suff} holds and this execution is sufficiently random. 
\end{example}
Intuitively speaking, any ``reasonable'' algorithm and reward distributions form a sufficiently random execution. As long as the reward distributions are constant w.r.t. the horizon $T$ and the algorithm conducts enough exploration, the execution will be sufficiently random. To demonstrate cases that are insufficiently random, consider the following example.
\begin{example}\label{example: ber suff}
Assume $K$ i.i.d. arms with rewards distributed $Bernoulli(p)$ for some $p \in (0,1)$, and $N$ agents for $N\geq 2$. We focus on the socially optimal algorithm: It picks fresh arms until a reward of $1$ is obtained and exploits it afterward for all the remaining agents. We show in \ifnum\Includeappendix=0{the appendix}\else{Claim~\ref{claim:example ber suff}}\fi~ that $\var{\Delta^t} \geq \frac{2p(1 - p)}{N}$ and $\var{\Delta^t} \leq  2pK$. 


Consequently, as long as  $\frac{2p(1 - p)}{N} \geq \frac{1}{\sqrt T}$ holds, i.e., for $T \geq \frac{N^2}{4p^2(1-p)^2}$, the execution is sufficiently random. However, if $2pK <\frac{1}{\sqrt T}$, which is the case if $p<\frac{1}{2K\sqrt T}$, the execution is insufficiently random.
\end{example}
Next, we derive a lower bound on the envy for sufficiently random executions.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{theorem}\label{thm: uni lower-bound}
Any algorithm as part of sufficiently random execution generates an envy of
\[\E{\max_{1\leq t \leq T} \env^t  (\uniord)} \geq c\sqrt{ \sum^{T}_{t=1}{\var{\dif^t}}},\]
where $c>0$ is a global constant that does not depend on the instance. 
\end{theorem}

\begin{proof}[Proof of Theorem~\ref{thm: uni lower-bound}]
    Noticeably, we can bound $\env^t$ using the envy between two specific agents.
    That is,
    \begin{align}\label{thm: mp uni lower-bound 1}
        \E{\max_{1\leq t \leq T} \env^t} =
        \E{\max_{i,j \in [N]}{\left\{ \max_{1\leq t \leq T} \env^t_{i,j} \right\}}} \geq
        \E{\max_{1\leq t \leq T} \abs{\env^t_{1,N}}} =
        %\E{ \max_{1\leq t \leq T}\abs{\sum^T_{t=1}{\adift{1}{N}}} } =
        \E{\max_{1\leq t \leq T}\abs{\sum^T_{t=1}{\dift}} },
    \end{align}
    where the last inequality is due to  Remark~\ref{remark: symmetric dif}. To express $\E{\max_{1\leq t \leq T}\abs{\sum^T_{t=1}{\dift}} }$ as a function of $\sum_t\var{\dift}$, we can lower bound its quadratic variation as we have done in the proof of the upper bound. Next, 
    we use the Burkholder-Davis-Gundy inequality~\cite{davis1970intergrability}. We present a simplified version of the theorem, as we are only interested in the special case of a discrete-time martingale with $L_1$ norm. %\footnote{The reader may wonder why we do not use the more intuitive and popular Marcinkiewicz–Zygmund (MZ) inequality~\cite{chow2003probability, ibragimov1999analogues}. The MZ inequality assumes independent random variables, while the sequence $({\dift})_t$ exhibits dependence, as learning algorithms can base actions on past rounds.}
    \begin{theorem}\label{thm:BDG}[Burkholder-Davis-Gundy inequality]
    Let $\{M^t\}_{t \geq 0}$ be a discrete-time martingale with $M_0 = 0$. There exist positive constants $A_1$ and $B_1$ such that 
    \[ 
    A_1 \, \E{\sqrt{\sum_{t=1}^T (M^t - M^{t-1})^2}}\leq \E{\max_{0 \leq t \leq T} \abs{M^t}}\leq B_1 \, \E{\sqrt{\sum_{t=1}^T (M^t - M^{t-1})^2}}.
    \]    
    \end{theorem}
    As we formally show in \ifnum\Includeappendix=0{the appendix}\else{Observation~\ref{envy is martingale}}\fi, the sequence $(\env^t_{1,N})_t$ forms a martingale; hence, an immediate corollary from Theorem~\ref{thm:BDG} and Inequality~\eqref{thm: mp uni lower-bound 1} is that\footnote{The reader might be tempted to use this theorem for the upper bound as well, obtaining a straightforward bound without additional intricate arguments. However, the maximal envy $\env^T$ is not a martingale, so we can only apply it to the envy between two agents. Replacing the $\max$ operator with a summation results in an upper bound of $O\left(N^2 \sqrt{\sum_{l=1}^{T} \var{\dift}}\right)$, which includes an  additional multiplicative factor of $\nicefrac{N^2}{\ln^2 N}$ to the bound of Theorem~\ref{thm: uni upper-bound}.}
    \begin{align}\label{eq: gdgggsds}
    \E{\max_{1\leq t \leq T} \env^t} \geq A_1 \E{ \sqrt{\sum^T_{t=1}{ (\dift)^2}} }.
    \end{align}
    
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    % The proof uses heavily the following theorem.
    % \begin{theorem}[Marcinkiewicz–Zygmund Inequality \cite{chow2003probability, ibragimov1999analogues}]\label{thm: MZ}
    % Let $Y_1, \dots, Y_n$ be independent random variables with $\E{Y_i} = 0$ and $\E{|Y_i|^p} <\infty$ for $1\leq p < \infty$. Then there exist global constants $A_p, B_p$ such that
    % \[
    %     {\displaystyle A_{p}\mathbb E\left[\left(\sum _{i=1}^{n}\left\vert Y_{i}\right\vert ^{2}\right)_{}^{p/2}\right]\leq \mathbb E\left[\left\vert \sum _{i=1}^{n}Y_{i}\right\vert ^{p}\right]\leq B_{p}\mathbb E\left[\left(\sum _{i=1}^{n}\left\vert Y_{i}\right\vert ^{2}\right)_{}^{p/2}\right]}.
    % \]
    % \end{theorem}
    % Due to the algorithm being symmetric, the series $\dift$ is a series of centered random variables (\autoref{thm: sum dif sg}) with finite moments (as rewards are bounded).
    % Additionally, the memory-freeness property implies the random variables are independent.
    % Thus, we can apply \autoref{thm: MZ} on the series $\dift$. Picking $p=1$ and plugging the theorem into Inequity~\eqref{thm: mp uni lower-bound 1}, we get
    % \begin{align*}
    %     \E{\env^T} \geq
    %     \E{ \abs{\sum^T_{t=1}{\dift}} } \geq
    %     A_1 \E{ \sqrt{\sum^T_{t=1}{ \dift^2}} }.
    % \end{align*}
    Next, we use the following auxiliary proposition, which provides a reverse Jensen's-like inequality of the square root function.
    \begin{proposition}\label{thm: board}
        Let $Y$ be a non-negative random variable with a finite second moment. It holds
        \begin{align*}
            \E{\sqrt{Y}} \geq \sqrt{\E{Y}}\left(1- \frac{\var{Y}}{2\E{Y}^2}\right).
            \end{align*}
    \end{proposition}
    We prove Proposition~\ref{thm: board} in \ifnum\Includeappendix=0{the appendix}\else{Appendix~\ref{appendix:uni}}\fi. Applying Proposition~\ref{thm: board} with $Y = \sum^T_{t=1}{ \dift^2}$ to Inequality~\eqref{eq: gdgggsds},
    \begin{align}\label{thm: mp uni lower-bound 2}
        \E{\max_{1\leq t \leq T} \env^t} \geq
        A_1 \E{ \sqrt{\sum^T_{t=1}{ \dift^2}} }\geq
        A_1 \sqrt{\E{ \sum^T_{t=1}{ \dift^2}} }
        \left(
        1 - \frac{\var{ \sum^T_{t=1}{ \dift^2} } }{2 \E{ \sum^T_{t=1}{ \dift^2} }^2 }
        \right) .
    \end{align}
    Next, due to our assumption that the execution is sufficiently random, we get  
    \begin{align}\label{thm: mp uni lower-bound 3}
        2 \left(\E{ \sum^T_{t=1}{ \dift^2} }\right)^2 =2 \left( \sum^T_{t=1}{ \E{ \dift^2} - \E{ \dift}^2}\right)^2 = 2 \left( \sum^T_{t=1}{\var{  \dif^t } }\right)^2 \geq 2T.
    \end{align}
    Furthermore, notice that $\var{ \sum^T_{t=1}{ \dift^2} } \leq T$ as the discrepancies $(\dift)_t$ are supported in the $[-1, 1]$ segment. Combining this fact with Inequality~\eqref{thm: mp uni lower-bound 3}, 
    \begin{align}\label{eq:m,bnhjikw}
    \frac{\var{ \sum^T_{t=1}{ \dift^2} } }{2 \E{ \sum^T_{t=1}{ \dift^2} }^2 }\leq \frac{T}{2T}=\frac{1}{2}.
    \end{align}
    Plugging Inequality~\eqref{eq:m,bnhjikw} to Inequality~\eqref{thm: mp uni lower-bound 2}, we conclude that 
    % \begin{align*}
    % \E{\env^T}
    % \geq
    % A_1 \sqrt{\E{ \sum^T_{t=1}{ \dift^2}} }
    % \left(1 - \frac{ T }{2 \sqrt{T}^2 }\right),
    % \end{align*}
    % where the last inequity holds due to the fact that 
    
    % and due to the execution being sufficiently random, i.e., there exists some $T_0$ s.t. if $T > T_0$ then $\sum^T_{t=1}{\var{\dift}}\geq1\cdot\sqrt{T}$ (\autoref{def: sufficiently random}).
    % hence,
    \[
    \E{\max_{1\leq t \leq T} \env^t} \geq
    A_1 \sqrt{\E{ \sum^T_{t=1}{ \dift^2}} }
    \left(1 -\frac{1}{2}\right)
    =
    \frac{A_1}{2}\sqrt{\sum^T_{t=1}{ \var{ \dift }}},
    \]
    where the last inequality is again due to $\E{\dift}=0$. This concludes the proof of Theorem~\ref{thm: uni lower-bound}.
\end{proof}
Theorems~\ref{thm: uni lower-bound} and~\ref{thm: uni upper-bound} imply the following corollary.
\begin{corollary}\label{cor: uni-envy}
Any algorithm as part of sufficiently random execution generates an envy of $\E{\max_{1\leq t \leq T} \env^t  (\uniord)} = \Theta\left(\sqrt{ \sum^{T}_{t=1}{\var{\dif^t}} }\right)$.  
\end{corollary}
Before we complete the section, we remark that high envy may still arise under insufficiently random executions. Indeed, although Definition~\ref{def: sufficiently random} gives a sufficient condition, it is not a necessary one. From a technical perspective, Theorem~\ref{thm: uni lower-bound} applies whenever the left-hand side of Inequality~\eqref{eq:m,bnhjikw} is constant. In certain cases, this allows us to relax the requirement for sufficiently random executions. The following proposition shows that this relaxation holds in the instance described in Example~\ref{example: ber suff}. Recall that the execution in this example is insufficiently random for $p < \frac{1}{2K\sqrt T}$. However, the next proposition shows that the tight bound in Corollary~\ref{cor: uni-envy} can still hold if $p$ is significantly smaller.
\begin{proposition}\label{prop:insufficient}
    For in the execution in Example~\ref{example: ber suff}, as long as    
    $p\in \left[\frac{N}{cT}, 1-\frac{N}{cT}\right]$ for a constant $c \geq 2$ and $T \geq N$, the envy satisfies $\E{\max_{1\leq t \leq T} \env^t  (\uniord)} = \Theta\left(\sqrt{ \sum^{T}_{t=1}{\var{\dif^t}} }\right)$.
\end{proposition}

