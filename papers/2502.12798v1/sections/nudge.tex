\section{Nudged and Adversarial Arrival}
\label{sec:nudge}
In this section, we address nudged arrival: We assume an exogenous arrival mechanism can influence the order in which agents arrive. Practically, this captures scenarios where the system sends push notifications or otherwise encourages some agents to arrive earlier or later. Our goal is to analyze the envy of arbitrary algorithms without changing the way they select arms. We stress that our analysis still assumes anonymous algorithms: The decision-making process is not affected by agent identities.

% \omer{USE THIS SOMEHOW
% In this section, we examine the envy dynamics of algorithms that \emph{ignore} agent identities \emph{during} the sessions. That is, \omer{we try to minimize envy by making high rewraded agents arrive ealier} the algorithm can affect the arrival order at the beginning of the round, but does not condition the decision on which arm to choose in each session on the arriving agent. \omer{this is good but not perfect}}
Subsection~\ref{subsec:nudged prot} introduces the nudged arrival protocol, $\sugord$, and the accompanying assumptions. Later,  Subsection~\ref{Envy Analysis} presents our main result of the section: An upper bound of $\env^T({\sugord})$ for a broad class of algorithms. Interestingly, we show that the bound depends on the instance parameters but not on the horizon $T$. Finally, we complete this section by adopting a complementary approach, where an adversary can pick the worst arrival order in terms of envy, and show that an $\Omega(T)$ regret is inevitable. For clarity, we remind the reader that for $q,i\in [N]$, $\ordv_t (q)=i$ implies that agent $i$ has arrive in the $q$'th session in round $t$ (similarly for $\ordv_t^{-1}(i)=q$). 
\subsection{Nudged Arrival Protocol}\label{subsec:nudged prot}
\begin{algorithm}[t]
\caption{Nudged Arrival}
\label{alg: sugg arr}
\SetAlgoLined
\LinesNumbered
\KwIn{horizon $T$, nudge parameter $\delta$} \label{line:input}
\For{round $t = 1$ to $T$}{ \label{line:for_loop} 
    let $\sigv_t$ be an $[N]\rightarrow [N]$ mapping such that 
    \[
    R^{t-1}_{\sigma_t(N)} \leq R^{t-1}_{\sigma_t(N-1)} \leq \dots \leq R^{t-1}_{\sigma_t(2)} \leq R^{t-1}_{\sigma_t(1)}.    
    \]
    \label{line:mapping}\\
    sample an arrival order $\ordv_t \sim \sugord(\sigma_t,\delta)$ \label{line:request}
}
\end{algorithm}
We describe the arrival protocol in Algorithm~\ref{alg: sugg arr}. It receives the horizon $T$ and a nudge parameter~$\delta$ as input, and interacts with any recommender algorithm through the horizon. In each round in the for loop of Line~\ref{line:for_loop}, we pick an \textit{ideal permutation} $\sigma_t$ that orders the agents according to their cumulative rewards.  The mapping $\sigma_t$ prioritizes agents according to their cumulative rewards in previous rounds, from the most rewarded one to the least rewarded one. In our notation, $\sigma_t : \{1,\dots,n\} \to [N]$, so that $\sigma_t(i)$ is the name of the agent in position $i$. Hence, $\sigma_t(1)$ is the agent with the highest cumulative reward so far, and $\sigma_t(N)$ is the agent with the lowest. While we assume the agents can be nudged toward this ordering $\sigma_t$, we do not claim that it is implemented or enforced in its exact form. Instead, in Line~\ref{line:request}, we sample an arrival order $\ordv$ from   $\sugord(\sigma_t,\delta)$, which is a distribution over permutations of $N$. Particularly, we assume that $\sugord$ satisfies the \emph{nudged arrival} property.

\begin{property}[Nudged Arrival]\label{prop:nudge}
Given a scalar \( \delta \in (0,1) \) and a mapping \( \sigv : [N] \rightarrow [N] \),  for every two agents \( i,j \) with \( \sigma^{-1}(i) < \sigma^{-1}(j) \) the distribution $\sugord(\sigv,\delta)$ satisfies
\begin{equation}\label{def: nudged} 
\Pr_{\ordv \sim \sugord(\sigma,\delta)}\left(\ordv^{-1}(i) < \ordv^{-1}(j)\right) \geq \frac{1+\delta}{2}. 
\end{equation}  
\end{property}
In other words, Algorithm~\ref{alg: sugg arr} introduces a structured form of randomness in the sequence of agent arrivals. Each round allows for the selection of a permutation $\sigma_t$, representing a preferred (but not guaranteed) ordering of agents. For any two agents $i$ and $j$, if the permutation prioritizes agent $i$'s arrival over agent $j$'s arrival, i.e., $\sigma_t^{-1}(i) < \sigma_t^{-1}(j)$, then agent $i$ is more likely to arrive before agent $j$. Particularly, the probability of agent $i$ preceding agent $j$ in the realized arrival order $\ordv_t$ is at least $\frac{1}{2} + \frac{\delta}{2}$, ensuring a consistent bias $\delta$ toward the preferred ordering. Property~\ref{prop:nudge} is inspired by several well-known models of stochastic ranking, like Mallows model~\cite{mallows1957non}, Plackett-Luce~\cite{marden1996analyzing} and also noisy comparison models~\cite{braverman2007noisy}. In \ifnum\Includeappendix=0{the appendix}\else{Section~\ref{appendix:nudge-models}}\fi, we describe how to derive $\delta$ from each of these models. By specifying Property~\ref{prop:nudge} rather than the full underlying ordering model, we preserve flexibility in how global orderings are derived.  
Next, we illustrate nudged arrival and the role it plays in envy dynamics.
\begin{example}[Nudged Arrival and Envy Dynamics]  \label{example: envy with sugg}
We reconsider the setting of Example~\ref{example 1}, with $K=2$ arms with  $\uni{0,1}$ rewards, $N=2$ agents, Algorithm~\ref{alguni}, but with the nudged arrival ${\sugord}$. To ease readability, we keep using $\env^T$ to denote $\env^T({\sugord})$, omitting the dependence on  ${\sugord}$. Suppose that after $t-1$ rounds, for some arbitrary $t \in [T]$, agent rewards satisfy $\env^{t-1} =\env^{t-1}_{1,2} = R^{t-1}_1 - R^{t-1}_2 > 1$, indicating that agent 2 envies agent 1.


In this scenario, the expected envy after round $t$ is given by:  
{  
\thinmuskip=2mu  
\medmuskip=3mu plus 2mu minus 3mu  
\thickmuskip=4mu plus 5mu minus 2mu  
\begin{align}\label{eq:env_update}  
\mathbb{E}[\env^t \mid \env^{t-1}_{1,2} > 1]  
&= \mathbb{E}\left[\left| \env^{t-1}_{1,2} + \Delta^t_{1,2} \right| \mid \env^{t-1}_{1,2} > 1 \right] = \env^{t-1}_{1,2} + \mathbb{E}\left[\Delta^t_{1,2} \mid\env^{t-1}_{1,2} > 1 \right]. 
\end{align}  
}  



The term $\mathbb{E}[\Delta^t_{1,2} \mid \env^{t-1}_{1,2} > 1 ]$ can be expressed as:  
{  
\thinmuskip=2mu  
\medmuskip=3mu plus 2mu minus 3mu  
\thickmuskip=4mu plus 5mu minus 2mu  
\begin{align}\label{eq:delta_expression}  
\E{\Delta^t_{1,2} \mid \env^{t-1}_{1,2} > 1} &= \E{r_{(2)}^t - r_{(1)}^t \mid \env^{t-1}_{1,2} > 1, \ordv_t = (2,1)}\prb{\ordv_t = (2,1) \mid \env^{t-1}_{1,2} > 1} \nonumber \\  
& \qquad + \E{r_{(1)}^t - r_{(2)}^t \mid \env^{t-1}_{1,2} > 1, \ordv_t  = (1,2)}\prb{\ordv_t = (1,2) \mid \env^{t-1}_{1,2} > 1} \nonumber\\
&= \frac{1}{8} \left[\prb{\ordv_t = (2,1) \mid \env^{t-1}_{1,2} > 1} - \prb{\ordv_t = (1,2) \mid \env^{t-1}_{1,2} > 1} \right],  
\end{align}  
}%
where we have used the fact that $\E{\rt{(2)} - \rt{(1)}} = \frac{1}{8}$ as Subsection~\ref{subsec:information} suggests and the fact that the rewards $r_{(i)}^t$ for $i\in \{1,2 \}$ are independent of the arriving agent's identity.

Under nudged arrival, the ideal permutation is $\sigma_t = (1,2)$, prioritizing agent $1$'s arrival over agent $2$'s arrival;therefore, the permutation $(1,2)$ is more likely than $(2,1)$, resulting in
\[
\prb{\ordv_t = (2,1) \mid \env^{t-1}_{1,2} > 1} - \prb{\ordv_t = (1,2) \mid \env^{t-1}_{1,2} > 1} \leq -\delta.
\]  
Thus, rewriting Equation~\eqref{eq:delta_expression}, we have $\E{\Delta^t_{1,2} \mid \env^{t-1}_{1,2} > 1}  \leq -\frac{\delta}{8}$. To conclude this example, we plug this result into Equation~\eqref{eq:env_update} and obtain
\[
\mathbb{E}[\env^t \mid \env^{t-1}_{1,2} > 1] \leq \env^{t-1}_{1,2}-\frac{\delta}{8},
\]
suggesting that the cumulative envy $\env^t$ is likely to decrease in round $t$ by a non-negligible value.
\end{example}
To be able to analyze envy dynamics and show that it cannot grow too much, we need to have some regularity assumptions on the way algorithms we analyze operate. Indeed, the envy reduction in Example~\ref{example: envy with sugg} relies heavily on the fact that $\E{\rt{(2)} - \rt{(1)}} = \frac{1}{8} > 0$. This inequality ensures that, in expectation, arriving second leads to a higher reward. While we aim to analyze any arbitrary algorithm, we need to ensure that the ordering $\sigma_t$ in Line~\ref{line:mapping} reduces envy in expectation.
The following natural assumption generalizes the behavior of Algorithm~\ref{alguni} in Example~\ref{example: envy with sugg}.\begin{assumption}\label{assumption: nudge alg ref}
In every round $t \in [T]$, the algorithm picks arms so that
\[
    \E{r_{(1)}^t} \leq \E{r_{(2)}^t}\leq \cdots \leq \E{r_{(N)}^t}.
\]
\end{assumption}
To satisfy this assumption,\footnote{In fact, all of our results holds for the much broader case where there exists a permutation $\sigma:N \rightarrow N$ and the algorithm picks arms so that 
$\E{r_{(\sigma(1))}^t} \leq \E{r_{(\sigma(2))}^t}\leq \cdots \leq \E{r_{(\sigma(N))}^t}$. In such a case, we would pick $\sigma_t$ in Line~\ref{line:mapping} of Algorithm~\ref{alg: sugg arr} so that $\left(R^{t-1}_{\sigma_t(\sigma(i))}\right)_i$ is an increasing series.} the algorithm at hand must have some information about the expected rewards; Bayesian information is a sufficient condition, although not necessary.


Furthermore, without loss of generality, we shall assume that $\prb{\Delta^t_{(N),(1)} \neq 0}>0$ in every round $t$. This is indeed without loss of generality, as Assumption~\ref{assumption: nudge alg ref} already guarantees that $\E{\Delta^t_{(N),(1)}}=\E{r^t_{(N)}-r^t_{(1)}} \geq 0 $, and any round in which $\prb{\Delta^t_{(N),(1)} \neq 0}=0$ does not affect envy and can be disregarded. Additionally, to simplify our analysis, we introduce the new notation $\tilde{\dif}$, denoting
\begin{equation}\label{eq def tdif}
\tdif = \min_{1\leq i <j \leq N, t\in [T]:  \prb{\Delta^t_{(j),(i)} \neq 0}>0 }\left\{ \E{\Delta^t_{(j),(i)} \mid \Delta^t_{(j),(i)} \neq 0} \right\}    
\end{equation}
The quantity $\tdif >0$ %, which is well defined as long as since we assume that $\prb{\Delta^t_{(N),(1)} \neq 0}>0$ for every $t$, 
represents a lower bound on our ability to decrease envy from round to round. Recall that Assumption~\ref{assumption: nudge alg ref} implies that $\E{\Delta^t_{(j),(i)}}=\E{r^t_{(j)}-r^t_{(i)}} \geq 0 $ for $j>i$. However, $\Delta^t_{(j),(i)}$ can take 0 sometimes,\footnote{For instance, if $N > K+1$, any explore-first algorithm would have $\Delta^t_{(N-1),(N)}=0$ as the algorithm will pick the same arm for both sessions.} which means there is no scope for nudged arrival to further reduce envy. As long as Assumption~\ref{assumption: nudge alg ref} holds and $ \prb{\Delta^t_{(j),(i)} \neq 0}>0$, we know that
\[
\E{\Delta^t_{(j),(i)} \mid \Delta^t_{(j),(i)} \neq 0}=\frac{ \E{\Delta^t_{(j),(i)}}}{\prb{\Delta^t_{(j),(i)} \neq 0}};
\] thus, we expect $\tdif$ to be significant. To illustrate, recall that in Example~\ref{example: envy with sugg} it holds that $\E{\Delta^t_{(2),(1)}}=\frac{1}{8}$, whereas $\E{\Delta^t_{(2),(1)} \mid \Delta^t_{(2),(1)} \neq 0}=\frac{1}{4}$.
\subsection{Envy Analysis}\label{Envy Analysis}
We are ready to present the main result of the section: Upper bounding the envy under nudged arrival.
\begin{theorem}\label{thm: sugg-envy}
    When executing any algorithm that satisfies Assumption~\ref{assumption: nudge alg ref} with nudged arrival, the expected envy is
    \[\E{\env^T({\sugord})}\leq (N-1)\left(2+\frac{128}{15\delta \tdif}\right) .\]
\end{theorem}
Notice that this upper bound does not depend on the horizon $T$. Intuitively, under nudged arrival, envy behaves like a random walk with a drift toward zero. Although each round may introduce a discrepancy (akin to a random fluctuation), the nudging mechanism consistently pushes the cumulative difference back toward zero. Furthermore, the bound is inversely proportional to $\delta$ and $\tdif$: As $\delta$ decreases, the nudging effect weakens and nudged arrival increasingly resembles uniform arrival. We suspect the terms in the bound are not tight; we discuss it in Section~\ref{sec:discussion}.
\begin{proof}[Proof of Theorem~\ref{thm: sugg-envy}]
Fix any arbitrary algorithm satisfying  Assumption~\ref{assumption: nudge alg ref} and any arbitrary~$t \in [T]$. The proof is outlined as follows:
\begin{enumerate}
    \item Step 1 introduces envy gaps as stochastic processes and the concept of envy excursions.
    \item Demonstrating that envy gaps are nontrivial to analyze, Step 2 presents a more friendly stochastic process that we prove to upper bound the envy gap almost surely.
    \item Step 3 leverages Property~\ref{prop:nudge} and concentration inequalities to upper bound large deviations of the friendly stochastic process.
    \item Lastly, Step 4 uses the tail formula and the concentration from Step 3 to bound to cumulative envy.
\end{enumerate}
\textbf{Step 1: Envy Gap and Excursion}
For every $t$, let $\sigma_t: [N] \rightarrow [N]$ be the ideal permutation from Line~\ref{line:mapping}, i.e.,  
\[
R^{t-1}_{\sigma_t(N)} \leq  R^{t-1}_{\sigma_t(N-1)} \leq \cdots \leq R^{t-1}_{\sigma_t(2)} \leq R^{t-1}_{\sigma_t(1)} .
\]
For every $i, 1\leq i \leq N-1$, we define the \emph{envy gap} $G_i^t = R^t_{\sigma_t(i)} - R^t_{\sigma_t(i+1)}$, representing the envy between the agent with the $i$-th highest reward and the agent with the $(i+1)$-th highest reward.
Consequently, we can define $\env^t$ using the envy gap sequence $(G^t_i)_i$ by
\begin{equation}\label{eq:jknbvfbjj}
\env^t=R^t_{\sigma_t(1)}-R^t_{\sigma_t(N)}=\sum_{i=1}^{N-1} R^t_{\sigma_t(i)}-R^t_{\sigma_t(i+1)} = \sum_{i=1}^{N-1} G_i^t.   
\end{equation}

Next, fix any arbitrary $i$ in the range. We continue by analyzing \emph{excursions} from low envy to high envy and showing they are relatively short, meaning that the expected envy $\E{G_i^t}$ is low. We define an excursion as a sequence of consecutive rounds during which the gap $G_i^\cdot$ exceeds $1$. 
Let $\underline{t} = \max{\left\{ \tau \mid 1 \leq \tau \leq t, G_i^{\tau} \leq 1 \right\}}$ denote last round $\tau$ before $t$ that $G_i^{\tau}$ was less than 1. Similarly,  let $\bar{t} = \min{\left\{ \tau \mid t\leq \tau \leq T, G_i^{\tau} \leq 1 \right\}}$ be the first round $\tau$ after $t$ where $G_i^{\tau}$ is less than 1. For the extreme case where $\bar{t}$ is undefined, we set $\bar{t} = T + 1$. Furthermore, let $D(t)$ denote $t$'s excursion, namely, the set of all the consecutive rounds $\tau$ that includes $t$ during which $G_i^\tau \geq 1$. Formally, $D(t) = \{\tau | \underline{t} < \tau < \bar{t}\}$. Notice that $D(t)$ is an empty set if and only if $G_i^t \leq 1$.

\textbf{Step 2: Auxiliary Stochastic Process}
The sequence $\{G_i^\tau\}_{\tau \in D(t)}$ is challenging to work with because the agents occupying the $i$-th and $(i+1)$-th highest reward position may change from round to round.  We refer to these changes as \emph{rank swaps}, which cause increments like  $G_i^{\tau+1} - G_i^\tau$ to lack a consistent structure. To address this complexity, we introduce the stochastic process $(M^\tau)_\tau$, which is easier to analyze.
%$G_i^\tau$ for $\tau \in D(t)$ is challenging to work with. To demonstrate, notice that the identities of the agent with the $i$-th highest reward and the agent with the $(i+1)$-th highest reward can change from round to round; thus, the increment $G_i^{\tau+1}-G_i^{\tau}$ has no clear structure. 
%To that end, we introduce the stochastic process $(M^\tau)_\tau$, which is easier to analyze. We define it as follows:
\begin{align*}
    M^\tau =
    \begin{cases}
        2 & \text{if } \tau = \underline{t}+1 \\
        M^{\tau-1}+r^\tau_{(i)}-r^\tau_{(i+1)} & \text{else}
        \end{cases}.
\end{align*}
Unlike $G_i^{\tau+1}-G_i^{\tau}$, the increments $M^{\tau+1}-M^{\tau} = r^\tau_{(i)}-r^\tau_{(i+1)}$ are more straightforward and negative in expectation due to Assumption~\ref{assumption: nudge alg ref}. The next proposition demonstrates that $M^\tau_i$ can assist when analyzing $G^\tau_i$.
\begin{proposition}\label{prop G less than M}
For every $\tau \in D(t)$, it holds that $G^\tau_i \leq M^\tau_i$ almost surely.
\end{proposition}
The proof of Proposition~\ref{prop G less than M} appears in \ifapp{Section~\ref{appendix:nudge}}{the appendix}. The main argument enabling this statement is that rank swaps can only decrease the increments $G_i^{\tau+1}-G_i^{\tau}$, but do not affect the increments $M^{\tau+1}-M^{\tau}$.

\textbf{Step 3: Concentration}
The recursive definition of $M^\tau$ implies that for every $\tau \in D(t)$, $M^\tau = 2+ \sum_{n= \underline{t}+2 }^{\tau} r^n_{(i)}-r^n_{(i+1)}$. The next proposition bounds large deviations of $M^\tau$.
\begin{proposition}\label{prop: sugg-m concentration}
    For any $n \in \mathbb{N}$ and $\tau \in D(t)$, it holds that
    \begin{align*}
    \prb{M^\tau > n}\leq \exp\left\{-\frac{(n-2)(\delta \tdif)}{8}\right\}.
    \end{align*}
\end{proposition}
%Intuitively, the bound in Proposition~\ref{prop: sugg-m concentration}  
The proof of Proposition~\ref{prop: sugg-m concentration} appears in \ifapp{Section~\ref{appendix:nudge}}{the appendix}. It leverages the Azuma-Hoeffding inequality and several algebraic tricks to obtain the bound. As we expect, as $\delta$ and $\tdif$ increase, the right-hand side becomes more significant. Alternatively, if the term $\delta \tdif$ approach zero, this bound become irrelevant as $\exp\{0\}=1$.

\textbf{Step 4: Tail Sum}
To finalize the proof, we use the tail-sum formula. Since $G_i^t$ is non-negative,
\begin{align*}
\E{G_i^t} &=
    \int_{x=0}^{\infty} \prb{G_i^t > x}dx \leq
    \sum_{n=0}^{\infty}{\int_{x=n}^{n+1} \prb{G_i^t > n} \,dx}=
    \sum_{n=0}^{\infty}{\prb{G_i^t > n}}.
\end{align*}    
Next, by applying Propositions~\ref{prop G less than M} and~\ref{prop: sugg-m concentration}, we conclude that
\begin{align}\label{eq:tail formula}
    \sum_{n=0}^{\infty}{\prb{G_i^t > n}}& \stackrel{\textnormal{Prop. \ref{prop G less than M}}}{\leq} \sum_{n=0}^{\infty}{\prb{M_i^t > n}} \leq 2+ \sum_{n=2}^{\infty}{\prb{M_i^t > n}}   \stackrel{\textnormal{Prop. \ref{prop: sugg-m concentration}}}{\leq} 2+ \sum_{n=2}^{\infty}\exp{\left\{-\frac{(n-2)\delta \tdif}{8} \right\}}\nonumber \\
    &= 2+ \sum_{n=0}^{\infty}\left(e^{-\frac{\delta \tdif}{8}} \right)^n = 2+\frac{1}{1-\exp{\left(\frac{-\delta \tdif}{8} \right)}} \stackrel{e^{-x}\leq 1-x+\frac{x^2}{2}}{\leq}
    2+\frac{1}{\frac{\delta \tdif}{8}-\frac{(\delta \tdif)^2}{128}}
 \nonumber \\
    & \stackrel{\delta \tdif \leq 1}{\leq} 2+\frac{1}{\frac{\delta \tdif}{8}-\frac{\delta \tdif}{128}} = 2+\frac{128}{15\delta \tdif}.
\end{align}
Ultimately, recall that Inequality~\eqref{eq:tail formula} applies to $\E{G_i^t}$ for every $i$; therefore, Equation~\eqref{eq:jknbvfbjj} ensures that $\E{\env^t}=\E{\sum_{i=1}^{N-1} G_i^t} \leq (N-1)\left(2+\frac{128}{15\delta \tdif}\right) $. This concludes the proof of Theorem~\ref{thm: sugg-envy}.
\end{proof}

{\color{green}




}
\subsection{Adversarial Arrival}
\label{sec: advord}
We end this section by focusing on the adversarial arrival order $\advord$. Intuitively, an adversary seeking to maximize envy would reverse the ideal permutation, placing agents in descending order of their current cumulative rewards. That is, sets the order $\ordv_t$ in round $t$ such that
\[
R^{t-1}_{\ordv_t(N)} \geq R^{t-1}_{\ordv_t(N-1)} \geq \dots \geq R^{t-1}_{\ordv_t(2)} \geq R^{t-1}_{\ordv_t(1)}.
\]
Indeed, it is easy to see that:
\begin{proposition}\label{thm: adv-envy}
When executing any algorithm that satisfies Assumption~\ref{assumption: nudge alg ref} with $\advord$, the expected envy is
    \[\E{\env^T ({\advord})}\geq \tilde{\dif}T.\]
\end{proposition}
\begin{proof}[Proof of Proposition~\ref{thm: adv-envy}]
Assume that the adversary picks agent 1 to be the first and agent $N$ to be the last, i.e., $\ordv_t(1)=1$ and $\ordv_t(N)=N$ for all $t\in [T]$. In such a case,
\begin{align*}
\E{\env^T({\advord})} &=
\E{\max_{i,j\in [N]}{\left\{ \sum^T_{t=1}{\adift{i}{j}} \right\} }} \geq \E{\sum^T_{t=1}{\dif_{N,1}^t}} = \E{\sum^T_{t=1}{\dif^t_{(N),(1)}}}  \geq T\E{\min_{1 \leq t \leq T}{\left\{\sdift{N}{1} \right\} }} \\
&\geq \tdif T.    
\end{align*}
This concludes the proof of Proposition~\ref{thm: adv-envy}.
\end{proof}

