\section{Socially Optimal Algorithms}\label{appendix:sociallyopt}
In this section, we consider the task of devising socially optimal algorithms. First, in Subsection~\ref{subsec:sw N=2}, we address the two-agent case. Then, in Subsection~\ref{subsec:sopt for N>2}, we develop algorithms for the $N>2$ case.

To ease readability, we make the assumption that reward distributions are stationary, i.e., $\mathcal{D}^1_i, \mathcal{D}^2_i, \dots \mathcal{D}^T_i$ are identical for every arm $a_i \in A$. Consequently,  $X^1_i, X^2_i, \dots X^T_i$ are i.i.d. and we drop the superscript. We stress that our results can also be easily extended to the non-stationary case. Furthermore, we let $\mu_i = \mathbb{E}[X_i^t]$ denote the expected reward of arm $a_i$.

\subsection{Social Welfare for $N=2$}
\label{subsec:sw N=2}

\begin{algorithm}[t]
\caption{Two-agents Socially Optimal Algorithm ($\sopt$)}
\label{alg: sopt}
\LinesNumbered
\DontPrintSemicolon 
\KwIn{horizon $T$, reward distributions $\mathcal{D}_1, \ldots, \mathcal{D}_K$}
Compute $(\is, \js)$ such that
\label{alg: sopt compute}
\begin{align}\label{eq: picking i,j star}
(\is, \js) \in \argmax_{(i,j) \in A^2} \left\{ \mu_i  + \prb{X_i < \mu_j}\mu_j+\prb{X_i \geq \mu_j}\E{X_i \mid X_i \geq \mu_j}  \right\}.
\end{align}\\
\For{round $t = 1$ to $T$}{\label{alg: sopt for}
    Pull $a_{\is}$ \label{alg: sopt 3}\\
    \lIf{$x^t_\is \geq \mu_\js$}{
        Pull $a_{\is}$ \label{alg: sopt if}
    }
    \lElse{
        Pull $a_{\js}$ \label{alg: sopt else}
    }
}
\end{algorithm}
In this section, we design $\sopt$, a socially optimal algorithm for the two-agent case, which we implement in Algorithm~\ref{alg: sopt}. $\sopt$ has Bayesian information, as it receives the reward distributions as inputs. In Line~\ref{alg: sopt compute}, it selects two arms $a_{\is}, a_{\js}$ according to Equation~\eqref{eq: picking i,j star}. As we prove formally, this selection maximizes $\E{{r^t_{(1)}}+{r^t_{(2)}}}$ for any $t$. Arms $a_{\is}, a_{\js}$ are the only arms the algorithm pulls during its execution.

In Line~\ref{alg: sopt for}, $\sopt$ begins interacting with the agents for $T$ rounds.
Notice $\sopt$ does not address the arrival of the agents at all: While the arrival function is crucial for measuring envy, it does not influence the SW.

$\sopt$ pulls arm $a_{\is}$ for the agent that arrives in the first session, 
and observes the realized reward $x^t_{\is}$ (Line~\ref{alg: sopt 3}). In Lines~\ref{alg: sopt if}--\ref{alg: sopt else}, $\sopt$ decides whether to pull the same arm in the second session or $a_{\js}$ instead, based on the realized $x^t_{\is}$:
If $x^t_{\is} \geq \mu_{\js}$ (Line~\ref{alg: sopt if}), i.e., we expect the reward of $a_{\js}$ to be less than or equal to the observed reward, $\sopt$ pulls $a_{\is}$ again.
Otherwise (Line~\ref{alg: sopt else}), we expect the reward of $a_{\js}$  to be greater than that of $a_{\is}$, so the algorithm pulls arm $a_{\js}$. Next, we prove the optimality of $\sopt$.

Before we prove the optimality of $\sopt$, present two propositions that assist in understanding its crux.
\begin{proposition}\label{prop:is}
$\sopt$ does not always pull the arm with the highest expected reward in the first session. That is, $\is$ is not necessarily $\argmax_{i\in \{1, \ldots, K \}}{\mu_i}$.
\end{proposition}

\begin{proof}[Proof of Proposition~\ref{prop:is}]
We show an example satisfying $\is \neq \argmax_{i\in \{1, \ldots, K \}}{\mu_i}$.
Consider $K=2$ arms, with the following distributions.
\begin{align*}
    X_1 \sim
    \begin{cases}
    0.75 & w.p. \ \  0.5 \\
    0.55 & w.p.\  \ 0.5
    \end{cases},
    X_2 \sim
    \begin{cases}
    1 & w.p. \ \  0.6 \\
    0 & w.p.\  \ 0.4
    \end{cases}.
\end{align*}
Observe that $\mu_1 = 0.65 > \mu_2=0.6$. Yet,
\begin{align*}
\mu_1  +\mu_2 \prb{X_1 < \mu_2}+\E{X_1 \mid X_1 \geq \mu_2}  \prb{X_1 \geq \mu_2}=
0.65  + 0.6 \cdot 0.5+ 0.75 \cdot 0.5 = 1.325,
\end{align*}
whereas,
\begin{align*}
\mu_2  +\mu_1 \prb{X_2 < \mu_1}+\E{X_2 \mid X_2 \geq \mu_1}  \prb{X_2 \geq \mu_1}=
0.6  + 0.65 \cdot 0.4 + 1 \cdot 0.6 = 1.46.
\end{align*}
Consequently, $\sopt$ chooses $(\is,\js) = (2,1)$.
This concludes the proof of Proposition~\ref{prop:is}.
\end{proof}
To get the intuition behind Proposition~\ref{prop:is}, recall that with the help of the information exposed by the first agent, the algorithm can make a better decision in the second session.
Therefore, we must find the perfect trade-off between the first agent's welfare (exploitation) and the leverage of the information they provide (exploration).
In contrast, we expect nothing but pure exploitation in the second session.
\begin{proposition}\label{prop:js}
Arm $a_\js$ has the highest expected reward among the remaining arms. I.e., $\js = \argmax_{j\in K\setminus\{\is\}}{\mu_j}$.
\end{proposition}
In other words, in the second session, the algorithm makes the choice that is the most rewarding.
\begin{proof}[Proof of Proposition~\ref{prop:js}]
Let $(\is, \js)$ be a pair of arms that maximizes Equation~\eqref{eq: picking i,j star}.
Suppose, for the sake of contradiction, that there exists ${j'}\notin \{\is,\js\}$ such that $\mu_{j'} > \mu_{\js}$;
thus, $\E{\max{\{X_{\is}, \mu_{{j'}}\}}} \geq \E{\max{\{X_{\is}, \mu_{\js}\}}}$.
I.e.,
\begin{align*}
    \E{X_{\is}} + \E{\max{\{X_{\is}, \mu_{{j'}}\}}} \geq \E{X_{\is}} + \E{\max{\{X_{\is}, \mu_{\js}\}}},
\end{align*}
where equality can occur if and only if $\prb{\max{\{X_{\is}, \mu_{\js}\}} = \mu_{\js}} = 0$.
In this case, the algorithm always pulls arm $a_{\is}$ in the second session, and arm $a_{\js}$ is irrelevant.
Hence, we assume strong inequality.
Notice that the left-hand side is exactly Equation~\eqref{eq: picking i,j star} with $(i,j) = (\is, {j'})$, and the right-hand side is exactly Equation~\eqref{eq: picking i,j star} with $(i,j) = (\is, \js)$.
Hence, we have obtained a contradiction to $(\is, \js)$ being a pair that maximizes Equation~\eqref{eq: picking i,j star}. This concludes the proof of Proposition~\ref{prop:js}.
\end{proof}
We are ready to prove the optimality of $\sopt$.
\begin{theorem}\label{thm:sopt is opt}
Fix any instance with $N=2$ and arbitrary reward distributions. For any algorithm $ALG$ with Bayesian information, it holds that
\[\sw\left(ALG\right) \leq \sw\left(\sopt\right).\]    
\end{theorem}
\begin{proof}[Proof of Theorem~\ref{thm:sopt is opt}]
Fix any instance with $N=2$ and arbitrary reward distributions. The social welfare of an algorithm $ALG$ over $T$ rounds is
\[
\sw(ALG) 
=\E{ \sum^2_{i=1}{R_i^T}}= \E{\sum_{t=1}^T \bigl(r^t_{(1)} + r^t_{(2)}\bigr)}
= T \, \E{r^1_{(1)} + r^1_{(2)}},
\]
where the last equality uses the fact that the rewards in each round are independent.

Thus, to show that $\sopt$ maximizes social welfare, it suffices to show that no algorithm can exceed its expected reward \emph{within a single round}. By the revelation principle~\cite{peters2001common}, there is an optimal \emph{threshold} algorithm: After observing the first-session reward, it decides in the second session by comparing the observed reward against the expected reward of any other arm.

Since $\sopt$ enumerates all pairs of arms $(i,j)$ in Equation~\eqref{eq: picking i,j star} and applies a greedy rule for the second session (choosing either the same arm $i$ or the other arm $j$ based on which is expected to yield a higher reward), it achieves the maximum expected reward per round. Multiplying by $T$ completes the proof.
\end{proof}

\subsection{Socially Optimal Algorithm for $N >2$}\label{subsec:sopt for N>2}
Next, we consider the problem of finding a socially optimal algorithm for $N >2$ agents. First, we present a socially optimal algorithm for the special case of Bernoulli rewards.

\begin{proposition}\label{prop:bernoulli sw optimal}
    Assume that $X_i \sim \ber{p_i}$ for every $i \in [K]$. An algorithm that pulls arms in descending order of $p_i$ until it realizes a reward of 1 is socially optimal for any number of agents $N$.
\end{proposition}
\begin{proof}[Proof of Proposition~\ref{prop:bernoulli sw optimal}]
    Since random algorithms are just a distribution over deterministic algorithms, we know there is an optimal algorithm that is deterministic. Additionally, as in the proof of Theorem~\ref{thm:sopt is opt}, it suffices to focus on a single and arbitrary round $t$.
    
    Notice that after we observe an arm such that $x^t_i =1$, it is optimal to pull it for all remaining agents.
    Similarly,  if we observe arm $a_i$ with $x^t_i =0$ it is strictly sub-optimal to pull it again, unless all arms yielded a reward of 0.
    Thus, the only thing left to prove is the optimality of the order in which the algorithm pulls arms.

    In this special case, we can use a reduction to a Pandora's Box (PB) problem~\cite{weitzman1978optimal}. We first describe the reduction, then characterize the optimal solution for the PB instance, and finally show the equivalence. 
       
    Let the PB instance include $K$ Bernoulli arms with success probability $p_i$ for every arm $i$ and costs $c_i = \frac{1}{B}$, where $B$ is a constant such that $B > \max_{i \in [K]} \frac{1}{p_i}$. Due to Weitzman's seminal result~\cite{weitzman1978optimal}, there exist indices $(\theta_i)_{i=1}^K$ such that the optimal sequence is descending in the index. Each index  $\theta_i$ is the solution to $\E{\max\left\{X_i - \theta_i,0 \right\}}  =c_i =\frac{1}{B}$. Thus, $\theta_i = 1- \frac{1}{B\cdot p_i}$. The optimal solution maximizes $\E{1-\frac{S}{B}}$, where $S$ is a r.v. that counts the number of useless arms pulled (arms with a realized reward of 0). Note that $S$ depends solely on the pulling order.

    Similarly, for our original problem, maximizing the social welfare amounts to maximizing $\E{N-S}$. Since $S$ is distributed identically in both problems, the optimal order for PB is optimal for the original problem as well.
\end{proof}
    % satisfy this condition;
    % hence, the index of each arm is $1- \frac{1}{p_i}$. 
    % Any optimal algorithm for the PB instance maximizes the reward minus the sum of costs; thus, it orders the arms to find a positive reward as quickly as possible. Using the same order ensures that we maximize the social welfare in our problem. 
    % Recall that in the PB instance,  we need to find for each arm $a_i$ an index $\theta_i$ that satisfies $\E{\max\left\{X_i - \theta_i,0 \right\}}  =c_i =1$.
    % It is easy to see that $\theta_i = 1- \frac{1}{p_i}$ satisfy this condition;
    % hence, the index of each arm is $1- \frac{1}{p_i}$. 
    % the optimal sequence for the PB instance is descending in the index.

% \omer{USE THE BELOW AS THE BASIS FOR THE DYNAMIC PROGRAMMING}
% Fix an arbitrary round $t$, and assume that all the rewards are supported in a finite set $V$. We now describe a dynamic programming procedure that finds the optimal algorithm in $O(\abs{V} N2^K)$. Let $B$ be a subset of arms $B \subseteq A$, $n$ denote a number of agents $n\in \{0,1,\dots,N$, and $v$ denote an arbitrary reward $v \in V$. We define the following function $f$:
% \begin{equation}\label{eq:dp f}
% f(n,B,v) =  \max \left\{v\cdot n, \max_{a\in B} \mathbb E\left[X_a + f\left(n-1,B\setminus \{a\}, \max\{v,X_a\}\right) \right] \right\}    
% \end{equation}

Next, we move beyond Bernoulli rewards. Fix an arbitrary round $t$, and assume that all rewards are supported in a finite set $V$ (we later explain how to relax this assumption). We now describe a dynamic programming procedure that finds the optimal algorithm with a computational complexity of $O(\abs{V} \cdot N \cdot 2^K)$. 

We consider the following parameters: $B \subseteq A$, representing a subset of available arms; $n \in \{0,1,\dots,N\}$, denoting the number of remaining agents; and $v \in V$, an arbitrary current reward that models the maximal reward of all observed arms in $A \setminus B$.


We define the function $f(n, B, v)$, representing the maximum expected social welfare achievable given the parameters $(n,B,v)$. To apply this dynamic programming approach, we first establish the base cases for the function $f$:
\begin{itemize}
    \item \textbf{No agents remaining ($n = 0$):} If there are no agents left to assign rewards, the maximum achievable reward is zero regardless of the subset of arms $B$ and the current reward $v$. Formally, 
    \[
    f(0, B, v) = 0 \quad \forall \, B \subseteq A, \, v \in V.
    \]
    
    \item \textbf{No available arms ($B = \emptyset$):} If there are no arms left to pull, the only option is to assign the current reward $v$ to all remaining agents. Thus, the maximum reward in this scenario is the product of $v$ and the number of remaining agents $n$. Formally, 
    \[
    f(n, \emptyset, v) = v \cdot n \quad \forall \, n \in \{0,1,\dots,N\}, \, v \in V.
    \]
\end{itemize}

We move on to the recursive definition of $f$:
\begin{equation}\label{eq:dp_f}
f(n, B, v) = \max \left\{ v \cdot n, \max_{a \in B} \mathbb{E}\left[ X_a^t + f\left(n - 1, B \setminus \{a\}, \max\{v, X_a^t\} \right) \right] \right\}
\end{equation}
The recursive relation in Equation~\eqref{eq:dp_f} considers two scenarios at each step:
\begin{enumerate}
    \item \textbf{Assigning the Current Reward ($v \cdot n$):} In this scenario, the algorithm assigns the current reward $v$ to all remaining $n$ agents without pulling any additional arms.
    
    \item \textbf{Pulling an Arm ($\max_{a \in B} \mathbb{E}\left[ X_a^t + f\left(n - 1, B \setminus \{a\}, \max\{v, X_a^t\} \right) \right]$):} Here, the algorithm selects an arm $a \in B$, observes the stochastic reward $X_a^t$, and then recursively assigns rewards to the remaining $n - 1$ agents. The subset of available arms is updated to $B \setminus \{a\}$, and the current reward is updated to $\max\{v, X_a^t\}$.
\end{enumerate}

The optimal value for round $t$ is obtained by evaluating the function $f$ at the initial conditions where all agents and arms are available, and the starting reward is zero:
\[
f(n = N, B = A, v = 0).
\]
This value represents the maximum expected social welfare achievable by the optimal algorithm for round $t$. 

\begin{theorem}\label{thm:optimality_runtime}
    The dynamic programming procedure defined by the function $f(n, B, v)$ in Equation~\eqref{eq:dp_f} correctly computes the maximum expected total social welfare achievable for round $t$. Furthermore, the procedure operates with a runtime complexity of $O(\abs{V} \cdot N\cdot K\cdot 2^K)$.
\end{theorem}

\begin{proof}
    The optimality of the procedure follows from standard dynamic programming arguments, relying on the fact that the overall problem can be constructed from optimal solutions to its subproblems. The function $f(n, B, v)$ is designed to represent the maximum expected total social welfare achievable given the state $(n, B, v)$. By taking the maximum of the two options (recall Equation~\eqref{eq:dp_f}, $f(n, B, v)$ ensures that the optimal decision is made at each state, thereby maximizing the expected total social welfare.

To determine the runtime complexity, we analyze the number of possible states and the computation required for each state. The function $f(n, B, v)$ is parameterized by the number of remaining agents $n$, which can take $N+1$ possible values; the subset of available arms $B$, which has $2^K$ possible subsets; and the current reward $v$, which can take $\abs{V}$ possible values. Therefore, the total number of states is $O(\abs{V} \cdot N \cdot 2^K)$. For each state $(n, B, v)$, the dynamic programming procedure performs a constant-time computation to calculate $v \cdot n$ and iterates over all arms in $B$, performing constant-time computations for each arm. Given that there are up to $K$ arms in $B$, the computation per state is $O(K)$. Consequently, the overall runtime complexity is $O(\abs{V} \cdot N \cdot K \cdot 2^K )$. 
\end{proof}


Finally, we can relax the finite-support assumption. In scenarios where rewards are arbitrary within the continuous interval $[0,1]$, we discretize the reward space by selecting a finite set $V$ that approximates the continuous range of rewards. By choosing an appropriate granularity for the discretization, we can control the trade-off between the accuracy of the approximation and the computational complexity of the algorithm. While a finer discretization yields a closer approximation to the true optimal solution, it simultaneously increases the size of the state space, thereby enhancing the computational burden. Conversely, a coarser discretization reduces computational requirements at the expense of approximation precision. As this approach is standard, we omit the details. 


Unfortunately, the procedure above is inefficient in the number of arms $K$. We leave the tasks of finding an optimal algorithm, proving hardness, and finding approximately optimal algorithms as open problems.
%, i.e., an $ALG'\in \mathcal A$ with runtime $poly\left(\frac{1}{\epsilon} \right)$ such that $\E{SW(ALG')} \geq \max_{ALG\in \mathcal A} \E{SW(ALG)} - \epsilon$, 