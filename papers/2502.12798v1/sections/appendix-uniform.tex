\section{Omitted Proofs from Section~\ref{sec:uniform}}\label{appendix:uni}
\subsection{Upper Bound}
\begin{proof}[Proof of Proposition~\ref{prop:envy is good SG}]
First, we note that the envy is a martingale.
\begin{observation}\label{envy is martingale}
For every $i,j\in[N]$, the sequence $\left(\env_{i,j}^t \right)_{t=1}^T$ is a martingale. 
\end{observation}
Furthermore, since $\dift$ is symmetric as Remark~\ref{remark: symmetric dif} hints, we can use the following result to connect its tail behavior and its variance. 
\begin{proposition}\label{thm: symmetric bounded sg}
Let $Y$ be a bounded random variable symmetric around $0$, i.e., for any $y \in \mathbb R$ it holds that $\prb{Y \geq y} = \prb{Y \leq -y}$. Then, $Y$ is $\sqrt{\var{Y}}$-subgaussian.
\end{proposition}
We prove Proposition~\ref{thm: symmetric bounded sg} after the end of this proof. Proposition~\ref{thm: symmetric bounded sg} suggests that $\dift$ is $\sqrt{\var{\dift}}$-subgaussian. Ultimately, Lemma~\ref{sg of sup of martingale} analyzes the subgaussianity parameter of the maximum of the martingale $(\env_{i,j}^t )_t$.
\begin{lemma}\label{sg of sup of martingale}
    Let $M^1, M^2,\dots M^T$ be a martingale with increments $Y^1,Y^2,\dots Y^T$, such that $Y^t \mid M^{t-1}$ is $\sigma_t$-SG. Then $\max_t M^t$ is $\left(\sqrt{\sum_{t=1}^T \sigma_t^2}\right)$-SG.
\end{lemma}
Lemma~\ref{sg of sup of martingale} suggests that $\max_{1\leq t\leq T} \env_{i,j}^t $ is 
$\left(\sqrt{\sum_{l=1}^t \var{\dift}}\right)$-SG, thereby concluding the proof of  Proposition~\ref{prop:envy is good SG}.
\end{proof}


\begin{proof}[Proof of Observation~\ref{envy is martingale}]
Recall that $\env_{i,j}^t = \sum^{t}_{l=1}{\Del{l}{i,j}}$. Since the order of selection at time $t$ is independent of $\ordr_t$, it holds that $\E{\env_{i,j}^{t+1}\mid \env_{i,j}^t} =\env_{i,j}^t$. Moreover, since $\E{\abs{\env_{i,j}^t}} \leq  T < \infty$, the stochastic process $\left(\env_{i,j}^t \right)_{t=1}^T$ is a martingale. 
\end{proof}

\begin{proof}[Proposition \ref{thm: symmetric bounded sg}]
    We begin by examining the Taylor polynomial of the function $f(z)=e^z$ around $0$.
    \[
    e^z=  1 + z + \frac{z^2}{2!} + \frac{z^3}{3!} e^{\xi_z},
    \]
    where the last term is the  Lagrange form of the remainder for some $\xi_z \in [-\abs{z}, \abs{z}]$.
    Using this expansion with $z=\lambda y$ and $\xi_{\lambda y} \in [-\abs{\lambda y}, \abs{\lambda y}]$ gets
    \[
    e^{\lambda y} = 1 + \lambda y + \frac{(\lambda y)^2}{2!} + \frac{(\lambda y)^3}{3!} e^{\xi_{\lambda y}}.
    \]
    If $-a<y<a$ then,
    \[
    e^{\lambda y} = 1 + \lambda y + \frac{(\lambda y)^2}{2!} + \frac{(\lambda y)^3}{3!} e^{\abs{\lambda a}}.
    \]
    Thus,
    \[
    \E{e^{\lambda Y}} \leq \E{1 + \lambda Y + \frac{(\lambda Y)^2}{2} + \frac{(\lambda Y)^3}{3} e^{|\lambda a|}} = 
    1 + \lambda \E{Y} + \frac{\lambda^2}{2}\E{Y^2} +\frac{\lambda^3 e^{|\lambda a|}}{3!} \E{Y^3}.
    \]
    Recall that $Y$ is symmetric around $0$ and hence $\E{Y} = \E{Y^3} = 0$, $\var{Y} = \E{Y^2}$.
    Combining these observations with the above we get
    \[
    \E{e^{\lambda Y}} \leq 1 + \frac{\lambda^2}{2}\var{Y} \leq e^{\frac{\lambda^2 \var{Y}}{2}},
    \]
    where the last inequality is due to that $1+z \leq e^z$ for all $z$.
    That sums up the proof of Lemma~\ref{thm: symmetric bounded sg}.
\end{proof}




\begin{proof}[Proof of Lemma~\ref{sg of sup of martingale}]
We begin by proving that $M^t$ is $\left(\sqrt{\sum_{l=1}^t \sigma_l^2}\right)$-SG, and then address $\max_t M^t$


The base case is $t=1$, where we have $M^1=Y^1$ and $Y^1$ is $\sigma_1$-SG by definition. Next, assume that the statement holds for $t=k-1$. It holds that 
\begin{align*}
\E{e^{\lambda M^{k}}}&=\E{\E{e^{\lambda M^{k}}\mid M^{k-1}}}=\E{\E{e^{\lambda (M^{k-1}+Y^k)}\mid M^{k-1}}} = \E{e^{\lambda M^{k-1}} \E{e^{\lambda Y^k}\mid M^{k-1}}}\\
& \leq \E{e^{\lambda M^{k-1}} e^{\frac{\lambda^2 \sigma_k^2}{2}}} \leq e^{\frac{\lambda^2 \sum_{l=1}^{k-1} \sigma_l^2}{2} }e^{\frac{\lambda^2 \sigma_k^2}{2}} = e^{\frac{\lambda^2 \sum_{l=1}^{k} \sigma_l^2}{2} },
\end{align*}
where we used total expectation, the fact that $Y^k\mid M^{k-1}$ is $\sigma_k$-SG and the inductive assumption.

Next, let $\tau$ denote the r.v. for which $M^\tau = \max_t M^t$. It holds that
\begin{align*}
    \E{e^{\lambda \max_t M^t}} &= \E{e^{\lambda M^\tau}} = \E{\E{e^{\lambda M^\tau} \mid \tau}} \leq \E{\E{e^{\lambda M^\tau} \mid \tau}} \leq \E{e^{\frac{\lambda^2 \sum_{l=1}^{\tau} \sigma_l^2}{2} }} \leq e^{\frac{\lambda^2 \sum_{l=1}^{T} \sigma_l^2}{2} },
\end{align*}
where the second to last step follows from the fact that $M^t$ is $\left(\sqrt{\sum_{l=1}^t \sigma_l^2}\right)$-SG. This completes the proof of Lemma~\ref{sg of sup of martingale}.
\end{proof}

\begin{proof}[Proof of Claim~\ref{claim: sg max}]
Fix $a \in \mathbb{R}$ and denote $Y_{\textnormal{max}} = \max_{i \in [n]}{\{Y_i\}}$ for convenience.
We begin by examining $e^{a\E{Y_{\textnormal{max}} }}$.
Using Jensenâ€™s inequality, we have that
\begin{align}\label{eq:asdfrsdgvbsdfg}
e^{a\E{Y_{\textnormal{max}} }} \leq \E{e^{a Y_{\textnormal{max}}}}= \E{\max_{i \in [n]}{\left\{ e^{a Y_i } \right\}}} \leq
\sum_{i=1}^{n}{\E{e^{a Y_i}}} \leq \sum_{i=1}^{n}{ e^{\frac{a^2\sigma^2}{2}} } = ne^{\frac{a^2\sigma^2}{2}}.    
\end{align}
Taking $\ln$ on both sides of Inequality~\eqref{eq:asdfrsdgvbsdfg},
\[
a\E{Y_{\textnormal{max}}}\leq \ln(n) + \frac{a^2\sigma^2}{2}.
\]
Diving by $a$, we obtain
\begin{align}\label{eq:dasolgbdf}
\E{Y_{\textnormal{max}}}\leq \frac{\ln(n)}{a} + \frac{a\sigma^2}{2}.
\end{align}
Inequality~\eqref{eq:dasolgbdf} holds for all $a\in \mathbb{R}$ and specifically for the minimizer of $\frac{\ln{(n)}}{a}\!+\!\frac{a\sigma^2}{2}$, which is $a=\frac{\sqrt{2 \ln{(n)}}}{\sigma}$.
We complete the proof of the claim by substituting $a$ with this value in Inequality~\eqref{eq:dasolgbdf}.
\end{proof}
\begin{proof}[Proof of Proposition~\ref{thm: threshold var}]
    First, since $\dift \leq 1$ almost surely, $\var{\dift} \leq 1$ holds trivially. For the more challenging expression, the proof relies on the fact that when executing an explore-first algorithm, after at most $K$ sessions of any round, all remaining $N-K$ agents must receive the same reward. Hence, at the end of every round $t$, we can find at least $\binom{N-K+1}{2}$ pairs of agents $i,j$ that satisfy $\dif^t_{i,j} = 0$. In other words, for any $i,j$, it holds that 
    \[
    \prb{{\dif^t_{i,j}}=0}\geq \frac{\binom{N-K+1}{2}}{\binom{N}{2}},
    \]
    where the randomness is taken over the stochasticity of the rewards and the arrival order. From that,
    \begin{align*}
        \var{\dift} & = \E{{\dift}^2} - \E{\dift}^2 = \E{{\dift}^2 \mid {\dift}^2 \neq 0}\cdot \prb{{\dift}^2 \neq 0} - \E{\dift}^2
        \nonumber \\&\leq
        1\cdot \left( 1 - \frac{\binom{N-K+1}{2}}{\binom{N}{2}} \right) - 0 =
        1 - \frac{ \left( N-K+1 \right)\left( N-K \right) }{N\left( N-1 \right)}.
    \end{align*}
    With the help of some algebraic operations, we can simplify the expression.
    \begin{align*}
        \var{\dift} & \leq
        1 - \frac{ \left( N-K+1 \right)\left( N-K \right) }{N\left( N-1 \right)} =
        \frac{ N^2 - N - \left( N^2 - 2NK + K^2 + N - K \right) }{ N\left( N-1 \right)} 
        \\&=
        \frac{2NK -2N + K -K^2 }{ N\left( N-1 \right)} =
        \frac{2N(K-1) - K(K-1) }{N\left( N-1 \right)} =
        \frac{(2N - K)(K-1) }{N\left( N-1 \right)}.
    \end{align*}
    This concludes the proof of Proposition~\ref{thm: threshold var}.
\end{proof}
\begin{proof}[Proof of Corollary~\ref{thm: sqrt TK N}]
The corollary holds since 
\begin{align*}
\E{\max_{1\leq t \leq T} \env^t} &\leq
2\sqrt{\ln{(N)}\sum^T_{t=1}{\var{\dift}}}\leq 
2\sqrt{\ln{(N)}\sum^T_{t=1}{\min\left\{1,\frac{2(K-1)}{N-1} \right\}}}\\
&\leq 2\sqrt{T\ln{(N)}{\frac{2(K-1)}{N-1}}}.    
\end{align*}
\end{proof}

\subsection{Lower Bound}\label{appendix:lower bound}
\begin{claim}\label{claim: uni suff}
For the execution in Example~\ref{example: uni suff}, it holds that $\var{\dift} = \frac{1}{12}$.
\end{claim}
\begin{proof}[Proof of Claim~\ref{claim: uni suff}]
We prove the claim by using the definition of variance.
\begin{align}\label{eq:adfgsfhsaagh}
\var{\dift} & = \E{({\dift})^2}-\E{{\dift}}^2\overset{(1)}{=}\E{\left(\rt{(1)} - \rt{(2)}\right)^2} = \E{{\rt{(1)}}^2} - 2\E{\rt{(1)}\rt{(2)}} + \E{{\rt{(2)}}^2}
\end{align}
Next, we apply the threshold structure and properties of the uniform distribution to Equation~\eqref{eq:adfgsfhsaagh}.
{\thinmuskip=0mu
\medmuskip=0mu plus 0mu minus 0mu
\thickmuskip=1mu plus 1mu minus 1mu
\begin{align*}
\textnormal{Eq. }\eqref{eq:adfgsfhsaagh} &= 
\E{{X_1}^2} - \frac{1}{2}\cdot 2\E{X_1 X_1 \mid X_1 \geq \frac{1}{2}} - \frac{1}{2}\cdot 2\E{ X_1 X_2\mid X_1 < \frac{1}{2}}+ \frac{1}{2}\cdot \E{{X_1}^2 \mid X_1 \geq \frac{1}{2}} + \frac{1}{2}\cdot \E{{X_2}^2 \mid X_1 < \frac{1}{2}} \\
&=
\E{{X_1}^2} - \frac{1}{2} \E{{X_1}^2 \mid X_1 \geq \frac{1}{2}} - \E{ X_1\mid X_1 < \frac{1}{2}} \E{ X_2} + \frac{1}{2} \E{{X_2}^2}
\\&=
\frac{3}{2}\E{{\uni{0,1}}^2} - \frac{1}{2} \E{{\uni{\frac{1}{2},1}}^2} - \E{\uni{0,\frac{1}{2}}}= \frac{3}{2} \cdot \frac{1}{3}- \frac{1}{2} \cdot \frac{7}{12} - \frac{1}{4} \cdot \frac{1}{2} \\
&= \frac{1}{12},
\end{align*}}%
where we have used the independence of $X_1$ and $X_2$.
\end{proof}


\begin{claim}\label{claim:example ber suff}
For the execution in Example~\ref{example: ber suff}, it holds that 
\begin{enumerate}
    \item $\var{\Delta^t} \geq \frac{2p(1 - p)}{N}$.
    \item $\var{\Delta^t} \leq  2(1-p)$.
    \item $\var{\Delta^t} \leq  2pK$.
\end{enumerate}
\end{claim}
\begin{proof}[Proof of Claim~\ref{claim:example ber suff}]
Fix two arbitrary agents $i$ and $j$. Recall that Remark~\ref{remark: symmetric dif} ensures that $\Delta_{i,j}^t$ is identically distributed, regardless of the indexes $i$ and $j$. Let the event $B_q$ indicate that the number of arms that realize a value of 0 is $q$, for $q \in \{0, \ldots, K\}$. Further, let $E$ denote the event that $i$ and $j$ receive different rewards. Then we have:
\[
\var{\Delta_{i,j}^t} = \E{(\Delta_{i,j}^t)^2} 
= \sum_{q=0}^K \bigl( \E{(\Delta_{i,j}^t)^2 \mid B_q, E}\Pr(B_q, E) 
+ \E{(\Delta_{i,j}^t)^2 \mid B_q, \overline{E}}\Pr(B_q, \overline{E}) \bigr).
\]
Since $(\Delta_{i,j}^t)^2$ takes the value 1 under event $E$ and 0 otherwise, we get:
\begin{equation}\label{eq:gknmhmgf}
\var{\Delta_{i,j}^t} = \sum_{q=0}^K (1 \cdot \Pr(B_q, E) + 0 \cdot \Pr(B_q, \overline{E})) 
= \sum_{q=1}^K \Pr(B_q, E).    
\end{equation}
Fix any $q\in[K]$. It holds that 
\begin{align}\label{eq:develop for q}
\Pr(B_q, E) = \Pr(B_q)\Pr(E \mid B_q) =  p (1 - p)^{q}\cdot \frac{2\binom{N-2}{q-1}}{\binom{N}{q}} =  2 p (1 - p)^{q}\frac{q(N-q)}{N(N-1)}.
\end{align}
Combining Equations~\eqref{eq:gknmhmgf} and~\eqref{eq:develop for q}, we get
\begin{align}\label{eq:var def}
\var{\Delta_{i,j}^t} = \sum_{q=1}^K 2 p (1 - p)^{q}\frac{q(N-q)}{N(N-1)}.
\end{align}
Since all summands are positive, we obtain the first part of the claim by bounding from below using only the $q=1$ term. That is, we obtain $\var{\Delta_{i,j}^t} \geq  \frac{2p(1 - p)}{N}$. 

For the other parts of the claim, observe that for every $q \in [K]$, $ \frac{q(N-q)}{N(N-1)} \leq 1$; hence, Equation~\eqref{eq:var def} implies that 
\begin{align}\label{eq:gdhfghsdfg}
\var{\Delta_{i,j}^t} \leq \sum_{q=1}^K 2 p (1 - p)^{q}.
\end{align}
From here, we use Inequality~\eqref{eq:gdhfghsdfg} to obtain the second and third parts of the claim. First,
\begin{align*}
\sum_{q=1}^K 2 p (1 - p)^{q} \leq 2 p (1 - p) \sum_{q=0}^\infty (1 - p)^{q} = \frac{ 2 p (1 - p)}{1-(1-p)} = 2(1 - p);
\end{align*}
thus, $\var{\Delta_{i,j}^t} \leq  2(1-p)$ as the second part of the claim implies. Using a different approach to upper bound Inequality~\eqref{eq:gdhfghsdfg}, we get
\begin{align*}
\sum_{q=1}^K 2 p (1 - p)^{q} \leq 2p \sum_{q=1}^K 1^{q}=2pK
\end{align*}
As the third part of the claim asserts. This completes the proof of Claim~\ref{claim:example ber suff}.
\end{proof}



\iffalse %This is for the "sophisticated bound
Next, we move to the third part of the claim. Applying another approach to upper bound the right-hand-side of Equation~\eqref{eq:var def}, we obtain
\begin{align}\label{eq:var upper}
\sum_{q=1}^K 2 p (1 - p)^{q}\frac{q(N-q)}{N(N-1)} &=  \frac{2p}{N (N-1)} \sum_{q=1}^K (1 - p)^{q}q(N-q) \leq \frac{2p}{N (N-1)} \sum_{q=1}^K (1 - p)^{q}q (N -1) 
\nonumber \\
& = \frac{2p}{N} \sum_{q=1}^K (1 - p)^{q}q   \leq \frac{2p}{N}  \sum_{q=0}^\infty (1 - p)^{q}q.
\end{align}
Due to  Observation~\ref{obs:geo and mul} below,
\[
\sum_{q=0}^\infty (1 - p)^{q}q \leq \frac{1-p}{p^2}.
\]
Combining this with Inequality~\eqref{eq:var upper}, we ultimately obtain 
\[
\var{\Delta_{i,j}^t} \leq \frac{2p}{N} \frac{1-p}{p^2} = \frac{2(1-p)}{N p}.
\]
This completes the proof of Claim~\ref{claim:example ber suff}.
\end{proof}
\begin{observation}\label{obs:geo and mul}
For any $x\in (0,1)$, it holds that 
\[
\sum_{q=0}^\infty q x^{q} \leq \frac{x}{(1-x)^2}.
\]
\end{observation}
\begin{proof}[Proof of Observation~\ref{obs:geo and mul}]
Starting from $\sum_{n=0}^\infty x^n = \frac{1}{1-x}$, we differentiate both sides by $x$ to obtain
\[
\frac{d}{dx}\left(\sum_{n=0}^\infty x^n \right) = \frac{d}{dx}\left( \frac{1}{1-x} \right)\Leftrightarrow \sum_{n=0}^\infty n x^{n-1}  = \frac{1}{(1-x)^2} \Leftrightarrow \sum_{n=0}^\infty n x^{n}  = \frac{x}{(1-x)^2},
\]
where the last transition follows from multiplying both sides by $x$.
\end{proof}
\fi


\begin{proof}[Proof of Proposition~\ref{thm: board}]
    The proof of Proposition~\ref{thm: board} relies on the following algebraic inequality, which we prove after this proof.
    \begin{observation}\label{obs: algebric}
        For any $a\geq 0$, it holds that $2a \geq 3a^2 - a^4$.
    \end{observation}
    
    Recall, $Y$ is a non-negative random variable and therefore $\sqrt{\frac{Y}{\E{Y}}}$ is non-negative as well.
    When setting $a=\sqrt{\frac{Y}{\E{Y}}}$ we have
    \begin{align*}
         \frac{2\sqrt{Y}}{\sqrt{\E{Y}}} \geq \frac{3Y}{\E{Y}} - \frac{Y^2}{\E{Y}^2},
    \end{align*}
    for any value $Y$ can take.
    
    Notice that $\E{Y}\geq 0$, hence, by multiplying each side of the inequality by $\frac{\sqrt{\E{Y}}}{2}$ we get
    \begin{align*}
        \sqrt{Y} \geq \sqrt{\E{Y}}\left(\frac{3Y\E{Y} - Y^2}{2\E{Y}^2}\right).
    \end{align*}
    Taking expectation on both sides yields
    \begin{align*}
        \E{\sqrt{Y}} & \geq
        \E{\sqrt{\E{Y}}\left(\frac{3Y\E{Y} - Y^2}{2\E{Y}^2}\right)} \overset{(1)}{=}
        \sqrt{\E{Y}}\left(\E{\frac{2Y\E{Y}}{2\E{Y}^2}} - \E{\frac{Y^2 - Y\E{Y}}{2\E{Y}^2}}\right)
        \\&\overset{(2)}{=}
        \sqrt{\E{Y}}\left(\frac{2\E{Y}\E{Y}}{2\E{Y}^2} - \frac{\E{Y^2} - \E{Y}\E{Y}}{2\E{Y}^2}\right) \overset{(3)}{=}
        \sqrt{\E{Y}}\left(1 - \frac{\var{Y}}{2\E{Y}^2}   \right) ,
    \end{align*}
    where $(1)$ and $(2)$ hold due to linearity of expectation and $(3)$ is by the definition of variance. This concludes the proof of Proposition~\ref{thm: board}.
\end{proof}

\begin{proof}[Proof of Observation~\ref{obs: algebric}]
    To prove the inequality, it is sufficient to prove that the function $f(a) = a^3 - 3a^2 -2$ is non-negative for all $a\geq 0$.
    It holds that $f'(a) = 3 a^2 - 3$; thus
    \begin{align*}
        f'(a) > 0, & \text{ when } 0 \leq a < 1 \\
        f'(a) = 0, & \text{ when } a = 1 \\
        f'(a) < 0, & \text{ when } 1 \leq a.
    \end{align*}
    I.e., $f(a)$ is monotonically decreasing for $0\leq a <1$ and monotonically increasing for $1 \leq a$. 
    Hence, for all $a \geq 0$ it holds that $f(1) \geq f(a) = 0$
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:insufficient}]
We prove the proposition by reiterating the proof of Theorem~\ref{thm: uni lower-bound} while avoiding using the definition of sufficient execution. It suffices to show that the left-hand side of Inequality~\eqref{eq:m,bnhjikw} is constant. Starting with the numerator,
\begin{align}\label{eq:lpods}
\var{ \sum^T_{t=1}{ \dift^2} } &= \sum^T_{t=1} \var{ \dift^2} =  \sum^T_{t=1} \left(\E{(\dift)^4}-\E{(\dift)^2}^2 \right) = \sum^T_{t=1} \left( \var{\dift} - \var{\dift}^2 \right) \nonumber \\
&= T\left( \var{\dift} - \var{\dift}^2 \right),
\end{align}
where we have used the fact that the algorithm is stationary over rounds and that $\dift$ only takes values in the $\{0,1\}$ set. We keep using the superscript $t$ in Equation~\eqref{eq:m,bnhjikw} to ease readability, and it could be any arbitrary $t \in [T]$.

Next, we consider the denominator of the left-hand side of Inequality~\eqref{eq:m,bnhjikw}.
\begin{align}\label{eq:ydots}
2 \E{ \sum^T_{t=1}{(\dift)^2} }^2 =2  \left(\sum^T_{t=1}{\E{(\dift)^2} }\right)^2 = 2  \left(\sum^T_{t=1}{\var{\dift} }\right)^2 =  2 \left(T{\var{\dift} }\right)^2 = 2T^2 \var{\dift}^2
\end{align}
Combining Equations~\eqref{eq:lpods} and~\eqref{eq:ydots}, we get
\begin{align}\label{eq:prea}
\frac{T\left( \var{\dift} - \var{\dift}^2 \right)}{2T^2 \var{\dift}^2}=\frac{\left( 1 - \var{\dift} \right)}{2T \var{\dift}} \leq 
\frac{1}{2T \var{\dift}} \leq \frac{1}{2T}\frac{N}{2p(1-p)},
\end{align}
where the last inequality follows from Claim~\ref{claim:example ber suff}. 
Furthermore, recall that the proposition assumption guarantees that $p\in \left[\frac{N}{cT}, 1-\frac{N}{cT}\right]$ for $c\geq 2$, suggesting that
\[
p (1-p)\geq \frac{N}{cT}\left(1- \frac{N}{cT}\right) \geq \frac{N}{cT}\left(1- \frac{1}{c}\right) \geq \frac{N}{cT}\left(\frac{c-1}{c}\right) \geq \frac{N}{cT}\cdot \frac{1}{2}.
\]
Plugging this into the right-hand-side of Inequality~\eqref{eq:prea},
\begin{align}
 \frac{1}{2T}\frac{N}{2p(1-p)} \leq \frac{N}{4T} \cdot \frac{1}{p(1-p)} \leq \frac{N}{4T} \cdot \frac{2c T}{N}  = \frac{c}{2}.
\end{align}
Having observed that the left-hand-side of Inequality~\eqref{eq:m,bnhjikw} is bounded by a constant w.r.t. $T$, we complete the proof by plugging this constant into Inequality~\eqref{thm: mp uni lower-bound 2}.
\end{proof}


