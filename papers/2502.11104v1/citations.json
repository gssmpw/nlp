[
  {
    "index": 0,
    "papers": [
      {
        "key": "fu2023specializing",
        "author": "Fu, Yao and Peng, Hao and Ou, Litu and Sabharwal, Ashish and Khot, Tushar",
        "title": "Specializing smaller language models towards multi-step reasoning"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "wanknowledge",
        "author": "Wan, Fanqi and Huang, Xinting and Cai, Deng and Quan, Xiaojun and Bi, Wei and Shi, Shuming",
        "title": "Knowledge Fusion of Large Language Models"
      },
      {
        "key": "wan2024fusechat",
        "author": "Wan, Fanqi and Zhong, Longguang and Yang, Ziyi and Chen, Ruijun and Quan, Xiaojun",
        "title": "Fusechat: Knowledge fusion of chat models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "boizard2024towards",
        "author": "Boizard, Nicolas and Haddad, Kevin El and Hudelot, C{\\'e}line and Colombo, Pierre",
        "title": "Towards cross-tokenizer distillation: the universal logit distillation loss for llms"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "cui2024multi",
        "author": "Cui, Xiao and Zhu, Mo and Qin, Yulei and Xie, Liang and Zhou, Wengang and Li, Houqiang",
        "title": "Multi-Level Optimal Transport for Universal Cross-Tokenizer Knowledge Distillation on Language Models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "zhang2024dual",
        "author": "Zhang, Songming and Zhang, Xue and Sun, Zengkui and Chen, Yufeng and Xu, Jinan",
        "title": "Dual-Space Knowledge Distillation for Large Language Models"
      }
    ]
  }
]