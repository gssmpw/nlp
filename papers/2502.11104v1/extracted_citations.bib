@article{boizard2024towards,
  title={Towards cross-tokenizer distillation: the universal logit distillation loss for llms},
  author={Boizard, Nicolas and Haddad, Kevin El and Hudelot, C{\'e}line and Colombo, Pierre},
  journal={arXiv preprint arXiv:2402.12030},
  year={2024}
}

@article{cui2024multi,
  title={Multi-Level Optimal Transport for Universal Cross-Tokenizer Knowledge Distillation on Language Models},
  author={Cui, Xiao and Zhu, Mo and Qin, Yulei and Xie, Liang and Zhou, Wengang and Li, Houqiang},
  journal={arXiv preprint arXiv:2412.14528},
  year={2024}
}

@inproceedings{fu2023specializing,
  title={Specializing smaller language models towards multi-step reasoning},
  author={Fu, Yao and Peng, Hao and Ou, Litu and Sabharwal, Ashish and Khot, Tushar},
  booktitle={International Conference on Machine Learning},
  pages={10421--10430},
  year={2023},
  organization={PMLR}
}

@article{wan2024fusechat,
  title={Fusechat: Knowledge fusion of chat models},
  author={Wan, Fanqi and Zhong, Longguang and Yang, Ziyi and Chen, Ruijun and Quan, Xiaojun},
  journal={arXiv preprint arXiv:2408.07990},
  year={2024}
}

@inproceedings{wanknowledge,
  title={Knowledge Fusion of Large Language Models},
  author={Wan, Fanqi and Huang, Xinting and Cai, Deng and Quan, Xiaojun and Bi, Wei and Shi, Shuming},
  booktitle={The Twelfth International Conference on Learning Representations},
    year={2024}
}

@inproceedings{zhang2024dual,
  title={Dual-Space Knowledge Distillation for Large Language Models},
  author={Zhang, Songming and Zhang, Xue and Sun, Zengkui and Chen, Yufeng and Xu, Jinan},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={18164--18181},
  year={2024}
}

