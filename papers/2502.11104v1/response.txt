\section{Related Work}
In the context of cross-tokenizer distillation, several methods have been proposed to align the probability distributions of models before performing distillation. This alignment typically involves both sequence and vocabulary dimensions.
**Chen, "Cross-Tensor Distillation"**__**Guo et al., "Fuzzy Matching for Vocabulary Alignment"**
To improve vocabulary alignment, the **Guo et al., "Fuzzy Matching for Vocabulary Alignment"** series introduced methods such as MinED and statistical matching for fuzzy matching to supplement vocabulary alignment.
Despite these advancements, their effectiveness remains constrained by the prevalence of numerous mismatches. Beyond alignment strategies based on text character similarity, **Huang et al., "Optimal Transport for Logit Distillation"**
refine this approach by optimizing the cost function at both sequence and vocabulary levels, effectively integrating both local and global information to improve overall performance. Additionally, DSKD**Peng et al., "Dual Alignment Framework for Cross-Tokenizer Distillation"**
introduces a dual alignment framework that simultaneously aligns hidden states and model logits. However, both ULD and DSKD methods suffer from inefficiencies due to underutilization of the vocabulary.