\section{Related Work}
In the context of cross-tokenizer distillation, several methods have been proposed to align the probability distributions of models before performing distillation. This alignment typically involves both sequence and vocabulary dimensions.
____ aligns sequences through dynamic programming and aligns vocabularies through exact matching. To improve vocabulary alignment, the____ series introduced methods such as MinED and statistical matching for fuzzy matching to supplement vocabulary alignment.
Despite these advancements, their effectiveness remains constrained by the prevalence of numerous mismatches. Beyond alignment strategies based on text character similarity, ____ proposed the use of optimal transport to quantify the distance between model logits. Furthermore, ____ refine this approach by optimizing the cost function at both sequence and vocabulary levels, effectively integrating both local and global information to improve overall performance. Additionally, DSKD____ introduces a dual alignment framework that simultaneously aligns hidden states and model logits. However, both ULD and DSKD methods suffer from inefficiencies due to underutilization of the vocabulary.