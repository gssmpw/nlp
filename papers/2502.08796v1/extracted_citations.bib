@article{BARONCOHEN198537,
title = {Does the autistic child have a “theory of mind” ?},
journal = {Cognition},
volume = {21},
number = {1},
pages = {37-46},
year = {1985},
issn = {0010-0277},
doi = {https://doi.org/10.1016/0010-0277(85)90022-8},
url = {https://www.sciencedirect.com/science/article/pii/0010027785900228},
author = {Simon Baron-Cohen and Alan M. Leslie and Uta Frith},
abstract = {We use a new model of metarepresentational development to predict a cognitive deficit which could explain a crucial component of the social impairment in childhood autism. One of the manifestations of a basic metarepresentational capacity is a ‘theory of mind’. We have reason to believe that autistic children lack such a ‘theory’. If this were so, then they would be unable to impute beliefs to others and to predict their behaviour. This hypothesis was tested using Wimmer and Perner's puppet play paradigm. Normal children and those with Down's syndrome were used as controls for a group of autistic children. Even though the mental age of the autistic children was higher than that of the controls, they alone failed to impute beliefs to others. Thus the dysfunction we have postulated and demonstrated is independent of mental retardation and specific to autism.
Résumé
Les auteurs présentent un nouveau mod`éle de développement méta-cognitif pour prédire le déficit cognitif qui rendrait compte d'un composant essentiel du handicap social de l'enfant autiste. Une des manifestations d'une capacité de base méta-cognitive est une ‘theorie de l'esprit'. Nous avons des raisons de croire que cette théorie fait defaut chez l'enfant autiste. Celui-ci serait done incapable d'attribuer des croyances aux autres ou de prédire leur comportement. Cette hypothèse a été testée avec le paradigme de jeu des marionettes utilisé par Wimmer et Perner. Des enfants normaux et des enfants avec trisomie 21 ont servi de groupe contrôle. Bien que Page mental des enfants autistes ait été plus élevé que deux du groupe contrôle, seuls les enfants autistes Wont pu attribuer aux autres des croyances. Ainsi le dysfonctionnement prévu a pu être démontre, il s'avère indépendant du retard mental et spécifique a l'autiste.}
}

@inproceedings{Li_2023,
   title={Theory of Mind for Multi-Agent Collaboration via Large Language Models},
   url={http://dx.doi.org/10.18653/v1/2023.emnlp-main.13},
   DOI={10.18653/v1/2023.emnlp-main.13},
   booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
   publisher={Association for Computational Linguistics},
   author={Li, Huao and Chong, Yu and Stepputtis, Simon and Campbell, Joseph and Hughes, Dana and Lewis, Charles and Sycara, Katia},
   year={2023} }

@inproceedings{Verma_2024, series={HRI ’24},
   title={Theory of Mind Abilities of Large Language Models in Human-Robot Interaction: An Illusion?},
   url={http://dx.doi.org/10.1145/3610978.3640767},
   DOI={10.1145/3610978.3640767},
   booktitle={Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction},
   publisher={ACM},
   author={Verma, Mudit and Bhambri, Siddhant and Kambhampati, Subbarao},
   year={2024},
   month=mar, collection={HRI ’24} }

@misc{amirizaniani2024llmsexhibithumanlikereasoning,
      title={Do LLMs Exhibit Human-Like Reasoning? Evaluating Theory of Mind in LLMs for Open-Ended Responses}, 
      author={Maryam Amirizaniani and Elias Martin and Maryna Sivachenko and Afra Mashhadi and Chirag Shah},
      year={2024},
      eprint={2406.05659},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.05659}, 
}

@misc{bianchi2024llmsnegotiatenegotiationarenaplatform,
      title={How Well Can LLMs Negotiate? NegotiationArena Platform and Analysis}, 
      author={Federico Bianchi and Patrick John Chia and Mert Yuksekgonul and Jacopo Tagliabue and Dan Jurafsky and James Zou},
      year={2024},
      eprint={2402.05863},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2402.05863}, 
}

@misc{chan2024negotiationtombenchmarkstresstestingmachine,
      title={NegotiationToM: A Benchmark for Stress-testing Machine Theory of Mind on Negotiation Surrounding}, 
      author={Chunkit Chan and Cheng Jiayang and Yauwai Yim and Zheye Deng and Wei Fan and Haoran Li and Xin Liu and Hongming Zhang and Weiqi Wang and Yangqiu Song},
      year={2024},
      eprint={2404.13627},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.13627}, 
}

@misc{chen2024theorymindseyereading,
      title={Through the Theory of Mind's Eye: Reading Minds with Multimodal Video Large Language Models}, 
      author={Zhawnen Chen and Tianchun Wang and Yizhou Wang and Michal Kosinski and Xiang Zhang and Yun Fu and Sheng Li},
      year={2024},
      eprint={2406.13763},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.13763}, 
}

@misc{cross2024hypotheticalmindsscaffoldingtheory,
      title={Hypothetical Minds: Scaffolding Theory of Mind for Multi-Agent Tasks with Large Language Models}, 
      author={Logan Cross and Violet Xiang and Agam Bhatia and Daniel LK Yamins and Nick Haber},
      year={2024},
      eprint={2407.07086},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.07086}, 
}

@misc{etesam2024contextualemotionrecognitionusing,
      title={Contextual Emotion Recognition using Large Vision Language Models}, 
      author={Yasaman Etesam and Özge Nilay Yalçın and Chuxuan Zhang and Angelica Lim},
      year={2024},
      eprint={2405.08992},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2405.08992}, 
}

@misc{gandhi2023understandingsocialreasoninglanguage,
      title={Understanding Social Reasoning in Language Models with Language Models}, 
      author={Kanishk Gandhi and Jan-Philipp Fränken and Tobias Gerstenberg and Noah D. Goodman},
      year={2023},
      eprint={2306.15448},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.15448}, 
}

@article{gopnik1988childrens,
  title={Children's understanding of representational change and its relation to the understanding of false belief and the appearance-reality distinction},
  author={Gopnik A. and Astington J. W.},
  journal={Child Development},
  volume={59},
  number={1},
  pages={26--37},
  year={1988},
  publisher={Wiley},
  doi={10.1111/j.1467-8624.1988.tb03192.x},
  url={https://doi.org/10.1111/j.1467-8624.1988.tb03192.x}
}

@misc{guo2023suspicionagentplayingimperfectinformation,
      title={Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4}, 
      author={Jiaxian Guo and Bo Yang and Paul Yoo and Bill Yuchen Lin and Yusuke Iwasawa and Yutaka Matsuo},
      year={2023},
      eprint={2309.17277},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2309.17277}, 
}

@inproceedings{hou-etal-2024-timetom,
    title = "{T}ime{T}o{M}: Temporal Space is the Key to Unlocking the Door of Large Language Models{'} Theory-of-Mind",
    author = "Hou, Guiyang  and
      Zhang, Wenqi  and
      Shen, Yongliang  and
      Wu, Linjuan  and
      Lu, Weiming",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand and virtual meeting",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.685",
    pages = "11532--11547",
    abstract = "Theory of Mind (ToM){---}the cognitive ability to reason about mental states of ourselves and others, is the foundation of social interaction. Although ToM comes naturally to humans, it poses a significant challenge to even the most advanced Large Language Models (LLMs). Due to the complex logical chains in ToM reasoning, especially in higher-order ToM questions, simply utilizing reasoning methods like Chain of Thought (CoT) will not improve the ToM capabilities of LLMs. We present TimeToM, which constructs a temporal space and uses it as the foundation to improve the ToM capabilities of LLMs in multiple scenarios. Specifically, within the temporal space, we construct Temporal Belief State Chain (TBSC) for each character and inspired by the cognition perspective of the social world model, we divide TBSC into self-world beliefs and social world beliefs, aligning with first-order ToM (first-order beliefs) and higher-order ToM (higher-order beliefs) questions, respectively. Moreover, we design a novel tool-belief solver that, by considering belief communication between characters in temporal space, can transform a character{'}s higher-order beliefs into another character{'}s first-order beliefs under belief communication period.",
}

@misc{huang2024notioncomplexitytheorymind,
      title={A Notion of Complexity for Theory of Mind via Discrete World Models}, 
      author={X. Angelo Huang and Emanuele La Malfa and Samuele Marro and Andrea Asperti and Anthony Cohn and Michael Wooldridge},
      year={2024},
      eprint={2406.11911},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2406.11911}, 
}

@misc{jung2024perceptionsbeliefsexploringprecursory,
      title={Perceptions to Beliefs: Exploring Precursory Inferences for Theory of Mind in Large Language Models}, 
      author={Chani Jung and Dongkwan Kim and Jiho Jin and Jiseon Kim and Yeon Seonwoo and Yejin Choi and Alice Oh and Hyunwoo Kim},
      year={2024},
      eprint={2407.06004},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.06004}, 
}

@misc{kim2023fantombenchmarkstresstestingmachine,
      title={FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions}, 
      author={Hyunwoo Kim and Melanie Sclar and Xuhui Zhou and Ronan Le Bras and Gunhee Kim and Yejin Choi and Maarten Sap},
      year={2023},
      eprint={2310.15421},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.15421}, 
}

@misc{kosinski2024evaluatinglargelanguagemodels,
      title={Evaluating Large Language Models in Theory of Mind Tasks}, 
      author={Michal Kosinski},
      year={2024},
      eprint={2302.02083},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.02083}, 
}

@misc{kosoy2023comparingmachineschildrenusing,
      title={Comparing Machines and Children: Using Developmental Psychology Experiments to Assess the Strengths and Weaknesses of LaMDA Responses}, 
      author={Eliza Kosoy and Emily Rose Reagan and Leslie Lai and Alison Gopnik and Danielle Krettek Cobb},
      year={2023},
      eprint={2305.11243},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.11243}, 
}

@misc{kwon2024llmseffectivenegotiatorssystematic,
      title={Are LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues}, 
      author={Deuksin Kwon and Emily Weiss and Tara Kulshrestha and Kushal Chawla and Gale M. Lucas and Jonathan Gratch},
      year={2024},
      eprint={2402.13550},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.13550}, 
}

@misc{leibo2021scalableevaluationmultiagentreinforcement,
      title={Scalable Evaluation of Multi-Agent Reinforcement Learning with Melting Pot}, 
      author={Joel Z. Leibo and Edgar Duéñez-Guzmán and Alexander Sasha Vezhnevets and John P. Agapiou and Peter Sunehag and Raphael Koster and Jayd Matyas and Charles Beattie and Igor Mordatch and Thore Graepel},
      year={2021},
      eprint={2107.06857},
      archivePrefix={arXiv},
      primaryClass={cs.MA},
      url={https://arxiv.org/abs/2107.06857}, 
}

@misc{li2024quantifyingaipsychologypsychometrics,
      title={Quantifying AI Psychology: A Psychometrics Benchmark for Large Language Models}, 
      author={Yuan Li and Yue Huang and Hongyi Wang and Xiangliang Zhang and James Zou and Lichao Sun},
      year={2024},
      eprint={2406.17675},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.17675}, 
}

@misc{liu2024interintentinvestigatingsocialintelligence,
      title={InterIntent: Investigating Social Intelligence of LLMs via Intention Understanding in an Interactive Game Context}, 
      author={Ziyi Liu and Abhishek Anand and Pei Zhou and Jen-tse Huang and Jieyu Zhao},
      year={2024},
      eprint={2406.12203},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2406.12203}, 
}

@inproceedings{ma-etal-2023-towards-holistic,
    title = "Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models",
    author = "Ma, Ziqiao  and
      Sansom, Jacob  and
      Peng, Run  and
      Chai, Joyce",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.72",
    doi = "10.18653/v1/2023.findings-emnlp.72",
    pages = "1011--1031",
    abstract = "Large Language Models (LLMs) have generated considerable interest and debate regarding their potential emergence of Theory of Mind (ToM). Several recent inquiries reveal a lack of robust ToM in these models and pose a pressing demand to develop new benchmarks, as current ones primarily focus on different aspects of ToM and are prone to shortcuts and data leakage. In this position paper, we seek to answer two road-blocking questions: (1) How can we taxonomize a holistic landscape of machine ToM? (2) What is a more effective evaluation protocol for machine ToM? Following psychological studies, we taxonomize machine ToM into 7 mental state categories and delineate existing benchmarks to identify under-explored aspects of ToM. We argue for a holistic and situated evaluation of ToM to break ToM into individual components and treat LLMs as an agent who is physically situated in environments and socially situated in interactions with humans. Such situated evaluation provides a more comprehensive assessment of mental states and potentially mitigates the risk of shortcuts and data leakage. We further present a pilot study in a grid world setup as a proof of concept. We hope this position paper can facilitate future research to integrate ToM with LLMs and offer an intuitive means for researchers to better position their work in the landscape of ToM.",
}

@misc{ma2023tomchallengesprincipleguideddatasetdiverse,
      title={ToMChallenges: A Principle-Guided Dataset and Diverse Evaluation Tasks for Exploring Theory of Mind}, 
      author={Xiaomeng Ma and Lingyu Gao and Qihui Xu},
      year={2023},
      eprint={2305.15068},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.15068}, 
}

@misc{mireshghallah2024llmssecrettestingprivacy,
      title={Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory}, 
      author={Niloofar Mireshghallah and Hyunwoo Kim and Xuhui Zhou and Yulia Tsvetkov and Maarten Sap and Reza Shokri and Yejin Choi},
      year={2024},
      eprint={2310.17884},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2310.17884}, 
}

@misc{pi2024dissectingullmanvariationsscalpel,
      title={Dissecting the Ullman Variations with a SCALPEL: Why do LLMs fail at Trivial Alterations to the False Belief Task?}, 
      author={Zhiqiang Pi and Annapurna Vadaparty and Benjamin K. Bergen and Cameron R. Jones},
      year={2024},
      eprint={2406.14737},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.14737}, 
}

@InProceedings{pmlr-v162-sclar22a,
  title = 	 {Symmetric Machine Theory of Mind},
  author =       {Sclar, Melanie and Neubig, Graham and Bisk, Yonatan},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {19450--19466},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/sclar22a/sclar22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/sclar22a.html},
  abstract = 	 {Theory of mind, the ability to model others’ thoughts and desires, is a cornerstone of human social intelligence. This makes it an important challenge for the machine learning community, but previous works mainly attempt to design agents that model the "mental state" of others as passive observers or in specific predefined roles, such as in speaker-listener scenarios. In contrast, we propose to model machine theory of mind in a more general symmetric scenario. We introduce a multi-agent environment SymmToM where, like in real life, all agents can speak, listen, see other agents, and move freely through the world. Effective strategies to maximize an agent’s reward require it to develop a theory of mind. We show that reinforcement learning agents that model the mental states of others achieve significant performance improvements over agents with no such theory of mind model. Importantly, our best agents still fail to achieve performance comparable to agents with access to the gold-standard mental state of other agents, demonstrating that the modeling of theory of mind in multi-agent scenarios is very much an open challenge.}
}

@misc{rabinowitz2018machinetheorymind,
      title={Machine Theory of Mind}, 
      author={Neil C. Rabinowitz and Frank Perbet and H. Francis Song and Chiyuan Zhang and S. M. Ali Eslami and Matthew Botvinick},
      year={2018},
      eprint={1802.07740},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1802.07740}, 
}

@inproceedings{sabour-etal-2024-emobench,
    title = "{E}mo{B}ench: Evaluating the Emotional Intelligence of Large Language Models",
    author = "Sabour, Sahand  and
      Liu, Siyang  and
      Zhang, Zheyuan  and
      Liu, June  and
      Zhou, Jinfeng  and
      Sunaryo, Alvionna  and
      Lee, Tatia  and
      Mihalcea, Rada  and
      Huang, Minlie",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.326",
    pages = "5986--6004",
    abstract = "Recent advances in Large Language Models (LLMs) have highlighted the need for robust, comprehensive, and challenging benchmarks. Yet, research on evaluating their Emotional Intelligence (EI) is considerably limited. Existing benchmarks have two major shortcomings: first, they mainly focus on emotion recognition, neglecting essential EI capabilities such as emotion management and thought facilitation through emotion understanding; second, they are primarily constructed from existing datasets, which include frequent patterns, explicit information, and annotation errors, leading to unreliable evaluation. We propose EmoBench, a benchmark that draws upon established psychological theories and proposes a comprehensive definition for machine EI, including Emotional Understanding and Emotional Application. EmoBench includes a set of 400 hand-crafted questions in English and Chinese, which are meticulously designed to require thorough reasoning and understanding. Our findings reveal a considerable gap between the EI of existing LLMs and the average human, highlighting a promising direction for future research. Our code and data are publicly available at https://github.com/Sahandfer/EmoBench.",
}

@inproceedings{sap-etal-2019-social,
    title = "Social {IQ}a: Commonsense Reasoning about Social Interactions",
    author = "Sap, Maarten  and
      Rashkin, Hannah  and
      Chen, Derek  and
      Le Bras, Ronan  and
      Choi, Yejin",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1454",
    doi = "10.18653/v1/D19-1454",
    pages = "4463--4473",
    abstract = "We introduce Social IQa, the first large-scale benchmark for commonsense reasoning about social situations. Social IQa contains 38,000 multiple choice questions for probing emotional and social intelligence in a variety of everyday situations (e.g., Q: {``}Jordan wanted to tell Tracy a secret, so Jordan leaned towards Tracy. Why did Jordan do this?{''} A: {``}Make sure no one else could hear{''}). Through crowdsourcing, we collect commonsense questions along with correct and incorrect answers about social interactions, using a new framework that mitigates stylistic artifacts in incorrect answers by asking workers to provide the right answer to a different but related question. Empirical results show that our benchmark is challenging for existing question-answering models based on pretrained language models, compared to human performance ({\textgreater}20{\%} gap). Notably, we further establish Social IQa as a resource for transfer learning of commonsense knowledge, achieving state-of-the-art performance on multiple commonsense reasoning tasks (Winograd Schemas, COPA).",
}

@misc{sap2023neuraltheoryofmindlimitssocial,
      title={Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs}, 
      author={Maarten Sap and Ronan LeBras and Daniel Fried and Yejin Choi},
      year={2023},
      eprint={2210.13312},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@misc{shapira2023cleverhansneuraltheory,
      title={Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models}, 
      author={Natalie Shapira and Mosh Levy and Seyed Hossein Alavi and Xuhui Zhou and Yejin Choi and Yoav Goldberg and Maarten Sap and Vered Shwartz},
      year={2023},
      eprint={2305.14763},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.14763}, 
}

@misc{ullman2023largelanguagemodelsfail,
      title={Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks}, 
      author={Tomer Ullman},
      year={2023},
      eprint={2302.08399},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2302.08399}, 
}

@inproceedings{van-duijn-etal-2023-theory,
    title = "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
    author = "van Duijn, Max  and
      van Dijk, Bram  and
      Kouwenhoven, Tom  and
      de Valk, Werner  and
      Spruit, Marco  and
      van der Putten, Peter",
    editor = "Jiang, Jing  and
      Reitter, David  and
      Deng, Shumin",
    booktitle = "Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL)",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.conll-1.25",
    doi = "10.18653/v1/2023.conll-1.25",
    pages = "389--402",
    abstract = "To what degree should we ascribe cognitive capacities to Large Language Models (LLMs), such as the ability to reason about intentions and beliefs known as Theory of Mind (ToM)? Here we add to this emerging debate by (i) testing 11 base- and instruction-tuned LLMs on capabilities relevant to ToM beyond the dominant false-belief paradigm, including non-literal language usage and recursive intentionality; (ii) using newly rewritten versions of standardized tests to gauge LLMs{'} robustness; (iii) prompting and scoring for open besides closed questions; and (iv) benchmarking LLM performance against that of children aged 7-10 on the same tasks. We find that instruction-tuned LLMs from the GPT family outperform other models, and often also children. Base-LLMs are mostly unable to solve ToM tasks, even with specialized prompting. We suggest that the interlinked evolution and development of language and ToM may help explain what instruction-tuning adds: rewarding cooperative communication that takes into account interlocutor and context. We conclude by arguing for a nuanced perspective on ToM in LLMs.",
}

@misc{wang2024metacognitivepromptingimprovesunderstanding,
      title={Metacognitive Prompting Improves Understanding in Large Language Models}, 
      author={Yuqing Wang and Yun Zhao},
      year={2024},
      eprint={2308.05342},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.05342}, 
}

@misc{wilf2023thinktwiceperspectivetakingimproves,
      title={Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities}, 
      author={Alex Wilf and Sihyun Shawn Lee and Paul Pu Liang and Louis-Philippe Morency},
      year={2023},
      eprint={2311.10227},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2311.10227}, 
}

@misc{xu2024faithfullogicalreasoningsymbolic,
      title={Faithful Logical Reasoning via Symbolic Chain-of-Thought}, 
      author={Jundong Xu and Hao Fei and Liangming Pan and Qian Liu and Mong-Li Lee and Wynne Hsu},
      year={2024},
      eprint={2405.18357},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.18357}, 
}

@misc{yu2024fewshotcharacterunderstandingmovies,
      title={Few-Shot Character Understanding in Movies as an Assessment to Meta-Learning of Theory-of-Mind}, 
      author={Mo Yu and Qiujing Wang and Shunchi Zhang and Yisi Sang and Kangsheng Pu and Zekai Wei and Han Wang and Liyan Xu and Jing Li and Yue Yu and Jie Zhou},
      year={2024},
      eprint={2211.04684},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2211.04684}, 
}

@misc{zhou2023farlargelanguagemodels,
      title={How FaR Are Large Language Models From Agents with Theory-of-Mind?}, 
      author={Pei Zhou and Aman Madaan and Srividya Pranavi Potharaju and Aditya Gupta and Kevin R. McKee and Ari Holtzman and Jay Pujara and Xiang Ren and Swaroop Mishra and Aida Nematzadeh and Shyam Upadhyay and Manaal Faruqui},
      year={2023},
      eprint={2310.03051},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.03051}, 
}

@inproceedings{zhou2023i,
    title={I Cast Detect Thoughts: Learning to Converse and Guide with Intents and Theory-of-Mind in Dungeons and Dragons}, 
    author={Pei Zhou and Andrew Zhu and Jennifer Hu and Jay Pujara and Xiang Ren and Chris Callison-Burch and Yejin Choi and Prithviraj Ammanabrolu},
    year={2023},
    booktitle={Proceedings of the 61th Annual Meeting of the Association for Computational Linguistics},
}

