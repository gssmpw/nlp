
\section{Introduction}
Federated learning (FL)~\cite{fedavg,fedavgm,scaffold,fedprox,fedadam} aims to train models using client data while safeguarding client privacy.
To ensure privacy protection, client data remains on local devices rather than being transmitted to a central server.
Instead, the model is trained locally on the client’s device, and the locally trained models are then transmitted to the server for aggregation into a global model.
By exchanging model parameters instead of client data, FL enables collaborative learning across distributed clients while preserving data privacy.

Large language models (LLMs)~\cite{GPT3,openai2023gpt4,anil2023palm,team2023gemini,team2024gemma,touvron2023llama1,touvron2023llama2,vicuna2023,alpaca,llama3.1,abdin2024phi,yang2024qwen2} have rapidly advanced and are now widely used in applications like question answering and chatbots.
Recent research explores training LLMs via FL, referred to as FedLLM~\cite{FedPETuning,FedIT,sun2024improving,kuang2024federatedscope,OpenFedLLM}, aiming to finetune LLMs with client data while maintaining privacy.

However, previous studies~\cite{FedPETuning,FedIT,sun2024improving,kuang2024federatedscope,OpenFedLLM} have overlooked responsible AI (RAI~\cite{henderson2018ethical,bender2021dangers,weidinger2021ethical,hhh,bai2022training}), which aims to ensure that an LLM generates safe, ethical, and harmless responses. 
Without RAI, the global model trained through FedLLM may generate harmful responses, known as \textit{red responses}.
Since client data is collected from real-world user interactions with LLM, it is often uncurated and may include harmful content, such as hate speech, harassment, or controversial statements that can incite conflict.
Training on such data introduces the risk of incorporating harmful content, which can lead to an unsafe global model. 
When such a model is distributed to all clients, it may lead to the widespread deployment of an unsafe model, highlighting the need for safeguards.

To address this risk of deploying unsafe models, we propose applying two well-known RAI methods to FedLLM: the safety filter~\cite{llamaguard} and constitutional AI (CAI)~\cite{CAI}.
As shown in Figure~\ref{fig:safetyfilter_CAI}, the safety filter is applied to client data to prevent the inclusion of harmful content, while CAI is applied to the global model to ensure safe responses.
However, simply applying CAI incurs a high computational cost. To address this, we introduce a cost-efficient CAI approach, reducing the computational cost of CAI by 96\%.
Our key research questions are as follows:
\begin{enumerate}[label=\textbf{RQ\arabic*:}, leftmargin=*]
\item Does FedLLM with data containing red responses truly degrade safety performance? If so, to what extent?
\item Are the safety filter and CAI effective in FedLLM? 
\end{enumerate}

Through comprehensive experiments, we answer these questions and show that both methods significantly enhance the LLM's ability to generate safe responses. 
On AdvBench, a safety evaluation benchmark, our methods improve performance by over 20\%.
To the best of our knowledge, this is the first study to integrate these RAI methods into FedLLM, laying the foundation for further research on responsible FedLLM.


\begin{figure}[t]
\centering
    \includegraphics[width=\linewidth]{images/FedLLM-RAI.png} 
    \vspace{-6mm}
    \caption{For the responsible federated large language model (FedLLM), we apply a safety filter and Constitutional AI (CAI) to improve safety.
}
    \label{fig:safetyfilter_CAI}
    % \vspace{-4mm}
    \Description{The figure illustrates the implementation of Responsible FedLLM, which incorporates a safety filter and Constitutional AI (CAI) to enhance the safety of LLM. The safety filter classifies and prevents harmful data from being used in local training, ensuring that only safe inputs contribute to model parameters. Meanwhile, CAI reinforces adherence to ethical guidelines by guiding the model’s behavior and responses. Together, these components help improve the safety of LLM.}
\end{figure}




\section{Preliminary: FedLLM Framework}\label{subsec:pre-fedllm}
As background, we introduce the FedLLM framework, illustrated in Figure~\ref{fig:FedLLM}.
In FedLLM, a pretrained LLM is finetuned using FL.
Previous studies~\cite{FedPETuning,FedIT,sun2024improving,kuang2024federatedscope,OpenFedLLM} employed widely used LLMs, such as LLaMA\cite{touvron2023llama2}-based models (e.g., Vicuna~\cite{vicuna2023} and Alpaca~\cite{alpaca}).
Since these LLMs often contain billions of parameters (e.g., over 7 billion), training all parameters incurs high costs.

Moreover, in FL, model parameters are frequently exchanged between clients and the server, significantly increasing communication costs for large models. 
To address this, the FedLLM framework employs parameter-efficient finetuning (PEFT)~\cite{adapter,lora}, which achieves comparable performance by introducing a small number of additional trainable parameters instead of updating the full model. 
Among PEFT methods, low-rank adaptation (LoRA)~\cite{lora} is the most widely used.

Based on the above concepts, FedLLM consists of the following steps:
\begin{enumerate}[label=(\arabic*), leftmargin=*]
    \item Distribute the pretrained LLM ($W_P$) from the server to the clients. $W_P$ remains frozen (unchanged) during training.

    \item Distribute the global LoRA ($W_G$) from the server to each client. 
    At each client, $W_G$ is integrated with $W_P$ to form the global model (global LLM).
    
    \item At each client, $W_G$ is finetuned using its private local data (private conversations with the LLM). The finetuned $W_G$ is called the local LoRA ($W_L$).
    
    \item Clients send their $W_L$ to the server, where they are aggregated to update the global LoRA $W_G$.
\end{enumerate}

Steps (2), (3), and (4) together constitute one round.
By repeating this round, the global LoRA $W_G$ is continuously updated, advancing the FL process.


\begin{figure}[t]
\centering
    \includegraphics[width=\linewidth]{images/FedLLM-LoRA.png} 
    \vspace{-6mm}
    \caption{Overview of FedLLM.
    $W$ represents the model weights. The pretrained LLM ($W_P$) remains frozen, and only the local LoRA weights ($W_L$) are finetuned.
}
    \label{fig:FedLLM}
    % \vspace{-2mm}
    \Description{The figure illustrates the Federated Learning with LLM (FedLLM) approach, where a pretrained LLM remains frozen while only the local LoRA weights are finetuned. This strategy enables efficient adaptation to decentralized data without updating the full model parameters. The notation W represents the model weights. By finetuning only the LoRA weights, this approach reduces computational overhead in FedLLM.}
\end{figure}


\section{Potential Risk in FedLLM}\label{sec:risk}
In FedLLM, harmful data (red responses) may be introduced in the training set in two ways:
(1) Some malicious clients can use red teaming prompts (queries designed to elicit red responses)~\cite{redteamingprompt,redteaminglm} to generate a large number of red responses.
(2) Even ordinary questions can unintentionally serve as red teaming prompts, potentially leading to controversial statements, which is one of the RAI issues. 
For example, a seemingly ordinary question like ``Who should I vote for in the presidential election?'' could result in a biased response that favors one side, which might be considered a red response. 
Therefore, addressing RAI issues in all clients' everyday conversations is crucial.

Training the local model (local LoRA) with harmful data can lead to an unsafe LLM that generates harmful responses.  
If such an unsafe LLM is aggregated into the global model (global LoRA) on the server, all clients face the risk of using an unsafe global model.



\section{Two Methods for Responsible FedLLM}
To address the potential risk in FedLLM, we apply two well-known RAI methods. 
As illustrated in Figure~\ref{fig:safetyfilter_CAI}, we enhance the safety of the aggregated global model (global LoRA) by applying the safety filter~\cite{llamaguard} to private local data (client data) to filter out harmful data.
Additionally, we apply constitutional AI (CAI)~\cite{CAI} to the global model to promote safe responses.
Implementing these two approaches improves model safety despite FedLLM’s unique characteristics, such as the ``client data being inaccessible from the server'' and ``multiple local models being aggregated into the global model.''



\subsection{Safety Filter}\label{subsec:safety}
The safety filter prevents harmful data from being used in local model training by filtering it out.
Specifically, the safety filter takes the query and the LLM's response as input and classifies them as safe or unsafe.  
Unsafe data is excluded from training.

Previous work~\cite{square} used the Transformer\cite{Transformer}-based model ELECTRA~\cite{electra} as a safety filter.
In our framework, we employ Llama Guard 3 (LG3)~\cite{llamaguard}, a Llama3.1\cite{llama3.1}-based model, recognized for its strong safety filtering capabilities.
Note that the safety filter does not require training on the client's device.  
LG3 is finetuned on the server and deployed to clients to filter out harmful data before client-side training.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{images/CAI-turns.png} 
    % \vspace{-6mm}
    \caption{For CAI, red and self-revised responses are collected over three conversation turns.
}
    \label{fig:CAI-turns}
    \Description{The figure illustrates the Constitutional AI (CAI) process, which consists of three turns of conversation with the LLM. In each turn, the model generates an initial response, followed by a refinement step where it aligns with ethical guidelines or safety constraints. This iterative process helps improve the model’s adherence to responsible AI principles by progressively refining its outputs. The structured dialogue ensures that the LLM produces responses that are more aligned with ethical considerations and user intent.}
\end{figure*}


\subsection{Constitutional AI (CAI)}\label{subsec:CAI}
Constitutional AI (CAI)~\cite{CAI} is a widely recognized RAI method that aims to ensure LLMs generate safe responses.
CAI defines constitutions, which are guidelines or principles the LLM must follow, and prompts the LLM to self-critique and revise its responses accordingly.
The self-revised responses are then used to continually refine the LLM, thereby ensuring that it better adheres to the constitution.
By enforcing ``not generating red responses'' as a constitutional rule, CAI effectively serves as an RAI method.

To apply CAI, we first collect red responses and self-revised responses from red prompts through the following process.
Figure~\ref{fig:CAI-turns} illustrates an example of this process.
\begin{enumerate}[label=(Turn~\arabic*), leftmargin=*]
    \item Using a red teaming prompt, the LLM generates a red response. 
    
    \item Using a self-critique prompt, the LLM evaluates and critiques its own response, generating a self-critical response.

    \item Using a self-revision prompt and the conversations from Turn 1 and Turn 2, the LLM generates a self-revised response by revising its red response.
\end{enumerate}

Then, CAI is applied based on the collected data.
CAI consists of two stages.
First, supervised finetuning (SFT) is performed on the LLM, where the ``red teaming prompt'' serves as input and the ``self-revised response'' as output.
Second, preference learning~\cite{PPO,DPO} (specifically, direct preference optimization (DPO)~\cite{DPO}) is applied.
DPO uses a preference dataset, which consists of two different responses to a single query, with a label indicating the preferred response.
The preferred response is labeled as ``chosen'', while the other is labeled as ``rejected''.  
DPO trains the LLM to prioritize generating the chosen response.  
To train the LLM to generate self-revised responses, we set the ``self-revised response'' as ``chosen'' and the ``red response'' as ``rejected''. 

As shown in Figure~\ref{fig:safetyfilter_CAI}, we apply CAI to the global model on the server, increasing the likelihood of generating safe responses. 
Note that we apply CAI at the end of each round to improve safety before the global model (global LoRA in FedLLM) is distributed to clients.

When applying CAI, the model is typically trained for at least one epoch on the CAI dataset~\cite{CAI}.
However, applying CAI in every round leads to substantial computational overhead: in our experiments, CAI takes approximately 80 minutes per epoch using four NVIDIA A100 GPUs.
To alleviate this burden, we propose a cost-efficient approach: instead of applying CAI for a full epoch in each round, we apply CAI for only a small number of iterations (50 iterations).
This shortens the CAI training time per round from 80 minutes to 3.2 minutes, resulting in a 96\% reduction.
Although applying CAI for one epoch per round is expected to yield better performance, our experiments demonstrate that our cost-efficient approach still achieves sufficiently effective results.


\begin{table*}[t] 
  \tabcolsep=0.15cm
    \begin{center}
    \caption{Results of FedLLM with the safety filter and CAI.  
Both methods significantly enhance the safety of the LLM.
    }
    \vspace{-2mm}
        \begin{tabular}{@{}r|l|cc|c|cc|c@{}}
            \toprule
            &  & \multicolumn{3}{c|}{\textbf{FL method: FedAvg}~\cite{fedavg}}
& \multicolumn{3}{c}{\textbf{FL method: SCAFFOLD}~\cite{scaffold}}\\ 
            \cline{3-8}
            & & \multicolumn{2}{c|}{Safety eval. (\%)} & Helpfulness & \multicolumn{2}{c|}{Safety eval. (\%)} & Helpfulness \\
            % \cline{3-10}
            \textbf{\#} & \textbf{Method} & AdvBench & \multicolumn{1}{c|}{HHH} &  MT-Bench  & AdvBench & \multicolumn{1}{c|}{HHH} &  MT-Bench \\
            \midrule
            1 & Pretrained LLM: Llama3.1-8B-Instruct & \textbf{99.6} & 60.0 & \textbf{6.8} & \textbf{99.6} & 60.0 & \textbf{6.8}  \\
            2 & FL & 72.5 & 49.3 & 2.7 & 72.7 & 49.5 & 2.9 \\
            \midrule
            3 & FL + Safety filter & 81.2 & 51.8 & 2.4  & 78.8 & 54.6 & 2.7  \\
            4 & FL + CAI & 96.2 & 57.3 & 5.8 & 96.5 & 62.6 & 5.9 \\
            5 & FL + Safety filter + CAI & 96.3 & \textbf{63.7} & 6.1 & 97.1 & \textbf{63.9} & 5.8 \\
            \bottomrule
        \end{tabular}
    \label{tab:FedLLM}
    \end{center}
\end{table*} 


\begin{table}[t] 
  \tabcolsep=0.15cm
    \begin{center}
    \caption{Performance of the safety filter (LG3).
    }
    \vspace{-2mm}
        \begin{tabular}{@{}l|cccc@{}}
            \toprule
            Method & Acc.~(\%) & Precision~(\%) & Recall~(\%) & Hmean~(\%) \\
            \midrule
            LG3 & 70.1 & \textbf{90.6} & 0.5 & 1.0  \\
            Finetuned LG3 & \textbf{75.5} & 56.7 & \textbf{73.7} & \textbf{64.1} \\
            \bottomrule
        \end{tabular}
    \label{tab:safety}
    \end{center}
\end{table} 


\section{Dataset Preparation}
\subsection{Training Dataset: SQuARe}
For responsible FedLLM experiments, we conduct FedLLM with a dataset containing red teaming prompts and red responses.
We use a safety benchmark dataset called SQuARe~\cite{square}, which provides annotations in both Korean and English. We use the English annotations.
SQuARe's training set consists of approximately 64K responses to 37K red teaming prompts. Among the 64K responses, 31K are red responses, while 33K are acceptable responses.
Acceptable responses are defined as harmless responses that respect diversity, adhere to ethical standards, promote pro-social behavior, or convey objective information without making future predictions.

As described in \S\ref{sec:risk}, ordinary conversations can also be considered harmful data in certain contexts.
To reflect this observation in our experimental design, we create a scenario where each client’s dataset includes a portion of harmful data.
We assume that the proportion of harmful responses is relatively low for ordinary users and design an experiment where 30\% of responses are harmful.
From the 64K responses, we create a subset for FedLLM, named SQuARe20K, consisting of 6K red responses and 14K acceptable responses.
This dataset is randomly distributed to clients, ensuring that each client receives approximately 30\% harmful data in their assigned dataset.

From the remaining 44K responses, we create a subset, named S-LG20K, consisting of 10K red responses and 10K acceptable responses for finetuning the safety filter. 
Based on S-LG20K, we generate S-CAI20K through the process illustrated in Figure~\ref{fig:CAI-turns}, another subset designed for CAI experiments.


\subsection{Evaluation Datasets}
To evaluate safety and helpfulness, we adopt the datasets used in OpenFedLLM~\cite{OpenFedLLM}:  
(1) AdvBench~\cite{advbench} and (2) the helpful, honest, and harmless (HHH)~\cite{hhh} dataset for safety evaluation, and (3) the Multi-turn benchmark (MT-Bench)~\cite{mtbench} for helpfulness evaluation.  
AdvBench consists of 520 red teaming prompts.  
HHH contains 438 samples, each with a red teaming prompt, two answer options (A and B), and a label indicating the correct answer.  
MT-Bench evaluates multi-turn conversation and instruction-following abilities, focusing on common use cases.  
It includes 80 multi-turn questions across 8 common categories: writing, roleplay, extraction, reasoning, math, coding, STEM, and humanities/social science.


\section{Experiments and Analysis}
\subsection{Implementation Detail}\label{subsec:imple}
\PAR{Models for LLM and Safety Filter}
We adopt the recently released Llama3.1-8B-Instruct~\cite{llama3.1} as the pretrained LLM and use LoRA~\cite{lora} for PEFT.
For the safety filter, we use Llama Guard 3 (LG3)~\cite{llamaguard}. 
We finetune LG3 on the S-LG20K dataset to improve its filtering accuracy.


\PAR{FedLLM Framework}
We use the OpenFedLLM~\cite{OpenFedLLM} framework for our FedLLM experiments.
We employ two well-known FL methods: FedAvg~\cite{fedavg} and SCAFFOLD~\cite{scaffold}.
Most hyperparameters follow the settings of OpenFedLLM, with only a few modifications.  
We use 20 clients, 50 rounds, 5 clients per round, and 25 iterations per round.  
The SQuARe20K datasets are evenly split into 20 parts, allocating 1K samples to each client.  
The batch size is 16.


\PAR{Computational Resource and Elapsed Time}
All training was conducted using four NVIDIA A100 GPUs. Training LG3 took 1 hour (5 epochs).
For FedLLM, FedAvg and SCAFFOLD required 140 and 230 minutes, respectively.
CAI took approximately 160 minutes.
For filtering user data, LG3 processed 1,000 samples in about 1 minute using a single A100 GPU.


\PAR{Evaluation Metric}
We evaluate the performance of the global model on the same metrics used in OpenFedLLM.  
For AdvBench, we use a rule-based method to determine whether a response is harmless, based on the presence of specific phrases.  
For HHH, accuracy is calculated by determining whether the correct choice is made between options (A) and (B).  
For MT-Bench, we follow the llm-as-a-judge method~\cite{mtbench}, using GPT-4~\cite{openai2023gpt4} to score each response on a scale from 1 to 10.  


\subsection{RQ1: Does FedLLM with red responses degrade safety?}
Table~\ref{tab:FedLLM} shows the results of FedLLM experiments.  
Comparing \#1 and \#2 in Table~\ref{tab:FedLLM}, conducting FedLLM with harmful data (SQuARe20K) significantly reduces safety performance.  
For FedAvg, AdvBench drops by 27.1\% and HHH by 10.7\%; for SCAFFOLD, AdvBench drops by 26.9\% and HHH by 10.5\%.  
Since SQuARe20K includes red teaming prompts and red responses, it appears that the model is trained to generate red responses.  
Conducting FedLLM with SQuARe20K also reduces MT-Bench performance, which decreases by 4.1 for FedAvg and 3.9 for SCAFFOLD.  


\subsection{RQ2: Are the safety filter and CAI effective?}
\PAR{Effectiveness of Safety Filter}
Table~\ref{tab:safety} presents the results of applying the safety filter (LG3) to SQuARe20K.  
LG3 without finetuning fails to detect most unsafe data, classifying almost all responses as safe, resulting in an Hmean of only 1.0\%.
In contrast, the finetuned LG3 achieves 64.1\% Hmean, indicating a significant improvement in filtering unsafe data after fine-tuning. 

While the finetuned LG3 is not perfect, applying it to FedLLM effectively improves safety.
Comparing \#2 and \#3 in Table~\ref{tab:FedLLM}, filtering data with the finetuned LG3 improves the performance of both AdvBench and HHH.  
For FedAvg, AdvBench improves by 8.7\% and HHH by 2.5\%; for SCAFFOLD, AdvBench improves by 6.1\% and HHH by 5.1\%. 
These results demonstrate that applying a safety filter to local data helps the global model generate safer responses.
Meanwhile, MT-Bench performance slightly decreases.  


\PAR{Effectiveness of CAI}  
Applying CAI to FedLLM is effective not only in enhancing safety but also in improving helpfulness.  
Comparing \#2, and \#4, applying CAI significantly improves the performance of both AdvBench and HHH.  
For FedAvg, AdvBench improves by 23.7\% and HHH by 8.0\%; for SCAFFOLD, AdvBench improves by 23.8\% and HHH by 13.1\%. 
CAI also improves the MT-Bench score by 3.1 for FedAvg and 3.0 for SCAFFOLD.


\PAR{Combining Safety Filter and CAI}
The combined use of the safety filter and CAI (\#5) enhances safety performance.  
Comparing \#4 and \#5, for FedAvg, AdvBench improves by 0.1\%, HHH by 6.4\%, and MT-Bench by 0.3.  
For SCAFFOLD, AdvBench improves by 0.6\%, and HHH by 1.3\%, while MT-Bench drops by 0.1.
These results suggest that the two methods are complementary, leading to improved safety performance when combined.  
This complementary effect likely arises because each method addresses different aspects of the problem: the safety filter excludes unsafe local data, while CAI enhances the LLM's responses through self-critique and self-revision.


\section{Conclusion}  
We have shown that including red responses in the training data of FedLLM significantly degrades safety.
To address this issue, we have demonstrated the effectiveness of the safety filter and CAI in improving the safety and robustness of FedLLM.
We hope our findings contribute to advancing responsible FedLLM research.
As future work, we plan to extend our approach to multimodal FedLLM by applying RAI methods to multimodal data.