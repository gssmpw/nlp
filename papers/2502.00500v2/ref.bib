@article{sme20,
  title={Denoising diffusion implicit models},
  author={Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  journal={arXiv preprint arXiv:2010.02502},
  year={2020}
}

@article{lcb+22,
  title={Flow matching for generative modeling},
  author={Lipman, Yaron and Chen, Ricky TQ and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matt},
  journal={arXiv preprint arXiv:2210.02747},
  year={2022}
}

@article{lgl22,
  title={Flow straight and fast: Learning to generate and transfer data with rectified flow},
  author={Liu, Xingchao and Gong, Chengyue and Liu, Qiang},
  journal={arXiv preprint arXiv:2209.03003},
  year={2022}
}

@article{hsg+22,
  title={Video diffusion models},
  author={Ho, Jonathan and Salimans, Tim and Gritsenko, Alexey and Chan, William and Norouzi, Mohammad and Fleet, David J},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={8633--8646},
  year={2022}
}

@article{jsl+24,
  title={Pyramidal flow matching for efficient video generative modeling},
  author={Jin, Yang and Sun, Zhicheng and Li, Ningyuan and Xu, Kun and Jiang, Hao and Zhuang, Nan and Huang, Quzhe and Song, Yang and Mu, Yadong and Lin, Zhouchen},
  journal={arXiv preprint arXiv:2410.05954},
  year={2024}
}

@article{bph+24,
  title={Video generation models as world simulators},
  author={Tim Brooks and Bill Peebles and Connor Holmes and Will DePue and Yufei Guo and Li Jing and David Schnurr and Joe Taylor and Troy Luhman and Eric Luhman and Clarence Ng and Ricky Wang and Aditya Ramesh},
  year={2024},
  url={https://openai.com/research/video-generation-models-as-world-simulators},
}

@article{wbs+04,
  title={Image quality assessment: from error visibility to structural similarity},
  author={Wang, Zhou and Bovik, Alan C and Sheikh, Hamid R and Simoncelli, Eero P},
  journal={IEEE transactions on image processing},
  volume={13},
  number={4},
  pages={600--612},
  year={2004},
  publisher={IEEE}
}

@inproceedings{rbl+22,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10684--10695},
  year={2022}
}

@article{sbd+24,
  title={Norm-guided latent space exploration for text-to-image generation},
  author={Samuel, Dvir and Ben-Ari, Rami and Darshan, Nir and Maron, Haggai and Chechik, Gal},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{s85,
  title={Animating rotation with quaternion curves},
  author={Shoemake, Ken},
  booktitle={Proceedings of the 12th annual conference on Computer graphics and interactive techniques},
  pages={245--254},
  year={1985}
}

@inproceedings{mha+23,
  title={Null-text inversion for editing real images using guided diffusion models},
  author={Mokady, Ron and Hertz, Amir and Aberman, Kfir and Pritch, Yael and Cohen-Or, Daniel},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6038--6047},
  year={2023}
}

@inproceedings{rkh+21,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{k13,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@article{wcm+24,
  title={Lavie: High-quality video generation with cascaded latent diffusion models},
  author={Wang, Yaohui and Chen, Xinyuan and Ma, Xin and Zhou, Shangchen and Huang, Ziqi and Wang, Yi and Yang, Ceyuan and He, Yinan and Yu, Jiashuo and Yang, Peiqing and others},
  journal={International Journal of Computer Vision},
  pages={1--20},
  year={2024},
  publisher={Springer}
}

@inproceedings{px23,
  title={Scalable diffusion models with transformers},
  author={Peebles, William and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4195--4205},
  year={2023}
}

@article{hwsl24,
  title={On statistical rates and provably efficient criteria of latent diffusion transformers (dits)},
  author={Hu, Jerry Yao-Chieh and Wu, Weimin and Song, Zhao and Liu, Han},
  journal={arXiv preprint arXiv:2407.01079},
  year={2024}
}

@article{ybr+19,
  title={Are transformers universal approximators of sequence-to-sequence functions?},
  author={Yun, Chulhee and Bhojanapalli, Srinadh and Rawat, Ankit Singh and Reddi, Sashank J and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:1912.10077},
  year={2019}
}

@article{vnn+17,
  title={Attention Is All You Need}, 
  author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  year={2017},
  journal={arXiv preprint arXiv:1706.03762},
}

@article{hja20,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}

@article{gde+20,
  title={Hippo: Recurrent memory with optimal polynomial projections},
  author={Gu, Albert and Dao, Tri and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1474--1487},
  year={2020}
}

@article{sph+22,
  title={Make-a-video: Text-to-video generation without text-video data},
  author={Singer, Uriel and Polyak, Adam and Hayes, Thomas and Yin, Xi and An, Jie and Zhang, Songyang and Hu, Qiyuan and Yang, Harry and Ashual, Oron and Gafni, Oran and others},
  journal={arXiv preprint arXiv:2209.14792},
  year={2022}
}

@article{vjp22,
  title={Mcvd-masked conditional video diffusion for prediction, generation, and interpolation},
  author={Voleti, Vikram and Jolicoeur-Martineau, Alexia and Pal, Chris},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={23371--23385},
  year={2022}
}

@inproceedings{brl+23,
  title={Align your latents: High-resolution video synthesis with latent diffusion models},
  author={Blattmann, Andreas and Rombach, Robin and Ling, Huan and Dockhorn, Tim and Kim, Seung Wook and Fidler, Sanja and Kreis, Karsten},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={22563--22575},
  year={2023}
}

@article{ssk+20,
  title={Score-based generative modeling through stochastic differential equations},
  author={Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  journal={arXiv preprint arXiv:2011.13456},
  year={2020}
}

@inproceedings{gnl+23,
  title={Preserve your own correlation: A noise prior for video diffusion models},
  author={Ge, Songwei and Nah, Seungjun and Liu, Guilin and Poon, Tyler and Tao, Andrew and Catanzaro, Bryan and Jacobs, David and Huang, Jia-Bin and Liu, Ming-Yu and Balaji, Yogesh},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={22930--22941},
  year={2023}
}

@article{azy+23,
  title={Latent-shift: Latent diffusion with temporal shift for efficient text-to-video generation},
  author={An, Jie and Zhang, Songyang and Yang, Harry and Gupta, Sonal and Huang, Jia-Bin and Luo, Jiebo and Yin, Xi},
  journal={arXiv preprint arXiv:2304.08477},
  year={2023}
}

@article{gwz+23,
  title={Reuse and diffuse: Iterative denoising for text-to-video generation},
  author={Gu, Jiaxi and Wang, Shicong and Zhao, Haoyu and Lu, Tianyi and Zhang, Xing and Wu, Zuxuan and Xu, Songcen and Zhang, Wei and Jiang, Yu-Gang and Xu, Hang},
  journal={arXiv preprint arXiv:2309.03549},
  year={2023}
}

@article{gyr+23,
  title={Animatediff: Animate your personalized text-to-image diffusion models without specific tuning},
  author={Guo, Yuwei and Yang, Ceyuan and Rao, Anyi and Liang, Zhengyang and Wang, Yaohui and Qiao, Yu and Agrawala, Maneesh and Lin, Dahua and Dai, Bo},
  journal={arXiv preprint arXiv:2307.04725},
  year={2023}
}

@article{bnh+22,
  title={ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers},
  author={Balaji, Yogesh and Nah, Seungjun and Huang, Xun and Vahdat, Arash and Song, Jiaming and Zhang, Qinsheng and Kreis, Karsten and Aittala, Miika and Aila, Timo and Laine, Samuli and others},
  journal={arXiv preprint arXiv:2211.01324},
  year={2022}
}

@article{zwy+22,
  title={Magicvideo: Efficient video generation with latent diffusion models},
  author={Zhou, Daquan and Wang, Weimin and Yan, Hanshu and Lv, Weiwei and Zhu, Yizhe and Feng, Jiashi},
  journal={arXiv preprint arXiv:2211.11018},
  year={2022}
}

@article{tby+19,
  title={Transformer dissection: a unified understanding of transformer's attention via the lens of kernel},
  author={Tsai, Yao-Hung Hubert and Bai, Shaojie and Yamada, Makoto and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1908.11775},
  year={2019}
}

@inproceedings{zhdk23,
  title={Kdeformer: Accelerating transformers via kernel density estimation},
  author={Zandieh, Amir and Han, Insu and Daliri, Majid and Karbasi, Amin},
  booktitle={International Conference on Machine Learning},
  pages={40605--40623},
  year={2023},
  organization={PMLR}
}

@article{bsz23,
  title={Algorithm and hardness for dynamic attention maintenance in large language models},
  author={Brand, Jan van den and Song, Zhao and Zhou, Tianyi},
  journal={arXiv preprint arXiv:2304.02207},
  year={2023}
}

@article{as24,
  title={Fast attention requires bounded entries},
  author={Alman, Josh and Song, Zhao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{syz24,
  title={Solving attention kernel regression problem via pre-conditioner},
  author={Song, Zhao and Yin, Junze and Zhang, Lichen},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={208--216},
  year={2024},
  organization={PMLR}
}

@article{cll+24_rope,
  title={Circuit Complexity Bounds for RoPE-based Transformer Architecture},
  author={Chen, Bo and Li, Xiaoyu and Liang, Yingyu and Long, Jiangxuan and Shi, Zhenmei and Song, Zhao},
  journal={arXiv preprint arXiv:2411.07602},
  year={2024}
}

@article{hlsl24,
  title={On computational limits of modern hopfield models: A fine-grained complexity analysis},
  author={Hu, Jerry Yao-Chieh and Lin, Thomas and Song, Zhao and Liu, Han},
  journal={arXiv preprint arXiv:2402.04520},
  year={2024}
}

@inproceedings{mosw22,
  title={Bounding the width of neural networks via coupled initialization a worst case analysis},
  author={Munteanu, Alexander and Omlor, Simon and Song, Zhao and Woodruff, David},
  booktitle={International Conference on Machine Learning},
  pages={16083--16122},
  year={2022},
  organization={PMLR}
}

@article{szz24,
  title={Training multi-layer over-parametrized neural network in subquadratic time},
  author={Song, Zhao and Zhang, Lichen and Zhang, Ruizhe},
  journal={ITCS},
  year={2024}
}

@inproceedings{als19_icml,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle={International conference on machine learning},
  pages={242--252},
  year={2019},
  organization={PMLR}
}

@article{dhs+22,
  title={Training overparametrized neural networks in sublinear time},
  author={Deng, Yichuan and Hu, Hang and Song, Zhao and Weinstein, Omri and Zhuo, Danyang},
  journal={arXiv preprint arXiv:2208.04508},
  year={2022}
}

@article{bpsw21,
  title={Training (Overparametrized) Neural Networks in Near-Linear Time},
  author={van den Brand, Jan and Peng, Binghui and Song, Zhao and Weinstein, Omri},
  journal={ITCS},
  year={2021}
}

@article{syz21,
  title={Does preprocessing help training over-parameterized neural networks?},
  author={Song, Zhao and Yang, Shuo and Zhang, Ruizhe},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={22890--22904},
  year={2021}
}

@article{als+23,
  title={Bypass exponential time preprocessing: Fast neural network training via weight-data correlation preprocessing},
  author={Alman, Josh and Song, Zhao and Zhang, Ruizhe and Zhuo, Danyang},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@article{hjk+23,
  title={Hyperattention: Long-context attention in near-linear time},
  author={Han, Insu and Jayaram, Rajesh and Karbasi, Amin and Mirrokni, Vahab and Woodruff, David P and Zandieh, Amir},
  journal={arXiv preprint arXiv:2310.05869},
  year={2023}
}

@article{smn+24,
  title={Discovering the gems in early layers: Accelerating long-context llms with 1000x input token reduction},
  author={Shi, Zhenmei and Ming, Yifei and Nguyen, Xuan-Phi and Liang, Yingyu and Joty, Shafiq},
  journal={arXiv preprint arXiv:2409.17422},
  year={2024}
}

@inproceedings{szz+21,
  title={Efficient attention: Attention with linear complexities},
  author={Shen, Zhuoran and Zhang, Mingyuan and Zhao, Haiyu and Yi, Shuai and Li, Hongsheng},
  booktitle={Proceedings of the IEEE/CVF winter conference on applications of computer vision},
  pages={3531--3539},
  year={2021}
}

@article{lll21,
  title={Research progress in attention mechanism in deep learning},
  author={LIU, Jian-wei and LIU, Jun-wen and LUO, Xiong-lin},
  journal={Chinese Journal of Engineering},
  volume={43},
  number={11},
  pages={1499--1511},
  year={2021},
  publisher={Chinese Journal of Engineering Editorial Office}
}

@article{lls+24_conv,
  title={Conv-basis: A new paradigm for efficient attention inference and gradient computation in transformers},
  author={Liang, Yingyu and Liu, Heshan and Shi, Zhenmei and Song, Zhao and Xu, Zhuoyan and Yin, Junze},
  journal={arXiv preprint arXiv:2405.05219},
  year={2024}
}

@article{lssz24_tat,
  title={Tensor attention training: Provably efficient learning of higher-order transformers},
  author={Liang, Yingyu and Shi, Zhenmei and Song, Zhao and Zhou, Yufa},
  journal={arXiv preprint arXiv:2405.16411},
  year={2024}
}

@article{lss+24,
  title={Multi-layer transformers gradient can be approximated in almost linear time},
  author={Liang, Yingyu and Sha, Zhizhou and Shi, Zhenmei and Song, Zhao and Zhou, Yufa},
  journal={arXiv preprint arXiv:2408.13233},
  year={2024}
}

@article{llss24_sparse,
  title={A tighter complexity analysis of sparsegpt},
  author={Li, Xiaoyu and Liang, Yingyu and Shi, Zhenmei and Song, Zhao},
  journal={arXiv preprint arXiv:2408.12151},
  year={2024}
}

@article{lls+24_prune,
  title={Beyond linear approximations: A novel pruning approach for attention matrix},
  author={Liang, Yingyu and Long, Jiangxuan and Shi, Zhenmei and Song, Zhao and Zhou, Yufa},
  journal={arXiv preprint arXiv:2410.11261},
  year={2024}
}

@article{cls+24,
  title={Hsr-enhanced sparse attention acceleration},
  author={Chen, Bo and Liang, Yingyu and Sha, Zhizhou and Shi, Zhenmei and Song, Zhao},
  journal={arXiv preprint arXiv:2410.10165},
  year={2024}
}

@article{lls+24_io,
  title={Fine-grained attention i/o complexity: Comprehensive analysis for backward passes},
  author={Li, Xiaoyu and Liang, Yingyu and Shi, Zhenmei and Song, Zhao and Zhou, Yufa},
  journal={arXiv preprint arXiv:2410.09397},
  year={2024}
}

@article{hwl24,
  title={Provably optimal memory capacity for modern hopfield models: Transformer-compatible dense associative memories as spherical codes},
  author={Hu, Jerry Yao-Chieh and Wu, Dennis and Liu, Han},
  journal={arXiv preprint arXiv:2410.23126},
  year={2024}
}

@article{hcl+24,
  title={Outlier-efficient hopfield layers for large transformer-based models},
  author={Hu, Jerry Yao-Chieh and Chang, Pei-Hsuan and Luo, Robin and Chen, Hong-Yu and Li, Weijian and Wang, Wei-Po and Liu, Han},
  journal={arXiv preprint arXiv:2404.03828},
  year={2024}
}

@article{whhl24,
  title={Uniform memory retrieval with larger capacity for modern hopfield models},
  author={Wu, Dennis and Hu, Jerry Yao-Chieh and Hsiao, Teng-Yun and Liu, Han},
  journal={arXiv preprint arXiv:2404.03827},
  year={2024}
}

@article{hyw+23,
  title={On sparse modern hopfield model},
  author={Hu, Jerry Yao-Chieh and Yang, Donglin and Wu, Dennis and Xu, Chenwei and Chen, Bo-Yu and Liu, Han},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@article{as24_arxiv,
  title={The fine-grained complexity of gradient computation for training large language models},
  author={Alman, Josh and Song, Zhao},
  journal={arXiv preprint arXiv:2402.04497},
  year={2024}
}

@article{gswy23,
  title={A fast optimization view: Reformulating single layer attention in llm based on tensor and svm trick, and solving it in matrix multiplication time},
  author={Gao, Yeqi and Song, Zhao and Wang, Weixin and Yin, Junze},
  journal={arXiv preprint arXiv:2309.07418},
  year={2023}
}

@article{bmr+20,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{wtb+22,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@article{al23,
  title={Physics of language models: Part 1, context-free grammar},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2305.13673},
  year={2023}
}

@article{j23,
  title={A latent space theory for emergent abilities in large language models},
  author={Jiang, Hui},
  journal={arXiv preprint arXiv:2304.09960},
  year={2023}
}

@article{xsw+23,
  title={Towards Few-Shot Adaptation of Foundation Models via Multitask Finetuning},
  author={Xu, Zhuoyan and Shi, Zhenmei and Wei, Junyi and Mu, Fangzhou and Li, Yin and Liang, Yingyu},
  journal={arXiv preprint arXiv:2402.15017},
  year={2024}
}

@article{lls+24_grok,
  title={Fourier circuits in neural networks: Unlocking the potential of large language models in mathematical reasoning and modular arithmetic},
  author={Li, Chenyang and Liang, Yingyu and Shi, Zhenmei and Song, Zhao and Zhou, Tianyi},
  journal={arXiv preprint arXiv:2402.09469},
  year={2024}
}

@inproceedings{xsl24,
  title={Do large language models have compositional ability? an investigation into limitations and scalability},
  author={Xu, Zhuoyan and Shi, Zhenmei and Liang, Yingyu},
  booktitle={ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models},
  year={2024}
}

@article{dls23,
  title={Attention scheme inspired softmax regression},
  author={Deng, Yichuan and Li, Zhihang and Song, Zhao},
  journal={arXiv preprint arXiv:2304.10411},
  year={2023}
}

@article{cll+24,
  title={Circuit Complexity Bounds for RoPE-based Transformer Architecture},
  author={Chen, Bo and Li, Xiaoyu and Liang, Yingyu and Long, Jiangxuan and Shi, Zhenmei and Song, Zhao},
  journal={arXiv preprint arXiv:2411.07602},
  year={2024}
}

@article{lss+24_relu,
  title={Looped relu mlps may be all you need as practical programmable computers},
  author={Liang, Yingyu and Sha, Zhizhou and Shi, Zhenmei and Song, Zhao and Zhou, Yufa},
  journal={arXiv preprint arXiv:2410.09375},
  year={2024}
}

@article{hwg+24,
  title={Fundamental limits of prompt tuning transformers: Universality, capacity and efficiency},
  author={Hu, Jerry Yao-Chieh and Wang, Wei-Po and Gilani, Ammar and Li, Chenyang and Song, Zhao and Liu, Han},
  journal={arXiv preprint arXiv:2411.16525},
  year={2024}
}

@article{wsh+24,
  title={Transformers are Deep Optimizers: Provable In-Context Learning for Deep Model Training},
  author={Wu, Weimin and Su, Maojiang and Hu, Jerry Yao-Chieh and Song, Zhao and Liu, Han},
  journal={arXiv preprint arXiv:2411.16549},
  year={2024}
}

@inproceedings{kwh23,
  title={On the computational complexity of self-attention},
  author={Keles, Feyza Duman and Wijewardena, Pruthuvi Mahesakya and Hegde, Chinmay},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={597--619},
  year={2023},
  organization={PMLR}
}

@article{hcs+22,
  title={Imagen video: High definition video generation with diffusion models},
  author={Ho, Jonathan and Chan, William and Saharia, Chitwan and Whang, Jay and Gao, Ruiqi and Gritsenko, Alexey and Kingma, Diederik P and Poole, Ben and Norouzi, Mohammad and Fleet, David J and others},
  journal={arXiv preprint arXiv:2210.02303},
  year={2022}
}

@article{dsxy23,
  title={Unmasking Transformers: A Theoretical Approach to Data Recovery via Attention Weights},
  author={Deng, Yichuan and Song, Zhao and Xie, Shenghao and Yang, Chiwun},
  journal={arXiv preprint arXiv:2310.12462},
  year={2023}
}

@article{kls+24,
  title={Curse of attention: A kernel-based perspective for why transformers fail to generalize on time series forecasting and beyond},
  author={Ke, Yekun and Liang, Yingyu and Shi, Zhenmei and Song, Zhao and Yang, Chiwun},
  journal={arXiv preprint arXiv:2412.06061},
  year={2024}
}

@article{lswy23,
  title={A theoretical insight into attack and defense of gradient leakage in transformer},
  author={Li, Chenyang and Song, Zhao and Wang, Weixin and Yang, Chiwun},
  journal={arXiv preprint arXiv:2311.13624},
  year={2023}
}

@article{gls+24,
  title={Toward Infinite-Long Prefix in Transformer},
  author={Gu, Jiuxiang and Liang, Yingyu and Shi, Zhenmei and Song, Zhao and Yang, Chiwun},
  journal={arXiv preprint arXiv:2406.14036},
  year={2024}
}

@article{kds+15,
   title={On Particle Methods for Parameter Estimation in State-Space Models},
   volume={30},
   ISSN={0883-4237},
   url={http://dx.doi.org/10.1214/14-STS511},
   DOI={10.1214/14-sts511},
   number={3},
   journal={Statistical Science},
   publisher={Institute of Mathematical Statistics},
   author={Kantas, Nikolas and Doucet, Arnaud and Singh, Sumeetpal S. and Maciejowski, Jan and Chopin, Nicolas},
   year={2015},
}

@article{aia+22,
  title={How to train your hippo: State space models with generalized orthogonal basis projections},
  author={Gu, Albert and Johnson, Isys and Timalsina, Aman and Rudra, Atri and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2206.12037},
  year={2022}
}

@article{gd23,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}

@article{dg24,
  title={Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality},
  author={Dao, Tri and Gu, Albert},
  journal={arXiv preprint arXiv:2405.21060},
  year={2024}
}

@article{zlz+24,
  title={Vision mamba: Efficient visual representation learning with bidirectional state space model},
  author={Zhu, Lianghui and Liao, Bencheng and Zhang, Qian and Wang, Xinlong and Liu, Wenyu and Wang, Xinggang},
  journal={arXiv preprint arXiv:2401.09417},
  year={2024}
}

@inproceedings{xyy+24,
  title={Segmamba: Long-range sequential modeling mamba for 3d medical image segmentation},
  author={Xing, Zhaohu and Ye, Tian and Yang, Yijun and Liu, Guang and Zhu, Lei},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={578--588},
  year={2024},
  organization={Springer}
}

@article{mlw24,
  title={U-mamba: Enhancing long-range dependency for biomedical image segmentation},
  author={Ma, Jun and Li, Feifei and Wang, Bo},
  journal={arXiv preprint arXiv:2401.04722},
  year={2024}
}

@article{rx24,
  title={Vm-unet: Vision mamba unet for medical image segmentation},
  author={Ruan, Jiacheng and Xiang, Suncheng},
  journal={arXiv preprint arXiv:2402.02491},
  year={2024}
}

@article{sld+24,
  title={Learning to (learn at test time): Rnns with expressive hidden states},
  author={Sun, Yu and Li, Xinhao and Dalal, Karan and Xu, Jiarui and Vikram, Arjun and Zhang, Genghan and Dubois, Yann and Chen, Xinlei and Wang, Xiaolong and Koyejo, Sanmi and others},
  journal={arXiv preprint arXiv:2407.04620},
  year={2024}
}

@inproceedings{nbsm23,
  title={Action matching: Learning stochastic dynamics from samples},
  author={Neklyudov, Kirill and Brekelmans, Rob and Severo, Daniel and Makhzani, Alireza},
  booktitle={International conference on machine learning},
  pages={25858--25889},
  year={2023},
  organization={PMLR}
}

@article{av22,
  title={Building normalizing flows with stochastic interpolants},
  author={Albergo, Michael S and Vanden-Eijnden, Eric},
  journal={arXiv preprint arXiv:2209.15571},
  year={2022}
}

@inproceedings{hbc23,
  title={Iterative $\alpha$-(de) blending: A minimalist deterministic diffusion model},
  author={Heitz, Eric and Belcour, Laurent and Chambon, Thomas},
  booktitle={ACM SIGGRAPH 2023 Conference Proceedings},
  pages={1--8},
  year={2023}
}

@article{tfm+23,
  title={Improving and generalizing flow-based generative models with minibatch optimal transport},
  author={Tong, Alexander and Fatras, Kilian and Malkin, Nikolay and Huguet, Guillaume and Zhang, Yanlei and Rector-Brooks, Jarrid and Wolf, Guy and Bengio, Yoshua},
  journal={arXiv preprint arXiv:2302.00482},
  year={2023}
}

@article{rgnl21,
  title={Moser flow: Divergence-based generative modeling on manifolds},
  author={Rozen, Noam and Grover, Aditya and Nickel, Maximilian and Lipman, Yaron},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17669--17680},
  year={2021}
}

@article{bcba+22,
  title={Matching normalizing flows and probability paths on manifolds},
  author={Ben-Hamu, Heli and Cohen, Samuel and Bose, Joey and Amos, Brandon and Grover, Aditya and Nickel, Maximilian and Chen, Ricky TQ and Lipman, Yaron},
  journal={arXiv preprint arXiv:2207.04711},
  year={2022}
}

@inproceedings{ekb+24,
  title={Scaling rectified flow transformers for high-resolution image synthesis},
  author={Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and M{\"u}ller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and others},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{lvs+24,
  title={Voicebox: Text-guided multilingual universal speech generation at scale},
  author={Le, Matthew and Vyas, Apoorv and Shi, Bowen and Karrer, Brian and Sari, Leda and Moritz, Rashel and Williamson, Mary and Manohar, Vimal and Adi, Yossi and Mahadeokar, Jay and others},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@article{vsl+23,
  title={Audiobox: Unified audio generation with natural language prompts},
  author={Vyas, Apoorv and Shi, Bowen and Le, Matthew and Tjandra, Andros and Wu, Yi-Chiao and Guo, Baishan and Zhang, Jiemin and Zhang, Xinyue and Adkins, Robert and Ngan, William and others},
  journal={arXiv preprint arXiv:2312.15821},
  year={2023}
}

@article{hvf+24,
  title={Sequence-Augmented SE (3)-Flow Matching For Conditional Protein Backbone Generation},
  author={Huguet, Guillaume and Vuckovic, James and Fatras, Kilian and Thibodeau-Laufer, Eric and Lemos, Pablo and Islam, Riashat and Liu, Cheng-Hao and Rector-Brooks, Jarrid and Akhound-Sadegh, Tara and Bronstein, Michael and others},
  journal={arXiv preprint arXiv:2405.20313},
  year={2024}
}

@article{bbd+24,
  title={$\pi_0$: A Vision-Language-Action Flow Model for General Robot Control},
  author={Black, Kevin and Brown, Noah and Driess, Danny and Esmail, Adnan and Equi, Michael and Finn, Chelsea and Fusai, Niccolo and Groom, Lachy and Hausman, Karol and Ichter, Brian and others},
  journal={arXiv preprint arXiv:2410.24164},
  year={2024}
}

@article{pzb+24,
  title={Movie gen: A cast of media foundation models},
  author={Polyak, Adam and Zohar, Amit and Brown, Andrew and Tjandra, Andros and Sinha, Animesh and Lee, Ann and Vyas, Apoorv and Shi, Bowen and Ma, Chih-Yao and Chuang, Ching-Yao and others},
  journal={arXiv preprint arXiv:2410.13720},
  year={2024}
}

@article{cyb+24,
  title={Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design},
  author={Campbell, Andrew and Yim, Jason and Barzilay, Regina and Rainforth, Tom and Jaakkola, Tommi},
  journal={arXiv preprint arXiv:2402.04997},
  year={2024}
}

@article{grs+24,
  title={Discrete flow matching},
  author={Gat, Itai and Remez, Tal and Shaul, Neta and Kreuk, Felix and Chen, Ricky TQ and Synnaeve, Gabriel and Adi, Yossi and Lipman, Yaron},
  journal={arXiv preprint arXiv:2407.15595},
  year={2024}
}

@article{cls24,
  title={Grams: Gradient Descent with Adaptive Momentum Scaling},
  author={Cao, Yang and Li, Xiaoyu and Song, Zhao},
  journal={arXiv preprint arXiv:2412.17107},
  year={2024}
}

@inproceedings{chzw23,
  title={Score approximation, estimation and distribution recovery of diffusion models on low-dimensional data},
  author={Chen, Minshuo and Huang, Kaixuan and Zhao, Tuo and Wang, Mengdi},
  booktitle={International Conference on Machine Learning},
  pages={4672--4712},
  year={2023},
  organization={PMLR}
}


@article{zpy+24,
  title={Open-sora: Democratizing efficient video production for all},
  author={Zheng, Zangwei and Peng, Xiangyu and Yang, Tianji and Shen, Chenhui and Li, Shenggui and Liu, Hongxin and Zhou, Yukun and Li, Tianyi and You, Yang},
  journal={arXiv preprint arXiv:2412.20404},
  year={2024}
}

@article{djp+24,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{lfx+24,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

@article{kw13,
      title={Auto-Encoding Variational Bayes}, 
      author={Diederik P Kingma and Max Welling},
      journal={arXiv preprint arXiv:1312.6114},
      year={2013}
}

@article{aaa23,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{scs+22,
  title={Photorealistic text-to-image diffusion models with deep language understanding},
  author={Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily L and Ghasemipour, Kamyar and Gontijo Lopes, Raphael and Karagol Ayan, Burcu and Salimans, Tim and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={36479--36494},
  year={2022}
}

@software{s23,
    title={Bark: Text-Prompted Generative Audio Model},
    author={Suno-AI},
    url={https://github.com/suno-ai/bark}
}

@article{tjy+24,
  title={Visual autoregressive modeling: Scalable image generation via next-scale prediction},
  author={Tian, Keyu and Jiang, Yi and Yuan, Zehuan and Peng, Bingyue and Wang, Liwei},
  journal={arXiv preprint arXiv:2404.02905},
  year={2024}
}

@article{ktz+24,
  title={HunyuanVideo: A Systematic Framework For Large Video Generative Models},
  author={Kong, Weijie and Tian, Qi and Zhang, Zijian and Min, Rox and Dai, Zuozhuo and Zhou, Jin and Xiong, Jiangfeng and Li, Xin and Wu, Bo and Zhang, Jianwei and others},
  journal={arXiv preprint arXiv:2412.03603},
  year={2024}
}

@article{nxz+24,
  title={Openvid-1m: A large-scale high-quality dataset for text-to-video generation},
  author={Nan, Kepan and Xie, Rui and Zhou, Penghao and Fan, Tiehan and Yang, Zhenheng and Chen, Zhijie and Li, Xiang and Yang, Jian and Tai, Ying},
  journal={arXiv preprint arXiv:2407.02371},
  year={2024}
}

@article{szs12,
  title={UCF101: A dataset of 101 human actions classes from videos in the wild},
  author={Khurram Soomro and Amir Roshan Zamir and Mubarak Shah},
  journal={arXiv preprint arXiv:1212.0402},
  year={2012}
}

@article{kcs+17,
  title={The kinetics human action video dataset},
  author={Kay, Will and Carreira, Joao and Simonyan, Karen and Zhang, Brian and Hillier, Chloe and Vijayanarasimhan, Sudheendra and Viola, Fabio and Green, Tim and Back, Trevor and Natsev, Paul and others},
  journal={arXiv preprint arXiv:1705.06950},
  year={2017}
}

@misc{pixabay,
  author       = {Pixabay},
  title        = {Pixabay - Stunning royalty-free images \& royalty-free stock},
  url = {https://pixabay.com},
}

@article{akl+16,
  title={Youtube-8m: A large-scale video classification benchmark},
  author={Abu-El-Haija, Sami and Kothari, Nisarg and Lee, Joonseok and Natsev, Paul and Toderici, George and Varadarajan, Balakrishnan and Vijayanarasimhan, Sudheendra},
  journal={arXiv preprint arXiv:1609.08675},
  year={2016}
}

@article{whl+23,
  title={Internvid: A large-scale video-text dataset for multimodal understanding and generation},
  author={Wang, Yi and He, Yinan and Li, Yizhuo and Li, Kunchang and Yu, Jiashuo and Ma, Xin and Li, Xinhao and Chen, Guo and Chen, Xinyuan and Wang, Yaohui and others},
  journal={arXiv preprint arXiv:2307.06942},
  year={2023}
}

@article{jgz+24,
  title={Miradata: A large-scale video dataset with long durations and structured captions},
  author={Ju, Xuan and Gao, Yiming and Zhang, Zhaoyang and Yuan, Ziyang and Wang, Xintao and Zeng, Ailing and Xiong, Yu and Xu, Qiang and Shan, Ying},
  journal={arXiv preprint arXiv:2407.06358},
  year={2024}
}

@inproceedings{csy24,
  title={How to Protect Copyright Data in Optimization of Large Language Models?},
  author={Chu, Timothy and Song, Zhao and Yang, Chiwun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={16},
  pages={17871--17879},
  year={2024}
}

@article{dsy24_a,
  title={Unlocking the Theory Behind Scaling 1-Bit Neural Networks},
  author={Daliri, Majid and Song, Zhao and Yang, Chiwun},
  journal={arXiv preprint arXiv:2411.01663},
  year={2024}
}

@article{dsy24_b,
  title={Attention is Naturally Sparse with Gaussian Distributed Input},
  author={Deng, Yichuan and Song, Zhao and Yang, Chiwun},
  journal={arXiv preprint arXiv:2404.02690},
  year={2024}
}