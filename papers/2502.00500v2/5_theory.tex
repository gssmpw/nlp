\section{Theory}\label{sec:theory}

This section provides several theoretical advantages of our VLFM. The approximation theory in this approach builds up based on using the Diffusion Transformer (DiT) \cite{px23}, which is a popular choice in previous empirical and theoretical part generative model works \cite{chzw23, hwsl24}, we briefly state its definitions in Section~\ref{sub:DiT}.

In addition, we provide the optimal polynomial projection guarantee and universal approximation theorem (with DiT) of VLFM in Section~\ref{sub:approx} to confirm its approximating ability. Besides, Section~\ref{sub:inter-extra_polation_theory} gives error bound of interpolation and extrapolation, and Section~\ref{sub:timescale_robustness} gives the supplementary property that VLFM's timescale robustness, which indicates its theoretical advantages.

\subsection{Diffusion Transformer (DiT)}\label{sub:DiT}

Diffusion Transformer \cite{px23} is a framework that utilizes Transformers \cite{vnn+17} as the backbone for Diffusion Models \cite{hja20,sme20}. Specifically, a Transformer block consists of a multi-head self-attention layer and a feed-forward layer, with both layers having a skip connection. 
We use ${\sf TF}^{h, m, r}: \R^{n \times d_0}\rightarrow \R^{n \times d_0}$ to denote a Transformer block.
Here $h$ and $m$ are the number of heads and head size in self-attention layer, and $r$ is the hidden dimension in feed-forward layer.
Let $X \in \R^{n \times d_0}$ be the model input. Then, we have the model output:
\ifdefined\isarxiv
\begin{align*}
    {\sf Attn}(X) := \sum_{i=1}^h {\sf Softmax}( X W_Q^i {W_K^i}^\top X^\top ) \cdot X W_V^i {W_O^i}^\top + X,
\end{align*}
\else
\begin{align*}
    & ~ {\sf Attn}(X) \\
    & ~ := \sum_{i=1}^h {\sf Softmax}( X W_Q^i {W_K^i}^\top X^\top ) \cdot X W_V^i {W_O^i}^\top + X,
\end{align*}
\fi
where the projection weights $W_K^i, W_Q^i, W_V^i, W_O^i \in \R^{d_0 \times m}$. Moreover,
\begin{align*}
    {\sf FF}(X) := \phi(X W_1 + {\bf 1}_n b_1^\top) \cdot W_2^\top + {\bf 1}_n b_2^\top + X.
\end{align*}
where  the projection weights $W_1, W_2 \in \R^{d_0 \times r}$, bias $b_1 \in \R^{r}, b_2 \in \R^{d_0}$, and $\phi$ is usually considered as the ReLU activated function.

In our work, we use Transformer networks with positional encoding $E\in\R^{n \times d_0}$. The transformer networks are then defined as the composition of Transformer blocks:
\begin{align*}
    {\cal T}_{P}^{h,m,r} = & ~ \{f_{{\cal T}}:\R^{ n \times d_0 }\rightarrow {\R^{n \times d_0}} \\
    & ~ \mid f_{{\cal T}}\text{ is a composition of blocks }{\sf TF}^{h,m,r}\text{'s}\}.
\end{align*}
For example, the following is a Transformer network consisting $K$ blocks and positional encoding
\begin{align*}
f_{{\cal T}}(X)= {\sf FF}^{(K)} \circ {\sf Attn}^{(K)} \circ  \cdots {\sf FF}^{(1)} \circ  {\sf Attn}^{(1)} (X+E).
\end{align*}

\subsection{Approximation via DiT}\label{sub:approx}



Before we state the approximation theorem, we define a reshaped layer that transforms concatenated input in flow matching into a length-fixed sequence of vectors. It is denoted as $R: \R^{d+\ell+1} \rightarrow \R^{n \times d_0}$. Therefore, in the following, we give the theorem utilizing DiT to minimize training objective ${\cal L}(\theta)$ to arbitrary error.

\begin{theorem}[Informal version of Theorem~\ref{thm:uat}]\label{thm:uat:informal}
    There exists a transformer network $f_{\cal T} \in {\cal T}_{P}^{2, 1, 4}$ defining function $F_\theta(z, c, t) := f_{\cal T}( R([z^\top, c^\top, t]^\top) )$ with parameters $\theta$ that satisfies ${\cal L}(\theta) \leq \epsilon$ for any error $\epsilon > 0$. 
\end{theorem}

\begin{proof}[Proof sketch of Theorem~\ref{thm:uat:informal}]
    Please refer to the proof of Theorem~\ref{thm:uat} for the detailed analysis.
\end{proof}


\subsection{Interpolation and Extrapolation}\label{sub:inter-extra_polation_theory}

Now, we theoretically discuss the approximating error of our VLFM in processing interpolation and extrapolation. It is considered a recovery of the original idea data from limited sub-sampled observations. This analysis is achieved by splitting the error into three parts, which are: 1) approximating error $\epsilon_1$ for HiPPO-LegS approximating the original data; 2) Gaussian error $\epsilon_2$ for the boundary of Gaussian vector $z$; 3) interpolation and extrapolation error $\epsilon_3$ that represents the training and predicting the difference between using original idea data $V$ and limited sub-sampled observations $\Phi \wt{V}$. We state the results as follows:
\begin{lemma}[Informal version of Lemma~\ref{lem:hippo_error}]\label{lem:hippo_error:informal}
    Denote failure probability $\delta \in (0, 0.1)$. Let the flow $\psi_t( \wt{u} )$ defined in Eq.~\eqref{eq:psi}. Denote $G := [g(\Delta t), g(2 \Delta t), \cdots, g(T)]^\top \in \R^{\frac{T}{\Delta t} \times s}$ and $\lambda^* := \lambda_{\min}(G) > 0$ as the minimum eigenvalue of $G$. Choosing $s = O(\frac{\Delta t}{T}\log((\frac{\Delta t}{T})^{1.5}\lambda^*))$. Denote $u_t = {\cal D}( V_{t} )$ for any $t \in [0, T]$. Especially, we define:
    \begin{itemize}
        \item Approximating error $\epsilon_1 := O(T^{k} s^{-k+1/2})$.
        \item Gaussian error $\epsilon_2 := O(\sqrt{d\log(d/\delta)})$.
        \item Interpolation and extrapolation error $\epsilon_3 := U d^{0.5} \sqrt{\frac{T}{\Delta t} - N} \cdot \exp(O(\frac{T}{\Delta t}s)) / \lambda^*$.
    \end{itemize}
    Then with a probability at least $1 - \delta$, we have:
    \begin{align*}
        \| \psi_t( \wt{u} ) - u_t \|_2 \leq \epsilon_1 + \epsilon_2 + \epsilon_3.
    \end{align*}
\end{lemma}

\begin{proof}{Proof sketch of Lemma~\ref{lem:hippo_error:informal}}
    This proof follows from its formal version in Lemma~\ref{lem:hippo_error}
\end{proof}

Having Lemma~\ref{lem:hippo_error:informal}, the concise bound for solving Eq.~\eqref{eq:main} could be given below:
\begin{theorem}[Informal version of Theorem~\ref{thm:inter_extra_polation}]\label{thm:inter_extra_polation:informal}
    Following Theorem~\ref{thm:uat:informal}, denote failure probability $\delta \in (0, 0.1)$ and arbitrary error $\epsilon_0 > 0$. Then with a probability at least $1 - \delta$, the network in Theorem~\ref{thm:uat:informal} satisfies Eq.~\eqref{eq:main} with $p = 2$ and
    \begin{align*}
        \epsilon = \epsilon_0 + L_0(\epsilon_1 + \epsilon_2 + \epsilon_3).
    \end{align*}
\end{theorem}

\begin{proof}[Proof sketch of Theorem~\ref{thm:inter_extra_polation:informal}]
    Please refer to Theorem~\ref{thm:inter_extra_polation} for complete proofs.
\end{proof}

{\bf Discussions.} Following the results of Lemma~\ref{lem:hippo_error:informal} and Theorem~\ref{thm:inter_extra_polation:informal}, we thus derive few insights as follows:
\begin{itemize}
    \item {\bf Optimal choice of $s$: A trade-off between $\epsilon_1$ and $\epsilon_3$. } As shown in the conditions of Lemma~\ref{lem:hippo_error:informal}, the larger value of the order of polynomials $s$ helps to decrease approximating error in the training dataset while also ruining the generalization ability.
    \ifdefined\isarxiv
    \else
    \vspace{-2mm}
    \fi
    \item {\bf Stable visual decoder. } Theorem~\ref{thm:inter_extra_polation:informal} shows a small value of $L_0$ (the stability and smoothness of visual decoder), which is important for the error of interpolation and extrapolation with an arbitrary frame rate.
    \ifdefined\isarxiv
    \else
    \vspace{-3mm}
    \fi
    \item {\bf Information. } Besides, a sub-linear factor $\sqrt{\frac{T}{\Delta t} - N}$, which stands for the obtained information about the continuous video, is vital as well for interpolation and extrapolation on data in distribution.
\end{itemize}
\ifdefined\isarxiv
\else
\vspace{-6mm}
\fi

\subsection{Timescale Robustness}\label{sub:timescale_robustness}

Following \cite{gde+20}, we demonstrate that projection onto latent patches $u_t$ is robust to timescales. Formally, the HiPPO-LegS operator is {\it timescale-equivariant}: dilating the input $u$ does not change the approximation coefficients $H_N$. At the same time, this property is working in the case of the discretized form $\wt{u}$. We emphasize that it is crucial to use flow matching to model the latent patches, where whatever the sampling method and frame rate are, it will not greatly harm VLFM's performance. We give its formal statement below.

\begin{lemma}[Proposition 3 of \cite{gde+20}, informal version of Lemma~\ref{lem:timescale_robustness}]\label{lem:timescale_robustness:informal}
    For any integer scale factor $\beta > 0$, the frames of video $\wt{V}_\tau$ is scaled to $\wt{V}_{\beta \tau}$ for each $\tau \in [\frac{T}{\Delta t}]$, it doesnâ€™t affect the result of $H_N$.
\end{lemma}
\ifdefined\isarxiv
\else
\vspace{-3mm}
\fi
\begin{proof}
    This lemma follows from Proposition 3 in \cite{gde+20}.
\end{proof}
\ifdefined\isarxiv
\else
\vspace{-6mm}
\fi