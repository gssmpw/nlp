\section{Conclusion} \label{sec:conclusion}

This paper proposes {\it Video Latent Flow Matching} (VLFM) for efficient training of a time-varying flow to approximate the sequence of latent patches of the obtained video. This approach is confirmed to enjoy theoretical benefits, including 1) universal approximation theorem via applying Diffusion Transformer architecture and 2) optimal polynomial projections and timescale by introducing HiPPO-LegS. Furthermore, we provide the generalization error bound of VLFM that is trained only on the limited sub-sampled video to interpolate and extrapolate the whole ideal video. We evaluate our VLFM on Stable Diffusion v1.5 with DDIM scheduler and the DiT-XL-2 model with datasets OpenVid-1M,
UCF-101,
Kinetics-400,
YouTube-8M,
InternVid,
MiraData, and
Pixabay. The experimental results validated the potential of our approach to become a novel and efficient training form for text-to-video generation.

{\bf Limitations. } Since the motivation of this paper focuses on simply and efficiently solving the main goal, it lacks enough exploring each design and how it affects the empirical performance, providing little insights for the follow-ups. Hence, we leave these comprehensive explorations, and its more concise theoretical working mechanism behind as future works. On the other hand, although VLFM simplifies the video modeling process, it necessitates additional computational consumption concerning the combination of the visual decoder part and the flow matching part at the inference stage. We also leave such exploration to a more efficient inference method as a future direction.