\ifdefined\isarxiv
\else
\vspace{-8mm}
\fi

\begin{figure*}[!ht]
\begin{center}
\centering
    \includegraphics[width=0.9\textwidth]{vlfm.pdf}
\end{center}
\caption{Illustration of the working mechanism behind {\it Video Latent Flow Matching}.}
\label{fig:vlfm}
\end{figure*}

\section{Introduction}

The rise of generative models has already demonstrated excellent performance in various fields like image generation \cite{scs+22,rbl+22}, text generation \cite{aaa23,djp+24,lfx+24}, video generation \cite{bph+24,zpy+24,jsl+24,tjy+24}, etc. \cite{s23}. Among them, some of the most popular algorithms - Flow Matching \cite{lcb+22,lgl22}, Diffusion \cite{hja20,sme20} and VAEs \cite{kw13}, perform surprise generative capabilities, however, requiring comprehensive computational resources for training. In particular, this efficiency drawback harms the development of more successful text-to-video modeling \cite{bph+24}, becoming a frontier challenge in the field of generative modeling.

The prior works about the generation from textual descriptions to realistic and coherent videos usually involve two strong pre-trained networks \cite{hsg+22,zpy+24}. One encodes input captions to rich embedding representations, and another one decodes from sequences of latent patches (also considered as Gaussian noise) under the guidance of text embedding representations. Although variants based on such modeling processes are already showing some fine initial results, the necessity of training on large-scale models and datasets leads these studies to be undemocratic \cite{bph+24,ktz+24}. In response to this issue, the motivation of this paper is to design a novel algorithm to simplify the process of text-to-video modeling.

In this paper, we propose \emph{Video Latent Flow Matching} (VLFM), which relies on the most advanced pre-trained image generation models (we call visual decoder in the range of this paper) for their extension in the field of text-to-video generation. In detail, we first introduce a deterministic inversion algorithm \cite{sme20,lcb+22,lgl22} to the visual decoder and apply this inversion operation to the frames of all videos, obtaining a sequence including initial latent patches from each video. Thus, the base of this paper is that a sequence of latent patches is a time-dependent and caption-conditional flow, so-called {\it Video Latent Flow}. Therefore, we use Flow Matching \cite{lcb+22,lgl22} to model it. 

Especially, we emphasize four advantages of our VLFM:
\begin{itemize}
    \item {\bf Modeling efficiency. } The modeling of VLFM only needs to fit ${\sf N}$ flows where ${\sf N}$ is the size of the training dataset. This computational requirement is close to training a text-to-image model.
    \item {\bf Optimal polynomial projections. } We use discrete HiPPO LegS to generate the time-dependent flow with provable optimal polynomial projections. The approximating error decreases with the enlarging order of polynomials.
    \item {\bf Arbitrary frame rate. } The reason for applying Flow Matching instead of other approaches is that it allows solving ODE with arbitrary time $t$. This further leads to precise video generation with high frame rates.
    \item {\bf Interpolation and extrapolation. } Besides, VLFM is suitable for interpolation and extrapolation for high-precision video recovery and generation since its generalization performance is confirmed in our theoretical part.
\end{itemize}
\ifdefined\isarxiv
\else
\vspace{-3mm}
\fi

In summary, we make the following contributions:
\begin{itemize}
    \item We give this paper's preliminary as a theoretical background with several mild assumptions in Section~\ref{sec:preli}. Hence, we state the derivation of our VLFM in Section~\ref{sec:vlfm}, which introduces the HiPPO framework to online approximate the sequence of latent patches.
    \item The theoretical benefits of VLFM are shown in Section~\ref{sec:theory}. We first utilize the universal approximation theorem of Diffusion Transformer (DiT) to ensure an appropriate learner for modeling. The approximation bound then is guaranteed. We also discuss how our VLFM processes interpolation and extrapolation to real-world videos with an upper bound on error and its timescale robustness.
    \item We validate our approach by conducting extensive experiments in Section~\ref{sec:exp}. Our model leverages DiT-XL-2 and is trained on a diverse collection of seven large-scale video datasets, including OpenVid-1M, MiraData, and videos from Pixabay. The results demonstrate strong performance in text-to-video generation, interpolation, and extrapolation, achieving robust and reliable outputs with significant potential for real-world video applications.
\end{itemize}

