\section{Related Work}
This section briefly reviews three topics that are closely related to this work: Text-to-Video Generation, Flow Matching, and Theory in Transformer-Based Models. 

{\bf Text-to-Video Generation.}
Text-to-video generation ____ is a specialized form of conditional video generation that aims to synthesize high-quality videos from textual descriptions. Recent advancements in this field have predominantly leveraged diffusion models ____, which iteratively refine video frames by learning to denoise samples from a normal distribution. This approach has proven effective in generating coherent and visually appealing videos.
Training strategies for text-to-video models vary widely. One common approach involves adapting pre-trained text-to-image models by incorporating temporal modules, such as temporal convolutions and attention mechanisms, to establish inter-frame relationships ____. For instance, PYoCo ____ introduced a noise prior technique and utilized the pre-trained eDiff-I model ____ as a starting point. Alternatively, some methods build on Stable Diffusion ____, leveraging its accessibility and pre-trained capabilities to expedite convergence ____. However, this approach can sometimes result in suboptimal outcomes due to the inherent distributional differences between images and videos. Another strategy involves training models from scratch on combined image and video datasets ____, which can yield superior results while requiring intensive computationally.
\ifdefined\isarxiv
\else
\vspace{-2mm}
\fi

{\bf Flow Matching.}
Flow Matching has emerged as a highly effective framework for generative modeling, demonstrating significant advancements across various domains, including video generation. Its simplicity and power have been validated in large-scale generation tasks such as image ____, video ____, speech ____,  audio ____, proteins ____, and robotics ____. Flow Matching originated from efforts to address the computational challenges associated with Continuous Normalizing Flows (CNFs), where early methods struggled with simulation inefficiencies ____. Modern Flow Matching algorithms ____ have since evolved to learn CNFs without explicit simulation, significantly improving scalability. Recent innovations, such as Discrete Flow Matching ____, have further expanded the applicability of this framework, making it a versatile tool for generative tasks.

{\bf Theory in Transformer-Based Models.}
Transformers have become a cornerstone in AI and are widely used in different areas, especially in NLP (Natural Language Process) and CV (Computer Vision). However, understanding the Transformers from a theoretical perspective remains an ongoing challenge. Several works have explored the theoretical foundations and computational complexities of the Transformers 
\ifdefined\isarxiv
____, 
\else
____
\fi
focusing on areas such as efficient Transformers ____, optimization ____, and the analysis of emergent abilities ____. Notably, ____ introduced an algorithm with provable guarantees for approximation of Transformers, ____ proved a lower bound for Transformers based on the Strong Exponential Time Hypothesis, and ____ provided both an algorithm and hardness results for static Transformers computation.
\ifdefined\isarxiv
\else
\vspace{-3mm}
\fi