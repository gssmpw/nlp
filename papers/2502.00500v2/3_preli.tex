\section{Preliminary}\label{sec:preli}

In this section, we formalize the background of this paper. We first introduce how we invert video frames into some latent space using the strong pre-trained visual decoder in Section~\ref{sub:data}. We state the definition of data and assumption in Section~\ref{sub:assumptions}. Section~\ref{sub:problem_def} defines the main problem we aim to address in this paper. We use integer $s$ to denote the order of polynomials. The dimensional number of the text embedding vector is given by integer $\ell$.

\subsection{Inverting Video Frames to Latent Patches}\label{sub:data}

\paragraph{Notations.} We use $D$ to denote the flattened dimension of real-world images. We use $d$ to represent the dimension of latent patches. We introduce $d_0$ as the dimension of Diffusion Transformers. We utilize $V: [0, T] \rightarrow \R^D$ to denote a video with $T$ duration, where $T$ is the longest time for each video. We omit $\nabla_t a(t)$ and $a'(t)$ to denote taking differentiation to some function $a(t)$ w.r.t. time $t$.

\paragraph{Visual decoder.} Here we denote the visual decoder ${\cal D}: \R^d \rightarrow \R^D$ satisfies that: For any flattened image $V \in \R^D$, there is a unique $u \in \R^d$ such that ${\cal D}(u) = V$. Then we say ${\cal D}$ is bijective. Denote the reverse function of ${\cal D}$ as ${\cal D}^{-1}: \R^D \rightarrow \R^d$. Note that this visual decoder ${\cal D}$ could be considered as any generative algorithm practically, e.g. LDM \cite{rbl+22}, DDIM \cite{sme20} and VAE \cite{k13}. We thus implement an inversion algorithm to invert video frames to latent patches \cite{mha+23}. In particular, we define these latent patches here, which depend on the detailed visual decoder. We consider these latent patches following arbitrary distribution.

We abuse the notation $u: [0, T] \rightarrow \R^d$ to denote a sequence of latent patches of a video $V$. In detail, we define: $u_t := {\cal D}^{-1}(V_t)$ for any $t \in [0, T]$.

\paragraph{Discretization for cases of real-world data.} We denote $\Delta t$ as the minimal time unit of measurement in the real world (Planck time). Hence, a video $V$ with $T$ duration can be divided into at most $\frac{T}{\Delta t}$ frames. We use matrix $\wt{V} \in \R^{\frac{T}{\Delta t} \times D}$ to denote the compact form of discretized video. We use $\Phi \in \{0, 1\}^{\frac{T}{\Delta t} \times N}$ for $N \leq \frac{T}{\Delta t}$ to denote the corresponding observation matrix due to the real-world consideration, especially $\Phi^\top {\bf 1}_{\frac{T}{\Delta t}} = {\bf 1}_{N}$. Then the practical form of latent patches is given by:
\begin{align}\label{eq:u_tau:informal}
    \wt{u}_\tau := {\cal D}^{-1}([ \Phi \wt{V} ]_\tau) \in \R^{d}, \forall \tau \in [N].
\end{align}

\subsection{Data and Assumptions}\label{sub:assumptions}

\paragraph{Caption-video data pairs.} Given a video distribution ${\cal V}$, we introduce a text embedding state distribution ${\cal C}$ that maps one-to-one to ${\cal V}$. Then for any video data $V \sim {\cal V}$, $c \in \R^{\ell}$ is denoted as the corresponding caption embedding state vector. We use ${\cal V}_c$ to denote the distribution that contains video and embedding vector, such that $(V, c) \sim {\cal V}_c$.

\paragraph{Assumptions.} Here we list several mild assumptions in this paper, such that:
\begin{itemize}
    \item {\bf $k$-differentiable latent patches $u$. } We assume $u: [0, T] \rightarrow \R^d$ is a differentiable function with order $k$. 
    \item {\bf Lipschitz smooth visual decoder function ${\cal D}$. } We assume the visual decoder function ${\cal D}$ is $L_0$-smooth for constant $L_0 > 0$. Formally, it is: $\| {\cal D}(x) - {\cal D}(y) \|_2 \leq L_0 \cdot \| x - y\|_2, \forall x, y \in \R^d$.
    \item {\bf Bounded entries of $u$.} For each entry in latent patches $u$, we assume it is smaller than a upper bound $U$ for some constant $U > 0$.
    \item {\bf Caption-to-latency function. } For any video-caption data $(V, c) \sim {\cal V}_c$, there exists a function ${\cal M}: [0, T] \times \R^\ell \rightarrow \R^D$ satisfies $V_t = {\cal M}_t(c)$. 
\end{itemize}


\subsection{Problem definition: Modeling Text-to-Latency Data from Discretized video}\label{sub:problem_def}

In this paper, we consider the video modeling problems as follows:
\begin{itemize}
    \item 
    Given a video-caption pair $({\cal V}, c) \sim {\cal V}_c$, we obtain the data $\wt{u}_\tau \in \R^d, \forall \tau \in [N]$ via Eq.~\eqref{eq:u_tau:informal}, we aim to find a algorithm that inputs a time $t \in [0, T]$ and encoded text state vector $c \in \R^{\ell}$ and output a predicted latent patch $\hat{u}_t \in \R^d$, it satisfies: 
    \begin{align}\label{eq:main}
        \| {\cal D}(\hat{u}_t) - V_t \|_p \leq \epsilon.
    \end{align}
    Here we denote the error $\epsilon \ge 0$ and some $\ell_p$ norm metric.
\end{itemize}

\paragraph{Connecting the main problem to interpolation and extrapolation.} Since the frames number $N$ of obtained video data may be greatly smaller than $T/\Delta t$. Recovering the continuous video data $T$ as completely as possible (both interpolation and extrapolation) would also be our goal in the range of this paper. Theoretically, we see such interpolation and extrapolation as one: given a discrete video data $\Phi \wt{V} \in \R^{N \times D}$, the sequence of latent patches is $\wt{u} = [\wt{u}_1^\top, \wt{u}_2^\top, \cdots, \wt{u}_N^\top] \in \R^{N \times d}$ using Eq.~\eqref{eq:u_tau:informal}. The text embedding state vector $c \in \R^{\ell}$ could be ensured by some video-to-caption methods. Our target is to find an algorithm that inputs $\wt{u}$ and outputs $\hat{u}_{\tau}, \tau \in [T/\Delta t]$ that meets the requirement: $\| {\cal D}(\hat{u}_\tau) - \wt{V}_\tau \|_p \leq \epsilon$ for error $\epsilon \ge 0$ and some $\ell_p$ norm metric.
