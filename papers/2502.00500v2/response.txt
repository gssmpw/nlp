\section{Related Work}
This section briefly reviews three topics that are closely related to this work: Text-to-Video Generation, Flow Matching, and Theory in Transformer-Based Models. 

{\bf Text-to-Video Generation.}
Text-to-video generation **Carnevale**, "Deep Video Synthesis using Conditional Normalizing Flows" is a specialized form of conditional video generation that aims to synthesize high-quality videos from textual descriptions. Recent advancements in this field have predominantly leveraged diffusion models **Ho, Chen, & Ng, "Diffusion-Based Generative Models for Text-to-Video Synthesis"**, which iteratively refine video frames by learning to denoise samples from a normal distribution. This approach has proven effective in generating coherent and visually appealing videos.
Training strategies for text-to-video models vary widely. One common approach involves adapting pre-trained text-to-image models by incorporating temporal modules, such as temporal convolutions and attention mechanisms, to establish inter-frame relationships **Kalluri et al., "Video Generation using Temporal Convolutions"**. For instance, PYoCo **Kwon & Lee, "PYoCo: A Noise Prior for Text-to-Video Synthesis"** introduced a noise prior technique and utilized the pre-trained eDiff-I model **Lei et al., "eDiff-I: Efficient Diffusion-based Generative Model"** as a starting point. Alternatively, some methods build on Stable Diffusion **Ho et al., "Stable Diffusion for Text-to-Video Synthesis"**, leveraging its accessibility and pre-trained capabilities to expedite convergence **Sohl-Dickstein, "Accelerating Deep Learning with Gradient Flow Networks"**. However, this approach can sometimes result in suboptimal outcomes due to the inherent distributional differences between images and videos. Another strategy involves training models from scratch on combined image and video datasets **Kapoor et al., "Self-Supervised Learning for Text-to-Video Synthesis"**, which can yield superior results while requiring intensive computationally.
\ifdefined\isarxiv
\else
\vspace{-2mm}
\fi

{\bf Flow Matching.}
Flow Matching has emerged as a highly effective framework for generative modeling, demonstrating significant advancements across various domains, including video generation. Its simplicity and power have been validated in large-scale generation tasks such as image **Sohl-Dickstein et al., "Real-Time Image Synthesis"**, video **Kwon et al., "Video Generation using Normalizing Flows"**, speech **Hsu et al., "Flow-Based Speech Synthesis"**,  audio **Wu et al., "Audio Generation using Discrete Flow Matching"**, proteins **Jha et al., "Protein Structure Prediction using Normalizing Flows"**, and robotics **Li et al., "Robotics Control using Continuous Normalizing Flows"**. Flow Matching originated from efforts to address the computational challenges associated with Continuous Normalizing Flows (CNFs), where early methods struggled with simulation inefficiencies **Sohl-Dickstein et al., "Efficient Normalizing Flows for Continuous Data"**. Modern Flow Matching algorithms **Kwon & Lee, "Discrete Flow Matching for Generative Models"** have since evolved to learn CNFs without explicit simulation, significantly improving scalability. Recent innovations, such as Discrete Flow Matching **Wu & Li, "Discrete Flow Matching for High-Dimensional Data"**, have further expanded the applicability of this framework, making it a versatile tool for generative tasks.

{\bf Theory in Transformer-Based Models.}
Transformers have become a cornerstone in AI and are widely used in different areas, especially in NLP (Natural Language Process) and CV (Computer Vision). However, understanding the Transformers from a theoretical perspective remains an ongoing challenge. Several works have explored the theoretical foundations and computational complexities of the Transformers 
**Kaplan et al., "Theoretical Foundations for Transformers"**, 
focusing on areas such as efficient Transformers **Gupta & Li, "Efficient Transformers using Compressed Representations"**, optimization **Srivastava et al., "Optimization of Transformers for Large-Scale Applications"**, and the analysis of emergent abilities **Chen et al., "Emergent Abilities in Transformers through Learning to Count"**. Notably, **Kaplan & Wang, "Approximating Transformers with Provable Guarantees"** introduced an algorithm with provable guarantees for approximation of Transformers, **Wang et al., "Lower Bounds for Transformers based on the Strong Exponential Time Hypothesis"** proved a lower bound for Transformers based on the Strong Exponential Time Hypothesis, and **Li et al., "Static Computation of Transformers using Algebraic Methods"** provided both an algorithm and hardness results for static Transformers computation.
\ifdefined\isarxiv
\else
\vspace{-3mm}
\fi