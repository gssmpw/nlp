\section{Related Work}

This section briefly reviews three topics that are closely related to this work: Text-to-Video Generation, Flow Matching, and Theory in Transformer-Based Models. 

{\bf Text-to-Video Generation.}
Text-to-video generation \cite{sph+22, vjp22, brl+23} is a specialized form of conditional video generation that aims to synthesize high-quality videos from textual descriptions. Recent advancements in this field have predominantly leveraged diffusion models \cite{ssk+20,hja20}, which iteratively refine video frames by learning to denoise samples from a normal distribution. This approach has proven effective in generating coherent and visually appealing videos.
Training strategies for text-to-video models vary widely. One common approach involves adapting pre-trained text-to-image models by incorporating temporal modules, such as temporal convolutions and attention mechanisms, to establish inter-frame relationships \cite{gnl+23, azy+23, sph+22, gwz+23, gyr+23}. For instance, PYoCo \cite{gnl+23} introduced a noise prior technique and utilized the pre-trained eDiff-I model \cite{bnh+22} as a starting point. Alternatively, some methods build on Stable Diffusion \cite{rbl+22}, leveraging its accessibility and pre-trained capabilities to expedite convergence \cite{brl+23,zwy+22}. However, this approach can sometimes result in suboptimal outcomes due to the inherent distributional differences between images and videos. Another strategy involves training models from scratch on combined image and video datasets \cite{hcs+22}, which can yield superior results while requiring intensive computationally.
\ifdefined\isarxiv
\else
\vspace{-2mm}
\fi

{\bf Flow Matching.}
Flow Matching has emerged as a highly effective framework for generative modeling, demonstrating significant advancements across various domains, including video generation. Its simplicity and power have been validated in large-scale generation tasks such as image \cite{ekb+24}, video \cite{pzb+24,jsl+24}, speech \cite{lvs+24},  audio \cite{vsl+23}, proteins \cite{hvf+24}, and robotics \cite{bbd+24}. Flow Matching originated from efforts to address the computational challenges associated with Continuous Normalizing Flows (CNFs), where early methods struggled with simulation inefficiencies \cite{rgnl21,bcba+22}. Modern Flow Matching algorithms \cite{lcb+22,lgl22,av22,nbsm23,hbc23,tfm+23} have since evolved to learn CNFs without explicit simulation, significantly improving scalability. Recent innovations, such as Discrete Flow Matching \cite{cyb+24,grs+24}, have further expanded the applicability of this framework, making it a versatile tool for generative tasks.

{\bf Theory in Transformer-Based Models.}
Transformers have become a cornerstone in AI and are widely used in different areas, especially in NLP (Natural Language Process) and CV (Computer Vision). However, understanding the Transformers from a theoretical perspective remains an ongoing challenge. Several works have explored the theoretical foundations and computational complexities of the Transformers 
\ifdefined\isarxiv
\cite{tby+19,zhdk23,bsz23,as24,syz24,cll+24_rope,hlsl24,mosw22,szz24,als19_icml,dhs+22,bpsw21,syz21,als+23,dsxy23,kls+24,lswy23,gls+24}, 
\else
\cite{tby+19,zhdk23,bsz23,as24,syz24,cll+24_rope,hlsl24,mosw22,szz24,als19_icml,dhs+22,bpsw21,syz21,als+23,dsxy23}
\fi
focusing on areas such as efficient Transformers \cite{hjk+23,smn+24,szz+21,lll21,lls+24_conv,lssz24_tat,lss+24,llss24_sparse,lls+24_prune,cls+24,lls+24_io,hwsl24,hwl24,hcl+24,whhl24,hyw+23,as24_arxiv,gswy23}, optimization \cite{dls23, csy24}, and the analysis of emergent abilities \cite{bmr+20,wtb+22,al23,j23,xsw+23,lls+24_grok,xsl24,cll+24,lss+24_relu,hwg+24,wsh+24, dsy24_b}. Notably, \cite{zhdk23, dsy24_a} introduced an algorithm with provable guarantees for approximation of Transformers, \cite{kwh23} proved a lower bound for Transformers based on the Strong Exponential Time Hypothesis, and \cite{as24} provided both an algorithm and hardness results for static Transformers computation.
\ifdefined\isarxiv
\else
\vspace{-3mm}
\fi