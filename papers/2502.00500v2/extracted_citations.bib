@article{al23,
  title={Physics of language models: Part 1, context-free grammar},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2305.13673},
  year={2023}
}

@article{als+23,
  title={Bypass exponential time preprocessing: Fast neural network training via weight-data correlation preprocessing},
  author={Alman, Josh and Song, Zhao and Zhang, Ruizhe and Zhuo, Danyang},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@inproceedings{als19_icml,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle={International conference on machine learning},
  pages={242--252},
  year={2019},
  organization={PMLR}
}

@article{as24,
  title={Fast attention requires bounded entries},
  author={Alman, Josh and Song, Zhao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{as24_arxiv,
  title={The fine-grained complexity of gradient computation for training large language models},
  author={Alman, Josh and Song, Zhao},
  journal={arXiv preprint arXiv:2402.04497},
  year={2024}
}

@article{av22,
  title={Building normalizing flows with stochastic interpolants},
  author={Albergo, Michael S and Vanden-Eijnden, Eric},
  journal={arXiv preprint arXiv:2209.15571},
  year={2022}
}

@article{azy+23,
  title={Latent-shift: Latent diffusion with temporal shift for efficient text-to-video generation},
  author={An, Jie and Zhang, Songyang and Yang, Harry and Gupta, Sonal and Huang, Jia-Bin and Luo, Jiebo and Yin, Xi},
  journal={arXiv preprint arXiv:2304.08477},
  year={2023}
}

@article{bbd+24,
  title={$\pi_0$: A Vision-Language-Action Flow Model for General Robot Control},
  author={Black, Kevin and Brown, Noah and Driess, Danny and Esmail, Adnan and Equi, Michael and Finn, Chelsea and Fusai, Niccolo and Groom, Lachy and Hausman, Karol and Ichter, Brian and others},
  journal={arXiv preprint arXiv:2410.24164},
  year={2024}
}

@article{bcba+22,
  title={Matching normalizing flows and probability paths on manifolds},
  author={Ben-Hamu, Heli and Cohen, Samuel and Bose, Joey and Amos, Brandon and Grover, Aditya and Nickel, Maximilian and Chen, Ricky TQ and Lipman, Yaron},
  journal={arXiv preprint arXiv:2207.04711},
  year={2022}
}

@article{bmr+20,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{bnh+22,
  title={ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers},
  author={Balaji, Yogesh and Nah, Seungjun and Huang, Xun and Vahdat, Arash and Song, Jiaming and Zhang, Qinsheng and Kreis, Karsten and Aittala, Miika and Aila, Timo and Laine, Samuli and others},
  journal={arXiv preprint arXiv:2211.01324},
  year={2022}
}

@article{bpsw21,
  title={Training (Overparametrized) Neural Networks in Near-Linear Time},
  author={van den Brand, Jan and Peng, Binghui and Song, Zhao and Weinstein, Omri},
  journal={ITCS},
  year={2021}
}

@inproceedings{brl+23,
  title={Align your latents: High-resolution video synthesis with latent diffusion models},
  author={Blattmann, Andreas and Rombach, Robin and Ling, Huan and Dockhorn, Tim and Kim, Seung Wook and Fidler, Sanja and Kreis, Karsten},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={22563--22575},
  year={2023}
}

@article{bsz23,
  title={Algorithm and hardness for dynamic attention maintenance in large language models},
  author={Brand, Jan van den and Song, Zhao and Zhou, Tianyi},
  journal={arXiv preprint arXiv:2304.02207},
  year={2023}
}

@article{cll+24,
  title={Circuit Complexity Bounds for RoPE-based Transformer Architecture},
  author={Chen, Bo and Li, Xiaoyu and Liang, Yingyu and Long, Jiangxuan and Shi, Zhenmei and Song, Zhao},
  journal={arXiv preprint arXiv:2411.07602},
  year={2024}
}

@article{cll+24_rope,
  title={Circuit Complexity Bounds for RoPE-based Transformer Architecture},
  author={Chen, Bo and Li, Xiaoyu and Liang, Yingyu and Long, Jiangxuan and Shi, Zhenmei and Song, Zhao},
  journal={arXiv preprint arXiv:2411.07602},
  year={2024}
}

@article{cls+24,
  title={Hsr-enhanced sparse attention acceleration},
  author={Chen, Bo and Liang, Yingyu and Sha, Zhizhou and Shi, Zhenmei and Song, Zhao},
  journal={arXiv preprint arXiv:2410.10165},
  year={2024}
}

@inproceedings{csy24,
  title={How to Protect Copyright Data in Optimization of Large Language Models?},
  author={Chu, Timothy and Song, Zhao and Yang, Chiwun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={16},
  pages={17871--17879},
  year={2024}
}

@article{cyb+24,
  title={Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design},
  author={Campbell, Andrew and Yim, Jason and Barzilay, Regina and Rainforth, Tom and Jaakkola, Tommi},
  journal={arXiv preprint arXiv:2402.04997},
  year={2024}
}

@article{dhs+22,
  title={Training overparametrized neural networks in sublinear time},
  author={Deng, Yichuan and Hu, Hang and Song, Zhao and Weinstein, Omri and Zhuo, Danyang},
  journal={arXiv preprint arXiv:2208.04508},
  year={2022}
}

@article{dls23,
  title={Attention scheme inspired softmax regression},
  author={Deng, Yichuan and Li, Zhihang and Song, Zhao},
  journal={arXiv preprint arXiv:2304.10411},
  year={2023}
}

@article{dsxy23,
  title={Unmasking Transformers: A Theoretical Approach to Data Recovery via Attention Weights},
  author={Deng, Yichuan and Song, Zhao and Xie, Shenghao and Yang, Chiwun},
  journal={arXiv preprint arXiv:2310.12462},
  year={2023}
}

@article{dsy24_a,
  title={Unlocking the Theory Behind Scaling 1-Bit Neural Networks},
  author={Daliri, Majid and Song, Zhao and Yang, Chiwun},
  journal={arXiv preprint arXiv:2411.01663},
  year={2024}
}

@article{dsy24_b,
  title={Attention is Naturally Sparse with Gaussian Distributed Input},
  author={Deng, Yichuan and Song, Zhao and Yang, Chiwun},
  journal={arXiv preprint arXiv:2404.02690},
  year={2024}
}

@inproceedings{ekb+24,
  title={Scaling rectified flow transformers for high-resolution image synthesis},
  author={Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and M{\"u}ller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and others},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{gls+24,
  title={Toward Infinite-Long Prefix in Transformer},
  author={Gu, Jiuxiang and Liang, Yingyu and Shi, Zhenmei and Song, Zhao and Yang, Chiwun},
  journal={arXiv preprint arXiv:2406.14036},
  year={2024}
}

@inproceedings{gnl+23,
  title={Preserve your own correlation: A noise prior for video diffusion models},
  author={Ge, Songwei and Nah, Seungjun and Liu, Guilin and Poon, Tyler and Tao, Andrew and Catanzaro, Bryan and Jacobs, David and Huang, Jia-Bin and Liu, Ming-Yu and Balaji, Yogesh},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={22930--22941},
  year={2023}
}

@article{grs+24,
  title={Discrete flow matching},
  author={Gat, Itai and Remez, Tal and Shaul, Neta and Kreuk, Felix and Chen, Ricky TQ and Synnaeve, Gabriel and Adi, Yossi and Lipman, Yaron},
  journal={arXiv preprint arXiv:2407.15595},
  year={2024}
}

@article{gswy23,
  title={A fast optimization view: Reformulating single layer attention in llm based on tensor and svm trick, and solving it in matrix multiplication time},
  author={Gao, Yeqi and Song, Zhao and Wang, Weixin and Yin, Junze},
  journal={arXiv preprint arXiv:2309.07418},
  year={2023}
}

@article{gwz+23,
  title={Reuse and diffuse: Iterative denoising for text-to-video generation},
  author={Gu, Jiaxi and Wang, Shicong and Zhao, Haoyu and Lu, Tianyi and Zhang, Xing and Wu, Zuxuan and Xu, Songcen and Zhang, Wei and Jiang, Yu-Gang and Xu, Hang},
  journal={arXiv preprint arXiv:2309.03549},
  year={2023}
}

@article{gyr+23,
  title={Animatediff: Animate your personalized text-to-image diffusion models without specific tuning},
  author={Guo, Yuwei and Yang, Ceyuan and Rao, Anyi and Liang, Zhengyang and Wang, Yaohui and Qiao, Yu and Agrawala, Maneesh and Lin, Dahua and Dai, Bo},
  journal={arXiv preprint arXiv:2307.04725},
  year={2023}
}

@inproceedings{hbc23,
  title={Iterative $\alpha$-(de) blending: A minimalist deterministic diffusion model},
  author={Heitz, Eric and Belcour, Laurent and Chambon, Thomas},
  booktitle={ACM SIGGRAPH 2023 Conference Proceedings},
  pages={1--8},
  year={2023}
}

@article{hcl+24,
  title={Outlier-efficient hopfield layers for large transformer-based models},
  author={Hu, Jerry Yao-Chieh and Chang, Pei-Hsuan and Luo, Robin and Chen, Hong-Yu and Li, Weijian and Wang, Wei-Po and Liu, Han},
  journal={arXiv preprint arXiv:2404.03828},
  year={2024}
}

@article{hcs+22,
  title={Imagen video: High definition video generation with diffusion models},
  author={Ho, Jonathan and Chan, William and Saharia, Chitwan and Whang, Jay and Gao, Ruiqi and Gritsenko, Alexey and Kingma, Diederik P and Poole, Ben and Norouzi, Mohammad and Fleet, David J and others},
  journal={arXiv preprint arXiv:2210.02303},
  year={2022}
}

@article{hja20,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}

@article{hjk+23,
  title={Hyperattention: Long-context attention in near-linear time},
  author={Han, Insu and Jayaram, Rajesh and Karbasi, Amin and Mirrokni, Vahab and Woodruff, David P and Zandieh, Amir},
  journal={arXiv preprint arXiv:2310.05869},
  year={2023}
}

@article{hlsl24,
  title={On computational limits of modern hopfield models: A fine-grained complexity analysis},
  author={Hu, Jerry Yao-Chieh and Lin, Thomas and Song, Zhao and Liu, Han},
  journal={arXiv preprint arXiv:2402.04520},
  year={2024}
}

@article{hvf+24,
  title={Sequence-Augmented SE (3)-Flow Matching For Conditional Protein Backbone Generation},
  author={Huguet, Guillaume and Vuckovic, James and Fatras, Kilian and Thibodeau-Laufer, Eric and Lemos, Pablo and Islam, Riashat and Liu, Cheng-Hao and Rector-Brooks, Jarrid and Akhound-Sadegh, Tara and Bronstein, Michael and others},
  journal={arXiv preprint arXiv:2405.20313},
  year={2024}
}

@article{hwg+24,
  title={Fundamental limits of prompt tuning transformers: Universality, capacity and efficiency},
  author={Hu, Jerry Yao-Chieh and Wang, Wei-Po and Gilani, Ammar and Li, Chenyang and Song, Zhao and Liu, Han},
  journal={arXiv preprint arXiv:2411.16525},
  year={2024}
}

@article{hwl24,
  title={Provably optimal memory capacity for modern hopfield models: Transformer-compatible dense associative memories as spherical codes},
  author={Hu, Jerry Yao-Chieh and Wu, Dennis and Liu, Han},
  journal={arXiv preprint arXiv:2410.23126},
  year={2024}
}

@article{hwsl24,
  title={On statistical rates and provably efficient criteria of latent diffusion transformers (dits)},
  author={Hu, Jerry Yao-Chieh and Wu, Weimin and Song, Zhao and Liu, Han},
  journal={arXiv preprint arXiv:2407.01079},
  year={2024}
}

@article{hyw+23,
  title={On sparse modern hopfield model},
  author={Hu, Jerry Yao-Chieh and Yang, Donglin and Wu, Dennis and Xu, Chenwei and Chen, Bo-Yu and Liu, Han},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@article{j23,
  title={A latent space theory for emergent abilities in large language models},
  author={Jiang, Hui},
  journal={arXiv preprint arXiv:2304.09960},
  year={2023}
}

@article{jsl+24,
  title={Pyramidal flow matching for efficient video generative modeling},
  author={Jin, Yang and Sun, Zhicheng and Li, Ningyuan and Xu, Kun and Jiang, Hao and Zhuang, Nan and Huang, Quzhe and Song, Yang and Mu, Yadong and Lin, Zhouchen},
  journal={arXiv preprint arXiv:2410.05954},
  year={2024}
}

@article{kls+24,
  title={Curse of attention: A kernel-based perspective for why transformers fail to generalize on time series forecasting and beyond},
  author={Ke, Yekun and Liang, Yingyu and Shi, Zhenmei and Song, Zhao and Yang, Chiwun},
  journal={arXiv preprint arXiv:2412.06061},
  year={2024}
}

@inproceedings{kwh23,
  title={On the computational complexity of self-attention},
  author={Keles, Feyza Duman and Wijewardena, Pruthuvi Mahesakya and Hegde, Chinmay},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={597--619},
  year={2023},
  organization={PMLR}
}

@article{lcb+22,
  title={Flow matching for generative modeling},
  author={Lipman, Yaron and Chen, Ricky TQ and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matt},
  journal={arXiv preprint arXiv:2210.02747},
  year={2022}
}

@article{lgl22,
  title={Flow straight and fast: Learning to generate and transfer data with rectified flow},
  author={Liu, Xingchao and Gong, Chengyue and Liu, Qiang},
  journal={arXiv preprint arXiv:2209.03003},
  year={2022}
}

@article{lll21,
  title={Research progress in attention mechanism in deep learning},
  author={LIU, Jian-wei and LIU, Jun-wen and LUO, Xiong-lin},
  journal={Chinese Journal of Engineering},
  volume={43},
  number={11},
  pages={1499--1511},
  year={2021},
  publisher={Chinese Journal of Engineering Editorial Office}
}

@article{lls+24_conv,
  title={Conv-basis: A new paradigm for efficient attention inference and gradient computation in transformers},
  author={Liang, Yingyu and Liu, Heshan and Shi, Zhenmei and Song, Zhao and Xu, Zhuoyan and Yin, Junze},
  journal={arXiv preprint arXiv:2405.05219},
  year={2024}
}

@article{lls+24_grok,
  title={Fourier circuits in neural networks: Unlocking the potential of large language models in mathematical reasoning and modular arithmetic},
  author={Li, Chenyang and Liang, Yingyu and Shi, Zhenmei and Song, Zhao and Zhou, Tianyi},
  journal={arXiv preprint arXiv:2402.09469},
  year={2024}
}

@article{lls+24_io,
  title={Fine-grained attention i/o complexity: Comprehensive analysis for backward passes},
  author={Li, Xiaoyu and Liang, Yingyu and Shi, Zhenmei and Song, Zhao and Zhou, Yufa},
  journal={arXiv preprint arXiv:2410.09397},
  year={2024}
}

@article{lls+24_prune,
  title={Beyond linear approximations: A novel pruning approach for attention matrix},
  author={Liang, Yingyu and Long, Jiangxuan and Shi, Zhenmei and Song, Zhao and Zhou, Yufa},
  journal={arXiv preprint arXiv:2410.11261},
  year={2024}
}

@article{llss24_sparse,
  title={A tighter complexity analysis of sparsegpt},
  author={Li, Xiaoyu and Liang, Yingyu and Shi, Zhenmei and Song, Zhao},
  journal={arXiv preprint arXiv:2408.12151},
  year={2024}
}

@article{lss+24,
  title={Multi-layer transformers gradient can be approximated in almost linear time},
  author={Liang, Yingyu and Sha, Zhizhou and Shi, Zhenmei and Song, Zhao and Zhou, Yufa},
  journal={arXiv preprint arXiv:2408.13233},
  year={2024}
}

@article{lss+24_relu,
  title={Looped relu mlps may be all you need as practical programmable computers},
  author={Liang, Yingyu and Sha, Zhizhou and Shi, Zhenmei and Song, Zhao and Zhou, Yufa},
  journal={arXiv preprint arXiv:2410.09375},
  year={2024}
}

@article{lssz24_tat,
  title={Tensor attention training: Provably efficient learning of higher-order transformers},
  author={Liang, Yingyu and Shi, Zhenmei and Song, Zhao and Zhou, Yufa},
  journal={arXiv preprint arXiv:2405.16411},
  year={2024}
}

@article{lswy23,
  title={A theoretical insight into attack and defense of gradient leakage in transformer},
  author={Li, Chenyang and Song, Zhao and Wang, Weixin and Yang, Chiwun},
  journal={arXiv preprint arXiv:2311.13624},
  year={2023}
}

@article{lvs+24,
  title={Voicebox: Text-guided multilingual universal speech generation at scale},
  author={Le, Matthew and Vyas, Apoorv and Shi, Bowen and Karrer, Brian and Sari, Leda and Moritz, Rashel and Williamson, Mary and Manohar, Vimal and Adi, Yossi and Mahadeokar, Jay and others},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@inproceedings{mosw22,
  title={Bounding the width of neural networks via coupled initialization a worst case analysis},
  author={Munteanu, Alexander and Omlor, Simon and Song, Zhao and Woodruff, David},
  booktitle={International Conference on Machine Learning},
  pages={16083--16122},
  year={2022},
  organization={PMLR}
}

@inproceedings{nbsm23,
  title={Action matching: Learning stochastic dynamics from samples},
  author={Neklyudov, Kirill and Brekelmans, Rob and Severo, Daniel and Makhzani, Alireza},
  booktitle={International conference on machine learning},
  pages={25858--25889},
  year={2023},
  organization={PMLR}
}

@article{pzb+24,
  title={Movie gen: A cast of media foundation models},
  author={Polyak, Adam and Zohar, Amit and Brown, Andrew and Tjandra, Andros and Sinha, Animesh and Lee, Ann and Vyas, Apoorv and Shi, Bowen and Ma, Chih-Yao and Chuang, Ching-Yao and others},
  journal={arXiv preprint arXiv:2410.13720},
  year={2024}
}

@inproceedings{rbl+22,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10684--10695},
  year={2022}
}

@article{rgnl21,
  title={Moser flow: Divergence-based generative modeling on manifolds},
  author={Rozen, Noam and Grover, Aditya and Nickel, Maximilian and Lipman, Yaron},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17669--17680},
  year={2021}
}

@article{smn+24,
  title={Discovering the gems in early layers: Accelerating long-context llms with 1000x input token reduction},
  author={Shi, Zhenmei and Ming, Yifei and Nguyen, Xuan-Phi and Liang, Yingyu and Joty, Shafiq},
  journal={arXiv preprint arXiv:2409.17422},
  year={2024}
}

@article{sph+22,
  title={Make-a-video: Text-to-video generation without text-video data},
  author={Singer, Uriel and Polyak, Adam and Hayes, Thomas and Yin, Xi and An, Jie and Zhang, Songyang and Hu, Qiyuan and Yang, Harry and Ashual, Oron and Gafni, Oran and others},
  journal={arXiv preprint arXiv:2209.14792},
  year={2022}
}

@article{ssk+20,
  title={Score-based generative modeling through stochastic differential equations},
  author={Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  journal={arXiv preprint arXiv:2011.13456},
  year={2020}
}

@article{syz21,
  title={Does preprocessing help training over-parameterized neural networks?},
  author={Song, Zhao and Yang, Shuo and Zhang, Ruizhe},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={22890--22904},
  year={2021}
}

@inproceedings{syz24,
  title={Solving attention kernel regression problem via pre-conditioner},
  author={Song, Zhao and Yin, Junze and Zhang, Lichen},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={208--216},
  year={2024},
  organization={PMLR}
}

@inproceedings{szz+21,
  title={Efficient attention: Attention with linear complexities},
  author={Shen, Zhuoran and Zhang, Mingyuan and Zhao, Haiyu and Yi, Shuai and Li, Hongsheng},
  booktitle={Proceedings of the IEEE/CVF winter conference on applications of computer vision},
  pages={3531--3539},
  year={2021}
}

@article{szz24,
  title={Training multi-layer over-parametrized neural network in subquadratic time},
  author={Song, Zhao and Zhang, Lichen and Zhang, Ruizhe},
  journal={ITCS},
  year={2024}
}

@article{tby+19,
  title={Transformer dissection: a unified understanding of transformer's attention via the lens of kernel},
  author={Tsai, Yao-Hung Hubert and Bai, Shaojie and Yamada, Makoto and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1908.11775},
  year={2019}
}

@article{tfm+23,
  title={Improving and generalizing flow-based generative models with minibatch optimal transport},
  author={Tong, Alexander and Fatras, Kilian and Malkin, Nikolay and Huguet, Guillaume and Zhang, Yanlei and Rector-Brooks, Jarrid and Wolf, Guy and Bengio, Yoshua},
  journal={arXiv preprint arXiv:2302.00482},
  year={2023}
}

@article{vjp22,
  title={Mcvd-masked conditional video diffusion for prediction, generation, and interpolation},
  author={Voleti, Vikram and Jolicoeur-Martineau, Alexia and Pal, Chris},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={23371--23385},
  year={2022}
}

@article{vsl+23,
  title={Audiobox: Unified audio generation with natural language prompts},
  author={Vyas, Apoorv and Shi, Bowen and Le, Matthew and Tjandra, Andros and Wu, Yi-Chiao and Guo, Baishan and Zhang, Jiemin and Zhang, Xinyue and Adkins, Robert and Ngan, William and others},
  journal={arXiv preprint arXiv:2312.15821},
  year={2023}
}

@article{whhl24,
  title={Uniform memory retrieval with larger capacity for modern hopfield models},
  author={Wu, Dennis and Hu, Jerry Yao-Chieh and Hsiao, Teng-Yun and Liu, Han},
  journal={arXiv preprint arXiv:2404.03827},
  year={2024}
}

@article{wsh+24,
  title={Transformers are Deep Optimizers: Provable In-Context Learning for Deep Model Training},
  author={Wu, Weimin and Su, Maojiang and Hu, Jerry Yao-Chieh and Song, Zhao and Liu, Han},
  journal={arXiv preprint arXiv:2411.16549},
  year={2024}
}

@article{wtb+22,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@inproceedings{xsl24,
  title={Do large language models have compositional ability? an investigation into limitations and scalability},
  author={Xu, Zhuoyan and Shi, Zhenmei and Liang, Yingyu},
  booktitle={ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models},
  year={2024}
}

@article{xsw+23,
  title={Towards Few-Shot Adaptation of Foundation Models via Multitask Finetuning},
  author={Xu, Zhuoyan and Shi, Zhenmei and Wei, Junyi and Mu, Fangzhou and Li, Yin and Liang, Yingyu},
  journal={arXiv preprint arXiv:2402.15017},
  year={2024}
}

@inproceedings{zhdk23,
  title={Kdeformer: Accelerating transformers via kernel density estimation},
  author={Zandieh, Amir and Han, Insu and Daliri, Majid and Karbasi, Amin},
  booktitle={International Conference on Machine Learning},
  pages={40605--40623},
  year={2023},
  organization={PMLR}
}

@article{zwy+22,
  title={Magicvideo: Efficient video generation with latent diffusion models},
  author={Zhou, Daquan and Wang, Weimin and Yan, Hanshu and Lv, Weiwei and Zhu, Yizhe and Feng, Jiashi},
  journal={arXiv preprint arXiv:2211.11018},
  year={2022}
}

