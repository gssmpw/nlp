[
  {
    "index": 0,
    "papers": [
      {
        "key": "sph+22",
        "author": "Singer, Uriel and Polyak, Adam and Hayes, Thomas and Yin, Xi and An, Jie and Zhang, Songyang and Hu, Qiyuan and Yang, Harry and Ashual, Oron and Gafni, Oran and others",
        "title": "Make-a-video: Text-to-video generation without text-video data"
      },
      {
        "key": "vjp22",
        "author": "Voleti, Vikram and Jolicoeur-Martineau, Alexia and Pal, Chris",
        "title": "Mcvd-masked conditional video diffusion for prediction, generation, and interpolation"
      },
      {
        "key": "brl+23",
        "author": "Blattmann, Andreas and Rombach, Robin and Ling, Huan and Dockhorn, Tim and Kim, Seung Wook and Fidler, Sanja and Kreis, Karsten",
        "title": "Align your latents: High-resolution video synthesis with latent diffusion models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "ssk+20",
        "author": "Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben",
        "title": "Score-based generative modeling through stochastic differential equations"
      },
      {
        "key": "hja20",
        "author": "Ho, Jonathan and Jain, Ajay and Abbeel, Pieter",
        "title": "Denoising diffusion probabilistic models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "gnl+23",
        "author": "Ge, Songwei and Nah, Seungjun and Liu, Guilin and Poon, Tyler and Tao, Andrew and Catanzaro, Bryan and Jacobs, David and Huang, Jia-Bin and Liu, Ming-Yu and Balaji, Yogesh",
        "title": "Preserve your own correlation: A noise prior for video diffusion models"
      },
      {
        "key": "azy+23",
        "author": "An, Jie and Zhang, Songyang and Yang, Harry and Gupta, Sonal and Huang, Jia-Bin and Luo, Jiebo and Yin, Xi",
        "title": "Latent-shift: Latent diffusion with temporal shift for efficient text-to-video generation"
      },
      {
        "key": "sph+22",
        "author": "Singer, Uriel and Polyak, Adam and Hayes, Thomas and Yin, Xi and An, Jie and Zhang, Songyang and Hu, Qiyuan and Yang, Harry and Ashual, Oron and Gafni, Oran and others",
        "title": "Make-a-video: Text-to-video generation without text-video data"
      },
      {
        "key": "gwz+23",
        "author": "Gu, Jiaxi and Wang, Shicong and Zhao, Haoyu and Lu, Tianyi and Zhang, Xing and Wu, Zuxuan and Xu, Songcen and Zhang, Wei and Jiang, Yu-Gang and Xu, Hang",
        "title": "Reuse and diffuse: Iterative denoising for text-to-video generation"
      },
      {
        "key": "gyr+23",
        "author": "Guo, Yuwei and Yang, Ceyuan and Rao, Anyi and Liang, Zhengyang and Wang, Yaohui and Qiao, Yu and Agrawala, Maneesh and Lin, Dahua and Dai, Bo",
        "title": "Animatediff: Animate your personalized text-to-image diffusion models without specific tuning"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "gnl+23",
        "author": "Ge, Songwei and Nah, Seungjun and Liu, Guilin and Poon, Tyler and Tao, Andrew and Catanzaro, Bryan and Jacobs, David and Huang, Jia-Bin and Liu, Ming-Yu and Balaji, Yogesh",
        "title": "Preserve your own correlation: A noise prior for video diffusion models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "bnh+22",
        "author": "Balaji, Yogesh and Nah, Seungjun and Huang, Xun and Vahdat, Arash and Song, Jiaming and Zhang, Qinsheng and Kreis, Karsten and Aittala, Miika and Aila, Timo and Laine, Samuli and others",
        "title": "ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "rbl+22",
        "author": "Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\\\"o}rn",
        "title": "High-resolution image synthesis with latent diffusion models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "brl+23",
        "author": "Blattmann, Andreas and Rombach, Robin and Ling, Huan and Dockhorn, Tim and Kim, Seung Wook and Fidler, Sanja and Kreis, Karsten",
        "title": "Align your latents: High-resolution video synthesis with latent diffusion models"
      },
      {
        "key": "zwy+22",
        "author": "Zhou, Daquan and Wang, Weimin and Yan, Hanshu and Lv, Weiwei and Zhu, Yizhe and Feng, Jiashi",
        "title": "Magicvideo: Efficient video generation with latent diffusion models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "hcs+22",
        "author": "Ho, Jonathan and Chan, William and Saharia, Chitwan and Whang, Jay and Gao, Ruiqi and Gritsenko, Alexey and Kingma, Diederik P and Poole, Ben and Norouzi, Mohammad and Fleet, David J and others",
        "title": "Imagen video: High definition video generation with diffusion models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "ekb+24",
        "author": "Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and M{\\\"u}ller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and others",
        "title": "Scaling rectified flow transformers for high-resolution image synthesis"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "pzb+24",
        "author": "Polyak, Adam and Zohar, Amit and Brown, Andrew and Tjandra, Andros and Sinha, Animesh and Lee, Ann and Vyas, Apoorv and Shi, Bowen and Ma, Chih-Yao and Chuang, Ching-Yao and others",
        "title": "Movie gen: A cast of media foundation models"
      },
      {
        "key": "jsl+24",
        "author": "Jin, Yang and Sun, Zhicheng and Li, Ningyuan and Xu, Kun and Jiang, Hao and Zhuang, Nan and Huang, Quzhe and Song, Yang and Mu, Yadong and Lin, Zhouchen",
        "title": "Pyramidal flow matching for efficient video generative modeling"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "lvs+24",
        "author": "Le, Matthew and Vyas, Apoorv and Shi, Bowen and Karrer, Brian and Sari, Leda and Moritz, Rashel and Williamson, Mary and Manohar, Vimal and Adi, Yossi and Mahadeokar, Jay and others",
        "title": "Voicebox: Text-guided multilingual universal speech generation at scale"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "vsl+23",
        "author": "Vyas, Apoorv and Shi, Bowen and Le, Matthew and Tjandra, Andros and Wu, Yi-Chiao and Guo, Baishan and Zhang, Jiemin and Zhang, Xinyue and Adkins, Robert and Ngan, William and others",
        "title": "Audiobox: Unified audio generation with natural language prompts"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "hvf+24",
        "author": "Huguet, Guillaume and Vuckovic, James and Fatras, Kilian and Thibodeau-Laufer, Eric and Lemos, Pablo and Islam, Riashat and Liu, Cheng-Hao and Rector-Brooks, Jarrid and Akhound-Sadegh, Tara and Bronstein, Michael and others",
        "title": "Sequence-Augmented SE (3)-Flow Matching For Conditional Protein Backbone Generation"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "bbd+24",
        "author": "Black, Kevin and Brown, Noah and Driess, Danny and Esmail, Adnan and Equi, Michael and Finn, Chelsea and Fusai, Niccolo and Groom, Lachy and Hausman, Karol and Ichter, Brian and others",
        "title": "$\\pi_0$: A Vision-Language-Action Flow Model for General Robot Control"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "rgnl21",
        "author": "Rozen, Noam and Grover, Aditya and Nickel, Maximilian and Lipman, Yaron",
        "title": "Moser flow: Divergence-based generative modeling on manifolds"
      },
      {
        "key": "bcba+22",
        "author": "Ben-Hamu, Heli and Cohen, Samuel and Bose, Joey and Amos, Brandon and Grover, Aditya and Nickel, Maximilian and Chen, Ricky TQ and Lipman, Yaron",
        "title": "Matching normalizing flows and probability paths on manifolds"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "lcb+22",
        "author": "Lipman, Yaron and Chen, Ricky TQ and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matt",
        "title": "Flow matching for generative modeling"
      },
      {
        "key": "lgl22",
        "author": "Liu, Xingchao and Gong, Chengyue and Liu, Qiang",
        "title": "Flow straight and fast: Learning to generate and transfer data with rectified flow"
      },
      {
        "key": "av22",
        "author": "Albergo, Michael S and Vanden-Eijnden, Eric",
        "title": "Building normalizing flows with stochastic interpolants"
      },
      {
        "key": "nbsm23",
        "author": "Neklyudov, Kirill and Brekelmans, Rob and Severo, Daniel and Makhzani, Alireza",
        "title": "Action matching: Learning stochastic dynamics from samples"
      },
      {
        "key": "hbc23",
        "author": "Heitz, Eric and Belcour, Laurent and Chambon, Thomas",
        "title": "Iterative $\\alpha$-(de) blending: A minimalist deterministic diffusion model"
      },
      {
        "key": "tfm+23",
        "author": "Tong, Alexander and Fatras, Kilian and Malkin, Nikolay and Huguet, Guillaume and Zhang, Yanlei and Rector-Brooks, Jarrid and Wolf, Guy and Bengio, Yoshua",
        "title": "Improving and generalizing flow-based generative models with minibatch optimal transport"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "cyb+24",
        "author": "Campbell, Andrew and Yim, Jason and Barzilay, Regina and Rainforth, Tom and Jaakkola, Tommi",
        "title": "Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design"
      },
      {
        "key": "grs+24",
        "author": "Gat, Itai and Remez, Tal and Shaul, Neta and Kreuk, Felix and Chen, Ricky TQ and Synnaeve, Gabriel and Adi, Yossi and Lipman, Yaron",
        "title": "Discrete flow matching"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "tby+19",
        "author": "Tsai, Yao-Hung Hubert and Bai, Shaojie and Yamada, Makoto and Morency, Louis-Philippe and Salakhutdinov, Ruslan",
        "title": "Transformer dissection: a unified understanding of transformer's attention via the lens of kernel"
      },
      {
        "key": "zhdk23",
        "author": "Zandieh, Amir and Han, Insu and Daliri, Majid and Karbasi, Amin",
        "title": "Kdeformer: Accelerating transformers via kernel density estimation"
      },
      {
        "key": "bsz23",
        "author": "Brand, Jan van den and Song, Zhao and Zhou, Tianyi",
        "title": "Algorithm and hardness for dynamic attention maintenance in large language models"
      },
      {
        "key": "as24",
        "author": "Alman, Josh and Song, Zhao",
        "title": "Fast attention requires bounded entries"
      },
      {
        "key": "syz24",
        "author": "Song, Zhao and Yin, Junze and Zhang, Lichen",
        "title": "Solving attention kernel regression problem via pre-conditioner"
      },
      {
        "key": "cll+24_rope",
        "author": "Chen, Bo and Li, Xiaoyu and Liang, Yingyu and Long, Jiangxuan and Shi, Zhenmei and Song, Zhao",
        "title": "Circuit Complexity Bounds for RoPE-based Transformer Architecture"
      },
      {
        "key": "hlsl24",
        "author": "Hu, Jerry Yao-Chieh and Lin, Thomas and Song, Zhao and Liu, Han",
        "title": "On computational limits of modern hopfield models: A fine-grained complexity analysis"
      },
      {
        "key": "mosw22",
        "author": "Munteanu, Alexander and Omlor, Simon and Song, Zhao and Woodruff, David",
        "title": "Bounding the width of neural networks via coupled initialization a worst case analysis"
      },
      {
        "key": "szz24",
        "author": "Song, Zhao and Zhang, Lichen and Zhang, Ruizhe",
        "title": "Training multi-layer over-parametrized neural network in subquadratic time"
      },
      {
        "key": "als19_icml",
        "author": "Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao",
        "title": "A convergence theory for deep learning via over-parameterization"
      },
      {
        "key": "dhs+22",
        "author": "Deng, Yichuan and Hu, Hang and Song, Zhao and Weinstein, Omri and Zhuo, Danyang",
        "title": "Training overparametrized neural networks in sublinear time"
      },
      {
        "key": "bpsw21",
        "author": "van den Brand, Jan and Peng, Binghui and Song, Zhao and Weinstein, Omri",
        "title": "Training (Overparametrized) Neural Networks in Near-Linear Time"
      },
      {
        "key": "syz21",
        "author": "Song, Zhao and Yang, Shuo and Zhang, Ruizhe",
        "title": "Does preprocessing help training over-parameterized neural networks?"
      },
      {
        "key": "als+23",
        "author": "Alman, Josh and Song, Zhao and Zhang, Ruizhe and Zhuo, Danyang",
        "title": "Bypass exponential time preprocessing: Fast neural network training via weight-data correlation preprocessing"
      },
      {
        "key": "dsxy23",
        "author": "Deng, Yichuan and Song, Zhao and Xie, Shenghao and Yang, Chiwun",
        "title": "Unmasking Transformers: A Theoretical Approach to Data Recovery via Attention Weights"
      },
      {
        "key": "kls+24",
        "author": "Ke, Yekun and Liang, Yingyu and Shi, Zhenmei and Song, Zhao and Yang, Chiwun",
        "title": "Curse of attention: A kernel-based perspective for why transformers fail to generalize on time series forecasting and beyond"
      },
      {
        "key": "lswy23",
        "author": "Li, Chenyang and Song, Zhao and Wang, Weixin and Yang, Chiwun",
        "title": "A theoretical insight into attack and defense of gradient leakage in transformer"
      },
      {
        "key": "gls+24",
        "author": "Gu, Jiuxiang and Liang, Yingyu and Shi, Zhenmei and Song, Zhao and Yang, Chiwun",
        "title": "Toward Infinite-Long Prefix in Transformer"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "tby+19",
        "author": "Tsai, Yao-Hung Hubert and Bai, Shaojie and Yamada, Makoto and Morency, Louis-Philippe and Salakhutdinov, Ruslan",
        "title": "Transformer dissection: a unified understanding of transformer's attention via the lens of kernel"
      },
      {
        "key": "zhdk23",
        "author": "Zandieh, Amir and Han, Insu and Daliri, Majid and Karbasi, Amin",
        "title": "Kdeformer: Accelerating transformers via kernel density estimation"
      },
      {
        "key": "bsz23",
        "author": "Brand, Jan van den and Song, Zhao and Zhou, Tianyi",
        "title": "Algorithm and hardness for dynamic attention maintenance in large language models"
      },
      {
        "key": "as24",
        "author": "Alman, Josh and Song, Zhao",
        "title": "Fast attention requires bounded entries"
      },
      {
        "key": "syz24",
        "author": "Song, Zhao and Yin, Junze and Zhang, Lichen",
        "title": "Solving attention kernel regression problem via pre-conditioner"
      },
      {
        "key": "cll+24_rope",
        "author": "Chen, Bo and Li, Xiaoyu and Liang, Yingyu and Long, Jiangxuan and Shi, Zhenmei and Song, Zhao",
        "title": "Circuit Complexity Bounds for RoPE-based Transformer Architecture"
      },
      {
        "key": "hlsl24",
        "author": "Hu, Jerry Yao-Chieh and Lin, Thomas and Song, Zhao and Liu, Han",
        "title": "On computational limits of modern hopfield models: A fine-grained complexity analysis"
      },
      {
        "key": "mosw22",
        "author": "Munteanu, Alexander and Omlor, Simon and Song, Zhao and Woodruff, David",
        "title": "Bounding the width of neural networks via coupled initialization a worst case analysis"
      },
      {
        "key": "szz24",
        "author": "Song, Zhao and Zhang, Lichen and Zhang, Ruizhe",
        "title": "Training multi-layer over-parametrized neural network in subquadratic time"
      },
      {
        "key": "als19_icml",
        "author": "Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao",
        "title": "A convergence theory for deep learning via over-parameterization"
      },
      {
        "key": "dhs+22",
        "author": "Deng, Yichuan and Hu, Hang and Song, Zhao and Weinstein, Omri and Zhuo, Danyang",
        "title": "Training overparametrized neural networks in sublinear time"
      },
      {
        "key": "bpsw21",
        "author": "van den Brand, Jan and Peng, Binghui and Song, Zhao and Weinstein, Omri",
        "title": "Training (Overparametrized) Neural Networks in Near-Linear Time"
      },
      {
        "key": "syz21",
        "author": "Song, Zhao and Yang, Shuo and Zhang, Ruizhe",
        "title": "Does preprocessing help training over-parameterized neural networks?"
      },
      {
        "key": "als+23",
        "author": "Alman, Josh and Song, Zhao and Zhang, Ruizhe and Zhuo, Danyang",
        "title": "Bypass exponential time preprocessing: Fast neural network training via weight-data correlation preprocessing"
      },
      {
        "key": "dsxy23",
        "author": "Deng, Yichuan and Song, Zhao and Xie, Shenghao and Yang, Chiwun",
        "title": "Unmasking Transformers: A Theoretical Approach to Data Recovery via Attention Weights"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "hjk+23",
        "author": "Han, Insu and Jayaram, Rajesh and Karbasi, Amin and Mirrokni, Vahab and Woodruff, David P and Zandieh, Amir",
        "title": "Hyperattention: Long-context attention in near-linear time"
      },
      {
        "key": "smn+24",
        "author": "Shi, Zhenmei and Ming, Yifei and Nguyen, Xuan-Phi and Liang, Yingyu and Joty, Shafiq",
        "title": "Discovering the gems in early layers: Accelerating long-context llms with 1000x input token reduction"
      },
      {
        "key": "szz+21",
        "author": "Shen, Zhuoran and Zhang, Mingyuan and Zhao, Haiyu and Yi, Shuai and Li, Hongsheng",
        "title": "Efficient attention: Attention with linear complexities"
      },
      {
        "key": "lll21",
        "author": "LIU, Jian-wei and LIU, Jun-wen and LUO, Xiong-lin",
        "title": "Research progress in attention mechanism in deep learning"
      },
      {
        "key": "lls+24_conv",
        "author": "Liang, Yingyu and Liu, Heshan and Shi, Zhenmei and Song, Zhao and Xu, Zhuoyan and Yin, Junze",
        "title": "Conv-basis: A new paradigm for efficient attention inference and gradient computation in transformers"
      },
      {
        "key": "lssz24_tat",
        "author": "Liang, Yingyu and Shi, Zhenmei and Song, Zhao and Zhou, Yufa",
        "title": "Tensor attention training: Provably efficient learning of higher-order transformers"
      },
      {
        "key": "lss+24",
        "author": "Liang, Yingyu and Sha, Zhizhou and Shi, Zhenmei and Song, Zhao and Zhou, Yufa",
        "title": "Multi-layer transformers gradient can be approximated in almost linear time"
      },
      {
        "key": "llss24_sparse",
        "author": "Li, Xiaoyu and Liang, Yingyu and Shi, Zhenmei and Song, Zhao",
        "title": "A tighter complexity analysis of sparsegpt"
      },
      {
        "key": "lls+24_prune",
        "author": "Liang, Yingyu and Long, Jiangxuan and Shi, Zhenmei and Song, Zhao and Zhou, Yufa",
        "title": "Beyond linear approximations: A novel pruning approach for attention matrix"
      },
      {
        "key": "cls+24",
        "author": "Chen, Bo and Liang, Yingyu and Sha, Zhizhou and Shi, Zhenmei and Song, Zhao",
        "title": "Hsr-enhanced sparse attention acceleration"
      },
      {
        "key": "lls+24_io",
        "author": "Li, Xiaoyu and Liang, Yingyu and Shi, Zhenmei and Song, Zhao and Zhou, Yufa",
        "title": "Fine-grained attention i/o complexity: Comprehensive analysis for backward passes"
      },
      {
        "key": "hwsl24",
        "author": "Hu, Jerry Yao-Chieh and Wu, Weimin and Song, Zhao and Liu, Han",
        "title": "On statistical rates and provably efficient criteria of latent diffusion transformers (dits)"
      },
      {
        "key": "hwl24",
        "author": "Hu, Jerry Yao-Chieh and Wu, Dennis and Liu, Han",
        "title": "Provably optimal memory capacity for modern hopfield models: Transformer-compatible dense associative memories as spherical codes"
      },
      {
        "key": "hcl+24",
        "author": "Hu, Jerry Yao-Chieh and Chang, Pei-Hsuan and Luo, Robin and Chen, Hong-Yu and Li, Weijian and Wang, Wei-Po and Liu, Han",
        "title": "Outlier-efficient hopfield layers for large transformer-based models"
      },
      {
        "key": "whhl24",
        "author": "Wu, Dennis and Hu, Jerry Yao-Chieh and Hsiao, Teng-Yun and Liu, Han",
        "title": "Uniform memory retrieval with larger capacity for modern hopfield models"
      },
      {
        "key": "hyw+23",
        "author": "Hu, Jerry Yao-Chieh and Yang, Donglin and Wu, Dennis and Xu, Chenwei and Chen, Bo-Yu and Liu, Han",
        "title": "On sparse modern hopfield model"
      },
      {
        "key": "as24_arxiv",
        "author": "Alman, Josh and Song, Zhao",
        "title": "The fine-grained complexity of gradient computation for training large language models"
      },
      {
        "key": "gswy23",
        "author": "Gao, Yeqi and Song, Zhao and Wang, Weixin and Yin, Junze",
        "title": "A fast optimization view: Reformulating single layer attention in llm based on tensor and svm trick, and solving it in matrix multiplication time"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "dls23",
        "author": "Deng, Yichuan and Li, Zhihang and Song, Zhao",
        "title": "Attention scheme inspired softmax regression"
      },
      {
        "key": "csy24",
        "author": "Chu, Timothy and Song, Zhao and Yang, Chiwun",
        "title": "How to Protect Copyright Data in Optimization of Large Language Models?"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "bmr+20",
        "author": "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",
        "title": "Language models are few-shot learners"
      },
      {
        "key": "wtb+22",
        "author": "Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others",
        "title": "Emergent abilities of large language models"
      },
      {
        "key": "al23",
        "author": "Allen-Zhu, Zeyuan and Li, Yuanzhi",
        "title": "Physics of language models: Part 1, context-free grammar"
      },
      {
        "key": "j23",
        "author": "Jiang, Hui",
        "title": "A latent space theory for emergent abilities in large language models"
      },
      {
        "key": "xsw+23",
        "author": "Xu, Zhuoyan and Shi, Zhenmei and Wei, Junyi and Mu, Fangzhou and Li, Yin and Liang, Yingyu",
        "title": "Towards Few-Shot Adaptation of Foundation Models via Multitask Finetuning"
      },
      {
        "key": "lls+24_grok",
        "author": "Li, Chenyang and Liang, Yingyu and Shi, Zhenmei and Song, Zhao and Zhou, Tianyi",
        "title": "Fourier circuits in neural networks: Unlocking the potential of large language models in mathematical reasoning and modular arithmetic"
      },
      {
        "key": "xsl24",
        "author": "Xu, Zhuoyan and Shi, Zhenmei and Liang, Yingyu",
        "title": "Do large language models have compositional ability? an investigation into limitations and scalability"
      },
      {
        "key": "cll+24",
        "author": "Chen, Bo and Li, Xiaoyu and Liang, Yingyu and Long, Jiangxuan and Shi, Zhenmei and Song, Zhao",
        "title": "Circuit Complexity Bounds for RoPE-based Transformer Architecture"
      },
      {
        "key": "lss+24_relu",
        "author": "Liang, Yingyu and Sha, Zhizhou and Shi, Zhenmei and Song, Zhao and Zhou, Yufa",
        "title": "Looped relu mlps may be all you need as practical programmable computers"
      },
      {
        "key": "hwg+24",
        "author": "Hu, Jerry Yao-Chieh and Wang, Wei-Po and Gilani, Ammar and Li, Chenyang and Song, Zhao and Liu, Han",
        "title": "Fundamental limits of prompt tuning transformers: Universality, capacity and efficiency"
      },
      {
        "key": "wsh+24",
        "author": "Wu, Weimin and Su, Maojiang and Hu, Jerry Yao-Chieh and Song, Zhao and Liu, Han",
        "title": "Transformers are Deep Optimizers: Provable In-Context Learning for Deep Model Training"
      },
      {
        "key": "dsy24_b",
        "author": "Deng, Yichuan and Song, Zhao and Yang, Chiwun",
        "title": "Attention is Naturally Sparse with Gaussian Distributed Input"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "zhdk23",
        "author": "Zandieh, Amir and Han, Insu and Daliri, Majid and Karbasi, Amin",
        "title": "Kdeformer: Accelerating transformers via kernel density estimation"
      },
      {
        "key": "dsy24_a",
        "author": "Daliri, Majid and Song, Zhao and Yang, Chiwun",
        "title": "Unlocking the Theory Behind Scaling 1-Bit Neural Networks"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "kwh23",
        "author": "Keles, Feyza Duman and Wijewardena, Pruthuvi Mahesakya and Hegde, Chinmay",
        "title": "On the computational complexity of self-attention"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "as24",
        "author": "Alman, Josh and Song, Zhao",
        "title": "Fast attention requires bounded entries"
      }
    ]
  }
]