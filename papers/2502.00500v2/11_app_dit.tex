\section{Diffusion Transformer} \label{sec:app:dit}

In this section, we first define the Diffusion Transformer in Appendix~\ref{sub:app:def}. Moreover, we introduce the Approximation via DiT in Appendix~\ref{sub:app:approx_dit}.

\subsection{Definitions} \label{sub:app:def}

\begin{definition}[Multi-head self-attention]\label{def:attn}
    Given $h$-heads query, key, value and output projection weights $\{(W_Q^i, W_K^i, W_V^i, W_O^i)\}_{i=1}^h \subset \R^{d_0 \times 4m}$ with each weight is a $d_0 \times m$ shape matrix, for an input matrix $X \in \R^{n \times d_0}$, we define a multi-head self-attention computation as follows:
    \begin{align*}
        {\sf Attn}(X) := \sum_{i=1}^h {\sf Softmax}( X W_Q^i {W_K^i}^\top X^\top ) \cdot X W_V^i {W_O^i}^\top + X \in \R^{n \times d_0}.
    \end{align*}
\end{definition}

\begin{definition}[Feed-forward]\label{def:feed_forward}
    Given two projection weights $W_1, W_2 \in \R^{d_0 \times r}$ and two bias vectors $b_1 \in \R^r$ and $b_2 \in \R^{d_0}$, for an input matrix $X \in \R^{n \times d_0}$, we define a feed-forward computation as follows:
    \begin{align*}
        {\sf FF}(X) := \phi(X W_1 + {\bf 1}_n b_1^\top) \cdot W_2^\top + {\bf 1}_n b_2^\top + X \in \R^{n \times d_0}.
    \end{align*}
    Here, $\phi$ is an activation function and usually be considered as ReLU.
\end{definition}

\begin{definition}[Transformer block]\label{def:transformer_tf}
    Given a set of model weights $\theta^{h, m, r} = \{ \{(W_Q^i, W_K^i, W_V^i, W_O^i)\}_{i=1}^h,$ $ W_1, W_2, b_1, b_2 \}$, the computation of a transformer block is given by the combination of multi-head self-attention computation (Definition~\ref{def:attn}) and feed-forward computation (Definition~\ref{def:feed_forward}). Formally, for an input matrix $X \in \R^{n \times d_0}$, we define:
    \begin{align*}
        {\sf TF}_{\theta^{h, m, r}}(X) := {\sf FF} \circ {\sf Attn}(X) \in \R^{n \times d_0}
    \end{align*}
\end{definition}

\begin{definition}[Reshape Layer]\label{def:R}
    We define the reshape network $R: \R^d \rightarrow \R^{n \times d_0}$.
\end{definition}

\begin{definition}[Complete transformer network]\label{def:model}
    We consider a transformer network as a composition of a transformer block (Definition~\ref{def:transformer_tf}) with model weight $\theta^{h, m, r}$, which is:
    \begin{align*}
        & ~ {\cal T}^{h, m, r} \\
        := & ~ \{ {\cal F}: \R^{n \times d_0} \rightarrow \R^{n \times d_0}~\\
        & ~ |~\text{${\cal F}$ is a composition of Transformer blocks ${\sf TF}_{\theta^{h, m, r}}$â€™s with positional embedding $E \in \R^{n \times d_0}$}\}
    \end{align*}
    We especially say $\theta^{h, m, r}$ is the model weight that contains $h$ heads, $m$ hidden size for attention and $r$ hidden size for feed-forward. See Example~\ref{exp:cal_F} for further explanation of the sequence-to-sequence mapping ${\cal F}$.
\end{definition}

\begin{example}\label{exp:cal_F}
    We here give an example for the sequence-to-sequence mapping ${\cal F}$ in Definition~\ref{def:model}: Denote $L$ as the number of layers in some transformer network. For an input matrix $X \in \R^{n \times d}$, we use $E \in \R^{n \times d}$ to denote the positional encoding, we then define:
    \begin{align*}
        {\cal F}(X) := {\sf TF}^L \circ {\sf TF}^{L-1} \circ \cdots \circ {\sf TF}^2 \circ {\sf TF}^1(X + E)
    \end{align*}
\end{example}

\subsection{Approximation via DiT} \label{sub:app:approx_dit}

\begin{theorem}\label{thm:uat}
    If the following conditions hold:
    \begin{itemize}
        \item Given a video caption distribution ${\cal V}_c$ as Definition~\ref{def:V_c}.
        \item For any $(V, c) \sim {\cal V}_c$, we define the discretized form of video as Definition~\ref{def:wt_V}.
        \item Let the observation matrix $\Phi: \{0, 1\}^{N \times \frac{T}{\Delta t}}$ be defined as Definition~\ref{def:Phi}.
        \item Let the visual decoder function $D: \R^d \rightarrow \R^D$ be defined as Definition~\ref{def:visual_decoder}.
        \item Let the ideal version of the sequence of latent patches $u \in \R^{\frac{T}{\Delta t} \times d}$ be defined as Definition~\ref{def:u}.
        \item Let the real-world version of the sequence of latent patches $\wt{u} \in \R^{N \times d}$ be defined as Definition~\ref{def:wt_u}.
        \item Let $H_N \in \R^{d \times s}$ be defined as Definition~\ref{def:H}.
        \item Let the function of polynomials $g(t)$ be defined as Definition~\ref{def:g_t}.
        \item Let the time-dependent mean of Gaussian distribution $\mu_t(\wt{u})$ be defined as Definition~\ref{def:mu}.
        \item Let the time-dependent standard deviation $\sigma_t(\wt{u})$ be defined as Definition~\ref{def:sigma}.
        \item Denote $\sigma_{\min} > 0$.
        \item Sample $z \sim \mathcal{N}(0, I_d)$.
        \item Define a model function $F_\theta: \R^d \times \R^\ell \times [0, T] \rightarrow \R^d$ with parameters $\theta$.
        \item Let the training objective ${\cal L}(\theta)$ be defined as Definition~\ref{def:L}.
    \end{itemize}
    Then there exists a transformer network $f_{\cal T} \in {\cal T}_{P}^{2, 1, 4}$ defining function $F_\theta(z, c, t) := f_{\cal T}( R([z^\top, c^\top, t]^\top) )$ with parameters $\theta$ that satisfies ${\cal L}(\theta) \leq \epsilon$ for any error $\epsilon > 0$.
\end{theorem}

\begin{proof}
    Following Assumption~\ref{ass:M}, we first denote $\wt{V}_{\tau} = \wt{{\cal M}}_\tau(c)$ for any $\tau \in [\frac{T}{\Delta t}]$ to discretize function ${\cal M}$. Then we have:
    \begin{align}\label{eq:wt_u_func}
        \wt{u}_{\tau} = {\cal D}^{-1}\Big( (\Phi \wt{{\cal M}}(c))_\tau \Big).
    \end{align}
    where this step follows from Definition~\ref{def:Phi} and Definition~\ref{def:visual_decoder}.

    Besides, we also have:
    \begin{align}\label{eq:mu_func}
        \mu_t(\wt{u}) = & ~ H_N g(t) \notag \\
        = & ~ \Big( H_{N-1} ( I_s - \frac{1}{N} A )^\top + \frac{1}{N} \wt{u}_{N} B^\top \Big) g(t) \notag\\
        = & ~ \Bigg( H_{N-2} \Big ( (I_s - \frac{1}{N-1} A )^\top + \frac{1}{N-1} \wt{u}_{N} B^\top \Big) ( I_s - \frac{1}{N} A )^\top + \frac{1}{N} \wt{u}_{N} B^\top \Bigg) g(t) \notag\\
        = & ~ \Bigg( H_0 \prod_{\tau=1}^N (I_s - \frac{1}{\tau}A)^\top + \sum_{\tau=1}^N \Big( \prod_{\tau'=1}^{\tau - 1} (I_s - \frac{1}{\tau'} A )^\top\Big) \cdot \frac{1}{N+1-\tau} \wt{u}_{N+1-\tau} B^\top \Bigg) g(t)
    \end{align}
    where these steps follow from Definition~\ref{def:mu} and simple algebras.

    Recall $F_\theta(z, c, t) := f_{\cal T}( R([z^\top, c^\top, t]^\top) )$, we choose $n=1$, then there is a target function given by:
    \begin{align*}
        &  ~ f_{\cal T}([z^\top, c^\top, t]) \\
        = & ~ \frac{\sigma_t'(\wt{u})}{\sigma_t(\wt{u})} ( z - \Bigg( H_0 \prod_{\tau=1}^N (I_s - \frac{1}{\tau}A)^\top + \sum_{\tau=1}^N \Big( \prod_{\tau'=1}^{\tau - 1} (I_s - \frac{1}{\tau'} A )^\top\Big) \cdot \frac{1}{N+1-\tau} \wt{u}_{N+1-\tau} B^\top \Bigg) g(t)  ) \\
        & ~ + \Bigg( H_0 \prod_{\tau=1}^N (I_s - \frac{1}{\tau}A)^\top + \sum_{\tau=1}^N \Big( \prod_{\tau'=1}^{\tau - 1} (I_s - \frac{1}{\tau'} A )^\top\Big) \cdot \frac{1}{N+1-\tau} \wt{u}_{N+1-\tau}' B^\top \Bigg) g(t) \\
        & ~ + \Bigg( H_0 \prod_{\tau=1}^N (I_s - \frac{1}{\tau}A)^\top + \sum_{\tau=1}^N \Big( \prod_{\tau'=1}^{\tau - 1} (I_s - \frac{1}{\tau'} A )^\top\Big) \cdot \frac{1}{N+1-\tau} \wt{u}_{N+1-\tau} B^\top \Bigg) g'(t) \\
        = & ~ \frac{\sigma_t'(\wt{u})}{\sigma_t(\wt{u})} ( z \\ 
        & ~ - \Bigg( H_0 \prod_{\tau=1}^N (I_s - \frac{1}{\tau}A)^\top + \sum_{\tau=1}^N \Big( \prod_{\tau'=1}^{\tau - 1} (I_s - \frac{1}{\tau'} A )^\top\Big) \cdot \frac{1}{N+1-\tau} {\cal D}^{-1}\Big( (\Phi \wt{{\cal M}}(c))_{N+1-\tau} \Big) B^\top \Bigg) g(t)  ) \\
        & ~ + \Bigg( H_0 \prod_{\tau=1}^N (I_s - \frac{1}{\tau}A)^\top + \sum_{\tau=1}^N \Big( \prod_{\tau'=1}^{\tau - 1} (I_s - \frac{1}{\tau'} A )^\top\Big) \cdot \frac{1}{N+1-\tau} \Big({\cal D}^{-1}\Big( (\Phi \wt{{\cal M}}(c))_{N+1-\tau} \Big) \Big)' B^\top \Bigg) g(t) \\
        & ~ + \Bigg( H_0 \prod_{\tau=1}^N (I_s - \frac{1}{\tau}A)^\top + \sum_{\tau=1}^N \Big( \prod_{\tau'=1}^{\tau - 1} (I_s - \frac{1}{\tau'} A )^\top\Big) \cdot \frac{1}{N+1-\tau} {\cal D}^{-1}\Big( (\Phi \wt{{\cal M}}(c))_{N + 1 - \tau} \Big) B^\top \Bigg) g'(t)
    \end{align*}
    where the first step follows the combination of Theorem~\ref{thm:close_form} and Eq.~\eqref{eq:mu_func}, and the differentiablity of $\wt{u}_\tau$ is ensure by Assumption~\ref{ass:k}, the second step follows from Eq.~\eqref{eq:wt_u_func}.

    Following Theorem 2 and Theorem 3 in \cite{ybr+19}, we thus complete the proof by obtaining the theorem result.
\end{proof}