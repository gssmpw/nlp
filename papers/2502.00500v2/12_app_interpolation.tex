\section{Interpolation and Extrapolation}
\label{sec:app:inter_extra}

This section first introduce properties of HiPPO-LegS in Appendix~\ref{sub:app:hippo_property}. Also, we bound the error of VLFM in Appendix~\ref{sub:app:error}.

\subsection{HiPPO-LegS Properties} \label{sub:app:hippo_property}

\begin{lemma}[Proposition 6 in \cite{gde+20}]\label{lem:optimal_projs}
    If the following conditions hold:
    \begin{itemize}
        \item Given a video caption distribution ${\cal V}_c$ as Definition~\ref{def:V_c}.
        \item For any $(V, c) \sim {\cal V}_c$, we define the discretized form of video as Definition~\ref{def:wt_V}.
        \item Let the observation matrix $\Phi: \{0, 1\}^{N \times \frac{T}{\Delta t}}$ be defined as Definition~\ref{def:Phi}.
        \item Let the visual decoder function $D: \R^d \rightarrow \R^D$ be defined as Definition~\ref{def:visual_decoder}.
        \item Let the ideal version of the sequence of latent patches $u \in \R^{\frac{T}{\Delta t} \times d}$ be defined as Definition~\ref{def:u}.
        \item Let the real-world version of the sequence of latent patches $\wt{u} \in \R^{N \times d}$ be defined as Definition~\ref{def:wt_u}.
        \item Let $H_N \in \R^{d \times s}$ be defined as Definition~\ref{def:H}.
        \item Let the function of polynomials $g(t)$ be defined as Definition~\ref{def:g_t}.
        \item Let the time-dependent mean of Gaussian distribution $\mu_t(\wt{u})$ be defined as Definition~\ref{def:mu}.
        \item Let the time-dependent standard deviation $\sigma_t(\wt{u})$ be defined as Definition~\ref{def:sigma}.
        \item Denote $\sigma_{\min} > 0$.
        \item Sample $z \sim \mathcal{N}(0, I_d)$.
        \item Define a model function $F_\theta: \R^d \times \R^\ell \times [0, T] \rightarrow \R^d$ with parameters $\theta$.
        \item Let the training objective ${\cal L}(\theta)$ be defined as Definition~\ref{def:L}.
        \item Let Assumptions~\ref{ass:k}, Assumption~\ref{ass:L_0}, Assumption~\ref{ass:M} and Assumption~\ref{ass:U} hold.
    \end{itemize}
    Then we have:
    \begin{align*}
        \| \mu_{\tau \cdot \Delta t}(\wt{u}) - \wt{u}_\tau \|_2 = O(t^{k}s^{-k+1/2})
    \end{align*}
\end{lemma}

\begin{proof}
    This lemma is a re-statement of Proposition 6 in \cite{gde+20}.
\end{proof}

\begin{lemma}[Proposition 3 in \cite{gde+20}]\label{lem:timescale_robustness}
    If the following conditions hold:
    \begin{itemize}
        \item Given a video caption distribution ${\cal V}_c$ as Definition~\ref{def:V_c}.
        \item For any $(V, c) \sim {\cal V}_c$, we define the discretized form of video as Definition~\ref{def:wt_V}.
        \item Let the observation matrix $\Phi: \{0, 1\}^{N \times \frac{T}{\Delta t}}$ be defined as Definition~\ref{def:Phi}.
        \item Let the visual decoder function $D: \R^d \rightarrow \R^D$ be defined as Definition~\ref{def:visual_decoder}.
        \item Let the ideal version of the sequence of latent patches $u \in \R^{\frac{T}{\Delta t} \times d}$ be defined as Definition~\ref{def:u}.
        \item Let the real-world version of the sequence of latent patches $\wt{u} \in \R^{N \times d}$ be defined as Definition~\ref{def:wt_u}.
        \item Let $H_N \in \R^{d \times s}$ be defined as Definition~\ref{def:H}.
        \item Let the function of polynomials $g(t)$ be defined as Definition~\ref{def:g_t}.
        \item Let the time-dependent mean of Gaussian distribution $\mu_t(\wt{u})$ be defined as Definition~\ref{def:mu}.
        \item Let the time-dependent standard deviation $\sigma_t(\wt{u})$ be defined as Definition~\ref{def:sigma}.
        \item Denote $\sigma_{\min} > 0$.
        \item Sample $z \sim \mathcal{N}(0, I_d)$.
        \item Define a model function $F_\theta: \R^d \times \R^\ell \times [0, T] \rightarrow \R^d$ with parameters $\theta$.
        \item Let the training objective ${\cal L}(\theta)$ be defined as Definition~\ref{def:L}.
        \item Let Assumptions~\ref{ass:k}, Assumption~\ref{ass:L_0}, Assumption~\ref{ass:M} and Assumption~\ref{ass:U} hold.
    \end{itemize}
    For any integer scale factor $\beta > 0$, the frames of video $\wt{V}_{\tau}$ is scaled to $\wt{V}_{\beta \tau}$, it doesn't affect the result of $H_N$ (Definition~\ref{def:H}).
\end{lemma}

\begin{proof}
    This lemma is a re-statement of Proposition 3 in \cite{gde+20}.
\end{proof}

\subsection{Error Bounds} \label{sub:app:error}

\begin{lemma}\label{lem:hippo_error}
    If the following conditions hold:
    \begin{itemize}
        \item Given a video caption distribution ${\cal V}_c$ as Definition~\ref{def:V_c}.
        \item For any $(V, c) \sim {\cal V}_c$, we define the discretized form of video as Definition~\ref{def:wt_V}.
        \item Let the observation matrix $\Phi: \{0, 1\}^{N \times \frac{T}{\Delta t}}$ be defined as Definition~\ref{def:Phi}.
        \item Let the visual decoder function $D: \R^d \rightarrow \R^D$ be defined as Definition~\ref{def:visual_decoder}.
        \item Let the ideal version of the sequence of latent patches $u \in \R^{\frac{T}{\Delta t} \times d}$ be defined as Definition~\ref{def:u}.
        \item Let the real-world version of the sequence of latent patches $\wt{u} \in \R^{N \times d}$ be defined as Definition~\ref{def:wt_u}.
        \item Let $H_N \in \R^{d \times s}$ be defined as Definition~\ref{def:H}.
        \item Let the function of polynomials $g(t)$ and matrix $G$ be defined as Definition~\ref{def:g_t}.
        \item Denote $1/\lambda^* := \lambda_{\min}(G) > 0$.
        \item Let the time-dependent mean of Gaussian distribution $\mu_t(\wt{u})$ be defined as Definition~\ref{def:mu}.
        \item Let the time-dependent standard deviation $\sigma_t(\wt{u})$ be defined as Definition~\ref{def:sigma}.
        \item Denote $\sigma_{\min} > 0$.
        \item Sample $z \sim \mathcal{N}(0, I_d)$.
        \item Define a model function $F_\theta: \R^d \times \R^\ell \times [0, T] \rightarrow \R^d$ with parameters $\theta$.
        \item Let the training objective ${\cal L}(\theta)$ be defined as Definition~\ref{def:L}.
        \item Let Assumptions~\ref{ass:k}, Assumption~\ref{ass:L_0}, Assumption~\ref{ass:M} and Assumption~\ref{ass:U} hold.
        \item $\delta \in (0, 1)$.
        \item Choosing $s = O(\frac{\Delta t}{T}\log((\frac{\Delta t}{T})^{1.5}/1/\lambda^*))$.
    \end{itemize}
    Particularly, we define:
    \begin{itemize}
        \item $\epsilon_1 := O(T^k s^{-k+1/2})$.
        \item $\epsilon_2 := O(\sqrt{d\log(d/\delta)})$.
        \item $\epsilon_3 := 1/\lambda^* U d^{0.5} \sqrt{\frac{T}{\Delta t} - N} \cdot \exp(O(\frac{T}{\Delta t}s))$.
    \end{itemize}
    Then with a probability at least $1 - \delta$, we have:
    \begin{align*}
        \| \psi_t(\wt{u}) - u_t \|_2 \leq \epsilon_1 + \epsilon_2 + \epsilon_3.
    \end{align*}
\end{lemma}

\begin{proof}
    We have:
    \begin{align*}
        \| \psi_t(\wt{u}) - u_t \|_2
        = & ~ \| \sigma_t(\wt{u}) \cdot z + \mu_t(\wt{u}) - u_t \|_2 \\
        \leq & ~ \| \sigma_t(\wt{u}) \cdot z \|_2 + \|  \mu_t(\wt{u}) - u_t \|_2 \\
        \leq & ~ \| z \|_2 + \|  \mu_t(\wt{u}) - u_t \|_2 \\
        \leq & ~ O( \sqrt{d \log(d/\delta)} ) +  \|  \mu_t(\wt{u}) - u_t \|_2 \\
        = & ~ \epsilon_2 + \|  \mu_t(\wt{u}) - u_t \|_2
    \end{align*}
    where the first step follows from Definition~\ref{def:psi}, the second step follows from triangle inequality, the third step follows from $\sigma_t(\wt{u}) \leq 1, \forall t \in [0, T]$ by some simple algebras and Definition~\ref{def:sigma}, the fourth step follows from the union bound of Gaussian tail bound (Fact~\ref{fac:gaussian_tail}), the last step follows from the definition of $\epsilon_2$.

    Then we get:
    \begin{align*}
        \|  \mu_t(\wt{u}) - u_t \|_2
        = & ~ \| H_N g(t) - u_t \|_2 \\
        = & ~ \| (M \cdot G)^\dagger (M \cdot u) \cdot g(t) - u_t \|_2 \\
        \leq & ~ \| (M \cdot G)^\dagger (M \cdot u) \cdot g(t) - G^\dagger u \cdot g(t) \|_2 + O((\frac{T}{\Delta t})^k s^{-k+1/2}) \\
        \leq & ~ \| ((M \cdot G)^\dagger (M \cdot u) - G^\dagger u \|_2 \cdot \| g(t) \|_2 + O((\frac{T}{\Delta t})^k s^{-k+1/2}) \\
        = & ~ \| ((M \cdot G)^\dagger (M \cdot u) - G^\dagger u \|_2 \cdot \| g(t) \|_2 + \epsilon_1
    \end{align*}
    where the first step follows from Definition~\ref{def:mu}, the second step follows from optimal error of solving $\| M G H - M u \|_2^2$, pesdueo-inverse matrix $(M \cdot G)^\dag \in \R^{d \times \frac{T}{\Delta t}}$ and defining a mask $M = \diag(m)$ where $m := \{0, 1\}^{\frac{T}{\Delta t}}$ and $\langle m, {\bf 1}_{\frac{T}{\Delta t}} \rangle = N$, the third step follows from the optimal error of solving $\| G H - u \|_2^2$, pesdueo-inverse matrix $G^\dag \in \R^{d \times \frac{T}{\Delta}}$ and Lemma~\ref{lem:optimal_projs}, the fourth step follows from Cauchy–Schwarz inequality and the last step follows from the definition of $\epsilon_2$.

    Next, we can show that:
    \begin{align*}
        \| (M \cdot G)^\dagger (M \cdot u) - G^\dagger u \|_2
        = & ~ \| (M \cdot G)^\dagger (M \cdot u) - G^\dagger (M \cdot u) + G^\dagger (M \cdot u) - G^\dagger u \|_2 \\
        \leq &~ \| (M \cdot G)^\dagger (M \cdot u)  - G^\dagger (M \cdot u) \|_2 + \| G^\dagger (M \cdot u) - G^\dagger u \|_2 \\
        \leq & ~ \| (M \cdot G)^\dagger - G^\dagger \|_2 \| (M \cdot u) \|_2 + \| G^\dagger \|_2 \| (M \cdot u) -  u \|_2
    \end{align*}
    where the first step follows from simple algebras, the second step follows from triangle inequality, the last step follows from Cauchy–Schwarz inequality.

    We first give:
    \begin{align}\label{eq:bound_G_dag}
        \| G^\dagger\|_2 \leq &~ 1/\lambda^* \sqrt{\frac{T}{\Delta t} \cdot s}
    \end{align}
    where this step follows from Definition~\ref{def:g_t}, Fact~\ref{fac:infity_norm_pesdueo_inverse} and the definition of $\ell_2$ norm.

    And:
    \begin{align*}
        \| u\|_2 \leq & ~ U \sqrt{\frac{T}{\Delta t} \cdot d}
    \end{align*}
    where this step follows from Assumption~\ref{ass:U} and the definition of $\ell_2$ norm.

    Also:
    \begin{align}\label{eq:bound_G}
        \| G\|_2 \leq & ~ \sqrt{\frac{T}{\Delta t} \cdot s} \exp( O(\frac{T}{\Delta t} \cdot s) ) 
    \end{align}
   where this step follows from Definition~\ref{def:g_t} and the definition of $\ell_2$ norm.

    Besides, we have:
    \begin{align*}
        \| (M \cdot G)^\dagger - G^\dagger \|_2
        \leq & ~ \frac{\| G^\dagger\|_2^2 \| I_{\frac{T}{\Delta t}} - M\|_2 \cdot \| G\|_2}{1 - \| G^\dagger\|_2 \cdot \| I_{\frac{T}{\Delta t}} - M\|_2 \cdot \| G\|_2} \\
        \leq & ~ \frac{{1/\lambda^*}^2 (\frac{T}{\Delta t} s)^{1.5} \sqrt{\frac{T}{\Delta t} - N} \cdot \exp(O(\frac{T}{\Delta t}s))}{1 - {1/\lambda^*} \frac{T}{\Delta t} s \sqrt{\frac{T}{\Delta t} - N}\cdot \exp(O(\frac{T}{\Delta t}s)) }
    \end{align*}
    where the first step follows from Fact~\ref{fac:pesdueo_inverse_diff}, simple algebras, and Cauchy–Schwarz inequality, the second step follows from Eq.~\eqref{eq:bound_G_dag}, Eq.~\eqref{eq:bound_G}, Definition~\ref{def:g_t} and simeple algebras.

    Combining all results, we get:
    \begin{align*}
        & ~ \| ((M \cdot G)^\dagger (M \cdot u) - G^\dagger u \|_2 \\
        \leq & ~ \frac{{1/\lambda^*}^2 (\frac{T}{\Delta t} s)^{1.5} \sqrt{\frac{T}{\Delta t} - N} \cdot \exp(O(\frac{T}{\Delta t}s))}{1 - {1/\lambda^*} \frac{T}{\Delta t} s \sqrt{\frac{T}{\Delta t} - N}\cdot \exp(O(\frac{T}{\Delta t}s)) } \cdot U \sqrt{\frac{T}{\Delta t} N d} + 1/\lambda^* \sqrt{\frac{T}{\Delta t} - N} \cdot U \sqrt{\frac{T}{\Delta t} \cdot d} \\
        \leq & ~ 1/\lambda^* U d^{0.5}  \sqrt{\frac{T}{\Delta t} (\frac{T}{\Delta t} - N)}  \cdot \Big( \frac{ {1/\lambda^*} (\frac{T}{\Delta t})^{1.5} N^{0.5} s^{1.5}  \cdot \exp(O(\frac{T}{\Delta t}s))}{1 - {1/\lambda^*} (\frac{T}{\Delta t})^{1.5} s \cdot \exp(O(\frac{T}{\Delta t}s))} + 1 \Big) \\
        \leq & ~ 1/\lambda^* U d^{0.5}  \sqrt{\frac{T}{\Delta t} (\frac{T}{\Delta t} - N)}  \cdot  \frac{ 1}{1 - {1/\lambda^*} (\frac{T}{\Delta t})^{1.5} s \cdot \exp(O(\frac{T}{\Delta t}s))}  \\
        \leq & ~ O\Big( 1/\lambda^* U d^{0.5}  \sqrt{\frac{T}{\Delta t} (\frac{T}{\Delta t} - N)}\Big)
    \end{align*}
    where the second and third steps follow from simple algebras, the last step follows from plugging the choice of $s$.

    Finally, we have:
    \begin{align*}
        \| ((M \cdot G)^\dagger (M \cdot u) - G^\dagger u \|_2 \cdot \| g(t) \|_2 
        \leq & ~ O\Big( 1/\lambda^* U d^{0.5}  \sqrt{\frac{T}{\Delta t} (\frac{T}{\Delta t} - N)}\Big) \cdot \sqrt{s} \exp(O(\frac{T}{\Delta t}s)) \\
        \leq & ~ 1/\lambda^* U d^{0.5} \sqrt{\frac{T}{\Delta t} - N} \cdot \exp(O(\frac{T}{\Delta t}s))  \\
        = & ~ \epsilon_3
    \end{align*}
    these steps follow from simple algebras, Definition~\ref{def:g_t} and the definition of $\epsilon_3$.
\end{proof}

\begin{theorem}\label{thm:inter_extra_polation}
    If the following conditions hold:
    \begin{itemize}
        \item Given a video caption distribution ${\cal V}_c$ as Definition~\ref{def:V_c}.
        \item For any $(V, c) \sim {\cal V}_c$, we define the discretized form of video as Definition~\ref{def:wt_V}.
        \item Let the observation matrix $\Phi: \{0, 1\}^{N \times \frac{T}{\Delta t}}$ be defined as Definition~\ref{def:Phi}.
        \item Let the visual decoder function $D: \R^d \rightarrow \R^D$ be defined as Definition~\ref{def:visual_decoder}.
        \item Let the ideal version of the sequence of latent patches $u \in \R^{\frac{T}{\Delta t} \times d}$ be defined as Definition~\ref{def:u}.
        \item Let the real-world version of the sequence of latent patches $\wt{u} \in \R^{N \times d}$ be defined as Definition~\ref{def:wt_u}.
        \item Let $H_N \in \R^{d \times s}$ be defined as Definition~\ref{def:H}.
        \item Let the function of polynomials $g(t)$ and matrix $G$ be defined as Definition~\ref{def:g_t}.
        \item Denote $1/\lambda^* := \lambda_{\min}(G) > 0$.
        \item Let the time-dependent mean of Gaussian distribution $\mu_t(\wt{u})$ be defined as Definition~\ref{def:mu}.
        \item Let the time-dependent standard deviation $\sigma_t(\wt{u})$ be defined as Definition~\ref{def:sigma}.
        \item Denote $\sigma_{\min} > 0$.
        \item Sample $z \sim \mathcal{N}(0, I_d)$.
        \item Define a model function $F_\theta: \R^d \times \R^\ell \times [0, T] \rightarrow \R^d$ with parameters $\theta$.
        \item Let the training objective ${\cal L}(\theta)$ be defined as Definition~\ref{def:L}.
        \item Let Assumptions~\ref{ass:k}, Assumption~\ref{ass:L_0}, Assumption~\ref{ass:M} and Assumption~\ref{ass:U} hold.
        \item $\delta \in (0, 1)$.
    \end{itemize}
    Particularly, we define:
    \begin{itemize}
        \item $\epsilon_1 := O(T^k s^{-k+1/2})$.
        \item $\epsilon_2 := O(\sqrt{d\log(d/\delta)})$.
        \item $\epsilon_3 := 1/\lambda^* U d^{0.5} \sqrt{\frac{T}{\Delta t} - N} \cdot \exp(O(\frac{T}{\Delta t}s))$.
    \end{itemize}
    Then with a probability at least $1 - \delta$, we have:
    \begin{align*}
        \| {\cal D}(z + \int_0^{t} F_\theta(z, c, t') \d t') - u_t\|_2 \leq \epsilon_0 + L_0 (\epsilon_1 + \epsilon_2 + \epsilon_3).
    \end{align*}
\end{theorem}


\begin{proof}
    This proof follows from the combination of Assumption~\ref{ass:L_0}, Theorem~\ref{thm:uat} and Lemma~\ref{lem:hippo_error}.
\end{proof}