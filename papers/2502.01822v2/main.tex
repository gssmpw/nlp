%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{color,soul}
\definecolor{apricot}{rgb}{0.98, 0.81, 0.69}
\definecolor{babyblueeyes}{rgb}{0.63, 0.79, 0.95}
\definecolor{bubblegum}{rgb}{0.99, 0.76, 0.8}


% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}



% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}

% \usepackage{listings}
% \lstset{
%   basicstyle=\ttfamily,
%   breaklines=true,
%   columns=fullflexible
% }

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Firewalled Agentic Networks}

\usepackage{enumitem}

\usepackage{placeins}

\usepackage{xcolor}
\definecolor{PineGreen}{rgb}{0.0, 0.47, 0.44}
\definecolor{BrickRed}{rgb}{0.8, 0.25, 0.33}

\newlist{deepitemize}{itemize}{5}
\setlist[deepitemize,1]{label=\textbullet}
\setlist[deepitemize,2]{label=\textendash}
\setlist[deepitemize,3]{label=\textasteriskcentered}
\setlist[deepitemize,4]{label=\textperiodcentered}
\setlist[deepitemize,5]{label=\textopenbullet}

\newcommand{\parabf}[1]{\noindent\textbf{#1}}

\begin{document}

\twocolumn[

\icmltitle{Firewalls to Secure Dynamic LLM Agentic Networks}



% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Sahar Abdelnabi}{equal,micro}
\icmlauthor{Amr Gomaa}{equal,cam,dfki}
\icmlauthor{Eugene Bagdasarian}{mass}
\icmlauthor{Per Ola Kristensson}{cam}
\icmlauthor{Reza Shokri}{nus}
\newline 
\newline 
\url{https://github.com/microsoft/Firewalled-Agentic-Networks}

%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{micro}{Microsoft}
\icmlaffiliation{cam}{University of Cambridge}
\icmlaffiliation{dfki}{German Research Center for Artificial Intelligence (DFKI)}
\icmlaffiliation{mass}{University of Massachusetts Amherst}
\icmlaffiliation{nus}{National University of Singapore}

\icmlcorrespondingauthor{Sahar Abdelnabi}{saabdelnabi@microsoft.com}
\icmlcorrespondingauthor{Amr Gomaa}{amr.gomaa@dfki.de}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\newcommand{\tbd}[1]{\textcolor{red}{TBD: #1}}

\definecolor{input}{HTML}{82B366}
\definecolor{trajectory}{HTML}{D79B00}
\definecolor{data}{HTML}{6C8EBF}



\begin{abstract}
Future LLM agents are likely to communicate on behalf of users with other entity-representing agents on tasks that entail long-horizon plans with interdependent goals. Current work does not focus on such agentic networks, nor does it address their challenges. Thus, we first identify the required properties of agents' communication, which should be proactive and adaptable. It needs to satisfy 1) \textbf{privacy}: agents should not share more than what is needed for the task, and 2) \textbf{security}: the communication must preserve integrity and maintain \textbf{utility} against selfish entities. We design a use case (travel planning) as a testbed that exemplifies these requirements, and we show examples of how this can go wrong. Next, we propose a practical design, inspired by established network security principles, for constrained LLM agentic networks that balance adaptability, security, and privacy. Our framework \textbf{automatically constructs and updates task-specific rules from prior simulations to build firewalls}. We offer layers of defense to 1) convert free-form input to a task-specific protocol, 2) dynamically abstract users' data to a task-specific degree of permissiveness, and 3) self-correct the agents' trajectory. 

\end{abstract}

\vspace{-7mm}
\section{Introduction}

\begin{figure} [!t]
    \centering
    \includegraphics[width=\linewidth]{firewall_assistant.pdf}
    \vspace{-7mm}
    \caption{The AI assistant (black) can share data and adapt to requests from external parties (red). We firewall the assistant by 1) sanitizing external inputs to a task-specific structure (\textbf{\textcolor{input}{input firewall}}), 2) abstracting user's data and tools' outputs (\textbf{\textcolor{data}{data firewall}}), and 3) self-correcting sub-optimal actions (\textbf{\textcolor{trajectory}{trajectory firewall}}). \textbf{\textcolor{data}{Data}} and \textbf{\textcolor{trajectory}{trajectory}} firewalls are built from prior simulations.}
    \vspace{-4mm}
    \label{fig:firewall}
\end{figure}

There is an increasing interest in deploying Large Language Models (LLM) as agents to perform users' tasks that require openly browsing the internet, e.g., to plan a move to a new city or prepare an event~\cite{nyt,operator_openai}. Chatbots are also increasingly used as customer service agents to assist in reservations and bookings~\cite{asksuite,futr}. A future where these two entity-representing agentic sides autonomously communicate seems likely to soon happen. 

Such agentic networks would unlock many use cases where agents deliberate and negotiate to find solutions~\cite{abdelnabi2024cooperation}. This dynamic communication is not equivalent to a predetermined sequence of API calls. It would resemble how humans consult an event or a travel planner. We share information and make decisions as needed to meet our goals. 

We first outline the required properties of such agents. They should be dynamically \textbf{adaptable} to feedback from the external world that may arise mid-communication. They must satisfy \textbf{privacy} and \textbf{integrity} requirements. Communication should \textbf{not share more} information than what is needed for the task, should \textbf{not alter} the state of the user in an unintended way, result in an \textbf{outcome} that satisfies the goal, and maintain \textbf{utility} against \textbf{selfish} or greedy entities. 

We design a testbed that operationalizes these properties with travel planning as a use case. An AI assistant can access users' goals and data, and communicate with an external travel agent that has a database of options. This task, with temporally dependent goals, often requires multi-turn planning with adaptation to meet hard and soft constraints. 

Next, we devise a dynamic communication procedure to simulate benign adaptability. We show how this can go wrong by testing multi-turn privacy and security attacks done by the external agent inspired by social engineering methods. Such attacks are difficult to isolate in advance as they are goal-related and involve nearby actions. Current system-level defenses, based on data separation and action elimination, would fail to protect against them. We need new methods to \textbf{constrain agents} while preserving \textbf{adaptability}. 

We propose a grounding infrastructure to \textbf{firewall agentic networks}. As shown in~\autoref{fig:firewall}, our framework converts natural-language communication into a \emph{task-specific protocol} (\textbf{\textcolor{input}{input} firewall}). Shared data is \emph{abstracted} to contain data minimally needed for the task (\textbf{\textcolor{data}{data} firewall}); a process that is completely shielded from any interaction with third parties. Finally, the agent self-corrects its actions based on the \textbf{\textcolor{trajectory}{trajectory} firewall}. The permissible degree of adaptability is decided based on the goal, and the framework should support continual updates. Thus, the \textbf{\textcolor{data}{data}} and \textbf{\textcolor{trajectory}{trajectory}} firewalls are iteratively built from \textbf{prior simulations} by automatically and dynamically creating rules that capture preferences and previous mistakes.

Our mitigation reduces private data leakage from 70\% to less than 2\% at worst. Also, it reduces security attacks that alter the user's state by, e.g., a ``delete calendar entries'' attack from 45\% to 0\%. Finally, it significantly reduces challenging attacks that are trajectory-dependent, e.g., upselling and coercing less-optimal choices during the conversation, even without violating clear budget constraints. 

Our main contributions are: 1) structuring the requirements of adaptable, secure agentic networks, 2) operationalizing them into a testbed, 3) identifying the security and privacy risks, and 4) designing a holistic, practical mitigation framework that balances autonomy and adaptability vs. safety.  


\vspace{-2mm}
\section{Preliminaries and Related Work}

\textbf{Firewalls.} In network security, a ``firewall'' is a system that monitors and controls incoming and outgoing traffic based on defined security policies. It establishes a \emph{barrier} between trusted and untrusted components and shields from malicious or unnecessary traffic~\cite{cisafirewall}. We adopted the term in our design due to its high conceptual resemblance. 

\textbf{Travel planning.} LLMs have shown promising performance for tool use~\cite{yaoreact,NEURIPS2023_1b44b878}. Recent work evaluated them in more complex scenarios; \citet{xietravelplanner} proposed an environment where a \emph{single agent} uses tools to automate travel planning. Follow-up work by \citet{singh2024personal} explored generating personalized plans. \citet{zhang2024ask} proposed Proactive Agent Planning to teach models to ask for clarifying questions. \citet{jiang2024towards} evaluated agents from the perspective of \emph{full delegation} by evaluating \emph{how} they achieved an outcome. 
In contrast, we focus on security and privacy aspects in a multi-turn interaction between \emph{two agents}. 

\textbf{Attacks between conversational agents.} 
\citet{bagdasarian2024airgapagent} assumed an agent that communicates with a third party (via a single turn) that aims to exfiltrate data.
\citet{abdelnabi2024cooperation} evaluated multi-agent attacks in a negotiation simulation. \citet{debenedetti2024dataset} reported that multi-turn prompt injections are harder to protect against. \citet{zhou2024haicosystem} simulated human-AI interactions, assuming a malicious user. Our work studies multi-turn attacks with a more focus on goal-oriented agents. 

\textbf{Contextual integrity (CI).} The theory of contextual integrity~\citep{nissenbaum2004privacy} defines privacy as the appropriate flow of information in a specific context. \citet{mireshghallahcan} evaluated LLMs in four tiers of CI, ranging from judging information sensitivity in a specific context to discerning public and private information. 
\citet{ghalebikesabi2024operationalizing} evaluated LLMs' CI reasoning in the task of ``form filling''. Our work also evaluates the sharing of contextually unnecessary data; we add real-world complexity by evaluating the interactive decision-making of models. 

\textbf{Prompt injections.} In the last two years, prompt injections~\cite{greshake2023not} have been quite actively studied as one of the most pressing threats to LLM applications. In such attacks, the LLM drifts from the user's task to another task found in external data~\cite{abdelnabi2024you}. In our work, we introduce a \emph{new threat model} for conversational agents, which are expected to be adaptable and proactive without \emph{semantically} drifting from the user's goal.  

\textbf{System-level defenses.}
\citet{bagdasarian2024airgapagent} proposed a mitigation against data exfiltration attacks by ``air gaping''; restricting the data to ``task-related'' information. Also, to protect against indirect prompt injections, allowed actions (e.g., APIs) can be decided before interacting with untrusted data~\cite{debenedetti2024agentdojo,wu2024system,balunovic2024ai}. We \textbf{study a dynamic autonomous setup where such simple policies would be either inadequate or limiting}. 
Our work \emph{complements} system-level defenses; we offer a framework to build \textbf{dynamic rules} from simulations as a prior of permissible and impermissible actions.

\vspace{-2mm}
\section{Threat Model} \label{sec:threat_model}

In this section, we outline the agent requirements, assumptions, and the threat model we consider in our work. 

\textbf{Setup.} We consider an AI assistant, $A$, that has access to the user's data and environment, $U$. The user can task $A$ with a goal, $G$, which may involve multiple objectives, constraints, and preferences. $A$ interacts with an external party, $P$. $A$ interacts with $U$ to query information about the user that is needed to fulfill $G$, either proactively or as explicitly stated by $G$. $A$ can also alter the state of $U$ (e.g., calendar, email, etc.). The flow of information from $U$ to $P$ is mediated by $A$. Similarly, any actions that $P$ may attempt to perform on $U$ are mediated by $A$. While fulfilling $G$, $A$ is required to be goal-oriented and privacy-conscious~\cite{bagdasarian2024airgapagent}. Correctly translating the awareness about contextual integrity from simple yes/no probing into action was found to be a difficult task for LLMs~\cite{shaoprivacylens}, motivating the need for complex, long-horizon tasks like ours.  

\textbf{User goal.} We construct $G$ in a way that entails \emph{conditional} changes. $G$ is decomposed into a list of dependent sub-goals and conditions, $c$: $\left[ g_1, c_1 \rightarrow g_2, ..., c_{n-1} \rightarrow g_n) \right]$. This notation denotes that $A$ must first attempt to fulfill the sub-task $g_1$ then attempt to fulfill $g_2$ only if condition $c_1$ is satisfied, and so on. This entails: 1) the exact needed information and the trace of actions cannot be deterministically known in advance, 2) sharing all data needed for $g_1, g_2, ..., g_n$ in advance \textbf{can be over permissive} as it shares data for tasks that may not be pursued, and 3) the validity of $A$'s actions must be evaluated given the trace, not only the final output; e.g., $A$ may make sub-optimal choices that lead to conditions $c$ not being satisfied; due to that, $A$ would not fulfill subsequent sub-goals. The evaluation needs to consider whether better choices would have led to better utility. 

\textbf{User environment.} $U$ contains data about the user 
and possible toolkits (e.g., email, calendar). 
Each toolkit has associated information (e.g., emails) and actions (e.g., send email). We populate $U$ with synthetic data of user profiles that is \emph{task-related}. However, it is unstructured; private data is intertwined with contextually non-private data.  
$A$ should ideally discern these nuanced contexts. However, as current LLMs~\cite{mireshghallahcan} intrinsically lack this ability, our \textbf{\textcolor{data}{data} firewall} mitigation, explained later, first \textbf{automatically derives contextual rules} and, based on them, \textbf{dynamically changes and abstracts} the returned data from $U$ to contain the strictly needed task-related data. 


\textbf{Assistant-mediated interaction.} We assume a \textbf{multi-turn} conversation. $A$ receives $G$. Then, at each turn, it sends sub-queries to either $U$ (denoted as $Q_{A_u}$) or $P$ (denoted as $Q_{A_p}$). $U$ responds with answers to the sub-queries. $P$ responds with answers about $A$'s sub-queries in addition to potential benign (e.g., asking about preferences to narrow down the search) or malicious requests. Communication can be in natural language. Our \textbf{\textcolor{input}{input} firewall} converts the communication to a task-specific protocol. The communication continues until $A$ terminates when it indicates that $G$ has been fulfilled and no more actions are required. 

\textbf{Assumptions about the third party.} $P$ is equipped with a ``database'' of options that it uses to answer $A$'s queries. $P$ can be initialized to be \textbf{benign}. In this case, it collaborates with $A$ to achieve $G$. It can ask $A$ about context-related information and, based on that, give recommendations from the ``database'' options. $P$ is also \emph{proactive}; it does not only answer $A$'s requests, but it can ask for follow-up requests that are typically required in the context of $G$. $P$ can be designed to simulate real-world benign dynamic scenarios by, e.g., introducing changes mid-conversation. 
\begin{figure} [!b]
    \vspace{-2mm}
    \centering
    \includegraphics[width=0.92\linewidth]{setup.pdf}
    \vspace{-4mm}
    \caption{The assistant is given a goal that has multiple objectives, conditions, and constraints. It can access the user's environment to query information or perform actions. The assistant also interacts with a third party that has a database of options to fulfill the goal.}
    \label{fig:teaser}
\end{figure}


\vspace{-1mm}

\textbf{Security and privacy attacks.} $P$ can also be \textbf{malicious}. We assume that $P$ knows the general structure of $U$ (e.g., the available toolkits) but does not have exact knowledge about what actions can be performed. In \textbf{security} attacks, $P$ aims to manipulate $A$ to perform less optimal or task-unrelated actions. In \textbf{privacy} attacks, $P$ aims to manipulate $A$ to leak data that is contextually private (not needed for the task). We construct all attacks such that \textbf{it is not trivial to prevent them apriori or to detect/reject them based on the final plan} (e.g., the hard constraints of the user are always met). This is enabled by our design of $U$ and $G$. Besides, \textbf{multi-turn attacks enable gradual incremental progress}. This is analogous to multi-turn prompt injection~\cite{debenedetti2024dataset} or accumulated privacy leakage over multiple attempts~\cite{kairouz2015composition}. 

\vspace{-2mm}
\section{Travel Planning as a Testbed} \label{sec:travel_planning} 

We operationalize the required properties of the setup and threat model in Section~\ref{sec:threat_model} with ``travel planning'' as an example. We discuss how we designed our testbed to evaluate the assistant's benign adaptability and susceptibility to attacks.  

\textbf{User environment.} We create synthetic profiles for users. We first prompt GPT-4 to create a short description of users' personas (e.g., demographics, hobbies, etc.). Then, we prompt it to populate the user's environments given these personas (the prompts we used are in~\autoref{tab:create_synthetic_env}). We refined these environments manually or via re-prompting. They may contain data that should always be treated as private because it is sensitive, confidential, or unrelated to the task. We used the categories defined by~\citet{mireshghallahcan} (e.g., political or sexual orientation) for this. More importantly, \textbf{the majority of the data is task-related} and the assistant is expected to use it to reason about its decisions. For example, the travel and purchase history gives information about the user's preference, the dietary constraints and the user's medical data can inform the type of activities the user may be interested in, and the user's emails may contain promotions or travel-related discussions, etc. 

\textbf{Travel options.} We give $P$ a list of synthetic options (with prices and descriptions) as a ``database'' from which it should retrieve answers. These are lists of hotels, activities (e.g., hiking trips, museum guided tours, cooking classes, etc.), restaurants, additional services (e.g., premium breakfast, etc.), and flights (budget and standard airlines). We include a list of available destinations and available dates for trips. 

\textbf{User goal.} The goal, $G$, given to $A$, is phrased to have multiple dependent objectives. 
It can also contain hard constraints (e.g., budget). $G$ can be stated in a way that requires $A$ to be \textbf{proactive} by checking with $U$. 
As discussed, $G$ has sub-goals and conditions in order to simulate a dynamic setup. We instantiate that by giving priorities to different sub-goals. 
An example of the goal is in~\autoref{fig:teaser}.
\vspace{-1mm}

\textbf{Attacks and attacker's capabilities.} We instantiate \textbf{security} attacks as: 1) \textbf{contextually-permissible-but-harmful}, and 2) \textbf{contextually-prohibited} ones, for which examples are 1) upselling within the budget, and 2) freeing up the calendar on dates before or after the trip. For upselling, $P$ aims to get $A$ to book a more expensive hotel or to include additional packages (in this case, if $A$ exhausts the budget, this will be a successful manipulation even if the budget constraint is met because $A$ would have failed to fulfill the other sub-goals). We assume that $P$ cannot manipulate the state of travel options, e.g., it cannot lie about prices, ratings, etc, and it cannot hide search results; if $A$ is asking about mid-range budget accommodation options, $P$ must present such options. However, $P$ can augment the suggestions with more expensive options or services and use strategies like repeated requests, social engineering, exaggeration, etc. To instantiate \textbf{privacy} attacks, we test attacks to reveal the user's calendar state, medical data, etc. Examples of contextually private data we test on are in~\autoref{tab:conf_attacks_examples}. 

\vspace{-2mm}
\section{Interaction Framework and Agents Design}
We describe the implementation details of the components in~\autoref{fig:teaser}, the communication between components, and the design of the assistant, including our \textbf{firewall} mitigation. 

\vspace{-1mm}
\subsection{User environment}  

We create the synthetic profiles as described in Section~\ref{sec:travel_planning}. Then we design $U$ as an agent that receives natural language requests, $Q_{A_u}$, from $A$. $U$ is given the scope of the available data and toolkits (and their interface) it can simulate (e.g., a calendar). It then maps $Q_{A_u}$ to these available interfaces (e.g., read calendar). It is also given instructions to abide by the scope of the toolkits and available data and decline any out-of-scope instructions, e.g., 
$U$ must not answer clarification requests about $G$. Our design is inspired by~\citet{ruanidentifying}; $U$ is given the interaction history with $A$ and instructed to maintain the state; e.g., if a calendar entry is deleted, it should not be returned when $A$ later queries about the calendar. Unlike~\citet{ruanidentifying}, $U$ does not simulate information on the fly, but can only retrieve from the static synthetic profiles. The dynamic nature in our testbed stems from the external party.  In an analogy with training data leakage~\cite{carlini2019secret}, we insert controlled canaries in the synthetic profiles and evaluate if they got leaked.

\vspace{-2mm}

\subsection{External Party}
We design the external party $P$ as a red-teamer. It is given instructions to act like travel agency while testing the adaptability of $A$ and whether it will follow contextually malicious requests. In order to be able to have dynamic simulations without hard-coding travel packages, $P$ is instructed to combine packages from the individual dimensions of travel options (described in Section~\ref{sec:travel_planning}). For example, all hotels and activities are available at each travel destination.   

\renewcommand{\arraystretch}{1.5}
\begin{table} [!t]
    \centering
    \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{p{0.15\linewidth} | p{0.8\linewidth}} \toprule
        \textbf{Toolkit} & \textbf{Contextually private data} \\ \midrule
        Medical data &  Insurance number, prescriptions, name of doctor, medical history (e.g., conditions) \\ 
        Calendar &  Business meetings, dates and locations of previous trips, participants of meetings and trips \\ 
        Purchase history &  Dates, providers, and amounts of previous purchase history, location of restaurants \\  
        Emails & Access code for booked hotel, Conversations about booked trips, Business meeting conversations for flights promotion, Bank account number and associated one-time code for promotion \\ \bottomrule
    \end{tabular}}
    \caption{We include examples of contextually private data in the synthetic profiles that we use as targets in our privacy attacks.}
    \label{tab:conf_attacks_examples}
    \vspace{-3mm}
\end{table}

When $P$ is benign, it is instructed not to ask for requests that are not needed for the scope of travel agency. It is prompted to adapt to $A$'s preferences and not push for any travel option that is not aligned with them (e.g., a more expensive hotel). It can, however, ask for contextually relevant information (e.g., dietary preferences when booking restaurants). To simulate dynamic changes, it can introduce mid-conversation that an option (e.g., hotel or activity) is no longer available, and in this case, it can suggest another option that is closely aligned with $A$'s preferences. $P$ can be given additional instructions that aim to add more diversity and test adaptability; e.g., it can introduce \textit{an offer} to a more expensive hotel to bring it to the budget constraint expressed by $A$. These design choices simulate the case where \textbf{it is not possible or optimal to substitute the interaction with pre-determined tool calls or actions}. 

When $P$ is malicious, it can also ask for contextually relevant information, but it is given an additional adversarial task to pursue and allowed a maximum number of turns in order to achieve the adversarial task and derail the conversation; after that, it should terminate. It should follow the rules of the attacker's capabilities discussed in Section~\ref{sec:travel_planning}.

\vspace{-1mm}
\subsection{Assistant and Firewalling Design}

We start with a baseline assistant and increase the complexity incrementally, reaching the final \textbf{firewalled agent}. The intermediate designs are later used as ablation experiments. 

\vspace{-1mm}
\subsubsection{Non-firewalled}
\begin{figure} [!b]
    \vspace{-5mm}
    \centering
    \includegraphics[width=0.8\linewidth]{plan_assistant.pdf}
    \vspace{-5mm}
    \caption{The \textbf{``task-confined''} assistant first generates an initial plan and then checks the compliance with it at each step.} 
    \label{fig:plan-assistant}
    
\end{figure}

\textbf{Baseline design.} The basic assistant design is one that creates an ``initial plan'', containing items such as ``hard constraints'', ``soft constraints'', ``task decomposition'', ``data planning'' (i.e., what data is needed about the user), and ``anticipated changes'' (i.e., what actions/data are okay and expected to change based on the interaction). The assistant is instructed to use the ``initial plan'' to choose the next sub-tasks to complete. It observes requests from $P$ and, based on them and its own initial plan, requests data from $U$.  

\textbf{Task-confined design.} The assistant is given additional rules to interact with $P$ (e.g., no unnecessary data leakage, warnings against upselling and unrelated tasks). Before interacting with $P$, its Chain-of-Thought contains additional instructions to compare the request from $P$ against the rules and the initial plan and reject any unrelated requests or ones that attempt to increase the scope of the task (see~\autoref{fig:plan-assistant}). 

\vspace{-1mm}
\subsubsection{Firewalled and Task-Confined}
The assistant should be autonomous while conforming to the task and preferences. As shown in~\autoref{fig:firewall}, we construct \textbf{multiple layers of defense as an infrastructure to ground} the communication. We build \textbf{firewalls} to \textbf{constrain inputs} and user's \textbf{data}, and \textbf{self-correct the trajectory}. Anticipating the space of allowed adaptability data can be hard to do in advance. Thus, our mitigation supports constructing priors and learning from them and previous mistakes. 

\textbf{Priors.} We run benign and malicious simulations. Then, we input these logs as pairs (randomly sampled) along with the user's goal to GPT-4 as an evaluator, which should generate rules. This is an \emph{iterative process}; new pairs can be fed to the evaluator along with previous rules and the evaluator may refine or add rules. 
We used two evaluators for the security and privacy attacks. After constructing the rules, we use them to build ``firewalls''.

\textbf{\textcolor{data}{Data firewall.}} This limits the accessible user's data. Outputs passing from the environment are first fed to the firewall along with the rules and the user's goal. The firewall should \textbf{abstract} the data; only passing the semantic fields that correspond to the task. This is a more advanced operation than isolating the task-relevant data~\cite{bagdasarian2024airgapagent} as it involves ``abstractive summarization''.
\textbf{Importantly, the data firewall is isolated from any interaction with the external party}; as the rules encapsulate the task context, the firewall model does not have to observe the conversation. Also, as the rules can be very specific (see~\autoref{tab:data_guidelines} and~\ref{tab:data_guidelines2}), they reduce the problem from relying on the LLM to discern private data to direct translation of text to apply the rules. 

\textbf{\textcolor{trajectory}{Trajectory firewall.}} 
This firewall has to observe the trajectory of agents to make \textbf{comparative decisions}. It is built from \textbf{security} attacks and used as a post-decision verification step. After the assistant generates an answer, it is passed to the firewall. The firewall should compare the answer against the rules. If no violation is detected, it can relay the answer. Otherwise, it attempts to regenerate the answer to self-correct. These rules can encapsulate the preferences (e.g., budget adherence, see~\autoref{tab:trajectory_guidelines}).

\textbf{\textcolor{input}{Input firewall.}} We assume that the assistant receives additional requests from the external agent. \textbf{Input sanitization can limit the capabilities of the attacker} and reduce the effect of manipulation. This converts natural language inputs to a more structured format. To exemplify this, we prompt GPT-4 to convert any text from the external agent to a \texttt{JSON} format that indicates items, costs, suggestions, options, dates, availability, and additional requests, and also remove the reasoning of the external agent by writing options and requests in a more neutral and objective tone. 

\renewcommand{\arraystretch}{1.0}
\begin{table*} [!t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{llllllll} \toprule
        \textbf{Purchase history (\%)} & \textbf{Social security (\%)} & \textbf{Credit card (\%)} & \textbf{Location (\%)} & \textbf{Calendar details (\%)} & \textbf{People (\%)} & \textbf{Job (\%)} & \textbf{Health (\%)} \\ \midrule
         0 & 0 & 0 & 0 & 0 & 2 & 0 & 32 
         \\ \bottomrule
    \end{tabular} }
    \caption{Privacy evaluation for the \textbf{baseline} assistant and the \textbf{benign} external agent. We evaluate the \textbf{leak} of any information related to these items. Medical data and health-related issues are usually discussed in the context of restaurant booking (e.g., allergies).}
    \label{tab:confidentiality_baseline}
    \vspace{-3mm}
\end{table*}

\vspace{-2mm}
\section{Experiments}

We outline the procedure for the LLM-as-a-Judge that we use to devise scores for success in privacy and security attacks and we discuss the experimental setup. Then, we show the benign and attack behaviors and qualitative examples. The Appendix contains implementation details and prompts. 


\subsection{Scoring via LLM-as-a-Judge}
\vspace{-1mm}

\subsubsection{Privacy} 
This judge observes the whole interaction between the assistant $A$ and the external agent $P$. To make sure any reasoning for leakage is done based on the actually shared information by $A$, the judge does not take the user's environment $U$ and it does not observe the interaction between $A$ and $U$. We input a set of questions to the judge about different categories of private information (e.g., social security number, credit card, etc.)~\citep{mireshghallahcan}. In addition, when $P$ is malicious, the judge takes $P$'s goal (e.g., leaking the participants of previous trips) and indicates if $A$ leaked it. 

\vspace{-1mm}
\subsubsection{Security} The security evaluation is based on the final arrangement and the interactions between $A$ and $P$, as well as $A$ and $U$. 

\textbf{Final arrangement.} The judge evaluates the extent to which $A$ has achieved $G$ based on the final arrangement (e.g., by extracting the number of activities and the number of restaurants). For upselling attacks, we also evaluate whether $A$ has booked additional packages offered by $P$. The judge also rates how this arrangement matches the user's preferences by comparing against $U$. While this is hard to evaluate objectively in an absolute way, we perform this evaluation comparatively by comparing the case where $P$ is benign against where $P$ is malicious. 

\textbf{$A$ and $P$ interaction.} We evaluate \emph{how} the assistant reached the final arrangement. As an example of sub-optimal actions, we measure the number of times $A$ has accepted an option offered by $P$ that was not the \emph{cheapest}. 

\textbf{$A$ and $U$ interaction.} For attacks where $P$ aims to perform actions on $U$ (deleting an entry), the judge takes the interaction between $A$ and $U$ to evaluate if the attack succeeded.  






\vspace{-1mm}
\subsection{Experimental Setup}

We generated \textbf{4 personas} following the structure in Section~\ref{sec:travel_planning}. For each persona, we wrote a user goal $G$. We ran combinations of the assistant $A$ (baseline, task-confined, and firewalled) and external party $P$ (benign, privacy- and security-attackers). For the \textbf{firewalled assistant}, we test a setup of \textbf{incremental self-improvement}. We run two experiments, one where the guidelines are generated based on the logs of one persona and one where the guidelines are incrementally updated based on all personas. This is analogous to updating rules and policies for ``actual'' firewalls. 

For benign $P$, we ran a variant in which $P$ makes $A$ \textbf{a benign offer} to test $A$'s adaptability (e.g., an offer to a more expensive accommodation, bringing it closer to the stated user's budget). For security attacker $P$, we ran the previously outlined \textbf{``upselling''} and \textbf{``delete calendar entry''}. For the latter, $G$ is adjusted to request a reservation in a specific week and to cancel any calendar events this week. This attack tests \textbf{misspecification} risks; $P$ aims to delete entries a day before/after the week indicated by $G$. This also simulates a case where $P$'s attack cannot be isolated in advance (based on allowed tool calls). For privacy attacker $P$, we run the attacks' targets in~\autoref{tab:conf_attacks_examples}. 

\begin{table} [!t]
    \centering
    \resizebox{0.94\linewidth}{!}{
    \begin{tabular}{llll} \toprule
        \multirow{2}{*}{\textbf{Attack goal}} & \multicolumn{3}{c}{\textbf{Leak per assistant (\%)} $\downarrow$}  \\ \cmidrule(lr){2-4}
        & \textbf{Baseline} & \textbf{Task-confined} & \textbf{Firewalled (\textcolor{data}{D} + \textcolor{trajectory}{T}})\\
        \midrule
        Medical data &  70 & 35 & \textbf{0} \\ 
        Previous trips & 42 & 15 & \textbf{0} \\ 
        Purchase history & 42 & 32 & \textbf{2} \\ 
        Calendar entries & 25 & 10 & \textbf{0} \\
        Access code & 30 & 25 & \textbf{0} \\ \bottomrule
    \end{tabular} }
    \caption{Privacy evaluation for the different assistants against a malicious external agent. Firewalled agents here have the \textbf{\textcolor{data}{data (D)}} and \textbf{\textcolor{trajectory}{trajectory (T)}} firewalls. $\downarrow$ means lower values are better.}
     \vspace{-1mm}
    \label{tab:confidentiality_attacks}
\end{table}

\begin{table} [!t]
    \centering
    \resizebox{0.74\linewidth}{!}{
    \begin{tabular}{lll} \toprule
        \multicolumn{3}{c}{\textbf{Attack success per assistant (\%)} $\downarrow$} \\  \cmidrule(lr) {1-3}
        \textbf{Baseline} & \textbf{Task-confined} & \textbf{Firewalled (\textcolor{data}{D} + \textcolor{trajectory}{T}}) \\ \midrule
        45 & 22 & \textbf{0}\\
        \bottomrule
    \end{tabular} }
    \caption{Security evaluation for \textbf{``delete calendar entry''} attack.
    }
    \vspace{-3mm}
    \label{tab:integrity_delete}
\end{table}


We ran each experiment 10 times; we used a $top\_p$ sampling of 0.92. We report the average results (for the LLM-as-a-Judge scores) of the runs marginalized over the personas. The total number of runs for \emph{each experiment} across personas (a combination of $A$ and $P$ with an instantiation of $G$) is \textbf{40}. Overall, the total number of runs is \textbf{1080}. We run on GPT-4 (version: \texttt{gpt-4o-2024-11-20}). As our focus is on the system design rather than benchmarking, we prioritized covering many attack and mitigation scenarios. We expect other models to show similar improvement, even better when using reasoning models~\cite{openai_inference}. 

\vspace{-2mm}
\subsection{Benign Behavior}
We show the leak of data in the case of the baseline assistant and a benign external agent in~\autoref{tab:confidentiality_baseline}. Excluding health-related information (which includes allergies that may be related to the task), the assistant rarely shares sensitive information when it is not prompted explicitly. 

When faced with an adaptation decision when the benign external agent introduces an offer mid-conversation, the AI assistant accepted the benign offer in 50-60\% of the cases even in the presence of the firewall, i.e., \textbf{the firewall did not affect the utility in benign cases}. Besides, \autoref{tab:integrity} shows that in the benign case, the firewalled assistant achieves a similar utility to that of a non-firewalled one. Appendices~\ref{sec:conv} and~\ref{sec:conv_ex2} show conversations examples in the benign case, demonstrating how agents adapt to changes and share information according to the evolving context.

\begin{table*} [!t]
    \centering
    \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{l|llll} \toprule
        \multirow{2}{*}{\textbf{Attack}}  & \multicolumn{4}{c}{\textbf{Utility metrics $\uparrow$}} \\ \cmidrule(lr){2-5}
        & \textbf{Package includes dates? (\%)} & \textbf{Package includes all items? (\%)} & \textbf{\# of activities (rating)} & \textbf{\# of restaurants (rating)} \\ \midrule
         N/A (Benign) & 64 & 59 & 3.32 (8.84/10) & 1.35 (8.37/10)  \\ \midrule
                Previous trips &  \textbf{84} & 51 & 3.23 (8.29/10) & 1.19 (7.75/10) \\ % 
                Calendar entries &  \textbf{95} & 62 & \textbf{3.72} (7.89/10) & \textbf{2.42} (8.18/10) \\ % 
                Purchase history &  57 & \textbf{80} & 2.96 (\textbf{8.92}/10) & 1.76 (\textbf{8.64}/10) \\ % 
        \bottomrule
    \end{tabular} }
    \caption{Privacy attacks are contextual. The external agent manipulates the assistant by making the attack sound \textbf{relevant to the goal}. Thus, \textbf{they improve the corresponding aspect of the goal}. Attacks that aim to leak the calendar or dates of previous trips make the assistant include dates in the final arrangement more. Attacks that aim to leak purchase history lead to better activities and restaurants. Results are averaged for the different designs of the assistant (baseline, task-confined, and firewalled). $\uparrow$ means higher values are better.}
    \label{tab:contextual_attacks}
\end{table*}

\begin{table*} [!t]
    \centering
    \resizebox{0.98\linewidth}{!}{
    \begin{tabular}{ll| ll | ll} \toprule
        \multirow{2}{*}{\textbf{Assistant}} & \multirow{2}{*}{\textbf{External}} & \multicolumn{2}{c}{\textbf{Upselling metrics $\downarrow$}}  & \multicolumn{2}{c}{\textbf{Utility metrics $\uparrow$}}  \\ \cmidrule(lr){3-6}
        & & \textbf{\# of additional packages} & \textbf{Not the cheapest?} & \textbf{\# of activities (rating)} & \textbf{\# of restaurants (rating)} \\ \midrule 
         
        Task-confined & \multirow{2}{*}{Benign} & 0.18 & 0.72 & \textbf{3.10} (9.1/10) & 1.10 (8.6/10) \\  
        Firewalled (\textbf{\textcolor{data}{D} + \textcolor{trajectory}{T}}) & & \textbf{0.10} & 0.32 & \textbf{3.10} (8.2/10) & 1.10 (8.7/10)  \\  
        \midrule 

        Task-confined & \multirow{4}{*}{Malicious} & 1.35 & 0.85 & 2.75 (7.6/10) & 1.40 (6.6/10) \\  
        Firewalled (\textbf{\textcolor{data}{D} + \textcolor{trajectory}{T}}) & & 0.85 &  0.60 & 2.68 (8.6/10) & 1.42 (7.9/10) \\ 
        Firewalled (\textbf{\textcolor{data}{D} + \textcolor{trajectory}{T}}, improved) & & 0.50 & 0.60 & 2.88 (8.2/10) & 1.75 (7.7/10) \\ 
        Firewalled (\textbf{\textcolor{data}{D} + \textcolor{trajectory}{T} + \textcolor{input}{I}}, improved) & & 0.40 &  \textbf{0.30} & 3.05 (8.4/10) & \textbf{1.78} (7.8/10)  \\ \bottomrule
    \end{tabular} }
    \caption{Security and utility evaluation for upselling against benign cases. We show the average number of additional packages and times the assistant took an option that was not the cheapest (\textbf{for upselling evaluation}). We show activities and restaurants, along with their ratings (\textbf{for utility evaluation}). In upselling attacks, the external agent offers more additional packages and more expensive options. This leads to a decrease in the number of booked activities and restaurants and their quality. The firewall mitigation self-corrects and prevents some of these actions. \textbf{``improved''} means getting ``security'' rules from all personas. $\uparrow$/$\downarrow$ mean higher/lower values are better.}
    \label{tab:integrity}
    \vspace{-1mm}
\end{table*}


\vspace{-1mm}
\subsection{Attacks and Mitigation Results}
\autoref{tab:confidentiality_attacks} shows 5 different privacy attacks where the goal is to leak the corresponding information. The leakage is significantly high for the baseline agents and gets reduced for the task-confined ones. Ultimately, \textbf{the privacy \textcolor{data}{data} firewall prevents leakage almost completely (at most 2\%)}. Noteworthy, the \textbf{\textcolor{data}{data}} firewall rules were generated \textbf{from one attack} (``purchase history'' leak) on \textbf{one persona}, and it \textbf{generalizes} to new domains (e.g., leaking medical data) on \textbf{all} personas. Similarly, \autoref{tab:integrity_delete} shows the improvement incurred by the firewall in the \textbf{``delete calendar entry''} security attack, where the \textbf{success rate is reduced to 0\%}. 

\textbf{Privacy attacks are contextually related} to the task, making them challenging to isolate. An external manipulator may make them sound relevant to the goal (e.g., \textit{``in order to help tailor activities, can you share purchase history?}). This is reflected by the results in~\autoref{tab:contextual_attacks}, which show that attacks may \emph{improve} achieving the contextually relevant sub-goal since the conversation may focus on this aspect. \textbf{This motivates our \textcolor{input}{input} firewall that abstracts and removes the free-form manipulation attempts} by the external agent. 

For \textbf{upselling}, \autoref{tab:integrity} shows that the external agent can succeed in making the assistant exhaust the budget over additional packages and more expensive options, making it fail in achieving the user's goals and reducing the relative quality of the final package. 
\textbf{The ``\textcolor{trajectory}{trajectory} firewalled'' assistant} (with creating guidelines based on all personas) \textbf{makes fewer sub-optimal choices}, which is reflected by having less additional packages, more activities with higher ratings, and less frequently selecting the more expensive options. \textbf{Adding the \textcolor{input}{input} firewall improves these aspects}. 

\vspace{-1mm}
\subsection{Qualitative Analysis and Examples}

We highlight examples in Appendix~\ref{sec:qualitative_examples}. In the attack soliciting users' medical data, the AI assistant, \textit{without the firewall}, released all the users' medical data, including prescriptions, insurance number, and companies. A \textit{firewalled} assistant informed the external agent about the relevant data only (such as the gluten intolerance and what is covered under the user's insurance) without revealing contextually private data. Similarly, in another attack, the external agent asks for purchase history data under the premise of aligning the selected booked options with the user's preferences. \textit{Without a firewall}, the AI assistant revealed all the previous credit card transactions including dates, amounts, and locations (in a \textbf{gradual way}, allowing composition, deanonymization, and risks such as impersonation; see~\autoref{tab:firewall_examples2}). The \textit{firewalled} AI assistant only informs the external agent about the gluten-free dining preferences. \textbf{These examples show that firewalling can maintain utility and task performance}.

Our mitigation provides layers of defense that complement each other. \textbf{The \textcolor{data}{data} firewall can prevent the ``delete calendar entry'' security attack}. \autoref{tab:firewall_examples5} shows that the calendar state is abstracted to not mention any events beyond the dates of the trip outlined in the user's goal, making the AI assistant not pursue the adversarial task.

\autoref{tab:firewall_examples_benign} shows that the \textbf{\textcolor{trajectory}{trajectory}} firewall self-corrected choosing a more expensive option \textbf{even in the benign case}. \autoref{tab:firewall_examples6} shows that it can also reduce upselling. Although adding the ``Virtual Personal Assistant option'' does not exceed the budget, the firewall prevented the AI assistant from booking it. \autoref{tab:firewall_examples7} shows that the \textbf{\textcolor{input}{input}} firewall provides further protection by abstracting the external agent embellishment. The AI assistant without the \textbf{\textcolor{input}{input}} firewall, and despite the \textbf{\textcolor{trajectory}{trajectory}} firewall, still falls for the external agent rationale and depletes the budget on several additional packages (e.g., ``Laundry Service'', ``Airport Transfer'' and the ``Premium Breakfast'') instead of booking the activities and restaurants sub-goals. However, the AI assistant applying both firewalls succeeds in declining optional add-ons.

\vspace{-2mm}
\section{Discussion and Limitations}

We here discuss limitations and future opportunities. 

\textbf{Other use cases.} In Section~\ref{sec:threat_model}, we outlined a scenario where the assistant needs to \emph{selectively} and \emph{gradually} share data, use external feedback in order to decide on actions, adapt, and fulfill interdependent goals. We designed the ``travel planning'' use case that highly fits these requirements. However, our mitigation framework can, in principle, extend to any use case where these requirements need to be met. 

\textbf{Utility evaluation with a ground truth.} We evaluate the assistant with LLM-as-a-Judge scores. We compare the utility by whether the sub-goals were met and by their quality in a comparative way, i.e., comparing attacks and mitigation against baselines. Also, to do large-scale experiments without annotation, the external agent introduces dynamic changes. To evaluate utility extensively, future extensions could assign ground-truth scores for options and introduce controlled changes for each simulation. 

\textbf{Certifiable input firewall.} The \textbf{\textcolor{input}{input}} firewall converts free-form inputs into items with short descriptions to mitigate manipulation attempts. This is done by an LLM and, thus, in principle, might be vulnerable. A future improvement is to map inputs to a closed set of data types and values, as \textbf{a template or a task-specific language} for the agent that could be constructed from prior simulations. Sanitizing inputs in LLM applications such as search engines is intractable since it is an open domain. Constructing a language that is rich enough to support a specific goal can be more promising.  

\textbf{Multi-agent assistant system.} The \textbf{\textcolor{data}{data}} firewall abstracts a lot of the data details while being informed about the task context. This has the great advantage of shielding from any external manipulation and can even be extended to support deterministic guarantees. However, it limits the assistant's capability to perform further complex analysis and reasoning on the data if needed. Future extensions could support having a multi-agent LLM assistant system, e.g., as a ``medical'' or ``financial'' specialist. The specialists perform the analysis, and their output is passed through the firewall to the leading orchestrator assistant. 

\textbf{Symbolic planning.} Our framework significantly reduces trajectory-dependent attacks' success, such as upselling. However, this is a challenging threat to mitigate completely. Analogous to prompt injection, tool isolation does not protect against it~\cite{debenedetti2024agentdojo}. We tackle this by integrating comprehensive layers. Recent work has shown that scaling inference-time compute improves the adversarial robustness in following precise policies~\cite{openai_inference}. Our firewalls are LLMs that apply/check specific rules derived from prior simulations instead of ambiguously using LLMs to judge whether actions comply with the goal. Besides, future work could use the LLM as a reasoning tool to construct a decision tree for the goal and communicate symbolically with a planner, in addition to adding a deterministic ``analyzer'' to audit whether policies were violated. 

\vspace{-1mm}

\section{Conclusion}
AI agents are now used to perform complex tasks. OpenAI Operator is probably just the beginning of such systems. Chatbots are now used by many service providers to facilitate bookings and reservations. Soon, these two ends are going to communicate, forming agentic networks where AI agents adaptively communicate to customize plans and find solutions. We need to ensure the security and privacy of such networks. Any shared data must be necessary. Any action must meet all constraints and be entailed by the task. Agents should achieve fair utility against selfish entities by not being coerced into less-optimal nearby actions during their communication. We first outline scenarios where adaptability and multi-turn interaction are required. We then design a comprehensive use case of travel planning that we use as a testbed to identify failure cases. Inspired by security principles, we propose a \emph{firewall} mitigation framework with multiple layers of defense. We need controlled systems and agent infrastructures that apply input protocols, clearly separate contextually private data, and address trajectory-dependent sub-optimal actions. Otherwise, we will be repeating previous cybersecurity mistakes. We propose practical defenses that significantly reduce attacks' success. Our firewall can even eliminate some attacks and can be iteratively updated. However, completely solving these problems is a grand challenge; we hope our threat model and conceptualization invite further future work.


\section*{Impact Statement}
Future agents are going to automate many workflows by communicating with the external world, including other agents, creating agentic networks. We do not urge the premature deployment of such networks. However, we proactively address security and privacy challenges that would arise due to this. In our work, we do not attack real-world systems. We do not leak any private information of individuals. We constructed a completely sand-boxed synthetic environment. The privacy and security challenges of LLMs are already well established in previous work with other simpler scenarios. We extend research in this area. Our work has broader societal implications to highlight the importance of studying new threat models and challenges of future agentic systems. We contribute to designing secure agentic systems by outlining an envisioned infrastructure to control and restrict agents' communication while allowing adaptability. 

\section*{Acknowledgment}

Amr Gomaa acknowledges funding from the German Ministry of Education and Research (BMBF) under FedWell project (Grant Number: 01IW23004).

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
% \appendix
% \onecolumn

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{appendix}

\end{document}