% Graph Neural Networks (GNNs) have made significant progress in processing unstructured data such as social networks, material graphs and molecules, showing the state-of-the-art performance, and are widely used in molecular property prediction, drug discovery and other fields. Its core is to encode node features and graph structure information through a message passing mechanism, and then pool the embedding of nodes or edges to generate a global vector of the graph and apply it to downstream tasks. However, the success of GNNs is often based on the assumption that the training and testing sets are independent and identically distributed, which is difficult to hold when deploying models to real-world scenarios. 
% Several studies have shown that GNNs tend to perform poorly on Out-of-distribution (OOD) data, which can significantly compromise their robustness and security. For instance, in molecular property prediction, such as determining toxicity, a well-trained GNNs may perform well on the known training distribution. However, in real-world scenarios, molecules with novel scaffolds or sizes, unseen in the training data, are inevitable. It is crucial that the model maintains its performance when encountering such data to mitigate potential safety risks.

Graph Neural Networks (GNNs) have made remarkable advancements in processing unstructured data, including social networks, material graphs, and molecules. These models have achieved state-of-the-art performance and are widely applied in tasks such as molecular property prediction and drug discovery. At the core of GNNs lies a message-passing mechanism, which encodes both node features and graph structure information. By aggregating node or edge embeddings, GNNs generate a global representation of the entire graph, which is then used for downstream tasks. However, this success is often built on the assumption that training and testing data are independent and identically distributed (IID), an assumption that rarely holds in real-world applications.

In practice, deploying GNNs in real-world scenarios reveals their vulnerability when faced with Out-of-Distribution (OOD) data. Several studies have demonstrated that GNNs often struggle to maintain performance when tested on data that differ significantly from the training set, which raises concerns about the models' robustness and security. For example, in molecular property prediction tasks, models may encounter molecules with novel scaffolds or sizes that were not part of the training distribution. Similarly, in social network analysis, the underlying graph structure could change due to the emergence of new communities or interactions, leading to unexpected behavior. These variations in data distributions, common across domains, highlight the need for models to perform consistently under OOD conditions to mitigate potential risks and ensure reliable outcomes.

Most recent research on graph OOD generalization has focused on invariant learning methods. These approaches aim to capture stable relationships between features and labels that remain consistent across different environments. By leveraging the principle of invariance, these methods enable the model to generalize effectively under distribution shifts. Typically, such methods assume the presence of environment labels during training, which facilitates the learning of these invariant relationships and allows for better OOD generalization performance.  This line of thinking originates from the computer vision (CV) field, where environment labels, such as background or image style, are often available or easily inferred.
In the context of graph data, accurately defining and distinguishing environments presents a fundamental challenge. Unlike images, which allow for clear segmentation based on visual cues, graphs inherently lack explicit structural boundaries, complicating the identification of distinct environments. This complexity poses a critical question: \textbf{Can we truly rely on these predefined environments, or is there a better way to achieve robust OOD generalization in graphs?}

Beyond the challenge of environment definition, another key obstacle is the \textbf{semantic cliff}, which affects the separability of invariant subgraphs. As illustrated in Figure 1, synthetic datasets like house and cycle suggest that identifying the invariant subgraph could allow effective classification. However, in practice, this task becomes difficult unless the dataset's structure is already known, allowing for targeted method design.
For example, in molecular graphs, a functional group often serves as the invariant subgraph. Even slight alterations, such as changing a single atom or bond, can drastically impact a molecule's properties or classification. This variability complicates the identification of invariant subgraphs that consistently map to the same class, particularly when working with unknown or novel datasets.

In this paper, we approach these two challenges from a fundamentally new perspective, one that shifts away from traditional reliance on predefined environments or explicit subgraph extraction. Instead of directly working with the raw graph structures or assuming the availability of environment labels, our method focuses on extracting invariant representations in the GNN-encoded latent space. This allows us to bypass the inefficiencies and limitations associated with environment-based partitioning and manual subgraph identification, offering a more flexible and generalizable solution to the OOD generalization problem in graphs.

Instead of relying on predefined environments or manually extracted subgraphs, our method introduces a novel approach to OOD generalization by focusing on invariant feature extraction in the GNN-encoded latent space. These features are projected into a unit-norm hyperspherical space, where the distances between class prototypes drive classification. To ensure robustness, we introduce two loss functions: the Invariant Feature Loss (IFL), which promotes stability in the learned features across different data distributions, and the Separation Loss (SL), which encourages clear differentiation between classes, even under unseen distribution shifts. These losses are designed to work synergistically, aligning the learned representations with the underlying structure of the graph data while maintaining generalization performance across varying distributions.


The main contributions of this paper are as follows:

\begin{itemize}[leftmargin=*]
    \item \textbf{Novel OOD Generalization Framework for Graphs}: We propose a new approach to graph OOD generalization by focusing on invariant feature extraction in the latent space of GNNs. This method bypasses the reliance on predefined environments or explicit subgraph extraction, offering a more flexible and scalable solution.
    
    \item \textbf{Hyperspherical Representation and Loss Functions}: We introduce a hyperspherical representation space for graph classification, along with two new loss functions—\textit{Invariant Feature Loss (IFL)} and \textit{Separation Loss (SL)}. These losses are specifically designed to ensure robustness against distribution shifts and improve the generalization capability of the model.
    
    \item \textbf{Theoretical Consistency and Empirical Validation}: We provide a detailed theoretical analysis demonstrating that our proposed losses align with the goals of robust OOD generalization. Additionally, our method is validated through extensive experiments on the DrugOOD and GOOD benchmarks, where it consistently outperforms existing methods.
\end{itemize}



% \colorbox{lightgray}{\begin{varwidth}{\linewidth}\textit{
% % Can we truly rely on these predefined environments, or is there a better way to achieve robust OOD generalization in graphs?
% How do we effectively capture stable patterns in graph data, when there are no clear\\ visual  cues or well-defined boundaries to guide the extraction of invariant features?}\end{varwidth}}

% However, unlike Euclidean data, graphs are complex and highly abstract non-Euclidean structures, which present two key challenges for out-of-distribution generalization: 1) \textit{Defining and partitioning environments effectively is difficult}. In computer vision, datasets can be manually divided into multiple subsets based on background or image style, but it is challenging or even infeasible for graphs since we don't have a clear and reasonable defination of the environment of a graph. Consequently, existing graph OOD methods based on invariant learning often fail to match the performance predicted by theoretical analysis, as they lack accurate environment information. 2) \textit{The 'semantic cliff' in graphs results in poor separability of the  invariant subgraph}. Even when invariant subgraph are identified across environments, they tend to be fragile and indivisible. For example, in molecular graphs, the invariant subgraph is typically a functional group conresponding to labels, but a change in just one atom or bond (represented by nodes and edges) can lead to drastic shifts in the molecule's properties or label. As a result, two seemingly similar invariant subgraphs may correspond to entirely different classes or attributes, which defeats the purpose of invariant learning (similar invariant subgraphs should have the same class).


% In this paper, we address these two problems from a new perspective, distinct from existing approaches. Our goal is to design an algorithm that ensures representations from different environments but with the same label exhibit maximal similarity, while representations with different labels, regardless of their environment, maintain maximal separability. In other words, we aim to maximize intra-class invariance and inter-class separability. To achieve this, we first perform invariant feature extraction in the GNN-encoded latent space, avoiding the inefficiency of directly extracting invariant subgraphs from the original graph. We then map the invariant features to a unit norm hyperspherical space for classification by comparing the distances between and the class prototypes. We propose two types of loss functions: $L_{intra-inv}$ which ensures intra-class feature invariance without requiring environmental information, and $L_{inter-sep}$ which maximizes class-based separability of the extracted invariant features. We provide a detailed theoretical analysis to demonstrate the consistency of our loss functions with the proposed objective. In terms of empirical performance, our experimental results on two graph OOD generalization benchmarks, DrugOOD and GOOD, validate the effectiveness of our method. The main contributions of this paper are as follows:

% \begin{itemize}
%     \item We first propose the OOD generalization method for graphs based on hyperspherical representation learning. Invariant feature extraction is conducted in the encoded latent space, and distance-based classification is performed in the unit hyperspherical space. Intra-class invariance and inter-class separability are ensured through the optimization of $L_{intra-inv}$ and $L_{inter-sep}$.
%     \item We provide a detailed theoretical analysis proving the consistency between our proposed loss functions and the objective of achieving OOD generalization.
%     \item Experimental results on the DrugOOD and GOOD benchmarks validate the effectiveness of our approach.
% \end{itemize}
%第一个：推导出这个理论框架，Eq 6
%第二个： 理论基础上，实现方法本身
%第三个： 实验



% To address the challenges of environment capturing and the semantic cliff, we propose a novel framework that combines \textbf{prototype-based invariant learning} with a \textbf{hyperspherical embedding space}. Our approach bypasses the need for explicit environment modeling by introducing prototypes as implicit representations of environment variants. This allows the model to extract robust invariant features across varying environments without relying on environment labels. To further enhance the class separability and ensure the model's robustness, we introduce two key loss functions: \ding{172} \textit{Invariant Prototype Matching Loss ($\mathcal{L}_{IPM}$)}  ensures that samples from the same class are assigned to the same class prototypes, acting as positive samples. In contrast, samples incorrectly assigned to these prototypes—due to environmental influences—are treated as negative samples, driving the model to improve its invariance to environment-induced variations. \ding{173} \textit{Prototype Separation Loss ($\mathcal{L}_{PS}$)}  optimizes the relationship between prototypes, making sure that prototypes belonging to the same class are similar, while those from different classes remain dissimilar. By refining the inter-class boundaries in hyperspherical space, this loss enhances the discriminative power of the model, thereby reducing the impact of the semantic cliff.
 %sx:我认为在latent space 提取不变特征不是很重要的创新，主要的创新还是首次将图不变学习和超球空间结合，利用超球空间中prototype的特性，保证在无环境信息的情况下学习到满足不变性的表征（对应挑战1）并提升可分性（挑战2）

% The main contributions of this paper are as follows:

% \begin{itemize}[leftmargin=*]
%     \item \textbf{Integration of Invariant Learning with Hyperspherical Embedding:} We introduce a novel framework that combines graph invariant learning with hyperspherical embedding, enabling robust invariant representation learning without explicit environment labels.
    
%     \item \textbf{Prototype-Based Implicit Environment Representation: } By leveraging multi-prototypes to represent implicit environment variants, our method addresses the challenge of environment capturing in graph data, eliminating the need for explicit environmental modeling.
    
%     \item \textbf{Novel Loss Functions for Invariance and Separability:}  We propose two key loss functions: the Invariant Prototype Matching Loss to enhance invariance, and the Prototype Separation Loss to improve inter-class separability, effectively addressing the semantic cliff problem and improving OOD generalization.
% \end{itemize}
