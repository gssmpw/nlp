Graph Neural Networks (GNNs) have made remarkable advancements in modeling and learning from graph-structured data across various scenarios, including, encompassing social networks~\citep{fan2020graph,chang2021sequential,li2022cyclic}, molecules~\citep{shui2020heterogeneous,liu2024data}, and knowledge graphs~\citep{kg_1,kg_2}. Despite the powerful representational capabilities of GNNs, their success often relies on the assumption that the training and testing data follow the same distribution. Unfortunately, such an assumption rarely holds in most real-world applications, where out-of-distribution (OOD) data from different distributions often occurs~\citep{OODsurvey,Obenchmark}. Empirical evidence has shown that GNNs often struggle to maintain performance when tested on OOD data that differ significantly from the training set~\citep{OODbench,li2022ood}. These vulnerabilities underscore the critical need to enhance the OOD generalization capabilities of GNNs, which has become a rapidly growing area of research~\citep{good,drugood,liu2024arc,pan2023prem}.

% Recently, invariant learning has emerged as a pivotal approach for addressing the challenges of OOD generalization~\citep{creager2021environment,ahuja2020empirical,sun2020latent,ye2021towards}. It posits that the data generation process comprises both invariant and environmental features, with only the former exhibits solid and deterministic predictive relationships with the label, while the latter may show spurious correlations with the labels and can vary significantly in response to distribution changes~\citep{yang2022learning,chen2024does}. The objective of invariant learning is to identify invariant representations from the input, thereby facilitating reliable predictions and generalization across diverse environments. Inspired by the success of invariant learning in the image domain~\citep{creager2021environment, ye2022ood}, graph invariant learning (GIL), rooted in the same theoretical framework, has recently emerged as a prominent solution for addressing analogous challenges in graph data~\citep{li2022learning, fan2022debiasing, miao2022interpretable, wu2022discovering}.  The fundamental assumption underlying GIL is that each graph sample can be divided into two distinct components, namely \textit{invariant subgraph} and \textit{environment subgraph}. 
% By effectively separating invariant and environmental subgraphs, GIL-based methods aim to achieve the same objectives as invariant learning in other domains. To this end, existing methods primarily focus on accurately modeling environmental subgraphs, employing carefully designed loss functions to minimize the correlations between the predicted environmental subgraphs and the labels~\citep{gui2024joint,piao2024improving}, or utilizing data augmentation strategies to simulate potential environments in wild data~\citep{wu2022handling,jia2024graph}.
{Building on the success of \textit{invariance principle} in addressing OOD generalization challenges in image data~\citep{creager2021environment,ye2022ood}, graph invariant learning~(GIL) has been proposed and recently emerged as a prominent solution for tackling its counterpart problem~\citep{li2022learning,fan2022debiasing,miao2022interpretable,wu2022discovering}.} The fundamental assumption underlying GIL is that each graph can be divided into two distinct components, namely \textit{invariant subgraph} and \textit{environment subgraph}. The former exhibits deterministic and solid predictive relationships with the label of the graph, while the latter may show spurious correlations with the labels and can vary significantly in response to distribution shifts~\citep{yang2022learning,chen2024does}. 
Theoretically, if invariant and environmental information can be accurately separated, GIL-based methods can effectively learn invariant representations from the input graph and make reliable predictions in OOD scenarios. Consequently, existing methods primarily focus on \textbf{capturing and modeling the environmental subgraphs,} employing carefully designed loss functions to minimize the correlations between the predicted environmental subgraphs and the labels~\citep{gui2024joint,piao2024improving}, or utilizing data augmentation strategies to simulate potential environments in wild data~\citep{wu2022handling,jia2024graph}.

\begin{figure}[t!]   

 \centering
 \subfigure[Diversity of environments.]{
   \includegraphics[width=0.21\textwidth]{1_intro/intro1.pdf}
   \label{subfig:intro1}
 } 
 \subfigure[Semantic cliff across classes.]{
   \includegraphics[width=0.21\textwidth]{1_intro/intro2.pdf}
   \label{subfig:intro2}
}
\vspace{-5mm}
\caption{Case examples of two challenges faced by graph out-of-dirstribution generalization}
\vspace{-5mm}
\label{fig:moti}
\end{figure}
{Although GIL-based methods are theoretically viable, they directly apply the concept of environment to the graph domain,  overlooking the \textbf{difficulties in capturing environment information} in real-world graph data.} These difficulties can be attributed to the \textit{diversity}, \textit{distinguishability}, and \textit{lack of labels} of practical graph environments. 
Specifically, the environments of graph data can exhibit substantial diversity in terms of sizes, shapes, and topological properties. Taking molecular graphs as an example, as shown in Fig.~\ref{subfig:intro1}, invariance can be associated with functional groups that determine specific chemical properties, whereas environmental factors manifest as structural variations such as different scaffolds and side chains~\citep{zhuang2023learning}, or assay and size of the molecular~\citep{drugood}. In this context, even with augmented or reorganized environments during training, GNN models struggle to identify all forms of environment subgraphs in real-world OOD samples.  Moreover, unlike image data, where environments can be explicitly defined (e.g., background or style)~\citep{lin2022zin}, the structural boundaries between invariant and environmental subgraphs in graph data are often indistinct, making environmental information difficult to delineate and label. Given the above difficulties, most existing GIL-based approaches struggle to effectively identify environmental information, leading to suboptimal performance, as empirically demonstrated by~\citep{chen2024does}. Consequently, a natural question arises: \textbf{\textit{(RQ1) Can we consider a more feasible way to learn invariant representations on graphs without explicitly modeling the environment information?}}

% While separability has been proven crucial in the graph learning domain~\citep{xia2024understanding}, it has not yet been incorporated into GIL for graph OOD generalization tasks.
{Furthermore, recent theoretical studies in OOD generalization emphasize that ensuring \textit{inter-class separation} is essential alongside learning invariant representation~\citep{ye2021towards,bai2024hypo}. However, no studies have yet incorporated this into GIL for graph OOD generalization. Motivated by the inherent properties of graphs~\citep{xia2024understanding}, we introduce another critical challenge in GIL, referred to as the \textbf{semantic cliff across different classes}.}
For instance, as shown in Fig.~\ref{subfig:intro2}, invariant subgraphs (e.g., functional groups) across different molecular classes often share similar structural characteristics, with distinctions frequently limited to a single atom or bond. In such cases, the decision boundaries between invariant representations can become blurred, exacerbating the challenge of separating classes, particularly when the invariant and environment subgraphs are not distinctly identifiable. Nevertheless, existing GIL-based methods primarily emphasize the extraction of invariant information while overlooking their inter-class separability, thereby resulting in suboptimal generalization performance. Given this shortage, a natural follow-up question arises: \textbf{\textit{(RQ2) How can we develop a more robust OOD generalization approach that better discriminates between invariant representations across different classes?}}

In this paper, our unique motivation lies in simultaneously addressing the above two research questions, for which we propose a novel \textbf{M}ulti-\textbf{P}rototype \textbf{H}yperspherical \textbf{I}nvariant \textbf{L}earning (\ourmethod for short) method. \ourmethod is built upon a novel framework, termed hyperspherical graph invariant learning, which incorporates two advanced design: 1)~\textbf{hyperspherical invariant representation extraction}, which maps the extracted invariant representations to hyperspherical space to enhance separability, and 2)~\textbf{multi-prototype hyperspherical classification }, which assigns multiple prototypes to each class and employs them as intermediate variables between the input and labels, thereby eliminating the necessity for explicit environmental modeling.
% More specifically, building upon our hyperspherical invariant learning framework, we propose a learning objective tailored for out-of-distribution generalization in real-world graph scenarios. This objective integrates two carefully designed loss terms, denoted as A and B. The first ensures that samples from the same class are correctly assigned to the corresponding prototype, thereby mitigating the interference from environmental factors and addressing RQ1. The second loss term pulls prototypes of the same class closer together while ensuring that those from different classes remain dissimilar. In this manner, $\mathcal{L}_{PS}$ enhances class separability within the context of OOD generalization and alleviates the semantic cliff issue presented in RQ2.
More specifically, we propose a more practical learning objective for \ourmethod that incorporates two well-crafted loss terms. To address \textbf{\textit{(RQ1)}}, we design an \textit{invariant prototype matching loss} ($\mathcal{L}_{IPM}$) that ensures samples from the same class are always 
assigned to the same class prototype in hyperspherical space. In this way, $\mathcal{L}_{IPM}$ can allow the model to extract robust invariant features across varying environments without explicitly modeling them. To answer \textbf{\textit{(RQ2)}}, we produce a \textit{prototype separation loss} ($\mathcal{L}_{PS}$) that pulls the prototypes belonging to the same class closer together while ensuring those from different classes remain dissimilar. In this way, $\mathcal{L}_{PS}$ enhances the class separability in the context of OOD generalization and mitigates the semantic cliff issue in graph data. %Innovatively, we propose to assign multiple prototypes for each class, which ensures adaptable decision boundaries and greater tolerance to environmental changes. 
To sum up, the main contributions of this paper are as follows:

\begin{itemize}[leftmargin=*]
    \item \textbf{Framework.} Derived from the objective of GIL, we introduce a novel hyperspherical graph invariant learning framework along with its corresponding  learning objective, ensuring robust invariant representation learning while eliminating the need for explicit environment modeling. 
    
    \item \textbf{Methodology.} Based on the proposed framework, we develop a novel graph OOD generalization method termed \ourmethod. It incorporates two key innovations: hyperspherical invariant representation extraction and  multi-prototype classification mechanism.
    
    \item \textbf{Experiments.}  We conduct extensive experiments to validate the effectiveness of \ourmethod, and the results demonstrate its superior generalization ability compared to state-of-the-art methods across various types of distribution shifts.
\end{itemize}

% \begin{figure*}[h]
% \centering 
% \subfigure[Two molecular graphs with the same functional group but a different scaffold] { 
% \includegraphics[width=0.4\columnwidth]{1_intro/fig1.pdf} 
% }
% \subfigure[K] { 
% \includegraphics[width=0.4\columnwidth]{1_intro/fig2 (2).pdf} 
% }

% \caption{Left, Middle:} 
% \label{t-sne}
% \vspace{-1pt}
% \end{figure*}
% \begin{figure*}[t!] 
% \centering    
% \includegraphics[scale=0.39]{1_intro/sx_fig1(1).png}
% \caption{Upper: molecular graphs with the same functional group but different scaffolds. down:  molecular graphs with the different functional group which is too similar. }  
% \label{fig:intro} 
% \end{figure*}