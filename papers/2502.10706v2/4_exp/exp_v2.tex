
In this section, we present our experimental setup  (Sec.~\ref{subsec:setup}) and showcase the results in (Sec.~\ref{subsec:results}). For each experiment, we first highlight the research question being addressed, followed by a detailed discussion of the findings.


\input{tables/main}

\subsection{Experimental Setup}\label{subsec:setup}
\textbf{Datasets.} We evaluate the performance of \ourmethod on two real-world benchmarks, GOOD~\citep{good} and DrugOOD~\citep{drugood}, with various distribution shifts to evaluate our method. Specifically, GOOD is a comprehensive graph OOD benchmark, and we selected three datasets: (1) GOOD-HIV~\citep{wu2018moleculenet}, a molecular graph dataset predicting HIV inhibition; (2) GOOD-CMNIST~\citep{arjovsky2019invariant}, containing graphs transformed from MNIST using superpixel techniques; and (3) GOOD-Motif~\citep{wu2022discovering}, a synthetic dataset where graph motifs determine the label. DrugOOD is designed for AI-driven drug discovery with three types of distribution shifts: scaffold, size, and assay, and applies these to two measurements (IC50 and EC50). Details of datasets are in Appendix \ref{appe:data}.%applied to six molecular datasets for predicting drug-target binding affinity.
%We employ two real-world benchmarks containing various distribution shifts for graph OOD generalization to evaluate the effectiveness of our method.
% \begin{itemize}
%     \item \textbf{GOOD} is a systematic and comprehensive graph OOD benchmark, offering detailed distributions partition across various types of graphs. For the graph classification task, we selected three distinct datasets: (1) GOOD-HIV, a molecular graph dataset with the task of binary classification to predict whether a molecule can inhibit HIV; (2) GOOD-CMNIST, which consists of graphs representing hand-written digits transformed from the MNIST database using superpixel techniques; and (3) GOOD-Motif, a synthetic dataset where each graph is formed by connecting a base graph with a motif, where the motif alone determines the label.
%     \item \textbf{DrugOOD} is an OOD benchmark designed specifically for AI-driven drug discovery, where the data consists of molecular graphs. DrugOOD includes three basis of distribution shift: scaffold, size, and assay, and applies these to two measurements (IC50 and EC50). The benchmark contains six datasets, and all of them are tasked with predicting drug-target binding affinity, framed as a binary classification problem.
% \end{itemize}

\noindent\textbf{Baselines}.  We compare \ourmethod against ERM and two kinds of OOD baselines: (1)~Traditional OOD generalization approaches, including  Coral~\citep{coral}, IRM~\citep{arjovsky2019invariant} and VREx~\citep{krueger2021out}; (2)~graph-specific OOD generalization methods, including environment-based approaches (MoleOOD~\citep{yang2022learning}, CIGA~\citep{chen2022learning}, GIL~\citep{li2022learning}, and GREA~\citep{liu2022graph}, IGM~\citep{jia2024graph}), causal explanation-based approaches (Disc~\citep{fan2022debiasing} and DIR\citep{wu2022discovering}), and advanced architecture-based approaches (CAL~\citep{sui2022causal} and GSAT~\citep{miao2022interpretable}, iMoLD~\citep{zhuang2023learning}), GALA~\citep{Equad}, EQuAD~\citep{gala}. Details of all baselines are in Appendix \ref{appe:baseline}.

\noindent\textbf{Implementation Details}. To ensure fairness, we adopt the same experimental setup as iMold across two benchmarks. For molecular datasets with edge features, we use a three-layer GIN with a hidden dimension of 300, while for non-molecular graphs, we employ a four-layer GIN with a hidden dimension of 128. The projector is a two-layer MLP with a hidden dimension set to half that of the GIN encoder. EMA rate $\alpha$ for prototype updating is fixed at 0.99. Adam optimizer is used for model parameter updates.  All baselines use the optimal parameters from their original papers. Additional hyperparameter details can be found in Appendix~\ref{appe:hyperparam}.

\subsection{Performance Comparison}\label{subsec:results}

In this experiment, we aim to answer
\textbf{Q1: Whether \ourmethod achieves the best performance on OOD generalization benchmarks?} 
The answer is \textbf{YES}, since \ourmethod shows the best results on the majority of datasets. Specifically, we have the following observations.

 \noindent$\rhd$ \textsf{State-of-the-art results.}
According to Table~\ref{tab:main}, \ourmethod achieves state-of-the-art performance on 9 out of 11 datasets, and secures the second place on the remaining dataset. The average improvements against the previous SOTA are $2.17\%$ on GOOD and $1.68\%$ on DrugOOD. Notably, \ourmethod achieves competitive performance across various types of datasets with different data shifts, demonstrating its generalization ability on different data. Moreover, our model achieves the best results in both binary and multi-class tasks, highlighting the effectiveness of the multi-prototype classifier in handling different classification tasks.

 \noindent$\rhd$ \textsf{Sub-optimal performance of environment-based methods.}
Among all baselines, environment-based methods only achieve the best performance on 3 datasets, while architecture-based OOD generalization methods achieve the best results on most datasets. These observations suggest that environment-based methods are limited by the challenge of accurately capturing environmental information in graph data, leading to a discrepancy between theoretical expectations and empirical results. In contrast, the remarkable performance of \ourmethod also proves that graph OOD generalization can still be achieved without specific environmental information.
% According to Table~\ref{tab:main}, \ourmethod shows \textit{state-of-the-art performance on 10 out of 11 datasets} and secures second place on the remaining dataset. Notably, \ourmethod achieves competitive performance across various types of datasets with different data shifts, demonstrating its superior ability to achieve environment invariance. The superior performance of \ourmethod in both binary and multi-class tasks highlights the strength of the multi-prototype-based classification approach. In contrast, we find that the \textit{environment-based methods display sub-optimal performance} in most cases, demonstrating their limitations in capturing the environments. The remarkable performance of \ourmethod also proves that graph OOD generalization can still be achieved without specific environmental information.
\begin{figure*}[!t]
    \centering
    \subfigure[Sensitivity of $k$]{ \label{subfig:paramk}
    \includegraphics[height=0.16\textwidth]{4_exp/param_hiv_k.pdf}
    }
    \hfill
    \subfigure[Sensitivity of $\beta$]{ \label{subfig:paramb}
    \includegraphics[height=0.16\textwidth]{4_exp/param_hiv_beta.pdf}
    }
    \hfill
    \subfigure[Impact of prototype updating mechanisms]{ \label{subfig:update}
    \includegraphics[height=0.16\textwidth]{4_exp/init.pdf}
    }
    \hfill
    \subfigure[Impact of different statistical metrics]{ \label{subfig:metric}
    \includegraphics[height=0.16\textwidth]{4_exp/std.pdf}
    }
    \vspace{-4mm}
    \caption{The two figures on the left present a hyperparameter analysis of the  $K$ and $\beta$, while the two figures on the right illustrate the comparison of different module designs on prototype update and metric used in Eq.~\eqref{classify}. } 
    \vspace{-2mm}
    \label{fig:four_images}
    
\end{figure*}
\subsection{Ablation Study}
We aim to discover 
\textbf{Q2: Does each module in \ourmethod contribute to effective OOD generalization?} The answer is \textbf{YES}, as removing any key component leads to performance degradation, as demonstrated by the results in Table~\ref{tab:ablation}. We have the following discussions.%according to our ablation experiments that verify the effectiveness of the loss constraint (i.e., $\mathcal{L}_{\mathrm{IPM}}$ and $\mathcal{L}_{\mathrm{PS}}$), and the components of our model, including hyperspherical projection, multi-prototype mechanism, weight updating, and weight pruning techniques. The results are shown in Table~\ref{tab:ablation}, with the following discussions.

% as we conduct experiments on three datasets to verify the role of our proposed loss constraint $\mathcal{L}_{\mathrm{IPM}}$, $\mathcal{L}_{\mathrm{PS}}$, and the component of our model: projector and weight pruning technique. The results are shown in Table~\ref{ablation}.
\input{tables/ablation}
\noindent$\rhd$ \textsf{Ablation on $\mathcal{L}_{\mathrm{IPM}}$ and $\mathcal{L}_{\mathrm{PS}}$.}
We remove $\mathcal{L}_{\mathrm{IPM}}$ and $\mathcal{L}_{\mathrm{PS}}$ in the Eq.~\eqref{eq: target2} respectively to explore their impacts on the performance of graph OOD generalization. The experimental results demonstrate a clear fact: merely optimizing for invariance (w/o $\mathcal{L}_{\mathrm{PS}}$) or separability (w/o $\mathcal{L}_{\mathrm{IPM}}$) weakens the OOD generalization ability of our model, especially for the multi-class classification task, as shown in CMNIST-color. This provides strong evidence that ensuring both invariance and separability is a sufficient and necessary condition for effective OOD generalization in graph learning.
\noindent$\rhd$ \textsf{Ablation on the design of \ourmethod.} To verify the effectiveness of each module designed for \ourmethod,
we conducted ablation studies by removing the hyperspherical projection(w/o Project), multi-prototype mechanism (w/o Multi-P), invariant encoder (w/o Inv.Enc), and prototype-related weight calculations (w/o Update) and pruning (w/o Prune). The results confirm their necessity. First, removing the hyperspherical projection significantly drops performance, as optimizing Eq.~(\ref{eq: target2}) requires hyperspherical space. Without it, results are even worse than ERM. Similarly, setting the prototype count to one blurs decision boundaries and affects the loss function $\mathcal{L}_{\mathrm{PS}}$, compromising inter-class separability. Lastly, replacing the invariant encoder $\mathrm{GNN}_{S}$ with $\mathrm{GNN}_{E}$ directly introduces environment-related noise, making it difficult to obtain effective invariant features, thus hindering OOD generalization.
Additionally, the removal of prototype-related weight calculations and weight pruning degraded prototypes into the average of all class samples, resulting in the prototypes degrading into the average representation of all samples in the class, failing to maintain classification performance in OOD scenarios.

\subsection{Visualized Validation}
In this subsection, we aim to investigate \textbf{Q3: Can these key designs (i.e., hyperspherical space and multi-prototype mechanism) tackle two unique challenges in graph OOD generalization tasks?} The answer is \textbf{YES}, we conduct the following visualization experiments to verify this conclusion. %\textbf{Q3: Whether hyperspherical Spaces are more separable than ordinary latent Spaces} Yes, the visualization shows the advantage of hyperspherical space over traditional latent space.

\noindent$\rhd$ \textsf{Hyperspherical representation space.} To validate the advantage of hyperspherical space in enhancing class separability, we compare the 1-order Wasserstein distance~\cite{villani2009optimal} between same-class and different-class samples, as shown in Fig.~\ref{wl_Dis}.
It is evident that \ourmethod produces more separable invariant representations (higher inter-class distance), while also exhibiting tighter clustering for samples of the same class (lower intra-class distance). In contrast, although traditional latent spaces-based SOTA  achieves a certain level of intra-class compactness, its lower separability hinders its overall performance. Additionally, we visualized the sample representations learned by our \ourmethod and SOTA using t-SNE in Fig.~\ref{tsne}, where corresponding phenomenon can be witnessed.
% we visualized the invariant representation 
% $\hat{z}_{inv}$ in both the training set (ID) and the test set (OOD). Using t-SNE, we visualize the representation distributions learned by \ourmethod and the SOTA method, CIGA, as shown in Fig.~\ref{fig:tsne}. 
% \begin{figure*}[t!] \label{v_prototype}
% \centering    
% \includegraphics[scale=0.39]{3_method/sx_prototype.pdf}
% \caption{In the case of the DrugOOD-IC50-assay (binary classification task), we set up three prototypes for each class and visualized the closest example to it. }  
% \label{fig:intro} 
% \end{figure*}





\begin{figure}[t]
\vspace{-3mm}
\centering 
\subfigure[1-order Wasserstein distance]{ \label{wl_Dis}
\includegraphics[width=0.17\textwidth,height=0.13\textwidth]{4_exp/wl_score.pdf}
}
\subfigure[T-SNE visualization, left: \ourmethod, right: SOTA ]{ \label{tsne}
\includegraphics[width=0.13\textwidth,height=0.13\textwidth]{4_exp/my.pdf}
\includegraphics[width=0.13\textwidth,height=0.13\textwidth]{4_exp/imold.pdf}
}
\vspace{-3mm}
\caption{Visualization and quantitative analysis of the separability advantages in hyperspherical space on HIV-scaffold.}
\vspace{-4mm}
\end{figure}

% \begin{figure}[t]
%     \centering
%     % \captionsetup[wragfigure]{ font=footnotesize}

%     \includegraphics[width=0.45\textwidth]{4_exp/wl_score.pdf}

%     \caption{The 1-order WL distance between samples of the same class (D(Y=0), D(Y=1)) and between samples of different classes (inter-distance).}
%     \vspace{-10pt}
%     \label{fig:wl}
% \end{figure}
\noindent$\rhd$ \textsf{Prototypes visualization.} We also reveal the characteristics of prototypes by visualizing samples that exhibit the highest similarity to each prototype. Fig.~\ref{fig:proto} shows that prototypes from different classes capture distinct invariant subgraphs, ensuring a strong correlation with their respective labels. Furthermore, within the same category, different prototypes encapsulate samples with varying environmental subgraphs. This validates that multi-prototype learning can effectively capture label-correlated invariant features without explicit environment definitions, which solve the challenges of out-of-distribution generalization in real-world graph data.
\begin{figure}[t]
    \centering
    % \captionsetup[wragfigure]{ font=footnotesize}

    \includegraphics[width=0.45\textwidth]{4_exp/proto.pdf}

    \caption{Visualizations of prototypes and invariant subgraphs~(highlighted) of IC50-assay dataset.}
    \label{fig:proto}
\vspace{-5mm}
\end{figure}

\subsection{In-Depth Analysis}
In this experiment, we will investigate \textbf{Q4: How do the details (hyperparameter settings and variable designs) in \ourmethod impact performance?} The following experiments are conducted to answer this question and the experimental results are in Fig.~\ref{fig:four_images}.

\noindent$\rhd$ \textsf{Hyperparameter Analysis.}
To investigate the sensitivity of the number of prototypes and the coefficient $\beta$ in $\mathcal{L}_{\mathrm{IPM}}$ on performance, we vary $k$ from 2 to 6 and $\beta$ from $\{0.01, 0.05, 0.1, 0.2, 0.3\}$. Our conclusions are as follows: \ding{192} In Fig.~\ref{subfig:paramk}, the best performance is achieved when the number of prototypes is approximately twice the number of classes. Deviating from this optimal range, either too many or too few prototypes negatively impacts the final performance. \ding{193} According to Fig.~\ref{subfig:paramb}, a smaller $\beta$  hampers the model’s ability to effectively learn invariant features, while selecting a moderate $\beta$ leads to the best performance.
 
% \noindent$\rhd$ \textsf{Numerical quantitative analysis} Analysis of $\beta$. To discover the sensitivity of \ourmethod to coefficient $\beta$ in $\mathcal{L}_{\mathrm{IPM}}$, we search $\beta$ from $\{0.01, 0.05, 0.1, 0.2, 0.3\}$ and present the results in Fig.~\ref{hyper_3} and \ref{hyper_4}. We observe that a small $\beta$ (e.g., 0.01 and 0.05) hampers the model’s ability to effectively learn invariant features, while selecting a moderate $\beta$ (i.e., 0.1) leads to the best performance.

\noindent$\rhd$ \textsf{Module design analysis.}
To investigate the impact of different prototype update mechanisms and statistical metrics in Eq.~\eqref{classify}, we conducted experimental analyses and found that \ding{192} According to Fig.~\ref{subfig:update}, all compared methods lead to performance drops due to their inability to ensure that the updated prototypes possess both intra-class diversity and inter-class separability, which is the key to the success of MPHIL's prototype update method. \ding{193} In Fig.~\ref{subfig:metric}, $\max$ achieves the best performance by selecting the most similar prototype to the sample, helping the classifier converge faster to the correct decision space.
% To discover the sensitivity of \ourmethod to coefficient $\beta$ in $\mathcal{L}_{\mathrm{IPM}}$, we search $\beta$ from $\{0.01, 0.05, 0.1, 0.2, 0.3\}$ and present the results in Fig.~\ref{hyper_3} and \ref{hyper_4}. We observe that a small $\beta$ (e.g., 0.01 and 0.05) hampers the model’s ability to effectively learn invariant features, while selecting a moderate $\beta$ (i.e., 0.1) leads to the best performance.



% \begin{figure*}[h]
% \centering 
% \subfigure[CIGA] { 
% \includegraphics[width=0.4\textwidth]{imold.pdf} \label{t1}
% }
% \subfigure[Ours] { 
% \includegraphics[width=0.4\textwidth]{my(1).pdf} \label{t2}
% }
% \caption{Left, Middle:} 
% \label{t-sne}
% \vspace{-1pt}
% \end{figure*}

