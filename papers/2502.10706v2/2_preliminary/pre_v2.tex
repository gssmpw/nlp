In this section, we introduce the preliminaries and background of this work, including the formulation of the graph OOD generalization problem, graph invariant learning, and hyperspherical embeddings.

% \subsection{Problem Formulation}
\subsection{Problem Formulation} 
In this paper, we focus on the OOD generalization problem on graph classification tasks~\citep{li2022out,jia2024graph,fan2022debiasing,wu2022discovering}. We denote a graph data sample as $(G,y)$, where $G \in \mathcal{G}$ represents a graph instance and $y \in \mathcal{Y}$ represents its label. The dataset collected from a set of environments $\mathcal{E}$ is denoted as $\mathcal{D} = \{\mathcal{D}^{e}\}_{e \in \mathcal{E}}$, where $\mathcal{D}^{e} =\{({G}^{e}_{i},{y}^{e}_{i})\}^{n^{e}}_{i=1}$ represents the data from environment $e$, and $n^e$ is the number of instances in environment $e$. Each pair $({G}^{e}_{i}, {y}^{e}_{i})$ is sampled independently from the joint distribution $P_{e}(\mathcal{G}, \mathcal{Y}) = P(\mathcal{G}, \mathcal{Y} | e)$. 
In the context of graph OOD generalization, the difficulty arises from the discrepancy between the training data distribution $P_{e_{tr}}(\mathcal{G}, \mathcal{Y})$ from environments $e_{tr} \in \mathcal{E}_{tr}$, and the testing data distribution $P_{e_{te}}(\mathcal{G}, \mathcal{Y})$ from unseen environments $e_{te} \in \mathcal{E}_{test}$, where $\mathcal{E}_{te} \neq \mathcal{E}_{tr}$. The goal of OOD generalization is to learn an optimal predictor $f: \mathcal{G} \rightarrow \mathcal{Y}$ that performs well across both training and unseen environments, $\mathcal{E}_{all} = \mathcal{E}_{tr} \cup \mathcal{E}_{te}$, i.e., 
\begin{equation}
\label{eq: OOD_target} \min_{f \in \mathcal{F}} \max_{e \in \mathcal{E}_{\mathrm{all}}} \mathbb{E}_{(G^{e}, y^{e}) \sim P_{e}}[\ell(f(G^e), y^e)], 
\end{equation}
where $\mathcal{F}$ denotes the hypothesis space, and $\ell(\cdot,\cdot)$ represents the empirical risk function. 

\subsection{Graph Invariant Learning (GIL)}
Invariant learning focuses on capturing representations that preserve consistency across different environments, ensuring that the learned invariant representation $\mathbf{z}_{inv}$ maintains consistency with the label $y$~\citep{mitrovic2020representation,wu2022discovering,chen2022learning}. Specifically, for graph OOD generalization, the objective of GIL is to learn an invariant GNN $f:= f_{c} \circ g$, where $g: \mathcal{G} \rightarrow \mathcal{Z}_{inv}$ is an encoder that extracts the invariant representation from the input graph $G$, and $f_{c}: \mathcal{Z}_{inv} \rightarrow \mathcal{Y}$ is a classifier that predicts the label $y$ based on $\mathbf{z}_{inv}$. From this perspective, the optimization objective of OOD generalization, as stated in Eq.~(\ref{eq: OOD_target}), can be reformulated as: 
\begin{equation}
\label{eq: causal} \max_{f_{c}, g} I(\mathbf{z}_{inv}; y), \text{ s.t. } \mathbf{z}_{inv} \perp e,\forall e \in \mathcal{E}_{tr}, \mathbf{z}_{inv} = g(G),
\end{equation}
{where $I(\mathbf{z}_{inv}; y)$ denotes the mutual information between the invariant representation $\mathbf{z}_{inv}$ and the label $y$.}
This objective ensures that $\mathbf{z}_{inv}$ is independent of the environment $e$, focusing solely on the most relevant information for predicting $y$. 

\subsection{Hyperspherical Embedding} 
Hyperspherical learning enhances the discriminative ability and generalization of deep learning models by mapping feature vectors onto a unit sphere~\citep{liu2017deep}. 
To learn a hyperspherical embedding for the input sample, its representation vector $\mathbf{z}$ is mapped into hyperspherical space with arbitrary linear or non-linear projection functions, followed by normalization to ensure that the projected vector $\hat{\mathbf{z}}$ lies on the unit hypersphere ($\|\hat{\mathbf{z}}\|^{2}=1$). 
To make classification prediction, the hyperspherical embeddings $\hat{\mathbf{z}}$ are modeled using the von Mises-Fisher (vMF) distribution~\citep{ming2022exploit}, with the probability density for a unit vector in class $c$ is given by:
\begin{equation}
\label{eq: vMF} p(\hat{\mathbf{z}}; \boldsymbol{\mu}^{(c)}, \kappa ) = Z(\kappa) \exp(\kappa {\boldsymbol{\mu}^{(c)}}^\top \hat{\mathbf{z}}), 
\end{equation} 
where $\boldsymbol{\mu}^{(c)}$ denotes the prototype vector of class $c$ with the unit norm, serving as the mean direction for class $c$, while $\kappa$ controls the concentration of samples around $\boldsymbol{\mu}_c$.
The term $Z(\kappa)$ serves as the normalization factor for the distribution. Given the probability model in Eq.(\ref{eq: vMF}), the hyperspherical embedding $\hat{\mathbf{z}}$ is assigned to class $c$ with the following probability:
\begin{equation} \label{eq: prototpye_1}
    \begin{aligned}
\mathbb{P}\left(y = c \mid \hat{\mathbf{z}}; \{\kappa, \boldsymbol{\mu}^{(i)}\}_{i = 1}^{C}\right) &= \frac{Z(\kappa) \exp \left(\kappa {\boldsymbol{\mu}^{(c)}}^{\top} \hat{\mathbf{z}}\right)}{\sum_{i = 1}^{C} Z(\kappa) \exp \left(\kappa {\boldsymbol{\mu}^{(i)}}^{\top} \hat{\mathbf{z}}\right)}\\ &= \frac{\exp \left({\boldsymbol{\mu}^{(c)}}^{\top} \hat{\mathbf{z}} / \tau\right)}{\sum_{i = 1}^{C} \exp \left({\boldsymbol{\mu}^{(i)}}^{\top} \hat{\mathbf{z}}/ \tau\right)},
    \end{aligned}
\end{equation}
where $\tau = 1/\kappa$ is a temperature parameter. In this way, the classification problem is transferred to the distance measurement between the graph embedding and the prototype of each class in hyperspherical space, where the class prototype is usually defined as the embedding centroid of each class.