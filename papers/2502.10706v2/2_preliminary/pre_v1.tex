\subsection{notion and problem formulation}
OOD generalization in graph classification presents significant challenges due to varying distributions across different environments.  In this setting, we consider a pair of random variables: $G \in \mathcal{G}$, representing a graph instance, and $y \in \mathcal{Y}$, its corresponding label. The supervised training data is collected from multiple environments, denoted as $\varepsilon_{tr}$. Formally, the dataset can be expressed as $D = \{D^{e}\}_{e \in \varepsilon_{tr}}$, where $D^{e} =\{(\mathcal{G}^{e}_{i},\mathcal{Y}^{e}_{i})\}^{n^{e}}_{i=1}$  represents the data from environment $e$, and $n^e$ is the number of instances in environment $e$. Each pair $(\mathcal{G}^{e}_{i}, \mathcal{Y}^{e}_{i})$ is sampled independently from the joint distribution $P_{e}(\mathcal{G}, \mathcal{Y}) = P(\mathcal{G}, \mathcal{Y} | e)$.


% We focus on the OOD generalization for graph classification that involves a pair of random variables over instances $G \in \mathcal{G} $ and $y \in \mathcal{Y}$, where $G$ represents the graph and $\mY$ represents its corresponding label.  Generally, our supervised training dataset are collected from a set of training environments $\varepsilon_{tr}: D =\{D^{e} \}_{e \in \varepsilon_{tr}}$, where $D^{e} =\{(\mG^{e}_{i},\mY^{e}_{i})\}^{n^{e}}_{i=1}$ is the dataset from environment $e \in \varepsilon_{tr}$ and $n^{e}$ is the number of instances in environment $e$. Each $(\mG^{e}_{i},\mY^{e}_{i})$ is an i.i.d. draw from the joint distribution $P_{e}(\mG,\mY) = P(\mG,\mY | e)$.

Graph classification under OOD settings introduces additional complexities compared to traditional supervised tasks. This difficulty arises from the discrepancy between the training data distribution $P_{e_{tr}}(\mathcal{G}, \mathcal{Y})$ for environments $e_{tr} \in \varepsilon_{tr}$, and the testing data distribution $P_{e_{test}}(\mathcal{G}, \mathcal{Y})$ for unseen environments $e_{test} \in \varepsilon_{test}$. The goal of OOD generalization is to learn an optimal predictor $f: \mathcal{G} \rightarrow \mathcal{Y}$ that performs well across both training and unseen environments, $\varepsilon_{all} = \varepsilon_{tr} \cup \varepsilon_{test}$. This can be formalized as the following optimization problem: 
\begin{equation}
\label{eq: OOD_target} \min_{f \in \mathcal{F}} \max_{e \in \mathcal{E}_{\mathrm{all}}} \mathbb{E}_{(G^{e}, Y^{e}) \sim P_{e}}[\ell(f(G^e), Y^e)], 
\end{equation}
where $\mathcal{F}$ denotes the hypothesis space, and $\ell(\cdot,\cdot)$ represents the empirical risk function. However, the complexity of graph structures makes it challenging to define meaningful environments for OOD generalization.

% where $\mathcal{F}$ is hypothesis space and $\ell(\cdot,\cdot )$ is the empirical risk function. The definition of environment is natural and easily accessible in fields such as computer vision. However, it is challenging or even impossible for a graph $G=(A,X)$ with structure information $A$ and feature due to its  non-Euclidean complexity.


% \begin{figure}[htbp]
%     \centering
%     \subfigure[Covariate SCM{
%         \label{scm1} \includegraphics[width=0.3\textwidth,height=0.3\textwidth]{ICLR 2025 Template/scm1.png} % 替换 image_a 为你的图片文件名
%     }
%     \subfigure[FIIF SCM]{
%         \label{scm2} \includegraphics[width=0.3\textwidth,height=0.3\textwidth]{ICLR 2025 Template/scm2.png} % 替换 image_b 为你的图片文件名
%     }
%     \subfigure[PIIF SCM]{
%          \includegraphics[width=0.3\textwidth,height=0.3\textwidth]{ICLR 2025 Template/scm3.png} % 替换 image_d 为你的图片文件名
%     }
%     \caption{Three structural causal models (SCMs) on graph distribution shifts, each node denotes a variable, and each directional edge represents causation. Grey nodes are variables that can be directly observed, white nodes means variables that can't get accessed.}
%     \label{fig_scm}
% \end{figure}

\subsection{Invariant learning}
% We build upon the graph generation process proposed by Chen, which has been widely adopted in research on analyse OOD generalization. As illustrated in Figure \ref{fig_scm}, the causal feature $G_{c}$ and the spurious feature $G_s$ jointly control the generation of the observed input feature of the graph $G$. Environmental factors $e$ influence $G_s$, causing it to vary in different environments , while $G_c$ remains stable. The causal feature $Z$ directly controls the label $y$, maintaining a stable relationship with it. In OOD scenarios, distribution shifts typically correspond to changes in environmental factors $e$ but with a  stable $G_c$. Moreover, the generation of $G_s$ may be influenced by the labels $y$ or the causal feature $G_c$, as depicted in Figures 1 (b) and 1(c), respectively.

Invariant learning focuses on capturing representations that remain consistent across different environments, aiming to ensure that the learned representation $z_{inv}$ maintains a stable relationship with the label $y$. Specifically, for graph OOD generalization, our goal is to learn an invariant GNN, $f:= f_{c} \circ g$, where $g: \mathcal{G} \rightarrow z_{inv}$ is an encoder that extracts the invariant representation from the input graph $G$, and $f_{c}: z_{inv} \rightarrow \mathcal{Y}$ is a classifier that predicts the label $y$ based on $z_{inv}$. From this perspective, the optimization objective of OOD generalization, as stated in Eq.\ref{eq: OOD_target}, can be reformulated as: 
\begin{equation}
\label{eq: causal} \max_{f_{c}, g} I(z_{inv}; y), \text{ s.t. } z_{inv} \perp e,\forall e \in \mathcal{E}_{tr}, z_{inv} = g(G). 
\end{equation}
This objective ensures that $z_{inv}$ remains independent of the environment $e$, focusing solely on the most relevant information for predicting $y$. While invariant learning offers a promising approach, it faces limitations in graph data, particularly due to the "semantic cliff." Small changes in a graph’s structure can lead to large shifts in labels, making the invariant representation of different class are not well-separable. This sensitivity highlights the inherent limitations of applying invariant learning to graphs.


% Invariant learning is primarily grounded in the data generation process shown in Figure 1, with the goal of enabling the model to learn the representation $z_{inv}$, which remains invariant across environments and exhibits the most stable correlation with the label $y$, treating it as the representation of  $G_{c}$. Specifically, for graph OOD generalization, our aim is to learn an invariant GNN $f:= f_{c} \circ 
%  h$, which is composed of a encoder $h : \mathcal{G} \rightarrow z_{inv}$ that extract invariant representation from the input graph $G$, and a classifier $f_{c} : z_{inv} \rightarrow \mathcal{Y}$ that predicts the label $y$ according to the $z_{inv}$. From tis prospective, the optimization objective of OOD generalization in Eq. (\ref{OOD_target}) can be reformulated as follows:
%  \begin{equation}\label{eq_causal}
%  \max_{f_{c}, h} I(z_{inv}; y), \text{ s.t. } z_{inv} \perp e,\forall e \in \mathcal{E}_{tr}, z_{inv} = h(G).
%  \end{equation}
%  This object ensures that $z_{inv}$ remains independent of the environment $E$, providing only the most relevant information about 
% $y$ under an additional constraint. However, identifying the precise environment of a graph proves to be challenging, even not infeasible, which presents a significant obstacle for achieving OOD generalization in graphs through invariant learning.
%\begin{equation}\label{eq_ib}
 %   \underset{\mZ}{\min} -I(\mY;\mZ)+\beta I(\mZ;\mX)
%\end{equation}
%where $I(\cdot;\cdot)$ is the mutual information between two variable and $\beta$ is the Lagrangian parameter to control the trade-off between the two terms. From the perspective of invariant learning, the Information Bottleneck (IB) approach can be viewed as an effective method for extracting features $z_{inv}=z$ from input $x$ that are invariant to the environment. This is because the first term of the Eq. (\ref{eq_ib}) ensures that $z$ retains the most relevant information for predicting the target $Y$, while the second term aim to compress the mutual information of $Z$ and $X$  to make $Z$ ignores irrelevant part of X with respect to the target $Y$. Thus the representation $Z$ which satisfy IB is more robust to noise and distribution shift and it has been proven to be an effective solution to solve the OOD generalization problem by several recent studies.
\subsection{graph hyperspherical embedding}
Hyperspherical embeddings enhance the discriminative ability and generalization of the model by mapping feature vectors onto a unit sphere. For a input graph $G$, its vector representation $z$ are encoded by a GNNs and then projected into a lower-dimensional hyperspherical space, followed by normalization to ensure that the projected vector $\hat{z}$ lies on the unit hypersphere.
% with $\hat{z} = MLP(z)$ and $\hat{z} = \hat{z} / \| \hat{z}\|_{2}$.
These hyperspherical embeddings $\hat{z}$ are modeled using the von Mises-Fisher (vMF) distribution, where the probability density for a unit vector in class $c$ is given by:
\begin{equation}
\label{eq: vMF} p(\hat{z}; \mu_c, \kappa ) = Z_d(\kappa) \exp(\kappa \mu_c^\top \hat{z}), 
\end{equation} 
where ${\mu}_c$ denotes the prototype of class $c$ with the unit norm and can also be treated as the mean direction of the class $c$, and $\kappa$ controls the concentration of samples around $\boldsymbol{\mu}_c$.
The term $Z_d(\kappa)$ serves as the normalization factor for the distribution. Given the probability model in Eq.(\ref{eq: vMF}), an embedding $\hat{\vz}_{i}$ is assigned to class $c$ with the following probability:
\begin{equation} \label{eq: prototpye_1}
    \begin{aligned}
p\left(y = c \mid \hat{z}; \{\kappa, \mu_{i}\}_{i = 1}^{C}\right) = \frac{Z_{d}(\kappa) \exp \left(\kappa \mu_{c}^{\top} \hat{z}\right)}{\sum_{i = 1}^{C} Z_{d}(\kappa) \exp \left(\kappa \mu_{i}^{\top} \hat{z}\right)} = \frac{\exp \left(\mu_{c}^{\top} \hat{z} / \tau\right)}{\sum_{i = 1}^{C} \exp \left(\mu_{i}^{\top} \hat{z} / \tau\right)},
    \end{aligned}
\end{equation}
where $\tau = 1/\kappa$ is a temperature parameter. While the hyperspherical embedding is naturally more separable and classifier friendly, ensuring its environmental robustness is critical for achieving effective OOD generalization in graph learning. The compatibility of this approach with invariant learning is particularly important, as the latter approach is guaranteed to learn a stable, environment-invariant hyperspherical representation. 


% Formulating the embedding space with a hypersphere model has been shown to be beneficial to obtain a more discriminative and well generalizated representation. We consider a deep neural network based encoder $h_{\theta} : \mathcal{X} \rightarrow \mathbb{R}^{d}$ that extract the feature embedding $z \in \mathbb{R}^{d}$ from the input $X \in \mathcal{X}$ with $z = h(x)$. A projector $g_{\phi} : \mathbb{R}^{d} \rightarrow \mathbb{R}^{\hat{d}}$ with normalization is utilized  to project the embedding $z$ from the original high-dimensional space to a low-dimensional space and obtain the hyperspherical embedding $\hat{z}$ via  $\hat{z} = g_{\phi}(z)$ and $\hat{z} = \hat{z} / \| \hat{z}\|_{2}$. The hyperspherical  embeddings $\hat{z}$ lying on the unit sphere ($\|\hat{z}\|^2 = 1$) can be naturally modeled by the von Mises-Fisher (vMF) distribution. The probability density for a unit vector $\hat{z} \in \mathbb{R}^{\hat{d}}$ in class $c$ is defined as :
% \begin{equation}\label{vMF}
%     p(\hat{\vz}; \boldsymbol{\mu}_c, \kappa )=Z_d(\kappa)\exp(\kappa\boldsymbol{\mu}_c^\top\hat{\vz}),
% \end{equation}

% where $\boldsymbol{\mu}_c$ denotes the prototype with unit norm and can also be treat as the mean direction of the class $c$ with $\boldsymbol{\mu}_c = \frac{1}{K} \sum \displaystyle_{k}\hat{\vz}_{k}, \vy_{k}=c$,  $\kappa$ represents the tightness around the $\boldsymbol{\mu}_c$ and $Z_d(\kappa)$ denotes the normalization factor. 

% Given the probability model in Eq.(\ref{vMF}), an embedding $\hat{\vz}_{i}$ is assigned to class $c$ with the following probability:
% \begin{equation} \label{prototpye_1}
%     \begin{aligned}
% p\left(y = c \mid \mathbf{z} ;\left\{\kappa, \boldsymbol{\mu}_{j}\right\}_{j = 1}^{C}\right)  = \frac{Z_{d}(\kappa) \exp \left(\kappa \boldsymbol{\mu}_{c}^{\top} \mathbf{z}\right)}{\sum_{j = 1}^{C} Z_{d}(\kappa) \exp \left(\kappa \boldsymbol{\mu}_{j}^{\top} \mathbf{z}\right)} 
%  = \frac{\exp \left(\boldsymbol{\mu}_{c}^{\top} \mathbf{z} / \tau\right)}{\sum_{j = 1}^{C} \exp \left(\boldsymbol{\mu}_{j}^{\top} \mathbf{z} / \tau\right)},
%     \end{aligned}
% \end{equation}
% where $\tau = 1/\kappa$ is a temperature parameter. Classification based on the distance to prototypes within hyperspherical embedding spaces has been widely adopted in OOD detection and has been practiced in OOD generalization. However, this approach has yet to gain significant traction within the graph learning community. Moreover, existing methods frequently overlook the robustness of the learned prototypes, that is, we hope that it is also invariant across environments to ensure that the out-of-distribution samples are matched to the correct prototype during classification.
%但我们需要跨环境不变的损失
% \section{motivation}
% Our work is motivated by maximizing inter-class invariance and inter-class separation without context dependence to ensure that the learned representations generalize to out-of-distribution environments while still being correctly classified by the model. This is inspired by the theoretical definition proposed by Ye et al, which shows that the error bound of OOD generalization is related to the \textit{Variation} and \textit{Informativeness} of the representation. The formal definitions are given as follows.

% \begin{definition}(Variation)\label{def_1}
% \textit{The variation of representation $\phi$ across a domain set $\mathcal{E}$ is}
% \begin{equation}
% \mathcal{V}(\phi, \mathcal{E}) = \max_{y \in \mathcal{Y}} \sup_{e, e' \in \mathcal{E}} \rho\left(\mathbb{P}(\phi^e | y), \mathbb{P}(\phi^{e'} | y)\right),
% \end{equation}
% \end{definition}

% \textit{where $\rho(\mathbb{P}, \mathbb{Q})$ is a symmetric distance (e.g., Wasserstein distance, total variation) between two distributions, and $\mathbb{P}(\phi^e | y)$ denotes the class-conditional distribution for features of samples in environment $e$. A feature $\phi$ is $\epsilon$-invariant across $\mathcal{E}$, if $\epsilon \geq \mathcal{V}(\phi, \mathcal{E})$.}
% \begin{definition}(Informativeness) 
% \textit{The informativeness of representation $\phi$ across a domain set $\mathcal{E}$ is}  
% \begin{equation}
% I_(\phi, \mathcal{E}) = \frac{1}{K(K-1)} \sum_{\substack{y \neq y' \\ y, y' \in \mathcal{Y}}} \min_{e \in \mathcal{E}} \rho\left(\mathbb{P}(\phi^e | y), \mathbb{P}(\phi^e | y')\right).
% \end{equation}
% \textit{A feature $\phi$ is $\delta$-informativeness across $\mathcal{E}$, if $\delta \leq \mathcal{V}(\phi, \mathcal{E})$.}
% \end{definition}
% Overall, \textit{Variation} measures whether the learned representations of the same class remain invariant across environments, while \textit{Informativeness} describes the distinguishability of these representations with respect to class. Furthermore, Ye et al. extended these definitions by providing an error bound for OOD generalization, theoretically proving that if a learner trained on in-distribution data can simultaneously minimize Variation and maximize Informativeness, it can generalize to the out-of-distribution.

% It is worth noting that although Variation and Informativeness are theoretically clear and reasonable, directly using them as optimization objectives to improve the generalization ability of GNNs is challenging. First, as mentioned in previous sections, it is difficult to obtain environment information in graph data. Second, the semantic cliff of graphs makes it more challenging to ensure that invariant features also exhibit high Informativeness. In the following sections, we will introduce how we leverage two practical loss functions to learn representations that achieve low \textit{Variation} and high \textit{Informativeness}.
% \subsection{main optimization objective }
% First, we give the overall optimization objective of the model, it can be formulated as follows:
% \begin{equation}
%     \mathcal{L}= \mathcal{L}_{intra-inv} + \mathcal{L}_{inter-sep} +\mathcal{L}_{cls},
% \end{equation}
% Overall, intra-class invariance is achieved by minimizing $\mathcal{L}_{intra-inv}$, while minimizing $\mathcal{L}_{inter-sep}$ is equivalent to maximizing inter-class separability. Finally, $\mathcal{L}_{cls}$ ensures the learned representation remains highly relevant to the label. All these operations are performed in hyperspherical space and do not require environmental information.
% \subsection{intra class invariance without $\mathcal{E}$}
% Now we introduce the loss function $\mathcal{L}_{intra-inv}$ that guarantees the learned representation is invariant across environments $\mathcal{E}$ without relying on detail environmental labels. It can be formulated as follows:
% \begin{equation}
%     \mathcal{L}_{intra-inv}= - \frac{1}{C}\sum_{y_c}^{C} \sum_{i=1}^{|y^c|} \log \frac{\underset{c=\hat{c} = y_c}{\sum}\exp \left( {(z_i^c)^\top (z_j^{\hat{c}}) }/{\tau} \right)}{\underset{c=\hat{c}=y_c}{\sum}\exp \left( {(z_i^c)^\top (z_j^{\hat{c}}) }/{\tau} \right)+\underset{c=y_{c},\hat{c} \neq y_c}{\sum}\exp \left( {(z_i^c)^\top (z_j^{\hat{c}}) }/{\tau} \right)} ,
% \end{equation}
% Intuitively, minimizing $\mathcal{L}_{intra-inv}$ corresponds to clustering the invariant features of samples from the same class as tightly as possible within the subspace of the hypersphere corresponding to that class, while ensuring a clear boundary between this subspace and those of other classes. This clustering-like approach does not require environmental information, as it guarantees that invariant representations of the same class remain close, regardless of the environment, thereby ensuring intra-class invariance.

% Next we theoretically prove the effectiveness of this loss. Frist, to align the objective in Definition \ref{def_1} with the cause-effect-based graph generation process shown in Figure 1, we present the following proposition to demonstrate that optimizing Variation is equivalent to satisfying the condition in Eq. (\ref{eq_causal}), meaning that the invariant features are independent of the environment.
% \begin{proposition}
% Minimizing Variation in Definition \ref{def_1} is equivalent to learning a $z_{inv}$ satisfying $z_{inv} \perp E$
% \end{proposition}
% This proposition further illustrates that minimizing intra-class variability guarantees the learned invariant representation and environment independence, and next we will give a theorem proving the relationship between loss function and variation.
% \begin{theorem}
%     if $\max_{y \in \mathcal{Y}}\min_{c=\hat{c}=y}\exp ( (z_i^c)^\top (z_j^{\hat{c}})) = \epsilon $, then we can have $f(\epsilon)  \geq \mathcal{V}(\phi, \mathcal{E})$ and say $z$ is $f(\epsilon)$-invariant across $\mathcal{E}$, where $f$ is  a monotonically increasing function depending on how the distribution distance is calculated, $\rho$. 
% \end{theorem}
% The above theorem theoretically proves that when we minimize the distance between invariant representations of the same category by minimizing the loss function, intra-class invariance can be guaranteed without the help of the environment
% \subsection{inter class separability with prototype ${\mu}^c$}
% We know that optimizing $\mathcal{L}_{intra-inv}$ can yield invariant features, but relying solely on these features is insufficient to achieve robust OOD generalization. It is crucial to ensure that the learned invariant features are class-discriminative, as we do not want the model to capture information unrelated to the label $y$, such as fixed noise. To address this, we propose maximizing inter-class separability to ensure correct classification, which is formally defined as follows:
% \begin{equation}
%     \mathcal{L}_{intra-inv} = \frac{1}{C} \sum_{i=1}^{C} \log \left( \frac{1}{C-1} \sum_{\substack{j \neq i,j \in \mathcal{Y}}} \exp \left( {\mu_i^\top \mu_j}/{\tau} \right) \right),
% \end{equation}
% The effectiveness of optimizing $\mathcal{L}_{intra-inv}$ lies in the fact that our classification is distance-based with respect to prototypes, so increasing the distance between different prototypes helps to enhance separability. Note that in hyperspherical space, prototypes can often be viewed as the center of a class.We also ensure that the learned invariant features match the correct prototype as much as possible by optimizing the classification loss $L_{cls}$ (cross entropy). Thus, ensuring the separability of the prototypes guarantees that the invariant features are also class-separable.

% First of all, we propose the following proposition that guarantees to minimize $L_{cls}$ such that the learned invariant representation is the most relevant to the label.
% \begin{proposition}
% Minimizing $L_{cls}$  is equivalent to learning a $z_{inv}$ that can maximum the $I(z_{inv};y)$.
% \end{proposition}
% However, when labels with similar semantics or invariant subgraphs are fragile, only minimizing $L_{cls}$cannot satisfy the inter-class separability. We propose the following proof to theoretically prove the feasibility of $L_{intra-inv}$.
% \begin{theorem}
%     if $\sum_{y \in \mathcal{Y}}\max{c=y}\exp ( (z_i^c)^\top (\mu^{{c}})) = \epsilon $ and $\sum_{y \in \mathcal{Y}}\min_{c=y, \hat{c}\neq y}\exp ( (z_i^c)^\top (z_j^{\hat{c}})) = \gamma$, then we can have $f(\gamma-2\epsilon)  \leq \mathcal{V}(\phi, \mathcal{E})$ and say $z$ is $f(\gamma-2\epsilon)$-informativeness across $\mathcal{E}$, where $f$ is  a monotonically increasing function depending on how the distribution distance is calculated, $\rho$. 
% \end{theorem}
% The above theorem further ensures that the learned invariant features are inter-class separable by increasing the distance between class prototypes in the hyperspherical space.