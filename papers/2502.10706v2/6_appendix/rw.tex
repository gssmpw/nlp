% \subsection{Out-of-Distribution (OOD) Generalization} Due to the sensitivity of deep neural networks to distributional shifts, their performance can vary dramatically, making out-of-distribution (OOD) generalization an important research topic~\citep{arjovsky2019invariant,li2022out,krueger2021out}. OOD generalization is more challenging than domain adaptation because it targets open environments, aiming to generalize from training on known distributions to completely unseen distributions~\citep{farahani2021brief,kundu2020towards}. The predominant approach for OOD generalization is invariant learning, which encourages the model to learn representations that remain consistent across environments, thus maintaining predictive power on OOD data~\citep{creager2021environment}. Additionally, methods such as distributionally robust optimization~\citep{dro}, data augmentation~\citep{da}, and test-time adaptation~\citep{tta} have also been proposed to tackle the OOD generalization problem.

\subsection{OOD Generalization and Invariant Learning}Due to the sensitivity of deep neural networks to distributional shifts, their performance can vary dramatically, making out-of-distribution (OOD) generalization an important research topic~\citep{arjovsky2019invariant,li2022out,krueger2021out}. The predominant approach for OOD generalization is invariant learning, which explores stable relationships between features and labels across environments, aiming to learn representations that remain effective in OOD scenarios~\citep{creager2021environment,ahuja2020empirical}. Its interpretability is ensured by a causal data generation process~\citep{sun2020latent}. \citep{ye2021towards} proved the theoretical error lower bound for OOD generalization based on invariant learning. Notably, the definition of environmental information is critical to invariant learning, yet it also restricts its further development, as it often requires the training set to encompass a diverse and comprehensive range of environments~\citep{lin2022zin,rosenfeld2020risks,nagarajan2020understanding}.
% It has been proved from theoretical and experimental perspectives~\citep{rosenfeld2020risks,nagarajan2020understanding}.

\subsection{OOD Generalization on Graphs}
Inspired by the sucess of invariant learning in image data, many OOD generalization methods in the graph domain have been proposed, adopting this core idea, and several representative works have emerged.~\citep{yang2022learning,li2022learning,liu2022graph,jia2024graph,fan2022debiasing,sui2022causal,miao2022interpretable}. 
Their core idea is to design an effective model or learning strategy that can identify meaningful invariant subgraphs from the input while ignoring the influence of environmental noise~\citep{wu2022discovering,chen2022learning}. However, the difficulty of modeling environment information in the graph domain has recently garnered attention, with researchers generally agreeing that directly applying invariant learning to graph OOD generalization presents challenges~\citep{chen2024does,zhuang2023learning}.

\subsection{Hyperspherical Learning} Hyperspherical learning has gained attention due to its advantages over traditional Euclidean methods in high-dimensional~\citep{davidson2018hyperspherical,ke2022hyperspherical}. The core idea lies in using a projector to project representations onto a unit sphere space for prototype-based classification~\citep{mettes2019hyperspherical}. Recently, hyperspherical learning has been extended to applications like contrastive learning and OOD detection,  allowing for better disentanglement of features~\citep{ming2022exploit,bai2024hypo}. Despite these advancements, the effective integration of hyperspherical representations with invariant learning to tackle graph OOD generalization has yet to be explored, with the primary challenge being the difficulty of accurately defining environmental information in these tasks.