\section{Design and implementation}

\subsection{Methodology}

To work on the engine, we deploy it in a development setup of a single Linux server (Intel Core i7-10700 CPU, 16 GB RAM) with a local NVMe device (Samsung PM9A1). All components are built locally and run as native binaries (not in containers). Evaluation of different layers is done in a top-down approach by replacing I/O operations with no-ops at various places:
\begin{itemize}
    \item To measure the performance of the frontend, we replace the controller's backend read and write commands, so instead of routing to replicas, I/Os are immediately completed (\textit{null backend}).
    \item Once the frontend bottleneck is reduced, the impact of controller-replica communication is evaluated similarly, by modifying the replica's dataconn server to reply as soon as requests are received (\textit{null storage}).
\end{itemize}
For measuring IOPS and bandwidth, we use the \textit{fio} utility, configured to perform direct I/Os on the resulting block device. In our---fairly recent---development server, the full system running a single controller and a single replica reading and writing in the filesystem over an NVMe disk (no volume snapshots), cannot achieve a performance greater that 50k/25k read/write IOPS. In comparison, running fio directly on the filesystem yields about 400k read/write IOPS.
The elements of the modified design (Fig. \ref{fig:new-sys}) are described in the following sections.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.77\linewidth]{figures/architecture_post.pdf}
    \caption{Modified architecture of the Longhorn engine. Changes described in the text are shown on the right.}
    \label{fig:new-sys}
\end{figure}

\subsection{Frontend}

Using the null backend, we measured that the stand-alone frontend could not surpass 60k read/write IOPS, which is a number too low compared to modern I/O standards. This lead us to assume that the TGT-based implementation imposes a significant bottleneck. After analyzing the code and exploring the capabilities of this frontend option, we traced the issue largely to the fact that all communication is done synchronously.
% Part of the problem is the integration with Longhorn, implemented to serve I/O with synchronous requests. High performance systems largely rely on asynchronous requests in order to boost performance. 

While a different approach over iSCSI may have provided a viable solution, there are several other technologies available for exposing userspace block devices more efficiently and easier to integrate with the rest of the engine. We also considered ``legacy'' options such as NBD (Network Block Device) \cite{nbd} that minimizes the stack to only the essential components, as the kernel can directly connect to the controller. NBD can be configured to utilize multiple client-server threads, which also helped in raising overall performance. A test deployment with an NBD server in Golang incorporated directly in the controller, allowed us to reach over 100k IOPS at the frontend.

Next we considered NVMe-oF and ublk, both of which had already been discussed by Longhorn’s development team. NVME-oF will be part of Longhorn v2. The ublk path had been suggested as an option for the current version of the engine and a proof-of-concept (PoC) implementation already existed. We used this PoC as a foundation and tailored it to work with the latest version of Longhorn.

In the ublk framework, there are two basic components: ublk\_drv and ublksrv. The ublk\_drv is the Linux kernel module, responsible for IO command communication, copying of pages and various administrative tasks regarding the exposed block device, such as add, delete, and recover. Ublksrv is the userspace application that serves the I/Os via a driver module (similar to TGT). The Longhorn ublk PoC already included a compatible driver. Another powerful ublk feature is multiple frontend queues. This increases the queue-depth of incoming I/Os, providing significant performance gains. Multiple queues were not enabled in the original PoC, however our implementation includes it as a configurable option. Enabling the option allows the frontend to serve just over 500k IOPS in our development setup.

% OLD TEXT

% \section{Frontend}

% \subsection{Overview}
% The first layer of Longhorn’s engine is the frontend, which exposes the persistent volume to the user’s node. The frontend establishes a connection between the exposed block device and the Longhorn engine, redirecting every I/O operation to the engine. By default, Longhorn uses TGT , an iSCSI-based frontend with a classic client-server configuration where the engine node acts as server and application nodes act as clients. To integrate the iSCSI TGT frontend into the Longhorn engine, the development team created a Golang wrapper that executes CLI commands on startup, establishing the connection between the server and client.

% \subsection{Identifying the problem}

% While iSCSI TGT frontend is a solid and highly available solution, it is an outdated technology. Limits the Longhorn performance to its capabilities, thus creating a significant bottleneck. After analyzing the code and exploring the capabilities of this frontend option, two main problems were found contributing to the bottleneck :
% \begin{enumerate*}[label=(\roman*)]
% \item Low performance
% \item Synchronous requests
% \end{enumerate*}.

% Concerning low performance issues we measured that stand-alone iSCSI TGT performance can go as high as 50k IOPS using a null backend, the fastest it can achieve. Those numbers are too low compared to modern I/O standards. Using Longhorn as backend the performance is capped at 35k IOPS.

% Part of the problem is the integration with Longhorn, implemented to serve I/O with synchronous requests. High performance systems often rely on asynchronous requests in order to boost performance. 
    
% Since the maximum performance of iSCSI TGT is 50k IOPS, redesigning its integration with Longhorn engine is not an optimal solution. There are a lot of new technologies developed for these use cases that are more efficient and easier to integrate with the rest of the engine. After exploring options like NBD and NVMe-oF, we determined that ublk is the most promising solution.  Ublk is a cutting-edge framework for exposing block devices to user space, and Longhorn’s development team has already begun to integrate it into the engine.

% \subsection{The Ublk framework}

% The ublk framework is a generic framework for implementing block device logic from userspace. Developed by Ming Lei, it is actively maintained and updated with new features and fixes. It can move virtual block drivers into userspace, such as loop, nbd and any custom driver a user can implement.It’s biggest advantage over other solutions is the use of io\_uring, a high performance Linux kernel system call interface for storage device asynchronous I/O operations There are two basic components : 
% \begin{enumerate*}[label=(\roman*)]
% \item ublk\_drv
% \item ublksrv
% \end{enumerate*}.

% The ublk\_drv is a Linux kernel module that has been merged in linux kernel v6.0.It is responsible for IO command communication, pages copy and various admin tasks regarding the exposed block device like add, delete and recovery.

% The ublksrv is the module’s backend that serves the issued I/O itself of redirect it to whomever is responsible for it. It is flexible and built in a way that anyone can implement a new driver and move the I/O logic over their target. For integration with Longhorn, a compatible driver must be created. 

% \subsection{Integration}

% As part of an ongoing investigation from the developers to enrich Longhorn with the feature to use ublk as frontend, a proof-of-concept (POC) implementation[BIB] has been introduced by a member of the development team. We used this POC as a foundation and enriched it to work with the live version of Longhorn.

% Another powerful feature ublk supports is the multiple frontend queues. This feature can multiply the q-depth of incoming I/O ,boosting  the performance of the framework by a massive amount. Although the POC integration had this feature disabled, after testing, we found that it works as intended after being integrated with the rest of the engine, so we included it in our integration as a feature that can be manipulated on startup along with other configurations. 
