% \section{Design}
\section{Longhorn architecture}

Longhorn components form a small web of microservices that provide distributed block storage for Kubernetes environments (Fig. \ref{fig:components}). At the core is the Longhorn engine, responsible for managing data replication and ensuring high availability across multiple replicas. Each Longhorn engine instance---a controller with associated replicas---implements a single volume. The Longhorn manager acts as the control plane, orchestrating the lifecycle of storage volumes, snapshots, and backups while interfacing with the Kubernetes API via the Longhorn CSI plugin. Additionally, the Longhorn UI provides a user-friendly dashboard for managing and monitoring storage operations. This work focuses on the engine service, which is the only component critical to performance since it implements the actual I/O path.

\begin{figure}[thb]
    \centering
    \includegraphics[width=0.90\linewidth]{figures/components.pdf}
    \caption{Longhorn components.}
    \label{fig:components}
\end{figure}

Longhorn developers refer to the engine as ``world's smallest storage controller''. Indeed its design is simple and lightweight, consisting of three basic components (Fig. \ref{fig:old-sys}):
\begin{enumerate*}[label=(\roman*)]
    \item the \textit{frontend}, which is responsible for interfacing with the OS, so the controller's block API can be accessed seamlessly by applications over a virtual block device,
    \item the \textit{controller}, which routes I/Os to the replicas, functioning as a simple RAID controller (although only supporting mirroring), and
    \item the \textit{replica}, which stores the volumes' blocks in an actual device (the current implementation uses sparse files for storage).
\end{enumerate*}
In the distributed environment of Kubernetes, all these components are deployed in containers and communicate over the network. The frontend and controller are grouped together and run on the same node, while the replicas typically run on different nodes (one replica can be colocated with the frontend/controller).

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.57\linewidth]{figures/architecture_pre.pdf}
    \caption{Original architecture of the Longhorn engine.}
    \label{fig:old-sys}
\end{figure}

The frontend creates a block device using the kernel iSCSI driver, by leveraging TGT, a third-party project that implements iSCSI targets in userspace. TGT has a plugin architecture that allows developers to materialize I/O operations using custom methods.
In order to connect TGT with the engine, the Longhorn TGT plugin uses a Unix socket and forwards each I/O request to the controller (the controller acts as the server, the TGT plugin as the client). Thus, each read and write issued in the exposed virtual iSCSI block device is redirected by the kernel to TGT, which, in turn, uses the Longhorn plugin to forward the request to the controller over a Unix socket.
% using multiple threads to monitor the connection.
TGT deployment is done by the controller. TGT is packaged along the controller, in the same container, and the controller includes a Golang wrapper that executes CLI commands on volume startup, which start TGT after the Unix socket from the controller's side is ready to accept connections.

The controller's basic responsibility is to accept the requests issued from the frontend, forward them to the replicas, and vice versa. The controller also employs a secondary out-of-band communication mechanism to receive management commands from the Longhorn manager. Such commands include volume start and stop, snapshot, and backup operations.

The layer of the controller that communicates with the replicas is internally called the \textit{backend}. Between the frontend and backend, the controller does not process requests (\textit{i.e.}, performs no erasure coding or deduplication). Each write is replicated to all replicas, and each read is served by one replica in round robin fashion. Note that each write creates multiple messages to replicas that all need to be executed before the command completes and the final response is sent back to the frontend.

The replica is the last componenent of the engine, responsible for storing the data in a physical medium. replicas consist of two basic layers: replica-to-controller communication and the storage mechanism. The communication part is implemented similarly to the controller's Unix socket server (albeit over TCP); the replica acts as the server, using multiple threads to serve TCP connections and handle received requests. The protocol used for exchanging commands and results is custom to Longhorn, internally called ``dataconn''.

The replica backing store receives read and write block requests that should be persisted to a physical medium. The default Longhorn implementation uses Linux sparse files for data storage, which effectively delegates space allocation to the filesystem and abstracts the underlying physical device, allowing it to write to diverse backends such as block devices, local storage, or cloud volumes.
Sparse files efficiently manage storage by only allocating space for blocks that have been written to. This reduces storage overhead for volumes with significant unused capacity, which is particularly useful in cloud environments where applications may allocate large volumes but only use a fraction of the space.

% The backing store is simply implemented using Linux sparse files to store the user's data. 
The engine supports snapshots, by creating a new sparse file for each snapshot that only records changes. The latest snapshot and ``version'' of each replica's storage is kept in a separate metadata file, in order to identify that replicas are consistent among each other. In the case of a faulty replica, the controller is responsible for identifying it and rebuilding it using data from the most up-to-date copy.

The engine is fully implemented in Golang. At the implementation level, the controller and replicas make heavy use of Golang channels as queues for assigning requests to connections and routing back responses. However, the code is very flexible and modular, allowing developers to easily change parts or build and integrate new functionality.
% This flexibility has encouraged us to step into it and provide some new approaches to some parts of the machine.

% OLD TEXT

% Longhorn Engine implements a lightweight block device storage controller capable of storing the data in a number of replicas. Its design is simple and divides the engine into three basic components: frontend, controller, and replica. Those three create the data path that each request follows. Those components are using programming techniques that make the code base easy to manipulate and enrich with features. 

% The first layer of the engine is its frontend. Since we want to implement a block device, the only applicable option is the iSCSI, but there are other options too. It consists of a kernel driver and an iSCSI daemon. The kernel driver is responsible for creating the virtual block device and establishing a connection between it and the iSCSI daemon. The daemon is responsible for serving the request using software. In order to connect this daemon with the rest of the engine, a driver was created that uses a Unix socket and forwards each request to the rest of the engine. So each request issued in the exposed virtual device redirects to the daemon using a kernel driver, and the custom driver issues the request to the controller using a Unix socket.

% The next part of the engine is the controller, which runs on the same machine as the frontend. Its basic responsibility is to accept the requests issued from the frontend and forward them to each replica and reply back to the frontend. It also monitors and executes any management command needed for the frontend and the replicas. The two components that forward the requests are the controller frontend and the controller backend. 

% The controller frontend receives requests from the engine's frontend and answers them using a Unix socket, using multiple threads to monitor the connection. Upon receiving a request, the controller's frontend provides it to the controller's backend.

% The next internal layer of the controller is its backend. This is where the controller communicates with each replica and forwards the requests. Each replica has several TCP connections, with two as default, to the controller to maximize the throughput. Every time a new request comes in, it is served with a different connection using the capabilities of the Golang channels. Similarly to frontend controller communication, this layer of communication uses the same principles, with the controller serving as the client. It transforms the message to the desired form, and using the right connection, it communicates with the replica. While writes need to be replicated to each replica, thus creating multiple messages for each request, reads are performed to a single replica selected from a round robin algorithm(?).

% Each replica, running on a different machine, is the last layer of the Longhorn engine. It is responsible for storing data and serving the controller at reads. Replicas consist of two basic components: the replica connection server and the backing-store system (?). The connection server is similar to the controller's frontend server, using multiple threads to monitor each TCP connection and forwarding each received request. The backing store system is a simple implementation using a Linux sparse file to store the user's data. Replicas also keep track of every change done on the sparse file in order to identify that they are consistent with the others. In the case of a faulty replica, the controller is responsible for identifying it and rebuilding it using data from the most up-to-date replica.

% The design of this engine is very flexible, allowing developers to build new components and integrate them into the engine. This flexibility encourages us to step into it and provide some new approaches to some parts of the machine.
