\subsection{Block storage}

% Each replica receives read and write block requests that should be persisted to a physical medium. The default Longhorn implementation uses Linux sparse files for data storage, which effectively delegates space allocation to the filesystem and abstracts the underlying physical device, allowing it to write to diverse backends such as block devices, local storage, or cloud volumes.
% Sparse files efficiently manage storage by only allocating space for blocks that have been written to. This reduces storage overhead for volumes with significant unused capacity, which is particularly useful in cloud environments where applications may allocate large volumes but only use a fraction of the space.

Enabling the full path of I/Os to the device now reveals that another bottleneck is present at the replica's backing store, which is capped at 128k/38k read/write IOPS with all the refactoring done in the controller.
This is expected, as the default storage scheme has several shortcomings: Sparse files require a filesystem optimized for sparse file usage, as the latter has to maintain block allocation metadata and handle underlying fragmentation. Furthermore, each replica maintains a separate metadata file per volume with overall volume information, including the name and ``version'' of the latest data file. Management of such metadata in external files introduces overheads; we have verified that disabling write versioning raises the write IOPS significantly, almost to the level of reads. In addition, the performance degrades severely as the number of snapshots grows, as each snapshot is based on a new sparse file that only records new blocks in respect to the previous. Reads in volumes with many snapshots may have to go through the whole chain of sparse files in order to find the actual data block.
% (a known issue in Longhorn \hl{[REF]}).

\begin{figure}[thb]
    \centering
    \includegraphics[width=0.64\linewidth]{figures/dbs.pdf}
    \caption{Internal structure of storage space managed by DBS.}
    \label{fig:dbs}
\end{figure}

To minimize the software layers involved in actual block storage and address the performance problem with multiple snapshots, we chose to redesign the block storage functionality, by introducing a custom, light-weight, direct-to-disk block storage implementation, offering a comprehensive framework for managing virtual volumes and snapshots on top of a physical block device or file. Each storage medium is managed by a low-level device layer and can contain multiple volumes, each having multiple snapshots. For interfacing, Direct Block Store (DBS) exposes an extensive API that provides high-level functionality for querying, managing, and manipulating volumes in a storage medium, as well as block-level I/O (read, write, unmap) for embedding into other applications. Additionally, DBS comes with a user-friendly command-line utility for device initialization, volume and snapshot management, and metadata queries. This allows performing operations such as adding, deleting, or renaming volumes, creating and cloning snapshots, and retrieving detailed information about the system's state outside Longhorn's context.

Logically, DBS divides the storage medium in fixed-size extents which correspond to the unit of allocation and management for each snapshot. Each 1 MB extent holds 32 4 KB blocks (which may or may not be all allocated) and belongs to a specific snapshot. Every volume is associated with a single snapshot; the latest in a volume-specific series of snapshots. A new volume always starts with a new snapshot; either empty or a clone of an existing one of any other volume (useful to restore data from a snapshot). Any volume can be deleted, which results in deleting the respective snapshot chain and deallocating all relevant extents. Any non top-level snapshot can be deleted; unique extents in that snapshot are merged with the next snapshot in the chain to maintain data integrity.

Internally, the storage medium is split into four regions (Fig. \ref{fig:dbs}): a superblock, a fixed-size region for volume and snapshot metadata, a variable-sized region for tracking the status of extents, and the remainder of the device, which stores actual user data. The snapshot extent maps (list of extents in order of position in snapshot) are not stored on the device, but are rather reconstructed at startup and kept in memory for maximum efficiency. Volume operations that only use in-memory extent metadata can proceed independently and are issued to the device in parallel to boost performance. Only writes to unallocated space require serialization, as they also update the superblock with the latest allocation mark. This also includes writes on previous snapshots extents, that are copied-on-write to new ones. DBS extensively utilizes bitmaps to quickly identify allocated regions for performance and---as in Longhorn's default storage scheme---uses direct I/O to bypass the OS cache when writing.

Overall, DBS is designed to deliver a stand-alone, high-performance block-level storage solution. It is written in Golang (using about 1000 lines of code) to be easily integrated into Longhorn, but may also prove suitable for other applications requiring direct access to raw storage with features that include volumes, as independent data domains, supporting point-in-time snapshots. In our development setup, DBS (deployed over the same filesystem) allows us to achieve end-to-end Longhorn performance of approximately 150k read/write IOPS, which corresponds to a 3x/6x improvement compared to the unmodified software.
