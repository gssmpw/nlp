\section{Introduction}

In cloud-native software architectures, the storage setup plays a significant role in determining the scalability and reliability properties of the whole system. Some core backend microservices, such as database engines, may handle this independently, by offering deployment options that distribute I/O operations across multiple instances, while automatically reacting to failures by exploiting internal data redundancies. However, all other microservices that require storage expect respective facilities to be provided by third-party software. For this reason, the cloud-native ecosystem is abundant with volume management solutions that either interface Linux-native or custom-built software defined storage (SDS) platforms to the APIs and deployment strategies of container orchestration environments. Implementing a cloud-native SDS has several benefits over choosing a similar product already available from a cloud provider, as it provides vendor independence, usually greater flexibility and cost efficiency, as well as advanced features that may not be part of standard offerings.

Longhorn \cite{longhorn} is a popular open-source, cloud-native volume manager, which implements its own distributed block storage system. It is a complete and independent SDS, handling internally all aspects related to capacity management, performance, fault tolerance, as well as interfacing with both Kubernetes and the end user. Longhorn is an actively developed and mature software, part of the CNCF software catalogue \cite{cncf}, however our installations have revealed that it currently lacks the ability to take advantage of the hardware performance available in servers that feature high-speed solid-state disks and high-bandwidth network connectivity.
% While this may not pose a momentous issue in current cloud deployments, where the environment imposes stringent limits on performance (unless much higher specifications are leased, which usually results in significantly increased costs), it is greatly restraining when the hardware is on-premise or part of a private system colocated in a data center.
This limits the applicability of Longhorn on setups with high I/O capabilities, as is often the case with the on-premise clouds or private systems colocated in a data center.

In this paper, we investigate a series of performance optimizations that collectively allow the system to achieve an order of magnitude better IOPS and bandwidth. We implement these optimizations in Longhorn's core, called the Longhorn engine \cite{longhorn-engine} (hereafter referenced simply as \textit{engine}), which is separately deployed in full for each managed volume. Each engine setup, consists of a \textit{controller} (data aggregator) and several \textit{replicas} (data storage endpoints). We remodel three points of the engine architecture, which we find to be the most critical to resulting performance:
\begin{enumerate*}[label=(\roman*)]
    \item the connectivity between the controller and the host operating system, where we use the \textit{ublk} framework \cite{ublk} instead of the current solution based on \textit{iSCSI/tgt} \cite{tgt},
    \item the communication between the controller and the replicas, where we employ a strategy that minimizes locks among involved threads, and
    \item the data storage scheme used by the replicas, which utilizes our custom direct-to-disk block management layer, named \textit{Direct Block Store} (DBS), instead of the default file-based implementation.
\end{enumerate*}

% Most importantly, as we have practically affected all parts of the engine excluding the simplistic replication and data routing mechanisms, we believe that this work can serve as a reference to other distributed SDS implementations. Therefore, we identify the contributions of this paper to the following:
Our contributions in this work are:
\begin{enumerate*}[label=(\roman*)]
    \item we present how new operating system technologies, such as ublk, can be integrated into SDS designs,
    \item our implementation analysis offers a comprehensive methodology to diagnose performance bottlenecks and assess their impact in SDS stacks, and
    \item we introduce DBS, which may not use novel design concepts, but as an open source solution can serve as a useful utility to solve similar problems in related projects.
\end{enumerate*}
As we have practically affected all parts of the engine excluding the simplistic replication and data routing mechanisms, our modified system can facilitate future work by allowing for further performance analysis and optimizations.

% In the following sections we briefly describe the engine's design, followed by a detailed analysis of our changes. A detailed evaluation of each change helps in understanding the accumulative effect the optimizations have in overall performance.



