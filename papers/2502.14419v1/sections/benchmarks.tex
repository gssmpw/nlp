\input{tables/iops}
\input{tables/bandwidth}

\section{Evaluation}

\subsection{Setup}

To evaluate the system we deploy the controller and replica on separate nodes. In contrast to our development setup, this allows us to account for any effects related to the physical network. We use two identical nodes from our local cluster equipped with dual Intel Xeon E5-2620v2 CPUs, 128 GB RAM, connected via 10 Gbps Ethernet. Data is stored in a Samsung PM1733 NVMe drive. These machines have limited CPU core counts compared to state-of-the-art technology, however their specifications and performance is better aligned with typical VM offerings currently available in the cloud.

Actually, we initially evaluated our system in AWS, using two c5d.2xlarge EC2 instances, a cost-efficient option also used by Longhorn developers for publishing their benchmarks \cite{longhorn_report}.
However, EC2 instances have limited maximum provisioned IOPS, regardless of the hard drive used. As expected, the software performed to the machine's limit, reaching AWS's 40k IOPS cap. Overcoming these limitations in AWS requires using significantly more expensive EC2 instances. To harness the performance of the features described in this work, deployments must use instances and volumes at least 5 times more expensive than c5d.2xlarge. In any case, the behavior of the system on higher-performance cloud nodes is similar to the one we observe in our local setup.

\subsection{Results}

The results for IOPS and bandwidth are presented in Tables \ref{tab:iops} and \ref{tab:bandwidth} respectively. As in the previous section, in each experiment we do multiple runs to measure (shown as table rows): 
\begin{enumerate*}[label=(\roman*)]
    \item the \textit{full engine} performance, end-to-end, which includes writing blocks to the disk,
    \item the performance up to the replica \textit{without storage} using a null storage drive, where I/Os are immediately completed at the replica, and
    \item the \textit{frontend only} using a null backend, where I/Os are immediately completed at the controller.
\end{enumerate*}
We follow the same top-down approach, starting from upstream Longhorn and integrating each new feature in the same progression (shown as table columns). This indicates where the bottleneck is in each step and highlights how each solution (ublk frontend, controller-replica communication, DBS) contributes to the performance of the whole system.
For our experiments we use fio to measure IOPS (4k, random) and bandwidth (1 MB, sequential). All I/Os are direct to the virtual block device, bypassing kernel caches.

Starting with 17k/13k read/write IOPS when running the full stack, we isolate the first bottleneck at the frontend and measure it cannot achieve more than 20k IOPS using the upstream TGT-based solution. Integrating the ublk frontend yields a 28x boost, that allows the next layers of the engine to perform better, except for write IOPS, where the storage scheme of Longhorn fails to take advantage of the faster frontend. The bandwidth is also largely affected by the new frontend (almost 2.5x/4.8x for reads/writes compared to upstream), which enables the system to almost saturate the 10 Gbps links.
In general, these numbers support the general consensus among the community that ublk is an ideal framework to export SDS stacks to applications.

The next step is to evaluate the improved controller-replica communication implementation. Even with the ublk frontend, upstream Longhorn cannot achieve over 100k IOPS going up to the replica (null storage). Our modified communication scheme boosts performance by 29\%/15\% for reads/writes. There is still room for improvement, as we would ideally like to match the frontend performance, however it is enough for the engine to reach the full 10 Gbps bandwidth even with the default storage backend. The updated controller-replica communication also boosts random read IOPS to storage by 17\%. Write IOPS are not affected, which indicates that the bottleneck is in the storage backend itself (other runs have proved that this limitation is caused by write versioning). Lastly, integration of the DBS backend raises write IOPS to the level of reads; the whole modified system now performs an order of magnitude better than the default. Note that DBS is designed to keep the same performance level regardless of the number of volume snapshots.

% OLD TEXT

% Following the implementation of each feature, we measured its performance individually and observed a minimum performance boost resulting from improved controller-replica communication and DBS integration. Our findings indicate that each layer's bottleneck and solution proposed are crucial, so the next layer provides a boost in performance. So in our benchmarks, we used a top-down approach, integrating each feature step by step to highlight its importance. We also split our benchmarks into three parts, disabling the functionality of the engine in certain parts in order to benchmark each feature separately.
 
% Our setup included one controller and one replica, without the use of snapshots. We tested these changes in our local cluster using two identical systems equipped with Intel(R) Xeon(R) CPU E5-2620 v2 @ 2.10GHz, 125GB memory, and â€¦ hard drives. The machines were connected via a 10 Gbps connection, which is enough for some experiments but limits the performance during some bandwidth benchmarks.

% \subsection{Frontend}

% During our frontend evaluation experiments, we used our integration and compared it with the live version of the Longhorn engine. The findings can be seen in Table 1. 
% We can see there is a big boost in the frontend's performance using the ublk framework. The first layer of the engine saw a 28x boost in performance, both on reads and writes IOPS. Regarding the next two layers of the engine, we saw a big boost even using the full engine, except for write IOPS, where the Longhorn's IO methods fail to take advantage of the faster frontend. The bandwidth seems to get a boost too, especially the frontend part. 

% Overall these numbers support our belief that ublk is a modern and more performant framework to implement software-defined storage frontends. 

% \input{tables/table1}
% \input{tables/table3}

% \subsection{Controller-Replica communication}

% After integrating the ublk frontend, we also included the improved controller-replica communication in our experiments. We use the same three breakpoints to evaluate the solution. 

% This improvement was focused on improving the server-client architecture, and our experiments indicate that we manage this. Compared to the longhorn live version, this approach serves requests with 10x IOPS. This approach does not match the performance gain on the front-end but is still a solid solution. We also measured a big boost in the bandwidth performance to numbers that are the limit of the connection used between the machines. Using a better connection, this number may see a small improvement based on some experiments performed in an environment where the replica and the controller were on the same machine, removing the network limitations.

% Using this solution, the Longhorn's backend managed to achieve a boost in performance. Random reads have seen a boost similar to the previous layer when writes double their performance compared with the previous solution. These findings seem to solidify our theory that the Longhorn backend needs a new approach.

% \subsection{Block storage}
% Our last integration was the DBS backend for the replicas IO. This method, despite being in early development, seems to perform better regarding IOPS. This modern solution managed to match the rest of the system's performance, achieving 10x IOPS compared to Longhorn's backend option on writes where Longhorn's implementation failed to keep up with the rest of the system.

% \textbf{DBS performance on vaccum}
% \input{tables/table2}
% \input{tables/table4}

% \subsection{AWS}

% Since most users run Longhorn combined with public cloud providers, we also benchmarked our features in the AWS environment. For this purpose, we used two c5d.xlarge EC2 instances, a cost-efficient option commonly used by Longhorn developers for their benchmarks.
    
% EC2 instances have a built-in maximum provisioned IOPS, regardless of the hard drive used. As expected, the features performed to the machine's limit, reaching AWS's 40k IOPS cap. Overcoming these limitations requires using higher-performance and more expensive EC2 instances. After some research, we found that to harness the performance of our features added to Longhorn, users must use instances and volumes at least 5 times more expensive than c5d.xlarge using its installed volume.
