\section{Related Work}
\subsection{Model Editing}
Model editing enables targeted behavioral modifications within specific domains and can be categorized as indirect editing and direct editing. Indirect model editing involves fine-tuning the model to update knowledge with specifically-designed objective \cite{zhu2020modifying, lee2022plug} or use meta-learning with hypernetworks to learn optimal parameter updates \cite{de2021editing,mitchell2021fast}. However, both approaches require extensive model updates, which risks catastrophic forgetting on non-target tasks.

%Meta-Learning, exemplified by MEND \cite{mitchell2021fast} and KE \cite{de2021editing}, employs hypernetworks to learn optimal parameter updates for knowledge modification while preserving model stability.
%Both fine-tuning and meta-learning require extensive model updates, risking catastrophic forgetting on non-target tasks.

Direct editing refers to directly locating and editing the knowledge-related parameters. Research indicate that factual knowledge is primarily stored in the MLP modules of transformer-based architectures \cite{geva2020transformer, geva2022transformer}.
Leveraging these insights, model-editing methods like ROME \cite{meng2022locating} employ causal tracing to identify and edit the parameters encoding the particular knowledge. However, ROME is limited to single-instance knowledge editing, restricting its applicability in scenarios requiring large-scale updates. MEMIT extends the approach to support batch knowledge editing, providing a scalable solution for efficient and precise modifications \cite{meng2022mass}. 
%By targeting a minimal set of relevant parameters, direct-edit achieves higher precision and reduce interference with the model’s overall performance compared to indirect model editing.

\subsection{Existing Defense to Jailbreak Attacks}
Recent studies reveal that jailbreak attacks \cite{zou2023universal, liu2023autodan, zhou2024don, chao2023jailbreaking} can bypass security alignment leading LLMs to generate harmful or unethical outputs.
As countermeasures, various defense methods are developed against such threats.
Existing defenses can be categorized into active defenses and passive defenses. Active defense enhances LLMs robustness against adversarial prompting by dynamically altering model parameters \cite{wang2022self, ganguli2022red, xu2024safedecoding, wang2024detoxifying, zhao2024defending}. A common approach to safety training involves constructing safety-relevant datasets and fine-tuning the model \cite{mazeika2024harmbench}.
Instead, passive defense aims to build auxiliary modules or use external safety methods including input and output filtering \cite{alon2023detecting}, input smoothing, sanitation and modification \cite{cao2023defending, jain2023baseline, zhou2024robust}.


%1) System-level defenses do not alter the LLM itself, but rather add external safety measures on top of the LLM.
%These include input and output filtering via perplexity filter \cite{alon2023detecting}, input smoothing \cite{cao2023defending}, input sanitization \cite{jain2023baseline} and modification \cite{zhou2024robust};.
%2)Model-level defenses alter the LLM parameters to reduce the risk of malicious use and improve robustness to adversarial prompting. Safety training is commonly approached via fine-tuning methods such as RLHF \cite{ouyang2022training} and DPO \cite{rafailov2024direct}.

\subsection{Model Editing as a Jailbreak Defense}
Several studies have explored LLMs model editing as a defense mechanism to precisely modify toxic regions \cite{wang2024detoxifying, zhao2024defending}.
\textit{DINM} \cite{wang2024detoxifying} and \textit{LED} \cite{zhao2024defending} are motivated by indirect model editing method that fine-tuning the toxic layer using specific objectives. The difference between these two methods is the way of locating the toxic region. 
%DINM \cite{wang2024detoxifying} treat the layer that most effectively separates the distributions of safe and unsafe responses  as the toxic layer, while LED
%These approaches emphasize the crucial role of identifying toxic region locations. DINM \cite{wang2024detoxifying}, given safe sequence $Y_{\text{safe}}$ and unsafe sequence $Y_{\text{unsafe}}$ as input consider the toxic layer to be the transformer layer that most effectively separates the distributions of them.
%LED \cite{zhao2024defending} identifies safety layers by selecting the top-k layers whose removal most frequently transforms safe outputs to unsafe ones during iterative pruning.
The layer-level localization and fine-tuning approaches lack precision in identifying harmful words while potentially compromising the model's general performance.
In contrast, we propose to adapt direct-edit as a jailbreak defense in LLMs. 
%can be applied to only those parameters tied to harmful tokens. This targeted approach aims to suppress problematic outputs without broadly disrupting the model’s behavior, thereby preserving performance on other tasks.
%To the best of our knowledge, this is the first work to employ a direct-editing approach for defense.