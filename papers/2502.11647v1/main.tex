% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}
\usepackage{multirow}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{dblfloatfix}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{tcolorbox}
\tcbuselibrary{breakable,skins,listings}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage{amssymb}
% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\captionsetup{font=small}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
\newcommand{\partitle}[1]{\smallskip\noindent \textbf{#1.}}

\title{DELMAN: Dynamic Defense Against Large Language Model Jailbreaking with Model Editing}

\author{Yi Wang\textsuperscript{1}, \;\;Fenghua Weng\textsuperscript{1}, \;\;Sibei Yang\textsuperscript{1},\\
\textbf{Zhan Qin}\textsuperscript{2}\textbf{,} \;\;\textbf{Minlie Huang}\textsuperscript{3}\textbf{,} \;\;\textbf{Wenjie Wang}\textsuperscript{1} \Thanks{W.Wang is the corresponding author.} \\
\textsuperscript{1}ShanghaiTech University, \textsuperscript{2}Zhejiang University, \textsuperscript{3}Tsinghua University\\
\texttt{\{wangyi2024,wengfh2023,yangsb,wangwj1\}@shanghaitech.edu.cn,} \\
\texttt{qinzhan@zju.edu.cn, aihuang@tsinghua.edu.cn}
}

\begin{document}

\maketitle
\begin{figure*}
    \centering
    \includegraphics[width=0.8\textwidth]{fig/main.png}
    \setlength{\abovecaptionskip}{0.2cm}
    \caption{Upper: The three phases of safety alignment during LLMs production. Lower: LLMs editing as a dynamic defense mechanism during the deployment stage.} 
    \label{fig:main}
    \vspace{-1em}
\end{figure*}
\begin{abstract}
Large Language Models (LLMs) are widely applied in decision making, but their deployment is threatened by jailbreak attacks, where adversarial users manipulate model behavior to bypass safety measures. Existing defense mechanisms, such as safety fine-tuning and model editing, either require extensive parameter modifications or lack precision, leading to performance degradation on general tasks, which is unsuitable to post-deployment safety alignment. To address these challenges, we propose \textit{DELMAN} (\textbf{D}ynamic \textbf{E}diting for \textbf{L}L\textbf{M}s J\textbf{A}ilbreak Defe\textbf{N}se), a novel approach leveraging direct model editing for precise, dynamic protection against jailbreak attacks.  \textit{DELMAN} directly updates a minimal set of relevant parameters to neutralize harmful behaviors while preserving the model's utility. To avoid triggering a safe response in benign context, we incorporate KL-divergence regularization to ensure updated model remains consistent with original model when processing benign queries. Experimental results demonstrate that \textit{DELMAN} outperforms baseline methods in mitigating jailbreak attacks while preserving the model’s utility, and adapts seamlessly to new attack instances, providing a practical and efficient solution for post-deployment model protection.
\end{abstract}


\section{Introduction}

%Large Language Models (LLMs) play a significant role in decision-making,  underscoring the importance of aligning LLMs with safety standards and human values. To ensure that generated content aligns with human values and avoids harmful information, various safety alignment methods are employed throughout the model production. As illustrated in the upper part of Figure \ref{fig:main}, the safety alignment process involves three main phases.

%First, original model providers, such as OpenAI and Meta, employ alignment techniques like Direct Preference Optimization (DPO) \cite{rafailov2024direct} or Reinforcement Learning with Human Feedback (RLHF) \cite{ouyang2022training} to ensure their models align with human preferences. These general foundation models are subsequently made available on platforms such as HuggingFace for further development. In the second phase, secondary developers adapt these foundation models to downstream applications \cite{wu2024pmc,cui2023chatlaw,yang2023fingpt}. As part of safety measures, safety-relevant dataset are conostructed for supervised fine-tuning. Finally, developers deploy the models into applications or websites for end-user interaction.

Large Language Models (LLMs) play a significant role in decision-making, underscoring the importance of aligning LLMs with safety standards and human values. To ensure that generated content aligns with human values and avoids harmful information, various safety alignment methods are employed throughout the model production pipeline, including pre-training by model providers, task-specific adaptations by secondary developers, and deployment for user interactions (illustrated in the upper part of Figure \ref{fig:main}). Among these three phases, the deployment stage poses the greatest safety risk, as adversarial users can launch ``jailbreak attacks'' by crafting prompts or optimized suffixes to bypass safety measures \cite{zou2023universal, liu2023autodan, zhou2024don, chao2023jailbreaking}. 

Considering that large-scale modifications to a model's architecture or parameters become impractical once deployed, and adversarial users represent only a minority, which making it infeasible to construct sufficient labeled datasets for fine-tuning, safety alignment in the deployment phase must meet three essential requirements: (1) \textbf{Minimal model modifications} to ensure efficiency; (2) \textbf{Targeted defenses} that address adversarial queries without compromising regular user interactions; (3) \textbf{Dynamic adaptability} to continuously counter emerging jailbreak examples without requiring extensive retraining. 
Existing defense mechanisms such as safety fine-tuning \cite{wang2022self, ganguli2022red, xu2024safedecoding} and model decoder modification \cite{wang2024detoxifying, zhao2024defending} are unsuitable due to their extensive changes to model architecture or parameters. 
Model editing, originally designed for knowledge correction \cite{zhu2020modifying, lee2022plug, de2021editing, mitchell2021fast, meng2022locating, meng2022mass}, has also been explored as a defense against jailbreak attacks. Approaches like \textit{DINM} and \textit{LED} \cite{wang2024detoxifying, zhao2024defending} rely on indirect model editing that fine-tunes specific layers, but they often lack precision in targeting harmful regions and risk degrading overall \mbox{model performance}.


%However, users may include adversaries who exploit these applications for harmful purposes. These adversaries might not only query harmful content but also employ "jailbreak attacks" to bypass the safety alignment established in earlier phases \cite{zou2023universal, liu2023autodan, zhou2024don, chao2023jailbreaking}. %cite jailbreaks attacks here.
%Such attacks include human-designed jailbreak templates or optimized suffixes that manipulate model behavior. Existing defense mechanisms such as safety fine-tuning \cite{wang2022self, ganguli2022red} or model decoder modification \cite{wang2024detoxifying, zhao2024defending} are not suitable for the user-end interaction phase. 

%Model editing , initially designed to address knowledge-related issues \cite{wang2024detoxifying, zhu2020modifying, lee2022plug, de2021editing,mitchell2021fast, meng2022locating, meng2022mass}, serves as a proper solution to defend against jailbreak attacks in the user-end interactionl. This is because, it can directly identify and neutralize the regions associated with these harmful knowledge, allowing fast, precise and  minimal changes to the deployed model. 
%The success of jailbreak attacks stems from attackers exploiting specific knowledge embedded in LLMs \cite{zou2023universal, chao2023jailbreaking}. From a defensive standpoint, the goal is to directly identify and neutralize the regions associated with these harmful knowledge. This objective of defending against jailbreak attacks aligns closely with the objective of model editing \cite{wang2024detoxifying}. Model editing was initially designed to address knowledge-related issues, such as correcting counterfactual information in language models, which can be categorized as indirect editing and direct editing. Indirect editing relies on fine-tuning the model with extensive parameter updates \cite{zhu2020modifying, lee2022plug, de2021editing,mitchell2021fast}, while direct editing focuses on identifying and modifying knowledge-relevant parameters \cite{meng2022locating, meng2022mass}, typically within the Multi Layer Perceptron (MLP) modules of transformers \cite{geva2020transformer}. 
%Motivated by the targeted behavioral modification capabilities of model editing, several studies have proposed using model editing as a defense against jailbreak attacks. For instance, DINM and LED \cite{wang2024detoxifying, zhao2024defending} leverage indirect model editing techniques, fine-tuning specific layers with carefully designed objectives. However, these layer-level localization and fine-tuning approaches lack precision in identifying harmful regions and risk compromising the model's general performance.

%By enabling targeted behavioral modifications within specific domains while preserving overall model performance, model editing provides a precise and effective approach to defending against jailbreak attacks, minimizing unintended consequences on the model's general capabilities \cite{wang2024detoxifying, zhao2024defending}.
%Unlike indirect editing, which risks catastrophic forgetting on non-target tasks, direct editing focuses on minimal parameter updates, minimizing interference with the model's overall performance.

A dynamic jailbreak defense mechanism is essential, one that is timely, precise, and minimal in required modifications to the deployed model while effectively countering adversarial attacks. To achieve this, our key motivation is to utilize direct editing that focuses on minimal parameter updates, minimizing interference with the model's overall performance. 
Specifically, in this work, we introduce \textit{DELMAN} (\textbf{D}ynamic \textbf{E}diting for \textbf{L}L\textbf{M}s J\textbf{A}ilbreak Defe\textbf{N}se), a novel approach that dynamically protects against jailbreak attacks by directly adjusting the weights of specific layers. 
As illustrated in Figure \ref{fig:method}, \textit{DELMAN} establishes a connection between harmful tokens and safe responses by computing an input vector $k^*$ from harmful tokens and optimizing a target output vector $v^*$ representing a safe response. The model's weights are then updated with a closed-form solution so that when the input vector is fed into the model, the output of the targeted layer aligns with the desired safe response, effectively minimizing the likelihood of generating harmful content. To avoid unintended trigger of safe responses in benign contexts (e.g. the word ``bomb'' in ``what is a bomb''), we incorporate neutral prompts containing harmful tokens in non-harmful contexts during optimization of the target output vector. KL-divergence \cite{kullback1951information} is applied to ensure that the updated model remains consistent with its original output distribution when processing these benign queries. This ensures that the model distinguishes between harmful and harmless uses of the same tokens, avoiding over-correction while maintaining its utility for normal tasks.

%Unlike existing model editing defenses that rely oncomputationally intensive fine-tuning of entire layers, which often compromising the model's utility, \textit{DELMAN} can operate efficiently with a small set of harmful queries or even a single harmful query. 
%By focusing on a minimal set of relevant parameters, \textit{DELMAN} achieves higher precision in mitigating harmful behavior while minimizing interference with the model's performance on general tasks. Additionally, \textit{DELMAN} supports continuous updates to counter new jailbreak instances without undermining previous edits, making it a robust and adaptable defense mechanism. 
Our contributions can be summarized as follows:
\begin{itemize}[leftmargin=10pt, itemsep=2pt, parsep=0pt, partopsep=0pt, topsep=0pt] 
    \item  We propose \textit{DELMAN}, a dynamic post-deployment defense that directly edits model parameters to neutralize harmful behaviors while preserving overall performance.
    \item \textit{DELMAN} focuses on minimal parameter editing utilizing only a small set of harmful queries, enabling rapid, precise, and adaptive defense against unseen jailbreak attempts.
    \item \textit{DELMAN} includes a KL-divergence regularization term to avoid triggering safe responses in benign contexts thus preserving normal utilities.
    \item Extensive experiments demonstrate \textit{DELMAN} outperforms baseline methods in mitigating jailbreak attacks while preserving the model's utility on normal tasks, as well as its transferability and generalization ability to unseen jailbreak attacks and harmful queries. A case study is also included to demonstrate that \textit{DELMAN} can support continuous updates to counter new jailbreak instances without undermining previous edits.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{fig/method.png}
    \vspace{-1.5em}
    \caption{\textit{DELMAN} consists of five steps: 1. Extract harmful tokens from the query; 2. Random context sequence generation; ‌3. Calculate $k^*$ of harmful tokens; 4. Estimate $v^*$ of safe response $Y_{target}$; 5. Update $W^{l^*}_{down}$ with $k^*$, $v^*$.}
    \label{fig:method}
    \vspace{-1em}
\end{figure}

\section{Related Work}
\subsection{Model Editing}
Model editing enables targeted behavioral modifications within specific domains and can be categorized as indirect editing and direct editing. Indirect model editing involves fine-tuning the model to update knowledge with specifically-designed objective \cite{zhu2020modifying, lee2022plug} or use meta-learning with hypernetworks to learn optimal parameter updates \cite{de2021editing,mitchell2021fast}. However, both approaches require extensive model updates, which risks catastrophic forgetting on non-target tasks.

%Meta-Learning, exemplified by MEND \cite{mitchell2021fast} and KE \cite{de2021editing}, employs hypernetworks to learn optimal parameter updates for knowledge modification while preserving model stability.
%Both fine-tuning and meta-learning require extensive model updates, risking catastrophic forgetting on non-target tasks.

Direct editing refers to directly locating and editing the knowledge-related parameters. Research indicate that factual knowledge is primarily stored in the MLP modules of transformer-based architectures \cite{geva2020transformer, geva2022transformer}.
Leveraging these insights, model-editing methods like ROME \cite{meng2022locating} employ causal tracing to identify and edit the parameters encoding the particular knowledge. However, ROME is limited to single-instance knowledge editing, restricting its applicability in scenarios requiring large-scale updates. MEMIT extends the approach to support batch knowledge editing, providing a scalable solution for efficient and precise modifications \cite{meng2022mass}. 
%By targeting a minimal set of relevant parameters, direct-edit achieves higher precision and reduce interference with the model’s overall performance compared to indirect model editing.

\subsection{Existing Defense to Jailbreak Attacks}
Recent studies reveal that jailbreak attacks \cite{zou2023universal, liu2023autodan, zhou2024don, chao2023jailbreaking} can bypass security alignment leading LLMs to generate harmful or unethical outputs.
As countermeasures, various defense methods are developed against such threats.
Existing defenses can be categorized into active defenses and passive defenses. Active defense enhances LLMs robustness against adversarial prompting by dynamically altering model parameters \cite{wang2022self, ganguli2022red, xu2024safedecoding, wang2024detoxifying, zhao2024defending}. A common approach to safety training involves constructing safety-relevant datasets and fine-tuning the model \cite{mazeika2024harmbench}.
Instead, passive defense aims to build auxiliary modules or use external safety methods including input and output filtering \cite{alon2023detecting}, input smoothing, sanitation and modification \cite{cao2023defending, jain2023baseline, zhou2024robust}.


%1) System-level defenses do not alter the LLM itself, but rather add external safety measures on top of the LLM.
%These include input and output filtering via perplexity filter \cite{alon2023detecting}, input smoothing \cite{cao2023defending}, input sanitization \cite{jain2023baseline} and modification \cite{zhou2024robust};.
%2)Model-level defenses alter the LLM parameters to reduce the risk of malicious use and improve robustness to adversarial prompting. Safety training is commonly approached via fine-tuning methods such as RLHF \cite{ouyang2022training} and DPO \cite{rafailov2024direct}.

\subsection{Model Editing as a Jailbreak Defense}
Several studies have explored LLMs model editing as a defense mechanism to precisely modify toxic regions \cite{wang2024detoxifying, zhao2024defending}.
\textit{DINM} \cite{wang2024detoxifying} and \textit{LED} \cite{zhao2024defending} are motivated by indirect model editing method that fine-tuning the toxic layer using specific objectives. The difference between these two methods is the way of locating the toxic region. 
%DINM \cite{wang2024detoxifying} treat the layer that most effectively separates the distributions of safe and unsafe responses  as the toxic layer, while LED
%These approaches emphasize the crucial role of identifying toxic region locations. DINM \cite{wang2024detoxifying}, given safe sequence $Y_{\text{safe}}$ and unsafe sequence $Y_{\text{unsafe}}$ as input consider the toxic layer to be the transformer layer that most effectively separates the distributions of them.
%LED \cite{zhao2024defending} identifies safety layers by selecting the top-k layers whose removal most frequently transforms safe outputs to unsafe ones during iterative pruning.
The layer-level localization and fine-tuning approaches lack precision in identifying harmful words while potentially compromising the model's general performance.
In contrast, we propose to adapt direct-edit as a jailbreak defense in LLMs. 
%can be applied to only those parameters tied to harmful tokens. This targeted approach aims to suppress problematic outputs without broadly disrupting the model’s behavior, thereby preserving performance on other tasks.
%To the best of our knowledge, this is the first work to employ a direct-editing approach for defense.

\section{Methods}

The idea behind \textit{DELMAN} is to mitigate a model's harmful behavior by directly modifying the weights of specific layers, establishing a direct association between harmful tokens and safe responses. Factual knowledge is stored in the MLP of specific layer $l$ \cite{meng2022locating}. The MLP acts as two-layer key–value memories where the neurons of the first layer $W^{l}_{gate}$ generate a key $k$, with which the $W^{l}_{down}$ retrieves an associated value $v$. The MLP layer can be expressed as: 
\begin{equation}
    k = \sigma(W^{l}_{gate}\;\gamma(a^{l}+h^{l-1})), v = W^{l}_{down}k,
\end{equation}
where $a^{l}$ is the attention output at layer $l$, $h^{l-1}$ is the hidden state of previous layer $l-1$, $\sigma$ is the activation function and $\gamma$ is the layernorm. \textit{DELMAN} aims to edit $W^{l}_{down}$ to rebuild the connection between harmful-token-related key representation $k^*$ and safe-response-related representation $v^*$. 
%Motivated by the finding that limiting the magnitude of parameter changes results in better robustness \cite{zhu2020modifying}, we spread updates evenly over the range of crucial layers $l \in \mathcal{R}$. We define the target layer $L = \operatorname{max}(\mathcal{R})$ at the end of the crucial layers, at which the new memories should be fully represented. We adopt the hyperparameters from MEMIT, where for Llama2, $\mathcal{R} = [7,8]$, and for Vicuna, $\mathcal{R} = [7,8]$ \cite{meng2022mass}. \textit{DELMAN} does not require further on-the-fly reasoning or searching to determine $l^*$, making procedure more streamlined. 
As illustrated in Figure \ref{fig:method}, \textit{DELMAN} achieves this through five key steps. In the following of this section, we first outline the process of identifying $k^*$ through harmful token extraction and random sequence generation. Then, we describe how to estimate the $v^*$ to establish its connection to $k^*$ that can generate safe responses. Last, we explain how to update the $W^{l^*}_{down}$, the MLP of specific layer $l^*$ (directly adopted from MEMIT \cite{meng2022mass}) accordingly.  


\subsection{Identify Key Representation $k^*$}
\label{subsec:3.1}
To identify the harmful-token-related key representation $k^*$, we first extract the harmful tokens from input queries that may trigger unsafe responses. To improve the stability of model editing on a specific harmful token, we generate multiple sequences that incorporate these tokens in varied contexts. Following that, we perform forward propagation for each sequence through the language model $f$ and use the internal representations at layer $l^*$ as harmful-token-related key representation $k^*$.

\partitle{Harmful tokens extraction} 
We automate this process using GPT-4 as a token extraction assistant, which analyzes each query to pinpoint tokens likely to trigger harmful outputs.
Formally, for each query in a set of harmful queries $q \in \mathcal{Q}_{harm}$, we extract a harmful token or phrase $t$, forming a set of consecutive harmful tokens $T_{h} = \{ t_1, t_2, \dots, t_n \}$, which can be defined as: $T_h = \operatorname{Extraction}(\mathcal{Q}_{harm})$.
The $\operatorname{Extraction}()$ is a carefully designed GPT-4 prompt (see Appendix \ref{app:extract}) that includes instructions to avoid generating any harmful content and to focus solely on the task of token extraction. 

\partitle{Random sequence generation} To enhance the accuracy of extracting the key vector $k^*
$ for the harmful tokens, we generate multiple sequences that incorporate these tokens. Formally, for each harmful token $t \in T_h$, we utilize GPT-4 to generate distinct sequences $\{x_j\}_{j=1}^N$, where $N=5$. These sequences are then used in the subsequent step to compute $k^*$. The prompt can be found \mbox{in Appendix~\ref{app:random}}.

\partitle{Calculate $k^*$ of harmful tokens} 
We perform forward propagation through the language model $f$ and average the internal representations at layer $l^*$ over $N$ generated sequences $x_j$ to represent the $k^*$ of harmful token $t$, which can be expressed as
\begin{equation}
\label{eq:k}
k^* = \frac{1}{N}\sum_{j=1}^{N}\sigma\big(W_{gate}^{l^*}\;\gamma(a_{x_j,t}^{l^*}+h_{x_j,t}^{l^*-1})\big),
\end{equation}
where $a_{x_j,t}^{l^*}$ and $h_{x_j,t}^{l^*-1}$ are the attention score and hidden score of the harmful token $t$ in sequence $x_j$ at layer $l^*$ and previous layer $l^*-1$ respectively. Aggregating key vectors over multiple sequences ensures that $k^*$ encodes robust, context-insensitive representations of harmful semantics.


\subsection{Estimate $v^*$ of Safe Response $Y_{target}$}
To establish the connection to $k^*$ that determines the model's likelihood of generating safe response, we optimize $v^*$ with the following loss function: 
\begin{equation}
    \label{eq:v_safe}
    L_{safe} = {-\log P_{f(m^{l^*}_i :=v)}[Y_{target} \,\big\vert\,q]},
\end{equation}
where $m_i^{l^*}$ refers to the MLP output activation at layer $l^*$ and position $i$, and $f(m_i^{l^*} := v)$ indicates the model $f$ with the specified activation replaced by vector $v$, and $q$ represents the harmful query in $\mathcal{Q}_{harm}$ introduced in Section \ref{subsec:3.1}.

To prevent unintended triggers of the safe response in ordinary contexts where the harmful token might appear benignly, we want the updated model to remain consistent with its original distribution when asked a benign query, thus avoiding the over-activation of the safe response in normal conversation. We use KL-divergence to achieve this, which can be formulated as:
\begin{equation}
    \label{eq:v_kl}
    \small
    L_{utility} = {KL\bigl(P_{f(m^{l^*}_i :=v)}\bigl[\;\cdot \mid q_u\bigr]\,\Bigm\Vert\,P_{\!f}\bigl[\;\cdot \mid q_u\bigr]\bigr)},
\end{equation}
where \(q_u\) is a neutral prompt of the form 
\emph{``What is \{\,harmful token\,\}?''}.
The optimization can be formulated as the following joint objective for $v^*$:
\begin{equation}
    \begin{aligned}
    \label{eq:v}
    {v}^* \;=\;
    \underset{{v}}{\mathrm{arg\,min}} [L_{safe} + \lambda L_{utility}].
    \end{aligned}
\end{equation}
Solving Eq.\ref{eq:v} yields the final value vector $v^*$, which can ensure that occurrences of the harmful token result in the safe response.


\subsection{Weight Update of $W_{down}^{l^*}$}
\label{subsec:weight_update}
After obtaining the pair $\bigl(k^*, v^*\bigr)$, we incorporate this new key-value association into the MLP at layer~$l^*$ by editing the matrix $W_{down}^{l^*}$ via solving the least-squares problem \cite{belinkov2019analysis}:
\begin{align}
    \label{eq:weight_update_objective}
    \small
    &\min_{\widehat{W_{down}^{l^*}} }
    \bigl\|\widehat{W_{down}^{l^*}}  K_D - V_D \bigr\|^2 \\
    \;\;
    &\text{subject to}\;\;
    \widehat{W_{down}^{l^*}} k^* = v^*.
\end{align}
Here, $K_D = [k_1^*,\,k_2^*,\,\ldots]$ is a matrix of key vectors, and $V_D = [v_1^*,\,v_2^*,\,\ldots]$ is the matrix of their corresponding value vectors. 
Eq.\ref{eq:weight_update_objective} can be solved with this closed form solution:
%and spread updates evenly over the range of crucial layers $l \in \mathcal{R}$ via:
\begin{equation}
    \label{eq:W}
    \small
    \widehat{W_{down}^{l^*}} 
    =
    W_{down}^{l^*}
      \;+\;
      R_{D} \,{K_{D}}^T
      \bigl(C^{l^*} \;+\; K_{D}\,{K_{D}}^T)^{-1},
\end{equation}
where $C^{l^*} = KK^\top$ denotes the covariance matrix of $K$, which is the key of original knowledge pair $K$ and $V$ at layer $l^*$, pre-cached from Wikipedia dataset. 
The term $R_D$ is defined as
\begin{equation}
    \label{eq:Rd}
    R_D =V_D - W_{down}^{l^*} K_D,
\end{equation}
which measures the residual error between the desired values $V_D$ and the model’s current outputs $W_{down}^{l^*}K_D$ at target layer $l^*$. 


\partitle{Practical scheme} In practice, instead of updating a single layer $l^*$, we spread the updates over a range of crucial layers $\mathcal{R} = \{l_1, l_2,..., L\}$ to limit the magnitude of parameter changes in a single layer, which results for better robustness \cite{zhu2020modifying}. For example, we directly adopt the finding in MEMIT and use the $7_{th}$ and $8_{th}$ layer as the crucial layers for \texttt{Llama2} and \texttt{Vicuna}. The $v^*$ and the residual in Eq.\ref{eq:Rd} is only estimated for the last crucial layer $L$. This residual is then distributed to the lower layer with a factor $L - l + 1$, which can be expressed as:
\begin{equation}
    \label{eq:Rd}
    R_D =\frac{V_D - W_{down}^{L} K_D}{L - l + 1}.
\end{equation}
By ensuring smaller changes in lower layers, \textit{DELMAN} can promote stability and avoid abrupt changes in a single layer. A detailed description of the algorithm is provided in Appendix \ref{app:alg}.

%we spread updates evenly over the range of crucial layers $l \in \mathcal{R}$. We define the target layer $L = \operatorname{max}(\mathcal{R})$ at the end of the crucial layers, at which the new memories should be fully represented. We adopt the hyperparameters from MEMIT, where for Llama2, $\mathcal{R} = [7,8]$, and for Vicuna, $\mathcal{R} = [7,8]$ \cite{meng2022mass}. \textit{DELMAN} does not require further on-the-fly reasoning or searching to determine $l^*$, making procedure more streamlined. 

%Overall, \textit{DELMAN} ensures that $k^*$ is now projected to $v^*$ while minimally affecting the model’s broader behavior. Hence, when the model detects an input corresponding to $k^*$, it not only generates a safe response for that token but also produces the same effect for tokens with similar semantics, thereby achieving broader protection.

\section{Experiments}

\begin{figure*}[t]
\centering
\includegraphics[width=.9\textwidth]{fig/Main_result_2x3.png}
\vspace{-.5em}
\caption{ASR across four datasets (HB, AB, JBB, and MI) for \texttt{Llama2-7B} (top row) and \texttt{Vicuna-7B} (bottom row) under three attack methods: \textit{GCG}, \textit{AutoDAN}, and \textit{PAIR}. Each bar group compares five defense strategies --- \emph{Original Model}, \emph{LoRA}, \emph{SafeDecoding}, \emph{LED}, and \emph{DELMAN}. Lower ASR indicates more robust defense.}
\label{fig:main_result}
\vspace{-1em}
\end{figure*}

We begin this section by detailing the configuration of our experiments, including evaluated datasets, jailbreak attacks, and models, along with compared baselines and evaluation metrics. Then, we present the effectiveness of \textit{DELMAN} in terms of defense performance and utility preservation. Next, we demonstrate the impact of single-behavior edit of \textit{DELMAN}, highlighting its transferability across datasets and harmful behaviors. Last, we use a consecutive edit case study to illustrate that each edit, once applied, does not interfere with the edit established in previous phases.
\subsection{Experiment Setup}

\begin{table*}[b]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{cc|c|ccccccc}
    \toprule % 上边框加粗
    \multirow{3}{*}{Model} & \multirow{3}{*}{Defense} & \multirow{3}{*}{MT-Bench} & \multicolumn{7}{c}{Downstream Tasks} \\
     &  &  & \makecell{Closed-\\domain QA} & Dialogue & NER & NLI & Reasoning & \makecell{Sentiment\\analysis} & Summarization \\ \midrule
    \multirow{5}{*}{\texttt{Vicuna-7B}} & \textit{Original Model} (82.1\%) & 6.77 & 0.777 & 0.483 & 0.287 & 0.563 & 0.982 & 0.862 & 0.272\\
    \cmidrule{2-10}
     & \textit{LoRA} (23.2\%)                        & 5.64 & 0.742 & 0.459 & 0.177 & 0.610 & 0.976 & 0.898 & 0.268\\ 
     & \textit{SafeDecoding} (10.7\%)                         & 6.61 & 0.671 & 0.314 & 0.098 & 0.536 & 0.969 & 0.645 & 0.174\\ 
     & \textit{LED} (8.8\%)                                  & 3.70 & 0.760 & \textbf{0.478} & \textbf{0.265} & 0.558 & 0.974 & 0.831 & \textbf{0.267}\\ 
     & \textit{DELMAN} (6.7\%)                              & \textbf{6.84} (\textcolor{green}{$\uparrow$}) & \textbf{0.762} & 0.470 & 0.254 & \textbf{0.560} & \textbf{0.981} & \textbf{0.854} & 0.260\\ \midrule 
    \multirow{5}{*}{\texttt{Llama2-7B}} & \textit{Original Model} (23.2\%) & 6.89 & 0.734 & 0.465 & 0.187 & 0.603 & 0.977 & 0.909 & 0.267\\ 
    \cmidrule{2-10}
     & \textit{LoRA} (8.6\%)                        & 6.90 & 0.769 & 0.480 & 0.288 & 0.551 & 0.976 & 0.854 & 0.259\\
     & \textit{SafeDecoding} (1.2\%)                        & 6.17 & 0.688 & 0.327 & 0.099 & 0.518 & \textbf{0.976} & 0.872 & 0.227\\ 
     & \textit{LED} (2.6\%)                                  & 5.80 & 0.705 & 0.425 & 0.228 (\textcolor{green}{$\uparrow$}) & 0.577 & 0.973 & 0.898 & \textbf{0.256}\\
     & \textit{DELMAN} (0.1\%)                               & \textbf{6.31} & \textbf{0.718} & \textbf{0.462} & \textbf{0.228} (\textcolor{green}{$\uparrow$}) & \textbf{0.612} (\textcolor{green}{$\uparrow$}) & 0.974 & \textbf{0.905} & 0.251\\ \bottomrule
    \end{tabular}
    }
    \caption{Utility evaluation of \textit{DELMAN} and baselines on \texttt{Vicuna-7B} and \texttt{Llama2-7B}, with the average ASR of each method is shown in parentheses. \textbf{Bold}: best score (excluding \textit{LoRA}); (\textcolor{green}{$\uparrow$}): improvement over \textit{Original Model}.}
    \label{tab:utility}
\end{table*}

\partitle{Datasets} To ensure a comprehensive evaluation of defense effectiveness against jailbreak attacks, we use the \textsc{HarmBench} \cite{mazeika2024harmbench} dataset for editing and evaluate across multiple testing benchmarks: \textsc{HarmBench} (HB), \textsc{AdvBench} (AB) \cite{zou2023universal}, \textsc{JailbreakBench} (JBB) \cite{chao2024jailbreakbench}, and \textsc{MaliciousInstruct} (MI) \cite{huang2023catastrophic}.
To comprehensively assess potential side effects of model editing on LLMs' general utility, we evaluate \textit{DELMAN} using \textit{MT-bench} \cite{zheng2023judging} and seven downstream tasks: \textit{Closed-domain QA}, \textit{Dialogue}, \textit{Named entity recognition (NER)}, \textit{Natural language inference (NLI)}, \textit{Reasoning}, \textit{Sentiment analysis} and \textit{Summarization}. The detail of the datasets and their evaluation metrics are presented in the appendix \ref{app:downstream_datasets}. 

\partitle{Evaluated jailbreak attacks and models} We use three leading jailbreak attack methods to demonstrate the defense performance  of \textit{DELMAN}: two optimization based attack \textit{GCG} \cite{zou2023universal}, \textit{AutoDAN} \cite{liu2023autodan} that search for adversarial suffix, and prompt-based attack \textit{PAIR} that rewrite the prompt to adversarial form \cite{chao2023jailbreaking}.  Our evaluation focuses on a strong aligned model, \texttt{Llama-2-7B-chat} \cite{touvron2023llama}, and a weak aligned model \texttt{Vicuna-7B-v1.5} \cite{zheng2023judging}. A detailed description of attack setup is provided in Appendix \ref{app:attack}.

\partitle{Baselines and evaluation metrics} 
We consider three different defense methods as baselines, \textit{SafeDecoding} \cite{xu2024safedecoding} an decoder modification method, Safety fine-tuning with \textit{LoRA} \cite{hu2021lora}, as well as \textit{LED} \cite{zhao2024defending}, an indirect editing method.
For all baseline methods, we follow their original papers' suggested hyper-parameter settings. A detailed description of baseline setup is provided in Appendix \ref{app:baseline}.
%We include LoRA \cite{hu2021lora} fine-tuned on \textsc{HarmBench} dataset as a defense baseline to compare fine-tuning and know\textit{LED}ge editing approaches. To quantify the defense effectiveness, 
We employ \textsc{HarmBench} classifier \cite{mazeika2024harmbench} to detect the harmful content in model responses. The primary evaluation metric is the Attack Success Rate (ASR), which measures the proportion of successful attacks over all tested examples. For a dataset $\mathcal{Q}_{harm}$ containing harmful queries $q$, ASR is formally defined as:
\begin{equation}
\text{ASR}(\mathcal{Q}_{harm}) = \frac{1}{|\mathcal{Q}_{harm}|} \sum_{q\in\mathcal{Q}_{harm}} \mathbb{I}(f(q))
\end{equation}
where $\mathbb{I}$ is the indicator function that returns 1 for successful attacks and 0 otherwise.

\subsection{Effectiveness of \textit{DELMAN}}
\partitle{Safety evaluation} Figure \ref{fig:main_result} compares \textit{DELMAN} with baselines and the \textit{Original Model} under three jailbreak attacks across four datasets. \textit{DELMAN} edits the model according to \textsc{HarmBench} (HB) data, and evaluates the edited model performance on AB, JBB and MI, showing its generalization ability on unseen datasets. The exact value of reduced ASR is relegated to Appendix \ref{app:data1}. We observe several key findings. First, compared to the original model, \textit{DELMAN} significantly reduces the ASR across all datasets (HB, AB, JBB, and MI) and against different attack types, including optimized suffix attacks (\textit{GCG}, \textit{AutoDAN}) and prompt-rewriting attacks (\textit{PAIR}), and in many cases \textit{DELMAN} is able to completely mitigate jailbreak attacks, reducing ASR to 0. Second, among baselines, \textit{LED} also demonstrates some defensive capability, even surpassing \textit{DELMAN} in certain scenarios within HB. However, \textit{LED} struggles on unseen datasets, indicating a lack of generalization. In contrast, \textit{LoRA} and \textit{SafeDecoding} perform worse, failing to bring ASR down to an acceptable level. Last, since \texttt{Llama2} already exhibits strong safety alignment, \textit{PAIR} has little effect on it. As a result, the improvements from \textit{DELMAN} in this \mbox{case are less pronounced}.
%Unlike \textit{LED}, which reduces ASR via partial layer tuning, \textit{DELMAN} employs token-level edits to surgically disentangle adversarial pathways linked to harmful behaviors, this approach disrupts the coherence of malicious prompts while preserving model utility.
%The results demonstrate that targeted parameter editing is more effective than general fine-tuning in defending against jailbreak attacks. \textit{DELMAN}'s focused parameter modification approach helps the model better maintain its safety constraints across different attack methods.

%\partitle{Performance of each single editing model and edit all}

%\partitle{Compare with baseline}
%Figure~\ref{fig:main_result} compares \textit{DELMAN} with \textit{LED} and the original model across three jailbreak attacks (GCG, AutoDAN, PAIR) and four datasets: \textsc{HarmBench} (HB), \textsc{AdvBench} (AB), \textsc{JailbreakBench} (JBB), and \textsc{MaliciousInstruct} (MI). 

%Notably, while \textit{LED} reduces ASR compared to the original model, our \textit{DELMAN} strategy secures markedly lower ASR rates across most tested datasets and attacks, suggesting that precise, token-level edits are more adept at disrupting the pathways through which harmful prompts induce misalignment. Beyond simply fine-tuning some layers, \textit{DELMAN} systematically disentangles key parameters associated with undesirable behaviors, effectively mitigating malicious prompt executions.Overall, \textit{DELMAN} demonstrates a more robust alignment mechanism against diverse jailbreaking vectors, outperforming the \textit{LED} baseline.
\partitle{Utility evaluation} We summarizes the performance of \textit{DELMAN}  and baselines on general-purpose tasks with \texttt{Vicuna-7B} and \texttt{Llama2-7B} on \textit{MT-Bench}, along with seven downstream tasks to comprehensively evaluate the model's utility in Table \ref{tab:utility}. The highest utility scores are highlighted in bold (except \textit{LoRA} which has the highest ASR), and scores that exceed those of the \textit{Original Model} are marked with (\textcolor{green}{$\uparrow$}). Overall, \textit{DELMAN} better preserves model utility compared to baseline approaches on most tasks. Notably, on \texttt{Vicuna-7B}, \textit{DELMAN} even achieves higher scores than the \textit{Original Model} on \textit{MT-Bench} (6.84 vs 6.77). For \texttt{Llama2-7B}, \textit{DELMAN} shows improvements over the \textit{Original Model} in several tasks, including \textit{NER} (0.228 vs 0.187) and \textit{NLI} (0.612 vs 0.603). Other defense methods like \textit{LED} and \textit{SafeDecoding} typically show performance drop. Although \textit{LED} achieves the highest scores in \textit{Dialogue}, \textit{NER} and \textit{Summarization} on \texttt{Vicuna-7B}, it experiences significant degradation on \textit{MT-Bench} (dropping to 3.70), as MT-bench evaluates through multi-turn interactions rather than single-task performance. \textit{SafeDecoding} shows consistent utility losses across most tasks. Figures \ref{fig:mtbench} present a detailed breakdown of model performance across \textit{MT-Bench} subcategories. The visualization particularly highlights \textit{DELMAN}'s advantages in preserving complex capabilities, with the largest area marked in dark blue. Notably, \textit{DELMAN} maintains strong performance in Reasoning, Writing, and Roleplay tasks, where \textit{LED} and \textit{SafeDecoding} exhibit substantial weaknesses. This demonstrates \textit{DELMAN}'s ability to balance robustness against jailbreak attacks while minimizing degradation in general utility.

%This superior preservation of capabilities stems from \textit{DELMAN}'s targeted parameter editing approach, which precisely modifies safety-related parameters while preserving the broader know\textit{LED}ge and reasoning capabilities encoded in the model.
\begin{figure*}[]
    \centering
    \includegraphics[width=0.45\linewidth]{fig/MT-Bench_vicuna.png}
    \hfill
    \hspace{-2em}
    \includegraphics[width=0.45\linewidth]{fig/MT-Bench_llama2.png}
    \caption{\small Comparison of \textit{MT-Bench} sub-scores across eight skill dimensions between different defense methods on \texttt{Vicuna-7B} (left) and \texttt{Llama2-7B} (right).}
    \label{fig:mtbench}
\end{figure*}

\begin{figure}[]
\setlength{\abovecaptionskip}{0.2cm}
  \centering
  \includegraphics[width=\linewidth]{fig/single-vicuna.png}
  \vspace{-.5em}
  \caption{ASR for \texttt{Vicuna-7B} after applying single-behavior \textit{DELMAN} against \textit{GCG} and \textit{AutoDAN} attacks.}
  \label{fig:single-vicuna}
    \vspace{-1em} 
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{fig/llama2-GCG-harmbench-heat.png}
    \includegraphics[width=\linewidth]{fig/llama2-AutoDAN-HarmBench-heat.png}
    \caption{\small
    ASR heatmaps for the cross-behavior  transfer results of single-behavior \textit{DELMAN} edit on \texttt{Llama2-7B} against \textit{GCG} (\textit{up}) and \textit{AutoDAN} (\textit{down}) attacks.}
    \label{fig:llama2_heatmap_gcg_autodan}
    \vspace{-1.5em}
\end{figure}

\subsection{Edit According to Harmful Behavior}
In this section, we investigate the effect of \textit{DELMAN} edit on individual harmful behavior and its impact on defending other unedited behavior.

\partitle{Effectiveness of \textit{DELMAN} on each harmful behavior} 
Figure~\ref{fig:single-vicuna} compares the performance of \textit{DELMAN} across individual \textsc{HarmBench} behavior, including chemical and biological (CheBio), cybercrime intrusion (CybIn), harassment and bullying (HaraBull), general harmful (GenHarm), illegal (Ill), and misinformation (MisInfo). The two figures demonstrate the ASR drop on \textit{GCG} and \textit{AutoDAN} after \textit{DELMAN} edits respectively.  In single-behavior editing, \textit{DELMAN} demonstrates significant effectiveness in defending against two types of jailbreak attacks.

%However, since these edits only modify parameters related to one specific type of harmful behavior, their effectiveness may be limited for other categories of unsafe queries, resulting in relatively higher ASR for off-category prompts. We further analyze how single-category edits may still provide partial protection against other types of attacks in the following discussion. Meanwhile, the \emph{edit-all} approach, which considers all categories during editing, achieves more balanced ASR reduction across different harmful behaviors, demonstrating the advantage of comprehensive defense editing.

\begin{figure}[t]
    \centering
    %-- Subfigure 1
    \begin{subfigure}[t]{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/k_class.png} % Replace with your actual file name
        \subcaption{The $k$ of harmful tokens across behaviors.}
        \label{fig:pca1}
    \end{subfigure}
    \hfill
    %-- Subfigure 2
    \begin{subfigure}[t]{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/k_dataset.png} % Replace with your actual file name
        \subcaption{The $k$ of harmful tokens across datasets.}
        \label{fig:pca2}
    \end{subfigure}
    \hfill
    \caption{Principal Component Analysis (PCA) visualizations of $k$ at the target layer $L$ of \texttt{Llama2-7B} across different behaviors and datasets.}
    \label{fig:two_pca}
    \vspace{-1em}
\end{figure}

\partitle{Cross-behavior observations} We further study the cross-behavior defense performance of \textit{DELMAN} with heatmap. 
%To further explore \textit{DELMAN} capacity for cross-domain generalization, we conducted experiments across the six constituent categories of \textsc{HarmBench}: \emph{chemical biological}, \emph{cybercrime intrusion}, \emph{harassment bullying}, \emph{harmful}, \emph{illegal}, and \emph{misinformation disinformation}. 
We perform single-behavior edits on each behavior with \textit{DELMAN}, and test the resulting model on all six categories, presenting a $6{\times}6$ ASR heatmap. Figure~\ref{fig:llama2_heatmap_gcg_autodan} presents the results for \texttt{Llama2-7B} under the \textit{GCG} and \textit{AutoDAN} jailbreak attacks. Notably, single-category edits in many cases show resilience to off-category attacks. For instance, focusing on CheBio class editing can also mitigate malicious queries from GenHarm or MisInfo classes, reducing ASR even for these distinct domains.

\subsection{Understanding the \textit{DELMAN} Transferability Across Datasets and Behaviors}
\textit{DELMAN} establishes a direct link between harmful tokens and specific responses to modify the model parameters effectively. To explain why modifying the model based on one set of harmful tokens from a specific harmful behavior also improves its robustness against different harmful behavior, and why edits made using examples from one dataset generalize to other datasets, we analyze the distribution of harmful token keys $k$ in the target model layer $l^*$ using Principal Component Analysis (PCA) \cite{PCA}. As shown in Figure \ref{fig:two_pca}, each cluster represents the $k$ of harmful token from a behavior (Figure \ref{fig:pca1}) or from a dataset (Figure \ref{fig:pca2}). We can note that harmful token keys $k$ in the target model layer $l^*$ from different categories or datasets exhibit substantial overlap in the embedding space, suggesting that instructions carrying malicious intent share similar representations across seemingly distinct harm classes or datasets. Through focused editing of these common token representations, \textit{DELMAN} effectively reduces various types of harmful outputs, including those from categories or datasets not seen during editing.

%A deeper look into the target-layer token distributions reveals that the harmful tokens $t$ used in different categories or datasets exhibit substantial overlap in the embedding space (see Figure~\ref{fig:four_pca}), suggesting that instructions carrying malicious intent share similar representations across seemingly distinct harm classes. Through focused editing of these common token representations, \textit{DELMAN} effectively reduces various types of harmful outputs, including those from categories not seen during editing.


\subsection{Consecutive Edits with \textit{DELMAN}}
%To investigate the feasibility of applying \textit{DELMAN} iteratively, we conduct sequential editing experiments on Llama2. Specifically, we select one category each from HB, AB, JBB, and MI datasets, and perform \textit{DELMAN} edits successively on these categories. After each edit, we evaluate: (1) ASR on the currently edited category, (2) ASR on all previous edited categories to check for potential recovery, and (3) ASR on the complete datasets to assess overall robustness.
In real-world deployment, adversarial parties may repeatedly attempt to jailbreak the model, making it crucial for dynamic and consecutive edits to maintain the effects of earlier modifications without interference. To evaluate the robustness of \textit{DELMAN} under consecutive edits, we conduct an experiment where edits are applied sequentially across different harmful behavior categories. Specifically, we select one category each from the HB, AB, JBB, and MI datasets and perform \textit{DELMAN} edits in succession. After each edit, we evaluate:
\begin{itemize}
    \item \textbf{ASR on the current edit category} to measure the immediate effectiveness of \textit{DELMAN}.
    \item \textbf{ASR on previously edited categories} to determine whether earlier modifications remain effective.
    \item \textbf{ASR on the full dataset} to assess the overall robustness of \textit{DELMAN} against diverse jailbreak attacks.
\end{itemize}
%The results, shown in Figure \ref{fig:case}, reveal several promising findings. First, the ASR of each edited category remains consistently low throughout subsequent edits, with no signs of recovery. This suggests that \textit{DELMAN} defense effects are persistent and stable. Second, we observe that successive edits actually lead to further ASR reductions in previously edited categories, indicating a positive transfer effect where later defenses reinforce earlier ones. Third, the overall ASR across complete datasets shows a steady decline, demonstrating that category-specific edits contribute to global robustness improvement. Most notably, these defensive benefits come without compromising the model's general capabilities, as evidenced by maintained or even improved MT-Bench scores (Table \ref{tab:delman_comparison}).
We used line charts to represent the overall ASR reduction across four successive edit phases for each edited behavior of HB dataset and the ASR of the entire HB dataset. As observed in Figure \ref{fig:case}, the overall ASR for the HB dataset consistently decreases with each edit, indicating that \textit{DELMAN} effectively reduces harmful behaviors across multiple categories and each edit achieves maximal ASR drop in its targeted behavior. Additionally, each category edited during the successive phases maintains its defense effectiveness, with no increase of ASR in subsequent edits. This demonstrates that each edit, once applied, is preserved and does not interfere with the defense applied in previous phases, ensuring continuous and cumulative reduction in \mbox{ASR across the dataset}.
%These findings demonstrate that \textit{DELMAN} enables compositional and incremental defense construction, where edits targeting specific harmful prompts naturally extend protection to other categories. This allows for gradual enhancement of model safety without computationally expensive retraining as new threats emerge, while maintaining the model's original capabilities.

%\subsection{Edit on Harmbench transfer to other dataset}
%dataset: Jailbreakbench, Advbench, MI\\
%\partitle{transferability of edit all} bar plot
%\partitle{transferability of classification+single editing} heatmap

%\subsection{transfer to other attack} table


\begin{figure}[ht]
    \centering\includegraphics[width=.8\linewidth]{fig/case1.png}
    \centering\includegraphics[width=.8\linewidth]{fig/case2.png}
    \caption{\small
    Defense performance of consecutive \textit{DELMAN} edits on \texttt{Llama2-7B} against \textit{GCG} attacks.}
    \label{fig:case}
\end{figure}
\vspace{-1em}
\section{Conclusion}
\vspace{-.5em}
In this work, we introduce \textit{DELMAN}, a novel defense mechanism that directly edits model parameters to neutralize harmful behaviors by forming explicit connections. \textit{DELMAN} brings minimal parameter modification, preserving the utility on normal tasks and is capable of dynamic and consecutive edits. Extensive experiments demonstrate superiority over existing baselines in terms of defense performance and utility preservation, as well as strong transferability. Overall, \textit{DELMAN} demonstrates how token-level editing method can effectively enhance model safety while maintaining performance. In the future, it would be interesting to investigate more efficient methods for harmful token identification, for instane, using a minimal set of tokens (e.g., 20-30 Tokens) to effectively cover the majority of harmful scenarios, which would significantly reduce computational costs. Additionally, exploring the application of \textit{DELMAN} to domain-specific LLMs and VLMs would validate its generalizability across different domains and modalities.



%achieves robust protection through minimal parameter modifications, focusing specifically on token-level editing. This token-centric strategy ensures strong generalization against various jailbreak attempts, as the presence of harmful tokens consistently triggers appropriate defensive responses regardless of prompt engineering.

%Our experiments demonstrate three key advantages: 1) Enhanced model robustness against harmful queries through targeted parameter editing; 2) Strong transferability and generalization ability to unseen attacks, complemented by the capability of successive edits; 3) Preserved model utility with performance on MT-Bench and downstream tasks remaining comparable to the original model. Overall, \textit{DELMAN} demonstrates how token-level editing method can effectively enhance model safety while maintaining performance.
\clearpage
\section*{Limitations}
\noindent The limitations of our study are as follows:

1. Our evaluations are currently restricted to general-purpose LLMs, leaving the applicability to domain-specialized models (e.g., medical or legal LLMs) and larger-scale models (e.g., 70B parameters) unexplored. Further investigation is required to assess its defense capabilities against domain-specific jailbreak attacks and potential impacts on domain expertise after editing.

2. \textit{DELMAN} relies on GPT-4 for harmful token extraction and context generation, which introduces dependency on external models and potential cost barriers.

3. The stability of consecutive edits, though preliminarily validated, needs deeper analysis to assess potential performance drift over extended deployment.

\section*{Ethics Statement}
\textit{DELMAN} directly edits parameters linked to harmful tokens, raising concerns about potential misapplication or unintended bias introduction. We advocate for responsible deployment where practitioners thoroughly validate parameter modifications and strictly limit edits to well-defined harmful content categories. While our approach offers fine-grained, post-deployment protection, it should be viewed as one component within a comprehensive safety framework that includes human oversight and established moderation systems to ensure ethical and harm-free interactions.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{main}
\clearpage
\appendix
\section{Algorithm}
\label{app:alg}
Algorithm \ref{alg:DELMAN} demonstrates the detailed procedure of \textit{DELMAN}.

\begin{algorithm*}
\caption{\textbf{\textit{DELMAN}: Dynamic Editing for LLM Jailbreak Defense}}
\label{alg:DELMAN}

\textbf{Input:} Original LLM $f$, Harmful query dataset $\mathcal{Q}_{\text{harm}}$, Target safe response $Y_{\text{target}}$, Target layers $\mathcal{R}$ and the last target layer $L$, Covariance matrix $C^{l}$ for each layer $l \in \mathcal{R}$, Number of random context sequences $N$, KL-divergence factor $\lambda$.

\textbf{Output:} Edited model $f'$

\begin{algorithmic}[1]
\State \textbf{Initialize:} $T_h \gets \varnothing$; $f' \gets f$
\For {$q \in \mathcal{Q}_{harm}$}
    \State $t \gets \operatorname{Extraction}(q)$
\EndFor
\State $T_{h} = \{ t_1, t_2, \dots, t_n \}$
\For{$t \in T_{h}$}
    \For{$j = 1$ to $N$}
        \State $x_{j,t} \gets \operatorname{GenerateSequence}(t)$ 
    \EndFor
\EndFor
\For {$t \in T_{h}$}
    \State ${v^*_t} \gets \underset{{v_t}}{\mathrm{arg\,min}} [L_{safe} + \lambda L_{utility}]$
        \Comment{Eq.\ref{eq:v}}
\EndFor
\State $V_D \gets [v_1^*, v_2^*,\dots, v_n^*]$

\For{$l \in \mathcal{R}$}
    \For {$t \in T_{h}$}
        \For{$j = 1$ to $N$}
        \State $k_{t,j}^{l} \gets \sigma\!\bigl(W_{gate}^{l}\,\gamma(a_{x_j,t}^{l}+h_{x_j,t}^{l-1})\bigr)$
            \Comment{Eq.\ref{eq:k}}
        \EndFor
    \State $k^{l}_t \gets \frac{1}{N}\sum_{j=1}^{N}k_{t,j}^{l}$
        \Comment{Eq.\ref{eq:k}}
    \EndFor
    \State $K^{l}_D \gets [k_1^{l},k_2^{l}, \dots, k_{n}^{l}]$
    \State $R^l_D = \frac{V_D - W_{down}^{L} K_D^L}{L - l + 1}$
         \Comment{Eq.\ref{eq:Rd}}
    \State $f'
    \gets
    W_{down}^{l}
      \;+\;
      R_{D}^{l} \,{K_{D}^{l}}^T
      \bigl(C^{l} \;+\; K_{D}^{l}\,{K_{D}^{l}}^T)^{-1}$
      \Comment{Eq.\ref{eq:W}}
\EndFor
\State return $f'$
\end{algorithmic}
\end{algorithm*}



\section{Detail Experiment Setting}
\subsection{Attack Setup}
\label{app:attack}
For \textit{GCG} \cite{zou2023universal}, \textit{AutoDAN} \cite{liu2023autodan}, and \textit{PAIR} \cite{chao2023jailbreaking}, we leverage multiple harmful query datasets: the complete \textsc{HarmBench} (200 queries), a subset of \textsc{AdvBench} (100 queries), the complete \textsc{JailbreakBench} (100 queries), and the complete \textsc{MaliciousInstruct} (100 queries). For each model under test, we generate specific attack prompts using every query from these datasets, following the original hyperparameter configurations as described in their respective papers.
\subsection{Baseline Setup}
\label{app:baseline}
\subsubsection{Original Model} Recent work \cite{inan2023llama, xu2024comprehensive} has demonstrated that system prompts and prompt formats can significantly influence the effectiveness of jailbreaking attacks. For experimental consistency and reproducibility, we utilize the default settings across all evaluations.

\partitle{Llama2 template}
\begin{tcolorbox}[
    colback=white,
    colframe=black,
    boxrule=0.5pt,
    left=5pt,
    right=5pt,
    top=5pt,
    bottom=5pt,
    breakable=true,
    width=\linewidth,
    fontupper=\small
]
{\ttfamily
[INST] <<SYS>>\\
You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\
\\
If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\
<</SYS>>\\
\\
\textcolor{blue}{\{instruction\}} [/INST]}
\end{tcolorbox}

\partitle{Vicuna template}
\begin{tcolorbox}[
    colback=white,
    colframe=black,
    boxrule=0.5pt,
    left=5pt,
    right=5pt,
    top=5pt,
    bottom=5pt,
    breakable=true,
    width=\linewidth,
    fontupper=\small
]
{\ttfamily
A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: \textcolor{blue}{\{instruction\}} ASSISTANT:}
\end{tcolorbox}

\subsubsection{DELMAN}
The editing dataset consists of 200 queries from \textsc{HarmBench}, along with a single uniform $Y_{target}$: \emph{``I'm sorry, I can't assist with that.''}. Our method does not require a dedicated safe response for every single question; Instead, just this one $Y_{target}$ is sufficient to provide robust defenses with minimal impact on the model’s utility. And we directly adopt the findings in MEMIT \cite{meng2022mass} and use the $7_{th}$ and $8_{th}$ layer as the crucial layers for \texttt{Llama2-7B} and \texttt{Vicuna-7B}. The hyperparameters of \textit{DELMAN} are shown in Table~\ref{tab:DELMAN_hp}.
\begin{table}[h!]
\centering
\begin{tabular}{ll}
\hline
\textbf{Hyperparameter} & \textbf{Value} \\
\hline
Target Layers $\mathcal{R}$ & [7,8] \\
Learning Rate of $v^*$ & 5e-1 \\
Weight Decay of $v^*$ & 0.5 \\
Gradient Steps of $v^*$ & 25 \\
Loss Layer of $v^*$  & 31 \\
KL-divergence Factor & 0.0625 \\
Gradient Norm Clamp Factor & 0.75 \\
Mom2 Update Weight & 15000 \\
Optimizer & Adam \\
\hline
\end{tabular}
\caption{\textit{DELMAN} hyperparameters. Values are shared across models unless specified.}
\label{tab:DELMAN_hp}
\end{table}


\subsubsection{LoRA}
We also apply \textit{LoRA} fine-tuning on the same 200 queries from the \textsc{HarmBench}; However, in this setup, each query is paired with a safe response generated by GPT-4 as the $Y_{target}$. We have verified that these $Y_{target}$ achieve 0 ASR on \textsc{HarmBench} classifier. Notably, if we were to follow the same strategy as used in \textit{DELMAN} and adopt a single uniform $Y_{target}$ for all queries, the model would inevitably converge to generating only that single response. This would severely limit the model's ability to provide diverse and contextually appropriate responses. The hyperparameters of \textit{LoRA} are shown in Table~\ref{tab:lora_hp}.
\begin{table}[h!]
\centering
\begin{tabular}{ll}
\hline
\textbf{Hyperparameter} & \textbf{Value} \\
\hline
LoRA Alpha & 8 \\
LoRA Rank & 32 \\
LoRA Dropout & 0.05 \\
Train Batch Size & 1 \\
Gradient Accumulation Steps & 8 \\
Train Epoch & 1 \\
Learning Rate (\texttt{Llama2-7B}) & 2e-3 \\
Learning Rate (\texttt{Vicuna-7B}) & 1e-3 \\
Optimizer & AdamW \\
\hline
\end{tabular}
\caption{\textit{LoRA} hyperparameters. Values are shared across models unless specified.}
\label{tab:lora_hp}
\end{table}


\subsubsection{SafeDecoding}
SafeDecoding \cite{xu2024safedecoding}, a safety enhancement method that operates by adjusting token probability distributions. This approach strengthens the model's security through two key mechanisms: boosting the probability of safety disclaimers while reducing the likelihood of potential jailbreak sequences. We utilized their publicly released fine-tuned versions of \texttt{Llama2} and \texttt{Vicuna} models.

\subsubsection{LED}
We used the same dataset as in the \textit{LoRA} setup. Since \textit{LED} \cite{zhao2024defending} did not provide an official code implementation, we reproduced their method following the procedures described in their paper. We selected the corresponding layers for each model according to their recommendations. The hyperparameters of \textit{LED} are shown in Table~\ref{tab:LED_hp}.
\begin{table}[h!]
\centering
\begin{tabular}{ll}
\hline
\textbf{Hyperparameter} & \textbf{Value} \\
\hline
Edit Layers (\texttt{Llama2-7B}) & [4,5,6,13,14,15] \\
Edit Layers (\texttt{Vicuna-7B}) & [9,10,11,13,14,15] \\
Target Layers & [29,30,31] \\
Learning Rate (\texttt{Llama2-7B}) & 8e-5 \\
Learning Rate (\texttt{Vicuna-7B}) & 5e-5 \\
Train Batch Size & 1 \\
Gradient Acc Steps & 8 \\
Train Epochs & 1 \\
Optimizer & AdamW \\
\hline
\end{tabular}
\caption{\textit{LED} hyperparameters. Values are shared across models unless specified.}
\label{tab:LED_hp}
\end{table}

\subsection{Downstream Task Datasets}
\label{app:downstream_datasets}
(1) \textit{Closed-domain QA} on the BoolQ \cite{clark2019boolq}, and the results were also measured by EM. (2) \textit{Dialogue} on the MuTual \cite{cui2020mutual}, and the results were measured by selecting one best-matched response from four available candidates \cite{lowe2015ubuntu}. (3) \textit{Named entity recognition (NER)} tested on CoNLL03 \cite{sang2003introduction} with F1 Score. (4) \textit{Natural language inference (NLI)} on RTE \cite{dagan2005pascal} measured by binary classification accuracy. (5) \textit{Reasoning} capability evaluated on GSM8K \cite{cobbe2021training} using solution accuracy. (6) \textit{Sentiment analysis} on SST2 \cite{socher2013recursive} measured by binary classification accuracy. (7) \textit{Summarization} skills evaluated on SAMSum \cite{gliwa2019samsum} using ROUGE metrics \cite{chin2004rouge}.








\section{Prompt}
\subsection{Prompt Used for Harmful Tokens Extraction via GPT-4}
\label{app:extract}
\begin{tcolorbox}[    
    enhanced,
    colback=gray!10,  % 背景色设为浅灰色
    colframe=black,   % 边框颜色
    boxrule=1.5pt,    % 边框粗细
    arc=5pt,          % 圆角大小
    left=10pt,        % 左内边距
    right=10pt,       % 右内边距
    top=5pt,          % 上内边距
    bottom=5pt        % 下内边距
]
\noindent You are tasked with analyzing queries to extract harmful tokens. Follow these instructions: 1. Do not generate harmful, inappropriate, or speculative content; 2. Do not fulfill or answer harmful queries; 3. Extract the most harmful tokens (keywords or phrases) that contribute to the query's harmful nature or potential misuse; 4. Output tokens as a list: \texttt{[token1, token2, ...]}. \\
\textbf{Query: \{INSERT QUERY HERE\}}
\end{tcolorbox}

\subsection{Prompt Used for Random Sequence Generation via GPT-4}
\label{app:random}
\begin{tcolorbox}[    
    enhanced,
    colback=gray!10,  % 背景色设为浅灰色
    colframe=black,   % 边框颜色
    boxrule=1.5pt,    % 边框粗细
    arc=5pt,          % 圆角大小
    left=10pt,        % 左内边距
    right=10pt,       % 右内边距
    top=5pt,          % 上内边距
    bottom=5pt        % 下内边距
]
\noindent You are tasked with generating random sequences that include the specified token. Follow these instructions: 1. Avoid generating harmful, inappropriate, or unsafe content; 2. Each sequence should be 15–30 words long; 3. Use the given token exactly once in each sequence.\\
\textbf{Tokens: \{INSERT TOKENS HERE\}}
\end{tcolorbox}


\section{Supplementary Materials}
\label{app:data}

\subsection{Effectiveness of \textit{DELMAN}}
\label{app:data1}
Table \ref{tab:gcg_pair_autodan} presents the exact value of reduced ASR by \textit{DELMAN} and baselines.

\subsection{Effectiveness of \textit{DELMAN} on Each Harmful Behavior}
Figure~\ref{fig:single-llama} compares the performance of \textit{DELMAN} on \texttt{Llama2-7B} across individual \textsc{HarmBench} behavior.
\begin{figure}[h!]
\setlength{\abovecaptionskip}{0.2cm}
  \centering
  \includegraphics[width=\linewidth]{app_fig/single-llama2.png}
  \caption{ASR for \texttt{Llama2-7B} after applying single-behavior editing against \textit{GCG} and \textit{AutoDAN} attacks.}
  \label{fig:single-llama}
\end{figure}

\subsection{Cross-Behavior Observations}
Figure~\ref{fig:vicuna_heatmap_gcg_autodan} presents the results for \texttt{Vicuna-7B} under the \textit{GCG} and \textit{AutoDAN} jailbreak attacks.
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{app_fig/vicuna-GCG-HarmBench-heat.png}
    \includegraphics[width=\linewidth]{app_fig/vicuna-AutoDAN-HarmBench-heat.png}
    \caption{\small
    ASR heatmaps for the cross-category transfer results of single-category \textit{DELMAN} defense on \texttt{Vicuna-7B} against \textit{GCG} (\textit{up}) and \textit{AutoDAN} (\textit{down}) attacks.}
    \label{fig:vicuna_heatmap_gcg_autodan}
\end{figure}

\subsection{Results of \textit{DELMAN} across Harmful and Clean Tokens}
Figure \ref{fig:harm_clean_pca} shows the $k$ and $v$ distribution differences between harmful and clean tokens. Notably, choosing harmful tokens is vital for preserving model utility: while editing with clean tokens also reduces ASR, these tokens frequently appear in benign queries across various contexts, leading to unnecessary modifications of the model's normal behaviors. In contrast, harmful tokens are primarily concentrated in unsafe queries, allowing for more precise interventions. This explains why editing based on clean tokens leads to significant degradation in \textit{MT-Bench} scores (see Table \ref{tab:delman_harm_clean}) - it unintentionally affects the model's processing of legitimate queries where these common tokens naturally occur. In our experiment, we define clean tokens as the third-to-last word in queries.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[t]{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{app_fig/k_harm_and_clean.png} % Replace with your actual file name
        \subcaption{The $k$ of harmful and clean tokens.}
        \label{fig:pca3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{app_fig/v_harm_and_clean.png} % Replace with your actual file name
        \subcaption{The $v$ of harmful and clean tokens.}
        \label{fig:pca4}
    \end{subfigure}
    \caption{Principal Component Analysis (PCA) visualizations of $k$ and $v$ at the target layer $L$ of \texttt{Llama2-7B} across harmful and clean tokens.}
    \label{fig:harm_clean_pca}
\end{figure}

\begin{table}[h!]
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{l|c|cccc}
\hline
\multirow{2}{*}{Method} & \multirow{2}{*}{MT-Bench} & \multicolumn{4}{c}{GCG} \\
\cline{3-6}
& & HB & AB & JBB & MI \\
\hline
\textit{DELMAN} & 6.31 & \textbf{0\%} & \textbf{0\%} & \textbf{0\%} & \textbf{1\%} \\
\textit{DELMAN(clean-token)} & 5.09(\textcolor{red}{$\downarrow$}) & 1\% & 1\% & 3\% & 1\% \\

\hline
\end{tabular}
}
\caption{ASR(\%) of \textit{GCG} attack and \textit{MT-Bench} score on \texttt{Llama2-7B} comparing vanilla \textit{DELMAN} and clean-token \textit{DELMAN}. \textbf{Bold}: lowest ASR.}
\label{tab:delman_harm_clean}
\end{table}



\subsection{Effectiveness of Sequential \textit{DELMAN}}
\begin{table}[h!]
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{l|c|cccc}
\hline
\multirow{2}{*}{Method} & \multirow{2}{*}{MT-Bench} & \multicolumn{4}{c}{GCG} \\
\cline{3-6}
& & HB & AB & JBB & MI \\
\hline
\textit{DELMAN}                   & 6.31 & \textbf{0\%} & \textbf{0\%} & \textbf{0\%} & \textbf{1\%} \\
\textit{DELMAN(Sequential-Case1)} & 6.35 & 3\% & 0\% & 10\% & 0\% \\
\textit{DELMAN(Sequential-Case2)} & 6.64 & 4\% & 5\% & 6\% & 0\% \\
\hline
\end{tabular}
}
\caption{ASR(\%) of \textit{GCG} attack and \textit{MT-Bench} score on \texttt{Llama2-7B} comparing vanilla \textit{DELMAN} and 4-Edit \textit{DELMAN}. \textbf{Bold}: lowest ASR.}
\label{tab:delman_seq}
\end{table}


\begin{table*}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{c|c|cccc|cccc|cccc}
\toprule
\multirow{2}{*}{\centering{Model}} 
& \multirow{2}{*}{\centering Defense} & \multicolumn{4}{c|}{GCG} & \multicolumn{4}{c|}{AutoDAN} & \multicolumn{4}{c}{PAIR} \\
& & HB & AB & JBB & MI & HB & AB & JBB & MI & HB & AB & JBB & MI \\
\midrule
\multirow{5}{*}{\centering \texttt{Vicuna-7B}} 
& \centering \textit{Original Model} & 92\%  & 89\%  & 89\%  & 94\%  & 69\%  & 78\%  & 73\%  & 83\%  & 80\%  & 75\%  & 77\%  & 86\%  \\
& \centering \textit{LoRA}        & 40\% & 18\% & 32\% & 8\% & 22\% & 29\% & 22\% & 32\% & 26\% & 13\% & 20\% & 16\% \\
& \centering \textit{SafeDecoding}        & 7\% & 4\% & \textbf{3\%} & 1\% & 17\% & 20\% & 18\% & 8\% & 16\% & 8\% & 15\% & 11\% \\
& \centering \textit{LED}        & \textbf{3\%} & 6\% & 34\% & 5\% & 11\% & 9\% & 8\% & 10\% & \textbf{4\%} & 5\% & \textbf{6\%} & 5\% \\
& \centering \textit{DELMAN}     & 11\% & \textbf{2\%} & 17\% & \textbf{1\%} & \textbf{4\%} & \textbf{2\%} & \textbf{8\%} & \textbf{5\%} & 10\% & \textbf{5\% }& 11\% & \textbf{5\%} \\
\midrule
\multirow{5}{*}{\centering \texttt{Llama2-7B}} 
& \centering \textit{Original Model} & 42\% & 39\% & 46\% & 45\% & 23\% & 19\% & 27\% & 30\% & 2\% & 1\% & 4\% & 0\% \\
& \centering \textit{LoRA}        & 13\% & 2\% & 50\% & 32\% & 1\% & 0\% & 1\% & 0\% & 2\% & 0\% & 2\% & 0\% \\
& \centering \textit{SafeDecoding}        & 0\% & 4\% & 1\% & 1\% & 0\% & 0\% & 0\% & 0\% & 1\% & 4\% & 3\% & 0\% \\
& \centering \textit{LED} & 2\% & 0\% & 8\% & 8\% & 2\% & 1\% & 2\% & 2\% & 1\% & 0\% & 4\% & 1\% \\
& \centering \textit{DELMAN} & \textbf{0\%} & \textbf{0\%} & \textbf{0\%} & \textbf{1\%} & \textbf{0\%} & \textbf{0\%} & \textbf{0\%} & \textbf{0\%} & \textbf{0\%} & \textbf{0\%} & \textbf{0\%} & \textbf{0\%} \\
\bottomrule
\end{tabular}
}
\caption{ASR (\%) of three jailbreak attacks (\textit{GCG}, \textit{PAIR}, \textit{AutoDAN}) across four datasets on different models, under different defense methods. \textbf{Bold}: lowest ASR.}
\label{tab:gcg_pair_autodan}
\end{table*}

\section{Computing Resources}
The experiments are carried out on 2 NVIDIA A40 GPUs with a total computation time of 680 GPU hours.

\end{document}
