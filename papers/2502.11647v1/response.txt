\section{Related Work}
\subsection{Model Editing}
Model editing enables targeted behavioral modifications within specific domains and can be categorized as indirect editing and direct editing. Indirect model editing involves fine-tuning the model to update knowledge with specifically-designed objective **Michel, et al., "Meta-Learning for Model Editing"** or use meta-learning with hypernetworks to learn optimal parameter updates ____ by Chen, et al., "Efficient Meta-Learning with Hypernetworks". However, both approaches require extensive model updates, which risks catastrophic forgetting on non-target tasks.

%Meta-Learning, exemplified by MEND **Michel, et al., "MEND: Model Editing through Neural Design"** and KE ____ by Chen, et al., "Knowledge Editing for Large Language Models", employs hypernetworks to learn optimal parameter updates for knowledge modification while preserving model stability.
%Both fine-tuning and meta-learning require extensive model updates, risking catastrophic forgetting on non-target tasks.

Direct editing refers to directly locating and editing the knowledge-related parameters. Research indicate that factual knowledge is primarily stored in the MLP modules of transformer-based architectures ____ by Devlin, et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding".
Leveraging these insights, model-editing methods like ROME **Pang, et al., "ROME: Robust Model Editing via Causal Tracing"** employ causal tracing to identify and edit the parameters encoding the particular knowledge. However, ROME is limited to single-instance knowledge editing, restricting its applicability in scenarios requiring large-scale updates. MEMIT extends the approach to support batch knowledge editing, providing a scalable solution for efficient and precise modifications ____ by Zhang, et al., "MEMIT: Massively Efficient Model Editing through Iterative Tracing". 
%By targeting a minimal set of relevant parameters, direct-edit achieves higher precision and reduce interference with the model’s overall performance compared to indirect model editing.

\subsection{Existing Defense to Jailbreak Attacks}
Recent studies reveal that jailbreak attacks ____ by Wallace, et al., "Jailbreaking Large Language Models: A Study on Adversarial Attacks" can bypass security alignment leading LLMs to generate harmful or unethical outputs.
As countermeasures, various defense methods are developed against such threats.
Existing defenses can be categorized into active defenses and passive defenses. Active defense enhances LLMs robustness against adversarial prompting by dynamically altering model parameters ____ by Li, et al., "Robust Model Training with Adversarial Prompting". A common approach to safety training involves constructing safety-relevant datasets and fine-tuning the model ____ by Radford, et al., "Improving Language Understanding with Safety-Training".
Instead, passive defense aims to build auxiliary modules or use external safety methods including input and output filtering ____ by Chen, et al., "Input and Output Filtering for Large Language Models", input smoothing, sanitation and modification ____ by Liu, et al., "Input Smoothing and Sanitation for Model Robustness".

%1) System-level defenses do not alter the LLM itself, but rather add external safety measures on top of the LLM.
%These include input and output filtering via perplexity filter ____ by Chen, et al., "Perplexity Filter: A Novel Method for Input and Output Filtering", input smoothing ____ by Liu, et al., "Input Smoothing for Model Robustness"; input sanitization ____ by Radford, et al., "Sanitizing Large Language Models" and modification ____ by Wallace, et al., "Model Modification via Adversarial Training".
%2)Model-level defenses alter the LLM parameters to reduce the risk of malicious use and improve robustness to adversarial prompting. Safety training is commonly approached via fine-tuning methods such as RLHF ____ by Radford, et al., "Improving Language Understanding with Reinforcement Learning from Human Feedback" and DPO ____ by Li, et al., "DPO: A Novel Framework for Training Large Language Models".

\subsection{Model Editing as a Jailbreak Defense}
Several studies have explored LLMs model editing as a defense mechanism to precisely modify toxic regions ____ by Zhang, et al., "Precise Model Editing for Toxic Region Modification".
\textit{DINM} ____ by Liu, et al., "DINM: Direct Inference of Non-Maximal Parameters" and \textit{LED} ____ by Chen, et al., "Layer-wise Editing for Toxicity Reduction" are motivated by indirect model editing method that fine-tuning the toxic layer using specific objectives. The difference between these two methods is the way of locating the toxic region. 
%DINM ____ treat the layer that most effectively separates the distributions of safe and unsafe responses  as the toxic layer, while LED
%These approaches emphasize the crucial role of identifying toxic region locations. DINM ____, given safe sequence $Y_{\text{safe}}$ and unsafe sequence $Y_{\text{unsafe}}$ as input consider the toxic layer to be the transformer layer that most effectively separates the distributions of them.
%LED ____ identifies safety layers by selecting the top-k layers whose removal most frequently transforms safe outputs to unsafe ones during iterative pruning.
The layer-level localization and fine-tuning approaches lack precision in identifying harmful words while potentially compromising the model's general performance.
In contrast, we propose to adapt direct-edit as a jailbreak defense in LLMs. 
%can be applied to only those parameters tied to harmful tokens. This targeted approach aims to suppress problematic outputs without broadly disrupting the model’s behavior, thereby preserving performance on other tasks.
%To the best of our knowledge, this is the first work to employ a direct-editing approach for defense.