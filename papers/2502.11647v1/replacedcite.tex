\section{Related Work}
\subsection{Model Editing}
Model editing enables targeted behavioral modifications within specific domains and can be categorized as indirect editing and direct editing. Indirect model editing involves fine-tuning the model to update knowledge with specifically-designed objective ____ or use meta-learning with hypernetworks to learn optimal parameter updates ____. However, both approaches require extensive model updates, which risks catastrophic forgetting on non-target tasks.

%Meta-Learning, exemplified by MEND ____ and KE ____, employs hypernetworks to learn optimal parameter updates for knowledge modification while preserving model stability.
%Both fine-tuning and meta-learning require extensive model updates, risking catastrophic forgetting on non-target tasks.

Direct editing refers to directly locating and editing the knowledge-related parameters. Research indicate that factual knowledge is primarily stored in the MLP modules of transformer-based architectures ____.
Leveraging these insights, model-editing methods like ROME ____ employ causal tracing to identify and edit the parameters encoding the particular knowledge. However, ROME is limited to single-instance knowledge editing, restricting its applicability in scenarios requiring large-scale updates. MEMIT extends the approach to support batch knowledge editing, providing a scalable solution for efficient and precise modifications ____. 
%By targeting a minimal set of relevant parameters, direct-edit achieves higher precision and reduce interference with the model’s overall performance compared to indirect model editing.

\subsection{Existing Defense to Jailbreak Attacks}
Recent studies reveal that jailbreak attacks ____ can bypass security alignment leading LLMs to generate harmful or unethical outputs.
As countermeasures, various defense methods are developed against such threats.
Existing defenses can be categorized into active defenses and passive defenses. Active defense enhances LLMs robustness against adversarial prompting by dynamically altering model parameters ____. A common approach to safety training involves constructing safety-relevant datasets and fine-tuning the model ____.
Instead, passive defense aims to build auxiliary modules or use external safety methods including input and output filtering ____, input smoothing, sanitation and modification ____.


%1) System-level defenses do not alter the LLM itself, but rather add external safety measures on top of the LLM.
%These include input and output filtering via perplexity filter ____, input smoothing ____, input sanitization ____ and modification ____;.
%2)Model-level defenses alter the LLM parameters to reduce the risk of malicious use and improve robustness to adversarial prompting. Safety training is commonly approached via fine-tuning methods such as RLHF ____ and DPO ____.

\subsection{Model Editing as a Jailbreak Defense}
Several studies have explored LLMs model editing as a defense mechanism to precisely modify toxic regions ____.
\textit{DINM} ____ and \textit{LED} ____ are motivated by indirect model editing method that fine-tuning the toxic layer using specific objectives. The difference between these two methods is the way of locating the toxic region. 
%DINM ____ treat the layer that most effectively separates the distributions of safe and unsafe responses  as the toxic layer, while LED
%These approaches emphasize the crucial role of identifying toxic region locations. DINM ____, given safe sequence $Y_{\text{safe}}$ and unsafe sequence $Y_{\text{unsafe}}$ as input consider the toxic layer to be the transformer layer that most effectively separates the distributions of them.
%LED ____ identifies safety layers by selecting the top-k layers whose removal most frequently transforms safe outputs to unsafe ones during iterative pruning.
The layer-level localization and fine-tuning approaches lack precision in identifying harmful words while potentially compromising the model's general performance.
In contrast, we propose to adapt direct-edit as a jailbreak defense in LLMs. 
%can be applied to only those parameters tied to harmful tokens. This targeted approach aims to suppress problematic outputs without broadly disrupting the model’s behavior, thereby preserving performance on other tasks.
%To the best of our knowledge, this is the first work to employ a direct-editing approach for defense.