\section{$k$-Additive Approximation Approach}
\label{sec:proposal}

In this section, we present our method \emph{SVA}$k_{\text{ADD}}$ to approximate Shapley values.
It builds upon the idea of adjusting a $k$-additive surrogate game $(N,\nu_k)$ to randomly sampled and evaluated coalitions.
Having fitted the surrogate game to represent the observed coalition values with minimal error, its own Shapley values $\phi^k$ can be interpreted as estimates $\hat\phi$ for %the true values
$\phi$ of $(N,\nu)$ since the fitting promises $\nu_k$ to be close to $\nu$.
See \cref{fig:kAdd} for an illustration of the approach.

\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\includegraphics[width=0.98\textwidth]{figures/kAdd.pdf}
\caption{
The sampled coalition values $\nu(A_1),\ldots,\nu(A_T)$ from the given game $(N,\nu)$ are used to fit a $k$-additive surrogate game $(N,\nu_k)$ in polynomial time.
The Shapley values $\phi_1^k,\ldots,\phi_n^k$ of $(N,\nu_k)$ are obtained immediately from its $k$-additive representation.
Since $\nu_k$ approximates $\nu$, these serve as estimates of the true Shapley values $\phi_1,\ldots,\phi_n$ of $(N,\nu)$.
}
\label{fig:kAdd}
\end{center}
\vskip -0.2in
\end{figure*}

\subsection{The $k$-Additive Optimization Problem}
\label{subsec:optimization}

We leverage the representation of $\nu_k$ by means of interactions as given in \cref{eq:iitomu_s}.
In particular, since $\nu_k$ is supposed to be $k$-additive, we specify $\nu_k$ as a linear transformation of interactions $I^k(B)$ for all $B \subseteq N$ of size $|B| \leq k$, allowing us to drop interactions of higher order than $k$: 
\begin{equation}
    \nu_k(A) = \sum_{\substack{B \subseteq N \\ |B| \leq k}} \gamma^{\left|B \right|}_{\left| A \cap B \right|} I^k(B) \, .
\end{equation}
Note that, given this representation, the Shapley values $\phi^k$ of the resulting game $(N,\nu_k)$ are obtained immediately by the interactions $I_i^k = \phi_i^k$, which will serve as estimates for the Shapley values $\phi$ of the game $(N,\nu)$, i.e.\ $I_i^k \approx \phi_i$.
The $k$-additive representation of $\nu_k$ comes with the advantage that the number of parameters $I^k(B)$ needed to define the surrogate game is reduced (as several parameters are set to zero).
The drawback of this strategy is the reduction in flexibility left to model the observed game $(N,\nu)$ according to the obtained evaluations.
However, we can still model interactions for coalitions up to $k$ players.
Empirically, works in the literature~\citep{Grabisch2002,Grabisch2006,Pelegrina2020,Pelegrina.2023} have been using $2$-additive or even $3$-additive games and obtained satisfactory results for modeling interactions.
Our goal is to fit $\nu_k$ as close as possible to $\nu$ and we therefore minimize the following expression, capturing by how much $\nu_k$ deviates from $\nu$:
\begin{equation}
    \sum_{A \in \mathcal{P}(N) \setminus \{\emptyset, N\}} w_A \left( \nu(A) - \nu_k(A) \right)^2 \, ,
\end{equation}
where $w_A$ is an importance weight associated to each coalition $A$.
We are eager to meet the desirable efficiency axiom such that the difference between and $\nu(N)$ and $\nu(\emptyset)$ is decomposed among the players within our approximated values $\phi^k$.
This is ensured by imposing the constraint $\nu(N) - \nu(\emptyset) = \nu_k(N) - \nu_k(\emptyset)$.
Hence, we arrive at the following optimization problem.
\begin{definition} \label{def:optimization}
    Given a cooperative game $(N, \nu)$, a degree of $k$-additivity $k \in \mathbb{N}$ with $k \leq n$, and weights $w_A \in \mathbb{R}$ associated with each coalition $A \subseteq N$, the $k$-additive optimization problem is given by the following constrained weighted least square optimization problem:
    \begin{equation*}
        \begin{array}{rl}
            \displaystyle\min_{I^k} & \sum\limits_{A \in \mathcal{P}(N) \setminus \left\{\emptyset, N \right\}} w_A \left( \nu(A) - \sum\limits_{\substack{B \subseteq N \\ |B| \leq k}} \gamma^{\left|B \right|}_{\left| A \cap B \right|} I^k(B) \right)^2 \\
            \text{s.t.} & \nu(N) - \nu(\emptyset) \ = \sum\limits_{\substack{B \subseteq N \\ |B| \leq k}} \left( \gamma^{\left|B \right|}_{\left| B \right|} - \gamma^{\left| B \right|}_0 \right) I^k(B)
        \end{array}
    \end{equation*}
\end{definition}
Solving the $k$-additive optimization is at the core of our approach.
In the remainder we describe how to overcome two key challenges.
First, we address in \cref{subsec:theory} how to choose the weights $w_A$ such that $\phi^k$ comes close to $\phi$.
Second, as the objective function sums up over exponential many coalitions, we present in \cref{subsec:sampling} our algorithm \emph{SVA}$k_{\text{ADD}}$ that constructs an approximative objective function by sampling coalitions and adding their error terms.

\subsection{Theoretical Soundness through Choice of Weights}
\label{subsec:theory}

Seeking precise estimates $\phi^k \approx \phi$, one may even raise the question if it is feasible to retrieve the exact Shapley values $\phi$ from the solution $I^k$ and how the weights $w_A$ have to be set to achieve this.
We analytically derive the correct weights and positively answer this question.
\begin{theorem} \label{the:solution}
    The solution to the $k$-additive optimization problem of any cooperative game $(N,\nu)$ for the cases of $k=1$, $k=2$, and $k=3$ with weights $w_A^* = \binom{n-2}{|A|-1}^{-1}$ yields the Shapley value, i.e.\
    \begin{equation*}
        I_i^k = \phi_i \, .
    \end{equation*}
\end{theorem}
See \cref{app:analysis} for the proof of \cref{the:solution}.
Note that these weights coincide with those derived by \citet{Charnes.1988} used in \citep{Lundberg.2017} for a different optimization problem.
The result implies that having observed the cooperative game $(N,\nu)$ in its entirety with all coalitions contained, our approach yields the exact Shapley values with no approximation error.
We interpret this as evidence for the soundness and theoretical foundation of our method.
Moreover, since the result holds irregardless of the shape of $\nu$, the game can even highly deviate from being $k$-additive and our estimates will still converge to its Shapley value.
Hence, $k$-additivity is not an assumption that our method requires but rather a tool to be leveraged.

We conjecture that \cref{the:solution} holds also true for arbitrary degrees of $k$-additivity and leave the proof  for future work due to the analytical challenge it poses.
Worth mentioning is that the hardness of incorporating Shapley interactions of higher degree into weighted least squares optimizations has already been acknowledged by \citet{Fumagalli.2024}.

\subsection{Approximating the $k$-Additive Optimization Problem via Sampling}
\label{subsec:sampling}

Computing the solution to the $k$-additive optimization problem (see \cref{def:optimization}) is practically infeasible since the objective compromises exponential many error terms w.r.t.\ $n$.
As a remedy we follow the same strategy as adopted in~\citep{Lundberg.2017,Pelegrina.2023} and approximate the objective function by sampling coalitions without replacement.
Let $\mathcal{M} = \{A_1,\ldots,A_T\}$ be the set of sampled coalitions with $A_i \neq A_j$ for all $i \neq j$ and the sequence $\nu_{\mathcal{M}} = (\nu(A_1),\ldots,\nu(A_T))$ representing its evaluated coalition values.
Thus, we solve the following optimization problem after sampling:
\begin{equation}
\label{eq:opt_kadd}
\begin{array}{rl}
\displaystyle\min_{I^k} & \sum\limits_{A \in \mathcal{M}\backslash \left\{\emptyset, N \right\}} w_{A} \left( \nu(A) - \sum\limits_{\substack{B \subseteq N \\ |B| \leq k}} \gamma^{\left|B \right|}_{\left| A \cap B \right|}I^k(B) \right)^2 \\
 \text{s.t.} & \nu(N) - \nu(\emptyset) = \sum\limits_{\substack{B \subseteq N \\ |B| \leq k}} \left( \gamma^{\left|B \right|}_{\left| B \right|} - \gamma^{\left|B \right|}_0 \right) I^k(B)
\end{array}
\end{equation}
To ensure the efficiency constraint, we force the sampling of $\emptyset$ and $N$.
Each coalition $A \in \mathcal{P}(N) \setminus \{\emptyset, N\}$ is drawn according to an initial probability distribution $p$ defined by
\begin{equation}
    p_A = \frac{w_A^*}{\sum\nolimits_{B \in N \setminus \{\emptyset, N\}} w_B^*} \, .
\end{equation}
After drawing a coalition $A$, we set $p_A$ to zero and normalize the remaining probabilities.
This procedure is repeated until $\left| \mathcal{M} \right| = T$. 
Algorithm~\ref{alg:proposal} presents the pseudo-code of \emph{SVA}$k_{\text{ADD}}$.
The algorithm requires the game $(N,\nu)$, the additivity degree $k$, and the budget $T$.
It starts by evaluating $\nu(\emptyset)$ and $\nu(N)$.
Thereafter, based on the (normalized) distribution $p$, it samples $T-2$ coalitions from $\mathcal{P}(N)$, evaluates each, and extends $\mathcal{M}$ as well as $\nu_{\mathcal{M}}$.
Finally, it solves the optimization problem in \cref{eq:opt_kadd} with weights $w_A^*$ given by \cref{the:solution} (see \cref{app:solution} for an analytical solution).
The extracted Shapley values $\phi^k$ of $\nu_k$ are returned as estimates $\hat\phi$ for the Shapley values $\phi$ of $(N,\nu)$.

\begin{algorithm}[ht]
    \caption{\emph{SVA}$k_{\text{ADD}}$}
    \label{alg:proposal}
		\begin{algorithmic}[1]
			\STATE \textbf{Input:} $(N,\nu)$, $k$, $T$		
            \STATE $\mathcal{M} \leftarrow \{\emptyset, N\}$
            \STATE $\nu_{\mathcal{M}} \leftarrow (\nu(\emptyset), \nu(N))$
            \WHILE{$\left| \mathcal{M} \right| < T$}
                \STATE Sample a coalition $A \in \mathcal{P}(N) \setminus \{\emptyset, N\}$ from normalized distribution $p$
                \STATE $\mathcal{M} \leftarrow \mathcal{M} \cup \{A\}$
                \STATE $\nu_{\mathcal{M}} \leftarrow (\nu_{\mathcal{M}},\nu(A))$
                \STATE $p_A \leftarrow 0$
            \ENDWHILE
            \STATE $(I^k(B))_{B \subseteq N : |B| \leq k} \leftarrow \textsc{\texttt{Solve}}(\mathcal{M},\nu_{\mathcal{M}},k)$ 
            \STATE \textbf{Output:} $I_1^k,\ldots,I_n^k$
    \end{algorithmic}
\end{algorithm}

We would like to emphasize that \cref{the:solution} does not make a statement about the obtained solution during sampling when not all coalitions are observed.
To the best of our knowledge, and it is also well-known, there exists no approximation guarantee for methods that estimate the Shapley value by means of a weighted least squares optimization problem.
The difficulty of obtaining a theoretical result is further elaborated by \cite{Covert.2021}.