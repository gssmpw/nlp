\section{Introduction}
\label{sec:intr}

The complexity of applied machine learning models experienced a rapid and certainly significant increase over the last decade.
On the contrary, this development comes with an ever-rising burden to understand a model's decision-making, reaching a point at which the inner workings are beyond human comprehension. %, and fittingly coining the term 'black box model'.
Meanwhile, societal and political influences led to a growing demand for trustworthy AI~\citep{Li2023}.
The field of Explainable AI (XAI) emerges to counteract these consequences, aiming to bring back understanding to the human user and developer.
Among the various explanation types~\citep{Molnar2021}, post-hoc additive explanations convince with an intuitive appeal: an observed numerical effect caused by the behavior of the black box model is divided among participating entities.
Additive feature explanations decompose a predicted value for a particular datapoint \cite{Lundberg.2017} or generalization performance on a test set \citep{Covert.2020} among the involved features, enabling feature importance scores.
Beyond explainability, this allows in feature engineering to conduct feature selection by removing features with irrelevant or even harmful contributions \citep{Cohen.2005, Marcilio2020}.

Treating this decomposition as a fair division problem opens the door to game theory which views the features as cooperating agents, forming groups called coalitions to achieve a task and collect a common reward that is to be shared.
Such scenarios are captured by the widely applicable notion of cooperative games~\citep{Peleg2007}, modeling the agents as a set of players $N$ and assuming that a real-valued worth $\nu(A)$ can be assigned to each coalition $A \subseteq N$ by a value function $\nu$.
Among multiple propositions the Shapley value \citep{Shapley.1953} prevailed as the most favored solution to the fair division problem.
It assigns to each player a share of the collective benefit, more precisely a weighted average of all its marginal contributions, i.e., the increase in collective benefit a player causes when joining a coalition.
Its popularity is rooted in the fact that it is provably the only solution to fulfill certain desirable axioms \citep{Shapley.1953} which arguably capture a widespread understanding of fairness.
For example, in the context of supply chain cooperation~\citep{Fiestras-Janeiro2011}, the gain in reduction cost when joining a coalition may be shared among companies based on the Shapley value.
The greater a company's marginal contributions to the cost reduction, the greater its received payoff, measured by the Shapley value.

The applicability of the Shapley value exceeds by far the sphere of economics as its utility has been recognized by researchers of various disciplines.
Most prominently, it has recently found its way into the branch of machine learning, especially as a model-agnostic approach, quantifying the importance of entities such as features, datapoints, and even model components like neurons in networks or base learners in ensembles (see \citep{Rozemberczki.2022} for an overview).
Adopting the game-theoretic view, these entities are understood as players which cause a certain numerical outcome of interest.
Shaping the measure of a coalition's worth adequately is pivotal to the informativeness of the importance scores obtained by the Shapley values.
For example, considering a model's generalization performance on a test dataset
%(such as accuracy)
restricted to the feature subset given by a coalition yields global feature importance scores \citep{Pfannschmidt.2016, Covert.2020}.
Conversely, local feature attribution scores are obtained by splitting the model's prediction value for a fixed datapoint \citep{Lundberg.2017}.
The Shapley value's purpose is not limited to provide additive explanations since it has also been proposed to perform data valuation \citep{Ghorbani.2019}, feature selection \citep{Cohen.2007}, ensemble construction \citep{Rozemberczki.2021}, and the pruning of neural networks \citep{Ghorbani.2020}.
Moreover, it has been applied to extract feature importance scores in several recent practical applications, such as in risk management~\citep{Nimmy2023}, energy management~\citep{Cai2023}, sensor array (re)design~\citep{Pelegrina2023b} and power distribution systems~\citep{Ebrahimi2024}.

The uniqueness of the Shapley value comes at a price that poses an inherent drawback to practitioners: its computation scales exponentially with the number of players taking part in the cooperative game.
Consequently, it becomes due to NP-hardness \cite{Deng.1994} quickly infeasible for increasing feature numbers or even a few datapoints, especially when complex models are in use whose evaluation is highly resource consuming.
As a viable remedy it is common practice to approximate the Shapley value while providing reliably precise estimates is crucial to obtain meaningful importance scores.
On this background, the recently sharp increase in attention that XAI attracted, has rapidly fueled the research on approximation algorithms, leading to a diverse landscape of approaches (see \citep{Chen.2023}) for an overview related to feature attribution).

\paragraph{Contribution.}
We propose with \emph{SVA}$k_{\text{ADD}}$ (Shapley Value Approximation under $k$-additivity) a novel approximation method for the Shapley value based on the concept of $k$-additive games whose structure elicits a denser parameterizable value function.
Fitting a $k$-additive surrogate game to randomly sampled coalition-value pairs comes with a twofold benefit.
First, it reduces flexibility, promising faster convergence and second, the Shapley values of the $k$-additive surrogate game are obtained immediately from its representation.
In summary, our contributions are:

\begin{itemize}
    \item[\textbf{(i)}] \emph{SVA}$k_{\text{ADD}}$ fits a $k$-additive surrogate game to sampled coalitions, trying to 
    mimick the given game by a simpler structure with a parameterizable degree of freedom while maintaining low representation error.
    The surrogate game's own Shapley values are obtained immediately due to its structure and yield precise estimates for the given game if the representation exhibits a good fit.

    \item[\textbf{(ii)}] \emph{SVA}$k_{\text{ADD}}$ does not require any structural properties of the value function.
    Thus, it is domain-independent and can be applied to any cooperative game oblivious to what players and payoffs represent.
    Specifically in the field of explainability, it is model-agnostic and can approximate local as well as global explanations.

    \item[\textbf{(iii)}] We prove the theoretical soundness of \emph{SVA}$k_{\text{ADD}}$ by showing analytically that its underlying optimization problem yields the Shapley value.
   
    \item[\textbf{(iv)}] We empirically compare \emph{SVA}$k_{\text{ADD}}$ to competitive baselines at the hand of various explanation tasks, and shed light onto the best fitting degree of $k$-additivity.
\end{itemize}

The remainder of this paper is organized as follows.
We describe existing works related to this paper in Section~\ref{sec:related_work}.
Section~\ref{sec:theory} introduces the theoretical background behind our proposal.
In Section~\ref{sec:proposal}, we present our novel approximation method.
We conduct experiments for several real-world datasets in Section~\ref{sec:exper}.
Finally, in Section~\ref{sec:conclusion}, we conclude our findings and highlight directions for future works.