\section{Cooperative Games Details} \label{app:cooperative_games}

The cooperative games used within our conducted experiments are based on explanation examples for real world data.
This section complete their brief description given in \cref{sec:exper}.
Across all cooperative games the players represent a fixed set of features given by a particular dataset.

\subsection{Global feature importance}

Seeking to quantify each feature's individual importance to a model's predictive performance, the value function is based on the model's performance of a hold out test set.
This necessitates to split the dataset at hand into training and test set.
Features outside of an inspected coalition $S$ are removed by retraining the model on the training set and measuring its performance on the test set.
For all games we a applied train-test split of 70\% to 30\% and a random forest consisting of 20 trees.
For classification the value function maps each coalition to the model's resulting accuracy on the test set minus the accuracy of the mode within the data such that the empty coalition has a value of zero.
For regression tasks the worth of a coalition is the reduction of the model's mean squared error compared to the empty set which is given by the mean prediction.
Again, the empty coalition has a value of zero.

\subsection{Local feature attribution}

Instead of assessing each feature's contribution to the predictive performance, its influence on a model's prediction for a fixed datapoint can also be investigated.
Hence, the value function is based on the model's predicted value.

\subsubsection{Adult classification}
A sklearn gradient-boosted tree classifies whether a person's annual salary exceeds 50,000 in the \emph{Adult} tabular dataset containing 14 features.
The predicted class probability of the true class is taken as the worth of a coalition $S$.
In order to render features outside of $S$ absent, these are imputed by their mean value such that the datapoint is compatible to the model's expected feature number.

\subsubsection{Image classification}

A \emph{ResNet18} model is used to classify images from \emph{ImageNet}.
Since the for error tracking necessary exact computation of Shapley values is infeasible for the given number of pixels, 14 semantic segments are formed after applying \emph{SLIC}.
These super-pixels form the player set.
Given that the model predicts class $c$ using the full image, the value function assigns to each coalition $S$ the predicted class probability of $c$ resulting from only including those super-pixels in $S$.
The other super-pixels are removed by mean imputation, setting them grey.

\subsubsection{IMDB sentiment analysis}

A \emph{DistilBERT} transformer  fine-tuned on the \emph{IMDB} dataset predicts the sentiment of a natural language sentence between -1 and 1.
The sentence is transformed into a sequence of tokens.
The input sentences are restricted to sentences that result in 14 tokens being represented by players of the cooperative game.
This allows to remove players in the tokenized representation of the transformer.
The predicted sentiment is taken as the worth of a coalition.


\subsection{Unsupervised feature importance}

In contrast to the previous settings, there is no available predictive model to investigate unlabeled data.
Still, each feature's contribution to the shared information within the data can be quantified and assigned as a score.
\citep{Balestra.2022} proposed to view the features $1,\ldots,n$ as random variables $X_1,\ldots,X_n$ such that the datapoints are realizations of their joint distribution.
Next, the worth of a coalition $S$ is given by their total correlation
\begin{equation*}
    \nu(S) = \sum\limits_{i \in S} H(X_i) - H(S)
\end{equation*}
where $H(X_i)$ denotes the Shannon entropy of $X_i$ and $H(S)$ the contained random variables joint Shannon entropy.
The utilized datasets are reduced in the number of features and datapoints to ease computation.
The \emph{Breast cancer} dataset contains 9 features and 286 datapoints.
The class label indicating the diagnosis is removed.
From the \emph{Big five} and \emph{FIFA 21} dataset 12 random features are selected out of the first 50 and the datapoints are reduced to the first 10,000.