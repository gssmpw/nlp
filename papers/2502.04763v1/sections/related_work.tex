\section{Related Work}
\label{sec:related_work}

The problem of approximating the Shapley value, and the recent interest it attracted from various communities, lead to a multitude of diverse approaches to overcome its complexity.
First to mention among the class of methods that can handle arbitrary games, without further assumptions on the structure of the value function, are those which construct mean estimates via random sampling.
Fittingly, the Shapley value of each player can be interpreted as the expected marginal contribution to a specific probability distribution over coalitions.
\citet{Castro.2009} propose with \emph{ApproShapley} the sampling of permutations from which marginal contributions are extracted.
Further works, following this paradigm, employ stratification by coalition size \citep{Maleki.2013,Castro.2017,vanCampen.2018, Okhrati.2020}, or utilize reproducing kernel Hilbert spaces \citep{Mitchell.2022}.
Departing from marginal contributions, \emph{Stratified SVARM} \citep{Kolpaczki.2024a} splits the Shapley value into multiple means of coalition values and updates the corresponding estimates with each sampled coalition, being further refined by \emph{Adaptive SVARM} \citep{Kolpaczki.2024b}.
Guided by a different representation of the Shapley value, \emph{KernelSHAP} \citep{Lundberg.2017} solves an approximated weighted least squares problem, to which the Shapley value is its solution.
\citet{Fumagalli.2023} prove its variant \emph{Unbiased KernelSHAP} \cite{Covert.2021} to be equivalent to importance sampling of single coalitions.
Joining this family, \cite{Pelegrina.2023} propose $k_{ADD}$-SHAP, which consists in a local explainability strategy that formulates the surrogate model assuming a $k$-additive game\footnote{
    Note that $k_{ADD}$-SHAP is limited to local explanations.
    In contrast, our proposed method \emph{SVA}$k_{\text{ADD}}$ differs by its applicability to any formulation of a cooperative game.
    Moreover, in the context of explainable AI, it is capable of providing global explanations.
}.
The authors locally adopt the Choquet integral as the interpretable model, whose parameters have a straightforward connection with the Shapley value.

On the contrary, tailoring the approximation to a specific application of interest by leveraging structural properties promises faster converging estimates.
In data valuation, including knowledge of how datapoints tend to contribute to a learning algorithm's performance has proven to be a fruitful, resulting in multiple tailored approximation methods \cite{Ghorbani.2019, Jia.2019a, Jia.2019b}.
In similar fashion \citet{Liben-Nowell.2012} leverage supermodularity in cooperative games.
Even further, value functions of certain parameterized shapes facilitate closed-form polynomial solutions of the Shapley value w.r.t.\ the number of involved players.
Examples include the voting game \citep{Bilbao.2000} and the minimum cost spanning tree games \citep{Granot.2002} being used in operations research.

Besides the Shapley value's prominence for explaining the decision-making of a model, it has also found its way to more applied tasks.
For instance, \citet{Nimmy2023} use it to quantify each feature's impact in predicting the risk degree in managing industrial machine maintenance,
\citet{Pelegrina2023b} apply it to evaluate the influence of each electrode on the quality of recovered fetal electrocardiograms, and \citet{Brusa2023} measure the features' importance towards machinery fault detection.
Worth mentioning, each application requires an appropriate modeling in terms of player set and value function in order to obtain meaningful explanations.
Moreover, Shapley values can be useful in feature engineering to perform feature selection.
For instance, features with low relevance towards the model performance may be removed from the dataset without an impact onto the prediction quality~\citep{Pelegrina2024}.