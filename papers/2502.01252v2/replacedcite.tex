\section{Related Work}
% related works可以并不长，主要是要分明一些，每个paragraph都恰到好处。

% ____ centers on symmetric provision games and appropriation games whose results were used to analyze asymmetric games further.____ proposes a new theory to decompose one asymmetric game into two independent symmetric games. ____ extended the meta-game analysis from symmetric to asymmetric games.

\textbf{Symmetric and asymmetric games.} Symmetric games are initially proposed by ____ and studied under the context of non-cooperative ____, economic ____, and two-person ____ games. ____ focus on pure strategy equilibrium with supermodular payoff functions. ____ studies symmetric games only with asymmetric equilibria. A few studies extend the theories on symmetric games to asymmetric settings ____, or transform asymmetric games to symmetric ones ____. These studies usually concentrate on a specific type of classic game such as metric games or poker.
____ and ____ are among the first to consider asymmetric games involving a combinatorial player in a variant of traveling salesman problem with multiple vehicles. ____ studies a security game where the defender decides the location of sources and the attacker chooses a path to find the source. ____ extends the idea of ____ and consider discrete time domains and moving targets. In the covering problem, ____ considers the failure of some nodes and models it as one zero-sum game. All of these studies are limited to finite games.

% In these game settings, all rely on the existence of equilibrium in the finite game. 

% \hp{DO is not mentioned?}

\textbf{Equilibrium learning in zero-sum games.} DO ____ is a powerful tool for solving complex strategic normal-form games by iteratively expanding the players' strategy sets and efficiently finding equilibria. The idea has been extended toward better NE computation, different forms of games, convergence rate, etc. ____ and ____ focus on accelerating the computation of the approximate equilibrium. Different diversity metrics are proposed by ____ to find more effective and various strategies. In extensive-form games, ____ works to achieve linear convergence to approximate equilibrium and ____ studies sample complexity. Except for DO and its variants, NE learning in zero-sum settings remains appealing in periodic games ____, polymatrix games ____, and Markov games ____, etc. As far as we know, they are all limited to matrix games in theories related to the existence and convergence of NE although ____ conduct experiments on continuous-action games by Deep RL. ____ study the NE convergence of continuous games but two players are symmetric.

%To solve the equilibrium in the zero-sum game ____, a large amount of research focuses on this and its branches like computational efficiency, strategy diversity in the normal game, and finding NE in the extensive game, or continuous game. 
  %DO ____ is a powerful tool for solving complex strategic normal-form games by iteratively expanding the players' strategy sets and efficiently finding equilibria. The idea has been extended toward better NE computation, different forms of games, convergence rate, etc. ____ and ____ focus on accelerating the computation of the approximate equilibrium. Different diversity metrics are proposed by ____ to find more effective and various strategies. ____ and ____ consider extensive-form games. ____ works to achieve linear convergence to approximate equilibrium. ____ studies sample complexity. \textcolor{blue}{Except for DO and its variants, NE learning in zero-sum settings remains appealing in periodic games ____, polymatrix games ____, and Markov games ____, etc.} As far as we know, they are all limited to matrix games, especially in theories related to the existence and convergence of NE. ____ study the NE convergence of continuous games but two players are symmetric.

\textbf{RL for COPs.} RL has emerged as an effective and generalizable method to solve COPs, where the underlying idea is to decompose the original combinatorial action selection in COPs into a sequence of greedily selected individual actions, using a deep RL policy or value function that is usually represented via various function approximation methods such as graph neural networks ____, recurrent ____, and attention networks ____. Search algorithms, such as active search ____, Monte Carlo tree search ____, and multiple rollouts ____, are further integrated into these frameworks to enhance the solution qualities of RL algorithms during inference time. Integrating representation learning and search algorithms, RL has shown promising abilities to learn efficient and generalizable solutions to complex COPs. This motivates us to adopt RL as the backbone method to compute the COPs in a subgame of the ACCESS games.

%Through appropriate neural networks and additional techniques mentioned above, the solution solved by RL surpasses approximated and heuristic algorithms and even draws near the exact solution. Hence, incorporating RL algorithms as the best response for combinatorial players to accelerate equilibrium computation is both reasonable and effective.


%____