% \section{Combinatorial-Continuous Double Oracle (CCDO)}
\section{CCDO \& CCDO-RL}

In this section, we introduce the Combinatorial-Continuous Double Oracle (CCDO) algorithm and prove its convergence in Section \ref{sec_alg}, and propose the practical version of CCDO, CCDO-RL with the convergence analysis in Sections \ref{sec_ccdorl} and \ref{secCCDOA_prf} respectively. CCDO has a similar algorithmic framework to DO but differs significantly in the convergence analysis. Moreover, we consider one phenomenon that never occurs in DO but is common in the COPs: the performance of the approximate best response (ABR) to another player's mixed policy is even worse than that of NE. To handle this phenomenon, we design a CCDO approximate (CCDOA) algorithm, and further propose CCDO-RL (Algorithm \ref{alg:CCDORL}), where we use RL as the underlying oracle solver for both players in the CCDOA framework inspired by recent advancements in using RL to solve COPs. We provide a further convergence analysis on CCDO-RL, and examine how different ABRs influence convergence. 

% In this section, we introduce the Combinatorial-Continuous Double Oracle (CCDO) algorithm and prove its convergence in Section \ref{sec_alg}, and propose the practical version of CCDO, CCDO-RL with the convergence analysis in Sections \ref{sec_ccdorl} and \ref{secCCDOA_prf} respectively. CCDO has a similar algorithmic framework to DO but differs significantly in the convergence analysis. Moreover, we consider one phenomenon that never occurs in DO but is common in the COPs: the performance of the approximate best response (ABR) to another player's mixed policy is even worse than that of NE. To handle this phenomenon, we design a CCDO approximate (CCDOA) algorithm, provide a further convergence analysis, and examine how different ABRs influence convergence. Inspired by recent advancements in using RL to solve COPs, we then further propose CCDO-RL (Algorithm \ref{alg:CCDORL}), in which we use RL as the underlying oracle solver for both players in the CCDOA framework. 

%Considering the effective solving in COPs, RL is the priority for the approximate best response. Based on CCDOA, CCDO-RL is proposed to solve the specific ACCES games on the CO field.


\subsection{CCDO and its Convergence}\label{sec_alg}

DO is originally proposed to solve NE in the large zero-sum matrix games \citep{mcmahan2003planning}. The key idea is to iteratively compute the mixed NE in the subgame and expand the subgame by the best response (BR) to the current NE of the subgame. We adopt the same algorithmic framework and propose CCDO, to solve the NE in ACCES games (see Algorithm \ref{alg:CCDO}). The difference with DO and its variants ODO/XDO is the stopping criterion, Line 6 in Algorithm \ref{alg:CCDO}. The original part in DO is subgame sets both remain unexpanded, i.e. $X_{k+1} = X_k, Y_{k+1} = Y_k$ which naturally holds on finite games but cannot be possibly guaranteed in ACCES games, even if iterating infinite times because of $Y$'s infiniteness.

We should not ignore that DO and its variants ODO/XDO can only guarantee convergence under a finite action space because the subgame can become the original game by adding the best response over a finite number of iterations. The infinity of the strategy space not only invalidates this guarantee but also fundamentally alters the structure of convergence analysis. Besides, the two players need to be analyzed separately in the proof because of the asymmetry of ACCES games.

The convergence guarantee of CCDO applies to a broader range of zero-sum games, not only in matrix games but also in the ACCES game. In other words, CCDO is the extensive version of DO. First, we prove the convergence of CCDO, providing the basis for the subsequent convergent proof of CCDOA and CCDO-RL. The full proof is provided in Appendix \ref{Appendix_B}.

%The difference with DO is the stopping criterion, Line 6 in Algorithm \ref{alg:CCDO}. The original part in DO is subgame sets both remain unexpanded, i.e. $X_{k+1} = X_k, Y_{k+1} = Y_k$ which naturally holds on finite games but cannot be possibly guaranteed in ACCES games, even if iterating infinite times because of $Y$'s infiniteness. 

%We should not ignore that DO and its variants can only guarantee convergence under a finite action space because the subgame can become the original game by adding the best response over a finite number of iterations. However, the infinity of the strategy space not only invalidates this guarantee but also fundamentally alters the structure of convergence analysis. Besides, the two players need to be analyzed separately in the proof because of the asymmetry of ACCES games. First, we prove the convergence of CCDO, providing the basis for the subsequent convergent proof of CCDOA and CCDO-RL. The full proof is provided in Appendix \ref{Appendix_B}. %From Theorem \ref{CCDO}, we can find the similarity with continuous games, i.e. iterating infinitely to find NE in the subsequence of subgame NE pairs. If we only need approximate NE, the algorithm can stop in finite iterations.
\begin{theorem}\label{CCDO}
    Given a two-player ACCESS game $\mathcal{G} = (X, Y, u)$, where $X$ is finite, $Y$ is a nonempty compact set, and the utility function $u$ is continuous in $Y$ when fixing the strategy in $X$, we have
    
    1. When $\epsilon = 0$, every weakly convergent subsequence in the subgame equilibrium sequence $\{(p_k^*, q_k^*)\}$ converges to the equilibrium of the whole game, possibly in an infinite number of iterations. 
    
    2. When $\epsilon > 0$, Algorithm \ref{alg:CCDO} converges to an $\epsilon$- equilibrium in a finite number of epochs.
\end{theorem}

\subsection{CCDOA and CCDORL}\label{sec_ccdorl}

Due to the NP-hardness of most COPs, finding the exact BR for the combinatorial player is computationally impractical. Hence, we use a more practical approximate version, CCDOA (Algorithm \ref{alg:CCDOA} of Appendix \ref{Appendix_B}), to solve the approximate NE. However, the approximation of BR may cause circumstances where the utility of the approximate best response is lower than that of NE in the subgame which never happens in CCDO. This issue not only has a tricky effect on the convergence analysis but adds computational overhead to solving NE and the memory burden of saving strategies, and may even prolong the iteration round. To address this, two discriminants (lines 6-13 in Algorithm \ref{alg:CCDOA}) are added to guarantee the optimality of the two ABRs $x_{k+1}^{\epsilon_1}, y_{k+1}^{\epsilon_2}$ in the subgame $ \mathcal{G}_k = (X_k, Y_k, u)$. %both in CCDOA and the following more practical implementation algorithm CCDO-RL, which uses RL as the underlying approximate BR solver.

There have been studies using RL to learn a generalized policy for certain COPs in graphs \citep{khalil2017learning, nazari2018reinforcement, bengio2020machine, Ou2021rldis, Feng2025hrl4sco}. The key idea is to decompose the node selection into a sequence and learn a heuristic policy for sequentially choosing nodes. The RL policy is usually trained on seen training graphs with the hope of generalizing to unseen test graphs of similar characteristics. Such generalization has been further enhanced via graph embedding techniques such as Structure to Vector \citep{Dai2016s2v} and Graph Convolutional Networks \citep{kipf2016semi} as the underlying value/policy network. The adversary's task is to choose optimal parameters in COPs. RL would also be a useful method to enhance the adversary's generalizability and solvability for diverse instances of the CO problem.
Hence, we propose CCDO-RL (see Algorithm \ref{alg:CCDORL}), a practical implementation of CCDOA where we use RL and graph embedding techniques as the underlying method to find the approximate BR for each player (Line 4 of Algorithm \ref{alg:CCDORL}). The mixed NE is solved by the supported enumeration algorithm \citep{roughgarden2010algorithmic}, utilizing the Nashpy implementation \citep{knight2018nashpy}.

% As an effective and widely applicable algorithm, RL is designed and applied in various fields. Especially in COPs, RL not only outperforms the approximation and heuristic algorithms in performance but also has generalization ability on unseen graphs, which can demonstrate the reasonability to choose RL as the approximate best response. 


%Based on CCDOA, the approximate algorithm closer to reality, CCDO-RL is designed. Pseudocodes of CCDOA and CCDO-RL are described in Algorithm \ref{alg:CCDOA} and \ref{alg:CCDORL} in the appendix \ref{Pseudo}.

% Reinforcement learning has been widely demonstrated as a promising tool for COPs due to its ability to learn fast and generalizable solutions to unseen COP instances while maintaining close-to-optimal solution quality~\citep{khalil2017learning,nazari2018reinforcement,deudon2018learning,bengio2020machine,kool2018attention,berto2023rl4co}. Hence, we eventually propose CCDO-RL (see Algorithm~\ref{alg:CCDORL}), a practical implementation of CCDOA where we use RL as the underlying method to find the approximate BR for each player (Line 4 of Algorithm \ref{alg:CCDORL}). %\hp{Need to go over the Algorithm}

% \begin{algorithm}[htbp]
%     \caption{Combinatorial-Continuous Double Oracle Reinforcement Learning Algorithm}
%     \label{alg:CCDORL}
%     \renewcommand{\algorithmicrequire}{\textbf{Input:}}
%     \renewcommand{\algorithmicensure}{\textbf{Output:}}
%     \begin{algorithmic}[1]
%         \REQUIRE Game $\mathcal{G} = (X, Y, u)$, $\epsilon \geq 0$.  %%input
%         \STATE  Initialize strategy set $\Pi_{1,0}$, $\Pi_{2,0}$.
%         \REPEAT
%         \STATE Solve the mixed equilibrium $\sigma_k^*$ in the subgame $(\Pi_{1,k}, \Pi_{2,k})$.
%         \STATE Find the approximate best response by RL algorithms $(\pi_{1,k}^*, \pi_{2,k}^*)$ \\
%         \IF{$U(\pi_{1,k}^*, \sigma_{2,k}^*) \leq U(\sigma_k^*)$} 
%         \STATE $\Pi_{1,k+1} = \Pi_{1,k}$
%         \ELSE
%         \STATE $\Pi_{1,k+1} = \Pi_{1,k} \cup \{\pi_{1,k}^*\}$.
%         \ENDIF
%         \IF{$U(\sigma_{1,k}^*, \pi_{2,k}^*) \geq U(\sigma_k^*)$}
%         \STATE $\Pi_{2,k+1} = \Pi_{2,k}$
%         \ELSE
%         \STATE $\Pi_{2,k+1} = \Pi_{2,k} \cup \{\pi_{2,k}^*\}$.
%         \ENDIF
%         \UNTIL{$U(\sigma_{1,k}^*, \pi_{2,k}^*) - U(\sigma_{1,k}^*, \pi_{2,k}^*)) \leq \epsilon$}
%         \ENSURE $\sigma_k^*$   %%output
%     \end{algorithmic}
% \end{algorithm}

\begin{algorithm}[ht]
    \caption{Combinatorial-Continuous Double Oracle Reinforcement Learning Algorithm}
    \label{alg:CCDORL}
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
    \renewcommand{\algorithmicensure}{\textbf{Output:}}
    \begin{algorithmic}[1]
        \REQUIRE Game $\mathcal{G} = (X, Y, u)$, $\epsilon \geq 0$.  %%input
        \ENSURE $\sigma_k^*$.   %%output
        \STATE  Initialize strategy set $\Pi_{1,0}$, $\Pi_{2,0}$.
        \REPEAT
            \STATE Solve the mixed equilibrium $\sigma_k^*$ in the subgame $(\Pi_{1,k}, \Pi_{2,k})$.
            \STATE Find the approximate best response by RL algorithms $(\pi_{1,k}^*, \pi_{2,k}^*)$. \\
            \STATE $\Pi_{1,k+1} = \Pi_{1,k} \cup \{\pi_{1,k}^*\}$, $\Pi_{2,k+1} = \Pi_{2,k} \cup \{\pi_{2,k}^*\}$.
            \IF{$U(\pi_{1,k}^*, \sigma_{2,k}^*) \leq U(\sigma_k^*)$} 
                \STATE $\Pi_{1,k+1} = \Pi_{1,k}$. 
            \ELSIF{$U(\sigma_{1,k}^*, \pi_{2,k}^*) \geq U(\sigma_k^*)$} 
                \STATE $\Pi_{2,k+1} = \Pi_{2,k}$.
            \ENDIF
        \UNTIL{$U(\pi_{1,k}^*, \sigma_{2,k}^*) - U(\sigma_{1,k}^*, \pi_{2,k}^*)) \leq \epsilon$}
    \end{algorithmic}
\end{algorithm}
\vspace{-\baselineskip}
% \begin{algorithm}[htbp]
% 	\caption{Combinatorial-Continuous Double Oracle Reinforcement Learning Algorithm}
% 	\label{alg:CCDORL}
% 	\LinesNumbered
% 	\KwIn{Game $\mathcal{G} = (X, Y, u)$, $\epsilon \geq 0$.}
% 	\KwOut{$\sigma_k^*$}
%      Initialize strategy set $\Pi_{1,0}$, $\Pi_{2,0}$\\
% 	\Repeat{$U(\sigma_{1,k}^*, \pi_{2,k}^*) - U(\sigma_{1,k}^*, \pi_{2,k}^*)) \leq \epsilon$}{
%      Solve the mixed equilibrium $\sigma_k^*$ in the subgame $(\Pi_{1,k}, \Pi_{2,k})$.\\
%      Find the approximate best response by RL algorithms $(\pi_{1,k}^*, \pi_{2,k}^*)$ \\
%      $\Pi_{1,k+1} = \Pi_{1,k} \cup \{\pi_{1,k}^*\}$, $\Pi_{2,k+1} = \Pi_{2,k} \cup \{\pi_{2,k}^*\}$.\\
%      \uIf{$U(\pi_{1,k}^*, \sigma_{2,k}^*) \leq U(\sigma_k^*)$}{
%         $\Pi_{1,k+1} = \Pi_{1,k}$.
%     }
%     \uIf{$U(\sigma_{1,k}^*, \pi_{2,k}^*) \geq U(\sigma_k^*)$}{
%         $\Pi_{2,k+1} = \Pi_{2,k}$.
%     }
% }
% \end{algorithm}

\subsection{Convergence of CCDOA and CCDO-RL}\label{secCCDOA_prf}

Next, we consider the convergence guarantee of CCDOA and CCDO-RL with ABRs (full proof in Appendix \ref{Appendix_B}). Apart from the convergence analysis, the influence of ABRs with different degrees of approximation on the number of algorithm inner iterations is also explained further. Additionally, the computational complexity of CCDO-RL is provided in Theorem \ref{thm_complex} in Appendix \ref{Appendix_B}.

%Next, we consider the convergence of CCDOA and CCDO-RL with approximate best responses (full proof in Appendix \ref{Appendix_B}). Apart from the convergence analysis, the influence of different forms of approximate best responses on the number of algorithm inner iterations is also explained further. Additionally, the computational complexity of CCDO-RL is provided in Theorem \ref{thm_complex} in Appendix \ref{Appendix_B}.

\begin{theorem}\label{CCDOA}
    Given $\mathcal{G} = (X, Y, u)$, where $X$ is finite, $Y$ is a nonempty compact set, and the utility function $u$ is continuous in $Y$ when fixing the strategy in $X$, with $\epsilon_1$ best response oracle for Player 1 in $X$ and $\epsilon_2$ best response oracle for Player 2 in $Y$, we have
    
    1. When $\epsilon>0$, for any form of approximate best response oracles, CCDOA and CCDO-RL can converge to a finitely supported $(\epsilon+\epsilon_1+\epsilon_2)$- equilibrium in a finite number of iterations.

    2. When $\epsilon=0$, if the approximate response oracle for Player 2 has a uniform lower bound for every mixed strategy in $\bigtriangleup_{X}$, i.e. 
    \begin{eqnarray}\label{y_br_condi}
        \begin{aligned}
            \forall p \in \bigtriangleup_{X}, \exists \epsilon_Y, s.t. U(p, BR^{\epsilon_2}_{2}(p)) \geq \min_{y\in Y}U(p, y)+\epsilon_Y,
        \end{aligned}
    \end{eqnarray}
    then CCDOA and CCDO-RL must converge to an $(\epsilon+\epsilon_1+\epsilon_2)$- equilibrium in a finite iterations.

    3.  When $\epsilon=0$, if CCDOA and CCDO-RL produce infinite iterations, every weakly convergent subsequence converges to an $\epsilon_1$- equilibrium.
\end{theorem}

% \begin{theorem}\label{CCDOA}
%     Given $\mathcal{G} = (X, Y, u)$, where $X$ is finite, $Y$ is a nonempty compact set, and the utility function $u$ is continuous in $Y$ when fixing the strategy in $X$, with $\epsilon_1$ best response oracle for Player 1 in $X$ and $\epsilon_2$ best response oracle for Player 2 in $Y$, we have
    
%     1. If terminating in finite iterations, then CCDOA and CCDO-RL on game $\mathcal{G}$ will converge to $(\epsilon+\epsilon_1+\epsilon_2)$- equilibrium. 
%     Especially, if the best response oracle for Player 2 has a lower bound for every mixed strategy in $\bigtriangleup_{X}$, i.e. 
%     \begin{eqnarray}\label{y_br_condi}
%         \begin{aligned}
%             \forall p \in \bigtriangleup_{X}, \exists \epsilon_Y, s.t. U(p, BR^{\epsilon_2}_{2}(p)) \geq \min_{y\in Y}U(p, y)+\epsilon_Y,
%         \end{aligned}
%     \end{eqnarray}
%     then CCDOA and CCDO-RL must converge to an $(\epsilon+\epsilon_1+\epsilon_2)$- equilibrium in a finite iterations.

%     2.  When $\epsilon=0$, if CCDOA and CCDO-RL produce infinite iterations, every weakly convergent subsequence converges to an $\epsilon_1$- equilibrium.

%     3. When $\epsilon>0$, for any form of approximate best response oracles, CCDOA and CCDO-RL can converge to a finitely supported $(\epsilon+\epsilon_1+\epsilon_2)$- equilibrium in a finite number of iterations.
% \end{theorem}
% The proof is provided in Appendix \ref{Appendix_B}.

% 这一段要分析一下定理2和3
%As stated in Theorem \ref{CCDOA}, the convergence result when $\epsilon > 0$ is analogous to that in Theorem \ref{CCDO} Item 2. Both algorithms are capable of converging to the approximate NE within a finite number of iterations. If the iteration continues indefinitely, the approximation of NE found by CCDO-RL depends solely on that of Player 1's ABR, i.e. $\epsilon_1$. However, when $\epsilon = 0$, CCDO-RL can terminate in a finite number of steps if the distance between the ABR and the best response for Player 2 is bounded below by $\epsilon_Y$. In contrast, CCDO may continue for an infinite number of iterations in the same problem setting. 

The convergence result for $\epsilon > 0$ in Theorem \ref{CCDOA} Item 1 is similar to Theorem \ref{CCDO} Item 2, converging to the approximate NE in a finite number of iterations. If the iteration continues indefinitely, the approximation of NE found by CCDO-RL depends solely on that of Player 1's ABR, i.e. $\epsilon_1$. 
When $\epsilon = 0$, CCDO may continue for an infinite number of iterations in the same problem setting. In contrast, CCDO-RL can terminate in finite rounds if the approximate error of ABRs for Player 2 is bounded below by $\epsilon_Y$ or the condition in Remark 2 is satisfied by Player 1 or 2.

\begin{remark}
      Except for the uniform lower bound for Player 2 in (\ref{y_br_condi}), if two absolute differences between BR and ABR do not converge to zero, including divergence and convergence to a positive number, i.e.
      $$\max_{x \in X}U(x, q_k^*) - U(x^{\epsilon_1}_{k+1}, q_k^*) \nrightarrow 0 \quad \mbox{or} \quad U(p_k^*, y^{\epsilon_2}_{k+1})-\max_{y \in Y}U(p_k^*, y) \nrightarrow 0, $$
        then CCDOA and CCDO-RL must terminate in a finite number of iterations, even if $\epsilon = 0$
\end{remark}

% \begin{remark}
%      We notice that an infinite number of iterations is equivalent to the condition that for each subgame $\mathcal{G}_k$ and its equilibrium $(p_k^*, q_k^*)$, $U(x^{\epsilon_1}_{k+1}, q_k^*) - U(p_k^*, y^{\epsilon_2}_{k+1}) > 0.$ If two absolute differences between $\mathbb{BR}$ and approximate $\mathbb{BR}$ do not converge to zero, i.e. $\max_{x \in X}U(x, q_k^*) - U(x^{\epsilon_1}_{k+1}, q_k^*) \nrightarrow 0,$ or $U(p_k^*, y^{\epsilon_2}_{k+1})-\max_{y \in Y}U(p_k^*, y) \nrightarrow 0,$ including divergence and convergence to a positive number, then CCDOA and CCDO-RL must terminate in a finite numbers of iterations. 
% \end{remark}

% In this section, we introduce the CCDO algorithm and its approximate version CCDOA in Section \ref{sec_alg}, and prove the convergence of the two algorithms in Sections \ref{secCCDO_prf} and \ref{secCCDOA_prf} respectively. CCDO has a similar algorithmic framework to DO but totally differs in the convergence analysis. Moreover, we consider one phenomenon that never occurs in DO but is common in the COPs, the performance of the approximate best response to another player's mixed policy is even worse than that of NE. To handle this phenomenon, we design CCDOA with a further convergence analysis. Inspired by recent advancements in using RL to solve COPs, we then further propose CCDO-RL (Algorithm \ref{alg:CCDORL}), in which we use RL as the underlying oracle solver for both players in the DO framework. 

% %Considering the effective solving in COPs, RL is the priority for the approximate best response. Based on CCDOA, CCDO-RL is proposed to solve the specific ACCES games on the CO field.


% \subsection{CCDO \& CCDOA}\label{sec_alg}

% DO is originally proposed to solve NE in the large zero-sum matrix games \citep{mcmahan2003planning}. The key idea is to iteratively compute the mixed NE in the subgame and expand the subgame by the best response (BR) to the current NE of the subgame. We adopt the same algorithmic framework and propose CCDO, to solve the NE in ACCES games (see Algorithm \ref{alg:CCDO} of Appendix \ref{Appendix_B}). The difference with DO is the stopping criterion, Line 6 in Algorithm \ref{alg:CCDO}, in which the original part in DO is subgame sets both remain unexpanded, i.e. $X_{k+1} = X_k, Y_{k+1} = Y_k$ which naturally holds on finite games but cannot be possibly guaranteed in ACCES games even if iterating infinite times because of $Y$'s infiniteness. %The pseudocode of CCDO is described in Algorithm \ref{alg:CCDO}.



% Due to the $NP$-hardness of most COPs, finding the exact best response for the combinatorial player is computationally impractical. Hence, we use a more practical approximate version, CCDOA (Algorithm \ref{alg:CCDOA} of Appendix \ref{Appendix_B}), to solve the approximate NE. However, the approximation of BR may cause circumstances where the utility of the approximate best response is lower than that of NE in the subgame, which never happens in CCDO and this will take a tricky effect on the convergence analysis and add computational overhead on solving NE in the subgame. To this end, two discriminants (lines 6-13 in Algorithm \ref{alg:CCDOA}) are added to guarantee the optimality of the two approximate best responses $x_{k+1}^{\epsilon_1}, y_{k+1}^{\epsilon_2}$ in the subgame $ \mathcal{G}_k = (X_k, Y_k, u)$ and simplify the subgame NE computation in CCDOA. %both in CCDOA and the following more practical implementation algorithm CCDO-RL, which uses RL as the underlying approximate BR solver.

% % As an effective and widely applicable algorithm, RL is designed and applied in various fields. Especially in COPs, RL not only outperforms the approximation and heuristic algorithms in performance but also has generalization ability on unseen graphs, which can demonstrate the reasonability to choose RL as the approximate best response. 


% %Based on CCDOA, the approximate algorithm closer to reality, CCDO-RL is designed. Pseudocodes of CCDOA and CCDO-RL are described in Algorithm \ref{alg:CCDOA} and \ref{alg:CCDORL} in the appendix \ref{Pseudo}.

% \subsection{CCDO-RL as a Practical Implementation of CCDOA}
% {\color{red}Recently, there have been studies that use RL to learn a generalized policy for certain combinatorial optimization problems on graphs \citep{khalil2017learning, nazari2018reinforcement, deudon2018learning, bengio2020machine}. The key idea is to decompose the selection of nodes into a sequence and learn a heuristic policy that selects nodes sequentially. The RL policy is usually trained on a set of seen training graphs, in the hope that it generalizes to unseen test graphs of similar characteristics. Such generalization has been further enhanced via graph embedding techniques such as Structure to Vector (S2V) \citep{Dai2016s2v} and Graph Convolutional Networks (GCNs) \citep{kipf2016semi} as the underlying value/policy network. Hence, we eventually propose CCDO-RL (see Algorithm \ref{alg:CCDORL}), a practical implementation of CCDOA where we use RL and graph embedding techniques as the underlying method to find the approximate BR for each player (Line 4 of Algorithm \ref{alg:CCDORL}).}

% % Reinforcement learning has been widely demonstrated as a promising tool for COPs due to its ability to learn fast and generalizable solutions to unseen COP instances while maintaining close-to-optimal solution quality~\citep{khalil2017learning,nazari2018reinforcement,deudon2018learning,bengio2020machine,kool2018attention,berto2023rl4co}. Hence, we eventually propose CCDO-RL (see Algorithm~\ref{alg:CCDORL}), a practical implementation of CCDOA where we use RL as the underlying method to find the approximate BR for each player (Line 4 of Algorithm \ref{alg:CCDORL}). %\hp{Need to go over the Algorithm}

% % \begin{algorithm}[htbp]
% %     \caption{Combinatorial-Continuous Double Oracle Reinforcement Learning Algorithm}
% %     \label{alg:CCDORL}
% %     \renewcommand{\algorithmicrequire}{\textbf{Input:}}
% %     \renewcommand{\algorithmicensure}{\textbf{Output:}}
% %     \begin{algorithmic}[1]
% %         \REQUIRE Game $\mathcal{G} = (X, Y, u)$, $\epsilon \geq 0$.  %%input
% %         \STATE  Initialize strategy set $\Pi_{1,0}$, $\Pi_{2,0}$.
% %         \REPEAT
% %         \STATE Solve the mixed equilibrium $\sigma_k^*$ in the subgame $(\Pi_{1,k}, \Pi_{2,k})$.
% %         \STATE Find the approximate best response by RL algorithms $(\pi_{1,k}^*, \pi_{2,k}^*)$ \\
% %         \IF{$U(\pi_{1,k}^*, \sigma_{2,k}^*) \leq U(\sigma_k^*)$} 
% %         \STATE $\Pi_{1,k+1} = \Pi_{1,k}$
% %         \ELSE
% %         \STATE $\Pi_{1,k+1} = \Pi_{1,k} \cup \{\pi_{1,k}^*\}$.
% %         \ENDIF
% %         \IF{$U(\sigma_{1,k}^*, \pi_{2,k}^*) \geq U(\sigma_k^*)$}
% %         \STATE $\Pi_{2,k+1} = \Pi_{2,k}$
% %         \ELSE
% %         \STATE $\Pi_{2,k+1} = \Pi_{2,k} \cup \{\pi_{2,k}^*\}$.
% %         \ENDIF
% %         \UNTIL{$U(\sigma_{1,k}^*, \pi_{2,k}^*) - U(\sigma_{1,k}^*, \pi_{2,k}^*)) \leq \epsilon$}
% %         \ENSURE $\sigma_k^*$   %%output
% %     \end{algorithmic}
% % \end{algorithm}

% % \begin{algorithm}[ht]
% %     \caption{Combinatorial-Continuous Double Oracle Reinforcement Learning Algorithm}
% %     \label{alg:CCDORL}
% %     \renewcommand{\algorithmicrequire}{\textbf{Input:}}
% %     \renewcommand{\algorithmicensure}{\textbf{Output:}}
% %     \begin{algorithmic}[1]
% %         \REQUIRE Game $\mathcal{G} = (X, Y, u)$, $\epsilon \geq 0$.  %%input
% %         \STATE  Initialize strategy set $\Pi_{1,0}$, $\Pi_{2,0}$.
% %         \REPEAT
% %         \STATE Solve the mixed equilibrium $\sigma_k^*$ in the subgame $(\Pi_{1,k}, \Pi_{2,k})$.
% %         \STATE Find the approximate best response by RL algorithms $(\pi_{1,k}^*, \pi_{2,k}^*)$ \\
% %         \IF{$U(\pi_{1,k}^*, \sigma_{2,k}^*) \leq U(\sigma_k^*)$ ($U(\sigma_{1,k}^*, \pi_{2,k}^*) \geq U(\sigma_k^*)$)} 
% %         \STATE $\Pi_{1,k+1} = \Pi_{1,k}$ $(\Pi_{2,k+1} = \Pi_{2,k})$
% %         \ELSE
% %         \STATE $\Pi_{1,k+1} = \Pi_{1,k} \cup \{\pi_{1,k}^*\}$  $(\Pi_{2,k+1} = \Pi_{2,k} \cup \{\pi_{2,k}^*\})$.
% %         \ENDIF
% %         \UNTIL{$U(\sigma_{1,k}^*, \pi_{2,k}^*) - U(\sigma_{1,k}^*, \pi_{2,k}^*)) \leq \epsilon$}
% %         \ENSURE $\sigma_k^*$   %%output
% %     \end{algorithmic}
% % \end{algorithm}
% \subsection{Convergence of CCDO} \label{secCCDO_prf}
% First, we prove the convergence of CCDO, providing the basis for the subsequent convergent proof of CCDOA and CCDO-RL. The full proof is provided in Appendix \ref{Appendix_B}. %From Theorem \ref{CCDO}, we can find the similarity with continuous games, i.e. iterating infinitely to find NE in the subsequence of subgame NE pairs. If we only need approximate NE, the algorithm can stop in finite iterations.
% \begin{theorem}\label{CCDO}
%     Given a two-player ACCESS game $\mathcal{G} = (X, Y, u)$, where $X$ is finite, $Y$ is a nonempty compact set, and the utility function $u$ is continuous in $Y$ when fixing the strategy in $X$, we have
    
%     1. When $\epsilon = 0$, every weakly convergent subsequence in the subgame equilibrium sequence $\{(p_k^*, q_k^*)\}$ converges to the equilibrium of the whole game, possibly in an infinite number of iterations. 
    
%     2. When $\epsilon > 0$, Algorithm \ref{alg:CCDO} converges to an $\epsilon$- equilibrium in a finite number of epochs.
% \end{theorem}
% \begin{algorithm}[htbp]
% 	\caption{Combinatorial-Continuous Double Oracle Reinforcement Learning Algorithm}
% 	\label{alg:CCDORL}
% 	\LinesNumbered
% 	\KwIn{Game $\mathcal{G} = (X, Y, u)$, $\epsilon \geq 0$.}
% 	\KwOut{$\sigma_k^*$}
%      Initialize strategy set $\Pi_{1,0}$, $\Pi_{2,0}$\\
% 	\Repeat{$U(\sigma_{1,k}^*, \pi_{2,k}^*) - U(\sigma_{1,k}^*, \pi_{2,k}^*)) \leq \epsilon$}{
%      Solve the mixed equilibrium $\sigma_k^*$ in the subgame $(\Pi_{1,k}, \Pi_{2,k})$.\\
%      Find the approximate best response by RL algorithms $(\pi_{1,k}^*, \pi_{2,k}^*)$ \\
%      $\Pi_{1,k+1} = \Pi_{1,k} \cup \{\pi_{1,k}^*\}$, $\Pi_{2,k+1} = \Pi_{2,k} \cup \{\pi_{2,k}^*\}$.\\
%      \uIf{$U(\pi_{1,k}^*, \sigma_{2,k}^*) \leq U(\sigma_k^*)$}{
%         $\Pi_{1,k+1} = \Pi_{1,k}$.
%     }
%     \uIf{$U(\sigma_{1,k}^*, \pi_{2,k}^*) \geq U(\sigma_k^*)$}{
%         $\Pi_{2,k+1} = \Pi_{2,k}$.
%     }
% }
% \end{algorithm}

% \subsection{Convergence of CCDOA and CCDO-RL}\label{secCCDOA_prf}
% Next, we consider the convergence of CCDOA and CCDO-RL with approximate best responses (full proof in Appendix \ref{Appendix_B}). Apart from the convergence analysis, the influence of different forms of approximate best responses on the number of algorithm inner iterations is also explained further. 
% \begin{theorem}\label{CCDOA}
%     Given $\mathcal{G} = (X, Y, u)$, where $X$ is finite, $Y$ is a nonempty compact set, and the utility function $u$ is continuous in $Y$ when fixing the strategy in $X$, with $\epsilon_1$ best response oracle for Player 1 in $X$ and $\epsilon_2$ best response oracle for Player 2 in $Y$, we have
    
%     1. If terminating in finite iterations, then CCDOA and CCDO-RL on game $\mathcal{G}$ will converge to $(\epsilon+\epsilon_1+\epsilon_2)$- equilibrium. 
%     Especially, if the best response oracle for Player 2 has a lower bound for every mixed strategy in $\bigtriangleup_{X}$, i.e. 
%     \begin{eqnarray}\label{y_br_condi}
%         \begin{aligned}
%             \forall p \in \bigtriangleup_{X}, \exists \epsilon_Y, s.t. U(p, BR^{\epsilon_2}_{2}(p)) \geq \min_{y\in Y}U(p, y)+\epsilon_Y,
%         \end{aligned}
%     \end{eqnarray}
%     then CCDOA and CCDO-RL must converge to an $(\epsilon+\epsilon_1+\epsilon_2)$- equilibrium in a finite iterations.

%     2.  When $\epsilon=0$, if CCDOA and CCDO-RL produce infinite iterations, every weakly convergent subsequence converges to an $\epsilon_1$- equilibrium.

%     3. When $\epsilon>0$, for any form of approximate best response oracles, CCDOA and CCDO-RL can converge to a finitely supported $(\epsilon+\epsilon_1+\epsilon_2)$- equilibrium in a finite number of iterations.
% \end{theorem}
% % The proof is provided in Appendix \ref{Appendix_B}.

% \begin{remark}
%      We notice that an infinite number of iterations is equivalent to the condition that for each subgame $\mathcal{G}_k$ and its equilibrium $(p_k^*, q_k^*)$, $U(x^{\epsilon_1}_{k+1}, q_k^*) - U(p_k^*, y^{\epsilon_2}_{k+1}) > 0.$ If two absolute differences between $\mathbb{BR}$ and approximate $\mathbb{BR}$ do not converge to zero, i.e. $\max_{x \in X}U(x, q_k^*) - U(x^{\epsilon_1}_{k+1}, q_k^*) \nrightarrow 0,$ or $U(p_k^*, y^{\epsilon_2}_{k+1})-\max_{y \in Y}U(p_k^*, y) \nrightarrow 0,$ including divergence and convergence to a positive number, then CCDOA and CCDO-RL must terminate in finite iterations. 
% \end{remark}

% \subsection{Analysis on Computational Complexity}

% \yh{The background of CO+ML should be stated in detail because relevant papers in this field will not be mentioned in related works. Besides, some assumptions such as a perfect representative network are appropriate.}

% \yh{Some state-of-the-art approximate algorithms for some CO problems such as TSP, CVRP, OP, and CSP should be referred to.}

% Because most CO problems are NP-hard, we are mainly concerned about the state-of-the-art approximate algorithms when analyzing the computational complexity of our algorithm \ref{alg:CCDOA}.

% Now for this form of the game, we know that $\forall \epsilon >0$, the algorithm \ref{alg:CCDOA} can converge to an approximate equilibrium in finite iterations which is assumed $p$.

