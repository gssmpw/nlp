\section{Related Work}
% related works可以并不长，主要是要分明一些，每个paragraph都恰到好处。

% \citep{cox2013provision} centers on symmetric provision games and appropriation games whose results were used to analyze asymmetric games further.\citep{tuyls2018symmetric} proposes a new theory to decompose one asymmetric game into two independent symmetric games. \citep{tuyls2018generalised} extended the meta-game analysis from symmetric to asymmetric games.

\textbf{Symmetric and asymmetric games.} Symmetric games are initially proposed by \citep{von1947theory} and studied under the context of non-cooperative \citep{nash1950non}, economic \citep{hammerstein1994game}, and two-person \citep{washburn2014two} games. \citet{amir2008symmetric} focus on pure strategy equilibrium with supermodular payoff functions. \citet{fey2012symmetric} studies symmetric games only with asymmetric equilibria. A few studies extend the theories on symmetric games to asymmetric settings \citep{cox2013provision,tuyls2018generalised}, or transform asymmetric games to symmetric ones \citep{tuyls2018generalised}. These studies usually concentrate on a specific type of classic game such as metric games or poker.
\citet{narasimha2013ant} and \citet{carlsson2009solving} are among the first to consider asymmetric games involving a combinatorial player in a variant of traveling salesman problem with multiple vehicles. \citet{jain2011double} studies a security game where the defender decides the location of sources and the attacker chooses a path to find the source. \citet{xu2014solving} extends the idea of \citet{jain2011double} and consider discrete time domains and moving targets. In the covering problem, \citet{rahmattalabi2019exploring} considers the failure of some nodes and models it as one zero-sum game. All of these studies are limited to finite games.

% In these game settings, all rely on the existence of equilibrium in the finite game. 

% \hp{DO is not mentioned?}

\textbf{Equilibrium learning in zero-sum games.} DO \citep{mcmahan2003planning} is a powerful tool for solving complex strategic normal-form games by iteratively expanding the players' strategy sets and efficiently finding equilibria. The idea has been extended toward better NE computation, different forms of games, convergence rate, etc. \citet{mcaleer2020pipeline} and \citet{zhou2023efficient} focus on accelerating the computation of the approximate equilibrium. Different diversity metrics are proposed by \citep{balduzzi2019open, perez2021modelling, liu2021towards, yao2024policy} to find more effective and various strategies. In extensive-form games, \citet{mcaleer2021xdo} works to achieve linear convergence to approximate equilibrium and \citet{tang2023regret} studies sample complexity. Except for DO and its variants, NE learning in zero-sum settings remains appealing in periodic games \citep{fiez2021online}, polymatrix games \citep{cai2016zero}, and Markov games \citep{zhu2020online}, etc. As far as we know, they are all limited to matrix games in theories related to the existence and convergence of NE although \citet{mcaleer2021xdo} conduct experiments on continuous-action games by Deep RL. \citet{balandat2016minimizing, Adam2021DOcontin} study the NE convergence of continuous games but two players are symmetric.

%To solve the equilibrium in the zero-sum game \citep{washburn2014two}, a large amount of research focuses on this and its branches like computational efficiency, strategy diversity in the normal game, and finding NE in the extensive game, or continuous game. 
  %DO \citep{mcmahan2003planning} is a powerful tool for solving complex strategic normal-form games by iteratively expanding the players' strategy sets and efficiently finding equilibria. The idea has been extended toward better NE computation, different forms of games, convergence rate, etc. \citet{mcaleer2020pipeline} and \citet{zhou2023efficient} focus on accelerating the computation of the approximate equilibrium. Different diversity metrics are proposed by \citep{balduzzi2019open, perez2021modelling, liu2021towards, yao2024policy} to find more effective and various strategies. \citet{tang2023regret} and \citet{mcaleer2021xdo} consider extensive-form games. \citet{mcaleer2021xdo} works to achieve linear convergence to approximate equilibrium. \citet{tang2023regret} studies sample complexity. \textcolor{blue}{Except for DO and its variants, NE learning in zero-sum settings remains appealing in periodic games \citep{fiez2021online}, polymatrix games \citep{cai2016zero}, and Markov games \citep{zhu2020online}, etc.} As far as we know, they are all limited to matrix games, especially in theories related to the existence and convergence of NE. \citet{balandat2016minimizing, Adam2021DOcontin} study the NE convergence of continuous games but two players are symmetric.

\textbf{RL for COPs.} RL has emerged as an effective and generalizable method to solve COPs, where the underlying idea is to decompose the original combinatorial action selection in COPs into a sequence of greedily selected individual actions, using a deep RL policy or value function that is usually represented via various function approximation methods such as graph neural networks \citep{khalil2017learning,joshi2019efficient, manchanda2020gcomb}, recurrent \citep{bello2016neural}, and attention networks \citep{kool2018attention}. Search algorithms, such as active search \citep{hottung2021efficient}, Monte Carlo tree search \citep{fu2021generalize}, and multiple rollouts \citep{kwon2020pomo}, are further integrated into these frameworks to enhance the solution qualities of RL algorithms during inference time. Integrating representation learning and search algorithms, RL has shown promising abilities to learn efficient and generalizable solutions to complex COPs. This motivates us to adopt RL as the backbone method to compute the COPs in a subgame of the ACCESS games.

%Through appropriate neural networks and additional techniques mentioned above, the solution solved by RL surpasses approximated and heuristic algorithms and even draws near the exact solution. Hence, incorporating RL algorithms as the best response for combinatorial players to accelerate equilibrium computation is both reasonable and effective.


%\cite{mcaleer2020pipeline, zhou2022efficient}