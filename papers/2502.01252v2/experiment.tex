\section{Experiments}\label{sec_exp}
%\hp{Accelerating IM simulation~\cite{tang2015influence}}

% \begin{itemize}
%     \item 6.1. Problem setting of three COPs, including the general model and three specific CO problems 
%     \item 6.2. Experiment Setting (hyperparameters, details of training, evaluation, and test) 写在appendix里吧
%     \item 6.3. Performance analysis 这个要占半页
% \end{itemize}

%\hp{need to think of a way to compress these tables / visuals.} 

%\hp{\cancel{Baselines}; hyperparamters; \cancel{metrics}; etc.}

With theoretical guarantees on the existence and convergence of NE for ACCES games, we are also interested in how our proposed algorithm CCDO-RL works empirically. To evaluate this, we conduct experiments of CCDO-RL on three distinct ACCES game instances introduced in Section \ref{sub_exp_ins} and analyze the performance of CCDO-RL in Section \ref{sub_train_eval}. Section 6.2.1 aims to empirically demonstrate the convergence (Figures \ref{fig_exploit_20} and \ref{fig_exploit_50}) of the algorithm CCDO-RL over realistic CO problems, and show its consistency with Theorem \ref{CCDOA}. Section 6.2.2 intends to show the average reward (to seen training graphs) as well as the generalizability (to unseen test graphs) of the combinatorial player in real-world ACCES games (shown in Tables \ref{tab_aver}, and \ref{tab_gene}).

\subsection{Three Instances of ACCES Games} \label{sub_exp_ins}
% \hp{This para does not make much sense. Need to follow the framework in the Preliminaries section.}
% For combinatorial optimization problems in real-world applications, situations are more complicated and intractable due to changeable environmental or physical parameters. The form of parameter sets is very crucial because different types have different solvability and computation complexity. Forms of parameter sets mainly contain discrete sets, interval sets \cite{buchheim2018robust} like polyhedral and ellipsoid, probability distributions \cite{carlsson2018wasserstein}, and variable functions \cite{krause2008robust}.

% In reality, these parameters are often impacted by some common factors, such as conditions of weather, transportation, and individual personalities. \cite{kalimeris2019robust} proposed an assumption that real instances (e.g. demands in CVRP, coverages in CSP) 
%Considering affected or attacked COPs, the real instance $\{\theta_{i}\}$ always relied on the estimated value $\{\hat{\theta}_{i}$\} and the variation determined by independent factors $\{g_{i}\}$ and environment/physical parameters/attacker actions $\{\eta\}$. The concrete parameter influence model is stated as follows:

We consider a certain COP which is parameterized with $\{\theta_{i}\}$, where $i$ is the index of nodes (such as a target in security games) -- e.g., such parameters can be interpreted as attack probability of targets.
%coverage radius, customer's demands, or attack probability of targets. 
In real-world applications, we often need to estimate such parameters before solving the COPs. Unfortunately, the estimation $\{\hat{\theta}_{i}\}$ often bears a gap to the true value $\{\theta_{i}\}$, which derives from e.g. environment (aleatoric) uncertainty, model (epistemic) uncertainty, or an attacker trying to manipulate the defender's utility. We use a generic model to formulate this gap:
\begin{equation}\label{linrob}
    \theta_{i} = \hat{\theta}_{i} + y \cdot \tau_{i},
\end{equation}
where $y$ represents the strategy of the nature/attacker, $\tau_{i}$ is the environment factors like weather and transportation conditions, or human subjective factors like the preference of the attacker. 
Such abstraction can represent a wide range of ACCES games, such as facility location covering problems \cite{an2020battery, TIRKOLAEE2020340}, CVRP \cite{vehiclerouting.ch8,dinh2018exact, FLORIO20231081}, security patrolling (OP) \citep{xu2021robust}, and influence maximization problem \cite{kalimeris2019robust}. We describe three instances of ACCES games based on the model (\ref{linrob}).%Based on this model (\ref{linrob}), we focus on three combinatorial optimization problems with attacks or environmental/physical influence.

% \hp{Hard to follow. We should point out what are the two players, what are X, Y, u etc}

\textbf{Adversarial Covering Salesman Problem (ACSP):} In a map of cities, every city $i$ has a coverage $\theta_{i}$. A salesman finds the shortest path such that all cities are visited or covered, with $\theta_{i}$ influenced by physical factors $\tau_i$ and transportation parameters $y$ based on Eq.(\ref{linrob}). The salesman is Player 1 where $X$ consists of the feasible paths of the salesman. Nature is Player 2 with $Y$ = $[0, 1]^K \ni y, K \in \mathbb{N}$. The utility function of Player 1 $u$ is the opposite of the total traveling distance.

\textbf{Adversarial Capacitated Vehicle Routing Problem (ACVRP):} A vehicle with a constrained capacity of goods finds the shortest path under the worst case with the $i_{th}$ customer's demand $\theta_i$ changed by environmental factors $\tau_i$ and weather parameter $y$ on Eq.(\ref{linrob}). The vehicle is Player 1 where $X$ is the set of the feasible path $x$. Nature is Player 2 where $Y$ is $[0, 1]^K \ni y, K \in \mathbb{N}$. The utility function of Player 1  $u$ is the opposite of total delivery distance satisfying all the demands of customers.


\textbf{Patrolling Game (PG):} The patrolling game is described in the introduction.

For all the problem instances, we run our algorithm on two problem sizes: 20 nodes and 50 nodes. The detailed description and problem parameters of the three game instances are in Appendix \ref{app_ex_para_set}.

% Similarly, in the vehicle route problem (VRP), conditions with correlated parameters arouse broad attention from scholars \cite{vehiclerouting.ch8,dinh2018exact,FLORIO20231081}. \cite{dinh2018exact} considered the demand correlation by geographical proximity of nodes, described by some independent random variables in the fractional form. \cite{FLORIO20231081} utilized 'external factors' to stand for unknown covariates affecting all demands and presented a Bayesian model to learn correlations. Further more, about IM problems, \cite{kalimeris2019robust} combined node features and uncertain hyperparameters to fit the influence probability on each edge.

% \subsection{Training CCDO-RL}

% For all the problems, CCDO-RL adopts the REINFORCE algorithm with an attention-based encoder-decoder framework \cite{kool2018attention} (used as an inductive graph representation component) to learn a (generalizable) COP solver for one player (protagonist), and PPO \cite{schulman2017proximal} to train a policy for the other player (adversary) whose strategy space is continuous. CCDO-RL is trained with 50 epochs on a set of 10,000 graphs (with 20 or 50 nodes). The hyperparameters of CCDO-RL are specified in Appendix \ref{app_ex_para_set} (Table \ref{tab_hyper_ccdorl}). Our code is included as supplementary material for ease of reproduction. 
% % \hp{need to specify hyperparas}

\subsection{Performance of CCDO-RL}\label{sub_train_eval}

Two aspects are evaluated for the performance of CCDO-RL, i.e., i) Convergence to NE (Section \ref{sub_per_conver}) exploring whether CCDO-RL can compute the NE, and ii) Protagonist policy's average reward and generalizability (Section \ref{sub_per_rob}). Generalizability refers to the ability of RL models trained on previously seen graphs (problem instances), to perform well on a new set of unseen test graphs. The model’s usability is enhanced by generalizability, rather than focusing solely on the average reward, which is a critical motivation in the literature on RL for COPs \citep{khalil2017learning, kool2018attention}.

For all the problems, CCDO-RL adopts the REINFORCE algorithm with an attention-based encoder-decoder framework \citep{kool2018attention} (used as an inductive graph representation component) to learn a generalizable COP solver for Player 1 (protagonist), and PPO to train a policy for Player 2 (adversary) whose strategy space is continuous. CCDO-RL is trained on a set of 10,000 graphs (with 20 or 50 nodes). The hyperparameters of CCDO-RL are specified in Appendix \ref{app_ex_para_set} (Table \ref{tab_hyper_ccdorl}). Our code is included as supplementary material and will be open-sourced for ease of reproduction. 

% \textbf{Training.} For all the problems, CCDO-RL adopts the REINFORCE algorithm with attention-based encoder-decoder framework \cite{kool2018attention} (used as an inductive graph representation component) to learn a (generalizable) COP solver for one player (protagonist), and PPO \cite{schulman2017proximal} to train a policy for the other player (adversary) whose strategy space is continuous. CCDO-RL is trained with 50 epochs on a set of 10,000 graphs (with 20 or 50 nodes). 

% \hp{We should first present results about convergence as it is mostly aligned with the theory.}

\subsubsection{Convergence to NE} \label{sub_per_conver}

Exploitability is a common metric to describe the closeness to true NE by calculating the sum of performance distances between each new best response and subgame NE, i.e. $\sum_{i=1,2} U(\pi_{i,k}^{br}, \sigma_{-i,k}) - U(\sigma)$ in the general two-player game. Since our game is zero-sum, the calculation is as follows:
\begin{equation*}
   \text{Exploitability}(\sigma) = \max_{\pi_1 \in \Sigma_1} U(\pi_1, \sigma_{2}) - \min_{\pi_2 \in \Sigma_2} U(\sigma_1, \pi_2).
\end{equation*}
From Figure \ref{fig_exploit_20}, we can see that CCDO-RL can converge to approximate NE in 25 iterations or less (in the PG setting), reaching 0.05 in ACSP, 0.10 in ACVRP, and 0.03 in PG with 20 nodes. Similar results are observed in problems with 50 nodes (see Figure \ref{fig_exploit_50} in Appendix \ref{app_exp}). These results validate the effectiveness of CCDO-RL in finding the NE for various types of games.

%Similarly, the exploitability of three COPs in 50 nodes is provided in the appendix \ref{app_exp}.
\vspace{-\baselineskip}
\begin{figure}[htbp]
	\centering
    \subfigure[ACSP20]{
    \label{csp20_nashconv}
    \includegraphics[scale=0.20]{Figures/nashconv_log_csp20_sm_7.eps}
    }
    \subfigure[ACVRP20]{
    \label{cvrp20_nashconv}%文中引用该图片代号
    \includegraphics[scale=0.20]{Figures/nashconv_log_svrp20_sm_7.eps}
    }
    \subfigure[PG20]{
    \label{opsa20_nashconv}
    \includegraphics[scale=0.20]{Figures/nashconv_log_pg20_sm_7.eps}
    }
    \caption{Exploitability curve of CCDO-RL on three games of 20 nodes}
    \label{fig_exploit_20}
\end{figure}
\vspace{-\baselineskip}
\subsubsection{Average reward and Generalizability of Combinatorial player} \label{sub_per_rob}
% \subsubsection{Robustness and Generalizability of Protagonist Policy} \label{sub_per_rob}
%\hp{CCDO-RL being better in these following metrics is only kind of a by-product.}

% \textbf{Evaluation.} The learned policies are then tested on 200 graphs, where 100 of them are randomly selected from the 10,000 training graphs, and the other 100 are unseen graphs. 
% We use two metrics to evaluate the performance of different policies for the protagonist player: \textbf{Average proportional loss} $R-$ describes the policy overfitting degree \citep{lanctot2017unified}; \textbf{Reward} evaluates the performance of the protagonist with the adversary under three COPs.  
% \begin{eqnarray}
%         &R- = (\hat{D} - \hat{O}) / \hat{D}.
% \end{eqnarray}
% in which $\hat{D}$ is the mean value of the diagonals and $\hat{O}$ is the mean value of the off-diagonals in the payoff matrix provided in the Appendix \ref{app_exp}.

% Because the protagonist policy is trained against a powerful adversary under our ACCES game setting, the obtained policy is naturally robust against adversarial perturbations. This subsection sheds a bit of light on this perspective and quantifies the extent of robustness of CCDO-RL as well as the ability of RL to generalize to unseen test graphs.

\textbf{Evaluation.} The learned policies are tested on 200 graphs, with 100 being randomly selected from the 10,000 training graphs (to show the average reward), and the other 100 being unseen graphs (to test policy generalization). We evaluate the performance of the protagonist with the adversary under three COPs. For each COP, the performance is considered both on the 20-node and 50-node map.
% We use two metrics to evaluate the performance of different policies for the protagonist player: \textbf{Average proportional loss} $R-$ describes the policy overfitting degree \citep{lanctot2017unified}; \textbf{Reward} evaluates the performance of the protagonist with the adversary under three COPs.

\textbf{Baselines.} There are heuristic algorithms for each game instance (Heuristic in Table \ref{tab_aver} and \ref{tab_gene}) and a single-player RL algorithm. For ACVRP, we adopt the Tabu Search algorithm (Tabu) \citep{li2020improved} as the heuristic algorithm, which is widely applied in the routing problem. For ACSP, the common benchmark local search algorithm, LS2 \citep{golden2012generalized}, is used. For PG, we choose the greedy algorithm as the baseline. The "RL against Stoc" algorithm in Tables \ref{tab_aver} and \ref{tab_gene} is identical to the protagonist model in CCDO-RL but trained in environments with stochastic adversarial perturbations.

% \textbf{Baselines.} There are a heuristic algorithms for each game instance {\color{red} (Heuristic mentioned in the Table \ref{tab_aver} and \ref{tab_gene})} and a single-player RL algorithm. For ACVRP, we adopt the Clarke-Wright (CW) algorithm \citep{pichpibul2013heuristic} and the Tabu Search algorithm (Tabu) \citep{li2020improved} as heuristics, which are applied widely in the routing problem. For ACSP, two common benchmark local search algorithms, LS1 and LS2 \citep{golden2012generalized}, are used. For PG, we choose a local search algorithm \citep{vansteenwegen2009iterated} and the greedy algorithm as the heuristic baselines. {\color{red} The "RL  against Stoc" algorithm referred to Tables \ref{tab_aver} and \ref{tab_gene}} is identical to the protagonist model in CCDO-RL {\color{red} but trained on environments with stochastic adversarial perturbations.} 

\textbf{Average Reward.}  As illustrated in Table \ref{tab_aver}, our algorithm achieves a better average reward than baselines (10.08\% improvement on average of all settings against two baselines), regardless of CO instance or problem size, when confronting the adversary trained by CCDO-RL. In the setting of CSP-20 nodes, the average reward is improved by 46.98\% compared to the heuristic and by 7.14\% compared with the RL against Stoc. For the 50-node setting, the improvements are 45.91\% and 5.28\% respectively. Similarly, the improvements in contrast to Heuristic and RL against Stoc are as follows: 1.72\% and 3.01\%  for CVRP-20 nodes, 0.75\% and 4.46\% for CVRP-50 nodes, 4.17\% and 1.48\% for PG-20 nodes, and 10.60\% and 4.38\% for PG-50 nodes.

\textbf{Generalizability.} From Table \ref{tab_gene}, CCDO-RL continues to achieve a better average reward when facing the adversary, demonstrating that the learned RL policies generalize well to unseen graphs. Even though the non-RL baselines do have access to the graph structures and other problem information of the unseen problem instances, CCDO-RL can obtain comparable performances without re-training on the new problem instances. The improvements versus Heuristic and RL against Stoc are 46.61\% and 7.02\% for CSP-20 nodes, 42.24\% and 3.94\% for CSP-50 nodes, 1.12\% and 1.56\% for CVRP-20 nodes, 0.90\% and 5.05\% for CVRP-50 nodes, 5.35\% and 2.40\% for PG-20 nodes, and 12.17\% and 10.33\% for PG-50 nodes. Even when confronting the stochastic adversary, CCDO shows superior generalizability compared to two baselines across three COPs, with average improvements of 6.31\%, 3.42\%, and 3.95\% respectively. Detailed results are provided in Appendix \ref{app_exp} (Tables \ref{tab_csp_full_20} - \ref{tab_op_full_50}). 
% The model’s usability is enhanced by the ability to generalize rather than focusing solely on the average reward, which is a critical motivation of the RL for combinatorial optimization literature \citep{khalil2017learning, kool2018attention}.  

\begin{remark}
    In CO problems (or more broadly, operations research and economics), it is known that achieving solution quality improvements against strong baselines (e.g., the RL methods trained with a stochastic adversary) is very challenging, and the margins are usually small \citep{kool2018attention}, sometimes even less than 1\%. However, these “tiny” marginal improvements in profits keep small business owners in the real world alive. Last, the improvement depends a lot on the problem settings, and we show that sometimes the improvement can be much more significant.
\end{remark}
\vspace{-\baselineskip}
% \textbf{Performance analysis.} The robustness results of CCDO-RL for ACSP are shown in Table \ref{tab_csp}. We have the following observations: 1) On both of the 100 seen/unseen graphs, single-player RL performs better than heuristic algorithms no matter whether attacked or not. (2) When confronting the adversary trained by CCDO-RL, CCDO-RL exceeds RL by 0.25 and 0.24 on the training set, and by 0.25 and 0.18 on the test set, respectively under the 20-node and 50-node graphs. This demonstrates the robustness of CCDO-RL. 3) Compared to the performance of the training set with that of the test set, we can see that RL and CCDO-RL both maintain a certain degree of generalization. Similar results for ACVRP (Table \ref{tab_cvrp}) and SPG (Table \ref{tab_op}) are provided in Appendix \ref{app_exp}. 

\begin{table}[ht]
  \caption{Average reward against CCDO-RL's adversary (on seen graphs)}
  \vspace{\baselineskip}
  \label{tab_aver}
  \centering
  \small
  \begin{tabular}{lllllll}
    \toprule
    \multirow{2}{*}{method} & \multicolumn{2}{c}{ACSP (Mean$\pm$Std)} & \multicolumn{2}{c}{ACVRP (Mean$\pm$Std)} & \multicolumn{2}{c}{PG (Mean$\pm$Std)} \\
    \cmidrule(r){2-3} \cmidrule{4-5} \cmidrule(r){6-7}
                            & 20 nodes & 50 nodes & 20 nodes & 50 nodes & 20 nodes & 50 nodes\\
    \midrule
    Heuristic & 6.13$\pm$1.20 & 7.55$\pm$1.42 & 7.65$\pm$1.23  & 13.38$\pm$1.70 & 2.64$\pm$1.03 & 4.53$\pm$1.84   \\
    RL against Stoc    & 3.50$\pm$0.47  & 4.55$\pm$0.62  & 7.55$\pm$1.16  & 13.90$\pm$1.63 & 2.71$\pm$0.90 & 4.80$\pm$2.18   \\
    CCDO-RL   & $\pmb{3.25}$$\pm$0.42 & $\pmb{4.31}$$\pm$0.51  & $\pmb{7.42}$$\pm$1.21  & $\pmb{13.28}$$\pm$1.52 &  $\pmb{2.75}$$\pm$0.87 & $\pmb{5.01}$$\pm$1.91  \\
    \bottomrule
  \end{tabular}
\end{table}
\vspace{-\baselineskip}

\begin{table}[htp]
  \caption{Generalizability against CCDO-RL's adversary (on unseen graphs)}
  \vspace{\baselineskip}
  \label{tab_gene}
  \centering
  \small
  \begin{threeparttable}
  \begin{tabular}{lllllll}
    \toprule
    \multirow{2}{*}{method} & \multicolumn{2}{c}{ACSP (Mean$\pm$Std)} & \multicolumn{2}{c}{ACVRP (Mean$\pm$Std)} & \multicolumn{2}{c}{PG (Mean$\pm$Std)} \\
    \cmidrule(r){2-3} \cmidrule{4-5} \cmidrule(r){6-7}
                            & 20 nodes & 50 nodes & 20 nodes & 50 nodes & 20 nodes & 50 nodes\\
    \midrule
    Heuristic & 6.20$\pm$1.33 & 7.60$\pm$1.37   & 7.64$\pm$1.30  & 13.27$\pm$1.87 & 2.43$\pm$0.98 & 4.19$\pm$1.69    \\
    RL against Stoc  & 3.56$\pm$0.37  & 4.57$\pm$0.58  & 7.67$\pm$1.30  & 13.85$\pm$1.53 &  2.50$\pm$0.95 & 4.26$\pm$2.17 \\
    CCDO-RL   & $\pmb{3.31}$$\pm$0.35 & $\pmb{4.39}$$\pm$0.52  & $\pmb{7.55}$$\pm$1.28  & $\pmb{13.15}$$\pm$1.59 & $\pmb{2.56}$$\pm$0.92 & $\pmb{4.70}$$\pm$1.94\\

    \bottomrule
  \end{tabular}
  \begin{tablenotes}
      \footnotesize
      \item[1] For the average reward of ACSP and ACVRP, smaller is better while for that of PG larger is better.
  \end{tablenotes}
  \end{threeparttable}
\end{table}
\vspace{-\baselineskip}
% two heuristics and one RL
% \begin{table}[ht]
%   \caption{{\color{red} Average reward of CCDO-RL (on seen graphs). For the value of CSP and CVRP, larger is better while for that of PG smaller is better.}}
%   \label{tab_aver}
%   \centering
%   \small
%   \begin{tabular}{lllllll}
%     \toprule
%     \multirow{2}{*}{method} & \multicolumn{2}{c}{CSP (Mean$\pm$Std)} & \multicolumn{2}{c}{CVRP (Mean$\pm$Std)} & \multicolumn{2}{c}{PG (Mean$\pm$Std)} \\
%     \cmidrule(r){2-3} \cmidrule{4-5} \cmidrule(r){6-7}
%                             & 20 nodes & 50 nodes & 20 nodes & 50 nodes & 20 nodes & 50 nodes\\
%     \midrule
%     Baseline 1 & 4.52$\pm$0.71  & 5.98$\pm$0.94 & 7.64$\pm$1.56  & 13.49$\pm$2.10 & 2.71$\pm$1.10 & 1.82$\pm$1.40   \\
%     Baseline 2 & 6.13$\pm$1.20 & 7.55$\pm$1.42   & 7.65$\pm$1.23  & 13.38$\pm$1.70 & 2.64$\pm$1.03 & 1.47$\pm$0.99  \\
%     RL {\color{red}against Stoc}    & 3.50$\pm$0.47  & 4.55$\pm$0.62  & 7.55$\pm$1.16  & 13.90$\pm$1.63 & 2.71$\pm$0.90 & 1.54$\pm$1.03   \\
%     CCDO-RL   & $\pmb{3.25}$$\pm$0.42 & $\pmb{4.31}$$\pm$0.51  & $\pmb{7.42}$$\pm$1.21  & $\pmb{13.28}$$\pm$1.52 &  $\pmb{2.75}$$\pm$0.87 & $\pmb{1.87}$$\pm$1.22  \\
%     \bottomrule
%   \end{tabular}
% \end{table}


% \begin{table}[htp]
%   \caption{{\color{red}Generalizability of CCDO-RL (on unseen graphs)}}
%   \label{tab_gene}
%   \centering
%   \small
%   \begin{threeparttable}
%   \begin{tabular}{lllllll}
%     \toprule
%     \multirow{2}{*}{method} & \multicolumn{2}{c}{CSP (Mean$\pm$Std)} & \multicolumn{2}{c}{CVRP (Mean$\pm$Std)} & \multicolumn{2}{c}{PG (Mean$\pm$Std)} \\
%     \cmidrule(r){2-3} \cmidrule{4-5} \cmidrule(r){6-7}
%                             & 20 nodes & 50 nodes & 20 nodes & 50 nodes & 20 nodes & 50 nodes\\
%     \midrule
%     Baseline 1 & 4.53$\pm$0.79  & 5.95$\pm$0.96 & 7.55$\pm$1.39  & 13.35$\pm$2.04 & 2.52$\pm$1.08 & $\pmb{1.86}$$\pm$1.44  \\
%     Baseline 2 & 6.20$\pm$1.33 & 7.60$\pm$1.37   & 7.64$\pm$1.3  & 13.27$\pm$1.87 & 2.43$\pm$0.98 & 1.52$\pm$1.20    \\
%     RL {\color{red}against Stoc}  & 3.56$\pm$0.37  & 4.57$\pm$0.58  & 7.67$\pm$1.30  & 13.85$\pm$1.53 &  2.50$\pm$0.95 & 1.03$\pm$5.05 \\
%     CCDO-RL   & $\pmb{3.31}$$\pm$0.35 & $\pmb{4.39}$$\pm$0.52  & $\pmb{7.55}$$\pm$1.28  & $\pmb{13.15}$$\pm$1.59 & $\pmb{2.56}$$\pm$0.92 & 1.35$\pm$5.09\\

%     \bottomrule
%   \end{tabular}
%   \begin{tablenotes}
%       \footnotesize
%       \item[1] For the value of CSP and CVRP, larger is better while for that of PG smaller is better.
%   \end{tablenotes}
%   \end{threeparttable}
% \end{table}
