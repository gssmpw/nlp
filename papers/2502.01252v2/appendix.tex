\newpage
\appendix
\section{Proofs and Analysis in Section 4}
\setcounter{proposition}{0}
\setcounter{equation}{0}
\setcounter{theorem}{0}
\subsection{Proofs in Section 4}\label{Appendix_A1}
% \hp{Check this part before submission. All props and theorems need to be consistent to the main paper.}

\begin{definition}\label{def3} {\normalfont[Weakly Convergence.]}
    Suppose $S$ is a space, probability measures $P_n$ weakly converges to $P$, written by $P_n \Rightarrow P$, if for every bounded continuous function $f$, $$\lim_{n \rightarrow \infty} \int_S f dP_n \rightarrow \int_S f dP.$$
\end{definition} 
If any sequence in a set has a weakly convergent subsequence, the set is weakly sequentially compact.
\begin{lemma}\label{lem1}
    $\mathcal{S'},\mathcal{S''}$ are uncorrelated general metric spaces and $P', P''$ are the probability measure on $\mathcal{S'},\mathcal{S''}$ respectively. Define $\mathcal{T} \triangleq S' \times S''$ as the product space of $S'$ and $S''$. if $\mathcal{T}$ is separable, then $P_n' \times P_n'' \Rightarrow P' \times P''$ if and only if $P_n' \Rightarrow P'$ and $P_n'' \Rightarrow P''$. \rm{(\citep{Myerson1991GameT}, Theorem 2.8)}
\end{lemma}

\begin{proposition} {\normalfont[Weakly Sequential Compactness.]}
    Suppose the ACCES game is $\mathcal{G} = (X, Y,u)$, where $X$ is finite, $Y$ is a nonempty compact metric space, and the utility function $u$ is continuous on $Y$ fixing $x \in X$. Then the joint mixed strategy space $\bigtriangleup \triangleq \bigtriangleup_X \times \bigtriangleup_Y$ is weakly sequentially compact. 
\end{proposition}
%\yh{This is self-created proof using Lemma 1.}
\begin{proof}
    Firstly, considering the condition in Lemma \ref{lem1}, we need to prove the product space $X \times Y$ is separable.
    The set $X$ is separable obviously, based on its finiteness and discreteness. And every compact metric space has a countable base, so separable. Hence the set $Y$ is separable. Then the product space $X \times Y$ is separable too.

    The next step is to prove weakly sequential compactness of set $\bigtriangleup_X$ and $\bigtriangleup_Y$. Due to the finiteness of $X$, $\bigtriangleup_X$ is a nonempty compact convex set on $\mathbb{R}^{|X|}$ where any element $p \in \bigtriangleup_X$ can be represented as $p = [p(x_1), ..., p(x_{|X|})]$ satisfying $\sum_{i=1}^{|X|} p(x_i) = 1, p_i \geq 0$. Since strong convergence is equivalent to weak convergence in the finite-dimensional normed space, the compact set $\bigtriangleup_X$ is weakly sequentially compact too.
    
    Besides, according to the properties of mixed strategies in continuous games mentioned in \cite{2007ContinuousGame}, $\bigtriangleup_Y$ is sequentially compact and closed, thus compact. Based on its compactness, proposition 1 of \cite{Adam2021DOcontin} guarantees that $\bigtriangleup_Y$ is weakly sequentially compact. 

    Therefore, due to Lemma \ref{lem1}, we can get $\bigtriangleup$ is weakly sequentially compact.
\end{proof}
\bigskip\bigskip\bigskip \bigskip\bigskip\bigskip\bigskip \bigskip


\begin{proposition}{\normalfont[Continuity of Expected Utility Function.]}
    The expected utility function $U(p,q) \triangleq \sum_{x\in X}\int_{y\in Y} p(x)u(x,y)dq$ is continuous on the joint mixed strategy space $\bigtriangleup$, $\forall p \in \bigtriangleup_X, q \in \bigtriangleup_Y$.
\end{proposition}
%\yh{This is self-created proof too. In a continuous game it is obviously true.}
\begin{proof}
    First we denote the related distance mapping $\rho_1$ and $\rho_2$ on $\bigtriangleup_X$ and $\bigtriangleup_Y$ respectively.
    $$\rho_1(p, p') = \sum_{x \in X} |p (x) - p' (x)|, \forall p, p' \in \bigtriangleup_X,$$
    $$\rho_2(q, q') = \sup_{y \in Y} |q(y) - q'(y)|, \forall q, q' \in \bigtriangleup_Y.$$
    Afterwards, we prove the continuousness of $U$ on $\bigtriangleup$. $\forall (p_0, q_0) \in \bigtriangleup, \forall (p, q) \in O((p_0, q_0), \delta) \cap \bigtriangleup$, which means $d((p,q), (p_0, q_0)) \triangleq \sqrt{\rho_1^2(p, p_0) + \rho_2^2(q, q_0)} \leq \delta,$
    \begin{align}
        |U(p, q) - U(p_0, q_0)| &= |\sum_{x\in X} p(x) \int_{y \in Y} u(x,y) dq - \sum_{x\in X} p_0(x) \int_{y \in Y} u(x,y) d q_0| \\
        &= |\sum_{x\in X}\int_{y \in Y} u(x,y) (p(x)dq - p_0 (x)d q_0) |\\
        &= |\sum_{x\in X}\int_{y \in Y} u(x,y) [(p(x)- p_0 (x))dq + p_0 (x)(dq - d q_0)] | \\
        &\leq |\sum_{x\in X}\int_{y \in Y} u(x,y)(p(x)- p_0 (x))dq| + |\sum_{x\in X}\int_{y \in Y} u(x,y)p_0 (x)(d q_0 - dq)|
    \end{align}
    Because $u(x,y)$ is continuous on $Y$ when fixing $x \in  X$, $X$ is finite, and $Y$ is nonempty compact metric space, $u(x,y)$ is bounded. 
    
    Assume $|u(x,y)| \leq M$, $d((p,q), (p_0, q_0)) \triangleq \sqrt{\rho_1^2(p, p_0) + \rho_2^2(q, q_0)} \leq \delta = \frac{\epsilon}{2M}$, we can get that
    \begin{align}
        |\sum_{x\in X}\int_{y \in Y} u(x,y)(p(x)- p_0 (x))dq| &\leq |\sum_{x\in X} (p(x)- p_0 (x)) \int_{y \in Y} u(x,y)dq| \\ 
        &\leq M |\sum_{x\in X} (p(x)- p_0 (x))| \leq M \rho_1(p, p_0)
    \end{align}
    \begin{align}
        |\sum_{x\in X}\int_{y \in Y} u(x,y)p_0 (x)(d q_0 - dq)| &\leq |\sum_{x\in X} p_0 (x) \int_{y \in Y} u(x,y)(d q_0 - dq)| \\ 
        &\leq \sum_{x\in X} p_0 (x) |\int_{y \in Y} u(x,y)(d q_0 - dq)|  \\
        &\leq \sup_{x\in X} |\int_{y \in Y} u(x,y)(d q_0 - dq)|\\
        &\leq M \rho_2(q, q_0)
    \end{align}

    Then  we can get that 
    \begin{align}
        |U(p, q) - U(p_0, q_0)| &\leq M \rho_1(p, p_0) + \sum_{x\in X} p_0(x)|\int_{y \in Y} u(x,y) dq -\int_{y \in Y} u(x,y) d q_0| \label{contin9} \\ 
        &\leq M \delta + M\delta = 2M \delta = \epsilon.
    \end{align}
    
    When $p_n \Rightarrow p$ and $q_n \Rightarrow q$, the above inequality still holds.
    $p_n \Rightarrow p \Leftrightarrow \rho_1(p_n, p) \rightarrow 0$ because Strong convergence is equivalent to weak convergence in finite-dimension metric spaces. Additionally, $|\int_{y \in Y} u(x,y) dq -\int_{y \in Y} u(x,y) d q_n| \rightarrow 0$ when fixing $x$. From inequality (\ref{contin9}), we can infer 
    \begin{align}
        |U(p, q) - U(p_n, q_n)| &\leq M \rho_1(p, p_n) + \sum_{x\in X} p_n(x)|\int_{y \in Y} u(x,y) dq -\int_{y \in Y} u(x,y) d q_n| \\
        &\leq M\rho_1(p, p_n) + \sup_{x \in X} |\int_{y \in Y} u(x,y) dq -\int_{y \in Y} u(x,y) d q_n| \\
        &= M\rho_1(p, p_n) + |\int_{y \in Y} u(\hat{x},y) dq -\int_{y \in Y} u(\hat{x},y) d q_n| \rightarrow 0,
    \end{align}
    where $\hat{x} = argmax_{x\in X} |\int_{y \in Y} u(\hat{x},y) dq -\int_{y \in Y} u(\hat{x},y) d q_n|$.
\end{proof}



\bigskip\bigskip\bigskip\bigskip\bigskip\bigskip\bigskip \bigskip
\begin{proposition} {\normalfont[Approximate NE of ACCES.]}
    $\mathcal{G}'=\langle X, Y, \Tilde{u} \rangle$ is $\alpha$-approximation of $\mathcal{G}=\langle X, Y, u \rangle$, where $\mathcal{G}$ is an ACCES game. $(p^*, q^*)$ is an $\epsilon$-equilibrium of $\mathcal{G}'$, then $(p^*, q^*)$ is an $(\epsilon + 2\alpha)$-equilibrium of $\mathcal{G}$.
\end{proposition}
\begin{proof}
    Define $\Tilde{U}(p, q) = \sum_{x\in X}\int_{y\in Y} p(x)\Tilde{u}(x, y)dq$, for $\forall (p,q) \in \bigtriangleup$ we can get 
    $$|U(p, q)-\Tilde{U}(p, q)| = |\sum_{x\in X}\int_{y\in Y} p(x)(u(x,y)-\Tilde{u}(x, y))dq| \leq \alpha \sum_{x\in X}\int_{y\in Y} p(x)dq = \alpha$$
    Next for any $q \in \bigtriangleup_Y$,
    $$
        |U(p^*, q^*)-U(p^*, q)| = |U(p^*, q^*) - \Tilde{U}(p^*, q^*) + \Tilde{U}(p^*, q^*) - \Tilde{U}(p^*, q) + \Tilde{U}(p^*, q) - U(p^*, q)| \leq 2\alpha + \epsilon.
    $$
    Similarly, it can be proved that $|U(p^*, q^*)-U(p, q^*)|\leq 2\alpha + \epsilon$, $\forall p \in \bigtriangleup_X$.
\end{proof}
%\bigskip\bigskip\bigskip\bigskip\bigskip\bigskip

\begin{proposition}{\normalfont[Essentially Finite of ACCES.]}
    For an ACCES $\mathcal{G}$, $\forall \alpha >0$, there exists an essentially finite strategic game $\hat{\mathcal{G}}=\langle X, \hat{Y}, \hat{u}\rangle$, s.t. $\hat{\mathcal{G}}$ is $\alpha$-approximation of $\mathcal{G}$.
\end{proposition}
\begin{proof}
    Due to that $\hat{\mathcal{G}}$ is $\alpha$-approximation of $\mathcal{G}$, then for any $x \in X, y \in Y$, $|u(x,y)-\hat{u}(x,y)| \leq \alpha$. On account that $u$ is continuous on $Y$ and $Y$ is a nonempty compact metric space, so $u$ is uniformly continuous on $Y$. According to the uniform continuousness of function $u$, $\forall \alpha >0$, $\exists \epsilon(x) >0$, when $|y - y'| < \epsilon(x)$, $|u(x,y)-u(x,y')|\leq \alpha$. Define $\epsilon = \min_{x \in X} \epsilon(x)$. Y is a nonempty compact metric space, hence it can be covered by finite open balls $O_j(y_j, \epsilon)$, i.e. $Y \subset \bigcup_{j}O_j(y_j, \epsilon)$. 

    Then for $\forall j$, $\forall y \in O_j(y_j, \epsilon), x \in X$, denote that $\hat{u}(x, y) = u(x, y_j)$.
    Hence, $\forall x \in X, \forall y \in Y$, $|u(x,y)-\hat{u}(x,y)| \leq \alpha$.
\end{proof}

\bigskip
%\bigskip\bigskip \bigskip\bigskip\bigskip\bigskip \bigskip\bigskip\bigskip\bigskip
\begin{theorem}\textbf{\emph{[Existence of NE]}}
    $\mathcal{G}=\langle X, Y, u \rangle$, where $X$ is finite space, $Y$ is nonempty compact metric space, $u: X \times Y \rightarrow \mathbb{R}$ is a continuous utility function on $Y$ when fixing $x \in X$. Game $\mathcal{G}$ has a mixed strategy Nash equilibrium.
\end{theorem}
\begin{proof}
    Suppose sequence $\{\alpha_n\}$ converges to zero, i.e. $\alpha_n \rightarrow 0$.
    For any $\alpha_n$, there exists an essentially finite game $\mathcal{G}_n$, which is $\alpha_n$-approximation of $\mathcal{G}$ (Proposition \ref{ess_appro_exist}). Due to Nash's theorem, mixed equilibrium $(p_n, q_n)$ of $\mathcal{G}_n$ exists. So $(p_n, q_n)$ is $2\alpha_n$-equilibrium of $\mathcal{G}$ (Proposition \ref{epsilon_alpha}). By Proposition \ref{prop1}, $(p_n, q_n)$ has a convergent subsequence. For brevity, this convergent subsequence is denoted by $\{(p_n, q_n)\}$, which converges to $(p^*,q^*)$. Based on Proposition \ref{epsi_equil_converge}, we can know that $(p^*,q^*)$ is a mixed equilibrium of $\mathcal{G}$.
\end{proof}

\subsection{Analysis on the existence of NE in N-player ACCES games} \label{Appendix_A2}

%\textcolor{blue}{
%Based on the above proof in the two-player ACCES game, we can extend the existence of NE to the $N$-player setting ($n$ combinatorial players and $N-n$ continuous players).} %Here is the concrete analysis (readers can prove it according to the proof line under the two-player setting).}
Our propositions and Theorem 2 can be extended to the N-player ACCES games naturally. The key point of the existence of NE to N-player ACCES games is two fundamental properties we propose in ACCES games, weakly sequential compactness of the mixed strategy space and continuity of the expected utility function (Propositions 1 and 2), and the approximation idea by finite games. We introduce these as follows.

\begin{itemize}
    \item \textbf{Two Properties}: In Proposition 1, we transform the weakly sequential compactness of the joint mixed strategy space into the separability and weakly sequential compactness of each single player by Lemma 1. In Proposition 2, we scale the distance between two mixed strategies to the sum of distances between a single player’s mixed strategies while fixing other players. According to the proof of these two propositions, they are all independent of the number of players.
    \item \textbf{The Approximation idea by finite games}: The main idea is to approximate the infinite continuous strategy space by finite grids by definitions of approximate games and essentially finite games. The idea and definitions are not limited to the two-player setting.
\end{itemize}
%\citep{zhang2020converging}



\newpage
\section{Proofs in Section 5}\label{Appendix_B}
\begin{theorem}
    Given a two-player ACCESS game $\mathcal{G} = (X, Y,u)$, where $X$ is finite, $Y$ is a nonempty compact set, and the utility function $u$ is continuous in $Y$ when fixing the strategy in $X$, we have
    
    1. When $\epsilon = 0$, every weakly convergent subsequence in the subgame equilibrium pair sequence $\{(p_k^*, q_k^*)\}$ converges to the equilibrium of the whole game, possibly in an infinite number of iterations. 
    
    2. When $\epsilon > 0$, Algorithm \ref{alg:CCDO} converges to an $\epsilon$- equilibrium in a finite number of epochs.
\end{theorem}
\begin{proof}
    At every epoch $k$, denote the protagonist policy $x_{k+1} \in X$ and adversary policy output $y_{k+1} \in Y$ as best responses to mixed equilibrium $(p_k^*, q_k^*)$ in the $k_{th}$ subgame $(X_k, Y_k, U)$, i.e.
    $$x_{k+1} = \mathrm{argmax}_{x \in X} U(x, q_k^*), y_{k+1} = \mathrm{argmin}_{y\in Y} U(p_k^*, y),$$
    noting that all maximizers and minimizers exist due to the finiteness of $X$, compactness of $Y$, and continuity of $u$ when fixing variable $x$.
    
    First, prove the efficiency of the stopping criterion, i.e. output $(p_k^*, q_k^*)$, satisfying this criterion, must be $\epsilon$- mixed equilibrium of game $\mathcal{G}$. The stopping criterion $$U(x_{k+1}, q_k^*) - U(p_k^*, y_{k+1}) \leq \epsilon,$$
    implies that 
    \begin{eqnarray}\label{mini}
        \begin{aligned}
        U(p_k^*, q_k^*) \leq U(x_{k+1}, q_k^*) &\leq U(p_k^*, y_{k+1}) + \epsilon \\
        &= \min_{y \in Y} U(p_k^*, y) + \epsilon = \min_{q \in \bigtriangleup_{Y}} U(p_k^*, q) + \epsilon,
        \end{aligned}
    \end{eqnarray}
     which means that $\forall q \in \bigtriangleup_{Y}$, $U(p_k, q_k) \leq U(p_k, q) + \epsilon$. 
     
     Similarly, we can get that 
     \begin{eqnarray}\label{maxi}
        \begin{aligned}
        U(p_k^*, q_k^*) \geq U(p_k^*, y_{k+1}) &\geq U(x_{k+1}, q_k^*) - \epsilon \\
        &= \max_{x \in X} U(x, q_k^*) - \epsilon = \max_{p \in \bigtriangleup_{X}} U(p, q_k^*) - \epsilon,
        \end{aligned}
    \end{eqnarray}
    that is $\forall p \in \bigtriangleup_{X}, U(p_k^*, q_k^*) \geq u(p, q_k^*) - \epsilon$. Combined (\ref{mini}) and (\ref{maxi}), mixed equilibrium $(p_k^*, q_k^*)$, meeting the condition, is a $\epsilon$-mixed equilibrium.

    Next, we need to prove its convergence of mixed Nash equilibrium, in other words, this algorithm \ref{alg:CCDO} can reach the terminal condition. 
    
    When $\epsilon = 0$, due to Proposition \ref{prop1}, every sequence in $\bigtriangleup_{X \times Y}$ has its own weakly convergent subsequence. For the sequence $\{(p_k^*, q_k^*)\}$, whose element $(p_k^*, q_k^*)$ is the mixed equilibrium of the $k_{th}$ subgame, there exists a weakly convergent subsequence, for simplicity denoted the same indices, i.e. $\{(p_{k}^*, q_{k}^*)\}$ converges to $(p^*, q^*)$. 

    Due to that $(p_{k}^*, q_{k}^*)$ is an equilibrium of the subgame $(X_{k}, Y_{k}, U)$, so $\forall k$, $\forall y \in Y_{k}$, we can know that 
    $$U(p_{k}^*, q_{k}^*) \leq U(p_{k}^*, y).$$
    Take the limit on both sides of this inequality, based on the continuousness of U on $\bigtriangleup_{X \times Y}$, we can get that 
    \begin{align}\label{cl_y}
        U(p^*, q^*) \leq U(p^*,y), \forall y \in cl(\cup Y_{k}).
    \end{align}

    Because $Y$ is compact, so for sequence $\{y_{k}\}$ there exists $\hat{y} \in cl(\cup Y_{k})$ s.t. $y_{k} \rightarrow \hat{y}, k \rightarrow \infty$. 
    
    Besides, we can get that 
    \begin{align}\label{br_y}
        U(p_k^*, y_{k+1}) \leq U(p_k^*,y), \forall y \in Y,
    \end{align}
    based on the definition of best response, which can infer 
    \begin{align}\label{ine_y}
        U(p^*, q^*) \leq U(p^*, \hat{y}) \leq U(p^*,y), \forall y \in Y,
    \end{align}
    in which the left-hand inequality above follows inequality (\ref{cl_y}) and the right-hand gets by taking limits on inequality (\ref{br_y}).
    
    For the reason that the strategy space $X$ is finite and the number of iterations is infinite, in the weakly convergent subsequence,
    \begin{eqnarray}\label{finite_prf}
        \begin{aligned}
        \exists k_0, s.t. \forall k > k_0, U(p_{k}^*, q_{k}^*) &= \max_{x \in X_k } U(x, q_{k}^*)\\
        &= \max_{x \in X } U(x, q_{k}^*) = U(x_{k+1}, q_{k}^*),
        \end{aligned}
    \end{eqnarray}
    
    which means that $x_{k+1} \in X_{k}, \forall k > k_0$. Therefore we can get that, $\forall x \in X$,
    \begin{eqnarray}\label{ine_x}
       \begin{aligned}
        U(x, q^*_k) &\leq U(x_{k+1}, q^*_k) = U(p^*_k, q_k^*), \forall k > k_0,\\
        \Longrightarrow U(x, q^*) &\leq U(p^*, q^*).
       \end{aligned} 
    \end{eqnarray}
    So we have proven that $$U(x, q^*) \leq U(p^*, q^*) \leq U(p^*,y), \forall x \in X, \forall y \in Y.$$
    Due to an equivalent condition to NE, that is$(p^*, q^*)$ is an equilibrium if and only if it follows 
\begin{eqnarray}\label{NE_equ_def}
    \begin{aligned}
         U(x, q^*) \leq U(p^*, q^*) \leq U(p^*, y), \forall x \in X, y \in Y,
    \end{aligned}
\end{eqnarray}
    we can says that $(p^*, q^*)$ is an equilibrium of $\mathcal{G}$.

    If $\epsilon >0$, (\ref{ine_y}) imply that 
    \begin{align}
        U(p_k^*, y_{k+1}) \leq U(p_k^*, q_k^*) \Longrightarrow U(p^*, \hat{y}) \leq U(p^*, q^*).
    \end{align}
    Combined with inequality (\ref{ine_y}), we know that 
    \begin{align}\label{y_lim}
        U(p_k^*, y_{k+1}) \rightarrow U(p^*, \hat{y}) = U(p^*, q^*).
    \end{align}
    Due to the fact (\ref{finite_prf}), after $k_0$ iterations, the strategy space of $X_k$ will not be expanded.
    So we can get
    \begin{eqnarray}\label{x_lim}
        \begin{aligned}
            \lim_{k \rightarrow \infty} U(x_{k+1}, q_k^*) = \lim_{k \rightarrow \infty} U(p_k^*, q_k^*) = U(p^*, q^*).
        \end{aligned}
    \end{eqnarray}
    Therefore, utilizing (\ref{y_lim}) and (\ref{x_lim}), 
    \begin{eqnarray}\label{term_lim}
        \begin{aligned}
            U(x_{k+1}, q_k^*) - U(p^*_k, y_{k+1}) \rightarrow 0, k \rightarrow \infty.
        \end{aligned}
    \end{eqnarray}
    In other words, if $\epsilon > 0$, this iterated process must be terminated within limited rounds and the output is $\epsilon$- equilibrium with finite supports.
\end{proof}




\newpage
% \begin{theorem}
%     Given $\mathcal{G} = (X, Y, u)$, where $X$ is finite, $Y$ is a nonempty compact set, and the utility function $u$ is continuous in $Y$ when fixing the strategy in $X$, with $\epsilon_1$ best response oracle for Player 1 in $X$ and $\epsilon_2$ best response oracle for Player 2 in $Y$, we have
    
%     1. If terminating in finite iterations, then CCDOA and CCDO-RL on game $\mathcal{G}$ will converge to $(\epsilon+\epsilon_1+\epsilon_2)$- equilibrium. 
%     Especially, if the best response oracle for Player 2 has a lower bound for every mixed strategy in $\bigtriangleup_{X}$, i.e. 
%     \begin{eqnarray}\label{y_br_condi_app}
%         \begin{aligned}
%             \forall p \in \bigtriangleup_{X}, \exists \epsilon_Y, s.t. U(p, BR^{\epsilon_2}_{2}(p)) \geq \min_{y\in Y}U(p, y)+\epsilon_Y,
%         \end{aligned}
%     \end{eqnarray}
%     then CCDOA and CCDO-RL must converge to an $(\epsilon+\epsilon_1+\epsilon_2)$- equilibrium in a finite iterations.

%     2.  When $\epsilon=0$, if CCDOA and CCDO-RL produce infinite iterations, every weakly convergent subsequence converges to an $\epsilon_1$- equilibrium.

%     3. When $\epsilon>0$, for any form of approximate best response oracles, CCDOA and CCDO-RL can converge to a finitely supported $(\epsilon+\epsilon_1+\epsilon_2)$- equilibrium in a finite number of iterations.
% \end{theorem}
\begin{theorem}
    Given $\mathcal{G} = (X, Y, u)$, where $X$ is finite, $Y$ is a nonempty compact set, and the utility function $u$ is continuous in $Y$ when fixing the strategy in $X$, with $\epsilon_1$ best response oracle for Player 1 in $X$ and $\epsilon_2$ best response oracle for Player 2 in $Y$, we have
    
    1. When $\epsilon>0$, for any form of approximate best-response oracles, CCDOA and CCDO-RL can converge to a finitely supported $(\epsilon+\epsilon_1+\epsilon_2)$- equilibrium in a finite number of iterations.

    2. When $\epsilon=0$, if the approximate response oracle for Player 2 has a uniform lower bound for every mixed strategy in $\bigtriangleup_{X}$, i.e. 
    \begin{eqnarray}\label{y_br_condi_app}
        \begin{aligned}
            \forall p \in \bigtriangleup_{X}, \exists \epsilon_Y, s.t. U(p, BR^{\epsilon_2}_{2}(p)) \geq \min_{y\in Y}U(p, y)+\epsilon_Y,
        \end{aligned}
    \end{eqnarray}
    then CCDOA and CCDO-RL must converge to an $(\epsilon+\epsilon_1+\epsilon_2)$- equilibrium in a finite iterations.

    3.  When $\epsilon=0$, if CCDOA and CCDO-RL produce infinite iterations, every weakly convergent subsequence converges to an $\epsilon_1$- equilibrium.
\end{theorem}
\begin{proof}
    (1) Due to that 
    $$U(x_{k+1}^{\epsilon_1}, q_k^*) - U(p_k^*, y_{k+1}^{\epsilon_2}) \leq U(x_{k+1}, q_k^*) - U(p_k^*, y_{k+1}),$$
    combined with (\ref{term_lim}), the Algorithm \ref{alg:CCDOA} can stop in a finite number of iterations if $\epsilon > 0$.
    
    (2) Similarly with Theorem \ref{CCDO}, firstly we prove the output must be $(\epsilon + \epsilon_1 + \epsilon_2)$- equilibrium if satisfying the stopping termination. Based on the definition of $\epsilon$- best response, we can know that
    \begin{eqnarray}\label{eps_br}
    \begin{aligned}
        \forall q \in \bigtriangleup_{Y}, U(BR_1^{\epsilon_1}(q), q) \geq \max_{x \in X} U(x,q) - \epsilon_1, \\
        \forall p \in \bigtriangleup_{X}, U(p, BR_2^{\epsilon_2}(p)) \leq \min_{y \in Y} U(p, y) + \epsilon_2. 
    \end{aligned}
    \end{eqnarray}
    For simplicity, suppose $BR_1^{\epsilon_1}(q_k^*) \triangleq x_{k+1}^{\epsilon_1}, BR_2^{\epsilon_2}(p_k^*) \triangleq y_{k+1}^{\epsilon_2}$ for subgame equilibrium $(p_k^*, q_k^*)$. The iteration process stops means that $U(x_{k+1}^{\epsilon_1}, q_k^*) - U(p_k^*, y_{k+1}^{\epsilon_2}) \leq \epsilon$. So we can get that
    \begin{eqnarray}\label{termi_y}
    \begin{aligned}
        U(p_k^*, q_k^*) \leq \max_{x \in X} U(x, q_k^*) &\leq U(x_{k+1}^{\epsilon_1}, q_k^*) + \epsilon_1\\
        &\leq U(p_k^*, y_{k+1}^{\epsilon_2}) + \epsilon + \epsilon_1\\
        &\leq \min_{y \in Y} U(p_k^*, y) + \epsilon + \epsilon_1 + \epsilon_2\\
        &\leq U(p_k^*, y) + \epsilon + \epsilon_1 + \epsilon_2, \forall y \in Y.
    \end{aligned}
    \end{eqnarray}
    Similarly, we can prove the parallel results on $X$.
    \begin{eqnarray}\label{termi_x}
    \begin{aligned}
        U(p_k^*, q_k^*) \geq \min_{y \in Y} U(p_k^*, y) &\geq U(p_k^*, y_{k+1}^{\epsilon_2}) - \epsilon_2\\
        &\geq U(x_{k+1}^{\epsilon_1}, q_k^*) - \epsilon - \epsilon_1\\
        &\geq \max_{x \in X} U(x, q_k^*) - \epsilon - \epsilon_1 - \epsilon_2\\
        &\geq U(x, q_k^*) - \epsilon - \epsilon_1 - \epsilon_2, \forall x \in X.
    \end{aligned}
    \end{eqnarray}
    So combined with (\ref{termi_y}) and (\ref{termi_x}), 
    $$\forall x \in X, \forall y \in Y, U(x, q_k^*) - \Bar{\epsilon} \leq U(p_k^*, q_k^*) \leq U(p_k^*, y) + \bar{\epsilon},$$
    in which $\bar{\epsilon} = \epsilon + \epsilon_1 + \epsilon_2$. Hence $(p_k^*, q_k^*)$ is the $\bar{\epsilon}$- equilibrium of game $\mathcal{G}$.
    If the $\epsilon_2$- best response has a lower bound, which means that $\forall i$,
    \begin{eqnarray}
    \begin{aligned}
        U(p_k^*, y_{k+1}) + \epsilon_Y \leq U(p_k^*, y_{k+1}^{\epsilon_2}) \leq U(p_k^*, q_k^*),
    \end{aligned}
    \end{eqnarray}
    assume $y_{k+1}^{\epsilon_2} \rightarrow \hat{y}^{\epsilon_2}, k \rightarrow \infty$, if iterating infinitely, take limits on both sides, based on (\ref{y_lim}) we can get 
    \begin{eqnarray}
        \begin{aligned}
            U(p^*, q^*) + \epsilon_Y = U(p^*, \hat{y}) + \epsilon_Y \leq U(p^*, \hat{y}^{\epsilon_2}) \leq U(p^*, q^*).
        \end{aligned}
    \end{eqnarray}
    Obviously, it's a contradiction. Hence this iterated process must terminate in finite rounds.

    (3) When $\epsilon = 0$, considering that the algorithm \ref{alg:CCDOA} produces an infinite number of iterations, the stopping termination always stands up, i.e. $\forall k, U(x_{k+1}^{\epsilon_1}, q_k^*) - U(p_k^*, y_{k+1}^{\epsilon_2}) > 0$. Without consideration of the effect of approximate best responses' properties on the judgment of termination condition, resembling the proof of theorem 3, based on (\ref{ine_y}) and (\ref{ine_x}) we can get 
    \begin{eqnarray}
        \begin{aligned}
            U(p_k^*, q_k^*) \leq U(p_k^*, y_{k+1}^{\epsilon_2}) \leq U(p_k^*, y_{k+1}) + \epsilon_2 \leq U(p_k^*, y) + \epsilon_2, \forall y \in Y, \\
            U(x, q_k^*) - \epsilon_1 \leq \max_{x \in X}U(x, q_k^*) - \epsilon_1 \leq U(x_{k+1}^{\epsilon_1}, q_k^*) = U(p_k^*, q_k^*), k \leq K_0, \forall x\in X.
        \end{aligned}
    \end{eqnarray}
    hence taking limits on both sides
    \begin{eqnarray}
        \begin{aligned}
             U(x, q^*) - \epsilon_1 \leq U(p^*, q^*) \leq U(p^*, y) + \epsilon_2, \forall x \in X, y \in Y,
        \end{aligned}
    \end{eqnarray}
    according to (\ref{NE_equ_def}), \textbf{every weakly convergent subsequence converges to a $\bm{\max \{\epsilon_1, \epsilon_2\}}$- equilibrium}. 
    
    As the example taken in (\ref{y_br_condi_app}), not all forms of approximate best response can breach the termination condition in the whole iteration process.
    
    Because of the fact that the strategy space $X$ is finite, combined with (\ref{finite_prf}) we can get that 
    \begin{eqnarray}\label{xy_lim_eps}
        \begin{aligned}
            &U(p_k^*, q_k^*) - U(p_k^*, y_{k+1}^{\epsilon_2}) > 0, k > K_0.\\
            \Longrightarrow & U(p_k^*, y_{k+1}) \leq U(p_k^*, y_{k+1}^{\epsilon_2}) \leq U(p_k^*, q_k^*).
        \end{aligned}
    \end{eqnarray}
    Integrated with (\ref{finite_prf}), easily we can get 
    \begin{eqnarray}\label{y_lim_br}
        \begin{aligned}
            U(p_k^*, y_{k+1}^{\epsilon_2}) \rightarrow U(p^*, \hat{y}^{\epsilon_2}) = U(p^*, q^*) = U(p^*, \hat{y}).
        \end{aligned}
    \end{eqnarray}
    Define $\triangle_k^y \triangleq U(p_k^*, y_{k+1}^{\epsilon_2}) - U(p_k^*, y_{k+1})$. From (\ref{y_lim_br}) and (\ref{finite_prf}), the following result can be derived:
    \begin{eqnarray}\label{diff_eps_y}
    \begin{aligned}
        \triangle_k^y \rightarrow 0, k \rightarrow \infty.
    \end{aligned}
    \end{eqnarray}
    According to (\ref{xy_lim_eps}), 
    \begin{eqnarray}
        \begin{aligned}
            U(p_k^*, q_k^*) \leq U(p_k^*, y_{k+1}^{\epsilon_2}) = U(p_k^*, y_{k+1}) + \triangle_k^y \leq U(p_k^*, y) + \triangle_k^y,
        \end{aligned}
    \end{eqnarray}
    Take limits on both sides, 
    \begin{eqnarray}
        \begin{aligned}
            U(p^*, q^*) \leq U(p^*, y).
        \end{aligned}
    \end{eqnarray}
    Combined with (\ref{NE_equ_def}), we can get that in infinite iterations, \textbf{every weakly convergent subsequence will converge to $\bm{\epsilon_1}$- equilibrium}.
\end{proof}
\bigskip\bigskip\bigskip
\begin{theorem}\label{thm_complex}
    If the combinatorial optimization player employs the state-of-the-art approximate algorithm, whose computational complexity is polynomial with respect to the scale of the problem noted as $f(n, m, log K)$, the continuous adversarial player adopts LinUCB with $d$-dimension input, then the computational complexity of the algorithm is $O(p(f(n, m, log K) + Td^3))$ if $\epsilon > 0$.
\end{theorem}
\begin{proof}
    Providing that there exists a representative network that can compress the CO problem into $d$-dimension vector with full information, then the overall computational complexity is 
    $$\mathcal{O}(f(n, m, log K)) \cdot p + \mathcal{O}(Td^3) \cdot p = \mathcal{O}(p(f(n, m, log K) + Td^3)),$$
    in which $T$ is the number of iterations in LinUCB, $K$ is the largest value of the single item in the CO problem.
\end{proof}

Note that among common algorithms for solving combinatorial optimization problems—namely, approximate algorithms, heuristic algorithms, and reinforcement learning (RL) methods—only approximate algorithms have a complexity analysis and performance guarantee. For heuristics and RL methods, complexity analysis remains an open challenge. Hence, we choose approximate algorithms as the approximate best response. Additionally, considering the continuous strategy space of the continuous player, LinUCB is an appropriate algorithm for computing its best response.

\newpage
\section{Pseudocode of Algorithms} \label{Pseudo}
The pseudocode for CCDO and CCDOA is presented in Algorithms \ref{alg:CCDO} and \ref{alg:CCDOA}, respectively.

\bigskip\bigskip\bigskip\bigskip

\begin{algorithm}[!h]
    \caption{Combinatorial-Continuous Double Oracle Algorithm}
    \label{alg:CCDO}
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
    \renewcommand{\algorithmicensure}{\textbf{Output:}}
    \begin{algorithmic}[1]
        \REQUIRE Game $\mathcal{G} = (X, Y, u)$, $\epsilon \geq 0$.
        \ENSURE $(p_k^*, q_k^*)$.   %%output
        \STATE  Initialize strategy set $X_1$, $Y_1$.
        \REPEAT
            \STATE Solve the mixed equilibrium $(p_k^*, q_k^*)$ in the subgame $(X_k, Y_k)$.
            \STATE Find the best response $x_{k+1}, y_{k+1}$: 
               $x_{k+1} \in \mathbb{BR}_1(q_k^*), y_{k+1} \in \mathbb{BR}_2(p_k^*)$. \\
            \STATE $X_{k+1} = X_{k} \cup \{x_{k+1}\}$, $Y_{k+1} = Y_{k} \cup \{y_{k+1}\}$.
        \UNTIL{$U(x_{k+1}, q_k^*) - U(p_k^*, y_{k+1}) \leq \epsilon$}
    \end{algorithmic}
\end{algorithm}

% \begin{algorithm}[!h]
% 	\caption{Combinatorial-Continuous Double Oracle Algorithm}
% 	\label{alg:CCDO}
% 	\LinesNumbered
% 	\KwIn{Game $\mathcal{G} = (X, Y, u)$, $\epsilon \geq 0$.}
% 	\KwOut{$(p_k^*, q_k^*)$}
%      Initialize strategy set $X_1$, $Y_1$.\\
% 	\Repeat{$U(x_{k+1}, q_k^*) - U(p_k^*, y_{k+1}) \leq \epsilon$}{
%      Solve the mixed equilibrium $(p_k^*, q_k^*)$ in the subgame $(X_k, Y_k)$.\\
%      Find the best response $x_{k+1}, y_{k+1}$: 
%                $x_{k+1} \in \mathbb{BR}_1(q_k^*), y_{k+1} \in \mathbb{BR}_2(p_k^*)$. \\
%      $X_{k+1} = X_{k} \cup \{x_{k+1}\}$, $Y_{k+1} = Y_{k} \cup \{y_{k+1}\}$.\\
% }
% \end{algorithm}

\bigskip\bigskip\bigskip\bigskip\bigskip\bigskip\bigskip\bigskip

\begin{algorithm}[ht]
    \caption{Combinatorial-Continuous Double Oracle Approximate Algorithm}
    \label{alg:CCDOA}
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
    \renewcommand{\algorithmicensure}{\textbf{Output:}}
    \begin{algorithmic}[1]
        \REQUIRE Game $\mathcal{G} = (X, Y, u)$, $\epsilon \geq 0$.  %%input
        \ENSURE $\sigma_k^*$.   %%output
        \STATE  Initialize strategy set $\Pi_{1,0}$, $\Pi_{2,0}$.
        \REPEAT
            \STATE Solve the mixed equilibrium $(p_k^*, q_k^*)$ in the subgame $(X_k, Y_k)$.
            \STATE Find the $\epsilon_1$- best response, $\epsilon_2$- best response $x_{k+1}^{\epsilon_1}, y_{k+1}^{\epsilon_2}$: \\
               $x_{k+1} \in \mathbb{BR}_1^{\epsilon_1}(q_k^*), y_{k+1} \in \mathbb{BR}_2^{\epsilon_2}(p_k^*)$. \\
            \STATE $X_{k+1} = X_{k} \cup \{x_{k+1}\}$, $Y_{k+1} = Y_{k} \cup \{y_{k+1}\}$.
            \IF{$U(x_{k+1}^{\epsilon_1}, q_k^*) \leq U(p_k^*, q_k^*)$} 
                \STATE $x_{k+1}^{\epsilon_1} = x$ random in $p_k^*$, $X_{k+1} = X_{k}$.
            \ELSE
                \STATE $X_{k+1} = X_{k} \cup \{x_{k+1}^{\epsilon_1}\}$.
            \ENDIF
            \IF{$U(p_k^*, y_{k+1}^{\epsilon_2}) \geq U(p_k^*, q_k^*)$}
                \STATE $y_{k+1}^{\epsilon_2} = y$ random in $q_k^*$, $Y_{k+1} = Y_{k}$.
            \ELSE
                \STATE $Y_{k+1} = Y_{k} \cup \{y_{k+1}^{\epsilon_2}\}$.
            \ENDIF
        \UNTIL{$U(x_{k+1}^{\epsilon_1}, q_k^*) - U(p_k^*, y_{k+1}^{\epsilon_2}) \leq \epsilon$}
    \end{algorithmic}
\end{algorithm}

% \begin{algorithm}[ht]
% 	\caption{Combinatorial-Continuous Double Oracle Approximate Algorithm}
% 	\label{alg:CCDOA}
% 	\LinesNumbered
% 	\KwIn{Game $\mathcal{G} = (X, Y, u)$, $\epsilon \geq 0$.}
%         \KwOut{$\sigma_k^*$}
%      Initialize strategy set $X_1$, $Y_1$.\\
% 	\Repeat{$U(x_{k+1}^{\epsilon_1}, q_k^*) - U(p_k^*, y_{k+1}^{\epsilon_2}) \leq \epsilon$}{
%      Solve the mixed equilibrium $(p_k^*, q_k^*)$ in the subgame $(X_k, Y_k)$.\\
%      Find the $\epsilon_1$- best response, $\epsilon_2$- best response $x_{k+1}^{\epsilon_1}, y_{k+1}^{\epsilon_2}$: 
%                $x_{k+1} \in \mathbb{BR}_1^{\epsilon_1}(q_k^*), y_{k+1} \in \mathbb{BR}_2^{\epsilon_2}(p_k^*)$. \\
%      $X_{k+1} = X_{k} \cup \{x_{k+1}\}$, $Y_{k+1} = Y_{k} \cup \{y_{k+1}\}$.\\
%      \eIf{$U(x_{k+1}^{\epsilon_1}, q_k^*) \leq U(p_k^*, q_k^*)$}{
%         $x_{k+1}^{\epsilon_1} = x$ random in $p_k^*$, $X_{k+1} = X_{k}$.
%     }{$X_{k+1} = X_{k} \cup \{x_{k+1}^{\epsilon_1}\}$.}
%     \eIf{$U(p_k^*, y_{k+1}^{\epsilon_2}) \geq U(p_k^*, q_k^*)$}{
%         $y_{k+1}^{\epsilon_2} = y$ random in $q_k^*$, $Y_{k+1} = Y_{k}$
%     }{$Y_{k+1} = Y_{k} \cup \{y_{k+1}^{\epsilon_2}\}$.}
% }
% \end{algorithm}

% \begin{algorithm}[ht]
%     \caption{Combinatorial-Continuous Double Oracle Approximate Algorithm}
%     \label{alg:CCDOA}
%     \renewcommand{\algorithmicrequire}{\textbf{Input:}}
%     \renewcommand{\algorithmicensure}{\textbf{Output:}}
%     \begin{algorithmic}[1]
%         \REQUIRE Game $\mathcal{G} = (X, Y, u)$, $\epsilon \geq 0$.  %%input
%         \STATE  Initialize strategy set $X_1$, $Y_1$.
%         \REPEAT
%         \STATE Solve the mixed equilibrium $(p_k^*, q_k^*)$ in the subgame $(X_k, Y_k)$.
%         \STATE Find the $\epsilon_1$- best response, $\epsilon_2$- best response $x_{k+1}^{\epsilon_1}, y_{k+1}^{\epsilon_2}$: \\
%                $x_{k+1} \in \mathbb{BR}_1^{\epsilon_1}(q_k^*), y_{k+1} \in \mathbb{BR}_2^{\epsilon_2}(p_k^*)$.
%         \STATE $X_{k+1} = X_{k} \cup \{x_{k+1}\}$, $Y_{k+1} = Y_{k} \cup \{y_{k+1}\}$
%         \IF{$U(x_{k+1}^{\epsilon_1}, q_k^*) \leq U(p_k^*, q_k^*)$}
%         \STATE $x_{k+1}^{\epsilon_1} = x$ random in $p_k^*$, $X_{k+1} = X_{k}$
%         \ELSE
%         \STATE $X_{k+1} = X_{k} \cup \{x_{k+1}^{\epsilon_1}\}$.
%         \ENDIF
%         \IF{$U(p_k^*, y_{k+1}^{\epsilon_2}) \geq U(p_k^*, q_k^*)$}
%         \STATE $y_{k+1}^{\epsilon_2} = y$ random in $q_k^*$, $Y_{k+1} = Y_{k}$
%         \ELSE
%         \STATE $Y_{k+1} = Y_{k} \cup \{y_{k+1}^{\epsilon_2}\}$.
%         \ENDIF
%         \UNTIL{$U(x_{k+1}^{\epsilon_1}, q_k^*) - U(p_k^*, y_{k+1}^{\epsilon_2}) \leq \epsilon$}
%         \ENSURE $(p_k^*, q_k^*)$   %%output
%     \end{algorithmic}
% \end{algorithm}
\newpage
\section{Experiments Parameters and Settings} \label{app_ex_para_set}
\subsection{Parameters of CCDO-RL} 

\begin{table}[h]
    \caption{Parameters of CCDO-RL}
    \vspace{\baselineskip}
    \label{tab_hyper_ccdorl}
    \centering
    \begin{tabular}{lllllll}
    \toprule
    parameter & ACVRP20 & ACVRP50 & ACSP20 & ACSP50 & PG20 & PG50 \\
    \midrule
    Iteration & 26 & 35 & 24 & 35 & 13 & 12 \\  
    Batchsize (prog/adv) & 512 & 512 & 512 & 512 & 1024 & 1024 \\
    Prog training epoch & 10 & 25 & 10 & 20 & 150 & 150 \\
    Prog training decoder & sampling & sampling & sampling & sampling & sampling & Top\_p-sampling\\
    Prog eval/test decoder & greedy & greedy & greedy & greedy & greedy & beam-search\\
    Learning rate (prog) & 1e-4 &1e-4&1e-4&1e-4&2e-4&2.5e-4 \\
    Adv BR training epoch & 5 & 20 & 20 & 20 & 20 & 50 \\
    Learning rate (adv) & 1e-4 & 5e-5 & 5e-5 & 5e-5 & 5e-5 & 5e-5 \\
    Clip range (PPO in adv) & 0.2 & 0.2 & 0.2  & 0.2 & 0.2 & 0.2 \\
    Value Func $\lambda$ (PPO) & 0.5 & 0.5 & 0.5 & 0.5 & 0.5  & 0.5 \\
    Entropy $\lambda$ (PPO) & 0.01 & 0.0  & 0.0 & 0.0 & 0.0 & 0.0 \\
    Max gradient (PPO) & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 \\
    \bottomrule
    \end{tabular}
\end{table}

\subsection{Problem Setting}

\subsubsection{ACSP}

The adversarial covering salesman problem (ACSP) is one variant of the traveling salesman problem (TSP) with adversaries. The biggest difference is that each city $i$ has one coverage radius $r_i$. If some unvisited cities are covered by visited cities, then these unvisited are seen as being visited in the TSP. Hence the salesman in the ACSP aims to find the shortest path such that all cities have been visited or covered. However, due to some external factors like transportation situations, the coverage radius may be influenced. Similar to the influence model in (\ref{linrob}), the real coverage radius is
\begin{eqnarray}
    r_i = \hat{r}_i + \sum_{m,n=1}^K \phi_{i,m}\phi_{i,n}y_{mn}, 
\end{eqnarray}
where $\phi_{i,m}$ is the $m_{th}$ element of $\phi_i$, i.e. the transportation vector at the city $i$ correlated with the city's location, i.e. $\phi_i = \frac{1}{2} \phi \cdot loc_i$ where $\phi$ is one constant $K \times 2$ matrix. and $y_{mn}$ is the component of the environmental parameter matrix $y$ controlled by the adversary. Hence the objective function is like the ACVRP,
\begin{eqnarray}
    \begin{aligned}
        \min_{x \in X_{csp}} \max_{y \in [0, 1]^9} length(x),
    \end{aligned}
\end{eqnarray}
where $length$ is the summary distance from the start to end (two cities can be different), and the path $x$ should visit or cover all cities. 

\subsubsection{ACVRP}

The adversarial capacitated vehicle routing problem (ACVRP) is that there is one depot and one vehicle with constrained good capacity which starts and ends at the depot where the vehicle can supplement goods. The objective of this vehicle is to find the shortest path while satisfying all the demands of customers on the map. Each customer $i$ owns its two-dimension position $(x,y)$ and an estimated demand $\hat{d}_i$. According to the influence model (\ref{linrob}), the real demand $d_i$ is set as follows:
\begin{eqnarray}
    d_i = \hat{d_i} + \sum_{m,n=1}^K \omega_{i,m}\omega_{i,n}y_{mn}, 
\end{eqnarray}
in which $\omega_{i,m}$ is the $m_{th}$ element of $\omega_i$, i.e. the weather vector at the customer point $i$, and $\alpha_{mn}$ is the component of the environmental parameter matrix $\alpha$ controlled by the adversary. In our setting, we assume that the weather condition is related to the customer's location, i.e. $\omega_i = \frac{1}{2} \omega \cdot loc_i$ where $\omega$ is one constant $K \times 2$ matrix. Before the vehicle chooses the customer to deliver goods, it can only know the estimated demand $\hat{d}_i$. When arriving at the chosen customer, the vehicle knows the real demand of this customer. It should be noted that goods can't be split up. In other words, if the remaining capacity of goods can't satisfy the real demand of the chosen customer, the vehicle should come there again until its current capacity meets the real demand $d_i$. 

Hence the objective function of the vehicle is to minimize the maximal path $x$ under the changeable environment parameters $y \in [0, 1]^9$, meeting all the demands of customers
\begin{eqnarray}
    \begin{aligned}
        \min_{x \in X_{cvrp}} \max_{y \in [0, 1]^9} length(x),
    \end{aligned}
\end{eqnarray}
where $length$ is the summary distance from the start to end, adopting the Euclidean distance. 
% Here we set $K=3$, $\underline{a} = 0, \overline{a} = 1$ (identical in the following CSP and OP).

\subsubsection{PG}

About the PG, we set it as the classical security patrolling game with two players: one defender and one attacker. There are $N$ targets to protect, each one has an individual prize $v_i$ and an estimated attack probability $\hat{p}_i$ defined by objective factors. The defender tries to find a path to maximize the cumulative prize attained from some targets prevented from attacks successfully under the total distance constraint. For the attacker, it will decide the real attack probability of each target based on the estimated probability $\hat{p}_i$ and its objective is to reduce the cost of being caught, equivalent to reducing the total patrolling revenue of the defender. Identically, the attack probability on each target also follows the influence model in (\ref{linrob}), 
\begin{eqnarray}
    p_i = \hat{p}_i + \sum_{m,n=1}^K \xi_{i,m}\xi_{i,n}y_{mn}, 
\end{eqnarray}
where $\xi_i$ is the attacker's preference vector to the target $i$, to keep consistent with the two settings before, we still set the preference vector as related to the location, i.e. $\xi_i = \frac{1}{2} \xi \cdot loc_i$ where $\xi$ is one constant $K \times 2$ matrix. The objective function of this security game is
\begin{eqnarray}
    \begin{aligned}
        \max_{x \in X_{op}} \min_{y \in [0, 1]^9} \sum_{i=1}^{N} p_i v_i \mathbb{I}_{\Pi},
    \end{aligned}
\end{eqnarray}
where $\Pi$ is the set of patrolled targets on the path $x$.

\subsection{Attention Model \& Hyperparameters}

\subsubsection*{Instance generation}
For ACVRP and ACSP, we use the default data generated from RL4CO \citep{berto2023rl4co}.
For PG, we use the instances generated from a generator provided in the AI for TSP competition hosted at IJCAI21 \footnote{https://github.com/paulorocosta/ai-for-tsp-competition}.  
% [引用的这个github库：https://github.com/paulorocosta/ai-for-tsp-competition]

\subsubsection*{Model Architecture}

For the protagonist of three COPs, we use the REINFORCE algorithm with the Attention network used by possessing the graph information in RL4CO. About decoders, we use the static embedding for ACVRP and PG provided in RL4CO \citep{berto2023rl4co}, and a dynamic embedding for ACSP from \citep{li2021deep}.

About the encoder, three instances all adopt the attention network with 8 heads while the number of the network layer is 3 for ACSP and ACVRP, and 5 for PG. Different COPs have their own state and context of the environment, input details for three COPs are as follows:
\begin{itemize}
    \item \textbf{ACVRP:} The environment context is each customer's location, demand, and weather vector. The state at time slot $t$ is the current location and remaining capacity of the vehicle.
    \item \textbf{ACSP:} The environment context is each city's location, estimated coverage, and transportation vector. The state at time slot $t$ is the current location( the chosen node to visit at the last time slot) and the start point.
    \item \textbf{PG:} The environment context is each target's location, estimated attack probability, and prize. The state at time slot $t$ is the start point and the current location of the defender.
\end{itemize}

\section{Discussion on scalability and potential optimization of CCDO-RL}

\subsection{Scalability Analysis of CCDO-RL}

In CCDO-RL, three components need to be trained or computed:
\begin{enumerate}
    \item The combinatorial player’s policy. This player solves a combinatorial optimization problem (COPs) under a specific strategy of the adversary.
    \item The continuous player (as the adversary) with an infinite continuous strategy space. 
    \item The computation of Mixed Nash Equilibria (NE) in the subgame.
\end{enumerate}

Next, we will analyze the computation time for each component individually, from both theoretical and experimental perspectives. For the experimental part, we will use the 50-node Patrolling Game (PG) scenario, which is the most challenging problem instance in our experiments, as an example.
\begin{enumerate}
    \item The combinatorial player is trained using Graph Neural Networks (GNN) and REINFORCE to find feasible and optimal solutions for NP-complete COPs. This complexity requires reinforcement learning to invest more time and data for effective model training. In the experiment, training a stable and high-performing combinatorial model takes 26 minutes (10000 data, 1024 batch size, 150 epochs) with the continuous player fixed.
    \item The continuous player is trained by PPO to tackle a one-step problem with a continuous objective function building on strategies of the combinatorial player. It still utilizes GNN to understand graph structure. One action per episode reduces training times compared to the combinatorial player while achieving similar approximate error levels. In our experiments, training a high-performing model takes only 4 to 5 minutes (10000 data, 1024 batch size, and 50 epochs), less than one-fifth of the time required for the combinatorial player. 
    \item For the NE solution, the mixed equilibria in a zero-sum game can be solved by the linear programming method which has polynomial complexity in the size of the game tree. From the perspective of theoretical complexity and experiment implementation, the computational time is negligible (less than 2s).
\end{enumerate}

From the statement above, we can conclude that more than five-sixths of the computation time is spent training the model or strategy of the combinatorial player. Therefore, a crucial aspect of addressing the scalability issue is to enhance the speed of solving the Combinatorial Optimization Problems (COPs) using Reinforcement Learning (RL).


\subsection{Potential Optimization of Scalability}

In this subsection, we briefly discuss two main aspects as potential ways of improvement.

\textbf{COPs Simplification Method}
\begin{enumerate}
    \item The pruning method: this one was introduced in the original scale of COPs to reduce the number of possibly useful actions. In this way, the computational burden will be decreased \citep{manchanda2019learning,lauri2023learning}.
    \item Broken down into subproblems: in some concrete COPs like TSP \citep{fu2021generalize}, and VRP \citep{hou2023generalize}, the originally large-scale problem can be broken down into smaller problems to solve, thereby reducing the solution difficulty.
\end{enumerate}

\textbf{RL algorithms}
\begin{enumerate}
    \item Learning Time Reduction: increase the sampling data quality by attaining good-performance data from pre-trained RL models or heuristic algorithms on COPs (seemingly like the model-based RL). 
    \item NN Model Adjustment: most constructive neural network fitting combinatorial optimization can not solve problems with large-scale instance sizes. One feasible way is to design an NN model with strong scalability which means that the trained model on small-scale problem instances can be used on large-scale ones, such as in influence maximization \citep{chen2023touplegdd}. 
    \item Distributed training: reduces the time required for training by splitting the computational workload across multiple devices.
\end{enumerate}

\subsection{Experiment results of CCDO-RL's scalability}

We test the CCDO-RL model (trained on 50-node graphs) on larger CSP and PG scenarios. On unseen 100-node and 200-node graphs (100 of each type), CCDO-RL outperformed other baselines while requiring significantly less test time compared to the heuristic algorithm (especially in CSP), as demonstrated in Tables \ref{tab_scale}.

\begin{table}[htb]
  \caption{Scalability results on ACSP and PG (smaller is better in ACSP, larger is better in PG)}
  \vspace{\baselineskip}
  \label{tab_scale}
  \centering
  \small
  \begin{tabular}{lllll}
    \toprule
    \multirow{2}{*}{method} & \multicolumn{2}{c}{CSP} & \multicolumn{2}{c}{PG} \\
    \cmidrule(r){2-3} \cmidrule{4-5} & 100 nodes & 200 nodes & 100 nodes & 200 nodes \\
    \midrule

    Heuristic & 7.38 (5h 46mins) & 6.95 (7h 16mins) & 7.71 (53s) & 11.01 (120s) \\
    RL against Stoc & 7.34  & 9.86  & 7.83  & 9.24 \\
    CCDO-RL   & $\pmb{4.61}$ & $\pmb{4.89}$ & $\pmb{8.42}$  & $\pmb{11.07}$ \\
    \bottomrule
  \end{tabular}
\end{table}



\clearpage\newpage
\section{Additional Experimental Results}\label{app_exp}
\subsection{Convergence to NE in 50-Node Graphs}

All experiments on three COPs are implemented in Python and conducted on two machines. One is NVIDIA GeForce RTX 4090 GPU and Intel 24GB 24-Core i9-13900K. The other is NVIDIA V100 GPU and Inter 256 GB 32-Core Xeon E5-2683 v4.
% Our code is open on the GitHub \url{https://anonymous.4open.science/r/acces_games-952D}.

Illustrated by Fig. \ref{fig_exploit_50}, we observe that CCDO-RL also converges close to the real NE in 35 iterations for ACSP and ACVRP, and for PG it takes 12 iterations. Their runtimes are 10h 20mins, 4h 40 mins, and 9h 6mins respectively. Exploitability of ACSP, ACVRP, and PG are 0.06, 0.27, and 0.13 respectively. The phenomenon that exploitabilities on three COPs of 50 nodes are all larger than that on the 20-node map is reasonable and acceptable because the hardness of solving solutions grows exponentially on NP-hard problems, such as these three. 
\vspace{-3mm}
\begin{figure}[htbp]
	\centering
    \subfigure[ACSP50]{
    \label{csp50_nashconv}
    \includegraphics[scale=0.20]{Figures/nashconv_log_csp50_sm_7.eps}
    }
    \subfigure[ACVRP50]{
    \label{cvrp50_nashconv}%文中引用该图片代号
    \includegraphics[scale=0.20]{Figures/nashconv_log_svrp50_sm_7.eps}
    }
    \subfigure[PG50]{
    \label{opsa50_nashconv}
    \includegraphics[scale=0.20]{Figures/nashconv_log_pg50_sm_7.eps}
    }
    \caption{Exploitability on Three COPs of 50 Nodes}
    \label{fig_exploit_50}
\end{figure}

% \subsection{Payoff Metrics of three COPs}
% In \citep{lanctot2017unified}, the \textbf{joint policy correlation (JPC)} is introduced to depict the overfitting degree of reinforcement learning algorithms. To describe the overfitting situation, an average propositional loss from the JPC matrix are proposed. The concrete formulation of the average proportional loss is 
% \begin{equation}
%     R\_ = (\hat{D}-\hat{O})/\hat{D},
% \end{equation}
% in which $\hat{D}$ is the mean value of the diagonals and $\hat{O}$ is the mean value of the off-diagonals in the JPC matrix.

% \begin{figure}[htbp]
% 	\centering
%     \subfigure[CVRP20 Payoff Metric]{
%     \label{cvrp20_payoff}%文中引用该图片代号
%     \includegraphics[scale=0.35]{Figures/JPC_log_svrp20.eps}
%     }
%     % \hspace{0.2in}
%     \subfigure[CVRP50 Payoff Metric]{
%     \label{cvrp50_payoff}%文中引用该图片代号
%     \includegraphics[scale=0.35]{Figures/JPC_log_svrp50.eps}
%     }
%     \subfigure[CSP20 Payoff Metric]{
%     \label{csp20_payoff}
%     \includegraphics[scale=0.35]{Figures/JPC_log_csp20.eps}
%     }
%     \subfigure[CSP50 Payoff Metric]{
%     \label{csp50_payoff}
%     \includegraphics[scale=0.35]{Figures/JPC_log_csp50.eps}
%     }
%     % \hspace{0.2in}
%     \subfigure[OP20 Payoff Metric]{
%     \label{op20_payoff}
%     \includegraphics[scale=0.35]{Figures/JPC_log_opsa20.eps}
%     }
%     \subfigure[OP50 Payoff Metric]{
%     \label{op50_payoff}
%     \includegraphics[scale=0.35]{Figures/JPC_log_opsa50.eps}
%     }
%     \caption{Payoff Metrics}
%     \label{payoff_metric}
% \end{figure}

\subsection{Full results of CCDORL} \label{app_rob_machine}
Here we replenish and analyze results against the stochastic adversary on three COPs. "Adv" or "no adv" columns in following tables indicates whether all instances are influenced by the learned adversary or a random adversary, respectively.

From "no adv" columns in Tables \ref{tab_csp_full_20} and \ref{tab_csp_full_50} (ACSP), we can see the average reward (seen graphs) and generalizability (unseen graphs) of the combinatorial player trained in CCDO-RL are both better than others, even though the RL baseline is trained against the stochastic adversary solely. The average improvement against RL baseline is 3.96\% on different types of graphs and different nodes. Similarly under the ACVRP and PG settings, average improvements against are 3.88\% and 2.72\% respectively. We can find CCDO-RL can also get the better reward under the usual stochastic setting, not just the adversarial setting.

\begin{table}[htb]
  \caption{Full results on ACSP in 20 nodes (smaller values are better)}
  \vspace{\baselineskip}
  \label{tab_csp_full_20}
  \centering
  \small
  \begin{tabular}{lllll}
    \toprule
    \multirow{2}{*}{method} & \multicolumn{2}{c}{seen graphs} & \multicolumn{2}{c}{unseen graphs} \\
    \cmidrule(r){2-3} \cmidrule{4-5} & no adv & adv & no adv & adv \\
    \midrule

    Heuristic & 6.17$\pm$1.23 & 6.13$\pm$1.20   & 6.03$\pm$1.30  & 6.20$\pm$1.33 \\
    RL against Stoc & 3.39$\pm$0.46  & 3.50$\pm$0.47  & 3.39$\pm$0.46  & 3.56$\pm$0.37 \\
    CCDO-RL   & $\pmb{3.18}$$\pm$0.44 & $\pmb{3.25}$$\pm$0.42  & $\pmb{3.19}$$\pm$0.41  & $\pmb{3.31}$$\pm$0.35 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[h]
  \caption{Full results on ACSP in 50 nodes (smaller values are better)}
  \vspace{\baselineskip}
  \label{tab_csp_full_50}
  \centering
  \small
  \begin{tabular}{lllll}
    \toprule
    \multirow{2}{*}{method} & \multicolumn{2}{c}{seen graphs} & \multicolumn{2}{c}{unseen graphs} \\
    \cmidrule(r){2-3} \cmidrule{4-5} & no adv & adv & no adv & adv \\
    \midrule

    Heuristic & 7.50$\pm$1.50 & 7.55$\pm$1.42 & 7.57$\pm$1.49 & 7.60$\pm$1.37 \\
    RL against Stoc & 4.29$\pm$0.61 & 4.55$\pm$0.62 & 4.20$\pm$0.56 & 4.57$\pm$0.58 \\
    CCDO-RL  & $\pmb{4.16}$$\pm$0.48 & $\pmb{4.31}$$\pm$0.51 & $\pmb{4.17}$$\pm$0.48 & $\pmb{4.39}$$\pm$0.52 \\
    \bottomrule
  \end{tabular}
\end{table}
\vspace{\baselineskip}

% \begin{table}[h]
%   \caption{Full results on ACSP (smaller values are better)}
%   \label{tab_csp_full}
%   \centering
%   \small
%   \begin{tabular}{lllllllll}
%     \toprule
%     \multirow{3}{*}{method} & \multicolumn{4}{c}{20 nodes (Mean$\pm$Std)} & \multicolumn{4}{c}{50 nodes (Mean$\pm$Std)} \\
%     \cmidrule(r){2-5} \cmidrule{6-9}
%                             & \multicolumn{2}{c}{seen graphs} & \multicolumn{2}{c}{unseen graphs} & \multicolumn{2}{c}{seen graphs} & \multicolumn{2}{c}{unseen graphs} \\
%     \cmidrule(r){2-3} \cmidrule{4-5} \cmidrule(r){6-7} \cmidrule{8-9}
%                             & no adv & adv & no adv & adv & no adv & adv & no adv & adv \\
%     \midrule

%     Heuristic & 6.17$\pm$1.23 & 6.13$\pm$1.20   & 6.03$\pm$1.30  & 6.20$\pm$1.33 & 7.50$\pm$1.50 & 7.55$\pm$1.42 & 7.57$\pm$1.49 & 7.60$\pm$1.37     \\
%     RL vs. Stoc & 3.39$\pm$0.46  & 3.50$\pm$0.47  & 3.39$\pm$0.46  & 3.56$\pm$0.37 & 4.29$\pm$0.61 & 4.55$\pm$0.62 & 4.20$\pm$0.56 & 4.57$\pm$0.58  \\
%     Ours   & $\pmb{3.18}$$\pm$0.44 & $\pmb{3.25}$$\pm$0.42  & $\pmb{3.19}$$\pm$0.41  & $\pmb{3.31}$$\pm$0.35 & $\pmb{4.16}$$\pm$0.48 & $\pmb{4.31}$$\pm$0.51 & $\pmb{4.17}$$\pm$0.48 & $\pmb{4.39}$$\pm$0.52 \\
%     \bottomrule
%   \end{tabular}
% \end{table}

% \begin{table}[h]
%   \caption{Full results on ACSP (smaller values are better)}
%   \label{tab_csp_full}
%   \centering
%   \small
%   \begin{tabular}{lllllllll}
%     \toprule
%     \multirow{3}{*}{method} & \multicolumn{4}{c}{20 nodes (Mean$\pm$Std)} & \multicolumn{4}{c}{50 nodes (Mean$\pm$Std)} \\
%     \cmidrule(r){2-5} \cmidrule{6-9}
%                             & \multicolumn{2}{c}{seen graphs} & \multicolumn{2}{c}{unseen graphs} & \multicolumn{2}{c}{seen graphs} & \multicolumn{2}{c}{unseen graphs} \\
%     \cmidrule(r){2-3} \cmidrule{4-5} \cmidrule(r){6-7} \cmidrule{8-9}
%                             & no adv & adv & no adv & adv & no adv & adv & no adv & adv \\
%     \midrule
%     LS1 & 4.56$\pm$0.94  & 4.52$\pm$0.71 & 4.56$\pm$0.94  & 4.53$\pm$0.79 & 6.02$\pm$0.94 & 5.98$\pm$0.94 & 5.98$\pm$0.99 & 5.95$\pm$0.96   \\
%     LS2 & 6.17$\pm$1.23 & 6.13$\pm$1.20   & 6.03$\pm$1.30  & 6.20$\pm$1.33 & 7.50$\pm$1.50 & 7.55$\pm$1.42 & 7.57$\pm$1.49 & 7.60$\pm$1.37     \\
%     RL     & 3.39$\pm$0.46  & 3.50$\pm$0.47  & 3.39$\pm$0.46  & 3.56$\pm$0.37 & 4.29$\pm$0.61 & 4.55$\pm$0.62 & 4.20$\pm$0.56 & 4.57$\pm$0.58  \\
%     Ours   & $\pmb{3.18}$$\pm$0.44 & $\pmb{3.25}$$\pm$0.42  & $\pmb{3.19}$$\pm$0.41  & $\pmb{3.31}$$\pm$0.35 & $\pmb{4.16}$$\pm$0.48 & $\pmb{4.31}$$\pm$0.51 & $\pmb{4.17}$$\pm$0.48 & $\pmb{4.39}$$\pm$0.52 \\
%     \bottomrule
%   \end{tabular}
% \end{table}

\begin{table}[htb]
  \caption{Full results on ACVRP in 20 nodes (smaller values are better)}
  \vspace{\baselineskip}
  \label{tab_cvrp_full_20}
  \centering
  \small
  \begin{tabular}{lllll}
    \toprule
    \multirow{2}{*}{method} & \multicolumn{2}{c}{seen graphs} & \multicolumn{2}{c}{unseen graphs}  \\
    \cmidrule(r){2-3} \cmidrule{4-5} 
                            & no adv & adv & no adv & adv \\
    \midrule
   
    Heuristic & 7.50$\pm$1.36 &  7.65$\pm$1.23  & 7.74$\pm$1.30  & 7.64$\pm$1.30  \\
    RL against Stoc & 7.68$\pm$1.32  & 7.55$\pm$1.16  & 7.70$\pm$1.30  & 7.67$\pm$1.30 \\
    CCDO-RL   & $\pmb{7.43}$$\pm$1.26 & $\pmb{7.42}$$\pm$1.21  & $\pmb{7.62}$$\pm$1.30  & $\pmb{7.55}$$\pm$1.28 \\
    \bottomrule
  \end{tabular}
\end{table}
\vspace{\baselineskip}

\begin{table}[htb]
  \caption{Full results on ACVRP in 50 nodes (smaller values are better)}
  \vspace{\baselineskip}
  \label{tab_cvrp_full_50}
  \centering
  \small
  \begin{tabular}{lllll}
    \toprule
    \multirow{2}{*}{method} & \multicolumn{2}{c}{seen graphs} & \multicolumn{2}{c}{unseen graphs}  \\
    \cmidrule(r){2-3} \cmidrule{4-5} 
                            & no adv & adv & no adv & adv \\
    \midrule
   
    Heuristic & 13.22$\pm$1.75 & 13.38$\pm$1.70 & 13.45$\pm$1.67 & 13.27$\pm$1.87 \\
    RL against Stoc & 13.89$\pm$1.85 & 13.90$\pm$1.63 & 13.95$\pm$1.70 & 13.85$\pm$1.53 \\
    CCDO-RL  & $\pmb{13.14}$$\pm$1.72 & $\pmb{13.28}$$\pm$1.52 & $\pmb{13.14}$$\pm$1.72 & $\pmb{13.15}$$\pm$1.59 \\
    \bottomrule
  \end{tabular}
\end{table}
\vspace{\baselineskip}
% \begin{table}[htb]
%   \caption{Full results on ACVRP (smaller values are better)}
%   \label{tab_cvrp_full}
%   \centering
%   \small
%   \begin{tabular}{lllllllll}
%     \toprule
%     \multirow{3}{*}{method} & \multicolumn{4}{c}{20 nodes (Mean$\pm$Std)} & \multicolumn{4}{c}{50 nodes (Mean$\pm$Std)} \\
%     \cmidrule(r){2-5} \cmidrule{6-9}
%                             & \multicolumn{2}{c}{training} & \multicolumn{2}{c}{test} & \multicolumn{2}{c}{training} & \multicolumn{2}{c}{test} \\
%     \cmidrule(r){2-3} \cmidrule{4-5} \cmidrule(r){6-7} \cmidrule{8-9}
%                             & no adv & adv & no adv & adv & no adv & adv & no adv & adv \\
%     \midrule
   
%     Heuristic & 7.50$\pm$1.36 &  7.65$\pm$1.23  & 7.74$\pm$1.30  & 7.64$\pm$1.30 & 13.22$\pm$1.75 & 13.38$\pm$1.70 & 13.45$\pm$1.67 & 13.27$\pm$1.87     \\
%     RL vs. Stoc & 7.68$\pm$1.32  & 7.55$\pm$1.16  & 7.70$\pm$1.30  & 7.67$\pm$1.30 & 13.89$\pm$1.85 & 13.90$\pm$1.63 & 13.95$\pm$1.70 & 13.85$\pm$1.53  \\
%     Ours   & $\pmb{7.43}$$\pm$1.26 & $\pmb{7.42}$$\pm$1.21  & $\pmb{7.62}$$\pm$1.30  & $\pmb{7.55}$$\pm$1.28 & $\pmb{13.14}$$\pm$1.72 & $\pmb{13.28}$$\pm$1.52 & $\pmb{13.14}$$\pm$1.72 & $\pmb{13.15}$$\pm$1.59 \\
%     \bottomrule
%   \end{tabular}
% \end{table}

% \begin{table}[htb]
%   \caption{Full results on ACVRP (smaller values are better)}
%   \label{tab_cvrp_full}
%   \centering
%   \small
%   \begin{tabular}{lllllllll}
%     \toprule
%     \multirow{3}{*}{method} & \multicolumn{4}{c}{20 nodes (Mean$\pm$Std)} & \multicolumn{4}{c}{50 nodes (Mean$\pm$Std)} \\
%     \cmidrule(r){2-5} \cmidrule{6-9}
%                             & \multicolumn{2}{c}{training} & \multicolumn{2}{c}{test} & \multicolumn{2}{c}{training} & \multicolumn{2}{c}{test} \\
%     \cmidrule(r){2-3} \cmidrule{4-5} \cmidrule(r){6-7} \cmidrule{8-9}
%                             & no adv & adv & no adv & adv & no adv & adv & no adv & adv \\
%     \midrule
%     CW & 7.49$\pm$1.55  & 7.64$\pm$1.56 & 7.69$\pm$1.49  & 7.55$\pm$1.39 & 13.36$\pm$2.14 & 13.49$\pm$2.10 & 13.30$\pm$2.26 & 13.35$\pm$2.04   \\
%     Tabu   & 7.50$\pm$1.36 &  7.65$\pm$1.23  & 7.74$\pm$1.30  & 7.64$\pm$1.30 & 13.22$\pm$1.75 & 13.38$\pm$1.70 & 13.45$\pm$1.67 & 13.27$\pm$1.87     \\
%     RL     & 7.68$\pm$1.32  & 7.55$\pm$1.16  & 7.70$\pm$1.30  & 7.67$\pm$1.30 & 13.89$\pm$1.85 & 13.90$\pm$1.63 & 13.95$\pm$1.70 & 13.85$\pm$1.53  \\
%     Ours   & $\pmb{7.43}$$\pm$1.26 & $\pmb{7.42}$$\pm$1.21  & $\pmb{7.62}$$\pm$1.30  & $\pmb{7.55}$$\pm$1.28 & $\pmb{13.14}$$\pm$1.72 & $\pmb{13.28}$$\pm$1.52 & $\pmb{13.14}$$\pm$1.72 & $\pmb{13.15}$$\pm$1.59 \\
%     \bottomrule
%   \end{tabular}
% \end{table}


\begin{table}
\centering
  \caption{Full results on PG in 20 nodes (larger values are better)}
  \vspace{\baselineskip}
  \label{tab_op_full_20}
  \small
  \begin{tabular}{lllll}
    \toprule
    \multirow{2}{*}{method} & \multicolumn{2}{c}{seen graphs} & \multicolumn{2}{c}{unseen graphs}  \\
    \cmidrule(r){2-3} \cmidrule{4-5} 
                            & no adv & adv & no adv & adv \\
    \midrule
    Heuristic & 2.70$\pm$1.15 & 2.64$\pm$1.03   & 2.64$\pm$1.18  & 2.43$\pm$0.98 \\
    RL against Stoc & $\pmb{2.81}$$\pm$1.25  & 2.71$\pm$0.90  & 2.71$\pm$1.35  & 2.50$\pm$0.95 \\
    CCDO-RL  & 2.75$\pm$1.06 & $\pmb{2.75}$$\pm$0.87  & $\pmb{2.77}$$\pm$1.19  & $\pmb{2.56}$$\pm$0.92 \\
    \bottomrule
  \end{tabular}
\end{table}
\vspace{\baselineskip}

\begin{table}
\centering
  \caption{Full results on PG in 50 nodes (larger values are better)}
  \vspace{\baselineskip}
  \label{tab_op_full_50}
  \small
  \begin{tabular}{lllll}
    \toprule
    \multirow{2}{*}{method} & \multicolumn{2}{c}{seen graphs} & \multicolumn{2}{c}{unseen graphs}  \\
    \cmidrule(r){2-3} \cmidrule{4-5} 
                            & no adv & adv & no adv & adv \\
    \midrule
    Heuristic & 4.69$\pm$1.81 & 4.53$\pm$1.84 & 4.47$\pm$2.02 & 4.19$\pm$1.69 \\
    RL against Stoc & 4.87$\pm$2.75 & 4.80$\pm$2.18 & 4.58$\pm$2.42 & 4.26$\pm$2.17 \\
    CCDO-RL & $\pmb{5.12}$$\pm$1.97 & $\pmb{5.01}$$\pm$1.91 & $\pmb{4.84}$$\pm$2.16 & $\pmb{4.70}$$\pm$1.94 \\
    \bottomrule
  \end{tabular}
\end{table}


% \begin{table}
% \centering
%   \caption{Full results on PG (larger values are better)}
%   \label{tab_op_full}
%   \small
%   \begin{tabular}{lllllllll}
%     \toprule
%     \multirow{3}{*}{method} & \multicolumn{4}{c}{20 nodes (Mean$\pm$Std)} & \multicolumn{4}{c}{50 nodes (Mean$\pm$Std)} \\
%     \cmidrule(r){2-5} \cmidrule{6-9}
%                             & \multicolumn{2}{c}{training} & \multicolumn{2}{c}{test} & \multicolumn{2}{c}{training} & \multicolumn{2}{c}{test} \\
%     \cmidrule(r){2-3} \cmidrule{4-5} \cmidrule(r){6-7} \cmidrule{8-9}
%                             & no adv & adv & no adv & adv & no adv & adv & no adv & adv \\
%     \midrule
%     Heuristic & 2.70$\pm$1.15 & 2.64$\pm$1.03   & 2.64$\pm$1.18  & 2.43$\pm$0.98 & 4.69$\pm$1.81 & 4.53$\pm$1.84 & 4.47$\pm$2.02 & 4.19$\pm$1.69 \\
%     RL vs. Stoc & $\pmb{2.81}$$\pm$1.25  & 2.71$\pm$0.90  & 2.71$\pm$1.35  & 2.50$\pm$0.95 & 4.87$\pm$2.75 & 4.80$\pm$2.18 & 4.58$\pm$2.42 & 4.26$\pm$2.17 \\
%     Ours    & 2.75$\pm$1.06 & $\pmb{2.75}$$\pm$0.87  & $\pmb{2.77}$$\pm$1.19  & $\pmb{2.56}$$\pm$0.92 & $\pmb{5.12}$$\pm$1.97 & $\pmb{5.01}$$\pm$1.91 & \pmb{4.84}$\pm$2.16 & $\pmb{4.70}$$\pm$1.94 \\
%     \bottomrule
%   \end{tabular}
% \end{table}

% \begin{table}
%   \caption{Full results on PG (larger values are better)}
%   \label{tab_op_full}
%   \centering
%   \small
%   \begin{tabular}{lllllllll}
%     \toprule
%     \multirow{3}{*}{method} & \multicolumn{4}{c}{20 nodes (Mean$\pm$Std)} & \multicolumn{4}{c}{50 nodes (Mean$\pm$Std)} \\
%     \cmidrule(r){2-5} \cmidrule{6-9}
%                             & \multicolumn{2}{c}{training} & \multicolumn{2}{c}{test} & \multicolumn{2}{c}{training} & \multicolumn{2}{c}{test} \\
%     \cmidrule(r){2-3} \cmidrule{4-5} \cmidrule(r){6-7} \cmidrule{8-9}
%                             & no adv & adv & no adv & adv & no adv & adv & no adv & adv \\
%     \midrule
%     LS & 2.78$\pm$1.09  & 2.71$\pm$1.10 & 2.74$\pm$1.24  & 2.52$\pm$1.08 & 1.78$\pm$1.71 & 1.82$\pm$1.40 & 1.83$\pm$1.61 & 1.86$\pm$1.44   \\
%     Greedy & 2.70$\pm$1.15 & 2.64$\pm$1.03   & 2.64$\pm$1.18  & 2.43$\pm$0.98 & 1.63$\pm$1.09 & 1.47$\pm$0.99 & 1.58$\pm$1.13 & 1.52$\pm$1.20   \\
%     RL     & $\pmb{2.81}$$\pm$1.25  & 2.71$\pm$0.90  & 2.71$\pm$1.35  & 2.50$\pm$0.95 & 1.99$\pm$1.44 & 1.54$\pm$1.03 & 1.47$\pm$2.32 & 1.03$\pm$5.05  \\
%     Ours    & 2.75$\pm$1.06 & $\pmb{2.75}$$\pm$0.87  & $\pmb{2.77}$$\pm$1.19  & $\pmb{2.56}$$\pm$0.92 & 1.99$\pm$1.44 & 1.87$\pm$1.22 & 1.28$\pm$5.10 & 1.35$\pm$5.09 \\
%     \bottomrule
%   \end{tabular}
% \end{table}

