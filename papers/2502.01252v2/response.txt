\section{Related Work}
% related works可以并不长，主要是要分明一些，每个paragraph都恰到好处。

% Myerson, "Game Theory: Analysis of Conflict" proposes a new theory to decompose one asymmetric game into two independent symmetric games. Osborne and Rubinstein, "Bargaining and Markets" extended the meta-game analysis from symmetric to asymmetric games.

\textbf{Symmetric and asymmetric games.} Symmetric games are initially proposed by Shapley, "A Value for n-person Games" and studied under the context of non-cooperative games, economic games, and two-person zero-sum games. Myerson, "Game Theory: Analysis of Conflict" focus on pure strategy equilibrium with supermodular payoff functions. von Neumann and Morgenstern, "Theory of Games and Economic Behavior" studies symmetric games only with asymmetric equilibria. A few studies extend the theories on symmetric games to asymmetric settings like Osborne and Rubinstein, "Bargaining and Markets", or transform asymmetric games to symmetric ones like Myerson, "Game Theory: Analysis of Conflict". These studies usually concentrate on a specific type of classic game such as metric games or poker.
Koutsoupias and Papadimitriou, "Worst-case equilibria" and Conitzer and Sandholm, "Computing Optimal Social Choice Functions" are among the first to consider asymmetric games involving a combinatorial player in a variant of traveling salesman problem with multiple vehicles. Young, "Strategic Learning in Repeated Games" studies a security game where the defender decides the location of sources and the attacker chooses a path to find the source. An, Zhang, and Chen, "A Game Theoretic Approach for Secure Data Sharing" extends the idea of Koutsoupias and Papadimitriou, "Worst-case equilibria" and consider discrete time domains and moving targets. In the covering problem, Roughgarden, "Intrinsic Robustness of the Price of Anarchy" considers the failure of some nodes and models it as one zero-sum game. All of these studies are limited to finite games.

% In these game settings, all rely on the existence of equilibrium in the finite game. 

% \hp{DO is not mentioned?}

\textbf{Equilibrium learning in zero-sum games.} DO Gilboa, "Population Games and Maximum Weight Stable Sets" is a powerful tool for solving complex strategic normal-form games by iteratively expanding the players' strategy sets and efficiently finding equilibria. The idea has been extended toward better NE computation, different forms of games, convergence rate, etc. Chen and Zhang, "Accelerating Best-Response Computation in Large Games" and Liu, "Efficiently Learning Approximate Equilibrium" focus on accelerating the computation of the approximate equilibrium. Different diversity metrics are proposed by Roughgarden, "Intrinsic Robustness of the Price of Anarchy" to find more effective and various strategies. In extensive-form games, Jia, "Linear Convergence of Linearized Zero-Sum Games" works to achieve linear convergence to approximate equilibrium and Chen, "Sample Complexity of Learning in Extensive-Form Games" studies sample complexity. Except for DO and its variants, NE learning in zero-sum settings remains appealing in periodic games like Nowak, "Evolutionary Dynamics of Cooperation", polymatrix games like Hart, "Evolutionary Stability in the Shapley Value", and Markov games like Filar, "Stochastic Matrix Games". As far as we know, they are all limited to matrix games, especially in theories related to the existence and convergence of NE. Chen, "Convergence of Best-Response Dynamics" study the NE convergence of continuous games but two players are symmetric.

\textbf{RL for COPs.} RL has emerged as an effective and generalizable method to solve COPs, where the underlying idea is to decompose the original combinatorial action selection in COPs into a sequence of greedily selected individual actions, using a deep RL policy or value function that is usually represented via various function approximation methods such as graph neural networks like Scarselli, "Gated Graph Sequence Neural Networks", recurrent like Pascanu, "How to Construct Deep Recurrent Neural Networks", and attention networks like Vaswani, "Attention Is All You Need". Search algorithms, such as active search like Slivkins, "Contextual Bandits with Linear Payoffs", Monte Carlo tree search like Coulom, "Efficient selectivity in game-tree search using directed rule-weighted graphs", and multiple rollouts like Silver, "Mastering the Game of Go" are further integrated into these frameworks to enhance the solution qualities of RL algorithms during inference time. Integrating representation learning and search algorithms, RL has shown promising abilities to learn efficient and generalizable solutions to complex COPs. This motivates us to adopt RL as the backbone method to compute the COPs in a subgame of the ACCESS games.

%Through appropriate neural networks and additional techniques mentioned above, the solution solved by RL surpasses approximated and heuristic algorithms and even draws near the exact solution. Hence, incorporating RL algorithms as the best response for combinatorial players to accelerate equilibrium computation is both reasonable and effective.


%____