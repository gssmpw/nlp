\section{Additional Analysis of Results}
\label{appendix:analysis}
\subsection{Breakdown of Results Across Normative Reasoning Categories}
\label{appendix:normative_reasoning_analysis}

Considering each taxonomy category 
(Figure~\ref{fig:tax_breakdown}), it is observed that foundation models consistently perform better on \textcolor[HTML]{E6A700}{coordination/proactivity} tasks, and 
\textcolor[HTML]{246D63}{safety}, and perform worse on 
\textcolor[HTML]{EA772F}{communication/legibility} and 
\textcolor[HTML]{356ABC}{politeness} tasks, with a performance gap of 10\% between the best and worst-scored taxonomy categories. This is primarily driven by the high context-sensitivity of \textcolor[HTML]{EA772F}{communication/legibility} and \textcolor[HTML]{356ABC}{politeness} norms, whose correct actions depend on understanding situational nuances, social interactions, and subtle cues in body language and facial expressions that are difficult to resolve.


\begin{figure}
    \includegraphics[width=\linewidth]{img/tax_breakdown.pdf}
    \caption{Accuracy of selecting both the correct behavior and justification across different norm dimensions, averaged over the top eight performing models. The results highlight variations in model performance, with dimensions like \textcolor[HTML]{246D63}{safety} and \textcolor[HTML]{E6A700}{coordination/proactivity} being relatively easier, while \textcolor[HTML]{EA772F}{communication/legibility} and \textcolor[HTML]{356ABC}{politeness} pose greater challenges.}
    \label{fig:tax_breakdown}
\end{figure}


\subsection{Breakdown of Results Across Activity Categories} 
\label{appendix:activity_analysis}
Investigating by activity categories (Figure~\ref{fig:act_breakdown}), we find a 15\% gap in performance for leading models %(Gemini 1.5 Pro, Qwen2.5 VL, and GPT-4o) 
between the highest-scored Art/Culture-related activity and the lowest-scored Shopping/Dining activity.
The contrast between Art/Culture actions, which primarily involve direct object manipulation or two-person interactions, and Shopping/Dining scenarios, which require understanding complex multi-person social dynamics and implicit situational norms, further supports our finding that limitations in normative knowledge, rather than reasoning capability, constitute the primary failure mode in AI models' normative reasoning.

\subsection{Results Across Closed-source Models and Open-source Models}
\label{appendix:open_source_analysis}
As observed in Table~\ref{tab:results}, the best open-source model Qwen2.5-VL scored 41.5\%, compared to the best model's (Gemini-1.5-Pro)'s score of 45.2\%, or a gap of 3.7\%. Closed-source models perform far better on average, with a mean accuracy of 40.3\% vs. open-source's 28.3\%,\footnote{This open-source bench is after exclusion of outliers such as Llama-3.2, which scored below 10\% in every task.} matching observations on similar higher-order reasoning benchmarks \citep{chow2025physbench}.

%that these models are more adept at understanding health- and safety-related norms than navigating social event norms, which often involve more nuanced social cues and complex interactions between multiple people.

\begin{figure}
    \includegraphics[width=\linewidth]{img/act_breakdown.pdf}
    \caption{Accuracy of selecting both the correct behavior and justification across different activity categories, averaged over the top eight performing models.}
    \label{fig:act_breakdown}
\end{figure}