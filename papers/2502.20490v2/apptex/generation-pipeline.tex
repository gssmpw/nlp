\section{Benchmark Generation Pipeline Details}
\label{appendix:BGPD}
\newcounter{num}
\setcounter{num}{1}

\paragraph{Phase \Roman{num}: Video Sampling} \dataset{} sources its videos from the Ego4D dataset ~\cite{grauman2022ego4dworld3000hours}, consisting of 3650 hours of richly annotated egocentric footage of commonplace human activities in context. We selected the Ego4D dataset as our video source for the following reasons:
(1) Its \textbf{egocentric perspective} aligns with human embodiment and the embodied systems this benchmark aims to support.
(2) It includes over 3.85 million \textbf{action-centric visual narrations}, facilitating the identification of unique actions.
(3) Its \textbf{diverse} range of situations and actions enables EgoNormia to comprehensively explore the space of physical-social norms.
% (1) the egocentric perspective matches the embodiment of humans and the typical embodied systems this benchmark is intended to support;
% (2) it features over 3.85 million action-centric visual narrations, which aid in targeting unique behaviors; and 
% (3) it is situation- and action- diverse, enabling EgoNormia to span the space of physical-social norms.

% We sampled all narrations mentioning two or more actors, then PoS-tagged the target narrations and clustered by verb category and scenario. From this set, a maximum of three samples were taken from each verb-scenario combination, in order to select a maximally long-tailed set of samples. Any scenarios involving card or board games were excluded, as these present monotonic situations where action alternatives relate to game rules instead of human social or physical norms.

We created a diverse dataset by selecting narrations that involved multiple actors, analyzing the verbs and scenarios present, and sampling up to three instances from each unique combination while excluding game-related scenarios to focus on natural social and physical interactions. This curation yielded 4446 unique samples, sourced from from unique 1870 videos.


\setcounter{num}{2}
\paragraph{Phase \Roman{num}: Answer Generation}
For each example, the goal is to produce four candidate answers, comprising one gold-standard response (i.e. best matching human expectations) and three distractors (not counting None, which is added after generation).
To generate high-quality alternative actions and justifications, we employ a structured, multi-shot pipeline with GPT-4o-based Chain-of-Thought prompting~\citep{wei2022chain}.

% The objective of this stage is to produce high-quality alternative actions and justifications. We generate four candidat answers, which will turn out to be one gold standard and three distractors in following stages. Given the low norm-behavior understanding of current models, a structured, multi-shot pipeline was employed to generate the \texttt{AJ} primitives.

% GPT-4o was used in every stage of the pipeline, and selected through experimentation: Actions generated by Gemini-1.5/2.0-Flash were convergent, while Claude-3.5 and Llama3.2 had high refusal rates in non-problematic scenarios. Across all generation stages, Chain-of-Thought~\cite{wei2022chain} prompting and a temperature of 1.0 were used to maximize model performance.
% GPT-4o was employed at every stage of the pipeline, chosen based on empirical evaluation. Actions generated by Gemini-1.5/2.0-Flash exhibited convergence, whereas Claude-3.5 and Llama3.2 demonstrated high refusal rates even in non-problematic scenarios. To optimize performance across all generation stages, Chain-of-Thought prompting~\cite{wei2022chain} was utilized alongside a temperature setting of 1.0.



\setcounter{num}{1}
Frames of sampled snippets of \textbf{Phase \Roman{num}} are first processed with a VLM to extract a scene context description $c$, consisting of the activity, the identities of the people involved, and the environment.
The context $c$ are then corrupted via LLM to programmatically modify the core context, to change the norms that are relevant in the context. Here, we leverage the defeasibility and compositionality of norms explored by NormBank ~\cite{ziems-etal-2023-normbank} to add, remove, or modify elements of the context, yielding three additional contexts, which form the context set.
% Further details on the corruption methodology are included in Appendix XXX. (Rejection of police/criminal/spy contexts)
Then an LLM generates a noisy set of actions $A^+$ and their justifications $J^+$ for each context $c$ in the context set, where the LLM is directed to generate the best action to perform in that given context, a justification for why that norm is most important, and also the categories to which each action belongs to. These are generated in a multi-turn way, where each inference uses the result of the previous stage as part of its input.
% Experimentation with single shot direct-from-video generation yielded generated \texttt{A}s that were similar to each other; this was rejected as the generation approach.

\setcounter{num}{3}
\paragraph{Phase \Roman{num}: Filtering}
The output of  \textbf{Phase II} consists of high-quality but noisy sets ($A^+,J^+$), as the wide scale of the action generation may yield trivially resolvable tasks, or those whose best action is ambiguous, even with context. % Further issues include failed or malformed generations, or the desired output structure not being matched.
Thus, we refine $A^+$ and $J^+$ with several filtering rounds to ensure the correctness, context-dependence, and high difficulty of questions, to yield a filtered $A$ and $J$ for each example: (i) \textbf{Normativity filtering}: We remove certain action descriptions can describe an action that's not feasibility or is harmful in any situation.
% - for instance, the action "Grab the lady's purse and run" is illegal through text parsing alone; this class of \texttt{AJs} trivialize the downstream task. Thus, each answer is individually inspected for safety and feasibility, any failing answer is regenerated and re-tested until the full set passes.
(ii) \textbf{Blind filtering.} To enforce EgoNormia tasks requring grounded visual reasoning to solve, a "blind" baseline is compiled: Any task whose gold standard answer is obviously correct without context, either due to nonsensical answers or leaky domain knowledge, is filtered out as they do not test visual normative reasoning. 
% Due to the low random success rate (4.0\%), blind filtering did not substantially risk removing data points that were well-formed but the blind model was able to guess successfully.

\setcounter{num}{4}
\paragraph{Phase \Roman{num}: Human Validation}

To ensure the clarity and alignment of answers with human normative reasoning, we employ a manual validation process:
(i) In the first round, annotators are engaged through Prolific to inspect every sample manually (The detailed procedures for onboarding and training the human annotators, as well as the instructions for the curation process are provided in the in Appendix~\ref{sec:HumanValidationProcess}). Annotators are responsible for three key tasks: for each example, verifying that the best action and justification are present in $A$ and $J$ without overlapping in meaning with any other alternatives; selecting other given actions and justifications that are appropriate in the given situation but do not represent the most normative choice; and confirming whether the best action $a$ is followed in the video afterwards. 
% Annotators are tasked with three primary responsibilities: (A) Verify that the best action \texttt{A} and justification \texttt{J} are present in \texttt{AJT+}, and do not overlap in meaning with any other \texttt{AJ} (B) Select the list of other given actions and justifications that are sensible in the given situation, but not the best action - i.e. actions that are expected in that context, but are not the most normative. 
% (C) Confirm whether the best action \texttt{A} is followed in-scene.
(ii) Two annotators must agree on the best action $a$ for a given $A$ and $J$ to be accepted; they are allowed to provide their own preferred $a$ and $j$ if no answer is correct. In cases of new annoated actions, $A$ and $J$ are manually reconciled by the authors and either modified or rejected outright. This reduces the number of admissible samples by 50\%. 
(iii) Finally, a second expert curation round is performed, to manually validate the difficulty and diversity of each sample. Only ~85\% of the examples that pass the first round also pass the second round, demonstrating the relative difficulty of generating nontrivial grounded norm-resolution situations.