\section{Details on RAG (NormThinker) Approach}
\label{appendix:indexing}

The section below provides details on the individual steps involved in the \dataset{} retrieval pipeline. We refer to the pipeline as NormThinker for brevity's sake.

NormThinker is built from indexed, ground-truth normative actions for a given \dataset{} datapoint, keyed to free-form text descriptions of the corresponding scene, or "contexts". In experiments with NormThinker, the full dataset was first clustered by high-level categories in Appendix~\ref{appendix:clustering}, then half of the datapoints per cluster (half of a total 1853 points in \dataset{}) were processed and stored in the NormThinker embedding database. In-domain evaluations were conducted exclusively on the unseen (i.e. not processed/embedded) task split. The processing step involves parsing the text context with a VLM (Gemini 1.5 Flash), which is subsequently converted into a text embedding that is indexed into the downstream embedding database.
\newline
When a video is queried, the context of the query video is parsed and converted to an embedding following the same method as above. This embedding is then used to retrieve the five closest contexts by cosine similarity. By indexing over a wide range of contexts in \dataset{}, we demonstrate the utility of the dataset's diversity, and minimize the effect of poorly-matched retrievals. We do not rigorously protect against poorly-matched retrievals, as NormThinker is designed primarily as a showcase of EgoNormia's direct utility for augmenting VLM norm understanding, and also as a demonstration of the relative ease of improving normative reasoning performance on current SOTA models, in order to motivate future work and exploration in this domain. Finally, the five corresponding ground-truth actions for these contexts are appended to the base model's prompt, and the rest of the pipeline proceeds as it does without retrieval.



% \section{Model Configuration}

% The table below details the configuration of models used in \dataset{} benchmarking.

% \begin{table}[h]
% \centering
% \begin{tabular}{ll}
% \toprule
% \textbf{Model} & \textbf{Configuration} \\
% \midrule
% BLIP-2 & \texttt{torch\_dtype = torch.float16, max\_new\_tokens = 200} \\
% InstructBLIP-t5-xl & \texttt{torch\_dtype = torch.float16, max\_new\_tokens = 200} \\
% InstructBLIP-t5-xxl & \texttt{torch\_dtype = torch.float16, max\_new\_tokens = 200} \\
% InstructBLIP-7B & \texttt{torch\_dtype = torch.float16, max\_new\_tokens = 200} \\
% InstructBLIP-13B & \texttt{torch\_dtype = torch.float16, max\_new\_tokens = 200} \\
% \bottomrule
% \end{tabular}
% \caption{Model configuration for \dataset{} benchmarking. Parameters not explicitly stated are left as the API default.}
% \end{table}


% \section{Introduction (Caleb's Version)}

% Language-based agents can serve within systems of other goal-oriented social agents, both human and machine, to collectively achieve positive outcomes \citep{}. For such a multi-agent system to be most successful, the system will need to reach a stable equilibrium that minimizes friction and aligns coordinated effort with a high level of efficiency and safety. \textit{Social norms} are the behavioral expectations that represent common, generalized solutions to long-horizon human coordination problems, serving as a form of social control \citep{hollander2011current,mukherjee2007emergence}, while also reinforcing both the autonomy and well-being of human agents \citep{verhagen2000norm}. 

% If Large Language Models (LLMs) can acquire knowledge of social norms directly through pre-training data \citep{}, inferential analogy \citep{fung2023normsage,kim2023soda}, or fine-tuning \citep{}, and if they can perform both physical and commonsense reasoning \citep{}, then they may be suitable controllers for social navigation \citep{mavrogiannis2023core} in embodied agents \citep{liembodied} like robots \citep{francis2023principles}.

% Concepts to elaborate:
% \begin{enumerate}
%     \item\textbf{Safety:} \citep{bera2017sociosense}
%     \item \textbf{Proxemics:} Proxemics is highly correlated with humans' perceived safety around other agents \citep{huang2022proxemics}, particularly with robots \citep{neggers2022determining}.
%     \item \textbf{Legibility:} The optimal behavior for a single agent may be the most direct or efficient course of action, but in multi-agent settings, coordination requires each agent to behave in a manner that clearly communicates that agent's goals and current state \citep{dragan2013legibility,wallkotter2021explainable}. 
%     \item \textbf{Cooperation} norms serve to make collaborative tasks more fluid and seamless, both objectively and subjectively. Cooperation norms should maximize the useful concurrent activity of teammates and minimize agents' idle time, like reducing the exchange gap time in turn-taking settings \citep{hoffman2019evaluating}.
%     \item \textbf{Proactivity:} While deference may be generally advisable for dyadic interactions, and humans may prefer this from robots in more passive roles \citep{kanda2002development,rubagotti2022perceived}, passivity may quickly induce deadlock in multi-agent systems \citep{francis2023principles,lyu2022responsibility}. Embodied agents should anticipate goal conflicts and move to resolve these conflicts proactively \citep{tan2020relationship}.
% \end{enumerate}

% Note: there are tons more references to riff on here \url{https://www.sciencedirect.com/science/article/pii/S0921889022000173#b72}


% Maximize trust \citep{}, 

\begin{comment}
\section{Metrics}
\label{appendix:metrics}
Accuracy is used the first two subtasks with a single ground-truth answer; IoU is used on the third, where multiple sensible choices exist between the ground truth and predicted sets.
This setup allows for \textbf{precise} control over task difficulty and target dimensions through the design of negative choices and \textbf{simplifies} evaluation by enabling the use of an aggregate answer-correctness accuracy metric.
In practice, action, justification, and sensibility subtasks are evaluated with \textbf{independent inference calls}, as selecting justification simultaneously with action leaks information about the scene and biases the model towards incorrect answers. %We measure both dimensions using accuracy (the proportion of tasks for which the model correctly identifies the normative action and justification). 
\end{comment}