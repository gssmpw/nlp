\section{Related Work}
\label{sec:related_work}

% \citet{ziems-etal-2023-normbank} introduced NormBank

% \subsection{Social Navigation for Embodied Agent}
% Social navigation for embodied agents encompasses the integration of social norms and behaviors into the path-planning algorithms of robots and other forms of agents. Previous studies have pioneered the development of social force models \citep{helbing1995social, leal2011everybody}, deep reinforcement learning approaches \citep{chen2018socially, everett2018motion, sathyamoorthy2020frozone}, and inverse reinforcement learning \citep{Kretzschmar2016SociallyCM} to enable agents to navigate complex environments without causing discomfort to any human present. This task usually requires the agent to actively perceive the environment \citep{daza2021approach}, track and predict the movement of pedestrians \citep{leal2011everybody, Kruse2013HumanawareRN, sathyamoorthy2020frozone}， and even account for invisible human \citep{singamaneni2022watch}.

\subsection{Video Question Answering}
Video Question Answering has emerged as a widely adopted benchmark for VLMs, framing visual understanding as a question-answering task~\cite{lei2019tvqalocalizedcompositionalvideo, yu2019activitynetqadatasetunderstandingcomplex, xiao2021nextqanextphasequestionansweringexplaining,zhu2023excalibur}. Many benchmarks employ MCQ tasks to simplify evaluation by providing an aggregate accuracy metric~\cite{chandrasegaran2024hourvideo1hourvideolanguageunderstanding, chinchure2024blackswanabductivedefeasible}. For example, VCR~\cite{zellers2019recognition} introduces Adversarial Matching to create challenging MCQs with minimal human intervention. HourVideo~\cite{chandrasegaran2024hourvideo1hourvideolanguageunderstanding} utilizes a five-stage pipeline to generate, refine, and filter diverse, high-quality MCQs. Similarly, EgoSchema~\cite{mangalam2023egoschemadiagnosticbenchmarklongform} leverages Ego4D~\cite{grauman2022ego4dworld3000hours} videos and implements several rounds of filtering and manual curation, to ensure that questions are both high-quality and sufficiently challenging~\cite{mangalam2023egoschemadiagnosticbenchmarklongform}.

\subsection{Social Commonsense and Norms}
Commonsense knowledge bases, such as ConceptNet~\citep{speer2017conceptnet} and ATOMIC~\citep{sap2019atomic}, provide AI systems with essential everyday information for tasks ranging from physical commonsense reasoning to explanation generation. NormBank~\citep{ziems-etal-2023-normbank} further enriches this landscape by offering situational contrast sets that support normative reasoning about unspoken social rules.
Complementing these resources, social intelligence benchmarks like the ToMi~\citep{le-etal-2019-revisiting} and FauxPas datasets~\citep{shapira-etal-2023-well}—along with simulation environments such as SOTOPIA~\citep{zhou2024sotopia, wang2024sotopiapiinteractivelearningsocially}—assess an agent’s ability to understand others' intentions and navigate complex social interactions.
Recent work has expanded these evaluations to embodied agents~\citep{kwon2024grounded, padmakumar2021teach} and diverse task scenarios ~\citep{wang-etal-2019-persuasion, Bakhtin2022HumanlevelPI}. Building on these insights, our work introduces a benchmark specifically designed to evaluate normative decision-making abilities.

% \subsection{Commonsense Knowledge Base}
% Commonsense knowledge bases are critical for providing artificial intelligence systems with the basic understanding and structured information of the world we humans take for granted. Works like ConceptNet \citep{speer2017conceptnet} and ATOMIC \citep{sap2019atomic} compile vast amounts of everyday knowledge, which can be further integrated into language models for diverse downstream tasks including physical commonsense reasoning \citep{bisk2019piqa}, social commonsense reasoning \citep{sap2019socialiqa, shapira2023clever, zadeh2019social}, commonsense
% explanation generation \citep{ji2020generating}, and social interaction evaluation \citep{zhou2024sotopia, lee2024evaluating}. Notably, NormBank \cite{ziems-etal-2023-normbank}, a knowledge base of social norms, utilizes situational context to create contrast sets of defeasible social norms, offering rich conditioning for normative reasoning. Our work specifically focuses on physical social norms in embodied agents. This involves not only understanding physical interactions and spatial awareness but also adhering to the unspoken rules of social conduct.

% \subsection{Social Intelligence Benchmarks}
% Evaluating the social intelligence of AI systems requires robust and comprehensive benchmarks that simulate real-world social interactions and assess an agent's ability to understand and respond appropriately to social cues. Benchmarks such as the ToMi dataset \citep{le-etal-2019-revisiting} and the FauxPas dataset \citep{shapira-etal-2023-well} draw inspirations from clinical tests and aim to assess agent's capability to reason about intents and beliefs of other agents. SOTOPIA~\cite{zhou2024sotopia, wang2024sotopiapiinteractivelearningsocially} simulates interactions between artificial agents under complex social scenarios and evaluate their social intelligence. Recent works have extended the evaluation to include various physical and social scenarios \citep{park2023generative}, formal structuring \citep{li2024social}, diverse task types \citep{wang-etal-2019-persuasion, Bakhtin2022HumanlevelPI}, and embodied agents \citep{kwon2024grounded, padmakumar2021teach}.

\begin{comment}
Table~\ref{tab:related_work} provides a comparative overview of recent norm reasoning and physical reasoning datasets, highlighting how our \dataset{} benchmark differs from existing works in considering physical social norm understanding.

\begin{table*}[ht]
\centering
\small
\begin{tabular}{llccccc}
\toprule
&  & Modality & \# Example & Norm & Physical & SOTA / Human \\
\midrule
\multirow{4}{*}{\rotatebox{90}{Norm}} & \textsc{NormDial}~\citep{li2023normdial} & Text &  & Yes & No & --- \\
& \textsc{NormSage}~\citep{fung2023normsage} & Text & & Yes & No & --- \\
& \textsc{Social-Chem}~\citep{forbes2020social} & Text & & Yes & No & --- \\
& \textsc{NormBank} \citep{ziems-etal-2023-normbank} & Text & & Yes & No & --- \\
\midrule
\multirow{4}{*}{\rotatebox{90}{Video}} & \textsc{VCR}~\citep{zellers2019recognition}                           & Video & 290K  & No  & Yes  & 91.4\% / 91.0\% \\
& \textsc{PACS}~\citep{yu2022pacs} & Audio + Video & 13400  & No  & Yes  & 70.0\% / 95.0\% \\
& \textsc{EgoSchema}~\citep{mangalam2023egoschemadiagnosticbenchmarklongform}  & Video & 5000 & No & Yes & 33.0\% / 76.0\% \\
& \textsc{HourVideo}~\citep{chandrasegaran2024hourvideo1hourvideolanguageunderstanding} & Video & 12976 & No & Yes & 37.3\% / 85.0\% \\
\midrule
\midrule
& \dataset{} & Video & 1853 & Yes & Yes & 45.2\% / 90.9\% \\
\bottomrule
\end{tabular}
\caption{Comparison between \dataset{} and othe norm reasoning and physical reasoning datasets TODO: Behavior (Action) Prediction, Rationale, Utility Norm, Non-Utility Norm}
\label{tab:related_work}
\end{table*}
\end{comment}

% \subsection{other potential related works}

% \begin{itemize}
%     \item \st{Embodied-RAG: General non-parametric Embodied Memory for Retrieval and Generation}
%     \item Black Swan
%     \item Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making
%     \item OpenEQA: Embodied Question Answering in the Era of Foundation Models
%     \item GRACE: Generating Socially Appropriate Robot Actions Leveraging LLMs and Human Explanations
%     \item LeLaN: Learning A Language-Conditioned Navigation Policy from In-the-Wild Videos
% \end{itemize}