\section{Dataset Construction {\color{red}IN PROGRESS}}
\label{sec:Dataset Construction}

% The purpose of this section is to describe the process of constructing the dataset, including the data sources, the annotation process, and the final dataset statistics.

% Introduce overall ideas of filtering for diversity, generation, enforcment of plausibility, negative salting, and blind filtering

\hao{Add around 3 examples of our tasks, label the options chosen by models}



% Curating for diversity and interestingness
The scale of the Ego4D dataset (3670 hours of footage across 9611 videos) presents challenges in consistently selecting diverse and action-rich scenes for action-justification (AJ) 
generation,
particularly because some contexts and activities are overrepresented in the dataset. Ego4D has low-level and simply structured (actor, verb, object) human annotations describing the most significant action that takes place every
few seconds (narrations), as well as "scenario"-level captions describing the category of activity taking place across the entire video.

Thus, we leverage the structure of the these narrations to curate our selection of scenes, using a dependency parser to extract the verb lemma of each narration
and clustering by alike verb senses to categorize scenes by the type of action taking place. These clusters are further filtered to only include scenes with at least two actors present,
and are finally sampled by scenario caption to yield a collection of scenes with unique action-scenario pairs for AJ generation.

We curate the videos from which the scenes are drawn to ensure that the scenes are context-rich and of high visual fidelity, but avoid making assumptions
on whether certain action-scenario combinations yield more interesting or challenging tasks, as trivial normative reasoning tasks are filtered by 
blind filtering (see Section \ref{sec:Blind Filtering}).

% Generation of actions and justifications
A key feature of the presented approach is the use of iterative prompting and self-verification to generate high quality, challenging actions and justifications
grounded to the context of a given scene.

We formulate detailed prompts that supply step-by-step instructions, exemplars, and constraints to guide the generation of actions and justifications. Importantly,
we prompt the actions and justifications to be orthogonal in meaning, so that each answer implies a materially different normative behavior or justification.

Since the information needed to determine the normative behavior varies across scenes, we do not enforce a fixed context taxonomy or normative reasoning type for each scene.

Answers and justifications are initially generated one-shot by prompting a VLM \yicheng{I think we should mention Gemini-2 here since that will explain why we exclude that later in our evaluation of VLMs?}%(mention Gemini 2.0?)
on the full video sample before and after the action (a maximum of 15 seconds before, and 5 seconds after),
the action extracted from the narration, and the aforementioned instruction prompt. 

\yicheng{I think we could just give the question here for simplicity?}
Standalone questions are not generated as the task is always to select a behavior or justification.

% Enforcing  plausibility
The initial answers and justifications often contain nonsensical or non-relevant assertions, and are thus trivially identifiable as incorrect.

Plausibility is enforced by using a POS tagger to extract 
the objects present in a given answers or justifications, and comparing them to the objects identifiable in the scene by a SOTA VLM \yicheng{mention which VLM?}. 
If the objects in the answer or justification are not present in the scene, the answer is re-generated by the VLM with the 
true scene objects supplied as corrective context.

This self-verification mechanism is repeated until the actions and justifications are plausible and grounded in the scene context.

The result of this step is a set of AJs that are contextually grounded and entirely plausible, though not necessarily
normatively correct. Human annotators then reject answers that are non-sensible* and supply correct
answers and justifications where needed.

*An answer is considered non-sensible if it makes assertions that are grossly misaligned with human normative reasoning - that is, if
it proposes actions that are not commonly seen in everyday life.
For example, if the scene depicts a person walking down a street, crossing the street on one's hands would be considered non-sensible, but plausible,
where crossing without looking both ways would be considered sensible and plausible, if not normatively correct. The use of human
annotators to enforce this constraint is critical, as VLMs do not currently have the ability to consistently distinguish between sensible and non-sensible actions.
%%%
% Talk about human annotation here (Mohammad)
%%%


% Blind filtering
The final set of tasks may still contain AJs whose best options are trivially identifiable through information leakage from the AJs themselves, or 
strongly held norms** or other biases in the VLMs used to generate the AJs.

\yicheng{Do we use the text LLM once, or multiple times and use majority voting to reduce randomness?}
To remedy this, blind filtering is performed with a blind (non-visual) LLM, on the full set of tasks; any 
tasks for which the LLM can identify the correct answer are removed from the dataset. This aggressively filters out tasks that are trivially solvable
and results in a rich dataset of normative reasoning tasks that are challenging and require a deep understanding of the scene context.

** For instance, passing a school bus with its stop sign extended is nearly universally considered non-normative;
any non-normative actions generated are thus likely easy to identify as incorrect.

An example data point can be found in Figure ~\ref{fig:data_example}.

\begin{figure*}
    \includegraphics[width=\linewidth]{img/example.png}
    \caption{Video samples and the relevant norm categories. Will update to high resolution version once finalized.}
    \label{fig:data_example}
\end{figure*}
