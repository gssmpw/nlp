\section{Evaluation}
\label{sec:evaluation}
Accuracy is used in the first two subtasks with a single ground-truth answer; intersection over union (IoU) is used on the third subtask, where multiple contextually-sensible action choices exist.
We evaluated the following state-of-the-art foundation models: Gemini 1.5 Flash/Pro \citep{team2024gemini}, GPT-4o \citep{hurst2024gpt}, Claude 3.5 Sonnet \citep{anthropic2024claude}, o3-mini\footnote{In this work, we use the \textit{medium} reasoning setting for o3-mini.} \citep{o3mini}, Deepseek R1 \citep{guo2025deepseek}, InternVL 2.5 \citep{chen2024expanding}, Qwen 2.5 VL \citep{Qwen2.5-VL}. To characterize the impact of visual priors on model performance, \dataset{} benchmarking was performed across three settings:
(a) \textbf{Blind} (no input), where only the questions are provided to the models;
(b) \textbf{Pipeline} (text-only), where a detailed description of the scene generated by Gemini 1.5 Flash is provided as part of the questions;
and (c) \textbf{Video}, where both video and questions are provided. For compatibility, videos are sampled at one frame per second and concatenated into a single image.
We use CoT prompting~\citep{wei2022chain} across all evaluations and provide results in Table~\ref{tab:results}. Appendix~\ref{appendix:full_results} presents the complete results, including those for additional models. Appendix~\ref{appendix:refusal} presents model refusal rates.
%and the default hyperparameters of each model across all evaluations. See Appendix J for details.
%\noindent \dataset{} evaluates model performance through multiple tasks per sample, with accuracy measured both in aggregate and within each taxonomy category. Each sample includes two independent five-way multiple-choice questions, where the objective is to identify the most appropriate action and behaviors based on the given scene priors. Additionally, a multiple-response question requires selecting all sensible actions from a predefined list. Each question is processed through a separate inference call, resulting in a total of three inferences per sample.

% \subsection{Baselines}

% In this section, we establish the evaluation conditions different multimodal models on understanding grounded norms. Our evaluation focuses on three categories of models.

% \paragraph{Blind LLMs} 

% To examine whether multiple-choice questions (MCQs) in our benchmark can be answered solely by analyzing the answer choices—without accessing the corresponding video content—we introduce a blind LLM baseline. This approach allows us to explore potential distribution biases between correct and incorrect answers. For this baseline, we evaluate Gemini 1.5 Flash , Gemini 1.5 Pro~\cite{team2024gemini}, and GPT-4o~\cite{hurst2024gpt}.

% \paragraph{Pipeline} 
% As an ablation study for multimodal video models, we experiment with a two-stage pipeline where Gemini 1.5 Flash first summarhyizes the video context in detail. The downstream model then receives this textual summary instead of raw video frames to select an answer. This setup quantifies the impact of non-textual visual elements—such as positioning, speed, and scene transitions—on normative reasoning.  We evaluate Gemini 1.5 Flash , Gemini 1.5 Pro~\cite{team2024gemini}, o3 mini, and Deepseek R1 ~\cite{guo2025deepseek} as downstream models.

% \paragraph{Video Models} Multimodal video models, such as GPT 4o, have demonstrated strong performance on object-parsing and commonsense reasoning benchmarks. These models can directly process image inputs, thus we supply the frames of the video to the sample as a combined image, sampled from the snippet at one frame per second, at the native resolution of Ego4D video, allowing evaluation of multimodal models that cannot process native video.
% % (XXX source here for Gemini sampling rate).

% \hao{Prompt each of RQ1 and RQ2 to a subsection}

\begin{table}[th]
\centering
\input{tables/results.tex}
\caption{
% \hao{Mention: (1) Bold means? (2) Fixed Best Output means? (3) Simple takeaways. Let people know how to interpret the table without moving away from it.}
\dataset{} benchmark results. \textit{Constant Choice} represents the best performance of selecting a constant choice for all questions. Bold values indicate the best performance in each category that are above the constant choice baseline.
}
\label{tab:results}
\end{table}

% \yicheng{Potential Analysis Score across different norm dimension (is VLM paticuarly bad at understanding norm of a certain domain. Scores with human interaction / animal interaction / no people (is VLM worse at interacting with human / animal than the enviornment?). Scores with high-stake and low-stake scenario (is VLM worse at high-stake scenarios, which will have a bigger imact to human if handled incorrectly). Scores with indoor and our door scene (is VLM worse at indoor task or out door task?). Scores with different time period (is VLM worse at night where light condition is worse?)}
% Describe eval protocol here - Eval is single-shot across entire dataset, only naive multimodal (no socratic), and human performance

% Insert table of results here for behavior, justification, followed, aggregate results


% Insert figure of examples of rich reasoning tasks, correct answers, and commonly-selected VLM answers

% \hao{Add the breakdown analysis to the paper}

% \paragraph{Foundation models demonstrate limited normative reasoning ability.}

\subsection{Results and Discussion}
% \subsection{RQ1: Limited Capability in Normative Decision-Making}
In evaluation on \dataset{}, most models obtain a mean accuracy lower than 40\% on \dataset{}, substantially exceeded by the average human score of 92.4\%. Gemini 1.5 Pro, the best-performing model, evaluated under vision inputs, achieved a mean accuracy of 45.3\%, suggesting that \textbf{current models have limited ability to make embodied normative decisions (RQ1)}.
% \paragraph{Closed-source models perform better than open-source models.} As observed in Table \ref{tab:results}, the best open-source model Qwen2.5-VL scored 41.5\%, compared to the best model's (Gemini-1.5-Pro)'s score of 45.2\%, or a gap of 3.7\%. Closed-source models perform far better on average, with a mean accuracy of 40.3\% vs. open-source's 28.3\% \footnote{This open-source bench is after exclusion of outliers such as Llama-3.2, which scored below 10\% in every task.}, matching observations on similar higher-order reasoning benchmarks \citep{chow2025physbench}.
% \newline
% \noindent \textbf{Selected models perform far below their peers.}
% \paragraph{Blind foundation models perform substantially worse than vision-input models.}
On the blind ablation, the accuracy of selecting both the correct behavior and justification drops by 22.1\% and 24.1\% for GPT-4o and Gemini 1.5 Pro, respectively. This demonstrates that foundation models cannot rely on distribution biases or textual cues \citep{Goyal_2017_CVPR} to solve \dataset{} tasks.
% , demonstrating the robustness of the benchmark.
% \paragraph{Video models perform better than description-input models.}
Furthermore, even with enriched textual descriptions and state-of-the-art reasoning models such as o3-mini, pipeline performance remains inferior to that of models with vision inputs.
This proves a fundamental limitation of language in capturing continuous, reasoning-subtle features such as spatial relationships, visible emotions and affect, and physical dynamics \citep{chen2024spatialvlm, zheng2024contphy}, and indicates the criticality of visual input for normative reasoning.

Notably, (I) Reasoning models like o3-mini and Deepseek R1 see the most considerable performance improvement between the blind setting and the pipeline setting ($+26.5\%$ and $+20.4\%$ respectively), scoring comparably to the best-performing video setting models.
We assume that normative reasoning scales strongly with general reasoning capability, while such inference-time scaling \citep{wu2024inferencescaling, snell2024inferencescaling} usually comes with a long latency that prevents it from embodied use cases.
(II) The best open-source models (Deepseek R1 and Qwen2.5 VL) are generally comparable to the best closed-source models, demonstrating that no model developers currently prioritize post-training for norm understanding; however, this also implies strong and easily-exploitable opportunities for developing norm-reasoning VLMs.

%suggesting a future where socially aware embodied models can be developed transparently and customized for applications without reliance on proprietary models.
%Furthermore, this highlights the necessity of evaluating normative reasoning in the video modality, as text alone cannot fully capture the complexity of real-world dynamics. %Notably, even the most advanced video model, Gemini 1.5 Pro, achieves only 45.2\% accuracy. %falling significantly short of human performance (90.9\%) by nearly a factor of two.

%Discussion on Model Performance
% \subsubsection{When do Foundation Models Fail in Embodied Normative Reasoning?}

\begin{figure}[!t]
\centering
    \includegraphics[width=0.7\linewidth]{img/Fail_fixed.pdf}
    \caption{Distribution of reasoning failure modes across GPT-4o, Gemini 1.5 Pro, and human evaluation. Annotations of 100 representative tasks revealed four primary failure modes, with norm sensibility errors being the most prevalent among models. The proportion of norm prioritization errors increases with overall performance on \dataset{}.}
    \label{fig:fail}
\end{figure}

% \hao{Point the readers to the Figures in the Appendix. You can use App. Fig. X}
To investigate \textbf{causes for the limited normative reasoning ability of VLMs (RQ2)}, we first examine performance variance across norm taxonomy categories (App. Fig. \ref{fig:tax_breakdown}) and activities (App. Fig. \ref{fig:act_breakdown}). Our findings indicate that models perform well in the \textcolor[HTML]{246D63}{safety} and \textcolor[HTML]{E6A700}{coordination/proactivity} dimensions but struggle with \textcolor[HTML]{EA772F}{communication/legibility}. In terms of activity categories, models excel in art/culture-related tasks but perform poorly in shopping-related scenarios. Detailed additional analyses can be found in Appendix~\ref{appendix:analysis}. We find that normative reasoning failures are \textit{due primarily to misaligned normative knowledge, incorrect norm prioritization, and situational misinterpretation}, rather than incorrect perception.

% \paragraph{Normative knowledge and prioritization errors are the primary reasoning failure mode.} 
We further categorize errors in normative reasoning by annotating the models' full CoT responses on 100 representative tasks of \dataset{}. Four failure modes were identified: (1) Norm sensibility errors, (2) Norm prioritization errors, (3) Perception errors, and (4) Answer refusal. The distribution of these model errors and human errors is shown in Figure \ref{fig:fail}. For models, the majority of failures were due to sensibility errors instead of perception, suggesting that foundation models are competent in processing the visual context of the video inputs but fail in performing sound normative reasoning on the parsed context. Furthermore, the ratio of norm prioritization errors grows as the overall performance increases (GPT-4o $<$ Gemini 1.5 Pro $<$ Human), suggesting more capable models struggle more with determining which norm should take precedence in ambiguous situations.



% \subsubsection{MCQ evaluation in the Presence vs. Absence of Humans}

% We conducted an additional ablation study to assess norm reasoning ability in scenarios where a human is explicitly present in the video versus when no human is present. (Figure ~\ref{fig:human}). For all models, norm reasoning performance appears weaker when a human is present, suggesting that reasoning about physical social norms involving human interactions poses greater challenges compared to interactions with the environment alone. This is likely because engaging with humans requires the model to infer their thoughts and feelings—an ability known as Theory-of-Mind, which many large language models struggle with ~\cite{ullman2023large}. Without a deep understanding of human intent, norm reasoning becomes more difficult.


% \begin{figure*}
%     \includegraphics[width=\linewidth]{img/human.png}
%     \caption{Accuracy of selecting both the correct behavior and justification in scenarios with and without a human present. The results indicate that norm reasoning performance tends to decline when a human is explicitly involved, suggesting that reasoning about social norms in human interactions is more challenging than in purely environmental contexts.}
%     \label{fig:human}
% \end{figure*}

