% \newpage
\section{\dataset{}}
\label{sec:task_overview}
\dataset{} is designed to achieve several goals: (1) \emph{diversity} across contexts and normative categories through uniqueness filters, (2) \emph{simplicity of use} through a multiple-choice question format with clear metrics, (3) \emph{high human consensus} via extensive manual validation requiring annotator agreement, and (4) \emph{high difficulty} and \emph{benchmark longevity} by designing tasks challenging to solve through superficial visual reasoning. 


% Normative reasoning requires parsing and understanding the context of a scene, identifying relevant norms, and selecting or moderating action to satisfy those norms. This is complicated by the breadth of potentially relevant contexts, the incompleteness of information available in the scene, the high defeasibility of norms,\footnote{Highly sensitive to small changes in context. If one is talking to another person, whether that person is a friend or a stranger completely changes the norms of the situation.} and the implicit, variable priority of norms. 
% Normative reasoning requires parsing and understanding the context of a scene, identifying norms that are relevant, and moderating behavior to satisfy those norms.
% This is complicated by the breadth of potentially relevant context like social relationships, goals of other actors, and scene history.  The incompleteness of information available in the scene like unknown goals, hidden objects, and ambiguous actions, the high defeasibility of norms (small perturbations in context can lead to large perturbations in correct behavior), and the implicit, variable priority of norms. % Include that norm-context dependence is long-tailed??? % Give concrete example to illustrate such as in https://arxiv.org/pdf/2209.06293?
% Explanation of the types of reasoning we test (The overall goal here is to describe what we do, little on why we do it)
% To comprehensively test normative reasoning ability, we design a task suite to encompass the selection of normative action and supporting justification.

% -- this is a proven method in 
\subsection{\dataset{} Task Definition}
We use a format of Multiple-Choice Questions~(MCQs) for all subtasks.  Example MCQs are shown in Figure~\ref{fig:example}. Detailed prompts for each subtask can be found in Appendix~\ref{appendix:prompts_evaluation}.
% The metrics used to compute the success on each task are located in Appendix~\ref{appendix:metrics}.

\label{sec:task_definition}
% Behavior
% A model is first provided with a video and a general description of the activity, and then asked to choose the most normatively appropriate next action to perform.\footnote{In the context of our benchmark, we use ``normative behavior'' and ``normative action'' interchangeably.}

\paragraph{Subtask 1: Action Selection.}  In this subtask, the model is provided with video frames of an activity and five candidate actions. Given these inputs, the model is asked to select the single most normatively appropriate action to perform in the context.\footnote{In the context of our benchmark, we use ``normative behavior'' and ``normative action'' interchangeably.} We enforce strict plausibility constraints on possible answers to ensure that the correct action is not trivially identifiable by visually parsing objects in-scene or eliminating obviously non-normative options. Figure~\ref{fig:teaser} shows several example action options, each illustrating a valid next step for the ego in the context of the video. To arrive at the correct choice C, proceeding to the dry ground, the model must consider multiple dimensions of normative behavior like \textcolor[HTML]{246D63}{safety}, \textcolor[HTML]{356ABC}{politeness}, and \textcolor[HTML]{C65B4E}{cooperation}. This subtask tests whether vision-language models can successfully make normative decisions in specific physical contexts. 
% Explain how we enforced this later


% Justification (do we need to explain why we test justification? neither VCR nor NYT captioning paper do)
\paragraph{Subtask 2: Justification Selection.} In this subtask, the model is provided with the same visual input as in Subtask 1 and is asked to select the best justification supporting its chosen normative action. For example, as shown in Figure~\ref{fig:teaser}, the model must select the appropriate justification for choosing action C (\emph{proceeding to the dry ground first}) instead of directly offering help or simply moving away. This subtask enables the benchmark to qualify whether the model can identify the relevant context and articulate the correct underlying reasoning for its normative decision, serving as a finer measure of normative reasoning. % , elements critical to normative reasoning.

% Assuming models need to provide natural-language justification for norm-guided actions in real-world deployment, we evaluate whether the model identifies the correct context and hypothetical reasoning in generating the normative action. \hao{Similarly here, could you point to the example in Figure 1}

\paragraph{Subtask 3: Sensibility.} To measure whether models understand the features that make action normative in context, we evaluate whether they can select the sensible (i.e. normative, but not necessarily best) options from the given actions. 
%Since this task involves matching of two lists, success is measures through an intersection over union (IoU) metric.


% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\linewidth]{figures/activity-diversity.pdf}
%     \caption{Through automatic clustering with GPT-4o, we categorize the videos into 5 high-level and 23 low-level categories.}
%     \label{fig:activity_cluster}
% \end{figure}


% Action followed
% We further test the ability to identify normative reasoning with action in-scene -- given the
% the ground truth normative behavior, the model must identify whether this behavior is followed in the scene.
% In our evaluation, we measure accuracy by the proportion of tasks for which the model correctly identifies the normative behavior and justification.
% Explain why we don't divide tasks by type of reasoning required
%While it is tempting to cluster tasks by type of inference and information required, real-life normative reasoning requires simultaneous reasoning across types of context; this is reflected in our evaluation tasks. Thus, clustering tasks by type of reasoning required would not be valuable in evaluating the model's ability to perform normative reasoning.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{img/example.pdf}
%     \caption{Example MCQs from \dataset{}. The correct answers are underlined. Three examples illustrate how incorrect physical reasoning can lead to the selection of inappropriate normative actions and justifications.
% In Video 1, the ego is at a scenic spot holding a phone. The normative action in this context would naturally be taking a picture, which Gemini correctly identifies. However, o3 incorrectly concludes that the ego is moving frequently when he is just standing still. As a result, it selects "holding the railing" as the correct action—despite no railing being present in the video.
% In Video 2, the ego is coaching another individual on how to perform a leg exercise by adjusting her position. Gemini misinterprets this as a "leg press exercise", leading to the incorrect conclusion that the appropriate action is to provide support for the "lift". Meanwhile, o3 prioritizes verbal communication, which is a reasonable choice but should not take precedence over actual physical guidance.
% The final video depicts a woman attempting to lift a sofa together with the ego. However, o3 misclassifies this scenario as entertainment rather than labor, resulting in an incorrect selection of both action and justification.
% \yanzhe{need to shorten this.}}
    \caption{Example MCQs with choices by o3-mini (with text descriptions) and Gemini 1.5 Pro (with videos). Correct answers are underlined.
In Video 1, o3-mini incorrectly concludes that the ego is "moving frequently" and wrongly selects "holding the railing" despite no railing being present.
In Video 2, Gemini misinterprets the scene as a "leg press exercise" and incorrectly opts to support a "lift".
In Video 3, o3-mini mistakenly categorizes this scenario as entertainment instead of housework, overlooking the fact that the women need assistance.}

    \label{fig:example}
\end{figure*}

\subsection{Benchmark Generation Pipeline}
\label{sec:benchmark_generation_pipeline}

The benchmark generation pipeline is described in Figure~\ref{fig:pipeline}. 
% A more detailed overview of the pipeline and methodology can be found in Appendix \ref{appendix:BGPD}.
Appendix~\ref{appendix:BGPD} contains a more detailed overview of the pipeline and methodology.
The pipeline consists of the the following steps:

\noindent\textbf{Phase I: Snippet Sampling.} 
% \dataset{} sources its videos from the Ego4D dataset ~\cite{grauman2022ego4dworld3000hours}, consisting of 3650 hours of richly annotated egocentric footage of commonplace human activities in context.
We sourced video samples from Ego4D~\cite{grauman2022ego4dworld3000hours} as it matches the egocentric embodiment of human normative reasoning. % \footnote{In other words, places one into the norm-resolution situation as a first-person actor.} 
To ensure diversity, we applied a multi-step filtering process, sampling each unique scenario-verb combination to select video snippets across a wide range of social and physical contexts.

%First, we removed videos featuring only a single actor, as these typically lack the complex social interactions central to our study. Next, we analyzed the narrations to extract verb-scenario combinations, treating each unique combination as a distinct interaction category. %By sampling up to three instances per unique combination, our uniqueness filter minimizes redundancy while representing a wide range of natural social and physical contexts. Finally, we excluded game-related scenarios to further emphasize interactions that reflect everyday human experiences. 
% \hao{no need to emphasize the stats of Ego4d, please describe how uniqueness filter works and how this addresses the first goal: diversity}
%(2) It includes over 3.85 million \textbf{action-centric visual narrations}, facilitating the identification of unique actions.
%(3) Its \textbf{diverse} range of situations and actions enables EgoNormia to comprehensively explore the space of physical-social norms.
% (1) the egocentric perspective matches the embodiment of humans and the typical embodied systems this benchmark is intended to support;
% (2) it features over 3.85 million action-centric visual narrations, which aid in targeting unique behaviors; and 
% (3) it is situation- and action- diverse, enabling EgoNormia to span the space of physical-social norms.

% We sampled all narrations mentioning two or more actors, then PoS-tagged the target narrations and clustered by verb category and scenario. From this set, a maximum of three samples were taken from each verb-scenario combination, in order to select a maximally long-tailed set of samples. Any scenarios involving card or board games were excluded, as these present monotonic situations where action alternatives relate to game rules instead of human social or physical norms.

% \noindent Diversity in samples was enforced through leveraging Ego4D's native action narrations and scene descriptions.

%by selecting unique combinations of verbs and situations, yielded 4446 unique samples, sourced from from unique 1870 videos.

%We created a diverse dataset by selecting narrations that involved multiple actors, analyzing the verbs and scenarios present, and sampling up to three instances from each unique combination while excluding game-related scenarios to focus on natural social and physical interactions. This curation 


\noindent\textbf{Phase II: Answer Generation.}
% For each video sample, four actions and justifications (one gold-standard pair and three distractor pairs) are generated using a structured, multi-shot pipeline with GPT-4o-based Chain-of-Thought prompting~\citep{wei2022chain}. 
% See Appendix~\ref{appendix:prompts} for detailed prompts.
For each video sample, we generate four pairs of actions and justifications---one ground truth pair and three distractor pairs.\footnote{None is added after generation to create five total options.} To create challenging distractors, we systematically perturb the original context by altering key details that influence the interpretation of the action, leading to plausible alternatives that require normative knowledge to disambiguate. Detailed prompts for answer generation can be found in Appendix~\ref{appendix:prompts_mcq}.

%For example, as illustrated in Figure~\ref{fig:teaser}, although all actions originate from the context “someone is stuck in mud,” different perturbations yield distinct interpretations. Option A is optimal when the context is a casual mud party without safety concerns. Option B fits a scenario where the context is perceived as a single-person mud race competition. Option C is the correct choice, corresponding to the actual scene where your hiking partner is stuck in mud with dry ground nearby. Option D applies when the context is that the stuck person is an experienced wild exploration leader who has explicitly signaled you to back off. 

%These options challenge model to correctly understand context in the video and perform correct normative reasoning in corresponding scene. By carefully perturbing contextual details, these distractors require models to accurately interpret the nuances in video context and perform appropriate normative reasoning. 
% \hao{No need to emphasize GPT or CoT. Emphasize how you perturb the context and why that leads to challenging distractors. One example, you can still use the hiking one.}

% For each video sample, four actions and justifications (one gold-standard pair and three distractor pairs) are generated. To ensure challenging distractors, we generate these distractors based on modified context. For example, the context in Figure ~\ref{fig:teaser}, though all actions are generated based on the context "someone is stuck in mud", option A could be optimal when you are just playing with your friend in a mud party where no satety concern exists, option B is the best when you are watching your friend finishing a single person mud race competition, option D would be best if the person is an experienced your wild exploration leader and has explictly signaled you to back off to give them the space to free themselves.
% See Appendix~\ref{appendix:prompts} for detailed prompts. \hao{No need to emphasize GPT or CoT. Emphasize how you perturb the context and why that leads to challenging distractors. One example, you can still use the hiking one.}

% Frames of sampled snippets of \textbf{Phase \Roman{num}} are first processed with a VLM to extract a scene context description $c$, consisting of the activity, the identities of the people involved, and the environment.
% The context $c$ are then corrupted via LLM to programmatically modify the core context, to change the norms that are relevant in the context. Here, we leverage the defeasibility and compositionality of norms explored by NormBank ~\cite{ziems-etal-2023-normbank} to add, remove, or modify elements of the context, yielding three additional contexts, which form the context set.
% % Further details on the corruption methodology are included in Appendix XXX. (Rejection of police/criminal/spy contexts)
% Then an LLM generates a noisy set of actions $A^+$ and their justifications $J^+$ for each context $c$ in the context set, where the LLM is directed to generate the best action to perform in that given context, a justification for why that norm is most important, and also the categories to which each action belongs to. These are generated in a multi-turn way, where each inference uses the result of the previous stage as part of its input.
% % Experimentation with single shot direct-from-video generation yielded generated \texttt{A}s that were similar to each other; this was rejected as the generation approach.

\noindent\textbf{Phase III: Filtering.}
The output of the second stage consists of high-quality but potentially noisy tasks; answers might be trivially resolvable, ambiguous, or nonsensical. Thus we perform \textbf{normativity filtering} by using chained LLMs to filter for answer feasibility and sensibility, then run \textbf{blind filtering} (i.e. no vision input) to remove questions answerable without context or through superficial reasoning, as these do not test \textit{embodied} normative reasoning, leaving only challenging, context-dependent questions.

%This filtering process eliminates tasks that might be easily solved through superficial cues in options, ensuring that only challenging, context-dependent questions are retained. % As detailed in Table~\ref{tab:results}, the blind filtering mechanism successfully reduces the performance of even the best blind models to below chance-level accuracy, indicating that blind filtering effectively removes NLP-resolvable tasks from our dataset.

% \hao{Add explanation why this make the task challenging. Also point to results to show that blind filters really work for all models (<chance level accuracy for all blind models).}
% Further issues include failed or malformed generations, or the desired output structure not being matched.
% Thus, we refine $A^+$ and $J^+$ with several filtering rounds to ensure the correctness, context-dependence, and high difficulty of questions, to yield a filtered $A$ and $J$ for each example: (i) \textbf{Normativity filtering}: We remove certain action descriptions can describe an action that's not feasibility or is harmful in any situation.
% % - for instance, the action "Grab the lady's purse and run" is illegal through text parsing alone; this class of \texttt{AJs} trivialize the downstream task. Thus, each answer is individually inspected for safety and feasibility, any failing answer is regenerated and re-tested until the full set passes.
% (ii) \textbf{Blind filtering.} To enforce EgoNormia tasks requring grounded visual reasoning to solve, a "blind" baseline is compiled: Any task whose gold standard answer is obviously correct without context, either due to nonsensical answers or leaky domain knowledge, is filtered out as they do not test visual normative reasoning. 
% Due to the low random success rate (4.0\%), blind filtering did not substantially risk removing data points that were well-formed but the blind model was able to guess successfully.

\noindent \textbf{Phase IV: Human Validation.}
Finally, two human validators are employed to verify the correct behavior and justification (manually adding them if not present or ambiguous), and to select the list of actions that are considered sensible. Two validators are used to ensure every datapoint receives independent agreement from two humans, ensuring that human agreement on \dataset{} is replicable.
The authors manually process datapoints where validators disagree on answers, ensuring that the benchmark remains challenging and achieves high human agreement. The detailed procedures for onboarding and training the human annotators, as well as the instructions for the curation process are provided in Appendix~\ref{sec:HumanValidationProcess}. 
% \hao{Say this addresses the 3rd desiderata -- human consensus}

%Annotators are responsible for three key tasks: for each example, verifying that the best action and justification are present in $A$ and $J$ without overlapping in meaning with any other alternatives; selecting other given actions and justifications that are appropriate in the given situation but do not represent the most normative choice; and confirming whether the best action $a$ is followed in the video afterwards. 
% Annotators are tasked with three primary responsibilities: (A) Verify that the best action \texttt{A} and justification \texttt{J} are present in \texttt{AJT+}, and do not overlap in meaning with any other \texttt{AJ} (B) Select the list of other given actions and justifications that are sensible in the given situation, but not the best action - i.e. actions that are expected in that context, but are not the most normative. 
% (C) Confirm whether the best action \texttt{A} is followed in-scene.
% (ii) Two annotators must agree on the best action $a$ for a given $A$ and $J$ to be accepted; they are allowed to provide their own preferred $a$ and $j$ if no answer is correct. In cases of new annoated actions, $A$ and $J$ are manually reconciled by the authors and either modified or rejected outright. This reduces the number of admissible samples by 50\%. 
% (iii) Finally, a second expert curation round is performed, to manually validate the difficulty and diversity of each sample. Only ~85\% of the examples that pass the first round also pass the second round, demonstrating the relative difficulty of generating nontrivial grounded norm-resolution situations.

\subsection{EgoNormia Statistics}
\label{sec:stats}
% Table~\ref{tab:dataset_statistics} presents the summary statistics for \dataset{}. The dataset comprises 1,856 data points sourced from 1,077 videos, averaging approximately 1.7 samples per video. To ensure high quality, we filtered out 58.3% of the original samples from Ego4D. Despite this aggressive filtering, the number of unique scenarios and actions per data point remains relatively stable, indicating that we successfully preserved the dataset's diversity.

The final \dataset{} split comprises a total of 1853 data points sourced from 1077 videos, averaging approximately 1.7 samples per video. 58.3\% of the initially sampled data points from Ego4D were filtered in prior processing steps. % Despite this aggressive filtering, the number of unique scenarios and actions per data point remains relatively stable, indicating that we successfully preserved the dataset's diversity. 
% \noindent 
Appendix~\ref{appendix:statistics} provides additional statistics for \dataset{}. Figure~\ref{fig:diversity} illustrates the distribution of activities in our dataset. We employ an automatic clustering method—--detailed in Appendix~\ref{appendix:clustering}—--that leverages GPT-4o to group the videos into 5 broad categories and 23 finer-grained subcategories.

% norms, for an average of 2.63 constraints per norm.
% The SCENE taxonomy broadly captures the kinds
% of constraints annotators were looking for 94% of

% EgoNormia consists of 1856 samples from Ego4D dataset that span the domain of physical-social norms, covering 108 commonplace scenarios.
% **Talk about taxonomy categories