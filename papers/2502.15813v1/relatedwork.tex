\section{Literature Review}
\subsection{Introduction}

Accurate stock market prediction remains challenging due to market volatility, non-linearity, and sensitivity to various factors. Traditional statistical methods such as linear regression and autoregressive models often fail to capture complex dependencies and temporal dynamics in financial data \citep{gandhmal2019}. This has led to increased interest in machine learning and deep learning approaches, which handle complex data sets more effectively and adapt to market fluctuations, improving the modelling of inter-stock relationships and temporal trends \citep{senapati2018}.

Early machine learning models such as Support Vector Machines (SVM) and Artificial Neural Networks (ANN) improved on traditional methods by identifying historical patterns, but struggled with the sequential nature of financial time series. Convolutional neural networks (CNNs) excel in capturing spatial patterns but are less effective with the temporal dependencies essential for stock forecasting \citep{senapati2018}. To address these limitations, advanced models such as Long-Short-Term Memory (LSTM) networks and Graph Neural Networks (GNNs) have emerged. LSTMs model long-term dependencies in time-series data, while GNNs capture relational data among stocks. Combining these models into hybrid architectures has shown promise in analysing both temporal and relational aspects of stock data \citep{thakkar2021, ran2024}.

Explainable AI (XAI) has gained importance in financial forecasting by enhancing transparency and making AI model decisions more interpretable, which is crucial for trust and compliance in financial markets. Studies by \citet{kuiper2022} highlight that integrating XAI improves decision-making by elucidating how predictions are made.

\subsection{Data Acquisition and Challenges}

Developing reliable stock market prediction models requires accurate data acquisition, but financial data is inherently noisy, incomplete, and unpredictable, posing challenges for machine learning algorithms \citep{hadavandi2010}. Robust preprocessing techniques, including handling missing data and noise management, are essential for enhancing data quality. Optimising hyperparameters during data processing is vital, especially with large and complex datasets. Scaling and normalisation improve the accuracy of models such as LSTM by maintaining variable relationships and preventing data distortion. Detecting and managing noise in financial time series further enhances model resilience in unpredictable market conditions \citep{yeung2020}. Thus, advanced preprocessing and meticulous hyperparameter tuning are necessary for accurate stock market predictions.

\subsection{Machine Learning Approaches in Financial Forecasting}

LSTM networks have become prominent in financial time-series prediction due to their ability to capture long-term dependencies and manage the vanishing gradient problem, making them suitable for modelling the non-linear and volatile nature of stock prices \citep{fjellstrom2022}. They have been applied successfully in various financial contexts, demonstrating adaptability in recognising complex patterns and robustness in volatile markets \citep{yeung2020, zhao2023}. Comparative studies have shown that LSTM outperforms traditional models such as ARIMA in stock price forecasting, particularly with non-linear time-series data \citep{shankar2022, jarrah2023}.

GNNs are essential for analysing stock relationships by capturing dependencies between stocks, modelling the interconnectedness that traditional methods overlook. \citep{shi2024} developed a graph-based CNN-LSTM model integrating relational data with time-series analysis, achieving more accurate predictions by leveraging GNN to capture stock interconnections. Other studies have emphasised the importance of capturing relational dependencies using Graph Convolutional Networks (GCN), showing that GCN outperforms traditional time-series models by considering both temporal and relational dynamics \citep{singh2021, chen2018}. These findings illustrate that GNNs improve stock market predictions by capturing complex relationships between stocks, and when combined with models such as LSTM, they effectively handle both relational and temporal dynamics.

Hybrid models integrating LSTM networks with GNNs leverage the strengths of both methods, enabling simultaneous modelling of temporal sequences and relational data. \citet{cheng2022} demonstrated significant enhancements in predictive accuracy by combining relational data from GNN and temporal patterns from other models. \citet{shi2024} found that such hybrid models achieve more accurate predictions by capturing both temporal dynamics and inter-stock relationships. However, these models face challenges such as increased computational demands and risks of overfitting or data leakage if not properly implemented \citep{tang2021}. Careful implementation and validation are necessary to avoid these pitfalls \citep{mehtab2020}.

Dynamic modelling approaches such as rolling window and expanding window analyses are essential for adapting to evolving patterns in stock market data. Rolling window analysis trains the model on a fixed window of recent data, shifting forward with each new prediction, which is effective for short-term predictions \citep{matsunaga2019}. Expanding window analysis enlarges the training dataset by adding new data while retaining all past observations, outperforming rolling windows in capturing long-term volatility patterns \citep{feng2024}. These methods enhance the adaptability of predictive models but can introduce biases if not properly managed, necessitating a balanced approach to optimise continuous learning while maintaining historical integrity.

\subsection{Research Challenges}

Despite the advancements in stock market prediction, several outstanding research issues persist:

\begin{enumerate}
    \item \textbf{Integration of Temporal and Relational Models:} While hybrid models combining LSTM and GNN have shown potential, empirical evaluations in real-world conditions remain limited. There is a need for more extensive studies assessing their performance in volatile, real-time trading environments\citep{chen2018, shi2024}.

    \item \textbf{Robust Data Handling:} Many models overlook real-time challenges such as noisy data, missing values, and market shifts. Although preprocessing methods have been proposed \citep{bhanja2018, yeung2020}, these need further refinement to better handle the complexities of financial data and enhance model resilience.

    \item \textbf{Explainability:} The lack of transparency in LSTM and GNN models poses challenges in interpreting predictions. While Explainable AI (XAI) offers solutions \citep{kuiper2022}, its application in financial models is still limited. Integrating XAI techniques could improve trust and compliance in AI-driven financial forecasting.

    \item \textbf{Scalability and Efficiency:} Hybrid models are often computationally intensive, making real-time application difficult. Future research should focus on optimising these models for better scalability without sacrificing accuracy, possibly through algorithmic innovations or hardware acceleration.

    \item \textbf{Real-Time Adaptation:} Although expanding window analysis improves real-time predictions \citep{feng2024}, models require better strategies for continuous adaptation to new data in fast-changing markets. This includes developing methods to quickly retrain models or update predictions without extensive computational overheads.
\end{enumerate}