\section{Introduction}
\label{intro}

\begin{figure}[t!]
    \includegraphics[width=\columnwidth]{figs/teaser4.pdf}
~\vspace{-2em}
\caption{\small \it
Comparison of Text-to-Image Generation: unguided sampling, guided sampling, and DICE.  
{\bf Top}: Average aesthetic score~\cite{schuhmann2022laion} over $5,000$ generated images plotted against the {\it number of function evaluations} (NFE). {\bf Bottom}: An example of image synthesis using different methods at NFE = 4,8,12, and 16. All images are generated by DreamShaper~\cite{DreamShaper} with the same random seed. Notably, compared to CFG-based sampling, \ourName achieves comparable image quality with only half the NFE.
}
\label{fig:teaser}
~\vspace{-2em}
\end{figure}

Recently, diffusion models~\cite{sohl2015deep,song2019ncsn,ho2020ddpm} have achieved remarkable advances, driven by continuously refined theoretical frameworks~\cite{song2021sde,karras2022edm,chen2024trajectory,kingma2024understanding} and fast evolution of effective model architectures~\cite{peebles2023scalable,bao2023all}.
Their impressive generation ability has brought text-to-image generation to unprecedented levels~\cite{rombach2022ldm,saharia2022photorealistic,podell2024sdxl,esser2024scaling}, and enables a multitude of new conditional generation tasks~\cite{croitoru2023diffusion,zhan2024conditional}.

In text-to-image generation, diffusion models use text embeddings produced by pre-trained encoders such as CLIP~\cite{radford2021learning} and T5~\cite{raffel2020exploring}~\cite{nichol2022glide,rombach2022ldm,saharia2022photorealistic}, which are vectors encapsulating the semantic content in the text prompts. However, the text embeddings are not specifically tailored for image generation. Moreover, images often encompass more detailed information than text prompts can convey, making precise text-image semantic alignment challenging~\cite{schrodi2024two}. Consequently, as illustrated in \cref{fig:teaser}, sampling with text-to-image models in the original conditional manner—referred to as \textit{unguided sampling}—generally results in blurry, distorted, and semantically inaccurate outputs~\cite{meng2023distillation,karras2024guiding}. To address the limited semantic signals provided by text embeddings, \textit{guided sampling} techniques~\cite{dhariwal2021diffusion,ho2022classifier} have been introduced to steer samples toward a more concentrated distribution, enhancing image quality and alignment with the text prompts.  

Classifier-Free Guidance (CFG)~\cite{ho2022classifier} is a widely adopted technique for guided sampling. It directs the generative process at each sampling step by extrapolating the direction between the original conditional prediction and an unconditional prediction, with the guidance strength modulated by a hyperparameter known as the guidance scale. The application of CFG enhances the quality of the image and the alignment of the text with the image, establishing it as a practical and essential method. However, CFG requires an additional model evaluation at each step, thus increasing the sampling overhead~\cite{ho2022classifier}. Moreover, it disrupts the ability of diffusion models to capture the underlying data distribution, complicating the interpretation of sampling dynamics and causing a divergence from their well-established theoretical foundations~\cite{karras2024guiding,zheng2024characteristic,bradley2024classifier}. 

To mitigate the increased sampling overhead, prior research distilled CFG into a single model evaluation at each sampling step. These approaches either fine-tune all parameters of the text-to-image model~\cite{meng2023distillation,li2024snapfusion} or train an auxiliary guided model~\cite{hsiao2024plug}. However, the theoretical issues associated with CFG in the distilled model remain unresolved. 


In this paper, we introduce \textbf{D}\textbf{I}stilling \textbf{C}FG by enhancing text \textbf{E}mbeddings (\ourName), a novel approach to achieving high-quality sample generation with unguided sampling by enhancing the model's conditioning, specifically through optimized text embeddings (see \cref{fig:comparison}). These embeddings are refined under CFG-based supervision by training a lightweight enhancer that operates independently of the primary text-to-image model. With these optimized embeddings, our strengthened unguided sampling accelerates high-quality image generation, matching the performance of guided sampling. \ourName maintains consistent theoretical foundations with traditional unguided sampling, offering improved interpretability in sampling dynamics. In addition, we show that \ourName embeddings can be used across different base models. This is due to the property that \ourName preserves essential semantic information while enhancing fine-grained details in the generated images. Extensive experiments across various text-to-image models, encompassing different model capacities, image styles, and network architectures, validate the effectiveness of our method in diverse scenarios. Furthermore, \ourName supports negative prompts for image editing to improve image quality further. 


