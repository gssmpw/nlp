\section{Method}
\label{sec:method}


\begin{figure}[t]
% \framebox[\columnwidth]{\rule{0pt}{4cm}}
\centerline{\includegraphics[width=\columnwidth]{figs/strengthen.pdf}}
`\vspace{-2em}
\caption{\small \it Images generated 
by Stable Diffusion v1.5~\cite{rombach2022ldm} on the CelebA-HQ dataset~\cite{karras2017progressive} using 10-step DPM-Solver++~\cite{lu2022dpmpp}.
Unguided sampling usually generates distorted or semantically inaccurate images. By fine-tuning a predefined text embedding, the optimized unguided sampling generates decent images on par with that obtained by the guided sampling. Images in the same column are generated with the same seed.}
\label{fig:motivation}
%\vskip -0.2in
 ~\vspace{-3em}
\end{figure}


%\subsection{Strengthen Unguided Sampling}
%\label{sec:dice}
Text-to-image diffusion models are trained on extensive datasets of text-image pairs~\cite{rombach2022ldm,podell2024sdxl,esser2024scaling}. In this process, text prompts are first encoded into embeddings using pre-trained text encoders~\cite{radford2021learning,raffel2020exploring} and then integrated into the model's inference through cross-attention modules. However, these models often struggle to generate images that closely align with the provided text prompts when using unguided sampling.

We hypothesize that this misalignment stems from two primary factors. First, there is an information imbalance between text and images. Images encapsulate rich details such as layout, texture, and fine-grained elements, whereas manually annotated captions typically describe only the main concepts~\cite{radford2021learning,schuhmann2022laion}. This disparity leads to a well-known modality gap between texts and images~\cite{liang2022mind,schrodi2024two}. Second, given the vastness of the text prompt space, the dataset may lack prompts specifically designed by users.

Consequently, text embeddings from pre-trained text encoders generally lack sufficient information to fully describe images. Simultaneously, the text-to-image model is tasked with performing few-shot or even zero-shot generation, contributing to the challenges observed in unguided sampling. 

Unguided sampling, while offering rapid execution and solid theoretical underpinnings, often yields subpar sample quality. To enhance this, we aim to enrich each text embedding with more detailed information about its corresponding image. A straightforward approach is fine-tuning the embeddings of a specific text prompt. For instance, as shown in \cref{fig:motivation}, we achieve optimized unguided sampling that produces images comparable in quality to those generated through guided sampling. 
This preliminary study confirms the effectiveness of strengthening a single text prompt. However, fine-tuning on every possible text prompts is impractical and risks knowledge degradation. 

\subsection{Our Proposed DICE}
\label{sec:dice}

We present \textbf{D}\textbf{I}stilling \textbf{C}FG by enhancing text \textbf{E}mbeddings (\ourName) that removes the reliance on CFG in the generative process while maintaining the benefits it provides. \ourName distills a CFG-based text-to-image diffusion model into a CFG-free version by refining the text encoder to replicate the CFG-based direction. DICE avoids the computational and theoretical drawbacks of CFG and enables high-quality, well-aligned image generation at a fast sampling speed.

Specifically, given a text embedding $\bfc$ encoded by the text encoder, we train a lightweight \textit{enhancer} $r_{\phi}(\cdot,\cdot): \bbR^{(K\times d_e) \times (K\times d_e)} \rightarrow \bbR^{K \times d_e}$ with the trainable parameters $\phi$, to strengthen the original text embedding, i.e., 
\begin{equation}
    \label{eq:enhancer}
    \bfc_{\phi} = \bfc + \alpha r_{\phi}(\bfc,\bfc_\text{null}),
\end{equation}
where $\alpha$ is a hyperparameter controlling the strength.
Similar to \cref{eq:euler}, the unguided sampling becomes
\begin{equation}
    \label{eq:euler_cfg}
    \bfx_s = \bfx_t + \left(s-t\right)\bfeps_{\theta}(\bfx_t,\bfc_{\phi}).
\end{equation}
We train the enhancer under CFG-based supervision and keep the model parameters $\theta$ completely frozen. 
Given image-embedding pairs $(\bfx_0, \bfc)$, the training loss of the enhancer is designed in a distillation manner, as,
\begin{equation}
    \label{eq:loss}
    \bbE_{t \sim \mathcal{U}(0,T), \bfx_t \sim \mathcal{N}(\bfx_0,t^2\bfI)}\lVert \bfeps_{\theta}(\bfx_t,\bfc_\phi) - \bfeps_{\theta}^{\omega,\bfc_\text{null}}(\bfx_t,\bfc) \rVert,
\end{equation}
where the parameter to learn is $\phi$ with $\theta$ fixed. In experiments, we set the guidance scale $\omega$ to 5. The detailed algorithm is shown in \cref{alg:alg}. %, and \cref{fig:comparison}. 


Different from the most related works~\cite{meng2023distillation,hsiao2024plug}, which also reduce the sampling overhead brought by CFG, our method completely decouples the enhancer from the text-to-image model by only modifying the model condition, or specifically, text embeddings. Therefore, the text-to-image model still predicts the score function with respect to the implicit noised data distribution but conditioned on $\bfc_\phi$, i.e., $\nabla_{\bfx_t} \log p_{t}(\bfx_t|\bfc_\phi)$. 
Consequently, the strengthened unguided sampling is grounded by the original theoretical foundations of diffusion models and generates samples from $\{p_{t}(\bfx_t|\bfc_\phi)\}_{t=0}^T$ theoretically. A more detailed comparison to previous works is included in \cref{sec:comparison_w_distillation}.


\begin{algorithm}[t]
   \caption{\ourName Training}
   \label{alg:alg}
\begin{algorithmic}
   \STATE {\bfseries Input:} dataset $\mathcal{D}$, guidance scale $\omega$, maximum timestamp $T$, text-to-image model $\bfeps_{\theta}(\cdot,\cdot)$, null text embedding $\bfc_\text{null}$, learning rate $\eta$
   \STATE {\bfseries Initialize:} enhancer $r_{\phi}(\cdot,\cdot)$
   \WHILE{not converged}
   \STATE Sample image-embedding pairs $(\bfx_0, \bfc) \sim D$
   \STATE Sample a timestamp $t \sim \mathcal{U}(0, T)$
   \STATE Forward diffusion process $\bfx_t \sim \mathcal{N}(\bfx_0, t^2I)$
   \STATE $\bfeps_{\theta}^{\omega,\bfc_{\text{null}}}(\bfx_t,\bfc) = \omega \bfeps_{\theta}(\bfx_t,\bfc) - \left(\omega - 1\right)  \bfeps_{\theta}(\bfx_t,\bfc_{\text{null}})$
   \STATE $\bfc_\phi = \bfc + \alpha r_{\phi}(\bfc,\bfc_\text{null})$
   \STATE $\mathcal{L}(\phi) = \lVert \bfeps_{\theta}(\bfx_t,\bfc_\phi) - \bfeps_{\theta}^{\omega,\bfc_\text{null}}(\bfx_t,\bfc) \rVert$
   \STATE $\phi \leftarrow \phi - \eta \nabla_{\phi}\mathcal{L}(\phi)$
   \ENDWHILE
\end{algorithmic}
\end{algorithm}



\subsection{Distilling Negative Prompts}
\label{sec:negative}
In text-to-image generation, descriptive text prompts are typically termed as positive prompts. However, images generated solely from positive prompts may sometimes contain unnatural elements, such as extra limbs or unwanted objects, or may not meet the desired quality standards. To address these issues, negative prompts are employed for image editing and quality enhancement. 

Given an embedding of a negative prompt (e.g., ``bad quality'') denoted as $\mathbf{c}_n$, it replaces the null text embedding in the classifier-free guidance (CFG) equation:
\begin{equation} \label{eq:cfg_neg} 
\bfeps_{\theta}^{\omega,\bfc_n}(\bfx_t,\bfc) = \omega \bfeps_{\theta}(\bfx_t,\bfc) - (\omega - 1) \bfeps_{\theta}(\bfx_t,\bfc_n). 
\end{equation}
Previous works that distill the guidance scale omit the entry for negative prompts, limiting practical applicability. Conversely, retaining the negative prompt in Equation \ref{eq:cfg_neg} perpetuates the same issues inherent in CFG. To overcome this, we integrate the embedding of negative text prompts into the enhancer and strengthen the original embedding by $\bfc_\phi = \bfc + \alpha r_{\phi}(\bfc, \bfc_n)$. During training, negative prompts are randomly sampled from open-source datasets, and the training process remains consistent with Algorithm 1, except that negative text embeddings replace all null text embeddings.