\section{Experiments}
\label{exp}

\begin{figure*}[t!]
\includegraphics[width=\textwidth]{figs/qualitative_new.pdf}
\caption{\small \it
Qualitative results on text-to-image models with different model capacities, image styles and network architectures. Images are generated by 20-step DPM-Solver++~\cite{lu2022dpmpp} on 7 text-to-image models including multiple Stable Diffusion v1.5 variants~\cite{rombach2022ldm}, SDXL~\cite{podell2024sdxl} and Pixart-$\alpha$~\cite{chen2024pixart}. The used prompts are provided in \cref{sec:app_details}.
}
\label{fig:qualitative}
~\vspace{-1em}
% \vskip -0.1in
\end{figure*}

\begin{figure*}[t]
    \includegraphics[width=0.99\textwidth]{figs/ablate_nfe.pdf}
    \caption{\small \it The comparison of FID (left), CLIP Score (middle), and Aesthetic Score (right) with respect to different numbers of function evaluations (NFE). Our proposed \ourName converges faster than the guided sampling based on CFG. 
    }
    \label{fig:ablate_nfe}
~\vspace{-1em}
\end{figure*}

\subsection{Settings}
\textbf{Models and datasets.} Our experiments are conducted on Stable Diffusion v1.5 (SD15)~\cite{rombach2022ldm}, Stable Diffusion XL (SDXL)~\cite{podell2024sdxl}, Pixart-$\alpha$~\cite{chen2024pixart} and a series of SD15-based open source variants, 
including DreamShaper\footnote{\url{https://huggingface.co/Lykon/DreamShaper}}, 
AbsoluteReality\footnote{\url{https://huggingface.co/digiplay/AbsoluteReality_v1.8.1}},
Anime Pastel Dream\footnote{\url{https://huggingface.co/Lykon/AnimePastelDream}},
DreamShaper PixelArt\footnote{\url{https://civitai.com/models/129879/dreamshaper-pixelart}}, 
and 3D Animation Diffusion\footnote{\url{https://civitai.com/models/118086?modelVersionId=128046}}. 
A detailed model summary is provided in \cref{sec:app_details}. We use MS-COCO 2017~\cite{lin2014microsoft} for training and evaluation.

\textbf{Training.} The lightweight enhancer consists of two fully-connected layers and an attention block. The number of trainable parameters is negligible compared to that of the text-to-image model (less than 1\%), resulting in negligible increased computational overhead. 
The strength $\alpha=1$ and guidance scale $\omega=5$ are fixed for all experiments. For optimization, we use Adam optimizer~\cite{kingma2014adam} with $\beta_1=0.9$, $\beta_2=0.999$, learning rate of 2e-4 and batch size of 128. Enhancers for SD15-based variants are trained with negative prompts. All enhancers are trained with around 8,000 gradient updates, requiring 4.5, 6, 18 hours for SD15-based variants, Pixart-$\alpha$ and Stable Diffusion XL, respectively, using 8 NVIDIA A100 GPUs.

\textbf{Evaluation.} The sample quality is evaluated by Fréchet Inception Distance (FID)~\cite{heusel2017gans}, CLIP Score (CS)~\cite{radford2021learning} and Aesthetic Score (AS)~\cite{schuhmann2022laion}. To compute FID, we generate 5K images using 5K prompts sampled from MS-COCO 2017 validation set and use the validation set as reference images. The generated 5K images are also used to compute CS and AS.



\begin{table}[t]
\captionsetup{skip=5pt}
\caption{Quantitative results evaluated by Fréchet Inception Distance (FID), CLIP Score (CS) and Aesthetic Score (AS). Images are generated with the same random seeds by 20-step DPM-Solver++~\cite{lu2022dpmpp}. NFE denotes the number of function evaluations. GD denotes Guided Distillation~\cite{meng2023distillation}, which is reimplemented with the same training iterations as our method.}
\label{tab:quantitative}
% \vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{lcccccc}
\toprule
Model & $\omega$ & FID ($\downarrow$) & CS ($\uparrow$) & AS ($\uparrow$) & NFE \\
\midrule
SD15                        & 1 & 32.80 & 21.99 & 5.03 & 20 \\
SD15                        & 5 & \textbf{22.04} & \textbf{30.22} & \textbf{5.36} & 40  \\
+ GD                        & 5 & 26.63 & \underline{29.22} & \underline{5.30} & 20  \\
\rowcolor[gray]{0.9} + \ourName & 1 & \underline{22.22} & 28.54 & 5.28 & 20 \\
\midrule
DreamShaper                 & 1 & \textbf{24.17} & 27.22 & 5.74 & 20 \\
DreamShaper                 & 5 & \underline{30.35} & \textbf{30.50} & \textbf{5.87} & 40 \\
+ GD                        & 5 & 36.24 & \underline{29.66} & 5.72 & 20  \\
\rowcolor[gray]{0.9} + \ourName & 1 & \underline{30.36} & 29.03 & \textbf{5.87} & 20 \\
\midrule
SDXL                        & 1 & 61.19 & 21.92 & 5.59 & 20 \\
SDXL                        & 5 & \textbf{23.95} & \textbf{32.10} & \underline{5.60} & 40 \\
\rowcolor[gray]{0.9} + \ourName & 1 & \underline{28.01} & \underline{30.63} & \textbf{5.68} & 20 \\
\midrule
Pixart-$\alpha$             & 1 & 41.74 & 25.30 & \textbf{6.11} & 20 \\
Pixart-$\alpha$             & 5 & \textbf{38.39} & \textbf{30.67} & \underline{6.03} & 40 \\
\rowcolor[gray]{0.9} + \ourName & 1 & \underline{39.80} & \underline{29.51} & \underline{6.01} & 20 \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.2in
\end{table}



\subsection{Text-to-Image Generation}
\label{sec:t2i}
We evaluate \ourName on text-to-image models of varying capacities, with the number of parameters ranging from 0.6B to 2.6B, architectures such as U-Net~\cite{ronneberger2015u} and DiT~\cite{peebles2023scalable}, and image styles including the dream, realistic, 3-D, pixel, and anime style. The quantitative results are presented in \cref{tab:quantitative}. Our strengthened unguided sampling achieves a sample quality comparable to that of guided sampling and largely outperforms the original unguided sampling with qualitative results shown in \cref{fig:qualitative}. Moreover, with only text embedding modified, \ourName achieves comparable results with Guidance Distillation~\cite{meng2023distillation}, which fine-tunes the whole text-to-image model.


\textbf{Sampling budget.} 
In \cref{fig:ablate_nfe}, we report the performance of our method and guided sampling evolving with the number of function evaluations (NFE). As CFG requires an additional model evaluation (\cref{eq:cfg}), the NFE for every single guided sampling step is 2 and otherwise 1. 
Our method demonstrates superiority over guided sampling when operating under a low NFE budget.
Besides, the sample quality of our method exhibits an early convergence, which can be attributed to the smoother sampling trajectories~\cite{chen2024trajectory} generated by unguided sampling, as revealed in~\cite{zhou2024simple}. More qualitative results are shown in \cref{sec:app_ablation}.


\textbf{Generalization.} As the enhancer operates independently of the text-to-image model, we investigate the feasibility of applying a well-trained enhancer to unseen text-to-image models. We separately train three enhancers (enhancer $i$, $i=1,2,3$) on three distinct text-to-image models, i.e., DreamShaper (model $1$), DreamShaper PixelArt (model $2$) and Anime Pastel Dream (model $3$). Subsequently, we plug each enhancer into all of the models for unguided image generation. The results in \cref{fig:generalization} shows that the enhancers exhibit strong generalization capabilities across diverse domains, consistently and significantly improving the original unguided sampling.

\subsection{Understanding the Enhanced Text Embedding}
\label{sec:discussions}
We next explore the underlying mechanism of our method and show how the enhanced text embeddings impact the sample quality and sampling dynamics through both quantitative and qualitative evidence. 

The text embedding used for text-to-image generation consists of a $\textless SOS\textgreater$ token, some semantic tokens and the remaining padded $\textless EOS\textgreater$ tokens.
As revealed by previous works, e.g.,~\cite{yu2024uncovering}, based on the position of the first $\textless EOS\textgreater$ token, a text embedding can be divided into a semantic embedding that contains most semantic information and a padding embedding that encodes more about the image details. To verify this, in \cref{fig:emb_rep}, we replace the original embedding with enhanced semantic and padding embeddings. To replace the padding embedding, we can first recognize the index of the first $\textless EOS\textgreater$ token. Then, replace the embedding after this token with an enhanced one. We compute the cosine similarity between 1,000 paired original and enhanced semantic embeddings, obtaining a mean value of 0.75 and standard deviation of 0.05, while for padding embeddings, they are 0.23 and 0.02.
This indicates that DICE emphasizes enhancing the padding embedding while mainly maintaining the original semantic embedding, leading to consistent semantic information but significantly improved image details.



\begin{figure}[t]
\begin{center}
\includegraphics[width=\columnwidth]{figs/generalization.pdf}
    \captionsetup{skip=2pt}
    \caption{\small \it Generalization of \ourName text enhancer across different base models. Text enhancer $i = 1,2,3$ is independently trained on model DreamShaper (model $1$), DreamShaper PixelArt (model $2$) and Anime Pastel Dream (model $3$), and applied to other base models. The DICE enhancers lead to better image qualities than the unguided sampling without strengthening text embedding.
    % Text prompt: ``\textit{Close-up photo of a princess}''.
    }
    \label{fig:generalization}
\end{center}
~\vspace{-2em}
%\vskip -0.2in
\end{figure}

\begin{figure}[H]
\begin{center}
    \includegraphics[width=\columnwidth]{figs/emb_rep.pdf}
    \captionsetup{skip=2pt}
    \caption{\small \it Replacing the original text embedding with the enhanced semantic and padding embedding. The enhanced padding embedding largely improves the sample quality. Images are generated by DreamShaper with text prompts ``\textit{a cute Persian cat sitting on the table}'' (top) and ``\textit{photo of an astronaut riding a horse}''.}
    \label{fig:emb_rep}
\end{center}
%\vskip -0.15in
~\vspace{-2em}
\end{figure}


\begin{figure}[t]
    \includegraphics[width=\columnwidth]{figs/ablate_alpha_mecha.pdf}
    \captionsetup{skip=2pt}
    \caption{\small \it Effect of the text enhancer strength $\alpha$. As $\alpha$ increases, the enhancer can maintain the semantic information while improving the sample quality. Images are generated by DreamShaper. 
    }
    \label{fig:ablate_alpha_fig}
%\vskip -0.15in
 ~\vspace{-2em}
\end{figure}


% \textbf{Effect of Strengthening Factor.} 
Serving as both further evidence and an ablation study, we examine the effect of the enhancer strength $\alpha$, which is fixed to 1 during training and $\alpha=0$ corresponds to the original unguided sampling. As shown in \cref{fig:ablate_alpha_fig} (with more quantitative results included in \cref{sec:app_ablation}), with the increase of $\alpha$, the generated image improves with richer details and stronger contrast while preserving consistent semantic information.
These results demonstrate that the enhancer manages to find semantically robust directions that steadily improve the image details. 

\begin{figure}[t!]
    \includegraphics[width=\columnwidth]{figs/attention_corgi_part.pdf}
    \captionsetup{skip=2pt}
    \caption{\small \it Visualization of cross-attention maps as sampling progresses. While all models can reproduce the main concepts, DICE emphasizes refining the image details. 
    }
    \label{fig:attn}
~\vspace{-3em}
%\vskip -0.1in
\end{figure}

% \textbf{Cross-attention maps.} 
In \cref{fig:attn}, we further visualize the cross-attention maps of the U-Net decoder with the 32$\times$32 resolution to show the changes in the sampling dynamics. While the main concepts are activated in all the cases, the enhanced embedding further activates the image details, which can be seen from the constantly changing background of the attention maps. This also aligns with our conclusion of the mechanism of \ourName dawn above.
See \cref{sec:app_ablation} for more results.



\subsection{Negative Prompts}
% \textbf{Negative prompts.} 
As shown in \cref{sec:negative}, \ourName sampling retains the capability of using negative prompts.
In \cref{fig:negative}, we show the effectiveness of \ourName on the two main purposes of using negative prompts.
Our method can perform desirable image editing and quality improvement, including modifying unnatural limbs, removing or changing unwanted features, and handling abstract prompts related to image quality.
\begin{figure*}[t]
    \begin{subfigure}[t]{0.488\textwidth}
        \includegraphics[width=\textwidth]{figs/negative_1.pdf}
        \caption{\small \it Negative prompts for image editing.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.50\textwidth}
        \includegraphics[width=\textwidth]{figs/negative_2.pdf}
        \caption{\small \it Negative prompts for quality improvement.}
    \end{subfigure}
    \captionsetup{skip=5pt}
    \caption{\small \it Performance of \ourName on negative prompts. Positive and negative prompts are denoted by $+$ and $-$.}
    \label{fig:negative}
~\vspace{-2em}
%\vskip -0.2in
\end{figure*}

