\begin{abstract}

Text-to-image diffusion models are capable of generating high-quality images, but these images often fail to align closely with the given text prompts. Classifier-free guidance (CFG) is a popular and effective technique for improving text-image alignment in the generative process. However, using CFG introduces significant computational overhead and deviates from the established theoretical foundations of diffusion models.
In this paper, we present \textbf{D}\textbf{I}stilling \textbf{C}FG by enhancing text \textbf{E}mbeddings (\ourName), a novel approach that removes the reliance on CFG in the generative process while maintaining the benefits it provides. \ourName distills a CFG-based text-to-image diffusion model into a CFG-free version by refining text embeddings to replicate CFG-based directions. In this way, we avoid the computational and theoretical drawbacks of CFG, enabling high-quality, well-aligned image generation at a fast sampling speed. Extensive experiments on multiple Stable Diffusion v1.5 variants, SDXL and PixArt-$\alpha$ demonstrate the effectiveness of our method. Furthermore, \ourName supports negative prompts for image editing to improve image quality further. 
Code will be available soon.
\end{abstract}