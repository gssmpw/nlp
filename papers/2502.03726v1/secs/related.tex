\section{Related Works}
\label{related}

To the best of our knowledge, no existing work has explored the distillation approach to remove classifier-free guidance (CFG). However, recent studies have investigated alternative methods to enhance the efficiency of CFG-based generators.

\textbf{Understanding CFG.} Guided sampling in diffusion models was initially achieved through classifier guidance~\cite{dhariwal2021diffusion}, utilizing an auxiliary classifier trained on noisy data. Classifier-free guidance (CFG)\cite{ho2022classifier} offers a streamlined approach to guided sampling without the need for a separately trained classifier and has become a fundamental technique in text-to-image generation\cite{rombach2022ldm}. Recent efforts have aimed to elucidate its mechanism. For instance, Characteristic Guidance~\cite{zheng2024characteristic} reveals that the score function in CFG does not adhere to the Fokker-Planck equation~\cite{lai2023fp}, and CFG is considered to blindly push samples in a model-preferred direction~\cite{karras2024guiding}. Additionally, a CFG step in stochastic sampling can be decomposed into a deterministic unguided step followed by a Langevin dynamics step~\cite{bradley2024classifier}. To date, the underlying behavior of CFG remains an area requiring further exploration.

\textbf{CFG distillation.} 
Previous works have proposed distilling CFG-based text-to-image models. Guided distillation~\cite{meng2023distillation} incorporates the guidance scale as a new model input through fine-tuning, a method later adopted by SnapFusion~\cite{li2024snapfusion} and FLUX\footnote{\url{https://github.com/black-forest-labs/flux}}. Plug-and-Play~\cite{hsiao2024plug} trains an auxiliary guided model attached to the U-Net decoder, which is transferable to new domains. In contrast, our method solely modifies the text conditioning without altering the generative process of diffusion models. A more detailed comparison is provided in \cref{sec:comparison_w_distillation}.

\textbf{Reward-based methods.} The text encoder plays a crucial role in text-to-image generation. Several studies aim to improve guided sampling by fine-tuning the text encoder through reinforcement learning~\cite{chen2025enhancing} and reward propagation~\cite{li2024textcraftor}. Our method differs in three key aspects. First, it is specifically designed to improve unguided sampling without relying on CFG. Second, it is trained under CFG-based supervision and does not require human feedback or any reward models. Finally, as demonstrated in \cref{sec:comparison_w_reward}, the text embeddings obtained by these methods are unsuitable for unguided sampling, highlighting the superiority of our method's distinct mechanism.
