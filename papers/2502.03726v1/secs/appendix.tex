\appendix
\onecolumn
\section{Appendix}
\label{sec:app}

\subsection{Additional Details}
\label{sec:app_details}

The text prompts and models used to generate \cref{fig:qualitative} are listed in \cref{tab:quantitative_setting}. They are selected to generate images with as many image styles and topics as possible.

In \cref{tab:model_setting}, we summarize all the text-to-image models used in our experiments. SD15 variants~\cite{rombach2022ldm} and SDXL~\cite{podell2024sdxl} use U-Net~\cite{ronneberger2015u} as backbone while PixArt-$\alpha$~\cite{chen2024pixart} uses DiT~\cite{peebles2023scalable}. Different pre-trained text encoders are used for each type of text-to-image model with the number of parameters ranging from different orders of magnitude. 

For the enhancer, we use two fully-connected layers and one cross-attention layer. The first fully-connected layer compresses the input two text embeddings into a context dimension of 512. The obtained two features are then input to the cross-attention layer and are finally extended to the original context dimension through the second fully-connected layer. The number of parameters of the obtained enhancer account for 0.21\%, 0.12\% and 0.86\% of SD15 variants, SDXL and PixArt-$\alpha$, respectively.
The extra sampling overhead of the enhancer is negligible since it only runs once for every generation. Additionally, we find that the performance of the enhancer is insensitive to model structure. The inner context dimension of 512 is set with the aim of reducing the number of parameters of the enhancer.



\begin{table}[t]
\caption{Text-to-image models and text prompts used in \cref{fig:qualitative}.}
\label{tab:quantitative_setting}
% \vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{p{5.5cm}p{10.5cm}}
\toprule
Model & Text prompt \\
\midrule
SDXL~\cite{podell2024sdxl}  & \textit{A rainy street, a racer on a white motorcycle by the street, bright neon lights, cyberpunk style, futuristic, 8k, best quality, clear background} \\
Anime Pastel Dream & \textit{A man in suits and hat, center, close-up, best quality} \\
Pixart-$\alpha$~\cite{chen2024pixart} & \textit{Epic scene, mountains, sunshine, trees, rocks, clear, realistic, best quality, best detail, aesthetic, masterpiece} \\
AbsoluteReality & \textit{Colorful flowers in a vase on a wooden table, sunshine, aesthetic, realistic, 8k, best quality} \\
3D Animation Diffusion & \textit{An anthropomorphic cat samurai wearing armor, bokeh temple background, colorful, masterpieces, best quality, aesthetic} \\
DreamShaper PixelArt & \textit{A countryside cottage on the edge of a cliff overlooking an ocean, pixel art} \\
DreamShaper & \textit{Photo of an astronaut riding a horse} \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
% \vskip -0.1in
\end{table}


\begin{table}[t]
\caption{Summary of used text-to-image models.}
\label{tab:model_setting}
% \vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{lccc}
\toprule
Model & SD15 variants & SDXL & PixArt-$\alpha$ \\
\midrule
Model architecture          & U-Net & U-Net & DiT   \\
\# of model parameters      & 0.86B & 2.58B & 0.61B \\
Text encoder                & CLIP ViT-L & CLIP ViT-L \& OpenCLIP ViT-bigG & Flan-T5-XXL \\
\# of tokens                & 77    & 77    & 120   \\
Context dimension           & 768   & 2048  & 4096  \\
\# of encoder parameters    & 0.12B & 0.82B & 4.76B \\
\# of enhancer parameters   & 1.84M & 3.15M & 5.25M \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
% \vskip -0.1in
\end{table}

 
\subsection{Visualizing the Enhanced Text Embeddings}
\label{sec:app_visulization}

We visualize 1,000 text embeddings with dimension of 59,136 (77 $\times$ 768) through standard principle component analysis (PCA). The results are shown in \cref{fig:3d}, where the blue and red scatters denotes the original and enhanced text embeddings respectively. Remarkably, the enhanced text embeddings are distributed on a low-dimensional manifold with 34\% of variance explained by the top 3 PCs, while for the original embeddings it is 8\%. For enhanced text embedding, 60 principle components (PCs) are required to reach a PCA ratio of 80\% and for the original text embedding it is around 300. Therefore, our enhancer locates a low-dimensional manifold in the vast ambient space which consists of around 60K dimensions.

\begin{figure}[t]
\begin{center}
    \includegraphics[width=0.8\textwidth]{figs/3d.pdf}
    \captionsetup{skip=2pt}
    \caption{Left: visualization of 1,000 text embeddings through standard PCA. Blue scatters are the original text embeddings. The enhanced embeddings (red scatters) are distributed on a low-dimensional manifold.
    Purple scatters are the projection of red scatters on 2-D planes. 
    Right: explained variance v.s. the number of principle components. 
    }
    \label{fig:3d}
\end{center}
\vskip -0.1in
\end{figure}


\subsection{Comparison with Distillation-based Methods}
\label{sec:comparison_w_distillation}
The most related works to ours are Guidance Distillation~\cite{meng2023distillation} and Plug-and-Play~\cite{hsiao2024plug}, which also aim at reducing the sampling overhead of CFG through distillation. In \cref{fig:comparison_w_distillation}, we provide an illustrative comparison between them and our method. 
Guidance Distillation takes guidance scale as an additional model entry and processes it in the way similar to the timestamp. The whole parameters of the text-to-image model are fine-tuned under CFG-based supervision. Plug-and-Play trains a guide model in a similar way, where the guide model interacts with the intermediate features of the text-to-image model as in ControlNet~\cite{zhang2023adding}. Our method, instead, completely decouples the enhancer from the text-to-image model by only modifying the text embedding, which is essentially the model condition. Despite a small number of trainable parameters, our method achieves comparable performance with Guidance Distillation as verified in \cref{sec:t2i}. This decoupling further enhances the interpretability of our method from both theoretical and empirical perspectives.

\begin{figure*}[t]
\begin{center}
    \begin{subfigure}[t]{0.33\textwidth}
        \includegraphics[width=\textwidth]{figs/comparison_1.pdf}
        \caption{Guidance Distillation~\cite{meng2023distillation}.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.33\textwidth}
        \includegraphics[width=\textwidth]{figs/comparison_2.pdf}
        \caption{Plug-and-Play~\cite{hsiao2024plug}.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.33\textwidth}
        \includegraphics[width=\textwidth]{figs/comparison_3.pdf}
        \caption{Our method.}
    \end{subfigure}
    \caption{Method comparison. (a) Guidance Distillation takes guidance scale as an additional entry and fine-tune the whole text-to-image model. The way to process the guidance scale is similar to that of the timestamp. (b) Plug-and-Play takes inspiration from ControlNet~\cite{zhang2023adding} and trains an external guide model to distill the guidance scale. (c) Our method trains an enhancer to strengthen the text embedding. We completely decouple the enhancer from the text-to-image model, which is extremely easy to implement and exhibit better interpretability.}
    \label{fig:comparison_w_distillation}
\end{center}
% \vskip -0.1in
\end{figure*}

\subsection{Comparison with Reward-based Methods}
\label{sec:comparison_w_reward}
Aiming at further enhancing the image quality given by guided sampling, previous works have proposed to fine-tune the text encoder through reinforcement learning~\cite{chen2025enhancing} and reward propagation~\cite{li2024textcraftor}. In \cref{fig:app_comp}, we provide a qualitative comparison between our method and these reward-based methods, i.e., TexForce~\cite{chen2025enhancing} and TextCraftor~\cite{li2024textcraftor}. Though these reward-based methods improve the sample quality of guided sampling, their obtained text embeddings are not applicable to unguided sampling. We re-train TexForce for unguided sampling but only observe minor improvement. Therefore, the mechanism of our method, as illustrated in \cref{sec:discussions}, is different from that of reward-based methods, which we hypothesis is due to the direct CFG-based supervision instead of reward models.

\subsection{Additional Ablation Studies}
\label{sec:app_ablation}

\textbf{Enhancer strength.} 
We take an ablation study on enhancer strength $\alpha$ which is fixed to 1 during training. The quantitative and qualitative results are shown in \cref{fig:ablation_alpha} and \cref{fig:ablation_alpha_qualitative}. As $\alpha$ increases in a certain range, the sample quality improves with richer detail and stronger contrast. 
The enhancer manages to find robust directions that are capable of strengthening the overall image quality while maintaining the semantic information.

\textbf{Training iterations.}
In \cref{fig:ablation_iter}, we show the performance of our method with respect to FID, CS and AS evolving with training iterations. Our method enjoys a fast convergence.

\textbf{Combined with guided sampling.} 
Our method is able to be combined with guided sampling as shown in \cref{fig:ablate_cfg_alpha}. Combined with guided sampling, our method rapidly improves the image contrast as guidance scale increases.

\textbf{NFE budgets.} 
In \cref{fig:ablate_nfe_qualitative} and \cref{fig:ablate_nfe_qualitative_variants}, we provide qualitative results as a supplement to \cref{fig:ablate_nfe} on all the text-to-image models involved in this paper, demonstrating the advantage of our method under low NFE budgets. 

\textbf{Sampling trajectory.} 
\cref{fig:traj} exhibits full sampling trajectories with the corresponding cross-attention maps. The cross-attention maps of \ourName differ from the original sampling in terms of the constantly changing background, meaning that our enhanced text embeddings put more attention on the overall image details instead of only the main concepts.



\begin{figure}[t]
\begin{center}
    \begin{subfigure}[t]{\textwidth}
        \includegraphics[width=\textwidth]{figs/app_comparison_sd15.pdf}
        \caption{Model: Stable Diffusion v1.5~\cite{rombach2022ldm}. Text prompt: ``\textit{an elephant on the grassland, cloud, high quality, realistic}''.}
    \end{subfigure}
    \vskip 0.15in
    \begin{subfigure}[t]{\textwidth}
        \includegraphics[width=\textwidth]{figs/app_comparison_dream.pdf}
        \caption{Model: DreamShaper. Text prompt: ``\textit{a supercar on the road, sunset, high quality}''.}
    \end{subfigure}
\caption{Comparison with reward-based methods, i.e., TexForce~\cite{chen2025enhancing} and TextCraftor~\cite{li2024textcraftor}. The text embeddings obtained by reward-based methods are not applicable to unguided sampling.}
\label{fig:app_comp}
\end{center}
% \vskip -0.2in
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[width=\textwidth]{figs/ablate_alpha.pdf}
\caption{Quantitative results on enhancer strength $\alpha$.}
\label{fig:ablation_alpha}
\end{center}
% \vskip -0.2in
\end{figure}


\begin{figure}[t]
\begin{center}
    \includegraphics[width=\textwidth]{figs/ablation_alpha.pdf}
    \vskip -0.1in
    \caption{Qualitative results on enhancer strength $\alpha$. The enhanced text embeddings exhibit strong semantic consistency. 
    Top: DreamShaper with text prompt ``\textit{A Corgi wearing sunglasses on the beach}''.
    Bottom: 3D Animation Diffusion with text prompt
    ``\textit{A house on an island surrounded by sea, 3D style, best detail, high quality, aesthetic}''.}
    \label{fig:ablation_alpha_qualitative}
\end{center}
% \vskip -0.2in
\end{figure}


\begin{figure}[t]
\begin{center}
\includegraphics[width=\textwidth]{figs/ablate_iter.pdf}
\caption{Quantitative results on the training iterations of our method.}
\label{fig:ablation_iter}
\end{center}
% \vskip -0.2in
\end{figure}

\begin{figure}[t]
\begin{center}
    \begin{subfigure}[t]{0.9\textwidth}
        \includegraphics[width=0.9\textwidth]{figs/ablate_cfg_alpha_girl.pdf}
        \caption{Model: DreamShaper. Text prompt: ``\textit{photo portrait of a girl}''.}
    \end{subfigure}
    \vskip 0.15in
    \begin{subfigure}[t]{0.9\textwidth}
        \includegraphics[width=0.9\textwidth]{figs/ablate_cfg_alpha_teddy.pdf}
        \caption{Model: DreamShaper. Text prompt: ``\textit{a teddy bear on the grass with balloons}''.}
    \end{subfigure}
\caption{Additional results on enhancer strength $\alpha$ and guidance scale $\omega$. Our enhanced text embeddings can also be combined with guided sampling (i.e., $\omega > 1$) to improve the overall image quality.}
\label{fig:ablate_cfg_alpha}
\end{center}
% \vskip -0.2in
\end{figure}


\begin{figure}[t]
\begin{center}
    \begin{subfigure}[t]{0.9\textwidth}
        \includegraphics[width=0.9\textwidth]{figs/ablate_nfe_qualitative_sd15.pdf}
        \caption{Model: Stable Diffusion v1.5~\cite{rombach2022ldm}. Text prompt: ``\textit{close-up photo of a cute smiling shiba inu}''.}
    \end{subfigure}
    \vskip 0.15in
    \begin{subfigure}[t]{0.9\textwidth}
        \includegraphics[width=0.9\textwidth]{figs/ablate_nfe_qualitative_dream.pdf}
        \caption{Model: DreamShaper. Text prompt: ``\textit{flowers on the table}''.}
    \end{subfigure}
    \vskip 0.15in
    \begin{subfigure}[t]{0.9\textwidth}
        \includegraphics[width=0.9\textwidth]{figs/ablate_nfe_qualitative_sdxl.pdf}
        \caption{Model: Stable Diffusion XL~\cite{podell2024sdxl}. Text prompt: ``\textit{a robot in the city, perfect detail, 8k, best quality}''.}
    \end{subfigure}
    \vskip 0.15in
    \begin{subfigure}[t]{0.9\textwidth}
        \includegraphics[width=0.9\textwidth]{figs/ablate_nfe_qualitative_pixart.pdf}
        \caption{Model: Pixart-$\alpha$~\cite{chen2024pixart}. Text prompt: ``\textit{close-up photo of an eagle on the cliff}''.}
    \end{subfigure}
\caption{Qualitative results under different NFEs. Guidance scale of 5 is used for all guided sampling.}
\label{fig:ablate_nfe_qualitative}
\end{center}
% \vskip -0.2in
\end{figure}


\begin{figure}[t]
\begin{center}
    \begin{subfigure}[t]{0.9\textwidth}
        \includegraphics[width=0.9\textwidth]{figs/ablate_nfe_qualitative_ab.pdf}
        \caption{Model: AbsoluteReality. Text prompt: ``\textit{photo portrait of a man}''.}
    \end{subfigure}
    \vskip 0.15in
    \begin{subfigure}[t]{0.9\textwidth}
        \includegraphics[width=0.9\textwidth]{figs/ablate_nfe_qualitative_pastel.pdf}
        \caption{Model: Anime Pastel Dream. Text prompt: ``\textit{an anime character on the cloud, sunset, close-up}''.}
    \end{subfigure}
    \vskip 0.15in
    \begin{subfigure}[t]{0.9\textwidth}
        \includegraphics[width=0.9\textwidth]{figs/ablate_nfe_qualitative_pixel.pdf}
        \caption{Model: DreamShaper PixelArt. Text prompt: ``\textit{a clean bedroom, pixel art}''.}
    \end{subfigure}
    \vskip 0.15in
    \begin{subfigure}[t]{0.9\textwidth}
        \includegraphics[width=0.9\textwidth]{figs/ablate_nfe_qualitative_3d.pdf}
        \caption{Model: 3D Animation Diffusion. Text prompt: ``\textit{a cute princess, cartoon, best quality}''.}
    \end{subfigure}
\caption{Qualitative results under different NFEs for other Stable Diffusion variants.}
\label{fig:ablate_nfe_qualitative_variants}
\end{center}
% \vskip -0.2in
\end{figure}


\begin{figure}[t]
\begin{center}
    \begin{subfigure}[t]{\textwidth}
        \includegraphics[width=\textwidth]{figs/traj_corgi.pdf}
        \caption{Model: DreamShaper~\cite{DreamShaper}. Text prompt: ``\textit{a Corgi wearing sunglasses on the beach}''.}
    \end{subfigure}
    \vskip 0.15in
    \begin{subfigure}[t]{\textwidth}
        \includegraphics[width=\textwidth]{figs/traj_car.pdf}
        \caption{Model: DreamShaper~\cite{DreamShaper}. Text prompt: ``\textit{a red car by the street}''.}
    \end{subfigure}
\caption{Qualitative results on sampling trajectory and cross-attention maps from $t=T$ to $t=0$. Images are generated by 10-step DPM-Solver++~\cite{lu2022dpmpp} with the same random seed in each subfigure.}
\label{fig:traj}
\end{center}
% \vskip -0.2in
\end{figure}