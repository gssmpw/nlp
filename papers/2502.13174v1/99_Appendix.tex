\newpage



\section{Implementation and experimental details}
\label{app:exp_details}

\subsection{Smoothing and contrast filtering}
\label{subsec:density_filters}
Filtering is a general concept in topology optimization, which aims to reduce artifacts and improve convergence.
\emph{Helmholtz PDE filtering} is a smoothing filter similar to Gaussian blurring, but easier to integrate with existing finite element solvers. %
By solving a Helmholtz PDE the material density $\rho$ is smoothened to prevents checkerboard patterns, which is typical for TO \citep{lazarov2011filters}.
\emph{Heaviside filtering} is a type of contrast filter, which enhances the distinction between solid and void regions.
The Heaviside filter function equals the sigmoid function up to scaling of the input. We therefore denote $\rho_h$ as the Heaviside-filtered $\rho$ as defined in Equation~\ref{eq:heaviside} Whereas $\beta$ is a parameter controlling the sharpness of the transform, similar to the inverse temperature of a classical softmax (higher beta means a closer approximation to a true Heaviside step function).
Note that in contrast to the Helmholtz PDE filter, the heaviside filter is not volume preserving. Therefore the volume constraint has to be applied to the modified output.





\subsection{Model hyperparameters}

We report additional information on the experiments and their implementation.
We run all experiments on a single GPU (NVIDIA Titan V12), but potentially across multiple CPU cores (up to 32).
For single-shape training, the maximum GPU memory requirements are less than 2GB for all experiments.

For the model to effectively learn high-frequency features, it is important to use a neural network represenation with a high frequency bias \citep{sitzmann2019siren,Teney2024NeuralRR}.
Hence, all models were trained using the real, 1D variant of the WIRE architecture \citep{saragadam2023wire}.
WIRE allows to adjust the frequency bias by setting the hyperparameters $\omega_0^1$ and $s_0$.
For this architecture, each layer consists of 2 Multi-layer perceptron (MLPs), one has a periodic activation function $cos(\omega_0 x)$, the other with a gaussian $e^{(s_0 x)^2}$. 
The post-activations are then multiplied element-wise.


\begin{table}[!ht]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lcccccc}
        & \textbf{MBB Beam} & \textbf{MBB Beam } & \textbf{Cantilever} & \textbf{Cantilever} & \textbf{JEB} & \textbf{JEB} \\ 
        Sampling & equidistant & uniform & equidistant & uniform & equidistant & uniform \\ 
        \hline
        Hidden layers & 32x3 & 32x3 & 32x3 & 32x3 & 64x3 & 64x3 \\ 
        $\omega_0^1$ for WIRE & 18 & 18 & 18 & 18 & 18 & 18 \\ 
        $s_0$ for WIRE & 20 & 20 & 10 & 10 & 6 & 6 \\ 
        Learning rate & $10^{-4}$ & 0.0001 & 0.0001 & 0.0001 & 0.001 & 0.001 \\ 
        \hline
        $\mathcal{Z}$ & $[0,1]^2$ & $[0,1]^2$ & $[0,1]^2$ & $[0,1]^2$ & $[0,0.25]^2$ & $[0,0.25]^2$ \\ 
        SIMP penalty $p$ & 3 & 3 & 3 & 3 & 1.5 & 1.5 \\ 
        $\beta$ annealing & $[0, 400]$ & $[0, 400]$ & $[0, 400]$ & $[0, 400]$ & $[0, 400]$ & $[0, 400]$ \\ 
        $\hat{\delta}(\Omega_\mathcal{Z})$ & 0.2 & 0.06 & 0.3 & 0.25 & 0.15 & 0.15 \\ 
        \# iterations & 400 & 650 & 400 & 1000 & 1000 & 1000 \\ 
        \# shapes per batch & 9 & 25 & 9 & 25 & 9 & 9 \\ 
    \end{tabular}
    }
\end{table}


\subsection{Reference Solutions} \label{subsec:ref_solutions}
We provide reference solutions to the problem settings generated by standard topology optimization (TO). We use the implementation of the SIMP method provided by the FeniTop library \cite{fenitop} as classical TO baseline. \newline
We also generate single solutions using GenTO without a diversity constraint or modulation variable as input. These single shape training runs showcase the baseline capability of GenTO. \newline
The reference solutions for MBB beam problem are shown in Figure \ref{fig:mbb_reference}, for the cantilever beam problem in Figure \ref{fig:cantilever_reference} and for the jeb engine bracket problem in Figure \ref{fig:simjeb_reference}.

\begin{figure*}[ht!]
\centering
\begin{subfigure}[t]{0.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/results/fenitop_mbb.pdf}
    \caption{Single solution generated with FeniTop}%
    \label{fig:fenitop_mbb}
\end{subfigure}
\begin{subfigure}[t]{0.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/results/genTO_single_mbb.pdf}
    \caption{Single solution generated with GenTO}%
    \label{fig:genTO_single_mbb}
\end{subfigure}

\caption{ Reference solutions for the MBB beam problem
}
\label{fig:mbb_reference}
\end{figure*}

\begin{figure*}[ht!]
\centering
\begin{subfigure}[t]{0.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/results/fenitop_cantilever.pdf}
    \caption{Single solution generated with FeniTop}%
    \label{fig:fenitop_cantilever}
\end{subfigure}
\begin{subfigure}[t]{0.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/results/genTO_single_cantilever.pdf}
    \caption{Single solution generated with GenTO}%
    \label{fig:genTO_single_cantilever}
\end{subfigure}

\caption{Reference solutions for the Cantilever problem}
\label{fig:cantilever_reference}
\end{figure*}

\begin{figure*}[ht!]
\centering
\begin{subfigure}[t]{0.4\textwidth}
    \includegraphics[width=\textwidth]{figures/results/simjeb_fenitop.pdf}
    \caption{Single solution generated with FeniTop}
    \label{fig:simjeb_fenitop}   
\end{subfigure}
\begin{subfigure}[t]{0.4\textwidth}
    \includegraphics[width=\textwidth]{figures/results/GenTO_single.png}
    \caption{Single solution generated with GenTO}
    \label{fig:genTO_single_simjeb}   
\end{subfigure}
\caption{Reference solutions for the jet engine bracket (JEB) problem}
\label{fig:simjeb_reference}
\end{figure*}

\FloatBarrier

\subsection{Additional Results}
We show additional qualitative results for our experiments.

\paragraph{MBB beam}
Figure \ref{fig:beam_uniform} shows the MBB beam, which was uniformly sampled during training, evaluated at equidistant points $\mathbf{z} \in \mathcal{Z}$.

\begin{figure}
    \centering
    \includegraphics[width=0.6\columnwidth]{figures/results/beam_uniform.png}
    \caption{MBB beam uniformly sampled during training from $\mathcal{Z} = [0,1]^2$. The depicted shapes for this figure are sampled equidistantly from $\mathcal{Z}$.}
    \label{fig:beam_uniform}
\end{figure}

\paragraph{Cantilever}
Figure \ref{fig:cantilever_uniform} shows the Cantilever, which was  uniformly sampled during training, evaluated at equidistant points $\mathbf{z} \in \mathcal{Z}$.


\begin{figure}
    \centering
    \includegraphics[width=0.6\columnwidth]{figures/results/cantilever_uniform.png}
    \caption{
    Cantilever uniformly sampled during training from $\mathcal{Z} = [0,1]^2$. The depicted shapes for this figure are sampled equidistantly from $\mathcal{Z}$.
    }
    \label{fig:cantilever_uniform}
\end{figure}


\paragraph{Jet engine bracket}
Figure \ref{fig:jeb_equi_top} and Figure \ref{fig:jeb_equi_diag} show the results of the Jet engine bracket (JEB), which was equidistantly taken from $\mathbf{z} \in \mathcal{Z} = [0,0.25]$ at the start of training.
Figure \ref{fig:jeb_uniform_top} and Figure \ref{fig:jeb_uniform_diag} show the results of a Jet engine bracket (JEB), which was uniformly sampled at each iteration $\mathbf{z} \in \mathcal{Z} = [0,0.25]$.

\begin{figure}
    \centering
    \includegraphics[width=0.6\columnwidth]{figures/results/jeb_equidistant.png}
    \caption{JEB (equidistant) top view:
    The depicted shapes for this figure are sampled equidistantly from $\mathcal{Z} = [0,0.25]^2$.}
    \label{fig:jeb_equi_top}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.6\columnwidth]{figures/results/jeb_equidistant_diagonal.png}
    \caption{JEB (equidistant) diagonal view: The depicted shapes for this figure are sampled equidistantly from $\mathcal{Z} = [0,0.25]^2$.}
    \label{fig:jeb_equi_diag}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.6\columnwidth]{figures/results/jeb_uniform_topview.png}
    \caption{JEB (uniform) top view:
    During training, modulation vectors $\mathbf{z}$ for this figure are sampled uniformly from $\mathcal{Z} = [0,0.25]^2$.}
    \label{fig:jeb_uniform_top}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.6\columnwidth]{figures/results/jeb_uniform_diagonal.png}
    \caption{JEB (uniform) diagonal view:
    During training, modulation vectors $\mathbf{z}$ for this figure are sampled uniformly from $\mathcal{Z} = [0,0.25]^2$.}
    \label{fig:jeb_uniform_diag}
\end{figure}

\subsection{Ablations}
\label{app:ablations}
We show additional figures for the ablations in Section \ref{ablations}.


\paragraph{Penalty $p=3$}
Tuning the penalty is important for GenTO to work.
We demonstrate this in Figure \ref{fig:jeb_p3}.
The shape converges to a unfavorable local minimum as the loss landscape is too sharp.

\paragraph{No annealing.}
For Figure \ref{fig:jeb_no_annealing}, the annealing was turned off.
The optimization fails to converge to a useful compliance value and does not fulfill the desired interface constraints.



\begin{figure*}[ht!]
\centering
\begin{subfigure}[t]{0.4\textwidth}
    \centering
    \includegraphics[width=\columnwidth]{figures/ablations/penalty3.png}
    \caption{}%
    \label{fig:jeb_p3}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.4\textwidth}
    \centering
    \includegraphics[width=\columnwidth]{figures/ablations/no_annealing.png}
    \caption{}%
    \label{fig:jeb_no_annealing}
\end{subfigure}

\caption{
Ablations for penalty and $\beta$ annealing.
(a) Jet engine bracket trained with penalty $p=3$.
(b) Jet engine bracket trained without annealing.
}
\label{fig:ablations_annealing_penalty}
\end{figure*}

\paragraph{Modulation space $\mathcal{Z}$}
Figure \ref{fig:failure_modulation} depicts the results of an MBB beam where the modulation space $\mathcal{Z}$ has a too low volume.

The resulting shapes are not as diverse despite applying the diversity constraint during training.


\begin{figure}
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/ablations/mbb_beam_failure_9.png}
    \caption{MBB beam results for an equidistantly sampled modulation space of $\mathcal{Z} = [0,0.1]^2$. The diversity metric is $0.0214$.}
    \label{fig:failure_modulation}
\end{figure}

\section{Geometric constraints}
\label{app:constraints}
We formulate geometric constraints analogously to GINNs in Table \ref{tab:constraints} for density representations.
There are two important differences when changing the shape representation from a signed distance function (SDF) to a density function.\newline
First, the level set $\partial \Omega_{\tau}$ changes from 
$ \partial \Omega_{\tau} = \left\{x \in \mathbb{R}^{d_x} | f_{\theta} = 0 \right\}$ to $ \partial \Omega_{\tau} = \left\{x \in \mathbb{R}^{d_x} | f_{\theta} = 0.5 \right\}$.
Second, for a SDF the shape $\Omega_{tau}$ is defined as the sub-level set $\Omega_{\tau} = \left\{x \in \mathbb{R}^{d_x} | f_{\theta} \leq 0 \right\}$ , whereas for a density it is the super-level set $\Omega_{\tau} = \left\{x \in \mathbb{R}^{d_x} | f_{\theta} \geq 0 \right\}$. 


\def\normal{ \frac{\nabla f(x)}{||\nabla f(x)||}}
\renewcommand{\arraystretch}{1.5} %
\begin{table*}
    \small
    \caption{
        Geometric constraints are derived from GINNs.
        The shape $\Omega$ and its boundary $\partial\Omega$ are implicitly defined by the level set $\tau$ of the function $f$.
        The shape must reside within the \emph{design region} $\mathcal{E}\subseteq\mathcal{X}$ and adhere to the \emph{interface} $\mathcal{I}\subset\mathcal{E}$ with a specified \emph{normal} $\bar{n}(x)$.
        }
    \centering
    \begin{tabular}{l|l|l|l}
        & Set constraint $c_i(\Omega)$ & Function constraint & Constraint violation $c_i(f)$ \\
        \hline
        Design region 
            & $\Omega \subset \mathcal{E}$ 
            & $f(x) < \tau \ \forall x \notin \mathcal{E} $
            & $\int_{\mathcal{X}\setminus\mathcal{E}} \left[ \operatorname{max}(0, f(x) - \tau) \right]^2 dx$ \\
        Interface 
            & $\partial\Omega \supset \mathcal{I}$
            & $f(x) = \tau \ \forall x \in \mathcal{I}$
            & $\int_\mathcal{I} \left[ f(x) - \tau \right]^2 dx$ \\
        Prescribed normal
            & $n(x)=\bar{n}(x) \ \forall x \in \mathcal{I}$ 
            & $\normal = \bar{n}(x) \ \forall x \in \mathcal{I}$
            & $\int_\mathcal{I} \left[ \normal - \bar{n}(x)\right]^2 dx$ \\
    \end{tabular}
    
    \label{tab:constraints}
\end{table*}

\section{Diversity}

\subsection{Diversity on the volume}
\label{subsec:volume_dissimilarity}

As noted by \citet{GINNs}, one can define a dissimilarity loss as the $L^p$ function distance
\begin{equation}
d(\Omega_i, \Omega_j) = \sqrt[p]{\int_\mathcal{X} (f_i(x) - f_j(x))^p dx}
\end{equation}
We choose $L^1$, to not overemphasize large differences in function values.
Additionally, we show in the next paragraph that for $L^1$ and for the extreme case where $f(x) \in \{0, 1\}$ this is equal to the Union minus Intersection of the shapes: 

\begin{equation}
    \int_\mathcal{X} |f_i(x) - f_j(x)| dx = \text{Vol}(\Omega_i \cup \Omega_j) - \text{Vol}(\Omega_i \cap \Omega_j)
\end{equation}


\subsection{$L^1$ distance on neural fields resembles Union minus Intersection}
To derive that the distance metric 

\begin{equation}
     d(\Omega_i, \Omega_j) = \sqrt[p]{\int_\mathcal{X} (f_i(x) - f_j(x))^p dx}
\end{equation}

for \( p=1 \) and \( f_i, f_j \in \{0,1\} \) corresponds to the union minus the intersection of the shapes, we consider the following cases:

\begin{table}[h!]
    \centering
    \begin{tabular}{c|c|c}
    \( f_i(x) \) & \( f_j(x) \) & \( |f_i(x) - f_j(x)| \) \\
    \hline
    1 & 1 & 0 \\
    1 & 0 & 1 \\
    0 & 1 & 1 \\
    0 & 0 & 0 \\
    \end{tabular}
    \caption{Function distance if the function only has binary values.}
    \label{tab:binary}
\end{table}


From the table, we observe that the integrand \( |f_i(x) - f_j(x)| \) is 1 when \( x \) belongs to one shape but not the other, and 0 when \( x \) belongs to both or neither.
Thus, the integral \( \int_\mathcal{X} |f_i(x) - f_j(x)| dx \) sums the volumes where \( x \) is in one shape but not the other, which is precisely the volume of the union of \( \Omega_i \) and \( \Omega_j \) minus the volume of their intersection.
It follows that 

\begin{equation}
    d(\Omega_i, \Omega_j) = \int_\mathcal{X} |f_i(x) - f_j(x)| dx = \text{Vol}(\Omega_i \cup \Omega_j) - \text{Vol}(\Omega_i \cap \Omega_j) 
\end{equation}.


\subsection{Diversity on the boundary via differentiable chamfer discrepancy}
\label{subsec:chamfer_diversity}

We continue from Equation \ref{eq:level_set}:
\begin{align}
    \frac{\partial L}{\partial \theta} &= \frac{\partial L}{\partial x} \frac{\partial x}{\partial y} \frac{\partial y}{\partial \theta} \nonumber \\ 
    &= \frac{\partial L}{\partial x} \frac{\nabla_x f_\theta}{|\nabla_x f_\theta|^2} \frac{\partial y}{\partial \theta}
\end{align}
The center term $\frac{\nabla_x f_\theta}{|\nabla_x f_\theta|^2}$ and the last term  $\frac{\partial y}{\partial \theta}$ can be obtained via automatic differentiation.
For the first term, we derive $\frac{\partial L}{\partial x}$, where $L$ is the one-sided chamfer discrepancy $\text{CD}(\partial \Omega_1, \partial \Omega_2)$.
\begin{align}
\frac{\partial}{\partial x}  \text{CD}(\partial \Omega_1, \partial \Omega_2)
&= \frac{\partial }{\partial x} \frac{1}{\left| \partial \Omega_1 \right|} \sum_{x \in \partial \Omega_1} \min_{y \in \partial \Omega_2} ||x-y||_2
\\
&= \frac{1}{\left| \partial \Omega_1 \right|} \min_{y \in \partial \Omega_2} \frac{x - y}{\|x - y\|}
\end{align}
This completes the terms in the chain rule.

\subsection{Finding surface points}
We describe the algorithm to locate boundary points of implicit shapes defined by a neural density field Algorithm \ref{alg:boundary_points}. \\
On a high level, the algorithm first identifies points inside the boundary where neighboring points lie on opposite sides of the level set.
Subsequently, it employs binary search to refine these boundary points.
The process involves evaluating the neural network to determine the signed distance or density values, which are then used to iteratively narrow down the boundary points.
We find that 10 binary steps suffice to reach the boundary sufficiently close.

\begin{algorithm}[h]
\caption{Find Boundary Points with Binary Search}
\label{alg:boundary_points}
\begin{algorithmic}
\STATE {\bfseries Input:}
regular point grid $x_i$, level set $l$, neural density field $f_\theta$, binary search steps $T$
\STATE Find points $x_i^\text{in} \in x_i$ for which $f(x_i^\text{in}) > l$
\STATE Find neighbor points $(x_i^\text{out})_i \in x_i$ to $x_i^\text{in}$ $f(x_\text{out}) < l$ \COMMENT{ for 2D/3D we check 4/6 neighbors respectively}
\STATE Refine pairs $(x_\text{in}, x_\text{out})_i$ via $T$ binary search steps
\STATE Return $((x_\text{in} + x_\text{out})/2)_i$
\end{algorithmic}
\end{algorithm}
