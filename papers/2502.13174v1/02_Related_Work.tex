
\section{Background} 
\label{sec:preliminaries}




\subsection{Topology optimization}
\label{subsec:TO}
Topology optimization (TO) is a computational method developed in the late 1980s to determine optimal structural geometries from mathematical formulations \citep{BENDSOE1988197}. It is widely used in engineering to design efficient structures while minimizing material usage.
TO iteratively updates a material distribution within a design domain under specified loading and boundary conditions to enhance properties like stiffness and strength. 
Due to the non-convex nature of most TO problems, convergence to a global minimum is not guaranteed. Instead, the goal is to achieve a near-optimal solution, where the objective closely approximates the global optimum.
There are four prominent method families widely recognized in TO.
In this work, we focus on \emph{SIMP} and refer the reader to \citet{yago_topology_2022} for a more detailed introduction to TO.



\paragraph{Solid Isotropic Material with Penalization (SIMP)} is a prominent TO method we adopt for GenTO.
SIMP starts by defining a mesh with mesh points $\mathbf{x}_i \in \mathcal{X}, i \in \{1, ..., N \}$ in the design region.
The aim is to find a binary density function at each mesh point $\rho(\mathbf{x}_i) \in \{0, 1\}$, where $\rho(\mathbf{x}_i) = 0$ represents void and $\rho(\mathbf{x}_i) = 1$ represents solid material. 
To make this formulation differentiable, the material density $\rho$ is relaxed to continuous values in $[0, 1]$. 
\
A common objective of SIMP is to minimize the compliance $C$, which is a measure of the structural flexibility of a shape. 
Intuitively, the lower the compliance, the stiffer the shape.
The SIMP objective is then formulated as a constrained optimization problem:
\begin{equation}
\begin{aligned}
\label{eq:simp}
\min : & \quad C(\rho) = \mathbf{u}^T \mathbf{K_\rho} \mathbf{u} \\
\text{s.t.} : & \quad V = \sum_{i=1}^N \mathbf{\rho}_i v_i \leq V^* \\
& \quad 0 \leq \mathbf{\rho}_i \leq 1 \quad \forall i \in N 
\end{aligned}
\end{equation}
where $\mathbf{u}$ is the displacement vector, $\mathbf{K}_\rho$ is the global stiffness matrix, $V$ is the shape volume, and $V^*$ is the target volume.
\
The density field is optimized iteratively.
In each iteration, a finite element (FEM) solver computes the compliance and provides gradients to update the density field $\rho$.
To encourage binary densities, intermediate values are penalized by raising $\rho$ to the power $p > 1$. 
Hence, the stiffness matrix is

\begin{equation}
\label{eq:penalization}
    \mathbf{K}_\rho = \sum_{i=1}^{N} \rho_i^p \mathbf{K}_i
\end{equation}

where $\mathbf{K}_i$ describes the stiffness of solid cells and depends on material properties.

\paragraph{Annealing} is employed to make the continuous relaxation closer to the underlying discrete problem \citep{Kirkpatrick1983Annealing}, enhancing the effectiveness of gradient-based optimization methods.
Annealing gradually adjusts the sharpness of a function during the optimization. 
For TO, this is often done by gradually increasing the penalty $p$ or by scheduling a sharpness filter.
A common choice is the Heaviside filter:
\begin{equation}
\label{eq:heaviside}
H(x, \beta) = 0.5 + \frac{\tanh(\beta(x - 0.5))}{2 \tanh(0.5 \beta)} \
\end{equation}
where $\beta$ is a parameter controlling the sharpness of the transform.



\paragraph{Topology optimization with neural reparametrization.}
Neural reparametrization in TO provides a flexible, mesh-free way to represent shapes, offering an alternative to traditional methods.
A neural network is used to learn the underlying distribution of material within a design space.
Previous works have explored TO with neural networks parametrizing the density field, primarily focusing on solver-in-the-loop training \citep{gillhofer2019gan, Bujny_TO_reparametrization, Chandrasekhar21tounn, Hoyer2019NeuralRI}, as early attempts to learn purely from data were deemed ineffective \citep{sosnovik_neural_2017}.
With GenTO we go a similar path, but learning multiple shapes with a single neural network.
\\
NTopo \citep{Zehnder21ntopo} is a conceptually related approach, which employs a conditional neural field to parametrize individual solutions across multiple TO problems. 
Specifically, NTopo conditions the neural field on factors such as target volume or force vector position, producing a unique solution for each configuration.
In contrast, GenTO extends this idea by enabling the discovery of multiple solutions for a single configuration, rather than restricting each configuration to a single solution. 
This ability to generate multiple valid designs broadens the potential applications of TO methodologies.



\paragraph{Topology optimization with multiple solutions} is important to address real-world engineering challenges, in particular when additional design considerations, such as manufacturability, and aesthetics, influence the selection of the design. 
By generating diverse solutions, engineers can evaluate and compare alternative designs, ultimately selecting the most suitable option in a post-processing stage.
However, classical TO algorithms typically yield a single solution and do not ensure convergence to a global minimum \citep{multi_TO_Papadopoulos_2021}. 
\\
To address this weakness, \citet{multi_TO_Papadopoulos_2021} introduced the deflated barrier (DB) method, the first approach capable of computing multiple solutions to a given TO problem. 
The DB method integrates deflation techniques, barrier methods, and primal-dual active set solvers to systematically explore different local minima. 
By employing a search strategy akin to depth-first search, DB identifies multiple solutions without relying on initial guess variations, thereby enhancing design diversity in TO.














\subsection{Shape generation with neural networks}


\paragraph{Neural fields} offer a powerful framework for geometry processing, utilizing neural networks to model shapes implicitly.
Unlike conventional explicit representations like meshes or point clouds, implicit shapes are defined as level sets of continuous functions.
This approach enables high-quality and topologically flexible shape parametrizations \citep{chen2019implicit}. The two prevalent methods for representing implicit shapes are Signed Distance Functions (SDF) \citep{park2019deepsdf, Atzmon2020sal} and density (or occupancy) \citep{mescheder2019occupancy} fields. We opt for the density representation due to its compatibility with SIMP optimization.
\\
A neural density field employs a neural network $f_\theta: \mathbb{R}^{d_x} \to (0, 1)$ with parameters $\theta$ and the shape $\Omega$ is defined as a level-set of $f_{\theta}$. For $\tau \in \mathbb{R}$, $\Omega := \{x \in \mathbb{R}^{d_x} | f_\theta(x) > \tau\}$, the boundary $\partial\Omega := \{x \in \mathbb{R}^{d_x} | f(x) = \tau \}$, where $d_x \in \{2, 3\}$ for 2D or 3D shapes.

\paragraph{Conditional neural fields.}
While a neural density field represents a single shape, a \emph{conditional} neural field represents a set of shapes with a single neural network \citep{Mehta21modulation}.
To this end, a modulation code $\mathbf{z} \in \mathbb{R}^{d_z}$ is provided as additional input to the network.
The resulting network $f_\theta(\mathbf{x}, \mathbf{z})$ parametrizes a set of shapes, which is modulated by $\mathbf{z}$.
There are different ways to incorporate the modulation vector into the network, such as input concatenation \citep{park2019deepsdf}, hypernetworks \citep{Ha17hypernetworks}, or attention \citep{Rebain2022Attention}.

\paragraph{Compression perspective.}
A neural implicit shape can be regarded as a lossy compression of a shape into the weights of the neural network \citep{Xie2022nfs}.
As an illustrative example, consider the memory requirements of a high-fidelity shape in a voxel representation compared to a neural density representation.
While the neural implicit shape might not perfectly approximate the voxel representation, it requires fewer bits of storage.
For \emph{conditional} neural fields, the compression factor is usually higher, as the network does not have to store the mutual information between the shapes for each shape individually.




\paragraph{Diversity constraint.}
To discover multiple solutions during optimization, we add a diversity constraint to the problem formulation \eqref{eq:simp}. The diversity constraint introduced by GINNs \citep{GINNs} defines a diversity measure $\delta$ on the set of shapes $\{ \Omega_i \}$ as follows:
\begin{align}
    \label{eq:div}
    \delta (\{ \Omega_i \}) = \left( \sum_j \left( \min_{k\neq j} d(\Omega_j, \Omega_k) \right)^{1/2} \right)^{2} \ .
\end{align}
This measure builds upon a chosen dissimilarity function $d(\Omega_i, \Omega_j)$.
Essentially, $\delta$ encourages diversity by maximizing the distance between each shape and its nearest neighbor.
GINNs utilize a dissimilarity function defined on the boundaries: $d(\Omega_1, \Omega_2) = \sqrt{\int_{\partial\Omega_i} f_j(x)^2 d{x} + \int_{\partial\Omega_j} f_i(x)^2 d{x}}$.
The authors demonstrated that for SDFs $f_1$ and $f_2$ of the shapes $\Omega_1$ and $\Omega_2$, this corresponds to the chamfer discrepancy. However, this result relies on the SDF assumption that $f_i(x)$ measures the distance of the point $x$ to the boundary $\partial \Omega_i$. It does not hold for more general implicit representations, such as the density fields we use. Therefore, we derive an alternative dissimilarity function tailored for density fields in Section \ref{sec:diversity}.


