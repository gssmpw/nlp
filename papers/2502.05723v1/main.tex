
%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
% \documentclass{article}
\documentclass[twocolumn,10pt]{article}

% Full-page, two-column format settings
\setlength{\columnsep}{0.25in}  % Space between columns
\textwidth 7in       % Adjusted width for full-page two-column format
\textheight 9.5in    % Adjusted height for full-page use
\oddsidemargin -0.25in
\evensidemargin -0.25in
\topmargin -0.75in
\headheight 0in
\headsep 0.2in
\footskip 0.4in


% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}

\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage{makecell}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
%\usepackage{hyperref}

\usepackage[hang,flushmargin]{footmisc} % footnote no indentation

\usepackage[noend]{algorithmic} % Or \usepackage[noend]{algpseudocode}
% Attempt to make hyperref and algorithmic work together better:
% \newcommand{\theHalgorithm}{\arabic{algorithm}}



%% \documentclass[11pt]{article}
% \documentclass{article}

\usepackage{commands}
\def\marrow{\marginpar[\hfill$\longrightarrow$]{$\longleftarrow$}}
\def\edith#1{\textsc{\color{magenta} (Edith says: }\marrow\textsf{\color{magenta} #1})}
\def\mihir#1{\textsc{ \color{red} (Mihir says: }\marrow\textsf{\color{red} #1})}
\def\uri#1{\textsc{ \color{blue} (Uri says: }\marrow\textsf{\color{blue} #1})}
\usepackage{enumitem}

\SetKwFor{OnInput}{\textbf{on input}}{}{}
\SetKwFor{Function}{\textbf{function}}{}{}

\newcommand{\DDD}{\mathcal{D}}
\newcommand{\al}{\alpha}
\newcommand{\eps}{\varepsilon}
\newcommand{\e}{\eps}
\newcommand{\f}{\frac}

\newcommand{\txtBkSketch}{\texttt{BkSketch}}
\newcommand{\txtSCest}{\texttt{StdEst}}
\newcommand{\txtRBCest}{\texttt{RobustEst}}
\newcommand{\txtRCest}{\texttt{TRobustEst}}

% \usepackage{icml2025}
% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% \icmltitlerunning{Breaking the Quadratic Barrier: Robust Cardinality Sketches for Adaptive Queries}

\title{\texorpdfstring{Breaking the Quadratic Barrier:\\ Robust Cardinality Sketches for Adaptive Queries}{Breaking the Quadratic Barrier: Robust Cardinality Sketches for Adaptive Queries}}

% \ignore{
\author{Edith Cohen\\
Google Research and Tel Aviv University\\
Mountain View, CA, USA\\
\texttt{edith@cohenwang.com}
\and
Mihir Singhal\\
UC Berkeley and Google Research\\
Berkeley, CA, USA\\
\texttt{mihir.a.singhal@gmail.com}
\and
Uri Stemmer\\
Tel Aviv University and Google Research\\
Tel Aviv-Yafo, Israel\\
\texttt{u@uri.co.il}
}
% }
\date{}



\begin{document}

\ignore{
\twocolumn[
\icmltitle{\texorpdfstring{Breaking the Quadratic Barrier:\\ Robust Cardinality Sketches for Adaptive Queries}{Breaking the Quadratic Barrier: Robust Cardinality Sketches for Adaptive Queries}}


% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Edith Cohen}{google,tau}
\icmlauthor{Mihir Singhal}{berkeley,google}
\icmlauthor{Uri Stemmer}{tau,google}
\end{icmlauthorlist}

\icmlaffiliation{tau}{School of Computer Science, Tel Aviv University, Israel}
\icmlaffiliation{google}{Google Research}
\icmlaffiliation{berkeley}{School of Computer Science, UC Berkeley, Berkeley, CA, USA}

\icmlcorrespondingauthor{Edith Cohen}{edith@cohenwang.com}
\icmlcorrespondingauthor{Mihir Singhal}{mihir.a.singhal@gmail.com}
\icmlcorrespondingauthor{Uri Stemmer}{u@uri.co.il}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Adaptive Inputs, Cardinality Sketches, Robustness}
\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\printAffiliationsAndNotice{} 

} %ignore ICML

\maketitle 
\begin{abstract}
%Cardinality sketches are compact data structures that significantly reduce storage, communication, and computational overhead while providing accurate approximations of distinct counts. The designs are randomized, with a sketching map sampled from a distribution and reused across multiple queries. This necessary randomness, however, introduces vulnerabilities: for non-adaptive queries (independent of prior responses), cardinality sketches provide statistical guarantees for $t$ queries, where $t$ scales exponentially with the sketch size $k$. However, under adaptive queries, these guarantees degrade to $t = \tilde{O}(k^2)$.
Cardinality sketches are compact data structures that efficiently estimate the number of distinct elements across multiple queries while minimizing storage, communication, and computational costs. However, recent research has shown that these sketches can fail under {\em adaptively chosen queries}, breaking down after approximately $\tilde{O}(k^2)$ queries, where $k$ is the sketch size.

In this work, we overcome this \emph{quadratic barrier} by designing robust estimators with fine-grained guarantees. Specifically, our constructions can handle an {\em exponential number of adaptive queries}, provided that each element participates in at most $\tilde{O}(k^2)$ queries. This effectively shifts the quadratic barrier from the total number of queries to the number of queries {\em sharing the same element}, which can be significantly smaller. Beyond cardinality sketches, our approach expands the toolkit for robust algorithm design.

%In this work, we overcome this \emph{quadratic barrier} by designing robust estimators with fine-grained guarantees. Specifically, when each key participates in at most $r = \tilde{O}(k^2)$ queries, our approach can handle an unlimited number of adaptive queries, effectively shifting the quadratic barrier on $t$ to the potentially much smaller $r$. Furthermore, we show that this result holds even when a fraction of the keys are \emph{heavy}, as is commonly observed in Pareto distributions, and demonstrate the potential for substantial practical gains. Beyond cardinality sketches, our approach broadens the toolkit for robust algorithm design.
% beyond generic methods constrained by $t = \tilde{O}(k^2)$.


% Additionally, our results reveal fundamental differences in robustness across statistics: for $\ell_2$ sketches, existing attacks remain effective with $t = \tilde{O}(k^2)$, even when $r = 1$.

\ignore{
===========
Composable sketch designs for fundamental aggregates, including cardinality (distinct count), randomly sample a sketching map and use it across queries. Randomness is known to be necessary but is vulnerable to adaptive queries. When inputs do not depend on the sketching map, the provided statistical guarantees are for answering correctly a number of queries $t$ that is exponential in the sketch size $k$. The guarantees with adaptive queries, however, are much weaker: Wrapper methods applied to the basic designs guarantee only $t = \tilde{O}(k^2)$ queries [Hassidim et al based on ADA]. Unfortunately, this quadratic bound was shown to be tight for cardinality sketching. In this work, we aim to mitigate this bad news with fine grained guarantees that apply with a large  number of queries $t$ when most keys  participate in a limited $r \ll t$ queries. 
For cardinality sketching, through a novel analysis method, we show that we can process an unlimited number of adaptive queries (with per-query guarantees similar to a non-adaptive setting) as long as $r=\tilde{O}(k^2)$, shifting the quadratic barrier to $r$ instead of $t$. 
Importantly, we extend the ADA toolkit for robust algorithm design to beyond the wrapper methods, which only guarantees $t = \tilde{O}(k^2)$ queries even in this case.
In contrast, for $\ell_2$ sketches, the known attacks apply with $t=\tilde{O}(k^2)$ even with $r=1$. }
\end{abstract}


\section{Introduction}

% \eccomment{$r$: per-key limit.  $t$: total number of queries. $n$: ground set size. $\rho$: randomness}
% \eccomment{To do:  Look into generalization with the binning statistics and $k$-partition}

When dealing with massive datasets, compact summary structures (known as sketches) allow us to drastically reduce storage, communication, and computation while still providing useful approximate answers.

Cardinality sketches are specifically designed to estimate the number of distinct elements in a query set~\cite{FlajoletMartin85,hyperloglog:2007,ECohen6f,ams99,BJKST:random02,KaneNW10,ECohenADS:TKDE2015,Blasiok20}. 
For a ground set $[n]$ of keys, a sketch is defined by a \emph{sketching map} $S$ that maps subsets $V\subset [n]$ to their sketches $S(V)$, and an \emph{estimator} that processes the sketch $S(U)$ and returns an approximation of the cardinality $|U|$.
 %
An important property of sketching maps is 
\emph{composability}: The sketch $S(U\cup V)$ of the union of two sets $U$ and $V$ can be computed directly from the sketches $S(U)$ and $S(V)$. Composability is crucial for most applications, particularly in distributed systems where data is stored and processed across multiple locations.

Cardinality sketches are extensively used in practice. \emph{MinHash sketches} are composable sketches based on hash mappings of keys to priorities, where the sketch of a set is determined by the minimum priorities of its elements \citep{FlajoletMartin85,hyperloglog:2007,ECohen6f,Broder:CPM00,Rosen1997a,ECohen6f,BRODER:sequences97,BJKST:random02}\footnote{For a survey see \cite{MinHash:Enc2008,Cohen:PODS2023}.}. Many practical implementations\footnote{See, e.g., \citep{datasketches,bigquerydocs}.} use MinHash sketches of various types, particularly bottom-$k$ and HyperLogLog sketches \cite{hyperloglog:2007,hyperloglogpractice:EDBT2013}. 

These sketches can answer an exponential number of queries (in the sketch size $k$) with a small relative error.
The design samples a sketching map from a distribution, and to ensure composability, \emph{the same map must be used to sketch all queries}. The guarantees are statistical: for \emph{any} sequence of queries, with \emph{high probability over the sampling of the map}.
The guarantees hold provided that the queries 
% are {\em fixed in advance}
{\em do not depend on the sampled sketching map}, which is known as the {\em non-adaptive setting}.



% Linear sketches~\citet{CormodeDIM03,distinct_deletions_Ganguly:2007,KaneNW10} apply with vectors $\boldsymbol{v}$ and we want to approximate the $\|\boldsymbol{v} \|_0$ (the number of nonzero entries).

 




\subsection{The adaptive setting}
In the adaptive setting, we assume that the sequence of queries may be chosen adaptively based on previous interactions with the sketch. This arises naturally when a feedback loop causes queries to depend on prior outputs. Sketching algorithms that guarantee utility in this setting are said to be {\em robust} to adaptive inputs.

The main challenge in the adaptive setting (compared to the non-adaptive setting) is that the queries become {\em correlated with the internal randomness of the sketch}. This would not be an issue if the sketching algorithm were deterministic, but unfortunately, randomness is necessary. In particular, any sublinear composable cardinality sketch that is statistically guaranteed to be accurate on all inputs must be randomized \citep{KaneNW10}. By this, we mean that the sketching map cannot be predetermined and must instead be sampled from a distribution. 
% When the inputs are adaptive, it may be possible to use fewer queries in order to zoom on inputs that are bad for the sketching map.


% for each particular query set $U$, the probability that it is 
% not estimated well using a sampled sketching map is exponentially small in the the sketch size. But each sketching map must fail on some inputs. When the inputs are adaptive, it may be possible to more quickly zoom on inputs that are bad for the sketching map.


\citet{HassidimKMMS20} presented a {\em generic robustness wrapper} that transforms a non-robust randomized sketch into a more robust one. Informally, this wrapper uses {\em differential privacy} \citep{dwork2006calibrating} to obscure the internal randomness of the sketching algorithm, effectively breaking correlations between the queries and the internal randomness. 

In more detail, to support $t$ adaptive queries, the wrapper of \citet{HassidimKMMS20} maintains approximately 
$\sqrt{t}$ independent copies of the non-robust sketch and answers each query by querying all (or some) of these sketches and aggregating their responses. 
As \citet{HassidimKMMS20} showed, this results in a more robust (and composable) sketch, that can support $t$ queries in total at the cost of increasing the space complexity by a factor of  
$\approx\sqrt{t}$.
Instantiating this wrapper with classical (non-robust) cardinality sketches results in a sketch for cardinality estimation that uses space $k\approx\sqrt{t}/\alpha^2$, where $\alpha$ is the accuracy parameter.

\textbf{Lower bounds.} 
Lower bounds on robustness are established by designing {\em attacks} in the form of adaptive sequences of queries. The objective of an attack is to force the algorithm to fail. An attack is more efficient if it causes the algorithm to fail using a smaller number of adaptive queries. We refer to number of queries in the attack as the {\em size} of the attack, which is typically a function of the sketch size $k$. Some attacks are {\em tailored} to a particular estimator, while others are {\em universal} in the sense that they apply to {\em any} estimator.


For cardinality sketches,
\citet{DBLP:journals/icl/ReviriegoT20} and~\citet{cryptoeprint:2021/1139} constructed $\tilde{O}(k)$-size attacks on the popular HLL sketch with its standard estimator. %\footnote{See \citep{hyperloglog:2007,hyperloglogpractice:EDBT2013}.},
\citet{AhmadianCohen:ICML2024} constructed 
$\tilde{O}(k)$-size attacks for popular MinHash sketching maps with their standard estimators, as well as a $\tilde{O}(k^2)$-size universal attacks. \citet{GribelyukLWYZ:FOCS2024} presented polynomial-size universal attacks on all linear sketching maps for cardinality estimation. Finally, \citet{CNSSS:ArXiv2024} presented optimal $\tilde{O}(k^2)$-size universal attacks on essentially all composable and linear sketching maps\footnotemark.


To summarize, there are matching upper and lower bounds of $t=\tilde{\Theta}(k^2)$ on the number of adaptive cardinality queries that can be approximated using a sketch of size $k$. The upper bound is obtained from the generic wrapper of \citet{HassidimKMMS20}, while the lower bound arises from the universal attack of \citet{CNSSS:ArXiv2024}. %against all composable and linear sketching maps for cardinality estimation. 
We refer to this limitation as the \emph{quadratic barrier}.



\subsection{Per-key participation}

Nevertheless, we can still hope for stronger {\em data-dependent} guarantees, and since cardinality sketches are widely used in practice, achieving this under realistic conditions is important. Specifically, we seek common properties of input queries %and a compatible sketch and estimator design 
such that, if these properties hold, we can guarantee accurate processing of $t \gg k^2$ adaptive queries.



We consider a parameter $r$ that is the \emph{per-key participation} in queries. Current attack constructions are such that most keys are involved in a large number of the queries and therefore $r\approx t$. However, in many realistic scenarios, the majority of keys participate in only a small number of queries and $r \ll t$. 
This pattern emerges when the distribution over the key domain shifts over time. For instance, the popularity of watched videos or browsed webpages can change over time, leading to a changing  set of access frequencies of keys.  Additionally, even when the query distribution is fixed, this pattern is consistent with Pareto-distributed frequencies, where a small fraction of keys (the ``heavy hitters'') appear in most queries, while most keys appear in only a limited number of queries. We therefore pose the following question:

\footnotetext{Here, ``essentially all'' means that the maps must satisfy certain basic reasonability conditions.}

\begin{ques} \label{main:problem}
Can we shift the quadratic barrier from the total number of queries $t$ to the typically much smaller parameter $r$, that is, can we design a robust (and composable) sketch of size $k\approx\sqrt{r}$ instead of $k\approx\sqrt{t}$? 
\end{ques}

\SetKwFunction{BkSketch}{BkSketch}
\SetKwFunction{SCest}{StdEst}
\SetKwFunction{RBCest}{RobustEst}
\SetKwFunction{RCest}{TRobustEst}

\subsection{Results Overview}
We provide an affirmative answer to \cref{main:problem}. Specifically, we design a sketch and estimator capable of handling an exponential number of adaptive queries provided that each key participates in at most $r=\tilde{O}(k^2)$
queries. We further provide an extension which maintain the guarantee even if this condition fails for a small fraction of the keys in each query. %Furthermore, our design gracefully degrades in approximation quality when a fraction of the keys in the sketch exceeds $r$ queries.

\textbf{Reformulating the robustness wrapper (\cref{framework:sec}).} As we mentioned, the generic wrapper of \citet{HassidimKMMS20} transforms non-robust sketches into more robust ones by obscuring their internal randomness using differential privacy. This effectively ``reduces'' the problem of designing a robust sketch to that of designing a suitable differentially private aggregation procedure. Our first contribution is to reformulate this wrapper so that the reduction is not to differential privacy, but rather to the problem of {\em adaptive data analysis (ADA)}.

In the ADA problem, we get an input dataset $S$ sampled from some unknown distribution $\DDD$, and then need to answer a sequence of {\em adaptively chosen statistical queries (SQ)} w.r.t.\ $\DDD$. This problem was introduces by \citet{DworkFHPRR15} who showed that it is possible to answer $\approx |S|^2$ statistical queries efficiently. The application of differential privacy as a tool for the ADA problem predated its application for robust data structures.

Our reformulated wrapper has two benefits: (1) It allows us to augment the generic wrapper with the granularity needed to address \cref{main:problem}, whereas the existing wrapper lacks this flexibility. (2) Even though our construction ultimately solves the ADA problem using differential privacy, other known solutions to the ADA problem now exist, and it is conceivable that future applications might need to leverage properties of these alternative solutions.



%Our first observation is that the existing robustness wrapper, which treats each sampled sketching map as a single privacy unit, lacks the granularity needed to address \cref{main:problem}. From a technical standpoint, tackling this challenge requires a fundamentally different approach to translating sketching maps into the statistical query (SQ) model, combined with a fine-grained analysis of SQ.

To this end, in \cref{framework:sec} we introduce a tool: an SQ framework with fine-grained generalization guarantees. %which combines individual privacy charging analysis with the SQ model. 
This framework addresses an analogous version of \cref{main:problem} for the ADA problem, where $k$
represents the sample size and $r$
is the maximum number of query predicates that are satisfied by a given key $x$.
To adapt this tool for sketching, we need to represent the randomness determining the sketching map as a sample from a product distribution and express the query response algorithm in terms of appropriate statistical queries. %The framework is general and can, in particular, be applied to analyze the robust version of CountSketch introduced by~\citet{CLNSSS:ICML2022}.


\textbf{Robust estimators for bottom-${\boldsymbol{k}}$ sketch.}
We use our fine-grained SQ framework in order to design a robust version for the popular bottom-$k$ MinHash cardinality sketch
\cite{Rosen1997a,ECohen6f,BRODER:sequences97,BJKST:random02}.
The randomness in the bottom-$k$ sketch corresponds to a map from keys to i.i.d.\ random priorities. The sketch $\txtBkSketch(V)$ of a subset $V$ includes  the $k$ keys with lowest priorities and their priority values. 
The standard cardinality estimator for this sketch returns a function of the highest priority in the sketch, which is a sufficient statistic for the cardinality. 
A sketch size of
$k=\tilde{\Omega}(\alpha^{-2})$ yields with high probability a relative error of $1\pm\alpha$.
The standard estimator, however, can be compromised using an attack with $t=\tilde{O}(k)$  queries~\cite{AhmadianCohen:ICML2024}. We present two robust estimators for the bottom-$k$ sketch that are analyzed using our fine-grained SQ framework:

\begin{itemize}[leftmargin=8.5pt,topsep=0pt,itemsep=0pt]
    \item \textbf{Basic Robust Estimator (\cref{bottomk:sec}).} We present a {\em stateless} estimator and show that for $\alpha\in (0,1)$ and $r=\tilde{\Omega}(k^2\alpha^{4})$, all estimates are accurate with high probability provided that all keys participate in no more than $r$ query sketches. In particular, since it always holds that $r\geq t$, this implies a guarantee of $t=\tilde{\Omega}(k^2\alpha^{4})$  on the number of adaptive queries.
Note that the sketch size ``budget'' of $k$ can be used to trade off accuracy and robustness.


\item \textbf{Tracking Robust Estimator (\cref{trackingest:Sec}).} 
We present another estimator that tracks the exposure of keys based on their participation in query sketches. Once a limit of $r$ is reached, the key is deactivated and is not used in future queries. 
The tracking estimator allows for smooth degradation and continues to be accurate as long as at most an $\alpha$ fraction of entries in the sketch are deactivated. 
Note that the tracking state is maintained by the query responder (server-side) and does not effect the computation or size of the sketch.
% The advantages of this approach is that it guarantees that the sketching map is not leaked and no adversary can compromise it.
% This is important because adversarial inputs can be constructed from workloads of benign queries \citep{AhmadianCohen:ICML2024} and this allow for robust long term use of the sketching map.
% This is much stronger than a guarantee in terms of the entries \emph{in the query set}. The sketch contains at most $k$ keys and the query set can be huge.
\end{itemize}



% with $k=\tilde{\Omega}(\sqrt{t}\alpha^{-2})$ for $t$ adaptive queries.




% \item [(i)] \textbf{Basic Robust Estimator} (\cref{bottomk:sec}): 
% This estimator does not maintain state from prior queries and accuracy and
%This estimator does not maintain state across queries. The statistical guarantees are for $r=\tilde{\Omega}(k^2\alpha^{-2})$, provided that all keys participate in no more than $r$ query sketches. 
% \ignore{It allows for smooth degradation in terms of the total number of keys that appeared in more than $r$ query sketches.}
% Our bottom-$k$ robust sketch is also a cleaner design compared with the super-sketch approach: We simply increase the size parameter of a standard bottom-$k$.
% is a standard sketch that can be scaled up to any size $k$ and has improved constant factors on the quadratic $t=\tilde{\Omega}(k^2)$ guarantee.


% We also show that the estimation quality declines with the number of keys in the query set that exceeded the participation limit. In particular, when the inputs do not adhere to participation limit, the responses leak information on the sketching map that may compromise it (in that an adversarial distribution can be constructed).

 
\textbf{Experiments.}
Finally, in \cref{experiments:sec}, we demonstrate the benefits of our fine-grained analysis using simulations on query sets sampled from uniform and Pareto distributions and observe $12 \times$ to $100 \times$ gains.


\subsection{Additional Related Work}

The adaptive setting has been studied extensively across multiple domains, including statistical queries~\citep{Freedman:1983,Ioannidis:2005,FreedmanParadox:2009,HardtUllman:FOCS2014,DworkFHPRR15,BassilyNSSSU:sicomp2021}, sketching and streaming algorithms~\citep{MironovNS:STOC2008,HardtW:STOC2013,BenEliezerJWY21,HassidimKMMS20,WoodruffZ21,AttiasCSS21,BEO21,DBLP:conf/icml/CohenLNSSS22,CNSS:AAAI2023Tricking,AhmadianCohen:ICML2024}, dynamic graph algorithms~\citep{ShiloachEven:JACM1981,AhnGM:SODA2012,gawrychowskiMW:ICALP2020,GutenbergPW:SODA2020,Wajc:STOC2020, BKMNSS22}, and machine learning~\citep{szegedy2013intriguing,goodfellow2014explaining,athalye2018synthesizing,papernot2017practical}.


\textbf{Lower bounds.}
For the ADA problem, 
\citet{HardtUllman:FOCS2014,SteinkeUllman:COLT2015} designed a quadratic-size universal attack, using Fingerprinting Codes~\citep{BonehShaw_fingerprinting:1998}. 
\citet{HardtW:STOC2013} designed a
polynomial-size universal attack on any linear sketching map for $\ell_2$ norm estimation.
\citet{DBLP:conf/nips/CherapanamjeriN20} constructed an
$\tilde{O}(k)$-size
attack on the Johnson Lindenstrauss Transform with the standard estimator.
\citet{BenEliezerJWY21} presented an
$\tilde{O}(k)$-size attack on the AMS sketch~\citep{ams99} with the standard estimator.
\citet{DBLP:conf/icml/CohenLNSSS22}
presented an
$\tilde{O}(k)$-size attack on Count-Sketch~\citep{CharikarCFC:2002} with the standard estimator.
\citet{CNSS:AAAI2023Tricking} presented  $\tilde{O}(k^2)$ size universal attack on the AMS sketch~\citep{ams99}  for $\ell_2$ norm estimation and on Count-Sketch~\cite{CharikarCFC:2002} (for  heavy hitter or inner product estimation).


% \ignore{

% The quintessential model for Adaptive Data Analysis (ADA) is  statistical queries (SQ) \citep{Kearns1998}. 
% There is a distribution $\Dd$ and a 
% statistical query with a predicate $h$ is for an estimate of the expected value $\E_{x\sim \Dd}[h(x)]$ of the predicate over $\Dd$. Statistical queries can be estimated from an i.i.d.\ sample $(x_1,\ldots,x_k)\sim \Dd^k$ using the empirical average $\frac{1}{k}\sum_{i\in[k]} h(x_i)$. 

% In non-adaptive settings, where queries are independent of prior results, it is possible to approximate an exponential number of queries in the sample size $k$, with small relative error. However, in adaptive settings, an adversary can exploit feedback from prior queries to overfit the sample, quickly constructing queries for which the empirical average deviates significantly from the true expectation.

% A simple approach to adaptive statistical queries is to partition our sample and use $\tilde{O}(\log k)$
% % $\approx 1/\alpha^2 \log(k)$ 
% fresh samples for each query. This allows for a linear  $t=\tilde{O}(k)$ number of adaptive queries.
% Advanced approaches improve this to support $t=\tilde{\Omega}(k^2)$ queries:
% \citep{DworkFHPRR:STOC2015,BassilyNSSSU:sicomp2021} showed that when the predicates preserve differential privacy of the sample then it still generalizes. Privacy is preserved by adding noise to the empirical averages before releasing them.
% % advanced composition~\citep{dwork2006calibrating}. 
% \citet{Blanc:STOC2023} proposed an alternative that does not add noise and instead uses a subsample to respond to each query.
% }%ignore


\section{Preliminaries}

\subsection{DP tools: linear queries with per-unit charging} \label{prelimDP:sec}

Differential privacy~\citep{dwork2006calibrating} (DP) is a Lipschitz-like stability property of algorithms, parametrized by $(\eps,\delta)$.
Two datasets $\boldsymbol{x},\boldsymbol{x}'\in X^n$ are \emph{neighboring} if they differ in at most one entry. Two probability distributions $\Dd$ and $\Dd'$ satisfy $\Dd\approx_{\eps,\delta} \Dd'$ if and only if for any measurable set of events $E$, $\Pr_D(E) \leq e^\eps \Pr_{\Dd'}(E)+\delta$ and $\Pr_{\Dd'}(E) \leq e^\eps \Pr_{\Dd}(E)+\delta$.
A randomized algorithm $A$ is \textit{$(\eps,\delta)$-DP} if for any two neighboring inputs $\boldsymbol{x}$ and $\boldsymbol{x}'$,
$A(\boldsymbol{x})\approx_{\eps,\delta} A(\boldsymbol{x}')$. DP algorithms compose in the sense that multiple applications of a DP algorithm to the dataset are also DP (with composed parameters). % We will use the following tools in a black-box manner.

\SetKwFunction{AboveThreshold}{AboveThreshold}
\SetKwFunction{BetweenThresholds}{BetweenThresholds}

Given a dataset 
$\boldsymbol{x} := (x_1,\ldots,x_n) \in X^n$ of items from domain $X$,
a \emph{counting query} is specified by a predicate 
$f:[n]\times X\to [0,1]$ and has the form
$f(\boldsymbol{x}) := \sum_{i\in [n]} f(i,x_i)$. For $\eps>0$, an algorithm that returns a noisy count 
$\hat{f}(\boldsymbol{x}) := f(\boldsymbol{x})+\Lap[1/\eps]$, where $\Lap$ is the Laplace distribution, satisfies $(\eps,0)$-DP. When multiple such tests are performed over the same dataset, the privacy parameters compose to $(r\eps,0)$-DP or alternatively to $(\sqrt{2r\log(1/\delta)}\eps +r\eps^2,\delta)$-DP for any $\delta>0$.

The Sparse Vector Technique (SVT) 
\citep{DNRRV:STOC2009,DBLP:conf/stoc/RothR10,DBLP:conf/focs/HardtR10,DBLP:books/sp/17/Vadhan17-dp-complex}
is a privacy analysis technique for a situation when an adaptive sequence of threshold tests on counting queries is performed on the same sensitive dataset $\boldsymbol{x}$. Each test $\AboveThreshold_\eps(f,T)$ is specified by a predicate $f$ and threshold value $T$.
The result is the noisy value $\hat{f}(\boldsymbol{x})$ if $\hat{f} > T$ and is $\perp$ otherwise. The appeal of the technique is a privacy analysis that only depends on the number $r$ of queries for which the test result is positive.

We will use here an extension of (a stateless version of) SVT, described in
\cref{algo:svt-individual}, 
that improves utility for the same privacy parameters  \citep{DBLP:conf/colt/KaplanMS21,feldman2021individual,CLNSSS:ICML2022,targetcharging:arxiv2023}.  
The algorithm maintains a set $A$ of \emph{active} indices that is initialized to all of $[n]$ and maintains charge counters $(C_i)_{i\in[n]}$, initialized to $0$.
For each query $(h,T)$, the response is   $\AboveThreshold^A_\eps(h,T)$ test result that is  $\hat{f} := \sum_{i\in A} h(i,x_i)+\Lap[1/\eps]$ if $\hat{h} > T$ and is $\perp$ otherwise. Note that $\AboveThreshold^A$ evaluates the query only over active indices. For each query with a positive (above) response, the algorithm increases the charge counts on all the indices that contributed to the query, namely, $h(i,x_i) = 1$. Once $C_i=r$, index $i$  is removed from the active set $A$ of indices.

The appeal of \cref{algo:svt-individual} is a fine-grained analysis that can only result in an improvement --
the privacy bounds have the same dependence on the parameter $r$, that in the basic approach bounds the total number $t$ of tests with positive outcomes and in the fine-grained one bounds the (potentially much smaller) per-index participation in such tests: 
\begin{theorem} [\cite{targetcharging:ICML2023} Privacy of Algorithm~\ref{algo:svt-individual}]\label{SVTindividual:thm}
For any $\eps < 1$ and $\delta \in (0, 1)$,  Algorithm~\ref{algo:svt-individual} is $(O(\sqrt{r \log(1/\delta)}\eps), 2^{-\Omega(r)} + \delta)$-DP (see \cref{thm:TCprivacy} for more precise expressions).
\end{theorem}

\begin{algorithm2e}[htbp]
    \caption{Linear Queries with Individual Privacy Charging}
    \label{algo:svt-individual}
    \DontPrintSemicolon
\small{    
%    \LinesNumbered
    \KwIn{
        Sensitive data set $(x_1,\ldots,x_n)\in X^n$; privacy budget $r > 0$; Privacy parameter $\eps>0$.
    }
    \lForEach(\tcp*[f]{Initialize counters}){$i\in [n]$}{
        $C_i\gets 0$ 
    }
    $A\gets [n]$ \tcp*{Initialize the active set}

\textbf{Function} $\AboveThreshold^A_\eps(h,T)$ \tcp*{\AboveThreshold query} 
\Indp
\KwIn{predicate $h:[n]\times X\to \{0,1\}$ and threshold $T\in \mathbb{R}$}
$\hat{h} \gets \left(\sum_{i\in A}h(i,x_i)\right) + \Lap(1/\eps) $ \tcp*{Laplace noise}
        \eIf(\tcp*[f]{Test against threshold}){$\hat{h} \ge T$}{
            \ForEach{$i\in A$ such that $h(i,x_i)  = 1$}{
                $C_i\gets C_i + 1$ \;
                \lIf{$C_i = r$}{
                    $A\gets A\setminus\{i\}$ 
                }
            }
            \Return{$\hat{h}$ \;
        }
        }{\Return{$\perp$}}
\Indm        
    \BlankLine
\OnInput(\tcp*[f]{Main Loop: process queries}){$(f,T)$}{ \Return{$\AboveThreshold^A_\eps(f,T)$}}
}
\end{algorithm2e}



\subsection{ADA tools}



The generalization property of differential privacy applies when the dataset $\bsx$ is sampled from a distribution. It states that if a predicate $h$ is selected in a way that preserves the privacy of the sampled points then we can bound its generalization error: the count over $\bsx$ is not too far from the expected count when we sample from the distribution. We will use the following variant of the cited works (see \cref{genproof:sec} for a proof):


% \begin{theorem}[Generalization property of DP \cite{DworkFHPRR15,BassilyNSSSU:sicomp2021,FeldmanS17}] \label{thm:DP-generalization} \label{thm:DP-gen-mod}
\begin{restatable}[Generalization property of DP \cite{DworkFHPRR15,BassilyNSSSU:sicomp2021,FeldmanS17}]{theorem}{dpgen} \label{thm:DP-generalization} \label{thm:DP-gen-mod}
Let $\mathcal{A}:X^n \to 2^{X}$ be an $(\eps, \delta)$-differentially private algorithm that operates on a dataset of size $n$ and outputs a predicate $h: X\to \{0,1\}$. Let $\Dd=D_1\times\cdots D_n$ be a product distribution over $X^n$, let $\bsx=(x_1,\ldots,x_n) \sim \Dd$ be a sample from $\Dd$, and let $h\gets \mathcal{A}(\bsx)$. Then for any $T\ge 1$ it holds that 
\footnotesize{
\begin{align*}
    \Pr_{\substack{\bsx\sim \Dd,\\ h\gets \mathcal{A}(\bsx)}}\left[ e^{-2\e} \E_{\boldsymbol{y}\sim \Dd} h(\boldsymbol{y}) - h(\bsx) > \frac{4}{\eps}\log(T+1) + 2Tn\delta \right] < \frac{1}{T},\\
    \Pr_{\substack{\bsx\sim \Dd,\\ h\gets \mathcal{A}(\bsx)}}\left[
h(\bsx) -
e^{2\e} \E_{\boldsymbol{y}\sim \Dd} h(\boldsymbol{y}) 
> \frac{4}{\eps}\log(T+1) + 2Tn\delta \right] < \frac{1}{T},
\end{align*}
}
% \end{theorem}
\end{restatable}

where $h(\bsy)$ denotes the total value of $h$ over elements of $\bsy$.

When applying \cref{thm:DP-generalization}, we will assume that $h$ also takes the index $i$ as an argument (so, we write $h(i, x_i)$ instead of $h(x_i)$). This is equivalent because we can replace $D_i$ with a distribution that samples the tuple $(i, x_i)$ for $x_i \sim D_i$. 

\section{ADA with fine-grained analysis} \label{framework:sec}

We now consider a variation of the ADA framework where
we sample a dataset $\boldsymbol{x}\sim \Dd$ from a  product distribution $\Dd$ and then process adaptive linear threshold queries as in Algorithm~\ref{algo:svt-individual} over the dataset $\boldsymbol{x}$. 
The benefit of this is obtaining bounds in terms of the 
per-key participation in queries (that is the number of queries $h$ for which $h(i,x_i)=1$), which is always lower
than the total number of queries. 
Moreover, the approach tolerates a small fraction of deactivated indices in each query, which simply contribute proportionally to the error. 

We bound the error due to generalization and sampling and due to the privacy noise and the deactivation of keys that reached the charging limit $r$:

\begin{lemma} [Generalization and sampling error bound] \label{generror:lemma}
    Let $\Dd=D_1\times\cdots\times D_n$ be a product distribution over $X^n$. Let 
    $\boldsymbol{x}\sim \Dd$ be a sampled dataset. Let $\alpha, \beta > 0$ be sufficiently small (i.e., smaller than some absolute constant).
    Consider an execution of \cref{algo:svt-individual} on dataset $\boldsymbol{x}$ with $m$ adaptive queries, parameter $r \gg \log (n/\beta)$, and \[ \e_0 \coloneqq \frac{\alpha}{4\sqrt{r\log(n/\beta)}}.\] 
    Then it holds that with probability at least $1-\beta$, for all of the $m$ query predicates $h$,
    {\small
\[
 \left| \E_{\boldsymbol{y}\sim \Dd} [h(\boldsymbol{y})] - h(\boldsymbol{x}) \right| <  \alpha \cdot \E_{\boldsymbol{y}\sim \Dd} [h(\boldsymbol{y})] + O\p{\frac{\log(m/\beta)}{\alpha}}.
\]}    

\end{lemma}
\begin{proof}
    The first claim of the sampling and generalization error, follows from \cref{SVTindividual:thm} and \cref{thm:DP-generalization}
    
    For the privacy parameters in \cref{SVTindividual:thm} we set $\delta = \beta/n^2$ and obtain $\eps = \sqrt{r\log(n^2/\beta)}\cdot\eps_0 < \al/2\sqrt 2$.
    From \cref{thm:DP-generalization} we get that, for each query $h$, 
    the additive error $\left| \E_{\boldsymbol{y}\sim \Dd} h(\boldsymbol{y}) - h(\boldsymbol{x}) \right|$ is at most \[
    (e^{2\e}-1)\cdot \E_{\boldsymbol{y}\sim \Dd} h(\boldsymbol{y}) + \frac{4}{\eps }\log(T+1) + 2Tn\delta,
    \]
    with probability at least $1-2/T$.
    Note that we have $e^{2\e}-1 < \al$. Thus, setting $T=2m/\beta$ (so that a union bound over all queries gives a failure probability of $1-\beta$), the result follows.
\end{proof}

\begin{claim} [Noise and deactivation error bounds] \label{noisedeactiveerror:claim}
Under the conditions of \cref{generror:lemma},
with probability at least $1-\beta$, for all $m$ query predicates $h$, for $\hat{h} \coloneqq \sum_{i\in A} h(i,x_i )+\Lap(1/\eps_0)$,
\begin{gather*}
    h(\boldsymbol{x}) - \hat{h} >
    - \log(2m/\beta)/\eps_0,\\
    h(\boldsymbol{x}) - \hat{h} <
    \log(2m/\beta)/\eps_0 + \sum_{i\in [n]\setminus A} h(i,x_i).
\end{gather*}
\end{claim}
\begin{proof}
    Each Laplace noise $\Lap(1/\eps_0)$ is bounded by $\pm\log(2m/\beta)/\eps_0$ with probability at least $1-\beta/m$, so by a union bound, with probability at least $1-\beta$, it is bounded as such for all queries, and the result follows immediately.
\end{proof}

The total error of $\hat{h}$ with respect to the expectation $\E_{\boldsymbol{y}\sim \Dd} h(\boldsymbol{y})$ is bounded by the sum of errors in \cref{generror:lemma} and \cref{noisedeactiveerror:claim}:
\begin{corollary} \label{totalerror:coro}
For some constants $c_1,c_2>0$, under the conditions of \cref{generror:lemma}, with probability at least $1-\beta$, for all $m$ queries $h$,
\[ -\Delta <
\E_{\boldsymbol{y}\sim \Dd} [h(\boldsymbol{y})] - \hat{h} 
< \Delta + \sum_{i\in [n]\setminus A} h(i,x_i),
\]
where
\[
\Delta = \alpha\cdot \E_{\boldsymbol{y}\sim \Dd} [h(\boldsymbol{y})] + O(\al^{-1} \sqrt{r} \log^{3/2}(mn/\beta)).
\]
\end{corollary}

We can apply this fine-grained ADA to analyze the robustness of randomized data structures (or algorithms) that sample randomness $\boldsymbol{\rho}$ and process adaptive queries $M_i$ that depend on the interaction till now and the randomness $\boldsymbol{\rho}$. 
To do so, we need to specify a product distribution 
$\mathcal{\Dd} = D_1 \times D_2 \times \dots \times D_n$
so that 
\begin{enumerate}
\item
    The distribution of $\boldsymbol{\rho}$ is $\Dd$.
\item The queries in the original problem can be specified in terms of  linear queries over $\bsr$ and have statistical guarantees of utility over $\bsr \sim \Dd$. 
% Moreover, we get utility also with additional relative error 
% so that we get utility if the approximation is good over the distribution of $\rho \sim \Dd$.
\end{enumerate}
When applying this to sketching maps which do not contain all the information of $\bsr$, we will need to ensure that the linear queries we use can be evaluated over the sketch.


% \ignore{
% \begin{remark}
%     Current writing is just for "above threshold" queries. But framework can be generalize to between threshold (pay only when close to threshold) and selection (multiple predicates and select one or top-$k$ scores and pay only for selections). That is, applications in the target charging framework. But the individual DP algorithms are just counts since this is what the ADA framework does (linear queries). 
% \end{remark}
% }


\section{The Bottom-\texorpdfstring{$k$}{k} Cardinality Sketch} \label{bottomk:sec}



\begin{algorithm2e}[t]\caption{Bottom-$k$ Cardinality Sketch and Standard Estimator}\label{bottomk:algo}
\DontPrintSemicolon
{\small
Sample $\rho_i \sim U[0, 1]$ for $i\in [n]$.\tcp{Randomness for the Sketching Map}

\Function(\tcp*[f]{Bottom-$k$ sketching map using $\boldsymbol{\rho}$}){ $\BkSketch_{\boldsymbol{\rho}}(V)$}
{
\KwIn{Set $V\subset [n]$}
\eIf{$|V|\leq k$}{\Return{$\{(i, \rho_i) \mid i \in V\}$ }}
{\Return{$\{(i, \rho_i) \mid i \in V, \rho_i < \boldsymbol{\rho}_{(k),V}\}$}\tcp{
where ${\boldsymbol{\rho}}_{(k),V}$ is the $k$th smallest in the multiset $\{\rho_i \mid i\in V\}$}
}}
\BlankLine
\Function(\tcp*[f]{Standard estimator}){$\SCest_k(S)$}{
\KwIn{A bottom-$k$ sketch $S$}
\eIf{$|S| < k$}{
    \Return{$|S|$} 
}{
$\tau \gets \max_{(i,\rho_i)\in S} \rho_i$\tcp*{$k$th smallest $\rho_i$}
        \Return{$(k-1)/\tau$}
}
}
\OnInput(\tcp*[f]{Main Loop}){$S$}{\Return{$\SCest_{k}(S)$}}
}
\end{algorithm2e}

\subsection{Sketch and standard estimator}
The bottom-$k$ cardinality sketch and standard estimator are described in \cref{bottomk:algo}. 
Let the ground set of keys be $[n]$.
We sample a vector of random values $\boldsymbol{\rho} \sim [0, 1]^n$.
% ; this will be the database which we keep private. 
That is, for each key $i \in [n]$ there is an associated i.i.d.\ $\rho_i \sim \Dd$.
The vector $\boldsymbol{\rho}$ specifies the bottom-$k$ sketching map $\txtBkSketch_{\boldsymbol{\rho}}(V)$ that maps a set $V\subset [n]$ to its sketch. The sketch consists of the pairs $(i, \rho_i)$ for the $k$ values of $i \in V$ such that $\rho_i$ is smallest. When $|V|\leq k$, the sketch contains all elements of $V$. Note that, though $n$ and also $|V|$ can be very large, the size of the sketch is at most $k$.   This sketching map is clearly composable.\footnote{The analysis uses a common assumption of full i.i.d.\ randomness in the specification of the sketching maps. Note that $O(\log k)$ bits of representation are sufficient. Implementations use pseudo-random hash maps $i\mapsto \rho_i$.}

For a query set $V\subset [n]$, we apply an estimator to the sketch $S := \txtBkSketch_r(V)$ to obtain an estimate of the cardinality of $V$. The standard estimator $\txtSCest(S)$ computes $\tau$ which is the $k$th order statistics of the $\rho_i$ values in the sketch, which is a sufficient statistic of the cardinality. It then returns the value $(k-1)/\tau$. The estimate is unbiased, has variance at most $|V|/(k-2)$, and an exponential tail (see e.g.~\cite{ECohenADS:TKDE2015}).
This standard estimator is know to optimally use the information in the sketch $S$ but
can be attacked with a linear number of queries~\cite{AhmadianCohen:ICML2024}.


\subsection{Basic robust estimator}

\begin{algorithm2e}[t]\caption{Basic Robust Cardinality Estimator}\label{bottomkrobust:algo}
\DontPrintSemicolon
{\small
\Function(\tcp*[f]{Estimate $|V|$ from $\BkSketch_{\boldsymbol{\rho}}(V)$}){$\RBCest_{k}(S)$}{
    \KwIn{A bottom-$k$ sketch $S$, $\alpha\in(0,0.5)$}
    
    \eIf(\tcp*[f]{Return exact value when $\leq k$}){$|S| < k$}{
        \Return{$|S|$}
    }{
        $\tau \gets k/2n$, $T = (1-\al)k$\;
        
        \While {$(\tau<1)$ and $(\tw{h}\gets \sum_{(i,\rho_i)\in S} \ind{(\rho_i < \tau)}+\Lap(1/\eps_0)) < T $} {
            $\tau \gets (1+\al/4)\tau$\;
        }
        
        \Return{$T/\tau$}        
    }  
}
\KwIn{Parameters $k, r, n\geq 1$ and $\al, \beta > 0$}
$\eps_0 \gets \f{\al/8}{4\sqrt{r \log(n/(\beta/4))}}$ \tcp{as \cref{generror:lemma} with $\frac{\al}{8}$, $\frac{\beta}{4}$}
\OnInput(\tcp*[f]{Main Loop}){$S$}{\Return{$\RBCest_{k,r}(S)$}}
}
\end{algorithm2e}



\cref{bottomkrobustbaseline:algo} describes a robust estimator $\txtRBCest$ that is applied to a bottom-$k$ sketch.

We analyze this estimator under the assumption that the query set sequence $(V_j)_{j\in [t]}$ has the property that each key
$i\in [n]$ 
appears in at most $r$ sketches in $( \txtBkSketch_{\boldsymbol{\rho}}(V_i))_{j\in[t]}$:
\begin{equation} \label{limitassumptionsketch:eq}
\forall i\in[n], \sum_{j\in[t]} \ind{i\in  \txtBkSketch_{\boldsymbol{\rho}}(V_i)} \leq r.
\end{equation}
Note that for this to hold it suffices that each key 
is included in at most $r$ query sets. That is,
$\forall i\in[n], \sum_{j\in[t]} \ind{i\in V_j} \leq r$.

\begin{theorem} [Basic robust estimator guarantee] \label{basicrobust:thm}
    If the query sequence in \cref{bottomkrobust:algo} satisfies \eqref{limitassumptionsketch:eq} for some $r \gg \log(n/\beta)$, then for a value of $k = O(\al^{-2} \sqrt r \log^{3/2}(n/\beta))$, every output will be $(1\pm\al)$-accurate with probability at least $1-\beta$.
\end{theorem}

% \eccomment{Shouldn't it be $1\pm\alpha$? Can we say that we can replace $n$ with the max of query set size and $t$? }

\subsection{Analysis of Basic robust estimator} \label{sec:analysis-basic}
In this section we prove \cref{basicrobust:thm}.

Before we start, we make some basic assumptions on the parameters which we will use throughout the proof. First, we pick
\[k = C \al^{-2} \sqrt r \log^{3/2}(n/\beta),\]
where the constant $C$ is chosen to be sufficiently large. Furthermore, note that if $k \ge n$ then we are always storing the whole set $V$ in the sketch, so we may assume that $k < n$ (and therefore $r < n^2$ and $\al > 1/\sqrt{n}$).

We map \cref{bottomkrobust:algo} to 
the framework of \cref{algo:svt-individual}, where the dataset is $\boldsymbol{\rho}$.

First, we show that we can consider the sum of $\ind{\rho_i < \tau}$ to be over the entire set $V$, rather than just those that are included in the bottom-$k$ sketch $S$:

\begin{lemma} \label{lemma:sub-h-hat}
Suppose that, in \cref{bottomkrobust:algo}, $\tw h = \sum_{(i,\rho_i)\in S} \ind{(\rho_i < \tau)}+\Lap(1/\eps_0))$ is replaced by $\hat h = \sum_{i \in V} \ind{(\rho_i < \tau)}+\Lap(1/\eps_0))$ (where the two Laplace random variables are coupled to be the same value). Then, the sequence of outputs of \cref{bottomkrobust:algo} changes with probability at most $\beta/4$.
\end{lemma}
\begin{proof}
Since $S$ contains the $k$ values of $i$ such that $\rho_i$ is minimal, the only way to have $\hat h \neq \tw h$ is to have the sum over $S$ be equal to $k$ and the sum over $V$ to be greater than $k$. The outputs may then only differ if $\tw h < T$, but the probability that $k + \Lap(1/\eps_0) < (1-\al)k$ is at most $e^{-\e_0 \al k} < \beta/\poly(n)$,
where the polynomial in $n$ can be made as large as we like (by setting the constant on $k$).

Now, note that the total number of queries to \cref{bottomkrobust:algo} cannot exceed $nr \le \poly(n)$ by \eqref{limitassumptionsketch:eq}. Furthermore, the total number of iterations of the while loop per call is at most $O(\al^{-1} \log n) = \poly(n)$.

The lemma follows by taking a union bound over all iterations of the while loop and over all queries to \cref{bottomkrobust:algo}.
\end{proof}

With this lemma in mind, we will henceforth assume through this entire section that \cref{bottomkrobust:algo} uses $\hat h$ instead of $\tw h$, introducing a failure probability of at most $\beta/4$.

Now, we will show that the execution of \cref{bottomkrobust:algo} can be performed via queries to \cref{algo:svt-individual}, rather than accessing $\bsr$ directly. Indeed, note that the counting query $\hat h$ takes the same form (except for the check being over $[n]$ instead of $A$) as its analog in \cref{algo:svt-individual}, where the query function is
\[
h_{V,\tau}(i,\rho_i) \coloneqq \ind{i\in V \land \rho_i<\tau}.
\]

Observe that for any query sketch $S$, there is at most one positive test in \cref{bottomkrobust:algo}. Therefore, per assumption \eqref{limitassumptionsketch:eq} on the input, each index appears in at most $r$ positive tests. Therefore, if we were to instead perform these tests using \cref{algo:svt-individual}, all indices would remain active and nothing would ever be removed from $A$. Thus, we would have that $A=[n]$, so indeed the values of $\hat h$ are identical in \cref{algo:svt-individual} and \cref{bottomkrobust:algo}. Thus, \cref{bottomkrobust:algo} can be simulated by queries to \cref{algo:svt-individual}, so we may apply the results of \cref{framework:sec}.

In order to apply \cref{totalerror:coro}, we need to first compute the expectation of $h_{V, \tau}$ on $\Dd$:
\begin{claim} \label{card:claim}
    \begin{equation} \label{expectation:eq}
\E_{\boldsymbol{y}} \left[h_{V,\tau}(\boldsymbol{y})\right] = \tau |V|.
\end{equation}
\end{claim}
\begin{proof}
  Observe that for $i\not\in V$, $h_{V,\tau}(i,y_i)=0$ for all $y_i$ and for $i\in V$,
$\E[h_{V,\tau}(i,y_i)] = \tau$.  
\end{proof}

Finally, recall from the proof of \cref{lemma:sub-h-hat} that the total number of iterations of the while loop (and thus the overall total number of calls to \cref{algo:svt-individual}) is at most $\poly(n)$, so in \cref{totalerror:coro} we can take $m=\poly(n)$.

\begin{algorithm2e}[t!]\caption{Tracking Robust Estimator}\label{bottomkrobustbaseline:algo}
\small{
\DontPrintSemicolon
\Function(\tcp*[f]{Robust Cardinality Estimate of $V$ from $\BkSketch_{\boldsymbol{\rho}}(V)$}){$\RCest_{k,r}(S)$}{
    \KwIn{A bottom-$k$ sketch $S$}
    
    \eIf(\tcp*[f]{Return exact value when $\leq k$}){$|S| < k$}{
        \Return{$|S|$}
    }{
        $\tau \gets k/2n$, $T \gets k/4$\;
        
        \While {$(\tau<1) \land (\tw h \ot \sum_{(i,\rho_i)\in S} \ind{(\rho_i < \tau)\land (C[i]<r)}+\Lap(1/\eps_0)) < T$} {
            $\tau \gets (1+\al/8)\tau$\;
        }
        % \eIf(\tcp*[f]{too many exposed keys}){$\tau>1$}{\Return{$\bot$}}{
        \ForEach(\tcp*[f]{Per-key tracking}){$(i,x_i)\in S$}{
            % \If(\tcp*[f]{increment exposure counter}){$x_i<\tau$}{\leIf{$i\not\in C$}{$C[i]\gets 1$}{$C[i]\gets C[i]+1$}}
            \lIf{$x_i<\tau$}{
                $C[i]\gets C[i]+1$
            }
        }
        
        \Return{$T/\tau$}
        % }
    }  
}
\KwIn{Parameters $k$, $r\geq 1$}
\tcp{Initialization}
$C\gets \{ \}$ \tcp*{Dictionary with default value $0$} 
$\eps_0 \gets \f{\al/16}{4\sqrt{r \log(n/(\beta/4))}}$\tcp{as \cref{generror:lemma} with $\frac{\al}{16}$, $\frac{\beta}{4}$}
\OnInput(\tcp*[f]{Main Loop}){$S$}{\Return{$\RCest_{k,r}(S)$}}
}
\end{algorithm2e}

Now, by \cref{totalerror:coro}, we have with probability at least $1-\beta/4$ that
\begin{gather}
\hat h < (1 + \al/8) \tau |V| + \al k / 8, \label{eq:hat-ub} \\
\hat h > (1 - \al/8) \tau |V| - \al k / 8, \label{eq:hat-lb}
\end{gather}
where we have used that
\[\Delta = O(\al^{-1} \sqrt r \log^{3/2}(n/\beta)) < \al k / 8,\]
by the choice of $k$. (Recall also that $[n] \setminus A$ is always empty, so the sum term in \cref{totalerror:coro} vanishes.) We assume henceforth that this probability-$(1-\beta/4)$ event does in fact occur. 

\begin{prop} \label{prop:low-guarantee}
Whenever $\tau < (1 - \al/2) T / |V|$, the while loop in \cref{bottomkrobust:algo} continues to the next value of $\tau$.
\end{prop}
\begin{proof}
Note that $\al k / 8 < \al T / 4$ since $T > k/2$. Thus, by \eqref{eq:hat-ub}, when $\tau < (1 - \al/2) T / |V|$, we have $\hat h < (1 + \al/8)(1-\al/2)T + \al T / 4 < T$ (for sufficiently small $\al$), so we are done.
\end{proof}

\begin{prop} \label{prop:high-guarantee}
Whenever $\tau > (1 + \al/2) T / |V|$, the while loop in \cref{bottomkrobust:algo} terminates.
\end{prop}
\begin{proof}
Again, by \eqref{eq:hat-lb}, when $\tau > (1 + \al/2) T / |V|$, we have $\hat h > (1 - \al/8)(1 + \al/2)T - \al T / 4 > T$, so we are done.
\end{proof}

Now, \cref{prop:low-guarantee} ensures that the output of the algorithm is always at least $(1-\al/2)|V|$. Moreover, since $\tau$ is incremented by factors of $1+\al/4$, there will be some value of $\tau$ tested that is between $(1 + \al/2) T / |V|$ and $(1 + \al) T / |V|$ (note that since $|V| \ge k$, we have $(1 + \al) T / |V| < 1$). By \cref{prop:high-guarantee}, this value will cause the while loop to terminate, yielding an output that is at most $(1+\al)|V|$. This completes the proof of \cref{basicrobust:thm}.


\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.32\textwidth]{plots/Trobust_uniforms5000sweepknum.png}
    \includegraphics[width=0.32\textwidth]{plots/Trubust_pareto2s5000sweepknum.png}
    \includegraphics[width=0.32\textwidth]{plots/Trobust_pareto15s5000sweepknum.png}
\caption{Number of guaranteed queries for sketch size $k$. The gain factor of \txtRCest\ over baseline is over two orders of magnitude with the Uniform distribution, $40\times$ for Pareto with $\alpha=2$, and $12\times$ for Pareto with $\alpha=1.5$.}
    \label{fig:fine-grained-gain}    
\end{figure*}   

When assumption 
\eqref{limitassumptionsketch:eq} does not hold, that is, when some keys get \emph{maxed} (have participated in more than $r$ query sketches), the guarantees are lost even when there are 
no maxed keys in the query set. The universal attack constructions of~\cite{AhmadianCohen:ICML2024,CNSSS:ArXiv2024} show this is unavoidable. The attack 
fixes a ground set $U$ and identifies keys with low priorities (these are the keys that tend to be maxed). The query sets that is $U$ with the identified keys deleted has cardinality close to $|U|$ but the estimates of \txtRBCest\ would be biased down.
In the next section we introduce a tracking estimator that allows for smooth degradation in accuracy guarantees as keys get maxed. 




\section{Robust estimator with tracking} \label{trackingest:Sec}


We next propose and analyze the estimator $\RCest$ in \cref{bottomkrobust:algo} that is an extension of \txtRBCest\ that includes tracking and deactivation of keys that appeared in the query sketches more than $r$ times. 
This estimator offers smooth degradation in estimate quality that depends only on the number of \emph{deactivated} keys present in the sketch and this is guaranteed as long as there are no queries where most of the sketch is deactivated. 
% It provides key-level robustness guarantees. Its advantages over basic:

% This estimator is guaranteed, on any input, not to leak information on the sketching map that can be used to construct adversarial inputs.







%\newcommand{\leIf}[3]{\textbf{if} #1 \textbf{then} #2 \textbf{else} #3}

% \begin{theorem} \label{thm:main-tracking}
\begin{restatable}[Analysis of \txtRCest]{theorem}{trackinganalysis}
\label{thm:main-tracking}
For a value of 
$k = O(\al^{-2} \sqrt r \log^{3/2}(n/\beta))$,
% $k=O(\al^{-2} \log(n/\beta) + \al^{-1} \sqrt r \log^{3/2}(mn/\beta))$, 
suppose that an adaptive adversary provides at most $m$ inputs to \cref{bottomkrobustbaseline:algo} such that the sketch of every input has at most $k/2$ deactivated keys. Then, with probability at least $1-\beta$, for every input whose sketch has at most $\al k / 4$ deactivated keys, the output is a $(1+\al)$-approximation of the true cardinality.
\end{restatable}

This theorem guarantees that, as long as no query has too many (more than $k/2$) deactivated keys, the results of \cref{bottomkrobustbaseline:algo} will continue to be accurate even for queries that have a few (at most $\al k/4$) deactivated keys. This allows the algorithm to continue guaranteeing accuracy even if a few keys are subject to many queries. We discuss the numerical advantages of this further in \cref{experiments:sec}.
The proof is analogous to \cref{sec:analysis-basic} and is provided in \cref{trackinganalysis:sec}.


\section{Empirical Demonstration} \label{experiments:sec}

We demonstrate the effectiveness of our fine-grained approach by
comparing the number of queries that can be answered effectively with \txtRCest\ to that of the baseline per-query analysis.
We use synthetically generated query sets sampled from Uniform and Pareto distributions with $\alpha\in \{1.5,2\}$ and $x_m=1$, support size of $10^6$ and query set size of $5\times 10^3$.
For each sketch size $k$, we match a value of the parameter $r= 0.002 k^2$ (with \txtRCest) and respectively $t= 0.002 k^2$ with baseline analysis.
% For noise scale $\eps=5/k$ and bounding the number of  \AboveThreshold tests using target-charging analysis. 

With \txtRCest, we count the number of queries for which at most $10\%$ of sketch entries are deactivated and stop when there is a sketch with $50\%$ of entries deactivated.
Figure~\ref{fig:fine-grained-gain} reports the number of queries with the baseline and \txtRCest\ estimators. The respective gain factor is measured by the ratio of the number of queries that can be effectively answered with per-key analysis to the baseline. We observe gains of two orders of magnitude for uniformly sampled query sets. This hold even without tracking -- using \txtRBCest\ -- where we stop as soon as there is a key that appeared in $r$ queries. For  Pareto query sets, tracking is necessary, as some keys do appear in many query sketches. We observe gains of $12\times$ for the very skewed $\alpha=1.5$ and $ 40\times$ with $\alpha=2$.


%The generalization bounds hold with estimator \RBCest, until $k/2$ keys in total are maxed, or with estimator \RCest, until there is a query sketch with more than $k/2$ keys deactivated. We count the number of queries that can be answered effectively when there are at most $10\%$ of sketch entries maxed or deactivated, respectively.






\section*{Conclusion}


Our work raises several follow-up questions. Our fine-grained robust estimators are specifically designed for the bottom-$k$ cardinality sketch.
We conjecture that it is possible to derive estimators with similar guarantees for other MinHash sketches, including the 
$k$-partition (PCSA -- Stochastic Averaging) cardinality sketches~\cite{FlajoletMartin85,hyperloglog:2007}. The missing piece is that the fine-grained ADA framework lacks the necessary flexibility, and requires an extension beyond plain linear queries. 
Another open question is whether similar results hold for other norms, particularly in scenarios where most inputs are sparse, and only a fraction of entries are 'heavy'meaning they are nonzero across many inputs. For $\ell_2$ norm estimation with the popular AMS sketch~\cite{ams99}, the answer is negative, as known quadratic-size attacks remain effective even when inputs are sparse with disjoint supports~\cite{CNSS:AAAI2023Tricking}. However, we conjecture that similar results are possible for sublinear statistics, including capping statistics~\cite{CapSampling,CohenGeri:NeurIPS2019},  whose sketches incorporate generalized cardinality sketches, and for (universal or specialized) bottom-$k$ sketches, which are weighted versions of the bottom-$k$ cardinality sketch.
  
% Our fine-grained robust estimators are specifically designed for the bottom-$k$ cardinality sketch. It would be nice to derive such estimators for $k$-partition (stochastic averaging) cardinality sketches such as the popular HyperLogLog~\cite{hyperloglog:2007}. The hurdle is that 
% the fine-grained ADA framework does not have the needed flexibility and we need to extend it beyond plain linear queries.


% Another question is whether we can hope for similar results for other norms, in the situation that most inputs are sparse and at most a fraction of the entries are `heavy'' in the sense that they are nonzero on many inputs.
% For $\ell_2$  norm estimation, with popular sketches~\cite{ams99}, the answer is negative -- known quadratic size attacks apply even with sparse inputs with disjoint supports~\cite{CNSS:AAAI2023Tricking}. The question is open for sublinear statistics, including capping statistics~\cite{CapSampling,CohenGeri:NeurIPS2019} that have sketches based on extensions of cardinality sketches.

% This can be extended with data-dependent privacy analysis on linear queries (e.g. selection, between thresholds) where we only pay for queries that hit the target. 




\newpage

\section*{Acknowledgments}

Edith Cohen was partially supported by Israel Science Foundation (grant 1156/23). 
Uri Stemmer was Partially supported by the Israel Science Foundation (grant 1419/24) and the Blavatnik Family foundation.

\ignore{
\section*{Impact Statement}

This paper presents work whose goal is to advance the field of 
Machine Learning. There are many potential societal consequences 
of our work, none which we feel must be specifically highlighted here.
}


% \bibliographystyle{plainnat}
\bibliographystyle{icml2025}
% \bibliography{main,references,robustHH}
\input{main.bbl}


\newpage
\appendix
\onecolumn



\section{Fine-grained SVT privacy bounds}
More precise bounds for Theorem~\ref{algo:svt-individual} and for standard SVT throught the target charging technique (TCT).
\begin{theorem}[Privacy of Target-Charging \cite{targetcharging:ICML2023} ]\label{thm:TCprivacy}
Algorithm~\ref{algo:svt-individual} (and per-query SVT) satisfy the following approximate DP privacy bounds:
\begin{align*}
&\left( (1+\alpha)\frac{r}{q}\eps, \delta^*(r,\alpha)\right) , & \text{for any $\alpha>0$;}\\
&\left( \frac{1}{2}(1+\alpha)\frac{r}{q} \eps^2  + \eps \sqrt{(1+\alpha)\frac{r}{q} \log(1/\delta)}, \delta + \delta^*(r,\alpha) \right), & \text{for any $\delta>0$, $\alpha>0$.}
\end{align*}
where $\delta^*(r,\alpha) \leq e^{-\frac{\alpha^2}{2(1+\alpha)} r}$ and $q=\frac{1}{e^\eps+1}$.
\end{theorem}

\section{Proof of extended generalization} \label{genproof:sec}

Here we will prove \cref{thm:DP-gen-mod}:

\dpgen*

The proof is obtained by transforming the following {\em expectation bound} into a {\em high probability bound}. 

\begin{lemma}[Expectation bound {\cite{kontorovich2022adaptive}}]\label{lem:MKLCondExpSQ}
    Let $\Bb$ be an $(\eps,\delta)$-differentially private algorithm that operates on $T$ sub-databases and outputs a predicate $h:X\rightarrow[0,1]$ and an index $t\in\{1,2,\dots,T\}$.
Let $\Dd=D_1\times\cdots D_n$ be a product distribution over $X^n$ be a distribution over $X$, let $\vec{\bsx}=(\bsx_1,\dots,\bsx_T)$ where every $\bsx_j \sim \Dd$ is sampled independently, and let $(h,t)\leftarrow \Bb\left(\vec{\bsx}\right)$.
Then,
$$
\E_{\substack{\vec{\bsx}\sim\Dd \\ (h,t)\leftarrow \Bb\left(\vec{\bsx}\right)}}\Big[ e^{-\eps} \cdot h(\Dd) \Big] - Tn\delta \;\;\;
\leq
\E_{\substack{\vec{\bsx}\sim\Dd \\ (h,t)\leftarrow \Bb\left(\vec{\bsx}\right)}}\Big[ h(\bsx_t) \Big] \;\;\;
\leq \E_{\substack{\vec{\bsx}\sim\Dd \\ (h,t)\leftarrow \Bb\left(\vec{\bsx}\right)}}\Big[ e^{\eps} \cdot h(\Dd) \Big] + Tn\delta.
$$
\end{lemma}
(Here, we use $h(\Dd)$ as shorthand to denote $\E_{\bsy \sim \Dd} h(\bsy)$.)
\begin{proof}
The proof is identical to that of Lemma 3.1 of \cite{kontorovich2022adaptive} with $\psi = 0$, and omitting the final inequality in the last chain of inequalities.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm:DP-gen-mod}]
We prove the first inequality; the second follows from similar arguments. 
Fix a product distribution $\Dd$ on $X$. Assume towards contradiction that with probability at least $1/T$ algorithm $\Aa$ outputs a predicate $h$ such that \ 
$e^{-2\eps} \cdot h(\Dd)-h(\bsx)> \frac{4}{\eps}\log(T+1) + 2Tn\delta$.
We now use $\Aa$ and $\Dd$ to construct the following algorithm $\Bb$ that contradicts Lemma~\ref{lem:MKLCondExpSQ}. We remark that algorithm $\Bb$ ``knows'' the distribution $\Dd$. This will still lead to a contradiction because the expectation bound of Lemma~\ref{lem:MKLCondExpSQ} holds for {\em every} differentially private algorithm and {\em every} underlying distribution.


\begin{algorithm2e}[htbp]
\caption{$\Bb$}\addcontentsline{lof}{figure}{Algorithm $\Bb$}
\DontPrintSemicolon
\KwIn{$T$ databases of size $n$ each: $\vec{\bsx}=(\bsx_1,\dots,\bsx_T)$}%, where $T\triangleq\left\lfloor \eps/\delta \right\rfloor$.

Define $h^0\equiv 0$ and set $F \ot \{(h^0,1)\}$.

\For{$t=1,...,T$}{
Let $h_t \leftarrow \Aa(\bsx_t)$, and set $F=F\cup\left\{\left(h_t,t\right)\right\}$}

Sample $(h^*,t^*)$ from $F$ with probability proportional to $\exp\left(\frac{\eps}{2} \left(e^{-2\eps} \cdot h^*(\Dd)-h^*(\bsx_{t^*})\right)\right)$.

\Return{$(h^*, t^*).$}
\end{algorithm2e}


Observe that $\Bb$ only accesses its input through $\Aa$ (which is $(\eps,\delta)$-differentially private) and the exponential mechanism (which is $(\eps,0)$-differentially private). Thus, by composition and post-processing, $\Bb$ is $(2\eps,\delta)$-differentially private. 
%
Now consider applying $\Bb$ on databases $\vec{\bsx} = (\bsx_1,\dots,\bsx_T)$ containing i.i.d.\ samples from $\Dd$. By our assumption on $\Aa$, for every $t$ we have that 
$e^{-2\eps} \cdot h_t(\Dd)-h_t(\bsx_t)\geq  \frac{4}{\eps} \log(T+1) + 2Tn\delta$
 with probability at least $1/T$. We therefore get
$$\Pr_{\substack{\vec{\bsx}\sim\Dd \\ \Bb\left(\vec{\bsx}\right)}}\left[{\max_{t \in [T]} \left\{ e^{-2\eps} \cdot h_t(\Dd)-h_t(\bsx_t) \right\} \geq \frac{4}{\eps} \log(T+1) + 2Tn\delta  }\right] \geq 1 - \left( 1 - 1/T \right)^T \geq \frac12.$$
The probability is taken over the random choice of
the examples in $\vec{\bsx}$ according to $\Dd$ and the generation of the predicates $h_t$ according to $\Bb(\vec{\bsx})$.
Thus, by Markov's inequality,
$$
\E_{\substack{\vec{\bsx}\sim\Dd \\ \Bb\left(\vec{\bsx}\right)}}\left[{\max\{0,\max_{t \in [T]}  \left\{ e^{-2\eps} \cdot h_t(\Dd)-h_t(\bsx_t) \right\} }\right] \geq \frac{2}{\eps} \log(T+1) + Tn\delta.
$$
Recall that the set $F$ (constructed in step~2 of algorithm $\Bb$) contains the predicate $h^0\equiv0$, and hence,
\begin{equation}\label{eq:LargeError}
\E_{\substack{\vec{\bsx}\sim\Dd \\ \Bb\left(\vec{\bsx}\right)}}\left[\max_{(h,t) \in F} \left\{ e^{-2\eps} \cdot h_t(\Dd)-h_t(\bsx_t) \right\}\right] =\E_{\substack{\vec{\bsx}\sim\Dd \\ \Bb\left(\vec{\bsx}\right)}}\left[{\max\{0,\max_{t \in [T]}  \left\{ e^{-2\eps} \cdot h_t(\Dd)-h_t(\bsx_t) \right\} }\right] \geq \frac{2}{\eps} \log(T+1) + Tn\delta.
\end{equation}

So, in expectation, the set $F$ contains a pair $(h,t)$ with large difference $e^{-2\eps} \cdot h(\Dd)-h(\bsx_t)$. In order to contradict the expectation bound of Lemma~\ref{lem:MKLCondExpSQ}, we need to show that this is also the case for the pair $(h^*,t^*)$ that is sampled in Step~3. Indeed, by the properties of the exponential mechanism, we have that
\begin{equation}
\E_{(h^*,t^*)\in_R F}\Big[ e^{-2\eps} \cdot h^*(\Dd)- h^*(\bsx_{t^*})   \Big] \geq \max_{(h,t)\in F} \{ e^{-2\eps} \cdot h(\Dd) - h(\bsx_{t}) \} - \frac{2}{\eps} \log(T+1). \label{eq:Utility}
\end{equation}
Taking the expectation also over $\vec{\bsx}\sim\Dd$ and $\Bb(\vec{\bsx})$ we get that
\begin{eqnarray*}
\E_{\substack{\vec{\bsx}\sim\Dd \\ \Bb\left(\vec{\bsx}\right)}}\Big[  e^{-2\eps} \cdot h^*(\Dd) - h^*(\bsx_{t^*})  \Big] 
&\geq& \E_{\substack{\vec{\bsx}\sim\Dd \\ \Bb\left(\vec{\bsx}\right)}}\Big[\max_{(h,t)\in F} \{ e^{-2\eps} \cdot h(\Dd) - h(\bsx_{t}) \} \Big] - \frac{2}{\eps} \log(T+1)\\
&\geq& \frac{2}{\eps} \log(T+1) + Tn\delta - \frac{2}{\eps} \log(T+1) = Tn\delta.
\end{eqnarray*}
This contradicts Lemma~\ref{lem:MKLCondExpSQ}.
\end{proof}

% \ignore{
% \begin{theorem}[Generalization property of DP \cite{DworkFHPRR15,BassilyNSSSU:sicomp2021,FeldmanS17}] \label{thm:DP-generalization}
% Let $\mathcal{A}:X^n \to 2^{X}$ be an $(\eps, \delta)$-differentially private algorithm that operates on a dataset of size $n$ and outputs a predicate $h:X\to \{0,1\}$. Let $\Dd$ be a distribution over $X$, let $\boldsymbol{x}=(x_1,\ldots,x_n) \sim \Dd^n$ be i.i.d.\ samples from $\Dd$, and let $h\gets \mathcal{A}(\boldsymbol{x})$. Then for any $T\ge 1$ it holds that \eccomment{Other direction holds as well. Add after we converge on what we need and in what form.}
% {\small
% \[
% \Pr_{\boldsymbol{x}\sim \Dd^d,\atop h\gets \mathcal{A}(\boldsymbol{x})}\left[ e^{-2\eps} \E_{y\sim \Dd} h(y) - \frac{1}{n} \sum_{i\in [n]} h(x_i )> \frac{4}{\eps n}\log(T+1) + 2T\delta \right] < \frac{1}{T}.
% \]}
% \end{theorem}
% }

\section{Analysis of the tracking estimator} \label{trackinganalysis:sec}

Here we will prove \cref{thm:main-tracking}:

\trackinganalysis*

The analysis is analogous to \cref{sec:analysis-basic}; indeed, we will reuse most of the results from that section. We may make the same assumptions on $k, r, \al$ as at the start of \cref{sec:analysis-basic}. Again, we begin with an analog of \cref{lemma:sub-h-hat}:

\begin{lemma} \label{lemma:sub-h-hat-tracking}
If $\tw h = \sum_{(i,\rho_i)\in S} \ind{(\rho_i < \tau)\land (C[i]<r)}+\Lap(1/\eps_0)$ is replaced by $\hat h = \sum_{i \in V} \ind{(\rho_i < \tau)\land (C[i]<r)}+\Lap(1/\eps_0)$, then the outputs of \cref{bottomkrobustbaseline:algo} change with probability at most $\beta/4$.
\end{lemma}
\begin{proof}
In order for $\tw h$ not to equal $\hat h$, the maximum value of $\rho_i$ in $S$ must be below $\tau$. However, in that case, by assumption, at most $k/2$ of these elements may be inactive, so the sum in $\tw h$ is at least $k/2$. In order for the output to change in any given step, we must then have $\tw h < T < \hat h$. However, this would require $\Lap(1/\e_0) < -k/4$, which has probability at most $e^{-\e_0 k/4} < m/\beta$. By a union bound (as in the proof of \cref{lemma:sub-h-hat}), we are done.
\end{proof}

Again, we assume henceforth that \cref{bottomkrobustbaseline:algo} uses $\hat h$ instead of $\tw h$. We now show once again that \cref{bottomkrobustbaseline:algo} can be simulated by calls to \cref{algo:svt-individual}, with the same function $h_{V, \tau}$ as in \cref{sec:analysis-basic}. Indeed, the only difference from \cref{algo:svt-individual} is that $C$ can only increment elements of the sketch $S$ rather than the whole set $V$, so we need to ensure that there are never keys $i \in V$ such that $\rho_i < \tau$ and $i \notin S$. We show this, along with the analogs of \cref{prop:low-guarantee} and \cref{prop:high-guarantee}, by induction:

\begin{claim} \label{claim:consistent-induct}
The following holds with probability at least $1-\beta/2$. Let $d$ be the number of deactivated elements in the sketch $S$. Then, whenever $\tau < (1 - \al/4) T/|V|$, the while loop in \cref{bottomkrobustbaseline:algo} continues to the next value of $\tau$, and whenever $\tau > ((1 + \al/4) T + d) / |V|$, it terminates. Moreover, the values in $C$ always match the values that \cref{algo:svt-individual} would have.
\end{claim}
\begin{proof}
We proceed by induction; suppose the statement has held true on all previous inputs and iterations of the while loop. We show that it holds on the current iteration --- note that we must have $\tau < (1 + 3\al/8) (T + d) / |V|$ by the inductive hypothesis, since otherwise the loop would have terminated in the previous step. Recall by assumption that $d < k/2$, and we set $T = k/4$, so this means that $\tau < \f78 \cdot k/|V|$.

We first show that on the current input, the keys $i \in V$ with $\rho_i < \tau$ are all in the sketch $S$. Indeed, by the inductive hypothesis, we may apply \cref{generror:lemma} on $h_{V, \tau}$:

\begin{align*}
    |\tau |V| - h_{V, \tau}(\bsr)| &< \f{\al}{32} \cdot \tau |V| + O(\log(m/\beta)/\al) \\
    &< \f{\al}{32} \cdot \tau |V| + \f{\al k}{8},
\end{align*}

Since $\tau |V| < 7k/8$, this means that we have $h_{V, \tau}(\bsr) < k$. However, $h_{V, \tau}(\bsr)$ is just the count of $i \in V$ such that $\rho_i < \tau$, so if this count is less than $k$, then all such $i \in V$ are included in the sketch (since it is a bottom-$k$ sketch).

Therefore, we have shown the second part of \cref{claim:consistent-induct}, since every value that would need to be incremented is actually in the sketch $C$. It remains to show the first part.

We now apply \cref{totalerror:coro} (again using the inductive hypothesis that our algorithm has matched \cref{algo:svt-individual}), to obtain that
\begin{gather}
\hat h < (1 + \al/16) \tau |V| + \al k / 16 + d, \label{eq:hat-ub-2} \\
\hat h > (1 - \al/16) \tau |V| - \al k / 16, \label{eq:hat-lb-2}
\end{gather}
where again, the value of $\D$ is at most $\al k / 16$ by the choice of $k$, and the sum in \cref{totalerror:coro} is bounded by the number of deactivated elements satisfying $h_{V,\tau}(i, \rho_i)=1$, which is at most $d$ (since we just showed that all such elements are in $S$). The remainder of this proof is now identical to that of \cref{prop:low-guarantee} and \cref{prop:high-guarantee}.
\end{proof}

From \cref{claim:consistent-induct}, we deduce (identically to the previous analysis) that whenever $d < \al k / 4$, the output of the algorithm is a $(1+\al)$-approximation of $|V|$.


\end{document}





\section{Related links}



Combine 
multiplicative statement:
\url{https://arxiv.org/pdf/1706.05069}
Theorem 4.1

With the additive statement for product dist:
Adaptive Data Analysis with Correlated Observations
\url{https://arxiv.org/pdf/2201.08704}
Lemma 3.1

Additive bound for generalization \url{https://arxiv.org/pdf/1511.02513}

Generalization guarantees for standard deviation
\url{https://vtaly.net/papers/FS_KLAS.1217.pdf}
\url{https://arxiv.org/abs/1706.05069}



\end{document}