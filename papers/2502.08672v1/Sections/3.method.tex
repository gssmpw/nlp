\section{Methodology}
\subsection{Data acquisition}
The dataset in this study was obtained from the UC Irvine ML repository, developed by Athanasios Tsanas and Max Little of Oxford University\cite{6}. The dataset comprises a variety of biological speech measures obtained from a cohort of 42 individuals diagnosed with early-stage PD. These participants were included in a six-month clinical experiment evaluating the efficacy of telemonitoring equipment for remote monitoring of symptom development. The audio recordings were taken at the patients' residences using an automated process. The table comprises several columns that include subject number, subject age, subject gender, time interval from baseline recruitment date, motor UPDRS, total UPDRS, and 16 biomedical voice measurements. Each row in the dataset represents one of the 5,875 voice recordings obtained from the persons under study. The primary objective of the data analysis is to utilize the 16 voice measurements in order to forecast the motor and total UPDRS scores, denoted as 'motor\_UPDRS' and 'total\_UPDRS,' respectively. The data is formatted as ASCII CSV. Each row within the CSV file represents an individual instance that corresponds to a voice recording. 


\subsection{Feature Selection with RFE}

In our Parkinson's UPDRS estimation dataset, we are inundated with a multitude of features, ranging from demographic information to clinical metrics. While all these features provide a holistic overview, not all might be directly influential in predicting UPDRS scores.

Recursive Feature Elimination (RFE)\cite{8} offers a systematic approach for eliminating duplicate or irrelevant features, refining our dataset to include the most essential qualities. When provided with a dataset:

\begin{equation}
    \mathcal{D} = \{(x_1, y_1), (x_2, y_2), \dots, (x_N, y_N)\}
\end{equation}


where $x_i \in \mathbb{R}^d$ and $y_i \in \mathbb{R}$, RFE operates by:

\begin{itemize}
    \item Train the estimator on $\mathcal{D}$.
    \item Rank the features based on importance, obtained from the estimator.
    \item Remove the feature with the least importance.
    \item Repeat until only $k$ features remain.
\end{itemize}

In the context of our project, we employ the RandomForestRegressor as our external estimator. This choice is motivated by the ability of Random Forests (RF)\cite{7} to offer an intrinsic feature ranking based on the average depth at which a feature is used to split the data across all trees.

\subsection{Data Augmentation via Jittering}

Due to the strict procedure of data gathering and the need to address privacy issues, medical datasets frequently exhibit limitations in terms of their size. The scarcity of data might potentially result in the problem of overfitting. Data augmentation techniques, like as jittering\cite{9}, can successfully address this issue by artificially increasing the size of the dataset.

Jittering, in essence, introduces slight, controlled perturbations to the data, simulating potential real-world variations. For a feature matrix $X \in \mathbb{R}^{N \times d}$, the jittered version is:

\begin{equation}
    X'_{ij} = X_{ij} + \epsilon_{ij}
\end{equation}


where $\epsilon_{ij} \sim \mathcal{N}(0, \sigma^2)$. Here, $\mathcal{N}$ represents a Gaussian distribution. By adding this Gaussian noise, we are mimicking the natural variability that might be seen in repeated measurements from Parkinson's patients, thus augmenting our dataset without deviating from potential real-world scenarios.

\subsection{Reshaping Data for LSTM}

In our project, we aspire to harness the temporal dependencies in the data, which is why we incorporate LSTMs. These networks require data to be structured in a specific format, encapsulating samples, sequence lengths, and features.

Considering a dataset with $N$ samples, each being a sequence of length $T$ and constituted of $d$ features, the data must be transformed to a tensor format as:

\begin{equation}
    X'' \in \mathbb{R}^{N \times T \times d}
\end{equation}



This reshaping ensures that the LSTM can effectively parse and process the sequences, offering insights into the temporal progression of Parkinson's Disease in patients and subsequently aiding in UPDRS estimation.


%%%%%%%%%%%%% Section

\subsection{Neural Network Design and Model Training}

The architecture of the neural network is meticulously designed to adapt to the temporal and sequential characteristics of the dataset. Several layers and techniques are interwoven to optimize the prediction of UPDRS scores.

\textbf{Input Layer}: The network initiates with an input layer tailored to receive sequences of shape $d \times 1$, where $d$ represents the number of selected features.

\textbf{Bidirectional LSTM Layer}: This layer, equipped with 100 LSTM units, captures both forward and backward dependencies within the sequential data. LSTM units excel at recognizing patterns over long sequences. They are governed by the following equations:

\begin{equation}
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
\end{equation}
\begin{equation}
i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
\end{equation}
\begin{equation}
\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)
\end{equation}
\begin{equation}
C_t = f_t \times C_{t-1} + i_t \times \tilde{C}_t
\end{equation}
\begin{equation}
o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
\end{equation}
\begin{equation}
h_t = o_t \times \tanh(C_t)
\end{equation}

Where:
\begin{itemize}
\item $f_t$, $i_t$, $o_t$ are the forget, input, and output gates, respectively.
\item $x_t$ is the input at time step $t$.
\item $h_{t-1}$ is the previous hidden state.
\item $W$ and $b$ denote weight matrices and biases respectively.
\item $\sigma$ is the sigmoid function.
\end{itemize}

\textbf{Attention Mechanism}: This layer allows the model to focus on different segments of the sequence, assigning varying attention depending on their relevance. Given a set of LSTM outputs $H = \{h_1, h_2, \dots, h_T\}$, where $T$ is the sequence length, the attention mechanism determines the attention weights $\alpha_t$ using:

\begin{equation}
\alpha_t = \frac{\exp(\text{score}(h_t, \text{context}))}{\sum_{j=1}^{T} \exp(\text{score}(h_j, \text{context}))}
\end{equation}

Here, the score function quantifies the relationship between the LSTM output $h_t$ and a context vector. The aggregated output, represented by the context vector, is:

\begin{equation}
\text{context\_vector} = \sum_{t=1}^{T} \alpha_t h_t
\end{equation}

\textbf{Dense, Batch Normalization, and Dropout Layers}: After processing through the attention mechanism, the network directs the context vector through dense layers featuring ReLU activations and L2 regularization. Batch normalization offers stability during learning. Dropout layers, with a 0.3 rate, prevent overfitting by occasionally turning off certain neurons in training.

\textbf{Output Layer}: This network culminates in an output layer, consisting of a single neuron, responsible for predicting the 'total\_UPDRS' score.

Regarding training, the \textbf{Learning Rate} undergoes dynamic adjustments through an exponential decay schedule. Commencing at 0.001, it decays by 0.9 every 10,000 steps.

For broader applicability, training employs a \textbf{5-fold Cross-Validation} method. The dataset divides into five segments, with each segment acting as a validation set once. Prior to training, data in every fold undergoes standardization. To boost training efficiency, the early stopping mechanism is integrated, focusing on validation loss.

