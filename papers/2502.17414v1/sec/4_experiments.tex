\section{Experiments}

\begin{table*}[t]
    \centering
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \caption{Quantitative comparison on motion generation.}
        \vspace{-2mm}
        \begin{tabular}{lccc}
            \toprule
            \multirow{2}{*}{\textbf{Metrics}} & \multicolumn{3}{c}{\textbf{AIST++ Dataset / In-House Dataset}} \\
            \cmidrule(lr){2-4}
            & FVD $\downarrow$ & DIV $\uparrow$ & BAS $\uparrow$ \\
            \midrule
            Ground Truth & 509.58/129.75 & 34.10/29.67 & 0.24/0.22 \\
            Hallo~\cite{hallo} & 548.81/249.12 & \textbf{28.66/28.98} & 0.16/0.20 \\
            Bailando~\cite{bailando} & 621.22/534.02 & 22.34/24.05 & 0.19/0.19 \\
            EDGE~\cite{edge} & 639.46/303.36 & 24.87/27.29 & \textbf{0.26/0.24} \\
            \midrule
            \papername & \textbf{531.52/238.22} & \underline{25.61/28.08} & \underline{0.23/0.21} \\
            \bottomrule
        \end{tabular}
        \label{tab:motiongen}
    \end{minipage}%
    \hspace{0.02\textwidth} % Adjust the spacing between the two tables
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \caption{Quantitative comparison on visual synthesis.}
        \vspace{-2mm}
        \begin{tabular}{lccc}
            \toprule
            \multirow{2}{*}{\textbf{Metrics}} & \multicolumn{3}{c}{\textbf{In-House Dataset}} \\
            \cmidrule(lr){2-4}
            & FVD $\downarrow$ & FID-VID $\downarrow$ & ID-SIM $\uparrow$ \\
            \midrule
            Hallo~\cite{hallo} & 609.08 & 76.99 & 0.4870 \\
            Bailando~\cite{bailando} + PG & 583.26 & 100.02 & 0.3392 \\
            EDGE~\cite{edge} + PG & 613.81 & 93.73 & 0.3034\\
            Our motion + PG & 735.05 & 72.71 & 0.4894 \\
            \midrule
            \papername & \textbf{507.06} & \textbf{61.94} & \textbf{0.5317} \\
            \bottomrule
        \end{tabular}
        \label{tab:dancegen}
    \end{minipage}
    \vspace{-4mm}
\end{table*}

\subsection{Implementation Details}
\noindent\textbf{Dataset.} Our model is trained on a curated visual-audio dataset comprising 107,546 monocular recordings of human dance performances in both indoor and outdoor settings, with an average clip duration of 30 seconds. Each video is cropped to a resolution of $896\times512$ and resampled to $30$ fps, encompassing a broad spectrum of half- to full-body dances across diverse music genres and showcasing individuals with varied body shapes and appearances.

\noindent\textbf{Training.} We train our full pipeline in three stages. First, we train a multi-part pose VQ-VAE to encode and quantize 60 joint coordinates, with keypoint confidence scores, into 5-part pose tokens. The pose of each body part uses 6 tokens, each with a unique 512-entry codebook of 6-dimensional embeddings. This VQ-VAE is trained for 40k steps with a batch size of 2048 at a learning rate of $2\times10^{-4}$.  Next, we train an autoregressive model for pose token prediction with 300k steps, initialized with pretrained GPT-2 model weights, using a batch size of 24 and a learning rate of $1\times10^{-4}$. The model is trained with 64-frame pose sequences with a context window of 2224 tokens in total, enabling extended dance motion generation via sliding segments during inference. In the final diffusion stage, we fine-tune the denoising UNet and ReferenceNet with two randomly selected frames from a video, followed by co-training the motion guidance decoder and the temporal module with diffusion losses on consecutive 16 frames. We train the diffusion stage with 90k steps at a learning rate of $1\times10^{-5}$ and a batch size of 16. All stages are trained with 8 A100 GPUs using the AdamW optimizer~\cite{yao2021adahessian}. 

\noindent\textbf{Inference.} For inference, we initiate our autoregressive dance motion generation from pose tokens encoded from the reference image pose, maintaining skeleton consistency with the specified body shape. Extended dance sequences are generated in 64-frame sliding segments with a 12-frame overlap, while 8 uniformly sampled frames from previous generated motions serve as global motion context. Using the full generated pose token sequences, we synthesize all video frames simultaneously, applying prompt traveling~\cite{tseng2022edge} to improve temporal smoothness.

\begin{table}
    \centering
    \caption{Ablation on our transformer model designs.}
    \vspace{-2mm}
    \begin{tabular}{lccc}
        \toprule
        \cmidrule(lr){2-3}
        & FVD $\downarrow$ & DIV $\uparrow$ & BAS $\uparrow$ \\
        \midrule
        w/o global music context & 265.73 & 27.04 & 0.2142 \\
        w/o global motion context & 247.54 & 26.42 & 0.2154 \\
        sub-dataset + GPT-medium & 402.63 & 24.40 & 0.2112 \\
        sub-dataset + GPT-small & 332.93 & 24.58 & 0.2046 \\
        \midrule
        \papername & \textbf{238.22} & \textbf{28.08} & \textbf{0.2182} \\
        \bottomrule
    \end{tabular}
    \vspace{-4mm}
    \label{tab:ablate}
\end{table}

\subsection{Evaluations and Comparisons}
To the best of our knowledge, no existing approach addresses music-driven 2D video generation from a single human image. For baseline comparisons, we adapted and combined established models to create two baselines. First, we adapted the audio-driven, diffusion-based portrait animation model Hallo~\cite{hallo}, retraining it for our task by substituting its audio features with our music embeddings to animate human images via cross-attention modules. For the second baseline, we utilize 3D music-to-dance generation models like Bailando~\cite{bailando} and EDGE~\cite{edge} for motion synthesis, projecting their outputs into 2D skeleton maps, which are then fed into a diffusion model with a pose guider for controlled image animation. We note that during our submission, the AIST dataset~\cite{aist-dance-db} was unavailable due to maintenance, and we were unable to train our model on the same dataset as Bailando~\cite{bailando} and EDGE~\cite{edge}. We conduct separate evaluations of all models on the test set of AIST++~\cite{aist++} (40 videos) as well as on a test split of our curated music-video dataset (230 videos).



\noindent \textbf{Quantitative Evaluation.} We numerically compare our method with baselines in terms of quality of both motion generation and video synthesis. Specifically, we calculate the Fr√©chet Video Distance (FVD)~\cite{unterthiner2018towards,ge2024content} between generated dance motions and ground-truth training sequences for assessment of motion fidelity. 
For motion diversity, we compute the distributional spread of generated pose features (DIV)~\cite{aist++,bailando}, for which the generated motion should aim to match the scores of the ground truth distribution rather than maximize their absolute values.  To numerically evaluate the alignment between the music and generated dancing poses, we follow~\cite{aist++,bailando} to measure the Beat Align Score (BAS) by the average temporal distance between each music beat and its closest dancing beat. These evaluations are conducted in 2D pose space, where we project the 3D skeleton motion of Bailando and EDGE into 2D and detect poses over synthesized videos from the retrained Hallo model. As shown in Tab.~\ref{tab:motiongen}, our method surpasses all baselines in terms of motion fidelity (FVD), while achieving the second-best music beat alignment (BAS) and diversity (DIV). Notably, EDGE is trained on sequences from professional dancers, whereas our dataset comprises videos of everyday individuals, reflected in the beat alignment score of the ground-truth videos. Despite this, our method significantly outperforms Bailando trained on AIST++ and Hallo trained on our curated dataset for beat alignment. Hallo entangles motion generation and video synthesis, and achieves a higher DIV score than~\papername~primarily due to its extremely noisy video outputs which results in jittering and chaotic skeleton motions following pose projection. 

\begin{figure}[t]
\centering
\includegraphics[width=0.98\linewidth]{figures/subject3.pdf}
\vspace{-2mm}
\caption{Human image animation after finetuning motion transformer with 30 dance videos of Subject Three. }
\vspace{-5mm}
\label{fig:characterized}
\end{figure}

For evaluation of video synthesis fidelity, we measure the FVD and FID-VID~\cite{balaji2019conditional} score between the ground-truth and generated dance videos. Additionally, we assess identity preservation using the ArcFace score~\cite{deng2019arcface}, which measures the cosine similarity of face identity features (ID-SIM). All metrics are evaluated on our test video dataset. As an extra baseline, we replace the motion generator in EDGE and Bailando with our trained transformer model and generate the animation using a pose guider. As shown in Tab.~\ref{tab:dancegen}, our method achieves the highest visual quality and identity preservation, which we attribute to our disentangled design of motion generation and video synthesis (compared to Hallo) and the use of an implicit motion token decoder rather than an explicit pose guider.

\noindent \textbf{Qualitative Comparisons.}
We present visual comparisons between our method and the baselines in Fig.~\ref{fig:baselines}. For more dynamic comparisons and user study on perceptual quality, please refer to our supplementary material. The modified Hallo~\cite{hallo} represents an end-to-end diffusion pipeline that directly synthesizes the final video without intermediate motion generation. However, it exhibits noticeable artifacts, particularly in large articulated body motions, and often fails to preserve the human body's articulation topology.  Bilando~\cite{bailando} and EDGE~\cite{edge} generate body motion in 3D space without considering the scene context or the human shape in the reference image. Despite post-processing for skeleton alignment in the pose guider input, these methods still struggle with significant identity and shape distortions, often producing unnatural interactions with the background scene. Furthermore, they do not model head and hand motions, leading to more rigid and less expressive dance movements compared to~\papername.


\noindent \textbf{Finetuning for Characterized Choreography.} While our method operates as a zero-shot pipeline, generalizing seamlessly to new reference images and music inputs, it can also be fine-tuned for characterized choreography using only a few sample dance videos. This adaptability is challenging for 3D motion generation models like EDGE~\cite{edge} and Bailando~\cite{bailando}, which require extensive effort in creating 3D dance movements. As shown in Fig.~\ref{fig:characterized} and our supplementary video, our method successfully captures and mimics the specific choreography after fine-tuning with only 30 dance videos from diverse performers, showcasing its efficiency and versatility in adapting to specific dance styles. 


\subsection{Ablation Study}
We conduct ablation studies by systematically removing individual design components from our full training pipeline. First, we assess the effectiveness of the multi-part VQ-VAE in capturing subtle and expressive human movements. Compared to a single-part whole-body VQ-VAE, our approach achieves a significantly lower L1 reconstruction error per joint (0.5px vs. 0.83 px), with pronounced improvements in the hands (0.4px vs. 0.64px) and head (0.42px vs. 0.52px). This improvement translates into superior motion diversity, expressiveness, and control precision in the diffusion model, as shown through quantitative metrics and visual examples in our supplementary material.
Next, we assess the impact of global music and motion context on motion generation. As presented in Tab.~\ref{tab:ablate}, both contexts contribute to producing more consistent, plausible, and music-synchronized motions. We further analyze the benefits of 2D motion modeling and transformer-based autoregressive generation by scaling both model parameters and dataset size. Across all metrics, we observe significant performance gains (Tab.~\ref{tab:ablate}) as the number of training parameters increases from 117M (GPT-small) to 345M (GPT-medium) and data scale from 10k to 100k videos, underscoring the scalability potential of monocular dance video data with our architecture, sheding light on further performance improvements as they scale. In Tab.~\ref{tab:dancegen}, we compare our token-based motion guidance with AdaIN against a pose guider with explicitly decoded skeleton map. Our method demonstrates reduced motion jittering and enhanced temporal consistency due to the temporal motion context, and showcases superior identity and body shape preservation, as also visually shown in our supplementary video. 