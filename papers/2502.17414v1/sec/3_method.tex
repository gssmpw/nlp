\begin{figure*}[bt!]
\centering
\vspace{-2mm}
\includegraphics[width=0.96\textwidth]{figures/full_pipeline_copy.pdf}
\caption{\textbf{Overview of~\papername.} 
We propose a cross-conditional transformer model to autoregressively generate 2D human poses synchronized with input music, followed by a diffusion model that produces high-fidelity videos from a single reference image $I_R.$ First, we develop a multi-part compositional tokenization for 2D poses, encoding and quantizing each body part independently with keypoint confidence. These tokens are then merged into a whole-body, confidence-aware pose using a shared decoder. Next, we train a GPT-based transformer to autoregressively predict future pose tokens with causal attention, conditioned on past poses and aligned music embeddings. For global music style and motion context, we incorporate the entire music sequence and sampled prior poses. With a learnable motion decoder, we generate multi-scale spatial pose guidance upsampled from a learned feature map, incorporating the generated motion tokens within a temporal window (16 frames) using AdaIN. By co-training the motion decoder and temporal modules, our diffusion model is capable of synthesizing temporally smooth and high-fidelity video frames, while maintaining consistent appearance with the reference image with a trained reference net. }
\vspace{-3mm}
\label{fig:overview1}
\end{figure*}

\section{Method}
Given a single portrait as the reference image $I_R$ and a conditioning music sequence represented as ${M_t},$ our objective is to generate a sequence of dance frames ${I_{M_t}}$, where $t=1,…, T$ denotes the frame index. The generated sequence ${I_{M_t}}$ seeks to maintain the appearance and background context depicted in $I_R$ while present expressive dance movements in harmony with the provided musical rhythm and beats. As illustrated in Fig.~\ref{fig:overview1}, our model is trained in two stages: transformer-based 2D human dance motion generation (Section.~\ref{sec:trans}), and diffusion-based video synthesis (Section.~\ref{sec:diff}) from the generated motion sequence. 


\subsection{Data Representations} 
\paragraph{Generative Visual Representations.} To generate human dance videos, we employ Latent Diffusion Models~\cite{rombach2022high} that synthesize
samples in the image latent space, facilitated by a pretrained image auto-encoder.  During training, latent representations of images are progressively corrupted with Gaussian noise $\epsilon,$ following the Denoising Diffusion Probabilistic Model (DDPM) framework~\cite{ho2020denoising,song2020score}. A UNet-based denoising framework, enhanced with spatial and temporal attention blocks, is then trained to learn the reverse denoising process. 

For training, we apply human-centric cropping to the videos, yielding a unified resolution of $896\times512,$ that encompasses half-body to full-body dances. Instead of directly capturing the intricate pixel-wise movements on music conditions, we first establish the correlation between music and human dance movement, which subsequently guides the final visual synthesis. 
\vspace{-4mm}
\paragraph{Motion Modeling.} In contrast to prior approaches that generate human motions in 3D space, we represent diverse dance motions as 2D pose sequences. Compared to its 3D counterparts, 2D human dance motions are widely accessible from large collections of monocular videos, eliminating the need for complex multiview capture setups or labor-intensive 3D animations. Furthermore, the detection of 2D poses is significantly more reliable and robust. To enhance the realism and motion expressiveness, 
we model not only large body articulations but also finer details such as head movements and hand gestures, capturing subtle nuances in motion. Notably, we incorporate keypoint detection confidence into our pose representation, allowing the model to account for motion blur and occlusions.  Each per-frame whole-body pose with $J$ joints is thus represented as $p \in \mathcal{R}^{J\times 3},$ where the last dimension encodes the keypoint confidence. 

\vspace{-4mm}
\paragraph{Music Embedding.}  Inspired from~\cite{edge}, we utilize the pre-trained Jukebox model~\cite{dhariwal2020jukebox} to extract rich musical features, supplemented by rhythmic information with one-hot encoding of music beats using an audio processing toolbox Librosa~\cite{jin2017towards}.  We resample and synchronize these extracted embeddings—denoted as $F^{J}_{1:T}$ for Jukebox and $F^{L}_{1:T}$ for Librosa—to the video frame rate, ensuring per-frame alignment between the music and visual elements. 


\subsection{Transformer-Based Music-to-Dance Motion Generation}
\label{sec:trans}

Given a collection of monocular, single-person, music-synchronized dance videos with paired 2D whole-body pose detections, we aim to model the intricate, non-linear correlation between skeletal dance motion and musical features. To achieve this, we first introduce a compositional, confidence-aware VQ-VAE, which captures diverse and nuanced human dance poses across different body parts. Next, we leverage a GPT-like transformer to autoregressively predict future pose tokens, modeling temporal motion transitions within the token space and aligning them with synchronized music embeddings.
\\
\noindent\textbf{Compositional Confidence-Aware Human Pose Tokenization.}  Our approach builds on the standard VQ-VAE framework, trained in a self-supervised manner. Given whole-body 2D poses with associated keypoint confidences $p$,  
an 1D convolutional encoder maps $p$ into a latent pose embedding $z_e(p) = E(p),$ which is then quantized by mapping to its nearest representation $z_q(p)$ within a learnable codebook,  and finally the decoder $D$ reconstructs the pose $\hat{p}$ from $z_q(p).$ However, as prior studies~\cite{bailando,yi2023generating,wang2024holistic} suggest, the dependencies between spatial keypoints are complex, and a vanilla VQ-VAE often struggles to capture subtle pose details, such as finger movements and head tilts, due to information loss during quantization and multi-frequency nature of pose variations across different body parts. 

To improve expressive coverage, we train independent 2D pose encoders $E^j$ and learn $B=5$ separate codebooks $Z^j_q$ for upper and lower half bodies, left and right hands, and head respectively,  allowing the model to spatially decompose 2D whole-body pose variations across different frequency levels. By partitioning poses this way, distinct body-part codes can be flexibly combined, enhancing the range of expressiveness represented within each individual codebook. To capture part-wise spatial correlations and ensure information flow across body parts, we concatenate the quantized pose latents and feed them into a shared decoder. The resulting embedding is then mapped to reconstructed keypoint coordinates through separate projection heads, enabling joint reconstruction while preserving nuanced part-specific details.

We train encoder and decoder simultaneously with the compositional codebooks with the following loss function:
\vspace{-3mm}
\begin{equation}
    \mathcal{L}_{\text{VQ}} = \sum_{j=1}^B \| \hat{p}^j - p^j ||_2 + \beta \sum_{j=1}^B \|\mathrm{sg} \left[z^j_e(p)\right] -  z^j_q(p) \|_2,
\vspace{-2mm}
\end{equation}
where $\mathrm{sg}$ is a stop gradient operation, and the second term is a commitment loss with a trade-off $\beta.$ We utilize exponential moving average (EMA)~\cite{vqvae} to update codebook vectors and remove the embedding loss $\sum_{j=1}^B \|z^j_e(p) - \mathrm{sg} \left[z^j_q(p)\right] \|_2$ for more stable training. 

\vspace{2mm}
\noindent\textbf{Cross-Conditional Autoregressive Motion Modeling.}
After training the compositional quantized multi-part codebooks, 2D human poses detected in our training videos can be represented as sequences of codebook tokens via encoding and quantization. Specifically, each detected pose is mapped to a sequence of corresponding codebook indices, structured as one-hot vectors indicating the nearest codebook entry for each element. We denote this as $C^j_{1:T} = ((c^j_{1, 1}, …, c^j_{K, 1}), \ldots, (c^j_{1, T},\ldots, c^j_{K, T}))$, 
% where $K$ is the number of tokens for the single-frame pose of body part $j$.
where $K$ is the number of tokens per body part $j$ in each frame.

With this quantized motion representation, we design a temporal autoregressive model to predict the multinomial distributions of possible subsequent poses for each body part, conditioned on both the input music embeddings $F^{J}_{1:T}$ and $F^{L}_{1:T}$ and the preceding motion tokens. Our motion generation transformer is conditioned on the music inputs in two ways. First, we combine the Jukebox and Librosa music embeddings to form starting tokens, which serves as a global music context, such as styles and  genres, that informs the entire motion sequence generation. Second,  inspired by~\cite{bailando}, we concatenate the frame-wise projected music embeddings with the corresponding motion tokens as inputs to the transformer model, ensuring precise synchronization between the motion and music features. This dual conditioning allows our model to produce both globally coherent and locally synchronized dance movements. We use $T=64$ frames for our autoregressive model training. To handle extended motion sequence generation with consistent motion styles and smooth transitions, we additionally incorporate cross-segment motion context into the transformer model. Specifically, we uniformly sample a subset of 8 frames from the previous motion segment as the motion context, and append them after the global music context inputs. We denote the combined context as $F_g.$

Since we model body parts independently, maintaining coherence in the assembled whole-body poses is essential to avoid asynchronous movements (e.g., upper and lower body moving in different directions). To address this, we leverage mutual information across multi-part motions, designing our model with cross-conditioning across body parts. Specifically, we employ a GPT model to estimate the joint distributions of $C^j_{1:T}$ as follows,
\vspace{-3mm}
\begin{gather}
\phi(C^{1:B}_{1:T} | F_g) = \prod_{t=1}^{T}\prod_{j=1}^{B} \prod_{k=1}^{K} \nonumber\\ \phi\bigl (c^j_{k,t} | c^{1:B}_{1:K, <t}, c^{<j}_{1:K, t}, c^{j}_{<k, t}, F^{J}_{\leq t}, F^{L}_{\leq t}, F_g \bigr )
\vspace{-2mm}
\end{gather}

We structure the cross-conditioning between body parts in two ways: (1) the current motion token is conditioned on all preceding motion information from all body parts, ensuring inter-part temporal coherence; (2) by ordering body parts as upper and lower body, head, and hands, we build the hierarchical dependencies from primary components (upper/lower body) to finer, high-frequency movements (head and hands). Since each body part’s pose is represented with a small set of tokens, we empirically observe that causal attention is sufficient to model the next-token distribution within each part. This modeling strategy preserves motion coherence of each body part as a whole, producing expressive and plausible dance movements.  

Our GPT model is optimized through supervised training using a cross-entropy loss on the next-token probabilities. Notably, because our pose tokenization includes associated keypoint confidences, the transformer also learns to model temporal variations in confidence, such as those caused by motion blur and occlusions, enabling it to capture more realistic motion distributions observed in videos.

\begin{figure*}[t!]\vspace{-5pt}
\centering
\vspace{-2mm}
 \includegraphics[width=0.95\linewidth]{figures/qualitative_copy.pdf}
  % \vspace{-15pt}
   \caption{\textbf{Qualitative Comparisons.} Among all the methods, \papername~achieves the most expressive and high-fidelity human dance video synthesis, maintaining the highest consistency with both the reference human characteristics and the background scene.
   }
   \vspace{-4mm}
    \label{fig:baselines}
\end{figure*}

\subsection{Diffusion-Based Dance Video Synthesis}
\label{sec:diff}
With the generated motion token sequences from our trained transformer model, we employ a diffusion model to synthesize high-resolution, realistic human dance videos, conditioned on the generated motions and a given reference image. To achieve this, we leverage a pretrained T2I diffusion backbone~\cite{rombach2022high} and incorporate additional temporal modules~\cite{guo2023animatediff} to improve cross-frame consistency and ensure temporal smoothness. For transferring the reference image context, a reference network, as a trainable copy of the backbone UNet, extracts reference features of identity appearance and background which are cross-queried by the self-attentions within the backbone UNet. Motion control is achieved through an addition module, often configured in ControlNet~\cite{zhang2023adding} or light-weight PoseGuider~\cite{hu2024animate}, which translates the motion conditions into 2D spatial guidance additive to the UNet features. 

To incorporate the generated motion tokens into human image animation, one approach is to utilize the trained VQVAE decoder
$D$ to decode pose tokens into keypoint coordinates, which are then visualized as 2D skeleton maps for motion conditioning. These skeleton maps can provide motion guidance to the final diffusion synthesis through a PoseGuider module. While effective to some extent, this approach introduces a non-differentiable process due to the skeleton map visualization, which impedes end-to-end training and often results in the loss of keypoint confidence information.
Additionally, since the temporal module is trained on real video data where pose sequences are typically smooth, it may struggle with jittery or inconsistent motions generated by the transformer model at inference.

In place of explicit translation of motion tokens into 2D skeleton maps as pose guider conditions, we introduce a trainable motion decoder that implicitly and directly translates the 1D pose tokens into 2D spatial guidance, added towards the intermediate features of the denoising UNet. Starting from a learnable 2D feature map, the decoder injects the 1D pose token sequences including the keypoint confidences with AdaIN layers~\cite{huang2017arbitrary}, progressively upsampling this feature map into multiple scales aligned with the resolutions of the denoising UNet features.  The motion decoder is trained alongside the temporal module within a 16-frame sequence window, effectively decoding token sequences into continuous pose guidance that integrates temporal context from adjacent frames. Moreover, by incorporating reference image context during training, we observe empirically that the synthesized pose guidance retains minimal identity and body shape information compared to skeleton-based pose guiders, enabling generated pose tokens to adapt seamlessly to characters with varied body shapes and appearances.