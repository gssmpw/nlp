\section{Conclusion}

We present \papername, a novel framework that unites an autoregressive transformer with a diffusion model to generate high-quality, music-driven human dance videos from a single reference image. Unlike prior works, \papername\ models and generates dance movements in 2D space, harnessing widely accessible 2D poses from monocular dance videos to capture diverse, expressive whole-body motions. Our method achieves state-of-the-art results in video quality, motion diversity and expressiveness, providing a scalable and adaptable solution for creating vivid, music-aligned dance videos across various human forms and styles.

\noindent\textbf{Limitations and Future Work.} Our model is trained solely on curated real-human daily dance videos from internet, 
which can be noisy and the dance movements may not exhibit the precision found in professional dancer videos.  Consequently, out-of-domain human images may lead to rendering artifacts, and the generated dance motions may occasionally lack alignment with music. While we designed our pipeline to be end-to-end trainable and scalable, we currently implement it in stages due to memory limitations. Future work will explore large-scale, multi-machine training to further enhance performance and efficiency.

\noindent\textbf{Ethics Statement.} Our work aims to improve human image animation from a technical perspective and is not intended for malicious use like fake videos. Therefore, synthesized videos should clearly indicate their artificial nature.




