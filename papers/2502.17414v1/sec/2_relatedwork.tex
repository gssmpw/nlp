\section{Related Work}

\subsection{Music to Dance Generation}
Significant progress has been made in realistic human motion generation~\cite{zhi2023livelyspeaker,bailando,edge,lucas2022posegpt,chen2023humanmac,zhang2023generating} from various inputs, such as input speech~\cite{ginosar2019learning,qian2021speech,liu2022learning,zhang2024dr2,ao2022rhythmic} or text~\cite{tevet2023human,jiang2023motiongpt,petrovich2022temos,zhang2024motiondiffuse}. However, the task of music-to-dance generation~\cite{bailando,alexanderson2023listen, edge,li2024lodge,li2024exploring,le2023music,Luo_2024_CVPR,zhang2024bidirectional} presents unique challenges: (1) ensuring the generated dance rhythmically aligns with the music, and (2) producing intricate motions with diverse styles and speeds. Due to data limitations, several 3D human pose datasets~\cite{aist++,li2023finedance,le2023music,Luo_2024_CVPR} have been proposed. The AIST++ dataset~\cite{aist++} is a 3D music pose dataset containing 1,408 dance motions tailored to various music genres. FineDance~\cite{li2023finedance}, a 3D dataset focusing on fine-grained hand motions, includes 14 hours of dance data. Models like Bailando~\cite{bailando} and EDGE~\cite{edge} have leveraged AIST++ for training. Bailando~\cite{bailando} pioneered using a VQ-VAE for 3D pose encoding, followed by an Actor-Critic GPT model to generate body poses conditioned on music. EDGE~\cite{edge} applied a diffusion framework to predict human poses from a noisy sequence, conditioning on music and employing long-form sampling for extended dance sequences. 
Despite these advancements, existing approaches trained on 3D datasets often generate dance poses with limited diversity and are primarily confined to a 3D format. X-Dancer overcomes these data constraints by leveraging a broad spectrum of 2D dance motions, aligning complex movements with musical beats, and utilizing widely available video data to enhance scalability.

\subsection{Human Image Animation}
With the advancements in diffusion models~\cite{ho2022video,guo2023animatediff,blattmann2023stable}, generating high-quality human videos has become feasible. The introduction of ControlNet~\cite{zhang2023adding} and PoseGuider~\cite{hu2024animate} has further empowered prior methods~\cite{hu2024animate,chang2023magicpose,xu2024magicanimate,zhu2024champ} to create realistic dance videos from a reference image using motion signals such as 2D skeletons, dense poses, and depth maps. While these methods provide explicit control over body poses and allow flexible motion manipulation, they often struggle to preserve the original body shape and other identity-specific features, leading to noticeable identity leakage, particularly when significant differences exist between the target and source.
Several methods~\cite{hallo,jiang2024loopy,tian2024emo} adopt an end-to-end pipeline for generating realistic talking human videos from audio by injecting audio features directly into the network via cross-attention layers. However, while these methods excel in capturing micro-expressions and lip movements, they fall short in handling dynamic, shape-preserving dance movements. X-Dancer uses a diffusion backbone with AdaIN to animate the reference image in an end-to-end framework, enabling high-quality, shape-consistent dance generation while effectively preserving the identity of the reference image.