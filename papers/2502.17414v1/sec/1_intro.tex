\section{Introduction}
\label{sec:intro}

Dance is a universal form of self-expression and social communication, deeply embedded in human behavior and culture. With the rise of social media platforms like TikTok and YouTube Shorts, people increasingly share self-expressive dance videos online. However, creating expressive choreography typically demands practice and even professional training. From a computational aspect, generating realistic dance movements is challenging due to the freeform, personalized nature of dance and its alignment with musical rhythm and structure. In this work, we tackle the challenge of creating continuous, expressive and lifelike dance videos from a single static image, driven solely by a music track.


This study addresses two key challenges in music-driven human image animation: (1) generating smooth, diverse full-body movements at finer scales that capture the complex, non-linear synchronization with music inputs, and (2) translating these generated body movements into high-fidelity video outputs that maintain visual consistency with the reference image and ensure temporal smoothness. Prior approaches~\cite{bailando,edge} mainly focus on computationally generating 3D human poses, such as SMPL skeletons~\cite{loper2023smpl}, from music inputs, utilizing diffusion- or GPT-based frameworks. While these methods excel in producing high-quality, clean motions, they are constrained by limited training datasets (primarily the multi-view AIST dataset~\cite{aist-dance-db,aist++}), which lack diversity, and contain 3D body poses only (excluding head and hands motions). Expanding these datasets with widely available 2D monocular dance videos would require 3D human pose estimation, which is often error-prone and risks degraded motion quality and consistency. Moreover, we target at photorealistic dance video generation rather than 3D skeleton or mesh animations. 

With recent advances in diffusion models that have achieved unprecedented diversity and quality in image and video generation,  numerous works have leveraged their generative capabilities to synthesize visually compelling videos by animating a reference image with motion signals such as 2D skeletons~\cite{hu2024animate,chang2023magicpose}, dense poses~\cite{xu2024magicanimate}, and depth maps~\cite{zhu2024champ}. Unlike motion transfer settings that derive motion from a driving video, our goal is to generate motion signals that are consistent with the reference body shape and aligned with the input musical beats. Recently, a few studies~\cite{hallo,tian2024emo} have attempted to synthesize visual outputs end-to-end from audio inputs using diffusion models. While these methods have advanced in realism and dynamic quality, they still struggle to capture long-range motion and audio context due to high computational demands. Moreover, these frameworks have primarily been validated on audio-driven portrait animations, leaving uncertain whether they can accommodate the complexities of full-body kinematics and rapid motion transitions.

To this end, we propose \papername, a unified framework that integrates an autoregressive transformer model for generating extended dance sequences attuned to input music, coupled with a diffusion model to produce high-resolution, lifelike videos. In contrast to prior methods focused on 3D music-to-dance motion generation, our approach models 2D human motion, leveraging widely accessible dance videos where 2D pose estimation is both reliable and scalable. For effective autoregressive motion generation, we develop a multi-part tokenization scheme for per-frame 2D whole-body poses, incorporating detected keypoint confidences to capture multi-scale human motions with motion blur and various occlusions. Thereafter we train a cross-modal transformer that auto-regressively predicts future N-frame pose tokens, paired with per-frame music features extracted with Jukebox~\cite{dhariwal2020jukebox} and Librosa~\cite{jin2017towards}. Our design enables the model to capture a broader diversity of expressive, music-aligned movements with enhanced scalability in both model complexity and data scale. We then leverage a T2I diffusion model with temporal layers to animate the reference image by implicitly translating the generated confidence-aware motion tokens into spatial guidance via a trainable motion token decoder. Specifically, it upsamples from a learnable feature map into multi-scale spatial feature guidance, integrating motion tokens through AdaIN~\cite{huang2017arbitrary} during the upsampling process.
By co-learning pose translation with temporal and appearance reference modules~\cite{guo2023animatediff,cao2023masactrl}, the diffusion backbone interprets motion tokens with temporal context and body shape reference,  leading to better shape-disentangled pose control,  smoother and more robust visual outputs even under low-confidence or jittering pose sequences.  
This design establishes an end-to-end transformer-diffusion learning framework, merging the transformer’s strengths in long-context understanding and generation with the diffusion model’s high-quality visual synthesis.

To the best of our knowledge, \papername~represents the first music-driven human image animation framework.
Trained on a curated dataset of 100K music-dance video clips, our method excels at generating diverse, expressive and detailed whole-body dance video animations attuned to input music, adaptable to both realistic and stylized human images across various body types. We comprehensively evaluate our model on challenging benchmarks, and~\papername~outperforms state-of-the-art 
baselines both quantitatively and qualitatively. Additionally, we highlight its scalability and customization capabilities, showcasing scalability across different model and data scales, as well as fine-tuning to specific choreography.  We summarize our contributions as follows,
\begin{itemize}[nolistsep,leftmargin=*]
\item A novel transformer-diffusion based music-to-dance human image animation, achieving state-of-the-art performance in terms of motion diversity, expressiveness, music alignment and video quality.

\item A cross-modal transformer model that captures long-range synchronized dance movements with music features, employing  a multi-scale tokenization scheme of whole-body 2D human poses with keypoint confidence. 

\item A diffusion-based human image animation model that interprets temporal pose tokens and translates them into consistent high-resolution video outputs.

\item Demonstration of captivating zero-shot music-driven human image animations, along with characterized choreography generation.  
\end{itemize}

