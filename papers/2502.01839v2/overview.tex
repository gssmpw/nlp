
\definecolor{subtleyellow}{RGB}{255, 255, 150}

\begin{table}[htbp]
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{AIME} & \textbf{MATH} & \textbf{LiveBench Math} & \textbf{LiveBench Reasoning} \\
\midrule
Pass@1 & 1 / 15 & 426 / 500 &  104 / 200 & 63 / 140 \\
Consistency@200 & 4 / 15  & 460 / 500 & 118 / 200  & 75 / 140 \\
Consistency@1,000 & 3 / 15  & 460 / 500 & 120 / 200  & 73 / 140 \\
\rowcolor{subtleyellow} Verification@200 & {\textbf{8}} / 15 & \textbf{467} / 500 & \textbf{135} / 200  & \textbf{97} / 140 \\
o1-Preview@1 & 7 / 15 & 428 / 500 & 131 / 200  & 95 / 140 \\
\bottomrule
\end{tabular}
\caption{Accuracy rates of the Gemini v1.5 Pro model using sampling-based search (Verification@200) on reasoning benchmarks, compared to other inference methods and o1-Preview performance.
Verification@200 consistently improves on Consistency@200 and surpasses o1-Preview.\protect\footnotemark{}
Each score reflects a single run, due to the high expense of search at this scale (see Section~\ref{sec:tech}).
}
\label{tab:main-sota}
\end{table}

\footnotetext{The o1-preview-2024-09-12 numbers in Table~\ref{tab:main-sota} use publicly reported figures, with MATH and AIME figures sourced from the OpenAI blog post \citep{o1-preview}, and LiveBench figures sourced from the LiveBench leaderboard (\href{https://livebench.ai/}{livebench.ai}). We found the performance of o1-Preview as accessed through the OpenAI API to slightly differ with publicly reported figures, e.g. scoring 26\% not 44\% on AIME, and scoring 77\% not 67\% on LiveBench Reasoning.}

\paragraph{Preview and outline.}
Table~\ref{tab:main-sota} summarizes our first finding: that, with effective self-verification, simply scaling sampling-based search is sufficient to approach state-of-art performance on reasoning and math benchmarks (AIME 2024 \citep{aime2024}, LiveBench Math, LiveBench Reasoning \citep{white2024livebenchchallengingcontaminationfreellm}, and the Berkeley MATH dataset \citep{DBLP:conf/nips/HendrycksBKABTS21}).
It depicts the accuracy of the Gemini v1.5 Pro model \citep{geminiteam2024gemini15unlockingmultimodal} when only one solution\footnote{As we focus on answering reasoning problems, we use ``model responses'' and ``model solutions'' interchangeably.} is attempted per question (Pass@1), when 200 solutions are attempted and the most common final answer is selected (Consistency@200, \cite{DBLP:conf/iclr/0002WSLCNCZ23}), and under sampling-based search, when 200 solutions are attempted and scored for correctness with the highest scorer selected (Verification@200, Algorithm~\ref{alg:verification-pipeline}).
With sampling-based search (Verification@200), Gemini v1.5 surpasses the performance of o1-Preview, a model explicitly trained on reasoning problems to leverage significant test-time compute and perform internal search.

The rest of this paper is devoted to studying the three key factors behind the numbers in Table~\ref{tab:main-sota}.
Section~\ref{subsec:scaling} analyzes the remarkable scalability of sampling-based search, as one varies both the compute spent on search and verification;
Section~\ref{subsec:implicit} analyzes the phenomenon of \emph{implicit scaling} and its role in driving this scalability; and
Section~\ref{section:pipeline} discusses  important principles for scaling self-verification capability, which may be of independent interest.
We also highlight deficits in the verification capabilities of frontier models with a new benchmark in Section~\ref{sec:benchmark}.
Technical details and detailed discussion of related work are found in Sections~\ref{sec:tech} and \ref{sec:related} respectively.




\begin{algorithm}[htbp]
\caption{Sampling-Based Search (Verification@$k_{\mathrm{inf}}$)}
\label{alg:verification-pipeline}
\begin{algorithmic}[1]

\Require Prompt $Q$, language model $\mathrm{LM}$, scaling parameters $k_{\mathrm{inf}}, k_{\mathrm{verif}}, k_{\mathrm{tie}}$.

\State Populate $\mathcal{S}$ with $k_{\mathrm{inf}}$ samples from $\mathrm{LM}(``\mathrm{Answer} \;Q")$. \Comment{\textbf{Stage 1: Generate Responses}}

\For{each candidate response $s_i \in \mathcal{S}$}\Comment{\textbf{Stage 2: Verify Responses}}
    \State Populate $\mathcal{V}_i$ with $k_{\mathrm{verif}}$ samples from $\mathrm{LM}(``\mathrm{Return}\;1[\mathrm{response} \; s_i \; \mathrm{to} \;Q \;\mathrm{is \;correct}]")$.
\EndFor

\State Gather the highest-scored response  $\mathcal{S}_{\mathrm{Best}} = \{s_i \mid i \in [k_{\mathrm{inf}}], \mathrm{Avg}(\mathcal{V}_i) \geq \max_{j \in [k_{\mathrm{inf}}]} \mathrm{Avg}(\mathcal{V}_j) - 0.05\}$.

\If{$|\mathcal{S}_{\mathrm{Best}}| = 1$}
    \State Return response $s_{i^*}$ where $i^* = \max_{j \in [k_{\mathrm{inf}}]} \mathrm{Avg}(\mathcal{V}_j)$.
\Else
    \For{each pair of candidate responses $(s_i, s_j) \in \binom{\mathcal{S}_{\mathrm{Best}}}{2}$}\Comment{\textbf{Tie-Break: Compare Responses}}
    \State Populate $\mathcal{C}_{i,j}$ with $k_{\mathrm{tie}}$ samples from $\mathrm{LM}(``\mathrm{Which\;of\;responses\;}\{s_i, s_j\}\mathrm{\;to\;}Q\;\mathrm{is \;correct?}")$. 
    \EndFor
    \State Return response $s_{i^*}$ where $i^*$ is the winner of the most matchups $\{\mathcal{C}_{i,j} \mid s_i, s_j \in \mathcal{S}_{\mathrm{Best}}\}$.
\EndIf

\end{algorithmic}
\end{algorithm}
