Recent advances in language models highlight the importance of test-time compute scaling wherein one uses more compute during inference to enhance reasoning capabilities~\citep{o1-preview, deepseekai2025deepseekr1incentivizingreasoningcapability, agarwal2024manyshotincontextlearning, DBLP:conf/nips/Wei0SBIXCLZ22, DBLP:conf/nips/YaoYZS00N23, akyurek2024surprisingeffectivenesstesttimetraining}.
There are many methods for increasing test-time compute usage, including implicitly encouraging longer responses via reinforcement learning~\citep{o1-preview, deepseekai2025deepseekr1incentivizingreasoningcapability} or explicitly via prompting \citep{DBLP:conf/nips/Wei0SBIXCLZ22, DBLP:conf/nips/YaoYZS00N23}.
However, \emph{sampling-based search}---an instance of the generate-and-test approach where a model generates many responses in parallel, e.g. via random sampling or delegation, and selects what the model guesses to be the best one---remains one of the most natural and fundamental paradigms.
In addition to being complementary with other test-time compute scaling strategies, 
it also has the unique advantage of being embarrassingly parallel and allowing for arbitrarily scaling: simply sample more responses \citep{cobbe_training_2021,DBLP:conf/iclr/0002WSLCNCZ23}.
As a result, sampling-based search plays an increasingly crucial role as language models are set loose on frontier mathematical and scientific problems where inference compute budgets reach thousands of dollars or more per problem.

Though recent works demonstrate the benefits of sampling-based search \citep{cobbe_training_2021,DBLP:conf/iclr/0002WSLCNCZ23,xue_rcot_2023}, many questions remain as to what scaling trends govern this fundamental test-time compute scaling strategy.
To develop this understanding, we study a  minimalist---yet remarkably effective---instantiation of sampling-based search that uses a language model \citep{geminiteam2024gemini15unlockingmultimodal} to both generate a set of candidate responses via random sampling and select the best one by attempting to verify each response with natural language.
Specifically, we consider the case where models must self-verify their responses to  select the best answer, and do not make the strong assumption that one can access ground-truth answers or symbolic systems that exactly verify correctness.
In this setup, we address the question:
{\em what test-time scaling trends emerge as we scale both the number of sampled responses and verification capabilities?}
In particular, what are the limits of scaling this simple sampling-based search paradigm and how much does one need to continuously scale verification capability as one scales up search?

\paragraph{Our findings.}
We first identify scaling trends demonstrating that reasoning performance continues to improve with sampling-based search even as test-time compute is scaled well beyond the point where the performance of self-consistency~\citep{DBLP:conf/iclr/0002WSLCNCZ23} saturates.
At sufficient scale, even our minimalist implementation provides a significant leap in reasoning accuracy, lifting Gemini v1.5 Pro performance beyond o1-Preview, and Gemini v1.5 Flash beyond Gemini v1.5 Pro, on reasoning benchmarks such as LiveBench~\citep{white2024livebenchchallengingcontaminationfreellm} and the AIME~\citep{aime2024}, exhibiting sustained power-law scaling on the latter.
This not only highlights the importance of sampling-based search for scaling capability, but also suggests the utility of sampling-based search as a simple baseline on which to compare other test-time compute scaling strategies and measure genuine improvements in models' search capabilities.

We then attribute much of the strong scaling trends of sampling-based search to an \emph{implicit scaling} phenomenon.
Contrary to the intuition that sampling more responses should impose a greater burden on the verifier and reduce verification accuracy, we observe that scaling sampling indirectly enhances verification accuracy.
At a high-level, this is because well-written responses are easier to verify than poorly written responses, and scaling sampling widens the pool of well-written candidates.

We further identify two effective strategies for scaling verification capabilities using test-time compute: (1) directly comparing candidate responses and (2) task-specific rewriting of candidate responses.
The former mitigates a core weakness of language models, which struggle to identify mistakes and hallucinations unless given their locations~\citep{tyen_llms_2024}, by leveraging the fact that differences between candidate responses provide a strong signal for where errors might be located.
The latter leverages our observation of \emph{output style suitability} where chain-of-thought output formats are beneficial when generating responses but harder to verify than more formal, mathematically conventional writing styles.
Surprisingly, while effective verification can be easily elicited from frontier models by communicating these strategies, we observe that frontier models have remarkably poor out-of-box verification capabilities and introduce a new benchmark to quantify these deficits.
