All experiments are run on Google Cloud with Gemini v1.5-Pro-002 and Gemini v1.5-Flash-002 models dated to September 2024.
Unless otherwise specified, the default parameters for our implementation of sampling-based search (Section~\ref{section:pipeline}) are $k_{\mathrm{inf}} = 200$, $\sigma_{\mathrm{inf}} = 1.5$, $k_{\mathrm{verif}} = 50$, $\sigma_{\mathrm{verif}} = 1$, and a maximum of 8,192 output tokens per query.
For all benchmarks, the scoring of candidate responses is performed using a language model rather than literal string comparison; details are in Appendix~\ref{app:scoring}.

\paragraph{Preliminary scoring.}
When generating $k_{\mathrm{verif}} = 50$ verification scores per candidate solution is too expensive, we first generate $k_{\mathrm{verif}} = 10$ preliminary verification scores and discard candidate solutions with an average score below $0.2$.
If a final answer is represented by more than 15 candidate responses, only the top 15--as measured by average preliminary score, tie-breaking randomly--are kept.
This results in a smaller pool of candidate solutions for which we compute all $k_{\mathrm{verif}} = 50$ verification scores.
Preliminary scoring is used on all datasets except AIME, which consists of 15 questions. 

\paragraph{Compute.}
On AIME, the verification process involves 32,000 
characters (roughly 13,000 tokens) of model output. %
Extrapolating from these figures, running the full sampling-based search pipeline on a question for $k_{\mathrm{inf}} = 200$ and $k_{\mathrm{verif}} = 50$ requires $200 \cdot 50 \cdot 13,000 \approx 130$M output tokens.
At around \$5/1M output tokens (public pricing of Gemini v1.5 Pro), this evaluates to approximately \$650 in cost.
Preliminary scoring reduces usage of output tokens by roughly 70\%, resulting in a per-question cost of \$200.
The use of Gemini Flash for verification further decreases cost to \$12 per question.

\paragraph{Datasets.}
Our MATH benchmark consists of 500 questions from the PRM800K \citep{DBLP:conf/iclr/LightmanKBEBLLS24} test split of Berkeley MATH \citep{DBLP:conf/nips/HendrycksBKABTS21}.
Our LiveBench Math benchmark consists of 200 randomly subsampled questions from the 368 available as of October 21st 2024, including AMC12 2023, AIME 2024, SMC 2023, USAMO 2023, IMO 2023, and synthetic math questions \citep{white2024livebenchchallengingcontaminationfreellm}.
Our LiveBench Reasoning benchmark consists of 140 questions from the 150 available as of October 21st 2024, including Zebra puzzles, Web-Of-Lies, and Spatial reasoning questions \citep{white2024livebenchchallengingcontaminationfreellm}.
Our AIME benchmark consists of the 15 questions in Exam II of AIME 2024 \citep{aime2024}. 
