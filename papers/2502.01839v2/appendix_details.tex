\subsection{Inference Prompts}
For questions from the MATH and AIME benchmarks, we use the following prompt.
\begin{tcolorbox}[title=MATH and AIME Prompt]
Please answer the following question. Think carefully and in a step-by-step fashion. At the end of your solution, put your final result in a boxed environment, e.g. $\boxed{42}$.

\textcolor{blue}{The question would be here.}
\end{tcolorbox}

For questions from the LiveBench Math and LiveBench Reasoning benchmarks, which already come with their own instructions and formatting requests, we do not provide any accompanying prompt and simply submit the model the question verbatim.
\begin{tcolorbox}[title=LiveBench Prompt]
\textcolor{blue}{The question would be here.}
\end{tcolorbox}

\subsection{LM-Based Scoring}
\label{app:scoring}

Given a tuple consisting of a question, ground-truth solution, and candidate response, we grade the correctness of the candidate response by querying a Gemini-v1.5-Pro-002 model to compare the candidate and ground-truth solutions.
This involves repeating the following process five times: (1) send a prompt to the model that provides the question, the correct ground-truth solution, and the candidate response, and asks the model to deliberate on the correctness of the candidate response; and (2) send a followup prompt to the model to obtain a correctness ruling in a structured format.
If a strict majority of (valid) responses to the second prompt evaluate to a JSON object with the key-value pair \tcbox[on line,  boxrule=0.5pt, top=0pt, bottom=0pt, left=1pt, right=1pt]{``student\_final\_answer\_is\_correct'' = True} rather than \tcbox[on line,  boxrule=0.5pt, top=0pt, bottom=0pt, left=1pt, right=1pt]{``student\_final\_answer\_is\_correct'' = False}, the candidate response is labeled correct. Otherwise, the candidate response is labeled incorrect.
These queries are all processed with temperature zero.
The prompts, which can be found at the end of this subsection,
 ask the language model to
(1) identify the final answer of the given response, (2) identify the final answer of the reference (ground truth) response, and (3) determine whether the final answer of the given response satisfactorily matches that of the reference response, ignoring
any non-substantive formatting disagreements.
In line with convention, we instruct our scoring system to ignore the correctness of the logic used to reach the final answer and rather only judge the correctness of the final answer.
The model is asked to label all non-sensical and incomplete responses as being incorrect.

As a form of quality assurance, every scoring output for the Consistency@200 and Verification@200 figures depicted in Table~\ref{tab:main-sota} was manually compared against human scoring.
No discrepancies between automated and human scoring were found on the MATH and AIME datasets for both Consistency@200 and Verification@200.
No discrepancies were found on LiveBench Reasoning for Consistency@200.
For Verification@200, one false positive (answer labeled by automated system as being incorrect but labeled by human as being correct) and one false negative  (answer labeled by automated system as being correct but labeled by human as being incorrect) were identified on LiveBench Reasoning; three false positives and four false negatives were identified on LiveBench Math.
For Consistency@200, two false negatives were identified on LiveBench Math.
This means that LM scoring matched human scoring 99\% of the time, and the choice of human versus automated scoring matters little to our results.

\begin{tcolorbox}[title=Prompt 1, breakable]
You are an accurate and reliable automated grading system. Below are two solutions to a math exam problem: a solution written by a student and the solution from the answer key. Your task is to check if the student's solution reaches a correct final answer. 

Your response should consist of three parts. First, after reading the question carefully, identify the final answer of the answer key's solution. Second, identify the final answer of the student's solution. Third, identify whether the student's final answer is correct by comparing it to the answer key's final answer.

\# The question, answer key, and student solution

The math exam question:

\verb|```|

\textcolor{blue}{The question would be here.}

\verb|```|

The answer key solution:

\verb|```|

\textcolor{blue}{The reference solution would be here.}

\verb|```|

The student's solution:

\verb|```|

\textcolor{blue}{The candidate solution would be here.}

\verb|```|

\# Your response format

Please structure your response as follows. PROVIDE A COMPLETE RESPONSE.

\verb|```|

\# Answer Key Final Answer

Identify the final answer of the answer key solution. That's all you need to do here: just identify the final answer.

A "final answer" can take many forms, depending on what the question is asking for; it can be a number (e.g., "37"), a string (e.g., "ABCDE"), a sequence (e.g., "2,3,4,5"), a letter (e.g., "Y"), a multiple choice option (e.g. "C"), a word (e.g., "Apple"), an algebraic expression (e.g. "$x^2 + 37$"), a quantity with units (e.g. "4 miles"), or any of a number of other options. If a solution concludes that the question is not answerable with the information provided or otherwise claims that there is no solution to the problem, let the final answer be "None". If the solution does not produce any final answer because it appears to be cut off partway or is otherwise non-sensical, let the solution's final answer be "Incomplete solution" (this could only ever possibly happen with the student solution).

YOUR RESPONSE HERE SHOULD BE BRIEF. JUST IDENTIFY WHAT THE QUESTION IS ASKING FOR, AND IDENTIFY THE ANSWER KEY'S FINAL ANSWER. DO NOT ATTEMPT TO ANSWER THE QUESTION OR EVALUATE INTERMEDIATE STEPS.

\# Student Solution Final Answer

Identify the final answer of the student solution.

YOUR RESPONSE HERE SHOULD BE BRIEF. JUST IDENTIFY WHAT THE QUESTION IS ASKING FOR, AND IDENTIFY THE STUDENT'S FINAL ANSWER. DO NOT ATTEMPT TO ANSWER THE QUESTION OR EVALUATE INTERMEDIATE STEPS.

\# Correctness

Simply evaluate whether the student's final answer is correct by comparing it to the answer key's final answer.

Compare the student's final answer against the answer key's final answer to determine if the student's final answer is correct.

* It does not matter how the student reached their final answer, so long as their final answer itself is correct.

* It does not matter how the student formatted their final answer; for example, if the correct final answer is \boxed{7 / 2}, the student may write ***3.5*** or \boxed{\mathrm{three\; and\; a\; half}} or $\boxed{\frac{14}{4}}$. It does not matter if the student's final answer uses the same specific formatting that the question asks for, such as writing multiple choice options in the form "(E)" rather than "***E***".

* It does not matter if the student omitted units such as dollar signs.

* If the student solution appears to be truncated or otherwise incoherent, e.g. due to a technical glitch, then it should be treated as being incorrect.

ONCE AGAIN, DO NOT EVALUATE INTERMEDIATE STEPS OR TRY TO SOLVE THE PROBLEM YOURSELF. THE ANSWER KEY IS ALWAYS RIGHT. JUST COMPARE THE FINAL ANSWERS. IF THEY MATCH, THE STUDENT ANSWER IS CORRECT. IF THEY DO NOT MATCH, THE STUDENT ANSWER IS INCORRECT.

\# Summary

* Answer key final answer: (The final answer of the answer key solution. Please remove any unnecessary formatting, e.g. provide "3" rather than "\boxed{3}", provide "E" rather than "***E***", provide "1, 2, 3" rather than "[1, 2, 3]", provide "4 ounces" rather than "4oz".)

* Student final answer: (The final answer of the student's solution. Please remove any unnecessary formatting, e.g. provide "3" rather than "\boxed{3}", provide "E" rather than "***E***", provide "1, 2, 3" rather than "[1, 2, 3]", provide "4 ounces" rather than "4oz".)

* Student final answer is correct?: (Does the student final answer match the answer key final answer? Please provide "true" or "false".)

\verb|```|

\end{tcolorbox}

\begin{tcolorbox}[breakable, title=Prompt 2]
Please structure your output now as JSON, saying nothing else. Use the following format:
\verb|```|
\{
    "answer\_key\_final\_answer": str (the final answer of the answer key solution; please remove any formatting"),
    "student\_final\_answer": str (the final answer of the student's solution; please remove any formatting"),
    "student\_final\_answer\_is\_correct": true/false,
\}
\end{tcolorbox}

\subsection{Implementation of Consistency@k}
Consistency@k measures the performance of a model by evaluating the correctness of the most common answer reached by the model after being run $k$ times.
An important consideration with implementing consistency@k is that there are many choices for the equivalence relation one can use to define ``the most common answer''.
We define two candidate responses as reaching the same answer if their final answer is the same.
We determine a candidate response's final answer by prompting a language model to identify the final answer from the candidate response; we then strip the extracted final answer of leading and trailing whitespace.
We determine equivalence with a literal string match.
After determining the most common final answer to a question, we use the string \tcbox[on line,  boxrule=0.5pt, top=0pt, bottom=0pt, left=1pt, right=1pt]{``The final answer is \textcolor{blue}{\{final answer\}}''} as the consistency@k response.
Note that we could have instead randomly chosen a candidate response corresponding to the most common final answer, and used that selected response as the consistency@k response---we have found that, because our LM-based scoring system evaluates correctness using only the final answer, this alternative results in the same consistency@k metrics.

\subsection{Benchmark Evaluation Prompts}
\label{app:benchmarkprompts}

The benchmark performances reported in Table~\ref{tab:benchmark} are obtained with the following prompts.
The following prompt is used for the comparison task.
\begin{tcolorbox}[title=Comparison Task Prompt Part 1]
\textcolor{blue}{Question here.}

  Here are two solutions to the above question. You must determine which one is correct. Please think extremely carefully. Do not leap to conclusions. Find out where the solutions disagree, trace them back to the source of their disagreement, and figure out which one is right.

  Solution 1:
  
\textcolor{blue}{First solution here.}

  Solution 2:
  
\textcolor{blue}{Second solution here.}
\end{tcolorbox}
\begin{tcolorbox}[title=Comparison Task Prompt Part 2]
Now summarize your response in a JSON format. Respond in the following format saying nothing else:

  \{
  
     "correct\_solution": 1 or 2
     
  \}
\end{tcolorbox}

The following prompt is used for the scoring task.
\begin{tcolorbox}[title=Scoring Task Prompt Part 1]
\textcolor{blue}{Question here.}

 I include below a student solution to the above question. Determine whether the student solution reaches the correct final answer in a correct fashion; e.g., whether the solution makes two major errors that still coincidentally cancel out. Please be careful and do not leap to conclusions without first reasoning them through.

Solution:

\textcolor{blue}{Solution here.}
\end{tcolorbox}
\begin{tcolorbox}[title=Scoring Task Prompt Part 2]
Now summarize your response in a JSON format. Respond in the following format saying nothing else:

\{
 
 "is\_solution\_correct": 'yes' or 'no'

\}
\end{tcolorbox}
