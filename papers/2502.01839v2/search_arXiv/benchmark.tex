Frontier language models demonstrate a remarkable mismatch between their problem-solving capabilities and poor out-of-box verification capabilities.
These limitations have largely been attributed to the inability of current language models to self-diagnose hallucinations or enforce rigour \citep{zhang_how_2023,orgad_llms_2024,snyder_early_2024,kamoi_evaluating_2024, tyen_llms_2024, DBLP:conf/iclr/0009CMZYSZ24}.
However, our findings that models can be directed to accurately perform verifications at scale suggest that these out-of-box limitations can be addressed with standard methods like instruction tuning.
We compiled a set of challenging reasoning problems and candidate solutions to provide a benchmark for these deficits.

Each entry in this benchmark consists of a question, a correct candidate response, and an incorrect candidate response, and is manually curated from the residuals of our sampling-based search experiments (Section~\ref{section:pipeline}).
An example entry from this benchmark can be found below (see Appendix~\ref{app:examplebenchmark} for more).

\vspace{0.4cm}
\begin{tcolorbox}[title=Question from LiveBench Reasoning (Web-of-Lies Puzzle), breakable]
In this question, assume each person either always tells the truth or always lies. The person at the campground thinks their friend is lying. Mateo is at the aquarium. The person at the restaurant says the person at the hotel lies. Farid is at the movie theater. The person at the movie theater says the person at the campground lies. Ryan is at the shopping mall. The person at the cafe says the person at the campground lies. The person at the observatory says the person at the museum lies. The person at the museum says the person at the restaurant tells the truth. The person at the campground says the person at the airport tells the truth. The person at the campground tells the truth. Luna is at the restaurant. The person at the movie theater saw a firetruck. Isabella is at the observatory. The person at the amusement park says the person at the museum tells the truth. The person at the restaurant says the person at the shopping mall tells the truth. Liam is at the beach. Ayaan is at the airport. The person at the restaurant says the person at the beach tells the truth. Maya is at the amusement park. The person at the restaurant says the person at the aquarium lies. The person at the restaurant says the person at the movie theater lies. The person at the library says the person at the campground tells the truth. Elowen is at the hotel. Priya is at the library. The person at the gym says the person at the restaurant tells the truth. Kehinde is at the cafe. Anika is at the museum. Quan is at the campground. Soren is at the gym. Does the person at the museum tell the truth? Does the person at the restaurant tell the truth? Does the person at the movie theater tell the truth? Think step by step, and then put your answer in **bold** as a list of three words, yes or no (for example, **yes, no, yes**). If you don't know, guess.
\end{tcolorbox}
\definecolor{mydarkgreen}{rgb}{0.0, 0.5, 0.0} %


\begin{paracol}{2}

\switchcolumn[0]
\begin{tcolorbox}[breakable, colframe=mydarkgreen!70, colback=mydarkgreen!10, title=Correct response]
Here's how we can break down this logic puzzle:

1. **Quan is at the campground and tells the truth.** This is a given fact.

...

4. **The person at the restaurant (Luna) says the person at the hotel (Elowen) lies.**  We don't know yet if Luna is lying or telling the truth.

...

10. **Since Luna tells the truth about Farid, Luna *always* tells the truth.**  This unlocks several other pieces of information.

...

13. **The person at the cafe (Kehinde) says the person at the campground lies.** Since Quan tells the truth, Kehinde is lying.

Therefore:
* **Anika (museum):** Tells the truth.
* **Luna (restaurant):** Tells the truth.
* **Farid (movie theater):** Lies.

**yes, yes, no**
\end{tcolorbox}

\switchcolumn
\begin{tcolorbox}[breakable, colframe=red!70, colback=red!10, title=Incorrect Response Due to Hallucination]
Here's how we can solve this logic puzzle:

1. **Quan is at the campground and tells the truth.** This is given.

2. **The person at the airport (Ayaan) tells the truth.** Quan (campground) says the person at the airport tells the truth, and Quan tells the truth.

...

4. **Luna (restaurant) lies.**  The person at the restaurant says the person at the hotel (Elowen) lies. \textbf{\color{red} Later, the person at the restaurant says the person at the campground (Quan) lies.} Since Quan tells the truth, Luna (restaurant) must be lying.

...

Therefore:
* **Anika (museum): lies**
* **Luna (restaurant): lies**
* **Farid (movie theater): lies**

**Answer: no, no, no** 
\end{tcolorbox}
\end{paracol}

\noindent\begin{minipage}{\textwidth}
\captionof{figure}{Example of an entry in our verification benchmark. The question is sourced from the LiveBench Reasoning benchmark, and the two responses are generated by Gemini v1.5 Pro. The green response has the correct final answer; the red response has the wrong final answer due to hallucinating a non-existent clause.}
\label{fig:example}
\vspace{0.4cm}
\end{minipage}

\noindent
On each entry, our benchmark studies verification accuracy on two tasks:
\begin{enumerate}
    \item \textbf{Scoring task.} When given only the question and one of the responses, is the model able to discern the correctness of the response?
    \item \textbf{Comparison task.} When provided the whole tuple with the correctness labels of the responses masked and a guarantee that at least one response is correct, is the model able to discern which response is correct and which is incorrect?
\end{enumerate}

\noindent
The scoring task is also evaluated over a separate set of (question, response) pairs where the response reaches the correct final answer by coincidence but contains fatal errors and should be labeled by a reasonable verifier as being incorrect; an example can be found in Appendix~\ref{app:examplebenchmark}.
In the scoring task, models are provided only with the task description; in the comparison task, models are provided only with the task description and a suggestion to identify disagreements between responses in its reasoning.

Table~\ref{tab:benchmark} lists the baseline performances of current commercial model offerings on this benchmark.
Gemini v1.5 Pro is omitted from the benchmark as the entries in the benchmark are curated from the residuals of Gemini v1.5 Pro.
The prompts used in Table~\ref{tab:benchmark} are provided in Appendix~\ref{app:benchmarkprompts}.

As we previously observed, and has been noted in prior works \citep{tyen_llms_2024, kamoi_evaluating_2024}, verification errors are typically due to low recall.
Even the easier comparison task, models perform only marginally better---and often worse---than random chance.
In many cases, Consistency@5 performs worse than one-shot inference because Consistency simply averages out noise from an output distribution, meaning that a model biased towards producing an incorrect answer will do so with higher probability under Consistency.
Addressing these deficits in verification capabilities---which we see as low-hanging fruit for post-training---would enable not only better sampling-based search, but also other downstream applications of verification including reinforcement learning \citep[e.g.][]{o1-preview,deepseekai2025deepseekr1incentivizingreasoningcapability}, data flywheeling \citep[e.g.,][]{welleck_generating_2022}, and end-user experience (see Section~\ref{sec:related} for further discussion).


\begin{table}[htbp]
\centering
\begin{tabular}{llcccccc}
\toprule
\textbf{Model} & \textbf{Metric} & \multicolumn{3}{c}{\textbf{Scoring Accuracy}} & \multicolumn{1}{c}{\textbf{Comparison Accuracy}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-6}
 &  & \textbf{Correct} & \textbf{Wrong} & \textbf{Flawed} &  \\
\midrule
\multirow{2}{*}{GPT-4o} & Pass@1    & 76.5\%  & 31.0\% & 22.2\% & 43.2\%\\
 & Consistency@5 & 77.4\% & 30.0\% & 11.1\% & 35.4\% \\
\midrule
\multirow{2}{*}{Claude 3.5 Sonnet} & Pass@1 & 89.6\% & 22.5\% & 33.3\% & 56.1\% \\
 & Consistency@5 & 90.3\% & 17.5\% & 33.3\% & 61.2\% \\
\midrule
\multirow{2}{*}{o1-preview} & Pass@1 & 100\% & 68.8\% & 80.0\% & 84.5\% \\
 & Consistency@5 & 100\% & 79.4\% & 88.8\% & 92\% \\
\midrule
\multirow{2}{*}{Gemini 2.0 Flash} & Pass@1 & 73.5\% & 44.5\% & 60\% & 58\%  \\
 & Consistency@5 & 77.4\% & 42.5\% & 66.6\% & 58.7\% \\
\midrule
\multirow{2}{*}{Gemini 2.0 Thinking Flash} & Pass@1 & 75.4\% & 56.5\% & 53.3\%  & 80\%  \\
 & Consistency@5 & 77.4\%  & 55\% & 55.5\%  & 89.1\% \\
\midrule
\multicolumn{2}{c}{Random guessing}  & 80\% & 20\% & 20\% & 50\% \\
\bottomrule
\end{tabular}
\caption{Accuracy rates of commercial language models on our verification benchmark. For the task of response scoring (Scoring Accuracy), accuracy rates are broken down for entries that require identifying a correct response as being correct (Correct), entries that require identifying a wrong response as being wrong (Wrong), and entries that require identifying a wrong response that coincidentally reaches the correct answer as being wrong (Flawed).
GPT-4o and Claude 3.5 Sonnet only perform marginally better than random guessing across all tasks. o1-Preview performs better, but still fails to identify 20-30\% of wrong responses.
}
\label{tab:benchmark}
\end{table}
