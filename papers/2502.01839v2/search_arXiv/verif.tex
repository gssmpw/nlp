\noindent
In the process of scaling sampling-based search, we identified two general principles for eliciting more accurate language model self-verification, that may be of independent interest.
\begin{enumerate}
    \item \emph{Compare responses to localize errors.} 
    Disagreements between candidate solutions strongly signal the potential locations of their errors.
    This can be leveraged to combat the fact that language models have low recall (i.e., often overlook) when asked to identify mistakes and hallucinations \citep{tyen_llms_2024,kamoi_evaluating_2024}, as models are able to identify errors when provided their locations  \citep{tyen_llms_2024}.
    Specifically, we can improve the self-verification of a candidate response by providing the verifier with other responses to compare the candidate against---an instance of implicit scaling.

    \item \emph{Rewrite responses for output style suitability.}
    The optimal output style of a language model should depend on the task.
    Writing in a linear chain of thought--- which includes detailing reasoning before committing to a claim---is effective when generating responses (search) \citep{DBLP:conf/nips/Wei0SBIXCLZ22}.
    However, responses are easier to verify when written rigorously, hierarchically, and modularly.
    This can be leveraged by having verifiers first rewrite candidate responses in, e.g., an expanded mathematically conventional theorem-lemma-proof format rather than directly evaluating chains-of-thought.
\end{enumerate}

\noindent
These principles also provide levers for scaling self-verification capability with test-time compute, including by (1) sampling and providing verifiers with more responses to compare between and (2) rewriting responses with increasing rigour and structure.

\subsection{Sampling-Based Search Implementation}
We now detail our minimalist implementation of sampling-based search (summarized in Algorithm~\ref{alg:verification-pipeline}) that uses only parallelizable blackbox queries to a language model.
It generates candidate responses by randomly sampling from models and select responses by asking models to self-verify; prompts are identical across all benchmarks and provided in the source code.

\paragraph{Step 1: Generate Candidate Responses.}
A language model generates $k_{\mathrm{inf}}$ candidate responses (candidate solutions) in parallel to each question, using temperature $\sigma_{\mathrm{inf}}$.

\paragraph{Step 2: Verify Candidate Responses.}
A language model generates $k_{\mathrm{verif}}$ binary ``verification scores'' for each candidate in parallel, indicating whether its final answer is correct.
Each scoring attempt is a single conversation thread that rewrites the response as a theorem, supporting lemmas, and proofs (examples in Appendix~\ref{app:rewrite}) and systematically scans for errors.
The highest scoring response is selected.

\paragraph{Tie-Break: Compare Candidate Responses.}
When the three highest scoring candidates score within 5\% of one another and disagree on the final answer, a language model directly compares the responses in pairwise matchups.
Each matchup is a single conversation thread that identifies where responses diverge and, at each such point, determines which side is correct.
Each matchup is repeated \(k_{\mathrm{tie}} = 100\) times.
The response with the most wins in the round-robin tournament is selected.

\subsection{Ablation Studies}
We can individually ablate the practices of comparing and rewriting candidate responses to confirm their role in eliciting greater verification capability.

\paragraph{Ablating comparisons.}
The step of asking models to directly compare candidate solutions with similar verification scores significantly increases sampling-based search performance.
This is demonstrated in Table~\ref{tab:ablation}, where we depict the accuracy rates from Table~\ref{tab:main-sota} alongside the accuracy rates after ablating the tie-breaking step.
These comparisons have the greatest impact when models struggle from low recall and excessively assign high verification scores.
On the MATH benchmark, which sees the greatest lift from comparisons, the average verification score of the top 3 candidate responses is nearly 90\%.
Recall that, as a result, the figures reported in Section~\ref{sec:allscaling} that omit tiebreaking significantly underestimate sampling-based search performances (Verification@k).

\begin{table}[htbp]
\centering
\begin{minipage}[c]{0.68\textwidth}
\vspace{0cm}
\begin{tabular}{lccccc}
\toprule
\textbf{Dataset} & \textbf{Cons@200} & \multicolumn{2}{c}{\textbf{Verification@200}} \\
                 &                   & \textbf{Without} & \textbf{With Tie-Break} \\
\midrule
MATH              & {460} / {500}   & {457} / {500}   & {467} / {500} \\
LiveBench Math        & {118} / {200}   & {125} / {200}   & {135} / {200} \\
LiveBench Reasoning & {75} / {140}    & {94} / {140}    & {97} / {140} \\
AIME              & {4} / {14}      & {7} / {14}      & {8} / {14}   \\
\bottomrule
\end{tabular}
\end{minipage}
\hfill
\begin{minipage}[c]{0.31\textwidth}
\vspace{0cm}
\includegraphics[width=\textwidth]{figs/tiebreak.png}
\end{minipage}

\caption{Accuracy rates of Gemini v1.5 Pro using sampling-based search, with and without tie-breaking.
Tie-breaking provides most of Verification@200's gains on Consistency@200 (Cons@200) on MATH and LiveBench Math, and smaller gains on AIME and LiveBench Reasoning.
}
\label{tab:ablation}
\end{table}


\paragraph{Ablating rewritings.}
We explored a limited number of prompts for self-verification, including prompts which omit instructing the model to rewrite responses.
We did not perform further prompt optimization and expect refinements would boost accuracy.
Table~\ref{tab:prompt_tuning_results} shows each promptâ€™s probability of mislabeling correct solutions (false positive) and incorrect solutions (false negative), with the former generally having a more severe impact on downstream performance.
We evaluated these prompts on 1,080 candidate responses to 54 level-5 questions from the MATH training split, and 120 candidate responses to 6 questions from AIME 2023.
A response is marked as incorrect if, of 20 verification attempts, the number finding an error in the solution exceeds the equal error rate threshold.


\emph{Main} refers to manually written prompts used in our experiments.
\emph{Shortened} refers to a shorter variant of ``Main'' that omits, e.g., instructions to avoid truncation.
\emph{Without Rewrite} refers to a variant of ``Main'' that omits instructing the verifier to first rewrite responses.
\emph{Split-Context} refers to a variant of ``Main'' that creates separate conversation threads to individually verify pieces of the response.

The gap between the performance of ``Main'' and ``Without Rewrite'' demonstrates that ablating the rewriting of solutions negatively impacts verification performance. Similarly, the gap with ``Split-Context'' demonstrates that splitting the verification process into separate conversation threads sharply decreases performance due to low precision, which we attribute to miscalibration.

\begin{table}[htbp]
\centering
    \hfill
\begin{minipage}[t]{0.5\textwidth}
\vspace{0.4cm}
    \begin{tabular}{lcc|cc}
        \toprule
        \textbf{Prompt Style} & \multicolumn{2}{c}{\textbf{MATH}} & \multicolumn{2}{c}{\textbf{AIME}} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
        & \textbf{FPR} & \textbf{FNR} & \textbf{FPR} & \textbf{FNR} \\
        \midrule
        Main            & 14\% & 17\% & 7\%  & 7\%  \\
        Shortened     & 17\% & 17\% & 7\%  & 7\%  \\
        Without Rewrite & 16\% & 18\% & 11\% & 12\% \\
        Split-Context   & 19\% & 23\% & 11\% & 14\% \\
        \bottomrule
    \end{tabular}
    \end{minipage}
    \hfill
\begin{minipage}[t]{0.45\textwidth}
\vspace{0cm}
\includegraphics[width=\textwidth]{figs/overlapping_circles.png}
\end{minipage}
    \caption{Verification scoring accuracy rates of the Gemini v1.5 Pro model for various prompts. False positive rate (FPR) refers to how often a correct response is labeled as incorrect; false negative rate (FNR) refers to how often an incorrect response is labeled as correct. %
    }
    \label{tab:prompt_tuning_results}
\end{table}
