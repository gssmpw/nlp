\subsection{Smaller Models}

\begin{table}[htbp]
\centering
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{Method} & \textbf{AIME} & \textbf{MATH} & \textbf{LiveBench Math} & \textbf{LiveBench Reasoning} \\
\midrule
\multirow{3}{*}{Pro v1.5} 
& Pass@1 & 1 / 15 & 426 / 500 &  104 / 200 & 63 / 140 \\
& Consistency@200 & 4 / 15  & 460 / 500 & 118 / 200  & 75 / 140 \\
& Verification@200 & 8 / 15 & 467 / 500 & 135 / 200  & 97 / 140 \\
\midrule
\multirow{3}{*}{Flash v1.5} 
& Pass@1 & 2 / 15 & 407 / 500 &  96 / 200 & 65 / 140 \\
& Consistency@200 & 3 / 15  & 440 / 500 & 92 / 200  & 84 / 140 \\
& Verification@200 & 5 / 15 & 445 / 500 & 104 / 200  & 84 / 140 \\
\midrule
\multirow{1}{*}{Pro+Flash v1.5} 
& Verification@200 & 7 / 15 &  456 / 500 & 119 / 200  & 84 / 140 \\
\bottomrule
\end{tabular}
\begin{minipage}[t]{0.57\textwidth}
\vspace{0.6cm}
\centering
\includegraphics[width=\textwidth]{figs/gemini_comparison.png}
\end{minipage}
\hfill
\begin{minipage}[t]{0.38\textwidth}
\vspace{0.2cm}
\includegraphics[width=\textwidth]{figs/gemini_comparison2.png}
\end{minipage}
\caption{Accuracy rates with sampling-based search using either the Gemini v1.5 Pro model to both generate and verify responses (Pro), Gemini v1.5 Flash to both generate and verify responses (Flash), or Gemini v1.5 Pro model to generate responses and v1.5 Flash to verify responses (Pro+Flash). Verification@200 exceeds Consistency@200 for all model choices, while Pro+Flash Verification@200 matches or exceeds Pro Consistency@200.}
\label{tab:flash-sota}
\end{table}

\noindent
We also observe sampling-based search to be a powerful tool for enhancing smaller, lower-cost models.
Here, we apply sampling-based search to Gemini v1.5 Flash model, which has a nearly 20x lower inference cost than Gemini v1.5 Pro.
Table~\ref{tab:flash-sota} lists the performance of using the Flash model to evaluate candidate responses generated by the Pro model (Pro+Flash), and the performance of using the Flash model end-to-end for sampling-based search (Flash).
Sampling-based search still provides a significant improvement in performance for both Flash and Pro+Flash.
Moreover, Verification@200 still provides significant improvements over Consistency@200, albeit lesser in magnitude than for end-to-end use of Gemini Pro.
In addition, Flash Verification@200 using Gemini Flash is competitive with Pro Consistency@200, while Pro+Flash Verification@200 exceeds Pro Consistency@200.
We highlight that Pro+Flash Verification@200 has roughly the compute cost of Consistency@500---as our sampling-based search implementation is minimally optimized for efficiency, we expect costs to further decrease.


\subsection{Performance by Subtask}

\begin{table}[htbp]
\centering
\begin{tabular}{lcccccc}
\toprule
\textbf{Dataset} & \textbf{Cons@200} & \textbf{Verif@200} & \multicolumn{2}{c}{\textbf{Improvement (\%)}} & \textbf{Pass@200} & \textbf{\# Questions} \\
\cmidrule(r){4-5}
& & & \textbf{(Abs)} & \textbf{(Rel)} & & \\
\midrule
Berkeley MATH & 92.0\% & 93.4\% & 2\% $\uparrow$ & 20.0\% & 99.0\% & 500 \\
AIME 2024 & 26.7\% & 53.3\% & 100\% $\uparrow$ & 57.1\% & 73.3\% & 15 \\
Web-of-Lies-v2* & 75.5\% & 91.8\% & 22\% $\uparrow$ & 66.5\% & 100.0\% & 49 \\
Spatial* & 33.3\% & 46.7\% & 40\% $\uparrow$ & 21.5\% & 95.6\% & 45 \\
Zebra Puzzle* & 50.0\% & 67.4\% & 35\% $\uparrow$ & 36.4\% & 97.8\% & 46 \\
Competition\textsuperscript{\textdagger} & 66.2\% & 83.1\% & 26\% $\uparrow$ & 63.1\% & 93.0\% & 71 \\
AMPS Hard\textsuperscript{\textdagger} & 70.6\% & 77.7\% & 10\% $\uparrow$ & 33.5\% & 91.8\% & 85 \\
Olympiad\textsuperscript{\textdagger} & 25.0\% & 22.7\% & 9\% $\downarrow$ & -11.2\% & 45.5\% & 44 \\
\bottomrule
\end{tabular}

\caption{
The Pass@200, Consistency@200 (Cons@200), and Verification@200 (Verif@200) accuracy rates of the Gemini v1.5 Pro model using sampling-based search.
LiveBench Math\textsuperscript{\textdagger{}} and LiveBench Reasoning\textsuperscript{*} numbers are divided per task.
Absolute \% Increase (Abs) is the percentage improvement of Verification@200 over Consistency@200.
Relative \% Increase (Rel) is (Verification@200 - Consistency@200) / (Pass@200 - Consistency@200).
}
\label{tab:performance_metrics}
\end{table}

\noindent
The LiveBench benchmarks each consist of multiple subtasks.
In Table~\ref{tab:performance_metrics}, we break down the numbers reported in Table~\ref{tab:main-sota} for each of these subtasks.
We also provide in Table~\ref{tab:performance_metrics} the Pass@200 scores of the Gemini Pro model, which measure the probability that of 200 attempted responses to a question at least one is correct.
Pass@200 upper bounds what one can hope to achieve through Verification or Consistency.
Verification provides the greatest gains on AIME 2024, Web-of-Lies, Competition, and Zebra Puzzle.
In contrast, Verification does not improve on Consistency on the Olympiad task of the LiveBench Math benchmark.
We attribute this to the unique question design of LiveBench Olympiad task questions, which is incompatible with our implementation of Verification (see Appendix~\ref{app:oddity}).
