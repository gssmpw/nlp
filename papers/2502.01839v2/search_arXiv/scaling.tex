This section examines how reasoning capability scales with two fundamental test-time compute axes:

\begin{itemize}[label=$\bullet$, left=0.1cm]
    \item \textbf{Search} refers to the compute used to discover candidate solutions. In this section, our knob for scaling search is the number of responses sampled for each reasoning problem ($k_{\mathrm{inf}}$ in Algorithm~\ref{alg:verification-pipeline}). %
    \item \textbf{Verification} refers to the compute used to scrutinize candidate solutions. Our knob for scaling verification is the number of verification scores we compute and average over per solution ($k_{\mathrm{verif}}$). %
\end{itemize}
For computational reasons, this section uses a streamlined form of Algorithm~\ref{alg:verification-pipeline} that omits tie-breaking. This, for example, results in significant underestimates of Verification@k on MATH (see Table~\ref{tab:ablation}). All figures are averaged over 20 random seeds, where each run subsamples solutions and verification scores from a primary run that sampled 200 solutions per question and 50 verification scores per solution.

\subsection{Scaling Trends}
\label{subsec:scaling}

\begin{figure}[tbp]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figs/AIME_absolute.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figs/MATH_absolute.png}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figs/LiveBenchMath_absolute.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figs/LiveBenchReasoning_absolute.png}
    \end{subfigure}

    \caption{Heatmap of Gemini v1.5 Pro accuracy rates using sampling-based search (without tie-breaking) as the number of responses generated (x-axis) and verification attempts (y-axis) increase.
    Warmer colors indicate higher accuracy (cubic scale).
    The largest gains occur when scaling both search and verification, with the strongest trend on AIME.
    }
    \label{fig:four_images}
\end{figure}

Figure~\ref{fig:four_images} provides a heatmap of Verification@k on each benchmark in Table~\ref{tab:main-sota}  as we scale search and verification.
In addition to clear burn-in costs along both axes of scale, we can observe that the largest performance gains are realized when search and verification are both scaled.
These trends also indicate that the performances of sampling-based search, as reported in Table~\ref{tab:main-sota}, have not yet been scaled to saturation on these benchmarks.
This scaling trend is strongest on the AIME benchmark, where performance is bottlenecked by $k$ (search); we attribute this bottleneck to the difficulty of the AIME questions resulting in correct solutions only appearing with very low probability (see Table~\ref{tab:verifscores}).

\subsection{Implicit Scaling}
\label{subsec:implicit}

\begin{figure}[tbp]
    \centering
    \includegraphics[width=\textwidth]{figs/side_by_side_plots.png}
    \caption{
    Plot of Gemini v1.5 Pro accuracy rates using sampling-based search (without tie-breaking and with $k_{\mathrm{verif}} = 50$) on \emph{ambiguous questions only} as the number of responses generated increases.
    A question is ambiguous when the model generates at least one candidate response with a correct final answer.
    Accuracy on ambiguous questions increases with search.}
    \label{fig:four_images_norm2}
\end{figure}

Scaling sampling-based search along the \emph{search} axis by sampling more solutions, i.e. increasing $k$, should have two effects on performance that partially cancel out: (1) the verifier must discriminate between more solutions, increasing the likelihood of error and (2) the generator is more likely to produce at least one solution that reaches a correct final answer, i.e. Pass@k increases.

\begin{figure}[htbp] %
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
    \hfill
        \includegraphics[width=0.9\textwidth]{figs/AIME_relative.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figs/LiveBenchReasoning_relative.png}
    \hfill
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
    \hfill
        \includegraphics[width=0.9\textwidth]{figs/LiveBenchMath_relative.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figs/MATH_relative.png}
    \hfill
    \end{subfigure}
    
    \caption{
    Heatmap of Gemini v1.5 Pro accuracy rates using sampling-based search (without tie-breaking) on \emph{ambiguous questions only} as the number of responses generated (x-axis) and verification attempts (y-axis) increase.
    Warmer colors indicate higher accuracy (linear scale).
    A question is ambiguous when the model generates at least one candidate response with a correct final answer.
    Accuracy on ambiguous questions increases with search (x-axis).
    }
    \label{fig:four_images_norm}
\end{figure}
To isolate the first effect, we study the model's Verification@k accuracy on ``ambiguous'' questions: questions where at least one of the modelâ€™s \(k\) candidate solutions reaches the correct final answer (note that Pass@k equals the number of ambiguous questions).
Figure~\ref{fig:four_images_norm2} and Figure~\ref{fig:four_images_norm} do exactly this, plotting Verification@k accuracy measured only on ambiguous questions from each benchmark.
To reduce noise in these figures, we deterministically omit benchmark questions that Consistency@200 answers correctly or where, with high probability, 50 random responses result in either all correct or all incorrect final answers.

After controlling for the growth of Pass@k, we should expect a trend of decreasing accuracy if we increase $k$ but keep the number of verification attempts constant.
However, Figure~\ref{fig:four_images_norm2} shows the reverse trend: accuracy increases with $k$.
This demonstrates an \emph{implicit scaling} of verification accuracy, where increasing the number of generated responses increases not only the chance that at least one response is correct (Pass@k) but also the chance that at least one of the correct responses is of higher quality.
Here, quality can be understood as the rigour or flawlessness of a response; a lower quality solution may be generally correct but fail to justify a non-trivial step or err in a non-critical step of its reasoning.

Implicit scaling suggests that verification should become more accurate, and sampling-based search should become more effective, with the use of more capable base models that produce more sound reasoning and compelling proofs of correctness.
Because the number of ambiguous questions strictly increases with more candidate solutions, the implicit scaling effect also explains the overall accuracy scaling gains in Figure~\ref{fig:four_images}: larger $k$ increases both the number of ambiguous questions (Pass@k) and accuracy on the set of ambiguous questions.

\subsection{The Long Tail of Response Distributions}
\label{subsec:tail}

\begin{figure}[htbp] %
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/AIME_lines.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/MATH_lines.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/LiveBenchMath_lines.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/LiveBenchReasoning_lines.png}
    \end{subfigure}

    \caption{Line graph depicting the accuracy rates of the Gemini v1.5 Pro model using sampling-based search as the number of candidate responses generated is scaled upwards.
    The number of verification attempts is fixed at 50 for all plots.
    The depicted accuracies are obtained without tie-breaking and may be lower than reported elsewhere.
    Verification@k improves with $k$ even when Consistency@k stagnates on AIME and LiveBench Reasoning.
    }
    \label{fig:four_images2}
\end{figure}

\noindent
We can directly observe Verification@k scaling beyond the saturation point of Consistency@k in Figure~\ref{fig:four_images2}, where we plot their performance after fixing the number of verification attempts at 50.
On AIME, the most technically challenging benchmark, Verification@k demonstrates power law scaling even as Consistency@k begins to plateau.
The rapid saturation of Consistency@k can be attributed to the fact that, while it is effective at small scales in averaging out noisy mistakes, it necessarily plateaus as it converges on the most probable response;  for example, Consistency@50 has the same accuracy as Consistency@10,000 on AIME.
Consider cheaply sampling a vast set of solutions from a weak but ergodic model: Consistency@k is unlikely to return a correct solution, but an effective verifier should still be expected to detect rare but correct solutions in the long-tail of the response distribution.
We find an example of this on the AIME 2024 exam, where the Gemini v1.5 model struggles to identify the correct answer to Problem 11 on Exam II.
Table~\ref{tab:verifscores} shows the final answers from 200 randomly sampled Gemini v1.5 solutions, of which only one is correct (``601,'' in green).
Consistency returns the incorrect answer of ``1'' (in red), which appears in over half the responses. In contrast, Verification successfully identifies the solution reaching the correct answer from the response distribution's long-tail, assigning a $\le$36\% score to each solution reaching a final answer of ``1'' but a 98\% score to the single solution reaching ``601''.
\begin{table}[htbp] %
\centering
\begin{minipage}[t]{0.42\textwidth}
\vspace{0pt}
\begin{tcolorbox}[title={Problem 11, AIME 2024}]
\raggedright
    Find the number of triples of nonnegative integers $(a, b, c)$ satisfying $a + b + c = 300$ and $a^2 b + a^2 c + b^2 a + b^2 c + c^2 a + c^2 b = 6,000,000$.
\end{tcolorbox}

\includegraphics[width=\textwidth]{figs/longtail.png}
\end{minipage}
\hfill
\begin{minipage}[t]{0.56\textwidth}
\vspace{0pt}
\begin{tabular}{cccc}
\toprule
\textbf{Verification Score} & \multicolumn{2}{c}{\textbf{Final Answer}} & \textbf{\# Solutions} \\
\midrule
\rowcolor{green}
0.98 & 601    & (Correct) & \textbf{1}   \\
0.76 & 6      & (Wrong)   & 11  \\
0.52 & 0      & (Wrong)   & 14  \\
0.40 & 7      & (Wrong)   & 21  \\
0.38 & 4      & (Wrong)   & 10  \\
\rowcolor{red} 0.36 & 1      & (Wrong)   & \textbf{124} \\
0.22 & 10     & (Wrong)   & 2   \\
0.20 & 3      & (Wrong)   & 9   \\
0.18 & 301    & (Wrong)   & 1   \\
0.16 & 45451  & (Wrong)   & 1   \\
0.14 & 101    & (Wrong)   & 2   \\
0.06 & 2      & (Wrong)   & 1   \\
0.04 & 45151  & (Wrong)   & 1   \\
0.04 & 303    & (Wrong)   & 1   \\
0.00 & 100    & (Wrong)   & 1   \\
\bottomrule
\end{tabular}
\end{minipage}

\raggedright
\caption{The final answers identified by the Gemini v1.5 Pro model to Problem 11 on AIME 2024, sorted by verification score and annotated with their multiplicity in 200 solution generations.
The correct final answer (green) is only found by 1 generated response whereas Consistency@200 selects an incorrect final answer (red) that is found by 124 generated responses.} %
\label{tab:verifscores}
\end{table}
Scaling verification capability is key to driving improved search, allowing for discerning between answers that appear correct with 98\% vs. 76\% confidence.
The fact that verification can be used to so effectively leverage the long-tail of model response distributions also suggests that Pass@k, not Pass@1, should be the key performance metric for search applications.
Existing post-training techniques (e.g., reinforcement learning from human feedback (RLHF) \citep{DBLP:conf/nips/Ouyang0JAWMZASR22}) which explicitly optimize for Pass@1 may potentially be doing so at the expense of Pass@k and inhibiting search capability.
