\paragraph{Test-time compute.}
Many of the recent advances in language model reasoning capabilities can be traced to increasing use of \textit{test-time compute}.
Inference strategies like chain-of-thought reasoning \citep{DBLP:conf/nips/Wei0SBIXCLZ22}, tree-of-thoughts \citep{DBLP:conf/nips/YaoYZS00N23} and self-critique  \citep{valmeekam2023largelanguagemodelsreally} result in improved reasoning performance at the cost of forming longer responses.
Reinforcement learning has emerged as a particularly successful strategy for effectively leveraging more test-time compute, wherein models learn from exploration to form lengthy chain-of-thought outputs that incorporate backtracking and search, despite not being explicitly taught to do so \citep{o1-preview, deepseekai2025deepseekr1incentivizingreasoningcapability}.
Inference-time model adaptation, whether through many-shot learning~\citep{agarwal2024manyshotincontextlearning, anil2024jailbreaking} or finetuning~\citep{akyurek2024surprisingeffectivenesstesttimetraining}, provides another avenue when training data is available.
We study sampling-based search: obtain a set of candidate responses from a model and apply an aggregation method to select a response, such as self-consistency/plurality voting \citep{DBLP:conf/iclr/0002WSLCNCZ23} or selecting a response with a reward/verifier model \citep{cobbe_training_2021}.
These various methods for scaling test-time compute are complementary; for example, sampling-based search can also be used on models trained to produce longer outputs.
We note that it is possible for models trained to produce long chains of thought to perform something resembling sampling-based search internally, in which case we still expect our observed scaling trends to hold.
However, we also expect explicit sampling-based search will remain indispensable, due to its greater parallelism and robustness than internally implemented search.

\paragraph{Scaling sampling-based search.}
The paradigm of sampling-based search provides three main knobs for scaling: generation, sampling, and selection.
While the cost of \emph{generating} each individual response can be scaled with previously mentioned interventions, such as chain-of-thought \citep[e.g.][]{DBLP:conf/nips/Wei0SBIXCLZ22}, reinforcement learning \citep[e.g.][]{o1-preview}, or inference-time adaptation \citep[e.g.][]{anil2024jailbreaking}, the cost of \emph{sampling} a set of responses can be scaled by increasing the number of responses generated~\citep{DBLP:conf/iclr/0002WSLCNCZ23,snell2024scalingllmtesttimecompute}.
We use random sampling to generate each set of candidate responses, which means the latter corresponds to simply taking more random draws.
However, this sampling can also be implemented in an agentic fashion, with a central model delegating the generation of responses so as to perform search more systematically.
The process of \emph{selecting} a response can be scaled by using more expensive rules: self-consistency provides a simple plurality voting rule at the lowest-cost end of the spectrum \citep{DBLP:conf/iclr/0002WSLCNCZ23}, while language model self-verification \citep[e.g.][see below]{xue_rcot_2023} and learned verification/reward models \citep[e.g.][see below]{cobbe_training_2021} provide a range of selection strategies that vary in cost and capability.
For more fine-grained control over the scaling of self-verification in our experiments, we apply plurality voting~\citep{DBLP:conf/iclr/0002WSLCNCZ23} to self-verification and vary our number of verification attempts per response.

\paragraph{Verification of language model outputs.}
A large body of recent work has studied the self-verification capabilities of large language models \citep[e.g.,][]{cobbe_training_2021,kadavath_language_2022,saunders_self-critiquing_2022,DBLP:conf/nips/KimBM23,DBLP:conf/nips/XieKZZKHX23,DBLP:conf/emnlp/WengZX0HLSLZ23,zhang_how_2023,xue_rcot_2023,li_making_2023,liu_large_2024,chow_inference-aware_2024,DBLP:conf/acl/JiangSYL0LK24,DBLP:conf/acl/DhuliawalaKXRLC24,snyder_early_2024,wu_large_2024,DBLP:conf/iclr/0009CMZYSZ24,kamoi_evaluating_2024,kamoi_when_2024,orgad_llms_2024,wen_language_2024,tyen_llms_2024,chen_simple_2024,kumar_training_2024,qu_recursive_2024,zhang_generative_2024,ko_real-time_2025,DBLP:conf/icml/HavrillaRNDZHR24}.
While some works---including ours---simply ask models to perform verification and parse the response, others have proposed custom methods of performing self-verification, including: recreating the problem from the response \citep{xue_rcot_2023,wu_large_2024}, masking and re-filling parts of the response \citep{DBLP:conf/emnlp/WengZX0HLSLZ23,DBLP:conf/acl/JiangSYL0LK24}, creating a rubric \citep{DBLP:conf/acl/DhuliawalaKXRLC24}, or asking models to choose from options \citep{DBLP:conf/nips/XieKZZKHX23,chen_simple_2024}.
Our work does not focus on optimizing for self-verification or advocate for any particular strategy.
However, in the course of performing our scaling study, we did identify several previously unstudied principles of self-verification that only arise at sufficiently large scale and may be of independent interest, including implicit scaling, output style suitability, and the importance of directly comparing responses.
Other related bodies of work study the learning of verifiers, often on top of a pretrained large language model~\citep[e.g.][]{cobbe_training_2021, saunders_self-critiquing_2022, li_making_2023, DBLP:conf/icml/HavrillaRNDZHR24, kumar_training_2024, qu_recursive_2024, chow_inference-aware_2024, zhang_generative_2024}, and the use of external tools for verification~\citep[e.g.][]{min_factscore_2023, gou_critic_2024, gao_embedding_2024,DBLP:conf/nips/KimBM23}.
We did not train customized verification models or permit verifier use of external tools in the listed experiments, as we found blackbox model access to be sufficient for effective verification at scale.
The limitations of model self-verification capabilities are also well-studied~\citep{kamoi_evaluating_2024, tyen_llms_2024, DBLP:conf/iclr/0009CMZYSZ24}, and can be remedied with external information \citep{DBLP:conf/iclr/0009CMZYSZ24} or hints for localizing errors \citep{tyen_llms_2024}.
Models especially struggle with self-diagnosing hallucinations \citep{zhang_how_2023,orgad_llms_2024,snyder_early_2024}, despite awareness of their own limitations \citep{kadavath_language_2022}, and are often incentivized to obfuscate errors \citep{wen_language_2024}.

\paragraph{Applications of verification.}
In addition to being used to select from candidate responses~\citep{cobbe_training_2021,li_making_2023,DBLP:conf/emnlp/WengZX0HLSLZ23,DBLP:conf/acl/JiangSYL0LK24,chen_simple_2024,DBLP:conf/nips/XieKZZKHX23}, verifiers can be used to guide iterative improvements to a model's output by providing feedback to the generating model~\citep{DBLP:conf/nips/KimBM23,xue_rcot_2023,valmeekam2023largelanguagemodelsreally,wu_large_2024,DBLP:conf/iclr/0009CMZYSZ24,DBLP:conf/acl/DhuliawalaKXRLC24, stechly2024chainthoughtlessnessanalysiscot, stechly2024selfverificationlimitationslargelanguage,qu_recursive_2024,DBLP:conf/icml/HavrillaRNDZHR24,ko_real-time_2025}. 
Another important application of verification is in enhancing model capabilities.
For example, verification results for model outputs can be fed back into models as feedback via in-context reinforcement learning~\citep{shinn_reflexion_2023}, reinforcement learning~\citep{uesato_solving_2022,peng_check_2023,madaan_self-refine_2023,kumar_training_2024, chow_inference-aware_2024}, or finetuning~\citep{ welleck_generating_2022,paul_refiner_2024,an_learning_2024,DBLP:journals/tmlr/SinghCAAPGLH0XP24}, in an approach known as data flywheeling.
Verification has also been explored as a means of encouraging models to produce better written responses~\citep{anil_learning_2021,kirchner2024proververifiergamesimprovelegibility}.
From a product perspective, verification capabilities are also important to the workflow of end users \citep{collins_evaluating_nodate}.
