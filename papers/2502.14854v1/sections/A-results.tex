\section{Results}
\subsection{Impact of chapter distance and book length on test set performance}
Figure \ref{fig:accuracy_chapter} shows that test set accuracy peaks when the distance between chapters in a claim is around 40--60K tokens (roughly the midpoint of a book). When that gap shrinks below or stretches beyond 60K tokens, performance dips by about 10\%, leaving no definitive pattern beyond this sweet spot.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{assets/plots/accuracy_by_chapter_distance.pdf}
    \caption{Accuracy of \pipeline-Prolong-balanced on the test set (book-level claims), grouped by the distance (in tokens) between source events in each claim.}
    \label{fig:accuracy_chapter}
\end{figure}


We also find that overall book length does not strongly influence accuracy, except in cases where the text exceeds 110K tokens. In these longer works, accuracy is about 5\% higher than in shorter books, as shown in Figure~\ref{fig:accuracy_book}. While this slight edge may hint at advantages in more expansive narratives, the modelâ€™s broader performance remains steady across most book lengths.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{assets//plots/accuracy_by_token.pdf}
    \caption{\prolongftbalanced's performance on test set, grouped by the number of tokens in each book.}
    \label{fig:accuracy_book}
\end{figure}

We finally examine the possible effect of event placement on \prolongftbalanced's performance on the test set. Interestingly, there is no strong "lost-in-the-middle" effect regarding event placement in the book \cite{liu2024lost}. As shown in Figure \ref{fig:accuracy_event}, accuracy is usually the highest when the claim involves events that appear at the beginning (0-0.4, around 82\%) rather than at the end of the book (0.8-1, around 78\%). 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{assets/plots/accuracy_by_event_placement.pdf}
    \caption{Accuracy of \prolongftbalanced\ on the test set (chapter-level claims), grouped by the event placement in the book (0-0.2 includes events are at the beginning, while 0.8-1 includes events towards the end). }
    \label{fig:accuracy_event}
\end{figure}

\subsection{False claim error analysis} \label{appendix:error-analysis}

In Table \ref{tab:definitions}, we provide detailed definitions for each category from the false claim error analysis in \S\ref{sec:error-analysis}. To explore instances where fine-tuned models still struggle, we conduct an in-depth analysis of \qwenftbalanced\ outputs. Of the 1,000 book-level claims in the test set, the model fails to verify 37 true claims and 97 false claims. This pattern is consistent with findings from NoCha \cite{karpinska_one_2024}, which highlight that models tend to have greater difficulty verifying false claims. Notably, in 95 cases, the model successfully validates the true claim but fails to validate the corresponding false claim. This raises an important question: \textit{What specific perturbations make a false claim appear true to the model?} 

Through careful manual analysis, we derive a taxonomy of such perturbations and present them in Table \ref{tab:error-analysis-dist}. The most frequent perturbations are changes to events (43.2\%) and people (31.6\%), such as altering actions or misattributing roles. Less frequent but notable are modifications to objects (15.8\%), locations (13.7\%), time (6.3\%), and affect (4.2\%). All these perturbations introduce plausible-sounding variations that the model may struggle to detect without fully understanding the narrative.

\begin{table*}[ht]
\centering
\scalebox{0.87}{
\begin{tabular}{p{0.1\textwidth}p{0.9\textwidth}}
\toprule
\multicolumn{1}{c}{\textsc{Category}} & \multicolumn{1}{c}{\textsc{Definition}} \\
\midrule
Event & Refers to the alteration or misrepresentation of the actions, occurrences, or processes described in a claim. \\
\midrule
Person & Involves substituting or misattributing individuals involved in a claim. \\
\midrule
Object & Concerns the manipulation or substitution of physical items or artifacts mentioned in a claim. \\
\midrule
Location & Relates to changing or misrepresenting the places where events occur. \\
\midrule
Time & Pertains to the sequencing or timing of events being distorted or swapped. \\
\midrule
Affect & Deals with altering the emotional state, attitude, or disposition described in a claim. \\
\bottomrule
\end{tabular}}
\caption{Definitions for each category of perturbations that cause a false claim to be misclassified as true in the error analysis in \S\ref{sec:error-analysis}.}
\label{tab:definitions}
\end{table*}



\subsection{Full results on LM Harness and HELMET}
\label{appendix:full_results}
%\chau{add some analyses here}
Table \ref{tab:lm-harness} shows the results of all models on popular short-form benchmarks. Overall, our fine-tuned models, especially \qwenftbalanced, do not degrade that significantly from the baseline models even though it has been fine-tuned on longer data. 
Table \ref{tab:helmet} shows the results of HELMET on recall, RAG, passage re-ranking, and retrieval tasks. Overall, fine-tuned models achieve synthetic recall and RAG scores comparable to the baseline models, while generally delivering improved re-ranking and more robust ICL performance.
\begin{table*}[htpb]
    \centering
    \scriptsize
    %\renewcommand{\arraystretch}{1.3}
    \begin{tabular}{lccccccccc}
        \toprule
        Models & IFEval & BBH & Math lvl5 & GPQA & MMLU-Pro & Arc-Challenge & GSM8K & HellaSwag & WinoGrande \\
        \midrule
        \llamainst\ & \textbf{59.35} & 50.93 & 12.81 & 31.96 & 37.77 & 51.54 & 75.06 & 59.05 & 74.19 \\
        \qweninst\ & 54.00 & 54.60 & \textbf{24.80} & 33.40 & 43.80 & 53.20 & 77.70 & 61.80 & 69.20 \\
        Prolong-instruct-noft & 58.87 & 49.86 & 5.28 & 29.35 & 32.43 & \textbf{58.36} & 68.06 & \textbf{80.75} & \textbf{74.43} \\
        \midrule
        \qwenftbalanced\ & 50.65 & \textbf{55.50} & 22.51 & \textbf{33.82} & \textbf{44.49} & 53.84 & \textbf{78.32} & 61.71 & 69.06 \\
        \prolongftbalanced\ & 7.91 & 48.03 & 5.42 & 27.88 & 32.35 & 50.68 & 60.80 & 60.44 & 73.24 \\
        \llamaftbalanced\ & 45.43 & 50.02 & 12.77 & 30.59 & 37.55 & 53.50 & 74.91 & 78.65 & 73.40 \\
        \prolongwp\ & 11.75 & 47.32 & 3.59 & 30.73 & 26.29 & 50.51 & 39.04 & 76.36 & 70.64 \\
        \prolongftbook\ & 6.39 & 49.31 & 5.53 & 29.47 & 32.38 & 54.78 & 62.02 & 79.27 & 72.93 \\
        \prolongftchapter\ & 4.59 & 49.64 & 5.63 & 29.77 & 32.35 & 54.27 & 61.22 & 79.14 & 74.27 \\
        \bottomrule
    \end{tabular}
    \caption{Performance on popular short-form benchmarks (evaluated using Language Model Evaluation Harness).}
    \label{tab:lm-harness}
    \vspace{-0.1in}
\end{table*}


\begin{table*}[ht]
  \centering
  \resizebox{\textwidth}{!}{%
  \small
  \begin{tabular}{l*{13}{c}}
    \toprule
    & \multicolumn{4}{c}{Synthetic Recall (Ruler)} 
    & \multicolumn{3}{c}{RAG} 
    & \multicolumn{1}{c}{Re-ranking} 
    & \multicolumn{5}{c}{ICL} \\
    \cmidrule(lr){2-5} \cmidrule(lr){6-8} \cmidrule(lr){9-9} \cmidrule(lr){10-14}
    Model & 
      niah\_mk\_2 & recall & 
      niah\_mk\_3 & recall & 
      niah\_mv & recall & 
      \makebox[1.5cm][c]{json\_kv} & 
      \makebox[1.5cm][c]{nqh} & 
      \makebox[1.5cm][c]{triviaqa} & 
      \makebox[1.5cm][c]{hotpotqa} & 
      \makebox[1.8cm][c]{msmarco} & 
      \makebox[1.5cm][c]{trec\_coarse} & 
      \makebox[1.5cm][c]{trec\_fine} \\
    \midrule
    \llamainst       & 98  & 88     & 78.75 & 96   & 48.17 & 80.67 & 56   & 13.66 & 73  & 72  & 91  & 91  & 88 \\
    \qweninst        & \textbf{100.0} & 98.0  & \textbf{83.3}  & 98.8 & 20.3  & 47.3  & 24.0 & 0     & 78.0 & 20.0 & 6.0 & 11.0 & 7.0 \\
    \prolonginst     & 98.0  & 98.0  & 46.8  & 99.3 & \textbf{54.3}  & 91.3  & \textbf{57.7} & 25.0  & 86.0 & 59.0 & \textbf{92.0} & 94.0 & 89.0 \\
    \midrule
    \prolongftbalanced & 99    & 92    & 27.75 & 99   & 52.5  & \textbf{92}    & 51.33& \textbf{27.50} & \textbf{92}   & 72   & 90   & 94   & \textbf{90} \\
    \qwenftbalanced   & 81    & 45    & 45    & 45   & 38.16 & 66.5  & 36   & 3.17  & 73   & 52   & 87   & 78   &  \\[0.5ex]
    \llamaftbalanced  & 98    & \textbf{99}    & 26    & \textbf{100}  & 48.5  & 85.17 & 55.67& 22.90 & 87   & \textbf{81}   & 90   & \textbf{95}   & 86 \\
    \prolongwp        & 99    & 87    & 31.75 & \textbf{100}  & 50.5  & 89.17 & 51.33& 18.30 & 91   & 65   & 91   & \textbf{95}   & 88 \\
    \bottomrule
  \end{tabular}%
  }
  \caption{Performance on HELMET for recall, RAG, passage re-ranking, and retrieval tasks. Fine-tuned models achieve synthetic recall and RAG scores comparable to the baseline models, while generally delivering improved re-ranking and more robust ICL performance.}
  \label{tab:helmet}
\end{table*}




\subsection{Performance of \prolongbase\ on claim verification and narrative understanding benchmarks}
Table \ref{tab:prolong-base-acc} shows accuracy of \prolongbase on long-context reasoning and narrative understanding benchmarks. Even though \prolongbase's test set performance is much worse than \prolonginst, performance on other narrative understanding tasks is comparable between the two models.
\begin{table}[htpb]
    \centering
    \small
    %\renewcommand{\arraystretch}{1.3}
    \begin{tabular}{cccc}
        \toprule
         \pipeline-Test & NarrativeQA & MuSR & $\infty$Bench QA\\
        \midrule
        23.9\% & 46.0\% & 39.8\% & 42.5\%\\
        \bottomrule
    \end{tabular}
    \caption{Performance of \prolongbase\ on long-context reasoning and narrative understanding benchmarks. Even though \prolongbase's test set performance is much worse than \prolonginst, performance on other narrative understanding tasks is comparable between the two models.}
    \label{tab:prolong-base-acc}
    \vspace{-0.1in}
\end{table}




\begin{table}[htbp]
    \centering
    \small
    \begin{tabular}{lcc}
        \toprule
        \multicolumn{1}{l}{} & \multicolumn{1}{c}{ProLong-\texttt{CP}-book} & \multicolumn{1}{c}{ProLong-\texttt{CP}-chap} \\
        \midrule
        Test-book     & 74.8\%  & 78.2\% \\
        Test-chapter  & 75.2\%  & 80.2\% \\
        \midrule
        Overall       & 75.0\%  & 79.2\% \\ 
        \bottomrule
    \end{tabular}
    \caption{Test set performance of models trained exclusively on either book-level claims or chapter-level claims, with accuracy measured for book-level, chapter-level, and overall claims. \texttt{CP} stands for \pipeline.}
    \label{tab:chapter_vs_book}
    \vspace{-0.1in}
\end{table}

\begin{table}[htbp]
\centering
\footnotesize
\begin{tabular}{lc}
\toprule
Models   & Groundedness \\ \midrule
\qweninst\     & 11.9\%\\
\llamainst\       & 16.8\%\\
\prolonginst\ & 19.6\%\\ \midrule
\qwenftbalanced\          & 67.1\%    \\
\llamaftbalanced\         & 75.9\%\\
\prolongftbalanced\       & \textbf{80.6}\%\\
\bottomrule
\end{tabular}
\caption{Percentage of grounded chain of thoughts being generated by baseline and finetuned models. Our fine-tuned models generate much more grounded chain of thoughts.}
\label{tab:cot-groundedness}
\vspace{-0.1in}
\end{table}