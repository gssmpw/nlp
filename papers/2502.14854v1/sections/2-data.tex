% \section{Compressing books into \dataname\ \mi{need better header}}
\section{\pipeline: generating high-quality synthetic data via compression}
% Our goal is to develop a synthetic data generation strategy to advance narrative claim understanding.
% Standard data generation methods, such as Self-instruct~\cite{wang2023selfinstructaligninglanguagemodels} and Alpaca~\cite{alpaca}, provide a strong LLM with a set of seed demonstrations and ask it to generate novel examples that resemble the demonstrations, a paradigm that has evolved to create popular datasets such as UltraChat~\cite{ding_enhancing_2023}.
% \mi{should we discuss synthetic long-context instruction following tasks, e.g. the ones in llama3.1 paper, and contrast our approach?}
In long-context settings, synthetic datasets have typically been created by selecting lengthy documents from an existing corpus and using an LLM to generate input-output pairs given either the entire document~\cite{bai_longalign_2024} or random excerpts~\cite{dubey2024llama, yang_qwen25-1m_2025}.
For our task, however, we show that these methods are insufficient:
\begin{itemize}
    \item \textit{Providing the LLM with the entire document}  results in much noisier data, as we show that producing high-quality, complex claims about long narratives is a fundamentally difficult task even for the best models (\S\ref{data:alt}).
    \item \textit{Providing only an excerpt from the long document}, on the other hand, precludes the model from generating claims that require global reasoning across the entire book.
\end{itemize}
We thus develop a two-stage strategy, \pipeline, which first compresses the narrative into chapter outlines and summaries, then prompts an LLM to produce claims and chain-of-thoughts grounded in the compressed narrative (\S\ref{data:claims_generation}).
We use the dataset generated by \pipeline\ to fine-tune open-weight models in \S\ref{sec:training}. \textbf{All prompts used in this section can be found in \S\ref{appendix:data-construction}.}

% The challenge we face is that na\"ively 
% Here, we discuss a na\"ive approach to generating synthetic claims directly from the book text for \dataname, analyzing its results and highlighting its limitations—specifically, the invalidity and lack of grounding in the generated claims. To produce more well-grounded claims, our final pipeline instead synthesizes claims from chapter outlines and book summaries using GPT-4o and Claude.\footnote{We use "Claude" to refer to Claude-3.5-Sonnet-v1 and "GPT-4o" to gpt-4o-2024-08-06, unless otherwise noted.} An evaluation of our claims and their corresponding chains of thought demonstrates that our pipeline produces more grounded claims with informative reasoning.

\vspace{-0.05in}
\subsection{Task setup}

% In \textit{narrative claim verification}, the model is given a book along with a claim about the book, and asked to determine whether the claim is valid based on the content. The first stage of our pipeline involves collecting books that are the basis for these claims.
Before describing how \pipeline\ works, we first establish the task definition for \textit{narrative claim verification}, then explain how we collect the books that serve as the foundation for this task.

\paragraph{Task definition:}
In \textit{narrative claim verification}, an LLM is given a book and a claim about the book. The task is to determine whether the claim is true or false while providing a clear explanation for its decision. A key aspect of the task is the inclusion of \textbf{true/false} \textit{narrative minimal pairs}~\cite{karpinska_one_2024}, where each false claim closely resembles its true claim counterpart but contains subtle inaccuracies (illustrated in Figure \ref{fig:overview}; see Table \ref{tab:error-analysis-dist} for more examples). The model is considered accurate only if it correctly verifies both claims in a pair, which reduces the chances of the model being correct for the wrong reason.
% We source our books from Project Gutenberg, while the claims and corresponding chain-of-thought reasoning~\cite{wei_chain--thought_2023} are generated by a model (see the last two columns of Figure \ref{fig:overview}). 

%\mi{need some transition here! jumps into details too fast. first explain the task: what are the inputs and outputs? then describe the book source/preproc. }\chau{added transition}
\paragraph{Gathering public domain books:} We collect \textbf{479} fictional books (e.g. Lewis Carroll's Alice in Wonderland, Agatha Christie's The Murder of Roger Ackroyd) from Project Gutenberg,\footnote{\href{https://www.gutenberg.org}{https://www.gutenberg.org}} with an average length of \textbf{90K} tokens\footnote{All token counts reported in this paper are computed using o200k\_base from \url{https://github.com/openai/tiktoken}.} and \textbf{23} chapters.\footnote{We do not include books longer than 128K tokens as many open-weight models cannot process anything beyond that number. We clean the manually downloaded books by removing supplementary content to prevent models from using these metadata as shortcuts for event retrieval.} While using these texts could raise concerns about LLMs memorizing both the source material and generated claims, \S\ref{appendix:memorization} shows that potential exposure to these texts in training data does not impact the task performance of baseline models.\footnote{\href{https://chatgptiseatingtheworld.com/wp-content/uploads/2025/01/Unredacted-Reply-of-Plaintiffs-1.pdf}{https://chatgptiseatingtheworld.com/wp-content/uploads/2025/01/Unredacted-Reply-of-Plaintiffs-1.pdf} shows that Llama models might have been trained on LibGen book data.}
% which might create a false impression of improved performance, 

%\mi{need to clearly define the task. give an example pair (or refer to fig). explain why we are generating paired claims (look at nocha paper if needed). say that model is only accurate if it gets both claims in a pair right. we haven't talked about pairs at all so far so this is really needed here beofre anything else. also mention u produce cot here}

% \paragraph{Preprocessing} We select books with at least 10 chapters and 17K tokens to have enough information to inform the generated claims, but no more than 128K tokens to fit the context window of our base models. The resulting collection features 479 works with an average length of 90K tokens and 23 chapters.

\subsection{Na\"ive claim generation using book texts}
\label{data:alt}
A simple approach for synthetic data generation is to prompt an LLM to generate claims directly from the book text. However, we show that this \naive\ method falls short in a long-context setting, which motivates us to develop \pipeline\ (\S\ref{data:claims_generation}).
% as it tends to produce claims that are more frequently invalid or overly grounded while also being more costly than our final data generation strategy.

\paragraph{The \naive\ method:} We provide Claude-3.5-Sonnet-v1 with the entire book text and prompt it to generate pairs of true/false claims along with corresponding chain-of-thought reasoning in a zero-shot manner. Note that we cannot use few-shot prompting due to the length of the book text. We finally prompt Claude to remove any duplicated claims among the generated claims.\footnote{We do not ask Claude to validate the generated claims, as this is a much more challenging task (as seen by Claude's 40.3\% accuracy on NoCha—one that resembles the very problem we aim to address in this paper.} %\mi{is cot produced here too? would be good to refer back to fig 2 to clarify exactly what is produced }\chau{yes, I added this detail to the first sentence}
% \mi{we are overloading verification. let's use validation instead?} 
% \mi{this para is way too long}

% \mi{can you use some font formatting to highlight the four types in the rest of this paragraph?}
% \mi{not clear that misattribution is a function of the generated cot}
\paragraph{Human validation:} We manually annotate 52 claims generated by \naive\ based on six books (Table~\ref{tab:six-books}).
% \footnote{Annotation took five hours in total on a spreadsheet.}
Across these claims, we identify four types of errors. Table~\ref{tab:naive} shows that \naive\ produces 11.5\% \textit{invalid} claims, often due to mislabeled false claims that are actually valid. It also generates a high number of \textit{misattributed} (28.9\%) claims that cite the wrong chapters in the produced chain-of-thought. 17.3\% of the claims are \textit{duplicated}, likely because Claude, lacking structured input, hallucinates source chapters. 15.4\% of the claims also include \textit{explicit references} to chapter numbers or direct quotes,\footnote{This happens despite explicit formatting instructions.} which makes claim verification significantly easier by revealing the evidence location. Beyond these errors, the \naive\ pipeline is costly at an estimate of \$0.07 USD per claim. To generate 19K claims, this would cost around \$1,330.
% Beyond these errors, the \naive\ pipeline is also costly at an estimate of \$1,284 (\$0.07 per claim).
% \mi{what is this estimate for?? i would state the cost per claim and then the cost to synthesize a dataset of 10K examples for reference}
% 15.4\% of claims also include chapter numbers or direct quotes despite explicit formatting instructions, which we attribute to the absence of few-shot examples. Each claim cites around 3.7 events that are usually key moments in the book, which makes verification more straightforward for the models. Beyond these errors, the \naive\ pipeline is also costly at an estimate of \$1,284 (\$0.20 per claim).
%We employ a bottom-up thematic analysis approach~\cite{strauss1994grounded}, where one author closely examines each claim, cross-references it with the cited sources, and identifies potential errors.

\begin{table*}[htbp]
\centering
\footnotesize
\scalebox{0.8}{
\begin{tabular}{p{0.08\textwidth}p{0.005\textwidth}p{0.005\textwidth}p{0.25\textwidth}p{0.66\textwidth}}
\toprule
\multicolumn{1}{c}{\textsc{Category}} & \multicolumn{1}{c}{\naive} & \multicolumn{1}{c}{\pipeline} &\multicolumn{1}{c}{\textsc{Error Definition}} & \multicolumn{1}{c}{\textsc{Example}} \\
\midrule
Invalid & 11.5\% &9.1\% & The claim is incorrect with respect to the book text, or the true/false claim pair is invalid. & Anne rejects three marriage proposals during her time at Redmond College: from Charlie Sloane, Gilbert Blythe, and Roy Gardner, {\color{purple}{all because she doesn't love them}}. \textit{(This false claim is not entirely false because Anne really didn’t love them or wasn’t initially aware of her romantic feelings.)}. \\
\midrule
Misattribution & 28.9\%& 4.6\%& The claim is valid, but the associated explanation does not cite the correct chapters. & Dr. Sheppard...was the last person known to have seen Roger Ackroyd alive at 8:50 PM on the night of the murder, and he later assisted Hercule Poirot in the investigation while simultaneously {\color{purple}{concealing Ralph Paton in a nursing home}}. \textit{(The explanation cites Chapter 1, 4, 16, and 20, but misses Chapter 24, which mentions that Ralph is in a nursing home)}\\
\midrule
Explicit references & 15.4\% & 0.0\%& The claim is easier to verify since it includes direct quotes and chapter references, eliminating the need for event retrieval. & Alice's pursuit of the White Rabbit, which begins with her following him down a rabbit hole in {\color{purple}{Chapter 1}}, continues throughout her adventure, including an encounter in the King and Queen of Hearts' court in {\color{purple}{Chapter 11}} where the Rabbit acts as a herald.\\
\midrule
Duplication & 17.3\%& 3.0\%& The claim describes the same events as another. Although their content is similar, differences in wording may allow both to pass our deduplication process.& "Dorian Gray's cruel rejection of Sibyl Vane after her poor performance as Juliet leads to her suicide, which Dorian callously dismisses by attending the opera the following night, resulting in the first noticeable change in his portrait [...]" versus "Dorian Gray's cruel rejection of Sibyl Vane after her poor performance as Juliet leads to her suicide, causing the first visible change in his portrait [...] culminate in his murder of Basil Hallward years later [...]." \\
\bottomrule
\end{tabular}
}
\caption{Error types among claims produced by \naive\ (52) and \pipeline\ (66) based on six books from Table \ref{tab:six-books}. Examples are selected from \naive\ claims. \pipeline\ produces much fewer claims with errors than \naive.}
\label{tab:naive}
\vspace{-0.15in}
\end{table*}

\subsection{Claim generation with \pipeline} 
\label{data:claims_generation}
To produce more valid and grounded claims, we use \textit{compressed} representations of the book content, namely chapter outlines and book summaries. These intermediate forms help anchor claims to specific events in the book, reducing the need to search through the entire text for relevant details. Additionally, this approach makes it easier to generate claims about lower-level events, addressing a major limitation of the \naive\ approach. We take a two-step approach to generating claims: (1) first compressing the books into a chapter outline and book summary and then (2) generating pairs of true/false claims at different scopes based on these compressed representations.
% —a compression strategy similar to \citet{xu2023recompimprovingretrievalaugmentedlms}. We use LLMs (Claude and GPT-4o) to construct these claims, which makes the pipeline more cost-effective, time-efficient, and adaptable to new long-form documents compared to using human annotators.

\paragraph{(1a) Compressing books into summaries:} Book summaries provide a global context for claim generation, ensuring that each claim is consistent with the entire book. We prompt GPT-4o to summarize the entire book into a few paragraphs. The summaries average 618 tokens in length.
%\mi{refer to prompt? or cite if u took prompt from an existing paper}

\paragraph{(1b) Compressing books into chapter outlines:} Chapter outlines provide a list of fine-grained events that can be used to construct grounded claims. We prompt Claude to generate the outline chapter-by-chapter.\footnote{We set temperature=0.0, max\_tokens=4096. We use Claude instead of GPT-4o because Claude includes more concrete and objective events for the outline, whereas GPT-4o tends to interpret events before including them.} Specifically, we provide each chapter text and instruct the LLM to return a structured outline, which includes a synopsis, major events (5–7 per chapter), and a character list. Our compression rate is 10.0\%, calculated by averaging the ratio of chapter outline length (8,745 tokens on average) to full book length (90,437 tokens on average) across all books.
%\mi{why not gpt4o? need to explain since you're changing models. footnote is fine}

%\mi{add transition sentence like "now that we've compressed our book, we turn to generating claims"}
\paragraph{(2a) Generating claims from compressed narratives:} Now that we have compressed our book into chapter outlines and book summaries, we use these components to generate true/false claims. To enable reasoning across different token ranges, we synthesize claims at two different scopes: 

\vspace{2pt}
\noindent \textcolor{orange}{\textbf{> Book-level claims:}} Claude is prompted to identify 2–3 key events from the outlines of at least 2 chapters, then use them to generate a claim. These claims require models to have a global understanding spanning multiple parts of the book.
% These claims match the complexity and scope of those found in NoCha~\cite{karpinska2024thousandpairsnovelchallenge}.\mi{prev sentence clearly not true based on results e.g. fig 1}

\vspace{2pt}
\noindent \textcolor{orange}{\textbf{> Chapter-level claims:}} Given the book summary and a single chapter outline, Claude is instructed to identify 2-3 key events of the chapter and use them to write a claim. While these claims do not necessitate global reasoning, they still require the model to search for correct chapter within a long text and perform intra-chapter reasoning (\S\ref{subsection:chap-book-ft}).

%\mi{this is not a good reason. instead provide some properties of these claims (e.g. they can still require complex intra-chapter reasoning and it is non-trivial to retrieve the right chapter)} 

%\paragraph{True/False claims:} 
%\mi{this para can be removed after u write the task def para earlier}
%Following \citet{karpinska2024thousandpairsnovelchallenge}, which evaluates accuracy based on correct True/False responses, we include both True and False claims to help the model distinguish valid from invalid statements. To maintain consistency, we generate claim pairs simultaneously rather than in separate stages. Each pair is supplemented with Chain of Thought (CoT) reasoning~\cite{wei_chain--thought_2023}, which details relevant events, their relationships, and how they support or contradict the claim's validity. 

\paragraph{(2b) Deduplicating and validating generated claims:} We use Claude to deduplicate generated claims (i.e. removing claims that have similar wording). As an additional filtering step, we use GPT-4o to validate the claims against the source chapter outlines by prompting it to assess whether all parts of a claim are supported by the outline.\footnote{We opt for GPT-4o to mitigate potential self-biases \cite{xu2024prideprejudicellmamplifies, panickssery2024llmevaluatorsrecognizefavor, li2025preferenceleakagecontaminationproblem}.} This step further emphasizes an advantage of \pipeline: unlike the \naive\ approach, our method allows for claim verification using the compressed chapter outline. To evaluate the reliability of LLM-based filtering, we manually review 72 claim pairs
% \footnote{From \textit{Oscar Wilde's The Picture of Dorian Gray}.} 
and only disagree in one instance, where GPT-4o deems a claim valid that we find too subjective. Overall, 59.4\% of the original claims are removed as duplicates, while 2.4\% are filtered out as invalid.
%\mi{you can contrast with validating the naive approach, which is a lot harder}
%\mi{what is the impact of this filtering? what percent of generated claims are removed as a result of these steps}\mi{note that you also perform filtering for naive}

\subsection{Human validation of \pipeline\ claims}
\label{data:claim_validation}

We use the same setup as described in \S\ref{data:alt} to manually evaluate 66 claims generated by the \pipeline\ pipeline. Table~\ref{tab:naive} provides a detailed breakdown of issues flagged in these claims, such as explicit references, invalidity, duplication, or misattribution. Notably, 83.3\% of the 66 claims are found to be completely free of errors—a significant improvement compared \naive's 26.9\%. \pipeline\ also costs less at \$0.05 USD per claim compared to \naive\ (\$0.07 per claim) and human annotators (\$1.7 per claim based on \citealt{karpinska_one_2024}).\footnote{See detailed cost analysis in \S\ref{appendix:data-cost}.} 
% We use \pipeline\ as our final generation pipeline. 
%\yapei{compare using per-claim cost?}
% The few invalid cases happen when the false claims are too subjective and could possibly be interpreted as true. Similarly, duplicate claims tend to involve the same events rather than exact copies, and misattributions occur when a particular event in the generated chain of thought is not fully supported by the source text.
% \pipeline\ generates claims that are more grounded and well-formatted, while still incorporating diverse events thanks to the use of chapter outlines and book summaries.

\subsection{Validating \pipeline\ chain-of-thoughts}
\label{data:cot_validation}
%\mi{need to motivate this, why didnt we just use human validators?}
We evaluate the groundedness of chain-of-thought reasoning for claims in our dataset by prompting an LLM to check if every event mentioned in the chain are supported by the chapter outline. We compute accuracy as the percentage of events in the CoT for true
% \footnote{We exclude false claims from this evaluation, as it is unclear how to measure the groundedness of a supposedly false CoT.} 
claims that are grounded in the book. To scale up evaluation, we use an LLM judge, DeepSeek-R1-Distill-Llama-70B~\cite{deepseekai2025deepseekr1incentivizingreasoningcapability}, instead of human evaluation. 
We find that 98.5\% of CoTs are grounded. The remaining ungrounded CoTs typically involve events open to multiple interpretations. Compared to \naive, \pipeline's CoTs are significantly easier to verify due to their explicit chapter references.
%This is because this task requires checking an average of 2,000 plot events (1,000 CoTs with on average two events each), making human annotation prohibitively expensive (see \S\ref{appendix:deepseek-cot} for model choice justification). 
%\mi{prev sentence is super confusing. what is a unit of reasoning?}
%\mi{do you mean like events mentioned in the cot?} 
%\mi{how does this compare to naive?} \chau{we already discussed the groundedness of naive and clipper as the percentage of misattributed claims in Table 1. \mi{still, state it again here to hammer home that clipper $>$ naive}}