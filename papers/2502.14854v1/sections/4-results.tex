\section{Results \& analysis}
Our fine-tuned models set a new state of the art for <10B models on long-context reasoning benchmarks while also improving baseline performance on related narrative understanding tasks. Below, we discuss our results in detail.
% Further analysis reveals room for improvement, as the models benefit more from chapter-level claims rather than book-level claims.


\begin{table*}[htbp] 
    \centering 
    \scriptsize 
    \resizebox{\textwidth}{!}{
        \begin{tabular}{p{\textwidth}} 
        \toprule 
        \cellcolor{gray!10}\textbf{Claim:} "Jacques consistently uses pastoral themes to express his love for Belle-bouche, first through metaphorical discussions of shepherds and shepherdesses, and later while walking through an orchard and discussing Arcadian lovers." \\
        \midrule 
        \end{tabular}
    }
    \resizebox{\textwidth}{!}{
        \begin{tabular}[t]{p{0.4\textwidth} p{0.6\textwidth}} 
        \parbox[t]{0.4\textwidth}{%
        \textbf{\llamainst's CoT:} ``The statement claims that Jacques consistently uses pastoral themes to express his love for Belle-bouche. To evaluate this statement, we need to examine the text and identify instances where Jacques uses pastoral themes to express his love for Belle-bouche. {\color{purple}The text mentions} that Jacques discusses crooks, flowers, and shepherdesses with Belle-bouche, and later, while walking through an orchard, he talks about Arcadian lovers, specifically mentioning Strephon, Chloe, Corydon, Daphnis, and Narcissus. These instances suggest that Jacques does use pastoral themes to express his love for Belle-bouche. However, we need to consider whether this is a consistent pattern throughout the text." \newline \textit{Answer: TRUE}%
        } & 
        \parbox[t]{0.6\textwidth}{%
        \textbf{\llamaftbalanced's CoT:} ``Here are the relevant details from the text: \\ \vspace{-0.12in}
        \begin{itemize}[leftmargin=*]
            \item {\color{teal}Chapter 2}: Jacques engages in a metaphorical discussion about shepherds and shepherdesses, expressing his love for Belle-bouche.
            \item {\color{teal}Chapter 17}: Jacques and Belle-bouche walk through the orchard, discussing the twentieth of May as a day for lovers.
            \item Relationship: Recurring theme of pastoral love
        \end{itemize}
        In {\color{teal}Chapter 2}, Jacques engages in a metaphorical discussion about shepherds and shepherdesses to express his love for Belle-bouche. This theme continues in {\color{teal}Chapter 17}, where Jacques and Belle-bouche walk through an orchard, discussing the twentieth of May as a day for lovers. This connection shows Jacques' consistent use of pastoral imagery to convey his romantic feelings for Belle-bouche throughout the story." \newline \textit{Answer: TRUE}%
        \vspace{2pt}
        } \\
        \bottomrule
        \end{tabular}
    }
    \caption{Comparison of CoT responses produced by \llamainst\ and \llamaftbalanced\ for a claim from John Esten Cooke's \textit{The Youth of Jefferson}. \llamaftbalanced's CoT response contains more explicit chapter references (highlighed in {\color{teal}green}) and details compared to that of \llamainst\ (highlighted in {\color{purple}red}).}
    \label{tab:cot-examples} 
    \vspace{-0.2in} 
\end{table*}


\subsection{\pipeline\ models outperform baselines on narrative claim verification} \label{subsec:main_results}
% \mi{you may want to split this into one para on your test set and one on nocha, each with headers}

% \yapei{todo: address the prolong base issue}
% \mi{more descriptive header!}
% \paragraph{Fine-tuning on our data improves performance on \pipeline-test:} 
On \pipeline-test, our fine-tuned models significantly outperform the instruct models they are initialized from (referred to as baselines),\footnote{\prolongftbalanced\ is initialized from \prolongbase\ instead of \prolonginst. However, since performing evaluation intended for instruct models on a continually pretrained model may not be ideal, we exclude \prolongbase's results from Table \ref{tab:main-result}. As shown in Table \ref{tab:prolong-base-acc}, \prolongbase\ performs significantly worse than \prolonginst\ on \pipeline-test.} as shown in Table \ref{tab:main-result}. 
% This improvement, while expected, is notable in its magnitude.
For example, \qwenftbalanced\ achieves over a 20\% performance gain compared to \qweninst, while \llamaftbalanced\ sees nearly triple the performance of \llamainst. These substantial improvements demonstrate the effectiveness of \pipeline-generated data.

% \mi{same here!}
\paragraph{Fine-tuning on our data improves performance on NoCha:} A similar trend is observed on NoCha. The performance improvements range from an 8\% gain for strong baselines like \qweninst\ to a dramatic twofold increase for weaker baselines such as \llamainst\ and \prolonginst. It is worth noting that all three baseline models initially perform below the random chance baseline of 25\%, but our fine-tuned models consistently surpass this threshold. 

\paragraph{Performance gap between \pipeline-test and NoCha:} We note that the performance gap between NoCha and \pipeline-test\ is likely due to the nature of the events involved in the claims. While \pipeline-test\ consists of synthetic claims derived from events in model-generated outlines, NoChaâ€™s human-written claims may involve reasoning about low-level details that may not typically appear in such generated outlines. Future work could incorporate more low-level events into chapter outlines to create a more diverse set of claims.
%We hope future work will explore synthetic data generation strategies that can help models improve more on complex reasoning tasks like NoCha.
% \mi{add sentence on implication for future work!}

%\yapei{but aren't chapter level claims also about details?},
% On both NoCha and our test set, our models significantly outperform their respective baseline models (Table \ref{tab:main-result}).\mi{i dont think this terminology is easy to understand. maybe write instead that our fine-tuned models outperform the instruct models that they are initialized to? this is also not surprising so you may want to state that.} The performance gains on our test set vary: \qwenftbalanced\ improves by over 20\%, while other fine-tuned models nearly triple their baseline performance by more than 40\%. We observe a similar trend on NoCha, with improvements as small as 8\% for already strong baselines like \qweninst, and as large as a twofold increase for weaker baselines such as \llamainst\ and \prolonginst. Notably, all baseline models initially perform below the random chance baseline of 25\%, but after fine-tuning, they consistently surpass this threshold. We note that the performance gap between NoCha and our test set is likely due to the nature of the events involved in the claims. \pipeline\ contains synthetic claims constructed with major events in the outline, which might make verification more straightforward. In contrast, NoCha's human-written claims contain lower-level plot details, which might be more challenging for LLMs.
%\mi{but we argue that clipper can generate claims about low-level events... maybe say nocha includes reasoning over things that wouldnt make it into an outline in the first place?}

% \mi{rewrite header, very confusing}\chau{is this better?\mi{how about something like Finetuning on our dataset also improves other narrative-related tasks}}
\subsection{Fine-tuning on \pipeline\ improves on other narrative reasoning tasks}  Beyond long-context reasoning, our models also show improvements in narrative understanding and short-context reasoning tasks. On NarrativeQA, which requires comprehension of movie scripts or full books, our best-performing models, \llamaftbalanced\ and \prolongftbalanced, achieve a 2\% and 5\% absolute improvement over their respective baselines. Similarly, on MuSR, a short-form reasoning benchmark, our strongest model, \qwenft, achieves 45.2\% accuracy, surpassing the 41.2\% baseline. However, these improvements are not consistent across all tasks. On $\infty$Bench QA, only \qwenftbalanced\ outperforms the baseline by approximately 7\%. In contrast, \llamaftbalanced\ and \prolongftbalanced\ show slight performance declines of up to 4\%. Thus, while fine-tuning on \pipeline\ data improves performance on reasoning and some aspects of narrative understanding, its transferability is not universal across domains.


% \mi{emphasize that it doesnt improve it THAT much compared to our data}
\subsection{Short-context claim data is less helpful}
% \yapei{can refer back to 3.1 and mention that for our task, training on long data is more effective than training on short data, which contradicts prev findings. then highlight importance of good long data.}
Contrary to prior studies suggesting short-form data benefits long-context tasks \cite{dubey2024llama, gao2024trainlongcontextlanguagemodels} more than long data, our results show otherwise. While \prolongwp, trained on short data, outperforms baselines, it underperforms across all four long-context benchmarks compared to models fine-tuned on our data. This underscores the need for high-quality long-context data generation pipelines like \pipeline.
% outperforms our three baseline models on \dataname-test (60.4\%), NoCha (24.1\%), and MuSR (45.2\%). However, when comparing to our fine-tuned models, 
% While strong performance on MuSR is expected given the benchmark's focus on short-form reasoning, the fact that short-form reasoning also improves performance on other long-context tasks is particularly interesting. This suggests that our training data format, which features detailed reasoning chains on relevant events and their relationships, contributes meaningfully to model improvement.

\subsection{Finetuning on CoTs results in more informative explanations}
We evaluate the groundedness of CoT reasoning generated by our fine-tuned models using DeepSeek-R1-Distill-Llama-70B (\S\ref{data:cot_validation}). Here, a reasoning chain is counted as grounded when every plot event in the chain can be found in the chapter outline that it cites. Table \ref{tab:cot-groundedness} shows that fine-tuning significantly improves groundedness across all models, with \prolongftbalanced\ achieving the highest rate (80.6\%), followed closely by \llamaftbalanced\ (75.9\%). Looking closer at the content of the explanations (Table \ref{tab:cot-examples}), the baseline model (\llamainst) often gives a generic response without citing any evidence, whereas \llamaftbalanced\ explicitly references Chapter 9 and specifies the cause-and-effect relationship.





\begin{table*}[htbp]
\centering
\footnotesize
\scalebox{0.87}{
\begin{tabular}{p{0.1\textwidth}p{0.06\textwidth}p{0.42\textwidth}p{0.42\textwidth}}
\toprule
\multicolumn{1}{c}{\textsc{Category}} & \multicolumn{1}{c}{\textsc{Freq (\%)}} & \multicolumn{1}{c}{\textsc{True Claim}} & \multicolumn{1}{c}{\textsc{False Claim}} \\
\midrule
Event & 43.2 & The Polaris unit, initially assigned to test a new audio transmitter on Tara, explores the planet's surface {\color{teal}using a jet boat without landing}. & The Polaris unit, initially assigned to test a new audio transmitter on Tara, explores the planet's surface by {\color{purple}landing their spaceship}. \\
\midrule
Person & 31.6 & The cattle herd stolen from Yeager by masked rustlers is later found in {\color{teal}General Pasquale}'s possession at Noche Buena. & The cattle herd stolen from Yeager by masked rustlers is later found in {\color{purple}Harrison}'s possession at Noche Buena. \\
\midrule
Object & 15.8 & The alien structure Ross enters contains both a chamber with {\color{teal}a jelly-like bed} and {\color{teal}a control panel capable of communicating with other alien vessels}. & The alien structure Ross enters contains both a chamber with {\color{purple}a metal bed} and {\color{purple}a control panel capable of time travel}. \\
\midrule
Location & 13.7 & Costigan rescues Clio twice: first from Roger on his planetoid, and later from a {\color{teal}Nevian city} using a stolen space-speedster. & Costigan rescues Clio twice: first from Roger on his planetoid, and later from a {\color{purple}Triplanetary city} using a stolen space-speedster. \\
\midrule
Time & 6.3 & Jean Briggerland's meeting with ex-convicts Mr. Hoggins and Mr. Talmot, where she suggests a burglary target, {\color{teal}follows} a failed attempt on Lydia's life involving a speeding car on the sidewalk. & Jean Briggerland's meeting with ex-convicts Mr. Hoggins and Mr. Talmot, where she suggests a burglary target, {\color{purple}precedes} a failed attempt on Lydia's life involving a speeding car on the sidewalk. \\
\midrule
Affect & 4.2 & David Mullins, who initially expresses {\color{teal}skepticism} about Chester's hiring, later fires Chester on false pretenses and immediately replaces him with Felix. & David Mullins, who initially expresses {\color{purple}enthusiasm} about Chester's hiring, later fires Chester on false pretenses and immediately replaces him with Felix. \\
\bottomrule
\end{tabular}}
\caption{A taxonomy of perturbations that cause a false claim to be misclassified as true. Corresponding details in true and false claims are respectively highlighed in {\color{teal}green} and {\color{purple}red}. Frequencies do not sum to 100\%, as some claims receive multiple labels. Category definitions and more detailed analysis in \S\ref{appendix:error-analysis}.}
\label{tab:error-analysis-dist}
\vspace{-0.2in}
\end{table*}

% \begin{table*}[htbp] 
%     \centering 
%     \scriptsize 
%     \resizebox{\textwidth}{!}{
%         \begin{tabular}{p{0.95\textwidth}} 
%         \toprule 
%         \cellcolor{gray!5}\textbf{Claim:} "Jacques consistently uses pastoral themes to express his love for Belle-bouche, first through metaphorical discussions of shepherds and shepherdesses, and later while walking through an orchard and discussing Arcadian lovers." \\
%         \midrule 
%         \textbf{\llamainst's CoT:} \\ 
%         ``The statement claims that Jacques consistently uses pastoral themes to express his love for Belle-bouche. To evaluate this statement, we need to examine the text and identify instances where Jacques uses pastoral themes to express his love for Belle-bouche. The text mentions that Jacques discusses crooks, flowers, and shepherdesses with Belle-bouche, and later, while walking through an orchard, he talks about Arcadian lovers, specifically mentioning Strephon, Chloe, Corydon, Daphnis, and Narcissus. These instances suggest that Jacques does use pastoral themes to express his love for Belle-bouche. However, we need to consider whether this is a consistent pattern throughout the text." \\
%         \textit{Answer: TRUE} \\ 
%         \midrule 
%         \textbf{\llamaftbalanced's CoT:} \\
%         ``Here are the relevant details from the text: \\ \vspace{-2pt} \begin{itemize} \item Chapter 2: Jacques engages in a metaphorical discussion about shepherds and shepherdesses, expressing his love for Belle-bouche. \item Chapter 17: Jacques and Belle-bouche walk through the orchard, discussing the twentieth of May as a day for lovers. \item Relationship: Recurring theme of pastoral love \end{itemize} In Chapter 2, Jacques engages in a metaphorical discussion about shepherds and shepherdesses to express his love for Belle-bouche. This theme continues in Chapter 17, where Jacques and Belle-bouche walk through an orchard, discussing the twentieth of May as a day for lovers. This connection shows Jacques' consistent use of pastoral imagery to convey his romantic feelings for Belle-bouche throughout the story." \\ 
%         \textit{Answer: TRUE} \\ 
%     \bottomrule 
%     \end{tabular} 
%     } 
%     \caption{Comparison of CoT responses produced by \llamainst\ and \llamaftbalanced\ for a claim from John Esten Cooke's \textit{The Youth of Jefferson}.} 
%     \label{tab:cot-examples} 
%     \vspace{-0.2in} 
% \end{table*}



% \mi{header is too informal}
\subsection{Small models struggle with book-level reasoning} 
\label{subsection:chap-book-ft}
Trained only on 8K chapter-level claims, \prolongftchapter\ outperforms \prolongftbook\ on both chapter- and book-level test subsets (Table \ref{tab:chapter_vs_book}). This likely reflects the limitations of smaller models (7B/8B) in handling the complex reasoning required for book-level claims, aligning with prior findings \cite{qi2024quantifyinggeneralizationcomplexitylarge}. The performance gap between the models is modest (4.2\%), and we leave exploration of larger models (>70B) to future work due to compute constraints.
% Although larger models (>70B) might be able to effectively learn the complex reasoning patterns in these multi-chapter claims, we leave this for future work due to limited compute resources. 


\subsection{Fine-tuned models have a difficult time verifying False claims} \label{sec:error-analysis}
% \mi{this can def be heavily shortened / go to appendix, the table itself is sufficient along with a couple sentences}
To study cases where fine-tuned models struggle, we analyze \qwenftbalanced\ outputs. Among 1,000 book-level claim pairs in \pipeline-test, the model fails to verify 37 true claims and 97 false claims, aligning with NoCha findings \cite{karpinska_one_2024} that models struggle more with false claims. We investigate perturbations that make false claims appear true and present a taxonomy with examples in Table \ref{tab:error-analysis-dist}, with further details in \S\ref{appendix:error-analysis}.
% Notably, in 95 cases, the model successfully validates the true claim but fails to validate the corresponding false claim. This raises an important question: \textit{What specific perturbations make a false claim appear true to the model?} Through careful manual analysis, we derive a taxonomy of such perturbations and present them in Table \ref{tab:error-analysis-dist}. The most frequent perturbations are changes to events (43.2\%) and people (31.6\%), such as altering actions or misattributing roles. Less frequent but notable are modifications to objects (15.8\%), locations (13.7\%), time (6.3\%), and affect (4.2\%). All these perturbations introduce plausible-sounding variations that the model may struggle to detect without fully understanding the narrative.\footnote{We provide definitions for each category in Appendix \ref{appendix:error-analysis}} 
%A closer examination of the chain-of-thoughts generated for these 95 claims reveals some recurring patterns: the model often fabricates evidence, applies incorrect reasoning, or completely ignores the perturbed details. Specific examples can be found in Appendix X. \yapei{do we need this part on CoT?}
