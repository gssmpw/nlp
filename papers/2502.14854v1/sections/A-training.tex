\section{Training}

\subsection{Codebases} To fine-tune models in the Llama family, we adopt the ProLong codebase,\footnote{\href{https://github.com/princeton-nlp/ProLong}{https://github.com/princeton-nlp/ProLong}} which integrates PyTorch~\cite{paszke2019pytorchimperativestylehighperformance} and Hugging Face~\cite{wolf2020huggingfacestransformersstateoftheartnatural} for model training, FlashAttention-2~\cite{dao2023flashattention2fasterattentionbetter} for efficient attention computation, and DeepSpeed Ulysses~\cite{jacobs_deepspeed_2023} for sequence parallelism, enabling training across 8 A100 GPUs. For fine-tuning \qweninst\ (Qwen-Instruct), we use the 360-LlamaFactory codebase,\footnote{\href{https://github.com/Qihoo360/360-LLaMA-Factory}{https://github.com/Qihoo360/360-LLaMA-Factory}} a modification of Llama-Factory\footnote{\href{https://github.com/hiyouga/LLaMA-Factory}{https://github.com/hiyouga/LLaMA-Factory}} that incorporates sequence parallelism via zigzag ring attention~\cite{liu2023blockwise,liu2023ring}. We choose \prolongbase\ (ProLong-base) over \prolonginst\ (ProLong-instruct) based on a small fine-tuning experiment, where we fine-tune both ProLong-instruct and ProLong-base on 2K training examples. This experiment shows that ProLong-base outperforms ProLong-instruct by 61.6\% and 59.7\%, respectively. %Additionally, since ProLong-base is continually trained from Llama-3-8B-instruct, it has already been exposed to instruct data.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{assets//graphics/bookclaim_ft_prompt.pdf}
    \caption{How we structure our finetuning prompts, which include system message, user message, and assistant message. Placeholders (colored in \textbf{\textcolor{lightcayenne}{light cayenne}}) will be replaced with actual text from the dataset. The contents between \texttt{<context>}, \texttt{<explanation>}, and \texttt{<answer>} tags are generated (Section \ref{data:claims_generation}).}
    \label{fig:sft_prompt}
\end{figure}
\subsection{Hyperparameter Tuning}
Table \ref{tab:hyperparam} summarizes the performance of each configuration from our hyperparameter tuning experiment on 100 samples from \pipeline's dev set.
\begin{table}[htbp]
\small
\centering
\begin{tabular}{@{}ccc@{}}
\toprule
Learning Rate & Batch Size &Dev Set Accuracy \\ \midrule
1e-5 & 16& 26\% \\
1e-6 & 16& \textbf{74}\% \\
1e-7 & 16& 71\% \\
1e-5 & 32& 34\% \\
1e-6 & 32& 73\% \\
1e-7 & 32& 69\% \\ \bottomrule
\end{tabular}
\caption{Hyperparameter tuning results. Each model is fine-tuned for 1 epoch and tested on a subset of 100 samples from our dev set.}
\label{tab:hyperparam}
\end{table}






