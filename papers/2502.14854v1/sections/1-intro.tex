\section{Introduction}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{assets/graphics/fixed_leaderboard}
    \caption{Results on \pipeline's test set and NoCha for baselines, small closed models, and \pipeline\ models. Fine-tuning on our synthetic data significantly improves narrative claim verification.}
    \label{fig:leaderboard}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{assets/graphics/fig1-final-final-final.pdf}
    \caption{Overview of the \pipeline~pipeline. (1) \textit{Compression}: We generate chapter outlines and book summaries using an LLM. Our books average 90,437 tokens, our outlines average 8,745 tokens, and our summaries average 618 tokens. (2) \textit{Synthetic claim generation}: We ask LLMs to generate true and false claims based on the outlines and summaries. Each generated claim comes with a chain-of-thought. The book texts, generated claims, and corresponding chains-of-thoughts are then used for supervised finetuning.}
    \label{fig:overview}
\end{figure*}
%\mi{it is confusing that we're using so many different LLMs here, i would just replace the specific model name with ``LLM'' and give specifics later}
%\mi{make sure to include the pipeline name throighout the intro}
Due to the high cost of human-annotated data, LLM developers increasingly rely on \emph{synthetic} data (generated by LLMs) to boost instruction following and reasoning capabilities~(\citealt{ding_enhancing_2023, lambert2024tulu3, yang_qwen25-1m_2025}, \emph{inter alia}). 
%\mi{cite things like tulu3, ultrachat, etc}. 
As the context size of LLMs extends to millions of tokens, it is important to ensure that we have scalable and performant strategies to create synthetic data for \emph{long-context} tasks. Prior work creates such data by (1) selecting a long document or a smaller chunk within; and (2) prompting an LLM to generate input/output pairs for various tasks using the selected text.~\cite{bai_longalign_2024, dubey2024llama}. %\mi{select the 2 most relevant citations for use here} 

While this strategy is effective for tasks like summarization and QA, we show that it breaks down for more complex reasoning-oriented tasks like \emph{narrative claim verification}, in which a model must judge whether a statement about a long input text is true or false.
The majority of narrative claims in the NoCha benchmark~\cite{karpinska_one_2024}, which was created by human readers of fictional books, can only be verified by \emph{global} reasoning over events, characters, and relationships.  This poses an immense challenge to even the best LLMs: OpenAI's o1-preview currently leads with an accuracy of 67.4\% (far below human performance). If no LLM can reliably solve the task, how can we produce and validate synthetic data for it?\footnote{Creating data for this task is challenging even for \emph{humans}: NoCha was created by people who first read an entire book and then were paid \$1.7 per claim they wrote about that book.} 
% Furthermore, how can we validate that the synthetic data is of high quality?

We tackle this challenge by introducing \pipeline, a synthetic data generation pipeline that operates in two stages (Figure \ref{fig:overview}). First, a long document is \emph{compressed} by an LLM into summaries and/or outlines that contain salient events and descriptions. Then, the LLM is prompted to generate claims based on the compressed narrative, with optional instructions to write claims that require reasoning over multiple chapters to verify. Each claim is accompanied by a chain-of-thought reasoning that grounds it within specific chapters where relevant events occur.

Compared to the na\"ive strategy of prompting an LLM with the entire (uncompressed) book, \pipeline\ significantly reduces noise in the generated claims and increases groundedness of generated explanations, while also costing roughly half as much. Why does this work? Prior work has shown that LLMs are high-quality summarizers of long documents~\cite{chang_booookscore_2024, kim_fables_2024}. In addition, by operating on compressed representations, we address the known degradation of instruction-following in long-context settings~\cite{wu_lifbench_2024, levy_same_2024} and thus reduce the complexity of the claim generation process.

%\mi{can we cite some paper that shows instruction following degrades as contexts grow in size as evidence? then intuitively using compressed contexts makes it easier to follow instructions}, addressing the instruction following degradation in long-context setting \yapei{\cite{wu_lifbench_2024, levy_same_2024}}

To verify the effectiveness of \pipeline, we use it to generate a dataset containing 19K claims about public-domain fictional books. Fine-tuning open-weight models like Llama-3.1-8B-Instruct \cite{dubey2024llama}, ProLong-512K-8B-Base \cite{gao_how_2024} and Qwen2.5-7B-Instruct~\cite{qwen_qwen25_2024} on this dataset, yields large improvements on narrative claim verification and positive transfer to other narrative-related tasks (e.g., NarrativeQA, MuSR). For instance, fine-tuning Llama-3.1-8B-Instruct on our data doubles its NoCha performance (from 16.5\% to 32.2\%) and almost triples its test set performance (from 27.9\% to 76.0\%). Our fine-tuned Qwen model sets a new state of the art on NoCha for $<$10B models, outperforming closed models like Gemini 1.5 Flash 8B~\cite{geminiteam2024gemini15unlockingmultimodal} and OpenAI o1-mini~\cite{openai_o1_2024}.

\vspace{-0.1pt}
While our approach is promising for improving long-context reasoning in open-weight models, our best models fall well short of the performance reached by closed LLMs such as o1 on NoCha. The performance gap between NoCha and \pipeline-test\ likely stems from the nature of the claims, as \pipeline-test\ features synthetic claims based on model-generated outlines, while NoCha’s human-written claims require reasoning about details often absent from such outlines. We analyze where our fine-tuned models can still improve, discovering that they benefit more from training on claims whose evidence is localized to a single chapter (and not more complex multi-chapter claims). We hope future work will leverage our data generation pipeline for fine-tuning larger models (e.g., $>$70B) on other long-context tasks to further improve global reasoning abilities.

% Reasoning over long texts (e.g., millions of tokens) is critical for real-world LLM applications such as summarization and question answering; however, it remains difficult even for the most powerful models~\cite{zhang2024inftybenchextendinglongcontext, bai2025longbenchv2deeperunderstanding}. One important task that offers a window into a model's long-context reasoning abilities is \emph{claim verification}, in which model must judge whether a statement about a long input text is true or false.  

% % The NoCha benchmark~\cite{karpinska2024thousandpairsnovelchallenge}, which contains 1001 complex claims  written by human readers about recently-published fictional books, highlights the gap in long-context reasoning ability between closed models and smaller open-weight ones: OpenAI's o1-preview leads all models with an accuracy of 67.4\%, while the top-scoring open model with $<$10B parameters (Qwen 2.5 7B-Instruct) slots in at 24.1\%, which is \emph{worse than random guessing (25\%)}! The majority of claims in NoCha require global reasoning over events, characters, and relationships, which small models struggle to do especially over longer books. How can we close this performance gap?


% A common strategy to improve smaller models on a specific task is to fine-tune them on synthetic data generated by stronger models \cite{alpaca, ding2023enhancingchatlanguagemodels, vicuna2023}\mi{yapei replace with new cites} \yapei{added a sentence in first para of sec 2 to explain common strategies for creating long-context synthetic data, which cites newer work. do we still need to cite those here?}. For narrative claim verification and other long-context tasks, however, creating synthetic data is not straightforward. When na\"ively prompting closed models like GPT-4o to create claims about long books, we obtain claims that are often invalid or misattributed.\footnote{Creating data for this task is challenging even for humans: NoCha was created by people who first read an entire book and then were paid \$3 per claim they wrote about that book.} Furthermore, generating synthetic data for the long-context setting can be \emph{expensive}. Na\"ively generating 19K claims from the book texts alone would cost around \$1,200 (around \$0.2 per claim), much more expensive than our pipeline at around \$986 (around \$0.1 per claim).

% We instead propose a two-stage synthetic data generation pipeline for narrative claim verification, in which (1) the input book is first compressed into a high-level summary and a set of chapter-level outlines, and then (2) claims are generated conditioned on the summary and outlines rather than the full book (Figure \ref{fig:overview}). This pipeline also enables us to effectively control the \emph{scope} of generated claims: in the second stage, we can prompt an LLM to produce claims that involve reasoning over events within either a single chapter or multiple chapters. Finally, this pipeline costs almost twice as cheap as the na\"ive prompting approach while also significantly reducing noise in the generated data. 

% To verify the effectiveness of our synthetic data pipeline, we use it to generate 19K claims about fictional books in the public domain, which also enables the release of this dataset to facilitate future research. Fine-tuning open-weight models such as \llamainst\ and Qwen2.5-7B-Instruct on our dataset yields large improvements on NoCha, as well as tasks that involve narrative understanding but not claim verification (e.g., NarrativeQA, MuSR). Specifically, the model fine-tuned on \llamainst\ doubles the baseline model’s performance on NoCha (boosting accuracy from 16.5\% to 32.2\%) and improves results on NarrativeQA—a long-form QA benchmark requiring comprehension of entire books or movie scripts—by 2\%. Our fine-tuned \dataname-Qwen-balanced sets a new state of the art on NoCha for $<$10B models, even outperforming some closed models like Gemini 1.5 Flash 8B and OpenAI o1-mini~\cite{openai_o1_2024}.

% While our approach is promising for improving long-context reasoning in open-weight models, our best models fall well short of the performance reached by closed LLMs such as o1. We analyze where our fine-tuned models can still improve, discovering that they benefit mainly from training on claims whose evidence is localized to a single chapter (and not more complex multi-chapter claims). We hope future work will leverage our data generation pipeline for fine-tuning larger models (e.g., $>$70B) to see if they can unlock more global reasoning abilities over long-context inputs.
% This fine-to-coarse data creation method can also be applied to any settings that require contructing synthetic data from very long documents.

% % \yapei{Call on people to apply this compression technique to any settings that require constructing synthetic data from very long documents?\mi{yes! we can say this is like a proof of concept for other long context synthetic data settings} }



% % \begin{enumerate}
%     \item People are increasingly interested in long context language models (e.g., applications XYZ), but reasoning over long contexts is hard even for the best models \yapei{works other than nocha that show this: \cite{zhang2024inftybenchextendinglongcontext, bai2025longbenchv2deeperunderstanding} both involve realistic tasks and show that even strong closed-source LLMs have low performance}
%     \item We look specifically at \emph{long-form claim verification}, in which a model needs to decide whether a statement about a provided text is true or false. \yapei{This task becomes particularly challenging when the contexts involve up to 128K tokens.}
%     \item There is a big gap in long context reasoning ability between open models and closed models on this task.\chau{Best closed-source (o1-preview [09/12]: 67.36\%) and open-source (LLaMA 3.1 405B: 47.21\%)} There's also a big gap between large models and small (<10B) models. (mention nocha specifics) \chau{qwen-2.5-7b-instruct=24.14\% and qwen-2.5-14b-instruct=30.05\%}
%     \item How far can we close this gap?
%     \item These days, people often generate synthetic data to improve their LLM's performance on specific tasks. 
%     \item However, it is hard to generate synthetic data for long context tasks, as they often require complex reasoning over huge sequences. NoCha was created by people who read a whole book and then were paid \yapei{around \$3} per claim they wrote.  You can't just prompt a model to produce such data (as we show). \yapei{We show that simply prompting a powerful LLM to generate claims given a full book often leads to a significant number of poorly formatted, invalid, duplicate, or misattributed claims.} Estimate cost for the naive approach is \$1,284 vs \$986. 
%     \item We instead use a two stage process to produce data: (1) compress the book into a collection of chapter-level outlines; (2) prompt an LLM to generate claims that can be verified using information from one or more bullets in the outline \yapei{we generate both claims and the CoT}
%     \item We fine-tune smaller open models on our synthetic data. These models perform below random chance before fine-tuning, but we manage to improve their performance to 32\%,  on par with smaller closed models like GPT-4o-mini (33.06\%) and Claude 3.5 Haiku (35.01\%) (Figure \ref{fig:leaderboard}). 
%     \item We also observe improvements on other narrative understanding tasks (e.g., \chau{NarrativeQA and MuSR}\cite{kocisky_narrativeqa_2018, sprague_musr_2024}), suggesting the transferability of synthetic claim verification data.
%     \item We observe that these small models benefit most from easier synthetic data, while harder claims do not seem to improve performance. We hope future work will examine the impact of this harder data on larger (>70B) models, as this was not feasible for us given our compute resources.
%     \item We analyze chains of thought generated by our fine-tuned models to better understand where they succeed and fail, finding XYZ. \yapei{We find that models finetuned with our synthetically generated chains of thoughts tend to generate chains of thoughts that are a lot more informative (including specific chapter references) and grounded in the outline. }
% \end{enumerate}

% Advances in AI system engineering~\cite{dao2022flashattentionfastmemoryefficientexact, liu2023ringattentionblockwisetransformers, jacobs2023deepspeedulyssesoptimizationsenabling}, model designs~\cite{chen_extending_2023, peng_yarn_2023, chen_longlora_2023}, and training recipes~\cite{fu2024dataengineeringscalinglanguage,gao2024trainlongcontextlanguagemodels, hu_longrecipe_2024} have significantly extended the context window of LLMs from 128K to over 1M tokens. However, reasoning benchmarks such as NoCha~\cite{karpinska2024thousandpairsnovelchallenge} and Babilong~\cite{kuratov_babilong_2024} have revealed that even the best long-context language models (LCLMs) such as Llama 3.1 405B-Instruct~\cite{dubey2024llama} struggle to synthesize information over extended context, while human annotators complete this task with ease.

% To bridge this performance gap, we use instruction tuning~\cite{ouyang_training_2022, sanh2022multitaskpromptedtrainingenables, wei2023chainofthoughtpromptingelicitsreasoning} to improve open-source LCLMs' ability to verify claims in fictional narrative, a task that requires retrieving and reasoning over related events in an extended context. We construct~\dataname, an instruction-tuning dataset for claim verification in fictional books (Figure~\ref{fig:overview}), with a focus on accessibility, cost-effectiveness, and quality. To ensure public availability and avoid copyright issues, we source our texts from Project Gutenberg. Rather than relying on human annotators, which would be prohibitively expensive for collecting 19K claims, we implement an automated claim synthesis pipeline, which can be applied to any existing books. To ensure that claims are well-grounded in the source material, our pipeline constructs claims from chapter outlines and book summaries instead of synthesizing claims based on the full texts. 
% We further differentiate claims by scope: chapter-level claims require reasoning about events within a single chapter, while more challenging book-level claims require reasoning across multiple chapters. The final dataset comprises 426 Project Gutenberg books paired with 19K True/False claims, completed for a total cost of \$986 USD (Appendix \ref{appendix:data-cost}).

% We use \dataname~to fine-tune open-source LCLMs, including Llama-3-8B-ProLong-512k-Base~\cite{gao2024trainlongcontextlanguagemodels}, LLaMA-3.1-8B~\cite{dubey2024llama}, and Qwen-2.5-7B-Instruct~\cite{qwen2.5, qwen2}. We demonstrate that fine-tuning ... \chau{TODO} on~\dataname~ improves performance on the NoCha public test set by over ... compared to the top-performing 8B model, Gemini 1.5 Flash 8B (08/27), and double the performance of the base model by 16\%, which emphasizes the value of our dataset for the claim verification tasks. This performance is closely followed by ... Our analysis also shows that the chains of thought generated during claim verification are also much more detailed than .... While our models get much better at reasoning over long context, they do not show sign of significant degradation on general-purpose benchmarks such as ... \chau{Add more findings here once we get all results.}


%Claim verification in this context presents a complex long-context challenge that requires models to retrieve, synthesize, and reason over extensive text. When applied to book-length fiction, this task becomes particularly challenging. First, it requires models to reason about explicit claims based on implicit narrative events. Second, these events are often scattered across the text, which forces models to develop retrieval capabilities that are not necessary in shorter contexts. Third, unlike tasks based on synthetic or fragmented texts~\cite{kuratov_babilong_2024, hsieh2024rulerwhatsrealcontext}, the implicit and continuous nature of fictional narratives more closely mirrors real-world reasoning challenges. This makes the task more realistic and difficult while remaining well within human capabilities, as demonstrated by high human accuracy rates~\cite{karpinska2024thousandpairsnovelchallenge}.