\section{Conclusion}
We present \pipeline, a compression-based synthetic data generation pipeline tailored to narrative claim generation. Using \pipeline, we create a dataset of 19K true/false claims at both the book and chapter level. Our fine-tuned models achieve state-of-the-art performance among <10B models on claim verification and narrative understanding tasks. Future work could explore the impact of book-level claims on larger models and experiment with approaches to generating more challenging claims to bridge the gap between synthetic test set and human-written benchmarks like NoCha. 