\section{Supervised finetuning for LLMs on \pipeline\ data}
\label{sec:training}
% \mi{need a better transition here. "now that we have demonstrated clipper produces synthetic data of high quality, we ...}
Having shown that \pipeline\ produces synthetic data of high quality, we now investigate the effects of training on such data. We apply supervised finetuning (SFT) to three models on our dataset: ProLong-512K-8B-Base \cite{gao2024trainlongcontextlanguagemodels},\footnote{Despite the name, this model has undergone instruction tuning before. The ProLong team ran continual pre-training on Llama-3-8B-Instruct to get this model.} Llama-3.1-8B-Instruct \cite{dubey2024llama}, and Qwen2.5-7B-Instruct \cite{qwen2.5}.\footnote{In subsequent sections, we refer to these models as \prolongbase, \llamainst, and \qweninst.} Our top model, \llamaft, achieves nearly three times the test set performance of \llamainst—boosting accuracy from 27.9\% to 76\%—while showing substantial gains in long-context reasoning and narrative understanding on tasks like NoCha, NarrativeQA, and MuSR. Moreover, all of our models outperform all existing $<$10B models on the NoCha benchmark.
% Moreover, all of our models achieve a new state of the art on NoCha for models under 10B parameters.\mi{how can all models achieve a state of hte art?}
% \mi{compared to what?}

%\mi{1-2 sentence summary of results here!} 

\subsection{Training setup}

% SFT on \dataname:\mi{need more descriptive header}
\paragraph{Data splits:} We divide our dataset into three parts: 16K claims (8K true/false pairs) for training, 2K for validation, and 1K for testing. Notably, the books in the test set do not overlap with those in the training or validation sets. For each entry, we combine the book text and claim to form the user prompt, and include the chain of thought reasoning along with the final answer as the assistant’s message (see Figure \ref{fig:sft_prompt}). %\mi{mention books are disjoint in each fold!}

\paragraph{Hyperparameters:} After performing hyperparameter tuning, we find that a learning rate of 1e-6 and a batch size of 16 yield the best performance on our dev set.\footnote{We perform hyperparameter tuning on learning rates of 1e-5, 1e-6, and 1e-7, along with batch sizes of 16 and 32. Tuning is done for one epoch on a subset of 2K training samples. Due to high GPU costs (each epoch takes $~$5 hours), we only conduct tuning on \prolongbase\ only.} We then fine-tune \qweninst, \llamainst, and \prolongbase\ using this configuration for one epoch on our full training set.
% \mi{prev sentences on finding optimal hyperparams should be footnote}

%\subsection{Ablation Setup} To measure the impact of different data mixtures, we conduct two additional ablation studies via SFT on Prolong-base.\mi{why is this its own subsection?}

% SFT on a single claim scope:\mi{header is hard to understand}
\paragraph{Ablation on the effect of claim scope:} Our dataset consists of 8K book-level claims and 8K chapter-level claims. To evaluate how claim scope affects performance, we fine-tune \prolongbase\ separately on each subset, resulting in \prolongftchapter\ and \prolongftbook.
% \mi{can you just merge this into the prev para since it's so short?}


\begin{table*}[htbp]
\centering
\footnotesize
% \resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c}
\toprule
\textbf{Models} & \textbf{\pipeline-test} & \textbf{NoCha} & \textbf{NarrativeQA} & \boldmath$\infty$\textbf{Bench QA} & \textbf{MuSR}\\
\midrule
Qwen2.5-7B-Instruct
  & 51.0\%  
  & 24.1\% 
  & 40.3\%  
  & 35.3\%
  & 41.2\%\\

Llama-3.1-8B-Instruct
  & 27.9\%  
  & 16.5\%
  & 47.7\%  
  & \textbf{47.8\%} 
  & 40.3\%\\

ProLong-512K-8B-Instruct
  & 34.5\%  
  & 16.9\%
  & 44.0\%  
  & 42.6\% 
  & 42.3\%\\
\midrule
\includegraphics[height=0.8em]{assets/graphics/elmo-clipper.png} Qwen2.5-7B-\pipeline\
  & 73.9\%  
  & \textbf{32.4\%}
  & 46.0\% 
  & 42.3\% 
  & \textbf{45.2\%}\\

\includegraphics[height=0.8em]{assets/graphics/elmo-clipper.png} Llama-3.1-8B-\pipeline\ 
  & \textbf{76.0\%}
  & 32.2\%
  & \textbf{49.0\%}  
  & 46.5\% 
  & 43.6\%\\

\includegraphics[height=0.8em]{assets/graphics/elmo-clipper.png} ProLong-512K-8B-\pipeline\
  & 75.0\% 
  & 32.3\%
  & \textbf{49.0\%}  
  & 38.5\%  
  & 44.5\%\\

ProLong-512K-8B-WritingPrompts
  & 63.0\%  
  & 24.1\%
  & 31.0\%  
  & 35.8\%  
  & \textbf{45.2\%}\\
\bottomrule
\end{tabular}
\caption{Model accuracy on claim verification (\pipeline-test, NoCha) and narrative understanding benchmarks (NarrativeQA, $\infty$Bench QA, MuSR). Finetuning models using \pipeline\ improves performance on claim verification and narrative understanding. }
\label{tab:main-result}
\vspace{-0.2in}
\end{table*}



\paragraph{Ablation on the effect of data length:}
Previous research has found that SFT on short data can improve long-context performance in tasks like QA and summarization more effectively than SFT on long data \cite{dubey2024llama, gao2024trainlongcontextlanguagemodels}. Given that our dataset consists of lengthy texts averaging 90K tokens, we investigate whether fine-tuning on shorter documents can similarly enhance long-context claim verification. We use WritingPrompts~\cite{fan-etal-2018-hierarchical}, a dataset of 300K human-written stories with an average length of 742 tokens. Since the texts are much shorter, we directly extract claims without generating outlines or summaries.\footnote{We use a prompt similar to the one in \S\ref{data:claims_generation}.} We collect 19K claims and train on \prolongbase\ to get \prolongwp.\footnote{After doing hyperparameter tuning on 2K training samples, we decide on the learning rate of 1e-5 and batch size of 16 as the best training configurations. We tested learning rates of 1e-5, 1e-6, 1e-7 and batch sizes of 8, 16, 32, 64.}
% \mi{shorten this para, move any hyperparam stuff to footnote}
% The texts in \dataname~are long (averaging 90K tokens), which raises the question of whether fine-tuning on claims about much shorter documents can yield similar improvements in reasoning performance. We apply our data synthesis pipeline to WritingPrompts~\cite{fan-etal-2018-hierarchical}. Unlike \dataname, WritingPrompts contains much shorter texts (742 tokens on average), which eliminates the need for chapter outlines or book summaries for grounding. Therefore, we extract claims directly from the texts using a nearly identical prompt to the one in Section \ref{data:claims_generation}. We collect 19K claims that require reasoning across multiple events within each short text using this pipeline. After doing hyperparameter tuning on 2K samples from the training set,\footnote{We tested learning rates of 1e-5, 1e-6, 1e-7 and batch sizes of 8, 16, 32, 64.} we decide on the learning rate of 1e-5 and batch size of 16 as the best training configurations. We call the resulting model \dataname-Prolong-WritingPrompts. 

%\paragraph{SFT on answers only} In our standard setup, each claim is accompanied by chain-of-thought reasoning, as shown in Figure \ref{fig:sft_prompt}. To examine the impact of this reasoning component, we remove it from the assistant prompt—excluding everything enclosed within \texttt{<explanation>} tags—and fine-tune Prolong using only the final answers. We call the resulting model \dataname-Prolong-NoCoT. 

\subsection{Evaluation}

Beyond claim verification, we expect that training on our synthetic dataset will also improve performance on related tasks. Therefore, we  include both reasoning and narrative understanding benchmarks that vary in input lengths and tasks.
% \footnote{Inference on \pipeline-test, NoCha, NarrativeQA, and $\infty$Bench QA uses vLLM \cite{kwon2023efficient}, while inference on MuSR uses Accelerate \cite{accelerate}.}
% \mi{need to write more about why we are eval on other non claim verification tasks! e.g. we want to see if SFT on bookclip improves perf on other narrative understanding tasks}

\paragraph{Claim verification:} To measure accuracy, we follow NoCha's approach to calculate the percentage of cases in which a model correctly verifies both true and false claims within a given pair.

\vspace{2pt}
\noindent \textbf{\includegraphics[height=0.6em]{assets/graphics/right-arrow.png} \pipeline-test} contains 1,000 true/false claim pairs drawn from 53 books, evenly split between book-level and chapter-level claims.

\vspace{2pt}
\noindent \textbf{\includegraphics[height=0.6em]{assets/graphics/right-arrow.png} NoCha} \cite{karpinska_one_2024} consists of 1,001 true/false claim pairs about recent fiction books (up to 336k tokens). These claims, crafted by annotators familiar with the books, are much harder to verify compared to those in \pipeline-test. We explore whether SFT on the relatively simpler claims in \pipeline\ can generalize to this challenging set of examples.

% \mi{put citation in para header not after colon}
% \mi{explain that nocha is much harder than your test set bc its written by humans, still we want to see if SFT on our simpler claims can generalize to this challenge set}

\paragraph{General narrative understanding:} We use three existing benchmarks as detailed below.

\vspace{2pt}
\noindent \textbf{\includegraphics[height=0.6em]{assets/graphics/right-arrow.png} NarrativeQA} \cite{kocisky_narrativeqa_2018} is a long-form Q\&A benchmark that requires models to process entire books or movie scripts to answer provided questions. The benchmark consists of 1,572 stories and summaries as well as 46,675 human-written questions. We use the HELMET implementation \cite{yen2024helmet} for this benchmark.

\vspace{2pt}
\noindent \textbf{\includegraphics[height=0.6em]{assets/graphics/right-arrow.png} $\infty$Bench QA} \cite{zhang2024inftybenchextendinglongcontext} is a long-form Q\&A benchmark that requires models to answer 351 questions about novels. We use the HELMET implementation, but remove the maximum output length limit, and use GPT-4o's judgment as a metric instead of ROUGE F1 (see \S\ref{appendix:infbench-metric} for explanation).

\vspace{2pt}
\noindent \textbf{\includegraphics[height=0.6em]{assets/graphics/right-arrow.png} MuSR} \cite{sprague_musr_2024} includes 756 algorithmically generated problems such as murder mysteries, object placement questions, and team allocation optimization. We use the LM Harness~\cite{eval-harness} implementation.

% \mi{if we need to cut space, we can compress all of these non claim verification benchmarks into one para. it might be better organization anyway to have them all in one}