\section*{Limitations}
We only perform hyperparameter tuning on \prolongbase\ due to the high cost of the training process. To put things into perspective, training a model on our full test set requires approximately 50 hours on 8 A100 GPUs, each costing \$2 per hour to rent. Even training on our tuning subset takes 6 hours. Therefore, extending training further is prohibitively expensive. 

Similarly, we do not hire human annotators to write claims for our dataset due to the prohibitive cost and the need for numerous annotators who have thoroughly read the books (Table \ref{tab:six-books}). While this decision may result in less complex claims (\S\ref{subsec:main_results}), our approach offers greater adaptability to new books while significantly reducing costs.

\section*{Ethical considerations}
All scientific artifacts, including generative models and book texts from Project Gutenberg, are used in accordance with their intended purpose to ensure ethical and responsible research practices. 

\section*{Acknowledgment}

We extend special gratitude to Marzena Karpinska for benchmarking our models on NoCha and giving us helpful insights into the models' behaviors. This project was partially supported by awards IIS-2046248, IIS-2312949, and IIS-2202506 from the National Science Foundation (NSF). 