\section{Evaluation}

\begin{table*}[ht]
\centering
\footnotesize
\scalebox{0.85}{
\begin{tabular}{p{0.15\textwidth}p{0.25\textwidth}p{0.2\textwidth}p{0.2\textwidth}p{0.2\textwidth}}
\toprule
\multicolumn{1}{c}{\textsc{Category}} & \multicolumn{1}{c}{\textsc{Definition}} & \multicolumn{1}{c}{\textsc{Example Question}} & \multicolumn{1}{c}{\textsc{Example Ground-truth}} & \multicolumn{1}{c}{\textsc{Example Model Answer}} \\
\midrule
\addlinespace
\multicolumn{5}{c}{\textbf{ Cases that should be counted as incorrect }} \\
\addlinespace
\midrule
Wrong & The model's answer is evidently wrong. & What is the home Edna moves into in New Orleans called? & The pigeon house & The text does not provide information about Edna moving \\
\midrule
\addlinespace
\multicolumn{5}{c}{\textbf{ Cases that should be counted as correct }} \\
\addlinespace
\midrule
Full match & The model's answer perfectly matches the ground-truth answer. & Which among Annalisa, Seb, Peyton, and Gannonmarie is not Mrs. Bronwyn's child? & Peyton & Peyton \\
\midrule
Correct (Phrasing) & The model's answer is correct, but it has a different phrasing than the ground-truth answer. & How old is Felicity at the start of his narration? & Thirty & Felicity is 30 years old at \\
\midrule
Correct (Long) & The model's answer is correct, but it is longer than the ground-truth. & What kind of pet does Madame Bowen keep? & a cat & Madame Bowen keeps a cat as a pet. \\
\midrule
Correct (Short) & The model's answer is correct, but it is shorter than the ground-truth. & Why is Tasha Teigan out of jail? & He has been released on parole. & Paroled. \\
\midrule
\addlinespace
\multicolumn{5}{c}{\textbf{ Cases where correctness is ambiguous }} \\
\addlinespace
\midrule
Truncation & The model's answer has been heavily truncated, making it impossible to tell the correctness of the answer. & What is to be built in place of the Lars home on Wickham Place? & Flats & The house on Wickham Place is to be replaced \\
\bottomrule
\end{tabular}}
\caption{Taxonomy from our analysis on the $\infty$Bench QA outputs of Qwen2.5-7B-Instruct and \qwenftbalanced. Example model outputs are from \qwenftbalanced\ except the one for ``Correct (Short)", which is from Qwen2.5-7B-Instruct (all generated under the default setup where maximum output tokens is set to 10).}
\label{tab:infbench-taxonomy}
\end{table*}


\begin{table}[ht]
\centering
\footnotesize
\scalebox{1}{
\begin{tabular}{p{0.15\textwidth}p{0.10\textwidth}p{0.10\textwidth}}
\toprule
\multicolumn{1}{c}{\textsc{Category}} & \multicolumn{1}{c}{\textsc{Qwen-Inst}} & \multicolumn{1}{c}{\textsc{Qwen-\texttt{BC}}} \\
\midrule
\addlinespace
\multicolumn{3}{c}{\textbf{ Cases that should be counted as incorrect }} \\
\addlinespace
\midrule
Wrong & 41 & 26 \\
\midrule
\addlinespace
\multicolumn{3}{c}{\textbf{ Cases that should be counted as correct }} \\
\addlinespace
\midrule
Full match & 17 & 4 \\
\midrule
Correct (Phrasing) & 3 & 7 \\
\midrule
Correct (Long) & 13 & 18 \\
\midrule
Correct (Short) & 2 & 0 \\
\midrule
\addlinespace
\multicolumn{3}{c}{\textbf{ Cases where correctness is ambiguous }} \\
\addlinespace
\midrule
Truncation & 24 & 45 \\
\bottomrule
\end{tabular}}
\caption{Raw counts of taxonomy categories for \qweninst\ and \qwenftbalanced, with outputs generated using the default maximum length of 10 tokens.}
\label{tab:infbench-counts}
\end{table}


\begin{table*}[ht]
\centering
\footnotesize
\scalebox{0.9}{
\begin{tabular}{p{0.2\textwidth}p{0.15\textwidth}p{0.2\textwidth}p{0.1\textwidth}p{0.3\textwidth}}
\toprule
\multicolumn{1}{c}{\textsc{Question}} & \multicolumn{1}{c}{\textsc{Ground-truth}} & \multicolumn{1}{c}{\textsc{Model Answer}} & \multicolumn{1}{c}{\textsc{ROUGE F1}} & \multicolumn{1}{c}{\textsc{Explanation}} \\
\midrule
How old is Felicity at the start of his narration? & Thirty & Felicity is 30 years old at & 0 & The model is correct, but it uses the numerical form of the number. \\
\midrule
What gender does Harris predict Cal will be? & MALE & Harris predicts that Cal will be a boy. & 0 & The model is correct, but it phrases it differently, resulting in no word overlap. \\
\midrule
When is Jarod's birthday? & NOVEMBER 9 & Jarod's birthday is on November 16 & 0.22 & The model is completely wrong, but it gets the same score as the model answer in the row below, which contains a correct answer. \\
\midrule
In which state is Gopher Prairie located? & Minnesota & Gopher Prairie is located in Minnesota. This is & 0.22 & The model is correct, but it gets the same score as the wrong model answer above, just because the output is much longer than the ground-truth. \\
\bottomrule
\end{tabular}}
\caption{Examples showing that ROUGE-F1 is an unreliable metric.}
\label{tab:infbench-rouge-examples}
\end{table*}


\subsection{Configuration for $\infty$Bench QA Evaluation} \label{appendix:infbench-metric}

In HELMET \cite{yen2024helmet}, for the $\infty$Bench QA task, the default configuration sets the output maximum length to 10 tokens and uses ROUGE F1 \cite{lin-2004-rouge} as the evaluation metric. Upon closer examination of the outputs from both models, we identify critical flaws in the default setup. These findings eventually motivate us to remove the maximum length restriction and adopt the LLM-as-a-judge evaluation approach using GPT-4o as the judge. Below, we provide more details on our analysis.

\paragraph{Setting max output length to 10 tokens frequently cause truncations:} In Table \ref{tab:infbench-taxonomy}, we show the taxonomy we derive from our analysis. Here, we define a truncation to be when a model's response is heavily cut off, \textit{making it impossible to determine the correctness of the output}. Out of all 100 evaluated examples, Qwen2.5-7B-Instruct's outputs get truncated 24 times, while \qwenftbalanced's outputs get truncated 45 times. After removing the 10-token maximum length restriction\footnote{Without the maximum length limit, Qwen2.5-7B-Instruct's outputs are on average 20.9 tokens long, and \qwenftbalanced's outputs have 25.8 tokens on average.}, we observe that 9 of the 24 previously truncated outputs from \qweninst should be counted as correct. For \qwenftbalanced, this correction is even more significant, with 25 of the 45 truncated outputs being technically correct. We combine these numbers with numbers from the four rows in Table \ref{tab:infbench-counts} that indicate correctness, and find that Qwen2.5-7B-Instruct has an overall accuracy of 44\%, while \qwenftbalanced\ has 54\%.

\paragraph{ROUGE F1 is not a reliable metric:} If we use ROUGE F1 as the metric, Qwen2.5-7B-Instruct achieves a score of 27.4, while \qwenftbalanced achieves a score of 18.0. This result sharply contrasts with the accuracies we obtain in the preceding paragraph, and does not reflect the actual performance of the models. Lots of prior work have shown that ROUGE correlates poorly with human judgment \cite{goyal2023newssummarizationevaluationera, chang_booookscore_2024}. Our manual analysis reveals that this metric is overly sensitive to length, and does not capture the correctness of the model outputs. We show several examples in Table \ref{tab:infbench-rouge-examples}.