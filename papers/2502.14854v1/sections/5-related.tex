\section{Related work}  
\paragraph{Long-context language modeling:}
The context windows of LLMs have expanded significantly~\cite{openai2024gpt4technicalreport, dubey2024llama, geminiteam2024gemini15unlockingmultimodal, yang_qwen25-1m_2025}, thanks to position interpolation and extrapolation techniques~\cite{press_train_2022, su_roformer_2023, peng_yarn_2023}, and efficient attention implementation~\cite{dao2022flashattentionfastmemoryefficientexact, dao2023flashattention2fasterattentionbetter,liu2023blockwise}. In addition, longer training data has been utilized during the continual pretraining stage~\cite{dubey2024llama, lieber2024jambahybridtransformermambalanguage, xiong_effective_2023} or alignment stage \cite{bai_longalign_2024, xiong2024artificialneedlesrealhaystacks,an2024makellmfullyutilize}.

%\paragraph{Evaluating Long-context Models} Many long-context benchmarks have been proposed, from Needle in a Haystack (NIAH) and its retrieval-based variants \cite{gkamradt_gkamradtllmtest_needleinahaystack_2024, hsieh_ruler_2024, mohtashami2023landmarkattentionrandomaccessinfinite, liu2024lost, lee2024longcontextlanguagemodelssubsume} and general-purpose benchmarks~\cite{tay2020longrangearenabenchmark, shaham_zeroscrolls_2023, yen2024helmet, zhang_inftybench_2024, an_l-eval_2023, bai_longbench_2023, dong2024bamboocomprehensivebenchmarkevaluating, li2024looglelongcontextlanguagemodels}. More recently, reasoning benchmarks~\cite{kuratov_babilong_2024, karpinska2024thousandpairsnovelchallenge, ling2025longreasonsyntheticlongcontextreasoning} have also been introduced, showing that LCLMs cannot effectively utilized the full long context. In our work, we investigate whether fine-tuning open-source models~\cite{qwen_qwen25_2024, dubey2024llama, gao_how_2024} on a long-context synthetic reasoning dataset can help LLMs utilize their context more effectively.

\paragraph{Instruction-tuning data generation:} 
Short-form instruction-tuning data generation methods either induce instruction data from sample outputs~\cite{honovich2022instructioninductionexamplesnatural, zhou2023largelanguagemodelshumanlevel, li2024selfalignmentinstructionbacktranslation} or generate instruction-output pairs simultaneously~\cite{wang2023selfinstructaligninglanguagemodels}. In long-context scenarios, instruction-following data is synthesized through instruction induction from long-form documents~\cite{bai_longalign_2024, pham_suri_2024, koksal_longform_2023}, random document segments~\cite{xiong_effective_2023, dubey2024llama, yang_qwen25-1m_2025}, or bootstrapping short documents~\cite{an2024makellmfullyutilize, xu2024chatqa2bridginggap, wu2024longcontextalignmentshort, wang2024bootstrapcontextlength}. \pipeline\ uses instruction induction from compressed document representations.
% generating both instructions and outputs via retrieval and query-focused summarization~\cite{wang2024bootstrapcontextlength}, 

\vspace{-2pt}
\paragraph{Reasoning alignment:}
Previous work have explored a variety of approaches to improving LLMs' reasoning abilities, including inference-time scaling~\cite{openai_o1_2024, deepseekai2025deepseekr1incentivizingreasoningcapability, muennighoff2025s1simpletesttimescaling}, prompting~\cite{wei_chain--thought_2023, kojima2023largelanguagemodelszeroshot,yao2023treethoughtsdeliberateproblem, wang2023selfconsistencyimproveschainthought}, and finetuning LLMs on reasoning data~\cite{chung_scaling_2022, huang-etal-2023-large, puerto2024finetuningdivergentchainsthought, yeo2025demystifyinglongchainofthoughtreasoning}. These reasoning data are either human-written rationale~\cite{alkhamissi_opt-r_2023} or chain of thoughts distilled from larger models~\cite{hsieh-etal-2023-distilling,li-etal-2023-symbolic, ho-etal-2023-large, zelikman2022starbootstrappingreasoningreasoning}. We find that finetuning on CoTs improves model explanations.
