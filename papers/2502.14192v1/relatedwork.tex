\section{Related Work}
\paragraph{Scientific Literature Domain Knowledge Graph} 
 Most of the early studies on scientific literature domain knowledge graph are limited to constructing knowledge graphs of the external features (such as title, author, publishers) from scientific papers \cite{zhang2019oag,tang2008arnetminer}. \citet{rs13132511} develops a methodology to identify innovative content in academic literature by extracting novel sentences, recognizing entities, and constructing a knowledge graph focused on innovation-related information in research papers.
 %\citet{du2022academic} constructed a knowledge graph for the knowledge units with semantic information in the paper, including background, topic, problem, research goal, etc., and use manual annotation to extract them. 
 ORKG \cite{jaradeh2019open} provides a structured framework to represent academic knowledge in papers as interconnected and semantically rich knowledge graphs according to user needs. In the field of NLP, NLP-KG \cite{schopf-matthes-2024-nlp} links Fields-of-Study, publications, authors, and venues through semantic relations to form a knowledge graph, and users can retrieve scientific research literature with research domain as the index.
\paragraph{Knowledge Graph Augment LLM Question Answering} 
% Similarly, there are datasets focused on evaluating the long context capabilities of language models.
The most direct way to augment LLM question answering is by integrating knowledge graphs into pre-training, such as using tasks like link prediction for additional supervision \cite{yasunaga2022deep}. Methods like KAPING \cite{baek-etal-2023-knowledge} retrieve relevant facts from the knowledge graph based on semantic similarity to guide LLM answers. Currently, Knowledge Augmented Generation (KAG) \cite{liang2024kag} is popular, using a mutual index structure between knowledge graphs and text to improve cross-document linking. MindMap \cite{wen-etal-2024-mindmap} helps LLM understand knowledge graph node relations by building mind maps, supporting evidence-based generation.
 %The most direct approach to augment LLM question answering is integrating the knowledge graph into LLM pretraining, such as using link prediction as additional supervision \cite{yasunaga2022deep}. 
 %Another approach is to inject the knowledge graph into LLM inference. Early efforts focused on fusing KG triples into the input of LLMS via attention, such as CNTF\cite{varshney2022commonsense} modeling common sense, named entities, and topically specific knowledge via multi-hop attention modules to facilitate dialogue generation tasks. 
 %Some commonly focus on target KG tasks, such as KAPING \cite{baek-etal-2023-knowledge}, based on the semantic similarity between the question and its related facts, retrieve facts related to the input question from the knowledge graph as hints to guide LLM to generate answers. 
 %At present, the popular method is Knowledge Augmented Generation (KAG) \cite{liang2024kag}, which introduces the mutual index structure between the knowledge graph and the original text block, and enhances the ability to link across documents. MindMap \cite{wen-etal-2024-mindmap} helps LLM understand the structural relation of relevant nodes in the knowledge graph by allowing LLM to build a mind map, and supports evidence-based generation.

%