\subsection{Open-Source Math PRM Training Details}
\label{sec:open-mathprm}


The open-source PRMs evaluated in this work utilize CoT training data derived from two mathematical datasets:
MATH~\citep{hendrycks2measuring} and GSM8K~\citep{cobbe2021training}.
The Math-Shepherd and RLHFlow/Deepseek-PRM-Data datasets are synthetically generated following the rollout method proposed by~\citet{wang2024math}.
Similarly, the MATH-APS dataset is produced using the synthetic generation technique introduced by~\citet{luo2024improve}. PRM800K, in contrast, consists of manually annotated labels and was specifically curated for the study by~\citet{lightman2023let}.


All PRMs are trained using the base LLMs of comparable model size and class,
including Mistral-7B~\citep{jiang2023mistral},
Llama-3.1-8B-Instruct~\citep{dubey2024llama},
and Qwen-2.5-Math 8B~\citep{yang2024qwen2}.


\begin{table*}[ht]
    \centering
    \caption{Training details of various Math PRMs}
    \resizebox{\linewidth}{!}
    {
    \begin{tabular}{lccr}
    \toprule
    \textbf{PRM} & \textbf{Base Model} & \textbf{Training Data} & \textbf{Training Method} \\
    \midrule
    Math-PSA & Qwen-2.5-Math-7B-Instruct & PRM800K, Math-Shepherd and MATH-APS & LoRA \\
    Math-Shepherd & Mistral-7B & Math-Shepherd & Full fine-tuning \\
    Qwen-2.5-Math-PRM & Qwen-2.5-Math-7B-Instruct & PRM800K & Full fine-tuning \\
    RLHFLow-Deepseek & Llama3.1-8B-Instruct & RLHFlow/Deepseek-PRM-Data & Full fine-tuning \\
    \midrule
    LlamaPRM800K & Llama3.1-8B-Instruct & PRM800K & Full fine-tuning \\
    QwenPRM800K & Qwen-2.5-Math-7B-Instruct & PRM800K & Full fine-tuning \\
    \bottomrule
    \end{tabular}
    }
    \label{tab:math_prm_details}
\end{table*}


\subsection{Details of PRM Training}
\label{sec:prm-train}




For training, we extract logits from the tokens \texttt{+} and \texttt{-} in the final layer of the LLM. The logit for \texttt{+} corresponds to a correct reasoning step, while the logit for \texttt{-} represents an incorrect step. We use four newline characters \texttt{\textbackslash n\textbackslash n\textbackslash n\textbackslash n} as the classification token, which is appended to the end of each reasoning step. We use standard cross-entropy loss and only compute it over our classification token.

For training our math PRMs on the PRM800K dataset (QwenPRM800K and LlamaPRM800K),
we employ a batch size of 128 and perform full fine-tuning. For experiments on mixed-domain datasets, we reduce the batch size to 32 due to smaller dataset size.

All training is conducted over a single epoch. For full fine-tuning, we use a learning rate of $1.25 \times 10^{-6}$, while for LoRA-based fine-tuning, we use a learning rate of $1.0 \times 10^{-4}$.


\clearpage