
\paragraph{Outcome Reward and Process Reward Models.}
PRMs have proven more effective than ORMs in enhancing LLM reasoning, particularly for mathematical tasks~\citep{luo2024improve, lightman2023let, sun2024easy}. Unlike ORMs, which focus on final outcomes,
PRMs provide step-by-step feedback, improving error detection in intermediate steps and multi-step tasks~\citep{luo2024improve,lightman2023let, uesato2022solving, wang2024math}.
Techniques like OmegaPRM~\citep{luo2024improve} and Math-Shepherd~\citep{wang2024math} reduce reliance on costly human annotations, while RLHflow~\citep{xiong2024rlhflowmath}, OpenR~\citep{wang2024openr} and ProcessBench~\citep{zheng2024processbench} advance PRM evaluation and training. However, the expertise of existing PRMs is mainly limited to mathematical reasoning. Our work extends the capability of PRM to multi-domain reasoning, using synthetic reasoning data for broader applicability. 




\paragraph{Test-Time Inference Algorithms.}
Test-time inference algorithms enhance LLM reasoning by adding computation during inference. AlphaCode~\citep{li2022competition} shows how test-time computing boosts competitive programming performance, while \citet{snell2024scaling} argue that scaling inference-time compute, rather than model parameters, yields better results by adapting compute allocation to prompt difficulty. Test-time inference includes self-improvement methods like Tree of Thoughts (ToT)~\citep{yao2024tree},
self-verification~\citep{weng2022large},
and stepwise self-evaluation~\citep{xie2024self},
and external-verifier methods such as verifier reranking~\citep{cobbe2021training,feng2023alphazero}, 
tool feedback~\citep{gou2023critic},
reward-guided reasoning~\citep{yang2024reinforcing},
and multi-agent debate~\citep{du2023improving}.
Our work uses PRMs for reranking solutions and guiding reasoning within the external verifier paradigm.



\paragraph{Synthetic Data Generation.}

Obtaining fine-grained step-wise labeling of CoTs via expert annotation is costly and time-consuming. 
Automated annotation methods, such as rollout-based approaches, reduce human effort but require numerous model inference, which is computationally expensive.
Recent effort to mitigate these limitations include multi-rollout generation per reasoning step~\citep{wang2024math,wang-etal-2024-multi-step} and efficiency improvements via binary search~\citep{luo2024improve}. 
Despite these advancements, the reliance on multiple model calls remains a bottleneck. Recent studies by~\citet{pnasFabrizio} and \citet{fonseca-cohen-2024-large} demonstrate the potential of LLMs as data labelers. In line with the work of~\citet{zheng2024processbench}, our work uses LLMs as labelers to automatically generate process reward annotations, enabling the production of synthetic data across multiple domains without high computational costs.