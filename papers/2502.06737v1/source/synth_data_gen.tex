\begin{figure*}[ht]
    \begin{center}
        \includegraphics[width=0.91\textwidth]{figures/Synthetic_Data_Pipeline.pdf}
        \caption{A diagram of the synthetic data generation pipeline. In the CoT Generation Stage, each question is used to generate 16 CoT solutions. Then, in the Auto-Labeling Stage, each CoT is evaluated to create step-wise labels. If a CoT step is labeled as \textbf{BAD}, all subsequent steps will be discarded.}
        \label{fig:synth-data-pipeline-diagram}
    \end{center}
    \vskip -0.2in
\end{figure*}




In order to obtain step-wise reasoning data for non-Math domains,
we devise a pipeline,
as outlined in~\Cref{fig:synth-data-pipeline-diagram},
to generate synthetic reasoning CoTs from existing question-answering data.
These CoTs are then given step-wise labels based on reasoning correctness.
We detail the synthetic data generation process in~\Cref{sec:cot-gen,sec:auto},
including methods to create and annotate reasoning steps. We also provide additional analysis on the quality of the generation pipeline in~\Cref{sec:auto-analysis-ablate}.


\subsection{Chain-of-Thought Generation}
\label{sec:cot-gen}




For the generation of CoTs,
we prompt Llama-3.1-8B-Instruct to produce step-by-step reasoning for each input question. For training, we source questions from the MMLU-Pro dataset~\citep{wang2024mmlupro}, selected for its high-quality, challenging problems spanning diverse topics.
From this dataset, we randomly sample up to 500 questions per domain, ensuring that it is disjoint to the subset used for evaluation. We then generate 16 CoTs for each sampled question. Post-generation, we filter out CoTs exceeding the 2,048-token limit or containing unparsable answers.


\subsection{Auto-Labeling}
\label{sec:auto}


To annotate our synthetic CoT data, we adopt an approach inspired by the critic models in the work of~\citet{zheng2024processbench}.
Specifically, we utilize Llama-3.1-70B-Instruct as a strong LLM to evaluate each CoT using step-by-step reasoning, locating the earliest erroneous step, if any. To enhance accuracy and consistency, we identified two key additional components.


First, we incorporate explicit step evaluation definitions,
inspired by~\citet{lightman2023let},
into the system prompt.
Steps are categorized as \textbf{GOOD},
\textbf{OK},
or \textbf{BAD}:
\textbf{BAD} for incorrect, unverifiable, or irrelevant steps;
\textbf{GOOD} for correct, verifiable, and well-aligned steps;
\textbf{OK} for intermediate cases.
Second, we also provide the reference ground-truth answer in the prompt.
The full prompt is detailed in~\Cref{sec:synth-gen-prompts}.


To convert the auto-labeling output to stepwise labels, we apply the following rule:
if no steps are detected as incorrect, all steps in the CoT are labeled as $1$.
If a step is detected as incorrect, all preceding steps are labeled as $1$, the incorrect step is labeled as $-1$,
and all subsequent steps are discarded.



In total, we sample 5,750 questions from MMLU-Pro. Among the 84,098 generated CoTs that passed filtering, 36,935 were labeled as having no incorrect steps and 47,163 were labeled as having at least one (see Table \ref{tab:dataset_composition}). This dataset, denoted as \emph{\ourdatatrain}, is the first open-source multi-domain reasoning dataset with step-wise labels.






To assess the quality of our auto-labeled data,
we conduct a manual evaluation on a random sample of 30 questions from the dataset.
For each question, we randomly select one CoT classified as entirely correct and two CoTs flagged as containing an incorrect step.
We then manually validate whether the auto-labeled judgments align with our own assessments.


For the CoTs labeled as correct by the auto-labeler, we observed an agreement rate of 83\% with our manual evaluations.
For CoTs labeled as incorrect, the agreement rate was 70\%.

Based on these results, we estimate that approximately 75\% of the CoTs in the entire dataset are correctly labeled.
This level of accuracy is comparable to that of manually-labeled CoT datasets,
such as PRM800K~\citep{lightman2023let},
which is estimated to achieve around 80\% accuracy.\footnote{Refer to \href{https://github.com/openai/prm800k/issues/12}{this GitHub issue} for a discussion on PRM800K's accuracy.}


\subsection{Auto-Labeling Prompt Analysis}
\label{sec:auto-analysis-ablate}


To further understand the factors influencing auto-labeling performance,
we conduct an evaluation of the auto-labeling using a simplified prompt.
Specifically, we remove the system prompt defining the types of reasoning steps and exclude the reference ground-truth answer from the prompt.
When re-evaluating the auto-labeling quality,
we observed a drastic drop in performance, with the agreement rate for CoTs labeled as correct by the original auto-labeler decreasing by over 70\%,
from 83\% to 7\%, while
the agreement rate for CoTs labeled as incorrect decreased
from 70\% to 62\%.


These results highlight the importance of providing both a well-defined prompt with step label definitions and access to the ground-truth answer in achieving high auto-labeling accuracy.
The ground-truth answer provides essential context on CoT final correctness and enhances the model's ability to evaluate reasoning steps effectively.


\subsection{Counterfactual Augmentation}


To generate additional examples of incorrect reasoning,
we explore methods for instructing an LLM to modify steps in our correct CoTs,
introducing specific types of errors.
We refer to this process broadly as counterfactual augmentation.
However, incorporating counterfactual error steps during PRM training was not observed to significantly improve performance.
Therefore, we defer specific details and experiments using counterfactual augmentation to~\Cref{sec:counter-aug}.
