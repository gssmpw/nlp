We present the implementation and evaluation of \ourprm, structured as follows.
First, \Cref{sec:mdprm-train} covers the various training configurations used.  
We then evaluate \ourprm via BoN and WMV in \Cref{sec:math-v-mdprm}, showing improved domain generalization compared to math PRMs.  
In \Cref{sec:m-v-mdprm-search}, we additionally discuss results using Beam Search and MCTS. Lastly, we examine \ourprm's ability to scale test-time compute for larger models such as Deepseek-R1~\cite{guo2025deepseek} in~\Cref{sec:deepseek}.



\subsection{Training of Our Multi-Domain PRM}
\label{sec:mdprm-train}

To train \ourprm, we employ a classification head atop an LLM,
optimizing with a cross-entropy loss applied to a special classification token appended at the end of each CoT step in \ourdatatrain.
Detailed specifics and hyperparameters are provided in~\Cref{sec:prm-train}.


We explore several training configurations,
including:
1) LoRA~\citep{hu2022lora} vs.~full fine-tuning for efficient training,
2) a base LLM vs.~a math PRM for initializing the PRM,
and 3) a Qwen-based PRM vs.~a Llama-based PRM for training.
Comprehensive experimental results for these studies are presented in the next section.
Based on those findings, our final,
our final multi-domain PRM, named~\ourprm, is initialized from our LlamaPRM800K---see~\Cref{sec:add-prm-train} for its details---fine-tuned using LoRA on our multi-domain training dataset.
% \vspace{-6mm}
\subsection{Math PRM vs.~\ourprm~on Reranking Based Inference-Time Methods}
\label{sec:math-v-mdprm}

We first report results of the reranking methods WMV and BoN on \ourdataeval.
For both methods, we adopt Min-aggregation, as it outperforms Average and Last in aggregating PRM step scores;
see~\Cref{sec:agg-comp} for comparison.
We also include MV as a baseline.



\textbf{Comparison with Math Open-Source PRMs.}
We evaluate our multi-domain PRM, \ourprm, against open-source math PRMs by partitioning~\ourdataeval~into three groups:
1) \emph{Math},
2) \emph{Math-adjacent}, i.e., Chemistry, Computer Science, Engineering, Physics,
and 3) \emph{non-Math-adjacent} domains.
As shown in~\Cref{fig:math-wmv-min},
our model consistently outperforms baselines in both WMV and BoN across all domain groups.


\begin{highlight}
    \paragraph{Finding 2:} 
    \emph{Fine-tuning with synthetic multi-domain data enhances the generalizability of PRM.}
\end{highlight}


For WMV, we can see the relative performance difference increase with domain distance from core mathematics. While performance of open-source math PRMs converges to the majority voting baseline in non-mathematical domains, our multi-domain PRM maintains robust generalization.


In BoN the superiority of our multi-domain PRM is even more pronounced. Unlike open-source math PRMs, which fail to surpass the baseline of MV in Math-adjacent and non-Math-adjacent domains,
our model consistently surpasses it across all domain groups.


See~\Cref{sec:bon-mv-bycat} for more fine-grained details where we plot WMV and BoN for every domain of~\ourdataeval.
The results are consistent with~\Cref{fig:math-wmv-min}, and~\ourprm~outperforms math PRMs in all domains.



\begin{figure*}[t]
    \begin{center}
        \includegraphics[width=.94\linewidth]{figures/new_figures/math_vs_nonmath_prm_min_agg.pdf}        
        \caption{Comparison of WMV (top) and BoN (bottom) using \ourprm~against open-source math PRMs on~\ourdataeval. We use min-aggregation and the CoTs are generated using Llama-3.1-8B-Instruct. \ourprm~has consistently better performance than math PRMs, and the differences become larger in domains not adjacent to Math.}
        \label{fig:math-wmv-min}
    \end{center}
    \vskip -0.2in
\end{figure*}


\textbf{Ablation Experiments Using Multi-Domain PRM Trained on Math Only Subset vs.~Random Subset.}

We further conduct an ablation study to evaluate the impact of training data diversity on the performance of our LlamaPRM800K Math PRM.
Specifically, we train one PRM using only the math subset of our multi-domain training data and another using a random subset of the \emph{same} size.
We refer to these two models as \ourprm~(Math subset) and \ourprm~(random subset), respectively.
This experiment tests that the improved performance of our multi-domain PRM is due to the domain-diversity of the CoT data and not merely from learning the in-distribution question and CoT formats of MMLU-Pro questions. If the latter is the case, both PRMs should perform similarly, given that they are exposed to the same amount of questions and CoT examples with the in-distribution format.

\begin{highlight}
    \paragraph{Finding 3:} 
    \emph{Domain diversity of CoTs in a training dataset plays an integral role in generalization of PRMs to multiple domains.}
\end{highlight}

As shown in \Cref{fig:prm-ablation}, \ourprm~(random subset) achieves superior performance in WMV compared to \ourprm~(Math subset).
This trend holds across both Math and non-Math domains. These findings suggest two key insights.
First, our PRM is not simply learning the question format but is acquiring knowledge on how to label reasoning across diverse domains. This is why training on diverse data enables better overall performance than training on same sized data in only one domain. Second, \ourprm~(random subset) also demonstrates slightly better performance in the math domain, indicating that training on a diverse dataset may facilitate positive transfer, where insights from other domains enhance reasoning in the Math domain.



\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/new_figures/prm_diff_train_subset_min_agg.pdf}
    \caption{Comparison of WMV using LlamaPRM800K, \ourprm~(Math subset) and \ourprm~(random subset). \ourprm~(random subset) achieves better performance than \ourprm~(Math subset) in Math and non-Math.}
    \label{fig:prm-ablation}
    \vskip -0.2in
\end{figure}



\textbf{Experiments Using Other Training Configurations.}
While our final version of~\ourprm~is trained from LlamaPRM800K on our synthetic data using LoRA, we also test the following training configurations on our multi-domain dataset:
\begin{itemize}[leftmargin=10px]

    \item \textbf{\ourprm~(Llama Base)}: We initialize training from Llama-3.1-8B-Instruct, and use LoRA fine-tuning with our multi-domain dataset.

    \item \textbf{\ourprm~(Qwen)}: We initialize training from QwenPRM800K PRM, and utilize LoRA fine-tuning with our multi-domain dataset.

    \item \textbf{\ourprm~(full-tuned)}: We initialize training from LlamaPRM800K PRM, and do \emph{full} fine-tuning with our multi-domain dataset.

\end{itemize}

The results are presented in~\Cref{fig:multiprm-trainexps}.
Comparing \ourprm~(Qwen) and \ourprm~(Llama), we observe that the QwenPRM800K~\ourprm~performs worse.
This highlights the importance of base model choices. Although Qwen-2.5-Math-7B, the base model for QwenPRM800K, is specialized in mathematical reasoning, its limitations in general-domain knowledge hinder its ability to fully leverage multi-domain training data.


\begin{highlight}
    \paragraph{Finding 4:} 
    \emph{Exposure to mathematical data beforehand can enhance a PRMs' ability to effectively leverage multi-domain CoT fine-tuning.}
\end{highlight}


Next, comparing \ourprm~(Llama Base) with \ourprm, we find that the latter achieves superior performance in Math while maintaining comparable performance in non-Math domains. This suggests that prior exposure to mathematical data enhances the modelâ€™s ability to benefit from further domain-specific training.

We note that \ourprm~(full-tuned) has worse performance than \ourprm.
This may be due to suboptimal hyperparameters leading to overfitting during full fine-tuning.


\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/new_figures/prm_diff_min_agg.pdf}
    \caption{Comparison of MVW using \ourprm~against other multi-domain PRMs trained using different configurations. \ourprm~has better WMV performance than all other models in both Math and non-Math domains.}
    \label{fig:multiprm-trainexps}
\end{figure}



    
\subsection{Math PRM vs. Multi-Domain PRM on Search Based Inference-Time Methods}
\label{sec:m-v-mdprm-search}

We evaluate the performance of math PRMs (using LlamaPRM800K) and~\ourprm~with beam search and MCTS on~\ourdataeval.
The results over questions in all domains, presented in~\Cref{fig:prm-mcts},
show that MCTS outperforms beam search and that they both do better than the MV baseline.
Regardless of the search algorithm used, consistent with our WMN and BoN results,~\ourprm~gives boosted performance over the math PRM.
Details by category results are presented in~\Cref{sec:mcts-detailed}.


\begin{figure}[t]
    \centering
    \includegraphics[width=.95\columnwidth]{figures/new_figures/mcts_vs_beam.pdf}
    \caption{Comparison of \ourprm~and LlamaPRM800K with beam search and MCTS. Overall in the diverse domains from~\ourdataeval, \ourprm~achieves better performance.}
    \label{fig:prm-mcts}
    \vskip -0.2in
\end{figure}


\subsection{Does PRM with Test-Time Compute help Reasoning Models?}
\label{sec:deepseek}



\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/weighted_majority_voting_comparison_min.pdf}
    \caption{Comparison of WMV using~\ourprm~against Qwen-2.5-Math-PRM on DeepSeek-R1 generated CoTs for the Law subset. \ourprm~has better performance than all other math PRMs.}
    \label{fig:multiprm-deepseek}
\end{figure}


We have shown that~\ourprm~can effectively leverage inference-time compute to increase LLM performance,
a natural question is whether this effectiveness extends to renowned strong reasoning models,
e.g., DeepSeek-R1~\citep{guo2025deepseek}.
Given that a well-trained reasoning model may already generate coherent and correct reasoning steps due to being trained for reasoning,
one might hypothesize that reranking methods like WMV and BoN brings marginal improvement over WM.


To test this, we evaluate the performance of~\ourprm~via WMV on DeepSeek-R1.
Due to budget constraints, we focus on the Law subset and sample 16 CoT responses per question. As shown in~\Cref{fig:multiprm-deepseek}, \ourprm~provides a slight but noticeable performance boost to DeepSeek-R1 during test-time inference despite the limited CoT samples. Significantly it outperforms both the math PRM \emph{and} the MV baseline.
This finding, though preliminary, nullifies the aforementioned hypothesis and suggests that---in fact---large reasoning models \emph{can} still benefit from PRMs during inference to further boost their performance beyond MV.




