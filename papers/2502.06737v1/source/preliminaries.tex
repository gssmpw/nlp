
Similar to the work of~\citet{lightman2023let},
we define process rewards to represent the correctness of each step, and whether it is logical and follows from previous steps.



To formally define a PRM,
we begin by specifying a CoT $S = (s_1, s_2, \dots, s_k)$ as a sequence of $k$ reasoning steps,
where $s_i$ is the $i$-th step in the CoT for each $i \in [k]$.
A PRM can then be formally characterized as a function that maps each CoT $S$ to an associated $k$-dimensional vector of rewards:
$\text{PRM}(S) \in [0,1]^k$.
The $i$-th coordinate of the output score vector, denoted as $\text{PRM}(S)_i$, represents the PRM score for the correctness of the reasoning step $s_i$.

\subsection{Score Aggregation Methods}

Using a PRM, we can obtain scores for each reasoning step.
To then scalarize the reward score vector of the whole CoT, we consider the following three aggregation methods.

\textbf{Min-Aggregation.}
We use the minimum PRM step score in a CoT as the aggregated score:
\[\text{Aggr}_\text{min} (S) = \min_{i\in[k]} \text{PRM}(S)_i.\]

\textbf{Last-Aggregation.}
We utilize the PRM score of the last step in a CoT as the aggregated score:
\[\text{Aggr}_\text{last} (S) = \text{PRM}(S)_k.\]

\textbf{Average-Aggregation.}
We employ the average PRM step score of the CoT as the aggregated score:
\[\text{Aggr}_\text{avg} (S) = \frac{1}{k}\sum_{i\in[k]}\text{PRM}(S)_i.\]


These aggregated scores are particularly useful for solution reranking and are employed in the test-time inference algorithms described below~\citep{wang2024math,sun2024easy,lightman2023let}.


\subsection{Inference-Time Methods}\label{sec:inference-time-methods}

In this section, we introduce three \emph{reranking}-based methods—Majority Voting, Weighted Majority Voting, and Best-of-$N$—along with two \emph{search}-based methods—Beam Search and Monte Carlo Tree Search.

Let $a_S$ denote the final answer in a CoT $S$, which in practice can be extracted using a suitable parser. Further let $\mathcal{S}_N =\{S_1, S_2, \dots, S_N\}$ denote a set of $N$ CoTs sampled i.i.d.~from a generator over a particular question.


\textbf{Majority Voting (MV).}
MV~\citep{wang2022self} is a robust baseline inference-time method that does not require a PRM. Specifically, we first sample $N$ candidate solutions to a problem from a generator. The final answer is then determined by selecting the solution that appears most frequently among these $N$ candidates:
\[\text{MV}(\mathcal{S}_N) = \argmax_{a_S: S \in \mathcal{S}_N} \sum_{i\in[N]} \mathds 1_{a_S}(a_{S_i}).\]


\textbf{Weighted Majority Voting (WMV).}
This method, as used by~\citet{uesato2022solving},
is similar to MV.
We still first sample $N$ candidate solutions.
However, we weight the frequencies of CoTs with identical answers by the aggregation scores. The final answer is the one with highest sum of weights:
\[\text{WMV}(\mathcal{S}_N) = \argmax_{a_S: S \in \mathcal{S}_N} \sum_{i \in [N]} \mathds 1_{a_S}(a_{S_i}) \cdot \text{Aggr}(S_i).\]


\textbf{Best-of-$N$ (BoN).}
This method also samples $N$ candidate solutions. It then reranks them using the aggregation score from a PRM. The answer of the solution with highest score is chosen as final answer:
\[\text{BoN}(\mathcal{S}_N) = \argmax_{a_S: S \in \mathcal{S}_N} \text{Aggr}(S).\]


\textbf{Beam Search.}
This method~\citep{snell2024scaling} is initialized with a fixed number of beams $N$ and width $M$. The process starts by sampling $N$ initial predictions for the first reasoning step. These are ranked via the PRM’s step score, retaining the top $\frac{N}{M}$ candidates. For each retained candidate, $M$ proposals for the next step are sampled, yielding $N$ new candidates. This iterates until all beams reach solutions or a maximum iteration limit. The final prediction is selected based on the highest aggregated PRM score across steps. See~\Cref{alg:beam} for details.




\textbf{Monte Carlo Tree Search (MCTS).}
MCTS is a search algorithm used during test-time inference~\citep{hao2023reasoning,feng2023alphazero} that iteratively builds a search tree to find the CoT with the highest aggregated PRM score.
A detailed description is presented in~\Cref{sec:search-algs} and the pseudo-code is provided in~\Cref{alg:mcts}.
