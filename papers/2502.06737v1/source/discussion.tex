

We proposed \ourprm~trained using synthetic reasoning data to address the limitations of existing math PRMs.
By leveraging a cost-efficient synthetic data generation pipeline, we enabled production of high-quality step-wise reasoning data and demonstrate that PRMs can effectively scale reasoning ability at inference time in diverse domains.




\textbf{Future Work.}
Several directions remain for advancing multi-domain PRMs. Can~\ourprm~be effectively used as a reward model for RL training? Can it improves RL training beyond math PRMs? Could more sophisticated counterfactual augmentation enhance PRM effectiveness? Also, evaluating PRMs on harder, open-ended problems would better assess their generalization. Lastly, more thorough exploration of PRMs with large models (e.g., GPT-4~\citep{achiam2023gpt} and DeepSeek-R1) could clarify their scalability and role in state-of-the-art reasoning systems.





