

We introduce various math PRMs used for comparison in~\Cref{sec:existing-mathprm}, present our multi-domain evaluation dataset in~\Cref{sec:mmlu-eval}, and provide a detailed analysis of the evaluation results in~\Cref{sec:mathprm-eval}.


\subsection{Open-Source Math PRMs}
\label{sec:existing-mathprm}



For evaluation,
we conduct experiments on a diverse set of models.
Our analysis includes four open-source math PRMs:
Math-PSA~\citep{wang2024openr},
Math-Shepherd~\citep{wang2024math},
RLHFLow-Deepseek~\citep{xiong2024rlhflowmath},
and Qwen-2.5-Math-PRM~\citep{zheng2024processbench}.


In addition to the open-source models,
two math PRMs based on open-source models are specifically trained in this work.
They are denoted as \emph{LlamaPRM800K} and \emph{QwenPRM800K}. More details are given in~\Cref{sec:open-mathprm}



\subsection{Multi-Domain Evaluation Dataset}
\label{sec:mmlu-eval}


For our multi-domain evaluation dataset, we curate questions sampled from the MMLU-Pro dataset~\citep{wang2024mmlupro}.
MMLU-Pro is designed to benchmark the reasoning abilities of LLMs and consists of college-level multiple choice questions in the following 14 domains:
\emph{Math}, \emph{Physics}, \emph{Chemistry}, \emph{Law}, \emph{Engineering}, \emph{Other}, \emph{Economics}, \emph{Health}, \emph{Psychology}, \emph{Business}, \emph{Biology}, \emph{Philosophy}, \emph{Computer Science}, and \emph{History}.


To craft our evaluation dataset, we randomly sample 150 questions from each domain. Due to duplicate questions, we discard 41 questions---23 from Biology, 10 from Health, 5 from Law, and 1 each from Business, Economics, and Philosophy. For each remaining question, we generate 128 candidate solutions using Llama-3.1-8B-Instruct~\citep{dubey2024llama} for MV, WMV, and BoN test-time inference algorithms.
Prompt details and generation parameters are provided in~\Cref{sec:synth-gen-prompts}.
We refer to this multi-domain evaluation dataset as \emph{\ourdataeval}.


\begin{table}[t]
\caption{Results of two open-source math PRMs on different domains in~\ourdataeval~when using WMV with min-aggregation on 16 CoTs generated per question using Llama-3.1-8B-Instruct. In parenthesis we report absolute difference between WMV and MV (WMV$-$MV). While WMV using math PRMs exhibits greater improvement in Math and Math-adjacent domains, there is no significant improvement on MV in other domains.}
\label{tab:math_prm_on_multi_domainsec4}
\small
\centering
\setlength{\tabcolsep}{3pt}
\begin{tabularx}{\linewidth}{l|c|
>{\centering\arraybackslash}X
>{\centering\arraybackslash}X}
\toprule
\textbf{Category} & \textbf{MV}  & \textbf{Math-Shepherd} & \textbf{Qwen-2.5-Math-PRM}   \\
\midrule
All & 57.15  & 57.66 (+0.51) & 58.17 (+1.02)\\
All except math & 56.61  & 57.01 (+0.40) & 57.32 (+0.71) \\
Math & 62.40  & 64.13 (+1.73) & 67.20 (+4.80)   \\
\midrule
Chemistry & 58.67  & 60.13 (+1.46) & 60.67 (+2.00) \\
Physics & 58.53 & 61.87 (+3.34) & 61.47 (+2.94) \\
\midrule
Biology & 75.38 & 75.38 (+0.00) & 75.69 (+0.31) \\
Psychology & 61.60  & 61.47 (-0.13) & 62.27 (+0.67)   \\
Law & 35.93 & 37.24 (+1.31) & 36.28 (+0.35)   \\
History & 49.20 & 49.87 (+0.67) & 49.40 (+0.20)  \\
Philosophy & 44.83  & 44.70 (-0.13) & 45.17 (+0.34) \\
\bottomrule
\end{tabularx}

    \vskip -0.2in
\end{table}



\subsection{Multi-Domain Performance of Math PRMs}
\label{sec:mathprm-eval}





We conduct comprehensive analyses on a diverse set of models. For clarity, we report results for two representative models here, with additional evaluations available in~\Cref{sec:mathprm-fullevals}. The first model, Math-Shepherd~\citep{wang2024math},
is trained on synthetically generated math data labeled via a rollout-based method.
The second model,
Qwen-2.5-Math-PRM~\citep{zheng2024processbench},
is a best-performing open-source PRM,
trained on the high-quality expert labeled PRM800K math dataset~\citep{lightman2023let}.


The PRMs are applied using WMV with min-aggregation. While math PRMs show significant improvements in mathematical reasoning domains,
their effectiveness in broader, non-mathematical areas remains limited.
Notably, in the Math category,
Qwen-2.5-Math-PRM and Math-Shepherd achieve relative gains of $+4.80$ and $+1.73$,
respectively,
outperforming the MV baseline.
Similar improvements are observed in Math-adjacent disciplines:
Chemistry ($+2.00$ for Qwen-2.5-Math-PRM) and Physics ($+3.34$ for Math-Shepherd),
underscoring their utility in tasks requiring mathematical reasoning.

\begin{highlight}
    \paragraph{Finding 1:} 
    \emph{Math PRMs struggle to generalize to broader domains.}
\end{highlight}

However, the benefits diminish sharply in non-mathematical areas.
For example, in Philosophy and History, we see gains of only $+0.34$ and +0.20\% respectively for the most performant PRM Qwen-2.5-Math-PRM.

The ``All except math'' aggregate further underscores this disparity, with PRMs achieving a maximum gain of $+0.71$ (Qwen-2.5-Math-PRM) compared with the majority voting baseline.

These results highlight a critical limitation: math PRMs trained exclusively on mathematical data lack the versatility to generalize beyond mathematical reasoning tasks. While they excel in contexts aligned with their training---quantitative reasoning---their capacity to evaluate reasoning quality in broader domains remains insufficient.
