

Large Language Models (LLMs) have demonstrated significant potential in tackling complex reasoning tasks.
Specifically,
they can employ a step-by-step Chain-of-Thought (CoT) approach to generate more accurate and reliable solutions~\citep{wei2022chain,NEURIPS2022_8bb0d291,yao2023react,NEURIPS2023_91edff07}.
Moreover, by using additional test-time computation, the reasoning performance of LLMs can be further enhanced~\citep{snell2024scaling,yao2024tree}.


An important and widely-adopted test-time computation method is using \emph{external verifiers},
such as reward models to rank multiple generated solutions and select the best answer~\citep{lightman2023let}.
Reward models evaluate the quality of solutions, helping guide LLMs toward better outputs.
In particular,
Outcome Reward Models (ORMs) are used to provide supervision based solely on the correctness of the final outcome.
However,
ORMs fail to address errors in intermediate steps,
limiting their effectiveness for complex, multi-step reasoning tasks~\citep{luo2024improve,lightman2023let, sun2024easy}.
Because ORMs suffer from this limitation, 
Process Reward Models (PRMs) have been proposed to offer
\emph{fine-grained, step-by-step feedback} on the correctness of each reasoning step~\citep{lightman2023let,uesato2022solving}.
PRMs have proven highly effective during inference, improving the reranking of generated solutions and guiding LLMs through search-based algorithms~\citep{feng2023alphazero,wang2024openr}.




\begin{figure*}[t]
    \begin{center}
        \includegraphics[width=\textwidth]{figures/multi_domain_prm-11.pdf}        
        \caption{
        Existing open-source PRMs trained on math datasets achieve strong math performance and can outperform a majority voting baseline when used via weighted majority voting. However, these PRMs fail to generalize to other domains (e.g., Law, Philosophy, and Biology), performing no better than the baseline. We propose a multi-domain PRM, \ourprm, obtained by further fine-tuning a math PRM on a synthetically generated multi-domain dataset. The resulting PRM effectively generalizes  beyond math, improving test-time reasoning across multiple domains.
        }
        \label{fig:math-prm-bad}
    \end{center}
    \vskip -0.2in
\end{figure*}


Several studies have shown that PRMs trained on extensive process supervision significantly outperform ORMs in mathematical reasoning tasks,
with notable improvements reported on datasets such as MATH500 and GSM800K~\citep{luo2024improve,lightman2023let,uesato2022solving}.
While substantial investigation has been made in creating training data~\citep{lightman2023let,wang2024math},
training PRMs~\citep{xiong2024rlhflowmath},
and evaluation~\citep{zheng2024processbench} with respect to mathematical reasoning,
the application of PRMs to \emph{non-mathematical domains}---such as Biology,
Chemistry, and Law---remains underexplored.
To investigate the capability of math PRMs in non-mathematical domains,
we test open-source math PRMs such as 
Math-Shepherd~\citep{wang2024math} and Qwen-2.5-Math-PRM~\citep{zheng2024processbench}.
Not surprisingly,
these PRMs demonstrate poor performance,
indicating their limited domain generalizability.
They exhibit only marginal improvements over the baseline in Law, Philosophy, and Biology
as illustrated in~\Cref{fig:math-prm-bad}.


To address this limitation,
we propose fine-tuning PRMs on a synthetically generated multi-domain CoT dataset,
to significantly enhance reasoning capabilities beyond mathematics.
We call this resulting multi-domain PRM~\emph{\ourprm}, short for versatile PRM.
Notably,
by sampling questions from the MMLU-Pro dataset~\citep{wang2024mmlupro},
we generate CoTs to produce step-by-step reasoning
using an LLM-based generator,
i.e., Llama-3.1-8B-Instruct~\citep{dubey2024llama},
and then auto-label them using an LLM-based labeler,
i.e., Llama-3.1-70B-Instruct~\citep{dubey2024llama}.
\ourprm, which is trained on the resulting synthetic multi-domain reasoning data,
shows strong performance across diverse domains.
We validate its superior performance using various empirical analyses on~\ourprm~against existing open-source PRMs.


Our contributions are summarized as follows:
\begin{enumerate}[leftmargin=10px]

    \item We identify the limited domain generalizability of open-source math PRMs in~\Cref{sec:math-lim}.

    \item We propose a novel data generation and annotation pipeline across multiple domains in~\Cref{sec:synth-data-gen}.

    \item We introduce a large-scale, high-quality multi-domain process supervision dataset, dubbed \emph{\ourdatatrain}.

    \item We train a well-generalized PRM that outperforms existing baselines, demonstrating strong generalization across diverse domains in~\Cref{sec:multi-eval}.

    \item We open-source the implementation of~\ourprm~with training details, our multi-domain reasoning data, and its model checkpoint; available at \url{https://github.com/UW-Madison-Lee-Lab/VersaPRM}.
\end{enumerate}