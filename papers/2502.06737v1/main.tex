\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{tcolorbox}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2025}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{dsfont}
\usepackage{enumitem}
\DeclareMathOperator*{\argmax}{arg\,max}
\usepackage[capitalize,noabbrev]{cleveref}

\newtcolorbox{highlight}[1][]{
    colback=yellow!10,
    colframe=gray!30,
    boxrule=0.5pt,
    arc=2pt,
    leftrule=3pt,
    rightrule=3pt,
    toprule=1pt,
    bottomrule=1pt,
    #1
}

\newcommand{\ourprm}{VersaPRM}
\newcommand{\ourdatatrain}{MMLU-Pro-CoT-Train (Labeled)}
\newcommand{\ourdataeval}{MMLU-Pro-CoT-Eval (Unlabeled)}


\icmltitlerunning{VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data}

\begin{document}

\twocolumn[
\icmltitle{VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Thomas Zeng}{yyy}
\icmlauthor{Shuibai Zhang}{yyy}
\icmlauthor{Shutong Wu}{yyy}
\icmlauthor{Christian Classen}{yyy}
\icmlauthor{Daewon Chae}{zzz}
\icmlauthor{Ethan Ewer}{yyy}
\icmlauthor{Minjae Lee}{fff}
\icmlauthor{Heeju Kim}{fff}
\icmlauthor{Wonjun Kang}{fff}
\icmlauthor{Jackson Kunde}{yyy}
\icmlauthor{Ying Fan}{yyy}
\icmlauthor{Jungtaek Kim}{yyy}
\icmlauthor{Hyung Il Koo}{fff}
\icmlauthor{Kannan Ramchandran}{bbb}
\icmlauthor{Dimitris Papailiopoulos}{yyy}
\icmlauthor{Kangwook Lee}{yyy}

\end{icmlauthorlist}

\icmlaffiliation{yyy}{University of Wisconsin--Madison}
\icmlaffiliation{zzz}{Korea University}
\icmlaffiliation{fff}{FuriosaAI}
\icmlaffiliation{bbb}{University of California, Berkeley}

\icmlcorrespondingauthor{Kangwook Lee}{kangwook.lee@wisc.edu}


\icmlkeywords{Process Reward Models, Multi-Domain Process Reward Models, Synthetic Reasoning Data}

\vskip 0.3in
]


\printAffiliationsAndNotice{}


\begin{abstract}
Process Reward Models (PRMs) have proven effective at enhancing mathematical reasoning for Large Language Models (LLMs) by leveraging increased inference-time computation. However, they are predominantly trained on mathematical data and their generalizability to non-mathematical domains has not been rigorously studied. In response, this work first shows that current PRMs have poor performance in other domains. To address this limitation, we introduce \textbf{\emph{VersaPRM}}, a multi-domain PRM trained on synthetic reasoning data generated using our novel data generation and annotation method. VersaPRM achieves consistent performance gains across diverse domains. For instance, in the MMLU-Pro category of Law, VersaPRM via weighted majority voting, achieves a 7.9\% performance gain over the majority voting baseline---surpassing Qwen2.5-Math-PRM's gain of 1.3\%. We further contribute to the community by open-sourcing all data, code and models for VersaPRM.
\end{abstract}



\setlength{\belowdisplayskip}{1pt}
\setlength{\belowdisplayshortskip}{1pt}
\setlength{\abovedisplayskip}{1pt}
\setlength{\abovedisplayshortskip}{1pt}


\section{Introduction}
\input{source/introduction}

\section{Related Work}
\input{source/related_works}


\section{Process Reward Models}
\input{source/preliminaries}

\section{Limitations of Process Reward Models Trained on Math Domain Data}
\label{sec:math-lim}

\input{source/mathprm_limited_gen}

\section{Automatic Generation of Multi-Domain Reasoning Data with Labels}
\label{sec:synth-data-gen}

\input{source/synth_data_gen}

\section{Multi-Domain Process Reward Model}
\label{sec:multi-eval}

\input{source/multidomain_prm}

\section{Discussion and Future Directions}
\input{source/discussion}


\section*{Acknowledgements}

Kangwook Lee is supported by NSF CAREER Award CCF-2339978, an Amazon Research Award, and a grant from FuriosaAI. In addition, Thomas Zeng acknowledges support from NSF under NSF Award DMS-202323 and Daewon Chae was supported by the Hyundai Motor Chung Mong-Koo Foundation.


\section*{Impact Statement}

Given the potential for LLMs to be used in unethical ways, such as spreading misinformation or manipulating public opinion, our Multi-Domain PRM could inadvertently contribute to such misuse. To mitigate these risks, it is essential to implement robust safeguards in training and inference.


\bibliography{refs}
\bibliographystyle{icml2025}


\newpage
\appendix
\onecolumn

\section{More Details on Synthetic Data Generation Pipeline}
\input{source/appendix/synth_data_gen_appendix}

\section{Additional Search Algorithm Details}
\label{sec:search-algs}
\input{source/appendix/search_algs}


\section{Additional PRM Training Details}
\label{sec:add-prm-train}
\input{source/appendix/prm_train_details}

\section{Additional PRM Training and Evaluation Experiments}
\input{source/appendix/prm_train_exps}


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
