% \begin{figure}
%     \centering
%     \resizebox{\linewidth}{!}{%
%     \begin{tikzpicture}
%         \node[inner sep=0pt] () at (0, 0) {\includegraphics[width=0.98\linewidth]{Images/bounds.pdf}};

%         \draw[darkgray] (-2.21, -1.1) -- (-2.21, -1.15);
%         \draw[darkgray] (-1.3, -1.1) -- (-1.3, -1.15);
%         \draw[darkgray] (-0.39, -1.1) -- (-0.39, -1.15);
        
%         \fill [white] (0.62,-1.38) rectangle (1.2,-1.18);
%         \fill [white] (2,-1.38) rectangle (2.6,-1.18);
%         \fill [white] (3.4,-1.38) rectangle (3.95,-1.18);

%         \node () at (0.9, -1.32) {\tiny $0$};
%         \node () at (2.3, -1.32) {\tiny $\frac{\pi}{2}$};
%         \node () at (3.73, -1.3) {\tiny $\pi$};

%         \draw[darkgray] (0.9, -1.1) -- (0.9, -1.15);
%         \draw[darkgray] (2.3, -1.1) -- (2.3, -1.15);
%         \draw[darkgray] (3.73, -1.1) -- (3.73, -1.15);
%     \end{tikzpicture}%
%     }
%     \caption{}
%     \label{fig:convergence_sim}
% \end{figure}

\section{Simulations}
\label{sec:simulations}

We now present a suite of simulations which allow us to characterize how the parameters in Section \ref{sec:theory} influence SSL training under idealized conditions. Full implementation and experiment details can be found in Appendix \ref{app:simulations}.

\subsection{Effect of the Embedding Norms on SSL Training}
\label{ssec:convergence_simulations}


We start by evaluating to what extent the embedding norms and angles between positive samples slow down convergence. Specifically, we sampled 500 pairs of points directly on $\mathbb{S}^{20}$.
We produce many such sets of samples while varying their mean embedding norms and $\phi_{ij}$ values. We then evaluate Theorem \ref{thm:convergence_rate} by
applying the cosine similarity gradients to all positive pairs of embeddings until convergence.

Figure~\ref{fig:convergence_sim} plots the number of steps until convergence and shows that, although the convergence rate depends on both parameters,
having large embedding norms is \emph{significantly} worse for optimization than having large angles between
positive pairs. In essence, the embedding norm's unbounded nature allows it to induce arbitrarily large slowdowns. Meanwhile, the angle between positive samples only has a non-negligible impact as the angle approaches its upper limit $\pi$. Because it is exponentially unlikely for the angle of \emph{every} positive pair to be close to $\pi$, we ignore the angular component of Theorem \ref{thm:convergence_rate} for the remainder of this paper and relegate its further discussion to Appendix \ref{app:opposite_halves_effect}.

\subsection{Effect of SSL Training on the Embedding Norms}
\label{ssec:confidence_simulations}

We now consider how SSL training affects the embedding norms via a simplified training setting where the data is generated from latent classes. Inspired by \citet{latent_inversion, latent_inversion_2}, consider an SSL dataset as a set of latent class distributions $\{\tilde{\mathcal{Z}}_1, ..., \tilde{\mathcal{Z}}_k\}$, where each $\tilde{\mathcal{Z}}_i$ is a probability distribution on the $d$-dimensional hypersphere $\mathbb{S}^d$. Let the observations $x \in \mathcal{X} \subset \mathbb{R}^D$ be obtained via a generating process $g: \mathbb{S}^{d} \rightarrow \mathbb{R}^D$. That is, our dataset is obtained by randomly choosing a probability distribution $\tilde{\mathcal{Z}}_i$, drawing a sample $\tilde{z}$ from it, and applying $g$ to $\tilde{z}$.
%Under appropriate choices for the distributions and training scheme, \cite{latent_inversion, latent_inversion_2} showed that optimizing a 
Following \citet{latent_inversion, latent_inversion_2}, we are training a neural network $f: \mathcal{X} \to \mathbb{S}^{d}$ via contrastive learning to produce a learned latent embedding $ f(\mathcal{X})$. 

We analyze the relationship of parameterized SSL training to the embedding norms by simulating the above scenario. Specifically, we choose centers %$\{c_1, \ldots c_4\}$ 
for 4 latent classes uniformly at random from $\mathbb{S}^{10}$. We then sample 4K points around these centers and normalize them to the hypersphere.
From this, we produce the dataset via generating process $g: \mathbb{S}^{10} \subset \mathbb{R}^{11} \rightarrow \mathbb{R}^{64}$, where $g$ is given by multiplication by a random matrix. We finally train a 2-layer feedforward network with the supervised InfoNCE loss function\footnote{The supervised InfoNCE loss explicitly chooses positive pairs as those which belong to the same class.} on this dataset.

Figure \ref{fig:density} plots each embedding's magnitude in the learned space as a function of (inverse) density in embedding space. We use the distance to an embedding's $10^\text{th}$ nearest neighbor under the cosine similarity metric as a proxy for inverse density. We see that embeddings in dense regions of the embedding space tend to have higher norm. For instance, there are \emph{no} embeddings which are both in a dense latent region (distance \mytexttilde$0.1$ to the $10^\text{th}$ neighbor) and have small norm (\mytexttilde$6$). This follows from Corollary \ref{cor:embeddings_grow}: dense regions of the embedding space receive the most gradient updates and, consequently, those embeddings will grow the most. We also modify the simulation for Figure \ref{fig:class_imbalance} by providing a class imbalance parameter to the class distribution. Namely, class $1$ now has sample probability \mytexttilde$1/2$, class $2$ has sample probability \mytexttilde$1/4$, and so on. We then see that over the course of training, the mean embedding norms for frequent classes grow to higher values than those for sparse classes.

\paragraph{Takeaways.} Across these experiments, samples which are seen more often have higher embedding norm under the InfoNCE loss. This can occur either due to the network considering these samples to be prototypical (and therefore embedding them in dense regions of the learned space) or being otherwise over-represented in the dataset. We point out that these are precisely the settings in which we expect a network to be confident in the embedding.  We leave a formal quantitative analysis
%it 
as an open question:
%to provide formal bounds which relate an embedding's norm and the model's certainty regarding the corresponding input:

\begin{question}
    What theoretical bounds can be made regarding a sample's embedding norm and (a) the accuracy with which it is classified or (b) the corresponding input's dissimilarity from the training data?
\end{question}
