\input{Figures/MeanEmbeddingMagnitudes}
\input{Figures/MagnitudeVsConfidence}
\section{Embedding Norm as Network Confidence}
\label{sec:norm_confidence}

Given the simulations in Section \ref{ssec:confidence_simulations}, we expect that training cos.sim.-based SSL models should result in common input samples receiving higher norm. We therefore use this section to show the various ways in which embedding norms encode a network's confidence in practice. All model implementation details can be found in Appendix \ref{app:experiment_setup}.

\paragraph{Embedding Norms Encode Novelty.} 
Figure \ref{fig:in_out_violin} demonstrates how embedding norms characterize a sample's ``out-of-distributionness''. On the left side of the figure, we trained SimCLR and SimSiam models on the Cifar-10 train set for 512 epochs and evaluated them across different data splits, normalizing all embedding norms by the Cifar-10 train set mean. The results reveal a clear pattern: embedding norms decrease progressively with increasing distributional distance from the training data. For example, the Cifar-10 test set contains novel but distributionally similar samples and therefore results in only slightly reduced norms. In contrast, the Cifar-100 data splits exhibit substantially smaller norms due to their greater distributional shift. This relationship holds symmetrically when training on Cifar-100 and evaluating on Cifar-10, as seen on the right side of Figure \ref{fig:in_out_violin}.

\paragraph{Embedding Norms Encode Classification Accuracy.} Another measure of a network's confidence in an embedding is the accuracy with which that sample is classified. To this end, we use the same experimental setup as above and train SimCLR and SimSiam on the Cifar-10, Cifar-100, and ImageNet100 datasets.\footnote{We default to the ImageNet-100 split \citep{understanding_contr_learn} at \texttt{huggingface.co/datasets/clane9/imagenet-100}.} We then normalize the embedding magnitudes by the maximum across the dataset and bucket the embeddings into ranges of $0.05$, giving us 20 embedding buckets over the dataset. Figure \ref{fig:norm_as_confidence} (left) then shows the per-bucket accuracy of a $k$-nn classifier which was fit on all the embeddings with respect to the cosine similarity metric. Indeed, we see that the $k$-nn classifier's accuracy shows a clear monotonic trend with the embedding norms across datasets and SSL models. 

\paragraph{Embedding Norms Encode Human Confidence.} Interestingly, not only does the embedding norm provide a measure for the sample's novelty and its classification accuracy, but it also provides a signal for human labelers' confidence and their agreement among one another. Using the Cifar-10-N and Cifar-100-N labels from \citet{cifarN}, where each training sample is labeled by the consensus label over multiple human annotators, Figure \ref{fig:norm_as_confidence} (middle) shows higher embedding norms correspond to more accurate consensus labels. Similarly, the Cifar-10-H dataset from \citet{cifarH} provides $\approx 50$ human predictions for each image from the Cifar-10 test set, allowing us to evaluate the labelers' entropy. Figure \ref{fig:norm_as_confidence} (right) shows that, as the embedding norms grow, the human labels have less entropy and are therefore more likely to agree with one another.

\paragraph{Takeaways.} Under the common assumption that an SSL embedding's direction represents its information, these experiments show that the embedding's norm represents how \emph{confident} the network was in this information. Furthermore, this measure of network confidence is inherent to all cos.sim.-based loss functions and emerges naturally during training. Thus, an SSL latent space looks less like a smooth sphere and more like a spiky ball, with the spikes corresponding to regions of known data samples. This observation has implications for few-shot learning settings, in which one has pre-trained on a large dataset and then wishes to adjust the model to a second, smaller dataset:

\begin{question}
    By using the embedding norm as a measure for a sample's novelty, can one more precisely guide the training process on unseen inputs?
\end{question} 