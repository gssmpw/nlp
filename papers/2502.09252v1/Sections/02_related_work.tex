
\section{Preliminaries and Related Work}

% Terminology needs to be explained here

Although this paper concerns general SSL techniques, we will use the familiar language of image representation learning. The typical SSL pipeline consists of obtaining two augmented variants $x_i$ and $x_j$ from an input image $x$ and ensuring that the corresponding embeddings $z_i$ and $z_j$ have high cosine similarity. We refer to $(x_i, x_j)$ (resp. $(z_i, z_j)$) as `positive' pairs of points (resp. embeddings). 
Since attracting all the points together will collapse the representation, each SSL method has an additional mechanism to prevent this. The most common approach enforces low similarity between embeddings of dissimilar inputs: for unrelated inputs $x_i$ and $x_k$, the embeddings $z_i$ and $z_k$ should be far apart.
% Old Figures
% \begin{figure*}
%     \centering
%     \begin{subfigure}{0.27\linewidth}
%     \includegraphics[trim={5cm 4.2cm 12cm 2.7cm},clip,width=\linewidth]{Images/contr_attractions.pdf}
%     \caption{The gradients w.r.t. $z_i$ in Proposition~\ref{prop:cos_sim_grads} and Corollary~\ref{cor:embeddings_grow} exclusively exist in the tangent space at $\vec{z}_i$.}
%     \label{fig:grad_visualization}
%     \end{subfigure}
%     \quad \quad \quad
%     \begin{subfigure}{0.6\linewidth}
%     \includegraphics[trim={1cm 4.5cm 8.5cm 4.5cm},clip,width=\linewidth]{Images/growing_embeddings.pdf}
%     \caption{\textcolor{red}{Just this panel in the one-column environment?} A visualization of the growing embeddings in Corollary~\ref{cor:embeddings_grow}. Blue points represent $z_i$ at iterations $t = 1, 2, 3$. Yellow points represent $z_i'$, i.e. the result of each step of gradient descent.}
%     \label{fig:growing_embeddings}
%     \end{subfigure}
%     \caption{\textcolor{red}{Standardize color scheme and figure design.} Visualization of the embedding norm's role in self-supervised learning.}
%     \label{fig:embedding_visualizations}
%     \vspace*{-0.25cm}
% \end{figure*}

\subsection{Preliminaries \& SSL methods}

Perhaps the prototypical method in this family is SimCLR~\cite{simclr}, which appears among a broader line of \emph{contrastive} methods for self-supervised learning on images. These all utilize repulsions from dissimilar (negative) samples to prevent representation collapse \citep{contr_pred_coding, data_efficient_cpc, moco, mocov2, pirl}. In practice, this is performed by optimizing the InfoNCE loss, which we write with respect to embedding $z_i$ as 
\begin{align}
    \label{eq:infonce}
    \mathcal{L}_{ij}(\mathbf{Z}) &= -\log \frac{\text{ExpSim}(z_i, z_j)}{\sum_{k \not\sim i} \text{ExpSim}(z_i, z_k)} \\
    &= -\underbrace{\hat{z}_i^\top \hat{z}_j}_{\mathcal{L}^\mathcal{A}_{ij}(\mathbf{Z})} + \underbrace{\log \left( S_i \right)}_{\mathcal{L}^\mathcal{R}_{ij}(\mathbf{Z})} \nonumber,
\end{align}
where $\hat{z}$ is the unit vector of $z$, $\text{ExpSim}(a, b) = \exp \big( \hat{a}^\top \hat{b} \big)$ is the exponent of the cosine similarity, and $S_i = \sum_{k \not\sim i} \text{ExpSim}(z_i, z_k)$ represents the sum over all $k \neq i$ in the batch. For ease of interpretation, we split the loss term into attractive and repulsive components, resp. $\mathcal{L}^\mathcal{A}_{ij}$ and $\mathcal{L}^\mathcal{R}_{ij}(\mathbf{Z})$. Note that $\mathcal{L}^\mathcal{A}_{ij}(\mathbf{Z})$ is simply the negative cosine similarity between positive pair $z_i$ and $z_j$.

Another common contrastive objective function is the triplet loss, wherein one normalizes the embeddings to the hypersphere and minimizes the mean squared error between positive samples while maximizing the mean squared error between negative ones \citep{triplet_loss}. Due to the normalization, this implicitly optimizes the cosine similarity between embeddings \citep{byol}.

Curiously, methods like \emph{BYOL}~\cite{byol} and \emph{SimSiam}~\cite{simsiam} showed that one can simply optimize $\mathcal{L}^\mathcal{A}_{ij}$, and avoid collapse by applying only the gradients to embedding $z_i$ (rather than to both $z_i$ and $z_j$). We refer to these as \emph{non-contrastive} methods. Throughout this paper, we will use SimCLR with the InfoNCE loss to represent a prototypical contrastive SSL model and SimSiam with the negative cosine similarity to represent a prototypical non-contrastive SSL model. As is standard \cite{dinov2}, we use the $k$-nn classifier accuracy on the embedding space to evaluate the quality of a learned representation.


% At its core, multi-modal learning often relies on contrastive objectives like InfoNCE to align representations across different modalities. CLIP \textcolor{red}{\cite{}}, for instance, uses InfoNCE with cosine similarity to maximize agreement between matched image-text pairs while pushing apart unmatched pairs. The cosine similarity metric is particularly useful because it focuses on directional alignment of normalized embeddings rather than absolute distances, making training more stable across different scales and modalities.  The effectiveness of this simple contrastive framework has made it a cornerstone of modern multi-modal learning, spawning numerous variations like \textcolor{red}{cite{ALIGN, Florence} (Need to make sure these use cosine sim as base)}. 

\subsection{Related Work}

\paragraph{Analysis of SSL Embeddings.} Due to its reliance on the cosine similarity, the seminal work of~\citet{understanding_contr_learn} showed that contrastive learning must satisfy two requirements: all positive pairs must be near one another (alignment) and all negative samples must spread evenly over the hypersphere (uniformity). Expanding on this blueprint, subsequent works have sought to formalize the learning capacity of contrastive methods \citep{arora_contr_theory, understanding_contr_learn, latent_inversion, provable_contr_guarantees, understanding_contr_learn_2} while much of the research into non-contrastive methods has focused on how their architectures help to prevent collapse \citep{dim_collapse_ssl, direct_pred, simsiam_avoid_collapse, BYOL_orthogonality}. %It has also been suggested that the cosine similarity is an imperfect similarity metric \citep{cos_similarity_inspection}.

\paragraph{SSL on Imbalanced Data.}

It is an active area of research to understand how contrastive learning performs over various data distributions \cite{latent_inversion}. Although \citet{robust_imbalance} showed that SSL training is relatively robust to imbalanced classes, there are mechanisms which can improve its performance \cite{divide_contrast, dassot}. Nonetheless, foundation models which use contrastive learning require balancing the data distribution before training \citep{demystifying_clip}.

\paragraph{Relationship of Embedding Norms and SSL.}
While SSL representation learning has largely focused on hyperspherical embedding distributions, some work has examined the embedding norm's role. \citet{normface, mentions_catch22, spherical_embeddings} all noted that embedding norms inversely scale gradients under the cosine similarity loss and suggested that the embeddings must grow. However, each paper largely brushed this interaction away: \citet{normface} suggested that \ltwo-normalization suffices to handle the embedding norms, \citet{mentions_catch22} states that attempts to resolve this interaction were unsuccessful and \citet{spherical_embeddings} provides a regularization term which shrinks the \emph{variance} of the embedding norms but leaves their average magnitude unmanaged. This paper therefore differs significantly from the prior literature by (i) extending these results to the InfoNCE loss, (ii) evaluating the full extent to which large embedding norms affect real-world training, and (iii) providing principled mechanisms for addressing this effect.

Similarly, it has been suggested in \citet{embed_norm_confidence_1}, \citet{embed_norm_confidence_2} and \citet{embed_norm_confidence_3} that an SSL embedding's magnitude could serve as a measure for the model's certainty. This was evidenced in two ways: first, by qualitatively showing that samples with high-embedding norm are good representatives of the classes (see Figure \ref{fig:cifar_norms}) and, second, by finding that the embedding norm is smaller for out-of-distribution (OOD) samples. Importantly, no explanation was given for this phenomenon and, to our knowledge, it has not been systematically evaluated. Thus, our work expands the literature by comprehensively establishing that embedding norms encode SSL model confidence and providing a thorough analysis for the phenomenon.