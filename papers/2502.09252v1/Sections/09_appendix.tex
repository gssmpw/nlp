\renewcommand{\thefigure}{S\arabic{figure}}
\setcounter{figure}{0}  
\renewcommand{\thetable}{S\arabic{table}}
\setcounter{table}{0} 


\section{Proofs}
\subsection{Proof of Proposition~\ref{prop:cos_sim_grads}}
\label{prf:prop_grad_grows}
\cosgrads*
\begin{proof}
    We are taking the gradient of $\mathcal{L}^\mathcal{A}_i$ as a function of $z_i$. The principal idea is that the gradient has a term with direction $\hat{z}_j$ and a term with direction $-\hat{z}_i$. We then disassemble the vector with direction $\hat{z}_j$ into its component parallel to $z_i$ and its component orthogonal to $z_i$. In doing so, we find that the two terms with direction $z_i$ cancel, leaving only the one with direction orthogonal to $z_i$.
    
    Writing it out fully, we have $\mathcal{L}^\mathcal{A}_i = -z_i^\top z_j / (\|z_i\| \cdot \|z_j\|)$. Taking the gradient amounts to using the quotient rule, with $f = -z_i^\top z_j$ and $g = \|z_i\| \cdot \|z_j\| = \sqrt{z_i^\top z_i} \cdot \sqrt{z_j^\top z_j}$. Taking the derivative of each, we have
    \begin{align*}
        f' &= -\mathbf{z}_j \\
        g' &= \|z_j\| \frac{z_i}{\sqrt{z_i^\top z_i}} = \|z_j\| \frac{\mathbf{z}_i}{\|z_i\|} \\
        \implies \frac{f' g - g' f}{g^2} &= \frac{- \left(\mathbf{z}_j \cdot \|z_i\| \cdot \|z_j\| \right) + \left(\|z_j\| \frac{\mathbf{z}_i}{\|z_i\|} \cdot z_i^\top z_j \right)}{\|z_i\|^2 \cdot \|z_j\|^2} \\
        &= \frac{-\mathbf{z}_j}{\|z_i\| \cdot \|z_j\|} + \frac{\mathbf{z}_i z_i^\top z_j}{\|z_i\|^3 \|z_j\|},
    \end{align*}
    where we use boldface $\mathbf{z}$ to emphasize which direction each term acts along. We now substitute $\cos(\phi_{ij}) = z_i^\top z_j / (\|z_i\| \cdot \|z_j\|)$ in the second term to get
    \begin{equation}
        \label{eq:quotient_rule}
        \frac{f' g - g' f}{g^2} = \frac{-\hat{z}_j}{\|z_i\|} + \frac{\mathbf{z}_i \cos(\phi)}{\|z_i\|^2}
    \end{equation}

    It remains to separate the first term into its sine and cosine components and perform the resulting cancellations. To do this, we take the projection of $\hat{z}_j = \mathbf{z}_j / \|z_j\|$ onto $\mathbf{z}_i$ and onto the plane orthogonal to $\mathbf{z}_i$. The projection of $\hat{z}_j$ onto $\mathbf{z}_i$ is given by
    \[ \cos \phi_{ij} \frac{\mathbf{z}_i}{\|z_i\|} \]
    while the projection of $\mathbf{z}_j / \|z_j\|$ onto the plane orthogonal to $\mathbf{z}_i$ is
    \[ \left( \mathbf{I} - \frac{z_i z_i^\top}{\|z_i\|^2} \right) \frac{\mathbf{z}_j}{\|z_j\|}. \]
    It is easy to assert that these components sum to $\mathbf{z}_j/\|z_j\|$ by replacing the $\cos \phi_{ij}$ by $\frac{z_i^\top z_j}{\|z_i\|\cdot \|z_j\|}$.

    We plug these into Eq.~\ref{eq:quotient_rule} and cancel the first and third term to arrive at the desired value:
    \begin{align*}
        \frac{f' g - g' f}{g^2} = &-\frac{1}{\|z_i\|} \cos \phi \frac{\mathbf{z}_i}{\|z_i\|} \\
        &- \frac{1}{\|z_i\|} \cdot \left( \mathbf{I} - \frac{z_i z_i^\top}{\|z_i\|^2} \right) \frac{\mathbf{z}_j}{\|z_j\|} \\
        &+ \frac{\mathbf{z}_i \cos(\phi)}{\|z_i\|^2} \\
        = &\frac{-1}{\|z_i\|} \cdot \left( \mathbf{I} - \frac{z_i z_i^\top}{\|z_i\|^2} \right) \frac{\mathbf{z}_j}{\|z_j\|}.
    \end{align*}
\end{proof}

We visualize the loss landscape of the cosine similarity function in Figure \ref{fig:cos_sim_surface}. 

\begin{figure}
    \centering
    \begin{subfigure}{0.45\linewidth}
        \centering 
        \includegraphics[width=1\linewidth]{Images/cosine_similarity_surface_with_circles.pdf}
    \end{subfigure}%
    \begin{subfigure}{0.45\linewidth}
        \centering 
        \includegraphics[width=0.8\linewidth]{Images/cosine_similarity_2D_heatmap.pdf}
    \end{subfigure}
    \caption{Cosine similarity with respect to the direction indicated by the blue line. Three circles of radii 0.5, 1, and 2 are superimposed to show that, for higher norms, the cosine similarity is less steep. Left: 3D Surface plot, right: 2D topview plot.}
    \label{fig:cos_sim_surface}
\end{figure}


\subsection{InfoNCE Gradients}
\label{app:infonce_grads}
\infoncegrads*
\begin{proof}
    We are interested in the gradient of $\mathcal{L}_i^\mathcal{R}$ with respect to $z_i$. By the chain rule, we get
    \begin{align*}
        \nabla_i^\mathcal{R} &= -\frac{\sum_{k \not\sim i} \text{ExpSim}(z_i, z_k) \frac{\partial \frac{z_i^\top z_k}{\|z_i\| \cdot \|z_k\|}}{\partial z_i}}{\sum_{k \not\sim i} \text{ExpSim}(z_i, z_k)} \\
        &= -\frac{\sum_{k \not\sim i} \text{ExpSim}(z_i, z_k) \frac{\partial \frac{z_i^\top z_k}{\|z_i\| \cdot \|z_k\|}}{\partial z_i}}{S_i}
    \end{align*}
    It remains to substitute the result of Prop. \ref{prop:cos_sim_grads} for $\partial \frac{z_i^\top z_k}{\|z_i\| \cdot \|z_k\|} / \partial z_i$.

    We sum this this with the gradients of the attractive term to obtain the full InfoNCE gradient, completing the proof.
\end{proof}

We note that the repulsive force is weighted average over a set of unit vectors. Consequently, the repulsive gradient is smaller than the attractive one. Additionally, we point out that these gradients are symmetric: just like positive and negative samples $z_j$ and $z_k$ affect $z_i$, $z_i$ affects $z_j$ and $z_k$.

\subsection{Proof of Corollary~\ref{cor:embeddings_grow}}
\label{prf:cor_embeddings_grow}
\begin{proof}
    First, consider that we applied the cosine similarity's gradients from Proposition~\ref{prop:cos_sim_grads}. Since $z_i$ and $(z_j)_{\perp z_i}$ are orthogonal, $\|z_i'\|_2^2 = \|z_i\|^2 + \frac{\gamma^2}{\|z_i\|^2}\|(z_j)_{\perp z_i}\|^2$. The second term is positive if $\sin \phi_{ij} > 0$.

    The same exact argument holds for the InfoNCE gradients. The gradient is orthogonal to the embedding, so a step of gradient descent can only increase the embedding's magnitude.
\end{proof}

\subsection{Proof of Theorem~\ref{thm:convergence_rate}}
\label{prf:thm_convergence_rate}
We first restate the theorem:

Let $z_i$ and $z_j$ be positive embeddings with equal norm, i.e. $\|z_i\| = \|z_j\| = \rho$. Let $z_i'$ and $z_j'$ be the embeddings after 1 step of gradient descent with learning rate $\gamma$. Then the change in cosine similarity is bounded from above by:
\begin{equation*}
    \hat{z}_i'^\top \hat{z}_j' - \hat{z}_i^\top \hat{z}_j < \frac{\gamma \sin^2 \phi_{ij}}{\rho^2} \left[ 2 - \frac{\gamma \cos \phi}{\rho^2} \right].
\end{equation*}

\noindent We now proceed to the proof:
\begin{proof}
    Let $z_i$ and $z_j$ be two embeddings with equal norm\footnote{We assume the Euclidean distance for all calculations.}, i.e. $\|z_i\| = \|z_j\| = \rho$. We then perform a step of gradient descent to maximize $\hat{z}_i^\top \hat{z}_j$. That is, using the gradients in \ref{prop:cos_sim_grads} and learning rate $\gamma$, we obtain new embeddings $z_i' = z_i + \frac{\gamma}{\|z_i\|} (\hat{z}_j)_{\perp z_i}$ and $z_j' = z_j + \frac{\gamma}{\|z_j\|} (\hat{z}_i)_{\perp z_j}$. Going forward, we write $\delta_{ij} = (\hat{z}_j)_{\perp z_i}$ and $\delta_{ji} = (\hat{z}_i)_{\perp z_j}$, so $z_i' = z_i + \frac{\gamma}{\rho} \delta_{ij}$ and $z_j' = z_j + \frac{\gamma}{\rho} \delta_{ji}$. Notice that since $z_i$ and $\delta_{ij}$ are orthogonal, by the Pythagorean theorem we have $\|z_i'\|^2 = \|z_i\|^2 + \frac{\gamma^2}{\rho^2}\|\delta_{ij}\|^2 \geq \|z_i\|^2$. Lastly, we define $\rho' = \|z_i'\| = \|z_j'\|$.

    We are interested in analyzing $\hat{z}_i'^\top \hat{z}_j' - \hat{z}_i^\top \hat{z}_j$. To this end, we begin by re-framing $\hat{z}_i'^\top \hat{z}_j'$:
    \begin{align*}
        \hat{z}_i'^\top \hat{z}_j' &= \left(\frac{z_i + \frac{\gamma}{\rho} \delta_{ij}}{\rho'}\right)^\top \left(\frac{z_j + \frac{\gamma}{\rho} \delta_{ji}}{\rho'}\right) \\
        &= \frac{1}{\rho'^2}\left[ z_i^\top z_j + \gamma \frac{z_i^\top \delta_{ji}}{\rho'} + \gamma \frac{z_j^\top \delta_{ij}}{\rho'} + \gamma^2 \frac{\delta_{ij}^\top \delta_{ji}}{\rho'^2} \right].
    \end{align*}

    We now consider that, since $\delta_{ij}$ is the projection of $\hat{z}_j$ onto the subspace orthogonal to $z_i$, we have that the angle between $z_i$ and $\delta_{ji}$ is $\pi/2 - \phi_{ij}$. Plugging this in and simplifying, we obtain
    \begin{align*}
        z_i^\top \delta_{ji} &= \|z_i\| \cdot \|\delta_{ji}\| \cos (\pi/2 - \phi_{ij}) \\
        &= \|z_i\| \cdot \|\delta_{ji}\| \sin \phi_{ij} \\
        &= \rho \sin^2 \phi_{ij}.
    \end{align*}
    By symmetry, the same must hold for $z_j^\top \delta_{ij}$.
    
    Similarly, we notice that the angle $\psi_{ij}$ between $\delta_{ij}$ and $\delta_{ji}$ is $\psi_{ij} = \pi - \phi_{ij}$. The reason for this is that we must have a quadrilateral whose four internal angles must sum to $2\pi$, i.e. $\psi_{ij} + \phi_{ij} + 2 \frac{\pi}{2} = 2 \pi$. Thus, we obtain $\delta_{ij}^\top \delta_{ji} = \|\delta_{ij}\| \cdot \|\delta_{ji}\| \cos(\psi) = -\sin^2 \phi_{ij} \cos \phi_{ij}$.

    We plug these back into our equation for $\hat{z}_i'^\top \hat{z}_j'$ and simplify:
    \begin{align*}
        \hat{z}_i'^\top \hat{z}_j' &= \frac{1}{\rho'^2}\left[ z_i^\top z_j + \gamma \frac{z_i^\top \delta_{ji}}{\rho} + \gamma \frac{z_j^\top \delta_{ij}}{\rho} + \gamma^2 \frac{\delta_{ij}^\top \delta_{ji}}{\rho^2} \right] \\
        &= \frac{1}{\rho'^2}\left[ z_i^\top z_j + \gamma \frac{\rho \sin^2 \phi_{ij}}{\rho} + \gamma \frac{\rho \sin^2 \phi_{ij}}{\rho} - \gamma^2 \frac{\sin^2 \phi_{ij} \cos \phi_{ij}}{\rho^2} \right] \\
        &= \frac{1}{\rho'^2}\left[ z_i^\top z_j + 2 \gamma \sin^2 \phi_{ij} - \gamma^2 \frac{\sin^2 \phi_{ij} \cos \phi_{ij}}{\rho^2} \right].
    \end{align*}

    We now consider the original term in question:
    \begin{align*}
        \hat{z}_i'^\top \hat{z}_j' - \hat{z}_i^\top \hat{z}_j &= \frac{1}{\rho'^2}\left[ z_i^\top z_j + 2 \gamma \sin^2 \phi_{ij} - \gamma^2 \frac{\sin^2 \phi_{ij} \cos \phi_{ij}}{\rho^2} \right] - \frac{z_i^\top z_j}{\rho^2} \\
        &\leq \frac{1}{\rho^2}\left[ z_i^\top z_j + 2 \gamma \sin^2 \phi_{ij} - \gamma^2 \frac{\sin^2 \phi_{ij} \cos \phi_{ij}}{\rho^2} \right] - \frac{z_i^\top z_j}{\rho^2} \\
        &= \frac{1}{\rho^2}\left[ 2 \gamma \sin^2 \phi_{ij} - \gamma^2 \frac{\sin^2 \phi_{ij} \cos \phi_{ij}}{\rho^2} \right] \\
        &= \frac{\gamma \sin^2 \phi_{ij}}{\rho^2}\left[ 2 - \frac{\gamma \cos \phi_{ij}}{\rho^2} \right]\\
        &\leq \frac{2 \gamma \sin^2 \phi_{ij}}{\rho^2}
    \end{align*}
    
    This concludes the proof.
\end{proof}

\section{Simulations}
\label{app:simulations}

\subsection{Aparametric Simulations}

For the simulations in Section \ref{ssec:convergence_simulations}, we produced two datasets, $\mathbf{X}_1$ and $\mathbf{X}_2$, independently by randomly sampling points in $\mathbb{R}^20$ from a standard normal distribution and normalizing them to the hypersphere. The $i$-th point in dataset $\mathbf{X}_1$ is the positive counterpart for the $i$-th point in dataset $\mathbf{X}_2$. The first dataset is then set to be static while the second is modified in order to control for the embedding norms and angles between positive pairs.

We optimize the cosine similarity by performing standard gradient descent on the embeddings themselves with learning rate $10$. We consider a dataset ``converged'' when the average cosine similarity between positive pairs exceeds $0.999$.

\paragraph{Controlling for angles.} In order to control for the angle between positive pairs, we use an interpolation value $\alpha \in [-1, 1]$. Let $x_1$ be a static embedding in $\mathbf{X}_1$ and $x_2$ be the embedding in $\mathbf{X}_2$ whose angle we wish to control. In expectation, $\phi(x_1, x_2)$ will be $\pi / 2$. We therefore define the embedding $x_2$ whose angle has been controlled as 
\[ x_2' = x_2 \cdot (1 - |\alpha|) + x_1 \cdot \alpha. \]

In essence, when $\alpha=0$, $x_2' = x_2$. However, when $\alpha=1$ (resp. $\alpha=-1$), $x_2' = x_1$ (resp. $x_2' = -x_1$).

\paragraph{Controlling for embedding norms.} This setting is simpler than the angles between positive pairs. We simply scale $\mathbf{X}_2$ by the desired value.

\subsection{Parametric Simulations}
\label{app:parametric_sim}

We restate the entire implementation for the simulations in Section \ref{ssec:confidence_simulations} for completeness. We choose centers for 4 latent classes $\{c_1, c_2, c_3, c_4\}$ uniformly at random from $\mathbb{S}^{10}$ by randomly sampling vectors from a standard multivariate normal distribution and normalizing them to the hypersphere. We then obtain the latent samples $\tilde{z}$ around center $c_i$ via $z \sim \mathcal{N}(c_i, 0.1 \cdot \mathbf{I})$ and re-normalizing to the hypersphere. For each center, we produce 1K latent samples; these constitute our latent classes. We depict an example of 8 such latent classes (in 3 dimensions) in Figure \ref{fig:orig_latents}. We finally obtain the dataset by generating a random matrix in $\mathbb{R}^{11 \times 64}$ and applying it to the latent samples.

We train the InfoNCE loss via a 2-layer feedforward neural network with the ReLU activation function in the hidden layer. The network's output dimensionality is $\mathbb{R}^{11}$ so that, after normalization, it can reconstruct the original latent classes. We train the network using the supervised InfoNCE loss with a batch size of 128. Each data point's positive pair is simply another data point from the same latent class.

We visualize the learned (unnormalized) embedding space in Figure \ref{fig:learned_latents}.

\begin{figure}
    \centering
    \begin{subfigure}{0.4\linewidth}
    \includegraphics[width=\linewidth]{Images/orig_latents.png}
    \caption{}
    \label{fig:orig_latents}
    \end{subfigure}
    \quad\quad
    \begin{subfigure}{0.4\linewidth}
    \includegraphics[width=\linewidth]{Images/learned_latents.png}
    \caption{}
    \label{fig:learned_latents}
    \end{subfigure}
    \caption{\emph{Left}: A depiction of $8$ latent classes in $3$D obtained via the description in Section \ref{app:parametric_sim}. Dashed lines represent vectors from the origin to the mean of the distribution. \emph{Right}: A depiction of the learned latent space (unnormalized) using the supervised InfoNCE loss after 50 epochs of training.}
    
\end{figure}


\section{Further Discussion and Experiments}
\label{app:experiments}

\subsection{Experimental Setup}
\label{app:experiment_setup}
Unless otherwise stated, we use a ResNet-50 backbone \cite{resnet} and the default settings outlined in the SimCLR \cite{simclr} and SimSiam \cite{simsiam} papers. We use $1$e-$6$ as the default SimCLR weight decay and $5$e-$4$ as the default SimSiam one. When running on Cifar-10 and Cifar-100, we amend the backbone network's first layer as detailed in \citet{simclr}. We use embedding dimensionality $256$ in SimCLR and $2048$ in SimSiam. When reporting embedding norms, we use the projector's output in SimCLR and the predictor's output in SimSiam: these are the spaces where the loss function is applied and therefore where our theory holds.

Due to computational constraints, we run with batch-size 256 in SimCLR. Although each batch is still 256 samples in SimSiam, we simulate larger batch sizes using gradient accumulation. Thus, our default batch-size for SimSiam is 1024. 

\subsection{Opposite-Halves Effects}
\label{app:opposite_halves_effect}

We devote this section of the Appendix to studying the role of the angle between positive samples on the cosine similarity's convergence under gradient descent. Referring back to Figure~\ref{fig:convergence_sim}, we see that the effect is most impactful when the angle between positive embeddings is close to $\pi$, i.e. $\phi_{ij} > \pi - \varepsilon$ for $\varepsilon \rightarrow 0$. The following result shows that this is exceedingly unlikely for a single pair of embeddings in high-dimensional space:
\begin{proposition}
    \label{prop:unlikely_opp_halves}
    Let $x_i, x_j \sim \mathcal{N}(0, \mathbf{I})$ be $d$-dimensional, i.i.d. random variables and let $0 < \varepsilon < 1$. Then \vspace*{-0.1cm}
    \begin{equation}
    \label{eq:opp_halves_unlikely}
    \mathbb{P}\left[ \hat{x}_i^\top \hat{x}_j \geq 1 - \varepsilon \right] \leq \frac{1}{2d(1-\varepsilon)^2}.
    \end{equation}\vspace*{-0.3cm}
\end{proposition}
\begin{proof}
By \citet{distribution_of_cosine_sim}, the cosine similarity between two i.i.d. random variables drawn from $\mathcal{N}(0, \mathbf{I})$ has expected value $\mu = 0$ and variance $\sigma^2 = 1/d$, where $d$ is the dimensionality of the space. We therefore plug these into Chebyshev's inequality:
\begin{align*}
    &\text{Pr} \left[ \left|\frac{x_i^\top x_j}{\|x_i\|\cdot \|x_j\|} - \mu \right|\geq k \sigma \right] \leq \frac{1}{k^2} \\
    \rightarrow & \text{Pr} \left[ \left |\frac{x_i^\top x_j}{\|x_i\|\cdot \|x_j\|} \right |\geq \frac{k}{\sqrt{d}} \right] \leq \frac{1}{k^2}
\end{align*}

\noindent We now choose $k = \sqrt{d}(1 - \varepsilon)$, giving us
\[ \mathbb{P}\left[ \left |\frac{x_i^\top x_j}{\|x_i\| \cdot \|x_j\|}\right | \geq 1 - \varepsilon \right] \leq \frac{1}{d(1-\varepsilon)^2}.\]

It remains to remove the absolute values around the cosine similarity. Since the cosine similarity is symmetric around $0$, the likelihood that its absolute value exceeds $1 - \varepsilon$ is twice the likelihood that its value exceeds $1- \varepsilon$, concluding the proof.

We note that this is actually an extremely optimistic bound since we have not taken into account the fact that the maximum of the cosine similarity is 1.
\end{proof}

The above proposition represents the likelihood that \emph{one} pair of embeddings has large angle between them. It is therefore \emph{exponentially} unlikely for every pair of embeddings in a dataset to have angle close to $\pi$, as we would require Proposition \ref{prop:unlikely_opp_halves} to hold across every pair of embeddings. Thus, the opposite-halves effect is exceedingly unlikely to occur.

\begin{table}
    \centering
    \quad
    \parbox{.47\linewidth}{
        \begin{tabular}{lrcc}
        \toprule
        Model & Dataset \quad\quad & \makecell{Effect Rate\\Epoch 1} & \makecell{Effect Rate\\Epoch 16} \\
        \midrule
        \multirow{2}{*}{SimCLR} & Imagenet-100 & 2\% & 0\%  \\
        & Cifar-100 & 11\% & 1\% \\
        \cmidrule{1-4}
        \multirow{2}{*}{SimSiam} & Imagenet-100 & 26\% & 1\% \\
        & Cifar-100 & 21\% & 0\% \\
        \cmidrule{1-4}
        \multirow{2}{*}{BYOL} & Imagenet-100 & 28\% & 1\% \\
        & Cifar-100 & 20\% & 0\% \\
        \bottomrule
        \end{tabular}
        \captionof{table}{The rate at which embeddings are on opposite sides of the latent space (angle between a positive pair is greater than $\pi / 2$) for various datasets and SSL models.}
        \label{tbl:opposite_halves_effect}
    }
    \hfill
    \parbox{.38\linewidth}{
        \begin{tabular}{cc ccc}
        \toprule
        \multirow{2}{*}{Epoch} & & \multicolumn{3}{c}{Batch Size}\\
        & & 256 & 512 & 1024 \\
        \cmidrule{3-5}
        \multirow{2}{*}{100} & Default & 46.1 & 41.2 & 32.6 \\
        & Cut ($c=9$) & 43.1 & 46.5 & 44.3 \\
        \cmidrule{2-5}
        \multirow{2}{*}{500} & Default & 59.1 & 60.4 & 61.3\\
        & Cut ($c=9$) & 59.4 & 58.9 & 61.5 \\
        \bottomrule
        \end{tabular}
        \captionof{table}{$k$-nn accuracies for SimSiam trained with various batch sizes. We performed training for both the default and cut-initialized variants and reported $k$-nn accuracies at 100 and 500 epochs.}
        \label{tbl:cut_batch_size}
    }
\end{table}

In accordance with this, Table~\ref{tbl:opposite_halves_effect} shows that, after one epoch of training, embeddings have angle greater than $\pi/2$ at a rate of around $5\%$ and $25\%$ for SimCLR and SimSiam/BYOL, respectively. So even if the `strongest' variant of the opposite-halves effect is not occurring, a weaker one may still be. However, very early into training (epoch 16), every method has a rate of effectively 0 for the opposite-halves effect. Furthermore, the rates in Table~\ref{tbl:opposite_half_effect} measure how often $\phi_{ij} > \frac{\pi}{2}$. This is the absolute weakest version of the opposite-halves effect. Thus, while some weak variant of the opposite-halves effect may occur at the beginning of training, it does not have a strong impact on the convergence dynamics and, in either case, disappears quite quickly.

\subsection{Weight Decay}
\label{app:weight_decay}

We evaluate the effect of weight decay in the imbalanced setting in \ref{fig:weight_decay_imbalanced}, which is an analog of Figure \ref{fig:weight_decay_ablation} for the imbalanced Cifar-10 dataset detailed in Section \ref{sec:convergence}. We again see that using weight decay controls for the embedding norms and improves the convergence of both models. In correspondence with the other results on imbalanced training, we find that stronger control over the embedding norms leads to improved convergence: the high weight decay value does not perform as poorly on SimCLR as in Figure \ref{fig:weight_decay_ablation} and, on SimSiam, outperforms the other weight decay options.

\begin{figure}
    \centering
    \begin{tikzpicture}
    \node () at (0, 0) {\includegraphics[width=0.4\linewidth]{Images/wd_sweep_imbalanced.png}};

    \draw[ballblue, line width=0.07cm] (-4, -3.8) -- (-3.4, -3.8);
    \draw[azure, line width=0.07cm] (-1.3, -3.8) -- (-0.7, -3.8);
    \draw[darkblue, line width=0.07cm] (2, -3.8) -- (2.6, -3.8);

    \node () at (-1.15, -3.33) {\small \textcolor{darkgray}{Train Epoch}};
    \node () at (1.95, -3.33) {\small \textcolor{darkgray}{Train Epoch}};

    \node[inner sep=0pt] () at (-2.5, -3.82) {\textcolor{darkgray}{\scriptsize No weight decay}};
    \node[inner sep=0pt] () at (0.48, -3.82) {\textcolor{darkgray}{\scriptsize Standard weight decay}};
    \node[inner sep=0pt] () at (3.57, -3.82) {\textcolor{darkgray}{\scriptsize High weight decay}};
        
        
    \end{tikzpicture}
    \caption{An analog to Figure \ref{fig:weight_decay_ablation} performed on the exponentially imbalanced Cifar-10 dataset. Weight decays are [$0$, $1$e-$5$, $5$e-$2$] for SimCLR and [$0$, $5$e-$4$, $5$e-$2$] for SimSiam. We plot the effective learning rate in the bottom row, calculated in accordance with Section \ref{sec:convergence}.}
    \label{fig:weight_decay_imbalanced}
\end{figure}

\subsection{Cut-Initialization}
\label{app:cut_init}
We plot the effect of the cut constant on the embedding norms and accuracies over training in Figure~\ref{fig:cut_experiments}. To make the effect more apparent, we use weight-decay $\lambda=5e-4$ in all models. We see that dividing the network's weights by $c>1$ leads to immediate convergence improvements in all models. Furthermore, this effect degrades gracefully: as $c > 1$ becomes $c < 1$, the embeddings stay large for longer and, as a result, the convergence is slower. We also see that cut-initialization has a more pronounced effect in attraction-only models -- a trend that remains consistent throughout the experiments.

We also show the relationship between cut-initialization and the network's batch size on SimSiam in Table \ref{tbl:cut_batch_size}. Consistent with the literature, we see that training with large batches provides improvements to training accuracy. However, we note that larger batch sizes also significantly slow down convergence. However, cut-initialization seems to counteract this and accelerate convergence accordingly. Thus, training with cut-initialization and large batches seems to be the most effective method for SSL training (at least in the non-contrastive setting).

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.95\textwidth]{Images/init_experiments.png}
    \caption{The effect of cut-initialization on Cifar10 SSL representations. $x$-axis and embedding norm's $y$-axis are log-scale. $\lambda=5$e$-4$ in all experiments.}
    \label{fig:cut_experiments}
\end{figure}

\section{More details on gradient scaling layer}
\label{app:grad_scaling}

An implementation of our GradScale layer can be found in Listing \ref{alg:grad_scaling_use}.
We note that this layer is purely a PyTorch optimization trick and does not amount to implicitly choosing a different loss function:

\begin{restatable}{proposition}{nopotential}
    \label{prop:no_potential}
    Let $t\in\mathbb{R}^n$ be a unit vector, $p: \mathbb{R}^n\backslash \{0\} \to [-1, 1], z\mapsto t^\top z/\|z\|$ the cosine similarity with respect to $t$, $\alpha \in \mathbb{R}$, and $\sigma: \mathbb{R}^n \to  \mathbb{R}, z\mapsto \|z\|^\alpha$. Then the vector field $\sigma\nabla p$ has a potential $q$, i.e., $\nabla q = \sigma \nabla p$, only for $\alpha=0$.
\end{restatable}

\begin{proof}
    Suppose $\sigma \nabla p$ has potential. Consider two paths with segments $s_1, s_2$ and $s_3, s_4$ going $t \to 2t \to -2t$ and $t \to -t \to -2t$, where the segments $s_1, s_4$ scaling $\pm t \to \pm2t$ are straight lines and the other segments $s_2, s_3$ follow great circles on $S^{n-1}$. By Proposition~\ref{prop:cos_sim_grads}, we know that $\nabla p(z)=0$ for $z\in \mathbb{R}_{\neq 0}\cdot t$. So $\sigma \nabla p$ is zero on $s_1$ and $s_4$. Moreover, we have
    \begin{align}
        \int_{s_2} \sigma \nabla p \,dz &= \int_{s_2} \|z\|^\alpha \nabla p \,dz
        = \int_{s_2} 2^\alpha \nabla p \,dz 
        = 2^\alpha \int_{s_2} \nabla p \,dz 
        = 2^\alpha \big(p(2t) - p(-2t)\big) = 2^{\alpha+1}
    \end{align}
    and similarly 
    \begin{align}
        \int_{s_3} \sigma \nabla p dz = 1^\alpha \cdot 2 = 2.
    \end{align}
    Since we assume the existence of a potential, we can use path independence to conclude 
    \begin{align}
        2^{\alpha+1} &= \int_{s_2} \sigma \nabla p \,dz 
        = \int_{s_1, s_2} \sigma \nabla p \,dz 
        = \int_{s_3, s_4} \sigma \nabla p \,dz 
        = \int_{s_3} \sigma \nabla p \,dz 
        = 2.
    \end{align}
    Thus, $\alpha=0$ and $\sigma$ does not perform any scaling.
\end{proof}




\begin{figure}
    \begin{lstlisting}[caption={PyTorch code for gradient scaling layer}, label={alg:grad_scaling}]
class scale_grad_by_norm(torch.autograd.Function):
    @staticmethod
    def forward(ctx, z, power=0):
        ctx.save_for_backward(z)
        ctx.power = power
        return z
    @staticmethod
    def backward(ctx, grad_output):
        z = ctx.saved_tensors[0]
        power = ctx.power
        norm = torch.linalg.vector_norm(z, dim=-1, keepdim=True)
        return grad_output * norm**power, None
\end{lstlisting}
\end{figure}

\begin{algorithm}[tb]
   \caption{Pytorch-like pseudo-code using the gradient scaling layer}
   \label{alg:grad_scaling_use}
\begin{algorithmic}
   \STATE {\bfseries Input:} Encoder network $model$, gradient scaling power $p$
   \STATE $z = model(batch)$
   \STATE $z = grad\_scaling\_layer.apply(z, p)$
   \STATE $sim = (\frac{z}{\|z\|})^T \frac{z}{\|z\|}$
   \STATE $loss = InfoNCE(sim)$
   \STATE $loss.backward()$
\end{algorithmic}
\end{algorithm}


\section{Additional figures}
We provide a bar plot analogous to Figure \ref{fig:in_out_violin} in Figure \ref{fig:in_out_distribution_norms}.

\begin{figure}
    \centering
    \begin{tikzpicture}   
        \node[inner sep=0pt] (image) at (0,0) {\includegraphics[width=\textwidth]{Images/Confidence/per_class_norms.pdf}};
    \end{tikzpicture}
    \caption{Bar plot which is analogous to Figure \ref{fig:in_out_violin} showing embedding magnitudes on each dataset split as a function of which dataset the model was trained on. All values are normalized by training set's mean embedding magnitude. Normalized means are represented by black bars. We use the same data augmentations for the train and test sets for consistency.}
    \label{fig:in_out_distribution_norms}
\end{figure}

We also show each Cifar-10 class's 10 highest and 10 lowest embedding-norm samples in Figure \ref{fig:cifar_norms}. These are obtained after training default SimCLR on Cifar-10 for 512 epochs. We see that the high-norm class representatives are prototypical examples of the class while the low-norm representatives are obscure and qualitatively difficult to identify. This property was originally shown by \citet{embed_norm_confidence_2}.

\begin{figure}
    \centering
    \includegraphics[width=0.48\linewidth]{Images/high_norm.png}
    \quad
    \includegraphics[width=0.48\linewidth]{Images/low_norm.png}
    \caption{\emph{Left}: highest-norm representatives (top 10) per class. \emph{Right}: lowest-norm representatives (bottom 10) per class. All from default SimCLR trained on Cifar-10.}
    \label{fig:cifar_norms}
\end{figure}