\section{The Properties of SSL Gradients}
\label{sec:theory}

We begin by studying the gradients of the cosine similarity with respect to an arbitrary point $z_i$. Throughout this section, we refer to $\mathbf{Z}$ as any set of points in $\mathbb{R}^d$, with no other assumptions over the distribution. The gradient acting on one of these points has the following structure:

\begin{restatable}{proposition}{cosgrads}[Prop. 3 in \citet{spherical_embeddings};\footnote{\citet{spherical_embeddings} also showed corresponding results under SGD with momentum and Adam optimization \citep{adam}.} proof in \ref{prf:prop_grad_grows}]
    \label{prop:cos_sim_grads}
    Let $\mathbf{Z}$ be a set of points in $\mathbb{R}^d$ and let $z_i$ and $z_j$ be a positive pair in $\mathbf{Z}$. Let $\phi_{ij}$ be the angle between $z_i$ and $z_j$. Then the gradient of $\mathcal{L}_{ij}^\mathcal{A}(\mathbf{Z})$ with respect to $z_i$ is
    \[ \nabla_i^\mathcal{A} = -\frac{1}{\|z_i\|} \left(\mathbf{I}_d - \frac{z_i z_i^\top}{\|z_i\|^2} \right) \frac{z_j}{\|z_j\|} = -\left( \frac{\hat{z}_j}{\|z_i\|} \right)_{\perp z_i} \]
    where $a_{\perp b}$ is the component of $a$ orthogonal to $b$.
    This has magnitude $\|\nabla_i^\mathcal{A}\| = \frac{\sin(\phi_{ij})}{\|z_i\|}$.
\end{restatable}

\noindent This has an easy interpretation: $\mathbf{I}_d - \frac{z_i z_i^\top}{\|z_i\|^2}$ projects the unit vector $\hat{z}_j$ onto the subspace orthogonal to $z_i$. This projected vector is then inversely scaled by $\|z_i\|$. We visualize this in Figure~\ref{fig:fig1}. A similar result holds for the InfoNCE loss:
\begin{restatable}{proposition}{infoncegrads}[Proof in \ref{app:infonce_grads}]
    \label{cor:infonce_grads}
    Let $\mathbf{Z}$ be a set of points in $\mathbb{R}^d$, $z_i$ and $z_j$ be a positive pair in $\mathbf{Z}$, and $\nabla_i^\mathcal{A}$ be as in Prop. \ref{prop:cos_sim_grads}. Then the gradient of $\mathcal{L}_{ij}(\mathbf{Z})$ with respect to $z_i$ is
    \begin{equation}
        \label{eq:infonce_grads}
        \nabla_i = \nabla_i^\mathcal{A} + \frac{1}{\|z_i\|} \cdot \sum_{k \not\sim i} \left( \hat{z}_k \cdot \frac{\text{\emph{ExpSim}}(z_i, z_k)}{S_i} \right)_{\perp z_i}.
    \end{equation}\vspace*{-0.2cm}
\end{restatable}

In essence, because the InfoNCE loss is a function of the cosine similarity, the chain rule implies that its gradients behave similarly to the cosine similarity's. Specifically, just like those of $\mathcal{L}_{ij}^\mathcal{A}$, the gradients of $\mathcal{L}_{ij}^\mathcal{R}$ have the properties that (1) they are inversely scaled by $\|z_i\|$ and (2) they exist in $z_i$'s tangent space. Since the InfoNCE loss is the sum of $\mathcal{L}_{ij}^\mathcal{A}$ and $\mathcal{L}_{ij}^\mathcal{R}$, these properties all extend to the InfoNCE loss as well. Going forward, we refer to any loss function or SSL model as \emph{cosine-similarity-based} (cos.sim.-based) if it exhibits these two properties.

\begin{figure*}
    \centering \hspace*{-0.3cm}
    \resizebox{0.27\linewidth}{!}{%
    \begin{subfigure}{0.27\linewidth}
    \captionsetup{oneside,margin={1cm,0cm}}
    \begin{tikzpicture}
        \node[inner sep=0pt] () at (0, 0) {\includegraphics[width=0.98\linewidth, trim={0cm, 0.23cm, 0cm, 0cm}, clip]{Images/vert_bounds.pdf}};

        % \draw[darkgray] (-2.21, -1.1) -- (-2.21, -1.15);
        % \draw[darkgray] (-1.3, -1.1) -- (-1.3, -1.15);
        % \draw[darkgray] (-0.39, -1.1) -- (-0.39, -1.15);
        
        \fill [white] (-1.2,-1.72) rectangle (-0.6,-1.95);
        \fill [white] (0.2,-1.72) rectangle (0.8,-1.95);
        \fill [white] (1.65,-1.72) rectangle (2.25,-1.95);

        \node () at (-0.87, -1.9) {\small $0$};
        \node () at (0.5, -1.9) {\small $\frac{\pi}{2}$};
        \node () at (1.95, -1.9) {\small $\pi$};

        % \draw[darkgray] (0.9, -1.1) -- (0.9, -1.15);
        % \draw[darkgray] (2.3, -1.1) -- (2.3, -1.15);
        % \draw[darkgray] (3.73, -1.1) -- (3.73, -1.15);
    \end{tikzpicture}
    \caption{}
    \label{fig:convergence_sim}
    \end{subfigure}%
    }
    \,\,\,
    \begin{subfigure}{0.35\linewidth}
    \captionsetup{oneside,margin={1cm,0cm}}
    \includegraphics[width=\linewidth]{Images/density.pdf}
    \caption{}
    \label{fig:density}
    \end{subfigure}
    \,
    \begin{subfigure}{0.35\linewidth}
    \captionsetup{oneside,margin={1cm,0cm}}
    \includegraphics[trim={0cm, 0cm, 0cm, 0.5cm}, clip, width=\linewidth]{Images/embed_norms_sim.pdf}
    \caption{}
    \label{fig:class_imbalance}
    \end{subfigure}
    \caption{Simulations studying the relationship between SSL training and embedding norms. \emph{Left}: Applying cosine similarity gradients to pairs of points converges slower as a function of the points' norm and the sin of their angle. \emph{Middle}: Training via InfoNCE to reconstruct latent classes induces higher norms in dense output regions. \emph{Right}: Training via InfoNCE leads to larger norm for high-frequency classes.}
    \label{fig:confidence_sim}
\end{figure*}

This orthogonality has also been noted in \citet{normface} and \citet{mentions_catch22}. As a direct consequence of this projection onto the tangent plane, applying the cosine similarity or InfoNCE gradients to a point \emph{must grow its magnitude} (visualized in Figure%\ref{fig:growing_embeddings})
~\ref{fig:fig1}, middle and right):
\begin{corollary}[First identified in \citet{normface}; Proof in~\ref{prf:cor_embeddings_grow}]
    \label{cor:embeddings_grow}
    Let $z \in \mathbb{R}^d$ and let $z'$ be the result of applying a step of gradient descent with respect to the cosine similarity (Prop.~\ref{prop:cos_sim_grads}) or InfoNCE (Prop.~\ref{cor:infonce_grads}) to $z$. Then
    $\|z'\| \geq \|z\|$.
\end{corollary}

The results in Propositions~\ref{prop:cos_sim_grads}, \ref{cor:infonce_grads} and Corollary~\ref{cor:embeddings_grow} reveal an inevitable catch-22 for self-supervised learning: we require small embeddings to avoid vanishing gradients but optimizing SSL loss functions grows the embeddings. We refer to this as the \emph{embedding-norm effect}. This effect also holds for the mean squared error between normalized embeddings and, by extension, the triplet loss.

Furthermore, Proposition \ref{prop:cos_sim_grads} has direct implications for convergence rates under the cosine similarity. The gradient's magnitude directly scales the learning rate, since $z_i' = z_i + \gamma \nabla_i = z_i + \left( \gamma \cdot \| \nabla_i \| \right) \hat{\nabla}_i.$ Thus, Proposition \ref{prop:cos_sim_grads} can be interpreted as saying that the embedding norm and the sin of the angle parameterize the model's learning rate. Indeed, both quadratically slow down convergence:

\begin{theorem}[Proof in \ref{prf:thm_convergence_rate}]
\label{thm:convergence_rate}
    Let $z_i$ and $z_j$ be embeddings with equal norm, i.e. $\|z_i\| = \|z_j\| = \rho$. Let $z_i' = z_i + \frac{\gamma}{\rho}(z_j)_{\perp z_i}$ and $z_j' = z_j
    + \frac{\gamma}{\rho}(z_i)_{\perp z_j}$ be the embeddings after maximizing the cosine similarity via a step of gradient descent with learning rate $\gamma$.
    Then the change in cosine similarity is bounded from above by:
        \begin{equation}
            \label{eq:thm_statement}
            \hat{z}_i'^\top \hat{z}_j' - \hat{z}_i^\top \hat{z}_j < \frac{2 \gamma \sin^2 \phi_{ij}}{\rho^2}.
        \end{equation}
\end{theorem}
Put simply, the change in the cosine similarity via a step of gradient descent scales quadratically with the embedding's norm and the sin of the angle to its positive counterpart.