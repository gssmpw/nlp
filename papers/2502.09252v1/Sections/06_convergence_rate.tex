\section{The Embedding-Norm Effect in Practice}
\label{sec:convergence}

To understand how the embedding-norm effect influences cosine-similarity-based SSL training, we investigate three distinct interventions which should mitigate it. These interventions provide controlled settings to analyze the empirical relationship between embedding norms and SSL training.

\paragraph{Weight Decay}

Our first intervention mechanism---weight decay---is already present in essentially all SSL models. The idea here is that adding a penalty on the weights implicitly regularizes the embedding norms \citep{normface}.
Figure \ref{fig:weight_decay_ablation} demonstrates this effect in SimCLR and SimSiam training: without weight decay ($\gamma=0$), embeddings grow unconstrained, while excessive weight decay ($\gamma=5e\text{-}2$) causes collapse. With appropriate values ($\gamma=1e\text{-}5$ for SimCLR, $\gamma=5e\text{-}4$ for SimSiam), norms decrease gradually, leading to improved $k$-nn accuracy. Interestingly, the embedding norm's impact on the $k$-nn accuracy is much more pronounced in the attraction-only setting. We also see that, even without weight decay, the embeddings can shrink (as occurs for SimSiam). We attribute this to the embeddings being produced by shared weights: although the gradients require all embeddings to grow, a shared weights matrix may not be able to independently update every point's position.

\input{Figures/weight_decay_ablation}

\paragraph{Cut-Initialization.}

As shown in Figure \ref{fig:weight_decay_ablation}, weight decay gradually reduces embedding norms over time, but doesn't control them at the start of training. To address this, we propose \emph{cut-initialization} - dividing all network weights by a constant $c$ at initialization. This ensures small embedding norms at initialization which are then kept small via weight decay. We implement this uniformly across all layers for simplicity (Listing \ref{alg:cut_init}). Interestingly, a variant of this can be found in HuggingFace's default image transformer code \cite{pytorch} and we know at least one cos.sim.-based SSL model which uses it \citep{beitv2}.

\begin{figure}
    \begin{lstlisting}[caption={PyTorch code for our cut-initialization layer.}, label={alg:cut_init}, captionpos=b]
@torch.no_grad()
def cut_init(model, c):
    for param in model.parameters():
        param.data = param.data / c
\end{lstlisting}
\end{figure}

We study the interplay between cut-initialization and weight decay values on SimCLR and SimSiam in Table \ref{tbl:wd_cut}. Specifically, we report the $k$-nn classification accuracy after 100 epochs on the CIFAR-100 and ImageNet-100 datasets and see that, in both the contrastive and non-contrastive settings, pairing cut-initialization with weight decay accelerates the training process. As was the case for the weight decay, the difference is more stark in the non-contrastive setting. $c=2$ performed best for SimCLR, providing an additional 2\% in accuracy at the default weight decay, while $c=8$ led to about a 10\% increase for SimSiam. For the remaining experiments, we use $c=3$ for SimCLR and $c=9$ for SimSiam. We report a variant of Figure \ref{fig:weight_decay_ablation} including BYOL experiments in Figure \ref{fig:cut_experiments}. We furthermore show the $k$-nn classifier accuracies at $500$ epochs with and without cut-initialization in Table \ref{tbl:accuracies}. Here we see that pairing SSL models with cut-initialization often helps the model reach higher final accuracies.

We also evaluate cut-initialization in imbalanced data settings. For Cifar-10, we use the exponential split from \cite{dassot}, where class $i$ has $n_i = 5000 \cdot 1.5^{-i}$ samples. Similarly for Cifar-100, the $i$-th class receives $n_i = 500 \cdot 1.034^{-i}$ samples. This way, all classes are represented and both imbalanced datasets contain roughly 15K samples. We also use Flowers102's naturally long-tailed test set for training \citep{flowers}, evaluating on its validation set. Table \ref{tbl:imbalanced_experiments} then shows that, in class-imbalanced settings, pairing SSL training with interventions on the embedding-norm effect can provide double-digit accuracy improvements.

\begin{table}
    \centering
    \captionof{table}{$k$-nn accuracy at epoch 100 for various values of cut-constant $c$ and weight decay $\lambda$. \emph{Left}: SimCLR  on Cifar-100. \emph{Right}: SimSiam on ImageNet-100. Default weight-decay is underlined.}
    \label{tbl:wd_cut}
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{cl cccc cc cccc}
    \toprule
    % & & \multicolumn{10}{c}{Weight Decay $\lambda$} \\
    & & \multicolumn{4}{c}{\large SimCLR} & & & \multicolumn{4}{c}{\large SimSiam} \vspace*{0.2cm}\\
    & & \multicolumn{4}{c}{Weight Decay $\lambda$} & & & \multicolumn{4}{c}{Weight Decay $\lambda$} \\
    & & 1e-8 & \underline{1e-6} & 5e-6 & 1e-5 & & & 5e-5 & 1e-4 & \underline{5e-4} & 1e-3\\
    \cmidrule{3-6} \cmidrule{9-12}
    \multirow{4}{*}{\rotatebox[origin=c]{90}{\makecell{Cut}}} & $c=1$ & 40.8 & 40.5 & 40.9 & 41.5 & & & 36.7 & 38.5 & 40.5 & 44.7 \\
    & $c=2$ & 42.7 & 42.8 & 42.9 & 42.2 & & & 41.1 & 44.0 & 48.8 & 42.1\\
    & $c=4$ & 42.3 & 41.4 & 42.0 & 41.1 & & & 40.2 & 41.2 & 49.8 & 49.1\\
    & $c=8$ & 37.1 & 36.8 & 37.9 & 37.3 & & & 44.4 & 46.4 & 50.2 & 53.2\\
    \bottomrule
    \end{tabular}%
    }
\end{table}


% \begin{table}
%     %\fontsize{10pt}{10pt}\selectfont
%     \centering
%     \begin{tabular}{cl cccc}
%     \midrule \vspace*{0.1cm}
%     & & \multicolumn{4}{c}{Weight Decay $\lambda$} \\
%     & & 5e-5 & 1e-4 & \underline{5e-4} & 1e-3 \\
%     \cmidrule{3-6}
%     \multirow{4}{*}{\rotatebox[origin=c]{90}{\makecell{Cut}}} & $c=1$ & 36.7 & 38.5 & 40.5 & 44.7 \\
%     & $c=2$ & 41.1 & 44.0 & 48.8 & 42.1 \\
%     & $c=4$ & 40.2 & 41.2 & 49.8 & 49.1 \\
%     & $c=8$ & 44.4 & 46.4 & 50.2 & 53.2 \\
%     \bottomrule
%     \end{tabular}
%     \captionof{table}{SimSiam $k$-nn accuracy (epoch 100) on Imagenet-100 for various values of $c$ and $\lambda$. Default weight-decay is underlined.}
%     \label{tbl:simsiam_wd_cut}
% \end{table}


\paragraph{GradScale Layer.}

Perhaps the cleanest way to overcome the embedding-norm effect is to simply rescale the gradient directly. We achieve this using a custom PyTorch \texttt{autograd.Function} which we refer to as \emph{GradScale} (for a full implementation, see Listing~\ref{alg:grad_scaling} in the Appendix). This layer accepts a \emph{power} parameter $p$ and is simply the identity function in the forward pass. However, the backwards pass multiplies each sample $z_i$'s contribution to the gradient by $\|z_i\|^p$. Thus, choosing power $p=0$ gives the default setting while choosing $p=1$ will cancel the gradients' dependence on the embedding norm. We visualize the resulting gradient fields with powers $p=0$ and $p=1$ for a 2D embedding space in Figure~\ref{fig:grad_scaling}. We refer to training with GradScale power $p=1$ simply as GradScale.

Under GradScale, the gradient norms differ from those during default training, necessitating a new learning rate schedule. Traditionally, SSL models are trained with $10$ epochs of linear warmup followed by a cosine-annealing schedule \citep{simclr}. However, the schedule has an implicit division by the embedding norms over the course of training. 
%Thus, the \emph{effective} learning rates are given by $\gamma_{\text{eff}}(t) = \gamma(t) \cdot b \cdot \big(\sum_{i=1}^b \|z_i(t)\|\big)^{-1}$, where $\gamma(t)$ is the learning rate schedule at epoch $t$ and $\frac{1}{b} \cdot \sum_{i=1}^b \|z_i(t)\|$ is a batch's mean embedding norm at epoch $t$. 
We therefore simulate this effective learning rate for the GradScale setting. Namely, we choose base learning rate $\gamma' = \gamma/6$ with 100 linear warmup epochs followed by cosine annealing, where $\gamma$ is the default learning rate. This was the first relatively stable schedule which we found for the $p=1$ setting and we performed no additional tuning.

Table \ref{tbl:accuracies} demonstrates that, when training remains stable, SimCLR's $k$-nn accuracy benefits from GradScale's embedding norm cancellation. Consistent with our cut-initialization experiments, this improvement becomes particularly pronounced on the class-imbalanced datasets in Table \ref{tbl:imbalanced_experiments}: GradScale provides a roughly 5\% accuracy increase across the imbalanced datasets. While these results are promising, we observed that GradScale with $p=1$ failed to converge on Imagenet-100 and for non-contrastive models. This limitation aligns with these models' known sensitivity issues \cite{simsiam_avoid_collapse, BYOL_orthogonality}.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Images/custom_autograd_function_two_settings.pdf}
    \caption{\emph{Left}: the default gradient field of the cosine similarity with respect to the north-pointing line (blue). \emph{Right}: the gradient field using the GradScale layer with $p=1$.}
    \label{fig:grad_scaling}
\end{figure}


\begin{table}
    \centering
    \caption{$k$-nn accuracies at epoch 500 for default, cut-initialized and GradScale training on standard image datasets.}\vspace*{0.1cm}
    \label{tbl:accuracies}
    \resizebox{\linewidth}{!}{%
        \begin{tabular}{cr c c c}
        \toprule
        & & Cifar-10 & Cifar-100 & Imagenet-100 \\
        \cmidrule{3-5}
        % \cmidrule(l{2pt}r{2pt}){3-3} \cmidrule(l{2pt}r{2pt}){4-4} \cmidrule(l{2pt}r{2pt}){5-5}
        \multirow{3}{*}{SimCLR} & Default & 85.2 & 51.9 & 59.4 \\
        & Cut ($c=3$) & 87.0 & 52.6 & 60.9 \\
        & GradScale & 86.5 & 54.0 & 01.0 \\
        \midrule
        % \cmidrule(l{2pt}r{2pt}){3-3} \cmidrule(l{2pt}r{2pt}){4-4} \cmidrule(l{2pt}r{2pt}){5-5}
        \multirow{2}{*}{SimSiam} & Default & 87.0 & 61.1 & 62.0 \\
        & Cut ($c=9$) & 89.0 & 61.5 & 67.2 \\
        % \cmidrule(l{2pt}r{2pt}){3-3} \cmidrule(l{2pt}r{2pt}){4-4} \cmidrule(l{2pt}r{2pt}){5-5} 
        %\multirow{2}{*}{BYOL} & Default & 88.2 & 61.9 & 67.7 \\
        %& Cut ($c=9$) & 88.6 & 61.4 & 68.6 \\
        \bottomrule
        \end{tabular}%
    }
\end{table}

\begin{table}
    \centering
    \caption{$k$-nn accuracies at epoch 500 for default, cut-initialized and GradScale training on class-imbalanced image datasets.}\vspace*{0.1cm}
    \label{tbl:imbalanced_experiments}
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{lrccc}
    \toprule
    & & \makecell{Cifar-10\\Unb.} & \makecell{Cifar-100\\Unb.} & \makecell{Flowers\\Unb.} \\
    \cmidrule{3-5}
    \multirow{3}{*}{SimCLR} & Default & 56.5 & 24.3 & 42.6\\
    & Cut $(c=3)$ & 61.1 & 26.3 & 61.6 \\
    & GradScale & 61.3 & 29.1 & 47.2 \\
    \midrule
    \multirow{2}{*}{SimSiam} & Default & 47.0 & 21.6 & 22.5 \\
    & Cut $(c=9)$ & 61.7 & 31.5 & 39.9 \\
    \bottomrule
    \end{tabular}%
    }
\end{table}

\paragraph{Takeaways.}

These results make it clear that the embedding norm effect impacts SSL training -- particularly in non-contrastive settings -- and can be mitigated using our proposed strategies. The effect appears most detrimental in class-imbalanced settings, aligning with our results on SSL confidence: imbalanced data creates variance in embedding norms, destabilizing training. Nonetheless, there remain questions which are beyond the scope of this work:
\begin{question}
    Why are non-contrastive architectures more sensitive to the embedding-norm effect?
\end{question}
In addition to seeing that the embedding-norm effect is more pronounced in attraction-only settings, we have found that the embeddings can shrink even in the absence of weight decay. We attribute both phenomena to the network's shared weights: while our theory predicts uniform embedding growth, producing these embeddings via a single set of weights creates competition between different regions of the latent space. This competition would be especially relevant for the InfoNCE loss, which enforces uniformity over the latent hypersphere \cite{understanding_contr_learn}.

\begin{question}
    Are there SSL training schemes in which the embedding-norm effect is beneficial?
\end{question}

We have been careful to not describe the embedding-norm effect as a strictly negative phenomenon. Consider the common transfer-learning setting in which prototypical class examples should anchor the learned representation \citep{prototypical_1, prototypical_2}. Our findings suggest the embedding-norm effect naturally supports this goal: prototypical examples should have large embedding norms and consequently would receive smaller gradients.