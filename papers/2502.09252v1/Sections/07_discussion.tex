\section{Discussion}

In this work, we investigated a fundamental aspect of cosine similarity-based self-supervised learning: embedding norms serve a dual role, both inversely scaling gradients and encoding model certainty. These characteristics are intrinsic to standard SSL models and our analysis showed how they alter the expected training dynamics in both standard and class-imbalanced settings.

Given that these properties are natively present in widely-used SSL approaches, they suggest several research directions beyond those already stated in the paper. First, embedding norms could serve as simple yet effective reliability metrics during inference. Second, the embedding norms provide a new lens for studying the modality gap in multi-modal representation learning \cite{mindthegap}. Lastly, the interplay between the embedding norm's roles--as both a confidence metric and gradient scalar--suggests training pipelines that explicitly leverage both mechanisms.

\section*{Acknowledgements}

Andrew Draganov is partially supported by the Independent Research Fund Denmark (DFF) under a Sapere Aude Research Leader grant No 1051-00106B. 
Sharvaree Vadgama is supported by the Hybrid Intelligence Center, a 10-year program funded by the Dutch Ministry of Education, Culture and Science through the Netherlands Organisation for Scientific Research (NWO). This work was partially funded by the Gemeinnützige Hertie-Stiftung, the Cyber Valley Research Fund (D.30.28739), and the National Institutes of Health (UM1MH130981). The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health. Dmitry Kobak is a member of the Germany’s Excellence cluster 2064 ``Machine Learning --- New Perspectives for Science'' (EXC 390727645). The authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Jan Niklas B\"ohm.
\newpage

% \section*{Impact Statement}

% The goal of our paper is to advance the field of machine learning. We do not see any potential societal consequences of our work that would be worth specifically highlighting.
