\section{Introduction}

Self-supervised learning (SSL) has emerged as a powerful tool for learning representations from unlabeled data. 
This is because, when trained on large unlabeled datasets with contrastive objectives, SSL models often achieve performance levels comparable to those of supervised methods and can even induce emergent properties \citep{simclr, dino}. SSL has also significantly advanced multi-modal learning, particularly in vision-language models \citep{imagebind, foundation_models}.
Indeed, prominent methods such as CLIP \citep{clip}, ALIGN \citep{align} and Florence \citep{florence} all rely on standard SSL objectives discussed in this paper.

%SSL on a hypersphere paragraph 2
These algorithms work by representing similar inputs near each other and dissimilar inputs far apart in an embedding space normalized to a hypersphere. This is done by optimizing objective functions based on the cosine similarity between embeddings: popular SSL frameworks like SimCLR \citep{simclr} and MoCo \citep{moco} optimize the InfoNCE objective, while SimSiam \citep{simsiam} and BYOL \citep{byol} optimize the cosine similarity directly. 
Thus, theoretical studies of SSL representations only consider the distribution of points on this latent hypersphere \citep{understanding_contr_learn, latent_inversion}. 

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{Images/fig1.pdf}
    \vspace*{-1.5em}
    \caption{\emph{Left}: Network outputs (blue) lie in ambient $\mathbb{R}^d$, but only their projections (white) onto the hypersphere $S^{d-1}$ enter the loss. However, the embedding norms (shades of blue) influence the norms of the SSL loss gradient. \emph{Middle}: Gradient field of the cosine similarity with respect to the orange direction. Highlighted gradients (black) illustrate the inverse relationship between the gradient and embedding norm (Proposition~\ref{prop:cos_sim_grads}) The blue points trace an embedding's trajectory using gradient descent. \emph{Right}: Because gradient updates are orthogonal to an embedding point, they must increase the point's norm (Corollary~\ref{cor:embeddings_grow}).}
    \label{fig:fig1}
\end{figure*}


At the same time, there has been some empirical evidence that the embedding norms prior to normalization contain meaningful information. For example, \citet{embed_norm_confidence_2} noted that embedding norms are related to network confidence. No theoretical explanation of this phenomenon exists. Separately, \citet{spherical_embeddings} and \citet{normface} showed that training based on the cosine similarity loss affects the embedding norms and that the embedding norms affect the gradient magnitudes. However, the implications of this for SSL methods have not been fully explored. This implies a clear gap in the literature: several works suggest interactions between SSL models and the embedding norms but the extent of this relationship is unknown. Our work provides the first thorough analysis showing how the embedding norms interact with SSL training dynamics.

\textbf{First,} we prove theoretical bounds showing that the embedding norms impose a quadratic slowdown on SSL convergence and verify that this occurs in simulation and in practice. At the same time, we show that optimizing the cosine similarity grows these embedding norms across SSL methods. This creates an inherent catch-22: small embedding norms are required to train SSL models but these norms grow during training. Put simply, \emph{effectively training SSL models requires managing the embedding norms}. We offer and evaluate several mechanisms for doing this.

\textbf{Second,} we argue that, because the embeddings grow with each gradient update, their norms naturally correspond to the frequency of observed latent features. Since models are more certain in frequently observed data \cite{long_tail_confidence}, we contend that \emph{SSL embedding norms encode a model's confidence}. We devise methods for studying this and show that it holds with remarkable consistency.

Moreover, these two phenomena interact during SSL training. For instance, since norms both encode uncertainty and scale gradients, SSL models implicitly learn unevenly over the samples --- an effect we show must be mitigated when training on imbalanced datasets. Throughout our paper, we draw attention to similar interactions between embedding norms and SSL dynamics by raising several open questions as directions for future work. 

Our code is available at \url{https://github.com/Andrew-Draganov/SSLEmbeddingNorms}. This paper supersedes our earlier workshop paper \cite{pitfalls}.