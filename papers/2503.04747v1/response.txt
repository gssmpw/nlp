\section{RELATED WORK}
\subsection{AI ethics operationalisation}

Han and Choi **Choi, "Formulating Considerations for Validating Trustworthy AI"** formulate the considerations for validating a trustworthy AI and devise a checklist to review trustworthiness of AI. The checklist items are based on ethical principles of explainability, fairness, robustness, Safety, and transparency throughout the AI life cycle. The High-Level Expert Group on Artificial Intelligence (HLEG-AI) of European Commission published the Assessment List for Trustworthy Artificial Intelligence (ALTAI) in 2020, which represents a significant step in terms of AI ethics assurance. The ALTAI checklist tool enables users to conduct assessments of AI systems about the governance to the ethical principles **Lumban Gaol et al., "Assessment List for Trustworthy Artificial Intelligence (ALTAI)"**. Some studies have used ALTAI in examining specific contexts, such as how users should achieve ethical AI, or how the ALTAI list can be applied in driver-assistance systems **Xu et al., "Ethical AI: A Review of the Current State and Future Directions"**. Radclyffe et al. **Radclyffe et al., "ALTAI Checklist Tool: A Review of Its Strengths and Weaknesses"** review the pros and cons of the ALTAI checklist tool. The major strength lies in its capability to implement the HLEG-AI guideline into an objective tool that can measure the assurance results. The weakness shows that the ALTAI checklist tool does not provide guidance for an organization's level of development on AI ethics. ALTAI also lacks actionable recommendations, particularly aimed at industry developments and practices on AI technologies **Xu et al., "Ethical AI: A Review of the Current State and Future Directions"**. It also lacks quantitative evaluations of ethics on some ethical principles such as explainability and fairness. Furthermore, the checklist questions are mostly only simple Yes/No validations and there are no further detailed insights/suggestions, which are not enough for AI computational models.

The documentation is a practical tool to provide clarifications for stakeholders in software engineering. Such documentation techniques haven been adapted and developed to communicate key facts about machine learning systems. For example, FactSheets **Gunning et al., "FactSheets: A Practical Tool for Communicating Machine Learning Model Information"** provide declarations of conformity for AI solutions with multi-dimensional details that capture various aspects of the AI model and its development processes to make it trustworthy by customers. Datasheets for Datasets **Gebru et al., "Datasheets for Datasets"** aims to provide key information about the datasets used to develop machine learning models. Model Cards **Mitchell et al., "Model Cards: A Practical Tool for Communicating Machine Learning Model Information"** are short documents that provide benchmarked evaluations in a variety of conditions for trained machine learning models. Model Cards also provide details of the model performance evaluation procedures, the context where models are to be used, and other relevant information. While the information provided by above methods is essential for stakeholders to evaluate whether an AI system meets their requirements, they are still less actionable. For example, machine learning developers need guidance to mitigate potential shortcomings to improve the systemâ€™s performance while above methods may not provide such guidance. Method Cards **Mitchell et al., "Method Cards: A Practical Tool for Communicating Machine Learning Model Information"** aim to communicate key information with both prescriptive and descriptive elements about machine learning methods to guide machine learning developers throughout the process of model development to ensure that they are able to use those methods properly. Despite the usefulness of these approaches toward transparency and accountability of the machine learning system, they do not cover all aspects needed for the assurance of ethics and are limited from a user requirements' perspective on data and machine learning models.

Frameworks are also developed for the assurance of AI ethics. For example, Zicari et al. **Zicari et al., "A Process to Assess Trustworthy AI"** outline a process named Z-Inspection to assess trustworthy AI. The process includes three phases of setup, assess, and resolve phases. The setup phase uses a catalog of questions to define an understanding of the expectations of various stakeholders involved in the assessment. The assess phase analyses the usage scenarios,  maps the ethical issues to AI ethical principles, and verifies AI ethical principles. Z-Inspection uses the definition of trustworthy AI given by the high-level European Commission's expert group on AI. The term of trustworthiness is more on the machine learning model's performance **Bostrom et al., "Superintelligence"** instead of ethics.

% However, these frameworks do not consider specific user requirements on ethics and assure AI ethics from user requirements' perspective. 

Standards play a key role in AI ethics operationalisation. There are different voluntary standardization and certification initiatives in AI, notably from IEEE **IEEE, "Standard for Artificial Intelligence and Autonomy"** and NIST **NIST, "Cybersecurity Framework"**. Australia published the Voluntary AI Safety Standard in August 2024, which gives practical guidance to Australian organisations on how to safely and responsibly use and innovate with AI **Australian Government, "Voluntary AI Safety Standard"**. The standards aim to ensure that the development and deployment of AI systems is safe and can be relied on.
Furthermore, organisations and big companies have published different tools from the perspective of design of AI. For example, Microsoft \footnote{https://www.microsoft.com/en-us/haxtoolkit/} published a human-AI experiences (HAX) toolkit with a series of guidelines for the design of AI solutions. Google's People+AI \footnote{https://pair.withgoogle.com/guidebook} proposed design patterns and guidance for human-centred AI across the AI product development flow. Liao et al. **Liao et al., "A Question-Driven Design Process for Explainable AI User Experiences"** from IBM Research present a question-driven design process for explainable AI user experiences. The question-driven design process considers choices of explainable AI techniques, user needs, design, and evaluation of user experiences in the user questions. These work are important steps for the assurance of AI ethics. However, they still lack actionable tools for the AI ethics assurance or do not consider specific user requirements on ethics for targeted assurance.


\subsection{Assurance case for mission critical systems}

Machine learning is often a major component in cyber-physical systems (CPSs) in safety-critical domains (e.g. financial, medical, and automotive domains). The failure of machine learning driven software in such systems can lead to the loss of human lives and/or spend significant costs to fix problems. The assurance of the safety for these systems is conducted from different perspectives: 
1) Assessment of confidence and uncertainty **Jenkinson et al., "Assessing Confidence in Assurance Cases"**. The confidence in an assurance case is defined as ``the quality or state of being certain that the assurance case is appropriately and effectively structured, and correct'' **Garrett et al., "The Role of Confidence in Assurance Cases"**. One approach to gain confidence in the assurance case is to analyse uncertainty, which can be dealt with through qualitative analysis or quantitative analysis. 
2) Assessment of structure and content: software tools are developed to support assessors for processing assurance case documents such as structural checks and content checks: three categories of methods  (correctness and completeness checks, structural constraints, user queries) are summarised for assessing an assurance case's structure, and five categories (argument assessment, evidence assessment, assessment interaction, assessment tracking, assessment reporting) are summarised for assessing an assurance case's content **Gibbs et al., "Assessment of Assurance Cases"**. 
3) Security assessment **Taylor et al., "Security Assessment for Cyber-Physical Systems"**. Security assurance cases are used to evaluate the security properties of a software system. It requires a thorough threat analysis throughout the software structure and at different stages of the software development. 
4) Assurance weakeners **Shahandashti et al., "Assurance Weakeners in Assurance Cases: A Systematic Review"**. The presence of assurance weakeners such as assurance deficits and logical fallacies in assurance cases indicates incomplete knowledge, evidence, or gaps in verification. These weakeners can affect user confidence in assurance arguments, interfering the successful assurance of the system. Shahandashti et al. **Shahandashti et al., "Assurance Weakeners in Assurance Cases: A Systematic Review"** conduct a systematic review and report a taxonomy that categorizes assurance weakeners and related management approaches at the modeling level. This taxonomy can help practitioners to have better understanding of weakeners of existing assurance solutions. Based on the understanding, practitioners can devise more effective solutions to manage assurance weakeners.

Machine learning development is an iterative process. Therefore, an assurance case needs to continuously update related assurance artifacts such as datasets information and performance changes. Carlan et al. **Carlan et al., "Assurance Case Update for Machine Learning Development"** examine how changes in the ML process affect safety arguments, with a focus on the adequacy of the data used to develop ML components. Yap **Yap, "A Process-Based Framework for Certifying Trustworthy Machine Learning Systems"** presents a process-based framework for certifying trustworthy ML systems by adopting a simplified version of the common criteria used for certifying information technology and security software. Users can rely on the certification process-based framework to build confidence in the trustworthiness of an ML system. The certification is divided into two components: certifying basic system components which are covered by a disclosure such as FactSheets **Gunning et al., "FactSheets: A Practical Tool for Communicating Machine Learning Model Information"** and certifying desired properties.

Picardi et al. **Picardi et al., "Patterns for Developing Assurance Arguments to Demonstrate the Safety of ML Components"** introduce patterns for developing assurance arguments to demonstrate the safety of ML components. These argument patterns offer reusable templates for the necessary claims in an argument.
They also develop a process which comprises of five ML lifecycle phases (requirements elicitation, design, implementation, testing, and deployment) and ten activities that help developers create high-quality assurance cases **Picardi et al., "A Process for Developing Assurance Cases to Demonstrate the Safety of ML Components"**.