@article{Davies_measure_2018,
  author    = {Sam Corbett{-}Davies and
               Sharad Goel},
  title     = {The Measure and Mismeasure of Fairness: {A} Critical Review of Fair Machine Learning},
  journal   = {CoRR},
  volume    = {abs/1808.00023},
  year      = {2018},
  url       = {http://arxiv.org/abs/1808.00023},
  archivePrefix = {arXiv},
  eprint    = {1808.00023},
}


@article{Mehrabi_survey_2019,
  author    = {Ninareh Mehrabi and
               Fred Morstatter and
               Nripsuta Saxena and
               Kristina Lerman and
               Aram Galstyan},
  title     = {A Survey on Bias and Fairness in Machine Learning},
  journal   = {CoRR},
  volume    = {abs/1908.09635},
  year      = {2019},
  url       = {http://arxiv.org/abs/1908.09635},
  archivePrefix = {arXiv},
  eprint    = {1908.09635},
}


@article{Bellamy_fairness_2018,
  author    = {Rachel K. E. Bellamy and
               Kuntal Dey and
               Michael Hind and
               Samuel C. Hoffman and
               Stephanie Houde and
               Kalapriya Kannan and
               Pranay Lohia and
               Jacquelyn Martino and
               Sameep Mehta and
               Aleksandra Mojsilovic and
               Seema Nagar and
               Karthikeyan Natesan Ramamurthy and
               John T. Richards and
               Diptikalyan Saha and
               Prasanna Sattigeri and
               Moninder Singh and
               Kush R. Varshney and
               Yunfeng Zhang},
  title     = {{AI} Fairness 360: An Extensible Toolkit for Detecting, Understanding,
               and Mitigating Unwanted Algorithmic Bias},
  journal   = {CoRR},
  volume    = {abs/1810.01943},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.01943},
  archivePrefix = {arXiv},
  eprint    = {1810.01943},
}


@inproceedings{Saxena_how_2019,
author = {Saxena, Nripsuta Ani and Huang, Karen and DeFilippis, Evan and Radanovic, Goran and Parkes, David C. and Liu, Yang},
title = {How Do Fairness Definitions Fare? Examining Public Attitudes Towards Algorithmic Definitions of Fairness},
year = {2019},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {99–106},
numpages = {8},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@Inbook{Kamiran_explainable_2013,
author="Kamiran, Faisal
and {\v{Z}}liobait{\.{e}}, Indr{\.{e}}",
title="Explainable and Non-explainable Discrimination in Classification",
bookTitle="Discrimination and Privacy in the Information Society: Data Mining and Profiling in Large Databases",
year="2013",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="155--170",
}

@misc{Asuncion_UCI_2007,
author = {A. Asuncion and D.J. Newman},
title = {UCI Machine Learning Repository},
year = {2007},
note = {\url{https://archive.ics.uci.edu/ml/index.php}},
}

@inproceedings{Kasirzadeh2021228,
author={Kasirzadeh, A. and Smart, A.},
title={The use and misuse of counterfactuals in ethical machine learning},
booktitle={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency ({FAccT 2021})},
year={2021},
pages={228-236},
}

@inproceedings{Barocas_hidden_2020,
author = {Barocas, Solon and Selbst, Andrew D. and Raghavan, Manish},
title = {The Hidden Assumptions behind Counterfactual Explanations and Principal Reasons},
year = {2020},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {80–89},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@article{McGrath_interpretable_2018,
  author    = {Rory McGrath and
               Luca Costabello and
               Chan Le Van and
               Paul Sweeney and
               Farbod Kamiab and
               Zhao Shen and
               Freddy L{\'{e}}cu{\'{e}}},
  title     = {Interpretable Credit Application Predictions With Counterfactual Explanations},
  journal   = {CoRR},
  volume    = {abs/1811.05245},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.05245},
  archivePrefix = {arXiv},
  eprint    = {1811.05245},
}
  

@article{Zhou_evaluating_2021,
AUTHOR = {Zhou, Jianlong and Gandomi, Amir H. and Chen, Fang and Holzinger, Andreas},
TITLE = {Evaluating the Quality of Machine Learning Explanations: A Survey on Methods and Metrics},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {593},
}


@inproceedings{Schumann_we_2020,
author = {Schumann, Candice and Foster, Jeffrey S. and Mattei, Nicholas and Dickerson, John P.},
title = {We Need Fairness and Explainability in Algorithmic Hiring},
year = {2020},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1716–1720},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}


@article{Begley_explainability_2020,
  author    = {Tom Begley and
               Tobias Schwedes and
               Christopher Frye and
               Ilya Feige},
  title     = {Explainability for fair machine learning},
  journal   = {CoRR},
  volume    = {abs/2010.07389},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.07389},
  archivePrefix = {arXiv},
  eprint    = {2010.07389},
}

@inproceedings{Lundbery_unified_2017,
author = {Lundberg, Scott M. and Lee, Su-In},
title = {A Unified Approach to Interpreting Model Predictions},
year = {2017},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {4768–4777},
location = {Long Beach, California, USA},
series = {NIPS'17}
}



@inproceedings{Schmidt_quantifying_2019,
author = {Philipp Schmidt and Felix Biessmann},
title = {Quantifying Interpretability and Trust in Machine Learning Systems},
year = {2019},
booktitle = {Proceedings of AAAI Workshop on Network Interpretability for Deep Learning 2019},
location = {Long Beach, California, USA},
}




@inproceedings{Hutchinson_50_2019,
author = {Hutchinson, Ben and Mitchell, Margaret},
title = {50 Years of Test (Un)Fairness: Lessons for Machine Learning},
year = {2019},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {49–58},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}


@misc{Wired_now_2021,
author = {Wired},
title = {Now for AI's Latest Trick: Writing Computer Code},
year = {2021},
note = {\url{https://www.wired.com/story/ai-latest-trick-writing-computer-code/}},
}  

@inproceedings{Zhao_men_2017,
    title = "Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints",
    author = "Zhao, Jieyu  and
      Wang, Tianlu  and
      Yatskar, Mark  and
      Ordonez, Vicente  and
      Chang, Kai-Wei",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    pages = "2979--2989",
}


@article{Miller_explanation_2019,
title = {Explanation in artificial intelligence: Insights from the social sciences},
journal = {Artificial Intelligence},
volume = {267},
pages = {1-38},
year = {2019},
author = {Tim Miller},
}

@inproceedings{Dodge_explaining_2019,
author = {Dodge, Jonathan and Liao, Q. Vera and Zhang, Yunfeng and Bellamy, Rachel K. E. and Dugan, Casey},
title = {Explaining Models: An Empirical Study of How Explanations Impact Fairness Judgment},
year = {2019},
booktitle = {Proceedings of the 24th International Conference on Intelligent User Interfaces},
pages = {275–285},
location = {Marina del Ray, California},
series = {IUI '19}
}


@misc{Baleis_cognitive_2019,
  title={Cognitive and Emotional Response to Fairness in AI -- A Systematic Review},
  author={Janine Baleis and Birte Keller and C. Starke and Frank Marcinkowski},
  year={2019},
  note = {\url{https://www.semanticscholar.org/paper/Implications-of-AI-(un-)fairness-in-higher-the-of-Marcinkowski-Kieslich/231929b1086badcbd149debb0abefc84cdb85665}},
}


@inproceedings{Grgic-Hlaca_human_2018,
author = {Grgic-Hlaca, Nina and Redmiles, Elissa M. and Gummadi, Krishna P. and Weller, Adrian},
title = {Human Perceptions of Fairness in Algorithmic Decision Making: A Case Study of Criminal Risk Prediction},
year = {2018},
booktitle = {Proceedings of the 2018 World Wide Web Conference},
pages = {903–912},
location = {Lyon, France},
series = {WWW '18}
}


@inproceedings{Grgic-Hlaca_beyond_2018,
author = {Nina Grgic-Hlaca and Muhammad Bilal Zafar and Krishna P. Gummadi and Adrian Weller},
title = {Beyond Distributive Fairness in Algorithmic Decision Making: Feature Selection for Procedurally Fair Learning},
year = {2018},
booktitle = {Proceedings of the Thirty-Second AAAI Conferenceon Artificial Intelligence (AAAI-18)},
pages = {51-60},
}

@article{Shin_role_2019,
title = {Role of fairness, accountability, and transparency in algorithmic affordance},
journal = {Computers in Human Behavior},
volume = {98},
pages = {277-284},
year = {2019},
author = {Donghee Shin and Yong Jin Park},
}

@misc{Starke_fairness_2021,
      title={Fairness Perceptions of Algorithmic Decision-Making: A Systematic Review of the Empirical Literature}, 
      author={Christopher Starke and Janine Baleis and Birte Keller and Frank Marcinkowski},
      year={2021},
      eprint={2103.12016},
      archivePrefix={arXiv},
      primaryClass={cs.HC}
}



@inproceedings{Wang_explanations_2021,
author = {Wang, Xinru and Yin, Ming},
title = {Are Explanations Helpful? A Comparative Study of the Effects of Explanations in AI-Assisted Decision-Making},
year = {2021},
booktitle = {Proceedings of 26th International Conference on Intelligent User Interfaces},
pages = {318–328},
publisher = {ACM}
}
  





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


@ARTICLE{Wang2011429,
author={Wang, C. and Mattila, A.S.},
title={A cross-cultural comparison of perceived informational fairness with service failure explanations},
journal={Journal of Services Marketing},
year={2011},
volume={25},
number={6},
pages={429-439},
doi={10.1108/08876041111161023},
note={cited By 52},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053054822&doi=10.1108%2f08876041111161023&partnerID=40&md5=e96532bfb838c96f5f3192719db4a25c},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wang2009795,
author={Wang, C.-Y. and Mattila, A.S. and Bartlett, A.},
title={An examination of explanation typology on perceived informational fairness in the context of air travel},
journal={Journal of Travel and Tourism Marketing},
year={2009},
volume={26},
number={8},
pages={795-805},
doi={10.1080/10548400903356194},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77649136585&doi=10.1080%2f10548400903356194&partnerID=40&md5=0af4beaae9cf3b42284f393cebf02e0c},
document_type={Article},
source={Scopus},
}

@ARTICLE{Byrne2008207,
author={Byrne, S. and Damon, F.},
title={To participate or not to participate? Voice and explanation effects on performance in a multi-period budget setting},
journal={British Accounting Review},
year={2008},
volume={40},
number={3},
pages={207-227},
doi={10.1016/j.bar.2008.04.001},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-51349094145&doi=10.1016%2fj.bar.2008.04.001&partnerID=40&md5=1ef594fe6571c23f64c9e6895b5c7145},
document_type={Article},
source={Scopus},
}

@ARTICLE{Cole2008107,
author={Cole, N.D.},
title={The effects of differences in explanations, employee attributions, type of infraction, and discipline severity on perceived fairness of employee discipline},
journal={Canadian Journal of Administrative Sciences},
year={2008},
volume={25},
number={2},
pages={107-120},
doi={10.1002/cjas.57},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-50649092684&doi=10.1002%2fcjas.57&partnerID=40&md5=79c10e6f8535b1cda187057c11d5c82c},
document_type={Article},
source={Scopus},
}

@ARTICLE{Harland1995183,
author={Harland, L.K. and Rauzi, T. and Biasotto, M.M.},
title={Perceived fairness of personality tests and the impact of explanations for their use},
journal={Employee Responsibilities and Rights Journal},
year={1995},
volume={8},
number={3},
pages={183-192},
}


@inproceedings{Ferreira_evidence_2020,
      title={Evidence-based explanation to promote fairness in AI systems}, 
      author={Juliana Jansen Ferreira and Mateus de Souza Monteiro},
      year={2020},
      booktitle = {CHI2020 Fair and Responsible AI Workshop},
}

@article{Rudin2020Age,
    journal = {Harvard Data Science Review},
    doi = {10.1162/99608f92.6ed64b30},
    number = {1},
    note = {https://hdsr.mitpress.mit.edu/pub/7z10o269},
    title = {The Age of Secrecy and Unfairness in Recidivism Prediction},
    url = {https://hdsr.mitpress.mit.edu/pub/7z10o269},
    volume = {2},
    author = {Rudin, Cynthia and Wang, Caroline and Coker, Beau},
    date = {2020-03-31},
    year = {2020},
    month = {3},
    day = {31},
}

% 1997 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{Hochreiter:1997:LSTMOriginal,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press},
  abstract={Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}
}

% 2013 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{Alemdar:2013:ArasDataset,
  title={ARAS human activity datasets in multiple homes with multiple residents},
  author={Alemdar, Hande and Ertan, Halil and Incel, Ozlem Durmaz and Ersoy, Cem},
  booktitle={2013 7th International Conference on Pervasive Computing Technologies for Healthcare and Workshops},
  pages={232--235},
  year={2013},
  organization={IEEE},
  abstract={The real world human activity datasets are of great importance in development of novel machine learning methods for automatic recognition of human activities in smart environments. In this study, we present the details of ARAS (Activity Recognition with Ambient Sensing) human activity recognition datasets that are collected from two real houses with multiple residents during two months. The datasets contain the ground truth labels for 27 different activities. Each house was equipped with 20 binary sensors of different types that communicate wirelessly using the ZigBee protocol. A full month of information which contains the sensor data and the activity labels for both residents was gathered from each house, resulting in a total of two months data. In the paper, particularly, we explain the details of sensor selection, targeted activities, deployment of the sensors and the characteristics of the collected data and provide the results of our preliminary experiments on the datasets.}
}

% 2015 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{Karpathy:2015:VisualizingUnderstandingRNNs,
  title={Visualizing and understanding recurrent networks},
  author={Karpathy, Andrej and Johnson, Justin and Fei-Fei, Li},
  journal={arXiv preprint arXiv:1506.02078},
  year={2015},
  abstract={Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood.  Using character-level language models as an interpretable testbed, we aim to bridgethis gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets.Moreover, our comparative analysis with finite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally,we provide analysis of the remaining errors and suggests areas for further study.}
}

@article{Li:2015:VisualizingUnderstandingNLP,
  title={Visualizing and understanding neural models in nlp},
  author={Li, Jiwei and Chen, Xinlei and Hovy, Eduard and Jurafsky, Dan},
  journal={arXiv preprint arXiv:1506.01066},
  year={2015}
}

% 2016 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{Sturm:2016:InterpretableDeepNNEEG,
  title={Interpretable deep neural networks for single-trial EEG classification},
  author={Sturm, Irene and Lapuschkin, Sebastian and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  journal={Journal of neuroscience methods},
  volume={274},
  pages={141--145},
  year={2016},
  
}

@article{Samek:2016:EvaluatingVizLRP,
  title={Evaluating the visualization of what a deep neural network has learned},
  author={Samek, Wojciech and Binder, Alexander and Montavon, Gr{\'e}goire and Lapuschkin, Sebastian and M{\"u}ller, Klaus-Robert},
  journal={IEEE transactions on neural networks and learning systems},
  volume={28},
  number={11},
  pages={2660--2673},
  year={2016},
  publisher={IEEE},
  abstract={Deep neural networks (DNNs) have demonstrated impressive performance in complex machine learning tasks such as image classification or speech recognition. However, due to their multilayer nonlinear structure, they are not transparent, i.e., it is hard to grasp what makes them arrive at a particular classification or recognition decision, given a new unseen data sample. Recently, several approaches have been proposed enabling one to understand and interpret the reasoning embodied in a DNN for a single test image. These methods quantify the “importance” of individual pixels with respect to the classification decision and allow a visualization in terms of a heatmap in pixel/input space. While the usefulness of heatmaps can be judged subjectively by a human, an objective quality measure is missing. In this paper, we present a general methodology based on region perturbation for evaluating ordered collections of pixels such as heatmaps. We compare heatmaps computed by three different methods on the SUN397, ILSVRC2012, and MIT Places data sets. Our main result is that the recently proposed layer-wise relevance propagation algorithm qualitatively and quantitatively provides a better explanation of what made a DNN arrive at a particular classification decision than the sensitivity-based approach or the deconvolution method. We provide theoretical arguments to explain this result and discuss its practical implications. Finally, we investigate the use of heatmaps for unsupervised assessment of the neural network performance.}
}

% 2017 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{Arras:2017:ExplainingRNNPredictionsSentimentAnalysis,
  title={Explaining recurrent neural network predictions in sentiment analysis},
  author={Arras, Leila and Montavon, Gr{\'e}goire and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  journal={arXiv preprint arXiv:1706.07206},
  year={2017},
  abstract={Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown to deliver insightful explanations in the form of input space relevances for understanding feed-forward neural network classification decisions. In the present work, we extend the usage of LRP to recurrent neural networks. We propose a specific propagation rule applicable to multiplicative connections as they arise in recurrent network architectures such as LSTMs and GRUs. We apply our technique to a word-based bi-directional LSTM model on a five-class sentiment prediction task, and evaluate the resulting LRP relevances both qualitatively and quantitatively, obtaining better results than a gradient-based related method which was used in previous work.}
}

@article{Arras:2017:WhatIsRelevantInTextDoc,
  title={" What is relevant in a text document?": An interpretable machine learning approach},
  author={Arras, Leila and Horn, Franziska and Montavon, Gr{\'e}goire and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  journal={PloS one},
  volume={12},
  number={8},
  year={2017},
  publisher={Public Library of Science},
}

@article{Montavon:2017:ExplainingNonlinearClassificationDeepTaylor,
  title={Explaining nonlinear classification decisions with deep taylor decomposition},
  author={Montavon, Gr{\'e}goire and Lapuschkin, Sebastian and Binder, Alexander and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  journal={Pattern Recognition},
  volume={65},
  pages={211--222},
  year={2017},
  publisher={Elsevier},
}

@inproceedings{Singh:2017:HumanActivityRecognitionRNN,
  title={Human activity recognition using recurrent neural networks},
  author={Singh, Deepika and Merdivan, Erinc and Psychoula, Ismini and Kropf, Johannes and Hanke, Sten and Geist, Matthieu and Holzinger, Andreas},
  booktitle={International Cross-Domain Conference for Machine Learning and Knowledge Extraction},
  pages={267--274},
  year={2017},
}

% 2018 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{Montavon:2018:MethodsForInterpretingUnderstandingNNs,
  title={Methods for interpreting and understanding deep neural networks},
  author={Montavon, Gr{\'e}goire and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  journal={Digital Signal Processing},
  volume={73},
  pages={1--15},
  year={2018},
  publisher={Elsevier}
}

@inproceedings{Yang:2018:ExplainingTherapyPredictionsLRP,
  title={Explaining therapy predictions with layer-wise relevance propagation in neural networks},
  author={Yang, Yinchong and Tresp, Volker and Wunderle, Marius and Fasching, Peter A},
  booktitle={2018 IEEE International Conference on Healthcare Informatics (ICHI)},
  pages={152--162},
  year={2018},
}


% 2019 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@incollection{Arras:2019:ExplainingInterpretingLSTMs,
  title={Explaining and interpreting LSTMs},
  author={Arras, Leila and Arjona-Medina, Jos{\'e} and Widrich, Michael and Montavon, Gr{\'e}goire and Gillhofer, Michael and M{\"u}ller, Klaus-Robert and Hochreiter, Sepp and Samek, Wojciech},
  booktitle={Explainable ai: Interpreting, explaining and visualizing deep learning},
  pages={211--238},
  year={2019},
  publisher={Springer}
}

@article{Kohlbrenner:2019:ShatteringGradients,
  title={Towards best practice in explaining neural network decisions with LRP},
  author={Kohlbrenner, Maximilian and Bauer, Alexander and Nakajima, Shinichi and Binder, Alexander and Samek, Wojciech and Lapuschkin, Sebastian},
  journal={arXiv preprint arXiv:1910.09840},
  year={2019}
}

@article{zhou_making_2016,
	title = {Making Machine Learning Useable by Revealing Internal States Update — A Transparent Approach},
	volume = {13},
	pages = {378--389},
	number = {4},
	journal = {International Journal of Computational Science and Engineering},
	author = {Zhou, Jianlong and Khawaja, M. Asif and Li, Zhidong and Sun, Jinjun and Wang, Yang and Chen, Fang},
	year = {2016}
}


@article{castelvecchi_can_2016,
	title = {Can we open the black box of {AI}?},
	volume = {538},
	pages = {20},
	number = {7623},
	journal = {Nature News},
	author = {Castelvecchi, Davide},
	year = {2016},
	month = {October},
}

@inproceedings{schneeberger_european_2020,
	location = {Cham},
	title = {The European Legal Framework for Medical {AI}},
	series = {Lecture Notes in Computer Science},
	pages = {209--226},
	booktitle = {Machine Learning and Knowledge Extraction},
	publisher = {Springer International Publishing},
	author = {Schneeberger, David and Stöger, Karl and Holzinger, Andreas},
	editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
	year = {2020},
}

@article{holzinger_can_2018,
	title = {Can we Trust Machine Learning Results? Artificial Intelligence in Safety-Critical Decision Support},
	volume = {112},
	pages = {42--43},
	number = {1},
	journal = {{ERCIM} News},
	author = {Holzinger, K. and Mak, Klaus and Kieseberg, Peter and Holzinger, A.},
	year = {2018}
}

@incollection{zhou_2d_2018,
	location = {Cham},
	title = {2D Transparency Space—Bring Domain Users and Machine Learning Experts Together},
	series = {Human–Computer Interaction Series},
	pages = {3--19},
	booktitle = {Human and Machine Learning: Visible, Explainable, Trustworthy and Transparent},
	publisher = {Springer International Publishing},
	author = {Zhou, Jianlong and Chen, Fang},
	year = {2018},
}

@book{zhou_human_2018,
	location = {Cham},
	pages = {3--19},
	title = {Human and Machine Learning: Visible, Explainable, Trustworthy and Transparent},
	publisher = {Springer},
	editor = {Zhou, Jianlong and Chen, Fang},
	year = {2018},
}

@article{carvalho_machine_2019,
	title = {Machine Learning Interpretability: A Survey on Methods and Metrics},
	volume = {8},
	pages = {832},
	number = {8},
	journal = {Electronics},
	author = {Carvalho, Diogo V. and Pereira, Eduardo M. and Cardoso, Jaime S.},
	year = {2019},
}


@article{arya_one_2019,
	title = {One Explanation Does Not Fit All: A Toolkit and Taxonomy of {AI} Explainability Techniques},
	url = {http://arxiv.org/abs/1909.03012},
	journal = {{arXiv}:1909.03012 [cs, stat]},
	author = {Arya, Vijay and Bellamy, Rachel K. E. and Chen, Pin-Yu and Dhurandhar, Amit and Hind, Michael and Hoffman, Samuel C. and Houde, Stephanie and Liao, Q. Vera and Luss, Ronny and Mojsilović, Aleksandra and Mourad, Sami and Pedemonte, Pablo and Raghavendra, Ramya and Richards, John and Sattigeri, Prasanna and Shanmugam, Karthikeyan and Singh, Moninder and Varshney, Kush R. and Wei, Dennis and Zhang, Yunfeng},
	year = {2019},
	eprinttype = {arxiv},
	eprint = {1909.03012},
}


@article{molnar_interpretable_2020,
	title = {Interpretable Machine Learning -- A Brief History, State-of-the-Art and Challenges},
	url = {http://arxiv.org/abs/2010.09337},
	journal = {{arXiv}:2010.09337 [cs, stat]},
	author = {Molnar, Christoph and Casalicchio, Giuseppe and Bischl, Bernd},
	year = {2020},
	month = {October},
	eprinttype = {arxiv},
	eprint = {2010.09337},
}

@techreport{Lee_formalising_2020,
	location = {Rochester, {NY}},
	title = {Formalising trade-offs beyond algorithmic fairness: lessons from ethical philosophy and welfare economics},
	url = {https://papers.ssrn.com/abstract=3679975},
	number = {{ID} 3679975},
	institution = {Social Science Research Network},
	type = {{SSRN} Scholarly Paper},
	author = {Lee, Michelle Seng Ah and Floridi, Luciano and Singh, Jatinder},
	year = {2020},
	month = {July},
}

@article{holzinger_measuring_2020,
	title = {Measuring the Quality of Explanations: The System Causability Scale ({SCS})},
	volume = {34},
	pages = {193--198},
	number = {2},
	journal = {{KI} - Künstliche Intelligenz},
	author = {Holzinger, Andreas and Carrington, André and Müller, Heimo},
	year = {2020},
}


@article{Warner_making_2021,
author = {Richard Warner and Robert H. Sloan},
title = {Making Artificial Intelligence Transparent: Fairness and the Problem of Proxy Variables},
journal = {Criminal Justice Ethics},
volume = {40},
number = {1},
pages = {23-39},
year  = {2021},
publisher = {Routledge},
}


@InProceedings{Zhou_effects_2017,
author="Zhou, Jianlong
and Arshad, Syed Z.
and Luo, Simon
and Chen, Fang",
editor="Bernhaupt, Regina
and Dalvi, Girish
and Joshi, Anirudha
and K. Balkrishan, Devanuj
and O'Neill, Jacki
and Winckler, Marco",
title="Effects of Uncertainty and Cognitive Load on User Trust in Predictive Decision Making",
booktitle="Human-Computer Interaction -- INTERACT 2017",
year="2017",
publisher="Springer",
address="Cham",
pages="23--39",
}

@incollection{Zhou_multimodal_2018,
author = {Zhou, Jianlong and Yu, Kun and Chen, Fang and Wang, Yang and Arshad, Syed Z.},
title = {Multimodal Behavioral and Physiological Signals as Indicators of Cognitive Load},
year = {2018},
isbn = {9781970001716},
publisher = {ACM and Morgan \& Claypool},
booktitle = {The Handbook of Multimodal-Multisensor Interfaces: Signal Processing, Architectures, and Detection of Emotion and Cognition - Volume 2},
pages = {287–329},
numpages = {43}
}


@inproceedings{Zhou_combining_2014,
author = {Zhou, Jianlong and Hang, Kevin and Oviatt, Sharon and Yu, Kun and Chen, Fang},
title = {Combining Empirical and Machine Learning Techniques to Predict Math Expertise Using Pen Signal Features},
year = {2014},
booktitle = {Proceedings of the 2014 ACM workshop on Multimodal Learning Analytics Workshop and Grand Challenge},
publisher = {ACM},
pages = {29–36},
location = {Istanbul, Turkey},
series = {MLA '14}
}


@article{sholihin_how_2013,
	title = {How Does Procedural Fairness Affect Performance Evaluation System Satisfaction? (Evidence from a {UK} Police Force)},
	volume = {15},
	doi = {10.22146/gamaijb.5445},
	shorttitle = {How Does Procedural Fairness Affect Performance Evaluation System Satisfaction?},
	pages = {231--247},
	journal = {Gadjah Mada International Journal of Business},
	author = {Sholihin, Mahfud},
	year = {2013},
	month = {12},
}  

@inproceedings{Yu_do_2019,
author = {Yu, Kun and Berkovsky, Shlomo and Taib, Ronnie and Zhou, Jianlong and Chen, Fang},
title = {Do I Trust My Machine Teammate? An Investigation from Perception to Decision},
year = {2019},
isbn = {9781450362726},
publisher = {ACM},
booktitle = {Proceedings of the 24th International Conference on Intelligent User Interfaces},
pages = {460–468},
location = {Marina del Ray, California, USA},
series = {IUI '19}
}

  

@inproceedings{Cai_effects_2019,
author = {Cai, Carrie J. and Jongejan, Jonas and Holbrook, Jess},
title = {The Effects of Example-Based Explanations in a Machine Learning Interface},
year = {2019},
doi = {10.1145/3301275.3302289},
booktitle = {Proceedings of the 24th International Conference on Intelligent User Interfaces},
pages = {258–262},
numpages = {5},
location = {Marina del Ray, California, USA},
series = {IUI '19}
}

  

@inproceedings{koh_understanding_2017,
	location = {Sydney, Australia},
	title = {Understanding Black-box Predictions via Influence Functions},
	booktitle = {Proceedings of {ICML} 2017},
	author = {Koh, Pang Wei and Liang, Percy},
	pages = {1885-1894},
	year = {2017},
	month = {july},
}

@article{alam_examining_2021,
	title = {Examining the effect of explanation on satisfaction and trust in {AI} diagnostic systems},
	volume = {21},
	doi = {https://doi.org/10.1186/s12911-021-01542-6},
	pages = {178},
	number = {1},
	journal = {{BMC} Medical Informatics and Decision Making},
	author = {Alam, Lamia and Mueller, Shane},
	year = {2021},
	month = {June},
}


@article{pieters_explanation_2011,
	title = {Explanation and trust: what to tell the user in security and {AI}?},
	volume = {13},
	doi = {https://doi.org/10.1007/s10676-010-9253-3},
	pages = {53--64},
	number = {1},
	journal = {Ethics and Information Technology},
	author = {Pieters, Wolter},
	year = {2011},
}

@inproceedings{zhou_effects_2019,
  title={Effects of influence on user trust in predictive decision making},
  author={Zhou, Jianlong and Li, Zhidong and Hu, Huaiwen and Yu, Kun and Chen, Fang and Li, Zelin and Wang, Yang},
  booktitle={Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems},
  pages={1--6},
  year={2019}
}

@inproceedings{zhou_survey_2020,
  title={A Survey on Ethical Principles of AI and Implementations},
  author={Zhou, Jianlong and Chen, Fang and Berry, Adam and Reed, Mike and Zhang, Shujia and Savage, Siobhan},
  booktitle={2020 IEEE Symposium Series on Computational Intelligence (SSCI)},
  pages={3010--3017},
  year={2020},
  organization={IEEE}
}

@article{taddeo2018ai,
  title={How AI can be a force for good},
  author={Taddeo, Mariarosaria and Floridi, Luciano},
  journal={Science},
  volume={361},
  number={6404},
  pages={751--752},
  year={2018},
  publisher={American Association for the Advancement of Science}
}



@article{zhou2015making,
  title={Making machine learning useable},
  author={Zhou, Jianlong and Chen, Fang},
  journal={International Journal of Intelligent Systems Technologies and Applications},
  volume={14},
  number={2},
  pages={91--109},
  year={2015},
  publisher={Inderscience Publishers (IEL)}
}

@article{zhou2015measurable,
  title={Measurable decision making with GSR and pupillary analysis for intelligent user interface},
  author={Zhou, Jianlong and Sun, Jinjun and Chen, Fang and Wang, Yang and Taib, Ronnie and Khawaji, Ahmad and Li, Zhidong},
  journal={ACM Transactions on Computer-Human Interaction (ToCHI)},
  volume={21},
  number={6},
  pages={1--23},
  year={2015},
  publisher={ACM New York, NY, USA}
}

@inproceedings{zhou2015informed,
  title={Be Informed and Be Involved: Effects of Uncertainty and Correlation on User's Confidence in Decision Making},
  author={Zhou, Jianlong and Bridon, Constant and Chen, Fang and Khawaji, Ahmad and Wang, Yang},
  booktitle={Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems},
  pages={923--928},
  year={2015}
}
@article{zhou2021evaluating,
  title={Evaluating the quality of machine learning explanations: A survey on methods and metrics},
  author={Zhou, Jianlong and Gandomi, Amir H and Chen, Fang and Holzinger, Andreas},
  journal={Electronics},
  volume={10},
  number={5},
  pages={593},
  year={2021},
  publisher={MDPI}
}

@article{morley2020initial,
  title={From what to how: an initial review of publicly available AI ethics tools, methods and research to translate principles into practices},
  author={Morley, Jessica and Floridi, Luciano and Kinsey, Libby and Elhalal, Anat},
  journal={Science and engineering ethics},
  volume={26},
  number={4},
  pages={2141--2168},
  year={2020},
  publisher={Springer}
}