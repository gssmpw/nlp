\section{Related Work}
\label{app:related-work}

There is a great deal of work on differentially private Gaussian mean estimation.
There are many axes on which to compare algorithms, representing tradeoffs between running time, robustness, type of privacy guarantee, and sample complexities.
Algorithms also differ in the style of error guarantee and the assumptions they make about the data distribution.
This section provides a brief overview of this prior work, with emphasis on algorithms that can be implemented. 
We restrict our review to the central model of differential privacy, where the data is held by a single curator.


Karwa and Vadhan~\cite{karwa2017finite} first established approaches for learning univariate Gaussians with optimal error, applying the Gaussian mechanism and stability-based techniques from~\cite{DworkL09}.
For isotropic multivariate data $\mc{N}(\mu,\mathbb{I})$ (or, equivalently, $\mc{N}(\mu,\Sigma)$ with $\Sigma$ known) under the assumption that the true mean has $\ell_2$ norm at most $R$, the naive Gaussian mechanism has error that scales linearly with $R$.
Later papers~\cite{kamath_KLSU19,biswas2020coinpress} give estimators with near-optimal error for this setting, with error depending logarithmically on a priori bounds on the range $R$ of the data.

References~\cite{HuangLY21, tsfadia2022friendlycore} give polynomial-time algorithms for private aggregation, with mean estimation as a central application. 
These 
aim to minimize the dependence of the error on the range of the data. 
In the known-covariance case, these have guarantees comparable to~\cite{biswas2020coinpress}. 
Taking an alternative approach, \cite{aumuller2023plan, dagan2024dimension} tailor the amount of noise dimension-by-dimension to dataset's variance, yielding improved guarantees for low-rank distributions, when the covariance matrix is (almost) known. 

In the unknown-covariance setting, one might aim for error guarantees in Mahalanobis distance.
The concurrent works of \cite{duchi2023fast,brown2023fast} gave the first polynomial-time differentially private algorithms achieving this goal with sample complexity linear in $d$.
Their algorithms' errors have no dependence on the range of the data or condition number.
Both algorithms are implementable but have involved analyses that result in significant constants in their privacy guarantees, yielding poor small-sample performance. 


From the above techniques, we experimentally compare with the Gaussian mechanism and with CoinPress \cite{biswas2020coinpress}. 
Huang, Liang, and Yi~\cite{HuangLY21} and Aum{\"u}ller, Lebeda, Nelson, and Pagh~\cite{aumuller2023plan} also release code but, in the setting of our simulations, perform similarly. 
These algorithms, and all described so far, end by introducing Gaussian noise. 

In contrast, our work sits within an established literature connecting differential privacy to robust statistics through the exponential mechanism.\footnote{See  \cite{avella2023differentially,yu2024gaussian} for insightful discussion on an alternative path, privatizing M-estimators through optimization.}
Formal statements of these connections appeared in \cite{DworkL09}, who introduced the \emph{propose-test-release} framework and, among other applications, analyzed the exponential mechanism over quantiles for univariate mean estimation.
This method performs very well in practice; similar ideas drive the DP Theil-Sen estimator for simple linear regression \cite{alabi2020differentially,sarathy2022analyzing}.
See~\Cref{sec:univariate_experiments} for more on univariate estimation.
A number of papers extend these connections, exploring when one can turn robust statistical algorithms into private ones and vice versa \cite{asi2020instance, liu2022differential,georgiev2022induces,hopkins2023robustness,alabi2022privately,asi2023robustness}. 
None of these algorithms seem amenable to implementation.

The Tukey depth mechanisms we consider represent a natural generalization of the univariate exponential mechanism run on quantiles.
Kaplan, Sharir, and Stemmer~\cite{kaplan2020find} applied this approach to the task of producing a point within the convex hull of the input dataset.
Liu, Kong, Kakade, and Oh~\cite{liu2021robust} and BGSUZ applied it to the the unknown covariance case, giving guarantees in Mahalanobis distance under sample complexity depending optimally on the dimension $d$. 
The application of~\cite{liu2021robust} requires prior knowledge of parameter bounds in the form of a bounding box $[-R,+R]^d$.
Its accuracy depends logarithimcally on $R$, whereas the Restricted Tukey Depth Mechanism of BGSUZ is free of any dependence on parameter bounds. 
Both approaches are computationally inefficient. 
Other work on computing depth functions privately includes \cite{ramsay2021differentially,cumings2022differentially,ramsay2023differentially}. 

\paragraph{Properties of the (Restricted) Tukey Depth Mechanism} The Restricted Tukey Depth Mechanism has several desirable properties.
First, it does not require any prior knowledge of parameters of the distribution, such as the covariance matrix $\Sigma$, its condition number $\kappa$, or a range $R$ such that $\|\mu\|_2\leq R$. 
This allows the data analyst to use it without spending privacy budget to estimate these hyperparameters (or guessing them, affecting its accuracy). 
Additionally, not only does the algorithm not need to know $R$, but also its accuracy does not depend on it at all.

Second, it is invariant under invertible affine transformation. If one translates, stretches, and/or rotates the data, runs the algorithm, and then reverses the transformation on the output, the end-to-end algorithm does not change. 
This follows from the fact that Tukey depth itself is affine-invariant.

Third, it has asymptotically optimal accuracy guarantees (Theorem~\ref{thm:BGSUZ}). 
The guarantee holds with respect to Mahalanobis distance $\|\hat{\mu}-\mu\|_\Sigma=\|\Sigma^{-1/2}(\hat{\mu}-\mu)\|_2$, an error metric which tightly captures the uncertainty of the true mean and characterizes the total variation distance between $\cN(\mu, \Sigma)$ and $\cN(\hat{\mu}, \Sigma)$ up to constants. 
Mahalanobis error $\alpha$ implies a Euclidean guarantee of $\|\hat\mu - \mu\|_2 \leq \alpha \sqrt{\|\Sigma\|_2}$. 
\begin{thm}[Theorem 3.2~\cite{brown2021covariance}]\label{thm:BGSUZ}
    For any $\eps,\delta >0$, the Restricted Tukey Depth Mechanism is $(\eps, \delta)$-differentially private.
    There exists an absolute constant $C$ such that, for any $0<\alpha,\beta,\eps < 1$, $0<\delta\le \frac{1}{2}$, mean $\mu$, and positive definite $\Sigma$, if $x\sim \cN(\mu,\Sigma)^{\otimes n}$ and
    \begin{equation}
        n \ge C\left(\frac{d + \log 1/\beta}{\alpha^2} + \frac{d + \log(1/\alpha\eps \beta)}{\alpha\eps} +  \frac{\log 1/\delta}{\eps} \right),
    \end{equation}
    then with probability at least $1-\beta$, $\|\cA(x)-\mu\|_{\Sigma}\leq\alpha$.
\end{thm}
The dependence on $\delta$ in the sample complexity is decoupled from $d$, which implies that we can ask for $\delta$ to be very small, on the order of $e^{-d}$. 
Furthermore, the constant $C$ is not too large, overall making it an algorithm whose accuracy is expected to be good in practice.

Finally, the algorithm is robust to data corruptions in the strong contamination model (Definition~\ref{def:strong-contamination}).
Theorem~\ref{thm:BGSUZ} still holds (up to a change in constant) when the data comes from an $\alpha$-corruption of data from a Gaussian.

For our experiments, we examine the standard Gaussian Mechanism, the practice-oriented CoinPress~\cite{biswas2020coinpress}, and a number of Tukey-based mechanisms: the Tukey Depth Mechanism over the hypercube of~\cite{liu2021robust} (BoxEM) and the Restricted Tukey Depth Mechanism of BGSUZ (REM) across different notions of depth, including the axis-aligned halfspaces used in~\cite{amin2022easy}.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[t]{0.48\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/anisotropic_example.pdf} %
        \caption{}
        \label{fig:top-left}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/depths_on_anisotropic.pdf} %
        \caption{}
        \label{fig:top-right}
    \end{subfigure}

    \vspace{1em} %
    \begin{subfigure}[t]{0.8\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/output_covariances.pdf} %
        \caption{}
        \label{fig:bottom}
    \end{subfigure}

    \caption{When data arise from a nonspherical distribution (a), the Tukey level sets (b) reflect this. In (c), we show the covariance of output distributions of different mechanisms: the empirical mean and the output of BoxEM-Exact have similar shapes. CoinPress reveals its use of spherical Gaussian noise. 
    (Here we use $n=200$;  CoinPress has relatively high error. To emphasize shape, we scaled the CoinPress covariance  down by a factor of ten.)
    Interestingly, BoxEM with axis-aligned depth seems to sit between the two methods.}
    \label{fig:main-figure}
\end{figure}