\pdfoutput=1
\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\graphicspath{{media/}}     % organize your images and other figures under media/ folder


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bbm}

\usepackage{multirow}
\usepackage{booktabs}
\usepackage{graphicx}   
\usepackage{subcaption} 
\usepackage{xspace}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{bm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{wrapfig}
\usepackage[hang]{footmisc}

\newcommand{\proj}{LLM-BP\xspace}
\newcommand{\GG}{\mathcal{G}}
\newcommand{\VV}{\mathcal{V}}
\newcommand{\EE}{\mathcal{E}}
\newcommand{\TT}{\mathcal{T}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\XX}{\boldsymbol{X}}
\newcommand{\YY}{\boldsymbol{Y}}
\newcommand{\xx}{\boldsymbol{x}}
\newcommand{\HH}{\boldsymbol{H}}
\newcommand{\hh}{\boldsymbol{h}}


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
%\newcommand{\theHalgorithm}{\arabic{algorithm}}
%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[LO]{Model Generalization on Text Attribute Graphs: Principles with Large Language Models}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}



  
%% Title
\title{Model Generalization on Text Attribute Graphs: \\
Principles with Large Language Models
%%%% Cite as
%%%% Update your official citation here when published 
%\thanks{\textit{\underline{Citation}}: 
%\textbf{Authors. Title. Pages.... DOI:000000/11111.}} 
}

\author{
  Haoyu Wang\\
  Department of ECE \\
Georgia Tech \\
  \texttt{haoyu.wang@gatech.edu} \\
  \And
  Shikun Liu\\
  Department of ECE \\
Georgia Tech \\
  \texttt{shikun.liu@gatech.edu} \\
  \And
  Rongzhe Wei\\
  Department of ECE \\
Georgia Tech \\
  \texttt{rongzhe.wei@gatech.edu} \\
  \And
  Pan Li\\
  Department of ECE \\
Georgia Tech \\
  \texttt{panli@gatech.edu} \\
  %% examples of more authors
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}


\begin{document}
\maketitle


\begin{abstract}
Large language models (LLMs) have recently been introduced to graph learning, aiming to extend their zero-shot generalization success to tasks where labeled graph data is scarce. Among these applications, inference over text-attributed graphs (TAGs) presents unique challenges: existing methods struggle with LLMs' limited context length for processing large node neighborhoods and the misalignment between node embeddings and the LLM token space. To address these issues, we establish two key principles for ensuring generalization and derive the framework \proj accordingly: (1) \textbf{Unifying the attribute space with task-adaptive embeddings}, where we leverage LLM-based encoders and task-aware prompting to enhance generalization of the text attribute embeddings; (2) \textbf{Developing a generalizable graph information aggregation mechanism}, for which we adopt belief propagation with LLM-estimated parameters that adapt across graphs. Evaluations on 11 real-world TAG benchmarks demonstrate that \proj significantly outperforms existing approaches, achieving 8.10\% improvement with task-conditional embeddings and an additional 1.71\% gain from adaptive aggregation. Task-adaptive embeddings and codes are publicly available at\footnote{\url{https://github.com/Graph-COM/LLM_BP}, \\ \url{https://huggingface.co/datasets/Graph-COM/Text-Attributed-Graphs}}.
\end{abstract}


\input{Arxiv/001-Introduction}

\input{Arxiv/002-Related_Work}

\input{Arxiv/003-Model_Generaliztaion_Principles}

\input{Arxiv/004-Experiments}

\input{Arxiv/007-Discussion}


\section*{Acknowledgments}
%We thank Yuhan Li, Dr. Jia Li from HKUST and Zhikai Chen, Dr. Jiliang Tang from Michigan State university for their valuable explorations and benchmarks on text-attributed graphs~\cite{li2024glbench, chen2024text}. Additionally, we acknowledge the implementation of LLaGA~\cite{chen2024llaga} by Runjin Chen, Dr. Zhangyang Wang from UT Austin. Our dataset pre-processing are built upon these works. We thank Dr. Junteng Jia and Dr. Austin R. Benson for their work~\cite{jia2021graph}, which has provided us with valuable inspiration in the generalized use of  graph structures. 
H. Wang, S. Liu, R. Wei and Dr. P. Li are partially supported by NSF awards IIS-2239565, CCF-2402816, IIS-2428777, PHY-2117997; DOE award DE-FOA-0002785; JPMC faculty awards; Microsoft Azure Research Credits for Generative AI; and Meta research award.

We extend our sincere gratitude to Dr. Hongbin Pei, Dr. Zhen Wang, and Jingcheng Cen for their valuable assistance in identifying the raw node text of the heterophilic graphs used in this study.





%Bibliography
\bibliographystyle{unsrt}  
\bibliography{references}  

\newpage

\input{Arxiv/006-Appendix}


\end{document}
