\section{Introduction} \label{sec:intro}

Inspired by the remarkable generalization capabilities of foundation models for text and image data~\cite{achiam2023gpt, liu2021swin, radford2021learning}, researchers have recently explored extending these successes to graph data~\cite{liu2023towards, mao2024graph, zhao2023gimlet, fan2024graph, he2023harnessing}, aiming to develop models that generalize to new or unseen graphs and thereby reduce reliance on costly human annotation~\cite{li2024glbench, chen2024text, feng2024taglas, li2024teg}. Among various types of graph data, \emph{text-attributed graphs} (TAGs) have found a wide range of applications. These graphs combine both topological relationships and textual attributes associated with each node, which naturally arises in recommendation systems (where user and item nodes may have textual descriptions or reviews)~\cite{bobadilla2013recommender}, academic graphs (where publications include extensive textual metadata)~\cite{mccallum2000automating, giles1998citeseer}, and financial networks (where transactions and accounts come with textual records)~\cite{kumar2016edge, kumar2018rev2}. Given the labeling challenges posed by cold-start problems in recommendation systems or fraud detection in financial networks, methods that can operate with limited labeled data are crucial. In particular, robust zero-shot node labeling across unseen TAGs has become an area of great interest.

Numerous studies have been dedicated to inference tasks on TAGs. Early efforts have primarily focused on adapting pre-trained language model (LM) encoders~\cite{li2024zerog, fang2024uniglm}, sometimes in combination with graph neural networks (GNNs)~\cite{hou2022graphmae, velivckovic2018deep}, to incorporate structural information. However, these approaches often struggle to achieve strong generalization performance, largely due to the limited capacity of the underlying models. With the advent of large language models (LLMs)~\cite{kaplan2020scaling,huang2022towards}, researchers have proposed two main strategies for integrating LLMs into TAG inference: 1) \textbf{Direct Node-Text Input.} Here, raw node texts are directly fed into LLMs. This method demonstrates reasonably good zero-shot performance on TAGs when text attributes are highly informative for node labels~\cite{chen2024exploring, li2024similarity}. However, when the textual attributes are insufficient, it becomes necessary to aggregate information from a larger neighborhood in the graph, while this is constrained by the limited context length LLMs can digest and reason over. 2) \textbf{Embedding-Based Integration.} In this approach, node texts and their neighboring structural information are first encoded into compressed embeddings, which are then processed by LLMs~\cite{chen2024llaga,tang2024graphgpt,luo2024enhance,wang2024llms,zhang2024graphtranslator}. Because LLMs are not inherently trained on arbitrary embedding spaces, aligning these embeddings with the LLM's token space is essential - an idea partly inspired by how vision-language models align multimodal data~\cite{radford2021learning, zhai2023sigmoid}. However, unlike the vision-language domain, where large-scale text–image pairs~\cite{schuhmann2022laion} are abundant, the graph domain typically lacks comparable datasets. This scarcity reduces the model’s generalization in practice.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\textwidth]{Arxiv/figures/pipeline/pipeline.pdf}
    \vspace{-0.2cm}
    \caption{The two generalization principles and the framework of LLM-BP.}
    \vspace{-0.4cm}
    \label{fig:pipe}
\end{figure*}



In contrast to prior heuristic approaches, this work aims to design a method from first principles for robust zero-shot generalization on TAGs. Because TAGs are inherently multimodal, the proposed method must simultaneously address potential distribution shifts in both textual attributes and graph structure. Specifically, text attributes can vary widely, for example from scientific papers~\cite{mccallum2000automating} to e-commerce product reviews~\cite{ni2019justifying}. Edge connection patterns can range from homophilic graphs such as citation networks, where papers on similar themes are linked~\cite{giles1998citeseer}, to heterophilic graphs such as webpages, which connect nodes with distinct topics~\cite{mernyei2020wiki}. Moreover, the labeling task itself can shift which requires a task-adaptive approach to process both textual features and network structure. Consequently, the core insight behind our model design is grounded in the following two key principles.

\textbf{Principle I: Unifying the text space and obtaining task-adaptive embeddings.} LLMs  offer powerful text-understanding capabilities that naturally unify the textual feature space. However, to handle the large-scale graph aggregation discussed later, we require these capabilities to extend beyond raw text to an embedding space. Hence, we propose to adopt LLM-based encoder models such as LLM2Vec~\cite{behnamghader2024llm2vec, li2024making} for text embedding. Although this approach might appear to be a naive extension of smaller LM-based embedding methods (e.g., those relying on SBERT~\cite{reimers2019sentence} or RoBERTa~\cite{liu2019roberta}), we argue that leveraging the decoder-induced encoder structure of LLMs is essential for achieving task-adaptive embeddings. In particular, we introduce a novel prompting strategy that encodes text attributes conditioned on inference-task descriptions, enabling significantly improved zero-shot inference - an ability not readily achieved by smaller LM-based embeddings.

\textbf{Principle II: Developing a generalizable adaptive graph information aggregation mechanism.} Graph structure determines the node neighboring relationships and thus the information aggregation from which nodes may benefit the inference. Inspired by the belief propagation (BP) algorithm~\cite{murphy2013loopy} that gives the optimal statistical inference over pairwise correlated random variables, we propose to regard the graph as a Markov Random Field (MRF), each node as a random variable, and mimic BP to aggregate information for node label inference. Because BP is rooted in basic mathematical principles, this approach is widely generalizable. Algorithmic adaptivity across different TAGs hinges on estimating the coupling coefficients in the graphs, which can be done by having LLMs analyze the attributes of sampled pairs of connected nodes. Moreover, this BP-inspired approach naturally adapts to varying levels of text attribute quality: nodes with higher-quality text attributes present greater influence on their neighbors, and vice versa.


By applying the two principles outlined above, we propose our new strategy, \proj, for zero-shot inference over TAGs. \proj does not require any training or fine-tuning. We evaluate \proj on 11 real-world TAG datasets from various domains, including citation networks~\cite{mccallum2000automating, giles1998citeseer, sen2008collective}, e-commerce~\cite{ni2019justifying}, knowledge graphs~\cite{mernyei2020wiki}, and webpage networks~\cite{craven1998learning}, covering both homophilic and heterophilic graph structures. 

Experimental results demonstrate the effectiveness of \proj. Notably, our task-conditional embeddings (Principle I) improve performance by $8.10\%$ on average compared to the best LM-based encoders. In addition, our BP-inspired aggregation mechanism (Principle II) provides an extra $1.71\%$ performance gain with our embeddings, demonstrating strong generalization across both homophilic and heterophilic TAGs.  Our experiments also reveal that current methods aligning graph-aggregated embeddings to LLM token spaces significantly underperform approaches that simply use smaller LM encoders without even incorporating graph structures. This outcome indicates that the primary source of generalization in these methods is the smaller LM’s text embeddings, rather than LLM-based reasoning on embeddings. It reinforces our earlier argument that limited training data hinders effective alignment in this context, urging caution for future work considering this strategy. %, casting doubt on the viability of such strategies within the graph learning domain.

%structure utlization, leveraging LLM agents, exhibits strong generalization capabilities across both homophilic and heterophilic TAGs, providing an additional $3.15\%$ performance improvement on average when applied to our encoding method.

%Unfortunately, our experiments also show that the Embedding-Based Integration methods that align graph-aggregated embeddings with the LLM token space significantly underperform those that only adopt smaller LM encoders even without incorporating graph structures. This suggests that the primary, if not the sole, source of generalization in these approaches stems from the smaller LM-encoded text embeddings rather than the reasoning capabilities of LLMs over these embeddings, which matches our previous argument on the lack of training data for achieving such alignment and poses a concern to this strategy in graph learning field. 

%Additionally, an intriguing phenomenon emerges from the experiments: for approaches that employ a projector to align latent graph embeddings with the LLM token space and utilize LLMs for reasoning~\cite{chen2024llaga, wang2024llms, tang2024graphgpt}, we find that merely using pre-trained LM encoders even without incorporating graph structures, significantly outperform these methods. This suggests that the primary, if not the sole, source of generalization in these approaches stems from the generalizable node representations rather than the reasoning capabilities of LLMs over latent graph embeddings.
%\pany{This work strongly rely on empirical performance, I suggest not to just say achieving SOTA or outperform baselines. Instead, argue how much gain can be brought by each technique: Conditional node text encoding? BP-based graph structure usage? }.  Meanwhile, our unified structure aggregation, . \hy{critic LLaGA or not here?} \pany{The current last paragraph is too high-level. Yes, indeed, we should argue against the approach that adopts alignment.}

%\pany{The last paragraph, still needs you to further work on it.}
%Interestingly, our study uncovers that relying solely on LLMs for reasoning doesn't necessarily enhance generalization. Instead, using text embeddings alone—such as those from Sentence-BERT~\cite{reimers2019sentence}—already delivers exceptional performance. These findings underscore the importance of understanding the principles that support the generalization of TAG models.


