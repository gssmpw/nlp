\section{More Related Works}
\label{sec:app_more_related_works}
\textbf{LLMs for Data Augmentation} annotate pseudo-labels via their advanced zero-shot text classification performance. \textit{E.g.}, LLM-GNN~\cite{chen2023label}, Cella~\cite{zhang2024cost} and ~\cite{hu2024low} propose heuristics to actively select and annotate pseudo-labels for supervised GNN training. ~\cite{pan2024distilling} performs knowledge distillation with LLMs as teachers. ~\cite{yu2023empower,li2024enhancing} generate synthetic node text with LLMs. The performance of these methods depend on the capability of LLM, and may still require relatively high annotating and training cost.

\textbf{LLMs for Graph Property Reasoning} focus on reason graph structure properties (e.g., shortest path, node degree, etc)~\cite{tang2024grapharena, dai2024large, yuan2024gracore, ouyang2024gundam}. Representative works include~\cite{perozzi2024let, chen2024graphwiz, zhang2024can, cao2024graphinsight, wei2024gita}.

\textbf{Tuning LMs/GNNs towards Better Task-Specific Performance} aims to push the limits of task-specific performance on TAGs other than generalization. These methods develop novel techniques to optimize LMs or GNNs for pushing the limits of in-domain performance~\cite{chien2021node, duan2023simteg, he2023harnessing, zhao2022learning, zhu2021textgnn, li2021adsgnn, yang2021graphformers, bi2021leveraging, pang2022improving, zolnai2024stage, yang2021graphformers}.

\textbf{Text embeddings} Generating unified text embeddings is a critical research area with broad applications, including web search and question answering. Numerous text encoders~\cite{reimers2019sentence, liu2019roberta, song2020mpnet} based on pre-trained language models have served as the foundation for various embedding models. Recently, decoder-only LLMs have been widely adopted for text embedding tasks~\cite{li2023towards, moreira2024nv} achieving remarkable performance on the Massive Text Embedding Benchmark (MTEB)~\cite{muennighoff2022mteb}. This progress stems from LLM2Vec~\cite{behnamghader2024llm2vec}, which introduces a novel unsupervised approach to transforming decoder-only LLMs into embedding models, including modifications to enable bidirectional attention. Recent findings~\cite{li2024making} suggest that retaining the unidirectional attention mechanism enhances LLM2Vecâ€™s empirical performance.



\section{Experiment Details}

\subsection{Dataset Details}
\label{sec:app_datasets}
\textbf{Meta-Data}
In Table.~\ref{tab:dataset_meta_data}, we show the meta-data of all the eleven datasets used in our experiments.

\input{Arxiv/tables/dataset_meta_data}


\textbf{Dataset Split} For the datasets (all the homophily graphs) that have been used for study in TSGFM~\cite{chen2024text}, we follow their implementation to perform data pre-processing, obtain raw texts and do data split, the introduction to data source can be found at Appendix.D.2 in their original paper, the code can be found at the link \footnote{\url{https://github.com/CurryTang/TSGFM/tree/master?tab=readme-ov-file}}.

As to the heterophily graphs, the four datasets are originally from~\cite{craven1998learning}. We obtain the raw texts from~\cite{yan2023comprehensive}, which can be found from\footnote{\url{https://github.com/sktsherlock/TAG-Benchmark/tree/master}}. As to data split, for zero-shot inference, all the nodes are marked as test data; for few-shot setting, $k$ labeled nodes are randomly sampled per class and the rests are marked as test data.
To the best of our knowledge, the four heterophily graph datasets used in this study are the only graphs that provide raw texts feature.





\subsection{LLM-BP Implementation Details}
\label{sec:app_llm_bp_implementation_details}
\textbf{Infrastructure and Seeds}
All the local experiments run on a server with AMD EPYC 7763 64-Core Processor and eight NVIDIA RTX 6000 Ada GPU cards, methods are mainly implemented with PyTorch~\cite{paszke2019pytorch}, Torch-Geometric~\cite{fey2019fast} and Huggingface Transformers~\cite{wolf2019huggingface}.
To obtain the embeddings, all the encoders that run locally on the server without API calling in this study run with the random seed $42$.


\textbf{Class Embedding}

$\bullet$ \textbf{Zero-Shot Setting:} We uniformly randomly sample $20 c$ nodes per graph, where $c$ denotes the number of classes, we employ GPT-4o~\cite{hurst2024gpt} to infer their labels. With the predictions from LLMs, the sampled nodes form distinct clusters. For each cluster, we take the top-$k$ (10 in the experiments) nodes whose embedding are closest with the cluster center and calculate their average embedding as the class embedding.

We notice that some works directly feed text descriptions into encoders as class embeddings~\cite{yang2024gl, chen2024text}, we find that different encoders can be highly sensitive to variations in text description. Therefore, we adopt the above method to ensure fairness among different encoders.


$\bullet$\textbf{Few-Shot Setting:} We directly take the class embedding as the averaged embeddings of labeled nodes per class.


\textbf{The Task-Adaptive Encoder:} We directly adopt the pre-trained LLM2Vec encoder released by~\cite{li2024making}, which is based on Mistral7B-v0.1~\cite{jiang2023mistral}. We check the pre-training data used in the original paper for aligning LLM decoders with the embedding space, the datasets are mainly for text-retrieval and therefore do not overlap with the TAG datasets adopted in our study. For detailed introduction of the datasets for LLM2Vec pre-training, see Section 4.1 training data in the original paper.
\label{sec:app_task_adaptive_implementation}
The task-adaptive prompting follows the format as:

\textit{
\text{$\langle$ Instruct $\rangle$}\\
``Given the \text{\{task description\}}, classify it into one of the following $k$ classes: \\
\text{\{class labels\}}\\
\text{$\langle$ query$\rangle$}\\
\text{\{raw node texts\}}.''\\
\text{$\langle$ response $\rangle$}\\}


, where the \{task descriptions\} prompts for each dataset is the same as that used for vanilla LLMs, see Table.~\ref{tab:vanilla_llm_task_description} for details. 


\textbf{Hyper-Parameters for BP algorithm}
For LLM-BP, we adopt $5$ message-passing layers, for its linear approximation form, we use a single layer.
The temperature hyper-parameter $\tau$ in computing node potential initialization in Eq.~\eqref{eq:node_potential} is set as $0.025$ for LLM-BP and $0.01$ for LLM-BP (appr.) across all the datasets. Attached in Table.~\ref{tab:pred_h} is the homophily ratio $r$ we used (predicted by GPT-4o-mini~\cite{hurst2024gpt}.
\input{Arxiv/tables/pred_h_numeric}



\subsection{Baseline Implementation Details}
\label{sec:app_implementation_details_baseline}
$\bullet$ \textbf{Vanilla Encoders} Vanilla encoders like SBert~\cite{reimers2019sentence}, Roberta~\cite{liu2019roberta} and text-embedding-3-large~\cite{openai2024textembedding} directly encode the raw text of the nodes. LLM2Vec uses the prompts:

\begin{align}
\label{eq:vanilla_llm2vec}
\resizebox{0.43 \textwidth}{!}{$\langle\text{Instruct}\rangle \{\text{task\_description}\} \langle\text{query}\rangle {X_i} \langle\text{response}\rangle$}.
\end{align}, where the $\{\text{task\_description}\}$ for each dataset is provided in Appendix.~\ref{sec:app_prompt_llm2vec_task_description}.

$\bullet$ \textbf{Vanilla LLMs} Prompts for GPT-4o and GPT-3.5-turbo adopts the format as follows:

\label{sec:app_vanilla_LLM_implementation}
\textit{
``role'': ``system''\\
``content'': ``You are a chatbot who is an expert in text classification''\\
``role'': ``user''\\
``content'': ``We have \text{\{task description\}} from the following $k$ categories: \text{\{class labels\}}\\
The text is as follows:\\
\text{\{raw node text\}}\\
Please tell which category the text belongs to:''
}


The \{task description\} for the vanilla LLMs for each class is provided in Appendix.~\ref{sec:app_prompt_vanilla_llm}.


$\bullet$ \textbf{Tuning LM/GNNs}
We adopt the pre-trained UniGLM~\cite{fang2024uniglm} released by the official implementation, which adopts Bert as the encoder, for direct inference.
For ZeroG~\cite{li2024zerog}, we re-implement the method and train it on ogbn-arxiv~\cite{hu2020open} for fair comparison with other baselines.

As to GNNs tuning methods, we pre-train GraphMAE~\cite{hou2022graphmae} and DGI~\cite{velivckovic2018deep} on ogbn-arxiv~\cite{hu2020open}, where the input for both models are from SBert~\cite{reimers2019sentence}, and we follow in implementation in TSGFM~\cite{chen2024text} benchmark.

$\bullet$ \textbf{Multi-Task GFMs} OFA~\cite{liu2023one} is trained on ogbn-arxiv~\cite{hu2020open}. As to GOFA, we directly adopt the model after pre-training~\cite{hu2021ogb, ding2023enhancing} and instruct fine-tuning on ogbn-arxiv~\cite{hu2020open} provided by the authors due to the huge pre-training cost, the zero-shot inference scheme also follows their original implementation.


$\bullet$ \textbf{LLMs with Graph Adapters} Both LLaGA~\cite{chen2024llaga} and GraphGPT~\cite{tang2024graphgpt} are trained on ogbn-arxiv~\cite{hu2020open}, we follow the hyper-parameter setting in their original implementation.


\section{Detailed Derivations}
\label{sec:app_detailed_derivation}
% \subsection{Derivation for Eq.~\eqref{eq:message_passing}}
% Here we present the derivation towards the message passing form in Eq.~\ref{eq:message_passing}. In node classification task, given node $i$, the objective is to minimize the mean-square error (MSE) of the node label prediction:
% \begin{align}
%     \min \text{MSE} (\hat{y_i}) = \mathbb{E}[(y_y - \hat{y_i})^2 | \XX],
% \end{align}, with the optimal solution $\hat{y_i}$ as:
% \begin{align}
%     \hat{y_i} = \sum_{y_i} y_i p(y_i |X),
% \end{align}where $p(y_i |X) = \sum_{Y \setminus i} \mathbb{P}(Y | X)$ is the posterior marginal. 
% Given the factorized form of the posterior distribution under the Markov Random Field (MRF) modeling $\mathbb{P}_\mathcal{G}(Y \mid X) \propto \prod_{i \in \mathcal{V}} \varphi_{X_i}(y_i) 
% \prod_{(i,j) \in \mathcal{E}} \psi_{ij}(y_i, y_j)$, where $\varphi_{X_i}(y_i) = \varphi_{y_i}(X_i)\phi_i(y_i)$, one can approximate $p(y_i |X)$ by running the loopy belief propagation:

% \begin{align}
%     m_{j\rightarrow i}^{(k)}(y_i) \cong \sum_{y_j} \psi_{ij}(y_i, y_j) \frac{p_j^{(k-1)}(y_j)}{m_{i\rightarrow j}^{(k-1)}(y_j)} \\
%     p_i^{(k)}(y_i) \approx p_i^{(0)}(y_i) \prod_{j \in \mathcal{N}(i)} m_{j\rightarrow i}^{(t)}(y_i),
% \end{align}$m^{(0)}_{i \rightarrow j}$ is initialized as $1 / c$ for all $i,j \in \mathcal{V}$.

% Considering numerical stability, the belief-propagation is updated in log-space, which leads to the message passing formulation in Eq.~\eqref{eq:message_passing}:
% \begin{align}
%  \log m_{j \to i}^{(k)}(y_i) \cong &\, \text{LSE}_{y_j}[\log \psi_{ij}(y_i,y_j) + \\ &
% \log p_j^{(k-1)}(y_j) - \log m_{i \to j}^{(k-1)}(y_j)], \nonumber \\
%  \log p_i^{(k)}(y_i) \cong &\log p_i^{(0)}(y_i) + \sum_{j \in \mathcal{N}(i)} \log m_{j \to i}^{(k)}(y_i),  \nonumber
% \end{align}where LSE stands for the log-sum-exp function.

% \subsection{Derivation for Eq.~\eqref{eq:message_passing}}
% In a node classification task, given node \(i\), we aim to minimize the mean-square error (MSE) of the node label prediction under observations \(\XX\):
% \begin{align}
%     \min \mathrm{MSE}\bigl(\hat{y}_i\bigr) 
%     \;=\; 
%     \mathbb{E}\Bigl[\bigl(y_i - \hat{y}_i\bigr)^2 \,\Bigm|\;\XX\Bigr].
% \end{align}
% The optimal solution \(\hat{y}_i\) is then
% \begin{align}
%     \hat{y}_i 
%     \;=\; 
%     \sum_{y_i}\,y_i\,p\bigl(y_i \,\bigm|\;\XX\bigr),
% \end{align}
% where \(p\bigl(y_i \,\bigm|\;\XX\bigr) = \sum_{Y \setminus i}\,\mathbb{P}\bigl(Y \,\bigm|\;\XX\bigr)\) is the posterior marginal.

% Under a Markov Random Field (MRF) assumption with graph \(\mathcal{G}=(\mathcal{V},\mathcal{E})\), the posterior factors as:
% \begin{align}
%     \mathbb{P}_\mathcal{G}(Y \mid \XX) 
%     \;\propto\; 
%     \prod_{i \in \mathcal{V}}\varphi_{X_i}\bigl(y_i\bigr)
%     \,\prod_{(i,j)\in \mathcal{E}}\psi_{ij}\bigl(y_i,y_j\bigr),
% \end{align}
% where \(\varphi_{X_i}\bigl(y_i\bigr)=\varphi_{y_i}\bigl(X_i\bigr)\phi_i\bigl(y_i\bigr)\).  
% To approximate each \(p(y_i\mid \XX)\), one can run \emph{loopy belief propagation} (LBP).  
% Let
% \begin{align}
%     p_j^{(k-1)}\bigl(y_j\bigr) 
%     \;=\; 
%     p_j^{(0)}\bigl(y_j\bigr)\,
%     \prod_{\ell\in \mathcal{N}(j)} m_{\ell \to j}^{(k-1)}\bigl(y_j\bigr),
%     \label{eq:p_j_definition}
% \end{align}
% where \(p_j^{(0)}\bigl(y_j\bigr)\) is the initial node potential, and \(m_{\ell\to j}^{(k-1)}\) are incoming messages from neighbors \(\ell\in \mathcal{N}(j)\).  
% Then the new message from node \(j\) to node \(i\) at iteration \(k\) takes the form
% \begin{align}
%     m_{j \to i}^{(k)}\bigl(y_i\bigr)
%     \;=\; 
%     \sum_{y_j}\,\psi_{ij}\bigl(y_i,y_j\bigr)\,
%     \frac{p_j^{(k-1)}\bigl(y_j\bigr)}{m_{i \to j}^{(k-1)}\bigl(y_j\bigr)},
%     \label{eq:message_update}
% \end{align}
% where dividing by \(m_{i \to j}^{(k-1)}\bigl(y_j\bigr)\) removes the previous contribution from node \(i\) to \(j\) (which is already included in \(p_j^{(k-1)}(y_j)\)) to avoid double counting.  
% Once all messages are updated, the \emph{node belief} can be written as
% \begin{align}
%     p_i^{(k)}\bigl(y_i\bigr) 
%     \;=\; 
%     p_i^{(0)}\bigl(y_i\bigr)\,
%     \prod_{j \in \mathcal{N}(i)} m_{j \to i}^{(k)}\bigl(y_i\bigr).
% \end{align}
% For numerical stability, we apply the same process in log-space (i.e., \(\log\!\sum\exp\) rather than \(\sum\) and products), yielding the message passing equations in Eq.~\eqref{eq:message_passing}.

% \begin{align}
%     \log m_{j \to i}^{(k)}\bigl(y_i\bigr)
%     &\;\cong\;
%     \mathrm{LSE}_{y_j}
%     \Bigl[\,
%         \log \psi_{ij}\bigl(y_i,y_j\bigr)
%         \;+\;
%         \log p_j^{(k-1)}\bigl(y_j\bigr)
%         \;-\;
%         \log m_{i \to j}^{(k-1)}\bigl(y_j\bigr)
%     \Bigr],
%     \nonumber\\
%     \log p_i^{(k)}\bigl(y_i\bigr)
%     &\;\cong\;
%     \log p_i^{(0)}\bigl(y_i\bigr)
%     \;+\;
%     \sum_{j\in \mathcal{N}(i)}
%     \log m_{j \to i}^{(k)}\bigl(y_i\bigr),
%     \label{eq:message_passing_log}
% \end{align}
% where \(\mathrm{LSE}\{\cdot\}\) denotes the log-sum-exp operator, and \(m_{i\to j}^{(0)}\) is initialized as a constant (e.g., \(1/c\)).
\subsection{Derivation for Eq.~\eqref{eq:message_passing}}
% In a node classification task, given node \(i\), we aim to minimize the mean-square error (MSE) of the node label prediction under observations \(\XX\):
% \begin{align}
%     \min \mathrm{MSE}\bigl(\hat{y}_i\bigr) 
%     \;=\; 
%     \mathbb{E}\Bigl[\bigl(y_i - \hat{y}_i\bigr)^2 \,\Bigm|\;\XX\Bigr].
% \end{align}
% The optimal solution \(\hat{y}_i\) then follows from
% \begin{align}
%     \hat{y}_i 
%     \;=\; 
%     \sum_{y_i}\,y_i\,p\bigl(y_i \,\bigm|\;\XX\bigr),
% \end{align}
% where \(p\bigl(y_i \,\bigm|\;\XX\bigr) = \sum_{Y\setminus i}\,\mathbb{P}\bigl(Y \,\bigm|\;\XX\bigr)\) is the posterior marginal.

% \paragraph{Factorized Posterior under MRF.}
% Assume a Markov Random Field (MRF) with graph \(\mathcal{G}=(\mathcal{V},\mathcal{E})\).  Then the posterior distribution factors as
% \begin{align}
%     \mathbb{P}_\mathcal{G}(Y \mid \XX) 
%     \;\propto\; 
%     \prod_{i \in \mathcal{V}}\varphi_{X_i}\bigl(y_i\bigr)
%     \,\prod_{(i,j)\in \mathcal{E}}\psi_{ij}\bigl(y_i,y_j\bigr),
% \end{align}
% where \(\varphi_{X_i}\bigl(y_i\bigr)=\varphi_{y_i}\bigl(X_i\bigr)\phi_i\bigl(y_i\bigr)\).  
% To approximate each \(p\bigl(y_i\mid \XX\bigr)\), one can employ \emph{loopy belief propagation} (LBP).  

% \paragraph{Node Belief and Messages.}
% Define the (intermediate) node belief at iteration \((k-1)\) as
% \begin{align}
%     p_j^{(k-1)}\bigl(y_j\bigr) 
%     \;=\; 
%     p_j^{(0)}\bigl(y_j\bigr)\,
%     \prod_{\ell\in \mathcal{N}(j)} m_{\ell \to j}^{(k-1)}\bigl(y_j\bigr),
%     \label{eq:p_j_definition}
% \end{align}
% where \(p_j^{(0)}\bigl(y_j\bigr)\) is the initial potential for node \(j\), and \(m_{\ell\to j}^{(k-1)}\) denotes the incoming message from neighbor \(\ell\).  
% The new message from node \(j\) to node \(i\) at iteration \(k\) then reads
% \begin{align}
%     m_{j \to i}^{(k)}\bigl(y_i\bigr)
%     \;=\; 
%     \sum_{y_j}\,\psi_{ij}\bigl(y_i,y_j\bigr)\,
%     \frac{p_j^{(k-1)}\bigl(y_j\bigr)}{m_{i \to j}^{(k-1)}\bigl(y_j\bigr)},
%     \label{eq:message_update}
% \end{align}
% where dividing by \(m_{i \to j}^{(k-1)}\bigl(y_j\bigr)\) cancels out the old contribution from \(i\) to \(j\) (already included in \(p_j^{(k-1)}\bigl(y_j\bigr)\)), thus preventing double-counting.

% \paragraph{Recovering the Reduced Product.}
% Meanwhile, the node belief for node \(i\) at iteration \(k\) is
% \begin{align}
%     p_i^{(k)}\bigl(y_i\bigr) 
%     \;=\; 
%     p_i^{(0)}\bigl(y_i\bigr)\,
%     \prod_{\ell \in \mathcal{N}(i)} m_{\ell \to i}^{(k)}\bigl(y_i\bigr).
%     \label{eq:node_belief}
% \end{align}
% From \eqref{eq:node_belief}, one observes
% \begin{align}
%     \varphi_{X_i}\bigl(y_i\bigr) 
%     \,\prod_{\ell \in \mathcal{N}(i)\setminus \{j\}} m_{\ell \to i}^{(k)}\bigl(y_i\bigr)
%     \;=\;
%     \frac{
%       \varphi_{X_i}\bigl(y_i\bigr)
%       \,\prod_{\ell \in \mathcal{N}(i)} m_{\ell \to i}^{(k)}\bigl(y_i\bigr)
%     }{
%       m_{j \to i}^{(k)}\bigl(y_i\bigr)
%     }
%     \;=\;
%     \frac{
%       p_i^{(k)}\bigl(y_i\bigr)
%     }{
%       m_{j \to i}^{(k)}\bigl(y_i\bigr)
%     },
%     \label{eq:factor_rewrite}
% \end{align}
% which is precisely the reduced product over all neighbors except \(j\).

% \paragraph{Log-Space Update.}
% For numerical stability, we often take the logarithm of both sides in \eqref{eq:message_update} and \eqref{eq:node_belief}, i.e., using \(\log\sum\exp(\cdot)\) rather than direct sums and products. This procedure yields the message passing equations in Eq.~\eqref{eq:message_passing}, typically written as:
% \begin{align}
%     \log m_{j \to i}^{(k)}\bigl(y_i\bigr)
%     &\;\cong\;
%     \mathrm{LSE}_{y_j}
%     \Bigl[\,
%         \log \psi_{ij}\bigl(y_i,y_j\bigr)
%         \;+\;
%         \log p_j^{(k-1)}\bigl(y_j\bigr)
%         \;-\;
%         \log m_{i \to j}^{(k-1)}\bigl(y_j\bigr)
%     \Bigr],
%     \nonumber\\
%     \log p_i^{(k)}\bigl(y_i\bigr)
%     &\;\cong\;
%     \log p_i^{(0)}\bigl(y_i\bigr)
%     \;+\;
%     \sum_{j\in \mathcal{N}(i)}
%     \log m_{j \to i}^{(k)}\bigl(y_i\bigr),
%     \label{eq:message_passing_log}
% \end{align}
% where \(\mathrm{LSE}\{\cdot\} \equiv \log \sum \exp(\cdot)\) and \(m_{i\to j}^{(0)}\) is initialized as a constant (e.g., \(1/c\)). By iterating these updates, LBP approximates each nodeâ€™s posterior marginal \(p(y_i\mid \XX)\), which in turn provides an estimate \(\hat{y}_i = \sum_{y_i} y_i \, p_i^{(k)}(y_i)\) for the MMSE criterion.
In a node classification task, given a node \(i\), our goal is to minimize the mean-square error (MSE) in predicting the node label under the observations \(\XX\):
\begin{align}
    \min \mathrm{MSE}\bigl(\hat{y}_i\bigr) 
    \;=\; 
    \mathbb{E}\Bigl[\bigl(y_i - \hat{y}_i\bigr)^2 \,\Bigm|\;\XX\Bigr].
\end{align}
The optimal solution \(\hat{y}_i\) is then given by:
\begin{align}
    \hat{y}_i 
    \;=\; 
    \sum_{y_i} y_i \, p\bigl(y_i \mid \XX\bigr),
\end{align}
where the posterior marginal \(p\bigl(y_i \mid \XX\bigr)\) is computed as:
\begin{align}
    p\bigl(y_i \mid \XX\bigr) = \sum_{Y \setminus i} \mathbb{P}\bigl(Y \mid \XX\bigr).
\end{align}

\paragraph{Factorized Posterior under an MRF.}
Assuming a Markov Random Field (MRF) over a graph \(\mathcal{G}=(\mathcal{V}, \mathcal{E})\), the posterior distribution factors as:
\begin{align}
    \mathbb{P}_\mathcal{G}(Y \mid \XX) 
    \;\propto\; 
    \prod_{i \in \mathcal{V}} \varphi_{X_i}\bigl(y_i\bigr)
    \prod_{(i,j) \in \mathcal{E}} \psi_{ij}\bigl(y_i, y_j\bigr),
\end{align}
where the node potential is defined as \(\varphi_{X_i}\bigl(y_i\bigr) = \varphi_{y_i}\bigl(X_i\bigr)\phi_i\bigl(y_i\bigr)\).

\paragraph{General Message-Passing Framework.}
To compute the marginal \(p(y_i \mid \XX)\), the loopy belief propagation (LBP) algorithm iteratively updates messages between nodes. The general message update rule from node \(i\) to node \(j\) at iteration \(k\) is:
\begin{align}
    m_{i \to j}^{(k)}\bigl(y_j\bigr) 
    \;=\; 
    \alpha_{i \to j} \sum_{y_i} \Bigg[\varphi_{X_i}\bigl(y_i\bigr) \psi_{ij}\bigl(y_i, y_j\bigr)
    \prod_{\ell \in \mathcal{N}(i) \setminus j} m_{\ell \to i}^{(k-1)}\bigl(y_i\bigr)\Bigg],
    \label{eq:general_message}
\end{align}
where \(\alpha_{i \to j}\) is a normalization constant ensuring the message sums to 1.

\paragraph{Node Belief Updates.}
The node belief \(p_i^{(k)}\bigl(y_i\bigr)\) at iteration \(k\) is obtained by combining the node potential with incoming messages from all neighbors:
\begin{align}
    p_i^{(k)}\bigl(y_i\bigr) 
    \;=\; 
    \varphi_{X_i}\bigl(y_i\bigr) 
    \prod_{\ell \in \mathcal{N}(i)} m_{\ell \to i}^{(k)}\bigl(y_i\bigr).
    \label{eq:node_belief_update}
\end{align}

\paragraph{Reformulating the Messages.}
Substituting Eq.~\eqref{eq:node_belief_update} into Eq.~\eqref{eq:general_message} simplifies the message-passing equation. The message from node \(i\) to node \(j\) at iteration \(k\) can be rewritten as:
\begin{align}
    m_{i \to j}^{(k)}\bigl(y_j\bigr) 
    \;=\; 
    \alpha_{i \to j} \sum_{y_i} \psi_{ij}\bigl(y_i, y_j\bigr) 
    \frac{p_i^{(k)}\bigl(y_i\bigr)}{m_{j \to i}^{(k-1)}\bigl(y_i\bigr)}.
    \label{eq:message_reformulation}
\end{align}
This reformulation prevents double-counting the contribution of node \(j\) to node \(i\) in the previous iteration.

\paragraph{Log-Space Stability.}
To avoid numerical underflow, the log-space version of the message update is commonly used:
\begin{align}
    \log m_{i \to j}^{(k)}\bigl(y_j\bigr) 
    &\;=\; 
    \mathrm{LSE}_{y_i} \Bigl[\log \psi_{ij}\bigl(y_i, y_j\bigr) + \log p_i^{(k)}\bigl(y_i\bigr) - \log m_{j \to i}^{(k-1)}\bigl(y_i\bigr)\Bigr],
\end{align}
where \(\mathrm{LSE}(\cdot) \equiv \log \sum \exp(\cdot)\).

\paragraph{Summary.}
By iteratively applying these message updates and node belief calculations, LBP provides an approximation for the posterior marginal \(p(y_i \mid \XX)\). The final prediction \(\hat{y}_i\) under the MMSE criterion is:
\begin{align}
    \hat{y}_i 
    \;=\; 
    \sum_{y_i} y_i \, p_i^{(k)}\bigl(y_i\bigr).
\end{align}
This completes the derivation of the message-passing update in Eq.~\eqref{eq:message_passing}.



\section{More Experiment Results}
\label{sec:app_more_experiment_results}


\subsection{Significance Test of Effectiveness of Task-Adaptive Encoding}
\input{Arxiv/tables/confidence}

\label{sec:app_confidence}

We conduct significance test on the improvment of task-adaptive encoding over vanilla LLm2Vec~\cite{li2024making} and Text-Embedding-3-Large~\cite{openai2024textembedding} under the zero-shot setting, with results shown in Table.~\ref{tab:confidence}. We replicate experiment for $100$ times with random seeds from $42$ to $141$ and obtain classification accuracy of each method. To check normality, we first apply Shapiro-Wilk test~\cite{SHAPIRO1965}. If the data follows a normal distribution, we perform a Paired-t test~\cite{student1908probable}; otherwise, we use Wilcoxon Signed-Rank test~\cite{wilcoxon1992individual}, with packages from SciPy~\cite{2020SciPy-NMeth}.
The lower and upper bounds under $90\%$ confidence interval are estimated with bootstrap algorithm~\cite{tibshirani1993introduction} to sample $10,000$ times.
Task-adaptive encoding show statistically significant improvement over vanilla LLM2Vec in $9$ out of $11$ dataset and outperforms Text-Embedding-3-Large in $8$ out of $11$ datasets (bolded in the table).

\subsection{LLM Agents' Prediction on Homophily Ratio $r$}
\input{Arxiv/figures/pred_h_more}
More prediction performance of GPT-4o, GPT-3.5-turbo and Mistral7b-Instruct-v3 are shown in Fig.~\ref{fig:predict_h_more}.

%\subsection{Effectiveness of Class Information on Text-Embedding-3-Large}
%The effectiveness of class information on text-embedding-3-large is shown in Fig.~\ref{fig:class_condition_text-embedding-3-large}.
%\input{Arxiv/figures/class_condition_text-embedding-3-large}

\subsection{Zero-Shot Comparison with LLM-GNN~\cite{chen2023label} and TEA-GLM~\cite{wang2024llms}}
\label{sec:app_more_baselines}
\input{Arxiv/tables/compare_llm_gnn}
\input{Arxiv/tables/compare_tea_glm}
Here we present the comparison with LLM-GNN~\cite{chen2023label} in Table.~\ref{tab:compare_llm_gnn}. We compare with three different graph active learning heuristics from their original paper. Our training-free methods, LLM-BP and LLM-BP (appr.) achieves top performance on Citeseer and Wikics, while performs comparably with the baselines in Cora and Pubmed. Note that the results of LLM-GNN are from Table. 2 in the original paper.


The comparison with TEA-GLM is shown in Table.~\ref{tab:compare_tea-glm}. Results of TEA-GLM are from Table.1 in their original paper.

\subsection{Experiment Results in Few-Shot Setting}
\label{sec:app_few_shot}
We use $10$ different random seeds from $42$ to $52$ to sample the $k$-shot labeled nodes from training dataset, and report the average accuracy and macro $F1$ score with standard variance. Results are shown in Table.~\ref{tab:few_shot}. Across all the $k$s, our LLM-BP achieves the top ranking performance across all the eleven datasets, exhibiting similar insights with the zero-shot setting.

\input{Arxiv/tables/few_shot}
\subsection{Zero-Shot Link Prediction Results}
\input{Arxiv/tables/link_pred}
\label{sec:app_link_prediction}

For each dataset, We randomly sample \& remove $1000$ edges and $1000$ node pairs from the graph as testing data. A straightforward approach is to compare the cosine similarity between node embeddings to determine the presence of a link. Specifically, we aggregate embeddings for $3$ layers on the incomplete graph and compute the cosine similarity between node representations, achieving better zero-shot performance than LLMs-with-Graph-Adapters methods~\cite{wang2024llms, chen2024llaga, tang2024graphgpt}, as shown in Table.~\ref{tab:link_prediction}. Note that the performance in the table refers to LLM-with-Graph-Adapters that have only been trained on other tasks and never on link prediction tasks.

We leave the design of task-adaptive embeddings and generalized graph structural utilization for link prediction as future work, including task-adaptive encoding prompts.







\section{Prompts}
\subsection{Task Description for Vanilla LLM2Vec without Class Information}
\label{sec:app_prompt_llm2vec_task_description}
Table.~\ref{tab:llm2vec_task_description} shows the task description for vanilla LLM2Vec encoder across all the datasets.
\input{Arxiv/tables/prompts/llm2vec_task_description}

\subsection{Prompts for Vanilla LLMs}
\label{sec:app_prompt_vanilla_llm}
Table.~\ref{tab:vanilla_llm_task_description} shows tha task description for vanilla LLM decoders.
\input{Arxiv/tables/prompts/vanilla_llm_decoder_task_description}

%\subsection{Class Description}
%\label{sec:app_class_description}

\newpage


%\input{Arxiv/tables/class_desc/Cora}

%\input{Arxiv/tables/class_desc/Citeseer}

%\input{Arxiv/tables/class_desc/Pubmed}
%\input{Arxiv/tables/class_desc/Sportsfit}
%\input{Arxiv/tables/class_desc/Cornell_Texas_Wisconsin_Washington}
%\input{Arxiv/tables/class_desc/bookhis}
%\input{Arxiv/tables/class_desc/bookchild}
%\input{Arxiv/tables/class_desc/Wikics}
