\section{Related Works}
%\pan{based on what I read: these works do not use llm, and also need model training?}
%\pan{Efforts to improve the model's zero-shot capability can be divided as follows}:
Here, we briefly review existing methods by examining how they enable model generalization across TAGs. 

\textbf{Tuning Smaller LM Encoders.} 
These methods typically rely on a source-domain graph for training. Notable works include ZeroG~\cite{li2024zerog} that tunes SBert~\cite{reimers2019sentence} on source datasets to align class-description embeddings with node text, thereby enhancing zero-shot performance on target datasets. Another approach, UniGLM~\cite{fang2024uniglm}, fine-tunes BERT~\cite{kenton2019bert} using contrastive learning on source datasets to yield a more generalizable encoder. GNNs trained with UniGLM embeddings in a supervised manner outperform models that directly adopt LM embeddings.

%These work node representation has recently attracted much research interest. 


\textbf{Training GNNs for Generalization.} These methods focus on leveraging graph structure in a generalizable manner. Among them, graph self-supervised learning~\cite{liu2022graph} is particularly common for producing representations without labeled data, often employing contrastive learning or masked modeling~\cite{velivckovic2018deep, hou2022graphmae, zhao2024all}. GraphMOE is a more recent technique inspired by the success of mixture-of-experts~\cite{shazeer2017outrageously}, pre-training parallel graph experts targeting different structures or domains~\cite{hou2024graphalign, liu2024one, xia2024anygraph, qin2023disentangled}. 
Others also consider LM-GNN co-training including \cite{he2024unigraph,zhu2024graphclip} that also follow a constrastive learning idea. %where \cite{he2024unigraph} proposes a novel structure-aware node text reconstruction scheme based on contrastive learning, and~\cite{zhu2024graphclip} introduces a contrastive graph-summary pretraining method combined with invariant learning. 
Note that, however, all these methods still require training.  %Recently a `Na\"{i}ve' method that simply aggregate the averaged neighbor embeddings exhibits good zero-shot performance on homophilic graphs~\cite{li2024glbench, chen2024text}. 
%\pany{this is too rough? can you elaborate on the technical aspects how they do so} 

%Works tuning small LM encoders or GNNs demonstrate promising zero-shot generalization capabilities. But their effectiveness can be heavily constrained by the limited complexity of the models. And insufficient training may even lead to negative transfer, particularly in cases of substantial node text domain shifts or graph structural transitions (e.g., from homophilic to heterophilic). 
In contrast to the above effort that adopts smaller LM encoders, works that involve the use of LLMs are reviewed in the following and may achieve better generalization. More related works including LLM-based data augmentations for GNN training for generalization and LLMs for other graph reasoning tasks can be found in Appendix.~\ref{sec:app_more_related_works}.

%\pany{why call llm as reasoners? Why not just finetuning (small) LM encoders; Train GNNs; }

%\pany{The name of each category should be properly given and match those later used in the exp. Also, it is unclear that the first two categories do not use llm and the rest use llms. This point should be clarified. moreover, how do you define/distinguish lm v.s. llm?}

\textbf{LLMs with Node-Text Input.} LLMs being directly fed with raw node texts demonstrates strong zero-shot ability on TAGs~\cite{chen2024exploring, huang2024can, li2024similarity}. However, they suffer from the limitation of not being able to incorporate graph structural information. % due to the limited context window size. That said, later we will show that, more capable LLMs, such as GPT-4o~\cite{hurst2024gpt}, serve as important baselines for zero-shot tasks on TAGs.

\textbf{LLMs with Graph-Embedding Input.} With smaller LM-encoded node embeddings, various strategies integrate graph structure by aggregating these embeddings, such as neighborhood-tree traversal or concatenating the averaged embeddings from different hops~\cite{chen2024llaga, tang2024graphgpt, luo2024enhance}, or via pre-trained GNNs~\cite{zhang2024graphtranslator, wang2024llms}. As mentioned earlier, these methods rely on aligning embeddings with the LLMs' token space. For instance, LLaGA~\cite{chen2024llaga} trains a simple MLP on citation networks and~\cite{wang2024llms} employs a linear projector on the ogbn-Arxiv~\cite{hu2020open} dataset, both using the next-token prediction loss, while \cite{tang2024graphgpt} adopts self-supervised structure-aware graph matching as the training objective. However, due to limited TAG-domain data, the space alignment in these methods often remains undertrained, leading to degraded performance. 

%LLMs are fed with latent graph representations instead of raw texts to integrate graph structures. These methods first adopt pre-trained LM encoders~\cite{reimers2019sentence} for node embeddings, and then encode graph structure either with handcrafted summarization (e.g., traversing the neighborhood tree or concatenating multi-hop embeddings)~\cite{chen2024llaga, tang2024graphgpt, luo2024enhance}, or with pre-trained GNNs~\cite{zhang2024graphtranslator, wang2024llms}. \pan{the sentence order should be shifted} 
%\hy{how to implement graph adapter}

%\pan{Given the intro has mentioned this, here, we just need to be brief. Perhaps, connect to the experiments: A projector is learned while just as the intro said, it suffers from xxx.} 
%A projector is always essential to align the graph representation space with the token space of LLMs, where LLaGA~\cite{chen2024llaga} trains a simple MLP on several citation networks, while ~\cite{wang2024llms} employs a linear projector trained on the ogbn-Arxiv~\cite{hu2020open} graph. Both methods formulate tasks on TAGs as next-token prediction tasks and use instruction fine-tuning to train the projector. Similarly, ~\cite{tang2024graphgpt} aligns a lightweight projector through self-supervised structure-aware graph matching, followed by instruction tuning.
%As mentioned in the introduction, existing methods train projectors on limited data, which significantly hinders their generalization ability. In addition, unlike CV~\cite{schuhmann2022laion}, currently there is even no large-scale and diverse graph data available for training such projectors with strong generalization ability, which further limits the effectiveness of these methods.

\textbf{Multi-Task Graph Foundation Models.} More ambitious studies aim to generalize across various graph-related tasks within a single framework. Notable approaches include graph prompting~\cite{liu2023graphprompt}, which introduces ``prompting nodes'' to transform diverse graph tasks into a unified format. These frameworks then train GNNs to address the tasks~\cite{li2024zerog, liu2023one, liu2024one} or further integrate LLMs~\cite{yang2024gl, kong2024gofa}. Although these works are impressive, they still fail to achieve zero-shot performance comparable to those methods that focus on specific graph data domains. %show promising generalization results and are selected as important baselines.\hy{don't know how to position}
% and training a unified model to solve them. 
%represent an interesting direction aimed at 
%\pany{I suggest to put some foundation model works in the main text, if space is allowed.}


%\shikun{not sure if this is a concern, but the related work section currently sounds like a longer and detailed version of intro paragraph 2}


%Supervised Fine-tuning LMs on a specific task achieves state-of-the-art performance through supervised fine-tuning. This demonstrates that the quality of node attributes can significantly influence the model's performance.
%\textbf{Task-Specific LM Fine-Tuning} via supervised learning demonstrates remarkable performance improvements~\cite{chien2021node,he2023harnessing,duan2023simteg,zhu2024efficient,zhu2024efficient,zhu2024efficient,zhao2022learning}.
