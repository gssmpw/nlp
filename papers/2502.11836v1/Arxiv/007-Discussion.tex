\section{Discussion and Limitations}
%Identifying the challenges in adopting LLMs to graphs, we propose two generalization principles for model generalization over TAG data. Following these principles, we introduce LLM-BP that integrates task-adaptive encoding for a unified node attribute space with belief propagation for generalized graph structural usage. Key algorithmic parameters are estimated using an LLM agent. Experimental results on node classification tasks demonstrate the effectiveness as well as generalizability of LLM-BP. 

Graph learning tasks often face substantial data constraints compared to other domains, underscoring the importance of establishing fundamental principles that foster model generalization. Our approach exemplifies this by leveraging LLMs to analyze graph data and determine suitable inference strategies, particularly via homophily estimation for belief propagation. While \proj achieves notable success on TAGs for node classification and extends partially to link prediction, it remains a step away from a fully comprehensive graph foundation model that addresses a wider range of graph learning tasks. Nonetheless, the core idea of leveraging LLM-driven graph analysis to guide algorithmic decisions aligned with task-specific inductive biases holds broad potential for future applications.

