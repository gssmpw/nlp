\begin{algorithm}[t]
\caption{LLM-BP}
\label{alg:llm_bp}
\begin{algorithmic}[1]
\INPUT TAG $(\mathcal{G}, \boldsymbol{X})$
\OUTPUT Class label prediction $\{\hat{y}_i\}_{i\in[n]}$
\STATE $\hh^{X}$ $\leftarrow$ Task-adaptive encoding following Eq.~\eqref{eq:task_adaptive_llm2vec}
\IF{zero-shot}
\STATE Sample $l \ll n$ nodes, infer labels with LLMs,
\STATE Nodes clusters based on LLM prediction,
\STATE $\mathbf{q}^{C}$ $\gets$ Average embedding of samples near center,
\ELSIF{few-shot}
\STATE $\mathbf{q}^{C}$ $\gets$ Average embedding of $k$ samples per class,
\ENDIF
\STATE Estimate $\psi_{ij}(y_i, y_j)$ by employing the LLM to analyze the graph data (e.g., using Eq.~\eqref{eq:bp} based on the estimated homophily level $r$.)
\STATE Initialize $p^{(0)}(y_i)$ $\gets$ Eq.~\eqref{eq:node_potential} and $m^{(0)}_{i \to j} (y_j) = 1$
\STATE Run LLM-BP (Eq.~\eqref{eq:message_passing}) for $L$ iterations or its approximation (Eq.~\eqref{eq:bp_appr}) for single iteration
% \IF{use \textbf{BP}}
% \STATE $\psi_{ij}(y_i, y_j)$ $\gets$ Eq.~\eqref{eq:bp}
% \ELSIF{use \textbf{BP (appr.)}}
% \STATE $\psi_{ij}(y_i, y_j)$  $\gets$ Eq.~\eqref{eq:bp_appr}
% \ENDIF
% \STATE Self-potential $p^{(0)}(y_i;x_i)$ $\gets$ Eq.~\eqref{eq:node_potential}.
% \STATE Initialize $m^{(0)}_{i \to j} (y_j) = 1$
% \STATE $\log p_i^{(k)}(y_i;x_i)$  $\gets$ Message passing in Eq.~\eqref{eq:message_passing} for  $k$ iterations.
\STATE $\hat{y_i}$ $\gets$ $\arg \max_{y_i} \log p_i^{(k)}(y_i;x_i)$

\end{algorithmic}
\end{algorithm}