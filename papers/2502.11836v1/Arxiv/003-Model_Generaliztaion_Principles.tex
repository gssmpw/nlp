
%\pan{introduce the principles for generalization: Principle 1, Principle 1+3 (method), Principle 1+3 (result); Principle 2, principle 2+3 (method), principle 1+2+3 result.  }

%\pan{Discuss and limitation.}

\section{Generalization Principles for \proj}
%Here we first provide the notation, followed by the three key principles for model generalization on TAG data. 
%\pany{why not put notation and problem formulation separately. Also, when only focusing on node classifcation, you should give the explanation again why only focusing on this task, more detailed than intro.}

\subsection{Notations and Problem Formulation} 

Let $(\GG, X, Y)$ represent a TAG of interest, where $\GG(\VV, \EE)$ denotes the graph structure, $\VV$ is the node set of size $n$, and $\EE$ is the edge set. The node textual attributes are represented as $X = \{X_1, ..., X_n\}$, and each node belongs to one of $c$ classes, with labels given by $Y=\{y_1,y_2,...,y_n\} \in [c]^n$. 

The objective is to infer the labels of nodes in TAGs based on the node attributes and graph structure. This study primarily focuses on the \textbf{zero-shot} setting, where no labeled data are assumed to be available in advance. Additionally, a \textbf{few-shot} setting is considered, where $k$ labeled nodes are known for each class. Due to space limitations, results for the few-shot setting are provided in Appendix~\ref{sec:app_few_shot}.  % are randomly sampled from the training dataset.
% by first mapping the natural language texts into latent embeddings, and further designing functions to incorporate the graph structure information into the latent node representations. 

\subsection{Motivation and the Overall Framework}
 LLMs are commonly used as decoders for next-token prediction. While LLMs excel at processing natural language inputs, they are not inherently compatible with graph data. Recently, some studies have explored methods to integrate graph data into LLMs, primarily for reasoning tasks~\cite{perozzi2024let, zhang2024can, tang2024grapharena}. 

In the context of TAGs, accurate node label inference relies on effectively combining the attributes of multiple nodes, especially when a node's individual attributes are insufficient to determine its label. However, as noted earlier, LLMs are constrained by limited context windows, making it challenging to process all attributes from the potentially large set of connected nodes. Traditional approaches to compressing graph structural information involve creating embeddings, such as using GNNs to aggregate information from the target node's neighbors. While effective, these embedding methods do not seamlessly integrate with LLM inputs and often require non-trivial training effort to align the LLMs' token space with the node embedding space~\cite{chen2024llaga, wang2024llms, tang2024graphgpt}. 

Our approach, \proj, does not confine LLMs to their traditional usage. We first leverage their capabilities to generate task-adaptive node embeddings. Then, instead of requiring LLMs to directly process these embeddings, LLMs are further employed to analyze graph data and provide generalizable guidance in aggregating these embeddings. These two steps are to match the two generalization principles proposed in Sec.~\ref{sec:intro}. Classification is ultimately performed by computing the cosine similarity between the final node embeddings $\mathbf{h}^X=[h_1^X,...,h_n^X]$ and candidate class embeddings $\mathbf{q}^C=[q_1^C,...,q_c^C]$. In the zero-shot setting, class embeddings are generated as follows: we randomly sample $l \ll n$ nodes and employ LLMs to infer their labels. The embeddings of sampled nodes form distinct clusters based on LLMs' prediction. We compute the average embedding of the embeddings closest to the cluster center to obtain the class embedding. 
In the few-shot setting, class embeddings are obtained by averaging the embeddings of labeled nodes within each class. See Appendix.~\ref{sec:app_llm_bp_implementation_details} for details.




% expect the LLMs to take all info to make the inference 


% obtained via e.g. graph neural n 


% their connects 
% %And node neighboring information in the graph structure determines  represents the correlation between these nodes. 

% Given LLMs suffer from either limited context window to digest multiple nodes and their connects 
% Our framework follows the d



% The key idea behind zero-shot inference is to compute the cosine similarity between the summarized node representations and the class label embeddings, with the latter being the embeddings of descriptions for the classes. For example, the description of \emph{``student''} class in the Cornell~\cite{craven1998learning} dataset is: \textit{``encompasses individuals actively enrolled in educational programs, ranging from undergraduate to graduate levels, across diverse disciplines. These individuals engage in academic activities, such as attending lectures, completing assignments, and participating in research or extracurricular projects, contributing significantly to the institution's learning environment.''} The class descriptions are represented by $\CC= \{C_1, ..., C_{c}\}$. A detailed description of each class for all the datasets used in the study is provided in Appendix.~\ref{sec:app_class_description}. 

% \textbf{Few-shot} is also considered in this study. We adopt the $c$-way-$k$-shot approach, where $k$ labeled nodes per class are randomly sampled from the training dataset. The average embedding of these nodes within each class is then used as the class label embedding, replacing the embeddings of class descriptions $\CC= \{C_1, ..., C_{c}\}$ used in the zero-shot setting.

% In the following sections, we present two key principles identified for designing a method for TAG generalization.

\subsection{Principle I: Task-Adaptive Node Embeddings $\mathbf{h}^X$}
%\pan{the contribution from LM encoder to LLM encoder is limited. } 

Creating generalizable text embeddings is no longer a significant challenge. Even smaller LM encoders, such as SBert~\cite{reimers2019sentence}, are capable of achieving this. Indeed, most existing works utilize these encoders to generate initial node embeddings for TAGs~\cite{chen2024text,chen2024llaga, tang2024graphgpt, wang2024llms}. However, for these embeddings to be directly usable for label prediction without the need for additional transformation models, it is crucial to incorporate task-specific information. In other words, the embeddings must be tailored to the specific task, resulting in what we term task-adaptive embeddings.

Achieving task adaptivity, however, presents a notable challenge. Smaller LM encoders lack the expressive power necessary to encode nuanced task-specific information. This limitation motivates our adoption of LLM-induced encoders, driven by the emergent capabilities of LLMs in contextual learning~\cite{sahoo2024systematic, chen2023unleashing}.

There have been recent advancements in extending LLMs to generate text embeddings~\cite{behnamghader2024llm2vec, muennighoff2022mteb}. In our approach, we utilize a form of LLM2Vec~\cite{behnamghader2024llm2vec}, which transforms LLM decoders into encoders via retaining the unidirectional attention mechanism. Following the methodology 
 in~\cite{li2024making}, we extract the output embedding of $\langle\text{response}\rangle$ - the token positioned immediately after the inputs - as the text embedding for the input node attributes.

To embed task-specific information into node embeddings, we propose a prompting strategy structured with the following template:
\begin{align}
\label{eq:task_adaptive_llm2vec}
\resizebox{0.43 \textwidth}{!}{$\langle\text{Instruct}\rangle \{\text{task\_description}\} \{\text{class\_info}\} \langle\text{query}\rangle {X_i} \langle\text{response}\rangle$}.
\end{align}

Here, $\langle \cdot \rangle$ encloses specific tokens. The task details are described in $\{\text{task\_description}\}$, and $\{\text{class\_info}\}$ contains the basic information of each class. An example is given in Fig.~\ref{fig:pipe}.
The class information serves as a crucial contextual enhancement, enabling LLMs to generate embeddings in a conditioned manner. %, based on their learned knowledge and understanding from the pre-training process. 
For more detailed on the class-conditional prompt for each dataset used in this study, refer to Appendix.~\ref{sec:app_llm_bp_implementation_details} and ~\ref{sec:app_prompt_llm2vec_task_description}.


% There has been much effort recently on extending LLMs to conducting effective text embeddings~\cite{}

% To generate embeddings, we following~\cite{li2024making} 
% convert LLM decoders into encoders by retaining their unidirectional attention mechanism and using the output embedding of the token $\langle\text{response}\rangle$, positioned immediately after the input tokens, as the vector representation of text.

% LLMs hold extensive power to understand text information associated with the TAG. However, the challenge is to incorporate the task information when to 

% The consistency of node attributes is the dominant factor in generalization on TAGs.
% As evidenced by the scaling laws~\cite{kaplan2020scaling} and the success of LLM2Vec~\cite{behnamghader2024llm2vec}, embeddings derived from larger LLM decoders pre-trained on large-scale, diverse datasets (e.g., Mistral-7B~\cite{jiang2023mistral}) exhibit inherently superior generalization~\cite{muennighoff2022mteb} compared to those from small LM encoders (e.g., SBert~\cite{reimers2019sentence}) with limited model complexity. Following~\cite{li2024making}, we
% %\pan{this is not that ``we propose''. We simply borrow..}
% convert LLM decoders into encoders by retaining their unidirectional attention mechanism and using the output embedding of the token $\langle\text{response}\rangle$, positioned immediately after the input tokens, as the vector representation of text.

% Furthermore, as illustrated by the toy example in Fig.~\ref{fig:pipe}, node embeddings should be task-adaptive, ensuring that their distance are conditioned on the classes. Motivated by LLMs' emergent capability in contextual learning~\cite{sahoo2024systematic, chen2023unleashing}, we introduce a novel prompting strategy for encoders converted from LLM decoders, class-conditional encoding, with the following template: 
% %\panq{why duplicate (1) and (2), just need one. }
% %For any text to encode, the template is as follows:
% %\begin{align}
% %\label{eq:llm2vec}
% %\resizebox{0.45 \textwidth}{!}{$\langle\text{Instruct}\rangle \{\text{task\_description}\}  \langle\text{query}\rangle \{q_i\} \langle\text{response}\rangle$},
% %\end{align}where $\langle \rangle$ contains the specific tokens. In $\{\text{task\_description}\}$, an example of encoding book descriptions on e-commerce networks could be: 
% %\pan{note that you have already considered to introduce conditional encoding, which means principle I and III should be merged}
% %\textit{``Encode the description or title of a book."}
% %The prompt is then followed by $q_i$ representing the raw text to be encoded. For details of the LLM encoder pre-training, instruction fine-tuning, see Appendix.~\ref{sec:app_implementation_details_llm2vec}.
% %Having established the unified text space with LLM2Vec encoder, we now introduce the technique that adapt this embedding space for specific tasks. In particular, we aim to generate class-conditioned embeddings from generic embedding space through prompting the encoder with downstream task label description.
% %\textbf{Principle I with Adaption to Node Classification:}
% \begin{align}
% \label{eq:task_adaptive_llm2vec}
% \resizebox{0.43 \textwidth}{!}{$\langle\text{Instruct}\rangle \{\text{task\_description}\} \{\text{class\_info}\} \langle\text{query}\rangle {X_i} \langle\text{response}\rangle$}.
% \end{align}Here, $\langle \rangle$ encloses specific tokens. The task details are described in $\{\text{task\_description}\}$, and $\{\text{class\_info}\}$ contains the basic information of each class. For instance, the prompt for encoding book titles or descriptions in the History~\cite{ni2019justifying} e-commerce network can be structured as:

% \textit{``Given the description or title of the book, classify it into one of the following 12 classes: \\
% World,  \\
% Americans, \\
% Military, \\
% ...,\\
% Arctic \& Antarctica."}

% The class information serves as a crucial contextual enhancement, enabling LLMs to generate embeddings in a conditioned manner, based on their learned knowledge and understanding from the pre-training process. For encoding class descriptions for zero-shot inference, the prompt for $\{\text{task\_description}\}$ is simply:
% ``\textit{Encode the text:}''. For detailed class-conditional prompting templates for each dataset used in this study, refer to Appendix.~\ref{sec:app_class_conditional_encoding_prompt}.


%For the text description for each class, one can still adopt the template in Eq.~\ref{eq:llm2vec}. 


%\shikun{maybe first add add an example to showcase the usage. Then after that, use some sentences describing the results. For instance, suppose we have the downstream tasks with ground truth label being the categories of products xxx, we prompt the LLM encoder with . As a result, we find the resulting embeddings align closer to the task context and become more distinguishable as shown in fig~\ref{fig:tsne}.}


%Each node $v_i$ can then be classified according to the maximum cosine similarity with the node embeddings: 
%\begin{align}
%\label{eq:cosine_classification}
%    \hat{y_i} = \arg \max_{i} cos(\XX^{\TT}_i, \XX^{\CC}_i).
%\end{align}
%\shikun{why here we want to mention how to classify the node embeddings}

%\rw{In this section, I outlined the general pairwise MRF formulation and deferred the discussion on leveraging LLMs to obtain node likelihoods to the next section, where the algorithms will be introduced. Also, I didn't cite Jia et al here, you can consider cite it where we introduce the assumption for edge coupling potentials.}

\subsection{Principle II: Generalizable Graph  Aggregation} \label{sec:principle2}
Graph structures can provide essential correlated information for node label inference by characterizing the relationships between node attributes and labels~\cite{zhu2003semi,kipf2016semi,velivckovic2017graph,hamilton2017inductive,zhu2020beyond,wei2022understanding}.
% \pan{add more citations, especially foundational works in this area}
 Specifically, we may consider each node's label and attributes as random variables, and each edge as a coupling between them for connected node pairs. The fundamental BP algorithm enables principled statistical inference over this set of correlated random variables~\cite{murphy2013loopy}. Since BP is inherently agnostic to the application domain of the TAG, emulating BP offers a mechanism to aggregate correlation information encoded in the graph structure across domains.

\textbf{Markov Random Field Modeling} We consider the joint probability distribution $\mathbb{P}_\mathcal{G}(Y, X)$ over the graph where $Y$ and $X$ denotes the random variables of node labels and attributes, respectively. %Given a graph $\mathcal{G}$ with node set $\mathcal{V}$ and edge set $\mathcal{E}$, 
In $\mathbb{P}_\mathcal{G}(Y, X)$, the distribution over the node labels given the graph structure is denoted as
\begin{align}
\mathbb{P}_\mathcal{G}(Y)
= \frac{1}{Z_{\mathbf{Y}}} \prod_{i \in \mathcal{V}} \phi_i(y_i) \prod_{(i,j) \in \mathcal{E}} \psi_{ij}(y_i, y_j).
\end{align}
Here $\phi_i(y_i)$ denotes the unary potential for node $i$, $\psi_{ij}(y_i, y_j)$ is the edge potential capturing the correlation between labels $y_i$ and $y_j$ of adjacent nodes, and $Z_{Y}$ is the normalization constant.  For node attributes, MRF modeling assumes that each node’s attributes are conditionally independent of others given the node labels, which can be characterized by the distribution:  
\begin{align}
\mathbb{P}_\mathcal{G}(X \mid Y) 
= \prod_{i \in \mathcal{V}} \mathbb{P}_\mathcal{G}(X_i \mid y_i) 
= \prod_{i \in \mathcal{V}} \varphi_{y_i}(X_i)
\end{align}
where $\varphi_{y_i}(X_i)$ captures the likelihood of having node $i$’s attributes $X_i$ given its label $y_i$. % to the probability of observing the corresponding feature $x_i$.

The proposed modeling approach is highly adaptive, as it can capture the varying graph connectivity patterns across different TAGs through interpretable edge potentials. For instance, $\psi_{ij}(y_i, y_j)$ represents the unnormalized likelihood that nodes with labels $y_i$ and $y_j$ are connected. This formulation naturally incorporates the modeling of graph homophily and heterophily: $\psi_{ij}(y_i, y_i) > \psi_{ij}(y_i, y_j)$ indicates homophily, while $\psi_{ij}(y_i, y_i) < \psi_{ij}(y_i, y_j)$ reflects heterophily. Furthermore, $\varphi_{y_i}(X_i)$ enables the model to account for variations in the quality of text attributes (w.r.t. their indicative power for the labels) across different TAGs, further enhancing its adaptivity. For node classification, we can infer $\mathbb{P}_\mathcal{G}(Y \mid X) \propto \prod_{i \in \mathcal{V}} \varphi_{X_i}(y_i) 
\prod_{(i,j) \in \mathcal{E}} \psi_{ij}(y_i, y_j)$ where $\varphi_{X_i}(y_i) = \varphi_{y_i}(X_i)\phi_i(y_i)$.

\input{Arxiv/llm_bp}

\textbf{Belief Propagation} %The BP algorithm is to make such an inference. 
 Exact inference for $\mathbb{P}_{\GG}(Y|X)$ is intractable in large-scale graphs with cycles~\cite{koller2009probabilistic}. In practice, loopy belief propagation (LBP) is often used to conduct an approximate inference~\cite{murphy2013loopy}, which follows: Initialize the distributions $p_j^{(0)}(y_j)\propto\varphi_{X_i}(y_i)$ and $m_{i \to j}^{(0)}(y_j) = 1/c$ for all $i,j\in\VV$. For $k=1,2,...,L$, we do 
 \begin{align}
 \label{eq:message_passing}
 \log m_{j \to i}^{(k)}(y_i) \cong &\, \text{LSE}_{y_j}[\log \psi_{ij}(y_i,y_j) + \\ &
\log p_j^{(k-1)}(y_j) - \log m_{i \to j}^{(k-1)}(y_j)], \nonumber \\
 \log p_i^{(k)}(y_i) \cong &\log p_i^{(0)}(y_i) + \sum_{j \in \mathcal{N}(i)} \log m_{j \to i}^{(k)}(y_i),  \nonumber
\end{align} where $\cong$ denotes the equality with difference up-to a constant. LSE stands for the log-sum-exp function: 
$\text{LSE}_{y_j} [f(y_i, y_j)] = \log \left[ \sum_{y_j} \exp (f(y_i, y_j)) \right]$. The final $\arg\max_{y_i} p_i^{(k)}(y_i)$ gives the label prediction. 
Detailed derivation can be found in Appendix.~\ref{sec:app_detailed_derivation}. %\pan{fixed notation in appendix accordingly}

\textbf{\proj} To execute the above LBP algorithm, we need to specify several components based on the TAG. First, $p_i^{(0)}(y_i)$ represents the distribution of the label $y_i$ given the observed attributes $X_i$ alone, which can be estimated using normalized cosine similarities: % between node and class label embeddings: applying the Softmax function~\cite{bridle1990probabilistic} to normalize the cosine similarity between node and class label embeddings:
\begin{align}
\label{eq:node_potential}
p_i^{(0)}(y_i) = \text{softmax}(\{\cos(h^{X}_i, q^{C}_k)/\tau\}_{k\in[c]})
\end{align}







%which within a unified probabilistic framework enables the MRF formulation to encompass both homophily and heterophily among connected nodes by encoding preferences for label similarity or difference.

%For node classification, we can infer $\mathbb{P}_\mathcal{G}(\mathbf{Y} \mid \mathbf{X})$. % for each node $i$. %, given the deterministic graph structure $\mathcal{G}$ and the observed attributes $\mathbf{X}$. 
%During inference, we achieve this by maximizing the posterior distribution of the labels:
%\begin{align}
%\arg\max_{\mathbf{Y}} \,& \mathbb{P}_\mathcal{G}(\mathbf{Y} \mid \mathbf{X}) 
%~\propto~ \\
%&\prod_{i \in \mathcal{V}} \bigl[\phi_i(y_i)\,\varphi_{x_i}(y_i)\bigr] 
%\prod_{(i,j) \in \mathcal{E}} \psi_{ij}(y_i, y_j).
%\end{align}
% First, we initialize $ p_i^{(0)}(y_i)$ by applying the Softmax function~\cite{bridle1990probabilistic} to normalize the cosine similarity between node and class label embeddings:
% \begin{align}
% %\label{eq:node_potential}
% p_i^{(0)}(y_i=c_j;x_i) = \text{softmax}(cos(\hh^{\XX}_i, \hh^{\CC}_j)/\tau)
% \end{align}
%\frac{\exp  \frac{1}{\tau} cos(\hh^{\XX}_i, \hh^{\CC}_j) }{\sum_j^c \exp  \frac{1}{\tau} cos( \hh^{\XX}_i, \hh^{\CC}_j )},
where $h_i^{X}$ and $h_k^{C}$ denote node $i$'s class-conditional embedding and class $k$'s embedding given by the LLM encoder as discussed in the previous section. %derived as described in the previous section, 
%and $\hh^{\CC}$ refers to the class embeddings, obtained from class descriptions in the zero-shot setting and from the average labeled node embeddings in the few-shot setting. 
$\cos(\cdot)$ denotes cosine similarity and $\tau$ is the temperature hyper-parameter. 

Second, we characterize the edge potentials $\psi_{ij}(y_i, y_j)$. We employ an LLM agent to assess the homophily level of the TAG. Specifically, we uniformly at random sample $T$ connected node pairs ($T\ll |\EE|$), and for each pair, we prompt the LLM to determine whether the two nodes belong to the same class based on their attributes, as illustrated in Fig.~\ref{fig:pipe}. The ratio of ``Yes'' responses, denoted by $r$ is used to set 
\begin{align}
\label{eq:bp}
\ \psi_{ij}(y_i, y_j) =
\begin{cases}
r, & \text{if } y_i = y_j \\
1-r, & \text{if } y_i \neq y_j
\end{cases},
\end{align}
Note that a more complex $\psi_{ij}(y_i, y_j)$ can be adopted by estimating the edge probabilities between any two classes. However, we choose the homophily level as a proof of concept. LLMs can provide a reasonably accurate estimation of the homophily level, as pairwise comparisons are typically much simpler tasks compared to full-scale classification.


% the graph structure and estimate $H$ by sampling a subset of node pairs, significantly smaller than the total number of edges in the graph.


% As a proof of concept, we assume 
% %, we adopt a widely-used assumption in graph statistical models, which posits that the coupling potential assigns one constant potential to edges between nodes of the same class and another constant potential for edges connecting nodes of different classes~\cite{holland1983stochastic,deshpande2018contextual}, specifically, we define the edge potential $\psi_{ij}(y_i, y_j)$ as:
% \begin{align}
% \label{eq:bp}
% \textbf{BP:} \ \psi_{ij}(y_i, y_j) =
% \begin{cases}
% H, & \text{if } y_i = y_j \\
% 1 - H, & \text{if } y_i \neq y_j
% \end{cases},
% \end{align}
% where $H \in [0,1]$ denotes the homophily ratio, indicating the proportion of edges that connect nodes belonging to the same class. We employ an LLM agent to analyze the graph structure and estimate $H$ by sampling a subset of node pairs, significantly smaller than the total number of edges in the graph.
% %using the following prompting template:
% %\textit{
% %``We have two $\{\textnormal{node\_type}\}$ from the following $\{\textnormal{\# class}\}$ categories:\\
% %$\{\textnormal{class\_info}\}$\\
% %The texts are as follows:\\
% %$\{\textnormal{Text 1}\}$\\
% %$\{\textnormal{Text 2}\}$\\
% %Please tell whether they belong to the same category or not by answering Yes or No after reasoning step by step.''
% %}
% An example is given in Fig.~\ref{fig:pipe}. Note that employing LLMs for such statistical analysis is much easier than directly instructing LLMs for text classification, due to the inherent fault tolerance provided by sampling.\hy{how to say}

\cite{wei2022understanding} demonstrated that linear propagation can approximate a single iteration of LBP when feature quality is limited. Based on this insight, we adopt the following approximate LBP formulation (denoted as BP appr.):
\begin{align}
 \log p_i^{(1)}(y_i) \cong &\log p_i^{(0)}(y_i)+
 \label{eq:bp_appr}\\
 &\text{sgn}(\log \frac{r}{1-r})\sum_{j \in \mathcal{N}(i)} \log p_j^{(0)}(y_i),  \nonumber    
\end{align}
where the homophily level $r$ influences the sign of the log-likelihood aggregation from neighboring nodes. We summarize the overall pipeline in Algorithm.~\ref{alg:llm_bp}

% Inspired by the empirical success of linear likelihood propagation as mentioned in~\cite{wei2022understanding}, we further propose a simplified linear approximation of the BP algorithm, whose edge potential is defined as:
% \begin{align}
% \label{eq:bp_appr}
% &\textbf{BP (appr.): }  \psi_{ij}(y_i, y_j) =\alpha \mathbbm{1}_{(H>t)} + \beta \mathbbm{1}_{(H \leq t)},
% \end{align}
% %\begin{align}
% %\label{eq:bp_appr}
% %&\textbf{BP (appr.): }  \psi_{ij}(y_i, y_j) = 
% %&\begin{cases}
% %    m, \ \text{if} \ y_i=y_j \ \text{and} \  H \geq t\\
% %    n, \ \text{if} \ y_i=y_j \ \text{and} \ H < t\\
% %    0, \ \text{if} \ y_i \neq y_j\\
% %\end{cases},
% %\end{align}
% where $t$ is the hyper-parameter serving as the threshold to determine homophily or heterophily of the graph structure, $\alpha, \beta$ are the two coefficients hyper-parameters for neighborhood node likelihood aggregation.
% \textcolor{red}{Original}
% Inspired by ~\cite{jia2021graph} \pany{We do not need to cite Jia. The key knowledge we use here is mrf and bp. We should cite them, these more foundamental concepts. Citing Jia et al will only distract people}, we model a TAG graph as a pair-wise Markov Random Field (MRF) \pany{it is not just model. We need to motivate why we consider this model and why we consider simulating BP.} and use its probabilistic framework to guide graph structure utilization. The probabilistic framework remains unchanged regardless the variance of the graph structures, thereby leading to unified structural interpretation .

% \textbf{Data Modeling.} In pair-wise MRF, we consider the generative process for the joint distribution of node labels and attributes. The joint probability distribution $p(\yy, \XX)$ \pany{using the capital $\mathbb{P}$} \pany{have we defined the notation $Y,\yy$. Also, why is there capital letters and little ones?}   is decomposed into two components: (1) the generation of the label distribution $p(\yy)$ \pany{This is not the common meaning of label distribution. Instead, this distribution is the distribution of y given the graph structure. So, the graph structure should be put in the notation as well. } and (2) the conditional distribution of attributes given labels $p(\XX \mid \yy)$. More formally, for the label distribution $p(\yy)$ in a pairwise Markov Random Field, it can be expressed as:
% % \begin{align*}
% %     p(\yy) = \frac{\psi(\yy)}{\sum_{\yy'} \psi(\yy')}, \psi(\yy) = \prod_{i \in \VV} \phi_i(y_i) \prod_{(i,j) \in \EE} \omega_{\mathbbm{1}(y_i = y_j)}
% % \end{align*}
% \begin{align}
%     p(\yy) = \frac{1}{Z_{\yy}} \prod_{i \in \VV} \phi_i(y_i) \prod_{(i,j) \in \EE} \psi_{ij}(y_i, y_j)
% \end{align}
% where $\phi_i(y_i)$ represents the unary potential for node $i$, $\psi_{ij}(y_i, y_j)$ denote the edge potential capturing the interactions between labels $y_i$ and $y_j$ of neighboring nodes, and $Z_{\yy}$ denote the normalization constant.

% For the attribute generation $p(\XX \mid \yy)$, the process assumes that the features of each node are conditionally independent given its corresponding label. Under this assumption, the distribution can be expressed in a factorized form as:
% \begin{align*}
%     p(\XX \mid \yy) = \prod_{i \in \VV} p(x_i \mid y_i) = \prod_{i \in \VV} \varphi_{x_i}(y_i).
% \end{align*}
% where $\varphi_{x_i}(\cdot)$ represents the likelihood function, which maps a given label $y_i$ to the probability of observing the corresponding feature $x_i$.

% \textbf{Principle II with Adaption to Node Classification:} \pany{why write this as an adaption? If the following is adaption, what do you mean by the above y.}

% In the context of a node classification task, the label inference involves estimating the conditional probability $p(y_i \mid \XX)$ for each node $i$ over deterministic graph structure \pany{graph structure should be highlighted here too} and attribute observations.
% % \textbf{Data Modeling.} In the MRF, we consider the generative process for the joint distribution of node labels and attributes. Specifically, the joint probability distribution $p(\yy, \XX)$ is decomposed into two components: (1) the generation of the label distribution $p(\yy)$ and (2) the conditional distribution of attributes given labels $p(\XX \mid \yy)$. More formally, for the label distribution $p(\yy)$ in a pairwise Markov Random Field, it can be expressed as:
% % % \begin{align*}
% % %     p(\yy) = \frac{\psi(\yy)}{\sum_{\yy'} \psi(\yy')}, \psi(\yy) = \prod_{i \in \VV} \phi_i(y_i) \prod_{(i,j) \in \EE} \omega_{\mathbbm{1}(y_i = y_j)}
% % % \end{align*}
% % \begin{align}
% %     p(\yy) = \frac{1}{Z_{\yy}} \prod_{i \in \VV} \phi_i(y_i) \prod_{(i,j) \in \EE} \psi_{ij}(y_i, y_j)
% % \end{align}
% % where $\phi_i(y_i)$ represents the unary potential for node $i$, and $\omega_{\mathbbm{1}(y_i = y_j)}$ denote the edge potential capturing the interactions between labels $y_i$ and $y_j$ of neighboring nodes. In this work, we adopt a widely-used assumption in graph statistical models, which posits that the coupling potential $\omega_{\mathbbm{1}(y_i = y_j)}$ assigns one constant potential to edges between nodes of the same class and another constant potential for edges connecting nodes of different classes~\cite{holland1983stochastic,deshpande2018contextual}.
% % For the attribute generation $p(\XX \mid \yy)$, the process assumes that the features of each node are conditionally independent given its corresponding label. Under this assumption, the distribution can be expressed in a factorized form as:
% % \begin{align*}
% %     p(\XX \mid \yy) = \prod_{i \in \VV} p(x_i \mid y_i) = \prod_{i \in \VV} \varphi_{x_i}(y_i).
% % \end{align*}
% % where $\varphi_{x_i}(\cdot)$ represents the likelihood function, which maps a given label $y_i$ to the probability of observing the corresponding feature $x_i$.
% During the inference stage, this is achieved by maximizing the posterior distribution of the observed attributes, i.e. \pany{just wrote $\max_{\yy} p(\yy \mid \XX) \propto p(\yy) p(\XX \mid \yy) = xxx$}
% \begin{align}
%     p(\yy \mid \XX) & = \frac{p(\XX, \yy)}{p(\XX)} \cong  \frac{p(\yy) p(\XX \mid \yy)}{\sum_{\yy'} p(\yy') p(\XX \mid \yy')} \\
%     & \cong \prod_{i \in \VV} \phi_i(y_i)\varphi_{x_i}(y_i) \prod_{(i,j) \in \EE} \psi_{ij}(y_i, y_j) 
%     % & \cong \prod_{i \in \VV} \phi_i(y_i)\varphi_{x_i}(y_i) \prod_{(i,j) \in \EE} \omega_{\mathbbm{1}(y_i = y_j)}
% \end{align}
% where 
% % the formulation represents a conditional random field~\cite{lafferty2001conditional} and 
% $\cong$ denotes “equality up to a normalization" \pany{no need this if use $\propto$}. To further characterize the edge potentials, we adopt a widely-used assumption in graph statistical models, which posits that the coupling potential assigns one constant potential to edges between nodes of the same class and another constant potential for edges connecting nodes of different classes~\cite{holland1983stochastic,deshpande2018contextual}, and we denote this potential as $\psi_{\mathbbm{1}(y_i = y_j)}$, and therefore \pany{no need to write the follow eq. Just denote $\psi_{ij}(y_i, y_j) = \psi_{\mathbbm{1}(y_i = y_j)}$}
% \begin{align} 
%     p(\yy \mid \XX) \cong \prod_{i \in \VV} \phi_i(y_i)\varphi_{x_i}(y_i) \prod_{(i,j) \in \EE} \psi_{\mathbbm{1}(y_i = y_j)}
% \end{align}
% \pany{Here explain how we define $\psi_{\mathbbm{1}(y_i = y_j)}$ in our case.} 
% % The assumption of data generation aligns with 2.1 in the original paper:

% % \begin{equation}
% %     p(\XX|y) = \prod_{i \in V} p(\XX_i|y_i) = \prod_{i \in V} f(y_i; \XX_i)
% % \end{equation}
% % During the inference stage, we have to assign labels according to the marginal likelihood $p(y_i | \XX)$. Since exact inference for the probability is intractable in large-scale graphs with cycles~\cite{}, we leverage the approximate inference algorithm via loopy belief propagation (LBP)~\cite{murphy2013loopy} and the belief-propagation process can be described as follows:
% % \begin{align}
% %     &\textbf{Initial: } p_i^{(0)}(y_i) = \text{\textcolor{red}{Introduce the initial condition}} \\
% %     &\textbf{Run $K$ Iterations: } \\
% %     &p_i^{(k)}(y_i) \cong p_i^{(0)}(y_i)\prod_{i \in \mathcal{N}(i)} m_{j \to i}^{(k)}(y_i), \\
% %     &m_{j \to i}^{(k)}(y_i) \cong \sum_{y_j} \psi_{\mathbbm{1}_{(y_i = y_j)}} p_j^{(k-1)} / m_{i \to j}^{(k-1)}(y_j)
% %     % &\log m_{ji}^{(t)}(y_i) \approx \mathrm{LSE}_{y_j} \left[ \log H_{ji}(y_j, y_i) \right. \\
% %     % & \quad + \log p_j^{(t-1)}(y_j) - \log m_{ij}^{(t-1)}(y_j) \left. \right]
% % \end{align}

% % In practice, people usually consider belief-propagation update in the log-space for numerical stability.
% During the inference stage, we assign labels by computing the marginal likelihood $p(y_i \mid \XX)$ \pany{as missing graph structure, this notation is confusing with those without graph structure}. Since exact inference for this probability is intractable in large-scale graphs with cycles~\cite{koller2009probabilistic}, we utilize the approximate inference algorithm via loopy belief propagation (LBP)~\cite{murphy2013loopy}. The belief propagation process is described as follows: \pany{do we really write so much for BP. I think this is more in the appendix. Here, our focus is more on how our alg implement of one iter BP. And, how LLMs help to estiamte the terms in the alg.}
% \begin{align}
% \label{eq:bp}
%     &\textbf{Initialization: } p_i^{(0)}(y_i) = \phi_i(y_i)\varphi_{x_i}(y_i), \label{eq:initialization} \\
%     &\textbf{Iterative Updates (for $K$ iterations):} \nonumber \\
%     &\quad p_i^{(k)}(y_i) \cong p_i^{(0)}(y_i) \prod_{j \in \mathcal{N}(i)} m_{j \to i}^{(k)}(y_i), \label{eq:message_update1} \\
%     &\quad m_{j \to i}^{(k)}(y_i) \cong \sum_{y_j} \psi_{\mathbbm{1}(y_i = y_j)} \frac{p_j^{(k-1)}(y_j)}{m_{i \to j}^{(k-1)}(y_j)}, \label{eq:message_update2}
% \end{align}
% where \( p_i^{(0)}(y_i) \) defines the prior beliefs about node \( i \), \( \mathcal{N}(i) \) denotes the set of neighbors of node \( i \), and \( \psi_{\mathbbm{1}(y_i = y_j)} \) represents the compatibility potential between nodes \( i \) and \( j \). The process iteratively updates the messages \( m_{j \to i} \) and beliefs \( p_i \) until convergence or a predefined maximum number of iterations \( K \) is reached. In practice, belief propagation updates are typically performed in log-space to enhance numerical stability.

% \begin{align}
%     &\log p_i^{(k)}(y_i) \cong \log p_i^{(0)}(y_i) + \sum_{j \in \mathcal{N}(i)} \log m_{j \to i}^{(k)}(y_i), \\
%     &\log m_{j \to i}^{(t)}(y_i) \cong \text{LSE}_{y_j} [\log \psi_{\mathbbm{1}_{(y_i = y_j)}} + \log p_j^{(k-1)}(y_j) - \log m_{i \to j}^{(k-1)}(y-j)]
% \end{align}

% where $\mathrm{LSE}_{y_j} \left[ f(y_i, y_j) \right] = \log \left[ \sum_{y_j} \exp \left( f(y_i, y_j) \right) \right]$.

% \begin{align}
% \label{eq:bp}
%     \text{BP:} equation
% \end{align}

% Inspired by ~\cite{wei2022understanding}, linear propagation of the likelihood may also lead to goo empirical performance. Therefore, we here propose another as alternative:

% \begin{align}
% \label{eq:bp_appr}
%     \text{BP. appr.}  equation
% \end{align}
%\input{Arxiv/llm_bp}
%Then the overall algorithm can be defined as in Algorithm.~\ref{alg:llm_bp}.





