\section{Experiments} 
%\pany{My expectation is to reserve at least 3 pages for exp. }
In this section, we evaluate \proj based on its two design principles, with a primary focus on zero-shot node classification tasks. Evaluations of few-shot node classification and link prediction tasks are provided in Appendix.~\ref{sec:app_few_shot}~\ref{sec:app_link_prediction}. First, we demonstrate the effectiveness of task-adaptive encoding and identify issues with existing methods that rely on aligning node embeddings with the LLM token space. Second, we validate the effectiveness of the proposed BP algorithm. Finally, we present the end-to-end performance of \proj, comparing it to state-of-the-art baselines. We first introduce the datasets and baselines used in the study:

\textbf{Datasets} As listed in Table~\ref{tab:datasets}, we selected eleven real-world TAG datasets that encompass a variety of text domain shifts, including citation networks, e-commerce data, knowledge graphs, and webpage networks, which cover both homophily and heterophily structures. For more details, see Appendix~\ref{sec:app_datasets}. 

\input{Arxiv/tables/datasets}

%\subsection{Baselines and Settings}
\textbf{Baselines:} We select representative baselines from all existing categories for model generalization on TAGs:

%\pany{these categories are better aligned with related works}
$\bullet$ \textit{Vanilla LM / LLM Encoders}: including Sentence-BERT (SBert)~\cite{reimers2019sentence}, RoBERTa~\cite{liu2019roberta}, text-embedding-3-large~\cite{openai2024textembedding}, and bge-en-icl~\cite{li2024making}, a state-of-the-art LLM2Vec encoder.

$\bullet$ \textit{Vanilla LLMs}: including GPT-3.5-turbo~\cite{achiam2023gpt} and GPT-4o~\cite{hurst2024gpt}, the latter being among the most advanced LLMs in reasoning. They process raw node texts without incorporating graph structures.

$\bullet$ \textit{Tuning LM Encoder / GNNs}: including ZeroG~\cite{li2024zerog}, UniGLM~\cite{fang2024uniglm} that tune LM encoders, ZeroG is specifically proposed for zero-shot node classification. DGI~\cite{velivckovic2018deep}, GraphMAE~\cite{hou2022graphmae} that perform Graph-SSL are also compared.
%\pany{try not duplicate much with related work. The related work should focus on the highlevel idea of these methods and the potential issues of their generalization. The exp just mentions that using these models from this category, maybe with a brief recap of their pipeline}

$\bullet$ \textit{LLMs with Graph Adapters}: including LLaGA~\cite{chen2024llaga}, TEA-GLM~\cite{wang2024llms}, and GraphGPT~\cite{tang2024graphgpt}, which are the three representative works adopting LLMs with projectors to align compressed node representations with the token space.%all of them are implemented with embeddings from SBert~\cite{reimers2019sentence}.

$\bullet$ \textit{Multi-Task Graph Foundation Models}: Consisting of OFA~\cite{liu2023one} and GOFA~\cite{kong2024gofa}, which are the state-of-the-art multi-task foundation models.

$\bullet$ \textit{LLMs for Data Augmentation}: referring to LLM-GNN~\cite{chen2023label}, specifically designed for zero-shot node classification, which utilizes LLMs as annotators for pseudo-labels and further train GNNs for inference.

$\bullet$ \textit{Neighborhood Aggregation (NA)}: referring to the training-free method proposed in ~\cite{yang2024gl}, which injects graph structural information into node representations by directly aggregating the averaged neighborhood embeddings.

\textbf{Settings:} Unlike LLM-BP which is training-free, most of the baselines--except from vanilla encoders, LLMs or NA--require pre-training. Methods of vanilla encoders and LLM-BP that require sampling nodes to obtain class embeddings under zero-shot settings are repeated 30 times with seed 42 to 71, and the average performance is reported in the following experiment sections. Implementation details for baselines and LLM-BP can be found in Appendix.~\ref{sec:app_implementation_details_baseline}~\ref{sec:app_llm_bp_implementation_details}.

\subsection{Evaluation for Task-Adaptive Node Embedding}
%\input{Arxiv/figures/principle_1}
\input{Arxiv/figures/principle_1_tsne}
 
%Experiments with encoders only in this part does not consider graph structure usage and directly classify nodes according to their maximum cosine similarity with the class description embeddings. \pan{This should be merged into Exp 1 and Exp 2 separately.}

$\bullet$ \textbf{Exp.1: Ineffectiveness of LLMs w/ Graph Adapters}  
%\pan{Also, I think the title would better say the ineffectiveness of Graph Adaptor.} 
%\pan{Explain that even without using graph structures xxx} 
Figure~\ref{fig:principle_1} illustrates the accuracy of encoder-based methods alongside two representative LLMs-with-graph-adapters methods across each dataset. Notably, using text embeddings generated by SBert~\cite{reimers2019sentence} without incorporating graph structural information significantly outperforms both LLaGA~\cite{chen2024llaga} and GraphGPT~\cite{tang2024graphgpt}. These two methods align node representations that combine SBert embeddings with graph information to the LLMs' token space via a projector. This finding suggests that the generalization capabilities of these approaches primarily stem from the pre-trained language model encoders rather than the LLMs' inherent understanding of TAG data. Consequently, future works should exercise caution when adopting this strategy. % Moreover, incorporating LLMs with the embeddings from LM encoders without sufficient training for the projector, may even hinder generalization.



$\bullet$ \textbf{Exp.2: Effectiveness of The Task-Adaptive Encoder}  

\input{Arxiv/figures/class_condition}
According to Figure~\ref{fig:principle_1}, the task-adaptive encoder achieves the best performance on most of the datasets, enhancing the vanilla LLM2Vec on average by $2.3\%$, highlighting the importance of incorporating task-specific information during encoding. 
To further illustrate this, we use the Citeseer~\cite{giles1998citeseer} dataset as an example and perform t-SNE visualization~\cite{van2008visualizing} on the embeddings derived from the encoders. As shown in Fig.~\ref{fig:tsne}, when provided with class information, the task-adaptive encoder generates embeddings that exhibit tighter clustering for the same class compared to other baselines. The significance test of improvement from task-adatove encoding is provided in Table.~\ref{tab:confidence} in Appendix.~\ref{sec:app_confidence}.
%\input{Arxiv/figures/tsne}

Note that the benefits of class information are observed only in encoders derived from LLM decoders potentially due to their strong contextual learning capabilities. As illustrated in Fig.~\ref{fig:class_condition}, incorporating class information into smaller LM encoders, such as SBert~\cite{reimers2019sentence} or RoBERTa~\cite{liu2019roberta}, may even degrade performance.  
Regarding Text-embedding-3-large~\cite{openai2024textembedding}, the impact of class information remains inconclusive due to the unknown internal mechanisms of the black-box encoder.







\subsection{Generalizable Graph Aggregation}
\input{Arxiv/figures/pred_h}

 %\pan{just explicitly say out principle II}
\input{Arxiv/tables/zero_shot}
\input{Arxiv/figures/principle_2}
$\bullet$ \textbf{Exp.3: LLM Agents for Homophily Level $r$ Estimation} As shown in Fig.\ref{fig:predict_h} (Left), we randomly sample $k$ edges ($k=100$ for large graphs and $k=50$ for small ones), incorporating them into prompts (Fig.\ref{fig:pipe}) for LLM-based estimation of the homophily level $r$ (Sec.\ref{sec:principle2}). We evaluate four LLMs: GPT-4o, GPT-4o-mini\cite{hurst2024gpt}, GPT-3.5-Turbo~\cite{achiam2023gpt}, and Mistral-7B-Instruct v0.3~\cite{jiang2023mistral}. Each model responds to each node pair over five trials, with the final estimate determined by majority voting. Full results are provided in Fig.\ref{fig:predict_h_more} in appendix, demonstrating that GPT-4o-mini and GPT-4o effectively estimate $r$, GPT-3.5-Turbo performs reasonably well, while Mistral-7B-Instruct-v0.3 fails. Balancing accuracy and cost efficiency, we select GPT-4o-mini's estimation (Fig.~\ref{fig:predict_h} Right) for subsequent studies. % and adopt its estimation throughout our study. 

$\bullet$ \textbf{Exp.4: Effectiveness of the BP Algorithm}
Experimental results are presented in Fig.~\ref{fig:principle_2}, where we evaluate the four approaches over various graph structures. Specifically, We compare the BP algorithm (Eq.~\ref{eq:bp}) and its linear approximation (Eq.~\ref{eq:bp_appr}) with vanilla encoders that do not utilize structure (Raw) and the NA baseline. For all the four encoders across all the datasets, the proposed BP algorithm slightly outperforms its linear approximation, and they consistently outperform Raw. Moreover, in most datasets, they also surpass the NA baseline, particularly on heteophilic graphs, where direct neighborhood embedding aggregation negatively affects performance. These results highlight the generalizability of our data modeling approach and the effectiveness of the key-parameter estimation design in BP.


\subsection{End-to-End Evaluation}

$\bullet$ \textbf{Exp.5: Main Results in the Zero-Shot Setting}
The main experimental results are presented in Table~\ref{tab:zero_shot}. Among the baselines, vanilla encoders and LLMs demonstrate strong zero-shot generalization. GPT-3.5-Turbo~\cite{achiam2023gpt} ranks first on the Sportsfit dataset, while GPT-4o~\cite{hurst2024gpt} achieves the best performance on Pubmed and Children.

UniGLM~\cite{fang2024uniglm} and ZeroG~\cite{li2024zerog} perform well in domains aligned with their pre-training, such as citation networks (e.g., ZeroG enhances SBert's performance on Cora, Pubmed, and Wikics). However, both struggle on TAGs with unseen text distributions (e.g., Sportsfit) or novel graph structures (e.g., webpage networks), suggesting that fine-tuned LM encoders may suffer performance degradation on out-of-domain TAGs.
Similarly, graph-SSL methods (DGI~\cite{velivckovic2018deep}, GraphMAE~\cite{hou2022graphmae}) show limited generalization across structural shifts. 

Among multi-task graph foundation models, GOFA achieves strong performance, likely benefiting from a larger pre-training corpus for graph-text alignment~\cite{hu2021ogb, ding2023enhancing} compared to GraphGPT~\cite{tang2024graphgpt} and LLaGA~\cite{chen2024llaga}, which are trained solely on ogbn-arxiv. However, GOFA still requires broader pre-training and instruction fine-tuning to improve generalization under text domain shifts, and its reliance on GNNs may limit effectiveness on heterophilic data.

Notably, LLM-BP and LLM-BP (appr.) achieve the highest average ranking across all datasets. Another interesting observation is that when we randomly sample $20c$ nodes to obtain the class embeddings with the help of LLMs following Algorithm.~\ref{alg:llm_bp}, the zero-shot performance of the encoders in this setting is comparable to their performance between $5$-$10$-shot setting as shown in Table.~\ref{tab:few_shot} in the Appendix.
Further comparisons with LLM-GNN~\cite{chen2023label} and TEA-GLM~\cite{wang2024llms} are provided in Appendix~\ref{sec:app_more_baselines}.


$\bullet$ \textbf{Exp.6: Main Results under Few-shot Setting} We conduct the evaluation in $k=1,3,5,10$-shot settings. %with $c$-way settings, where $c$ remains the same as the total number of classes in each dataset. 
Using 10 different random seeds, we sample the shots from the training set and repeat the experiments 10 times. The experimental results are presented in Table~\ref{tab:few_shot} in Appendix~\ref{sec:app_more_experiment_results}. Across all 
$k$-shot settings, LLM-BP and LLM-BP (appr.) outperform the baseline models. %such as SBERT~\cite{reimers2019sentence}, Text-Embedding-3-Large~\cite{openai2024textembedding}, and LLM2Vec~\cite{li2024making}.

%$\bullet$ \textbf{Exp.7: Zero-Shot Link Prediction}
%We also extend our framework into the zero-shot link prediction task to validate the generalizability of our principles. Results are shown in Table.~\re in Appendix.~\ref{sec:app_link_prediction}.




%\subsection{Ablation Study}

%\textbf{Number of Aggregation Layers}

%A Table.