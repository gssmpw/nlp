\documentclass[11pt,a4paper]{article}
\usepackage[margin=1in]{geometry}
\newcommand\hmmax{0}
\newcommand\bmmax{0}
\usepackage[font=concrete]{chihao}

\usepackage{tocloft}
\setlength{\cftbeforesecskip}{0.7em}


\usepackage{booktabs}
\usepackage{threeparttable}
\usepackage{multirow}
\usepackage{array}
\usepackage{makecell}

\usepackage{catchfilebetweentags}

\newcommand{\DTV}{\!{TV}}
\newcommand{\vol}{\!{vol}}


\title{On the query complexity of sampling from non-log-concave distributions}

\author{Yuchen He\\Shanghai Jiao Tong University \\ \textsf{yuchen\_he@sjtu.edu.cn} \and Chihao Zhang \\Shanghai Jiao Tong University \\ \textsf{chihao@sjtu.edu.cn}}

%\date{}

\begin{document}
\maketitle

\begin{abstract}
    We study the problem of sampling from a $d$-dimensional distribution with density $p(x)\propto e^{-f(x)}$, which does not necessarily satisfy good isoperimetric conditions.

    Specifically, we show that for any $L,M$ satisfying $LM\ge d\ge 5$, $\eps\in \tp{0,\frac{1}{32}}$, and any algorithm with query accesses to the value of $f(x)$ and $\grad f(x)$, there exists an $L$-log-smooth distribution with second moment at most $M$ such that the algorithm requires $\tp{\frac{LM}{d\eps}}^{\Omega(d)}$ queries to compute a sample whose distribution is within $\eps$ in total variation distance to the target distribution. We complement the lower bound with an algorithm requiring $\tp{\frac{LM}{d\eps}}^{\+O(d)}$ queries, thereby characterizing the tight (up to the constant in the exponent) query complexity for sampling from the family of non-log-concave distributions.

    Our results are in sharp contrast with the recent work of Huang et al. (COLT'24), where an algorithm with quasi-polynomial query complexity was proposed for sampling from a non-log-concave distribution when $M=\!{poly}(d)$. Their algorithm works under the stronger condition that all distributions along the trajectory of the Ornstein-Uhlenbeck process, starting from the target distribution, are $\+O(1)$-log-smooth. We investigate this condition and prove that it is strictly stronger than requiring the target distribution to be $\+O(1)$-log-smooth. Additionally, we study this condition in the context of mixtures of Gaussians.

    Finally, we place our results within the broader theme of ``sampling versus optimization'', as studied in Ma et al. (PNAS'19). We show that for a wide range of parameters, sampling is strictly easier than optimization by a super-exponential factor in the dimension $d$.

\end{abstract}

%\newpage

\setcounter{tocdepth}{1}
\tableofcontents

%\ctodo{make sure the TOC does not exceed one page.}

\newpage

\section{Introduction}
The problem of sampling from a given distribution has gained much attention in recent years due to its  wide applications in many fields such as machine learning, physics, finance and statistics (e.g. \cite{ADDJ03,K06,D07,LB21}). Given query access to the value and gradients of a potential function $f:\bb R^d\to \bb R$, the task is to generate a sample from the distribution $\mu$ with density $p_\mu\propto e^{-f}$ on $\bb R^d$ within as few queries as possible.

Many algorithms have been developed to address this problem. The Langevin algorithm and its variants are the most commonly used ones (e.g. \cite{RT96,CB18,CCBJ18,Wib19,CLA+21,LPW24}). 
% This family includes unadjusted Langevin algorithms (\cite{RT96,CB18,DM19,VW19,CEL+24,MFH+23}), underdamped Langevin algorithms (\cite{CCBJ18,SL19,ZCL+23,LPW24}), Metropolis-adjusted Langevin algorithms (\cite{RS02,BH13,DCWY19,CLA+21}), proximal Langevin algorithms (\cite{Ber18,Wib19,LL22}) and so on. 
Polynomial-time convergence of Langevin-based algorithms is guaranteed when the target distribution $\mu$ exhibits good properties such as isoperimetric properties (e.g. \cite{VW19,CEL+24,MFH+23,ZCL+23}) or log-concavity (e.g. \cite{SL19,DCWY19,AC24}).
% However, for general non-log concave distributions, these algorithms may not be effective.

However, many interesting sampling problems are not log-concave and do not satisfy good isoperimetric properties. In recent years, there has been growing interest in sampling from general non-log-concave distributions. In this work, as well as in most of the literature on non-log-concave sampling, the distributions are required to satisfy only the following two weak assumptions.

\begin{assumption}\label{assump:moment}
    The second moment of $\mu$ is bounded, i.e. $\E[X\sim \mu]{\|X\|^2}\leq M$ for some $M<\infty$.
\end{assumption}

\begin{assumption}\label{assump:smooth}
    The potential function $f$ is differentiable and $\grad f$ is $L$-Lipschitz, i.e, for any $x,y\in \bb R^d$, $\|\grad f(x)-\grad f(y)\| \leq L \|x-y\|$.
\end{assumption}

Note that \Cref{assump:smooth} is equivalent to requiring $\mu$ to be $L$-log-smooth. There are mainly two lines of work regarding non-log-concave sampling. 

The first line aims at understanding the astonishing performance of the  score-based generative models (SGMs) in practice. It is proved that assuming the score functions can be accurately estimated, an accurate sampler with polynomially many queries exists (e.g. \cite{CCL+23,CCL+23b,CLL23,BBDD24}). %\ctodo{There might be many other refs.}

Another line of work is to identify the inherent sample complexity under the value / gradient oracle model. For special distributions such as the mixture of Gaussians of some particular shapes, polynomially many queries are sufficient to get an accurate sample (\cite{GLR18,GTC24}). On the other hand, a lower bound of $e^{\Omega(d)}$ for general non-log-concave sampling problems is established in \cite{GLR18}.

%In this paper, we require the target distributions to satisfy only the following two very weak assumptions, which are commonly used in most works.
% In most studies, the target distributions are only required to satisfy the following two very weak assumptions.

% \htodo{mention that $LM\geq d$.}
%Yet, due to the challenging nature of this problem, there is still no clear understanding of it. 
 To the best of our knowledge, the query complexity for sampling in terms of the parameters $L$ and $M$ 
 is not known yet. The recent work~\cite{HZD+24} designed a quasi-polynomial algorithm under a stronger assumption. Their algorithm further requires that \emph{the distributions during the Ornstein-Uhlenbeck process (OU process) starting from the target distribution $\mu$ to be $L$-log-smooth} and has query complexity roughly $\exp\tp{\+O(L^3)\cdot \!{polylog}(Ld+M)}$. Therefore, if all the distributions along the trajectory of the OU process is $\+O(1)$-log-smooth and $M=\!{polylog}(d)$, their algorithm is quasi-polynomial. Breaking the exponential barrier turns out to be significant in both theory and applications. However, as far as we can see, the relationship between this ``smoothness along the trajectory'' assumption used in their work and \Cref{assump:smooth} remains unclear, which limits the applicability of their results.

% For general non-log-concave distributions, the performance of these algorithms are not guaranteed. Some other algorithm models have been proposed to address non-log-concave cases, such as the simulated tempering (\cite{WSH09,GLR18,QW24}), annealing schemes (\cite{GTC24,HMRW24}) and denoising diffusion probabilistic models (DDPMs) (\cite{CCL+23,CLL23,HZD+24}). 
% Many works aim to establish convergence guarantees under minimal assumptions on the bound of the second moment and the smoothness of the potential function. 
% Building on these assumptions, the work of \cite{GTC24} designed an algorithm and derives a complexity bound with regard to the action of the curve of certain probability measures related to $\mu$, using annealed Markov chain Monte Carlo (MCMC) algorithms. 
% In the recent series of works related to DDPMs, it is further assumed that distributions during the Ornstein-Uhlenbeck process (OU process) starting from $\mu$ are also log-smooth (for example, Assumption [A1] in \cite{HZD+24}). Under these assumptions, \cite{CCL+23} demonstrated that as long as the score functions can be estimated, a polynomial sampler exists. The work of \cite{HZD+24} further provided a theoretical implementation for estimating the score function, leading to a quasi-polynomial complexity upper bound for non-log-concave sampling problems.

% Besides sampling algorithms, the lower bounds on the query complexity is also important to provide insights into this problem. Research on sampling lower bounds are relatively scarce. The first dimension-dependent lower bound for general log-concave sampling was only provided in the last few years by \cite{CDL+24}. For non-log concave cases, the work of \cite{GLR18} proved that $e^{\Omega\tp{d}}$ queries are necessary for any algorithm to guarantee an error of $0.3$ in total variation distance. A similar exponential bound is also demonstrated in \cite{Cha24}. However, a general lower bound that incorporates dependencies on the error $\eps$, smoothness parameter $L$, and the second moment $M$ has yet to be established. The form of the optimal query complexity bound is still unclear under these minimal assumptions on the target distribution.

%Inspired by this trend of works, we want to investigate the conditions under which a sub-exponential sampler exists for non-log concave distributions. 
% In this work, we study the problem of sampling from general non-log-concave distributions under these two mild assumptions:

In this work, we make progress in understanding the query complexity for sampling under \Cref{assump:moment} and \Cref{assump:smooth}. Additionally, we further investigate the ``smoothness along the trajectory'' property mentioned above and, finally, compare our results with the query complexity for general non-convex optimization. 


% Taken together, these two bounds reveal that the optimal query complexity should be $tp{ \frac{LM}{d\eps}}^{\+(d)}$. This is the first characterization of the optimal bound for general non-log-concave sampling. 


%We also investigate the conditions to guarantee the smoothness during the entire OU process. This provides a deeper understanding of the situations where the quasi-polynomial bound in \cite{HZD+24} holds. 
% We find a class of $O(1)$-log smooth distributions whose smoothness bound will evolve to $\omega(1)$ during the OU process and provide an analysis for the conditions to guarantee the smoothness during the entire OU process on the mixture of Gaussian distributions.

% Given the exponential lower bound for arbitrary $L$, we are also interested in the conditions for the quasi-polynomial bound in \cite{HZD+24} to hold. 
% They claim that their smoothness requirement can be relaxed to the smoothness of the target distribution only, and thus their convergence bound holds under the very mild conditions \Cref{assump:moment}~and~\ref{assump:smooth}. However, we find that there can be large gap between the smoothness bound of the target distribution and the distributions in the OU process. We provided some examples where, in these cases, though the target distribution is $\+O(1)$-log-smooth, the bound in \cite{HZD+24} is super-exponential.

% In addition to the above results on sampling, we also consider optimization problems. 
% The relationship between optimization and sampling has long been a topic of interest (\cite{MCJ+19,Tal19,CS24}). We provide an optimization lower bound of  $e^{\frac{d}{2}\Omega\tp{\log LM}}$ and by comparing this with our sampling upper bound, we conclude that in general non-convex situations, sampling can indeed be simpler than optimization.

% We design a sampling algorithm which always terminates within $e^{\+O\tp{d\log \frac{LM}{d\eps}}}\cdot \!{poly}\tp{\eps^{-1},d,L,M}$ queries and established a corresponding query complexity lower bound of $e^{\frac{d}{2}\log \Omega\tp{\frac{LM}{d\eps}}}$. Taken together, these two bounds reveal that the optimal query complexity should be $e^{\Theta\tp{d\log \frac{LM}{d\eps}}}$. This is the first characterization of the optimal bound for general non-log-concave sampling. 


\subsection{Main results}
% Our main results can be summarized in the following four aspects.
We summarize our main results in the following four aspects.

\paragraph{Query complexity lower bounds for sampling}
% We prove a general lower bound with respect to $L,M,d$ and $\eps$. If an algorithm can sample from any distribution that satisfies \Cref{assump:moment}~and~\ref{assump:smooth} with a total variation distance error of no more than $\eps$, it will inevitably require $e^{\frac{d}{2}\log \Omega\tp{\frac{LM}{d\eps}}}$ queries (see \Cref{thm:}). 



First, we provide a general query complexity lower bound. We prove that to guarantee an error of $\eps$ in total variation distance, $\tp{\frac{LM}{d\eps}}^{\Omega(d)}$ queries is inevitable. See \Cref{thm:main-lb} below for the formal statement. 

\begin{theorem}\label{thm:main-lb}
    Let $\eps\in \tp{0,\frac{1}{32}}$. For any $L,M>0$ such that $LM\geq d$ and for any $d\geq 5$, if a sampling algorithm $\+A$ always terminates within $K$ queries on any target distribution $\mu$ under \Cref{assump:moment}~and~\ref{assump:smooth}, and guarantees that the distribution of $\+A$'s output, denoted as $\tilde \mu$, satisfies $\DTV(\tilde \mu, \mu)\leq \eps$, then $K = \tp{\frac{LM}{d\eps}}^{\Omega(d)}$.
\end{theorem}
%\htodo{I write $e^{\Omega\tp{\frac{d}{2}\log \frac{LM}{d\eps}}}$ rather than $e^{\frac{d}{2}\log \Omega\tp{\frac{LM}{d\eps}}}$ because it seems that we cannot get $e^{\frac{d}{2}\log \Omega\tp{\frac{LM}{d\eps}}}$ and can only get $e^{\frac{d}{2}\log \tilde\Omega\tp{\frac{LM}{d\eps}}}$}
%\ctodo{Can we write $\tp{\frac{LM}{d\eps}}^{\frac{d}{2}\tp{1-o(1)}}$. (I think it is somehow important to keep the constant $\frac{1}{2}$ explicit in the bound, since we believe that it is the correct constant. This constant matters a lot in some cases, e.g. \Lovasz local lemma)}
% We remark that the assumption $LM=\Omega(d)$ is natural because under \Cref{assump:moment}~and~\ref{assump:smooth}, $LM$ is at least $d$ (see \Cref{lem:lb-LM}).
Besides, when $\frac{LM}{d\eps}=\omega(1)$, the constant coefficient in the exponent is $\frac{1}{2}-o\tp{\frac{1}{d}}$, i.e., we can prove a lower bound of the form $\tp{\frac{LM}{d\eps}}^{\frac{d}{2}-o(1)}$. The detailed formula is given in \Cref{thm:main}.  

%To the best of our knowledge, this is the first general lower bound result on this problem. Our lower bound covers the previous lower bound in~\cite{GLR18} and our techniques can also be applied to~\cite{Cha24} to establish a more general bound in their context.


Note that $LM\ge d$ is a natural assumption for distributions (see \Cref{lem:lb-LM}). This indicates that, in general exponentially many queries are needed for sampling, and when $LM=\omega(d)$, super-exponential queries are required. This result significantly improved previous lower bound of $e^{\Omega(d)}$ in~\cite{GLR18}. Our techniques can also be applied to~\cite{Cha24} to establish a more general bound in their context.

%Besides, we design a sampling algorithm which requires at most $\tp{\frac{LM}{d\eps}}^{\+O(d)}$ queries of $f$ and $\grad f$. Taken together, we get a tight (up to the constant in the exponent) bound for general non-log-concave sampling problems.

% \htodo{Our methods are similar to holden lee. But in \Cref{sec:lb}, we demonstrated why we cannot introduce the dependency on $\eps$ if we still use their hard instances. }

\paragraph{Query complexity upper bounds for sampling}
We also design an algorithm to sample from $\mu$ with an $\eps$ error in total variation distance for any $\mu$ satisfying \Cref{assump:moment}~and~\ref{assump:smooth}, with the query complexity bounded by $\tp{\frac{LM}{d\eps}}^{\+O(d)}\cdot \!{poly}(\eps^{-1},d,L,M)$. %See the following \Cref{thm:main-ub}.
\begin{theorem}\label{thm:main-ub}
    Assume $d\ge 3$. There exists an algorithm such that, for any distribution $\mu$ with density $p_\mu(x)\propto e^{-f_\mu(x)}$ where $f_\mu\in C^1(\bb R^d)$, $\grad f(0)=0$, and satisfies \Cref{assump:moment}~and~\ref{assump:smooth}, outputs a sample $x$ with distribution $\tilde \mu$ satisfying $\DTV(\mu,\tilde \mu)\le \eps$ within $\tp{\frac{LM}{d\eps}}^{\+O(d)}\cdot \!{poly}\tp{\eps^{-1},d,L,M}$ query accesses to $f_\mu$ and $\grad f_\mu$, for any $\eps\in (0,1)$. 
\end{theorem}
The assumption $\grad_\mu f(0)=0$ is for the sake of simplicity. One can first apply a gradient descent algorithm to find a stationary point of $f$ and shift the origin to that point. The form of this bound matches our lower bound in \Cref{thm:main-lb}, which suggests that the optimal query complexity for sampling from non-log-concave distributions should be $\tp{\frac{LM}{d\eps}}^{\Theta(d)}$. 
%\ctodo{The notation $\E[X\sim p]{.}$.}

\paragraph{The smoothness evolvement during the OU process}

As mentioned before, the work of~\cite{HZD+24} presented a quasi-polynomial algorithm for sampling from distributions with second moment at most $M=\!{poly}(d)$. They further require that all distribution along the trajectory of the OU process initialized at the target distribution is $\+O(1)$-log-smooth. Our lower bound already implies that the condition is strictly stronger than merely requiring the target distribution to be $\+O(1)$-smooth. We further construct an explicit family of distributions, called ``stitched Gaussians'', and calculate its evolvement of the smoothness property along the OU trajectory. This family of distributions are $\+O(1)$-log-smooth while along the OU process, the Hessian of their log density becomes unbounded.

%We also investigate the ``smoothness along the trajectory'' condition mentioned above and show that it is strictly stronger than merely requiring the initial distribution to be smooth. Specifically, we construct explicit distributions which are $\+O(1)$-log-smooth while along the OU process, the Hessian of their log density becomes unbounded. Therefore, it is interesting to study under what conditions, the $\+O(1)$-log-smoothness condition can be preserved during the OU process. We obtain some partial results for the mixtures of Gaussians.

%Besides the work on complexity bounds, to provide an intuitive comparison between our results and the results in \cite{HZD+24}, we compare \Cref{assump:smooth} with the smoothness condition in their work. They assume the distributions throughout the entire OU process to be $L$-log-smooth. Previous works do not pay much attention to distinguishing between these two smoothness conditions. However, we find that even though the initial distribution is $\+O(1)$-log-smooth, it might evolve into an $\omega(1)$-log-smooth distribution during the OU process. 
\begin{theorem}\label{thm:main-smooth}
    For the OU process $\ab\{X_t\}_{t\geq 0}$, let $\mu_t$ be the distribution of $X_t$ and its density is $p_t$. For arbitrary $s=\Omega(d)$, there exists an initial distribution $\mu$ which is $\+O(1)$-log-smooth but $\norm{\grad^2 \log p_t(x)}_{\!{op}} \geq \Omega\tp{e^{-2t}s-1}$ for any $t\geq \frac{\log 10}{2}$ at some point $x$.
\end{theorem}
% The specific distributions we find that satisfy the property in \Cref{thm:main-smooth} is a family of multi-modal distributions called the stitched Gaussian. 
Our results indicate that on those distributions with the property in \Cref{thm:main-smooth}, the upper bound of \cite{HZD+24}, which is $e^{\tilde{\+O}(L^3)}$ with $L$ being the smoothness bound during whole the OU process, can be very large. In contrast, our algorithm only requires smoothness of the initial distribution, allowing it to achieve a better complexity bound than~\cite{HZD+24} in these cases.

Moreover, our results also suggest that it is interesting to study under what conditions, the $\+O(1)$-log-smoothness condition can be preserved during the OU process. We obtain some partial results for mixtures of Gaussian and summarize them in \Cref{tab:result-comp}.

%Furthermore, this surprising result raises an interesting question: when will the property of $\+O(1)$-smoothness be preserved throughout the OU process? We provide an analysis to this question on the mixture of Gaussian distributions and summarize our results in \Cref{tab:result-comp}. 

% We compare our \Cref{assump:smooth} with the smoothness assumption in previous DDPM-based works, where they assume the distributions throughout the entire OU process are $L$-smooth. We prove that, even though the initial distribution is $\+O(1)$-smooth, it might evolve into an $\omega(1)$-smooth distribution during the OU process. This is a surprising result and suggests that assuming the smoothness throughout the entire OU process may not be trivial.

\paragraph{The comparison between sampling and optimization} 

Finally, we provide a comparison between the sampling and optimization tasks. We establish the lower bound in \Cref{thm:main-opt-lb} for optimization problems in non-convex cases based on the results in \cite{MCJ+19}. Let $x^*$ be the minimizer of function $f$ and $x^{(k)}$ be the point queried by the algorithm at step $k$. 
\begin{theorem}\label{thm:main-opt-lb}
    Assume $LM=\Omega(d)$ and $d\geq 8$. If an optimization algorithm $\+A$ queries at most $K$ points and can guarantee that $\min_{k\leq K} \abs{f(x^{(k)}) - f(x^*)}<1$ with constant probability for any $L$-smooth function $f:\bb R^d\to \bb R$ satisfying that the second moment of the distribution $\mu$ with density $\propto e^{-f}$ is $\Theta(M)$, then $K$ is at least $ (\alpha\cdot LM)^{\frac{d}{2}}$ for some universal constant $\alpha>0$.
\end{theorem}
On the other hand, for constant $\eps>0$, our upper bound for the sampling task shows that one requires only $\tp{\frac{LM}{d}}^{\+O(d)}$ queries to draw an approximate sample from $\mu$. This demonstrates that, in the context of non-convex optimization and non-log-concave sampling under \Cref{assump:moment}~and~\ref{assump:smooth}, when $LM=\Theta(d)$, sampling from a distribution with density $\propto e^{-f}$ can indeed be faster than finding the minimizer of $f$ by a super-exponential factor.

% We compared the complexity of the sampling and optimization problems under these two assumptions. The relationship between optimization and sampling has always been a topic of interest.  
% This suggests that, for this specific class of functions, sampling is harder than optimization.


\subsection{Technical overview}

\subsubsection{The lower bound}

The general idea to prove a query complexity lower bound, as used in previous work~\cite{GLR18,Cha24}, is to construct many distributions that are difficult for the algorithm to distinguish. Specifically, for each $v\in \bb R^d$, one can perturb the density of a base distribution $\mu_0$ in $\+B_r(v)$, a ball of radius $r$ centered at $v$, to get a new distribution $\mu_v$.  For two vectors $u$ and $v$ with $\+B_r(u)\cap \+B_r(v)=\emptyset$, after proper perturbation, one can get $\DTV(\mu_{v},\mu_u)\approx \eps$. Suppose one can find $n$ disjoint balls, with centers $\set{v_1,\dots,v_n}$. Then if the algorithm wants to sample from $\mu_{v_i}$ within error $\eps$, it should first recognize $v_i$ among the $n$ candidates. This indicates that when the input instance is $\mu_0$, the algorithm must query almost every $\+B_r(v_i)$, which gives the lower bound $\Omega(n)$.

Then at a high level, proving lower bounds can be viewed as packing as many disjoint $\+B_r(v)$'s in the domain of the base distribution while maintaining the desired properties (smoothness and bounded second moment) of the distribution. Previous works~\cite{GLR18,Cha24} 
%Suppose one can pack $n$ disjoint balls with centers $\set{v_i}_{i\in [n]}$. Then at a high level, the lower bound come from the following fact: any algorithm that can correctly sample from all these distributions must query almost every $B_r(v_i)$ when ithe
chose a Gaussian distribution as the base $\mu_0$ and proved a lower bound of $e^{\Omega(d)}$ using the above arguments. 

In this work, we want to derive a general lower bound with regard to all parameters $d,L,M$ and $\eps$. Our key observation is that using Gaussian as the base distribution is suboptimal. Let us explain the reason.
%To achieve the correct dependence on $\eps$, however, this simple base distribution is not viable. 
%If we simply choose $\mu_0=\+N\tp{0,\frac{M}{d}I_d}$ (the reason for using this Gaussian is that its second moment is exactly $M$), we claim that $r$ should be $\Omega\tp{\sqrt{\frac{d}{L\eps}}}$ and the number of balls we can pack is smaller than $n$.
There exists a trade-off between packing more disjoint balls and maintaining properties of the perturbed distributions. To pack more balls, one needs the radius $r$ to be as small as possible, and the center $v_i$ should be as far as possible from the origin. However,
%Note that the main challenge for the above strategy is to design a proper perturbation rule. The following requirements are necessary for a legal perturbation:
% To see the reasons behind this, we now examine what issues arise when $\mu_0=\+N\tp{0,\frac{M}{d}I_d}$ (the reason for using this Gaussian is that its second moment is exactly $M$). 

% To simulate this new distribution, the algorithm must recognize the perturbed region first. For simplicity, we let the perturbed region to be a ball $\+B_r(v)$ with center vector $v$ and radius $r$. To select such $r$ and $v$'s, we need the following requirements:
\begin{itemize}
    % \item [1.] 
    % By perturbing the density of $\mu_0$ in $\+B_r(v)$, we get a new distribution $\mu_v$. 
    % After perturbation, the mass in $\+B_r(v)$ increases by $\approx \eps$. Then the total variation distance between $\mu_{v}$ and $\mu_{u}$ will be $\approx \eps$ for those $u,v$ such that $\+B_r(u)\cap \+B_r(v)=\emptyset$. Thus, we can reduce problem of sampling with error $\epsilon$ to the problem of recognizing the perturbed regions.
    \item [1.] As the mass of perturbed area is approximately $\eps$ more than the base distribution, each $\+B_r(v_i)$ should be placed inside $\+B_R$ for $R=\+O\tp{\sqrt{\frac{M}{\eps}}}$ to ensure that the second moment of $\mu_{v_i}$ remains $\+O(M)$.
    \item [2.] The radius $r$ must be large enough to guarantee a smooth transition at the boundary, ensuring that $\mu_{v_i}$ remains $\+O(L)$-log-smooth.
\end{itemize}
% Under these conditions, we can find $n=()$ disjoint balls with centers $\ab\{v_i\}_{i\in[n]}$ and $r=\+O\tp{\sqrt{\frac{d}{L}}}$. To recognize the perturbed ball among the $n$ ones, it will take $()$ queries. 
Suppose we pick a Gaussian distribution as the base distribution $\mu_0$ as in the previous work, namely that $\mu_0 = \+N\tp{0,\frac{M}{d}\!{Id}_d}$ (so that the second moment of $\mu_0$ is $M$). Let function $h_0$ and $f_i$ be the log-density of $\+N\tp{0,\frac{M}{d}\!{Id}_d}$ and $\mu_{v_i}$ respectively.  Suppose we want to pack all balls with their centers at $R\cdot\+S_{d-1}$ (the sphere with radius $R$). Then to maintain the second moment of each $\mu_{v_i}$ to be $\+O(M)$, one should pick $R = \+O\tp{\sqrt{\frac{M}{\eps}}}$. For those points $x$ with $\norm{x}\approx R\approx \sqrt{\frac{M}{\eps}}$, we have $h_0(x)\approx \frac{d}{\eps}$. 

% If we pack these balls inside $\+B_{R}$ with $R=\+O\tp{\sqrt{\frac{M}{\eps}}}$, to obtain the desired form lower bound, we want at least $()$ balls and thus we need $r=\+O(\sqrt{\frac{d}{L}})$. 
To guarantee the mass of $\+B_r(v)$ to be $\approx \eps$, we have $\int_{\+B_r(v_i)} \tp{e^{-f_{i}(x) } - e^{-h_0(x)}} \d x \approx \eps$. This indicates that $h_0(x) -f_i(x) \approx \frac{d}{\eps}$ inside $\+B_r(v_i)$. To further guarantee the $\+O(L)$-smoothness of $f_i$, $r$ should be $\Omega\tp{\sqrt{\frac{d}{L\eps}}}$. One can pack approximately $\tp{\frac{R}{r}}^d$ (\Cref{lem:disjointcap}) many disjoint balls with radius $r$ centering at $R\cdot \+S_{d-1}$, which gives a lower bound approximately $\tp{\alpha\cdot \frac{LM}{d}}^{\frac{d}{2}}$ for some universal constant $\alpha>0$. The dependency on $\eps$ has been cancelled, and it is obviously not optimal!
% Then the difference in the potential function between the center of the ball and its outer border needs to be $\Omega\tp{\frac{d}{\eps}}$ and  the potential function in this region is only $\+O\tp{\frac{L}{\eps}}$-smooth rather than $\+O(L)$-smooth. When $\eps$ is small, this falls far short of meeting our third requirement. 

The key trade-off of the construction comes from how to efficiently perturb the mass in each $\+B_r(v)$. Recall that one requires
\[
    \int_{\+B_r(v_i)} \tp{e^{-f_{i}(x) } - e^{-h_0(x)}} \d x \approx \eps
\]
while keeping $r$ small and simultaneously $f_i - h_0$ small. We observe that, ignoring low order terms in $r$, the integral on the LHS is almost equal to $e^{-h_0(x)}e^{h_0(x)-f_i(x)}$. Imagine we are filling a mound with sand within a small circular area at an elevation of $ e^{-h_0(x)} $. We aim for the amount of sand used to satisfy $ e^{-h_0(x)} \cdot e^{h_0(x)-f_i(x)} \approx \eps $, while ensuring the mound is not too steep -- meaning the height difference \( h_0(x) - f_i(x) \) should be minimized. The most efficient approach is to raise the base elevation, i.e., maximize $e^{-h_0(x)}$, thereby reducing the required height increment.
%Moreover, the value of $f_i-h_0$ affects the value of the integral at an exponential rate while affects the value of 

Therefore, at a very high level, our construction for the base distribution $p_{\mu_0}\propto e^{-f_{\mu_0}}$ in the lower bound proof is to first modify the Gaussian by creating a plateau in a ring, and then perturb the mass on the plateau. The construction is illustrated in \Cref{fig:lb}. Of course, the plateau itself may affect the smoothness and the second moment. Nevertheless, we carefully pick the location, shape and the mass of the plateau, and are able to pack approximately $\tp{\alpha\cdot \frac{LM}{d\eps}}^{\frac{d}{2}}$ disjoint balls with radius $r=\tilde{\+O}\tp{\sqrt{\frac{d}{L}}}$, which provides the desired optimal lower bound. 

% We artificially increase the density of $\+N\tp{0,\frac{M}{d}I_d}$ (or equivalently, decreasing the value of $h_0$) on the ring $\+B_R\setminus \+B_{\frac{R}{2}}$ to create a new base distribution $\mu_0$ with density $\propto e^{-f_{\mu_0}}$. Then, we place $\+B_r(v_i)$ with $\|v_i\|=\frac{3R}{4}$ and $r=\tilde{\+O}\tp{\sqrt{\frac{d}{L}}}$ on the ring. By perturbing the values in $\+B_r(v_i)$, we get a new distribution $\mu_{v_i}$. By carefully choosing the density values on the ring, we can ensure that \Cref{assump:moment} still hold for $\mu_0$ and each $\mu_{v_i}$. Most importantly, $f_{\mu_0}(x) - f_i(x) = {\+O}\tp{d\log \frac{LM}{d\eps}} \ll h_0(x) - f_i(x)$ inside $\+B_r(v_i)$. This ensures the $\+O(L)$-smoothness inside $\+B_r(v_i)$ when $r=\+O\tp{\sqrt{\frac{d}{L}}}$. 
% the second moment of the new base distribution is still $\+O(M)$, and the difference in potential function values between the center and outer border of $\+B_r(v)$ is $\+O\tp{d\log \frac{LM}{d\eps}}$. This ensures the $\+O(L)$-smoothness inside and around $\+B_r(v)$. 




%Intuitively, the advantage of this $\mu_0$ is that,  it provides a \emph{high platform} on the ring and the transition from $h_0$ to $f_i$ is partly undertaken by the ring. Although the width of this ring is $\+O(R)$, it is part of the base instance and does not affect the packing of small balls. Therefore, this addresses the issue when using Gaussian as the base instance.

\begin{figure}[h!]
	\centering
    % \ExecuteMetaData[figures.tex]{lbfigure}
    \includegraphics[scale=0.3]{fig_lb.png}
  \caption{Construction of $f_{\mu_0}$ and $f_{\mu_v}$.\protect\footnotemark}
  \label{fig:lb}
\end{figure}
\footnotetext{In the above three figures of \Cref{fig:lb}, the deeper color represents larger density.}


\subsubsection{The upper bound}

%To sample from general non-log-concave distributions under \Cref{assump:moment}~and~\ref{assump:smooth}, the main challenge is that, the target distribution $\mu$ may have extremely small probability values in certain regions, causing the algorithm to get trapped locally and converge slowly. To mitigate this issue, we first modify the potential function $f_{\mu}$ before running a sampling algorithm. 

It is challenging to establish suitable isoperimetric inequalities directly for distributions solely satisfying \Cref{assump:moment} and~\ref{assump:smooth} due to the existence of point with extremely small density. However, we observe that: 1) the mass of the target distribution $\mu$ with density $\propto e^{-f_{\mu}}$ is concentrated in $\+B_{R}$ for a sufficiently large radius $R$; 2) the total mass of the region with extremely small density values inside $\+B_{R}$ is small.
% For those extremely small $\mu(x)$ inside $\+B_{R}$, we can truncate these extreme values of $f_{\mu}$ without affect $\mu$ much.
Based on these observations, we construct another distribution $\pi$  with density $\propto e^{-f_{\pi}}$ which is close to $\mu$ in total variation distance and is easier to sample from. Basically the distribution $\pi$ is Gaussian outside $\+B_R$ and is the truncation (remove points with extremely small density) of $\mu$ inside $\+B_R$. The construction of $f_{\pi}$ can be described in the following steps:
%\htodo{Here I use $\+B_R$ and I don't mention the $\+B_{2R}$ used in \Cref{sec:ub} for simplicity. So here the descriptions are slightly different with our actual implementation. Does this matter?} 
\begin{itemize}
    \item Step 1: Discretize $\+B_R$ into small cubes. Use the value of $f_{\mu}$ at the center of each cube to estimate the values inside the cube. Use these approximations of each cube to calculate a rough estimation of the minimum value $\wh f^* \approx f^*= \min_{x\in \+B_R} f(x)$ and the normalizing factor $\wh Z_{\mu}\approx \int_{\bb R^d} e^{-f_{\mu}(x)} \dd x$.
    \item Step 2: For $x\in \+B_R$, as shown in \Cref{fig:ub}, if $f_{\mu}(x) - \wh f^*$ exceeds some threshold $h_1-\wh f^*$, let $f_{\pi}(x)$ be the smooth truncation of $f_{\mu}(x)$. Otherwise, let $f_{\pi}(x) = f_{\mu}(x)$. By doing so, we guarantee that $f_{\pi}(x) - \wh f^*$ is always bounded by $h_2-\wh f^*$ for some value $h_2$ and extremely small density values in $\pi$ is circumvented.
    \item Step 3: For $x\not \in \+B_R$, %since the impact of this area is minimal, we can adopt a more straightforward construction. As \Cref{fig:ub} shows, 
    we define $f_{\pi}(x)$ by replacing the original density $\frac{e^{-f_{\mu}(x)}}{\wh Z_{\mu}}$ with the density of a Gaussian $\+N\tp{0, \frac{M}{\eps d}\cdot \!{Id}_d}$.
\end{itemize}

\begin{figure}[h!]
	\centering
    % \ExecuteMetaData[figures.tex]{ubfigure}
    \includegraphics[scale=0.35]{fig_ub.png}
  \caption{Construction of $f_{\pi}$.}
  \label{fig:ub}
\end{figure}

By smoothing the above construction appropriately, we can prove the smoothness of $f_{\pi}$ and prove a bound for the \Poincare constant of $\pi$. Besides, the value of $f_{\pi}$ and $\grad f_{\pi}$ can be efficiently calculated given query access to $f_{\mu}$ and $\grad f_{\mu}$. Then we can apply the averaged Langevin algorithm in \cite{BCE+22} to sample from $\pi$ and the output distribution is also close to $\mu$ in total variation distance.

Note that in Step 1, we use a grid-based approximation to find $\wh f^*$, which is an optimization task. However, the comparison between \Cref{thm:main-ub,thm:main-opt-lb} shows that our sampling algorithm has lower complexity than solving the optimization problem itself. This is because the required precision for this optimization in our task is very low (see \Cref{prop:Z-and-fmin}). We make a more thorough discussion on the theme ``sampling versus optimization'' in \Cref{sec:sampling-vs-opt}.

%Therefore, even though our sampling algorithm first requires solving an optimization problem, the sampling task is actually simpler than the optimization. 

% The idea of truncation in step 2 has also been used in a recent work~\cite{Cha24}, with different implementation and purpose. Their goal is to ensure the effectiveness of importance sampling. They truncate regions where the potential function is small, transforming the original distribution into a log-concave one to facilitate approximate sampling. In contrast, we truncate regions with large potential values (i.e., very small density), and to keep $\pi$ close enough to $\mu$, our truncation must be more sophisticated.
%\htodo{Do we need this comparison with the technique in \cite{Cha24}? If need, please check whether this is correct.}
%\ctodo{I think it is not necesssary.}



\section{Preliminaries}

\paragraph{Notations}
In this paper, all logarithms refer to the natural logarithms with base $e$. 
For two distributions $\mu$ and $\nu$ over $\bb R^d$ with density function $p_{\mu}$ and $p_{\nu}$, the total variation distance is defined as $\DTV(\mu,\nu)=\frac{1}{2}\int_{\bb R^d}\abs{p_\mu(x)-p_\nu(x)}\dd x$. The Kullback-Leibler divergence (KL divergence) is defined as $\!{KL}(\mu\|\nu)=\E[X\sim \mu]{\log \frac{p_\mu(X)}{p_\nu(X)}}$. 

Unless otherwise specified, the distributions we consider in this paper are all under \Cref{assump:moment}~and~\ref{assump:smooth}. We say a distribution $\mu$ is $L$-log-smooth if its potential function is $L$-smooth.  We always assume $LM\geq d$ because from \Cref{lem:lb-LM}, $LM$ is lower bounded by $d$ as long as $\grad f(0)=0$.

We use the notation $\+N(u,\Sigma)$ to denote a Gaussian distribution over $\bb R^d$ with mean $u\in \bb R^{d}$ and covariance matrix $\Sigma\in \bb R^{d\times d}$. We use $\!{Id}_d$ to denote the identity matrix in $\bb R^{d\times d}$ and let $\|A\|_{\!{op}}$ denote the operator norm of a matrix $A$.

% \paragraph{Balls in $d$ dimension}

For a ball centered at a point $v\in \bb R^d$ with radius $R$, we denote it as $\+B_R(v)$. When $v=0$, we abbreviate $\+B_R(v)$ as $\+B_R$.
%  Recall that $\!{vol}(\+B_R) = \frac{(\pi R^2)^{\frac{d}{2}}}{\Gamma\tp{\frac{d}{2}+1}}$.
%  We also find the following bounds for $\!{vol}(\+B_R)$ useful.

% \begin{proposition}\label{prop:dballvolbound}
% $\tp{\frac{e\pi R^2}{d}}^{\frac{d}{2}}\le\!{vol}\tp{\+B_R} \le \tp{\frac{2e\pi R^2}{d}}^{\frac{d}{2}}$.
 
% \end{proposition}

\paragraph{The mollifier}\label{sec:mollifier}

Define the function $q_{\!{mol}}\colon \bb R\to [0,1]$ as 
$
q_{\!{mol}}(z) = 
    \begin{cases}
        0, & z<0,\\
        6z^5-15z^4+10z^3, &z\in [0,1],\\
        1, & z>1.                
    \end{cases}
$
It is a mollifier between $0$ and $1$, and it is easy to see that $q_{\!{mol}}$ has the following properties.
\begin{proposition}
    The following holds for $q_{\!mol}$.
    \begin{itemize}
        \item For $z\le 0$, $q_{\!{mol}}(z)=0$; for $z\ge 1$, $q_{\!{mol}}(z)=1$; and for $z\in (0,1)$, $q_{\!{mol}}(z)\in (0,1)$.
        \item $q'_{\!{mol}}(0)=q'_{\!{mol}}(1)=0$ and for each $z\in [0,1]$, $\abs{q'_{\!{mol}}(z)}<\infty$.
        \item $q''_{\!{mol}}(0)=q''_{\!{mol}}(1)=0$ and for each $z\in [0,1]$, $\abs{q''_{\!{mol}}(z)}<\infty$.
    \end{itemize}
\end{proposition}

% \paragraph{The Markov's inequality} For any non-negative random variable $X$ and any positive real number $s$, the Markov's inequality states that 
% \[
%     \Pr{X\geq s}\leq \frac{\E{X}}{s}.
% \]

% For each $q\in (1,\infty)$, the Renyi divergence between two distributions $\nu$ and $\pi$ is defined as
% \[
%     R_q(\nu\|\pi) = \frac{1}{q-1}\ln\tp{\bigg\|\frac{\dd \nu}{\dd \pi}\bigg\|^q_{L^q(\pi)}}
% \]




\paragraph{The Langevin dynamics and Ornstein-Uhlenbeck process}
The Langevin dynamics is a continuous-time process $\ab\{X_t\}_{t\geq 0}$ described in the following stochastic differential equation:
\begin{equation}
    \d X_t = - \grad f(X_t)\d t + \sqrt{2}\d B_t, \label{eq:LD}
\end{equation}
where $f:\bb R^d\to \bb R$ is a differentiable function and $\ab\{B_t\}_{t\geq 0}$ is the standard Brownian motion. The Ornstein-Uhlenbeck process (OU process) is a special case of \Cref{eq:LD} with $f(x) = \frac{\|x\|^2}{2}$. It is well known that the law of $X_t$ always converges to the standard Gaussian distribution in the OU process.

\paragraph{The \Poincare inequality}
We say a distribution $\mu$ satisfies the \Poincare inequality with a constant $C>0$ if for all $f\in C^1(\bb R^d)$ with $\E{f^2}< \infty$, it holds that
\begin{equation}\label{eqn:def-PI}
    \Var[\mu]{f} \le \frac{1}{C}\cdot \E[\mu]{\|\grad f\|^2}.
\end{equation}
We use $C_{\!{PI}}$ to denote the largest $C>0$ so that \eqref{eqn:def-PI} holds. We also call $C_{\!{PI}}$ the \Poincare constant of $\mu$. 

% \subsection{The log-Sobolev inequality}

% We say a distribution $\mu$ satisfies the log-Sobolev inequality with a constant $C>0$ if for all $f\in C^1(\bb R^d)$ with $\E{f^2}< \infty$, it holds that
% \begin{equation}\label{eqn:def-LSI}
%     \Ent{f^2} \le \frac{2}{C}\cdot \E{\norm{\grad f}^2},
% \end{equation}
% where $\Ent{g}$, the entropy of a function $g$, is defined as
% \[
%     \Ent{g} \defeq \E{g\log g}-\E{g}\E{\log g}.
% \]
% We use $C_{\!{LSI}}$ to denote the largest $C>0$ so that \eqref{eqn:def-LSI} holds. We also call $C_{\!{LSI}}$ the log-Sobolev constant of $\mu$. 

\input{lower-bound}

\input{upper-bound}

\input{smoothness}

\input{optimization}

\section{Conclusion and open problems}

In the paper, we studied the query complexity of sampling from $L$-log-smooth distributions with the second moment at most $M$. For this family of distributions, we established a $\tp{\frac{LM}{d\eps}}^{\Theta(d)}$ query complexity bound. It is an interesting question to explore the correct constant in the exponent. 
\begin{problem}
    Determine the infimum $c>0$ such that the query complexity of sampling from an $L$-log-smooth distribution with the second moment at most $M$ is $\tp{\alpha\cdot \frac{LM}{d\eps}}^{cd}$ for some universal constant $\alpha>0$.
\end{problem}
In light of our lower bound proof, we conjecture that the correct constant is $\frac{1}{2}$. In fact, our algorithm for the upper bound compromised significantly on this constant during the truncation of target distribution. Our approach also has the drawback of only applying to bounding the total variation distance. An optimal algorithm might rely on directly establishing functional inequalities (e.g. weak \Poincare inequality~\cite{HMRW24}) for the target distribution.

\begin{problem}
    Establish tight functional inequalities for $L$-log-smooth distributions with the second moment at most $M$.
\end{problem}

One of the motivation of this work is to understand the extent to which the quasi-polynomial time algorithm of~\cite{HZD+24} applies. As we investigated in \Cref{sec:OU-smooth}, finding a criterion for being $\+O(1)$-smooth along the trajectory of the OU process is a challenging task.

\begin{problem}
    Understand to what extent a diffusion process (not restricted to the OU process in ~\cite{HZD+24}) can maintain the $\+O(1)$-smoothness of the initial distribution. Can this condition result in efficient algorithm as well?
\end{problem}

Finally, can we characterize the query complexity for sampling from general non-log-concave distributions. Even in the case of mixtures of Gaussians, this remains a challenging problem.

\begin{problem}
    Characterize the condition under which sampling from a mixture of Gaussians has sub-exponential query complexity.    
\end{problem}



\section*{Acknowledgements}

The authors would like to thank Zongchen Chen for bringing the Hubbard-Stratonovich transform of Ising model into our attention, in particular its connection with multi-modal sampling. 
\bibliographystyle{alpha}
\bibliography{arxiv}
\appendix
\input{appendix}

\end{document}
