\section{The smoothness conditions}\label{sec:OU-smooth}

In this section, we will compare \Cref{assump:smooth} with the smoothness assumption in \cite{HZD+24} and prove \Cref{thm:main-smooth}.

Recall that we assume the target distribution $\mu$ with density $p_{\mu}$ to be $L$-log-smooth. This assumption is typically essential for bounding the discretization error of sampling algorithms. In many works based on denoising diffusion probabilistic models (DDPMs) (e.g. \cite{CCL+23,LLT23,CLL23,HZD+24}), they further assume that the distributions during the OU process starting from $\mu$ are also $L$-log-smooth. 
% \ctodo{What is the $L$-smoothness assumption in those ``score function estimation $\implies$ good sampler'' works}

The definition for the OU process is as follows. Suppose we start from a random point $X_0\sim \mu$. The OU process $\set{X_t}_{t\geq 0}$ evolves with the following equation
\[
    \d X_t = -X_t \dd t + \sqrt{2}\d B_t
\]
where $\set{B_t}_{t>0}$ is the standard Brownian motion. The solution of the above equations is
\begin{equation}
    X_t = e^{-t}X_0 + \sqrt{2}\cdot e^{-t}\int_{0}^t e^s \d B_s. \label{eq:OU}
    % X_t = e^{-t}X_0 + \sqrt{1-e^{-2t}} Z_t, \label{eq:OU}
\end{equation}
From direct calculation, we know that $
\sqrt{2}\cdot e^{-t}\int_{0}^t e^s \d B_s \sim \+N\tp{0, (1-e^{-2t})I_d}$.
% where $Z_t$ is drawn from a standard Gaussian distribution.

There have been many convergence guarantees for the DDPMs.  Let $\mu_t$ be the distribution of $X_t$ and $p_t$ be the corresponding density function. Assuming the second-moment of $\mu$ is bounded by $M$ and $\log p_t$ being $L$-smooth for any $t\geq 0$, the work of \cite{HZD+24} proposed an algorithm that guarantees with high probability, the output distribution is $\tilde{\+O}(\eps)$-close to the target distribution in KL divergence, requiring at most $\exp\set{\+O\tp{L^3\cdot \log^3\frac{Ld+M}{\eps}} \cdot \max\ab\{\log\log Z^2,1\}}$ queries \footnote{Here $Z$ is the maximum norm of particles appeared in their algorithm.}. 
% This implies that as long as the smoothness condition of $\log p_t$ is satisfied with constant $L$, a quasi-polynomial sampling algorithm exists. 

It has been known that a smooth $\log p_{\mu}$ can imply the smoothness of $\log p_t$ in some specific cases, for example when $t$ is small (\cite{CLL23}) or when $\mu$ is strongly log-concave (\cite{LPSR21}). Via the techniques in Lemma~12 of \cite{CLL23}, one can prove an $O(d)$ upper bound of $\|\grad^2 \log p_t\|_{\!{op}}$ in expectation when $t=\Omega(1)$. However, this bound does not offer much utility for the algorithm of \cite{HZD+24}, as their results will be super-exponential under an $O(d)$-smoothness bound.

If the smoothness bound for $\log p_{\mu}$ is large, the bound in \cite{HZD+24} will be poor. Fortunately, we can assume $\log p_{\mu}$ to be $\+O(1)$-smooth without loss of generality. This is because we can always scale the domain to adjust the distribution's smoothness bound, while not changing the product of the smoothness bound and the second moment $M$ (\Cref{lem:LM}). Therefore, with the initial distribution being $\+O(1)$-log-smooth and the second moment being polynomial in $d$, if the $\log p_t$'s also remain $\+O(1)$-smooth, quasi-polynomial sampler exists. 
% So the relationship between the smoothness of the initial distribution and that of the distributions during the OU process is worth studying. 

% Hence previous works do not pay much attention to distinguishing between these two smoothness conditions.

Therefore, we are interested in the conditions for the $O(1)$-smoothness to be kept during the entire OU process. Our results in \Cref{thm:main-lb} indicates an exponential lower bound even for $O(1)$-log-smooth distributions. Then we know that for those hard instances constructed in \Cref{sec:lb}, $\log p_t$ cannot always be $O(1)$-smooth during the OU process. Otherwise a quasi-polynomial sampler exists from \cite{HZD+24}. 
% we found that the smoothness bound for $\log p_t$ can be significantly worse than that of the initial $\log p_{\mu}$.
In \Cref{subsec:stitched}, we introduce a family of $\+O(1)$-log-smooth distribution, which we refer to as the stitched Gaussian distributions and is a simplified version of those hard instances in \Cref{sec:lb}. As the OU process evolves, the bound on the smoothness of stitched Gaussians can become $\omega(1)$ at certain time $t$. 
% Given this counterexample, the relationship between the smoothness of the initial distribution and that of the distributions during the OU process is worth studying. 

To see the case for other non-log-concave distributions, in \Cref{subsec:mix}, we considered a class of classical multi-modal distributions, the mixture of Gaussians, and provide analyses of their smoothness properties with different parameter settings. Although mixture of Gaussians appear to be quite similar to the stitched Gaussians, our results demonstrate that they exhibit fundamentally different behaviors in terms of smoothness. To be specific, we show that for a mixture of two Gaussians with mean $u_1$ and $u_2$, if their covariance matrices $\Sigma_1=\Sigma_2\succeq \Omega(1)\cdot \!{Id}_d$, the smoothness of the distributions are almost determined by $\|u_1-u_2\|$ and the $\log p_t$'s will inherit the $\+O(1)$-smoothness is $\log p_{\mu}$ is $\+O(1)$-smooth. In contrast, when the covariance matrices differ, even with $\|u_1-u_2\|=o(1)$, the initial $\log p_{\mu}$ is not $L$-smooth for any $L=o(d)$. 

We also explore the mixture of multiple Gaussians in \Cref{subsec:mix}. For those cases with more components, the analysis becomes more complex. Even when all covariance matrices are the same, it is challenging to derive a concise rule to characterize the relationship between smoothness and the distances between the means of the Gaussians. We give an example where the centers of components are far apart, yet the mixture distribution remains $\+O(1)$-log-smooth, and this smoothness is preserved during the OU process.

An overview of the main results of this section is given in \Cref{tab:result-comp}, where we show the smoothness bounds of $\log p_{\mu}$ and corresponding $\log p_t$ in different cases, as well as whether the $\+O(1)$-smoothness property is preserved during the OU process.

\begin{table*}[htbp]
	\centering
	\caption{The Comparison between the Smoothness Bounds}
	\label{tab:result-comp}
  \begin{threeparttable}
\begin{tabular}{m{1.7cm}<{\centering}m{2.7cm}<{\centering}m{3.9cm}<{\centering}m{3.7cm}<{\centering}m{2cm}<{\centering}}
	\toprule
	& {Parameters \textcolor{red}{\tnote{1}}} & {Smoothness Bound for $\log p_{\mu}$} & {Smoothness Bound for $\log p_t$} & {Keep $\+O(1)$-smooth?}\\
	\midrule
	Stitched Gaussian & \shortstack{$m=2$ \\ $\Sigma_1=\Sigma_2=\!{Id}_d$ \\ $\|u_1-u_2\|^2 = s = \Omega(d)$} & \shortstack{$\+O(1)$\\ (\Cref{lem:stitchsmooth})} & \shortstack{ $\Omega\tp{e^{-2t}s - 1}$ \textcolor{red}{\tnote{2}}
        %$\+O(\max\ab\{1,e^{-2t}s\})$
    \\ for $t>\frac{\log 10}{2}$\\ (\Cref{thm:stitched2})} & No \\
    \hline 
    Mixture of Gaussians & \shortstack{$m=2$\\
    $\Sigma_1=\Sigma_2\succeq \Omega(1)\cdot \!{Id}_d$ \\
    $\|u_1-u_2\|^2=s$} & \shortstack{$\+O(\max\ab\{1,s\})$\\ (\Cref{lem:2-same})} & \shortstack{$\+O(\max\ab\{1,e^{-2t}s\})$\\ (\Cref{lem:2-same})} & Yes \\
    \hline 
    Mixture of Gaussians & \shortstack{$m=2$\\
    $\Sigma_1=\frac{\!{Id}_d}{2},\Sigma_2= \!{Id}_d$ \\
    $\|u_1-u_2\|=o(1)$} & \shortstack{$\Omega(d)$\\ (\Cref{lem:2-diff})} & - & NA \\
    \hline 
    Mixture of Gaussians & \shortstack{$m=2^d$\\
    $\Sigma_1=\cdots=\Sigma_m=J$ \\
    $\|u_i-u_j\|^2= \+O(d)$ \textcolor{red}{\tnote{3}}} & \shortstack{$\+O(1)$\\ (\Cref{lem:mixture1})} & \shortstack{$\+O(1)$\\ (\Cref{cor:mixture2})} & Yes \\
	\bottomrule
\end{tabular}
\begin{tablenotes}
	\footnotesize
    \item[\textcolor{red}{1}] Note that both the stitched Gaussian and the mixture of Gaussians are constructed based on Gaussian distributions with different parameters. We assume the Gaussian distributions used in the construction are $\+N(u_1,\Sigma_1),\+N(u_2,\Sigma_2), \dots, \+N(u_m,\Sigma_m)$ respectively.
    \item[\textcolor{red}{2}] We say the smoothness bound for a function $f$ is $\Omega(c)$ if there exists some $x\in \bb R^d$ such that $\norm{\grad^2 f(x)}_{\!{op}} = \Omega(c)$.
	\item[\textcolor{red}{3}] Here $J$ is a symmetric matrix with $\delta_{\!{Id}_d}\preceq J \preceq (1-\delta)\!{Id}_d$ for some constant $\delta\in (0,1/2)$. For the detailed construction of $\ab\{u_i\}_{i\in[m]}$, see \Cref{eq:HS-mix}.
  \end{tablenotes}
\end{threeparttable}
\end{table*}

\subsection{The stitched Gaussian distributions}\label{subsec:stitched}

In this section,  we show that the smoothness bound of $\log p_t$ can differ significantly from that of $\log p_{\mu}$ on the \emph{stitched Gaussian distributions}.
% we prove that the smoothness bound for $\log p$ and $\log p_t$ can vary significantly when $p$ is a stitched Gaussian distribution. 

The stitched Gaussian distributions are a class of distributions constructed by interpolating between multiple Gaussian components. Recall that $q_{\!{mol}}$ is the mollifier defined in \Cref{sec:mollifier}. Let $u\in \bb R^d$ be a vector satisfying $\|u\|^2 \geq 100 d$. Define $\mathfrak{g}_{u}(x) = q_{\!{mol}}\tp{10\tp{\frac{\|x-u\|}{\|u\|} - 0.4}}$ and 
\[
    f_{\mu}(x) = \mathfrak{g}_{u}(x)\cdot \frac{\|x\|^2}{2} + (1-\mathfrak{g}_{u}(x))\cdot \frac{\|x-u\|^2}{2}.
\]  
The specific type of stitched Gaussians we consider here has a density function of $p_{\mu}\propto e^{-f_{\mu}}$.

% When the information is clear from context, we abbreviate $\mathfrak{g}_{\left[\frac{2\|u\|}{5},\frac{\|u\|}{2}\right]}$ as $\mathfrak{g}$ for simplicity.
Let $r=\|u\|$. We can divide $\bb R^d$ into two parts $\+B_{\frac{r}{2}}(u) = \set{x\in \bb R^d: \| x - u\| \leq 0.5 \|u\|}$ and $\ol{\+B_{\frac{r}{2}}(u)} = \bb R^d \setminus \+B_{\frac{r}{2}}(u)$. By definition of $\mathfrak{g}_{u}$, for $x\in \ol{\+B_{\frac{r}{2}}(u)}$, $f_{\mu}(x) = \frac{\|x\|^2}{2}$. Furthermore, inside $\+B_{\frac{r}{2}}(u)$, when $x\in \+B_{\frac{2r}{5}}(u) =\set{x\in \bb R^d: \| x - u\| \leq 0.4 \|u\|}$, $f_{\mu}(x) = \frac{\|x-u\|^2}{2}$. In $\+B_{\frac{r}{2}}(u)\setminus \+B_{\frac{2r}{5}}(u)$, these two Gaussians are ``stitched together'', which means the density function transitions smoothly from a Gaussian distribution centered at $0$ to another Gaussian distribution centered at $u$.

\subsubsection{The smoothness of $\log p_{\mu}$}\label{subsec:stitched1}

We first prove that $\log p_{\mu}$ is indeed $\+O(1)$-smooth.

\begin{lemma}\label{lem:stitchsmooth}
    The function $\log p_{\mu}$ is $\+O(1)$-smooth for any $u\in \bb R^d$.
\end{lemma}
\begin{proof}
    For $x\in \+B_{\frac{2r}{5}}(u)$, we know that $f_{\mu}(x) = \frac{\|x-u\|^2}{2}$. Therefore $\grad^2 \log p_{\mu}(x) = \grad^2 f_{\mu}(x) = \!{Id}_d$. Similarly, for $x\in \ol{\+B_{\frac{r}{2}}(u)}$, $f_{\mu}(x)=\frac{\|x\|^2}{2}$ and $\grad^2 \log p_{\mu}(x) = \grad^2 f_{\mu}(x) = \!{Id}_d$. So it only remains to deal with those $x\in \+B_{\frac{r}{2}}(u) \setminus \+B_{\frac{2r}{5}}(u)$.

    % Let $\+B_{\frac{r}{2}}(u)\setminus \+B_{\frac{2r}{5}}(u) = \+B_{\frac{r}{2}}(u)\setminus \+B_{\frac{2r}{5}}(u) = \set{x\in \bb R^d:0.4 \|u\| < \| x - u\| \leq 0.5 \|u\|}$.
    For $x\in \+B_{\frac{r}{2}}(u)\setminus \+B_{\frac{2r}{5}}(u)$, 
    \begin{align*}
        \grad f_{\mu}(x) = \mathfrak{g}_{u}(x)\cdot x + \frac{\|x\|^2}{2} \cdot \grad \mathfrak{g}_{u}(x) - \frac{\|x - u\|^2}{2}\cdot \grad \mathfrak{g}_{u}(x)+ (1-\mathfrak{g}_{u}(x))\cdot (x-u)
    \end{align*}
    and consequently,
    \begin{align}
        \grad^2 f_{\mu}(x) &= \underbrace{x\cdot \grad \mathfrak{g}_{u}(x)^{\top}}_{(a_1)} + \underbrace{\grad \mathfrak{g}_{u}(x)\cdot  x^{\top}}_{(b_1)} + \underbrace{\mathfrak{g}_{u}(x)\cdot \!{Id}_d}_{(c_1)} + \underbrace{\frac{\|x\|^2}{2}\cdot \grad^2 \mathfrak{g}_{u}(x)}_{(d_1)} \notag \\
        &\quad - \underbrace{(x-u)\cdot \grad \mathfrak{g}_{u}(x)^{\top}}_{(a_2)} - \underbrace{\grad \mathfrak{g}_{u}(x)\cdot (x-u)^{\top}}_{(b_2)} + \underbrace{(1-\mathfrak{g}_{u}(x))\cdot \!{Id}_d}_{(c_2)} -\underbrace{\frac{\|x - u\|^2}{2}\cdot \grad^2 \mathfrak{g}_{u}(x)}_{(d_2)}. \label{eq:1}
    \end{align}

    By the definition of $\mathfrak{g}_{u}(x)$, we have
     \[
        % \grad \mathfrak{g}_{u}(x) = \frac{10(x-u)}{\|x-u\|\cdot \|u\|}\cdot  \frac{\dd q}{\dd y}\Bigg|_{y=10\tp{\frac{\|x-u\|}{\|u\|}-0.4}}
        \grad \mathfrak{g}_{u}(x) = \frac{10(x-u)}{\|x-u\|\cdot \|u\|}\cdot  q'_{\!{mol}}\tp{10\tp{\frac{\|x-u\|}{\|u\|}-0.4}}
        % \grad h\tp{10\tp{\frac{\|x-2u\|}{\|u\|}-1.5}}
     \]
     and 
     \begin{align*}
        %  \grad^2 \mathfrak{g}_{u}(x) &= \frac{100(x-u)(x-u)^{\top}}{\|x-u\|^2\cdot \|u\|^2}\cdot  \frac{\dd^2 q}{\dd y^2}\Bigg|_{y=10\tp{\frac{\|x-u\|}{\|u\|}-0.4}} \\
        %  &\quad + \frac{10}{\|u\|}\tp{\frac{\!{Id}_d}{\|x-u\|} - \frac{(x-u)(x-u)^{\top}}{\|x-u\|^3}} \cdot  \frac{\dd q}{\dd y}\Bigg|_{y=10\tp{\frac{\|x-u\|}{\|u\|}-0.4}}.
        \grad^2 \mathfrak{g}_{u}(x) &= \frac{100(x-u)(x-u)^{\top}}{\|x-u\|^2\cdot \|u\|^2}\cdot  q''_{\!{mol}}\tp{10\tp{\frac{\|x-u\|}{\|u\|}-0.4}} \\
        &\quad + \frac{10}{\|u\|}\tp{\frac{\!{Id}_d}{\|x-u\|} - \frac{(x-u)(x-u)^{\top}}{\|x-u\|^3}} \cdot  q''_{\!{mol}}\tp{10\tp{\frac{\|x-u\|}{\|u\|}-0.4}}.
     \end{align*}

     Then we calculate the terms in \Cref{eq:1} one by one. We have 
     \[ 
        (a_1) = \frac{10x(x-u)^{\top}}{\|x-u\|\cdot \|u\|}\cdot  q'_{\!{mol}}\tp{10\tp{\frac{\|x-u\|}{\|u\|}-0.4}}.
    \]
    Since $x\in 
    \+B_{\frac{r}{2}}(u)\setminus \+B_{\frac{2r}{5}}(u)$, we have $\|x-u\|=\Theta\tp{\|u\|}$ and $\|x\|=\Theta\tp{\|u\|}$. Recall that $q'_{\!{mol}}=\+O(1)$. So we have $\+O(1)\cdot \!{Id}_d\mge (a_1)\mge -\+O(1)\cdot \!{Id}_d$. We can also prove such bounds for $(a_2), (b_1)$ and $(b_2)$ in the same way.

    For the terms $(c_1)$ and $(c_2)$, we know that $\mathfrak{g}_{u}(x)\in [0,1]$ for all $x\in \+B_{\frac{r}{2}}(u)\setminus \+B_{\frac{2r}{5}}(u)$. Therefore, we have $\+O(1)\cdot \!{Id}_d\mge (c_1)\mge 0$ and $\+O(1)\cdot \!{Id}_d\mge (c_2)\mge 0$.

    For $x\in \+B_{\frac{r}{2}}(u)\setminus \+B_{\frac{2r}{5}}(u)$, $\|x\|^2=\Theta(\|u\|^2)$ and $\|x-u\|^2=\Theta(\|u\|^2)$. Since $q''_{\!{mol}}$ and $q'_{\!{mol}}$ are all $\+O(1)$, we have that $\+O\tp{\frac{1}{\|u\|^2}}\cdot \!{Id}_d\mge \grad^2 \mathfrak{g}_{u}(x) \mge 0$. Therefore, $\+O(1)\cdot \!{Id}_d\mge (d_1)\mge 0$ and $\+O(1)\cdot \!{Id}_d\mge (d_2)\mge 0$.

    Combining all these together, we know that $\log p_{\mu}$ is $\+O(1)$-smooth.
\end{proof}

\subsubsection{The smoothness of $\log p_t$}\label{subsec:stitched2}

We then show that for arbitrary $u\in \bb R^d$ with $\|u\|^2\geq 100d$, when $t$ satisfies $e^{-2t}<0.1$, there exists $x_0\in \bb R^d$ such that $\|\grad^2 \log p_t(x_0)\|_{\!{op}}\geq \Omega\tp{e^{-2t}\|u\|^2-1}$.
% $\log p_t(x_0) \not \mle L\cdot \!{Id}_d$ for any $L=o(\max\ab\{e^{-2t}\|u\|^2,1\})$. 
Combining this with the results in \Cref{subsec:stitched1}, it indicates that the smoothness bound for $\log p_t$ can be much larger than that for $\log p_{\mu}$. In other words, the smoothness of the initial distribution does not necessarily imply the smoothness during the OU process. 

We first see how the distribution evolves during the process. From \Cref{eq:OU}, for any $x\in \bb R^d$,
\[
    p_t(x) \propto \int_{\bb R^d} p\tp{\frac{y}{e^{-t}}} \cdot e^{-\frac{\|x-y\|^2}{2(1-e^{-2t})}} \dd y.
\]
For a fixed $x_0\in \bb R^d$, consider the distribution $\nu_t$ with density $q_t(y)\propto p_{\mu}\tp{\frac{y}{e^{-t}}} \cdot e^{-\frac{\|x_0-y\|^2}{2(1-e^{-2t})}}$ for each $y\in \bb R^d$. To calculate the Hessian of $\log p_t$, we use \Cref{prop:stitched-decomp}.
\begin{proposition}[Corollary of Lemma 22 in \cite{CLL23}]\label{prop:stitched-decomp}
    We have $\grad^2 \log p_t(x_0) = \frac{1}{(1-e^{-2t})^2}\cdot \!{Cov}_{Y\sim \nu_t}[Y] - \frac{\!{Id}_d}{1-e^{-2t}}$.
\end{proposition}

In the remaining part of this section, we aim to prove the following lemma.
\begin{theorem}\label{thm:stitched2}
    For arbitrary $u\in \bb R^d$ with $\|u\|^2\geq 100d$, when $t$ satisfies $e^{-2t}<0.1$ and when $x_0=\frac{e^{-t}u}{2}$, $\|\!{Cov}_{Y\sim \nu_t}[Y]\|_{\!{op}} = \Omega\tp{e^{-2t}\|u\|^2}$ and consequently, $\|\grad^2 \log p_t(x_0)\|_{\!{op}}\geq \Omega\tp{e^{-2t}\|u\|^2-1}$.
    % for any $L=o(\max\ab\{e^{-2t}\|u\|^2,1\})$, $\!{Cov}_{Y\sim \nu_t}[Y] \not\mle L \cdot \!{Id}_d$ and consequently, $\grad^2 \log p_t(x_0)\not \mle L\cdot \!{Id}_d$.
\end{theorem}

We will always assume $x_0=\frac{e^{-t}u}{2}$ in the following analyses. Before calculating $\!{Cov}_{Y\sim \nu_t}[Y]$, we see some basic properties of the distribution $\nu_t$. 

Define $c_t = \frac{d}{2}\log \tp{2\pi \sigma_t^2}$ where $\sigma_t^2 =  e^{-2t} (1-e^{-2t})$. Let $\+N_1$ and $\+N_2$ represent the two Gaussian distributions $\+N\tp{\frac{e^{-3t}u}{2}, \sigma_t^2 \cdot \!{Id}_d}$ and $\+N\tp{e^{-t}\tp{1-\frac{e^{-2t}}{2}}u, \sigma_t^2 \cdot \!{Id}_d}$ respectively. Let $h_1$ and $h_2$ be the potential function of these two Gaussian distributions. That is
\[
    h_1(y) = \frac{\norm{y-\frac{e^{-3t}u}{2}}^2}{2e^{-2t}(1-e^{-2t})} + c_t \quad \mbox{and}\quad h_2(y) = \frac{\norm{y- \tp{e^{-t}\tp{1-\frac{e^{-2t}}{2}}u }}^2}{2e^{-2t}(1-e^{-2t})} + c_t.
\]
Define $f_t:\bb R^d \to \bb R$ as 
\begin{align*}
    f_t(y) &= \mathfrak{g}_{u}\tp{\frac{y}{e^{-t}}}\cdot h_1(y) + \tp{1-\mathfrak{g}_{u}\tp{\frac{y}{e^{-t}}}}\cdot  h_2(y). 
\end{align*}

\begin{lemma}\label{lem:stitched0}
    With $x_0=\frac{e^{-t}u}{2}$, we have $q_t(y) \propto e^{-f_t(y)}$.
\end{lemma}

The proof of \Cref{lem:stitched0} is provided in \Cref{subsec:proof2}. Recall that $\+B_{\frac{r}{2}}(u) = \set{x\in \bb R^d: \| x - u\| \leq 0.5 \|u\|}$ and $\+B_{\frac{2r}{5}}(u) = \set{x\in \bb R^d: \| x - u\| \leq 0.4 \|u\|}$. For a set $S\subseteq \bb R^d$ and a real number $c\neq 0$, let $c\cdot S = \set{x\in \bb R^d: \frac{x}{c}\in S}$ be the scaled set. Therefore, outside the ball $e^{-t}\cdot \+B_{\frac{r}{2}}(u)$, $\mathfrak{g}_{u}\tp{\frac{y}{e^{-t}}}=1$ and $f_t(y)\equiv h_1(y)$. Inside $e^{-t}\cdot \+B_{\frac{r}{2}}(u)$, the potential function $f_t$ is the interpolating of $h_1$ and $h_2$ and when $y\in e^{-t}\cdot \+B_{\frac{2r}{5}}(u)$, $f_t(y)\equiv h_2(y)$. 

We claim that the density $q_t$ can be decomposed into a Gaussian density $e^{-h_1}$ plus some density function $p_{\gamma_t}$ which is only supported on $e^{-t}\cdot \+B_{\frac{r}{2}}(u)$. That is, we can find $\delta_t\in (0,1)$ and a distribution $\gamma_t$ with density $p_{\gamma_t}$ such that for any $y\in \bb R^d$, $ q_t(y) = (1-\delta_t) e^{-h_1(y)} + \delta_t\cdot p_{\gamma_t}(y)$. The existence of such $\delta_t$ and $\gamma_t$ is guaranteed by the properties given in the following two lemmas. The proofs of \Cref{lem:stitched3,lem:normalizing} are given in \Cref{subsec:proof2}.

\begin{lemma}\label{lem:stitched3}
    For any $y\in e^{-t}\cdot \+B_{\frac{r}{2}}(u)$, $e^{-h_1(y)} \leq e^{-f_t(y)}$.
\end{lemma}
Let $Z_t=\int_{\bb R^d} e^{-f_t(y)} \dd y$ be the normalizing factor.
\begin{lemma}\label{lem:normalizing}
    When $e^{-2t}<0.1$ and $\|u\|^2\geq 100d$, we have $1.8\leq Z_t\leq 2$.
\end{lemma}


Furthermore, we can prove that $\delta_t=\Theta(1)$ for any $t$ satisfying $e^{-2t}<0.1$.

\begin{lemma}\label{lem:stitched4}
    We have $0.3 < \delta_t < 0.7$ when $e^{-2t}<0.1$ and $\|u\|^2\geq 100d$.
\end{lemma}

We prove \Cref{lem:stitched4} in \Cref{subsec:proof2}. Equipped with these lemmas, we are now ready to prove \Cref{thm:stitched2}.
\begin{proof}[Proof of \Cref{thm:stitched2}]
    We first decompose $\!{Cov}_{Y\sim \nu_t}[Y]$.
    By definition,
    \begin{align*}
        \!{Cov}_{Y\sim \nu_t}[Y] &= \E[Y\sim \nu_t]{(Y-\E[\nu_t]{Y})(Y-\E[\nu_t]{Y})^{\top}} \\
        &= (1-\delta_t)\cdot \E[Y\sim \+N_1]{(Y-\E[\nu_t]{Y})(Y-\E[\nu_t]{Y})^{\top}} + \delta_t\cdot \E[Y\sim \gamma_t]{(Y-\E[\nu_t]{Y})(Y-\E[\nu_t]{Y})^{\top}} \\
        &= (1-\delta_t)\cdot \E[Y\sim \+N_1]{(Y-\E[\+N_1]{Y})(Y-\E[\+N_1]{Y})^{\top}} \\
        &\quad + (1-\delta_t) \cdot \tp{\E[\+N_1]{Y} - \E[\nu_t]{Y}}\tp{\E[\+N_1]{Y} - \E[\nu_t]{Y}}^{\top} \\
        &\quad  + \delta_t \cdot \E[Y\sim \gamma_t]{(Y-\E[\gamma_t]{Y})(Y-\E[\gamma_t]{Y})^{\top}} + \delta_t \cdot \tp{\E[\gamma_t]{Y} - \E[\nu_t]{Y}}\tp{\E[\gamma_t]{Y} - \E[\nu_t]{Y}}^{\top} \\
        &= (1-\delta_t)\cdot\!{Cov}_{\+N_1}[Y] + (1-\delta_t) \cdot \tp{\E[\+N_1]{Y} - \E[\nu_t]{Y}}\tp{\E[\+N_1]{Y} - \E[\nu_t]{Y}}^{\top}\\
        &\quad + \delta_t\cdot\!{Cov}_{\gamma_t}[Y] + \delta_t \cdot \tp{\E[\gamma_t]{Y} - \E[\nu_t]{Y}}\tp{\E[\gamma_t]{Y} - \E[\nu_t]{Y}}^{\top}.
    \end{align*}
    Since $q_t(y) = (1-\delta_t)\cdot e^{-h_1(y)} + \delta_t \cdot p_{\gamma_t}(y)$, the expectation $\E[\nu_t]{Y} = (1-\delta_t)\cdot \E[\+N_1]{Y} + \delta_t\cdot \E[\gamma_t]{Y}$. Then we have 
    \[
        \E[\nu_t]{Y} - \E[\+N_1]{Y} = \delta_t\cdot \tp{\E[\gamma_t]{Y} - \E[\+N_1]{Y}} 
    \]
    and
    \[
        \E[\nu_t]{Y} - \E[\gamma_t]{Y} = (1-\delta_t)\cdot \tp{\E[\+N_1]{Y} - \E[\gamma_t]{Y}}.
    \]
    Therefore,
    \begin{align*}
        \!{Cov}_{Y\sim \nu_t}[Y] &= (1-\delta_t)\cdot\!{Cov}_{\+N_1}[Y] + \delta_t\cdot\!{Cov}_{\gamma_t}[Y] \\
        &\quad + \tp{\delta_t^2(1-\delta_t) + (1-\delta_t)^2\delta_t} \cdot \tp{\E[\+N_1]{Y} - \E[\gamma_t]{Y}}\tp{\E[\+N_1]{Y} - \E[\gamma_t]{Y}}^{\top} \\
        & = (1-\delta_t)\cdot \!{Cov}_{\+N_1}[Y] + \delta_t\cdot\!{Cov}_{\gamma_t}[Y] + \delta_t(1-\delta_t) \cdot \tp{\E[\gamma_t]{Y} - \frac{e^{-3t}u}{2}}\cdot \tp{\E[\gamma_t]{Y} - \frac{e^{-3t}u}{2}}^\top.
    \end{align*}

    We then show that the matrix $\tp{\E[\gamma_t]{Y} - \frac{e^{-3t}u}{2}}\cdot \tp{\E[\gamma_t]{Y} - \frac{e^{-3t}u}{2}}^\top$ cannot be bouned by $L\cdot \!{Id}_d$ for any $L=o(e^{-2t}\|u\|^2)$. To show this, we only need to prove $\norm{\E[\gamma_t]{Y} - \frac{e^{-3t}u}{2}}^2 = \Omega\tp{e^{-2t}\|u\|^2}$. Recall that $\gamma_t$ is supported only on $ e^{-t}\cdot \+B_{\frac{r}{2}}(u) = \set{y\in \bb R^d: \| y - e^{-t}u\| \leq 0.5 e^{-t} \|u\|}$. For each $y\in e^{-t}\cdot \+B_{\frac{r}{2}}(u)$, with $e^{-2t}<0.1$,
    \[
        \norm{ y- \frac{e^{-3t}u}{2} } \geq \|y\| - \norm{\frac{e^{-3t}u}{2}} \geq \frac{e^{-t}}{2} \|u\| - \frac{e^{-3t}}{2} \|u\| > \frac{e^{-t}}{4} \|u\|.
    \]
    So $\norm{\E[\gamma_t]{Y} - \frac{e^{-3t}u}{2}}^2$ can be lower bounded by 
    \[
        \inf_{y\in  e^{-t}\cdot \+B_{\frac{r}{2}}(u)} \norm{y - \frac{e^{-3t}u}{2}}^2  \geq \frac{e^{-2t}}{16} \|u\|^2 = \Omega\tp{e^{-2t}\|u\|^2}.
    \]
    
    Since $\!{Cov}_{\+N_1}[Y]\mge 0$, $\!{Cov}_{\gamma_t}[Y]\mge 0$ and from \Cref{lem:stitched4}, $\delta_t=\Theta(1)$, this indicates that $\|\!{Cov}_{Y\sim \nu_t}[Y]\|_{\!{op}} = \Omega\tp{e^{-2t}\|u\|^2}$.
    % this indicates that for any $L=o(e^{-2t}\|u\|^2)$, $\!{Cov}_{Y\sim \nu_t}[Y] \not\mle L \cdot \!{Id}_d$. 
    The remaining of the lemma then follows from \Cref{prop:stitched-decomp}.
\end{proof}

\subsubsection{Proofs for the supporting lemmas in \Cref{subsec:stitched2}}\label{subsec:proof2}
\begin{proof}[Proof of \Cref{lem:stitched0}]
    By definition, for any $y\in \bb R^d$,
    \begin{align}
        q_t(y) &\propto \exp\set{-\mathfrak{g}_{u}\tp{\frac{y}{e^{-t}}}\cdot \tp{\frac{\|y\|^2}{2e^{-2t}} +\frac{\|y-x_0\|^2}{2(1-e^{-2t})}} - \tp{1-\mathfrak{g}_{u}\tp{\frac{y}{e^{-t}}}}\cdot \tp{\frac{\|y-e^{-t}\cdot u\|^2}{2e^{-2t}} + \frac{\|y-x_0\|^2}{2(1-e^{-2t})}} } \notag \\
        &= \exp\left\{-\mathfrak{g}_{u}\tp{\frac{y}{e^{-t}}}\cdot \frac{\|y\|^2 + e^{-2t}\|x_0\|^2 - e^{-2t}(y^{\top}x_0 + x_0^{\top}y)}{2e^{-2t}(1-e^{-2t})} \right. \notag \\
        & \quad  \left. - \tp{1-\mathfrak{g}_{u}\tp{\frac{y}{e^{-t}}}}\cdot \frac{\|y\|^2 - (1-e^{-2t})e^{-t}(u^{\top}y + y^{\top}u) - e^{-2t}(x_0^{\top}y + y^{\top}x_0) + (1-e^{-2t})e^{-2t}\|u\|^2 + e^{-2t}\|x_0\|^2}{2e^{-2t}(1-e^{-2t})} \right\} \notag  \\
        &= \exp\left\{-\mathfrak{g}_{u}\tp{\frac{y}{e^{-t}}}\cdot \tp{\frac{\|y-e^{-2t}x_0\|^2}{2e^{-2t}(1-e^{-2t})} + \frac{\|x_0\|^2}{2}} \right. \notag \\
        & \quad \left.- \tp{1-\mathfrak{g}_{u}\tp{\frac{y}{e^{-t}}}}\cdot \tp{\frac{\|y- \tp{e^{-t}(1-e^{-2t})u + e^{-2t}x_0}\|^2}{2e^{-2t}(1-e^{-2t})} + \frac{\|x_0 - e^{-t}u\|^2}{2}}\right\}.\label{eq:stitched1}
    \end{align}
    When we choose $x_0 = \frac{e^{-t}u}{2}$, we can further simplify \Cref{eq:stitched1} as
    \begin{align*}
        q_t(y) & \propto \exp\left\{-\mathfrak{g}_{u}\tp{\frac{y}{e^{-t}}}\cdot \tp{\frac{\norm{y-\frac{e^{-3t}u}{2}}^2}{2e^{-2t}(1-e^{-2t})} + \frac{\|e^{-t}u\|^2}{8}} \right. \notag \\
        & \quad \left.- \tp{1-\mathfrak{g}_{u}\tp{\frac{y}{e^{-t}}}}\cdot \tp{\frac{\norm{y- \tp{e^{-t}\tp{1-\frac{e^{-2t}}{2}}u }}^2}{2e^{-2t}(1-e^{-2t})} + \frac{\|e^{-t}u\|^2}{8}}\right\} \notag  \\
        &\propto \exp\left\{-\mathfrak{g}_{u}\tp{\frac{y}{e^{-t}}}\cdot \frac{\norm{y-\frac{e^{-3t}u}{2}}^2}{2e^{-2t}(1-e^{-2t})}  - \tp{1-\mathfrak{g}_{u}\tp{\frac{y}{e^{-t}}}}\cdot \frac{\norm{y- \tp{e^{-t}\tp{1-\frac{e^{-2t}}{2}}u }}^2}{2e^{-2t}(1-e^{-2t})} \right\}  \notag \\
        &\propto \exp\left\{-\mathfrak{g}_{u}\tp{\frac{y}{e^{-t}}}\cdot \tp{\frac{\norm{y-\frac{e^{-3t}u}{2}}^2}{2e^{-2t}(1-e^{-2t})} + c_t} - \tp{1-\mathfrak{g}_{u}\tp{\frac{y}{e^{-t}}}}\cdot \tp{\frac{\norm{y- \tp{e^{-t}\tp{1-\frac{e^{-2t}}{2}}u }}^2}{2e^{-2t}(1-e^{-2t})} + c_t}\right\}
    \end{align*}
\end{proof}


\begin{proof}[Proof of \Cref{lem:stitched3}]
    Recall that for any $y\in \bb R^d$,
    \[
    h_1(y) = \frac{\norm{y-\frac{e^{-3t}u}{2}}^2}{2e^{-2t}(1-e^{-2t})} + c_t \quad \mbox{and}\quad h_2(y) = \frac{\norm{y- \tp{e^{-t}\tp{1-\frac{e^{-2t}}{2}}u }}^2}{2e^{-2t}(1-e^{-2t})} + c_t.
    \]
    To prove this lemma, we need to show that
    \[
        \norm{y-\frac{e^{-3t}u}{2}}^2 \geq \norm{y- \tp{e^{-t}\tp{1-\frac{e^{-2t}}{2}}u }}^2
    \]
    for each $y\in e^{-t}\cdot \+B_{\frac{r}{2}}(u)$. This is equivalent to say
    \begin{equation*}
        \inner{\tp{2e^{-t} - 2e^{-3t}}u}{y} \geq e^{-2t}(1-e^{-2t}) \|u\|^2,
    \end{equation*}
    and this can be further simplified to $2\inner{u}{y} \geq e^{-t}\|u\|^2$.

    Since for $y\in e^{-t}\cdot \+B_{\frac{r}{2}}(u)$, $y$ satisfies $\| y - e^{-t}u\| \leq 0.5 e^{-t} \|u\|$. Therefore, for each $y\in e^{-t}\cdot\+B_{\frac{r}{2}}(u)$, we have
    \begin{equation*}
        \|y\| \geq 0.5 e^{-t}\|u\| \quad \mbox{and} \quad 2e^{-t}\inner{u}{y} \geq \|y\|^2 + 0.75 e^{-2t} \|u\|^2.
    \end{equation*}
    This indicates that $2\inner{u}{y} \geq e^{-t}\|u\|^2$.
\end{proof}


\begin{proof}[Proof of \Cref{lem:normalizing}]
    On the one hand,
    \begin{align*}
        Z_t \leq \int_{\bb R^d} e^{-h_1(y)} \dd y + \int_{\bb R^d} e^{-h_2(y)} \dd y \leq 2.
    \end{align*}
    On the other hand,
    \begin{align*}
        Z_t &\geq \int_{e^{-t}\cdot \+B_{\frac{2r}{5}}(u)} e^{-h_2(y)} \dd y + \int_{e^{-t}\cdot \ol{\+B_{\frac{r}{2}}(u)}} e^{-h_1(y)} \dd y \\
        & = \Pr[Y\sim \+N_2]{Y\in e^{-t}\cdot \+B_{\frac{2r}{5}}(u)} + \Pr[Y\sim \+N_1]{Y\in e^{-t}\cdot \ol{\+B_{\frac{r}{2}}(u)}}\\
        &\geq 2- \frac{20 d}{\|u\|^2} >1.8.
    \end{align*}
    where in the second inequality we use \Cref{prop:stitched0} and the last inequality is due to $\|u\|^2\geq 100 d$.
\end{proof}

Before proving \Cref{lem:stitched4}, we first give the following concentration bounds for $\+N_1$ and $\+N_2$. Recall that $\+N_1$ and $\+N_2$ represent the two Gaussian distributions $\+N\tp{\frac{e^{-3t}u}{2}, \sigma_t^2 \cdot \!{Id}_d}$ and $\+N\tp{e^{-t}\tp{1-\frac{e^{-2t}}{2}}u, \sigma_t^2 \cdot \!{Id}_d}$ respectively. 
\begin{proposition}\label{prop:stitched0}
    When $e^{-2t}<0.1$, we have $\Pr[Y\sim \+N_1]{Y\in e^{-t}\cdot \ol{\+B_{\frac{r}{2}}(u)}} \geq 1 - \frac{10d}{\|u\|^2}$ and $\Pr[Y\sim \+N_2]{Y\in e^{-t}\cdot \+B_{\frac{2r}{5}}(u)} \geq 1- \frac{10d}{\|u\|^2}$.
\end{proposition}
\begin{proof}
    We first prove $\Pr[Y\sim \+N_1]{Y\in e^{-t}\cdot \ol{\+B_{\frac{r}{2}}(u)}} \geq 1 - \frac{10d}{\|u\|^2}$. By definition, 
    \[
        e^{-t}\cdot \ol{\+B_{\frac{r}{2}}(u)} = \set{y\in \bb R^d: \|y - e^{-t}u\| > 0.5e^{-t}\|u\|}.
    \]
    Since 
    \[
        \|y - e^{-t}u\| \geq \tp{e^{-t} - \frac{e^{-3t}}{2}} \|u\| - \norm{y - \frac{e^{-3t}}{2} u },
    \]
    we have $\set{y\in \bb R^d:\ \norm{y - \frac{e^{-3t}}{2} u } < \tp{0.5 e^{-t} - \frac{e^{-3t}}{2}}\|u\|} \subseteq e^{-t}\cdot \ol{\+B_{\frac{r}{2}}(u)}$. Therefore, 
    \begin{align*}
        \Pr[Y\sim \+N_1]{Y\in e^{-t}\cdot \ol{\+B_{\frac{r}{2}}(u)}} &\geq \Pr[Y\sim \+N_1]{\norm{Y - \frac{e^{-3t}}{2} u }^2 < \tp{0.5 e^{-t} - \frac{e^{-3t}}{2}}^2\|u\|^2}  \\
        &\geq 1 - \frac{d\cdot \sigma_t^2}{\tp{0.5 e^{-t} - \frac{e^{-3t}}{2}}^2\|u\|^2} \\
        &\geq 1 - \frac{10d}{\|u\|^2}
    \end{align*}
    where the second inequality follows from the Markov's inequality and the last inequality is due to $e^{-2t}< 0.1$.

    We then prove that $\Pr[Y\sim \+N_2]{Y\in e^{-t}\cdot \+B_{\frac{2r}{5}}(u)} \geq 1- \frac{10d}{\|u\|^2}$. By definition, 
    \[
        e^{-t}\cdot \+B_{\frac{2r}{5}}(u) = \set{y\in \bb R^d: \|y - e^{-t}u\| < 0.4e^{-t}\|u\|}.
    \]
    Since
    \[
        \|y - e^{-t}u\| \leq \norm{y -  e^{-t}\tp{1-\frac{e^{-2t}}{2}}u} + \frac{e^{-3t}}{2}\|u\|,
    \]
    we have $\set{y\in \bb R^d :\ \norm{ y -  e^{-t}\tp{1-\frac{e^{-2t}}{2}}u}\leq\tp{0.4 e^{-t} - \frac{e^{-3t}}{2}}\|u\|} \subseteq e^{-t}\cdot \+B_{\frac{2r}{5}}(u)$. Similarly,
    \begin{align*}
        \Pr[Y\sim \+N_2]{Y\in e^{-t}\cdot \+B_{\frac{2r}{5}}(u)} &\geq \Pr[Y\sim \+N_2]{\norm{Y -  e^{-t}\tp{1-\frac{e^{-2t}}{2}}}^2 \leq\tp{0.4 e^{-t} - \frac{e^{-3t}}{2}}^2\|u\|^2}  \\
        &\geq 1- \frac{d\cdot \sigma_t^2}{\tp{0.4 e^{-t} - \frac{e^{-3t}}{2}}^2\|u\|^2} \\
        &\geq 1- \frac{10d}{\|u\|^2}.
    \end{align*}
\end{proof}

Now we give a proof of \Cref{lem:stitched4}.
\begin{proof}[Proof of \Cref{lem:stitched4}]
    Recall that the distribution $\gamma_t$ is only supported on $e^{-t}\cdot \+B_{\frac{r}{2}}(u)$. The choice of $\delta_t$ satisfies
    \begin{equation}
        \Pr[Y\sim \nu_t]{Y\in e^{-t}\cdot \+B_{\frac{r}{2}}(u)} = (1-\delta_t)\cdot \Pr[Y\sim N_1]{Y\in e^{-t}\cdot \+B_{\frac{r}{2}}(u)} + \delta_t. \label{eq:decomp}
    \end{equation}
    From \Cref{prop:stitched0} and \Cref{lem:normalizing}, we have
    \[
        \Pr[Y\sim \nu_t]{Y\in e^{-t}\cdot \+B_{\frac{r}{2}}(u)} \geq \frac{1}{Z_t}\cdot \Pr[Y\sim \+N_2]{Y\in e^{-t}\cdot \+B_{\frac{2r}{5}}(u)} \geq \frac{1-\frac{10d}{\|u\|^2}}{2} >0.4,
    \]
    and 
    \[
        \Pr[Y\sim \nu_t]{Y\in e^{-t}\cdot \+B_{\frac{r}{2}}(u)} \leq \frac{1}{Z_t}\cdot\tp{\Pr[Y\sim \+N_2]{Y\in e^{-t}\cdot \+B_{\frac{r}{2}}(u)} + \Pr[Y\sim \+N_1]{Y\in e^{-t}\cdot \+B_{\frac{r}{2}}(u)}} \leq \frac{1}{Z_t}\cdot \tp{1 + \frac{10d}{\|u\|^2}} < 0.7.
    \]
    
    From \Cref{prop:stitched0}, $\Pr[Y\sim N_1]{Y\in e^{-t}\cdot \+B_{\frac{r}{2}}(u)} \leq \frac{10d}{\|u\|^2} \leq 0.1$. So the RHS of \Cref{eq:decomp} satisfies
    \[
        \delta_t \leq (1-\delta_t)\cdot \Pr[Y\sim N_1]{Y\in e^{-t}\cdot \+B_{\frac{r}{2}}(u)} + \delta_t \leq 0.1 + 0.9 \delta_t.
    \]
    Combining above inequalities, we have $0.3< \delta_t < 0.7$.
\end{proof}

\subsection{Smoothness for the mixture of Gaussian distributions}\label{subsec:mix}

The analysis in the previous section indicates that
% that the smoothness parameter can become larger during the OU process compared to the stitched Gaussian distribution at the start. 
an $\+O(1)$-log-smooth initial distribution does not guarantee $\+O(1)$-smoothness after the process evolves. 
% Therefore, it is worth  investigating what kind of initial distributions can preserve such desirable smoothness properties.
In this subsection, we consider a family of classic multi-modal distributions, the mixture of Gaussian distributions. Mixture of Gaussians appear to be similar to stitched Gaussians, but the analysis below will reveal fundamental differences in their smoothness behaviors. 

We study the cases where each Gaussian component has a covariance $\Sigma_i \succeq \Omega(1)\!{Id}_d$. 
% Let $p$ and $p_t$ denote the density functions of the initial distribution and the distribution at time $t$ during the OU process respectively. 
We first considered the simple case with only two components, and then extend the analysis to mixtures with multiple components.

% To be specific, we show that for a mixture of two Gaussians with mean $u_1$ and $u_2$, if their covariance matrices are the same, the distributions at the beginning and during the process will always be $\+O\tp{\max\ab\{e^{-2t}\|u_1-u_2\|^2,1\}}$-smooth. In contrast, when the covariance matrices differ, even with $\|u_1-u_2\|$ being a constant, $\log p$ is not $L$-smooth for any $L=o(d)$. 

% However, for those cases with more components, the analysis becomes more complex. Even when all covariance matrices are the same, it is challenging to derive a concise rule to characterize the relationship between smoothness and the distances between the means of the Gaussians. We give an example where the centers of components are far apart, yet the mixture distribution remains $\+O(1)$-smooth, and this smoothness is preserved during the OU process.

\subsubsection{Mixture of Gaussian distributions and its evolution during the OU process}

Consider $m$ Gaussian distributions over $\bb R^d$, each with mean $u_i\in \bb R^d$ and covariance $\Sigma_i\in \bb R^{d\times d}$. Define function $f_i:\bb R^d\to \bb R$ as $f_i(x) = \frac{1}{2}(x-u_i)^{\top}\Sigma_i^{-1}(x-u_i) + \frac{1}{2}\log\tp{\tp{2\pi}^{d} \abs{\Sigma_i}}$ for each $x\in \bb R^d$. Then $e^{-f_i}$ is the density function of the Gaussian distribution $\+N\tp{u_i,\Sigma_i}$. Suppose $\mu$ is the mixture of Gaussian with density $p_{\mu}(x) = \sum_{i=1}^m w_i e^{-f_i(x)}$, where $w_i\in(0,1)$ is the weight of the $i$-th component and $\sum_{i=1}^m w_i=1$. Then $-\grad \log p_{\mu}(x) = \frac{\sum_{i=1}^m w_i\grad f_i(x)\cdot e^{-f_i(x)}}{p_{\mu}(x)}$ and 
\begin{align}
    -\grad^2 \log p_{\mu}(x) &= \frac{\sum_{i=1}^m w_i\grad^2 f_i(x)\cdot e^{-f_i(x)} - \sum_{i=1}^m w_i\grad f_i(x)\grad f_i(x)^{\top}\cdot e^{-f_i(x)}}{\sum_{i=1}^m w_i e^{-f_i(x)}} \notag  \\
    &\quad + \frac{\tp{\sum_{i=1}^m w_i\grad f_i(x)\cdot e^{-f_i(x)}}\tp{\sum_{i=1}^m w_i\grad f_i(x)\cdot e^{-f_i(x)}}^{\top}}{\tp{\sum_{i=1}^m w_i e^{-f_i(x)}}^2} \notag \\
    &= \frac{\sum_{i=1}^m\sum_{j=1}^m w_iw_j \tp{\grad f_i(x) \grad f_j(x)^{\top} - \frac{1}{2}\grad f_i(x) \grad f_i(x)^{\top} - \frac{1}{2}\grad f_j(x) \grad f_j(x)^{\top}} e^{-f_i(x)-f_j(x)}}{\tp{\sum_{i=1}^m w_i e^{-f_i(x)}}^2} \notag \\
    &\quad + \frac{\sum_{i=1}^m w_i\grad^2 f_i(x)\cdot e^{-f_i(x)} }{\sum_{i=1}^m w_i e^{-f_i(x)}} \notag \\
    &= - \underbrace{\frac{\sum_{1\leq i<j\leq m} w_iw_j \tp{\grad f_i(x) - \grad f_j(x)}\tp{\grad f_i(x) - \grad f_j(x)}^{\top} e^{-f_i(x)-f_j(x)}}{\tp{\sum_{i=1}^m w_i e^{-f_i(x)}}^2}}_{(A)} \notag \\
    &\quad + \underbrace{\frac{\sum_{i=1}^m w_i\grad^2 f_i(x)\cdot e^{-f_i(x)} }{\sum_{i=1}^m w_i e^{-f_i(x)}}}_{(B)}. \label{eq:smooth1}
\end{align}
From \Cref{eq:smooth1}, $\grad^2 \log p_{\mu}$ is determined by two parts: the weighted mixture of the Hessian of each component (term $(B)$), and the interaction between different components (term $(A)$).

Then we see how this distribution evolves during the OU process. Recall that the trajectory of the OU process is given by $X_t = e^{-t}X_0 + \sqrt{2}\cdot e^{-t}\int_{0}^t e^s \d B_s$ and $
\sqrt{2}\cdot e^{-t}\int_{0}^t e^s \d B_s \sim \+N\tp{0, (1-e^{-2t})I_d}$. If $X_0$ is drawn from some Gaussian distribution $\+N(u,\Sigma)$, at time $t$, the distribution of $X_t$ will be $\+N\tp{e^{-t}u, e^{-2t}\Sigma + (1-e^{-2t})\!{Id}_d}$. Hence, if the initial distribution is $\mu$, i.e., the weighted mixture of $\ab\{\+N(u_i, \Sigma_i)\}_{i\in[m]}$, then the distribution of $X_t$ will be the mixture of $\ab\{\+N\tp{e^{-t}u_i, e^{-2t}\Sigma_i + (1-e^{-2t})\!{Id}_d} \}_{i\in[m]}$ with the same weights. Let $\Sigma_i^{(t)} =  e^{-2t}\Sigma_i + (1-e^{-2t})\!{Id}_d$. That is to say, this distribution is still a mixture of Gaussians and has density $p_t(x) = \sum_{i=1}^n w_i e^{-f^{(t)}_i(x)}$, where $e^{-f^{(t)}_i(x)}$ is the density function of $\+N\tp{e^{-t}u_i, \Sigma_i^{(t)}}$.
 


\subsubsection{Mixture of two Gaussians}\label{subsubsec:2gaussian}
We first see the case when $m=2$. 

\paragraph{Gaussians with the same covariance: distance of means determines}~

When the covariances are the same and are bounded, the rules are simple and straightforward. The smoothness of the mixture distribution is totally determined by the distance of centers.

\begin{lemma}\label{lem:2-same}
    When $m=2$ and $\Sigma_1 = \Sigma_2 = \Sigma$ for some matrix $\Sigma\succeq \Omega(1)\!{Id}_d$, we have
    \[
        -\+O\tp{\|u_1-u_2\|^2}\cdot \!{Id}_d \preceq -\grad^2 \log p_{\mu}(x) \preceq \Sigma^{-1}, 
    \]
    and 
    \[
         -\+O\tp{e^{-2t}\|u_1-u_2\|^2}\cdot \!{Id}_d \preceq -\grad^2 \log p_t(x) \preceq \+O(1)\!{Id}_d
    \]
    for any $t>0$.
    
    On the other hand, for any $L=o(\|u_1-u_2\|^2)$, $-\grad^2 \log p(x) \not\succeq -L\cdot \!{Id}_d$ and $-\grad^2 \log p_t(x) \not\succeq -e^{-2t}L\cdot \!{Id}_d$.
\end{lemma}
\begin{proof}
    When $m=2$ and $\Sigma_1 = \Sigma_2 = \Sigma$ for some matrix $\Sigma\in \bb R^{d\times d}$, we have $\grad f_1(x) = \Sigma^{-1}(x-u_1)$, $\grad f_2(x) = \Sigma^{-1}(x-u_2)$ and $\grad^2 f_1(x) = \grad^2 f_2(x) = \Sigma^{-1}$.
According to \Cref{eq:smooth1}, 
\begin{align*}
    -\grad^2 \log p_{\mu}(x) = - \frac{w_1w_2\cdot \Sigma^{-1}(u_1-u_2)(u_1-u_2)^{\top}\Sigma^{-1}\cdot e^{-f_1(x) - f_2(x)}}{\tp{w_1 e^{-f_1(x)} + w_2 e^{-f_2(x)}}^2} + \Sigma^{-1}.
\end{align*}
From the Cauchy-Schwartz inequality,
\[
    0 \leq \frac{w_1w_2\cdot e^{-f_1(x) - f_2(x)}}{\tp{w_1 e^{-f_1(x)} + w_2 e^{-f_2(x)}}^2} \leq \frac{1}{4}.
\]
Therefore
\[
    -\frac{1}{4}\Sigma^{-1}(u_1-u_2)(u_1-u_2)^{\top}\Sigma^{-1} + \Sigma^{-1} \preceq -\grad^2 \log p_{\mu}(x) \preceq \Sigma^{-1}.
\]
It can be easily prove that $\Sigma^{-2}$ is also upper bounded by $\+O(1)\!{Id}_d$. The result then follows from the fact that 
\[
    \Sigma^{-1}(u_1-u_2)(u_1-u_2)^{\top}\Sigma^{-1} \preceq \|\Sigma^{-1}(u_1-u_2)\|^2 \cdot \!{Id}_d = (u_1-u_2)^{\top}\Sigma^{-2}(u_1-u_2)\cdot \!{Id}_d \preceq \+O(\|u_1-u_2\|^2)\cdot \!{Id}_d.
\]

% This indicates that the potential function of the initial distribution is at least $\+O\tp{\|u_1-u_2\|^2}$-smooth (?).

Let $\Sigma^{(t)} = e^{-2t}\Sigma + (1-e^{-2t})\!{Id}_d$. For the distributions during the OU process, repeating the above calculations, we can get that
\[
     -\frac{e^{-2t}}{4}\tp{\Sigma^{(t)}}^{-1}(u_1-u_2)(u_1-u_2)^{\top}\tp{\Sigma^{(t)}}^{-1} + \tp{\Sigma^{(t)}}^{-1} \preceq -\grad^2 \log p_t(x) \preceq \tp{\Sigma^{(t)}}^{-1}.
\]
By the definition of $\Sigma^{(t)}$, $ \tp{\Sigma^{(t)}}^{-1} \preceq \frac{1}{1-ce^{-2t}}\cdot \!{Id}_d \preceq \+O(1) \!{Id}_d$ for some universal constant $c<1$. 
Therefore, at time $t$, $-\grad^2 \log p_t \succeq -\+O( \|u_1-u_2\|^2) \!{Id}_d$.
\end{proof}


\paragraph{Gaussians with different covariances}~

When the two components have different covariance matrices, the smoothness parameter can be $\+O(d)$ even when $\|u_1-u_2\|$ is small. Here is an example.

\begin{lemma}\label{lem:2-diff}
    Let $p_{\mu}(x) = \frac{1}{2}e^{-f_1(x)} + \frac{1}{2}e^{-f_2(x)}$ with $f_1(x) = \|x-u_1\|^2 + \frac{d}{2}\log \pi$ and $f_2(x) = \frac{\|x-u_2\|^2}{2} + \frac{d}{2}\log 2\pi$ for arbitrary vectors $u_1,u_2\in \bb R^d$. Then there exists $x\in \bb R^d$ such that $\|\grad^2 \log p_{\mu}(x)\|_{\!{op}} = \Omega{d\log 2 + 2\|u_1-u_2\|^2}$.
    % $\grad^2 \log p_{\mu}(x) \not\preceq L \cdot \!{Id}_d$ for any $L< d\log 2 + 2\|u_1-u_2\|^2$.
\end{lemma}
\begin{proof}
    From \Cref{eq:smooth1}, 
    \begin{align*}
        -\grad^2 \log p_{\mu}(x) = \underbrace{\frac{2e^{-f_1(x)}\cdot \!{Id}_d + e^{-f_2(x)}\cdot \!{Id}_d}{e^{-f_1(x)} + e^{-f_2(x)}}}_{(a)} - \underbrace{\frac{e^{-f_1(x)-f_2(x)}\cdot \tp{x-(2u_1-u_2)}\tp{x-(2u_1-u_2)}^{\top}}{\tp{e^{-f_1(x)} + e^{-f_2(x)}}^2}}_{(b)}.
    \end{align*}
    For $(a)$, it is easy to know that $\!{Id}_d\preceq (a) \preceq 2\!{Id}_d$. For $(b)$, we will find a specific $x$ such that $(b)$ is large.
    When $x$ satisfies $\|x - (2u_1-u_2)\|^2 = d\log 2 + 2\|u_1-u_2\|^2$, by direct calculation, we have 
    \begin{align*}
        f_1(x) - f_2(x) &= \frac{1}{2}\tp{\|x\|^2 - (2u_1-u_2)^{\top}x - x^{\top}(2u_1-u_2) + 2\|u_1\|^2 - \|u_2\|^2 - d\log 2} \\
        &= \frac{1}{2}\tp{\|x - (2u_1-u_2)\|^2 - 4\|u_1\|^2 - \|u_2\|^2 + 2u_1^{\top}u_2 + 2u_2^{\top}u_1 + 2\|u_1\|^2 - \|u_2\|^2 - d\log 2}  \\
        &= \frac{1}{2}\tp{\|x - (2u_1-u_2)\|^2 - d\log 2 - 2\|u_1-u_2\|^2} \\
        &= 0.
    \end{align*}
    Therefore, $0\preceq (b) = \frac{1}{4}\cdot \tp{x-(2u_1-u_2)}\tp{x-(2u_1-u_2)}^{\top}$ and the optimal upper bound for $(b)$ is $\frac{\|x-(2u_1-u_2)\|^2}{4}\cdot \!{Id}_d = \tp{d\log 2 + 2\|u_1-u_2\|^2}\cdot \!{Id}_d$.
\end{proof}

% These results also holds whenever the number of components is a constant.

\subsubsection{Mixture of multiple Gaussians}
Things are more complicated and subtle for the mixture of multiple Gaussian distributions, even when their covariance matrices are the same. When they have the same covariance matrix, via similar arguments in \Cref{lem:2-same}, we can prove that $ -\+O\tp{\max_{i,j\in[m]}\|u_i-u_j\|^2}\cdot \!{Id}_d \preceq-\grad^2 \log p_{\mu}(x) \preceq \+O(1)\!{Id}_d$ and $ -\+O\tp{\max_{i,j\in[m]}e^{-2t}\|u_i-u_j\|^2}\cdot \!{Id}_d \preceq-\grad^2 \log p_t(x) \preceq \+O(1)\!{Id}_d$. However, these bounds may not be tight. A large distance between the centers of the components does not necessarily imply a lack of smoothness. The potential function may still be and maintain $\+O(1)$-smooth during the OU process in this case. We give an example in this section.

Let $J\in \bb R^{d\times d}$ be a symmetric and positive definite matrix and $h$ be an arbitrary vector in $\bb R^d$. Consider the distribution $\mu$ over $\bb R^d$ with density
\begin{equation}
    p_{\mu}(x) \propto \sum_{\sigma\in \ab\{\pm 1\}^d} \exp\set{ - \frac{1}{2}x^{\top}J^{-1}x + \tp{J^{-1}h+\sigma}^{\top}x}. \label{eq:HS-mix}
\end{equation}

This distribution is induced when applying the Hubbard-Stratonovich transform to the Ising model (see Appendix E in \cite{KLR22}). 
% The Ising model with interaction matrix $J$ is a distribution over $\ab\{\pm 1\}^d$ with density $p_J(\sigma)\propto \exp\set{\inner{\sigma}{J\sigma}}$ for any $\sigma\in \ab\{\pm 1\}^d$. 
Note that each $\sigma\in  \ab\{\pm 1\}^d$ corresponds to a Gaussian component $\+N(J\sigma+h, J^{-1})$. For a vector $x\in \bb R^d$, let $x(i)$ denote its $i$-th component for any $i\in[d]$. The following lemma shows that this distribution is log-smooth and even strongly log-concave if $J$ is within a moderate range.

\begin{lemma}\label{lem:mixture1}
    If $\delta\cdot \!{Id}_d\preceq J\preceq (1-\delta)\cdot \!{Id}_d$ for some $\delta\in (0,1/2)$, the distribution defined in \Cref{eq:HS-mix} satisfies $\frac{\delta}{1-\delta}\cdot \!{Id}_d \preceq -\grad^2 \log p_{\mu}(x) \preceq \frac{1}{\delta}\cdot \!{Id}_d$ for any $x\in \bb R^d$.
\end{lemma}
\begin{proof}
    By the definition in \Cref{eq:HS-mix}, 
    \begin{align*}
        p_{\mu}(x) &\propto \sum_{\sigma\in \ab\{\pm 1\}^d} \exp\set{ - \frac{1}{2}x^{\top}J^{-1}x + \tp{J^{-1}h+\sigma}^{\top}x} \\
        &= \exp\set{ - \frac{1}{2}x^{\top}J^{-1}x + h^{\top}J^{-1}x} \cdot \sum_{\sigma\in \ab\{\pm 1\}^d} \exp\set{\sigma^{\top}x} \\
        &= \exp\set{ - \frac{1}{2}x^{\top}J^{-1}x + h^{\top}J^{-1}x} \cdot \prod_{i=1}^d \tp{e^{x(i)}+e^{-x(i)}}.
    \end{align*}
    Therefore, $-\grad \log p_{\mu}(x) = J^{-1}x - J^{-1}h - z_x$ where $z_x\in \bb R^d$ and $z_x(i) = \frac{e^{x(i)} - e^{-x(i)}}{e^{x(i)} + e^{-x(i)}}$ for each $i\in[d]$. Consequently, $-\grad^2 \log p_{\mu}(x) = J^{-1} - A_x$ where $A_x$ is a diagonal matrix in $\bb R^{d\times d}$ and $A_x(i,i) = 1 - \tp{\frac{e^{x(i)} - e^{-x(i)}}{e^{x(i)} + e^{-x(i)}}}^2$ for each $i\in[d]$.

    Since $\delta\cdot \!{Id}_d\preceq J\preceq (1-\delta)\cdot \!{Id}_d$, for any $v\in \bb R^d$,
    \[
        v^{\top}J^{-1}v = \tp{J^{-\frac{1}{2}}v}^{\top}J^{-\frac{1}{2}}v \succeq \frac{1}{1-\delta}v^{\top}\cdot J^{-\frac{1}{2}} J J^{-\frac{1}{2}}\cdot v=\frac{1}{1-\delta}v^{\top}v,
    \]
    and 
    \[
        v^{\top}J^{-1}v = \tp{J^{-\frac{1}{2}}v}^{\top}J^{-\frac{1}{2}}v \preceq \frac{1}{\delta}v^{\top}\cdot J^{-\frac{1}{2}} J J^{-\frac{1}{2}}\cdot v = \frac{1}{\delta}v^{\top}v.
    \]
    % \[
    %     \frac{3}{2}v^{\top}v = \frac{3}{2}v^{\top}\cdot J^{-\frac{1}{2}} J J^{-\frac{1}{2}}\cdot v \preceq \tp{J^{-\frac{1}{2}}v}^{\top}J^{-\frac{1}{2}}v \preceq 3v^{\top}\cdot J^{-\frac{1}{2}} J J^{-\frac{1}{2}}\cdot v = 3v^{\top}v.
    % \]
    Thus $\frac{1}{1-\delta}\cdot \!{Id}_d\preceq J^{-1}\preceq \frac{1}{\delta}\cdot \!{Id}_d$. For the matrix $A_x$, we know $0\preceq A_x\preceq \!{Id}_d$. Combining these together, we can get the desired result.

\end{proof}

% \htodo{Define the notation $x(i)$ and $p*q$.}

For two distributions $\pi$ and $\nu$ with density $p_\pi$ and $p_{\nu}$ respectively, define $\pi*\nu$ as the distribution with density $p_{\pi*\nu}(x)\propto \int_{\bb R^d} p_{\pi}(y)\cdot p_{\nu}(x-y)\dd y$.
When the initial distribution is both strongly log-concave and log-smooth, we can show that the $-\grad^2 \log p_t$ is also bounded via the following lemma and its corollary.
\begin{lemma}[Lemma 28 in \cite{LPSR21}]\label{lem:m-gaussian1}
      Suppose $\pi$ is a probability density function on $\bb R^d$ such that $M_{1}^{-1} \preceq -\grad^2 \log p_{\pi}(x) \preceq M_2^{-1}$ for some $M_1,M_2\in \bb R^{d\times d}$. Let $\nu$ be the density function of $\+N(0,M)$. Then
      \[
        (M_1+M)^{-1} \preceq  -\grad^2 \log p_{\pi*\nu}(x) \preceq (M_2+M)^{-1}.
      \]
\end{lemma}

\begin{corollary}\label{cor:mixture2}
    During the OU process with starting distribution defined in \Cref{eq:HS-mix}, 
    \[
        \frac{1}{1+\frac{1-2\delta}{\delta}e^{-2t}}\cdot \!{Id}_d \preceq -\grad^2 \log p_t(x) \preceq \frac{1}{1-(1-\delta)e^{-2t}} \cdot \!{Id}_d
    \]
    for any $t>0$ and any $x\in \bb R^d$.
\end{corollary}
\begin{proof}
    Recall that $X_t = e^{-t}X_0 + \sqrt{2}\cdot e^{-t}\int_{0}^t e^s \d B_s$. Therefore $\mu_t = \mu'*\nu$ where $\mu'$ is the distribution with density $p_{\mu'}(x) \propto p_{\mu}\tp{\frac{x}{e^{-t}}}$ and $\nu$ is $\+N(0,(1-e^{-2t})\!{Id}_d)$. From \Cref{lem:m-gaussian1}, with $M_1 = \frac{1-\delta}{\delta}\cdot e^{-2t}\!{Id}_d$, $M_2 = \delta\cdot e^{-2t}\!{Id}_d$ and $M=(1-e^{-2t})\!{Id}_d$, we have 
    \[
        \frac{1}{1+\frac{1-2\delta}{\delta}e^{-2t}}\cdot \!{Id}_d \preceq -\grad^2 \log p_t(x) \preceq \frac{1}{1-(1-\delta)e^{-2t}} \cdot \!{Id}_d.
    \]
\end{proof}

Thus, despite being a mixture of Gaussian distributions where the component centers might be far apart, it is still $\+O(1)$-log-smooth and remains $\+O(1)$-log-smooth throughout the OU process.


\begin{remark}
    The motivation for exploring the distributions in \Cref{eq:HS-mix} is to study the Ising model, which is a distribution over $\ab\{\pm 1\}^d$ with density $p_{J,h}(\sigma)\propto \exp\set{\frac{1}{2}\inner{\sigma}{J\sigma} + \inner{h}{\sigma}}$ for any $\sigma\in \ab\{\pm 1\}^d$. 

    The Hubbard-Stratonovich transform states that the Ising model can be reduced to sampling from the distribution in \Cref{eq:HS-mix}: Consider the joint distribution over $\ab\{\pm 1\}^d\times \bb R^d$ with density $p_{J,h}(\sigma,x)\propto \exp\tp{-\frac{1}{2}x^\top J^{-1} x+(J^{-1}h+\sigma)^\top x}$. We can prove that
    \begin{itemize}
        \item its marginal density on $\bb R^d$ is exactly the $p_{\mu}$ in \Cref{eq:HS-mix};
        \item $p_{\mu}(x)\propto \exp\set{ - \frac{1}{2}x^{\top}J^{-1}x + h^{\top}J^{-1}x} \cdot \prod_{i=1}^d \tp{e^{x(i)}+e^{-x(i)}}$ and thus the unnormalized density and the gradients of the potential function can be calculated in polynomial time for any $x$;
        \item the conditional distribution with density $p_{J,h}(\sigma|x) \propto \exp\set{\inner{\sigma}{x}}$ is a product distribution and can be sampled efficiently.
    \end{itemize}
    The proofs are similar to Lemma E.1 in \cite{KLR22}.

    % Its marginal density on $\bb R^d$ is exactly \Cref{eq:HS-mix}. Furthermore, we can prove that the conditional distribution $p_{J,h}(\sigma|x) \propto \exp\set{\inner{\sigma}{x}}$, which is a product distribution on $\ab\{\pm 1\}^d$ and can be efficiently sampled from (the proof is similar to Lemma E.1 in \cite{KLR22}).

    Therefore, sampling from the Ising model can be executed in two steps: 1) sample $X\sim \mu$; 2) sample $\sigma$ from the distribution with density $p_{J,h}(\sigma|X)$. Hence, the hardness of this problem is closely related to the hardness of sampling from mixture of Gaussians. Given \Cref{lem:mixture1}, when $0\prec J\prec \!{Id}_d$, the distribution $\mu$ can be simulated in polynomial time using Langevin-based algorithms (e.g., the algorithm in \cite{CCBJ18}) and thus also gives a polynomial complexity upper bound for the Ising model. On the other hand, \cite{GKK24} proved that for any real $c>1$, the existence of polynomial samplers for Ising model with arbitrary $0\prec J\prec (1+c)\!{Id}_d$ implies $\*{NP}=\*{RP}$. This in turn indicates that, assuming  $\*{NP}\neq\*{RP}$, sampling from the mixture of Gaussians in such a special structure with $0\prec J\prec (1+c)\!{Id}_d$ is generally hard.
\end{remark}
% \htodo{Is the main idea of this remark is clear?}
% \ctodo{Very good}





