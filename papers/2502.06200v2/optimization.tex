\section{Comparison with optimization}\label{sec:sampling-vs-opt}
In this section, we prove \Cref{thm:main-opt-lb} and compare the hardness of optimization problems and sampling problems.

It is shown in \cite{MCJ+19} that for an $L$-log-smooth distribution $\mu$ which are $m$-strongly log-concave outside a ball of radius $R$, sampling can be done within $\tilde{\+O}\tp{\frac{dL^2}{\eps^2 m^2}\cdot e^{32LR^2}}$ queries, while optimizing the potential function of $\mu$ needs $\Omega\tp{\tp{\frac{LR^2}{\eps}}^{\frac{d}{2}}}$ queries in the worst case. This indicates that when $LR^2 = o(d)$, optimization can be harder than sampling.

% The relationship between optimization and sampling has always been a topic of interest. The work of \cite{MCJ+19} proves that when $f$ is $L$-smooth and  is $m$-strongly convex outside a region of radius $R$, the sampling upper bound is linear in $d$, while the optimization lower bound is exponential in $d$. This suggests that, for this specific class of functions, sampling is harder than optimization.

In this work, we consider a more general case, where the function $f$ and distribution $\mu$ with density $\propto e^{-f}$ are only required to satisfy \Cref{assump:moment} and \ref{assump:smooth}. In the optimization problem, we require the algorithm to output a point $x\in \bb R^d$ such that $\abs{f(x)-f(x^*)}\leq \eps$, where $x^*$ is the minimizer of $f$. 
% Let $\+U$ be the set of $L$-smooth functions such that for any $f\in \+U$, the second moment of $\mu\propto e^{-f}$ is $\Theta(M)$ for some $M=\Omega\tp{\frac{d}{L}}$ and $\grad f(0)=0$. 
%  We will prove the following theorem in \Cref{subsec:opt-lb}.

% show in \Cref{subsec:opt-lb} that, for any algorithm solving the optimization problem and guaranteeing $\+O(1)$ error with constant probability, there exists some functions in $\+U$, which requires at least $K_{o} = e^{\frac{d}{2}\log \Omega\tp{LM})}$ queries. 

% \subsection{Proof of \Cref{thm:main-opt-lb}}\label{subsec:opt-lb}

To prove the complexity lower bound, we first see the hard instances constructed in \cite{MCJ+19}.
\begin{lemma}[Lemma 17 in \cite{MCJ+19}]\label{lem:packing}
    For $R>r>0$, there exists $\+X_{R}\subset \bb R^d$ with $\abs{\+X_R}= \Big\lfloor \tp{\frac{R-r}{2r}}^d \Big\rfloor$ such that $\bigcup_{x\in \+X_R} \+B_r(x)\subset \+B_R$ and $\+B_r(x)\cap \+B_r(y) = \emptyset$ for any $x\neq y\in \+X_R$.
\end{lemma}

The work of \cite{MCJ+19} focused the cases where $f$ is $L$-smooth and is $m$-strongly convex outside a region of radius $R$. Consider the set $\+X_{\frac{R}{2}}$. We index the vertices in $\+X_{\frac{R}{2}}$ by $\left[\abs{\+X_{\frac{R}{2}}}\right]$. For each point $x_i\in \+X_{\frac{R}{2}}$, construct a function $f_i$ as follows:
\begin{equation}
    f_i(x) = \begin{cases}
        \frac{\eps}{2} \cdot \cos\tp{\frac{\pi}{r^2}\tp{\|x-x_i\|^2 - r^2}} - \frac{\eps}{2}, & \|x-x_i\|\leq r\\
        0, & \|x-x_i\|> r, \|x\| \leq \frac{R}{2} \\
        m\tp{\|x\| - \frac{R}{2}}^{2}, & \|x\|>\frac{R}{2}
    \end{cases}, \label{eq:opt1}
\end{equation}
where $r\defeq \sqrt{\frac{(2\pi^2 + \pi) \eps}{L}}$. 
From Lemma 18 in \cite{MCJ+19}, the functions defined in \Cref{eq:opt1} are all $L$-smooth. 
\begin{lemma}[Lemma 18 in \cite{MCJ+19}]\label{lem:opt-lb3}
    Let $L\geq 2m$. The functions in \Cref{eq:opt1} are $L$-smooth.
\end{lemma}

In Appendix C of \cite{MCJ+19}, they prove the following result.
\begin{theorem}\label{thm:lb-opt}
    For any $R>0$, $L\geq 2m>0$ and $\eps<\+O(LR^2)$, for any algorithm in $\+A$, there exists $i\in \left[\abs{\+X_{\frac{R}{2}}}\right]$, such that the algorithm requires at least $K=\Omega\tp{\tp{\frac{LR^2}{\eps}}^{\frac{d}{2}}}$ iterations on $f_i$ to guarantee that $\min_{k\leq K} \abs{f_i(x^K) - f_i(x^*)}<\eps$ with constant probability.
\end{theorem}

For each $x_i\in \+X_{\frac{R}{2}}$, consider a distribution $\mu_i$ whose density is in proportion to $e^{-f_i}$ (it is easy to see that this function is integrable). The following lemma shows that the second moment of this distribution is bounded.

\begin{lemma}\label{lem:lb-moment}
    If $LR^2\geq 6d$, $d\geq 8$ and $\eps<1$, choosing $m=\frac{L}{2}$, we have $\frac{R^2}{18(e+1)} \leq \E[X\sim \mu_i]{\|X\|^2} \leq 5R^2$.
\end{lemma}
\begin{proof}
    We first prove the upper bound for $\E[X\sim \mu_i]{\|X\|^2}$. Let $Z= \int_{x\in \bb R^d} e^{-f_i(x)} \dd x$. Let $\+B_{\frac{R}{2}}$ denote the ball with radius $R/2$ centered at $0$. Then 
    \begin{equation}
        Z\geq \int_{\|x\|\leq \frac{R}{2}} e^{-f_i(x)}\dd x \geq \int_{\|x\|\leq \frac{R}{2}} 1 \dd x = \!{vol}\tp{\+B_{\frac{R}{2}}} = \frac{\pi^{\frac{d}{2}}\cdot \tp{\frac{R}{2}}^{d}}{\Gamma\tp{\frac{d}{2}+1}} \geq \tp{\frac{e\pi\cdot R^2}{4d}}^{\frac{d}{2}}. \label{eq:lb-Z}
    \end{equation}
    By definition,
    \begin{align*}
        \E[X\sim \mu_i]{\|X\|^2} &= \int_{\+B_{\frac{R}{2}}} \|x\|^2\cdot \frac{e^{-f_i(x)}}{Z} \dd x + \int_{\bb R^d \setminus \+B_{\frac{R}{2}}} \|x\|^2\cdot \frac{e^{-f_i(x)}}{Z} \dd x \\
        &\leq 4R^2 \int_{\+B_{\frac{R}{2}}} \frac{e^{-f_i(x)}}{Z} \dd x + \frac{1}{Z}\cdot \int_{\bb R^d \setminus \+B_{\frac{R}{2}}} \|x\|^2\cdot e^{-f_i(x)} \dd x \\
        &\leq 4R^2 + \frac{1}{Z}\cdot \int_{\bb R^d \setminus \+B_{\frac{R}{2}}} \|x\|^2\cdot e^{-f_i(x)} \dd x.
    \end{align*}
    For the second term, we have
    \begin{align*}
        &\phantom{{}={}}\int_{\bb R^d \setminus \+B_{\frac{R}{2}}} \|x\|^2\cdot e^{-f_i(x)} \dd x \\
        &= \int_{\bb R^d \setminus \+B_{\frac{R}{2}}} \|x\|^2\cdot e^{-\frac{L}{2}\tp{\|x\|^2 + \frac{R^2}{4} - R\|x\|}} \dd x \\
        &\leq \int_{\bb R^d \setminus \+B_{\frac{R}{2}}} \|x\|^2\cdot e^{-\frac{L}{2}\tp{\|x\|^2 + \frac{R^2}{4} - \frac{\|x\|^2}{2}}} \dd x \\
        &= \frac{1}{2}e^{-\frac{LR^2}{8} + \frac{d}{2}\log \frac{4\pi}{L}}\cdot  \int_{\bb R^d \setminus \+B_{\frac{R}{2}}} \|x\|^2\cdot 2e^{-\frac{L\|x\|^2}{4} - \frac{d}{2}\log \frac{4\pi}{L}} \dd x\\
        &\leq \frac{1}{2}e^{-\frac{LR^2}{8} + \frac{d}{2}\log \frac{4\pi}{L}}\cdot \E[X\sim \+N(0, \frac{2\!{Id}_d}{L})]{\|X\|^2}\\
        &= \frac{d}{L}\cdot e^{-\frac{LR^2}{8} + \frac{d}{2}\log \frac{4\pi}{L}}.
    \end{align*}
    Combining \Cref{eq:lb-Z} and the fact that $LR^2\geq 6d$, we have
    \begin{align*}
        \E[X\sim \mu_i]{\|X\|^2} &\leq 4R^2 + \frac{1}{Z}\cdot \int_{\bb R^d \setminus \+B_{\frac{R}{2}}} \|x\|^2\cdot e^{-f(x)} \dd x \\
        &\leq 4R^2 + \frac{d}{L}\cdot e^{-\frac{LR^2}{8} + \frac{d}{2}\log \frac{4\pi}{L} - \frac{d}{2}\log \frac{e\pi\cdot R^2}{4d}} \\
        &< 4R^2 + \frac{d}{L} < 5 R^2.
    \end{align*}
    \bigskip

    Then we prove the lower bound for $\E[X\sim \mu_i]{\|X\|^2}$. Consider a distribution $\mu'$ supported only on $\+B_{\frac{R}{2}}$, whose density is proportional to $e^{-f_i(x)}$ on $\+B_{\frac{R}{2}}$.
    Since for each $x\in \bb R^d \setminus \+B_{\frac{R}{2}}$, $\|x\|^2 \geq \frac{R^2}{4} > \frac{R^2}{18(e+1)}$, it suffices to prove $\E[X\sim \mu']{\|X\|^2}\geq \frac{R^2}{18(e+1)}$. Let $Z' = \int_{\+B_{\frac{R}{2}}} e^{-f_i(x)}\dd x$. Then
    \[
        Z' \leq \!{vol}(\+B_{\frac{R}{2}}) + e^{\eps}\cdot \!{vol}(\+B(x_i,r)) \leq (e+1)\cdot \!{vol}(\+B_{\frac{R}{2}}).
    \]
    Therefore,
    \begin{align*}
        \E[X\sim \mu']{\|X\|^2} & = \frac{1}{Z'} \int_{\+B_{\frac{R}{2}}} \|x\|^2 e^{-f_i(x)}\dd x \\
        &\geq \frac{1}{Z'} \int_{\+B_{\frac{R}{2}}} \|x\|^2 \dd x \\
        &\geq \frac{\frac{R^2}{9}\cdot \tp{\!{vol}(\+B_{\frac{R}{2}}) - \!{vol}(\+B(0,R/3))}}{(e+1)\cdot \!{vol}(\+B_{\frac{R}{2}})} \\
        &\geq \frac{R^2}{18(e+1)}.
    \end{align*}
\end{proof}

Then \Cref{thm:main-opt-lb} is a direct corollary of \Cref{thm:lb-opt}, \Cref{lem:opt-lb3,lem:lb-moment} by choosing $R=\Theta(\sqrt{M})$. 

Note that from \Cref{thm:main-ub}, our sampling algorithm needs $\tp{\frac{LM}{d}}^{\+O(d)}$ queries to simulate the distribution $\mu \propto e^{-f}$ within $0.01$ error in total variation  distance. For $LR^2=LM=\Theta(d)$ and for sufficiently large $d$, the lower bound in \Cref{thm:main-opt-lb} is larger by a factor of $d^{\Theta(d)}$ than this upper bound for sampling. 
Compare to the results in \cite{MCJ+19}, our results further demonstrate that for a broader class of distributions or functions and a wider range of parameters, sampling is simpler than optimization.

% \htodo{Here we require $\grad f(0)=0$ in the sampling upper bound but not in optimization lower bound. But I think this is not a big deal because \cite{MCJ+19} also has this problem.}



\begin{remark}
    If we delve into the proofs of the lower bounds for sampling and optimization, we can gain some intuition about why optimization is harder. Notably, in both proofs, the hard instances are constructed by locally modifying the function values or density values on a base instance. The challenge for the algorithm is to recognize the modified local area, such as the ball $\+B_r(x_i)$ in \Cref{eq:opt1} and $\+B_{r_2}(v)$ in \Cref{sec:hardinstance}. The more balls we can pack, the harder it becomes for the algorithm to identify the modified ball.

    On the other hand, to ensure $L$-smoothness, a smooth transition is needed at the border of the modified local area, meaning the radius of the ball cannot be too small. Taking the error $\eps$ as a constant for example, we observe the following:
    \begin{itemize}
        \item In the optimization lower bound, the difference between the function value inside and outside the ball should be $\Omega(1)$. As a consequence, the radius should be $\Omega\tp{\sqrt{\frac{1}{L}}}$.
        \item In the sampling lower bound, the mass inside the ball must be $\Omega(1)$. This requires a larger difference between function values inside and outside the ball, which in turn require the radius to be $\Omega\tp{\sqrt{\frac{d}{L}}}$.
    \end{itemize}
    Thus, when the error requirements are the same, the hard instances in the sampling lower bound is easier for the algorithm to recognize. This provides an intuition for why optimization is harder than sampling.
\end{remark}