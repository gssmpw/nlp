\section{Related Works}
\vspace{-2mm}
	\subsection{LLM Compression Techniques}
	\vspace{-2mm}
	Recent advancements in model compression techniques have significantly enhanced the efficiency of deploying LLMs while maintaining their performance. Widely explored approaches include weight quantization**Zhang, "Efficient Weight Quantization"**, network pruning**He, "Deep Net Pruning"**, and hybrid methods**Kim, "Hybrid Model Compression"**.
	In unstructured pruning, SparseGPT**Fan, "Sparse GPT"** prunes weights based on their importance, as determined by the Hessian matrix. However, it faces challenges in achieving optimal speedup, particularly due to hardware compatibility issues. Structured pruning methods, in contrast, are more hardware-friendly. LLM-Pruner**Li, "LLM Pruner"** selectively removes non-critical coupled structures using gradient information. LaCo**Wang, "LaCo Layer-wise Pruning"** introduces a layer-wise pruning strategy, where subsequent layers collapse into preceding ones. **Chen, "Layer-pruning and PEFT"** explores the effectiveness of basic layer-pruning techniques combined with parameter-efficient fine-tuning (PEFT). Additionally, SliceGPT**Peng, "Slice GPT Post-training Sparsification"** has pioneered post-training sparsification, emphasizing the importance of layer removal order for optimal performance.
	Quantization techniques offer another significant avenue for compression. GPTQ**Zhou, "Layer-wise Quantization and Error Compensation"** applies layer-wise quantization and reduces quantization errors through second-order error compensation. AWQ**Liu, "Activation-aware Weight Quantization"** introduces activation-aware weight quantization, employing a scale transformation between weights and activations. Moreover, BiLLM**Gao, "Binary-weight LLM Compression"** and ARB-LLM**Chen, "Adaptive Rounding Binary-weight LLM"** achieve further compression to 1-bit while maintaining remarkable performance. More recently, STB-LLM**Wang, "Sparse Transformer-based LLM"** combines 1-bit quantization with pruning to achieve even greater memory reduction for LLMs.
	However, many of these compression techniques face challenges related to hardware compatibility, often requiring custom CUDA kernels**Zhang, "Custom CUDA Kernels for Real-time Inference"** to enable real-time inference speedup.
	
	
	
	\vspace{-1.2mm}
	\subsection{SVD-based LLM Compression}
	\vspace{-1.2mm}
	Singular Value Decomposition (SVD) is a widely used technique for reducing matrix size by approximating a matrix with two smaller, low-rank matrices**Golub, "Matrix Approximation using SVD"**. Although SVD-based methods have demonstrated potential in compressing LLMs, their full capabilities remain underexplored. Standard SVD typically focuses on compressing the original weight matrix without considering the significance of individual parameters, which can lead to considerable compression errors. To address this, **Li, "FWSVD with Fisher Information"** introduced FWSVD, which incorporates Fisher information to weight the importance of parameters. However, this method requires complex gradient calculations, making it resource-intensive. Another limitation of standard SVD is the impact of activation distribution on compression errors. To mitigate this, **Zhang, "ASVD with Diagonal Scaling"** proposed ASVD, which scales the weight matrix with a diagonal matrix that accounts for the influence of input channels on the weights. Subsequently, **Wang, "SVD-LLM with Truncation and Whitening"** introduced SVD-LLM, which establishes a connection between singular values and compression loss. This work demonstrates that truncating the smallest singular values after data whitening effectively minimizes compression loss. Despite these advancements, existing methods still exhibit significant accuracy loss at higher compression ratios and lack a comprehensive approach for compensating compressed weights after SVD truncation. Furthermore, most methods apply a uniform compression ratio across all transformer layers, overlooking the varying importance of different layers. $\ours$ seeks to address these limitations by proposing an adaptive compensation method (\textbf{adaComp}) and an importance-aware adaptive compression ratio method (\textbf{adaCR}).
	
	
	\vspace{-2.5mm}