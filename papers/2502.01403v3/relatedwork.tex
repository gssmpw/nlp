\section{Related Works}
\vspace{-2mm}
	\subsection{LLM Compression Techniques}
	\vspace{-2mm}
	Recent advancements in model compression techniques have significantly enhanced the efficiency of deploying LLMs while maintaining their performance. Widely explored approaches include weight quantization~\cite{frantar2022gptq, lin2024awq}, network pruning~\cite{frantar2023sparsegpt, ma2023llmpruner, yang2024laco, gromov2024unreasonable, ashkboos2024slicegpt}, and hybrid methods~\cite{dong2024stbllm}.
	In unstructured pruning, SparseGPT~\cite{frantar2023sparsegpt} prunes weights based on their importance, as determined by the Hessian matrix. However, it faces challenges in achieving optimal speedup, particularly due to hardware compatibility issues. Structured pruning methods, in contrast, are more hardware-friendly. LLM-Pruner~\cite{ma2023llmpruner} selectively removes non-critical coupled structures using gradient information. LaCo~\cite{yang2024laco} introduces a layer-wise pruning strategy, where subsequent layers collapse into preceding ones. ~\citet{gromov2024unreasonable} explores the effectiveness of basic layer-pruning techniques combined with parameter-efficient fine-tuning (PEFT). Additionally, SliceGPT~\cite{ashkboos2024slicegpt} has pioneered post-training sparsification, emphasizing the importance of layer removal order for optimal performance.
	Quantization techniques offer another significant avenue for compression. GPTQ~\cite{frantar2022gptq} applies layer-wise quantization and reduces quantization errors through second-order error compensation. AWQ~\cite{lin2024awq} introduces activation-aware weight quantization, employing a scale transformation between weights and activations. Moreover, BiLLM~\cite{huang2024billm} and ARB-LLM~\cite{li2024arb} achieve further compression to 1-bit while maintaining remarkable performance. More recently, STB-LLM~\cite{dong2024stbllm} combines 1-bit quantization with pruning to achieve even greater memory reduction for LLMs.
	However, many of these compression techniques face challenges related to hardware compatibility, often requiring custom CUDA kernels~\cite{dong2024stbllm} to enable real-time inference speedup.
	
	
	
	\vspace{-1.2mm}
	\subsection{SVD-based LLM Compression}
	\vspace{-1.2mm}
	Singular Value Decomposition (SVD) is a widely used technique for reducing matrix size by approximating a matrix with two smaller, low-rank matrices~\cite{GOLUB1987317}. Although SVD-based methods have demonstrated potential in compressing LLMs, their full capabilities remain underexplored. Standard SVD typically focuses on compressing the original weight matrix without considering the significance of individual parameters, which can lead to considerable compression errors. To address this, \citet{hsu2022languagemodel} introduced FWSVD, which incorporates Fisher information to weight the importance of parameters. However, this method requires complex gradient calculations, making it resource-intensive. Another limitation of standard SVD is the impact of activation distribution on compression errors. To mitigate this, \citet{yuan2024asvd} proposed ASVD, which scales the weight matrix with a diagonal matrix that accounts for the influence of input channels on the weights. Subsequently, \citet{wang2024svdllm} introduced SVD-LLM, which establishes a connection between singular values and compression loss. This work demonstrates that truncating the smallest singular values after data whitening effectively minimizes compression loss. Despite these advancements, existing methods still exhibit significant accuracy loss at higher compression ratios and lack a comprehensive approach for compensating compressed weights after SVD truncation. Furthermore, most methods apply a uniform compression ratio across all transformer layers, overlooking the varying importance of different layers. $\ours$ seeks to address these limitations by proposing an adaptive compensation method (\textbf{adaComp}) and an importance-aware adaptive compression ratio method (\textbf{adaCR}).
	
	
	\vspace{-2.5mm}