[
  {
    "index": 0,
    "papers": [
      {
        "key": "frantar2022gptq",
        "author": "Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan",
        "title": "Gptq: Accurate post-training quantization for generative pre-trained transformers"
      },
      {
        "key": "lin2024awq",
        "author": "Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song",
        "title": "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "frantar2023sparsegpt",
        "author": "Frantar, Elias and Alistarh, Dan",
        "title": "Sparsegpt: Massive language models can be accurately pruned in one-shot"
      },
      {
        "key": "ma2023llmpruner",
        "author": "Xinyin Ma and Gongfan Fang and Xinchao Wang",
        "title": "LLM-Pruner: On the Structural Pruning of Large Language Models"
      },
      {
        "key": "yang2024laco",
        "author": "Yifei Yang and Zouying Cao and Hai Zhao",
        "title": "LaCo: Large Language Model Pruning via Layer Collapse"
      },
      {
        "key": "gromov2024unreasonable",
        "author": "Andrey Gromov and Kushal Tirumala and Hassan Shapourian and Paolo Glorioso and Daniel A. Roberts",
        "title": "The Unreasonable Ineffectiveness of the Deeper Layers"
      },
      {
        "key": "ashkboos2024slicegpt",
        "author": "Saleh Ashkboos and Maximilian L. Croci and Marcelo Gennari do Nascimento and Torsten Hoefler and James Hensman",
        "title": "SliceGPT: Compress Large Language Models by Deleting Rows and Columns"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "dong2024stbllm",
        "author": "Dong, Peijie and Li, Lujun and Zhong, Yuedong and Du, Dayou and Fan, Ruibo and Chen, Yuhan and Tang, Zhenheng and Wang, Qiang and Xue, Wei and Guo, Yike and others",
        "title": "STBLLM: Breaking the 1-Bit Barrier with Structured Binary LLMs"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "frantar2023sparsegpt",
        "author": "Frantar, Elias and Alistarh, Dan",
        "title": "Sparsegpt: Massive language models can be accurately pruned in one-shot"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "ma2023llmpruner",
        "author": "Xinyin Ma and Gongfan Fang and Xinchao Wang",
        "title": "LLM-Pruner: On the Structural Pruning of Large Language Models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "yang2024laco",
        "author": "Yifei Yang and Zouying Cao and Hai Zhao",
        "title": "LaCo: Large Language Model Pruning via Layer Collapse"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "gromov2024unreasonable",
        "author": "Andrey Gromov and Kushal Tirumala and Hassan Shapourian and Paolo Glorioso and Daniel A. Roberts",
        "title": "The Unreasonable Ineffectiveness of the Deeper Layers"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "ashkboos2024slicegpt",
        "author": "Saleh Ashkboos and Maximilian L. Croci and Marcelo Gennari do Nascimento and Torsten Hoefler and James Hensman",
        "title": "SliceGPT: Compress Large Language Models by Deleting Rows and Columns"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "frantar2022gptq",
        "author": "Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan",
        "title": "Gptq: Accurate post-training quantization for generative pre-trained transformers"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "lin2024awq",
        "author": "Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song",
        "title": "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "huang2024billm",
        "author": "Huang, Wei and Liu, Yangdong and Qin, Haotong and Li, Ying and Zhang, Shiming and Liu, Xianglong and Magno, Michele and Qi, Xiaojuan",
        "title": "Billm: Pushing the limit of post-training quantization for llms"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "li2024arb",
        "author": "Li, Zhiteng and Yan, Xianglong and Zhang, Tianao and Qin, Haotong and Xie, Dong and Tian, Jiang and Kong, Linghe and Zhang, Yulun and Yang, Xiaokang and others",
        "title": "ARB-LLM: Alternating Refined Binarizations for Large Language Models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "dong2024stbllm",
        "author": "Dong, Peijie and Li, Lujun and Zhong, Yuedong and Du, Dayou and Fan, Ruibo and Chen, Yuhan and Tang, Zhenheng and Wang, Qiang and Xue, Wei and Guo, Yike and others",
        "title": "STBLLM: Breaking the 1-Bit Barrier with Structured Binary LLMs"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "dong2024stbllm",
        "author": "Dong, Peijie and Li, Lujun and Zhong, Yuedong and Du, Dayou and Fan, Ruibo and Chen, Yuhan and Tang, Zhenheng and Wang, Qiang and Xue, Wei and Guo, Yike and others",
        "title": "STBLLM: Breaking the 1-Bit Barrier with Structured Binary LLMs"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "GOLUB1987317",
        "author": "G.H. Golub and Alan Hoffman and G.W. Stewart",
        "title": "A generalization of the Eckart-Young-Mirsky matrix approximation theorem"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "hsu2022languagemodel",
        "author": "Yen-Chang Hsu and Ting Hua and Sungen Chang and Qian Lou and Yilin Shen and Hongxia Jin",
        "title": "Language model compression with weighted low-rank factorization"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "yuan2024asvd",
        "author": "Zhihang Yuan and Yuzhang Shang and Yue Song and Qiang Wu and Yan Yan and Guangyu Sun",
        "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing Large Language Models"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "wang2024svdllm",
        "author": "Xin Wang and Yu Zheng and Zhongwei Wan and Mi Zhang",
        "title": "SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression"
      }
    ]
  }
]