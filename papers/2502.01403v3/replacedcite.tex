\section{Related Works}
\vspace{-2mm}
	\subsection{LLM Compression Techniques}
	\vspace{-2mm}
	Recent advancements in model compression techniques have significantly enhanced the efficiency of deploying LLMs while maintaining their performance. Widely explored approaches include weight quantization____, network pruning____, and hybrid methods____.
	In unstructured pruning, SparseGPT____ prunes weights based on their importance, as determined by the Hessian matrix. However, it faces challenges in achieving optimal speedup, particularly due to hardware compatibility issues. Structured pruning methods, in contrast, are more hardware-friendly. LLM-Pruner____ selectively removes non-critical coupled structures using gradient information. LaCo____ introduces a layer-wise pruning strategy, where subsequent layers collapse into preceding ones. ____ explores the effectiveness of basic layer-pruning techniques combined with parameter-efficient fine-tuning (PEFT). Additionally, SliceGPT____ has pioneered post-training sparsification, emphasizing the importance of layer removal order for optimal performance.
	Quantization techniques offer another significant avenue for compression. GPTQ____ applies layer-wise quantization and reduces quantization errors through second-order error compensation. AWQ____ introduces activation-aware weight quantization, employing a scale transformation between weights and activations. Moreover, BiLLM____ and ARB-LLM____ achieve further compression to 1-bit while maintaining remarkable performance. More recently, STB-LLM____ combines 1-bit quantization with pruning to achieve even greater memory reduction for LLMs.
	However, many of these compression techniques face challenges related to hardware compatibility, often requiring custom CUDA kernels____ to enable real-time inference speedup.
	
	
	
	\vspace{-1.2mm}
	\subsection{SVD-based LLM Compression}
	\vspace{-1.2mm}
	Singular Value Decomposition (SVD) is a widely used technique for reducing matrix size by approximating a matrix with two smaller, low-rank matrices____. Although SVD-based methods have demonstrated potential in compressing LLMs, their full capabilities remain underexplored. Standard SVD typically focuses on compressing the original weight matrix without considering the significance of individual parameters, which can lead to considerable compression errors. To address this, ____ introduced FWSVD, which incorporates Fisher information to weight the importance of parameters. However, this method requires complex gradient calculations, making it resource-intensive. Another limitation of standard SVD is the impact of activation distribution on compression errors. To mitigate this, ____ proposed ASVD, which scales the weight matrix with a diagonal matrix that accounts for the influence of input channels on the weights. Subsequently, ____ introduced SVD-LLM, which establishes a connection between singular values and compression loss. This work demonstrates that truncating the smallest singular values after data whitening effectively minimizes compression loss. Despite these advancements, existing methods still exhibit significant accuracy loss at higher compression ratios and lack a comprehensive approach for compensating compressed weights after SVD truncation. Furthermore, most methods apply a uniform compression ratio across all transformer layers, overlooking the varying importance of different layers. $\ours$ seeks to address these limitations by proposing an adaptive compensation method (\textbf{adaComp}) and an importance-aware adaptive compression ratio method (\textbf{adaCR}).
	
	
	\vspace{-2.5mm}