@article{GOLUB1987317,
    title = {A generalization of the Eckart-Young-Mirsky matrix approximation theorem},
    journal = {Linear Algebra and its Applications},
    year = {1987},
    author = {G.H. Golub and Alan Hoffman and G.W. Stewart},
}

@inproceedings{ashkboos2024slicegpt,
      title={SliceGPT: Compress Large Language Models by Deleting Rows and Columns}, 
      author={Saleh Ashkboos and Maximilian L. Croci and Marcelo Gennari do Nascimento and Torsten Hoefler and James Hensman},
      year={2024},
      booktitle={ICLR}
}

@inproceedings{dong2024stbllm,
  title={STBLLM: Breaking the 1-Bit Barrier with Structured Binary LLMs},
  author={Dong, Peijie and Li, Lujun and Zhong, Yuedong and Du, Dayou and Fan, Ruibo and Chen, Yuhan and Tang, Zhenheng and Wang, Qiang and Xue, Wei and Guo, Yike and others},
  booktitle={ICLR},
  year={2025}
}

@inproceedings{frantar2022gptq,
  title={Gptq: Accurate post-training quantization for generative pre-trained transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  booktitle={ICLR},
  year={2023}
}

@inproceedings{frantar2023sparsegpt,
  title={Sparsegpt: Massive language models can be accurately pruned in one-shot},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={ICML},
  year={2023},
}

@inproceedings{gromov2024unreasonable,
      title={The Unreasonable Ineffectiveness of the Deeper Layers}, 
      author={Andrey Gromov and Kushal Tirumala and Hassan Shapourian and Paolo Glorioso and Daniel A. Roberts},
      year={2025},
      booktitle={ICLR}
}

@inproceedings{hsu2022languagemodel,
      title={Language model compression with weighted low-rank factorization}, 
      author={Yen-Chang Hsu and Ting Hua and Sungen Chang and Qian Lou and Yilin Shen and Hongxia Jin},
      year={2022},
      booktitle={ICLR}
}

@inproceedings{huang2024billm,
  title={Billm: Pushing the limit of post-training quantization for llms},
  author={Huang, Wei and Liu, Yangdong and Qin, Haotong and Li, Ying and Zhang, Shiming and Liu, Xianglong and Magno, Michele and Qi, Xiaojuan},
  booktitle={ICML},
  year={2024}
}

@inproceedings{li2024arb,
  title={ARB-LLM: Alternating Refined Binarizations for Large Language Models},
  author={Li, Zhiteng and Yan, Xianglong and Zhang, Tianao and Qin, Haotong and Xie, Dong and Tian, Jiang and Kong, Linghe and Zhang, Yulun and Yang, Xiaokang and others},
  booktitle={ICLR},
  year={2025}
}

@inproceedings{lin2024awq,
  title={AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song},
  booktitle={MLSys},
  year={2024}
}

@inproceedings{ma2023llmpruner,
      title={LLM-Pruner: On the Structural Pruning of Large Language Models}, 
      author={Xinyin Ma and Gongfan Fang and Xinchao Wang},
      year={2023},
      booktitle={NeurlPS}
}

@inproceedings{wang2024svdllm,
      title={SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression}, 
      author={Xin Wang and Yu Zheng and Zhongwei Wan and Mi Zhang},
      year={2025},
      booktitle={ICLR}
}

@inproceedings{yang2024laco,
      title={LaCo: Large Language Model Pruning via Layer Collapse}, 
      author={Yifei Yang and Zouying Cao and Hai Zhao},
      year={2024},
      booktitle={EMNLP}
}

@article{yuan2024asvd,
      title={ASVD: Activation-aware Singular Value Decomposition for Compressing Large Language Models}, 
      author={Zhihang Yuan and Yuzhang Shang and Yue Song and Qiang Wu and Yan Yan and Guangyu Sun},
      year={2024},
      journal={arXiv preprint arXiv:2312.05821}
}

