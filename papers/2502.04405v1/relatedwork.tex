\section{Related Works}
Existing ANN-SNN conversion methods can be categorized into two groups: one-stage and two-stage conversion.

% \subsection{Direct training} This approach aims to overcome the non-differentiable problem in training, facilitating the direct training of SNNs. For example, Bi-SNN~\cite{xiao2022towards} introduces a bi-directional spiking RNN designed for Natural Language Processing (NLP) tasks.
% To further explore larger models, SpikeGPT~\cite{zhu2023spikegpt} integrates the powerful Transformer architecture with SNNs.
% In addition, SpikeBERT~\cite{lv2024spikebertlanguagespikformerlearned} explores knowledge transfer from the ANN-based BERT model to spiking-based models using knowledge distillation, achieving lower energy consumption. However, the performance remains unsatisfactory compared to the ANN-based BERT model.
% To further enhance the performance, SpikingBERT~\cite{bal2024spikingbert} proposes a fully-operational spiking language model with an efficient spiking attention mechanism that achieves promising results. Nonetheless, the high-performance SpikingBERT relies on $80$ time steps, which leads to a considerable inference latency.
% SpikeLM~\cite{xing2024spikelm} introduces a novel elastic bi-spiking mechanism to further reduce the latency.
% In summary, surrogate gradients used in direct training methods introduce additional computational costs, compromising the low-cost advantage of SNN. In addition, such cost is higher for LLMs, even when knowledge distillation is employed.

\subsection{One-stage ANN-SNN Conversion} 
One-stage ANN-SNN conversion aims to directly convert ANN models to SNN models without any further optimization on the converted SNN models, e.g., fine-tuning. This type of method focuses on reducing the conversion as much as possible. For instance, Cao et al.~\cite{cao2015spiking} initially introduced the one-stage method by training ANNs with ReLU activation functions and then replacing these activations with spiking neurons. Based on this, Diehl et al.~\cite{Diehl2015FastclassifyingHS} proposed model-based and data-based normalization to narrow the gap between ANN and SNN. Furthermore, Sengupta et al.~\cite{Sengupta2018GoingDI} introduced scaling methods to normalize weights and thresholds of SNNs, improving the conversion performance. To further mitigate conversion loss, Rueckauer et al.~\cite{rueckauer2016theory} and Han et al.~\cite{han2020deep} introduced a ``reset-by-subtraction'' mechanism, which preserves temporal information and reduces information loss, enhancing precision during conversion. More recently, Bu et al.~\cite{Bu2022OptimizedPI} analyzed conversion error and proposed the Quantization Clip-Floor-Shift activation function to replace ReLU in ANNs. This method can effectively approximate the SNN activation function and reduce conversion loss. One-stage ANN-SNN conversion has shown promising performance, however, it typically requires a large number of time steps to achieve SOTA performance. Note that `time step' is the number of cycles used to analog the dynamic behavior of neurons, and plenty of time steps can cause the lengthy inference latency and huge energy consumption of the SNN model. Therefore, it is impractical to apply the one-stage methods to complex datasets and models with larger parameter scales, i.e., LLMs.


%QCFS[1]从理论上分析了ANN-SNN的转换误差，然后提出了量化裁剪偏移（quantization clip-floor-shift）激活函数来代替源ANNs中的ReLU激活函数，以更好地逼近SNNs的激活函数。我们算法的第一阶段采用了其量化裁剪偏移函数，但不同于其从头开始训练一个ANN，我们使用预训练大模型权重进行初始化并进行少量的全参数量微调，提高了效率。
%LTL[2] \cite{yang2022training}提出了一种本地串联学习规则（Local Tandem Learning (LTL) rule），通过遵从学生-老师的学习范式，利用预训练ANNs的中间特征表示来高效监督SNNs的训练，从而显著降低计算复杂度并加速网络收敛。
%EAC[3] \cite{li2024error}提出了一种逐层校准算法来优化SNN网络参数，首先使用网格搜索搜索最小化ANN和SNN激活值均方误差的膜阈值，然后采用贪心的方式从第一层网络进行逐层校验，使用随机梯度下降对模型权重、初始膜电位进行更新减少ANN和SNN的激活值误差。
% 与这两种算法相比较，我们的算法同样利用了ANN的中间特征来逐层校正，但与之不同的是，我们只对膜阈值和初始膜电位进行优化，而不是整个模型参数，相比他们的方法我们的方法更加简单高效，对计算资源要求更低。

\begin{figure}[!t]
\vskip 0.1in
    \centering
    \includegraphics[width=1\linewidth]{pic/overall3.pdf}
    \caption{The overall framework of the proposed FAS method. \textit{QC errors} is composed of the \textit{quantization error} and the \textit{clipping error}.}
    \vskip -0.1in
    \label{fig3}
\end{figure}

\subsection{Two-stage ANN-SNN Conversion}
Two-stage ANN-SNN conversion involves additionally optimizing the SNN converted by the one-stage methods to further to improve its performance. For example, SPR~\cite{hao2023reducing} proposed an optimization strategy that uses residual membrane potential to reduce unevenness errors for converted SNN models. Similarly, COS~\cite{Hao2023BridgingTG} optimized the converted SNN models by shifting the initial membrane potential. However, SPR and COS require additional time steps to gather necessary prior information, which can reduce efficiency. To address this, LTL~\cite{yang2022training} introduced a local tandem learning rule, which can efficiently guide the training of the converted SNN models. In addition, EAC~\cite{li2024error} proposed a layer-wise calibration algorithm to optimize the converted SNN models. Specifically, this method first used grid search to find the optimal membrane threshold. Then, it adopted a greedy strategy for layer-by-layer validation and used stochastic gradient descent to update the parameters of the SNN model. However, LTL and EAC need to optimize all parameters of the converted SNN model. In comparison, the proposed FAS method only optimizes the membrane threshold and initial membrane potential. FAS is simpler and more effective than other peer competitors, especially for LLMs.


% Two-stage ANN-SNN conversion involves further optimizing the converted SNN model to improve its performance. For example, SPR~\cite{hao2023reducing} establishes the mathematical relationship between residual membrane potential and unevenness error, proposing an optimization strategy that uses residual membrane potential to reduce unevenness errors. Similarly, COS \cite{Hao2023BridgingTG} detects offset spikes through the residual membrane potential and optimizes the conversion by shifting the initial membrane potential. However, both SPR and COS require additional time steps to gather necessary prior information, which can reduce efficiency.
% LTL \cite{yang2022training} introduces the Local Tandem Learning rule, which follows the student-teacher learning paradigm. By leveraging the intermediate feature representations of pre-trained ANNs, LTL efficiently supervises the training of SNNs, reducing computational complexity and accelerating network convergence.
% EAC  \cite{li2024error} introduced a layer-wise calibration algorithm to optimize the parameters of SNNs. This method first uses grid search to find the membrane threshold that minimizes the mean square error of the activation values between ANNs and SNNs. Then, it adopts a greedy approach for layer-by-layer validation starting from the first layer, using stochastic gradient descent to update the model weights and initial membrane potentials, reducing the activation value errors between ANNs and SNNs.
% In comparison, our algorithm also utilizes the intermediate features of ANNs for layer-wise correction but differs in that we optimize only the membrane threshold and initial membrane potential, rather than the entire model parameters. This makes our approach simpler, more efficient, and less resource-intensive than the others.
% In comparison to these two approaches, our algorithm also utilizes the intermediate features of ANNs for layer-wise correction. However, unlike these methods, we optimize only the membrane threshold and initial membrane potential, rather than the entire model parameters. Our approach is therefore simpler, more efficient, and less resource-intensive than theirs.

% Existing works in this category mainly focus on computer vision tasks, with few of them paying attention to language and vision-language tasks. SNN-RNN~\cite{diehl2016conversion} demonstrates the {\it `train-and-constrain'} methodology, which involves training RNN and discretizing the weights to satisfy SNN constraints. Moreover,  
% ConvSNN~\cite{Lv2023SpikingCN} presents a two-step {\it `conversion + fine-tuning'} method for training SNNs on text classification tasks. In the first step, an ANN is trained from scratch. In the second step, the SNN is further trained using surrogate gradients with the trained ANN weights. It can achieve highly competitive results with $50$ time steps. However, its lengthy time steps demand a huge number of computational resources.
% In comparison, our method is more efficient as it fine-tunes pre-trained models instead of training from scratch. Furthermore, we only optimize the thresholds and initial membrane potentials, avoiding the additional training for conversion. In addition, existing studies focus on the RNNs and CNNs. Thus, their performance is limited due to the model size.  Instead, our focus is on LLMs for high performance on different language and vision-language tasks.