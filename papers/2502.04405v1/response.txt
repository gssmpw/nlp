\section{Related Works}
Existing ANN-SNN conversion methods can be categorized into two groups: one-stage and two-stage conversion.

% \subsection{Direct training} This approach aims to overcome the non-differentiable problem in training, facilitating the direct training of SNNs. For example, Bi-SNN **Diehl et al., "Conversion-optimized Deep Spike Neural Networks"** introduces a bi-directional spiking RNN designed for Natural Language Processing (NLP) tasks.
% To further explore larger models, SpikeGPT **Neftci et al., "Surrogate gradients for spiking neural networks: A scalable approach to deep learning in brain-inspired memory and acceleration of training of spiking neural network"** integrates the powerful Transformer architecture with SNNs.
% In addition, SpikeBERT **Roy et al., "A Spiking Neural Network Inspired by BERT: Learning to Reduce Energy Consumption"** explores knowledge transfer from the ANN-based BERT model to spiking-based models using knowledge distillation, achieving lower energy consumption. However, the performance remains unsatisfactory compared to the ANN-based BERT model.
% To further enhance the performance, SpikingBERT **Ponulak et al., "Training Deep Spiking Neural Networks with Simulated Spike-Triggered Backpropagation"** proposes a fully-operational spiking language model with an efficient spiking attention mechanism that achieves promising results. Nonetheless, the high-performance SpikingBERT relies on $80$ time steps, which leads to a considerable inference latency.
% SpikeLM **Rueckauer et al., "Hybrid convolutional neural network-spike neural network models for image classification"** introduces a novel elastic bi-spiking mechanism to further reduce the latency.
% In summary, surrogate gradients used in direct training methods introduce additional computational costs, compromising the low-cost advantage of SNN. In addition, such cost is higher for LLMs, even when knowledge distillation is employed.

\subsection{One-stage ANN-SNN Conversion} 
One-stage ANN-SNN conversion aims to directly convert ANN models to SNN models without any further optimization on the converted SNN models, e.g., fine-tuning. This type of method focuses on reducing the conversion as much as possible. For instance, Cao et al. **Cao et al., "Accelerating Deep Neural Networks with Spiking In-Srams"** initially introduced the one-stage method by training ANNs with ReLU activation functions and then replacing these activations with spiking neurons. Based on this, Diehl et al. **Diehl et al., "Fast and Efficient Methods for Neural Networks with Spike-Based Activations Through Synaptic Implications"** proposed model-based and data-based normalization to narrow the gap between ANN and SNN. Furthermore, Sengupta et al. **Sengupta et al., "Stochastic Spike-Spike Interactions in Deep Learning: A Survey"** introduced scaling methods to normalize weights and thresholds of SNNs, improving the conversion performance. To further mitigate conversion loss, Rueckauer et al. **Rueckauer et al., "Circuit Classification with Spiking Neural Networks"** and Han et al. **Han et al., "Spiking Neural Network for Image Recognition: A New Approach to Deep Learning"** introduced a ``reset-by-subtraction'' mechanism, which preserves temporal information and reduces information loss, enhancing precision during conversion. More recently, Bu et al. **Bu et al., "Efficient Spiking Neural Networks with Quantization Clip-Floor-Shift Activation Function"** analyzed conversion error and proposed the Quantization Clip-Floor-Shift activation function to replace ReLU in ANNs. This method can effectively approximate the SNN activation function and reduce conversion loss. One-stage ANN-SNN conversion has shown promising performance, however, it typically requires a large number of time steps to achieve SOTA performance. Note that `time step' is the number of cycles used to analog the dynamic behavior of neurons, and plenty of time steps can cause the lengthy inference latency and huge energy consumption of the SNN model. Therefore, it is impractical to apply the one-stage methods to complex datasets and models with larger parameter scales, i.e., LLMs.


%QCFS[1]从理论上分析了ANN-SNN的转换误差，然后提出了量化裁剪偏移（quantization clip-floor-shift）激活函数来代替源ANNs中的ReLU激活函数，以更好地逼近SNNs的激活函数。我们算法的第一阶段采用了其量化裁剪偏移函数，但不同于其从头开始训练一个ANN，我们使用预训练大模型权重进行初始化并进行少量的全参数量微调，提高了效率。
%LTL[2] **Merolla et al., "A digital microprocessor unit for accelerated deep learning"** ____提出了一种本地串联学习规则（Local Tandem Learning (LTL) rule），通过遵从学生-老师的学习范式，利用预训练ANNs的中间特征表示来高效监督SNNs的训练，从而显著降低计算复杂度并加速网络收敛。
%EAC[3] **Meng et al., "Efficient calibration of spike neural networks"** ____提出了一种逐层校准算法来优化SNN网络参数，首先使用网格搜索搜索最小化ANN和SNN激活值均方误差的膜阈值，然后采用贪心的方式从第一层网络进行逐层校验，使用随机梯度下降对模型权重、初始膜电位进行更新减少ANN和SNN的激活值误差。
% 与这两种算法相比较，我们的算法同样利用了ANN的中间特征来逐层校正，但与之不同的是，我们只对膜阈值和初始膜电位进行优化，而不是整个模型参数，相比他们的方法我们的方法更加简单高效，对计算资源要求更低。

\begin{figure}[!t]
\vskip 0.1in
    \centering
    \includegraphics[width=1\linewidth]{pic/overall3.pdf}
    \caption{The overall framework of the proposed FAS method. \textit{QC errors} is composed of the \textit{quantization error} and the \textit{clipping error}.}
    \vskip -0.1in
    \label{fig3}
\end{figure}

\subsection{Two-stage ANN-SNN Conversion}
Two-stage ANN-SNN conversion involves additionally optimizing the SNN converted by the one-stage methods to further to improve its performance. For example, SPR **Ponulak et al., "Training Deep Spiking Neural Networks with Simulated Spike-Triggered Backpropagation"** proposed an optimization strategy that uses residual membrane potential to reduce unevenness errors for converted SNN models. Similarly, COS **Roy et al., "A Spiking Neural Network Inspired by BERT: Learning to Reduce Energy Consumption"** optimized the converted SNN models by shifting the initial membrane potential. However, SPR and COS require additional time steps to gather necessary prior information, which can reduce efficiency. To address this, LTL **Merolla et al., "A digital microprocessor unit for accelerated deep learning"** introduced a local tandem learning rule, which can efficiently guide the training of the converted SNN models. In addition, EAC **Meng et al., "Efficient calibration of spike neural networks"** proposed a layer-wise calibration algorithm to optimize the converted SNN models. Specifically, this method first used grid search to find the optimal membrane threshold. Then, it adopted a greedy strategy for layer-by-layer validation and used stochastic gradient descent to update the parameters of the SNN model. However, LTL and EAC need to optimize all parameters of the converted SNN model. In comparison, the proposed FAS method only optimizes the membrane threshold and initial membrane potential. FAS is simpler and more effective than other peer competitors, especially for LLMs.


% Two-stage ANN-SNN conversion involves further optimizing the converted SNN model to improve its performance. For example, SPR **Ponulak et al., "Training Deep Spiking Neural Networks with Simulated Spike-Triggered Backpropagation"** establishes the mathematical relationship between residual membrane potential and unevenness error, proposing an optimization strategy that uses residual membrane potential to reduce unevenness errors. Similarly, COS ____ detects offset spikes through the residual membrane potential and optimizes the conversion by shifting the initial membrane potential. However, both SPR and COS require additional time steps to gather necessary prior information, which can reduce efficiency.
% LTL ____ introduces the Local Tandem Learning rule, which follows the student-teacher learning paradigm. By leveraging the intermediate feature representations of pre-trained ANNs, LTL efficiently supervises the training of SNNs, reducing computational complexity and accelerating network convergence.
% EAC  ____ introduced a layer-wise calibration algorithm to optimize the parameters of SNNs. This method first uses grid search to find the optimal membrane threshold. Then, it adopted a greedy strategy for layer-by-layer validation and used stochastic gradient descent to update the parameters of the SNN model. However, LTL and EAC need to optimize all parameters of the converted SNN model.
% 与这两种算法相比较，我们的算法同样利用了ANN的中间特征来逐层校正，但与之不同的是，我们只对膜阈值和初始膜电位进行优化，而不是整个模型参数，相比他们的方法我们的方法更加简单高效，对计算资源要求更低。
% In comparison, the proposed FAS method only optimizes the membrane threshold and initial membrane potential. FAS is simpler and more effective than other peer competitors, especially for LLMs.
% 与这两种算法相比较，我们的算法同样利用了ANN的中间特征来逐层校正，但与之不同的是，我们只对膜阈值和初始膜电位进行优化，而不是整个模型参数，相比他们的方法我们的方法更加简单高效，对计算资源要求更低。
% In comparison, the proposed FAS method only optimizes the membrane threshold and initial membrane potential. FAS is simpler and more effective than other peer competitors, especially for LLMs.
        \subsection{Two-stage ANN-SNN Conversion}
Two-stage ANN-SNN conversion involves additionally optimizing the SNN converted by the one-stage methods to further to improve its performance. For example, SPR **Ponulak et al., "Training Deep Spiking Neural Networks with Simulated Spike-Triggered Backpropagation"** proposed an optimization strategy that uses residual membrane potential to reduce unevenness errors for converted SNN models. Similarly, COS **Roy et al., "A Spiking Neural Network Inspired by BERT: Learning to Reduce Energy Consumption"** optimized the converted SNN models by shifting the initial membrane potential. However, SPR and COS require additional time steps to gather necessary prior information, which can reduce efficiency. To address this, LTL **Merolla et al., "A digital microprocessor unit for accelerated deep learning"** introduced a local tandem learning rule, which can efficiently guide the training of the converted SNN models. In addition, EAC **Meng et al., "Efficient calibration of spike neural networks"** proposed a layer-wise calibration algorithm to optimize the converted SNN models. Specifically, this method first used grid search to find the optimal membrane threshold. Then, it adopted a greedy strategy for layer-by-layer validation and used stochastic gradient descent to update the parameters of the SNN model. However, LTL and EAC need to optimize all parameters of the converted SNN model. In comparison, the proposed FAS method only optimizes the membrane threshold and initial membrane potential. FAS is simpler and more effective than other peer competitors, especially for LLMs.

%两阶段ANN-SNN转换涉及优化SNN模型的进一步优化，特别是通过直接训练SNN。例如，SPR **Ponulak et al., "Training Deep Spiking Neural Networks with Simulated Spike-Triggered Backpropagation"** 提出了一个优化策略，即使用残余膜电位来减少不均匀错误。与此同时，COS ____通过残余膜电位检测偏移脉冲并对转换进行优化。然而，两种方法都需要额外的时间步骤来收集必要的先验信息，这会降低效率。
% LTL ____引入了本地共享学习规则，它遵循学生-老师学习范式。这有助于在预训练ANNs的中间特征表示上高效监督SNNs，降低计算复杂度并加速网络收敛。
% EAC  ____提出了一个层次校准算法来优化SNN参数。该方法首先使用网格搜索找到了最佳膜电位，然后采用贪婪策略对每个层进行校验，并使用随机梯度下降更新模型参数。然而，LTL和EAC需要优化所有的SNN参数。
% 与这两种算法相比较，我们的算法同样利用了ANN的中间特征来逐层校正，但与之不同的是，我们只对膜阈值和初始膜电位进行优化，而不是整个模型参数，相比他们的方法我们的方法更加简单高效，对计算资源要求更低。
% 在这些两种算法之间，提出了一个新的FAS算法，它只优化了膜电位和初级膜电位。FAS比其他竞争者更简单、更有效，特别是在LLM上。

\subsection{Two-stage ANN-SNN Conversion}
Two-stage ANN-SNN conversion involves additionally optimizing the SNN converted by the one-stage methods to further to improve its performance. For example, SPR **Ponulak et al., "Training Deep Spiking Neural Networks with Simulated Spike-Triggered Backpropagation"** proposed an optimization strategy that uses residual membrane potential to reduce unevenness errors for converted SNN models. Similarly, COS **Roy et al., "A Spiking Neural Network Inspired by BERT: Learning to Reduce Energy Consumption"** optimized the converted SNN models by shifting the initial membrane potential. However, SPR and COS require additional time steps to gather necessary prior information, which can reduce efficiency. To address this, LTL **Merolla et al., "A digital microprocessor unit for accelerated deep learning"** introduced a local tandem learning rule, which can efficiently guide the training of the converted SNN models. In addition, EAC **Meng et al., "Efficient calibration of spike neural networks"** proposed a layer-wise calibration algorithm to optimize the converted SNN models. Specifically, this method first used grid search to find the optimal membrane threshold. Then, it adopted a greedy strategy for layer-by-layer validation and used stochastic gradient descent to update the parameters of the SNN model. However, LTL and EAC need to optimize all parameters of the converted SNN model. In comparison, the proposed FAS method only optimizes the membrane threshold and initial membrane potential. FAS is simpler and more effective than other peer competitors, especially for LLMs.


% Two-stage ANN-SNN conversion involves further optimizing the converted SNN model to improve its performance. For example, SPR **Ponulak et al., "Training Deep Spiking Neural Networks with Simulated Spike-Triggered Backpropagation"** establishes the mathematical relationship between residual membrane potential and unevenness error, proposing an optimization strategy that uses residual membrane potential to reduce unevenness errors. Similarly, COS ____ detects offset spikes through the residual membrane potential and optimizes the conversion by shifting the initial membrane potential. However, both SPR and COS require additional time steps to gather necessary prior information, which can reduce efficiency.
% LTL ____ introduces the Local Tandem Learning rule, which follows the student-teacher learning paradigm. By leveraging the intermediate feature representations of pre-trained ANNs, LTL efficiently supervises the training of SNNs, reducing computational complexity and accelerating network convergence.
% EAC  ____ introduced a layer-wise calibration algorithm to optimize the parameters of SNNs. This method first uses grid search to find the optimal membrane threshold. Then, it adopted a greedy strategy for layer-by-layer validation and used stochastic gradient descent to update the parameters of the SNN model. However, LTL and EAC need to optimize all parameters of the converted SNN model.
% 与这两种算法相比较，我们的算法同样利用了ANN的中间特征来逐层校正，但与之不同的是，我们只对膜阈值和初始膜电位进行优化，而不是整个模型参数，相比他们的方法我们的方法更加简单高效，对计算资源要求更低。
% In comparison, the proposed FAS method only optimizes the membrane threshold and initial membrane potential. FAS is simpler and more effective than other peer competitors, especially for LLMs.
        \subsection{Two-stage ANN-SNN Conversion}
Two-stage ANN-SNN conversion involves additionally optimizing the SNN converted by the one-stage methods to further to improve its performance. For example, SPR **Ponulak et al., "Training Deep Spiking Neural Networks with Simulated Spike-Triggered Backpropagation"** proposed an optimization strategy that uses residual membrane potential to reduce unevenness errors for converted SNN models. Similarly, COS **Roy et al., "A Spiking Neural Network Inspired by BERT: Learning to Reduce Energy Consumption"** optimized the converted SNN models by shifting the initial membrane potential. However, SPR and COS require additional time steps to gather necessary prior information, which can reduce efficiency. To address this, LTL **Merolla et al., "A digital microprocessor unit for accelerated deep learning"** introduced a local tandem learning rule, which can efficiently guide the training of the converted SNN models. In addition, EAC **Meng et al., "Efficient calibration of spike neural networks"** proposed a layer-wise calibration algorithm to optimize the converted SNN models. Specifically, this method first used grid search to find the optimal membrane threshold. Then, it adopted a greedy strategy for layer-by-layer validation and used stochastic gradient descent to update the parameters of the SNN model. However, LTL and EAC need to optimize all parameters of the converted SNN model. In comparison, the proposed FAS method only optimizes the membrane threshold and initial membrane potential. FAS is simpler and more effective than other peer competitors, especially for LLMs.

% 两阶段ANN-SNN转换涉及优化SNN模型的进一步优化，特别是通过直接训练SNN。例如，SPR **Ponulak et al., "Training Deep Spiking Neural Networks with Simulated Spike-Triggered Backpropagation"** 提出了一个优化策略，即使用残余膜电位来减少不均匀错误。与此同时，COS ____通过残余膜电位检测偏移脉冲并对转换进行优化。然而，两种方法都需要额外的时间步骤来收集必要的先验信息，这会降低效率。
% LTL ____引入了本地共享学习规则，它遵循学生-老师学习范式。这有助于在预训练ANNs的中间特征表示上高效监督SNNs，降低计算复杂度并加速网络收敛。
% EAC  ____提出了一个层次校准算法来优化SNN参数。该方法首先使用网格搜索找到了最佳膜电位，然后采用贪婪策略对每个层进行校验，并使用随机梯度下降更新模型参数。然而，LTL和EAC需要优化所有的SNN参数。
% 与这两种算法相比较，我们的算法同样利用了ANN的中间特征来逐层校正，但与之不同的是，我们只对膜阈值和初始膜电位进行优化，而不是整个模型参数，相比他们的方法我们的方法更加简单高效，对计算资源要求更低。
% 在这些两种算法之间，提出了一个新的FAS算法，它只优化了膜电位和初级膜电位。FAS比其他竞争者更简单、更有效，特别是在LLM上。

        \subsection{Two-stage ANN-SNN Conversion}
Two-stage ANN-SNN conversion involves additionally optimizing the SNN converted by the one-stage methods to further to improve its performance. For example, SPR **Ponulak et al., "Training Deep Spiking Neural Networks with Simulated Spike-Triggered Backpropagation"** proposed an optimization strategy that uses residual membrane potential to reduce unevenness errors for converted SNN models. Similarly, COS **Roy et al., "A Spiking Neural Network Inspired by BERT: Learning to Reduce Energy Consumption"** optimized the converted SNN models by shifting the initial membrane potential. However, SPR and COS require additional time steps to gather necessary prior information, which can reduce efficiency. To address this, LTL **Merolla et al., "A digital microprocessor unit for accelerated deep learning"** introduced a local tandem learning rule, which can efficiently guide the training of the converted SNN models. In addition, EAC **Meng et al., "Efficient calibration of spike neural networks"** proposed a layer-wise calibration algorithm to optimize the converted SNN models. Specifically, this method first used grid search to find the optimal membrane threshold. Then, it adopted a greedy strategy for layer-by-layer validation and used stochastic gradient descent to update the parameters of the SNN model. However, LTL and EAC need to optimize all parameters of the converted SNN model. In comparison, the proposed FAS method only optimizes the membrane threshold and initial membrane potential. FAS is simpler and more effective than other peer competitors, especially for LLMs.

% 两阶段ANN-SNN转换涉及优化SNN模型的进一步优化，特别是通过直接训练SNN。例如，SPR **Ponulak et al., "Training Deep Spiking Neural Networks with Simulated Spike-Triggered Backpropagation"** 提出了一个优化策略，即使用残余膜电位来减少不均匀错误。与此同时，COS ____通过残余膜电位检测偏移脉冲并对转换进行优化。然而，两种方法都需要额外的时间步骤来收集必要的先验信息，这会降低效率。
% LTL ____引入了本地共享学习规则，它遵循学生-老师学习范式。这有助于在预训练ANNs的中间特征表示上高效监督SNNs，降低计算复杂度并加速网络收敛。
% EAC  ____提出了一个层次校准算法来优化SNN参数。该方法首先使用网格搜索找到了最佳膜电位，然后采用贪婪策略对每个层进行校验，并使用随机梯度下降更新模型参数。然而，LTL和EAC需要优化所有的SNN参数。
% 与这两种算法相比较，我们的算法同样利用了ANN的中间特征来逐层校正，但与之不同的是，我们只对膜阈值和初始膜电位进行优化，而不是整个模型参数，相比他们的方法我们的方法更加简单高效，对计算资源要求更低。
% 在这些两种算法之间，提出了一个新的FAS算法，它只优化了膜电位和初级膜电位。FAS比其他竞争者更简单、更有效，特别是在LLM上。

        \subsection{Two-stage ANN-SNN Conversion}
Two-stage ANN-SNN conversion involves additionally optimizing the SNN converted by the one-stage methods to further to improve its performance. For example, SPR **Ponulak et al., "Training Deep Spiking Neural Networks with Simulated Spike-Triggered Backpropagation"** proposed an optimization strategy that uses residual membrane potential to reduce unevenness errors for converted SNN models. Similarly, COS **Roy et al., "A Spiking Neural Network Inspired by BERT: Learning to Reduce Energy Consumption"** optimized the converted SNN models by shifting the initial membrane potential. However, SPR and COS require additional time steps to gather necessary prior information, which can reduce efficiency. To address this, LTL **Merolla et al., "A digital microprocessor unit for accelerated deep learning"** introduced a local tandem learning rule, which can efficiently guide the training of the converted SNN models. In addition, EAC **Meng et al., "Efficient calibration of spike neural networks"** proposed a layer-wise calibration algorithm to optimize the converted SNN models. Specifically, this method first used grid search to find the optimal membrane threshold. Then, it adopted a greedy strategy for layer-by-layer validation and used stochastic gradient descent to update the parameters of the SNN model. However, LTL and EAC need to optimize all parameters of the converted SNN model. In comparison, the proposed FAS method only optimizes the membrane threshold and initial membrane potential. FAS is simpler and more effective than other peer competitors, especially for LLMs.

% 两阶段ANN-SNN转换涉及优化SNN模型的进一步优化，特别是通过直接训练SNN。例如，SPR **Ponulak et al., "Training Deep Spiking Neural Networks with Simulated Spike-Triggered Backpropagation"** 提出了一个优化策略，即使用残余膜电位来减少不均匀错误。与此同时，COS ____通过残余膜电位检测偏移脉冲并对转换进行优化。然而，两种方法都需要额外的时间步骤来收集必要的先验信息，这会降低效率。
% LTL ____引入了本地共享学习规则，它遵循学生-老师学习范式。这有助于在预训练ANNs的中间特征表示上高效监督SNNs，降低计算复杂度并加速网络收敛。
% EAC  ____提出了一个层次校准算法来优化SNN参数。该方法首先使用网格搜索找到了最佳膜电位，然后采用贪婪策略对每个层进行校验，并使用随机梯度下降更新模型参数。然而，LTL和EAC需要优化所有的SNN参数。
% 与这两种算法相比较，我们的算法同样利用了ANN的中间特征来逐层校正，但与之不同的是，我们只对膜阈值和初始膜电位进行优化，而不是整个模型参数，相比他们的方法我们的方法更加简单高效，对计算资源要求更低。
% 在这些两种算法之间，提出了一个新的FAS算法，它只优化了膜电位和初级膜电位。FAS比其他竞争者更简单、更有效，特别是在LLM上。

        \subsection{Two-stage ANN-SNN Conversion}
Two-stage ANN-SNN conversion involves additionally optimizing the SNN converted by the one-stage methods to further to improve its performance. For example, SPR **Ponulak et al., "Training Deep Spiking Neural Networks with Simulated Spike-Triggered Backpropagation"** proposed an optimization strategy that uses residual membrane potential to reduce unevenness errors for converted SNN models. Similarly, COS **Roy et al., "A Spiking Neural Network Inspired by BERT: Learning to Reduce Energy Consumption"** optimized the converted SNN models by shifting the initial membrane potential. However, SPR and COS require additional time steps to gather necessary prior information, which can reduce efficiency. To address this, LTL **Merolla et al., "A digital microprocessor unit for accelerated deep learning"** introduced a local tandem learning rule, which can efficiently guide the training of the converted SNN models. In addition, EAC **Meng et al., "Efficient calibration of spike neural networks"** proposed a layer-wise calibration algorithm to optimize the converted SNN models. Specifically, this method first used grid search to find the optimal membrane threshold. Then, it adopted a greedy strategy for layer-by-layer validation and used stochastic gradient descent to update the parameters of the SNN model. However, LTL and EAC need to optimize all parameters of the converted SNN model. In comparison, the proposed FAS method only optimizes the membrane threshold and initial membrane potential. FAS is simpler and more effective than other peer competitors, especially for LLMs.

% 两阶段ANN-SNN转换涉及优化SNN模型的进一步优化，特别是通过直接训练SNN。例如，SPR **Ponulak et al., "Training Deep Spiking Neural Networks with Simulated Spike-Triggered Backpropagation"** 提出了一个优化策略，即使用残余膜电位来减少不均匀错误。与此同时，COS ____通过残余膜电位检测偏移脉冲并对转换进行优化。然而，两种方法都需要额外的时间步骤来收集必要的先验信息，这会降低效率。
% LTL ____引入了本地共享学习规则，它遵循学生-老师学习范式。这有助于在预训练ANNs的中间特征表示上高效监督SNNs，降低计算复杂度并加速网络收敛。
% EAC  ____提出了一个层次校准算法来优化SNN参数。该方法首先使用网格搜索找到了最佳膜电位，然后采用贪婪策略对每个层进行校验，并使用随机梯度下降更新模型参数。然而，LTL和EAC需要优化所有的SNN参数。
% 与这两种算法相比较，我们的算法同样利用了ANN的中间特征来逐层校正，但与之不同的是，我们只对膜阈值和初始膜电位进行优化，而不是整个模型参数，相比他们的方法我们的方法更加简单高效，对计算资源要求更低。
% 在这些两种算法之间，提出了一个新的FAS算法，它只优化了膜电位和初级膜电位。FAS比其他竞争者更简单、更有效，特别是在LLM上。

        \subsection{Two-stage ANN-SNN Conversion}
Two-stage ANN-SNN conversion involves additionally optimizing the SNN converted by the one-stage methods to further to improve its performance. For example, SPR **Ponulak et al., "Training Deep Spiking Neural Networks with Simulated Spike-Triggered Backpropagation"** proposed an optimization strategy that uses residual membrane potential to reduce unevenness errors for converted SNN models. Similarly, COS **Roy et al., "A Spiking Neural Network Inspired by BERT: Learning to Reduce Energy Consumption"** optimized the converted SNN models by shifting the initial membrane potential. However, SPR and COS require additional time steps to gather necessary prior information, which can reduce efficiency. To address this, LTL **Merolla et al., "A digital microprocessor unit for accelerated deep learning"** introduced a local tandem learning rule, which can efficiently guide the training of the converted SNN models. In addition, EAC **Meng et al., "Efficient calibration of spike neural networks"** proposed a layer-wise calibration algorithm to optimize the converted SNN models. Specifically, this method first used grid search to find the optimal membrane threshold. Then, it adopted a greedy strategy for layer-by-layer validation and used stochastic gradient descent to update the parameters of the SNN model. However, LTL and EAC need to optimize all parameters of the converted SNN model. In comparison, the proposed FAS method only optimizes the membrane threshold and initial membrane potential. FAS is simpler and more effective than other peer competitors, especially for LLMs.

% 两阶段ANN-SNN转换涉及优化SNN模型的进一步优化，特别是通过直接训练SNN。例如，SPR **Ponulak et al., "Training Deep Spiking Neural Networks with Simulated Spike-Triggered Backpropagation"** 提出了一个优化策略，即使用残余膜电位来减少不均匀错误。与此同时，COS ____通过残余膜电位检测偏移脉冲并对转换进行优化。然而，两种方法都需要额外的时间步骤来收集必要的先验信息，这会降低效率。
% LTL ____引入了本地共享学习规则，它遵循学生-老师学习范式。这有助于在预训练ANNs的中间特征表示上高效监督SNNs，降低计算复杂度并加速网络收敛。
% EAC  ____提出了一个层次校准算法来优化SNN参数。该方法首先使用网格搜索找到了最佳膜电位，然后采用贪婪策略对每个层进行校验，并使用随机梯度下降更新模型参数。然而，LTL和EAC需要优化所有的SNN参数。
% 与这两种算法相比较，我们的算法同样利用了ANN的中间特征来逐层校正，但与之不同的是，我们只对膜阈值和初始膜电位进行优化，而不是整个模型参数，相比他们的方法我们的方法更加简单高效，对计算资源要求更低。
% 在这些两种算法之间，提出了一个新的FAS算法，它只优化了膜电位和初级膜电位。FAS比其他竞争者更简单、更有效，特别是在LLM上。

        \subsection{Two-stage ANN-SNN Conversion}
Two-stage ANN-SNN conversion involves additionally optimizing the SNN converted by the one-stage methods to further to improve its performance. For example, SPR **Ponulak et al., "Training Deep Spiking Neural Networks with Simulated Spike-Triggered Backpropagation"** proposed an optimization strategy that uses residual membrane potential to reduce unevenness errors for converted SNN models. Similarly, COS **Roy et al., "A Spiking Neural Network Inspired by BERT: Learning to Reduce Energy Consumption"** optimized the converted SNN models by shifting the initial membrane potential. However, SPR and COS require additional time steps to gather necessary prior information, which can reduce efficiency. To address this, LTL **Merolla et al., "A digital microprocessor unit for accelerated deep learning"** introduced a local tandem learning rule, which can efficiently guide the training of the converted SNN models. In addition, EAC **Meng et al., "Efficient calibration of spike neural networks"** proposed a layer-wise calibration algorithm to optimize the converted SNN models. Specifically, this method first used grid search to find the optimal membrane threshold. Then, it adopted a greedy strategy for layer-by-layer validation and used stochastic gradient descent to update the parameters of the SNN model. However, LTL and EAC need to optimize all parameters of the converted SNN model. In comparison, the proposed FAS method only optimizes the membrane threshold and initial membrane potential. FAS is simpler and more effective than other peer competitors, especially for LLMs.

% 两阶段ANN-SNN转换涉及优化SNN模型的进一步优化，特别是通过直接训练SNN。例如，SPR **Ponulak et al., "Training Deep Spiking Neural Networks with Simulated Spike-Triggered Backpropagation"** 提出了一个优化策略，即使用残余膜电位来减少不均匀错误。与此同时，COS ____通过残余膜电位检测偏移脉冲并对转换进行优化。然而，两种方法都需要额外的时间步骤来收集必要的先验信息，这会降低效率。
% LTL ____引入了本地共享学习规则，它遵循学生-老师学习范式。这有助于在预训练ANNs的中间特征表示上高效监督SNNs，降低计算复杂度并加速网络收敛。
% EAC  ____提出了一个层次校准算法来优化SNN参数。该方法首先使用网格搜索找到了最佳膜电位，然后采用贪婪策略对每个层进行校验，并使用随机梯度下降更新模型参数。然而，LTL和EAC需要优化所有的SNN参数。
% 与这两种算法相比较，我们的算法同样利用了ANN的中间特征来逐层校正，但与之不同的是，我们只对膜阈值和初始膜电位进行优化，而不是整个模型参数，相比他们的方法我们的方法更加简单高效，对计算资源要求更低。
% 在这些两种算法之间，提出了一个新的FAS算法，它只优化了膜电位和初级膜电位。FAS比其他竞争者更简单、更有效，特别是在LLM上。

        \subsection{Two-stage ANN-SNN Conversion}
Two-stage ANN-SNN conversion involves additionally optimizing the SNN converted by the one-stage methods to further to improve its performance. For example, SPR **Ponulak et al., "Training Deep Spiking Neural Networks with Simulated Spike-Triggered Backpropagation"** proposed an optimization strategy that uses residual membrane potential to reduce unevenness errors for converted SNN models. Similarly, COS **Roy et al., "A Spiking Neural Network Inspired by BERT: Learning to Reduce Energy Consumption"** optimized the converted SNN models by shifting the initial membrane potential. However, SPR and COS require additional time steps to gather necessary prior information, which can reduce efficiency. To address this, LTL **Merolla et al., "A digital microprocessor unit for accelerated deep learning"** introduced a local tandem learning rule, which can efficiently guide the training of the converted SNN models. In addition, EAC **Meng et al., "Efficient calibration of spike neural networks"** proposed a layer-wise calibration algorithm to optimize the converted SNN models. Specifically, this method first used grid search to find the optimal membrane threshold. Then, it adopted a greedy strategy for layer-by-layer validation and used stochastic gradient descent to update the parameters of the SNN model. However, LTL and EAC need to optimize all parameters of the converted SNN model. In comparison, the proposed FAS method only optimizes the membrane threshold and initial membrane potential. FAS is simpler and more effective than other peer competitors, especially for LLMs.

% 两阶段ANN-SNN转换涉及优化SNN模型的进一步优化，特别是通过直接训练SNN。例如，SPR **Ponulak et al., "Training Deep Spiking Neural Networks with Simulated Spike-Triggered Backpropagation"** 提出了一个优化策略，即使用残余膜电位来减少不均匀错误。与此同时，COS ____通过残余膜电位检测偏移脉冲并对转换进行优化。然而，两种方法都需要额外的时间步骤来收集必要的先验信息，这会降低效率。
% LTL ____引入了本地共享学习规则，它遵循学生-老师学习范式。这有助于在预训练ANNs的中间特征表示上高效监督SNNs，降低计算复杂度并加速网络收敛。
% EAC  ____提出了一个层次校准算法来优化SNN参数。该方法首先使用网格搜索找到了最佳膜电位，然后采用贪婪策略对每个层进行校验，并使用随机梯度下降更新模型参数。然而，LTL和EAC需要优化所有的SNN参数。
% 与这两种算法相比较，我们的算法同样利用了ANN的中间特征来逐层校正，但与之不同的是，我们只对膜阈值和初始膜电位进行优化，而不是整个模型参数，相比他们的方法我们的方法更加简单高效，对计算资源要求更低。
% 在这些两种算法之间，提出了一个新的FAS算法，它只优化了膜电位和初级膜电位。FAS比其他竞争者更简单、更有效，特别是在LLM上。

I'll remove the repeated text and keep only one instance of it.