We introduce a two-stage approach as shown in \refFig{fig:framework} that integrates recent developments in LLMs with subjective rationalizability in hypergames to solve the problem of dynamic coalition structure prediction, as defined in Section~\ref{sec:problem}.
% \ak{Add figure.}

The first stage identify the set of candidate agreements $\agreements_t$ being discussed given a play $\rho_t$. 
%In the first stage, we identify the set of agreements $\agreements_t$ being discussed given a play $\rho_t$. 
We assume that all agreements are discussed in natural language and no side channels exist for forming agreements. 
% Given this assumption, the first stage of our approach is designed to identify the set of agreements $\agreements_t$ being discussed given a play $\rho_t$. 
Letting $C_t$ be the coalition structure at round $t$, the set $\agreements_t$ determines the set of edges of $C_t$.


The second stage assigns weights to the edges in $\agreements_t$, referred to as the \emph{rationalizability score}. 
For an agreement $\alpha \in \agreements_t$, this score represents the likelihood that an external observer, with access to the full state, action, and dialogue history, believes that $\alpha$ will be honored by both players. 
To compute the rationalizability score $\weight_i(\alpha)$, we estimate the likelihood of a player $P_i$ honoring the agreement using a L2-hypergame constructed by filtering the messages to include only those exchanged between the two players involved in $\alpha$.


Formally, the rationalizability score of an agreement $\alpha$ for a player $P_i$ is computed by evaluating the strategic value (utility) of $\alpha$ for $P_i$ in its hypergame $H_i^1$. Formally, it is given by
\begin{align*}
	\weight_i(\alpha) = V_i(\alpha) * V_{j}^i(\alpha),
\end{align*}
where $V_i(\alpha)$ is the game-theoretic value of agreement for $P_i$ and $V_{j}^i(\alpha)$ is $P_i$'s belief about the likelihood of $P_j$ honoring the agreement.
Based on \refDef{def:sr}, this weight reflects how subjectively rationalizable an agreement is for $P_i$, with higher weights indicating greater rationalizability.


Given the rationalizability scores $\weight_i(\alpha)$ and $\weight_j(\alpha)$ of $P_i$ and $P_j$, respectively, the rationalizability score of the agreement $\alpha$ for an external observer is given by 
\begin{align*}
	\weight(\alpha) = \weight_i(\alpha) * \weight_j(\alpha).
\end{align*}
%We refer to $\weight(\alpha)$ as the the rationalizability score of $\alpha$. 

Note that the proposed two stage approach separates the language-based reasoning from game-theoretic one.
The strategic values $V_i$ and $V_j^i$ are derived from game-theoretic solution concepts and do not rely on dialogue. 
Whereas, the set of agreements $\agreements_t$ is inferred directly from the dialogue.

%To compute the rationalizability score $\weight_i(\alpha)$, we first estimate the likelihood of a player $P_i$ honoring the agreement based on a L2-hypergame constructed by filtering the messages to include only those involving the two players involved in $\alpha$. 
%As illustrated in Fig.~\ref{}, a player's subjective view is constructed by filtering the messages they sent or received. 
%The collective subjective views of all players form the second-level hypergame.

%The second stage assigns weights to the edges in $\agreements_t$.
%We refer to these weights as \emph{rationalizability score}.
%For an agreement $\alpha \in \agreements_t$, the rationalizability score represents the likelihood that an external observer (who has access to all state, action, and dialogue history) believes that $\alpha$ will be honored by both players. 
%To compute the rationalizability score $\weight_i(\alpha)$ for agreement $\alpha$, we first determine the likelihood that a player $P_i$ will honor an agreement given its subjective view of the game. 
%As shown in Fig.~\ref{}, the subjective view of a player is constructed by filtering the messages sent by or received by that player. 
%The collection of subjective games of all players constitutes the second-level hypergame. 


%The second stage focuses on assigning weights to the edges in $C_t$, representing the likelihood that both players will honor an agreement, as perceived by an external observer. To achieve this, we calculate the strategic value (i.e., utility) for each player by constructing their subjective view of the game, which is captured by the following equation:
%\begin{align*}
%	\weight_i(\alpha) = V_i(\alpha) * V_{j}^i(\alpha),
%\end{align*}
%where $V_i(\alpha)$ is the game-theoretic value of agreement for $P_i$ and $V_{j}^i(\alpha)$ is the likelihood that $P_i$ believes that the other player will honor the agreement. 
%Hence, based on \refDef{def:sr}, the weight gives a player an indication of how rationalizable an agreement is, with higher weights corresponding to agreements that are more subjectively rationalizable.
%
%
%Within each player's subjective view, the strategic value of an agreement is evaluated using a game-theoretic solution concept, without considering any dialogue between the players. On the other hand, the likelihood assessment incorporates insights gained from natural language negotiations with the other player.

% To construct $C_t$ from perspective of external observer, we first determine how rationalizable a given agreement $\alpha = (u_1, u_2, a_1, a_2) \in \agreements$ is for each player $i = 1, 2$ given by
% \begin{align*}
% 	\weight_i(\alpha) = V_i(\alpha) * V_{3-i}^i(\alpha),
% \end{align*}
% where $V_i(\alpha)$ is the game-theoretic value of agreement for $P_i$ and $V_{3-i}^i(\alpha)$ is the game-theoretic value of agreement for the other player as perceived by $P_i$. 

%Given $\weight_1(\alpha)$ and $\weight_2(\alpha)$, the weight of $\alpha$ for an external observer is given by 
%\begin{align*}
%	\weight(\alpha) = \weight_1(\alpha) * \weight_2(\alpha).
%\end{align*}
%We refer to $\weight(\alpha)$ as the the rationalizability score of $\alpha$. 

%We employ subjective rationalizability to define the weight function for an agreement. 
%Without loss of generality, let $\alpha = (u_1, u_2, a_1, a_2) \in \agreements$ be an agreement between players $P_1$ and $P_2$. 
%\begin{align*}
%	\weight(\alpha) = V_1(\alpha) * V_2^1(\alpha)
%\end{align*}

In the remainder of this section, we first outline how agreements are identified from dialogue, followed by the computation of their rationalizability score.

\subsection{Agreement Detection}\label{sec:agreement}
To detect agreements in Diplomacy gameplay from game transcripts, we combine (1) a filtering stage where mentioned locations that a coalition can be formed over are extracted by a language model, and (2) an intent extraction stage where specialized Diplomacy models are used to extract player intents for classification. 

Figure~\ref{fig:detection} outlines our method for agreement detection. First, for each state-players tuple $(S, P_1, P_2)$, we first prompt GPT-4o\footnote{\url{https://platform.openai.com/docs/models/gpt-4o}}, a strong language model, with the dialogue between the two players at state $S$ and information about the Diplomacy board. We then use the model to extract all locations that were explicitly mentioned in negotiation between $P_1$ and $P_2$. In addition to information about whether two countries are sufficiently close to form a coalition, we will use this in a later filtering step. 

We then leverage the intent models used in CICERO \citep{CICERO}; these are 2.7-billion parameter language models that predict player actions from dialogue. Specifically, they are trained using behavioral cloning over a subset of ``truthful'' player dialogues collected from WebDiplomacy. Notably, this intent model only takes the conversation between $P_1$ and $P_2$ for a given phase, excluding any dialogue either player had with other players, to restrict the model to direct coordination between the two players.  By computing a distribution of move likelihoods over all possible moves for a unit before and after player dialogue, we can estimate whether a coalition was formed over the unit in question. We extract for each $(S, P_1, P_2, u \in u_1 \cup u_2)$ a most likely action $a^*$ for the unit in this state. We also compute the probability of $a^*$ before and after dialogue, as well as the entropy of the distribution of moves $P(a | S, P_1, P_2)$ before and after dialogue. After filtering out all units where a coalition is not possible, or where none of the territories involved in the move are mentioned in the dialogue, we then train a logistic regression classifier on these features to predict whether a coalition was formed over the unit in question.

This method allows us to leverage the advantages of both using a larger, general language model and a smaller, Diplomacy-specialized language model. While using the intent model allows us to capture more implicit coalition agreements that may not be identified with an explicit parser, it may also raise many false negatives due to noise in how the distribution changes as a result of unrelated dialogue. Adding a filtering step allows us to identify cases where the distribution shifts due to identifiable discussion of the provinces in question, as identified by the larger model. Indeed, in Section~\ref{sec:valid}, we show that this hybrid method outperforms methods that only rely on large language model annotation or learning from intent distributions.

\begin{figure*}
    \includegraphics[width=0.8\textwidth]{figures/agreement.pdf}
    \caption{An overview of our agreement detection framework. In this case, we are analyzing whether Italy and Austria have come to an agreement over Italy's unit F ION, and determine that an agreement has been reached for Italy to move this unit to the Eastern Mediterranean Sea (EAS).}
    \label{fig:detection}
\end{figure*}

\subsection{Strategic Value of Agreements}
%\aknote{
%Challenge: Large game action space.
%Review DNVI algorithm. 
%
%DNVI algorithm provides value for actions, not agreements. To obtain value for agreements, define formula. 
%
%Explain insight behind this formula. 
%
%Q. Can you prove asymptotic property of value: if sampled sufficiently enough, the computed value of agreement converges to true value?
%}








%\textbf{Challenge.}
Determining the strategic value $V_i(\alpha)$ of an agreement $\alpha = (u_1, u_2,$ $ a_1, a_2) \in \agreements$ for a player $P_i$ is a challenging task in large games like Diplomacy. 
It requires $P_i$ to determine the rational actions for all units controlled by $P_i$ as well as the other players conditioned on the unit $u_1$ being assigned action $a_1$ and the unit $u_2$ being assigned action $a_2$. 
Traditional game-theoretic approaches \citep{gibbons1992primer} enumerate all possible actions and evaluate them under a solution concept to determine the action that yields highest value from a given state. 
These approaches are inapplicable to games like Diplomacy due to the large size of players' action spaces.

%\textbf{Our approach: Deep RL intro}
Instead, we employ a deep reinforcement learning approach that first learns a probability distribution 
\begin{align} \label{eq:joint-action-distribution}
	\prob(a \mid s_0, \ldots, s_t, a_0, \ldots, a_{t-1},  \alpha)
\end{align} 
over joint actions of all players conditioned on $P_i$ and $P_j$ honoring a given agreement in addition to the state and action histories.
Intuitively, every joint action in the support of the distribution in \refEq{eq:joint-action-distribution} constitutes a Nash equilibrium in which $P_i$ and $P_j$ honor the agreement $\alpha$.


% Then, we sample from this distribution to compute the expected value of the next state. These steps are described in detail below.
%Lastly, we determine the value of agreement by comparing the expected value with 


\textbf{Learning joint action distribution.} 
We leverage order sampling models trained as part of the CICERO agent \citep{CICERO}, which use Double oracle reinforcement learning for action exploration (DORA) \citep{bakhtin2021no} to learn the distribution in \refEq{eq:joint-action-distribution}. DORA simultaneously learns a state-value function and an joint action probability distribution using neural networks trained by bootstrapping on an approximate Nash equilibrium for the stage game each turn.

DORA is a Nash Q-Learning based approach to approximate Nash equilibrium in games with large state and action spaces.
It accommodates the large action spaces of Diplomacy by training a neural network $\pi(s; \theta_\pi)$ to predict joint action probability distribution with parameters $\theta_\pi$ that approximates the distribution of actions under the equilibrium policy at state $s$.
The candidate actions to explore are determined by sampling a large number of actions from $\pi(s; \theta_\pi)$ for each player and selecting actions with highest likelihood.
The Nash equilibrium is then estimated using regret minimization \citep{foster1999regret} in the matrix sub-game that includes only the sampled actions, assuming that the values of successor states are given by a learned network $\mathbf{V}(s; \theta_v)$, using the following update equation: 
% 
\begin{align*}
	\mathbf{V}(s) \gets (1 - \beta) \mathbf{V}(s) + \beta(r(s) + \gamma \sigma(a) \mathbf{V}(T(s, a))).
\end{align*}


%exactly computing the Nash equilibrium sigma for the 1-step matrix game at a state is also infeasible. Therefore, we train a policy proposal network pi(s;theta-pi) with parameters theta-pi that approximates the distribution of actions under the equilibrium policy at state s. 
%We generate a candidate action set at state s by sampling Nb actions from pi(s;theta-pi) for each player and selecting the Nc Nb actions with highest likelihood. We then approximate a NE sigma via RM (described in Section 2.5) in the restricted matrix game in which each player is limited to their Nc actions assuming successor states' values are given by our current value network V (s;theta-v).
%We refer interested readers to [cite] for more details.


% Our motivation to use DORA is due to the prior work on Diplomacy and 6-player poker, which has shown that DORA and similar approaches often perform well in practice [cite]. 
We refer interested readers to \citep{bakhtin2021no} for more details on the implementation. While we rely on CICERO-trained models for this work, versions of all of the specialized Diplomacy models used can be trained in novel game settings where human data is available \citep{bakhtin2021no}.

\textbf{Value of agreement.} 
The order sampling model and the value model provide a way to determine not only the joint action probabilities conditioned on an agreement, but also the value of the resulting state. 
Hence, we determine the value of an agreement by sampling from these distributions and computing the expected value of next state reached by the player by honoring $\alpha$,
\begin{align} \label{eq:value-of-agreement}
	V_i(\alpha) = \sum \prob(a \mid \vec{s}_t, \vec{a}_{t-1}, \alpha)  \mathbf{V}(s'),
\end{align}
where $s' = T(s, a)$ is the new state reached when joint action $a$ is performed in state $s$. 

%
%\hrule 
%
%Determining the game-theoretic value $V_i(\alpha)$ of an agreement $\alpha \in \agreements$ for a player $P_i$ is a challenging task in large games like Diplomacy, in which the number of legal actions for a player on a turn is often more than $10^{20}$ [cite].
%Traditional approaches ...  enumerative approach ... 
%
%Here, we build upon the Deep RL based approach proposed in [cite].  In principle, this approach provides a method to measure a Shapley value of a state \ak{validate statement} for a player by approximating the Nash equilibrium using Deep RL approach.
%Our motivation behind using this method is the convergence guarantees it provides ... state them. Also, the practical success demonstrated by deep Nash value iteration algorithm, when initialized with a human-bootstrapped model, defeats multiple prior human-derived agents in no-press Diplomacy by a wide margin.
%
%Although DNVI computes value of a state for a player, it does not provide any insight about how a player can enforce a visit to that state. 
%A subset of agreements might be needed to enforce a transition to that state. 
%A specific agreement may be a part of several such subsets. 
%Hence, a player must determine the value of each agreements before deciding the subset of agreements to honor.  
%
%
%Next, we briefly introduce DNVI approach highlighting its salient features and then, show how to compute the value of an agreement utilizing the value of a state. 
%
%
%DORA simultaneously learns a state-value function and an action proposal distribution via neural networks trained by bootstrapping on an approximate Nash equilibrium for the stage game each turn.
%
%The core of the learning procedure is based on Nash Q-Learning as presented in Section 2.3, but simplified to use only a value function, with adaptations for large action spaces and function approximation.
%
%Copy Eq. (5). 
%
%Since the state space is large, we use a deep neural network V (s;theta-v) with parameters theta-v to approximate V (s). Since the action space is large, exactly computing the Nash equilibrium sigma for the 1-step matrix game at a state is also infeasible. Therefore, we train a policy proposal network pi(s;theta-pi) with parameters theta-pi that approximates the distribution of actions under the equilibrium policy at state s. We generate a candidate action set at state s by sampling Nb actions from pi(s;theta-pi) for each player and selecting the Nc Nb actions with highest likelihood. We then approximate a NE sigma via RM (described in Section 2.5) in the restricted matrix game in which each player is limited to their Nc actions assuming successor states' values are given by our current value network V (s;theta-v).
%
%\ak{Copy Loss functions for action discovery.}
%
%We show in Appendix B that in 2p0s games, the exact tabular form of the above algorithm with mild assumptions and without the various approximations for deep RL provably converges to a NE.
%
%The biggest differences between our algorithm and AlphaZero are that 1-ply RM acts as the search algorithm instead of Monte Carlo tree search (just as in [11]), the value update is uses a 1-step bootstrap instead of the end-of-game-value (which allows the trajectory generation to be off-policy if desired), and the policy network acts as an action proposer for the search but does not regularize the search thereafter. Our algorithm is also similar to a recent method to handle large action spaces for Q-learning [31] and reduces to it in single-agent settings.
%
%\hrule 
%
%\ak{Value of state to value of agreement.}



\subsection{Perceived Value of Agreement to Opponent}

While \refEq{eq:value-of-agreement} determines the value of an agreement for $P_i$, it does not allow $P_i$ to estimate its value for $P_j$ due to incomplete information about $P_j$'s negotiations with others. 
Instead, $P_i$ must infer $P_j$'s intent from their mutual dialogue and by estimating the value of various actions in the current state for $P_j$.


%While \refEq{eq:value-of-agreement} provides a method to determine value of an agreement for a player $P_i$, it does not enable $P_i$ to estimate the value of that agreement for the other player $P_j$. 
%This is because $P_i$ has incomplete information about the negotiations $P_j$ has had with players apart from $P_i$.
%Therefore, $P_i$ must estimate the value of agreement for $P_j$ by determining its intent implicit within the dialogue between $P_i$ and $P_j$.

We interpret a players' intent as a probability distribution over the actions they assign to their units in the next round. 
To estimate the likelihood that $P_j$ will honor an agreement from $P_i$'s perspective, we approximate the intent distribution discussed in \refSec{sec:agreement}. 
This enables us to extract action probabilities from the dialogue to inform the agreement value computation.

%By interpreting the intention of a player to be actions as the probability distribution over the actions that player assigns to each of its units in the next round, we first learn this probability distribution. 
%Using this distribution, we then introduce a method for $P_1$ to determine the likelihood that $P_2$ will honor the agreement. We use the intent models discussed in Section~\ref{sec:agreement} to compute a distribution over all actions for a unit given the dialogue. Using this model, we are able to extract action probabilities from the dialogue, which we use to inform our agreement value computation.

% \textbf{Learning action probability from dialogue.} 
% To obtain a dialogue conditional
% action prediction model, we used the WEBDIPLOMACY dataset to train Transformer-based
% seq2seq models to predict end-of-turn action sequences for a given player[cite].
% % conditional on dialogue history, draw, state, order history, current game state, game metadata, and player rating [cite]. 
% This yields a probability distribution over actions of $P_j$, $\prob(a \in A_j \mid \vec{s}_t, \vec{d}_t, \vec{a}_{t-1})$, where $\vec{s}_t$ is the state history, $\vec{d}_t$ is the dialogue history, and $\vec{a}_{t-1}$ is the action history until round $t$.
% The model used the same dictionary as GPT2
% [cite], with the addition of 109 special tokens, representing power names, location names, and
% other commonly used input text.
% To predict potential actions that $P_j$ may use, we restricted the dialogue history seen by the model to only messages sent between $P_i$ and $P_j$. The purpose of this restriction was to signal to the action-prediction model that $P_j$ is only coordinating with $P_i$. % We similarly injected “agreement messages” to and from each player.. 

%\textbf{Likelihood of honoring agreement.} 
Given the distribution in \refEq{eq:joint-action-distribution}, we compute the likelihood of $P_j$ respecting $\alpha$ using the following equation. 
We denote the support of a probability distribution $\mathbf{d}$ by $\supp(\mathbf{d})$. 
Given a joint action $a \in A$, let  $\indicator_j(a, \alpha) \mapsto \{\top, \bot\}$ denote whether the action $a$ assigns the same action with $P_j$'s unit as that assigned under $\alpha$. 
\begin{align*}
%\resizebox{0.48\textwidth}{!}{
	V_j^i(\alpha) = \sum\limits_{a \in \supp(\mathbf{d})} \frac{\beta \prob(a \mid \vec{s}_t, \vec{d}_t, \vec{a}_{t-1})}{\beta \prob(a \mid \vec{s}_t, \vec{d}_t, \vec{a}_{t-1}) + (1 - \beta) \prob(a' \mid \vec{s}_t, \vec{d}_t, \vec{a}_{t-1})},
%}
\end{align*}
where $\beta = \indicator_j(a, \alpha)$ and $a' \neq a$ is a valid action assigned to unit $u_2$ by $P_j$.

Intuitively, $V_j^i(\alpha)$ measures the relative value that $P_j$ achieves by selecting an action that honors $\alpha$ when compared with  selecting an action that does not honor $\alpha$, as players are more likely to honor agreements that are more strategically advantangeous. 


%%
%%is not suitable to determine  value of agreement 
%%
%%
%%
%%
%%Knowing the game-theoretic value of an agreement for a player is not sufficient in a game like Diplomacy. 
%%A player must also anticipate whether the second player in agreement has a strong incentive to honor this agreement. 
%%However, the approach from previous subsection to determine value of agreement for another player is not feasible because of incomplete information. 
%%Hence, a player must base its understanding of value of agreement for other player on the bilateral communication it has had with the other player. 
%%Here, we describe a method to recognize the implicit intent and predict the behavior of other player based on dialogue. 
%
%
%We employ Cicero's intent recognition module for this purpose. The module is trained to predict ??? 
%However, we must predict the likelihood of the player honoring a given agreement. 
%
%We first present brief overview of intent recognition module and then show how it can be used to estimate the value of that agreement for the other player from ego player's perspective. 
%
%\textbf{Intent Recognition Module.} 
%We trained a conditional dialogue model, i.e., we learned the distribution p(x|y, z), where z
%is some desired controllable attribute which can be any function of (x, y). In this way, Equation D.1.1 becomes:
%
%\ak{Add equation.}
%Supplied the anchor policy by which CICERO’s strategic reasoning stays compatible with
%human conventions, allows CICERO’s actions to be highly flexible and adaptive to the dialogue it receives from other players. Trained on human diplomacy games to imitate/predict the joint actions of all 7 players, given the board state and history and the dialogue
%history. At test time, used as the anchor policy for piKL, as well as to initially sample actions to be considered by piKL.
%
%To obtain a dialogue-conditional
%action prediction model, we used the WEBDIPLOMACY dataset to train Transformer-based
%seq2seq models to predict end-of-turn action sequences for a given player conditional on dialogue history, draw, state, order history, current game state, game metadata, and player rating
%(see §D.1.1 for a description of these text features)
%
%The model used the same dictionary as GPT2
%(78), with the addition of 109 special tokens, representing power names, location names, and
%other commonly used input text. Th
%
%
%To predict action sequences for B (the message recipient), we restricted the dialogue
%history seen by the model to only messages sent between A and B up to and including
%timestamp t. The purpose of this restriction was to signal to the action-prediction model
%that B is only coordinating with A. We similarly injected “agreement messages” to and
%from each player.. 
%
%
%\textbf{Perceived Value of Agreement.} \ak{TODO}.




%\subsection{Rationalizability}
%
%\aknote{
%Background on hypergame theory and subjective rationalizability. 
%
%Q. Can we update rationalizability score to solve subset selection problem? 
%
%Subset selection problem. 
%
%How do we solve it? Formula for computing rationalizability score. 
%}
%
%
%Hypergame theory deals with misperceptions of agents (deci-
%sion makers) in games by relaxing common knowledge often
%assumed in the standard game theory [1, 2]. It is the basic
%idea of hypergames that each agent is supposed to possess
%independently a subjective view about a game called her
%subjective game and make a decision based on it. 
%
%In game theory, Bayesian games are often referred to
%as the standard model to deal with incomplete information
%[3]. Whileahypergamecan technicallybereformulatedasa
%Bayesian game under specific conditions, the reformulation
%requires the agents to be aware of every possibility indeed
%relevant to the situation [4]. Therefore hypergames are unique
%in that they can directly deal with unawareness of agents.
%
%In order to predict an agent’s choice in one-shot hyper-
%game, that is, a hypergame played only once, several solution
%concepts have been proposed [8]. They are typically based
%on the following idea. First, an analyzer fixes the level of
%hierarchy of perceptions and finds out an “equilibrium” (e.g.,
%Nash equilibrium) in the subjective games of the lowest  level
%of the hierarchy. Then it is supposed that best responses are
%taken sequentially at each level. For example, consider a two-
%level hypergame played by two agents, i and j. According to
%the idea, in order to analyze agent i’s choice, we first need to
%know an equilibrium in the subjective game of viewpoint ji.
%If agent j’s some action constitutes an equilibrium there, then,
%expecting agent j would take the action, agent i chooses a best
%response to it in viewpoint i’s subjective game
%
%
%The notion of rationalizability to hypergames so as to examine the precise prediction of an agent’s choice in a hypergame.
%Subjective rationalizability
%is defined not for agents but for viewpoints, so, for instance,
%agent i can think of agent j’s choice as viewpoint ji’s
%subjectively rationalizable action.
%\ak{We restrict game to level-2 hypergame in this paper assuming that a player bases its decisions on only P2's perception constructed from ... Our choice of hypergame theory enables, in future, to relax this assumption and incorporate what P1 thinks about potential agreements between P2 and P3 when evaluating value of an agreement between P1 and P2 for the latter.}
%We, however, prove that, under a condition called inside common
%knowledge [6], subjective rationalizability is equivalent to
%rationalizability and show that an agent’s subjectively ration-
%alizable action can be easily derived by applying the result.
%
%
%The concept of subjective
%rationalizability can be understood based on the following
%principle. The lowest agent in a viewpoint would take a
%best response to actions which she thinks the other agents
%would choose. When expecting the choices of the others, she
%considers that each of the other agents takes a best response
%to actions which she thinks the agent thinks the other agents
%would choose, and her inference goes on for further lower
%viewpoints. When agent i makes a decision in this way, her
%choice can be predicted as a subjectively rationalizable action
%of viewpoint i.Thus we may also say it is agent i’s subjectively
%rationalizable action.
%
%The proposition describes the sufficient condition of the
%existence of subjectively rationalizable action for a particular
%viewpoint in a hypergame. \ak{Argue that subjectively rationalizable actions always exists in Diplomacy.}
%
%\ak{Use the inference ability to Bayesian inference.}
%
%\textbf{Evaluating Rationalizability over Subsets of Agreements.} Given potential agreements, how to identify the value of an agreement while considering all subsets of potential agreements?  
%
%1. Use DNVI to determine conditional probability given the agreement, allowing player to choose freely among other actions. 
%
%2. Use DNVI to determine conditional probability given not agreement. 
%
%3. By comparing the two determine relative value of agreements. 