\section{Proposed Approach} \label{sec:method}
In this section, we first briefly review the state-of-the-art LLMs and key prompt engineering techniques, establishing the foundations of our work. We then introduce our LLM-driven detection framework, \tool, in detail.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{figures/overview.png}
  \caption{Overview of LLM-driven Automated Detection Framework.}
  \Description{The framework includes a knowledge synthesizer, a prompt generator and an auditor.}
  \label{fig:overview}
\end{figure}


\subsection{LLMs and Prompt Design Methods} \label{sec:promptmethod}
The rapid advancements in large language models (LLMs) have been driven by improvements in machine learning algorithms, computational power, and extensive training datasets.
State-of-the-art models like ChatGPT series\footnote{https://openai.com/chatgpt/overview}, Claude series\footnote{https://www.anthropic.com/claude}, and open-source models like Llama series\footnote{https://www.llama.com/}, Qwen series\footnote{https://www.alibabacloud.com/en/solutions/generative-ai/qwen}have significantly advanced natural language processing (NLP), excelling in tasks such as text generation, summarization, question answering, and program bug detection~\cite{li2023adapting}.
While LLMs have demonstrated remarkable performance across various tasks, their ability  in reasoning and addressing complex problems remains highly dependent on the quality of the prompts provided~\cite{wei2022chain}. 
To maximize their potential, innovative prompt engineering techniques have been developed, such as Chain-of-Thought (CoT)\cite{wei2022chain}, Least-to-Most\cite{zhou2022least}, and Complex CoT~\cite{fu2022complexity}, which guide models to decompose complex tasks into smaller, more structured steps. 
Remarkably, even simple zero-shot CoT prompts like "let's think step by step" have demonstrated improvements of up to 60\% on specific datasets~\cite{kojima2022large}.

Despite these advancements, designing efficient and effective prompts remains a challenge, particularly for complex tasks like POM detection. Building upon these techniques, our work integrates automated prompt design into a systematic, multi-LLM framework. By synthesizing domain-specific knowledge and dynamically generating tailored prompts, we enable LLMs to address the intricate challenge of detecting price oracle manipulation vulnerabilities in a scalable and automated manner. This eliminates reliance on manual intervention, achieving both precision and efficiency.
% Additionally, efforts are being made to design automatic prompt-generation systems, reducing the need for manual efforts. Agent-Pro\cite{zhang2024agent} introduces an LLM-based agent that uses policy-level reflection and optimization to learn from interactive experiences, gradually refining its behavior. Similarly, rStar~\cite{qi2024mutual} employs a self-play mutual reasoning framework, leveraging Monte Carlo Tree Search to create reasoning trajectories and a secondary model to validate them, thereby improving reasoning accuracy through mutual agreement.

% In this paper, we leverage the power of prompt design to utilize LLMs for detecting price oracle manipulation vulnerabilities. By automating prompt generation through LLMs themselves, we aim to enhance their language understanding and analytical capabilities, achieving more accurate and efficient detection of price oracle manipulations.

\subsection{\tool: LLM-driven Automated Detection Framework}
In this section, we introduce the three core components of \tool. As illustrated in Fig.~\ref{fig:overview}, our framework is composed of the \textbf{Domain Knowledge Synthesizer}, \textbf{Prompt Generator}, and \textbf{Auditor}, each playing a crucial role in automating and optimizing the detection process.



\subsubsection{Domain Knowledge Synthesizer}
This module synthesizes precise insights about price oracle vulnerabilities by leveraging top-tier academic literature, minimizing reliance on developer or auditor expertise through the automated summarization of key definitions and patterns.
Furthermore, it ensures the use of high-quality domain-specific information, mitigating the influence of web-scale noisy data used in the pre-training phase of the model.

\begin{tcolorbox}[float, colback=gray!20, colframe=gray!50, title={Knowledge Synthesizing Example}, label=knowledgesynthesizer]
\textbf{System:} You are a blockchain researcher. You are good at creating precise, comprehensive problem definitions. \\
\\
\textbf{User:} You are given descriptions of the price oracle manipulation problem from top academic conference papers, delimited by triple backticks \texttt{```}. Your task is to analyze these descriptions and provide a concise, unified definition of the price oracle manipulation problem. \texttt{```} + \{knowledge\} + \texttt{```}. Return the definition in the JSON format: \{output format\}. \\
\\
\textbf{Assistant:} The price oracle manipulation problem refers to a vulnerability in decentralized finance (DeFi) applications where an attacker can manipulate the price of a cryptocurrency in a decentralized exchange (DEX) that serves as the price oracle for the victim smart contract. The attack consists of two steps: 1. Price Oracle Manipulation (POM): The attacker trades a large volume of the cryptocurrency in the DEX, tilting the balance of the trading pair and causing the price to deviate from the actual market price. 2. Arbitrage: The attacker then exploits the price disparity between the manipulated price and the actual market price to obtain the cryptocurrency at the lower, manipulated price and sell it at the higher, actual market price, profiting from the difference. This vulnerability arises when the victim smart contract relies on the manipulated price provided by the DEX price oracle, rather than using more robust price oracles or mechanisms to mitigate price manipulation.
\end{tcolorbox}

To achieve this, we selected papers relevant to the topic of price oracle problems from prestigious conferences, such as the IEEE Symposium on Security and Privacy (S\&P), International Conference on Software Engineering (ICSE), IEEE European Symposium on Security and Privacy Workshops (EuroS\&PW), and ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA).
From these sources, we identified seven papers~\cite{xi2024pomabuster,deng2024safeguarding,zhang2023demystifying,wu2021defiranger,wang2021promutator,kong2023defitainter, mo2023toward} that are particularly relevant to price oracle manipulation.
We manually excerpted paragraphs within these papers that describe price oracle problems, including definitions, common attack vectors, techniques, and defense mechanisms. 
We believe this manual step, while necessary, presents minimal challenges. This is because the published academic papers typically employ clear subtitles to delineate problem descriptions, making the identification and extraction of relevant information straightforward.

Subsequently, we prompt the LLM model to summarize the paragraphs on price oracle manipulation. This synthesized definition serves as the sole knowledge for the prompt generator, ensuring that the generated prompts are grounded in accurate and comprehensive domain knowledge.
An example of the knowledge synthesis process, including the prompt and response, is provided in \textit{Knowledge Synthesizing Example}~\ref{knowledgesynthesizer}.

This module is pivotal for extending our framework to address other issues. By extracting high-quality, relevant descriptions of specific problems, the framework can be easily adapted to tackle vulnerabilities such as \textit{Privilege Escalation} and \textit{Inconsistent State Updates}~\cite{zhang2023demystifying}.


\subsubsection{Prompt Generator and Auditor}

% \begin{table}[]
% \caption{Prompt Types and Examples of Prompt Templates}
% \label{table:promptTypes}
% \begin{tabularx}{\textwidth}{p{3cm} X}
% \toprule
% Prompt Type          & Example                                               \\ \toprule
% Zero-shot CoT Prompt        &  Generate a chain-of-thought prompt \textbf{step by step} to an AI language model to accurately detect price oracle manipulation issues in the given smart contract. + \{output format\}   \\ \midrule
% Role-based Prompt    & \textbf{You are an experienced prompt engineer}, + \{task\}.       \\ \midrule
% Positive Prompt      & You are an experienced prompt..., + \{task\}, + \textbf{remember to base your generated prompt solely on the price oracle description above}.              \\ \midrule
% Negative Prompt      & You are an experienced prompt ..., + \{task\}, + remember to ..., \textbf{do not make assumptions about external factors...}.  \\ \midrule
% Context-based Prompt & \{task relevant context\}, + you are an..., + \{task\}, + remember to ..., + do not make .... \\ \bottomrule
% \end{tabularx}
% \end{table}


\begin{tcolorbox}[float, colback=gray!20, colframe=gray!50, title={CoT Prompt Generating Example}, label=cotprompt]
\textbf{System:} \textcolor{orange}{You are an experienced prompt engineer.} \\
\\
\textbf{User:} Your task is to provide a chain-of-thought prompt to an AI language model to accurately detect price oracle manipulation issues which could lead to financial losses or unfair advantages in the given smart contract \textcolor{purple}{step by step.} \textcolor{blue}{Remember to base your generated prompt solely on the price oracle description in triple backticks \texttt{```}}. \textcolor{red}{Do not make assumptions about external factors or systems not mentioned in the given information.} \texttt{```} + \textcolor{brown}{\{synthesized knowledge\}} + \texttt{```}. Return the prompt in the JSON format: \{output format\}. \\
\\
\textbf{Assistant:} description: Detect price oracle manipulation vulnerabilities in a smart contract,  steps:
step: 1, description: Identify the price oracle used by the smart contract. Determine if the price oracle is a decentralized exchange (DEX) or another external price feed.,     \\ 
questions: \\
\hspace*{1cm} a. What is the price oracle used by the smart contract?       \\
\hspace*{1cm} b. Is the price oracle a decentralized exchange (DEX) or another external price feed? \\
    step: 2, description: Analyze the smart contract's reliance on ...
\end{tcolorbox}


The synthesized domain knowledge is integrated into the \textbf{Prompt Generator}, a component that combines extracted insights with tailored instructions.
We employ several techniques to enhance the prompt generator's ability to produce structured and actionable CoT prompts.
As demonstrated in \textit{Prompt Generating Example}\ref{cotprompt}, the \textbf{System} and \textbf{User} represent the input provided by \tool, while the \textbf{Assistant} showcases the model’s output.
In this example, the sentence highlighted in orange represents a role-based prompt, which assigns a specific relevant role to guide the prompt. 
This technique has proven effective in various tasks, with Zhang et al.~\cite{zheng2023helpful} reporting approximately a 20\% improvement in accuracy compared to the \textit{Simple Prompt} approach.
Building on this, we introduced the \textit{Zero-shot CoT Prompt} approach, described in Section~\ref{sec:promptmethod}.
This method utilizes the "magical" phrase \textbf{step by step}, highlighted in brown, to encourage the model to perform logical reasoning.
Another technique employed is the use of positive and negative prompts, inspired by conditional generation models like Stable Diffusion~\cite{ban2025understanding}. 
Positive prompts, highlighted in blue, provide explicit guidance, while negative prompts, shown in red, define constraints to avoid irrelevant or misleading outputs.
These prompts are important because LLM models are pre-trained on general web-scale data, this pre-trained knowledge may conflict with the actual context of POM and thus interfere the analysis of the underlying task. For instance, such knowledge often leads to extraneous alarms, like presuming that an oracle owner’s potential to modify the oracle inherently makes the "set oracle" function vulnerable. 
While these findings may hold in broader contexts, they are not classified as POM attacks from a developer’s perspective and therefore increase false alarm (as revealed in the later section).
By adhering to the constraints defined by positive and negative prompts, the generated Cot prompt remain highly focused and relevant to the specific vulnerabilities under investigation.
Next, we incorporate synthesized domain knowledge, highlighted in brown, from the \textit{Knowledge Synthesizer} into the prompts, creating what we call \textit{Context-based Prompts}. 
With this enriched knowledge, the prompt generator can deliver task-aware and accurate instructions, ensuring that the prompts are highly aligned with the nuances of the vulnerabilities being analyzed. 
The complete prompt generated by this process is provided in Appendix~\ref{app:componentprompts}.

% As shown in Table~\ref{table:promptTypes}, the \textit{Zero-shot CoT Prompt} approach involves directly posing the query with the "magical" phrase \textbf{step by step} to form the prompt as shown in the table. 
% Building on this, we introduce role-based prompts, assigning roles like "experienced prompt engineer" or "experienced smart contract auditor" to the LLM. 
% This technique has demonstrated efficacy in various tasks, with Zhang et al.~\cite{zheng2023helpful} reporting approximately a 20\% improvement in accuracy compared to the \textit{Simple Prompt} approach.
% Another technique employed is the use of positive and negative prompts, inspired by conditional generation models like Stable Diffusion~\cite{ban2025understanding}. 
% Positive prompts provide explicit guidance, such as “Based solely on the knowledge provided ...” while negative prompts set clear constraints, like “Do not make assumptions about external factors ...”.
% It is important to note that while LLM models are pre-trained on general web-scale data, this pre-trained knowledge may conflict with the actual context of POM and thus interfere the analysis of the underlying task. For instance, such knowledge often leads to extraneous alarms, like presuming that an oracle owner’s potential to modify the oracle inherently makes the "set oracle" function vulnerable. 
% While these findings may hold in broader contexts, they are not classified as POM attacks from a developer’s perspective and therefore increase false alarm (as revealed in the later section).
% By adhering to the constraints defined by positive and negative prompts, the generated Cot prompt remain highly focused and relevant to the specific vulnerabilities under investigation.
% Next, we incorporate synthesized domain knowledge from the \textit{Knowledge Synthesizer} into the prompts, creating what we call \textit{Context-based Prompts}. 
% With this enriched knowledge, the prompt generator can deliver task-aware and accurate instructions, ensuring that the prompts are highly aligned with the nuances of the vulnerabilities being analyzed. 
% An example output of the Prompt Generator is shown in \textit{Prompt Generating Example}, the complete prompt generated is shown in Appendix~\ref{componentprompts}.



Lastly, the generated CoT prompt is utilized by the \textbf{Auditor} component, which employs an LLM model to detect POM vulnerabilities in the provided smart contract.
The input to the Auditor is shown in \textit{Auditor Prompt Example} below.

\begin{tcolorbox}[colback=gray!20, colframe=gray!50, title={Auditor Prompt Example}, label=prompt_auditor]
\textbf{System:} You are an experienced expert on auditing price oracle manipulation problems. Your task is to conduct a thorough audit on the provided solidity file to identify all potential price oracle manipulation vulnerabilities. \\

\textbf{User:} \{generated CoT prompt\} + \{smart contract\} + Respond the prompt in the following JSON format: \{output format\}.
\end{tcolorbox}

In addition to the techniques outlined above, we emphasize the importance of fostering internal reasoning before generating responses, aligning with OpenAI's \textbf{o1} reasoning model.\footnote{https://platform.openai.com/docs/guides/reasoning}
To minimize manual effort across components, we incorporate this concept into the output format.
As demonstrated in the \textit{Output Format} example, the model is instructed to include key fields such as \textbf{beneficiary}, \textbf{victim}, and \textbf{reason} for each finding. These fields encourage the model to engage in critical reasoning, ensuring that relevant actors are accurately identified and justifications are provided for each result.
This structured approach not only enhances the quality and interpretability of the output but also mitigates hallucination issues, reduces false alarms, and improves detection accuracy while minimizing noise in the analysis.

\begin{tcolorbox}[colback=gray!20, colframe=gray!50, title={Output Format Example}, label=outputformat]
    Analyze the smart contract delimited with \texttt{```}. Respond all the vulnerabilities with the following JSON format: \\
    \{vulnerable: yes, function: functionName, contract: contract name of the vulnerable function, beneficiary: ..., victim: ..., reason:...\} \\

    - vulnerable should be yes if the vulnerability exists, otherwise no. \\
    - \textbf{beneficiary} should be the role who will gain in the vulnerability. \\
    - \textbf{victim} should be the role who will suffer a loss or disadvantage in the vulnerability. \\
    - \textbf{reason} should describe why you think it is vulnerable and how to manipulate the price oracle to exploit this vulnerability.
\end{tcolorbox}






