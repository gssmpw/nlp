\section{Evaluation}\label{sec:evaluation}
In this section, we present the dataset utilized for our evaluation and discuss the findings derived from addressing the following research questions (RQs):

\begin{enumerate}[label=RQ\arabic*:, left=1em] 
\item How effective are the state-of-the-art (SOTA) tool GPTScan and a zero-shot CoT prompt approach?
\item How effective is \tool~when evaluated on diverse real-world projects with human-curated knowledge?
\item How does the Knowledge Synthesizer impact the framework's performance in comparison to human-curated knowledge?
% \item How well does \tool, using the best configuration, perform in detecting POMs in emerging audit projects on Code4Rena platform?
\end{enumerate}


\subsection{Dataset} \label{dataset}
Our dataset is curated to reflect real-world scenarios and challenges, providing a robust benchmark for evaluating the framework. As shown in Table~\ref{table:dataset}, which summarizes the key statistics for the datasets, including the number of projects, the number of vulnerabilities for each dataset, and the average number of functions and lines of code (LoC) for each project, it generally consists of two categories: real-world attacked DeFiHacks projects and Code4Rena audit contest projects. To focus specifically on price oracle manipulation, we extended both datasets, ensuring comprehensive and reliable evaluation. 
By leveraging projects with documented exploitation reports and rigorous audit reports, we implemented a stringent method to categorize the vulnerabilities, thereby minimizing potential reporting biases and enhancing the empirical reliability of our evaluation framework. 

\begin{table}
    \caption{Datasets for Evaluation}
    \label{table:dataset}
    \begin{tabular}{@{}lcccc@{}}
    \toprule
    Dataset  & Projects & Vulnerabilities & Functions (Avg) & LoC (Avg)\\ \midrule
    DeFiHack  & 31       & 36      & 24  &  1,630 \\
    Code4Rena  & 14       & 24      & 210 & 11,926   \\ \midrule
    Total & 45       & 60      & null  & null   \\ \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Real-World Attacked Projects}
The dataset includes 31 projects that have experienced real-world attacks, sourced from two reliable datasets:
\begin{itemize}
\item \textbf{11 Projects from GPTScan's DeFiHack Dataset~\cite{sun2023gptscan}:} This dataset, designed by GPTScan, originally contained 13 projects. We excluded two projects that were unrelated to oracle manipulation to ensure relevance.

\item \textbf{20 Projects from SOK~\cite{zhou2023sok}:} These projects, categorized in the original SOK paper as on-chain oracle manipulation, liquidity borrowing and depositing issues, and slippage exploitation observed between 2021 and 2022, are included to increase the dataset size.
\end{itemize}

\subsubsection{Code4Rena Projects}
Code4rena~\cite{code4rena} is a leading audit contest platform for pre-deployment projects. The platform engages project developers to commit bounties up to \$1M as incentives to draw participants from all over the world. 
Community experts selected and developers collaboratively review the submitted bug reports and reward the participants based on the severity and frequency of a particular bug submission. This incentive-driven process guarantees the integrity and credibility of the bugs reported, forming the ground truth for our study.

The dataset incorporates 14 projects from the Code4Rena platform:
\begin{itemize}
\item \textbf{6 Projects from Zhang et al.~\cite{zhang2023demystifying}:} These are derived from Zhang et al.'s original set of 11 price oracle manipulation projects on Code4Rena. To ensure fair comparison with GPTScan, we excluded two misclassified projects and three incomplete ones.
\item \textbf{8 Projects Newly Curated:} To enhance the robustness of the evaluation, we extended the dataset by including additional projects from the Code4Rena platform\footnote{\url{https://github.com/code-423n4/code423n4.com/tree/main/\_data/reports}} directly. 
We filtered projects from 2022 and 2023 using keywords such as \textit{frontrunning, slippage, flash loan, sandwich, price oracle}, and \textit{manipulation} in the audit reports. 
Each selected bug was manually checked to ensure it was a POM bug, and all projects relevant were confirmed to compile successfully. 
As a result, 8 projects were added to the dataset.
\end{itemize}
By focusing solely on price oracle manipulation, we enhanced both real-world attacked and Code4Rena datasets to ensure reliable evaluation. 
% While GPTScan uses a larger dataset covering various vulnerability types, our work narrows its scope to this specific vulnerability, resulting in a highly specialized and expanded dataset essential for precise analysis.




\subsection{Baseline} \label{baseline}
Since very few works have addressed the problem of price oracle manipulation, most existing analyses are post-mortem and rely on transaction data, which differs from our approach. Our goal is to prevent attacks before they occur.
To evaluate our framework, we select one state-of-the-art (SOTA) LLM-based tool, GPTScan, as our primary baseline. GPTScan utilizes an LLM (initially ChatGPT-3.5) to analyze pre-tagged functions potentially susceptible to specific vulnerability types through predefined scenarios and rules. The tool then applies static analysis to validate the LLM-generated findings and filter out false positives, thereby enhancing detection precision.
Given the deprecation of ChatGPT-3.5, we substituted it with ChatGPT-4o-mini, which OpenAI recommends as a more cost-effective and improved alternative.\footnote{https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/}
Additionally, the original GPTScan implementation suffered from unstructured LLM output, which hindered systematic analysis by the static analysis module. To address this limitation, we modified the tool's code to enforce structured JSON output supported by ChatGPT-4o-mini. These modifications ensure more consistent and reliable vulnerability assessments, improving the overall performance of the analysis pipeline.

To complement GPTScan, we design a zero-shot CoT prompt based on the assumption that a common LLM user, equipped with basic knowledge of prompt engineering and price oracle issues, could generate.
This baseline zero-shot CoT prompt generally follows the zero-shot prompting guideline outlined in the \textit{Prompt Engineering Guide}. \footnote{https://www.promptingguide.ai/}
We adopt the role where the model acts as \textit{an experienced expert on auditing price oracle manipulation problems}, with the task of identifying all potential vulnerabilities in a provided Solidity file.
The final prompt, structured for clarity and consistency, is shown below:

\begin{tcolorbox}[colback=gray!20, colframe=gray!50, title={Zero-shot CoT Prompt}, label=simplebaseline]
\textbf{System:} You are an experienced expert on auditing price oracle manipulation problems. Your task is to conduct a thorough audit on the provided solidity file step by step to identify all potential price oracle manipulation vulnerabilities. \\
\\
\textbf{User:} Analyze the smart contract delimited with \texttt{```}. Respond with all the vulnerabilities with the following JSON format: \{output format\}.
\end{tcolorbox}


\subsection{Model Choices and Hyperparameters} \label{subsec:settings}

\begin{table}
    \caption{Model Descriptions}
    \label{table:models}
    \begin{tabular}{@{}lcccc@{}}
    \toprule
    Models & Versions         & Knowledge Cutoff & Context Window & Max Tokens \\ \midrule
    ChatGPT-4o & 2024-05-13 & Oct 2023         & 128k         & 4,096       \\
    Claude3.5-Sonnet & 2024-06-20 & July 2024        & 200k         & 8,192         \\
    ChatGPT-4o-mini  &  2024-07-18    &  Oct 2023         & 128k       & 16,384      \\ 
    Claude3-haiku  &  2024-07-18    & Aug 2023        & 200k       & 	4,096       \\ \bottomrule
    \end{tabular}%
\end{table}

We utilized four language models, as described in Table~\ref{table:models}. These models were selected to represent the latest advancements in industry that are both accessible and affordable for everyday users.
The \textit{Knowledge Cutoff} column is the cutoff date of the training data, indicating the recency of the knowledge embedded in the model.
The \textit{Context Window} is the maximum combined length of input tokens and output tokens that the model can process in a single query.
The \textit{max\_tokens} is the upper limit of tokens that the model can generate as output. 
Although these models allow for higher token generation, the maximum output token limit was set to $1,024$ in our experiments to prioritize longer input.

\begin{table}[]
\caption{Model Parameters}
\label{table:modelparams}
\begin{tabular}{@{}lccc@{}}
\toprule
Parameters  & Knowledge Synthesizer & Generator & Auditor \\ \midrule
temperature & 0                     & 0         & 1.0     \\
top\_p      & 1.0                   & 1.0       & 1.0     \\ \bottomrule
\end{tabular}
\end{table}

In this study, we evaluated the performance of the models by adjusting two key parameters: \textit{temperature} and \textit{top\_p}. The \textit{temperature} parameter controls the randomness of the output, with higher values promoting greater variability and lower values yielding more deterministic results. In contrast, \textit{top\_p} applies nucleus sampling, where only tokens contributing to the top \textit{p} probability mass are considered. For instance, a \textit{top\_p} value of 0.1 restricts the model to tokens comprising the top 10\% of cumulative probability. OpenAI generally advises modifying either \textit{top\_p} or \textit{temperature}, but not both simultaneously.

In this evaluation, we set the \textit{top\_p} parameter to its default value of 1.0, ensuring that the full probability mass was considered. The \textit{temperature} parameter was configured as follows: a value of 0 was applied for the \textit{Knowledge Synthesizer} and \textbf{Prompt Generator} to prioritize accuracy and consistency, while the \textbf{Auditor} was assigned a value of 1.0 to encourage diverse and comprehensive vulnerability identification, thereby fully leveraging the model’s capabilities. All other parameter settings adhered to their default values as outlined in the OpenAI API documentation~\cite{openaiparams}.

To mitigate the impact of randomness inherent in the models, each prompt was executed on the dataset three times. The models were instructed to identify vulnerable functions and provide detailed explanations of the vulnerabilities. The outputs were formatted in JSON to facilitate efficient post-processing.


\subsection{Identifying the Best Configuration for \tool} ~\label{subsec:baselines}
This section is to establish the most effective combination of components for the POM detection task.
We start by evaluating the performance of the \textit{Auditor} component in isolation. 
Subsequently, we enhance the setup by incorporating the \textit{Prompt Generator} with human-curated knowledge to improve performance.
Finally, we automate the knowledge extraction process by integrating the \textit{Knowledge Synthesizer} for better usability.

\subsubsection{Identifying the Best Auditor Model}

We assessed the zero-shot CoT prompt (described in Section~\ref{simplebaseline}) on the DeFiHacks dataset to identify the most suitable model as the auditor for detecting POM issues. For comparison, we also evaluated GPTScan on the same dataset.
The DeFiHacks dataset was chosen because it allows for more efficient manual verification of the outputs, facilitating the identification of optimal parameter combinations. 

The results are summarized in Table~\ref{table:baselines}.  The metrics used to assess performance include \textbf{False Negatives (FN)}, which stands for the number of vulnerabilities incorrectly classified as safe; \textbf{True Positives (TP)}, the number of the vulnerabilities correctly identified and \textbf{False Positives (FP)}, the number of instances incorrectly flagged as vulnerable but are actually safe.
We evaluated performance using precision, recall, and F1 score as key indicators. \textbf{Precision} measures the accuracy of positive predictions, while \textbf{Recall} assesses the model's ability to identify all relevant instances.
The \textbf{F1 score}, as the harmonic mean of precision and recall, offers a balanced metric, particularly valuable for imbalanced class distributions. For detailed formulas, refer to Appendix~\ref{app:formulas}.

% \textbf{Precision} stands for the ratio of \textbf{TP} to \textbf{(TP+FP)}, indicating the accuracy of the positive predictions. 
% \textbf{Recall} stands for the ratio of \textbf{TP} to \textbf{(TP+FN)}, measuring the ability to identify all relevant instances. 
% The \textbf{F1} score stands for the harmonic mean of precision and recall, providing a single metric that balances both concerns, especially useful when the class distribution is imbalanced.




\begin{table}
\caption{Average Performance of Baselines on DeFiHacks}
\label{table:baselines}
\begin{tabular}{@{}l|c|cccc@{}}
\toprule
          & GPTScan & \multicolumn{4}{c}{Zero-shot CoT}                                      \\
Models    & 4o-mini & 4o    & Sonnet & 4o-mini                                         & Haiku \\ \midrule
FN        & 26.67   & 25.67 & 29.67  & 14.33                                           & 26.33 \\
TP        & 9.33    & 10.33 & 6.33   & 21.67                                           & 9.67  \\
FP        & 20.67   & 19.67 & 23.00  & 58.33                                           & 19.33 \\ \midrule
Recall    & 0.259   & 0.287 & 0.176  & 0.602                                           & 0.269 \\
Precision & 0.311   & 0.344 & 0.216  & 0.271                                           & 0.333 \\
F1        & 0.283   & 0.313 & 0.194  & \color[HTML]{C834FC} 0.374 & 0.297 \\ \bottomrule
\end{tabular}
\end{table}

As shown in Table~\ref{table:baselines}, almost all the models achieved higher F1 Score performance compared to GPTScan. Among the evaluated models, 4o-mini from the zero-shot CoT approach demonstrated superior performance, achieving the highest F1 Score (0.374) on the DeFiHacks dataset, outperforming GPTScan (0.283) and other models, including 4o version. Similarly, Claude's Haiku outperformed its more advanced counterpart, Sonnet, in terms of F1 Score (0.297 vs. 0.194).
A detailed review of the results shows that more complex and advanced models tend to generate more conservative output, while this leads to lower false positive, it produces fewer overall predictions,  which is more likely to miss true vulnerabilities.

Notably, while GPTScan significantly reduced false positives, it also increased false negatives. This trade-off resulted in a marginally higher precision (0.311 vs. 0.271) but at the expense of a markedly lower recall (0.259 vs. 0.602).
% 4o-mini also achieved the highest recall score (0.602) among all models, significantly surpassing GPTScan's recall (0.259), which indicates improvements in the model's ability to detect vulnerabilities since GPTScan's development. However, its precision (0.271) remains relatively low, indicating room for further improvement in reducing false positives.


% \begin{tcolorbox}[colback=gray!20!white, colframe=gray!75!black, boxsep=5pt, arc=4pt, boxrule=1pt, left=0pt, right=0pt]
% \textbf{Finding 1:} 
% GPTScan lags behind the simple baselines in F1 Score, highlighting the rapid progress in model capabilities. 
% \end{tcolorbox}

\begin{tcolorbox}[colback=gray!20!white, colframe=gray!75!black, boxsep=5pt, arc=4pt, boxrule=1pt, left=0pt, right=0pt]
\textbf{Finding 1:}
4o-mini demonstrates potential as a better auditor. "Mini" versions like 4o-mini and Claude's Haiku outperformed their flagship counterparts, with 4o-mini achieving the highest F1 Score (0.374) among all models. 
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!20!white, colframe=gray!75!black, boxsep=5pt, arc=4pt, boxrule=1pt, left=0pt, right=0pt]
In response to \textbf{RQ1}: Despite advancements in model performance, both GPTScan and the zero-shot CoT approaches show limited effectiveness in reliably detecting price oracle problems, emphasizing the need for further refinement.
\end{tcolorbox}


To clarify the discrepancy between our evaluation and the report from GPTScan's paper, it is important to note that the metrics used in both evaluations differ. In GPTScan's report, they emphasize the identification of vulnerability types across projects.
For instance, in the project \textit{Hack-20210603-PancakeHunny}, 7 vulnerabilities of \textit{Flash Loan Price (FLP)} were found in their report, but only 1 true positive (TP) was counted in their precision and recall calculations. This difference in how vulnerabilities are calculated and counted contributes to the significant variation in the results presented here.


\subsubsection{Identifying the Best Prompt Generator with Manually Curated Knowledge}

Building on the results of the previous section, where "4o-mini" was identified as the best auditor model using the zero-shot CoT prompt, we now explore varying models for the prompt generator to determine the optimal combination for detecting POM vulnerabilities. This section also tries to validate the performance of the best auditor model identified again.

To balance computational efficiency with reliability, we adopted a strategic optimization approach.
Specifically, we stabilized the domain knowledge component by integrating human-curated expertise into the prompt generator. This refinement allowed us to constrain the experimental space and focus on identifying optimal combinations of models for vulnerability detection.
With the optimal model combinations identified, we can systematically reduce manual intervention with the knowledge synthesizer and progressively automate our vulnerability detection framework, ultimately advancing towards a more autonomous and robust oracle manipulation detection system.
The curated domain knowledge that underpins this optimization is presented in Appendix~\ref{app:humanknowledge}.



\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{figures/varyingModels.png}
  \caption{F1 Score of Varying Prompt Generator and Auditor with Human-curated Knowledge.}
  \Description{Performance of varying models for prompt generator and auditor.}
  \label{fig:varymodel}
\end{figure}

The results are presented in Figure~\ref{fig:varymodel}, which illustrates a comparative analysis of different model configurations applied to the DeFiHacks dataset. The raw data for these results is provided in Table~\ref{table:aigenprompt}.
In this figure, the y-axis represents performance, measured by the F1-score. 
Each bar cluster in the x-axis denote the choice of auditor model (ranges in "4o", "Sonnet", "4o-mini" and "Haiku").
Within each cluster, different bars represent different choices of the paired generator models (ranges in "4o-generator", "Sonnet-generator", "4o-mini-generator" and "Haiku-generator") as well as the two baselines (GPTScan and zero-shot CoT baseline).
Note that, we stick to use 4o-mini for GPTScan in all the comparisons, and the results reported in Section~\ref{subsec:baselines} is reused for the zero-shot CoT.



\begin{table}
\caption{Average Performance of Varying Prompt Generator and Auditor with Human-curated Knowledge}
\label{table:aigenprompt}
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{@{}l|cccc|cccc|cccc|cccc@{}}
\toprule
Prompt Gen & 4o    & Sonnet & 4o-mini & Haiku & 4o     & Sonnet & 4o-mini & Haiku  & 4o      & Sonnet  & 4o-mini & Haiku   & 4o    & Sonnet & 4o-mini & Haiku \\ 
Auditor          & 4o    & 4o     & 4o      & 4o    & Sonnet & Sonnet & Sonnet  & Sonnet & 4o-mini & 4o-mini & 4o-mini & 4o-mini & Haiku & Haiku  & Haiku   & Haiku \\ \midrule
FN               & 23.33 & 23.00  & 25.33   & 24.67 & 30.33  & 28.67  & 30.67   & 27.67  & 18.00   & 12.33   & 12.67   & 13.20   & 22.33 & 26.67  & 24.00   & 25.67 \\
TP               & 12.67 & 13.00  & 10.67   & 11.33 & 5.67   & 7.33   & 5.33    & 8.33   & 18.00   & 23.67   & 23.33   & 22.80   & 13.67 & 9.33   & 12.00   & 10.33 \\
FP               & 16.67 & 15.67  & 15.33   & 11.67 & 23.33  & 22.00  & 23.33   & 21.67  & 51.00   & 67.00   & 60.00   & 48.80   & 15.33 & 20.00  & 16.67   & 18.67 \\ \midrule
Recall           & 0.352 & 0.361  & 0.296   & 0.315 & 0.157  & 0.204  & 0.148   & 0.231  & 0.500   & 0.657   & 0.648   & 0.633   & 0.380 & 0.259  & 0.333   & 0.287 \\
Precision        & 0.432 & 0.453  & 0.410   & 0.493 & 0.195  & 0.250  & 0.186   & 0.278  & 0.261   & 0.261   & 0.280   & 0.318   & 0.471 & 0.318  & 0.419   & 0.356 \\
F1               & 0.388 & 0.402  & 0.344   & 0.384 & 0.174  & 0.224  & 0.165   & 0.253  & 0.343   & 0.374   & 0.391   & {\color[HTML]{C834FC} 0.424}   & {\color[HTML]{C834FC} 0.421} & 0.286  & 0.371   & 0.318 \\ \bottomrule
\end{tabular}
\end{adjustbox}
\end{table}


As demonstrated in the Figure, the performance varies significantly across different combinations of prompt generators and auditor models.
Nevertheless, our framework consistently outperforms the zero-shot CoT across 12 out of 16 model configurations, underscoring the efficacy of leveraging human-curated knowledge and optimized, context-aware prompts. 
Among the results obtained and showed in Table~\ref{table:aigenprompt} and Figure~\ref{fig:varymodel}, two combinations stand out: 1). "Haiku" as the prompt generator paired with "4o-mini" as the auditor: This combination achieved the highest F1 score of 0.424, representing a 13.4\% improvement over the zero-shot CoT of 4o-mini. It also attained the highest recall (0.633), making it ideal for identifying a broader range of vulnerabilities.
2). "4o" as the prompt generator paired with "Haiku" as the auditor: This combination achieved an F1 score of 0.421, a 41.8\% improvement over the zero-shot CoT of Haiku. It exhibited higher precision, making it preferable for scenarios where minimizing false positives is critical.
These results highlight the synergistic benefits of pairing complementary model architectures, enabling tailored optimization for varying detection priorities.



\begin{tcolorbox}[colback=gray!20!white, colframe=gray!75!black, boxsep=5pt, arc=4pt, boxrule=1pt, left=0pt, right=0pt]
\textbf{Finding 2:} 
Our framework, enhanced with human-curated knowledge, improves upon the baseline prompt in most cases (12 out of 16). Notably, two combinations—‘Haiku-generator’ with the ‘4o-mini’ auditor and ‘4o-generator’ with the ‘Haiku’ auditor—achieved the highest F1-scores of 0.424 and 0.421, compared to the baseline's best of 0.374.
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!20!white, colframe=gray!75!black, boxsep=5pt, arc=4pt, boxrule=1pt, left=0pt, right=0pt]
In response to \textbf{RQ2}: \tool~identifies two combinations that outperform the best baseline performance with human-curated knowledge, demonstrating the efficacy of \tool.
\end{tcolorbox}



\subsubsection{Identifying the Best Knowledge Synthesizer for Automation}
With the best auditor and prompt generator combinations identified, the next step is to optimize the knowledge synthesizer for summarizing domain-specific information, ultimately aiming for a fully automated vulnerability detection framework. The knowledge synthesizer plays a critical role in utilizing domain knowledge from external domain-specific sources, and integrating them into the detection pipeline.
To systematically evaluate the knowledge synthesizer's effectiveness, we fixed the two top-performing combinations of Prompt Generator and Auditor from \textbf{Finding 2} (Haiku-generator with 4o-mini auditor, and 4o-generator with Haiku auditor) while varying the knowledge synthesizer models. Performance metrics included consistency, accuracy, and the overall F1-score.

As summarized in Table~\ref{table:aigenknowledge}, the combination of Haiku as the Knowledge Synthesizer, Prompt Generator, and Auditor (Haiku-Haiku-4o-mini) achieved the best performance, with an F1-score of 0.426. Remarkably, this result slightly surpasses that of the human-curated knowledge framework (0.426 vs. 0.424), demonstrating the potential of fully automated domain knowledge synthesis in enhancing vulnerability detection capabilities.
This improvement is particularly noteworthy as it highlights the ability of automated frameworks to match—and even exceed—manual approaches. Additionally, the Haiku-Haiku-4o-mini configuration achieved this result without requiring extensive manual intervention, marking a significant step toward a robust and autonomous system.



\begin{table}
\caption{Average Performance of Varying Knowledge Synthesizer}
\label{table:aigenknowledge}
\begin{tabular}{@{}l|cccc|cccc@{}}
\toprule
Knowledge Synthesizer & 4o      & Sonnet  & 4o-mini & Haiku   & 4o    & Sonnet & 4o-mini & Haiku \\ 
\midrule
Prompt Generator      & Haiku   & Haiku   & Haiku   & Haiku   & 4o    & 4o     & 4o      & 4o    \\
Auditor               & 4o-mini & 4o-mini & 4o-mini & 4o-mini & Haiku & Haiku  & Haiku   & Haiku \\
\midrule
FN                    & 11.00   & 10.00   & 14.00   & 12.00   & 26.00 & 23.00  & 26.00   & 25.00 \\
TP                    & 25.00   & 26.00   & 22.00   & 24.00   & 10.00 & 13.00  & 10.00   & 11.00 \\
FP                    & 86.00   & 74.67   & 56.00   & 52.67   & 18.00 & 17.00  & 20.00   & 19.00 \\ 
\midrule
Recall                & 0.694   & 0.722   & 0.611   & 0.667   & 0.278 & 0.361  & 0.278   & 0.306 \\
Precision             & 0.225   & 0.258   & 0.282   & 0.313   & 0.357 & 0.433  & 0.333   & 0.367 \\
F1                    & 0.340   & 0.380   & 0.386   & {\color[HTML]{C834FC} 0.426}   & 0.313 & 0.394  & 0.303   & 0.333 \\ \bottomrule
\end{tabular}
\end{table}

\begin{tcolorbox}[colback=gray!20!white, colframe=gray!75!black, boxsep=5pt, arc=4pt, boxrule=1pt, left=0pt, right=0pt] 
\textbf{Finding 3:}
The combination of Haiku-Haiku-4o-mini achieved the highest F1 score (0.426), slightly outperforming the human-curated knowledge framework (0.424). This result underscores the potential of fully automated domain knowledge synthesis for advancing vulnerability detection.
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!20!white, colframe=gray!75!black, boxsep=5pt, arc=4pt, boxrule=1pt, left=0pt, right=0pt] 
\textbf{Finding 4:}
The best performance is achieved using a combination of less complex models (Haiku-Haiku-4o-mini), demonstrating that larger models do not necessarily lead to better results. 
This finding highlights the potential for accessibility and suggests the feasibility of using alternative models similar in size and complexity.
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!20!white, colframe=gray!75!black, boxsep=5pt, arc=4pt, boxrule=1pt, left=0pt, right=0pt]
In response to \textbf{RQ3}:  the \textit{Knowledge Synthesizer} automates knowledge extraction and slightly surpasses human-curated knowledge in both recall and precision, enhancing the framework's performance, while reducing manual effort.
\end{tcolorbox}


\subsection{Comparison of \tool~and GPTScan on Code4Rena}
To evaluate the effectiveness of \tool~on fully developed projects with all supporting files, we applied the best-performing configuration identified in previous sections—Haiku as the Knowledge Synthesizer and Prompt Generator, paired with 4o-mini as the Auditor—on the Code4Rena dataset. 
This dataset presents additional challenges due to its complexity and diverse set of components, providing a rigorous test of the system's capabilities. 
For comparison, we also evaluated the state-of-the-art tool GPTScan on the same dataset.

\begin{table}[]
\caption{Performance of \tool~and GPTScan on Code4rena Dataset}
\label{table:comparisonc4}
\begin{tabular}{@{}lccc|ccc@{}}
\toprule
Metrics    & FN & TP & FP    & Recall & Precision & F1    \\ \midrule
\tool      & 11 & 13 & 128.3 & 0.54   & 0.092     & 0.157 \\
GPTScan    & 21 & 3  & 29    & 0.13   & 0.094     & 0.107 \\ \bottomrule
\end{tabular}
\end{table}

The performance comparison between \tool~ and GPTScan are summarized in Table~\ref{table:comparisonc4}.
\tool~demonstrated a significant improvement in recall (0.54) compared to GPTScan (0.13), indicating a superior ability to detect vulnerabilities in the dataset. 
This higher recall highlights the effectiveness of the LLM generated knowledge synthesis and optimized prompt generation.
However, \tool's precision (0.092) remains comparable to GPTScan (0.094), reflecting the challenges in reducing false positives in complex datasets.
Despite the trade-offs, \tool~achieved a higher overall F1-score of 0.157 compared to GPTScan's 0.107, underscoring its improvements on complex dataset.

These findings highlight that a simple and general framework \tool~ can outperform state-of-the-art tools like GPTScan, even in complex and challenging datasets. 
This positions \tool~as a promising solution for advancing vulnerability detection frameworks.

\begin{tcolorbox}[colback=gray!20!white, colframe=gray!75!black, boxsep=5pt, arc=4pt, boxrule=1pt, left=0pt, right=0pt] 
\textbf{Finding 5:}
\tool~outperforms GPTScan on the challenging Code4Rena dataset, achieving higher recall (0.54 vs. 0.13) and F1-score (0.157 vs. 0.107). However, its precision remains on par with GPTScan, reflecting the need for further refinements to reduce false positives.
\end{tcolorbox}