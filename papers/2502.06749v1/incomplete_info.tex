\section{Incomplete Information Setting}\label{sec:incomplete}
In this section, we switch our attention to the \emph{incomplete} information setting. Our first set of results provide a characterization of when the optimal effort profile for agents can be efficiently computed. We highlight that \emph{under partial uncertainty} and for general weighted $\ell_p$ costs, the optimization program of the agent is convex and can be solved efficiently (Lemma~\ref{prop:partial_incomp_convex}). We also show a technical result in Lemma~\ref{lem:delta_charac}, capturing when the above convex program is feasible and when it is not. We conclude by providing a negative result in the setting where there is uncertainty over both the classifier and the causal graph (Proposition~\ref{prop:full_uncert_nonconvex})---in this case, we show that the agent's optimization program is non-convex and hence, \emph{hard to solve} in the worst case. 

We then aim to characterize what the optimal effort profiles look like. In the $\ell_1$ cost case, we show a sharp contrast with the complete information case: namely, an agent may be willing to expend effort across several features, as opposed to a single one in the complete information case (Lemma~\ref{lem:l1_incomp}). In the $\ell_2$ case, we provide a semi-closed form characterization of the agent's optimal effort profile (Theorem~\ref{thm:incomp_l2}). We also highlight how, under some partial information models, it is possible to provide a more interpretable characterization of how the effort per feature depends on $\E[\C h]$ and $\text{Var}(\C h)$ (Corollary~\ref{cor:prop_effort}): the higher the expected importance $(\C h)_i$ of a feature $i$, the more effort is spent towards it, and the higher the variance of $\C h$ towards feature $i$, the less effort is spent towards it. 

Recall that for the incomplete information setting, the agent's optimization problem is: 
\begin{align}\label{opt:agent_br_incomp}
    \effopt(\Pi_h, \Pi_{\C}) &= ~arg\min_{\eff} \quad \textsf{Cost}(\eff) \quad \text{s.t.}\\
    &\bP_{h \sim \Pi_h, \C \sim \Pi_{\C}}\left[ (\C h)^{\top}\eff \geq \alpha \right] \geq 1-\delta, \quad \delta \in (0,1). \nonumber
\end{align}

Below, we briefly remind the reader of the models of information used in this paper.

\paragraph{Models of Information.} We remind the reader here that there can be two different sources of uncertainty: i) the principal's classifier; and, ii)  the edge weights of the causal graph $\cG$ (the graph topology is assumed to be common knowledge). In particular, we have three following combinations of where uncertainty lies in our problem:
\begin{enumerate}
    \item Uncertainty only exists in the principal's classifier, the causal graph is fully known;
    \item Uncertainty only exists in the edge weights of the causal graph, the classifier is fully known; 
    \item Uncertainty exists over both the classifier and the causal graph.
\end{enumerate}
We will henceforth refer to models $1$ and $2$ as models of \textit{partially incomplete information}. 

\subsection{Optimal Effort Computation}

We start by analyzing the computation of an agent's optimal effort in Models $1$ and $2$:

\paragraph{Models $1$ and $2$:} Since uncertainty manifests in the form of Gaussian priors for the agent (as per assumption), it is easy to see that for information models $1$ and $2$ above, \textit{the overall uncertainty is also Gaussian}: i.e., $\C h$ is a Gaussian random variable. In that case, we can rewrite the agent's optimization problem as follows: 
\begin{align}\label{opt:incomp_gaussian}
    \effopt(\Pi_h, \Pi_{\C}) = ~arg\min \quad &\textsf{Cost}(\eff) \\
    \text{s.t.}~~& \bP_{(\C h) \sim \mathcal{N}(\mu_{\C h}, \Sigma_{\C h})} \left[ (\C h)^{\top}\eff \geq \alpha \right] \geq 1-\delta. \nonumber
\end{align}
Our first result is that above problem is a convex optimization problem under Models 1 and 2. 
\begin{lem}\label{lem:partial_incomp_convex}
Under partially incomplete information (models $(1)$ and $(2)$) and cost functions given by Eq.~\eqref{eq:cost}, the agent's optimization problem to find the optimal effort profile $\effopt$, given by \eqref{opt:incomp_gaussian}, is a convex program for any $\delta \leq \frac{1}{2}$. 
\end{lem}


The proof can be found in Appendix~\ref{app:partial_incomp_convex}.



The last result shows that under limited uncertainty, the agent can still efficiently solve for an effort profile that helps her to pass the classifier with high probability. 

\begin{remark}
We note however that this optimization problem \eqref{opt:incomp_gaussian} is not always feasible:  
To see this, take a look at the worst case when an agent has no information about the problem: i.e., $\Sigma_{\C h} \to +\infty$. Then an agent who is acting blind manipulates in a direction that lowers their true score $h_0^\top x$ with probability exactly $1/2$. I.e., with probability at least $1/2$, they never pass the classifier, and a probability of $1-\delta$ with $\delta$ small cannot be guaranteed. As $\delta \to 0$, the uncertainty intuitively makes it impossible for the agent to guarantee that they will pass the classifier, making the problem also infeasible. 
\end{remark}

To further investigate this, we provide a complete characterization of when optimization problem \eqref{opt:incomp_gaussian} is feasible as a function of $\delta$. The following result highlights the trade-off between the degree of uncertainty in the model and the highest coverage probability ($1-\delta$) that can be achieved.
\begin{lem}\label{lem:delta_charac}
Suppose that $\Sigma_{\C h}$ is positive definite. Then the optimization problem \eqref{opt:incomp_gaussian} is:
\begin{enumerate}
    \item feasible when $\delta > \Phi^{-1}\left( - \| \Sigma_{\C h}^{-1/2} \mu_{\C h} \|_2 \right)$, and
    \item infeasible otherwise, 
\end{enumerate}
where $\Phi^{-1}(\cdot)$ indicates the inverse of the standard normal CDF. 
\end{lem}

The proof is given in Appendix~\ref{app:delta_charac}. Here it is also worth pointing out the main difference with the complete information setting --- \emph{with complete information, an agent can always pass the classifier by choosing effort correctly, unlike the incomplete information setting where a positive outcome is not guaranteed.}

\paragraph{Model $3$: }We now show that under model $(3)$ when there is uncertainty over both the classifier and the causal graph, solving for the optimal effort profile is a non-convex problem in general, and classical optimization algorithms cannot be directly used here.

\begin{prop}\label{prop:full_uncert_nonconvex}
Under incomplete information model $(3)$ and cost functions given by Eq.~\eqref{eq:cost}, the agent's optimization problem, given by \eqref{opt:agent_br_incomp}, is a non-convex program.
\end{prop}

The proof can be found in Appendix~\ref{app:full_uncert_nonconvex}. 
Since solving non-convex optimization problems to global optimality can be NP-hard in the worst case, the above result shows that Model $3$ is likely not computationally tractable. 

\subsection{Characterization of Optimal Effort Profiles}\label{subsec:incomp_effort}

Under partially incomplete information (uncertainty over either the principal's classifier or the edge weights of the causal graph, but not both) with Gaussian priors, we have shown that the agent's optimization problem reduces to the following convex program: 
\begin{align}\label{opt:agent_br_convex}
     \effopt(\Pi_h, \Pi_{\C}) = arg\min_{\eff} \quad &\textsf{Cost}(\eff) \\
     \text{s.t.} \quad &\alpha - \mu_{\C h}^{\top}\eff - p_{\delta}\cdot ||\Sigma_{\C h}^{1/2}\eff||_2 \leq 0, \nonumber
\end{align}
where $(\C h) \sim \mathcal{N}(\mu_{\C h}, \Sigma_{\C h})$, $\delta \leq \frac{1}{2}$ and $p_{\delta} = \Phi^{-1}(\delta)$. Our goal is to gain insights into the agent's optimal effort profile for the cost function class outlined in Eq.~\eqref{eq:cost}. While Program~\eqref{opt:agent_br_convex} can be complex and highly non-convex in the general case, we highlight that we can still obtain structural results for $\ell_1$-costs and a semi-closed form characterization for $\ell_2$-costs.


\subsubsection{Case $1$ (Weighted $\ell_1$-norm costs)}
We have the following optimization problem for the agent: 
\begin{align*}
    \min_{\eff} \quad &\sum_{f \in \cF} c_f |\eff_f| \quad \text{s.t.}\\
    & \alpha - \mu_{\C h}^{\top}\eff - p_{\delta}\cdot ||\Sigma_{\C h}^{1/2}\eff||_2 \leq 0.
\end{align*}

Our first result shows that there is a sharp contrast in the structure of the optimal effort profile for $\ell_1$ costs between the complete information setting and partially incomplete information setting.  
\begin{lem}\label{lem:l1_incomp}
Under partially incomplete information, the optimal effort profile $\effopt$ for an agent with weighted $\ell_1$-norm costs requires investment of effort into more than one feature in the worst case.  
\end{lem}


The proof can be found in Appendix~\ref{app:l1_incomp}. 



\subsubsection{Case $2$ ($\ell_2$-norm costs)}
We have the following optimization problem for the agent: 
\begin{align*}
    \min_{\eff} \quad &||\eff||_2 \quad \text{s.t.}\\
    & \alpha - \mu_{\C h}^{\top}\eff - p_{\delta}\cdot ||\Sigma_{\C h}^{1/2}\eff||_2 \leq 0.
\end{align*}
\begin{thm}\label{thm:incomp_l2}
The optimal effort profile $\effopt$ for an agent with $\ell_2$-norm cost function under partially incomplete information, is of the following form: 
\[
       \effopt = \lambda^* \left(k_1 I + k_2 \Sigma_{\C h} \right)^{-1}\mu_{\C h},
\]
where $k_1, k_2, \lambda^* > 0$. 
\end{thm}


The full proof can be found in Appendix~\ref{app:incomp_l2}.


\xhdr{An interpretable special case.}
We study a special case of our problem where we can provide an intuitive explanation of how agents decide to exert effort. In particular, we assume the following. 
\begin{aspt}
$\Sigma_{\C h}$ is a diagonal matrix. We denote $(\Sigma_{\C h})_f$ the $f$-th diagonal entry, which corresponds to the uncertainty with respect to the total contribution by feature $f$.
\end{aspt}

We first note that $\Sigma_{\C h}$ being diagonal arises in very natural settings. One such setting is when i) the uncertainty is on the causal graph $\cG$ and ii) the causal graph is bipartite: i.e., features are either \emph{causal} (they affect other features, but cannot be affected themselves) or \emph{proxy} (they are affected by causal features, but cannot affect any other feature). In this case, causal features only have outgoing edges, while proxy features only have incoming edges. Formally: 

\begin{prop}\label{prop:bipartite}
Suppose $\cG$ is a bipartite graph; further, suppose the agent only has uncertainty over the weights of the graph (model $2$), i.e. $\Sigma_h = \bf{0}$. Then, $\Sigma_{\C h}$ is a diagonal matrix.
\end{prop}


The proof can be found in Appendix~\ref{app:bipartite}.


We now highlight our result relating the effort spent on feature $f$ to the total expected contribution of that feature, $(\mu_{\C h})_f$, and the variance of the contribution of said feature, $(\Sigma_{\C h})_f$:

\begin{cor}\label{cor:prop_effort}
If $\Sigma_{\C h}$ is a diagonal matrix with entries $(\Sigma_{\C h})_f$ corresponding to feature $f$, then the optimal effort profile $\effopt$ has the following form: 
\[
          \effopt_f =  \frac{\lambda^*(\mu_{\C h})_f}{k_1 + k_2 \cdot (\Sigma_{\C h})_f} \quad \forall~f \in \cF.
\]
\end{cor}

\noindent 
This result shows that in the optimal effort profile, the agent invests more effort into features that have a higher expected contribution $(\mu_{Ch})_f$. Further, the denominator highlights that agent may shy away from features they have a lot of uncertainty about: a high value of $(\Sigma_{\C h})_f$ leads to a lower effort invested in that feature. 

\subsection{$\beta$-desirability under Incomplete Information} 
We conclude this section with a discussion on how to induce $\beta$-desirable effort profiles under incomplete information. As we see throughout Section~\ref{subsec:incomp_effort}, it may be difficult to characterize the agent's optimal effort in closed form under incomplete information, except for some limited cases. Here, we focus on providing broad insights here and build on this discussion through numerical experiments in Section~\ref{sec:experiments}.  

\paragraph{The Interpretable Special Case: Diagonal Covariance $\Sigma_{\C h}$:}
In this special setting where we have an interpretable form of the agent's optimal effort profile, we can identify conditions that guarantee investment in $\beta$-desirable effort profiles by rational agents. We present the following result:
\begin{cor}\label{corr:beta_model2}
Suppose that the covariance matrix of feature importance, given by $\Sigma_{\C h}$, is a diagonal matrix. In that setting, if all features have the same overall level of uncertainty and the mean feature importance vector $\mu_{\C h}$ satisfies (which follows from Corollary~\ref{cor:prop_effort}): 
\[
      \| \left(\mu_{\C h}\right)_{\des} \|_2 \geq  \frac{\beta}{\sqrt{1-\beta^2}} \| \left(\mu_{\C h}\right)_{\und} \|_2,
\]
then the best response of a rational agent with $\ell_2$-norm cost is to invest in a $\beta$-desirable effort profile. 
\end{cor}

The above result should be intuitive---when agents face the same degree of uncertainty about the importance of all features, they choose which features to invest effort in based on the mean importance of the features. Therefore, it makes sense that the higher the total net importance (measured by the $\ell_2$-norm) of the set of desirable features, higher the incentive for agents to invest in desirable effort profiles. Finally, we relate $\beta$-desirability to the uncertainty on given features: 

\begin{cor}\label{cor:prop_effort_variance}
$\effopt_f =  \frac{\lambda^*(\mu_{\C h})_f}{k_1 + k_2 \cdot (\Sigma_{\C h})_f}$
is decreasing in $(\Sigma_{\C h})_f$.
\end{cor}

In the general case where different features have different levels of uncertainty, if desirable features have a high degree of uncertainty, this pushes agents away from $\beta$-desirable effort profiles. On the other hand, more uncertainty on undesirable features is good for $\beta$-desirability. This is because having a higher degree of uncertainty (higher variance $(\Sigma_{\C h})_f$) about the importance of a feature actively discourages agents from investing effort into said feature. 


\paragraph{What does it mean for $\Sigma_{\C h}$ to not be diagonal?} We provide a short discussion of when non-diagonal covariance matrix arise. In particular, non-diagonal covariance matrices $\Sigma_{\C h}$ arise:
\begin{enumerate}
    \item always under Model $1$ (i.e. where the causal graph is fully known, but there is uncertainty over the classifier), and 
    \item under Model $2$ when the causal graph $\cG$ is not bipartite.
\end{enumerate}
We explore the non-diagonal $\Sigma_{\C h}$ case in greater detail in Section~\ref{sec:experiments}, with experiments on real data that consider more general cases where $\Sigma_{\C h}$ may not be diagonal, and in particular covering Model $1$, when the classifier is not fully known to an agent. Our experiments suggest that many of the same insights about $\beta$-desirability hold (even \emph{without} the assumption that $\Sigma_{\C h}$ is diagonal).








