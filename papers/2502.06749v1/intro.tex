\section{Introduction}\label{sec:intro}

The widespread adoption of automated decision-making systems has brought significant attention to the issue of \emph{strategic classification}---a machine learning setting where individuals modify their features to secure favorable outcomes. This phenomenon is common in many domains: students enroll in preparatory courses to enhance their chances at college admission; job seekers tailor their resumes to align them with AI-based hiring algorithms, and individuals adjust their financial behaviors to improve credit scores. Some of these modifications reflect \emph{genuine efforts} to enhance one's qualifications or financial responsibility (e.g., acquiring new skills or consistently paying off loans), while others \emph{effectively game} the system, e.g., artificially boosting credit scores by opening new credit lines or strategically targeting specific keywords in algorithmic resume screening.

The distinction between \emph{desirable} and \emph{undesirable} modifications is not always clear-cut. While gaming is typically regarded as problematic, even genuine improvements can vary in how desirable they are. For example, in healthcare, encouraging patients to adopt preventive lifestyle changes (such as improved diet and regular exercise) may be preferable to medical interventions like medication or surgery for conditions such as obesity or hyperlipidemia. This highlights the nuanced nature of strategic classification: interventions that lead to real improvements may still not align with preferred or \emph{desirable} forms of improvement, where desirability is decided by the learner.

Further, a key challenge in strategic classification is that features are often interdependent. That is, modifications to one feature can have cascading effects on others. For example, increasing the number of credit cards an individual holds will also lower their credit utilization percentage, indirectly influencing their credit-worthiness. Similarly, reducing alcohol consumption or improving dietary habits can mitigate multiple health risks, such as obesity, hyperlipidemia, and cardiovascular diseases. These dependencies are best captured using a \emph{causal graph}, a framework that has been explored in a limited amount of prior work~\cite{kleinberg2019classifiers,miller2020strategic,shavit2020incentives,bechavod2022gaming} in the specific context of strategic classification. 

Our work builds upon this causal perspective on strategic classification, investigating how agents respond to decision-making systems and, in particular, when their strategic behavior aligns with desirable modifications. We adopt a framework in which a principal (e.g., a decision-maker or machine learning classifier) deploys a model, and agents (or individuals) strategically adjust their features to maximize their probability of receiving a favorable classification outcome. 

\vspace{5pt}

\noindent {{\bf Summary of Contributions.}} Our contributions are as follows: 

\emph{Our Model:}  In Section~\ref{sec:model}, we introduce our model to study incentivizing desirable efforts in the context of causal strategic classification. We build on previous work in two different ways: i) first, we introduce incomplete information to the study of causality in strategic classification, highlighting situations where an agent may not know the classifier, the causal graph, or both; and ii) we introduce a new notion of $\beta$-desirability which quantifies the extent to which agents invest effort in features deemed desirable by the principal.

\emph{Complete Information:}  In Section~\ref{sec:complete}, assuming agents have full knowledge of the classifier and the causal structure, we characterize the optimal effort profiles under various modification cost structures. We establish theoretical conditions guaranteeing investment in desirable effort profiles by rational agents. We also demonstrate that finding classifiers that induce desirable behavior is, in general, a non-convex problem.  
However, we show that when the principal chooses only \emph{one} desirable feature to incentivize, the problem of finding good classifiers becomes convex. We also provide a simple convexification heuristic  for when the number of desirable features is more than one,  ensuring that chosen classifiers do \emph{not} incentivize more than a certain amount of undesirable feature effort.

\emph{Incomplete Information:}  We extend our analysis to the setting where agents lack information about either the classifier or the causal graph (or both) in Section~\ref{sec:incomplete}. There, incomplete information is modeled as agents having Gaussian priors about the classifier and the causal graph. First, we show that in presence of uncertainty over both the classifier and the causal graph, choosing how to invest effort optimally is a non-convex problem for the agent. However, the problem becomes tractable under \emph{partial uncertainty}, and we provide a semi-closed-form characterization of optimal effort profiles for agents in partial uncertainty settings. %

\emph{Case study:} Finally, in Section~\ref{sec:experiments}, we complement our theoretical insights in the incomplete information setting with numerical experiments, basing our experimental setup on a medical study from previous work that predicts \emph{risk of cardiovascular disease} (CVDs) in adults. In the process, we provide insights into how to incentivize changes in desirable features under uncertainty.



\vspace{5pt}
\noindent {\bf Related Work:} 
Strategic classification, a machine learning setting where agents can manipulate or modify their own features to improve their outcomes, has been widely studied in recent years under a range of assumptions. This belongs to a broad class of problems in economics called principal-agent problems where agents act strategically in their self-interests which are often misaligned with the principal's interests~\cite{grossman1992analysis,ross1973economic,laffont2009theory,sappington1991incentives}. Early works in strategic classification focused on scenarios where agents manipulate their observable features solely to ``game'' a published classifier, thereby increasing their chances of a favorable label without genuinely improving underlying attributes (e.g., \cite{hardt2016strategic, braverman_randomness, dong2017strategicclassificationrevealedpreferences,zhang2022fairness,lechner2023strategic,chen2020learning,ahmadi2021strategic,sundaram2023pac}). Over time, the literature expanded to consider settings where agents make substantive changes to their features (i.e., effectively investing in real improvements) rather than relying on superficial modifications (e.g., \cite{kleinberg2019classifiers,bechavod2022information,bechavod2022gaming,shavit2020incentives,harris2021stateful}). In many cases, actual improvement involves investing effort which is not directly observable by the principal --- this again has similarities to the notion of moral hazard in insurance markets~\cite{pauly1968economics,arrow1968economics} and other general settings~\cite{arrow1978uncertainty}. There has also been interest in fairness in strategic classification (e.g., \cite{milli2019social,hu2019disparate,estornell2023group}) but this line of work is more distantly related to ours.

A useful tool to model manipulations as opposed to effective improvement is \emph{causality}. Causal modeling has been extensively studied in decision-making and machine learning, starting with~\cite{pearl2000causality}; see~\cite{kaddour2022causalmachinelearningsurvey} for a recent survey of causal machine learning. 
In the context of strategic classification, a few recent studies have incorporated causal modeling to account for interdependencies among features~\cite{kleinberg2019classifiers,shavit2020incentives,bechavod2022gaming, blum_game_improve,miller2020strategic,horowitz2023causalstrategicclassificationtale}.~\cite{miller2020strategic} highlights that in strategic classification, learning a classifier that incentivizes gaming as opposed to improvement is as hard as learning the underlying causal graph between features.
~\cite{shavit2020incentives} and~\cite{bechavod2022gaming} both focus on special cases of linear causal graphs, unlike our work that considers general cases of linear graphs.~\cite{blum_game_improve} explore strategic classification using a different structural framework known as ``manipulation graphs,'' where each agent has a fixed set of costly effort profiles that they may choose from, defining a bipartite graph between initial agent features and induced features after exerting effort---this is, again, a special case of causal graphs.~\cite{horowitz2023causalstrategicclassificationtale}, similarly to us, distinguish among causal, non-causal (or ``correlative''), and unobserved features. However, they focus on a different objective of maximizing predictive accuracy, we are interested in incentivizing ``desirable'' modifications.

Perhaps closest to our work is the work of~\cite{kleinberg2019classifiers}. Like us, they focus on general causal graphs; however, we highlight several major differences. First, we focus on \emph{classification} settings, while~\cite{kleinberg2019classifiers} focus on regression and scoring settings. Second, we highlight differences in our agent model, where our agents invest effort to pass the classifier with reasonably high probability, while agents in~\cite{kleinberg2019classifiers} \emph{always} exert effort to improve their score. Third, we note that our cost model is strictly more general: where~\cite{kleinberg2019classifiers} focuses on linear costs, our work considers general $\ell_p$-costs. Our results show that this choice of cost is important, noting a sharp distinction in agent behavior between the cases of $\ell_1$-cost and $\ell_p$ costs for $p > 1$. Finally, unlike \cite{kleinberg2019classifiers}, our study incorporates \emph{incomplete information}, where agents may not fully understand either the causal graph or the deployed classifier.

%\textcolor{red}{I think this paragraph could use general purpose references when it comes to information asymmetries/etc. to buff up the references. Right now we are a bit short} 
A closely related line of work investigates strategic classification under varying models of \emph{information} available to agents. In many real-world settings, agents may have \emph{incomplete information} about the classifier---either because it is too complex, or because the learner's model is proprietary, or the causal relationships governing feature interactions~\cite{bechavod2022information,cohen2024bayesianstrategicclassification,ghalme2021strategic}. Or, agents might misperceive the classifier due to behavioral biases~\cite{ebrahimi2024double}.
However, we are not aware of any work studying uncertainty on causal graphs, and to the best of our knowledge, we are the first work in the space of strategic classification to incorporate \emph{both} causal modeling and incomplete information in strategic classification.






