\section{Model}\label{sec:model}
We consider a \emph{binary classification} problem, where there is an interaction between a principal and an agent. The principal, also known as the \emph{learner}, deploys a machine learning model or classifier. Then, the model assigns a (binary) score or a decision to the agents, based on their \emph{features}. Finally, agents \emph{respond} to the deployed classifier, potentially changing their features to obtain better outcomes, at a cost. We provide a detailed description of the model below.

\subsection{Agent Model: Features and Utilities}

Formally, let $\cF \in \mathbb{R}^d$ be the set of all features and $\calY = \{-1, +1\}$ be the set of labels. Each agent $k$ is defined by a pair $(x_k,y_k)$, where $x_k \in \cF$ is the agent's feature vector and $y_k$ is the agent's true label (i.e., their actual fit or qualification for the classification task at hand). 


\xhdr{Causal modeling of feature interactions.} We adopt a \emph{causal} perspective on strategic classification, where different features can impact each other---i.e., a change in a feature $i$ (e.g., alcohol consumption or diet) that has a causal relationship with feature $j$ (e.g., cholesterol) will also induce a change in feature $j$. 

The chain of causality between the different features can be captured using a weighted directed graph $\cG = (\cF, \cA, w)$. We will henceforth call graph $\cG$ the \emph{causal graph}. We slightly abuse notation here and denote the set of nodes in the graph also by $\cF$ to indicate that each feature in $\cF$ corresponds to a node on $\cG$. $\cA$ represents the set of directed edges on $\cG$, where an edge from features $i$ to $j$ indicates that $i$ is causal for $j$. Finally, $w: \cA \rightarrow \R$ captures the weights of the edges. We make no assumption on the structure of $\cG$, other than the fact that it is a directed \emph{acyclic} graph.
\footnote{This is a standard assumption in the causal strategic classification~\cite{miller2020strategic,kleinberg2019classifiers} }
We represent all necessary information about the graph using an \emph{adjacency matrix} $A \in \R^{d \times d}$:

\begin{defn}[Adjacency Matrix of a Causal Graph]
\begin{align}\label{eq:adj_matrix}
       A_{ij} = \begin{cases}
       0, \quad \quad \quad \, \text{if $a_{ij} \notin \cA$,}\\
       w(a_{ij}), \quad \text{if $a_{ij} \in \cA$.}
       \end{cases}
\end{align}
\end{defn}

Here, if there is an edge $a_{ij} \in \cG$, then feature $i \in \cF$ \emph{causally affects} feature $j \in \cF$; in other words, by changing feature $i$, an agent is making an implicit change on feature $j$. Finally, the weight of edge $a_{ij}$, given by $ w(a_{ij})$ indicates that if the value of feature $i$ improves by a unit amount, then the value of the downstream feature $j$ will improve by $w(a_{ij})$\footnote{Throughout the paper, we assume that causal relationships are linear. This is another common assumption in the literature~\cite{kleinberg2019classifiers,shavit2020incentives}}. Importantly, \textit{edge weights can be negative}---an increase in causal feature $i$ might lead to a decrease in feature $j$. 

\paragraph{Desirable vs Undesirable Features} Importantly, we assume that the set of features $\cF$ is divided by the principal into two kinds of features: \textbf{desirable} features and \textbf{undesirable} features denoted by sets $\des$ %(where $|\des| = n$)
and $\und$ %(where $|\und| = k$) 
respectively. Roughly speaking, \emph{desirable} features are those that the principal wants to incentivize the agent to change; e.g., in the health application of the Introduction, ``alcohol consumption'' would be a \emph{desirable} feature that the principal (for example, the agent's primary care physician) would like to see changed\footnote{Here, we would like to see the value of the feature lowered. Note that whether we want to increase or decrease the value of a feature has no bearing on whether it is desirable; only the fact that this is one of the target features we aim to change is.}. Undesirable features, on the other hand, can be considered as features that %can potentially be gamed; 
we would like to disincentivize agents from modifying directly: e.g., directly intervening to lower an agent's cholesterol level via medication such as statins may be less desirable than promoting lifestyle changes (lower alcohol consumption, improved diet, etc.) that will also lower their cholesterol.


\begin{remark}[Relationship between causality and desirability ]
In the traditional causality literature on strategic classification, features on a causal graph are usually classified into \textit{causal} and \textit{non-causal} or \emph{proxy} features. Causal features are those which affect some specified output variable of interest while proxy features are those which do not. A natural question is whether and how desirability of a feature relates with its causality. 

Proxy features are generally seen as undesirable to modify: this is because they do not change the root cause behind an agent's label, hence do not lead to true improvements in said label---this is often referred to as ``gaming'' the classifier. However, causal features may still be undesirable, even if they do not lead to gaming. For example, in our healthcare example, both diet and cholesterol levels are causal for predicting the risk of cardio-vascular disease---in particular, diet is directly causal for cholesterol levels, and cholesterol levels are directly causal for cardio-vascular disease. Yet, it may be preferable to incentivize sustainable interventions such as a better diet (prevention), rather than resorting to short-term fixes like cholesterol-lowering medications that may have significant side effects (treatment). 
\end{remark}

\subsection{Principal - Agent Interaction}

%The principal and the agents know the causal graph $\cG$ fully. 
The principal deploys a \emph{linear classifier} denoted as $h_0 \in \R^{d}$ from its normal vector. Under this classifier, an agent with feature vector $x \in \R^{d}$ is assigned a score of $s(x) = h_0^\top x$. There is a pre-determined threshold $\tau \in \R$ and the classification decision $y$ for said agent is given by: 
\begin{align*}
    y(x) = \mathbf{1}\left[s(x) \geq \tau \right]. 
\end{align*}

\xhdr{Agent's Best Response.} The agent is assumed to have Gaussian priors $\Pi_h := \mathcal{N}(\mu_h, \Sigma_h)$ (where $\mu_h \in \R^{|\cF|}, \Sigma_h \in \R^{|\cF|\times |\cF|}$) over the principal's deployed classifier $h_0$ and $\Pi_{\C} := \mathcal{N}(\mu_w, \Sigma_w)$ (where $\mu_w \in \R^{|\cA|}, \Sigma_w \in \R^{|\cA| \times |\cA|}$) over the edge weights of the causal graph $\cG$\footnote{Gaussian priors are frequently used to model incomplete information~\cite{kong2020information,elzayn2019price}}. The agent is always assumed to know the topology of the causal graph. We explore two kinds of information structures: 
\begin{enumerate}
    \item The \textit{Complete Information setting} where the agent fully knows the classifier $h_0$, i.e, $\mu_h = h_0$ and $\Sigma_h = \bf{0}$ and the weights of all edges of $\cG$, i.e., $\mu_w = w$ and $\Sigma_w = \bf{0}$. See Section~\ref{sec:complete} for the complete information setting. 
    \item The \textit{Incomplete Information setting} where i) there is uncertainty over the principal's classifier $h_0$, i.e., $\mu_h$ may differ from $h_0$ (bias) and $\Sigma_h \neq \bf{0}$ (variance), and/or ii) there is uncertainty over the edge weights of the causal graph $\cG$, i.e., $\mu_w$ may differ from $w$ (bias) and $\Sigma_w \neq \bf{0}$ (variance). See Section \ref{sec:incomplete} for the incomplete information setting. 
\end{enumerate}
If $y(x) = 0$, the agent also knows the amount $\alpha > 0$ by which she fell short of passing the classifier; e.g., in a loan approval setting, an agent may know their current credit score and the threshold credit score that the bank uses to decide who gets approved for a loan. They might, however, not fully understand how said credit score is calculated in the first place.

The agent's goal is to obtain a positive classification outcome, i.e. $y(x) = 1$ (``to pass the classifier''). Therefore, she attempts to \emph{change} her feature vector $x$ by investing some \emph{exogenous effort} $\eff \in \R^{|\cF|}$, which we call the agent's \emph{exogenous effort profile}. Importantly, this exogenous effort profile, in our model, is exerted directly on features\footnote{This is without loss of generality, by a simple revelation principle type of argument}: i.e., $e$ is a vector quantifying how much an agent changes their features \emph{directly}. However, remember that exerting exogeneous effort on a subset of the features $\cF$ can also lead to features, particularly those that no effort was exerted on, to change \emph{endogenously}, due to causality. We call this the \emph{induced} or \emph{endogenous feature change}.

Now, suppose the agent's modified feature vector after investing effort is given by $x'(\eff)$. We define the net change in features due to effort $\eff$ as:
\[
      \Delta x(\eff) = x'(\eff) - x,
\]
where $\Delta x(\eff)$ can be computed using the structure of the causal graph $\cG$. This effort comes at a cost, modeled through a \emph{cost function} 
\[
\Cost: \R^{d} \to \R_{\geq 0},
\]
where $\Cost(e)$ is the cost incurred to perform effort $\eff$. We mainly focus on (weighted) $\ell_p$-norm cost functions for all $p \geq 1$. Formally, 
\begin{align}\label{eq:cost}
             &\textsf{Cost}(\eff) = \left(\sum_{f \in \cF} c_f\left \vert \eff_f \right \vert^p \right)^{1/p},~\text{where}~c_f > 0~~\forall f \in \cF,
\end{align}
where $c_f$ represents a cost multiplier associated with investing unit exogenous effort into feature $f$. 



To express $\Delta x(\eff)$ in terms of the causal graph $\cG$, we define the \emph{contribution matrix} of $\cG$, which allows us to quantify how much effort profile $\eff$ maps to a total change in features, including both exogenous effort and induced feature changes:

\begin{defn}\label{def:cont_matrix}
The contribution matrix $\C \in \R^{d \times d}$ associated with causal graph $\cG$ is: 
\begin{align*}
      &\C_{ii} = 1 \quad \forall~i \in [d], \quad \text{and}\\
      &\C_{ij} = \sum_{p \in \mathcal{P}_{ij}} \omega(p) \quad \forall~ i,j \in [d],~i \neq j,
\end{align*}
where $\mathcal{P}_{ij}$ is the set of all directed paths from node $i$ to node $j$ on $\cG$ and $\omega(p)$ is the weight of path $p \in \mathcal{P}_{ij}$ with $\omega(p) = \prod_{a \in \cA, a \subset p}w(a)$. 
\end{defn}

Note that in causal graphs, a given feature $i$ may affect another feature $j$ directly---in which case there is an edge of non-zero weight from $i$ towards $j$, but also indirectly, through other features. For example, there may be a path from feature $i$ to feature $j$ through intermediary features $i_1,\ldots, i_k$, where $i \to i_1 \to i_2 \to \ldots \to i_k \to j$\footnote{$x \to y $ indicates that $x$ is directly causal for $y$.}. Therefore, investing effort in feature $i$ will lead to modifications of not only feature $i$, but also all features $i_1, \ldots, i_k,~j$ that it is directly or indirectly causal for. The contribution matrix $\C$ quantifies the impact of any given feature $i$ on any other feature $j$, \emph{even when the features have an indirect causal relationship}.

\begin{obs}\label{obs:comoute_contri}
Given matrix $A$ as defined in Equation~\eqref{eq:adj_matrix}, the contribution matrix is given by 
\[
\C = \sum_{k=0}^{|\cF|} A^k,
\]
and therefore can be computed in polynomial time in $|\cF|$. Intuitively, $A^k$ represents the contribution of all paths of size exactly $k$; because our graph is acyclic, the longest path must have length at most $|\cF|$. 
\end{obs}
\begin{proof}
This is a well-known result, but we provide a proof in Appendix~\ref{sec:app_sup} for completeness.
\end{proof}
\noindent 
Given the contribution matrix $\C$ and exogenous effort profile $\eff$, the net change in features $\Delta x(\eff)$ will be given by:
\begin{align}\label{eq:delta_x}
       \Delta x(\eff) = \C^\top \eff.
\end{align}
To simplify notation, we drop the dependence on $e$ and write $\Delta x$ when clear from context. We demonstrate how to construct $\C$ and how to compute $\Delta x$ given an effort profile $\eff$ through a toy example in Figure~\ref{fig:schematic_C}.
\begin{figure}[hbt!]
    \centering
    \includegraphics[width=.85\textwidth]{Figures/schematic_C.png}
    \caption{Consider a simple causal graph $\cG$ with $|\cF| = 3$. Feature $1$ directly affects features $2$ and $3$ and feature $2$ directly affects feature $3$. Feature $1$ also indirectly affects feature $3$ through the path $1 \to 2 \to 3$. The contribution matrix $\C$ captures both of these effects.}
    \label{fig:schematic_C}
\end{figure}

The agent chooses their optimal effort profile $\effopt$ that ensures that $y(x'(\effopt)) = +1$ with probability at least $1-\delta$, while incurring the minimum possible cost. We call the effort profile $\effopt(\Pi_h, \Pi_{\C})$ the agent's \emph{best response} to priors ($\Pi_h, \Pi_{\C}$). Formally: 
\begin{align*}
   \effopt (\Pi_h, \Pi_{\C}) = \arg \min_{\eff} \quad &\Cost(\eff) \\
    \text{s.t.} \quad &\bP_{h \sim \Pi_h, \C \sim \Pi_{\C}}\left[h^{\top}\C^{\top}\eff \geq \alpha \right] \geq 1-\delta \numberthis{\label{opt:agent_br}}.
\end{align*}
Note that the constraint ensures that an agent passes the classifier with probability at least $1-\delta$, measured with respect to their prior on the causal graph and on the classifier. In particular, if they exert effort profile $\eff$, their features change by $\C^\top \eff$, so their score changes by $h^{\top}\C^{\top}\eff$, and they have to improve their score by $\alpha$ to make a positive classification outcome. 

\subsection{Incentivizing Effort towards Desirable Features} 

We are interested in the \emph{properties} of the effort profile that the agent exerts as a result of best-responding to the principal's classifier, and in particular understanding the amount of effort they exert towards desirable features in set $\des$ and undesirable features $\und$. The goal is to promote effort towards desirable features and away from undesirable features, i.e. to understand \textit{when is it in the agent's best interest to invest more effort into desirable versus undesirable features?}. 

To do so, we define a notion of $\beta$-desirability, that measures the ratio of investment in features in $\des$ vs $\und$: 

\begin{defn}[$\beta$-desirable effort profiles]\label{defn:good}
Given $0 < \beta \leq 1$, an exogenous effort profile $\eff$ is said to be $\beta$-``desirable" if: 
\[
    \|\eff_\des\|_2 \geq \beta \|\eff\|_2, 
\]
i.e., the magnitude of effort made towards desirable features is at least a fraction $\beta$ of the magnitude of total effort. 
\end{defn}

From the principal's point of view, incentivizing $\beta$-desirable effort profiles is not straightforward since agents are strategic, and may prefer undesirable features if they are low-cost to manipulate. 
