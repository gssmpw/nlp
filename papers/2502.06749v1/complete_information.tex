\section{The Complete Information Setting} \label{sec:complete}

In the complete information setting, the agent knows precisely the true classifier $h_0$ deployed by the principal---equivalently, her prior $\Pi_h$ satisfies $\bh = h_0$ (the mean belief matches the true classifier) and the covariance is given by $\Sigma_h = \bf{0}$ (there is no uncertainty). She also fully knows the causal graph $\cG$---i.e., $\bw = w$ and $\Sigma_w = \bf{0}$. Therefore, in the complete information case, the deterministic tuple ($h_0, \C$) is enough to characterize agent beliefs. 

When the agent has no uncertainty about either the classifier or the causal graph, the agent's optimization problem ~\eqref{opt:agent_br} can be written as:
\begin{align*}
   \effopt (h_0, \C) = \arg \min_{\eff} &\quad \Cost(\eff) \\
    \text{s.t.}~~&\quad (\C h_0)^{\top}\eff \geq \alpha  \numberthis{\label{opt:agent_fullinfo}}.
\end{align*}
In other words, the agent must find the minimum-cost effort profile that passes the (known) classifier $h_0$. We first prove a technical result that helps further simplify the complete information setting. Roughly speaking, the proposition states that for the complete information case, it suffices to only focus on non-negative efforts for all features.

\begin{prop}\label{prop:y_pos}
For the cost functions defined in \eqref{eq:cost}, we can assume that: 
$(\C h_0)_f \geq 0$ and that $(e)_f \geq 0~~\forall f \in \cF$,  without loss of generality.
\end{prop}

The proof of the proposition can be found in Appendix~\ref{app:y_pos}. 



\paragraph{Computation of the Optimal Effort Profile.} Using Proposition \ref{prop:y_pos}, we can rewrite optimization problem~(\ref{opt:agent_fullinfo}) under the complete information setting as follows: 
\begin{align}\label{opt:full_info_final}
     \effopt(h_0, \C) = \arg \min_{\eff \geq 0} \quad \textsf{Cost}(\eff)  \quad s.t. \quad (\C h_0)^{\top}\eff \geq \alpha, 
\end{align}
where $\C h_0 \geq 0$. Program~(\ref{opt:full_info_final}) is a convex optimization problem: the objective is convex for our cost functions, and all constraints are linear. As such, this program can be solved efficiently. 

\subsection{Characterization of Optimal Effort Profiles}

We now investigate the structural properties of the optimal effort profile $\effopt$ for the cost function in Eq.~\eqref{eq:cost}, individually for the cases of i) $p = 1$ and ii) $p > 1$. 
The contributions of this section are two-fold: first, we show that the \emph{structure} of the optimal effort profile largely depends on the cost function that an agent optimizes over. In particular, we prove that in the case of the $\ell_1$ cost function, agents only invest effort into \emph{one} feature when best-responding to the classifier $h_0$ (Lemma~\ref{lem:linear_onefeature}). Instead, for general $\ell_p$ costs with $p > 1$, the agents' effort profile is \emph{significantly} more diversified and covers any non-trivial dimension i.e., one with contribution $(\C h_0)_f > 0$ (Lemma~\ref{lem:l2_effort}). Second, we derive conditions under which effort profiles that put a significant amount of weight on \emph{desirable} features are incentivized, providing insights as to how to set a classifier $h_0$ to incentivize effort exertion on \emph{desirable} features (Theorems~\ref{thm:l1_good} and ~\ref{thm:l2_good}). 


\subsubsection{Case 1 ($\ell_1$-norm costs)} 
For $p=1$, we have the following optimization problem for the agent: 
\begin{align}\label{opt:linear}
     \min_{\eff \geq 0} \quad c^{\top}\eff \quad \text{s.t.} \quad 
     (\C h_0)^{\top}\eff \geq \alpha. 
\end{align}

We first show that the case where $\alpha \leq 0$ is insignificant and hence we will only focus on $\alpha > 0$. Indeed, $\alpha \leq 0$ indicates that the agent has already passed the classifier (therefore has a non-positive distance from the decision boundary) and hence, they should not invest any effort into modifying features. Formally, 
\begin{prop}\label{prop:alpha_neg}
When $\alpha \leq 0$, then $\effopt = 0$ which means that the agent does not need to invest any effort to change her features. 
\end{prop}
\begin{proof}
Note that the objective value is greater than or equal to zero since $c_f > 0$ for all $f \in \cF$ and $\eff \geq 0$. But, since $\alpha \leq 0$, $\eff= 0$ is feasible to the above problem and it also achieves an objective value of $0$. Therefore, $\eff = 0$ must be optimal.  
\end{proof}

\noindent 
We next focus entirely on the case where $\alpha > 0$. Our first result is characterizing the structure of the agent's best response. 

\begin{lem}\label{lem:linear_onefeature}
When $\alpha > 0$, there exists an optimal effort profile for the agent in which she needs to modify \textbf{exactly one feature} to pass the classifier $h_0$. The optimal feature to modify $f^*$ is the one which offers the best ratio of contribution to cost, i.e., 
\[
     \fstar \in \arg\max_{f \in \cF}~\frac{(\C h_0)_f}{c_f}, 
\]
and the optimal amount of effort to be invested into that feature is given by: 
\[
     \eff_{\fstar} = \frac{\alpha}{(\C h_0)_{\fstar}}.
\]
\end{lem}


The proof of the lemma can be found in Appendix~\ref{app:linear_onefeature}.


\paragraph{Conditions for $\beta$-desirability.} The previous lemma provides key insights into the optimal effort profile of an agent. It shows that the agent has to invest effort into a single feature which offers her the best ``bang-per-buck". Let $\mathcal{I}^*$ be the set of all such features, i.e.:
\[
    \cI^* = \left\{\fstar:~\fstar \in \arg\max_{f \in \cF}\frac{(\C h_0)_f}{c_f} \right\}. 
\]
Therefore, by Definition~\ref{defn:good}, if $\cI^* \cap \und = \emptyset$, then the agent's best response is guaranteed to be a $\beta$-desirable effort profile. We now formalize this idea and present the main result of this section:
\begin{thm}\label{thm:l1_good}
If there exists a desirable feature $\fstar$ ($\fstar \in \des$) such that: 
\[
      \max_{f \in \mathcal{\und}} \frac{(\C h_0)_f}{c_f} < \frac{(\C h_0)_{\fstar}}{c_{\fstar}},
\]
then the agent's best response is always a $\beta$-desirable effort profile for any $\beta \in (0, 1]$. 
\end{thm}
\begin{proof}
The proof follows directly from Lemma~\ref{lem:linear_onefeature} and Definition~\ref{defn:good}.
\end{proof}

\subsubsection{Case 2 ($\ell_p$-norm costs for $p > 1$)}

For $p > 1$, the agent is solving the following optimization problem when best-responding:  
\begin{align}\label{opt:q_norm}
    \min_{\eff \geq 0} \quad \left(\sum_{f \in \cF}c_f (\eff_f)^p\right)^{1/p} \quad 
    \text{s.t.} \quad (\C h_0)^{\top}\eff \geq \alpha.
\end{align}
\begin{lem}\label{lem:l2_effort}
When the cost function is the weighted $\ell_p$-norm of the effort for $p > 1$, the optimal effort profile for the agent $\effopt$ satisfies: 
\[
        \effopt_f \propto \left( \frac{(\C h_0)_f}{c_f} \right)^{1/(p-1)} \quad \forall~f \in \cF.
%            \effopt = \frac{\alpha (\C h_0)}{||\C h_0||_2^2},
\]
\end{lem}


The proof can be found in Appendix~\ref{app:l2_effort}.



\paragraph{Conditions for $\beta$-desirability} Since we know the structure of the agent's optimal effort profile, we can identify conditions under which the best response is $\beta$-desirable. 
\begin{thm}\label{thm:l2_good}
For a $\ell_p$-norm cost function with $p > 1$, the agent's best response is always a $\beta$-desirable effort profile if:  
\[
    \left[ \sum_{f \in \des} \left( \frac{(\C h_0)_f}{c_f} \right)^{2/(p-1)} \right]^{1/2} \geq \frac{\beta}{\sqrt{1-\beta^2}}  \left[ \sum_{f \in \und} \left( \frac{(\C h_0)_f}{c_f} \right)^{2/(p-1)} \right]^{1/2}
\]
\end{thm}
\noindent
When $c = \bf{1}$ and $p = 2$, the condition reduces to: 
\[
       \lVert(\C h_0)_{\des}\rVert_2 \geq \frac{\beta}{\sqrt{1-\beta^2}}\lVert(\C h_0)_{\und}\rVert_2,
\]
In other words, in the complete information setting and when agents have $\ell_2$ cost functions, if the magnitude of the net contribution per unit cost along the desirable features relative to the undesirable features is high enough, then it is always in the agent's best interest to invest more in desirable features. 

\subsection{Desirable Classifiers and Where to Find Them}

So far, we have provided conditions which help the principal answer the following problem: ``does a given classifier $h_0$ induce strategic agents to invest only in $\beta$-desirable effort profiles?'' This means that given a classifier $h_0$, we can check that the classifier incentivizes a good profile. However, this does not answer the question of developing algorithms for \emph{finding} such a profile. We start this section with a negative result: the set of $\beta$-desirable classifiers, is, in general, non-convex (Lemma~\ref{lem:comp_des_nonconvex})---in particular, there are typically no well-known methods for finding a good effort profile in high dimensions. However, we investigate a simple condition under which the problem becomes convex (Proposition~\ref{prop:convex_p13}), and we also provide a heuristic (using convexification) that ensures that not too much effort is spent on undesirable features (Proposition~\ref{prop:convex_relax}). The benefit of having a convex design space of desirable classifiers is that i) we can find a desirable classifier using standard optimization techniques, and ii) it allows the principal to choose a classifier that \emph{simultaneously} minimizes (convex) accuracy losses and induces desirable effort profiles.

\paragraph{The Space of Desirable Classifiers is Non-Convex.}
Our first main result shows that finding desirable classifiers is a non-convex problem, hence effectively computationally \textit{hard} in general. 

\begin{lem}\label{lem:comp_des_nonconvex}
There exists an instance of the problem $(\C,h_0)$ and $\beta > 0$, where the space of $\beta$-desirable classifiers $\cH$ is non-convex.
\end{lem}


The proof of the lemma is provided in Appendix~\ref{app:comp_des_nonconvex}. In fact, whenever there is more than one desirable feature, i.e., $|\des| > 1$, $\cH$ can be shown to be a non-convex set. 



Since the space of desirable classifiers is, in the worst case, non-convex, finding the best classifier in this set (that maximizes some classification accuracy metric) is equivalent to solving a non-convex optimization problem to global optimality, which is typically an NP-hard problem. 

\paragraph{Special Case: When the Learner Focuses on Incentivizing a Single Desirable Feature.} We now highlight a special case where the space of $\beta$-desirable classifiers is \textit{convex}, for any $\beta > 0$ for a specific range of $p$ values. Namely, we focus on the special case of the principal having a \emph{single} desirable feature that they wish to target. Note that this assumption is actually aligned with what we expect to see happening in real life: indeed, a principal may define for themselves which feature they want to incentivize, and focus on one feature where they would really like to see improvements, especially if this is a feature that has historically not been properly leveraged. Not only that, but by targeting a single feature, they may lower the agents' cognitive load for best-responding, which is always desirable in practice. 

\begin{prop}\label{prop:convex_p13}
Suppose that there is only a single desirable feature, i.e., that $|\des| = 1$. Then for any $\beta > 0$, the space of $\beta$-desirable classifiers $\cH$ is convex for any $\ell_p$-norm cost function with $p \in [1, 3]$.
\end{prop}


The proof can be found in Appendix~\ref{app:convex_p13}.


\paragraph{Minimizing Undesirable Features.}  When $|\des| > 1$, we know that in general, the set $\cH$ of $\beta$-desirable classifiers is not convex, and difficult to optimize over. However, we propose a convexification heuristic (parameterized by $\gamma$) where the principal just tries to design a classifier such that ``the total contribution of undesirable features is no more than $\gamma$'':

\begin{prop}\label{prop:convex_relax}
Let $\cH_{w(\und) \leq \gamma} = \{h_0:~ \| (\C h_0)_{\und}\|_{2/(p-1)} \leq \gamma\}$. Then for any $\gamma > 0$, $\cH_{w(\und) \leq \gamma}$ is convex for any $\ell_p$-norm cost function with $p \in [1, 3]$.
\end{prop}


The proof is nearly identical to that of Proposition~\ref{prop:convex_p13} and is omitted to avoid repetition. This result helps the principal guarantee that they can bound the effort exerted on undesirable features, even if they are not able to guarantee a certain target level of $\beta$-desirable effort.











  


    






