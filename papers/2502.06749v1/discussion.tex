\section{Discussion}\label{sec:discussion}
In this paper, we adopt a causal perspective to the problem of strategic classification. The principal deploys a linear classifier which classifies agents ``positive" or ``negative" based on a set of features embedded on a \textit{causal graph}. Since agents are strategic, they are expected to invest effort ``cleverly" to modify their features in the hopes of a positive classification outcome while incurring the minimum possible cost. 
Therefore, understanding how agents respond when they have different levels of access to information about the deployed classifier and the causal graph, is significant to the principal from the perspective of classifier design. The principal's goal is the following: design a classifier which incentivizes agents to invest in \textit{desirable} effort profiles. 

The main contributions of our paper are two-fold: i) We characterize the design space of \textit{desirable} classifiers for the broad class of weighted $\ell_p$-norm agent cost functions under complete information, while clearly demonstrating computational challenges of finding such classifiers. We also identify special settings and relaxations which can render the design problem computationally tractable. ii) We try to understand strategic agent behavior in response to classifiers under incomplete information settings. In particular, we show that under totally incomplete information (uncertainty over both the classifier and the causal graph), finding the agent's best response is computationally difficult. However it becomes tractable under partially incomplete information where there is uncertainty over either the classifier or the causal graph and we provide insights about the structural properties of the agent's best response under this setting. Finally, we use these results to gain some useful insights (through numerical experiments) into the question of how to design \textit{desirable} classifiers, even under incomplete information. 

There are many avenues of future work. Our model of uncertainty involves agents having Gaussian priors over the classifier or the edge weights of the causal graph or both, under the assumption that the causal graph topology is always fully known. In real life, there may be other forms of uncertainty --- for example, when there are a large number of features, it may be unreasonable to assume that agents have complete knowledge about the causal relationships between features. It may also be interesting to explore if there are other kinds of information structures which are more interpretable --- for example, instead of priors independently on the classifier and the causal graph, agents have access to an ordering (or partial ordering) on features in terms of their relative importance. This information structure subsumes necessary information from both the classifier and the causal graph but is easier to understand and hence, might be easier to respond to. Our work also has interesting extensions in the domain of fairness. Different populations may have different levels of uncertainty in their priors which might lead to markedly different abilities of each of those groups to respond to the principal's classifier --- the downstream fairness in classification of such information asymmetry may be worth exploring.  