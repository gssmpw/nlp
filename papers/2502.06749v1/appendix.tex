\newpage
\section{Additional Details for Experimental Section~\ref{sec:experiments}}\label{sec:app_addn_exp}
\subsection*{Supplementary Tables}
\begin{table}[h!]
\centering
\begin{tabular}{|c|| c|c|c|c || c|c|c|c||}
\hline 
Features & Alcohol & Diet & Activity & Smoking & DM & HPL & HPT & Obesity
\\
\hline 
\hline 
Alcohol & 0 & 0 & 0 & 0 & 0.10 & 0.14 & 0.62 & 0.64 \\
\hline 
Diet & 0 & 0 & 0 & 0 & 0.84 & 0.84 & 0.84 & 0.86 \\
\hline 
Activity & 0 & 0 & 0 & 0 & 0.82 & 0.82 & 0.82 & 0.82 \\
\hline 
Smoking & 0 & 0 & 0 & 0 & 0.52 & 0.34 & 0.86 & 0 \\
\hline
DM & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
\hline
HPL & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
\hline
HPT & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
\hline
Obesity & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
\hline
\end{tabular}
\caption{Adjacency matrix $A$ which captures the edge weights of the causal graph in Figure~\ref{fig:causal_graph}}\label{table:weights}
\end{table}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\section{Proofs for the Complete Information Setting}

\subsection{Proof of Proposition~\ref{prop:y_pos}}\label{app:y_pos}
Let $\effopt$ be the optimal effort profile for the agent, i.e., the profile that corresponds to the solution of \ref{opt:agent_fullinfo}. First, note that for any feature $f \in \cF$, $(\C h_0)_f = 0$ implies $\effopt_f = 0$. Indeed, if feature $f$ has no contribution towards the classification decision, then no effort should be expended on $f$ in the optimal effort profile. 

Now, we can focus on features for which $(\C h_0)_f \neq 0$. When $(\C h_0)_f < 0$, we will show that $\effopt_f \leq 0$. Suppose that $\effopt_f > 0$. In this case, we can construct a new effort profile $\eff'$ as follows: $\eff'_f = 0$ and $\eff'_g = \effopt_g$ for all $g \in \cF, g \neq f$. It is easy to see that $\eff'$ is still feasible but $\textsf{Cost}(\eff') < \textsf{Cost}(\effopt)$ which contradicts the fact that $\effopt$ is the optimal solution. Therefore, $\effopt_f \leq 0$. Similarly, we can show that when $(\C h_0)_f > 0$, $\effopt_f \geq 0$. 

The above discussion implies that whenever $(\C h_0)_f \neq 0$, then $(\C h_0)_f \effopt_f \geq 0$ - therefore, without loss of generality, it suffices to assume $(\C h_0)_f > 0$ and only search over the space $\eff \geq 0$ (because the optimal solution $\effopt \geq 0$). This concludes the proof.      


\subsection{Proof of Lemma~\ref{lem:linear_onefeature}}\label{app:linear_onefeature}

The optimization problem in \eqref{opt:linear} (which we will call the primal problem $P$) is a linear program whose feasible region is given by the following polyhedron $Q = \left\{\eff \in \R^{n+k}: (\C h_0)^{\top}\eff \geq \alpha, \eff \geq 0\right\}$. Our first goal is to argue that the optimal solution is a corner point of $Q$ which requires us to prove the following: i) firstly, $Q$ has at least one corner point, and ii) the optimal solution is bounded which would imply that it must be at a corner point of $Q$. Note that the polyhedron $Q$ has no line (because it is a subset of the positive orthant) and therefore, it must have at least one corner point\footnote{for more details on the polyhedral theory related to linear optimization, refer to \cite{bertsimas}}. We can now write the dual problem ($D$) as follows:
\begin{align*}
    \max_{\pi} \quad &\alpha \pi \quad s.t. \quad \quad (D)\\
    & \pi \cdot (\C h_0) \leq c    \quad (\eff)\\
    & \pi \geq 0. 
\end{align*}
We know that the dual problem ($D$) is feasible ($\pi = 0$ is feasible to $D$) which implies that the optimal solution to ($P$) cannot be unbounded. Hence, we conclude that there must exist a corner point optimal solution to problem ($P$). Now, note that all corner points of $Q$ are obtained by the intersection of the hyperplane $(\C h_0)^{\top}\eff \geq \alpha$ with the positive axes. So any corner point of $Q$ must be of the form where exactly one entry corresponding to some feature $f$ is positive (i.e., takes value $\frac{\alpha}{(\C h_0)_f}$) and all other entries are zero. This implies that there exists an optimal effort profile where the agent needs to modify exactly one feature, proving the first part of the lemma. 

For proving the second part, we will use the complementary slackness conditions on the dual constraints. We already know that there exists an optimal primal solution where there is some feature $\fstar$ with $\eff_{\fstar} = \frac{\alpha}{(\C h_0)_{\fstar}}$ and $\eff_{f'} = 0$ for all $f' \neq \fstar$. Let $\pistar$ be the optimal dual solution. Using complementary slackness, we know that $\pistar \cdot (\C h_0)_{\fstar} = c_{\fstar}$ which implies that: 
\[
        \pistar = \frac{c_{\fstar}}{(\C h_0)_{\fstar}}. 
\]
Since $\pistar$ must also be feasible to ($D$), we must have: 
\[
       \pistar \leq \frac{c_{f'}}{(\C h_0)_{f'}} \quad \forall~f' \neq \fstar,~(\C h_0)_{f'} \neq 0,  
\]
which implies that: 
\[
        \fstar \in \arg\max_{f \in \cF} \frac{(\C h_0)_f}{c_f}.  
\]
This concludes the proof of the lemma. 



\subsection{Proof of Lemma~\ref{lem:l2_effort}}\label{app:l2_effort}

We solve the constrained optimization problem using Lagrange multipliers. Define the Lagrangian as follows:
\[
    \mathcal{L}(\eff, \pi) = \left(\sum_{f \in \cF}c_f (\eff_f)^p\right)^{1/p} +\pi \left(- (\C h_0)^{\top}\eff + \alpha \right),
\]
where $\pi$ is the Lagrange multiplier associated with the constraint as defined earlier. This gives us the following set of KKT conditions:
\begin{align*}
    &\nabla_{\eff} \mathcal{L}(\eff, \pi) = 0, \\
    &\pi \cdot (-(\C h_0)^{\top}\eff + \alpha ) = 0, \\
    &\pi \geq 0, \\
    &\alpha - (\C h_0)^{\top}\eff \leq 0, ~\eff \geq 0. 
\end{align*}
Since our optimization problem is convex, it suffices to find a pair $\left(\effopt, \pistar \right)$ that satisfies the KKT conditions and we can automatically conclude that $\effopt$ is optimal to the primal problem.

First, we show that the constraint $(\C h_0)^{\top}\eff \geq \alpha$ must be active at the optimal solution. We prove this by contradiction. 
Suppose, if possible that $-(\C h_0)^{\top}\effopt + \alpha < 0$. However, this means that we can obtain the optimal solution $\effopt$ by solving the primal problem as if it were unconstrained. In that case, it must be that $\effopt = 0$, but observe that $\eff = 0$ is not even feasible (and hence cannot be optimal). This implies that the constraint must hold at equality. Therefore, we can solve for $\effopt$ and $\pi^*$ by solving the following system:
\begin{align*}
    -(\C h_0)^{\top}\eff + \alpha &= 0,\\
    \nabla_e \mathcal{L}(\eff, \pi) &= 0.
\end{align*}
Now, 
\[
    \left(\nabla_{\eff} \mathcal{L}(\eff, \pi) \right)_f = \frac{\partial \mathcal{L}}{\partial \eff_f} = \frac{{c_f (\eff_f)}^{p-1}}{ \left[ \left(\sum \limits_{f \in \cF}c_f (\eff_f)^p \right)^{1/p} \right]^{p-1} } - \pi \cdot \left( \C h_0 \right)_f.
\]
We have already argued that $\effopt \neq 0$. Therefore, $\pistar > 0$. This implies that for all features $f \in \cF$, whenever $(\C h_0)_f > 0$, we must have: 
\[
        \effopt_f \propto \left(\frac{(\C h_0)_f}{c_f}\right)^{1/(p-1)}, 
\]
and when $(\C h_0)_f = 0$, the condition holds trivially. This concludes the proof of the lemma. 


\subsection{Proof of Lemma~\ref{lem:comp_des_nonconvex}}\label{app:comp_des_nonconvex}

The set of $\beta$-desirable classifiers $\cH$ is given as follows: 
\begin{align*}
    \cH := \left\{h_0 \in \R^{|\cF|}: \effopt(h_0, \C) \text{ is $\beta$-desirable}, \C h_0 \geq 0 \right\}.
\end{align*}
We define the set $\cZ := \left\{ (\C h_0): h_0 \in \cH \right\}$.
Suppose that $\C$ is full row-rank. This implies that $\cH$ is convex if and only if $\cZ$ is convex\footnote{This is a standard result in linear algebra; the proof provided in Appendix~\ref{sec:app_sup} for completeness}. Therefore, in order to complete the proof, it suffices to show that the transformed set $\cZ$ is non-convex in the worst case. We now provide instances of problems where $\cZ$ is non-convex and the agents incur $\ell_p$-norm cost functions with $p = 1$ and $p > 1$. 
Recall from Theorems~\ref{thm:l1_good} and \ref{thm:l2_good} that the set $\cZ$ is given as follows: 
\[
    \cZ =\left\{ z \in \R_{\geq 0}^{|\cF|}: \max_{f \in \und} \frac{z_f}{c_f} < \max_{f \in \des}\frac{z_f}{c_f} \right\} \quad \text{($p = 1$)}
\]
\[
    \cZ = \left\{ z \in \R_{\geq 0}^{|\cF|}:  \left[ \sum_{f \in \des} \left( \frac{z_f}{c_f} \right)^{2/(p-1)} \right]^{1/2} \geq \frac{\beta}{\sqrt{1-\beta^2}}  \left[ \sum_{f \in \und} \left( \frac{z_f}{c_f} \right)^{2/(p-1)} \right]^{1/2}  \right\} \quad \text{($p > 1$)}
\]

\paragraph{Weighted $\ell_1$-norm cost function:} Consider a setting where there are $4$ features with $\des = \left\{1,2\right\}$ and $\und = \left\{3,4\right\}$. Suppose that the cost vector equals $c = \bf{1}$. In this case, 
\[
       \cZ = \left\{z \in \R_{\geq 0}^4: \max(z_3, z_4) < \max(z_1, z_2)\right\}.
\]
Now, choose $z' :=(4, 7, 3, 6)$ and $z'':= (7, 4, 3, 6)$. Both are clearly points in $\cZ$. However, for $\alpha = 0.5$, $\alpha z' + (1-\alpha)z'':= (5.5, 5.5, 3, 6) \notin \cZ$. Therefore, $\cZ$ is not a convex set.  

\paragraph{Weighted $\ell_p$-norm cost function:} Consider a setting where there are $3$ features with $\des = \left\{1,2\right\}$ and $\und = \left\{3\right\}$. Let $p = 2$, $c = \bf{1}$ and $\beta = \frac{1}{\sqrt{2}}$. Then $\cZ$ is given by:
\[
    \cZ = \left\{z \in \R_{\geq 0}^3: \sqrt{z_1^2 + z_2^2} \geq z_3 \right\}
\]
$(0, 1, 1)$ and $(1, 0, 1)$ are points in $\cZ$, but the point halfway between them, given by $(0.5, 0.5, 1)$ is clearly not in $\cZ$. Therefore, $\cZ$ is not a convex set. This concludes the proof.  

\subsection{Proof of Proposition~\ref{prop:convex_p13}}\label{app:convex_p13}

We will verify convexity separately for the cases with $\ell_1$-norm and $\ell_p$-norm ($1 < p \leq 3$) cost functions. \smallskip

\noindent\emph{The $\ell_1$-norm case:} When the cost function is a weighted $\ell_1$-norm, the set of desirable classifiers is given by %$
\[
     \cH := \left\{h_0 \in \R^{|\cF|}: \C h_0 \geq 0, \max_{f \in \und}\frac{(\C h_0)_f}{c_f} < \max_{f \in \des}\frac{(\C h_0)_f}{c_f} \right\}.
\]
Now, suppose $|\des| = 1$ and there is some feature $f_d \in \des$. 
Then, we can rewrite $\cH$ as follows: 
\[
     \cH := \left\{h_0 \in \R^{|\cF|}: \C h_0 \geq 0, \max_{f \in \cF \setminus \left\{f_d\right\}} \frac{(\C h_0)_f}{c_f} - \frac{(\C h_0)_{f_d}}{c_{f_d}} < 0  \right\}. 
\]
In order to show that $\cH$ is a convex set, it suffices to show that the function $g(h_0) = \max_{f \in \cF \setminus \left\{f_d\right\}} \frac{(\C h_0)_f}{c_f} - \frac{(\C h_0)_{f_d}}{c_{f_d}}$ is a convex function. Function $g(\cdot)$ corresponds to the sum of a maximum of linear functions (which is convex) and a linear function; hence, function $g(\cdot)$ is convex.

\smallskip
\noindent\emph{The $\ell_p$-norm case with $p > 1$:} For $\ell_p$-norm cost functions with $p > 1$, set $\cH$ is given by:
\[
    \cH \triangleq \left\{ h_0 \in \R^{|\cF|}: \C h_0 \geq 0,  \left[ \sum_{f \in \des} \left( \frac{(\C h_0)_f}{c_f} \right)^{2/(p-1)} \right]^{1/2} \geq \frac{\beta}{\sqrt{1-\beta^2}}  \left[ \sum_{f \in \und} \left( \frac{(\C h_0)_f}{c_f} \right)^{2/(p-1)} \right]^{1/2}  \right\}
\]
Using the fact that $|\des| = 1$, we can rewrite $\cH$ as follows: 
\[
      \cH := \left\{h_0 \in \R^{|\cF|}: \C h_0 \geq 0, (\C h_0)_{f_d} \geq K \left[ \sum_{f \in \und} \left( \frac{(\C h_0)_f}{c_f} \right)^{2/(p-1)} \right]^{(p-1)/2}  \right\}
\]
where $K = c_{f_d} \left(\frac{\beta}{\sqrt{1-\beta^2}} \right)^{(p-1)} > 0$. 
Now in order to complete the proof, we need to show that the function $r(h_0)$ is convex, where $r(h_0)$ is given by: 
\[
    r(h_0) = K \left[ \sum_{f \in \und} \left( \frac{(\C h_0)_f}{c_f} \right)^{2/(p-1)} \right]^{(p-1)/2} - (\C h_0)_{f_d}.
\]
When $1 < p \leq 3$, we can rewrite $r(h_0)$ as follows: 
\[
    r(h_0) = K \|Bh_0 \|_{2/(p-1)} - (\C h_0)_{f_d},
\]
where $B \in \R^{(|\cF|-1) \times (|\cF|-1)}$. Note that $K \| Bh_0 \|_{2/(p-1)}$ is a convex function in $h_0$ since this is a $q$-norm for $q = \frac{2}{p-1} \geq 1$. This makes $r(h_0)$ a convex function in $h_0$ (sum of a convex function and a linear function is convex) and concludes the proof.  




%
%

%
%

%
%
%
%

%
%
%

%
%
%

%%


\section{Proofs for the Incomplete Information Setting}

\subsection{Proof of Lemma~\ref{lem:partial_incomp_convex}}
\label{app:partial_incomp_convex}

Since the cost functions defined in Eq.~\eqref{eq:cost} are convex, in order to complete the proof, it suffices to show that the feasible space of the optimization problem in~\eqref{opt:incomp_gaussian}, is convex. We have already argued that under incomplete information models $(1)$ and $(2)$, $\C h$ is a multivariate Gaussian, i.e, $\C h \sim \mathcal{N}(\mu_{\C h}, \Sigma_{\C h})$ for some $\mu_{\C h} \in \R^{|\cF|}$ and $\Sigma_{\C h} \in \R^{|\cF|\times |\cF|}$. This implies, 
\[
       (\C h)^{\top}\eff \sim \mathcal{N}\left(\mu_{\C h}^{\top}\eff, \eff^{\top}\Sigma_{\C h} \eff \right). 
\]
This allows us to rewrite the LHS of the probability constraint as follows: 
\begin{align*}
    \bP\left[(\C h)^{\top}\eff \geq \alpha \right] &= \bP\left[ \frac{(\C h)^{\top}\eff - \mu_{\C h}^{\top}\eff}{\sqrt{\eff^{\top}\Sigma_{\C h} \eff}} \geq \frac{\alpha - \mu_{\C h}^{\top}\eff}{\sqrt{\eff^{\top}\Sigma_{\C h} \eff}} \right] \\
    &= \bP\left[Z \geq \frac{\alpha - \mu_{\C h}^{\top}\eff}{\sqrt{\eff^{\top}\Sigma_{\C h} \eff}} \right] \quad \text{(where $Z \sim \mathcal{N}(0, 1)$)} \\
    &= \Phi^c \left( \frac{\alpha - \mu_{\C h}^{\top}\eff}{\sqrt{\eff^{\top}\Sigma_{\C h} \eff}} \right). 
\end{align*}
Therefore, 
\begin{align*}
    \bP\left[(\C h)^{\top}\eff \geq \alpha \right] \geq 1-\delta &\iff \Phi^c \left( \frac{\alpha - \mu_{\C h}^{\top}\eff}{\sqrt{\eff^{\top}\Sigma_{\C h} \eff}} \right) \geq 1-\delta \\
    &\iff \Phi\left( \frac{\alpha - \mu_{\C h}^{\top}\eff}{\sqrt{\eff^{\top}\Sigma_{\C h} \eff}} \right) \leq \delta \\
    &\iff \frac{\alpha - \mu_{\C h}^{\top}\eff}{\sqrt{\eff^{\top}\Sigma_{\C h} \eff}} \leq p_{\delta}   \quad \text{(where $p_{\delta} = \Phi^{-1}(\delta)$)} \\
    &\iff \alpha - \mu_{\C h}^{\top}\eff - p_{\delta}\cdot \sqrt{\eff^{\top}\Sigma_{\C h} \eff} \leq 0.
\end{align*}
When $\delta = \frac{1}{2}$, $p_{\delta} = 0$ and the above constraint reduces to a polyhedral constraint, making the problem trivially convex. On the other hand, note that when $\delta < \frac{1}{2}$, $p_{\delta} < 0$. Now, since $\Sigma_{\C h}$ is a covariance matrix, it is always symmetric and positive semidefinite and therefore, $\Sigma_{\C h}^{1/2}$ exists (it is also symmetric and positive semidefinite!). In that case, we can express $\sqrt{\eff^{\top}\Sigma_{\C h} \eff}$ as follows: 
\[
    \sqrt{\eff^{\top}\Sigma_{\C h} \eff} = \sqrt{ \eff^{\top}\Sigma_{\C h}^{1/2}\Sigma_{\C h}^{1/2}\eff } = \sqrt{(\Sigma_{\C h}^{1/2}\eff)^{\top}(\Sigma_{\C h}^{1/2}\eff)} = \sqrt{||\Sigma_{\C h}^{1/2}\eff ||_2^2} = ||\Sigma_{\C h}^{1/2}\eff||_2. 
\]
Now, $||\Sigma^{1/2}\eff||_2$ is a convex function in $\eff$ (because all norms are convex functions). Similarly, $-p_{\delta}\cdot ||\Sigma_{\C h}^{1/2}\eff||_2$ is also a convex function because $-p_{\delta} > 0$. The term $\alpha -\mu_{\C h}^{\top}\eff$ is affine in $\eff$ and therefore, convex by default. Putting everything together, we conclude that $\alpha - \mu_{\C h}^{\top}\eff - p_{\delta}\cdot \sqrt{\eff^{\top}\Sigma_{\C h} \eff}$ is a convex function in $\eff$ which makes the constraint: 
\[
     \alpha - \mu_{\C h}^{\top}\eff - p_{\delta}\cdot \sqrt{\eff^{\top}\Sigma_{\C h} \eff} \leq 0
\]
a convex constraint. This concludes the proof of the proposition. 

\subsection{Proof of Proposition~\ref{prop:full_uncert_nonconvex}}\label{app:full_uncert_nonconvex}

In order to complete this proof, it suffices to provide a counter-example where the program in \eqref{opt:agent_br_incomp} is non-convex. Consider the simplest possible setting where there can be uncertainty in both the classifier and the causal graph. Suppose, there is only one feature, i.e., $|\cF| = 1$. Let $\omega \sim \mathcal{N}(0, 1)$ be the random variable that captures the uncertainty in the contribution of the feature (encodes uncertainty in the causal graph) and $h \sim \mathcal{N}(0, 1)$ be the random variable that captures the uncertainty in the classifier weight on the feature. Note that $\omega \perp h$. We will show that the feasible space given by:
\[
       \bP\left[ (\omega h) \eff \geq \alpha \right] \geq 1-\delta
\]
is non-convex, which is equivalent to showing that the function $f(\eff)$ given by:
\[
     f(\eff) = \bP\left[ (\omega h) \eff \geq \alpha \right]
\]
is not concave. Below in Figure~\ref{fig:non_convex_counter}, we plot $f(\eff)$ as a function of one-dimensional effort $\eff$. Since there is no closed-form expression for the distribution of the product of two independent standard normal random variables, we obtain empirical estimates for the probability at each $\eff$ using Monte-Carlo simulations. Clearly, $f(\eff)$ is not concave. 
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.5\linewidth]{Figures/nonconvex.png}
    \caption{Plot of $f(\eff)$ with $\alpha = 1$}
    \label{fig:non_convex_counter}
\end{figure}
This concludes the proof of the proposition. 

\subsection{Proof of Lemma~\ref{lem:delta_charac}}\label{app:delta_charac}

Recall that the feasible space of the agent's optimization problem in the partially incomplete information case is given by: 
\[
     \alpha - \mu_{\C h}^{\top}\eff - p_{\delta}\cdot \|\Sigma_{\C h}^{1/2}\eff \|_2 \leq 0.
\]
This feasible space is empty at a given $\delta$ if we have: 
\[
    \alpha - \mu_{\C h}^{\top}\eff - p_{\delta}\cdot \|\Sigma_{\C h}^{1/2}\eff \|_2 > 0 \quad \forall~\eff\iff \min_{\eff} \quad g(\eff) = \alpha - \mu_{\C h}^{\top}\eff - p_{\delta}\cdot \|\Sigma_{\C h}^{1/2}\eff \|_2 > 0. 
\]
Now consider the convex unconstrained optimization problem: $\min_{\eff}~g(\eff)$. Then, 
\[
   \nabla g(\eff) = -\mu_{\C h} - p_{\delta}\cdot \frac{\Sigma_{\C h}\eff}{\| \Sigma_{\C h}^{1/2}\eff \|_2}. 
\]
Now there are $2$ cases: 
\noindent\emph{$\nabla g(\eff) = 0$ has a solution $\hat \eff$:} Clearly, $\hat \eff \neq 0$ because the gradient is not defined at $\eff = 0$. In that case, $\hat \eff$ satisfies: 
    \[
          - p_{\delta}\cdot \frac{\Sigma_{\C h}\hat \eff}{\| \Sigma_{\C h}^{1/2}\hat \eff \|_2} = \mu_{\C h}.  
    \]
    Now, $\hat \eff$ must be a global minimizer of $g$ because $g(\cdot)$ is a convex function. We will show that $g(\hat \eff) = \alpha$: 
    \begin{align*}
        g(\hat \eff) &= \alpha - \mu_{\C h}^{\top}\hat \eff - p_{\delta}\cdot \| \Sigma_{\C h}^{1/2}\hat \eff \|_2 \\
        &= \alpha + p_{\delta}\cdot \frac{\hat \eff^{\top}\Sigma_{\C h}\hat \eff }{\| \Sigma_{\C h}^{1/2}\hat \eff \|_2} - p_{\delta}\cdot \| \Sigma_{\C h}^{1/2}\hat \eff \|_2 \quad \text{(using the condition from $\nabla g(\hat \eff) = 0$)}\\
        &= \alpha + p_{\delta}\cdot \| \Sigma_{\C h}^{1/2}\hat \eff \|_2 - p_{\delta}\cdot \| \Sigma_{\C h}^{1/2}\hat \eff \|_2 \\
        &= \alpha. 
    \end{align*}
    Since $\alpha > 0$, the problem is always infeasible for this particular value of $p_{\delta}$. Since $\Sigma_{\C h}$ is positive definite, $\Sigma_{\C h}^{-1/2}$ exists, therefore we have: 
    \[
         p_{\delta} = - \| \Sigma_{\C h}^{-1/2} \mu_{\C h} \|_2 \iff \delta = \Phi^{-1}\left( - \| \Sigma_{\C h}^{-1/2} \mu_{\C h} \|_2 \right). 
    \]

\noindent\emph{$\nabla g(\eff) = 0$ has no solution:} This means that either the unique optimal solution is at the point where the gradient does not exist, i.e, $\eff = 0$, or the solution is unbounded. The first subcase clearly leads to infeasibility as $g(0) = \alpha > 0$ while the second sub-case leads to a non-empty feasible region for Problem \eqref{opt:incomp_gaussian}. We will now try to derive conditions on $\delta$ which lead to each subcase. 

    Suppose that the unique optimal solution is $\eff = 0$. This means that for any direction $d$, $g(0 + d) > g(0)$ or equivalently, 
    \begin{align*}
         \alpha - \mu_{\C h}^{\top}d - p_{\delta}\cdot \| \Sigma_{\C h}^{1/2}d \|_2 > \alpha \quad \forall~d 
         \iff -p_{\delta} \cdot \| \Sigma_{\C h}^{1/2}d \|_2 > \mu_{\C h}^{\top}d \quad \forall~ d.
    \end{align*}
    Note that $\mu_{\C h}^{\top}d = \mu_{\C h}^{\top}\Sigma_{\C h}^{-1/2}\Sigma_{\C h}^{1/2}d = (\Sigma_{\C h}^{-1/2}\mu_{\C h})^{\top}(\Sigma_{\C h}^{1/2}d) \leq \| \Sigma_{\C h}^{-1/2}\mu_{\C h} \|_2 \cdot \|\Sigma_{\C h}^{1/2}d \|_2$ where the last inequality follows from the Cauchy-Schwartz inequality. In fact, for $d^* = \Sigma_{\C h}^{-1}\mu_{\C h}$ (which exists since $\Sigma_{\C h}$ is positive definite and hence, invertible), we have equality. But since $-p_{\delta} \cdot \| \Sigma_{\C h}^{1/2}d \|_2 > \mu_{\C h}^{\top}d$ for all directions $d$, it must hold for $d^*$ as well, which implies: 
    \[
         \| \Sigma_{\C h}^{-1/2}\mu_{\C h} \|_2 \cdot \|\Sigma_{\C h}^{1/2}d^* \|_2 < -p_{\delta} \cdot \|\Sigma_{\C h}^{1/2}d^* \|_2,
    \]
    which means that $-p_{\delta} >  \| \Sigma_{\C h}^{-1/2}\mu_{\C h} \|_2$ or equivalently, $\delta < \Phi^{-1}\left( - \| \Sigma_{\C h}^{-1/2} \mu_{\C h} \|_2 \right)$. 

    Similarly, if the solution is unbounded, there must exist a direction $d'$ at $0$ such that:
    \[
        \alpha - \mu_{\C h}^{\top}d' - p_{\delta}\cdot \| \Sigma_{\C h}^{1/2}d' \|_2 < \alpha, 
    \]
    or equivalently, $-p_{\delta} \cdot \| \Sigma_{\C h}^{1/2}d' \|_2 < \mu_{\C h}^{\top}d'$. Using a similar argument as above, we can show that this can happen only when: 
    \[
         \delta > \Phi^{-1}\left( - \| \Sigma_{\C h}^{-1/2} \mu_{\C h} \|_2 \right).
    \]
\noindent
This concludes the proof of the lemma. 

\subsection{Proof of Lemma~\ref{lem:l1_incomp}}\label{app:l1_incomp}
In order to complete the proof, it suffices to construct an instance of the problem where the optimal effort profile is not a corner point. Consider a setting where $|\cF| = 2$, $\alpha > 0$ and $\delta < \frac{1}{2}$.  Suppose, the features are identical in all respects, i.e., $(\mu_{\C h})_1 = (\mu_{\C h})_2 = \bar \mu > 0$, $\Sigma_{\C h} = \begin{bmatrix}\sigma^2 & 0\\0 & \sigma^2\end{bmatrix}$ and $c_1 = c_2 = c$. Additionally, suppose that $\bar \mu > -p_{\delta}\sigma$. We first make the following observations: 
\begin{itemize}
    \item If $\effopt$ is not a corner point, it must be symmetric, i.e., $\effopt_1 = \effopt_2$. 
    \item Since $\bar \mu > 0$ and $\delta < \frac{1}{2}$, it must be that $\effopt \geq 0$ (otherwise, we have infeasibility).  
\end{itemize}
Now, there are only two possible corner point solutions: either of the form $(\eff, 0)$ or $(0, \eff)$. In order for either of them to be optimal, the constraint must be active at that point. Solving, we obtain: 
\[
      \eff = \frac{\alpha}{\bar \mu + p_{\delta}\sigma}; \quad \text{and} \quad \textsf{Cost} = \frac{c\alpha}{\bar \mu + p_{\delta}\sigma}. 
\]
However, we will now construct a non-corner point solution $(\eff', \eff')$ where the constraint is active and which produces a strictly better objective value. Solving, we obtain:
\[
    \eff' = \frac{\alpha}{2\left( \bar \mu + \frac{p_{\delta}}{\sqrt{2}}\cdot \sigma\right)}; \quad \text{and} \quad \textsf{Cost} = \frac{c\alpha}{\left( \bar\mu + \frac{p_{\delta}}{\sqrt{2}}\cdot \sigma\right)}, 
\]
which is strictly smaller than the earlier cost (since $p_{\delta} < 0$). This concludes the proof. 

\subsection{Proof of Theorem~\ref{thm:incomp_l2}}\label{app:incomp_l2}

We will use the KKT conditions to obtain the agent's optimal effort profile $\effopt$.
The Lagrangian $\mathcal{L}(\cdot, \cdot)$ for the above problem is given by:
\begin{align*}
    \mathcal{L}(\eff, \lambda) = ||\eff||_2 + \lambda \left(\alpha - \mu_{\C h}^{\top}\eff - p_{\delta} \cdot ||\Sigma_{\C h}^{1/2}\eff||_2\right), 
\end{align*}
where $\lambda$ is the Lagrange multiplier. 
We can now write the KKT conditions as follows: 
\begin{align*}  
    & \frac{\eff}{||\eff||_2} + \lambda \cdot \left( -p_{\delta}\cdot \frac{\Sigma_{\C h} \eff}{||\Sigma_{\C h}^{1/2}\eff||_2} - \mu_{\C h} \right) = 0,\\
    & p_{\delta} \cdot ||\Sigma_{\C h}^{1/2}\eff||_2 + \mu_{\C h}^{T}\eff = \alpha, \\
    & \lambda > 0. 
\end{align*}
Since we have a convex program, it is sufficient to find a pair $(\effopt, \lambda^*)$ satisfying the KKT conditions and we can immediately conclude that $\effopt$ is an optimal solution to our original problem. 
Using the first equality above, we infer that the optimal effort $\effopt$ must be of the following form: 
\[
        \effopt = \lambda^* \left(k_1 I + k_2 \Sigma_{\C h} \right)^{-1}\mu,
\]
where $k_1 = \frac{1}{||\effopt||_2} > 0$ and $k_2 = \frac{-\lambda^* p_{\delta}}{||\Sigma_{\C h}^{1/2}\effopt||_2} > 0$ (so, the inverse exists). In order to obtain the exact expression for $\effopt$, we need to use the other equality condition and solve simultaneously for $k_1$, $k_2$ and $\lambda^*$. This concludes the proof of the lemma. 

\subsection{Proof of Proposition~\ref{prop:bipartite}}\label{app:bipartite}
Since $\cG$ is a bipartite graph, the set of nodes (in this case, same as set of features) $|\cF|$ can be partitioned into two sets $\cF_{out}$ and $\cF_{in}$ such that $\cF_{in} \cup \cF_{out} = \cF$, $\cF_{in} \cap \cF_{out} = \emptyset$ and all arcs in $\cA$ are directed from $\cF_{out}$ towards $\cF_{in}$. 

Recall that $\Sigma_{\C h}$ is the covariance matrix of $\C h$ where $\C \sim \Pi_{\C}$ and $h \sim \Pi_h$. However, when there is uncertainty only over the edge weights of $\cG$, it is clear that $\Sigma_{\C h} = Cov(\C h_0)$. Therefore, in order to show that $\Sigma_{\C h}$ is a diagonal matrix, it suffices to show that: 
\[
     \forall~f_1, f_2 \in \cF, f_1 \neq f_2, \quad (\C h_0)_{f_1} \perp (\C h_0)_{f_2},
\]
i.e., $(\C h_0)_{f_1}$ and $(\C h_0)_{f_2}$ are independent random variables. Firstly, observe that for any feature $f \in \cF_{in}$, we must have: 
\[
        (\C h_0)_f = 0.
\]
This is because feature $f$ has no outgoing edges (since $f \in \cF_{in}$) and therefore, $\C_{f,.} = \bf{0}^{\top}$ which implies $(\C h_0)_f = \C_{f,.}h_0 = 0$. This automatically implies that the covariance of $(\C h_0)_f$ with any other random variable is also zero. Therefore, we only need to prove that $Cov\left( (\C h_0)_{f_1}, (\C h_0)_{f_2} \right) = 0$ when $f_1, f_2$ both are in $\cF_{out}$. Note that: 
\[
     (\C h_0)_{f_1} = \sum_{f \in \cF} \C_{f_1, f} h_{0,f} \quad \text{and} \quad (\C h_0)_{f_2} = \sum_{f \in \cF} \C_{f_2, f} h_{0,f}.
\]
Therefore, 
\begin{align*}
    Cov\left( (\C h_0)_{f_1}, (\C h_0)_{f_2} \right) &= Cov\left( \sum_{f \in \cF} \C_{f_1, f} h_{0,f}, \sum_{f \in \cF} \C_{f_2, f} h_{0,f} \right) \\
    &= \sum_{f \in \cF} \sum_{f' \in \cF} (h_{0,f}\cdot h_{0,f'}) \cdot Cov(\C_{f_1,f}, \C_{f_2,f'})
\end{align*}
We now argue case by case: 
\begin{itemize}
    \item $f, f' \in \cF_{out}$: In this case, $Cov(\C_{f_1,f}, \C_{f_2,f'}) = 0$ because there can be no edges from either $f_1$ or $f_2$ to $f$ or $f'$ since all of them are nodes in $\cF_{out}$. 
    \item $f \in \cF_{out}, f' \in \cF_{in}$: In this case, $\C_{f_1,f} = 0$ by the same argument as above. Therefore, the covariance must be $0$.
    \item $f' \in \cF_{out}, f \in \cF_{in}$: In this case, $\C_{f_2,f'} = 0$ which makes the covariance $0$.
    \item $f \in \cF_{in}, f' \in \cF_{in}$: Finally, if both $f$ and $f'$ are in $\cF_{in}$, there can be edges from $f_1$ and $f_2$ towards $f$ and $f'$. But those edges are disjoint and therefore, independent which makes the covariance term $0$.
\end{itemize}
This concludes the proof.


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\section{Supplementary Proofs}\label{sec:app_sup}

\subsection{Proof of Observation~\ref{obs:comoute_contri}}
The key step to complete the proof is to show that $A_{ij}^k$ captures the influence exerted by feature $i$ on feature $j$ through a directed path on the graph that is exactly $k$ hops long. We will prove by induction. 

\paragraph{Base case ($k = 0$):} When $k = 0$, there exists no directed path from feature $i$ to feature $j$ unless $i = j$. Therefore, all off-diagonal entries are $0$. The only entries appear on the diagonal because feature $i$ affects itself with a unit positive multiplier. This gives us the identity matrix in $|\cF|$ dimensions which is exactly given by $A^0$. 

\paragraph{General case:} Suppose that the induction hypothesis holds for some $k > 1$. We will now show that it also holds for $k+1$. Note that: 
\[
       A_{ij}^{k+1} = \sum_{n=1}^{|\cF|} A_{in}^{k}\cdot A_{nj}.
\]
Since the induction hypothesis is true, $A_{in}^k$ captures the influence exerted by feature $i$ on feature $n$ through a directed path exactly $k$ hops long. $A_{nj}$ represents the direct influence exerted by feature $n$ on feature $j$ (in exactly $1$ hop). Therefore, the product measures the influence of feature $i$ on feature $j$ exerted on a directed path $k+1$ hops long. The sum over all features in $\cF$ captures all such directed paths from $i$ to $j$. Thus, our induction hypothesis is also true for $k+1$. 

Finally, to compute $\C$, we need to sum the influences of directed paths of all lengths starting at node $i$ and ending in node $j$. Since $\cG$ is a directed acyclic graph with $|\cF|$ nodes, the length of the maximum directed path from $i$ to $j$ is at most $|\cF|-1$ hops long or conservatively $|\cF|$ hops long (note that if there are no directed paths of length $k$ from $i$ to $j$, $A_{ij}^k = 0$. So, it does not hurt to be conservative). This leads to the final expression of $\C$: 
\[
      \C = \sum_{k=0}^{|\cF|} A^k.
\]
To conclude the proof, we need to argue about the time complexity of computing $\C$, given matrix $A$. Multiplying $2$ matrices of size $|\cF| \times |\cF|$ takes $O(|\cF|^3)$ time and we need to execute $O(|\cF|)$ such matrix multiplication steps to compute the different powers of $A$. Therefore, the overall time complexity is polynomial in $|\cF|$.   

\subsection{Proof of Supporting Result in Lemma~\ref{lem:comp_des_nonconvex}}
We made the following observation in our proof of Lemma~\ref{lem:comp_des_nonconvex}:
\begin{blob}
Let $X \in \R^n$ and $M \in \R^{n \times n}$. Define set $Y$ as follows: 
\[
        Y := \{y:~\exists~x \in X \quad \text{s.t.}\quad y = Mx\}
\]
When $M$ is full row-rank, set $X$ is convex if and only if set $Y$ is convex.
\end{blob}
\noindent
We provide a formal proof here. We need to show both directions. 

($\implies$) Suppose, set $X$ is convex. We need to show that set $Y$ is convex. Let $y_1, y_2 \in Y$ such that $y_1 \neq y_2$. Pick any $\lambda \in [0,1]$. Then there must exist $x_1, x_2 \in X$ such that $y_1 = Mx_1$ and $y_2 = Mx_2$. Clearly $x_1 \neq x_2$. Since $X$ is a convex set, $\lambda x_1 + (1-\lambda)x_2 \in X$. This implies, 
\begin{align*}
    \lambda y_1 + (1-\lambda)y_2 &= \lambda Mx_1 + (1-\lambda)Mx_2\\
    &= M \left(\lambda x_1 + (1-\lambda)x_2 \right) \in Y. 
\end{align*}

($\impliedby$) For the other direction, we assume that set $Y$ is convex and we need to show that set $X$ is convex. Pick any two elements $x_1, x_2 \in X, x_1 \neq x_2$ and any $\lambda \in [0,1]$. Let $y_1 = Mx_1$ and $y_2 = Mx_2$. Clearly, $y_1, y_2 \in Y$ (by definition). Note that $y_1 \neq y_2$ (otherwise, we would have $Mx_1 = Mx_2$ which implies that $x_1 - x_2 \in \text{Nullspace}(M)$. But $\text{Nullspace}(M) = \emptyset$ as $M$ is full row-rank). Additionally, $\lambda y_1 + (1-\lambda)y_2 \in Y$ since $Y$ is a convex set. This implies, 
\begin{align*}
    \lambda x_1 + (1-\lambda)x_2 &= \lambda M^{-1}y_1 + (1-\lambda)M^{-1}y_2 \quad \text{($M^{-1}$ exists because $rank(M) = n$)}\\
    &= M^{-1} \left( \lambda y_1 + (1-\lambda)y_2 \right) \in X. 
\end{align*}
The last part follows from noting that $\lambda y_1 + (1-\lambda)y_2 \in Y$ and since $M$ is full row-rank, the pre-image of $\lambda y_1 + (1-\lambda)y_2$ must be unique. This concludes both directions of the proof. 
