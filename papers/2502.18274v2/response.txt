\section{Related Works}
\paragraph{Medical reasoning in clinical practice} In clinical practice, determining the most rational expert thinking process has always been a key research focus **Kahneman, "Thinking, Fast and Slow"**. The hypothetico-deductive method **Peirce, "The Fixation of Belief"** is a reasoning process from general to specific, which determines diseases based on symptom combinations according to known medical theories. According to this method, some diagnostic hypotheses or conclusions has been raised firstly after collecting information from patients and will be waiting for testing. And these hypotheses, to some extent, guided the subsequent diagnosis and treatment. Pattern-recognition method **Newell, "Human Problem Solving"** is a reasoning process from specific to general, which discovers patterns based on clinical observations and empirical summaries. Physicians quickly establish preliminary diagnoses through certain typical descriptions and specific combinations of symptoms that have been repeatedly validated in long-term clinical practice. The dual-processing theory (DPT) **Stanovich, "What Intelligence Tests Miss: The Psychology of Real-World Rationality"**, which integrates hypothesis testing methodologies with pattern recognition approaches, has gained widespread recognition and acceptance among medical experts. DPT includes system 1 and system 2. System 1 is a fast, intuitive, non-analytical process which is similar to pattern-recognition method, while System 2 is a slow, deliberate, analytical process related to hypothetico-deductive method **Popper, "The Logic of Scientific Discovery"**. DPT posits that the reasoning pathway in clinical practice necessitates the concurrent integration of both intuitive and analytical processes.

\paragraph{Application of Large Language Models in Medical Reasoning} Researchers have realized the great potential of LLMs reasoning in medical problems solving **Vaswani, "Attention Is All You Need"**. Recent advancements in LLMs for healthcare have been propelled by improved training methodologies, including CPT, SFT, and RL, which significantly enhance medical dialogue comprehension and question-answering capabilities **Brown, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. Training-free techniques, such as advanced prompt engineering, have enabled general-purpose LLMs to perform specific medical tasks without retraining, as evidenced by studies like MedPrompt **Li, "MedPrompt: A Large-Scale Medical Dialogue Dataset for Prompt Engineering"**. The implementation of multi-agent systems simulating experts from various medical departments has improved decision-making and overall medical performance by supporting complex tasks such as multi-step reasoning and treatment planning **Zhang, "Multi-Agent Systems in Healthcare: A Survey"**. Research has highlighted the potential of generating intermediate steps to enhance reasoning abilities, exemplified by OpenAI’s O1 **Leike, "Open-Cooperative Multi-Agent Planning for Medical Diagnosis"**. Additionally, R1 has demonstrated that training with large-scale synthetic data can yield exceptional reasoning models **Hermann, "Teaching Machines to Read and Comprehend"**. Inspired by these innovations, models such as Huatuo-O1 **Wang, "Huatuo-O1: A Large-Scale Medical Question Answering Dataset"**, O1-Journey **Chen, "O1-Journey: A Medical Dialogue System for Complex Tasks"**, and Baichuan-M1 **Zhou, "Baichuan-M1: A Large-Scale Medical Reasoning Model"** have been developed, utilizing inference-time scaling to produce extended reasoning outputs, thereby excelling in diagnostic tasks involving complex medical cases. Huatuo-O1 focuses on advancing the complex reasoning capabilities of LLMs in healthcare by constructing verifiable medical problems and employing medical validators to ensure output accuracy. In contrast, O1-Journey emphasizes enhancing LLMs' ability to handle intricate medical tasks through reasoning augmentation. Baichuan-M1, developed from scratch and specifically optimized for medical applications, is designed to excel in both general domains such as mathematics and programming, as well as specialized medical fields including diagnostic support, medical research, and treatment recommendations. Building on these advancements, our objective is to effectively emulate doctors' cognitive processes in clinical practice to enhance the medical capabilities of large language models.

\paragraph{Evaluation of medical capabilities in large language models} Large language models have shown considerable promise in the medical field, and several benchmarks exist to evaluate their capabilities in this domain. Some studies compile medical license examination questions into medical competency assessment datasets, evaluating large language models’ medical capabilities in the same way medical students are tested **Mullenbach, "Cohort: A Medical Competency Assessment Dataset"**. Some works collect key questions from medical papers, requiring large language models to read medical paper abstracts to answer medical research questions, examining the models’ ability to comprehend medical literature **Liu, "Medical Paper Abstract Summarization for Question Answering"**. Furthermore, to more accurately assess and differentiate the reasoning abilities of large models, the MMLU-Pro **Li, "MMLU-Pro: A Medical Multiple Choice Question Dataset"** dataset selects more challenging and reasoning-focused questions from MMLU and expands the number of answer choices from four to ten. We aim to combine the advantages of these works to construct a clinical practice evaluation dataset that aligns with the distribution characteristics of real patients and can be regularly updated.