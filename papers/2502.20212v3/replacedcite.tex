\section{Related work}
\subsubsection{The Taylor-nets}
Given observational phase trajectory data of the Hamiltonian system \eqref{2}:
\begin{align}\label{dataH0}
\mathcal{D}=\{(\boldsymbol{y}_0^i,y_1^i)|i=1,\cdots,N\},
\end{align}
neural network inversion of the governing ODE can be typically accomplished by the ODE-nets, which parameterize the latent vector field function $f(\boldsymbol{y}):=J^{-1}\nabla H(y)$ by $f_{net}(\boldsymbol{y},\theta)$ and incorporate it into a certain numerical integrator (such as the Euler method with time step $h$) to form the loss at time $t_1=t_0+h$ corresponding to the observation time of $\boldsymbol{y}_1^i$, i.e. 
$$Loss=\frac{1}{N}\sum_{i=1}^N\|\hat{\boldsymbol{y}}_1^i-\boldsymbol{y}_1^i\|^2$$
with 
$$\hat{\boldsymbol{y}}_1^i=y_1^i+hf_{net}(\boldsymbol{y}_1^i,\theta).$$
The ODE-nets are usually fully-connected feedforward neural networks, which are trained by using optimizers with backpropagation or its improved variants (____).   

The HNN method (____) begins the structure-preserving learning of Hamiltonian systems, which is committed to not only get the governing ODE but also capture its special structure. The HNN realized the structure recognition (energy preservation) by parameterizing the Hamiltonian function $H(\boldsymbol{y})$ of the Hamiltonian system, instead of learning the entire vector field function $f(\boldsymbol{y})$ as was done with the general ODE-nets, since the output $f_{net}(\boldsymbol{y},\theta^*)$ of general ODE-nets may not be the gradient of a certain function after being multiplied by $J$ from the left, owing to the training errors, which would consequently lead to erasure of the symplectic gradient flow structure of the original system. The loss function used by HNN is 
$$ Loss_{HNN}=\left\|\frac{\partial H_{net}}{\partial \boldsymbol{p}}-\frac{d \boldsymbol{q}}{d t}\right\|_2+\left\|\frac{\partial H_{net}}{\partial \boldsymbol{q}}+\frac{d \boldsymbol{p}}{d t}\right\|_2,$$
which needs observations of $(\dot{\boldsymbol{p}},\dot{\boldsymbol{q}})$. By learning the Hamiltonian function $H(\boldsymbol{y})$, the HNN acquires the symplectic structure of the system and thus enables more accurate prediction, energy preservation and faster training.

Taylor-nets (____) introduce a new way of learning separable Hamiltonian systems, where the Hamiltonian function can be split into a function of $\boldsymbol{p}$ and a function of $\boldsymbol{q}$, namely,
\begin{align}
H(\boldsymbol{p},\boldsymbol{q})=T(\boldsymbol{p})+U(\boldsymbol{q}).
\end{align}
Taylor-nets inherited the structure-preserving idea, while directly learning the gradient of the Hamiltonian function $H(\boldsymbol{y})$, i.e. $T_{\boldsymbol{p}} 
\,\,(=\frac{\partial H}{\partial \boldsymbol{p}})$ and $U_{\boldsymbol{q}} 
\,\,(=\frac{\partial H}{\partial \boldsymbol{q}})$, instead of learning $H(\boldsymbol{y})$. This however poses a challenge to their network construction, since as gradients, the networks of them,  $T_{\boldsymbol{p}}(\boldsymbol{p},\theta_{\boldsymbol{p}})$ and $U_{\boldsymbol{q}}(\boldsymbol{q},\theta_{\boldsymbol{q}})$ should have symmetric Jacobian matrices. To realize this, they proposed the following form of networks:  
\begin{align}\label{8}
\boldsymbol{T}_{\boldsymbol{p}}\left(\boldsymbol{p}, \boldsymbol{\theta}_{\boldsymbol{p}}\right)=\left(\sum_{i=1}^S \boldsymbol{A}_i^T \circ f_i \circ \boldsymbol{A}_i-\boldsymbol{B}_i^T \circ f_i \circ \boldsymbol{B}_i\right) \circ \boldsymbol{p}+\boldsymbol{b}_1,
\end{align}
\begin{align}\label{9}
\boldsymbol{U}_{\boldsymbol{q}}\left(\boldsymbol{q}, \boldsymbol{\theta}_{\boldsymbol{q}}\right)=\left(\sum_{i=1}^S \boldsymbol{C}_i^T \circ f_i \circ \boldsymbol{C}_i-\boldsymbol{D}_i^T \circ f_i \circ \boldsymbol{D}_i\right) \circ \boldsymbol{q}+\boldsymbol{b}_2,
\end{align}
where 'o' denotes the function composition, $A_i, B_i,C_i,D_i$ are matrices of learnable network parameters with $d$ columns, $b_1,b_2$ are $d$-dimensional bias vectors of learnable network parameters, and the activation functions $f_i$ are the terms in the Taylor expansion:
\begin{align}
f_i(x)=\frac{1}{i!}x^i,
\end{align}
which acts element-wise on a $d$-dimensional vector, and are called Taylor activations. It is not difficult to check that the Jacobian matrices of \eqref{8} and \eqref{9}, i.e. $T_{\boldsymbol{pp}}(\boldsymbol{p},\theta_{\boldsymbol{p}})$ and $U_{\boldsymbol{qq}}(\boldsymbol{q},\theta_{\boldsymbol{q}})$ are both symmetric. Moreover, the use of the Taylor expansion form enhances the expressive capability of the neural network (____).

For structure-preservation the Taylor-nets embedded the nets \eqref{8} and \eqref{9} into a symplectic Runge-Kutta numerical scheme which is explicit for separable Hamiltonian systems, to forward the inputs by fully-connected feedforward neural network with symplectic structure. 

The symplecticity-preserving network architecture together with the Taylor activation functions make the Taylor-nets significantly superior to other classical neural networks in learning Hamiltonian systems, in terms of prediction accuracy, convergence rate and robustness, with only small sample size and training data, as well as short training period.   

\subsubsection{The Pad\'e activation unit (PAU) }
The PAU (____) was proposed considering the excellent performance of the Pad\'e approximations to most existing activation functions,  and the merits of making activation functions learnable. Concretely, the PAUs are activation functions of the following form
\begin{align}\label{pau1}
F(x)=\frac{P(x)}{Q(x)}=\frac{\sum_{j=0}^m a_j x^j}{1+\left|\sum_{k=1}^n b_k x^k\right|}=\frac{a_0+a_1 x+a_2 x^2+\cdots+a_m x^m}{1+\left|b_1 x+b_2 x^2+\cdots+b_n x^n\right|},
\end{align}
where $a_i$ and $b_i$ are learnable parameters, which can be trained together with other network parameters, and bring flexibility to the activation function. The absolute value in the denominator is designed to avoid zero value of the denominator. 

The application of PAUs was shown to be able to increase the predictive performance and robustness of the neural networks.  
%The aforementioned $f_i$ acts as an activation function in the neural network, adding nonlinearity to the network. 
% A similar activation function based on the Taylor expansion has been mentioned in ____. 
%However, the performance of Taylor approximation is often weaker than that of Pad√© approximation in many cases.