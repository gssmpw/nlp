\section{Related Work}
\label{sec:related-work}

Our work connects to three main ideas: methods for interpreting vision models, methods for controlling model behavior, and sparse autoencoder development.
We discuss each in relation to our approach.

\subsection{Interpretability Methods}

Feature visualization methods \citep{simonyan2013deep,zeiler2014visualizing,mordvintsev2015inceptionism,olah2017feature} reveal interpretable concepts through synthetic images. 
While these approaches demonstrate that models learn meaningful features, the generated images can be unrealistic or misleading, and the methods cannot validate if these visualizations actually drive model behavior. In contrast, our SAE approach identifies features through real image examples and enables direct testing of their causal influence.

Network dissection \citep{bau2017broden,zhou2018interpreting,hernandez2022natural,ghiasi2022vision} attempts to map individual neurons to semantic concepts using labeled datasets. However, this approach struggles with distributed representations where concepts span multiple neurons or single neurons encode multiple concepts (polysemanticity). The sparse features learned by our SAEs naturally decompose these distributed representations into interpretable components.

Concept bottleneck models (CBMs; \citealt{ghorbani2019towards,koh2020concept}), prototype-based approaches \citep{chen2019looks,nauta2021looks,donnelly2022deformable,willard2024looks}, and prompting-based methods
\citep{chowdhury2025prompt,paul2024simple} explicitly incorporate interpretable concepts into model architecture. 
While powerful, these methods require model retraining and cannot analyze existing architectures. 
SAEs can be applied to any pre-trained vision transformer.


Follow-up work addresses these shortcomings: %converting pre-trained models to CBMs 
\citet{yuksekgonul2023posthoc} convert pre-trained models to CBMS; \citet{schrodi2024concept,tan2024explain} develop CBMs with open vocabularies rather than a fixed concept set.
In contrast to these works, SAEs reveal a model’s intrinsic knowledge in a task‐agnostic manner by decomposing dense activations into sparse, monosemantic features without any retraining. 
This plug‐and‐play approach not only faithfully captures the vision model’s internal representations but also enables precise interventions, making SAEs a more flexible tool for validating model interpretation.

Testing with Concept Activation Vectors (TCAV) \citep{kim2018tcav} attempts to connect human concepts to model behavior by identifying directions in activation space that correlate with human-specified concepts (like ``striped'' or ``furry''). 
While TCAV can reveal correlations between activation patterns and semantic concepts, it cannot validate if these concepts actually drive model decisions. 
This highlights a fundamental limitation of correlation-based interpretation methods: showing that activations correlate with human concepts does not prove those concepts cause model behavior. 
Our SAE approach moves beyond correlation by enabling controlled experiments---we can actively suppress or enhance specific features and observe the effects on model behavior, providing causal evidence for our interpretations.

\subsection{Related Control Methods}

Adversarial examples \citep{szegedy2013intriguing,goodfellow2014explaining,moosavi2016deepfool} demonstrate that small, carefully crafted perturbations can dramatically change model predictions. Later work extended this to universal perturbations \citep{moosavi2017universal} and model reprogramming \citep{elsayed2018adversarial}. 
However, these perturbations are typically uninterpretable---we cannot understand why they work. 
Our SAE-based interventions provide interpretable control by manipulating specific semantic features.

Model editing methods for language models \citep{meng2022rome,meng2023memit} enable precise modification of model behavior by directly editing weights or activations. While successful for language tasks, these approaches have not been extended to vision models. Our work provides the first general framework for interpretable editing of vision model behavior.

Recent counterfactual explanation methods \citep{goyal2019counterfactual} attempt to identify minimal input changes that alter predictions. 
While related, these approaches focus on individual examples rather than discovering and manipulating general features as our method does.

CBMs \citep{koh2020concept} enable editing but only in the last concept layer without spatial resolution. 
In contrast, our approach allows for localized manipulation, making it widely applicable to various vision tasks.

\subsection{Sparse Autoencoders}

\citet{makhzani2013k,makhzani2015winner} apply $k$-sparse autoencoders to learn improved image representations.
\citet{subramanian2018spine} apply $k$-sparse autoencoders to static word embeddings (word2vec \citep{mikolov2013word2vec} and GloVe \citep{pennington2014glove}) to improve interpretability.
\citet{zhang2019word,yun2021transformer,bricken2023monosemanticity,templeton2024scaling,gao2024scaling} apply sparse autoencoders to transformer-based language model activations and find highly interpretable features.
We extend these ideas to vision, showing that sparsity can reveal interpretable features in visual as well as linguistic domains.
% Concurrent work \citep{lim2024sparse,thasarathan2025universal} applies SAEs to vision models.

% Through this synthesis of interpretation, control, and sparsity, our work enables both understanding and manipulation of vision model behavior in ways previously impossible. The SAE framework provides a unified approach to discovering interpretable features and validating their causal influence through controlled intervention.