\documentclass[10pt]{article}
\usepackage[preprint]{tmlr}


% Import additional packages before hyperref
\newcommand{\todo}[1]{{\color{red}[TODO: #1]}}
\newcommand{\sam}[1]{{\color{blue}[Sam: #1]}}
\newcommand{\david}[1]{{\color{orange}[David: #1]}}

\usepackage{multirow}
\usepackage{microtype}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{stackengine}
\usepackage{tabularx}

\usepackage[detect-all,separate-uncertainty = true]{siunitx}
\sisetup{
    output-exponent-marker=\ensuremath{\mathrm{e}},
    group-separator= {,},
    group-minimum-digits = 4,
    list-final-separator={, },
    mode={math},
    retain-explicit-plus
}

\usepackage{epigraph}
\setlength{\epigraphrule}{0pt} % Remove the rule between quote and source
% \renewcommand{\epigraphflush}{flushright} % Alignment of the quote

\usepackage[pagebackref,breaklinks,colorlinks,citecolor=blue]{hyperref}
\usepackage[capitalise,noabbrev]{cleveref}

\crefname{equation}{Eq.}{Eqs.}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\hooktext}{\scriptsize}

\makeatletter
\renewcommand{\paragraph}{%
  \@startsection{paragraph}{4}%
  {\z@}{1.5ex \@plus 1ex \@minus .2ex}{-0.5em}%
  {\normalfont\normalsize\bfseries}%
}
\makeatother

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\setlist{nosep}

\title{Sparse Autoencoders for Scientifically Rigorous Interpretation of Vision Models}

\author{\name Samuel Stevens \email stevens.994@osu.edu \\
      \addr The Ohio State University
      \AND
      \name Wei-Lun Chao \email chao.209@osu.edu \\
      \addr The Ohio State University
      \AND
      \name Tanya Berger-Wolf \email berger-wolf.1@osu.edu \\
      \addr The Ohio State University
      \AND
      \name Yu Su \email su.809@osu.edu \\
      \addr The Ohio State University}

\begin{document}

\maketitle

\begin{abstract}
To truly understand vision models, we must not only interpret their learned features but also validate these interpretations through controlled experiments.
Current approaches either provide interpretable features without the ability to test their causal influence, or enable model editing without interpretable controls. 
We present a unified framework using sparse autoencoders (SAEs) that bridges this gap, allowing us to discover human-interpretable visual features and precisely manipulate them to test hypotheses about model behavior. 
By applying our method to state-of-the-art vision models, we reveal key differences in the semantic abstractions learned by models with different pre-training objectives. 
We then demonstrate the practical usage of our framework through controlled interventions across multiple vision tasks. 
We show that SAEs can reliably identify and manipulate interpretable visual features without model re-training, providing a powerful tool for understanding and controlling vision model behavior\footnote{Project website: \url{https://osu-nlp-group.github.io/SAE-V}}.
\end{abstract}




\section{Introduction}\label{sec:introduction}

\epigraph{We do not have knowledge of a thing until we have grasped its why, that is to say, its cause.}
{--- Aristotle}

% \epigraph{What I cannot create, I do not understand.}
% {---Richard Feynman}

\begin{figure*}[t]
    \centering
    \small
    \setlength{\tabcolsep}{4pt}
    \fontfamily{cmss}\selectfont
    \begin{tabular}{cccc}
        % \toprule
        & Observe & Hypothesis & Experiment \\
        \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \addlinespace
        \raisebox{-2pt}{Genomics} &
        \stackunder
            {\includegraphics[width=0.06\textwidth]{figures/hook/trimmed_dna-basic.png}}
            {\parbox{0.275\textwidth}{\hooktext We \textbf{observe} the DNA sequence of a blue jay (\textit{Cyanocitta cristata}), a bird with distinct blue wings.}} &
        \stackunder
            {\includegraphics[width=0.06\textwidth]{figures/hook/trimmed_dna-identified.png}}
            {\parbox{0.275\textwidth}{\hooktext We \textbf{hypothesize} that these genes cause the blue coloration.}} &
        \stackunder
            {\includegraphics[width=0.06\textwidth]{figures/hook/trimmed_dna-knockout.png}}
            {\parbox{0.275\textwidth}{\hooktext We edit the gene (gene knockout) to \textbf{validate} that the hypothesized genes \textit{do} cause blue coloration.}} \\
        \addlinespace
        % \midrule
        \cmidrule(lr){1-4}
        \addlinespace
        \raisebox{25pt}{Grad-CAM} &
        \stackunder
            {\includegraphics[width=0.16\textwidth]{figures/hook/gradcam-basic.png}}
            {\parbox{0.275\textwidth}{\hooktext We \textbf{observe} a CUB-2011-trained ViT correctly predict ''blue jay.''}} &
        \stackunder
            {\includegraphics[width=0.16\textwidth]{figures/hook/gradcam-identified.png}}
            {\parbox{0.275\textwidth}{\hooktext Grad-CAM \textbf{hypothesizes} the highlighted pixels are integral to the ViT's prediction.}} &
        \stackunder
            {\includegraphics[width=0.16\textwidth]{figures/hook/gradcam-knockout.png}}
            {\parbox{0.28\textwidth}{\hooktext How can we empirically \textbf{validate} GradCAM's hypothesis?}} \\
        \addlinespace
        % \midrule
        \cmidrule(lr){1-4}
        \addlinespace
        \raisebox{23pt}{SAEs} & 
        \stackunder
            {\includegraphics[width=0.16\textwidth]{figures/hook/sae-basic.png}}
            {\parbox{0.275\textwidth}{\hooktext We \textbf{observe} a CUB-2011-trained ViT correctly predict ``blue jay.'' We \emph{manually} choose to inspect the bird's wing.}} &
        \stackunder
            {\includegraphics[width=0.16\textwidth]{figures/hook/sae-identified.png}}
            {\parbox{0.275\textwidth}{\hooktext Our SAE finds similar patches from images not necessarily of blue jays; we \textbf{hypothesize} this ``blue feathers'' feature is integral to the ViT's prediction.}} & 
        \stackunder
            {\includegraphics[width=0.16\textwidth]{figures/hook/sae-knockout.png}}
            {\parbox{0.275\textwidth}{\hooktext We suppress ``blue feathers'' and observe changed behavior: the ViT predicts ``Clark's nutcracker,'' a similar species besides no coloration (examples above).}}\\
        % \addlinespace\bottomrule
    \end{tabular}
    \caption{
        We compare the scientific method when applied to genomics and deep learning model interpretation.
        Grad-CAM \citep{selvaraju2017grad} leverages saliency maps to produce hypothetical explanations for model predictions, but provides no natural way to experimentally validate the hypothesis.
        In comparison, our proposed use of sparse autoencoders (SAEs) for vision models naturally enables experimental validation via feature suppression. See \cref{sec:control-experiments} for more examples of experimental validation.
    }\label{fig:hook}
    \vspace{-12pt}
\end{figure*}


{Understanding deep neural networks requires more than passive observation---it demands the ability to test hypotheses through controlled intervention.} 
This mirrors the scientific method itself: true understanding emerges not from mere observation, but from our ability to make and test predictions through controlled experiments. 
Biologists did not truly understand how genes control traits until they could manipulate DNA and observe the effects. 
Similarly, understanding vision models requires not just observing behavior, but systematically testing explanations through controlled experiments.

Applying the scientific method to understanding vision models requires three key capabilities. 
First, we need observable features that correspond to human-interpretable concepts like textures, objects, or abstract properties that appear consistently across images. 
Biologists need measurable markers of gene expression; we need reliable ways to identify specific visual concepts within our models. 
Second, we must be able to precisely manipulate these features to test hypotheses about their causal role---like geneticists' knockout experiments to validate gene function. 
Finally, methods must work with existing models, just as biological techniques must work with existing organisms rather than requiring genetic redesign.

The scientific method advances understanding through a systematic cycle: observing phenomena, forming hypotheses about their causes, and validating these hypotheses through controlled experiments \citep{poincare1914science,popper1959logic}. 
This method drove discoveries from genetics to behavioral psychology---Mendel observed patterns in pea plants and tested inheritance theories through careful breeding, while Pavlov's observation of dogs salivating before feeding led to controlled experiments demonstrating learned associations between stimuli. 
This sequence of scientific discovery---from observation to hypothesis to experimental validation---provides a proven template that, surprisingly, has not been systematically applied to understanding vision models.
Interpretable features enable us to form meaningful hypotheses from observations. 
Precise control mechanisms let us design experiments to test these hypotheses. Compatibility with existing models allows us to validate our findings on models of interest. 
The scientific method breaks down if any component is missing: observations become untestable, experiments become uninterpretable, or validation becomes impossible.

Current approaches to understanding vision models fall short of these requirements, preventing rigorous scientific investigation.
Feature visualization~\citep{erhan2009visualizing,mordvintsev2015deepdream,olah2017feature} methods provide interpretable observations but no mechanism to validate if these observations actually drive model behavior \citep{geirhos2024dont}.
Adversarial examples \citep{szegedy2013intriguing,goodfellow2014explaining} enable manipulation but offer no interpretable explanation for how they work.
Network dissection~\citep{bau2017broden,hernandez2021natural,kalibhat2023identifying} attempts to map individual neurons to semantic concepts, but struggles with distributed representations where concepts span multiple neurons or single neurons encode multiple concepts. 
This makes it difficult to reliably identify and manipulate specific semantic features.
This fragmentation---where methods either enable interpretation without control or control without interpretation---undermines our ability to build reliable understanding of these systems

\begin{figure*}
    \centering
    \small
    \includegraphics[width=\textwidth]{figures/overview2.jpg}
    \caption{Sparse autoencoders (SAEs) trained on pre-trained ViT activations discover a wide spread of features across both visual patterns and semantic structures. We show eight different features from an SAE trained on ImageNet-1K activations from a CLIP-trained ViT-B/16.
    }\label{fig:overview2}
    \vspace{-12pt}
\end{figure*}

We demonstrate that \textbf{sparse autoencoders} (\textbf{SAEs}; \citealt{makhzani2013k,subramanian2018spine,huben2024sparse}) enable this kind of rigorous scientific investigation.
SAEs transform entangled activation vectors into higher-dimensional sparse representations, in which each nonzero element likely corresponds to a distinct semantic concept.
In the case of bird classification, one sparse dimension might indicate blue plumage. 
Because the SAE is trained to reconstruct the original activations, each element explicitly corresponds to a particular direction in the original dense activation space. 
This direct mapping allows us to selectively modify that feature---suppressing the blue signal---to observe consequent shifts in model predictions.
This unification of interpretation with controlled intervention completes the cycle of observation, hypothesis, and experiment (see \cref{fig:hook}), and we validate its effectiveness through experiments across multiple core contributions.

First, we show how SAEs enable systematic observation of learned features, revealing fundamental differences between models like CLIP \citep{radford2021learning} and DINOv2 \citep{oquab2023dinov2} (\cref{sec:understanding}). 
We discover that CLIP learns to recognize country-specific visual patterns, from architectural landmarks to sporting events (\cref{sec:cultural-understanding}) and style-agnostic representations (\cref{sec:semantic-abstraction}), suggesting that language supervision leads to rich world knowledge that purely visual training cannot achieve.
Just as comparative biology reveals how different environments shape evolution \citep{grant2006evolution,losos2011lizards,brawand2014genomic}, our analysis shows how different training objectives lead models to develop qualitatively different internal representations.

Second, we validate our interpretations through controlled experiments across multiple tasks. 
When models detect features corresponding to bird markings, we confirm their causal role by showing that modifying them predictably changes species classification (\cref{sec:classification}). 
Similarly, with semantic segmentation, we show that identified features enable precise, targeted manipulation: we can suppress specific semantic concepts (like ``sand'' or ``grass'') while preserving all other scene elements, demonstrating both the semantic meaning of our features and their independence from unrelated concepts (\cref{sec:semseg}).
We achieve this through multiple interactive dashboards, enabling readers to readily explore these features.\footnote{\href{https://OSU-NLP-Group.github.io/SAE-V\#demos}{\url{https://OSU-NLP-Group.github.io/SAE-V\#demos}}}

Third, we provide a public, extensible codebase that works with any vision transformer without modification, enabling broad scientific investigation of modern vision models.
We demonstrate this flexibility through fine-grained classification and semantic segmentation experiments, with future updates planned.

Through these contributions, we show that rigorous understanding of vision models requires both interpretation and experimental validation, and that SAEs provide a natural concrete implementation of such a framework.


\section{Related Work}\label{sec:related-work}

Our work connects to three main ideas: methods for interpreting vision models, methods for controlling model behavior, and sparse autoencoder development.
We discuss each in relation to our approach.

\subsection{Interpretability Methods}

Feature visualization methods \citep{simonyan2013deep,zeiler2014visualizing,mordvintsev2015inceptionism,olah2017feature} reveal interpretable concepts through synthetic images. 
While these approaches demonstrate that models learn meaningful features, the generated images can be unrealistic or misleading, and the methods cannot validate if these visualizations actually drive model behavior. In contrast, our SAE approach identifies features through real image examples and enables direct testing of their causal influence.

Network dissection \citep{bau2017broden,zhou2018interpreting,hernandez2022natural,ghiasi2022vision} attempts to map individual neurons to semantic concepts using labeled datasets. However, this approach struggles with distributed representations where concepts span multiple neurons or single neurons encode multiple concepts (polysemanticity). The sparse features learned by our SAEs naturally decompose these distributed representations into interpretable components.

Concept bottleneck models (CBMs; \citealt{ghorbani2019towards,koh2020concept}), prototype-based approaches \citep{chen2019looks,nauta2021looks,donnelly2022deformable,willard2024looks}, and prompting-based methods
\citep{chowdhury2025prompt,paul2024simple} explicitly incorporate interpretable concepts into model architecture. 
While powerful, these methods require model retraining and cannot analyze existing architectures. 
SAEs can be applied to any pre-trained vision transformer.


Follow-up work addresses these shortcomings: %converting pre-trained models to CBMs 
\citet{yuksekgonul2023posthoc} convert pre-trained models to CBMS; \citet{schrodi2024concept,tan2024explain} develop CBMs with open vocabularies rather than a fixed concept set.
In contrast to these works, SAEs reveal a model’s intrinsic knowledge in a task‐agnostic manner by decomposing dense activations into sparse, monosemantic features without any retraining. 
This plug‐and‐play approach not only faithfully captures the vision model’s internal representations but also enables precise interventions, making SAEs a more flexible tool for validating model interpretation.

Testing with Concept Activation Vectors (TCAV) \citep{kim2018tcav} attempts to connect human concepts to model behavior by identifying directions in activation space that correlate with human-specified concepts (like ``striped'' or ``furry''). 
While TCAV can reveal correlations between activation patterns and semantic concepts, it cannot validate if these concepts actually drive model decisions. 
This highlights a fundamental limitation of correlation-based interpretation methods: showing that activations correlate with human concepts does not prove those concepts cause model behavior. 
Our SAE approach moves beyond correlation by enabling controlled experiments---we can actively suppress or enhance specific features and observe the effects on model behavior, providing causal evidence for our interpretations.

\subsection{Related Control Methods}

Adversarial examples \citep{szegedy2013intriguing,goodfellow2014explaining,moosavi2016deepfool} demonstrate that small, carefully crafted perturbations can dramatically change model predictions. Later work extended this to universal perturbations \citep{moosavi2017universal} and model reprogramming \citep{elsayed2018adversarial}. 
However, these perturbations are typically uninterpretable---we cannot understand why they work. 
Our SAE-based interventions provide interpretable control by manipulating specific semantic features.

Model editing methods for language models \citep{meng2022rome,meng2023memit} enable precise modification of model behavior by directly editing weights or activations. While successful for language tasks, these approaches have not been extended to vision models. Our work provides the first general framework for interpretable editing of vision model behavior.

Recent counterfactual explanation methods \citep{goyal2019counterfactual} attempt to identify minimal input changes that alter predictions. 
While related, these approaches focus on individual examples rather than discovering and manipulating general features as our method does.

CBMs \citep{koh2020concept} enable editing but only in the last concept layer without spatial resolution. 
In contrast, our approach allows for localized manipulation, making it widely applicable to various vision tasks.

\subsection{Sparse Autoencoders}

\citet{makhzani2013k,makhzani2015winner} apply $k$-sparse autoencoders to learn improved image representations.
\citet{subramanian2018spine} apply $k$-sparse autoencoders to static word embeddings (word2vec \citep{mikolov2013word2vec} and GloVe \citep{pennington2014glove}) to improve interpretability.
\citet{zhang2019word,yun2021transformer,bricken2023monosemanticity,templeton2024scaling,gao2024scaling} apply sparse autoencoders to transformer-based language model activations and find highly interpretable features.
We extend these ideas to vision, showing that sparsity can reveal interpretable features in visual as well as linguistic domains.
% Concurrent work \citep{lim2024sparse,thasarathan2025universal} applies SAEs to vision models.

% Through this synthesis of interpretation, control, and sparsity, our work enables both understanding and manipulation of vision model behavior in ways previously impossible. The SAE framework provides a unified approach to discovering interpretable features and validating their causal influence through controlled intervention.

\section{Methodology}\label{sec:methodology}

\subsection{Observations: Model Behaviors}
Vision models demonstrate diverse capabilities across multiple tasks. 
Understanding these systems requires explanations that satisfy three key criteria: they must be human-interpretable, testable through controlled experiments, and enable precise intervention in model behavior.

\subsection{Hypotheses: SAE-Generated Explanations}\label{sec:sae-training}


\begin{figure*}
    \centering
    \small
    \includegraphics[width=\textwidth]{figures/overview.pdf}
    \caption{Given a picture and a set of highlighted patches, we find exemplar images by (1) getting ViT activations for each patch, (2) computing a sparse representation for each highlighted patch (\cref{eq:enc,eq:act}), (3) summing over sparse representations, (4) choosing the top $k$ features by activation magnitude and (5) finding existing images that maximize these features.}\label{fig:overview}
\end{figure*}

Sparse autoencoders generate testable hypotheses by decomposing dense activation vectors into sparse feature vectors. Given an $d$-dimensional activation vector $\mathbf{x} \in \mathbb{R}^d$ from an intermediate layer $l$ of a vision transformer, an SAE maps $\mathbf{x}$ to a sparse representation $f(\mathbf{x})$ (\cref{eq:enc,eq:act}) and reconstructs the original input (\cref{eq:dec}). 
We use ReLU autoencoders \citep{bricken2023monosemanticity,templeton2024scaling}:
\begin{align}
    \mathbf{h} &= W_\text{enc} (\mathbf{x} - b_\text{dec}) + b_\text{enc} \label{eq:enc} \\
    f(\mathbf{x}) &= \text{ReLU}(\mathbf{h}) \label{eq:act} \\
    \mathbf{\hat{x}} &= W_\text{dec} f(\mathbf{x}) + b_\text{dec} \label{eq:dec}
\end{align}
where $W_\text{enc} \in \mathbb{R}^{n \times d}$, $b_\text{enc} \in \mathbb{R}^{n}$, $W_\text{dec} \in \mathbb{R}^{d \times n}$ and $b_\text{dec} \in \mathbb{R}^{d}$.
The training objective minimizes reconstruction error while encouraging sparsity:
\begin{equation}
    \mathcal{L}(\theta) = ||\mathbf{x} - \mathbf{\hat{x}}||_2^2 + \lambda \mathcal{S}(f(\mathbf{x}))
\end{equation}
where $\lambda$ controls the sparsity penalty and $\mathcal{S}$ measures sparsity (L1 norm for training, L0 for model selection).

We train on $N$ randomly sampled patch activation vectors from the residual stream of layer $l$ in a vision transformer. 
Following prior work \citep{templeton2024scaling}, we subtract the mean activation vector and normalize activation vectors to unit norm before training.
Detailed reproduction instructions are \href{https://osu-nlp-group.github.io/SAE-V/saev/#guide-to-training-saes-on-vision-models}{available}.
Given a pre-trained ViT like CLIP or DINOv2, an image, and one or more patches of interest, we leverage a trained SAE to find similar examples (\cref{fig:overview}).

\subsection{Experiments: Testing Through Control}

We validate SAE-proposed explanations through a general intervention framework that leverages the common pipeline of vision tasks: an image is first converted into $p$ $d$-dimensional activation vectors in $\mathbb{R}^{p \times d}$ (e.g., from a vision transformer), then these activation vectors are mapped by a task-specific decoder to produce outputs in the task-specific output space $\mathcal{O}$. 
For instance, in semantic segmentation each patch's activation vector $\mathbb{R}^d$ is fed to a decoder head that assigns pixel-level segmentation labels, and these pixel labels are assembled into the final segmentation map.

Given a task-specific decoder $\mathcal{M}\colon \mathbb{R}^{p \times d} \rightarrow \mathcal{O}$ that maps from $n$ activations vectors to per-patch class predictions, our intervention process proceeds in six steps:
\begin{enumerate}
   \item{Encode and reconstruct: $f(\mathbf{x}) = \text{ReLU}(W_\text{enc} (\mathbf{x} - b_\text{dec}) + b_\text{enc})$ and $\mathbf{\hat{x}} = W_\text{dec} f(\mathbf{x}) + b_\text{dec}$.}
   \item{Calculate reconstruction error \citep{templeton2024scaling}: $\mathbf{e} = \mathbf{x} - \mathbf{\hat{x}}$.}
   \item{Modify individual values of $f(\mathbf{x})$ to get $f(\mathbf{x})'$.}
   \item{Reconstruct modified activations: $\mathbf{\hat{x}}' = W_\text{dec} f(\mathbf{x})' + b_\text{dec}$.}
   \item{Add back error: $\mathbf{x}' = \mathbf{e} + \mathbf{\hat{x}}'$.}
   \item{Compare outputs $\mathcal{M}(\mathbf{x})$ versus $\mathcal{M}(\mathbf{x}')$.}
\end{enumerate}
In plain language: We start by converting an image into a set of activation vectors---one per patch---that capture the ViT's internal representation.
Then, we encode these vectors into a sparse representation that highlights the key features, while also tracking the small differences (errors) between the sparse form and the original. 
Next, we deliberately tweak the sparse representation to modify a feature of interest. 
After reconstructing the modified activation vectors (and adding back the previously captured details), we pass them through the task-specific decoder (e.g., for classification, segmentation, or another vision task) to see how the output changes. 
By comparing the original and altered outputs, we can determine whether and how the targeted feature influences the model's behavior.

% Feature modifications are scaled relative to training-time maximum activations, with values in $[-20, 20]$ representing $-20\times$ to $20\times$ the maximum observed activation. 
% To identify relevant features for modification, we combine automated metrics (like F1 scores between feature activations and ground truth) with manual verification through activation visualization (\cref{sec:control-experiments}).

\begin{figure*}[t]
    \centering
    \small
    \setlength{\tabcolsep}{1pt}
    \begin{tabular}{ccccccc}
        \multicolumn{4}{c}{(a) \texttt{CLIP-24K/6909}: ``Brazil''} & \multicolumn{2}{c}{(b) \textit{Not} Brazil} & \multirow{3}{*}{\includegraphics[width=22pt]{figures/clip-vs-dinov2/legend.png}} \\
        \cmidrule(lr){1-4} \cmidrule(lr){5-6}
        \includegraphics[width=0.15\textwidth]{figures/clip-vs-dinov2/clip-6909-positive-1.png} & 
        \includegraphics[width=0.15\textwidth]{figures/clip-vs-dinov2/clip-6909-positive-2.png} & 
        \includegraphics[width=0.15\textwidth]{figures/clip-vs-dinov2/clip-6909-positive-3.png} & 
        \includegraphics[width=0.15\textwidth]{figures/clip-vs-dinov2/clip-6909-positive-4.png} &
        \includegraphics[width=0.15\textwidth]{figures/clip-vs-dinov2/clip-6909-negative-1.png} &
        \includegraphics[width=0.15\textwidth]{figures/clip-vs-dinov2/clip-6909-negative-2.png} \\[12pt]
        \multicolumn{4}{c}{(c) \texttt{DINOv2-24K/9823}} & \multicolumn{2}{c}{(d) ImageNet-1K Exemplars} \\ 
        \cmidrule(lr){1-4} \cmidrule(lr){5-6}
        \includegraphics[width=0.15\textwidth]{figures/clip-vs-dinov2/dinov2-9823-positive-1.png} & 
        \includegraphics[width=0.15\textwidth]{figures/clip-vs-dinov2/dinov2-9823-positive-2.png} & 
        \includegraphics[width=0.15\textwidth]{figures/clip-vs-dinov2/dinov2-9823-positive-3.png} & 
        \includegraphics[width=0.15\textwidth]{figures/clip-vs-dinov2/dinov2-9823-positive-4.png} &
        \includegraphics[width=0.15\textwidth]{figures/clip-vs-dinov2/dinov2-9823-example-1.png} &
        \includegraphics[width=0.15\textwidth]{figures/clip-vs-dinov2/dinov2-9823-example-2.png} 
    \end{tabular}
    \caption{
        CLIP learns robust cultural visual features.
        \textbf{Top Left (a):} A ``Brazil'' feature (\texttt{CLIP-24K/6909}) responds to distinctive Brazilian imagery including Rio de Janeiro's urban landscape, the national flag, and the iconic sidewalk tile pattern of Copacabana Beach
        \textbf{Top Right (b):} \texttt{CLIP-24K/6909} does not respond to other South American symbols like Machu Picchu or the Argentinian flag.
        \textbf{Bottom Left (c):} We search DINOv2's SAE for a similar ``Brazil'' feature and find that \texttt{DINOv2-24K/9823} fires on Brazilian imagery.
        \textbf{Bottom Right (d):} However, maximally activating ImageNet-1K examples for \texttt{DINOv2-24K/9823} are of lamps, convincing us that \texttt{DINOv2-24K/9823} does not reliably detect Brazilian cultural symbols.
    }\label{fig:clip-vs-dinov2-cultural}
\end{figure*}

\section{SAE-Enabled Analysis of Vision Models}\label{sec:understanding}

Sparse autoencoders (SAEs) provide a powerful new lens for understanding and comparing vision models. 
By decomposing dense activation vectors into interpretable features, SAEs enable systematic analysis of what different architectures learn and how their training objectives shape their internal representations. 
We demonstrate that even simple manual inspection of top-activating images for SAE-discovered features can reveal fundamental differences between models that would be difficult to detect through other methods.

While prior work has used techniques like TCAV \citep{kim2018tcav} to probe for semantic concepts in vision models, these methods typically test for pre-defined concepts and rely on supervised concept datasets. 
In contrast, our SAE-based approach discovers interpretable features directly from model activations without requiring concept supervision. 
We train SAEs on intermediate layer activations following the procedure detailed in \cref{sec:methodology}, then analyze the highest-activating image patches for each learned feature as in \cref{fig:overview}. 
This straightforward process reveals both specific features (e.g., a feature that fires on dental imagery) and model-level patterns (e.g., one model consistently learning more abstract features than another).
% Technical details of the process are provided in \cref{app:analysis-details}.

We refer to individual SAE features using a standardized format \texttt{MODEL-WIDTH/INDEX}, where MODEL identifies the vision transformer the SAE was trained on (e.g., CLIP or DINOv2), WIDTH indicates the number of features in the SAE (e.g., 24K for \num{24576}), and INDEX uniquely identifies the specific feature. 
For example, \texttt{CLIP-24K/20652} refers to feature \num{20652} from a \num{24576}-dimensional SAE trained on CLIP activations.


\subsection{Language Supervision Enables Cultural Understanding}\label{sec:cultural-understanding}

We analyze SAE features and find that CLIP learns remarkably robust representations of cultural and geographic concepts--a capability that appears absent in DINOv2's purely visual representations. 
This finding demonstrates how language supervision enables learning of abstract cultural features that persist across diverse visual manifestations.

Consider CLIP's representation of country-specific visual features. 
We find individual SAE features that consistently activate on imagery associated with specific countries, while remaining inactive on visually similar but culturally distinct images (\cref{fig:clip-vs-dinov2-cultural}). 
For instance, CLIP reliably detects German visual elements across architectural landmarks (like the Brandenburg Gate), sports imagery (German national team uniforms), and cultural symbols (Oktoberfest celebrations).
Crucially, this feature remains inactive on other European architectural landmarks or sporting events, suggesting it captures genuine cultural associations rather than just visual similarities.

Similarly, we find features that activate on distinctly Brazilian imagery, spanning Rio de Janeiro's urban landscape, the national flag, coastal scenes, and cultural celebrations. 
These features show selective activation, responding strongly to Brazilian content while remaining inactive on visually similar scenes from other South American locations. 
This selective response pattern suggests CLIP has learned to recognize and group culturally related visual elements, even when they share few low-level visual features.
See \cref{fig:additional-clip-vs-dinov2-cultural} for additional examples of this phenomena.

In contrast, when we analyze DINOv2's features, we find no comparable country-specific representations. 
This fundamental difference reveals how language supervision guides CLIP to learn culturally meaningful visual abstractions that pure visual training does not discover.

\begin{figure*}[t]
    \centering
    \small
    \setlength{\tabcolsep}{1pt}
    \begin{tabular}{cccccccc}
        \multicolumn{5}{c}{\texttt{CLIP-24K/20652}} & \multicolumn{2}{c}{ImageNet-1K Exemplars} & \multirow{3}{*}{\includegraphics[width=19.5pt]{figures/clip-vs-dinov2/legend.png}} \\ 
        \cmidrule(lr){1-5} \cmidrule(lr){6-7}
        \includegraphics[width=0.13\textwidth]{figures/clip-vs-dinov2/clip-crash-positive-1.png} & 
        \includegraphics[width=0.13\textwidth]{figures/clip-vs-dinov2/clip-crash-positive-2.png} & 
        \includegraphics[width=0.13\textwidth]{figures/clip-vs-dinov2/clip-crash-positive-3.png} & 
        \includegraphics[width=0.13\textwidth]{figures/clip-vs-dinov2/clip-crash-positive-4.png} &
        \includegraphics[width=0.13\textwidth]{figures/clip-vs-dinov2/clip-crash-positive-5.png} &
        \includegraphics[width=0.13\textwidth]{figures/clip-vs-dinov2/clip-20652-1.png} &
        \includegraphics[width=0.13\textwidth]{figures/clip-vs-dinov2/clip-20652-2.png} \\[12pt]
        \multicolumn{5}{c}{\texttt{DINOv2-24K/9672}} & \multicolumn{2}{c}{ImageNet-1K Exemplars} \\ 
        \cmidrule(lr){1-5} \cmidrule(lr){6-7}
        \includegraphics[width=0.13\textwidth]{figures/clip-vs-dinov2/dinov2-crash-positive-1.png} & 
        \includegraphics[width=0.13\textwidth]{figures/clip-vs-dinov2/dinov2-crash-positive-2.png} & 
        \includegraphics[width=0.13\textwidth]{figures/clip-vs-dinov2/dinov2-crash-positive-3.png} & 
        \includegraphics[width=0.13\textwidth]{figures/clip-vs-dinov2/dinov2-crash-positive-4.png} &
        \includegraphics[width=0.13\textwidth]{figures/clip-vs-dinov2/dinov2-crash-positive-5.png} &
        \includegraphics[width=0.13\textwidth]{figures/clip-vs-dinov2/dinov2-9672-1.png} &
        \includegraphics[width=0.13\textwidth]{figures/clip-vs-dinov2/dinov2-9672-2.png}\\
    \end{tabular}
    \caption{
        CLIP learns unified representations of abstract concepts that persist across visual styles.
        Highlighted patches indicate feature activation strength.
        \textbf{Upper Left:} We find a CLIP SAE feature (\texttt{CLIP-24K/20652}) that consistently activates on ``accidents'' or ``crashes'': car accidents, plane crashes, cartoon depictions of crashes and generally damaged metal.
        \textbf{Upper Right:} Two exemplar images from ImageNet-1K for feature \texttt{CLIP-24K/20652}.
        \textbf{Lower Left:} We probe an SAE trained on DINOv2 activations. \texttt{DINOv2-24K/9762} is the closest feature, but does not reliably fire on all the examples.
        \textbf{Lower Right:} Two exemplar images from ImageNet-1K for feature \texttt{DINOv2-24K/9762} clarifies that it does not match the semantic concept of ``crash.''
    }\label{fig:clip-vs-dinov2-semantic}
\end{figure*}

\subsection{Language Supervision Induces Semantic Abstraction}\label{sec:semantic-abstraction}

Beyond cultural concepts, CLIP learns abstract semantic features that persist across visual styles--a capability absent in DINOv2--revealing how language supervision enables human-like semantic concept formation.

% A striking example emerges in CLIP's representation of mechanical accidents and crashes. 
For example, we discover an SAE feature (\texttt{CLIP-24K/20652}) that activates on accident scenes across photographs of car accidents, crashed planes and cartoon depictions of crashes (\cref{fig:clip-vs-dinov2-semantic}). 
This feature fires strongly whether processing a news photograph of a car accident or a stylized illustration of a collision--suggesting CLIP has learned a truly abstract representation of ``accident'' that transcends visual style.

In contrast, DINOv2's features fragment these examples across multiple low-level visual patterns. 
While it learns features that detect specific visual aspects of accidents (like crumpled metal in photographs), no single feature captures the semantic concept across different visual styles. 
This suggests that without language supervision, DINOv2 lacks the learning signal needed to unite these diverse visual presentations into a single abstract concept.
% This pattern extends beyond accident scenes to other semantic concepts, from object states to event types (see \cref{app:additional-clip-vs-dinov2-semantic} for additional examples). 
We hypothesize that CLIP's language supervision provides explicit signals to group visually distinct but semantically related images, while DINOv2's purely visual training offers no such bridge across visual styles. 
The ability to form such abstract semantic concepts, independent of visual style, represents a fundamental difference in how these models process and represent visual information.

\subsection{Implications for Vision Model Selection}

Recent work has demonstrated empirical benefits from combining different vision encoders in vision-language models \citep{liu2024llava,liu2024llava1_5,lu2024deepseek_vl}. 
\citet{tong2024eyes} constructed a challenging dataset for VLMs using only CLIP as a visual encoder.
Cambrian-1 \citep{tong2024cambrian} found improved performance when incorporating both CLIP and DINOv2 encoders, and \citet{jiang2023clip} systematically examined the distinct contributions of different visual encoders in VLMs. 
However, the mechanistic basis for these improvements has remained unclear.

Our SAE-driven analysis provides a possible explanation for these empirical findings.
The distinct feature patterns we observe--CLIP's abstract semantic concepts versus DINOv2's style-specific features--suggest these models develop complementary rather than redundant internal representations.
When CLIP learns to recognize ``accidents'' across various visual styles, or country-specific features across diverse contexts, it develops abstractions that may help with high-level semantic understanding. 
Meanwhile, DINOv2's more granular features could provide detailed visual information that complements CLIP's abstract representations.
Rather than treating encoders as interchangeable components to be evaluated purely on benchmark performance, practitioners might choose encoders based on characterized feature patterns. 
% However, further work is needed to fully understand how these distinct representational strategies contribute to model performance across different tasks and domains.

\begin{figure*}[t]
    \centering
    \small
    \includegraphics[width=\linewidth]{figures/classification.pdf}
    \caption{
    Demonstrating the scientific method for understanding vision model behavior using sparse autoencoders (SAEs). 
    \textbf{Left:} We observe that CLIP predicts ``Blue Jay.'' 
    \textbf{Upper Middle:} We select the bird's wing in the input image; the SAE proposes a hypothesis that the most salient feature is ``blue feathers'' via exemplar images.
    \textbf{Lower Middle:} We validate this hypothesis through controlled intervention by suppressing the identified ``blue feathers'' feature in the model's activation space.
    \textbf{Right:} we observe a change in behavior: the predicted class shifts away from ``Blue Jay'' towards ``Clark Nutcracker'', a similar bird besides the lack of blue plumage. 
    This three-step process of observation, hypothesis formation, and experimental validation enables systematic investigation of how vision models process visual information.
    % \todo{(1) use plasma colormap for the examples.}
    }\label{fig:classification}
\end{figure*}

\section{Validating Hypotheses of Vision Model Behavior}\label{sec:control-experiments}

Understanding how vision models process information requires proving that our explanations accurately capture the model's behavior. 
While prior work has demonstrated interpretable features or model control in isolation, validating explanations requires showing both that discovered features are meaningful and that we can precisely manipulate them. 
We demonstrate that SAE-derived features enable reliable control across two complementary visual understanding tasks--classification and segmentation--providing strong evidence that our interpretations faithfully capture model behavior.

Vision models process information across multiple levels of abstraction, from local visual features to global semantic reasoning. 
Standard evaluation approaches focus on a single level of processing, limiting their ability to validate feature explanations. 
We overcome this by strategically selecting tasks that test different aspects of visual understanding: classification validates precise control over visual attributes and segmentation demonstrates spatial coherence of identified features,
% and visual question answering proves our manipulations preserve rather than destroy learned semantic representations.

For each task, we first train or utilize an existing task-specific prediction head. 
We then perform controlled feature interventions using our SAE and compare model behavior before and after intervention. 
Success across these diverse challenges, which we demonstrate through extensive qualitative results, provides strong evidence that our method identifies meaningful and faithful features.



\subsection{Image Classification}\label{sec:classification}

Precisely manipulating individual visual features is essential for validating our understanding of how vision models make decisions. 
While prior work identifies features through post-hoc analysis, proving causal relationships requires demonstrating that controlled feature manipulation yields predictable output changes.
Using fine-grained bird classification as a testbed, we show that SAE-derived features enable precise control: we can manipulate specific visual attributes like beak shape or plumage color while preserving other traits, producing predictable changes in classification outputs that validate our feature-level explanations of model behavior.

We train a sparse autoencoder on activations from layer \num{11} of a CLIP-pretrained ViT-B/16 \citep{radford2021learning}, using the complete ImageNet-1K dataset \citep{russakovsky2015imagenet} to ensure broad feature coverage. 
The SAE uses a $32\times$ expansion factor (\num{24576} features) to capture a rich vocabulary of visual concepts. 
Through an interactive exploration interface, we identify interpretable features by examining patches that maximally activate specific SAE features. 
For example, selecting the blue feathers of a blue jay (\textit{Cyanocitta cristata}) reveals SAE features that consistently activate on similar colorations across the dataset.

To validate the proposed explanation of these features, we can manipulate them by adjusting their values in the SAE's latent space.
When we reduce the ``blue feathers''  feature in a blue jay image, the model's prediction shifts from blue jay to Clark's nutcracker (\textit{Nucifraga columbiana})--a semantically meaningful change that aligns with ornithological knowledge (\cref{fig:classification}).
See \cref{app:classification-details} for SAE details, linear classifier details, and additional qualitative examples.

% While our current interface limits manipulation to single features at a time, the approach generalizes beyond bird classification and provides a powerful tool for understanding vision model decisions. 
% Some manipulations can lead to unexpected results, highlighting areas for future investigation into feature interactions and stability.


\begin{figure*}[t]
    \centering
    \small
    \includegraphics[width=\linewidth]{figures/semseg.pdf}
    \caption{\textbf{Far Left:} We train a linear head to predict semantic segmentation classes for each patch.
    \textbf{Middle Left:} We choose all sand-filled patches in the input image to inspect. 
    \textbf{Middle Right:} Our SAE proposes exemplar images for the maximally activating sparse dimensions, as in \cref{sec:classification}, suggesting that DINOv2 is learning a sand feature.
    \textbf{Far Right:} We suppress the sand feature in not just the selected patches, but \textit{all} patches. We modify all activation vectors and pass them to DINOv2's final transformer layer followed by our trained linear segmentation head. We see that the head predicts ``earth, ground'' and ``water'' for the former sand patches. Both classes are good second choices if ``sand'' is unavailable. Notably, other patches are not meaningfully affected, demonstrating the pseudo-orthogonality of the SAE's learned feature vectors.
    }\label{fig:semseg}
\end{figure*}

\subsection{Semantic Segmentation}\label{sec:semseg}

Moving beyond single-feature control requires proving that discovered features can be manipulated independently—a key challenge that traditional interpretability methods struggle to address.
When features are entangled, attempts to modify one aspect of an image often produce unintended changes in others, making precise intervention impossible.
Through semantic segmentation, we demonstrate that SAE features form a pseudo-orthogonal basis for the model's representational space—while not mathematically perpendicular, these features are functionally independent in practice.
When we suppress semantic concepts like ``sand'' or ``grass'', we observe consistent changes in targeted regions while leaving other predictions intact, demonstrating this functional independence.

We train a linear probe on vision model patch-level features on the ADE20K semantic segmentation dataset \citep{zhou2017ade20k} following the setup in \citet{oquab2023dinov2}.
Specifically, given an image-level representation tensor $\mathbb{R}^{p \times p \times d}$, we train a linear probe to predict a low-resolution $p \times p \times C$ logit map, which is up-sampled to full resolution.
We achieve \num{35.1} mIoU on the ADE20K validation set.
While this method falls short of state-of-the-art methods, it is a sufficient testbed for studying feature control.
Specific training details are are available in \cref{app:semseg-details}.

We train a sparse autoencoder on ImageNet-1K activations from layer 11 of a DINOv2-pretrained ViT-B/16 following the procedure described in \cref{sec:methodology}.

To validate our method's ability to manipulate semantic features, we developed an interactive interface that allows precise control over individual SAE features. 
Users can select specific image patches and modify their feature activations in both positive and negative directions, enabling targeted manipulation of semantic concepts. 
This approach provides several advantages over automated evaluation metrics: it allows exploration of feature interactions, demonstrates the spatial coherence of manipulations, and reveals how features compose to form higher-level concepts. 
The interface makes it possible to test hypotheses about learned features through direct experimentation--for instance, we can verify that ``sand'' features truly capture sand-like textures by suppressing them and observing consistent changes across diverse images, as shown in \cref{fig:semseg}.

\section{Conclusion}\label{sec:conclusion}

We demonstrate that sparse autoencoders enable both hypothesis formation and controlled testing for vision models, enabling both interpretation of learned features and precise control over model behavior.

Our work reveals fundamental insights about representation learning in vision models. We show that language supervision guides CLIP toward human-interpretable abstractions that generalize across visual styles, while purely visual training in DINOv2 produces more granular, style-specific features. 
This finding suggests that achieving human-like visual abstraction may require bridging between different modalities or forms of supervision.

Through controlled experiments across classification, segmentation, and image-text tasks, we validate that SAE-discovered features capture genuine causal relationships in model behavior rather than mere correlations. 
The ability to reliably manipulate these features while maintaining semantic coherence demonstrates that vision transformers learn decomposable, interpretable representations even without explicit supervision.

However, significant challenges remain. 
Current methods for identifying manipulable features still require manual exploration, and the relationship between feature interventions and model behavior becomes increasingly complex for higher-level tasks. 
Future work should focus on automating feature discovery, understanding feature interactions across different model architectures, and extending reliable control to more sophisticated visual reasoning tasks.

More broadly, our results suggest that combining interpretation with control may be essential for developing truly trustworthy AI systems. 
Just as scientific understanding emerges from our ability to both explain and manipulate natural phenomena, meaningful understanding of neural networks requires tools that unite explanation with experimentation. 
We believe SAEs provide a promising foundation for building such tools.

% \section{Limitations}


% \begin{enumerate}
%     \item{Feature interventions are not linear. As you increase the interventions beyond the observed values, you produce artifacts in the downstream modules.}
% \end{enumerate}

% These experiments demonstrate that SAEs discover semantically meaningful, spatially coherent features that can be precisely controlled. 
% However, several limitations and open questions remain. 
% First, while our interface enables precise manipulation of individual features, discovering which features to manipulate currently requires manual exploration. 
% Future work could automate this process by learning mappings between natural language descriptions and SAE features. 
% Second, our qualitative evaluation approach, while effective for demonstrating control, makes it difficult to systematically compare against other methods or across model architectures. 
% Developing quantitative metrics that capture both semantic meaningfulness and controllability remains an open challenge. 
% Finally, while we show successful feature control on semantic segmentation, the relationship between SAE features and model behavior becomes more complex for higher-level tasks like image captioning or visual question answering.

% Our results suggest that vision transformers learn to decompose images into meaningful, manipulable components even without explicit supervision. 
% This raises intriguing questions about the relationship between self-supervised learning objectives and human-interpretable concepts. 
% Future work could explore how different pre-training approaches affect the semantic organization of these learned features, potentially informing the design of more interpretable vision models.

\section*{Acknowledgment}
We thank colleagues from the Imageomics Institute and the OSU NLP group for their constructive feedback. This research is supported by National
Science Foundation OAC-2118240. We are thankful for the generous support of the computational resources by the Ohio Supercomputer Center.

\clearpage
\bibliography{main}
\bibliographystyle{icml2025}

\clearpage
\appendix
\onecolumn

\setcounter{table}{0}
\renewcommand\thetable{\Alph{section}\arabic{table}}
\setcounter{figure}{0}
\renewcommand\thefigure{\Alph{section}\arabic{figure}}

\section*{Appendices}

We provide additional details omitted in the main text:
\begin{enumerate}[nosep]
    \item{\cref{app:training-details}: SAE Training Details (for \cref{sec:sae-training})}
    \item{\cref{app:additional}: Additional Examples (for \cref{sec:understanding})}
    \item{\cref{app:classification-details}: Classification Details (for \cref{sec:classification})}
    \item{\cref{app:semseg-details}: Classification Details (for \cref{sec:semseg})}
\end{enumerate}

\section{SAE Training Details}\label{app:training-details}

Our training code is \href{https://github.com/OSU-NLP-Group/SAE-V/blob/main/saev/training.py}{publicly available}, along with instructions to \href{https://osu-nlp-group.github.io/SAE-V/saev/#guide-to-training-saes-on-vision-models}{train your own SAEs for ViTs}.
Below, we describe the technical details necessary to re-implement our work.

We save all patch-level activation vectors for a given dataset from layer $l$ of a pre-trained ViT to disk, stored in a sharded format in 32-bit floating points.
Future work should explore lower-precision storage.
In practice, we explore $12$-layer ViTs and record activations from layer $11$ after all normalization.

We then train a newly initialized SAE on \num{100}M activations.
$W_\text{enc}$ and $W_\text{dec}$ are initialized using PyTorch's \texttt{kaiming\_uniform\_} initialization and $b_\text{enc}$ is zero-initialized.
$b_\text{dec}$ is initialized as the mean of \num{524288} random samples from the dataset, as recommended by prior work \citep{templeton2024scaling,templeton2024update}.

We use mean squared error $||\mathbf{\hat{x}} - \mathbf{x}||_2^2$ as our reconstruction loss, L1 length of $f(\mathbf{x})$ scaled by the current sparsity coefficient $\lambda$ as our sparsity loss, and track L0 throughout training.

The learning rate $\eta$ and sparsity coefficient $\lambda$ are linearly scaled from $0$ to their maximum over 500 steps each and remain at their maximum for the duration of training.

Columns of $W_\text{dec}$ are normalized to unit length after every gradient update, and gradients parallel to the columns are removed before gradient updates.

\begin{table}[t]
    \centering
    \small
    \begin{tabular}{lr}
        \toprule
        Hyperparameter & Value \\ 
        \midrule
        Hidden Width & \num{24576} ($32\times$ expansion) \\
        Sparsity Coefficient $\lambda$ & $\{4\times10^{-4},8\times10^{-4},1.6\times10^{-3}\}$ \\
        Sparsity Coefficient Warmup & \num{500} steps \\
        Batch Size & \num{16384} \\
        Learning Rate $\eta$ & $\{3\times10^{-4},1\times10^{-3},3\times10^{-3}\}    $ \\
        Learning Rate Warmup & \num{500} steps \\
        \bottomrule
    \end{tabular}
    \caption{Hyperparameters for training SAEs. Sparsity coefficient $\lambda$ and learning rate $\eta$ are chosen qualitatively by inspecting discovered features.}
    \label{tab:hyperparameters}
\end{table}

\section{Additional Examples}\label{app:additional}

Additional examples of CLIP learning cultural features are in \cref{fig:additional-clip-vs-dinov2-cultural}.

\begin{figure*}[t]
    \centering
    \small
    \setlength{\tabcolsep}{1pt}
    \begin{tabular}{ccccc}
        \multicolumn{4}{c}{\texttt{CLIP-24K/7622}: ``United States of America''} & \textit{Not} USA \\ % \multirow{3}{*}{\includegraphics[width=24.82pt]{figures/clip-vs-dinov2/legend.png}} \\ 
        \cmidrule(lr){1-4} \cmidrule(lr){5-5}
        \includegraphics[width=0.18\textwidth]{figures/clip-vs-dinov2/clip-usa-positive-1.png} & 
        \includegraphics[width=0.18\textwidth]{figures/clip-vs-dinov2/clip-usa-positive-2.png} & 
        \includegraphics[width=0.18\textwidth]{figures/clip-vs-dinov2/clip-usa-positive-3.png} & 
        \includegraphics[width=0.18\textwidth]{figures/clip-vs-dinov2/clip-usa-positive-5.png} &
        \includegraphics[width=0.18\textwidth]{figures/clip-vs-dinov2/clip-usa-negative-1.png} \\
        \multicolumn{4}{c}{\texttt{CLIP-24K/13871}: ``Germany''} & \textit{Not} Germany \\ % \multirow{3}{*}{\includegraphics[width=24.82pt]{figures/clip-vs-dinov2/legend.png}} \\ 
        \cmidrule(lr){1-4} \cmidrule(lr){5-5}
        \includegraphics[width=0.18\textwidth]{figures/clip-vs-dinov2/clip-13871-positive-1.png} & 
        \includegraphics[width=0.18\textwidth]{figures/clip-vs-dinov2/clip-13871-positive-2.png} & 
        \includegraphics[width=0.18\textwidth]{figures/clip-vs-dinov2/clip-13871-positive-3.png} & 
        \includegraphics[width=0.18\textwidth]{figures/clip-vs-dinov2/clip-13871-positive-5.png} &
        \includegraphics[width=0.18\textwidth]{figures/clip-vs-dinov2/clip-13871-negative-2.png}
    \end{tabular}
    \caption{
        Additional examples of cultural features learned by CLIP.
        \textbf{Top:} \texttt{CLIP-24K/7622} responds to symbolism from the United States of America, including a portrait of George Washington, but not to a portrait of King Louis XIV of France.
        \textbf{Bottom:} \texttt{CLIP-24K/13871} activates strongly on the Brandenburg Gate and other German symbols, but not on visually similar flags like the Belgian flag.}\label{fig:additional-clip-vs-dinov2-cultural}
\end{figure*}

% \begin{figure*}[t]
%     \centering
%     \small
%     \setlength{\tabcolsep}{1pt}
%     \begin{tabular}{ccccccccc}
%         \multicolumn{4}{c}{(a) \texttt{CLIP-24K/13871} ``Germany''} & \multicolumn{2}{c}{(b) \textit{Not} Germany} & \multicolumn{2}{c}{(c) ImageNet-1K Examples} & \multirow{3}{*}{\includegraphics[width=17.49pt]{figures/clip-vs-dinov2/legend.png}} \\ 
%         \cmidrule(lr){1-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
%         \includegraphics[width=0.112\textwidth]{figures/clip-vs-dinov2/clip-13871-positive-1.png} & 
%         \includegraphics[width=0.112\textwidth]{figures/clip-vs-dinov2/clip-13871-positive-2.png} & 
%         \includegraphics[width=0.112\textwidth]{figures/clip-vs-dinov2/clip-13871-positive-3.png} & 
%         \includegraphics[width=0.112\textwidth]{figures/clip-vs-dinov2/clip-13871-positive-5.png} &
%         \includegraphics[width=0.112\textwidth]{figures/clip-vs-dinov2/clip-13871-negative-1.png} &
%         \includegraphics[width=0.112\textwidth]{figures/clip-vs-dinov2/clip-13871-negative-2.png} &
%         \includegraphics[width=0.112\textwidth]{figures/clip-vs-dinov2/clip-13871-example-1.png} &
%         \includegraphics[width=0.112\textwidth]{figures/clip-vs-dinov2/clip-13871-example-2.png} \\[12pt]
%         \multicolumn{4}{c}{(d) \texttt{DINOv2-24K/1204}} & \multicolumn{2}{c}{(e) \textit{Not} Germany} & \multicolumn{2}{c}{(f) ImageNet-1K Examples} \\ 
%         \cmidrule(lr){1-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
%         \includegraphics[width=0.112\textwidth]{figures/clip-vs-dinov2/dinov2-1204-positive-1.png} & 
%         \includegraphics[width=0.112\textwidth]{figures/clip-vs-dinov2/dinov2-1204-positive-2.png} & 
%         \includegraphics[width=0.112\textwidth]{figures/clip-vs-dinov2/dinov2-1204-positive-3.png} & 
%         \includegraphics[width=0.112\textwidth]{figures/clip-vs-dinov2/dinov2-1204-positive-5.png} &
%         \includegraphics[width=0.112\textwidth]{figures/clip-vs-dinov2/dinov2-1204-negative-1.png} &
%         \includegraphics[width=0.112\textwidth]{figures/clip-vs-dinov2/dinov2-1204-negative-2.png} &
%         \includegraphics[width=0.112\textwidth]{figures/clip-vs-dinov2/dinov2-1204-example-1.png} &
%         \includegraphics[width=0.112\textwidth]{figures/clip-vs-dinov2/dinov2-1204-example-2.png} 
%     \end{tabular}
%     \caption{
%         CLIP learns robust cultural visual features. Highlighted patches indicate feature activation strength.
%         \textbf{Top Left (a):} The ``Germany'' feature (\texttt{CLIP-24K/13871}) activates strongly on the Brandenburg Gate other German symbols.
%         \textbf{Top Middle (b):} \texttt{CLIP-24K/13871} remains inactive on unrelated European symbols like the Eiffel Tower or the Belgian flag. 
%         \textbf{Top Right (c):} Maximally activating images for \texttt{CLIP-24K/13871} in ImageNet-1K are both German flags.
%         \textbf{Bottom Left (d):} 
%         \textbf{Bottom Middle (e):}
%         \textbf{Bottom Right (f):} 
%     }\label{fig:additional-clip-vs-dinov2-cultural}
% \end{figure*}

% \subsection{Additional Semantic Examples}\label{app:additional-clip-vs-dinov2-semantic}

% \todo{Fill this in.}


\section{Classification Details}\label{app:classification-details}

Detailed instructions for reproducing our results using our public codebase are \href{https://osu-nlp-group.github.io/SAE-V/contrib/classification}{available on the web}.

We use an SAE trained on \num{100}M CLIP ViT-B/\num{16} activations from layer \num{11} on ImageNet-1K's training split of \num{1.2}M images following the procedure in \cref{app:training-details}.

\subsection{Task-Specific Decoder Training (Image Classification)}

For image classification, we train a linear probe on the [CLS] token representations extracted from a CLIP-pretrained ViT-B/16 model \citep{radford2021learning} using the CUB-2011 dataset \citep{wah2011cub}. The following details describe our experimental setup to ensure exact reproducibility.

\paragraph{Data Preprocessing.}
Each input image is first resized so that its shorter side is 256 pixels, followed by a center crop to obtain a 224$\times$224 image. The resulting image is then normalized using the standard ImageNet mean and standard deviation for RGB channels. No additional data augmentation is applied.

\paragraph{Feature Extraction.}
We use a CLIP-pretrained ViT-B/16 model. From the final layer of the ViT, we extract the [CLS] token representation. The backbone is kept frozen during training, and no modifications are made to the extracted features.

\paragraph{Classification Head.}
The classification head is a linear layer mapping the [CLS] token’s feature vector to logits corresponding to the 200 classes in the CUB-2011 dataset.

\paragraph{Training Details.}
We train the linear probe on the CUB-2011 training set using the AdamW \citep{loshchilov2017adamw} optimizer for 20 epochs with a batch size of 512. 
We performed hyperparameter sweeps over the learning rate with values in \{$10^{-5}$, $10^{-4}$, $10^{-3}$, $10^{-2}$, $10^{-1}$, $1.0$\} and over weight decay with values in \{$0.1$, $0.3$, $1.0$, $3.0$, $10.0$\}. 
Based on validation accuracy, we selected a learning rate of $10^{-3}$ and a weight decay of 0.1. The CLIP backbone remains frozen during this process.
The trained linear probe achieves a final accuracy of \num{79.9}\% on the CUB-2011 validation set.

\subsection{Additional Examples}

We provide additional examples of observing classification predictions, using SAEs to form a hypothesis explaining model behavior, and experimentally validating said hypothesis through feature suppression in \cref{fig:classification-extra1,fig:classification-extra2,fig:classification-extra3}.
Each figure links to a live, web-based demo where readers can complete the ``observe, hypothesize, experiment'' sequence themselves.

\begin{figure*}
    \centering
    \small
    \includegraphics[width=\linewidth]{figures/classification-extra1.jpg}
    \caption{Tropical Kingbirds have a distinctive yellow chest. When we suppress a ``yellow feathers'' feature, our linear classifier predicts Gray Kingbird, a similar species but with a gray chest. This example is available at \href{https://osu-nlp-group.github.io/SAE-V/demos/classification?example=5099}{\url{https://osu-nlp-group.github.io/SAE-V/demos/classification?example=5099}}}\label{fig:classification-extra1}
\end{figure*}
\begin{figure*}
    \centering
    \small
    \includegraphics[width=\linewidth]{figures/classification-extra2.jpg}
    \caption{Canada Warblers have a distinctive black necklace on the chest. \texttt{CLIP-24K/20376} fires on similar patterns; when we suppress this feature, the linear classifier predicts Wilson Warbler, a similar species without the distinctive black necklace. This example is available at \href{https://osu-nlp-group.github.io/SAE-V/demos/classification?example=1129}{\url{https://osu-nlp-group.github.io/SAE-V/demos/classification?example=1129}}}\label{fig:classification-extra2}
\end{figure*}
\begin{figure*}
    \centering
    \small
    \includegraphics[width=\linewidth]{figures/classification-extra3.jpg}
    \caption{Purple finches have bright red coloration on the head and neck area; when we suppress \texttt{CLIP-24K/10273}, which appears to be a ``red feathers'' feature, our classifier predicts Field Sparrow, which has similar wing banding but no red coloration. 
    This example is available at \href{https://osu-nlp-group.github.io/SAE-V/demos/classification?example=4139}{\url{https://osu-nlp-group.github.io/SAE-V/demos/classification?example=4139}}}\label{fig:classification-extra3}
\end{figure*}
% \begin{figure*}
%     \centering
%     \small
%     \includegraphics[width=\linewidth]{figures/classification-extra4.pdf}
%     \caption{\href{https://osu-nlp-group.github.io/SAE-V/demos/classification?example=5099}{\url{https://osu-nlp-group.github.io/SAE-V/demos/classification?example=5269}}}\label{fig:classification-extra4}
% \end{figure*}

\section{Semantic Segmentation Details}\label{app:semseg-details}

Detailed instructions for reproducing our results using our public codebase are \href{https://osu-nlp-group.github.io/SAE-V/contrib/semseg}{available on the web}.

\subsection{Task-Specific Decoder Training (Semantic Segmentation)}

For semantic segmentation, we train a single linear segmentation head on features extracted from a frozen DINOv2-pretrained ViT-B/14 model \citep{oquab2023dinov2} using the ADE20K dataset \citep{zhou2017ade20k}. 
We aim to map patch-level features to 151 semantic class logits. 
The following details describe our experimental setup to ensure exact reproducibility.

\paragraph{Data Preprocessing.}
Each image is first resized so that its shorter side is 256 pixels, followed by a center crop to obtain a 224$\times$224 image. The cropped images are normalized using the standard ImageNet mean and standard deviation for RGB channels. No additional data augmentation is applied.

\paragraph{Feature Extraction.}
We use a DINOv2-pretrained ViT-B/14 with 224$\times$224 images, which results in a $16\times16$ grid of $14\times14$ pixel patches. 
The final outputs from the ViT (after all normalization layers) are used as features. 
We exclude the [CLS] token and any register tokens, retaining only the patch tokens, thereby producing a feature tensor of shape 1$6\times16\times d$, where $d=768$ is the ViT's output feature dimension.

\paragraph{Segmentation Head.}
The segmentation head is a linear layer applied independently to each patch token. 
This layer maps the $d$-dimensional feature vector to a \num{151}-dimensional logit vector, corresponding to the \num{151} semantic classes. 
For visualization purposes, we perform a simple upsampling by replicating each patch prediction to cover the corresponding $14\times14$ pixel block. 
For quantitative evaluation (computing mIoU), the $16\times16$ logit map is bilinearly interpolated to the full image resolution.

\paragraph{Training Details.}
We train the segmentation head using the standard cross entropy loss. 
The DINOv2 ViT-B/14 backbone remains frozen during training, so that only the segmentation head is updated. 
We use AdamW \citep{loshchilov2017adamw} with a batch size of \num{1024} over \num{400} epochs. 
We performed hyperparameter sweeps over the learning rate with values in \{$3\times10^{-5}$, $1\times10^{-4}$, $3\times10^{-4}$, $1\times10^{-3}$, $3\times10^{-3}$, $1\times10^{-2}$\} and over weight decay values in \{$1\times10^{-1}$, $1\times10^{-3}$, $1\times10^{-5}$\}. No learning rate schedule is applied; the chosen learning rate is kept constant throughout training. The segmentation head is initialized with PyTorch's default initialization.

\paragraph{Evaluation.}
For evaluation, the predicted $16\times16$ logit maps are upsampled to the full image resolution using bilinear interpolation before computing the mean Intersection-over-Union (mIoU) with the ground-truth segmentation masks. Our final segmentation head achieves an mIoU of \num{35.1} on the ADE20K validation set.

\subsection{Additional Examples}

We provide additional examples of observing segmentation predictions, using SAEs to form a hypothesis explaining model behavior, and experimentally validating said hypothesis through feature suppression in \cref{fig:semseg-extra1,fig:semseg-extra2,fig:semseg-extra3,fig:semseg-extra4}.
These additional examples further support the idea of SAEs discovering features that are pseduo-orthogonal.
Each figure links to a live, web-based demo where readers can complete the ``observe, hypothesize, experiment'' sequence themselves.

\begin{figure*}
    \centering
    \small
    \includegraphics[width=\linewidth]{figures/semseg-extra1.pdf}
    \caption{DINOv2 correctly identifies the framed painting. We find that \texttt{DINOv2-24K/16446} fires for paintings, and that suppressing this feature removes the painting without meaningfully affecting other parts of the image, despite modifying all patches. This example is available at \href{https://osu-nlp-group.github.io/SAE-V/demos/semseg?example=1633}{\url{https://osu-nlp-group.github.io/SAE-V/demos/semseg?example=1633}}.}\label{fig:semseg-extra1}
\end{figure*}

\begin{figure*}
    \centering
    \small
    \includegraphics[width=\linewidth]{figures/semseg-extra2.pdf}
    \caption{We find that feature \texttt{DINOv2-24K/5876} fires for toilets, and that suppressing this feature removes the toilet without meaningfully affecting other parts of the image, despite modifying all patches. This example is available at \href{https://osu-nlp-group.github.io/SAE-V/demos/semseg?example=1099}{\url{https://osu-nlp-group.github.io/SAE-V/demos/semseg?example=1099}}.}\label{fig:semseg-extra2}
\end{figure*}

\begin{figure*}
    \centering
    \small
    \includegraphics[width=\linewidth]{figures/semseg-extra3.pdf}
    \caption{After suppressing a ``bed'' feature (\texttt{DINOv2-24K/18834}), the segmentation head predicts ``pillow'' for the pillows and ``table'' for the bed spread. This examples is available at \href{https://osu-nlp-group.github.io/SAE-V/demos/semseg?example=1117}{\url{https://osu-nlp-group.github.io/SAE-V/demos/semseg?example=1117}}.}\label{fig:semseg-extra3}
\end{figure*}

\begin{figure*}
    \centering
    \small
    \includegraphics[width=\linewidth]{figures/semseg-extra4.pdf}
    \caption{We remove all cars from the scene by suppressing a ``car''-like feature (\texttt{DINOv2-24K/7235}). Notably, the van on the right of the image is also removed. This examples is available at \href{https://osu-nlp-group.github.io/SAE-V/demos/semseg?example=1852}{\url{https://osu-nlp-group.github.io/SAE-V/demos/semseg?example=1852}}.}\label{fig:semseg-extra4}
\end{figure*}


\end{document}