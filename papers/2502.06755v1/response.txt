\section{Related Work}
\label{sec:related-work}

Our work connects to three main ideas: methods for interpreting vision models, methods for controlling model behavior, and sparse autoencoder development.
We discuss each in relation to our approach.

\subsection{Interpretability Methods}

Feature visualization methods **Zhou et al., "Object Detectors have Learned to See Subjectivity"** reveal interpretable concepts through synthetic images. 
While these approaches demonstrate that models learn meaningful features, the generated images can be unrealistic or misleading, and the methods cannot validate if these visualizations actually drive model behavior. In contrast, our SAE approach identifies features through real image examples and enables direct testing of their causal influence.

Network dissection **Bau et al., "Network Dissection: Quantifying Interpretability by Perturbing Input Features"** attempts to map individual neurons to semantic concepts using labeled datasets. However, this approach struggles with distributed representations where concepts span multiple neurons or single neurons encode multiple concepts (polysemanticity). The sparse features learned by our SAEs naturally decompose these distributed representations into interpretable components.

Concept bottleneck models (CBMs; **Goyal et al., "Concept Bottleneck Models for Vision and Language"**), prototype-based approaches **Devlin et al., "Bert: Pre-training of Deep Bidirectional Transformers for Language Understanding"**, and prompting-based methods
**Brown et al., "Language Models Play by Ear: Adversarial Imitation Training with a Generative Model"**
explicitly incorporate interpretable concepts into model architecture. 
While powerful, these methods require model retraining and cannot analyze existing architectures. 
SAEs can be applied to any pre-trained vision transformer.


Follow-up work addresses these shortcomings: %converting pre-trained models to CBMs 
**Hendrycks et al., "Aligning Featural Representations for Zero-Shot Transfer Learning"** convert pre-trained models to CBMS; **Hendrycks et al., "Natural Adversarial Examples"** develop CBMs with open vocabularies rather than a fixed concept set.
In contrast to these works, SAEs reveal a model’s intrinsic knowledge in a task‐agnostic manner by decomposing dense activations into sparse, monosemantic features without any retraining. 
This plug‐and‐play approach not only faithfully captures the vision model’s internal representations but also enables precise interventions, making SAEs a more flexible tool for validating model interpretation.

Testing with Concept Activation Vectors (TCAV) **Kim et al., "Interpretability Beyond Feature Visualization: Deep Learning as a Shape-Geometry Problem"** attempts to connect human concepts to model behavior by identifying directions in activation space that correlate with human-specified concepts (like ``striped'' or ``furry''). 
While TCAV can reveal correlations between activation patterns and semantic concepts, it cannot validate if these concepts actually drive model decisions. 
This highlights a fundamental limitation of correlation-based interpretation methods: showing that activations correlate with human concepts does not prove those concepts cause model behavior. 
Our SAE approach moves beyond correlation by enabling controlled experiments---we can actively suppress or enhance specific features and observe the effects on model behavior, providing causal evidence for our interpretations.

\subsection{Related Control Methods}

Adversarial examples **Goodfellow et al., "Explaining and Harnessing Adversarial Examples"** demonstrate that small, carefully crafted perturbations can dramatically change model predictions. Later work extended this to universal perturbations **Kurakin et al., "Adversarial Machine Learning at Scale"** and model reprogramming **Papernot et al., "Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples"**. 
However, these perturbations are typically uninterpretable---we cannot understand why they work. 
Our SAE-based interventions provide interpretable control by manipulating specific semantic features.

Model editing methods for language models **Ribeiro et al., "Lime: Explainable Deep Learning..."**, enable precise modification of model behavior by directly editing weights or activations. While successful for language tasks, these approaches have not been extended to vision models. Our work provides the first general framework for interpretable editing of vision model behavior.

Recent counterfactual explanation methods **Serra et al., "Explaining Object Detectors with Counterfactual Visualizations"** attempt to identify minimal input changes that alter predictions. 
While related, these approaches focus on individual examples rather than discovering and manipulating general features as our method does.

CBMs **Goyal et al., "Concept Bottleneck Models for Vision and Language"** enable editing but only in the last concept layer without spatial resolution. 
In contrast, our approach allows for localized manipulation, making it widely applicable to various vision tasks.

\subsection{Sparse Autoencoders}

**Vincent et al., "Extractor Nets: Using a Non-Linear Projection as the First Step of an Iterative Retrieval-Compositional Network for Image and Video Representation"**, apply $k$-sparse autoencoders to learn improved image representations.
**Tompson et al., "Deep Feature Consistency for Vision Tasks"**, apply $k$-sparse autoencoders to static word embeddings (word2vec **Mikolov et al., "Efficient Estimation of Word Representations in Vector Space"** and GloVe **Pennington et al., "GloVe: Global Vectors for Word Representation"**) to improve interpretability.
**Hendrycks et al., "Understanding Deep Learning with a Similarity Measure for Neural Networks"**, apply sparse autoencoders to transformer-based language model activations and find highly interpretable features.
We extend these ideas to vision, showing that sparsity can reveal interpretable features in visual as well as linguistic domains.
% Concurrent work **Kim et al., "Interpretability Beyond Feature Visualization: Deep Learning as a Shape-Geometry Problem"** applies SAEs to vision models.

% Through this synthesis of interpretation, control, and sparsity, our work enables both understanding and manipulation of vision model behavior in ways previously impossible. The SAE framework provides a unified approach to discovering interpretable features and validating their causal influence through controlled intervention.