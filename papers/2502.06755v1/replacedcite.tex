\section{Related Work}
\label{sec:related-work}

Our work connects to three main ideas: methods for interpreting vision models, methods for controlling model behavior, and sparse autoencoder development.
We discuss each in relation to our approach.

\subsection{Interpretability Methods}

Feature visualization methods ____ reveal interpretable concepts through synthetic images. 
While these approaches demonstrate that models learn meaningful features, the generated images can be unrealistic or misleading, and the methods cannot validate if these visualizations actually drive model behavior. In contrast, our SAE approach identifies features through real image examples and enables direct testing of their causal influence.

Network dissection ____ attempts to map individual neurons to semantic concepts using labeled datasets. However, this approach struggles with distributed representations where concepts span multiple neurons or single neurons encode multiple concepts (polysemanticity). The sparse features learned by our SAEs naturally decompose these distributed representations into interpretable components.

Concept bottleneck models (CBMs; ____), prototype-based approaches ____, and prompting-based methods
____ explicitly incorporate interpretable concepts into model architecture. 
While powerful, these methods require model retraining and cannot analyze existing architectures. 
SAEs can be applied to any pre-trained vision transformer.


Follow-up work addresses these shortcomings: %converting pre-trained models to CBMs 
____ convert pre-trained models to CBMS; ____ develop CBMs with open vocabularies rather than a fixed concept set.
In contrast to these works, SAEs reveal a model’s intrinsic knowledge in a task‐agnostic manner by decomposing dense activations into sparse, monosemantic features without any retraining. 
This plug‐and‐play approach not only faithfully captures the vision model’s internal representations but also enables precise interventions, making SAEs a more flexible tool for validating model interpretation.

Testing with Concept Activation Vectors (TCAV) ____ attempts to connect human concepts to model behavior by identifying directions in activation space that correlate with human-specified concepts (like ``striped'' or ``furry''). 
While TCAV can reveal correlations between activation patterns and semantic concepts, it cannot validate if these concepts actually drive model decisions. 
This highlights a fundamental limitation of correlation-based interpretation methods: showing that activations correlate with human concepts does not prove those concepts cause model behavior. 
Our SAE approach moves beyond correlation by enabling controlled experiments---we can actively suppress or enhance specific features and observe the effects on model behavior, providing causal evidence for our interpretations.

\subsection{Related Control Methods}

Adversarial examples ____ demonstrate that small, carefully crafted perturbations can dramatically change model predictions. Later work extended this to universal perturbations ____ and model reprogramming ____. 
However, these perturbations are typically uninterpretable---we cannot understand why they work. 
Our SAE-based interventions provide interpretable control by manipulating specific semantic features.

Model editing methods for language models ____ enable precise modification of model behavior by directly editing weights or activations. While successful for language tasks, these approaches have not been extended to vision models. Our work provides the first general framework for interpretable editing of vision model behavior.

Recent counterfactual explanation methods ____ attempt to identify minimal input changes that alter predictions. 
While related, these approaches focus on individual examples rather than discovering and manipulating general features as our method does.

CBMs ____ enable editing but only in the last concept layer without spatial resolution. 
In contrast, our approach allows for localized manipulation, making it widely applicable to various vision tasks.

\subsection{Sparse Autoencoders}

____ apply $k$-sparse autoencoders to learn improved image representations.
____ apply $k$-sparse autoencoders to static word embeddings (word2vec ____ and GloVe ____) to improve interpretability.
____ apply sparse autoencoders to transformer-based language model activations and find highly interpretable features.
We extend these ideas to vision, showing that sparsity can reveal interpretable features in visual as well as linguistic domains.
% Concurrent work ____ applies SAEs to vision models.

% Through this synthesis of interpretation, control, and sparsity, our work enables both understanding and manipulation of vision model behavior in ways previously impossible. The SAE framework provides a unified approach to discovering interpretable features and validating their causal influence through controlled intervention.