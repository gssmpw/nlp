
\clearpage

\onecolumn
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thetable}{S\arabic{table}}
\renewcommand{\thesection}{S\arabic{section}}
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{section}{0}
{
    \centering
    \Large
    \textbf{Introducing Visual Perception Token into Multimodal Large Language Model\\\textit{- Supplementary Material -}}\\
    \vspace{1.0em}
}

\section{Implement Details}
\subsection{Training Details}
Our training process consists of two phases: alignment and finetuning. 
The alignment stage aligns the additional vision features with the LLM embeddings. If the original vision encoder is used for re-encoding, the alignment stage is omitted. We use the same image-text pair data for the LLaVA 1.5 alignment, and only use the additional vision branch as the LLM's input. During training, all components except the projector are frozen. In this phase, we train the model for 1 epoch with a learning rate of 2e-3 and a batch size of 128.
The second finetuning stage allows the model to learn to output the correct Region Selection Tokens and to transmit information through the Vision Re-Encoding Tokens. We finetune the model using our constructed  dataset, as well as remaining samples from the LLaVA 1.5 finetuning dataset that were not included in our dataset. In this stage, all components except the original visual encoder and the additional vision encoder are unfrozen. In this phase, we train the model for 1 epoch with a learning rate of 2e-5 and a batch size of 256. For both the first and the second phase, we use AdamW optimizer. The experiments are deployed on 8 A100 GPU. The total training time is about 20 hours. For the 7B model, the rank of the LoRA is set to 512.

\subsection{Evaluation Prompt}
Following established practices~\cite{mmvet,llava15}, we used GPT-4o (2024-08-06) to evaluate the alignment between the model's responses and the ground truth for each question. We use the evaluation prompt in \cite{shao2024visual}.
\begin{tcolorbox}[parbox=false,colback=fullgreen!10, colframe=fullgreen!50, title=Evaluation Prompt, coltitle=black]
You are responsible for proofreading the answers, you need to give a score to the model's answer by referring to the standard answer, based on the given question. The full score is 1 point and the minimum score is 0 points. Please output the score in the form 'score: $<$score$>$'. The evaluation criteria require that the closer the model's answer is to the standard answer, the higher the score.

\noindent Question:  $<$question$>$

\noindent Ground Truth:  $<$ground truth$>$

\noindent Answer:  $<$answer$>$
\end{tcolorbox}

\subsection{Template of the Training Data Examples}
Here, we show the format of our training examples. 
The training example for the Region Selection Token is essentially the same as the samples used in \cite{shao2024visual}, except that the method for representing regions has changed from bounding boxes to region tokens.  
The training example for the Vision Re-Encoding Token is almost identical to the data in the original LLava~\cite{llava} fine-tuning dataset, with the only difference being the insertion of an additional round of dialogue between the original question and answer. This added dialogue includes the Vision Re-Encoding Token.  
\begin{tcolorbox}[parbox=false,colback=fullgreen!10, colframe=fullgreen!50, title=Template of Training Example for Region Selection Token, coltitle=black]
\textbf{User}: $<$image$>$ $<$question$>$ Please identify the region that can help you answer the question better, and then answer the question.

\noindent\textbf{Assistant}: $<$Region\_Selection\_Start$>$ $<$x\_min$>$ $<$y\_min$>$ $<$x\_max$>$ $<$y\_max$>$ $<$Region\_Selection\_End$>$.

\noindent\textbf{User}: $<$image$>$

\noindent\textbf{Assistant}: $<$ground truth$>$
\end{tcolorbox}
\begin{tcolorbox}[parbox=false,colback=fullgreen!10, colframe=fullgreen!50, title=Template of Training Example for Vision Re-Encoding Token, coltitle=black]
\textbf{User}: $<$image$>$ $<$question$>$ Please require additional perception features, and then answer the question.

\noindent\textbf{Assistant}: $<$Re-Encoding\_Start$>$  $<$Re-Encoding\_Control$>$ $<$Re-Encoding\_End$>$.

\noindent\textbf{User}: $<$image$>$

\noindent\textbf{Assistant}: $<$ground truth$>$
\end{tcolorbox}

The training for the free-choice experiment differs from other experiments only in the sample template. For the free-choice experiment, we removed the additional prompt from the questions. The training sample template is as follows.

\begin{tcolorbox}[parbox=false,colback=fullgreen!10, colframe=fullgreen!50, title=Template of Training Example for Region Selection Token (Free Choice), coltitle=black]
\textbf{User}: $<$image$>$ $<$question$>$

\noindent\textbf{Assistant}: $<$Region\_Selection\_Start$>$ $<$x\_min$>$ $<$y\_min$>$ $<$x\_max$>$ $<$y\_max$>$ $<$Region\_Selection\_End$>$.

\noindent\textbf{User}: $<$image$>$

\noindent\textbf{Assistant}: $<$ground truth$>$
\end{tcolorbox}
\begin{tcolorbox}[parbox=false,colback=fullgreen!10, colframe=fullgreen!50, title=Template of Training Example for Vision Re-Encoding Token (Free Choice), coltitle=black]
\textbf{User}: $<$image$>$ $<$question$>$

\noindent\textbf{Assistant}: $<$Re-Encoding\_Start$>$  $<$Re-Encoding\_Control$>$ $<$Re-Encoding\_End$>$.

\noindent\textbf{User}: $<$image$>$

\noindent\textbf{Assistant}: $<$ground truth$>$
\end{tcolorbox}

\section{Supplementary Experiments}
We conducted experiments on the MME~\cite{mme} and MM-Bench~\cite{mmb} benchmarks without using the Visual Perception Token, allowing the model to generate answers directly. This assessed the impact of our fine-tuning on general benchmarks. Results in \cref{tab:benchmark} show that our model does not cause degeneration and even improves performance on these benchmarks. 
\input{tab/benchmark}

To verify the advantage of the Region Selection Token over direct BBox prediction, we compared the predicted regions with ground truth using IoU and Intersection over Ground Truth (IoGT), defined as:  
$$(\text{IoGT} = \frac{\text{Area of } (GT \cap \text{Pred})}{\text{Area of } GT}).$$ Results in \cref{tab:iou} show that Region Selection Token significantly outperforms direct BBox prediction in accuracy.  
\input{tab/iou}

% \input{tab/refcoco}
\section{Further Examples}
Here we present additional examples obtained using the visual perception token. \cref{fig:moreexample:g3,fig:moreexample:g4} include the responses generated with the Vision Re-Encoding Token. \cref{fig:moreexample:g1,fig:moreexample:g2} present the responses generated with the Region Selection Token, with the regions selected by the Region Selection Token highlighted in the images.
\input{fig/examples/g3}
\input{fig/examples/g4}
\input{fig/examples/g1}
\input{fig/examples/g2}

\section{Additional Related Works}
\subsection{Reasoning Token} 
In Large Language Model (LLM), there are tokens, similar to Visual Perception Token, which are designed to control the generation process of LLM. These token are termed reasoning tokens or planning token and have recently been introduced in OpenAI's o1 model~\cite{openai_reasoning_guide} and other LLMs.
For example, to enhances models' reasoning capabilities, reasoning tokens were explicitly integrated into OpenAI's o1 models to segment prompts into smaller, manageable parts, exploring multiple response strategies before generating the final output~\cite{openai_reasoning_guide}.  Similar methods aim to incorporate CoT reasoning into language models through planning tokens or distillation techniques. For example, a hierarchical generation framework using planning tokens has been proposed, embedding high-level plans at each reasoning stage with minimal parameter increase~\cite{wang2024guiding}. Moreover, techniques like Rephrase and Respond have been distilled back into models, improving efficiency and accuracy in reasoning, as demonstrated in \cite{distill2to1}.

Our work focuses on MLLMs, where we design visual perception tokens to enhance the visual perception capabilities of MLLMs, not for LLM.
% In contrast, previous approaches in language models have primarily focused on improving the reasoning abilities of LLMs. 
Moreover, our exploration goes beyond LLM reasoning tokens. Unlike these tokens, which merely trigger specific actions and lack the ability to convey detailed instructions or rich information, we focus on designing tokens capable of transmitting nuanced control information for fine-grained visual perception.

\section{Discussion}
\textbf{Adaptability of Visual Perception Token.}
The design of the visual perception token depends on the specific visual perception method. In this paper, we use Crop and the addition of vision features as examples to introduce two types of visual perception tokens. However, our approach can be extended to other visual prompting techniques or visual encoder models, and even to LLM-agent or LLM-tool systems beyond vision.


