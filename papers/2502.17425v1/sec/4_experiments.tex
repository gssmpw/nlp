
\section{Experiments}
\subsection{Main Results}

We used Qwen2-VL-2B or Qwen2-VL-7B~\cite{Qwen2VL} as the base MLLM model and DINOv2~\cite{oquab2024dinov} or SAM~\cite{sam} as the additional vision feature extractor. We use CLIP to denote the case when the original vision branch is used in the re-encoding process. The 2B models are fully fine-tuned, while the 7B model are tuned using LoRA.
The datasets described in \cref{sec:data} were used to finetune the model. 
We then compared its performance with the original Qwen2-VL-2B and Qwen2-VL-7B models.

We evaluated the performance of Qwen2-VL-2B-VPT on the test sets of the datasets used in \cref{sec:data}. When an official test split was available, we used it directly. When no official test split was provided, we randomly split the data into training and testing sets and filter out any images that appeared in both sets to prevent data leakage. Additionally, we also conducted evaluations on Flickr~\cite{flickr}, DUDE~\cite{dude}, and POPE~\cite{pope} datasets, which were not used in training, to assess the generalization capability of our method. 

Following established practices~\cite{mmvet,llava15}, we used GPT-4o (2024-08-06) to evaluate the alignment between the model's responses and the ground truth for each question. A higher score indicates a higher degree of matching, with 0 representing no match and 1 indicating a perfect match. We reported the average matching scores for each dataset. The prompts used in the evaluation are included in the supplementary material.
\input{fig/examples}

The experimental results are presented in \cref{tab:main}. In terms of overall average performance, 2B model with Visual Perception Tokens outperforms the 7B model without Visual Perception Tokens by a significant margin. 
A more detailed analysis is as follows:
(1) The trend of using different vision encoders for re-encoding remains consistent, with the 2B model enhanced by Visual Perception Tokens outperforming the 7B model.  
(2) We implemented two methods for triggering Visual Perception Tokens. The first method explicitly enforces the use of either Region Selection or DINO Feature Tokens across the entire dataset, aligning with real-world scenarios where users may require explicit control. This corresponds to most results in \cref{tab:main}.  
The second allows the model to autonomously decide whether to use Perception Tokens and which type to apply, corresponding to the ``Free Choice'' results in \cref{tab:main}. This ``Free Choice'' mechanism achieves even better performance than the first approach.
(3) The Visual Perception Token shows a clear advantage in Visual Reasoning and Fine-Grained VQA tasks, with improvements of 0.161 and 0.207 over the 7B model, respectively. However, in some datasets for General VQA and Text/OCR-related VQA tasks, the 2B model with Visual Perception Tokens only performs comparably to the 7B model, without a significant performance gain.
(4) The Visual Perception Token remains effective in zero-shot settings. During evaluation, we included three datasets that were not used in training. On these datasets, the 2B-VPT model still outperforms or matches the performance of the 7B model.
(5) The Visual Perception Token is effective for both high- and low-resolution images. On average, when using low-resolution images as input, the 2B+VPT model improves by 0.094 over the 7B model, while for high-resolution images, it shows an improvement of 0.084 over the 7B model.

In \cref{fig:examples}, we show examples demonstrating the effectiveness of Visual Perception Tokens.
% , with additional cases provided in the supplementary material. 
Visual Perception Tokens are particularly effective for several types of queries:
The first category involves locating small regions within large documents or images. The small regions can be signatures, footnotes, page numbers, product brands, logos, or small text on clothing, which are often too small for direct OCR by MLLMs. However, Region Selection Tokens can pinpoint these areas.
The second category addresses hallucination issues. For example, when asked what is next to a dog, text-based hallucination might generate an answer like ``a person''. Region Selection Tokens mitigate such errors by grounding responses in image.
The third category involves identifying or counting objects in complex scenes. Vision Re-Encoding Tokens excel in these tasks because encoding the image twice enhances segmentation and detection.

\subsection{Discussion on Region Selection Token}
In this subsection, we assess the necessity of introducing Region Selection Tokens and examine whether directly using bounding boxes is an effective approach for indicating selected regions. Additionally, we conduct an ablation study on the granularity parameter $k$.

Due to the cost of experiments, we did not train models on all the datasets used in the main experiment for ablation. Instead, we focused on training using the DocVQA, TextVQA, and TextCaps datasets and evaluated on their test splits. This choice was made because, for text/OCR-related VQA tasks, the query-related region is typically small, and the images often contain complex objects that distract the MLLM.  Therefore, the quality of region selection substantially affects the final VQA performance. 
% Consequently, the VQA performance on these datasets serves as a representative benchmark for assessing the quality of region selection. 
We trained five models in total. The first four models employed Region Selection Tokens with $k$ values of 4, 8, 16, and 32, where a larger $k$ indicates finer granularity in the image partition. The fifth model directly used bounding boxes, as described in \cite{shao2024visual}. The results are presented in \cref{tab:ablation_k}. 
% No additional expansion was applied during the cropping process.

The findings are twofold: (1) Region Selection Tokens prove to be more effective than bounding boxes. As shown in the results, models with $k=8$ and $k=16$ significantly outperformed model that used bounding boxes as region indicators. We also observed that when MLLMs utilized bounding boxes, they generated many invalid bounding boxes, such as those where the height or width exceeded the original image dimensions.
(2) There is an optimal granularity $k$. In our experiments, $k=8$ generally yielded the best performance. When $k$ is too small, the cropped region remains large and can still contain complex scenes or multiple objects, which fails to guide the model's attention effectively. Conversely, when $k$ is too large, more new tokens are required, increasing the learning difficulty for the MLLM. Moreover, overly fine-grained region descriptions do not contribute to VQA performance. These two factors together leads to an overall decline in performance.

\subsection{Discussion on Vision Re-Encoding Token}
In this subsection, we validate the effectiveness of the Vision Re-Encoding Token and conduct an ablation on it. 

To verify the effectiveness of the Vision Re-Encoding Token, \cref{tab:validation_dino_token} presents the performance comparison between two model variants. The model with control information (last row in the table) includes the hidden state of the $<$Re-Encoding\_Control$>$ token as an input to the projector, enabling fine-grained control over the embeddings fed into the LLM. 
In contrast, the model without control information  (middle row in the table) uses directly the re-encoded image feature as input to the LLM, after aligning its dimension with the embedding dimension of the LLM. In this experiment, DINO-v2 is used as the additional vision encoder.
The experimental results show that the additional control information carried by the $<$Re-Encoding\_Control$>$ token significantly enhances model performance.

\input{tab/ablation_k}
\input{tab/validation_dino_ctrl}
\input{tab/ablation_no_control_token}
Next, we conduct an ablation study on the number of $<$Re-Encoding\_Control$>$ tokens and mask modeling. Considering the computational cost, in these experiments, we trained  models using only the training data from the CUB Bird dataset and LLaVA Instruction Tuning data from COCO and GQA. 
Results are presented in \cref{tab:ablation_num_dino_ctrl} and indicate the following conclusions.
First, increasing the number of $<$Re-Encoding\_Control$>$ tokens from 1 to 2 provides limited performance gains, while increasing to 4 leads to a decline. This is due to the projector`s lightweight design, making it susceptible to over-parameterization and overfitting.  
Second, introducing masked modeling improves the performance. Further experiments show that fine-tuning the linear layer connecting the hidden states of $<$Re-Encoding\_Control$>$ in the projector for 1 extra epoch further improves performance, confirming that these hidden states encode valuable control information.