\section{Visual Perception Token}

We add two types of extra Visual Perception Tokens to the original MLLM vocabulary. \cref{tab:compare} provides a comparison of their key attributes. Previously, MLLM  could only generate rationales and answers in natural language. However, with the addition of Visual Perception Tokens, the MLLM can now output valuable information in non-natural language form. These Visual Perception Tokens serve primarily as triggers; when a Visual Perception Token is output, the MLLM initiates additional visual perception processes.

\input{fig/bboxtoken}
\input{tab/compare_tokens}

\subsection{Region Selection Tokens} 
\label{sec:31}

Please see \cref{fig:bbox_token} for the relationship between the Region Selection Tokens we used and a precise bbox and see \cref{fig:head:task} for an example when MLLM response with Region Selection Tokens. 

Each group of Region Selection Tokens represents a bounding box (bbox). After the MLLM outputs a group of Region Selection Tokens, the original image will be cropped according to the bbox, preserving only the regions relevant to the query, and then be re-input into the MLLM. This ``crop and re-input'' approach enhances visual perception performance by directly increasing resolution. Although simple, it has been shown to be highly effective~\cite{llava15,shao2024visual}. For example, in document understanding and OCR-related tasks, the region containing the target text is often small, while the original document or image may be large and might even need to be downsized to fit the model input. In such cases, recognizing the query-relevant text becomes very challenging. However, cropping the image and then upsizing the relevant region before inputting it into the model significantly improves task performance.

Given an $h \times w$ image and a rectangular region $R$, a bounding box $[x_{\text{min}}, y_{\text{min}}, x_{\text{max}}, y_{\text{max}}]$ can precisely describe the position of region $R$, where $(x_{\text{min}}, y_{\text{min}})$ and $(x_{\text{max}}, y_{\text{max}})$ represent the coordinates of the top-left and bottom-right pixels of region $R$, respectively. However, precise pixel coordinates are not necessary for VQA tasks. Previous research also shows that MLLMs struggle to interpret and generate precise bounding boxes~\cite{gpt4early}, especially when the input image resolution is not fixed, as is often the case in practice. In such cases, MLLMs struggle to infer the original image resolution from the patchified image embeddings, making it difficult to generate valid bounding boxes.
Therefore, instead of using exact pixel coordinates to describe a region's bounding box, we describe only the approximate location of the region. We divide the $h \times w$ image evenly into a grid of $k \times k$ rectangular cells, with each cell sized $\frac{h}{k} \times \frac{w}{k}$. Each cell can be indexed by its row and column, with the top-left cell indexed as $(0, 0)$ and the top-right cell as $(k-1, 0)$. We use the indices of the cells containing the top-left and bottom-right pixels of region $R$ to describe its location. In our implementation, we set $k=8$.

A group of Region Selection Tokens includes six consecutive tokens: a $<$Region\_Selection\_Start$>$ token, two tokens representing the index of the top-left cell, two tokens representing the index of the bottom-right cell, and a $<$Region\_Selection\_End$>$ token.
To enable the model to better distinguish between horizontal and vertical coordinates, we added $2k$ tokens, $<\!\!x_0\!\!>,\cdots,<\!\!x_{k-1}\!\!>$ and $<\!\!y_0\!\!>,\cdots,<\!\!y_{k-1}\!\!>$, specifically for indicating cell indices. For example, the top-left cell is represented as $<\!\!x_0\!\!><\!\!y_0\!\!>$ and the top-right cell as $<\!\!x_{k-1}\!\!><\!\!y_0\!\!>$.
Given a group of Region Selection Tokens, the pixel coordinates for the top-left and bottom-right corners of the region to be cropped are calculated as $([x_{\text{min}} \times \frac{w}{k}, y_{\text{min}} \times \frac{h}{k})$ and $((x_{\text{max}}+1) \times \frac{w}{k}, (y_{\text{max}}+1) \times \frac{h}{k}])$, respectively. 

Based on this design, Region Selection Tokens have clear and interpretable semantics, indicating specific locations within the image. During training, Region Selection Tokens are learned using the next token prediction loss.

\subsection{Vision Re-Encoding Tokens} 
Please see \cref{fig:head:task} for an example when MLLM response with Vision Re-Encoding Tokens. 

The Vision Re-Encoding Tokens trigger an additional vision encoder, such as DINO, to re-encode the original image, with the resulting vision features processed by a projector before being input into the MLLM. 

Each set of Vision Re-Encoding Tokens consists of three tokens: $<$Re-Encode\_Start$>$ token, $<$Re-Encoding\_Control$>$, and a $<$Re-Encode\_End$>$ token. The hidden state of the $<$Re-Encoding\_Control$>$ token token together with the vision features is input into the projector, allowing further control over the final embedding sequence fed into the LLM. 
To enable $<$Re-Encoding\_Control$>$ to freely convey any control information, we do not calculate the loss at the $<$Re-Encoding\_Control$>$ token during training. As a result, the MLLM's output at the $<$Re-Encoding\_Control$>$ token can be arbitrary. We only require its hidden states to be informative. From this perspective, $<$Re-Encoding\_Control$>$ does not convey clear, interpretable semantics and cannot be decoded into a specific word or phrase. This is also the reason that additional $<$Re-Encode\_Start$>$ and $<$Re-Encode\_End$>$ tokens are used to mark the presence of $<$Re-Encoding\_Control$>$.

\textbf{Mask Modeling}. To enhance the performance of $<$Re-Encoding\_Control$>$ token, we adopt a training approach similar to Masked Language Modeling. During training, 50\% of the samples containing the Vision Re-Encoding Token undergo extra masking.  
We modify the attention mask for these samples, ensuring that the tokens corresponding to the answer in the dialogue can only access the $<$Re-Encoding\_Control$>$ token while being restricted from accessing the tokens corresponding to the original question and image embedding.

\input{fig/forward}
\input{tab/dataset_composition}
\section{MLLM with Visual Perception Token}
\subsection{Architecture}
When the Vision Re-Encoding Token triggers re-encoding, we use either an additional DINO or SAM model or the MLLMâ€™s original vision encoder. In all cases, an extra projector is added to align vision features with LLM embeddings. This projector is a cross-attention module that takes the hidden states of the $<$Re-Encoding\_Control$>$ token as the keys and values, and the vision features as the query. This design enables the hidden states of the $<$Re-Encoding\_Control$>$ token to control the vision features finally input to the LLM. See \cref{fig:forward} for an illustration of the generation process with different types of Visual Perception Tokens. Below, we specifically describe the forward process in the modified MLLM.

An MLLM typically includes a LLM, a vision encoder  $f_v$, and a projector  $g_v$  that connects $f_v$ and the LLM. For an input image  $\bm{x}$ , the image features encoded by  $f_v$  are denoted as  $\bm{z} = f_v(\bm{x})$. After alignment through  $g_v$ , the resulting image embeddings can be represented as  $\bm{h} = g_v(\bm{z})$. These image embeddings are then concatenated with text embeddings to form the input for the LLM.
When the LLM outputs Region Selection Tokens, the cropped image  $\bm{x}'$  is reprocessed through the original vision encoder and projector, resulting in  $\bm{h}' = g_v(f_v(\bm{x}'))$, which is appended to the previous image and text embeddings as input to the LLM.

\input{tab/main}

Let  $f_D$  denote the vision model for re-encoding,  $g_D$  the projector between  $f_D$ and the LLM, and  $\bm{h}_{DC} \in \mathbb{R}^{1 \times d_h}$  denote the hidden state of the $<$Re-Encoding\_Control$>$ token, where  $d_h$  is the hidden size of the LLM. When the LLM outputs $<$Re-Encoding\_Control$>$ token, the original image is input  into the $f_D$, yielding features  $\bm{z}_D = f_D(\bm{x}) \in \mathbb{R}^{N \times d_z}$ , where  $N$  is the sequence length of the re-encoded image features and  $d_z$  is the hidden size of $f_D$. The features  $\bm{z}_D$  and  $\bm{h}_{DC}$  are then input to the projector, resulting in the embeddings  $\bm{h}_D = g_D(\bm{z}_D, \bm{h}_{DC}) \in \mathbb{R}^{N \times d_h}$. $\bm{h}_D$ is then concatenated with the previous image and text embeddings as input to the LLM.

\subsection{Training Data for Visual Perception Token}\label{sec:data}
We constructed the training dataset for Visual Perception Token based on the datasets from \cite{llava15} and \cite{shao2024visual}. Our training data covers four types of tasks: Text/OCR-Related VQA, Spatial Reasoning, General VQA, and Fine-Grained VQA. The Text/OCR-Related VQA and Spatial Reasoning tasks are used to create training samples for Region Selection Token. The General VQA and Fine-Grained VQA tasks are used to construct training samples for Vision Re-Encoding Tokens. Please see \cref{tab:data} for the composition of training dataset.

\textbf{Samples with Region Selection Tokens}. We used the DocVQA~\cite{docvqa},  TextVQA~\cite{textvqa}, and TextCaps~\cite{textcaps} datasets to build training data for Text/OCR-Related tasks. DocVQA images include photos of documents such as books and invoices, while TextVQA and TextCaps images feature natural scenes such as billboards and store signs. Both datasets involve questions requiring reasoning about text within images. For these datasets, we use the text regions' bounding boxes obtained in \cite{shao2024visual}, and convert the bounding boxes into Region Selection Token sequences as described in \cref{sec:31}. The training data for Spatial Reasoning tasks included images from the VSR~\cite{vsr}, GQA~\cite{hudson2019gqa}, and OpenImage~\cite{openimage} datasets, which contain real-world scenes and questions about the spatial relationships between objects. We used the filtered bounding boxes and question-answer pairs from \cite{shao2024visual} and converted the bounding boxes into the corresponding Region Selection Tokens.


\textbf{Samples with Vision Re-Encoding Tokens}. For General VQA tasks, we used data from COCO~\cite{coco} and GQA~\cite{hudson2019gqa} as provided by LLaVA 1.5~\cite{llava15}. We inserted an additional MLLM response with Vision Re-Encoding Tokens between the original question and response. For Fine-Grained VQA tasks, we used the CUB-200-2011~\cite{WahCUB_200_2011} dataset, which includes images of 200 bird species and annotations about detailed attributes of these birds. We used the question-answer pairs generated in \cite{shao2024visual} and inserted an MLLM response with Vision Re-Encoding Tokens between the questions and answers.
