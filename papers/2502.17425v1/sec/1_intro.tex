\section{Introduction}
\label{sec:intro}

Multimodal Large Language Model (MLLM) depend on the perception capabilities of their vision encoder to process and utilize visual information. During this process, MLLM utilizes a vision encoder and a projector to embed visual information into the language space. The quality of Visual Perception determines whether MLLMs can accurately distinguish objects in an image~\cite{mot}, whether MLLMs can rely on visual information to answer questions instead of generating textual hallucinations~\cite{leng2024mitigating}, and whether MLLMs can perform precise reasoning about spatial relationships~\cite{cheng2024spatialrgpt}, among other tasks.
While current MLLM systems demonstrate strong capabilities in visual information understanding~\cite{gpt4mllm,gemini,Qwen2VL,llava15}, they lack the ability to autonomously control their Visual Perception processes. Instead, these systems depend on manually designed pipelines to perform specific image annotations or visual features enhancement~\cite{yu2024api,mot}. 

In this work, we explore the task of enabling MLLMs to autonomously control their Visual Perception processes. 
Previously, MLLM-based agents and MLLMs equipped with function-calling or tool-use capabilities can be considered as having the ability to control subsequent tasks. They utilize the output of the LLM as arguments for subsequent functions or tool use. However, such control information is confined to the natural language space.  
The advantage of control signals in the natural language space lies in their interpretability, clear supervision signals, and ease of training data construction. However, these signals are constrained by specific formats. Additionally, natural language inherently contains redundancy, leading to efficiency issues.  
In this work, we aim to explore control signals beyond the natural language space. However, we also require that these signals remain naturally compatible with the next-token prediction paradigm of LLMs. 
To address this, we propose the concept of ``Visual Perception Tokens''. These tokens are integrated into the MLLM vocabulary and can be generated by the MLLM through next-token prediction, similar to natural language generation. These tokens do not correspond to specific words or characters in natural language; instead, their primary function is to trigger additional Visual Perception processes and convey control information for these processes. 

\setulcolor{fullred!50} 
We designed two types of Visual Perception Tokens. The first type is \ul{Region Selection Token}, which instruct the MLLM to crop the input image and encode again the important regions relevant to the query using the vision encoder. 
\setulcolor{fullpurple!70} 
The second type is the \ul{Vision Re-Encoding Token}, which signals the model to input the image into (additional) vision encoder and use the resulting vision features to supplement the original MLLM's vision features. 
A projector takes both the additional vision features and the hidden state of the \ul{Vision Re-Encoding Token} as inputs, enabling fine-grained control beyond merely triggering the vision encoder.
In this work, we explore using an additional DINO model, a SAM model, or the modelâ€™s original vision branch as the vision encoder.
During the generation process, if the MLLM outputs any Visual Perception Token, the corresponding additional perception process is triggered, and the extra embedding sequence derived from the image is concatenated to the original LLM input. The LLM then continues generating the response in the form of next-token prediction. \cref{fig:head:task} illustrates the VQA process incorporating visual perception tokens. \cref{fig:forward} provides a more detailed depiction of how visual perception tokens are generated by the MLLM and how they are utilized to control the visual perception process.

To train the MLLM to use Visual Perception Tokens, we constructed the Visual Perception Token training dataset, which includes 829k samples spanning four task categories: General VQA, Fine-Grained VQA, Spatial Reasoning, and Text/OCR-Related VQA. Experiments demonstrated that Visual Perception Token significantly enhances the MLLM's ability to autonomously control and refine its visual perception.

Our contributions can be summarized as follows:
\begin{enumerate}[labelwidth=0pt,labelsep=3pt,itemindent=0em]
\item We explored a novel task: enabling MLLMs to autonomously control their visual perception process. 
\item We designed two types of Visual Perception Tokens: one enabling the MLLM to select regions of interest, and another allowing the MLLM to incorporate additional vision features and control the final embeddings input to the language model.
\item Experimental results demonstrate the effectiveness of our approach. On tasks such as Spatial Reasoning and Fine-Grained VQA, models equipped with Visual Perception Tokens achieved performance improvements of 34.6\% and 32.7\% over the 7B baseline model, respectively.
\end{enumerate}