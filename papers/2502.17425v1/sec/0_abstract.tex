\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\maketitle
% \vspace{-2.5em}
\begin{center}
    \centering
    \captionsetup{type=figure}
    \begin{subfigure}[b]{0.73\linewidth}
        \includegraphics[page=1, trim=0in 0in 1.9in 0in, clip, width=\textwidth]{fig/head2.pdf}
        \caption{Response process of an MLLM equipped with Visual Perception Tokens.}
        \label{fig:head:task}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.26\linewidth}
        \includegraphics[page=1, trim=5.4in 0in 0in 0in, clip, width=\textwidth]{fig/head2.pdf}
        \caption{Average Performance.}
        \label{fig:head:performance}
    \end{subfigure}
    \vspace{-0.5em}
    \caption{The Visual Perception Token aids MLLMs by triggering and controlling additional visual perception processes. \cref{fig:head:task} illustrates the response process of MLLMs equipped with the Region Selection Token or Vision Re-Encoding Token. The regions marked in the image are selected by the Region Selection Token. \cref{fig:head:performance} presents the average performance of a 2B model with the Visual Perception Token across various VQA tasks (higher is better, with 1 being the maximum).}
    \label{fig:head}
\end{center}%
}]
\let\thefootnote\relax\footnotetext{\textsuperscript{*} Equal Contribution; \textsuperscript{\textdagger} Corresponding Author.}

\begin{abstract}
To utilize visual information, Multimodal Large Language Model (MLLM) relies  on the perception process of its vision encoder. The completeness and accuracy of visual perception significantly influence the precision of spatial reasoning, fine-grained understanding, and other tasks. 
However, MLLM still lacks the autonomous capability to control its own visual perception processes, for example, selectively reviewing specific regions of an image or focusing on information related to specific object categories.
In this work, we propose the concept of Visual Perception Token, aiming to empower MLLM with a mechanism to control its visual perception processes. We design two types of Visual Perception Tokens, termed the Region Selection Token and the Vision Re-Encoding Token. MLLMs autonomously generate these tokens, just as they generate text, and use them to trigger additional visual perception actions. The Region Selection Token explicitly identifies specific regions in an image that require further perception, while the Vision Re-Encoding Token uses its hidden states as control signals to guide additional visual perception processes.
Extensive experiments demonstrate the advantages of these tokens in handling spatial reasoning, improving fine-grained understanding, and other tasks. On average, the introduction of Visual Perception Tokens improves the performance of a 2B model by 30.9\%, increasing its score from 0.572 to 0.749, and even outperforms a 7B parameter model by 20.0\% (from 0.624). Please check out our repo \href{https://github.com/yu-rp/VisualPerceptionToken}{here}.
\end{abstract}