% \clearpage
\section{Related Work}

\subsection{Visual Prompting}
From a technical perspective, our approach can also be regarded as a learnable visual prompting method.
Visual prompting is a key technique in vision models, especially for segmentation tasks \cite{sam,vp_3,vp_2}. It uses a prompt encoder to interpret manual annotations, such as points and masks, to control segmentation granularity and assist instance selection. Recent advancements show that LVLMs can interpret visual cues like circles and color masks in a zero-shot manner without an additional encoder \cite{vp_clip_1,vp_clip_2}. 
Building on this, \cite{som} and \cite{fgvp} have utilized segmentation model-generated masks as visual prompts, enhancing LVLM performance in segmentation and grounding. However, these methods are query-agnostic, while VQA tasks require adapting visual information based on the query. \cite{yu2024api} addresses this by using an auxiliary VLM to generate query-specific visual prompts, overlaying attention maps to guide focus on relevant regions.

Built on these learning-free methods, \cite{shao2024visual} trains the MLLM to output bounding boxes of important regions. The image is then cropped and re-input for inference, creating a "crop and re-input" CoT process. Compared to \cite{shao2024visual}, our design of Region Selection Tokens does not rely on bounding box information in the natural language space but instead uses specialized tokens to indicate the location of important regions. This design simplifies training and mitigates the issue of MLLMs having difficulty aligning the image coordinate system with coordinates described in natural language.

\subsection{Visual Perception in MLLM}
Currently, MLLMs have developed the ability to extract visual information, enabling them to be used not only for general image-based question answering and chat~\cite{llava,llava15,cogvlm,gpt4v,gemini,Qwen2VL,Qwen-VL,ma2023llmpruner}, but also for applications in 3D understanding~\cite{hong20233dllminjecting3dworld,zhu2024llava,cvpr23_3dclr}, video analysis~\cite{damonlpsg2024videollama2,damonlpsg2023videollama,tang2024salmonn,sun2024videosalmonn}, domain-specific image question answering~\cite{li2023llavamed}.

Our objective is to further enhance the visual perception capabilities of MLLMs using Visual Perception Tokens. In addition to the aforementioned visual prompting techniques, there are various other research directions focused on improving MLLM visual perception performance.

\cite{mot} identifies the ambiguity within CLIP's features as a limitation that adversely affects MLLMs' visual perception performance. To overcome this, it proposes incorporating additional DINOv2~\cite{oquab2024dinov} encoders to boost the visual perception capabilities of MLLMs. 
\cite{wang2024pictureworththousandwords} observed that even with access to image data, MLLMs can sometimes base their responses on textual information and hallucinations instead of directly leveraging the visual content. To address this, \cite{wang2024pictureworththousandwords} suggests adding image captions to improve visual perception accuracy.
In the specific task of spatial reasoning, visual perception focuses on inferring spatial relationships between objects. To enhance a model's spatial reasoning capabilities, \cite{cheng2024spatialrgpt} proposed using depth maps of scenes as additional input. Moreover, \cite{cvpr23_3dclr} has explored how to leverage multiview scene information to improve the visual perception of MLLMs. To transform multiview information into a suitable input format, \cite{cvpr23_3dclr} employed a neural field representation. Similarly, to address the limitations of 2D images in physical world reasoning, \cite{Chen_2024_CVPR,zhu2024llava} have investigated MLLM visual perception methods based on 3D data. In this case, the input to MLLMs can include additional 3D point cloud features~\cite{hong20233dllminjecting3dworld} and 3D scene graph features~\cite{conceptgraphs}.

The above approaches aim to provide MLLMs with better visual inputs to enhance their visual perception abilities. In contrast, our approach features iterative perception, allowing MLLMs to provide feedback on the perception process, conduct multiple rounds of visual perception, and exercise control over the visual perception process.