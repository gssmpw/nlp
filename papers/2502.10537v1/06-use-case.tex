\section{Use Case}
\label{sec:use-case}

% ** This dataset might not be the best to use for our use case because they created the dataset following 19 harm categories, which we are basically just replicating using Divisi **

To demonstrate a possible analytical process using Divisi on real data, we applied the system to the task of identifying large language model (LLM) prompts that can lead to unsafe responses.
While LLMs' emergent capabilities enable remarkable performance on many tasks, these models can also generate offensive or harmful content if not appropriately aligned with the intended values of its developers~\cite{ji_beavertails_2023}.
What kinds of prompts would result in unsafe responses? Moreover, what prompts might lead to more \textit{heterogeneity} in response safety (some responses are safe, others unsafe)? 
% We aimed to evaluate whether these questions could be answered using an exploratory subgroup analysis approach with Divisi.
To answer these questions, we analyzed the PKU-SafeRLHF dataset~\cite{ji_pku-saferlhf_2024}, which consists of about 145K LLM-generated responses to about 38K prompts. %, served as the context of our use case.
Each row contains a free-text prompt, two alternative responses to the prompt, and labels about each response's perceived safety. % assigned by a team of annotators with AI assistance.
Because the prompts were themselves created by asking various LLMs to generate requests in ``harm categories'' predefined by the dataset creators, we were able to corroborate our analysis against these categories.
However, such metadata is not required to use Divisi.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth, alt={Two screenshots of Divisi showing subgroups on the PKU-SafeRLHF dataset. In (A), the top subgroups for responses that are Both Unsafe are "launder, laundered", "poison, poisonous" and "boss, don, sir"', and "phishing, ddos, botnet.'" In (B), two subgroups are selected that have high rates of Different Safety: "address, addresses" and "try, trying". The Subgroup Map shows that the overlap of these two subgroups has an even higher rate of heterogeneity than either subgroup individually.}]{figures/llm_safety.pdf}
    \caption{Subgroups identified during exploratory subgroup analysis on LLM safety evaluation data~\cite{ji_pku-saferlhf_2024}. Divisi surfaces prompts involving criminal activity as consistently unsafe (A). For prompts with more heterogeneous responses (B), one subgroup involves asking the LLM to provide people's addresses (blue group), which becomes more heterogeneous when adding the words ``try'' or ``trying'' (orange group).}
    \label{fig:llm-safety}
\end{figure*}

We used a grouped bag-of-words approach to derive metadata features for each prompt.
The unique words in the prompts were clustered using a simple word embedding model, after which we took the top 5,000 word clusters and used the presence of any word in a cluster as each metadata feature.
Because these clusters were derived from an embedding model, they represented a variety of concepts, including topics, keywords, and grammatical functions.
For instance, one feature checked whether the prompt contained cyberattack-related words (\texttt{phishing, ddos, botnet}), while another checked for quantifier words (\texttt{any, some, few, enough, least}).
For each prompt, we defined the binary output metric ``Both Unsafe'' to measure whether both candidate responses were rated unsafe, and ``Different Safety'' to capture when one response was rated safe and the other unsafe.
Running the Divisi algorithm on the 75K training data instances with 5,000 features and 200 sampled rows took roughly 6 seconds.

In the Divisi interface, we first enabled a ``Both Unsafe'' Binary Outcome Rate ranking function to find out what kinds of prompts were associated with consistently unsafe responses.
We used Divisi's Jupyter Notebook integration to pull matching instances for each returned subgroup, allowing us to peruse example prompts.
As shown in Fig. \ref{fig:llm-safety}A, many of the subgroups with the most consistently unsafe responses had to do with specific criminal activity, such as money laundering and cyber-attacks.
The fifth subgroup on the list involved an intersection of \texttt{enforcement, personnel} and \texttt{law, laws}, containing prompts about evading the authorities for which responses were both unsafe 84\% of the time.
Interestingly, by disabling the individual features one by one, we could see that the presence of \texttt{enforcement, personnel} was almost sufficient to yield consistently unsafe responses (80\%), but \texttt{law, laws} alone was only consistently unsafe 52\% of the time.
This could suggest that responses to prompts about violating laws that do not directly mention law enforcement are considered less ``unsafe.''

We also analyzed prompts in which one response was labeled safe and the other unsafe, to understand areas of greater disagreement or heterogeneity.
As shown in Fig. \ref{fig:llm-safety}B, a subgroup of prompts with requests for addresses occupied one corner of the Subgroup Map and showed a Different Safety rate of 29\%, compared to 15.9\% on average.
By hovering on each rule in the Subgroups Table, we looked for other subgroups that might cover the same area of the plot, and we found that the feature \texttt{try, trying} overlapped with the addresses bubbles substantially.
In fact, the Subgroup Intersections chart showed that for prompts that matched both rules, the rate of heterogeneity was over 38\%.
This intersection, which was small and would have been difficult to find without Divisi's visualization of overlap, could point to linguistic features that might make LLMs more likely to avoid completing the user's request.

Using Divisi, we were able to build a case comprised of several subgroups defined by interpretable, clear-cut rules, without significant manual analysis but still curated for meaningfulness.
After plotting these subgroups on the Subgroup Map, we observed that there were still many regions of heterogeneous safety that were uncovered by our selected groups.
With further analysis, we could examine this ``long tail'' of prompts by creating progressively smaller subgroups, to see if additional patterns emerge.