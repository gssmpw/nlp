\section{Subgroup Discovery Algorithm}
\label{sec:algorithm}

% \item \textit{Provide ways for users to configure algorithmic results for faster performance or more thorough search.} Existing slice discovery algorithms~\cite{sagadeeva_sliceline_2021,pastor_looking_2021} typically perform exhaustive search over subgroups that meet the minimum size constraints, which can be prohibitively slow for large, wide datasets. We aimed to allow users to view \textit{approximations} of the optimal set of subgroups at customizable levels of speed and coverage. \label{goal:sd-approximate}
    % \item \textit{Support balancing multiple criteria of interest.}
Existing subgroup discovery algorithms (as reviewed in Sec. \ref{sec:related-subgroup-analysis}) are predominantly designed to be run once and to retrieve subgroups that are mathematically optimal by some predefined criterion.
While these approaches are useful when the analyst has a specific goal in mind, their running time and lack of flexibility suggest they may be less compatible with the interactive exploratory subgroup analysis process described above.
We therefore developed a novel algorithm that better supports this iterative sense-making process by addressing two key requirements implied by the design goals in Sec. \ref{sec:formative}: 

\begin{itemize}
    \item \textit{Configurably approximate search.} Scalability is a major consideration for exploratory subgroup analysis, as data scientists may want to perform subgroup discovery many times as they refine their goals and intents. To support task \ref{task:find-subgroups} (Find Subgroups), the algorithm should allow users to control how deeply to search for subgroups, allowing them to get initial results quickly.
    \item \textit{Defining and weighing multiple criteria of interest.} Existing algorithms~\cite{chung_slice_2020,pastor_looking_2021} primarily focus on a single criterion (such as error rate), and they find problematic subgroups relative to the entire dataset. However, task \ref{task:find-gaps} (Find Gaps) requires data scientists to change their search criteria on-the-fly or target the search to specific regions of the dataset. Allowing users to search by multiple criteria at once could help them more flexibly express what constitutes an ``interesting'' subgroup.
\end{itemize}

Below we describe how our algorithm meets these requirements, and we present an evaluation of its accuracy and performance on three datasets compared to prior slice discovery approaches.
Throughout these sections, we use the running example of the UCI Census Income dataset~\cite{adult_2}, which consists of records for 48,842 individuals with 12 distinct features.
We train a classification model to predict whether each individual has an income of at least \$50K/yr, resulting in an error rate of 11.5\%.
Our goal in the running example is to find meaningful subsets of the data in which the model errs more often than average.

\subsection{Problem Setup}

Intuitively, the objective of subgroup discovery is to find subsets of a dataset, each defined using a clear-cut rule, that have interesting differences in some metric compared to the overall dataset.
For example, in the Census Income dataset, we could define a subgroup using the rule \texttt{relationship = "Husband" \& age = "45 - 65"}.
We can formalize this task by defining some notation: Let $X$ be a matrix of values representing a dataset, with $N$ instances and $M$ features per instance.
Note that each feature has to be discrete or categorical in order to define rules, so numerical features need to be binned into discrete categories.
Our goal is to find subgroups $S$, where each group is defined by a rule combining up to $L$ features in the form \texttt{X1 = "v1" \& X2 = "v2" \& ...}.
We then score and order these subgroups according to \textit{ranking functions}, which are each defined as functions of some length-$N$ outcome vector $Y$ and the subgroup $S$.

The simplest way to achieve the above, i.e. generate the top rule-based subgroups for any given set of ranking functions, would be to exhaustively score and rank all possible intersections of up to $L$ features.
However, even for a relatively small dataset with 100 features and two values per feature, there are over 1.3 million rules possible with $L = 3$. % (200+200*198/2+200*198*196/6)
Divisi therefore aims to ensure that the subgroups it returns are close to those that would be returned by naive iteration, but \textit{without} enumerating all possible feature combinations.

% We note that this formulation of subgroup discovery is similar to previous slice-based evaluation techniques such as Slice Finder and SliceTeller~\cite{chung_slice_2020,zhang_sliceteller_2022}, with the addition of arbitrary user-defined ranking functions.
% Unlike other approaches such as Domino and the Spotlight~\cite{eyuboglu_domino_2022,deon_spotlight_2022}, Divisi does not depend on learned representations for each instance, so in order to work with unstructured data it requires tabular metadata features to be extracted for each instance.
% However, it is often more straightforward to generate a large set of discrete-valued features to describe unstructured data, than it is to identify which features are most discriminative for subgroup analysis.
% We demonstrate the feasibility of this approach using the Reviews dataset in Sec. \ref{sec:performance-eval}, and we discuss further applications in the Discussion (Sec. \ref{sec:discussion}).

% As with other slice discovery methods, Divisi requires that the input features be discrete or categorical. Users can specify the list of variables to be used for slicing through an external configuration in Tempo query language syntax, either by adapting the variables used for modeling with the \tqlinline{cut} command or by defining new variables altogether. In our running example, health data analyst Ava uses the binary and categorical variables they had previously defined as-is, and they discretize the continuous variables into ``low,'' ``normal,'' and ``high'' bins.

\subsection{Sampling Approach}
\label{sec:sampling-approach}

Given the discrete-valued input matrix described above, Divisi works by sampling a small number of data points, then performing approximate subgroup discovery constrained to rules that \textit{match} the sampled point. 
In other words, the results of the subgroup discovery algorithm will consist exclusively of groups that contain at least one of the sampled points. 
Our key insight is that as more rows are sampled, the likelihood of finding any subgroup that matches a reasonable proportion of the dataset increases \textit{independently} of the size of the dataset. 
Users can therefore configure how many points to sample (and by extension, the running time of the algorithm) based on the size of subgroup they are looking for.
Furthermore, by sampling specifically from rows that contain outcomes of interest (such as positive labels or errors), we can further reduce the computation needed to find the most relevant groups.

Divisi uses a beam search algorithm to progressively find high-scoring rules with more input features (illustrated in Fig. \ref{fig:slice-finding}). For each of the $n$ sampled ``source rows'' (highlighted row in Figs. \ref{fig:slice-finding}A and B), the algorithm first scores and ranks all univariate rules that contain the row according to all ranking functions separately (Fig. \ref{fig:slice-finding}C). Then, the top $k$ rules according to \textit{each} ranking function are expanded by one feature, again testing all single-feature additions that match the source row (Fig. \ref{fig:slice-finding}D). Subgroups that contain a smaller proportion of the dataset than the user-defined minimum size $p_\text{min}$ are filtered out. This process continues until the user-defined maximum number of rule features $L$ has been reached, and the algorithm returns the ranked results over all subgroups that were evaluated during any iteration. 

In summary, Divisi provides five parameters to address the goal of configurably approximate search: the sample count $n$ (default 100), a binary mask within which to sample source rows (default none), the minimum subgroup size $p_\text{min}$ (default 0.01), the beam size $k$ (default 50), and the maximum rule length $L$ (default 3).
In practice, it is sufficient to set $n$ and $p_\text{min}$ based on the size of the desired subgroups.
% \edit{In summary, Divisi provides the following parameters to address the goal of configurably approximate search, though in practice only $n$ and $p_\text{min}$ need to be set based on the user's preferences:
% \begin{itemize}
%     \item \textbf{Sample count $n$ (default 100):} the number of rows that serve as a source row for the beam search algorithm. Larger values increase running time and the likelihood of finding smaller subgroups. This is distinct from downsampling the dataset, as subgroups are still scored over the entire dataset by default. Moreover, this does not limit the number of subgroups that can be found.
%     \item \textbf{Sampling mask (default none):} a binary mask over the rows of the dataset indicating where to sample source rows from. For large datasets, sampling specifically from rows that contain the outcome of interest can reduce the number of samples needed.
%     \item \textbf{Minimum support $p_\text{min}$ (default 0.01):} the smallest proportion of the dataset that a returned subgroup can have. Larger values decrease running time by eliminating subgroups that are too small.
%     \item \textbf{Beam size $k$ (default 50):} the number of top rules that are expanded in each iteration of the beam search algorithm. Larger values increase running time by including more possible intersections.
%     \item \textbf{Maximum rule length $L$ (default 3)}: the maximum number of features that can be included in a rule. Larger values increase running time by evaluating more complex rules.
% \end{itemize}
% }

\begin{figure*}
    \centering
    \includegraphics[width=0.75\linewidth, alt={Diagram showing how the Divisi sampling approach works on a toy dataset with five columns X1-5, and an outcome metric Y. The fourth row is sampled, with values X1 = 0, X2 = 1, X3 = 1, X4 = 0, X5 = 0, and Y = 0. From this, five rules are scored, one for each feature value. The top two rules are X2 = 1 and X4 = 0, and these are expanded in the next stage by testing adding an additional feature to them. This process repeats until the maximum rule length is reached.}]{figures/slice_finding.pdf}
    \caption{Divisi's subgroup discovery algorithm takes as input a matrix of discrete-valued input features (A) and one or more score functions, in this case a Binary Outcome Rate score over the outcomes in (B). For each sampled row (C), the algorithm first scores each single-feature slice containing that row (D), then iteratively expands the top $k$ slices using other features that match the sampled row (E). In this example, $k = 2$ and the minimum slice size is 2 instances.}
    \label{fig:slice-finding}
\end{figure*}

\subsubsection{Testing Robustness} \label{sec:testing-robustness}
Prior approaches for slice discovery typically either do not account for the robustness of subgroups or they use statistical methods such as $\alpha$-investing to mitigate the false discovery rate problem~\cite{chung_slice_2020,pastor_looking_2021}. Assessing the reliability of a subgroup is important for Divisi, particularly when the group is small relative to the overall dataset; however, $p$-value thresholds may be less appropriate for exploratory analysis across several different metrics. Therefore, we split the data into \textit{discovery} and \textit{evaluation} sets~\cite{green_subgroup_2021}, such that all initial scoring and ranking occurs on the discovery set while interactive re-ranking utilizes the evaluation set. This ensures that the metrics displayed for each subgroup are obtained separately from those used to develop the results.

\subsection{Ranking Functions}

Ranking functions compute a non-negative value for each subgroup that is higher if the subgroup more closely matches a criterion of interest. Divisi can in theory support any function that can be implemented in Python and that operates on an outcome variable and a binary subgroup mask.
In the interactive interface, the user can select from the six pre-defined ranking functions below. Through experimentation on datasets with binary, categorical, and continuous target variables, we found that these functions covered the majority of use cases:
\begin{itemize}[leftmargin=*]
    \item \textit{Binary Outcome Rate (Precision).} This function simply measures the ratio of the mean of a particular outcome variable within the subgroup to the overall mean. More formally, given an outcome vector $Y$ and a subgroup $S$ containing a subset of instances, the outcome rate score is calculated as
    \begin{equation}
    \text{Score}(S; Y) = \frac{\sum_{i \in S} y_i}{|S|}
    \end{equation}
    This ranking function can be used to rank subgroups by the rate of any binary metric, such as positive labels, positive predictions, model errors, or even similarity to another subgroup. By taking the inverse of the function, we can find subgroups with lower rates than average.

    \item \textit{Binary Outcome Coverage (Recall).} Complementary to the outcome rate, outcome coverage measures the proportion of all instances with a positive outcome that are captured within a subgroup, also known as recall:
    \begin{equation}
        \text{Score}(S; Y) = \frac{\sum_{i \in S} y_i}{\sum_{i = 1}^{N} y_i}
    \end{equation}
    % Similar to the above, the coverage metric can be used with any binary metric. Although the definitions are equivalent, we refer to these functions as ``rate'' and ``coverage'' to avoid confusion with precision and recall as these typically relate to a specific set of ground-truth labels.

    \item \textit{Interaction Effect.} This function penalizes rules with extraneous slicing features by taking the ratio of the binary outcome rate in the current subgroup against the maximum rate in all subgroups defined by subsets of the current group's slicing features. For example, for the rule \texttt{A = 1 \& B = 3}, this function would divide the outcome rate by the maximum of the rates for the rules \texttt{A = 1}, \texttt{B = 3}, and the overall dataset. A value greater than 1 indicates that all features play a role in elevating the outcome rate.
    
    \item \textit{Mean Difference.} This ranking function measures the difference between the mean of a continuous outcome in a subgroup and the overall mean:
    \begin{equation}
        \text{Score}(S; Y) = \left|\frac{\sum_{i \in S} y_i}{|S|} - \frac{\sum_{i=1}^{N} y_i}{N}\right|
    \end{equation}
    The Mean Difference score can be used to rank subgroups in regression models by how different the in-group predictions are from the true values.

    \item \textit{Group Size.} To prioritize groups that have good support while not being too large, we score subgroups using a Gaussian function of the group size with a configurable ideal mean and spread.

    \item \textit{Simple Rule.} This function penalizes subgroups that are defined using too many features in the rule, using an inverse logarithmic function.
\end{itemize}
% Note that all of these functions are non-negative by definition, guaranteeing that the top $k$ slices for any weighted sum of the score functions will be among the top $k$ slices for each individual score function. This enables us to efficiently re-rank slices for different combinations of score functions, addressing \ref{goal:sd-score-fns}.

\subsection{Accuracy and Performance Evaluation}
\label{sec:performance-eval}

\begin{figure*}
    \centering
    \includegraphics[width=0.75\textwidth, alt={Six performance evaluation charts, three for running time in seconds and three for recall, within which each shows results for Census Income, Airline, and Reviews. Divisi was tested with 10, 20, 50, 100, and 200 samples taken, and compared to Lattice Search and Frequent Itemset implementations. In all results, running time decreases as the minimum subgroup size increases from 1\% to 10\%, and recall increases. Recall is always 100\% for Lattice Search and Frequent Itemset as these methods are not approximate, and it is 0.5 or above for all settings of Divisi. In Census Income, all running times are within 0.3-3 seconds. In Airline, Lattice Search performs comparably to Divisi (around 10 seconds) while Frequent Itemset ranges from 15 to 100 seconds. In Reviews, Lattice Search ranges from 100 to 4,000 seconds, while Divisi always remains under 300 seconds (Frequent Itemset fails to run).}]{figures/performance_eval.pdf}
    \caption{Average running times and accuracy (recall in top 50 returned results) for different parameter settings of Divisi, compared against a Lattice Search and Frequent Itemset approach. (We were unable to run the Frequent Itemset approach on the Reviews dataset due to excessive memory consumption, so we only report its performance on the Census Income and Airline datasets.) Shaded regions represent one standard deviation over 10 trials.}
    \label{fig:performance-eval}
\end{figure*}

To test the Divisi algorithm's ability to return results with configurable levels of approximation and speed, we conducted an evaluation of its accuracy and time performance under different parameter settings.
We performed subgroup discovery on three datasets of varying size and dimensionality: UCI Census Income~\cite{adult_2} (48K instances, 12 features), an Airline Passenger Satisfaction dataset sourced from Kaggle~\cite{noauthor_airline_2019} (129K instances, 22 features), and a subset of the Yelp review dataset\footnote{To produce a discrete-valued tabular representation of the review dataset, we used a grouped bag-of-words approach, similar to the analysis shown in Sec. \ref{sec:use-case}.}~\cite{zhang_2015_yelp} (200K instances, 2,000 features).
Further details on the datasets used are provided in Sec. A of the Supplementary Material.
% The size of the datasets ranged from 48,000 in Census Income to over 200,000 instances in Reviews, and from 12 features in Census Income to 2,000 in Reviews.

For comparison, we tested two baseline approaches: a Lattice Search algorithm similar to Slice Finder~\cite{chung_slice_2020} and a Frequent Itemset approach based on DivExplorer~\cite{pastor_looking_2021}. 
To ensure that each method would rank subgroups the same way, we used their underlying algorithms to retrieve candidate subgroups, then used Divisi's ranking functions (a Binary Outcome Rate score for error and a Group Size function) to rank the top 100 subgroups.
We measured the recall of each Divisi output against the outputs of Lattice Search and Frequent Itemset, since these two approaches are both exhaustive and produce identical results.

As shown in Fig. \ref{fig:performance-eval}, different parameter settings of Divisi can result in either more accurate or faster results. Divisi's running time (upper panels) is mostly dependent on the size of the dataset and the number of sampled rows $n$. 
In contrast, the Lattice Search and Frequent Itemset methods are markedly slower for smaller minimum subgroup sizes, which increase the number of viable subgroups. 
Divisi's recall (lower panels) generally increases as the minimum subgroup size increases and as the number of sampled instances increases, yielding a more extensive search.

Notably, Divisi remains feasible to run as data dimensionality grows regardless of parameter setting.
Its running time is comparable to the Lattice Search algorithm in the Census Income and Airline datasets, but up to two orders of magnitude faster in the wider Reviews dataset.
Moreover, Divisi can be parallelized to sample different subsets of the data in different worker threads, leading to even faster performance.
These results suggest that Divisi could be used in place of existing Lattice Search methods without sacrificing runtime or accuracy in smaller data settings, while making subgroup analysis more feasible in larger datasets.
Additionally, while rule-based subgroup discovery has been largely infeasible on large text datasets using existing methods, Divisi's efficiency makes this approach much more practical, even with thousands of word- or concept-level features.