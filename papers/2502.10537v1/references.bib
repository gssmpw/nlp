
@article{sacha_what_2017,
	title = {What you see is what you can change: {Human}-centered machine learning by interactive visualization},
	volume = {268},
	issn = {18728286},
	url = {http://dx.doi.org/10.1016/j.neucom.2017.01.105},
	doi = {10.1016/j.neucom.2017.01.105},
	abstract = {Visual analytics (VA) systems help data analysts solve complex problems interactively, by integrating automated data analysis and mining, such as machine learning (ML) based methods, with interactive visualizations. We propose a conceptual framework that models human interactions with ML components in the VA process, and that puts the central relationship between automated algorithms and interactive visualizations into sharp focus. The framework is illustrated with several examples and we further elaborate on the interactive ML process by identifying key scenarios where ML methods are combined with human feedback through interactive visualization. We derive five open research challenges at the intersection of ML and visualization research, whose solution should lead to more effective data analysis.},
	journal = {Neurocomputing},
	author = {Sacha, Dominik and Sedlmair, Michael and Zhang, Leishi and Lee, John A. and Peltonen, Jaakko and Weiskopf, Daniel and North, Stephen C. and Keim, Daniel A.},
	year = {2017},
	note = {Publisher: Elsevier B.V.},
	keywords = {Visual analytics, Interaction, Information visualization, Machine learning},
	pages = {164--175},
	file = {PDF:/Users/vsivaram/Zotero/storage/9V92MMVL/1-s2.0-S0925231217307609-main.pdf:application/pdf},
}

@article{Liu2017a,
	title = {Towards better analysis of machine learning models: {A} visual analytics perspective},
	volume = {1},
	issn = {2468502X},
	url = {http://dx.doi.org/10.1016/j.visinf.2017.01.006},
	doi = {10.1016/j.visinf.2017.01.006},
	abstract = {Interactive model analysis, the process of understanding, diagnosing, and refining a machine learning model with the help of interactive visualization, is very important for users to efficiently solve real-world artificial intelligence and data mining problems. Dramatic advances in big data analytics have led to a wide variety of interactive model analysis tasks. In this paper, we present a comprehensive analysis and interpretation of this rapidly developing area. Specifically, we classify the relevant work into three categories: understanding, diagnosis, and refinement. Each category is exemplified by recent influential work. Possible future research opportunities are also explored and discussed.},
	number = {1},
	journal = {Visual Informatics},
	author = {Liu, Shixia and Wang, Xiting and Liu, Mengchen and Zhu, Jun},
	year = {2017},
	note = {arXiv: 1702.01226
Publisher: Elsevier B.V.},
	keywords = {Machine learning, Diagnosis, Interactive model analysis, Interactive visualization, Refinement, Understanding},
	pages = {48--56},
	file = {PDF:/Users/vsivaram/Zotero/storage/GY4YRG6H/1-s2.0-S2468502X17300086-main.pdf:application/pdf},
}

@article{chen_visual_2018,
	title = {Visual exploration and comparison of word embeddings},
	volume = {48},
	issn = {1045926X},
	url = {https://doi.org/10.1016/j.jvlc.2018.08.008},
	doi = {10.1016/j.jvlc.2018.08.008},
	abstract = {Word embeddings are distributed representations for natural language words, and have been wildly used in many natural language processing tasks. The word embedding space contains local clusters with semantically similar words and meaningful directions, such as the analogy. However, there are different training algorithms and text corpora, which both have a different impact on the generated word embeddings. In this paper, we propose a visual analytics system to visually explore and compare word embeddings trained by different algorithms and corpora. The word embedding spaces are compared from three aspects, i.e., local clusters, semantic directions and diachronic changes, to understand the similarity and differences between word embeddings.},
	number = {August},
	journal = {Journal of Visual Languages and Computing},
	author = {Chen, Juntian and Tao, Yubo and Lin, Hai},
	year = {2018},
	note = {Publisher: Elsevier Ltd},
	keywords = {Visual comparison, Word embeddings},
	pages = {178--186},
	file = {PDF:/Users/vsivaram/Zotero/storage/BZKSBZ3S/embedding-comparison-vis.pdf:application/pdf},
}

@article{Smilkov2016,
	title = {Embedding {Projector}: {Interactive} {Visualization} and {Interpretation} of {Embeddings}},
	url = {http://arxiv.org/abs/1611.05469},
	abstract = {Embeddings are ubiquitous in machine learning, appearing in recommender systems, NLP, and many other applications. Researchers and developers often need to explore the properties of a specific embedding, and one way to analyze embeddings is to visualize them. We present the Embedding Projector, a tool for interactive visualization and interpretation of embeddings.},
	number = {Nips},
	author = {Smilkov, Daniel and Thorat, Nikhil and Nicholson, Charles and Reif, Emily and Viégas, Fernanda B. and Wattenberg, Martin},
	year = {2016},
	note = {arXiv: 1611.05469},
	file = {PDF:/Users/vsivaram/Zotero/storage/SN57E5RE/embedding proj.pdf:application/pdf},
}

@article{Liu2019,
	title = {Latent space cartography: {Visual} analysis of vector space embeddings},
	volume = {38},
	issn = {14678659},
	doi = {10.1111/cgf.13672},
	abstract = {Latent spaces—reduced-dimensionality vector space embeddings of data, fit via machine learning—have been shown to capture interesting semantic properties and support data analysis and synthesis within a domain. Interpretation of latent spaces is challenging because prior knowledge, sometimes subtle and implicit, is essential to the process. We contribute methods for “latent space cartography”, the process of mapping and comparing meaningful semantic dimensions within latent spaces. We first perform a literature survey of relevant machine learning, natural language processing, and scientific research to distill common tasks and propose a workflow process. Next, we present an integrated visual analysis system for supporting this workflow, enabling users to discover, define, and verify meaningful relationships among data points, encoded within latent space dimensions. Three case studies demonstrate how users of our system can compare latent space variants in image generation, challenge existing findings on cancer transcriptomes, and assess a word embedding benchmark.},
	number = {3},
	journal = {Computer Graphics Forum},
	author = {Liu, Yang and Jun, Eunice and Li, Qisheng and Heer, Jeffrey},
	year = {2019},
	keywords = {Visual analytics, Human-centered computing → Visualization},
	pages = {67--78},
	file = {PDF:/Users/vsivaram/Zotero/storage/IIZGP4I2/2019-LatentSpaceCartography-EuroVis.pdf:application/pdf},
}

@article{amershi_modeltracker_2015,
	title = {{ModelTracker} : {Redesigning} {Performance} {Analysis} {Tools} for {Machine} {Learning}},
	author = {Amershi, Saleema and Chickering, Max and Drucker, Steven M and Lee, Bongshin and Simard, Patrice and Suh, Jina},
	year = {2015},
	note = {ISBN: 9781450331456},
	pages = {337--346},
	file = {PDF:/Users/vsivaram/Zotero/storage/PKM5G45K/2702123.2702509.pdf:application/pdf},
}

@article{zhang_manifold_2019,
	title = {Manifold: {A} {Model}-{Agnostic} {Framework} for {Interpretation} and {Diagnosis} of {Machine} {Learning} {Models}},
	volume = {25},
	issn = {19410506},
	doi = {10.1109/TVCG.2018.2864499},
	abstract = {Interpretation and diagnosis of machine learning models have gained renewed interest in recent years with breakthroughs in new approaches. We present Manifold, a framework that utilizes visual analysis techniques to support interpretation, debugging, and comparison of machine learning models in a more transparent and interactive manner. Conventional techniques usually focus on visualizing the internal logic of a specific model type (i.e., deep neural networks), lacking the ability to extend to a more complex scenario where different model types are integrated. To this end, Manifold is designed as a generic framework that does not rely on or access the internal logic of the model and solely observes the input (i.e., instances or features) and the output (i.e., the predicted result and probability distribution). We describe the workflow of Manifold as an iterative process consisting of three major phases that are commonly involved in the model development and diagnosis process: inspection (hypothesis), explanation (reasoning), and refinement (verification). The visual components supporting these tasks include a scatterplot-based visual summary that overviews the models' outcome and a customizable tabular view that reveals feature discrimination. We demonstrate current applications of the framework on the classification and regression tasks and discuss other potential machine learning use scenarios where Manifold can be applied.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Zhang, Jiawei and Wang, Yang and Molino, Piero and Li, Lezhi and Ebert, David S.},
	year = {2019},
	pmid = {30130197},
	note = {arXiv: 1808.00196},
	keywords = {model comparison, Interactive machine learning, model debugging, performance analysis},
	pages = {364--373},
	file = {PDF:/Users/vsivaram/Zotero/storage/S4YKWEB2/1808.00196.pdf:application/pdf},
}

@book{yuan_isea_2022,
	title = {{iSEA}: {An} {Interactive} {Pipeline} for {Semantic} {Error} {Analysis} of {NLP} {Models}},
	volume = {1},
	isbn = {978-1-4503-9144-3},
	abstract = {Error analysis in NLP models is essential to successful model development and deployment. One common approach for diagnosing errors is to identify subpopulations in the dataset where the model produces the most errors. However, existing approaches typically define subpopulations based on pre-defined features, which requires users to form hypotheses of errors in advance. To complement these approaches, we propose iSEA, an Interactive Pipeline for Semantic Error Analysis in NLP Models, which automatically discovers semantically-grounded subpopulations with high error rates in the context of a human-in-the-loop interactive system. iSEA enables model developers to learn more about their model errors through discovered subpopulations, validate the sources of errors through interactive analysis on the discovered subpopulations, and test hypotheses about model errors by defining custom subpopulations. The tool supports semantic descriptions of error-prone subpopulations at the token and concept level, as well as pre-defined higher-level features. Through use cases and expert interviews, we demonstrate how iSEA can assist error understanding and analysis.},
	publisher = {Association for Computing Machinery},
	author = {Yuan, Jun and Vig, Jesse and Rajani, Nazneen},
	year = {2022},
	doi = {10.1145/3490099.3511146},
	note = {arXiv: 2203.04408v1
Publication Title: International Conference on Intelligent User Interfaces, Proceedings IUI
Issue: 1},
	keywords = {Error Analysis, xAI},
	file = {PDF:/Users/vsivaram/Zotero/storage/AFH5IF5S/2203.04408.pdf:application/pdf},
}

@article{wu_errudite_2020,
	title = {Errudite: {Scalable}, reproducible, and testable error analysis},
	doi = {10.18653/v1/p19-1073},
	abstract = {Though error analysis is crucial to understanding and improving NLP models, the common practice of manual, subjective categorization of a small sample of errors can yield biased and incomplete conclusions. This paper codifies model and task agnostic principles for informative error analysis, and presents Errudite, an interactive tool for better supporting this process. First, error groups should be precisely defined for reproducibility; Errudite supports this with an expressive domain-specific language. Second, to avoid spurious conclusions, a large set of instances should be analyzed, including both positive and negative examples; Errudite enables systematic grouping of relevant instances with filtering queries. Third, hypotheses about the cause of errors should be explicitly tested; Errudite supports this via automated counterfactual rewriting. We validate our approach with a user study, finding that Errudite (1) enables users to perform high quality and reproducible error analyses with less effort, (2) reveals substantial ambiguities in prior published error analyses practices, and (3) enhances the error analysis experience by allowing users to test and revise prior beliefs.},
	journal = {ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference},
	author = {Wu, Tongshuang and Ribeiro, Marco Tulio and Heer, Jeffrey and Weld, Daniel S.},
	year = {2020},
	note = {ISBN: 9781950737482},
	pages = {747--763},
	file = {PDF:/Users/vsivaram/Zotero/storage/464X4LM2/P19-1073.pdf:application/pdf},
}

@article{cito_explaining_2021,
	title = {Explaining mispredictions of machine learning models using rule induction},
	doi = {10.1145/3468264.3468614},
	abstract = {While machine learning (ML) models play an increasingly prevalent role in many software engineering tasks, their prediction accuracy is often problematic. When these models do mispredict, it can be very difficult to isolate the cause. In this paper, we propose a technique that aims to facilitate the debugging process of trained statistical models. Given an ML model and a labeled data set, our method produces an interpretable characterization of the data on which the model performs particularly poorly. The output of our technique can be useful for understanding limitations of the training data or the model itself; it can also be useful for ensembling if there are multiple models with different strengths. We evaluate our approach through case studies and illustrate how it can be used to improve the accuracy of predictive models used for software engineering tasks within Facebook.},
	journal = {ESEC/FSE 2021 - Proceedings of the 29th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
	author = {Cito, Jürgen and Dillig, Isil and Kim, Seohyun and Murali, Vijayaraghavan and Chandra, Satish},
	year = {2021},
	note = {ISBN: 9781450385626},
	keywords = {machine learning, explainability, rule induction},
	pages = {716--727},
	file = {PDF:/Users/vsivaram/Zotero/storage/BXQSFKNC/3468264.3468614.pdf:application/pdf},
}

@article{umarani_sampling_2010,
	title = {Sampling based {Association} {Rules} {Mining} -- {A} {Recent} {Overview}},
	volume = {2},
	number = {2},
	journal = {International Journal on Computer Science and Engineering},
	author = {Umarani, V and Punithavalli, M},
	year = {2010},
	keywords = {association rule, data reduction technique, datamining, frequent, mining, sampling},
	pages = {314--318},
	file = {PDF:/Users/vsivaram/Zotero/storage/D5JL5NRS/sampling association rule.pdf:application/pdf},
}

@article{sagadeeva_sliceline_2021,
	title = {{SliceLine}: {Fast}, {Linear}-{Algebra}-based {Slice} {Finding} for {ML} {Model} {Debugging}},
	issn = {07308078},
	doi = {10.1145/3448016.3457323},
	abstract = {Slice finding - -a recent work on debugging machine learning (ML) models - -aims to find the top-K data slices (e.g., conjunctions of predicates such as gender female and degree PhD), where a trained model performs significantly worse than on the entire training/test data. These slices may be used to acquire more data for the problematic subset, add rules, or otherwise improve the model. In contrast to decision trees, the general slice finding problem allows for overlapping slices. The resulting search space is huge as it covers all subsets of features and their distinct values. Hence, existing work primarily relies on heuristics and focuses on small datasets that fit in memory of a single node. In this paper, we address these scalability limitations of slice finding in a holistic manner from both algorithmic and system perspectives. We leverage monotonicity properties of slice sizes, errors and resulting scores to facilitate effective pruning. Additionally, we present an elegant linear-algebra-based enumeration algorithm, which allows for fast enumeration and automatic parallelization on top of existing ML systems. Experiments with different real-world regression and classification datasets show that effective pruning and efficient sparse linear algebra renders exact enumeration feasible, even for datasets with many features, correlations, and data sizes beyond single node memory.},
	journal = {Proceedings of the ACM SIGMOD International Conference on Management of Data},
	author = {Sagadeeva, Svetlana and Boehm, Matthias},
	year = {2021},
	note = {ISBN: 9781450383431},
	keywords = {model debugging, data coverage, frequent itemset mining, large-scale machine learning, linear algebra, slice finding},
	pages = {2290--2299},
	file = {PDF:/Users/vsivaram/Zotero/storage/FHIQRHDQ/sigmod2021b_sliceline.pdf:application/pdf},
}

@article{cabrera_fairvis_2019,
	title = {{FAIRVIS}: {Visual} {Analytics} for {Discovering} {Intersectional} {Bias} in {Machine} {Learning}},
	doi = {10.1109/VAST47406.2019.8986948},
	abstract = {The growing capability and accessibility of machine learning has led to its application to many real-world domains and data about people. Despite the benefits algorithmic systems may bring, models can reflect, inject, or exacerbate implicit and explicit societal biases into their outputs, disadvantaging certain demographic subgroups. Discovering which biases a machine learning model has introduced is a great challenge, due to the numerous definitions of fairness and the large number of potentially impacted subgroups. We present FAIRVIS, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models. Through FAIRVIS, users can apply domain knowledge to generate and investigate known subgroups, and explore suggested and similar subgroups. FAIRVIS's coordinated views enable users to explore a high-level overview of subgroup performance and subsequently drill down into detailed investigation of specific subgroups. We show how FAIRVIS helps to discover biases in two real datasets used in predicting income and recidivism. As a visual analytics system devoted to discovering bias in machine learning, FAIRVIS demonstrates how interactive visualization may help data scientists and the general public understand and create more equitable algorithmic systems.},
	journal = {2019 IEEE Conference on Visual Analytics Science and Technology, VAST 2019 - Proceedings},
	author = {Cabrera, Angel Alexander and Epperson, Will and Hohman, Fred and Kahng, Minsuk and Morgenstern, Jamie and Chau, Duen Horng},
	year = {2019},
	note = {arXiv: 1904.05419
ISBN: 9781728122847},
	keywords = {visual analytics, subgroup discovery, intersectional bias, Machine learning fairness},
	pages = {46--56},
	file = {PDF:/Users/vsivaram/Zotero/storage/SWJJT2BI/1904.05419.pdf:application/pdf},
}

@article{kahng_visual_2016,
	title = {Visual exploration of machine learning results using data cube analysis},
	doi = {10.1145/2939502.2939503},
	abstract = {As complex machine learning systems become more widely adopted, it becomes increasingly challenging for users to understand models or interpret the results generated from the models. We present our ongoing work on developing interactive and visual approaches for exploring and understanding machine learning results using data cube analysis. We propose MLCube, a data cube inspired framework that enables users to define instance subsets using feature conditions and computes aggregate statistics and evaluation metrics over the subsets. We also design MLCube Explorer, an interactive visualization tool for comparing models' performances over the subsets. Users can interactively specify operations, such as drilling down to specific instance subsets, to perform more in-depth exploration. Through a usage scenario, we demonstrate how MLCube Explorer works with a public advertisement click log data set, to help a user build new advertisement click prediction models that advance over an existing model.},
	journal = {HILDA 2016 - Proceedings of the Workshop on Human-In-the-Loop Data Analytics},
	author = {Kahng, Minsuk and Fang, Dezhi and Chau, Duen Horng},
	year = {2016},
	note = {ISBN: 9781450342070},
	keywords = {Data visualization, Machine learning, Data cube, Interactive data analysis},
	file = {PDF:/Users/vsivaram/Zotero/storage/8XYV3GF6/2939502.2939503.pdf:application/pdf},
}

@article{perer_frequence_2014,
	title = {Frequence: {Interactive} mining and visualization of temporal frequent event sequences},
	doi = {10.1145/2557500.2557508},
	abstract = {Extracting insights from temporal event sequences is an important challenge. In particular, mining frequent patterns from event sequences is a desired capability for many domains. However, most techniques for mining frequent patterns are ineffective for real-world data that may be lowresolution, concurrent, or feature many types of events, or the algorithms may produce results too complex to interpret. To address these challenges, we propose Frequence, an intelligent user interface that integrates data mining and visualization in an interactive hierarchical information exploration system for finding frequent patterns from longitudinal event sequences. Frequence features a novel frequent sequence mining algorithm to handle multiple levels-of-detail, temporal context, concurrency, and outcome analysis. Frequence also features a visual interface designed to support insights, and support exploration of patterns of the level-of-detail relevant to users. Frequence's effectiveness is demonstrated with two use cases: Medical research mining event sequences from clinical records to understand the progression of a disease, and social network research using frequent sequences from Foursquare to understand the mobility of people in an urban environment. © 2014 ACM.},
	journal = {International Conference on Intelligent User Interfaces, Proceedings IUI},
	author = {Perer, Adam and Wang, Fei},
	year = {2014},
	note = {ISBN: 9781450321846},
	keywords = {Visual Analytics, Frequent Sequence Mining, Temporal Visualization},
	pages = {153--162},
	file = {PDF:/Users/vsivaram/Zotero/storage/3AD42FL6/2557500.2557508.pdf:application/pdf},
}

@article{zhang_sliceteller_2022,
	title = {{SliceTeller}: {A} {Data} {Slice}-{Driven} {Approach} for {Machine} {Learning} {Model} {Validation}},
	volume = {29},
	issn = {19410506},
	doi = {10.1109/TVCG.2022.3209465},
	abstract = {Real-world machine learning applications need to be thoroughly evaluated to meet critical product requirements for model release, to ensure fairness for different groups or individuals, and to achieve a consistent performance in various scenarios. For example, in autonomous driving, an object classification model should achieve high detection rates under different conditions of weather, distance, etc. Similarly, in the financial setting, credit-scoring models must not discriminate against minority groups. These conditions or groups are called as \&\#x201C;{\textless}italic{\textgreater}Data Slices{\textless}/italic{\textgreater}\&\#x201D;. In product MLOps cycles, product developers must identify such critical data slices and adapt models to mitigate data slice problems. {\textless}italic{\textgreater}Discovering{\textless}/italic{\textgreater} where models fail, {\textless}italic{\textgreater}understanding{\textless}/italic{\textgreater} why they fail, and {\textless}italic{\textgreater}mitigating{\textless}/italic{\textgreater} these problems, are therefore essential tasks in the MLOps life-cycle. In this paper, we present {\textless}bold{\textgreater}{\textless}italic{\textgreater}SliceTeller{\textless}/italic{\textgreater}{\textless}/bold{\textgreater} , a novel tool that allows users to debug, compare and improve machine learning models driven by {\textless}italic{\textgreater}critical{\textless}/italic{\textgreater} data slices. {\textless}italic{\textgreater}SliceTeller{\textless}/italic{\textgreater} automatically discovers problematic slices in the data, helps the user understand why models fail. More importantly, we present an efficient algorithm, {\textless}bold{\textgreater}{\textless}italic{\textgreater}SliceBoosting{\textless}/italic{\textgreater}{\textless}/bold{\textgreater}, to estimate trade-offs when prioritizing the optimization over certain slices. Furthermore, our system empowers model developers to compare and analyze different model versions during model iterations, allowing them to choose the model version best suitable for their applications. We evaluate our system with three use cases, including two real-world use cases of {\textless}italic{\textgreater}product development{\textless}/italic{\textgreater}, to demonstrate the power of {\textless}italic{\textgreater}SliceTeller{\textless}/italic{\textgreater} in the debugging and improvement of product-quality ML models.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Zhang, Xiaoyu and Ono, Jorge Piazentin and Song, Huan and Gou, Liang and Ma, Kwan Liu and Ren, Liu},
	year = {2022},
	note = {Publisher: IEEE},
	keywords = {Analytical models, Data models, Human-in-the-loop, Optimization, Adaptation models, Computational modeling, Data Slicing, Data Validation, Data-Centric AI, Model Evaluation, Model Validation, Predictive models, Training},
	pages = {842--852},
	file = {PDF:/Users/vsivaram/Zotero/storage/W6IFDB62/SliceTeller_A_Data_Slice-Driven_Approach_for_Machine_Learning_Model_Validation.pdf:application/pdf},
}

@article{pastor_looking_2021,
	title = {Looking for {Trouble}: {Analyzing} {Classifier} {Behavior} via {Pattern} {Divergence}},
	issn = {07308078},
	doi = {10.1145/3448016.3457284},
	abstract = {Machine learning models may perform differently on different data subgroups, which we represent as itemsets (i.e., conjunctions of simple predicates). The identification of these critical data subgroups plays an important role in many applications, for example model validation and testing, or evaluation of model fairness. Typically, domain expert help is required to identify relevant (or sensitive) subgroups. We propose the notion of divergence over itemsets as a measure of different classification behavior on data subgroups, and the use of frequent pattern mining techniques for their identification. A quantification of the contribution of different attribute values to divergence, based on the mathematical foundations provided by Shapley values, allows us to identify both critical and peculiar behaviors of attributes. Extended experiments show the effectiveness of the approach in identifying critical subgroup behaviors.},
	journal = {Proceedings of the ACM SIGMOD International Conference on Management of Data},
	author = {Pastor, Eliana and De Alfaro, Luca and Baralis, Elena},
	year = {2021},
	note = {ISBN: 9781450383431},
	keywords = {bias detection, classifier validation, fairness in machine learning, machine-learning model debugging, shapley value},
	pages = {1400--1412},
	file = {PDF:/Users/vsivaram/Zotero/storage/JQ4DSVG6/DivExplorer.pdf:application/pdf},
}

@article{chung_slice_2020,
	title = {Slice {Finder}: {Automated} {Data} {Slicing} for {Model} {Validation}},
	volume = {32},
	issn = {15582191},
	doi = {10.1109/TKDE.2019.2916074},
	abstract = {As machine learning systems become democratized, it becomes increasingly important to help users easily debug their models. However, current data tools are still primitive when it comes to helping users trace model performance problems all the way to the data. We focus on the particular problem of slicing data to identify subsets of the validation data where the model performs poorly. This is an important problem in model validation because the overall model performance can fail to reflect that of the smaller subsets, and slicing allows users to analyze the model performance on a more granular-level. Unlike general techniques (e.g., clustering) that can find arbitrary slices, our goal is to find interpretable slices (which are easier to take action compared to arbitrary subsets) that are problematic and large. We propose mathsf\{Slice Finder\}SliceFinder, which is an interactive framework for identifying such slices using statistical techniques. Applications include diagnosing model fairness and fraud detection, where identifying slices that are interpretable to humans is crucial. This research is part of a larger trend of Big data and Artificial Intelligence (AI) integration and opens many opportunities for new research.},
	number = {12},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Chung, Yeounoh and Kraska, Tim and Polyzotis, Neoklis and Tae, Ki Hyun and Whang, Steven Euijong},
	year = {2020},
	note = {arXiv: 1807.06068},
	keywords = {Data slicing, model analysis, model validation},
	pages = {2284--2296},
	file = {PDF:/Users/vsivaram/Zotero/storage/DWGD4EXD/Slice_Finder_Automated_Data_Sclicing_for_Model_Val.pdf:application/pdf},
}

@article{zhang_drml_2022,
	title = {{DrML}: {Diagnosing} and {Rectifying} {Vision} {Models} using {Language}},
	abstract = {Recent multi-modal contrastive learning models have demonstrated the ability to learn an embedding space suitable for building strong vision classifiers, by leverag-ing the rich information in large-scale image-caption datasets. Our work highlights a distinct advantage of this multi-modal embedding space: the ability to diagnose vision classifiers through natural language. The traditional process of diagnosing model behaviors in deployment settings involves labor-intensive data acquisition and annotation. Our proposed method, DrML, can discover high-error data slices, identify influential attributes and further rectify undesirable model behaviors, without requiring any visual data. Through a combination of theoretical explanation and empirical verification, we present conditions under which classifiers trained on embeddings from one modality can be equivalently applied to embeddings from another modality. On a range of image datasets with known error slices, we demonstrate that our method can effectively identify the error slices and influential attributes, and can further use language to rectify failure modes of the classifier.},
	journal = {NeurIPS Workshop},
	author = {Zhang, Yuhui and Haochen, Jeff Z and Huang, Shih-Cheng and Wang, Kuan-Chieh and Zou, James and Yeung, Serena},
	year = {2022},
	note = {arXiv: 2302.04269v1},
	pages = {1--27},
	file = {PDF:/Users/vsivaram/Zotero/storage/TNTJ3Y5U/2302.04269.pdf:application/pdf},
}

@article{jain_distilling_2022,
	title = {Distilling {Model} {Failures} as {Directions} in {Latent} {Space}},
	url = {http://arxiv.org/abs/2206.14754},
	abstract = {Existing methods for isolating hard subpopulations and spurious correlations in datasets often require human intervention. This can make these methods labor-intensive and dataset-specific. To address these shortcomings, we present a scalable method for automatically distilling a model's failure modes. Specifically, we harness linear classifiers to identify consistent error patterns, and, in turn, induce a natural representation of these failure modes as directions within the feature space. We demonstrate that this framework allows us to discover and automatically caption challenging subpopulations within the training dataset. Moreover, by combining our framework with off-the-shelf diffusion models, we can generate images that are especially challenging for the analyzed model, and thus can be used to perform synthetic data augmentation that helps remedy the model's failure modes. Code available at https://github.com/MadryLab/failure-directions},
	author = {Jain, Saachi and Lawrence, Hannah and Moitra, Ankur and Madry, Aleksander},
	year = {2022},
	note = {arXiv: 2206.14754},
	file = {PDF:/Users/vsivaram/Zotero/storage/Y5VS9LTE/2206.14754.pdf:application/pdf},
}

@article{ribeiro_adaptive_2022,
	title = {Adaptive {Testing} and {Debugging} of {NLP} {Models}},
	volume = {1},
	doi = {10.18653/v1/2022.acl-long.230},
	abstract = {Current approaches to testing and debugging NLP models rely on highly variable human creativity and extensive labor, or only work for a very restrictive class of bugs. We present AdaTest, a process which uses large scale language models (LMs) in partnership with human feedback to automatically write unit tests highlighting bugs in a target model. Such bugs are then addressed through an iterative text-fix-retest loop, inspired by traditional software development. In experiments with expert and non-expert users and commercial / research models for 8 different tasks, AdaTest makes users 5-10x more effective at finding bugs than current approaches, and helps users effectively fix bugs without adding new bugs.},
	author = {Ribeiro, Marco Tulio and Lundberg, Scott},
	year = {2022},
	pages = {3253--3267},
	file = {PDF:/Users/vsivaram/Zotero/storage/VKGLEFBS/2022.acl-long.230.pdf:application/pdf},
}

@article{deon_spotlight_2022,
	title = {The {Spotlight}: {A} {General} {Method} for {Discovering} {Systematic} {Errors} in {Deep} {Learning} {Models}},
	doi = {10.1145/3531146.3533240},
	abstract = {Supervised learning models often make systematic errors on rare subsets of the data. When these subsets correspond to explicit labels in the data (e.g., gender, race) such poor performance can be identified straightforwardly. This paper introduces a method for discovering systematic errors that do not correspond to such explicitly labelled subgroups. The key idea is that similar inputs tend to have similar representations in the final hidden layer of a neural network. We leverage this structure by "shining a spotlight"on this representation space to find contiguous regions in which the model performs poorly. We show that the Spotlight surfaces semantically meaningful areas of weakness in a wide variety of existing models spanning computer vision, NLP, and recommender systems, and we verify its performance through quantitative experiments.},
	journal = {ACM International Conference Proceeding Series},
	author = {D'Eon, Greg and D'Eon, Jason and Wright, James R. and Leyton-Brown, Kevin},
	year = {2022},
	note = {arXiv: 2107.00758
ISBN: 9781450393522},
	keywords = {fairness, deep learning, auditing, distributional robustness},
	pages = {1962--1981},
	file = {PDF:/Users/vsivaram/Zotero/storage/5PAYL6XD/2107.00758.pdf:application/pdf},
}

@inproceedings{zhu_generating_2022,
	title = {Generating {Interpretable} {Data}-{Based} {Explanations} for {Fairness} {Debugging} using {Gopher}},
	volume = {1},
	isbn = {978-1-4503-9249-5},
	abstract = {Machine learning (ML) models, while increasingly being used to make life-altering decisions, are known to reinforce systemic bias and discrimination. Consequently, practitioners and model developers need tools to facilitate debugging for bias in ML models. We introduce Gopher, a system that generates compact, interpretable and causal explanations for ML model bias. Gopher identifies the top-k coherent subsets of the training data that are root causes for model bias by quantifying the extent to which removing or updating a subset can resolve the bias. We describe the architecture of Gopher and will walk the audience through real-world use cases to highlight how Gopher generates explanations that enable data scientists to understand how subsets of the training data contribute to the bias of a machine learning (ML) model. Gopher is available as open-source software; The code and the demonstration video are available at https://gopher-sys.github.io/.},
	booktitle = {Proceedings of the {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Zhu, Jiongli and Pradhan, Romila and Glavic, Boris and Salimi, Babak},
	year = {2022},
	doi = {10.1145/3514221.3520170},
	note = {arXiv: 2112.09745v2
Publication Title: Proceedings of the ACM SIGMOD International Conference on Management of Data
Issue: 1
ISSN: 07308078},
	keywords = {explanations, interpretability, fairness, data debugging},
	file = {PDF:/Users/vsivaram/Zotero/storage/BX988AKL/2112.09745.pdf:application/pdf},
}

@article{eyuboglu_domino_2022,
	title = {Domino: {Discovering} {Systematic} {Errors} with {Cross}-{Modal} {Embeddings}},
	url = {http://arxiv.org/abs/2203.14960},
	abstract = {Machine learning models that achieve high overall accuracy often make systematic errors on important subsets (or slices) of data. Identifying underperforming slices is particularly challenging when working with high-dimensional inputs (e.g. images, audio), where important slices are often unlabeled. In order to address this issue, recent studies have proposed automated slice discovery methods (SDMs), which leverage learned model representations to mine input data for slices on which a model performs poorly. To be useful to a practitioner, these methods must identify slices that are both underperforming and coherent (i.e. united by a human-understandable concept). However, no quantitative evaluation framework currently exists for rigorously assessing SDMs with respect to these criteria. Additionally, prior qualitative evaluations have shown that SDMs often identify slices that are incoherent. In this work, we address these challenges by first designing a principled evaluation framework that enables a quantitative comparison of SDMs across 1,235 slice discovery settings in three input domains (natural images, medical images, and time-series data). Then, motivated by the recent development of powerful cross-modal representation learning approaches, we present Domino, an SDM that leverages cross-modal embeddings and a novel error-aware mixture model to discover and describe coherent slices. We find that Domino accurately identifies 36\% of the 1,235 slices in our framework - a 12 percentage point improvement over prior methods. Further, Domino is the first SDM that can provide natural language descriptions of identified slices, correctly generating the exact name of the slice in 35\% of settings.},
	author = {Eyuboglu, Sabri and Varma, Maya and Saab, Khaled and Delbrouck, Jean-Benoit and Lee-Messer, Christopher and Dunnmon, Jared and Zou, James and Ré, Christopher},
	year = {2022},
	note = {arXiv: 2203.14960},
	pages = {1--28},
	file = {PDF:/Users/vsivaram/Zotero/storage/DFF4QQM3/2203.14960.pdf:application/pdf},
}

@article{chen_mandoline_2021,
	title = {Mandoline: {Model} {Evaluation} under {Distribution} {Shift}},
	url = {http://arxiv.org/abs/2107.00643},
	abstract = {Machine learning models are often deployed in different settings than they were trained and validated on, posing a challenge to practitioners who wish to predict how well the deployed model will perform on a target distribution. If an unlabeled sample from the target distribution is available, along with a labeled sample from a possibly different source distribution, standard approaches such as importance weighting can be applied to estimate performance on the target. However, importance weighting struggles when the source and target distributions have non-overlapping support or are high-dimensional. Taking inspiration from fields such as epidemiology and polling, we develop Mandoline, a new evaluation framework that mitigates these issues. Our key insight is that practitioners may have prior knowledge about the ways in which the distribution shifts, which we can use to better guide the importance weighting procedure. Specifically, users write simple "slicing functions" - noisy, potentially correlated binary functions intended to capture possible axes of distribution shift - to compute reweighted performance estimates. We further describe a density ratio estimation framework for the slices and show how its estimation error scales with slice quality and dataset size. Empirical validation on NLP and vision tasks shows that {\textbackslash}name can estimate performance on the target distribution up to \$3{\textbackslash}times\$ more accurately compared to standard baselines.},
	author = {Chen, Mayee and Goel, Karan and Sohoni, Nimit and Poms, Fait and Fatahalian, Kayvon and Ré, Christopher},
	year = {2021},
	note = {arXiv: 2107.00643},
	pages = {1--33},
	file = {PDF:/Users/vsivaram/Zotero/storage/P6482GGB/2107.00643.pdf:application/pdf},
}

@inproceedings{suresh_kaleidoscope_2023,
author = {Suresh, Harini and Shanmugam, Divya and Chen, Tiffany and Bryan, Annie G and D'Amour, Alexander and Guttag, John and Satyanarayan, Arvind},
title = {Kaleidoscope: Semantically-grounded, context-specific ML model evaluation},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581482},
doi = {10.1145/3544548.3581482},
abstract = {Desired model behavior often differs across contexts (e.g., different geographies, communities, or institutions), but there is little infrastructure to facilitate context-specific evaluations key to deployment decisions and building trust. Here, we present Kaleidoscope, a system for evaluating models in terms of user-driven, domain-relevant concepts. Kaleidoscope’s iterative workflow enables generalizing from a few examples into a larger, diverse set representing an important concept. These example sets can be used to test model outputs or shifts in model behavior in semantically-meaningful ways. For instance, we might construct a “xenophobic comments” set and test that its examples are more likely to be flagged by a content moderation model than a “civil discussion” set. To evaluate Kaleidoscope, we compare it against template- and DSL-based grouping methods, and conduct a usability study with 13 Reddit users testing a content moderation model. We find that Kaleidoscope facilitates iterative, exploratory hypothesis testing across diverse, conceptually-meaningful example sets.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {775},
numpages = {13},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{robertson_angler_2023,
	title = {Angler : {Helping} {Machine} {Translation} {Practitioners} {Prioritize} {Model} {Improvements}},
	volume = {1},
	doi = {10.1145/3544548.3580790},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems} ({CHI} '23)},
	publisher = {Association for Computing Machinery},
	author = {Robertson, Samantha and Wang, Zijie J and Moritz, Dominik and Kery, Mary Beth and Hohman, Fred},
	year = {2023},
	doi = {10.1145/3544548.3580790},
	note = {Publication Title: , April 23â•ﬁ28, 2023, Hamburg, Germany
Issue: 1},
	keywords = {model evaluation, visual analytics, acm reference format, dominik moritz, machine translation, mary beth kery, Model evaluation, machine translation, visual anal, samantha robertson, wang, zijie j},
	file = {PDF:/Users/vsivaram/Zotero/storage/RA4MKCBZ/23_translate_chi.pdf:application/pdf},
}

@article{munechika_visual_2022,
	title = {Visual {Auditor}: {Interactive} {Visualization} for {Detection} and {Summarization} of {Model} {Biases}},
	doi = {10.1109/VIS54862.2022.00018},
	abstract = {As machine learning (ML) systems become increasingly widespread, it is necessary to audit these systems for biases prior to their de-ployment. Recent research has developed algorithms for effectively identifying intersectional bias in the form of interpretable, underper-forming subsets (or slices) of the data. However, these solutions and their insights are limited without a tool for visually understanding and interacting with the results of these algorithms. We propose Visual Auditor, an interactive visualization tool for auditing and summarizing model biases. Visual Auditor assists model validation by providing an interpretable overview of intersectional bias (bias that is present when examining populations defined by multiple features), details about relationships between problematic data slices, and a comparison between underperforming and overper-forming data slices in a model. Our open-source tool runs directly in both computational notebooks and web browsers, making model auditing accessible and easily integrated into current ML development workflows. An observational user study in collaboration with domain experts at Fiddler AI highlights that our tool can help ML practitioners identify and understand model biases.},
	journal = {Proceedings - 2022 IEEE Visualization Conference - Short Papers, VIS 2022},
	author = {Munechika, David and Wang, Zijie J. and Reidy, Jack and Rubin, Josh and Gade, Krishna and Kenthapadi, Krishnaram and Chau, Duen Horng},
	year = {2022},
	note = {arXiv: 2206.12540
ISBN: 9781665488129},
	keywords = {Visualization, Human-centered computing},
	pages = {45--49},
	file = {PDF:/Users/vsivaram/Zotero/storage/YSZKDTAE/2206.12540.pdf:application/pdf},
}

@inproceedings{hedderich_label-descriptive_2022,
	title = {Label-{Descriptive} {Patterns} and {Their} {Application} to {Characterizing} {Classification} {Errors}},
	url = {https://proceedings.mlr.press/v162/hedderich22a.html},
	abstract = {State-of-the-art deep learning methods achieve human-like performance on many tasks, but make errors nevertheless. Characterizing these errors in easily interpretable terms gives insight into whether a classifier is prone to making systematic errors, but also gives a way to act and improve the classifier. We propose to discover those feature-value combinations (i.e., patterns) that strongly correlate with correct resp. erroneous predictions to obtain a global and interpretable description for arbitrary classifiers. We show this is an instance of the more general label description problem, which we formulate in terms of the Minimum Description Length principle. To discover a good pattern set, we develop the efficient Premise algorithm. Through an extensive set of experiments we show it performs very well in practice on both synthetic and real-world data. Unlike existing solutions, it ably recovers ground truth patterns, even on highly imbalanced data over many features. Through two case studies on Visual Question Answering and Named Entity Recognition, we confirm that Premise gives clear and actionable insight into the systematic errors made by modern NLP classifiers.},
	language = {en},
	urldate = {2023-11-13},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Hedderich, Michael A. and Fischer, Jonas and Klakow, Dietrich and Vreeken, Jilles},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {8691--8707},
	file = {Full Text PDF:/Users/vsivaram/Zotero/storage/KIH3AVDC/Hedderich et al. - 2022 - Label-Descriptive Patterns and Their Application t.pdf:application/pdf},
}

@misc{suzuki_rule_2023,
	title = {Rule {Mining} for {Correcting} {Classification} {Models}},
	url = {http://arxiv.org/abs/2310.06446},
	abstract = {Machine learning models need to be continually updated or corrected to ensure that the prediction accuracy remains consistently high. In this study, we consider scenarios where developers should be careful to change the prediction results by the model correction, such as when the model is part of a complex system or software. In such scenarios, the developers want to control the specification of the corrections. To achieve this, the developers need to understand which subpopulations of the inputs get inaccurate predictions by the model. Therefore, we propose correction rule mining to acquire a comprehensive list of rules that describe inaccurate subpopulations and how to correct them. We also develop an efficient correction rule mining algorithm that is a combination of frequent itemset mining and a unique pruning technique for correction rules. We observed that the proposed algorithm found various rules which help to collect data insufficiently learned, directly correct model outputs, and analyze concept drift.},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Suzuki, Hirofumi and Iwashita, Hiroaki and Takagi, Takuya and Fujishige, Yuta and Hara, Satoshi},
	month = oct,
	year = {2023},
	note = {arXiv:2310.06446 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {arXiv.org Snapshot:/Users/vsivaram/Zotero/storage/EHTMITF3/2310.html:text/html;Full Text PDF:/Users/vsivaram/Zotero/storage/WEU83WBP/Suzuki et al. - 2023 - Rule Mining for Correcting Classification Models.pdf:application/pdf},
}

@article{piorkowski_aimee_2023,
	title = {{AIMEE}: {An} {Exploratory} {Study} of {How} {Rules} {Support} {AI} {Developers} to {Explain} and {Edit} {Models}},
	volume = {7},
	issn = {2573-0142},
	shorttitle = {{AIMEE}},
	url = {https://dl.acm.org/doi/10.1145/3610046},
	doi = {10.1145/3610046},
	abstract = {In real-world applications when deploying Machine Learning (ML) models, initial model development includes close analysis of the model results and behavior by a data scientist. Once trained, however, models may need to be retrained with new data or updated to adhere to new rules or regulations. This presents two challenges. First, how to communicate how a model is making its decisions before and after retraining, and second how to support model editing to take into account new requirements. To address these needs, we built AIMEE (AI Model Explorer and Editor), a tool created to address these challenges by providing interactive methods to explain, visualize, and modify model decision boundaries using rules. Rules should benefit model builders by providing a layer of abstraction for understanding and manipulating the model and reduces the need to modify individual rows of data directly. To evaluate if this was the case, we conducted a pair of user studies totaling 23 participants to evaluate AIMEE's rules-based approach for model explainability and editing. We found that participants correctly interpreted rules and report on their perspectives of how rules are beneficial (and not), ways that rules could support collaboration, and provide a usability evaluation of the tool.},
	language = {en},
	number = {CSCW2},
	urldate = {2023-12-05},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	author = {Piorkowski, David and Vejsbjerg, Inge and Cornec, Owen and Daly, Elizabeth M. and Alkan, Öznur},
	month = sep,
	year = {2023},
	pages = {1--25},
	file = {Full Text PDF:/Users/vsivaram/Zotero/storage/R8C9JY5F/Piorkowski et al. - 2023 - AIMEE An Exploratory Study of How Rules Support A.pdf:application/pdf},
}

@article{bogl_visual_2013,
	title = {Visual {Analytics} for {Model} {Selection} in {Time} {Series} {Analysis}},
	volume = {19},
	issn = {1077-2626},
	url = {http://ieeexplore.ieee.org/document/6634112/},
	doi = {10.1109/TVCG.2013.222},
	abstract = {Model selection in time series analysis is a challenging task for domain experts in many application areas such as epidemiology, economy, or environmental sciences. The methodology used for this task demands a close combination of human judgement and automated computation. However, statistical software tools do not adequately support this combination through interactive visual interfaces. We propose a Visual Analytics process to guide domain experts in this task. For this purpose, we developed the TiMoVA prototype that implements this process based on user stories and iterative expert feedback on user experience. The prototype was evaluated by usage scenarios with an example dataset from epidemiology and interviews with two external domain experts in statistics. The insights from the experts’ feedback and the usage scenarios show that TiMoVA is able to support domain experts in model selection tasks through interactive visual interfaces with short feedback cycles.},
	language = {en},
	number = {12},
	urldate = {2024-01-15},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Bogl, Markus and Aigner, Wolfgang and Filzmoser, Peter and Lammarsch, Tim and Miksch, Silvia and Rind, Alexander},
	month = dec,
	year = {2013},
	pages = {2237--2246},
	file = {Bogl et al. - 2013 - Visual Analytics for Model Selection in Time Serie.pdf:/Users/vsivaram/Zotero/storage/C2DBQJXS/Bogl et al. - 2013 - Visual Analytics for Model Selection in Time Serie.pdf:application/pdf},
}

@article{bernard_visual-interactive_2019,
	title = {Visual-{Interactive} {Preprocessing} of {Multivariate} {Time} {Series} {Data}},
	volume = {38},
	issn = {1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13698},
	doi = {10.1111/cgf.13698},
	abstract = {Pre-processing is a prerequisite to conduct effective and efficient downstream data analysis. Pre-processing pipelines often require multiple routines to address data quality challenges and to bring the data into a usable form. For both the construction and the refinement of pre-processing pipelines, human-in-the-loop approaches are highly beneficial. This particularly applies to multivariate time series, a complex data type with multiple values developing over time. Due to the high specificity of this domain, it has not been subject to in-depth research in visual analytics. We present a visual-interactive approach for preprocessing multivariate time series data with the following aspects. Our approach supports analysts to carry out six core analysis tasks related to pre-processing of multivariate time series. To support these tasks, we identify requirements to baseline toolkits that may help practitioners in their choice. We characterize the space of visualization designs for uncertainty-aware pre-processing and justify our decisions. Two usage scenarios demonstrate applicability of our approach, design choices, and uncertainty visualizations for the six analysis tasks. This work is one step towards strengthening the visual analytics support for data pre-processing in general and for uncertainty-aware pre-processing of multivariate time series in particular.},
	language = {en},
	number = {3},
	urldate = {2024-01-29},
	journal = {Computer Graphics Forum},
	author = {Bernard, Jürgen and Hutter, Marco and Reinemuth, Heiko and Pfeifer, Hendrik and Bors, Christian and Kohlhammer, Jörn},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.13698},
	keywords = {CCS Concepts, • Human-centered computing → Visual analytics, Mathematics of computing → Time series analysis},
	pages = {401--412},
	file = {Full Text PDF:/Users/vsivaram/Zotero/storage/NUNASBML/Bernard et al. - 2019 - Visual-Interactive Preprocessing of Multivariate T.pdf:application/pdf;Snapshot:/Users/vsivaram/Zotero/storage/GWEM8Z59/cgf.html:text/html},
}

@inproceedings{sun_dfseer_2020,
	address = {Honolulu HI USA},
	title = {{DFSeer}: {A} {Visual} {Analytics} {Approach} to {Facilitate} {Model} {Selection} for {Demand} {Forecasting}},
	isbn = {978-1-4503-6708-0},
	shorttitle = {{DFSeer}},
	url = {https://dl.acm.org/doi/10.1145/3313831.3376866},
	doi = {10.1145/3313831.3376866},
	language = {en},
	urldate = {2024-01-29},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Sun, Dong and Feng, Zezheng and Chen, Yuanzhe and Wang, Yong and Zeng, Jia and Yuan, Mingxuan and Pong, Ting-Chuen and Qu, Huamin},
	month = apr,
	year = {2020},
	pages = {1--13},
	file = {Full Text PDF:/Users/vsivaram/Zotero/storage/RSMHL5LA/Sun et al. - 2020 - DFSeer A Visual Analytics Approach to Facilitate .pdf:application/pdf},
}

@inproceedings{xu_mtseer_2021,
	address = {Yokohama Japan},
	title = {{mTSeer}: {Interactive} {Visual} {Exploration} of {Models} on {Multivariate} {Time}-series {Forecast}},
	isbn = {978-1-4503-8096-6},
	shorttitle = {{mTSeer}},
	url = {https://dl.acm.org/doi/10.1145/3411764.3445083},
	doi = {10.1145/3411764.3445083},
	language = {en},
	urldate = {2024-01-29},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Xu, Ke and Yuan, Jun and Wang, Yifang and Silva, Claudio and Bertini, Enrico},
	month = may,
	year = {2021},
	pages = {1--15},
	file = {Full Text PDF:/Users/vsivaram/Zotero/storage/33BGDXMK/Xu et al. - 2021 - mTSeer Interactive Visual Exploration of Models o.pdf:application/pdf},
}

@article{liu_patterns_2017,
	title = {Patterns and {Sequences}: {Interactive} {Exploration} of {Clickstreams} to {Understand} {Common} {Visitor} {Paths}},
	volume = {23},
	issn = {1077-2626},
	shorttitle = {Patterns and {Sequences}},
	url = {http://ieeexplore.ieee.org/document/7539341/},
	doi = {10.1109/TVCG.2016.2598797},
	abstract = {Modern web clickstream data consists of long, high-dimensional sequences of multivariate events, making it difﬁcult to analyze. Following the overarching principle that the visual interface should provide information about the dataset at multiple levels of granularity and allow users to easily navigate across these levels, we identify four levels of granularity in clickstream analysis: patterns, segments, sequences and events. We present an analytic pipeline consisting of three stages: pattern mining, pattern pruning and coordinated exploration between patterns and sequences. Based on this approach, we discuss properties of maximal sequential patterns, propose methods to reduce the number of patterns and describe design considerations for visualizing the extracted sequential patterns and the corresponding raw sequences. We demonstrate the viability of our approach through an analysis scenario and discuss the strengths and limitations of the methods based on user feedback.},
	language = {en},
	number = {1},
	urldate = {2024-01-29},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Liu, Zhicheng and Wang, Yang and Dontcheva, Mira and Hoffman, Matthew and Walker, Seth and Wilson, Alan},
	month = jan,
	year = {2017},
	pages = {321--330},
	file = {Liu et al. - 2017 - Patterns and Sequences Interactive Exploration of.pdf:/Users/vsivaram/Zotero/storage/D5PX8K8X/Liu et al. - 2017 - Patterns and Sequences Interactive Exploration of.pdf:application/pdf},
}

@inproceedings{du_interactive_2020,
	address = {Honolulu HI USA},
	title = {Interactive {Event} {Sequence} {Prediction} for {Marketing} {Analysts}},
	isbn = {978-1-4503-6819-3},
	url = {https://dl.acm.org/doi/10.1145/3334480.3382971},
	doi = {10.1145/3334480.3382971},
	abstract = {Timestamped event sequences are analyzed to tackle varied problems but have unique challenges in interpretation and analysis. Especially in event sequence prediction, it is difﬁcult to convey the results due to the added uncertainty and complexity introduced by predictive models. In this work, we design and develop ProFlow, a visual analytics system for supporting analysts’ workﬂow of exploring and predicting event sequences. Through an evaluation conducted with four data analysts in a real-world marketing scenario, we discuss the applicability and usefulness of ProFlow as well as its limitations and future directions.},
	language = {en},
	urldate = {2024-01-29},
	booktitle = {Extended {Abstracts} of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Du, Fan and Guo, Shunan and Malik, Sana and Koh, Eunyee and Kim, Sungchul and Liu, Zhicheng},
	month = apr,
	year = {2020},
	pages = {1--8},
	file = {Du et al. - 2020 - Interactive Event Sequence Prediction for Marketin.pdf:/Users/vsivaram/Zotero/storage/2DBE4UX8/Du et al. - 2020 - Interactive Event Sequence Prediction for Marketin.pdf:application/pdf},
}

@inproceedings{guo_visualizing_2019,
	address = {Glasgow Scotland Uk},
	title = {Visualizing {Uncertainty} and {Alternatives} in {Event} {Sequence} {Predictions}},
	isbn = {978-1-4503-5970-2},
	url = {https://dl.acm.org/doi/10.1145/3290605.3300803},
	doi = {10.1145/3290605.3300803},
	language = {en},
	urldate = {2024-01-29},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Guo, Shunan and Du, Fan and Malik, Sana and Koh, Eunyee and Kim, Sungchul and Liu, Zhicheng and Kim, Donghyun and Zha, Hongyuan and Cao, Nan},
	month = may,
	year = {2019},
	pages = {1--12},
	annote = {This paper provides evidence that people want to see both future temporal events (actions or states) and outcomes. This is one of the main things that flexibility about the outcome variable in our system allows.
},
	file = {Full Text PDF:/Users/vsivaram/Zotero/storage/UA8HXR8R/Guo et al. - 2019 - Visualizing Uncertainty and Alternatives in Event .pdf:application/pdf},
}

@article{cho_stroscope_2014,
	title = {Stroscope: {Multi}-{Scale} {Visualization} of {Irregularly} {Measured} {Time}-{Series} {Data}},
	volume = {20},
	issn = {1077-2626},
	shorttitle = {Stroscope},
	url = {http://ieeexplore.ieee.org/document/6702502/},
	doi = {10.1109/TVCG.2013.2297933},
	abstract = {For irregularly measured time-series data, the measurement frequency or interval is as crucial information as measurements are. A well-known time-series visualization such as the line graph is good at showing an overall temporal pattern of change; however, it is not so effective in revealing the measurement frequency/interval while likely giving illusory conﬁdence in values between measurements. In contrast, the bar graph is more effective in showing the frequency/interval, but less effective in showing an overall pattern than the line graph. We integrate the line graph and bar graph in a uniﬁed visualization model, called a ripple graph, to take the beneﬁts of both of them with enhanced graphical integrity. Based on the ripple graph, we implemented an interactive timeseries data visualization tool, called Stroscope, which facilitates multi-scale visualizations by providing users with a graphical widget to interactively control the integrated visualization model. We evaluated the visualization model (i.e., the ripple graph) through a controlled user study and Stroscope through long-term case studies with neurologists exploring large blood pressure measurement data of stroke patients. Results from our evaluations demonstrate that the ripple graph outperforms existing time-series visualizations, and that Stroscope has the efﬁcacy and potential as an effective visual analysis tool for (irregularly) measured time-series data.},
	language = {en},
	number = {5},
	urldate = {2024-01-29},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Cho, Myoungsu and Kim, Bohyoung and Bae, Hee-Joon and Seo, Jinwook},
	month = may,
	year = {2014},
	pages = {808--821},
	file = {Cho et al. - 2014 - Stroscope Multi-Scale Visualization of Irregularl.pdf:/Users/vsivaram/Zotero/storage/ACDKDR5S/Cho et al. - 2014 - Stroscope Multi-Scale Visualization of Irregularl.pdf:application/pdf},
}

@incollection{costabile_representing_2005,
	address = {Berlin, Heidelberg},
	title = {Representing {Unevenly}-{Spaced} {Time} {Series} {Data} for {Visualization} and {Interactive} {Exploration}},
	volume = {3585},
	isbn = {978-3-540-28943-2 978-3-540-31722-7},
	url = {http://link.springer.com/10.1007/11555261_66},
	abstract = {Visualizing time series is useful to support discovery of relations and patterns in financial, genomic, medical and other applications. Often, measurements are equally spaced over time. We discuss the challenges of unevenly-spaced time series and present four representation methods: sampled events, aggregated sampled events, event index and interleaved event index. We developed these methods while studying eBay auction data with TimeSearcher. We describe the advantages, disadvantages, choices for algorithms and parameters, and compare the different methods for different tasks. Interaction issues such as screen resolution, response time for dynamic queries, and learnability are governed by these decisions.},
	language = {en},
	urldate = {2024-01-29},
	booktitle = {Human-{Computer} {Interaction} - {INTERACT} 2005},
	publisher = {Springer Berlin Heidelberg},
	author = {Aris, Aleks and Shneiderman, Ben and Plaisant, Catherine and Shmueli, Galit and Jank, Wolfgang},
	editor = {Costabile, Maria Francesca and Paternò, Fabio},
	year = {2005},
	doi = {10.1007/11555261_66},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {835--846},
	file = {Aris et al. - 2005 - Representing Unevenly-Spaced Time Series Data for .pdf:/Users/vsivaram/Zotero/storage/ABV6IGSR/Aris et al. - 2005 - Representing Unevenly-Spaced Time Series Data for .pdf:application/pdf},
}

@misc{yuan_visual_2022,
	title = {Visual {Exploration} of {Machine} {Learning} {Model} {Behavior} with {Hierarchical} {Surrogate} {Rule} {Sets}},
	url = {http://arxiv.org/abs/2201.07724},
	abstract = {One of the potential solutions for model interpretation is to train a surrogate model: a more transparent model that approximates the behavior of the model to be explained. Typically, classification rules or decision trees are used due to the intelligibility of their logic-based expressions. However, decision trees can grow too deep and rule sets can become too large to approximate a complex model. Unlike paths on a decision tree that must share ancestor nodes (conditions), rules are more flexible. However, the unstructured visual representation of rules makes it hard to make inferences across rules. To address these issues, we present a workflow that includes novel algorithmic and interactive solutions. First, we present Hierarchical Surrogate Rules (HSR), an algorithm that generates hierarchical rules based on user-defined parameters. We also contribute SuRE, a visual analytics (VA) system that integrates HSR and interactive surrogate rule visualizations. Particularly, we present a novel feature-aligned tree to overcome the shortcomings of existing rule visualizations. We evaluate the algorithm in terms of parameter sensitivity, time performance, and comparison with surrogate decision trees and find that it scales reasonably well and outperforms decision trees in many respects. We also evaluate the visualization and the VA system by a usability study with 24 volunteers and an observational study with 7 domain experts. Our investigation shows that the participants can use feature-aligned trees to perform non-trivial tasks with very high accuracy. We also discuss many interesting observations that can be useful for future research on designing effective rule-based VA systems.},
	urldate = {2024-02-02},
	publisher = {arXiv},
	author = {Yuan, Jun and Barr, Brian and Overton, Kyle and Bertini, Enrico},
	month = jan,
	year = {2022},
	note = {arXiv:2201.07724 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Human-Computer Interaction},
	file = {arXiv.org Snapshot:/Users/vsivaram/Zotero/storage/DLCQFXIR/2201.html:text/html;Full Text PDF:/Users/vsivaram/Zotero/storage/JYUV3YHE/Yuan et al. - 2022 - Visual Exploration of Machine Learning Model Behav.pdf:application/pdf},
}

@misc{bhattacharya_exmos_2024,
	title = {{EXMOS}: {Explanatory} {Model} {Steering} {Through} {Multifaceted} {Explanations} and {Data} {Configurations}},
	shorttitle = {{EXMOS}},
	url = {http://arxiv.org/abs/2402.00491},
	doi = {10.1145/3613904.3642106},
	abstract = {Explanations in interactive machine-learning systems facilitate debugging and improving prediction models. However, the effectiveness of various global model-centric and data-centric explanations in aiding domain experts to detect and resolve potential data issues for model improvement remains unexplored. This research investigates the influence of data-centric and model-centric global explanations in systems that support healthcare experts in optimising models through automated and manual data configurations. We conducted quantitative (n=70) and qualitative (n=30) studies with healthcare experts to explore the impact of different explanations on trust, understandability and model improvement. Our results reveal the insufficiency of global model-centric explanations for guiding users during data configuration. Although data-centric explanations enhanced understanding of post-configuration system changes, a hybrid fusion of both explanation types demonstrated the highest effectiveness. Based on our study results, we also present design implications for effective explanation-driven interactive machine-learning systems.},
	urldate = {2024-02-05},
	author = {Bhattacharya, Aditya and Stumpf, Simone and Gosak, Lucija and Stiglic, Gregor and Verbert, Katrien},
	month = feb,
	year = {2024},
	note = {arXiv:2402.00491 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	annote = {Comment: This is a pre-print version only for early release. Please view the conference published version from ACM CHI 2024 to get the latest version of the paper},
	file = {arXiv.org Snapshot:/Users/vsivaram/Zotero/storage/EEB6WY9G/2402.html:text/html;Full Text PDF:/Users/vsivaram/Zotero/storage/9FWJNW44/Bhattacharya et al. - 2024 - EXMOS Explanatory Model Steering Through Multifac.pdf:application/pdf},
}

@inproceedings{amershi_examining_2010,
	address = {Atlanta Georgia USA},
	title = {Examining multiple potential models in end-user interactive concept learning},
	isbn = {978-1-60558-929-9},
	url = {https://dl.acm.org/doi/10.1145/1753326.1753531},
	doi = {10.1145/1753326.1753531},
	language = {en},
	urldate = {2024-02-05},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Amershi, Saleema and Fogarty, James and Kapoor, Ashish and Tan, Desney},
	month = apr,
	year = {2010},
	pages = {1357--1360},
	file = {Full Text PDF:/Users/vsivaram/Zotero/storage/P5SCFVJV/Amershi et al. - 2010 - Examining multiple potential models in end-user in.pdf:application/pdf},
}

@inproceedings{kulesza_principles_2015,
	address = {Atlanta Georgia USA},
	title = {Principles of {Explanatory} {Debugging} to {Personalize} {Interactive} {Machine} {Learning}},
	isbn = {978-1-4503-3306-1},
	url = {https://dl.acm.org/doi/10.1145/2678025.2701399},
	doi = {10.1145/2678025.2701399},
	abstract = {How can end users eﬃciently inﬂuence the predictions that machine learning systems make on their behalf? This paper presents Explanatory Debugging, an approach in which the system explains to users how it made each of its predictions, and the user then explains any necessary corrections back to the learning system. We present the principles underlying this approach and a prototype instantiating it. An empirical evaluation shows that Explanatory Debugging increased participants’ understanding of the learning system by 52\% and allowed participants to correct its mistakes up to twice as eﬃciently as participants using a traditional learning system.},
	language = {en},
	urldate = {2024-02-05},
	booktitle = {Proceedings of the 20th {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {ACM},
	author = {Kulesza, Todd and Burnett, Margaret and Wong, Weng-Keen and Stumpf, Simone},
	month = mar,
	year = {2015},
	pages = {126--137},
	file = {Kulesza et al. - 2015 - Principles of Explanatory Debugging to Personalize.pdf:/Users/vsivaram/Zotero/storage/L78SGIR3/Kulesza et al. - 2015 - Principles of Explanatory Debugging to Personalize.pdf:application/pdf},
}

@inproceedings{talbot_ensemblematrix_2009,
	address = {Boston MA USA},
	title = {{EnsembleMatrix}: interactive visualization to support machine learning with multiple classifiers},
	isbn = {978-1-60558-246-7},
	shorttitle = {{EnsembleMatrix}},
	url = {https://dl.acm.org/doi/10.1145/1518701.1518895},
	doi = {10.1145/1518701.1518895},
	language = {en},
	urldate = {2024-02-05},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Talbot, Justin and Lee, Bongshin and Kapoor, Ashish and Tan, Desney S.},
	month = apr,
	year = {2009},
	pages = {1283--1292},
	file = {Full Text PDF:/Users/vsivaram/Zotero/storage/ET8UXNDI/Talbot et al. - 2009 - EnsembleMatrix interactive visualization to suppo.pdf:application/pdf},
}

@misc{guo_survey_2020,
	title = {Survey on {Visual} {Analysis} of {Event} {Sequence} {Data}},
	url = {http://arxiv.org/abs/2006.14291},
	abstract = {Event sequence data record series of discrete events in the time order of occurrence. They are commonly observed in a variety of applications ranging from electronic health records to network logs, with the characteristics of large-scale, high-dimensional, and heterogeneous. This high complexity of event sequence data makes it difficult for analysts to manually explore and find patterns, resulting in ever-increasing needs for computational and perceptual aids from visual analytics techniques to extract and communicate insights from event sequence datasets. In this paper, we review the state-of-the-art visual analytics approaches, characterize them with our proposed design space, and categorize them based on analytical tasks and applications. From our review of relevant literature, we have also identified several remaining research challenges and future research opportunities.},
	urldate = {2024-02-06},
	publisher = {arXiv},
	author = {Guo, Yi and Guo, Shunan and Jin, Zhuochen and Kaul, Smiti and Gotz, David and Cao, Nan},
	month = jun,
	year = {2020},
	note = {arXiv:2006.14291 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
	file = {arXiv.org Snapshot:/Users/vsivaram/Zotero/storage/YZPP3RPT/2006.html:text/html;Full Text PDF:/Users/vsivaram/Zotero/storage/9TDWT735/Guo et al. - 2020 - Survey on Visual Analysis of Event Sequence Data.pdf:application/pdf},
}

@article{wu_modeling_2018,
	title = {Modeling asynchronous event sequences with {RNNs}},
	volume = {83},
	issn = {15320464},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1532046418300996},
	doi = {10.1016/j.jbi.2018.05.016},
	abstract = {Sequences of events have often been modeled with computational techniques, but typical preprocessing steps and problem settings do not explicitly address the ramiﬁcations of timestamped events. Clinical data, such as is found in electronic health records (EHRs), typically comes with timestamp information. In this work, we deﬁne event sequences and their properties: synchronicity, evenness, and co-cardinality; we then show how asynchronous, uneven, and multi-cardinal problem settings can support explicit accountings of relative time. Our evaluation uses the temporally sensitive clinical use case of pediatric asthma, which is a chronic disease with symptoms (and lack thereof) evolving over time. We show several approaches to explicitly incorporating relative time into a recurrent neural network (RNN) model that improve the overall classiﬁcation of patients into those with no asthma, those with persistent asthma, those in long-term remission, and those who have experienced relapse. We also compare and contrast these results with those in an inpatient intensive care setting.},
	language = {en},
	urldate = {2024-02-06},
	journal = {Journal of Biomedical Informatics},
	author = {Wu, Stephen and Liu, Sijia and Sohn, Sunghwan and Moon, Sungrim and Wi, Chung-il and Juhn, Young and Liu, Hongfang},
	month = jul,
	year = {2018},
	pages = {167--177},
	file = {Wu et al. - 2018 - Modeling asynchronous event sequences with RNNs.pdf:/Users/vsivaram/Zotero/storage/WC44S84W/Wu et al. - 2018 - Modeling asynchronous event sequences with RNNs.pdf:application/pdf},
}

@article{monroe_temporal_2013,
	title = {Temporal {Event} {Sequence} {Simplification}},
	volume = {19},
	issn = {1941-0506},
	url = {https://ieeexplore.ieee.org/abstract/document/6634100},
	doi = {10.1109/TVCG.2013.200},
	abstract = {Electronic Health Records (EHRs) have emerged as a cost-effective data source for conducting medical research. The difficulty in using EHRs for research purposes, however, is that both patient selection and record analysis must be conducted across very large, and typically very noisy datasets. Our previous work introduced EventFlow, a visualization tool that transforms an entire dataset of temporal event records into an aggregated display, allowing researchers to analyze population-level patterns and trends. As datasets become larger and more varied, however, it becomes increasingly difficult to provide a succinct, summarizing display. This paper presents a series of user-driven data simplifications that allow researchers to pare event records down to their core elements. Furthermore, we present a novel metric for measuring visual complexity, and a language for codifying disjoint strategies into an overarching simplification framework. These simplifications were used by real-world researchers to gain new and valuable insights from initially overwhelming datasets.},
	number = {12},
	urldate = {2024-02-20},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Monroe, Megan and Lan, Rongjian and Lee, Hanseung and Plaisant, Catherine and Shneiderman, Ben},
	month = dec,
	year = {2013},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	keywords = {Data visualization, Data mining, Electronic medical records, Complexity theory, electronic heath records, Event sequences, Market research, simplification, temporal query},
	pages = {2227--2236},
	file = {IEEE Xplore Abstract Record:/Users/vsivaram/Zotero/storage/73B7DIR2/6634100.html:text/html;IEEE Xplore Full Text PDF:/Users/vsivaram/Zotero/storage/KWF7FQFI/Monroe et al. - 2013 - Temporal Event Sequence Simplification.pdf:application/pdf},
}

@incollection{simoff_datajewel_2008,
	address = {Berlin, Heidelberg},
	title = {{DataJewel}: {Integrating} {Visualization} with {Temporal} {Data} {Mining}},
	volume = {4404},
	isbn = {978-3-540-71079-0 978-3-540-71080-6},
	shorttitle = {{DataJewel}},
	url = {http://link.springer.com/10.1007/978-3-540-71080-6_19},
	abstract = {In this chapter we describe DataJewel, a new temporal data mining architecture. DataJewel tightly integrates a visualization component, an algorithmic component and a database component. We introduce a new visualization technique called CalendarView as an implementation of the visualization component, and we introduce a data structure that supports temporal mining of large databases. In our architecture, algorithms can be tightly integrated with the visualization component and most existing temporal data mining algorithms can be leveraged by embedding them into DataJewel. This integration is achieved by an interface that is used by both the user and the algorithms to assign colors to events. The user interactively assigns colors to incorporate domain knowledge or to formulate hypotheses. The algorithm assigns colors based on discovered patterns. The same visualization technique is used for displaying both data and patterns to make it more intuitive for the user to identify useful patterns while exploring data interactively or while using algorithms to search for patterns. Our experiments in analyzing several large datasets from the airplane maintenance domain demonstrate the usefulness of our approach and we discuss its applicability to domains like homeland security, market basket analysis and web mining.},
	language = {en},
	urldate = {2024-02-20},
	booktitle = {Visual {Data} {Mining}},
	publisher = {Springer Berlin Heidelberg},
	author = {Ankerst, Mihael and Kao, Anne and Tjoelker, Rodney and Wang, Changzhou},
	editor = {Simoff, Simeon J. and Böhlen, Michael H. and Mazeika, Arturas},
	year = {2008},
	doi = {10.1007/978-3-540-71080-6_19},
	note = {ISSN: 0302-9743, 1611-3349
Series Title: Lecture Notes in Computer Science},
	pages = {312--330},
	file = {Ankerst et al. - 2008 - DataJewel Integrating Visualization with Temporal.pdf:/Users/vsivaram/Zotero/storage/KHWUKCJQ/Ankerst et al. - 2008 - DataJewel Integrating Visualization with Temporal.pdf:application/pdf},
}

@misc{guo_survey_2020-1,
	title = {Survey on {Visual} {Analysis} of {Event} {Sequence} {Data}},
	url = {http://arxiv.org/abs/2006.14291},
	abstract = {Event sequence data record series of discrete events in the time order of occurrence. They are commonly observed in a variety of applications ranging from electronic health records to network logs, with the characteristics of large-scale, high-dimensional, and heterogeneous. This high complexity of event sequence data makes it difficult for analysts to manually explore and find patterns, resulting in ever-increasing needs for computational and perceptual aids from visual analytics techniques to extract and communicate insights from event sequence datasets. In this paper, we review the state-of-the-art visual analytics approaches, characterize them with our proposed design space, and categorize them based on analytical tasks and applications. From our review of relevant literature, we have also identified several remaining research challenges and future research opportunities.},
	urldate = {2024-02-20},
	publisher = {arXiv},
	author = {Guo, Yi and Guo, Shunan and Jin, Zhuochen and Kaul, Smiti and Gotz, David and Cao, Nan},
	month = jun,
	year = {2020},
	note = {arXiv:2006.14291 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
	file = {arXiv.org Snapshot:/Users/vsivaram/Zotero/storage/W266YPLH/2006.html:text/html;Full Text PDF:/Users/vsivaram/Zotero/storage/ANWDXJ5J/Guo et al. - 2020 - Survey on Visual Analysis of Event Sequence Data.pdf:application/pdf},
}

@article{du_coping_2017,
	title = {Coping with {Volume} and {Variety} in {Temporal} {Event} {Sequences}: {Strategies} for {Sharpening} {Analytic} {Focus}},
	volume = {23},
	issn = {1077-2626},
	shorttitle = {Coping with {Volume} and {Variety} in {Temporal} {Event} {Sequences}},
	url = {http://ieeexplore.ieee.org/document/7429778/},
	doi = {10.1109/TVCG.2016.2539960},
	abstract = {The growing volume and variety of data presents both opportunities and challenges for visual analytics. Addressing these challenges is needed for big data to provide valuable insights and novel solutions for business, security, social media, and healthcare. In the case of temporal event sequence analytics it is the number of events in the data and variety of temporal sequence patterns that challenges users of visual analytic tools. This paper describes 15 strategies for sharpening analytic focus that analysts can use to reduce the data volume and pattern variety. Four groups of strategies are proposed: (1) extraction strategies, (2) temporal folding, (3) pattern simpliﬁcation strategies, and (4) iterative strategies. For each strategy, we provide examples of the use and impact of this strategy on volume and/or variety. Examples are selected from 20 case studies gathered from either our own work, the literature, or based on email interviews with individuals who conducted the analyses and developers who observed analysts using the tools. Finally, we discuss how these strategies might be combined and report on the feedback from 10 senior event sequence analysts.},
	language = {en},
	number = {6},
	urldate = {2024-02-20},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Du, Fan and Shneiderman, Ben and Plaisant, Catherine and Malik, Sana and Perer, Adam},
	month = jun,
	year = {2017},
	pages = {1636--1649},
	file = {Du et al. - 2017 - Coping with Volume and Variety in Temporal Event S.pdf:/Users/vsivaram/Zotero/storage/B8SXIB4A/Du et al. - 2017 - Coping with Volume and Variety in Temporal Event S.pdf:application/pdf},
}

@misc{montambault_pixal_2022,
	title = {{PIXAL}: {Anomaly} {Reasoning} with {Visual} {Analytics}},
	shorttitle = {{PIXAL}},
	url = {http://arxiv.org/abs/2205.11004},
	abstract = {Anomaly detection remains an open challenge in many application areas. While there are a number of available machine learning algorithms for detecting anomalies, analysts are frequently asked to take additional steps in reasoning about the root cause of the anomalies and form actionable hypotheses that can be communicated to business stakeholders. Without the appropriate tools, this reasoning process is time-consuming, tedious, and potentially error-prone. In this paper we present PIXAL, a visual analytics system developed following an iterative design process with professional analysts responsible for anomaly detection. PIXAL is designed to fill gaps in existing tools commonly used by analysts to reason with and make sense of anomalies. PIXAL consists of three components: (1) an algorithm that finds patterns by aggregating multiple anomalous data points using first-order predicates, (2) a visualization tool that allows the analyst to build trust in the algorithmically-generated predicates by performing comparative and counterfactual analyses, and (3) a visualization tool that helps the analyst generate and validate hypotheses by exploring which features in the data most explain the anomalies. Finally, we present the results of a qualitative observational study with professional analysts. These results of the study indicate that PIXAL facilitates the anomaly reasoning process, allowing analysts to make sense of anomalies and generate hypotheses that are meaningful and actionable to business stakeholders.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Montambault, Brian and Brumar, Camelia D. and Behrisch, Michael and Chang, Remco},
	month = may,
	year = {2022},
	note = {arXiv:2205.11004 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
	file = {arXiv.org Snapshot:/Users/vsivaram/Zotero/storage/SM4BTRHD/2205.html:text/html;Full Text PDF:/Users/vsivaram/Zotero/storage/9Y2XR9XB/Montambault et al. - 2022 - PIXAL Anomaly Reasoning with Visual Analytics.pdf:application/pdf},
}

@inproceedings{guo_nugget_2011,
	address = {London, United Kingdom},
	title = {Nugget {Browser}: {Visual} {Subgroup} {Mining} and {Statistical} {Significance} {Discovery} in {Multivariate} {Datasets}},
	isbn = {978-1-4577-0868-8},
	shorttitle = {Nugget {Browser}},
	url = {http://ieeexplore.ieee.org/document/6004012/},
	doi = {10.1109/IV.2011.21},
	abstract = {Discovering interesting patterns in datasets is a very important data mining task. Subgroup patterns are local ﬁndings identifying the subgroups of a population with some unusual, unexpected, or deviating distribution of a target attribute. However, this pattern discovery task poses several compelling challenges. First, computational data mining techniques can generally only discover and extract pre-deﬁned patterns. Second, since the extracted patterns are typically multi-dimensional arbitrary-shaped regions, it is very difﬁcult to convey in an easily interpretable manner. Finally, in order to assist analysts in exploring their discoveries and understanding the relationships among patterns, as well as connections between patterns and the underlying data instances, an integrated visualization system is greatly needed. In this paper, we present a novel subgroup pattern extraction and visualization system, called the Nugget Browser, that takes advantage of both data mining methods and interactive visual exploration. The system accepts analysts’ mining queries interactively, converts the query results into an understandable form, builds visual representations, and supports navigation and exploration for further analyses.},
	language = {en},
	urldate = {2024-04-10},
	booktitle = {2011 15th {International} {Conference} on {Information} {Visualisation}},
	publisher = {IEEE},
	author = {Guo, Zhenyu and Ward, Matthew O. and Rundensteiner, Elke A.},
	month = jul,
	year = {2011},
	pages = {267--275},
	file = {Guo et al. - 2011 - Nugget Browser Visual Subgroup Mining and Statist.pdf:/Users/vsivaram/Zotero/storage/W3XBWA3C/Guo et al. - 2011 - Nugget Browser Visual Subgroup Mining and Statist.pdf:application/pdf},
}

@incollection{ferrandez_vicente_visualization_2022,
	address = {Cham},
	title = {Visualization {Methods} for {Exploratory} {Subgroup} {Discovery} on {Time} {Series} {Data}},
	volume = {13259},
	isbn = {978-3-031-06526-2 978-3-031-06527-9},
	url = {https://link.springer.com/10.1007/978-3-031-06527-9_4},
	abstract = {This paper presents visualization methods for exploratory subgroup discovery, focusing on numeric time series data. We provide four novel visualizations for the inspection and understanding of subgroups. These visualizations facilitate interpretation in order to get insights into the data and the respective subgroups, while also supporting statistical interpretation and assessment of the subgroups and their respective parameters. Furthermore, we illustrate the approach in the context of complex time series data – speciﬁcally on team interactions in the aﬀective computing context.},
	language = {en},
	urldate = {2024-04-10},
	booktitle = {Bio-inspired {Systems} and {Applications}: from {Robotics} to {Ambient} {Intelligence}},
	publisher = {Springer International Publishing},
	author = {Hudson, Dan and Wiltshire, Travis J. and Atzmueller, Martin},
	editor = {Ferrández Vicente, José Manuel and Álvarez-Sánchez, José Ramón and De La Paz López, Félix and Adeli, Hojjat},
	year = {2022},
	doi = {10.1007/978-3-031-06527-9_4},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {34--44},
	file = {Hudson et al. - 2022 - Visualization Methods for Exploratory Subgroup Dis.pdf:/Users/vsivaram/Zotero/storage/C2DG26MA/Hudson et al. - 2022 - Visualization Methods for Exploratory Subgroup Dis.pdf:application/pdf},
}

@inproceedings{slyman_vlslice_2023,
	address = {Paris, France},
	title = {{VLSlice}: {Interactive} {Vision}-and-{Language} {Slice} {Discovery}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350307184},
	shorttitle = {{VLSlice}},
	url = {https://ieeexplore.ieee.org/document/10376686/},
	doi = {10.1109/ICCV51070.2023.01403},
	abstract = {Recent work in vision-and-language demonstrates that large-scale pretraining can learn generalizable models that are efficiently transferable to downstream tasks. While this may improve dataset-scale aggregate metrics, analyzing performance around hand-crafted subgroups targeting specific bias dimensions reveals systemic undesirable behaviors. However, this subgroup analysis is frequently stalled by annotation efforts, which require extensive time and resources to collect the necessary data. Prior art attempts to automatically discover subgroups to circumvent these constraints but typically leverages model behavior on existing task-specific annotations and rapidly degrades on more complex inputs beyond “tabular” data, none of which study vision-and-language models. This paper presents VLSlice, an interactive system enabling user-guided discovery of coherent representation-level subgroups with consistent visiolinguistic behavior, denoted as vision-and-language slices, from unlabeled image sets. We show that VLSlice enables users to quickly generate diverse high-coherency slices in a user study (n=22) and release the tool publicly1.},
	language = {en},
	urldate = {2024-04-10},
	booktitle = {2023 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Slyman, Eric and Kahng, Minsuk and Lee, Stefan},
	month = oct,
	year = {2023},
	pages = {15245--15255},
	file = {Slyman et al. - 2023 - VLSlice Interactive Vision-and-Language Slice Dis.pdf:/Users/vsivaram/Zotero/storage/5SUQ9LM2/Slyman et al. - 2023 - VLSlice Interactive Vision-and-Language Slice Dis.pdf:application/pdf},
}

@article{hurley_interactive_2022,
	title = {Interactive {Slice} {Visualization} for {Exploring} {Machine} {Learning} {Models}},
	volume = {31},
	issn = {1061-8600, 1537-2715},
	url = {https://www.tandfonline.com/doi/full/10.1080/10618600.2021.1983439},
	doi = {10.1080/10618600.2021.1983439},
	language = {en},
	number = {1},
	urldate = {2024-04-10},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Hurley, Catherine B. and O’Connell, Mark and Domijan, Katarina},
	month = jan,
	year = {2022},
	pages = {1--13},
	file = {Full Text PDF:/Users/vsivaram/Zotero/storage/LHLEXAF9/Hurley et al. - 2022 - Interactive Slice Visualization for Exploring Mach.pdf:application/pdf},
}

@inproceedings{scholz_sampling-based_2005,
	address = {Chicago Illinois USA},
	title = {Sampling-based sequential subgroup mining},
	isbn = {978-1-59593-135-1},
	url = {https://dl.acm.org/doi/10.1145/1081870.1081902},
	doi = {10.1145/1081870.1081902},
	language = {en},
	urldate = {2024-04-11},
	booktitle = {Proceedings of the eleventh {ACM} {SIGKDD} international conference on {Knowledge} discovery in data mining},
	publisher = {ACM},
	author = {Scholz, Martin},
	month = aug,
	year = {2005},
	pages = {265--274},
	file = {Full Text PDF:/Users/vsivaram/Zotero/storage/2DXYUJSD/Scholz - 2005 - Sampling-based sequential subgroup mining.pdf:application/pdf},
}

@inproceedings{kearns_empirical_2019,
	address = {Atlanta GA USA},
	title = {An {Empirical} {Study} of {Rich} {Subgroup} {Fairness} for {Machine} {Learning}},
	isbn = {978-1-4503-6125-5},
	url = {https://dl.acm.org/doi/10.1145/3287560.3287592},
	doi = {10.1145/3287560.3287592},
	language = {en},
	urldate = {2024-04-11},
	booktitle = {Proceedings of the {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
	month = jan,
	year = {2019},
	pages = {100--109},
	file = {Full Text PDF:/Users/vsivaram/Zotero/storage/SPJN2C55/Kearns et al. - 2019 - An Empirical Study of Rich Subgroup Fairness for M.pdf:application/pdf},
}

@article{su_subgroup_2009,
	title = {Subgroup {Analysis} via {Recursive} {Partitioning}},
	issn = {1556-5068},
	url = {http://www.ssrn.com/abstract=1341380},
	doi = {10.2139/ssrn.1341380},
	abstract = {Subgroup analysis is an integral part of comparative analysis where assessing the treatment effect on a response is of central interest. Its goal is to determine the heterogeneity of the treatment effect across subpopulations. In this paper, we adapt the idea of recursive partitioning and introduce an interaction tree (IT) procedure to conduct subgroup analysis. The IT procedure automatically facilitates a number of objectively deﬁned subgroups, in some of which the treatment effect is found prominent while in others the treatment has a negligible or even negative effect. The standard CART (Breiman et al., 1984) methodology is inherited to construct the tree structure. Also, in order to extract factors that contribute to the heterogeneity of the treatment effect, variable importance measure is made available via random forests of the interaction trees. Both simulated experiments and analysis of census wage data are presented for illustration.},
	language = {en},
	urldate = {2024-04-11},
	journal = {SSRN Electronic Journal},
	author = {Su, Xiaogang and Tsai, Chih-Ling and Wang, Hansheng and Nickerson, David M. and Li, Bogong},
	year = {2009},
	file = {Su et al. - 2009 - Subgroup Analysis via Recursive Partitioning.pdf:/Users/vsivaram/Zotero/storage/GIU3NLH5/Su et al. - 2009 - Subgroup Analysis via Recursive Partitioning.pdf:application/pdf},
}

@inproceedings{nagpal_interpretable_2020,
	address = {Toronto Ontario Canada},
	title = {Interpretable subgroup discovery in treatment effect estimation with application to opioid prescribing guidelines},
	isbn = {978-1-4503-7046-2},
	url = {https://dl.acm.org/doi/10.1145/3368555.3384456},
	doi = {10.1145/3368555.3384456},
	language = {en},
	urldate = {2024-04-11},
	booktitle = {Proceedings of the {ACM} {Conference} on {Health}, {Inference}, and {Learning}},
	publisher = {ACM},
	author = {Nagpal, Chirag and Wei, Dennis and Vinzamuri, Bhanukiran and Shekhar, Monica and Berger, Sara E. and Das, Subhro and Varshney, Kush R.},
	month = apr,
	year = {2020},
	pages = {19--29},
	file = {Full Text PDF:/Users/vsivaram/Zotero/storage/TYR2MV2S/Nagpal et al. - 2020 - Interpretable subgroup discovery in treatment effe.pdf:application/pdf},
}

@inproceedings{kwon_rmexplorer_2022,
	address = {Oklahoma City, OK, USA},
	title = {{RMExplorer}: {A} {Visual} {Analytics} {Approach} to {Explore} the {Performance} and the {Fairness} of {Disease} {Risk} {Models} on {Population} {Subgroups}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66548-812-9},
	shorttitle = {{RMExplorer}},
	url = {https://ieeexplore.ieee.org/document/9973226/},
	doi = {10.1109/VIS54862.2022.00019},
	abstract = {Disease risk models can identify high-risk patients and help clinicians provide more personalized care. However, risk models developed on one dataset may not generalize across diverse subpopulations of patients in different datasets and may have unexpected performance. It is challenging for clinical researchers to inspect risk models across different subgroups without any tools. Therefore, we developed an interactive visualization system called RMExplorer (Risk Model Explorer) to enable interactive risk model assessment. Speciﬁcally, the system allows users to deﬁne subgroups of patients by selecting clinical, demographic, or other characteristics, to explore the performance and fairness of risk models on the subgroups, and to understand the feature contributions to risk scores. To demonstrate the usefulness of the tool, we conduct a case study, where we use RMExplorer to explore three atrial ﬁbrillation risk models by applying them to the UK Biobank dataset of 445,329 individuals. RMExplorer can help researchers to evaluate the performance and biases of risk models on subpopulations of interest in their data.},
	language = {en},
	urldate = {2024-04-11},
	booktitle = {2022 {IEEE} {Visualization} and {Visual} {Analytics} ({VIS})},
	publisher = {IEEE},
	author = {Kwon, Bum Chul and Kartoun, Uri and Khurshid, Shaan and Yurochkin, Mikhail and Maity, Subha and Brockman, Deanna G and Khera, Amit V and Ellinor, Patrick T and Lubitz, Steven A and Ng, Kenney},
	month = oct,
	year = {2022},
	pages = {50--54},
	file = {Kwon et al. - 2022 - RMExplorer A Visual Analytics Approach to Explore.pdf:/Users/vsivaram/Zotero/storage/4R234ZRP/Kwon et al. - 2022 - RMExplorer A Visual Analytics Approach to Explore.pdf:application/pdf},
}

@article{dingen_regressionexplorer_2019,
	title = {{RegressionExplorer}: {Interactive} {Exploration} of {Logistic} {Regression} {Models} with {Subgroup} {Analysis}},
	volume = {25},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1077-2626, 1941-0506, 2160-9306},
	shorttitle = {{RegressionExplorer}},
	url = {https://ieeexplore.ieee.org/document/8464305/},
	doi = {10.1109/TVCG.2018.2865043},
	abstract = {We present RegressionExplorer, a Visual Analytics tool for the interactive exploration of logistic regression models. Our application domain is Clinical Biostatistics, where models are derived from patient data with the aim to obtain clinically meaningful insights and consequences. Development and interpretation of a proper model requires domain expertise and insight into model characteristics. Because of time constraints, often a limited number of candidate models is evaluated. RegressionExplorer enables experts to quickly generate, evaluate, and compare many different models, taking the workﬂow for model development as starting point. Global patterns in parameter values of candidate models can be explored effectively. In addition, experts are enabled to compare candidate models across multiple subpopulations. The insights obtained can be used to formulate new hypotheses or to steer model development. The effectiveness of the tool is demonstrated for two uses cases: prediction of a cardiac conduction disorder in patients after receiving a heart valve implant and prediction of hypernatremia in critically ill patients.},
	language = {en},
	number = {1},
	urldate = {2024-04-11},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Dingen, Dennis and Van'T Veer, Marcel and Houthuizen, Patrick and Mestrom, Eveline H. J. and Korsten, Erik H.H.M. and Bouwman, Arthur R.A. and Van Wijk, Jarke},
	month = jan,
	year = {2019},
	pages = {246--255},
	file = {Dingen et al. - 2019 - RegressionExplorer Interactive Exploration of Log.pdf:/Users/vsivaram/Zotero/storage/GVHS9I3D/Dingen et al. - 2019 - RegressionExplorer Interactive Exploration of Log.pdf:application/pdf},
}

@misc{xuan_attributionscanner_2024,
	title = {{AttributionScanner}: {A} {Visual} {Analytics} {System} for {Metadata}-{Free} {Data}-{Slicing} {Based} {Model} {Validation}},
	shorttitle = {{AttributionScanner}},
	url = {http://arxiv.org/abs/2401.06462},
	abstract = {Data slice-finding is an emerging technique for evaluating machine learning models. It works by identifying subgroups within a specified dataset that exhibit poor performance, often defined by distinct feature sets or meta-information. However, in the context of unstructured image data, data slice-finding poses two notable challenges: it requires additional metadata -- a laborious and costly requirement, and also demands non-trivial efforts for interpreting the root causes of the underperformance within data slices. To address these challenges, we introduce AttributionScanner, an innovative human-in-the-loop Visual Analytics (VA) system, designed for data-slicing-based machine learning (ML) model validation. Our approach excels in identifying interpretable data slices, employing explainable features extracted through the lens of Explainable AI (XAI) techniques, and removing the necessity for additional metadata of textual annotations or cross-model embeddings. AttributionScanner demonstrates proficiency in pinpointing critical model issues, including spurious correlations and mislabeled data. Our novel VA interface visually summarizes data slices, enabling users to gather insights into model behavior patterns effortlessly. Furthermore, our framework closes the ML Development Cycle by empowering domain experts to address model issues by using a cutting-edge neural network regularization technique. The efficacy of AttributionScanner is underscored through two prototype use cases, elucidating its substantial effectiveness in model validation for vision-centric tasks. Our approach paves the way for ML researchers and practitioners to drive interpretable model validation in a data-efficient way, ultimately leading to more reliable and accurate models.},
	urldate = {2024-04-11},
	publisher = {arXiv},
	author = {Xuan, Xiwei and Ono, Jorge Piazentin and Gou, Liang and Ma, Kwan-Liu and Ren, Liu},
	month = jan,
	year = {2024},
	note = {arXiv:2401.06462 [cs]
version: 1},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 11 pages, 11 figures},
	file = {arXiv.org Snapshot:/Users/vsivaram/Zotero/storage/MX669RG8/2401.html:text/html;Full Text PDF:/Users/vsivaram/Zotero/storage/ICN8HDAT/Xuan et al. - 2024 - AttributionScanner A Visual Analytics System for .pdf:application/pdf},
}

@inproceedings{ahn_escape_2023,
	address = {Hamburg Germany},
	title = {{ESCAPE}: {Countering} {Systematic} {Errors} from {Machine}’s {Blind} {Spots} via {Interactive} {Visual} {Analysis}},
	isbn = {978-1-4503-9421-5},
	shorttitle = {{ESCAPE}},
	url = {https://dl.acm.org/doi/10.1145/3544548.3581373},
	doi = {10.1145/3544548.3581373},
	language = {en},
	urldate = {2024-04-11},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Ahn, Yongsu and Lin, Yu-Ru and Xu, Panpan and Dai, Zeng},
	month = apr,
	year = {2023},
	pages = {1--16},
	file = {Full Text PDF:/Users/vsivaram/Zotero/storage/EI5X9BYP/Ahn et al. - 2023 - ESCAPE Countering Systematic Errors from Machine’.pdf:application/pdf},
}

@inproceedings{kerrigan_slicelens_2023,
	address = {Seattle WA USA},
	title = {{SliceLens}: {Guided} {Exploration} of {Machine} {Learning} {Datasets}},
	isbn = {9798400702167},
	shorttitle = {{SliceLens}},
	url = {https://dl.acm.org/doi/10.1145/3597465.3605217},
	doi = {10.1145/3597465.3605217},
	language = {en},
	urldate = {2024-04-11},
	booktitle = {Proceedings of the {Workshop} on {Human}-{In}-the-{Loop} {Data} {Analytics}},
	publisher = {ACM},
	author = {Kerrigan, Daniel and Bertini, Enrico},
	month = jun,
	year = {2023},
	pages = {1--7},
	file = {Full Text PDF:/Users/vsivaram/Zotero/storage/9ABPVMMP/Kerrigan and Bertini - 2023 - SliceLens Guided Exploration of Machine Learning .pdf:application/pdf},
}

@article{xenopoulos_calibrate_2023,
	title = {Calibrate: {Interactive} {Analysis} of {Probabilistic} {Model} {Output}},
	volume = {29},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1077-2626, 1941-0506, 2160-9306},
	shorttitle = {Calibrate},
	url = {https://ieeexplore.ieee.org/document/9904444/},
	doi = {10.1109/TVCG.2022.3209489},
	abstract = {Analyzing classification model performance is a crucial task for machine learning practitioners. While practitioners often use count-based metrics derived from confusion matrices, like accuracy, many applications, such as weather prediction, sports betting, or patient risk prediction, rely on a classifier’s predicted probabilities rather than predicted labels. In these instances, practitioners are concerned with producing a calibrated model, that is, one which outputs probabilities that reflect those of the true distribution. Model calibration is often analyzed visually, through static reliability diagrams, however, the traditional calibration visualization may suffer from a variety of drawbacks due to the strong aggregations it necessitates. Furthermore, count-based approaches are unable to sufficiently analyze model calibration. We present Calibrate, an interactive reliability diagram that addresses the aforementioned issues. Calibrate constructs a reliability diagram that is resistant to drawbacks in traditional approaches, and allows for interactive subgroup analysis and instance-level inspection. We demonstrate the utility of Calibrate through use cases on both real-world and synthetic data. We further validate Calibrate by presenting the results of a think-aloud experiment with data scientists who routinely analyze model calibration.},
	language = {en},
	number = {1},
	urldate = {2024-04-12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Xenopoulos, Peter and Rulff, Joao and Nonato, Luis Gustavo and Barr, Brian and Silva, Claudio},
	month = jan,
	year = {2023},
	pages = {853--863},
	file = {Xenopoulos et al. - 2023 - Calibrate Interactive Analysis of Probabilistic M.pdf:/Users/vsivaram/Zotero/storage/2H72AK9Z/Xenopoulos et al. - 2023 - Calibrate Interactive Analysis of Probabilistic M.pdf:application/pdf},
}

@article{wang_causal_2022,
	title = {Causal {Rule} {Sets} for {Identifying} {Subgroups} with {Enhanced} {Treatment} {Effects}},
	volume = {34},
	issn = {1091-9856, 1526-5528},
	url = {https://pubsonline.informs.org/doi/10.1287/ijoc.2021.1143},
	doi = {10.1287/ijoc.2021.1143},
	abstract = {A key question in causal inference analyses is how to find subgroups with elevated treatment effects. This paper takes a machine learning approach and introduces a generative model, causal rule sets (CRS), for interpretable subgroup discovery. A CRS model uses a small set of short decision rules to capture a subgroup in which the average treatment effect is elevated. We present a Bayesian framework for learning a causal rule set. The Bayesian model consists of a prior that favors simple models for better interpretability as well as avoiding overfitting and a Bayesian logistic regression that captures the likelihood of data, characterizing the relation between outcomes, attributes, and subgroup membership. The Bayesian model has tunable parameters that can characterize subgroups with various sizes, providing users with more flexible choices of models from the treatment-efficient frontier. We find maximum a posteriori models using iterative discrete Monte Carlo steps in the joint solution space of rules sets and parameters. To improve search efficiency, we provide theoretically grounded heuristics and bounding strategies to prune and confine the search space. Experiments show that the search algorithm can efficiently recover true underlying subgroups. We apply CRS on public and real-world data sets from domains in which interpretability is indispensable. We compare CRS with state-of-the-art rule-based subgroup discovery models. Results show that CRS achieves consistently competitive performance on data sets from various domains, represented by high treatment-efficient frontiers.
            Summary of Contribution: This paper is motivated by the large heterogeneity of treatment effect in many applications and the need to accurately locate subgroups for enhanced treatment effect. Existing methods either rely on prior hypotheses to discover subgroups or greedy methods, such as tree-based recursive partitioning. Our method adopts a machine learning approach to find an optimal subgroup learned with a carefully global objective. Our model is more flexible in capturing subgroups by using a set of short decision rules compared with tree-based baselines. We evaluate our model using a novel metric, treatment-efficient frontier, that characterizes the trade-off between the subgroup size and achievable treatment effect, and our model demonstrates better performance than baseline models.},
	language = {en},
	number = {3},
	urldate = {2024-06-03},
	journal = {INFORMS Journal on Computing},
	author = {Wang, Tong and Rudin, Cynthia},
	month = may,
	year = {2022},
	pages = {1626--1643},
	file = {Full Text PDF:/Users/vsivaram/Zotero/storage/WM3FA6VH/Wang and Rudin - 2022 - Causal Rule Sets for Identifying Subgroups with En.pdf:application/pdf},
}

@article{liu_automated_nodate,
	title = {Automated {Detection} of {Causal} {Inference} {Opportuni}- ties: {Regression} {Discontinuity} {Subgroup} {Discovery}},
	abstract = {The gold standard for the identification of causal effects are randomized controlled trials (RCT), but RCTs may not always be feasible to conduct. When treatments depend on a threshold however, such as the blood sugar threshold for diabetes diagnosis, we can still sometimes estimate causal effects with regression discontinuities (RDs). RDs are valid when units just above and below the threshold have the same distribution of covariates and thus no confounding in the presence of noise, establishing an as-if randomization. In practice however, implementing RD studies can be difficult as identifying treatment thresholds require considerable domain expertise – furthermore, the thresholds may differ across subgroups (e.g., the blood sugar threshold for diabetes may differ across demographics), and ignoring these differences can lower statistical power. Finding the thresholds and to whom they apply is an important problem currently solved manually by domain experts, and data-driven approaches are needed when domain expertise is not sufficient. Here, we introduce Regression Discontinuity SubGroup Discovery (RDSGD), a machinelearning method that identifies statistically powerful and interpretable subgroups for RD thresholds. Using a medical claims dataset with over 60 million patients, we apply RDSGD to multiple clinical contexts and identify subgroups with increased compliance to treatment assignment thresholds. As treatment thresholds matter for many diseases and policy decisions, RDSGD can be a powerful tool for discovering new avenues for causal estimation.},
	language = {en},
	author = {Liu, Tony and Lawlor, Patrick and Ungar, Lyle and Kording, Konrad and Ladhania, Rahul},
	file = {Liu et al. - Automated Detection of Causal Inference Opportuni-.pdf:/Users/vsivaram/Zotero/storage/RN65RJXA/pdf.pdf:application/pdf},
}

@article{allen_interpretable_2024,
	title = {Interpretable {Machine} {Learning} for {Discovery}: {Statistical} {Challenges} and {Opportunities}},
	volume = {11},
	issn = {2326-8298, 2326-831X},
	shorttitle = {Interpretable {Machine} {Learning} for {Discovery}},
	url = {https://www.annualreviews.org/content/journals/10.1146/annurev-statistics-040120-030919},
	doi = {10.1146/annurev-statistics-040120-030919},
	abstract = {New technologies have led to vast troves of large and complex data sets across many scientific domains and industries. People routinely use machine learning techniques not only to process, visualize, and make predictions from these big data, but also to make data-driven discoveries. These discoveries are often made using interpretable machine learning, or machine learning models and techniques that yield human-understandable insights. In this article, we discuss and review the field of interpretable machine learning, focusing especially on the techniques, as they are often employed to generate new knowledge or make discoveries from large data sets. We outline the types of discoveries that can be made using interpretable machine learning in both supervised and unsupervised settings. Additionally, we focus on the grand challenge of how to validate these discoveries in a data-driven manner, which promotes trust in machine learning systems and reproducibility in science. We discuss validation both from a practical perspective, reviewing approaches based on data-splitting and stability, as well as from a theoretical perspective, reviewing statistical results on model selection consistency and uncertainty quantification via statistical inference. Finally, we conclude byhighlighting open challenges in using interpretable machine learning techniques to make discoveries, including gaps between theory and practice for validating data-driven discoveries.},
	language = {en},
	number = {1},
	urldate = {2024-06-03},
	journal = {Annual Review of Statistics and Its Application},
	author = {Allen, Genevera I. and Gan, Luqin and Zheng, Lili},
	month = apr,
	year = {2024},
	pages = {97--121},
	file = {Allen et al. - 2024 - Interpretable Machine Learning for Discovery Stat.pdf:/Users/vsivaram/Zotero/storage/MZLLMEUE/Allen et al. - 2024 - Interpretable Machine Learning for Discovery Stat.pdf:application/pdf},
}

@misc{romero_explaining_2024,
	title = {Explaining {Agent}-{Based} {Modeling} {Outputs} {Using} {Subgroup} {Discovery}: {A} {Case} {Study} in {Marketing}},
	shorttitle = {Explaining {Agent}-{Based} {Modeling} {Outputs} {Using} {Subgroup} {Discovery}},
	url = {https://www.ssrn.com/abstract=4727322},
	doi = {10.2139/ssrn.4727322},
	abstract = {Agent-based modeling (ABM) is a popular tool for simulating complex systems. During the simulation, ABM generates valuable information about the agents’ roadmap, but the large volume of generated data is difficult to analyze and understand. Improving the understanding of ABM outputs is a key challenge, which can also lead to increase the model understanding. However, most studies analyzing model outputs have mainly focused on model validation or identifying relationships between input and output, with limited emphasis on extracting insights about agent behavior from simulated data. We propose the application of Explainable Artificial Intelligence techniques, specifically Subgroup Discovery (SD), to improve model transparency and interpretability. We provide a methodology for applying SD to ABM for extracting useful knowledge of agents’ behavior with respect to a variable of interest. We compare the performance of different SD algorithms and show the effectiveness of our methodology through a case study in the marketing area. We analyze the obtained set of rules from the point of view of a marketing expert. The results show that our methodology can provide a deeper understanding of consumer behavior and market dynamics, providing potential areas of opportunity for marketers.},
	language = {en},
	urldate = {2024-06-03},
	author = {Romero, Elena and Carmona Del Jesús, Cristóbal José and Cordon, Oscar and Del Jesus, María José and Damas, Sergio and Chica, Manuel},
	year = {2024},
	file = {Romero et al. - 2024 - Explaining Agent-Based Modeling Outputs Using Subg.pdf:/Users/vsivaram/Zotero/storage/HWSF2WXN/Romero et al. - 2024 - Explaining Agent-Based Modeling Outputs Using Subg.pdf:application/pdf},
}

@misc{bargagli-stoffi_causal_2024,
	title = {Causal {Rule} {Ensemble}: {Interpretable} {Discovery} and {Inference} of {Heterogeneous} {Treatment} {Effects}},
	shorttitle = {Causal {Rule} {Ensemble}},
	url = {http://arxiv.org/abs/2009.09036},
	abstract = {In health and social sciences, it is critically important to identify subgroups of the study population where there is notable heterogeneity of treatment effects (HTE) with respect to the population average. Decision trees have been proposed and commonly adopted for the data-driven discovery of HTE due to their high level of interpretability. However, single-tree discovery of HTE can be unstable and oversimplified. This paper introduces the Causal Rule Ensemble (CRE), a new method for HTE discovery and estimation using an ensemble-of-trees approach. CRE offers several key features, including 1) an interpretable representation of the HTE; 2) the ability to explore complex heterogeneity patterns; and 3) high stability in subgroups discovery. The discovered subgroups are defined in terms of interpretable decision rules. Estimation of subgroup-specific causal effects is performed via a two-stage approach, for which we provide theoretical guarantees. Through simulations, we show that the CRE method is highly competitive compared to state-of-the-art techniques. Finally, we apply CRE to discover the heterogeneous health effects of exposure to air pollution on mortality for 35.3 million Medicare beneficiaries across the contiguous U.S.},
	language = {en},
	urldate = {2024-06-03},
	publisher = {arXiv},
	author = {Bargagli-Stoffi, Falco J. and Cadei, Riccardo and Lee, Kwonsang and Dominici, Francesca},
	month = may,
	year = {2024},
	note = {arXiv:2009.09036 [stat]},
	keywords = {Statistics - Machine Learning, Statistics - Methodology},
	file = {Bargagli-Stoffi et al. - 2024 - Causal Rule Ensemble Interpretable Discovery and .pdf:/Users/vsivaram/Zotero/storage/JAASSQJS/Bargagli-Stoffi et al. - 2024 - Causal Rule Ensemble Interpretable Discovery and .pdf:application/pdf},
}

@article{liu_exploratory_2020,
	title = {Exploratory {Data} {Mining} for {Subgroup} {Cohort} {Discoveries} and {Prioritization}},
	volume = {24},
	copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {2168-2194, 2168-2208},
	url = {https://ieeexplore.ieee.org/document/8825474/},
	doi = {10.1109/JBHI.2019.2939149},
	abstract = {Finding small homogeneous subgroup cohorts in large heterogeneous populations is a critical process for hypothesis development in biomedical research. Concurrent computational approaches are still lacking in robust answers to the question “what hypotheses are likely to be novel and to produce clinically relevant results with well thought-out study designs?” We have developed a novel subgroup discovery method which employs a deep exploratory mining process to slice and dice thousands of potential subpopulations and prioritize potential cohorts based on their explainable contrast patterns and which may provide interventionable insights. We conducted computational experiments on both synthesized data and a clinical autism data set to assess performance quantitatively for coverage of pre-deﬁned cohorts and qualitatively for novel knowledge discovery, respectively. We also conducted a scaling analysis using a distributed computing environment to suggest computational resource needs for when the subpopulation number increases. This work will provide a robust data-driven framework to automatically tailor potential interventions for precision health.},
	language = {en},
	number = {5},
	urldate = {2024-06-12},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Liu, Danlu and Baskett, William and Beversdorf, David and Shyu, Chi-Ren},
	month = may,
	year = {2020},
	pages = {1456--1468},
	file = {Liu et al. - 2020 - Exploratory Data Mining for Subgroup Cohort Discov.pdf:/Users/vsivaram/Zotero/storage/FWM7TPEI/Liu et al. - 2020 - Exploratory Data Mining for Subgroup Cohort Discov.pdf:application/pdf},
}

@inproceedings{harrison_jupyterlab_2024,
	address = {Honolulu HI USA},
	title = {{JupyterLab} in {Retrograde}: {Contextual} {Notifications} {That} {Highlight} {Fairness} and {Bias} {Issues} for {Data} {Scientists}},
	isbn = {9798400703300},
	shorttitle = {{JupyterLab} in {Retrograde}},
	url = {https://dl.acm.org/doi/10.1145/3613904.3642755},
	doi = {10.1145/3613904.3642755},
	language = {en},
	urldate = {2024-08-07},
	booktitle = {Proceedings of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Harrison, Galen and Bryson, Kevin and Bamba, Ahmad Emmanuel Balla and Dovichi, Luca and Binion, Aleksander Herrmann and Borem, Arthur and Ur, Blase},
	month = may,
	year = {2024},
	pages = {1--19},
	file = {Full Text PDF:/Users/vsivaram/Zotero/storage/2JWDIWUU/Harrison et al. - 2024 - JupyterLab in Retrograde Contextual Notifications.pdf:application/pdf},
}

@incollection{yada_interesting_2014,
	address = {Berlin, Heidelberg},
	title = {Interesting {Subset} {Discovery} and {Its} {Application} on {Service} {Processes}},
	volume = {3},
	isbn = {978-3-642-45251-2 978-3-642-45252-9},
	url = {https://link.springer.com/10.1007/978-3-642-45252-9_14},
	abstract = {Various real-life datasets can be viewed as a set of records consisting of attributes explaining the records and set of measures evaluating the records. We address the problem of automatically discovering interesting subsets from such a dataset, such that the discovered interesting subsets have signiﬁcantly different characteristics of performance than the rest of the dataset. We present an algorithm to discover such interesting subsets. The proposed algorithm uses a generic domainindependent deﬁnition of interestingness and uses various heuristics to intelligently prune the search space in order to build a solution scalable to large size datasets. We present application of the interesting subset discovery algorithm on four real-world case-studies and demonstrates the effectiveness of the interesting subset discovery algorithm in extracting insights in order to identify problem areas and provide improvement recommendations to wide variety of systems.},
	language = {en},
	urldate = {2024-08-19},
	booktitle = {Data {Mining} for {Service}},
	publisher = {Springer Berlin Heidelberg},
	author = {Natu, Maitreya and Palshikar, Girish Keshav},
	editor = {Yada, Katsutoshi},
	year = {2014},
	doi = {10.1007/978-3-642-45252-9_14},
	note = {Series Title: Studies in Big Data},
	pages = {245--269},
	file = {Natu and Palshikar - 2014 - Interesting Subset Discovery and Its Application o.pdf:/Users/vsivaram/Zotero/storage/NJDQ8Q9M/Natu and Palshikar - 2014 - Interesting Subset Discovery and Its Application o.pdf:application/pdf},
}

@article{gamberger_expert-guided_2002,
	title = {Expert-{Guided} {Subgroup} {Discovery}: {Methodology} and {Application}},
	volume = {17},
	issn = {1076-9757},
	shorttitle = {Expert-{Guided} {Subgroup} {Discovery}},
	url = {http://arxiv.org/abs/1106.4576},
	doi = {10.1613/jair.1089},
	abstract = {This paper presents an approach to expert-guided subgroup discovery. The main step of the subgroup discovery process, the induction of subgroup descriptions, is performed by a heuristic beam search algorithm, using a novel parametrized definition of rule quality which is analyzed in detail. The other important steps of the proposed subgroup discovery process are the detection of statistically significant properties of selected subgroups and subgroup visualization: statistically significant properties are used to enrich the descriptions of induced subgroups, while the visualization shows subgroup properties in the form of distributions of the numbers of examples in the subgroups. The approach is illustrated by the results obtained for a medical problem of early detection of patient risk groups.},
	urldate = {2024-08-19},
	journal = {Journal of Artificial Intelligence Research},
	author = {Gamberger, D. and Lavrac, N.},
	month = dec,
	year = {2002},
	note = {arXiv:1106.4576 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	pages = {501--527},
	file = {arXiv Fulltext PDF:/Users/vsivaram/Zotero/storage/NK3I9L7L/Gamberger and Lavrac - 2002 - Expert-Guided Subgroup Discovery Methodology and .pdf:application/pdf;arXiv.org Snapshot:/Users/vsivaram/Zotero/storage/P4RQKR25/1106.html:text/html},
}

@article{lavrac_decision_2004,
	title = {Decision {Support} {Through} {Subgroup} {Discovery}: {Three} {Case} {Studies} and the {Lessons} {Learned}},
	volume = {57},
	issn = {0885-6125},
	shorttitle = {Decision {Support} {Through} {Subgroup} {Discovery}},
	url = {http://link.springer.com/10.1023/B:MACH.0000035474.48771.cd},
	doi = {10.1023/B:MACH.0000035474.48771.cd},
	abstract = {This paper presents ways to use subgroup discovery to generate actionable knowledge for decision support. Actionable knowledge is explicit symbolic knowledge, typically presented in the form of rules, that allows the decision maker to recognize some important relations and to perform an appropriate action, such as targeting a direct marketing campaign, or planning a population screening campaign aimed at detecting individuals with high disease risk. Different subgroup discovery approaches are outlined, and their advantages over using standard classiﬁcation rule learning are discussed. Three case studies, a medical and two marketing ones, are used to present the lessons learned in solving problems requiring actionable knowledge generation for decision support.},
	language = {en},
	number = {1/2},
	urldate = {2024-08-19},
	journal = {Machine Learning},
	author = {Lavrač, Nada and Cestnik, Bojan and Gamberger, Dragan and Flach, Peter},
	month = oct,
	year = {2004},
	pages = {115--143},
	file = {Lavrač et al. - 2004 - Decision Support Through Subgroup Discovery Three.pdf:/Users/vsivaram/Zotero/storage/X3IESJWQ/Lavrač et al. - 2004 - Decision Support Through Subgroup Discovery Three.pdf:application/pdf},
}

@article{gamberger_active_2003,
	title = {Active subgroup mining: a case study in coronary heart disease risk group detection},
	volume = {28},
	issn = {0933-3657},
	shorttitle = {Active subgroup mining},
	url = {https://www.sciencedirect.com/science/article/pii/S0933365703000344},
	doi = {10.1016/S0933-3657(03)00034-4},
	abstract = {This paper presents an approach to active mining of patient records aimed at discovering patient groups at high risk for coronary heart disease (CHD). The approach proposes active expert involvement in the following steps of the knowledge discovery process: data gathering, cleaning and transformation, subgroup discovery, statistical characterization of induced subgroups, their interpretation, and the evaluation of results. As in the discovery and characterization of risk subgroups, the main risk factors are made explicit, the proposed methodology has high potential for patient screening and early detection of patient groups at risk for CHD.},
	number = {1},
	urldate = {2024-08-19},
	journal = {Artificial Intelligence in Medicine},
	author = {Gamberger, Dragan and Lavrač, Nada and Krstačić, Goran},
	month = may,
	year = {2003},
	keywords = {Active mining, Coronary heart disease, Machine learning, Non-invasive cardiovascular tests, Risk group detection, Subgroup discovery},
	pages = {27--57},
	file = {ScienceDirect Snapshot:/Users/vsivaram/Zotero/storage/DRPB9L68/S0933365703000344.html:text/html},
}


@incollection{green_subgroup_2021,
	address = {Cambridge},
	title = {Subgroup {Analysis}: {Pitfalls}, {Promise}, and {Honesty}},
	isbn = {978-1-108-47850-2},
	shorttitle = {Subgroup {Analysis}},
	url = {https://www.cambridge.org/core/books/advances-in-experimental-political-science/subgroup-analysis-pitfalls-promise-and-honesty/0FA22B419390CA59E5F5D8C1BCA4F210},
	abstract = {Experiments often focus on recovering an average effect of a treatment on an outcome. A subgroup analysis involves identifying subgroups of observations for which the treatment is particularly efficacious or deleterious. Since these subgroups are not preregistered but instead discovered from the data, significant inferential issues emerge. We discuss methods for conduct honest inference on subgroups, meaning generating valid p-values and confidence intervals which account for the fact that the subgroups were not specified a priori. Central to this approach is the split-sample strategy, where half the data is used to identify effects and the other half to test them. After an intuitive and formal discussion of these issues, we provide simulation evidence and two examples illustrating these concepts in practice.},
	urldate = {2024-03-13},
	booktitle = {Advances in {Experimental} {Political} {Science}},
	publisher = {Cambridge University Press},
	author = {Ratkovic, Marc},
	editor = {Green, Donald P. and Druckman, James N.},
	year = {2021},
	doi = {10.1017/9781108777919.020},
	keywords = {machine learning, causal inference, heterogeneous treatment effects, split sample, subgroup analysis},
	pages = {271--288},
	file = {Full Text PDF:/Users/vsivaram/Zotero/storage/3WHQYJ75/Ratkovic - 2021 - Subgroup Analysis Pitfalls, Promise, and Honesty.pdf:application/pdf;Snapshot:/Users/vsivaram/Zotero/storage/23SRS3A4/0FA22B419390CA59E5F5D8C1BCA4F210.html:text/html},
}


@article{wang_statistics_2007,
	title = {Statistics in {Medicine} — {Reporting} of {Subgroup} {Analyses} in {Clinical} {Trials}},
	volume = {357},
	issn = {0028-4793},
	url = {https://www.nejm.org/doi/full/10.1056/NEJMsr077003},
	doi = {10.1056/NEJMsr077003},
	abstract = {The analysis of subgroups is often used as a way to glean additional information from data sets. The strengths and weaknesses of this approach and new Journal policies concerning the reporting of subgroup analyses are discussed in this article. The strengths and weaknesses of subgroup analyses and new Journal policies concerning the reporting of this approach are discussed in this article. Medical research relies on clinical trials to assess therapeutic benefits. Because of the effort and cost involved in these studies, investigators frequently use analyses of subgroups of study participants to extract as much information as possible. Such analyses, which assess the heterogeneity of treatment effects in subgroups of patients, may provide useful information for the care of patients and for future research. However, subgroup analyses also introduce analytic challenges and can lead to overstated and misleading results.1–7 This report outlines the challenges associated with conducting and reporting subgroup analyses, and it sets forth guidelines for their use in the . . .},
	number = {21},
	urldate = {2024-08-30},
	journal = {New England Journal of Medicine},
	author = {Wang, Rui and Lagakos, Stephen W. and Ware, James H. and Hunter, David J. and Drazen, Jeffrey M.},
	month = nov,
	year = {2007},
	note = {Publisher: Massachusetts Medical Society
\_eprint: https://www.nejm.org/doi/pdf/10.1056/NEJMsr077003},
	pages = {2189--2194},
	file = {Full Text PDF:/Users/vsivaram/Zotero/storage/3A5HQEEI/Wang et al. - 2007 - Statistics in Medicine — Reporting of Subgroup Ana.pdf:application/pdf},
}

@article{helal_subgroup_2016,
	title = {Subgroup {Discovery} {Algorithms}: {A} {Survey} and {Empirical} {Evaluation}},
	volume = {31},
	issn = {1860-4749},
	shorttitle = {Subgroup {Discovery} {Algorithms}},
	url = {https://doi.org/10.1007/s11390-016-1647-1},
	doi = {10.1007/s11390-016-1647-1},
	abstract = {Subgroup discovery is a data mining technique that discovers interesting associations among different variables with respect to a property of interest. Existing subgroup discovery methods employ different strategies for searching, pruning and ranking subgroups. It is very crucial to learn which features of a subgroup discovery algorithm should be considered for generating quality subgroups. In this regard, a number of reviews have been conducted on subgroup discovery. Although they provide a broad overview on some popular subgroup discovery methods, they employ few datasets and measures for subgroup evaluation. In the light of the existing measures, the subgroups cannot be appraised from all perspectives. Our work performs an extensive analysis on some popular subgroup discovery methods by using a wide range of datasets and by defining new measures for subgroup evaluation. The analysis result will help with understanding the major subgroup discovery methods, uncovering the gaps for further improvement and selecting the suitable category of algorithms for specific application domains.},
	language = {en},
	number = {3},
	urldate = {2024-08-30},
	journal = {Journal of Computer Science and Technology},
	author = {Helal, Sumyea},
	month = may,
	year = {2016},
	keywords = {Artificial Intelligence, evaluation, measure, pruning, searching, subgroup discovery},
	pages = {561--576},
	file = {Full Text PDF:/Users/vsivaram/Zotero/storage/9V3UYFZE/Helal - 2016 - Subgroup Discovery Algorithms A Survey and Empiri.pdf:application/pdf},
}


@inproceedings{cabrera_zeno_2023,
	address = {Hamburg Germany},
	title = {Zeno: {An} {Interactive} {Framework} for {Behavioral} {Evaluation} of {Machine} {Learning}},
	isbn = {978-1-4503-9421-5},
	shorttitle = {Zeno},
	url = {https://dl.acm.org/doi/10.1145/3544548.3581268},
	doi = {10.1145/3544548.3581268},
	language = {en},
	urldate = {2024-03-02},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Cabrera, Ángel Alexander and Fu, Erica and Bertucci, Donald and Holstein, Kenneth and Talwalkar, Ameet and Hong, Jason I. and Perer, Adam},
	month = apr,
	year = {2023},
	pages = {1--14},
	file = {Full Text PDF:/Users/vsivaram/Zotero/storage/N7G43IJU/Cabrera et al. - 2023 - Zeno An Interactive Framework for Behavioral Eval.pdf:application/pdf},
}


@article{marchionini_exploratory_2006,
	title = {Exploratory search: from finding to understanding},
	volume = {49},
	issn = {0001-0782, 1557-7317},
	shorttitle = {Exploratory search},
	url = {https://dl.acm.org/doi/10.1145/1121949.1121979},
	doi = {10.1145/1121949.1121979},
	abstract = {Research tools critical for exploratory search success involve the creation of new interfaces that move the process beyond predictable fact retrieval.},
	language = {en},
	number = {4},
	urldate = {2024-08-31},
	journal = {Communications of the ACM},
	author = {Marchionini, Gary},
	month = apr,
	year = {2006},
	pages = {41--46},
	file = {Full Text PDF:/Users/vsivaram/Zotero/storage/E7B36YVD/Marchionini - 2006 - Exploratory search from finding to understanding.pdf:application/pdf},
}

@book{white_exploratory_2009,
	address = {Cham},
	series = {Synthesis {Lectures} on {Information} {Concepts}, {Retrieval}, and {Services}},
	title = {Exploratory {Search}: {Beyond} the {Query}—{Response} {Paradigm}},
	copyright = {https://www.springer.com/tdm},
	isbn = {978-3-031-01132-0 978-3-031-02260-9},
	shorttitle = {Exploratory {Search}},
	url = {https://link.springer.com/10.1007/978-3-031-02260-9},
	language = {en},
	urldate = {2024-08-31},
	publisher = {Springer International Publishing},
	author = {White, Ryen W. and Roth, Resa A.},
	year = {2009},
	doi = {10.1007/978-3-031-02260-9},
	file = {Full Text PDF:/Users/vsivaram/Zotero/storage/NNS8DRX3/White and Roth - 2009 - Exploratory Search Beyond the Query—Response Para.pdf:application/pdf},
}

@article{wongsuphasawat_voyager_2016,
	title = {Voyager: {Exploratory} {Analysis} via {Faceted} {Browsing} of {Visualization} {Recommendations}},
	volume = {22},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1077-2626},
	shorttitle = {Voyager},
	url = {http://ieeexplore.ieee.org/document/7192728/},
	doi = {10.1109/TVCG.2015.2467191},
	abstract = {General visualization tools typically require manual speciﬁcation of views: analysts must select data variables and then choose which transformations and visual encodings to apply. These decisions often involve both domain and visualization design expertise, and may impose a tedious speciﬁcation process that impedes exploration. In this paper, we seek to complement manual chart construction with interactive navigation of a gallery of automatically-generated visualizations. We contribute Voyager, a mixed-initiative system that supports faceted browsing of recommended charts chosen according to statistical and perceptual measures. We describe Voyager’s architecture, motivating design principles, and methods for generating and interacting with visualization recommendations. In a study comparing Voyager to a manual visualization speciﬁcation tool, we ﬁnd that Voyager facilitates exploration of previously unseen data and leads to increased data variable coverage. We then distill design implications for visualization tools, in particular the need to balance rapid exploration and targeted question-answering.},
	language = {en},
	number = {1},
	urldate = {2024-08-31},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Wongsuphasawat, Kanit and Moritz, Dominik and Anand, Anushka and Mackinlay, Jock and Howe, Bill and Heer, Jeffrey},
	month = jan,
	year = {2016},
	pages = {649--658},
	file = {Wongsuphasawat et al. - 2016 - Voyager Exploratory Analysis via Faceted Browsing.pdf:/Users/vsivaram/Zotero/storage/K9LVRW8P/Wongsuphasawat et al. - 2016 - Voyager Exploratory Analysis via Faceted Browsing.pdf:application/pdf},
}


@book{tukey_exploratory_1970,
	address = {Reading, MA},
	title = {Exploratory {Data} {Analysis}},
	isbn = {978-0-608-08225-7},
	abstract = {The approach in this introductory book is that of informal study of the data. Methods range from plotting picture-drawing techniques to rather elaborate numerical summaries. Several of the methods are the original creations of the author, and all can be carried out either with pencil or aided by hand-held calculator.},
	language = {en},
	publisher = {Addison Wesley Publishing Company},
	author = {Tukey, John Wilder},
	year = {1970},
	note = {Google-Books-ID: F6IIxgEACAAJ},
	keywords = {Mathematics / Probability \& Statistics / General},
}

@inproceedings{yee_2003_faceted,
author = {Yee, Ka-Ping and Swearingen, Kirsten and Li, Kevin and Hearst, Marti},
title = {Faceted metadata for image search and browsing},
year = {2003},
isbn = {1581136307},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/642611.642681},
doi = {10.1145/642611.642681},
abstract = {There are currently two dominant interface types for searching and browsing large image collections: keyword-based search, and searching by overall similarity to sample images. We present an alternative based on enabling users to navigate along conceptual dimensions that describe the images. The interface makes use of hierarchical faceted metadata and dynamically generated query previews. A usability study, in which 32 art history students explored a collection of 35,000 fine arts images, compares this approach to a standard image search interface. Despite the unfamiliarity and power of the interface (attributes that often lead to rejection of new search interfaces), the study results show that 90\% of the participants preferred the metadata approach overall, 97\% said that it helped them learn more about the collection, 75\% found it more flexible, and 72\% found it easier to use than a standard baseline system. These results indicate that a category-based approach is a successful way to provide access to image collections.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {401–408},
numpages = {8},
keywords = {faceted metadata, image search interfaces},
location = {Ft. Lauderdale, Florida, USA},
series = {CHI '03}
}

@article{cabrera2021deblinder,
author = {Cabrera, '{A}ngel Alexander and Druck, Abraham J. and Hong, Jason I. and Perer, Adam},title = {Discovering and Validating AI Errors With Crowdsourced Failure Reports},year = {2021},issue_date = {October 2021},publisher = {Association for Computing Machinery},address = {New York, NY, USA},volume = {5},number = {CSCW2},url = {https://doi.org/10.1145/3479569},doi = {10.1145/3479569},journal = {Proc. ACM Hum.-Comput. Interact.},month = oct,articleno = {425},numpages = {22}}


@article{Cavallo2019,
	title = {Clustrophile 2: {Guided} {Visual} {Clustering} {Analysis}},
	volume = {25},
	issn = {19410506},
	doi = {10.1109/TVCG.2018.2864477},
	abstract = {Data clustering is a common unsupervised learning method frequently used in exploratory data analysis. However, identifying relevant structures in unlabeled, high-dimensional data is nontrivial, requiring iterative experimentation with clustering parameters as well as data features and instances. The number of possible clusterings for a typical dataset is vast, and navigating in this vast space is also challenging. The absence of ground-truth labels makes it impossible to define an optimal solution, thus requiring user judgment to establish what can be considered a satisfiable clustering result. Data scientists need adequate interactive tools to effectively explore and navigate the large clustering space so as to improve the effectiveness of exploratory clustering analysis. We introduce Clustrophile 2, a new interactive tool for guided clustering analysis. Clustrophile 2 guides users in clustering-based exploratory analysis, adapts user feedback to improve user guidance, facilitates the interpretation of clusters, and helps quickly reason about differences between clusterings. To this end, Clustrophile 2 contributes a novel feature, the Clustering Tour, to help users choose clustering parameters and assess the quality of different clustering results in relation to current analysis goals and user expectations. We evaluate Clustrophile 2 through a user study with 12 data scientists, who used our tool to explore and interpret sub-cohorts in a dataset of Parkinson's disease patients. Results suggest that Clustrophile 2 improves the speed and effectiveness of exploratory clustering analysis for both experts and non-experts.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Cavallo, Marco and Demiralp, Çaǧatay},
	year = {2019},
	note = {arXiv: 1804.03048},
	keywords = {Clustering tour, Clustrophile, Dimensionality reduction, Explainability, Exploratory data analysis, Guided data analysis, Interactive clustering analysis, Interpretability, Unsupervised learning, Visual data exploration recommendation, What-if analysis},
	pages = {267--276},
	file = {PDF:/Users/vsivaram/Zotero/storage/2W7MEE4S/clustrophile.pdf:application/pdf},
}


@inproceedings{kim_interpretability_2018,
	title = {Interpretability {Beyond} {Feature} {Attribution}: {Quantitative} {Testing} with {Concept} {Activation} {Vectors} ({TCAV})},
	shorttitle = {Interpretability {Beyond} {Feature} {Attribution}},
	url = {https://proceedings.mlr.press/v80/kim18d.html},
	abstract = {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net’s internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result–for example, how sensitive a prediction of “zebra” is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.},
	language = {en},
	urldate = {2024-09-02},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {2668--2677},
	file = {Full Text PDF:/Users/vsivaram/Zotero/storage/TD7APN9J/Kim et al. - 2018 - Interpretability Beyond Feature Attribution Quant.pdf:application/pdf},
}


@article{ribeiro_anchors_2018,
	title = {Anchors: {High}-{Precision} {Model}-{Agnostic} {Explanations}},
	volume = {32},
	issn = {2374-3468, 2159-5399},
	shorttitle = {Anchors},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11491},
	doi = {10.1609/aaai.v32i1.11491},
	abstract = {We introduce a novel model-agnostic system that explains the behavior of complex models with high-precision rules called anchors, representing local, “sufﬁcient” conditions for predictions. We propose an algorithm to efﬁciently compute these explanations for any black-box model with high-probability guarantees. We demonstrate the ﬂexibility of anchors by explaining a myriad of different models for different domains and tasks. In a user study, we show that anchors enable users to predict how a model would behave on unseen instances with less effort and higher precision, as compared to existing linear explanations or no explanations.},
	language = {en},
	number = {1},
	urldate = {2024-05-24},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	month = apr,
	year = {2018},
	file = {Ribeiro et al. - 2018 - Anchors High-Precision Model-Agnostic Explanation.pdf:/Users/vsivaram/Zotero/storage/MM2BKMX4/Ribeiro et al. - 2018 - Anchors High-Precision Model-Agnostic Explanation.pdf:application/pdf},
}


@article{epperson_dead_2023,
	title = {Dead or {Alive}: {Continuous} {Data} {Profiling} for {Interactive} {Data} {Science}},
	issn = {1077-2626, 1941-0506, 2160-9306},
	shorttitle = {Dead or {Alive}},
	url = {https://ieeexplore.ieee.org/document/10301695/},
	doi = {10.1109/TVCG.2023.3327367},
	abstract = {Profiling data by plotting distributions and analyzing summary statistics is a critical step throughout data analysis. Currently, this process is manual and tedious since analysts must write extra code to examine their data after every transformation. This inefficiency may lead to data scientists profiling their data infrequently, rather than after each transformation, making it easy for them to miss important errors or insights. We propose continuous data profiling as a process that allows analysts to immediately see interactive visual summaries of their data throughout their data analysis to facilitate fast and thorough analysis. Our system, AutoProfiler, presents three ways to support continuous data profiling: (1) it automatically displays data distributions and summary statistics to facilitate data comprehension; (2) it is live, so visualizations are always accessible and update automatically as the data updates; (3) it supports follow up analysis and documentation by authoring code for the user in the notebook. In a user study with 16 participants, we evaluate two versions of our system that integrate different levels of automation: both automatically show data profiles and facilitate code authoring, however, one version updates reactively (“live”) and the other updates only on demand (“dead”). We find that both tools, dead or alive, facilitate insight discovery with 91\% of user-generated insights originating from the tools rather than manual profiling code written by users. Participants found live updates intuitive and felt it helped them verify their transformations while those with on-demand profiles liked the ability to look at past visualizations. We also present a longitudinal case study on how AutoProfiler helped domain scientists find serendipitous insights about their data through automatic, live data profiles. Our results have implications for the design of future tools that offer automated data analysis support.},
	language = {en},
	urldate = {2024-03-11},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Epperson, Will and Gorantla, Vaishnavi and Moritz, Dominik and Perer, Adam},
	year = {2023},
	pages = {1--11},
	file = {Epperson et al. - 2023 - Dead or Alive Continuous Data Profiling for Inter.pdf:/Users/vsivaram/Zotero/storage/6V8GAN3H/Epperson et al. - 2023 - Dead or Alive Continuous Data Profiling for Inter.pdf:application/pdf},
}

@ARTICLE{stolte_2002_polaris,
  author={Stolte, C. and Tang, D. and Hanrahan, P.},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Polaris: a system for query, analysis, and visualization of multidimensional relational databases}, 
  year={2002},
  volume={8},
  number={1},
  pages={52-65},
  keywords={Polarization;Visual databases;Data visualization;Multidimensional systems;Warehousing;Scientific computing;Relational databases;Spatial databases;Displays;Feedback},
  doi={10.1109/2945.981851}}


@inproceedings{pirolli_sensemaking_2005,
	address = {McLean, VA},
	title = {The sensemaking process and leverage points for analyst technology as identified through cognitive task analysis},
	booktitle = {Proceedings of the {International} {Conference} on {Intelligence} {Analysis}},
	author = {Pirolli, Peter and Card, Stuart},
	month = jan,
	year = {2005},
	file = {Full Text PDF:/Users/vsivaram/Zotero/storage/RWI8FJXM/Pirolli and Card - 2005 - The sensemaking process and leverage points for an.pdf:application/pdf},
}


@article{cabrera_what_2022,
	title = {What {Did} {My} {AI} {Learn}? {How} {Data} {Scientists} {Make} {Sense} of {Model} {Behavior}},
	issn = {1073-0516},
	doi = {10.1145/3542921},
	abstract = {Data scientists require rich mental models of how AI systems behave to effectively train, debug, and work with them. Despite the prevalence of AI analysis tools, there is no general theory describing how people make sense of what their models have learned. We frame this process as a form of sensemaking and derive a framework describing how data scientists develop mental models of AI behavior. To evaluate the framework, we show how existing AI analysis tools fit into this sensemaking process and use it to design AIFinnity , a system for analyzing image-and-text models. Lastly, we explored how data scientists use a tool developed with the framework through a think-aloud study with 10 data scientists tasked with using AIFinnity to pick an image captioning model. We found that AIFinnity ’s sensemaking workflow reflected participants’ mental processes and enabled them to discover and validate diverse AI behaviors.},
	journal = {ACM Transactions on Computer-Human Interaction},
	author = {Cabrera, Ángel Alexander and Ribeiro, Marco Tulio and Lee, Bongshin and DeLine, Rob and Perer, Adam and Drucker, Steven M.},
	year = {2022},
	file = {PDF:/Users/vsivaram/Zotero/storage/F75ITW75/3542921.pdf:application/pdf},
}

@article{lee_2021_lux,
author = {Lee, Doris Jung-Lin and Tang, Dixin and Agarwal, Kunal and Boonmark, Thyne and Chen, Caitlyn and Kang, Jake and Mukhopadhyay, Ujjaini and Song, Jerry and Yong, Micah and Hearst, Marti A. and Parameswaran, Aditya G.},
title = {Lux: always-on visualization recommendations for exploratory dataframe workflows},
year = {2021},
issue_date = {November 2021},
publisher = {VLDB Endowment},
volume = {15},
number = {3},
issn = {2150-8097},
url = {https://doi.org/10.14778/3494124.3494151},
doi = {10.14778/3494124.3494151},
abstract = {Exploratory data science largely happens in computational notebooks with dataframe APIs, such as pandas, that support flexible means to transform, clean, and analyze data. Yet, visually exploring data in dataframes remains tedious, requiring substantial programming effort for visualization and mental effort to determine what analysis to perform next. We propose Lux, an always-on framework for accelerating visual insight discovery in dataframe workflows. When users print a dataframe in their notebooks, Lux recommends visualizations to provide a quick overview of the patterns and trends and suggests promising analysis directions. Lux features a high-level language for generating visualizations on demand to encourage rapid visual experimentation with data. We demonstrate that through the use of a careful design and three system optimizations, Lux adds no more than two seconds of overhead on top of pandas for over 98\% of datasets in the UCI repository. We evaluate Lux in terms of usability via interviews with early adopters, finding that Lux helps fulfill the needs of data scientists for visualization support within their dataframe workflows. Lux has already been embraced by data science practitioners, with over 3.1k stars on Github.},
journal = {Proc. VLDB Endow.},
month = {nov},
pages = {727–738},
numpages = {12}
}

@article{felix_texttile_2017,
	title = {{TextTile}: {An} {Interactive} {Visualization} {Tool} for {Seamless} {Exploratory} {Analysis} of {Structured} {Data} and {Unstructured} {Text}},
	volume = {23},
	issn = {1941-0506},
	shorttitle = {{TextTile}},
	url = {https://ieeexplore.ieee.org/document/7539549/?arnumber=7539549},
	doi = {10.1109/TVCG.2016.2598447},
	abstract = {We describe TextTile, a data visualization tool for investigation of datasets and questions that require seamless and flexible analysis of structured data and unstructured text. TextTile is based on real-world data analysis problems gathered through our interaction with a number of domain experts and provides a general purpose solution to such problems. The system integrates a set of operations that can interchangeably be applied to the structured as well as to unstructured text part of the data to generate useful data summaries. Such summaries are then organized in visual tiles in a grid layout to allow their analysis and comparison. We validate TextTile with task analysis, use cases and a user study showing the system can be easily learned and proficiently used to carry out nontrivial tasks.},
	number = {1},
	urldate = {2024-09-06},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Felix, Cristian and Pandey, Anshul Vikram and Bertini, Enrico},
	month = jan,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	keywords = {Business, Collaboration, Data analysis, Data visualization, Exploratory Text Analysis, Keyword search, Knowledge Discovery, Medical services, Text Visualization, Visualization},
	pages = {161--170},
	file = {IEEE Xplore Full Text PDF:/Users/vsivaram/Zotero/storage/J75IG3BE/Felix et al. - 2017 - TextTile An Interactive Visualization Tool for Se.pdf:application/pdf},
}

@article{Liu2017,
	title = {Visualizing {High}-{Dimensional} {Data}: {Advances} in the {Past} {Decade}},
	volume = {23},
	issn = {10772626},
	doi = {10.1109/TVCG.2016.2640960},
	abstract = {Massive simulations and arrays of sensing devices, in combination with increasing computing resources, have generated large, complex, high-dimensional datasets used to study phenomena across numerous fields of study. Visualization plays an important role in exploring such datasets. We provide a comprehensive survey of advances in high-dimensional data visualization that focuses on the past decade. We aim at providing guidance for data practitioners to navigate through a modular view of the recent advances, inspiring the creation of new visualizations along the enriched visualization pipeline, and identifying future opportunities for visualization research.},
	number = {3},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Liu, Shusen and Maljovec, Dan and Wang, Bei and Bremer, Peer Timo and Pascucci, Valerio},
	year = {2017},
	pmid = {28113321},
	keywords = {visualization, computational modeling, data models, high-dimensional data, multidimensional data, Taxonomy},
	pages = {1249--1268},
	file = {PDF:/Users/vsivaram/Zotero/storage/LYBN2MV8/07784854.pdf:application/pdf},
}



@article{munzner_nested_2009,
	title = {A {Nested} {Model} for {Visualization} {Design} and {Validation}},
	volume = {15},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1077-2626},
	url = {http://ieeexplore.ieee.org/document/5290695/},
	doi = {10.1109/TVCG.2009.111},
	abstract = {We present a nested model for the visualization design and validation with four layers: characterize the task and data in the vocabulary of the problem domain, abstract into operations and data types, design visual encoding and interaction techniques, and create algorithms to execute techniques efﬁciently. The output from a level above is input to the level below, bringing attention to the design challenge that an upstream error inevitably cascades to all downstream levels. This model provides prescriptive guidance for determining appropriate evaluation approaches by identifying threats to validity unique to each level. We also provide three recommendations motivated by this model: authors should distinguish between these levels when claiming contributions at more than one of them, authors should explicitly state upstream assumptions at levels above the focus of a paper, and visualization venues should accept more papers on domain characterization.},
	language = {en},
	number = {6},
	urldate = {2024-09-07},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Munzner, Tamara},
	month = nov,
	year = {2009},
	pages = {921--928},
	file = {Munzner - 2009 - A Nested Model for Visualization Design and Valida.pdf:/Users/vsivaram/Zotero/storage/86IC4Z8C/Munzner - 2009 - A Nested Model for Visualization Design and Valida.pdf:application/pdf},
}


@inproceedings{Brehmer_2014_dr,
	address = {Paris, France},
	title = {Visualizing {Dimensionally}-{Reduced} {Data} : {Interviews} with {Analysts} and a {Characterization} of {Task} {Sequences}},
	doi = {https://doi.org/10.1145/2669557.2669559},
	abstract = {We characterize five task sequences related to visualizing dimensionally-reduced data, drawing from data collected from interviews with ten data analysts spanning six application domains, and from our understanding of the technique literature. Our characterization of visualization task sequences for dimensionallyreduced data fills a gap created by the abundance of proposed techniques and tools that combine high-dimensional data analysis, dimensionality reduction, and visualization, and is intended to be used in the design and evaluation of future techniques and tools. We discuss implications for the evaluation of existing work practices, for the design of controlled experiments, and for the analysis of post-deployment field observations.},
	booktitle = {Proceedings of the {Fifth} {Workshop} on {Beyond} {Time} and {Errors}: {Novel} {Evaluation} {Methods} for {Visualization}},
	author = {Brehmer, Matthew and Sedlmair, Michael and Ingram, Stephen and Munzner, Tamara},
	year = {2014},
	note = {ISBN: 9781450332095},
	keywords = {dimensionally-reduced data, interview study, tasks},
	pages = {1--8},
	file = {PDF:/Users/vsivaram/Zotero/storage/5YI7NDLA/visualizing dr data.pdf:application/pdf},
}


@article{maaten_visualizing_2008,
	title = {Visualizing {Data} using t-{SNE}},
	volume = {9},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v9/vandermaaten08a.html},
	abstract = {We present a new technique called "t-SNE" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images ofobjects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
	number = {86},
	urldate = {2024-09-07},
	journal = {Journal of Machine Learning Research},
	author = {Maaten, Laurens van der and Hinton, Geoffrey},
	year = {2008},
	pages = {2579--2605},
	file = {Full Text PDF:/Users/vsivaram/Zotero/storage/TI6552IB/Maaten and Hinton - 2008 - Visualizing Data using t-SNE.pdf:application/pdf},
}

@misc{adult_2,
  author       = {Becker, Barry and Kohavi, Ronny},
  title        = {{Adult}},
  year         = {1996},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C5XW20}
}

@inproceedings{zhang_2015_yelp,
author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
title = {Character-level convolutional networks for text classification},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {649–657},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}


@misc{noauthor_airline_2019,
	title = {Airline {Passenger} {Satisfaction}},
	url = {https://www.kaggle.com/datasets/teejmahal20/airline-passenger-satisfaction},
	publisher = {Kaggle},
	year = {2019},
}


@article{shen_interactive_2014,
	title = {Interactive notebooks: {Sharing} the code},
	volume = {515},
	copyright = {2014 Springer Nature Limited},
	issn = {1476-4687},
	shorttitle = {Interactive notebooks},
	url = {https://www.nature.com/articles/515151a},
	doi = {10.1038/515151a},
	abstract = {The free IPython notebook makes data analysis easier to record, understand and reproduce.},
	language = {en},
	number = {7525},
	urldate = {2024-09-10},
	journal = {Nature},
	author = {Shen, Helen},
	month = nov,
	year = {2014},
	note = {Publisher: Nature Publishing Group},
	keywords = {Communication, Computational biology and bioinformatics, Information technology, Publishing},
	pages = {151--152},
	file = {Full Text PDF:/Users/vsivaram/Zotero/storage/ETJMU3W8/Shen - 2014 - Interactive notebooks Sharing the code.pdf:application/pdf},
}

@article{2014_infovis_upset,
  title = {UpSet: Visualization of Intersecting Sets},
  author = {Alexander Lex and Nils Gehlenborg and Hendrik Strobelt and Romain Vuillemot and Hanspeter Pfister},
  journal = {IEEE Transactions on Visualization and Computer Graphics (InfoVis)},
  doi = {10.1109/TVCG.2014.2346248},
  volume = {20},
  number = {12},
  pages = {1983--1992},
  year = {2014}
}

@article{ji_beavertails_2023,
	title = {{BeaverTails}: {Towards} {Improved} {Safety} {Alignment} of {LLM} via a {Human}-{Preference} {Dataset}},
	volume = {36},
	shorttitle = {{BeaverTails}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/4dbb61cb68671edc4ca3712d70083b9f-Abstract-Datasets_and_Benchmarks.html},
	language = {en},
	urldate = {2024-09-10},
	journal = {Advances in Neural Information Processing Systems},
	author = {Ji, Jiaming and Liu, Mickel and Dai, Josef and Pan, Xuehai and Zhang, Chi and Bian, Ce and Chen, Boyuan and Sun, Ruiyang and Wang, Yizhou and Yang, Yaodong},
	month = dec,
	year = {2023},
	pages = {24678--24704},
	file = {Full Text PDF:/Users/vsivaram/Zotero/storage/7DT9DZWL/Ji et al. - 2023 - BeaverTails Towards Improved Safety Alignment of .pdf:application/pdf},
}


@misc{ji_pku-saferlhf_2024,
	title = {{PKU}-{SafeRLHF}: {A} {Safety} {Alignment} {Preference} {Dataset} for {Llama} {Family} {Models}},
	shorttitle = {{PKU}-{SafeRLHF}},
	url = {http://arxiv.org/abs/2406.15513},
	doi = {10.48550/arXiv.2406.15513},
	abstract = {In this work, we introduce the PKU-SafeRLHF dataset, designed to promote research on safety alignment in large language models (LLMs). As a sibling project to SafeRLHF and BeaverTails, we separate annotations of helpfulness and harmlessness for question-answering pairs, providing distinct perspectives on these coupled attributes. Overall, we provide 44.6k refined prompts and 265k question-answer pairs with safety meta-labels for 19 harm categories and three severity levels ranging from minor to severe, with answers generated by Llama-family models. Based on this, we collected 166.8k preference data, including dual-preference (helpfulness and harmlessness decoupled) and single-preference data (trade-off the helpfulness and harmlessness from scratch), respectively. Using the large-scale annotation data, we further train severity-sensitive moderation for the risk control of LLMs and safety-centric RLHF algorithms for the safety alignment of LLMs. We believe this dataset will be a valuable resource for the community, aiding in the safe deployment of LLMs.},
	urldate = {2024-09-10},
	publisher = {arXiv},
	author = {Ji, Jiaming and Hong, Donghai and Zhang, Borong and Chen, Boyuan and Dai, Josef and Zheng, Boren and Qiu, Tianyi and Li, Boxun and Yang, Yaodong},
	month = jun,
	year = {2024},
	note = {arXiv:2406.15513 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: a sibling project to SafeRLHF and BeaverTails},
	file = {arXiv Fulltext PDF:/Users/vsivaram/Zotero/storage/N4MQRWVA/Ji et al. - 2024 - PKU-SafeRLHF A Safety Alignment Preference Datase.pdf:application/pdf},
}


@article{cao_data_2018,
	title = {Data {Science}: {A} {Comprehensive} {Overview}},
	volume = {50},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Data {Science}},
	url = {https://dl.acm.org/doi/10.1145/3076253},
	doi = {10.1145/3076253},
	abstract = {The 21st century has ushered in the age of big data and data economy, in which
              data DNA
              , which carries important knowledge, insights, and potential, has become an intrinsic constituent of all data-based organisms. An appropriate understanding of data DNA and its organisms relies on the new field of
              data science
              and its keystone,
              analytics
              . Although it is widely debated whether big data is only hype and buzz, and data science is still in a very early phase, significant challenges and opportunities are emerging or have been inspired by the research, innovation, business, profession, and education of data science. This article provides a comprehensive survey and tutorial of the fundamental aspects of data science: the evolution from data analysis to data science, the data science concepts, a big picture of the era of data science, the major challenges and directions in data innovation, the nature of data analytics, new industrialization and service opportunities in the data economy, the profession and competency of data education, and the future of data science. This article is the first in the field to draw a comprehensive big picture, in addition to offering rich observations, lessons, and thinking about data science and analytics.},
	language = {en},
	number = {3},
	urldate = {2024-09-10},
	journal = {ACM Computing Surveys},
	author = {Cao, Longbing},
	month = may,
	year = {2018},
	pages = {1--42},
	file = {Full Text PDF:/Users/vsivaram/Zotero/storage/ITSSBJQF/Cao - 2018 - Data Science A Comprehensive Overview.pdf:application/pdf},
}


@article{lycett_datafication_2013,
	title = {‘{Datafication}’: making sense of (big) data in a complex world},
	volume = {22},
	copyright = {http://www.springer.com/tdm},
	issn = {0960-085X, 1476-9344},
	shorttitle = {‘{Datafication}’},
	url = {https://www.tandfonline.com/doi/full/10.1057/ejis.2013.10},
	doi = {10.1057/ejis.2013.10},
	language = {en},
	number = {4},
	urldate = {2024-09-10},
	journal = {European Journal of Information Systems},
	author = {Lycett, Mark},
	month = jul,
	year = {2013},
	pages = {381--386},
	file = {Full Text PDF:/Users/vsivaram/Zotero/storage/M87T39QA/Lycett - 2013 - ‘Datafication’ making sense of (big) data in a co.pdf:application/pdf},
}


@misc{noauthor_customer_2024,
	title = {Customer {Segmentation}: {Definition}, {Examples} + {How} to {Do} {It}},
	shorttitle = {Customer {Segmentation}},
	url = {https://www.coursera.org/articles/customer-segmentation},
	abstract = {Discover what customer segmentation is, how it helps businesses succeed, and how to segment and reach your customers more effectively.},
author = {{Coursera Staff}},
	language = {en},
	urldate = {2024-09-10},
	journal = {Coursera},
	month = jan,
	year = {2024},
	file = {Snapshot:/Users/vsivaram/Zotero/storage/NUFX5KQ8/customer-segmentation.html:text/html},
}


@article{tausczik_psychological_2010,
	title = {The {Psychological} {Meaning} of {Words}: {LIWC} and {Computerized} {Text} {Analysis} {Methods}},
	volume = {29},
	copyright = {http://journals.sagepub.com/page/policies/text-and-data-mining-license},
	issn = {0261-927X, 1552-6526},
	shorttitle = {The {Psychological} {Meaning} of {Words}},
	url = {http://journals.sagepub.com/doi/10.1177/0261927X09351676},
	doi = {10.1177/0261927X09351676},
	abstract = {We are in the midst of a technological revolution whereby,for the first time,researchers can link daily word use to a broad array of real-world behaviors.This article reviews several computerized text analysis methods and describes how Linguistic Inquiry and Word Count (LIWC) was created and validated. LIWC is a transparent text analysis program that counts words in psychologically meaningful categories. Empirical results using LIWC demonstrate its ability to detect meaning in a wide variety of experimental settings, including to show attentional focus, emotionality, social relationships, thinking styles, and individual differences.},
	language = {en},
	number = {1},
	urldate = {2024-09-10},
	journal = {Journal of Language and Social Psychology},
	author = {Tausczik, Yla R. and Pennebaker, James W.},
	month = mar,
	year = {2010},
	pages = {24--54},
	file = {Tausczik and Pennebaker - 2010 - The Psychological Meaning of Words LIWC and Compu.pdf:/Users/vsivaram/Zotero/storage/2LK3JEGE/Tausczik and Pennebaker - 2010 - The Psychological Meaning of Words LIWC and Compu.pdf:application/pdf},
}


@inproceedings{furnkranz_brief_2015,
	address = {Cham},
	title = {A {Brief} {Overview} of {Rule} {Learning}},
	isbn = {978-3-319-21542-6},
	doi = {10.1007/978-3-319-21542-6_4},
	abstract = {In this paper, we provide a brief summary of elementary research in rule learning. The two main research directions are descriptive rule learning, with the goal of discovering regularities that hold in parts of the given dataset, and predictive rule learning, which aims at generalizing the given dataset so that predictions on new data can be made. We briefly review key learning tasks such as association rule learning, subgroup discovery, and the covering learning algorithm, along with their most important prototypes. The paper also highlights recent work in rule learning on the Semantic Web and Linked Data as an important application area.},
	language = {en},
	booktitle = {Rule {Technologies}: {Foundations}, {Tools}, and {Applications}},
	publisher = {Springer International Publishing},
	author = {Fürnkranz, Johannes and Kliegr, Tomáš},
	editor = {Bassiliades, Nick and Gottlob, Georg and Sadri, Fariba and Paschke, Adrian and Roman, Dumitru},
	year = {2015},
	keywords = {Association Rule, Association Rule Mining, Frequent Itemsets, Inductive Logic Programming, Rule Discovery},
	pages = {54--69},
}


@article{han_mining_2000,
	title = {Mining frequent patterns without candidate generation},
	volume = {29},
	issn = {0163-5808},
	url = {https://dl.acm.org/doi/10.1145/335191.335372},
	doi = {10.1145/335191.335372},
	abstract = {Mining frequent patterns in transaction databases, time-series databases, and many other kinds of databases has been studied popularly in data mining research. Most of the previous studies adopt an Apriori-like candidate set generation-and-test approach. However, candidate set generation is still costly, especially when there exist prolific patterns and/or long patterns.In this study, we propose a novel frequent pattern tree (FP-tree) structure, which is an extended prefix-tree structure for storing compressed, crucial information about frequent patterns, and develop an efficient FP-tree-based mining method, FP-growth, for mining the complete set of frequent patterns by pattern fragment growth. Efficiency of mining is achieved with three techniques: (1) a large database is compressed into a highly condensed, much smaller data structure, which avoids costly, repeated database scans, (2) our FP-tree-based mining adopts a pattern fragment growth method to avoid the costly generation of a large number of candidate sets, and (3) a partitioning-based, divide-and-conquer method is used to decompose the mining task into a set of smaller tasks for mining confined patterns in conditional databases, which dramatically reduces the search space. Our performance study shows that the FP-growth method is efficient and scalable for mining both long and short frequent patterns, and is about an order of magnitude faster than the Apriori algorithm and also faster than some recently reported new frequent pattern mining methods.},
	number = {2},
	urldate = {2024-09-11},
	journal = {SIGMOD Rec.},
	author = {Han, Jiawei and Pei, Jian and Yin, Yiwen},
	month = may,
	year = {2000},
	pages = {1--12},
	file = {Full Text PDF:/Users/vsivaram/Zotero/storage/VWUH8IWQ/Han et al. - 2000 - Mining frequent patterns without candidate generat.pdf:application/pdf},
}


@article{hullman_designing_2021,
	title = {Designing for {Interactive} {Exploratory} {Data} {Analysis} {Requires} {Theories} of {Graphical} {Inference}},
	url = {https://hdsr.mitpress.mit.edu/pub/w075glo6},
	doi = {10.1162/99608f92.3ab8a587},
	abstract = {Research and development in computer science and statistics have produced increasingly sophisticated software interfaces for interactive and exploratory analysis, optimized for easy pattern finding and data exposure. But design philosophies that emphasize exploration over other phases of analysis risk confusing a need for flexibility with a conclusion that exploratory visual analysis is inherently “model free” and cannot be formalized. We describe how without a grounding in theories of human statistical inference, research in exploratory visual analysis can lead to contradictory interface objectives and representations of uncertainty that can discourage users from drawing valid inferences. We discuss how the concept of a model check in a Bayesian statistical framework unites exploratory and confirmatory analysis, and how this understanding relates to other proposed theories of graphical inference. Viewing interactive analysis as driven by model checks suggests new directions for software and empirical research around exploratory and visual analysis. For example, systems might enable specifying and explicitly comparing data to null and other reference distributions and better representations of uncertainty. Implications of Bayesian and other theories of graphical inference can be tested against outcomes of interactive analysis by people to drive theory development.},
	language = {en},
	urldate = {2024-09-11},
	journal = {Harvard Data Science Review},
	author = {Hullman, Jessica and Gelman, Andrew},
	month = jul,
	year = {2021},
	file = {Hullman and Gelman - 2021 - Designing for Interactive Exploratory Data Analysi.pdf:/Users/vsivaram/Zotero/storage/CAVNGBLK/Hullman and Gelman - 2021 - Designing for Interactive Exploratory Data Analysi.pdf:application/pdf},
}


@inproceedings{alkhatib_street-level_2019,
	address = {New York, NY, USA},
	series = {{CHI} '19},
	title = {Street-{Level} {Algorithms}: {A} {Theory} at the {Gaps} {Between} {Policy} and {Decisions}},
	isbn = {978-1-4503-5970-2},
	shorttitle = {Street-{Level} {Algorithms}},
	url = {https://doi.org/10.1145/3290605.3300760},
	doi = {10.1145/3290605.3300760},
	abstract = {Errors and biases are earning algorithms increasingly malignant reputations in society. A central challenge is that algorithms must bridge the gap between high-level policy and on-the-ground decisions, making inferences in novel situations where the policy or training data do not readily apply. In this paper, we draw on the theory of street-level bureaucracies, how human bureaucrats such as police and judges interpret policy to make on-the-ground decisions. We present by analogy a theory of street-level algorithms, the algorithms that bridge the gaps between policy and decisions about people in a socio-technical system. We argue that unlike street-level bureaucrats, who reflexively refine their decision criteria as they reason through a novel situation, street-level algorithms at best refine their criteria only after the decision is made. This loop-and-a-half delay results in illogical decisions when handling new or extenuating circumstances. This theory suggests designs for street-level algorithms that draw on historical design patterns for street-level bureaucracies, including mechanisms for self-policing and recourse in the case of error.},
	urldate = {2024-06-27},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Alkhatib, Ali and Bernstein, Michael},
	month = may,
	year = {2019},
	pages = {1--13},
	file = {Full Text:/Users/vsivaram/Zotero/storage/TYGJRGIZ/Alkhatib and Bernstein - 2019 - Street-Level Algorithms A Theory at the Gaps Betw.pdf:application/pdf},
}


@inproceedings{tal_target_2023,
	address = {Montr{\textbackslash}'\{e\}al QC Canada},
	title = {Target specification bias, counterfactual prediction, and algorithmic fairness in healthcare},
	isbn = {9798400702310},
	url = {https://dl.acm.org/doi/10.1145/3600211.3604678},
	doi = {10.1145/3600211.3604678},
	language = {en},
	urldate = {2024-02-26},
	booktitle = {Proceedings of the 2023 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {ACM},
	author = {Tal, Eran},
	month = aug,
	year = {2023},
	pages = {312--321},
	file = {Full Text PDF:/Users/vsivaram/Zotero/storage/64PLWCR4/Tal - 2023 - Target specification bias, counterfactual predicti.pdf:application/pdf},
}


@article{cohen_personalized_2021,
	title = {Personalized lab test models to quantify disease potentials in healthy individuals},
	volume = {27},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-170X},
	url = {https://www.nature.com/articles/s41591-021-01468-6},
	doi = {10.1038/s41591-021-01468-6},
	abstract = {Standardized lab tests are central for patient evaluation, differential diagnosis and treatment. Interpretation of these data is nevertheless lacking quantitative and personalized metrics. Here we report on the modeling of 2.1 billion lab measurements of 92 different lab tests from 2.8 million adults over a span of 18 years. Following unsupervised filtering of 131 chronic conditions and 5,223 drug–test pairs we performed a virtual survey of lab tests distributions in healthy individuals. Age and sex alone explain less than 10\% of the within-normal test variance in 89 out of 92 tests. Personalized models based on patients’ history explain 60\% of the variance for 17 tests and over 36\% for half of the tests. This allows for systematic stratification of the risk for future abnormal test levels and subsequent emerging disease. Multivariate modeling of within-normal lab tests can be readily implemented as a basis for quantitative patient evaluation.},
	language = {en},
	number = {9},
	urldate = {2024-09-11},
	journal = {Nature Medicine},
	author = {Cohen, Netta Mendelson and Schwartzman, Omer and Jaschek, Ram and Lifshitz, Aviezer and Hoichman, Michael and Balicer, Ran and Shlush, Liran I. and Barbash, Gabi and Tanay, Amos},
	month = sep,
	year = {2021},
	note = {Publisher: Nature Publishing Group},
	keywords = {Medical research, Risk factors},
	pages = {1582--1591},
}

@inproceedings{lam_concept_2024,
author = {Lam, Michelle S. and Teoh, Janice and Landay, James A. and Heer, Jeffrey and Bernstein, Michael S.},
title = {Concept Induction: Analyzing Unstructured Text with High-Level Concepts Using LLooM},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642830},
doi = {10.1145/3613904.3642830},
abstract = {Data analysts have long sought to turn unstructured text data into meaningful concepts. Though common, topic modeling and clustering focus on lower-level keywords and require significant interpretative work. We introduce concept induction, a computational process that instead produces high-level concepts, defined by explicit inclusion criteria, from unstructured text. For a dataset of toxic online comments, where a state-of-the-art BERTopic model outputs “women, power, female,” concept induction produces high-level concepts such as “Criticism of traditional gender roles” and “Dismissal of women’s concerns.” We present LLooM, a concept induction algorithm that leverages large language models to iteratively synthesize sampled text and propose human-interpretable concepts of increasing generality. We then instantiate LLooM in a mixed-initiative text analysis tool, enabling analysts to shift their attention from interpreting topics to engaging in theory-driven analysis. Through technical evaluations and four analysis scenarios ranging from literature review to content moderation, we find that LLooM’s concepts improve upon the prior art of topic models in terms of quality and data coverage. In expert case studies, LLooM helped researchers to uncover new insights even from familiar datasets, for example by suggesting a previously unnoticed concept of attacks on out-party stances in a political social media dataset.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {766},
numpages = {28},
keywords = {data visualization, human-AI interaction, large language models, topic modeling, unstructured text analysis},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{sivaraman_emblaze_2022,
author = {Sivaraman, Venkatesh and Wu, Yiwei and Perer, Adam},
title = {Emblaze: Illuminating Machine Learning Representations through Interactive Comparison of Embedding Spaces},
year = {2022},
isbn = {9781450391443},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490099.3511137},
doi = {10.1145/3490099.3511137},
abstract = {Modern machine learning techniques commonly rely on complex, high-dimensional embedding representations to capture underlying structure in the data and improve performance. In order to characterize model flaws and choose a desirable representation, model builders often need to compare across multiple embedding spaces, a challenging analytical task supported by few existing tools. We first interviewed nine embedding experts in a variety of fields to characterize the diverse challenges they face and techniques they use when analyzing embedding spaces. Informed by these perspectives, we developed a novel system called Emblaze that integrates embedding space comparison within a computational notebook environment. Emblaze uses an animated, interactive scatter plot with a novel Star Trail augmentation to enable visual comparison. It also employs novel neighborhood analysis and clustering procedures to dynamically suggest groups of points with interesting changes between spaces. Through a series of case studies with ML experts, we demonstrate how interactive comparison with Emblaze can help gain new insights into embedding space structure.},
booktitle = {Proceedings of the 27th International Conference on Intelligent User Interfaces},
pages = {418–432},
numpages = {15},
keywords = {animation, dimensionality reduction, embedding space comparison, machine learning, visualization},
location = {Helsinki, Finland},
series = {IUI '22}
}

@INPROCEEDINGS{sivaraman_2024_counterpoint,
  author={Sivaraman, Venkatesh and Elavsky, Frank and Moritz, Dominik and Perer, Adam},
  booktitle={2024 IEEE Visualization and Visual Analytics (VIS)}, 
  title={Counterpoint: Orchestrating Large-Scale Custom Animated Visualizations}, 
  year={2024},
  volume={},
  number={},
  pages={16-20},
  keywords={Performance evaluation;Visualization;Visual analytics;Rendering (computer graphics);Visualization Toolkits;Animation;Web Interfaces;Software System Structures},
  doi={10.1109/VIS55277.2024.00011}}
