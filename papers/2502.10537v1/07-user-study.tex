\section{User Study}
\label{sec:user-study}

To evaluate data scientists' use of exploratory subgroup analysis, we conducted a think-aloud study with Divisi centered around three research questions:
\begin{enumerate}
\item What is the role of subgroup analysis in data scientists’ existing workflows?
\item What insights do data scientists uncover during the different stages of exploratory subgroup analysis?
\item What opportunities do data scientists perceive to use exploratory subgroup analysis in their work?
\end{enumerate}

\subsection{Study Design}

Each think-aloud session lasted around one hour and consisted of an introduction and pre-study survey, two hands-on tasks using the Divisi interface, and a final debriefing interview (see Sec. B.2 in the Supplementary Material for the questions used). 
% We aimed to understand the current role of subgroup analysis in data scientists’ existing workflows through pre-study questions and by asking participants how they would approach the subgroup analysis tasks before using Divisi. 
% % Participants were encouraged to  throughout the analysis process.
% Then, we gathered insights into the patterns and discoveries data scientists could uncover through the two tasks. 
% Then, with the debrief questions, we gathered new opportunities participants perceived, both from their experiences with Divisi and from their own project contexts. 
% Additionally, we also evaluated how data scientists perceived Divisi’s different features and how well the features support the needs of data scientists for subgroup analysis, by gathering their feedback on the tool throughout the study session. 
%add more details of participants?
In the introduction phase, we gathered background information about the participants’ experience with data science and subgroup analysis, setting the context for the tasks that followed. Participants completed a pre-study survey that contained questions pertaining their occupation, years of experience with data and machine learning, their familiarity with Python, and their familiarity with and approach to subgroup analysis. 
%Participants were also encouraged to provide additional context verbally.

The two tasks in the study required participants to use Divisi to analyze the Airline Passenger Satisfaction dataset mentioned in Sec. \ref{sec:performance-eval}. 
The dataset contains results of a survey of 129,880 passengers’ satisfaction with the airline based on various aspects of the airline services. 
The 22 features in the dataset included demographic information (e.g., Gender and Age), traveler information (Customer Type, Type of Travel, Class), flight information (Flight Distance, Departure Delay in Minutes, Arrival Delay in Minutes), ratings for individual aspects of the flight experience (inflight Wi-Fi service, cleanliness, etc.), and an overall satisfaction rating of satisfied or not satisfied. We converted the 5-point ratings for aspects of the flight experience into ``not satisfied,'' ``neutral,'' and ``satisfied'' to simplify the space of possible subgroups for the user study.
The two tasks were as follows:
\begin{enumerate}[label={\bfseries Study Task \arabic*:}, ref={Study Task \arabic*},itemsep=1ex,labelindent=0pt, wide=0pt]
    \item \textbf{Dissatisfaction.} Participants were asked to discover and interpret subgroups of data that could provide insights into what types of customers tended to be dissatisfied with their experience. Overall, 57\% of instances in the dataset had an overall rating of dissatisfied, meaning that the outcome of interest was fairly common. We asked participants to perform any analysis they would want to be able to present a comprehensive report to stakeholders at the airline company.
    %An example of how the interface looked during this task, and the subgroup results that appeared, is shown in Fig. \ref{fig:airline-example}. 
    \label{study-task:dissatisfaction}

    \item \textbf{Model Errors.} We trained a binary classification model to predict each passenger's overall satisfaction rating, resulting in an error rate of 4.8\%. Participants were then asked to find out what subgroups tended to have higher-than-average model error rates. Similar to the dissatisfaction analysis task, participants were told to curate their insights for stakeholders of the airline company. This task was slightly more difficult because of the involvement of a classification model as well as the lower outcome base rate, which made the discovered rules slightly more complex to interpret. \label{study-task:error}
\end{enumerate}
Before the tasks began, the interviewers provided a brief tutorial of how to use Divisi using an annotated screenshot. Participants were also given guidance during the session on how to use the tool for specific tasks that they expressed interest in performing.
After completing the tasks, we asked participants to evaluate the usefulness of the various features of Divisi, and to reflect on how the system could apply to their past projects. 

We recruited 13 data scientists with some experience with machine learning and data science, particularly involving large-scale tabular or text data. 
(None of the experts from the formative work also participated in this study.)
Participants were compensated \$20 USD, and the protocol was approved by our institutional IRB.
Each one-hour session was conducted on Zoom and was recorded and transcribed for analysis.

\subsection{Analysis}
To analyze the data collected during the study, we took an open coding approach on the transcripts and video recordings of the study sessions. Two of the authors each coded all transcripts, discussing disagreements and reaching consensus as needed, to understand participants' perspectives and answer the three research questions above. The videos were used to supplement the transcripts with specifics about the participants' actions using the system. The coders generated 483 codes after annotating all of the study transcripts. We then used affinity diagramming to group codes into higher-level categories to reveal broader themes and opportunities.

\subsection{Results}
Overall, participants were able to uncover a broad range of insights into both dissatisfaction and model errors in the Airline dataset using Divisi.
They also compared the process of exploratory subgroup analysis to their current workflows, and they suggested ways to make Divisi easier to learn and more broadly applicable.

\subsubsection{Current Practices: Participants have few structured ways to perform subgroup analysis}
\label{sec:results-current-practices}
Participants in our study had varying degrees of experience with subgroup analysis (9/13 reported having used it in a prior project), but none used tools specific to subgroup analysis.
Many of those who used subgroup analysis mentioned using off-the-shelf programming libraries such as \texttt{pandas} and \texttt{matplotlib} (5/13 participants) to manually create filtering rules, which limited their opportunities for discovery: 
\begin{quote}
\textit{``When I'm doing data analysis, most of the time it's just for creating the plots using matplotlib. I'm just exploring the features that I'm interested in, but I cannot explore all of the things.''} (participant 1, denoted P1)
\end{quote}
When it came to interpreting models or identifying features associated with an outcome, there were few interactive ways to do so. 
Some participants described using clustering analyses to identify useful subgroups in an unsupervised manner (3/13).
However, these participants found clustering to be cumbersome for exploratory analysis because of the challenge of going into each cluster and interpreting what kinds of instances it contained.
Instead, most others described training simple models such as logistic regression to predict the outcome of interest, \textit{``and then just [seeing] what that spits out''} (P10).
Participants (8/13) mentioned that compared to these strategies, using Divisi would make it easier and faster to narrow down which variables to focus on:
% ``I think it'd be really helpful to see the slices that i'm not aware of'' (P8)
\textit{``Now I have a starting point. I can just start diving in''} (P6).
% ``this is the equivalent of like diving straight into the patient and being like, these people had symptoms, these people didn't'' (P6).

\subsubsection{Discovery: Participants identify surprising insights in subgroup results}
\label{sec:results-discovery}
All participants were able to quickly make assessments of what types of instances led to both dissatisfaction (\ref{study-task:dissatisfaction}) and model errors (\ref{study-task:error}), performing \ref{task:find-subgroups} (Find Subgroups) using the results in the Subgroups Table.
A strategy taken by several participants (6/13) was to glance over the table and find individual features and values that appeared frequently: \textit{``From this page overall I could see that people who are not satisfied with the Wi-Fi service have higher dissatisfaction rate''} (P7).
In this way, participants found that people with dissatisfied or neutral ratings with Wi-Fi service, gate location, and online boarding often tended to be dissatisfied overall.
Likewise, in \ref{study-task:error}, customers who were not part of the airline's loyalty program (identified by 8/13 participants) and those with several neutral ratings (7/13) were frequently found to be mispredicted.

In many cases, the patterns participants observed in the subgroup results went against their intuitions, suggesting that they might not have uncovered the same subgroups had they been guided only by their own expectations.
For instance, several users described expecting to see passengers who were dissatisfied with the flight delay (4/13 participants) or baggage handling (P5), neither of which were commonly-surfaced subgrouping features: \textit{``It is kind of surprising to me, because I think the departure delay... will be the most important thing that people will complain about''} (P1).
To verify that dissatisfaction with delays was indeed less than the other returned subgroups, P1 created a custom rule representing high departure delay and compared its metrics to the rest of the table (an instance of \ref{task:investigate-data-features}, Investigate Data Features).

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth, alt={Screenshot of Divisi from the user study in which a participant has three subgroups selected and is visualizing their overlap in the Subgroup Map. Almost all shaded bubbles (Dissatisfaction instances) are covered by the selection. In the Ranking Functions, the user has both Dissatisfaction and Dissatisfaction Coverage enabled with equal weighting.}]{figures/airline_example_user.pdf}
    \caption{During \ref{study-task:dissatisfaction}, P13 investigated the intersections of three rules involving the passenger's fare class and their satisfaction with the Wi-Fi service and online boarding features. They decided to interpret these subgroups because as shown in the Subgroup Map at right, they cover almost all of the shaded (dissatisfied) bubbles.}
    \label{fig:airline-example}
\end{figure*}

Participants used the Ranking Functions extensively during the discovery process.
Although many participants (5/13) found the ranking functionality confusing at first, they ultimately found it helpful to tailor the results to the kinds of subgroups they were looking for.
The most common criterion was ranking by the Binary Outcome Rate function for dissatisfaction or error (11/13 participants); however, many (7/13) also spent time prioritizing subgroups using the Binary Outcome Coverage function.
To get a holistic sense of the dissatisfied or mispredicted instances, participants saw coverage as a way to find subgroups that were \textit{``actually most representative of that group''} (P8).
Users (6/13) also adjusted the Simple Rule function for the purposes of familiarizing themselves with the dataset's variables and finding subgroups that would be easier to explain to a stakeholder: \textit{``Whatever conclusions you have, you want them to be simple enough. People usually don't care about if you... go very deep on different clusters of customers''} (P12).
Putting these desiderata together, P11 described how they set up their ranking weights:
\begin{quote}
\textit{``First, I don't want the rules to be too complex, so I want to keep it simple, but not that simple. Not just one [feature]... And I want these rules to be representative. So actually, I want the size to be like around 10\% [of the dataset]... And also I want the percentage of the errors as high as possible.''}
\end{quote}
Since different users expressed different criteria for what a ``good'' subgroup would look like, we observed more variation in analysis paths than if only one set of rankings were available.

% \begin{enumerate}
    % \item By looking over the slice results list, participants identify single features that seem to contribute highly to the outcome of interest. ``so from this page overall I could see that people who are not satisfied with the Wi-fi service have higher dissatisfaction rate.'' (P7) ``I see customer type in most of these subgroups, and there are all disloyal customers, which could indicate that the model's not really good at using the variable customer type.'' (P7) ``Well, customer type seems to be really important seems to... be, I guess, a tricky predictor'' (P8) ``It's interesting to me that all of these are like neutral predictors. Which in retrospect makes sense. If I feel neutral about it, then I don't want to say I'm dissatisfied or not about it.'' (P6) 
    % \item Participants frequently find surprising subgroups and patterns in the dataset. ``It is kind of surprising to me, because I think the departure delay... will be the most important thing that people will complain about'' (P1).
    % \item Participants take diverse strategies to manipulate on the ranking functions to achieve the slices they think will be most useful. ``That's like my 1st kind of criteria I have for slicing the data like, which of these slices are actually most representative of that group.'' (P8) ``so at this point, I want to see the error is as much as as high as possible.'' (P11) ``the 1st thing was, I wanted to know where was the model performing worse? Where was the highest error? ... So I upped the weight of the error.'' (P8) ``whatever conclusions you have. You want them to be simple enough. People usually don't care about if you like, go like very deep on, like different clusters of customers and stuff.'' (P12) Mixed criteria: ``First, I don't want the rules to be too complex, so I want to keep it simple, but not that simple. Not just one. Yeah, so not that simple. And I want these, I want these rules to be representative. So actually, I want the cons to be like around 10\% that's what I mentioned. And a score functions here. And also I want the percentage of the errors as high as possible.'' (P11) ``I kind of want to, you want to see like, the main effects before you see the like, these are kind of like, oh, the interaction effects.'' (P5)
% \end{enumerate}

\subsubsection{Evaluation: Participants use subgroup editing tools extensively to test feature interactions}
\label{sec:results-evaluation}

Participants schematized their understanding of subgroup behaviors (\ref{task:schematize}) and searched for additional evidence (\ref{task:search-evidence}) in diverse ways, most often by editing existing subgroup rules and creating new ones.
Most straightforwardly, several participants tested alternative values for features to observe correlations in outcomes: \textit{``The longer the flight [the greater the satisfaction]. Your... 10,000-mile business flight has only 24\% dissatisfaction, but at 500 [miles] it's 52\%''} (P10).
By performing this lightweight editing in subgroups defined by multiple features, participants (6/13) reasoned about interaction effects, i.e., feature values that had more pronounced impacts on outcomes when a different feature took a particular value.
For instance, by interactively toggling between economy and business class customers, P12 found that \textit{``in-flight service doesn't matter as much for economy customers.''}
This type of interaction helped 3/13 participants conclude that combining ratings with different polarities could cause higher error rates: \textit{``People are maybe very satisfied with something, but they may be unsatisfied with something else''} (P1).

In addition to editing rules, 6/13 participants used the custom rule feature to test hypotheses, driven by both prior knowledge and the patterns they observed.
For example, some participants (4/13) tried to extend rules they found in the Subgroups Table with additional features:
\begin{quote}
\textit{``I think I have to look closer [at] people who were neutral on the in-flight service, and then combine more within that subgroup to see if there's something actionable... Maybe that experience intersects with something else, and those two things are intertwined and actionable to improve the service.''} (P8)
\end{quote}
Except for one experienced user who wanted to add features to a rule in bulk using Python code (P10), Divisi's rule editor was sufficient for any query that users wanted to make.
Custom rules thereby helped participants both investigate features of interest (\ref{task:investigate-data-features}) and build up a more robust understanding of feature interactions (\ref{task:schematize}).

While evaluating subgroups, some participants (5/13) wondered whether the subgrouping features could be appearing due to randomness or spurious correlations, an evaluation aspect that Divisi does not currently support.
While Divisi splits the data into discovery and evaluation sets to mitigate false discoveries (Sec. \ref{sec:testing-robustness}), it is always possible that two features are highly correlated so that slicing by one of them is sufficient to slice by both.
For example, looking at a rule with high overall dissatisfaction but a satisfied rating on the departure/arrival time, P7 expressed the desire to dig deeper into other feature values within the subgroup to test whether \textit{``people in this subgroup are not satisfied with other aspects''}.

% \begin{enumerate}
    % \item Participants use slice customization heavily to understand how different feature values interact with each other. Correlations in feature values: ``The longer the flight. You're most like a 10,000-mile business flight has only 24\% of satisfaction. But at 500 it's 52.'' (P10) ``If you like. Have a lot of dissatisfaction ... you kind of can say that like in flight service doesn't matter as much for economy customers as [for business]'' (P12) This helped a few participants intuit that conflicting ratings could cause higher error rates: ``So people are maybe very satisfied with something, but they may be unsatisfied with something else.'' (P1)

    % \item Participants envision creating custom slices to test hypotheses driven by both prior knowledge and the patterns they observe. ``So I think I have to look closer of people who were neutral on the in-flight service, and then combine more with within that subgroup to see if there's something actionable. That and how like, maybe that experience intersects with something else, and those 2 things are intertwined and actionable to improve the service.'' (P8) ``what I would actually do in this situation is I would probably get go like into Python myself, and, like, find all these columns, and like, make all the neutral.'' (P10)
    % \item Participants express skepticism that the features and values shown in the tool are always representative and reliable due to correlation vs. causation. ``Maybe it's just pure random chances. Yeah, I mean, because models are random, like they? Probably it's probably just due to chance.'' (P9) being very satisfied with dep/arr time convenient: ``maybe people in these subgroups are not satisfied with other aspects.'' (P7)
% \end{enumerate}

\subsubsection{Curation: The Subgroup Map helps more experienced participants choose subgroups to prioritize.}
\label{sec:results-curation}

We observed that Curation actions, i.e. building a case (\ref{task:build-case}) and finding gaps (\ref{task:find-gaps}), were primarily done by participants with more familiarity with subgroup analysis. 
Nine out of 13 participants used the Subgroup Map and targeted search features to build understanding about the contents of the dataset and to choose which subgroups to analyze further.
Some (4 out of 9 who used the map) looked at the Distinguishing Features for each bubble to associate types of people with areas of the map: \textit{``I always expect... there's, say, a portion of the graph to be something like satisfied loyal customers.''} (P9)
Others (3/9) performed slice search within selected regions to characterize them using rules.
Furthermore, by plotting subgroups on the map and seeing how they were distributed, participants (4/9) noticed that some subgroups could be further divided into more detailed categories: \textit{``So you have like, multiple kinds of disloyal customers''} (P12).

Consistent with the Subgroup Map's goal of communicating overlap and coverage, participants (6/9) used the visualization to eliminate subgroups that covered very similar sets of instances on the plot: \textit{``They're just like, not satisfied with various aspects, but they fall in the same place''} (P12).
Others (3/9) used it to assess at-a-glance how many of the positive outcomes were covered by their selected subgroups, to decide when they had completed the task to their satisfaction.
For example, P13 began by selecting the top five subgroups, visualizing them in the Subgroup Map, then narrowing them down to capture the same population with just three groups (Fig. \ref{fig:airline-example}).
Two participants followed an iterative methodology (similar to \ref{task:find-gaps}, Find Gaps) of adding and removing subgroups from the map to cover as many shaded bubbles (positive outcomes) as possible, while conducting targeted search within the white bubbles (negative outcomes) to find subgroups to remove.
Participants who used the Subgroup Map to understand coverage found it particularly helpful:
\begin{quote}
\textit{``Without... the visualization, I would think I get the representative groups. But with it, I'm thinking, still... many errors are missing. So I think it's a good way to use the visualization, because it makes me think of if I really grasped the majority of the errors.''} (P11)
\end{quote}

% \begin{enumerate}
    % \item Participants use the subgroup map to spatialize their understanding of the different characteristics in the dataset. lasso selecting and finding subgroups within a selection ``Because they are close to each other, and I want to see like, what kind of people they will be.'' (P1) ``I didn't even realize age is a variable in this. We've not looked at age.'' (P10) ``I always expect, like, you know, there's say, a portion of the graph to be something like, you know, satisfied loyal customers, or something like that.'' (P9) ``so you have like, multiple kinds of disloyal customers'' (P12) 
    % \item Participants use the subgroup map to choose the slices they want to analyze further. `` they're just like, not satisfied with various aspects, but they fall in the same place'' (P12) ***``Because if without, without the image or the visualization, I would think I get the representative groups, but with with it, I'm thinking, still other still, many errors are missing. So I think it's a good, good way to use the visualizations, because it makes me think of if I really grasp the majority of the errors.'' (P11)
% \end{enumerate}

\subsubsection{Perceived Opportunities: Making Divisi more actionable and broadly accessible}
\label{sec:results-perceived-opps}
Participants envisioned many ways in which Divisi could support their current and prior projects, including better understanding clinical trial results (P4), analyzing test results in educational experiments (P8), and debugging image generation models using keywords in their caption text (P13).
They suggested ways to extend the exploratory subgroup analysis workflow to accommodate these use cases, such as measuring the statistical significance of subgroup differences (P7) or displaying metrics that were more relevant to the use case (P9).
Participants also wondered if Divisi's workflow could be applied to data types that were more difficult to slice by, such as images or representations of molecules (2/13).
These data types would likely require clever ways to create metadata features for rules that are both clear-cut and interpretable.

Participants considered how metadata features were defined in our user study tasks, and some wanted to adjust the feature definitions to help them mine better subgroups. 
For example, some participants (4/13) found rules with ``neutral'' rating values not very actionable, and they wanted to either exclude that feature value from subgroups or re-bin the ratings into ``satisfied'' and ``not satisfied.''
Others described wanting to remove features that they suspected would mostly contain spurious correlations, though they wanted to do so without injecting personal bias into the process: \textit{``I have my own mental heuristic, for which features... are very low signal-to-noise, and I'm trying to remove the noise''} (P6).
While participants could in theory have edited the code to create the grouping features within the Jupyter Notebook, it would be helpful to directly support this aspect of subgroup analysis within the tool.

Finally, several participants commented that the Subgroup Map (6/13) and the Ranking Functions (5/13) were initially difficult to understand.
These users suggested simplifying how subgroups were initially presented to them, such as by enabling only one ranking function at first (P4).
Those confused by the Subgroup Map commented that clearly indicating how the chart was generated would likely make it more trustworthy (P12) and clarify the relationship between bubbles and subgroups (P1).
While these features were largely perceived as useful (8/13) once participants were familiar with how they worked, more guidance could help those less experienced with subgroup analysis leverage them effectively.

% \begin{enumerate}
    % \item Participants want the tool to provide subgroup results that directly apply into their work, such as statistical tests. ``it's not involving the tests that I'm usually using or the metrics that I'm usually using, so I am not sure how to interpret the results academically.'' (P7) ``the numbers are there, and it's something that I can tangibly report.'' (P9)
    % \item Participants want to filter the subgroup space for actionability and simplicity, provided that they can do so without biasing their search. ``Online boarding of neutral... I don't know what neutral means.'' (P5) ``So basically, I want to get rid of this one type of travel and class. Actually, we can do nothing about that, like the class eco class is just normally or naturally bad. So if you want to pick class of eco, you will not, like, definitely not get that good service.'' (P11) ``I have my own mental heuristic, for which features, probably like don't are like very low signal to noise, and I'm trying to remove the noise'' (P6)
    % \item Participants find the searching and mapping features unintuitive without explanation. ``It's good to be a user to see this visualization. But how to interpret it to audience?'' (P3) ``I can't really interpret like say, why, say, for example, these 2 are close.'' (P9) ``I think that's really cool. I was just slightly lost'' (P2) ``for some people this could be difficult to handle, as you may not know what kind of score functionality you're using now, because it's a combination of different functions.'' (P1)
% \end{enumerate}
% 1. Analyses on the airline dataset
% 2. Perceptions and usage of features of the tool
% 3. How participants see subgroup discovery fitting in their workflow
% 4. Improvements to the tool