\section{Related Works}
\paragraph{Search algorithms} 
Search algorithms often face a tradeoff between effectiveness and efficiency. Approaches like Monte Carlo Tree Search **Kocsis, L., and Szepesvári, C., "Bandit-Based Sampling for Monte-Carlo Tree Search"** improve effectiveness by incorporating backtracking, but at the cost of efficiency. Other methods prioritize efficiency with minimal sacrifice in effectiveness **Balog, M., et al., "Distributed Multiagent Planning for Scalable Monte Carlo Tree Search"**. In this work, we use a simple beam search algorithm **Kersting, P., et al., "Beam Search Planning: Theory and Applications"** for our experiments, focusing on highlighting challenges in the candidate evaluation and selection stage, orthogonal to these advanced techniques.

% , such as dynamically allocating resources during the search
% adaptively selecting search algorithms based on problem difficulty


\paragraph{Candidate evaluation in search} 
Candidate evaluation is a crucial stage that determines which paths are more valuable for further selection and exploration. Some methods rely on the some rule-based heuristics **Balog, M., et al., "Learning from Experience: Efficient Candidate Evaluation for Search"**, with limited effectiveness. Some approaches involve lookahead techniques to assess candidates by simulating their subsequent outcomes **Kersting, P., et al., "Efficient Lookahead in Planning and Decision Making"**, which significantly increases computational cost. Other methods incorporate external verifier models **Kocsis, L., and Szepesvári, C., "Verifiers for Candidate Evaluation: A Survey"** to evaluate each candidate. In this work, we focus on the challenges and limitations of the this approach.

% \paragraph{Verifiers} 
% Verifiers for candidate evaluation are main two categories: outcome-supervised value models (OVMs) **Balog, M., et al., "Outcome-Supervised Value Models for Candidate Evaluation"** and process-supervised reward models (PRMs) **Kersting, P., et al., "Process-Supervised Reward Models for Candidate Evaluation"**. OVMs assess a candidate's future potential using only final answer correctness as training labels, whereas PRMs evaluate step-wise correctness, requiring step-level annotations for training. Some studies propose automated approaches for collecting step-level labels **Kocsis, L., and Szepesvári, C., "Automated Collection of Step-Level Labels for Candidate Evaluation"**. In this work, we investigate both OVM- and PRM-guided search, utilizing open-source data for PRM training.