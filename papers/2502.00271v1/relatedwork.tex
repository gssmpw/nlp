\section{Related Works}
\paragraph{Search algorithms} 
Search algorithms often face a tradeoff between effectiveness and efficiency. Approaches like Monte Carlo Tree Search~\cite{RAP23,tian24} improve effectiveness by incorporating backtracking, but at the cost of efficiency. Other methods prioritize efficiency with minimal sacrifice in effectiveness~\cite{rebase24}. In this work, we use a simple beam search algorithm~\cite{OVM23,AlphaMath24} for our experiments, focusing on highlighting challenges in the candidate evaluation and selection stage, orthogonal to these advanced techniques.

% , such as dynamically allocating resources during the search
% adaptively selecting search algorithms based on problem difficulty


\paragraph{Candidate evaluation in search} 
Candidate evaluation is a crucial stage that determines which paths are more valuable for further selection and exploration. Some methods rely on the some rule-based heuristics~\cite{deepseekprover15-24}, with limited effectiveness. Some approaches involve lookahead techniques to assess candidates by simulating their subsequent outcomes~\cite{Snell24,Wan24}, which significantly increases computational cost. Other methods incorporate external verifier models~\cite{OVM23,Snell24} to evaluate each candidate. In this work, we focus on the challenges and limitations of the this approach.

% \paragraph{Verifiers} 
% Verifiers for candidate evaluation are main two categories: outcome-supervised value models (OVMs)~\cite{OVM23} and process-supervised reward models (PRMs)~\cite{PRM24}. OVMs assess a candidate's future potential using only final answer correctness as training labels, whereas PRMs evaluate step-wise correctness, requiring step-level annotations for training. Some studies propose automated approaches for collecting step-level labels~\cite{Luo24,Math-Shepherd24}. In this work, we investigate both OVM- and PRM-guided search, utilizing open-source data for PRM training.