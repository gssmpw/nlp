\label{sec::ann}
In this section, we describe the methodology used to generate pupil position and blink annotations from Angelopoulos's dataset~\cite{angelopoulos2020event}. First, the annotations are computed using an automated process. These annotations are subsequently validated and, if necessary, corrected through manual verification.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\textwidth]{image/Polarity_frame.png}
  \caption{Examples of frames generated accumulating events at 200Hz}
  \label{fig:Polarity_frame}
\end{figure}

\subsection{Automatic annotation } \label{automatic_annotation }
A notable drawback of event cameras is the difficulty of directly applying traditional frame-based algorithms to their output data. This issue arises because event cameras generate data as a continuous stream of asynchronous events. Unlike frame-based cameras, event cameras only capture changes in individual pixels, without producing complete images. This limitation impacts the data visualization and the annotation process. Therefore, the first step in the automatic annotation pipeline is to generate RGB frames from the event stream. These frames possess all the characteristics of traditional camera images, allowing them to be visualized and annotated by both the automatic pipeline and the users.

\begin{figure}
\centering
\includegraphics[width=0.80\textwidth, trim=80px 19px 170px 0px, clip]{image/Pipeline_auto.pdf}
\caption{Schema of the automatic annotation pipeline. The system takes the frame generated by accumulating events as input and first predicts if there has been eye movement. If a saccade is detected, the pupil center is determined using a template matching strategy followed by RANSAC estimation.}
\label{fig:auto_annotation_pipeline}
\end{figure}

To generate the event frames, we aggregated events at a sampling rate of $200 Hz$ (i.e., a frame every $5ms$). This means that all events, which represent changing pixels, in a $5ms$ window are summed to reconstruct an RGB image of the eye.  This sampling rate was selected to capture all relevant eye movements. Indeed, saccadic movements typically span from $30 ms$ to $80 ms$, while microsaccadic movements occur within a $10 ms$ to $30 ms$ range~\cite{holmqvist_eye_movements}. The need for such a high sampling frequency to accurately capture all types of eye movements introduces substantial challenges in the annotation process. As the sampling frequency increases, there is a proportional increase in the volume of frames that require annotation, complicating the data processing workflow.

The result of the accumulation process are RGB frames where the only colored points are the events accumulated during the sampling period. The color of the points indicates the polarity of each event: green for positives and red for the negative ones. The output of this first phase of the pipeline is shown in Fig.~\ref{fig:Polarity_frame}. 


The next phase involves automatically generating the raw annotations for each frame. Due to the high frequency at which the data are processed, a significant portion of the frames contain limited information, with most colored pixels attributable to noise. This phenomenon occurs when the eye is nearly stationary; if no movement is detected by the sensor, no events are generated. In these scenarios, where only noise data are available, automatic annotation is problematic due to the lack of a clear pupil. In this scenario, manual inspection would only increase the effort required by the human operator without adding additional benefit to the final result.


%Then, we start by generating a base annotation using an automatic process, in order to reduce manual rework to a minimum. Most of the frames, at this frequency, contain noise and not eye movements, so it would not be feasible to manually inspect these frames. 

%The automatic annotation pipeline is shown in Fig.~\ref{fig:auto_annotation_pipeline}, and it's divided into three major steps:
%\begin{enumerate}
%    \item We need to detect if either a saccade or blink is ongoing.
%    \item If a saccade is ongoing, we find a tentative eye center using a pattern-matching methodology.
%    \item We further improve the center using RANSAC fit on the events contained in a ROI centered on the tentative center.
%\end{enumerate}
%These three steps utilize the method described in~\cite{mentasti2024event}. The saccade classification is performed using a running mean with the same parameters outlined in the article. Similarly, the pattern-matching methodology employs a slightly improved kernel compared with the one presented in that study. Although the RANSAC fit remains the same, it uses a higher number of iterations in this scenario: 1000 iterations.


The automatic annotation pipeline is shown in Fig.~\ref{fig:auto_annotation_pipeline}, and it is divided into three major steps inspired by the method described in:~\cite{mentasti2024event}:
\begin{itemize}
\item \textbf{Eye Movement Detection}: In this step, we need to detect if either a saccade or a blink is ongoing. We can detect the presence of a saccade using a threshold technique. If the events in the frame are more than 150, there is a saccade; otherwise, it is not a saccade. Also, the first time that a frame is detected as a saccade, it is labeled as \textit{SACCADE\_START}. Similarly, the frame is labeled as \textit{SACCADE\_END} if it is the last frame before switching to the frames without detected saccades.
\item \textbf{Match Template:} If a saccade is ongoing, the match template step finds a tentative eye center using a pattern-matching methodology. To do this, we use a 2D convolution similar to~\cite{mentasti2024event}, but with a slightly improved kernel compared with the one presented in the original work. In particular, to improve system performance, we employ only 8 different templates that represent different movement directions. These kernels are applied to the polarity images (i.e., the frames reconstructed integrating events data). The output of this phase is shown in Fig.~\ref{fig:convolutions}. Then, each result is multiplied to create heatmaps, identifying movement direction. The highest-scoring template indicates the motion direction; a bounding box, representing the Region Of Interest (ROI) of the frame, is built centered at the maximum point of the heatmap. The result is visible in~\ref{fig:tentative_center}.
\item \textbf{RANSAC Estimation}: After the match template step, we execute the Random Sample Consensus (RANSAC) algorithm~\cite{fischler1981random}, considering only the events inside the ROI detected earlier. In particular, we employ RANSAC to fit an ellipse described using the equation of a generic conic with the f parameter fixed to -1. Although the RANSAC fit follows the method described in~\cite{mentasti2024event}, it uses 1000 iterations in this scenario to prioritize reliable results over fast execution. The final result is visible in~\ref{fig:ransac_fit}.
\end{itemize}

\begin{figure}[t]
    \centering
    
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/convolutions_r_cropped.png}
        \caption{Result of a convolution from match template phase, the light blue zone indicates where the pupil can be located.}
        \label{fig:convolutions}
    \end{subfigure}
    \hfill % optional, for better spacing
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/tentative_center_cropped.png}
        \caption{Expected pupil center after the match template step.}
        \label{fig:tentative_center}
    \end{subfigure}
    \hfill % optional, for better spacing
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/ransac_fit_cropped.png}
        \caption{Estimated pupil center after the RANSAC estimation step.}
        \label{fig:ransac_fit}
    \end{subfigure}
    
    \caption{Results from different operations performed during the pupil center localization. As we can see, the RANSAC estimation step increases the accuracy of the estimation made by the match template step.}
    \label{fig:predictor_3g}
\end{figure}


\subsection{Fine Tuning Annotation using Human Annotators} \label{fine_tune_annotation}

After generating and automatically annotating each frame, as described in the previous section, the labels are inspected by human annotators with expertise in eye-tracking and event cameras to validate and correct the annotations.

The annotation fine-tuning process is divided into two parts. The first part aims to assign each frame a specific label describing the state of the eye, such as a saccade or blink. In this stage, each annotator reviews the automatic annotations generated in the previous step. Annotators are presented with each frame that contains a number of events exceeding a pre-set threshold, which varies between users. This method excludes all frames with a low number of events, where the eye's state is not clearly defined, and the human annotator would be unable to provide accurate additional information.

Using a simple keyboard shortcut-based interface, each annotator is able to modify the eye center's position, to change the saccade status (\textit{SACCADE\_START}, \textit{SACCADE\_IN\_PROGRESS}, \textit{SACCADE\_END}) and select the current blink status (\textit{BLINK\_STATUS}, \textit{BLINK\_IN\_PROGRESS}, \textit{BLINK\_END}). Having a base annotation greatly speed-up the review process, as for many frames the base annotation itself is correct.

In the second stage, the quality of the labels is evaluated using another custom-made tool. Considering the high frequency of annotations, we expect extremely small eye movements to occur between frames. To leverage this, we introduced a metric based on the difference in eye position between consecutive frames. A threshold for this distance is set to identify potential label anomalies, under the assumption that pupil movement should not exceed this distance within a $5 ms$ interval. The output from this phase is displayed as an interactive plot (see Fig.~\ref{fig:anomaly_detection}). Reviewers can then zoom into specific plot regions where the threshold is breached and directly jump to the corresponding labels for correction.



\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\textwidth]{image/anomaly_detection_detail.png}
  \caption{Interactive plot for annotation correction. Delta in x,y from an eye position to the next is shown in blue and red. Saccades are marked in violet (rising edge when the saccade starts, falling edge when the saccade ends). Blinks are indicated similarly in orange. At the end of the sequence, a possible anomaly is marked in green.}
  \label{fig:anomaly_detection}
\end{figure}


This second step is performed iteratively until the plot no longer exhibits any significant anomalies. Some peak distances may persist due to eye movements occurring during blinks. The presence of these peaks is largely attributed to different physiological mechanisms; for example, downward eye movements during blinks are well-documented in the literature~\cite{Khazali2017}. Moreover, saccades and blinks can be simultaneous.

