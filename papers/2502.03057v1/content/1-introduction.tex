Eye tracking is a technology that measures and records the eye movements and positions of an individual~\cite{andrychowicz2018basic}, providing valuable insights into visual attention and cognitive processes~\cite{Popa2015Reading}~\cite{Skaramagkas2021Review}. This technology has profound implications for various applications, ranging from user experience research~\cite{Burger2018Suitability} and medical diagnostics~\cite{Liu2021The} to advanced human-computer interaction systems~\cite{Bulling2010Toward}~\cite{Zhang2017Eye}. Moreover, with the advent and rapid diffusion of virtual reality devices~\cite{angelov2020modern}, augmented reality systems~\cite{arena2022overview}, and smart eyewear, eye-tracking technology has become a central component in these systems to enhance the user experience and provide a more immersive and interactive environment.

Thanks to the proliferation of smart wearables and smart eyewear in recent years~\cite{spil2019adoption}, research on this topic has witnessed a significant boost. In particular, in recent years, we have seen rapid growth in the availability of commercial eye-tracking technologies~\cite{KassnerPateraBulling}. These devices, equipped with advanced sensors, enable continuous and unobtrusive monitoring of eye movements in real-world settings. This has opened up new possibilities for applications such as biometric authentication, emotion recognition, and user experience optimization.

Despite the recent advancements, these new technologies still struggle to provide real-time information, or they require heavy computational power and high-end PCs to process the data~\cite{wan2021robust}. Recent advancements in computer vision have significantly enhanced the accuracy of eye-tracking technology. In particular, machine learning and deep learning approaches have enabled more accurate and robust detection and tracking of eye movements, even in challenging conditions such as varying lighting and head movements~\cite{zhu2007novel}. Techniques such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have been instrumental in improving the precision and speed of eye tracking systems~\cite{fuhl2016pupilnet}~\cite{santini2018pure}. Nevertheless, the improved accuracy has come at the cost of using powerful GPUs to run the deep-learning models, making the whole eye-tracking system more resource-demanding and distant from a slim eye-wear system, where all the computation is performed directly on the device.

The growing accuracy in the eye tracking process and the increase in computational requirements are related to how traditional eye tracking has been performed, using cameras looking at the user's eye. Traditionally, an eye-tracking pipeline is divided into a series of refinement blocks that gradually find the pupil in the image, then the center, and track it through time. It is then expected that an increase in accuracy can lead to higher computational requirements. For this reason, in recent years, alternative solutions in eye tracking have been proposed, from non-camera based like microelectromechanical systems (MEMSs) to scan the eyes~\cite{zafar2023investigation} to photodetector-based solutions~\cite{crafa2024towards}. Other solutions focused on new and innovative camera systems, particularly event-based cameras~\cite{gallego2020event}, which, unlike traditional frame-based sensors, return only the changing pixels from one frame to the next, in an asynchronous way. This feature can be exploited to detect only the pupil in the frame and drastically increase the performance of the pupil detector while maintaining low computational power.

 

In particular, event-based eye tracking has proved to be a promising solution. However, annotated data with proper ground truth is required to validate eye-tracking algorithms and train deep-learning models. Camera-based eye-tracking datasets are fairly common and have been recorded for many years~\cite{winkler2013overview}. In contrast, to the best of our knowledge, the number of datasets for these new sensors is extremely limited due to the novelty of the event-based approach. Indeed, only four datasets are available~\cite{angelopoulos2020event}~\cite{bonazzi2024retina}~\cite{wang2024eventbasedeyetrackingais}~\cite{eveyepaper}, but each of them presents some limitations that are further discussed in the following sections.

\begin{comment}Indeed, only two datasets are available~\cite{angelopoulos2020event}~\cite{bonazzi2024retina}. A third one exists ~\cite{wang2024eventbasedeyetrackingais}, yet was collected without IR-pass filter so it contains events of object reflections mixed with eye movements.

In Angelopoulos, the dataset has limited ground truth, due to the difficulties in generating a pupil-level annotation from the sparse event data. Indeed, Angelopoulos's dataset provides as ground truth the looked point on a screen, not the pupil position in the image frame. This type of annotation has been widely used for many years in the eye-tracking field~\cite{holmqvist_eye_movements} but introduces additional calibration and nonlinearities in the system, which makes a proper evaluation of the pure pupil detection and tracking algorithm less accurate.
\end{comment}

For this reason, in this work, we present an improved version of the original dataset proposed by Angelopoulos in~\cite{angelopoulos2020event}. In particular, we present a pipeline for semi-automatic annotation of event data for pupil detection and tracking. We also provide the scientific community with the generated pupil position for Angelopoulos's dataset at 200Hz. The computed annotations are made publicly available at \footnote{\url{https://github.com/AIRLab-POLIMI/event_based_gaze_tracking_gt}}.

This work is organized as follows: Section~\ref{sec::soa} presents the current state of the art in eye-tracking, with a specific focus on the event-based scenario and the datasets available. Subsequently, Section~\ref{sec::ann} describes our proposed pipeline for automatic annotation generation from event data. In Section~\ref{sec::eval}, we highlight key statistics regarding the size of the annotations generated. Finally, Section~\ref{sec::end} offers concluding remarks and summarizes the findings of this paper.