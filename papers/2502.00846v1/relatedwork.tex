\section{Related Work}
\paragraph{Robust Frequentist Federated Learning} In the frequentist setting, building on the seminal paper of \citet{mcmahan2017}, many approaches have aimed at mitigating challenges in FL, such as robustness to adversarial servers through secure aggregation \citep{chen2022}, to stragglers \citep{tziotis2023}, heterogenous data in out--of--distribution generalisation \citep{tenison2023}, heterogeneous and asynchronous clients \citep{fraboni2023}, or finding weaknesses in communications \citep{zhu2019, zhao2023}. 
More recently, %
work on robust server aggregations achieves robustness against Byzantine clients that aim to deteriorate model performance \citep{allouah2024, bao2024}. However these do not allow principled uncertainty quantification.

\paragraph{Federated Bayesian Inference}
Federated and distributed Bayesian methods aim to approximate the posterior as if it had been computed with the data of all clients available at a central server. 
Early work on distributed Bayesian inference includes Bayesian opinion pools \citep{genest1984, carvalho2023}, and the Bayesian Committee machine \citep{tresp2000}, which aim to find a consensus among a collection of Bayesian beliefs. 
Works that aim to operationalise this in the distributed setting, where data is split IID across clients, include Expectation Propagation \citep{minka2001b, opper2005, hasenclever2017, vehtari2020}, and consensus based Monte Carlo \citep{scott2016}.
In the Federated setting this assumption is often violated, as data is not split homogeneously and IID across participating devices. 
From this perspective, most approaches to Bayesian FL can be categorised into finding an approximate posterior through variational inference \citep{corinzia2021, ashman2022, kassab2022,heikillae2023, hassan2024, vedadi2024},  Markov Chain Monte Carlo \citep{al-shedivat2021,mekkaoui2021,kotelevskii2022, guo2023, hasan2024}, or directly learning a Bayesian neural network \citep{yurochkin2019, zhang2022} or Gaussian Process \citep{achituve2021}. However, none of these  are robust to contamination and model misspecification.



\paragraph{Robust Bayesian Inference} 
Although the existing Bayesian FL methods address some of the challenges of federated learning, such as communication constraints and data heterogeneity, they still aim to approximate the Bayesian posterior, which in itself is a flawed objective under model misspecification \citep{walker2013, berk1966, bernardo2000}.
In the global, non-federated case, several methods have been proposed to combat misspecification in the Bayesian setting \citep{gruenwald2012}, with the most promising direction being Generalised Bayesian Inference \citep{hooker2014, bissiri2016, ghosh2016a, jewson2018, miller2021, alquier2021,jeremias2022, matsubara2022}. In this work we capitalise on this front and bring robustness to model misspecification in the federated setting.