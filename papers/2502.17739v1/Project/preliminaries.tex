\section{Preliminaries}
GNNs are machine learning models designed to process graph-structured data, where entities (nodes) are connected by relationships (edges). They are widely used in applications such as social network analysis, molecular chemistry, and recommendation systems. GNNs leverage a fundamental mechanism called \textbf{message passing}, in which nodes iteratively update their representations by aggregating information from their neighbors.

The message passing process in GNNs can be viewed as a diffusion of information through the graph. At each layer $k$, the representation of a node $v$ is updated as follows:
\begin{equation*}
\mathbf{h}_v^{(k)} = \text{UPDATE}\left( \mathbf{h}_v^{(k-1)}, \text{AGGREGATE}\left( \left\{ \text{M}\left( \mathbf{h}_v^{(k-1)}, \mathbf{h}_u^{(k-1)}, \mathbf{e}_{vu} \right) : u \in \mathcal{N}(v) \right\} \right) \right)
\end{equation*}
where $ \mathbf{h}_v^{(k)} $ denotes the representation of node $ v $ at layer $ k $, $ \mathcal{N}(v) $ is the set of neighbors of node $ v $, and the aggregation function $ \text{AGGREGATE} $ combines information from the neighboring nodes $ \{ \mathbf{h}_u^{(k)} : u \in \mathcal{N}(v) \} $ according to a specified rule (e.g., sum, mean, or weighted sum).  The update rule for the node's representation, denoted by $ \text{UPDATE} $, governs how the aggregated information and the previous node representation $ \mathbf{h}_v^{(k-1)} $ are combined to produce the new representation $ \mathbf{h}_v^{(k)} $. The function $ \text{M} $ specifies how the features of node $ v $ and its neighbors interact, potentially incorporating edge features such as $ \mathbf{e}_{vu} $.

Each GNN layer corresponds to one \textit{hop} of information propagation, meaning a GNN with $k$ layers can capture information from nodes within $k$-hops of a target node, enabling it to produce expressive representations of graph features.


However, GNNs still face limitations in distinguishing certain non-isomorphic graphs. One way to construct such counterexamples is through \textbf{k-hop isomorphism}. Two graphs $ G_1 = (V_1, E_1) $ and $ G_2 = (V_2, E_2) $ are \textbf{k-hop isomorphic} if their $ k $-hop induced subgraphs are isomorphic.


\begin{definition}[\textbf{$k$-Hop Isomorphism}]
Let $ G_1 = (V_1, E_1) $ and $ G_2 = (V_2, E_2) $ be two graphs. We say that $ G_1 $ and $ G_2 $ are \textbf{k-hop isomorphic} if there exists a bijection $ f: V_1 \to V_2 $ such that for all $ v_i \in V_1 $, the induced subgraphs $ G_1[\mathcal{N}_k(v_i)] $ and $ G_2[\mathcal{N}_k(f(v_i))] $ are isomorphic, where $ \mathcal{N}_k(v) $ is the set of nodes within $ k $-hops of node $ v $.
\end{definition}

This result follows directly from the fact that the message-passing mechanism aggregates information across identical $k$-hop neighborhoods, leading to indistinguishable representations for $k$-hop isomorphic graphs.

\begin{theorem}[\textbf{Weight Consistency under $k$-Hop Isomorphism in GNNs}]
Given two graphs $ G_1 $ and $ G_2 $ that are $ k $-hop isomorphic, a GNN with $ k $ layers cannot distinguish between them. Formally, for any GNN $ f_{\theta} $ with $ k $ layers, we have:
\[
f_{\theta}(G_1) = f_{\theta}(G_2),
\]
where $ f_{\theta}(G) $ represents the learned representation of graph $ G $.
\end{theorem}


To address the strictness of $ k $-hop isomorphism, we introduce a relaxed notion called \textit{$ k $-hop similarity}, which allows for variations in edge structure of the $ k $-hop neighborhoods while preserving the set of nodes.

\begin{definition}[\textbf{$k$-Hop Similarity}]
Two graphs $ G_1 = (V_1, E_1) $ and $ G_2 = (V_2, E_2) $ are \textbf{$ k $-hop similar} if there exists a one-to-one mapping $ \phi $ between their $ k $-hop neighborhoods such that:
\[
\mathcal{N}_k(v_1) \sim \mathcal{N}_k(v_2), \quad \forall v_1 \in G_1, v_2 \in G_2,
\]
where $ \sim $ denotes an isomorphism of node sets within the $ k $-hop neighborhoods without requiring exact edge alignment.
\end{definition}

By relaxing the assumption of strict isomorphism and instead considering $k$-hop similarity, we aim to investigate whether GNNs will continue to be unable to distinguish between these graphs, as proposed in Conjecture \ref{conjecture: weight_consistency}, and how this influences their performance, particularly in relation to emerging phenomena such as oversmoothing.

In the following sections, we will first present the theoretical and empirical framework designed to validate or refute our conjecture. Based on the outcomes, we will then examine the implications of our findings for GNN performance, particularly with respect to the phenomenon of oversmoothing.


