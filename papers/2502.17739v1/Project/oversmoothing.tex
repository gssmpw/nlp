\section{Oversmoothing through Graph Topology}
In the previous section, we empirically established that training the same GNN model architecture on a graph and its $k$-hop similar graph results in consistent predictions (Conjecture \ref{conjecture: weight_consistency}). In this section, we explore how this helps explain the oversmoothing phenomenon in deep GNNs.

To begin, consider a connected graph. For a sufficiently large value of $k$, the $k$-th power of this graph becomes a complete graph.

\begin{proposition}[\textbf{$k$-th Power of a Connected Graph}] The $k$-th power of a connected graph is a complete graph. \end{proposition}

This means that when training a very deep GNN on a connected graph, the effect of training is equivalent to training it on a complete graph. In the context of the GNN message-passing scheme, training on a complete graph causes all node features to be mixed together. As a result, node representations converge to identical values, which is the essence of oversmoothing. This can be formally expressed as:
\begin{theorem}[\textbf{$|V|$-hop Similarity}]
\label{thm: vhop}
Let $f_\theta$ denote a deep GNN parameterized by $\theta$, and let $G = (V, E)$ be a connected graph, where $V$ is the set of vertices and $E$ is the set of edges. Suppose $f_\theta$ is trained on graph $G$. We conjecture that training $f_\theta$ on $G$ is approximately equivalent to training it on the complete graph $K_{|V|}$, provided that the depth of the GNN is sufficiently large. Formally, if $f_\theta^{G}$ denotes the function induced by the GNN trained on graph $G$, and $f_\theta^{K_{|V|}}$ denotes the function induced by training the GNN on the complete graph, then for a sufficiently deep GNN: $f_\theta^{G} \approx f_\theta^{K_{|V|}}.$
\end{theorem}

This explains why deep GNNs often suffer from oversmoothing, where node features become indistinguishable as the network depth increases.

In practical settings, however, many real-world graphs are not fully connected; instead, they exhibit community structures with distinct connected components. In these cases, oversmoothing will occur within each connected component or cluster. Specifically, Theorem \ref{thm: vhop} can be generalized to arbitrary graph structures, demonstrating that the equivalence property holds independently within each connected component. This is formalized in Theorem \ref{thm:compo_hop}, which explains how oversmoothing manifests within each cluster.

\begin{theorem}[\textbf{Component-wise $|V_i|$-hop Similarity}]
\label{thm:compo_hop}
Let $f_\theta$ denote a deep GNN parameterized by $\theta$, and let $G = (V, E)$ be a graph with k connected components ${G_i = (V_i, E_i)}_{i=1}^k$, where $\bigcup_{i=1}^k V_i = V$ and $\bigcup_{i=1}^k E_i = E$. Suppose $f_\theta$ is trained on graph $G$. We conjecture that training $f_\theta$ on $G$ is approximately equivalent to training it on a graph $K_G$ that is the union of complete graphs, where each complete graph corresponds to a connected component of $G$. Formally, let $K_G = \bigcup_{i=1}^k K_{|V_i|}$, where $K_{|V_i|}$ is the complete graph on vertex set $V_i$. If $f_\theta^G$ denotes the function induced by the GNN trained on graph $G$, and $f_\theta^{K_G}$ denotes the function induced by training the GNN on $K_G$, then for a sufficiently deep GNN: $f_\theta^G \approx f_\theta^{K_G}$.
\end{theorem}

This suggests that, in graphs with a community structure or multiple disconnected components, oversmoothing may not be a global issue but could manifest separately within each community. As a result, even shallow GNNs operating on these graphs might suffer from oversmoothing within individual components, which can undermine the ability of the model to capture meaningful distinctions between nodes. In other words, \textbf{\textit{GNNs that rely on the message-passing scheme are fundamentally constrained by the topology of their input graphs}}, which dictates both their expressiveness and their susceptibility to oversmoothing.