\section{Methods}
The two major challenges we face are (1) the large feature space due to the multimodal, heterogeneous patient risk factors; (2) the label imbalance due to the low prevalence of CA in pediatric CICU patients.  
Our proposed model, \modelname, is a multimodal transformer late-fusion model, which utilize both raw structured EHR features (tabular) and derived textualized EHR features (textual) as input. As can be seen in Fig.~\ref{fig:model}, \modelname ~consists of three modules: tabular-transformer $\mathcal{T}_{tab}$, pre-trained textual-transformer $\mathcal{T}_{txt}$, and fusion-transformer $\mathcal{T}_{fus}$. The tabular-transformer is corresponding to obtain latent representation from patient's raw EHR data, denoted by $\mathbf{h}_{tab}=\mathcal{T}_{tab}(\mathbf{x}_{t\leq \tau})$. The textual-transformer is corresponding to obtain latent representation from patient's textualized EHR data, denoted by $\mathbf{h}_{txt}=\mathcal{T}_{txt}(g(\mathbf{x}_{t\leq \tau}))$ where $g(\cdot)$ denotes the textualization function for EHR. After obtaining latent representations from two views, we utilize the fusion-transformer to predict the likelihood of cardiac arrest, denoted by $\hat{y}=\mathcal{T}_{fus}(\mathbf{h}_{tab},\mathbf{h}_{txt})$.

\begin{figure*}[ht!]
\centering
\includegraphics[width=\linewidth]{images/fusion_arch.pdf}
\caption{Model Architecture of \modelname.}
\vspace{-0.4cm}
\label{fig:model}
\end{figure*}

\subsection{Tabular Transformer}
Tabular models (\textit{e.g.} XGBoost~\cite{chen2016xgboost}, LightGBM) have demonstrated excessive performance in EHR-based risk predictions. Here, we also employ a tabular view to handle the numerical and categorical risk factors that widely exist in the EHR. We follow the common practice to preprocess these tabular features, where numerical ones are passed through without change and categorical ones are encoded as monotonically increasing integers. 
For the static risk factors $\mathbf{x}^{(s)}$, we keep them as the original format. For the temporal risk factors, aggregation process is needed to accommodate the input format requirements. Therefore, 
\begin{equation}
    \mathbf{x}^{(t)}_{\text{agg}} = \phi(\mathbf{x}^{(t)}_{0:\tau}),\label{eq:agg}
\end{equation}
where $\phi: \mathbb{R}^{d_t \times \tau}\rightarrow \mathbb{R}^{d_t}$ denotes the aggregation operator that maps the time-series features into tabular features. A wide range of aggregation operation can be used for $\phi$, and we opt to $LAST(\cdot)$ which keeps the last observed element from the whole time-series. The input of tabular-transformer $\mathcal{T}_{tab}$ is $\mathbf{x}_{t\leq\tau}=(\mathbf{x}^{(s)},\mathbf{x}^{(t)}_{\text{agg}})$ after applying Eq.~\eqref{eq:agg}. 

We adapt a tabular feature oriented Transformer~\cite{huang2020tabtransformer,gorishniy2021FT-Trans} variation as our tabular view. In a nutshell, the numerical and categorical inputs are first transformed to dense embeddings and then feed into a stack of Transformer layers. The dense embedding for $i$-th factor $x_{(i)} \in \mathbb{X}_i$ of $\mathbf{x}_{t\leq\tau}$ is computed by
\begin{equation}
    \mathbf{e}_{(i)}=\xi_{(i)}(x_{(i)})+\mathbf{b}_{(i)},
\end{equation}
where $\mathbf{e}_{(i)}(\cdot) \in \mathbb{R}^d$ is $i$-th the transformed dense embedding, $\xi_{(i)}: \mathbb{X}_{(i)} \rightarrow \mathbb{R}^d$ is the $i$-th transformation function, and $\mathbf{b}_{(i)} \in \mathbb{R}^d$ is $i$-th feature transformation bias. 
Specifically, we implement $\xi_{(i)}(\cdot)$ as the element-wise multiplication 
\begin{equation}
\xi_{(i)}(x_{(i)}^{num})=x_{(i)}^{num}\cdot \mathbf{W}_{(i)}^{num}
\end{equation}
with the learnable vector $\mathbf{W}_{(i)}^{num} \in \mathbb{R}^d$ for numerical format features, and $\xi_{(i)}$ as the categorical embedding lookup 
\begin{equation}
\xi_{(i)}(x_{(i)}^{cat})=\mathbf{W}_{(i)}^{cat} \mathbb{1}(x_{(i)}^{cat})
\end{equation}
with one-hot encoding function $\mathbb{1}(\cdot): \mathbb{X}_{(i)}\rightarrow \mathbb{R}^d$ and the lookup table $\mathbf{W}_{(i)}^{(cat)} \in \mathbb{R}^{c_{i}\times d}$. 
Overall, 
\begin{equation}
    \mathbf{e}_{tab}= stack(\mathbf{e}_{(1)}^{num},\dots,\mathbf{e}_{(|\mathbf{x}^{num}|)}^{num},\mathbf{e}_{(1)}^{cat},\dots,\mathbf{e}_{(|\mathbf{x}^{cat}|)}^{cat}),
\end{equation}
where $\mathbf{e}_{tab} \in \mathbb{R}^{\mathbf{|x|}\times d}$. Then, L standard general Transformer layers $\mathcal{F}_1, \dots, \mathcal{F}_L$ are applied 
\begin{equation}
\mathbf{h}_k=\mathcal{F}_{k}(\mathbf{h}_{k-1})
\label{eq:multi_transformer}
\end{equation}
with $\mathbf{h}_{0}=\mathbf{e}_{tab}$ and $\mathbf{h}_L\in\mathbb{R}^{h_{tab}}$. Please refer to the original paper~\cite{vaswani2017attention} for technical details of the multi-head self-attention of transformer layer $\mathcal{F}$. 

\subsection{Textual Transformer}
The drawback of tabular view is that the aggregation process simplifies the time-series risk factors. We propose a textual view to fully capture the trend and change of these time-series risk factors by presenting their value ranges. Moreover, the static risk factors can also be effectively captured by directly presenting them in the textual format. A pre-trained textual-transformer $\mathcal{T}_{txt}$ is employed to obtain the latent embedding of the textualized EHR data $\mathbf{D}=g(\mathbf{x}_{t\leq \tau})$. 
In general, the textualized representation of $i$-th factor $x_{(i)} \in \mathbb{X}_i$ of $\mathbf{x}_{t\leq\tau}$ is computed by
\begin{equation}
\mathbf{D}_{(i)}=g_{(i)}(\mathbf{x}_{(i)}),
\end{equation}
where $g_{(i)}(\cdot)$ is the i-th textualization function, and $D_{(i)}$ denotes a sequence of length $|D_{(i)}|$ textual tokens $\{d_1,d_2,\dots,d_{|D_{(i)}|}\}$. 
Specifically, we implement $g_{(i)}(\cdot)$ using the following template for static feature $x_{(i)} \in \mathbb{X}_{(i)}$
\begin{equation}
g_{(i)}(x_{(i)})=\text{factorName}(x_{(i)})\doubleplus \text{``:"}\doubleplus x_{(i)}
\end{equation}
with $\text{factorName}(\cdot)$ denotes retrieving the factor name of $x_{(i)}$, and $\doubleplus$ denotes text concatenation function. For instance, $g_{(i)}(x_{(i)})=\text{``Gender: Female"}$ when $x_{(i)}=\text{``Female"}$ indicating the patient's gender.
Moreover, we implement $g_{(i)}(\cdot)$ using the following template for variable length time-series feature $\mathbf{x}_{(i),0:\tau}$  
\begin{multline}
g_{(i)}(\mathbf{x}_{(i),0:\tau})=\text{factorName}(\mathbf{x}_{(i)})\doubleplus \text{``:"} \doubleplus\\
\left\{
    \begin{array}{ c l }
    \textrm{set}(\mathbf{x}_{(i),0:\tau}), & \quad \textrm{if } \mathbf{x}_{(i)} \textrm{ is categorical}\\
    \min\text{--}\max(\mathbf{x}_{(i),0:\tau}), & \quad \textrm{otherwise},\\
    \end{array}
\right.\label{eq:ts_textual}
\end{multline}
where we keep all distinct values of each categorical time-series risk factor, and value min-max range of each numerical time-series factor. As an illustration, for categorical time-series risk factor \textit{skin color}, the textualized result is ``Skin Color: Pink; Pale.'' which covers all recorded values. Similarly, for numerical time-series risk factor \textit{body temperature}, the textualized result is ``Body Temp: 91.4--98.4.''. 
With the help of proposed textualization process, the heterogeneous multimodal EHR data is represented into a unified textual format which can be effectively handled by textual transformer, capturing dynamic trend of time-series. 

Pre-trained on massive textual corpus that covers public EHR databases, transformer-based models have shown promising in adults-centered risk prediction tasks~\cite{lu2024UQFM,chen2024adapting,shi2024ehragent}.
We employ a standard bidirectional encoder-only transformer (\textit{i.e.}, BERT~\cite{kenton2019bert}) for the pediatric cardiac arrest prediction, since it essentially is a sequence prediction problem after textualization of EHR. Follow the common practice, we add a special sequence token ``[CLS]'' at the beginning of the converted text, which is used as the text-view representation of one patient's EHR, 
\begin{equation}
\mathbf{h}_{BERT} = \text{BERT}(\text{[CLS]}, \mathbf{D}),
\end{equation}
where $\mathbf{h}_{BERT}\in \mathbb{R}^{(|\mathbf{D}|+1)\times d_{txt}}$ and $\text{BERT}(\cdot)$ denotes the employed bi-directional transformer model. It is worth noting that BERT here is different from the multi transformer layer in Eq.~\eqref{eq:multi_transformer}, since $\text{BERT}(\cdot)$ contains additional word embedding layer and its all learnable parameters are pre-trained. We use the ``[CLS]'' corresponding embedding as the text-view embedding of textualized EHR, thus $h_{txt}=h_{BERT}[0,:] \in \mathbb{R}^{d_{txt}}$.
% add special design for lab results: only keep abnormal.
One technical challenge for our textual transformer module is the maximum sequence length. To accommodate with that constraint, we further polish the textualization function $g_{}(\cdot)$ for time-series laboratory test results. Inspired by the clinical decision making process that paying more attentions to abnormal findings~\cite{jung2009clinical}, we filter out laboratory results that fall into the normal reference range and only keep the names of abnormal ones. Therefore, an example of textualized laboratory results can be ``(High) Sodium; Potassium. (Low) Glucose.''.
Furthermore, we add section headers, including ``Demographics'', ``Vitals'', ``Assessments'', ``Labs'' and ``Medications'',  for each covered EHR table, as can be seen in the toy example of Fig.~\ref{fig:model}. These above ad-hoc design aims to make the textualized EHR more readable and concise for the textual-transformer.

\subsection{Fusion Transformer}
After obtain the view-specific dense representation of patient EHR from tabular-transformer by $\mathbf{h}_{tab}=\mathcal{T}_{tab}(\mathbf{x}_{t\leq \tau})$ and textual transformer by $\mathbf{h}_{txt}=\mathcal{T}_{txt}(g(\mathbf{x}_{t\leq \tau}))$, we utilize a late-fusion strategy~\cite{shi2021multimodal,lu2023mug} to calculate the final probability of the pediatric patient's cardiac arrest onset. 
We still use the powerful Transformer architecture as the major technical component, with a concatenation operation to pool the two representations
\begin{equation}
\hat{p}=sigmoid(W(\mathcal{F}_{1:M}(\mathbf{h_{tab}},\mathbf{h_{txt}}))+b),
\end{equation}
where $\hat{p} \in [0, 1]$ denotes the predicted probablity of CA onset, $\mathcal{F}_{1:M}(\cdot)$ denotes the multiple transformer layers, and $\mathcal{W}(\cdot)+b$ denotes the last prediction layer.  

The focal loss~\cite{ross2017focal} is used to train the three modules of \modelname to deal with the class imbalance (CA incidence=3.1\%)
\begin{equation}
FL(p_{true})=-(1-p_{true})^{\gamma}\log(p_{true}),
\end{equation}
where
\begin{equation}
p_{true}=
\left\{
    \begin{array}{ c l }
    \hat{p}, & \quad \textrm{if } y=1,\\
    1-\hat{p}, & \quad \textrm{otherwise},\\
    \end{array}
\right.
\end{equation}
and $\gamma$ is a hyperparameter focusing on reduce the loss contribution from well-classified examples ($p_{true} \approx1$).

% a late-fusion model
% where separate neural operations are conducted
% on each data type and extracted high-level representations are aggregated near the output layer.
% Specifically, MLPs are used for tabular modality, and transformers are used for text and image
% modalities. After that, dense vector embeddings
% from the last layer of each network are pooled
% into one vector, and the final prediction is obtained via an additional two-layer MLP
% concat is used as default.
