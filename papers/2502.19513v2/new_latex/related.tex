\section{Related Work}
\label{sec:related}
\paragraph{Self-Supervised Learning.}
Self-supervised learning (SSL) has become a widely adopted method in various domains, including computer vision \citep{chen2020simple,he2020momentum, he2022masked} and natural language processing \citep{devlin2018bert,radford2019language}.
The standard self-supervised learning pipeline consists of two stages: the self-supervised learning stage to learn rich feature representations and the supervised learning stage to adapt to downstream tasks. 
\mixtraining integrates several self-supervised learning epochs and supervised learning epochs into a new mixtraining phase, featuring a smooth transition between two objectives (for better accuracy) and an optimized computation allocation.

\paragraph{Efficient Training.}
Various methods have been proposed to reduce computation costs in model training, including
model compression~\cite{han2015deep}, pruning~\cite{han2015learning}, parameter-efficient strategies~\cite{hu2021lora} and data-efficient strategies~\cite{yao2022nlp, mindermann2022prioritized, bengar2021reducing,wang2023data}. 
These methods focus on a different direction in the compute-performance trade-off by reducing the compute with minimal performance loss.
\mixtraining can be 
integrated with these approaches to further improve its efficiency.

\paragraph{Other related work.}
Exploring the synergy between self-supervised and supervised learning, previous studies have focused on domain adaptation~\cite{pan2020unsupervised}, mitigating catastrophic forgetting~\cite{mehta2023empirical, he2021analyzing}, and improving data efficiency~\cite{zhai2019s4l, yao2022nlp}. \mixtraining{} introduces a new mixtraining phase that interpolates the self-supervised learning phase and the supervised learning phase, and carefully analyze its compute-performance trade-off.
