\section{Introduction}

\begin{figure}[t]
\centering
\includegraphics[width=0.9\textwidth]{figure/figure1_.pdf}
\caption{
\mixtraining demonstrates significant accuracy and computation gains over standard self-supervised learning plus supervised learning pipeline across various data limitations levels (10\%, 50\%, and 100\%). 
Experiments are conducted on the TinyImageNet dataset with the ViT-Tiny model; 
Each set of three points on the same line represents results obtained with different training durations (50, 75, and 100 epochs).
}
\label{fig:intro_fig1}
\end{figure} 

Deep neural networks have achieved remarkable success in various domains, including computer vision~\cite{he2022masked,wu2022tinyvit} and natural language processing~\cite{devlin2018bert,cnbc2023chatgpt}. However, their reliance on extremely large-scale datasets, such as ImageNet-21K~\citep{ridnik2021imagenet} and MTEB~\citep{muennighoff2022mteb}, poses a significant challenge in data-limited scenarios. When data is scarce or costly to acquire, achieving high accuracy becomes significantly more difficult, underscoring the need for methods that can effectively utilize limited resources.

A promising approach for data-limited scenarios is self-supervised learning (\SSLalone), which leverages unlabeled data to learn informative representations and has shown notable improvements across various 
downstream tasks \citep{he2022masked, radford2019language}. However, these benefits often come with a substantial computational cost, as \SSLalone typically requires an additional and often prolonged training phase. 
This extended training can be a major bottleneck in compute-constrained settings, making it crucial to balance the trade-off between performance gains and computational efficiency.
To address this critical challenge, we develop a new training framework \mixtraining, which adds an additional \emph{mixtraining phase} in between the vanilla self-supervised learning phase (\SSLalone) and supervised learning phase (\SL). 
At a high level, the mixtraining phase {``merges''} several self-supervised learning epochs and supervised learning epochs together, featuring a smooth transition between two objectives (see \cref{fig:comparison}).
Unlike the conventional \SSL pipeline that treats self-supervised learning and supervised learning as separate stages, which often causes abrupt transitions, \mixtraining leverages the mixtraining phase to interpolate and unify the whole training process. 
By merging several self-supervised learning and supervised learning epochs together, \mixtraining consolidates some shared computation steps, which further reduces computation overhead and improves training efficiency.

Compared to the standard self-supervised learning plus supervised learning pipeline, \mixtraining creates a better compute-performance trade-off by offering a Pareto improvements over both accuracy and training latency.
As highlighted in \cref{fig:intro_fig1}, compared to \SSL, \mixtraining achieves higher accuracy and lower latency across all settings with various data limitation levels and training epochs.
For instance, on the full TinyImageNet dataset, \mixtraining achieves 18.89\% relative accuracy gains (8.81\% absolute accuracy gains) and 1.29$\times$ speedups. When data is limited, the \mixtraining achieves more significant accuracy gain: for instance, at 10\% of data limitation level, \mixtraining achieves 105.58\% relative accuracy gains (10.78\% absolute accuracy gains) and 1.24$\times$ speedups. 

\paragraph{Contributions.}
We develop a novel \mixtraining framework that offers a Pareto improvement over the conventional self-supervised learning plus supervised learning pipeline.
The main features and contributions of \mixtraining{} are highlighted as follows.
\begin{itemize}[left=0pt]
\item \textbf{Better Compute-Performance Trade-off.}
Compared to the standard \SSL pipeline, \mixtraining  features a smooth transition between two objectives (for better accuracy) and an optimized computation allocation.
We conduct extensive experiments across various datasets and settings and demonstrate the efficacy of \mixtraining.
As an example, \mixtraining achieves 18.89\% performance gain (8.81\% absolute) on the TinyImageNet dataset and 1.29$\times$ speedups for the ViT-Tiny model (\cref{fig:intro_fig1}).

\item \textbf{Versatility and Plug-and-Play Integration.}
\mixtraining is designed to be model-agnostic and can be effortlessly integrated into various settings, such as single-task and multi-task learning settings. Its modular structure allows researchers to plug and play different \SSLalone and \SL components without extensive re-engineering, ensuring broad applicability across a variety of machine learning domains. 
\end{itemize}

\paragraph{Organization.}
The rest of the paper is organized as follows.
We introduce our \mixtraining framework in \cref{sec:method}, including its design intuition, operational procedure, and extensions to multi-task learning.
We conduct extensive experiments in \cref{sec:experiment} to verify the superiority of \mixtraining over conventional pipelines. 
We discuss additional related work in \cref{sec:related} and conclude with future directions in \cref{sec:conclusion}.

\begin{figure*}[t]
\centering
\includegraphics[width=0.9\textwidth]{figure/comparision_ssl.pdf}
\caption{Comparison of \mixtraining with the standard SSL+SL framework.  
(a) The standard SSL+SL paradigm featuring an abrupt transition from self-supervised objective ($\mathsf{obj}_{\mathsf{ssl}}$) to supervised objective ($\mathsf{obj}_{\mathsf{sl}}$).
(b) Our \mixtraining framework features an added mixtraining phase in the middle. The mixtraining phase optimizes towards a mixed objective ($\mathsf{obj}_{\mathsf{mix}}$), which enables a smooth transition from the self-supervised objective to the supervised objective. 
}
\label{fig:comparison}
\end{figure*}