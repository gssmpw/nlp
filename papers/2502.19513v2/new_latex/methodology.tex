
\section{Methodology}
\label{sec:method}

We briefly introduce the standard self-supervised learning plus supervised learning (SSL+SL) framework in \cref{sec: classical pretrain_finetune}. We introduce our \mixtraining{} framework in \cref{sec:mixtraining}, which consists of its design intuition (\cref{sec:mixtraining_intuition}) and operational procedure (\cref{sec:mixtraining_operational_procedure}).
We provide extensions of the \mixtraining framework to more general settings in \cref{sec:generalizability}.



\subsection{Background: The Standard Self-supervised Learning plus Supervised Learning Framework}
\label{sec: classical pretrain_finetune}

The standard self-supervised learning plus supervised learning framework typically consists of two phases: \emph{self-supervised learning phase (SSL)} and \emph{supervised learning phase (SL)}. In the self-supervised learning phase, a backbone model with a self-supervised learning head is trained to help the model learn general feature representations. Specifically, this process usually relies on learning from unlabeled data with reconstruction tasks \citep{hinton1993autoencoders, he2022masked} or predicting manually masked tokens \citep{devlin2018bert, radford2019language, brown2020language}. 
The backbone model is further refined in the supervised learning phase, together with a supervised learning head. This step adapts the model to downstream tasks, usually achieving better performances than directly training the downstream tasks.
\cref{fig:comparison}(a) shows a general pipeline of the standard SSL+SL framework, which has now become the go-to approach for training large models \citep{wang2023large,llm_survey}.

\subsection{A New Framework: \mixtraining }
\label{sec:mixtraining}

\begin{figure*}[t]
\centering
\includegraphics[width=0.9\textwidth]{figure/computation.pdf}
\caption{Comparison of \mixtraining with the standard SSL+SL framework.   \mixtraining achieves computation gains over the standard SSL+SL framework.
\emph{Top: Standard SSL+SL.} Data first goes through a self-supervised learning pass ($F_{\mathsf{ssl,1}}\rightarrow F_{\mathsf{ssl,2}} \rightarrow B_{\mathsf{ssl, 2}} \rightarrow B_{\mathsf{ssl, 1}}$) and then goes through a supervised learning pass ($F_{\mathsf{sl,1}}\rightarrow F_{\mathsf{sl,2}} \rightarrow B_{\mathsf{sl, 2}} \rightarrow B_{\mathsf{sl, 1}}$). 
\emph{Bottom: \mixtraining.} We merge two forward passes ($F_{\mathsf{ssl,1}}$ and $F_{\mathsf{sl,1}}$) over the backbone model together into a single pass $F_{\mathsf{mix,1}}$ 
, and use its result for both self-supervised head and supervised head; the backward passes ($B_{\mathsf{ssl,1}}$ and $B_{\mathsf{sl,1}}$) are merged into $B_{\mathsf{mix,1}}$ 
(bottom left). Our modifications reduce computation requirements and allow better parallelization (bottom right). 
}
\label{fig:computation}
\end{figure*}

\subsubsection{Design Intuition behind \mixtraining{}}
\label{sec:mixtraining_intuition}

While the standard SSL+SL framework has achieved remarkable success, its self-supervised learning and supervised learning phases are completely separated, leaving no  
room for further optimization.
To enable closer interactions between these two phases,  we propose a novel \mixtraining{} framework---as shown in \cref{fig:comparison}---which {effectively merges several self-supervised learning and supervised learning epochs into an additional \emph{mixtraining phase}, featuring a smooth transition between learning objectives.} 
%that blends 

\mixtraining introduces a new mixtraining phase that allows joint updates of self-supervised and supervised objectives, which is in contrast with the standard SSL+SL framework where one \emph{first} updates the self-supervised objective and \emph{then} updates the supervised objective.
At a high level, the benefits of this phase are rooted in the dedicated joint optimization objective.
The joint objective can be easily understood as a weighted average of the self-supervised objective and the supervised objective to balance these two objectives. 
We next explain the design intuition behind the mixtraining phase to achieve both \emph{computation gains} and \emph{accuracy gains}.
\looseness=-1

\paragraph{Accuracy Gains.}

Since the self-supervised learning phase aims at learning general representations and the following supervised learning phase aims at learning task-specific information, intuitively, these two phases optimize the model in different directions. 
The standard SSL+SL framework features an abrupt change in optimization directions during the transition from self-supervised learning and supervised learning (\cref{fig:comparison}(a)), which may cause instability in model performance \citep{mosbach2020stability}. 
Indeed, recent research shows that, in certain settings, supervised learning can lead to worse model performance \citep{peters2019tune, kumar2022fine}.
In our \mixtraining framework, the mixtraining phase creates a middle ground (i.e., a weighted combination of two objectives), allowing a rather smooth transition from the self-supervised learning objectives to the supervised learning objective, as illustrated in \cref{fig:comparison}(b). We hypothesize that such a smooth transition avoids instability in phase transition, thus allowing the model to better adapt to the target task and achieve higher accuracy (we empirically verify this hypothesis in \cref{sec:experiment}). 

\paragraph{Computation Gains.} 
In the standard SSL+SL framework, we first run self-supervised learning passes with data $(x_\mathpt, y_\mathpt)$, and then run supervised learning passes with data $(x_\mathft, y_\mathft)$.
This process involves forward/backward passes of both $(x_\mathpt, y_\mathpt)$ and $(x_\mathft, y_\mathft)$, and strictly follows a \emph{sequential order} to compute each sub-processes (top part of \cref{fig:computation}).
In contrast, \mixtraining aims to jointly optimize self-supervised and supervised objectives. 
Specifically, it ``merges'' $(x_\mathpt, y_\mathpt)$ and $(x_\mathft, y_\mathft)$ into a \emph{mixed data} $(x_\mathmix, y_\mathmix)$ and thus merging the separate forward passes over the backbone model into a single pass, and use its result for both self-supervised head and supervised head;
from the computation aspect, 
% \zx{with asynchronous computation}, 
we also merge the backward passes of self-supervised learning and supervised learning tasks over the backbone model into a single pass (bottom left part of \cref{fig:computation}).
Since the size of the backbone model is usually much larger than the size of self-supervised and supervised heads \citep{he2022masked,du2021simple,yang2023swin3d}, the merge of forward/backward passes over the backbone model allows us to reduce computation compared to the synchronous setting substantially.
Additionally, the merge of forward/backward passes over the backbone model allows better parallelization of forward/backward passes over the self-supervised and supervised heads (right part of \cref{fig:computation}), which further speeds up the computation. 

\subsubsection{The \mixtraining Procedure}
\label{sec:mixtraining_operational_procedure}

In this section, we introduce the operational procedure of our \mixtraining framework in detail.
Besides the number of self-supervised learning epoch $e_\mathpt$ and the number of supervised learning epoch $e_\mathft$, \mixtraining takes as input a hyperparameter \mixratio $\rho \in [0, 1]$ to determine the number of self-supervised learning/supervised learning epochs $e_{\mathsf{mix}}$ to be merged into the mixtraining phase.
We set 
\begin{equation}
e_{\mathsf{mix}} = \floor{\rho \min(e_{\mathsf{ssl}}, e_{\mathsf{sl}})}.
\label{eq:mix_epoch}
\end{equation}
Our \mixtraining framework then operates by running (i) the vanilla self-supervised learning phase for $e_{\mathpt} - e_{\mathsf{mix}}$ epochs, (ii) the mixtraining phase for $e_{\mathsf{mix}}$ epochs, and (iii) the vanilla supervised learning phase for $e_{\mathft} - e_{\mathsf{mix}}$ epochs, as shown in \cref{alg:algorithm}. 


Since the self-supervised learning and supervised learning phases are standard, in the following, we mainly discuss the mixtraining phase.
In the mixtraining phase, we (i) design a mixing function $g$ to generate \emph{a mixed dataset}, and (ii) design \emph{a joint optimization objective} as supervision signal.
We next highlight the design choice for these two parts.

\begin{algorithm}[t]
\caption{The \mixtraining{} Framework}
\label{alg:algorithm}
	\renewcommand{\algorithmicrequire}{\textbf{Input:}}
	\renewcommand{\algorithmicensure}{\textbf{Output:}}
\begin{algorithmic}[1]
\REQUIRE Self-supervised learning epoch $e_{\mathsf{ssl}}$, Supervised learning epoch $e_{\mathsf{sl}}$, \mixratio $\rho \in [0,1]$.
\STATE Initialize model parameters $\theta$.
\STATE Calculate mixtraining epoch $e_{\mathsf{mix}}$ via \cref{eq:mix_epoch}.
\STATE \textcolor{greygreen}{$\triangleright$ Vanilla Self-supervised Learning Phase}
\FOR{$e = 1$ to $e_{\mathsf{ssl}} - e_{\mathsf{mix}}$} 
    \STATE Conduct vanilla self-supervised learning phase \textit{w.r.t.} self-supervised loss \( {\ell}_{\mathsf{ssl}}(x; \theta) \) to optimize $\theta$.
\ENDFOR

\STATE \textcolor{greygreen}{$\triangleright$ \mixtraining{} Phase} 
\FOR{$e = 1$ to $e_{\mathsf{mix}}$}
    \STATE Optimize the model parameter $\theta$ with respect to the joint optimization objective as in \cref{eq:target}.
\ENDFOR

\STATE \textcolor{greygreen}{$\triangleright$ Vanilla Supervised Learning Phase}
\FOR{$e = 1$ to $e_{\mathsf{sl}} - e_{\mathsf{mix}}$}
    \STATE Conduct vanilla  supervised learning phase \textit{w.r.t.} supervised loss \( {\ell}_{\mathsf{sl}}(f(x), y ; \theta) \) to optimize $\theta$.
\ENDFOR
\end{algorithmic}
\end{algorithm}

\paragraph{The Mixed Dataset.}
The goal of creating a mixed dataset $\mathcal{D}_{\mathsf{mix}} = g(\mathcal{D_{\mathsf{ssl}}},\mathcal{D_{\mathsf{sl}}})$ is to extract information stored in self-supervised learning dataset $\mathcal{D_{\mathsf{ssl}}}=\{x_i\}_i$ and supervised learning dataset $\mathcal{D_{\mathsf{sl}}}=\{(x_i,y_i)\}_i$, featuring a smooth transition between two learning objectives \cref{fig:comparison} 
 (b).
In the simple case where self-supervised learning and supervised learning use the same feature representation (i.e., $x_i$), we can simply set $g (\mathcal{D}_{\mathsf{ssl}}, \mathcal{D}_{\mathsf{sl}}) = \mathcal{D}_{\mathsf{sl}}$. 
We remark that the importance of this simple case is usually {overlooked}: conducting self-supervised learning and supervised learning on the same ImageNet dataset allows one to boost the top-1 classification accuracy to 84.9\% from 82.5\%, without using extra data \citep{he2022masked}.

We next discuss the general case where the self-supervised learning dataset is not the same as the supervised learning dataset, i.e., $ \mathcal{D}_{\mathsf{ssl}} \neq \mathcal{D}_{\mathsf{sl}}$. Inspired by the mixup method in machine learning to improve the generalization and robustness to adversarial examples \citep{zhang2017mixup}, we consider a \emph{randomized} mixing function $g$, which randomly mixes up data points from both datasets. 
Specifically, we set 
\begin{align}
\label{eq:mix_dataset}
{\cal D}_{\mathsf{mix}} = g (\mathcal{D}_{\mathsf{ssl}}, \mathcal{D}_{\mathsf{sl}}) = \{  (x_{\mathsf{mix}}, y_{\mathsf{sl}}): x_{\mathsf{mix}} = \lambda \, x_{\mathsf{sl}} + (1 - \lambda) \, x_{\mathsf{ssl}}, (x_{\mathsf{sl}}, y_{\mathsf{sl}}) \in \mathcal{D}_{\mathsf{sl}}, x_\mathssl \in \cD_\mathssl \},
\end{align}

where for each $(x_{\mathsf{sl}}, y_{\mathsf{sl}}) \in \mathcal{D}_{\mathsf{sl}}$, we randomly draw a self-supervised data point $x_{\mathsf{ssl}} \sim \mathcal{D}_{\mathsf{ssl}}$ and generate a mixup feature $\lambda \, x_{\mathsf{sl}} + (1 - \lambda) \, x_{\mathsf{ssl}}$, where $\lambda$ is a hyperparameter of user's choice (we set $\lambda = 0.5$ in our experiments to balance the contribution from both datasets). 
We adopt the supervised label $y_{\mathsl}$ to provide supervised signal since the self-supervised part generally doesn't require labels.
\looseness=-1

\paragraph{The Joint Optimization Objective.}
Let $\theta$ denote the model parameters. Let \( {\ell}_{\mathsf{ssl}}(x; \theta) \) denote the self-supervised loss, e.g., MSE reconstruction loss for masked autoencoders \citep{he2022masked}, and let \( {\ell}_{\mathsf{sl}}(f(x), y ; \theta) \) denote the supervised loss, e.g., cross-entropy for image classification \citep{krizhevsky2012imagenet}. Let \( \mathcal{D}_{\mathsf{ssl}} \) represent the SSL dataset, which contains examples \( \{x_i\}_i \), and \( \mathcal{D}_{\mathsf{sl}} \) represent the SL dataset, which also contains examples \( \{(x_i, y_i)\}_i \). These datasets serve as the sources for self-supervised and supervised learning, respectively. The classical \SSL framework first optimizes $ \min_\theta \mathbb{E}_{x,y \sim \mathcal{D}_{\mathsf{ssl}}} [{\ell}_{\mathsf{ssl}}(x; \theta)] $ and then optimizes $ \min_\theta \mathbb{E}_{x,y \sim \mathcal{D}_{\mathsf{sl}}} [{\ell}_{\mathsf{sl}}(f(x), y; \theta)]$.

To integrate self-supervised and supervised learning objectives, \mixtraining considers a weighted combination of these two objectives and optimizes the following goal:
\begin{equation}
\min_\theta \mathbb{E}_{x,y \sim \mathcal{D}_{\mathsf{mix}}} [\alpha \, {\ell}_{\mathsf{ssl}}(x; \theta) + (1-\alpha)\, {\ell}_{\mathsf{sl}}(f(x), y; \theta)],
\label{eq:target}
\end{equation}
where \( \mathcal{D}_{\mathsf{mix}} \) is the mixed dataset and \( \alpha\) is a hyperparameter \lossratio{} designed to balance the focus between learning general representations (by optimizing self-supervised loss \( {\ell}_{\mathsf{ssl}}(x; \theta) \)) and achieving specific target (by optimizing supervised loss \( {\ell}_{\mathsf{sl}}(f(x), y ; \theta) \)). 

\subsubsection{Extensions to Multi-Task Learning}
\label{sec:generalizability}

% \paragraph{Extension to Multi-Task Learning.} 
While specific choices of the \emph{mixed dataset} and the \emph{joint optimization objective} are provided in this section, we remark that these two sub-components are designed in a modular way: researchers have flexibility in selecting their own way of creating the mixed dataset and the joint optimization objective, tailored to specific training goals.
For instance, while \cref{alg:algorithm} in designed primarily for single-task learning, it can be seamlessly integrated into the multi-task setting. 
To do that, in mixtraining phase of multi-task learning, we can modify \cref{eq:target} to incorporate the mixed training objectives for all tasks.
We conduct extensive experiments in \cref{sec:experiment} and show that \mixtraining is effective in both single-task and multi-task settings. 