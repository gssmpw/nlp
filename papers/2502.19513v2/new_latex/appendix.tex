\newpage
\appendix
\onecolumn

\section{Additional Implementation Details}

\label{sec:implement}

\subsection{Software and Hardware Dependencies}
All of the codes are based on \texttt{PyTorch}\footnote{\url{https://pytorch.org/}}\citep{paszke2019pytorch} with \texttt{timm} library\footnote{\url{https://huggingface.co/timm}}\citep{wightman2021resnet}. All experiments in this paper are running on one NVIDIA RTX 6000 Ada GPU. 

\subsection{Reuse Implementations of Different Self-supervised Learning Tasks. }

\textbf{MAE.} Reuse operation in the MAE process is more complicated, starting with an unmasked encoder forward pass to obtain intermediate features, and then truncating these features based on a predefined masked ratio. A random mask is applied to these truncated features for reconstructing input images. The MAE loss is calculated only from unmasked patches, while the finetuning head computes classification loss using the full intermediate features.


\subsection{Implementation Details of Computer Vision Models}

\textbf{ViT-T.} Our ViT-T implementations are largely derived from the seminal work from~\cite{wu2022tinyvit} and shrink the decoder to 2 layers as suggested in the reproduction challenge by~\citet{charisoudis2023re}. Specifically, we set the embedding dimension (`emb\_dim') to 192, with the encoder and decoder configured to 12 and 2 layers respectively, alongside 3 heads each for both encoder and decoder. The masking ratio is maintained at 0.75 as~\citet{he2022masked} suggests. For the CIFAR-10, CIFAR-100, and SVHN datasets, the image resolution is standardized to 32x32 pixels with a patch size of 2, while the image resolution is 64x64 pixels with a patch size of 4 for TinyImageNet to ensure uniform computational complexity across all experiments. 

\textbf{Preprocessing.} For the preprocessing of CIFAR-10. CIFAR-100 and SVHN, we adopt simple preprocessing as in~\cite{he2016deep}, which randomly crops the images to a size of $32 \times 32$ pixels, with a padding of 4 pixels on each side of the image, then randomly flips the images horizontally with a 50\% probability. For TinyImageNet, we follow preprocessing in the reproduction challenge by~\citet{charisoudis2023re}, aiming to maintain consistency with established benchmarks and facilitate fair comparison. 

\textbf{Hyperparameters.} In~\cref{tab:hyperparameters}, we report detailed hyperparameters used in our experiments. Note that, following~\citet{charisoudis2023re}, in TinyImageNet dataset, we slightly modify the hyperparameters in this table, where base learning rate to 1e-3 and 2e-3, betas to (0.9, 0.95) and (0.9, 0.999), weight decay to 0.15 and 0.05, for pre-training and finetuning, respectively.

\begin{table}[!htbp]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Hyperparameters} & \textbf{Self-Supervised learning} & \textbf{Supervised Learning} \\
\midrule
Batch Size & 256 & 256 \\
Base Learning Rate & 1.5e-4 & 1e-3 \\
Learning Rate Scheduler & CosineAnnealing & CosineAnnealing \\
Optimizer & AdamW & AdamW \\
Betas & (0.9, 0.95) & (0.9, 0.95) \\
Weight Decay & 0.05 & 0.05 \\
Warmup Epoch & 20 & 5 \\
\bottomrule
\end{tabular}
\caption{Detailed training hyperparameters for baselines in this paper.}
\label{tab:hyperparameters}
\end{table}

\section{Additional Experimental Results}
\label{app:experiment_results}
We report additional experimental results under various data limitation levels and training epochs, in both single-task setting and multi-task setting.
\mixtraining generally achieves Pareto improvements over the \SSL baseline: \mixtraining achieves higher accuracy in 60 out of 66 settings and lower latency in 66 out of 66 settings over \SSL.
These results show that \mixtraining provides a better compute-performance trade-off compared to the standard $\SSL$ pipeline both in the single-task and multi-task settings across various data limitation levels.

\subsection{Additional Experimental Results in Single-Task Setting}
We report experimental results in the single-task setting under various data limitation levels. Results with 10\% data limitation level is provided in \cref{tab:parameter_results_epoch}, and results with $\crl{25\%, 50\%, 75\%, 100\%}$ data limitation levels are presented below. \mixtraining achieves higher accuracy and lower latency in 36 out of 36 settings over \SSL:
\mixtraining achieves up to 83.07\% relative accuracy gain (14.38\% absolute accuracy gain) and up to 1.30$\times$ speedups.

\label{sec:appendix_more_parameter_study_epoch}

\begin{table*}[!htbp]
\centering

\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccc}
\toprule
\multirow{2}{*}{\textbf{Datasets}} &\multirow{2}{*}{\textbf{Computation (epoch)}}& \multicolumn{2}{c}{\textbf{\SL}} & \multicolumn{2}{c}{\textbf{\SSL}}& \multicolumn{2}{c}{\textbf{\mixtraining}}\\
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}&
 & Accuracy$\uparrow$ & Latency(s)$\downarrow$  & Accuracy$\uparrow$ & Latency(s)$\downarrow$  & Accuracy$\uparrow$ & Latency(s)$\downarrow$ \\
\midrule
\multirow{3}{*}{\textbf{TinyImageNet}}        &  50                  & 17.60\% &   1268.41 & 17.31\% &2771.72   & \textbf{31.69\%} & 2129.89   \\
 & 75         & {19.49\%} &  1921.23  & {19.80\%} & 4223.24   & \textbf{31.90\%} & 3281.00 \\

                            &100  & 20.91\% &2619.14   & 21.78\% &  5813.27 & \textbf{31.43\%} &  4516.69\\
\midrule
 \multirow{3}{*}{\textbf{CIFAR-10}} &50           & {65.03\%} & 646.45 & 67.65\% &  1409.32 & \textbf{70.69\%} &  1076.16  \\

    &75                    & 65.69\% &  985.54  & 68.57\% &  2135.05 & \textbf{71.29\%} & 1719.24   \\
 & 100       & 66.03\% &  1335.79  & {67.95\%} &  2774.24 & \textbf{72.61\%} &  2206.13  \\
\midrule
 \multirow{3}{*}{\textbf{CIFAR-100}}     &50                      & 30.87\% &  651.25 & 34.46\% &  1517.26 & \textbf{36.87\%} &  1161.89\\
  &75        & {31.21\%} &  1033.91 & {35.05\%} &  2237.72 & \textbf{38.43\%} &  1720.56  \\

                      &100    & 31.49\% &  1346.96 & 34.93\% & 2882.62 & \textbf{38.55\%} &  2226.92 \\

\bottomrule
\end{tabular}
}
\caption{Parameter study on training epochs in single-task learning setting under 25\% data limitation. Accuracy gains are highlighted in \textbf{bold}. \mixtraining achieves a Pareto improvement over the \SSL baseline: \mixtraining achieves higher accuracy and lower latency in 9 out of 9 settings.
}
\label{tab:main_results_appen1}
\end{table*}


\begin{table*}[!htbp]
\centering

\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccc}
\toprule
\multirow{2}{*}{\textbf{Datasets}} &\multirow{2}{*}{\textbf{Computation (epoch)}}& \multicolumn{2}{c}{\textbf{\SL}} & \multicolumn{2}{c}{\textbf{\SSL}}& \multicolumn{2}{c}{\textbf{\mixtraining}}\\
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}&
 & Accuracy$\uparrow$ & Latency(s)$\downarrow$  & Accuracy$\uparrow$ & Latency(s)$\downarrow$  & Accuracy$\uparrow$ & Latency(s)$\downarrow$ \\
\midrule
\multirow{3}{*}{\textbf{TinyImageNet}}       &  50                  & 24.95\% &   2496.29 & 27.27\% &  5401.40 & \textbf{40.37\%} &   4280.53 \\
  & 75         & {28.81\%} &  3827.39  & {31.15\%} &  8363.36  & \textbf{41.99\%} & 6530.16 \\

                           &100  & 30.36\% & 5181.44  & 34.30\% & 11449.43  & \textbf{42.77\%} &8868.88  \\
\midrule
 \multirow{3}{*}{\textbf{CIFAR-10}} &50           & {73.70\%} & 1252.78 & 75.26\% &  2703.05 & \textbf{78.83\%} &  2133.61  \\

       &75                    & 74.63\% &  1931.89  & 76.40\% &  4155.73 & \textbf{79.31\%} & 3255.72   \\
 & 100       & 75.00\% &  2593.27  & {77.06\%} &  5354.08 & \textbf{79.95\%} &  4247.78  \\
\midrule
 \multirow{3}{*}{\textbf{CIFAR-100}}     &50                      & 42.32\% &  1248.02 & 44.95\% &  2924.08 & \textbf{48.53\%} &  2246.85\\
 &75        & {42.46\%} &  1981.46 & {45.23\%} &  4355.99 & \textbf{48.87\%} &  3275.14  \\

                      &100    & 42.50\% &  2580.08 & 46.02\% & 5551.88 & \textbf{48.60\%} &  4298.75 \\

\bottomrule
\end{tabular}
}
\caption{Parameter study on training epochs in single-task learning setting under 50\% data limitation. Accuracy gains are highlighted in \textbf{bold}. \mixtraining achieves a Pareto improvement over the \SSL baseline: \mixtraining achieves higher accuracy and lower latency in 9 out of 9 settings.
}
\label{tab:main_results_appen2}
\end{table*}



\begin{table*}[!htbp]
\centering

\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccc}
\toprule
\multirow{2}{*}{\textbf{Datasets}} &\multirow{2}{*}{\textbf{Computation (epoch)}}& \multicolumn{2}{c}{\textbf{\SL}} & \multicolumn{2}{c}{\textbf{\SSL}}& \multicolumn{2}{c}{\textbf{\mixtraining}}\\
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}&
 & Accuracy$\uparrow$ & Latency(s)$\downarrow$  & Accuracy$\uparrow$ & Latency(s)$\downarrow$  & Accuracy$\uparrow$ & Latency(s)$\downarrow$ \\
\midrule
 \multirow{3}{*}{\textbf{TinyImageNet}}       &  50                  & 33.16\% &  3713.97  & 34.81\% &  8062.47 & \textbf{46.44\% }&  6369.23  \\
  & 75         & {36.64\%} &  5708.06  & {40.31\%} & 12538.24   & \textbf{49.29\%} &  9779.95\\

                           &100  & 36.98\% &  7709.21 & 42.73\% &17296.22   & \textbf{49.30\%} &13304.60  \\
\midrule
 \multirow{3}{*}{\textbf{CIFAR-10}} &50           & {78.08\%} & 1868.81 & 79.60\% &  4033.65 & \textbf{82.91\%} &  3163.51  \\

        &75                    & 78.74\% &  2856.25  & 81.24\% &  6216.65 & \textbf{83.68\%} & 4762.06   \\
 & 100       & 79.22\% &  3844.27  & {82.11\%} &  7993.72 & \textbf{83.97\%} &  6234.03  \\
\midrule
 \multirow{3}{*}{\textbf{CIFAR-100}}    &50                      & 48.99\% &  2009.17 & 51.85\% &  4345.47 & \textbf{54.30\%} &  3329.29\\
  &75        & {49.00\%} &  2929.78 & {53.25\%} &  6312.75 & \textbf{55.05\%} &  4857.74  \\

                       &100    & 48.97\% &  3814.54 & 53.79\% & 8226.83 & \textbf{55.84\%} &  6354.76 \\

\bottomrule
\end{tabular}
}
\caption{Parameter study on training epochs in single-task learning setting under 75\% data limitation. Accuracy gains are highlighted in \textbf{bold}. \mixtraining achieves a Pareto improvement over the \SSL baseline: \mixtraining achieves higher accuracy and lower latency in 9 out of 9 settings.
}
\label{tab:main_results_appen3}
\end{table*}


\begin{table*}[!htbp]
\centering

\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccc}
\toprule
\multirow{2}{*}{\textbf{Datasets}} &\multirow{2}{*}{\textbf{Computation (epoch)}}& \multicolumn{2}{c}{\textbf{\SL}} & \multicolumn{2}{c}{\textbf{\SSL}}& \multicolumn{2}{c}{\textbf{\mixtraining}}\\
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}&
 & Accuracy$\uparrow$ & Latency(s)$\downarrow$  & Accuracy$\uparrow$ & Latency(s)$\downarrow$  & Accuracy$\uparrow$ & Latency(s)$\downarrow$ \\
\midrule
 \multirow{3}{*}{\textbf{TinyImageNet}}      &  50                  & 39.75\% &   4957.82 & 42.16\% &  10714.36 & \textbf{50.96\%} & 8410.97   \\
 & 75         & {41.74\%} &  7560.40  & {44.22\%} &   16498.39 & \textbf{53.09\%} & 13111.49 \\

                              &100  & 41.96\% &10257.12   & 46.65\% & 22917.29  & \textbf{55.46\%} & 17795.47 \\
\midrule
 \multirow{3}{*}{\textbf{CIFAR-10}} &50           & {80.78\%} & 2489.38 & 83.75\% &  5365.33 & \textbf{85.60\%} &  4193.42  \\

        &75                    & 81.59\% &  3818.88  & 84.48\% &  8070.96 & \textbf{86.79\%} & 6515.22   \\
 & 100       & 81.52\% &  5155.59  & {84.69\%} &  10730.52 & \textbf{87.13\%} &  8274.45  \\
\midrule
 \multirow{3}{*}{\textbf{CIFAR-100}}     &50                      & 54.25\% &  2713.55 & 56.91\% &  5763.71 & \textbf{58.67\%} &  4421.63\\
  &75        & {54.71\%} &  3881.84 & {56.95\%} &  8104.50 & \textbf{59.11\%} &  6459.23  \\

                     &100    & 54.72\% &  5022.96 & 57.92\% & 10960.08 & \textbf{59.95\%} &  8457.93 \\

\bottomrule
\end{tabular}
}
\caption{Parameter study on training epochs in single-task learning settings with full data. Accuracy gains are highlighted in \textbf{bold}. \mixtraining achieves a Pareto improvement over the \SSL baseline: \mixtraining achieves higher accuracy and lower latency in 9 out of 9 settings.
}
\label{tab:main_results_appen4}
\end{table*}

\subsection{Additional Experimental Results in Multi-Task Setting}
We report experimental results in the multi-task setting under various data limitation levels. Results with $\crl{10\%, 25\%, 50\%, 75\%, 100\%}$ data limitation levels are presented below. \mixtraining achieves higher accuracy in 24 out of 30 settings and lower latency in 30 out of 30 settings over \SSL:
\mixtraining achieves up to 34.72\% relative accuracy gain (17.25\% absolute accuracy gain) and up to 1.31$\times$ speedups.

\begin{table*}[!tbp]
\centering

\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccc}
\toprule
\multirow{2}{*}{\textbf{Datasets}} &\multirow{2}{*}{\textbf{Computation (epoch)}}& \multicolumn{2}{c}{\textbf{\SL}} & \multicolumn{2}{c}{\textbf{\SSL}}& \multicolumn{2}{c}{\textbf{\mixtraining}}\\
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}&
 & Accuracy$\uparrow$ &  Latency(s)$\downarrow$  & Accuracy$\uparrow$ &  Latency(s)$\downarrow$  & Accuracy$\uparrow$ &  Latency(s)$\downarrow$ \\

\midrule
 \multirow{3}{*}{\textbf{CIFAR-10}} &50           & {53.93\%} &526.70  & 56.50\% &  1142.78  & \textbf{60.68\%} &  876.11   \\
     &75                    & 54.14\% &  818.64   & 58.05\% &   1756.56 & \textbf{62.67\%} &  1368.43   \\
 & 100       & 55.20\% &1064.90  & {59.02\%} & 2326.15   & \textbf{63.44\%} &1782.50     \\
\midrule
  \multirow{3}{*}{\textbf{SVHN}}     &50                      & 38.98\% &  526.70 & 49.68\% &  1142.78& \textbf{66.93\%} &  876.11\\
   &75        & {44.64\%} &  818.64 & {59.29\%} &  1756.56 & \textbf{74.68\%} & 1368.43  \\

        &100    & 47.86\% &  1064.90 & 62.49\% & 2326.15 & \textbf{76.77\%} &  1782.50 \\

\bottomrule
\end{tabular}
}
\caption{Parameter study on training epochs in multi-task learning setting under 10\% data limitation. Accuracy gains are highlighted in \textbf{bold}; results under other data limitation levels are deferred to \cref{sec:appendix_more_parameter_study_epoch}. \mixtraining achieves a Pareto improvement over the \SSL baseline: \mixtraining achieves higher accuracy and lower latency in 6 out of 6 settings.
}
\label{tab:parameter_results_epoch_mul}
\end{table*}



\begin{table*}[!htbp]
\centering

\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccc}
\toprule
\multirow{2}{*}{\textbf{Datasets}} &\multirow{2}{*}{\textbf{Computation (epoch)}}& \multicolumn{2}{c}{\textbf{\SL}} & \multicolumn{2}{c}{\textbf{\SSL}}& \multicolumn{2}{c}{\textbf{\mixtraining}}\\
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}&
 & Accuracy$\uparrow$ & Latency(s)$\downarrow$  & Accuracy$\uparrow$ & Latency(s)$\downarrow$  & Accuracy$\uparrow$ & Latency(s)$\downarrow$ \\

\midrule
 \multirow{3}{*}{\textbf{CIFAR-10}} &50           & {67.22\%} &1245.69 & 69.97\% &2700.01  & \textbf{72.51\%} &  2076.19   \\
     &75                    & 67.72\% &   1935.73 & 71.72\% & 4166.39  & \textbf{73.22\%} & 3234.31     \\
 & 100       & 68.07\% &  2520.87  & {73.05\%} &  5487.76  & \textbf{73.97\%} & 4228.44   \\
\midrule
  \multirow{3}{*}{\textbf{SVHN}}     &50                      & 74.03\% &  1245.69 & 82.05\% &  2700.01 & \textbf{86.31\%} &  2076.19\\
   &75        & {76.13\%} &  1935.73 & {84.09\%} &  4166.39 & \textbf{87.30\%} &  3234.31  \\

        &100    & 76.58\% &  2520.87 & 86.85\% & 5487.76 & \textbf{88.17\%} &  4228.44 \\

\bottomrule
\end{tabular}
}
\caption{Parameter study on training epochs in multi-task learning setting under 25\% data limitation. Accuracy gains are highlighted in \textbf{bold}. \mixtraining achieves a Pareto improvement over the \SSL baseline: \mixtraining achieves higher accuracy and lower latency in 6 out of 6 settings.
}
\label{tab:parameter_results_epoch_mul_appen1}
\end{table*}


\begin{table*}[!htbp]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccc}
\toprule
\multirow{2}{*}{\textbf{Datasets}} &\multirow{2}{*}{\textbf{Computation (epoch)}}& \multicolumn{2}{c}{\textbf{\SL}} & \multicolumn{2}{c}{\textbf{\SSL}}& \multicolumn{2}{c}{\textbf{\mixtraining}}\\
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}&
 & Accuracy$\uparrow$ & Latency(s)$\downarrow$  & Accuracy$\uparrow$ & Latency(s)$\downarrow$  & Accuracy$\uparrow$ & Latency(s)$\downarrow$ \\

\midrule
 \multirow{3}{*}{\textbf{CIFAR-10}} &50           & {74.63\%} &2466.30 & 78.94\% &  5323.23 & \textbf{79.29\%} & 4321.87    \\
     &75                    & 76.23\% &     3821.66 & \textbf{80.11\%} &  8225.18  & 79.95\% &  6361.40    \\
 & 100       & 76.96\% &   5048.61   & \textbf{81.26\%} & 10778.72  & {80.98\%} &   8355.64  \\
\midrule
  \multirow{3}{*}{\textbf{SVHN}}     &50                      & 85.33\% &  2466.30 & 89.57\% &  5323.23 & \textbf{89.94\%} &  4321.87\\
   &75        & {86.41\%} &  3821.66 & {90.97\%} &  8225.18 & \textbf{91.36\%} &  6361.40  \\

        &100    & 87.88\% &  5048.61 & 92.12\% & 10778.72 & \textbf{92.23\%} &  8355.64 \\

\bottomrule
\end{tabular}
}
\caption{Parameter study on training epochs in multi-task learning setting under 50\% data limitation. Accuracy gains are highlighted in \textbf{bold}. \mixtraining achieves a Pareto improvement over the \SSL baseline: \mixtraining achieves higher accuracy in 4 out of 6 settings and lower latency in 6 out of 6 settings.
}
\label{tab:parameter_results_epoch_mul_appen2}
\end{table*}


\begin{table*}[!htbp]
\centering

\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccc}
\toprule
\multirow{2}{*}{\textbf{Datasets}} &\multirow{2}{*}{\textbf{Computation (epoch)}}& \multicolumn{2}{c}{\textbf{\SL}} & \multicolumn{2}{c}{\textbf{\SSL}}& \multicolumn{2}{c}{\textbf{\mixtraining}}\\
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}&
 & Accuracy$\uparrow$ & Latency(s)$\downarrow$  & Accuracy$\uparrow$ & Latency(s)$\downarrow$  & Accuracy$\uparrow$ & Latency(s)$\downarrow$ \\

\midrule
 \multirow{3}{*}{\textbf{CIFAR-10}} &50           & {79.28\%} &3671.34  & 82.41\% &  8010.30 & \textbf{83.00\%} &   6478.40  \\
     &75                    & 80.69\% &   5744.50  & {83.79\%} & 12374.39  & \textbf{84.05\%} & 9432.45    \\
 & 100       & 80.73\% & 7528.28    & \textbf{84.86\%} &  16235.42 & {84.82\%} &   12534.42 \\
\midrule
  \multirow{3}{*}{\textbf{SVHN}}     &50                      & 89.19\% &  3671.34 & 91.92\% &  8010.30 & \textbf{92.60\%} &  6478.40\\
   &75        & {90.18\%} &  5744.50 & {93.03\%} &  12374.39 & \textbf{93.20\%} &  9432.45  \\

        &100    & 90.71\% &  7528.28 & 93.16\% & 16235.42 & \textbf{93.71\%} &  12534.42 \\

\bottomrule
\end{tabular}
}
\caption{Parameter study on training epochs in multi-task learning setting under 75\% data limitation. Accuracy gains are highlighted in \textbf{bold}. \mixtraining achieves a Pareto improvement over the \SSL baseline: \mixtraining achieves higher accuracy in 5 out of 6 settings and lower latency in 6 out of 6 settings.
}
\label{tab:parameter_results_epoch_mul_appen3}
\end{table*}



\begin{table*}[!htbp]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccc}
\toprule
\multirow{2}{*}{\textbf{Datasets}} &\multirow{2}{*}{\textbf{Computation (epoch)}}& \multicolumn{2}{c}{\textbf{\SL}} & \multicolumn{2}{c}{\textbf{\SSL}}& \multicolumn{2}{c}{\textbf{\mixtraining}}\\
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}&
 & Accuracy$\uparrow$ & Latency(s)$\downarrow$  & Accuracy$\uparrow$ & Latency(s)$\downarrow$  & Accuracy$\uparrow$ & Latency(s)$\downarrow$ \\

\midrule
 \multirow{3}{*}{\textbf{CIFAR-10}} &50           & {82.68\%} &4885.81 & \textbf{86.17\%} &  10577.04 & {85.74\%} &    8567.67 \\
     &75                    & 83.49\% &   7557.23   & {86.55\%} &16327.94  & \textbf{86.57\%} &   12651.54  \\
 & 100       & 84.14\% &  10017.95   & \textbf{87.66\%} &   21561.18  & {87.25\%} &   16671.68  \\
\midrule
  \multirow{3}{*}{\textbf{SVHN}}     &50                      & 90.83\% &  4885.81 & \textbf{93.50\%} &  10577.04 & {93.44\%} &  8567.67\\
   &75        & {91.78\%} &  7557.23 & {93.77\%} &  16327.94 & \textbf{94.32\%} &  12651.54  \\

        &100    & 91.75\% &  10017.95 & 94.17\% & 21561.18 & \textbf{94.63\%} &  16671.68 \\

\bottomrule
\end{tabular}
}
\caption{Parameter study on training epochs in multi-task learning settings with full data. Accuracy gains are highlighted in \textbf{bold}. \mixtraining achieves a Pareto improvement over the \SSL baseline: \mixtraining achieves higher accuracy in 3 out of 6 settings and lower latency in 6 out of 6 settings.
}
\label{tab:parameter_results_epoch_mul_appen4}
\end{table*}