\section{Conclusion}
\label{sec:conclusion}
We introduced \mixtraining, an innovative framework that interleaves multiple self-supervised learning (SSL) and supervised learning (SL) epochs within a unified training phase, enabling a smooth transition between the two learning objectives. By enhancing the synergy between SSL and SL, \mixtraining achieves significant accuracy improvements while consolidating shared computation steps to reduce computational cost. 
Extensive experiments demonstrate that \mixtraining offers a  superior compute-performance trade-off compared to the conventional \SSL pipeline: \mixtraining achieves substantial improvements in model accuracy while significantly accelerating training latency.
Furthermore, modular design of \mixtraining allows for seamlessly integration across various settings, including both single-task and multi-task learning settings.

\paragraph{Future Directions.}
An important next step is to evaluate \mixtraining on larger-scale experiments with more powerful models, such as ViT-Giant \citep{zhai2022scaling}, and larger datasets, such as ImageNet-21K \citep{ridnik2021imagenet}. However, due to computational constraints, we were unable to conduct experiments on these larger-scale settings.