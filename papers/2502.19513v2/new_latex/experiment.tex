\begin{table*}[!tbp]
\centering

\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccccccccc}
\toprule
\multirow{2}{*}{\textbf{Datasets}}&\multirow{2}{*}{\textbf{Baselines}} && &\multicolumn{2}{c}{\textbf{10\%}} & \multicolumn{2}{c}{\textbf{25\%}}& \multicolumn{2}{c}{\textbf{50\%}}&\multicolumn{2}{c}{\textbf{75\%}}&\multicolumn{2}{c}{\textbf{100\%}}\\
\cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12} \cmidrule(lr){13-14}& &&&Accuracy$\uparrow$ & Latency(s)$\downarrow$  & Accuracy$\uparrow$ & Latency(s)$\downarrow$  & Accuracy$\uparrow$ & Latency(s)$\downarrow$ & Accuracy$\uparrow$ & Latency(s)$\downarrow$ & Accuracy$\uparrow$ & Latency(s)$\downarrow$ \\

\midrule
\multirow{3}{*}
{\textbf{TinyImageNet}}&\multirow{1}{*}{\textbf{\SL}}                 &&&7.24\%      & 1102.42   &20.91\% &2619.14   & 30.36\% & 5181.44&36.98\%&7709.21&41.96\%&10257.12   \\
           
&\multirow{1}{*}{\textbf{\SSL}}                 & &&10.21\% &2382.06    & 21.78\% & 5813.27  & 34.30\% & 11449.43&42.73\%&17296.22&46.65\%& 22917.29  \\
             
&\multirow{1}{*}{\textbf{\mixtraining}}                      &&& \textbf{20.99\%} & 1913.75   &\textbf{ 31.43\%} & 4516.69  & \textbf{42.77\%} &  8868.88&\textbf{49.30\%}&13304.60&\textbf{55.46\%}& 17795.47 \\
          



\midrule

\multirow{3}{*}{\textbf{CIFAR-10}}&\multirow{1}{*}{\textbf{\SL}}                 &&&53.40\%      &589.44    &66.03\% &1335.79   & 75.00\% & 2593.27 &79.22\%&3844.27&81.52\%&5155.59  \\
               
&\multirow{1}{*}{\textbf{\SSL}}                 &&&56.79\% &1253.65    & 67.95\% & 2774.24  & 77.06\% & 5354.08&82.11\%&7993.72&84.69\%&10730.52   \\
               
&\multirow{1}{*}{\textbf{\mixtraining}}                      &&&\textbf{ 60.45\%} & 962.16  & \textbf{72.61\%} &2206.13&\textbf{79.95\%}&4247.78&\textbf{83.97\%}&6234.03&\textbf{87.13\%}&  8274.45  \\

\midrule

\multirow{3}{*}{\textbf{CIFAR-100}}&\multirow{1}{*}{\textbf{\SL}}                 &&&19.05\%      & 609.11   &31.49\% &1346.96   & 42.50\% & 2580.08&48.97\%&3814.54&54.72\%&5022.96   \\

&\multirow{1}{*}{\textbf{\SSL}}                 &&& 22.27\% &1279.78    & 34.93\% & 2882.62  & 46.02\% & 5551.88&53.79\%&8226.83&57.92\%&10960.08   \\
               
&\multirow{1}{*}{\textbf{\mixtraining}}                      &&& \textbf{25.19\%} &1010.94  &\textbf{38.55\%}&2226.92&\textbf{48.60\%}&4298.75  &\textbf{ 55.84\%} & 6354.76  & \textbf{59.95\%} &  8457.93  \\
        


\bottomrule
\end{tabular}
}
\caption{Accuracy and latency comparison of \mixtraining{} and baselines in the single-task setting with various data limitation levels.
In each setting, we highlight the highest achieved accuracy level.
\mixtraining achieves a Pareto improvement over the \SSL baseline: \mixtraining achieves higher accuracy and lower latency in 15 out of 15 settings. 
}
\label{tab:main_results_tab1}
\end{table*}

\begin{table}[!tbp]
\centering

\resizebox{0.7\textwidth}{!}{

\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{\textbf{Baselines}} & \multicolumn{2}{c}{\textbf{TinyImageNet to CIFAR-10}} & \multicolumn{2}{c}{\textbf{TinyImageNet to CIFAR-100}}\\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} 
 & Accuracy$\uparrow$ & Latency(s)$\downarrow$  & Accuracy$\uparrow$ & Latency(s)$\downarrow$\\
\midrule
\textbf{\SL}     					& 81.76\% & 5557.86   & 54.22\% &  5361.83 \\
\textbf{\SSL}         &84.48\%  &   11910.06 &56.22\%  & 11762.73 \\

\textbf{\mixtraining}          					& \textbf{89.19\%} & 9031.80 & \textbf{58.49\%} &8931.53  \\

\bottomrule
\end{tabular}
}


\caption{
Accuracy and latency comparison of \mixtraining{} and baselines in the single-task and multi-task setting with different datasets. We use TinyImageNet as the self-supervised learning dataset and use CIFAR-10/CIFAR-100 as the supervised learning dataset. In each setting, we highlight the highest achieved accuracy level. \mixtraining achieves a Pareto improvement over the \SSL baseline: \mixtraining achieves higher accuracy and lower latency in all settings.
} 
\label{tab:add_results}
\end{table}

\begin{table*}[!tbp]
\centering


\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccccccccccc}
\toprule
\multirow{2}{*}{\textbf{Datasets}}&\multirow{2}{*}{\textbf{Baselines}} &&& \multicolumn{2}{c}{\textbf{10\%}} & \multicolumn{2}{c}{\textbf{25\%}}& \multicolumn{2}{c}{\textbf{50\%}}&\multicolumn{2}{c}{\textbf{75\%}}&\multicolumn{2}{c}{\textbf{100\%}}\\
\cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12}\cmidrule(lr){13-14}& &&&Accuracy$\uparrow$ & Latency(s)$\downarrow$  & Accuracy$\uparrow$ & Latency(s)$\downarrow$  & Accuracy$\uparrow$ & Latency(s)$\downarrow$ & Accuracy$\uparrow$ & Latency(s)$\downarrow$ & Accuracy$\uparrow$ & Latency(s)$\downarrow$ \\
\midrule
\multirow{3}{*}{\textbf{CIFAR-10}+\textbf{SVHN}}&\multirow{1}{*}{\textbf{\SL}} &   &             &51.53\%      &1064.90     &72.33\% &  2520.87 & 82.42\% & 5048.61 &85.72\%&7528.28&87.95\%&  10017.95\\
               
% \midrule
&\multirow{1}{*}{\textbf{\SSL}}  &  &           &60.76\% &  2326.15   & 79.95\% &  5487.76  &\textbf{ 86.69\%} &  10778.72&{89.01\%}&16235.42&{90.92\%}& 21561.18   \\
               
% \midrule
&\multirow{1}{*}{\textbf{\mixtraining}}   &    &               &\textbf{ 70.11\%} &1782.50  & \textbf{81.07\%} &4228.44&{86.61\%}&8355.64&\textbf{89.27\%}&12534.42&\textbf{90.94\%}&    16671.68\\



\bottomrule
\end{tabular}
}
\caption{
Average accuracy and total latency comparison of \mixtraining{} and baselines in the multi-task setting with various data limitation levels. We report the average accuracy over CIFAR-10 and SVHN datasets; we defer the separate accuracy results to \cref{app:experiment_results}. In each setting, we highlight the highest achieved accuracy level. \mixtraining achieves a Pareto improvement over the \SSL baseline: \mixtraining achieves higher accuracy in 4 out of 5 settings and lower latency in 5 out of 5 settings. 
}
\label{tab:main_results_tab2}
\end{table*}



\section{Experiments}

\label{sec:experiment}


\subsection{Setup}
\label{sec:setup}
\paragraph{Datasets.} We conduct experiments on standard computer vision datasets, including CIFAR-10~\cite{krizhevsky2009learning},  SVHN~\cite{netzer2011reading},
CIFAR-100~\cite{krizhevsky2009learning}, and TinyImageNet~\cite{le2015tiny}. 
For single-task learning, we consider datasets CIFAR-10, CIFAR-100, TinyImageNet; for multi-task learning, we consider datasets CIFAR-10 and SVHN.
\paragraph{Models.} 
Our model consists of three components: a shared backbone model, a classification head for \SL, and a reconstruction head for \SSLalone. 
We use the standard ViT-Tiny (ViT-T) \citep{dosovitskiy2020image} as the backbone and the classification head. 
For the reconstruction head, we use the masked autoencoder (MAE) decoder \citep{he2022masked} of depth 2.
\looseness=-1


\paragraph{Baselines.}
We evaluate the performance of our algorithm (\cref{alg:algorithm}) against the following baselines:
\begin{itemize}[left=0pt]
    \item \emph{Supervised learning (SL).} Conduct standard supervised learning on the backbone and the classification head with cross-entropy loss for $e_\mathsl$ epochs.
    \item \emph{Self-supervised learning + supervised learning (SSL+SL).} Conduct self-supervised learning on the backbone and the reconstruction head with  MSE loss for $e_{\mathssl}$ epochs and then conduct standard supervised learning with cross-entropy loss for $e_\mathsl$ epochs.
\end{itemize}
The comparison between \SL and \SSL reflects the compute-performance trade-off: \SSL  achieves better performances at the cost of the added computation in the \SSLalone step. 
We aim to provide a better compute-performance trade-off with \mixtraining, i.e., a Pareto improvement over the \SSL baseline.
\paragraph{Evaluation Metrics.}
For each method, we measure its performance by the accuracy on the downstream classification task and its computation cost by the end-to-end training latency (on the same machine). We report the average accuracy and latency over 4 runs with different random seeds.
We calculate the speedups of our method as the ratio between the latency of \SSL and \mixtraining.


\paragraph{Other Implementation Details.} 
We perform experiments on various data limitation levels by randomly select a fraction of $p$ data points from the original dataset; we choose $p \in \{10\%, 25\%, 50\%, 75\%, 100\%\}$. 
In our main experiments, we set training epoch $e_\mathsl = e_{\mathssl} = 100$, \lossratio $\alpha = 0.5$, and \mixratio $\rho = 0.5$;
we conduct detailed parameter studies for these quantities in \cref{sec:parameter_study}. We defer our experimental details to  \cref{sec:implement}.
\looseness=-1




\subsection{Main Results}
We conduct experiments on the single-task setting in \cref{sec:single_main}, with the same dataset for \SSLalone and \SL, and \cref{sec:diff_main}, with different datasets for \SSLalone and \SL.
We conduct experiments on the multi-task setting in \cref{sec:mul_main}.

\subsubsection{Performance Analysis of Single-Task Learning Setting}
\label{sec:single_main}

In this section, we evaluate the performance of \mixtraining in the single-task setting with various data limitation levels.
We conduct experiments on CIFAR-10, CIFAR-100, and TinyImageNet datasets, and show the results in \cref{tab:main_results_tab1}.
As expected, the comparison between \SSL and \SL reflects a compute-performance trade-off: \SSL achieves better accuracy at the cost of larger latency. 
Our \mixtraining method achieves a Pareto improvement over the \SSL baseline: \mixtraining achieves higher accuracy and lower latency in 15 out of 15 settings.
Compared to baselines, 
\mixtraining achieves significant accuracy gains:
for instance, on the full TinyImageNet dataset, \mixtraining achieves 18.89\% relative accuracy gain (8.81\% absolute accuracy gain) over \SSL and 32.17\% relative accuracy gain (13.50\% absolute accuracy gain) over \SL. 
The accuracy gains are more significant under limited data:
for instance, on the TinyImageNet dataset and at data limitation level of 10\%, \mixtraining achieves 105.58\% relative accuracy gain (10.78\% absolute accuracy gain) over \SSL and 189.92\% relative accuracy gain (13.75\% absolute accuracy gain) over \SL. 
In terms of computation cost (reflected as training latency), \mixtraining saves more compute and achieves 1.30$\times$ speedup compared to \SSL.
These results show that \mixtraining provides a better compute-performance trade-off compared to the standard $\SSL$ pipeline in the single-task setting across various data limitation levels.



\subsubsection{Self-supervised and Supervised Learning on Different Datasets}
\label{sec:diff_main}
In this section, we evaluate the performance of \mixtraining in settings where the full self-supervised learning dataset and supervised learning dataset are different. We conduct self-supervised learning on the TinyImageNet dataset, and supervised learning on the CIFAR-10 or CIFAR-100 datasets. We show the results in \cref{tab:add_results}. The comparison between \SSL and \SL can still reflect the compute-performance trade-off: \SSL achieves better accuracy at the cost of larger latency. Our \mixtraining method achieves a Pareto improvement over the \SSL baseline: \mixtraining achieves higher accuracy and lower latency in all settings. Compared to baselines, \mixtraining achieves significant accuracy gains: for instance, on the CIFAR-10 dataset, \mixtraining achieves 5.58\% relative accuracy gain (4.71\% absolute accuracy gain) over \SSL and 9.09\% relative accuracy gain (7.43\% absolute accuracy gain). In terms of computation cost (reflected as training latency), \mixtraining saves more compute and achieves 1.32$\times$ speedup compared to \SSL. These results show that \mixtraining can provide a better compute-performance trade-off compared to the standard \SSL pipeline for the more general setting with different self-supervised learning and supervised learning datasets.


\subsubsection{Performance Analysis of multi-task Setting}
\label{sec:mul_main}
In this section, we assess the effectiveness of \mixtraining in the multi-task setting under different levels of data constraints. We perform the experiments on CIFAR-10 and SVHN datasets and show the results in \cref{tab:main_results_tab2}. As expected, the comparison between \SSL and \SL demonstrates a compute-performance trade-off: \SSL achieves better accuracy at the cost of larger latency. Our \mixtraining approach achieves a Pareto improvement over the \SSL baseline: \mixtraining achieves higher average accuracy in 4 out of 5 settings and lower total latency in 5 out of 5 settings. Compared to baselines, \mixtraining achieves significant accuracy gains especially when data is limited: for instance, at data limitation level of 10\%, \mixtraining achieves 15.39\% relative accuracy gain (9.35\% absolute accuracy gain) over \SSL and 36.06\% relative accuracy gain (18.58\% absolute accuracy gain) over \SL. In terms of computation cost (reflected as training latency), \mixtraining saves more compute and achieves 1.31$\times$ speedup compared to \SSL. These results show that \mixtraining provides a better compute-performance trade-off compared to the standard \SSL pipeline in the multi-task setting across various data limitation levels.





\begin{table}[!tbp]
\centering
% \scriptsize

\resizebox{0.7\textwidth}{!}{

\begin{tabular}{l|ccc|cc}
\toprule
\multirow{2}{*}{$\alpha$} & \multicolumn{3}{c}{\textbf{Single-Task Learning}} & \multicolumn{2}{c}{\textbf{Multi-Task Learning}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-6}
& \textbf{TinyImageNet} & \textbf{CIFAR-10} & \textbf{CIFAR-100} & \textbf{CIFAR-10} & \textbf{SVHN} \\
\midrule

0.01          & 41.58\% & 79.61\% & 52.58\% & 80.17\% & 91.89\% \\
0.1           & \underline{53.17\%} & 82.19\% & 55.21\% & 84.02\%  & 92.53\% \\
0.5           & \textbf{53.86\%} & \underline{85.75\%} & \textbf{58.88\%} & 87.25\% & \textbf{94.50\%}  \\
0.9           & 48.18\% & \textbf{86.13\%} & \underline{58.47\%} & \textbf{87.54\%} & \underline{94.46\%} \\
0.99          & 42.67\% & 83.75\% & 56.04\% & \underline{87.34\%} & 94.19\% \\

\bottomrule
\end{tabular}
}

\caption{Parameter study on \lossratio{} $\alpha$. We train model with full data and set $\rho = 0.75$; other experimental settings remain the same as Table~\ref{tab:main_results_tab1} and Table~\ref{tab:main_results_tab2}.
The best and second-best accuracies are highlighted in \textbf{bold} and \underline{underline}, respectively. }
\label{tab:parameter_alpha_}
\end{table}


\subsection{Parameter Study}
\label{sec:parameter_study}


In this section, we explore the impacts of varying \lossratio{} $\alpha$, \mixratio{} $\rho$, and training epochs $e_\mathssl$ and $e_\mathsl$ for \mixtraining.

\subsubsection{Impact of \lossratio{} $\alpha$}
We study the impact of varying hyperparameter \lossratio{} $\alpha$ on model accuracy in this section. 
We conduct experiments with $\alpha \in \crl{0.01, 0.1, 0.5, 0.9, 0.99}$ and report the accuracy in \cref{tab:parameter_alpha_}; we didn't report the latency since varying $\alpha$ doesn't change the overall computation cost.
As shown in \cref{tab:parameter_alpha_}, \lossratio $\alpha = 0.5$ generally leads to good accuracy gains---either achieving the highest accuracy (3 out of 5) or achieving the second-best accuracy (1 out of 5). This indicates that a well-chosen $\alpha$ should appropriately balance self-supervised learning and supervised learning objectives in \mixtraining. An $\alpha = 0.5$ allows the model to focus on both self-supervised learning and supervised learning objectives. Therefore, we recommend setting $\alpha = 0.5$ in experiments.

\begin{table}[!tbp]
\centering
\resizebox{0.7\textwidth}{!}{
\begin{tabular}{l|ccc|cc}
\toprule
\multirow{3}{*}{$\rho$} & \multicolumn{3}{c}{\textbf{Single-Task Learning}} & \multicolumn{2}{c}{\textbf{Multi-Task Learning}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-6}
& \textbf{TinyImageNet} & \textbf{CIFAR-10} & \textbf{CIFAR-100} & \textbf{CIFAR-10} & \textbf{SVHN} \\
\midrule
0.25 & \underline{55.08\%}   & 85.49\% &  \textbf{60.98\% } & \underline{87.37\%}  & \textbf{94.46\%} \\
0.50 & \textbf{55.40\%}  & 85.54\%  & \underline{60.15\%} & 86.46\%  & 94.21\%  \\
0.75 & 53.86\%  & \textbf{85.75\%}  & 58.88\% & 86.35\%  & 93.66\% \\
1.00 & 49.21\% & 84.33\%  & 55.80\% & 85.50\%  & 92.95\% \\

\bottomrule
\end{tabular}
}
\caption{Parameter study on \mixratio{} $\rho$. We train model with full data and set $\alpha = 0.5$; other experimental settings remain the same as Table~\ref{tab:main_results_tab1} and Table~\ref{tab:main_results_tab2}.
The best and second-best accuracies are highlighted in \textbf{bold} and \underline{underline}, respectively.
}
\label{tab:parameter_study_rho}
\end{table}

\subsubsection{Impact of \mixratio{} $\rho$}

\begin{table*}[h]
\centering


\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccc}
\toprule
\multirow{2}{*}{\textbf{Datasets}} &\multirow{2}{*}{\textbf{Computation (epoch)}}& \multicolumn{2}{c}{\textbf{\SL}} & \multicolumn{2}{c}{\textbf{\SSL}}& \multicolumn{2}{c}{\textbf{\mixtraining}}\\
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}&
 & Accuracy$\uparrow$ & Latency(s)$\downarrow$  & Accuracy$\uparrow$ & Latency(s)$\downarrow$  & Accuracy$\uparrow$ & Latency(s)$\downarrow$ \\
\midrule
\multirow{3}{*}{\textbf{TinyImageNet}}       &  50                  & 7.69\% & 536.82   & 8.67\% & 1136.96  & \textbf{16.53\%} & 896.16   \\
   & 75         & {7.62\%} &809.11    & {9.89\%} &  1762.56  & \textbf{20.23\%} &1376.22  \\

                              &100  & 7.24\% &1102.42   & 10.21\% & 2382.06  & \textbf{20.99\%} &1913.75  \\
\midrule
 \multirow{3}{*}{\textbf{CIFAR-10}} &50           & {53.28\%} & 287.83 & 54.75\% &  637.27 & \textbf{57.60\%} &  473.80  \\
     &75                    & 53.07\% &  434.86  & 55.59\% &  937.01 & \textbf{58.59\%} & 742.53   \\
 & 100       & 53.40\% &  589.44  & {56.79\%} &  1253.65 & \textbf{60.45\%} &  962.16  \\
\midrule
  \multirow{3}{*}{\textbf{CIFAR-100}}     &50                      & 19.08\% &  294.14 & 20.26\% &  677.26 & \textbf{23.36\%} &  516.54\\
   &75        & {19.39\%} &  464.97 & {21.15\%} &  981.43 & \textbf{24.27\%} &  770.75  \\

        &100    & 19.05\% &  609.11 & 22.27\% & 1279.78 & \textbf{25.19\%} &  1010.94 \\

\bottomrule
\end{tabular}
}
\caption{Parameter study on training epochs in single-task learning setting under 10\% data limitation. Accuracy gains are highlighted in \textbf{bold}; results under other data limitation levels are deferred to \cref{sec:appendix_more_parameter_study_epoch}. 
\mixtraining achieves a Pareto improvement over the \SSL baseline: \mixtraining achieves higher accuracy and lower latency in 9 out of 9 settings.
}
\label{tab:parameter_results_epoch}
\end{table*}





We study the impact of varying the hyperparameter \mixratio $\rho$ on model accuracy in this section. We conduct experiments with $\rho \in \crl{0.25, 0.5, 0.75, 1}$ and report the accuracy in \cref{tab:parameter_study_rho}; we didn't report latency since large $\rho$ reduces latency, as analyzed in \cref{sec:mixtraining}.
In the single-task setting, $\rho = 0.5$ or $\rho = 0.25$ leads to better accuracy; in the multi-task setting, $\rho = 0.25$ leads to better accuracy. 
Since larger $\rho$ reduces latency (\cref{sec:mixtraining}), its selection should be guided by individual priorities, such as accelerating the learning process or achieving higher accuracy.



\subsubsection{Impact of training epochs}
We study the impact of varying training epochs $e_\mathssl$, $e_\mathsl$ on model accuracy and training latency. 
For simplicity, we set $e_\mathssl = e_\mathsl$ and choose its value from \{50, 75, 100\}.
We present results of single-task learning under 10\% data limitation, and defer the complete experimental results to \cref{app:experiment_results}.
We see that the \SL baseline doesn't benefits from the added computation: increasing the training epochs from 50 to 100 leads to minimal or no accuracy gains. 
On the other side, \SSL and \mixtraining achieves higher accuracy with added compute. 
Compared to \SSL, \mixtraining achieves higher accuracy and lower latency in 9 out of 9 settings; these improvements demonstrate a better compute-performance trade-off.


\subsection{\mixtraining Learns Robust Representations}
We evaluate the effectiveness of \mixtraining in preserving self-supervised objectives by visualizing several raw and reconstructed images in \cref{fig:visualization_}.
The standard \SSL approach (second row) significantly deteriorates model's reconstruction ability, failing to reconstruct the original images effectively. This degradation is likely due to the supervised learning phase interfering with the knowledge acquired during self-supervised learning. On the other hand, \mixtraining (third row) preserves its reconstruction ability: images reconstructed by the model trained with \mixtraining closely resemble the original images, albeit with some added noise. These results show that \mixtraining enables the model to retain its reconstruction ability \textit{even after standard supervised learning}.
We hypothesize that \mixtraining facilitates the learning of more robust feature representations and mitigates catastrophic forgetting, thereby preserving the modelâ€™s ability to maintain self-supervised objectives throughout subsequent training phases.
\looseness=-1


\begin{figure}[!tbp]
\centering
\includegraphics[width=.8\textwidth]{figure/figure3.pdf}
\caption{Reconstruction examples from TinyImageNet. The \SSL approach significantly deteriorates the reconstruction abilities (second row), yet \mixtraining{} maintains competitive reconstruction abilities (third row).
}
\label{fig:visualization_}
\end{figure}

