\documentclass[journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{array}
% \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}

% Ours
\usepackage{amsmath}
\usepackage{threeparttable}
\usepackage{booktabs}
\usepackage{makecell}
%\usepackage{wrapfig}  % can not be used  officially
\usepackage{multirow}
\usepackage{bbding}
\usepackage[square,sort,comma,numbers]{natbib}

\usepackage{amssymb}
\usepackage[table]{xcolor}

\usepackage{caption}     % 用于自定义标题
\usepackage{subcaption}  % 用于创建子图

\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{balance}
\begin{document}
\title{Enhancing Audio-Visual Spiking Neural Networks through Semantic-Alignment and Cross-Modal Residual Learning}
\author{Xiang He\textsuperscript{*} \thanks{Xiang He is with the Brain-inspired Cognitive Intelligence Lab, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China, and School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing 100049, China.},
Dongcheng Zhao\textsuperscript{*} \thanks{Dongcheng Zhao is with the Brain-inspired Cognitive Intelligence Lab, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China.},
Yiting Dong \thanks{Yiting Dong and Guobin Shen are with the Brain-inspired Cognitive Intelligence Lab,Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China, and School of Future Technology, University of Chinese Academy of Sciences, Beijing 100049, China.},
Guobin Shen, %\thanks{Guobin Shen is with the Brain-inspired Cognitive Intelligence Lab,Institute of Automation, Chinese Academy of Sciences, Beijing 100190,China, and School of Future Technology, University of Chinese Academy of Sciences, Beijing 100049, China.}
Xin Yang, \thanks{Xin Yang is with the CAS Key Laboratory of Molecular Imaging, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China.}
Yi Zeng \thanks{Yi Zeng is with the Brain-inspired Cognitive Intelligence Lab, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China, and Center for Long-term Artificial Intelligence, Beijing 100190, China, and University of Chinese Academy of Sciences, Beijing 100049, China, and Key Laboratory of Brain Cognition and Brain-inspired Intelligence Technology, Chinese Academy of Sciences, Shanghai, 200031, China. }

\thanks{
  \textsuperscript{*}These authors contributed equally.}

\thanks{
  The corresponding author is Xin Yang (e-mail: xin.yang@ia.ac.cn) and Yi Zeng (e-mail: yi.zeng@ia.ac.cn).
}

}



\markboth{Journal of \LaTeX\ Class Files,~Vol.~18, No.~9, September~2020}%
{How to Use the IEEEtran \LaTeX \ Templates}

\maketitle

\begin{abstract}
    %   作为大脑启发的计算模型，脉冲神经网络（SNN）在模拟大脑信息处理方面具有独特的优势。然而，现有的脉冲神经网络模型主要侧重于单模态处理，缺乏高效的跨模态信息融合，从而限制了其在现实世界多模态场景中的适用性。
    % 为解决这一局限，我们提出了一种语义感知跨模态残差学习（S-CMRL）框架，该框架基于基于变换器的多模态 SNN 架构，可实现视觉和听觉信息的有效融合。S-CMRL 利用时空脉冲注意机制提取跨模态互补特征，采用跨模态互补设计加强特征整合，并进一步采用语义相关优化机制加强跨模态特征的一致性和互补性。在 CREMA-D、UrbanSound8K-AV 和 MNIST-DVS-NTIDIGITS 上进行的大量实验表明，S-CMRL 优于现有的多模态 SNN 方法。为了便于重现，我们在 https://github.com/Brain-Cog-Lab/S-CMRL 上公开了代码。
    Humans interpret and perceive the world by integrating sensory information from multiple modalities, such as vision and hearing. Spiking Neural Networks (SNNs), as brain-inspired computational models, exhibit unique advantages in emulating the brain’s information processing mechanisms. However, existing SNN models primarily focus on unimodal processing and lack efficient cross-modal information fusion, thereby limiting their effectiveness in real-world multimodal scenarios.
    To address this challenge, we propose a semantic-alignment cross-modal residual learning (S-CMRL) framework, a Transformer-based multimodal SNN architecture designed for effective audio-visual integration. S-CMRL leverages a spatiotemporal spiking attention mechanism to extract complementary features across modalities, and incorporates a cross-modal residual learning strategy to enhance feature integration. Additionally, a semantic alignment optimization mechanism is introduced to align cross-modal features within a shared semantic space, improving their consistency and complementarity. Extensive experiments on three benchmark datasets CREMA-D, UrbanSound8K-AV, and MNISTDVS-NTIDIGITS demonstrate that S-CMRL significantly outperforms existing multimodal SNN methods, achieving the state-of-the-art performance. The code is publicly available at \texttt{https://github.com/Brain-Cog-Lab/S-CMRL}.
\end{abstract}

\begin{IEEEkeywords}
Spiking Neural Networks, Audio-Visual Learning, Semantic-Alignment Cross-Modal Residual Learning
\end{IEEEkeywords}


\section{INTRODUCTION}

% 人类对外部世界的感知来自于对视觉、听觉和语言等多种模态信息的整合。与依赖单一模态相比，多模态特征能提供更丰富、更全面的信息表征。此外，多模态的整合还能增强感知的稳健性，促进对环境更深入的理解和更有效的互动~/cite{ernst2004merging, noppeney2021perceptual}。在这些感知模式中，视觉和听觉是获取外部信息的两个主要渠道~（cite{enoch2019evaluating, bulkin2006seeing}，它们的整合在日常生活中起着举足轻重的作用。例如，在弱光环境下，听觉线索可以弥补视觉信息的不足，通过多种模式的融合增强对外部环境的感知并减少不确定性。近年来，视听多模态学习在多个领域取得了显著进展，包括视听语音识别~/cite{kim2021cromm, peng2022balanced, yeo2024akvsr}、视频声音分离~/cite{gan2020music}、视频声源定位~/cite{hu2020discriminative}和视听事件定位~/cite{lin2019dual, feng2023css, he2024cace}。这些发展凸显了利用视觉和听觉通道的互补信息来解决感知领域复杂挑战的重要性。
Human perception of the external world arises from the integration of information across multiple modalities, including vision, hearing, and language. Compared to unimodal perception, multimodal learning provides a richer and more comprehensive representation of information. Furthermore, the integration of multiple modalities enhances perceptual robustness, facilitating a deeper understanding of the environment~\cite{ernst2004merging, noppeney2021perceptual}. 
Among these modalities, vision and hearing serve as the two primary sensory pathways for acquiring external information~\cite{enoch2019evaluating, bulkin2006seeing}, and their integration plays a pivotal role in daily life. For example, in low-light environments, auditory cues can compensate for insufficient visual information, enabling a more accurate perception of the external environment and reducing uncertainty. In recent years, audio-visual multimodal learning has achieved remarkable advancements in various applications, such as audio-visual speech recognition~\cite{kim2021cromm, peng2022balanced, yeo2024akvsr}, video sound separation~\cite{gan2020music}, video sound source localization~\cite{hu2020discriminative}, and audio-visual event localization~\cite{lin2019dual, feng2023css, he2024cace}. These developments highlight the importance of leveraging complementary information from both visual and auditory modalities to address complex challenges in perception.

\begin{figure}[t]
	\centering
		\includegraphics[width=1.0\linewidth]{fig/compare.pdf}
	\caption{ %  不同的脉冲神经网络跨模态融合方法。(a) 香草融合, 一般通过直接相加各模态特征进行融合; (b) 带有跨模态注意力机制的融合; (c) 我们提出的语义感知跨模态残差学习融合。其中，``Q'': 查询嵌入; ``K''： 键嵌入； ``V''： 值嵌入。
  Different cross-modal fusion methods in spiking neural networks. (a) Direct fusion, which typically sums the features from different modalities directly. (b) Fusion with cross-modal attention mechanisms. (c) Our proposed semantic-alignment cross-modal residual learning fusion.
  ``Q'': Query embedding; ``K'': Key embedding; ``V'': Value embedding.}
	\label{fig_compare}
\end{figure}


% 脉冲神经网络（SNN）是受生物神经系统启发的计算模型。其独特的特性和架构为模仿大脑的信息处理机制提供了固有的优势。与传统的人工神经网络（ANN）不同，SNN 通过离散的脉冲序列传输信息。与传统的人工神经网络相比，SNN 固有的稀疏脉冲和事件驱动计算模式大大降低了功耗。与大脑类似，这些脉冲按时间编码信息，使 SNNs 能够处理时空数据并有效适应动态环境。近年来，SNNs 在计算机视觉等各种应用中都表现出了卓越的性能~/cite{deng2022temporal, zhou2022spikformer, li2022spike, shen2023brain、 feng2024spiking, xie2024eisnet}，自然语言处理~/cite{shen2023astrocyte, su2024snn, xiao2022towards}，以及音频处理~/cite{wang2024spikevoice, yang2024svad, pan2021multi}。
Spiking Neural Networks (SNNs), inspired by biological nervous systems, provide a compelling computational paradigm characterized by event-driven information processing and sparse activation. Unlike traditional Artificial Neural Networks (ANNs), SNNs transmit information through discrete spike sequences. The sparse spikes and event-driven computation paradigm inherent to SNNs significantly reduces power consumption. Similar to the brain, these spikes encode information temporally, enabling SNNs to handle spatiotemporal data.
In recent years, SNNs have demonstrated impressive performance across diverse applications, including computer vision~\cite{deng2022temporal, zhou2022spikformer, li2022spike, shen2023brain, feng2024spiking, xie2024eisnet}, natural language processing~\cite{xiao2022towards, shen2023astrocyte, su2024snn}, and audio processing~\cite{pan2021multi, wang2024spikevoice, yang2024svad}.

% 尽管SNN研究取得了重大进展，但大多数现有模型都侧重于单模态处理，对多模态SNN的探索十分有限。相比之下，人类智能本身就是多模态的：婴儿在发育过程中表现出多模态学习范式，而成年人则自然而然地参与多模态表征~/cite{lin2023multimodality}。因此，作为大脑启发的神经网络模型，尖峰神经网络（SNN）自然会朝着解决多模态感知挑战的方向发展。然而，目前关于多模态 SNN 的研究往往无法有效整合多模态信息。它们往往只是简单地将两种模态的特征结合起来~/cite{zhang2020efficient, liu2022event}，或者忽略了听觉和视觉模态的不同特点，对模态间的互补信息缺乏足够的挖掘~/cite{guo2023transformer、 jiang2023cmci}，如图（ref{fig_compare}(a)和图（ref{fig_compare}(b)）所示，从而阻碍了先进的多模态脉冲神经网络的发展。
Despite significant progress in SNN research, most existing models focus on unimodal processing, with limited exploration of multimodal SNNs. 
In contrast to human cognition, where multimodal integration is fundamental~\cite{zaadnoordijk2022lessons, lin2023multimodality}, 
existing multimodal SNN approaches often fall short in effectively integrating multimodal information. Some studies simply combine the features of two modalities in a straightforward manner~\cite{zhang2020efficient, liu2022event}, while others overlook the distinct characteristics of auditory and visual modalities and lack sufficient exploration of the complementary information between modalities~\cite{guo2023transformer, jiang2023cmci}, as illustrated in Fig.~\ref{fig_compare}(a) and Fig.~\ref{fig_compare}(b). These methods do not fully exploit intermodal complementarity, which limits their effectiveness in multimodal learning.

% 多模态学习的一个基本挑战来自于不同模态之间特征分布的固有差异。由于不同模态之间的分布存在差异，若仅仅将它们的特征进行直接融合，往往会引发模态间的信息冲突。例如，当视觉模型在某些情况下性能较弱而听觉模型表现更佳时，引入的跨模态特征反而会受到低质量视觉信息的干扰，从而导致性能欠优。为验证这一点，我们选取了 Guo 等人~\cite{guo2023transformer}提出的跨模态注意力方法来提取跨模态特征。随后，我们将跨模态特征视作对单模态特征的“互补”，并通过残差的方式与原有单模态特征进行融合。我们在CRMEA-D数据集中进行了实验，如图~\ref{with_res}所示。实验结果显示，保留单模态特征并在此基础上额外引入跨模态残差，可以获得更优的模型性能。该结果启示我们，应当在充分利用模态间互补性的同时，兼顾视觉和听觉模态中各自的独有特征，以实现高效的视听模态整合。
A fundamental challenge in multimodal learning arises from the inherent differences in feature distributions across modalities. Consequently, a naive direct fusion of these features often results in conflicts between modalities. For example, when the visual data is less informative in some situations while auditory data is more discriminative, the introduced cross-modal features may be interfered by low-quality visual information, leading to suboptimal performance. To verify this, we select the cross-modal attention method proposed by Guo et al.~\cite{guo2023transformer} to extract cross-modal features. Subsequently, we regard the cross-modal features as “complementary” to the unimodal features and fuse them as residuals with the original unimodal features. We conduct experiments using the spiking Transformer in the CRMEA-D~\cite{cao2014crema} dataset, as shown in Fig.~\ref{with_res}. The experimental results show that preserving the unimodal features while incorporating cross-modal residuals can achieve better model performance. This result highlights the importance of preserving modality-specific representations while leveraging complementary intermodal information to achieve efficient audio-visual modal integration.


\begin{figure}[t]
	\centering
		\includegraphics[width=0.9\linewidth]{fig/withres.pdf}
	\caption{ %  Spiking Transformer在CRMEA-D数据集上的实验结果。CMRL代表Cross-Modal Residual Learning。在 CRMEA-D 数据集中，视觉信息比音频信息更弱。由于模态间的分布差异，现有的跨模态融合技术只能带来有限的改进。当把跨模态特征作为残差纳入原始模态特征时，所观察到的性能提升凸显了保留原始模态语义特征的重要性。
  Experimental results of the Spiking Transformer on the CRMEA-D dataset.
  CMRL represents Cross-Modal Residual Learning. Due to the weaker visual signals compared to audio in CREMA-D, traditional cross-modal fusion strategies show limited improvement. When incorporating cross-modal features as residuals into the unimodal representations, model performance improves. This highlights the importance of preserving the unimodal-specific semantic features.}
	\label{with_res}
\end{figure}

% 基于上述想法，我们提出了一种语义感知跨模态学习（S-CMRL）框架，该框架基于变压器驱动的多模态脉冲神经网络，用于视听学习。所提出的模型将音频和视频数据编码为顺序输入，并采用跨模态脉冲注意机制来引入语义信息，从而指导跨模态特征的残差学习。通过有效整合来自不同模态的互补信息，S-CMRL 框架增强了视觉和听觉数据之间的协同作用，解决了特定模态之间的冲突，提高了整体多模态学习性能。
To address the above challenge, we propose a \textbf{S}emantic-Alignment \textbf{C}ross-\textbf{M}odal \textbf{R}esidual \textbf{L}earning (S-CMRL) framework, which is based on a Transformer-driven multimodal spiking neural network for audio-visual learning. As illustrated in Fig.~\ref{fig_compare}(c), the proposed framework encodes audio and visual data as sequential inputs and employs a cross-modal spiking attention mechanisms to introduce semantic information that guides the residual learning of cross-modal features.
By effectively integrating cross-modal complementary information, S-CMRL enhances the intermodal collaboration while mitigating conflicts that arise from direct fusion. 

%具体来说，我们的框架由两个主要模块组成。首先，我们提出了跨模态互补时空尖峰注意（CCSSA）模块。该模块扩展了传统的单模态尖峰转换器，以适应双模态输入，并从另一种模态中提取互补语义信息。这些互补特征被视为 “残差”，融合到原始模态特征中，从而实现有效的跨模态整合，同时保留模态特有的特征。这种方法缓解了简单融合方法带来的冲突，增强了网络应对复杂场景的能力。其次，我们引入了一种语义关联优化机制来完善跨模态残差特征。通过在共享语义空间内调整视觉和听觉模式中同一类别的特征，该机制加强了一致性并提高了互补表征的质量。
Specifically, our framework consists of two primary modules. 
First, we propose a cross-modal complementary spatiotemporal spiking attention (CCSSA) module. This module extends the traditional unimodal spiking Transformer to accommodate dual-modal inputs and extract complementary semantic information from another modality. These complementary features, treated as “residuals,” are fused into the original modality’s features, allowing effective cross-modal integration while preserving modality-specific characteristics. This approach mitigates conflicts from simple fusion methods and enhances the network’s ability to address complex scenarios. Second, we introduce a semantic alignment optimization (SAO) mechanism to refine cross-modal residual features. By aligning cross-modal features from the same category across visual and auditory modalities within a shared semantic space, this mechanism reinforces consistency and improves the quality of complementary representations. 

% 我们在视听分类数据集 CREMA-D 和 UrbanSound8K-AV 以及神经形态数据集 MNISTDVS-NTIDIGITS 上评估了我们的框架。实验结果表明，我们的多模态 SNN 在所有三个数据集上都达到了最先进的性能，即使在噪声条件下也能获得稳健的结果。
We evaluate our framework on three audio-visual datasets CREMA-D and UrbanSound8K-AV, as well as the neuromorphic dataset MNISTDVS-NTIDIGITS. Experimental results demonstrate that our multimodal SNN achieves state-of-the-art performance on all three datasets, while exhibiting strong robustness under noisy conditions. 

In summary, the main contributions of this paper can be summarized as follows:

% Specifically, our framework consists of two core modules. First, we propose a cross-modal complementary spatiotemporal spiking attention mechanism. Our model extends the traditional single-modal spiking Transformer, encoding single-modal information to obtain input features and then employing cross-modal complementary spatiotemporal spiking attention to extract complementary semantic information from the other modality. We treat these complementary features as “residuals” fused into the original modality’s unique features. This design not only allows each modality to benefit from cross-modal complementary features but also preserves modality-specific characteristics, thus avoiding the conflicts often caused by simple fusion methods and enhancing the network’s ability to handle complex scenarios.

% Second, we propose a semantic correlation optimization Mechanism to further refine the complementary semantic features in the cross-modal residual branch. Specifically, within the same batch of data, we consider samples from both the visual and auditory modalities belonging to the same category as positive sample pairs, and through semantic alignment, we reinforce the semantic-level consistency of cross-modal features. This semantic-enhanced optimization strategy ensures the high-quality representation of residual information, thereby achieving more accurate modality-complementary representations.

% We validate our method on the audio-visual classification datasets CREMA-D and UrbanSound8K-AV, as well as on the neuromorphic dataset MNISTDVS-NTIDIGITS. Experimental results demonstrate that our multimodal SNN model outperforms other existing methods, achieving optimal performance on all three datasets. Moreover, our network remains robust even in the presence of noise. In summary, the main contributions of this paper can be summarized as follows:

\begin{itemize} 
  \item We propose a novel cross-modal complementary spatiotemporal spiking attention mechanism, which effectively integrates cross-modal complementary information while preserving modality-specific semantic information, thereby enhancing representational expressiveness.
  \item We propose a semantic alignment optimization mechanism to align cross-modal features within a shared semantic space, improving cross-modal feature consistency and overall multimodal learning performance. 
  \item Based on these two modules, we construct a semantic-alignment cross-modal residual learning framework for multimodal SNNs. This framework provides an efficient feature fusion strategy and achieves state-of-the-art performance on three public datasets, demonstrating superior accuracy and robustness compared to existing methods. 
\end{itemize}

\section{RELATED WORK}
\subsection{Audio-Visual Learning}
% 在神经网络中，多模态融合整合了两种或两种以上的模态，以应对复杂的任务，并提高模型的准确性和泛化能力。视听学习主要侧重于揭示视觉和听觉模态之间的关系，并将特征融合作为核心研究重点。Hu 等人 ~\cite{hu2016temporal} 和 Yang 等人 ~\cite{yang2020telling} 探索了特征串联，沿特定维度融合视听特征，生成统一的特征向量。除了简单的并集之外，Wu 等人还提出了双重注意力匹配模块，以促进更高层次的事件信息建模。此外，一些研究还解决了单模态不平衡的问题~/cite{feichtenhofer2019slowfast, peng2022balanced}，通过加强跨模态对齐和减少模态间差异来优化融合。Yeo 等人~\cite{yeo2024akvsr}利用音频模式作为补充源，解决了视觉模式中语音信息不足的问题。在视听模式中，视觉部分通常提供空间信息，而听觉部分则捕捉时间动态；因此，每种模式都呈现出不同的表征形式和语义信息。因此，设计一个能够动态融合这些互补元素的网络，已成为视听学习研究中的一个关键挑战。
In neural networks, multimodal fusion integrates two or more modalities to tackle complex tasks and improve model accuracy and generalization. Audio-visual learning primarily focuses on uncovering relationships between visual and auditory modalities, with feature fusion as the central research focus. Hu et al.~\cite{hu2016temporal} and Yang et al.~\cite{yang2020telling} explored feature concatenation, fusing audio-visual features along specific dimensions to generate a unified feature vector. Moving beyond simple concatenation, Wu et al.~\cite{wu2019dual} proposed a dual attention matching module to facilitate higher-level event information modeling. Moreover, certain studies addressed the issue of single-modal imbalance~\cite{feichtenhofer2019slowfast, peng2022balanced} by strengthening cross-modal alignment and reducing inter-modal discrepancies to optimize fusion. Yeo et al.~\cite{yeo2024akvsr} tackled the problem of insufficient speech information in the visual modality by leveraging the audio modality as a complementary source. In audio-visual modalities, the visual modality typically provides spatial information, whereas the auditory component captures temporal dynamics; each modality thus presents distinct representational patterns and semantic information. Consequently, designing a network that can dynamically fuse these complementary elements has become a pivotal challenge in audio-visual learning research.

\subsection{Unimodal Spiking Neural Networks}
% 现有的脉冲神经网络研究工作大多集中在对单模态的研究上。

% 在视觉相关任务中，Wu等人~\cite{wu2018spatio}对发射脉冲的函数进行梯度近似以实现梯度计算，并在空间域和时间域分别使用反向传播，即时空反向传播（Spatio-Temporal Backpropagation，STBP）。在其进一步研究中~\cite{wu2019direct}，将常用的LIF神经元模型~\cite{dayan2005theoretical}表达为离散的可迭代的形式。Deng等人~\cite{deng2022temporal}引入了时间效率训练方法，使SNN收敛到更平坦的最小值。Zhou等人~\cite{zhou2022spikformer}提出了Spikformer，利用脉冲自注意力机制，通过使用脉冲形式的Query、Key和Value来建模稀疏的视觉特征。
% 在音频相关任务中，Auge等人~\cite{auge2021end}使用共振神经元作为脉冲神经网络的输入层，用于在线音频分类。Yu等人~\cite{yu2022stsc}提出了由时间响应滤波器模块和前馈横向抑制模块组成的时空突触连接模块, 在语音数字识别上进行了实验验证。
% 尽管在单一模态上取得了显著进展, 随着多模态学习的兴起，结合不同模态信息以增强模型的表达能力已成为提升模型泛化性的有效途径。因此，如何将传统的单模态SNN技术扩展到多模态场景，成为当前及未来脉冲神经网络领域中的重要挑战之一。

Most existing research on spiking neural networks (SNNs) focuses on single-modal tasks. 
For vision-related tasks, Wu et al.~\cite{wu2018spatio} introduced gradient approximation of spike functions for gradient computation, applying backpropagation in both the spatial and temporal domains, namely Spatio-Temporal Backpropagation (STBP). In subsequent work~\cite{wu2019direct}, they presented a discrete and iterative form of the commonly used LIF neuron model~\cite{dayan2005theoretical}. Deng et al.~\cite{deng2022temporal} proposed a temporal-efficient training approach that converges SNNs to flatter minima. Zhou et al.~\cite{zhou2022spikformer} introduced Spikformer, incorporating a spiking self-attention mechanism that leverages spike-based queries, keys, and values to capture sparse visual features.
In audio-related tasks, Auge et al.~\cite{auge2021end} employed resonator neurons as the input layer of an SNN for online audio classification. Yu et al.~\cite{yu2022stsc} proposed a spatiotemporal synaptic connection module composed of a temporal response filter module and a feedforward lateral inhibition module, demonstrating its efficacy in spoken digit recognition.
Although unimodal approaches have achieved significant progress, the rising importance of multimodal learning highlights the need to integrate diverse modalities to enhance the representational capacity and generalization of models. Consequently, extending traditional unimodal SNN techniques to multimodal contexts is emerging as a key challenge in the current and future development of spiking neural networks.

\subsection{Audio-Visual Spiking Neural Networks}
% 视听脉冲神经网络的研究相对较少，早期的一些工作主要局限于简单地连接或叠加视觉和听觉模态的特征。Zhang等人~\cite{zhang2020efficient}通过兴奋性和抑制性侧连接的方式，将针对单模态训练的SNN进行跨模态的耦合；Liu等人~\cite{liu2022event}提出了一种基于注意力机制的跨模态网络，利用注意力机制评估各模态的重要性，并将权重分配给两种模态，以实现跨模态融合。随后，研究者们开始设计更为优越的多模态融合方式以适应多模态脉冲神经网络的发展。Guo等人~\cite{guo2023transformer}将SNN和Transformer相结合，整合了视觉和听觉的单模态子网络，并引入新型的脉冲交叉注意模块，用于多模态视听分类；Jiang等人~\cite{jiang2023cmci}提出了一种跨模态电流集成模块，对不同模态的脉冲神经网络进行特征级或决策级融合。尽管如此，这些方法仍然忽视了听觉和视觉模态的独有特征以及其间的互补作用。在我们的工作中，我们保留了听觉和视觉模态的独有特征，并使单模态能够从跨模态的互补特征中受益，从而实现更高效的跨模态融合。
Research on audio visual spiking neural networks (AV-SNNs) remains limited. Early studies primarily focused on simple connections or straightforward combinations of visual and auditory modality features. Zhang et al.~\cite{zhang2020efficient} employed excitatory and inhibitory lateral connections to facilitate cross-modal coupling in SNNs trained on individual modalities. Liu et al.~\cite{liu2022event} introduced an attention-based cross-modal network that leverages an attention mechanism to weigh each modality's contribution, enabling cross-modal fusion.
More recently, researchers have developed advanced multimodal fusion methods to improve AV-SNNs. Guo et al.~\cite{guo2023transformer} integrated SNNs with Transformers, combining unimodal sub-networks for visual and auditory modalities and proposing a novel spiking cross-attention module for audio-visual classification. Jiang et al.~\cite{jiang2023cmci} proposed a cross-modal current integration module that fuses SNNs from different modalities at either the feature or decision level.
However, existing approaches overlook the unique characteristics of auditory and visual modalities and their complementary interactions. In this work, we preserve the distinct features of each modality and design mechanisms that enable each unimodal branch to leverage complementary information from the other. This approach achieves more effective cross-modal fusion and enhances multimodal learning performance.

\section{Preliminary}
% 在本节中，我们首先正式定义了研究问题，并介绍了本研究中使用的关键神经元模型，即 “漏电整合与发射（LIF）神经元 ”和 关键网络模型，即“脉冲Transformer”。这些组件是高效多模态数据融合的基础。
In this section, we first formally define the research problem and introduce the key neuron model used in this study, namely the ``Leaky Integrate-and-Fire (LIF) Neuron'', and the crucial network model, the ``Spiking Transformer''. These components are fundamental to efficient multimodal data fusion.

\subsection{Problem Definition}
% 对于给定的多模态数据集$\mathcal{D}$，可以表示为$\mathcal{D}=\left\{\left(\boldsymbol{x}_{i}^{a}, \boldsymbol{x}_{i}^{v},  y_{i}\right)\right\}_{i=1}^{n_t}$, 其中$\boldsymbol{x}_{i}^{a} \in \mathcal{X}^a$, $\boldsymbol{x}_{i}^{v} \in \mathcal{X}^v$, 分别代表输入音频模态和视觉模态的输入数据，$y_{i} \in \mathcal{Y}$是对应的标签，$n_t$是对应任务的总训练样本数目。
% 研究目标是训练一个具有参数$\theta$的模型$f_{\boldsymbol{\theta}} \colon \mathcal{X}^a  \times \mathcal{X}^v \rightarrow \mathcal{Y}$, 以实现从音频-视觉模态输入到类别标签$\hat y=f_{\boldsymbol{\theta}}\left(\boldsymbol{x}^{a}, \boldsymbol{x}^{v}\right)$的预测：
For a given multimodal dataset $\mathcal{D}$, it can be expressed as $\mathcal{D}=\left\{\left(\boldsymbol{x}_{i}^{a}, \boldsymbol{x}_{i}^{v}, y_{i}\right)\right\}_{i=1}^{n_ t}$, where $\boldsymbol{x}_{i}^{a} \in \mathcal{X}^a$ and $\boldsymbol{x}_{i}^{v} \in \mathcal{X}^v$ denote the input data of the audio modality and visual modality, respectively. $y_{i} \in \mathcal{Y}$ is the corresponding label and $n_t$ is the total number of training samples for the corresponding task.
The objective of our research is to learn a multimodal model with the parameter $\theta$, 
denoted as $f_{\boldsymbol{\theta}}$, to predict class labels from audio-visual inputs:
\begin{equation}
    f_{\boldsymbol{\theta}} \colon \mathcal{X}^a \times \mathcal{X}^v \rightarrow \mathcal{Y}.
\end{equation}

The model is optimized by minimizing the expected risk based on the cross-entropy loss function $\mathcal{L}_{ce}$:
\begin{equation}
    \underset{\boldsymbol{\theta}}{\operatorname{argmin}} \mathbb{E}_{\left(\boldsymbol{x}^a, \boldsymbol{x}^v, y\right) \sim \mathcal{D}} \left[\mathcal{L}_{ce}\left(f_{\boldsymbol{\theta}}\left(\boldsymbol{x}^a, \boldsymbol{x}^v\right), y\right)\right].
\end{equation}
% 其中$\mathcal{L}_{ce}$代表交叉熵损失。
In this study, our primary focus is on designing the model $f_{\boldsymbol{\theta}}$ to achieve efficient cross-modal feature fusion. To this end, we leverage a combination of SNNs and Transformer architectures for the model $f_{\boldsymbol{\theta}}$, aiming to enhance the spatiotemporal modeling capabilities of multimodal feature fusion through efficient spike-based information processing.

\subsection{Leaky Integrate-and-Fire (LIF) Neuron}
% 在脉冲神经网络中，LIF（Leaky Integrate-and-Fire）神经元是最常见的神经元模型。LIF神经元模型的核心思想是，膜电势随着输入电流的累积而增加，并随着时间的推移逐渐泄漏，当累计电流达到某一阈值时，神经元会发放一个脉冲，随后膜电势重置为静息电位$V_{reset}$。在静息电位$V_{reset}$设置为$0$的情况下，LIF模型的膜电位更新公式可以表示为以下离散形式：
The Leaky Integrate-and-Fire (LIF) neuron is a fundamental component of SNNs. Its membrane potential increases with the accumulation of the input current and leaks gradually over time. When the potential reaches a certain threshold, the neuron emits a spike, and subsequently the membrane potential resets to the resting potential $V_{reset}$. With the resting potential $V_{reset}$ set to $0$, the membrane potential update equation of the LIF model can be expressed in the following discrete form:
\begin{gather}
    \boldsymbol{V}^{l}(t) = \boldsymbol{V}^{l} (t-1) + \frac{1}{\tau}
    \left(\boldsymbol{W}^l \boldsymbol{S}^{l-1}(t) - \boldsymbol{V}^{l} (t-1)\right), \\
    \boldsymbol{S}^{l}(t) = \Theta\left(\boldsymbol{V}^{l}(t) - V_{th}\right),\\
    \boldsymbol{V}^{l}(t) = \boldsymbol{V}^{l}(t) \cdot \left(1 - \boldsymbol{S}^{l}(t)\right),
    \label{eq1}
\end{gather}
% 其中，$\tau$ 是泄漏因子，$\boldsymbol{V}^{l}(t)$ 表示在时间步 $t$ 时层 $l$ 中神经元的膜电位。$\boldsymbol{W}^l$ 和 $\boldsymbol{S}^l$ 分别表示层 $l$ 的权重矩阵和在层 $l$ 中发放的脉冲。$\Theta$ 为海维赛德阶跃函数（Heaviside step function）。在本研究中，泄漏因子 $\tau$ 设置为 2.0，阈值 $V_{th}$ 设置为 1.0。
where $\tau$ is the leakage factor and $\boldsymbol{V}^{l}(t)$ denotes the membrane potential of the neuron in layer $l$ at time step $t$. $\boldsymbol{W}^l$ and $\boldsymbol{S}^l$ denote the weight matrix of layer $l$ and the spikes fired in layer $l$, respectively. The $\Theta$ is the Heaviside step function. In our study, the leakage factor $\tau$ is set to 2.0 and the threshold $V_{th}$ is set to 1.0.


\subsection{Spiking Transformer}
SNNs excel in temporal information modeling and energy-efficient computation, while the Transformer architecture is well known for its ability to capture long-range dependencies. To harness the advantages of both, we choose the Spiking Transformer as our backbone model. Specifically, we integrate the Spiking Patch Splitting (SPS) and Spiking Self-Attention (SSA) mechanisms, as proposed in~\cite{zhou2022spikformer}, to improve multimodal feature representation.

In SPS module, input data from both the audio and visual modalities are encoded and projected into a $D$-dimensional spiking feature space, where they are partitioned into fixed-size feature patches of size $N$. Each SPS module consists of multiple submodules, each comprising a convolutional layer, batch normalization, LIF neurons, and a max-pooling layer. After passing the SPS module, the audio and visual inputs, $\boldsymbol{x}^a$ and $\boldsymbol{x}^v$, are transformed into patch sequences:
\begin{equation}
  \boldsymbol{x}^a = \mathcal{SPS}_a (\boldsymbol{x}^a), \quad \boldsymbol{x}^v = \mathcal{SPS}_v (\boldsymbol{x}^v),
\end{equation}
where $\boldsymbol{x}^a, \boldsymbol{x}^v \in \mathbb{R}^{B \times T \times N \times D}$, and $B$, $T$, $N$, and $D$ denote the batch size, temporal length, spatial locations, and feature dimensions, respectively..

Spiking Self-Attention (SSA) is a spike-based variant of the conventional self-attention mechanism, designed to bridge the incompatibility between standard attention operations and spiking attention. In spiking self-attention, queries, keys, and values are represented in a pure spike format. Following the formulation in~\cite{zhou2022spikformer}, the computation is performed as follows:
\begin{equation}
  \begin{gathered}
      Q = \mathcal{S} \mathcal{N}^Q\left(\operatorname{BN}\left(\boldsymbol{x} W^Q\right)\right),\\
      K = \mathcal{S N}^K\left(\operatorname{BN}\left(\boldsymbol{x} W^K\right)\right), \\
      V = \mathcal{S} \mathcal{N}^V\left(\operatorname{BN}\left(\boldsymbol{x} W^V\right)\right), \\
      \operatorname{SSA}(\boldsymbol{x}) = \mathcal{S N}\left(\operatorname{BN}\left(\operatorname{Linear}\left(\mathcal{S N}\left(Q K^{\mathrm{T}} V \cdot s\right) \right)\right)\right),
  \end{gathered}
  \label{ssa}
  \end{equation}
where $Q$, $K$, and $V$ denote the query, key, and value, respectively; $s$ is the scaling factor; $\operatorname{Linear}$ represents the linear transformation layer; BN denotes Batch Normalization; and $\mathcal{SN}$ stands for the spike neuron layer. Notably, spiking self-attention does not change the dimension of the input features. Thus, the output satisfies $\operatorname{SSA}(\boldsymbol{x}) \in \mathbb{R}^{B \times T \times N \times D}$.

By combining SPS and SSA, the Spiking Transformer effectively leverages the sparse activation characteristic of spike neurons while preserving the powerful spatiotemporal information modeling capabilities inherent to the Transformer architecture. This fusion provides efficient and precise representations for multimodal learning.

\section{Methods}
A key challenge in multimodal learning is how to efficiently integrate cross-modal information to fully leverage its complementary characteristics. Traditional approaches typically employ feature concatenation or simple aggregation for classification. For naive multimodal fusion, where different modality features are directly summed, the Spiking Transformer model $f_{\boldsymbol{\theta}}$ can be formulated as:
\begin{gather}
    \boldsymbol{z}^a = g_a(\boldsymbol{x}^a)= \operatorname{SSA}_a(\boldsymbol{x}^a), \;
    \boldsymbol{z}^v = g_v(\boldsymbol{x}^v) = \operatorname{SSA}_v(\boldsymbol{x}^v), \label{eq:12}\\
    f_{\boldsymbol{\theta}}\left(\boldsymbol{x}^a, \boldsymbol{x}^v\right) = h \circ \Gamma\left(\operatorname{MLP}_a(\boldsymbol{z}^a) + \operatorname{MLP}_v(\boldsymbol{z}^v)\right),
\end{gather}
where $g: \mathcal{X} \rightarrow \mathcal{Z}$ represents the mapping from the input space $\mathcal{X}$ to the feature space $\mathcal{Z}$, $\operatorname{MLP}$ refers to a multilayer perceptron layer, and $\Gamma$ denotes global average pooling. The function $h: \mathcal{Z} \rightarrow \mathcal{Y}$ maps features to output predictions, typically implemented as a linear layer.

% 然而，这些天真的策略忽略了模态之间复杂的时空依赖关系，限制了模型的表征能力。此外，由于缺乏对跨模态特征的明确约束，可能会导致语义错位，降低融合的效果。为了克服这些挑战，我们提出了a semantic-alignment cross-modal residual learning (S-CMRL) 框架，our framework consists of two primary modules. 下面将进行详细介绍。
However, this fusion strategy overlooks the complex spatiotemporal dependencies between modalities, limiting the model’s representation capacity. Moreover, the lack of explicit constraints on cross-modal features can result in semantic mismatch, reducing the effectiveness of fusion. 

To overcome these challenges, we propose the semantic-alignment cross-modal residual learning framework, which consists of two primary modules. These two modules are described in detail in the following section.

\begin{figure}[t]
	\centering
		\includegraphics[width=1.0\linewidth]{fig/CCSSA.pdf}
	\caption{ % 跨模态互补时空脉冲注意示意图, 以音频特征的互补特征$\boldsymbol{x}^a_{\text{res}}$的计算过程为例。最好以彩色形式观看。
  Schematic of cross-modal complementary spatio-temporal spiking attention, using the computation process of the complementary feature $\boldsymbol{x}^a_{\text{res}}$ in audio features as an example. Best viewed in color.
  }
	\label{figccssa}
\end{figure}


\subsection{Cross-modal Complementary Spatiotemporal Spiking Attention}
% 在传统的多模态融合方法中，特征映射函数$g$仅处理单一模态的信息，未能充分考虑不同模态之间的信息交互，如公式~\ref{eq:12}所示。这限制了模型利用各模态间互补信息的能力。为了克服这一局限性，为此，我们提出了跨模态互补时空脉冲注意力机制（Cross-modal Complementary Spatiotemporal Spiking Attention, CCSSA）。CCSSA通过捕捉并融合来自另一模态的互补语义特征，提升原始模态特征的表达能力，如图~\ref{figccssa}所示。具体而言，我们将来自另一模态的互补特征作为“残差”，并将其融合到原始模态的独有特征中。该过程可以公式化表示为：
A major limitation of existing multimodal fusion approaches is that their feature mapping functions $g$ process each modality independently, as illustrated in Eq.~\ref{eq:12}, failing to explicitly model cross-modal complementary relationships. To overcome this constraint, we propose the cross-modal complementary spatiotemporal spiking attention (CCSSA) mechanism, as shown in Fig.~\ref{figccssa}, which enables each modality to dynamically integrate complementary information from the other modality and improves the representational capacity of the original modality.
Specifically, the complementary features from the other modality are treated as residuals and are fused with the distinctive features of the original modality. This process can be mathematically formulated as:
\begin{gather}
    g_a(\boldsymbol{x}^a)= \boldsymbol{x}^a + \alpha \cdot \operatorname{CCSSA}_a(\boldsymbol{x}^a, \boldsymbol{x}^v) ,\label{eq14}\\
    g_v(\boldsymbol{x}^v)= \boldsymbol{x}^v + \alpha \cdot \operatorname{CCSSA}_v(\boldsymbol{x}^v, \boldsymbol{x}^a) ,\label{eq15}
\end{gather}
% 其中$\alpha$是超参数，用于控制互补信息的融合强度。
where $\alpha$ is a hyperparameter to control the fusion strength of complementary information.

% 在我们的方法中，CCSSA不仅在空间维度上捕捉互补信息，还在时间维度上进行建模，从而实现跨模态的时空信息融合。CCSSA由空间互补脉冲注意力（Spatial Complementary Spiking Attention, SCSA）和时间互补脉冲注意力（Temporal Complementary Spiking Attention, TCSA）两部分组成。以$\boldsymbol{x}^a_{\text{res}} = \operatorname{CCSSA}_a(\boldsymbol{x}^a, \boldsymbol{x}^v)$为例，给定听觉和视觉的输入特征$\boldsymbol{x}^a \in \mathbb{R}^{T \times B \times N \times D}$，$\boldsymbol{x}^v \in \mathbb{R}^{T \times B \times N \times D}$，其计算过程如下：
In our approach, CCSSA not only captures complementary information in the spatial dimension but also models complementary relationships in the temporal dimension, thereby enables effective cross-modal spatio-temporal information fusion. CCSSA is composed of two key components: spatial complementary spiking attention (SCSA) and temporal complementary spiking attention (TCSA). Let the auditory and visual input features be denoted as $\boldsymbol{x}^a$ and $\boldsymbol{x}^v$ respectively, each of dimension $\mathbb{R}^{T \times B \times N \times D}$. In the following, we will take the computation process of $\operatorname{CCSSA}_a(\boldsymbol{x}^a, \boldsymbol{x}^v)$ as an example to describe the computation of SCSA and TCSA in detail.

\paragraph{Spatial Complementary Spiking Attention (SCSA)}
% SCSA 专注于捕捉空间维度上的互补信息。它通过对空间位置之间的相关性建模来增强跨模态的信息融合。首先，我们根据公式~ref{ssa}计算跨模态空间注意力的互补特征$\mathcal{S}^a$，其中查询输入$Q_s$是音频模态特征$\boldsymbol{x}^a$，键输入$K_s$和值输入$V_s$是视觉模态特征$\boldsymbol{x}^v$。$mathcal{S}^a$ 表示查询音频特征在视觉模态中的映射，以捕捉音频和视觉之间相互关联和互补的信息。
SCSA is designed to capture complementary information in the spatial dimension. It enhances information fusion across modalities by modeling the correlations between spatial locations. First, we compute the complementary features $\mathcal{S}^a$ of cross-modal spatial attention according to the Eq.~\ref{ssa}, where the input to the query $Q_s$ is the audio modality feature $\boldsymbol{x}^a$ and the inputs to the key $K_s$ and value $V_s$ are the visual modality features $\boldsymbol{x}^v$. The $\mathcal{S}^a$ represents the mapping of the query audio feature representation in the visual modality to capture the interrelated information between audio and vision.

% 随后，对空间注意力的输出进行空间归一化，得到每个时间维度上带有空间特征的紧凑表示：
Subsequently, the output of spatial attention is spatially normalized to obtain a compact spatial feature representation:
\begin{equation}
    \mathbf{\mathcal{S}^a}_{\text{reduced}} = \frac{1}{N} \sum_{i=1}^{N} \mathbf{\mathcal{S}^a}_{:, :, i, :} \; \in \mathbb{R}^{T \times B \times D},
\end{equation}
% 这里，$N$ 表示空间维度的大小，$\mathbf{\mathcal{S}^a}_{:, :, i, :}$ 表示第 $i$ 个空间位置的特征。通过求平均操作，$\mathbf{\mathcal{S}^a}_{\text{reduced}}$ 将空间维度的信息聚合到时间和批次维度上，形成一个紧凑的特征表示。
% 接下来，将归一化后的特征扩展回原始的空间维度，以便与后续的处理模块进行融合：
where $N$ denotes the size of the spatial dimension, and $\mathbf{\mathcal{S}^a}_{:, :, i, :}$ denotes the feature of the $i$-th spatial location. Through the averaging operation, $\mathbf{\mathcal{S}^a}_{\text{reduced}}$ aggregates the information from the spatial dimension to the temporal and batch dimensions to form a compact feature representation.

To ensure information integrity and compatibility with subsequent processing, we expand the normalized features back to the original spatial dimension:
\begin{equation}
    \mathbf{\mathcal{S}^a}_{\text{expanded}} = \mathbf{\mathcal{S}^a}_{\text{reduced}} \otimes \mathbf{1}_N \; \in \mathbb{R}^{T \times B \times N \times D},
\end{equation}
% 其中，$\mathbf{1}_N$ 表示一个大小为 $N$ 的全1向量，$\otimes$ 表示张量的外积操作。通过这种方式，$\mathbf{\mathcal{S}^a}_{\text{expanded}}$ 保留了归一化后的特征，同时扩展到原始的空间维度，确保信息的完整性和一致性。
where $\mathbf{1}_N$ denotes an all-ones vector of size $N$, and $\otimes$ denotes the outer product operation. This expansion ensures that $\mathbf{\mathcal{S}^a}_{\text{expanded}}$ retains the compactness of the normalized features while restoring the original spatial resolution for subsequent cross-modal fusion.

\begin{figure*}[t]
  \centering
    \includegraphics[width=0.95\linewidth]{fig/overview.pdf}
  \caption{ %  拟议的语义感知跨模态残差学习方法概述。该网络架构通过独立且相互关联的路径处理视觉和听觉输入。这些路径最终在一个中央模块中融合，该模块采用了一种新颖的跨模态互补时空脉冲注意机制，能够有效挖掘视觉和听觉模态中的互补信息，并将其作为残差融入各模态的独特特征表示中。此外，语义相关性优化机制进一步增强了跨模态特征的互补性，从而提升了模型的整体表现。
  Overview of proposed semantic-alignment cross-modal residual learning framework. The network processes visual and auditory inputs through independent pathways. Following positional embedding, These pathways converge in a central module, which employs a novel cross-modal complementary spatiotemporal spike attention mechanism. This mechanism effectively exploits complementary information between modalities and integrates it as residuals into the unique feature representations of each modality. Additionally, the semantic alignment optimization further enhances the consistency of cross-modal features.}
  \label{fig}
\end{figure*}


% TCSA主要关注时间维度上的互补信息，通过对时间序列的相关性进行建模，实现不同模态在时间上的信息融合，从而增强模型对时序动态变化的捕捉能力。在具体实现中，TCSA通过计算查询$Q_{t}$和键$K_{t}$的内积来获得注意力权重，并将其应用于值$V_{t}$的表示，得到初步的时间注意力输出$\mathbf{\mathcal{T}^a}$. TCSA的计算过程与SCSA类似, 参见公式~\ref{scsa}。值得注意的是，TCSA在计算过程中涉及到输入数据的维度变换，其中，音频和视觉模态的数据分别表示为$\boldsymbol{x}^a \in \mathbb{R}^{B \times N \times T \times D}$，$\boldsymbol{x}^v \in \mathbb{R}^{B \times N \times T \times D}$。
\paragraph{Temporal Complementary Spiking Attention (TCSA)}
TCSA is designed to extract complementary information along the temporal dimension. By modeling temporal correlations, it facilitates the integration of temporal information across different modalities and enhances the model’s capacity to capture temporal dynamics. During computation, the input features are first rearranged to accommodate temporal attention calculations as $\boldsymbol{x}^a \in \mathbb{R}^{B \times N \times T \times D}$ and $\boldsymbol{x}^v \in \mathbb{R}^{B \times N \times T \times D}$. 
We then compute the complementary features $\mathcal{T}^a$ of cross-modal temporal attention using Eq.~\ref{ssa}.
% 得到时间互补特征$\mathbf{\mathcal{T}^a}$后，我们对其进行维度变换，使其重新变为$\mathbf{\mathcal{T}^a} \in \mathbb{R}^{T \times B \times N \times D}$.
After obtaining the temporal complementary feature $\mathbf{\mathcal{T}^a}$, we perform a dimensional transformation on it so that it becomes $\mathbf{\mathcal{T}^a} \in \mathbb{R}^{T \times B \times N \times D}$.

% 随后，对时间维度进行归一化处理，得到每个批次和空间位置上带有时间特征的紧凑表示：
Subsequently, the temporal dimension is normalized to obtain a compact representation with temporal features at each batch and spatial location:
\begin{equation}
    \mathbf{\mathcal{T}^a}_{\text{reduced}} = \frac{1}{T} \sum_{j=1}^{T} \mathbf{\mathcal{T}^a}_{j, :, :, :} \; \in \mathbb{R}^{B \times N \times D},
\end{equation}
% 这里，$T$ 表示时间维度的长度，$\mathbf{\mathcal{T}^a}_{j, :, :, :}$ 表示第 $j$ 个时间步的特征。通过对时间维度的求平均操作，$\mathbf{\mathcal{T}^a}_{\text{reduced}}$ 将时间维度的信息聚合到批次和空间维度上，形成一个紧凑的特征表示。
where $T$ is the time dimension length and $\mathbf{\mathcal{T}^a}_{j, :, :, :}$ represents the features at the $j$-th time step. 
% 接下来，将归一化后的特征扩展回原始的时间维度，以便与后续的处理模块进行融合：
Next, the normalized features are expanded back to the original time dimension for fusion with subsequent processing modules:
\begin{equation}
    \mathbf{\mathcal{T}^a}_{\text{expanded}} = \mathbf{\mathcal{T}^a}_{\text{reduced}} \otimes \mathbf{1}_T \; \in \mathbb{R}^{T \times B \times N \times D}.
\end{equation}
where $\mathbf{1}_T$ denotes an all-ones vector of size $T$.

\paragraph{Spatiotemporal Complementary Fusion}
% 有了空间和时间维度上的互补信息，我们将其进行逐元素相乘以实现融合：
With complementary information in spatial and temporal dimensions, we fuse them through element-wise multiplication:
\begin{equation}
    \operatorname{CCSSA}_a(\boldsymbol{x}^a, \boldsymbol{x}^v) = \mathbf{\mathcal{S}^a}_{\text{expanded}} \ast \mathbf{\mathcal{T}^a}_{\text{expanded}} \; \in \mathbb{R}^{T \times B \times N \times D}.
\end{equation}
% 最终，将融合后的互补特征$\operatorname{CCSSA}_a(\boldsymbol{x}^a, \boldsymbol{x}^v)$作为“残差”添加到原始模态的独有特征中。$\operatorname{CCSSA}_v(\boldsymbol{x}^v, \boldsymbol{x}^a)$的计算过程与上述类似，此处不再赘述。
% 这种跨模态互补时空脉冲注意力机制充分利用了不同模态之间的互补信息，通过在空间和时间两个维度上捕捉互补信息，增强了特征表示的丰富性和表达能力。
Finally, the fused complementary feature $\operatorname{CCSSA}_a(\boldsymbol{x}^a, \boldsymbol{x}^v)$ is incorporated as residual into the original modality-specific feature, enhancing the expressiveness of cross-modal information. 


\subsection{Semantic Alignment Optimization}
% 在CCSSA中，我们得到了跨模态互补特征$\boldsymbol{x}^a_{\text{res}} = \operatorname{CCSSA}_a(\boldsymbol{x}^a, \boldsymbol{x}^v)$与$\boldsymbol{x}^v_{\text{res}} = \operatorname{CCSSA}_v(\boldsymbol{x}^v, \boldsymbol{x}^a)$。尽管 CCSSA 能够有效地融合跨空间和时间维度的互补特征，但跨模态语义偏移问题仍然存在，这可能导致融合后的表征未能充分捕捉到不同模态之间共享的语义信息。具体来说，跨模态语义偏移的主要原因在于不同模态之间固有的分布差异。由于各模态具有各自独特的特征表达方式，它们的特征在共享语义空间中的分布往往表现出不一致性，进而导致特征的失衡。这种分布不一致性使得模型难以在统一的语义空间中学习到稳定的跨模态语义映射，从而影响多模态表示的融合效果。
Within the CCSSA mechanism, we obtain cross-modal complementary features $\boldsymbol{x}^a_{\text{res}} = \operatorname{CCSSA}_a(\boldsymbol{x}^a, \boldsymbol{x}^v)$ and $\boldsymbol{x}^v_{\text{res}} = \operatorname{CCSSA}_v(\boldsymbol{x}^v, \boldsymbol{x}^a)$. Although CCSSA enables complementary feature fusion across spatial and temporal dimensions, cross-modal semantic shift may still persists. Specifically, the primary cause of cross-modal semantic shift lies in the inherent distribution differences between modalities. Due to the unique feature representations of each modality, their features often exhibit inconsistencies in distribution within the shared semantic space. This distributional inconsistency makes it difficult for the model to learn a stable cross-modal semantic mapping in a unified feature space, thus impairing the effectiveness of multimodal fusion.

% 为了解决这个问题，我们提出了跨模态语义对齐（Cross-Modal Semantic Alignment，CSA），它在共享语义空间内明确地对齐跨模态特征。这一机制旨在提高语义一致性，并强化多模态表征。SAO通过以下损失函数实现：
To address this issue, we propose semantic alignment optimization (SAO), which explicitly aligns cross-modal features within a shared semantic space. SAO aims to improve semantic consistency and strengthen multimodal representations, which is implemented through the following loss function:
\begin{equation}
    \begin{gathered}
    \mathcal{L}_{\text {sao}} =\frac{1}{B} \sum_{i =1}^B \frac{1}{T} \sum_{t =1}^T-\log \left\{\frac{\exp \left(\boldsymbol{x}^{a,t}_{\text{res}, i} \cdot \boldsymbol{x}^{v,t}_{\text{res}, i} / \tau\right)}{\sum_{j=1}^B \exp \left(\boldsymbol{x}^{a,t}_{\text{res}, i} \cdot \boldsymbol{x}^{v,t}_{\text{res}, i} / \tau\right)}\right\},
    \end{gathered}
  \end{equation}
  % 其中，$i \in \left\{1, 2\cdots B\right\}$ 表示样本的索引。$\boldsymbol{x}^{a,t}_{\text{res}}$ 为跨模态互补特征$\boldsymbol{x}^a_{\text{res},i}$在$t$时刻时, 批数据$B$中第$i$个样本的特征，$\tau \in \mathcal{R}^{+}$ 为温度系数，用于调节分布的平滑程度。
  where $i \in \left\{1, 2, \cdots, B\right\}$ denotes the sample index within a batch, and $\boldsymbol{x}^{a,t}_{\text{res},i}$ represents the cross-modal complementary feature $\boldsymbol{x}^a_{\text{res}}$ of the $i$-th sample at time step $t$ in batch $B$. The parameter $\tau \in \mathbb{R}^{+}$ is a temperature coefficient used to adjust the smoothness of the distribution.
  
  % 损失函数$\mathcal{L}^{text {sao}}$旨在通过在共享语义空间内的同一时间步显式对齐跨模态互补特征，提高语义一致性，来优化模型的性能，。通过结合SAO机制，该模型可以在多模态融合过程中进一步提高跨模态互补特征的语义相关性，从而获得更稳健、更具区分度的跨模态表征。最终优化目标为
  The loss function $\mathcal{L}_{\text{sao}}$ aims to optimize the model's performance by explicitly aligning complementary cross-modal features at the same time step within the shared semantic space. By incorporating the SAO mechanism, the model can improve the semantic consistency of cross-modal complementary features during the multimodal fusion, leading to more robust and discriminative cross-modal representations. The final optimization objective is given by:
  \begin{equation}
      \mathcal{L} = \mathcal{L}_{ce} + \mathcal{L}_{\text {sao}}.
  \end{equation}
  
\subsection{Overall Architecture}
% 在本研究中，我们提出了一种视听多模态尖峰神经网络框架，如图所示。该框架旨在充分利用视觉和听觉模态之间的互补信息，从而增强多模态特征融合的鲁棒性和判别能力。该模型由三个核心模块组成： 尖峰集群分割、跨模态时空互补尖峰注意和跨模态语义对齐。
In this study, we propose an audiovisual multimodal spiking neural network framework, as illustrated in Fig.~\ref{fig}. This framework is designed to fully exploit the complementary information between visual and auditory modalities, thereby enhancing the robustness and discriminative ability of multimodal feature fusion. The model consists of three core modules: spiking patch splitting, cross-modal complementary spatiotemporal spiking attention, and semantic alignment optimization.


% 首先，原始的视觉和听觉输入由 SPS 模块处理，该模块将连续的图像帧和 log-Mel 频谱图转换为离散的尖峰表示。这种转换对于使输入数据适应 SNN 的事件驱动特性、实现高能效异步计算至关重要。然后，尖峰表征被输入 CCSSA 模块，该模块捕捉并整合跨空间和时间维度的模态之间的互补信息，确保有效的多模态特征融合。

First, the raw visual and auditory inputs are processed by the SPS module, which converts sequential image frames and log-Mel spectrograms into discrete spike representations. This transformation is crucial for adapting the input data to the SNNs, enabling energy-efficient computation. The position embedded spike representations are then fed into the CCSSA module, which captures and integrates complementary information between modalities across both spatial and temporal dimensions, ensuring effective multimodal feature fusion. 

% 之后，SAO 机制会在共享语义空间内进一步完善和调整跨模态特征。通过在视觉和听觉表征之间明确执行语义一致性，SAO 可减轻特定模态特征的干扰，并增强跨模态特征的一致性。最终，融合后的特征将通过分类头，执行最终的多模态识别任务。这一集成框架充分利用了尖峰神经元和跨模态注意机制的互补优势，为稳健的视听处理提供了一种高效且符合生物学原理的解决方案。
Following this, the SAO mechanism further refines and aligns cross-modal features within a shared semantic space. By explicitly aligning cross-modal features, SAO mitigates modality-specific feature interference and enhances cross-modal feature consistency. 

Finally, the fused features are passed through a classification head, which performs the final multimodal recognition task. This integrated framework leverages the complementary strengths of spiking neurons and cross-modal attention mechanisms, offering an efficient and biologically plausible solution for robust audiovisual processing.

\section{EXPERIMENTS}
% 我们首先介绍了实验中用到的三个数据集以及对应的实验设置。随后，我们在这些数据集上进行了实验，比较了我们的方法和目前最先进的方法，实验结果表明，我们的方法在干净数据和含噪声数据中均优于现有的方法。最后，我们进行了定量的消融研究，以展示所提方法各部分的有效性。
In this section, we first describe the three datasets employed in our experiments and detail their corresponding experimental setups. We then conduct experiments on these datasets to compare our method with the current state-of-the-art approaches. The results demonstrate that our method outperforms existing methods under both clean and noisy conditions. Finally, we perform comprehensive ablation studies to showcase the effectiveness of each component in our proposed method.
\subsection{Datasets}
\subsubsection{CREMA-D}
% CREMA-D~\cite{cao2014crema}是一个用于语音情感识别的视听数据集，包含来自91名演员的7442段时长为2到3秒的视频片段。该数据集涵盖了六种最常见的情感类别：愤怒、快乐、悲伤、中性、厌恶和恐惧。数据集的划分遵循Peng等人~\cite{peng2022balanced}的方法，按照9：1的比例随机划分为训练集和验证集，以及测试集。最终，训练集和验证集包含6698个样本，测试集包含744个样本。
CREMA-D~\cite{cao2014crema} is an audiovisual dataset for speech emotion recognition containing 7442 video clips of 2 to 3 seconds duration from 91 actors. The dataset covers the six most common emotion categories: anger, happiness, sadness, neutrality, disgust and fear. The division of the dataset follows the method of Peng et al.~\cite{peng2022balanced}, which randomly divides the dataset into training and validation sets, as well as a test set, in a ratio of 9:1. Ultimately, the training and validation sets contain 6698 samples, and the test set contains 744 samples.

\subsubsection{UrbanSound8K-AV}
% UrbanSound8K-AV数据集~\cite{guo2023transformer}是将 UrbanSound8K 音频数据集~\cite{salamon2014dataset}与其对应的图像数据集相结合而成。UrbanSound8K-AV 数据集包含与 UrbanSound8K 音频数据集相同数量的样本，总计8732个视听样本。每个样本由一张高分辨率的彩色图像和一个 4 秒的音频信号组成。我们遵循Guo等人~\cite{guo2023transformer}的方式，按照7：3的比例将数据集随机划分为训练集和测试集。
The UrbanSound8K-AV dataset ~\cite{guo2023transformer} is a combination of the UrbanSound8K audio dataset ~\cite{salamon2014dataset} and its corresponding image dataset. The UrbanSound8K-AV dataset contains the same number of samples as the UrbanSound8K audio dataset, totaling 8732 audiovisual samples. Each sample consists of a high-resolution color image and a 4-second audio signal. We follow Guo et al.~\cite{guo2023transformer} by randomly dividing the dataset into training and test sets in a 7:3 ratio.

\subsubsection{MNISTDVS-NTIDIGITS}
% MNIST-DVS - N-TIDIGITS是将MNIST-DVS 数据集~\cite{serrano2013128}和N-TIDIGITS 数据集~\cite{serrano2013128}拼接而成的视听数据集。不同于传统传感器，这些数据集是由动态视觉传感器（DVS）和动态音频传感器（DAS）收集成的时空事件数据，亦称为神经形态数据集。遵循Liu等人~\cite{liu2022event}的设置, 我们选择N-TIDIGITS中的10种数字类别（“zero”、“1”至“9”），共4500个音频样本被重复使用，以与MNIST-DVS数据集匹配，最终形成10000个视听样本。数据集按照9:1的比例随机划分为训练集和测试集。
MNISTDVS-NTIDIGITS is a audio-visual dataset spliced together from the MNISTDVS dataset \cite{serrano2013128} and the NTIDIGITS dataset \cite{serrano2013128}. Unlike traditional sensors, these datasets are spatio-temporal event data collected by dynamic visual sensors (DVS) and dynamic audio sensors (DAS), also known as neuromorphic datasets. Following Liu et al.~\cite{liu2022event}, we select 10 numerical categories in N-TIDIGITS (“zero”, “1” to “9 “), a total of 4500 audio samples are reused to match the MNIST-DVS dataset, resulting in 10000 audiovisual samples. The dataset is divided into training and test sets in a 5:5 ratio.
\begin{table*}[t]
  \centering
  \begin{threeparttable}
    \resizebox{1.0\linewidth}{!}{
      \begin{tabular}{cccccc}
        \toprule
        \textbf{Dataset} &\textbf{Category} & \textbf{Methods} & \textbf{Architecture} & \textbf{T} & \textbf{Accuracy}\\
        \midrule
        \multirow{7}{*}{\text { CRMEA-D }} &\multirow{4}{*}{\text { ANN}} & \text { OGM-GE} \citep{peng2022balanced}& \text {ResNet-18} & - & $62.20^*$ \\
        & & \text {MSLR} \cite{yao2022modality} & \text{ResNet-18} & - & $64.42^*$\\ 
        & & \text {PMR} \cite{fan2023pmr} & \text{ResNet-18} & - & $65.30^*$\\
        & & \text {AGM} \cite{li2023boosting} & \text{ResNet-18} & - & $70.16^*$\\
        \cmidrule{2-6}
        & \multirow{4}{*}{\text{Multi-modal SNN}} & \text { WeightAttention}~\cite{liu2022event} & \text {Spiking Transformer} & 4 & $ 64.78$ \\ 
        & & \text{SCA}~\cite{guo2023transformer} & \text{Spiking Transformer} & 4 & $66.53$\\
        & & \text{CMCI}~\cite{jiang2023cmci} & \text{Spiking Transformer} & 4 & $70.02$\\
        & & \cellcolor{gray!10} \text { S-CMRL}  \text {(Ours) } & \cellcolor{gray!10}\text {Spiking Transformer} & \cellcolor{gray!10}4 & \cellcolor{gray!10}$\mathbf{73.25}$ \\
        \midrule
        \multirow{4}{*}{\text { UrbanSound8K-AV }} & \multirow{4}{*}{\text{Multi-modal SNN}} & \text { WeightAttention}~\cite{liu2022event} & \text {Spiking Transformer} & 4 & $ 93.11^*/97.60$ \\ 
        & & \text{SCA}~\cite{guo2023transformer} & \text{Spiking Transformer} & 4 & $96.85^*/97.44$\\
        & & \text{CMCI}~\cite{jiang2023cmci} & \text{Spiking Transformer} & 4 & 97.90\\
        & & \cellcolor{gray!10} \text { S-CMRL}  \text {(Ours) } & \cellcolor{gray!10}\text {Spiking Transformer} & \cellcolor{gray!10}4 & \cellcolor{gray!10}$ \mathbf{98.13}$ \\
        \midrule
        \multirow{4}{*}{\text { MNISTDVS-NTIDIGITS }} & \multirow{4}{*}{\text{Multi-modal SNN}} & \text { WeightAttention}~\cite{liu2022event} & \text {Spiking Transformer} & 16 & $ 99.14$ \\ 
        & & \text{SCA}~\cite{guo2023transformer} & \text{Spiking Transformer} & 16 & 98.98\\
        & & \text{CMCI}~\cite{jiang2023cmci} & \text{Spiking Transformer} & 16 & 99.04\\
        & & \cellcolor{gray!10} \text { S-CMRL}  \text {(Ours) } & \cellcolor{gray!10}\text {Spiking Transformer} & \cellcolor{gray!10}16 & \cellcolor{gray!10}$ \mathbf{99.28}$ \\
        \bottomrule
    \end{tabular}
    }
  \end{threeparttable}
  \caption{Comparison of S-CMRL with state-of-the-art methods on three datasets. The symbol (*) denotes results reported in reference papers, while others are reproduced for fair evaluation.}
  \label{sota}
\end{table*}

\subsection{Experimental Settings}
% 在输入编码策略方面，对于静态图像，我们采用直接编码方法，并根据脉冲神经网络的时间步对静态图像进行多次复制。CREMA-D 和 UrbanSound8K-AV 数据集中的图像被调整为128$\times$128的分辨率，并进行了随机裁剪的数据增强。对视觉事件数据，我们将所有事件数据整合成帧并调整尺寸，MNIST-DVS数据集中的图像分辨率调整为26x26。对于音频数据处理，我们首先对所有音频数据进行归一化处理，然后重新采样至 22,050 Hz，以确保输入尺寸的一致性。预处理后的波形通过短时傅里叶变换（STFT）转换为时频表示，采用窗口长度（n\_fft）为 512，重叠长度（hop\_length）为 353，从而生成二维频谱图。在 STFT 处理后，计算幅值频谱图并进行对数变换（偏移量为 1e-7），最终将其调整至与图像相同的分辨率。

% 在训练设置方面，所有的实验在2张 NVIDIA A100 GPU上实现，优化器采用Adam~\cite{Adam2015}，初始学习率为5e-3, 整个训练过程持续100个epoch, batch size设置为128。
% LIF神经元的初始膜电势为0，阈值设置为1，时间步长设置为4. 我们采用参数为$\alpha=4$的Sigmoid函数 $\operatorname{Sigmoid}(x)=1/(1+\exp (-\alpha x))$作为神经元的代理梯度。 

For visual preprocessing, we employ a direct coding method for still images, where static images are replicated multiple times according to the time steps of the SNN to maintain temporal consistency. Images in the CREMA-D and UrbanSound8K-AV datasets are resized to 128 $\times$ 128 pixels, with random cropping applied for data augmentation. For event-based visual data, we aggregate all event streams into frame representations and resize them, where images in the MNISTDVS dataset are resized to 26 $\times$ 26 pixels to ensure compatibility with the model architecture. 

For audio preprocessing, all raw waveforms are first normalized and resampled to 22,050 Hz to standardize input dimensions across datasets. The preprocessed signals are then transformed into time-frequency representations using the Short-Time Fourier Transform (STFT) with an FFT window length of 512 and a hop length of 353 to generate 2D spectrograms. The resulting amplitude spectrograms undergo logarithmic transformation with an offset of 1e-7 to enhance feature representation and are resized to match the corresponding image dimensions to ensure uniform multimodal input.

All experiments are based on Brain-Cog~\cite{Zeng2023} framework and are conducted on a single NVIDIA A100 GPU. The model is optimized using the Adam optimizer~\cite{Adam2015} with an initial learning rate of $5 \times 10^{-3}$. The training process spans 100 epochs, with a batch size of 128. For the LIF neuron configuration, the initial membrane potential is set to 0, the firing threshold is fixed at 1, and the simulation time step is set to 4. To enable effective gradient backpropagation through spiking neurons, We adapt the Sigmoid function $\operatorname{Sigmoid}(x)=1/(1+\exp (-\alpha x))$ with the parameter $\alpha=4$ as the neuron's surrogate gradient. 

\subsection{Comparison with the State-of-the-Art}
% 我们首先在CRMEA-D和UrbanSound8K-AV数据集上使用 Spiking Transformer 网络对所提出的语义感知驱动的跨模态残差学习框架进行了评估，并将其与现有的跨模态融合方法如 WeightAttention~\cite{liu2022event}，SCA~\cite{guo2023transformer}和CMCI~\cite{jiang2023cmci}进行了比较。由于现有方法均未公开代码实现，我们对其进行了复现，结果以星号标注，实验结果如表 \ref{sota} 所示。实验结果表明，所提出的方法在现有方法中达到了最先进的性能。

% 具体而言，在CREMA-D数据集上，S-CMRL方法实现了 74.60\% 的准确率，相比之下，其他多模态SNN方法如WeightAttention (64.78\%)，SCA(66.53\%)，CMCI(70.02\%)的性能均低于S-CMRL，这一显著的性能提升验证了所提方法的有效性。值得注意的是，S-CMRL方法实现的74.60\%的准确率也优于基于人工神经网络（ANN）的方法，这展示了Spiking Transformer网络模型在整合视听信息时的高效性。
% 在UrbanSound8K-AV数据集上，S-CMRL方法达到了98.28\%的准确率，超过了其他多模态SNN方法，包括CMCI（97.90\%）、WeightAttention（97.60\%）和SCA（97.44\%）。这一结果表明，S-CMRL表现出更强的泛化能力。

We evaluate the proposed semantic-alignment cross-modal residual learning framework using the Spiking Transformer network on the CRMEA-D and UrbanSound8K-AV datasets, and compare it with existing cross-modal fusion methods such as WeightAttention~\cite{ liu2022event}, SCA~\cite{guo2023transformer} and CMCI~\cite{jiang2023cmci}. Since none of the existing methods are publicly available in code implementation, we reproduce them in the same experimental configuration and mark asterisks in the results from the original papers for comparison. The experimental results are shown in Table~\ref{sota}, demonstrating that our proposed method achieves state-of-the-art performance across all datasets.

Specifically, the S-CMRL method achieves the 73.25\% accuracy on the CREMA-D dataset, compared to other multimodal SNN methods such as WeightAttention (64.78\%), SCA (66.53\%), and CMCI (70.02\%), which have lower performance than S-CMRL. This significant performance improvement validates the effectiveness of the proposed method. Notably, the 73.25\% accuracy achieved by the S-CMRL method also outperforms the artificial neural network based method, which demonstrates the efficiency of the Spiking Transformer network model in integrating audiovisual information.

On the UrbanSound8K-AV dataset, the S-CMRL method achieves an accuracy of 98.13\%, outperforming other multimodal SNN methods, including CMCI (97.90\%), WeightAttention (97.60\%), and SCA (97.44\%). This result indicates that S-CMRL exhibits stronger generalization ability, effectively capturing robust cross-modal representations across different environmental soundscapes.

% 进一步地，我们在神经形态数据集 MNISTDVS-NTIDIGITS 上对 S-CMRL 方法进行了验证。结果表明，S-CMRL 达到了 99.28% 的准确率，分别超越了 WeightAttention (99.14%)、SCA (98.98%) 和 CMCI (99.04%)，进一步证明了该方法在更广泛的视听数据集中的有效性。
Furthermore, we evaluated the S-CMRL method on the neuromorphic MNISTDVS-NTIDIGITS dataset. The results show that S-CMRL achieved an accuracy of 99.28\%, surpassing WeightAttention (99.14\%), SCA (98.98\%), and CMCI (99.04\%), which further demonstrates the method’s effectiveness across a broader range of audiovisual datasets.


\begin{figure}
	\centering
	\subcaptionbox{Image with Gaussian noise. \label{fig: resulta}}
	{\includegraphics[width=0.49\linewidth]{fig/noise-vis.pdf}}
	\subcaptionbox{Audio with Gaussian noise. \label{fig: resultb}}
	{\includegraphics[width=0.49\linewidth]{fig/noise-audio.pdf}}
	\caption{ % CRMEA-D数据集中, 不同噪声水平下视觉与音频数据的可视化展示。每个子图的纵坐标代表信噪比（SNR）值，数值越小，噪声强度越大。从上至下，图示呈现了噪声强度由低至高（30至0）的变化过程。横坐标展示了三种不同的情绪类别：Neutral（中性）、Happy（快乐）和Disgust（厌恶）。
  Visualization of visual and audio data under different noise intensity in CRMEA-D dataset. The vertical coordinate of each sub-figure represents the SNR value, the smaller the value, the higher the noise intensity. From top to bottom, the graphs present the variation of noise intensity from low to high (30 to 0). The horizontal coordinates show three different emotion categories: Neutral, Happy and Disgust.
  }
  \label{fignoise}
\end{figure}


\subsection{Noise Robustness}
% 此外，我们对提出的模型在抗噪性能方面进行了评估，并与现有多模态融合方法进行了比较。具体地，我们将噪声高斯白噪声$n$加到原始图像$\boldsymbol{x}^v$和音频$\boldsymbol{x}^v$上，得到带噪声的信号输入。添加的高斯白噪声$n\sqrt{\frac{\mathbb{E}[x^2]}{10^{\text{SNR}/10}}} \cdot \mathcal{N}(0, 1)$, 其中，$\mathbb{E}[x^2]$ 是原始模态输入 $x$ 的均方值（即信号功率），$\text{SNR}$ 是信噪比，以分贝（dB）为单位，$\mathcal{N}(0, 1)$ 表示均值为 0、方差为 1 的标准正态分布。
To assess the robustness of S-CMRL to noise, we evaluate the proposed model in terms of noise resistance and compare it with existing multimodal fusion methods. Specifically, we add noisy Gaussian white noise $n$ to the original visual image $\boldsymbol{x}^v$ and audio $\boldsymbol{x}^a$ to obtain the signal input with noise. The added Gaussian white noise follows:
\begin{equation}
  n = \sqrt{\frac{\mathbb{E}[x^2]}{10^{\text{SNR}/10}}} \cdot \mathcal{N}(0,1),
\end{equation}
% 其中，$\mathbb{E}[x^2]$ 是原始模态输入 $x$ 的均方值（即信号功率），$\text{SNR}$ 是信噪比，以分贝（dB）为单位，$\mathcal{N}(0, 1)$ 表示均值为 0、方差为 1 的正态分布。
where $\mathbb{E}[x^2]$ is the mean square value (i.e., signal power) of the original modal input $x$, $\text{SNR}$ is the signal-to-noise ratio in decibels (dB), and $\mathcal{N}(0, 1)$ denotes the normal distribution with a mean of zero and variance of one.

% 我们对所添加的噪声进行了可视化，以更直观地展示噪声对图像和音频数据的影响。图~\ref{fignoise}展示了不同噪声强度下的视觉和音频数据。具体而言，图~\ref{fignoise}（a）呈现了视觉数据，图~\ref{fignoise}（b）展示了音频数据。从图中可以看出，当信噪比（SNR）小于或等于20时，噪声对原始数据的干扰较为显著。
We visualize the added noise to show more intuitively the effect of noise on the image and audio data. Fig.~\ref{fignoise} presents the visual and audio data under different noise intensities. Specifically, Fig.~\ref{fignoise}(a) presents the visual data and Fig.~\ref{fignoise}(b) presents the audio data. As can be seen from the figure, when the signal-to-noise ratio (SNR) is less than or equal to 20, the noise interferes more significantly with the original data.

% 我们将所提的S-CMRL方法与现有的跨模态融合方法, 包括 WeightAttention，SCA和CMCI进行了比较，图~\ref{snr}展示了不同噪声强度下，模型的性能表现。结果表明，所提出的方法在各种噪声强度条件下均获得了最优性能，表现出较高的鲁棒性，有效降低了噪声对信号处理的影响，验证了其有效性。

We compare S-CMRL with WeightAttention, SCA, and CMCI under different noise intensities, with results depicted in Fig.~\ref{snr}. The results show that the proposed method obtains the best performance under most noise conditions, exhibits high robustness, effectively reduces the influence of noise on signal processing, and verifies the effectiveness.

\subsection{Ablation study}
% 这一节中，我们进行实验以验证所提方法各部分的有效性。
In this section, we conduct experiments to verify the validity of each part of the proposed method.


\begin{figure}[t]
	\centering
		\includegraphics[width=0.95\linewidth]{fig/accuracy_comparison_snr.pdf}
	\caption{Model accuracy of different multimodal fusion methods for different noise intensities. }
	\label{snr}
\end{figure}

\subsubsection{\bf %跨模态互补时空脉冲注意力机制的有效性
The effectiveness of cross-modal complementary spatio-temporal spiking attention mechanisms
} 
% 为了验证通过时空脉冲注意力获取互补特征以及将跨模态互补特征作为残差融合的有效性，我们将跨模态互补时空注意力机制与三种变体进行比较：1) 不使用任何互补特征，即视觉和音频特征直接送入网络进行视听整合 2）使用跨模态互补空间注意力机制，获得跨模态空间互补特征来作为残差融合 3）使用跨模态互补时间注意力机制，获得跨模态时间互补特征来作为残差融合。
To validate the effectiveness of obtaining complementary features through spatio-temporal attention and using cross-modal complementary features as residual fusion, we compare the cross-modal complementary spatio-temporal attention mechanism with three variants: 1) without using any complementary features, i.e., the visual and audio features are fed directly into the network for audio-visual integration 2) using the cross-modal complementary spatial attention mechanism, obtaining cross-modal spatial complementary features for use as residual fusion 3) Using cross-modal complementary temporal attention mechanism to obtain cross-modal temporal complementary features for residual fusion.

\begin{table}[t]
  \centering
  \begin{threeparttable}
    \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{ccc}
        \toprule
        Datasets & Methods & Accuracy\\
        \midrule
        \multirow{4}{*}{CRMEA-D} & w/o CCSSA & 69.62\%\\
        & w/ CCSSA-Spatial-only & 70.70\%\\
        & w/ CCSSA-Temporal-only & 70.83\%\\
        & w/ CCSSA-Spatiotemporal & \textbf{72.72}\%\\
        \midrule
        \multirow{4}{*}{UrbanSound8K-AV} & w/o CCSSA & 97.44\%\\
        & w/ CCSSA-Spatial-only & 97.82\%\\
        & w/ CCSSA-Temporal-only & 97.79\%\\
        & w/ CCSSA-Spatiotemporal & \textbf{98.05}\%\\
        \bottomrule
    \end{tabular}}
  \end{threeparttable}
  \caption{Ablation experiments with cross-modal complementary spatio-temporal attention mechanisms.}
  \label{Ablation_Variation}
\end{table}

% 实验结果如表~\ref{Ablation_Variation}所示，可以看到，在不使用跨模态互补时空注意力机制后，网络的性能大幅下降，特别是在REMA-D数据集中，准确率下降约4\%，这说明CCSSA对特征融合过程中互补信息的整合起到了关键作用。进一步地，单独使用空间注意力机制或时间注意力机制相比于不使用互补特征，均能提升性能，但仅依赖其中一种注意力机制的提升幅度有限。这表明，虽然空间和时间维度上的互补信息各自具有一定的贡献，但单一维度的互补信息难以全面捕捉多模态数据的复杂关系。最重要的是，结合空间和时间两个维度的跨模态互补时空注意力机制能够实现最大的性能提升，验证了CCSSA能够更全面地整合不同模态之间的互补信息，显著增强特征表示的丰富性和表达能力。
The experimental results are shown in Table \ref{Ablation_Variation}, and it can be seen that the performance of the network decreases dramatically without the use of the cross-modal complementary spatial-temporal attention mechanism, especially in the REMA-D dataset, where the accuracy decreases by about 3\%, which suggests that the CCSSA plays a key role in integrating the complementary information in the feature fusion process. Further, using either the spatial attention mechanism or the temporal attention mechanism alone improves the performance compared to not using complementary features, but relying on only one of the attention mechanisms provides limited improvement. This suggests that while complementary information in the spatial and temporal dimensions each has a contribution to make, it is difficult to fully capture the complex relationships in multimodal data with complementary information in a single dimension. Most importantly, the cross-modal complementary spatio-temporal attention mechanism that combines both spatial and temporal dimensions is able to achieve the greatest performance enhancement, which verifies that CCSSA is able to more comprehensively integrate the complementary information between different modalities and significantly enhance the richness and expressiveness of the feature representation.

\begin{table}[t]
  \centering
  \begin{threeparttable}
    \resizebox{0.9\linewidth}{!}{
    \begin{tabular}{ccc}
        \toprule
        Datasets & Methods & Accuracy\\
        \midrule
        \multirow{2}{*}{CRMEA-D} & w/ CCSSA w/o SAO & 72.72\%\\
        & w/ CCSSA w/ SAO & \textbf{73.25}\%\\
        \midrule
        \multirow{2}{*}{UrbanSound8K-AV} & w/ CCSSA w/o SAO & 98.05\%\\
        & w/ CCSSA w/ SAO & \textbf{98.13}\%\\
        \bottomrule
    \end{tabular}}
  \end{threeparttable}
  \caption{Ablation experiments for semantic alignment optimization mechanisms.}
  \label{Ablation_Semantic}
\end{table}


\subsubsection{\bf %语义关联优化机制的有效性
Effectiveness of Semantic Alignment Optimization Mechanisms}
% 为了验证语义关联优化机制的有效性，我们在引入跨模态互补时空注意力机制(CCSSA)的基础上，分别添加了SCO机制，并在两个数据集（CRMEA-D和UrbanSound8K-AV）上进行了实验对比。实验结果如表~\ref{Ablation_Semantic}所示。从表~\ref{Ablation_Semantic}可以看出，在CRMEA-D数据集中，加入语义关联优化机制后，模型的准确率从73.52\%提升至74.60\%；在UrbanSound8K-AV数据集中，语义关联优化机制的引入也给模型带来了性能提升。这表明SCO机制在增强跨模态特征的语义一致性和互补性方面发挥了关键作用，验证了SCO机制能够有效优化语义空间中的特征对齐，从而提升多模态融合的整体性能。
In order to verify the effectiveness of the semantic alignment optimization mechanism, we added the SAO mechanism based on the introduction of the cross-modal complementary spatio-temporal attention mechanism (CCSSA), respectively, and conducted experimental comparisons on two datasets (CRMEA-D and UrbanSound8K-AV). The experimental results are shown in Table~\ref{Ablation_Semantic}. From Table~\ref{Ablation_Semantic}, it can be seen that in the CRMEA-D dataset, the addition of the semantic alignment optimization mechanism improves the accuracy of the model from 72.72\% to 73.25\%; in the UrbanSound8K-AV dataset, the introduction of the semantic alignment optimization mechanism also brings about a performance enhancement to the model. This indicates that the SAO mechanism plays a key role in enhancing the semantic consistency and complementarity of cross-modal features, and verifies that the SAO mechanism can effectively optimize the feature alignment in the semantic space, thus improving the overall performance of multimodal fusion.

\begin{figure}
	\centering
	\subcaptionbox{CRMEA-D dataset. \label{fig: resulta}}
	{\includegraphics[width=0.49\linewidth]{fig/aba_a.pdf}}
	\subcaptionbox{UrbanSound8K-AV dataset.\label{fig: resultb}}
	{\includegraphics[width=0.49\linewidth]{fig/aba_b.pdf}}
	\caption{ % “两个数据集的消融实验结果概览：CREMA-D（a）和UrbanSound8K-AV（b）。每个条形图展示了在不同实验设置下的准确度。橙色圆点表示‘已使用’，空心圆点表示‘未使用’。”
  Overview of the ablation experiment results for two datasets: CREMA-D (a) and UrbanSound8K-AV (b). Each bar chart shows the accuracy under different experimental settings.
  }
  \label{fig_ablation}
\end{figure}


\subsubsection{\bf %消融实验结果总结
Summary of ablation experiment results
}
% 为了清晰地展示我们所提出方法各部分对模型性能的贡献，我们在图~\ref{fig_ablation}中总结了两种方法的实验结果。可以看出，单独使用CCSSA能提升模型性能，在CREMA-D数据集上，准确率达到了 73.52\%。与基线结果（即未使用任何我们的方法时达到的 69.62\% 准确率）相比，网络准确率提高了约 4\%，验证了我们方法的有效性。此外，通过在CCSSA上添加语义关联优化，我们的方法达到了最佳结果，即74.60\%。在UrbanSound8K-AV数据集上的实验结果也展示了相似的提升，进一步证明了我们方法的优越性。
In order to clearly demonstrate the contribution of each part of our proposed method to the model performance, we summarize the experimental results of the two methods in Fig.~\ref{fig_ablation}. It can be seen that using CCSSA alone improves the model performance with an accuracy of 72.72\% on the CREMA-D dataset. Compared with the baseline results (i.e., 69.62\% accuracy achieved without using any of our methods), the network accuracy is improved by about 3\%, validating the effectiveness of our methods. Furthermore, by adding semantic alignment optimization to CCSSA, our method achieves the best result, i.e., 73.25\%. Experimental results on the UrbanSound8K-AV dataset demonstrate a similar improvement, further proving the superiority of our method.

\begin{figure}[t]
	\centering
		\includegraphics[width=0.9\linewidth]{fig/hyperparameter.pdf}
	\caption{Experimental results of hyperparameter settings on CRMEA-D dataset. The middle line of each box indicates the median. The model performs best when $\alpha$ is set to 1.5.}
	\label{fig_hyperparameter}
\end{figure}

\section{Discussion and Analysis}
\subsection{Hyperparameter Settings}
% 我们使用跨模态互补时空脉冲注意力获得跨模态的互补特征，并将其作为残差与原始特征进行融合，通过超参数$\alpha$来控制互补信息的融合强度，如公式~\ref{eq14} 和~\ref{eq15}中所示。为了展示不同 $\alpha$ 值下跨模态互补特征融合强度对模型性能的影响，我们对比了不同 $\alpha$ 值的实验结果。图~\ref{fig_hyperparameter}展示了三个不同随机种子情况下，模型准确率的平均值。当$\alpha=0.0$时，仅使用单模态信息，不进行跨模态的信息交互。从表中可以看出，只要加入跨模态互补特征，模型的准确率均有所提升，这表明跨模态的互补特征丰富了原有模态的特征信息。在CRMEA-D数据集中，最优的性能在适中的$\alpha=1.0$时出现。当融合强度过大时，即$\alpha=3.0$，模型性能相对出现下降，这说明跨模态互补特征无法完全反映输入的特征信息，也进一步验证了保留原始特征的必要性。

% 此外，我们对语义关联优化机制中的不同超参数$\tau$对模型性能的影响也进行了实验，结果如表~\ref{Hyperparameter}所示。实验结果显示，当$\tau=1.0$时，模型的准确率达到最高的75.13\%，表明温度参数$\tau$在调节模型输出分布时起到了关键作用，适当的$\tau$值能够优化模型的预测能力。

We use cross-modal complementary spatio-temporal spiking attention to obtain cross-modal complementary features and fuse them with the original features as residuals, and control the fusion strength of the complementary information through the hyperparameter $\alpha$, as shown in Eqs.~\ref{eq14} and \ref{eq15}. In order to demonstrate the effect of cross-modal complementary feature fusion strength on the model performance under different $\alpha$ values, we compare the experimental results under different $\alpha$ values. Fig.~\ref{fig_hyperparameter} displays the average model accuracy across three different random seeds.
When $\alpha=0.0$, only unimodal information is used, and no cross-modal information is interacted. It can be seen that the accuracy of the model is improved whenever cross-modal complementary features are added, which indicates that the cross-modal complementary features enrich the feature information of the original modality. In the CRMEA-D dataset, the optimal performance occurs at a moderate $\alpha=1.5$. When the fusion intensity is further increased, i.e., $\alpha=2.0$, the model performance is relatively degraded, which indicates that the cross-modal complementary features cannot fully reflect the input feature information, and further validates the necessity of retaining the original features.

\subsection{Unimodal Gain Analysis}
% 在我们的方法中，提出了跨模态互补时空脉冲注意力机制。该机制在空间和时间两个维度上获取不同模态之间的互补信息，并将这些信息作为“残差”融合到原始模态的专有特征中，从而显著提升模型的整体性能。为了验证跨模态特征的引入对单模态性能的提升作用，我们首先训练了一个多模态模型，然后提取了每个单模态分支的模型权重，并用它来作为该模态的初始权重，在独立的单模态中进行训练。

% 实验结果如图~\ref{multimodal_finetune}所示。可以看到，引入跨模态特征后，单模态模型的准确率得到了提升，这表明单模态可以从跨模态的互补特征中受益, 验证了所提融合策略的有效性。然而，在CRMEA-D数据集中，音频模态在采用多模态训练得到的权重进行微调后，准确率反而略有下降（从65.86%降至64.11%），我们认为是由于CRMEA-D数据集中音频和视觉模态之间的差异过大（相比于音频模态的65.65%准确率，视觉模态的准确率为43.15%）。这一结果再次强调了在跨模态融合过程中，保留单模态独有特征的重要性，以避免因模态间差异导致的性能下降。

In our approach, we propose a cross-modal complementary spatiotemporal spiking attention mechanism. This mechanism acquires complementary information between different modalities across both spatial and temporal dimensions, then integrates this information as “residuals” into the dedicated features of each original modality, thereby significantly improving overall model performance. To verify how cross-modal features enhance single-modality performance, we first train a multimodal model, then extract the model weights for each unimodal branch and use it as the initial weights for that modality in an independent unimodal training.

The experimental results are shown in Fig~\ref{multimodal_finetune}. It shows that the accuracy of the unimodal model is improved after incorporating the cross-modal features, which indicates that the unimodal can benefit from the complementary features of the cross-modal modality, and verifies the effectiveness of the proposed fusion strategy. However, in the CRMEA-D dataset, the accuracy of the audio modality is slightly decreased (from 65.86\% to 64.11\%) after fine-tuning with the weights obtained from the multimodal training, attribute to the large discrepancy between the audio and visual modalities in the CRMEA-D dataset (compared to the 65.65\% accuracy of the audio modality, the accuracy of the visual modality is 43.15\%). This result re-emphasizes the importance of preserving unimodal unique features during cross-modal fusion to avoid performance degradation due to inter-modal differences.

\begin{table}[!t]
  \centering
  \begin{threeparttable}
    \resizebox{1.0\linewidth}{!}{
      \begin{tabular}{ccccc}
        \toprule
        \multirow{2}{*}{Methods} & \multicolumn{2}{c}{CRMEA-D} &\multicolumn{2}{c}{UrbanSound8K-AV}  \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
        & audio & visual & audio & visual \\
        \midrule
        w/o S-CMRL & $\mathbf{65.86}$ & $43.15$ & $91.11$ & $87.63$ \\
        w/ S-CMRL & $64.11_{-1.75}$ & $\mathbf{46.24}_{+3.09}$ & $\mathbf{92.67}_{+1.56}$ & $\mathbf{88.97}_{+1.34}$\\
        \bottomrule
    \end{tabular}
    }
  \end{threeparttable}
  \caption{  %多模态给单模态带来的信息增益。下标表示相对于第一行所示基线，提高的准确率。
    Information gain from multimodality to unimodality. 
    The subscript indicates the improved accuracy with respect to the baseline shown in the 1st row.}
  \label{multimodal_finetune}
\end{table}


\subsection{Qualitative Visualization Analysis}
% 为了评估我们方法在学习跨模态互补特征，以及在单模态上提供有效特征信息的能力，我们采用了 grad-cam++~cite{chattopadhay2018grad} 可视化方法。该方法能够突显原始图像中对模型最终分类决策贡献最大的局部位置。理想情况下，音频特征的融入应使得视觉部分更专注于发声区域。例如，在“狗叫”这一类别中，视觉部分应该集中于狗的嘴巴位置，这样可以帮助模型更好地理解声音的来源与相关特征。
To evaluate the effectiveness of our method in learning cross-modal complementary features and providing effective feature information for unimodal tasks, we use the Grad-CAM++~\cite{chattopadhay2018grad} visualization method. This method is able to highlight the local regions of the original image that contribute most to the model's final classification decision. Ideally, incorporating audio features should cause the visual part to focus more on the sound-producing regions. For example, in the ``dog barking'' category, the visual attention should be concentrated on the dog's mouth, which helps the model better understand the source and related features of the sound.

% 为了更全面地分析我们方法的有效性，我们对比了三种不同的设置，如图~\ref{gradcam}所示：第一行展示了原始图像的地面真实情况，作为对比基础。第二行展示了我们的方法（S-CMRL）在没有采用 CCSSA（跨模态特征选择）时的效果。在这一行中，我们可以观察到，尽管模型仍然注意到了一些区域，但相较于理想情况，视觉部分缺乏对关键信息的集中关注，尤其是在“狗叫”这一类别中，模型的注意力分布较为分散，未能将焦点集中在狗的嘴巴部位。这表明，在缺少 CCSSA 的情况下，模型未能充分利用音频信息来引导视觉注意力，从而影响了对关键特征的提取和分类决策。
To comprehensively evaluate the effectiveness of our method, we compare three different settings, as shown in Fig.~\ref{gradcam}: The first row displays the ground truth of the original images, serving as the baseline for comparison. The second row shows the effect of our method, S-CMRL without CCSSA. In this row, we observe that although the model still pays attention to some regions, compared to the ideal case, the visual part lacks a focused attention on key information. Specifically, in the ``Dog bark'' category, the attention is more dispersed, failing to focus on the dog's mouth. This indicates that in the absence of CCSSA, the model fails to effectively leverage audio information to guide visual attention, thereby affecting the extraction of key features and classification decisions.

% 相比之下，底部行展示了 S-CMRL 方法的效果。在这一行中，模型通过有效融合音频和视觉信息，能够显著提高对关键特征（如狗的嘴巴位置以及摩托车中的发动机）的关注度。这一结果表明，音频特征的引入与跨模态互补特征的学习，使得模型能够在视觉图像中更加专注于与声音相关的区域，从而显著提升了分类决策的准确性。这验证了我们方法在跨模态特征融合方面的有效性，尤其是在关注关键发声部位的能力上。
In contrast, the bottom row shows the effect of the S-CMRL method. In this case, the model effectively integrates audio and visual information, significantly enhancing the attention to key features, such as the dog's mouth and the engine in the motorcycle. This result demonstrates that the incorporation of audio features and the learning of cross-modal complementary features allow the model to focus more on regions relevant to sound in the visual image, thus significantly improving the accuracy of classification. This validates the effectiveness of our method in cross-modal feature fusion, especially in terms of its ability to focus on key sound-related areas.

\begin{figure}[t]
	\centering
		\includegraphics[width=1.0\linewidth]{fig/grad-cam.pdf}
	\caption{%UrbanSound8K-AV数据集中的类别激活映射示意图，图中选取了四个类别进行展示。顶部行显示了原始图像的真实标签（ground truth），中间行展示了我们的方法（S-CMRL）没有CCSSA时的效果，底部行展示了我们的方法（S-CMRL）的效果。
  Class activation mapping in the UrbanSound8K-AV dataset, with four categories selected for presentation. The top row shows the ground truth of the original image, the middle row shows the effect of our method (S-CMRL) without CCSSA, and the bottom row shows the effect of S-CMRL.}
	\label{gradcam}
\end{figure}



\section{CONCLUSION}
% 本文提出了一种语义感知的跨模态残差学习框架（S-CMRL），基于Transformer的多模态脉冲神经网络，有效整合了视觉和听觉模态的信息。通过引入跨模态互补时空脉冲注意力机制，S-CMRL能够在空间和时间两个维度上提取不同模态的互补特征，并将其作为残差融入原始特征中，从而增强特征表示的丰富性和表达能力。此外，语义关联优化机制通过在语义空间中对齐跨模态特征，进一步提升了特征的一致性和互补性。实验结果表明，S-CMRL在多个公开数据集上（包括CREMA-D、UrbanSound8K-AV和MNIST-DVS-NTIDIGITS）实现了优于现有方法的性能，且在噪声干扰下展现出较强的鲁棒性。消融实验进一步验证了所提机制在提升模型性能中的关键作用。
% 本研究为多模态脉冲神经网络的特征融合与表示提供了一种新的有效方法，展示了SNN在处理复杂多模态任务中的潜力。
This paper proposes a semantic-alignment cross-modal residual learning (S-CMRL) framework, a Transformer-based multimodal spiking neural network that effectively integrates visual and auditory modalities. By introducing a cross-modal complementary spatiotemporal spiking attention mechanism, S-CMRL extracts complementary features across both spatial and temporal dimensions and incorporates them as residual connections into the original features, thereby enhancing the richness and expressive capacity of feature representations. Furthermore, a semantic alignment optimization mechanism aligns cross-modal features in the semantic space, further improving their consistency and complementarity. Experimental results indicate that S-CMRL surpasses existing methods on multiple public datasets including CREMA-D, UrbanSound8K-AV, and MNISTDVS-NTIDIGITS, and demonstrates strong robustness under noisy conditions. Ablation studies validate the critical role of the proposed mechanism in boosting model performance. This work provides a novel and effective approach for feature fusion and representation in multimodal spiking neural networks, underscoring the potential of SNNs in handling complex multimodal tasks.

\bibliographystyle{IEEEtran}
\bibliography{New_IEEEtran_how-to}


\end{document}