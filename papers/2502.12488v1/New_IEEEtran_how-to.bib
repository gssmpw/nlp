@inproceedings{pian2023audio,
  title={Audio-visual class-incremental learning},
  author={Pian, Weiguo and Mo, Shentong and Guo, Yunhui and Tian, Yapeng},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={7799--7811},
  year={2023}
}

@article{huang2022masked,
  title={Masked autoencoders that listen},
  author={Huang, Po-Yao and Xu, Hu and Li, Juncheng and Baevski, Alexei and Auli, Michael and Galuba, Wojciech and Metze, Florian and Feichtenhofer, Christoph},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={28708--28720},
  year={2022}
}

@article{tong2022videomae,
  title={Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training},
  author={Tong, Zhan and Song, Yibing and Wang, Jue and Wang, Limin},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={10078--10093},
  year={2022}
}

@inproceedings{mo2023class,
  title={Class-incremental grouping network for continual audio-visual learning},
  author={Mo, Shentong and Pian, Weiguo and Tian, Yapeng},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={7788--7798},
  year={2023}
}

@inproceedings{mo2023audio,
  title={Audio-visual grouping network for sound localization from mixtures},
  author={Mo, Shentong and Tian, Yapeng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10565--10574},
  year={2023}
}

@inproceedings{he2024cace,
  title={CACE-Net: Co-guidance Attention and Contrastive Enhancement for Effective Audio-Visual Event Localization},
  author={He, Xiang and Liu, Xiangxi and Li, Yang and Zhao, Dongcheng and Shen, Guobin and Kong, Qingqun and Yang, Xin and Zeng, Yi},
  booktitle={Proceedings of the 32nd ACM International Conference on Multimedia},
  pages={985--993},
  year={2024}
}

@article{enoch2019evaluating,
  title={Evaluating whether sight is the most valued sense},
  author={Enoch, Jamie and McDonald, Leanne and Jones, Lee and Jones, Pete R and Crabb, David P},
  journal={JAMA ophthalmology},
  volume={137},
  number={11},
  pages={1317--1320},
  year={2019},
  publisher={American Medical Association}
}

@article{bulkin2006seeing,
  title={Seeing sounds: visual and auditory interactions in the brain},
  author={Bulkin, David A and Groh, Jennifer M},
  journal={Current opinion in neurobiology},
  volume={16},
  number={4},
  pages={415--419},
  year={2006},
  publisher={Elsevier}
}

@article{potamianos2004audio,
  title={Audio-visual automatic speech recognition: An overview},
  author={Potamianos, Gerasimos and Neti, Chalapathy and Luettin, Juergen and Matthews, Iain},
  journal={Issues in visual and audio-visual speech processing},
  volume={22},
  pages={23},
  year={2004},
  publisher={MIT Press Cambridge}
}

@inproceedings{peng2022balanced,
  title={Balanced multimodal learning via on-the-fly gradient modulation},
  author={Peng, Xiaokang and Wei, Yake and Deng, Andong and Wang, Dong and Hu, Di},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={8238--8247},
  year={2022}
}

@article{ernst2004merging,
  title={Merging the senses into a robust percept},
  author={Ernst, Marc O and B{\"u}lthoff, Heinrich H},
  journal={Trends in cognitive sciences},
  volume={8},
  number={4},
  pages={162--169},
  year={2004},
  publisher={Elsevier}
}

@article{noppeney2021perceptual,
  title={Perceptual inference, learning, and attention in a multisensory world},
  author={Noppeney, Uta},
  journal={Annual review of neuroscience},
  volume={44},
  pages={449--473},
  year={2021},
  publisher={Annual Reviews}
}

@inproceedings{gan2020music,
  title={Music gesture for visual sound separation},
  author={Gan, Chuang and Huang, Deng and Zhao, Hang and Tenenbaum, Joshua B and Torralba, Antonio},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10478--10487},
  year={2020}
}

@article{hu2020discriminative,
  title={Discriminative sounding objects localization via self-supervised audiovisual matching},
  author={Hu, Di and Qian, Rui and Jiang, Minyue and Tan, Xiao and Wen, Shilei and Ding, Errui and Lin, Weiyao and Dou, Dejing},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={10077--10087},
  year={2020}
}

@inproceedings{lin2019dual,
  title={Dual-modality seq2seq network for audio-visual event localization},
  author={Lin, Yan-Bo and Li, Yu-Jhe and Wang, Yu-Chiang Frank},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={2002--2006},
  year={2019},
  organization={IEEE}
}

@inproceedings{wang2022dualprompt,
  title={Dualprompt: Complementary prompting for rehearsal-free continual learning},
  author={Wang, Zifeng and Zhang, Zizhao and Ebrahimi, Sayna and Sun, Ruoxi and Zhang, Han and Lee, Chen-Yu and Ren, Xiaoqi and Su, Guolong and Perot, Vincent and Dy, Jennifer and others},
  booktitle={European Conference on Computer Vision},
  pages={631--648},
  year={2022},
  organization={Springer}
}

@article{yu2022stsc,
  title={Stsc-snn: Spatio-temporal synaptic connection with temporal convolution and attention for spiking neural networks},
  author={Yu, Chengting and Gu, Zheming and Li, Da and Wang, Gaoang and Wang, Aili and Li, Erping},
  journal={Frontiers in Neuroscience},
  volume={16},
  pages={1079357},
  year={2022},
  publisher={Frontiers Media SA}
}

@inproceedings{hu2016temporal,
  title={Temporal multimodal learning in audiovisual speech recognition},
  author={Hu, Di and Li, Xuelong and others},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3574--3582},
  year={2016}
}

@inproceedings{yang2020telling,
  title={Telling left from right: Learning spatial correspondence of sight and sound},
  author={Yang, Karren and Russell, Bryan and Salamon, Justin},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={9932--9941},
  year={2020}
}

@inproceedings{wu2019dual,
  title={Dual attention matching for audio-visual event localization},
  author={Wu, Yu and Zhu, Linchao and Yan, Yan and Yang, Yi},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={6292--6300},
  year={2019}
}

@inproceedings{liu2022event,
  title={Event-based multimodal spiking neural network with attention mechanism},
  author={Liu, Qianhui and Xing, Dong and Feng, Lang and Tang, Huajin and Pan, Gang},
  booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={8922--8926},
  year={2022},
  organization={IEEE}
}

@inproceedings{feichtenhofer2019slowfast,
  title={Slowfast networks for video recognition},
  author={Feichtenhofer, Christoph and Fan, Haoqi and Malik, Jitendra and He, Kaiming},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={6202--6211},
  year={2019}
}

@inproceedings{deng2022temporal,
  title={Temporal Efficient Training of Spiking Neural Network via Gradient Re-weighting},
  author={Deng, Shikuang and Li, Yuhang and Zhang, Shanghang and Gu, Shi},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{li2021free,
  title={A free lunch from ANN: Towards efficient, accurate spiking neural networks calibration},
  author={Li, Yuhang and Deng, Shikuang and Dong, Xin and Gong, Ruihao and Gu, Shi},
  booktitle={International conference on machine learning},
  pages={6316--6325},
  year={2021},
  organization={PMLR}
}

@inproceedings{zhou2022spikformer,
  title={Spikformer: When Spiking Neural Network Meets Transformer},
  author={Zhou, Zhaokun and Zhu, Yuesheng and He, Chao and Wang, Yaowei and Shuicheng, YAN and Tian, Yonghong and Yuan, Li},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{li2022spike,
  title={Spike calibration: Fast and accurate conversion of spiking neural network for object detection and segmentation},
  author={Li, Yang and He, Xiang and Dong, Yiting and Kong, Qingqun and Zeng, Yi},
  journal={arXiv preprint arXiv:2207.02702},
  year={2022}
}

@inproceedings{su2023deep,
  title={Deep directly-trained spiking neural networks for object detection},
  author={Su, Qiaoyi and Chou, Yuhong and Hu, Yifan and Li, Jianing and Mei, Shijie and Zhang, Ziyang and Li, Guoqi},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={6555--6565},
  year={2023}
}

@article{luo2020siamsnn,
  title={SiamSNN: spike-based siamese network for energy-efficient and real-time object tracking},
  author={Luo, Yihao and Xu, Min and Yuan, Caihong and Cao, Xiang and Xu, Yan and Wang, Tianjiang and Feng, Qi},
  journal={arXiv preprint arXiv:2003.07584},
  volume={10},
  pages={978--3},
  year={2020}
}

@inproceedings{cheni2021reducing,
  title={Reducing latency in a converted spiking video segmentation network},
  author={Cheni, Qinyu and Rueckauer, Bodo and Li, Li and Delbruck, Tobi and Liu, Shih-Chii},
  booktitle={2021 IEEE International Symposium on Circuits and Systems (ISCAS)},
  pages={1--5},
  year={2021},
  organization={IEEE}
}

@inproceedings{godet2021starflow,
  title={Starflow: A spatiotemporal recurrent cell for lightweight multi-frame optical flow estimation},
  author={Godet, Pierre and Boulch, Alexandre and Plyer, Aur{\'e}lien and Le Besnerais, Guy},
  booktitle={2020 25th International conference on pattern recognition (ICPR)},
  pages={2462--2469},
  year={2021},
  organization={IEEE}
}

@article{zaadnoordijk2022lessons,
  title={Lessons from infant learning for unsupervised machine learning},
  author={Zaadnoordijk, Lorijn and Besold, Tarek R and Cusack, Rhodri},
  journal={Nature Machine Intelligence},
  volume={4},
  number={6},
  pages={510--520},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{lin2023multimodality,
  title={Multimodality helps unimodality: Cross-modal few-shot learning with multimodal models},
  author={Lin, Zhiqiu and Yu, Samuel and Kuang, Zhiyi and Pathak, Deepak and Ramanan, Deva},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={19325--19337},
  year={2023}
}

@article{zhang2020efficient,
  title={An efficient threshold-driven aggregate-label learning algorithm for multimodal information processing},
  author={Zhang, Malu and Luo, Xiaoling and Chen, Yi and Wu, Jibin and Belatreche, Ammar and Pan, Zihan and Qu, Hong and Li, Haizhou},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  volume={14},
  number={3},
  pages={592--602},
  year={2020},
  publisher={IEEE}
}


@article{guo2023transformer,
  title={Transformer-based spiking neural networks for multimodal audio-visual classification},
  author={Guo, Lingyue and Gao, Zeyu and Qu, Jinye and Zheng, Suiwu and Jiang, Runhao and Lu, Yanfeng and Qiao, Hong},
  journal={IEEE Transactions on Cognitive and Developmental Systems},
  year={2023},
  publisher={IEEE}
}

@inproceedings{jiang2023cmci,
  title={CMCI: A Robust Multimodal Fusion Method for Spiking Neural Networks},
  author={Jiang, Runhao and Han, Jianing and Xue, Yingying and Wang, Ping and Tang, Huajin},
  booktitle={International Conference on Neural Information Processing},
  pages={159--171},
  year={2023},
  organization={Springer}
}

@inproceedings{auge2021end,
  title={End-to-end spiking neural network for speech recognition using resonating input neurons},
  author={Auge, Daniel and Hille, Julian and Kreutz, Felix and Mueller, Etienne and Knoll, Alois},
  booktitle={International Conference on Artificial Neural Networks},
  pages={245--256},
  year={2021},
  organization={Springer}
}

@article{wu2018spatio,
  title={Spatio-temporal backpropagation for training high-performance spiking neural networks},
  author={Wu, Yujie and Deng, Lei and Li, Guoqi and Zhu, Jun and Shi, Luping},
  journal={Frontiers in neuroscience},
  volume={12},
  pages={331},
  year={2018},
  publisher={Frontiers Media SA}
}

@inproceedings{wu2019direct,
  title={Direct training for spiking neural networks: Faster, larger, better},
  author={Wu, Yujie and Deng, Lei and Li, Guoqi and Zhu, Jun and Xie, Yuan and Shi, Luping},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={1311--1318},
  year={2019}
}

@book{dayan2005theoretical,
  title={Theoretical neuroscience: computational and mathematical modeling of neural systems},
  author={Dayan, Peter and Abbott, Laurence F},
  year={2005},
  publisher={MIT press}
}

@inproceedings{zheng2021going,
  title={Going deeper with directly-trained larger spiking neural networks},
  author={Zheng, Hanle and Wu, Yujie and Deng, Lei and Hu, Yifan and Li, Guoqi},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  number={12},
  pages={11062--11070},
  year={2021}
}

@inproceedings{fan2023pmr,
  title={Pmr: Prototypical modal rebalance for multimodal learning},
  author={Fan, Yunfeng and Xu, Wenchao and Wang, Haozhao and Wang, Junxiao and Guo, Song},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={20029--20038},
  year={2023}
}

@inproceedings{wu2022characterizing,
  title={Characterizing and overcoming the greedy nature of learning in multi-modal deep neural networks},
  author={Wu, Nan and Jastrzebski, Stanislaw and Cho, Kyunghyun and Geras, Krzysztof J},
  booktitle={International Conference on Machine Learning},
  pages={24043--24055},
  year={2022},
  organization={PMLR}
}

@inproceedings{yao2022modality,
  title={Modality-specific learning rates for effective multimodal additive late-fusion},
  author={Yao, Yiqun and Mihalcea, Rada},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2022},
  pages={1824--1834},
  year={2022}
}

@inproceedings{li2023boosting,
  title={Boosting multi-modal model performance with adaptive gradient modulation},
  author={Li, Hong and Li, Xingyu and Hu, Pengbo and Lei, Yinuo and Li, Chunxiao and Zhou, Yi},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={22214--22224},
  year={2023}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{cao2014crema,
  title={Crema-d: Crowd-sourced emotional multimodal actors dataset},
  author={Cao, Houwei and Cooper, David G and Keutmann, Michael K and Gur, Ruben C and Nenkova, Ani and Verma, Ragini},
  journal={IEEE transactions on affective computing},
  volume={5},
  number={4},
  pages={377--390},
  year={2014},
  publisher={IEEE}
}

@inproceedings{salamon2014dataset,
  title={A dataset and taxonomy for urban sound research},
  author={Salamon, Justin and Jacoby, Christopher and Bello, Juan Pablo},
  booktitle={Proceedings of the 22nd ACM international conference on Multimedia},
  pages={1041--1044},
  year={2014}
}

@article{serrano2013128,
  title={A 128 $\times$ 128 1.5\% Contrast Sensitivity 0.9\% FPN 3 $\mu$s Latency 4 mW Asynchronous Frame-Free Dynamic Vision Sensor Using Transimpedance Preamplifiers},
  author={Serrano-Gotarredona, Teresa and Linares-Barranco, Bernab{\'e}},
  journal={IEEE Journal of Solid-State Circuits},
  volume={48},
  number={3},
  pages={827--838},
  year={2013},
  publisher={IEEE}
}

@article{anumula2018feature,
  title={Feature representations for neuromorphic audio spike streams},
  author={Anumula, Jithendar and Neil, Daniel and Delbruck, Tobi and Liu, Shih-Chii},
  journal={Frontiers in neuroscience},
  volume={12},
  pages={23},
  year={2018},
  publisher={Frontiers Media SA}
}

@inproceedings{Adam2015,
  author       = {Diederik P. Kingma and
                  Jimmy Ba},
  title        = {Adam: {A} Method for Stochastic Optimization},
  booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015},
  year         = {2015},
}


@article{kim2021cromm,
  title={Cromm-vsr: Cross-modal memory augmented visual speech recognition},
  author={Kim, Minsu and Hong, Joanna and Park, Se Jin and Ro, Yong Man},
  journal={IEEE Transactions on Multimedia},
  volume={24},
  pages={4342--4355},
  year={2021},
  publisher={IEEE}
}

@article{yeo2024akvsr,
  title={Akvsr: Audio knowledge empowered visual speech recognition by compressing audio knowledge of a pretrained model},
  author={Yeo, Jeong Hun and Kim, Minsu and Choi, Jeongsoo and Kim, Dae Hoe and Ro, Yong Man},
  journal={IEEE Transactions on Multimedia},
  year={2024},
  publisher={IEEE}
}

@article{feng2023css,
  title={Css-net: A consistent segment selection network for audio-visual event localization},
  author={Feng, Fan and Ming, Yue and Hu, Nannan and Yu, Hui and Liu, Yuanan},
  journal={IEEE Transactions on Multimedia},
  year={2023},
  publisher={IEEE}
}

@article{xie2024eisnet,
  title={EISNet: A Multi-Modal Fusion Network for Semantic Segmentation with Events and Images},
  author={Xie, Bochen and Deng, Yongjian and Shao, Zhanpeng and Li, Youfu},
  journal={IEEE Transactions on Multimedia},
  year={2024},
  publisher={IEEE}
}

@article{li2024spiking,
  title={Spiking Neural Networks with Consistent Mapping Relations Allow High-Accuracy Inference},
  author={Li, Yang and He, Xiang and Kong, Qingqun and Zeng, Yi},
  journal={Information Sciences},
  pages={120822},
  year={2024},
  publisher={Elsevier}
}

@article{shen2023astrocyte,
  title={Astrocyte-Enabled Advancements in Spiking Neural Networks for Large Language Modeling},
  author={Shen, Guobin and Zhao, Dongcheng and Dong, Yiting and Li, Yang and Li, Jindong and Sun, Kang and Zeng, Yi},
  journal={arXiv preprint arXiv:2312.07625},
  year={2023}
}

@article{shen2023brain,
  title={Brain-inspired neural circuit evolution for spiking neural networks},
  author={Shen, Guobin and Zhao, Dongcheng and Dong, Yiting and Zeng, Yi},
  journal={Proceedings of the National Academy of Sciences},
  volume={120},
  number={39},
  pages={e2218173120},
  year={2023},
  publisher={National Acad Sciences}
}

@article{su2024snn,
  title={SNN-BERT: Training-efficient Spiking Neural Networks for energy-efficient BERT},
  author={Su, Qiaoyi and Mei, Shijie and Xing, Xingrun and Yao, Man and Zhang, Jiajun and Xu, Bo and Li, Guoqi},
  journal={Neural Networks},
  volume={180},
  pages={106630},
  year={2024},
  publisher={Elsevier}
}

@article{xiao2022towards,
  title={Towards energy-preserving natural language understanding with spiking neural networks},
  author={Xiao, Rong and Wan, Yu and Yang, Baosong and Zhang, Haibo and Tang, Huajin and Wong, Derek F and Chen, Boxing},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={31},
  pages={439--447},
  year={2022},
  publisher={IEEE}
}

@article{wang2024spikevoice,
  title={SpikeVoice: High-Quality Text-to-Speech Via Efficient Spiking Neural Network},
  author={Wang, Kexin and Zhang, Jiahong and Ren, Yong and Yao, Man and Shang, Di and Xu, Bo and Li, Guoqi},
  journal={arXiv preprint arXiv:2408.00788},
  year={2024}
}

@inproceedings{yang2024svad,
  title={SVAD: A Robust, Low-Power, and Light-Weight Voice Activity Detection with Spiking Neural Networks},
  author={Yang, Qu and Liu, Qianhui and Li, Nan and Ge, Meng and Song, Zeyang and Li, Haizhou},
  booktitle={ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={221--225},
  year={2024},
  organization={IEEE}
}

@article{pan2021multi,
  title={Multi-tone phase coding of interaural time difference for sound source localization with spiking neural networks},
  author={Pan, Zihan and Zhang, Malu and Wu, Jibin and Wang, Jiadong and Li, Haizhou},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={29},
  pages={2656--2670},
  year={2021},
  publisher={IEEE}
}

@article{feng2024spiking,
  title={Spiking generative adversarial network with attention scoring decoding},
  author={Feng, Linghao and Zhao, Dongcheng and Zeng, Yi},
  journal={Neural Networks},
  pages={106423},
  year={2024},
  publisher={Elsevier}
}

@inproceedings{chattopadhay2018grad,
  title={Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks},
  author={Chattopadhay, Aditya and Sarkar, Anirban and Howlader, Prantik and Balasubramanian, Vineeth N},
  booktitle={2018 IEEE winter conference on applications of computer vision (WACV)},
  pages={839--847},
  year={2018},
  organization={IEEE}
}

@article{Zeng2023,
  doi = {10.1016/j.patter.2023.100789},
  url = {https://doi.org/10.1016/j.patter.2023.100789},
  year = {2023},
  month = jul,
  publisher = {Cell Press},
  pages = {100789},
  author = {Yi Zeng and Dongcheng Zhao and Feifei Zhao and Guobin Shen and Yiting Dong and Enmeng Lu and Qian Zhang and Yinqian Sun and Qian Liang and Yuxuan Zhao and Zhuoya Zhao and Hongjian Fang and Yuwei Wang and Yang Li and Xin Liu and Chengcheng Du and Qingqun Kong and Zizhe Ruan and Weida Bi},
  title = {{BrainCog}: A spiking neural network based,  brain-inspired cognitive intelligence engine for brain-inspired {AI} and brain simulation},
  journal = {Patterns}
}