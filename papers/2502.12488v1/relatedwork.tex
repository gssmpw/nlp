\section{RELATED WORK}
\subsection{Audio-Visual Learning}
% 在神经网络中，多模态融合整合了两种或两种以上的模态，以应对复杂的任务，并提高模型的准确性和泛化能力。视听学习主要侧重于揭示视觉和听觉模态之间的关系，并将特征融合作为核心研究重点。Hu 等人 ~\cite{hu2016temporal} 和 Yang 等人 ~\cite{yang2020telling} 探索了特征串联，沿特定维度融合视听特征，生成统一的特征向量。除了简单的并集之外，Wu 等人还提出了双重注意力匹配模块，以促进更高层次的事件信息建模。此外，一些研究还解决了单模态不平衡的问题~/cite{feichtenhofer2019slowfast, peng2022balanced}，通过加强跨模态对齐和减少模态间差异来优化融合。Yeo 等人~\cite{yeo2024akvsr}利用音频模式作为补充源，解决了视觉模式中语音信息不足的问题。在视听模式中，视觉部分通常提供空间信息，而听觉部分则捕捉时间动态；因此，每种模式都呈现出不同的表征形式和语义信息。因此，设计一个能够动态融合这些互补元素的网络，已成为视听学习研究中的一个关键挑战。
In neural networks, multimodal fusion integrates two or more modalities to tackle complex tasks and improve model accuracy and generalization. Audio-visual learning primarily focuses on uncovering relationships between visual and auditory modalities, with feature fusion as the central research focus. Hu et al.~\cite{hu2016temporal} and Yang et al.~\cite{yang2020telling} explored feature concatenation, fusing audio-visual features along specific dimensions to generate a unified feature vector. Moving beyond simple concatenation, Wu et al.~\cite{wu2019dual} proposed a dual attention matching module to facilitate higher-level event information modeling. Moreover, certain studies addressed the issue of single-modal imbalance~\cite{feichtenhofer2019slowfast, peng2022balanced} by strengthening cross-modal alignment and reducing inter-modal discrepancies to optimize fusion. Yeo et al.~\cite{yeo2024akvsr} tackled the problem of insufficient speech information in the visual modality by leveraging the audio modality as a complementary source. In audio-visual modalities, the visual modality typically provides spatial information, whereas the auditory component captures temporal dynamics; each modality thus presents distinct representational patterns and semantic information. Consequently, designing a network that can dynamically fuse these complementary elements has become a pivotal challenge in audio-visual learning research.

\subsection{Unimodal Spiking Neural Networks}
% 现有的脉冲神经网络研究工作大多集中在对单模态的研究上。

% 在视觉相关任务中，Wu等人~\cite{wu2018spatio}对发射脉冲的函数进行梯度近似以实现梯度计算，并在空间域和时间域分别使用反向传播，即时空反向传播（Spatio-Temporal Backpropagation，STBP）。在其进一步研究中~\cite{wu2019direct}，将常用的LIF神经元模型~\cite{dayan2005theoretical}表达为离散的可迭代的形式。Deng等人~\cite{deng2022temporal}引入了时间效率训练方法，使SNN收敛到更平坦的最小值。Zhou等人~\cite{zhou2022spikformer}提出了Spikformer，利用脉冲自注意力机制，通过使用脉冲形式的Query、Key和Value来建模稀疏的视觉特征。
% 在音频相关任务中，Auge等人~\cite{auge2021end}使用共振神经元作为脉冲神经网络的输入层，用于在线音频分类。Yu等人~\cite{yu2022stsc}提出了由时间响应滤波器模块和前馈横向抑制模块组成的时空突触连接模块, 在语音数字识别上进行了实验验证。
% 尽管在单一模态上取得了显著进展, 随着多模态学习的兴起，结合不同模态信息以增强模型的表达能力已成为提升模型泛化性的有效途径。因此，如何将传统的单模态SNN技术扩展到多模态场景，成为当前及未来脉冲神经网络领域中的重要挑战之一。

Most existing research on spiking neural networks (SNNs) focuses on single-modal tasks. 
For vision-related tasks, Wu et al.~\cite{wu2018spatio} introduced gradient approximation of spike functions for gradient computation, applying backpropagation in both the spatial and temporal domains, namely Spatio-Temporal Backpropagation (STBP). In subsequent work~\cite{wu2019direct}, they presented a discrete and iterative form of the commonly used LIF neuron model~\cite{dayan2005theoretical}. Deng et al.~\cite{deng2022temporal} proposed a temporal-efficient training approach that converges SNNs to flatter minima. Zhou et al.~\cite{zhou2022spikformer} introduced Spikformer, incorporating a spiking self-attention mechanism that leverages spike-based queries, keys, and values to capture sparse visual features.
In audio-related tasks, Auge et al.~\cite{auge2021end} employed resonator neurons as the input layer of an SNN for online audio classification. Yu et al.~\cite{yu2022stsc} proposed a spatiotemporal synaptic connection module composed of a temporal response filter module and a feedforward lateral inhibition module, demonstrating its efficacy in spoken digit recognition.
Although unimodal approaches have achieved significant progress, the rising importance of multimodal learning highlights the need to integrate diverse modalities to enhance the representational capacity and generalization of models. Consequently, extending traditional unimodal SNN techniques to multimodal contexts is emerging as a key challenge in the current and future development of spiking neural networks.

\subsection{Audio-Visual Spiking Neural Networks}
% 视听脉冲神经网络的研究相对较少，早期的一些工作主要局限于简单地连接或叠加视觉和听觉模态的特征。Zhang等人~\cite{zhang2020efficient}通过兴奋性和抑制性侧连接的方式，将针对单模态训练的SNN进行跨模态的耦合；Liu等人~\cite{liu2022event}提出了一种基于注意力机制的跨模态网络，利用注意力机制评估各模态的重要性，并将权重分配给两种模态，以实现跨模态融合。随后，研究者们开始设计更为优越的多模态融合方式以适应多模态脉冲神经网络的发展。Guo等人~\cite{guo2023transformer}将SNN和Transformer相结合，整合了视觉和听觉的单模态子网络，并引入新型的脉冲交叉注意模块，用于多模态视听分类；Jiang等人~\cite{jiang2023cmci}提出了一种跨模态电流集成模块，对不同模态的脉冲神经网络进行特征级或决策级融合。尽管如此，这些方法仍然忽视了听觉和视觉模态的独有特征以及其间的互补作用。在我们的工作中，我们保留了听觉和视觉模态的独有特征，并使单模态能够从跨模态的互补特征中受益，从而实现更高效的跨模态融合。
Research on audio visual spiking neural networks (AV-SNNs) remains limited. Early studies primarily focused on simple connections or straightforward combinations of visual and auditory modality features. Zhang et al.~\cite{zhang2020efficient} employed excitatory and inhibitory lateral connections to facilitate cross-modal coupling in SNNs trained on individual modalities. Liu et al.~\cite{liu2022event} introduced an attention-based cross-modal network that leverages an attention mechanism to weigh each modality's contribution, enabling cross-modal fusion.
More recently, researchers have developed advanced multimodal fusion methods to improve AV-SNNs. Guo et al.~\cite{guo2023transformer} integrated SNNs with Transformers, combining unimodal sub-networks for visual and auditory modalities and proposing a novel spiking cross-attention module for audio-visual classification. Jiang et al.~\cite{jiang2023cmci} proposed a cross-modal current integration module that fuses SNNs from different modalities at either the feature or decision level.
However, existing approaches overlook the unique characteristics of auditory and visual modalities and their complementary interactions. In this work, we preserve the distinct features of each modality and design mechanisms that enable each unimodal branch to leverage complementary information from the other. This approach achieves more effective cross-modal fusion and enhances multimodal learning performance.