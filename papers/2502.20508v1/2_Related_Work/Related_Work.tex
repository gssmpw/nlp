\section{Related Work}

% \noindent\textbf{Planning using LLMs:} Large language models are revolutionizing our approach to planning tasks. They will become important tools for solving planning-related tasks across diverse domains. Investigation of their capabilities has shown that they can assist in solving problems like task planning, heuristic guidance, and common-sense reasoning. LLM-Planner, proposed by \citet{song2023llmplanner}, demonstrated few-shot grounded planning by updating high-level plans based on real-time feedback from the environment. \citet{zhao2024large} demonstrates that integrating LLMs with algorithms like Monte-Carlo Tree Search (MCTS) \cite{10.5555/1777826.1777833,swiechowski2023monte} can considerably improve task planning. While \citet{valmeekam2023planningabilitieslargelanguage} demonstrated the usefulness of LLMs in guiding established planners, it also came in the spotlight that LLMs are still not able to generate effective plans independently and across multiple domains. Another highlighted problem is that LLMs struggle to address subgoal interactions and need supplementary technologies for meaningful planning capabilities \citep{kambhampati2023role}. Strategies like chain-of-thought prompting and fine-tuning improve the LLM performance, but they also expose the low efficiency of LLMs when facing novel and unseen challenges \citep{yang2023planning, bohnet2024exploring}.

% \noindent\textbf{LLMs in Travel Planning: }Travel Planning is often a tedious process. Multiple subgoals like budget, schedule, and optimization of travel options are often very confounding and difficult to solve for. On top of it, personalization based on  the traveler's current and previous choices becomes overwhelming for humans as well. But the advent of LLMs show a hope of reducing all this effort to a few lines of text. In this regard, \citet{xie2024travelplanner} crafted a realistic benchmark for testing this theory with 1,225 queries. With eight commonsense constraints and five hard constraints, they found out that the LLMs performed very poorly for this task due to their inability to manage multi-constraint tasks. Although \citet{hao2025largelanguagemodelssolve} has shown impressive results in this task, they have omitted many constraints, and thus simplified the task. Lack of a real-world dataset is another limitation found in these studies.

% \noindent\textbf{Current Evaluation Scenario:} The LLMs have been judged for their travel planning capabilities using various different evaluation metrics. Ability to deliver a plan through a limited number of steps, without failing, was judged by the Delivery Rate metric. Further, \citet{xie2024travelplanner} judged them by measuring the adherence to explicit user needs and implicit real-world considerations by the language agents. And if a plan met all three aforementioned checks, it was considered good enough for practical purposes. This benchmark evaluation strategy has been followed by subsequent papers. \citet{chen2024travelagentaiassistantpersonalized} introduced new metrics for measuring rationality, personalization, and comprehensiveness. \citet{singh-etal-2024-personal} introduced the metric 'Preference Rate' which measured the ratio of personalized plans that were preferred by the language agent over generic plans. But, to judge a plan as practical, we must evaluate it for other metrics that check the temporal, spatial, and sequential accuracy. We have addressed this in \textit{TripCraft} for better examination of the generated plans.


\noindent\textbf{Planning with Large Language Models.}  
Large language models (LLMs) have demonstrated significant potential in various planning tasks, including task scheduling, heuristic guidance, and commonsense reasoning \cite{borro2025large, huang2024making, valmeekam2023planningabilitieslargelanguage, prasad2024adapt, pallagani2023understanding, lee2025evolving}. LLM-Planner \citep{song2023llmplanner} introduced few-shot grounded planning, dynamically updating high-level plans based on real-time feedback. \citet{zhao2024large} showed that integrating LLMs with classical planning techniques, such as Monte Carlo Tree Search (MCTS) \citep{10.5555/1777826.1777833,swiechowski2023monte}, enhances task-planning efficiency. However, despite their promise, LLMs struggle with generating effective plans independently across diverse domains \citep{valmeekam2023planningabilitieslargelanguage}. Moreover, they face challenges in handling subgoal dependencies and require external reasoning mechanisms for robust planning \citep{kambhampati2023role}. Techniques such as chain-of-thought prompting and fine-tuning can improve performance but expose limitations when encountering novel, complex scenarios \citep{yang2023planning,bohnet2024exploring}.  

\noindent\textbf{LLMs in Travel Planning.}  
Automated travel planning is inherently complex, requiring the optimization of multiple subgoals such as scheduling, budgeting, and route efficiency, while also incorporating user preferences. The emergence of LLMs presents an opportunity to streamline this process through natural language interaction \citep{xi2025rise, jonnala2025exploring}. \citet{xie2024travelplanner} introduced a benchmark with 1,225 travel-related queries, assessing LLMs against eight commonsense and five hard constraints. Their study revealed that LLMs struggle with multi-constraint optimization, leading to suboptimal travel plans. While papers like \citep{hao2025largelanguagemodelssolve} and \citep{gundawar2024robust} reported strong performance in travel planning, their methodology omitted key constraints, simplifying the task. A major limitation in these studies is the absence of real-world datasets that incorporate public transit schedules, event calendars, and personalization factors, restricting their applicability \citep{shao2024chinatravel}.  

\noindent\textbf{Evaluation of LLM-Generated Travel Plans.}  
Existing evaluations of LLM-based travel planning rely on discrete constraint-checking methodologies. Metrics such as Delivery Rate measure an LLM’s ability to generate a plan without failure, while \citet{xie2024travelplanner} introduced assessments for explicit user requirements and implicit real-world feasibility. If a plan met all three criteria, it was deemed viable. Subsequent studies have built upon this framework; for instance, \citet{chen2024travelagentaiassistantpersonalized} introduced metrics for rationality, personalization, and comprehensiveness, while \citet{singh-etal-2024-personal} proposed the Preference Rate metric, quantifying how often a personalized plan was favored over a generic one. However, evaluating travel plans solely based on constraint adherence is insufficient. A robust assessment must consider temporal, spatial, and sequential coherence—dimensions largely overlooked in prior works. Our proposed benchmark, \tripcraft, addresses this gap by introducing continuous evaluation metrics that provide a fine-grained analysis of itinerary quality.  

