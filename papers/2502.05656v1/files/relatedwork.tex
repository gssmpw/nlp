%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Related Work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Work}

Our work builds upon several streams of research, which we now discuss in detail.

\subsection{Transformer Architectures}
The transformer architecture \citep{vaswani2017attention} has dramatically transformed natural language processing and computer vision. Its design—relying on self-attention mechanisms and position-wise feed-forward networks—has led to state-of-the-art performance on a variety of tasks. Subsequent works, such as BERT \citep{devlin2019bert} and the GPT series \citep{radford2018improving,radford2019language}, have further refined these ideas, emphasizing the importance of scale and pre-training. Our analysis extends this body of work by providing a continuous-time perspective on transformer updates, thereby offering new insights into their stability and expressivity.

\subsection{Continuous-Time Models and Neural ODEs}
The recent introduction of Neural Ordinary Differential Equations (Neural ODEs) \citep{Chen2018NeuralODEs} has opened the door to interpreting deep networks as continuous dynamical systems. In this framework, the evolution of hidden states is governed by an ODE, and the network can be viewed as a discretization of this continuous process. Our work leverages similar ideas to bridge transformer architectures and continuous-time dynamics, providing a rigorous foundation for understanding the convergence of discrete transformer updates to an underlying ODE.

\subsection{Dynamical Systems Perspective in Deep Learning}
Viewing deep neural networks through the lens of dynamical systems has yielded valuable insights into their training dynamics and expressivity \citep{ruthotto2018deep,haber2017stable}. Several studies have analyzed stability, convergence, and robustness by examining the flow of activations over layers. Our work builds on these ideas by demonstrating that under standard Lipschitz continuity assumptions—and even more so under a one-sided Lipschitz condition—the transformer update not only approximates a continuous flow but also exhibits contractive behavior that contributes to model robustness.

\subsection{Iterative Reasoning and Feedback Convergence}
A recent work by \citet{feinashley2025iterate} introduces a unified framework for iterative reasoning and feedback convergence, which generalizes classical update schemes via non-Euclidean geometries. This framework encompasses many iterative methods—including mirror descent and dynamic programming—while providing rigorous guarantees on accelerated convergence and expressivity through feedback mechanisms. In our work, we show that the transformer update can be interpreted as a specific instance of this broader iterative update paradigm, thereby connecting transformer dynamics with continuous-time flows and the emerging theory of iterative reasoning.

\subsection{Stability and Robustness in Deep Networks}
Stability is a key property for ensuring the robustness of deep models, particularly in the face of perturbations such as adversarial attacks or numerical errors \citep{cisse2017parseval,miyato2018spectral}. Prior works have explored regularization techniques to control the Lipschitz constant of networks and thereby improve their robustness. Our theoretical results connect these ideas to transformer architectures by showing that when the underlying mapping satisfies a one-sided Lipschitz condition with a negative constant, the dynamics are contractive. This not only explains the empirical robustness of transformers but also suggests avenues for designing more stable models.
