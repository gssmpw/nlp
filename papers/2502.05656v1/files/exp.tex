%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Experiments
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiments}

In this section, we present a series of experiments designed to validate our theoretical results. We evaluate (i) the convergence behavior of the forward Euler discretization of the transformer flow, (ii) the robustness and error propagation under perturbations, and (iii) the accelerated convergence of the iterative update framework as applied to transformer dynamics.

\subsection{Convergence of the Euler Method on a Synthetic Dynamical System}
\label{sec:toy_dynamical_system}

We first consider a synthetic dynamical system given by the linear ODE
\[
\frac{d\mathbf{x}(t)}{dt} = A\, \mathbf{x}(t),
\]
where \(A \in \mathbb{R}^{d \times d}\) is chosen such that \(\|A\|\le L\) and its eigenvalues have negative real parts, ensuring stability and satisfying the one-sided Lipschitz condition with some \(\lambda < 0\). We set \(d=10\) and select \(A\) as a stable random matrix (e.g., by generating a random matrix and subtracting a multiple of the identity). The exact solution is given by
\[
\mathbf{x}(t) = e^{At}\,\mathbf{x}(0).
\]

We compute the forward Euler approximation
\[
\mathbf{x}_{n+1} = \mathbf{x}_n + \Delta t\, A\,\mathbf{x}_n,\quad \Delta t = \frac{1}{N},
\]
for various values of \(N\). We then measure the maximum error over the interval \(t\in[0,1]\):
\[
E(N) = \max_{0\le n\le N}\|\mathbf{x}(n\Delta t) - \mathbf{x}_n\|.
\]
Figure~\ref{fig:euler_convergence} shows a log-log plot of \(E(N)\) versus \(N\), which confirms the \(O(1/N)\) convergence rate predicted by classical numerical analysis.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{imgs/plot_experiment1_euler_convergence.png}
  \caption{Log-log plot of the maximum error \(E(N)\) versus the number of time steps \(N\) for the Euler method. The observed \(O(1/N)\) convergence rate verifies the theoretical analysis.}
  \label{fig:euler_convergence}
\end{figure}

\subsection{Robustness and Error Propagation Under Perturbations}
\label{sec:robustness}

To validate the discrete contractivity result (Theorem~\ref{thm:discrete_contractivity}), we simulate two trajectories of the discrete update starting from slightly different initial conditions, \(\mathbf{x}_0\) and \(\mathbf{y}_0\). Using the same linear system as above, we compute the evolution of the error
\[
\epsilon_n = \|\mathbf{x}_n - \mathbf{y}_n\|.
\]
Theorem~\ref{thm:discrete_contractivity} predicts that
\[
\epsilon_n \le e^{\lambda\, n\Delta t} \|\mathbf{x}_0 - \mathbf{y}_0\|,
\]
with \(\lambda < 0\) leading to exponential decay. Figure~\ref{fig:error_decay} plots \(\epsilon_n\) versus \(n\) and verifies that the error decays at the predicted exponential rate, demonstrating robustness to small perturbations.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{imgs/plot_experiment2_robustness.png}
  \caption{Semilog-y plot of the error \(\epsilon_n\) versus iteration \(n\). The exponential decay confirms that perturbations are attenuated, consistent with the contractivity of the update.}
  \label{fig:error_decay}
\end{figure}

\subsection{Accelerated Convergence via the Iterative Update Framework}
\label{sec:accelerated_experiment}

We now test the accelerated convergence of the unified iterative update framework described in \cite{feinashley2025iterate}. We consider a fixed-point iteration problem where the operator \(\mathcal{T}\) is defined as
\[
\mathcal{T}(s) = s + B\,(s-s^*),
\]
with \(B\) chosen such that \(\mathcal{T}\) is contractive in a Bregman divergence \(D(\cdot,\cdot)\). We compare two update schemes:
\begin{itemize}[leftmargin=2em]
    \item \textbf{Standard Update:} \( s_{t+1} = s_t + \alpha\, \bigl[\mathcal{T}(s_t) - s_t\bigr] \) with a constant averaging parameter \(\alpha = 1/N\).
    \item \textbf{Adaptive (Accelerated) Update:} \( s_{t+1} = s_t + \alpha_t\, \bigl[\mathcal{T}(s_t) - s_t\bigr] \) with \(\alpha_t = \frac{2}{t+2}\).
\end{itemize}

For each scheme, we measure the error \(e_t = D(s_t, s^*)\) over iterations. As established in \cite{feinashley2025iterate}, the adaptive update should achieve an \(O(1/t^2)\) convergence rate. Figure~\ref{fig:accelerated_convergence} presents a log-log plot of \(e_t\) versus \(t\) for both methods. The plot clearly demonstrates that the adaptive update converges significantly faster, in line with the \(O(1/t^2)\) rate.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{imgs/plot_experiment3_accelerated_convergence.png}
  \caption{Log-log plot comparing the convergence errors of the standard update and the adaptive (accelerated) update. The adaptive update achieves an \(O(1/t^2)\) convergence rate, indicating faster convergence.}
  \label{fig:accelerated_convergence}
\end{figure}

\subsection{Implementation Details}

All experiments were implemented in Python using NumPy and SciPy for numerical computations and Matplotlib for plotting. Reproducibility was ensured by fixing random seeds; detailed hyperparameter settings are provided in the supplementary material. Each experiment was run over multiple trials, and error bars (where applicable) denote one standard deviation over these trials.

\subsection{Discussion of Experimental Results}

Our experimental results provide compelling empirical evidence in support of our theoretical analysis:
\begin{itemize}[leftmargin=2em]
    \item The convergence experiments (Section~\ref{sec:toy_dynamical_system}) confirm that the forward Euler discretization converges at the expected \(O(1/N)\) rate.
    \item The robustness study (Section~\ref{sec:robustness}) verifies that, under the one-sided Lipschitz condition, perturbations decay exponentially with depth, underscoring the stability of transformer updates.
    \item The accelerated convergence experiments (Section~\ref{sec:accelerated_experiment}) clearly demonstrate that employing an adaptive averaging parameter leads to significantly faster convergence, achieving an \(O(1/t^2)\) rate. This has important implications for transformer architectures, suggesting that integrating iterative feedback mechanisms and adaptive update schemes can enhance both convergence speed and overall robustness.
\end{itemize}

These experiments not only validate our theoretical findings but also provide practical insights into the design of transformer models and iterative reasoning systems. Future work may extend these studies to large-scale transformer architectures and real-world tasks, exploring how these convergence properties impact performance in natural language processing and other domains.
