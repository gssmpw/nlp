\section{Introduction}
\label{sec:intro}
Recent advances in deep learning have been propelled by the success of transformer architectures \citep{vaswani2017attention}, which excel across domains such as natural language processing and computer vision. \textbf{Yet despite their empirical achievements, a fundamental theoretical understanding of how transformer layers evolve token representations remains elusive.} In this paper, we address this gap by showing that standard transformer updates can be interpreted as a forward Euler discretization of a continuous-time dynamical system. 

We begin by placing our analysis in the context of three principal research themes. \textbf{First}, we draw on the rich literature on transformer architectures \citep{devlin2019bert,radford2018improving,radford2019language}, framing them in a continuous-time perspective that reveals new insights about their stability and expressivity. \textbf{Second}, we leverage the viewpoint of neural ordinary differential equations (Neural ODEs) \citep{Chen2018NeuralODEs}, highlighting how transformers can be viewed as discrete realizations of continuous flows. \textbf{Third}, we connect to the broader dynamical systems perspective in deep learning \citep{ruthotto2018deep,haber2017stable}, establishing that transformers with standard Lipschitz assumptions can exhibit contractive behavior under a one-sided Lipschitz condition. 

\textbf{Our primary contributions} can be summarized as follows:

\begin{itemize}
    \item \textbf{Transformer Flow Approximation:} We rigorously prove that transformer updates converge to the unique solution of a corresponding ordinary differential equation (ODE), thus formalizing the notion that layer stacking in transformers approximates a continuous flow.

    \item \textbf{Stability via One-Sided Lipschitz Conditions:} We demonstrate that if the transformerâ€™s mapping satisfies a one-sided Lipschitz condition with a negative constant, then the dynamics are contractive. This property ensures that small perturbations are dampened exponentially, contributing to the robustness of deep transformer models.

    \item \textbf{Unification with Iterative Reasoning Frameworks:} By showing that transformer updates are special cases of a broader iterative update scheme \citep{feinashley2025iterate}, we bridge the gap between classical iterative methods (e.g., mirror descent) and modern transformer architectures. This unification paves the way for accelerated convergence strategies in transformer-like systems.

    \item \textbf{Empirical Validation:} We present experiments on synthetic and controlled settings that confirm the convergence rates, stability, and robustness derived in our theoretical results. We also illustrate how adaptive averaging parameters can yield faster convergence in practice.
\end{itemize}

\textbf{Overall, our work provides both a solid theoretical foundation} for interpreting transformer layers as discretized continuous flows and new perspectives on improving their convergence and stability. By linking transformers to well-studied tools in numerical analysis and dynamical systems theory, we open the door to innovative architectural designs and algorithmic improvements that harness accelerated iterative methods. 
