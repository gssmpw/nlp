\section{Omitted proof in \cref{subsubsec:investment}}\label{appendix: seq general cost}


\begin{proof}[Proof of \cref{lem: optimal max qualified improving effort}]
    % Suppose there exists a feasible uninformed random order mechanism in which no agent prefers a two-step strategy. 
    % Denote the two tests used in this mechanism by $\tilde \classifier_A$ and $\tilde \classifier_B$. 
    % Denote the probability of the order $\tilde \classifier_A \rightarrow \tilde \classifier_B$ (the probability of first using $\tilde \classifier_A$ and then using $\tilde \classifier_B$) in this mechanism by 
    % $q$.   
    Since this mechanism $(\tilde \classifier_A, \tilde \classifier_B,q,\nullset)$ is feasible, i.e., no unqualified agent is selected, we can infer that the region that satisfies both tests $\tilde \classifier_A$ and $\tilde \classifier_B$ is contained in the qualified region $\classifier_A \cap \classifier_B$, i.e., $\tilde \classifier_A\cup\tilde \classifier_B\subset\classifier_A \cap \classifier_B$. If not, then there is an unqualified agent with attributes $\features \in \tilde\classifier_A \cap \tilde\classifier_B \setminus (\classifier_A \cap \classifier_B)$ and this agent can get accepted by this mechanism without changing his attributes. A contradiction.
    
    Let $O$ be the intersection point of the boundary lines of $\classifier_A$ and $\classifier_B$.
    Let $\tilde O$ be the intersection point of  the boundary lines of $\tilde \classifier_A$ and $ \tilde \classifier_B$. 
    We use $\classifier_A'$ and $\classifier_B'$ to denote the tests shifted from $\tilde\classifier_A$ and $\tilde\classifier_B$ by $O - \tilde O$, respectively. 
    Thus, the boundary lines of the two tests $\classifier_A'$ and $\classifier_B'$ also intersect at point $O$. 
    
    We show that the uninformed random order mechanism with two tests $\classifier_A'$ and $\classifier_B'$ and probability $q$ is also feasible, i.e., no qualified agent is selected.
    It is equivalent to show that any agent being accepted by this new mechanism has qualified attributes, i.e., attributes in the qualified region $\classifier_A \cap \classifier_B$.
    Since the cost is translation invariant, it must be that no agent prefers any two-step strategy in this new mechanism $(\classifier_A',\classifier_B',\probprincipal,\nullset)$. Otherwise, we can find an agent who prefers a two-step strategy in the initial uninformed random order mechanism with tests $\tilde\classifier_A$ and $\tilde \classifier_B$ and probability $q$.
    Note that the region that satisfies both tests $\classifier_A'$ and $\classifier_B'$ is also contained in $\classifier_A \cap \classifier_B$, i.e., $\classifier_A' \cap \classifier_B'\subset \classifier_A \cap \classifier_B$. 
    Combining with the fact that agent only uses one-step strategy in the new mechanism, we can conclude that whoever gets accepted by the new mechanism must eventually have attributes in the qualified region.
    Therefore, this new uninformed random order mechanism only accepts qualified agents after investment. 
    
    We now show that the uninformed random order mechanism with two tests $\classifier_A$ and $\classifier_B$ and probability $q$ is also feasible. To prove this, we show that there is no agent prefers any two-step strategy in this mechanism $(\classifier_A,\classifier_B,q,\nullset)$. Suppose there is an agent with attributes $\features$ who prefers a two-step strategy $(\features, \firstfeatures, \secondfeatures)$. Let $\features'$ be the attributes in $\classifier_A \cap \classifier_B$ that minimizes the cost $c(\features,\features',\features')$. Then, we have the expected utility of this two-step strategy is at least the utility of improving to $\features'$. 
    Next, we show that this agent can get accepted with the same probability by taking this two-step strategy in the uninformed random order mechanism with tests $\classifier_A',\classifier_B'$ and randomization probability $q$, i.e., $(\classifier_A',\classifier_B',\probprincipal,\nullset)$. 
    Without loss of generality, we consider that $\firstfeatures$ satisfies $\classifier_A$ but not $\classifier_B$ and $\secondfeatures$ satisfies $\classifier_B$ but not $\classifier_A$. Since $\classifier_A'$ goes through $O$ and $\classifier_A' \cap \classifier_B'$ is contained in $\classifier_A \cap \classifier_B$, we have $\classifier_A \setminus \classifier_B \subseteq \classifier_A' \setminus \classifier_B$. Thus, we can infer that $\firstfeatures$ also satisfies $\classifier_A'$. Similarly, we have that $\secondfeatures$ satisfies $\classifier_B'$. Thus, this agent can also get accepted with this two-step strategy in the uninformed random order mechanism with tests $\classifier_A'$ and $\classifier_B'$. 
    Let $\features''$ be the attributes in $\classifier_A' \cap \classifier_B'$ that minimizes the cost $c(\features,\features'',\features'')$.
    Since $\classifier_A' \cap \classifier_B'$ is contained in $\classifier_A \cap \classifier_B$, we have $c(\features,\feature'',\features'') \geq c(\features, \features',\features')$. Therefore, this agent would prefer the two-step strategy in the uninformed random order mechanism with $\classifier_A'$ and $\classifier_B'$, which provides a contradiction.

    Since there is no agent prefers a two-step strategy in the uninformed random order mechanism with two tests $\classifier_A,\classifier_B$ and probability $q$, every agent has the same best response under $(\classifier_A,  \classifier_B,q,\nullset)$  as in the optimal simultaneous mechanism $(\classifier_A,  \classifier_B)$.
    We conclude that 
    this uninformed random order mechanism accepts the same set of agent as in the optimal simultaneous mechanism. 
    Hence it implements the optimal simultaneous mechanism. 
\end{proof}


%---------------------------------------







\begin{proof}[Proof of \cref{thm:optimal sequential metric investement}]
    % Since the cost function is additive and translation invariant, the cost function is homogeneous. For any $\features, \firstfeatures, \features', \firstfeatures'$ that satisfy $\firstfeatures' - \features' = \gamma \cdot (\firstfeatures - \features)$ for $\gamma > 0$, we have $c(\firstfeatures', \features') = \gamma \cdot c(\firstfeatures , \features)$.
    
    % Consider any fixed classifier $\classifier_1$.
    % Consider any attributes $\features$ that do not satisfy this classifier $\classifier_1$. Let $\features'$ be the attributes that minimize the cost $c(\features,\features')$ among all attributes satisfying $\classifier_1$. 
    % Then, for any other attributes $\tilde \features \neq \features$, let $\tilde \features' \in \classifier_1$ be the attributes that minimizes the cost $c(\tilde \features, \tilde \features')$. 
    % Since the cost is translation invariant and homogeneous, we have two vectors $\features' - \features$ and $\tilde \features' - \tilde \features$ are parallel. 



    Consider any fixed half plane $\classifier_A$. Suppose there exists a half plane $\classifier_B$ such that in the  mechanism $(\classifier_A,\classifier_B,q,\nullset)$, \cref{condition:one-step} holds. 
    Let $\theta \in (0, 180^{\circ})$ be the angle from $\classifier_A$ to $\classifier_B$. 
     

    \paragraph{Step 1}
    We first show that for any half plane $\tilde \classifier_B$ that has an angle $\tilde \theta > \theta$ from $\classifier_A$,
    \cref{condition:one-step} holds in the  mechanism $(\classifier_A,\tilde\classifier_B,q,\nullset)$.
    
    Since the angle $\tilde \theta > \theta$, we have (1) the set of attributes that satisfies $\tilde \classifier_B$ but not $\classifier_A$, i.e., $\tilde \classifier_B \setminus \classifier_A$  is contained in the region $\classifier_B \setminus \classifier_A$; (2) the set of attributes that satisfies $\classifier_A$ but not $\tilde \classifier_B$, i.e., $ \classifier_A \setminus \tilde\classifier_B$  is contained in the region $\classifier_A \setminus \classifier_B$; and (3) the qualified region $\classifier_A \cap \classifier_B$ is contained in $\classifier_A \cap \tilde \classifier_B$. 
    
    \textbf{Case 1.} Consider any agent with attributes $\features \in \tilde \classifier_B \setminus \classifier_A$.
    First, the least costly one-step strategy is the same in $(\classifier_A,\tilde\classifier_B,q,\nullset)$ and in $(\classifier_A,\classifier_B,q,\nullset)$.
    Second, the least costly two-step strategy to first wait and then change to other attributes to pass $\classifier_A$ is also the same in $(\classifier_A,\tilde\classifier_B,q,\nullset)$ and in $(\classifier_A,\classifier_B,q,\nullset)$.
    These together imply that if such an agent 
    prefers a two-step strategy in the uninformed random order mechanism $(\classifier_A,\tilde\classifier_B,q,\nullset)$, then this agent also prefers the two-step strategy in the uninformed random order mechanism $(\classifier_A,\classifier_B,q,\nullset)$. 
    Hence, this agent with attributes $\features$ will not prefer a two-step strategy in the uninformed random order mechanism $(\classifier_A,\tilde\classifier_B,q,\nullset)$.
    
    \textbf{Case 2.} Consider any agent with attributes in the region $\classifier_A \setminus \tilde \classifier_B$. Since such attributes are also in $\classifier_A \setminus \classifier_B$, we know from the assumption that such attributes do not prefer a two-step strategy.
    
    \textbf{Case 3.} Consider any agent with attributes $\features$ that  satisfy neither $\classifier_A$ nor $\tilde \classifier_B$. 
    Let $(\features,\firstfeatures,\secondfeatures)$ be the least costly two-step strategy that first satisfies the test $\tilde \classifier_B$.
    Then, we have $\firstfeatures$ satisfy only test $\tilde \classifier_B$ and $\secondfeatures$ satisfy only test $\classifier_A$.
    
    Consider an agent with attributes $\firstfeatures$.
    He has one available two-step strategy, which is to wait in the first test and then change to $\secondfeatures$ in the second test.
    Such a two-step strategy yields him an expected payoff $1-q - \onecost(\firstfeatures,\secondfeatures)$.
    Let $(\firstfeatures, \features')$ be the least costly one-step strategy for the agent with attributes $\firstfeatures$. 
    Since $\firstfeatures$ satisfy $\tilde \classifier_B$ but not $\classifier_A$, we have shown that this agent with attributes $\firstfeatures$ prefers a one-step strategy. 
    This implies that $1- q - \onecost(\firstfeatures,\secondfeatures) \leq 1 - \onecost(\firstfeatures, \features')$. 
    Therefore, we have the expected utility of the two-step strategy $(\features, \firstfeatures, \secondfeatures)$ is at most
    $$
    1-q - c(\features, \firstfeatures, \secondfeatures) = 1-q - \onecost(\features, \firstfeatures) - \onecost(\firstfeatures, \secondfeatures) \leq 1- \onecost(\features, \firstfeatures) - \onecost(\firstfeatures,\features') \leq 1 - \onecost(\features,\features'),
    $$
    where the first equality is because the cost is additive and the last inequality is from the triangle inequality of the cost function. 
    Similarly, the agent with attributes $\features$ also does not prefer any two-step strategy that first satisfies $\classifier_A$.
    Thus, the agent with attributes $\features$ prefers some one-step strategy, i.e., \cref{condition:one-step} holds in the mechanism $(\classifier_A,\tilde\classifier_B,q,\nullset)$.

\paragraph{Step 2} 
   Next we show that for any fixed half plane $\classifier_A$, there is an angle $\theta^*(\classifier_A) \in (0,180^{\circ})$ such that for any half plane $\classifier_B$, if the angle between $\classifier_A$ and  $\classifier_B$ is greater than $\theta^*(\classifier_A)$, then for any $q\in [0,1]$, the mechanism $(\classifier_A,\classifier_B,q,\nullset)$, \cref{condition:one-step} holds.
   The proof is constructive.
   We first construct a half plane $\tilde \classifier_B$ (Step 2a).
   Second, we show that
   for the uninformed random order mechanism with tests $\classifier_A$ and $\tilde \classifier_B$, the best one-step strategy is less costly than the best two-step strategy that first satisfies $\tilde \classifier_B$ and then $\classifier_A$ (Step 2b). 
Let $\tilde O$ be the intersection point of the boundaries of $\classifier_A$ and $\tilde \classifier_B$.
Third, we then show that this property  is preserved when we rotate the half plane $\tilde\classifier_B$ around $\tilde O$ to increase the angle between $\classifier_A$ and $\tilde\classifier_B$ (Step 2c). 
Fourth, we show that after the rotation, there exists a half plane $\classifier_B$ that satisfies that the best one-step strategy is less costly than the best two-step strategy that first satisfies $\classifier_A$ (Step 2d). 
   
    \paragraph{Step 2a}
    Consider any attributes $\features$ that do not satisfy $\classifier_A$. Let $\features'$ be the attributes that minimize the cost $\onecost(\features,\features')$ among all attributes satisfying $\classifier_A$.
    Let $v$ be the vector orthogonal to the vector $\features' - \features$, i.e. $v \cdot (\features' - \features) = 0$.
    We pick the direction (of $v$) so that the clockwise angle from the normal vector $\weights_A$ of $\classifier_A$ to $v$ is in $(0, 180^{\circ})$ (See \cref{fig: general cost investment optimal sequential case 1}).\footnote{In this way, the angle between $\classifier_A$ and 
 any half plane $\tilde \classifier_B$ whose normal vector is $v$ is between $(0,180^{\circ})$.} 
 Consider any half plane $\tilde \classifier_B$ whose normal vector is $v$. %\xqcomment{where is the intersection point? which direction of v?}. 
%Then, for the uninformed random order mechanism with tests $\classifier_A$ and $\tilde \classifier_B$, the best one-step strategy is less costly than  the best two-step strategy that first satisfies $\tilde \classifier_B$.

%We only need to consider any attributes $\features$ that satisfy neither $\classifier_A$ nor $\tilde \classifier_B$, because ....

\begin{figure}[t]
\centering
\begin{tikzpicture}[xscale=6,yscale=6]

\draw [domain=-0.4:0.4, thick] plot (\x, {3/4*\x});
\node [above] at (0.4,0.3 ) {$\tilde\classifier_B$};
\draw [thick] (-0.4,0) -- (0.4,0);
\node [right] at (0.4,0 ) {$\classifier_A$};
\node [above] at (0.3,0 ) {$+$};
\node [right] at (0.3,0.22) {$+$};

 \draw [<-]  (0.34,0.3) to [bend left=45]  (0.4,0.3);

\node [above] at (0,0 ) {\footnotesize$\tilde O$};

\node at (-0.2,-0.08 ) {\textbullet};
\node [left] at (-0.2,-0.08 ) {\footnotesize$\features$};

\node at (-0.28/3,0 ) {\textbullet};
\node [above] at (-0.28/3,0 ) {\footnotesize$\features'$};


\node at (0.04,-0.120 ) {\textbullet};
\node [right] at (0.04,-0.12 ) {\footnotesize$\firstfeatures$};

\node at (0.2,0 ) {\textbullet};
\node [below] at (0.22,0 ) {\footnotesize$\secondfeatures$};

\draw [ ->] (-0.2,-0.08 )  -- (-0.28/3,0 );

\draw [ ->, red] (-0.28/3,0 )-- (0.18/4-0.28/3,-0.06);
\node [right, red] at (0.18/4-0.28/3,-0.06) {\footnotesize$v$};

\draw [ ->, blue] (-0.2,-0.08 )  -- (0.04,-0.120 );
\draw [ ->, blue] (0.04,-0.120 )  -- (0.2,0);



\end{tikzpicture}
\caption{Step 2a in \cref{thm:optimal sequential metric investement}}
\label{fig: general cost investment optimal sequential case 1}
% \rule{0in}{1.2em}$^\dag$\scriptsize In this graph, 
\end{figure}


\paragraph{Step 2b} We show that the best two-step strategy that first satisfies $\tilde\classifier_B$ and then $\classifier_A$ is more costly than some one-step strategy.
Suppose the best two-step strategy that first satisfies $\tilde\classifier_B$ and then $\classifier_A$ is $\strategies=(\firstfeatures,\secondfeatures)$.
Then the vector $\secondfeatures-\firstfeatures$ is parallel to the vector $\features'-\features$ by absolute homogeneity.
Since $\firstfeatures$ satisfy $\tilde\classifier_B$ and the normal vector of $\tilde\classifier_B$ is orthogonal to $\features'-\features$, the attributes $\secondfeatures$ also satisfy $\tilde\classifier_B$. 
Thus, attributes $\secondfeatures$ satisfy both $\classifier_A$ and $\tilde \classifier_B$.
By triangle inequality, $c(\features,\firstfeatures,\secondfeatures)\leq \onecost(\features,\secondfeatures)$.
Hence for any $q$, the agent receives lower expected utility from the two-step strategy $(\features,\firstfeatures,\secondfeatures)$ than the utility from one-step strategy $(\features,\features_2)$.



\paragraph{Step 2c}
     We fix the intersection point $\tilde O$ of the boundaries of $\classifier_A$ and $\tilde \classifier_B$ and rotate $\tilde \classifier_B$ to increase the angle between $\classifier_A$ and $\tilde \classifier_B$.
     We call the half plane $\hat \classifier_B$ after rotation.
    For the same attributes $\features$, let $\hat\strategies=(\features_1,\features_2)$ be the best two-step strategy that first satisfies $\hat \classifier_B$ after the rotation.
    % Let $\hat\weights_B$ be the normal vector of $\hat\classifier_B$ after the rotation. 
    Notice that after the rotation, $\hat\classifier_B \setminus \classifier_A$ is contained in $\tilde \classifier_B \setminus \classifier_A$. 
    Hence the two-step strategy $\hat\strategies=(\features_1,\features_2)$ is also available to the agent before the rotation.
    By revealed preference, we can infer that the total cost of the best two-step strategy $\hat\strategies=(\features_1,\features_2)$ under test $\classifier_A$ and $\hat\classifier_B$ is weakly higher than the two-step strategy $\strategies=(\firstfeatures,\secondfeatures)$ that the agent uses under test $\classifier_A$ and $\tilde\classifier_B$.
    % attributes $\features$ is further away from $\tilde\classifier_B$. Hence $\onecost(\features,\features_1)$ weakly increases while by absolute homogeneity, $\features_2 - \features_1$ is still in parallel to $\secondfeatures - \firstfeatures$. Thus, we have $\features_2$ satisfies both $\classifier_A$ and $\tilde\classifier_B$. }
    % Then, we have the angle between $\weights_B$ and $\features_2 - \features_1$ is between $(-90^{\circ},90^{\circ})$.\xqcomment{seems incorrect}
    % Thus, we have $\features_2$ satisfies both $\classifier_A$ and $\tilde\classifier_B$.
    Hence, the best one-step strategy still dominates the best two-step strategy that first satisfies $\hat \classifier_B$ as the angle between $\classifier_A$ and $\hat \classifier_B$ increases.
    
\paragraph{Step 2d}
Again, consider the half plane $\tilde \classifier_B$  and $\hat \classifier_B$, where the latter is obtained by  rotating $\tilde\classifier_B$ to increase its angle to $\classifier_A$.
Denote the angle between $\tilde\classifier_B$ and $\classifier_A$ by $\tilde \theta$ and the angle between $\hat\classifier_B$ and $\classifier_A$ by $\hat \theta$.
We have $\hat\theta >\tilde \theta$.
% there is a half plane $\classifier_B$ such that the best two-step strategy that first satisfies $\classifier_A$ is no better than the one-step strategy.
    Consider any attributes $ \features_1 \in \classifier_A \setminus \tilde \classifier_B$. 
    Consider any attributes $\tilde \features_2$ that satisfies only $\tilde \classifier_B$ but not $\classifier_A$. Let $\tilde u = \tilde\features_2 - \features_1$ be the direction moving to $\tilde\features_2$.

    As we have argued in step 1, Since the angle $\hat \theta >\tilde \theta$, we have (1) the set of attributes that satisfy $\hat \classifier_B$ but not $\classifier_A$, i.e., $\hat \classifier_B \setminus \classifier_A$  is contained in the region $\tilde \classifier_B \setminus \classifier_A$; (2) the set of attributes that satisfy $\classifier_A$ but not $\hat \classifier_B$, i.e., $ \classifier_A \setminus \hat\classifier_B$  is contained in the region $\classifier_A \setminus \tilde\classifier_B$; and (3) the qualified region $\classifier_A \cap \tilde \classifier_B$ is contained in $\classifier_A \cap \hat \classifier_B$.
    
    Hence we can find  attributes $\hat \features_2$ that satisfies only $\hat \classifier_B$ but not $\classifier_A$ such that $\hat\features_2 =\features_1 +\lambda \tilde u$ for some $\lambda > 1$, i.e., the direction $\hat \features_2 -\features_1$ is in parallel to $\tilde u = \tilde\features_2 - \features_1$.

    Since the cost function is absolute homogeneous, 
    % As the angle between $\classifier_A$ and $\tilde \classifier_B$ increases, 
    % % the cost from $\features_1$ to its closest attributes $\tilde \features_2$
    %  the Euclidean distance from $\features_1$ to its closest attributes $\tilde \features_2$ in $\tilde \classifier_B \setminus \classifier_A$ on the direction $u$ only increases. 
    % Since the cost is homogeneous, 
    the cost to reach $\hat \classifier_B \setminus \classifier_A$ from $\features_1$ only increases.
    % is greater than the cost to reach $\tilde \classifier_B \setminus \classifier_A$ from $\features_1$ . 
    Similarly, the cost to reach $\hat \classifier_B \cap \classifier_A$ from $\features_1$ only decreases. 
    Therefore, as the rotation angle increases, there exists a half plane $\classifier_B$ with an angle $\theta^*(\classifier_A)$ from $\classifier_A$ such that for any attribute $\features_1 \in \classifier_A \setminus \classifier_B$, the attributes $\features_2$ in $\classifier_B$ that minimizes the cost $\onecost(\features_1,\features_2)$ also satisfies $\classifier_A$.
    Consider any attributes $\features \in \classifier_A^\compl \cap \classifier_B^\compl$. Let $(\features,\features_1,\features_2)$ be the best two-step strategy that first satisfies $\classifier_A$. Since $\features_1$ satisfies only $\classifier_A$, we have $\features_2$ satisfies both $\classifier_A$ and $\classifier_B$. By the triangle inequality of the cost function, we have $\cost(\features,\features_1,\features_2) \leq \onecost(\features,\features_2)$.
    
    Thus, there exists an angle $\theta^*(\classifier_A) \in (0, 180^{\circ})$ such that when the angle between $\classifier_A$ and $\tilde \classifier_B$ is $\theta^*(\classifier_A)$, no agent prefers the two-step strategy.  

    Finally, by \cref{lem: optimal max qualified improving effort}, when no agent prefers the two-step strategy in the uninformed random order mechanism with tests $\classifier_A$ and $\classifier_B$, this uninformed random order mechanism is the optimal sequential mechanism. This random order mechanism achieves the first best.
\end{proof}








%------------------------------------------
\begin{proof}[Proof of \cref{prop:investment fixed order better than informed}]
    Suppose there exists a feasible informed random order mechanism. Let $\tilde \classifier_A$ and $\tilde \classifier_B$ be the two tests used in this mechanism. 
    Denote the intersecting points of the two boundary lines of  $\tilde \classifier_A$ and $\tilde \classifier_B$ by $\tilde O$.
    
    We first show that the fixed-order mechanism $(\tilde \classifier_A,\tilde \classifier_B,1)$ is feasible. The proof for the feasibility of another fixed-order mechanism $(\tilde \classifier_A,\tilde \classifier_B,0)$ is analogous.

    Since the informed random order mechanism with two tests $\tilde \classifier_A$ and $\tilde \classifier_B$ is feasible, the region $\tilde \classifier_A \cap \tilde \classifier_B$ is contained in $\classifier_A \cap \classifier_B$. 
    Otherwise, the agent with attributes in $\tilde \classifier_A \cap \tilde \classifier_B \setminus \classifier_A \cap \classifier_B$ is accepted without improvement. In the fixed order mechanism, the agent becomes qualified if this agent eventually moves to $ \classifier_A \cap  \classifier_B$. 
    
    Suppose an agent takes a one-step strategy in the fixed order mechanism.
    For this strategy to be a best response, the agent must be accepted.
    Hence his final attributes must fall in $\tilde \classifier_A \cap \tilde \classifier_B$.
    This implies that such an agent eventually becomes qualified.
    It remains to show that any agent who takes a two-step strategy also becomes qualified.

    
    Suppose an agent with attributes $\features$ takes a two-step strategy $\strategies(\features)=( \firstfeatures, \secondfeatures)$ in the fixed order mechanism.
    We will show that the final attributes $\secondfeatures$ satisfies $\classifier_A$ and $\classifier_B$. 
 
    \begin{claim}\label{claim: final attributes}
        The final attributes $\secondfeatures$ satisfy $\tilde \classifier_B$ but not $\tilde \classifier_A$. 
    \end{claim}

    \begin{proof}[Proof of \cref{claim: final attributes}]
        Since this agent takes a two-step strategy,
    for this strategy to be a best response, it must be that the agent is accepted by the mechanism.
    Hence we know that $\firstfeatures\in\tilde \classifier_A$ and $\secondfeatures\in \tilde\classifier_B$.
    Suppose $\secondfeatures$ satisfy $\tilde\classifier_A$.
    Then  by the triangle inequality of the cost function, this agent has a lower cost for directly moving to $\secondfeatures$.
    Note that the final attributes $\secondfeatures$ are the attributes that minimize the cost $\onecost(\firstfeatures,\secondfeatures)$ among all attributes satisfying the classifier $\tilde\classifier_B$, and the cost $\onecost(\firstfeatures,\secondfeatures) \leq 1$. 
    % Since the cost function is additive and translation invariant, the cost is also homogeneous. This means for any $\features, \firstfeatures, \features', \firstfeatures'$ that satisfy $\firstfeatures' - \features' = \gamma \cdot (\firstfeatures - \features)$ for $\gamma > 0$, we have $c(\firstfeatures', \features') = \gamma \cdot c(\firstfeatures , \features)$. \xqcomment{This seems to say we need the metric induced by a norm.} 
    Note that the attributes $\firstfeatures$ satisfy the classifier $\tilde \classifier_A$ but do not satisfy $\tilde \classifier_B$. 
    We show that the attributes $\firstfeatures$ are on the boundary of the classifier $\tilde\classifier_A$. 
    Suppose the attributes $\firstfeatures$ are not on the boundary of the classifier $\tilde\classifier_A$. 
    Then, the line segment between $\firstfeatures$ and $\secondfeatures$ intersects the boundary of $\tilde \classifier_A$ at some attributes $\features'$. 
    Since the cost is additive and induced by the metric $d$, we have
    \begin{align*}
        c(\features,\firstfeatures,\secondfeatures) &= \onecost(\features,\firstfeatures) + \onecost(\firstfeatures,\secondfeatures) = \onecost(\features,\firstfeatures) + \onecost(\firstfeatures, \features') + \onecost(\features', \secondfeatures) \\
        &\geq \onecost(\features,\features') + \onecost(\features',\secondfeatures) = c(\features,\features',\secondfeatures).
    \end{align*}
    Thus, the agent will prefer a two-step strategy $(\features,\features',\secondfeatures)$, which gives a contradiction.
    % Since the cost is additive\xqcomment{homogenous instead of additive?}, the attributes $\firstfeatures$ are on the boundary of the classifier $\tilde \classifier_1$. 
    \end{proof}
    
    
    \begin{claim}\label{claim: pair of attributes}
        There exists a pair of attributes $\tilde \features$ and $\tilde \features'$ that satisfies the following three properties: 
        \begin{enumerate}
            \item the attributes $\tilde \features$ lie on the boundary of the classifier $\tilde \classifier_A$;
    \item the attributes $\tilde \features'$ are the attributes that minimize the cost $\onecost(\tilde \features, \tilde \features')$ among all attributes satisfying the classifier $\tilde \classifier_B$; 
    \item the cost from $\tilde \feature$ to $\tilde \features'$ is $\onecost(\tilde\features, \tilde \features') = 1$.
        \end{enumerate}
    \end{claim}

\begin{proof}[Proof of \cref{claim: pair of attributes}]
     Consider any attributes $\hat \features$ on the boundary of the classifier $\tilde \classifier_A$ that do not satisfy the classifier $\tilde \classifier_B$. Let the attributes $\hat \features'$ be the attributes that minimize the cost $\onecost(\hat \features, \hat \features')$ among all attributes satisfying the classifier $\tilde \classifier_B$.
    Since the cost is translation invariant and homogeneous, we have $\hat \features' - \hat \features$ is parallel to $\secondfeatures - \firstfeatures$. 
    Let $\weights_B$ be the weight vector of the classifier $\tilde \classifier_B$.
    As the attributes $\hat \features$ move away from the classifier $\tilde \classifier_B$, i.e. $\weights_B \cdot \hat\features$ decreases, the norm of the vector $\hat \features' - \hat\features$ increases. 
    Thus, there exists a pair of such attributes $\tilde \features$ and $\tilde \features'$ that satisfies $\tilde \features' - \tilde \features = (\secondfeatures - \firstfeatures)/ \onecost(\firstfeatures,\secondfeatures)$, which implies $\onecost(\tilde\features, \tilde \features') = 1$.
\end{proof}
   
    \begin{claim}\label{claim: qualified attributes}
         $\tilde \features'$ is qualified, i.e., $\tilde \features'\in\classifier_A \cap \classifier_B$.
    \end{claim}
    \begin{proof}[Proof of \cref{claim: qualified attributes}]
         Consider the agent with attributes $\tilde \features$. Under the informed random order mechanism, this agent prefers the two-step strategy because the cost for directly improving to $\tilde \classifier_A \cap \tilde \classifier_B$ is greater than one. 
    The best strategy for this agent is to first provide the original attributes $\tilde \features$, and if he passes the first test, improve the attributes to $\tilde\features'$ in the second test.
    If the informed random order mechanism uses the classifier $\tilde \classifier_A$ first and then uses the classifier $\tilde \classifier_B$ with probability $q$, then this agent is accepted with probability $q$, and the expected utility is zero. 
    Since the informed random order mechanism is feasible, the final attributes $\tilde \features'$ is in $\classifier_A \cap \classifier_B$. 
    \end{proof}
   
    
    % According to the construction, the attributes $\secondfeatures$  are closer to $\tilde \classifier_A$ than the attributes $\tilde \features'$, which means $\secondfeatures \cdot \tilde \weights_A \leq \tilde \features' \cdot \tilde \weights_A$ where $\tilde \weights_A$ is the normal vector of the test $\tilde \classifier_A$. Therefore, the attributes $\secondfeatures$ are also contained in $\classifier_A \cap \classifier_B$.\xqcomment{suggested change: 
    According to the construction, the attributes $\secondfeatures$ lie on the line segment connecting $\tilde \features'$ and $\tilde O$.
    Notice that both $\tilde \features'$ and $\tilde O$ are qualified, and the qualified region is convex.
    Hence $\secondfeatures$ are also qualified.  
    
     To summarize, every agent accepted by the fixed order mechanism is qualified.
     Hence the fixed order mechanism is  feasible. 

    Since the two fixed-order mechanisms $(\tilde \classifier_A,\tilde \classifier_B,1)$ and $(\tilde \classifier_A,\tilde \classifier_B,0)$ are feasible, we can apply the same argument as in \cref{lem:feasible-informed-rand-distance cost} by constructing a mixed mechanism to show that one of these two fixed-order mechanism is no worse than the informed random order mechanism. 
\end{proof}