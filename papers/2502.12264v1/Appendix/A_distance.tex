\section{Omitted proof in \cref{sec: distance cost} (manipulation)}\label{appendix: distance cost}

\input{Appendix/A_characterization}

\subsection{Proof of main results in \cref{subsec:seq manipulation}}

%--------------------informed------------------

\begin{proof}[Proof of \cref{lem:gain non-parallel tests}]
   From \cref{sec: characterization BR} we know that the set of attributes selected is $\manipulation_1=\setperp_A \cup \setperp_B \cup\setonetwo_1 \cup \setO $.
   Apply \cref{lmm: Bq invariant with q} and \cref{lem: Cq}. 
\end{proof}

\begin{proof}[Proof of \cref{lem:loss non-parallel tests}]
    The proof uses similar argument as the previous one and hence is omitted.
\end{proof}

\begin{proof}[Proof of \cref{lem:feasible-informed-rand-distance cost}]
First, we  show that the fixed order mechanism $(\tilde\classifier_A,\tilde\classifier_B,1)$ is feasible.
That is, we want to show that for $\features$ that are accepted by $(\tilde\classifier_A,\tilde\classifier_B,1)$, they are also qualified.
We partition the set of attributes that are accepted by $(\tilde\classifier_A,\tilde\classifier_B,1)$ into two subsets $F_1$ and $F_2$. 
We show that for any attributes in the first subset $F_1$ are also accepted by the informed random order mechanism (Step 1).
Since the informed random order mechanism is feasible, we know that any attributes in the first subset $F_1$ are qualified.
As for the second subset $F_2$, we show that it is contained in a convex set, whose extreme points are in $F_1$ and hence are qualified(Step 2).
Since the qualified region is also convex, we can infer that the second subset $F_2$ is contained in the qualified region.
Hence any attributes in the second subset are also qualified.
The feasibility of $(\tilde\classifier_A,\tilde\classifier_B,0)$ can be shown analogously.
Lastly, we show that one of the two fixed order mechanisms is no worse than the informed random-order mechanism (Step 3).


\paragraph{Step 1} We first show that the set of attributes that (1) are accepted by informed random order mechanism  $(\tilde\classifier_A,\tilde\classifier_B,q,\test_1)$ and (2) satisfy either $\tilde\classifier_A$ or $\tilde\classifier_B$, contains the set of attributes that (1) are accepted by the fixed order mechanism  $(\tilde\classifier_A,\tilde\classifier_B,1)$ and (2) satisfy either $\tilde\classifier_A$ or $\tilde\classifier_B$.
We call the former set set $I$, the latter set set $F_1$.

Consider any $\features$ that are in set $F_1$, i.e.,  any $\features$ that (1) are accepted by the fixed order mechanism  $(\tilde\classifier_A,\tilde\classifier_B,1)$ and (2) satisfy either $\tilde\classifier_A$ or $\tilde\classifier_B$.
We distinguish three cases.

\textbf{Case 1:} Consider $\features$ that satisfy both $\tilde\classifier_A$ and $\tilde\classifier_B$.
Under the strategy $\strategies=(\features,\features)$, 
such attributes are accepted by both the informed random order mechanism and the fixed order mechanism.

\textbf{Case 2:} Consider $\features$ that satisfy $\tilde\classifier_A$ but not $\tilde\classifier_B$.
We can infer that either (1) the best response is a one-step strategy, i.e.,  $\firstfeatures=\secondfeatures$ that satisfy both $\tilde\classifier_A$ and $\tilde\classifier_B$, or (2) the best response  is a two-step strategy, i.e., $\firstfeatures$  satisfy $\tilde\classifier_A$ but not $\tilde\classifier_B$, and $\secondfeatures$ satisfy $\tilde\classifier_B$ but not $\tilde\classifier_A$.
If the former case is true, then such attributes are also accepted by the informed random order mechanism and hence are also in set $I$.
If the latter case is true, then we must have $\firstfeatures=\features$ because of triangle inequality.
Moreover, we can infer that such a strategy is profitable, i.e., $1-c(\features,\features,\secondfeatures)\geq 0$.
This implies that when such attributes use the same strategy in the informed random order mechanism, the expected utility is $q[1-c(\features,\features,\secondfeatures)]\geq 0$ and they get accepted by the informed random order mechanism with probability at least $q$.
Therefore, such attributes are also in set  $I$.

\textbf{Case 3:} Consider $\features$ that satisfy $\tilde\classifier_B$ but not $\tilde\classifier_A$.
Similarly, we can infer that either (1) it is a one-step strategy, i.e.,  $\firstfeatures=\secondfeatures$ that satisfy both $\tilde\classifier_A$ and $\tilde\classifier_B$, or (2) it is a two-step strategy, i.e., $\firstfeatures$  satisfy $\tilde\classifier_A$ but not $\tilde\classifier_B$, and $\secondfeatures$ satisfy $\tilde\classifier_B$ but not $\tilde\classifier_A$.
If the former case is true, then such attributes are also accepted by the informed random order mechanism and hence are also in set  $I$.
If the latter case is true, then we can infer that such a strategy is profitable, i.e., $1-c(\features,\firstfeatures,\secondfeatures)\geq 0$.
This implies that such attributes can use a strategy $(\features,\firstfeatures,\firstfeatures)$ in the informed random order mechanism, the expected utility is $(1-q)[1-\onecost(\features,\firstfeatures)]\geq 0$, because $\onecost(\features,\firstfeatures)\leq c(\features,\firstfeatures,\secondfeatures)$ by monotonicity.
Hence such attributes are  accepted by the informed random order mechanism with probability at least $1-q$.
Therefore, such attributes are also in set  $I$.



\paragraph{Step 2} By the first characterization, we know that $\setonetwo_1\cup \setO$ contains the set of attributes that (1) are accepted by the fixed order mechanism  $(\tilde\classifier_A,\tilde\classifier_B,1)$, and (2) satisfy neither $\tilde\classifier_A$ nor $\tilde\classifier_B$.
Call the latter set set $F_2$.

Notice that $\setonetwo_1\cup \setO$ is convex and the qualified region is convex.
To show that $F_2$ is contained in the qualified region, 
it suffices to show that the extreme points of $\setonetwo_1\cup \setO$ are qualified.
This is true because the extreme points of $\setonetwo_1\cup \setO$ are in set $F_1$.
To see this, denote the intersecting point of the boundary lines of $ \tilde\classifier_A$ and $\tilde\classifier_B$ by $\tilde O$.
The extreme points of $\setonetwo_1\cup \setO$ are $\tilde O$, $\tilde\pointonetwo_1$ and another point in $ \tilde\classifier_B$.

\paragraph{Step 3}
 we introduce a mixed mechanism that mixes two fixed-order mechanisms as follows:
     announce the fixed-order mechanism $(\tilde\classifier_A,\tilde\classifier_B,1)$ with probability $q$; and announce the other fixed-order mechanism $(\tilde\classifier_A,\tilde\classifier_B,0)$ with probability $1-q$. 
     Such a mixed mechanism only accepts qualified agent.
    
    We first show that this mixed fixed-order mechanism is better than the random-order mechanism.  
    More specifically, this mixed fixed-order mechanism accepts all attributes that are accepted by the random-order mechanism with weakly higher probability.
    Suppose the agent is accepted with probability one in the random-order mechanism. Then, this agent must use a one-step strategy and provide attributes that satisfy both $\tilde \classifier_A$ and $\tilde \classifier_B$ in the first test. In this case, the agent can always get accepted in both fixed-order mechanisms by adopting the same strategy. Thus, this agent is accepted with probability one in the mixed mechanism. 
    
    Suppose the agent gets accepted with probability $q$ in the random-order mechanism. This means the agent chooses a two-step strategy that first provides attributes $\firstfeatures$ that satisfy only test $\tilde \classifier_A$ and then provides $\secondfeatures$ that satisfy another test $\tilde \classifier_B$. 
    The expected utility of this agent is $q- d(\features,\firstfeatures)- q\times d(\firstfeatures,\secondfeatures) \geq 0$.  
    Since the cost is non-negative, we have $1- d(\features,\firstfeatures)-  d(\firstfeatures,\secondfeatures) \geq 0$.
    In the fixed-order mechanism $(\tilde\classifier_A,\tilde\classifier_B,1)$, this agent can adopt the same strategy to get utility $1- d(\features,\firstfeatures)-  d(\firstfeatures,\secondfeatures) \geq 0$.
    Thus, this agent gets accepted with probability at least $q$ in the mixed mechanism. 
    Similarly, we can show that for any agent who is accepted by the random-order mechanism with probability $1-q$ is also accepted with probability at least $1-q$ in the mixed mechanism.
    Therefore, the mixed fixed-order mechanism is no worse than the random-order mechanism. 
    
    Finally, it is easy to see that one (the better one) of the two fixed-order mechanisms is no worse than the mixed mechanism.
    

%     Denote the angle between $ \tilde\classifier_A$ and $\classifier_B$ by $\theta( \tilde\classifier_A,\classifier_B)$.
%     Denote the angle between $ \classifier_A$ and $\tilde\classifier_B$ by $\theta( \classifier_A,\tilde\classifier_B)$.

% \paragraph{Step 1:} Any feasible simultaneous mechanism $(\tilde \classifier_A,\tilde \classifier_B)$ must satisfy $\theta( \tilde\classifier_A,\classifier_B)\leq\theta$ and $\theta( \classifier_A,\tilde\classifier_B)\leq \theta$.

% Suppose $\theta( \tilde\classifier_A,\classifier_B)>\theta$. 
% This implies that the boundary lines of $\tilde \classifier_A$ and $ \classifier_A$ intersect at some point that is in $ \classifier_A\cap \classifier_B$.
% This further implies that $\tilde \classifier_A\cap \tilde \classifier_B$ includes some attributes that are outside $ \classifier_A\cap \classifier_B$.
% The informed random order mechanism  $(\tilde\classifier_A,\tilde\classifier_B,q,\test_1)$ cannot be feasible. A contradiction.
% Analogously, we can show that $\theta( \classifier_A,\tilde\classifier_B)>\theta$ is not possible.

% \paragraph{Step 2:}


\end{proof}

%---------------------------------------------
%------------proofs of lemmas for thm 1-------
\begin{proof}[Proof of \cref{lem:feasible-uninformed-rand-distance cost}]

We first show that two fixed-order mechanisms $(\tilde\classifier_A,\classifier_B',1)$ and $(\classifier_A',\tilde\classifier_B,0)$ are feasible.
Consider the fixed-order mechanism $(\tilde\classifier_A,\classifier_B',1)$.
By the characterization in Section~\ref{sec: characterization BR}, the manipulation set for this mechanism consists of sets $\setO$, $\setonetwo_1(\tilde\classifier_A,\classifier_B')$, $\setperp_A(\tilde\classifier_A,\classifier_B')$, and $\setperp_B(\tilde\classifier_A,\classifier_B')$.
Since the uninformed random-order mechanism $(\tilde\classifier_A,\tilde\classifier_B,q,\test_1)$ is feasible, we have the intersection point $O$ has a distance at least $1/\eta$ to the boundary of both classifiers $\tilde\classifier_A$ and $\tilde\classifier_B$, otherwise there exists unqualified agent can directly move to point $O$ with cost $1$.
Note that the classifier $\classifier_B'$ has the same normal vector $\weights_B$ as the classifier $\classifier_B$. 
Thus, any attributes in $\tilde \classifier_A \cap \classifier_B'$ have a distance at least $1/\eta$ to the boundary of $\classifier_A$ and $\classifier_B$.
Therefore, we have $\setperp_B(\tilde \classifier_A, \classifier_B', 1)$ and $\setO$ are contained in $\classifier_A \cap \classifier_B$.
By Lemma~\ref{lem: Cq}, we have $\setonetwo_1(\tilde \classifier_A, \classifier_B') = OE_1\tilde E_1 E_1'$.
Note that the boundary of $\classifier_B'$ is parallel to the boundary of $\classifier_B$.
Thus, we have attributes $E_1$ and $E_1'$ are
contained in $\classifier_A \cap \classifier_B$.
The attributes $\tilde E_1$ is the intersection of $\tilde\classifier_A$ and $\classifier_B$, which implies the attributes $\tilde E_1$ is also in $\classifier_A \cap \classifier_B$.
Since the qualified region is convex, we have the set $\setonetwo_1 \subset \classifier_A \cap \classifier_B$.
Thus, the fixed-order mechanism $(\tilde\classifier_A,\classifier_B',1)$ is feasible. 
With a similar analysis, we have $(\classifier_A', \tilde\classifier_B, 0)$ is also feasible. 

We next show that one of these two fixed-order mechanisms is no worse than the uninformed random-order mechanism. 
Suppose the boundaries of two tests $\tilde \classifier_A$ and $\tilde \classifier_B$ are parallel to the boundaries of $\classifier_A$ and $\classifier_B$ respectively. 
By Proposition~\ref{prop: coverage of Mq}, in this case, the manipulation set of the uninformed random-order mechanism $(\tilde\classifier_A,\tilde\classifier_B,q,\varnothing)$ is contained in the manipulation set of one fixed-order mechanism.

We now consider the case where the boundaries of $\tilde\classifier_A$ and $\tilde\classifier_B$ are not parallel to the boundaries of $\classifier_A$ and $\classifier_B$.
We show that a mixed mechanism that announces the fixed-order mechanism  $(\tilde\classifier_A,\classifier_B',1)$ with probability $q$ and the fixed-order mechanism $(\classifier_A', \tilde\classifier_B, 0)$ with probability $1-q$ is no worse than the uninformed random order mechanism $(\tilde\classifier_A,\tilde\classifier_B,q,\varnothing)$.
By Lemma~\ref{lem: Cq}, we have $\setonetwo_q(\tilde \classifier_A, \tilde \classifier_B) = OE_q \tilde E_q E_q'$. Since $\tilde E_q$ is on the boundary of $\tilde\classifier_A$, we must have $\tilde E_q$ is in $O \tilde E_1$, otherwise $\tilde E_q$ is not qualified. 
We also know that $E_q$ and $E_q'$ have a distance $q$ to the intersection $O$, which means $E_q$ and $E_q'$ are accepted by the fixed-order mechanism. 
Note that the set of attributes $\calA_1$ accepted by the fixed-order mechanism $(\tilde\classifier_A, \classifier_B', 1)$ is convex. 
Since attributes $O$, $E_q$, $\tilde E_q$, and $E_q'$ are contained in $\calA_1$, we have $\setonetwo_q(\tilde \classifier_A, \tilde \classifier_B) = OE_q\tilde E_qE_q'$ is contained in $\calA_1$.
Similarly, we have the set $\settwoone_q$ is contained in the set of attributes $\calA_2$ accepted by the fixed-order mechanism $(\classifier_A',\tilde \classifier_B,0)$.
If an agent is accepted by the random order mechanism $(\tilde\classifier_A, \tilde \classifier_B,q,\varnothing)$ with probability $1$, then the attributes of this agent must be in $\setO \cup (\tilde\classifier_A \cap \tilde \classifier_B) \cup \setperp_A(\tilde\classifier_A, \tilde \classifier_B) \cup \setperp_B(\tilde\classifier_A, \tilde \classifier_B)$. 
Note that $\setO$, $\tilde\classifier_A \cap \tilde \classifier_B$, $\setperp_A(\tilde\classifier_A, \tilde \classifier_B)$, and $\setperp_B(\tilde\classifier_A, \tilde \classifier_B)$ are all contained by both $\calA_1$ and $\calA_2$. 
Thus, these agents are also accepted with probability $1$ in the mixed fixed-order mechanism. 
If an agent is accepted by the random order mechanism $(\tilde\classifier_A, \tilde \classifier_B,q,t_1)$ with probability $q$, then the attributes of this agent must be in $\setonetwo_q(\tilde\classifier_A,\tilde\classifier_B)$, which is contained in $\calA_1$.
Thus, this agent is accepted with probability at least $q$ in the mixed mechanism.
Similarly, if the agent is accepted by the random order mechanism $(\tilde\classifier_A, \tilde \classifier_B,q,\varnothing)$ with probability $1-q$, then the attributes of this agent must be in $\settwoone_q(\tilde\classifier_A,\tilde\classifier_B)\subset \calA_2$.
Thus, this agent is accepted by the mixed mechanism with probability at least $1-q$.
Since one of the two fixed-order mechanisms is no worse than the mixed mechanism, we get the conclusion.
\end{proof}


We now prove our main theorem. 

\begin{proof}[Proof of Theorem~\ref{thm: optimal max qualified}]
    By \cref{lem:feasible-informed-rand-distance cost} and \cref{lem:feasible-uninformed-rand-distance cost},  we have the best sequential mechanism is a fixed order mechanism. Moreover, to avoid selecting any unqualified agent, we must have $\tilde \classifier_A \cap \tilde \classifier_B \subset \classifier_A \cap \classifier_B$.
\end{proof}

\subsection{Omitted proof in \cref{subsec: simultaneous manipulation}}
\begin{proof}[Proof of \cref{lem:fix-simultaneous-distance cost}]
      This result is implied by \cref{thm:opt_manipulation}.
\end{proof}

\subsection{Omitted proof in \cref{subsec:cheap talk manipulation}}\label{appendix:cheap talk manipulation}

First we state the following two lemmas (partially), which characterize the set of attributes that can get selected by these two fixed-order mechanisms.


\begin{lemma}\label{lem:gain non-parallel tests}
 Under the fixed-order procedure $(\classifier_A^+,\widehat\classifier_B,0)$, we have the following:
 \begin{itemize}
     \item Each type in the triangle $\Updelta\text{OAB}$ has a profitable strategy to get selected.
     \item No unqualified type has a strictly profitable strategy to get selected.
 \end{itemize}
\end{lemma}

\begin{lemma}\label{lem:loss non-parallel tests}
 Under the fixed-order procedure $(\classifier_A^+,\classifier_B^+,1)$, we have the following:
 \begin{itemize}
     \item Each type in $\classifier_A\cap\classifier_B\setminus \Updelta\text{OAB}$ has a profitable strategy to get selected.
     \item Each type in the triangle $\Updelta\text{OAB}$ does not have a profitable strategy to get selected.
     \item No unqualified type has a strictly profitable strategy to get selected.
 \end{itemize}
\end{lemma}

\begin{proof}[Proof of \cref{thm: optimal max qualified cheap talk}]
The fixed-order mechanism is described as the following: 
If the agent reports a type in the triangle $\Updelta \text{OAB}\subset \classifier_A\cap\classifier_B$, then the principal announces the fixed-order mechanism $(\classifier_A^+,\widehat\classifier_B,0)$.
 If the agent reports a type in $ \classifier_A\cap\classifier_B\setminus \Updelta \text{OAB}$, then the principal announces the fixed-order mechanism $(\classifier_A^+,\classifier_B^+,1)$. 
 If the agent reports a type that is not in $ \classifier_A\cap\classifier_B$, then the principal randomly announces either $(\classifier_A^+,\widehat\classifier_B,0)$ or $(\classifier_A^+,\classifier_B^+,1)$.

First, we show that this fixed-order mechanism is feasible. 
By \cref{lem:gain non-parallel tests} and \cref{lem:loss non-parallel tests}, no unqualified type has a profitable strategy to get selected under either $(\classifier_A^+,\widehat\classifier_B,0)$ or $(\classifier_A^+,\classifier_B^+,1)$.
Hence no unqualified type has a profitable strategy to get selected regardless of the reporting strategy.

Next we show that under this fixed-order mechanism, every qualified agent is selected.
By \cref{lem:gain non-parallel tests} and \cref{lem:loss non-parallel tests}, each type in the triangle $\Updelta \text{OAB}$ has a profitable strategy to get selected by the fixed-order mechanism $(\classifier_A^+,\widehat\classifier_B,0)$ but not in the fixed-order mechanism $(\classifier_A^+,\classifier_B^+,1)$.
Hence each type in the triangle $\Updelta \text{OAB}$ has incentive to truthfully report his type and gets selected. Again, by \cref{lem:loss non-parallel tests}, each type in $\classifier_A\cap\classifier_B\setminus \Updelta\text{OAB}$ has a profitable strategy to get selected by the fixed-order mechanism $(\classifier_A^+,\classifier_B^+,1)$ if he reports truthfully.\footnote{Here some types in $\classifier_A\cap\classifier_B\setminus \Updelta\text{OAB}$ might have incentive to misreport if they can get selected by the fixed-order mechanism $(\widehat\classifier_A,\classifier_B^+,1)$ with a lower cost. }
\end{proof}