\section{Related Work}
\label{sect:relatedwork}
One of the challenges with most learning-based methods is that they require supervision and ground truth model information. 
This information is difficult to obtain accurately from real-world scenes and is often based on data from synthetic scenes. 
Drawing on the continuity of implicit representations____ and differentiable volumetric rendering (\textbf{DVR})____, end-to-end training of a 3D model directly from 2D images has been developed. 
This advancement has led to the emergence of implicit neural representations, especially Volumetric Radiance Field-based methods, as a powerful alternative to 3D scene reconstruction, particularly in novel view synthesis (\textbf{NVS}).
Neural Radiance Field (\textbf{NeRF})____ introduced by Mildenhall \etal has been the seminal work for many high-quality NVS follow-up methods.
NeRF learns complex scenes as continuous particle fields of volume density and view-dependent color modeled by an MLP. Using positional encoding and DVR techniques at inference achieves unprecedented photorealistic rendering quality.
However, due to dense queries in a large MLP, training and rendering times remain time-consuming.

The NeRF method takes as input a set of calibrated images with known poses: the extrinsic parameters (camera position and orientation), and the intrinsic ones (typically, the focal length and the principal point), usually obtained via SfM tools, \eg, COLMAP____.
NeRF generates images using a differentiable volume renderer, where camera rays traverse a finite cubic volume containing the scene's contents.

More specifically, the 3D position of a sampled point $\textbf{x} = (x,y,z) = \textbf{r}(t)$ and the viewing direction $\textbf{d} = (\theta,\phi)$ along a camera ray $\textbf{r}(t) = \textbf{o} + t\textbf{d}$, emitted from the camera's projection center $\textbf{o}$, are input into an MLP with weights $\boldsymbol\Theta$. The network outputs the volume density $\boldsymbol\sigma$ at that location, along with the color $\textbf{c} = (r, g, b)$ corresponding to the particle's radiance:
$$
\boldsymbol\sigma(t), \textbf{c}(t)=\operatorname{MLP}_{\Theta}(\textbf{r}(t), \textbf{d})
$$
\\
The predicted color $\hat{\mathbf{C}}(\mathbf{r})$ of the pixel is calculated by using these estimated densities and colors to approximate the volume rendering integral through numerical quadrature, as discussed by Max ____:
$$
\begin{gathered}
\hat{\mathbf{C}}(\mathbf{r})=\sum_k T\left(t_k\right) \alpha\left(\boldsymbol\sigma\left(t_k\right) \delta_k\right) \textbf{c}\left(t_k\right), \\
T\left(t_k\right)=\exp \left(-\sum_{k^{\prime}=1}^{k-1} \boldsymbol\sigma\left(t_{k^{\prime}}\right) \delta_{k^{\prime}}\right), \quad \alpha(x)=1-\exp (-x),
\end{gathered}
$$
where $T\left(t_k\right)$ denotes the accumulated transmittance, accounting for occlusion along the ray, and $ \delta_k = t_{k+1} - t_k $ represents the distance between two adjacent points along the ray.

During training, the squared errors measured from the pixels of input images are back-propagated through differentiable volume renderer to update the MLP weights. 
As a result, NeRF MLP learns a latent representation of the geometry and directional appearance of the scene. 
At inference, novel views from unknown camera poses are generated from the trained MLP using the same volume rendering technique.
Since the model size and the number of samples per ray are fixed before training, NeRF relies solely on the distribution of points from the coarse-to-fine volume sampling strategy to retrieve visible content.
As a result, NeRF supports only a single LOD, providing uniform detail across the entire image without clearly distinguishing between content near the camera and distant objects.

\textbf{Level of detail - High fidelity.} Level of detail (LOD) concepts were first applied to neural representations in Mip-NeRF____ and VolSDF____. Similar to image processing and rendering, this helps adjust the complexity of the content in the scene, thereby reducing the aliasing effect.
Mip-NeRF, in particular, used integrated position encoding (IPE) over conical frustums of pixel-footprint-cone-casting, to encode scene representation at various scales.
Instant-NGP (iNGP)____ adopted a hash encoding approach using a pyramid of multi-scale 3D grids to store learned features in a hash table, which were then processed by a compact MLP. This structure enables faster computation during both training and inference stages and allows control over the entire scene's LOD by adjusting two parameters: $N_{max}$, the finest resolution per axis of the highest grid level, and $T$, the fixed size of the GPU hash tables containing the features.
Developed on top of both Mip-NeRF and iNGP, Zip-NeRF____ used supersampling and reweighting to parameterize frustum along each ray using iNGPâ€™s multiresolution grid, allowing to achieve anti-aliasing and scale-reasoning while being significantly faster.
BungeeNeRF____ progressively trained NeRF MLP blocks to render satellite-ground cityscape scenes with extreme variations in scale. Each block has its own output, allowing the model to reasonably learn the finer details in the later blocks.
RING-NeRF____ represented 3D scenes as a continuous multi-scale model with an invariant decoder across spatial and scale domains. 
Its continuous coarse-to-fine optimization further enhanced reconstruction stability and accuracy.
VR-NeRF____ introduced a custom multi-camera cluster rig to densely capture real-world environments in high-resolution and high dynamic range, and used a LOD-focused rendering that can perform on a large-scale scene.
However, all these approaches only focus on the entire scene representation, neglecting the fine details of individual objects within the scene.

\textbf{Unbounded scene.} Unbounded outdoor scenes with very distant backgrounds are one of the limitations of NeRF-like methods because they require dense sampling of points along rays in space.
This would cause unbalanced details of objects near and far from the camera, resulting in blurry, low-resolution renderings.
NeRF++____ improved upon NeRF by dividing the scene into an inner unit sphere for the foreground and an outer space mapped to a more compact volume by an inverted sphere for the background.
Mip-NeRF 360____ is an extension of Mip-NeRF____ that enhances performance on unconstrained scenes by using non-linear scene parameterization and a coarse-to-fine online distillation framework with a Proposal Network.
Due to its high efficiency, these unbounded properties are applied in most of the later development methods, including in the foundation of our method.

\textbf{Compositional NeRF.} Both scene synthesis and manipulation can benefit from the compositional representation of scenes based on their internal components. However, decomposing and composing NeRFs remains challenging due to the nature of their implicit representation. NeRF-like methods often learn entire scenes holistically, making it difficult to capture the semantics of specific objects or the locations of regions.
Panoptic Neural Fields____, Nerflets____, NSGs____ learn multiple structured hybrid representations for urban scenes, enabling panoptic segmentation and object manipulation in NVS.
However, these category-specific methods only work on object representations that have been defined or constrained to a domain or type during or before training.
Moreover, the reconstruction fidelity is quite low and unstable due to the limited number of viewpoints available for the objects.
Other studies used object-level annotations for supervision.
ObjectNeRF____ used a two-pathway architecture to encode scene and object radiance fields separately, thus allowing for the manipulation of objects while maintaining scene consistency.
UDC-NeRF____ is an improvement of ObjectNeRF____, which integrated the decomposition and composition process into the learning model to improve the quality and consistency of the display.
However, these approaches rely heavily on view-consistent ground truth instance segmentation masks during training, which are costly to obtain in practice.
In contrast, our method does not require mask annotations and can render detailed images for user-specified ROIs, enabling NVS with high fidelity, consistency, and efficiency. 
Furthermore, built on top of Nerfacto from the Nerfstudio framework____, our method can effectively handle unbounded scenes and benefit from fast training and rendering. 
Nerfacto, indeed inspired by Mip-NeRF 360____ and iNGP____, also integrates ideas of important improvements from works such as NeRF-W____ and Ref-NeRF____.