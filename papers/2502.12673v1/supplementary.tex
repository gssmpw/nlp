% % ---------------------------------------------------------------------

% \begin{document}

\title{Supplementary Material for ROI-NeRFs: \\Hi-Fi Visualization of Objects of Interest \\within a Scene by NeRFs Composition}
% \title{ROI-NeRFs: Hi-Fi Visualization \\ of Objects of Interest within a Scene \\by NeRFs Composition\\
% \ \\
% \LARGE Supplementary Material
% }
% \title{ROI-NeRFs: Hi-Fi Visualization \\ of Objects of Interest within a Scene \\by NeRFs Composition}

\author[1,2]{Quoc-Anh Bui}
\author[1]{Gilles Rougeron}
\author[2]{Géraldine Morin}
\author[2]{Simone Gasparini}
\affil[1]{Université Paris-Saclay, CEA, List, F-91120, Palaiseau, France}
\affil[2]{Université de Toulouse, Toulouse INP -- IRIT, France}
\renewcommand\Affilfont{\itshape\small}

% \centerline{Supplementary Material}
% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

\IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

% \maketitle

\paragraph*{
This Supplementary Material complements our ROI-NeRFs paper, providing additional implementation details, evaluations, and analyses. 
It also includes a discussion of potential perspectives not covered in the main paper due to space constraints.
}

\setcounter{section}{2}


\section{Method}
\label{sect_method}

\setcounter{subsection}{2}
\subsection{Scene composition}
\label{sect:method:compo}

\textbf{Uniform Ray Composition.}\\
The composed rays from the ROI Samples Replacement strategy have uneven point counts due to the unpredictable number of samples within the AABB.
\cref{fig:sample_compo} shows an example of this problem. Similar to the toy example in \cref{fig:optimize_compo}, each ray from the Scene and ROI NeRFs has a fixed sample count of $6$.
In \textbf{(a)}, Scene samples are marked as blue dots with numbers, while points within the AABB replaced by the ROI are shown as green dots with letters. In \textbf{(b)}, the three tensor lines represent corresponding composed ray points with varying numbers of samples, preventing parallel tensor operations.
This mismatch prevents parallel ray processing using tensor operations.
Therefore, a naive approach of separating and processing these composed rays individually is inefficient, as it compromises the speed and efficiency of GPU parallelism, resulting in a significant and unacceptable increase in image inference time.

To address this challenge, that is, to ensure a fixed number of points on all composed rays, we propose a uniformity-preserving technique that combines all samples from both Scene and ROI rays, then making unnecessary points ``invisible'', as shown in \cref{fig:optimize_compo}.
In \textbf{(a)} and \textbf{(b)}, all samples from both Scene and ROI rays are combined to ensure an equal number of points on the composed rays, totaling $6 + 6 = 12$. 
Unnecessary points in \textbf{(c)}, such as Scene points inside the AABB (\textit{\textcolor{blue}{2}, \textcolor{blue}{3}}) and ROI points outside the AABB (\textit{\textcolor{ForestGreen}{a}, \textcolor{ForestGreen}{e}, \textcolor{ForestGreen}{f}}), are turned into invisible.
These invisible points are then set to the position of the ray origin ensuring no impact on ray aggregation, as shown in \textbf{(d)}.
\begin{figure}[!t]
\centering
\includegraphics[width=1\columnwidth]{fig8a_sample_compo.png}
\caption{
An example of an uneven number of points on the composed rays.
Scene samples are marked as blue with numbers, while replaced ROI points are shown as green with letters.
% \textbf{(b)} 
% The three tensor lines represent corresponding composed ray points with varying numbers of samples, preventing parallel tensor operations.
}
\label{fig:sample_compo}
\end{figure}
\begin{figure}[!ht]
\centering
\includegraphics[width=1\columnwidth]{fig8b_optimize_compo.png}
\caption{
 Uniform composition for efficient ray aggregation.
 % \textbf{(a)} All samples from both Scene and ROI rays are combined \textbf{(b)} to ensure an equal number of points on the composed rays, here $6+6=12$. 
 % \textbf{(c)} Unnecessary points, such as Scene points inside the AABB (\textit{\textcolor{blue}{2}, \textcolor{blue}{3}}) and ROI points outside the AABB (\textit{\textcolor{ForestGreen}{a}, \textcolor{ForestGreen}{e}, \textcolor{ForestGreen}{f}}), are turned into invisible.
 % \textbf{(d)} These invisible points are then set to the position of the ray origin ensuring no impact on ray aggregation.
 % We also show the importance of repositioning the empty points, otherwise small, thin surfaces, such as the mesh surface of a chair in the render, may be affected by invisibility during the composition.
}
\label{fig:optimize_compo}
\end{figure}
These points are assigned a density of zero  and repositioned to the ray’s origin, where content is rarely found. 
This approach prevents the impact of unnecessary points, resulting in a similarity to color pixel synthesis with only valid points on rays. 
\IEEEpubidadjcol
The repositioning of empty points is crucial, as demonstrated in renders of \cref{fig:optimize_compo}, where small, thin surfaces, such as a chair's mesh, might be affected by invisibility during composition.
Furthermore, we calculate density and color only for samples contributing to ray aggregation, keeping MLP queries efficient. 
The final rendering thus reduces artifacts and maintains performance, offering a high-quality result with minimal computational overhead. 

\section{Experiments}
\label{sect:exp}

\setcounter{subsection}{1}
\subsection{Quantitative Evaluation} 
\label{sect:exp:quanti_comparison}

\input{table_psnr_baselines}
\cref{tab:baselines} reports the PSNR scores for four baseline methods evaluated on the test set of each ROI across both datasets.
Scene NeRFs and Full NeRFs were trained with the default $100$k iterations to ensure optimal learning of scene details. 
However, for these Nerfacto-based methods, increasing the model size or the number of ROI images does not consistently improve rendering quality.
Indeed, for certain evaluations, such as the $\mathit{Table3}$ and $\mathit{Wreath}$ in the \textit{Egypt} dataset, the ``huge'' model showed a slight decrease in quality compared to the smaller ``big'' version. 
Similarly, in the \textit{Hôtel de la Marine} dataset, despite a larger training set with more ROI images, the Full model's PSNR score remained lower than that of the Scene model.
Thus, training the entire scene with a larger model to improve local detail quality can be both costly and inconsistent.

\input{table_quanti_eval_single}
\cref{tab:quanti_single} presents the quantitative evaluations of our proposed method.
We compare the results of each baseline version ($1$--$4$) with our compositional approach, which combines the ROI NeRF with the baseline models. 
The experiments focus on a single ROI per dataset, $\mathit{Table1}$ in \textit{Egypt} and $\mathit{Fruit}$ in \textit{Hôtel de la Marine}, to avoid any bias that a varying number of ROIs could introduce in the views.

As noted, since ROI NeRFs lacked information about the surrounding scene, this led to worse scores compared to baseline methods on test sets with sufficiently distant cameras, where the ROI’s occupied area in the image is not too big.
\cref{tab:quanti_single} confirms this for the \textit{Egypt} dataset.
However, in the \textit{Hôtel de la Marine} dataset, the ROI NeRF scores are better than the baselines. 
This can be explained by the nature of the dataset, which consists mainly of two types of camera views: (1) close-up views of the table area containing all the objects of interest, where the objects occupy a large part of the image, and (2) wider views, which are excluded from the ROI set during camera selection due to their distance.
In addition, the large number of close-up images selected for training the ROI NeRFs results in the neighboring regions also being encoded in detail. 
Taken together, these factors result in significantly better ROI scores than the baselines. 
However, despite the better scores driven by the dominance of ROIs in the rendered image, artifacts and missing geometric information persist due to the limited generality of the ROI NeRF, as illustrated in \cref{fig:hotel_roi_artifact}.

\begin{figure*}[ht!]
	\centering
	\includegraphics[width=1\linewidth]{hotel_missing_geo.png}
    % \centering
	\caption[]{Missing geometric information in general views of the ROI NeRFs, compared to the Scene NeRF method and our approach, indicated by red arrows.
 }
	\label{fig:hotel_roi_artifact}
\end{figure*}


\input{table_details_compo}
\input{table_interval}
\cref{tab:details_compo} provide detailed PSNR scores for our method, including the multiple-composition approach, for each object in both datasets. 
The multiple-composition method combines all valid visible ROIs in the corresponding inference view, using the same test images as the single-composition and baseline methods to ensure compatibility and fairness.
Similarly, our method of compositing multiple ROIs also consistently improves the rendering quality of objects for any model and outperforms the single-composition method.
This is reasonable because the quality improvement depends on the number of ROIs visible in the render view.

\input{table_psnr_aabb}
We also evaluated the single-composition methods only in the ROI's AABBs, as shown in \cref{tab:aabb}.
The improvement in the ROI of our method over the baselines is substantial, particularly when contrasted with the corresponding scores calculated over the whole image in \cref{tab:quanti_single}, which are drowned out by the other part of the images.
Additionally, the PSNR scores for ROI NeRFs within the AABB are higher than those for the whole image, which demonstrates that the LOD is only high in the ROI.

\setcounter{subsection}{3}
\subsection{Ablations} 
We provide a full evaluation of our method's components' impacts on all four baseline methods in Ablation \cref{tab:ablation}.
\input{table_ablation}

\section{Limitations and Perspectives}
\label{sect:limitation}

\begin{figure}[ht]
	\centering
	\includegraphics[width=1\linewidth]{color_shift.png}
    % \centering
	\caption[]{The color shifting issue occurs when compositing an ROI NeRF trained on close-up images that differ in color and brightness from those used for the Scene NeRF. In this example, while the pattern details on the stone table are enhanced, the carved area shows a color mismatch with the background, causing the ROI to stand out from the rest of the image.
 }
	\label{fig:limit_color_shift}
\end{figure}

\begin{figure}[ht]
	\centering
	\includegraphics[width=1\linewidth]{glossy.png}
    % \centering
	\caption[]{
     Failure cases in composing glossy reflective objects. Inferred images from the proposed composition method are shown for two objects in the \textit{Hôtel de la Marine} dataset, with noisy composition patches occurring in ROIs highlighted by red arrows.
 }
	\label{fig:limit_glossy}
\end{figure}

As noted about the limitations of our methods, we provide example images of artifacts in the ROIs during compositing. \cref{fig:limit_color_shift} shows the color shifting issue, while \cref{fig:limit_glossy} illustrates the failure on reflective objects.

While ROIs may typically be implicitly defined during the image-capturing stage due to more detailed captures, in our proof-of-concept they are manually selected by the user by drawing AABBs around objects in the sparse point cloud space to simplify processing.
Alternatively, AABBs could be automatically generated based on user-selected objects or identified autonomously in regions with a high concentration of input images.
Future developments could explore methods where users localize objects in a few images using manual selection or object segmentation tools, such as the Segment Anything Model (SAM)~\cite{kirillov2023segany}.
The 3D AABBs could then be automatically generated by consistently identifying visible 3D keypoints in the 2D images or through point region classification using 3D point clustering algorithms, like HDBSCAN~\cite{mcinnes2017hdbscan}.

The Camera Selection technique for scene decomposition currently relies solely on the number of visible keypoints to select cameras.
While this approach provides stable performance for partitioning, we still lack a clear definition of how many images focusing on different parts of the ROI are needed to effectively train the ROI NeRF. 
To address this issue, we could integrate the uncertainty estimation of the information gain in scene understanding to identify the most informative views.
Thus, we could efficiently capture and add images to the training set efficiently with minimal additional resources as shown in \textit{ActiveNeRF}~\cite{pan2022activenerf} or \textit{IOVS4NeRF}~\cite{xie2024iovs4nerf}.

% \abc{Add Hierachie decomposition,many level of decompose, components. }
The scene decomposition and recomposition strategy has proven to enhance rendering quality consistently. 
A potential direction for future work is hierarchical decomposition and composition. 
For instance, at level 1, large objects like dining tables can be separated and trained at the corresponding LOD, followed by level 2, where details such as patterns or small objects on the table are trained at a finer LOD. 
This process can continue for additional levels. 
During inference, depending on the viewpoint, the NeRF of each component at each level will be selected appropriately to achieve LOD-focused high-quality rendering.
This approach could also be applied to articulated objects, inspired by \textit{NARF22}~\cite{lewis2022narf}.

An unexplored aspect of our method is the ability to edit objects in the scene. 
To implement this, it is necessary to tighten AABBs around the object or preferably to define the mask of the object of interest.
We could perform segmentation on 2D images to reconstruct consistent masks for objects of interest, inspired by \textit{ObjectNeRF}~\cite{yang2021objectnerf} and \textit{UDC-NeRF}~\cite{wang2023udcnerf}.
AABBs could be refined either by intersecting these object masks through space or by propagating image mask IDs to the 3D sparse point cloud, in a similar fashion as \textit{K3BO}~\cite{10222520}.
Another approach is to employ 3D segmentation after estimating the object's geometry through rapid training of a NeRF model on the object. 
After segmentation, objects could be removed, replaced, or moved. 
However, this creates new issues as the scene illumination has to remain consistent while object appearance is embedded in their respective NeRFs. 
Similarly, any holes left under moved objects should be filled in.
Inpainting methods like those in \textit{Removing Objects From NeRF}~\cite{Weder2023Removing} could then be applied.

% \bibliographystyle{IEEEtran}
% \bibliography{bibsample}

% \vfill
% \end{document}