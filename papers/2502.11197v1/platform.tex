\section{{\platformName} framework}
\subsection{Competitive search background}\label{sec_background}
The \platformName \platform supports the simulation of ranking competitions. We now turn to describe the structure of ranking competitions, and more generally, competitive search \cite{kurland_competitive_2022}.

A ranking game between publishers (document authors) is driven by their ranking incentives. That is, we assume that some publishers are motivated to have their documents highly ranked for a query. In response to a ranking induced for the query, the publishers (players) might modify their documents so as to improve their future ranking. In the general setup \cite{raifer_information_2017}, every player begins with an \textit{initial document} pre-determined by the system designer. During each round of the game, every player: (1) modifies her document, and (2) receives information about the ranking induced for the query; the player can observe the content of other documents in the ranking. Players may then modify their documents in subsequent rounds with the goal of promoting their rankings with respect to the query of the game. We refer to a collection of games, each associated with a different query, as a {\em competition}.

% \ref{fig_scheme_game}.

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figs/platform/shceme_game_2.jpg}
%     \caption{Scheme of a ranking game.}
%     \label{fig_scheme_game}
% \end{figure}

% Each competition simulated on our platform comprised a batch of games. Each game is associated with a query and begins with an initial document containing information relevant to that query. A set of agents is assigned to participate in the game. During each round, players receive feedback on the documents published by other players and modify their candidate documents accordingly to achieve the highest ranking.

\subsection{{\platformName} {\platform}}
The core component of {\platformName} is the {\platformName} {\platform}, a highly configurable simulator designed to simulate ranking competitions.
% This simulator enables the study of repeated ranking games, where document authors (players) compete for a query, and iteratively modify their document to promote their rankings for an undisclosed ranking function.
It allows fine-grained control over key parameters, including the ranking function, the query for which every game is performed, initial documents, and the type of the players: a specific type of a player is referred to as an {\em agent}. Specifically, \platformName{} is highly supports players based on LLMs. Additionally, it supports the configuration of the number of simulated games, the number of players per game and the number of rounds per game, providing flexibility for diverse experimental setups. The simulation output includes a dataset containing all documents generated by players across rounds, along with their rankings, forming the basis for competition analysis.

% Formally, our framework focuses on three primary aspects: \textit{game properties}, \textit{agent properties}, and \textit{ranking function properties}.

% \textit{Game properties} are the parameters of games, such as the number of rounds in every repeated game, the number of players in each game, and other relevant settings. (Further details are provided in Section \ref{platform_game_properties}.)

% \textit{Agent properties} refer to as the characteristics of the agents, including the agent type (e.g., LLM-based agent or human agent), the structure of the message she received by the system designer, and additional behavioral traits; more information on \textit{Agent properties} can be found in Section \ref{platform_agent_properties}).

% \textit{Ranking function} properties include the type of the ranking function that induces the ranking of players' documents in each round of the games (more details are provided in Section \ref{platform_ranking_properties}).

The simulation parameters can be configured via a \textit{json} file or customized directly within the {\platformName} {\platform} code-base, offering significant flexibility for tailoring the platform to various experimental scenarios. The code-base is publicly available\footnote{\url{https://github.com/csp-platform/Simulator}.}.

% In section \ref{section_data}, we demonstrate the platform's capabilities by generating datasets from various types of competitions involving LLMs. Additionally, in section \ref{section_analysis}, we present an array of methods for analyzing and comparing competitions to gain insights into competition's dynamics. We used those methods to compare different types of competitions.

\subsubsection{Competition properties}\label{platform_game_properties}

A ranking competition is structured as a collection of games. Each game is composed of a set of players competing over a few rounds for a predefined query. A competition is highly configurable, allowing customization of parameters such as the assigned query for every game, and the corresponding initial document, the number of rounds and more.
% The platform supports two operational modes: \textit{round-by-round} and \textit{game-by-game}. In \textit{round-by-round} mode, each round of the competition involves all games being played simultaneously, with players modifying their documents once per round. This mode can accommodate online scenarios, such as competitions involving human participants against other humans or against LLMs. In contrast, the \textit{game-by-game} mode involves completing all rounds of a single game before proceeding to the next. This approach offers advantages in optimizing memory allocation efficiency and reducing runtime.
% Our \platformName{} \platform{} has the option to simulate a competition from a predefined state (e.g., after several rounds of each game). This functionality help exploring different scenarios that start from different entry point. Additionaly, it facilitates splitting lengthy simulations into smaller batches for computational and experimental efficiency. 
Additional, our \platformName{} \platform{} offers the capability to simulate a competition from a predefined state (e.g., after several rounds of each game). This functionality aids in exploring various scenarios that begin from different starting points. Moreover, it allows for breaking lengthy simulations into smaller batches.

% Additionally, it enables researchers to explore competitions under varying initial conditions, offering insights into how different starting points influence the outcomes and dynamics of the competition.

% This approach offers advantages in terms of efficiency of memory allocation and reduced runtime.

\subsubsection{Player properties}\label{platform_agent_properties}
We assume that the same set of players is assigned to each game within a competition. Consequently, the only difference between games in a competition is the query being played and the corresponding initial document.
% However, our simulator is versatile and can also enable competitions involving predefined documents or mixed scenarios with LLMs and predefined documents.
Players are exposed to documents from previous rounds along with their corresponding rankings. Note that the rankings are the only signals available to players regarding the undisclosed ranking function. Accordingly, players modify their documents so as to be ranked higher in subsequent rounds. Different players may interpret and utilize these signals in various ways to guide their document modifications. In this paper, we focus on scenarios where different LLMs compete against each other. 

\subsubsection{Ranking function properties} \label{platform_ranking_properties}
The ranking function determines the rankings of players' documents in every round of a game. The {\platformName} {\platform} is designed to be highly flexible, allowing the integration of any type of ranking function such as feature-based or neural-based ranking functions.
Additionally, the {\platformName} {\platform} supports implementing penalty mechanisms to enhance the dynamics of players in the competition. Inspired by ranking competitions among human participants \cite{mordo_search_2025, raifer_information_2017, nachimovsky_ranking-incentivized_2024}, these mechanisms can penalize players—by demoting their rankings—for copying documents from others or for refrain from modifying their documents across multiple rounds.


\subsection{{\platformName} {\analyzer} and {\platformName} {\compare}}
We introduce two additional components in the {\platformName} framework designed to facilitate the analysis of competitions.

The {\platformName} {\analyzer} is a module designed to enable in-depth analysis of an individual competition. The analysis is focused on different measures related to different aspects of the competition dynamics. For example, we measure how the diversity of documents of ranked lists evolves over rounds \cite{mordo_search_2025}.
% The {\platformName} {\analyzer} framework is implemented in a \textit{Python} module that processes competition datasets 
% through the following steps: (1) integrating the dataset of a ranking competition into a background index\footnote{We used Clueweb09 in our demonstration. The integration is needed for collection statistics.} implemented using Pyserini \cite{Lin_etal_SIGIR2021_Pyserini}; (2) extracting document features, such as Okapi BM25 scores; (3) generating document representations, such as TF.IDF and SBERT \cite{reimers_sentence-bert_2019}; and (4) performing analyses on the documents using different document representations and measures. Further details on the measures are formally described in Section \ref{label_subsection_evaluation_measures}.
For cases where comparisons between datasets generated under different competitive scenarios are of interest, we developed {\platformName} {\compare}, an interactive tool (dashboard) for comparing competitions with different configurations.

The functionalities of {\platformName} {\analyzer} and {\platformName} {\compare} are demonstrated using several research questions. (See Section \ref{label_section_RQs}.) The code-bases are available on GitHub\footnote{{\platformName} {\analyzer}: \url{https://github.com/csp-platform/Analyzer} and {\platformName} {\compare}: \url{https://github.com/csp-platform/Compare}}. Note that {\platformName} {\analyzer} and {\platformName} {\compare} were also designed to analyze and compare ranking competitions, even if they were not generated by the {\platformName} {\platform}; e.g., competitions conducted between human players \cite{raifer_information_2017, goren_driving_2021, nachimovsky_ranking-incentivized_2024}.