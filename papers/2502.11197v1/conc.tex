\section{Conclusions and Future Work}
We introduced a novel simulation platform, the \platformName{} \platform, designed to simulate ranking competitions with LLMs as document authors. Along with the platform, we presented the \platformName{} \analyzer and \platformName{} \compare tools, which facilitate the analysis of individual competitions and the comparison of multiple competitions. We demonstrated the capabilities of \platformName{} by generating multiple datasets with LLM-based agents and comparing the results across different datasets. Additionally, we compared LLM-based competitions with a human-based competition to study the behavioral dynamics of the two types of agents 
(LLM and human).

We found that LLMs exhibit strategic behavior similar to that of human players \cite{basat_game_2017, raifer_information_2017}, with an even greater tendency to adopt the \textit{mimicking-the-winner} strategy. This strategy results in herding behavior towards the top-ranked players \cite{goren_driving_2021}, leading to a reduced diversity in the corpus \cite{mordo_search_2025}. Further exploration revealed that the most influential factor affecting the behavior of LLMs is the language model they use, followed by the prompt provided. In contrast, the choice of ranking function had minimal to no effect on the dynamics of the competition.

For future work, we plan to use \platformName{} so as to study competitions that include different types of LLMs competing against each other, as well as humans competing against LLM agents.

% \begin{itemize}
%     \item Online (with human games)
%     \item multiple queries
%     \item mix llms-humans
%     \item planted documents
%     \item players are familiar with the number of round
%     \item 
% \end{itemize}
% XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX 