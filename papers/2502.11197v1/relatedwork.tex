\section{Related Work}
Previous studies addressed theoretical and empirical results. Kurland and Tennenholtz \cite{kurland_competitive_2022} highlight several perspectives in the competitive search ecosystem. From \textit{the ranker perspective}, the focus is on designing mechanisms that can enhance social welfare (i.e., mitigate herding of publishers and promote fairness among publishers \cite{kurland_competitive_2022}). \textit{The publisher perspective} includes benefits to document authors who strategically manipulate their documents to improve the future rankings. \textit{The user perspective} includes benefit of the user, who interacts with the search engine to satisfy an information need expressed using a query. To empirically explore these perspectives, ranking competitions in various settings have been conducted and analyzed.

\myparagraph{Ranking competitions} Game theory \cite{aumann1995repeated, fudenberg_game_1991} provides a foundational framework for modeling repeated ranking competitions where document authors do not cooperate.

Several studies have used controlled ranking competitions as a tool to study dynamics in competitive settings. Raifer et al. \cite{raifer_information_2017} conducted the earliest controlled ranking competitions among students, revealing a key strategy employed by publishers: mimicking the documents most highly ranked in previous rounds. Goren et al. \cite{goren_driving_2021} demonstrated how search engines could drive predefined and targeted content effects, leading to the herding phenomenon \cite{banerjee_simple_1992}. Nachimovsky et al. \cite{nachimovsky_ranking-incentivized_2024} extended this line of research by conducting ranking competitions where authors competed to improve their ranks with respect to multiple queries.

In addition to these studies, another line of research focuses on developing algorithmic adversarial attacks designed to manipulate rankings and promote specific documents. Examples include Castillo et al.'s work on adversarial strategies \cite{castilo_adverserial}, Raval and Schwing's exploration of one-shot adversarial attacks \cite{raval_one_2020} and Mordo et al.'s study on strategic document manipulation in competitive search setting with diversity-based ranking \cite{mordo_search_2025}.


\myparagraph{LLMs in competitive search} The use of large language models (LLMs) in competitive search setups has several aspects. First, a growing number of document authors leverage LLMs to modify and even generate content for search engine optimization (SEO) purposes \cite{Memon2024-re, Aggarwal2023-aw, nachimovsky_ranking-incentivized_2024, wu_survey_2024}. LLMs can generate high-quality content \cite{nachimovsky_ranking-incentivized_2024}, sometimes indistinguishable from content generated by humans \cite{wu_survey_2024}, and aimed at improving rankings. Second, various tasks require data annotation are not scalable, and can benefit from LLM assistance \cite{tan_large_2024, faggioli_perspectives_2023}. Organizing ranking competitions among human participants, similar to conducting annotation tasks, has traditionally posed significant challenges of scalability. LLMs provide new opportunities for studying ranking competitions by simulating document authors, enabling researchers to simulate competitive search scenarios efficiently.

% LLMs provide a compelling alternative to human participants, enabling researchers to simulate competitive search scenarios efficiently. For instance, LLMs can emulate agents engaging in "white hat" optimization tactics or facilitate the design of optimal ranking functions to promote social welfare.
Third, the emergence of LLMs has introduced novel dynamics in human-LLM interactions, particularly within competitive search settings \cite{nachimovsky_ranking-incentivized_2024, bardas_prompt-based_2025}. For example, researchers can study how document authors react to LLM-generated content \cite{nachimovsky_ranking-incentivized_2024}, or explore how LLMs perform when competing against human players in ranking games \cite{bardas_prompt-based_2025}. Finally, LLMs themselves are the basis for the potential shift from search engines to (conversational) question answering (QA) systems (e.g., \cite{perplexity}). Additionally, these systems can be used for zero-shot or few-shot ranking tasks \cite{Hou2024-tm, Qin2023-hg, bardas_prompt-based_2025}.

% Finally, LLMs themselves are increasingly being employed as ranking functions. For instance: Encoders such as E5 \cite{wang_text_2024} and Contriever \cite{izacard_unsupervised_2022}; chatbot-based systems such as ChatGPT used for 


\myparagraph{Agents in decision-making tasks} Recent research has increasingly highlighted the capabilities of large language models (LLMs) in decision-making tasks, demonstrating their potential to function as autonomous agents in complex economic environments that often require advanced strategic reasoning \cite{horton_large_2023, wang_text_2024, zhu_capturing_2024, li_stride_2024, shapira_glee_2024, noauthor_letta_nodate}. In other work, LLMs were used to simulate users in information retrieval and recommendation systems \cite{breuer_report_2024, zhang_generative_2024, wang_user_2024}, and we use them to simulate authors.

\myparagraph {LLM-based multi-agent simulation} Various tools and frameworks have been developed to create agents powered by LLMs\footnote{For example, \url{https://www.letta.com} and \url{https://github.com/Thytu/Agentarium/tree/main}} \cite{feng_agile_2024, qiao_taskweaver_2024, liu_agentlite_2024}. However, these tools are not easily customizable to simulate ranking competitions. They lack specific modules designed to facilitate the analysis and exploration of unique aspects of ranking competitions, such as dynamic interactions between agents and ranking strategies.