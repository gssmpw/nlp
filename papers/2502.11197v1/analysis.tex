\section{Analysis of Competition Datasets}\label{section_analysis}
In this section, we present an extensive analysis of seven datasets (out of the 22 created datasets) generated using the {\platformName} {\platform}, selected due to space constraints. All the datasets and their corresponding analyses, conducted using the {\platformName} framework, are available in our GitHub repository\footnote{\url{https://github.com/csp-platform/Datasets}.}. In Section \ref{label_section_RQs} we present three research questions used to compare the seven datasets. Then, in Section \ref{label_subsection_evaluation_measures} we discuss the measures used to analyze the datasets and to compare them. The analysis addressing the three research questions is detailed in Sections \ref{section_rq1}, \ref{section_rq2}, and \ref{section_rq3}. In this section, we define a "winner" as the author of the top-ranked document in a given round, or the document itself. Conversely, "losers" refer to authors (or documents thereof) whose documents did not achieve the highest rank in that round.

\subsection{Research questions}\label{label_section_RQs}
We address three main research questions:
\begin{itemize}
    \item (RQ1) What are the differences between competitions with LLMs as players and competitions with humans as players in terms of player's strategies, resulting documents, and more? To address this question, we utilized \ASRC's dataset \cite{mordo_search_2025} described in Section \ref{section_data}.
    \item (RQ2) How do the characteristics of LLM-based agents (specifically, the language model and prompt type) affect the dynamic of the competition? For this analysis, we fixed the ranking function to the (unsupervised) E5 \cite{wang_text_2024}.
    \item (RQ3) How does the choice of ranking function affects the competition dynamics? For this comparison, we fixed the LLM to Llama3.1 \cite{dubey_llama_2024}.
\end{itemize}
A summary of the datasets used for addressing these research questions is presented in Table \ref{label_table_RQs}. Additionally, for every research question, we focused on a subset of measures, prioritizing those that provided the most insightful findings. We emphasize that overall our findings are consistent among all measures.

% All datasets and the full comparative results are publicly available in our GitHub repository.

\myparagraph{Quality and relevance judgements}
We annotated the datasets used for addressing the three research questions. (See Table \ref{label_table_RQs}.) Each document was judged for binary relevance to a query by three crowd workers (English speakers) on the Connect platform via CloudResearch \cite{noauthor_introducing_2024}. Three workers annotated the quality of each document with the categories: valid, keyword-stuffed\footnote{Adding query terms to documents in an excessive manner.}, and spam. We used the same instructions to annotators as presented in past work \cite{nachimovsky_ranking-incentivized_2024, mordo_search_2025}. For each document, the final quality grade was defined as the number of annotators who judged the document as valid. Accordingly, the final relevance grade was the number of annotators who marked the document as relevant. Due to budget limitations, annotations were not performed on datasets where the Contriever \cite{izacard_unsupervised_2022} ranking function was used. Additionally, we annotated only the top-ranked document from every two rounds. The inter-annotator agreement rates (free-marginal multi-rater Kappa \cite{fleiss_measuring_1971}) ranged between 45.2\% and 66.8\% for quality judgments. For relevance judgments, the agreement rates ranged from 46.5\% to 66.0\%. Since in \ASRC \cite{mordo_search_2025}, every document was annotated by five workers, we calibrated the relevance and quality grades to 3 by multiplying the relevance and quality grades of \ASRC{} \cite{mordo_search_2025} by $\frac{3}{5}$. In addition, we selected from \ASRC \cite{mordo_search_2025} only the top-ranked documents to align with the comparison with the LLM-based competitions here.

\begin{table}[t]
\caption{Research questions (RQs) and the corresponding competition datasets used to address them. Each competition includes the following components: either Llama or Gemma-LLM; a ranking function (E5, Contriever, or Okapi); Listwise or Pairwise \contextualized{} of the prompts; and four or five players per game (unless specified otherwise, the competition includes five players). One of the competitions in RQ2 includes a modified \shared{} that instructs the LLM not to copy other players' documents (denoted as 'no-copy'). We annotated all the listed datasets, excluding datasets with a Contriever ranking function. \kq, \kr{} are the inter-annotator agreement rates (free-marginal multi-rater Kappa) of the quality and relevance judgements, respectively.}
\label{label_table_RQs}
\centering
\input{tables/RQs}
\end{table}


\subsection{Measures}\label{label_subsection_evaluation_measures}
We propose a broad set of measures applied to the competitions datasets. These measures were designed to facilitate the analysis of competitions and to gain multifaceted insights into the dynamics and outcomes of competitive search scenarios. To quantify similarity between documents we employ the TF.IDF\footnote{The results obtained using alternative text representations, including E5 \cite{wang_text_2024} with cosine similarity and SBERT \cite{reimers_sentence-bert_2019} with cosine similarity, were consistent with those attained for TF-IDF. Thus, they are omitted for brevity.} text representation with cosine similarity. Note that additional text representations can be easily integrated into \platformName and may, in some cases, provide different insights \cite{mordo_search_2025}. The measures are averaged over queries (games) unless stated otherwise.

We categorized the measures to five classes of competition properties:

% we employ various text representations, including (unsupervised) E5\footnote{intfloat/e5-large-unsupervised.} \cite{wang_text_2024} representation with cosine similarity; SBERT \footnote{all-MiniLM-L6-v2.} \cite{reimers_sentence-bert_2019} representation with cosine similarity; and JACCARD similarity

% (1) mimicking the winner; in past work the strategy of mimicking the top-ranked documents ("winners") was observed, and formally proofed as the optimal strategy of players in ranking games  \cite{raifer_information_2017}. (2) Diversity of the ranked list of documents; inspired by the work of Mordo et al. \cite{tommy}(3) Convergence of the competition; i.e., analyze the modifications of documents in consecutive rounds.(4) Relevance and Quality; and (5) Winner (author of top-ranked document in a ranked list) Properties.
% The evaluation methods measures corresponding to each class of properties are fully implemented within the {\platformName} {\analyzer}. 
\begin{itemize}
    \item \textit{\textbf{Mimicking-the-winner}} (mimicking content from top-ranked documents) has been identified, both theoretically and empirically, as a strategy employed by players who loose in a round to improve their chance of winning in subsequent rounds \cite{raifer_information_2017}. Following prior work, we analyzed the document modification strategies by analyzing the changes in the documents' feature values\footnote{The features are selected from \url{http://www.research.microsoft.com/en-us/projects/mslr} or from past work on ranking competitions \cite{raifer_information_2017, nachimovsky_ranking-incentivized_2024, mordo_search_2025}.} over rounds \cite{qin_introducing_2013,nachimovsky_ranking-incentivized_2024, raifer_information_2017, mordo_search_2025}. The background dataset used to compute IDF based features was Clueweb09\footnote{\url{https://lemurproject.org/clueweb09/}.}.
    We analyzed changes in feature values of winner documents between consecutive rounds; $W_i$ and $W_{i+1}$ are the
winner documents in rounds $i$ and $i+1$, respectively. We focus on cases where $W_i$ and $W_{i+1}$ are produced by different
players as it was observed in past work
\cite{raifer_information_2017, mordo_search_2025} that players who win a round are
unlikely to substantially modify their document for the subsequent round.

% on changes in documents that lost for at least three consecutive rounds before they reach the first position. We compare the average feature values of winners in each corresponding round (\w) with those of the loser  (\LL). Inspired by Raifer et al. \cite{raifer_information_2017}, we split the losers into two groups according to whose feature values were lower than or equal to the winner (\llew); or greater than the winner (\lgw), three rounds prior to their win.
Additional approach to studying the phenomenon of \textit{mimicking-the-winner} is to analyze the similarity of winners' documents between consecutive rounds \cite{mordo_search_2025}. As in the feature values analysis, we focus on cases where $W_i$ and $W_{i+1}$ are produced by different players. Note that high similarity values indicate that a player has made modifications that make her document similar to the previously winning document. We also analyze the similarity between the top two ranked documents as an additional indicator of the herding effect\footnote{The tendency of players' documents to converge towards similar content, leading to a reduction in topical diversity in the ranked list.} \cite{goren_driving_2021}. If the second-ranked player is \textit{mimicking-the-winner}, we expect an increased similarity between the top two ranked documents.
\item \textit{\textbf{Diversity}} of documents. We measure diversity at two levels: the ranked-list-level and the player level. Diversity at the ranked-list-level is computed by the minimum of inter-document similarity (averaged over queries) across rounds \cite{mordo_search_2025}. Diversity at the player level is computed by the similarity of documents produced by the same player between consecutive rounds. It is averaged over players and queries across rounds.
\item \textit{\textbf{Convergence}} of a competition. Informally, this class includes measures that assess whether documents continue to be modified as the game progresses. As in the \textit{Diversity} class of measures, we measure convergence at the ranked-list-level and at the player level. As for the ranked-list-level, we compute the minimum of the inter-document similarity in a ranked list, enabling us to observe how inter-document similarities in a ranked list evolve over rounds. We also examined the number of unique documents over rounds to detect potential convergence to unique documents. At the player level, convergence was estimated by measuring the similarity between documents of the same player between consecutive rounds, as computed in the \textit{Diversity} class of measures. The resulting player level analysis will indicate how players modify their documents in comparison to their documents from the previous round.

\item \textit{\textbf{Quality and relevance}} of documents in a competition. We analyzed the quality and relevance grades of top-ranked documents in each competition across rounds. The annotation process and the methodology for computing the quality and relevance grades are detailed in Section \ref{label_section_RQs}.

\item \textit{\textbf{Top-ranked players' statistics}}. We analyzed two statistics of the top-ranked players in a competition. The first is the proportion of the wins, computed by the ratio of the number of wins by the best-performing player in a game to the average number of expected wins\footnote{The average number of expected wins in a game is computed as the total number of rounds divided by the number of players in a game.} assuming every player has a random chance to win a round of a game (averaged over games). For instance, in our competitions with five players and 30 rounds, one extreme case occurs when each player wins randomly, resulting in a ratio of 1. In the opposite extreme, if a single player wins all 30 rounds in all games, the ratio would be $\frac{30}{6} = 5$. The second statistic is about winning players. We identify the overall best-performing player across all games in a competition (i.e., the player with the highest number of wins, aggregated over rounds and games).
% We analyzed the best agents only on competitions in which LLMs were the agents.
\end{itemize}

\subsection{Results (RQ1): human vs. LLM-based agents}\label{section_rq1}

We now turn to the comparison between competitions with LLM-based agents and those conducted with human players. For the LLM-based competitions, we used the same ranking function and the same number of players per game as used in {\ASRC} \cite{mordo_search_2025}.
% However, no penalty mechanism was applied to maintain alignment with the prompts and setup presented in Bardas et al. \cite{bardas_prompt-based_2025}.
We employed Llama-based models, utilizing both Pairwise and Listwise prompts, along with the \shared{} as described in Bardas et al. \cite{bardas_prompt-based_2025}. Each player was assigned a distinct persona as described in Section \ref{section_data} excluding the persona of a Data Science professor, which was randomly selected to be omitted.

We now turn to study the \textit{mimicking-the-winner} phenomenon. Specifically, we
analyze changes in feature values of winner documents as described in Section \ref{label_subsection_evaluation_measures} \cite{raifer_information_2017, nachimovsky_ranking-incentivized_2024, mordo_search_2025}. The features are divided into two main categories: query-independent and query-dependent features. The query-independent features are (i) \textbf{Length}: document length, (ii) \textbf{StopwordRatio}: the ratio of stopwords to non stopwords in the document; the INQUERY stopword list was used \cite{Allan+al:00a}, and (iii) \textbf{Entropy}: the entropy of the unsmoothed unigram maximum likelihood estimate induced from the document. The query-dependent features are (iv) \textbf{LM.DIR}: the query likelihood score of a document where document language models are Dirichlet smoothed with smoothing parameter set to 1000 \cite{zhai_study_2001}, (v) \textbf{TF}: the sum of query term frequencies in the document, and (vi) \textbf{BM25}: the Okapi similarity between the query and the document. 

The results are presented in Figure \ref{fig_rq1_feature_strategies}. A general decreasing trend in the feature values was observed across rounds in almost all cases, except for Entropy in the human competition and Length in the LLM-based competitions. Notably, the query-dependent feature values exhibited greater differences and a steeper decrease in the human competition compared to the LLM-based competitions. This observation suggests that, similar to human participants \cite{raifer_information_2017}, LLM-based agents may adopt a \textit{mimicking-the-winner} strategy to align with winning features.

Further exploration of the \textit{mimicking-the-winner} phenomenon is presented in Figure \ref{label_rq1_winner_sim}, where we analyze the similarity of winners' documents between consecutive rounds. For all three competitions, winners become increasingly similar across rounds. In the human competition, similarity levels are initially lower than the LLM-based competitions; however, they exhibit a steeper increase across rounds. These results are consistent with those presented in Figure \ref{fig_rq1_feature_strategies}, where a decreasing trend in feature value differences was observed. This decrease aligns with an increase in the similarity of winning documents across consecutive rounds.

In Figure \ref{label_rq1_top2_sim} we present the similarity between the top two ranked players' documents. The results indicate that in all three competitions, the top two ranked players become more similar as the competition progresses. Nevertheless, similarity levels remain slightly lower in the human competition. This observation may be attributed to the lower number of rounds in the human competition. Another possible explanation could be that LLM players have a similar configuration in competitions, leading them to generate highly similar documents. Recall that the only difference between LLM players in a competition was their persona.

We now turn to analyze the \textit{Diversity} class of measures. Figure \ref{label_rq1_group_diam} presents the minimum inter-document similarity in a ranked list over rounds. For all three competitions, as the game progresses, the documents in ranked lists become increasingly similar, leading to a decrease in ranked list diversity. Notably, the level of similarity in the two LLM-based competitions is higher than that observed in \ASRC{} \cite{mordo_search_2025}. Further analysis of the average number of unique documents (Figure \ref{label_rq1_unique}) reveals a general decreasing trend in LLM-based competitions, in contrast to a more stable trend in the human competition. Recall that non-unique documents means the players essentially copied other players' documents. By the end of seven rounds, the \ASRC{}'s dataset \cite{mordo_search_2025} contained an average of 3.8 unique documents per round. In comparison, a slightly lower number of unique documents was observed in the LLM-based competitions, with averages of 3.75 and 3.6 in the competitions involving Pairwise and Listwise agents, respectively.


% Listwise prompts, there is a sharper decline in the number of unique documents compared to the slower decline observed in the competition with Pairwise agents. In contrast, \ASRC{}'s human competition does not exhibit a clear trend in this regard. By the end of seven rounds, \ASRC{} \cite{mordo_search_2025} dataset included 3.8 unique documents (on average) per round, comparing a smaller amount of unique documents observed in the LLM competitions (3.75 and 3.6 in the competitions with the Pairwise and Listwise agents, respectively).

% Extending the analysis of the diversity in ranked lists to more rounds reveals that in the LLM competitions the diversity keep decreasing as the rounds advance. 


The findings just presented, along with the observed \textit{mimicking-the-winner} phenomenon (Figures \ref{fig_rq1_feature_strategies} and \ref{label_rq1_winner_sim}), attest that LLM-based agents tend to generate less diverse documents than human participants. This reduction in content diversity may lead to a sub-optimal user welfare \cite{basat_game_2017} and an increased herding effect, negatively impacting the topical diversity of the resulting corpus \cite{goren_driving_2021}. These effects appear to be more pronounced in LLM-based competitions compared to those involving human players.

Figure \ref{label_rq1_conse_player} illustrates the similarity of players' documents across consecutive rounds. At the beginning of all three competitions, players tended to make more substantial modifications to their documents. In \ASRC{} \cite{mordo_search_2025}, only seven rounds are available, limiting conclusions about longer-term trend of convergence and diversity. In both LLM-based competitions, the similarity of players' documents between consecutive rounds saturates and converges over time, providing empirical evidence that players modify their documents less as the rounds progress.

% Finally, \ref{label_rq1_} analyzes the similarity of consecutive documents produced by players. This analysis confirms that both human players and LLM-based players tend to modify their documents less as the game advances, demonstrating convergence at the individual player level as well.
% We emphasize the importance of using different similarity metrics since they give different insights about the dynamics of the game.

Figure \ref{label_rq1_annotation} shows that the quality and relevance grades are quite different between the LLMs and the human competitions. The quality grades in the human competition were lower than those in the LLM-based competitions, showing an increasing trend over the seven rounds, ranging from 2.05 to 2.15. In contrast, the quality grades in the LLM-based competitions ranged between 2.3 and 2.7, without a clear trend over the rounds. This finding aligns with previous reports indicating that LLMs consistently generate high-quality content in ranking competitions \cite{wu_survey_2024, nachimovsky_ranking-incentivized_2024}. Unlike the quality grades, relevance grades in the LLM competition were slightly lower compared to the human competition. In all three competitions, no clear trend in relevance grades over the rounds was observed.

We now turn to the analysis of the proportion of the wins. Recall from Section \ref{label_subsection_evaluation_measures} that it is computed as the ratio between the actual number of wins by the best-performing players and the expected number of wins, assuming that each player has an equal probability of winning a round (averaged over the games). In \ASRC{}'s dataset \cite{mordo_search_2025}, the resultant proportion of the wins was 2.62, compared to 1.4 observed in both LLM-based competitions. This suggests that in \ASRC{} \cite{mordo_search_2025}, there is often a dominant player, whereas in LLM-based competitions, wins are more evenly shared among players. One possible explanation for this difference is the varying levels of motivation among human players, as individual participants may have different incentives that influence their engagement and performance. Another explanation for the reduced proportion of the wins in the LLM-based competitions is that the LLM-based players have similar characteristics, which leads them to exhibit more uniform behavior.

\newcommand{\figWidth}{1.1in}
\newcommand{\figHeight}{1.1in}
\newcommand{\redSpace}{-.1in}
\begin{figure}[t]
\centering 
\includegraphics[scale=0.35]{figs/RQ1/graphs_comparison.png}
  \begin{tabular}{ccc}
    \multicolumn{3}{c}{\textbf{Query dependent features}} \\
    \includegraphics[width=\figWidth,height=\figHeight]{figs/RQ1/graphs_comparison/plot_cons_winners_feature_values/feature_TF.png} & \hspace*{\redSpace} \includegraphics[width=\figWidth,height=\figHeight]{figs/RQ1/graphs_comparison/plot_cons_winners_feature_values/feature_Okapi.png} & \hspace*{\redSpace} \includegraphics[width=\figWidth,height=\figHeight]{figs/RQ1/graphs_comparison/plot_cons_winners_feature_values/feature_LM.png} \\
    TF & BM25 & LM.DIR \\
    \multicolumn{3}{c}{\textbf{Query independent features}} \\
    \includegraphics[width=\figWidth,height=\figHeight]{figs/RQ1/graphs_comparison/plot_cons_winners_feature_values/feature_LEN.png} & \hspace*{\redSpace} \includegraphics[width=\figWidth,height=\figHeight]{figs/RQ1/graphs_comparison/plot_cons_winners_feature_values/feature_FracStop.png} & \hspace*{\redSpace} \includegraphics[width=\figWidth,height=\figHeight]{figs/RQ1/graphs_comparison/plot_cons_winners_feature_values/feature_ENT.png} \\
    Length & StopwordRatio & Entropy \\
  \end{tabular}
  \caption{\label{fig_rq1_feature_strategies} Average absolute difference of feature values of winner documents in rounds $i$ ($W_i$) and $i+1$ ($W_{i+1}$).}
\end{figure}


\begin{figure}[ht!]
    \centering
    \includegraphics[scale=0.35]{figs/RQ1/graphs_comparison.png}
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \begin{minipage}[t]{0.32\textwidth}
            \includegraphics[width=\linewidth]{figs/RQ1/graphs_comparison/winner_similarity_over_time/tf_idf_similarity-winner_similarity_over_time.png}
        \subcaption{Class: M.}
        \label{label_rq1_winner_sim}
        \end{minipage}
        \hfill
        \begin{minipage}[t]{0.32\textwidth}
            \includegraphics[width=\linewidth]{figs/RQ1/graphs_comparison/plot_first_second_similarity_over_time/1-2-tf_idf_similarity-first_second_similarity_over_time.png}
            \subcaption{Class: M.}
            \label{label_rq1_top2_sim}
        \end{minipage}
        \hfill
        \begin{minipage}[t]{0.32\textwidth}
            \includegraphics[width=\linewidth]{figs/RQ1/graphs_comparison/plot_diameter_and_average_over_time-min/tf_idf_similarity-min_over_time.png}
            \subcaption{Class: D, C.}
            \label{label_rq1_group_diam}
        \end{minipage}
        \begin{minipage}[t]{0.32\textwidth}
            \includegraphics[width=\linewidth]{figs/RQ1/graphs_comparison/average_unique_documents_over_time/average_unique_documents_over_time.png}
            \subcaption{Class: D,C.}
            \label{label_rq1_unique}
        \end{minipage}
        \hfill
        \begin{minipage}[t]{0.32\textwidth}
            \centering
            \includegraphics[width=\linewidth]{figs/RQ1/graphs_comparison/average_of_player_documents_consecutive_rounds/tf_idf_similarity-average_similarity_consecutive_rounds.png}
            \subcaption{Class: D,C.}
            \label{label_rq1_conse_player}
        \end{minipage}
        \hfill
        % \begin{minipage}[c]{0.32\textwidth}
            % \includegraphics[width=\linewidth]{figs/RQ1/graphs_comparison/average_and_diameter_of_player_documents-min/bert_embeddings_similarity-min_player_documents.png}
            % \subcaption{(e) similarity metric: BERT; Class: Diversity, Convergence.}
                % \centering
            %     \includegraphics[width=\linewidth]{figs/RQ1/annotation_comparison/quality.png}
            %     \subcaption{Quality grades.}
            %     \label{label_rq1_quality}
            %     % \vspace{1pt}
            %     \includegraphics[width=\linewidth]{figs/RQ1/annotation_comparison/relevance.png}
            % \subcaption{Relevance grades.}
            % \label{label_rq1_relevance}
        % \end{minipage}
        \begin{minipage}[t]{0.32\textwidth}
            \centering
            \includegraphics[width=\linewidth]{figs/RQ1/annotation_comparison/annotation.png}
            \subcaption{Class: R (top) and Q (bottom).}
            \label{label_rq1_annotation}
        \end{minipage}
    \end{minipage}
    \hfill
    \caption{Comparing LLM-based competitions with a competition with human players \cite{mordo_search_2025} (RQ1): {\LlamaEfiveListwiseFour}, {\LlamaEfivePairwiseFour} and {\ASRC} \cite{mordo_search_2025}. (a) average (over queries) similarity of winners between consecutive rounds ($i, i+1$); (b) the average (over queries) similarity between the two highest ranked documents; (c) the average (over queries) min inter-document similarity in a ranked list over rounds; (d) the number of unique documents over rounds; (e) the average (over queries and players) similarity of players' documents between consecutive rounds ($i, i+1$); (f) the average relevance (top) and quality (bottom) grades over rounds. \ASRC{} \cite{mordo_search_2025} consists of 15 games with seven rounds; The LLM-based competitions consist of 30 games with 30 rounds. We refer to the classes of measures \textit{\underline{M}imicking-the-winner}, \textit{\underline{D}iversity}, \textit{\underline{C}onvergence} and \textit{\underline{R}elevance and \underline{Q}uality} as \textbf{M}, \textbf{D}, \textbf{C}, \textbf{R} and \textbf{Q}, respectively.}
    \label{fig_compare_RQ1}
\end{figure}


\subsection{Results (RQ2): comparing LLM-based agents}\label{section_rq2}
We proceed to compare different LLM-based competitions while keeping the ranking function fixed as the cosine of document and query E5 embeddings, henceforth referred to as the E5 ranking function. To potentially increase the competition dynamics with respect to the four-players competitions that were addressed in RQ1, we increased the number of players in each game to five. We explored variations of the competition with the Listwise prompt\footnote{Recall that Bardas et al. \cite{bardas_prompt-based_2025} identified it as one of the two best-performing \contextualized{} of the prompt.} (\LlamaEfiveListwise{}): (1) changing the \contextualized{} to Pairwise (\LlamaEfivePairwise{}), (2) modifying the \shared{} to instruct the LLM not to copying other players' documents (\LlamaEfiveListwiseNoCopy{}), and (3) replacing Llama with Gemma as the LLM (\GemmaEfiveListwise{}).

% The full list of parameters of every competition appears in Appendix \ref{appendix_dataset_parameterts} Tables \ref{table_data_param} and \ref{table_datasets}.

In the \textit{Mimicking-the-winner} class of measures, both the similarity of winners' documents between consecutive rounds (Figure \ref{label_rq2_winner_sim}) and the similarity between the top two players (Figure \ref{label_rq2_top2_sim}) generally increased over round across all competitions. Notably, the similarity levels for Gemma were consistently higher than those for the other competitions and also leveled off way more quickly. Overall, the Gemma-LLM exhibits a stronger tendency to mimic documents compared to the Llama LLM competitions.

The minimum inter-document similarity over rounds (Figure \ref{label_rq2_group_diam}) is higher in the competition with Gemma-LLM compared to all other three competitions with Llama-based agents. It attests to a lower ranked list diversity in the Gemma-based competition compared to the Llama-based competitions. The analysis of unique documents (Figure \ref{label_rq2_unique}) reveals a decline in the number of unique documents across rounds in all competitions. Notably, the competition involving Gemma players exhibits the steepest decrease, with the proportion of unique documents converging to approximately 50\%. This results in an average of 2.5 unique documents out of a total of 5 in the ranked list. In the competition with the Pairwise prompts, the copying behavior was observed to a smaller extent. Between the two Listwise competitions, the \LlamaEfiveListwiseNoCopy{} competition included more unique documents on average, consistent with the instructions in the \shared{} to avoid copying documents of other players.

As for the \textit{Convergence} class, examination of the similarity of players' documents between consecutive rounds (Figure \ref{label_rq2_conse_player}) reveals that the competition with Gemma-LLM (\GemmaEfiveListwise{}) exhibits an increasing and higher levels of similarity over rounds, in comparison to the Llama-LLM competitions (\LlamaEfiveListwise{}, \LlamaEfivePairwise{} and \LlamaEfiveListwiseNoCopy{}). We conclude that the competition with Gemma-LLM converges more rapidly comparing to the other competitions.
Overall, our findings suggest that the most influential factor that affects the dynamics of the competitions, as reflected by the measures, is the language model of the agents rather than the provided prompts. As for the Llama competitions, a slightly higher levels of \textit{mimicking-the-winner} were observed, with respect to the corresponding measures, when the Listwise prompt was used compared to the Pairwise prompt. This observation may be attributed to structural differences between the prompts: the Pairwise prompt provides a limited context regarding other rankings, whereas the Listwise prompt contains a broader view of competing players. Consequently, this broader context facilitates the adoption of the strategy proven to be optimal \cite{raifer_information_2017}, which involves mimicking winning documents.

The quality grades (Figure \ref{label_rq2_annotation}) in the LLM competitions show a marginal tendency to decrease over rounds. This suggests that the competition dynamics may contribute to a gradual decline in overall corpus quality. Moreover, competitions with the Listwise prompts, specifically \LlamaEfiveListwise{} and \GemmaEfiveListwise{}, included documents with slightly higher quality compared to \LlamaEfivePairwise{} and \LlamaEfiveListwiseNoCopy{}. As for the relevance grades (Figure  \ref{label_rq2_annotation}), the \LlamaEfiveListwise{} competition exhibits marginally higher relevance grades across most rounds compared to other competitions. Specifically, the competition using prompts that explicitly instruct players to avoid copying (\LlamaEfiveListwiseNoCopy{}) appears to yield lower quality and relevance grades across most rounds compared to the competition without such an instruction (\LlamaEfiveListwise{}). A possible explanation is that restricting copying may hinder players from adopting the \textit{mimicking-the-winner} strategy \cite{raifer_information_2017}, which is associated with achieving higher rankings. Since top-ranked documents are generally more relevant, this restriction could negatively impact overall document quality and relevance. 

\begin{figure}[ht!]
    \centering
    \includegraphics[scale=0.35]{figs/RQ2/graphs_comparison.png}
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \begin{minipage}[t]{0.32\textwidth}
            \includegraphics[width=\linewidth]{figs/RQ2/graphs_comparison/winner_similarity_over_time/tf_idf_similarity-winner_similarity_over_time.png}
        \subcaption{Class: M.}
        \label{label_rq2_winner_sim}
        \end{minipage}
        \hfill
        \begin{minipage}[t]{0.32\textwidth}
            \includegraphics[width=\linewidth]{figs/RQ2/graphs_comparison/plot_first_second_similarity_over_time/1-2-tf_idf_similarity-first_second_similarity_over_time.png}
            \subcaption{Class: M.}
            \label{label_rq2_top2_sim}
        \end{minipage}
        \hfill
        \begin{minipage}[t]{0.32\textwidth}
            \includegraphics[width=\linewidth]{figs/RQ2/graphs_comparison/plot_diameter_and_average_over_time-min/tf_idf_similarity-min_over_time.png}
            \subcaption{Class: D, C.}
            \label{label_rq2_group_diam}
        \end{minipage}        
        \begin{minipage}[t]{0.32\textwidth}
            \includegraphics[width=\linewidth]{figs/RQ2/graphs_comparison/average_unique_documents_over_time/average_unique_documents_over_time.png}
            \subcaption{Class: D, C.}
            \label{label_rq2_unique}
        \end{minipage}
        \hfill
        \begin{minipage}[t]{0.32\textwidth}
            \includegraphics[width=\linewidth]{figs/RQ2/graphs_comparison/average_of_player_documents_consecutive_rounds/tf_idf_similarity-average_similarity_consecutive_rounds.png}
            \subcaption{Class: D, C.}
            \label{label_rq2_conse_player}
        \end{minipage}
        \hfill
        % \begin{minipage}[c]{0.32\textwidth}
            % \includegraphics[width=\linewidth]{figs/RQ1/graphs_comparison/average_and_diameter_of_player_documents-min/bert_embeddings_similarity-min_player_documents.png}
            % \subcaption{(e) similarity metric: BERT; Class: Diversity, Convergence.}
        %         \centering
        %         \includegraphics[width=\linewidth]{figs/RQ2/annotation_comparison/quality.png}
        %         \subcaption{Quality grades.}
        %         \label{label_rq2_quality}
        %         % \vspace{1pt}
        %         \includegraphics[width=\linewidth]{figs/RQ2/annotation_comparison/relevance.png}
        %     \subcaption{Relevance grades.}
        %     \label{label_rq2_relevance}
        % \end{minipage}
        \begin{minipage}[t]{0.32\textwidth}
            \centering
            \includegraphics[width=\linewidth]{figs/RQ2/annotation_comparison/annotation.png}
            \subcaption{Class: R (top) and Q (bottom).}
            \label{label_rq2_annotation}
        \end{minipage}
        \hfill
    \end{minipage}
    \hfill
    \caption{Comparison of LLM-based agents (RQ2): \LlamaEfiveListwise{}, {\LlamaEfivePairwise{}}, {\LlamaEfiveListwiseNoCopy} and {\GemmaEfiveListwise{}}. (a) average (over queries) similarity of winners between consecutive rounds ($i, i+1$); (b) the average (over queries) similarity between the two highest ranked documents; (c) the average (over queries) min inter-document similarity in a ranked list over rounds; (d) the number of unique documents over rounds; (e) the average (over queries and players) similarity of players' documents between consecutive rounds ($i, i+1$); (f) the average relevance (top) and quality (bottom) grades over rounds. Each competition consists of 30 games with 30 rounds. We refer to the classes of measures \textit{\underline{M}imicking-the-winner}, \textit{\underline{D}iversity}, \textit{\underline{C}onvergence} and \textit{\underline{R}elevance and \underline{Q}uality} as \textbf{M}, \textbf{D}, \textbf{C}, \textbf{R} and \textbf{Q}, respectively.}
    \label{fig_compare_RQ2}
\end{figure}

\subsection{Results (RQ3): Llama agents with different rankers
}\label{section_rq3}

We next turn to study the effect of the ranking function on the competition. To this end, we fix the LLM to Llama3.1, and study the ranking functions: the cosine of document and
query E5 \cite{wang_text_2024} embeddings (refer to as E5 ranking function), the cosine of document and
query Contriever embeddings \cite{izacard_unsupervised_2022} (refer to as Contriever ranking function), and Okapi \cite{maron_relevance_1960}. We used agents with both Pairwise and Listwise prompts.

The average similarity between winners (Figure \ref{label_rq3_winner_sim}) was similar across the ranking functions. At the end of the competition with the Okapi ranking function and players with the Listwise prompt, an higher similarity with respect to consecutive winners was observed, compared to other competitions. Additionally, the average similarity between the top two players (Figure \ref{label_rq3_top2_sim}) increased over rounds. For all ranking functions, the choice of ranking function did not substantially affect both measures of the \textit{Mimicking-the-winner} class. As discussed in Section \ref{section_rq2} and illustrated in Figure \ref{label_rq3_top2_sim}, the type of prompt (Listwise vs. Pairwise) has a significantly greater impact than the ranking function.

As for the diversity at the ranked-list-level, measured by both the minimum inter-document similarity in the ranked list and the number of unique documents, we observe the same trend of being unaffected by the ranking function (Figures \ref{label_rq3_group_diam} and \ref{label_rq3_unique}). We can see two distinct "clusters" for both the Listwise and Pairwise LLM-based competitions, with minimal impact with respect to the choice of the ranking function. The convergence and diversity at the player level measured by the similarity of players' documents between consecutive rounds (Figure \ref{label_rq3_conse_player}), demonstrated an increasing trend (over rounds) across all the competitions. Once again, the differences observed across ranking functions remain minimal. However, in the competition with the Okapi ranking function and agents with the Listwise prompt, slightly elevated similarity levels were observed in the final rounds compared to all other competitions.

The quality and relevance grades were similar across all the competitions. (See Figure \ref{label_rq3_annotation}.) The quality grades across all competitions (excluding those with Contriever ranking functions, which were not annotated) exhibit a general tendency to decline over the rounds. Interestingly, the quality grades of \LlamaEfiveListwise{} were slightly higher compared to all other competitions. For relevance grades, the values consistently exceed 1.9 (out of possible 3) in most rounds, with no significant influence observed by the choice of ranking function.

Overall, the ranking function appears to have minimal to no significant impact on the competition dynamics with respect to the analyzed measures. We showed above that the type of LLM and the prompt have a much larger effect.

% The difference between listwise and pairwise prompts is further evident in group-level diversity, as indicated by the group diameter similarity (measured using SBERT) and the average similarity between consecutive documents (measured using TF.IDF). Using both lexical and semantic similarity metrics, we found that listwise prompts consistently resulted in lower diversity levels compared to pairwise prompts across all three ranking functions. This conclusion is also supported by the analysis of unique documents, which showed that competitions using listwise prompts produced fewer unique documents than those using pairwise prompts, regardless of the ranking function.

% The only metric where the ranking function noticeably influenced competition dynamics was the diameter similarity of individual players' documents when similarity was measured using BERT. While other similarity metrics produced slightly different results, the distinction between listwise and pairwise prompts remained evident across all ranking functions.\ref{label_rq1_winner_sim}

\begin{figure}[ht!]
    \centering
    \includegraphics[scale=0.35]{figs/RQ3/graphs_comparison.png}
    \begin{minipage}[t]{0.47\textwidth}
        \centering
        \begin{minipage}[t]{0.32\textwidth}
            \includegraphics[width=\linewidth]{figs/RQ3/graphs_comparison/winner_similarity_over_time/tf_idf_similarity-winner_similarity_over_time.png}
        \subcaption{Class: M.}
        \label{label_rq3_winner_sim}
        \end{minipage}
        \hfill
        \begin{minipage}[t]{0.32\textwidth}
            \includegraphics[width=\linewidth]{figs/RQ3/graphs_comparison/plot_first_second_similarity_over_time/1-2-tf_idf_similarity-first_second_similarity_over_time.png}
            \subcaption{Class: M.}
            \label{label_rq3_top2_sim}
        \end{minipage}
        \hfill
        \begin{minipage}[t]{0.32\textwidth}
            \includegraphics[width=\linewidth]{figs/RQ3/graphs_comparison/plot_diameter_and_average_over_time-min/tf_idf_similarity-min_over_time.png}
            \subcaption{Class: D, C.}
            \label{label_rq3_group_diam}
        \end{minipage}
        % Stop Cover
        \begin{minipage}[t]{0.32\textwidth}
            \includegraphics[width=\linewidth]{figs/RQ3/graphs_comparison/average_unique_documents_over_time/average_unique_documents_over_time.png}
            \subcaption{Class: D, C.}
            \label{label_rq3_unique}
        \end{minipage}
        \hfill
        \begin{minipage}[t]{0.32\textwidth}
            \includegraphics[width=\linewidth]{figs/RQ3/graphs_comparison/average_of_player_documents_consecutive_rounds/tf_idf_similarity-average_similarity_consecutive_rounds.png}
            \subcaption{Class: D, C.}
            \label{label_rq3_conse_player}
        \end{minipage}
        \hfill
        % \begin{minipage}[c]{0.32\textwidth}
            % \includegraphics[width=\linewidth]{figs/RQ1/graphs_comparison/average_and_diameter_of_player_documents-min/bert_embeddings_similarity-min_player_documents.png}
            % \subcaption{(e) similarity metric: BERT; Class: Diversity, Convergence.}
        %         \centering
        %         \includegraphics[width=\linewidth]{figs/RQ3/annotation_comparison/quality.png}
        %         \subcaption{Quality grades.}
        %         \label{label_rq3_quality}
        %         % \vspace{1pt}
        %         \includegraphics[width=\linewidth]{figs/RQ3/annotation_comparison/relevance.png}
        %         \subcaption{(g) Relevance grades.}
        %         \label{label_rq3_relevance}
        % \end{minipage}
        \begin{minipage}[t]{0.32\textwidth}
            \includegraphics[width=\linewidth]{figs/RQ3/annotation_comparison/annotation.png}
            \subcaption{Class: R (top) and Q (bottom).}
            \label{label_rq3_annotation}
        \end{minipage}
    \end{minipage}
    \hfill
    \caption{Comparison of ranking functions in LLM-based competitions (RQ3): \LlamaEfiveListwise{}, \LlamaEfivePairwise{}, \LlamaContListwise{}, \LlamaContPairwise{}, \LlamaOkapiListwise{}, and \LlamaOkapiPairwise{}. (a) average (over queries) similarity of winners between consecutive rounds ($i, i+1$); (b) the average (over queries) similarity between the two highest ranked documents; (c) the average (over queries) min inter-document similarity in a ranked list over rounds; (d) number of unique documents over rounds; (e) the average (over queries and players) similarity of players' documents between consecutive rounds ($i, i+1$); (f) the average relevance (top) and quality (bottom) grades over rounds. Each competition consists of 30 games with 30 rounds. We refer to the classes of measures \textit{\underline{M}imicking-the-winner}, \textit{\underline{D}iversity}, \textit{\underline{C}onvergence} and \textit{\underline{R}elevance and \underline{Q}uality} as \textbf{M}, \textbf{D}, \textbf{C}, \textbf{R} and \textbf{Q}, respectively.}
    \label{fig_compare_RQ3}
\end{figure}