\section{Introduction}
Ranking incentives can drive corpus dynamics in competitive search
settings \cite{kurland_competitive_2022}: document authors
(publishers) might respond to rankings induced for queries of interest
by modifying their documents. The goal of the modification is to
improve future ranking. A case in point, in Web search, the documents most highly ranked often attract most clicks \cite{Joachims+al:05a}. Hence, for queries of commercial intent, high ranks are of utmost importance.


This practice of modifying documents to improve their future ranking
is often referred to as search engine optimization (SEO)
\cite{Gyongyi+Molina:05a}. Competitive search
\cite{kurland_competitive_2022}, which is our focus in this paper, refers
to white hat SEO: legitimate modifications that do not hurt content
quality and/or the search ecosystem.

There are a few recent studies of competitive search. Ben Basat et
al. \cite{Basat+al:17a} showed using game theoretic analysis that the
probability ranking principle (PRP) \cite{Robertson:77a} is not
optimal in competitive search: it leads to reduced topical diversity
in the long run. The implication is that the static view of a
corpus in most work on ad hoc retrieval falls short given the dynamics
driven by ranking incentives. Raifer et
al. \cite{raifer_information_2017} showed using theoretical and
empirical analysis that a prevalent strategy of publishers which leads to an equilibrium is to mimic
content in the documents most highly ranked in the past. Indeed, previous rankings are the only signal about the undisclosed ranking function.
Goren et
al. \cite{goren_driving_2021} empirically showed that this strategy
leads to a publisher herding effect with potentially unwarranted implications; e.g., reducing the volume of content relevant to a query in the corpus.

Performing empirical studies of competitive search is an extremely
difficult challenge \cite{kurland_competitive_2022}. For example, in
the Web setting, multiple factors can affect the changes of Web pages,
many of which may not be due to ranking incentives. Hence, while there
are observational studies of Web dynamics (e.g.,
\cite{Radinski+al:13a}), analyzing ranking-incentivized publisher
strategies and corpus effects remains an open question. This challenge
drove forward a new type of empirical analysis in recent work on
competitive search: controlled ranking competitions were held between
students
\cite{raifer_information_2017,goren_ranking-incentivized_2020,goren_driving_2021,nachimovsky_ranking-incentivized_2024}. The
students modified documents to have them highly ranked for queries
they were assigned to. They competed over a few rounds, in each of
which they were shown the last ranking induced by the undisclosed
ranking function. The resultant datasets were used for various
analyses
\cite{raifer_information_2017, goren_ranking-incentivized_2020, goren_driving_2021,Wu+al:23a,nachimovsky_ranking-incentivized_2024}.

Ranking competitions with humans acting as publishers are valuable for
offline analysis of the {\em specific} competitions that took place. However,
studying new retrieval methods and/or publishers' document
modification strategies, which significantly affect the competition, is
practically impossible: each new design choice calls for re-running the
competition. Kurland and Tennenholtz \cite{kurland_competitive_2022} mentioned this challenge as a potential barrier to empirical study of competitive search and suggested to run ranking competitions using automated agents. Their call of arms, together with the increasing proliferation over the Web of generated AI content, is the motivation for the work we report here.

We present a platform dubbed \platformName: a multi-agent platform
that uses large language models (LLMs) as publishers (agents) in
ranking competitions. The platform allows the execution of large-scale
and highly varied ranking competitions where various factors can
be controlled: the query for which the competition is held, the
ranking function, the LLM and its prompt. The platform also provides a
competition Analyzer module that analyzes a single competition using
various measures. In addition, the platform includes a Compare module that
allows to compare different competitions; e.g., competitions run with different LLMs or different rankers. The \platformName platform will be made publicly
available upon acceptance of this paper, and is currently available
for reviewing purposes at
{\url{https://github.com/csp-platform/Simulator}.}

To demonstrate the merits of \platformName, we used it to run many
ranking competitions for sets of queries where we varied the LLM and
its prompt and the ranking function. We present analysis of the
resultant competition datasets using the Analyzer and Compare modules. For example, we
compare the corpus dynamics with that reported in past human-based
ranking competitions and reveal that LLM-based agents reduce content diversity in the corpus to a larger extent than humans. Furthermore, the analysis of the competitions shows that the ranking function has less effect on the
dynamics than the choice of the LLM which serves as a publisher.
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figs/platform/ecosystem.jpg}
%     \caption{Components of the \textsc{\platformName} framework: (1) \textsc{Platform} simulates a ranking competition; (2) \textsc{Analyzer} analyzes a single competition; and (3) \textsc{Compare} performs comparisons across multiple competitions.}
%     \label{fig_lemss_ecosystem}
% \end{figure}
% motivation:
% \begin{itemize}
%     \item help IR designers to consider different aspects of a IR system
%     \item how llms compete in IR ecosystems
% \item simulate how LLMs behave in competitive search settings.
% \end{itemize}
