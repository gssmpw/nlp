\section{Creating Datasets Using \platformName}\label{section_data}

We created 22 datasets using the {\platformName} {\platform}, each produced through a simulation representing a distinct configuration of a ranking competition among LLMs.
We analyzed the datasets using the {\platformName} {\analyzer} and the {\platformName} {\compare} modules to demonstrate an analysis of a single competition and comparison of multiple competitions. (The analysis is presented in Section \ref{section_analysis}.)
Additional objective of collecting data from LLM-based competitions is to compare the properties of ranking competitions conducted among LLMs with those involving human players. To this end, we utilized the dataset of a ranking competition conducted between students, organized by Mordo et al. \cite{mordo_search_2025}. The competition included 15 games, each for a different query selected from TREC09-TREC12. Every game included four students and lasted for seven rounds. The ranking function was the cosine between a query and document embedding induced using E5 \cite{wang_text_2024}. The competition was approved by ethics committees \cite{mordo_search_2025}.

% A penalty mechanism was implemented to encourage players to actively modify their documents and to discourage copying of other documents.

% The dataset includes: (1) the documents of the competition; (2) the ranking of every document; (3) the relevance judgments; and (4) the quality judgments.

\subsection{The resultant datasets of LLM-based competitions}
The parametrization of the {\platformName} {\platform} enables a diverse and flexible range of options for simulating ranking competitions and generating their corresponding datasets. Our goal in this paper is to demonstrate {\platformName}' capabilities and not to find the best performing agents. We adopt a specific class of prompt-based agents developed by Bardas et al. \cite{bardas_prompt-based_2025}. Note that other types of LLMs (for instance, fine-tuned LLMs) can be integrated in the {\platformName} {\platform}. The proposed agents are LLMs that operate based on instructions provided through prompts. Each agent is defined by a specific LLM model and a prompt given each round, that guides its behavior in the ranking game. Each prompt has two parts: a general shared part which describes the task (denoted \textit{\shared}) and a context-specific part, specific to a prompt, which provides information about past rankings (denoted \textit{\contextualized}).
 % Two components: (1) the foundational model of the LLM, along with its specific parameters (e.g., Llama3.1 \cite{dubey_llama_2024} with temperature=0.5, $top_p = 0.9$, etc); (2) The prompts used to instruct the LLM.
 % Two types of prompts were discussed in Bardas et al.\cite{bardas_prompt-based_2025}: \textit{System prompt} and \textit{User prompt}. 

The \shared{} includes the general background for the game, which outlines the game's rules and restrictions, the player's current document, and the assigned query. Additionally, the \shared{} instructs players to maintain similarity to their original document as a guideline to ensure faithfulness to the original content.
% For more details see Figure \ref{prompt_system} in Appendix \ref{appendix_prompts}.
% In competitions conducted among students \cite{raifer_information_2017, nachimovsky_ranking-incentivized_2024, mordo_search_2025}, a behavior of mimicking the winner was observed, where authors of lower-ranked documents copied content from top-ranked documents. This behavior negatively impacted the dynamics of the game, leading to a reduction in topical diversity \cite{goren_driving_2021}.
% competition organizers implemented penalty mechanisms, such as reducing the bonus points students could earn for participating. Since our setting involves LLMs as players,
To mitigate the potential behavior of \textit{mimicking-the-winner} (mimicking content from the top-ranked documents) as observed in competitions between students \cite{raifer_information_2017}, we introduced a modified \shared{}. It is building upon the version presented by Bardas et al. \cite{bardas_prompt-based_2025}, with an additional instruction explicitly designed to discourage general copying of content during the game. This mitigation is crucial, as the long-term effect of \textit{mimicking-the-winner} can lead to a herding effect, ultimately reducing topical diversity in the corpus and limiting the competitive dynamics of the game \cite{goren_driving_2021}.
% (see Figure \ref{prompt_system_no_copy} in Appendix \ref{appendix_prompts}).

The \contextualized{} of the prompt includes information about past rankings, to serve as a signal about the ranking function. (Recall that the ranking function is not disclosed.) It guides the LLM's actions in subsequent rounds to try to achieve the highest ranking. Bardas et al. \cite{bardas_prompt-based_2025} conducted a series of experiments aimed to find the optimal parametrization of the \contextualized s which maximizes ranking promotion. We use the two best performing \contextualized s reported by Bardas et al. \cite{bardas_prompt-based_2025}. These prompts led to the highest ranking promotion for the modified documents: Pairwise prompt and Listwise prompt. The Pairwise prompt contains pairs of randomly selected documents and their rank preference by the ranking function, from the last three rounds. The Listwise prompt contains ranked lists from the two last rounds.


We demonstrate the platform's capabilities using a competition among LLMs.
% A summary of the parameters defining the configuration of each competition and the resultant datasets appear in Appendix \ref{appendix_dataset_parameterts} Tables \ref{table_data_param} and \ref{table_datasets}.
Each competition consisted of 30 games, with each game held for a query with a commercial intent, selected from the TREC09â€“TREC12 datasets\footnote{We used the following queries: 9,17,21,29,34,45,48,55,59,61,64,69,74,75,78,83,96,98,124,144,\\ 161,164,166,167,170,180,182,193,194,195 from \url{https://trec.nist.gov/data/webmain.html}.}. The initial documents, those provided to the players at the beginning of \ASRC's dataset \cite{mordo_search_2025}.

Each game was played for 30 rounds. We used different lightweight instruct-tuned (<10B parameters) language models as agents: Llama 3.1 \cite{dubey_llama_2024}, Gemma2 \cite{gemma_team_gemma_nodate}, Qwen2.5 \cite{qwen_qwen25_2024} and Ministral\footnote{meta-llama/Meta-Llama-3.1-8B-Instruct, google/gemma-2-9b-it, Qwen/Qwen2.5-7B-Instruct and mlx-community/Ministral-8B-Instruct-2410-bf16 from Hugging face repository.} \cite{jiang_mistral_2023}.
For demonstration purposes, we use Llama3.1 and Gemma2 in the analysis presented in Section \ref{section_analysis}, and prompt them with the Listwise and Pairwise prompts. Due to computational limitations, each competition utilized only a single language model and \contextualized{}, with the same commonly used hyper-parameters \cite{li_examining_2024}: a $top_p$\footnote{Controls the probability mass from which the model samples its next token, ensuring that only the most probable tokens, comprising 90\% of the cumulative distribution, are considered.} value of 0.9 and a temperature\footnote{Controls the randomness of language model output.} of 0.5, enabling LLMs to generate diverse content while prioritizing the most probable tokens. To introduce variation among competing players, each player was assigned a persona \cite{shapira_can_2024,samuel_personagym_2024}. This approach aimed to induce diverse behavioral patterns among the players in the same game. The personas were generated as follows: we selected the Educational environment from Samuel et al. \cite{samuel_personagym_2024} to align with the topic interests of students, given that previous ranking competitions were conducted between student. Then, we used GPT-4o to generate five personas using the prompt provided by Samuel et al. \cite{samuel_personagym_2024} with an additional general description of ranking competitions. The resulting personas included a BSc student, a professional writer, a professional editor, an English teacher, and a Data Science professor.

Four or five players competed against each other in each game. Three distinct unsupervised ranking functions were applied. Two dense retrieval methods and one sparse method: E5 \cite{wang_text_2024}, Contriever\footnote{intfloat/e5-large-unsupervised and facebook/contriever from Hugging face repository.} \cite{izacard_unsupervised_2022} and Okapi BM25 \cite{maron_relevance_1960}. To compute the score in dense retrieval methods for a given document-query pair, we first generated the embedding vectors for both the document and the query using the embedder model, then computed the cosine similarity between these vectors. For extracting IDF (inverse document frequency) based features in Okapi, we used the English Wikipedia dataset with 59k pages, from a 2020 dump (Krovetz stemmed) \cite{Frej2020Wikir,Frej2020MlWikir}.

The LLMs sometimes generate text that exceeds the word limit allowed in the \shared{} or includes unnecessary headers and prefixes\footnote{For instance: "The modified document is:" or "Here is the document text:".}. To prevent exceeding token limits, we truncated the generated documents to the first 256 tokens\footnote{To align with competitions conducted between students \cite{raifer_information_2017, nachimovsky_ranking-incentivized_2024, mordo_search_2025}.}. To address the presence of unwarranted tokens, we incorporated a post-edit step in which players utilize their own LLM with a targeted cleaning prompt. This prompt directs the LLM to remove any headers or prefixes from the generated document.

% , ensuring that only the main part of the document is extracted prior to submission to the ranker.

% The prompt used for this cleaning process is detailed in Figure \ref{prompt_clean} in Appendix \ref{appendix_prompts}.

The competitions with five players per game resulted in 4500 documents, and those with four players per game yielded 3600 documents.
% Recall, the goal of this research is not to identify the optimal agents that achieve the highest ranking, as explored in Bardas et al. \cite{bardas_prompt-based_2025}. Instead, we focus on demonstrating the platform's functionality, leaving the exploration of optimization objectives for future work.