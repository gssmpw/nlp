\section{Related Work}
Zero-Input AI (ZIA) sits at the intersection of multi-modal machine learning, brain-computer interfaces (BCIs), reinforcement learning (RL), and edge computing, drawing on and extending a rich body of prior research. This section surveys these fields, highlighting their contributions and limitations relative to ZIA's goal of proactive, input-free intent prediction with real-time, edge-optimized execution.

\subsection{Multi-Modal Machine Learning}
Multi-modal learning has gained prominence for integrating heterogeneous data sources into unified representations, a cornerstone of ZIA's signal fusion pipeline. CLIP ____ pioneered contrastive learning to align text and image embeddings, achieving robust zero-shot classification by maximizing mutual information between modalities. Similarly, ImageBind ____ extends this paradigm to six modalities (text, images, audio, depth, thermal, IMU), using a shared embedding space to enable cross-modal retrieval. These models excel in static, task-specific settings but lack temporal reasoning critical for intent prediction over time-series signals like gaze or EEG. 

Earlier efforts, such as the Multi-Modal Transformer ____, introduced cross-modal attention to fuse audio and visual inputs for emotion recognition, demonstrating improved performance over uni-modal baselines. However, these approaches assume pre-aligned inputs and predefined tasks, whereas ZIA targets dynamic, unprompted intent inference from unaligned physiological and contextual cues.

Other frameworks, like Data2Vec ____, leverage self-supervised learning across text, speech, and images, predicting latent representations to unify modalities. While versatile, they prioritize generalization over real-time constraints, with inference latencies often exceeding hundreds of milliseconds—far beyond ZIA's 100 ms target. The lack of proactive intent modeling in these works underscores a key gap: multi-modal AI remains reactive, awaiting user-defined queries rather than anticipating needs from passive signals.

\subsection{Brain-Computer Interfaces (BCIs)}
BCIs provide a direct precedent for intent prediction from bio-signals, particularly EEG, a core modality in ZIA. Seminal work by ____ established EEG-based control for cursor movement, achieving binary intent classification (e.g., left vs. right) with accuracies of 80-90\% in controlled settings. More recent advances, such as DeepConvNet ____, apply convolutional neural networks to raw EEG, improving motor imagery classification to $\sim$85\% across four classes. These systems rely on explicit user training and predefined task sets, contrasting with ZIA's goal of unsupervised, free-form intent detection.

Non-invasive BCIs have also explored event-related potentials (ERPs) for intent, with ____ decoding P300 signals for spelling tasks at $\sim$80\% accuracy. However, ERP-based methods require stimulus-driven paradigms (e.g., visual cues), limiting their applicability to zero-input contexts where no external trigger exists. Invasive approaches, like Neuralink's primate studies ____, achieve higher precision via implanted electrodes, but their impracticality for consumer use drives ZIA toward scalable, non-invasive solutions. BCI research excels at signal-specific intent but lacks multi-modal integration and edge deployment, key differentiators for ZIA.

\subsection{Reinforcement Learning (RL)}
RL offers a foundation for ZIA's adaptive intent prediction. Proximal Policy Optimization (PPO) ____ provides a stable, sample-efficient policy gradient method, widely adopted in robotics and gaming (e.g., OpenAI's Dota 2 agent), balancing exploration and exploitation via clipped objectives. Deep Deterministic Policy Gradient (DDPG) ____ extends RL to continuous action spaces, applied in autonomous driving ____. These methods optimize policies against well-defined rewards, but ZIA adapts them to sparse, implicit feedback (e.g., intent overrides), a less-explored domain.

Human-in-the-loop RL, as in ____, incorporates user preferences to refine policies, achieving robust alignment in text generation tasks. Yet, these approaches assume active user input, unlike ZIA's passive signal reliance. Adaptive BCIs ____ use RL to tune EEG classifiers but focus on single modalities without ZIA's multi-modal complexity. RL's strength in dynamic adaptation is tempered by its computational overhead, motivating ZIA's edge optimization focus.

\subsection{Edge AI and Real-Time Systems}
Edge computing is critical for ZIA's low-latency, privacy-preserving goals. ____ survey edge AI, reporting inference times of 50–100 ms for vision models on mobile GPUs, leveraging quantization and pruning. MobileNet ____ optimizes convolutional networks for edge devices, achieving $\sim$70\% accuracy on ImageNet with $\sim$100\,\text{ms} latency on smartphones. More recently, EfficientNet ____ scales model depth efficiently, but its complexity exceeds ZIA's real-time needs without further compression.



Real-time systems like YOLOv4 ____ push object detection to 30-50 ms on edge hardware, using techniques like FP16 quantization—paralleling ZIA's approach. However, these models process single-modal inputs (e.g., images), not ZIA's multi-modal streams. Edge-optimized transformers, such as Linformer ____ and Performer ____, reduce attention complexity to \(\mathcal{O}(N)\), aligning with ZIA's linear attention strategy, though they target language or vision, not physiological signals. Edge AI excels in latency but lacks intent-driven, multi-modal frameworks.

\subsection{Critical Gaps and ZIA's Position}
Across these fields, several gaps emerge:
\begin{itemize}
    \item Multi-modal ML lacks temporal intent modeling and proactive inference, focusing on static tasks.
    \item BCIs excel at bio-signal intent but require explicit triggers or training, missing ZIA's zero-input ethos.
    \item RL adapts policies dynamically but rarely handles multi-modal, passive inputs under edge constraints.
    \item Edge AI achieves real-time performance but prioritizes single-modal efficiency over intent prediction.
\end{itemize}

ZIA uniquely bridges these domains, integrating multi-modal fusion, uncertainty-aware inference, adaptive learning, and edge optimization into a cohesive, theoretical framework for proactive HCI—a synthesis not addressed in prior work.