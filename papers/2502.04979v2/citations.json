[
  {
    "index": 0,
    "papers": [
      {
        "key": "pmlr-v235-chen24e",
        "author": "Chen, Lichang and Chen, Jiuhai and Goldstein, Tom and Huang, Heng and Zhou, Tianyi",
        "title": "{I}nstruct{Z}ero: Efficient Instruction Optimization for Black-Box Large Language Models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "lin2023use",
        "author": "Lin, Xiaoqiang and Wu, Zhaoxuan and Dai, Zhongxiang and Hu, Wenyang and Shu, Yao and Ng, See-Kiong and Jaillet, Patrick and Low, Bryan Kian Hsiang",
        "title": "Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled with Transformers"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "shi2024best",
        "author": "Shi, Chengshuai and Yang, Kun and Yang, Jing and Shen, Cong",
        "title": "Best arm identification for prompt learning under a limited budget"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "liu2021makes",
        "author": "Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, Bill and Carin, Lawrence and Chen, Weizhu",
        "title": "What Makes Good In-Context Examples for GPT-$3 $?"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "mishra2021natural",
        "author": "Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Hajishirzi, Hannaneh",
        "title": "Natural instructions: Benchmarking generalization to new tasks from natural language instructions"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "kumar2021reordering",
        "author": "Kumar, Sawan and Talukdar, Partha",
        "title": "Reordering examples helps during priming-based few-shot learning"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "lu2021fantastically",
        "author": "Lu, Yao and Bartolo, Max and Moore, Alastair and Riedel, Sebastian and Stenetorp, Pontus",
        "title": "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "hu2023prompt",
        "author": "Hu, Shengchao and Shen, Li and Zhang, Ya and Tao, Dacheng",
        "title": "Prompt-tuning decision transformer with preference ranking"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "hu2024prompt",
        "author": "Hu, Shengchao and Zhao, Wanru and Lin, Weixiong and Shen, Li and Zhang, Ya and Tao, Dacheng",
        "title": "Prompt Tuning with Diffusion for Few-Shot Pre-trained Policy Generalization"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "wang2024hierarchical",
        "author": "Wang, Zhe and Wang, Haozhu and Qi, Yanjun",
        "title": "Hierarchical Prompt Decision Transformer: Improving Few-Shot Policy Generalization with Global and Adaptive"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "xu2023hyper",
        "author": "Xu, Mengdi and Lu, Yuchen and Shen, Yikang and Zhang, Shun and Zhao, Ding and Gan, Chuang",
        "title": "Hyper-decision transformer for efficient online policy adaptation"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "yang2024pre",
        "author": "Yang, Yu and Xu, Pan",
        "title": "Pre-trained Language Models Improve the Few-shot Prompt Ability of Decision Transformer"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "shi2024unleashing",
        "author": "Shi, Ruizhe and Liu, Yuyao and Ze, Yanjie and Du, Simon Shaolei and Xu, Huazhe",
        "title": "Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "zhengdecomposed",
        "author": "Zheng, Hongling and Shen, Li and Luo, Yong and Liu, Tongliang and Shen, Jialie and Tao, Dacheng",
        "title": "Decomposed Prompt Decision Transformer for Efficient Unseen Task Generalization"
      }
    ]
  }
]