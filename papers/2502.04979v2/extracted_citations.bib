@article{hu2023prompt,
  title={Prompt-tuning decision transformer with preference ranking},
  author={Hu, Shengchao and Shen, Li and Zhang, Ya and Tao, Dacheng},
  journal={arXiv preprint arXiv:2305.09648},
  year={2023}
}

@article{hu2024prompt,
  title={Prompt Tuning with Diffusion for Few-Shot Pre-trained Policy Generalization},
  author={Hu, Shengchao and Zhao, Wanru and Lin, Weixiong and Shen, Li and Zhang, Ya and Tao, Dacheng},
  journal={arXiv preprint arXiv:2411.01168},
  year={2024}
}

@article{kumar2021reordering,
  title={Reordering examples helps during priming-based few-shot learning},
  author={Kumar, Sawan and Talukdar, Partha},
  journal={arXiv preprint arXiv:2106.01751},
  year={2021}
}

@inproceedings{lin2023use,
  title={Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled with Transformers},
  author={Lin, Xiaoqiang and Wu, Zhaoxuan and Dai, Zhongxiang and Hu, Wenyang and Shu, Yao and Ng, See-Kiong and Jaillet, Patrick and Low, Bryan Kian Hsiang},
  booktitle={NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following},
  year={2023}
}

@article{liu2021makes,
  title={What Makes Good In-Context Examples for GPT-$3 $?},
  author={Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, Bill and Carin, Lawrence and Chen, Weizhu},
  journal={arXiv preprint arXiv:2101.06804},
  year={2021}
}

@article{lu2021fantastically,
  title={Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity},
  author={Lu, Yao and Bartolo, Max and Moore, Alastair and Riedel, Sebastian and Stenetorp, Pontus},
  journal={arXiv preprint arXiv:2104.08786},
  year={2021}
}

@article{mishra2021natural,
  title={Natural instructions: Benchmarking generalization to new tasks from natural language instructions},
  author={Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2104.08773},
  pages={839--849},
  year={2021}
}

@InProceedings{pmlr-v235-chen24e,
  title = 	 {{I}nstruct{Z}ero: Efficient Instruction Optimization for Black-Box Large Language Models},
  author =       {Chen, Lichang and Chen, Jiuhai and Goldstein, Tom and Huang, Heng and Zhou, Tianyi},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {6503--6518},
  year = 	 {2024},
  volume = 	 {235}
}

@inproceedings{shi2024best,
  title={Best arm identification for prompt learning under a limited budget},
  author={Shi, Chengshuai and Yang, Kun and Yang, Jing and Shen, Cong},
  booktitle={ICLR 2024 Workshop on Understanding of Foundation Model},
  year={2024}
}

@inproceedings{shi2024unleashing,
  title={Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning},
  author={Shi, Ruizhe and Liu, Yuyao and Ze, Yanjie and Du, Simon Shaolei and Xu, Huazhe},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{wang2024hierarchical,
  title={Hierarchical Prompt Decision Transformer: Improving Few-Shot Policy Generalization with Global and Adaptive},
  author={Wang, Zhe and Wang, Haozhu and Qi, Yanjun},
  journal={arXiv preprint arXiv:2412.00979},
  year={2024}
}

@article{xu2023hyper,
  title={Hyper-decision transformer for efficient online policy adaptation},
  author={Xu, Mengdi and Lu, Yuchen and Shen, Yikang and Zhang, Shun and Zhao, Ding and Gan, Chuang},
  year={2023},
  booktitle={The Eleventh International Conference on Learning Representations}
}

@inproceedings{yang2024pre,
  title={Pre-trained Language Models Improve the Few-shot Prompt Ability of Decision Transformer},
  author={Yang, Yu and Xu, Pan},
  booktitle={Workshop on Training Agents with Foundation Models at RLC 2024},
  year={2024}
}

@inproceedings{zhengdecomposed,
  title={Decomposed Prompt Decision Transformer for Efficient Unseen Task Generalization},
  author={Zheng, Hongling and Shen, Li and Luo, Yong and Liu, Tongliang and Shen, Jialie and Tao, Dacheng},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

