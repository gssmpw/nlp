\documentclass{article} % For LaTeX2e
\usepackage[accepted]{icml2021}
% \usepackage{icml2021}

%% for arXiv version
% \usepackage{arxiv}
% \usepackage{natbib}


\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{amsfonts}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{lipsum}

\usepackage{url}

\usepackage{makecell}

\input{math_commands.tex}

%% for arXiv version
\title{Enhancing Pre-Trained Decision Transformers with Prompt-Tuning Bandits}

\author{Finn Rietz\thanks{Work performed while the author was an intern at King (part of Microsoft).} \\
\"Orebro University\\
\texttt{finn.rietz@oru.se} \\
\And
Oleg Smirnov \& Sara Karimi \& Lele Cao\\
Microsoft Gaming\\
\texttt{\{oleg.smirnov,sarakarimi,lelecao\}@microsoft.com}
}

\begin{document}

\twocolumn[

\icmltitle{Enhancing Pre-Trained Decision Transformers with Prompt-Tuning Bandits}

\begin{icmlauthorlist}
\icmlauthor{Finn Rietz}{orebro,msft}
\icmlauthor{Oleg Smirnov}{msft}
\icmlauthor{Sara Karimi}{msft}
\icmlauthor{Lele Cao}{msft}
\end{icmlauthorlist}

\icmlaffiliation{orebro}{\"Orebro University}
\icmlaffiliation{msft}{Microsoft Gaming}

\icmlcorrespondingauthor{Finn Rietz}{finn.rietz@oru.se}


\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

%% for arXiv version
% \maketitle

\printAffiliationsAndNotice{}

\begin{abstract}
Harnessing large offline datasets is vital for training foundation models that can generalize across diverse tasks. 
Offline Reinforcement Learning (RL) offers a powerful framework for these scenarios, enabling the derivation of optimal policies even from suboptimal data. 
The Prompting Decision Transformer (PDT) % has extended the Decision Transformer (DT) to multi-task settings by introducing stochastic trajectory prompts.
is an offline RL multi-task model that distinguishes tasks through stochastic trajectory prompts, which are task-specific tokens maintained in context during rollouts.
% However, uniformly sampling prompts from expert demonstrations may oversample non-informative segments, limiting downstream performance. 
However, PDT samples these tokens uniformly at random from per-task demonstration datasets, failing to account for differences in token informativeness and potentially leading to performance degradation.
To address this limitation, we introduce a scalable bandit-based prompt-tuning method that dynamically learns to construct high-performance trajectory prompts. Our approach significantly enhances downstream task performance without modifying the pre-trained Transformer backbone. 
Empirical results on benchmark tasks and a newly designed multi-task environment demonstrate % significant performance gains, illustrating the methodâ€™s effectiveness in bridging pre-training and adaptation in offline RL.
the effectiveness of our method, creating a seamless bridge between general multi-task offline pre-training and task-specific online adaptation.
\end{abstract}

%% for arXiv version
% \twocolumn

% ------------------------------------------------------------ %

\section{Introduction}
\label{sec:introduction}

% \begin{itemize}
%     \item We want to exploits heaps of offline data to train foundation agents
%     \item DT is seminal method for offline RL
%     \item PDT generalized DT form single- to multi-task setting, facilitating large models and generalized pre-training
%     \item PDT identifies the task / MDP at hand via stochastic trajectory prompts
%     \item Crucial limitation: Not all prompts are equally informative, but PDT samples them uniformly at random from some dataset
%     \item Perhaps counterintuitively, this limitation is present even for fully observable MDPs, since there might be overlap in the set of expert demonstrations
 %    \item Addressing this, we propose a scalable and robust, yet simple, bandit prompt-tuning method that boost performance at downstream tasks, without requiring expensive weight updates of the underlying transformer
% \end{itemize}
The ability to exploit large amounts of offline data is crucial for training foundation models capable of generalizing across diverse tasks~\citep{radford2018improving, reed2022generalist, brohan2022rt}. In Reinforcement Learning (RL), a seminal contribution is the Decision Transformer (DT)~\citep{chen2021decision}, which reframes offline RL~\citep{levine2020offline} as a sequence modeling problem, thereby leveraging powerful Transformer architectures. The Prompting Decision Transformer (PDT)~\citep{xu2022prompting} further extends DT from single-task to multi-task settings, enabling large-scale models and generalized pre-training in offline meta- and few-shot RL~\citep{xu2022prompting, mitchell2021offline}. Analogous to prompting in Large Language Models (LLMs), PDT differentiates tasks through a \textit{stochastic trajectory prompt} prepended to the context, allowing it to identify and model optimal action marginals for each task in the offline dataset.

% PDT introduces stochastic trajectory prompts to identify the specific task or Markov Decision Process (MDP) at hand, enabling the agent to adapt dynamically across diverse environments.
However, PDT samples segments of prompts uniformly at random from expert demonstrations, overlooking a key limitation: \textit{not all prompts are equally informative for differentiating tasks}. Surprisingly, this issue persists even in offline datasets from fully observable Markov Decision Processes (MDPs). 
%We hypothesize that the oversampling of non-informative segments from expert demonstrations can diminish prompt informativeness, thereby obscuring their utility and ultimately leading to performance degradation.
We hypothesize that the sampling of non-informative prompts from the expert demonstrations can diminish PDT's ability to differentiate between tasks, thereby leading to performance degradation.

To address this shortcoming, we introduce a scalable, robust, and computationally efficient bandit-based prompt-tuning method for PDT. By optimizing prompt selection, our approach boosts downstream task performance without costly weight updates to the underlying Transformer backbone. Our experiments reveal clear performance gains with the proposed method, effectively bridging the gap between pre-training and adaptation.


% ------------------------------------------------------------ %

\section{Preliminaries}
\label{sec:preliminaries}
In this section, we introduce key concepts and terminologies that form the foundation of this work.

\subsection{Problem definition: Offline multi-task RL}
The offline multi-task RL problem is formalized similarly to prior works~\citep{xu2022prompting, mitchell2021offline}. The objective is to solve a set of training tasks $\mathcal{T}^\text{train}$, with the option to evaluate task generalization capabilities on a holdout test set $\mathcal{T}^\text{test}$. 

\begin{figure*}[]
   \centering
    \begin{equation}
        \rho = \big(
        \overbrace{
        \hat{r}_j^\star, \mathbf{s}_j^\star, \mathbf{a}_j^\star, 
        \dots,
        \hat{r}_{j+H}^\star, \mathbf{s}_{j+H}^\star, \mathbf{a}_{j+H}^\star}
        ^{\text{ $\Tilde{\tau}_1$: segment 1}},
        \dots,
        \overbrace{
        \hat{r}_k^\star, \mathbf{s}_k^\star, \mathbf{a}_k^\star, 
        \dots,
        \hat{r}_{k+H}^\star, \mathbf{s}_{k+H}^\star, \mathbf{a}_{k+H}^\star}
        ^{\text{$\Tilde{\tau}_J$: segment } J}
        \big)
    \label{eq:pdt}
    \end{equation}
\end{figure*}
\begin{figure*}
   \centering
    \begin{equation}
    \mathbf{x} = 
        \big( \rho \big) \odot
        \big(
        \overbrace{
        \hat{r}_{t-K}, \mathbf{s}_{t-K}, \mathbf{a}_{t-K}, 
        \hat{r}_{t-K+1}, \mathbf{s}_{t-K+1}, \mathbf{a}_{t-K+1}
        \dots,
        \hat{r}_{t}, \mathbf{s}_{t}, \mathbf{a}_{t}
        }^{\omega_{K:t}: K \text{most recent transitions}}
        \big)
    \label{eq:pdt-seq}
    \end{equation}
\end{figure*}

For each task $\mathcal{T}_i \in \mathcal{T}^\text{train}$, a dataset $\mathcal{D}_i$ is provided, consisting of trajectories sampled from the corresponding MDP $\mathcal{M}_i = \langle \mathcal{S}_i, \mathcal{A}_i, r_i, d_i, \gamma_i, \mu_i^0 \rangle$. Here, $\mathcal{S}_i$ is the state space, $\mathcal{A}_i$ is the action space, $r_i: \mathcal{S}_i \times \mathcal{A}_i \to \mathbb{R}$ represents the reward function, $d_i: \mathcal{S}_i \times \mathcal{A}_i \times \mathcal{S}_i \to [0, 1]$ defines the discrete-time transition dynamics, $\gamma_i \in (0, 1]$ is the discount factor, and $\mu_i^0$ is the initial state distribution of MDP $i$. 

The goal is to learn a generalized policy, $\pi(\mathbf{s}, \psi) \to \mathbf{a}$, capable of solving all tasks in $\mathcal{T}^\text{train}$. 
Here, $\psi$ serves as an auxiliary input to the policy that sufficiently describes the task $\mathcal{T}_i$. This input can take forms such as a task index, one-hot embedding, or task parameter, ensuring that the policy is aware of the current target task. 
In the case of PDT, $\psi$ is the \textit{stochastic trajectory prompt} sampled from the demonstration set for the target task.
For each task, the optimal generalized policy is expected to maximize the corresponding expected discounted reward objective specific to task $i$.
\begin{equation}\label{eq:generalized-objective}
    J(\pi, \psi) = \mathbb{E} \left[ \sum_{t=0}^\infty \gamma_{i, t} r_i(\mathbf{s}_t, \mathbf{a}_t) \right]
\end{equation}
The expectation is taken over trajectories where actions are sampled from the policy $\pi$, and states are sampled using the transition dynamics $d_i$ of the MDP $\mathcal{M}_i$. 
In the offline setting, only the pre-collected datasets $\mathcal{D} = \{\mathcal{D}_0, \dots, \mathcal{D}_n\}$ are available for learning, with no additional data collection allowed. 
A simulator may be used for policy evaluation but not for gathering new training data due to the offline nature of the problem.

\subsection{Prompting Decision Transformer}
\citet{chen2021decision} reframed RL as a sequence modeling problem, leveraging the capabilities of Transformer architectures to model trajectories. A trajectory in DT is represented as a sequence of triplets $(\hat{r}_t, \mathbf{s}_t, \mathbf{a}_t)$, where $\hat{r}_t = \sum_{t' = t} ^T r_{t'}$ is the return-to-go, $\mathbf{s}_t$ is the state, and $\mathbf{a}_t$ is the action at time step $t$. DT captures the temporal and causal dependencies between states, actions, and rewards, enabling the model to predict optimal actions directly, without relying on explicit value functions or policies. This design is particularly effective in offline RL settings, where pre-collected datasets of trajectories are available for training.

\citet{xu2022prompting} extended DT to a multi-task offline RL setting by introducing stochastic trajectory prompts as task-specific context, enabling the model to identify underlying MDPs.  
Such a prompt $\rho$ is composed of $J$ trajectory segments, each of length $H$, resulting in a total of $J \times H \times 3$ prompt tokens, as per Equation~\ref{eq:pdt}, where the superscript $^\star$ denotes tokens associated with the prompt.

For a particular training task $\mathcal{T}_i \in \mathcal{T}^\text{train}$, PDT learns to model the sequence in Equation~\ref{eq:pdt-seq} by autoregressively predicting the action tokens, where $\odot$ denotes concatenation. 
While PDT randomly samples the $J$ segments that constitute the prompt $\rho$ from a set of expert demonstrations $\mathcal{P}_i$ for each task, we propose a method to optimize segments selection, aiming to enhance the downstream task performance.

%\begin{itemize}
%    \item PDT is a multi-task offline RL model and adequate for out setting 
%    \item The model learns to identify MDPs by stochastic trajectory prompts
%    \item Such a prompt consists of $J$ segments of length $H$, total number of tokens is $J \times H \times 3$
%    \item PDT samples the $J$ segments in the prompt randomly, we instead aim to optimize the prompt to increase performance
%\end{itemize}

\subsection{Multi-Armed Bandits}
Multi-Armed Bandits (MABs) provide a framework for optimizing stochastic reward functions, making them an effective tool for tasks like prompt optimization. 
In the standard MAB setting, at each time step $k$, the agent selects an action (or ``arm'') $a_k \in \mathcal{A}$, where $\mathcal{A}$ is the set of available arms. 
Here, we use $k$ to denote bandit time steps, to avoid confusion with the MDPs time $t$.
The agent then receives a stochastic reward $r_k \sim R(a_k)$, with the goal of maximizing the cumulative reward $\sum_{k=1}^K r_k$ over a time horizon $K$~\citep{auer2002finite}. This requires balancing the exploration of arms to gather information about their reward distributions $R(a)$ and exploitation of arms with known high rewards while minimizing cumulative regret, defined as:
\begin{equation}
    \text{Regret}(K) = \sum_{k=1}^K \left[ \max_{a \in \mathcal{A}} \mathbb{E}[R(a)] - \mathbb{E}[r_k] \right]
\end{equation}

Contextual MABs (CMAB) extend this framework by incorporating side information (or ``context'') $\mathbf{c}_k \in \mathcal{C}$ which is observed before selecting an arm. The reward distribution is then conditioned on both the arm and the context, $r_k \sim R(a_k \mid \mathbf{c}_k)$. The agent's objective is to learn a policy $\pi: \mathcal{C} \to \mathcal{A}$ that maximizes the expected reward $\mathbb{E}\left[\sum_{k=1}^K R(\pi(\mathbf{c}_k) \mid \mathbf{c}_k)\right]$. By leveraging shared features across arms through the context $\mathbf{c}_k$, CMABs enable more efficient learning and better generalization, particularly in settings where arms share intrinsic characteristics~\citep{li2010contextual}.

% ------------------------------------------------------------ %

\section{Related Work}
\label{sec:related_work}
Recently, there has been a surge in methods across various domains aimed at enhancing the performance and generalization of Transformer-based approaches through automatic prompt tuning. The main challenge lies in the computationally expensive optimization of discrete prompts, guided by stochastic feedback from evaluating a black-box target model.

\textbf{Prompting in LLMs}. For LLMs, \citet{pmlr-v235-chen24e} introduced {I}nstruct{Z}ero, which uses Bayesian optimization to explore low-dimensional soft prompt vectors. These vectors are then passed to an open-source LLM to generate instructions for the black-box LLM. Building on this, \citet{lin2023use} proposed INSTINCT, which replaces the Gaussian process in Bayesian optimization with a neural network surrogate, leveraging a neural bandit algorithm to enhance expressivity. Additionally, \citet{shi2024best} demonstrated that the fixed-budget MAB framework enables learning the optimal prompt within a limited number of LLM evaluations. These methods rely on optimization in continuous spaces, followed by prompt reconstruction, whereas our approach directly operates within the DT prompt space.

Another line of research on prompt augmentation, or demonstration learning, investigates how selecting examples from a demonstration pool affects downstream performance. \citet{liu2021makes} utilized sentence embeddings to choose examples aligned with the input, while \citet{mishra2021natural} highlighted the benefits of combining both positive and negative examples for improved guidance. Approaches by \citet{kumar2021reordering} and \citet{lu2021fantastically} focused on optimizing the order of demonstrations using separator tokens and entropy-based scoring. In contrast, our method moves beyond reliance on the selection and ordering of the trajectory prompts and focuses on constructing optimal prompts by combining different segments from these trajectory prompts.

\textbf{Multi-task DT}. For RL, \citet{hu2023prompt} proposed to generate DT prompt candidates by perturbing the initial trajectory with Gaussian noise and using online or offline training to compute feedback for a ranking optimization algorithm. Similarly, the Prompt Diffuser~\citep{hu2024prompt} frames instruction optimization as a conditional generative modeling task, generating prompts starting from random noise. In contrast, our method focuses on constructing prompts from expert demonstrations, as noise-based approaches are not suitable in certain settings, such as discrete spaces. \citet{wang2024hierarchical} proposed hierarchical prompting that provides context-specific guidance through two levels of prompting: one that encapsulates task-specific information and another that uses a set of demonstration segments to guide the rollouts. Hyper-decision transformer~\citep{xu2023hyper} augmented the base DT with a hyper-network module to achieve adaptation to unseen novel tasks. However, both of those methods rely mainly on the presence of larger amounts of demonstration data, while our approach requires only a few demonstrations from which we learn to select the best prompt.

\textbf{LLM for RL}. At the intersection of LLM and RL domains, \citet{yang2024pre} and the closely related LaMo method~\citep{shi2024unleashing} proposed leveraging a pre-trained LLM as an initializer for PDT, harnessing rich linguistic knowledge to boost performance on unseen tasks. In another work, \citet{zhengdecomposed} introduced an approach to decompose the prompt into cross-task and task-specific components, ensuring more robust test-time adaptation to unseen tasks. Furthermore, the model is initialized by incorporating parameters from a pre-trained language model to provide it with prior knowledge. Compared to those, our approach avoids reliance on an additional large model, sidestepping the fine-tuning and scalability challenges inherent in such methods.

% ------------------------------------------------------------ %
\begin{figure*}[tbp]
    \centering
    \includegraphics[width=0.9\linewidth]{imgs/prompt-tuning-bandit.png}
    \caption{Overview of our inference time bandit-based prompt tuning for multi-task learning with prompting decision transformer (PDT). Each $z_i$ represents a triplet $(\hat{r}_i, \mathbf{s}_i, \mathbf{a}_i)$. The bandit explores the demonstration dataset $\mathcal{P}_i$ for the current task $i$ to find the segments $\Tilde{\tau}$ that induce the best prompt. The online return $G_k$ achieved by the underlying PDT model at round $k$ and using prompt $\rho_k$ serves as reward for the bandit. For simplicity in our illustration, we set $H=2$.}
    \label{fig:prompt_dt_bandit}
\end{figure*}

\section{Method}
\label{sec:method}
In this section, we present an inference time prompt-tuning method for offline multi-task RL.

Given a dataset $\mathcal{D} = \{ \mathcal{D}_0, \dots, \mathcal{D}_n \}$ of trajectories for $n$ training tasks $\mathcal{T}^\text{train}$, we utilize the original PDT method to learn the generalized policy described in Equation~\ref{eq:generalized-objective}. Details of the algorithm and training process are provided by~\citet{xu2022prompting}.

An optimal PDT model $\pi^*(\mathbf{x}; \theta)$ is assumed to be trained until convergence on $\mathcal{D}$. After training, the goal is to evaluate and enhance the model's performance on a specific task $\mathcal{T}_i$, either from the set of holdout test tasks $\mathcal{T}^\text{test}$ or from the training set $\mathcal{T}^\text{train}$. We assume access to a small set of demonstrations $\mathcal{P}_i$ (not necessarily of expert quality) for the target task, which serve as a source for sampling prompts, as well as a simulator of the corresponding $\mathcal{M}_i$ to perform online evaluations. 
Our primary objective is to \textit{identify the stochastic trajectory prompt in $\mathcal{P}_i$ that maximizes performance} when applied to the pre-trained generalized policy. %In the next section, we introduce our bandit-based prompt-tuning approach to this problem.

\subsection{Bandit-based Prompt Tuning}
% \begin{itemize}
    % \item We have one simplifying assumption stating that all expert demonstrations, from which we sample the prompt segments, are equally informative. This allows us to just sample random trajectory indices from $\mathcal{P}_i$ and find the best segments from those indices with the bandit.
    % \item This means that for any given expert demonstration, we must solely identify the best segment in that demonstration
    % \item Oleg: MAB solves for sequential decision making, where contexts are revealed one at a time. We should explain how this fits our setup where all demonstrations are available from the start.
    % \item CMAB with one arm per possible prompt in the dataset doesn't scale
    % \item Instead we propose CMAB with number of arms equal to $J$
    % \item Each arm $j$ predicts the PDTs return when segment $\tau$ is used at index $j$ in the trajectory prompt
    % \item The bandit learns which segment to put into which position of the prompt, to maximize the return of the underlying PDT
    %\item $\epsilon$-greedy vs UCB/TS; pre-trained Transformer featurizer vs training a future model from scratch - pros and cons, leveraging the inductive bias, being able to compute features for all demonstrations ahead of time and learning only the reward head
% \end{itemize}

We propose leveraging a bandit-based approach to efficiently identify the optimal trajectory prompt. A na\"ive implementation of bandit-based prompt tuning would involve deploying a bandit with one arm for each possible prompt $\rho$ constructible from $\mathcal{P}_i = (\tau_1, \dots, \tau_n)$, i.e. considering all possible combinations of segments among demonstration trajectories. This approach, however, scales extremely poorly, as the number of prompts (and consequently, the size of the bandit problem) grows linearly with the total number of transitions in $\mathcal{P}_i$ and combinatorially with $J$, the number of segments in each prompt. Additionally, treating each prompt as a separate and independent arm disregards prompt similarity, leading to inefficient sample complexity.

We propose optimizing the selection of segments $\Tilde{\tau}$ that form the stochastic trajectory prompt $\rho = (\Tilde{\tau}_1, \dots, \Tilde{\tau}_J)$ using a contextual MAB algorithm. Unlike traditional contextual MAB settings, where contexts $\mathbf{c}_k$ are revealed incrementally at each bandit time step $k$, our framework assumes that the entire dataset $\mathcal{P}_i$ is available upfront.

As illustrated in Figure~\ref{fig:prompt_dt_bandit}, at each bandit step $k$, the bandit selects a prompt $\rho_k$, balancing exploration and exploitation within the prompt space. In each round, the bandit selects $J$ segments to construct the trajectory prompt $\rho_k$ which is then passed to the pre-trained PDT $\pi^*(\mathbf{x}_k; \theta)$. The PDT is rolled out in $\mathcal{M}_i$ using the selected prompt, and the resulting performance is logged as $G_i^k = \sum_{t=0}^T r_i(\mathbf{s}_t, \mathbf{a}_t) \mid \mathbf{a}_t \sim \pi^*(\mathbf{x}_k; \theta)$. The input to the PDT at each MDP step $t$ is given by $\mathbf{x}_k = \rho_k \odot \omega_{K:t}$ (see Eq.~\ref{eq:pdt-seq}), where the trajectory prompt $\rho_k$ remains fixed throughout the rollout, and the MDP transitions $\omega_{K:t}$ are updated dynamically after each step. The process of constructing the prompt $\rho_k$ through the bandit mechanism is discussed in detail in the next section.

The performance of rollout $k$, $G_i^k$, serves as the reward from the bandit's perspective, and the tuple $\langle \rho_k, G_i^k \rangle$ is stored for training the reward model. The pseudocode of the proposed method is provided in Algorithm~\ref{alg:bandit_steps}.

\subsection{Scalable and sample-efficient bandit architecture}
Our bandit features $J$ arms, one for each segment in the stochastic trajectory prompt. Each arm $j$ maintains an independent reward model $\phi_j$, which predicts the performance of the underlying PDT when a given segment $\Tilde{\tau}$ is placed at position $j$ in the prompt. Prompt selection is conducted by allowing each arm to either explore segments using strategies such as UCB~\citep{li2010contextual}, $\epsilon$-greedy, or other mechanisms, or to exploit based on accumulated knowledge.

To utilize the learned models $\phi_j$, the reward for each segment $\{ \Tilde{\tau}_0, \dots, \Tilde{\tau}_n \}$ is predicted for every available slot $j$, resulting in a prediction matrix $\mathbf{Y} \in \mathcal{R}^{J \times |\mathcal{P}_i|}$, where $|\mathcal{P}_i|$ denotes the number of (overlapping) segments that can be extracted from the trajectories in $\mathcal{P}_i$. 
For clarity, if $\mathcal{P}_i$ contains $M$ expert trajectories, each of length $L$, and each prompt segment has a length $H$, the total number of segments is given by $|\mathcal{P}_i| = M \times (L - H + 1)$.
As outlined in Algorithm~\ref{alg:select_prompt}, the $J$ optimal segments with the highest predicted performance are then selected by computing $\arg \max$ over the segment dimension of $\mathbf{Y}$.
After each bandit step $k$, all reward models are independently updated using gradient descent on the accumulated $\langle$prompt, reward$\rangle$ pairs, as detailed in Algorithm~\ref{alg:update_bandit}. While this approach introduces high variance due to the omission of correlations between prompt segments, it performs effectively in practice, as demonstrated by the experimental results. % and the example provided in Equation~\ref{bandit_segment_example}, where $H$ is set to two for illustrative purposes:
% \begin{figure*} %% for ICML two-column format
%    \centering
% \begin{equation}
% \label{bandit_segment_example}
%     \tau_i = \big(
%     \lefteqn{\overbrace{\phantom{
%         \hat{r}_1^\star, \mathbf{s}_1^\star, \mathbf{a}_1^\star, 
%         \hat{r}_2^\star, \mathbf{s}_2^\star, \mathbf{a}_2^\star
%     }}^{\text{segment $\Tilde{\tau}_1$}}}
%     \hat{r}_1^\star, \mathbf{s}_1^\star, \mathbf{a}_1^\star, 
%     \underbrace{
%         \hat{r}_2^\star, \mathbf{s}_2^\star, \mathbf{a}_2^\star, 
%         \hat{r}_3^\star, \mathbf{s}_3^\star, \mathbf{a}_3^\star
%     }_{\text{segment $\Tilde{\tau}_2$}}, 
%     \dots,
%     \lefteqn{\overbrace{\phantom{
%         \hat{r}_{L-2}^\star, \mathbf{s}_{L-2}^\star, \mathbf{a}_{L-2}^\star, 
%         \hat{r}_{L-1}^\star, \mathbf{s}_{L-1}^\star, \mathbf{a}_{L-1}^\star
%     }}^{\text{segment $\Tilde{\tau}_{L-H}$}}}
%     \hat{r}_{L-2}^\star, \mathbf{s}_{L-2}^\star, \mathbf{a}_{L-2}^\star, 
%     \underbrace{
%         \hat{r}_{L-1}^\star, \mathbf{s}_{L-1}^\star, \mathbf{a}_{L-1}^\star, 
%         \hat{r}_{L}^\star, \mathbf{s}_{L}^\star, \mathbf{a}_{L}^\star
%     }_{\text{segment $\Tilde{\tau}_{L-H+1}$}}
%     \big)
% \end{equation}

% \end{figure*}
%%Given a trajectory length $L$ and a segment length $H$ (set to two for illustrative purposes), each trajectory $\tau_i$ contains $L-H+1$ overlapping segments. Each arm $j$ is responsible for selecting the optimal segment for its corresponding slot in the final prompt, choosing from $M$ possible segment options available for that position.


\begin{figure}[t]
        \begin{algorithm}[H]
            \caption{Prompt-Tuning Bandit}\label{alg:bandit_steps}
            \begin{algorithmic}[1]
                \item[] \textbf{Input:} Pre-trained PDT parameter $\theta$, Simulator $\mathcal{M}_i$, expert demonstrations $\mathcal{P}_i$
                \item[] \textbf{Initialize:} Bandit parameter $\phi^0$, dataset $\mathcal{B} \leftarrow \empty \{ \}$
                \FOR{bandit steps $k \in K$}
                    \STATE \textbf{Algorithm~\ref{alg:select_prompt}}: Select prompt $\rho_k$ for current rollout $k$ using parameters $\phi^k$
                    \STATE Performance metric $G_i^k = 0$
                    \FOR{MDP steps $t \in T$}
                        \STATE Make PDT input (Eq.~\ref{eq:pdt-seq}): $\mathbf{x}_k = \rho_k \odot \omega_{K:t}$
                        \STATE Sample action from PDT: $\mathbf{a}_t \sim \pi^*(\mathbf{x}_k; \theta)$
                        \STATE Step environment: $r_t, \mathbf{s}_{t+1} \sim \mathcal{M}_i (\mathbf{s}_t, \mathbf{a}_t)$
                        \STATE Log reward: $G_i^k = G_i^k + r_t$
                    \ENDFOR
                    \STATE Store data $\mathcal{B} \leftarrow B \cup \langle \rho_k, G_i^k \rangle$
                    \STATE \textbf{Algorithm~\ref{alg:update_bandit}}: Update $\phi^k$ using $\mathcal{B}$, yielding $\phi^{k+1}$
                \ENDFOR
                \item[] \textbf{Return:} Final bandit parameter $\phi^K$
            \end{algorithmic}
        \end{algorithm}
\end{figure}
\begin{figure}[t]
    \begin{minipage}[t]{0.48\textwidth} % Left algorithm (48% width)
\begin{algorithm}[H]
            \caption{CMAB Prompt Selection}\label{alg:select_prompt}
            \begin{algorithmic}[1]
                
                \item[] \textbf{Input:} Expert demonstrations $\mathcal{P}_i$, bandit parameter $\phi = \langle \phi_1, \cdots, \phi_J \rangle$
                \item[] \textbf{Initialize:} Prediction matrix $\mathbf{Y} = [\ ]$
                \FOR{segment $\Tilde{\tau} \in \mathcal{P}_i$}
                \STATE Predict reward $ \mathbf{\hat{y}} = [ \phi_1(\Tilde{\tau}), \dots, \phi_J(\Tilde{\tau}) ]$
                \STATE Append row to prediction matrix $\begin{bmatrix} \mathbf{Y} \\ \mathbf{\hat{y}} \end{bmatrix}$
                \ENDFOR
                % \STATE $\rho_j = \mathcal{P}_i[\arg \max(\mathbf{Y}[j, :])]\ \forall j \in \{1, ..., J\}$
                \item[] \textbf{Return:} $\rho = \mathcal{P}_i[\arg \max(\mathbf{Y}[j, :])]\ \forall j \in \{1, ..., J\}$
            \end{algorithmic}
        \end{algorithm}
    \end{minipage}%
    \hfill
    \begin{minipage}[t]{0.48\textwidth} % Right algorithm (48% width)
        \begin{algorithm}[H]
            \caption{CMAB Update}\label{alg:update_bandit}
            \begin{algorithmic}[1]
            \item[] \textbf{Input:} Bandit dataset $\mathcal{B} = \mathbf{X}, \mathbf{y}$, bandit parameter $\phi = \langle \phi_1, \cdots, \phi_J \rangle$, learning rate $\alpha$
                \FOR{reward model $j \in J$}
                    \STATE Get segments at $j$-th index $\mathbf{X}_j = \mathbf{X}_{[j]}$
                    \STATE Predict reward $\mathbf{\hat{y}}_j = \phi_j(\mathbf{X}_j)$
                    \STATE Define $\mathcal{L}(\phi_j) = \text{MSE}(\mathbf{\hat{y}}_j, \mathbf{y})$
                    \FOR{gradient steps $l = 0, \dots, L$}
                    \STATE $\phi_j^{l+1} = \phi_j^l - \alpha \nabla \mathcal{L}(\phi_j^l)$
                    \ENDFOR                        
                \ENDFOR
            \item[] \textbf{Return:} New parameter $\phi = \langle \phi_1^L, \cdots, \phi_J^L \rangle$
        \end{algorithmic}
        \end{algorithm}
    \end{minipage}
\end{figure}


\subsection{Arm features}\label{sec:arm-features}
Learning reward models $\phi_j: \Tilde{\tau} \to \mathbb{R}$ for ``raw'' trajectory segments becomes impractical for MDPs with large state spaces, such as those involving pixel-space observations and high-dimensional actions. The key issue lies in the input size of these reward models, which scales as $H \times (|\mathcal{S}| + |\mathcal{A}| + 1)$, growing linearly with $\mathcal{S}, \mathcal{A}$, and $H$.
This limitation can be addressed by integrating a feature extractor $\Psi: {\Tilde{\tau}} \to \mathbb{R}^d$ that embeds raw trajectory segments from the input space into a latent feature space. Such a feature extractor can be pre-trained using various methods on the demonstration dataset, enabling the reward models to operate more efficiently in the resulting embeddings' space.

Alternatively, the pre-trained PDT model itself can be leveraged as a feature extractor by utilizing the hidden representation of prompt tokens as the embedding for the prompt. This approach not only mitigates the scaling issue but also aligns with the inductive biases of the pre-trained PDT, which is expected to encode meaningful task-relevant information. The reward models then operate on these fixed-size embeddings rather than raw, flattened segments, significantly improving scalability.



% ------------------------------------------------------------ %
\section{Experiments}
\label{sec:experiments}

% In this section, we present an evaluation of the proposed method in comparison to baseline approaches. Additionally, we conduct empirical ablation studies to analyze and highlight the individual contributions of the key components of the methodology.
In this section, we evaluate the proposed prompt-tuning method, focusing on whether inference-time performance gains can be attributed to prompt tuning. Additionally, we analyze its impact on prompt selection, assess improvements in OOD generalization, examine robustness to prompt dataset quality, and investigate performance in high-dimensional state and action spaces.

\subsection{Environments, baselines, and implementation}
%
\begin{figure}
    \centering
    \begin{subfigure}{0.215\textwidth}
        \includegraphics[width=\linewidth]{imgs/2d_point_agent_radii_stop_env.png}
        \caption{Multiple tasks.}
        \label{subfig:2d_env_tasks}
    \end{subfigure}
    \begin{subfigure}{0.22\textwidth}\includegraphics[width=\linewidth]{imgs/suboptimal_vs_expert_trajs.png}
        \caption{Exemplary trajectories.}
        \label{subfig:expert_vs_novice}
    \end{subfigure}
    \caption{Our \textbf{Sparse 2D point} environment. \textbf{Left}: Multiple tasks can be induced through the task parameters $r$ and $\alpha$, corresponding to radius and angle. \textbf{Right}: Expert and novice trajectories for a particular task.}
    \label{fig:2d_env_overview}
\end{figure}

\textbf{Environments}. While most offline RL and DT-based studies have predominantly used MuJoCo~\citep{todorov2012mujoco} benchmarks as test beds, the tasks in this benchmark often lack the features required for a robust multi-task generalization and prompt-tuning study. To address this limitation, experimental results are reported not only on MuJoCo but also on a newly designed ``Sparse 2D point'' environment specifically tailored to better evaluate multi-task generalization.

\quad\textbf{\texttt{Sparse 2D point}}: This task involves mixed control of a planar point agent, where the state representation consists of the agent's current 2D coordinates. The agent always begins at $(0,0)$ with the objective of reaching a specified goal coordinate. It has two continuous actions for movement across the plane and a binary \texttt{stop} action. The task requires the agent to navigate to the goal coordinate and appropriately select the \texttt{stop} action. A sparse reward is provided upon selecting the \texttt{stop} action, proportional to the agent's distance from the goal. When \texttt{stop} is selected within close proximity of the goal, the environment provides a reward bonus of 10, which is discounted based on the number of wasted steps. Tasks are distinguished by varying goal coordinates, distributed on circles with different radii and angles, as depicted in Figure~\ref{subfig:2d_env_tasks}. 
% The split into training task $\mathcal{T}^\text{train}$ and testing tasks $\mathcal{T}^\text{test}$ is described in Appendix~\ref{app:2D-env-task-split}.
While the goal is not explicitly part of the state, it is implicitly encoded through the reward function, ensuring that each task remains a fully observable MDP.
    
\quad\textbf{\texttt{MuJoCo Half Cheetah}}: As introduced by~\citet{xu2022prompting}, this task involves continuous joint control of a planar ``half-cheetah'' agent. The action space and state space have dimensionalities of 7 and 20, respectively. Different tasks are defined by varying target velocities for the agent, with the reward being proportional to the discrepancy between the agent's actual and target velocities. Although the target velocity is not explicitly included in the state, it is implicitly encoded through the reward function, ensuring that each task corresponds to a fully observable MDP.

\textbf{Baselines}. Our proposed method is evaluated against the following RL baselines: (i) an optimal policy oracle, (ii) a DT trained on multiple tasks without additional enhancements, (iii) and a vanilla PDT, which samples prompts uniformly at random, 
(iv) and a Gaussian perturbation-based algorithm, where we effectively perform hill climbing in the prompt space, similarly to~\citet{hu2023prompt}.
%and perturbation-based tuning algorithm~\citep{hu2023prompt}.

\textbf{Implementation details}. All experiments were conducted on an instance equipped with NVIDIA T4 GPU with 8GB of memory, utilizing the PyTorch library~\citep{paszke2019pytorch}. Details of all hyperparameter configurations are provided in Appendix~\ref{appx:training_details}.

\textbf{Offline reinforcement learning dataset}
For each training task $i$, PDT requires a dataset of trajectories $\mathcal{T}_i$ and a dataset of expert demonstrations $\mathcal{P}_i$. For the \texttt{Mujoco Half Cheetah} tasks, we use datasets from \citet{xu2022prompting} comprising 40 tasks, each corresponding to a distinct velocity. Of these, five tasks $\{2, 7, 15, 23, 26\}$ are reserved for evaluation, and the other 35 tasks are used for training. 

To generate datasets for the \texttt{Sparse 2D point} environment, we employ PPO~\citep{schulman2017proximal} on 60 tasks with three discrete radii and 20 discrete angles. The first 48 tasks are used for training, and the remaining 12 are reserved for testing generalization. 
For more task details see Sec.~\ref{app:2D-env-task-split} in the Appendix.
PPO is run for 1M steps on each training task, and the resulting trajectories are stored as $\mathcal{D}_i$. The highest-return trajectories are selected to form $\mathcal{P}_i$. To reduce the computational cost of computing $\arg\max$ over all possible prompt segments, $\mathcal{P}_i$ is limited to 10 randomly sampled high-return trajectories.

\subsection{Results and Analysis}
%
\begin{figure*}[ht]
    \centering
    % \includegraphics[width=1\linewidth]{imgs/spatio_temporal_segments.png}
    \includegraphics[width=1\linewidth]{imgs/spatio_temporal_segments_allTasks.png}
    \includegraphics[width=0.35\linewidth]{imgs/spatio_temporal_segments-colorbar.png}
    \caption{Spatio-temporal visualization of prompt selection across tasks. Each dot represents the mean segment coordinates, colored by performance. The red diamond marks the starting state, while the red stars indicate the goal coordinates of tasks used in the experiment. The shaded region indicates the holdout test tasks.}
    \label{fig:spatio-temporal}
\end{figure*}

\begin{table}[t]
\centering
\begin{tabular}{lccc}
Method & $J=1$ & $J=2$ & $J=4$ \\
\hline
PDT, no tuning & 0.0 $\pm$ 2.1 & 6.3 $\pm$ 0.8 & 8.3 $\pm$ 0.6 \\
$\epsilon$-Greedy$^\Psi$ & 9.0 $\pm$ 0.6 & 9.4 $\pm$ 0.3 & \textbf{9.6 $\pm$ 0.2} \\
UCB$^\Psi$ & 9.2 $\pm$ 0.5 & 8.8 $\pm$ 0.9 & 8.8 $\pm$ 0.9 \\
$\epsilon$-Greedy, raw & 8.9 $\pm$ 0.5 & \textbf{9.5 $\pm$ 0.3} & 8.9 $\pm$ 0.7 \\
UCB, raw & \textbf{9.4 $\pm$ 0.5} & \textbf{9.5 $\pm$ 0.3} & 9.3 $\pm$ 0.4 \\
Gaussian perturbation & 5.8 $\pm$ 3.8 & 7.9 $\pm$ 1.6 & 6.2 $\pm$ 4.0 \\
\hline
\end{tabular}
\begin{tabular}{ll}
Standard DT & -64.3 $\pm$ 24.1 \\
\hline
\end{tabular}
\caption{Inference time performance in the \texttt{Sparse 2D point} environment. $\Psi$ denotes that the bandit's reward model operates on encoded trajectory segments; otherwise, it processes non-encoded trajectory segments. Results are averaged across training tasks, three random seeds, and the last 50 rollouts.}\label{tab:2d-tuning-results}
\end{table}

\begin{table}[]
\centering
\begin{tabular}{lcc}
Method & $J=1, H=5$ & $J=2, H=20$ \\
\hline
PDT, no tuning & -44.75 $\pm$ 0.91 &  -42.68 $\pm$ 1.3\\
%PDT, fine-tuning & -71.51 $\pm$ 15.85 &  -61.57 $\pm$ 8.09 \\
$\epsilon$-greedy CMAB\textsuperscript{$\Psi$} & \textbf{-42.60 $\pm$ 3.77} & \textbf{ -34.12 $\pm$ 3.12}\\
$\epsilon$-greedy CMAB & -76.50 $\pm$ 6.68  &  -58.76 $\pm$ 8.21\\
\hline
\end{tabular}
\caption{Inference time performance in the \texttt{Mujoco Half Cheetah} environment. Columns represent models with increasing numbers of trajectory segments ($J$) and segment lengths ($H$) in the prompt. Results are
averaged over multiple tasks, seeds, and rollouts.}\label{tab:half-cheetah-results}
\end{table}


\textbf{Does bandit-based prompt-tuning improve the inference-time performance of a frozen PDT backbone?}

To investigate this question empirically, we compare the inference-time performance of a standard PDT model trained to convergence, sampling demonstrations randomly from $\mathcal{P}_i$, with the same model enhanced by our bandit-based prompt selection method. 
We conduct 250 online rollouts 
% across multiple training tasks 
on the training tasks with the largest radius,
both with and without prompt tuning and report the mean performance from the final 50 rollouts, averaged over three seeds. Results for the \texttt{Sparse 2D point} environment are presented in Table~\ref{tab:2d-tuning-results}.

The results show that the proposed method consistently enhances the PDT backbone's performance, achieving optimal returns. This effect is most prominent in the $J=1$ setting, as larger $J$ (i.e., more segments and tokens in the prompt) naturally improves PDT's performance, even without prompt-tuning, reducing the performance margin for tuning. 
% In this environment, random sampling sufficiently provides prompts with tokens that enable PDT to identify the target task. 
This implies that, for the \texttt{Sparse 2D Point} environment, randomly sampling \textit{large} prompts suffices for finding tokens that enable the PDT to identify the target task.
We observe no significant differences between $\epsilon$-greedy and UCB-based exploration strategies or between training the bandit's reward models using ``raw'' trajectory segments compared to Transformer-based representations, as described in Section~\ref{sec:arm-features}.

Compared to the Gaussian perturbation baseline, bandit-based tuning achieves consistently higher returns with substantially lower variance. Notably, Gaussian perturbation reduces performance when $J > 1$, likely because it introduces noise across the entire prompt, including segments that are already effective. 
% In contrast, our bandit approach selectively explores segments without applying unnecessary changes, 
In contrast, our bandit approach is more efficient and robust, since it models each segment as an independent arm.
This allows it to retain optimal segments while continuing to explore others, reliably boosting PDT to optimal performance even for $J > 1$. 
The standard DT baseline further highlights the importance of prompts in distinguishing tasks. Additional plots are provided in Appendix~\ref{app:plots_2d_rollouts}.

\sloppy
Results for the higher-dimensional \texttt{MuJoCo Half Cheetah} task are presented in Table~\ref{tab:half-cheetah-results}. In this more complex environment, the proposed method provides substantial performance improvements over the baseline pre-trained model. As $J$ and $H$ increase, the gains from the bandit approach become even more pronounced. 

Training the bandit's reward models with ``raw'' trajectory segments ($\epsilon$-greedy CMAB) instead of Transformer representations ($\epsilon$-greedy CMAB\textsuperscript{$\Psi$}) leads to degraded performance, emphasizing the importance of trajectory encoding for addressing scaling challenges in complex environments. The results in Table~\ref{tab:half-cheetah-results} represent the running mean performance from the last 10 rollouts, averaged over a selected set of training tasks and three seeds. 

% and \ref{app:plots_mujoco_heetah}.

\textbf{Which prompts does bandit-tuning select?}
% \textbf{Number and length of segments.} We perform experiments to analyze how varying the number of prompt tokens impacts the performance of the PDT model and the effectiveness of the bandit-based prompt tuning approach. The results show that increasing the number of segments in the prompt improves PDT performance but simultaneously reduces the potential for further improvement through the bandit approach. Specifically, when the number of segments is set to include all possible segments from the trajectories, the improvement margin for the bandit model diminishes, making its contribution less significant. Similar trends are observed when varying the segment length, mirroring the effects of changing the number of segments.

We qualitatively analyze how the bandit's prompt selection strategy evolves over time and how it learns to identify high-performance segments. For this analysis, we use PDT with one segment per prompt ($J=1$) and $\epsilon$-greedy exploration, where $\epsilon$ is annealed from 1 to 0 over the first 30 rounds. 

Figure~\ref{fig:spatio-temporal} illustrates the prompt segments selected by the bandit over an increasing number of rounds for 20 tasks corresponding to different angles. The segments are represented by their mean spatial coordinates. During the initial rounds, the bandit explores by uniformly sampling segments from the prompt datasets $\mathcal{P}_i$, encountering both low- and high-performing prompts. In this \texttt{Sparse 2D point} environment, low-performance prompts are associated with segments closer to the starting state at the circle's center, as these fail to clearly indicate where the agent should stop. Over time, the bandit exploits its accumulated knowledge to disregard these low-performance prompts and instead selects segments nearer to the goal coordinates, which are more informative for task identification.
% These goal-aligned segments provide more informative cues for differentiating tasks, explaining the PDT's strong performance when using such prompts.

\textbf{Can prompt-tuning exploit non-expert datasets?}

% \textbf{Demonstration data quality.}  This approach effectively discards uninformative segments and offers a simple yet powerful way to create an optimal prompt.
%
%
While the PDT model with a random prompt selection strategy relies on access to expert demonstrations, our method demonstrates greater robustness to the quality of demonstration data. To examine this, we generate additional prompt datasets $\{\mathcal{P}_i^{\%j}\}, j \in \{0, 10, \dots, 100\}$, which combine $j\%$ of expert data along with novice demonstrations, selected from trajectories in the bottom 5th percentile of task returns (see Fig.~\ref{subfig:expert_vs_novice}). 

When sampling prompts from these mixed datasets, the bandit-based prompt optimization significantly enhances the PDT model's robustness and improves its performance, as shown in Table~\ref{tab:2d-prompt-quality-exp}. This approach reduces reliance on pure expert demonstrations by learning to identify the optimal prompt from any arbitrary mixture dataset.
%
%
\begin{table}[t]
\centering
\resizebox{0.5\textwidth}{!}{ % Scale to column width
\begin{tabular}{cccc}
\makecell{Expert percentage $j\%$} & No tuning & $\epsilon$-greedy & UCB\\
\hline
0\% & -39.4 $\pm$ 16.8 & -12.1 $\pm$ 19.4 & -3.8 $\pm$ 10.0 \\
10\% & -40.8 $\pm$ 14.1 & 4.4 $\pm$ 3.4 & 7.3 $\pm$ 1.9 \\
20\% & -49.6 $\pm$ 32.8 & 5.5 $\pm$ 3.4 & 9.4 $\pm$ 0.5 \\
30\% & -50.1 $\pm$ 27.4 & 6.5 $\pm$ 1.3 & \textbf{9.8 $\pm$ 0.3} \\
40\% & -41.4 $\pm$ 26.4 & 2.1 $\pm$ 6.4 & 8.6 $\pm$ 2.0 \\
50\% & -12.9 $\pm$ 4.0 & 5.1 $\pm$ 4.4 & 8.7 $\pm$ 0.8 \\
60\% & -14.1 $\pm$ 5.0 & 7.6 $\pm$ 1.5 & 8.5 $\pm$ 1.2 \\
70\% & -28.6 $\pm$ 17.0 & 8.4 $\pm$ 0.9 & 7.7 $\pm$ 1.3 \\
80\% & -12.3 $\pm$ 11.1 & 8.6 $\pm$ 1.0 & \textbf{9.8 $\pm$ 0.3} \\
90\% & {0.9 $\pm$ 0.5} & 8.6 $\pm$ 0.9 & 8.6 $\pm$ 1.0 \\
100\% & \textbf{0.7 $\pm$ 1.7} & \textbf{9.7 $\pm$ 0.4} & 9.7 $\pm$ 0.4 \\
\hline
\end{tabular}
}
\caption{
Without prompt-tuning, PDT's performance deteriorates with the percentage of expert trajectories in $\mathcal{P}_i$. 
Our prompt-tuning method is robust with respect to the percentage of expert data and achieves near-optimal performance with as little as 10\% expert demonstrations. Results are averaged over three seeds for a single training task.}
\label{tab:2d-prompt-quality-exp}
\end{table}

\textbf{Does prompt-tuning improve OOD task generalization}?

% The ability to generalize to unseen, out-of-distribution tasks is crucial for generally capable foundation agents.
% The inductive bias through prompting as well as the PDT's generalized, multi-task training setting make it a promising model for training foundation agents. 
% PDT's multi-task pre-training framework, combined with its inductive bias through prompting, enables strong out-of-distribution (OOD) task generalization capabilities~\citep{xu2022prompting}. 
To assess the benefit of prompt-tuning on
% this, 
out-of-distribution (OOD) tasks,
we evaluate the pre-trained PDT model on OOD tasks within the \texttt{Sparse 2D point} environment, where goal locations lie on the largest circle with angles exceeding those in training, as depicted in Figure~\ref{fig:spatio-temporal}. We perform 250 rollouts, calculate the mean over the final 50, and average the results across three seeds. Results in Table~\ref{tab:2d-ood-finetuning-generalization} show that prompt-tuning consistently improves performance, achieving gains comparable to the in-distribution setting, though not always reaching optimal levels.

Further OOD generalization results for the \texttt{Mujoco Half Cheetah} environment are presented in Table~\ref{tab:half-cheetah-results-ood}. Using the same evaluation protocol (multiple tasks, 250 rollouts with the mean of the final 10 averaged across three seeds), we observe that the bandit-based prompt-tuning method significantly enhances PDT's generalization to OOD tasks. Notably, this improvement becomes increasingly evident with larger prompt sequences (i.e., increasing $J$ and $H$), further underscoring the efficacy of this approach.

Notably, fine-tuning the PDT backbone exclusively on target task data for 250 epochs results in a performance decline across both tested environments. This degradation is hypothesized to stem from the large size of the Transformer backbone, the absence of regularization provided by diverse multi-task training data, and a misalignment between offline action loss and online task performance. These factors likely lead to overfitting on the fine-tuning dataset without yielding improved online returns. Supporting plots are provided in Appendix~\ref{app:ood-training}.

\begin{table}[]
\centering
\begin{tabular}{lccc}
Method & Pre-trained $\theta$ & Fine-tuned $\theta$ \\
\hline
PDT, no tuning & -6.7 $\pm$ 7.0 & -28.3 $\pm$ 22.7 \\
With $\epsilon$-greedy bandit & \textbf{-2.4 $\pm$ 8.7} & \textbf{-21.5 $\pm$ 22.3} \\
% With UCB bandit (16) & -10.1 $\pm$ 14.8 & -23.5 $\pm$ 22.1 \\
% With UCB bandit (64) & -4.6 $\pm$ 6.9 & -18.8 $\pm$ 20.1 \\
% With UCB bandit (128) & -5.9 $\pm$ 12.8 & -18.3 $\pm$ 20.2 \\
\hline
\end{tabular}
\caption{Performance on OOD tasks in \texttt{Sparse 2D point} environment. Columns differentiate between the pre-trained and fine-tuned PDT parameter.}\label{tab:2d-ood-finetuning-generalization}
\end{table}

\begin{table}[]
\centering
\begin{tabular}{lcc}
Method & $J=1, H=5$ & $J=2, H=20$ \\
\hline
PDT, no tuning & -36.83 $\pm$ 2.58 & -35.04 $\pm$ 3.60\\
PDT, fine-tuning & -71.51 $\pm$ 15.85 &  -61.57 $\pm$ 8.09 \\
$\epsilon$-greedy CMAB\textsuperscript{$\Psi$} & \textbf{-30.80 $\pm$ 2.48} & \textbf{-25.13 $\pm$ 1.56}\\
%$\epsilon$-greedy CMAB & -41.90 $\pm$ 10.48 & -29.58 $\pm$ 1.02\\
\hline
\end{tabular}
\caption{Performance on OOD tasks in the \texttt{Mujoco Half Cheetah} environment. Columns represent models with increasing numbers of trajectory segments ($J$) and segment lengths ($H$) in the prompt.}
\label{tab:half-cheetah-results-ood}
\end{table}

% ------------------------------------------------------------ %

\section{Conclusion}
\label{sec:conclusion}
This work introduces a bandit-based prompt-tuning method extending PDT and addressing the limitations of uniform prompt sampling. By leveraging a contextual MAB framework, our approach optimizes the selection of trajectory segments to maximize task performance and enhance adaptation to unseen tasks. Experimental results highlight consistent improvements across diverse tasks, demonstrating the efficacy and robustness of the proposed method in multiple environments. 
% While the scalability of the approach is challenged by large datasets, incorporating feature extractors or learned samplers presents promising directions for future work. 
This method not only advances the state of prompt optimization in PDT but also contributes to the broader integration of offline RL and sequence modeling paradigms.

\textbf{Limitation and broader impact}

A key limitation of the proposed method is its high variance, stemming from the assumption that each reward model treats its corresponding segment as the sole factor influencing performance, and ignoring correlations between segments. This simplification opens avenues for future work aimed at disentangling these dependencies to improve model robustness and accuracy. Another challenge arises when the size of the expert demonstration dataset becomes prohibitively large. Iterating over the entire dataset becomes computationally expensive, especially given the combinatorial growth factor associated with the number of prompt segments, $J$. A potential solution could involve learning a sampler to identify promising segments, which could then be paired with reward models for further refinement. These areas highlight opportunities for advancing the methodology to address scalability and efficiency.

% ------------------------------------------------------------ %

% TODO: uncomment for the camera-ready version
% \section*{Acknowledgments}
% This work was partially funded by Wallenberg Autonomous Systems Programs (WASP).

\bibliography{main}
\bibliographystyle{iclr2025_conference}

\clearpage
\onecolumn
\appendix

\section*{Appendix}

\section{\texttt{Sparse 2D point} environment details}\label{app:2D-env-task-split}
Our \texttt{Sparse 2D point} environment offers a continuous task space, parameterized by the angle and radius, for arbitrary goal locations on the 2D plane.
We discretize the task space by using three discrete radii, $(0.9, 1.9, 2.9)$, and 20 discrete angles $(0.1 \cdot \pi, 0.2 \cdot \pi, \dots, 1.9 \cdot \pi)$, instead of sampling continuous task parameters. 
This is primarily done to separate datasets for different tasks, since PDT requires expert demonstration for each training task. 
To separate these $3 \cdot 20 = 60$ tasks into the training set $\mathcal{T}^\text{train}$ and testing set $\mathcal{T}^\text{test}$, all tasks with an angles greater than $1.5 \cdot \pi$ (independently of the radius) are treated as testing task and are not part of the training set.
This split yields 48 training tasks, and 12 testing tasks. 
Spatially, the test set is indicated by the shaded are in Figure~\ref{fig:spatio-temporal}.

\section{Training details}
\label{appx:training_details}
Details of the hyperparameters for the DT and PDT models are provided in Table~\ref{tab:dt_hyperparameters}, while those for the bandit model are listed in Table~\ref{tab:cmab_hyperparameters}. Game-specific hyperparameters are reported separately in Table~\ref{tab:pdt_hyperparameters_env}. The implementations of DT and PDT were adopted from \citet{minimal_decision_transformer} with minimal modifications to integrate seamlessly with the CMAB prompt-tuning framework.
For the Gaussian perturbation baseline, we sample an initial prompt randomly from the expert demonstration dataset for the target task, then apply Gaussian noise and perform hill climbing. We linearly anneal the scale of the exploration noise from 1.1 to 0.1 over the course of the 250 online rollouts. 

Our source code is publicly available at \url{https://retracted-during-review}.

\begin{table*}[h]
\begin{minipage}{0.48\linewidth}
    \begin{center}
        \begin{tabular}{ll}
            \multicolumn{1}{c}{Hyperparameter}  & \multicolumn{1}{c}{Value} \\
            \hline \\
            Number of transformer blocks & 3\\
            Number of attention heads & 1 \\
            Embedding dimension & 128 \\
            Transformer activation function & GELU \\
            MLP activation function & ReLU \\
            MLP hidden layers & 2 \\
            MLP width & 128 \\
            Batch size (per task) & 8\\
            Learning rate & 1e-4\\
            Learning rate decay weight & 1e-4\\
        \end{tabular}
    \end{center}
    \caption{Hyperparameter values for PDT and DT models.}
    \label{tab:dt_hyperparameters}
\end{minipage}%
\hfill
\begin{minipage}{0.48\linewidth}
    \begin{center}
        \begin{tabular}{ll}
            \multicolumn{1}{c}{Hyperparameter}  & \multicolumn{1}{c}{Value} \\
            \hline \\
            Batch size & All data\\
            Learning rate & 1e-3\\
            Evaluation trials & 250\\
            $\epsilon$ in $\epsilon$-greedy & 0.1\\
            $c$ (exploration parameter) in UCB & 3\\
            Reward model type & MLP \\
            MLP hidden layers & 2 \\
            MLP width & 16 \\
            MLP activation function & ReLU \\
        \end{tabular}
    \end{center}
    \caption{Hyperparameters of Contextual Multi-armed Bandit}
    \label{tab:cmab_hyperparameters}
\end{minipage}
\end{table*}

\begin{table*}[ht]
    \begin{center}
    \begin{tabular}{llll}
    \multicolumn{1}{c}{Environment}  &\multicolumn{1}{c}{Target Return} &\multicolumn{1}{c}{Prompt Length $(J \times H)$ } &\multicolumn{1}{c}{Context Length}
    \\ \hline \\
    2D-Point-Sparse & 10 & $(1 \times 3), (2 \times 3), (4 \times 3)$ & 5\\
    Cheetah-vel & 0 & $(1 \times 5), (2 \times 20)$ & 20 \\
    \end{tabular}
    \end{center}
    \caption{Environment-specific hyperparameters of DT and PDT.}
    \label{tab:pdt_hyperparameters_env}
\end{table*}
% \clearpage



\section{Appendix: Additional plots}
\label{appx:additional_plots}
%
%
\subsection{Online rollouts in \texttt{Sparse 2D point}}\label{app:plots_2d_rollouts}
\begin{figure}[H]
    \centering
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/random_plot_allPromptTuningData_allTasks.png}
        \caption{Online performance of \textbf{standard PDT, without prompt-tuning}.}
        \label{fig:standardPDT_onlineRollouts_2d}
    \end{subfigure}
    \vspace{0.5em}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/EPS-features_plot_allPromptTuningData_allTasks.png}
        \caption{Online performance \textbf{with prompt-tuning}, using \textbf{$\epsilon$-greedy exploration} and \textbf{transformer features $\phi$} as segment representation for the bandit's reward models.}
        \label{fig:epsgreedy_features_onlineRollouts_2d}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/UCB-features_plot_allPromptTuningData_allTasks.png}
        \caption{Online performance \textbf{with prompt-tuning}, using \textbf{UCB exploration} and \textbf{transformer features $\phi$} as segment representation for the bandit's reward models.}
        \label{fig:ucb_features_onlineRollouts_2d}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/EPS-rawSegments_plot_allPromptTuningData_allTasks.png}
        \caption{Online performance \textbf{with prompt-tuning}, using \textbf{$\epsilon$-greedy exploration} and \textbf{raw trajectory segments} instead of transformer features for the bandit's reward models.}
        \label{fig:epsgreedy_rawSegments_onlineRollouts_2d}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/UCB-rawSegments_plot_allPromptTuningData_allTasks.png}
        \caption{Online performance \textbf{with prompt-tuning}, using \textbf{UCB exploration} and \textbf{raw trajectory segments} instead of transformer features for the bandit's reward models.}
        \label{fig:ucb_rawSegments_onlineRollouts_2d}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/hillclimbing_plot_allPromptTuningData_allTasks.png}
        \caption{Online performance \textbf{with prompt-tuning}, using \textbf{Gaussian noise} and \textbf{hill climbing} to optimize prompt selection.}
        \label{fig:hillclimbing_onlineRollouts_2d}
    \end{subfigure}
    \caption{Plots of data used in Table~\ref{tab:2d-tuning-results}, averaged over three seeds and all training tasks. The dashed line marks the optimal return at +10. }
    \label{fig:all_prompt_tuning_results}
\end{figure}
\clearpage


%%\subsection{Online rollouts Mujoco half-cheetah}\label{app:plots_mujoco_heetah}
%%\begin{figure}[H]
%%    \centering
%%    \begin{subfigure}{\linewidth}
%%        \centering
%%        \includegraphics[width=\linewidth]{imgs/}
%%        \caption{Online performance of \textbf{standard PDT, without prompt-tuning}.}
%%        \label{fig:standardPDT_onlineRollouts_cheetah}
%%    \end{subfigure}
%%    \vspace{0.5em}
%%    \begin{subfigure}{\linewidth}
%%        \centering
%%        \includegraphics[width=\linewidth]{imgs/}
%%        \caption{Online performance \textbf{with prompt-tuning}, using \textbf{$\epsilon$-greedy exploration} and \textbf{transformer features $\phi$} as segment representation for the bandit's reward models.}
%%        \label{fig:epsgreedy_features_onlineRollouts_cheetah}
%%    \end{subfigure}
%%    \begin{subfigure}{\linewidth}
%%        \centering
%%        \includegraphics[width=\linewidth]{imgs/}
%%        \caption{Online performance \textbf{with prompt-tuning}, using \textbf{$\epsilon$-greedy exploration} and \textbf{raw trajectory segments} instead of transformer features for the banditâ€™s reward models.}
%%        \label{fig:epsgreedy_onlineRollouts_cheetah}
%%    \end{subfigure}
%%    \caption{Plots of data used in Table~\ref{tab:half-cheetah-results}, averaged over three seeds and all training tasks.}
%%    \label{fig:all_prompt_tuning_results_cheetah}
%%\end{figure}


\subsection{Prompt quality experiment for the \texttt{Sparse 2D point} environment}\label{app:plots_prompt_quality}
\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.55\linewidth]{imgs/mixtureExp_legend_j1.png}
    \vfill
    %     \begin{subfigure}{0.2\textwidth}
    %    \includegraphics[width=\linewidth]{imgs/suboptimal_vs_expert_trajs.png}
    %     \caption{Mixture data}
    %    \label{fig:mixture data}
    % \end{subfigure}
    % \hfill
    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=\linewidth]{imgs/mixtureExp_random_j1.png}
        \caption{Baseline PDT, no tuning}
        \label{fig:mixture-no-tuning}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=\linewidth]{imgs/mixtureExp_epsgreedy_j1.png}
        \caption{$\epsilon$-greedy prompt tuning}
        \label{fig:mixture-eps-greedy}
    \end{subfigure}
        \hfill
    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=\linewidth]{imgs/mixtureExp_UCB_j1.png}
        \caption{UCB prompt tuning}
        \label{fig:mixture-ucb}
    \end{subfigure}
    \caption{Prompt quality experiment. Color indicates percentage of expert demonstrations in $\mathcal{P}_i$. Without prompt-tuning, PDT's performance degrades and is roughly proportional to the percentage of expert demonstrations in the dataset. Our prompt-tuning method quickly recovers and converges to near-optimal performance by finding the high-performance prompts in the mixture dataset. The shaded region denotes 1 standard deviation around the mean, averaged over three random seeds for a single training task.}
    \label{fig:results-mixture-exp}
\end{figure*}
\subsection{Out-of-distribution finetuning}\label{app:ood-training}
\begin{figure*}[h!]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\linewidth]{imgs/finetuning_action_loss.png}
        \caption{Offline action loss}
        \label{fig:2d-finetune-loss}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\linewidth]{imgs/finetuning_return.png}
        \caption{Online task return}
        \label{fig:2d-finetune-return}
    \end{subfigure}
    \caption{Fine-tuning of the PDT backbone on individual OOD tasks from our \texttt{Sparse 2D point} environment indicating a disconnect between the offline action loss and online task performance. Results are averaged for all fine-tuning tasks and PDT backbones trained with three different random seeds. The shaded area indicates one standard deviation around the mean.}
    \label{fig:finetune-training}
\end{figure*}


\end{document}
