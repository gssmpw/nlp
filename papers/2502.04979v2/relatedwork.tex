\section{Related Work}
\label{sec:related_work}
Recently, there has been a surge in methods across various domains aimed at enhancing the performance and generalization of Transformer-based approaches through automatic prompt tuning. The main challenge lies in the computationally expensive optimization of discrete prompts, guided by stochastic feedback from evaluating a black-box target model.

\textbf{Prompting in LLMs}. For LLMs, \citet{pmlr-v235-chen24e} introduced {I}nstruct{Z}ero, which uses Bayesian optimization to explore low-dimensional soft prompt vectors. These vectors are then passed to an open-source LLM to generate instructions for the black-box LLM. Building on this, \citet{lin2023use} proposed INSTINCT, which replaces the Gaussian process in Bayesian optimization with a neural network surrogate, leveraging a neural bandit algorithm to enhance expressivity. Additionally, \citet{shi2024best} demonstrated that the fixed-budget MAB framework enables learning the optimal prompt within a limited number of LLM evaluations. These methods rely on optimization in continuous spaces, followed by prompt reconstruction, whereas our approach directly operates within the DT prompt space.

Another line of research on prompt augmentation, or demonstration learning, investigates how selecting examples from a demonstration pool affects downstream performance. \citet{liu2021makes} utilized sentence embeddings to choose examples aligned with the input, while \citet{mishra2021natural} highlighted the benefits of combining both positive and negative examples for improved guidance. Approaches by \citet{kumar2021reordering} and \citet{lu2021fantastically} focused on optimizing the order of demonstrations using separator tokens and entropy-based scoring. In contrast, our method moves beyond reliance on the selection and ordering of the trajectory prompts and focuses on constructing optimal prompts by combining different segments from these trajectory prompts.

\textbf{Multi-task DT}. For RL, \citet{hu2023prompt} proposed to generate DT prompt candidates by perturbing the initial trajectory with Gaussian noise and using online or offline training to compute feedback for a ranking optimization algorithm. Similarly, the Prompt Diffuser~\citep{hu2024prompt} frames instruction optimization as a conditional generative modeling task, generating prompts starting from random noise. In contrast, our method focuses on constructing prompts from expert demonstrations, as noise-based approaches are not suitable in certain settings, such as discrete spaces. \citet{wang2024hierarchical} proposed hierarchical prompting that provides context-specific guidance through two levels of prompting: one that encapsulates task-specific information and another that uses a set of demonstration segments to guide the rollouts. Hyper-decision transformer~\citep{xu2023hyper} augmented the base DT with a hyper-network module to achieve adaptation to unseen novel tasks. However, both of those methods rely mainly on the presence of larger amounts of demonstration data, while our approach requires only a few demonstrations from which we learn to select the best prompt.

\textbf{LLM for RL}. At the intersection of LLM and RL domains, \citet{yang2024pre} and the closely related LaMo method~\citep{shi2024unleashing} proposed leveraging a pre-trained LLM as an initializer for PDT, harnessing rich linguistic knowledge to boost performance on unseen tasks. In another work, \citet{zhengdecomposed} introduced an approach to decompose the prompt into cross-task and task-specific components, ensuring more robust test-time adaptation to unseen tasks. Furthermore, the model is initialized by incorporating parameters from a pre-trained language model to provide it with prior knowledge. Compared to those, our approach avoids reliance on an additional large model, sidestepping the fine-tuning and scalability challenges inherent in such methods.

% ------------------------------------------------------------ %
\begin{figure*}[tbp]
    \centering
    \includegraphics[width=0.9\linewidth]{imgs/prompt-tuning-bandit.png}
    \caption{Overview of our inference time bandit-based prompt tuning for multi-task learning with prompting decision transformer (PDT). Each $z_i$ represents a triplet $(\hat{r}_i, \mathbf{s}_i, \mathbf{a}_i)$. The bandit explores the demonstration dataset $\mathcal{P}_i$ for the current task $i$ to find the segments $\Tilde{\tau}$ that induce the best prompt. The online return $G_k$ achieved by the underlying PDT model at round $k$ and using prompt $\rho_k$ serves as reward for the bandit. For simplicity in our illustration, we set $H=2$.}
    \label{fig:prompt_dt_bandit}
\end{figure*}