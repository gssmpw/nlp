\section{Acceleration Platform and Interface}
\label{sec:hardware}

As illustrated in Figure~\ref{fig:overall}, \name{} provides the user hardware accelerator with a wrapper around the large genomic dataset.
The primary hardware components in the architecture are the decompression accelerators and the compression accelerators, illustrated with an example configuration in Figure~\ref{fig:hardware}.
Each compression and decompression accelerator needs access to a fixed-size buffer in HBM, so they must be coupled with sets of HBM pseudo channels (PC).
This is described in more detail in the following sections.

For an FPGA deployment of \name{}, the number of compressors and decompressors in the system can be configured for each application's requirements, within the physical boundaries of the platform.
For example, the prototype's U50 platform has 32 HBM channels, 256~MB each, and the human reference genome requires the capacity of three channels.
As a result, up to 10 decompressors can be instantiated.

\begin{figure}[htb]
    \centering
    \includegraphics[width=.65\columnwidth,page=5]{figures/figures-swjun-crop.pdf}
    \caption{An example design with three decompressors (D) and one compressor, consuming 17 HBM PCs.}
    \label{fig:hardware}
\end{figure}


\subsection{Compression Accelerator}
\label{sec:compression_arch}

Due to the memory capacity requirement of the cuckoo hash offset table, \name{} divides the compression process between hardware acceleration and the software manager.
For example, the cuckoo hash table to encode the human genome is at least 16~GBs, leaving no space even on medium-range FPGA platforms such as Alveo U55C.
Instead, it is left to the software to maintain and query the offset table in memory, perform the reference comparisons, and encode the compressed data, as described in Figure~\ref{fig:fixed-width}.
The hardware accelerator is responsible for providing a stream of hash values and supplementary information to the software over PCIe.
The hardware component is mainly responsible for two tasks: Computing the hash values for the cuckoo hash, and performing probabilistic filtering to reduce the load on the software.

Figure~\ref{fig:compression_hardware} shows the architecture of the hardware accelerator.
The accelerator ingests input data either in the form of ASCII-encoded text such as FASTA, or as a 2-bit binary encoded format generated by user kernels.
ASCII input is first encoded in binary format, and a stride shifter emits a stream of k-mers into an array of four pipelined hash function modules.
Since our prototype uses a $K$ value of 64, the datapath between the stride shifter and the four hash functions is 128 bits wide.
The hardware needs four hash functions because two hashes are needed to query the cuckoo hash, and two more are needed to query the cuckoo hash based on the reverse complement of the k-mer.
The computed hash values are independently routed to one of many memory controllers, each interfacing with an HBM PC, to check the probabilistic filter.
The filter results, along with the original k-mer as emitted by the stride shifter, are collected at the outbound encoder to organize them into a software-friendly format.
By default, \name{} uses the Murmur3 hash function to achieve both high software and hardware throughput.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\columnwidth,page=6]{figures/figures-swjun-crop.pdf}
    \caption{Hardware portion of the compressor.}
    \label{fig:compression_hardware}
\end{figure}

The probabilistic filter is not part of the algorithm as described in Section~\ref{sec:algorithm}, but a transparent optimization to improve effective software performance.
Its task is to reduce the number of hash values the software has to verify.
This is done via another, much smaller cuckoo hash table in accelerator memory, as depicted in Figure~\ref{fig:compression_hardware}.
The four hash values point to the same index as they would in the software-side hash table, but each element in the accelerator table is only 4 bits long.
Instead of storing the 4-byte offset into the reference, the 4 bits hold a substring of the reference itself.
Specifically, it holds the least significant 4 bits of the reference k-mer which maps to this location.
More specifically, given a k-mer ``\texttt{str}'', which generates four hash values \texttt{hash$_1$} to \texttt{hash$_4$}, if \texttt{str[3:0]} does not match \texttt{filter\_table[hash$_1$]}, then we can be sure that software does not need to check \texttt{hash$_1$}.
The performance impact of this optimization is quite significant, as presented in Section~\ref{sec:compression_performance}.
As a result, the compression accelerator needs about 2~GB of HBM memory, as depicted in Figure~\ref{fig:hardware}.

Because 2~GB of HBM memory is spread across multiple pseudo channels (256~MB each in our prototype), which can operate in parallel to achieve very high aggregate bandwidth, one may worry that the encoder and its datapath may not be fast enough to keep up with memory bandwidth.
Unfortunately, the inherently random, fine-grained 4-bit table lookups result in very low effective memory performance.
Not only is random access inherently slower on DRAM technologies, including HBM~\cite{hbm_xilinx}, but the minimum burst length of HBM is typically long, around 256 bits, resulting in severe I/O amplification.
This is one of the reasons hash table lookups need to be filtered using fast accelerator HBM, instead of the host software working with larger, but slower DDR memories.

The software needs to compare the target k-mer with the potential reference k-mers discovered via the cuckoo hash, but the software may not have access to the target k-mers.
This may be because genomic data is being generated by the user kernel, or because the target exists in ASCII format, which is computationally expensive to compare against, which we demonstrate in Section~\ref{sec:compression_performance}.
To overcome this limitation, the hardware sends both the target k-mer, along with the four hashes to the software for comparison.
Since the k-mer for the last shift would exist in software already, each shift only needs to send the next 32-bit stride.
In our prototype configuration of $K=64$, this means each stride shift results in 20 bytes of data transfer (4 bytes $\times$4 hash values + 4-byte stride).

The remaining process of compression and data organization in software is described in more detail in Section~\ref{sec:software}.


\subsection{Decompression Accelerator}
\label{sec:decompression_arch}

The architecture of each decompression accelerator is very simple despite the high performance, thanks to the algorithmic modifications that simplify hardware design.
Figure~\ref{fig:decompression_hardware} presents the architecture of a compression accelerator.
The accelerator scans through both the header and payload of the compressed input using a sliding window of width 4.
The accelerator needs to consider a window of slots at once, because the k-mer loaded from memory for each encoded index is only 64 bases, or 128 bits.
Sending a memory request for each index would severely degrade performance if the memory bus is wider than this.
For example, our FPGA prototype running at 250~MHz needs a 512-bit memory bus to make the best use of memory bandwidth, and transmitting 128-bits per cycle will waste too much bandwidth.
Similarly, verbatim encoded data is stored in units of 32 bits, and decoding one such element per cycle will lead to bandwidth waste.

The sliding window parser considers four header bits at once, and if it discovers consecutive verbatim encodings, or consecutive continuations, it can either decode up to four verbatim encodings in one cycle, or issue a single 512-bit memory request instead of four requests over four cycles.
We emphasize that this is made possible thanks to both strategies 1 and 2 in Section~\ref{sec:algorithm}, fixed-width encodings and grouped headers.
Having grouped headers allows the parser to quickly determine consecutive elements, and fixed encoding width not only makes headers small since size information does not need to be encoded, but it also makes it simple to slide a fixed-width window through both header and payload.

On the other hand, if each header element was interleaved with variable-width payload elements, the next header can only be decoded \emph{after} the current payload was decoded, making it difficult to scan a wide window at once.
Furthermore, looking at the next element also would require variable-width shifting, which is known to be resource-heavy and slow.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\columnwidth,page=7]{figures/figures-swjun-crop.pdf}
    \caption{Decompression hardware accelerator.}
    \label{fig:decompression_hardware}
\end{figure}

Memory read responses and the parsed verbatim elements are collected at the shuffler, which uses a multi-cycle pipelined shifter and a compaction module based on sorting networks to shift and concatenate data across different cycles and remove gaps.
The resulting output is a gap-free continuous stream of genomic data, which is naturally expected by the user kernel.

We note that although three HBM channels (250~MB each) are assigned to each decompressor to accommodate the whole human reference genome, the single 512-bit output bus of the decompressor never seems to become a bottleneck.
On the one hand, three memory channels operating at peak sequential bandwidth will overrun the decompressor's output by almost a factor of three.
However, we experienced with our FPGA prototype implementation that the random nature of the reference lookups results in low effective bandwidth of each channel, resulting in a good balance between the bus widths.
For this reason, the reference stored in HBM for one decompressor cannot be shared with another decompressor.
Since memory bandwidth is already being fully utilized, two decompressors sharing a reference memory will cause immediate performance degradation.
More detailed insight into bandwidth usage is provided in Section~\ref{sec:compression_performance}.

