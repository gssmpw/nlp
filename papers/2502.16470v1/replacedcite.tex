\section{Background and Related Works}
\label{sec:background}

\subsection{Computational Genomics and its Applications}

The increasingly wide availability of personal genomic data, coupled with rapid algorithmic advancements related to computational genomics, are enabling many important and useful applications, including personalized and precision medicine____, wide-scale genome-wide association studies
____ for disease analysis, future agriculture____, and tracing and managing pandemics____.
Emerging solutions suggest highly effective personalized therapies for previously untreatable diseases may become available in the near future____.

The fundamental task of computational genomics is to extract insight from the output available from modern sequencing technologies.
Because we cannot simply read genomic data from cell samples in a single complete string yet, what sequencing machines do is generate vast amounts of randomly sampled substrings from the whole genome.
These substrings are called ``reads'', and we cannot know where they were sampled from, and they may contain noise and other probabilistic errors.
Based on the average length of reads, sequencing technologies are categorized into short reads (hundreds or less) or long reads (thousands to millions).
Furthermore, since the DNA is a double helix consisting of two complementary strands, a read may come from either strand, resulting in either a ``forward'' read, or a ``reverse complement'' read.
A forward read can be computed from the reverse read and vice versa, and the existence of opposite directions in the sample must be considered during analysis.

The first task of computational genomics is typically to reconstruct the reads into a whole genome while removing noise and errors, typically using numerous overlapping samples, in a process called ``sequence assembly''.
Assembly can be done using a pre-assembled reference genome of an organism to take advantage of similarities between individuals (``reference-based'' assembly) or just between the reads themselves ``De Novo'' assembly), either because a reference is not available, or to avoid errors such as ``reference drift''____.
Both approaches depend on ``sequence alignment'', discovering locations with similar substrings.


\subsection{Genomics and Acceleration}

Since genome analysis is becoming essential in various fields of human life, while the price of getting genomic sequence data has been decreasing, it is becoming extremely difficult to analyze the enormous size of the genome sequence data without acceleration.
Much prior research has introduced acceleration to sequence alignment, read mapping, classification, and structure similarities for faster analysis via various acceleration platforms, such as GPUs____, FPGAs____, ASICs____, PIMs____, Quantum computing____, and In-Storage processing____.

Among the acceleration platforms, GPUs and FPGAs are the most popular platforms because they are more affordable and off-the-shelf available.
Thanks to the capability to compose fine-grained application-specific reconfigurable logic, FPGAs often show better performance and power efficiency than GPU-based accelerators, especially for the representative genome analysis algorithm, sequence alignment____. 


\subsection{Memory and Storage Systems}

Along with the looming end of Moore's law scalability, the future of memory scalability is also uncertain.
Density of capacitor-based DRAM in the 2D space is expected to stop scaling, with no feasible alternative____.
One direction of continued improvement is through 3D stacking, for memories such as High Bandwidth Memory (HBM), where bandwidth continues to scale with ever-wider communication bandwidth using Through-Silicon Vias (TSV).
However, while bandwidth continues to scale, the height of 3D stacking faces physical limitations, making capacity scaling more uncertain____.
This is a concerning prospect as the capacity scale of problems such as computational genomics continues to grow.
As a result, heterogeneous systems combining DRAM, NVMe, and more are under active investigation____, as well as memory disaggregated over a network, using emerging fabrics such as CXL____.


\subsection{Genome Compression}

As the scale of genomic data continues to grow, compression is becoming more important to reduce both data archiving and transmission.
Unfortunately, conventional data-agnostic compression algorithms dictionaries and Huffman coding have shown very little compression efficiency due to the high entropy of genomic data____.

Many genome-specific compression schemes are under active investigation, and they can largely be categorized into two groups.
The first group is reference-free compression, where the algorithm only looks at the genome to compress, and takes advantage of statistical trends it can find____.
These schemes typically require little memory, but achieve low compression efficiency.
The second group is reference-based compression, which uses a reference, (or ``consensus'') and takes advantage of the similarity between individuals of the same species to encode only the differences____.
These schemes often achieve very high compression ratios in the hundreds, but require more memory because the entire reference needs to be memory-resident.
Other approaches are being explored as well, including using machine learning models ____.

Acceleration of genome compression is also under active investigation to overcome the slow speed of these algorithms, using FPGAs____, and GPUs____.