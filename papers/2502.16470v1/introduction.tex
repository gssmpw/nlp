\section{Introduction}
\label{sec:introduction}

The rapid advancement of genome sequencing technologies has led to an explosion of collected genomic data, projected to become one of the largest and computationally demanding areas of computing in the next decade~\cite{stephens2015bigdatagenomics}.
Figure~\ref{fig:growth} presents both the exponential reduction of genome sequencing costs~\cite{dna_sequencing_cost}, and the corresponding exponentially increasing size of genomic databases~\cite{genbank_bases}.
In fact, the projected growth of genomic data is expected to outpace even Moore's law, growing faster than the rates of both computational power and memory capacity scaling~\cite{chen2020parc,kaplan2020bioseal,turakhia2018darwin}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\columnwidth,page=8]{figures/figures-swjun-crop.pdf}
    \caption{Exponentially cheaper sequencing leads to exponential growth in data.}
    \label{fig:growth}
\end{figure}

It is crucial to address the gap between data growth and computational capacity scaling, considering that some useful genomic analytics tasks are already too computationally expensive for real-world deployment.
Exceptional advancements beyond Moore's law must be made to ensure we can actually take advantage of the increasing availability of personal genomic data and rapid algorithmic advancements.
One representative example is De Novo sequence assembly, assembling genome reads among themselves without depending on a reference.
De Novo assembly can improve the quality of personalized genomic data analysis to levels sufficient for effective personalized medicine~\cite{ashley2016towardsprecisionmedicine,chaisson2015geneticdenovo}, but their computational overhead and cost are already considered too high to be viable in a clinical setting~\cite{ashley2016towardsprecisionmedicine}.

Because data growth is outpacing Moore's law, existing hardware and software solutions will not be sufficient to enable the benefits that genomics data may provide us.

One promising solution is computation accelerators as coprocessors.
A wide range of accelerators for genomics are being explored for various genomics applications, taking advantage of highly parallel fabrics including such as Graphic Processing Units (GPU)~\cite{guo2013gpu,liu2009cudasw++,liu2010cudasw++,jia2011metabing,liu2013cudasw++,cho2013xsd,su2014gpu,liu2015gswabe,wilton2015arioc,de2016cudalign,nishimura2017accelerating,kobus2017accelerating,houtgast2017efficient,cheng2018bitmapper2,ahmed2019gasal2,zeni2020logan,gamaarachchi2020gpu,kobus2021metacache,wang2023gpmeta,ju2024seedhitgpu}, Field-Programmable Gate Arrays (FPGA)~\cite{arram2015fpgareferencecompressiongenomic,chen2023efficientsequencingcompressionfpga,chen2014accelerating,zhao2017streaming,liao2018adaptively,qiao2019fpga,chen2021high,haghi2021fpga,li2021pipebsw,liyanage2023efficient}, Application-Specific Integrated Circuits (ASIC)~\cite{madhavan2014race,turakhia2017darwin,turakhia2018darwin,huangfu2018radar,fujiki2018genax,turakhia2019darwin}, Processing-In-Memory (PIM)~\cite{zokaee2018aligner,angizi2019aligns,gupta2019rapid,kaplan2020bioseal,chen2020parc,cali2020genasm,khatamifard2021genvom,li2021pim,wu2021sieve,shahroodi2022krakenonmem,shahroodi2022demeter,hanhan2022edam,zou2022biohd,jahshan2023dash,zhang2023aligner} and more~\cite{sarkar2019algorithm,sarkar2021quaser,mansouri2022genstore,varsamis2023quantum,wu2024abakus,ghiasi2024megis}.
These accelerators not only supply high computational throughput, but are also equipped with high-performance memory fabric, such as High-Bandwidth Memory (HBM), to satisfy both the computational and data access performance requirements of genomics applications.

Unfortunately, the performance of conventional accelerators equipped with high-performance on-device memory is often limited by the data movement overhead when the dataset exceeds on-device memory~\cite{zhang2019flashgpu,dhar2019near,lee2020optimizing,jonatan2024scalability}.
This is concerning because memory capacity scalability is unclear in the near future~\cite{kim2015architectural,shiratake2020scaling,kim2024present,hyun2024pathfinding}.
For workloads with low locality, every new access exceeding local memory must be transported over either PCIe or network links, resulting in orders of magnitude performance degradation due to orders of magnitude slower than 3D-stacked HBM.
Many important genomics applications fall into this category, including long read sequence alignment, pre-alignment filtering, overlap graph construction, and more.
In these applications, each read is loaded into accelerator memory, processed once, and then discarded to make space for the next data.
Indeed, many accelerator designs have pointed to data movement overhead as the primary bottleneck of performance~\cite{mailthody2019deepstore,liang2019ins,zhang2020dram,fang2020memory,preethi2020fpga,kim2021behemoth,singh2021fpga,zhou2022survey,knoben2023improving}.

In this work, we address these issues with \textbf{\name{}}, a genomics accelerator platform which provides application-specific hardware accelerators with high-bandwidth access to genomics data beyond on-device memory capacity.
\name{} introduces a novel, hardware-codesigned compression algorithm with low hardware overhead, whose hardware implementation works together with a software interface to mitigate the bandwidth gap between on-device memory and host-side PCIe or CXL/network interfaces.
As a result, \name{} provides practically unlimited capacity via NVMe storage or disaggregated memory, at DRAM-level bandwidth.
Figure~\ref{fig:overall} illustrates an example accelerator configuration which provides the user kernel with three parallel for data input.
The number of compressor/decompressor modules can be configured according to the requirements of the application.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\columnwidth,page=2]{figures/figures-swjun-crop.pdf}
    \caption{User kernels have access to unlimited high-performance memory capacity over compressed PCIe.}
    \label{fig:overall}
\end{figure}

The compression/decompression module of \name{} achieves multiple conflicting criteria, including compression efficiency, compression, and decompression throughput, and low hardware resource utilization, by using a reference-based algorithm with hardware-aware optimizations such as fixed-width grouped header encoding, fixed-k k-mer based matching as well as wide mismatch strides.
The compression module also uses a hardware-accelerated probabilistic filter to reduce the pressure on the memory system.
All of the hardware accelerators are managed by a host-side software management layer, which maintains the data structures for the random accesses into the genome data, as well as orchestrate the data movement between the compressed genome database and the user-provided application-specific hardware accelerator kernel.

We evaluate \name{} with an FPGA prototype using the Xilinx Alveo U50 board, on various genomic workloads, including pre-alignment filtering, using long read and reference genomes from various organisms.
The U50 is a useful and representative evaluation platform thanks to its realistic PCIe Gen~3 $\times$16 interface and 3D-stacked HBM memory, both of which are the typical limiting factors in data-intensive analytics.
It is also useful as a deployment target for \name{} by itself, since it is a mid-range FPGA affordable to many potential users, and FPGAs have often demonstrated superior performance and power efficiency compared to other programmable accelerators such as GPUs, on many representative genomics-related workloads~\cite{chen2016spark,fei2018fpgasw,rucci2018swifold,wu2019fpga,fujiki2020seedex}.

\name{} on the FPGA prototype can provide a user kernel with practically unlimited memory capacity, divided into multiple channels similar to HBM.
The physical constraints of the U50 FPGA allow up to ten channels, each delivering up to 16~GB/s of genomic data bandwidth, adding up to 160~GB/s bandwidth without running into the PCIe bottleneck.
This is about 31\% of the peak HBM performance, while consuming only 30\% of the chip space.
This is made possible by the hardware co-optimized compression algorithm achieving high compression ratios between 24$\times$ to 100+$\times$, competitive with state-of-the-art schemes.
We note that the power constraints specific to the U50 platform limit the accessible HBM bandwidth to less than half the peak bandwidth ($\sim$200~GB/s), also limiting the nominal \name{} bandwidth to 70~GB/s.
The throttled speed is still an order of magnitude higher than the PCIe bandwidth limitation, and near peak memory bandwidth that the DDR4 memory system can deliver on the higher-end U250 FPGA.
We note that this power throttling does not exist on other HBM-enabled FPGAs, such as the U55C or U280.
\name{} also achieves high compression performance of up to 3.7~GB/s, an order of magnitude faster than any existing FPGA genomic compression accelerator.

We also demonstrate overall system performance improvements on an important real-world workload of pre-alignment filtering for long-read alignment, and demonstrate almost 6$\times$ performance improvement over a conventional FPGA accelerator within the constraints of the U50, and almost 15$\times$ if we assume no HBM power throttling.

\textbf{Implications of \name{} are profound.}
Evaluation results show that using a small, fixed amount of on-device HBM memory, \name{} can deliver user accelerator kernels with memory-class bandwidth to practically unlimited capacity of genomics data via host-side NVMe, CXL, or disaggregated memory.
This has the attractive implication that achieving data scalability can be separated from memory capacity scalability, which is expected to be unfortunately difficult in the near future~\cite{kim2015architectural,shiratake2020scaling,kim2024present,hyun2024pathfinding}.

The rest of this paper is organized as follows:
First, we present background and related works in Section~\ref{sec:background}.
We present the \name{} compression and decompression algorithms in detail in Section~\ref{sec:algorithm}.
The software and hardware components of the \name{} system are described in Section~\ref{sec:software} and Section~\ref{sec:hardware}, respectively.
We provide evaluations and comparative results in Section~\ref{sec:evaluation}, and conclude with discussions in Section~\ref{sec:conclusion}.

