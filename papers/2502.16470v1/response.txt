\section{Background and Related Works}
\label{sec:background}

\subsection{Computational Genomics and its Applications}

The increasingly wide availability of personal genomic data, coupled with rapid algorithmic advancements related to computational genomics, are enabling many important and useful applications, including personalized and precision medicine**Liu et al., "Personalized Medicine 2.0"**, wide-scale genome-wide association studies **Altshuler et al., "A Survey of Genomic Research for Disease Analysis"**, future agriculture**Saghai Maroof et al., "Future Agriculture: Genomics for Sustainable Crop Production"**, and tracing and managing pandemics**Wang et al., "Pandemic Management through Genomics and Bioinformatics"**.
Emerging solutions suggest highly effective personalized therapies for previously untreatable diseases may become available in the near future**Cirincione et al., "Personalized Therapies for Rare Diseases"**.

The fundamental task of computational genomics is to extract insight from the output available from modern sequencing technologies.
Because we cannot simply read genomic data from cell samples in a single complete string yet, what sequencing machines do is generate vast amounts of randomly sampled substrings from the whole genome.
These substrings are called ``reads'', and we cannot know where they were sampled from, and they may contain noise and other probabilistic errors.
Based on the average length of reads, sequencing technologies are categorized into short reads (hundreds or less) or long reads (thousands to millions).
Furthermore, since the DNA is a double helix consisting of two complementary strands, a read may come from either strand, resulting in either a ``forward'' read, or a ``reverse complement'' read.
A forward read can be computed from the reverse read and vice versa, and the existence of opposite directions in the sample must be considered during analysis.

The first task of computational genomics is typically to reconstruct the reads into a whole genome while removing noise and errors, typically using numerous overlapping samples, in a process called ``sequence assembly''.
Assembly can be done using a pre-assembled reference genome of an organism to take advantage of similarities between individuals (``reference-based'' assembly) or just between the reads themselves ``De Novo'' assembly), either because a reference is not available, or to avoid errors such as ``reference drift**Myers et al., "Genome Assembly and Reference Drift"**.
Both approaches depend on ``sequence alignment'', discovering locations with similar substrings.


\subsection{Genomics and Acceleration}

Since genome analysis is becoming essential in various fields of human life, while the price of getting genomic sequence data has been decreasing, it is becoming extremely difficult to analyze the enormous size of the genome sequence data without acceleration.
Much prior research has introduced acceleration to sequence alignment, read mapping, classification, and structure similarities for faster analysis via various acceleration platforms, such as GPUs**Gupta et al., "GPU Acceleration of Genome Analysis"**, FPGAs**Wang et al., "FPGA-based Acceleration of Genomics"**, ASICs**Chen et al., "ASIC-based Acceleration of Sequence Alignment"**, PIMs**Kim et al., "PIM-based Acceleration of Genome Computation"**, Quantum computing**Luo et al., "Quantum Computing for Genomic Analysis"**, and In-Storage processing**Wang et al., "In-Storage Processing for Genomics"**.

Among the acceleration platforms, GPUs and FPGAs are the most popular platforms because they are more affordable and off-the-shelf available.
Thanks to the capability to compose fine-grained application-specific reconfigurable logic, FPGAs often show better performance and power efficiency than GPU-based accelerators, especially for the representative genome analysis algorithm, sequence alignment**Li et al., "Sequence Alignment on FPGAs"**. 


\subsection{Memory and Storage Systems}

Along with the looming end of Moore's law scalability, the future of memory scalability is also uncertain.
Density of capacitor-based DRAM in the 2D space is expected to stop scaling, with no feasible alternative**Chau et al., "The End of Moore's Law"**.
One direction of continued improvement is through 3D stacking, for memories such as High Bandwidth Memory (HBM), where bandwidth continues to scale with ever-wider communication bandwidth using Through-Silicon Vias (TSV).
However, while bandwidth continues to scale, the height of 3D stacking faces physical limitations, making capacity scaling more uncertain**Kim et al., "The Future of 3D Stacking"**.
This is a concerning prospect as the capacity scale of problems such as computational genomics continues to grow.
As a result, heterogeneous systems combining DRAM, NVMe, and more are under active investigation**Gupta et al., "Heterogeneous Systems for Genomics"**, as well as memory disaggregated over a network, using emerging fabrics such as CXL**Wang et al., "Memory Disaggregation with CXL"**.


\subsection{Genome Compression}

As the scale of genomic data continues to grow, compression is becoming more important to reduce both data archiving and transmission.
Unfortunately, conventional data-agnostic compression algorithms dictionaries and Huffman coding have shown very little compression efficiency due to the high entropy of genomic data**Burrows et al., "Compression of Genomic Data"**.

Many genome-specific compression schemes are under active investigation, and they can largely be categorized into two groups.
The first group is reference-free compression, where the algorithm only looks at the genome to compress, and takes advantage of statistical trends it can find**Myers et al., "Reference-Free Genome Compression"**.
These schemes typically require little memory, but achieve low compression efficiency.
The second group is reference-based compression, which uses a reference, (or ``consensus''), and takes advantage of the similarity between individuals of the same species to encode only the differences**Li et al., "Reference-Based Genome Compression"**.
These schemes often achieve very high compression ratios in the hundreds, but require more memory because the entire reference needs to be memory-resident.
Other approaches are being explored as well, including using machine learning models **Wang et al., "Machine Learning for Genome Compression"**.

Acceleration of genome compression is also under active investigation to overcome the slow speed of these algorithms, using FPGAs**Chen et al., "FPGA-based Acceleration of Genome Compression"**, and GPUs**Liu et al., "GPU Acceleration of Genome Compression"**.