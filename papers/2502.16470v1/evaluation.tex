\section{Evaluation}
\label{sec:evaluation}

We evaluated \name{} using a prototype constructed using an affordable Xilinx U50 Datacenter FPGA accelerator card with 8~GB of 3D-stacked HBM, plugged into either a desktop-class or server-class computers with up to 48 Xeon cores and hundreds of GB of DDR4 memory.
The U50 FPGA card is plugged into the host machine via a PCIe Gen3x16 connection, where our framework was able to achieve a steady-state transfer rate of 9.1 GB/s download and 8.4~GB/s upload.


\subsection{Resource Utilization}

Table~\ref{tab:resource_utilization} presents the resource utilization of the \name{} compressor and decompressor modules.
These resources are needed in addition to the baseline utilization by the Xilinx XRT platform.
Because both modules depend on in-memory data structures for reference and probabilistic filtering, the primary resource limitation is HBM.
We demonstrate below that \name{} can achieve memory-class performance even within the stringent resource limitations of the affordable U50 device.

\begin{table}[htb]
    \centering
    \caption{Resource Utilization on the U50 FPGA.}
    \label{tab:resource_utilization}
    \begin{tabular}{|l||c|c|c|}
        \hline
        Module & LUT & DSP & HBM Channels \\
        \hline
         Compressor $\times$1 &  36K (4.14\%) & 480 (8\%) & 8 (25\%) \\
         Decompressor $\times$1 & 26K (2.99\%) & 4 (0.07\%) & 3 (9.3\%) \\
        \hline
    \end{tabular}
\end{table}


\subsection{Evaluated Datasets}

Table~\ref{tab:evaluated_datasets} presents the datasets we have used to evaluate \name{}, spanning reference and long-read datasets from human, mouse, and corn genomes.
We note that we also made use of three additional reference genomes, HG19~\cite{raney2024ucsc}, GRCm39~\cite{GRCm39}, and B73~\cite{B73}.
These were used as compression references, and not included in the analysis. 

\begin{table}[htb]
    \centering
    \caption{Evaluated datasets}
    \label{tab:evaluated_datasets}
    \begin{tabular}{|l|c|c|c|c}
        \hline
        Dataset & Organism & Type & Size (GB) \\
        \hline
        \hline
         HG16~\cite{raney2024ucsc} & Human & Reference & 2.67 \\
         HG38~\cite{raney2024ucsc} & Human & Reference & 2.75 \\
         \hline
         HG002~\cite{HG002} & Human & Long-Read & 121.19 \\
         HG003~\cite{HG003} & Human & Long-Read & 204.78 \\
         HG004~\cite{HG004} & Human & Long-Read & 205.62 \\
         mouse-rep1~\cite{mouse-rep1} & Mouse & Long-Read & 129.85 \\
         mouse-rep2~\cite{mouse-rep2} & Mouse & Long-Read & 131.48 \\
         maize-B73-rep1~\cite{maize-B73-rep1} & Maize & Long-Read & 130.77 \\
        \hline
    \end{tabular}
\end{table}


\subsection{Compression Efficiency Evaluation}
\label{sec:compression_efficiency}

We first present the compression efficiency of \name{}'s hardware-optimized genome compression algorithm.
The algorithm is defined with two parameters: stride $S$ and K-mer width $K$.
We performed exhaustive problem space exploration to determine the optimal $K$ and $S$ parameters, which we summarize below.
We note all compression ratios are based on the ASCII text (8 bits per base)  representation.

\subsubsection{Impact of stride width}
Figure~\ref{fig:compression_efficiency_stride} represents the compression efficiency with different stride configurations, for different datasets.
Each dataset uses a reference genome appropriate for its organism.
$K$ is fixed to 64 in this presentation, but similar behavior was observed with different $K$ values.
Measurements show that $S$ values of 8 and 16 yield the best compression efficiencies.
\name{} uses 16 by default for $S$, since the stride of 16 results in 32-bit verbatim encoding, which is the same width as the reference genome index for matches.
This makes the hardware decoder design more compact.

The reason smaller $S$ values result in lower compression is due to the header overhead.
While smaller strides result in slightly more k-mers being matched to the reference, it also results in more header bits because each verbatim encoding is smaller.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\columnwidth,page=1]{figures/figures-smlim-crop.pdf}
    \caption{Compression efficiency with different $S$ (Log-scale).}
    \label{fig:compression_efficiency_stride}
\end{figure}

\subsubsection{Impact of $K$ width}
Figure~\ref{fig:compression_efficiency_kmer} presents the compression efficiency of \name{} with different $K$ values.
$S$ is fixed to 16 in these experiments.
The results fall into two different categories.
When compressing reference genomes, (HG16 and HG38) larger $K$ typically yields better compression.
For long-read sequences, compression efficiency peaks at 64 and then declines.
This difference appears to come from quality differences between the two categories.
The pre-assembled references have most read errors and other noise removed, and comparison between them can take advantage of the genetic similarity between the two individuals.
On the other hand, long-read datasets still have numerous read errors and other noises, which makes long matches less likely.

Most experiments with long reads show the best compression at $K$ of 64 (Human, Mouse) and 128 (Maize).
We choose the $K$ value of 64 by default, to emphasize its use for Humans.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\columnwidth,page=2]{figures/figures-smlim-crop.pdf}
    \caption{Compression efficiency with different $K$ (Log-scale).}
    \label{fig:compression_efficiency_kmer}
\end{figure}

\subsubsection{Comparison against state-of-the-art}
Figure \ref{fig:compression_efficiency_comparison} evaluates the compression efficiency of \name{} against other state-of-the-art genome-specific compression tools.
SPRING (2018, \cite{chandak2018spring}) and CoLoRdN (2022, \cite{kokot2022colord}) are reference-free algorithms, while CRAM (2022, \cite{bonfield2022cram}) and CoLoRdR (2022, \cite{kokot2022colord}) are reference based.
\name{} was configured with the default parameter of $S=16$, $K=64$.
We emphasize that the y-axis is a log scale.
The \name{} compression algorithm, even with its optimizations geared towards efficient hardware implementations, achieves competitive efficiency compared to state-of-the-art algorithms.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\columnwidth,page=3]{figures/figures-smlim-crop.pdf}
    \caption{Compression efficiency comparison with state-of-the-art tools (Log-scale).}
    \label{fig:compression_efficiency_comparison}
\end{figure}


\subsection{Compression/Decompression Performance}
\label{sec:compression_performance}

Here, we present the performance of the compression and decompression accelerators implementing the \name{} algorithm.

\subsubsection{Decompression performance}
Figure~\ref{fig:single_decompressor_throughput} presents the throughput of a single decompressor module, with different datasets.
It also presents the bandwidth pressure put on the PCIe to achieve each decompressor output.
Thanks to the simple hardware design facilitated by the algorithmic optimizations, each decompressor can very nearly saturate the 512-bit output bus for all datasets tested, while consuming only 3\% of the chip space on the U50 FPGA.
Such high performance is also achieved while putting minimal bandwidth on the PCIe bus, thanks to the high compression efficiency.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\columnwidth,page=4]{figures/figures-smlim-crop.pdf}
    \caption{Single decompressor throughput.}
    \label{fig:single_decompressor_throughput}
\end{figure}

We also present the decompressor performance when working with realistic ensembles of genomic datasets.
Figure~\ref{fig:ensemble_decompressor_throughput} presents the total bandwidth achieved with \name{}, when working with multiple datasets in parallel through multiple decompressor channels.
The evaluated ensembles are described in Table~\ref{tab:decompressor_set_1}.
The names of each set describe how many references are used, and how many long read datasets are used (e.g., ``1R3'' means one reference and three long reads).
Each set represents a type of analysis.
For example, 2R2-1 represents a multi-reference alignment workload with a large read dataset, and 1R3-4 represents aligning a very large read dataset against a single reference, and 0R4-1 represents pairwise alignments for De Novo assembly.
We note that datasets for different organisms are not distinguished for this experiment.
Each dataset is used simply as a real-world dataset with different compression characteristics.

\begin{table}[htb]
    \centering
    \caption{The sets of decompressors}
    \label{tab:decompressor_set_1}
    \begin{tabular}{|l|c|}
        \hline
         Set & Composition \\
        \hline
         2R2-1 & HG16+HG38+HG002+HG003 \\
         2R2-2 & HG16+HG38+HG004+mouse-rep1 \\
         1R3-1 & HG38+HG002+HG003+HG004 \\
         1R3-2 & HG38+HG002+mouse-rep1+mouse-rep2 \\
         1R3-3 & HG38+mouse-rep1+mouse-rep2+maize-B73-rep1 \\
         0R4-1 & HG002+mouse-rep1+mouse-rep2+maize-B73-rep1 \\
        \hline
    \end{tabular}
\end{table}

Figure~\ref{fig:ensemble_decompressor_throughput} presents the total decompression performance across four decompressor modules, for each ensemble.
We can see that all configurations reach close to the peak bandwidth of four 512-bit buses (64~GB/s).
More importantly, none of the configurations reach the PCIe bandwidth bottleneck, as presented by the PCIe pressure bars.
0R4-1 comes close, but is still below the 8.4~GB/s upload limit.

We note that two copies of 1R3-1 or 0R4-1 are possible to deploy within the physical limitations of the U50, and such a configuration will overrun the PCIe bandwidth limitation, due to the relatively low compression efficiency.
But fortunately or unfortunately, the power throttling of the U50 HBM limits the performance before we run into this bottleneck, as presented in the following section.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\columnwidth,page=5]{figures/figures-smlim-crop.pdf}
    \caption{Ensemble decompressor throughput.}
    \label{fig:ensemble_decompressor_throughput}
\end{figure}

Figure~\ref{fig:throttled_bancroft} shows the best performance \name{} can achieve on the U50 platform, and compares it against other comparable configurations.
Here, we experiment with one long read dataset (HG002) aligned against multiple reference genomes, in a multi-reference alignment situation.
This was chosen as the best-case, but still realistic example, which can take advantage of the high compression efficiency on references to avoid the PCIe bandwidth limitation.
We notice that the real-world measured throughput of \name{} on the U50 stalls around 70~GB/s, due to the limited 7.8~W HBM power budget on the U50.
This is around 34\% of the nominal real-world performance of the U50 HBM, when the dataset is loaded in an uncompressed format directly from the HBM.
The measured performance of \name{} is similar to the peak memory bandwidth available on the higher-end Alveo U250 FPGA platform equipped with DDR4 memory.

On devices with no power throttling, the ideal bandwidth attainable from this HBM device is 512~GB/s.
On such a device, \name{} can operate up to ten decompressors in parallel, only limited by the HBM capacity necessary to store the compression reference genome.
Across 10 decompressors, \name{} would achieve 160~GB/s of total throughput, which is 31\% of the peak HBM bandwidth.
We emphasize again that the compression reference cannot be shared across multiple decompressors, because the primary limitation to an individual decompressor's performance is random access into the compression reference.
Sharing a reference across two decompressors will effectively halve the performance of each decompressor.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\columnwidth,page=13]{figures/figures-swjun-crop.pdf}
    \caption{Impact of HBM power throttling on peak HBM throughput and \name{} performance.}
    \label{fig:throttled_bancroft}
\end{figure}

We also present the performance comparison against a GPU implementation in the following section, after compression performance evaluation.

\subsubsection{Compression performance}
\name{}'s compressor design chose to partition the work between the host software and the accelerator hardware, because the cuckoo hash table used during compression was too large to comfortably keep on the accelerator memory.
Figure~\ref{fig:compression_performance} compares the performance between different partitioning of work, and how many host CPU threads are necessary to reach maximum performance.

We can see that the full software is primarily computation-bound, as performance continuously increases with more threads.
Offloading hashing alone did not have much performance benefits, since the Murmur3 hash we use is actually quite lightweight.
Offloading ASCII parsing (``Encoder'') brings significant performance improvements, but we notice that performance improvements are sublinear after 16 threads.
After encoding and hashing are both offloaded to the accelerator, the only work left on the software is cuckoo hash lookup and matching against the reference.
Reference matching is not the primary performance bottleneck since we can use simple \texttt{memcmp} thanks to the aligned binary representation we presented in Section~\ref{sec:software}.
The reason for sublinear performance in this situation was found to be the 32-bit random access into the cuckoo hash causing excessive cache misses, turning the host memory system into the bottleneck.
\name{} overcomes this using the probabilistic filter, which reduces host CPU and memory pressure and allows the overall system to reach maximum performance limited only by duplex communication over the PCIe, with only 16 CPU threads.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\columnwidth,page=3]{figures/figures-swjun-crop.pdf}
    \caption{Compression performance with gradual accelerator offloading on the HG002 long read dataset.}
    \label{fig:compression_performance}
\end{figure}

The compression performance of \name{} is orders of magnitude higher than any existing genome compression accelerator, as illustrated in Figure~\ref{fig:compressor_comparison}.
We compare against two published genome compression accelerators, Arram15~\cite{arram2015fpgareferencecompressiongenomic} and Chen23~\cite{chen2023efficientsequencingcompressionfpga},
We note that the FPGA platforms used are different and \name{} uses the host memory to host the cuckoo hash, rendering raw LUT count comparison inaccurate.
However, the trend is clear.
\name{} achieves almost 100$\times$ the performance of Arram15, and over 30$\times$ Chen23.
We emphasize that Figure~\ref{fig:compression_efficiency_comparison} is presented in logarithmic scale.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\columnwidth,page=14]{figures/figures-swjun-crop.pdf}
    \caption{FPGA genome compression comparisons (Log-scale).}
    \label{fig:compressor_comparison}
\end{figure}

\subsubsection{GPU comparisons}
Figure~\ref{fig:compressor_comparison_gpu} compares the compression and decompression performance of \name{} against GPU implementations of genome compression~\cite{guo2013gpu}, based on the worst-case measured performance of single \name{} compression and decompression modules (B73 Maize).
Three systems are compared (Guo13~\cite{guo2013gpu}, Amich20~\cite{amich2020gpu}, DeLuca22~\cite{deluca2022gpu}).
The latter two used multiple current-generation GPUs in parallel, so we present performance normalized to a single GPU.
Guo13 used a now-outdated K20c, so we also present a scaled projection based on the relative performance against a modern RTX~409 GPU (Guo13P).
Only Guo13 presents a decompression performance, which is similar to its compression performance.

\name{} achieves superior performance compared to all systems compared against.
We also emphasize that \name{}'s decompressor consumes only 3\% of the U50 chip, while the GPU implementations use all available GPU resources.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\columnwidth,page=15]{figures/figures-swjun-crop.pdf}
    \caption{Comparison with GPU-accelerated compression (Log-scale).}
    \label{fig:compressor_comparison_gpu}
\end{figure}


\subsection{Case Study: Pre-Alignment Filtering}

We use pre-alignment filtering acceleration as a driving memory-intensive application.
Sequence alignment is one of the most fundamental components of computational genomics.
A genomic sequence may need to be aligned against one or more references simultaneously or against one or more sequences simultaneously~\cite{leggett2016nanookmultireference,hohl2002efficient,angiuoli2011mugsy,schmidt2019accurate}, for De Novo assembly and evolutionary studies.

For the popular seed-and-extend method of alignment, which is currently the de facto standard, each read can be matched to numerous potentially matching reads in the database, or locations in the reference, during the seeding process.
Most of these matches turn out to be unfruitful after proper alignment.
Pre-alignment filtering helps circumvent the high computational overhead of actual long read alignment by quickly filtering out unlikely matches based on edit distance thresholds, often resulting in an order of magnitude reduction of end-to-end work for both long and short reads, without loss of accuracy~\cite{alser2019shouji,alser2017gatekeeper, alser2017magnet,alser2020sneakysnake}.

We note that, especially for sequence-to-sequence alignment, every new filtering operation needs to transport one or more read sequences to the accelerator.
This is because of the sheer size of the sequence-read databases, as well as the inherently unpredictable nature of sequence-to-sequence matching.
Every potential alignment operation statistically involves a long read which must be newly read and transported to accelerator memory.
For simplicity and consistency, even for sequence-to-sequence alignment, we refer to each of the sequence pairs as ``reference'' and ``read'' from now on.

\subsubsection{Pre-Alignment Filtering Algorithm}
For evaluation, we implement a Shifted Hamming Distance (SHD) type filter (SHD~\cite{xin2015shiftedhammingdistance}, GateKeeper~\cite{alser2017gatekeeper,bingol2024gatekeepergpu}, MAGNET~\cite{alser2017magnet}), since they have shown to have good and accurate filtering while highly parallelizable, often limited by memory bandwidth~\cite{alser2017gatekeeper}.
Figure~\ref{fig:shd} demonstrates the key idea of SHD filters, filtering based on an edit distance of 1.
A filter with an edit distance of one shifts the reference by one to the left and right, resulting in three shifted copies.
The read sequence (which may have substitutions, insertions, and deletions) is then compared against each copy, to create three-bit masks where ``0'' and ``1'' represent a matched base and a non-match, respectively.
The three masks are \texttt{AND}'d together, and the number of ``1''s remaining becomes the edit distance, which is correctly 2 in this case.
Different designs have introduced numerous optimizations on top of this idea, including turning short runs of zeros (e.g., ``1001'') into ones (``1111'').

\begin{figure}[htb]
    \centering
    \includegraphics[width=.9\columnwidth,page=10]{figures/figures-swjun-crop.pdf}
    \caption{Pre-alignment filtering using Shifted Hamming Distance.}
    \label{fig:shd}
\end{figure}

We evaluate this application on \name{} using a popular configuration with an edit distance of 5, meaning 11 copies are created by shifting the reference from 1 to 5 times in each direction.
The filtering kernel processes the 11 shifted references and one read all at once using a tree of comparators looking at disjoint windows of 512 bits.
A configurable amendment module after the comparator can flexibly apply various optimizations introduced by algorithms such as MAGNET~\cite{alser2017magnet}.
Then, a tree of saturating 3-bit adders computes the number of 1's in the final vector.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\columnwidth,page=9]{figures/figures-swjun-crop.pdf}
    \caption{\name{} improves FPGA performance utilization by over 6$\times$ compared to the PCIe bottleneck.}
    \label{fig:filtering_performance}
\end{figure}

\subsubsection{Evaluation Against FPGA Platforms}
Figure~\ref{fig:filtering_performance} presents the performance evaluation of this application on various FPGA accelerator environments.
In addition to simple 1-to-1 mapping, we also consider a multi-reference alignment scenario where a read is mapped to multiple references to improve accuracy or for evolutionary research~\cite{leggett2016nanookmultireference,hohl2002efficient,angiuoli2011mugsy,schmidt2019accurate}.
Compared systems include \name{}(R), the ``real'' implementation of \name{} subject to the U50-specific HBM throttling, and the ``ideal'' \name{}(I), the projected performance without throttling.

This performance is compared against the internal throughput of three different FPGA accelerators, assuming all data already exists in device memory: U250(I), the ideal performance of the Alveo U250 device with DDR4 memory, as well as U50(R) and U50(I), the U50 device where HBM is power-throttled and not, respectively.

But as we repeatedly emphasize in this work, accelerator internal throughput is \textbf{\underline{not representative}} of real-world performance because if the read dataset exceeds memory capacity, every request must transport some data to the accelerator.
In reality, communication overhead over PCIe dominates the runtime for this application.
The hatched red box represents the internal throughput limited by PCIe performance.
The final bar in each group represents the real-world performance considering the PCIe bandwidth bottleneck.

The results show that \name{} achieves over 6$\times$ higher performance compared to the realistic system limited by PCIe performance.
Even compared to the \emph{internal} performance of the in-memory U250(I), \name{} performs delivers over 85\% of the performance.
When compared to the upper-bound performance U50(R) which assumes an unrealistic scenario where where data movement is overhead-free, \name{} delivers performance within the order of magnitude margin, at 30\%.
This means \name{} can operate at 30\% of the peak utilization of the device, while providing practically infinite memory capacity over PCIe.
This is a stark upgrade from the mere 4\% utilization achieved over uncompressed PCIe.


\subsubsection{Evaluation Against GPU Platforms}
We also compare \name{}'s performance numbers against recently published GPU implementations described in Table~\ref{tab:filter_gpus}.
This table lists the publication year, GPU platform used, and the memory bandwidth of the GPU platform.
For reference, HBM on the U50 has a catalog maximum bandwidth of 512~GB/s, but is throttled at 202~GB/s due to power system limitations.
In comparison, all GPU systems have superior memory bandwidth.

\begin{table}[h]
    \centering
    \caption{GPU implementations of pre-alignment filtering.}
    \begin{tabular}{|c|c|c|}
        \hline
        Gatekeeper (GK) & SneakySnake (SS) & SeedHit (SH) \\
        \hline
         2020 \cite{bingol2024gatekeepergpu} & 2024 \cite{alser2020sneakysnake} & 2024 \cite{ju2024seedhitgpu} \\
        GTX 1080ti & RTX 3080ti & RTX 3080ti \\
        484 GB/s & 912 GB/s & 912 GB/s \\   
        \hline
    \end{tabular}
    \label{tab:filter_gpus}
\end{table}

When GPU platform performance difference between different datasets, we chose the fastest one listed.
The end-to-end performance comparisons can be seen in Figure~\ref{fig:gpucomp}, presented in terms of base pairs processed per second.
\name{}(R) running at 29.2~GB/s on the one-to-one filtering corresponds to 116 billion base pairs compared per second.
We note that Gatekeeper implements an SHD-based algorithm, and the others implement other GPU-optimized algorithms.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\columnwidth,page=11]{figures/figures-swjun-crop.pdf}
    \caption{\name{} overcomes PCIe limitations to achieve high performance.}
    \label{fig:gpucomp}
\end{figure}

The results in Figure~\ref{fig:gpucomp} emphasize the benefits of \name{}.
First, we note that the best internal throughput of both FPGA (U50-I) and GPU (SH) are somewhat similar, despite GPU's higher memory bandwidth.
However, we emphasize again that if the dataset exceeds on-device memory, each request needs to transport either reference, read, or both, over PCIe to GPU memory before analysis.
We note that the GPU systems listed have presented performance with datasets that fit in on-device memory.

Even with the faster PCIe Gen4 $\times$16 available to the RTX~3080ti GPUs, the vast majority of the computational capacity is wasted in real-world scenarios with data transfer requirements, resulting in the very low utilization (3\%) presented in the figure.

\name{} is an attractive solution to overcome this problem, as it significantly outperforms the PCIe-constrained GPU system despite the last-generation PCIe Gen3 limitation.

