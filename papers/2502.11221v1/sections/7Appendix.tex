\section{Appendix}
\label{sec:app_table}


\begin{table*}[!h]
\renewcommand{\arraystretch}{1.5} 
\small
\centering
\caption{Summary of Foundations in LLM Planning (Section \ref{sec:foundations})}
\vspace{-0.5em}
\scalebox{0.67}{
\begin{tabular}{|p{2.8cm}|p{2.2cm}|p{3.0cm}|p{5.5cm}|p{6.0cm}|}

\hline
\textbf{Method Name} & \textbf{Dataset} & \textbf{Evaluation Metric} & \textbf{Methods} & \textbf{Major Contribution} \\

\hline
DEPS \par \cite{wang2023describe} & Alfworld & Success Rate, Average Episode Length & 1. LLM generates PDDL goal from task description \par 2. Sampling world beliefs and using symbolic planning \par 3. Plan generator (BFS(f)) for generating plans & 1.LLM-DP achieves 96$\%$ success in Alfworld, outperforming the ReAct baseline (53$\%$) with fewer actions. \par 2.Enhances correctness in LLM planning by dynamically adjusting plans in response to environmental feedback and sub-goal feasibility.\\

\hline
ProgPrompt \par \cite{singh2023progprompt} & VirtualHome, Real-World Robot Tasks & Success Rate, Goal Conditions Recall, Executability & 1. Pythonic program generation for task planning \par  2. Use of natural language comments and assertions for feedback \par 3. Integration with real-time environment state feedback during task execution & 1. ProgPrompt significantly outperforms baseline methods by using programmatic LLM prompts to generate executable task plans, achieving up to 1.00 SR and 1.00 Exec in various VirtualHome tasks. It also adapts well to real-world robot tasks, with a Plan SR of 1 in most cases. \par 2. Improves correctness by incorporating environmental state feedback directly into the planning process. \\

\hline
Adaplanner \par \cite{sun2024adaplanner} & ALFWorld and MiniWoB++ & Success Rate & Closed-loop approach allowing LLM agent to refine self-generated plan adaptively & 1. Uses code-style LLM prompt structure and skill discovery mechanism. Achieves 91.79$\%$ success rate on ALFWorld tasks. \par 2. Achieves 91.11$\%$ success rate on MiniWoB++ tasks with feedback. \par 3. Improves planning correctness by adaptively refining plans based on environmental feedback, effectively managing complex sequential tasks. \\

\hline
GNN-Enhanced Task Planner \par \cite{wu2024can} & HuggingFace, Multimedia tasks, Daily Life API tasks, TMDB API tasks & Node F1-Score, Link F1-Score, Task Accuracy, Token Consumption & 1. Integration of GNNs for task graph navigation \par 2. Training-free (SGC) and training-required (GraphSAGE) approaches \par  3. GNN-based node and edge selection for task planning \par 4. Training fine-tuned models for better task retrieval & 1. GNN-enhanced planning outperforms LLM-based solutions by improving task accuracy (up to 9$\%$) with reduced token consumption. The proposed method scales well with larger task graphs and improves planning efficiency by a significant margin.\par 2.Addresses correctness by effectively mapping task dependencies in a graph structure, enhancing sub-task selection and sequence planning. \\

\hline
SelfGoal \par \cite{yang2024selfgoal} & Public Goods Game, Guess 2/3 of the Average, First-Price Auction, Bargaining & Success Rate, TrueSkill Score, Contribution Consistency & 1. Constructs GOALTREE to decompose high-level goals dynamically \par 2. Uses Search Module to select the most relevant subgoals \par 3. Decomposition updates based on environmental feedback \par 4. Adaptive subgoal tree refinement during task execution & 1. SelfGoal achieves a 94$\%$ success rate in dynamic multi-agent environments, outperforming baselines (e.g., ReAct, ADAPT) by dynamically adjusting subgoal guidance. It significantly improves task performance, especially in cooperative and competitive tasks.\par 2. SelfGoal enhances correctness and adaptability in planning by dynamically adjusting subgoals and actions based on ongoing task performance and feedback.\\

\hline
ADaPT \par \cite{prasad2023adapt} & ALFWorld, WebShop, TextCraft $\&$ hotel data & Success Rate, Task Complexity Handling, Sub-Task Decomposition Efficiency & 1. As-needed recursive decomposition of complex tasks \par  2. Planner and executor modules with dynamic failure adaptation \par 3. Multi-level decomposition for task complexity handling & ADaPT improves success rates by up to 33$\%$ over baselines, dynamically decomposing complex tasks as needed. It handles task complexity efficiently with up to 28.3$\%$ higher success rates on ALFWorld and 27$\%$ higher on WebShop.\\

\hline
LLM+P \par \cite{liu2023llm+} & Blocksworld, Grippers, Barman, Termes, and other robot planning scenarios & Success Rate, Optimal Plan Success, Plan Execution Time & 1. Converts natural language problem description into PDDL format \par 2. Uses classical planners (e.g., FAST-DOWNWARD) to generate optimal plans \par 3. Translates planner output back into natural language & 1. LLM+P significantly outperforms LLMs in solving long-horizon planning problems, achieving optimal plans in robot domains with high success rates (up to 100$\%$ on Blocksworld) and minimal execution time. 2. LLM+P improves correctness by integrating LLMs with classical planning techniques to generate and execute optimal plans.\\

\hline
LLM-DP \par \cite{dagan2023dynamic} & Alfworld & Success Rate, Average Episode Length & 1. LLM converts task descriptions into executable PDDL goals \par 2. Symbolic planner (BFS(f)) generates valid plans \par 3. Belief sampling to generate multiple world states for planning & LLM-DP outperforms the ReAct baseline, achieving 96$\%$ success in Alfworld, with significantly fewer actions (13.16 vs. 18.69) per task and higher efficiency due to belief sampling and symbolic planning integration.\\

\hline
PDDL World Model Generation \par \cite{guan2023leveraging} & Household, Tyreworld, Logistics & Error Count, Success Rate & 1. LLM-based PDDL generation for task planning \par 2. Error correction using LLMs as feedback interfaces \par 3. Use of external planners for generating feasible plans from PDDL models & 1. The paper shows that GPT-4 is capable of generating high-quality PDDL models with fewer errors than GPT-3.5-Turbo. It demonstrates that GPT-4-based world models lead to a 95$\%$ success rate in planning tasks using classical planners like Fast Downward. \par 2. Enhances plan correctness by using LLM-generated PDDL models in conjunction with domain-independent planners, ensuring plans are not only feasible but also optimal.\\

\hline
LLM Planning Evaluation \par \cite{valmeekam2023planning} & Blocksworld, Mystery Blocksworld & Success Rate, Levenshtein Edit Distance & 1. Evaluates LLMs (GPT-3, BLOOM) on common planning domains \par 2. Three evaluation modes: autonomous, heuristic, and human-in-the-loop \par 3. Comparison of LLM-generated plans with optimal solutions using automated planning tools & LLMs struggle with autonomous plan generation (3$\%$ success rate), but perform better when used for heuristic guidance in planning tasks (with LPG) and assist human planners by improving task completion accuracy (74$\%$ to 82$\%$).\\

\hline

\end{tabular}
}
\label{tab:foundation_1}
\end{table*}


\begin{table*}[!h]
\renewcommand{\arraystretch}{1.5} 
\small
\centering
\caption{Summary of Foundations in LLM Planning (Section \ref{sec:foundations})}
\vspace{-0.5em}
\scalebox{0.67}{
\begin{tabular}{|p{2.8cm}|p{2.2cm}|p{3.0cm}|p{5.5cm}|p{6.0cm}|}

\hline
\textbf{Method Name} & \textbf{Dataset} & \textbf{Evaluation Metric} & \textbf{Methods} & \textbf{Major Contribution} \\


\hline
Tree Search for Language Model Agents \par \cite{koh2024tree} & WebArena, VisualWebArena & Success Rate, Search Budget Efficiency & 1. Best-first search algorithm using language model feedback \par 2. Backtracking to refine paths based on value function \par 3. Model-based value function for search guidance & 1. Tree Search improves the GPT-4o agent's success rate by 39.7$\%$ on VisualWebArena and 28.0$\%$ on WebArena. The search mechanism enhances multi-step planning and exploration, setting a new state-of-the-art in web task completion. \par 2. Enhances correctness by using a best-first search strategy that adapts to environmental feedback, improving decision-making in complex web tasks.\\

\hline
SimPlan \par \cite{hirsch2024s}  & Blocksworld, Ferry, Grippers, Depots, Minigrid &Success Rate & 1. Hybrid approach combining LLMs with greedy best-first search \par 2. Utilizes external world modeling tools \par 3. Heuristic-driven planning for enhanced action ranking \par 4. Optimizes path selection through search algorithms & 1.SimPlan outperforms baseline LLM-based planners by integrating world modeling and search algorithms, achieving high success rates in domains like Blocksworld and Ferry. It highlights the effectiveness of combining LLMs with traditional planning methods. \par 2. Addresses limitations in LLMs' planning capabilities by integrating classical planning methodologies, enhancing correctness through structured problem-solving. \\

\hline
LATS \par \cite{zhou2023language} & HotPotQA, WebShop, HumanEval, Game of 24 & Pass@1 Accuracy, Success Rate  & 1. Integrates MCTS-based search for reasoning, acting, and planning \par 2. External feedback integration with self-reflection for better decision-making \par 3. Dynamic sampling of multiple action paths for exploration & 1.LATS achieves state-of-the-art results, including 92.7$\%$ Pass@1 accuracy on HumanEval with GPT-4 and surpassing ReAct with 75.9$\%$ success rate on WebShop. It unifies reasoning, acting, and planning in one framework and improves decision-making by using Monte Carlo Tree Search.\par2.Significantly enhances correctness in planning by introducing a tree search framework that adapts to environmental feedback and multiple reasoning paths.\\

\hline
Tree of Thought \par \cite{yao2024tree} & Game of 24, Creative Writing, Mini Crosswords & Success Rate, Passage Coherency, Task Completion Time  & 1. Tree-based search for exploring multiple reasoning paths \par 2. Self-evaluation and decision-making with backtracking \par 3. Application of breadth-first and depth-first search \par 4. Thought decomposition for structured problem-solving & 1.ToT significantly improves problem-solving performance, with 74$\%$ success in Game of 24 compared to 4$\%$ for traditional Chain-of-Thought prompting. It introduces a new framework for deliberate planning and exploration in LLM-based reasoning tasks.\par 2.Enhances correctness by allowing for exploration of multiple paths and adjusting strategies based on real-time feedback.\\
 
\hline
Thought of Search \par \cite{katz2024thought} & 24 Game, Mini Crosswords, BlocksWorld, PrOntoQA & Success Rate, Plan Time, Model Evaluation Calls & 1. Using LLMs to generate code for search components (successor functions and goal tests) \par 2. Search performed using BFS and DFS \par 3. Minimizes LLM calls for efficiency & 1. Thought of Search achieves 100$\%$ accuracy across datasets with minimal LLM calls (1–2 calls per function). Significantly more efficient than existing LLM-based planning methods.\par 2. Significantly improves correctness by verifying search components for soundness and completeness, reducing computational inefficiency.\\

 
\hline
RAP \par \cite{hao2023reasoning}& Blocksworld, GSM8K, PrOntoQA & Success Rate, Plan Generation Success, Task Completion Time & 1. Repurposes LLM as both a world model and a reasoning agent \par  2. Uses Monte Carlo Tree Search (MCTS) for strategic exploration \par 3. Reward-based planning for balancing exploration and exploitation & 1. RAP achieves 33$\%$ relative improvement in plan generation over GPT-4 with CoT, and outperforms baseline methods on math reasoning and logical inference tasks, achieving a 64$\%$ success rate on Blocksworld and 94.2$\%$ accuracy on PrOntoQA.\par 2. Enhances correctness in LLM planning by effectively balancing exploration and exploitation in reasoning, leading to higher task success rates. \\
 
\hline
 LLM-MCTS \par \cite{zhao2024large} & Virtual Home & Success Rate, Task Completion Time, Task Complexity & 1. Combines LLM as a commonsense world model and heuristic policy in MCTS \par 2. Utilizes LLM to predict object locations and generate action suggestions \par 3. Improves planning efficiency by limiting search space with a heuristic policy \par 4. World model and policy guide MCTS to select promising action paths & LLM-MCTS achieves up to 91.4$\%$ success in simple tasks, outperforms GPT-3.5-based policies and traditional MCTS approaches, and demonstrates superior performance for large-scale task planning with partial observations.\\
  
\hline
Visual Semantic Planning with GPT-2 \par \cite{jansen2020visually} & ALFRED & Success Rate, Prediction Accuracy  & 1. Uses GPT-2 for sequence-to-sequence translation of natural language directives into action plans \par 2. Evaluates command prediction accuracy using strict and permissive scoring metrics \par 3. Focuses on generating multi-step plans with minimal visual input & 1. The GPT-2 model achieved 26$\%$ accuracy in generating correct visual semantic plans without visual input, increasing to 58$\%$ when provided with starting location. This demonstrates that detailed task planning can be done using language models alone. \par 2. Improves correctness and efficiency in task planning by leveraging LLMs for both heuristic guidance and world modeling, leading to better performance in complex tasks.\\
  
\hline 
RobLM \par \cite{chalvatzaki2023learning} & ALFRED & Task Success Rate, Plan Accuracy, Argument Accuracy, Sub-Task Success Rate & 1. Fine-tunes GPT-2 for task planning with grounded scene graph input \par  2. Uses Graph2NL to convert scene graphs into natural language for model input \par 3. Task and geometric grounding for robot execution \par 4. Grounding high-level actions to low-level robotic commands & 1.RobLM outperforms classical planning systems in several task categories, achieving high success rates (up to 98$\%$ for "SliceObject") and demonstrating the feasibility of grounded LLMs for robotic task planning. \par 2. Demonstrates correctness in generating actionable plans from textual directives alone, significantly improving plan success rates without relying on visual data.\\



\hline

\end{tabular}
}
\label{tab:foundation_2}
\end{table*}

  
\begin{table*}[!h]
\renewcommand{\arraystretch}{1.5} 
\small
\centering
\caption{Summary of Foundations in LLM Planning (Section \ref{sec:foundations})}
\vspace{-0.5em}
\scalebox{0.67}{
\begin{tabular}{|p{2.8cm}|p{2.2cm}|p{3.0cm}|p{5.5cm}|p{6.0cm}|}

\hline
\textbf{Method Name} & \textbf{Dataset} & \textbf{Evaluation Metric} & \textbf{Methods} & \textbf{Major Contribution} \\

\hline
Agent-FLAN \par \cite{chen2024agent} & HALF-World, WebShop, Mind2Web, Knowledge Graph, Operating System, Database, ToolBench &Accuracy, hallucination scores & 1.Fine-tuning LLMs (Llama2-7B) for agent tasks \par 2.Decomposing training data into capabilities like reasoning, retrieval, understanding, instruction following
\par 3. Negative sample generation for hallucination mitigation & 1. Outperforms previous works by 3.5$\%$ on agent tasks \par 2. Significantly reduces hallucinations through negative sample learning\par 3. Demonstrates scaling law for data and model size improvement\par 4. Enhances general capabilities like reasoning and instruction-following in general NLP tasks\\
  
\hline
AgentOhana \par \cite{zhang2024agentohana} & Webshop, HotpotQA, ToolAlpaca, ToolBench, AlfWorld, APIbank, Mind2Web, Knowledge Graph, Database & Success Rate, Task Completion Time, Reward Accuracy & 1. Standardizes heterogeneous multi-turn agent trajectories from diverse environments\par  2. AgentRater for filtering low-quality trajectories \par 3. Generic dataloader for efficient data handling during training & AgentOhana aggregates and unifies agent trajectories from ten diverse environments, achieving significant improvements in model performance and task execution. The xLAM-v0.1 model trained using AgentOhana outperforms GPT-4 and other models in benchmarks like Webshop and ToolEval.\\
  
\hline
NAT \par \cite{wang2024learning}& GSM8K, ASDiv, SVAMP, MultiArith, HotpotQA, StrategyQA & Accuracy, F1-Score, EM Score & 1. Incorporates both positive and negative samples in fine-tuning LLMs for agent tasks \par  2. Introduces a prefix/suffix to indicate whether trajectories are positive or negative \par 3. Differentiates high-quality and low-quality negative examples \par 4. Fine-grained NAT with two-level classification of negative examples & 1. NAT achieves a 64.64$\%$ average performance across GSM8K, ASDiv, SVAMP, and MultiArith using 2k positive and 10k negative samples. It outperforms Vanilla and NUT methods in various tasks, demonstrating the value of negative examples for agent fine-tuning. \par 2.Enhances correctness by utilizing negative examples to inform the fine-tuning process, improving task success rates\\
  
\hline
ETO \par \cite{song2024trial} & WebShop, ScienceWorld, ALFWorld & Average Reward, Success Rate, Task Solving Efficiency & 1. Exploration phase for failure trajectory collection \par 2. Contrastive learning using failure-success trajectory pairs \par 3. DPO loss for iterative policy refinement \par 4. Iterative optimization cycle for improvement & 1. ETO demonstrates a 22$\%$ improvement in task-solving efficiency over traditional behavioral cloning, surpassing baselines like PPO and RFT. It significantly improves performance in both in-domain and out-of-domain tasks, especially when expert trajectories are unavailable. \par 2. Significantly improves correctness in task planning by allowing agents to learn from both successful and failed interactions, leading to robust policy enhancements. \\


 \hline
MC-DML \par \cite{shimonte2025} & Jericho Benchmark & Success Rate, Planning Efficiency &1. Integrates Monte Carlo Tree Search (MCTS) with LLMs for more effective planning. \par 2. Uses in-trial and cross-trial memory mechanisms to enhance decision-making based on past failures. & 1. Strengthens the foundation of LLM planning by combining search algorithms (MCTS) with language-based reasoning, enabling more structured and adaptive decision-making. \par 2. Improves correctness by leveraging memory-guided LLM policies to adjust action probabilities dynamically, ensuring valid and effective decision paths in text-based planning.\\

 \hline
ARMAP \par \cite{chenautonomous2025}.& Multiple agent benchmarks & Success Rate &1. Uses LLMs to navigate environments and generate diverse action trajectories. \par 2. Employs a separate LLM to create and refine a reward model from these trajectories, integrating it with various planning algorithms. & 1. Establishes a novel foundation for LLM planning by automating the creation of reward models, enhancing decision-making capabilities of LLM agents. \par 2. Demonstrates a significant advancement in LLM agents' ability to handle complex, multi-step decision-making tasks without human-annotated data, making the approach highly scalable and adaptable.\\
\hline

\end{tabular}
}
\label{tab:foundation_3}
\end{table*}



\begin{table*}[!h]
\renewcommand{\arraystretch}{1.5} 
\small
\centering
\caption{Summary of Completeness in LLM Planning (Section \ref{sec:completeness})}
\vspace{-0.5em}
\scalebox{0.67}{
\begin{tabular}{|p{2.8cm}|p{2.2cm}|p{3.0cm}|p{5.5cm}|p{6.0cm}|}

\hline
\textbf{Method Name} & \textbf{Dataset} & \textbf{Evaluation Metric} & \textbf{Methods} & \textbf{Major Contribution} \\

\hline
PDDL World Model Generation \par \cite{guan2023leveraging} & Household, Tyreworld, Logistics & Error Count, Success Rate & 1. LLM-based PDDL generation for task planning \par 2. Error correction using LLMs as feedback interfaces \par 3. Use of external planners for generating feasible plans from PDDL models & The paper shows that GPT-4 is capable of generating high-quality PDDL models with fewer errors than GPT-3.5-Turbo. It demonstrates that GPT-4-based world models lead to a 95$\%$ success rate in planning tasks using classical planners like Fast Downward.\\

\hline
Large Language Models Can Solve Real-World Planning Rigorously with Formal Verification Tools \par \cite{hao2024large} & TravelPlanner, UnsatChristmas, TSP, Block Picking, Task Allocation, Warehouse & Success Rate, Optimization Rate, Interactive Repair Success Rate & 1. Converts natural language travel planning queries into SMT (Satisfiability Modulo Theory) problems \par 2. Uses LLMs to generate steps and code for formal solvers \par 3. Provides feedback and suggestions for unsatisfiable queries \par 4. Zero-shot generalization to new tasks and constraints & 1.TravelPlan-LLM achieves 93.9$\%$ success in solving complex multi-constraint travel planning problems and offers interactive plan repair for unsatisfiable queries. It generalizes well to new problem domains, including TSP and task allocation. \par 2.Enhances completeness by using formal tools to verify that LLM-generated plans are both correct and complete, ensuring that all solutions are viable and no false solutions are proposed.\\

\hline
PPNL \par \cite{aghzal2023can} & Grid environments, ALFRED & Success Rate, Optimal Rate, Exact Match Accuracy, Unreachable Accuracy, Feasible Rate & 1. Few-shot prompting with GPT-4 for path planning \par 2. Action-and-effect prompting to guide long-term spatial reasoning \par 3. CoT and ReAct prompting to enhance spatial-temporal reasoning and obstacle avoidance \par 4. Multi-goal path planning with hierarchical task decomposition & 1.PPNL evaluates LLMs on path planning tasks, demonstrating GPT-4’s spatial reasoning ability when prompted effectively. ReAct prompting achieves 96.1$\%$ success rate, while fine-tuned models like T5 outperform in in-distribution settings but struggle with generalization. \par 2.  Directly addresses the completeness of LLM planning by evaluating their ability to identify unsolvable problems and correctly navigate solvable scenarios.\\

\hline
LRM Evaluation \par \cite{valmeekam2024llms} & Blocksworld, Mystery Blocksworld, Randomized Mystery Blocksworld & Success Rate, Execution Time & 1. Comparison of LLMs and LRMs using the PlanBench test set \par 2. Zero-shot and one-shot prompting for evaluation \par 3. Performance measured on standard Blocksworld tasks as well as obfuscated versions (Mystery Blocksworld) \par 4. Efficiency evaluation of reasoning tokens used by o1 &LRM o1 outperforms all LLMs, achieving 97.8$\%$ on Blocksworld, but struggles with performance on Mystery Blocksworld (52.8d$\%$). LLMs fail to generalize well, showing significant limitations when faced with obfuscated tasks.\\

\hline

\end{tabular}
}
\label{tab:completeness}
\end{table*}


\begin{table*}[!h]
\renewcommand{\arraystretch}{1.5} 
\small
\centering
\caption{Summary of Executability in LLM Planning (Section \ref{sec:executability})}
\vspace{-0.5em}
\scalebox{0.67}{
\begin{tabular}{|p{2.8cm}|p{2.2cm}|p{3.0cm}|p{5.5cm}|p{6.0cm}|}

\hline
\textbf{Method Name} & \textbf{Dataset} & \textbf{Evaluation Metric} & \textbf{Methods} & \textbf{Major Contribution} \\

\hline
Adaplanner \par \cite{sun2024adaplanner} & ALFWorld and MiniWoB++ & Success Rate & Closed-loop approach allowing LLM agent to refine self-generated plan adaptively & $ \bullet$ Uses code-style LLM prompt structure and skill discovery mechanism. Achieves 91.79$\%$ success rate on ALFWorld tasks. \par $ \bullet$ Achieves 91.11$\%$ success rate on MiniWoB++ tasks with feedback.\\

\hline
Inner Monologue \par \cite{huang2022inner} & Tabletop and mobile manipulation tasks in both simulated and real-world settings & Success Rate & Uses LLMs for generating sequential, actionable plans based on feedback.
\par Employs closed-loop feedback incorporating success detection, scene descriptions, and human interactions. & Demonstrates that LLMs can effectively integrate multimodal feedback to improve planning and execution in robotic tasks.
\par Shows adaptability of LLMs to dynamic environments and tasks, significantly enhancing task success rates in complex manipulation scenarios.\\

\hline
LLM-Planner \par \cite{song2023llm} & ALFRED & Success Rate & Uses GPT-3 for generating high-level plans.
\par Incorporates dynamic grounded re-planning based on environment feedback. & Achieves high few-shot performance on the ALFRED dataset, demonstrating the ability to quickly adapt to new tasks with minimal training data.
\par Introduces dynamic re-planning to adjust plans based on real-time environmental changes, enhancing task success under varied conditions.\\

\hline
G-PlanET \par \cite{lin2023grounded} & ALFRED & Key Action Score & Utilizes encoder-decoder LMs with a focus on grounded planning for embodied tasks.\par Introduces object tables as environmental input for LMs to perceive and plan actions. & First study to investigate LMs' capability in grounded planning for embodied tasks. \par Developed a new evaluation metric (KAS) tailored for assessing the quality of generated plans in realistic environments.\\

\hline
LLP \par \cite{sharma2021skill} & ALFRED & Task Completion rates & Utilizes latent language to segment and label hierarchical tasks in demonstrations.
\par Employs weak and partial natural language supervision to train policies. & Demonstrates effective policy training with minimal natural language annotations, achieving performance comparable to models using more extensive data. \par Introduces a method that allows hierarchical policies to be trained using unannotated action sequences by inferring natural language descriptions.\\

\hline
SayCan \par \cite{ahn2022can} & Real-World Kitchen environment & Plan Success Rate, Execution Success Rate & Leverages LLMs to score the likelihood of robotic skills contributing to task completion. \par Combines LLM outputs with affordance functions from reinforcement learning to prioritize feasible actions. & Enables robots to execute complex, long-horizon tasks using natural language instructions.
\par Nearly doubles performance over non-grounded baselines by integrating real-world affordance functions with LLM predictions.\\


\hline
ADaPT \par \cite{prasad2023adapt} & ALFWorld, WebShop, TextCraft $\&$ hotel data & Success Rate, Task Complexity Handling, Sub-Task Decomposition Efficiency & 1. As-needed recursive decomposition of complex tasks \par  2. Planner and executor modules with dynamic failure adaptation \par 3. Multi-level decomposition for task complexity handling & ADaPT improves success rates by up to 33$\%$ over baselines, dynamically decomposing complex tasks as needed. It handles task complexity efficiently with up to 28.3$\%$ higher success rates on ALFWorld and 27$\%$ higher on WebShop.\\

\hline
LLM-DP  \par \cite{dagan2023dynamic} & Alfworld & 	Success Rate, Average Episode Length & 1. LLM converts task descriptions into executable PDDL goals \par 2. Symbolic planner (BFS(f)) generates valid plans \par 3. Belief sampling to generate multiple world states for planning & LLM-DP outperforms the ReAct baseline, achieving 96$\%$ success in Alfworld, with significantly fewer actions (13.16 vs. 18.69) per task and higher efficiency due to belief sampling and symbolic planning integration.\\

\hline
LLM-MCTS \par \cite{zhao2024large} & Virtual Home & Success Rate, Task Completion Time, Task Complexity & 1. Combines LLM as a commonsense world model and heuristic policy in MCTS \par 2. Utilizes LLM to predict object locations and generate action suggestions \par 3. Improves planning efficiency by limiting search space with a heuristic policy \par 4. World model and policy guide MCTS to select promising action paths & LLM-MCTS achieves up to 91.4$\%$ success in simple tasks, outperforms GPT-3.5-based policies and traditional MCTS approaches, and demonstrates superior performance for large-scale task planning with partial observations.\\

\hline
Corrective Re-prompting \par \cite{raman2022planning} & Virtual Home & Executability Rate, Semantic Correctness, Number of Re-prompts & 1. Utilizes precondition errors for re-prompting to generate corrective actions.
\par Employs a template-based prompt strategy that incorporates error details for plan adjustments. & 1. Introduced a novel approach to improve plan executability and correctness using error-driven re-prompting.
\par 2. Demonstrated substantial improvements in task execution rates and reduced the number of necessary re-prompts compared to baselines.\\

\hline
\end{tabular}
}


\label{tab:executability_1}
\end{table*}



\begin{table*}[!h]
\renewcommand{\arraystretch}{1.5} 
\small
\centering
\caption{Summary of Executability in LLM Planning (Section \ref{sec:executability})}
\vspace{-0.5em}
\scalebox{0.67}{
\begin{tabular}{|p{2.8cm}|p{2.2cm}|p{3.0cm}|p{5.5cm}|p{6.0cm}|}

\hline
\textbf{Method Name} & \textbf{Dataset} & \textbf{Evaluation Metric} & \textbf{Methods} & \textbf{Major Contribution} \\

\hline
LLM-Planner \par \cite{song2023llm} & ALFRED & Success Rate & Uses GPT-3 for generating high-level plans.
\par Incorporates dynamic grounded re-planning based on environment feedback. & Achieves high few-shot performance on the ALFRED dataset, demonstrating the ability to quickly adapt to new tasks with minimal training data.
\par Introduces dynamic re-planning to adjust plans based on real-time environmental changes, enhancing task success under varied conditions.\\

\hline 
SayCanPay \par \cite{hazra2024saycanpay} & Ravens (Tower of Hanoi), BabyAI, VirtualHome & Planning Success, Cost-Effectiveness, Generalization & 1. Heuristic search-based planning framework using LLMs \par  2. Three-step process: Say (generate actions), Can (action feasibility), Pay (action payoff) \par 3. Beam search for action selection & SayCanPay outperforms other LLM-based planning approaches, achieving higher planning success rates (e.g., 94$\%$ on BabyAI), improved cost-efficiency, and better generalization across environments.\\

\hline
BrainBody-LLM \par \cite{bhat2024grounding} & Virtual Home, Franka Research 3 & Success Rate, Goal Condition Recall & Employs two separate LLMs for high-level planning and low-level control.
\par Utilizes closed-loop state feedback to iteratively refine plans based on environmental feedback. & - Achieved a 29$\%$ improvement in task-oriented success rates over existing methods.
\par Demonstrated effective grounding of language models in robotic task planning with minimal human intervention.\\

\hline
CLP \par \cite{yuan2023distilling} & CoScript 3 & Constraint faithfulness, Executability rate & 1.Uses an over-generate-then-filter approach to improve LLMs' planning.
\par  2. Distills structured script knowledge into smaller models using LLMs. & 1.Introduced constrained language planning as a new task, allowing for step-by-step procedural reasoning under constraints.
\par 2.Proposed CoScript, a dataset of 55,000 constraint-driven scripts, improving planning performance.
\par 3.Smaller models trained on CoScript outperform LLMs in constrained planning accuracy.\\

\hline
PLaSma \par \cite{brahmanplasma} & COPLAN, VirtualHome & Human Evaluation: Coverage, Order, Overall Quality, Executability, Correctness & 1. Symbolic procedural knowledge distillation \par 2. Multi-task and task-specific distillation \par 3. Verifier-guided step-wise beam search \par 4. Constrained and counterfactual replanning tasks & PlaSma enhances smaller models with procedural knowledge, outperforming GPT-3 in tasks like goal-based planning and counterfactual replanning. Achieves up to 93$\%$ success in counterfactual replanning.\\

\hline
PRoC3S \par \cite{curtis2024trust} & Various simulated and real-world domains 3 & Success Rate & 1. Uses LLMs to generate programs handling continuous parameters and constraints.\par 2.Incorporates continuous constraint satisfaction to adjust plans based on real-time feedback. & 1. Pioneered the integration of LLMs with constraint satisfaction techniques for complex robotic tasks. \par 2.Demonstrated superior efficiency and effectiveness in diverse manipulation tasks compared to existing methods.\\

\hline
BrainBody-LLM \par \cite{bhat2024grounding} & Virtual Home, Franka Research 3 & Success Rate, Goal Condition Recall & Employs two separate LLMs for high-level planning and low-level control.
\par Utilizes closed-loop state feedback to iteratively refine plans based on environmental feedback. & - Achieved a 29$\%$ improvement in task-oriented success rates over existing methods.
\par Demonstrated effective grounding of language models in robotic task planning with minimal human intervention.\\

\hline
ProgPrompt \par \cite{singh2023progprompt} & VirtualHome, Real-World Robot Tasks & Success Rate, Goal Conditions Recall, Executability & 1. Pythonic program generation for task planning \par  2. Use of natural language comments and assertions for feedback \par 3. Integration with real-time environment state feedback during task execution & ProgPrompt significantly outperforms baseline methods by using programmatic LLM prompts to generate executable task plans, achieving up to 1.00 SR and 1.00 Exec in various VirtualHome tasks. It also adapts well to real-world robot tasks, with a Plan SR of 1 in most cases.\\

\hline
ISR-LLM \par \cite{zhou2024isr} & Diverse planning problem domains & Success Rate & 1.Utilizes iterative self-refinement to enhance LLM-based planning.
\par 2.Employs an LLM translator to convert natural language to PDDL, aiding in plan formulation and refinement. & 1.Introduced a novel framework that significantly improves feasibility and success in long-horizon task planning.
\par 2. Demonstrated superior task accomplishment rates across multiple domains compared to state-of-the-art LLM-based planners.\\

\hline
ADaPT \par \cite{prasad2023adapt} & ALFWorld, WebShop, TextCraft $\&$ hotel data & Success Rate, Task Complexity Handling, Sub-Task Decomposition Efficiency & 1. As-needed recursive decomposition of complex tasks \par  2. Planner and executor modules with dynamic failure adaptation \par 3. Multi-level decomposition for task complexity handling & ADaPT improves success rates by up to 33$\%$ over baselines, dynamically decomposing complex tasks as needed. It handles task complexity efficiently with up to 28.3$\%$ higher success rates on ALFWorld and 27$\%$ higher on WebShop.\\

\hline
SelfGoal \par \cite{yang2024selfgoal} & Public Goods Game, Guess 2/3 of the Average, First-Price Auction, Bargaining & Success Rate, TrueSkill Score, Contribution Consistency & 1. Constructs GOALTREE to decompose high-level goals dynamically \par 2. Uses Search Module to select the most relevant subgoals \par 3. Decomposition updates based on environmental feedback \par 4. Adaptive subgoal tree refinement during task execution & SelfGoal achieves a 94$\%$ success rate in dynamic multi-agent environments, outperforming baselines (e.g., ReAct, ADAPT) by dynamically adjusting subgoal guidance. It significantly improves task performance, especially in cooperative and competitive tasks.\\

\hline
\end{tabular}
}


\label{tab:executability_2}
\end{table*}



\begin{table*}[!h]
\renewcommand{\arraystretch}{1.5} 
\small
\centering
\caption{Summary of Optimality in LLM Planning (Section \ref{sec:optimality})}
\vspace{-0.5em}
\scalebox{0.67}{
\begin{tabular}{|p{2.8cm}|p{2.2cm}|p{3.0cm}|p{5.5cm}|p{6.0cm}|}

\hline
\textbf{Method Name} & \textbf{Dataset} & \textbf{Evaluation Metric} & \textbf{Methods} & \textbf{Major Contribution} \\
\hline
TTG \par \cite{ju2024globe} & Synthetic travel requests and flight $\&$ hotel data & Exact Match accuracy, Cost ratio, Net Promoter Score & 1. Fine-tuned LLM for translating NL requests to symbolic form \par 2. MILP solver for optimal travel itineraries \par 3. Real-time response (<5 seconds) with guaranteed optimality & TTG achieves 91$\%$ translation accuracy with LLM, guarantees optimal itineraries with minimal cost, and provides a user-friendly system with a high NPS (35-40$\%$) on generated itineraries.\\

\hline
ToolChain*  \par \cite{zhuang2023toolchain} & Home Search, Trip Booking, Google Sheets, Virtual Home, GSM8K & Success Rate, Efficiency (Time), Running Time Comparison & 1. A* search-based planning algorithm for tool-use tasks \par 2. Efficient node expansion and pruning using task-specific cost functions \par 3. Combines exploration and exploitation for optimal solutions & ToolChain* improves success rate by 3.1$\%$ and reduces planning time by 7.35x compared to baselines like MCTS, demonstrating its efficiency in navigating expansive action spaces.\\

\hline 
SayCanPay \par \cite{hazra2024saycanpay} & Ravens (Tower of Hanoi), BabyAI, VirtualHome & Planning Success, Cost-Effectiveness, Generalization & 1. Heuristic search-based planning framework using LLMs \par  2. Three-step process: Say (generate actions), Can (action feasibility), Pay (action payoff) \par 3. Beam search for action selection & SayCanPay outperforms other LLM-based planning approaches, achieving higher planning success rates (e.g., 94$\%$ on BabyAI), improved cost-efficiency, and better generalization across environments.\\

\hline 
Beyond A* \par \cite{lehnert2024beyond} & Sokoban, Maze Navigation & Success Weighted by Cost (SWC), Improved Length Ratio (ILR), Optimal Plan Success Rate & 1. Trains Transformer to imitate A* search dynamics \par 2. Fine-tunes to generate shorter execution traces \par 3. Bootstraps from search dynamics for optimized plan generation  & 1. Demonstrates novel training methods that integrate search dynamics into Transformer training, enhancing planning efficiency. \par 2. Achieves significant reductions in search steps (up to 26.8$\%$ fewer than A*) and improves task-solving performance on complex puzzles and navigation tasks.\\


\hline
LLMFP \par \cite{hao2024planning} &9 diverse planning tasks (multi-constraint decision-making and multi-step planning) & Plan optimality, constraint satisfaction rate & 1. Converts natural language planning problems into formal optimization problems using zero-shot LLM reasoning. \par 2. Uses SMT solvers to guarantee that the generated plans are both logically correct and executable. & 1. Introduces a novel method of achieving optimality in LLM planning by rigorously constructing optimization problems from natural language inputs, significantly outperforming traditional planning models. \par 2. Demonstrates strong performance across a range of complex tasks, achieving over 83$\%$ optimal rates, thereby highlighting the effectiveness of integrating LLMs with classical optimization approaches for high-quality planning outcomes. \\
\hline

\end{tabular}
}
\label{tab:optimality}
\end{table*}




\begin{table*}[!h]
\renewcommand{\arraystretch}{1.5} 
\small
\centering
\caption{Summary of Representation in LLM Planning (Section \ref{sec:representation})}
\vspace{-0.5em}
\scalebox{0.67}{
\begin{tabular}{|p{2.8cm}|p{2.2cm}|p{3.0cm}|p{5.5cm}|p{6.0cm}|}

\hline
\textbf{Method Name} & \textbf{Dataset} & \textbf{Evaluation Metric} & \textbf{Methods} & \textbf{Major Contribution} \\

\hline
LLM-GoalTrans\par\cite{xie2023translating} & Blocksworld, ALFRED & Goal Translation Success Rate & 1. Uses GPT-3.5 to translate natural language instructions into PDDL goals. \par 2.Employs n-shot learning to improve translation accuracy. & 1. Demonstrated effective translation of natural language to PDDL goals, with high success rates in structured environments. \par 2.Highlighted the challenges and limitations of LLMs in tasks requiring numerical or physical reasoning.\\


\hline
ISR-LLM\par\cite{zhou2024isr} & Diverse planning problem domains & Success Rate & 1.Utilizes iterative self-refinement to enhance LLM-based planning.
\par 2.Employs an LLM translator to convert natural language to PDDL, aiding in plan formulation and refinement. & 1.Introduced a novel framework that significantly improves feasibility and success in long-horizon task planning.
\par 2. Demonstrated superior task accomplishment rates across multiple domains compared to state-of-the-art LLM-based planners.\\

\hline
Adaplanner \par\cite{sun2024adaplanner} & ALFWorld and MiniWoB++ & Success Rate & Closed-loop approach allowing LLM agent to refine self-generated plan adaptively & 1. Uses code-style LLM prompt structure and skill discovery mechanism. Achieves 91.79$\%$ success rate on ALFWorld tasks. \par 2. Achieves 91.11$\%$ success rate on MiniWoB++ tasks with feedback.\\


\hline
LLM-GenPlan\par\cite{silver2024generalized} & Various PDDL domains & Percentage of tasks solved & 1.Uses GPT-4 to synthesize domain-specific Python programs for task planning.
\par 2.Incorporates Chain-of-Thought summarization and automated debugging in the process. & 1.Demonstrated GPT-4's effectiveness as a generalized planner across several PDDL domains. \par 2.Highlighted the significant benefits of automated debugging and the mixed impacts of Chain-of-Thought summarization in planning tasks.\\


\hline
LLM+P\par\cite{liu2023llm+} & Blocksworld, Grippers, Barman, Termes, and other robot planning scenarios & Success Rate, Optimal Plan Success, Plan Execution Time & 1. Converts natural language problem description into PDDL format \par 2. Uses classical planners (e.g., FAST-DOWNWARD) to generate optimal plans \par 3. Translates planner output back into natural language & LLM+P significantly outperforms LLMs in solving long-horizon planning problems, achieving optimal plans in robot domains with high success rates (up to 100$\%$ on Blocksworld) and minimal execution time.\\


\hline
PDDL World Model Generation\par\cite{guan2023leveraging} & Household, Tyreworld, Logistics & Error Count, Success Rate & 1. LLM-based PDDL generation for task planning \par 2. Error correction using LLMs as feedback interfaces \par 3. Use of external planners for generating feasible plans from PDDL models & The paper shows that GPT-4 is capable of generating high-quality PDDL models with fewer errors than GPT-3.5-Turbo. It demonstrates that GPT-4-based world models lead to a 95$\%$ success rate in planning tasks using classical planners like Fast Downward.\\


\hline
LLM-DP \par\cite{dagan2023dynamic} & Alfworld & 	Success Rate, Average Episode Length & 1. LLM converts task descriptions into executable PDDL goals \par 2. Symbolic planner (BFS(f)) generates valid plans \par 3. Belief sampling to generate multiple world states for planning & LLM-DP outperforms the ReAct baseline, achieving 96$\%$ success in Alfworld, with significantly fewer actions (13.16 vs. 18.69) per task and higher efficiency due to belief sampling and symbolic planning integration.\\


\hline
LTL-Gen \par\cite{pan2023data} & Custom datasets with LTL/natural language pairs & Accuracy & 1. Uses large-scale semantic parsing with minimal human-labeled data.\par 2.Employs synthetic data generation and constrained decoding. & 1.Demonstrates high accuracy with minimal human data compared to prior work. \par 2.Enhances data efficiency for natural language to LTL translation, making it feasible for broader applications.\\


\hline
AutoTAMP \par\cite{chen2024autotamp}& Custom 2D task domains & Success Rate & 1. Translates NL instructions to STL using LLMs. \par 2.Utilizes autoregressive re-prompting for syntactic and semantic error correction.
\par 3.Plans trajectories with a formal STL planner. & 1. Introduces a novel method for autoregressive re-prompting to correct translation errors.\par 2.Demonstrates significant improvements in task success rates with hard geometric and temporal constraints.\par 3.Provides a robust framework that combines LLM translation capabilities with formal planning efficiency.\\

\hline
G-PlanET\par\cite{lin2023grounded} & ALFRED & Key Action Score & Utilizes encoder-decoder LMs with a focus on grounded planning for embodied tasks.\par Introduces object tables as environmental input for LMs to perceive and plan actions. & First study to investigate LMs' capability in grounded planning for embodied tasks. \par Developed a new evaluation metric (KAS) tailored for assessing the quality of generated plans in realistic environments.\\

\hline
PPNL\par\cite{aghzal2023can} & Grid environments, ALFRED & Success Rate, Optimal Rate, Exact Match Accuracy, Unreachable Accuracy, Feasible Rate & 1. Few-shot prompting with GPT-4 for path planning \par 2. Action-and-effect prompting to guide long-term spatial reasoning \par 3. CoT and ReAct prompting to enhance spatial-temporal reasoning and obstacle avoidance \par 4. Multi-goal path planning with hierarchical task decomposition & PPNL evaluates LLMs on path planning tasks, demonstrating GPT-4’s spatial reasoning ability when prompted effectively. ReAct prompting achieves 96.1$\%$ success rate, while fine-tuned models like T5 outperform in in-distribution settings but struggle with generalization.\\

\hline
ProgPrompt\par\cite{singh2023progprompt} & VirtualHome, Real-World Robot Tasks & Success Rate, Goal Conditions Recall, Executability & 1. Pythonic program generation for task planning \par  2. Use of natural language comments and assertions for feedback \par 3. Integration with real-time environment state feedback during task execution & ProgPrompt significantly outperforms baseline methods by using programmatic LLM prompts to generate executable task plans, achieving up to 1.00 SR and 1.00 Exec in various VirtualHome tasks. It also adapts well to real-world robot tasks, with a Plan SR of 1 in most cases.\\

\hline
Adaplanner \par\cite{sun2024adaplanner} & ALFWorld and MiniWoB++ & Success Rate & Closed-loop approach allowing LLM agent to refine self-generated plan adaptively & 1. Uses code-style LLM prompt structure and skill discovery mechanism. Achieves 91.79$\%$ success rate on ALFWorld tasks. \par 2. Achieves 91.11$\%$ success rate on MiniWoB++ tasks with feedback.\\

\hline
LID\par\cite{li2022pre}& VirtualHome, BabyAI & Task completion rates, Generalization to novel scenes and tasks & 1. Utilizes pre-trained LMs to convert observations, goals, and history into sequential data for decision-making. \par 2. Employs active data gathering to train policies without pre-collected expert data. & 1. Demonstrates that pre-trained LMs can significantly improve task completion rates and generalization in interactive decision-making. \par 2. Provides insights into the effectiveness of sequential input representations and LM-based weight initialization for generalization.\\

\hline
SayCan\par\cite{ahn2022can} & Real-World Kitchen environment & Plan Success Rate, Execution Success Rate & Leverages LLMs to score the likelihood of robotic skills contributing to task completion. \par Combines LLM outputs with affordance functions from reinforcement learning to prioritize feasible actions. & Enables robots to execute complex, long-horizon tasks using natural language instructions.
\par Nearly doubles performance over non-grounded baselines by integrating real-world affordance functions with LLM predictions.\\

\hline
\end{tabular}
}

\label{tab:representation_1}
\end{table*}


\begin{table*}[!h]
\renewcommand{\arraystretch}{1.5} 
\small
\centering
\caption{Summary of Representation in LLM Planning (Section \ref{sec:representation})}
\vspace{-0.5em}
\scalebox{0.67}{
\begin{tabular}{|p{2.8cm}|p{2.2cm}|p{3.0cm}|p{5.5cm}|p{6.0cm}|}

\hline
\textbf{Method Name} & \textbf{Dataset} & \textbf{Evaluation Metric} & \textbf{Methods} & \textbf{Major Contribution} \\

\hline
PLaG \par \cite{lin2024graph}& AsyncHow & accuracy & 1. Utilizes graph-based representations to instruct LLMs in planning tasks. \par 2. Combines natural language with graph theory for enhanced task planning. &1. Successfully introduces a graph-theoretical approach to enhance LLMs' performance in asynchronous planning tasks. \par 2. Demonstrates the utility of graph-enhanced prompts to improve accuracy across different LLM architectures.\\

\hline
GNN-Enhanced Task Planner \par \cite{wu2024can} & HuggingFace, Multimedia tasks, Daily Life API tasks, TMDB API tasks & Node F1-Score, Link F1-Score, Task Accuracy, Token Consumption & 1. Integration of GNNs for task graph navigation \par 2. Training-free (SGC) and training-required (GraphSAGE) approaches \par  3. GNN-based node and edge selection for task planning \par 4. Training fine-tuned models for better task retrieval & GNN-enhanced planning outperforms LLM-based solutions by improving task accuracy (up to 9$\%$) with reduced token consumption. The proposed method scales well with larger task graphs and improves planning efficiency by a significant margin.\\

\hline
\end{tabular}
}


\label{tab:representation_2}
\end{table*}




\begin{table*}[!h]
\renewcommand{\arraystretch}{1.5} 
\small
\centering
\caption{Summary of Generalization in LLM Planning (Section \ref{sec:generalization})}
\vspace{-0.5em}
\scalebox{0.67}{
\begin{tabular}{|p{2.8cm}|p{2.2cm}|p{3.0cm}|p{5.5cm}|p{6.0cm}|}

\hline
\textbf{Method Name} & \textbf{Dataset} & \textbf{Evaluation Metric} & \textbf{Methods} & \textbf{Major Contribution} \\


\hline
LLM-GenPlan\par \cite{silver2024generalized} & Various PDDL domains & Percentage of tasks solved & 1.Uses GPT-4 to synthesize domain-specific Python programs for task planning.
\par 2.Incorporates Chain-of-Thought summarization and automated debugging in the process. & 1.Demonstrated GPT-4's effectiveness as a generalized planner across several PDDL domains. \par 2.Highlighted the significant benefits of automated debugging and the mixed impacts of Chain-of-Thought summarization in planning tasks.\\

\hline
VOYAGER \par \cite{wang2023voyager} & Minecraft & Unique items collected, Distance traveled, Tech tree milestones & 1. Uses an automatic curriculum to maximize exploration. \par 2. Maintains a skill library for storing and retrieving executable behaviors. \par 3. Employs an iterative prompting mechanism that incorporates feedback for continuous improvement. & 1. First LLM-powered embodied agent for lifelong learning in Minecraft, capable of continuous self-improvement. \par 2. Demonstrates significant improvements in discovery and task completion, outperforming state-of-the-art methods in efficiency and generalization.\\

\hline
\end{tabular}
}

\label{tab:generalization}
\end{table*}



%%%% Efficiency %%%%


\begin{table*}[!h]
\renewcommand{\arraystretch}{1.5} 
\small
\centering
\caption{Summary of Efficiency in LLM Planning (Section \ref{sec:efficiency})}
\vspace{-0.5em}
\scalebox{0.67}{
\begin{tabular}{|p{2.8cm}|p{2.2cm}|p{3.0cm}|p{5.5cm}|p{6.0cm}|}

\hline
\textbf{Method Name} & \textbf{Dataset} & \textbf{Evaluation Metric} & \textbf{Methods} & \textbf{Major Contribution} \\
\hline
Adaplanner \par \cite{sun2024adaplanner} & ALFWorld and MiniWoB++ & Success Rate & Closed-loop approach allowing LLM agent to refine self-generated plan adaptively & 1. Uses code-style LLM prompt structure and skill discovery mechanism. Achieves 91.79$\%$ success rate on ALFWorld tasks. \par 2. Achieves 91.11$\%$ success rate on MiniWoB++ tasks with feedback. \par 3. Improves planning correctness by adaptively refining plans based on environmental feedback, effectively managing complex sequential tasks. \par\\

\hline
Query-Efficient Planning \par \cite{gonzalez2024query} & PlanBench, Logistics, Grippers, Robotouille & Success Rate & 1. Uses LLMs as heuristics in search-based planning to propose actions or select states. \par 2. Employs generative LLM planners that dynamically adapt plans based on feedback from a world model. & 1. Enhances query efficiency in planning by leveraging LLMs to reduce the number of queries to the world model, thus saving computational resources and time. \par 2. Demonstrates that LLM-based generative planners can effectively adapt to feedback, improving correctness and efficiency in planning tasks by avoiding cul-de-sacs and refining action sequences dynamically.\\

\hline
Tree-planner \par \cite{hu2023tree} & VirtualHome & Success Rate, Executability, Goal Conditions Recall, Error, Correction Token Cost & 1. Plan sampling using LLM for prospective plans \par 2. Action tree construction \par 3. Grounded deciding with real-time observations  &1.  Tree-Planner achieves a 1.29$\%$ improvement in Success Rate and a 53.29$\%$ reduction in token cost compared to ITERATIVE-PLANNER and a 40.52$\%$ reduction in error corrections.  \par 2. Significantly enhances efficiency in LLM planning by reducing the number of LLM calls and interaction with the world model, and by minimizing input/output lengths and model sizes.\\


\hline
Thought of Search \par \cite{katz2024thought} & 24 Game, Mini Crosswords, BlocksWorld, PrOntoQA & Success Rate, Plan Time, Model Evaluation Calls & 1. Using LLMs to generate code for search components (successor functions and goal tests) \par 2. Search performed using BFS and DFS \par 3. Minimizes LLM calls for efficiency & Thought of Search achieves 100$\%$ accuracy across datasets with minimal LLM calls (1–2 calls per function). Significantly more efficient than existing LLM-based planning methods.\\

\hline
Chain-of-Symbols \par \citep{hu2023chain} & Brick World, NLVR-based Manipulation, Natural Language Navigation, SPARTUN & Accuracy, Precision, Recall, Token Usage & 1. Translates spatial relationships to symbolic representation \par 2. Uses chained intermediate steps for efficient planning \par 3. Reduced redundancy in input tokens &  1. COS improves accuracy by 60.8$\%$ on Brick World tasks (from 31.8$\%$ to 92.6$\%$) and reduces token usage by 65.8$\%$. \par 2.Significantly enhances the correctness and efficiency of LLM planning by reducing unnecessary token usage by up to 65.8$\%$ and improving accuracy by up to 60.8$\%$ in complex spatial tasks. \\

\hline 
Beyond A* \par \cite{lehnert2024beyond} & Sokoban, Maze Navigation & Success Weighted by Cost (SWC), Improved Length Ratio (ILR), Optimal Plan Success Rate & 1. Trains Transformer to imitate A* search dynamics \par 2. Fine-tunes to generate shorter execution traces \par 3. Bootstraps from search dynamics for optimized plan generation  & 1. Demonstrates novel training methods that integrate search dynamics into Transformer training, enhancing planning efficiency. \par 2. Achieves significant reductions in search steps (up to 26.8$\%$ fewer than A*) and improves task-solving performance on complex puzzles and navigation tasks.\\

\hline
LLM-DP \par \cite{dagan2023dynamic} & Alfworld & 	Success Rate, Average Episode Length & 1. LLM converts task descriptions into executable PDDL goals \par 2. Symbolic planner (BFS(f)) generates valid plans \par 3. Belief sampling to generate multiple world states for planning & 1. LLM-DP outperforms the ReAct baseline, achieving 96$\%$ success in Alfworld, with significantly fewer actions (13.16 vs. 18.69) per task and higher efficiency due to belief sampling and symbolic planning integration. \par 2. Reduces computational costs by decreasing the number of search steps required for planning, improving efficiency by up to 26.8$\%$\\

\hline
DEPS \par \cite{wang2023describe} & Alfworld & Success Rate, Average Episode Length & 1. LLM generates PDDL goal from task description \par 2. Sampling world beliefs and using symbolic planning \par 3. Plan generator (BFS(f)) for generating plans & 1. LLM-DP achieves 96$\%$ success in Alfworld, outperforming the ReAct baseline (53$\%$) with fewer actions. \par 2.Reduces reliance on extensive LLM calls and minimizes human involvement by integrating automatic PDDL generation, thereby enhancing the efficiency of planning in dynamic environments.\\

\hline
PLaSma \par \cite{brahmanplasma} & COPLAN, VirtualHome & Human Evaluation: Coverage, Order, Overall Quality, Executability, Correctness & 1. Symbolic procedural knowledge distillation \par 2. Multi-task and task-specific distillation \par 3. Verifier-guided step-wise beam search \par 4. Constrained and counterfactual replanning tasks & 1. PlaSma enhances smaller models with procedural knowledge, outperforming GPT-3 in tasks like goal-based planning and counterfactual replanning. Achieves up to 93$\%$ success in counterfactual replanning. \par 2.Demonstrates efficiency by enabling smaller models to perform complex planning tasks with reduced computational costs.\\

\hline
KnowNo \par \cite{ren2023robots} & Simulated and Real Robot Tasks & Success Rate, Help Reduction, Task Success with Human Help, Prediction Set Size & 1. Use of Conformal Prediction (CP) for uncertainty calibration \par 2. Selection of multiple candidate actions \par 3. Minimizes human intervention based on prediction set size & 1. KNOWNO achieves 85$\%$ task success while reducing human help by 10–24$\%$. Guarantees task success with a calibrated confidence level using CP. \par 2. Enhances efficiency and autonomy by reducing the need for human intervention, aligning uncertainty to ensure task success \\

\hline
AutoToS \par \cite{cao2024automating} & BlocksWorld, 24 Game, Sokoban, Mini Crosswords, PrOntoQA & Success Rate, Plan Accuracy, Number of Language Model Calls, Time to Solve & 1. Automates feedback process for search components 2. Uses soundness and completeness checks to refine the generated code \par 3.Incorporates both generic and domain-specific unit tests & 1. AutoToS achieves 100$\%$ accuracy across five domains with minimal human feedback. Significantly reduces the number of model calls and ensures soundness and completeness. \par 2.Improves efficiency by automating the feedback process, significantly reducing the number of LLM calls and human involvement.\\
\hline
\end{tabular}
}
\label{tab:efficiency}
\end{table*}
