\section{Introduction}

Planning, which involves generating a sequence of actions to reach a desired goal state \cite{newell1958elements, kartam1990towards}, is fundamental to human intelligence. For example, when planning a trip to San Francisco, one would search for flights, book tickets based on budget and schedule, arrange local transportation to the airport, and consider alternatives in case of cancellations. These planning tasks require complex reasoning, world knowledge, decision-making, and the ability to adapt, making them a significant challenge for humans. To date, there has been a growing focus on developing LLM planners to automate these complex tasks.


A comprehensive survey of LLM planners would significantly propel research in this field. Prior studies have explored planning methods and evaluation benchmarks \cite{huang2024understanding, li2024lasp}. \citet{huang2024understanding} categorized planning methods into decomposition, plan selection, external modules, reflection, and memory, while \citet{li2024lasp} reviewed evaluation benchmarks across various domains. However, many of these benchmarks and systems are tailored to specific problems, making it hard to compare LLM planners across domains or determine the best planner for new tasks. Further, there is a lack of clear and consistent evaluation criteria. We believe this gap may hinder the development of advanced LLM planners.

Our survey builds on the foundational work of \citet{kartam1990towards} to address key evaluation criteria for LLM planners. 
The original paper highlighted challenges in evaluating early AI planning systems, which relied on heuristics and were confined to research labs. The initial criteria were categorized into performance, representation, and communication issues.
With more advanced LLM planning, we reexamine this critical framework and focus on six key evaluation criteria: \emph{completeness}, \emph{executability}, \emph{optimality}, \emph{representation}, \emph{generalization}, and \emph{efficiency}. For each criterion, we provide a thorough analysis of representative works, highlighting their strengths and weaknesses.

We contribute to the literature by addressing key research questions in LLM planning: What foundational capabilities distinguish them from earlier AI planners? How can we comprehensively measure their performance? We examine the datasets, evaluation methods, and metrics available to the community. We also highlight crucial areas where research is still lacking, including representation, hallucination, alignment, multi-agent planning, connections to agentic workflows, aiming to fill these gaps and advance the field. Figure \ref{fig:taxonomy} presents a taxonomy of six key performance criteria and representative techniques. For those new to LLM planning, we recommend a thorough read, while experts can focus on specific sections. Each section offers clear definitions, relevant works, and includes links to tables in the Appendix. We will dive into the details in the following sections.