\section{LLM Planning Foundations (Tables \ref{tab:foundation_1}-\ref{tab:foundation_3})} \label{sec:foundations} 

We begin by exploring LLM planning foundations, covering widely-used paradigms to provide background for readers unfamiliar with the field. It is broken down into four parts.

\vspace{-0.1in}
\paragraph{Task Decomposition} Task decomposition breaks down abstract goals into specific, manageable sub-goals. It helps mitigate errors by enabling verification at each step and makes LLM reasoning more tractable by narrowing the knowledge space.

Task decomposition can be performed \emph{sequentially, in parallel, or asynchronously}. Specifically, sequential decomposition \cite{wang2023describe, singh2023progprompt, sun2024adaplanner, wu2024can} 
requires that the precondition of the subsequent subgoal is the effect of the preceding subgoal. 
In contrast, parallel decomposition \cite{yang2024selfgoal} involves subgoals that share the \emph{same} precondition and effect, where achieving the final goal requires completing only \emph{one} of these subgoals. Asynchronous decomposition \cite{lin2024graph} involves parallelizing subgoals as well. However, these subgoals in distinct branches have \emph{unique} preconditions and effects. Asynchronous decomposition requires the completion of \emph{all} subgoals to achieve the overall goal. 

Moreover, task decomposition can be performed \emph{recursively}, applying any of the above three approaches at each step. For example, \citet{prasad2023adapt} recursively break down the goal until each subgoal can be easily executed in the environment.  

\vspace{-0.1in}
\paragraph{LLM + Classical Planner} Studies \cite{valmeekam2023planning, kambhampati2024can, kambhampati2024llms} show that LLMs struggle with independent planning. Classical planners, such as Fast Downward \cite{helmert2006fast}, ensure correct plans but depend on experts to translate user queries into formal representations, limiting scalability. A hybrid approach integrating LLMs with classical planners combines the world knowledge of LLMs with the precision and reliability of classical methods, addressing their individual limitations.

When integrated with classical planners, LLMs \emph{translate natural language problems into formal representations} or \emph{generate initial plans}. For example, LLM+P \cite{liu2023llm+}, LLM-DP \cite{dagan2023dynamic}, and \citet{guan2023leveraging} use LLMs to convert planning problems into PDDL  \cite{McDermott1998PDDLthePD}, solved by Fast Downward or BFS(f) \cite{lipovetzky2014width}. \citet{valmeekam2023planning} employs LLMs to generate an initial plan, guiding the LPG planner \cite{gerevini2002lpg}, which iteratively refines it until a correct solution is found.  

\vspace{-0.1in}
\paragraph{Search Algorithm} Search algorithms, including \emph{Breadth-First Search, Depth-First Search} \cite{yao2024tree, katz2024thought}, \emph{Monte Carlo Tree Search} \cite{hao2023reasoning, zhou2023language, zhao2024large, shimonte2025}, and \emph{Greedy Best-First Search} \cite{koh2024tree, hirsch2024s}, have been applied to improve LLM-based planning. These algorithms treat planning as a search problem, using search policy to guide the exploration of various possibilities. Search algorithms excel in planning problems by offering systematic exploration, optimality guarantees, and formal verification without requiring extensive domain knowledge, though they may be computationally intensive compared to more specialized methods like task decomposition.

All search algorithms consist of four core components: (1) \emph{Search Policy} determines node exploration order, which are defined by the underlying search algorithm and are independent of LLMs. (2) \emph{Expansion} generates possible actions from a state, often using LLMs to propose actions based on user instructions and current environment. (3) \emph{World Models} define state transitions based on action preconditions and effects, using LLMs \cite{hao2023reasoning}, classical planners \cite{hirsch2024s}, or external environment simulators \cite{zhou2023language, zhao2024large, koh2024tree}. (4) \emph{Evaluation} assesses state progress toward the goal via scores computed by predefined functions \cite{katz2024thought}, LLM/LVM ratings \cite{yao2024tree, hao2023reasoning, zhou2023language}, log-likelihood scores \cite{hirsch2024s}, voting \cite{yao2024tree}, self-consistency scores \cite{zhou2023language} or reward models \cite{chenautonomous2025}. 

\vspace{-0.1in}
\paragraph{Fine-tuning} \label{subsec:instruction-tuning}Current LLMs are not specifically trained for agentic tasks like planning, and prompt-based methods, which do not update model parameters, cannot fundamentally improve performance in these areas \cite{chen2023fireact, wang2024learning}. Fine-tuning, either focused on \emph{planning-specific tasks} or \emph{broader agentic capabilities}, enhances planning correctness by directly updating LLM parameters.

Planning-specific fine-tuning involves training a pretrained model on planning-focused tasks (e.g., Blocksworld or ALFWorld \cite{shridhar2020alfworld}), to improve planning performance. For example, \citet{jansen2020visually} and \citet{chalvatzaki2023learning} fine-tuned GPT-2 \cite{radford2019language} on ALFWorld, demonstrating its effectiveness in robotics planning. 

Generalized agentic fine-tuning optimizes models using datasets that include both general tasks (e.g., question answering) and diverse agentic tasks (e.g., reasoning, planning, and tool use). This approach is motivated by two key insights: (1) focusing too narrowly on planning may degrade general capabilities \cite{chen2024agent}, and (2) agentic tasks share overlapping capabilities. For instance, reasoning and tool-use tasks often involve planning components. Thus, fine-tuning on a broader set of agentic tasks can simultaneously enhance planning performance and other interrelated capabilities, like reasoning, which are integral to planning. Moreover, standardizing trajectory formats from different tasks \cite{zhang2024agentohana, chen2024agent}, as well as incorporating unsuccessful reasoning or planning trajectories \cite{wang2024learning, chen2024agent, song2024trial} can further enhance learning and performance.