\section{Criterion II: Executability (Tables \ref{tab:executability_1}-\ref{tab:executability_2})} \label{sec:executability}

Executability checks if a plan can be carried out in a given environment while meeting all constraints. A executable plan must use only allowed actions and recognizable objects. Beyond basic precondition and postcondition rules, planners must consider extra constraints, such as avoiding sugar when baking a cake for diabetics \cite{yuan2023distilling}. Importantly, executability and correctness are orthogonal: \emph{an executable plan isn’t necessarily correct}, since it might be grounded and follow all constraints but still fail to reach the goal; likewise, \emph{a correct plan isn’t always executable} since it may only include high-level steps that can’t be executed in a specific environment. Real-world applications typically require plans that are both correct and executable, especially when the executors are not humans (e.g., robots and computers).

To ensure plans are executable, researchers have proposed several approaches, including Object Grounding, Action Grounding, Closed-Loop Systems, and Sample-then-Filter. \smallskip

\noindent\textbf{Object Grounding} Object grounding ensures the LLM planner uses objects available in the current environment when generating plans. The simplest way to do this is by feeding observed or available objects into the planner via prompts \citep{huang2022inner, song2023llm, lin2023grounded, singh2023progprompt} or neural embeddings \cite{sharma2021skill, ahn2022can}. In partially observed environments, where some object information are uncertain (e.g., needing to clean a cup that could be in a cabinet, drawer, or fridge), the planner can generate multiple possible plans, one for each scenario, and select the first feasible one \citep{prasad2023adapt, dagan2023dynamic, zhao2024large}. \citet{sun2024adaplanner} takes a different approach, first generating a plan with placeholders for objects, then filling in the blanks with observed objects during execution.\smallskip

\noindent\textbf{Action Grounding} Action grounding ensures all actions in a plan can actually be executed in the current environment. Like object grounding, the simplest way is to explicitly list all admissible actions in LLMs’ inputs \cite{singh2023progprompt}. If a step goes beyond the executor’s capabilities (e.g., combining multiple allowed actions into one), the LLM planner should be reprompted to break it down until every step is executable \citep{prasad2023adapt}. 

Hierarchical Planning is another common method for grounding actions in LLM planning \citep{huang2022language, raman2022planning, song2023llm, hazra2024saycanpay, bhat2024grounding}. It starts with high-level steps and then translates each one into a sequence of executable actions. This can be done in two ways: either generating all high-level steps first and then refining them into actions or translating each step as it's generated. If an action isn't exactly admissible, the closest valid action is retrieved instead \citep{huang2022language, raman2022planning}. \smallskip

\noindent\textbf{Sample-then-Filter} Since LLMs alone can't guarantee plans meet all constraints, this approach first generates multiple plans and then verifies them, selecting only those that pass all checks. \citet{yuan2023distilling} ranks InstructGPT-generated plans using cosine similarity with task embeddings and selects the most similar one. \citet{brahmanplasma} applies a verifier-guided beam search, keeping the top-K plans based on correctness and constraint adherence at each step. \citet{curtis2024trust} generates Pythonic plans with parameter ranges, tests them with a simulator or classifier, and prompts the LLM to revise if constraints are still violated. \smallskip

\noindent\textbf{Closed-Loop Systems} 
A closed-loop system in LLM planning means the model adapts its plan based on feedback from executors \cite{prasad2023adapt, yang2024selfgoal}, simulators \cite{bhat2024grounding}, validators \cite{zhou2024isr, silver2024generalized}, other LLMs \cite{wang2023describe}, or even humans \cite{huang2022inner}, when the initial plan are inexecutable. It reprompts the LLM planner to replan 
until the plan is fully executable. Unlike open-loop systems \cite{huang2022language}, which lack feedback, closed-loop planning helps reduce hallucinations and enables LLMs to handle complex, long-horizon, and dynamic environments \cite{wang2023describe}.

Closed-loop systems fall into two types: \emph{implicit} and \emph{explicit} \cite{sun2024adaplanner}. Implicit systems only fix the failed action \cite{raman2022planning, singh2023progprompt, zhou2024isr, prasad2023adapt, yang2024selfgoal}, while explicit systems regenerate the entire plan \cite{sun2024adaplanner}. Though explicit systems require more computation, they prevent errors from compounding across steps.
