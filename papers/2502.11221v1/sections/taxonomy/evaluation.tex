\section{Evaluation} \label{sec:evaluation}

\vspace{-0.05in}
 \paragraph{Datasets}  LLM planning evaluation is conducted on two types of datasets: \emph{planning-focused datasets} and \emph{downstream-task datasets}. 

Planning-focused datasets primarily assess planning abilities. The most common scenarios include (1) \emph{Embodied environments}, (2) \emph{Task scheduling} , (3) \emph{Games}, and (4) \emph{Task decomposition} \cite{li2024lasp}. Figure \ref{fig:taxonomy} presents commonly used planning datasets; readers can refer to \citet{li2024lasp} for further details.

While most of the datasets mentioned above assess whether the generated plans are correct, some specifically target key performance criteria in LLM planning. For \emph{grounding}, Open Grounded Planning \cite{guo2024opengrounded} and Embodied Agent Interface \cite{li2024embodied} evaluate performance in embodied environments, while CoScript \cite{yuan2023distilling}, TravelPlanner \cite{xie2023translating}, and PPNL \cite{aghzal2023can} focus on planning problems with constraints. For \emph{representation}, Planetarium \cite{zuo2024planetarium} assesses LLMs' ability to translate natural language into PDDL. For \emph{optimality}, \citet{lin2024graph} and \citet{gonzalez2025robotouille} introduce tasks requiring optimal plans using asynchronous actions. PPNL \cite{aghzal2023can} can also evaluate a planner’s ability to \emph{identify unachievable goals (i.e., completeness)}.

Planning abilities can also be evaluated through downstream tasks, where planning is integral to task completion, and stronger planning skills enhance overall performance. Downstream tasks can be categorized as follows: (1) \emph{Agentic tasks}, including reasoning-oriented tasks, tool-use-oriented tasks, programming tasks, and web tasks \cite{zhou2023webarena, liu2023agentbench, li2023api, xu2023tool}, (2) \emph{Generation tasks}, including video \cite{lin2023videodirectorgpt}, image \cite{zala2023diagrammergpt} and text generation \cite{moryossef2019step}. Please refer to Figure \ref{fig:taxonomy} for example datasets.
 
\vspace{-0.08in}
\paragraph{Methods}
The most common approach to evaluating LLM planning is to test it in a simulated environment and validate the generated plans using either \emph{an internal verifier} provided by the environment or \emph{external verifier} (e.g., VAL \cite{howey2004val}) to ensure they achieve the intended goal. When ground-truth plans are available, LLM-generated plans can also be \emph{compared against these reference plans} \cite{zheng2024natural}.

The second evaluation method is \emph{human evaluation}, typically used in the following cases: (1) No available verifier: when certain simulated environments (e.g., VirtualHome) or real-world scenarios (e.g., using a mobile manipulator) lack automated verification; (2) Open-ended problems: tasks with ambiguous instructions or generative outputs (e.g., text or images) where multiple valid solutions may differ from the ground truth.

The final evaluation method, \emph{LLM-as-a-Judge}, uses another LLM to automatically assess the quality of generated plans in the cases mentioned above. This approach has been increasingly adopted in recent LLM planning research \cite{guo2024opengrounded, o2023bioplanner}. Compared to human evaluation, LLM judges are faster and more cost-effective, making them especially valuable for evaluating large datasets. However, this method has limitations, such as position bias, length bias, self-inconsistency, and sensitivity to prompts \cite{zheng2023judging, ye2024justice, wei2024systematic}. Addressing these issues is crucial to ensure reliable assessments. For more details on LLM-as-a-Judge, please see \citet{li2024generation, li2024llms, gu2024survey}.
 
\vspace{-0.05in}
\paragraph{Metrics}\;
Figure \ref{fig:taxonomy} summarizes commonly used evaluation metrics for planning-focused tasks, along with representative works. Performance criteria are measured using specific metrics: (1) \emph{Completeness}: success rate and goal condition recall measure whether the generated plan reaches final or stepwise goals, while classification metrics (e.g., true negative rate, false negative rate, and unreachable accuracy) assess the planner’s ability to identify unachievable tasks. When ground-truth plans are available, the exact match score is used. (2) \emph{Executability}: executability rate evaluates whether the plan can be executed in the environment, while constraint pass rate checks if constraints are met.  (3) \emph{Optimality}: measured by the optimality rate (i.e., the percentage of optimally solved tasks). (4) \emph{Efficiency}: common metrics include inference time, input and output token counts, number of plan steps, and model size.  (5) \emph{Representation}: the number of parseable problems indicates correct translations.  (6) \emph{Generalization}: all these metrics can also be applied to unseen scenarios to assess generalization. See Figure \ref{fig:taxonomy} for definitions of individual metrics and representative works.
