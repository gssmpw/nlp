\section{Criterion V: Generalization (Table \ref{tab:generalization})} \label{sec:generalization}

Generalization refers to LLM planners' ability to apply learned strategies to new, more complex out-of-domain scenarios beyond its training environment, which can be enhanced through three key approaches: \emph{fine-tuning} (described previously in Section \ref{sec:foundations}), \emph{generalized planning}, and \emph{skill storage}. Given the diverse user queries in the real-world deployments, ensuring LLM planners' generalizability is important alongside other performance.

\vspace{-0.05in}
\paragraph {Generalized Planning} Generalized planning extracts common patterns from training solutions to tackle larger, more complex tasks within the same domain \cite{srivastava2011new}. 
For example, in the Delivery dataset \cite{yang2022pg3}, models trained on small-scale deliveries (9–17 locations) can generalize to larger ones (70–100 locations) using the same core strategy. 
\citet{silver2024generalized} approached this by prompting LLMs to summarize the domain and generate a minimal, generalizable Python-based plan.  

\vspace{-0.05in}
\paragraph {Skill Storage} Skill storage focuses on learning and reusing previously acquired skills to tackle new problems. E.g., \citet{wang2023voyager} introduced a skill library that stores successfully executed skills (e.g., Combat Zombie). These skills are abstracted and generalized for reuse in similar situations (e.g., fighting spiders involves similar actions to fighting zombies). When encountering an unseen task, the LLM planning system retrieves relevant learned skills based on the task and current states, then applies them to generate an effective solution.
