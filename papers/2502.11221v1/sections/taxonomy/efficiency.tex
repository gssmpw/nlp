\section{Criterion VI: Efficiency (Table \ref{tab:efficiency})} 
\label{sec:efficiency}

Efficiency in LLM planning means reducing computational and monetary costs by decreasing LLM calls, world model interactions, input and output lengths, and model sizes. This is crucial especially developing planners based on commercial LLMs.

\vspace{-0.08in}
 \paragraph{Reduced LLM and World Model Calls} To decrease LLM and world model calls, several tricks are used: (1) generating the entire plan in one shot instead of step-by-step to reduce redundant prompts \cite{hu2023tree, sun2024adaplanner, gonzalez2024query}; (2) checking feasibility by world models only at the end of each subgoal, not after every action \citep{sun2024adaplanner, gonzalez2024query}; (3) merging plans with the same prefix actions or subgoals to avoid duplicate world model checks when sampling multiple plans \citep{hu2023tree}; and (4) in tree search-based methods, querying the LLM once to list all possible actions and states, and generate a state transition and goal check function, avoiding repeated LLM and world model calls at each node \citep{katz2024thought}.  

\vspace{-0.05in}
\paragraph{Shorter Inputs and Outputs} Reducing input and output length includes \emph{decreasing prompt and plan tokens} and \emph{minimizing actions in the final plan} to alleviate the load on executors. For spatial reasoning and planning, \citep{hu2024chain} introduces Chain-of-Symbols (CoS), a compact symbolic representation that replaces natural language descriptions in CoT \cite{wei2022chain} trajectories. \citet{lehnert2024beyond} uses search dynamic bootstrapping to iteratively fine-tune a LLM, replacing training cases with solutions with less tokens and equal optimality. To minimize actions, \citet{dagan2023dynamic} and \citet{wang2023describe} use action selectors based on predefined rules or trained models to find the shortest successful plan.     

\vspace{-0.05in}
\paragraph{Smaller Model Sizes} Shrinking the model size can reduce the computational burden, accelerating training and inference while lowering costs. To train a smaller planning model, \citet{brahmanplasma} uses GPT-3 \cite{brown2020language} as the teacher and T5 \cite{raffel2020exploring} as the student, distilling the teacher's planning capabilities into the more compact student model.
