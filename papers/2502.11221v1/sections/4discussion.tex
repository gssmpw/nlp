\vspace{-0.04in}
\section{Discussion}
\label{sec:discussion}

In this section, we discuss the limitations in current LLM planning research studies, and suggest future directions for improvement and more comprehensive evaluations of LLM planning performance.

\vspace{-0.08in}
\paragraph{Datasets and Baselines} The current evaluation of LLM planning has its limitations, primarily because studies often rely on limited datasets and baselines. This makes it tough to fairly and comprehensively compare different methods. Most studies only use a few datasets from a single domain and difficulty level, and they do not evaluate all the six performance criteria. Inconsistent dataset choices make direct comparisons difficult. On top of that, many studies only compare against basic baselines such as CoT or ReAct, which does not help in comparing more advanced approaches. To fix this, a public, standardized leaderboard should be set up that covers all performance criteria, uses consistent evaluation metrics, includes a variety of baseline and advanced methods, and utilizes diverse datasets spanning multiple domains and difficulty levels. Another useful direction would be to create multilingual planning datasets and assess LLM performance across different languages.

\vspace{-0.08in}
\paragraph{Representation}\; LLMs are highly sensitive to prompts \cite{sclar2024quantifyinglanguagemodelssensitivity, razavi2025benchmarkingpromptsensitivitylarge}, but most research relies on natural language without comparing them to alternative formats, such as PDDL or Python, for describing domains and problems. Some studies \cite{singh2023progprompt, aghzal2024look} suggest that using Python to represent planning problems can improve performance, but automatically translating natural language problem descriptions into Python remains challenging, particularly for non-experts. If LLMs are to handle this translation effectively, additional datasets and evaluations are needed to assess their performance. Furthermore, little research has been conducted on how different prompt templates impact LLM planning performance, or on the best output formats for representing plans. Lastly, most fine-tuning methods rely on natural language data without exploring other formats, such as symbolic representations. Filling these gaps requires building benchmarks like Planetarium \cite{zuo2024planetarium} and carefully choosing representation formats in experiments.

\vspace{-0.08in}
%\vspace{-0.07in}
%\vspace{-0.05in}
\paragraph{Hallucination}  LLMs often experience hallucinations \cite{huang2023survey}, which present two major challenges in planning. First, they might struggle to assess if a plan is achievable given a specific problem description \cite{aghzal2023can, kambhampati2024can}. Second, they can generate inadmissible actions and non-existent objects, requiring translation or expert intervention to correct them \cite{huang2022language, raman2022planning}. This increases the cost of planning systems. Further research is needed to understand the root causes and improve LLMs' ability to accurately identify unachievable plans. Evaluating the impact of these hallucinations remains an important research direction.

\vspace{-0.08in}
%\vspace{-0.07in}
%\vspace{-0.05in}
\paragraph{Human Preference Alignment}\; There is a gap in understanding whether system generated plans align with human preferences. It is crucial for open-ended problems where humans execute the plans. Ensuring alignment with human preferences is vital for safety, feasibility, and usability, particularly in personalized planning tasks such as calendar and travel planning. Additionally, \citet{aghzal2024look} found that LLM planners often fail to achieve optimality in path planning, frequently producing unnecessarily long plans. This may stem from inherent length bias in LLMs, which tend to generate longer sequences. Alignment techniques such as RLHF \cite{ouyang2022training} and DPO \cite{rafailov2024direct} may help alleviate this issue, as humans generally prefer shorter plans for their efficiency, simplicity, and cognitive ease. Further investigation is needed to better align LLM planners with human preferences.

\vspace{-0.07in}
%\vspace{-0.05in}
\paragraph{Cost Effectiveness} Current methods, particularly task decomposition and search-based approaches, often consume a large number of tokens due to lengthy prompts and repeated LLM queries. While heuristic search is considered more efficient than task decomposition, it still requires substantial repeated prompting. To improve cost-effectiveness, we may summarize problem descriptions and enhance heuristic evaluations, e.g., by improving LLM uncertainty estimation \cite{huang2024survey} and verification \cite{li2024systematicanalysisllmcontributions}. These improvements would help reduce prompt length and enable the early stopping of unpromising partial plans.

\vspace{-0.07in}
%\vspace{-0.05in}
\paragraph{Multi-Agent Planning} Most existing research focuses on single-agent planning, where only one agent performs a task. Multi-agent planning \cite{konolige1980multiple, torreno2017cooperative} is more challenging, as it involves multiple agents (e.g., robots) working together or competing in parallel. Despite its complexity, multi-agent planning has received limited attention in AI planning research. It often requires coordinating multiple agents in collaborative or competitive environments where they operate simultaneously. The major challenge lies in developing effective communication protocols and cooperation strategies while generating viable plans for their collective actions.

\vspace{-0.07in}
%\vspace{-0.05in}
\paragraph{Reasoning, Tool Use, and Memory}\quad There is often limited discussion on how other components of LLM agents, such as reasoning, tool use, and memory, affect planning performance. In particular, when LLMs are combined with classical planners or optimizers, it is crucial that the LLM accurately translates the planning problem into the appropriate domain representation to ensure correct plan generation. Currently, these approaches rely on human-selected planners and optimizers. Treating them as tools that LLMs can autonomously choose from could be an exciting prospect. This also raises the question of whether LLMs can effectively select the best tool for a given planning task. Future research should look into enhancing these agentic capabilities in LLM-based planning.




%In this section, we discuss limitations of the current LLM planning work and point out future directions to further enhance and more comprehensively evaluate the LLM planning performance. \hwnote{Need to make the logic in the following paragraphs correct.} 

% The current evaluation of LLM planning is insufficient, due to the limited datasets and baselines used in each study, making it difficult to comprehensively and fairly compare different proposed methods. First of all, most studies use a limited number of datasets with a single domain and difficulty level, and none of them evaluates all six aforementioned performance criteria. Inconsistent datasets choices further hinder direct and fair comparisons. Second of all, many studies only compare against basic baselines, such as CoT and ReAct, making it hard to directly compare more advanced approaches from their reported results. To address these issues, a public, standardized leaderboard should be established. It should incorporate all performance criteria, consistent evaluation metrics, a range of baseline and advanced methods, as well as diverse datasets covering multiple domains, difficulty levels. Another direction is to create multilingual planning datasets and assess LLM performance across languages.%\smallskip

% There is no benchmark that comprehensively evaluates LLM-based planners across all aforementioned six performance criteria. Additionally, most of the examined studies use a limited number of datasets within a single domain and difficulty level. As a result, the reported findings fail to capture all the limitations of the proposed methods, making it challenging for practitioners to select the most suitable approach for their specific needs. Moreover, differences in datasets across studies hinder direct and fair comparisons. Furthermore, many works rely on basic baseline models, such as CoT and ReAct, making it difficult to compare more advanced approaches. Moving forward, a public leaderboard should be established, incorporating all six performance criteria, standardized evaluation metrics, both classical and advanced baseline models, and diverse datasets spanning multiple domains and difficulty levels. \smallskip

% There is no benchmark to comprehensively test LLM-based planners with all the aforementioned six performance criteria. Also, most of the work in the examined papers are using limited number of datasets within a single domain and single difficulty. Thus, the reported results cannot reveal all the limitations of the proposed methods, making it hard for the practitioners to choose which one to use based on their specific needs. Also, Their datasets are also different from other work, making them difficult to compare them directly and fairly. Furthermore, most of the work have very basic baseline models, such as CoT and ReAct, which makes it difficult to compare these more advanced work with each other. In the future, there should be a public leaderboard, with all the six performance criteria, standardized evaluation metrics, both classical and advanced baseline models, and diverse datasets with multiple difficulty and domains. 

% No unified baseline models. Baselines from most of the papers are CoT, ReAct, and ToT, with no advanced developed methods surveyed by this paper.

% Although there are some benchmarks [Embodied interface, PPNL] which target multiple criteria, the datasets and the tasks contained in the benchmark are not diverse enough. 



% The current investigation of input, output, and fine-tuning data representation is insufficient. LLMs are highly sensitive to prompts \cite{sclar2024quantifyinglanguagemodelssensitivity, razavi2025benchmarkingpromptsensitivitylarge}, yet most studies rely solely on natural language representations without comparing alternative formats such as PDDL or Python for describing domains and problems. Some research \cite{singh2023progprompt, aghzal2024look} suggests that using Python to represent problems can improve performance, but the challenge of automatically translating natural language instructions into Python, especially for non-experts, remains unresolved. If LLMs are to be used for this translation, additional datasets and evaluations are needed to assess their effectiveness. Additionally, no studies have explored how different prompt templates affect LLM-based planning performance. Similarly, there is little discussion on the choice of output formats for representing plans, and it remains unclear whether output representation affects planning performance. Finally, most fine-tuning-based methods rely on natural language data without exploring other formats, such as symbolic representations. Addressing these gaps requires both constructing benchmarks like Planetarium \cite{zuo2024planetarium} and carefully selecting representation formats in experiments.

% The current investigation of representation of inputs, outputs and fine-tuning data are not sufficient. LLMs are highly sensitive to prompts \cite{sclar2024quantifyinglanguagemodelssensitivity, razavi2025benchmarkingpromptsensitivitylarge}, yet most studies rely on natural language representations without comparing alternative formats, such as PDDL or Python, for describing domains and problems. Some research \cite{singh2023progprompt, aghzal2024look} suggests that representing problems with Python code can improve performance, but the challenge of translating natural language instructions into Python automatically remains unresolved. If LLMs are to be used for this translation, additional datasets and evaluation are needed to explore their effectiveness. Furthermore, no studies have examined how different prompt templates impact LLM-based planning performance. Similarly, there is little discussion on the choice of output formats for representing plans, and it remains unclear whether output representation affects planning performance. Finally, most of fine-tuning work are using data of natrual language, without explore other formats, such as symbolic representation. All these unknowns deserve further investigation by both constructing the benchmarks like Planetarium \cite{zuo2024planetarium} and carefully choose representation formats during experiments. \smallskip
% LLMs are known sensitive to the prompts. However, most of the work are using natural language representation instead of comparing different representations (such as PDDL, Python) to describe the domain and problems on their tasks. Also, some work found representing plans with Python code can improve the performances, but they did not clearly solve the problem how to convert the natural language instructions into these python code. If we can resort to LLMs, more datasets needs to be developed to test this translation ability. Also, no work discusses the impact of different prompt templates to the performance of the LLM planning. Similar to the inputs, there is no work talking about which output formats should be chosen to represent the plans. It is even unknown if the output representation can actually affect the planning performance. \smallskip

% Inputs (Prompt): 
% What format is the best way to represent the (different) problems? PDDL? Python code? There are only a few studies discussing this (e.g., [Aghzal 2024] for path planning).
% What prompt templates to choose? Since LLMs are very sensitive to the prompt templates, different prompt templates might have different planning performance. 
% If the LLMs can perform better by using formal representation to describe the problem [Aghzal 2024, Pallagani 2023], how should we convert the natural language into these formal representations for non-experts? Can we use another LLM to translate them? How are their performance ⇒ Need more datasets to test this, (e.g. Planetarium [Zuo et al.] for PDDL, but what about other formats, such as Python code?)% Outputs: 



% LLMs suffer from hallucination \cite{huang2023survey}, creating two major challenges in LLM-based planning. First, they struggle to determine whether a given problem is achievable \cite{aghzal2023can, kambhampati2024can}. Second, they generate inadmissible actions and non-existent objects in plans, which needs extra translation and corrections and reduce the efficiency of the planning system \cite{huang2022language, raman2022planning}. Further research is needed to understand the underlying causes of these issues and improve LLM-based planning ability to accurately identify unachievable problems. Additionally, further evaluating the impact of hallucinations on LLM planning remains an important research direction.

%How to further evaluate the hallucination impact on LLM planning is another direction. %\smallskip
% It is known that LLMs suffer from hallucination. In terms of the LLM planning, it causes two problems: fail to identify the achievability of an given problem [Aghzal 2024, ASU o1 paper], and generate nonadmissble actions and non-existing objects in the plans [Huang 2022, Raman 2023]. There is more work to do to investigate why and how these could happen. Also, improving the LLM planning for identifying the unachievable problems is an very important problem.  


% While current LLM planning research focuses on standard evaluation metrics, no work has examined whether the generated plans align with human preferences. This is a crucial question, especially for open-ended problems where humans execute the plans. Ensuring alignment with human preferences is essential for safety, feasibility, and usability. This issue is particularly relevant for personalized planning tasks, such as scheduling (e.g., calendar and travel planning). Additionally, \citet{aghzal2024look} found that LLM-based planners fail to achieve optimality in path-planning problems, often producing unnecessarily long plans. This may stem from an inherent length bias in LLMs, which tend to generate longer sequences. Alignment techniques like RLHF \cite{ouyang2022training} and DPO \cite{rafailov2024direct} could help address this issue, as humans generally prefer shorter plans for their efficiency, simplicity, and cognitive ease. Further investigate is needed for these issues and better align LLM-based planners with human preferences.

% Although the current LLM planning is focusing on the aforementioned evaluation metrics, there is no work focusing on if the output plans are preferred by humans. This is a very important question, especially when the problem is open-ended and the executors are human beings. Also, aligning with Human preference is necessary to make sure all the steps are safe and secure and executable to conduct. Also, this problem is very important to do the personalization planning, especially for task scheduling (e.g., calendar and travel planning). Furthermore, \citet{aghzal2024look} demonstrates that LLM planner cannot achieve the optimality on the path planning problems, ending with outputting unnecessarily long plans. This might be due to the internal length bias of the LLMs, which is they prefer to generate longer sequences than shorter ones. So alignment approaches, such as RLHF and DPO, might be the solution to solve this problem since humans generally prefer shorter plans due to cognitive ease, efficiency, and reduced complexity. \smallskip





% Current approaches, especially task decomposition and search-based methods, often requires significant token consumption due to lengthy prompts and repeated queries. To further improve efficiency, more compact problem descriptions and better heuristic evaluations (e.g., improved LLM uncertainty estimation \cite{huang2024survey} and LLM-verification \cite{li2024systematicanalysisllmcontributions}) should be introduced, which would reduce prompt lengths and enable early-stopping of unpromising partial plans. 

% , also, it is important to excel across many tasks but still faces efficiency bottlenecks. Approaches  How to design better heuristic score function (e.g., model better uncertain) and do the early stopping needs the future investigation. \hwnote{Althgouth heuristic search are more efficient than task decomposition, they still involves hugh amount of repeated prompting.} \smallskip



% Multi-agent planning \cite{konolige1980multiple, torreno2017cooperative}, a classical and more complex AI planning problem, has received limited attention in LLM planning research. Unlike single-agent scenarios, multi-agent planning involves coordinating multiple agents (e.g., robots) that can operate simultaneously and independently. The challenge lies in developing effective cooperation and communication protocols among these agents while generating viable plans for their collective actions.

%Multi-agent planning is a classical problems in the AI planning field, which are very rarely discussed by LLM planning studies. Multiple-agent planning involves multiple agents (e.g., robots) which can operate in parallel and independently, how to make them cooperate and communicate with each other, and plan for this scenario is more challenging than current single-agent scenarios.

% However, most of the examined work are focused on single-agent planning, which means we only plan for one agent (e.g. one robot). Since multiple agents can operate in parallel independently, how to make them cooperate and communicate with each other is more challenging. \smallskip 



% There is limited discussion on how other LLM agent components (i.e., reasoning, tool usage, and memory) impact planning performance. Additionally, while combining LLMs with classical planners or optimizers can ensure correct plan generation if the LLM's translation is accurate, these approaches rely on human-selected planners and optimizers. Treating external planners and optimizers as tools that LLMs can autonomously choose raises the question of whether LLMs can select the most suitable tool based on the given planning problem. Future research should explore improving LLM-based planning by enhancing these agentic capabilities.

% There are limited discussion about the impact of other LLM agent components (i.e., reasoning, tool-use and memory) on its planning abilities. Additionally, although LLM + classical planner / optimizer can guarantee to generate the correct plan if the LLM translation are correct, these work rely on human preselected planners and optimizers. If we consider external planners and optimizers as the external tools which can be used by LLMs, there is more evaluation to see if LLMs can automatically select the appropriate tools based on the planning problems given to them. Thus, the future research can also explore the direction to improving the LLM planning by enhancing other agentic abilities.

%Secondly, there is limited discussion about if improved reasoning abilities and extra internal and external memory can improve the LLM planning abilities. Thus, the future resaerch can also explore the direction to improving the LLM planning by enhancing other agentic abilities.


% First of all, planning requires reasoning abilities, however, there are not a lot of work verifying this and which reasoning methods are the best for LLM planning, given the fact that the most commonly-used reasoning method (i.e., CoT) have limited improvement in limited scenarios. Secondly,  Lastly, like reasoning, there is not a lot of work discussing about the impact of both internal and external memory for the LLM planning abilities. In the future, 

% Firstly, CoT is the most commonly-used reasoning method used in the LLM planning tasks. However, there is no discussion about if this is the best way for LLM planning given the fact that CoT only improves the performance when the human demonstra \citet{} 

% % planning needs reasoning, reasoning and planning are intertwined, which means enhancing reasoning abilities can also improve the planning abilities. However, there is not a lot of work illustrating this. So in the future, we can also test which reasoning methods we should use. Currently, the most common way to do this is CoT and [Stechly 2024] demonstrates the CoT might only depend on a specific prompt engineering to achieve the improvement on the planning tasks. 

% % Reasoning:
% % Since reasoning and planning are intertwined, which means enhancing reasoning abilities can also improve the planning abilities. However, there is not a lot of work illustrating this. So in the future, we can also test which reasoning methods we should use. Currently, the most common way to do this is CoT and [Stechly 2024] demonstrates the CoT might only depend on a specific prompt engineering to achieve the improvement on the planning tasks. 
% External tools:
% When discussing LLM + Classical Planners / Optimizer, no paper is talking about why the specific classical planner or optimizer is used. If we consider these planners and optimizers as a tool, then can LLM automatically choose the optimal or suitable one to use? And also which representation to use? All these are to achieve the fully automatic planning system. 
% Memory
% Although some work exploring the impact of LLM memory for the planning ability, there should be more work exploring this area. Some work talking about this (they talk about how to store the learned successful and failure trajectories and experiences in the memory and facilitate the future planning): “Large Language Models Are Semi-ParametricReinforcement Learning Agents”, “Agent Planning with World Knowledge Mode”.

%\paragraph{Multilingual}




