\section{Benchmark}
\label{sec:benchmark}
In this section, two benchmarks, \textbf{CVQA} and \textbf{CPVQA}, are introduced in detail. First, we present the structural composition of the two benchmarks in Sec.~\ref{subsec:benchmark structure}, followed by an extension of our construction method and the effectiveness of this method is demonstrated in Sec.~\ref{subsec:extension method}.
\subsection{Benchmark Structure}
\label{subsec:benchmark structure}
In this subsection, we introduce the image sources and question types of our proposed benchmarks.\\
\textbf{Image source.} In this study, we selected the game \textit{Can You Escape?}~\cite{game} as our image source due to its close resemblance to real-world visuals. The image named ``scene1'' in each image set is the main scene, and the other images are details of this scene. In addition to visual realism, our benchmarks offer several significant advantages: 
1). Unified scene themes: While most existing datasets treat a single image as an independent scene, our benchmarks utilize multiple images with the same theme to create a cohesive and complete scene. 
2). Diverse task types: Our benchmarks encompass a variety of reasoning tasks within the same scenario, including reasoning about relationships between entities, interpreting digital codes, and rearranging sequences based on colors and shapes. 
3). Strong adaptability: We utilize advanced image processing tools to modify and augment images, a process that would be costly in real-world scenarios. This approach allows us to expand our benchmarks in a cost-effective manner.\\
\textbf{Question types.} In this paper, three question types are defined for \textbf{CVQA} and two for \textbf{CPVQA}. Specifically:
(1) In \textbf{CVQA}, we design three distinct types of questions to represent multi-graph reasoning tasks: props usage, password clues, prop collection. 
\begin{itemize}
    \item \textbf{Props usage}: In this task, the question explicitly identifies the tool in text form, prompting the model to use this prop to interact with a specific entity in the image. According to general real-world rules, in all scenarios, the tool can only interact with a unique entity within the given image.
    \item \textbf{Password clues}: In this task, the question provides a textual description that refers to an image, which acts as an index. The model is required to infer the corresponding image based on the given description, which may involve objects such as a digital password lock or entities arranged in random order.
    \item \textbf{Prop collection}:  In the this task, the questions act as guides, directing the model to identify two related entities within a scene composed of multiple images. In every image, this pair of related entities appears together, with one of them being an object that humans can physically interact with or pick up (e.g., a CD can be picked up, a stationary player cannot).
\end{itemize}
(2) In \textbf{CPVQA}, the image input task, specifically the password clues task in \textbf{CVQA}, primarily requires the model to identify relevant clues rather than producing precise outputs. However, by refining and standardizing benchmarks, we can further explore and assess the model’s capabilities in greater depth. In this study, we categorize the questions in the password clues task into two distinct types: password and sequence rearrangement.
\begin{itemize}
    \item \textbf{Password.} This task requires the model to infer a string of letters and numbers from a scene composed of multiple images. The question specifies the components (letters, numbers or a combination) and the number of digits in the final result. In some images, character combinations may appear in different forms, such as \texttt{Arabic numerals} or \texttt{Roman numerals}. It is important to note that in certain images, the final character sequence may need to be reorganized based on attributes like color, shape, or other distinguishing features.
    \item \textbf{Sequence rearrangement.} The task involves determining the correct order of entities in a given image (e.g., the order of the trophies in Figure~\ref{fig:problem_formulation}) within a scene composed of multiple images. The entities are encoded based on their order in the image from the question (e.g., `ABCD' from left to right). However, the final encoded sequence is influenced by information from other images, such as color, shape, or other attributes. To account for the possibility that the language model may repeat the question, we have designed the final true sequence to differ from the initial encoded sequence designed in the question.
\end{itemize}
\subsection{Benchmark Extension}
\label{subsec:extension method}
Existing image generation and automatic annotation methods are convenient but unreliable (e.g., GPT-generated text may be garbled). This error is critical for reasoning tasks, so we use manual annotation to expand benchmarks while ensuring data quality. This section  presents the scene expansion method, benchmark analysis, and score comparison, demonstrating its effectiveness.\\
\textbf{Scene extension.} In this study, we utilize \textbf{CVQA} and \textbf{CPVQA} as benchmarks to evaluate the reasoning ability of models in scenarios that involve multiple images and a single question. These benchmarks were designed with rigor in mind, emphasizing thematic consistency across scenarios and the provision of clear, unique answers. However, the benchmark’s limited size posed challenges to fully supporting our evaluation system. We utilize the image editing tool \texttt{Photoshop}~\cite{ps} to modify and expand the images.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figure/fig_benchmark.pdf}
    \caption{\textbf{Extension Methods.} Two examples of extension methods applied to the same scenario. In these examples, (A) indicates that \textit{expansion without altering the original answer}, while (B) indicates that \textit{expansion by altering the original answer}.}
    \label{fig:benchmark_example}
\end{figure}
Our expansion methods are categorized into two types: 
1) \textit{Expansion without altering the original answer}: In this approach, we rearrange the positions of the color blocks within the image without changing the original answer. Additionally, in some cases, we replace certain sequences that do not affect the final result, such as the symbols shown in Figure~\ref{fig:benchmark_example} (a).
2) \textit{Expansion by altering the original answer}: On one hand, sequences of characters and entities are modified by altering their order in certain samples. On the other hand, attributes such as color, shape, and other factors that indirectly influence the sequence of characters and entities in the image are adjusted, as illustrated in Figure~\ref{fig:benchmark_example} (b).\\
\textbf{Benchmark statistics.} In the \textbf{CVQA} benchmark, the base dataset consists of 522 images across 123 scenarios. By applying the scene expansion method, we doubled the dataset, resulting in 1,003 images and 227 scenarios. For the \textbf{CPVQA} benchmark, the base dataset contains 180 images and 32 scenarios. Using the same expansion method, we increased the dataset nearly fourfold, bringing the total to 661 images and 147 scenarios.\\
\textbf{Benchmark validity.} To verify the effectiveness of the extended benchmark in evaluating model capabilities, we compare the results of the extended benchmark with the original benchmark, utilizing various methods to answer questions across multiple models. 
The results are shown in Figure~\ref{fig:benchmark validity}, where the x-axis represents the captions from different sources, the $y$-axis represents the model, and the $z$-axis shows the number of differing answer scores for the same task in both the extended and original benchmarks (if both scores are 0, they are considered the same). For the same task across the two benchmarks, variations in score differences across models and methods indicate the effectiveness of our expansion approach in modifying scenarios.
\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figure/fig_3d_vailty.pdf}
    \caption{\textbf{Extension Method Validity.} Original benchmark \textbf{vs.} Extended benchmark: The number of differing results across various methods and models for the same task highlights the effectiveness of the extended benchmark, where `a' represents \textbf{CVQA}, and `b' represents \textbf{CPVQA}. Numbers `1', `2' and `3' correspond to the LLMs reasoning and MLLMs reasoning of method model inference~(Sec.~\ref{subsec:LLMs and MLLMs}) and semantic retrieval~(Sec.~\ref{subsec:semantic and visual retrieval}) respectively.}
    \label{fig:benchmark validity}
\end{figure}