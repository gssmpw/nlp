\section{Introduction}
\label{sec:intro}
In complex scenarios, humans can integrate multiple perceptual information to perform combinatorial reasoning. As shown in Figure~\ref{fig:intro_example}, color sequences are utilized to reorder colored numbers within complex scenarios. 
In recent years, Multi-modal Large Language Models (MLLMs) \cite{chatgpt,LLaVA,Qwen-vl,Qwenmax,Geminipro} have advanced the development of visual language tasks by integrating visual encoders into pre-trained LLMs \cite{llama,Vicuna,chatgpt} to enable visual processing capabilities. 
Increasing research attention has been directed toward visual understanding in complex scenarios, including identifying positional relationships among entities~\cite{gsrbench,srbench} and reasoning in complex scenarios~\cite{Yu2020ReClorAR,Sinha2018CompositionalLU}, such as visual commonsense reasoning~\cite{vcr,GVQA,Wang2020VisualCR}, which often relies on single facts rather than integrating multiple information sources. However, prior studies have not examined models’ abilities to combine multiple perceptual inputs and perform combinatorial reasoning in complex scenarios. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figure/figure1_2.pdf}
    \caption{\textbf{Task Example.} Illustrates combinatorial reasoning with multiple perceptual inputs in complex scenarios.}
    \label{fig:intro_example}
\end{figure}

Unfortunately, there are currently no datasets or simulation scenarios that combine multiple perceptual information for combinatorial reasoning. However, we find that human often pay attention to different clues and then infer the answer when playing a game called escape room. We try to utilize ChatGPT-4o~\cite{chatgpt}, a powerful closed-source model that can perform reasoning, to conduct exploratory experiments in the game \textit{Can you escape? }~\cite{game}, as shown in Sec.~\ref{subsec:explori}, and the results failed in reasoning in 36 complex scenarios. To address the lack of evaluation in integrating multiple perceptual inputs and performing combinatorial reasoning in complex scenarios—where the intricacy and accuracy of the task make errors significantly impact the reasoning process—we meticulously curate two benchmarks with enhanced rigor. Specifically, we manually select 155 complex scenarios from this game, encompassing environments commonly encountered in real life (such as schools, laboratories, and stadiums). Furthermore, we systematically modify the pigment-related information in the images, thereby expanding the benchmark size to 2.4 times its original scale.

Specifically, we propose two benchmarks: 1) \textbf{C}lue-\textbf{V}isual \textbf{Q}uestion \textbf{A}nswering~(\textbf{CVQA}) comprises three task types designed to test a model’s ability to integrate textual and visual perceptual information to identify relevant clues from unspecified visual data. 
The first task involves reasoning with a combination of unspecified visual entities located in different scenes that can interact, such as a ``key'' in one scene that can be used to unlock a ``lock'' in another scene. 
The second task involves reasoning with specified textual entities and unspecified visual entities, where the entities mentioned in the text can interact with a certain combination of visual entities in complex scenarios. 
The third task utilizes specified sequence types (e.g., number sequences, letter sequences, etc.) to combine all visual scenes to infer clues, such as `Four colored numbers' and `Four color sequences' in Figure~\ref{fig:intro_example}. 
2) \textbf{C}lue of \textbf{P}assword-\textbf{V}isual \textbf{Q}uestion \textbf{A}nswering~(\textbf{CPVQA}) consists of two task types that evaluate the model’s ability to combine visual perceptual information for accurate combinatorial reasoning. These tasks involve reasoning across all visual scenes to determine the final numeric sequence (e.g., 2536 in Figure~\ref{fig:intro_example}) or entity code sequence (e.g., BCDA in Figure~\ref{fig:problem_formulation}).
In particular, our benchmarks include several unique features: 
1) The questions in our benchmarks are designed to guide the model to achieve combinatorial reasoning in complex scenarios. 
2) The topics in each of our complex scenarios remain consistent by selecting visually similar images from the same game level~\cite{game} rather than from existing datasets.
3) Our benchmarks' scale is manually expanded to produce high-quality evaluations, with the effectiveness of this approach demonstrated in Sec.~\ref{subsec:extension method}.

To enhance the model’s capability in integrating multiple perceptual inputs for combinatorial reasoning in complex scenes, we propose three methodological approaches from distinct perspectives: 1) From the context, contextual learning ~\cite{fewshotlearning} has been shown to be effective. We utilize text as a medium for reasoning enhancement in Sec.~\ref{subsec:LLMs and MLLMs}. 2) From the model, the probability difference in prediction logic is used to enhance reasoning ~\cite{COTwithoutprompting}, as shown in Sec.~\ref{subsec:COT without prompting}. 3) From the data, based on recent progress ~\cite{samcamsemantic,samsemantic}, we propose an image-based semantic retrieval method in Sec.~\ref{subsec:semantic and visual retrieval} to enhance the visual details of MLLMs.

We make a thorough evaluation of various well-known MLLMs and LLMs on our proposed benchmarks. In \textbf{CVQA}, results of contextual learning~(Sec.~\ref{subsec:LLMs and MLLMs}) are suboptimal, notably, Chain-Of-Thoughts~(COT) reasoning without prompting~(Sec.~\ref{subsec:COT without prompting}) and semantic retrieval~(Sec.~\ref{subsec:semantic and visual retrieval}) improve performance by up to 19\%. In \textbf{CPVQA}, performance falls short of expectations. Overall, while our method enhances combinatorial reasoning in complex scenarios, model accuracy remains low. The main problems are the tendency to answer questions based on a single source of information, over-reliance on information in the question, or reasoning based solely on the model's prior knowledge.

Our contributions can be summarized as follows: 
1) To our knowledge, this is the first study for combinatorial reasoning in complex scenarios requiring multiple perceptual inputs.
2) Experimental results indicate that even SOTA closed-source models~(such as ChatGPT-4o) fail to complete our proposed benchmark, we analysis of their errors and potential improvements. 
3) The proposed plug-and-play method achieves comparable improvements over SOTA closed-source models.