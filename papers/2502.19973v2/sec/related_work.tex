\section{Related Work}
\label{sec:related work}
\textbf{Reasoning with LLMs.} LLMs~\cite{Vicuna,llama,internLM} are increasingly used in various reasoning tasks, including mixed reasoning~\cite{Yu2020ReClorAR}, arithmetic reasoning~\cite{gsm8k,GSMSymbolic}, and deductive reasoning~\cite{ProOnto}. Earlier research~\cite{ftforreasoning,ftforreasoning2,Yang2022GeneratingNL} has enhanced the reasoning abilities of LLMs through fine-tuning, but later studies~\cite{fewshotlearning,Emergent} revealed that fine-tuning can be both costly and less effective compared to using contextual examples. More recently, several strategies have been developed to leverage in-context Learning~\cite{Dong2022ASO,Bertsch2024InContextLW} and prompt design~\cite{Amatriain2024PromptDA,Shinn2023ReflexionLA,relatedprompt} methods, such as COT prompt~\cite{Wei2022ChainOT,Kojima2022LargeLM}, incremental reasoning~\cite{Zheng2023ProgressiveHintPI}, and Tree-of-Thought prompt~\cite{Yao2023TreeOT}. Previous methods have made significant contributions to enhancing model reasoning abilities; however, they primarily focus on individual reasoning within scenarios. In contrast, this paper aims to explore and address the combinatorial reasoning abilities of LLMs in complex scenarios.

\noindent\textbf{Benchmark.} Previous traditional vision-language benchmarks have primarily focused on specific tasks, such as visual commonsense reasoning~\cite{visualcommon,Wang2020VisualCR}, textual reasoning based on visual information~\cite{textvqa,He2021VisualSA}, logical question reasoning~\cite{CLEVR}, and other vision-language tasks~\cite{nextqa,videoqa}. 
Most of these benchmarks~\cite{okvqa,aokvqa,GVQA,vqav2} are designed in simple scenarios, lacking consideration for complex scenarios. This paper contends that, while existing benchmarks~\cite{mme,seedbench,mmbench,multiimages1,multiimages2,Plummer2015Flickr30kEC} are valuable for synthesizing vision-language capabilities, there is a need to focus on tasks in complex scenarios to promote continuous advancements in AI systems. Specifically, our benchmark offers advantages such as a detailed classification of question types and a focus on constructing benchmarks in complex scenes.

% \noindent\textbf{MLLM.} Building upon the significant achievements of LLMs~\cite{bert,limittran,ftLM}, researchers have increasingly focused on developing MLLMs to enhance multimodal understanding capabilities. Notable works, including LLaVA~\cite{LLaVA}, MiniGPT-4~\cite{minigpt4}, and InstructBLIP~\cite{instructblip}, employ visual instruction fine-tuning to boost the visual comprehension of MLLMs. Other methods, such as COVLM~\cite{covlm}, ControlMLLM~\cite{ControlMLLMTV}, and DetGPT~\cite{Pi2023DetGPTDW}, emphasize refining entity relationships and positional awareness in visual contexts. Recent studies~\cite{Kil2024IIMMRIA,Hu2024LeveragingHT} have adopted effective strategies to leverage visual information more flexibly. While many studies~\cite{Prophet,Chen2024AnII,Guo2022FromIT,Xiao2023Florence2AA} rely on single-image inputs, Recent research~\cite{multiimages1,multiimages2} has started incorporating multiple images into applicable MLLMs, like ChatGPT-4~\cite{chatgpt} and Qwen-Max~\cite{Qwenmax}. Nonetheless, most current MLLMs—particularly open-source models—are limited to single-image inputs. Our objective is to evaluate the visual combinatorial reasoning abilities of MLLMs and to offer valuable design insights for creating more robust and cost-effective MLLMs.