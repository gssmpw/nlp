\section{Experiments}
\label{sec:experiments}
This section first introduces the details of exploratory experiment (Sec.~\ref{subsec:explori}), which utilizing the high-level reasoning model to perform combinatorial reasoning in complex scenarios. Next, we describe the experimental setup (Sec.~\ref{subsec:experiment setup}), which includes the benchmark evaluation mechanism and the models used in the experiments. Finally the results~(Sec.~\ref{subsec:main results}) and in-depth analysis (Sec.~\ref{subsec:experiment analysis}) of our proposed methods are presented.
\subsection{Exploratory Experiment}
\label{subsec:explori}
We test with manual prompts to stimulate more thoughtful model responses, as shown in Figure ~\ref{fig:intro_conformal}. The specific steps are as follows:
\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figure/figure2-126.pdf}
    \caption{\textbf{Exploration Experiments.} Examples guide models to solve complex scenes through reasoning with manual prompts.}
    \label{fig:intro_conformal}
\end{figure}

(1) The model utilizes 5 different binary classification task prompts to make choices: `choose 0' indicates that the model should continue exploring a given scene $\mathcal{S}$, while `choose 1' signals that the model is ready to make a final action to attempt unveiling the mystery.

(2) A joint calibration set is constructed based on the original calibration set and the confidence interval from conformal prediction, allowing us to calculate degree of inconsistency: $p = \frac{ |\mathcal{D'}|+1}{|\mathcal{D}|+1}$, where $\mathcal{D'}$ represents samples in the calibration set $\mathcal{D}$ with confidence scores greater than or equal to the confidence of the current sample. A lower $p$ value indicates a higher confidence level.

(3) The choice with the lowest $p$ among the 5 prompts is utilized as the model’s most confident next action. If `choose 0' is selected, artificial prompts and ``previous actions'' are provided as text input, with $\mathcal{S}$ as the image input. $\mathcal{S}$ is then described in detail and summarized as context in ``previous actions''. If `choose 1' is selected, all scene images are utilized as image input, along with artificial prompts and ``previous actions'' as text input, to predict the final action needed to unveiling the mystery. As shown in Figure~\ref{fig:intro_conformal}, the model selects `choose 1' after first selecting `choose 0', enabling it to achieve step-by-step reasoning in the complex scenario over two steps.

We employ a high-level reasoning model ChatGPT-4o~\cite{chatgpt} to perform thoughtful reasoning across 36 complex scenarios. Surprisingly, none of the 36 responses are correct, suggesting that prompt engineering has minimal impact on this task, even with manually crafted prompts.
\subsection{Experimental Setup}
\label{subsec:experiment setup}
In this subsection, we first introduce the evaluation mechanism of our proposed benchmarks, followed by a detailed description of the models utilized in our experiments.\\
\textbf{Evaluation.} In this study, the evaluation mechanisms for \textbf{CVQA} and \textbf{CPVQA} differ. Specifically: (1) For \textbf{CVQA}, this paper require the final result to be a sentence that includes the image name and an interactive entity. We then calculate the number Ground Truth (GT) present in this sentence, denoted as $n_c$. If there are $x$ questions in total, the final score is formalized as:
\begin{equation}
    Score(CVQA) = \frac{\sum_1^x{\min(1,{n_c}/3})}{x}
\end{equation}
(2) For \textbf{CPVQA}, this study evaluates the final result as an ordered string defined by the question for each scenario (e.g., ‘ABCD’ from left to right, as shown in Figure~\ref{fig:problem_formulation}). The GT order is unique; if the predicted order is correct, the score is 1, otherwise, it is 0. If there are $x_1$ questions in total, the final score is formalized as:
\begin{equation}
    Score(CPVQA) \frac{\sum_{i=1}^{x_1} \delta_i}{x_1},\quad 
    \delta_i = \begin{cases}
    1, & \text{True} \\
    0, & \text{False}
    \end{cases}
\end{equation}\\
\textbf{Models.} 
In this paper, we utilize four open-source and six closed-source models to evaluate the performance of various models and configurations.
1). For caption generation, we utilize three open-source models: LLaVA-1.6-7b~\cite{LLaVA}, LLaVA-1.6-13b~\cite{LLaVA}, and Qwen-VL~\cite{Qwen-vl}. 2). For model reasoning tasks, we utilize open-source models such as Mistral-7b~\cite{Mistral7}, LLaVA-1.6-7b, LLaVA-1.6-13b, and Qwen-VL. Additionally, we utilize closed-source models including ChatGPT-3.5-turbo~\cite{chatgpt}, ChatGPT-4, ChatGPT-4o, ChatGPT-4V, Gemini Pro~\cite{Geminipro}, and Qwen-Max~\cite{Qwenmax}.
\subsection{Main Results}
\label{subsec:main results}
We first present the model score results for method contextual learning (Sec.~\ref{subsec:LLMs and MLLMs}) in Tables~\ref{tab:llm reasoning experiments} and~\ref{tab:mllm reasoning experiments}, and then present the score results of our improved methods—COT reasoning (Sec.~\ref{subsec:COT without prompting}) and semantic retrieval (Sec.~\ref{subsec:semantic and visual retrieval})—through ablation experiments shown in Tables~\ref{tab:cot without prompting} and~\ref{tab:semantic retrieval}, respectively.
\begin{table}[ht]
\resizebox{\linewidth}{!}{
\begin{tabular}{l|l|llll|lll}
\hline
\multicolumn{2}{c|}{\cellcolor[HTML]{ECF4FF}Benchmark}       & \multicolumn{4}{c|}{\cellcolor[HTML]{ECF4FF}CVQA(Score)/\%}                                            & \multicolumn{3}{c}{\cellcolor[HTML]{ECF4FF}CPVQA(Score)/\%}                    \\ \hline
Model         & Caption                & \cellcolor[HTML]{EFEFEF}all & \cellcolor[HTML]{EFEFEF}pc & \cellcolor[HTML]{EFEFEF}pu & \cellcolor[HTML]{EFEFEF}pcl & \cellcolor[HTML]{EFEFEF}all & \cellcolor[HTML]{EFEFEF}pas & \cellcolor[HTML]{EFEFEF}seq \\ \hline
ChatGPT-3.5-turbo & Qwen-VL       & {22.03} & {25.64} & {45.45} & {12.56} & {4.70} & {1.12} & {10.00} \\
ChatGPT-3.5-turbo & LLaVA-1.6-7b  & {20.70} & {23.08} & {44.24} & {11.41} & {2.68} & {1.12} & {5.00} \\
ChatGPT-3.5-turbo & LLaVA-1.6-13b & {21.73} & {37.18} & {46.06} & {9.82} & {3.36} & {1.12} & {6.67} \\ \hline
ChatGPT-4 &Qwen-VL &{29.52} &{\textbf{70.51}} &{51.53} &{13.93} &{6.04} &{1.12} &{\underline{13.33}}\\
ChatGPT-4 &LLaVA-1.6-7b &{30.84} &{\underline{67.95}} &{53.33} &{15.75} &{4.70} &{0} &{11.67}\\
ChatGPT-4 &LLaVA-1.6-13b &{28.05} &{\underline{67.95}} &{47.88} &{13.47} &{2.68} &{2.25} &{3.33}\\  \hline
ChatGPT-4o &Qwen-VL &{33.04} &{42.39} &{59.39} &{21.46} &{6.04} &{\underline{4.49}} &{8.33} \\
ChatGPT-4o &LLaVA-1.6-7b &{29.52} &{37.18} &{\textbf{60.61}} &{16.44} &{6.04} &{1.12} &{\underline{13.33}} \\
ChatGPT-4o &LLaVA-1.6-13b &{29.07} &{46.15} &{50.30} &{18.04} &{6.71} &{1.12} &{\textbf{15.00}} \\ \hline
Qwen-Max &Qwen-VL &{27.02} &{50.00} &{43.03} &{16.70} &{6.71} &{2.25} &{13.33}\\
Qwen-Max &LLaVA-1.6-7b &{24.67} &{35.90} &{50.91} &{12.79} &{5.37} &{\textbf{6.74}} &{3.33}\\
Qwen-Max &LLaVA-1.6-13b &{24.96} &{39.74} &{42.42} &{15.75} &{5.37} &{1.12} &{11.67}\\   \hline
Gemini Pro  &Qwen-VL &{26.28} &{25.64} &{28.48} &{25.57} &{4.70} &{1.12} &{10.00}\\
Gemini Pro  &LLaVA-1.6-7b &{27.75} &{29.49} &{31.52} &{26.03} &{\underline{7.38}} &{2.25} &{\textbf{15.00}}\\
Gemini Pro  &LLaVA-1.6-13b &{27.75} &{34.62} &{32.12} &{24.89} &{4.70} &{2.25} &{8.33}\\    \hline
Mistral-7b  &Qwen-VL &{\underline{36.12}} &{60.26} &{49.70} &{\underline{26.71}} &{\textbf{8.72}} &{\underline{4.49}} &{\textbf{15.00}}\\
Mistral-7b  &LLaVA-1.6-7b &{33.92} &{61.54} &{\underline{56.97}} &{20.32} &{4.70} &{1.12} &{10.00}\\
Mistral-7b  &LLaVA-1.6-13b &{\textbf{36.86}} &{64.10} &{47.88} &{\textbf{27.85}} &{6.04} &{1.12} &{\underline{13.33}}\\    \hline
% Gemma2-9b   &Qwen-VL &{31.13} &{23.08} &{51.12} &{24.66} &{/} &{/} &{/}\\
% Gemma2-9b   &LLaVA-1.6-7b &{26.14} &{23.07} &{50.91} &{17.35} &{/} &{/} &{/}\\
% Gemma2-9b   &LLaVA-1.6-13b &{25.70} &{20.51} &{56.36} &{15.07} &{/} &{/} &{/}\\ \hline
\end{tabular}
}
\caption{\textbf{The main results of method LLMs reasoning} (Sec.~\ref{subsec:LLMs and MLLMs})\textbf{.} 
In the tables of this paper, the \textbf{bold font} is the first place in the same task category, and the \underline{underlined font} is the second place; `Caption' indicates the type of model that generates captions; and `all' represents the total score of all tasks in the benchmark, `pc' represents \textbf{p}rop \textbf{c}ollection , `pu' represents \textbf{p}rops \textbf{u}sage, `pcl' represents \textbf{p}assword \textbf{cl}ues, `pas' represents \textbf{pas}sword, and `seq' represents \textbf{seq}uence rearrangement.}
\label{tab:llm reasoning experiments}
\end{table}
\\\textbf{Results of LLMs reasoning. }Table~\ref{tab:llm reasoning experiments} presents the reasoning results of LLMs utilizing method LLM reasoning~(Sec.~\ref{subsec:LLMs and MLLMs}) on the \textbf{CVQA} and \textbf{CPVQA} benchmarks, showing that current models do not perform well on these tasks. First, the best results appear sequentially across most models, highlighting that no model excels completely at the tasks designed for our benchmarks. In \textbf{CVQA}, the `all' category represents the total score across the three task types, with the highest score reaching only 36.86\%, which shows no outstanding performance. In \textbf{CPVQA}, the highest score is 8.72\%. 
In summary, our findings indicate that the scoring results for utilizing LLMs in our proposed benchmarks are unsatisfactory, suggesting that LLMs alone struggle to perform combinatorial reasoning that integrates multiple perceptual inputs in complex scenes.
\begin{table}[ht]
\resizebox{\linewidth}{!}{
\begin{tabular}{l|l|llll|lll}
\hline
\multicolumn{2}{c|}{\cellcolor[HTML]{ECF4FF}Benchmark}       & \multicolumn{4}{c|}{\cellcolor[HTML]{ECF4FF}CVQA(Score)/\%}                                            & \multicolumn{3}{c}{\cellcolor[HTML]{ECF4FF}CPVQA(Score)/\%}                    \\ \hline
Model         & Caption                & \cellcolor[HTML]{EFEFEF}all & \cellcolor[HTML]{EFEFEF}pc & \cellcolor[HTML]{EFEFEF}pu & \cellcolor[HTML]{EFEFEF}pcl & \cellcolor[HTML]{EFEFEF}all & \cellcolor[HTML]{EFEFEF}pas & \cellcolor[HTML]{EFEFEF}seq \\ \hline
ChatGPT-4V  &Qwen-VL &{19.82} &{35.90} &{36.36} &{10.73} &{\underline{4.70}} &{\textbf{4.49}} &{\underline{5.00}}\\
ChatGPT-4V  &LLaVA-1.6-13b &{16.59} &{23.08} &{33.33} &{9.13} &{\textbf{5.37}} &{1.12} &{
\textbf{11.67}}\\    \hline
Qwen-Max    &Qwen-VL &{\textbf{27.90}} &{\underline{36.00}} &{\textbf{50.91}} &{17.81} &{2.68} &{1.12} &{\underline{5.00}}\\
Qwen-Max    &LLaVA-1.6-13b &{23.79} &{\textbf{38.46}} &{\underline{47.27}} &{12.33} &{0.67} &{0} &{1.67} \\ \hline
Qwen-VL     &Qwen-VL &{9.25} &{15.38} &{19.39} &{4.34} &{0.67} &{1.12} &{0}\\
Qwen-VL     &LLaVA-1.6-7b &{12,92} &{29.49} &{15.76} &{8.90} &{1.34} &{1.12} &{1.67}\\
Qwen-VL     &LLaVA-1.6-13b &{11.45} &{19.23} &{23.03} &{5.71} &{1.34} &{\underline{2.25}} &{0}\\ \hline
LLaVA-1.6-7b     &Qwen-VL &{16.30} &{19.23} &{30.91} &{10.27} &{0.67} &{0} &{1.67}\\
LLaVA-1.6-7b     &LLaVA-1.6-7b &{18.36} &{15.38} &{34.55} &{12.79} &{0} &{0} &{0}\\
LLaVA-1.6-7b     &LLaVA-1.6-13b &{21.29} &{25.64} &{32.73} &{16.21} &{1.67} &{\underline{2.25}} &{0}\\ \hline
LLaVA-1.6-13b     &Qwen-VL &{23.94} &{32.05} &{38.18} &{17.12} &{1.34} &{1.12} &{1.67}\\
LLaVA-1.6-13b     &LLaVA-1.6-7b &{23.94} &{30.77} &{31.52} &{\underline{19.18}} &{0.67} &{1.12} &{0}\\
LLaVA-1.6-13b     &LLaVA-1.6-13b &{\underline{26.14}} &{33.33} &{36.36} &{\textbf{21.00}} &{0.67} &{1.12} &{0}\\ \hline
\end{tabular}
}
\caption{\textbf{The main results of method MLLMs reasoning} (Sec.~\ref{subsec:LLMs and MLLMs}), and different from those of method LLMs reasoning in Table~\ref{tab:llm reasoning experiments} in that the main scene is utilized as the image input.}
\label{tab:mllm reasoning experiments}
\end{table}
\\\textbf{Results of MLLMs reasoning. }The main results of reasoning with the MLLM reasoning method~(Sec.~\ref{subsec:LLMs and MLLMs}) are shown in Table~\ref{tab:mllm reasoning experiments}. The closed-source model Qwen-max achieves the best performance in \textbf{CVQA} when captions are generated by Qwen-vl. However, when the main scene is provided to the model as the ``scene theme'', the overall performance of MLLM reasoning is lower than that of LLM reasoning. In \textbf{CVQA}, MLLM reasoning reaches only 27.90\% of the highest score, while LLM reasoning achieves up to 36.86\%. Similarly, in \textbf{CPVQA}, MLLM reasoning achieves only 5.37\% of the highest score, whereas LLM reasoning reaches 8.72\%. Additionally, when utilizing the same closed-source model, MLLM reasoning scores are generally lower than those of LLM reasoning; for instance, ChatGPT-4V performs lower than ChatGPT-4 in \textbf{CVQA}, and Qwen-max performs lower than Qwen-max without main scene input in \textbf{CPVQA}. This suggests that utilizing non-retrieved images as embeddings in complex scenes may weaken the model’s combinatorial reasoning ability. In summary, adding ``scene theme'' offers limited benefits for improving reasoning performance.
\begin{table}[ht]
\resizebox{\linewidth}{!}{
\begin{tabular}{l|l|c|llll|lll}
\hline
\multicolumn{3}{c|}{\cellcolor[HTML]{ECF4FF}Benchmark}       & \multicolumn{4}{c|}{\cellcolor[HTML]{ECF4FF}CVQA(Score)/\%}                                            & \multicolumn{3}{c}{\cellcolor[HTML]{ECF4FF}CPVQA(Score)/\%}                    \\ \hline
Model(LLMs)         & Caption    &COT             & \cellcolor[HTML]{EFEFEF}all & \cellcolor[HTML]{EFEFEF}pc & \cellcolor[HTML]{EFEFEF}pu & \cellcolor[HTML]{EFEFEF}pcl & \cellcolor[HTML]{EFEFEF}all & \cellcolor[HTML]{EFEFEF}pas & \cellcolor[HTML]{EFEFEF}seq \\ \hline
Mistral-7b &Qwen-VL & &{36.12} &{60.26} &{49.70} &{26.71} &{8.72} &\cellcolor[HTML]{fea3a0}{4.49} &{15.00} \\
Mistral-7b &Qwen-VL &\checkmark &\cellcolor[HTML]{fea3a0}{55.21} &\cellcolor[HTML]{fea3a0}{70.50} &\cellcolor[HTML]{fea3a0}{64.85} &\cellcolor[HTML]{fea3a0}{48.86} &\cellcolor[HTML]{fea3a0}{10.07} &{3.37} &\cellcolor[HTML]{fea3a0}{20.00} \\ \hdashline 
Mistral-7b &LLaVA-1.6-7b & &{33.92} &{61.54} &{56.97} &{20.32} &{4.70} &{1.12} &{10.00} \\
Mistral-7b &LLaVA-1.6-7b &\checkmark &\cellcolor[HTML]{fea3a0}{49.19} &\cellcolor[HTML]{fea3a0}{69.23} &\cellcolor[HTML]{fea3a0}{72.12} &\cellcolor[HTML]{fea3a0}{36.99} &\cellcolor[HTML]{fea3a0}{9.40} &\cellcolor[HTML]{fea3a0}{6.74} &\cellcolor[HTML]{fea3a0}{13.33} \\ \hdashline 
Mistral-7b &LLaVA-1.6-13b & &{36.86} &{64.10} &{47.88} &{27.85} &\cellcolor[HTML]{fea3a0}{6.04} &{1.12} &\cellcolor[HTML]{fea3a0}{13.33} \\
Mistral-7b &LLaVA-1.6-13b &\checkmark &\cellcolor[HTML]{fea3a0}{51.40} &\cellcolor[HTML]{fea3a0}{73.08} &\cellcolor[HTML]{fea3a0}{64.24} &\cellcolor[HTML]{fea3a0}{42.69} &{4.03} &\cellcolor[HTML]{fea3a0}{2.25} &{6.67} \\ \hline 
Model(MLLMs)         & Caption    &COT             & \cellcolor[HTML]{EFEFEF}all & \cellcolor[HTML]{EFEFEF}col & \cellcolor[HTML]{EFEFEF}use & \cellcolor[HTML]{EFEFEF}pas & \cellcolor[HTML]{EFEFEF}all & \cellcolor[HTML]{EFEFEF}num & \cellcolor[HTML]{EFEFEF}ord \\ \hline
LLaVA-1.6-7b &Qwen-VL & &\cellcolor[HTML]{fea3a0}{16.30} &{19.23} &\cellcolor[HTML]{fea3a0}{30.91} &\cellcolor[HTML]{fea3a0}{10.27} &\cellcolor[HTML]{fea3a0}{0.67} &{0} &\cellcolor[HTML]{fea3a0}{1.67}\\
LLaVA-1.6-7b &Qwen-VL &\checkmark &{11.75} &\cellcolor[HTML]{fea3a0}{23.08} &{22.42} &{5.71} &{0} &{0} &{0}\\ \hdashline
LLaVA-1.6-7b &LLaVA-1.6-7b & &\cellcolor[HTML]{fea3a0}{18.36} &\cellcolor[HTML]{fea3a0}{15.38} &\cellcolor[HTML]{fea3a0}{34.55} &\cellcolor[HTML]{fea3a0}{12.79} &{0} &{0} &{0}\\
LLaVA-1.6-7b &LLaVA-1.6-7b &\checkmark &{15.71} &{14.10} &{31.52} &{10.46} &{0} &{0} &{0}\\ \hdashline
LLaVA-1.6-7b &LLaVA-1.6-13b & &\cellcolor[HTML]{fea3a0}{21.29} &\cellcolor[HTML]{fea3a0}{25.64} &\cellcolor[HTML]{fea3a0}{32.73} &\cellcolor[HTML]{fea3a0}{16.21} &\cellcolor[HTML]{fea3a0}{1.67} &\cellcolor[HTML]{fea3a0}{2.25} &{0}\\
LLaVA-1.6-7b &LLaVA-1.6-13b &\checkmark &{13.95} &{16.67} &{29.70} &{7.53} &{0.67} &{1.12} &{0}\\ \hline
LLaVA-1.6-13b &Qwen-VL & &\cellcolor[HTML]{fea3a0}{23.94} &\cellcolor[HTML]{fea3a0}{32.05} &\cellcolor[HTML]{fea3a0}{38.18} &\cellcolor[HTML]{fea3a0}{17.12} &{1.34} &{1.12} &{1.67}\\
LLaVA-1.6-13b &Qwen-VL &\checkmark &{16.59} &{21.79} &{27.88} &{11.42} &\cellcolor[HTML]{fea3a0}{14.09} &\cellcolor[HTML]{fea3a0}{3.37} &\cellcolor[HTML]{fea3a0}{30.00}\\ \hdashline
LLaVA-1.6-13b &LLaVA-1.6-7b & &\cellcolor[HTML]{fea3a0}{23.94} &\cellcolor[HTML]{fea3a0}{30.77} &\cellcolor[HTML]{fea3a0}{31.52} &\cellcolor[HTML]{fea3a0}{19.18} &{0.67} &{1.12} &{0}\\
LLaVA-1.6-13b &LLaVA-1.6-7b &\checkmark &{14.39} &{26.92} &{19.39} &{10.27} &\cellcolor[HTML]{fea3a0}{16.78} &\cellcolor[HTML]{fea3a0}{8.98} &\cellcolor[HTML]{fea3a0}{28.33}\\ \hdashline
LLaVA-1.6-13b &LLaVA-1.6-13b & &\cellcolor[HTML]{fea3a0}{26.14} &\cellcolor[HTML]{fea3a0}{33.33} &\cellcolor[HTML]{fea3a0}{36.36} &\cellcolor[HTML]{fea3a0}{21.00} &{0.67} &{1.12} &{0}\\
LLaVA-1.6-13b &LLaVA-1.6-13b &\checkmark &{13.51} &{29.49} &{20.61} &{7.99} &\cellcolor[HTML]{fea3a0}{16.11} &\cellcolor[HTML]{fea3a0}{10.11} &\cellcolor[HTML]{fea3a0}{25.00}\\ \hline
\end{tabular}
}
\caption{\textbf{The ablation results of method COT reasoning without prompting} (Sec.~\ref{subsec:COT without prompting}). \textcolor[HTML]{fea3a0}{Red areas} indicate better results with or without COT reasoning, `\checkmark' denotes taking COT reasoning.}
\label{tab:cot without prompting}
\end{table}
\\\textbf{Ablation on COT reasoning without prompting.} In Table~\ref{tab:cot without prompting}, we utilize the ablation method to demonstrate the effect of COT reasoning without prompting in method COT reasoning without prompting~(Sec.~\ref{subsec:COT without prompting}). 1) In the LLM reasoning method, the open-source model Mistral-7b was utilized as the base model. Table~\ref{tab:cot without prompting} presents the COT reasoning method significantly improved the model’s performance on the \textbf{CVQA} benchmark. However, it only partially enhance performance on the \textbf{CPVQA} benchmark. 2) In the MLLM reasoning method, this paper explores models with different parameter sizes of LLaVA-1.6. We find that in \textbf{CVQA}, the COT reasoning method had little to no effect on improving the performance of MLLMs. In \textbf{CPVQA}, COT reasoning method led to greater improvements for models with larger parameters (13b), which may be related to the random design of our COT reasoning method. Since the number of parameters is proportional to the vocabulary size, models with more parameters have a higher likelihood of generating a complex COT reasoning path. The COT reasoning method may be more effective for text reasoning and models with larger parameters, due to its inherent randomness.
\begin{table}[ht]
\resizebox{\linewidth}{!}{
\begin{tabular}{l|l|c|llll|lll}
\hline
\multicolumn{3}{c|}{\cellcolor[HTML]{ECF4FF}Benchmark}       & \multicolumn{4}{c|}{\cellcolor[HTML]{ECF4FF}CVQA(Score)/\%}                                            & \multicolumn{3}{c}{\cellcolor[HTML]{ECF4FF}CPVQA(Score)/\%}                    \\ \hline
Model         & Caption    &DOI             & \cellcolor[HTML]{EFEFEF}all & \cellcolor[HTML]{EFEFEF}pc & \cellcolor[HTML]{EFEFEF}pu & \cellcolor[HTML]{EFEFEF}pcl & \cellcolor[HTML]{EFEFEF}all & \cellcolor[HTML]{EFEFEF}pas & \cellcolor[HTML]{EFEFEF}seq\\ \hline
LLaVA-1.6-7b &Qwen-VL &- &{16.30} &{19.23} &{30.91} &{10.27} &{0.67} &{0} &{1.67} \\ \hdashline
LLaVA-1.6-7b &Qwen-VL &\checkmark &{18.21} &\cellcolor[HTML]{fea3a0}{25.64} &{33.94} &{10.96} &\cellcolor[HTML]{fea3a0}{2.01} &\cellcolor[HTML]{fea3a0}{2.25} &{1.67} \\ \hdashline
LLaVA-1.6-7b &Qwen-VL &\ding{55} &\cellcolor[HTML]{fea3a0}{26.58} &{21.79} &\cellcolor[HTML]{fea3a0}{49.07} &\cellcolor[HTML]{fea3a0}{18.72} &\cellcolor[HTML]{fea3a0}{2.01} &{0} &\cellcolor[HTML]{fea3a0}{5.00} \\ \hline
LLaVA-1.6-7b &LLaVA-1.6-7b &- &{18.36} &{15.38} &{34.55} &{12.79} &{0} &{0} &{0} \\ \hdashline
LLaVA-1.6-7b &LLaVA-1.6-7b &\checkmark &{19.24} &\cellcolor[HTML]{fea3a0}{23.08} &{35.15} &{12.56} &{0.67} &{0} &{1.67} \\ \hdashline
LLaVA-1.6-7b &LLaVA-1.6-7b &\ding{55} &\cellcolor[HTML]{fea3a0}{26.28} &{21.79} &\cellcolor[HTML]{fea3a0}{52.73} &\cellcolor[HTML]{fea3a0}{17.12} &\cellcolor[HTML]{fea3a0}{2.68} &\cellcolor[HTML]{fea3a0}{1.12} &\cellcolor[HTML]{fea3a0}{5.00} \\ \hline
LLaVA-1.6-7b &LLaVA-1.6-13b &- &\cellcolor[HTML]{fea3a0}{23.94} &\cellcolor[HTML]{fea3a0}{32.05} &{38.18} &\cellcolor[HTML]{fea3a0}{17.12} &{1.34} &\cellcolor[HTML]{fea3a0}{1.12} &{1.67} \\ \hdashline
LLaVA-1.6-7b &LLaVA-1.6-13b &\checkmark &{19.82} &\cellcolor[HTML]{fea3a0}{32.05} &{33.94} &{12.33} &{0.67} &\cellcolor[HTML]{fea3a0}{1.12} &{0} \\ \hdashline
LLaVA-1.6-7b &LLaVA-1.6-13b &\ding{55} &{23.35} &{24.36} &\cellcolor[HTML]{fea3a0}{50.30} &{13.01} &\cellcolor[HTML]{fea3a0}{2.01} &{0} &\cellcolor[HTML]{fea3a0}{5.00} \\ \hline
Qwen-VL &Qwen-VL &- &{9.25} &{15.38} &{19.39} &{4.34} &{0.67} &\cellcolor[HTML]{fea3a0}{1.12} &{0} \\ \hdashline
Qwen-VL &Qwen-VL &\checkmark &{10.13} &{11.54} &{23.03} &{5.02} &{0} &{0} &{0} \\ \hdashline
Qwen-VL &Qwen-VL &\ding{55} &\cellcolor[HTML]{fea3a0}{20.70} &\cellcolor[HTML]{fea3a0}{20.51} &\cellcolor[HTML]{fea3a0}{41.82} &\cellcolor[HTML]{fea3a0}{12.79} &\cellcolor[HTML]{fea3a0}{1.34} &\cellcolor[HTML]{fea3a0}{1.12} &\cellcolor[HTML]{fea3a0}{1.67} \\ \hline
Qwen-VL &LLaVA-1.6-7b &- &{12,92} &\cellcolor[HTML]{fea3a0}{29.49} &{15.76} &{8.90} &\cellcolor[HTML]{fea3a0}{1.34} &\cellcolor[HTML]{fea3a0}{1.12} &\cellcolor[HTML]{fea3a0}{1.67} \\ \hdashline
Qwen-VL &LLaVA-1.6-7b &\checkmark &{12.92} &{20.51} &{23.03} &{7.76} &{0.67} &{0} &\cellcolor[HTML]{fea3a0}{1.67} \\ \hdashline
Qwen-VL &LLaVA-1.6-7b &\ding{55} &\cellcolor[HTML]{fea3a0}{22.76} &{25.64} &\cellcolor[HTML]{fea3a0}{45.45} &\cellcolor[HTML]{fea3a0}{13.70} &\cellcolor[HTML]{fea3a0}{1.34} &\cellcolor[HTML]{fea3a0}{1.12} &\cellcolor[HTML]{fea3a0}{1.67} \\ \hline
Qwen-VL &LLaVA-1.6-13b &- &{11.45} &\cellcolor[HTML]{fea3a0}{19.23} &{23.03} &{5.71} &{1.34} &{2.25} &{0} \\ \hdashline
Qwen-VL &LLaVA-1.6-13b &\checkmark &{12.92} &\cellcolor[HTML]{fea3a0}{26.92} &{24.24} &{6.16} &\cellcolor[HTML]{fea3a0}{2.01} &\cellcolor[HTML]{fea3a0}{3.37} &{0} \\ \hdashline
Qwen-VL &LLaVA-1.6-13b &\ding{55} &{19.53} &{24.36} &\cellcolor[HTML]{fea3a0}{42.42} &\cellcolor[HTML]{fea3a0}{10.05} &{1.34} &{1.12} &\cellcolor[HTML]{fea3a0}{1.67} \\ \hline
\end{tabular}
}
\caption{\textbf{The ablation results of method semantic retrieval} (Sec.~\ref{subsec:semantic and visual retrieval}), where the DOI indicates Description of Other Images. The `DOI' in the table, (\checkmark) combines other image captions in the final prediction; (\ding{55}) excludes combination, aligning with semantic retrieval (Sec.~\ref{subsec:semantic and visual retrieval}); (-) uses only MLLM for reasoning.}
\label{tab:semantic retrieval}
\end{table}
\\\textbf{Ablation on semantic retrieval.} We compare the results of three methods in Table~\ref{tab:semantic retrieval}, the MLLM reasoning method in contextual learning (Sec.~\ref{subsec:LLMs and MLLMs}), utilizing inputs $\mathcal{C}$ and $v'$, and only $v'$ as input, without $\mathcal{C}$ (Sec.~\ref{subsec:semantic and visual retrieval}). Specifically, in \textbf{CVQA}, the method that relies solely on $v'$ as input, without incorporating $\mathcal{C}$, demonstrates a significant advantage. Notably, when utilizing captions generated by Qwen-VL and employing Qwen-VL for reasoning, this approach consistently achieves the highest scores. In contrast, the method utilizing both $\mathcal{C}$ and $v'$ yields suboptimal results, suggesting that in \textbf{CVQA}, our approach in semantic retrieval (Sec.~\ref{subsec:semantic and visual retrieval}) outperforms contextual learning (Sec.~\ref{subsec:LLMs and MLLMs}). Similarly, in \textbf{CPVQA}, the method that relies solely on $v'$ without incorporating $\mathcal{C}$ demonstrates a slight performance advantage over other approaches, whereas the MLLM reasoning method within the contextual learning framework produces less than optimal results.\\
\textbf{Additional Experiments. }
\label{AppendixB}
Given that models capable of handling multi-image inputs exist. To maintain the comprehensiveness of our experiment, we utilize multi-image input models to directly evaluate their performance on our proposed benchmarks. The results are presented in Table~\ref{tab:mut-model}. 
\begin{table}[ht]
\resizebox{\linewidth}{!}{
\begin{tabular}{ll|llll|lll}
\hline
\multicolumn{2}{c|}{Model}      & all(\textbf{CVQA})   & pc    & pu    & pcl   & all(\textbf{CPVQA})   & pas   & seq   \\ \hline
\multicolumn{2}{l|}{LLaVA-OV-7b~\cite{Li2024LLaVAOneVisionEV}}            & 24.08 & \underline{35.89} & \textbf{40.00} & 15.98 & 0.45   & \underline{6.67}  & 0  \\
\multicolumn{2}{l|}{Qwen2-vl-7B~\cite{Qwen2VL}}                & 18.94 & 20.51 & 24.24 & 16.67 & 2.91    & 3.88   & 2.25   \\
\multicolumn{2}{l|}{Qwen-vl-max~\cite{Qwenmax}}                & \underline{30.10} & 30.77 & \underline{26.06} & \underline{31.51} & \underline{3.58}    & 3.37   & \underline{3.89}   \\
\multicolumn{2}{l|}{Qvq-72B-preview~\cite{qvq-72b-preview}}            & \textbf{46.70} & \textbf{43.59} & \textbf{40.00} & \textbf{49.77} & \textbf{20.36}   & \textbf{26.58}  & \textbf{11.11}  \\
 \hline
\end{tabular}
}
\caption{\textbf{Evaluation on multiple image inputs.} Performance of models with multiple images as input on benchmarks.}
\label{tab:mut-model}
\end{table}
Even the large-parameter model Qwen-vl-max achieves an overall performance of only 30.10\% on our \textbf{CVQA} benchmark, and more shockingly, it scores just 3.58\% on the \textbf{CPVQA}. However, while the score of Qvq-72B is not particularly high, it outperforms other models and methods.\\
\subsection{In-depth Analysis}
\label{subsec:experiment analysis}
In our exploration and experiments, we identify three primary types of errors in the model’s reasoning. First, the model often relies on a single source of information rather than integrating multiple relevant inputs for joint reasoning. For instance, it may generate the answer ``6325'' without considering the color order depicted in Figure~\ref{fig:intro_example}, highlighting deficiencies in its multimodal retrieval and integration capabilities. Second, the model tends to over-rely on the explicit content of the query without critically analyzing additional contextual information. For example, in Figure ~\ref{fig:problem_formulation}, it directly concludes that the final trophy sequence is ``ABCD'' reflecting a tendency to follow surface-level cues rather than engage in deeper reasoning. Finally, when faced with ambiguous or incomplete information, the model often determines that the provided data is insufficient and instead resorts to prior knowledge for inference. This over-reliance on pre-existing biases underscores its limitations in fine-grained recognition and precise analytical reasoning.\\
Furthermore, effective methods, as shown in Tables~\ref{tab:cot without prompting} and~\ref{tab:mut-model}, models using COT or trained on COT data perform better, suggesting that enhanced reasoning methods, such as COT~\cite{COTwithoutprompting,Wei2022ChainOT} and RL-trained models~\cite{DeepSeekAI2025DeepSeekR1IR}, may be more effective in analyzing information from multiple sources. Another notable observation is that text-based reasoning frequently outperforms joint reasoning with the input image, indicating that the integration of multiple visual information sources warrants further refinement.