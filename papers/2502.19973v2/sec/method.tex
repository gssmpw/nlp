\section{Methods}
\label{sec:method}
This section proposes three baseline methods to address our benchmarks without relying on prompt engineering. First, reasoning with models is discussed in Sec.\ref{subsec:LLMs and MLLMs}. Next, COT reasoning without prompts is covered in Sec.\ref{subsec:COT without prompting}. Finally, an approach for semantic retrieval on image sets is introduced in Sec.\ref{subsec:semantic and visual retrieval}. In this study, considering that most MLLMs~\cite{Mialon2023AugmentedLM} at this stage can effectively handle single-image input, all of methods are based on single-image inputs.
\subsection{Contextual Learning}
\label{subsec:LLMs and MLLMs}
In this subsection, we introduce two approaches for inference using LLMs and MLLMs.\\
\textbf{LLMs reasoning.} The benchmark we design consists of an image set with multiple images $V = \{v_1,v_2,\dots,v_n\}$ and a text-based question $T = (Q_{c} | Q_{p})$ as input. Since most current MLLMs effectively process only single images, captions are first generated for each image utilizing MLLMs. The scenario’s question, along with the captions from all images, is then input into LLMs for prediction:
\begin{equation}
    A_{LLM} = LLM\{T,\sum_{i=1}^n[Caption(v_i)]\}
\end{equation}
where $LLM\{T_1,T_2\}$ denotes that the LLM utilizes $T_1$ and $T_2$ as text inputs for prediction, generating the prediction result $A_{LLM}$, and $Caption(v)$ represents the conversion of image $v$ into caption utilizing MLLM.\\
\textbf{MLLMs reasoning.} In this study, each scene in the benchmark contains a main scene $V_m$ that, although not rich in detail, conveys the overall theme and key entities of the scene. Therefore, we first use the visual input from the $V_m$ to help MLLMs understand the theme of the scene, and then combine the detailed descriptions $\sum_{i=1}^n[Caption(v_i)]$ of individual images for the final prediction, specifically:
\begin{equation}
    A_{MLLM} = MLLM\{V_m,T,\sum_{i=1}^n[Caption(v_i)]\}
\end{equation}
where $MLLM\{v,T_m\}$ represents the MLLM utilizes $v$ as image input and $T_m$ as text input for prediction, generating the prediction result $A_{MLLM}$.
\subsection{COT Reasoning Without Prompting}
\label{subsec:COT without prompting}
This subsection primarily introduces a COT reasoning without prompting method. Specifically, we introduce the minimum margin decoding strategy to address uncertainty and then generate coherent answers.\\
\textbf{Minimum margin decoding.} Greedy decoding selects the best option at each step. Inspired by research~\cite{COTwithoutprompting}, the minimum margin decoding strategy not only considers the best option but also measures the model’s confidence by comparing the probability difference between the best and second-best options. Specifically, we calculate the probabilities of the best option $x1_t$ and the second-best option $x2_t$ at each step, utilizing their difference to represent the confidence: 
\begin{equation}
\Delta_t = p(x1_t | x_{<t}) - p(x2_t | x_{<t})
\end{equation}
where $t$ is each generation step and $p(x_t | x_{<t}$ represents the probability distribution of the model. The overall confidence of the final answer can be formalized as: 
\begin{equation}
\Delta_{\text{answer}} = \frac{1}{|x_{\text{answer}}|} \sum_{t \in \text{answer}} \Delta_t
\end{equation}
where $|x_{\text{answer}}|$ represents the total number of steps in the answer. A larger $\Delta_{\text{answer}}$ value indicates that the model has a higher confidence in the entire answer.\\
\textbf{Generate coherent answers.} In this study, when the model is uncertain at a given step $t$ and $\Delta_t$ falls below a certain threshold, we introduce randomness to ensure the consistency, credibility, and diversity of the generated results. Specifically, we use a temperature sampling method to introduce this randomness. The process is formulated as:
\begin{equation}
p(x_t | x_{<t}) = \frac{p(x_t | x_{<t})^{1/\mathcal{T}}}{\sum_{x' \in \mathcal{V}} p(x' | x_{<t})^{1/\mathcal{T}}}
\end{equation}
where $\mathcal{T}$ is a temperature parameter that controls the strength of randomness, and $\mathcal{V}$ is the vocabulary.
\subsection{Semantic Retrieval}
\label{subsec:semantic and visual retrieval}
In this subsection, we introduce a semantic retrieval method for handling multiple image inputs. While the main scene in contextual learning/~(Sec.~\ref{subsec:LLMs and MLLMs}) of this study provides the MLLM with key information, such as the theme of the scene as the image input, it may not always be the image most relevant to the answer. Therefore, across all known scenes, we evaluate the semantic relevance between the question and the scene descriptions to identify the image likely to be relevant to the answer. 
This study implements two methods:

(1)We utilize MLLM to generate captions $\mathcal{C}$ for complex scenes $V = \{v_1,v_2\dots,v_n\}$:
\begin{equation}
    \mathcal{C} = \sum_{i=1}^n[Caption(v_i)]
    \label{equ_captions}
\end{equation}
provide the LLM with $\mathcal{C}$ as background, and combine them with the scenario’s question. The LLM then determines the most semantically relevant image $v'$, which is considered the one most likely related to the answer. This image’s encoding is input into the MLLM as a visual embedding, along with the question and captions $\mathcal{C}$, for prediction.

(2) We utilize the language model’s semantic discernment capability to identify the caption $Caption(v')$ of the image $v'$ that is most likely relevant to the question, given $\mathcal{C}$ as background context in Equation~\ref{equ_captions}. After identifying this image $v'$, we input its encoding, together with the question, into the MLLM for prediction.
