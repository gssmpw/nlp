\begin{abstract}\label{sec:absract}
Combining multiple perceptual inputs and performing combinatorial reasoning in complex scenarios is a sophisticated cognitive function in humans. 
With advancements in multi-modal large language models, recent benchmarks tend to evaluate visual understanding across multiple images. 
However, they often overlook the necessity of combinatorial reasoning across multiple perceptual information. 
To explore the ability of advanced models to integrate multiple perceptual inputs for combinatorial reasoning in complex scenarios, we introduce two benchmarks: \textbf{C}lue-\textbf{V}isual \textbf{Q}uestion \textbf{A}nswering~(\textbf{CVQA}), with three task types to assess visual comprehension and synthesis, and \textbf{C}lue of \textbf{P}assword-\textbf{V}isual \textbf{Q}uestion \textbf{A}nswering~(\textbf{CPVQA}), with two task types focused on accurate interpretation and application of visual data. 
For our benchmarks, we present three plug-and-play approaches: utilizing model input for reasoning, enhancing reasoning through minimum margin decoding with randomness generation, and retrieving semantically relevant visual information for effective data integration. 
The combined results reveal current modelsâ€™ poor performance on combinatorial reasoning benchmarks, even the state-of-the-art (SOTA) closed-source model achieves only 33.04\% accuracy on \textbf{CVQA}, and drops to 7.38\% on \textbf{CPVQA}. Notably, our approach improves the performance of models on combinatorial reasoning, with a 22.17\% boost on \textbf{CVQA} and 9.40\% on \textbf{CPVQA} over the SOTA closed-source model, demonstrating its effectiveness in enhancing combinatorial reasoning with multiple perceptual inputs in complex scenarios. The code will be publicly available.
\end{abstract}

