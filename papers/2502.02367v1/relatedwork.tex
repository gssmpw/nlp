\section{Related works}
% \label{related} 
 


% \textbf{Noise-to-Data.} The first attempt to bind generative models and thermodynamics is Energy-Based models(EBM)\cite{lecun2005loss}. These models find energy state function for data and generate new samples through simulation of Langevin Dynamics \cite{du2019implicit}. Another class of generative models \cite{sohl2015deep} inspired by thermodynamics is Diffusion Models(DM), which are extremely popular nowadays. These models are composed of forward and backward stochastic processes \cite{song2020score}. While the forward process corrupts data, injecting Gaussian noise simultaneously, the backward process reverses the forward, recovering data. The more recent Poisson Flow Generation Models (PFGM) \cite{xu2022poissonflowgenerativemodels}  is inspired by the electrostatic theory. PFGM regards the data as charges in a hyperplane and approximates electric field in augmented space between the hyperplane and upper semi-sphere, whose flux is uniform. Then, PFGM simulates backward ODE along field lines , generating  data samples. 

% Nonetheless, DM and PFGM is only applied for \textbf{Noise-to-data} setting and can't build transformation between untractable distributions. 

% \textbf{Data-to-Data.} There are some generative models, allowing build maps between complex data distributions. Instead of adding and removing noise as DM, Flow matching (FM) \cite{lipman2022flow,liu2022flow} is ODE-based approach that transforms probability distributions. FM learns the time derivative of this transformation(a.k.a flow) as a time-dependent vector field (velocity), approximating  this by neural networks with $\mathcal{L}$2 loss function. Bridge Matching (BM) \cite{shi2024diffusion, albergo2022building, gushchin2024adversarial}  is SDE-based methodology for which FM is the limiting  case.