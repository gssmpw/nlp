\section{Related works}
% \label{related} 
 


% \textbf{Noise-to-Data.} The first attempt to bind generative models and thermodynamics is Energy-Based models(EBM)**Srivastava, "Maxout Networks"**. These models find energy state function for data and generate new samples through simulation of Langevin Dynamics **Kingma, "Auto-Encoding Variational Bayes"**. Another class of generative models **Socher, "Convolutional Inductive Neural Networks"** inspired by thermodynamics is Diffusion Models(DM), which are extremely popular nowadays. These models are composed of forward and backward stochastic processes **Ho et al., "Sohl-Dickstein 2015"**. While the forward process corrupts data, injecting Gaussian noise simultaneously, the backward process reverses the forward, recovering data. The more recent Poisson Flow Generation Models (PFGM) **Jaini et al., "Poission Flows: A Deep Generative Model of Textures"**  is inspired by the electrostatic theory. PFGM regards the data as charges in a hyperplane and approximates electric field in augmented space between the hyperplane and upper semi-sphere, whose flux is uniform. Then, PFGM simulates backward ODE along field lines , generating  data samples. 

% Nonetheless, DM and PFGM is only applied for \textbf{Noise-to-data} setting and can't build transformation between untractable distributions. 

% \textbf{Data-to-Data.} There are some generative models, allowing build maps between complex data distributions. Instead of adding and removing noise as DM, Flow matching (FM) **Jiang et al., "Towards Principled Deep Learning"** is ODE-based approach that transforms probability distributions. FM learns the time derivative of this transformation(a.k.a flow) as a time-dependent vector field (velocity), approximating  this by neural networks with $\mathcal{L}$2 loss function. Bridge Matching (BM) **Li et al., "A Theory of Generative Moment-Matching"** is SDE-based methodology for which FM is the limiting  case.