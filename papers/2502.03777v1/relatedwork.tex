\section{Related Work}
\subsection{Test-time adaption}
Test-time adaptation (TTA)~\citep{TTA-MEMO,TTA-TPT,TTA-SwapPrompt,TTA-TDA,TTA-RLCF,TTA-NotEnough,TTA-Adapting,TTA-MODE} enables models to adapt changing distributions during testing time without accessing to the source domain data or extensive target domain data. Within the spectrum of TTA settings, \textit{e.g.}, ``fully'' TTA~\citep{TTA-Tent,TTA-Delta}, ``online'' TTA~\citep{TTA-Stationary,TTA-NotEnough}, ``continuous'' TTA~\citep{TTA-VIDA,TTA-EcoTTA}, and ``prior'' TTA~\citep{TTA-prior-1,TTA-prior-2}, ``online'' TTA~\citep{TTA-TPT,TTA-TDA,TTA-RLCF} focuses on adapting to individual samples and is particularly valuable in many application domains, such as autonomous driving, where weather conditions are constantly changing, and road monitoring, where traffic patterns are continually evolving. MEMO~\citep{TTA-MEMO} is the pioneering work that proposes consistent predictions across diverse augmented views. Following this, TPT~\citep{TTA-TPT} notably enhances the generalization capabilities of the CLIP~\citep{VLMs-Openai-CLIP} model to unseen test data by entropy minimization. SwapPrompt~\citep{TTA-SwapPrompt} utilizes online and target prompts, enhancing the CLIP's adaptability by preserving historical information and alternating prediction. In contrast, TDA~\citep{TTA-TDA} adapts to streaming input data by constructing a dynamic key-value cache from historical data. RLCF~\citep{TTA-RLCF} incorporates reinforcement learning to distill knowledge into more compact models. Among these works, MEMO~\citep{TTA-MEMO}, TPT~\citep{TTA-TPT}, and RLCF~\citep{TTA-RLCF} are particularly challenging, as the model is reset after adapting a test instance, obviating the need to retain historical knowledge, and thereby accommodating continuously shifting test distributions. Nonetheless, these methods are primarily designed for multi-class classification and may not be as effective in the more common multi-label scenario.

\subsection{Prompt Learning in VLMs}
Visual-language models~(VLMs)~\citep{VLMs-ALBEF,VLMs-RegionCLIP,VLMs-Openai-CLIP,VLMs-BLIP-2,VLMs-XVLM2}, trained on massive image-text pairs~\citep{ITP-CC3M,ITP-Laion-5b}, have demonstrated remarkable proficiency in cross-task learning. To further enhance the transfer abilities of CLIP~\citep{VLMs-Openai-CLIP}, researchers have developed various prompt learning techniques~\citep{Prompt-CoOp,Prompt-CoCoOp,Prompt-MAPLE,Prompt-PromptKD,Prompt-TCP,TAI-TAIDPT,TAI-PVP}. For instance, the groundbreaking work CoOp~\citep{Prompt-CoOp}, and its advancement CoCoOp~\citep{Prompt-CoCoOp}, are the first to propose optimizing context vectors to improve the generalization capabilities of CLIP. Maple~\citep{Prompt-MAPLE} introduces a multimodal prompt learning method, designed to recalibrate both visual and language modalities. Dept~\citep{Prompt-DEPT} and PromptKD~\citep{Prompt-PromptKD} take on the challenge from the perspectives of knowledge retention and distillation, respectively, to promote robust generalization on novel tasks. Exploiting the aligned visual-language space of CLIP~\citep{VLMs-Openai-CLIP}, TAI-DPT~\citep{TAI-TAIDPT}, PVP~\citep{TAI-PVP} and RC-TPL~\citep{TAI-RC-TPL} propose to regard texts as images for prompt tuning in zero-shot multi-label image classification. Investigations like DualCoOp~\citep{Prompt-DualCoOp}, DualCoOp++~\citep{Prompt-DualCoOp++}, and VLPL~\citep{Prompt-VLPL} consider more intricate tasks, enhancing multi-label classification capabilities in the partial-label scenario. In contrast, our study focuses on a training-free paradigm, termed multi-label test-time adaptation, which obviates the need for the source training data and is exclusively at the testing instance level.