\documentclass{article} % For LaTeX2e
\usepackage[table]{xcolor}
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{amsthm}
\newtheorem{proposition}{Proposition}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx,wrapfig}
\usepackage{amsmath}
\usepackage{ulem} 
\usepackage{enumitem}
\usepackage[thicklines]{cancel}
\usepackage{booktabs}
\usepackage{ulem}
\usepackage{multirow}
\usepackage{algpseudocode}
\usepackage{caption}

\usepackage[ruled,longend,linesnumbered]{algorithm2e}
\renewcommand{\algorithmiccomment}[1]{\hfill $\triangleright$ #1}

\newcommand{\myfootnote}[1]{\footnotemark[1]}
\newcommand{\addsymbolfootnote}{\footnotetext[1]{Corresponding author}}

\makeatother

\title{Multi-Label Test-Time Adaptation with Bound Entropy Minimization}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

% \author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
% about author (webpage, alternative address)---\emph{not} for acknowledging
% funding agencies. Funding acknowledgements go at the end of the paper.} \\
% Department of Computer Science\\
% Cranberry-Lemon University\\
% Pittsburgh, PA 15213, USA \\
% \texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
% \And
% Ji Q. Ren \& Yevgeny LeNet \\
% Department of Computational Neuroscience \\
% University of the Witwatersrand \\
% Joburg, South Africa \\
% \texttt{\{robot,net\}@wits.ac.za} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
% }

\author{
Xiangyu Wu$^{1,2}$,~~Feng Yu$^{1}$,~~Qing-Guo Chen$^2$,~~Yang Yang$^1$\thanks{Corresponding author},~~Jianfeng Lu$^1$\myfootnote{Corresponding author} \\
$^1$Nanjing University of Science and Technology\\
$^2$Alibaba International Digital Commerce Group\\
% \{wxy\_yyjhl,yyang,lujf\}@njust.edu.cn, qyjiang24@gmail.com, \\
% \{yixin.wyf,qingguo.cqg\}@alibaba-inc.com
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle
\vspace{-2em}
\begin{abstract}
Mainstream test-time adaptation~(TTA) techniques endeavor to mitigate distribution shifts via entropy minimization for multi-class classification, inherently increasing the probability of the most confident class. However, when encountering multi-label instances, the primary challenge stems from the varying number of labels per image, and prioritizing only the highest probability class inevitably undermines the adaptation of other positive labels. To address this issue, we investigate TTA within multi-label scenario~(\textbf{ML--TTA}), developing \textbf{B}ound \textbf{E}ntropy \textbf{M}inimization~(\textbf{BEM}) objective to simultaneously increase the confidence of multiple \textit{top} predicted labels. Specifically, to determine the number of labels for each augmented view, we retrieve a paired caption with yielded textual labels for that view. These labels are allocated to both the view and caption, called \textit{weak label set} and \textit{strong label set} with the same size \textit{k}. Following this, the proposed BEM considers the highest \textit{top-k} predicted labels from view and caption as a single entity, respectively, learning both view and caption prompts concurrently. By binding \textit{top-k} predicted labels, BEM overcomes the limitation of vanilla entropy minimization, which exclusively optimizes the most confident class. Across the MSCOCO, VOC, and NUSWIDE multi-label datasets, our ML--TTA framework equipped with BEM exhibits superior performance compared to the latest SOTA methods, across various model architectures, prompt initialization, and varying label scenarios. The code is available at \url{https://github.com/Jinx630/ML-TTA}.
\end{abstract}

% 视觉语言模型强大的zero-shot能力，分布变化的online测试点会失效。test-time adaptation技术被提出..., 通过熵最小化来增大正确类别的置信度，例如...，我们可以轻易的证明熵损失在增大最置信类别概率的同时，减小其他负类别的预测概率。然而，在现实..., 多标签图像...，包含大量的正标签, 如果直接直接应用...最大化...必然会...。因此，我们可以设想一个这样的场景，即图像标签不多，数量已知，这样..., 然而，这在实际应用中显然是不可行的。，为了解决以上问题，在本论文中，我们首次提出了多标签测试时适应技术。我们首先观察到这样一类工作..., 这启发我们...。因此，在本论文中，我们首先证明将所有正..., 可以同时增大所有..., 由于图像标签数量..., 我们借助TAI-DPT..., 引入文本描述数据来推动多标签测试时适应技术。这有两个好处...。

\section{Introduction}
The advent of vision-language models~(VLMs)~\citep{VLMs-Openai-CLIP,VLMs-ALBEF,VLMs-BLIP-2,VLMs-XVLM2} has facilitated remarkable generalization capabilities by pretraining on massive datasets. Nonetheless, VLMs such as CLIP~\citep{VLMs-Openai-CLIP}, still require sophisticated prompt learning techniques when confronted with considerable discrepancies between training and testing domains, to prevent performance degradation due to distribution shifts occurring during testing time.

Fortunately, recent advancements~\citep{TTA-TPT,TTA-DiffTPT,TTA-SwapPrompt,TTA-DART,TTA-DMN,TTA-RLCF,TTA-TDA,TTA-C-TPT,TTA-Fast} allow for immediate adaptation to any distribution of test instance during testing time, which is known as Test-Time Adaptation~(TTA). As pioneering works, TPT~\citep{TTA-TPT} and its enhancement, DiffTPT~\citep{TTA-DiffTPT}, select a set of confident augmented views, learning instance-level prompt for each test instance. DART~\citep{TTA-DART} and DMN~\citep{TTA-DMN}, to fully utilize the encountered knowledge from past samples, design dual-modal knowledge retention prompts and dynamic dual-memory networks, respectively, to adaptively incorporate historical knowledge. The central premise of these methods is entropy minimization, which aims to minimize inconsistency and uncertainty over the model predictions, and further increase the prediction probability of the highest confidence class, a theory that is readily demonstrable.

\begin{figure*}
 \centering
 \includegraphics[scale=0.85]{figures/Performance.pdf}
 \vspace{-0.5em}
 \captionsetup{font={footnotesize}}
 \caption{(a). Compared to CLIP~\citep{VLMs-Openai-CLIP}, ML--TTA increases all positive label logits simultaneously, while others focus only on \textit{top-1} class. (b). Comparison of various methods on images with varying numbers. Compared to CLIP, as the number of labels per image rises, the adaptability of TPT~\citep{TTA-TPT} and RLCF~\citep{TTA-RLCF} in handling multi-label images shows a marked decrease.}
 \vspace{-1.5em}
 \label{fig:Performance-comparsion}
\end{figure*}

Although entropy loss is advantageous for TTA as an uncertainty metric, a natural question arises: Can it be reliably applied to instances with multiple positive labels? As illustrated in Figure~\ref{fig:Performance-comparsion}~(a), for the positive label set~\{\textit{keyboard, phone, remote, mouse, book}\}, compared to CLIP, all methods consistently boost the probability of the most confident class, \textit{keyboard}. Nonetheless, TPT~\citep{TTA-TPT} and RLCF~\citep{TTA-RLCF} adversely impair the remaining positive labels. This indicates that existing TTA methods primarily focus on increasing the confidence of \textit{top-1} label, leading to insufficient adaptation for other positive labels. Given this, we expect to treat the highest \textit{top-k} positive labels as a single label, aiming to simultaneously increase the predicted confidence of multiple \textit{top-k} labels. However, positive label sets are not known in advance in real applications.

Based on the preceding discussion, we investigate the TTA within multi-label scenario~(\textbf{ML--TTA}) and propose a novel theoretical optimization objective named \textbf{B}ound \textbf{E}ntropy \textbf{M}inimization~(\textbf{BEM}), which posits that when the highest \textit{top-k} predicted labels~(\textit{k} being the size of positive label set) share identical probabilities, the entropy loss will uniformly increase the probabilities of all \textit{top-k} classes. Consider a multi-label test image with a set of augmented views, to determine the number of positive labels for each view, we retrieve a paired caption with derived textual labels for each view, which then serves as \textit{weak label set} of size \textit{k} for the corresponding view. Furthermore, owing to the aligned visual-language space of CLIP~\citep{VLMs-Openai-CLIP}, texts can be treated as pseudo-images with known positive labels, a premise corroborated by recent academic research~\citep{TAI-TAIDPT,TAI-RC-TPL,TAI-PT-Text,TAI-PVP}. Drawing inspiration from these findings, we conceptualize each paired caption as a pseudo-view possessing a known label set, termed \textit{strong label set}, of the same size \textit{k}, since the textual labels are directly derived from captions. 

Upon determining the \textit{weak label set} for each view and the \textit{strong label set} for each paired caption, the proposed BEM objective binds the highest \textit{top-k} predicted labels as a single label for both view and caption. By optimizing the view prompt and caption prompt, the model is encouraged to concurrently increase the confidence of the \textit{top-k} classes. Additionally, since some augmented views and paired captions may fail to capture the target label area, leading to misleading predictions, we adopt \textit{confidence selection} utilized in TPT~\citep{TTA-TPT} to filter out ``noisy'' views and captions with high entropy~(\textit{i.e.}, low confidence). Consequently, in this paper, starting from TPT, the developed ML--TTA framework equipped with BEM endows the CLIP's adaptability of multi-label instances during testing. Our contributions are summarized as follows:

\begin{itemize}[leftmargin=*]
\item We examine the \textbf{M}ulti-\textbf{L}abel \textbf{T}est-\textbf{T}ime \textbf{A}daptation~(ML--TTA) and propose \textbf{B}ound \textbf{E}ntropy \textbf{M}inimization~(BEM), which simultaneously increase the probabilities of all highest \textit{top} labels.
\item BEM binds \textit{weak label set} of view and \textit{strong label set} of the caption as a single label, respectively, learning instance-level view and caption prompts for adapting multi-label test instances.
\item On the MSCOCO, VOC, and NUSWIDE datasets, ML--TTA outperforms the original CLIP model as well as other state-of-the-art TTA methods designed for multi-class classification, across various model architectures, prompt initialization, and varying label scenarios.
\end{itemize}

\section{Related Work}
\subsection{Test-time adaption}
Test-time adaptation (TTA)~\citep{TTA-MEMO,TTA-TPT,TTA-SwapPrompt,TTA-TDA,TTA-RLCF,TTA-NotEnough,TTA-Adapting,TTA-MODE} enables models to adapt changing distributions during testing time without accessing to the source domain data or extensive target domain data. Within the spectrum of TTA settings, \textit{e.g.}, ``fully'' TTA~\citep{TTA-Tent,TTA-Delta}, ``online'' TTA~\citep{TTA-Stationary,TTA-NotEnough}, ``continuous'' TTA~\citep{TTA-VIDA,TTA-EcoTTA}, and ``prior'' TTA~\citep{TTA-prior-1,TTA-prior-2}, ``online'' TTA~\citep{TTA-TPT,TTA-TDA,TTA-RLCF} focuses on adapting to individual samples and is particularly valuable in many application domains, such as autonomous driving, where weather conditions are constantly changing, and road monitoring, where traffic patterns are continually evolving. MEMO~\citep{TTA-MEMO} is the pioneering work that proposes consistent predictions across diverse augmented views. Following this, TPT~\citep{TTA-TPT} notably enhances the generalization capabilities of the CLIP~\citep{VLMs-Openai-CLIP} model to unseen test data by entropy minimization. SwapPrompt~\citep{TTA-SwapPrompt} utilizes online and target prompts, enhancing the CLIP's adaptability by preserving historical information and alternating prediction. In contrast, TDA~\citep{TTA-TDA} adapts to streaming input data by constructing a dynamic key-value cache from historical data. RLCF~\citep{TTA-RLCF} incorporates reinforcement learning to distill knowledge into more compact models. Among these works, MEMO~\citep{TTA-MEMO}, TPT~\citep{TTA-TPT}, and RLCF~\citep{TTA-RLCF} are particularly challenging, as the model is reset after adapting a test instance, obviating the need to retain historical knowledge, and thereby accommodating continuously shifting test distributions. Nonetheless, these methods are primarily designed for multi-class classification and may not be as effective in the more common multi-label scenario.

\subsection{Prompt Learning in VLMs}
Visual-language models~(VLMs)~\citep{VLMs-ALBEF,VLMs-RegionCLIP,VLMs-Openai-CLIP,VLMs-BLIP-2,VLMs-XVLM2}, trained on massive image-text pairs~\citep{ITP-CC3M,ITP-Laion-5b}, have demonstrated remarkable proficiency in cross-task learning. To further enhance the transfer abilities of CLIP~\citep{VLMs-Openai-CLIP}, researchers have developed various prompt learning techniques~\citep{Prompt-CoOp,Prompt-CoCoOp,Prompt-MAPLE,Prompt-PromptKD,Prompt-TCP,TAI-TAIDPT,TAI-PVP}. For instance, the groundbreaking work CoOp~\citep{Prompt-CoOp}, and its advancement CoCoOp~\citep{Prompt-CoCoOp}, are the first to propose optimizing context vectors to improve the generalization capabilities of CLIP. Maple~\citep{Prompt-MAPLE} introduces a multimodal prompt learning method, designed to recalibrate both visual and language modalities. Dept~\citep{Prompt-DEPT} and PromptKD~\citep{Prompt-PromptKD} take on the challenge from the perspectives of knowledge retention and distillation, respectively, to promote robust generalization on novel tasks. Exploiting the aligned visual-language space of CLIP~\citep{VLMs-Openai-CLIP}, TAI-DPT~\citep{TAI-TAIDPT}, PVP~\citep{TAI-PVP} and RC-TPL~\citep{TAI-RC-TPL} propose to regard texts as images for prompt tuning in zero-shot multi-label image classification. Investigations like DualCoOp~\citep{Prompt-DualCoOp}, DualCoOp++~\citep{Prompt-DualCoOp++}, and VLPL~\citep{Prompt-VLPL} consider more intricate tasks, enhancing multi-label classification capabilities in the partial-label scenario. In contrast, our study focuses on a training-free paradigm, termed multi-label test-time adaptation, which obviates the need for the source training data and is exclusively at the testing instance level.

\section{Method}
In Sec.~\ref{entropy minimization}, we review the entropy minimization widely used in TTA. In Sec.~\ref{Bound}, we highlight the issue that vanilla entropy minimization predominantly increases the probability of \textit{top-1} predicted label and propose a new proposition \textbf{B}ound \textbf{E}ntropy \textbf{M}inimization~(BEM). In Sec.~\ref{ML--TTA}, we present a \textbf{M}ulti-\textbf{L}abel \textbf{T}est-\textbf{T}ime \textbf{A}daptation~(ML--TTA) framework, incorporating BEM, which binds the highest \textit{top} predicted labels of both augmented views and paired captions as an individual single label. ML--TTA consists of view-caption constructing~(Sec.~\ref{caption allocating}) and label binding~(Sec.~\ref{label binding}).

\subsection{Preliminaries}\label{entropy minimization}
The purpose of Test-Time Adaptation is to utilize each test instance once for immediate adaptation before inference, without any prior assumptions about the test data distribution. For the TTA of VLMs, let $\mathcal{M}_\theta$ denote the CLIP model trained on the training dataset $\mathcal{D}^{\text{train}}=\{ (\mathbf{x}_i^{\text{train}},\mathbf{y}_i^{\text{train}})~|~\mathbf{x}_i^{\text{train}}\!\in\! \mathcal{X}^{\text{train}}, \mathbf{y}_i^{\text{train}}\!\in\!\mathcal{Y}^{\text{train}}\}_{i=1}^{M^{\text{train}}}$. The TTA approach, TPT~\citep{TTA-TPT}, incorporates the Marginal Entropy Minimization (MEM) objective to adapt $\mathcal{M}_\theta$ using a solitary instance $\mathbf{x}^{\text{test}}$ from the testing dataset $\mathcal{D}^{\text{test}}=\{ (\mathbf{x}_i^{\text{test}},\mathbf{y}_i^{\text{test}})~|~\mathbf{x}_i^{\text{test}}\!\in\! \mathcal{X}^{\text{test}}, \mathbf{y}_i^{\text{test}}\!\in\!\mathcal{Y}^{\text{test}}\}_{i=1}^{M^{\text{test}}}$. 

Given a test instance $\mathbf{x}^{\text{test}}$ and a set $\mathcal{A}$ of $N$ random augmentation functions, $\mathbf{x}^{\text{test}}$ is first augmented $N$ times to generate a set of different views, represented as $\mathbf{X}^{\text{test}}=\{ \mathbf{x}_j^{\text{test}}~|~\mathbf{x}_j^{\text{test}}=\mathcal A_j(\mathbf{x^{\text{test}}})\}_{j=1}^{N}$. TTA aims to minimize the marginal entropy of these augmented views, encouraging the model to perform consistent and confident predictions. The entropy of an augmented view is defined as:
\begin{equation}
 H(p(\cdot|\mathbf{x}_j^{\text{test}})) = -\sum_{l=1}^{L} p(y = l|\mathbf{x}_j^{\text{test}}) \log(p(y = l|\mathbf{x}_j^{\text{test}}),
\end{equation}
where $l \in \mathcal{Y}^{\text{test}}$ and $L$ is the number of labels in $\mathcal{Y}^{\text{test}}$. The core principle of TPT is to minimize the marginal entropy of the prediction probability distributions of selected confident augmented views by a ratio $\tau$, thereby encouraging the model to make consistent predictions. After obtaining the average entropy of these confident views, denoted as $\tilde{H}$, TPT updates the prompt using a single gradient descent step based on $\tilde{H} $ and performs immediate inference on this test instance. Once inference is done, the model’s prompt and optimizer are reset promptly for adaptation to the next test instance. Owing to its simplicity and effectiveness, Marginal Entropy Minimization has emerged as a \textit{de facto} standard in modern TTA.

\subsection{Bound Entropy Minimization}\label{Bound}
It can be observed that the TPT method selects a subset of confident augmented views with lower entropy~(\textit{i.e.}, high confidence) from $\mathbf{X}^{\text{test}}$, continually minimizing the average entropy of these confident views to maintain consistent model predictions across these views. With respect to vanilla entropy minimization within TTA, the following proposition holds.

\begin{proposition} \label{proposition-1}
Consider the output logits of a confident view $x$, denoted as $\mathbf{s} = (s_1, s_2, \dots, s_L) $, where, without loss of generality, we assume $ s_1 > s_2 > \dots > s_L $. It can be deduced that the entropy loss $ H = H(p(\cdot|x)) $ decreases as $ s_1 $ increases, and $ H $ increases as the sum of the remaining logits, $ S_{\text{rest}} = \sum_{i=2}^{L} s_i $, decreases. Formally, this relationship can be expressed as:
\begin{equation}
 \nabla_{s_1}H = \frac{\partial H}{\partial s_1} < 0 \quad \text{and} \quad \nabla_{s_\text{rest}}H = \frac{\partial H}{\partial S_{\text{rest}}} > 0.
\end{equation}
\end{proposition}
\vspace{-1em}
A detailed proof is provided in the Appendix. Following a single gradient descent update step, we can derive $s_1^{(t+1)} = s_1^{(t)} - \alpha \nabla_{s_1} H$ and $S_\textit{rest}^{(t+1)} = S_\textit{rest}^{(t)} - \alpha \nabla_{S_{\textit{rest}}} H$, where $\alpha$ denotes the learning rate. Therefore, Proposition~\ref{proposition-1} indicates that the nature of entropy loss is to increase the probability of the most confident class while diminishing the cumulative probability of the rest classes. Hence, when adapting to single-label test instances, the goal of vanilla entropy minimization is to solely maximize the probability of the \textit{top-1} predicted label, disregarding changes in the probabilities of the remaining labels.

In contrast, in the context of multi-label test-time adaptation, where the test instance may include a set of positive labels $L_p=\{l_{p1}, l_{p2}, ..., l_{pk}\}$. In this case, regardless of whether the \textit{top-1} predicted label is the element of the positive label set $L_p$, the entropy loss will inevitably decrease the prediction probabilities of the other positive labels within $L_p$ while increasing the probability of the most confident class. This may lead to the model overemphasizing the \textit{top-1} predicted label and inadequately adapting to the other positive labels. Therefore, for test-time adaptation in multi-label data, we propose the following proposition, termed Bound Entropy Minimization.

\begin{proposition} \label{proposition-2}
Consider the output logits of a confident view $x$, denoted as $\mathbf{s} = (s_1, s_2, \dots, s_L) $, where, without loss of generality, we assume $s_1 > s_2 > \dots > s_L$. We define the modified logits as $\mathbf{s}' = (s_1', s_2', \ldots, s_L')$, where $s_i' = a_i + s_i$ for $i \leq k$ with $ a_i = s_1 - s_i$ and $s_i' = s_i$ for $i > k$. Here, $a_i$ is a constant value that does not participate in differentiation, resulting in $s_i' = s_1$ for all $i \leq k$. Let $ S_{\text{rest}} = \sum_{i=k+1}^L s_i$. For the modified logits $\mathbf{s}'$, we define the modified probability $\mathbf{p}' = \text{Softmax}(\mathbf{s}')$, and the modified entropy as $H' = -\sum_{i=1}^L p_i' \log p_i'$. It follows that:
\begin{equation}
 \nabla_{s_i}H' = \frac{\partial H'}{\partial s_i} < 0, \quad \forall i \leq k \quad \text{and} \quad \nabla_{s_\text{rest}}H' = \frac{\partial H'}{\partial S_{\text{rest}}} > 0.
\end{equation}
\end{proposition}
\vspace{-1em}
A detailed proof is provided in Appendix. Likewise, after one step of gradient descent optimization, the prediction probabilities of all \textit{top-k} predicted labels will further increase due to $\nabla_{s_i}H'<0$ for all $i<=k$ and $\nabla_{s_\text{rest}}H'>0$. Therefore, from Proposition~\ref{proposition-2}, to be robust against distribution shifts with multiple labels, it is crucial to determine the number of positive labels for adapting multi-label test instances. In the following subsection, we will introduce a novel Multi-Label Test-Time Adaptation framework by employing proposition~\ref{proposition-2} and incorporating text captions into the adaptation system.

\begin{figure*}[htp]
 \centering
 \includegraphics[scale=0.82]{figures/Overview.pdf}
 \vspace{-1em}
 \caption{Overview of proposed multi-label test-time adaption.}
 \vspace{-1em}
 \label{fig:overview}
\end{figure*}

\subsection{Multi-Label Test-Time Adaptation}\label{ML--TTA}
\subsubsection{View-Caption Constructing}\label{caption allocating}
Benefiting from the aligned space of CLIP, any image can be assigned a most similar caption from an offline text description base based on similarity retrieval. As depicted in Figure~\ref{fig:overview}, given a test image $\mathbf{x}^{\text{test}}$ and a collection of random augmentation functions $\mathcal{A}=\{\mathcal{A}_1, \mathcal{A}_2, ..., \mathcal{A}_N\}$, $\mathbf{x}^{\text{test}}$ is first augmented $N$ times to generate a set of different augmented views. For each augmented view, we retrieve the most similar caption from an offline text description database to serve as its paired caption. The views generating and caption allocating can be expressed as:
\begin{equation}
 X^{\text{test}}=\{ \mathbf{x}_i^{\text{test}}~|~\mathbf{x}_i^{\text{test}}=\mathcal{A}_i(\mathbf{x}^{\text{test}}) \}_{i=1}^{N},~T^{\text{test}}=\{ \mathbf{t}_i^{\text{test}}~|~\mathbf{t}_i^{\text{test}}=\mathcal{R}_i(\mathbf{x}_i^{\text{test}}) \}_{i=1}^N,
\end{equation}
where $\mathcal{A}_i$ and $\mathcal{R}_i$ represents augmentation and retrieval by computing similarity. To streamline the retrieval process, we directly utilize the method proposed in PVP~\citep{TAI-PVP}, which employs LLama-2-7B~\citep{LLaMA-2} to construct the text description base, each text is a description of a natural scene containing several categories. Then, CLIP is used to extract text embeddings and construct an offline database of size $B\times d$, where $B$ denotes the number of test descriptions and $d$ denotes the embedding dimension. More details of the text description base construction are provided in the appendix.

% Additionally, we can pre-compute the text embedding of these text descriptions offline, diminishing the computational cost of caption encoding during the adaptation phase. 

The goal of TTA is to calibrate the model for a single unlabeled test instance. Clearly, a single instance is insufficient for tuning the entire CLIP model to learn domain-specific knowledge. Consequently, as shown in Figure~\ref{fig:overview}, akin to prompt tuning paradigm, we design two identical prompts, referred to as view prompt and caption prompt, denoted by $\mathbf{V}$ and $\mathbf{C}$, respectively. Treating prompt tuning at test-time as a way to furnish customized context for individual test instances. Benefiting from the aligned space of CLIP, the representations of images and texts share similar semantic information, therefore, the paired caption can be considered as a "pseudo image" with accurate textual labels, encouraging the model to learn visual-related knowledge and complementary information from views and captions jointly. For $L$ categories, we initialize the view and caption prompts with template ``\textit{a photo of a} $\textbf{[CLS]}_j$'', in which $\textbf{[CLS]}_j$ represents the $j$-th label name, \textit{e.g.}, dog or cat, results in $\mathbf{v}_j$ and $\mathbf{c}_j$. Once the paired views and captions are obtained, we compute the logits for each view $\mathbf{x}_i^{\text{test}}$ on $L$ view prompts and for each caption $\mathbf{t}_i^{\text{test}}$ on $L$ caption prompts as below:
\begin{equation}
 s_{ij}^{\mathbf{x}^{\text{test}}}=\langle \mathrm{Enc^I}(\mathbf{x}_i^{\text{test}} ), \mathrm{Enc^T}(\mathbf{v}_j ) \rangle, s_{ij}^{\mathbf{t}^{\text{test}}}=\langle \mathrm{Enc^T}(\mathbf{t}_i^{\text{test}} ), \mathrm{Enc^T}(\mathbf{c}_j ) \rangle,
\end{equation}
where $\mathrm{Enc^I}$ and $\mathrm{Enc^T}$ represent the frozen image encoder and text encoder of CLIP, $\langle \cdot,\cdot \rangle$ signifies the dot product. As stated in proposition~\ref{proposition-2}, the crux of adapting multi-label instance lies in identifying the size of positive label set for each view $\mathbf{x}_i^{\text{test}}$ and caption $\mathbf{t}_i^{\text{test}}$. 

% \begin{algorithm}
%     \KwIn{x,y}
%     \KwOut{z}
    
%     \Begin{   
%         \While{Condition is achieved}
%         {
%         \eIf{Condition2}
%         {result1}
%         {result2}
%         {result3}
%         }  
%     }
% \caption{Label Binding}
% \end{algorithm}

\normalem
\begin{algorithm}
\caption{Label Binding Algorithm}
\label{alg:label_binding}
% \SetAlgoLined
\KwIn{Logits $\mathbf{s}_{i}$ before label binding and the size of weak label set $k^{\mathbf{x}_i}$.}
\KwOut{Modified logits $\tilde{\mathbf{s}}_{i}$ after label binding.}
$m_i = \max_j{s_{ij}}$ \;
\For{$j = 1$ \KwTo $L$}{
$a_{ij} = \text{detach}\left( m_i - s_{ij} \right)$ \algorithmiccomment{Detach from gradient.} \; 
    \If{$\mathrm{Rank}_{(s_{ij},\mathbf{s}_{i})} \leq k^{\mathbf{x}_i}$}{
        $\tilde{s}_{ij} = a_{ij} + s_{ij}$ \algorithmiccomment{Bind $s_{ij}$ if $j$-th label is in highest \textit{top}-$k^{\mathbf{x}_i}$ predicted labels.} \;
    }
    \Else{
        $\tilde{s}_{ij} = s_{ij}$ \; 
    }
}
$\tilde{\mathbf{s}}_{i} = \left( \tilde{s}_{i0}, \tilde{s}_{i1}, \cdots, \tilde{s}_{iL} \right)$
\end{algorithm}
\ULforem


\subsubsection{Label Binding} \label{label binding}
Obviously, the positive label set for $\mathbf{x}_i^{\text{test}}$ is not feasible to obtain directly. Fortunately, the textual labels for $\mathbf{t}_i^{\text{test}}$, which we refer to as \textit{strong label set}, can be readily derived through noun filtering, \textit{e.g.}, \textit{A truck drives past a black car with a suitcase on top.} with extracted \textit{strong label set} being \textit{truck, car, suitcase}. Moreover, this set can also serve as a pseudo-positive label set, termed the \textit{weak label set}, for $\mathbf{x}_i^{\text{test}}$. Consequently, we treat the size of \textit{strong label set} as the \textit{top-k} bound highest logits of captions, akin to views. The binding operation for $s_{ij}^{\mathbf{x}^{\text{test}}}$ and $s_{ij}^{\mathbf{t}^{\text{test}}}$ can be expressed as:
\begin{small}
\begin{equation}
\begin{split}
 & \tilde{s}_{ij}^{\mathbf{x}^{\text{test}}}\!=\!((m_i^{\mathbf{x}^{\text{test}}}\!-\!s_{ij}^{\mathbf{x}^{\text{test}}})\!+\!s_{ij}^{\mathbf{x}^{\text{test}}}) \cdot \mathbb{I}(\mathrm{Rank}_{(s_{ij}^{\mathbf{x}^{\text{test}}},\mathbf {s}_{i}^{\mathbf{x}^{\text{test}}})}\!\leq\!k^{\mathbf{x}_i^{\text{test}}})\!+\!s_{ij}^{\mathbf{x}^{\text{test}}} \cdot \mathbb{I}(\mathrm{Rank}_{(s_{ij}^{\mathbf{x}^{\text{test}}},\mathbf {s}_{i}^{\mathbf{x}^{\text{test}}})}\!>\!k^{\mathbf{x}_i^{\text{test}}}), \\
 & \tilde{s}_{ij}^{\mathbf{t}^{\text{test}}}\!=\!((m_i^{\mathbf{t}^{\text{test}}}\!-\!s_{ij}^{\mathbf{t}^{\text{test}}})\!+\!s_{ij}^{\mathbf{t}^{\text{test}}}) \cdot \mathbb{I}(\mathrm{Rank}_{(s_{ij}^{\mathbf{t}^{\text{test}}},\mathbf {s}_{i}^{\mathbf{t}^{\text{test}}})}\!\leq\!k^{\mathbf{t}_i^{\text{test}}})\!+\!s_{ij}^{\mathbf{t}^{\text{test}}} \cdot \mathbb{I}(\mathrm{Rank}_{(s_{ij}^{\mathbf{t}^{\text{test}}},\mathbf {s}_{i}^{\mathbf{t}^{\text{test}}})}\!>\!k^{\mathbf{t}_i^{\text{test}}}),
\end{split}
\end{equation}
\end{small}where $( ( m_i^{\mathbf x^{\text {test}}} - s_{ij}^{\mathbf x^{\text {test}}} ) + s_{ij}^{\mathbf x^{\text {test}}} )$ employs stop-gradient operation follow VQ-VAE~\cite{VQ-VAE}, $\mathbf {s}_{i}^{\mathbf{x}^{\text{test}}}=(s_{i1}^{\mathbf{x}^{\text{test}}}, s_{i2}^{\mathbf{x}^{\text{test}}}, ..., s_{iL}^{\mathbf{x}^{\text{test}}})$ and $\mathbf {s}_{i}^{\mathbf{t}^{\text{test}}}=(s_{i1}^{\mathbf{t}^{\text{test}}}, s_{i2}^{\mathbf{t}^{\text{test}}}, ..., s_{iL}^{\mathbf{t}^{\text{test}}})$ denotes the logits before binding, $m_i^{\mathbf{x}^{\text{test}}}$ and $m_i^{\mathbf{t}^{\text{test}}}$ denotes the maximum logit of $\mathbf {s}_{i}^{\mathbf{x}^{\text{test}}}$ and $\mathbf {s}_{i}^{\mathbf{t}^{\text{test}}}$, respectively, $\mathbb{I}(\cdot)$ denotes the indicator function, and $\mathrm{Rank}(s,\mathbf{s})$ indicates the descending rank of $s$ within $\mathbf{s}$, $k^{\mathbf{x}_i^{\text{test}}}$ and $k^{\mathbf{t}_i^{\text{test}}}$ denotes the size of \textit{weak label set} of $i$-th augmented view and \textit{strong label set} of $i$-th paired caption. The algorithm process of label binding is presented in algorithm~\ref{alg:label_binding}. We provide a detailed label binding process using a $3$-class classification task in the Appendix.

To reduce the noise brought by random augmentation and the noise in the caption caused by noisy views, we employ \textit{confidence selection} to filter out noisy views and captions with higher entropy (\textit{i.e.}, lower confidence). Such noisy views may, due to random cropping augmentation, exclude the target label area, leaving only irrelevant background information. Similarly, the retrieved paired captions for these noisy views will lack any pertinent textual labels. We selected views and captions with lower predicted entropy by a ratio $\tau$, yielding $\{\check{\mathbf{x}}_i^{\text{test}}\}_{i=1}^{\tau N}$ for views and $\{\check{\mathbf{t}}_i^{\text{test}}\}_{i=1}^{\tau N}$ for captions. Taking views $\check{\mathbf{x}}_i^{\text{test}}$ as an example, the probability of $\check{\mathbf{x}}_i^{\text{test}}$ on $L$ labels denoted as $\mathbf{p} = \text{Softmax}(\tilde{\mathbf{s}}_i^{\check{\mathbf{x}}_i^{\text{test}}})$, the average predicted entropy of the filtered low-entropy views can be expressed as:
\begin{equation}\label{entropy}
 \tilde{H}_{\text{avg}}^{\check{\mathbf{x}}^{\text{test}}} = \frac{1}{\tau N} \sum_{i=1}^{\tau N} \left( -\sum_{l=1}^{L} p(y = l|\check{\mathbf{x}}_i^{\text{test}}) \log(p(y = l|\check{\mathbf{x}}_i^{\text{test}}))\right).
\end{equation}
Subsequently, the bound entropy optimization objective of view prompt $\mathbf{V}$ is to minimize the predicted entropy through $\tilde{H}_{\text{avg}}^{\check{\mathbf{x}}^{\text{test}}}$. For the objective of caption prompt $\mathbf{C}$, we replace $\check{\mathbf{x}}_i^{\text{test}}$ in Eq.(\ref{entropy}) with confident captions $\check{\mathbf{t}}_i^{\text{test}}$ to obtain $\tilde{H}_{\text{avg}}^{\check{\mathbf{t}}^{\text{test}}}$.

\subsubsection{Overall Objective of ML--TTA}
ML--TTA calculates the predicted bound entropy of confident augmented views and paired captions, optimizing both view prompt and caption prompt with a single step of gradient descent, and simultaneously increasing the probability of highest \textit{top} predicted labels. Then, the overall bound entropy loss is given by:
\begin{equation}
 \tilde{H}_{\text{BEM}} = \tilde{H}_{\text{avg}}^{\check{\mathbf{x}}^{\text{test}}} + \tilde{H}_{\text{avg}}^{\check{\mathbf{t}}^{\text{test}}}.
\end{equation}
After optimizing the prompts, ML--TTA immediately infers the test instance $\mathbf{x}^{\text{test}}$ and resets the parameters of the prompts~($\mathbf{V}$ and $\mathbf{C}$) and the state of optimizer to adapt to the next test instance. During the inference phase, we separately compute the similarity between the view prompt $\mathbf{V}$ and the test instance $\mathbf{x}^{\text{test}}$, as well as the similarity between the caption prompt $\mathbf{C}$ and the test instance $\mathbf{x}^{\text{test}}$, and directly add these two similarities to obtain the final prediction result.

\section{Experiment}
\subsection{Experimental Setup}
\textbf{Benchmarks.} We utilize the widely employed CLIP~\citep{VLMs-Openai-CLIP} model as source model and select the multi-label datasets VOC~\citep{DATASET-VOC2007}, MSCOCO~\citep{DATASET-COCO}, and NUSWIDE~\citep{DATASET-NUSWIDE} as target domains. The VOC dataset includes 20 categories, covering both VOC2007 and VOC2012 versions, which contain 4,952 and 5,823 test images, respectively. The MSCOCO dataset extends the category range to 80, and for testing purposes, we employ the validation sets of COCO2014 with 40,504 images and COCO2017 with 5,000 images, as the test set labels are not accessible. The NUSWIDE dataset includes 81 categories with a total of 83,898 test images of lower resolution, which presents a broader category spectrum than MSCOCO.

\textbf{Implementation details.}
All experiments are based on the CLIP model, encompassing RN50, RN101, ViT-B/32, and ViT-B/16 architectures, each consisting of an image encoder and a corresponding text encoder. For the initialization of the view and caption prompts, we employ the token embedding of the ``a photo of a'' hard prompt as initialization weights and another using learned prompts from CoOp~\citep{Prompt-CoOp} and MaPLE~\citep{Prompt-MAPLE}. The learning rate for the view prompt is 1e-2, while for the caption prompt is 1e-3. For all settings, multi-label test-time adaptation is performed on a single instance, \textit{i.e.}, the batch size is 1. The ratio for filtering confident views and captions is 0.1. The optimizer is AdamW~\citep{AdamW} with a single update step, followed by immediate inference on the test instance. Following PVP~\citep{TAI-PVP}, we collect 100k text descriptions for each dataset, resulting in a total size of 300k text description base. All experiments are evaluated by the mean Average Precision~(mAP) metric, defined as $mAP=\frac{1}{L}\sum_{i=1}^L AP_i$, where $L$ is the number of categories, and $AP_i$ is the area under the Precision-Recall curve for the $i$-th category.

\subsection{Comparisons with State-of-the-art}
\begin{table*}[ht]
\vspace{-0.4em}
\captionsetup{font={footnotesize}}
\caption{Comparison with CLIP and SOTAs on adapting multi-label instances with different architectures.}
\vspace{-0.8em}
\centering
\renewcommand{\arraystretch}{1} % Adjusts the row height
\setlength{\tabcolsep}{1pt} % Adjusts the column width
\begin{tabular}{m{0.5cm}|l|c|ccccc|c}
\toprule[1.5pt]
& {\footnotesize \textbf{Methods}} & \textit{\footnotesize \textbf{Epsdoic}} & {\footnotesize \textbf{COCO2014}} & {\footnotesize \textbf{COCO2017}} & {\footnotesize \textbf{VOC2007}} & {\footnotesize {\textbf{VOC2012}}} & {\footnotesize \textbf{NUSWIDE}} & {\footnotesize \textbf{Average}} \\ \midrule[0.7pt]
 \multirow{9}{*}[1ex]{\rotatebox{90}{\normalsize \textbf{RN-50}}} & {\small CLIP}${}_{\textcolor{red}{\rm ~[\text{ICML 2022}]}}$ & \checkmark & {\small 47.53} & {\small 47.32} & {\small 75.91} & {\small 74.25} & {\small 41.53} & {\small 57.31} \\ \cmidrule(r){2-9}
& {\small DMN}${}_{\textcolor{red}{\rm ~[\text{CVPR 2024}]}}$ & $\times$ & {\small 44.54} & {\small 44.18} & {\small 74.87} & {\small 74.13} & {\small 41.32} & {\small 55.81} \\
% & {\small DART}${}_{\textcolor{red}{\rm ~[\text{AAAI 2024}]}}$ & $\times$ & {\small /} & {\small /} & {\small /} & {\small /} & {\small /} & {\small /} \\
& {\small TDA}${}_{\textcolor{red}{\rm ~[\text{CVPR 2024}]}}$ & $\times$ & {\small \underline{48.91}} & {\small \underline{49.11}} & {\small \underline{76.64}} & {\small \underline{75.12}} & {\small \underline{42.34}} & {\small \underline{58.42}} \\ \cmidrule(r){2-9}
& {\small TPT}${}_{\textcolor{red}{\rm ~[\text{NeurIPS 2022}]}}$ & \checkmark & {\small 48.52} & {\small 48.51} & {\small 75.54} & {\small 73.92} & {\small 41.97} & {\small 57.69} \\
& {\small DIffTPT}${}_{\textcolor{red}{\rm ~[\text{ICCV 2023}]}}$ & \checkmark & {\small 48.56} & {\small 48.67} & {\small 75.89} & {\small 74.13} & {\small 41.33} & {\small 57.72} \\
& {\small RLCF}${}_{\textcolor{red}{\rm ~[\text{ICLR 2024}]}}$ & \checkmark & {\small 36.87} & {\small 36.73} & {\small 65.75} & {\small 64.73} & {\small 29.83} & {\small 46.78} \\
% & \cellcolor{gray!10}{\small \textbf{ML--TTA}} & \cellcolor{gray!10}\checkmark & \cellcolor{gray!10}{\small 51.31} & \cellcolor{gray!10}{\small 51.19} & \cellcolor{gray!10}{\small 77.92} & \cellcolor{gray!10}{\small 76.31} & \cellcolor{gray!10}{\small 42.23} & \cellcolor{gray!10}{\small xxx} \\
& \cellcolor{gray!25}{\small \textbf{ML--TTA}~(Ours)} & \cellcolor{gray!25}\checkmark & \cellcolor{gray!25}{\small \textbf{51.58}} & \cellcolor{gray!25}{\small \textbf{51.39}} & \cellcolor{gray!25}{\small \textbf{78.62}} & \cellcolor{gray!25}{\small \textbf{76.63}} & \cellcolor{gray!25}{\small \textbf{42.53}} & \cellcolor{gray!25}{\small \textbf{60.15}} \\
\midrule[0.7pt]
\midrule[0.7pt]
 \multirow{9}{*}[1ex]{\rotatebox{90}{\normalsize \textbf{RN-101}}} & {\small CLIP}${}_{\textcolor{red}{\rm ~[\text{ICML 2022}]}}$ & \checkmark & {\small 48.83} & {\small 48.15} & {\small 76.72} & {\small 74.21} & {\small 41.93} & {\small 57.97} \\ \cmidrule(r){2-9}
& {\small DMN}${}_{\textcolor{red}{\rm ~[\text{CVPR 2024}]}}$ & $\times$ & {\small 46.28} & {\small 45.44} & {\small 76.82} & {\small 75.32} & {\small 42.71} & {\small 57.31} \\
% & {\small DART}${}_{\textcolor{red}{\rm ~[\text{AAAI 2024}]}}$ & $\times$ & {\small /} & {\small /} & {\small /} & {\small /} & {\small /} & {\small /} \\
& {\small TDA}${}_{\textcolor{red}{\rm ~[\text{CVPR 2024}]}}$ & $\times$ & {\small \underline{50.19}} & {\small \underline{49.78}} & {\small \underline{78.12}} & {\small \underline{77.13}} & {\small \underline{43.13}} & {\small \underline{59.67}} \\ \cmidrule(r){2-9}
& {\small TPT}${}_{\textcolor{red}{\rm ~[\text{NeurIPS 2022}]}}$ & \checkmark & {\small 49.71} & {\small 48.89} & {\small 74.82} & {\small 73.39} & {\small 43.10} & {\small 57.98} \\
& {\small DIffTPT}${}_{\textcolor{red}{\rm ~[\text{ICCV 2023}]}}$ & \checkmark & {\small 49.45} & {\small 49.19} & {\small 74.98} & {\small 74.31} & {\small 42.93} & {\small 58.17} \\
& {\small RLCF}${}_{\textcolor{red}{\rm ~[\text{ICLR 2024}]}}$ & \checkmark & {\small 40.53} & {\small 39.79} & {\small 71.21} & {\small 69.63} & {\small 31.77} & {\small 50.59} \\
% & \cellcolor{gray!10}{\small \textbf{ML--TTA}} & \cellcolor{gray!10}\checkmark & \cellcolor{gray!10}{\small 52.33} & \cellcolor{gray!10}{\small 51.69} & \cellcolor{gray!10}{\small 78.44} & \cellcolor{gray!10}{\small 77.73} & \cellcolor{gray!10}{\small 43.35} & \cellcolor{gray!10}{\small xxx} \\
& \cellcolor{gray!25}{\small \textbf{ML--TTA}~(Ours)} & \cellcolor{gray!25}\checkmark & \cellcolor{gray!25}{\small \textbf{52.92}} & \cellcolor{gray!25}{\small \textbf{52.24}} & \cellcolor{gray!25}{\small \textbf{78.72}} & \cellcolor{gray!25}{\small \textbf{78.13}} & \cellcolor{gray!25}{\small \textbf{43.62}} & \cellcolor{gray!25}{\small \textbf{61.13}} \\
\midrule[0.7pt]
\midrule[0.7pt]
 \multirow{9}{*}[1ex]{\rotatebox{90}{\normalsize \textbf{ViT-B/32}}} & {\small CLIP}${}_{\textcolor{red}{\rm ~[\text{ICML 2022}]}}$ & \checkmark & {\small 50.31} & {\small 50.15} & {\small 77.18} & {\small 76.85} & {\small 42.90} & {\small 59.48} \\ \cmidrule(r){2-9}
& {\small DMN}${}_{\textcolor{red}{\rm ~[\text{CVPR 2024}]}}$ & $\times$ & {\small 49.32} & {\small 48.13} & {\small 77.42} & {\small 76.60} & {\small 43.41} & {\small 58.98} \\
% & {\small DART}${}_{\textcolor{red}{\rm ~[\text{AAAI 2024}]}}$ & $\times$ & {\small /} & {\small /} & {\small /} & {\small /} & {\small /} & {\small /} \\
& {\small TDA}${}_{\textcolor{red}{\rm ~[\text{CVPR 2024}]}}$ & $\times$ & {\small \underline{51.23}} & {\small \underline{51.49}} & {\small \underline{77.62}} & {\small \underline{77.12}} & {\small \textbf{44.13}} & {\small \underline{60.32}} \\ \cmidrule(r){2-9}
& {\small TPT}${}_{\textcolor{red}{\rm ~[\text{NeurIPS 2022}]}}$ & \checkmark & {\small 48.12} & {\small 48.63} & {\small 74.21} & {\small 71.93} & {\small 43.63} & {\small 57.30} \\
& {\small DIffTPT}${}_{\textcolor{red}{\rm ~[\text{ICCV 2023}]}}$ & \checkmark & {\small 48.73} & {\small 49.19} & {\small 74.50} & {\small 72.98} & {\small 43.42} & {\small 57.76} \\
& {\small RLCF}${}_{\textcolor{red}{\rm ~[\text{ICLR 2024}]}}$ & \checkmark & {\small 50.28} & {\small 49.59} & {\small 77.12} & {\small 76.83} & {\small 43.29} & {\small 59.42} \\
% & \cellcolor{gray!10}{\small \textbf{ML--TTA}} & \cellcolor{gray!10}\checkmark & \cellcolor{gray!10}{\small 52.91} & \cellcolor{gray!10}{\small 53.13} & \cellcolor{gray!10}{\small 78.59} & \cellcolor{gray!10}{\small 77.55} & \cellcolor{gray!10}{\small 43.91} & \cellcolor{gray!10}{\small xxx} \\
& \cellcolor{gray!25}{\small \textbf{ML--TTA}~(Ours)} & \cellcolor{gray!25}\checkmark & \cellcolor{gray!25}{\small \textbf{52.83}} & \cellcolor{gray!25}{\small \textbf{52.99}} & \cellcolor{gray!25}{\small \textbf{78.70}} & \cellcolor{gray!25}{\small \textbf{77.97}} & \cellcolor{gray!25}{\small \underline{44.12}} & \cellcolor{gray!25}{\small \textbf{61.32}} \\
\midrule[0.7pt]
\midrule[0.7pt]
 \multirow{9}{*}[-1ex]{\rotatebox{90}{\normalsize \textbf{ViT-B/16}}} & {\small CLIP}${}_{\textcolor{red}{\rm ~[\text{ICML 2022}]}}$ & \checkmark & {\small 54.42} & {\small 54.13} & {\small 79.58} & {\small 79.25} & {\small 45.65} & {\small 62.61} \\ \cmidrule(r){2-9}
& {\small DMN}${}_{\textcolor{red}{\rm ~[\text{CVPR 2024}]}}$ & $\times$ & {\small 52.52} & {\small 52.37} & {\small 79.83} & {\small 79.67} & {\small 46.27} & {\small 62.13} \\
& {\small DART}${}_{\textcolor{red}{\rm ~[\text{AAAI 2024}]}}$ & $\times$ & {\small 54.73} & {\small 54.68} & {\small 79.91} & {\small 78.56} & {\small 45.91} & {\small 62.76} \\
& {\small TDA}${}_{\textcolor{red}{\rm ~[\text{CVPR 2024}]}}$ & $\times$ & {\small \underline{55.21}} & {\small \underline{55.46}} & {\small \underline{80.12}} & {\small \underline{79.92}} & {\small \textbf{46.72}} & {\small \underline{63.49}} \\ \cmidrule(r){2-9}
& {\small TPT}${}_{\textcolor{red}{\rm ~[\text{NeurIPS 2022}]}}$ & \checkmark & {\small 53.32} & {\small 54.20} & {\small 77.54} & {\small 77.39} & {\small 46.15} & {\small 61.72} \\
& {\small DIffTPT}${}_{\textcolor{red}{\rm ~[\text{ICCV 2023}]}}$ & \checkmark & {\small 53.91} & {\small 54.15} & {\small 77.93} & {\small 77.24} & {\small 46.13} & {\small 61.87} \\
& {\small RLCF}${}_{\textcolor{red}{\rm ~[\text{ICLR 2024}]}}$ & \checkmark & {\small 54.21} & {\small 54.43} & {\small 79.29} & {\small 79.26} & {\small 43.18} & {\small 62.07} \\
% & \cellcolor{gray!10}{\small \textbf{ML--TTA}} & \cellcolor{gray!10}\checkmark & \cellcolor{gray!10}{\small 57.09} & \cellcolor{gray!10}{\small 57.37} & \cellcolor{gray!10}{\small 80.82} & \cellcolor{gray!10}{\small 80.74} & \cellcolor{gray!10}{\small 46.32} & \cellcolor{gray!10}{\small xxx} \\
& \cellcolor{gray!25}{\small \textbf{ML--TTA}~(Ours)} & \cellcolor{gray!25}\checkmark & \cellcolor{gray!25}{\small \textbf{57.52}} & \cellcolor{gray!25}{\small \textbf{57.49}} & \cellcolor{gray!25}{\small \textbf{81.28}} & \cellcolor{gray!25}{\small \textbf{81.13}} & \cellcolor{gray!25}{\small \underline{46.55}} & \cellcolor{gray!25}{\small \textbf{64.80}} \\
\bottomrule[1.5pt]
\end{tabular}
\label{tab:different arch}
\vspace{-0.5em}
\end{table*}

To our knowledge, our work is the first to investigate the feasibility of traditional entropy minimization in the multi-label setting. Therefore, in this section, we select the original CLIP model and other SOTA methods for multi-class scenarios as baselines, including \textit{episdoic} methods that do not require retaining historical knowledge (TPT~\cite{TTA-TPT}, DiffTPT~\cite{TTA-DiffTPT}, RCLF~\cite{TTA-RLCF}) and \textit{online} methods that do (DMN~\cite{TTA-DMN}, TDA~\cite{TTA-TDA}).

% In this section, we evaluate the proposed ML--TTA framework with the original CLIP model and other state-of-the-art TTA methods, including \textit{online} methods, where the model can constantly store and utilize knowledge from historically adapted samples, \textit{e.g.}, DMN~\citep{TTA-DMN}, TDA~\citep{TTA-TDA} and DART~\citep{TTA-DART}~(designed for ViT-B/16 architecture), and \textit{episdoic} methods, where the model does not require storing any historical information and are suitable for scenarios where test instances are independently distributed, \textit{e.g.}, TPT~\citep{TTA-TPT}, DiffTPT~\citep{TTA-DiffTPT} and RLCF~\citep{TTA-RLCF}. 

\textbf{Results on different architectures.} Table~\ref{tab:different arch} compares ML--TTA with both \textit{online} and \textit{episdoic} TTA methods on different CLIP~\citep{VLMs-Openai-CLIP} architectures, demonstrating the superior performance across various multi-label datasets. Specifically, for the RN50 and RN101 architectures on COCO2014/2017~\citep{DATASET-COCO} datasets, ML--TTA achieves $4 \!\! \sim \!\! 5$\% improvement in mAP over the original CLIP~\citep{VLMs-Openai-CLIP} model, whereas TPT~\citep{TTA-TPT} and DiffTPT~\citep{TTA-DiffTPT} yield only $1$\% enhancement. Despite introducing dual-memory network knowledge from historical samples, DMN~\citep{TTA-DMN} and TDA~\cite{TTA-TDA} present a slight performance decline, due to intensifying the optimization bias towards \textit{top-1} label. Notably, RLCF~\citep{TTA-RLCF} employs a reinforcement learning-based knowledge distillation and more adaptation steps, resulting in a catastrophic degradation in the multi-label adaptation performance for smaller models due to excessive optimizations of \textit{top-1} label. On the VOC2012/2017~\citep{DATASET-VOC2007} datasets, TPT and DiffTPT also show $1 \!\! \sim \!\! 2$\% decrease in performance compared to CLIP, whereas ML--TTA still maintains $2 \!\! \sim \!\! 3$\% performance improvement, indicating the robustness of ML--TTA in multi-label adaptation across various model architectures and datasets.

For the vision transformer series architectures, compared to CLIP~\citep{VLMs-Openai-CLIP}, ML--TTA consistently achieves $2 \!\! \sim \!\! 4$\% mAP improvement on the COCO2014/2017~\citep{DATASET-COCO} and VOC2007/2012~\citep{DATASET-VOC2007} datasets. However, most TTA methods, except TDA~\citep{TTA-TDA} and DART~\citep{TTA-DART}, exhibit a slight performance decrement, particularly among episodic methods. Additionally, we observed an intriguing observation: all TTA methods, excluding RLCF~\citep{TTA-RLCF}, fail to substantially enhance the mAP performance of CLIP~\citep{VLMs-Openai-CLIP} on the NUSWIDE~\citep{DATASET-NUSWIDE} dataset, with an improvement of merely about $1$\%. This may be attributed to the low image resolution of NUSWIDE dataset, where random data augmentation struggles to preserve sufficient visual information. Consequently, adapting to multi-labels for small targets may become a research topic in the future.

\begin{table*}[ht]
\vspace{-0.5em}
\captionsetup{font={footnotesize}}
\caption{Comparison with SOTAs on adapting multi-label instances with different prompt initialization.}
\vspace{-0.5em}
\centering
\renewcommand{\arraystretch}{1} % Adjusts the row height
\setlength{\tabcolsep}{1pt} % Adjusts the column width
\begin{tabular}{m{0.5cm}|l|c|ccccc|c}
\toprule[1.5pt]
& {\footnotesize \textbf{Methods}} & \textit{\footnotesize \textbf{Epsdoic}} & {\footnotesize \textbf{COCO2014}} & {\footnotesize \textbf{COCO2017}} & {\footnotesize \textbf{VOC2007}} & {\footnotesize {\textbf{VOC2012}}} & {\footnotesize \textbf{NUSWIDE}} & {\footnotesize \textbf{Average}} \\ \midrule[0.7pt]
 \multirow{9}{*}[3ex]{\rotatebox{90}{\normalsize \textbf{CoOp}}} & {\small CoOp}${}_{\textcolor{red}{\rm ~[\text{IJCV2022}]}}$ & \checkmark & {\small 56.12} & {\small 56.35} & {\small 79.14} & {\small 77.85} & {\small 46.74} & {\small 63.24} \\ \cmidrule(r){2-9}
& {\small TDA}${}_{\textcolor{red}{\rm ~[\text{CVPR 2024}]}}$ & $\times$ & {\small \underline{56.93}} & {\small \underline{57.15}} & {\small \underline{80.20}} & {\small \underline{78.58}} & {\small \underline{47.82}} & {\small \underline{64.13}} \\ \cmidrule(r){2-9}
& {\small TPT}${}_{\textcolor{red}{\rm ~[\text{NeurIPS 2022}]}}$ & \checkmark & {\small 55.35} & {\small 55.23} & {\small 79.72} & {\small 77.85} & {\small 47.27} & {\small 63.08} \\
& {\small DIffTPT}${}_{\textcolor{red}{\rm ~[\text{ICCV 2023}]}}$ & \checkmark & {\small 55.30} & {\small 55.47} & {\small 79.86} & {\small 77.61} & {\small 47.13} & {\small 63.07} \\
& {\small RLCF}${}_{\textcolor{red}{\rm ~[\text{ICLR 2024}]}}$ & \checkmark & {\small 56.72} & {\small 56.18} & {\small 80.15} & {\small 78.24} & {\small 47.62} & {\small 63.78} \\
& \cellcolor{gray!25}{\small \textbf{ML--TTA}~(Ours)} & \cellcolor{gray!25}\checkmark & \cellcolor{gray!25}{\small \textbf{59.68}} & \cellcolor{gray!25}{\small \textbf{59.33}} & \cellcolor{gray!25}{\small \textbf{83.17}} & \cellcolor{gray!25}{\small \textbf{81.36}} & \cellcolor{gray!25}{\small \textbf{48.12}} & \cellcolor{gray!25}{\small \textbf{66.33}} \\
\midrule[0.7pt]
\midrule[0.7pt]
 \multirow{9}{*}[3ex]{\rotatebox{90}{\normalsize \textbf{Maple}}} & {\small Maple}${}_{\textcolor{red}{\rm ~[\text{CVPR2023}]}}$ & \checkmark & {\small 62.18} & {\small 62.35} & {\small 85.34} & {\small 84.79} & {\small 48.42} & {\small 68.62} \\ \cmidrule(r){2-9}
& {\small TDA}${}_{\textcolor{red}{\rm ~[\text{CVPR 2024}]}}$ & $\times$ & {\small 63.25} & {\small 63.64} & {\small \underline{85.76}} & {\small 84.15} & {\small \underline{49.55}} & {\small \underline{69.27}} \\ \cmidrule(r){2-9}
& {\small TPT}${}_{\textcolor{red}{\rm ~[\text{NeurIPS 2022}]}}$ & \checkmark & {\small \underline{63.36}} & {\small \underline{63.75}} & {\small 85.04} & {\small 83.92} & {\small 48.90} & {\small 69.01} \\
& {\small DIffTPT}${}_{\textcolor{red}{\rm ~[\text{ICCV 2023}]}}$ & \checkmark & {\small 62.93} & {\small 63.14} & {\small 85.15} & {\small 83.78} & {\small 48.81} & {\small 68.76} \\
& {\small RLCF}${}_{\textcolor{red}{\rm ~[\text{ICLR 2024}]}}$ & \checkmark & {\small 62.84} & {\small 62.90} & {\small 85.35} & {\small \underline{85.28}} & {\small 49.37} & {\small 69.15} \\
& \cellcolor{gray!25}{\small \textbf{ML--TTA}~(Ours)} & \cellcolor{gray!25}\checkmark & \cellcolor{gray!25}{\small \textbf{64.75}} & \cellcolor{gray!25}{\small \textbf{64.86}} & \cellcolor{gray!25}{\small \textbf{86.40}} & \cellcolor{gray!25}{\small \textbf{85.69}} & \cellcolor{gray!25}{\small \textbf{50.21}} & \cellcolor{gray!25}{\small \textbf{70.38}} \\
\bottomrule[1.5pt]
\end{tabular}
\label{tab:prompt weight}
\vspace{-0.5em}
\end{table*}

\textbf{Results on different prompt initialization.} For this comparison, we adopt the learned prompt from CoOp~\citep{Prompt-CoOp} and Maple~\citep{Prompt-MAPLE} to initialize the prompt weights, replacing the template ``a photo of a \textbf{[CLS]}'' employed in the original CLIP model. As shown in Table~\ref{tab:prompt weight}, the application of both CoOp and Maple prompt weights in our ML–TTA framework results in a significant enhancement of over $4$\% in mAP on the COCO2014/2017 datasets. For instance, the mAP increases from $47.53$\% to $51.58$\% and from $47.32$\% to $51.39$\% on COCO2014/2017 with CoOp prompt initialization, whereas other \textit{episdoic} methods, TPT~\citep{TTA-TPT} and DiffTPT~\citep{TTA-DiffTPT}, yield improvements of no more than $1.5$\%. Moreover, ML--TTA also surpasses TDA~\citep{TTA-TDA}, which is designed by dynamically employing the historical sample knowledge, on both CoOp and Maple prompt initialization across all datasets.

\begin{wraptable}{r}{7.2cm}
\vspace{-1.2em}
\centering
\captionsetup{font={footnotesize}}
\caption{Results on different label counts.}
\vspace{-0.8em}
\renewcommand{\arraystretch}{1} % Adjusts the row height
\setlength{\tabcolsep}{3pt} % Adjusts the column width
\begin{tabular}{l|c|c|c|c}
\toprule[1pt]
{\footnotesize \textbf{Methods}} & {\footnotesize \textbf{\{1,2\}}} & {\footnotesize \textbf{\{3,4\}}} & {\footnotesize \textbf{\{5,6,7\}}} & {\footnotesize \textbf{\{$>=$8\}}} \\ \midrule[0.4pt] 
{\small CLIP}${}_{\textcolor{red}{\rm ~[\text{ICML 2022}]}}$ & {\small 62.76} & {\small \underline{55.41}} & {\small \underline{49.89}} & {\small \underline{41.07}} \\
{\small TPT}${}_{\textcolor{red}{\rm ~[\text{NeurIPS 2022}]}}$ & {\small 62.88} & {\small 53.05} & {\small 45.57} & {\small 37.43} \\
{\small DiffTPT}${}_{\textcolor{red}{\rm ~[\text{ICCV 2023}]}}$ & {\small 61.97} & {\small 52.67} & {\small 44.32} & {\small 36.89} \\
{\small RLCF}${}_{\textcolor{red}{\rm ~[\text{ICLR 2024}]}}$ & {\small \underline{66.01}} & {\small 51.65} & {\small 43.32} & {\small 35.08} \\
\cellcolor{gray!25}{\small \textbf{ML--TTA}~(Ours)} & \cellcolor{gray!25}{\small \textbf{67.14}} & \cellcolor{gray!25}{\small \textbf{57.59}} & \cellcolor{gray!25}{\small \textbf{51.68}} & \cellcolor{gray!25}{\small \textbf{41.32}} \\
\bottomrule[1pt]
\end{tabular}
\label{tab:label counts}
\vspace{-1.5em}
\end{wraptable}
\textbf{Results on different label counts.}
Apart from the analysis of architecture and prompt initialization weights, we explore a more challenging scenario in Table~\ref{tab:label counts}, where the COCO2014 dataset is divided into subsets with incrementally increasing numbers of image labels per part, \textit{e.g.}, \{1,2\} represents the number of labels $L$ per image is either 1 or 2. When $L\in \{1,2\}$, TPT achieves only a negligible improvement compared to CLIP and shows large considerable performance degradation in other situations as well as DiffTPT. RLCF improves significantly when $L\in \{1,2\}$, but its performance sharply declines as $L$ increases. In contrast, our ML--TTA framework outperforms CLIP across all situations, demonstrating that ML--TTA not only can address the distribution shifts during testing but also effectively handle varying numbers of labels in testing instances.

\begin{wraptable}{r}{6.5cm}
\vspace{-1.2em}
\centering
\captionsetup{font={footnotesize}}
\caption{Results on adaptation complexity.}
\vspace{-0.8em}
\renewcommand{\arraystretch}{0.8} % Adjusts the row height
\setlength{\tabcolsep}{2pt} % Adjusts the column width
\begin{tabular}{l|c|c|c|c}
\toprule[1pt]
{\footnotesize \textbf{Methods}} & {\small TPT} & {\small DiffTPT} & {\small RLCF} & {\small ML-TTA} \\ \midrule[0.4pt] 
{\small Adapting Time} & {\small \textbf{0.21}s} & {\small 0.41s} & {\small 0.45s} & {\small \underline{0.24}s} \\
{\small mAP} & {\small 48.52} & {\small \underline{48.56}} & {\small 36.87} & {\small \textbf{51.58}} \\
\bottomrule[1pt]
\end{tabular}
\label{tab:adaptation complexity}
\vspace{-1.5em}
\end{wraptable}
\textbf{Results on adaptation complexity.}
Furthermore, we analyze adapting time per test instance with methods that also do not require retaining historical knowledge. Table~\ref{tab:adaptation complexity} shows that ML-TTA presents a significant advantage compared to DiffTPT, which involves generating multiple pseudo-images via a diffusion model, and RLCF, which requires distillation from a teacher model along with more gradient update steps. Compared to the benchmark TPT, ML-TTA increases adapting time due to simultaneous optimizing view and caption prompts.

\subsection{Abltion Studies.}
\begin{wraptable}{r}{7.5cm}
\vspace{-4em}
\centering
\captionsetup{font={footnotesize}}
\caption{Ablation studies of different components.}
\vspace{-0.8em}
\renewcommand{\arraystretch}{0.95} % Adjusts the row height
\setlength{\tabcolsep}{2.3pt} % Adjusts the column width
\begin{tabular}{l|c|c|c|c}
\toprule[1pt]
\multirow{2}{*}{\footnotesize \textbf{Methods}} & \multicolumn{2}{c|}{\footnotesize \textbf{RN50}} & \multicolumn{2}{c}{\footnotesize \textbf{ViT-B/16}} \\ \cline{2-5}
 & {\scriptsize \textbf{COCO2014}} & {\scriptsize \textbf{VOC2007}} & {\scriptsize \textbf{COCO2014}} & {\scriptsize \textbf{VOC2007}} \\ \midrule[0.4pt] 
{\footnotesize VP~(\textit{i.e.}, TPT)} & {\small 48.51} & {\small 75.52} & {\small 53.32} & {\small 77.57} \\
{\footnotesize VP+BEM} & {\small 48.96} & {\small 76.31} & {\small 53.58} & {\small 77.89} \\ \midrule[0.4pt]
{\footnotesize CP} & {\small 49.12} & {\small 76.16} & {\small 55.14} & {\small 78.93} \\
{\footnotesize CP+BEM} & {\small 49.54} & {\small 76.75} & {\small 55.64} & {\small 79.58} \\ \midrule[0.4pt]
{\footnotesize VP+CP} & {\small 51.22} & {\small 77.98} & {\small 57.14} & {\small 80.85} \\
\cellcolor{gray!25}{\footnotesize \textbf{VP+CP+BEM}} & \cellcolor{gray!25}{\small \textbf{51.58}} & \cellcolor{gray!25}{\small \textbf{78.62}} & \cellcolor{gray!25}{\small \textbf{57.52}} & \cellcolor{gray!25}{\small \textbf{81.28}} \\
\bottomrule[1pt]
\end{tabular}
\label{tab:Components}
\vspace{-1.2em}
\end{wraptable}
\textbf{Different components.} In Table~\ref{tab:Components}, we discuss the effectiveness of different components within our proposed ML–TTA framework on the COCO2014 and VOC2007 datasets, including view prompt~(VP, \textit{i.e.}, TPT~\citep{TTA-TPT}), caption prompt~(CP), and Bound Entropy Minimization~(BEM). Across both RN50 and ViT-B/16 architectures, BEM consistently enhances the mAP performance of VP, CP, and VP+CP, which indicates the reasonable effectiveness of our proposed Bound Entropy Minimization objective. Furthermore, we observe that CP and CP+BEM always achieve superior performance compared to VP and VP+BEM in all settings. Such phenomenon shows treating text as a pseudo-image with a known label set to adapt multi-label test instance is more reliable than augmented views, as the positive label set of views is pseudo.

\subsection{Further Analysis}

\begin{figure}[htbp]
\centering
\begin{minipage}{.5\textwidth}
 \centering
 \captionsetup{font={footnotesize}}
 \captionof{table}{Comparison with binary cross-entropy loss.}
 \vspace{-0.8em}
 \renewcommand{\arraystretch}{1.2} % Adjusts the row height
 \setlength{\tabcolsep}{2.3pt} % Adjusts the column width
 \begin{tabular}{l|c|c|c|c}
 \toprule[1pt]
 \multirow{2}{*}{\footnotesize \textbf{Methods}} & \multicolumn{2}{c|}{\footnotesize \textbf{RN50}} & \multicolumn{2}{c}{\footnotesize \textbf{ViT-B/16}} \\ \cline{2-5}
 & {\scriptsize \textbf{COCO2014}} & {\scriptsize \textbf{VOC2007}} & {\scriptsize \textbf{COCO2014}} & {\scriptsize \textbf{VOC2007}} \\ \midrule[0.4pt] 
 {\footnotesize CLIP} & {\small 47.53} & {\small 75.91} & {\small 54.42} & {\small 79.58} \\ \midrule[0.4pt]
 {\footnotesize VP+CP+BCE} & {\small 48.39} & {\small 75.75} & {\small 54.51} & {\small 78.59} \\
 \cellcolor{gray!25}{\footnotesize \textbf{VP+CP+BEM}} & \cellcolor{gray!25}{\small \textbf{51.58}} & \cellcolor{gray!25}{\small \textbf{78.62}} & \cellcolor{gray!25}{\small \textbf{57.52}} & \cellcolor{gray!25}{\small \textbf{81.28}} \\
 \bottomrule[1pt]
 \end{tabular}
 \label{tab:loss-functions}
\end{minipage}%
\hfill
\begin{minipage}{.45\textwidth}
 \centering
  \vspace{-3em}
 \includegraphics[width=0.9\textwidth]{figures/augmented-views.pdf}
 \captionsetup{font={footnotesize}}
 \vspace{-0.6em}
 \caption{Results on different number of views.}
 \label{fig:augmented views}
\end{minipage}
\end{figure}

\textbf{Loss functions.} Here, we conduct a comparison between Bound Entropy Minimization~(BEM) and the conventional binary cross-entropy (BCE) loss function in multi-label classification tasks. Specifically, we regarded the \textit{weak label set} of confident views as hard labels for those views and the \textit{strong label set} of confident captions as hard labels for those captions, then using BCE loss to optimize the view and caption prompts. The results are shown in Table~\ref{tab:loss-functions}. Compared to CLIP, the mAP improvement using BCE loss on the COCO2014 is less than $1$\%. In contrast, our BEM objective surpasses BCE loss by $3 \!\! \sim \!\! 4$\% in mAP across all benchmarks, which demonstrates BEM is not only more effective than vanilla entropy minimization but also more robust compared to binary cross-entropy loss. BCE loss is not suitable for optimizing a single test instance.


\textbf{Number of augmented views.} Following TPT~\citep{TTA-TPT}, we conduct parameter experiments of different numbers of augmented views on the COCO2014 dataset using ViT-B/16 architecture. As depicted in Figure~\ref{fig:augmented views}, as the number of views increases from 1 to 128, the mAP performance of RLCF and ML--TTA both show an upward trend and begin to stabilize at 64 views. Surprisingly, the performance curve of TPT does not have any regularity, which implies that vanilla entropy minimization, by focusing only on the label with the highest probability, leads to unstable adaptation for multi-label instances.

\begin{wraptable}{r}{9.5cm}
\vspace{-2em}
\centering
\captionsetup{font={footnotesize}}
\caption{Results on different numbers of retrieved captions.}
\vspace{-0.8em}
\renewcommand{\arraystretch}{0.95} % Adjusts the row height
\setlength{\tabcolsep}{2.2pt} % Adjusts the column width50
\begin{tabular}{l|l|cc|c|c|c|c|c|c|c|c}
\toprule[1pt]
 \multicolumn{2}{c|}{\footnotesize \textbf{Datasets}} & {\scriptsize \textbf{CLIP}} & {\scriptsize \textbf{TPT}} & {\scriptsize \textbf{1}} & {\scriptsize \textbf{2}} & {\scriptsize \textbf{4}} & {\scriptsize \textbf{8}} & {\scriptsize \textbf{16}} & {\scriptsize \textbf{32}} & {\scriptsize \textbf{64}} & {\scriptsize \textbf{128}} \\ \midrule[0.4pt]
\multirow{2}{*}[0ex]{\rotatebox{45}{\scriptsize \textbf{RN50}}} & {\scriptsize COCO2014} & {\scriptsize 47.53} & {\scriptsize 48.52} & {\scriptsize 51.35} & {\scriptsize 51.37} & {\scriptsize 51.41} & {\scriptsize 51.49} & {\scriptsize 51.58} & {\scriptsize \textbf{51.59}} & {\scriptsize 51.55} & {\scriptsize 51.48} \\
& {\scriptsize VOC2007}& {\scriptsize 75.91} & {\scriptsize 75.54} & {\scriptsize 78.29} & {\scriptsize 78.33} & {\scriptsize 78.48} & {\scriptsize 78.54} & {\scriptsize \textbf{78.61}} & {\scriptsize 78.59} & {\scriptsize 78.53} & {\scriptsize 78.42} \\ \midrule[0.4pt]
\multirow{2}{*}[0ex]{\rotatebox{45}{\scriptsize \textbf{ViT-B/16}}} & {\scriptsize COCO2014}& {\scriptsize 54.42} & {\scriptsize 53.32} & {\scriptsize 57.23} & {\scriptsize 57.33} & {\scriptsize 57.41} & {\scriptsize 57.48} & {\scriptsize 57.49} & {\scriptsize 57.52} & {\scriptsize 57.55} & {\scriptsize \textbf{57.58}} \\
& {\scriptsize VOC2007}& {\scriptsize 79.58} & {\scriptsize 77.54} & {\scriptsize 81.06} & {\scriptsize 81.12} & {\scriptsize 81.21} & {\scriptsize 81.24} & {\scriptsize \textbf{81.28}} & {\scriptsize 81.19} & {\scriptsize 81.15} & {\scriptsize 80.98} \\
\bottomrule[1pt]
\end{tabular}
\label{tab:retrieved captions}
\vspace{-1.2em}
\end{wraptable}
\textbf{Number of retrieved captions.} We also investigate the impact of allocating different numbers of retrieved captions for each augmented view on the performance of ML–TTA. As shown in Table~\ref{tab:retrieved captions}, when only one caption is allocated to each view, ML--TTA outperforms CLIP or TPT by $3 \!\! \sim \!\! 4$\%. As the number of captions increases, the performance of ML--TTA gradually improves until it stabilizes. For the VOC2007 dataset, too many captions can lead to a slight decrease in performance, as captions that are not highly similar to the view may introduce noisy positive labels that do not exist in the corresponding view.

\section{Conclusion}
In this paper, we investigate a test-time adaptation framework~(ML--TTA) designed for multi-label data without making any presumptions about the distribution of the test instances. The proposed Bound Entropy Minimization~(BEM) objective overcomes the limitation of the vanilla entropy loss, which only optimizes the most confident class. By conceptualizing paired captions as pseudo-views with a known label set, ML--TTA employs BEM to adapt to multi-label test instances by allocating \textit{weak label set} to each augmented view and \textit{strong label set} to each paired caption, binding the \textit{top-k} predicted labels with the highest probabilities. Extensive experiments on the MSCOCO, VOC, and NUSWIDE datasets demonstrate that ML--TTA framework outperforms the source model CLIP and other state-of-the-art test-time adaptation methods, across various model architectures, prompt initialization, and varying label scenarios.

\normalem
\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage

\appendix

\begin{center}
\bf\Large Appendix for Multi-Label Test-Time Adaptation with \\ Bound Entropy Minimization
\end{center}

\section{Proof}
\subsection{Proof of Proposition 1}\label{proof-proposition-1}
Proposition 1. Consider a model's output logits of a selected view $x$, denoted as $\mathbf{s} = (s_1, s_2, \dots, s_L) $, where without loss of generality, we assume $ s_1 > s_2 > \dots > s_L $. It follows that the entropy loss $ H = H(p(\cdot|x)) $ decreases as $ s_1 $ increases, and $ H $ increases as the sum of the remaining logits, $ S_{\text{rest}} = \sum_{i=2}^{L} s_i $, decreases. Formally, this can be written as:
\begin{equation}
 \frac{\partial H}{\partial s_1} < 0 \quad \text{and} \quad \frac{\partial H}{\partial S_{\text{rest}}} > 0.
\end{equation}

\proof{
We denote the predicted probability $p(y=l|x) = \frac{\exp{s_l}}{\sum_{i=1}^L \exp{s_i}}$ as $p_l$ for simplicity, where $s_i$ is the logit of the i-th category. We first calculate the partial derivative of $s_i$ with respect to $p_l$:
\begin{equation}
 \begin{split}
 \frac{\partial p_l}{\partial s_i} &= \frac{\partial}{\partial s_i} \left( \frac{\exp{s_l}}{\sum_{j=1}^L \exp{s_j}} \right) \\
 &= \frac{\delta_{l,i} \exp{s_l}}{\sum_{j=1}^L \exp{s_j}} - \exp{s_l} \frac{ \exp{s_i}}{\left( \sum_{j=1}^L \exp{s_j} \right)^2} \\
 &= \delta_{l,i} p_l - p_l p_i
 \end{split}
\end{equation}
where $\delta_{i,j}=1$ only if $i=j$, else is $\delta_{i,j}=0$. We can now directly calculate the partial derivative of $H$ for $s_i$.
\begin{equation}
 \begin{split}
 \frac{\partial H}{\partial s_i} &= \frac{\partial}{\partial s_i} \left( - \sum_{l=1}^L p_l \log{p_l} \right) \\
 &= - \sum_{l=1}^L \left( \frac{\partial p_l}{\partial s_i} \log{p_l} + p_l \frac{1}{p_l}\frac{\partial p_l}{\partial s_i} \right) \\
 &= - \sum_{l=1}^L \left( \delta_{l,i} p_l \log{p_l} - p_l p_i \log{p_l} + \delta_{l,i} p_l - p_l p_i \right) \\
 &= p_i \log{p_i}+ p_i - \sum_{l=1}^L \left( - p_l p_i \log{p_l} - p_l p_i \right) \\
 &= \left( p_i \log{p_i}+ p_i\right) \left( \sum_{l=1}^L p_l \right) - \sum_{l=1}^L \left( - p_l p_i \log{p_l} - p_l p_i \right) \\
 &= - \sum_{l=1}^L \left( p_l p_i \log{p_i} - p_l p_i \log{p_l} + p_l p_i- p_l p_i \right) \\
 &= - \sum_{l=1}^L p_l p_i \log{\frac{p_i}{p_l}} \\
 \end{split}
\end{equation}
where the fourth equivalent uses the property of $\delta_{i,j}$ and fifth equivalent uses $\sum_{l=1}^L p_l = 1$. Since we assume $ s_1 > s_2 > \dots > s_L $, then the probabilities have the same order $ p_1 > p_2 > \dots > p_L $, therefor:
\begin{equation}
 \begin{split}
 \frac{\partial H}{\partial s_1} &= - \sum_{l=1}^L p_l p_1 \log{\frac{p_1}{p_l}} = - \sum_{l=2}^L p_l p_1 \log{\frac{p_1}{p_l}} < 0 \\
 \end{split}
\end{equation}
as $\log{\frac{p_1}{p_l}} > 0$ for all $l>1$, therefor we proof the first inequality in proposition 1. To prove the second inequality, we first calculate the sum of the partial derivative of $H$ for all logits.
\begin{equation}
 \begin{split}
 \sum_{i=1}^L \frac{\partial H}{\partial s_i} &= \sum_{i=1}^L \left( - \sum_{l=1}^L p_l p_i \log{\frac{p_i}{p_l}} \right) \\
 &= - \sum_{i=1}^L \sum_{l=1}^L \left( p_l p_i\log{p_i} - p_l p_i\log{p_l} \right) \\
 &= - \sum_{i=1}^L \sum_{l=1}^L \left( p_l p_i\log{p_i} - p_i p_l\log{p_i} \right) \\
 &= 0
 \end{split}
 \label{equ:sum_of_gradient}
\end{equation}
where we change the position of index $i$ and $l$ for the second term in the double summation to get the third equivalent. Now the second inequality is easy to get:
\begin{equation}
 \begin{split}
 \frac{\partial H}{\partial S_{\text{rest}}} &= \sum_{i=2}^L \frac{\partial H}{\partial s_i} \bigg/ \frac{\partial S_{\text{rest}}}{\partial s_i} \\
 &= \sum_{i=2}^L \frac{\partial H}{\partial s_i} \bigg/ 1 \\
 &= \sum_{i=1}^L \frac{\partial H}{\partial s_i} - \frac{\partial H}{\partial s_1} \\
 &= - \frac{\partial H}{\partial s_1} > 0 \\
 \end{split}
\end{equation}
}

\subsection{Proof of Proposition 2}\label{proof-proposition-2}
\textit{proposition 2. Consider a model's output logits of a selected view $x$, denoted as $\mathbf{s} = (s_1, s_2, \dots, s_L) $, where without loss of generality, we assume $s_1 > s_2 > \dots > s_L$. We define the modified logits as $\mathbf{s}' = (s_1', s_2', \ldots, s_L')$, where $s_i' = a_i + s_i$ for $i \leq k$ with $ a_i = s_1 - s_i$ and $s_i' = s_i$ for $i > k$. Here, $a_i$ is a constant that does not participate in differentiation, resulting in $s_i' = s_1$ for all $i \leq k$. Let $ S_{\text{rest}} = \sum_{i=k+1}^L s_i$. For the modified logits $\mathbf{s}'$, we define the modified probability $\mathbf{p}' = \text{Softmax}(\mathbf{s}')$, and the modified entropy as $H' = -\sum_{i=1}^L p_i' \log p_i'$. It follows that:}
\begin{equation}
 \frac{\partial H'}{\partial s_i} < 0, \quad \forall i \leq k \quad \text{and} \quad \frac{\partial H'}{\partial S_{\text{rest}}} > 0.
\end{equation}

\proof{
With the assumption $s_1 > s_2 > \dots > s_L$ and the definition of $\mathbf{s}'$, we have $s_1' = s_2' = \dots = s_k' > \dots > s_L'$. Use the conclusion in proposition 1, for $i \leq k$, we have:
\begin{equation}
 \begin{split}
 \frac{\partial H'}{\partial s_i} = \frac{\partial H'}{\partial s_i'} \frac{\mathrm{d} s_i'}{\mathrm{d} s_i} 
 = \frac{\partial H'}{\partial s_i'} \times 1
 = - \sum_{l=1}^L p_l' p_i' \log{\frac{p_i'}{p_l'}} 
 = - \sum_{l=k+1}^L p_l' p_i' \log{\frac{p_i'}{p_l'}} < 0& \\
 , \quad \forall i \leq k& \\
 \end{split}
\end{equation}
where $p_i' > p_l'$ for $i\leq k$ and $l > k$ as $s_1' = s_2' = \dots = s_k' > \dots > s_L'$. Similar to the proof of proposition 1, we use the conclusion of $\sum_{i=1}^L \frac{\partial H}{\partial s_i'} = 0$, which has been proved in Equation.~\ref{equ:sum_of_gradient} to prove the second inequality. 
\begin{equation}
 \begin{split}
 \frac{\partial H'}{\partial S_{\text{rest}}} &= \sum_{i=k+1}^L \frac{\partial H}{\partial s_i} \bigg/ \frac{\partial S_{\text{rest}}}{\partial s_i} \\
 &= \sum_{i=k+1}^L \frac{\partial H}{\partial s_i} \bigg/ 1 \\
 &= \sum_{i=1}^L \frac{\partial H}{\partial s_i} - \sum_{i=1}^k \frac{\partial H}{\partial s_i} \\
 &= - \sum_{i=1}^k \frac{\partial H}{\partial s_i} > 0 \\
 \end{split}
\end{equation}
}

\section{Detailed Label Binding Process.}
In this section, we present a certain example to showcase the calculation of Label Binding~\ref{label binding}. Label binding refers to making the \textit{top-k} predicted logits equal, as expressed below:
\begin{small}
\begin{equation}
    \tilde s_{ij}^{\mathbf x^{\text {test}}}=( ( m_i^{\mathbf x^{\text {test}}} - s_{ij}^{\mathbf x^{\text {test}}} ) + s_{ij}^{\mathbf x^{\text {test}}} ) \times \mathbb I( \mathrm {Rank}\_{( s_{ij}^{\mathbf x^{\text {test}}}, \mathbf s_{i}^{\mathbf x^{\text {test}}})} \leq k^{\mathbf x_i^{\text {test}}} )+ s_{ij}^{\mathbf x^{\text {test}}} \times \mathbb I (\mathrm {Rank}\_{(s_{ij}^{\mathbf{x}^{\text {test}}}, \mathbf s_i^{\mathbf x^{\text {test}}})} > k^{\mathbf x_i^{\text {test}}} ),
\end{equation}\label{label bining example}
\end{small}
\vspace{-1.5em}

Since label binding (making ... equal) is non-differentiable, we employ the stop-gradient operation in VQ-VAE~\cite{VQ-VAE} for backpropagation, $i.e.$ $( ( m_i^{\mathbf x^{\text {test}}} - s_{ij}^{\mathbf x^{\text {test}}} ) + s_{ij}^{\mathbf x^{\text {test}}} )$ to perform label binding. 

Taking a $3$-class classification task as an example with class labels of $(1,2,3)$, assuming $k^{\mathbf x_i^{\text {test}}}$ is $2$, and the label binding process is $\mathbf s = [\mathbf {0.9}, \mathbf {0.7}, 0.3] \rightarrow \mathbf s^{'} = [\mathbf {0.9}, \mathbf {0.9}, 0.3]$. $\tilde s_{ij}^{\mathbf x^{\text test}}$ represents the logit of the $j$-th class in the $i$-th augmented view after label binding, $e.g.$, $\tilde s_{i2}^{\mathbf x^{\text test}}$ changes from $\mathbf {0.7}\rightarrow\mathbf{0.9}$. $m_i^{\mathbf x^{\text {test}}}$ denotes the maximum value of $\mathbf s$, which is $\mathbf {0.9}$. $\mathbb I(\cdot)$ is the indicator function. $\mathrm {Rank}\_{(a, \mathbf b)}$ indicates the descending rank of $a$ within $\mathbf{b}$, $e.g.$, $\mathrm {Rank}\_{(0.7, \mathbf s)} = 2$. The process for computing the bound logit for each class is as follows:
\begin{small}
    \begin{equation}
        \begin{split}
            \tilde s_{i1}^{\mathbf x^{\text {test}}} & = ( ( 0.9 - 0.9 ) + 0.9 ) \times \mathbb I( \mathrm {Rank}\_{( 0.9, \mathbf s)} \leq 2)+ 0.9 \times \mathbb I (\mathrm {Rank}\_{(0.9, \mathbf s)} > 2) \\ 
            & = 0.9 \times \mathbb I( 1 \leq 2)+ 0.9 \times \mathbb I (1 > 2) \\
            & = 0.9, \\
            \tilde s_{i2}^{\mathbf x^{\text {test}}} & =( ( 0.9 - 0.7 ) + 0.7 ) \times \mathbb I( \mathrm {Rank}\_{( 0.7, \mathbf s)} \leq 2)+ 0.7 \times \mathbb I (\mathrm {Rank}\_{(0.7, \mathbf s)} > 2) \\
            & = 0.9 \times \mathbb I( 2 \leq 2)+ 0.7 \times \mathbb I (2 > 2) \\
            & = 0.9, \\
            \tilde s_{i3}^{\mathbf x^{\text {test}}} & =( ( 0.9 - 0.3 ) + 0.3 ) \times \mathbb I( \mathrm {Rank}\_{( 0.3, \mathbf s)} \leq 2)+ 0.3 \times \mathbb I (\mathrm {Rank}\_{(0.3, \mathbf s)} > 2) \\
            & = 0.9 \times \mathbb I( 3 \leq 2)+ 0.3 \times \mathbb I (3 > 2) \\ 
            & = 0.3,
        \end{split}
    \end{equation}
\end{small}
label binding process changes the logits from $[\mathbf {0.9}, \mathbf {0.7}, 0.3] \rightarrow [\mathbf {0.9}, \mathbf {0.9}, 0.3]$.

\section{Text description base construction}\label{text base}
Here, we present the construction of the text description base using Large language models~(LLMs). Initially, for a set of labels, denoted as $\mathcal{L}=\{ l_1, l_2, ..., l_L \}$, where $L$ represents the total number of labels across all multi-label datasets. Following PVP~\citep{TAI-PVP}, we define a prompt template to instruct LLama-2-7B~\citep{LLaMA-2}, generating descriptions that describe a nature scene, which is as follows:

\noindent\textit{PROMPT: Make a sentence to describe a photo. Requirements: Each sentence should be less than 15 words and include keywords: $\{l_{i_1}, l_{i_2}, \dots, l_{i_j}\}$},

where $\{l_{i_1}, l_{i_2}, \dots, l_{i_j}\}$ is a subset of $\mathcal{L}$ with $i \leq 5$. We randomly sample $j$ categories from $\mathcal{L}$ and input these categories along with the prompt template into LLMs to automatically generate text descriptions. After obtaining generated descriptions, we employ the nouns filtering strategy used in PVP to extract textual labels for each description. Some examples are illustrated below:

\begin{enumerate}
    \item A \underline{hot dog} \underline{toaster} is positioned next to a \underline{stop sign}. 
    \item A group of \underline{girls} enjoying a game of \underline{frisbee} while sitting on \underline{chairs}.
    \item The little \underline{boy} dreams of becoming a \underline{pilot} as he falls asleep with his \underline{aeroplane}.
    \item \underline{Remotes} control the \underline{TV}, allowing \underline{people} to enjoy their favorite shows.
    \item A \underline{motorbike} speeds past a \underline{man} wearing a \underline{tie}, as he holds a \underline{wine glass} in one hand.
\end{enumerate}
where the underlined words indicate the textual labels extracted from the corresponding description. However, due to the uncontrollable quality and relevance of the paired captions generated by LLMs, these captions may not always accurately represent the image contents. In real-world scenarios, besides adopting a confident-based filtering strategy to filter out views and captions with high entropy (\textit{e.g.}, low confidence), we can also explore more robust strategies to retrieve paired captions, such as, constructing high-quality and content-rich text description databases, ensembling label sets from multiple captions, or improving the similarity retrieval strategy, thereby reducing the impact of noise on the model's adaptation.

\end{document}