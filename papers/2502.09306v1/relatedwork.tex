\section{Related Work}
\label{sec:related_work}


\paragraph{Score-based generative models.
}
Our approach is similar to earlier generative modelling techniques based on annealed Langevin dynamics \citep{song2019generative}, which inspired the advancement of diffusion models.
The existing literature analysing these Langevin Monte Carlo algorithms is limited.
\citet{block2022generativemodelingdenoisingautoencoders} derive an error bound in Wasserstein distance that scales exponentially with the data dimension, while \citet{lee2022convergence} establish a non-asymptotic bound in total variation, which is weaker than our bound in \gls*{KL}, as implied by Pinsker's inequality.

On the other hand, the convergence of diffusion models \citep{song2020score, ho2020denoising} has been extensively studied. 
Early results either established non quantitative bounds \citep{pidstrigach2022scorebased}, relied on restrictive assumptions about the data distribution, such as functional inequalities \citep{lee2022convergence}, or exhibited exponential dependence on the problem parameters \citep{ de_bortoli2022convergence}. 
Recent works have established polynomial convergence bounds under more relaxed assumptions \citep{Chen2022ImprovedAO, chen2023sampling, benton2024nearly, li2024towards}. In particular, \citet{Chen2022ImprovedAO} introduce two bounds on the $\kl$ error: a linear bound in the data dimension under smoothness conditions along the entire diffusion path, and a second one which scales quadratically with $d$, achieved through early stopping and the assumption of a finite second-order moment on $\pid$. In contrast, \citet{benton2024nearly} provide a bound that is linear in the data dimension, up to logarithmic factors, assuming only that the data distribution has a finite second-order moment. Their proof exploits the specific structure of the \gls*{OU} process to control the error arising from discretising the reverse \gls*{SDE}.


\paragraph{Stochastic interpolants.}

Stochastic interpolants \citep{albergo2023stochastic} are generative models that unify flow-based and diffusion-based methods. These models make use of a broad class of continuous-time stochastic processes designed to bridge any two arbitrary probability density functions exactly in finite time, akin to our work. 
Specifically, the formulation of linear one-sided stochastic interpolants \citep{albergo2023stochastic, gao2024gaussian}, which interpolate between a Gaussian and the data distribution, is equivalent to the Gaussian diffusion path \eqref{eq:convolutional_path}. 
Unlike our approach, they incorporate intractable control terms into the drift of the \gls*{SDE} to ensure the marginals have the desired distributions. This may result in  numerical instabilities caused by singularities in the drift at $t= T$ \citep[Section 6]{albergo2023stochastic}. In contrast, we implement the diffusion path using Langevin dynamics. Furthermore, their theoretical analysis does not include explicit non-asymptotic convergence bounds.

\paragraph{Tempering.}
Tempering \citep{PhysRevLett.57.2607, geyer_mcmc_92, Marinari_1992} is a well-known technique in the sampling literature that involves sampling the system at multiple temperatures: starting with higher temperatures to facilitate transitions between modes, gradually cooling the system to focus on the local structure of the target distribution. 
The sequence of tempered target distributions is typically defined using the geometric path, as it can be computed in closed form when the target density is known up to a normalising constant.
Recently, several works have established theoretical guarantees for the convergence of geometric annealed Langevin Monte Carlo for non-log-concave distributions. In particular, \citet{guo2024provablebenefitannealedlangevin} provides a bound on the $\kl$  similar to that of Theorem~\ref{theorem:discretisation_analysis_convolutional_path}. However, they are unable to obtain a closed-form expression for the action of the path. 
Besides, \citet{chehab2024provableconvergencelimitationsgeometric} derive upper and lower convergence bounds for the $\kl$ of the marginals, based on functional inequalities assumptions. 
In particular, they demonstrate that in some cases the log-Sobolev constant of the intermediate distributions along the path can deteriorate exponentially compared to those of the base and data distributions, unlike for the diffusion path.