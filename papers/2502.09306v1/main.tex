\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{hyperref}
\usepackage{dsfont}
\usepackage{booktabs}       
\usepackage{amsfonts}       
\hyphenpenalty=50000
\usepackage{amsmath, amssymb}
\usepackage[a4paper,hscale=0.7,vscale=0.75,centering]{geometry}
\usepackage{fullpage}
\usepackage{bbm}
\usepackage{graphicx}
\usepackage{float}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[square,sort]{natbib}
\usepackage{authblk}
\usepackage{amsmath, amsthm}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{upgreek}
% Acknowledgments
\long\def\acks#1{\vskip 0.3in\noindent{\large\bf Acknowledgments and Disclosure of Funding}\vskip 0.2in
\noindent #1}
\sloppy

\usepackage{microtype}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{nicefrac}       
\usepackage{mathtools}
\usepackage{soul}


\usepackage[acronym,nowarn]{glossaries}
\makeglossaries

\newacronym{DALMC}{{\textsc{\small DALMC}}}{diffusion annealed Langevin Monte Carlo}
\newacronym{DALD}{{\textsc{\small DALD}}}{diffusion annealed Langevin dynamics}

\newacronym{SGM}{SGM}{score-based generative model}
\newacronym{OU}{OU}{Ornstein-Uhlenbeck}
\newacronym{SDE}{SDE}{stochastic differential equation}
\newacronym{KL}{KL}{Kullback-Leibler}

% abbreviation package

\newcommand{\md}{\mathrm{d}}
\newcommand{\alphamin}{\alpha_{\min}}
\newcommand{\alphamax}{\alpha_{\max}}
\newcommand{\taumin}{\tau_{\min}}
\newcommand{\taumax}{\tau_{\max}}
% \usepackage{subcaption}
\usepackage{graphicx}
\usepackage{dsfont}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\arginf}{arg\,inf}
\DeclareMathOperator*{\kl}{KL}
\DeclareMathOperator*{\pid}{\pi_{\text{data}}}
\DeclareMathOperator*{\score}{\text{score}}



\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem*{assumptionb}{A\hspace{-2pt} 4}
\newtheorem{assumptionnew}{B\hspace{-2pt}}
\newtheorem{assumptiongla}{C\hspace{-2pt}}
\newtheorem{assumptiongla2}{D\hspace{-2pt}}
\newtheorem{obs}{Note}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\usepackage{xcolor}


% acronyms

\usepackage{hyperref}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\newtheorem{assumption}{\textbf{A}\hspace{-2pt}}
\Crefname{assumption}{\textbf{A}\hspace{-3pt}}{\textbf{H}\hspace{-3pt}}
\crefname{assumption}{\textbf{A}}{\textbf{A}}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0.5em}  

\title{Non-asymptotic Analysis of Diffusion Annealed Langevin Monte Carlo for Generative Modelling}
\author[1]{Paula Cordero-Encinar}
\author[1]{\"{O}. Deniz Akyildiz}
\author[1, 2]{Andrew B. Duncan}
\affil[1]{Imperial  College London}
\affil[2]{The Alan Turing Institute}
\begin{document}

\maketitle


\vskip 0.3in

\begin{abstract}
We investigate the theoretical properties of general diffusion (interpolation) paths and their Langevin Monte Carlo implementation, referred to as \gls*{DALMC}, under weak conditions on the data distribution. Specifically, we analyse and provide non-asymptotic error bounds for the annealed Langevin dynamics where the path of distributions is defined as Gaussian convolutions of the data distribution as in diffusion models. We then extend our results to recently proposed heavy-tailed (Student's $t$) diffusion paths, demonstrating their theoretical properties for heavy-tailed data distributions for the first time. Our analysis provides theoretical guarantees for a class of score-based generative models that interpolate between a simple distribution (Gaussian or Student's $t$) and the data distribution in \textit{finite time}. This approach offers a broader perspective compared to standard score-based diffusion approaches, which are typically based on a forward \gls*{OU} noising process.
\end{abstract}

\section{Introduction}
\label{section:introduction}
\Glspl*{SGM} \citep{song2020score, ho2020denoising} have become immensely popular in recent years due to their excellent performance in generating high-quality data. 
This success has led to widespread adoption across various generative modelling tasks, e.g., image generation \citep{dhariwal2021diffusion, Rombach_2022_CVPR, saharia2022photorealistic}, audio generation \citep{Ruan_2023_CVPR}, 
reward maximisation \citep{pmlr-v162-janner22a, he2023diffusion}. 
Additionally, their remarkable performance has sparked significant interest within the theoretical community to better understand the structure and properties of these models \citep{lee2022convergence, Chen2022ImprovedAO,
chen2023sampling, benton2024nearly}.

The goal of generative modelling is to learn the underlying probability distribution $\pid$ from a given set of samples.
Diffusion models, a particular class of \glspl*{SGM}, achieve this by using a \textit{forward process}, typically an \gls*{OU} process, to construct a path of probability distributions from the data distribution towards a simpler one -- a Gaussian. 
The time-reversed process can be characterised \citep{anderson1982reverse} but necessitates the knowledge of the scores of the marginal distributions along this path. 
These scores are usually intractable - hence they are learnt by noising the data and applying score matching techniques \citep{hyvarinen2005estimation, pascal_score_matching, pmlr-v115-song20a}. 
The learnt scores are then used to sample from the path by discretising the time-reversed diffusion process \citep{song2020score}.

\begin{figure}[t]
\vskip 0.2in
  \centering
  \begin{subfigure}{0.33\textwidth}
         \includegraphics[trim={45 65 45 65}, clip, width=\textwidth]{plots/geom_path.pdf}
         \caption{Geometric path}
         \label{fig:geometric_path_mixture_u_g}
     \end{subfigure}
     \begin{subfigure}{0.33\textwidth}
         \includegraphics[trim={45 65 45 65}, clip, width=\textwidth]{plots/conv_path.pdf}
        \caption{Diffusion path}
         \label{fig:convolutional_path_mixture_u_g}
     \end{subfigure}
    
     \caption{A visual comparison of the geometric path versus the diffusion path for $(\mu_t)_{t\in[0,1]}$. The \textit{base} distribution is given by $\mu_0 := \mathcal{N}(0,1)$ and the data distribution, $\mu_1 := \pid$, is a mixture of a Gaussian and a smoothed uniform distribution (see Section~\ref{sec:gaussian_diffusion}). As observed by \citet{chehab2024provableconvergencelimitationsgeometric}, the geometric path in (a) creates intermediate multimodal distributions which are hard to sample from. In contrast, the diffusion path in (b) stays unimodal throughout, offering more favourable properties.}
  \label{fig:convolutional_vs_geometric}
\vskip -0.2in
\end{figure}
While the forward \gls*{OU} process is mathematically convenient, it does not capture the whole idea of bridging distributions and  requires infinite time to interpolate between the data distribution $\pid$ and a Gaussian measure. In practice, however, diffusion models consider the evolution of the \gls*{OU} process only up to a finite final time $T$. Thus, the path does not fully bridge $\pid$ and a standard Gaussian.
During generation, these models instead evolve samples along a sequence of interpolated distributions between the final marginal distribution of the OU process at time $T$ and $\pid$ (although, in practice, they are initialised from a Gaussian). 
Specifically, this interpolation is characterised by defining intermediate random variables $X_t \sim \mu_t$ \footnote{In our case, the \text{base} (simple) distribution is defined at time $0$ as $\mu_0$, and the data distribution is defined at time $T$, $\mu_T = \pid$. This contrasts with standard diffusion models where the base distribution is defined at time $T$ and the data distribution at time $0$.} \citep{chehab2024practicaldiffusionpathsampling} as
\begin{align}\label{eq:one-sided-interpolant}
X_t = \sqrt{\lambda_t} X + \sqrt{1-\lambda_t} Z,
\end{align}
for $t \in [0, T]$, where $X \sim \pi_{\text{data}}$, $Z \sim \mathcal{N}(0, I)$ is independent of $X$ and a  schedule $\lambda_t = \min\{1, e^{-2(T-t)}\}$. 

The interpolation perspective of diffusion models has been investigated, see, e.g., \citet{albergo2023stochastic, gao2024gaussian}. Notably, the path in Eq.~\eqref{eq:one-sided-interpolant} is a special case of the \textit{one-sided stochastic interpolants} \citep{albergo2023stochastic}. 
As outlined in these works, the  reverse process can be made to exactly interpolate between a base distribution $\nu$ and $\pi_{\text{data}}$ in finite time by using an appropriate schedule $\lambda_t$ and introducing control terms in the corresponding \glspl*{SDE}. 
Similarly to the score term in diffusion models, these control terms are intractable and need to be learnt.

In this work, we adopt a practical approach to general linear interpolation paths between a simple base distribution $\nu$ and $\pi_{\text{data}}$, that is, $X_t = \sqrt{\lambda_t} X + \sqrt{1-\lambda_t} Z$, where $X\sim\pid$, $Z\sim\nu$ independent of $X$ and $\lambda_t\in[0,1]$, $\lambda_T=1$. 
In particular, we explore 
the behaviour of Langevin dynamics driven by the gradients of $\log\mu_t$ for $t \in [0, T]$, where $\mu_t$ are the intermediate distributions, i.e., $X_t\sim \mu_t$. 
Our approach is akin to earlier generative modelling methods based on \textit{annealed Langevin dynamics} \citep{song2019generative} which led to the development of diffusion models. However, there has been limited work analysing these methods under minimal assumptions on $\pid$. 
\citet{block2022generativemodelingdenoisingautoencoders} provide the first theoretical analysis in Wasserstein distance under smoothness and dissipativity of the data distribution. They show that the error depends exponentially on the dimension.
In contrast, \citet{lee2022convergence} provides a non-asymptotic bound in total variation under smoothness conditions and a bounded log-Sobolev constant of the data distribution. Specifically, we make the following contributions.

\paragraph{Contributions} 
\begin{itemize}
\item We provide an analysis of annealed Langevin dynamics methods driven by general linear interpolation paths between $\nu$ and $\pi_{\text{data}}$, which we term diffusion annealed Langevin Monte Carlo (\gls*{DALMC}).
In the case where $\nu$ is a Gaussian distribution, we derive non-asymptotic convergence bounds in \gls*{KL} divergence under different assumptions.

By assuming that $\pid$ has a finite second-order moment $M_2$, 
 $\log \pid$ has Lipschitz gradients and either $\pid$ is strongly convex outside a ball or $\nabla^2\log\pid$ decays to $0$ sufficiently fast (as is the case for Student's $t$-like distributions), we show in Corollary~\ref{corollary:complexity_bounds} that \gls*{DALMC} requires $\mathcal{O}\left(\frac{d (M_2 \vee d)^2 L_{\max}^2}{\varepsilon^6}\right)$ steps to achieve $\varepsilon^2$-accurate sampling from $\pid$ in \gls*{KL} divergence with a sufficiently accurate score estimator. Here, $d$ is the dimension of the data and $L_{\max} := \max_{t \in [0,T]} L_t$ where $L_t$ denotes the Lipschitz constant of $\nabla\log\mu_t$, which we prove to be finite in Lemma~\ref{lem:regularity_of_gaussian_path}, improving the results of \citet[Proposition 20]{gao2024gaussian} under the specified conditions.
Furthermore, under slightly less restrictive assumptions involving smoothness of $\pid$ with constant $L_\pi$, bounded second order moment $M_2$ and $\mathbb{E}_{\pid} \left\Vert\nabla \log\pid\left(Y\right)\right\Vert^{8}\leq K_\pi^2$, we demonstrate that the data distribution can be approximated to $\varepsilon^2$-accuracy in \gls*{KL} divergence with $\mathcal{O}\left(\frac{(M_2 \vee d)^2(d^2\vee L_\pi^2d \vee K_\pi) L_\pi}{\varepsilon^6}\right)$ steps. 
To the best of our knowledge, these are the first results obtained in \gls*{KL} divergence for these Langevin-dynamics driven generative models \citep{song2019generative}.

\item We then extend this analysis into recent heavy-tailed diffusion models \citep{pandey2024heavy} based on Student's $t$ noising distributions, that is, when the base distribution $\nu$ is chosen to be a Student's $t$ distribution.
In this case, assuming that the data distribution is smooth, has a finite second-order moment and exhibits a tail behaviour similar to that of a multivariate Student's $t$ distribution, we show that \gls*{DALMC} can be used to sample from the data distribution with the same complexity as the Gaussian case.  
As far as we are aware, this is the first analysis of heavy-tailed diffusion models with explicit complexity estimates.
\item We show that, under certain conditions on the covariances, a mixture of Gaussians with different covariances satisfy smoothness conditions and is strongly log-concave outside of a ball, implying a finite log-Sobolev constant. 
This result is of independent interest, as most analyses of Gaussian mixtures in the literature primarily focus on the equal covariance setting.
\end{itemize}

The rest of the paper is organised as follows. Section~\ref{sec:background} presents our setting and necessary background. Section~\ref{sec:gaussian_diffusion}, provides a non-asymptotic analysis of the general diffusion paths with Gaussian base distribution and their implementation via Langevin dynamics.
In Section~\ref{sec:heavy_tailed_diffusion}, we extend our analysis to heavy-tailed diffusion models.
Section~\ref{sec:related_work} discusses related literature, followed by the conclusion.

\paragraph{Notation}
Let $d$ be the dimension of data. Let $A, B$ be square matrices of the same dimension, we say $A\preccurlyeq B$ if $B-A$ is a positive semidefinite matrix and $\Vert\cdot\Vert_F$ denotes the Frobenius norm. For $a, b>0$, we write $a\lesssim b$ or $a=\mathcal{O}(b)$ to indicate that $a\leq C b$ for an absolute constant $C\geq 0$, and $a\asymp b$ if $a=\mathcal{O}(b)$ and $b=\mathcal{O}(a)$. For $f:\mathbb{R}^d\to\mathbb{R}^d$ and a probability measure $\mu$ on $\mathbb{R}^d$, we define $\Vert f\Vert_{L^2(\mu)}:=\left(\int\Vert f\Vert \md \mu\right)^{1/2}$ and  $M_{2} := \mathbb{E}_{\pid}[\Vert X\Vert^2]$.

\section{Generative Modelling via Diffusion Paths}
\label{sec:background}

We present the background and setting for our analysis.


\subsection{Diffusion Paths}
In practice, implementing the reverse process in diffusion models consists in sampling along a path of probability distributions $(\mu_t)_{t\in [0,T]}$, which starts at a simple distribution $\mu_0$ and ends at an arbitrarily complex data distribution $\mu_T = \pi_{\text{data}}$. 
In particular, when the forward process is an \gls*{OU} process and evolves the data distribution for time $T$, the starting distribution of the reversed process takes the form ${e^{dT}}\pid(e^T x)*\ \mathcal{N}(0, (1-e^{-2T})I)$ and the interpolated distributions $(\mu_t)_{t}$ are the marginals of the \gls*{OU} process.
Building on this, we can describe a more general version of the probability distribution paths that diffusion models attempt to sample from \citep{chehab2024practicaldiffusionpathsampling}, as 
\begin{align}
    \mu_t(x) = \frac{\pi_{\text{data}}(x/\sqrt{\lambda_t})}{{\lambda_t}^{d/2}}  * \frac{\nu \left({x}/{\sqrt{1-\lambda_t}}\right)}{(1-\lambda_t)^{d/2}},\label{eq:convolutional_path}
\end{align}
where $*$ denotes the convolution operation, $\nu$ describes the base or \textit{noising} distribution, and $\lambda_t$ is an increasing function called schedule, such that, $\lambda_t\in[0, 1]$ and $\lambda_T=1$. We refer to the probability path $(\mu_t)_{t\in [0,T]}$ in~\eqref{eq:convolutional_path} as the \textit{diffusion path}. In the setting of the \gls*{OU} (i.e. variance preserving) process, $\lambda_t$ corresponds to $\lambda_t = \min\{1, e^{-2(T-t)}\}$.

The diffusion path with the \gls*{OU} schedule has demonstrated very good performance in the generative modelling literature and has recently started to be explored for sampling \citep{huang2024reverse, richter2024improved, vargas2024transport}. For instance, \citet{phillips2024particle} empirically observed that the diffusion path may have a more favourable geometry for the Langevin sampler than the geometric path, obtained by taking the geometric mean of the base and target distributions, as is typically done in annealing due to the tractability of the score (Figure \ref{fig:convolutional_vs_geometric}).
 
While successful, the use of the \gls*{OU} process presents some challenges in practice. 
As mentioned earlier, the forward \gls*{OU} process cannot reach $\nu$ in finite time, meaning that, in theory the reversed path starts from a non-Gaussian distribution $\mu_0$. However, in practice, the paths are initialised from Gaussians, introducing a bias that is present in error bounds \citep{Chen2022ImprovedAO, chen2023sampling, benton2024nearly}. In our setting, by selecting an appropriate schedule for the diffusion path \eqref{eq:convolutional_path}, which satisfies $\lambda_0=0$ and $\lambda_T=1$, the path of probability distributions $(\mu_t)_{t \in [0, T]}$ can interpolate exactly between $\mu_0 = \nu$ and $\mu_T=\pid$ in finite time, unlike the \gls*{OU} process. This formulation is equivalent to that of linear one-sided stochastic interpolants which can also be realised through \glspl*{SDE} \citep[Theorem 5.3]{albergo2023stochastic}.


We will next explore an alternative approach for generative modelling with general linear diffusion paths, namely, running annealed Langevin dynamics on paths $(\mu_t)_{t\in [0, T]}$ that are constructed to meet the correct marginals.


\subsection{Annealed Langevin Dynamics for Diffusion Paths}

For general diffusion paths, the ``reverse process'' cannot be described by a closed form \gls*{SDE}. 
While \citet{albergo2023stochastic}, estimate the intractable drift term of the \gls*{SDE} using neural networks, their approach can experience numerical instabilities at $t=T$ (see \citet[Section 6]{albergo2023stochastic}) due to singularities in the drift term.
Therefore, in this work, we focus on \textit{annealed Langevin dynamics} \citep{song2019generative} to explicitly implement a sampler along the diffusion path, avoiding the extra control terms introduced in \citet{albergo2023stochastic}. Note that the score at each time $t$ can be learnt via score matching techniques, as in \citet{song2019generative}.

Our annealed Langevin dynamics consists of running a time-inhomogeneous Langevin \gls*{SDE}, where the drifts are given by the scores of reparametrised probability distributions from the diffusion path $(\hat{\mu}_t = \mu_{\kappa t})_{t\in[0,T/\kappa]}$, for some $0<\kappa<1$. That is, we will use the following \gls*{SDE}
\begin{equation}\label{eq:annealed_langevin_sde}
 \md X_t = \nabla \log \hat{\mu}_t(X_t) \md t + \sqrt{2} \md B_t\quad t\in[0, T/\kappa],
\end{equation}
where {$X_0 \sim \mu_0=\nu$ and $(B_t)_{t\geq 0}$ is a Brownian motion. 
We refer to \eqref{eq:annealed_langevin_sde} as \textit{diffusion annealed Langevin dynamics}. 
This strategy does provide a viable alternative to implement interpolation paths as the scores can be learnt. 
In particular, we consider the diffusion annealed Langevin Monte Carlo (\gls*{DALMC}) algorithm given by a simple Euler-Maruyama discretisation of \eqref{eq:annealed_langevin_sde}  and the use of a score approximation function $s_\theta(x, t)$ \citep{song2019generative}:
\begin{equation}\label{eq:annealed_langevin_mcmc_algorithm_score_approx}
    X_{l+1} = X_l + h_l s_{\theta}(X_l, t_l) + \sqrt{2 h_l} \xi_l,
\end{equation}
where $h_l > 0$ is the step size, $\xi_k\sim \mathcal{N}(0,I)$, $s_\theta(x, t)$ approximates $\nabla\log\hat{\mu}_{t}(x)$, $l\in\{1,\dots, M\}$ and $0=t_0<\dots<t_M=T/\kappa$ is a discretisation of the interval $[0,T/\kappa]$. 


It is important to note that, even if simulated exactly, diffusion annealed Langevin dynamics introduces a bias, as the marginal distributions of the solution of the \gls*{SDE} \eqref{eq:annealed_langevin_sde} do not exactly correspond to $(\hat{\mu}_t)_t$, unlike in the stochastic interpolants formulation \citep{albergo2023stochastic}. One of the contributions of our work will be to quantify this bias non-asymptotically.
A key component in determining the effectiveness of the diffusion annealed Langevin dynamics will be the action of the curve of probability measures $\mu =(\mu_t)_{t\in[0, T]}$ interpolating between the base distribution and the data distribution, denoted by $\mathcal{A}(\mu)$. As noted by \citet{guo2024provablebenefitannealedlangevin}, the action serves as a measure of the cost of transporting $\nu$ to $\pid$ along the given path. Formally, the action of an absolutely continuous curve of probability measures \citep{lisini2007characterization} with finite second-order moment is defined as follows 
\begin{equation*}
  \mathcal{A}(\mu):=\int_0^T \lim_{\delta\to 0}\frac{W_2(\mu_{t+\delta}, \mu_t)}{\vert \delta\vert}.  
\end{equation*}
Based on Theorem 1 from \citet{guo2024provablebenefitannealedlangevin}, we have that the \gls*{KL} divergence between the path measure of the diffusion annealed Langevin dynamics \eqref{eq:annealed_langevin_sde}, $\mathbb{P}_{\text{DALD}} = (p_{t,\text{DALD}})_{t\in[0, T/\kappa]}$, and that of a reference \gls*{SDE} such that the marginals at each time have distribution $\hat{\mu}_t$, $\mathbb{P}=(\hat{\mu}_{t})_{t\in[0, T/\kappa]}$, can be bounded in terms of the action.
In particular, when $p_0 = p_{0, \text{DALD}}$, it follows from Girsanov's theorem that
\begin{equation*}
 \kl\left(\mathbb{P}\;||\mathbb{P}_{\text{DALD}}\right)\leq \kappa\mathcal{A}(\mu).
\end{equation*}
See Theorem \ref{theorem:preliminaries_continuous_time_kl_bound} in Appendix \ref{appendix:background} for the proof and further details. Note that by the data processing inequality, we have that $ \kl\left(\pid\;||p_{T/\kappa,\text{DALD}}\right)\leq \kl\left(\mathbb{P}\;||\mathbb{P}_{\text{DALD}}\right)$, meaning that the \gls*{KL} divergence between the data distribution and the final marginal distribution of the diffusion annealed Langevin dynamics \eqref{eq:annealed_langevin_sde} is bounded, provided that the action is finite. In that case by choosing $\kappa = \mathcal{O}(\varepsilon^2/\mathcal{A}(\mu))$, we ensure that $\kl\left(\pid\;||p_{T/\kappa,\text{DALD}}\right)\lesssim \varepsilon^2$. 


\subsection{Initial Assumptions}
In what follows, we will provide an in-depth analysis of the \gls*{DALMC} algorithm when the base distribution $\nu$ is Gaussian or multivariate Student's $t$ distribution. The latter relates to recent heavy-tailed diffusion models \citep{pandey2024heavy}.  
Our results in both cases are based on the following assumptions, with additional ones introduced later as necessary.

First, as is typical in the diffusion model literature we require an $L^2$ accurate score estimator \citep{Chen2022ImprovedAO, chen2023sampling}.
\begin{assumption}\label{assumption:score_approximation}
    The score approximation function $s_\theta(x, t)$ satisfies 
    \begin{equation*}
        \sum_{l=0}^{M-1} h_l\mathbb{E}_{\hat{\mu}_t}\left[\left\Vert \nabla \log \hat{\mu}_l(X_{t_l}) - s_\theta(X_{t_l}, t_l)\right\Vert^2\right] \leq \varepsilon_{score}^2.
    \end{equation*}
    where $0=t_0<t_1<\dots<t_M=T/\kappa$ is a discretisation of the interval $[0,T/\kappa]$.
\end{assumption}

\begin{assumption}\label{assumption:finite_second_order_moment} The data distribution $\pi_{\text{data}}$ has a finite second-order moment, that is, $M_2 = \mathbb{E}_{\pid}[\Vert X\Vert^2] < \infty$.
\end{assumption}

\section{Gaussian Diffusion Paths}\label{sec:gaussian_diffusion}
\label{section:analysis}
In this section, we focus on analysing algorithms to simulate the diffusion path $(\mu_t)_{t\in [0,T]}$ defined in \eqref{eq:convolutional_path} when the base distribution $\nu$ is Gaussian, $\nu\sim \mathcal{N}(m_\nu, \sigma^2 I)$. For simplicity, we will assume that $\pid$ has mean $0$ and set $m_\nu=0$.  
This diffusion path has the remarkable property, illustrated in Figure~\ref{fig:convolutional_vs_geometric}, that when $\pid$ has finite log-Sobolev and Poincaré constants, these constants remain uniformly bounded along the entire path, as summarised in the following result.
\begin{proposition}\label{prop:bounded_log_sobolev} If $\pid$ has a finite log-Sobolev constant $C_{\text{LSI}}(\pid)$, respectively Poincaré constant $C_{\text{PI}}(\pid)$, the Gaussian diffusion path $(\mu_t)_{t \in [0,T]}$ defined in \eqref{eq:convolutional_path} with base distribution $\nu\sim\mathcal{N}(0, \sigma^2I)$ satisfies for all $t\in[0, T]$
\begin{align*}
    C_{\text{LSI}}(\mu_t)&\leq \lambda_t C_{\text{LSI}}(\pid) + (1-\lambda_t) C_{\text{LSI}}(\nu), \\
    C_{\text{PI}}(\mu_t)&\leq \lambda_t C_{\text{PI}}(\pid) + (1-\lambda_t) C_{\text{PI}}(\nu),
\end{align*}
respectively, where $C_{\text{LSI}}(\nu) = C_{\text{PI}}(\nu) = \sigma^{2}$.
\end{proposition}
The proof follows immediately from \citet[Propositions 2.3.3 and 2.3.7]{sinho_book}. This result is highly favourable, as, unlike geometric annealing \citep{chehab2024provableconvergencelimitationsgeometric}, the log-Sobolev and Poincaré constants remain uniformly bounded along the entire path by the worst constant independently of the distance between $\pid$ and $\nu$. 
We can visually observe this in Figure \ref{fig:convolutional_vs_geometric}, when the data distribution is given by a mixture of a Gaussian and a smoothed uniform distribution, $\pid = (1-e^{-m^2/4)}\mathcal{N}(m, 1) + e^ {-m^2/4} u_m$,
where $u_m$ is the smoothed uniform distribution on $I_m=[-m , 2m]$ for $m=10$ \citep{chehab2024provableconvergencelimitationsgeometric}.

Motivated by this, we analyse the diffusion annealed Langevin dynamics \eqref{eq:annealed_langevin_sde} to simulate from \eqref{eq:convolutional_path}. 

\subsection{Analysis of the Gaussian Diffusion Path}\label{sec:gaussian_diffusion_analysis}
We start by analysing the properties of the \textit{Gaussian diffusion path} $(\mu_t)_{t\in [0, T]}$. 
\paragraph{Smoothness of $(\mu_t)_t$.} We consider the following assumption on the Lipschitz continuity of the scores $\nabla\log\mu_t$.
\begin{assumption}\label{assumption:lipschitz_score_across_convolutional_diffusion}
    For all $t\in[0, T]$, the scores of the intermediate distributions $\nabla\log\mu_t(x)$ are Lipschitz with finite constant $L_t$.
\end{assumption}
\Cref{assumption:finite_second_order_moment} and \Cref{assumption:lipschitz_score_across_convolutional_diffusion} are sufficient for one of our non-asymptotic analyses of the Gaussian diffusion path (Theorem \ref{theorem:discretisation_analysis_convolutional_path}). 
However, \Cref{assumption:lipschitz_score_across_convolutional_diffusion} is generally difficult to verify. 
Therefore, we introduce two \textit{alternative} assumptions, \Cref{assumption:grad_log_lipschitzness_convexity_outside_of_a_ball} and \Cref{assumption:grad_log_lipschitzness_hessian_decay}, which separately ensure that $(\nabla \log\mu_t)_{t\in[0, T]}$ satisfies assumption \Cref{assumption:lipschitz_score_across_convolutional_diffusion}. In particular, we show that \Cref{assumption:grad_log_lipschitzness_convexity_outside_of_a_ball} is satisfied by a mixture of Gaussians with different covariances, given certain conditions on the covariances. While assumption \Cref{assumption:grad_log_lipschitzness_hessian_decay} is shown to hold for heavy-tailed data distributions. 
\begin{assumption}\label{assumption:grad_log_lipschitzness_convexity_outside_of_a_ball}
     The data distribution $\pi_{\text{data}}$ has density with respect to Lebesgue, which we write  $\pi_{\text{data}} \propto e^{-V_\pi}$. The potential $V_\pi$ has Lipschitz continuous gradients, with Lipschitz constant $L_\pi$. In addition, $V_\pi$ is strongly convex outside of a ball of radius $r$ with convexity parameter $M_\pi>0$, that is,
     \begin{equation*}
         \inf_{\Vert x\Vert \geq r} \nabla^2 V_\pi \succcurlyeq M_{\pi} I, \quad \inf_{\Vert x\Vert < r} \nabla^2 V_\pi \succcurlyeq -L_{\pi} I.
     \end{equation*}
\end{assumption}

In Lemma \ref{lemma:implications_between_assumptions} of the Appendix, we demonstrate that assumption \Cref{assumption:grad_log_lipschitzness_convexity_outside_of_a_ball} extends the standard assumption on the data distribution that $\pid$ is modelled as a convolution of a compactly supported distribution $\Tilde{\pi}$ and a Gaussian, see, e.g., \citet[Theorem~1]{saremi2024chain} or \citet[Assumption~0]{grenioux2024stochastic}, under some conditions on the compact support of $\Tilde{\pi}$.
Additionally, we prove that a mixture of Gaussians with different covariances satisfies assumption \Cref{assumption:grad_log_lipschitzness_convexity_outside_of_a_ball} under some mild conditions on the covariances (see Lemma \ref{lemma:example_mixture_gaussians_satisfies_assumption} and Remark \ref{remark:counter_example_mixture_gaussian} for a further discussion).
However, Lemma  \ref{lemma:d_mixture_gaussians_not_expressed_as_convoltuion_with_compactly_supported} shows that, in general, a mixture of Gaussians cannot be expressed as a convolution of a compactly supported measure with a Gaussian.
We consider the results regarding the mixture of Gaussians to hold independent significance, as we could not find explicit results in the literature addressing the smoothness properties in this case.

Leveraging the existence of a smooth strongly convex approximation of $V_\pi$ \citep{doi:10.1073/pnas.1820003116} and the Holley-Stroock perturbation lemma \citep{RefWorks:RefID:85-holley1987logarithmic}, we show that under \Cref{assumption:grad_log_lipschitzness_convexity_outside_of_a_ball}, $\pi_{\text{data}}$  satisfies a log-Sobolev inequality with a finite constant which itself implies a finite Poincaré constant (Lemma \ref{lemma:assumption_implies_LSI}) -- which is sufficient for Proposition~\ref{prop:bounded_log_sobolev} to hold. As a consequence, \Cref{assumption:grad_log_lipschitzness_convexity_outside_of_a_ball} implies that the data distribution $\pid$ has finite second order moment (i.e. \Cref{assumption:grad_log_lipschitzness_convexity_outside_of_a_ball} $\Rightarrow$ \Cref{assumption:finite_second_order_moment}).

On the other hand, heavy-tailed data distributions, such as Student's $t$-like distributions, do not satisfy assumption \Cref{assumption:grad_log_lipschitzness_convexity_outside_of_a_ball}, since their potential $V_\pi$ is not strongly convex outside of a ball. Specifically, the Hessian of the potential tends to zero as $\Vert x\Vert$ tends to infinity. 
We provide the following alternative assumption for heavy-tailed data distributions.
\begin{assumption}\label{assumption:grad_log_lipschitzness_hessian_decay}
     The data distribution $\pid$ has density with respect to Lebesgue, which we write  $\pi\propto e^{-V_\pi}$. The potential $V_\pi$ has Lipschitz continuous gradients, with Lipschitz constant $L_\pi$. In addition, $\nabla^2 V_\pi(x)$ decays to 0 with order $\mathcal{O}(\Vert x\Vert^{-2}I)$ as $\Vert x\Vert$ tends to $\infty$. That is, outside of a ball of radius $r$ we have that 
     \begin{equation*}
        - \frac{I}{\alpha_1 +\alpha_2 \Vert x\Vert^{2}} \preccurlyeq \nabla^2 V_\pi(x) \preccurlyeq \frac{I}{\beta_1 +\beta_2 \Vert x\Vert^{2}} \; \; \Vert x\Vert > r, 
     \end{equation*}
     where $\alpha_1, \alpha_2,\beta_1,\beta_2\in\mathbb{R}$.
\end{assumption}
In Appendix \ref{appendix:comments_assumption_hessian_decay} we show that multivariate Student's $t$ distributions of the form
\begin{equation*}
    \pid(x) \propto \left( 1 + \frac{1}{\alpha} (x-\mu)^{\intercal} \Sigma^{-1} (x-\mu)\right)^{-( \alpha + d)/2}
\end{equation*}
satisfy assumption \Cref{assumption:grad_log_lipschitzness_hessian_decay}.

The following Lemma establishes that assumption \Cref{assumption:lipschitz_score_across_convolutional_diffusion} holds when the data distribution satisfies either \Cref{assumption:grad_log_lipschitzness_convexity_outside_of_a_ball} or \Cref{assumption:grad_log_lipschitzness_hessian_decay}.

\begin{lemma}\label{lem:regularity_of_gaussian_path} Under \Cref{assumption:grad_log_lipschitzness_convexity_outside_of_a_ball} or \Cref{assumption:grad_log_lipschitzness_hessian_decay}, we have that for all $t\in[0,T]$ the score $\nabla \log \mu_t(x)$ is Lipschitz continuous with constant $L_t$ provided in the proof (i.e. \Cref{assumption:grad_log_lipschitzness_convexity_outside_of_a_ball} $\Rightarrow$ \Cref{assumption:lipschitz_score_across_convolutional_diffusion} and \Cref{assumption:grad_log_lipschitzness_hessian_decay} $\Rightarrow$ \Cref{assumption:lipschitz_score_across_convolutional_diffusion}).
\end{lemma}
An important element in the proof, given in Appendix \ref{proof:lem:smoothness_of_gaussian_path}, is the generalisation of the Poincaré inequality for vector-valued functions, which is presented in Lemma \ref{lemma:PI_for_vector_valued_functions}. 
Notably, these bounds improve those in \citet[Proposition 20]{gao2024gaussian} under the specified conditions.

It is important to emphasise that a significant number of works in the diffusion models literature, e.g. \citet{lee2022convergence, Chen2022ImprovedAO}; \citet{chen2023sampling}, assume that $\nabla\log\mu_t$ is Lipschitz for all $t$, with the Lipschitz constant bounded over time. 
In contrast, we have demonstrated that this condition arises naturally under assumptions \Cref{assumption:grad_log_lipschitzness_convexity_outside_of_a_ball} or \Cref{assumption:grad_log_lipschitzness_hessian_decay} on the target distribution.


\paragraph{Action of $(\mu_t)_{t}$.}  
To derive a bound on the action necessary for the convergence analysis, we make the following assumption on the schedule. 
\begin{assumption}\label{assumption:schedule_form}
    Let $\lambda_t:\mathbb{R}^+\to[0,1]$ be non-decreasing in $t$ and weakly differentiable, such that there exists a constant $C_\lambda$ satisfying either of the following conditions
    \begin{equation*}
        \max_{t\in[0,T]}\vert \partial_t{\log \lambda_t}\vert \leq C_\lambda
    \end{equation*}
    or
  \begin{equation*}
        \max_{t\in[0,T]}\left\vert \frac{\partial_t{\lambda_t}}{\sqrt{\lambda_t(1-\lambda_t)}}\right\vert \leq C_\lambda.
    \end{equation*}
\end{assumption}
Notably schedules of the form $\lambda_t = 0.5(1+\cos(\pi(1-(t/T)^\phi)))$, $0.5(1+\tanh(\phi(t/T-0.5)))$ with $\phi\in\mathbb{R}^+$, sigmoid-type schedules, or the schedule corresponding to the \gls*{OU} process $e^{-2(T-t)}$, among others, satisfy the previous assumption. 
When $\lambda_0=0$, the first condition in \Cref{assumption:schedule_form} requires that the derivative of the schedule at $t=0$ is close to zero, meaning that the schedule grows very slowly at the beginning. 
This intuitively captures the importance of the initial stages in the Langevin diffusion generation process. For instance, when the data distribution consists of two distant modes, the diffusion needs to allocate the correct proportion of mass to each mode. During the early stages, as the mass separates towards each mode, employing a slower-increasing schedule can aid in this process. As the mass approaches each mode, the probability of it jumping between modes decreases rapidly, making a slow initial increase essential for effective separation.
The second condition ensures that the schedule also becomes flat as it approaches $\lambda_T=1$. 
This promotes a more refined and detailed generation process, enabling the model to converge more precisely to the data distribution.

Under assumption \Cref{assumption:schedule_form} on the schedule, we derive the following bound on the action of $\mu=(\mu_t)_{t\in[0, T]}$.
\begin{lemma}\label{lemma:action_bound} 
If $\pid$ and $\lambda_t$ satisfy assumptions \Cref{assumption:finite_second_order_moment} and \Cref{assumption:schedule_form}, respectively, 
the action for the Gaussian diffusion path $\mathcal{A}_\lambda(\mu)$ can be upper bounded by
\begin{equation*}
    \mathcal{A}_\lambda(\mu)\lesssim C_\lambda \left(\mathbb{E}_{\pid}\left[\Vert X\Vert^2\right] +  d\right)\lesssim M_{2} \vee d.
\end{equation*}
\end{lemma}
The proof is given in Appendix \ref{proof:lemma:bound_action}. It is worth highlighting that unlike for the geometric path \citep{guo2024provablebenefitannealedlangevin}, for the diffusion path we get an explicit bound on the action under a mild assumption on the schedule. Furthermore, we observe in the proof that selecting the mean and variance of the base distribution $\nu$ close to that of the target results in a tighter bound for the action.


\subsection{Analysis of the Gaussian \gls*{DALMC} Algorithm}
 We now analyse the convergence of the \gls*{DALMC} algorithm \eqref{eq:annealed_langevin_mcmc_algorithm_score_approx} for a Gaussian base distribution.


\begin{theorem}\label{theorem:discretisation_analysis_convolutional_path}
Under \Cref{assumption:finite_second_order_moment}, \Cref{assumption:lipschitz_score_across_convolutional_diffusion} and \Cref{assumption:schedule_form}, the \gls*{DALMC} algorithm \eqref{eq:annealed_langevin_mcmc_algorithm_score_approx} initialised at $X_0\sim\hat{\mu}_0$ and with an approximate score which satisfies \Cref{assumption:score_approximation}, yields the following bound  
\begin{align*}
    \kl\left(\mathbb{P}\;||\mathbb{Q}_\theta\right)
    \lesssim &\left(1+\frac{L_{\max}^2}{M^2\kappa^4}\right) \kappa  \left(\mathbb{E}_{\pid}\left[\Vert X\Vert^2\right] +  d\right)\\
    &+ \frac{d}{M\kappa^2}\left(1+  \frac{L_{\max}}{M\kappa}\right)\int_{0}^{T}L_{ t}^2 \ \md t +  \varepsilon_{\score}^2,
\end{align*}
where $\mathbb{Q}_\theta= (q_{\theta, \lambda_t})_{t\in[0,T]}$ is the path measure of the continuous-time interpolation of  \eqref{eq:annealed_langevin_mcmc_algorithm_score_approx}, $\mathbb{P}$ is that of a reference \gls*{SDE} such that the marginals at each time t have distribution $\hat{\mu}_t$, $M$ denotes the number of steps, $T/\kappa = \sum_{l=1}^M h_l$ and $L_t$ is the Lipschitz constant of $\nabla\log \mu_t$, $L_{\max} = \max_{[0,T]} L_t$.
\end{theorem}
The proof, included in Appendix \ref{proof:theorem:discretisation_path_analysis}, mainly relies on an application of Girsanov's theorem, the bound on the action and the Lipschitzness of $\nabla\log\mu_t$. 
Additionally, we note in the proof that, smaller step sizes $h_l$ are preferred when the Lipschitz constant $L_t$ is larger to obtain a tighter bound.

This result allows us to provide a bound on the iteration complexity of the \gls*{DALMC} algorithm.
\begin{corollary}\label{corollary:complexity_bounds}
For $T\geq 1$, $\kappa<1$ and $M$, there always exists a sequence of step sizes $h_k= T_{k}-T_{k-1}$ such that $\sum_{k=1}^{M} h_k = T/\kappa$. Then, if we take $\kappa = \mathcal{O}\left(\frac{\varepsilon_{\score}^2}{M_2 \vee d}\right)$ and $M = \mathcal{O}\left(\frac{d (M_2 \vee d)^2L_{\max}^2}{\varepsilon_{\score}^6}\right)$, we have $\kl\left(\mathbb{P}\;||\mathbb{Q}_\theta\right) = \mathcal{O}(\varepsilon_{\emph{\text{score}}}^2)$. Hence, for any $\varepsilon = \mathcal{O}(\varepsilon_{\score})$, and under assumptions \Cref{assumption:finite_second_order_moment}, \Cref{assumption:lipschitz_score_across_convolutional_diffusion} and \Cref{assumption:schedule_form}, the \gls*{DALMC} algorithm \eqref{eq:annealed_langevin_mcmc_algorithm_score_approx} initialised at $X_0\sim\hat{\mu}_0$  requires at most $\mathcal{O}\left(\frac{d (M_2 \vee d)^2L_{\max}^2}{\varepsilon^6}\right)$ steps to approximate $\pid$ to within $\varepsilon^2$ \gls*{KL} divergence, that is $\kl(\pid\Vert q_{\theta, \lambda_T})\leq \varepsilon^2$, assuming a sufficiently accurate score estimator.
\end{corollary} 

It is important to note that this bounds are less favourable than those of diffusion models \citep{Chen2022ImprovedAO, chen2023sampling, benton2024nearly}, which explains the success of these models compared to diffusion annealed Langevin-based algorithms. This difference mainly arises because the Langevin \gls*{SDE} implementation \eqref{eq:annealed_langevin_sde} introduces an implicit bias, whereas the reverse \gls*{SDE} in diffusion models ensures that the law of the solution of the \gls*{SDE} exactly matches the intermediate marginal distributions.
Additionally, the use of an exponential integrator scheme in diffusion models, benefiting from the linear term in the drift of the reverse \gls*{SDE}, contrasts with the Euler-Maruyama discretisation used here, leading to an improvement in the discretisation error. 

\subsection{Analysis under Relaxed Assumptions}
In this section, we introduce a less restrictive assumption for the data distribution that generalises \Cref{assumption:grad_log_lipschitzness_convexity_outside_of_a_ball} and \Cref{assumption:grad_log_lipschitzness_hessian_decay}. Under this assumption, we derive an error bound for the \gls*{DALMC} algorithm without relying on the smoothness of $\log\mu_t$ along the diffusion path, in contrast to the proof of Theorem~\ref{theorem:discretisation_analysis_convolutional_path}.

\begin{assumption}\label{assumption:pi_data_conditions_less_restrictive}
    The data distribution $\pid$ has density with respect to Lebesgue, which we write $\pid\propto e^{-V_\pi}$, and a finite second order moment. The potential $V_\pi$ has Lipschitz continuous gradient, with Lipschitz constant $L_\pi$, and 
    \begin{equation*}
        \mathbb{E}_{\pid} \left\Vert\nabla V_\pi\left(X\right)\right\Vert^{8} \leq K_\pi^2.
    \end{equation*} 
\end{assumption}
In Appendix~\ref{appendix:comments_new_less_restrictiv_assumption}, we show that both \Cref{assumption:grad_log_lipschitzness_convexity_outside_of_a_ball} and \Cref{assumption:grad_log_lipschitzness_hessian_decay} (with finite second-order moment) imply assumption \Cref{assumption:pi_data_conditions_less_restrictive}. 
Besides, since under \Cref{assumption:pi_data_conditions_less_restrictive} the data distribution has a finite second-order moment, if the schedule also satisfies  \Cref{assumption:schedule_form}, then the bound on the action established in Lemma~\ref{lemma:action_bound} remains valid. 
This enables us to obtain the following complexity guarantees for the \gls*{DALMC} algorithm under this new assumption.
\begin{theorem}\label{theorem:convergence_relaxed_assumption}
    Under \Cref{assumption:schedule_form} and \Cref{assumption:pi_data_conditions_less_restrictive}, the \gls*{DALMC} algorithm \eqref{eq:annealed_langevin_mcmc_algorithm_score_approx} initialised at $X_0\sim\hat{\mu}_0$  and with an approximate score which satisfies \Cref{assumption:score_approximation} leads to 
\begin{align*}
    \kl\left(\mathbb{P}\;||\mathbb{Q}_\theta\right)\lesssim&\frac{dL_\pi}{M^2\kappa^3} + \frac{(d^2\vee L_\pi^2d \vee K_\pi)}{M\kappa^2} \\
    &+ \kappa (\mathbb{E}_{\pid}[\Vert X\Vert^2] + d) + \varepsilon_{\text{score}}^2,
\end{align*}
where $\mathbb{Q}_\theta= (q_{\theta, \lambda_t})_{t\in[0,T]}$ is the path measure corresponding to the continuous-time interpolation of algorithm \eqref{eq:annealed_langevin_mcmc_algorithm_score_approx}, $\mathbb{P}$ is that of a reference \gls*{SDE} such that the marginals at each time t have distribution $\hat{\mu}_t$ and $M$ denotes the number of steps. Therefore, under assumptions  \Cref{assumption:schedule_form} and \Cref{assumption:pi_data_conditions_less_restrictive}, the \gls*{DALMC} algorithm \eqref{eq:annealed_langevin_mcmc_algorithm_score_approx} initialised at $X_0\sim\hat{\mu}_0$ with approximate scores requires at most $\mathcal{O}\left(\frac{(M_2\vee d)^2(d^2\vee L_\pi^2 d\vee K_\pi)L_\pi}{\varepsilon^6}\right)$ steps to approximate $\pid$ to within $\varepsilon^2$ $\kl$ divergence, that is $\kl(\pid\Vert q_{\theta, \lambda_T})\leq \varepsilon^2$, assuming a sufficiently accurate score estimator, i.e. $\varepsilon_{\score} = \mathcal{O}(\varepsilon)$. If $M_2= \mathcal{O}(d)$, $L_\pi= \mathcal{O}(\sqrt{d})$ and $K_\pi= \mathcal{O}(d^2)$, then $M = \mathcal{O}\left(\frac{d^4L_\pi}{\varepsilon^6}\right)$.
\end{theorem}
See Appendix~\ref{appendix:convergence_relaxed_assumption} for the proof.
Relaxing the assumptions results in a dimensional dependence on the number of steps that is one order worse compared to Corollary \ref{corollary:complexity_bounds}.

\section{Heavy-Tailed Diffusion Paths} \label{sec:heavy_tailed_diffusion}
We now analyse the annealed Langevin diffusion path \eqref{eq:annealed_langevin_sde} when the base  distribution $\nu\in\mathcal{P}(\mathbb{R}^d)$ is a Student's $t$-distribution, $\nu\sim t(0, \sigma^2I, \alpha)$, with tail index $\alpha>2$
\begin{equation*}
    \nu(x) \propto \left( 1 + \frac{\Vert x\Vert^2}{\alpha\sigma^2}\right)^{-( \alpha + d)/2}.
\end{equation*}
It is worth noting that the $t$-distribution is not a stable distribution, unlike the Gaussian family, meaning that the convolution of two $t$-distributions is not necessarily a $t$-distribution. \citet{NADARAJAH2005715} provides explicit expressions for the density function of the convolution of one dimensional $t$-distributions with unit variance, but only when both degrees of freedom are odd. Closed-form expressions cannot be derived in one dimension when either of the two degrees of freedom  is even. In the $d$-dimensional case, only closed forms can be derived when $\alpha + d$ is even.

\subsection{Analysis of the Heavy-Tailed Diffusion Path}
\paragraph{Smoothness of $(\mu_t)_t$.} We require for the discretisation analysis that the intermediate distributions of the heavy-tailed diffusion path satisfy smoothness conditions given in \Cref{assumption:lipschitz_score_across_convolutional_diffusion}. 
We show below that this assumption holds when the data distribution $\pid$ satisfies the following conditions.
\begin{assumption}\label{assumption:heavy_tailed_target_condition}
The data distribution $\pid$ has density with respect to the Lebesgue measure. $\nabla\log\pid$ is Lipschitz continuous with constant $L_\pi$ and $\Vert\nabla\log\pid\Vert^2\leq C_\pi$ almost surely.
\end{assumption}

In particular, Lemma~\ref{lem:regularity_of_heavy_tailed_path} below demonstrates that this assumption holds when the data distribution $\pid$ can be expressed as the convolution of a compactly supported measure and a $t$-distribution. 
\begin{assumption}\label{assumption:compact_plus_t_distribution_target}
     Let $ X$ be a $d$-dimensional random vector $X \sim\pid$, such that $X = U + G$, where $\Vert U - m_{\pi}\Vert^2 \leq  d R^2$ holds almost surely, and $G \sim t(0, \tau^2 I, \Tilde{\alpha})$ is independent of $U$. 
\end{assumption}
Lemma~\ref{lemma:wegihted_pi} in the Appendix shows that if $\pid$ satisfies assumption \Cref{assumption:compact_plus_t_distribution_target}, then it has a finite weighted Poincaré constant.
This extends the result of \citet{functional_inequalities_compactly_supported_assumption} to convolutions of compactly supported measures with $t$-distributions.
However, unlike the multivariate Gaussian case, our bound on the weighted Poincaré constant is not dimension-free.

The following result shows that \Cref{assumption:heavy_tailed_target_condition} $\Rightarrow$ \Cref{assumption:lipschitz_score_across_convolutional_diffusion} and \Cref{assumption:compact_plus_t_distribution_target} $\Rightarrow$ \Cref{assumption:heavy_tailed_target_condition}.
\begin{lemma}\label{lem:regularity_of_heavy_tailed_path} 
Under \Cref{assumption:heavy_tailed_target_condition} and taking the base distribution $\nu\sim t(0, \sigma^2 I, \alpha)$, we have that for all $t\in[0,T]$ $\nabla \log \mu_t(x)$ is Lipschitz (\Cref{assumption:lipschitz_score_across_convolutional_diffusion}) with constant $L_t$ provided in the proof. Besides, \Cref{assumption:heavy_tailed_target_condition} holds when $\pid$ is a convolution of a compactly supported measure and a multivariate $t$ distribution (\Cref{assumption:compact_plus_t_distribution_target}).
\end{lemma}
The proof is provided in Appendix~\ref{proof:lem:smoothness_of_heavy_tail_path}.
\paragraph{Action of $(\mu_t)_{t}$.} To derive a bound on the action, necessary for the discretisation analysis, we introduce an assumption on the schedule similar to that of \Cref{assumption:schedule_form}.
\begin{assumption}\label{assumption:schedule_form_heavy_tail_diffusion}
    Let $\lambda_t:\mathbb{R}^+\to[0,1]$ be non-decreasing in $t$ and weakly differentiable, such that there exists a constant $C_\lambda$ satisfying
    \begin{equation*}
        \max_{t\in[0,T]}\left\vert \frac{\partial_t{\lambda_t}}{\sqrt{\lambda_t(1-\lambda_t)}}\right\vert \leq C_\lambda.
    \end{equation*}
\end{assumption}
Intuitively, schedules with derivatives close to $0$ as $t$ approaches $0$ and $T$ satisfy the previous assumption.
In particular, schedules of the form $\lambda_t = 0.5(1+\cos(\pi(1-(t/T)^\phi)))$, $0.5 ( 1 + \tanh(\phi(t-0.5)))$, where $\phi\in\mathbb{R}^+$, fulfil \Cref{assumption:schedule_form_heavy_tail_diffusion}. Under the previous assumption, we compute the following bound on the action. The proof is provided in Appendix \ref{proof:lem:action_bound_heavy_tail_diffusion}. 

\begin{lemma}\label{lemma:action_bound_heavy_tail_diffusion}  
If $\pid$ has finite second-order moment (\Cref{assumption:finite_second_order_moment}), $\nu\sim t(0, \sigma^2 I, \alpha)$ with tail index $\alpha>2$ and $\lambda_t$ satisfies \Cref{assumption:schedule_form_heavy_tail_diffusion}, 
the action for the heavy-tailed diffusion path $\mathcal{A}_\lambda(\mu)$ can be effectively upper bounded as follows
\begin{equation*}
    \mathcal{A}_\lambda(\mu)\leq \frac{C_\lambda\pi}{8} \left(\mathbb{E}_{\pid}\left[\Vert X\Vert^2\right] + \frac{\sigma^2 d\alpha}{\alpha-2}\right).
\end{equation*}
\end{lemma}

\subsection{Analysis of the Heavy-Tailed \gls*{DALMC} Algorithm} 
The following theorem establishes a bound for the discretisation error of the heavy-tailed \gls*{DALMC} algorithm with an approximated score. The proof is given in Appendix \ref{proof:theorem:discretisation_path_analysis_heavy_tail_diffusion}. 
\begin{theorem}\label{theorem:discretisation_analysis_convolutional_heavy_tailed_diffusion}
Assume the data distribution $\pid$ satisfies assumption  \Cref{assumption:finite_second_order_moment} (finite second-order moment) and \Cref{assumption:lipschitz_score_across_convolutional_diffusion} (which holds under \Cref{assumption:heavy_tailed_target_condition}),
$\nu\sim t(0, \sigma^2 I, \alpha)$ with $\alpha>2$ and
let the schedule satisfy \Cref{assumption:schedule_form_heavy_tail_diffusion}, with $\lambda_{\kappa t}/\lambda_{\kappa (t + \delta)} = \mathcal{O}(1+ \delta)$, $\delta<<1$. 
Then, the heavy-tailed \gls*{DALMC} algorithm with an approximated score satisfying \Cref{assumption:score_approximation} and initialised at $X_0\sim\hat{\mu}_0$, guarantees that 
\small
\begin{align*}
    \kl&\left(\mathbb{P}||\mathbb{Q}_\theta\right)
    \lesssim \left(1+\frac{L_{\max}^2}{M^2\kappa^2}+\frac{1}{M^2\kappa^4}\right) \kappa \left(\mathbb{E}_{\pid}\left[\Vert X\Vert^2\right] + d\right) \\
    &+ \frac{d}{M\kappa^2}\left(1 + \frac{\alpha}{\alpha -2}+  \frac{L_{\max}}{M\kappa}\right)\int_{0}^{T}L_{ t}^2 \ \md t + \varepsilon_{\score}^2,
\end{align*}
\normalsize
where $\mathbb{Q}_\theta = (q_{\theta, \lambda_t})_{t\in[0,T]}$ is the path measure of the continuous-time interpolation of \eqref{eq:annealed_langevin_mcmc_algorithm_score_approx}, $\mathbb{P}$ is that of a reference \gls*{SDE} such that the marginals at each time t have distribution $\hat{\mu}_t$, $M$ denotes the number of steps, $T/\kappa = \sum_{l=1}^M h_l$ and $L_t$ is the Lipschitz constant of $\nabla\log \mu_t$, $L_{\max} = \max_{[0,T]} L_t$. Therefore, under  \Cref{assumption:finite_second_order_moment}, \Cref{assumption:lipschitz_score_across_convolutional_diffusion} and \Cref{assumption:schedule_form_heavy_tail_diffusion}, the heavy-tailed \gls*{DALMC} algorithm \eqref{eq:annealed_langevin_mcmc_algorithm_score_approx} initialised at $X_0\sim\hat{\mu}_0$ requires at most $\mathcal{O}\left(\frac{d (M_2 \vee d)^2 L_{\max}^2}{\varepsilon^6}\right)$ steps to approximate $\pid$ to within $\varepsilon^2$ \gls*{KL} divergence, that is $\kl(\pid\Vert q_{\theta, \lambda_T})\leq \varepsilon^2$, assuming a sufficiently accurate score estimator, i.e. $\varepsilon_{\score} = \mathcal{O}(\varepsilon)$.
\end{theorem}
Note that as the tail index $\alpha$ tends to $\infty$, which corresponds to $\nu$ approaching a Gaussian distribution, the bound on $\kl\left(\mathbb{P} || \mathbb{Q}_\theta\right)$ recovers that of Theorem \ref{theorem:discretisation_analysis_convolutional_path}. Furthermore, since $\alpha/(\alpha-2)\leq 3$ for $\alpha>2$, the iteration complexity of the heavy-tailed \gls*{DALMC} algorithm is identical to that of the Gaussian \gls*{DALMC} algorithm corresponding to the Gaussian diffusion path.

\section{Related Work}\label{sec:related_work}


\paragraph{Score-based generative models.
}
Our approach is similar to earlier generative modelling techniques based on annealed Langevin dynamics \citep{song2019generative}, which inspired the advancement of diffusion models.
The existing literature analysing these Langevin Monte Carlo algorithms is limited.
\citet{block2022generativemodelingdenoisingautoencoders} derive an error bound in Wasserstein distance that scales exponentially with the data dimension, while \citet{lee2022convergence} establish a non-asymptotic bound in total variation, which is weaker than our bound in \gls*{KL}, as implied by Pinsker's inequality.

On the other hand, the convergence of diffusion models \citep{song2020score, ho2020denoising} has been extensively studied. 
Early results either established non quantitative bounds \citep{pidstrigach2022scorebased}, relied on restrictive assumptions about the data distribution, such as functional inequalities \citep{lee2022convergence}, or exhibited exponential dependence on the problem parameters \citep{ de_bortoli2022convergence}. 
Recent works have established polynomial convergence bounds under more relaxed assumptions \citep{Chen2022ImprovedAO, chen2023sampling, benton2024nearly, li2024towards}. In particular, \citet{Chen2022ImprovedAO} introduce two bounds on the $\kl$ error: a linear bound in the data dimension under smoothness conditions along the entire diffusion path, and a second one which scales quadratically with $d$, achieved through early stopping and the assumption of a finite second-order moment on $\pid$. In contrast, \citet{benton2024nearly} provide a bound that is linear in the data dimension, up to logarithmic factors, assuming only that the data distribution has a finite second-order moment. Their proof exploits the specific structure of the \gls*{OU} process to control the error arising from discretising the reverse \gls*{SDE}.


\paragraph{Stochastic interpolants.}

Stochastic interpolants \citep{albergo2023stochastic} are generative models that unify flow-based and diffusion-based methods. These models make use of a broad class of continuous-time stochastic processes designed to bridge any two arbitrary probability density functions exactly in finite time, akin to our work. 
Specifically, the formulation of linear one-sided stochastic interpolants \citep{albergo2023stochastic, gao2024gaussian}, which interpolate between a Gaussian and the data distribution, is equivalent to the Gaussian diffusion path \eqref{eq:convolutional_path}. 
Unlike our approach, they incorporate intractable control terms into the drift of the \gls*{SDE} to ensure the marginals have the desired distributions. This may result in  numerical instabilities caused by singularities in the drift at $t= T$ \citep[Section 6]{albergo2023stochastic}. In contrast, we implement the diffusion path using Langevin dynamics. Furthermore, their theoretical analysis does not include explicit non-asymptotic convergence bounds.

\paragraph{Tempering.}
Tempering \citep{PhysRevLett.57.2607, geyer_mcmc_92, Marinari_1992} is a well-known technique in the sampling literature that involves sampling the system at multiple temperatures: starting with higher temperatures to facilitate transitions between modes, gradually cooling the system to focus on the local structure of the target distribution. 
The sequence of tempered target distributions is typically defined using the geometric path, as it can be computed in closed form when the target density is known up to a normalising constant.
Recently, several works have established theoretical guarantees for the convergence of geometric annealed Langevin Monte Carlo for non-log-concave distributions. In particular, \citet{guo2024provablebenefitannealedlangevin} provides a bound on the $\kl$  similar to that of Theorem~\ref{theorem:discretisation_analysis_convolutional_path}. However, they are unable to obtain a closed-form expression for the action of the path. 
Besides, \citet{chehab2024provableconvergencelimitationsgeometric} derive upper and lower convergence bounds for the $\kl$ of the marginals, based on functional inequalities assumptions. 
In particular, they demonstrate that in some cases the log-Sobolev constant of the intermediate distributions along the path can deteriorate exponentially compared to those of the base and data distributions, unlike for the diffusion path.


\section{Conclusions} 
\label{sec:conclusions}

In this work we provided a rigorous non-asymptotic analysis of Diffusion Annealed Langevin Monte Carlo (\gls*{DALMC}) for generative modelling, focusing on both Gaussian and heavy-tailed diffusion paths. By examining general diffusion paths that interpolate between complex data distributions and simpler base distributions, we have obtained theoretical insights into the convergence behaviour of \gls*{DALMC} under a range of assumptions.
For Gaussian diffusion paths, we derived explicit non-asymptotic path-wise error bounds in KL divergence, improving upon prior results by relaxing smoothness assumptions and addressing the bias introduced through discretisation. Extending the framework to heavy-tailed diffusion paths, such as those based on Student’s $t$-distributions, we presented the first theoretical guarantees for these models, demonstrating comparable complexity to Gaussian diffusion paths under mild conditions.

Our analysis highlighted how smoothness assumptions, such as Lipschitz continuity of the scores and properties of the data distribution (e.g., bounded second-order moment and convexity or heavy-tailed behaviour), naturally ensure bounded action and efficient convergence. This generalisation broadens the applicability of \gls*{DALMC} beyond the  settings considered in prior work. While \gls*{DALMC} introduces some bias compared to reverse \gls*{SDE} implementations, it avoids numerical instabilities and provides a simpler approach, making it a compelling alternative for score-based generative modelling, in certain settings.

Looking ahead, further work could focus on developing more efficient numerical schemes, reducing dimensional dependencies in error bounds, and applying this framework to other generative models.

\newpage
\section*{Acknowledgments}
PCE would like to thank Arnaud Guillin, Paul Felix Valsecchi Oliva, Pierre Monmarché and Yanbo Tang for their insightful discussions.
PCE is supported by EPSRC through the Modern Statistics and Statistical Machine Learning (StatML) CDT programme, grant no. EP/S023151/1.

\bibliography{refs}
\bibliographystyle{plainnat}



% APPENDIX

\newpage
\appendix
\onecolumn
\section{Background}\label{appendix:background}
We introduce some concepts from optimal transport and the Girsanov theorem which will be useful for the subsequent analysis.

\paragraph{Optimal transport.}
Let $v=(v_t:\mathbb{R}^d\to\mathbb{R}^d)$ be a vector field and $\mu=(\mu_t)_{t\in[a,b]}$ be a curve of probability measures on $\mathbb{R}^d$ with finite second-order moments. $\mu$ is generated by the vector field $v$ if the continuity equation
\begin{equation*}
    \partial_t\mu_t + \nabla\cdot(\mu_tv_t)=0,
\end{equation*}
holds for all $t\in[a,b]$. The metric derivative of $\mu$ at $t\in[a,b]$ is then defined as
\begin{equation*}
    \left\vert\Dot{\mu}\right\vert_t:=\lim_{\delta\to0}\frac{W_2(\mu_{t+\delta}, \mu_t)}{\left\vert\delta\right\vert}.
\end{equation*}
If $\left\vert\Dot{\mu}\right\vert_t$ exists and is finite for all $t\in[a,b]$, we say that $\mu$ is an absolutely continuous curve of probability measures. 
\citet{ambrosio2000rectifiable} establish weak conditions under which a curve of probability measures with finite second-order moments is absolutely continuous. 

By \citet[Theorem 8.3.1]{gradients_flows_book} we have that among all velocity fields $v_t$ which produce the same flow $\mu$, there is a unique optimal one with smallest $L^p(\mu_t; X)$-norm. This is summarised in the following lemma.
\begin{lemma}[Lemma 2 from \citet{guo2024provablebenefitannealedlangevin}]\label{lemma:optimal_vector_field}
    For an absolutely continuous curve of probability measures $\mu =(\mu_t)_{t\in[a,b]}$, any vector field $(v_t)_{t\in[a,b]}$ that generates $\mu$ satisfies $\left\vert\Dot{\mu}\right\vert_t\leq\Vert v_t\Vert_{L^2(\mu_t)}$ for almost every $t\in[a, b]$. Moreover, there exists a unique vector field $v_t^\star$ generating $\mu$ such that $\left\vert\Dot{\mu}\right\vert_t = \Vert v_t^\star\Vert_{L^2(\mu_t)}$ almost everywhere.
\end{lemma}
We also introduce the action of the absolutely continuous curve $(\mu_t)_{t\in[a,b]}$ since it will play a key role in our convergence results. In particular, we define the action $\mathcal{A}(\mu)$ as
\begin{equation*}
  \mathcal{A}(\mu):=\int_a^b\left\vert\Dot{\mu}\right\vert_t^2  \md t.  
\end{equation*}

\paragraph{Girsanov's theorem.} Consider the \gls*{SDE}
\begin{equation*}
    \md X_t = b(X_t, t)\md t + \sigma(X_t,t)\md B_t,
\end{equation*}
for $t\in[0,T]$, where $(B_t)_{t\in[0,T]}$ is a standard Brownian motion in $\mathbb{R}^d$. Denote by $\mathbb{P}^X$ the \emph{path measure} of the solution $X = (X_t)_{t\in[0,T]}$ of the \gls*{SDE}, which characterises the distribution of $X$ over the sample space $\Omega$. 

The $\kl$ divergence between two path measures can be characterised as a consequence of Girsanov's theorem \citep{karatzas}. In particular, the following result will be central in our analysis.
\begin{lemma}\label{lemma:girsanov_theorem}
    Consider the following two \glspl*{SDE} defined on a common probability space $(\Omega, \mathcal{F}, \mathbb{P})$
    \begin{equation*}
        \md X_t = a_t(X)\md t+ \sqrt{2}\md B_t,\quad\quad \md Y_t = b_t(Y)\md t+ \sqrt{2}\md B_t, \quad\quad t\in[0, T]
    \end{equation*}
with the same initial conditions $X_0, Y_0\sim \mu_0$. Denote by $\mathbb{P}^X$ and $\mathbb{P}^Y$ the path measures of the processes $X$ and $Y$, respectively. It follows that 
\begin{equation*}
    \kl (\mathbb{P}^X\Vert\mathbb{P}^Y) = \frac{1}{4}\mathbb{E}_{X\sim \mathbb{P}^X}\left[\int_0^T\Vert a_t(X)-b_t(X)\Vert^2\md t\right].
\end{equation*}
\end{lemma}

\paragraph{Preliminary results.}
\citet[Theorem 1]{guo2024provablebenefitannealedlangevin}  provide convergence guarantees for the continuous-time geometric annealed Langevin dynamics based on the action of the curve of probability measures given by the geometric mean of the base and target distributions.
Their result can be adapted to our setting as follows.
\begin{theorem}[Theorem 1 \citep{guo2024provablebenefitannealedlangevin}]\label{theorem:preliminaries_continuous_time_kl_bound}
    Let \emph{$\mathbb{P}_{\text{DALD}} = (p_{t,\text{DALD}})_{t\in[0, T/\kappa]}$} be  the path measure of the diffusion annealed Langevin dynamics \eqref{eq:annealed_langevin_sde}, and $\mathbb{P}=(\hat{\mu}_{t})_{t\in[0, T/\kappa]}$ that of a reference \gls*{SDE} such that the marginals at each time have distribution $\hat{\mu}_t$. If \emph{$ p_{0, \text{DALD}}= p_0$}, the \gls*{KL} divergence between the path measures is upper bounded by
    \emph{\begin{equation*}     \kl(\mathbb{P}\Vert\mathbb{P}_{\text{DALD}})\leq \kappa \mathcal{A}(\mu).
    \end{equation*}}
\end{theorem}
\begin{proof}
Let $\mathbb{P}$ be the path measure corresponding to the following reference SDE
\begin{equation*}
    \md Y_t = (\nabla\log \hat{\mu}_t + v_t)(Y_t)\md t + \sqrt{2}\md B_t,\; t\in[0,T/\kappa].
\end{equation*}  
The vector field $v = (v_t)_{t\in[0,T/\kappa]}$ is designed such that $Y_t\sim\hat{\mu}_t$ for all $t\in[0, T/\kappa]$. 
Using the Fokker-Planck equation, we have that 
\begin{equation*}
    \partial\hat{\mu}_t = \nabla\cdot\left(\hat{\mu}_t(\nabla\log\hat{\mu}_t + v_t)\right)  + \Delta \hat{\mu}_t = -\nabla\cdot(\hat{\mu}_t v_t), \; t\in[0,T/\kappa].
\end{equation*}
This implies that $v_t$ satisfies the continuity equation and hence generates the curve of probability measures $(\hat{\mu}_t)_t$. Leveraging Lemma \ref{lemma:optimal_vector_field}, we choose $v$ to be the one that minimises the $L^2(\hat{\mu}_t)$ norm, resulting in $\Vert v_t\Vert_{L^2(\hat{\mu}_t)} = \left\vert\Dot{\hat{\mu}}\right\vert_t$ being the metric derivative.
Using the form of Girsanov's theorem given in Lemma \ref{lemma:girsanov_theorem} we have
\begin{align*}
    \kl\left(\mathbb{P}\;||\mathbb{P}_{\text{DALD}}\right) &= \frac{1}{4}\mathbb{E}_{\mathbb{P}}\left[\int_0^{T/\kappa}\left\Vert  v_t(X_t)\right\Vert^2\md t\right] = \frac{1}{4}\int_0^{T/\kappa}\left\Vert  v_t(X_t)\right\Vert_{L^2(\hat{\mu})}^2\md t = \frac{1}{4}\int_0^{T/\kappa}\left\vert\Dot{\hat{\mu}}\right\vert_t^2\md t \\
    & = \frac{\kappa}{4}\int_0^{T}\left\vert\Dot{{\mu}}\right\vert_t^2\md t = \frac{\kappa \mathcal{A}(\mu)}{4},
\end{align*}
where we have used that $\left\vert\Dot{\hat{\mu}}\right\vert_t = \kappa \left\vert\Dot{\mu}\right\vert_t $ and the change of variable formula. 
\end{proof}



\section{Proofs of Section~\ref{sec:gaussian_diffusion}}

\subsection{Comments on Assumption \Cref{assumption:grad_log_lipschitzness_convexity_outside_of_a_ball}}\label{appendix:comments_assumption}
\input{appendix/assumption_1}

\subsection{Comments on Assumption \Cref{assumption:grad_log_lipschitzness_hessian_decay}}\label{appendix:comments_assumption_hessian_decay}
\input{appendix/assumption_hessian_decay}

\subsection{Proof of Lemma~\ref{lem:regularity_of_gaussian_path}}\label{proof:lem:smoothness_of_gaussian_path}
\input{appendix/vector_pi}
\input{appendix/proof_smoothness}

\input{appendix/proof_smoothness_heavy_tail_target}

\subsection{Proof of Lemma~\ref{lemma:action_bound}}
\input{appendix/action_analysis}\label{proof:lemma:bound_action}

\subsection{Proof of Theorem~\ref{theorem:discretisation_analysis_convolutional_path}}\label{proof:theorem:discretisation_path_analysis}
\input{appendix/theorem_discretisation_reparametrised}

\subsection{Proof of Corollary~ \ref{corollary:complexity_bounds}}\label{proof:corollaries:discretisation_approx_score_and_complexity}
\input{appendix/corollary_discretisation_score_approx}

\subsection{Comment on Assumption~\Cref{assumption:pi_data_conditions_less_restrictive}}\label{appendix:comments_new_less_restrictiv_assumption}
\input{appendix/comments_new_less_restrictiv_assumption}

\subsection{Proof of Theorem~\ref{theorem:convergence_relaxed_assumption}}\label{appendix:convergence_relaxed_assumption}
\input{appendix/additional_proof_less_assumption}

%%%%%%%%%%%%%%
\section{Proofs of Section~\ref{sec:heavy_tailed_diffusion}}

\subsection{Comments on Assumption \Cref{assumption:compact_plus_t_distribution_target}}\label{appendix:comments_assumption_compact_plus_heavy_tail}
\input{appendix/assumption_compact_plus_heavy_tail}

\subsection{Proof of Lemma~\ref{lem:regularity_of_heavy_tailed_path}}\label{proof:lem:smoothness_of_heavy_tail_path}
\input{appendix/proof_smoothness_of_heavy_tail_path_alternative}


\subsection{Proof of Lemma~\ref{lemma:action_bound_heavy_tail_diffusion}}\label{proof:lem:action_bound_heavy_tail_diffusion}
\input{appendix/action_bound_heavy_tail_diffusion}


\subsection{Proof of Theorem~\ref{theorem:discretisation_analysis_convolutional_heavy_tailed_diffusion}
}\label{proof:theorem:discretisation_path_analysis_heavy_tail_diffusion}
\input{appendix/theorem_discretisation_reparametrised_heavy_tailed_diffusion}







\end{document}





