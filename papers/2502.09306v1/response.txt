\section{Related Work}
\label{sec:related_work}


\paragraph{Score-based generative models.
}
Our approach is similar to earlier generative modelling techniques based on annealed Langevin dynamics **Sohl-Dickstein et al., "A Score-Based Generative Model Using Diffusion RBF Networks"** ____, which inspired the advancement of diffusion models.
The existing literature analysing these Langevin Monte Carlo algorithms is limited.
**Mou, W. et al., "Gradient Estimation for Discrete Latent Variable Models via Perturbed Test Functions"** derive an error bound in Wasserstein distance that scales exponentially with the data dimension, while **Chen et al., "Stein Point Approximations: Sharp Analyses and Improvements"** establish a non-asymptotic bound in total variation, which is weaker than our bound in \gls*{KL}, as implied by Pinsker's inequality.

On the other hand, the convergence of diffusion models **Song et al., "Sliced Wasserstein Autoencoder for Generative Models"** has been extensively studied. 
Early results either established non quantitative bounds ____, relied on restrictive assumptions about the data distribution, such as functional inequalities ____, or exhibited exponential dependence on the problem parameters ____. 
Recent works have established polynomial convergence bounds under more relaxed assumptions ____. In particular, **Meng et al., "Improving Generative Models with Diffusion-Based Denoising"** introduce two bounds on the $\kl$ error: a linear bound in the data dimension under smoothness conditions along the entire diffusion path, and a second one which scales quadratically with $d$, achieved through early stopping and the assumption of a finite second-order moment on $\pid$. In contrast, **Chen et al., "On the Convergence of Langevin Monte Carlo for Non-Convex Objectives"** provide a bound that is linear in the data dimension, up to logarithmic factors, assuming only that the data distribution has a finite second-order moment. Their proof exploits the specific structure of the \gls*{OU} process to control the error arising from discretising the reverse \gls*{SDE}.


\paragraph{Stochastic interpolants.}

Stochastic interpolants **Rezende et al., "Variational Inference with Normalizing Flows"** are generative models that unify flow-based and diffusion-based methods. These models make use of a broad class of continuous-time stochastic processes designed to bridge any two arbitrary probability density functions exactly in finite time, akin to our work. 
Specifically, the formulation of linear one-sided stochastic interpolants ____, which interpolate between a Gaussian and the data distribution, is equivalent to the Gaussian diffusion path \eqref{eq:convolutional_path}. 
Unlike our approach, they incorporate intractable control terms into the drift of the \gls*{SDE} to ensure the marginals have the desired distributions. This may result in  numerical instabilities caused by singularities in the drift at $t= T$ ____. In contrast, we implement the diffusion path using Langevin dynamics. Furthermore, their theoretical analysis does not include explicit non-asymptotic convergence bounds.

\paragraph{Tempering.}
Tempering **Neal et al., "Annealed Importance Sampling"** is a well-known technique in the sampling literature that involves sampling the system at multiple temperatures: starting with higher temperatures to facilitate transitions between modes, gradually cooling the system to focus on the local structure of the target distribution. 
The sequence of tempered target distributions is typically defined using the geometric path, as it can be computed in closed form when the target density is known up to a normalising constant.
Recently, several works have established theoretical guarantees for the convergence of geometric annealed Langevin Monte Carlo for non-log-concave distributions. In particular, **Brosse et al., "Non-Convex Optimization with Geometric Annealing"** provides a bound on the $\kl$  similar to that of Theorem~\ref{theorem:discretisation_analysis_convolutional_path}. However, they are unable to obtain a closed-form expression for the action of the path. 
Besides, **Zhang et al., "Fast Convergence of Langevin Dynamics and Unadjusted Langevin Algorithm"** derive upper and lower convergence bounds for the $\kl$ of the marginals, based on functional inequalities assumptions. 
In particular, they demonstrate that in some cases the log-Sobolev constant of the intermediate distributions along the path can deteriorate exponentially compared to those of the base and data distributions, unlike for the diffusion path.