\begin{lemma}\label{lemma:auxiliary_change_score_1}
    Suppose that $p(x) \propto e^{-V(x)}$ is a probability density on $\mathbb{R}^d$, where $\nabla V(x)$ is Lipschitz continuous with constant $L$ and let $\varphi_{\sigma^2}(x)$ be the density function of a Student's t distribution $t(0, \sigma^2 I, \alpha)$. 
   Then 
    \begin{equation*}
        \left\Vert \nabla \log \frac{p(x)}{p*\varphi_{\sigma^2}(x)} \right\Vert\leq  L\ \mathbb{E}_{Y|x} \left[\left \Vert Y- x\right\Vert\right],
    \end{equation*}
    where the distribution of $Y|x$ is of the form $\frac{p(y)\varphi_\sigma^2(x-y)}{p(x)*\varphi_\sigma^2(x)}$.
\end{lemma}
\begin{proof}
Observe that 
\begin{align*}
   \nabla \log p*\varphi_{\sigma^2}(x) &= -\frac{\int_{\mathbb{R}^d} \nabla V(y) e^{-V(y)}\left(1+\frac{1}{\alpha}\frac{\Vert y-x\Vert^2}{\sigma^2}\right)^{-\frac{\alpha + d}{2}} \md y}{\int_{\mathbb{R}^d}e^{-V(y)} \left(1+\frac{1}{\alpha}\frac{\Vert y-x\Vert^2}{\sigma^2}\right)^{-\frac{\alpha + d}{2}} \md y} = -\frac{\int_{\mathbb{R}^d} \nabla V(y) p(y) \varphi_{\sigma^2}(x-y) \md y}{\int_{\mathbb{R}^d}p(y) \varphi_{\sigma^2}(x-y) \md y} = -\mathbb{E}_{\gamma_{x, \sigma^2}}\left[\nabla V(Y)\right],
\end{align*}
where $\gamma_{x, \sigma^2}$ denotes the probability density
\begin{equation*}
    \gamma_{x, \sigma^2}(y)=\frac{ p(y) \varphi_{\sigma^2}(x-y)}{ p(x)* \varphi_{\sigma^2}(x)}.
\end{equation*}
Using the Lipschitzness of $\nabla V$, we have
\begin{align*}
     \left\Vert\nabla \log \frac{p(x)}{p*\varphi_{\sigma^2}(x)} \right\Vert &= \left\Vert  \mathbb{E}_{\gamma_{x, \sigma^2}}\left[\nabla V(Y)- \nabla V(x)\right]\right\Vert \leq L\ \mathbb{E}_{\gamma_{x, \sigma^2}} \left[\left \Vert Y- x\right\Vert\right].
\end{align*}
\end{proof}

\begin{lemma}\label{lemma:auxiliary_change_score_2}
 With the setting in Lemma~\ref{lemma:auxiliary_change_score_1}. Denote $p_\lambda(x)=\lambda^d p(\lambda x)$ for $\lambda\geq 1$. Then 
    \begin{align*}
        \left\Vert \nabla \log \frac{p(x)}{p_\lambda *\varphi_{\sigma^2}(x)} \right\Vert\lesssim& \lambda (\lambda-1) \Vert x\Vert + (\lambda-1)\Vert \nabla V(x)\Vert+ \lambda^2 L\ \mathbb{E}_{Y|x}\left[\Vert Y- x\Vert\right],
    \end{align*}
\end{lemma}
where the law of $Y|x$ is given by $\frac{p_\lambda(y)\varphi_\sigma^2(x-y)}{p_\lambda(x)*\varphi_\sigma^2(x)}$.
\begin{proof}
Using the triangle inequality,
\begin{equation*}
    \left\Vert \nabla \log \frac{p(x)}{p_{\lambda}*\varphi_{\sigma^2}(x)}\right\Vert \leq \left\Vert \nabla \log \frac{p(x)}{p_\lambda(x)}\right\Vert + \left\Vert \nabla \log \frac{p_\lambda(x)}{p_{\lambda}*\varphi_{\sigma^2}(x)}\right\Vert.
\end{equation*}
The first term can be bounded as
\begin{align*}
    \left\Vert \nabla \log \frac{p(x)}{p_\lambda(x)}\right\Vert &= \left\Vert \lambda \nabla V(\lambda x)-\nabla V(x)\right\Vert \leq \left\Vert \lambda \nabla V(\lambda x)-\lambda\nabla V(x)\right\Vert+ \left\Vert \lambda \nabla V(x)-\nabla V(x)\right\Vert \\
    & \leq \lambda (\lambda-1) \Vert x\Vert + (\lambda-1)\Vert \nabla V(x)\Vert.
\end{align*}
By the result in Lemma~\ref{lemma:auxiliary_change_score_1}, we have the following bound for the second term
\begin{equation*}
    \left\Vert \nabla \log \frac{p_\lambda(x)}{p_{\lambda}*\varphi_{\sigma^2}(x)}\right\Vert \lesssim \lambda^2 L\ \mathbb{E}_{Y|x}\left[\Vert Y-x\Vert\right],
\end{equation*}
where we have used that $\lambda \nabla V(\lambda x)$ is $\lambda^2 L$-Lipschitz and $Y|x$ has a distribution of the form
\begin{equation*}
    \frac{p_\lambda(y)\varphi_\sigma^2(x-y)}{p_\lambda(x)*\varphi_\sigma^2(x)}.
\end{equation*}
\end{proof}



\begin{proof}[Proof of Theorem \ref{theorem:discretisation_analysis_convolutional_heavy_tailed_diffusion}]
Similarly to the proof of Theorem~\ref{theorem:discretisation_analysis_convolutional_path}, using Girsanov's theorem, we have that the following bound for $\kl(\mathbb{P}||\mathbb{Q})$.
\begin{align}
    \kl&\left(\mathbb{P}\;||\mathbb{Q}\right)= \frac{1}{4}\int_0^{T/\kappa}\mathbb{E}_{\mathbb{P}}\left[\left\Vert \nabla\log \hat{\mu}_t(X_t)-\nabla \log \hat{\mu}_{t_-}(X_{t_-}) + v_t(X_t)\right\Vert^2\right]\md t\nonumber\\
    \leq&  \sum_{l=1}^M\int_{t_{l-1}}^{t_l} L_{\kappa t}^2 \ \mathbb{E}_{\mathbb{P}} \left[\left\Vert X_t-X_{t_-}\right\Vert^2\right]\md t+ \int_0^{T/\kappa} \Vert v_t\Vert^2_{L^2({\hat{\mu}}_t)} \md t+ \int_0^{T/\kappa}\mathbb{E}_{\mathbb{P}} \left[\left\Vert \nabla\log \frac{\hat{\mu}_t(X_{t_-})}{\hat{\mu}_{t_-}(X_{t_-})}\right\Vert^2\right]\md t, \label{eq:kl_path_bound_intermediate_heavy_tail_diffusion}
\end{align}
where we have used that  $\nabla\log\hat{\mu}_{t}$ is Lipschitz with constant $L_{\kappa t}$. 
First, we bound the change in the score function $\mathbb{E}_{\mathbb{P}} \left[\left\Vert \nabla\log \frac{\hat{\mu}_t(X_{t_-})}{\hat{\mu}_{t_-}(X_{t_-})}\right\Vert^2\right]$. Let $t\geq t_{-}$, we can write
\begin{equation*}
    \hat{\mu}_{t_-} = T_{\sqrt{\frac{\lambda_{\kappa t}}{\lambda_{\kappa t_-}}}} \#\hat{\mu}_t * t\left(0, \left(\sqrt{1-\lambda_{\kappa t_-}}-\sqrt{\frac{(1-\lambda_{\kappa t})\lambda_{\kappa t_-}}{\lambda_{\kappa t}}}\right)^2\sigma^2 I, \alpha\right) = T_{\sqrt{\frac{\lambda_{\kappa t}}{\lambda_{\kappa t_-}}}} \#\hat{\mu}_t * t\left(0, \gamma_t\sigma^2 I, \alpha\right) ,
\end{equation*}
where the pushforward $T_{\lambda}\#$ is defined as $T_{\lambda}\#\mu(x) = \lambda^d \mu(\lambda x)$ and we have abused notation by identifying $t(0, \gamma_t\sigma^2 I, \alpha)$ with its density function. Using the result in Lemma~\ref{lemma:auxiliary_change_score_2}, we have
\begin{align*}
        \left\Vert \nabla\log \frac{\hat{\mu}_t(X_{t_-})}{\hat{\mu}_{t_-}(X_{t_-})}\right\Vert^2\lesssim& \frac{\lambda_{\kappa t}}{\lambda_{\kappa t_-}} \left(\sqrt{\frac{\lambda_{\kappa t}}{\lambda_{\kappa t_-}}}-1\right)^2 \Vert X_{t_-}\Vert^2 + \left(\sqrt{\frac{\lambda_{\kappa t}}{\lambda_{\kappa t_-}}}-1\right)^2\Vert \nabla \log \hat{\mu}_t(X_{t_-})\Vert^2 \\
        &+  \left(\frac{\lambda_{\kappa t}}{\lambda_{\kappa t_-}}\right)^2 L_{\kappa t}^2 \mathbb{E}_{Y|X_{t_-}}\left[\Vert Y-X_{t_-}\Vert^{2}\right],
    \end{align*}
    where the distribution of $Y|X_t$ is given by
    \begin{equation*}
        Y|X_{t_-}\sim \frac{T_{\sqrt{\frac{\lambda_{\kappa t}}{\lambda_{\kappa t_-}}}} \#\hat{\mu}_t (y)\ \varphi_{\gamma_t\sigma^2}(X_{t_-}-y)}{T_{\sqrt{\frac{\lambda_{\kappa t}}{\lambda_{\kappa t_-}}}} \#\hat{\mu}_t * \varphi_{\gamma_t\sigma^2} (X_{t_-})} = \frac{T_{\sqrt{\frac{\lambda_{\kappa t}}{\lambda_{\kappa t_-}}}} \#\hat{\mu}_t (y)\ \varphi_{\gamma_t\sigma^2}(X_{t_-}-y)}{\hat{\mu}_{t_-} (X_{t_-})},
    \end{equation*}
    where $\varphi_{\gamma_t\sigma^2}$ is the density function of a Student's $t$ distribution of the form $t\left(0, \gamma_t\sigma^2 I, \alpha\right)$.
Therefore, we have that
\begin{equation*}
    \mathbb{E}_{\mathbb{P}}\left[\mathbb{E}_{Y|X_{t_-}}\left[\Vert Y-X_{t_-}\Vert^{2}\right]\right] = \mathbb{E}_{X_{t_-}, Y} \Vert Y-X_{t_-}\Vert^2,
\end{equation*}
where the joint distribution of $( X_{t_-}, Y)\sim \rho_{( X_{t_-}, Y)}(x, y)$ is of the form
\begin{equation*}
    \rho_{( X_{t_-}, Y)}(x, y)\propto T_{\sqrt{\frac{\lambda_{\kappa t}}{\lambda_{\kappa t_-}}}} \#\hat{\mu}_t (y)\ \varphi_{\gamma_t\sigma^2}(x-y).
\end{equation*}
Using a change of measure, it follows that $Y$ is independent of $X_{t_-}- Y$ and the distribution of $X_{t_-}- Y$ is $t(0, \gamma_t\sigma^2 I, \alpha)$ with $\alpha>2$. This results into
\begin{equation*}
\mathbb{E}_{\mathbb{P}}\left[\mathbb{E}_{Y|X_{t_-}}\left[\Vert Y-X_{t_-}\Vert^{2}\right]\right] = \mathbb{E}_{Z\sim t(0, \gamma_t\sigma^2 I, \alpha)}\left[\Vert Z\Vert^2\right] = \gamma_t\sigma^2 d\frac{\alpha}{\alpha-2}.
\end{equation*}
By assumption on the schedule 
\begin{align*}
    \frac{\lambda_{\kappa t_-}}{\lambda_{\kappa t}} = \mathcal{O}(1+ h_l)
, \quad \quad \left(\sqrt{\frac{\lambda_{\kappa t}}{\lambda_{\kappa t_-}}}-1\right)^2 = \mathcal{O}(h_l^2), \quad\quad \gamma_t \lesssim 1-\frac{\lambda_{\kappa t_-}}{\lambda_{\kappa t}}= \mathcal{O}\left( h_l\right).
\end{align*}
Given that $X_t = \sqrt{\lambda_t} X + \sqrt{1-\lambda_t} \sigma^2 Z$ for $X_t\sim\hat{\mu}_t$, we derive the following moment bound
\begin{align*}
    \mathbb{E}_{\mathbb{P}}\left[\left\Vert X_{t_-}\right\Vert^2\right] =& \mathbb{E}_{\mathbb{P}}\left[\left\Vert \sqrt{\lambda_{\kappa t_-}} X + \sqrt{1-\lambda_{\kappa t_-}} Z\right\Vert^2\right] = \lambda_{\kappa t_-} \mathbb{E}_{\pid}\left[\left\Vert X\right\Vert^2\right] + (1-\lambda_{\kappa t_-})\sigma^2 \frac{\alpha d}{\alpha-2} \lesssim \mathbb{E}_{\pid}\left[\left\Vert X\right\Vert^2\right] + d.
\end{align*}
Similarly to the proof of Theorem~\ref{theorem:discretisation_analysis_convolutional_path}, it holds that 
\begin{align*}
    \mathbb{E}_{\mathbb{P}}\left[\left\Vert \nabla\log\hat{\mu}_t(X_{t_-})\right\Vert^2\right] \leq  L_{\kappa t} d  + L_{\kappa t}^2\mathbb{E}_{\mathbb{P}} \left[\left\Vert X_t-X_{t_-}\right\Vert^2\right]. 
\end{align*}
This results into
\begin{align*}
    \mathbb{E}_{\mathbb{P}} \left[\left\Vert \nabla\log \frac{\hat{\mu}_t(X_{t_-})}{\hat{\mu}_{t_-}(X_{t_-})}\right\Vert^2\right]\lesssim & h_l^2 \left(\mathbb{E}_{\pid}\left[\left\Vert X\right\Vert^2\right] + d\right) + d h_l^2 L_{\kappa t}+ h_l^2 L_{\kappa t}^2 \mathbb{E}_{\mathbb{P}} \left[\left\Vert X_t-X_{t_-}\right\Vert^2\right]  +h_l L_{\kappa t}^2\sigma^2 d \frac{\alpha}{\alpha-2}.
\end{align*}
Substituting this expression into \eqref{eq:kl_path_bound_intermediate_heavy_tail_diffusion}, we have
\begin{align*}
    \kl\left(\mathbb{P}\;||\mathbb{Q}\right)\lesssim& \sum_{l=1}^M\int_{t_{l-1}}^{t_l} L_{\kappa t}^2 \ \mathbb{E}_{\mathbb{P}} \left[\left\Vert X_t-X_{t_-}\right\Vert^2\right]\md t+ \int_0^{T/\kappa} \Vert v_t\Vert^2_{L^2({\hat{\mu}}_t)} \md t\\
    &+ \sum_{l=1}^M\int_{t_{l-1}}^{t_l} \left(d h_l^2 L_{\kappa t} + h_l^2 \left(\mathbb{E}_{\pid}\left[\left\Vert X\right\Vert^2\right] + d\right) +h_l L_{\kappa t}^2\sigma^2 d \frac{\alpha}{\alpha-2}\right) \md t.
\end{align*}
Using the bound derived in \eqref{eq:intermediate_bound_discretisation_paths}, it follows
\begin{align*}
    \kl\left(\mathbb{P}\;||\mathbb{Q}\right)\lesssim& \sum_{l=1}^M \left( 1 + h_l^2\max_{[t_{l-1}, t_l]} L_{t}^2\right)
   \int_{t_{l-1}}^{t_l} \left\vert\Dot{\hat{\mu}}\right\vert_t^2\ \md t + \left(d h_l\int_{t_{l-1}}^{t_l} L_{\kappa t}^2 \ \md t\right) \left(1 + h_l\max_{[t_{l-1}, t_l]} L_{t}\right) \\
   &+ \sum_{l=1}^M\int_{t_{l-1}}^{t_l} \left(d h_l^2 L_{\kappa t} + h_l^2 \left(\mathbb{E}_{\pid}\left[\left\Vert X\right\Vert^2\right] + d\right) +h_l L_{\kappa t}^2\sigma^2 d \frac{\alpha}{\alpha-2}\right) \md t.\\
\end{align*}
Let $h = \max_{l\in\{1, \dots, M\}} h_l$, we can further simplify the previous expression to obtain
\small
\begin{align*}
    \kl&\left(\mathbb{P}\;||\mathbb{Q}\right)\lesssim \sum_{l=1}^M (1+h^2 L_{\max}^2)\int_{t_{l-1}}^{t_l} \left\vert\Dot{\hat{\mu}}\right\vert_t^2 \ \md t\ + d\ h(1+ h L_{\max}) \int_{t_{l-1}}^{t_l} L_{\kappa t}^2 \ \md t\\
    &\quad\quad\quad\quad+ \frac{T}{\kappa}h^2\left(\mathbb{E}_{\pid}\left[\left\Vert X\right\Vert^2\right] + d\right) + dh \frac{\sigma^2\alpha}{\alpha-2} \int_{0}^{T/\kappa} L_{\kappa t}^2 \ \md t\\
     \lesssim& (1+h^2 L_{\max}^2) \int_{0}^{T/\kappa} \left\vert\Dot{\hat{\mu}}\right\vert_t^2 \ \md t\ + 
 d\ h\left(1+ \frac{\sigma^2\alpha}{\alpha-2}+ h L_{\max}\right)\int_{0}^{T/\kappa} L_{\kappa t}^2 \ \md t + \frac{T}{\kappa} h^2\left(\mathbb{E}_{\pid}\left[\left\Vert X\right\Vert^2\right] + d\right)\\
 =&  (1+h^2 L_{\max}^2) \kappa \mathcal{A}_{\lambda}(\mu)  + 
\frac{d\ h}{\kappa} (1+ h L_{\max})\int_{0}^{T} L_{ t}^2 \ \md t+ \frac{T}{\kappa} h^2\left(\mathbb{E}_{\pid}\left[\left\Vert X\right\Vert^2\right] + d\right).
\end{align*}
\normalsize
The step size $h$ can be expressed in terms of the number of steps $M$ and $\kappa$ as $h\asymp \frac{1}{M\kappa}$. Therefore, we have
\small
\begin{align*}
    \kl\left(\mathbb{P}\;||\mathbb{Q}\right)\lesssim&\left(1+\frac{L_{\max}^2}{M^2\kappa^2}\right) \kappa \mathcal{A}_\lambda(\mu) + \frac{d}{M\kappa^2}\left(1+\frac{\sigma^2\alpha}{\alpha -2} +   \frac{L_{\max}}{M\kappa}\right)\int_{0}^{T} L_{ t}^2 \ \md t+ \frac{1}{M^2\kappa^3}\left(\mathbb{E}_{\pid}\left[\left\Vert X\right\Vert^2\right] + d\right)\\
    \lesssim& \left(1+\frac{L_{\max}^2}{M^2\kappa^2}+\frac{1}{M^2\kappa^4}\right) \kappa \left(\mathbb{E}_{\pid}\left[\Vert X\Vert^2\right] + d\right) + \frac{d}{M\kappa^2}\left(1 + \frac{\sigma^2\alpha}{\alpha -2}+  \frac{L_{\max}}{M\kappa}\right)\int_{0}^{T} L_{ t}^2 \ \md t,
\end{align*}
\normalsize
where we have used the bound on the action obtained in Lemma \ref{lemma:action_bound_heavy_tail_diffusion} and $T = \mathcal{O}(1)$. To conclude, note that 
\begin{align*}
    \kl\left(\mathbb{P}\;||\mathbb{Q}_\theta\right)\lesssim& \int_0^{T/\kappa}\mathbb{E}_{\mathbb{P}}\left[\left\Vert \nabla\log \hat{\mu}_t(X_t)-\nabla \log \hat{\mu}_{t_-}(X_{t_-}) + v_t(X_t)\right\Vert^2\right]\md t \\
    &+ \int_0^{T/\kappa}\mathbb{E}_{\mathbb{P}}\left[\left\Vert \nabla \log \hat{\mu}_{t_-}(X_{t_-}) - s_\theta(X_{t_-}, t_-)\right\Vert^2\right]\md t\\
    =& \kl\left(\mathbb{P}\;||\mathbb{Q}\right)+ \sum_{l=0}^{M-1} h_l\mathbb{E}_{\hat{\mu}_t}\left[\left\Vert \nabla \log \hat{\mu}_l(X_{t_l}) - s_\theta(X_{t_l}, t_l)\right\Vert^2\right] =\kl\left(\mathbb{P}\;||\mathbb{Q}\right)+\varepsilon_{\text{score}}^2\\
    \lesssim& \left(1+\frac{L_{\max}^2}{M^2\kappa^2}+\frac{1}{M^2\kappa^4}\right) \kappa \left(\mathbb{E}_{\pid}\left[\Vert X\Vert^2\right] + d\right) + \frac{d L_{\max}^2}{M\kappa^2}\left(1 + \frac{\sigma^2\alpha}{\alpha -2}+  \frac{L_{\max}}{M\kappa}\right)+\varepsilon_{\text{score}}^2\\
    \lesssim& \left(1+\frac{L_{\max}^2}{M^2\kappa^2}+\frac{1}{M^2\kappa^4}\right) \kappa \left(M_2 \vee d\right) + \frac{d L_{\max}^2}{M\kappa^2}\left(1 + \frac{\alpha}{\alpha -2}+  \frac{L_{\max}}{M\kappa}\right)+\varepsilon_{\text{score}}^2.
\end{align*}
We can conclude that by taking 
\begin{align*}
    \kappa = \mathcal{O}\left(\frac{\varepsilon_{\text{score}}^2}{M_2 \vee d}\right),\quad M = \mathcal{O}\left(\frac{d (M_2 \vee d)^2 L_{\max}^2}{\varepsilon_{\text{score}}^6}\right),
\end{align*}
we have that $\kl\left(\mathbb{P}\;||\mathbb{Q}_\theta\right)\lesssim\varepsilon_{\text{score}}^2$. Therefore, for any $\varepsilon = \mathcal{O}(\varepsilon_{\score})$, the heavy-tailed \gls*{DALMC} algorithm requires at most 
\begin{equation*}
     M = \mathcal{O}\left(\frac{d (M_2 \vee d)^2 L_{\max}^2}{\varepsilon^6}\right)
\end{equation*}
steps to approximate $\pid$ to within $\varepsilon^2$ in \gls*{KL} divergence.
\end{proof}