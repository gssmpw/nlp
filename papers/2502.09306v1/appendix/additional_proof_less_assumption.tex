\begin{proof}
    Let $\mathbb{Q}$ denote the path measure associated with the continuous-time interpolation of the modified \gls*{DALMC} algorithm  with exact scores \eqref{eq:auxiliary_algorithm_exact_scores}, which corresponds to the SDE 
\begin{equation*}
    \md X_t = \nabla\log \hat{\mu}_{t_-}(X_{t_-})\md t + \sqrt{2}\md B_t,\; t\in\left[0,T/\kappa\right],
\end{equation*}
where given a discretisation of the interval $\left[0,T/\kappa\right]$, $0=t_0<t_1<\dots<t_M=T/\kappa$, we define $t_-:=t_{l-1}$ when $t\in [t_{l-1}, t_l)$ for $l=1,\dots, M$. On the other hand, let $\mathbb{P}$ be the path measure corresponding to the following reference SDE
\begin{equation*}
    \md X_t = (\nabla\log \hat{\mu}_t + v_t)(X_t)\md t + \sqrt{2}\md B_t,\; t\in\left[0,T/\kappa\right].
\end{equation*}
As in the proof of Theorem~\ref{theorem:discretisation_analysis_convolutional_path}, the vector field $v = (v_t)_{t\in\left[0,T/\kappa\right]}$ is designed such that $X_t\sim\hat{\mu}_t$ for all $t\in\left[0, T/\kappa\right]$ and $\Vert v_t\Vert_{L^2(\hat{\mu}_t)} = \left\vert\Dot{\hat{\mu}}\right\vert_t$.
Using Girsanov's theorem we have
\begin{align}
    \kl\left(\mathbb{P}\;||\mathbb{Q}\right)=& \frac{1}{4}\int_0^{T/\kappa}\mathbb{E}_{\mathbb{P}}\left[\left\Vert \nabla\log \hat{\mu}_t(X_t)-\nabla \log \hat{\mu}_{t_-}(X_{t_-}) + v_t(X_t)\right\Vert^2\right]\md t\nonumber\\
    \lesssim& \int_0^{T/\kappa}\mathbb{E}_{\mathbb{P}} \left[\left\Vert \nabla\log \hat{\mu}_t(X_t)-\nabla \log \hat{\mu}_{t_-}(X_{t_-})\right\Vert^2\right]\md t + \int_0^{T/\kappa} \Vert v_t\Vert^2_{L^2({\hat{\mu}}_t)} \md t. \nonumber
\end{align}
Note that the first term involves both a time and space discretisation error. 
Inspired by \citet{Chen2022ImprovedAO}, we start analysing
\begin{equation*}
    E_{t, s} = \mathbb{E}_{\mathbb{P}}\left[\left\Vert \nabla\log \hat{\mu}_t(X_t) - \nabla\log \hat{\mu}_s(X_s)\right\Vert^2\right],
\end{equation*}
with $0\leq s< t\leq 1$. Recall that we can write
\begin{equation*}
    X_s = \sqrt{\frac{\lambda_{\kappa s}}{\lambda_{\kappa t}}} X_t + \sigma \left(\sqrt{1-\lambda_{\kappa s}} - \sqrt{\frac{(1-\lambda_{\kappa t})\lambda_{\kappa s}}{\lambda_{\kappa t}}}\right) Z = \alpha_{t, s} X_t + \Tilde{\sigma}_{t,s} Z,
\end{equation*}
where $Z\sim \mathcal{N}(0, I)$ independent of $X_t$. So, we have
\begin{equation*}
    \hat{\mu}_s(x) \propto \int \alpha_{t, s}^{-d} \ \hat{\mu}_t\left(\frac{y}{\alpha_{t,s}}\right) e^{-\frac{\Vert x-y\Vert^2}{2\Tilde{\sigma}_{t,s}}} \md y = \int \hat{\mu}_t(y)e^{-\frac{\Vert x-\alpha_{t, s}y\Vert^2}{2\Tilde{\sigma}_{t,s}}} \md y.
\end{equation*}
Therefore, similarly to the proof of Lemma  \ref{lem:regularity_of_gaussian_path}, we can express the score of $\hat{\mu}_s$ in terms of that of $\hat{\mu}_t$,
\begin{equation*}
    \nabla\log\hat{\mu}_s (x) = \alpha_{t,s}^{-1} \ \mathbb{E}_{Y\sim\varphi_{t| x, s}} \left[\nabla \log \hat{\mu}_t(Y)| x, s\right],
\end{equation*}
where $\varphi_{t| x,s}(y)\propto \hat{\mu}_t(y)e^{-\frac{\Vert x - \alpha_{t,s}y\Vert^2}{2\Tilde{\sigma}_{t,s}^2}}$.
Substituting this we have
\begin{align}
    E_{t, s} \leq& 2 \mathbb{E}_{\mathbb{P}}\left[\left\Vert \nabla\log \hat{\mu}_s(X_s) - \nabla\log \hat{\mu}_t(\alpha_{t, s}^{-1}X_s)\right\Vert^2\right] + 2 \mathbb{E}_{\mathbb{P}}\left[\left\Vert \nabla\log \hat{\mu}_t(X_t) - \nabla\log \hat{\mu}_t(\alpha_{t, s}^{-1}X_s)\right\Vert^2\right] \nonumber \\
    =& 2 \mathbb{E}_{\mathbb{P}}\left[\left \Vert \mathbb{E}_{\varphi_{t|X_s,s}}\left[(\alpha_{t,s}^{-1}-1)\nabla \log \hat{\mu}_t(Y) + \nabla\log \hat{\mu}_t(Y) - \nabla\log\hat{\mu}_{t}(\alpha_{t,s}^{-1}X_s)\right]\right\Vert^2\right] \nonumber\\
    & +  2 \mathbb{E}_{\mathbb{P}}\left[\left\Vert \nabla\log \hat{\mu}_t(X_t) - \nabla\log \hat{\mu}_t(\alpha_{t, s}^{-1}X_s)\right\Vert^2\right] \nonumber\\
    \leq & 4(\alpha_{t,s}^{-1} -1)^2 \ \mathbb{E} \left[\Vert \nabla\log \hat{\mu}_t(X_t)\Vert^2\right] + 6 \ \mathbb{E}_{\mathbb{P}}\left[\left\Vert \nabla\log \hat{\mu}_t(X_t) - \nabla\log \hat{\mu}_t(\alpha_{t, s}^{-1}X_s)\right\Vert^2\right]. \label{eq:auxiliary_time_space_discretisation_error}
\end{align}
Using the score expressions provided in Lemma \ref{lem:regularity_of_gaussian_path} Eq. \eqref{eq:score_conv_expressions}, we can bound $\mathbb{E} \left[\Vert \nabla\log \hat{\mu}_t(X_t)\Vert^2\right]$ as follows
\begin{align*}
    \mathbb{E}_{\hat{\mu}_t}\left[\Vert \nabla\log 
 \hat{\mu}_t(X_t)\Vert^2\right] \leq& \frac{1}{\sigma^2(1-\lambda_{\kappa t})} \mathbb{E}_{\hat{\mu}_t}\mathbb{E}_{\rho_{t, X_t}}\left[\left\Vert\frac{X_t - Y}{\sigma\sqrt{1-\lambda_{\kappa t}}}\right\Vert^2\right] = \frac{d}{\sigma^2(1-\lambda_{\kappa t})},  \\
\mathbb{E}_{\hat{\mu}_t}\left[\Vert \nabla\log 
 \hat{\mu}_t(X_t)\Vert^2\right] \leq & \frac{1}{\lambda_{\kappa t}} \mathbb{E}_{\hat{\mu}_t}\mathbb{E}_{\rho_{t, X_t}}\left[\left\Vert\nabla V_{\pi}\left(\frac{Y}{\sqrt{\lambda_{\kappa t}}}\right)\right\Vert^2\right] = \frac{1}{\lambda_{\kappa t}} \mathbb{E}_{\pid}\left[\left\Vert\nabla V_{\pi}\left(Y\right)\right\Vert^2\right] \leq \frac{L_\pi}{\lambda_{\kappa t}} d,
\end{align*}
where we have used the Lipschitzness of $\pid$.
This provides, $\mathbb{E} \left[\Vert \nabla\log \hat{\mu}_t(X_t)\Vert^2\right]\leq \min\left\{\frac{d}{\sigma^2(1-\lambda_{\kappa t})}, \frac{L_\pi d}{\lambda_{\kappa t}}\right\}$.
Following a similar argument to \citet[Lemma 13]{Chen2022ImprovedAO},  we  study the second term in \eqref{eq:auxiliary_time_space_discretisation_error},
\begin{align*}
    \mathbb{E}\left[\left\Vert \nabla\log \hat{\mu}_t(X_t) - \nabla\log \hat{\mu}_t(\alpha_{t, s}^{-1}X_s)\right\Vert^2\right] =& \mathbb{E}\left[\left\Vert \int_{0}^1 \nabla^2\log \hat{\mu}_t(X_t + aZ_{t,s})Z_{t,s} \md a\right\Vert^2\right] \\
    \leq & \int_{0}^1 \mathbb{E}\left[\left\Vert  \nabla^2\log \hat{\mu}_t(X_t + aZ_{t,s})Z_{t,s} \right\Vert^2\right] \md a,
\end{align*}
where $Z_{t,s} = \alpha_{t,s}^{-1} X_s - X_t\sim \mathcal{N}\left(0, \left(\frac{\Tilde{\sigma}_{t,s}}{\alpha_{t,s}}\right)^2 I\right)$ is independent of $X_t$. For simplicity, we denote
\begin{equation*}
    \hat{\sigma}_{t,s} = \frac{\Tilde{\sigma}_{t,s}}{\alpha_{t,s}} =\sigma\left( \sqrt{\frac{(1-\lambda_{\kappa s})\lambda_{\kappa t}}{\lambda_{\kappa s}}} - \sqrt{1-\lambda_{\kappa t}}\right).
\end{equation*}
To bound $\mathbb{E}\left[\left\Vert  \nabla^2\log \hat{\mu}_t(X_t + aZ_{t,s})Z_{t,s} \right\Vert^2\right]$ we consider the following change of variable
\begin{align*}
    \mathbb{E}\left[\left\Vert  \nabla^2\log \hat{\mu}_t(X_t + aZ_{t,s})Z_{t,s} \right\Vert^2\right] &= \mathbb{E} \left[\left\Vert  \nabla^2\log \hat{\mu}_t(X_t)Z_{t,s} \right\Vert^2 \frac{\md P_{X_t + a Z_{t,s}, Z_{t,s}}(X_t, Z_{t,s})}{\md P_{X_t, Z_{t,s}}(X_t, Z_{t,s})}\right] \\
    &\lesssim \left(\mathbb{E}\left\Vert  \nabla^2\log \hat{\mu}_t(X_t)Z_{t,s} \right\Vert^4\mathbb{E}\left(\frac{\md P_{X_t + a Z_{t,s}, Z_{t,s}}(X_t, Z_{t,s})}{\md P_{X_t, Z_{t,s}}(X_t, Z_{t,s})}\right)^2\right)^{1/2}.
\end{align*}
Let $M_t = \nabla^2\log \hat{\mu}_t(X_t)\left(\nabla^2\log \hat{\mu}_t(X_t)\right)^\intercal$ and $N_{t,s} = Z_{t,s}Z_{t,s}^\intercal$, which by definition they are independent. We now need to bound the two previous factors. By the properties of the tensor product, we have
\begin{equation*}
    \mathbb{E}\left\Vert  \nabla^2\log \hat{\mu}_t(X_t )Z_{t,s} \right\Vert^4 = \mathbb{E}\left[\text{Tr}\left(M_t^\intercal Z_{t,s}\right)^2\right] = \left\langle \mathbb{E} M_t \otimes M_t, \mathbb{E} Z_{t,s}\otimes Z_{t,s}\right\rangle.
\end{equation*}
Using the properties of the $\chi^2$ distribution, we obtain
\begin{equation*}
    \mathbb{E} (Z_{t,s}\otimes Z_{t,s})_{i_1, i_2, i_3, i_4} = \begin{cases}
        3 \Hat{\sigma}_{t,s}^4, \; i_1=i_2=i_3=i_4, \\
        \Hat{\sigma}_{t,s}^4\; i_1 \neq i_2, (i_1, i_2) = (i_3, i_4) \;\text{or}\; (i_1, i_2) = (i_4, i_3),\\
        0, \;\text{otherwise}.
    \end{cases}
\end{equation*}
Substituting this we have
\begin{align*}
    \mathbb{E}&\left\Vert  \nabla^2\log \hat{\mu}_t(X_t )Z_{t,s} \right\Vert^4 \lesssim \Hat{\sigma}_{t,s}^4 \left(\sum_{ (i_1, i_2) = (i_3, i_4)} + \sum_{ (i_1, i_2) = (i_4, i_3)}\right) \mathbb{E}(M_t\otimes M_t)_{i_1, i_2, i_3, i_4}\\
    &\lesssim \Hat{\sigma}_{t,s}^4 \sum_{ (i_1, i_2) = (i_3, i_4)} \mathbb{E}(M_t\otimes M_t)_{i_1, i_2, i_3, i_4}\lesssim \Hat{\sigma}_{t,s}^4  \mathbb{E}\Vert M_t\Vert_F^2 \lesssim \Hat{\sigma}_{t,s}^4 \mathbb{E}\Vert \nabla^2\log \hat{\mu}_t(X_t)\Vert_F^4.
\end{align*}
Applying Lemma 12 from \citet{Chen2022ImprovedAO}, it follows that
\begin{equation*}
    \mathbb{E}\Vert \nabla^2\log \hat{\mu}_t(X_t)\Vert_F^4 \lesssim \left(\frac{d}{\sigma^2(1-\lambda_{\kappa t})}\right)^4.
\end{equation*}
This bound becomes arbitrarily large as $\lambda_{\kappa t}$ tends to $1$, however, using the alternative expression for the Hessian provided in \eqref{eq:hessian_conv_2}, we can write
\begin{align*}
    \nabla^2 \log \hat{\mu}_t(X_t) &=-\frac{1}{\lambda_{\kappa t}}\left(\mathbb{E}_{\rho_{t, X_t}}\left[\nabla^2 V_\pi\left(\frac{Y}{\sqrt{\lambda_{\kappa t}}}\right)\right] - \text{Cov}_{\rho_{t, X_t}}\left[\nabla V_\pi\left(\frac{Y}{\sqrt{\lambda_{\kappa t}}}\right)\right]\right),
\end{align*}
where $\rho_{t, X_t}(y)\propto e^{-V_\pi(y/\sqrt{\lambda_{\kappa t}})-\Vert x-y\Vert^2/(2\sigma^2(1-\lambda_{\kappa t}))}$.
Due to the Lipschitzness of $\pid$, we have
\begin{equation*}
    - L_\pi I \preccurlyeq\mathbb{E}_{\rho_{t, X_t}}\left[\nabla^2 V_\pi\left(\frac{Y}{\sqrt{\lambda_{\kappa t}}}\right)\right]\preccurlyeq L_\pi I,
\end{equation*}
where the Frobenius norm of the identity matrix is $\Vert I\Vert_F=\sqrt{d}$.
For the covariance term we proceed as follows
\begin{align*}
    &\frac{1}{\lambda_{\kappa t}^4}\mathbb{E}_{\hat{\mu}_t} \left\Vert \text{Cov}_{\rho_{t, X_t}}\left[\nabla V_\pi\left(\frac{Y}{\sqrt{\lambda_{\kappa t}}}\right)\right]\right\Vert_F^4 \leq \frac{1}{\lambda_{\kappa t}^4} \mathbb{E}_{\hat{\mu}_t}\mathbb{E}_{\rho_{t,X_t}} \left\Vert\nabla V_\pi\left(\frac{Y}{\sqrt{\lambda_{\kappa t}}}\right) \nabla V_\pi\left(\frac{Y}{\sqrt{\lambda_{\kappa t}}}\right)^\intercal\right\Vert_F^4\\
    &= \frac{1}{\lambda_{\kappa t}^4} \mathbb{E}_{T_{\lambda_{\kappa t}^{-1/2}}\#\pid} \left\Vert\nabla V_\pi\left(\frac{Y}{\sqrt{\lambda_{\kappa t}}}\right) \nabla V_\pi\left(\frac{Y}{\sqrt{\lambda_{\kappa t}}}\right)^\intercal\right\Vert_F^4 = \frac{1}{\lambda_{\kappa t}^4}  \mathbb{E}_{T_{\lambda_{\kappa t}^{-1/2}}\#\pid} \left\Vert\nabla V_\pi\left(\frac{Y}{\sqrt{\lambda_{\kappa t}}}\right)\right\Vert^{8} \\
    & =\frac{1}{\lambda_{\kappa t}^4}  \mathbb{E}_{\pid} \left\Vert\nabla V_\pi\left(Y\right)\right\Vert^{8} \leq \frac{K_\pi^2}{\lambda_{\kappa t}^4}.
\end{align*}
Therefore, we have
\begin{equation*}
    \mathbb{E}\Vert \nabla^2\log \hat{\mu}_t(X_t)\Vert_F^4 \lesssim \min \left\{\frac{d^4}{\sigma^8(1-\lambda_{\kappa t})^4}, \frac{L_\pi^4 d^2 + K_\pi^2}{\lambda_{\kappa t}^4}\right\}.
\end{equation*}
Next, we bound the term concerning the change of variable
\begin{align*}
    \mathbb{E}\left(\frac{\md P_{X_t + a Z_{t,s}, Z_{t,s}}(X_t, Z_{t,s})}{\md P_{X_t, Z_{t,s}}(X_t, Z_{t,s})}\right)^2 =& \mathbb{E}\left(\frac{\md P_{X_t + a Z_{t,s}| Z_{t,s}}(X_t| Z_{t,s})}{\md P_{X_t | Z_{t,s}}(X_t| Z_{t,s})}\right)^2 \leq \mathbb{E}\left(\frac{\md P_{X_t + a Z_{t,s}| Z_{t,s}, X_{T_{\kappa}}}(X_t| Z_{t,s}, X_{T_{\kappa}})}{\md P_{X_t | Z_{t,s}, X_{T_{\kappa}}}(X_t| Z_{t,s}, X_{T_{\kappa}})}\right)^2\\
    =& \mathbb{E}\left(\frac{\md P_{X_t + a Z_{t,s}| Z_{t,s}, X_{T_{\kappa}}}(X_t| Z_{t,s}, X_{T_{\kappa}})}{\md P_{X_t |  X_{T_{\kappa}}}(X_t|  X_{T_{\kappa}})}\right)^2,
\end{align*}
where we have used the data processing inequality and $X_{T_{\kappa}}\sim \pid$. Since $X_t + a Z_{t,s}|(Z_{t,s}, X_{T_{\kappa}})\sim \mathcal{N}(\sqrt{\lambda_{\kappa t}} X_{T_{\kappa}}+aZ_{t,s}, \sigma^2(1-\lambda_{\kappa t}))$ and $X_t|X_{T_{\kappa}}\sim\mathcal{N}(\sqrt{\lambda_{\kappa t}}X_{T_\kappa}, \sigma^2(1-\lambda_{\kappa t}))$, we can explicitly compute the previous expression, as it corresponds to the $\chi^2$ divergence between two Gaussians, that is,
\begin{align*}
    \mathbb{E}\left(\frac{\md P_{X_t + a Z_{t,s}, Z_{t,s}}(X_t, Z_{t,s})}{\md P_{X_t, Z_{t,s}}(X_t, Z_{t,s})}\right)^2\leq \mathbb{E}\exp \left(\frac{a^2\Vert Z_{t,s}\Vert^2}{\sigma^2(1-\lambda_{\kappa t})}\right)\leq \left(1-2a^2\left(1-\sqrt{\frac{\lambda_{\kappa t}(1-\lambda_{\kappa s})}{\lambda_{\kappa s}(1-\lambda_{\kappa t})}}\right)^2\right)^{-d/2},
\end{align*}
where we have used the expression of the moment generating function of a $\chi^2$ distribution. Under the assumption on the schedule and for $t-s<<1$, it follows that 
\begin{equation*}
    \mathbb{E}\left(\frac{\md P_{X_s + a Z_{t,s}, Z_{t,s}}(X_s, Z_{t,s})}{\md P_{X_s, Z_{t,s}}(X_s, Z_{t,s})}\right)^2\lesssim 1.
\end{equation*}
Putting all this together, we have
\begin{align*}
    \mathbb{E}\left[\left\Vert \nabla\log \hat{\mu}_t(X_t) - \nabla\log \hat{\mu}_t(\alpha_{t, s}^{-1}X_s)\right\Vert^2\right] \lesssim & \Hat{\sigma}_{t,s}^2 \min \left\{\frac{d^2}{\sigma^4(1-\lambda_{\kappa t})^2}, \frac{L_\pi^2 d + K_\pi}{\lambda_{\kappa t}^2}\right\}\\
    \lesssim &\sigma^2 (t-s) \min \left\{\frac{d^2}{\sigma^4(1-\lambda_{\kappa t})^2}, \frac{L_\pi^2 d\vee K_\pi}{\lambda_{\kappa t}^2}\right\}
\end{align*}
for $t-s<<1$. Substituting this into \eqref{eq:auxiliary_time_space_discretisation_error}, it follows
\begin{align*}
    E_{t,s}\lesssim (t-s)^2d\min\left\{\frac{1}{\sigma^2(1-\lambda_{\kappa t})}, \frac{L_\pi}{\lambda_{\kappa t}}\right\} + \sigma^2 (t-s) \min \left\{\frac{d^2}{\sigma^4(1-\lambda_{\kappa t})^2}, \frac{L_\pi^2 d \vee K_\pi}{\lambda_{\kappa t}^2}\right\}.
\end{align*}
Therefore, this results into the following bound for the \gls*{KL}
\begin{align*}
    \kl\left(\mathbb{P}\;||\mathbb{Q}\right)\lesssim & \sum_{l=1}^M \int_{t_{l-1}}^{t_l} \mathbb{E}_{\mathbb{P}}  \left[\left\Vert \nabla\log \hat{\mu}_t(X_t)-\nabla \log \hat{\mu}_{t_-}(X_{t_-})\right\Vert^2\right]\md t + \int_0^{T/\kappa} \Vert v_t\Vert^2_{L^2({\hat{\mu}}_t)} \md t\\
    &\lesssim \sum_{l=1}^M h_l^3 d\min\left(\frac{1}{\sigma^2(1-\lambda_{\kappa t_{l}})}, \frac{L_\pi}{\lambda_{\kappa t_{l-1}}}\right) + \sigma^2 h_l^2 \min \left(\frac{d^2}{\sigma^4(1-\lambda_{\kappa t_{l}})^2}, \frac{L_\pi^2 d \vee K_\pi}{\lambda_{\kappa t_{l-1}}^2}\right)+  \kappa \mathcal{A}_{\lambda}(\mu).  
\end{align*}
Let $\mathbb{Q}_\theta$ be the path measure of the continuous-time interpolation of the \gls*{DALMC} algorithm \eqref{eq:annealed_langevin_mcmc_algorithm_score_approx}. When implementing the algorithm with step sizes $h_l\asymp 1/( M \kappa)$, we have
\begin{align*}
    \kl\left(\mathbb{P}\;||\mathbb{Q}_\theta\right)\lesssim & \kl\left(\mathbb{P}\;||\mathbb{Q}\right) + \sum_{l=0}^{M-1} h_l\mathbb{E}_{\hat{\mu}_t}\left[\left\Vert \nabla \log \hat{\mu}_l(X_{t_l}) - s_\theta(X_{t_l}, t_l)\right\Vert^2\right]
    \\\lesssim &\frac{dL_\pi}{M^2\kappa^3} + \frac{(d^2\vee L_\pi^2d \vee K_\pi)}{M\kappa^2} + \kappa (\mathbb{E}_{\pid}[\Vert X\Vert^2] + d) + \varepsilon_{\text{score}}^2,
\end{align*}
where we have used the bound on the action given in Lemma~\ref{lemma:action_bound_heavy_tail_diffusion}.
We can conclude that by taking
\begin{equation*}
    \kappa = \mathcal{O}\left(\frac{\varepsilon_{\text{score}}^2}{M_2 \vee d}\right),\quad M = \mathcal{O}\left(\frac{(M_2 \vee d)^2(d^2\vee L_\pi^2d \vee K_\pi) L_\pi}{\varepsilon_{\text{score}}^6}\right),
\end{equation*}
we guarantee that $\kl(\mathbb{P}\ \vert \mathbb{Q}_\theta)\leq \varepsilon_{\text{score}}^2$. 
Therefore, for any $\varepsilon = \mathcal{O}(\varepsilon_{\score})$, the \gls*{DALMC} algorithm under relaxed assumptions requires at most 
\begin{equation*}
     M = \mathcal{O}\left(\frac{(M_2 \vee d)^2(d^2\vee L_\pi^2d \vee K_\pi) L_\pi}{\varepsilon^6}\right)
\end{equation*}
steps to approximate $\pid$ to within $\varepsilon^2$ in \gls*{KL} divergence.
Note that if $M_2= \mathcal{O}(d)$, $L_\pi=\mathcal{O}(\sqrt{d})$ and $K_\pi= \mathcal{O}(d^2)$, then the number of steps is of order 
\begin{equation*}
    M = \mathcal{O}\left(\frac{T^2 d^4L_\pi}{\varepsilon^6}\right).
\end{equation*}
\end{proof}
