\begin{proof}
Based on the \gls*{KL} bound established in Theorem~\ref{theorem:discretisation_analysis_convolutional_path}, we can obtain the iteration complexity of the \gls*{DALMC} algorithm \eqref{eq:annealed_langevin_mcmc_algorithm_score_approx}. Observe that by selecting 
\begin{equation*}
    \kappa = \mathcal{O}\left(\frac{\varepsilon_{\text{score}}^2}{M_2\vee d}\right),\quad M = \mathcal{O}\left(\frac{d(M_2\vee d)^2L_{\max}^2}{\varepsilon_{\text{score}}^6}\right),
\end{equation*}
it follows that $\kl\left(\mathbb{P}\;||\mathbb{Q}_\theta\right)\leq \varepsilon_{\text{score}}^2$. Therefore, for any $\varepsilon = \mathcal{O}(\varepsilon_{\score})$, the \gls*{DALMC} algorithm requires at most 
\begin{equation*}
     M = \mathcal{O}\left(\frac{d (M_2 \vee d)^2 L_{\max}^2}{\varepsilon^6}\right)
\end{equation*}
steps to approximate $\pid$ to within $\varepsilon^2$ in \gls*{KL} divergence.
\end{proof}