A typical assumption in the literature \citep{saremi2024chain, grenioux2024stochastic} considers the data distribution is given by the convolution of a compactly supported measure and a Gaussian distribution, which can be formalised as follows.
\begin{assumption}\label{assumption:compact_plus_gaussian_target_1}
     Let $ X$ be a $d$-dimensional random vector $X \sim\pi_{\text{data}}$, such that $X = U + G$, where $\Vert U - m_{\pi}\Vert^2 \leq  d R^2$ holds almost surely and $G \sim \mathcal{N}(0, \tau^2 I)$ is independent of $U$. 
\end{assumption}
We demonstrate that assumption \Cref{assumption:compact_plus_gaussian_target_1} implies that the potential $V_\pi$ has Lipschitz continuous gradients, satisfies the dissipativity condition and, also ensures that assumption \Cref{assumption:lipschitz_score_across_convolutional_diffusion} is satisfied. Furthermore, under additional assumptions on the compactly supported measure in \Cref{assumption:compact_plus_gaussian_target_1}, we show that \Cref{assumption:compact_plus_gaussian_target_1} entails \Cref{assumption:grad_log_lipschitzness_convexity_outside_of_a_ball}.
\begin{lemma}\label{lemma:implications_between_assumptions}
Let $\pid\propto e^{-V_\pi}$,    assumption \Cref{assumption:compact_plus_gaussian_target_1} implies that $V_\pi$ has Lipschitz continuous gradients and satisfies the dissipativity inequality
\begin{equation*}
    \langle\nabla V_\pi(x), x\rangle\geq a_\pi \Vert x\Vert^2 - b_\pi,
\end{equation*}
with constants $a_\pi, b_\pi>0$. Furthermore, $\pid$ has a finite log-Sobolev constant.
\end{lemma}
\begin{proof}
 First we show that if $\pi_{\text{data}}\propto e^{-V_\pi}$ satisfies \Cref{assumption:compact_plus_gaussian_target_1} then $V_\pi$ is gradient Lipschitz. 
 Let $X\sim\pi_{\text{data}}$, $U\sim\Tilde{\pi}$ with compact support and $G\sim\gamma=\mathcal{N}(0, \tau^2I)$ independent of $U$. By assumption \Cref{assumption:compact_plus_gaussian_target_1} we have $X = U + G$ or equivalently $\pi_{\text{data}} = \Tilde{\pi}*\gamma$. 
 Using that if $g$ is a differentiable function then $\nabla(f*g) = f * (\nabla g)$, we have that
\begin{align}
-\nabla\log\pid(x) &= \frac{1}{\tau^2}\left(x-\mathbb{E}_{\rho_x}[Y]\right) \label{eq:covariance_compact_plus_gaussian_score}\\
    -\nabla^2 \log\pi_{\text{data}}(x) &= \frac{1}{\tau^2}\left(I -\frac{1}{\tau^2} \text{Cov}_{{\rho_x}} \left[Y\right]\right), \label{eq:covariance_compact_plus_gaussian_hessian}
\end{align}
where $\rho_x(y) \propto \Tilde{\pi}(y)\gamma(x-y)$ and $\text{Cov}_{\rho_x}[Y] = \mathbb{E}_{\rho_x}[Y Y^\intercal] - \mathbb{E}_{\rho_x}[Y] \mathbb{E}_{\rho_x}[Y]^\intercal$.
Note that $\rho_x(y)$ has bounded support independent of $x$ by \Cref{assumption:compact_plus_gaussian_target_1}. Therefore, the eigenvalues of $\text{Cov}_{{\rho_x}} \left[Y\right]$ can be upper bounded by a constant independent of $x$. 
That is, for any $a\in\mathbb{R}^d$ with $\Vert a\Vert=1$
\begin{align*}
    -a^{\intercal}\left(\nabla^2 \log \pi_{\text{data}}(x)\right) a &= \tau^{-2} -\tau^{-4} a^{\intercal} \text{Cov}_{\rho_x}\left[Y\right]a\geq \tau^{-2}  - \tau^{-4} a^\intercal \mathbb{E}_{\rho_x}[Y Y^\intercal] a\\
    &\geq \tau^{-2} -\tau^{-4} \mathbb{E}_{\rho_x}\left[(a^{\intercal}Y)^2\right] \geq \tau^{-2}  -\tau^{-4}  \mathbb{E}_{\rho_x}[\Vert Y \Vert^2] \\
    &\geq \tau^{-2}-\tau^{-4} (m_\pi + dR^2),
\end{align*}
where we have used Cauchy-Schwarz inequality and \Cref{assumption:compact_plus_gaussian_target_1}.
On the other hand, since the covariance matrix is positive semidefinite, we have
\begin{equation*}
    -\nabla^2 \log\pi_{\text{data}} (x)\preccurlyeq \tau^{-2} I.
\end{equation*}
Therefore, the Hessian $-\nabla^2\log\pi_{\text{data}}$ satisfies
\begin{equation*}
     \left(\tau^{-2}-\tau^{-4} (m_\pi + dR^2)\right) I \preccurlyeq -\nabla^2 \log \pi_{\text{data}}(x) \preccurlyeq \tau^{-2} I,
\end{equation*}
proving that $-\nabla\log\pid$ is gradient Lipschitz with constant $L_\pi\leq \max\left\lbrace \tau^{-2}, \vert\tau^{-2}-\tau^{-4} (m_\pi + dR^2)\vert \right\rbrace$.

On the other hand, using the expression for $\nabla V_\pi = -\nabla\log\pid$ in \eqref{eq:covariance_compact_plus_gaussian_score}, we have 
\begin{align*}
    \langle\nabla V_\pi(x), x\rangle &= \frac{1}{\tau^2}\left\langle x- \mathbb{E}_{\rho_x}[Y], x\right\rangle = \frac{1}{\tau^2}\left(\Vert x\Vert^2-\mathbb{E}_{\rho_x}[\langle Y, x\rangle]\right)\geq \frac{1}{\tau^2}\left(\Vert x\Vert^2-\Vert x\Vert\mathbb{E}_{\rho_x}[\Vert Y\Vert]\right)\\
    &\geq \frac{1}{\tau^2}\left(\Vert x\Vert^2-\Vert x\Vert(m_\pi + \sqrt{d}R)\right) \geq \frac{\Vert x\Vert^2}{2\tau} -(m_\pi + \sqrt{d}R)^2,
\end{align*}
where we have used that $\rho_x(y) \propto \Tilde{\pi}(y)\gamma(x-y)$ has bounded support. This establishes the dissipativity inequality.

Finally, thanks to the dissipativity condition together with Lipschitz gradient, it follows from \citet{cattiaux2010note} that $\pid$ has a finite log-Sobolev constant.
\end{proof}

We now demonstrate the \Cref{assumption:compact_plus_gaussian_target_1} implies \Cref{assumption:lipschitz_score_across_convolutional_diffusion}.
\begin{lemma}
    If $\pid$ satisfies assumption \Cref{assumption:compact_plus_gaussian_target_1}, then the scores $\nabla\log\mu_t$ of the intermediate probability densities of the Gaussian diffusion path are Lipschitz continuous for all $t$, that is, assumption \Cref{assumption:lipschitz_score_across_convolutional_diffusion} is satisfied.
\end{lemma}
\begin{proof}
    Recall that the intermediate random variables of the Gaussian diffusion path, are given by
    \begin{equation*}
        X_t = \sqrt{\lambda_t} X + \sqrt{1-\lambda_t} \sigma Z
    \end{equation*}
    where $X\sim\pid$ and $Z\sim\mathcal{N}(0, I)$ independent of $X$. Using assumption \Cref{assumption:compact_plus_gaussian_target_1}, it follows that 
    \begin{equation*}
        X_t \overset{d}{=} \sqrt{\lambda_t} U + \sqrt{(1-\lambda_t)\sigma^2 + \lambda_t\tau^2} Z,
    \end{equation*}
    where $U\sim\Tilde{\pi}$ is compactly supported and $Z\sim\mathcal{N}(0, I)$ independent of $\Tilde{\pi}$. 
    By applying the result from the previous lemma (Lemma \ref{lemma:implications_between_assumptions}), we conclude that $\nabla\log\mu_t$, where $\mu_t\sim X_t$, is Lipschitz continuous with constant
    \begin{equation*}
        L_t \leq \max\left\lbrace \tau_t^{-1}, \vert\tau_t^{-1}-\tau_t^{-2} (m_\pi + dR^2)\vert \right\rbrace,
    \end{equation*}
    where $\tau_t^2 = (1-\lambda_t)\sigma^2 + \lambda_t\tau^2$. This completes the proof.
\end{proof}

We note that in general, \Cref{assumption:compact_plus_gaussian_target_1} does not imply strong convexity outside of a ball. 
For example, consider the following example in $\mathbb{R}^2$. Let $\pid = \Tilde{\pi} * \gamma$ and
$\Tilde{\pi} = \frac{1}{2}(\delta_{y_1} +\delta_{y_2})$, where $y_i = (0, (-1)^iR)$. Consider a point in $\mathbb{R}^2$ of the form $(x, 0)$. We have that the conditional measure $\rho_{(x,0)}$, where $\rho_x(y) \propto \Tilde{\pi}(y)\gamma(x-y)$, satisfies
\begin{equation*}
    \rho_{(x,0)}(y) = \Tilde{\pi}(y).
\end{equation*}
Therefore, the covariance term in the expression of the Hessian in \eqref{eq:covariance_compact_plus_gaussian_hessian} is given by
\begin{equation*}
    \text{Cov}_{\rho_{(x,0)}}(Y) = R^2\left(\begin{matrix}
        0& 0\\
        0& 1
    \end{matrix}\right).
\end{equation*}
Substituting this into the expression of the Hessian, it follows that
\begin{equation*}
    -\nabla^2\log\pid ((x, 0)) = \frac{I}{\tau^2} - \frac{R^2}{\tau^4}\left(\begin{matrix}
        0& 0\\
        0& 1
    \end{matrix}\right).
\end{equation*}
Taking $R$ to be greater than $\tau$, we have that  $-\nabla^2\log\pid$ evaluated at points on the axis $x=0$ is not positive definite, meaning that $\pid$ cannot be strongly convex outside of any ball. 
Under the additional assumption that the support of the compactly supported measure $\Tilde{\pi}$ is convex and dense in its ambient space, \Cref{assumption:compact_plus_gaussian_target_1} implies \Cref{assumption:grad_log_lipschitzness_convexity_outside_of_a_ball}.
This result is formalised in the following lemma.
\begin{lemma}
    Let $\pid=\Tilde{\pi}*\gamma\in\mathcal{P}(\mathbb{R}^d)$ satisfy assumption \Cref{assumption:compact_plus_gaussian_target_1}. If the support of $\Tilde{\pi}$ is convex and dense in $\mathbb{R}^d$, then $\pid$ satisfies assumption \Cref{assumption:grad_log_lipschitzness_convexity_outside_of_a_ball}.
\end{lemma}
\input{appendix/strong_convexity_outside_ball}
 
We now show that a mixture of Gaussians with different covariances satisfies assumption \Cref{assumption:grad_log_lipschitzness_convexity_outside_of_a_ball} under mild conditions, but it does not generally satisfy assumption \Cref{assumption:compact_plus_gaussian_target_1}.

\begin{lemma}\label{lemma:example_mixture_gaussians_satisfies_assumption}
Let $\pi=\sum_{i=1}^M w_i p_i$ be a mixture of Gaussians in $\mathbb{R}^d$ where $w_i$ and $p_i$ denote the weight and the probability density function, respectively, of the $i$-th component of the mixture which has mean $\mu_i$ and covariance $\Sigma_i$. If for any pair $\{i, j\}$ with $\Sigma_i\neq \Sigma_j$ there exists a unit vector $u$ such that
\begin{equation*}
    u^\intercal\left(\Sigma_i^{-1}-\Sigma_j^{-1}\right)u = 0,
\end{equation*}
but one of the following conditions hold
\begin{enumerate}
    \item[(i)] $u$ is an eigenvector of $\left(\Sigma_i^{-1}-\Sigma_j^{-1}\right)$ with eigenvalue $0$.
    \item[(ii)] $u^\intercal\left(\Sigma_i^{-1}\mu_i-\Sigma_j^{-1}\mu_j\right)\neq 0$.
    \item[(iii)] There exists $k\in\{1, \dots, M\}$ such that $ u^\intercal\left(\Sigma_i^{-1}-\Sigma_k^{-1}\right)u>0$ or $ u^\intercal\left(\Sigma_j^{-1}-\Sigma_k^{-1}\right)u>0$.
\end{enumerate}
Then $\nabla \log \pi$ is Lipschitz. Moreover, if also for any pair $\{i, j\}$ with $\Sigma_i\mu_i\neq \Sigma_j\mu_j$ there exists a unit vector $u$ such that $u^\intercal\left(\Sigma_i^{-1}-\Sigma_j^{-1}\right)u = 0$ but either condition $(ii)$ or $(iii)$ hold,
then $\pi$ satisfies assumption \Cref{assumption:grad_log_lipschitzness_convexity_outside_of_a_ball}.
\end{lemma}
Before presenting the proof, we observe that in one dimension, a mixture of Gaussians with different variances always satisfies the condition stated in the previous lemma, and thus assumption \Cref{assumption:grad_log_lipschitzness_convexity_outside_of_a_ball} holds.
\begin{proof}
We first establish that $\nabla\log\pi$ is Lipschitz continuous by bounding the spectral norm of the Hessian $\nabla^2 \log\pi$. We have the following expressions for $\nabla\log\pi$ and $\nabla^2 \log\pi$
\begin{equation*}
    \nabla\log\pi = \frac{\sum_i w_i p_i \nabla\log p_i}{\sum_i w_i p_i}
\end{equation*}
\begin{align}
    \nabla^2\log\pi & = \frac{\sum_i w_i \nabla^2 p_i}{\sum_i w_i p_i} -\frac{(\sum_i w_i p_i \nabla\log p_i) (\sum_i w_i p_i \nabla\log p_i^\intercal)}{(\sum_i w_i p_i)^2}.\label{eq:hessian_expression_gaussian_mixture_different_covariances}
\end{align}
Observe that 
\begin{equation*}
    \nabla^2 p_i =  p_i \nabla^2\log p_i + p_i  \nabla\log p_i (\nabla\log p_i)^{\intercal}.
\end{equation*}
Substituting this we have
\begin{align*}
    -\nabla^2\log\pi  =&  \frac{\sum_i w_i p_i \Sigma_i^{-1}}{\sum_i w_i p_i}-\frac{\sum_{i,j} w_iw_j p_i p_j [\nabla\log p_i (\nabla\log p_i)^\intercal-\nabla\log p_i (\nabla\log p_j)^\intercal]}{(\sum_i w_i p_i)^2} \\
    =&  \frac{\sum_i w_i p_i \Sigma_i^{-1}}{\sum_i w_i p_i}-\frac{\sum_{i,j} w_iw_j p_i p_j [\Sigma_i^{-1}(x-\mu_i)(x-\mu_i)^\intercal\Sigma_i^{-1} -\Sigma_i^{-1}(x-\mu_i)(x-\mu_j)^\intercal\Sigma_j^{-1}]}{(\sum_i w_i p_i)^2}\\
    =&  \frac{\sum_i w_i p_i \Sigma_i^{-1}}{\sum_i w_i p_i}-\frac{\sum_{i,j} w_iw_j p_i p_j [\Sigma_i^{-1}xx^\intercal(\Sigma_i^{-1}-\Sigma_j^{-1}) +\frac{1}{2}(\Sigma_i^{-1}\mu_i-\Sigma_j^{-1}\mu_j)(\Sigma_i^{-1}\mu_i-\Sigma_j^{-1}\mu_j)^\intercal]}{(\sum_i w_i p_i)^2}\\
    & + \frac{1}{2} \frac{\sum_{i,j} w_iw_j p_i p_j [(\Sigma_i^{-1}\mu_i -\Sigma_j^{-1}\mu_j)x^\intercal(\Sigma_i^{-1}-\Sigma_j^{-1}) + (\Sigma_i^{-1} -\Sigma_j^{-1})x(\Sigma_i^{-1}\mu_i -\Sigma_j^{-1}\mu_j)^\intercal]}{(\sum_i w_i p_i)^2}\\
    =&  \frac{\sum_i w_i p_i \Sigma_i^{-1}}{\sum_i w_i p_i}-\frac{1}{2}\frac{\sum_{i,j} w_iw_j p_i p_j [(\Sigma_i^{-1}-\Sigma_j^{-1})xx^\intercal(\Sigma_i^{-1}-\Sigma_j^{-1}) +(\Sigma_i^{-1}\mu_i-\Sigma_j^{-1}\mu_j)(\Sigma_i^{-1}\mu_i-\Sigma_j^{-1}\mu_j)^\intercal]}{(\sum_i w_i p_i)^2}\\
     & + \frac{\sum_{i,j} w_iw_j p_i p_j [\Sigma_i^{-1}\mu_ix^\intercal(\Sigma_i^{-1}-\Sigma_j^{-1}) + (\Sigma_i^{-1} -\Sigma_j^{-1})x\mu_i^\intercal \Sigma_i^{-1}]}{(\sum_i w_i p_i)^2}.
\end{align*}
Note that in the case of equal covariances $(\Sigma_i = \Sigma_j)$ the terms involving $x$ cancel out. Since the covariance matrices satisfy $\sigma_{i,\min} I\preccurlyeq\Sigma_i\preccurlyeq \sigma_{i,\max} I$ and the norm of the means $\Vert \mu_i\Vert$ is finite for all $i$, the following terms of the previous expression 
\begin{align*}
    A_x + B_x = \frac{\sum_i w_i p_i \Sigma_i^{-1}}{\sum_i w_i p_i}-\frac{1}{2 }\frac{\sum_{i,j} w_iw_j p_i p_j (\Sigma_i^{-1}\mu_i-\Sigma_j^{-1}\mu_j)(\Sigma_i^{-1}\mu_i-\Sigma_j^{-1}\mu_j)^\intercal}{(\sum_i w_i p_i)^2}
\end{align*}
are uniformly bounded above and below for all $x$.
We now focus on the remaining terms which can be rewritten as
\begin{equation*}
     C_x = -\frac{1}{4}\frac{\sum_{i,j} w_iw_j p_i p_j [M_x + M_x^{\intercal}]}{(\sum_i w_i p_i)^2}, \quad M_x = (\Sigma_i^{-1}-\Sigma_j^{-1})x\left[x^\intercal(\Sigma_i^{-1}-\Sigma_j^{-1}) -4 \mu_i^{\intercal}\Sigma_i^{-1}\right]
\end{equation*}
aiming to establish an upper bound for the spectral norm of $C_x$ when $\Vert x\Vert$ tends to $\infty$. Hence, from this point onwards, we consider $x$ such that $\Vert x \Vert>\max_i{\Vert \mu_i \Vert}$.
Using the triangle inequality and the submultiplicativity property of the spectral norm we obtain
\begin{align}
    &\quad\quad\quad\quad\quad\Vert C_x\Vert_2 \leq \frac{1}{4}\sum_{i, j}\frac{ w_iw_j p_i p_j }{(\sum_i w_i p_i)^2}\Vert M_x + M_x^{\intercal} \Vert_2 \label{eq:sum_spectral_norm}\\
    \Vert M_x + M_x^{\intercal} \Vert_2 &\leq  2\left\Vert\Sigma_i^{-1}-\Sigma_j^{-1}\right\Vert_2^2 \Vert x x^\intercal\Vert_2 + 4 \left\Vert\Sigma_i^{-1}-\Sigma_j^{-1}\right\Vert_2
    \left\Vert\Sigma_i^{-1}\right\Vert_2\left\Vert x\mu_i^{\intercal} + \mu_ix^{\intercal}\right\Vert_2 \nonumber
    \\&\leq 2\Vert x\Vert^2\left(\left\Vert\Sigma_i^{-1}-\Sigma_j^{-1}\right\Vert_2^2 + 4 \left\Vert\Sigma_i^{-1}-\Sigma_j^{-1}\right\Vert_2
    \left\Vert\Sigma_i^{-1}\right\Vert_2\right) \leq 10\Vert x\Vert^2\left(\left\Vert\Sigma_i^{-1}\right\Vert_2+ \left\Vert\Sigma_j^{-1}\right\Vert_2\right)^2.\nonumber
\end{align}
Let us define the following sets
\begin{align*}
   D_1 &= \{\{i, j\}|1\leq i, j\leq M, \Sigma_i\neq \Sigma_j \;\text{and}\; \nexists\; u\; |\;\Vert u\Vert = 1\;\text{and}\; u^\intercal\left(\Sigma_i^{-1}-\Sigma_j^{-1}\right)u = 0\},\\
   D_2 &= \{\{i, j\}|1\leq i, j\leq M, \Sigma_i\neq \Sigma_j \;\text{and}\; \exists\; u\; |\;\Vert u\Vert = 1\;\text{and}\; u^\intercal\left(\Sigma_i^{-1}-\Sigma_j^{-1}\right)u = 0\;\text{and}\; (i)\;\text{holds}\},\\
   D_3 &= \{\{i, j\}|1\leq i, j\leq M, \Sigma_i\neq \Sigma_j \;\text{and}\; \exists\; u\; |\;\Vert u\Vert = 1\;\text{and}\; u^\intercal\left(\Sigma_i^{-1}-\Sigma_j^{-1}\right)u = 0\;\text{and}\; (ii)\;\text{holds}\},\\
   D_4 &= \{\{i, j\}|1\leq i, j\leq M, \Sigma_i\neq \Sigma_j \;\text{and}\; \exists\; u\; |\;\Vert u\Vert = 1\;\text{and}\; u^\intercal\left(\Sigma_i^{-1}-\Sigma_j^{-1}\right)u = 0\;\text{and}\; (iii)\;\text{holds}\},
\end{align*}
We also consider the partition of the unit sphere 
$S^{d-1}\subset \mathbb{R}^d$ into disjoint subsets
\begin{equation*}
    S^{d-1} = P_{+}^{(j, i)} \cup P_{-}^{(j, i)} \cup P_{0}^{(j, i)},
\end{equation*}
where $P_{+}^{(j, i)} = \{u\in S^{d-1}| u^{\intercal}(\Sigma_j^{-1}-\Sigma_i^{-1})u>0\}$, with $P_{-}^{(j, i)} $ and $ P_{0}^{(j, i)}$ defined analogously. 

We analyse the terms in the sum \eqref{eq:sum_spectral_norm} separately, depending on the set  $D_k$ to which the pair $\{i, j\}$ belongs. 
\begin{enumerate}
    \item[(1)] Consider the pairs $\{i, j\}\in D_1$, it follows that
\begin{align*}
    \sum_{\{i, j\}\in D_1}\frac{ w_iw_j p_i p_j }{(\sum_i w_i p_i)^2}\Vert M_x + M_x^{\intercal} \Vert_2  \leq 10 \max_{\{i, j\}\in D_1}\left\{\left(\left\Vert\Sigma_i^{-1}\right\Vert_2+ \left\Vert\Sigma_j^{-1}\right\Vert_2\right)^2\right\}\sum_{\{i,j\}\in D_1}\frac{ w_iw_j p_i p_j }{(\sum_i w_i p_i)^2}\Vert x\Vert^2.
\end{align*}
To analyse each term in the previous sum, we first consider the case where one  covariance matrix majorises the other. Specifically, without loss of generality, we assume that for the pair $\{i,j\}\in D_1$ we have $\Sigma_i\succ \Sigma_j$, which implies $\Sigma_{j}^{-1}-\Sigma_i^{-1}\succ \alpha I$ for some $\alpha>0$. We observe that 
\begin{align}
 - \frac{1}{2} x^{\intercal}\left(\Sigma_j^{-1}-\Sigma_i^{-1}\right)x + x^{\intercal}\left(\Sigma_j^{-1}\mu_j-\Sigma_i^{-1}\mu_i\right) &\leq -\frac{1}{2} \alpha  \Vert x\Vert^2 + 2 \Vert x\Vert \left(\left \Vert \Sigma_j^{-1/2}\right \Vert_2^2 \Vert \mu_j\Vert + \left \Vert \Sigma_i^{-1/2}\right\Vert_2^2 \Vert \mu_i\Vert   \right) \nonumber\\
 &\leq -\frac{1}{2} \alpha  \Vert x\Vert^2 + 2 \Vert x\Vert \left(\sigma_{j, \min}^{-1} \Vert \mu_j\Vert + \sigma_{i, \min}^{-1} \Vert \mu_i\Vert   \right), \label{eq:mixture_gaussians_term_bound_1}
\end{align}
which gives
\begin{align}
    \frac{ w_iw_j p_i p_j  \Vert x\Vert^2}{(\sum_i w_i p_i)^2}&\leq \frac{ w_iw_j p_i p_j \Vert x\Vert^2}{(w_i p_i)^2} =\frac{ w_j p_j  \Vert x\Vert^2}{w_i p_i}\nonumber\\
    &=  \frac{w_j \det(\Sigma_i)^{1/2}}{w_i \det(\Sigma_j)^{1/2}}  e^{-\frac{1}{2}\left(\mu_j^\intercal \Sigma_j^{-1} \mu_j-\mu_i^\intercal \Sigma_i^{-1} \mu_i\right)} \Vert x\Vert^2 e^{-\frac{1}{2} x^{\intercal}\left(\Sigma_j^{-1}-\Sigma_i^{-1}\right)x + x^{\intercal}\left(\Sigma_j^{-1}\mu_j-\Sigma_i^{-1}\mu_i\right)}  \xrightarrow{\Vert x\Vert \to \infty} 0.\label{eq:mixture_gaussians_term_bound_2}
\end{align}
On the other hand, when $\Sigma_j^{-1}-\Sigma_i^{-1}$ is neither positive-definite nor negative-definite, for every $x\in\mathbb{R}^d$ we can write $x = \Vert x\Vert u$, where $u$ is a unit vector satisfying $u\in P_{+}^{(j, i)}$ or $u\in P_{-}^{(j, i)}$ because $P_0^{(j, i)}$ is empty by definition for $\{i, j\}\in D_1$.
These two cases can be treated simultaneously since the indices $i, j$ are interchangeable. Without loss of generality, we assume that $u\in P_{+}^{(j, i)}$. Following a similar approach to equations \eqref{eq:mixture_gaussians_term_bound_1} and \eqref{eq:mixture_gaussians_term_bound_2}, we have 
    \begin{align*}
        \frac{ w_iw_j p_i p_j  \Vert x\Vert^2}{(\sum_i w_i p_i)^2}\leq \frac{ w_iw_j p_i p_j \Vert x\Vert^2}{(w_i p_i)^2} =\frac{ w_j p_j  \Vert x\Vert^2}{w_i p_i} \xrightarrow{\Vert x\Vert \to \infty} 0.
    \end{align*}
Therefore, for every unit vector $u\in S^{d-1}$, the limit of each term in the sum over $\{i, j\}\in D_1$  along the line $\Vert x\Vert u$ is zero as $\Vert x\Vert$ tends to $\infty$. Since the sum contains finitely many terms, this implies  
\begin{equation*}
    \lim_{\Vert x\Vert\to\infty} f_{D_1}(\Vert x\Vert u) = \lim_{\Vert x\Vert\to\infty}\sum_{\{i, j\}\in D_1}\frac{ w_iw_j p_i(\Vert x\Vert u) p_j(\Vert x\Vert u)}{(\sum_i w_i p_i(\Vert x\Vert u))^2}\left\Vert M_{\Vert x\Vert u} + M_{\Vert x\Vert u}^{\intercal} \right\Vert_2  = 0.
\end{equation*}
Since $f_{D_1}$ is a continuous function and $S^{d-1}$ is compact, the behaviour of $f_{D_1}$ can be controlled uniformly across all directions. That is, for every $\varepsilon>0$ there exists $R>0$ such that $\Vert x\Vert>R$ implies
\begin{equation*}
    \left\vert\sum_{\{i, j\}\in D_1}\frac{ w_iw_j p_i p_j }{(\sum_i w_i p_i)^2}\Vert M_x + M_x^{\intercal} \Vert_2\right\vert  <\varepsilon.
\end{equation*}
\item[(2)] For $\{i,j\}\in D_2\cup D_3 \cup D_4$, following the same reasoning as above, the limit of the spectral norm of each term when $\Vert x\Vert$ tends to $\infty$ along the directions $u\in P_{+}^{(j, i)}$ or $u\in P_{-}^{(j, i)}$ is $0$. 
To analyse the limit along the directions $u\in P_{0}^{(j, i)}$, we consider the following cases.
\begin{itemize}
    \item $\{i, j\} \in D_2$. For every $x$ such that $x = \Vert x\Vert u$ with $u\in P_{0}^{(j, i)}$, we have that $\left(\Sigma_i^{-1}-\Sigma_j^{-1}\right) u = 0_d$. Consequently, $M_x = 0_{d\times d}$, which demonstrates that the limit along these directions is $0$.
    \item $\{i, j\} \in D_3$. Take $u\in P_{0}^{(j, i)}$, by condition $(ii)$ we have that either $u^\intercal\left(\Sigma_j^{-1}\mu_j-\Sigma_i^{-1}\mu_i\right)<0$ or $u^\intercal\left(\Sigma_j^{-1}\mu_j-\Sigma_i^{-1}\mu_i\right)> 0$. Without loss of generality, assume that $u^\intercal\left(\Sigma_j^{-1}\mu_j-\Sigma_i^{-1}\mu_i\right)< 0$. Then for every $x = \Vert x\Vert u$ we have
\small
\begin{align*}
    &\frac{ w_iw_j p_i p_j }{(\sum_i w_i p_i)^2}\Vert M_x + M_x^{\intercal} \Vert_2 \leq 10 \left(\left\Vert\Sigma_i^{-1}\right\Vert_2+ \left\Vert\Sigma_j^{-1}\right\Vert_2\right)^2 \frac{w_j p_j }{w_i p_i}\Vert x \Vert^2\\
    &\leq 10 \left(\left\Vert\Sigma_i^{-1}\right\Vert_2+ \left\Vert\Sigma_j^{-1}\right\Vert_2\right)^2 \frac{w_j \det(\Sigma_i)^{1/2}}{w_i \det(\Sigma_j)^{1/2}}  e^{-\frac{1}{2}\left(\mu_j^\intercal \Sigma_j^{-1} \mu_j-\mu_i^\intercal \Sigma_i^{-1} \mu_i\right)} \Vert x\Vert^2 e^{\Vert x\Vert u^{\intercal}\left(\Sigma_j^{-1}\mu_j-\Sigma_i^{-1}\mu_i\right)}  \xrightarrow{\Vert x\Vert \to \infty} 0.
\end{align*}
\normalsize



    
    \item $\{i, j\} \in D_4$. Take $u\in P_{0}^{(j, i)}$, by condition $(iii)$, there exists $k$ such that $u\in P_{+}^{(j, k)}$ or $u\in P_{+}^{(i, k)}$. These two cases are symmetric and can be treated together. Without loss of generality, assume $u\in P_{+}^{(j, k)}$. In this case, following a similar argument to those in equations \eqref{eq:mixture_gaussians_term_bound_1} and \eqref{eq:mixture_gaussians_term_bound_2}, we have that for every $x = \Vert x\Vert u$
\begin{equation*}
    \frac{ w_iw_j p_i p_j }{(\sum_i w_i p_i)^2}\Vert M_x + M_x^{\intercal} \Vert_2 \leq 10 \left(\left\Vert\Sigma_i^{-1}\right\Vert_2+ \left\Vert\Sigma_j^{-1}\right\Vert_2\right)^2 \frac{w_j p_j }{w_k p_k}\Vert x \Vert^2 \xrightarrow{\Vert x\Vert \to \infty} 0.
\end{equation*}
\end{itemize}
\end{enumerate} 
Since the sum in equation \eqref{eq:sum_spectral_norm} contains a finite number of terms, we have that for every $u\in S^{d-1}$
\begin{equation*}
    \lim_{\Vert x\Vert\to\infty}\left\Vert C_{\Vert x\Vert u}\right\Vert_2  = 0.
\end{equation*}
Furthermore, because the function $\Vert C_x\Vert_2$ is continuous and $S^{d-1}$ is compact, the limit $\lim_{\Vert x\Vert\to\infty}\left\Vert C_{x}\right\Vert_2$ exists and is equal to zero.
Consequently, $\Vert C_x\Vert_2$ is bounded for all $x\in\mathbb{R}^d$, which concludes that $\nabla\log\pi$ is Lipschitz.

To complete the proof, we need show that $-\nabla\log\pi$ is strongly convex outside of a ball of radius $r$. Using the same technique as above, we analyse the spectral norm of $B_x$. Let us define the following sets
\begin{align*}
   D_5 &= \{\{i, j\}|1\leq i, j\leq M, \Sigma_i\mu_i\neq \Sigma_j\mu_j \;\text{and}\; \nexists\; u\; |\;\Vert u\Vert = 1\;\text{and}\; u^\intercal\left(\Sigma_i^{-1}-\Sigma_j^{-1}\right)u = 0\},\\
   D_6 &= \{\{i, j\}|1\leq i, j\leq M, \Sigma_i\mu_i\neq \Sigma_j \mu_j\;\text{and}\; \exists\; u\; |\;\Vert u\Vert = 1\;\text{and}\; u^\intercal\left(\Sigma_i^{-1}-\Sigma_j^{-1}\right)u = 0\;\text{and}\; (ii)\;\text{holds}\},\\
   D_7 &= \{\{i, j\}|1\leq i, j\leq M, \Sigma_i\mu_i\neq \Sigma_j \mu_j\;\text{and}\; \exists\; u\; |\;\Vert u\Vert = 1\;\text{and}\; u^\intercal\left(\Sigma_i^{-1}-\Sigma_j^{-1}\right)u = 0\;\text{and}\; (iii)\;\text{holds}\},
\end{align*}
it follows that
\begin{equation*}
    \Vert B_x\Vert_2 \leq \max_{\{i, j\}\in D_5\cup D_6\cup D_7} \left\Vert \Sigma_i^{-1}\mu_i-\Sigma_j^{-1}\mu_j \right\Vert^2 \sum_{\{i, j\}\in D_5\cup D_6\cup D_7} \frac{ w_iw_j p_i p_j}{(\sum_i w_i p_i)^2}.
\end{equation*}
By applying the same reasoning as above, we find that for each pair in the sum 
\begin{equation*}
    \lim_{\Vert x\Vert\to\infty}\frac{ w_iw_j p_i p_j}{(\sum_i w_i p_i)^2} = 0.
\end{equation*}
Since there is a finite number of pairs $\{i,j\}$ in $D_5\cup D_6\cup D_7$, we can conclude that 
\begin{equation*}
    \lim_{\Vert x\Vert\to\infty}\Vert B_x\Vert_2 = 0.
\end{equation*}
Thus, as $\Vert x\Vert$ tends to $\infty$, the only term whose spectral norm does not vanish is
\begin{equation*}
    A_x = \frac{\sum_i w_i p_i \Sigma_i^{-1}}{\sum_i w_i p_i}\succcurlyeq \frac{\sum_i w_i p_i \sigma_{i, \max}^{-1}}{\sum_i w_i p_i} I \succcurlyeq \min_i\{\sigma_{i, \max}^{-1}\} I.
\end{equation*}
Therefore, we can conclude that $\pi$ is strongly log-concave outside of a ball, and hence satisfies assumption \Cref{assumption:grad_log_lipschitzness_convexity_outside_of_a_ball}.
\end{proof}
\begin{remark}\label{remark:counter_example_mixture_gaussian}
A concurrent work \citep{gentilonisilveri2025logconcavityscoreregularityimproved} examines the smoothness of a mixture of Gaussians with covariances of the form $\Sigma_i=\sigma_i^2 I$, a specific case that satisfies the assumption of Lemma~\ref{lemma:example_mixture_gaussians_satisfies_assumption}. 
However, their result does not extend to the case of general covariance matrices. 
In particular, the following example, which falls outside the assumptions of Lemma~\ref{lemma:example_mixture_gaussians_satisfies_assumption}, serves as a counterexample. Let $\pi = \frac{1}{2}(p_1 + p_2)$, where $p_i=\mathcal{N}(\mu, \Sigma_i)$ with $\mu=(1,0)$ and 
\begin{equation*}
    \Sigma_1^{-1} = \begin{pmatrix}
        2 & 1\\
        1& 2\\
    \end{pmatrix}, \quad 
    \Sigma_2^{-1} = \begin{pmatrix}
        2 & 0\\
        0& 3\\
    \end{pmatrix}.
\end{equation*}
We can show that $\nabla^2\log\pi((x, 0))$ is unbounded when $\Vert x\Vert\to\infty$. To establish this, first note that 
\begin{equation*}
    p_1((x,0)) = (2\pi\sqrt{3})^{-1} e^{-(x-1)^2},\quad p_2((x,0)) = (2\pi\sqrt{6})^{-1} e^{-(x-1)^2}.
\end{equation*}
Substituting this into the expression of the Hessian given above \eqref{eq:hessian_expression_gaussian_mixture_different_covariances} we have
\begin{align*}
    -\nabla^2\log\pi((x,0)) = \frac{1}{1+\sqrt{2}}\begin{pmatrix}
        2\sqrt{2} +1&\sqrt{2}\\
        \sqrt{2}& 2\sqrt{2} +3
    \end{pmatrix}- (x-1)^2\frac{\sqrt{2}}{3 + \sqrt{2}}\begin{pmatrix}
        0&0\\
        0& 1
    \end{pmatrix},
\end{align*}
which is clearly unbounded as $\Vert x\Vert\to\infty$, meaning that $\nabla\log\pi$ is not Lipschitz continuous.
\end{remark}
Besides, in general a mixture of Gaussians with different covariances does not satisfy assumption \Cref{assumption:compact_plus_gaussian_target_1}. 
\begin{lemma}\label{lemma:d_mixture_gaussians_not_expressed_as_convoltuion_with_compactly_supported}
Let $\pi = \sum_{i=1}^M w_i\mathcal{N}(\mu_i, \Sigma_i)$ be a mixture of Gaussians in $\mathbb{R}^d$. If either one of the two following assumptions holds
\begin{enumerate}
    \item[(i)] There exists at least one covariance matrix $\Sigma_i$ that cannot be expressed as  $\Sigma_i= \sigma_i I$.
    \item[(ii)] There exists at least one pair $\{i, j\}$ such that $\Sigma_i\neq \Sigma_j$.
\end{enumerate}
Then, $\pi$ does not satisfy assumption \Cref{assumption:compact_plus_gaussian_target_1}.
\end{lemma}
\begin{proof}
We want to determine if $\pi(x)$ can be written as
\begin{equation*}
    \pi(x) = (h*\gamma)(x),
\end{equation*}
where $h$ is a compactly supported measure and $\gamma$ is a Gaussian distribution $\gamma = \mathcal{N}(0, \tau^2 I)$ for some $\tau^2$. 
Assume that $\pi(x) = (h*\gamma)(x)$, we will show that $h$ cannot be compactly supported for any $\tau^2$.

Since the convolution in real space corresponds to multiplication in Fourier space, we have
\begin{equation*}
    \hat{\pi}(k) = \hat{h}(k)\hat{\gamma}(k)
\end{equation*}
where $ \hat{\pi}(k), \hat{h}(k), \hat{\gamma}(k)$ denote the respective Fourier transforms, which have the following expressions
\begin{equation*}
    \hat{\pi}(k) = \sum_{i=1}^M w_i e^{-\frac{1}{2}k^{\intercal}\Sigma_i k - i \mu_i^{\intercal}k}, \quad  \hat{\gamma}(k) = e^{-\frac{1}{2}\tau^2 k^\intercal I k}.
\end{equation*}
Then, the function $\hat{h}(k)$ has to satisfy
\begin{equation*}
    \hat{h}(k) = \frac{\hat{\pi}(k) }{\hat{\gamma}(k)} = \sum_{i=1}^M w_i e^{-\frac{1}{2}k^\intercal (\Sigma_i-\tau^2 I) k- i \mu_i^\intercal k} .
\end{equation*}
Note that $\tau^2$ needs to satisfy $\tau^2 I\preccurlyeq \Sigma_i$ for $i = 1, \dots, M$ as otherwise the inverse Fourier transform of $\hat{h}$ would not yield a real-valued function. 
Under this condition, we have that 
\begin{equation*}
    h(x) = \sum_{i=1}^M w_i \mathcal{N}(\mu_i, \Sigma_i-\tau^2 I) 
\end{equation*}
and since, by assumption, there exist either an index $i$ such that $\Sigma_i \neq \sigma_i I$, or a pair $\{i, j\}$ such that $\Sigma_i \neq \Sigma_j$, then $h$ cannot be compactly supported for any choice of $\tau^2$.

Note that when neither condition $(i)$ nor $(ii)$ holds, we can take $h = \sum_{i=1}^M w_i\delta_{\mu_i}$, where $\delta_{\mu_i}$ denotes a Dirac delta function centred at $\mu_i$.
\end{proof}


One final implication of assumption \Cref{assumption:grad_log_lipschitzness_convexity_outside_of_a_ball} is provided in the following proposition.
\begin{proposition}[Proposition 1 \citep{doi:10.1073/pnas.1820003116}]\label{lemma:assumption_implies_LSI}
    If  $\pid$ satisfies assumption \Cref{assumption:grad_log_lipschitzness_convexity_outside_of_a_ball}, then it has a finite log-Sobolev constant $C_{\text{LSI}, \pid}\leq \frac{2}{M_\pi} e^{16 L_\pi r^2}$.
\end{proposition}
\begin{proof}
Recall that under \Cref{assumption:grad_log_lipschitzness_convexity_outside_of_a_ball} we have that $\pi_{\text{data}} \propto e^{-V_\pi}$ satisfies
     \begin{equation*}
         \inf_{\Vert x\Vert \geq r} \nabla^2 V_\pi \succcurlyeq M_{\pi} I, \quad -L_{\pi} I\preccurlyeq \nabla^2 V_\pi \preccurlyeq L_{\pi} I.
     \end{equation*}
By \citet[Lemma 1]{doi:10.1073/pnas.1820003116}, there exists $\Tilde{V}_\pi\in C^1(\mathbb{R}^d)$ such that $\Tilde{V}_\pi$ is $M_\pi/2$ strongly convex on $\mathbb{R}^d$ and has a Hessian $\nabla^2 \Tilde{V}_\pi$ that exists everywhere on $\mathbb{R}^d$. Therefore, using the Bakry-Ã‰mery criterion \citep{bakry_emery}, $\Tilde{\pi}\propto e^{-\Tilde{V}_\pi}$ satisfies log-Sobolev inequality with constant $C_{\text{LSI}, \Tilde{\pi}}\leq 2/M_\pi$. 
Moreover, Lemma 1 in \citet{doi:10.1073/pnas.1820003116} also guarantees that
\begin{equation*}
    \sup \left(\Tilde{V}_\pi(x) - V_\pi\right) - \inf\left(\Tilde{V}_\pi(x) - V_\pi\right) \leq 16 L_\pi r^2.
\end{equation*}
Applying  the Holley-Stroock perturbation principle \citep{RefWorks:RefID:85-holley1987logarithmic}, it follows that $\pid$ has a finite log-Sobolev constant satisfying 
\begin{equation*}
    C_{\text{LSI}, \pid}\leq\frac{2}{ M_{\pi}}e^{16 L_\pi r^2}.
\end{equation*}

\end{proof}

