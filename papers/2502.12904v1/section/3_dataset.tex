\section{The \ourbench~Benchmark}
% one round
% multi-round
% template
\subsection{An Overview of \ourbench}

We introduce \ourbench, a novel bilingual (English and Chinese) benchmark meticulously curated to evaluate the LLM's ability to defend against fraud and phishing inducement in five real-world scenarios: \textit{Fraudulent Service, Impersonation, Phishing Scams, Fake Job Posting and Online Relationship.} The detailed scenarios coverage and dataset statistics are presented in Table~\ref{tab:dataset_statistics}. 
\ourbench consists of a comprehensive collection of fraudulent cases, manually gathered from social media, news reports,\footnote{See \href{https://www.bbc.com/news/topics/cvwydw4g8pzt}{BBC Fraud News}} lecture materials, and prior single-task fraud datasets in Figure~\ref{fig:existing}. It consists of two subsets: \ourbasedata and \ourlevelupdatset. 
\ourbasedata is directly generated by a state-of-the-art reasoning LLM from our selected real-world fraud cases, while \ourlevelupdatset~ is a rule-based augmentation of the base dataset, designed for multi-round dialogue setting.  Section~\ref{sec:data_construct_process} provides a detailed explanation of our data construction pipeline, illustrated in Figure~\ref{fig:datagen-pipeline}.

Our primary goal is to evaluate the defensive capabilities of LLMs not only in fraud detection within Helpful Assistant settings where LLMs provide decision-making advice but also in role-playing scenarios, which are crucial for multi-agent LLM systems and personalized LLMs. To achieve this, we assess LLM performance in both single-turn and multi-turn interactions, introducing the Defense Success Rate (DSR) as a key metric to measure a model's resilience against attempts to refine fraudulent information. Further details on this evaluation framework are provided in Section~\ref{sec:evaflow}.

% Moreover, in Section~\ref{sec:compair}, we compare \ourbench with existing fraud benchmarks, highlighting its advancements over previous work and demonstrating its significance in enhancing the robustness of LLMs against diverse fraud tactics.
\input{figure/datacreation}
\input{table/datasetstatitic}
\subsection{Dataset Construction Process}
\label{sec:data_construct_process}

% \paragraph{Data Collection Pipeline.}
\noindent {\bf Data Collection Pipeline.}
Our benchmark collection process consists of three main stages. First, we filter real-world \textit{fraud cases} from existing datasets, news sources, social media platforms, and government lectures. 
The filtering criterion ensures that the selected cases are \textit{not ambiguous}, which was defined as if all of our data annotation researchers agree that it should not be classified as fraudulent data. For example, consider the following case from the Fake Job Posting dataset:
\begin{center}
\label{Fake Job Posting example}
\begin{tcolorbox}[colback=white, colframe=black, title= \textcolor{white}{\small An \textit{ambiguous} fake job posting example}, width=0.9\linewidth]
    \small ACCION is currently seeking a professional individual enthusiastic about ... . We are looking for someone focused on creating an exceptional customer service and shopping experience for \dots, modelling a positive customer service spirit, exhibiting a friendly and helpful attitude with customers and associates \dots \textbf{401k}, healthcare program, dental insurance, life insurance lots of benefits \dots
\end{tcolorbox}
\end{center}

This job posting presents uncertainty in determining whether it should be classified as fraudulent because the description \textit{lacks a clear fraudulent intent}—whether it aims to steal users' personal information, facilitate human trafficking \footnote{See \href{https://edition.cnn.com/2025/01/14/china/china-actor-thailand-scam-myanmar-intl-hnk/index.html}{A Chinese actor was abducted from Thailand}} or charge hidden service fees. Additionally, the mention of 
``401k'' benefits may be misleading, but their fraudulent nature is difficult to define, as benefits can vary depending on salary levels and employment conditions. To ensure the reliability of our dataset, we filter out all such ambiguous cases manually. As a result, we identify 146 distinct fraud cases with clear fraudulent intent, which we categorize into five main classes.

Secondly, after collecting all fraud cases, we manually extract a set of \textit{fraudulent strategies}~$\mathbf{FS} = \{ fs_1, fs_2, \dots, fs_n \}$ and the underlying \textit{fraudulent intentions}~$\mathbf{FI} = \{ fi_1, fi_2, \dots, fi_m \}$ from previous cases. For example, in the case of fake job postings, $\mathbf{FS}$ includes strategies such as:
Work-from-Home with Minimal Effort (as shown in Figure \ref{fig:dataoverview}); Unusual Application Process (e.g., hiring via messaging apps); Upfront Payments (e.g., requests for application or training fees); Suspicious Travel Benefits (e.g., fully paid international business trips to high-risk regions). The corresponding $\mathbf{FI}$ includes: Identity Theft (stealing personal data for fraud); Forced Labor or Human Trafficking (coercing victims into exploitative work conditions); and Organized Crime Recruitment (manipulating individuals into illicit activities). This extraction process enables a more systematic expansion of our dataset \textit{by incorporating key fraud patterns and underlying motivations.} This serves as a foundation for generating real-world-inspired fraud data \textit{with clear objectives and well-defined risks}. We list the
$\mathbf{FS}$ and $\mathbf{FI}$ for each fraud class in Appendix~\ref{app:Fraudulent Keys Extraction}.  Additionally, in this step, we extract the identity portrait of potential "victims" by performing entity extraction and analysis on fraud cases. This information is then used to construct the system prompt for our role-play setting in subsequent evaluations. 

After we extract $\mathbf{FS}$ and $\mathbf{FI}$, we use SoTA open-source reasoning LLM \texttt{Deepseek-R1}~\footnote{\url{https://www.deepseek.com/}} based on selected $\{(fs,fi)_k\}_{k=1}^K=\mathcal{S}(\mathbf{FS},\mathbf{FI})$ to generate a series of fraud data for us, where $\mathcal{S}$ denotes a human-curated selection process that ensures reasonable combinations.
The detailed prompting strategy used to elicit these responses from \texttt{Deepseek-R1} is presented in Appendix~\ref{app:prompt_basedata_elicit}. This process
results in a diverse bilingual collection of 2300 test samples for different categories. Meanwhile, we also compare other SoTA LLMs like \texttt{GPT-4o} for this data generation and discuss them in Appendix~\ref{app:Data Generation Comparison}.

\paragraph{Data Quality Control.}
% \noindent {\bf Data Quality Control.}
To ensure the quality of our synthetic data, we implement a three-stage data cleaning process. In the first stage, we observed that the generated fraud samples sometimes include risk warnings (e.g., 'This notification simulates real-world fraud prevention protocols for training purposes. All contact details are fictional but structurally valid'). We remove these warning messages to maintain the authenticity of the fraudulent intent in our dataset. In the second stage, we address placeholder text (e.g., ``\texttt{[Your University Name]}'') that appears in LLM-generated content. We manually review and replace these placeholders with contextually appropriate information, ensuring that elements such as email addresses, phone numbers, and physical addresses maintain consistent formatting throughout the dataset. This prevents the model from detecting fraudulent messages through simplistic pattern matching of placeholder text rather than fully understanding the situation. Finally, we check all the datasets we get and filter all \textit{ambiguous}
samples as we mentioned in the previous section, and after filter almost 7\% of our dataset, we got \ourbasedata denoted as $\mathcal{D}^{(0)}$, contains 2141 samples.

\paragraph{Rule-based Fraud Data Augmentation.}
% \noindent {\bf Rule-based Fraud Data Augmentation}
For a multi-round setting, where the ``victim'' requests additional information or further communication before making a final decision, we construct a \textit{level-up } dataset, \ourlevelupdatset, denoted as \( \mathcal{D}^{(i)}_{\text{level-up}} \), where \( i \) represents the \( i \)-th augmented dataset. Our augmentation pipeline follows the three-stage online fraud strategy: \textbf{Building Credibility (\(C\))}, \textbf{Creating Urgency (\(U\))}, and \textbf{Exploiting Emotional Appeal (\(E\))}.

First, for each sample \( s \in \mathcal{D}^{(0)} \), we instruct \texttt{Deepseek-R1} to augment it according to the \(C\) strategy. This involves incorporating \textit{additional background details, enhancing credibility, and injecting fabricated official information} into \( s \), yielding the first-level dataset \( \mathcal{D}^{(1)}\).  Next, for each sample in \( \mathcal{D}^{(1)} \), we apply the \(U\) strategy, introducing elements that impose \textit{time pressure or consequences for inaction}, resulting in the second-level dataset \( \mathcal{D}^{(2)} \).  Finally, we augment \( \mathcal{D}^{(2)} \) using the \(E\) strategy. This step adds emotionally compelling content designed to~\textit{ evoke empathy, trust, or a sense of obligation,} producing the final-level dataset \( \mathcal{D}^{(3)} \).  This structured augmentation process simulates \textit{real-world fraudulent interactions} by progressively refining deceptive strategies at each round of the conversation. The detailed Data Augmentation prompt is presented in Appendix~\ref{app:prompt_augmenteddata_elicit}.

\subsection{Evaluation Workflow}
\label{sec:evaflow}
\paragraph{Evaluation in Two Real-world Scenarios.} 


As illustrated in Figure~\ref{fig:evaflow}, we evaluate the robustness of large language models (LLMs) against fraud and phishing inducement in two widely used real-world settings: Role-play and Helpful Assistant. The Role-play setting is commonly employed in multi-agent systems and persona-based LLM research, whereas the Helpful Assistant setting involves LLMs providing advice before users make decisions. 

\paragraph{Evaluation in Multi-round Fraud.}
In both settings, we assess LLM performance in a multi-round fraud inducement. First, the``victim'' LLM generates a response to an initial fraud sample \( s^{(0)}_i \in \mathcal{D}^{(0)} \). We then employ \texttt{GPT-4o-mini} as a judge model~\citep{zheng2023judging, gu2024survey}(see Appendix~\ref{app:LLM as a judge} for more information), which evaluates the response and categorizes it into one of the following three outcomes: Success Identified Fraud (i.e., the LLM successfully identified and rejected the fraudulent request); Failure Against Fraud (i.e., the LLM was misled by the fraudulent request); More Details Needed (i.e., the LLM requests additional clarification before making a judgment). We also provide a detailed study of the trustfulness of LLM to judge response in Appendix~\ref{app:Human vs LLM evaluation} If the ``victim'' LLM are requesting more details, we provide it with the corresponding sample \( s^{(1)}_i \in \mathcal{D}^{(1)}\) from our \ourlevelupdatset dataset. This iterative process continues with samples \( s^{(2)}_i \) and  \( s^{(3)}_i \) from increasingly challangeing  \( \mathcal{D}^{(2)}\)  and  \( \mathcal{D}^{(3)}\) respectively. If the model repeatedly requests additional details without ultimately identifying the fraudulent of the request after four rounds of conversation, we also classify this as a failure in defending against this sample.

\paragraph{Evaluation Metric.}
We introduce the Defense Success Rate (\(\text{DSR}\)) as a metric for evaluating LLM robustness against fraud requests. For each sample \( s^{(0)}_i \in \mathcal{D}^{(0)} \), if the model successfully identifies the fraud inducement in all four conversation rounds, we classify it as a ``Defense Success''. Thus, the mathematical formulation of 
\(\text{DSR}\) can be given by:

\[
\text{DSR} = \frac{|\{ s_i \mid \text{Defense Success}~ s_i \}|}{|\mathcal{D}^{(0)}|}
\]

To better analyze the model's ability to identify different fraudulent argumentation strategies in multi-round conversations, we use \(\text{DSR}@k\) and \(\text{AVG}(k)\)as evaluation metrics. \(\text{DSR}@k\) represents the probability of ``Defense Success'' until the \(k\)-th round of a fraud conversation, measured across all samples. Specifically, \( k \in \{ 0,1,2,3\} \), where \( k=0 \) corresponds to the initial conversation when the model first receives \( s_i \in \mathcal{D}^{(0)} \).
\(\text{AVG}(k)\) represents the average number of conversation rounds required for the LLM to successfully identify fraudulent intent. For computational convenience, if a sample is never successfully identified as fraudulent, we set its corresponding \( k \) value to maximum round plus one, i.e., 4.

The formal definitions of these metrics are as follows:  
\[
\text{DSR}@k = \frac{|\{ s_i \mid\text{Defense Success} ~ s_i~ \text{until round } k \}|}{|\mathcal{D}^{(0)}|}
\]

\[
\text{AVG}(k) = \frac{1}{|\mathcal{D}^{(0)}|} \sum_{s_i \in \mathcal{D}^{(0)}} k_i
\]

These metrics provide a comprehensive assessment of the model’s ability to defend fraudulent intent across multiple interaction rounds, offering valuable insights into its robustness against deceptive tactics.
% \input{table/overall-performance}

% We introduce the Multi-Round Defend Success Rate (\(\text{MR-DSR}\)) as a metric for evaluating LLM robustness against fraud requests. Specifically, \(\text{MR-DSR}@k\), where \(k \in \{0, 1, 2, 3\}\), represents the success rate at which the model correctly identifies and rejects fraudulent attempts in the \(k\)-th fraud round. A higher MR-DSR indicates a stronger defense against deceptive content. At \(k = 0\) the metric evaluates the model’s performance on the \ourbasedata, providing a baseline measure of its fraud detection capability. For  \(k > 0\) the metric assesses the model’s performance as it encounters increasingly sophisticated fraudulent attempts from \( \mathcal{D}^{(k)}_{\text{level-up}}\) .
% Mathematically, MR-DSR can be defined as:

% \[
% \text{MR-DSR}@k = \frac{\left| \{s_j \in \mathcal{D}^{(k)} \mid \text{Success Identify}~ s_j \} \right|}{\left| \mathcal{D}^{(k)} \right|}
% \]


%To analyze the trend of MR-DSR across four fraud interaction rounds, we evaluate the Cumulative MR-DSR at round \( k \) (\(\text{CMR-DSR}@k\)) for each model. This metric captures the cumulative success rate of fraud detection up to round \( k \). CMR-DSR@k is defined as:
%\[
%\text{CMR-DSR}@k = \sum_{i=0}^{k} \text{MR-DSR}@i
%\]
%On the other hand, when referring to the general Defend Success Rate (DSR) of a model, we typically use \(\text{CMR-DSR}@3\), which represents the cumulative MR-DSR across all four rounds.
% In this paper, we evaluate the defense performance of models using the keyword-matching-based Attack Success Rate (ASR)(see Alg.~\ref{alg:ASR}). This metric determines whether specific keywords which represent attcking succssful(e.g., ``Yes'' or ``No'') appear in the model's response to induced prompts. It is defined as:
% \begin{equation}
% \text{ASR} 
% = \frac{1}{|P|} \sum_{i=1}^{|P|} \mathbf{1} ( \operatorname{is\_success}(P_i) )
% \label{eq:ASR}
% \end{equation}
% where \( P \) is the set of induced prompts, and \( \operatorname{is\_success}(P_i) \) indicates whether the model’s response to prompt \( P_i \) contains a succssful meaning keyword. A lower ASR indicates stronger model defenses. 
