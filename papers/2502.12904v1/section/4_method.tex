\begin{comment}
\section{Evaluation Workflow}
\syinline{move this to section 3}
In this section, we describe the workflow of evaluating the fraud defending performance of LLMs. 
\subsection{Evaluation in Two Real-world Scenarios}
\input{figure/evaworkflow}
\paragraph{LLM as a Assistant.}
\paragraph{Role-playing Scamming.}
\input{figure/refine_and_roleplay}


\subsection{Evaluation Metrics}
\paragraph{Attack Success Rate.}
In this paper, we evaluate the defense performance of models using the keyword-matching-based Attack Success Rate (ASR)(see Alg.~\ref{alg:ASR}). This metric determines whether specific keywords which represent attcking succssful(e.g., ``Yes'' or ``No'') appear in the model's response to induced prompts. It is defined as:
\begin{equation}
\text{ASR} 
= \frac{1}{|P|} \sum_{i=1}^{|P|} \mathbf{1} ( \operatorname{is\_success}(P_i) )
\label{eq:ASR}
\end{equation}
where \( P \) is the set of induced prompts, and \( \operatorname{is\_success}(P_i) \) indicates whether the modelâ€™s response to prompt \( P_i \) contains a succssful meaning keyword. A lower ASR indicates stronger model defenses. 


\paragraph{ Average Number of Refinements.}
Furthermore, we introduce a new metric for \textit{Refined Fraud Judgment Challenge} task, Average Number of Refinements (Avg-NoR) (see Alg.~\ref{alg:ANR}), which quantifies the number of prompt refinements required for a successful attack. It is defined as:

\begin{equation}
\text{Avg-NoR} 
= \frac{\sum_{i \in S} n_i + \sum_{j \in F} (C_{\max} + 1)}{|S| + |F|}
\label{eq:ANR}
\end{equation}

where \( S \) and \( F \) denote the sets of successful and failed attack attempts, respectively. \( n_i \) represents the number of refinements for a successful attack for prompt \( P_i \), starting from \( n_i = 0 \) to at most \( n_i = C_{\max} \). The term \( C_{\max} \) is the maximum refinement limit. For failed attacks, it is assumed that the full refinement budget is used, contributing \( C_{\max} + 1 \) per attempt. A higher Avg-NoR indicates stronger model robustness against induced attacks.
\end{comment}