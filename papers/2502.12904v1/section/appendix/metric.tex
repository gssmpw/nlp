\section{Metric Details}
\subsection{LLM as a Judge Prompt}
\label{app:LLM as a judge}
In this paper, we employ \texttt{GPT-4o-mini} as an automated judge to assess the defense status across all evaluation rounds. The judgment process follows a structured decision-making framework based on the predefined Judgment Rule Prompt, as detailed in Figure~\ref{fig:JudgementRulePrompt}.

This prompt is meticulously designed to ensure consistency, objectivity, and robustness in classification. It operates through a three-tiered decision process, where the model first checks for explicit fraud-related indicators to determine a "YES" classification(represents \textbf{Defense Success}). If such indicators are absent, the model evaluates whether the content suggests precautionary advice or lacks definitive fraud-related markers, leading to a "NEXT ROUND" classification(represents \textbf{More Details Needed}). In all other cases, where the content conveys trust, neutral statements, or objective facts, the model assigns a "NO" classification(represents \textbf{Defense Failure}).

Furthermore, the conflict resolution mechanism within the prompt ensures that priority is given to fraud-related terms, even if they appear in a cautionary context. In cases where risk awareness and trust coexist, the model defaults to a "YES" classification to maintain a conservative security stance. The inclusion of explicit examples further reinforces the modelâ€™s interpretability and alignment with the intended classification strategy.
\input{figure/gptjudge}
\newpage

\subsection{Human vs LLM Evaluation}
\label{app:Human vs LLM evaluation}
To ensure that \texttt{GPT-4o-mini} serves as a fair judge in classifying responses into "Defense Success", "Defense Failure", and "Next Round", we conducted a comparison against human labellers. We randomly selected 100 samples (50 in Chinese and 50 in English) from the one-round assistant results pool to assess response match rates between human evaluations and \texttt{GPT-4o-mini}'s classifications. As shown in Table \ref{human}, \texttt{GPT-4o-mini} demonstrated high agreement with human labellers across both languages. 
\texttt{GPT-4o-mini} achieves high agreement with human labellers across most categories. In the "Defense Failure" category, where the model correctly identifies fraudulent messages and rejects them, the match rate is 80.95\% for Chinese and 80.56\% for English, indicating strong alignment with human evaluators. For "Defense Success", where fraud attempts successfully deceive the model, the agreement rate is even higher, suggesting that \texttt{GPT-4o-mini} reliably identifies these cases similarly to human judges.

A key difference emerges in the "Next Round" category, where the model chooses to request more information before making a final classification. \texttt{GPT-4o-mini} assigns "Next Round" more frequently than human labellers, with a match rate of 44.44\% for Chinese and 54.55\% for English. This indicates that \texttt{GPT-4o-mini} is more conservative in uncertain cases, favouring additional clarification before committing to a classification. This cautious approach is advantageous in fraud detection, as it helps reduce false negatives, where fraudulent cases might otherwise be misclassified as non-fraudulent. By seeking additional context, the model minimizes premature dismissals of potentially deceptive content.
\input{table/human_align}
\begin{comment}
\subsection{Keyword-Matching-Based Defense Success Rate}
Following Alg.~\ref{alg:ASR} is the detailed Keyword-Matching-Based Defense Success Rate (ASR) algorithm.
\begin{algorithm*}
\caption{Keyword-Matching-Based Defense Success Rate (ASR)}
\label{alg:ASR}
\begin{algorithmic}

\Require LLM model $M$, Set of attack prompts $\mathcal{P} = \{P_1, P_2, ..., P_k\}$, Keyword set $\mathcal{K} = \{K_1, K_2, ..., K_m\}$ \Comment{e.g $K_i=$ "YES", $K_j=$ "agree"}
\Ensure ASR: Defense Success rate based on keyword matching

\State $successful\_attacks \gets 0$  \Comment{Count successful jailbreaks}
\State $total\_attempts \gets |\mathcal{P}|$  \Comment{Total number of attack prompts}

\For{\textbf{each} $P_i \in \mathcal{P}$}
    \State $response \gets M(P_i)$  \Comment{Query the model with the attack prompt}
    
    \If{\Call{is\_success}{$response$, $\mathcal{K}$}}
        \State $successful\_attacks \gets successful\_attacks + 1$
    \EndIf
\EndFor

\State \Return $\frac{successful\_attacks}{total\_attempts} \times 100\%$  \Comment{Compute ASR percentage}
\\
\Function{is\_success}{$response$, $\mathcal{K}$}
    \For{\textbf{each} $K_j \in \mathcal{K}$}
        \If{$K_j$ appears in $response$}
            \State \Return True
        \EndIf
    \EndFor
    \State \Return False
\EndFunction

\end{algorithmic}
\end{algorithm*}
    
\end{comment}
\begin{comment}

\subsection{Average Number of Refinments}
Below Alg.~\ref{alg:ANR} is the detailed Average Number of Refinements (Avg-NoR) algorithm implementation.

\begin{algorithm*}
\caption{Average Number of Refinements (Avg-NoR)}
\label{alg:ANR}
\begin{algorithmic}
\Require LLM model $M$, Set of attack prompts $\mathcal{P} = \{P_1, P_2, ..., P_k\}$, Refinements cap $C_{\max}$
\Ensure Avg-NoR: Average number of refinements
\State $total\_refinements \gets 0$  \Comment{Initialize total refinement counter}
\State $successful\_attacks \gets 0$  \Comment{Count successful jailbreaks}

\For{\textbf{each} $P_i \in \mathcal{P}$}
    \State $n \gets 0$  \Comment{Initialize refinement counter for this attack}
    \State $success \gets False$

    \While{$n \leq C_{\max}$ \textbf{and} $success = False$}
        \State $response \gets M(P_n)$  \Comment{Query the model with the current prompt}
        
        \If{\Call{is\_jailbreak\_successful}{$response$}}
            \State $success \gets True$
        \Else
            \State $P_{n+1} \gets$ \Call{refine}{$P_n$}  \Comment{Refine the attack prompt}
            \State $n \gets n + 1$
        \EndIf
    \EndWhile

    \If{$success = True$}
        \State $total\_refinements \gets total\_refinements + n$
    \Else
        \State $total\_refinements \gets total\_refinements + (C_{\max}+1)$
    \EndIf
    \State $successful\_attacks \gets successful\_attacks + 1$
\EndFor

\If{$successful\_attacks > 0$}
    \State \Return $\frac{total\_refinements}{successful\_attacks}$  \Comment{Compute the average NoR}
\Else
    \State \Return $C_{\max} + 1$  \Comment{Indicate that no successful jailbreaks occurred}
\EndIf
\end{algorithmic}
\end{algorithm*}
    
\end{comment}