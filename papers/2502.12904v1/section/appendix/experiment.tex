\section{Experiments Details}
\subsection{Overall Model Performance on \ourbench Benchmark}
Table~\ref{tab:overall_performance} presents the overall performance of various LLMs evaluated on the \textbf{\ourbench} benchmark, which assesses their robustness against fraudulent prompts. 
The results reveal several key trends in fraud resistance across different models. \texttt{Claude-3.5} series demonstrate the strongest defense mechanisms, with \texttt{Claude-3.5-sonnet} achieving the highest success rate, suggesting a highly refined alignment strategy. \texttt{Gemini-1.5} series and \texttt{GPT-4o} also perform competitively, surpassing all evaluated Meta Llama-3 models, which exhibit moderate resistance. Among open-weight models, \texttt{Deepseek-v3} and \texttt{Llama-3-405B} show reasonable robustness, but they still lag behind their proprietary API-based counterparts, likely due to the absence of extensive safety alignment.
Notably, older and lightweight models such as \texttt{GPT-3.5-turbo} and \texttt{GLM-3-turbo} perform significantly worse, with high failure rates indicating susceptibility to adversarial exploitation. This highlights the importance of continuous advancements in safety alignment and fraud detection strategies.
\input{table/overall-performance}

\subsection{Detailed Comparison of the Performance of the Top 6 LLMs by Category}

\input{figure/overall_bycat_radar}

To thoroughly evaluate the performance of different LLMs in fraud defense tasks, we selected the six models with the best overall performance out of 15 candidates: \texttt{Claude-3.5-sonnet}, \texttt{Claude-3.5-haiku}, \texttt{Gemini-1.5-pro}, \texttt{Gemini-1.5-flash}, \texttt{GPT-4o}, and \texttt{GPT-o3-mini}. As shown in Figure \ref{fig:overall_bycat_radar}, these LLMs exhibit significant differences in their DSR. We conducted a detailed comparison and analysis across five types of fraud categories.

\paragraph{Performance Differences Across Evaluated LLMs} As shown in Figure \ref{fig:overall_bycat_radar}, \texttt{Claude-3.5-sonnet} and \texttt{Claude-3.5-haiku} deliver the best overall performance, achieving over 95\% DSR in Impersonation, Fraudulent Service and Online Relationship. In comparison, \texttt{Gemini-1.5-pro} and \texttt{Gemini-1.5-flash} are slightly weaker, with less effective defense in complex categories like Phishing Scams and Fake Job Posting, though they maintain high DSR in Fraudulent Service and Impersonation. \texttt{GPT-4o} performs consistently with \texttt{Gemini-1.5-flash}, and surpassing \texttt{Gemini-1.5-flash} in Online Relationship. \texttt{GPT-o3-mini} performs the weakest, with significantly lower DSR in Fake Job Posting compared to the other LLMs.

\paragraph{Performance Differences Across Fraud Categories}

The varying difficulty of defending against different fraud categories has a noticeable impact on LLMs performance. Fraudulent Service, Impersonation and Online Relationship are the categories where most LLMs perform relatively well, with significantly higher DSR compared to other categories. This suggests that the fraudulent patterns in these categories are more apparent, allowing the LLMs to detect and defend against them more accurately. However, for Phishing Scams and Fake Job Posting, the DSR are generally lower, indicating that the fraudulent tactics in these categories may be more subtle or complex, posing greater challenges to the LLMs' detection capabilities. Notably, \texttt{Claude-3.5-sonnet} and \texttt{Claude-3.5-haiku} demonstrate significantly better defense performance in Phishing Scams and Fake Job Posting compared to other LLMs, further showcasing their ability to detect more sophisticated forms of fraud.