\section{Experiments}
\subsection{Experimental Setup}
% \paragraph{LLMs Under Evaluation.}
\input{table/main_results}

we evaluate 15 different LLMs in our \ourbench, including both proprietary (\textit{API-based}) and open-source (\textit{Open-weights}) models, across 7 model families: \texttt{GPT, Claude, Gemini, GLM, Doubao, DeepSeek}, and \texttt{LLaMA}, which cover various model sizes. The details of the evaluated models are provided in Table~\ref{tab:model_details} (see Appendix~\ref{app:model_choice}). 
For each model, we assess its performance on the open QA task under both the Helpful Assistant and Role-play settings as introduced in Section~\ref{sec:evaflow}. Detailed instructions for model generation prompts can be found in Appendix~\ref{app:Two Real-world Scenarios Prompt}. Our evaluation employs \texttt{GPT-4o-mini} as an automated judge to assess model responses across multiple rounds. The complete prompt template we used for judgment, along with a consistency experiment involving human annotators, is detailed in Appendix~\ref{app:LLM as a judge}.

% \input{table/assVSrole_dsr}
\subsection{Main Results}

Table~\ref{tab:mainresult} presents the comprehensive Defense Success Rate (DSR) of different LLMs, including overall and category-specific scores across Helpful Assistant and Role-play settings. Our main finding can be concluded as the following parts:

% while Table~\ref{tab:overall_performance} ranks models by performance. \syinline{delete because overlap with main result OD Col}

\textbf{(i)} \textbf{Our results highlight the challenging nature of our \ourbench}, which presents significant risks for LLMs to identify and defend against fraud and phishing inducements, especially in the Fake Job Posting category, most LLMs are almost unable to identify Fake Job Postings in a Role-play setting, which means that using large models for tasks like job screening and application submissions may carry significant risks~\cite{li-etal-2024-large}.
\input{figure/DSRK}

% Role-play setting which was wildly used in personslized LLMs and agent system.

\textbf{(ii)} \textbf{There is a disparity between different models, settings, and languages.} For instance, \texttt{Claude-3.5-sonnet} leads with a 92.55\% overall DSR across all fraud categories followed by \texttt{Claude-3.5-haiku} at 88.28\%, showing their robustness against fraud information, while other widely used models such as \texttt{GPT-3.5-turbo} and \texttt{GLM-3-turbo} have a huge gap between different fraud categories and settings. For example, in the Online Relationship categories, \texttt{GPT-3.5-turbo}'s DSR sharply decreased after we gave it a role-play prompt. Additionally, as shown in Figure~\ref{fig:settingsandlanguages}, there is a performance gap between Chinese and English. In most of the LLMs (except for \texttt{Doubao-lite-32k}), the DSR in English outperforms that in Chinese.

\textbf{(iii)} \textbf{Open-source LLMs can outperform proprietary LLMs}, and smaller LLMs can also surpass larger models with more parameters. For example, in Table~\ref{tab:mainresult}, we found that \texttt{R1-Llama-70B}, which was fine-tuned on distilled reasoning data from \texttt{Deepseek}, demonstrates competitive performance with \texttt{Deepseek-V3} and \texttt{Llama-3.1-405B}. Additionally, the \texttt{GPT-3.5-turbo} and \texttt{GLM} model families show weaker performance compared to the open-source models we evaluated.
\input{figure/envszhdsr_rolevsass}
\input{figure/AVGK}

\iffalse
Our analysis highlights significant disparities in fraud defense capabilities. Among API-based models, \texttt{Claude-3.5-sonnet} leads with a 92.55\% DSR, excelling in Impersonation and Online Relationship fraud detection, followed by \texttt{Claude-3.5-haiku} at 88.28\%. In contrast, \texttt{GPT-3.5-turbo} and \texttt{GLM-3-turbo} perform poorly \syinline{don't show your result like only xx good xx poor}, with DSRs of 43.49\% and 38.92\%, respectively, indicating vulnerability to fraudulent manipulation.
\fi

\iffalse
Open-weight models generally underperform \syinline{no... instead use open llm \textbf{can} be outperformed}, with \texttt{Deepseek-R1-Distill-Llama-70B} achieving the highest DSR at 67.40\%, still lagging behind API models. Others, including \texttt{Deepseek-V3} and \texttt{Llama-3.1}, show moderate effectiveness but struggle in Phishing Scams and Fake Job Positions, with DSRs below 50\%.
\fi


\iffalse
This performance gap reflects the differences in scale, training data, and safety alignment. API models benefit from extensive security-focused training, whereas open-weight models, constrained by resources and priorities, often lack robust safeguards.
\fi


\subsection{Discussion and Future Work}

In this section, we provide further insights into the performance of the LLM across various dimensions, such as languages, tasks, and multi-round conversation. 

\paragraph{Cross-Language Defense Performance Gap.}
As illustrated in the right panel of Figure~\ref{fig:settingsandlanguages}, models demonstrate notably higher Defense Success Rates (DSR) when responding to English fraud attempts compared to Chinese ones. This disparity is particularly pronounced in the \texttt{Llama} model family. This observation highlights a significant concern regarding multilingual models: while they continue to expand their language support, security considerations appear to be unevenly addressed across different languages, a phenomenon also noted by \citeauthor{wang2023all}. Our development of this bilingual benchmark aims to advance the study of LLM safety beyond English-centric evaluation, pushing toward more comprehensive and equitable security measures across languages.

\paragraph{Impact of Role-playing on Fraud Detection Performance.}
As demonstrated in Figures~\ref{fig:settingsandlanguages} and \ref{fig:assVSrole}, assigning specific roles to LLMs significantly compromises their fraud detection capabilities. This degradation manifests not only in a substantial decrease in overall Defense Success Rate (DSR) compared to the Helpful Assistant setting but also in reduced effectiveness during multi-round conversations. Figure~\ref{fig:assVSrole} reveals that under role-play conditions, the defense rate increases more gradually compared to the assistant setting. Furthermore, analysis of Figure~\ref{fig:avg} indicates that role-playing significantly increases the number of conversation rounds required for fraud detection. This extended detection time poses a heightened financial risk to users in real-world scenarios, providing more opportunities for potentially fraudulent activities. These findings underscore the critical need for enhanced vigilance against fraud risks in agent-based systems and other personalized LLM applications.
\paragraph{Ethical Considerations and Potential Misuse.}
While \ourbench utilizes LLM-generated fraud content for testing purposes, it is crucial to acknowledge the potential risks of such capabilities. Large Language Models could be misappropriated to generate sophisticated fraudulent content and craft deceptive schemes, posing significant societal risks. We strongly advocate for responsible AI development practices and emphasize that the methodologies presented here should be used exclusively for defensive research and system improvement.

% \paragraph{Models Outperform on Helpful Assistant Compared to Role-play.}
% Figure~\ref{fig:assVSrole} shows that models perform similarly overall in both \textit{Helpful Assistant} and \textit{Role-play} tasks but are notably weaker in the latter. Table~\ref{tab:mainresult} highlights that this gap is largely due to poor performance in the Fraud domain, especially in the Fake Job Position category. The difference stems from role-based vigilance: in the \textit{Helpful Assistant} task, the model acts as an advisor, inherently cautious about misleading information and more likely to scrutinize potential fraud. In contrast, the \textit{Role-play} task requires the model to embody a victim’s perspective, which reduces its critical stance.

% \paragraph{Models Exhibit Stronger Fraud Defense in English Than in Chinese.}
% % \input{figure/language_dsr_distribution}
% As illustrated in left part of Figure~\ref{fig:settingsandlanguages}, LLMs generally achieve higher Defense Success Rates (DSR) on English fraud scenarios compared to Chinese ones. The English DSR scores are more concentrated within the 60–80 range, whereas the Chinese DSR scores are more dispersed, primarily between 47 and 70. This suggests that while models are relatively robust in detecting fraudulent intent in English, their defense mechanisms in Chinese are more inconsistent, potentially due to differences in language-specific training and fraud pattern recognition.
%\paragraph{Stepwise Performance.}
%\paragraph{By Category Performance.}
%\input{figure/overall_bycat_radar}




\iffalse
To assess defenses against fraud and phishing, we evaluate 15 LLMs across two categories, as detailed in Table~\ref{tab:model_details} (Appendix~\ref{app:model_choice}). The first category includes \textbf{API-based models}, which are commercial LLMs with undisclosed parameter counts, typically larger in scale and superior in performance. The second category consists of \textbf{open-weight models} from both academia and industry, evaluated across three parameter scales: small (\textasciitilde{}8B), medium (\textasciitilde{}70B), and large (\textasciitilde{}405B). \syinline{we don't need to explain what is api base}
\fi

\iffalse
\paragraph{Evaluation Prompt Setup.}
For our evaluation, we employ GPT-4o-mini as an automated judge~\citep{zheng2023judging, gu2024survey} to assess model responses across multiple rounds. The judge classifies each response into one of three categories: "YES", "NO", or "NEXT ROUND". The complete prompt template used for judgment is detailed in Appendix~\ref{app:LLM as a judge}.
\fi