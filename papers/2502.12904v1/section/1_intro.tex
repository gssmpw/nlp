\section{Introduction}

\input{figure/evaworkflow}
\input{figure/datasetoverview}
With the rapid advancement of artificial intelligence, large language models (LLMs) and LLM-powered agents have become accessible to various real-world applications, including financial services~\cite{lee2024survey, wang-brorsson-2025-large}, e-commerce ~\cite{pengecellm,palen2024investigating}, and recommender systems~\cite{kim2024large}. These models are widely used to assist users with decision-making tasks such as contract review~\cite{ma2024combining}, online shopping~\cite{jinshopping}, investment, and job-searching advice~\cite{yu2024fincon, 10.1145/3626772.3657680}. However, recent studies have highlighted their susceptibility to misinformation, data poisoning, and adversarial manipulation~\cite{DBLP:journals/corr/abs-2308-05374,DBLP:journals/corr/abs-2409-08087, siciliano2023adversarial}, posing significant risks when these models fail to detect internet fraud as they increasingly take on decision-making roles.

Although previous studies have demonstrated that LLMs have the potential to detect fraud and phishing attempts, a comprehensive benchmark that closely mirrors real-world fraud scenarios remains lacking~\cite{okosun2023evolution}. Existing evaluations, such as phishing email detection~\cite{yasin2016intelligent,uddin4785953explainable} and fake job identification~\cite{dutta2020fake}, are often limited to simple classification tasks and fail to incorporate multi-round assessments and emerging fraud strategies, such as fake actor recruitment~\footnote{\href{https://www.theguardian.com/world/2025/jan/14/wang-xing-chinese-actor-abduction-thailand-myanmar-scam-ntwnfb}{A kidnapped Chinese actor, a scam gang and a very public rescue operation}}.%This limitation can lead to \textit{overly optimistic} assessments of evaluation results.
However, fraud detection in practical settings typically involves multi-turn interactions and unfolds dynamically during user-LLM exchanges. This limitation may lead to overly optimistic assessments of model performance (we provide a detailed discussion of the shortcomings of existing benchmarks in Appendix~\ref{app:Dataset Comparison}).

To address this concern and advance the field of LLM safety evaluation, we propose \ourbench, a more challenging benchmark designed to evaluate LLMs' ability to defend against internet fraud and phishing in real-world scenarios. \ourbench includes frauds sourced from previous phishing scams, fake job posting datasets, social media, news, etc., and is categorized into five main classes: Fraudulent Services, Impersonation, Phishing Scams, Fake Job Postings, and Online Relationships. Our benchmark consists of 8,564 carefully selected fraudulent samples, encompassing a base dataset \ourbasedata and a rule-based augmented level-up dataset \ourlevelupdatset. An overview of our dataset is presented in Figure \ref{fig:dataoverview}, and the detailed dataset construction process is elaborated in Section \ref{sec:data_construct_process}.
\input{figure/stepbystepfraud}
Furthermore, to more effectively evaluate real-world usage cases of LLM-assisted decision-making processes, we designed our evaluation framework with two settings: 
the \textbf{Helpful Assistant} and the \textbf{Role-play} settings. In the Helpful-Assistant Setting, as illustrated in the left part of Figure \ref{fig:evaflow},  we provide the ``victim'' LLM with a general ``you are a helpful assistant'' instruction and use the model for advice, which is widely used in LLM chatbots and assistants~\cite{dam2024complete}. In the Role-Play setting, we provide the models with a role-play system prompt, asking the model to assume a specific role (e.g., ``Suppose you are \dots, what will you do?''). This setting is commonly employed in agent-based systems~\cite{wang2023survey, li2024personal_llm_agents} and personalized LLMs~\cite{tseng-etal-2024-two, zollo2024personalllm}. Unlike previous benchmarks, our benchmark also presents a multi-round evaluation pipeline, as illustrated in Figure~\ref{fig:evaflow}. In this pipeline, we evaluate the model against \textit{Credibility Building, Urgency Creating, Emotional Appeal Exploiting} step-by-step augmented fraud, as shown in Figure~\ref{fig:stepbystep}. To quantify the model's ability to identify and resist fraudulent or phishing attempts, we introduce Defense Success Rate  \(\text{DSR}\), \(\text{DSR}@k\) and \(\text{AVG}(k)\) to evaluate performance in multi-round interaction scenarios.

We evaluate 15 open-source and advanced proprietary LLMs from different scales and families (such as GPT, GLM, Claude, etc.) on \ourbench. Our key findings are summarized as follows:
\begin{itemize}
    \item\ourbench presents significant challenges for LLMs in fraud detection, particularly in the Fake Job Posting category. Notably, the Role-play settings drastically reduce the models' Defense Success Rate.
    \item Fraud detection performance varies considerably across models, settings, and languages. While models like \texttt{Claude-3.5-sonnet} demonstrate strong robustness, others achieve only 38.92\%â€“83.27\% overall DSR, with notably lower performance in Chinese compared to English.
    \item LLMs can be leveraged to synthesize fraudulent datasets tailored to specific strategies and user backgrounds, posing serious risks for misuse.
\end{itemize}

Our goal with \ourbench is to contribute to the development of safer AI assistants and agent systems. We believe it will help mitigate the risks of telecom fraud and other online scams by equipping LLMs with more robust fraud detection capabilities, ultimately enhancing trust and security in AI-powered decision-making.

% 5 open-weight LLMs, including \texttt{Deepseek-V3} and the \texttt{Llama-3.1-it-turbo} series, as well as 10 advanced  proprietary models such as \texttt{GPT-4o}, the \texttt{Claude-3.5} series, and the \texttt{Gemini-1.5} series, and so on.

% Internet scams have evolved from simple phishing emails and Nigerian prince scams \cite{okosun2023evolution} into highly sophisticated, AI-driven fraud \cite{leong2024intersection}, involving counterfeit IDs, forged contracts, and deepfake verification videos, that bypass traditional detection methods with alarming efficiency. In January 2025, Chinese actor Xing Wang fell victim to an elaborate scam involving a fraudulent film production contract. Posing as industry professionals, scammers provided convincing legal documents and official-looking credentials to lure the actor into this business travel. As AI-powered tools become more integrated into everyday decision-making, from financial planning to contract verification, a new concern emerges: Can LLMs be manipulated in similar ways? Given the increasing reliance on AI assistants for advice, it is important to understand whether LLMs can identify fraud or fall victim to deceptive interactions themselves.

% With the recent release of DeepSeek-R1 \cite{}, we observed its strong capability in generating highly realistic scam-like scripts, also shown from security assessments from KELA and Cisco,\footnote{See \href{https://www.kelacyber.com/blog/deepseek-r1-security-flaws/?utm_campaign=Blogs&utm_medium=email&_hsenc=p2ANqtz-9H6WQNWVbJjz1xxfx2uA3AqWTb0fSdyunPtx_MU5pR46sonOkZdNj7eNVrdNge_099UF7YAoY2BNnbHUrVGLTbI8e9zMeTLpQ-Pmm9Ht05YAzAABw&_hsmi=344373757&utm_content=344373757&utm_source=hs_email}{KELA: DeepSeek R1: Malicious Use Made Easy}. and \href{https://blogs.cisco.com/security/evaluating-security-risk-in-deepseek-and-other-frontier-reasoning-models}{Cisco: Evaluating Security Risk in DeepSeek and Other Frontier Reasoning Models}} raising concerns about how adversarial LLMs could be used to deceive other AI models. This motivates us to conduct a systematic evaluation of LLM defenses against fraud, as DeepSeek-R1 becomes a powerful tool for simulating adversarial attacks against commercial and open-source LLMs.


% \syinline{background: LLM wildly used in (assisted) decision making

% Previous studies have shown that LLMs' decisions are highly influenced by misinformation and poisoned data, posing significant risks if they fail to identify such threats.

% Although prior research [XX] has demonstrated the potential of LLMs in detecting fake job postings or FDB, 

% however, our findings in Table xx show that they struggle to effectively identify new or more sophisticated types of fraud,.., and multi-round of chat,  highlighting the need for more comprehensive datasets.

% LLM is wildly used in syntactic data (and ), and r-1 o-1 like a model can generate xx data 

% dataset 
% evaluate pipeline

% we build a dataset
% evaluate 
% found a high risk of xx

% }
