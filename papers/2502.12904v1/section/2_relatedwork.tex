\section{Related Work}
\input{figure/existing_dataset}
Internet fraud encompasses various cybercrimes that occur over the internet or via email, including celebrity
impersonation, phishing, and other hacking activities designed to deceive individuals for financial gain—or even to compromise their personal safety~\cite{ye2023study}. As Large Language Models (LLMs) and LLM-based agent systems become increasingly integral to automated decision-making processes, it is crucial to develop robust safeguards that protect these systems against fraudulent manipulation and phishing attempts.

Current single-task fraud benchmarks, such as FGRC-SCD, BothBosu Scam Dialogues~\footnote{See Hugging Face for \href{https://huggingface.co/datasets/Abooooo/FGRC-SCD}{FGRC-SCD} and \href{https://huggingface.co/datasets/BothBosu/scam-dialogue/}{Scam Dialogues}}, the Phishing Email Dataset~\cite{al2024novel}, the Fake Job Posting dataset~\cite{shivamb_fake_job_posting}, the Amazon Fraud Dataset Benchmark (FDB)~\cite{grover2022fraud}, which primarily constructs fraud cases based on incorrect or missing credit card information, and DetoxBench~\cite{chakraborty2024detoxbench}, which focuses on fraud and fake email detection, each target specific aspects of fraud detection. However, their narrow focus is on isolated tasks—such as classifying fraudulent messages or emails, as illustrated in Figure~\ref{fig:existing}. However, as LLMs are increasingly integrated into various applications and fraud schemes continue to evolve, existing single-task fraud benchmarks have become insufficient for comprehensive evaluation, their rigid structures and single-turn evaluation approaches fail to capture the complexities of real-world fraud scenarios, which often involve multi-turn interactions and progressive fraud strategy. 
% As demonstrated by our experiments in Section\syinline{add right ref}, these limitations lead to an overly simplistic assessment of fraud identification and defense capabilities.
% potentially leading to overly optimistic assessment results. 
To push the boundaries of what LLMs can achieve, \ourbench introduces a significantly more comprehensive evaluation by incorporating a diverse range of real-world fraud scenarios spanning five key domains. Furthermore, our benchmark assesses LLMs' resilience to fraud in both Role-play and Helpful Assistant settings, integrating multi-turn evaluations to better reflect real-world interactions. This approach allows for a more rigorous assessment of an LLM’s ability to detect and resist fraudulent attempts over extended conversations.


% Current fraud detection research leverages a variety of datasets tailored to specific fraud categories, as shown in Figure \ref{fig:existing}. The BothBosu Dataset \cite{} simulates typical phishing scenarios through role-playing dialogues, effectively capturing real-world impersonation and phishing attempts that target unsuspecting individuals. Similarly, the FGRC-SCD Dataset\cite{} focuses on financial scams, especially those targeting investment platforms, with an emphasis on non-English contexts, such as Mandarin-language scams. The Phishing Email Dataset compiles a broad spectrum of fraudulent emails, including prize scams, crypto airdrop schemes, and fake event invitations, which are designed to extract sensitive user data. Additionally, the Fake Job Posting Dataset highlights fraudulent job offers crafted to deceive individuals into sharing personal information or paying for non-existent services.

% Another significant benchmark is the Amazon Fraud Dataset Benchmark (FDB), which includes data on fraudulent IP addresses, fake Twitter accounts, and suspicious URLs. While this dataset provides valuable information for training machine learning models, its focus diverges from customer-facing fraud scenarios like phishing emails or scam messages. The exception is its collection of job postings, where 18,000 job descriptions are included, about 800 of which are identified as fake—this subset closely aligns with real-world fraudulent job scams. However, datasets like FDB lack the realistic role-playing dialogue data needed for evaluating how LLMs handle fraud in conversational contexts. Furthermore, URL-based fraud data in such benchmarks offer limited utility for LLM evaluation, as they do not provide the linguistic nuances required for adversarial prompt testing.
% \szinline{adding one more benchmark DetoxBench: https://arxiv.org/pdf/2409.06072v1}
% Despite the availability of these datasets, existing benchmarks often fall short in capturing the complexity and evolving nature of fraud tactics, particularly in multi-turn conversational settings. Moreover, many datasets are limited in language scope, predominantly focusing on English, and are not regularly updated to reflect the latest fraud trends. This highlights the pressing need for comprehensive, multilingual, and up-to-date datasets that can evaluate LLM performance across diverse fraud scenarios and defense strategies.
