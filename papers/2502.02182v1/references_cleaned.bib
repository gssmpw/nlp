@INPROCEEDINGS{chu_prediction_2020,
  author={Chu, Slo-Li and Abe, Kuniya and Yokota, Hideo and Sudo, Kazuhiro and Nakamura, Yukio and Chang, Yuan-Hsiang and Fang, Liang-Che and Tsai, Ming-Dar},
  booktitle={2020 42nd Annual International Conference of the IEEE Engineering in Medicine \& Biology Society (EMBC)}, 
  title={Prediction for Morphology and States of Stem Cell Colonies using a LSTM Network with Progressive Training Microscopy Images}, 
  year={2020},
  volume={},
  number={},
  pages={1820-1823},
  keywords={Training;Microscopy;Computer architecture;Morphology;Corporate acquisitions;Hip;Microprocessors},
  doi={10.1109/EMBC44109.2020.9175759},

@inproceedings{chu_prediction_2020_old,
	title = {Prediction for Morphology and States of Stem Cell Colonies using a {LSTM} Network with Progressive Training Microscopy Images},
	abstract = {We present a new {LSTM} (P-{LSTM}: Progressive {LSTM}) network, aiming to predict morphology and states of cell colonies from time-lapse microscopy images. Apparent short-term changes occur in some types of time-lapse cell images. Therefore, long-term-memory dependent {LSTM} networks may not predict accurately. The P-{LSTM} network incorporates the images newly generated from cell imaging progressively into {LSTM} training to emphasize the {LSTM} short-term memory and thus improve the prediction accuracy. The new images are input into a buffer to be selected for batch training. For real-time processing, parallel computation is introduced to implement concurrent training and prediction on partitioned images.Two types of stem cell images were used to show effectiveness of the P-{LSTM} network. One is for tracking of {ES} cell colonies. The actual and predicted {ES} cell images possess similar colony areas and the same transitions of colony states (moving, merging or morphology changing), although the predicted colony mergers may delay in several time-steps. The other is for prediction of {iPS} cell reprogramming from the {CD}34+ human cord blood cells. The actual and predicted {iPS} cell images possess high similarity evaluated by the {PSNR} and {SSIM} similarity evaluation metrics, indicating the reprogramming {iPS} cell colony features and morphology can be accurately predicted.},
	eventtitle = {2020 42nd Annual International Conference of the {IEEE} Engineering in Medicine \& Biology Society ({EMBC})},
	pages = {1820--1823},
	booktitle = {2020 42nd Annual International Conference of the {IEEE} Engineering in Medicine \& Biology Society ({EMBC})},
	author = {Chu, Slo-Li and Abe, Kuniya and Yokota, Hideo and Sudo, Kazuhiro and Nakamura, Yukio and Chang, Yuan-Hsiang and Fang, Liang-Che and Tsai, Ming-Dar},
	urldate = {2025-01-24},
	date = {2020-07},
	note = {{ISSN}: 2694-0604},
	keywords = {Computer architecture, Corporate acquisitions, Hip, Microprocessors, Microscopy, Morphology, Training},
}

@online{noauthor_temporal_nodate,
	title = {Temporal Encoding in Biological Imaging {\textbar} Elicit},
	urldate = {2025-01-24},
}

@article{zhao_insights_2024,
	title = {Insights into Cellular Evolution: Temporal Deep Learning Models and Analysis for Cell Image Classification},
	shorttitle = {Insights into Cellular Evolution},
	abstract = {I.
          
            A
            bstract
          
          Understanding the temporal evolution of cells poses a significant challenge in developmental biology. This study embarks on a comparative analysis of various machine-learning techniques to classify sequences of cell colony images, thereby aiming to capture dynamic transitions of cellular states. Utilizing transfer learning with advanced classification networks, we achieved high accuracy in single-timestamp image categorization. We introduce temporal models—{LSTM}, R-Transformer, and {ViViT}—to explore the effectiveness of integrating temporal features in classification, comparing their performance against non-temporal models. This research benchmarks various machine learning approaches in understanding cellular dynamics, setting a foundation for future studies to enhance our understanding of cellular developments with computational methods, contributing significantly to biological research advancements.},
	author = {Zhao, Xinran and De Perez, Alexander Ruys and Dimitrova, Elena S. and Kemp, Melissa and Anderson, Paul E.},
	urldate = {2025-01-24},
	date = {2024-03-12},
	langid = {english},
    journal={bioRxiv},
}



@article{su_spatiotemporal_2017,
  author={Su, Yu-Ting and Lu, Yao and Chen, Mei and Liu, An-An},
  journal={IEEE Access}, 
  title={Spatiotemporal Joint Mitosis Detection Using CNN-LSTM Network in Time-Lapse Phase Contrast Microscopy Images}, 
  year={2017},
  volume={5},
  number={},
  pages={18033-18041},
  keywords={Feature extraction;Microscopy;Machine learning;Hidden Markov models;Computer architecture;Spatiotemporal phenomena;Visualization;Biomedical imaging;computer vision;mitosis detection;machine learning;stem cell},
}

@article{simon_causalxtract_2025,
	title = {{CausalXtract}, a flexible pipeline to extract causal effects from live-cell time-lapse imaging data},
	volume = {13},
	issn = {2050-084X},
	abstract = {Live-cell microscopy routinely provides massive amounts of time-lapse images of complex cellular systems under various physiological or therapeutic conditions. However, this wealth of data remains difficult to interpret in terms of causal effects. Here, we describe {CausalXtract}, a flexible computational pipeline that discovers causal and possibly time-lagged effects from morphodynamic features and cell–cell interactions in live-cell imaging data. {CausalXtract} methodology combines network-based and information-based frameworks, which is shown to discover causal effects overlooked by classical Granger and Schreiber causality approaches. We showcase the use of {CausalXtract} to uncover novel causal effects in a tumor-on-chip cellular ecosystem under therapeutically relevant conditions. In particular, we find that cancer-associated fibroblasts directly inhibit cancer cell apoptosis, independently from anticancer treatment. {CausalXtract} uncovers also multiple antagonistic effects at different time delays. Hence, {CausalXtract} provides a unique computational tool to interpret live-cell imaging data for a range of fundamental and translational research applications.},
	pages = {RP95485},
	journal = {{eLife}},
	author = {Simon, Franck and Comes, Maria Colomba and Tocci, Tiziana and Dupuis, Louise and Cabeli, Vincent and Lagrange, Nikita and Mencattini, Arianna and Parrini, Maria Carla and Martinelli, Eugenio and Isambert, Herve},
	editor = {Postovit, Lynne-Marie},
	urldate = {2025-01-24},
	date = {2025-01-17},
	note = {Publisher: {eLife} Sciences Publications, Ltd},
	keywords = {causal discovery, causal inference, granger causality, live-cell imaging, time-lapse image analysis, tumor on chip},
}

@article{ker_engineered_2011,
	title = {An Engineered Approach to Stem Cell Culture: Automating the Decision Process for Real-Time Adaptive Subculture of Stem Cells},
	volume = {6},
	issn = {1932-6203},
	shorttitle = {An Engineered Approach to Stem Cell Culture},
	abstract = {Current cell culture practices are dependent upon human operators and remain laborious and highly subjective, resulting in large variations and inconsistent outcomes, especially when using visual assessments of cell confluency to determine the appropriate time to subculture cells. Although efforts to automate cell culture with robotic systems are underway, the majority of such systems still require human intervention to determine when to subculture. Thus, it is necessary to accurately and objectively determine the appropriate time for cell passaging. Optimal stem cell culturing that maintains cell pluripotency while maximizing cell yields will be especially important for efficient, cost-effective stem cell-based therapies. Toward this goal we developed a real-time computer vision-based system that monitors the degree of cell confluency with a precision of 0.791±0.031 and recall of 0.559±0.043. The system consists of an automated phase-contrast time-lapse microscope and a server. Multiple dishes are sequentially imaged and the data is uploaded to the server that performs computer vision processing, predicts when cells will exceed a pre-defined threshold for optimal cell confluency, and provides a Web-based interface for remote cell culture monitoring. Human operators are also notified via text messaging and e-mail 4 hours prior to reaching this threshold and immediately upon reaching this threshold. This system was successfully used to direct the expansion of a paradigm stem cell population, C2C12 cells. Computer-directed and human-directed control subcultures required 3 serial cultures to achieve the theoretical target cell yield of 50 million C2C12 cells and showed no difference for myogenic and osteogenic differentiation. This automated vision-based system has potential as a tool toward adaptive real-time control of subculturing, cell culture optimization and quality assurance/quality control, and it could be integrated with current and developing robotic cell cultures systems to achieve complete automation.},
	pages = {e27672},
	number = {11},
	journal = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Ker, Dai Fei Elmer and Weiss, Lee E. and Junkers, Silvina N. and Chen, Mei and Yin, Zhaozheng and Sandbothe, Michael F. and Huh, Seung-il and Eom, Sungeun and Bise, Ryoma and Osuna-Highley, Elvira and Kanade, Takeo and Campbell, Phil G.},
	urldate = {2025-01-24},
	date = {2011-11-16},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Cell cultures, Cell differentiation, Cell staining, Computer imaging, Forecasting, Phase contrast microscopy, Polynomials, Stem cells},
}

@article{takeuchi_prediction_2024,
	title = {Prediction of cell cycle distribution after drug exposure by high content imaging analysis using low-toxic {DNA} staining dye},
	volume = {12},
	rights = {© 2024 The Authors. Pharmacology Research \& Perspectives published by British Pharmacological Society and American Society for Pharmacology and Experimental Therapeutics and John Wiley \& Sons Ltd.},
	issn = {2052-1707},
	abstract = {Interference in cell cycle progression has been noted as one of the important properties of anticancer drugs. In this study, we developed the cell cycle prediction model using high-content imaging data of recipient cells after drug exposure and {DNA}-staining with a low-toxic {DNA} dye, {SiR}-{DNA}. For this purpose, we exploited {HeLa} and {MCF}7 cells introduced with a fluorescent ubiquitination-based cell cycle indicator (Fucci). Fucci-expressing cancer cells were subjected to high-content imaging analysis using {OperettaCLS} after 36-h exposure to anticancer drugs; the nuclei were segmented, and the morphological and intensity properties of each nucleus characterized by {SiR}-{DNA} staining were calculated using imaging analysis software, Harmony. For the use of training, we classified cells into each phase of the cell cycle using the Fucci system. Training data (n = 7500) and validation data (n = 2500) were randomly sampled and the binary classification prediction models for G1, early S, and S/G2/M phases of the cell cycle were developed using four supervised machine learning algorithms. We selected random forest as the model with the best performance through 10-fold cross-validation; the accuracy rate was approximately 75\%–87\%. Regarding feature importance, variables expected to be biologically related to the cell cycle, for example, signal intensity and nuclear size, were highly ranked, suggesting the validity of the model. These results showed that the cell cycle can be predicted in cancer cells by simply exploiting the current prediction model using fluorescent images of {DNA}-staining dye, and the model could be applied for the use of future ex vivo drug sensitivity diagnosis.},
	pages = {e1203},
	number = {3},
	journal = {Pharmacology Research \& Perspectives},
	author = {Takeuchi, Kazuma and Nishimura, Yumiko and Matsubara, Takayoshi and Isoyama, Sho and Suzuki, Asuka and Matsuura, Masaaki and Dan, Shingo},
	urldate = {2025-01-24},
	date = {2024},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/prp2.1203},
	keywords = {cell cycle, cell line, high-content imaging analysis, machine learning},
}

@article{kastan_cell-cycle_2004,
	title = {Cell-cycle checkpoints and cancer},
	volume = {432},
	issn = {1476-4687},
	abstract = {All life on earth must cope with constant exposure to DNA-damaging agents such as the Sun's radiation. Highly conserved DNA-repair and cell-cycle checkpoint pathways allow cells to deal with both endogenous and exogenous sources of DNA damage. How much an individual is exposed to these agents and how their cells respond to DNA damage are critical determinants of whether that individual will develop cancer. These cellular responses are also important for determining toxicities and responses to current cancer therapies, most of which target the DNA.},
	number = {7015},
	journal = {Nature},
	author = {Kastan, Michael B. and Bartek, Jiri},
	month = nov,
	year = {2004},
	pages = {316--323},
}

@software{meert_dtaidistance_2020,
	title = {{DTAIDistance}},
	abstract = {Improved compilation for M1/M2, and Py3.7
	
Improved plotting warping paths
	
Improved kmeans clustering for ndim},
	version = {v2.3.10},
	publisher = {Zenodo},
	author = {Meert, Wannes and Hendrickx, Kilian and Van Craenendonck, Toon and Robberechts, Pieter and Blockeel, Hendrik and Davis, Jesse},
	urldate = {2025-01-24},
	date = {2020-08-11},
}

@article{liu_mechanisms_2018,
	title = {Mechanisms of the {CDK}4/6 inhibitor palbociclib ({PD} 0332991) and its future application in cancer treatment (Review)},
	volume = {39},
	issn = {1021-335X},
	abstract = {An uncontrolled cell cycle is an obvious marker of tumor cells. The G1‑S phase is an important restriction point in the normal cell cycle, but in cancer cells the restriction function is reduced, leading to uncontrolled cell proliferation. Two cyclin‑dependent kinases ({CDKs}), {CDK}4 and {CDK}6, play a crucial role in the G1‑S phase transition. Inhibitors of {CDK}4/6 are presently the subjects of numerous studies, and {PD} 0332991, an inhibitor of {CDK}4/6, has been used to treat hormone receptor ({HR})‑positive, advanced‑stage breast cancer. This inhibitor has also been studied in other cancers, such as lung cancer. In this review, we will discuss the regulation of the normal cell cycle transition from G1 to S phase, the most promising inhibitor of {CDK}4/6, {PD} 0332991, as applied in different cancers, and finally we propose a mechanism of acquired resistance as well as the incredible potential for {CDK}4/6 inhibitors in the treatment of cancer. Briefly, we assert that, going forward, a new treatment pattern for cancer may be a combination therapy with a cell cycle inhibitor and a molecular targeted drug.},
	pages = {901--911},
	number = {3},
	journal = {Oncology Reports},
	author = {Liu, Minghui and Liu, Hongyu and Chen, Jun},
	urldate = {2025-01-23},
	date = {2018-03-01},
	note = {Publisher: Spandidos Publications},
}

@article{tinevez_trackmate_2017,
	title = {{TrackMate}: An open and extensible platform for single-particle tracking},
	volume = {115},
	issn = {1046-2023},
	series = {Image Processing for Biologists},
	shorttitle = {{TrackMate}},
	abstract = {We present {TrackMate}, an open source Fiji plugin for the automated, semi-automated, and manual tracking of single-particles. It offers a versatile and modular solution that works out of the box for end users, through a simple and intuitive user interface. It is also easily scriptable and adaptable, operating equally well on 1D over time, 2D over time, 3D over time, or other single and multi-channel image variants. {TrackMate} provides several visualization and analysis tools that aid in assessing the relevance of results. The utility of {TrackMate} is further enhanced through its ability to be readily customized to meet specific tracking problems. {TrackMate} is an extensible platform where developers can easily write their own detection, particle linking, visualization or analysis algorithms within the {TrackMate} environment. This evolving framework provides researchers with the opportunity to quickly develop and optimize new algorithms based on existing {TrackMate} modules without the need of having to write de novo user interfaces, including visualization, analysis and exporting tools. The current capabilities of {TrackMate} are presented in the context of three different biological problems. First, we perform Caenorhabditis-elegans lineage analysis to assess how light-induced damage during imaging impairs its early development. Our {TrackMate}-based lineage analysis indicates the lack of a cell-specific light-sensitive mechanism. Second, we investigate the recruitment of {NEMO} ({NF}-κB essential modulator) clusters in fibroblasts after stimulation by the cytokine {IL}-1 and show that photodamage can generate artifacts in the shape of {TrackMate} characterized movements that confuse motility analysis. Finally, we validate the use of {TrackMate} for quantitative lifetime analysis of clathrin-mediated endocytosis in plant cells.},
	pages = {80--90},
	journal = {Methods},
	shortjournal = {Methods},
	author = {Tinevez, Jean-Yves and Perry, Nick and Schindelin, Johannes and Hoopes, Genevieve M. and Reynolds, Gregory D. and Laplantine, Emmanuel and Bednarek, Sebastian Y. and Shorte, Spencer L. and Eliceiri, Kevin W.},
	urldate = {2025-01-23},
    year = {2017},
	date = {2017-02-15},
	keywords = {Clathin-mediated endocytosis, Image analysis, Microscopy, Open-source software, Phototoxicity, Single-particle tracking},
}

@inproceedings{weigert_nuclei_2022,
	title = {Nuclei Instance Segmentation and Classification in Histopathology Images with Stardist},
	abstract = {Instance segmentation and classification of nuclei is an impor-tant task in computational pathology. We show that {StarDist}, a deep learning nuclei segmentation method originally devel-oped for fluorescence microscopy, can be extended and suc-cessfully applied to histopathology images. This is substan-tiated by conducting experiments on the Lizard dataset, and through entering the Colon Nuclei Identification and Counting ({CoNIC}) challenge 2022, where our approach achieved the first spot on the leaderboard for the segmentation and clas-sification task for both the preliminary and final test phase.},
	eventtitle = {2022 {IEEE} International Symposium on Biomedical Imaging Challenges ({ISBIC})},
	pages = {1--4},
	booktitle = {2022 {IEEE} International Symposium on Biomedical Imaging Challenges ({ISBIC})},
	author = {Weigert, Martin and Schmidt, Uwe},
	urldate = {2025-01-23},
	date = {2022-03},
	keywords = {Colon, Deep learning, Fluorescence, Histopathology, Image segmentation, Microscopy, Task analysis, challenge, deep learning, histopathology, image segmentation},
}

@article{preibisch_globally_2009,
	title = {Globally optimal stitching of tiled 3D microscopic image acquisitions},
	volume = {25},
	issn = {1367-4803},
	abstract = {Motivation: Modern anatomical and developmental studies often require high-resolution imaging of large specimens in three dimensions (3D). Confocal microscopy produces high-resolution 3D images, but is limited by a relatively small field of view compared with the size of large biological specimens. Therefore, motorized stages that move the sample are used to create a tiled scan of the whole specimen. The physical coordinates provided by the microscope stage are not precise enough to allow direct reconstruction (Stitching) of the whole image from individual image stacks.Results: To optimally stitch a large collection of 3D confocal images, we developed a method that, based on the Fourier Shift Theorem, computes all possible translations between pairs of 3D images, yielding the best overlap in terms of the cross-correlation measure and subsequently finds the globally optimal configuration of the whole group of 3D images. This method avoids the propagation of errors by consecutive registration steps. Additionally, to compensate the brightness differences between tiles, we apply a smooth, non-linear intensity transition between the overlapping images. Our stitching approach is fast, works on 2D and 3D images, and for small image sets does not require prior knowledge about the tile configuration.Availability: The implementation of this method is available as an {ImageJ} plugin distributed as a part of the Fiji project ({FijiisjustImageJ}: http://pacific.mpi-cbg.de/).Contact:  tomancak@mpi-cbg.de},
	pages = {1463--1465},
	number = {11},
	journal = {Bioinformatics},
	shortjournal = {Bioinformatics},
	author = {Preibisch, Stephan and Saalfeld, Stephan and Tomancak, Pavel},
	urldate = {2025-01-23},
	date = {2009-06-01},
}


@article{khan_transformers_2022,
	title = {Transformers in Vision: A Survey},
	volume = {54},
	issn = {0360-0300},
	shorttitle = {Transformers in Vision},
	abstract = {Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks, e.g., Long short-term memory. Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text, and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers, i.e., self-attention, large-scale pre-training, and bidirectional feature encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization), and three-dimensional analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works. We hope this effort will ignite further interest in the community to solve current challenges toward the application of transformer models in computer vision.},
	pages = {200:1--200:41},
	number = {10},
	journal = {{ACM} Comput. Surv.},
	author = {Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
	urldate = {2025-01-23},
	date = {2022-09-13},
}

@article{selva_video_2023,
	title = {Video Transformers: A Survey},
	volume = {45},
	issn = {1939-3539},
	shorttitle = {Video Transformers},
	abstract = {Transformer models have shown great success handling long-range interactions, making them a promising tool for modeling video. However, they lack inductive biases and scale quadratically with input length. These limitations are further exacerbated when dealing with the high dimensionality introduced by the temporal dimension. While there are surveys analyzing the advances of Transformers for vision, none focus on an in-depth analysis of video-specific designs. In this survey, we analyze the main contributions and trends of works leveraging Transformers to model video. Specifically, we delve into how videos are handled at the input level first. Then, we study the architectural changes made to deal with video more efficiently, reduce redundancy, re-introduce useful inductive biases, and capture long-term temporal dynamics. In addition, we provide an overview of different training regimes and explore effective self-supervised learning strategies for video. Finally, we conduct a performance comparison on the most common benchmark for Video Transformers (i.e., action classification), finding them to outperform 3D {ConvNets} even with less computational complexity.},
	pages = {12922--12943},
	number = {11},
	journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Selva, Javier and Johansen, Anders S. and Escalera, Sergio and Nasrollahi, Kamal and Moeslund, Thomas B. and Clapés, Albert},
	urldate = {2025-01-23},
	date = {2023-11},
	note = {Conference Name: {IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Artificial intelligence, Current transformers, Data models, Market research, Task analysis, Tokenization, Training, Visualization, computer vision, self-attention, transformers, video representations},
}

@article{tong_videomae_2022,
	title = {{VideoMAE}: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training},
	volume = {35},
	shorttitle = {{VideoMAE}},
	pages = {10078--10093},
	journal = {Advances in Neural Information Processing Systems},
	author = {Tong, Zhan and Song, Yibing and Wang, Jue and Wang, Limin},
	urldate = {2025-01-23},
	date = {2022-12-06},
	langid = {english},
}


@article{gu_mamba_2024,
  title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}


@misc{kraus_masked_2024,
	title = {Masked Autoencoders for Microscopy are Scalable Learners of Cellular Biology},
	abstract = {Featurizing microscopy images for use in biological research remains a significant challenge, especially for large-scale experiments spanning millions of images. This work explores the scaling properties of weakly supervised classifiers and self-supervised masked autoencoders ({MAEs}) when training with increasingly larger model backbones and microscopy datasets. Our results show that {ViT}-based {MAEs} outperform weakly supervised classifiers on a variety of tasks, achieving as much as a 11.5\% relative improvement when recalling known biological relationships curated from public databases. Additionally, we develop a new channel-agnostic {MAE} architecture ({CA}-{MAE}) that allows for inputting images of different numbers and orders of channels at inference time. We demonstrate that {CA}-{MAEs} effectively generalize by inferring and evaluating on a microscopy image dataset ({JUMP}-{CP}) generated under different experimental conditions with a different channel structure than our pretraining data ({RPI}-93M). Our findings motivate continued research into scaling self-supervised learning on microscopy data in order to create powerful foundation models of cellular biology that have the potential to catalyze advancements in drug discovery and beyond.},
	number = {{arXiv}:2404.10242},
	publisher = {{arXiv}},
	author = {Kraus, Oren and Kenyon-Dean, Kian and Saberian, Saber and Fallah, Maryam and {McLean}, Peter and Leung, Jess and Sharma, Vasudev and Khan, Ayla and Balakrishnan, Jia and Celik, Safiye and Beaini, Dominique and Sypetkowski, Maciej and Cheng, Chi Vicky and Morse, Kristen and Makes, Maureen and Mabey, Ben and Earnshaw, Berton},
	urldate = {2025-01-23},
	date = {2024-04-16},
	eprinttype = {arxiv},
	eprint = {2404.10242 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{melnychuk_causal_2022,
	title = {Causal Transformer for Estimating Counterfactual Outcomes},
	abstract = {Estimating counterfactual outcomes over time from observational data is relevant for many applications (e.g., personalized medicine). Yet, state-of-the-art methods build upon simple long short-term memory ({LSTM}) networks, thus rendering inferences for complex, long-range dependencies challenging. In this paper, we develop a novel Causal Transformer for estimating counterfactual outcomes over time. Our model is specifically designed to capture complex, long-range dependencies among time-varying confounders. For this, we combine three transformer subnetworks with separate inputs for time-varying covariates, previous treatments, and previous outcomes into a joint network with in-between cross-attentions. We further develop a custom, end-to-end training procedure for our Causal Transformer. Specifically, we propose a novel counterfactual domain confusion loss to address confounding bias: it aims to learn adversarial balanced representations, so that they are predictive of the next outcome but non-predictive of the current treatment assignment. We evaluate our Causal Transformer based on synthetic and real-world datasets, where it achieves superior performance over current baselines. To the best of our knowledge, this is the first work proposing transformer-based architecture for estimating counterfactual outcomes from longitudinal data.},
	eventtitle = {International Conference on Machine Learning},
	pages = {15293--15329},
	booktitle = {Proceedings of the 39th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Melnychuk, Valentyn and Frauen, Dennis and Feuerriegel, Stefan},
	urldate = {2025-01-23},
	date = {2022-06-28},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@inproceedings{vaswani_attention_nodate,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 volume = {30},
 year = {2017}
}

@article{hochreiter_long_1997,
	title = {Long Short-Term Memory},
	volume = {9},
	issn = {0899-7667},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory ({LSTM}). Truncating the gradient where this does not do harm, {LSTM} can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. {LSTM} is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, {LSTM} leads to many more successful runs, and learns much faster. {LSTM} also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	pages = {1735--1780},
	number = {8},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	urldate = {2025-01-23},
    year = {1997},
	date = {1997-11},
	note = {Conference Name: Neural Computation},
}

@article{sakaue-sawano_visualizing_2008,
	title = {Visualizing Spatiotemporal Dynamics of Multicellular Cell-Cycle Progression},
	volume = {132},
	issn = {0092-8674, 1097-4172},
	pages = {487--498},
	number = {3},
	journal = {Cell},
	shortjournal = {Cell},
	author = {Sakaue-Sawano, Asako and Kurokawa, Hiroshi and Morimura, Toshifumi and Hanyu, Aki and Hama, Hiroshi and Osawa, Hatsuki and Kashiwagi, Saori and Fukami, Kiyoko and Miyata, Takaki and Miyoshi, Hiroyuki and Imamura, Takeshi and Ogawa, Masaharu and Masai, Hisao and Miyawaki, Atsushi},
	urldate = {2025-01-23},
	date = {2008-02-08},
    year = {2008},
	pmid = {18267078},
    journal={Cell},
	keywords = {{CELLCYCLE}},
}

@article{wang_live-cell_2020,
	title = {Live-cell imaging and analysis reveal cell phenotypic transition dynamics inherently missing in snapshot data},
	volume = {6},
	abstract = {Recent advances in single-cell techniques catalyze an emerging field of studying how cells convert from one phenotype to another, in a step-by-step process. Two grand technical challenges, however, impede further development of the field. Fixed cell–based approaches can provide snapshots of high-dimensional expression profiles but have fundamental limits on revealing temporal information, and fluorescence-based live-cell imaging approaches provide temporal information but are technically challenging for multiplex long-term imaging. We first developed a live-cell imaging platform that tracks cellular status change through combining endogenous fluorescent labeling that minimizes perturbation to cell physiology and/or live-cell imaging of high-dimensional cell morphological and texture features. With our platform and an A549 {VIM}-{RFP} epithelial-to-mesenchymal transition ({EMT}) reporter cell line, live-cell trajectories reveal parallel paths of {EMT} missing from snapshot data due to cell-cell dynamic heterogeneity. Our results emphasize the necessity of extracting dynamical information of phenotypic transitions from multiplex live-cell imaging.},
	pages = {eaba9319},
	number = {36},
	journal = {Science Advances},
	author = {Wang, Weikang and Douglas, Diana and Zhang, Jingyu and Kumari, Sangeeta and Enuameh, Metewo Selase and Dai, Yan and Wallace, Callen T. and Watkins, Simon C. and Shu, Weiguo and Xing, Jianhua},
	urldate = {2025-01-23},
	date = {2020-09-04},
    year = {2020},
	note = {Publisher: American Association for the Advancement of Science},
}

@article{moreno-andres_livecellminer_2022,
	title = {{LiveCellMiner}: A new tool to analyze mitotic progression},
	volume = {17},
	issn = {1932-6203},
	shorttitle = {{LiveCellMiner}},
	abstract = {Live-cell imaging has become state of the art to accurately identify the nature of mitotic and cell cycle defects. Low- and high-throughput microscopy setups have yield huge data amounts of cells recorded in different experimental and pathological conditions. Tailored semi-automated and automated image analysis approaches allow the analysis of high-content screening data sets, saving time and avoiding bias. However, they were mostly designed for very specific experimental setups, which restricts their flexibility and usability. The general need for dedicated experiment-specific user-annotated training sets and experiment-specific user-defined segmentation parameters remains a major bottleneck for fully automating the analysis process. In this work we present {LiveCellMiner}, a highly flexible open-source software tool to automatically extract, analyze and visualize both aggregated and time-resolved image features with potential biological relevance. The software tool allows analysis across high-content data sets obtained in different platforms, in a quantitative and unbiased manner. As proof of principle application, we analyze here the dynamic chromatin and tubulin cytoskeleton features in human cells passing through mitosis highlighting the versatile and flexible potential of this tool set.},
	pages = {e0270923},
	number = {7},
	journal = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Moreno-Andrés, Daniel and Bhattacharyya, Anuk and Scheufen, Anja and Stegmaier, Johannes},
	urldate = {2025-01-23},
	date = {2022-07-07},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Anaphase, Cell cycle and cell division, Chromatin, Data visualization, Metaphase, Mitosis, Morphogenic segmentation, Prophase},
}

@article{held_cellcognition_2010,
	title = {{CellCognition}: time-resolved phenotype annotation in high-throughput live cell imaging},
	volume = {7},
	rights = {2010 Springer Nature America, Inc.},
	issn = {1548-7105},
	shorttitle = {{CellCognition}},
	abstract = {Incorporation of time information into the annotation of distinct biological states in automated fluorescence time-lapse live-cell imaging of complex cellular dynamics reduces both classification noise and confusion between cell states with similar morphology. A computational framework for achieving this is implemented in the open-source software package {CellCognition}.},
	pages = {747--754},
	number = {9},
	journal = {Nature Methods},
	shortjournal = {Nat Methods},
	author = {Held, Michael and Schmitz, Michael H. A. and Fischer, Bernd and Walter, Thomas and Neumann, Beate and Olma, Michael H. and Peter, Matthias and Ellenberg, Jan and Gerlich, Daniel W.},
	urldate = {2025-01-23},
	date = {2010-09},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Machine learning, Software, Time-lapse imaging},
}


@article{jose_automatic_2024,
	title = {Automatic detection of cell-cycle stages using recurrent neural networks.},
	volume = {19},
	copyright = {Copyright: © 2024 Jose et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted  use, distribution, and reproduction in any medium, provided the original author  and source are credited.},
	issn = {1932-6203},
	abstract = {Mitosis is the process by which eukaryotic cells divide to produce two similar daughter cells with identical genetic material. Research into the process of  mitosis is therefore of critical importance both for the basic understanding of  cell biology and for the clinical approach to manifold pathologies resulting from  its malfunctioning, including cancer. In this paper, we propose an approach to  study mitotic progression automatically using deep learning. We used neural  networks to predict different mitosis stages. We extracted video sequences of  cells undergoing division and trained a Recurrent Neural Network (RNN) to extract  image features. The use of RNN enabled better extraction of features. The  RNN-based approach gave better performance compared to classifier based feature  extraction methods which do not use time information. Evaluation of precision,  recall, and F-score indicates the superiority of the proposed model compared to  the baseline. To study the loss in performance due to confusion between adjacent  classes, we plotted the confusion matrix as well. In addition, we visualized the  feature space to understand why RNNs are better at classifying the mitosis stages  than other classifier models, which indicated the formation of strong clusters  for the different classes, clearly confirming the advantage of the proposed  RNN-based approach.},
	language = {eng},
	number = {3},
	journal = {PloS one},
	author = {Jose, Abin and Roy, Rijo and Moreno-Andrés, Daniel and Stegmaier, Johannes},
	year = {2024},
	pmid = {38466708},
	pmcid = {PMC10927108},
	keywords = {*Mitosis, *Neural Networks, Computer},
	pages = {e0297356},
}



@article{narotamo_machine_2021,
	title = {A machine learning approach for single cell interphase cell cycle staging},
	volume = {11},
	rights = {2021 The Author(s)},
	issn = {2045-2322},
	abstract = {The cell nucleus is a tightly regulated organelle and its architectural structure is dynamically orchestrated to maintain normal cell function. Indeed, fluctuations in nuclear size and shape are known to occur during the cell cycle and alterations in nuclear morphology are also hallmarks of many diseases including cancer. Regrettably, automated reliable tools for cell cycle staging at single cell level using in situ images are still limited. It is therefore urgent to establish accurate strategies combining bioimaging with high-content image analysis for a bona fide classification. In this study we developed a supervised machine learning method for interphase cell cycle staging of individual adherent cells using in situ fluorescence images of nuclei stained with {DAPI}. A Support Vector Machine ({SVM}) classifier operated over normalized nuclear features using more than 3500 {DAPI} stained nuclei. Molecular ground truth labels were obtained by automatic image processing using fluorescent ubiquitination-based cell cycle indicator (Fucci) technology. An average F1-Score of 87.7\% was achieved with this framework. Furthermore, the method was validated on distinct cell types reaching recall values higher than 89\%. Our method is a robust approach to identify cells in G1 or S/G2 at the individual level, with implications in research and clinical applications.},
	pages = {19278},
	number = {1},
	journal = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Narotamo, Hemaxi and Fernandes, Maria Sofia and Moreira, Ana Margarida and Melo, Soraia and Seruca, Raquel and Silveira, Margarida and Sanches, João Miguel},
	urldate = {2025-01-17},
	date = {2021-09-29},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cellular imaging, Genomics, Image processing},
}

@misc{zhang_mixste_2022,
	title = {{MixSTE}: Seq2seq Mixed Spatio-Temporal Encoder for 3D Human Pose Estimation in Video},
	shorttitle = {{MixSTE}},
	abstract = {Recent transformer-based solutions have been introduced to estimate 3D human pose from 2D keypoint sequence by considering body joints among all frames globally to learn spatio-temporal correlation. We observe that the motions of different joints differ significantly. However, the previous methods cannot efficiently model the solid inter-frame correspondence of each joint, leading to insufficient learning of spatial-temporal correlation. We propose {MixSTE} (Mixed Spatio-Temporal Encoder), which has a temporal transformer block to separately model the temporal motion of each joint and a spatial transformer block to learn inter-joint spatial correlation. These two blocks are utilized alternately to obtain better spatio-temporal feature encoding. In addition, the network output is extended from the central frame to entire frames of the input video, thereby improving the coherence between the input and output sequences. Extensive experiments are conducted on three benchmarks (Human3.6M, {MPI}-{INF}-3DHP, and {HumanEva}). The results show that our model outperforms the state-of-the-art approach by 10.9\% P-{MPJPE} and 7.6\% {MPJPE}. The code is available at https://github.com/{JinluZhang}1126/{MixSTE}.},
	number = {{arXiv}:2203.00859},
	publisher = {{arXiv}},
	author = {Zhang, Jinlu and Tu, Zhigang and Yang, Jianyu and Chen, Yujin and Yuan, Junsong},
	urldate = {2025-01-10},
	date = {2022-04-25},
	eprinttype = {arxiv},
	eprint = {2203.00859 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{patro_mamba-360_2024,
	title = {Mamba-360: Survey of State Space Models as Transformer Alternative for Long Sequence Modelling: Methods, Applications, and Challenges},
	shorttitle = {Mamba-360},
	abstract = {Sequence modeling is a crucial area across various domains, including Natural Language Processing ({NLP}), speech recognition, time series forecasting, music generation, and bioinformatics. Recurrent Neural Networks ({RNNs}) and Long Short Term Memory Networks ({LSTMs}) have historically dominated sequence modeling tasks like Machine Translation, Named Entity Recognition ({NER}), etc. However, the advancement of transformers has led to a shift in this paradigm, given their superior performance. Yet, transformers suffer from \$O(N{\textasciicircum}2)\$ attention complexity and challenges in handling inductive bias. Several variations have been proposed to address these issues which use spectral networks or convolutions and have performed well on a range of tasks. However, they still have difficulty in dealing with long sequences. State Space Models({SSMs}) have emerged as promising alternatives for sequence modeling paradigms in this context, especially with the advent of S4 and its variants, such as S4nd, Hippo, Hyena, Diagnol State Spaces ({DSS}), Gated State Spaces ({GSS}), Linear Recurrent Unit ({LRU}), Liquid-S4, Mamba, etc. In this survey, we categorize the foundational {SSMs} based on three paradigms namely, Gating architectures, Structural architectures, and Recurrent architectures. This survey also highlights diverse applications of {SSMs} across domains such as vision, video, audio, speech, language (especially long sequence modeling), medical (including genomics), chemical (like drug design), recommendation systems, and time series analysis, including tabular data. Moreover, we consolidate the performance of {SSMs} on benchmark datasets like Long Range Arena ({LRA}), {WikiText}, Glue, Pile, {ImageNet}, Kinetics-400, sstv2, as well as video datasets such as Breakfast, {COIN}, {LVU}, and various time series datasets. The project page for Mamba-360 work is available on this webpage.{\textbackslash}url\{https://github.com/badripatro/mamba360\}.},
	number = {{arXiv}:2404.16112},
	publisher = {{arXiv}},
	author = {Patro, Badri Narayana and Agneeswaran, Vijay Srinivas},
	urldate = {2025-01-07},
	date = {2024-04-24},
	eprinttype = {arxiv},
	eprint = {2404.16112 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Multimedia, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{gu_efficiently_2022,
	title = {Efficiently Modeling Long Sequences with Structured State Spaces},
	abstract = {A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including {RNNs}, {CNNs}, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of \$10000\$ or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model ({SSM}) {\textbackslash}( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) {\textbackslash}), and showed that for appropriate choices of the state matrix {\textbackslash}( A {\textbackslash}), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the {SSM}, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning {\textbackslash}( A {\textbackslash}) with a low-rank correction, allowing it to be diagonalized stably and reducing the {SSM} to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91{\textbackslash}\% accuracy on sequential {CIFAR}-10 with no data augmentation or auxiliary losses, on par with a larger 2-D {ResNet}, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation \$60{\textbackslash}times\$ faster (iii) {SoTA} on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.},
	number = {{arXiv}:2111.00396},
	publisher = {{arXiv}},
	author = {Gu, Albert and Goel, Karan and Ré, Christopher},
	urldate = {2025-01-07},
	date = {2022-08-05},
    year={2022},
	journal = {arXiv},
	eprint = {2111.00396 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{donahue_long-term_nodate,
	title = {Long-Term Recurrent Convolutional Networks for Visual Recognition and Description},
	abstract = {Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or “temporally deep”, are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a ﬁxed spatio-temporal receptive ﬁeld or simple temporal averaging for sequential processing, recurrent convolutional models are “doubly deep” in that they can be compositional in spatial and temporal “layers”. Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term {RNN} models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately deﬁned and/or optimized.},
	author = {Donahue, Jeffrey and Hendricks, Lisa Anne and Guadarrama, Sergio and Rohrbach, Marcus and Venugopalan, Subhashini and Saenko, Kate and Darrell, Trevor},
	langid = {english},
}

@article{jin_imbalanced_2021,
	title = {An Imbalanced Image Classification Method for the Cell Cycle Phase},
	volume = {12},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2078-2489},
	abstract = {The cell cycle is an important process in cellular life. In recent years, some image processing methods have been developed to determine the cell cycle stages of individual cells. However, in most of these methods, cells have to be segmented, and their features need to be extracted. During feature extraction, some important information may be lost, resulting in lower classification accuracy. Thus, we used a deep learning method to retain all cell features. In order to solve the problems surrounding insufficient numbers of original images and the imbalanced distribution of original images, we used the Wasserstein generative adversarial network-gradient penalty ({WGAN}-{GP}) for data augmentation. At the same time, a residual network ({ResNet}) was used for image classification. {ResNet} is one of the most used deep learning classification networks. The classification accuracy of cell cycle images was achieved more effectively with our method, reaching 83.88\%. Compared with an accuracy of 79.40\% in previous experiments, our accuracy increased by 4.48\%. Another dataset was used to verify the effect of our model and, compared with the accuracy from previous results, our accuracy increased by 12.52\%. The results showed that our new cell cycle image classification system based on {WGAN}-{GP} and {ResNet} is useful for the classification of imbalanced images. Moreover, our method could potentially solve the low classification accuracy in biomedical images caused by insufficient numbers of original images and the imbalanced distribution of original images.},
	pages = {249},
	number = {6},
	journal = {Information},
	author = {Jin, Xin and Zou, Yuanwen and Huang, Zhongbing},
	urldate = {2025-01-06},
	date = {2021-06},
	langid = {english},
	note = {Number: 6
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Wasserstein generative adversarial network-gradient penalty, cell cycle, deep learning, image classification, imbalanced image datasets, residual network},
}

@article{eulenberg_reconstructing_2017,
	title = {Reconstructing cell cycle and disease progression using deep learning},
	volume = {8},
	rights = {2017 The Author(s)},
	issn = {2041-1723},
	abstract = {We show that deep convolutional neural networks combined with nonlinear dimension reduction enable reconstructing biological processes based on raw image data. We demonstrate this by reconstructing the cell cycle of Jurkat cells and disease progression in diabetic retinopathy. In further analysis of Jurkat cells, we detect and separate a subpopulation of dead cells in an unsupervised manner and, in classifying discrete cell cycle stages, we reach a sixfold reduction in error rate compared to a recent approach based on boosting on image features. In contrast to previous methods, deep learning based predictions are fast enough for on-the-fly analysis in an imaging flow cytometer.},
	pages = {463},
	number = {1},
	journal = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Eulenberg, Philipp and Köhler, Niklas and Blasi, Thomas and Filby, Andrew and Carpenter, Anne E. and Rees, Paul and Theis, Fabian J. and Wolf, F. Alexander},
	urldate = {2025-01-06},
	date = {2017-09-06},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cell division, Image processing, Machine learning},
}

@article{blasi_label-free_2016,
	title = {Label-free cell cycle analysis for high-throughput imaging flow cytometry},
	volume = {7},
	rights = {2016 The Author(s)},
	issn = {2041-1723},
	abstract = {Imaging flow cytometry combines the high-throughput capabilities of conventional flow cytometry with single-cell imaging. Here we demonstrate label-free prediction of {DNA} content and quantification of the mitotic cell cycle phases by applying supervised machine learning to morphological features extracted from brightfield and the typically ignored darkfield images of cells from an imaging flow cytometer. This method facilitates non-destructive monitoring of cells avoiding potentially confounding effects of fluorescent stains while maximizing available fluorescence channels. The method is effective in cell cycle analysis for mammalian cells, both fixed and live, and accurately assesses the impact of a cell cycle mitotic phase blocking agent. As the same method is effective in predicting the {DNA} content of fission yeast, it is likely to have a broad application to other cell types.},
	pages = {10256},
	number = {1},
	journal = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Blasi, Thomas and Hennig, Holger and Summers, Huw D. and Theis, Fabian J. and Cerveira, Joana and Patterson, James O. and Davies, Derek and Filby, Andrew and Carpenter, Anne E. and Rees, Paul},
	urldate = {2025-01-06},
	date = {2016-01-07},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Bioinformatics, Cell division, Flow cytometry},
}

@misc{kraus_masked_2024-1,
	title = {Masked Autoencoders for Microscopy are Scalable Learners of Cellular Biology},
	abstract = {Featurizing microscopy images for use in biological research remains a significant challenge, especially for large-scale experiments spanning millions of images. This work explores the scaling properties of weakly supervised classifiers and self-supervised masked autoencoders ({MAEs}) when training with increasingly larger model backbones and microscopy datasets. Our results show that {ViT}-based {MAEs} outperform weakly supervised classifiers on a variety of tasks, achieving as much as a 11.5\% relative improvement when recalling known biological relationships curated from public databases. Additionally, we develop a new channel-agnostic {MAE} architecture ({CA}-{MAE}) that allows for inputting images of different numbers and orders of channels at inference time. We demonstrate that {CA}-{MAEs} effectively generalize by inferring and evaluating on a microscopy image dataset ({JUMP}-{CP}) generated under different experimental conditions with a different channel structure than our pretraining data ({RPI}-93M). Our findings motivate continued research into scaling self-supervised learning on microscopy data in order to create powerful foundation models of cellular biology that have the potential to catalyze advancements in drug discovery and beyond.},
	number = {{arXiv}:2404.10242},
	publisher = {{arXiv}},
	author = {Kraus, Oren and Kenyon-Dean, Kian and Saberian, Saber and Fallah, Maryam and {McLean}, Peter and Leung, Jess and Sharma, Vasudev and Khan, Ayla and Balakrishnan, Jia and Celik, Safiye and Beaini, Dominique and Sypetkowski, Maciej and Cheng, Chi Vicky and Morse, Kristen and Makes, Maureen and Mabey, Ben and Earnshaw, Berton},
	urldate = {2024-12-20},
	date = {2024-04-16},
	eprinttype = {arxiv},
	eprint = {2404.10242 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{kraus_masked_2024-2,
	title = {Masked Autoencoders for Microscopy are Scalable Learners of Cellular Biology},
	abstract = {Featurizing microscopy images for use in biological research remains a significant challenge, especially for large-scale experiments spanning millions of images. This work explores the scaling properties of weakly supervised classifiers and self-supervised masked autoencoders ({MAEs}) when training with increasingly larger model backbones and microscopy datasets. Our results show that {ViT}-based {MAEs} outperform weakly supervised classifiers on a variety of tasks, achieving as much as a 11.5\% relative improvement when recalling known biological relationships curated from public databases. Additionally, we develop a new channel-agnostic {MAE} architecture ({CA}-{MAE}) that allows for inputting images of different numbers and orders of channels at inference time. We demonstrate that {CA}-{MAEs} effectively generalize by inferring and evaluating on a microscopy image dataset ({JUMP}-{CP}) generated under different experimental conditions with a different channel structure than our pretraining data ({RPI}-93M). Our findings motivate continued research into scaling self-supervised learning on microscopy data in order to create powerful foundation models of cellular biology that have the potential to catalyze advancements in drug discovery and beyond.},
	number = {{arXiv}:2404.10242},
	publisher = {{arXiv}},
	author = {Kraus, Oren and Kenyon-Dean, Kian and Saberian, Saber and Fallah, Maryam and {McLean}, Peter and Leung, Jess and Sharma, Vasudev and Khan, Ayla and Balakrishnan, Jia and Celik, Safiye and Beaini, Dominique and Sypetkowski, Maciej and Cheng, Chi Vicky and Morse, Kristen and Makes, Maureen and Mabey, Ben and Earnshaw, Berton},
	urldate = {2024-12-20},
	date = {2024-04-16},
	eprinttype = {arxiv},
	eprint = {2404.10242 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{li_predicting_2024,
	title = {Predicting cell cycle stage from 3D single-cell nuclear-stained images},
	rights = {© 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial} 4.0 International), {CC} {BY}-{NC} 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	abstract = {The cell cycle governs the proliferation, differentiation, and regeneration of all eukaryotic cells. Profiling cell cycle dynamics is therefore central to basic and biomedical research spanning development, health, aging, and disease. However, current approaches to cell cycle profiling involve complex interventions that may confound experimental interpretation. To facilitate more efficient cell cycle annotation of microscopy data, we developed {CellCycleNet}, a machine learning ({ML}) workflow designed to simplify cell cycle staging with minimal experimenter intervention and cost. {CellCycleNet} accurately predicts cell cycle phase using only a fluorescent nuclear stain ({DAPI}) in fixed interphase cells. Using the Fucci2a cell cycle reporter system as ground truth, we collected two benchmarking image datasets and trained two {ML} models—a support vector machine ({SVM}) and a deep neural network—to classify nuclei as being in either the G1 or S/G2 phases of the cell cycle. Our results suggest that {CellCycleNet} outperforms state-of-the-art {SVM} models on each dataset individually. When trained on two image datasets simultaneously, {CellCycleNet} achieves the highest classification accuracy, with an improvement in {AUROC} of 0.08–0.09. The model also demonstrates excellent generalization across different microscopes, achieving an {AUROC} of 0.95. Overall, using features derived from 3D images, rather than 2D projections of those same images, significantly improves classification performance. We have released our image data, trained models, and software as a community resource.},
	publisher = {{bioRxiv}},
	author = {Li, Gang and Nichols, Eva K. and Browning, Valentino E. and Longhi, Nicolas J. and Camplisson, Conor and Beliveau, Brian J. and Noble, William Stafford},
	urldate = {2024-12-12},
	date = {2024-09-01},
    journal={bioRxiv},
	langid = {english},
}

@article{rosen_toward_2024,
	title = {Toward universal cell embeddings: integrating single-cell {RNA}-seq datasets across species with {SATURN}},
	volume = {21},
	rights = {2024 The Author(s)},
	issn = {1548-7105},
	shorttitle = {Toward universal cell embeddings},
	abstract = {Analysis of single-cell datasets generated from diverse organisms offers unprecedented opportunities to unravel fundamental evolutionary processes of conservation and diversification of cell types. However, interspecies genomic differences limit the joint analysis of cross-species datasets to homologous genes. Here we present {SATURN}, a deep learning method for learning universal cell embeddings that encodes genes’ biological properties using protein language models. By coupling protein embeddings from language models with {RNA} expression, {SATURN} integrates datasets profiled from different species regardless of their genomic similarity. {SATURN} can detect functionally related genes coexpressed across species, redefining differential expression for cross-species analysis. Applying {SATURN} to three species whole-organism atlases and frog and zebrafish embryogenesis datasets, we show that {SATURN} can effectively transfer annotations across species, even when they are evolutionarily remote. We also demonstrate that {SATURN} can be used to find potentially divergent gene functions between glaucoma-associated genes in humans and four other species.},
	pages = {1492--1500},
	number = {8},
	journal = {Nature Methods},
	shortjournal = {Nat Methods},
	author = {Rosen, Yanay and Brbić, Maria and Roohani, Yusuf and Swanson, Kyle and Li, Ziang and Leskovec, Jure},
	urldate = {2024-12-11},
	date = {2024-08},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Data integration, Evolution, Machine learning, Software, Transcriptomics},
}

@misc{dao_transformers_2024,
	title = {Transformers are {SSMs}: Generalized Models and Efficient Algorithms Through Structured State Space Duality},
	shorttitle = {Transformers are {SSMs}},
	abstract = {While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models ({SSMs}) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between {SSMs} and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality ({SSD}) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective {SSM} that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.},
	number = {{arXiv}:2405.21060},
	publisher = {{arXiv}},
	author = {Dao, Tri and Gu, Albert},
	urldate = {2024-12-06},
	date = {2024-05-31},
	eprinttype = {arxiv},
	eprint = {2405.21060 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@online{noauthor_self-supervised_nodate,
	title = {Self-supervised vision transformers accurately decode cellular state heterogeneity {\textbar} {bioRxiv}},
	urldate = {2024-10-07},
}

@article{ibrahim_label-free_2023,
	title = {Label-free identification of protein aggregates using deep learning},
	volume = {14},
	issn = {2041-1723},
	abstract = {Abstract
            Protein misfolding and aggregation play central roles in the pathogenesis of various neurodegenerative diseases ({NDDs}), including Huntington’s disease, which is caused by a genetic mutation in exon 1 of the Huntingtin protein (Httex1). The fluorescent labels commonly used to visualize and monitor the dynamics of protein expression have been shown to alter the biophysical properties of proteins and the final ultrastructure, composition, and toxic properties of the formed aggregates. To overcome this limitation, we present a method for label-free identification of {NDD}-associated aggregates ({LINA}). Our approach utilizes deep learning to detect unlabeled and unaltered Httex1 aggregates in living cells from transmitted-light images, without the need for fluorescent labeling. Our models are robust across imaging conditions and on aggregates formed by different constructs of Httex1. {LINA} enables the dynamic identification of label-free aggregates and measurement of their dry mass and area changes during their growth process, offering high speed, specificity, and simplicity to analyze protein aggregation dynamics and obtain high-fidelity information.},
	pages = {7816},
	number = {1},
	journal = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Ibrahim, Khalid A. and Grußmayer, Kristin S. and Riguet, Nathan and Feletti, Lely and Lashuel, Hilal A. and Radenovic, Aleksandra},
	urldate = {2024-08-29},
	date = {2023-11-28},
	langid = {english},
}

@article{sakaue-sawano_genetically_2017,
	title = {Genetically Encoded Tools for Optical Dissection of the Mammalian Cell Cycle},
	volume = {68},
	issn = {10972765},
	abstract = {Eukaryotic cells spend most of their life in interphase of the cell cycle. Understanding the rich diversity of metabolic and genomic regulation that occurs in interphase requires the demarcation of precise phase boundaries in situ. Here, we report the properties of two genetically encoded ﬂuorescence sensors, Fucci({CA}) and Fucci({SCA}), which enable realtime monitoring of interphase and cell-cycle biology. We re-engineered the Cdt1-based sensor from the original Fucci system to respond to S phase-speciﬁc {CUL}4Ddb1-mediated ubiquitylation alone or in combination with {SCFSkp}2-mediated ubiquitylation. In cultured cells, Fucci({CA}) produced a sharp triple color-distinct separation of G1, S, and G2, while Fucci({SCA}) permitted a two-color readout of G1 and S/G2. Fucci({CA}) applications included tracking the transient G1 phase of rapidly dividing mouse embryonic stem cells and identifying a window for {UVirradiation} damage in S phase. These results show that Fucci({CA}) is an essential tool for quantitative studies of interphase cell-cycle regulation.},
	pages = {626--640.e5},
	number = {3},
	journal = {Molecular Cell},
	shortjournal = {Molecular Cell},
	author = {Sakaue-Sawano, Asako and Yo, Masahiro and Komatsu, Naoki and Hiratsuka, Toru and Kogure, Takako and Hoshida, Tetsushi and Goshima, Naoki and Matsuda, Michiyuki and Miyoshi, Hiroyuki and Miyawaki, Atsushi},
	urldate = {2021-11-05},
	date = {2017-11},
	langid = {english},
}

@article{koh_quantitative_2017,
	title = {A quantitative {FastFUCCI} assay defines cell cycle dynamics at a single-cell level},
	volume = {130},
	issn = {0021-9533, 1477-9137},
	abstract = {The fluorescence ubiquitination-based cell cycle indicator ({FUCCI}) is a powerful tool for use in live cells but current {FUCCI}-based assays have limited throughput in terms of image processing and quantification. Here, we developed a lentiviral system that rapidly introduced {FUCCI} transgenes into cells by using an all-in-one expression cassette, {FastFUCCI}. The approach alleviated the need for sequential transduction and characterisation, improving labelling efficiency. We coupled the system to an automated imaging workflow capable of handling large datasets. The integrated assay enabled analyses of single-cell readouts at high spatiotemporal resolution. With the assay, we captured in detail the cell cycle alterations induced by antimitotic agents. We found that treated cells accumulated at G2 or M phase but eventually advanced through mitosis into the next interphase, where the majority of cell death occurred, irrespective of the preceding mitotic phenotype. Some cells appeared viable after mitotic slippage, and a fraction of them subsequently re-entered S phase. Accordingly, we found evidence that targeting the {DNA} replication origin activity sensitised cells to paclitaxel. In summary, we demonstrate the utility of the {FastFUCCI} assay for quantifying spatiotemporal dynamics and identify its potential in preclinical drug development.},
	pages = {512--520},
	number = {2},
	journal = {Journal of Cell Science},
	author = {Koh, Siang-Boon and Mascalchi, Patrice and Rodriguez, Esther and Lin, Yao and Jodrell, Duncan I. and Richards, Frances M. and Lyons, Scott K.},
	urldate = {2018-10-02},
	date = {2017-01-15},
	langid = {english},
}

@article{zikry_cell_2024,
	title = {Cell cycle plasticity underlies fractional resistance to palbociclib in {ER}+/{HER}2− breast tumor cells},
	volume = {121},
	issn = {0027-8424, 1091-6490},
	abstract = {The {CDK}4/6 inhibitor palbociclib blocks cell cycle progression in Estrogen receptor–positive, human epidermal growth factor 2 receptor–negative ({ER}+/{HER}2−) breast tumor cells. Despite the drug’s success in improving patient outcomes, a small percentage of tumor cells continues to divide in the presence of palbociclib—a phenomenon we refer to as fractional resistance. It is critical to understand the cellular mechanisms underlying fractional resistance because the precise percentage of resistant cells in patient tissue is a strong predictor of clinical outcomes. Here, we hypothesize that fractional resistance arises from cell-to-cell differences in core cell cycle regulators that allow a subset of cells to escape {CDK}4/6 inhibitor therapy. We used multiplex, single-cell imaging to identify fractionally resistant cells in both cultured and primary breast tumor samples resected from patients. Resistant cells showed premature accumulation of multiple G1 regulators including E2F1, retinoblastoma protein, and {CDK}2, as well as enhanced sensitivity to pharmacological inhibition of {CDK}2 activity. Using trajectory inference approaches, we show how plasticity among cell cycle regulators gives rise to alternate cell cycle “paths” that allow individual tumor cells to escape palbociclib treatment. Understanding drivers of cell cycle plasticity, and how to eliminate resistant cell cycle paths, could lead to improved cancer therapies targeting fractionally resistant cells to improve patient outcomes.},
	pages = {e2309261121},
	number = {7},
	journal = {Proceedings of the National Academy of Sciences},
	shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
	author = {Zikry, Tarek M. and Wolff, Samuel C. and Ranek, Jolene S. and Davis, Harris M. and Naugle, Ander and Luthra, Namit and Whitman, Austin A. and Kedziora, Katarzyna M. and Stallaert, Wayne and Kosorok, Michael R. and Spanheimer, Philip M. and Purvis, Jeremy E.},
	urldate = {2024-08-13},
	date = {2024-02-13},
	langid = {english},
}

@article{bailly_detecting_2024,
	title = {Detecting abnormal cell behaviors from dry mass time series},
	volume = {14},
	issn = {2045-2322},
	abstract = {Abstract
            The prediction of pathological changes on single cell behaviour is a challenging task for deep learning models. Indeed, in self-supervised learning methods, no prior labels are used for the training and all of the information for event predictions are extracted from the data themselves. We present here a novel self-supervised learning model for the detection of anomalies in a given cell population, {StArDusTS}. Cells are monitored over time, and analysed to extract time-series of dry mass values. We assessed its performances on different cell lines, showing a precision of 96\% in the automatic detection of anomalies. Additionally, anomaly detection was also associated with cell measurement errors inherent to the acquisition or analysis pipelines, leading to an improvement of the upstream methods for feature extraction. Our results pave the way to novel architectures for the continuous monitoring of cell cultures in applied research or bioproduction applications, and for the prediction of pathological cellular changes.},
	pages = {7053},
	number = {1},
	journal = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Bailly, Romain and Malfante, Marielle and Allier, Cédric and Paviolo, Chiara and Ghenim, Lamya and Padmanabhan, Kiran and Bardin, Sabine and Mars, Jérôme},
	urldate = {2024-08-12},
	date = {2024-03-25},
	langid = {english},
}

@misc{khatri_deep_2024,
	title = {Deep learning based reconstruction of embryonic cell-division cycle from label-free microscopy time-series of evolutionarily diverse nematodes},
	abstract = {Abstract
          
            Microscopy of cellular dynamics during embryogenesis of non-model organisms can be tech- nically challenging due to limitations of molecular labelling methods. Label-free differential interference contrast ({DIC}) microscopy of the first embryonic cell division of nematodes related to
            Caenorhabditis elegans
            has been successfully employed to examine the constraints and divergence of intra-cellular mechanisms during this asymmetric cell division. However, identifying stages of the cell division cycle were performed interactively, pointing to a need to automate of cell stage identification from {DIC} microscopy. To this end, we have trained deep convolutional neural networks ({CNNs}), both pre-existing such as {ResNet}, {VGGNet} and {EfficientNet}, and a customized shallow network, {EvoCellNet}, to automatically classify first-embryonic division into the stages: (i) pro-nuclear migration and (ii) centration and rotation, (iii) spindle elongation and (iv) cytokinesis, with all networks performing with 91\% or greater accuracy. The activations of the networks superimposed on the images result in segmentation-free detection of intracellular features such as pro-nuclei, spindle and spindle- poles in case of the shallow {EvoCellNet}, while {ResNet}, {VGGNet} and and {EfficientNet} detect large-scale, features that are less biologically meaningful. The {UMAP} space representation combined with support vector machines ({SVM}) allows for stage boundary identification and recovers a cyclical map connecting the states (i) to (iv) of the division. This approach could be used to automate quantification of cell division stages and sub-cellular dynamics without explicit labelling in label-free microscopy.
          
          
            Summary
            
              We have trained multiple convolutional neural networks ({CNNs}) to classify the stages of cell division from the first embryonic division of diverse nematodes, evolutionarily related to
              Caenorhabditis elegans
              . We find two classifiers, {VggNet} and a customized {EvoCellNet}, can detect intracellular features and a {UMAP} representation can reconstruct the cyclical progression of first embryonic division from related species.},
	author = {Khatri, Dhruv and Athale, Chaitanya A.},
	urldate = {2024-08-12},
	date = {2024-05-09},
	langid = {english},
}

@article{edgar_endocycles_2014,
	title = {Endocycles: a recurrent evolutionary innovation for post-mitotic cell growth},
	volume = {15},
	issn = {1471-0072, 1471-0080},
	shorttitle = {Endocycles},
	pages = {197--210},
	number = {3},
	journal = {Nature Reviews Molecular Cell Biology},
	shortjournal = {Nat Rev Mol Cell Biol},
	author = {Edgar, Bruce A. and Zielke, Norman and Gutierrez, Crisanto},
	urldate = {2024-08-12},
	date = {2014-03},
	langid = {english},
}

@article{la_manno_rna_2018,
	title = {{RNA} velocity of single cells},
	volume = {560},
	rights = {2018 Springer Nature Limited},
	issn = {1476-4687},
	abstract = {{RNA} abundance is a powerful indicator of the state of individual cells. Single-cell {RNA} sequencing can reveal {RNA} abundance with high quantitative accuracy, sensitivity and throughput1. However, this approach captures only a static snapshot at a point in time, posing a challenge for the analysis of time-resolved phenomena such as embryogenesis or tissue regeneration. Here we show that {RNA} velocity—the time derivative of the gene expression state—can be directly estimated by distinguishing between unspliced and spliced {mRNAs} in common single-cell {RNA} sequencing protocols. {RNA} velocity is a high-dimensional vector that predicts the future state of individual cells on a timescale of hours. We validate its accuracy in the neural crest lineage, demonstrate its use on multiple published datasets and technical platforms, reveal the branching lineage tree of the developing mouse hippocampus, and examine the kinetics of transcription in human embryonic brain. We expect {RNA} velocity to greatly aid the analysis of developmental lineages and cellular dynamics, particularly in humans.},
	pages = {494--498},
	number = {7719},
	journal = {Nature},
	author = {La Manno, Gioele and Soldatov, Ruslan and Zeisel, Amit and Braun, Emelie and Hochgerner, Hannah and Petukhov, Viktor and Lidschreiber, Katja and Kastriti, Maria E. and Lönnerberg, Peter and Furlan, Alessandro and Fan, Jean and Borm, Lars E. and Liu, Zehua and van Bruggen, David and Guo, Jimin and He, Xiaoling and Barker, Roger and Sundström, Erik and Castelo-Branco, Gonçalo and Cramer, Patrick and Adameyko, Igor and Linnarsson, Sten and Kharchenko, Peter V.},
	urldate = {2024-07-19},
	date = {2018-08},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Differentiation, Genome informatics},
}

@article{riba_cell_2022,
	title = {Cell cycle gene regulation dynamics revealed by {RNA} velocity and deep-learning},
	volume = {13},
	rights = {2022 The Author(s)},
	issn = {2041-1723},
	abstract = {Despite the fact that the cell cycle is a fundamental process of life, a detailed quantitative understanding of gene regulation dynamics throughout the cell cycle is far from complete. Single-cell {RNA}-sequencing ({scRNA}-seq) technology gives access to these dynamics without externally perturbing the cell. Here, by generating {scRNA}-seq libraries in different cell systems, we observe cycling patterns in the unspliced-spliced {RNA} space of cell cycle-related genes. Since existing methods to analyze {scRNA}-seq are not efficient to measure cycling gene dynamics, we propose a deep learning approach ({DeepCycle}) to fit these patterns and build a high-resolution map of the entire cell cycle transcriptome. Characterizing the cell cycle in embryonic and somatic cells, we identify major waves of transcription during the G1 phase and systematically study the stages of the cell cycle. Our work will facilitate the study of the cell cycle in multiple cellular models and different biological contexts.},
	pages = {2865},
	number = {1},
	journal = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Riba, Andrea and Oravecz, Attila and Durik, Matej and Jiménez, Sara and Alunni, Violaine and Cerciat, Marie and Jung, Matthieu and Keime, Céline and Keyes, William M. and Molina, Nacho},
	urldate = {2024-07-19},
	date = {2022-05-23},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cell division, Embryonic stem cells, Machine learning, Transcriptomics},
}

@article{nurse_long_2000,
	title = {A long twentieth century of the cell cycle and beyond},
	volume = {100},
	issn = {0092-8674},
	pages = {71--78},
	number = {1},
	journal = {Cell},
	shortjournal = {Cell},
	author = {Nurse, P.},
	date = {2000-01-07},
	pmid = {10647932},
	keywords = {Animals, Biology, Cell Cycle, History, 20th Century, Humans},
}

@article{sakaue-sawano_genetically_2017-1,
	title = {Genetically Encoded Tools for Optical Dissection of the Mammalian Cell Cycle},
	volume = {68},
	issn = {10972765},
	abstract = {Eukaryotic cells spend most of their life in interphase of the cell cycle. Understanding the rich diversity of metabolic and genomic regulation that occurs in interphase requires the demarcation of precise phase boundaries in situ. Here, we report the properties of two genetically encoded ﬂuorescence sensors, Fucci({CA}) and Fucci({SCA}), which enable realtime monitoring of interphase and cell-cycle biology. We re-engineered the Cdt1-based sensor from the original Fucci system to respond to S phase-speciﬁc {CUL}4Ddb1-mediated ubiquitylation alone or in combination with {SCFSkp}2-mediated ubiquitylation. In cultured cells, Fucci({CA}) produced a sharp triple color-distinct separation of G1, S, and G2, while Fucci({SCA}) permitted a two-color readout of G1 and S/G2. Fucci({CA}) applications included tracking the transient G1 phase of rapidly dividing mouse embryonic stem cells and identifying a window for {UVirradiation} damage in S phase. These results show that Fucci({CA}) is an essential tool for quantitative studies of interphase cell-cycle regulation.},
	pages = {626--640.e5},
	number = {3},
	journal = {Molecular Cell},
	shortjournal = {Molecular Cell},
	author = {Sakaue-Sawano, Asako and Yo, Masahiro and Komatsu, Naoki and Hiratsuka, Toru and Kogure, Takako and Hoshida, Tetsushi and Goshima, Naoki and Matsuda, Michiyuki and Miyoshi, Hiroyuki and Miyawaki, Atsushi},
	urldate = {2024-05-15},
	date = {2017-11},
	langid = {english},
}


@inproceedings{schmidt_cell_2018,
  author    = {Uwe Schmidt and Martin Weigert and Coleman Broaddus and Gene Myers},
  title     = {Cell Detection with Star-Convex Polygons},
  booktitle = {Medical Image Computing and Computer Assisted Intervention - {MICCAI} 
  2018 - 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part {II}},
  pages     = {265--273},
  year      = {2018},
}


@article{stallaert_structure_2022,
	title = {The structure of the human cell cycle},
	volume = {13},
	issn = {2405-4712},
	abstract = {Understanding the organization of the cell cycle has been a longstanding goal in cell biology. We combined time-lapse microscopy, highly multiplexed single-cell imaging of 48 core cell cycle proteins, and manifold learning to render a visualization of the human cell cycle. This data-driven approach revealed the comprehensive “structure” of the cell cycle: a continuum of molecular states that cells occupy as they transition from one cell division to the next, or as they enter or exit cell cycle arrest. Paradoxically, progression deeper into cell cycle arrest was accompanied by increases in proliferative effectors such as {CDKs} and cyclins, which can drive cell cycle re-entry by overcoming p21 induction. The structure also revealed the molecular trajectories into senescence and the unique combination of molecular features that define this irreversibly arrested state. This approach will enable the comparison of alternative cell cycles during development, in response to environmental perturbation and in disease. A record of this paper’s transparent peer review process is included in the supplemental information.},
	pages = {230--240.e3},
	number = {3},
	journal = {Cell Systems},
	shortjournal = {Cell Systems},
	author = {Stallaert, Wayne and Kedziora, Katarzyna M. and Taylor, Colin D. and Zikry, Tarek M. and Ranek, Jolene S. and Sobon, Holly K. and Taylor, Sovanny R. and Young, Catherine L. and Cook, Jeanette G. and Purvis, Jeremy E.},
	urldate = {2023-12-06},
	date = {2022-03-16},
	keywords = {cell cycle, machine learning, manifold learning, quiescence, senescence, single-cell imaging},
}

@misc{lederer_statistical_2024,
	title = {Statistical inference with a manifold-constrained {RNA} velocity model uncovers cell cycle speed modulations},
	rights = {© 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial} 4.0 International), {CC} {BY}-{NC} 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	abstract = {Across a range of biological processes, cells undergo coordinated changes in gene expression, resulting in transcriptome dynamics that unfold within a low-dimensional manifold. Single-cell {RNA}-sequencing ({scRNA}-seq) only measures temporal snapshots of gene expression. However, information on the underlying low-dimensional dynamics can be extracted using {RNA} velocity, which models unspliced and spliced {RNA} abundances to estimate the rate of change of gene expression. Available {RNA} velocity algorithms can be fragile and rely on heuristics that lack statistical control. Moreover, the estimated vector field is not dynamically consistent with the traversed gene expression manifold. Here, we develop a generative model of {RNA} velocity and a Bayesian inference approach that solves these problems. Our model couples velocity field and manifold estimation in a reformulated, unified framework, so as to coherently identify the parameters of an autonomous dynamical system. Focusing on the cell cycle, we implemented {VeloCycle} to study gene regulation dynamics on one-dimensional periodic manifolds and validated using live-imaging its ability to infer actual cell cycle periods. We benchmarked {RNA} velocity inference with sensitivity analyses and demonstrated one- and multiple-sample testing. We also conducted Markov chain Monte Carlo inference on the model, uncovering key relationships between gene-specific kinetics and our gene-independent velocity estimate. Finally, we applied {VeloCycle} to in vivo samples and in vitro genome-wide Perturb-seq, revealing regionally-defined proliferation modes in neural progenitors and the effect of gene knockdowns on cell cycle speed. Ultimately, {VeloCycle} expands the {scRNA}-seq analysis toolkit with a modular and statistically rigorous {RNA} velocity inference framework.},
	publisher = {{bioRxiv}},
	author = {Lederer, Alex R. and Leonardi, Maxine and Talamanca, Lorenzo and Herrera, Antonio and Droin, Colas and Khven, Irina and Carvalho, Hugo {JF} and Valente, Alessandro and Mantes, Albert Dominguez and Arabí, Pau Mulet and Pinello, Luca and Naef, Felix and Manno, Gioele La},
	urldate = {2024-01-23},
	date = {2024-01-22},
	langid = {english},
	note = {Pages: 2024.01.18.576093
Section: New Results},
}

@article{liang_latent_2020,
	title = {Latent periodic process inference from single-cell {RNA}-seq data},
	volume = {11},
	rights = {2020 The Author(s)},
	issn = {2041-1723},
	abstract = {The development of a phenotype in a multicellular organism often involves multiple, simultaneously occurring biological processes. Advances in single-cell {RNA}-sequencing make it possible to infer latent developmental processes from the transcriptomic profiles of cells at various developmental stages. Accurate characterization is challenging however, particularly for periodic processes such as cell cycle. To address this, we develop Cyclum, an autoencoder approach identifying circular trajectories in the gene expression space. Cyclum substantially improves the accuracy and robustness of cell-cycle characterization beyond existing approaches. Applying Cyclum to removing cell-cycle effects substantially improves delineations of cell subpopulations, which is useful for establishing various cell atlases and studying tumor heterogeneity.},
	pages = {1441},
	number = {1},
	journal = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Liang, Shaoheng and Wang, Fang and Han, Jincheng and Chen, Ken},
	urldate = {2023-07-27},
	date = {2020-03-18},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Bioinformatics, Cancer genomics, Cell division},
}

@incollection{ulicna_convolutional_2022,
	location = {New York, {NY}},
	title = {Convolutional Neural Networks for Classifying Chromatin Morphology in Live-Cell Imaging},
	isbn = {978-1-07-162221-6},
	abstract = {Chromatin is highly structured, and changes in its organization are essential in many cellular processes, including cell division. Recently, advances in machine learning have enabled researchers to automatically classify chromatin morphology in fluorescence microscopy images. In this protocol, we develop user-friendly tools to perform this task. We provide an open-source annotation tool, and a cloud-based computational framework to train and utilize a convolutional neural network to automatically classify chromatin morphology. Using cloud compute enables users without significant resources or computational experience to use a machine learning approach to analyze their own microscopy data.},
	pages = {17--30},
	booktitle = {Chromosome Architecture: Methods and Protocols},
	publisher = {Springer {US}},
	author = {Ulicna, Kristina and Ho, Laure T. L. and Soelistyo, Christopher J. and Day, Nathan J. and Lowe, Alan R.},
	editor = {Leake, Mark C.},
	urldate = {2024-07-18},
	date = {2022},
	langid = {english},
	keywords = {Cell cycle, Computational biology, Image analysis, Live-cell imaging, Machine learning},
}

@article{rivoir_pitfalls_2024,
	title = {On the pitfalls of Batch Normalization for end-to-end video learning: A study on surgical workflow analysis},
	volume = {94},
	issn = {1361-8415},
	shorttitle = {On the pitfalls of Batch Normalization for end-to-end video learning},
	abstract = {Batch Normalization’s ({BN}) unique property of depending on other samples in a batch is known to cause problems in several tasks, including sequence modeling. Yet, {BN}-related issues are hardly studied for long video understanding, despite the ubiquitous use of {BN} in {CNNs} (Convolutional Neural Networks) for feature extraction. Especially in surgical workflow analysis, where the lack of pretrained feature extractors has led to complex, multi-stage training pipelines, limited awareness of {BN} issues may have hidden the benefits of training {CNNs} and temporal models end to end. In this paper, we analyze pitfalls of {BN} in video learning, including issues specific to online tasks such as a ’cheating’ effect in anticipation. We observe that {BN}’s properties create major obstacles for end-to-end learning. However, using {BN}-free backbones, even simple {CNN}–{LSTMs} beat the state of the art on three surgical workflow benchmarks by utilizing adequate end-to-end training strategies which maximize temporal context. We conclude that awareness of {BN}’s pitfalls is crucial for effective end-to-end learning in surgical tasks. By reproducing results on natural-video datasets, we hope our insights will benefit other areas of video learning as well. Code is available at: https://gitlab.com/nct\_tso\_public/pitfalls\_bn.},
	pages = {103126},
	journal = {Medical Image Analysis},
	shortjournal = {Medical Image Analysis},
	author = {Rivoir, Dominik and Funke, Isabel and Speidel, Stefanie},
	urldate = {2024-07-18},
	date = {2024-05-01},
	keywords = {Anticipation, Batch normalization, {BatchNorm}, End-to-end, Surgical phase, Surgical workflow, Video learning},
}

@article{rivoir_pitfalls_2024-1,
	title = {On the pitfalls of Batch Normalization for end-to-end video learning: A study on surgical workflow analysis},
	volume = {94},
	issn = {1361-8415},
	shorttitle = {On the pitfalls of Batch Normalization for end-to-end video learning},
	abstract = {Batch Normalization’s ({BN}) unique property of depending on other samples in a batch is known to cause problems in several tasks, including sequence modeling. Yet, {BN}-related issues are hardly studied for long video understanding, despite the ubiquitous use of {BN} in {CNNs} (Convolutional Neural Networks) for feature extraction. Especially in surgical workflow analysis, where the lack of pretrained feature extractors has led to complex, multi-stage training pipelines, limited awareness of {BN} issues may have hidden the benefits of training {CNNs} and temporal models end to end. In this paper, we analyze pitfalls of {BN} in video learning, including issues specific to online tasks such as a ’cheating’ effect in anticipation. We observe that {BN}’s properties create major obstacles for end-to-end learning. However, using {BN}-free backbones, even simple {CNN}–{LSTMs} beat the state of the art on three surgical workflow benchmarks by utilizing adequate end-to-end training strategies which maximize temporal context. We conclude that awareness of {BN}’s pitfalls is crucial for effective end-to-end learning in surgical tasks. By reproducing results on natural-video datasets, we hope our insights will benefit other areas of video learning as well. Code is available at: https://gitlab.com/nct\_tso\_public/pitfalls\_bn.},
	pages = {103126},
	journal = {Medical Image Analysis},
	shortjournal = {Medical Image Analysis},
	author = {Rivoir, Dominik and Funke, Isabel and Speidel, Stefanie},
	urldate = {2024-07-18},
	date = {2024-05-01},
	keywords = {Anticipation, Batch normalization, {BatchNorm}, End-to-end, Surgical phase, Surgical workflow, Video learning},
}

@article{lawrence_gaussian_nodate,
	title = {Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data},
	abstract = {In this paper we introduce a new underlying probabilistic model for principal component analysis ({PCA}). Our formulation interprets {PCA} as a particular Gaussian process prior on a mapping from a latent space to the observed data-space. We show that if the prior’s covariance function constrains the mappings to be linear the model is equivalent to {PCA}, we then extend the model by considering less restrictive covariance functions which allow non-linear mappings. This more general Gaussian process latent variable model ({GPLVM}) is then evaluated as an approach to the visualisation of high dimensional data for three different data-sets. Additionally our non-linear algorithm can be further kernelised leading to ‘twin kernel {PCA}’ in which a mapping between feature spaces occurs.},
	author = {Lawrence, Neil D},
	langid = {english},
}

@article{li_review_2016,
	title = {A review on Gaussian Process Latent Variable Models},
	volume = {1},
	rights = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {24682322},
	abstract = {Gaussian Process Latent Variable Model ({GPLVM}), as a ﬂexible bayesian non-parametric modeling method, has been extensively studied and applied in many learning tasks such as Intrusion Detection, Image Reconstruction, Facial Expression Recognition, Human pose estimation and so on. In this paper, we give a review and analysis for {GPLVM} and its extensions. Firstly, we formulate basic {GPLVM} and discuss its relation to Kernel Principal Components Analysis. Secondly, we summarize its improvements or variants and propose a taxonomy of {GPLVM} related models in terms of the various strategies that be used. Thirdly, we provide the detailed formulations of the main {GPLVMs} that extensively developed based on the strategies described in the paper. Finally, we further give some challenges in next researches of {GPLVM}.},
	pages = {366--376},
	number = {4},
	journal = {{CAAI} Transactions on Intelligence Technology},
	shortjournal = {{CAAI} Transactions on Intelligence Technology},
	author = {Li, Ping and Chen, Songcan},
	urldate = {2024-05-27},
	date = {2016-10},
	langid = {english},
}

@article{ahmed_grandprix_2019,
	title = {{GrandPrix}: scaling up the Bayesian {GPLVM} for single-cell data},
	volume = {35},
	issn = {1367-4803, 1460-2059},
	shorttitle = {{GrandPrix}},
	pages = {47--54},
	number = {1},
	journal = {Bioinformatics},
	author = {Ahmed, Sumon and Rattray, Magnus and Boukouvalas, Alexis},
	editor = {Stegle, Oliver},
	urldate = {2019-12-18},
	date = {2019-01-01},
	langid = {english},
	note = {{ZSCC}: {NoCitationData}[s1]},
}

@misc{lalchand_generalised_2022,
	title = {Generalised Gaussian Process Latent Variable Models ({GPLVM}) with Stochastic Variational Inference},
	abstract = {Gaussian process latent variable models ({GPLVM}) are a flexible and non-linear approach to dimensionality reduction, extending classical Gaussian processes to an unsupervised learning context. The Bayesian incarnation of the {GPLVM} Titsias and Lawrence, 2010] uses a variational framework, where the posterior over latent variables is approximated by a well-behaved variational family, a factorized Gaussian yielding a tractable lower bound. However, the non-factories ability of the lower bound prevents truly scalable inference. In this work, we study the doubly stochastic formulation of the Bayesian {GPLVM} model amenable with minibatch training. We show how this framework is compatible with different latent variable formulations and perform experiments to compare a suite of models. Further, we demonstrate how we can train in the presence of massively missing data and obtain high-fidelity reconstructions. We demonstrate the model's performance by benchmarking against the canonical sparse {GPLVM} for high-dimensional data examples.},
	number = {{arXiv}:2202.12979},
	publisher = {{arXiv}},
	author = {Lalchand, Vidhi and Ravuri, Aditya and Lawrence, Neil D.},
	urldate = {2024-05-27},
	date = {2022-04-09},
	eprinttype = {arxiv},
	eprint = {2202.12979 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
}

@misc{zhang_bayesian_2023,
	title = {Bayesian Non-linear Latent Variable Modeling via Random Fourier Features},
	abstract = {The Gaussian process latent variable model ({GPLVM}) is a popular probabilistic method used for nonlinear dimension reduction, matrix factorization, and state-space modeling. Inference for {GPLVMs} is computationally tractable only when the data likelihood is Gaussian. Moreover, inference for {GPLVMs} has typically been restricted to obtaining maximum a posteriori point estimates, which can lead to overfitting, or variational approximations, which mischaracterize the posterior uncertainty. Here, we present a method to perform Markov chain Monte Carlo ({MCMC}) inference for generalized Bayesian nonlinear latent variable modeling. The crucial insight necessary to generalize {GPLVMs} to arbitrary observation models is that we approximate the kernel function in the Gaussian process mappings with random Fourier features; this allows us to compute the gradient of the posterior in closed form with respect to the latent variables. We show that we can generalize {GPLVMs} to non-Gaussian observations, such as Poisson, negative binomial, and multinomial distributions, using our random feature latent variable model ({RFLVM}). Our generalized {RFLVMs} perform on par with state-of-the-art latent variable models on a wide range of applications, including motion capture, images, and text data for the purpose of estimating the latent structure and imputing the missing data of these complex data sets.},
	number = {{arXiv}:2306.08352},
	publisher = {{arXiv}},
	author = {Zhang, Michael Minyi and Gundersen, Gregory W. and Engelhardt, Barbara E.},
	urldate = {2024-05-27},
	date = {2023-06-14},
	eprinttype = {arxiv},
	eprint = {2306.08352 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{qiu_interpretable_2022,
	title = {Interpretable Deep Representation Learning from Temporal Multi-view Data},
	abstract = {In many scientiﬁc problems such as video surveillance, modern genomic analysis, and clinical studies, data are often collected from diverse domains across time that exhibit time-dependent heterogeneous properties. It is important to not only integrate data from multiple sources (called multi-view data), but also to incorporate time dependency for deep understanding of the underlying system. Latent factor models are popular tools for exploring multi-view data. However, it is frequently observed that these models do not perform well for complex systems and they are not applicable to time-series data. Therefore, we propose a generative model based on variational autoencoder and recurrent neural network to infer the latent dynamic factors for multivariate time-series data. This approach allows us to identify the disentangled latent embeddings across multiple modalities while accounting for the time factor. We invoke our proposed model for analyzing three datasets on which we demonstrate the effectiveness and the interpretability of the model.},
	number = {{arXiv}:2005.05210},
	publisher = {{arXiv}},
	author = {Qiu, Lin and Chinchilli, Vernon M. and Lin, Lin},
	urldate = {2024-04-15},
	date = {2022-10-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2005.05210 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{ainsworth_oi-vae_nodate,
	title = {oi-{VAE}: Output Interpretable {VAEs} for Nonlinear Group Factor Analysis},
	abstract = {Deep generative models have recently yielded encouraging results in producing subjectively realistic samples of complex data. Far less attention has been paid to making these generative models interpretable. In many scenarios, ranging from scientiﬁc applications to ﬁnance, the observed variables have a natural grouping. It is often of interest to understand systems of interaction amongst these groups, and latent factor models ({LFMs}) are an attractive approach. However, traditional {LFMs} are limited by assuming a linear correlation structure. We present an output interpretable {VAE} (oi-{VAE}) for grouped data that models complex, nonlinear latent-to-observed relationships. We combine a structured {VAE} comprised of group-speciﬁc generators with a sparsity-inducing prior. We demonstrate that oi-{VAE} yields meaningful notions of interpretability in the analysis of motion capture and {MEG} data. We further show that in these situations, the regularization inherent to oi-{VAE} can actually lead to improved generalization and learned generative processes.},
	author = {Ainsworth, Samuel K and Foti, Nicholas J and Lee, Adrian K C and Fox, Emily B},
	langid = {english},
}

@article{ngiam_multimodal_nodate,
	title = {Multimodal Deep Learning},
	abstract = {Deep networks have been successfully applied to unsupervised feature learning for single modalities (e.g., text, images or audio). In this work, we propose a novel application of deep networks to learn features over multiple modalities. We present a series of tasks for multimodal learning and show how to train deep networks that learn features to address these tasks. In particular, we demonstrate cross modality feature learning, where better features for one modality (e.g., video) can be learned if multiple modalities (e.g., audio and video) are present at feature learning time. Furthermore, we show how to learn a shared representation between modalities and evaluate it on a unique task, where the classiﬁer is trained with audio-only data but tested with video-only data and vice-versa. Our models are validated on the {CUAVE} and {AVLetters} datasets on audio-visual speech classiﬁcation, demonstrating best published visual speech classiﬁcation on {AVLetters} and eﬀective shared representation learning.},
	author = {Ngiam, Jiquan and Khosla, Aditya and Kim, Mingyu and Nam, Juhan and Lee, Honglak and Ng, Andrew Y},
	langid = {english},
}

@article{akkus_multimodal_2023,
	title = {Multimodal Deep Learning},
	rights = {Creative Commons Attribution 4.0 International},
	abstract = {This book is the result of a seminar in which we reviewed multimodal approaches and attempted to create a solid overview of the field, starting with the current state-of-the-art approaches in the two subfields of Deep Learning individually. Further, modeling frameworks are discussed where one modality is transformed into the other, as well as models in which one modality is utilized to enhance representation learning for the other. To conclude the second part, architectures with a focus on handling both modalities simultaneously are introduced. Finally, we also cover other modalities as well as general-purpose multi-modal models, which are able to handle different tasks on different modalities within one unified architecture. One interesting application (Generative Art) eventually caps off this booklet.},
	author = {Akkus, Cem and Chu, Luyang and Djakovic, Vladana and Jauch-Walser, Steffen and Koch, Philipp and Loss, Giacomo and Marquardt, Christopher and Moldovan, Marco and Sauter, Nadja and Schneider, Maximilian and Schulte, Rickmer and Urbanczyk, Karol and Goschenhofer, Jann and Heumann, Christian and Hvingelby, Rasmus and Schalk, Daniel and Aßenmacher, Matthias},
	urldate = {2024-04-15},
	date = {2023},
	note = {Publisher: [object Object]
Version Number: 1},
	keywords = {Computation and Language (cs.{CL}), {FOS}: Computer and information sciences, Machine Learning (cs.{LG}), Machine Learning (stat.{ML})},
}

@article{qiu_variational_2022,
	title = {{VARIATIONAL} {INTERPRETABLE} {DEEP} {CANONICAL} {CORRELATION} {ANALYSIS}},
	abstract = {The main idea of canonical correlation analysis ({CCA}) is to map different views onto a common latent space with maximum correlation. We propose a deep interpretable variational canonical correlation analysis ({DICCA}) for multi-view learning. The developed model extends the existing latent variable model for linear {CCA} to nonlinear models through the use of deep generative networks. {DICCA} is designed to disentangle both the shared and view-specific variations for multi-view data. To further make the model more interpretable, we place a sparsity-inducing prior on the latent weight with a structured variational autoencoder that is comprised of view-specific generators. Empirical results on real-world datasets show that our method is competitive across domains.},
	author = {Qiu, Lin and Chinchilli, Vernon M and Lin, Lin},
	date = {2022},
	langid = {english},
}

@article{uhler_machine_2022,
	title = {Machine Learning Approaches to Single-Cell Data Integration and Translation},
	volume = {110},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {0018-9219, 1558-2256},
	pages = {557--576},
	number = {5},
	journal = {Proceedings of the {IEEE}},
	shortjournal = {Proc. {IEEE}},
	author = {Uhler, Caroline and Shivashankar, G. V.},
	urldate = {2024-04-12},
	date = {2022-05},
}

@article{bach_probabilistic_nodate,
	title = {A Probabilistic Interpretation of Canonical Correlation Analysis},
	abstract = {We give a probabilistic interpretation of canonical correlation ({CCA}) analysis as a latent variable model for two Gaussian random vectors. Our interpretation is similar to the probabilistic interpretation of principal component analysis (Tipping and Bishop, 1999, Roweis, 1998). In addition, we cast Fisher linear discriminant analysis ({LDA}) within the {CCA} framework.},
	author = {Bach, Francis R and Jordan, Michael I},
	langid = {english},
}

@article{ghahramani_probabilistic_2015,
	title = {Probabilistic machine learning and artificial intelligence},
	volume = {521},
	issn = {0028-0836, 1476-4687},
	pages = {452--459},
	number = {7553},
	journal = {Nature},
	shortjournal = {Nature},
	author = {Ghahramani, Zoubin},
	urldate = {2024-04-12},
	date = {2015-05-28},
	langid = {english},
}

@article{gundersen_end--end_nodate,
	title = {End-to-end training of deep probabilistic {CCA} on paired biomedical observations},
	abstract = {Medical pathology images are visually evaluated by experts for disease diagnosis, but the connection between image features and the state of the cells in an image is typically unknown. To understand this relationship, we develop a multimodal modeling and inference framework that estimates shared latent structure of joint gene expression levels and medical image features. Our method is built around probabilistic canonical correlation analysis ({PCCA}), which is ﬁt to image embeddings that are learned using convolutional neural networks and linear embeddings of paired gene expression data. We train the model end-to-end so that the {PCCA} and neural network parameters are estimated simultaneously. We demonstrate the utility of this method in constructing image features that are predictive of gene expression levels on simulated data and the Genotype-Tissue Expression data. We demonstrate that the latent variables are interpretable by disentangling the latent subspace through shared and modality-speciﬁc views.},
	author = {Gundersen, Gregory and Dumitrascu, Bianca and Ash, Jordan T and Engelhardt, Barbara E},
	langid = {english},
}

@misc{garnelo_conditional_2018,
	title = {Conditional Neural Processes},
	abstract = {Deep neural networks excel at function approximation, yet they are typically trained from scratch for each new function. On the other hand, Bayesian methods, such as Gaussian Processes ({GPs}), exploit prior knowledge to quickly infer the shape of a new function at test time. Yet {GPs} are computationally expensive, and it can be hard to design appropriate priors. In this paper we propose a family of neural models, Conditional Neural Processes ({CNPs}), that combine the beneﬁts of both. {CNPs} are inspired by the ﬂexibility of stochastic processes such as {GPs}, but are structured as neural networks and trained via gradient descent. {CNPs} make accurate predictions after observing only a handful of training data points, yet scale to complex functions and large datasets. We demonstrate the performance and versatility of the approach on a range of canonical machine learning tasks, including regression, classiﬁcation and image completion.},
	number = {{arXiv}:1807.01613},
	publisher = {{arXiv}},
	author = {Garnelo, Marta and Rosenbaum, Dan and Maddison, Chris J. and Ramalho, Tiago and Saxton, David and Shanahan, Murray and Teh, Yee Whye and Rezende, Danilo J. and Eslami, S. M. Ali},
	urldate = {2024-04-12},
	date = {2018-07-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1807.01613 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{adzemovic_beyond_2024,
	title = {Beyond Kalman Filters: Deep Learning-Based Filters for Improved Object Tracking},
	rights = {Creative Commons Attribution 4.0 International},
	shorttitle = {Beyond Kalman Filters},
	abstract = {Traditional tracking-by-detection systems typically employ Kalman filters ({KF}) for state estimation. However, the {KF} requires domain-specific design choices and it is ill-suited to handling non-linear motion patterns. To address these limitations, we propose two innovative data-driven filtering methods. Our first method employs a Bayesian filter with a trainable motion model to predict an object's future location and combines its predictions with observations gained from an object detector to enhance bounding box prediction accuracy. Moreover, it dispenses with most domain-specific design choices characteristic of the {KF}. The second method, an end-to-end trainable filter, goes a step further by learning to correct detector errors, further minimizing the need for domain expertise. Additionally, we introduce a range of motion model architectures based on Recurrent Neural Networks, Neural Ordinary Differential Equations, and Conditional Neural Processes, that are combined with the proposed filtering methods. Our extensive evaluation across multiple datasets demonstrates that our proposed filters outperform the traditional {KF} in object tracking, especially in the case of non-linear motion patterns -- the use case our filters are best suited to. We also conduct noise robustness analysis of our filters with convincing positive results. We further propose a new cost function for associating observations with tracks. Our tracker, which incorporates this new association cost with our proposed filters, outperforms the conventional {SORT} method and other motion-based trackers in multi-object tracking according to multiple metrics on motion-rich {DanceTrack} and {SportsMOT} datasets.},
	author = {Adžemović, Momir and Tadić, Predrag and Petrović, Andrija and Nikolić, Mladen},
	urldate = {2024-04-12},
	date = {2024},
	note = {Publisher: [object Object]
Version Number: 1},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences},
}

@article{bao_deep_2021,
	title = {A deep Kalman filter network for hand kinematics estimation using {sEMG}},
	volume = {143},
	issn = {01678655},
	pages = {88--94},
	journal = {Pattern Recognition Letters},
	shortjournal = {Pattern Recognition Letters},
	author = {Bao, Tianzhe and Zhao, Yihui and Zaidi, Syed Ali Raza and Xie, Shengquan and Yang, Pengfei and Zhang, Zhiqiang},
	urldate = {2024-04-12},
	date = {2021-03},
	langid = {english},
}

@article{hovart_deep_2023,
	title = {Deep Kalman Filters Can Filter},
	rights = {Creative Commons Attribution 4.0 International},
	abstract = {Deep Kalman filters ({DKFs}) are a class of neural network models that generate Gaussian probability measures from sequential data. Though {DKFs} are inspired by the Kalman filter, they lack concrete theoretical ties to the stochastic filtering problem, thus limiting their applicability to areas where traditional model-based filters have been used, e.g.{\textbackslash} model calibration for bond and option prices in mathematical finance. We address this issue in the mathematical foundations of deep learning by exhibiting a class of continuous-time {DKFs} which can approximately implement the conditional law of a broad class of non-Markovian and conditionally Gaussian signal processes given noisy continuous-times measurements. Our approximation results hold uniformly over sufficiently regular compact subsets of paths, where the approximation error is quantified by the worst-case 2-Wasserstein distance computed uniformly over the given compact set of paths.},
	author = {Hovart, Blanka and Kratsios, Anastasis and Limmer, Yannick and Yang, Xuwei},
	urldate = {2024-04-12},
	date = {2023},
	note = {Publisher: [object Object]
Version Number: 1},
	keywords = {60G35, 62M20, 68T07, 41A65, {FOS}: Computer and information sciences, {FOS}: Mathematics, Machine Learning (cs.{LG}), Machine Learning (stat.{ML}), Neural and Evolutionary Computing (cs.{NE}), Numerical Analysis (math.{NA}), Probability (math.{PR})},
}

@article{krishnan_deep_2015,
	title = {Deep Kalman Filters},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	abstract = {Kalman Filters are one of the most influential models of time-varying phenomena. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption in a variety of disciplines. Motivated by recent variational methods for learning deep generative models, we introduce a unified algorithm to efficiently learn a broad spectrum of Kalman filters. Of particular interest is the use of temporal generative models for counterfactual inference. We investigate the efficacy of such models for counterfactual inference, and to that end we introduce the "Healing {MNIST}" dataset where long-term structure, noise and actions are applied to sequences of digits. We show the efficacy of our method for modeling this dataset. We further show how our model can be used for counterfactual inference for patients, based on electronic health record data of 8,000 patients over 4.5 years.},
	author = {Krishnan, Rahul G. and Shalit, Uri and Sontag, David},
	urldate = {2024-04-12},
	date = {2015},
	note = {Publisher: [object Object]
Version Number: 2},
	keywords = {{FOS}: Computer and information sciences, Machine Learning (cs.{LG}), Machine Learning (stat.{ML})},
}

@article{karl_deep_2016,
	title = {Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	shorttitle = {Deep Variational Bayes Filters},
	abstract = {We introduce Deep Variational Bayes Filters ({DVBF}), a new method for unsupervised learning and identification of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, {DVBF} can overcome intractable inference distributions via variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction.},
	author = {Karl, Maximilian and Soelch, Maximilian and Bayer, Justin and van der Smagt, Patrick},
	urldate = {2024-04-12},
	date = {2016},
	note = {Publisher: [object Object]
Version Number: 3},
	keywords = {{FOS}: Computer and information sciences, {FOS}: Electrical engineering, electronic engineering, information engineering, Machine Learning (cs.{LG}), Machine Learning (stat.{ML}), Systems and Control (eess.{SY})},
}

@inproceedings{salaun_comparing_2019,
	location = {Boca Raton, {FL}, {USA}},
	title = {Comparing the Modeling Powers of {RNN} and {HMM}},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	isbn = {978-1-72814-550-1},
	eventtitle = {2019 18th {IEEE} International Conference On Machine Learning And Applications ({ICMLA})},
	pages = {1496--1499},
	booktitle = {2019 18th {IEEE} International Conference On Machine Learning And Applications ({ICMLA})},
	publisher = {{IEEE}},
	author = {Salaun, Achille and Petetin, Yohan and Desbouvries, Francois},
	urldate = {2024-04-11},
	date = {2019-12},
}

@incollection{friedman_canonical_1995,
	location = {New York, {NY}},
	title = {The Canonical Correlations of Matrix Pairs and their Numerical Computation},
	volume = {69},
	isbn = {978-1-4612-8703-2 978-1-4612-4228-4},
	pages = {27--49},
	booktitle = {Linear Algebra for Signal Processing},
	publisher = {Springer New York},
	author = {Golub, Gene H. and Zha, Hongyuan},
	editor = {Bojanczyk, Adam and Cybenko, George},
	editorb = {Friedman, Avner and Miller, Willard},
	editorbtype = {redactor},
	urldate = {2024-04-11},
	date = {1995},
	langid = {english},
	note = {Series Title: The {IMA} Volumes in Mathematics and its Applications},
}

@article{li_survey_2019,
	title = {A Survey of Multi-View Representation Learning},
	volume = {31},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {1041-4347, 1558-2191, 2326-3865},
	pages = {1863--1883},
	number = {10},
	journal = {{IEEE} Transactions on Knowledge and Data Engineering},
	shortjournal = {{IEEE} Trans. Knowl. Data Eng.},
	author = {Li, Yingming and Yang, Ming and Zhang, Zhongfei},
	urldate = {2024-04-11},
	date = {2019-10-01},
}

@article{radhakrishnan_cross-modal_2023,
	title = {Cross-modal autoencoder framework learns holistic representations of cardiovascular state},
	volume = {14},
	issn = {2041-1723},
	abstract = {Abstract
            A fundamental challenge in diagnostics is integrating multiple modalities to develop a joint characterization of physiological state. Using the heart as a model system, we develop a cross-modal autoencoder framework for integrating distinct data modalities and constructing a holistic representation of cardiovascular state. In particular, we use our framework to construct such cross-modal representations from cardiac magnetic resonance images ({MRIs}), containing structural information, and electrocardiograms ({ECGs}), containing myoelectric information. We leverage the learned cross-modal representation to (1) improve phenotype prediction from a single, accessible phenotype such as {ECGs}; (2) enable imputation of hard-to-acquire cardiac {MRIs} from easy-to-acquire {ECGs}; and (3) develop a framework for performing genome-wide association studies in an unsupervised manner. Our results systematically integrate distinct diagnostic modalities into a common representation that better characterizes physiologic state.},
	pages = {2436},
	number = {1},
	journal = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Radhakrishnan, Adityanarayanan and Friedman, Sam F. and Khurshid, Shaan and Ng, Kenney and Batra, Puneet and Lubitz, Steven A. and Philippakis, Anthony A. and Uhler, Caroline},
	urldate = {2024-04-11},
	date = {2023-04-28},
	langid = {english},
}

@article{yang_multi-domain_2021,
	title = {Multi-domain translation between single-cell imaging and sequencing data using autoencoders},
	volume = {12},
	issn = {2041-1723},
	abstract = {Abstract
            The development of single-cell methods for capturing different data modalities including imaging and sequencing has revolutionized our ability to identify heterogeneous cell states. Different data modalities provide different perspectives on a population of cells, and their integration is critical for studying cellular heterogeneity and its function. While various methods have been proposed to integrate different sequencing data modalities, coupling imaging and sequencing has been an open challenge. We here present an approach for integrating vastly different modalities by learning a probabilistic coupling between the different data modalities using autoencoders to map to a shared latent space. We validate this approach by integrating single-cell {RNA}-seq and chromatin images to identify distinct subpopulations of human naive {CD}4+ T-cells that are poised for activation. Collectively, our approach provides a framework to integrate and translate between data modalities that cannot yet be measured within the same cell for diverse applications in biomedical discovery.},
	pages = {31},
	number = {1},
	journal = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Yang, Karren Dai and Belyaeva, Anastasiya and Venkatachalapathy, Saradha and Damodaran, Karthik and Katcoff, Abigail and Radhakrishnan, Adityanarayanan and Shivashankar, G. V. and Uhler, Caroline},
	urldate = {2024-04-11},
	date = {2021-01-04},
	langid = {english},
}

@article{alert_physical_2020,
	title = {Physical Models of Collective Cell Migration},
	volume = {11},
	issn = {1947-5454, 1947-5462},
	abstract = {Collective cell migration is a key driver of embryonic development, wound healing, and some types of cancer invasion. Here we provide a physical perspective of the mechanisms underlying collective cell migration. We begin with a catalogue of the cell-cell and cell-substrate interactions that govern cell migration, which we classify into positional and orientational interactions. We then review the physical models that have been developed to explain how these interactions give rise to collective cellular movement. These models span the sub-cellular to the supracellular scales, and they include lattice models, phase fields models, active network models, particle models, and continuum models. For each type of model, we discuss its formulation, its limitations, and the main emergent phenomena that it has successfully explained. These phenomena include flocking and fluid-solid transitions, as well as wetting, fingering, and mechanical waves in spreading epithelial monolayers. We close by outlining remaining challenges and future directions in the physics of collective cell migration.},
	pages = {77--101},
	number = {1},
	journal = {Annual Review of Condensed Matter Physics},
	shortjournal = {Annu. Rev. Condens. Matter Phys.},
	author = {Alert, Ricard and Trepat, Xavier},
	urldate = {2024-03-01},
	date = {2020-03-10},
	eprinttype = {arxiv},
	eprint = {1905.07675 [physics, q-bio]},
	keywords = {Physics - Biological Physics, Quantitative Biology - Tissues and Organs},
}

@report{vercurysse_geometry-driven_2022,
	title = {Geometry-driven migration efficiency of autonomous epithelial cell clusters},
	abstract = {The directed migration of epithelial cell collectives through coordinated movements plays a crucial role in various physiological and pathological processes and is increasingly understood at the level of large confluent monolayers. However, numerous processes rely on the migration of small groups of polarized epithelial clusters in complex environments, and their responses to external geometries remain poorly understood. To address this, we cultivated primary epithelial keratocyte tissues on adhesive microstripes, creating autonomous epithelial clusters with well-defined geometries. We showed that their migration efficiency is strongly influenced by the contact geometry, and the orientation of cell-cell contacts with respect to the direction of migration. To elucidate the underlying mechanisms, we systematically explored possible cell-cell interactions using a minimal active matter model. Our investigations revealed that a combination of velocity and polarity alignment with contact regulation of locomotion captures the experimental data, which we then validated via force and intracellular stress measurements. Furthermore, we predict that this combination of rules enables efficient navigation in complex geometries, which we confirm experimentally. Altogether, our findings provide a conceptual framework for extracting interaction rules governing the behavior of active systems interacting with physical boundaries, as well as designing principles for collective navigation in complex microenvironments.},
	institution = {Biophysics},
	type = {preprint},
	author = {Vercurysse, Eléonore and Brückner, David B. and Gómez-González, Manuel and Remson, Alexandre and Luciano, Marine and Kalukula, Yohalie and Rossetti, Leone and Trepat, Xavier and Hannezo, Edouard and Gabriele, Sylvain},
	urldate = {2024-03-01},
	date = {2022-07-18},
	langid = {english},
}

@article{cadart_size_2018,
	title = {Size control in mammalian cells involves modulation of both growth rate and cell cycle duration},
	volume = {9},
	issn = {2041-1723},
	abstract = {Abstract
            Despite decades of research, how mammalian cell size is controlled remains unclear because of the difficulty of directly measuring growth at the single-cell level. Here we report direct measurements of single-cell volumes over entire cell cycles on various mammalian cell lines and primary human cells. We find that, in a majority of cell types, the volume added across the cell cycle shows little or no correlation to cell birth size, a homeostatic behavior called “adder”. This behavior involves modulation of G1 or S-G2 duration and modulation of growth rate. The precise combination of these mechanisms depends on the cell type and the growth condition. We have developed a mathematical framework to compare size homeostasis in datasets ranging from bacteria to mammalian cells. This reveals that a near-adder behavior is the most common type of size control and highlights the importance of growth rate modulation to size control in mammalian cells.},
	pages = {3275},
	number = {1},
	journal = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Cadart, Clotilde and Monnier, Sylvain and Grilli, Jacopo and Sáez, Pablo J. and Srivastava, Nishit and Attia, Rafaele and Terriac, Emmanuel and Baum, Buzz and Cosentino-Lagomarsino, Marco and Piel, Matthieu},
	urldate = {2024-02-25},
	date = {2018-08-16},
	langid = {english},
}

@article{penas_annotation-free_2023,
	title = {Annotation-free learning of a spatio-temporal manifold of the cell life cycle},
	volume = {3},
	issn = {2633-903X},
	abstract = {The cell cycle is a complex biological phenomenon, which plays an important role in many cell biological processes and disease states. Machine learning is emerging to be a pivotal technique for the study of the cell cycle, resulting in a number of available tools and models for the analysis of the cell cycle. Most, however, heavily rely on expert annotations, prior knowledge of mechanisms, and imaging with several fluorescent markers to train their models. Many are also limited to processing only the spatial information in the cell images. In this work, we describe a different approach based on representation learning to construct a manifold of the cell life cycle. We trained our model such that the representations are learned without exhaustive annotations nor assumptions. Moreover, our model uses microscopy images derived from a single fluorescence channel and utilizes both the spatial and temporal information in these images. We show that even with fewer channels and self-supervision, information relevant to cell cycle analysis such as staging and estimation of cycle duration can still be extracted, which demonstrates the potential of our approach to aid future cell cycle studies and in discovery cell biology to probe and understand novel dynamic systems.},
	pages = {e19},
	journal = {Biological Imaging},
	author = {Peñas, Kristofer delas and Dmitrieva, Mariia and Waithe, Dominic and Rittscher, Jens},
	urldate = {2024-01-16},
	date = {2023-01},
	langid = {english},
	note = {Publisher: Cambridge University Press},
	keywords = {Cell life cycle, machine learning, self-supervision},
}

@article{grant_accurate_2018,
	title = {Accurate delineation of cell cycle phase transitions in living cells with {PIP}-{FUCCI}},
	volume = {17},
	issn = {1538-4101, 1551-4005},
	pages = {2496--2516},
	number = {21},
	journal = {Cell Cycle},
	shortjournal = {Cell Cycle},
	author = {Grant, Gavin D. and Kedziora, Katarzyna M. and Limas, Juanita C. and Cook, Jeanette Gowen and Purvis, Jeremy E.},
	urldate = {2024-01-10},
	date = {2018-11-17},
	langid = {english},
}

@article{chao_evidence_2019,
	title = {Evidence that the human cell cycle is a series of uncoupled, memoryless phases},
	volume = {15},
	issn = {1744-4292, 1744-4292},
	number = {3},
	journal = {Molecular Systems Biology},
	shortjournal = {Mol Syst Biol},
	author = {Chao, Hui Xiao and Fakhreddin, Randy I and Shimerov, Hristo K and Kedziora, Katarzyna M and Kumar, Rashmi J and Perez, Joanna and Limas, Juanita C and Grant, Gavin D and Cook, Jeanette Gowen and Gupta, Gaorav P and Purvis, Jeremy E},
	urldate = {2021-11-23},
	date = {2019-03},
	langid = {english},
	note = {{ZSCC}: 0000047},
}

@article{stallaert_molecular_2022,
	title = {The molecular architecture of cell cycle arrest},
	volume = {18},
	issn = {1744-4292},
	abstract = {Abstract The cellular decision governing the transition between proliferative and arrested states is crucial to the development and function of every tissue. While the molecular mechanisms that regulate the proliferative cell cycle are well established, we know comparatively little about what happens to cells as they diverge into cell cycle arrest. We performed hyperplexed imaging of 47 cell cycle effectors to obtain a map of the molecular architecture that governs cell cycle exit and progression into reversible (?quiescent?) and irreversible (?senescent?) arrest states. Using this map, we found multiple points of divergence from the proliferative cell cycle; identified stress-specific states of arrest; and resolved the molecular mechanisms governing these fate decisions, which we validated by single-cell, time-lapse imaging. Notably, we found that cells can exit into senescence from either G1 or G2; however, both subpopulations converge onto a single senescent state with a G1-like molecular signature. Cells can escape from this ?irreversible? arrest state through the upregulation of G1 cyclins. This map provides a more comprehensive understanding of the overall organization of cell proliferation and arrest.},
	pages = {e11087},
	number = {9},
	journal = {Molecular Systems Biology},
	author = {Stallaert, Wayne and Taylor, Sovanny R and Kedziora, Katarzyna M and Taylor, Colin D and Sobon, Holly K and Young, Catherine L and Limas, Juanita C and Varblow Holloway, Jonah and Johnson, Martha S and Cook, Jeanette Gowen and Purvis, Jeremy E},
	urldate = {2023-12-06},
	date = {2022-09},
	note = {Publisher: John Wiley \& Sons, Ltd},
	keywords = {cell cycle, proliferation, quiescence, senescence, single-cell},
}

@article{toulany_uncovering_2023,
	title = {Uncovering developmental time and tempo using deep learning},
	rights = {2023 The Author(s)},
	issn = {1548-7105},
	abstract = {During animal development, embryos undergo complex morphological changes over time. Differences in developmental tempo between species are emerging as principal drivers of evolutionary novelty, but accurate description of these processes is very challenging. To address this challenge, we present here an automated and unbiased deep learning approach to analyze the similarity between embryos of different timepoints. Calculation of similarities across stages resulted in complex phenotypic fingerprints, which carry characteristic information about developmental time and tempo. Using this approach, we were able to accurately stage embryos, quantitatively determine temperature-dependent developmental tempo, detect naturally occurring and induced changes in the developmental progression of individual embryos, and derive staging atlases for several species de novo in an unsupervised manner. Our approach allows us to quantify developmental time and tempo objectively and provides a standardized way to analyze early embryogenesis.},
	pages = {1--11},
	journal = {Nature Methods},
	shortjournal = {Nat Methods},
	author = {Toulany, Nikan and Morales-Navarrete, Hernán and Čapek, Daniel and Grathwohl, Jannis and Ünalan, Murat and Müller, Patrick},
	urldate = {2023-11-29},
	date = {2023-11-23},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Embryogenesis, Embryology, Imaging, Machine learning, Zebrafish},
}

@article{taieb_fuccitrack_2022,
	title = {{FUCCItrack}: An all-in-one software for single cell tracking and cell cycle analysis},
	volume = {17},
	issn = {1932-6203},
	shorttitle = {{FUCCItrack}},
	abstract = {Beyond the more conventional single-cell segmentation and tracking, single-cell cycle dynamics is gaining a growing interest in the field of cell biology. Thanks to sophisticated systems, such as the fluorescent ubiquitination-based cell cycle indicator ({FUCCI}), it is now possible to study cell proliferation, migration, changes in nuclear morphology and single cell cycle dynamics, quantitatively and in real time. In this work, we introduce {FUCCItrack}, an all-in-one, semi-automated software to segment, track and visualize {FUCCI} modified cell lines. A user-friendly complete graphical user interface is presented to record and quantitatively analyze both collective cell proliferation as well as single cell information, including migration and changes in nuclear or cell morphology as a function of cell cycle. To enable full control over the analysis, {FUCCItrack} also contains features for identification of errors and manual corrections.},
	pages = {e0268297},
	number = {7},
	journal = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Taïeb, Hubert M. and Bertinetti, Luca and Robinson, Tom and Cipitria, Amaia},
	urldate = {2023-11-10},
	date = {2022-07-06},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Cell cycle and cell division, Cell migration, Cell proliferation, Cell staining, Computer software, Fluorescence imaging, G1 phase, Open source software},
}

@article{he_cell_2022,
	title = {Cell Cycle Stage Classification Using Phase Imaging with Computational Specificity},
	volume = {9},
	abstract = {Traditional methods for cell cycle stage classification rely heavily on fluorescence microscopy to monitor nuclear dynamics. These methods inevitably face the typical phototoxicity and photobleaching limitations of fluorescence imaging. Here, we present a cell cycle detection workflow using the principle of phase imaging with computational specificity ({PICS}). The proposed method uses neural networks to extract cell cycle-dependent features from quantitative phase imaging ({QPI}) measurements directly. Our results indicate that this approach attains very good accuracy in classifying live cells into G1, S, and G2/M stages, respectively. We also demonstrate that the proposed method can be applied to study single-cell dynamics within the cell cycle as well as cell population distribution across different stages of the cell cycle. We envision that the proposed method can become a nondestructive tool to analyze cell cycle progression in fields ranging from cell biology to biopharma applications.},
	pages = {1264--1273},
	number = {4},
	journal = {{ACS} Photonics},
	shortjournal = {{ACS} Photonics},
	author = {He, Yuchen R. and He, Shenghua and Kandel, Mikhail E. and Lee, Young Jae and Hu, Chenfei and Sobh, Nahil and Anastasio, Mark A. and Popescu, Gabriel},
	urldate = {2023-11-14},
	date = {2022-04-20},
	note = {Publisher: American Chemical Society},
}

@article{rappez_deepcycle_2020,
	title = {{DeepCycle} reconstructs a cyclic cell cycle trajectory from unsegmented cell images using convolutional neural networks},
	volume = {16},
	issn = {1744-4292, 1744-4292},
	number = {10},
	journal = {Molecular Systems Biology},
	shortjournal = {Mol Syst Biol},
	author = {Rappez, Luca and Rakhlin, Alexander and Rigopoulos, Angelos and Nikolenko, Sergey and Alexandrov, Theodore},
	urldate = {2020-10-30},
	date = {2020-10},
	langid = {english},
	note = {{ZSCC}: 0000000},
}

@inproceedings{ulicna_learning_2023,
	title = {Learning dynamic image representations for self-supervised cell cycle annotation},
	abstract = {Time-based comparisons of single-cell trajectories are challenging due to their intrinsic heterogeneity, autonomous decisions, dynamic transitions and unequal lengths. In this paper, we present a self-supervised framework combining an image autoencoder with dynamic time series analysis of latent feature space to represent, compare and annotate cell cycle phases across singlecell trajectories. In our fully data-driven approach, we map similarities between heterogeneous cell tracks and generate statistical representations of single-cell trajectory phase durations, onset and transitions. This work is a first effort to transform a sequence of learned image representations from cell cycle-specific reporters into an unsupervised sequence annotation.},
	institution = {Cell Biology},
  maintitle = {ICML},
  booktitle = {Workshop on Computational Biology},    
	type = {preprint},
	author = {Ulicna, Kristina and Kelkar, Manasi and Soelistyo, Christopher J and Charras, Guillaume T and Lowe, Alan R},
	urldate = {2023-06-09},
	date = {2023-05-31},
	langid = {english},
}

@article{patel_computer_2021,
	title = {Computer vision reveals hidden variables underlying {NF}-κB activation in single cells},
	volume = {7},
	issn = {2375-2548},
	abstract = {An image-based machine learning approach finds mechanisms underlying variable activation of {NF}-κB pathway in single cells.
          , 
            Individual cells are heterogeneous when responding to environmental cues. Under an external signal, certain cells activate gene regulatory pathways, while others completely ignore that signal. Mechanisms underlying cellular heterogeneity are often inaccessible because experiments needed to study molecular states destroy the very states that we need to examine. Here, we developed an image-based support vector machine learning model to uncover variables controlling activation of the immune pathway nuclear factor κB ({NF}-κB). Computer vision analysis predicts the identity of cells that will respond to cytokine stimulation and shows that activation is predetermined by minute amounts of “leaky” {NF}-κB (p65:p50) localization to the nucleus. Mechanistic modeling revealed that the ratio of {NF}-κB to inhibitor of {NF}-κB predetermines leakiness and activation probability of cells. While cells transition between molecular states, they maintain their overall probabilities for {NF}-κB activation. Our results demonstrate how computer vision can find mechanisms behind heterogeneous single-cell activation under proinflammatory stimuli.},
	pages = {eabg4135},
	number = {43},
	journal = {Science Advances},
	shortjournal = {Sci. Adv.},
	author = {Patel, Parthiv and Drayman, Nir and Liu, Ping and Bilgic, Mustafa and Tay, Savaş},
	urldate = {2023-04-05},
	date = {2021-10-22},
	langid = {english},
}

@incollection{leake_convolutional_2022,
	location = {New York, {NY}},
	title = {Convolutional Neural Networks for Classifying Chromatin Morphology in Live-Cell Imaging},
	volume = {2476},
	isbn = {978-1-07-162220-9 978-1-07-162221-6},
	abstract = {Chromatin is highly structured, and changes in its organization are essential in many cellular processes, including cell division. Recently, advances in machine learning have enabled researchers to automatically classify chromatin morphology in ﬂuorescence microscopy images. In this protocol, we develop userfriendly tools to perform this task. We provide an open-source annotation tool, and a cloud-based computational framework to train and utilize a convolutional neural network to automatically classify chromatin morphology. Using cloud compute enables users without signiﬁcant resources or computational experience to use a machine learning approach to analyze their own microscopy data.},
	pages = {17--30},
	booktitle = {Chromosome Architecture},
	publisher = {Springer {US}},
	author = {Ulicna, Kristina and Ho, Laure T. L. and Soelistyo, Christopher J. and Day, Nathan J. and Lowe, Alan R.},
	editor = {Leake, Mark C.},
	urldate = {2023-07-19},
	date = {2022},
	langid = {english},
	note = {Series Title: Methods in Molecular Biology},
}

@misc{gallusser_self-supervised_2023,
	title = {Self-supervised dense representation learning for live-cell microscopy with time arrow prediction},
	abstract = {State-of-the-art object detection and segmentation methods for microscopy images rely on supervised machine learning, which requires laborious manual annotation of training data. Here we present a self-supervised method based on time arrow prediction pre-training that learns dense image representations from raw, unlabeled live-cell microscopy videos. Our method builds upon the task of predicting the correct order of time-ﬂipped image regions via a single-image feature extractor and a subsequent time arrow prediction head. We show that the resulting dense representations capture inherently time-asymmetric biological processes such as cell divisions on a pixel-level. We furthermore demonstrate the utility of these representations on several live-cell microscopy datasets for detection and segmentation of dividing cells, as well as for cell state classiﬁcation. Our method outperforms supervised methods, particularly when only limited ground truth annotations are available as is commonly the case in practice. We provide code at https://github.com/weigertlab/tarrow.},
	number = {{arXiv}:2305.05511},
	publisher = {{arXiv}},
	author = {Gallusser, Benjamin and Stieber, Max and Weigert, Martin},
	urldate = {2023-06-09},
	date = {2023-05-09},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2305.05511 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{cuny_cell_2022,
	title = {Cell region fingerprints enable highly precise single-cell tracking and lineage reconstruction},
	volume = {19},
	issn = {1548-7091, 1548-7105},
	pages = {1276--1285},
	number = {10},
	journal = {Nature Methods},
	shortjournal = {Nat Methods},
	author = {Cuny, Andreas P. and Ponti, Aaron and Kündig, Tomas and Rudolf, Fabian and Stelling, Jörg},
	urldate = {2023-11-29},
	date = {2022-10},
	langid = {english},
}


@inproceedings{he2016deep,
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  biburl = {https://www.bibsonomy.org/bibtex/2f08d8f1a1881a5c9ee27060e40ada500/weihao},
  booktitle = {Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition},
  issn = {1063-6919},
  keywords = {a},
  location = {Las Vegas, NV, USA},
  month = jun,
  pages = {770--778},
  publisher = {IEEE},
  series = {CVPR '16},
  timestamp = {2024-01-08T12:26:11.000+0100},
  title = {{Deep Residual Learning for Image Recognition}},
  year = 2016
  }


@inproceedings{wen2022transformers,
  title     = {Transformers in Time Series: A Survey},
  author    = {Wen, Qingsong and Zhou, Tian and Zhang, Chaoli and Chen, Weiqi and Ma, Ziqing and Yan, Junchi and Sun, Liang},
  booktitle = {Proceedings of the Thirty-Second International Joint Conference on
               Artificial Intelligence, {IJCAI-23}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Edith Elkind},
  pages     = {6778--6786},
  year      = {2023},
  month     = {8},
  note      = {Survey Track},
}  
  
  
@article{hewamalage2021recurrent,
title = {Recurrent Neural Networks for Time Series Forecasting: Current status and future directions},
journal = {International Journal of Forecasting},
volume = {37},
number = {1},
pages = {388-427},
year = {2021},
issn = {0169-2070},
author = {Hansika Hewamalage and Christoph Bergmeir and Kasun Bandara},
keywords = {Big data, Forecasting, Best practices, Framework},
abstract = {Recurrent Neural Networks (RNNs) have become competitive forecasting methods, as most notably shown in the winning method of the recent M4 competition. However, established statistical models such as exponential smoothing (ETS) and the autoregressive integrated moving average (ARIMA) gain their popularity not only from their high accuracy, but also because they are suitable for non-expert users in that they are robust, efficient, and automatic. In these areas, RNNs have still a long way to go. We present an extensive empirical study and an open-source software framework of existing RNN architectures for forecasting, and we develop guidelines and best practices for their use. For example, we conclude that RNNs are capable of modelling seasonality directly if the series in the dataset possess homogeneous seasonal patterns; otherwise, we recommend a deseasonalisation step. Comparisons against ETS and ARIMA demonstrate that (semi-) automatic RNN models are not silver bullets, but they are nevertheless competitive alternatives in many situations.}
}


@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}


@article{van2016wavenet,
  title={Wavenet: A generative model for raw audio},
  author={Van Den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray and others},
  journal={arXiv preprint arXiv:1609.03499},
  volume={12},
  year={2016}
}


@article{mahecic2022event,
  title={Event-driven acquisition for content-enriched microscopy},
  author={Mahecic, Dora and Stepp, Willi L and Zhang, Chen and Griffi{\'e}, Juliette and Weigert, Martin and Manley, Suliana},
  journal={Nature methods},
  volume={19},
  number={10},
  pages={1262--1267},
  year={2022},
  publisher={Nature Publishing Group US New York}
}
