\section{Taxonomy of Image Inversion Approaches} \label{sec:method}

\tikzstyle{leaf}=[draw=black,
    rounded corners,minimum height=1em,
    text width=24.50em, edge=black!10, 
    text opacity=1, align=center,
    fill opacity=.3,  text=black,font=\scriptsize,
    inner xsep=3pt, inner ysep=1pt,
    ]
\tikzstyle{leaf1}=[draw=black,
    rounded corners,minimum height=1em,
    text width=6.28em, edge=black!10, 
    text opacity=1, align=center,
    fill opacity=.5,  text=black,font=\scriptsize,
    inner xsep=3pt, inner ysep=1pt,
    ]
\tikzstyle{leaf2}=[draw=black, 
    rounded corners,minimum height=1em,
    text width=6.28em, edge=black!10, 
    text opacity=1, align=center,
    fill opacity=.8,  text=black,font=\scriptsize,
    inner xsep=3pt, inner ysep=1pt,
    ]

\begin{figure*}[ht]
\centering
\resizebox{1.00\linewidth}{!}{
\begin{forest}
for tree={
    forked edges,
    grow=east,
    reversed=true,
    anchor=base west,
    parent anchor=east,
    child anchor=west,
    base=middle,
    font=\scriptsize,
    rectangle,
    rounded corners=0.1pt, 
    draw=black, 
    edge=black!50, 
    rounded corners,
    align=center,
    minimum width=2em, 
    l sep=5pt,
    s sep=1pt,
    inner xsep=3pt, 
    inner ysep=1pt,
  },
  where level=1{text width=4em}{}, 
  where level=2{text width=1em,font=\scriptsize}{},
  where level=3{text width=8em,font=\scriptsize}{},
  where level=4{text width=10em,font=\scriptsize}{},
  where level=5{text width=1em,font=\scriptsize}{},
  [Generative Model Inversion,rotate=90,anchor=north,edge=black!50,fill=myblue,draw=black
    [GAN Inversion,edge=black!50,text width=4.5 em, fill=myred
        [Encoder-based \\ Approaches, leaf2, fill=myred
            [Direct Mapping Encoder, leaf1,text width=12 em,fill=myred
                 [{IcGAN~\cite{IcGAN}, e4e~\cite{e4e}},leaf,text width=23 em, fill=myred]
            ]
            [Multi-Module Encoder, leaf1,text width=12 em,fill=myred
                 [{DIEGAN~\cite{DIEGAN}, GradStyle~\cite{GradStyle}},leaf,text width=23 em,fill=myred]
            ]
        ]
        [Latent Optimization \\ Approaches, leaf2,fill=myred
            [Pure Optimization, leaf1,text width=12 em, fill=myred[InvertingGAN~\cite{InvertingGAN}, leaf,text width=23 em, edge=black!50, fill=myred]
            ] 
            [Extended Latent \\Space Optimization, leaf1,text width=12 em,fill=myred
                 [{Image2StyleGAN~\cite{Image2StyleGAN}, PTI~\cite{PTI}} ,
leaf,text width=23 em,fill=myred]
            ] 
            [Extra Network \\Optimization, leaf1, text width=12 em, fill=myred
                 [{HyperEditor~\cite{HyperEditor}, InvSR~\cite{InvSR}}, leaf,text width=23 em,fill=myred]
            ]  
        ]
        [Hybrid Approaches, leaf2,fill=myred
            [Shallow Integration, leaf1,text width=12 em, fill=myred
                 [{iGAN~\cite{iGAN}, SDIC~\cite{SDIC}},leaf,text width=23 em,fill=myred]
            ]  
            [Integration, leaf1,text width=12 em, fill=myred
                 [{LOR~\cite{LOR}, StyleFeatureEditor~\cite{StyleFeatureEditor}},leaf,text width=23 em,fill=myred]
            ]  
        ]
    ]
    [Diffusion Model \\ Inversion,edge=black!50,text width=4.5em, fill=myyellow
    	[Training-free \\ Approaches,leaf2, fill=myyellow,
            [Attention-based Control, leaf1, text width=12 em, fill=myyellow
                 [{Prompt-to-Prompt~\cite{p2p}, FreePromptEditing~\cite{FreePromptEditing}, \\
                 StyleID~\cite{StyleID}, TIC~\cite{TIC}},leaf,text width=23 em,fill=myyellow]
            ]  
            [Embedding Replacement, leaf1,text width=12 em, fill=myyellow
                 [{Null-Text Inversion~\cite{Null-Text}, BIRD~\cite{BIRD}, \\ Negative-prompt Inversion~\cite{Negative-prompt}},leaf,text width=23 em,fill=myyellow]
            ]  
            [Feature Fusion, leaf1,text width=12 em, fill=myyellow 
                 [{TF-ICON~\cite{TF-ICON}, TIGIC~\cite{TIGIC}, \\ FreeControl~\cite{FreeControl}},leaf,text width=23 em,fill=myyellow]
            ]  
         ]
         [Fine-tuning \\Approaches, leaf2, fill=myyellow
            [Subject Embedding \\Optimization, leaf1,text width=12 em,fill=myyellow
                 [{DreamBooth~\cite{DreamBooth}, FaceChain-SuDe~\cite{FaceChain-SuDe},\\ Photoswap~\cite{Photoswap}}, leaf,text width=23 em,fill=myyellow]
            ]
            [Vector Projection \\ Mechanism, leaf1, text width=12 em, fill=myyellow
                 [{Forgedit~\cite{Forgedit}, DAC~\cite{DAC}},leaf,text width=23 em,fill=myyellow]
            ]
            [Attention Optimization, leaf1, text width=12 em, fill=myyellow
                 [CLiC~\cite{CLiC},leaf,text width=23 em,fill=myyellow]
            ]
         ]
         [Extra Trainable\\ Module Approaches, leaf2, fill=myyellow
            [Noise Prediction, leaf1,text width=12 em, fill=myyellow
                 [{TurboEdit~\cite{TurboEdit}, InvSR~\cite{InvSR}},leaf,text width=23 em,fill=myyellow]
            ]
            [Feature Extraction , leaf1,text width=12 em, fill=myyellow
                 [{InST~\cite{InST}, StyleDiffusion~\cite{StyleDiffusion},\\DETEX~\cite{DETEX}},leaf,text width=23 em,fill=myyellow]
            ]
            [Encoder-base, leaf1, text width=12 em, fill=myyellow
                 [DiffEditor~\cite{DiffEditor} ,leaf,text width=23 em,fill=myyellow]
            ]
         ]
    ]
    [Promising Technologies\\ on the Rise, edge=black!50,text width=7em, fill=mypurple
    [DiT framework,leaf1, text width=10 em, fill=mypurple
    [{DiT4Edit~\cite{DiT4Edit}, stable Flow~\cite{Stable}},leaf1,text width=30 em, fill=mypurple]
    ]
    [Rectified Flow framework,leaf1, text width=10 em,fill=mypurple
    [{RFInversion~\cite{RFInversion}, GNRl~\cite{GNRI}, RF-Sower-Edit~\cite{RF-Solver-Edit}},leaf2,text width=30 em, fill=mypurple]]
    ]
  ]
\end{forest}
}
\caption{A taxonomy of generative model inversion approaches from GANs to Diffusion and beyond.}
\label{fig:taxonomy_of_GMI}
\vspace{-1.5em}
\end{figure*}



\subsection{Background and Definition of Image Inversion}
\nibf{Generative Model Inversion} 
refers to the process of recovering the latent variables of a pretrained generative model given an observed output \(x_0\). As shown in ~\cref{fig:teaser}-\textbf{\uppercase\expandafter{\romannumeral1}}, suppose we have a generative model $F$ that maps a latent code \(z\) from a latent space \(Z\) to the output image space \(X\): 
\begin{equation}
F: Z \to X, \quad x = F(z).
\end{equation} 
Given an input image \(x_0 \in X\), the goal of inversion is to find a latent code \(z^* \in Z\) such that the reconstructed image \(x = F(z^*)\) is as close as possible to the input \(x_0\). This process can be formulated as the following optimization problem:

\begin{equation}
z^* = \underset{z}{\mathrm{argmin}} \, \mathcal{L}(F(z), x_0),
\end{equation} 
where \( \mathcal{L} \) is a predefined loss function that measures the discrepancy between the reconstructed image \(x = F(z)\) and the original input \(x_0\). Common choices for \( \mathcal{L} \) include pixel-wise, perceptual, and SSIM losses.

\nibf{GAN Inversion} 
is a specific instance of generative model inversion. As show in ~\cref{fig:teaser}-\textbf{\uppercase\expandafter{\romannumeral2}}, the generator is denoted as $G$, which maps latent variables \(z\) from the latent space \(Z\) to the output space \(X\): $G: Z \to X$, where $x = G(z).$

Its goal is to find a latent code \(z^* \in Z\) such that the reconstructed image \(x = G(z^*)\) is as close as possible to the input \(x_0\), expressed as the following optimization problem:
\vspace{-0.2em}
\begin{equation}
z^* = \underset{z}{\mathrm{argmin}} \, \mathcal{L}(G(z), x_0),
\end{equation} 
\vspace{-0.2em}
where \( \mathcal{L} \) measures the discrepancy between the reconstructed image \(x = G(z)\) and the input \(x_0\). In GAN inversion, additional regularization terms may be introduced to ensure the latent code \(z^*\) remains semantically meaningful or lies within the generator's latent space distribution.

\nibf{Diffusion Model Inversion}
models are stepwise generative models that create data \(x_0\) by iteratively denoising an initial noise \(z_T\). As shown in ~\cref{fig:teaser}-\textbf{\uppercase\expandafter{\romannumeral3}}, the generative process of a diffusion model is described as:  
\vspace{-0.5em}
\begin{equation}
\quad p_\theta(x_{0:T}) = p(z_T) \prod_{t=1}^T p_\theta(z_{t-1} | z_t),
\end{equation} 
where \(z_t\) represents the intermediate latent variable at timestep \(t\), \(z_T\) is the initial noise, \(T\) is the total number of timesteps, and \(p_\theta(z_{t-1} | z_t)\) denotes the conditional denoising probability parameterized by \(\theta\). 
The goal of diffusion model inversion is to recover the initial noise \(z_T^*\) such that the reconstructed image \(x_0\) matches the input \(x_0\). This can be formulated as:  
\vspace{-0.5em}
\begin{equation}
z_T^* = \underset{z_T}{\mathrm{argmin}} \, \mathcal{L}(z_T, x_0),
\end{equation}  
or further decomposed into intermediate optimization problems at each timestep \(t\):  

\begin{equation}
z_t^* = \underset{z_t}{\mathrm{argmin}} \, \mathcal{L}(z_t, x_0).
\end{equation}  
Diffusion model inversion involves iteratively optimizing the intermediate latent variables \(z_t\) at each timestep, eventually recovering the initial noise \(z_T\). This process can also incorporate the denoising likelihood \(p_\theta(z_{t-1} | z_t)\) for timestep-by-timestep optimization. 
Detailed methods in \cref{fig:taxonomy_of_GMI} are detailed in the following sections

\subsection{GAN Inversion}
GAN inversion techniques can be broadly classified into three categories: 
\textit{i)} Encoder-based approaches prioritize speed and conditional control, \textit{ii)} Latent Optimization approaches emphasize reconstruction fidelity, and \textit{iii)} Hybrid approaches seek to balance these competing objectives. This categorization reflects the primary strategy employed to map a real image back into the latent space of a pre-trained GAN.

\nibf{Encoder-based Approaches.} 
In encoder-based image inversion methods, researchers focus on constructing a direct mapping from image space to the GAN latent space. These methods are renowned for their efficient inference speed and practical utility, making them particularly suitable for real-time processing scenarios and conditional generation tasks. A seminal work in this domain, IcGAN~\cite{IcGAN}, innovatively designed a dual-encoder architecture responsible for extracting the latent features $\mathbf{z}$ and the associated conditional information $\mathbf{y}$ from images. This breakthrough enabled conditional image editing based on cGANs~\cite{cGANs}. Subsequently, e4e~\cite{e4e} proposed a specialized encoding scheme tailored to the characteristics of StyleGAN~\cite{StyleGAN}. The core of this approach lies in the progressive offset training strategy combined with adversarial constraints, effectively bridging the gap between latent encoding and the native StyleGAN space, thereby significantly enhancing the model's editability and fidelity. Recent advancements such as DIEGAN~\cite{DIEGAN} employ a hybrid network that combines encoder-derived latent codes with randomly sampled codes for diverse image restoration and editing tasks, utilizing a two-stage training framework. Additionally, GradStyle~\cite{GradStyle} introduces a dual-stream framework integrating progressive residual alignment and global alignment modules, aimed at high-fidelity image reconstruction and flexible attribute manipulation. Overall, these technical approaches rely on establishing direct image-to-latent space mapping to achieve rapid inversion operations. In practical development, it remains crucial to carefully balance reconstruction quality and editability.

\nibf{Latent Optimization Approaches.} 
Latent optimization methods directly optimize the latent variables of a pre-trained GAN to find the optimal latent code reconstruct a given image, achieving high reconstruction fidelity without additional modules. InvertingGAN~\cite{InvertingGAN} uses gradient descent to directly invert GAN generators, achieving high-quality image reconstruction. Image2StyleGAN~\cite{Image2StyleGAN} embeds images into StyleGAN's extended W+ space via optimization, improving embedding quality and style transfer capabilities. PTI~\cite{PTI} combines latent space optimization with local generator fine-tuning, balancing reconstruction quality and editability by adapting the generator to specific input image characteristics.  These approaches prioritize reconstruction accuracy by iteratively refining latent codes, albeit at the cost of increased computational time. 
Considering the latency requirements of practical applications and the emergence of a series of high-quality diffusion generative models, such approaches have gradually faded from researchers' focus in recent years.

\nibf{Hybrid Approaches.} 
Hybrid methods combine the strengths of both encoder-based and latent optimization approaches, typically using an encoder to provide an initial estimate of the latent code, which is then refined through optimization, allowing a balance between speed and reconstruction quality, while also enabling more complex editing tasks. iGAN~\cite{iGAN} combines deep learning predictors and optimization techniques to efficiently project images onto the GAN manifold and enable interactive editing. SDIC~\cite{SDIC} learns a spatial-contextual discrepancy map to compensate for information loss in latent codes and GAN generators, generating high-quality reconstructions and edits. LOR~\cite{LOR} combines encoders and latent space direction optimization for high-quality face reenactment via joint training and feature space refinement. StyleFeatureEditor~\cite{StyleFeatureEditor} adopts a two-phase learning approach to train an inverter and a feature editor, addressing detail reconstruction and editability in high-dimensional feature spaces. These approaches leverage the efficiency of encoders and the precision of latent optimization, offering a versatile solution for GAN inversion and image editing.

\nibf{Pros and Cons Discussion.}
GAN inversion for image editing faces challenges in balancing reconstruction fidelity, editability, and computational efficiency. The field is moving towards hybrid methods that merge the speed of encoders with the precision of latent optimization. These approaches aim to enable real-time, high-quality image manipulation, preserving essential details and supporting complex editing tasks.

\subsection{Diffusion Model Inversion.}
According to the differences in training strategies, the Inversion of Diffusion Model for Image methods can be categorized into three types: training-free methods, fine-tuning methods, and methods with additional trainable modules. This classification provides a clear reflection of the differences in inversion strategies with respect to model training.

\nibf{Training-free Approaches.} 
Training-free methods aim to enable efficient image inversion and editing by directly utilizing pre-trained diffusion models, without relying on additional training or fine-tuning. This category is highly regarded for its convenience and flexibility, especially in scenarios demanding high efficiency. Prompt-to-Prompt (P2P)\cite{p2p} exemplifies this, achieving intuitive text-driven image editing by manipulating cross-attention maps, eliminating the need for additional model adjustments and providing a direct means of control for text-guided image manipulation. Building upon this, Null-Text Inversion\cite{Null-Text} leverages DDIM inversion, reconstructing and editing images by optimizing unconditional text embeddings, avoiding dependence on specific text information and enhancing editing flexibility. To achieve more precise inversion, EDICT~\cite{EDICT} introduces coupling transformations and mixing layers, aiming to improve the fidelity of the inversion results. Negative-prompt Inversion~\cite{Negative-prompt} takes a different approach, replacing null-text embeddings with negative-prompt embeddings, achieving efficient inversion and editing without optimization, simplifying the operation process. Furthermore, TIC~\cite{TIC} enhances features of DDIM inversion to improve editing consistency, ensuring the naturalness of the edited results. StyleID~\cite{StyleID} achieves training-free style transfer through self-attention feature replacement and initial noise adjustment, making style conversion more convenient. TF-ICON~\cite{TF-ICON} leverages exceptional prompts and attention injection to achieve cross-domain image composition, providing more possibilities for image creation. FreePromptEditing~\cite{FreePromptEditing} achieves efficient text-guided editing by modifying self-attention maps, improving editing efficiency. DesignEdit~\cite{DesignEdit} proposes a multi-layered latent decomposition and fusion framework, aiming to improve editing quality, making the editing results more refined. FreeControl~\cite{FreeControl} introduces spatial control, utilizing feature subspaces for spatial adjustment of text-to-image generation, providing stronger control capabilities for image generation. Blended Diffusion~\cite{Blended} blends text-guided diffusion latents with input noise, enabling seamless local image editing, making local editing more natural. InterpretDiffusion~\cite{InterpretDiffusion} optimizes latent vectors, exploring semantic directions applicable to fair, safe, and responsible generation, providing technical support for the ethical application of generative models. BIRD~\cite{BIRD} achieves training-free blind image restoration by optimizing the initial noise of pre-trained diffusion models, providing new ideas for image restoration. Therefore, training-free methods hold a significant position in image inversion due to their high efficiency and flexibility, along with continuously enriching technical means.

\nibf{Fine-tuning Approaches.} 
Fine-tuning methods improve the adaptability and performance of pre-trained diffusion models on specific tasks by adjusting their parameters. Unlike training-free methods, fine-tuning methods focus on targeted optimization of the model to adapt to specific application scenarios. DreamBooth~\cite{DreamBooth} is a representative example, binding specific subjects with rare identifiers by fine-tuning a small set of input images and introducing class-related prior preservation loss to enhance generation diversity, thus excelling in personalized generation. Forgedit~\cite{Forgedit} improves fine-tuning strategies with vector projection, achieving efficient text-guided image editing and enhancing editing efficiency. Photoswap~\cite{Photoswap}, building on this, combines attention swapping and fine-tuned subject concept learning for more personalized subject replacement. FaceChain-SuDe~\cite{FaceChain-SuDe} introduces Subject-Derived regularization (SuDe), enabling models to inherit common attributes of their category while retaining individual characteristics by fine-tuning partial parameters, thus possessing unique advantages in fields such as face editing. CLiC~\cite{CLiC} introduces innovative fine-tuning and cross-attention optimization, enabling in-context learning and transfer of local visual concepts, providing stronger semantic understanding capabilities for image editing. As such, fine-tuning methods can effectively enhance model performance on specific tasks, but also require a certain training cost, necessitating a trade-off between performance and cost.

\nibf{Extra Trainable Module.} 
Extra trainable module methods enhance the functionality of diffusion models by introducing new modules or adjusting network structures, particularly excelling in image editing tasks. Compared to fine-tuning methods, this category typically possesses greater scalability and flexibility, capable of handling more complex editing tasks. For example, StyleDiffusion~\cite{StyleDiffusion} optimizes input embeddings for cross-attention "value" layers, achieving text-guided localized or holistic style editing, providing new ideas for style transfer. DAC~\cite{DAC} introduces two LoRA modules within a doubly abductive reasoning framework to fine-tune the UNet and text encoder, thereby improving fidelity and editability, providing a guarantee for high-quality image editing. InST~\cite{InST} utilizes an attention-based textual inversion module for efficient artistic style transfer, further expanding the application scope of style transfer. DETEX~\cite{DETEX} achieves separation of background and pose information by fine-tuning subject embeddings on Stable Diffusion and introducing trainable attribute mappers, enabling flexible generation control, providing more refined control capabilities for image editing. TurboEdit~\cite{TurboEdit} trains encoder networks to reduce computational costs, thereby improving the efficiency of image editing. InvSR~\cite{InvSR} combines a noise predictor network with partial noise prediction strategies, achieving flexible and efficient super-resolution inversion, enhancing the resolution of image editing. Overall, extra trainable module methods provide diffusion models with greater scalability and flexibility, enabling them to better adapt to various image editing tasks, but also require more sophisticated design and optimization.


\nibf{Pros and Cons Discussion.}

Diffusion model inversion faces challenges in balancing reconstruction accuracy with editability, particularly in maintaining consistency and fidelity during manipulation. The field is progressing towards faster, more controllable, and inversion-free editing techniques, with a focus on improving the robustness and real-time applicability of these models for diverse image editing tasks.


\subsection{Promising Technologies on the Rise}

The latent space of GANs can be distorted, limiting editability, while Diffusion Models require many iterative steps, leading to high computational costs. To address these issues, Diffusion Transformers (DiT)~\cite{DiT} and Rectified Flows have emerged, offering new possibilities for image editing. 
\textbf{\textit{1) The DiT framework}}, with its global receptive field and strong modeling capabilities, excels in image generation and editing. Compared to GANs, DiT's latent space is smoother and more amenable to semantic operations, allowing for more controllable editing. Unlike traditional Diffusion Models, DiT achieves high-quality inversion with fewer steps, enhancing efficiency. For example, DiT4Edit~\cite{DiT4Edit} optimizes generation speed and editing quality by using DPM-Solver for fewer steps, unifying control of self- and cross-attention for layout and consistency, and introducing patch merging to reduce computation during inference. Stable Flow~\cite{Stable}, also based on DiT, selectively injects attention features into vital layers to maintain high-quality editing while preserving non-edited regions. 
\textbf{\textit{2) Rectified Flow framework}} avoids GAN latent space distortion and reduces Diffusion Model computational costs by directly mapping image space to latent space. RF Inversion~\cite{RFInversion} uses stochastic rectified differential equations and a dynamically controlled vector field for image inversion and editing without additional parameter training, balancing faithfulness and editability. GNRI~\cite{GNRI} improves text-to-image diffusion model inversion quality and speed by introducing a guidance regularization term into the Newton-Raphson optimization process. RF-Solver-Edit~\cite{RF-Solver-Edit}, using the training-free sampler RF-Solver, reduces ODE-solving errors with high-order Taylor expansion and achieves high-quality editing while preserving source image structure integrity. 
Overall, both above frameworks offer superior controllability and inversion efficiency, representing significant advancements and direction.

