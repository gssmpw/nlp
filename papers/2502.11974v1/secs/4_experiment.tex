\section{Application Tasks} \label{sec:experiment}
\subsection{Popular Datasets}
\nibf{LAION}~\cite{LAION}
is a vast, open multimodal dataset with billions of image-text pairs across various languages, domains, and content. Its diversity is crucial for inversion tasks in generative models, particularly in cross-modal retrieval and generative AI training. Generative models can learn image-text relationships, facilitating the reverse mapping of semantic representations in latent space for text-guided image editing.

\nibf{PIE-bench}~\cite{TurboEdit}
is a specialized benchmark dataset for evaluating image editing models, comprising 700 images of natural and artificial scenes and covering 10 editing types like object replacement and style modification. Each entry includes detailed source and target image descriptions, editing instructions, and annotated editing masks. In generative model inversion, PIE-bench guides fine-grained latent space operations using these instructions and masks, allowing precise evaluation of image editing performance.

\nibf{FFHQ}~\cite{StyleGAN}
is a high-quality, diverse face dataset with 70,000 images up to 1024$\times$1024 resolution, encompassing various ages, genders, ethnicities, and backgrounds. It is extensively used in generative model inversion for facial generation and style transfer tasks. By inverting the latent vector, models can edit specific facial attributes (\textit{e.g.}, expression or hairstyle) while preserving other features.

\nibf{MS COCO}~\cite{MSCOCO}
offers 328,000 images across 91 categories in everyday scenes, with high-quality annotations for object detection, instance segmentation, panoptic segmentation, human keypoint detection, and image captioning. In generative model inversion, MS COCO's text descriptions and annotations assist models in locating specific object representations in the latent space, facilitating image editing and reconstruction in complex scenes.

\nibf{CelebA-HQ}~\cite{CelebA-HQ}
comprises 30,000 high-quality face images at 1024$\times$1024 resolution, annotated with 40 attribute labels (\textit{e.g.}, gender, age, hairstyle, and expressions). These labels enable precise control over attributes in generative model inversion. By editing the latent representation, models can manipulate specific attributes, such as changing hairstyles, while preserving other facial features.

\nibf{DreamBooth}~\cite{DreamBooth}
includes 3$\sim$5 high-quality images of 30 subjects and 25 text prompts for evaluating subject-driven generation tasks like recontextualization, property modification, accessorization, and artistic rendering. In generative model inversion, DreamBooth aids in exploring subject-specific representations in the latent space. By using unique identifiers and class names, the model enables personalized subject generation and attribute editing while preventing semantic drift. 

\subsection{Mainstream Applications}
Generative model inversion is pivotal in image editing and generation. By encoding real images into a latent space, image inversion methods leverage the capabilities of the latent space for manipulating images. We outlines the applications (summarized in \cref{table:application}) across various mainstream tasks without focusing on the implementation framework:

\begin{enumerate}[leftmargin=*,topsep=0pt,itemsep=0pt,align=left]
    \item \textbf{Object Editing:} Object editing enables the editing of specific objects within an image, including operations such as object replacement, removal, and addition. Methods include Prompt-to-Prompt~\cite{p2p} and Null-Text Inversion~\cite{Null-Text}.

    \item \textbf{Attribute Editing:} Attribute editing facilitates the modification of object attributes within an image, including editing the color, texture, and shape of objects, as well as the age, expression, and hairstyle of people.
        \begin{itemize}[leftmargin=*,topsep=0pt,itemsep=0pt,align=left]
            \item \textbf{Editing Direction:} Modifies attributes by adjusting the latent space vector of the image. Example methods: iGAN~\cite{iGAN}, Image2StyleGAN~\cite{Image2StyleGAN}, e4e~\cite{e4e}, \textit{etc}.
            \item \textbf{Prompt Editing:} Modifies attributes by modifying the text prompt. Example methods: HyperEditor~\cite{HyperEditor}, Prompt-to-Prompt~\cite{p2p}, Null-Text Inversion~\cite{Null-Text}.
        \end{itemize}

    \item \textbf{Style Transfer:} Style transfer allows for the transfer of style from one image to another, and it can also be achieved through Prompt-based style transfer, thereby changing the overall style of the image.
        \begin{itemize}[leftmargin=*,topsep=0pt,itemsep=0pt,align=left]
            \item \textbf{Based on Reference Image:} Changes the style of the target image using the style of a reference image. Example methods: Image2StyleGAN~\cite{Image2StyleGAN} and InST~\cite{InST}.
            \item \textbf{Based on Semantic Information:} Uses semantic information such as text prompts to control style transfer. Example methods: HyperEditor~\cite{HyperEditor}, Prompt-to-Prompt\cite{p2p}, Null-Text Inversion~\cite{Null-Text}.
        \end{itemize}

    \item \textbf{Image Concept Decoupling:} This task refers to learning local concepts from a single image and using them to generate other objects, thereby achieving independent control over different concepts in the image. CLiC~\cite{CLiC} is frequently employed.

    \item \textbf{Spatially Aware Editing:} Spatially aware editing enables editing images based on spatial information, including operations such as removal, movement, scaling, rotation, and translation. In addition to using Prompt control, Point drag control can also be used to achieve more refined spatial editing. DiffEditor~\cite{DiffEditor} and DesignEdit~\cite{DesignEdit} are commonly used methods.

    \item \textbf{Controllable Image Generation:} Allows users to control the image generation process through various inputs (\textit{e.g.}, sketches, segmentation maps, poses, etc.). Typical method is FreeControl~\cite{FreeControl}.

    \item \textbf{Personalized Generation:} Personalized generation can be used to reproduce specific subjects in different scenarios, such as generating images of specific subjects in different artistic styles, different perspectives, different accessories, different expressions, and different backgrounds.
        \begin{itemize}[leftmargin=*,topsep=0pt,itemsep=0pt,align=left]
            \item \textbf{Single-Sample Personalized Generation:} Personalizes generation using only one reference image. Example method: FaceChain-SuDe~\cite{FaceChain-SuDe}.
            \item \textbf{Multi-Sample Personalized Generation:} Personalizes generation using multiple reference images for better results. Typical methods: DreamBooth~\cite{DreamBooth} and DETEX~\cite{DETEX}.
        \end{itemize}

    \item \textbf{Image Restoration:} Image restoration can be used to repair damaged images, common sub-tasks are as follows:
    \begin{itemize}[leftmargin=*,topsep=0pt,itemsep=0pt,align=left]
        \item \textbf{Image Super-Resolution:} Increases the resolution of an image, making it clearer. Typical methods: BIRD~\cite{BIRD} and InvSR~\cite{InvSR}.
        \item \textbf{Denoising:} Removes noise from an image, making it cleaner. Typical method: BIRD~\cite{BIRD}.
        \item \textbf{Inpainting:} Repairs missing or damaged parts of an image. Typical methods: DIEGAN~\cite{DIEGAN}, and TurboEdit~\cite{TurboEdit}.
        \item \textbf{Deblurring:} Removes blur from an image, making it clearer. Typical method: BIRD~\cite{BIRD}.
    \end{itemize}

    \item \textbf{Image Fusion:} Image fusion refers to integrating content from two or more images into one image, thereby creating a new image. Photoswap~\cite{Photoswap}, TF-ICON~\cite{TF-ICON}, and TIGIC\cite{TIGIC} are commonly used methods.

\end{enumerate}

\begin{table}[tp]
    \centering
    \caption{Methods for downstream tasks. \gan{Rose red} and \dif{orange} colors represent GAN and diffusion inversion methods, respectively.}
    \label{table:application}
    \renewcommand{\arraystretch}{1.2}
    \setlength\tabcolsep{3.0pt}
    \resizebox{\columnwidth}{!}{ 
        \begin{tabular}{lp{0.9\linewidth}}
        \toprule[0.17em]
        \textbf{Task} & \textbf{Representative Methods} \\
        \midrule
        \makecell[l]{Object Editing} & \makecell[l]{\gan{iGAN}, \dif{Prompt-to-Prompt}, \dif{Null-Text Inversion} \\\dif{EDICT}, \dif{StyleDiffusion}, \dif{Negative-prompt Inversion} \\ \dif{Forgedit}, \dif{FreePromptEditing}, \dif{DAC}, \dif{TurboEdit}, \dif{TIC}} \\
        \midrule
        \makecell[l]{Attribute Editing} & \makecell[l]{\textbf{Editing Direction:} \gan{iGAN}, \gan{IcGAN}\\ \gan{Image2StyleGAN},\gan{ e4e},\gan{ PTI}, \gan{SDIC}, \gan{LOR}\\ \gan{GradStyle},\gan{ StyleFeatureEditor} \\ \textbf{Prompt Editing:} \gan{HyperEditor}, \dif{Prompt-to-Prompt}\\ \dif{Null-Text Inversion}, \dif{EDICT}, \dif{StyleDiffusion}\\ \dif{Negative-prompt Inversion}, \dif{Forgedit}\\ \dif{FreePromptEditing}, \dif{DAC}, \dif{TurboEdit}, \dif{TIC}} \\
        \midrule
        \makecell[l]{Style Transfer} & \makecell[l]{\textbf{Reference-Based:} \gan{Image2StyleGAN}, \dif{InST}, \dif{StyleID}\\ \textbf{Semantic-Based:} \gan{HyperEditor}, \dif{Prompt-to-Prompt}\\ \dif{Null-Text Inversion}, \dif{EDICT}, \dif{StyleDiffusion}\\ \dif{FreePromptEditing}, \dif{DAC}} \\
        \midrule
        \makecell[l]{Image Concept Decoupling} & \makecell[l]{\dif{CLiC}} \\
        \midrule
        \makecell[l]{Spatial-Aware Editing} & \makecell[l]{\dif{DiffEditor},\dif{ DesignEdit}} \\
        \midrule
        \makecell[l]{Controllable Image Generation} & \makecell[l]{\dif{FreeControl}} \\
        \midrule
        \makecell[l]{Personalized Generation} & \makecell[l]{\textbf{Single-Sample Personalization:} \dif{FaceChain-SuDe} \\ \textbf{Multi-Sample Personalization:} \dif{DreamBooth}, \dif{DETEX}} \\
        \midrule
        \makecell[l]{Image Restoration} & \makecell[l]{\textbf{Super-Resolution:} \dif{BIRD}, \dif{InvSR} \\ \textbf{Denoising:} \dif{BIRD} \\ \textbf{Inpainting:} \gan{DIEGAN}, \dif{TurboEdit}, \dif{Blended Diffusion} \\ \textbf{Deblurring:} \dif{BIRD}} \\
        \midrule
        \makecell[l]{Image Fusion} & \makecell[l]{\textbf{Subject Replacement:} \dif{Photoswap} \\ \textbf{Pasting:} \dif{TF-ICON}, \dif{TIGIC}} \\
        \bottomrule[0.17em]
        \end{tabular}
    }
\end{table}


\subsection{Related Domain Research}
To further enrich the research on inversion tasks, we briefly review related popular tasks beyond image inversion to strengthen this inspection. 

\nibf{Video Inversion.}
In the video domain, RIGID~\cite{RIGID} introduces an additional trainable temporal encoder combined with self-supervised constraints to learn inter-frame temporal coherence, enabling high-quality video reconstruction and editing without retraining the generator. Built on StyleGAN2~\cite{StyleGAN2}, it optimizes frame consistency and global identity preservation. Meanwhile, Videoshop~\cite{Videoshop} allows users to edit the first frame of a video and automatically propagates the changes across the entire sequence using noise extrapolation and latent normalization techniques, maintaining semantic consistency, motion naturalness, and geometric stability.

\nibf{Audio Inversion.}
MusicMagus\cite{MusicMagus} is a zero-shot music editing method that leverages pretrained diffusion models with latent space manipulation and cross-attention constraints, enabling text-guided audio editing without additional training. Similarly, ZETA~\cite{ZETA} introduces a zero-shot audio editing method using diffusion model inversion to extract noise semantic directions for fine-grained audio editing, supporting tasks such as text-guided removal, style changes, improvisation, and pitch adjustments.

\nibf{3D Inversion.}
In the 3D domain, E3DGE~\cite{E3DGE} is an encoder-based method that leverages self-supervised learning with global latent codes and pixel-aligned local features for high-fidelity 3D face reconstruction and consistent editing. Its dual-module structure includes global and local encoders to learn coarse 3D shapes and detailed textures, ensuring high consistency during viewpoint changes and semantic editing. Meanwhile, SHAP-EDITOR\cite{SHAP-EDITOR} trains a universal latent editor network by distilling knowledge from 2D diffusion models (\textit{e.g.}, InstructPix2Pix~\cite{InstructPix2Pix}), transferring the understanding of natural language instructions into the 3D latent space for fast and high-quality 3D editing.

\nibf{Medical Image.}
In the medical domain, TPDM\cite{TPDM} leverages two perpendicular 2D diffusion models to jointly model 3D medical data distributions, achieving significant improvements in tasks such as Z-axis super-resolution, compressed sensing MRI, and sparse-view CT reconstruction. InverseSR~\cite{InverseSR}, on the other hand, optimizes latent codes to adapt to both high-sparsity and low-sparsity scenarios, enabling high-resolution MRI reconstruction from low-resolution MRI, excelling in tasks like slice imputation and tumor/lesion filling.



