\section{Related Works}
\label{sec:related}

To our knowledge, no previous works are analyzing non-causal \textsc{nlg} for non-English languages. This section will discuss relevant contributions to this work, mainly focusing on non-causal \textsc{nlg} and other Viterbi algorithm applications in \textsc{nlp}.

\subsection{Causality in generative transformer language models}
\label{sec:noncausal_related}

Transformers are unsupervised learners thanks to their self-attention mechanism **Vaswani et al., "Attention Is All You Need"** ____, which controls the impact of the context on the model's output. The original transformer architecture is composed of an encoder and a decoder. While the encoder's attention is bidirectional, the decoder has a masked multi-head attention block that masks non-causal context and a bidirectional multi-head attention block that receives non-causal information from the encoder.

Although the encoder-decoder architecture is widely used in some \textsc{nlp} applications like machine translation **Bahdanau et al., "Neural Machine Translation by Jointly Learning to Align and Translate"** ____, other transformer-based models only use one of these two components. By excluding the encoder, we eliminate all non-causal contextual dependencies, thus using only the masked attention of the decoder. Currently, decoder-only transformers are the most effective task-agnostic \textsc{nlg} systems.

While open domain \textsc{nlg} is mainly causal, there are a few non-causal \textsc{nlg} solutions. Most non-causal \textsc{nlg} systems are focused on particular tasks such as speech recognition **Graves et al., "Sequence Transduction with Recurrent Neural Networks"** ____, style transfer and grammar correction __**, textual data augmentation __**, and dialog systems ____.

Non-causal language models can also be trained for masked Language Modeling (\textsc{mlm}) **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** ____. \textsc{mlm} is an \textsc{nlg} task consisting of predicting masked words within a sentence. Some generative systems use bidirectional transformers trained on this task to recursively generate and fill masked tokens ____. As these can be filled in any location within the text, these models can produce text in a non-causal way.

Non-causal \textsc{nlg} strategies perform much worse in English than their causal counterparts ____. However, to our knowledge, no prior research has been conducted on non-causal \textsc{nlg} in languages other than English. This work aims to evaluate whether bidirectional transformers trained on the \textsc{mlm} task could be successfully exploited in Spanish \textsc{nlg}.

\subsection{Viterbi algorithm in \textsc{nlp} }
The Viterbi algorithm **Forney, "Maximum-Likelihood Sequence Estimation of Digital Sequences in the Presence of Intersymbol Interference"** __ is a dynamic programming algorithm used to find the most probable sequence of hidden states, known as the Viterbi path, given a sequence of observed events.

The Viterbi algorithm has been used in \textsc{nlp} applications such as speech recognition **Graves et al., "Sequence Transduction with Recurrent Neural Networks"** ____, in which the audio data is the observed sequence of events and the state space is defined by tokenized text strings, and in part-of-speech (\textsc{pos}) tagging **Vaswani et al., "Attention Is All You Need"** ____, in which the observed sequence is the given text and the state space is given by all possible \textsc{pos} tags. 

Even conceptually close, the Viterbi algorithm formulation proposed in this paper is different from that of those other \textsc{nlp} works. In our approach, instead of the tokenizer's vocabulary or \textsc{pos} tags, the Markov chain states are given by the initial phrase in all the possible different states of completion. The transition probabilities are computed using the bidirectional transformer language model described in Section \ref{sec:model_results}.

As previously said, our Viterbi algorithm-based approach explained in Section \ref{sec:method} is, to our knowledge, the first to estimate the optimal order for word generation in a language, contributing to the development of new non-causal \textsc{nlg} methodologies.