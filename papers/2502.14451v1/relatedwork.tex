\section{Related Works}
\label{sec:related}

To our knowledge, no previous works are analyzing non-causal \textsc{nlg} for non-English languages. This section will discuss relevant contributions to this work, mainly focusing on non-causal \textsc{nlg} and other Viterbi algorithm applications in \textsc{nlp}.

\subsection{Causality in generative transformer language models}
\label{sec:noncausal_related}

Transformers are unsupervised learners thanks to their self-attention mechanism \citep{attention-2017}, which controls the impact of the context on the model's output. The original transformer architecture is composed of an encoder and a decoder. While the encoder's attention is bidirectional, the decoder has a masked multi-head attention block that masks non-causal context and a bidirectional multi-head attention block that receives non-causal information from the encoder.

Although the encoder-decoder architecture is widely used in some \textsc{nlp} applications like machine translation \citep{Kawara2021, Nguyen2021}, other transformer-based models only use one of these two components. By excluding the encoder, we eliminate all non-causal contextual dependencies, thus using only the masked attention of the decoder. Currently, decoder-only transformers are the most effective task-agnostic \textsc{nlg} systems.

While open domain \textsc{nlg} is mainly causal, there are a few non-causal \textsc{nlg} solutions. Most non-causal \textsc{nlg} systems are focused on particular tasks such as speech recognition \citep{Chen2021, Wang2022}, style transfer and grammar correction \citep{Kaneko2020}, textual data augmentation \citep{Park2019}, and dialog systems \citep{Yu2021, Wang2023}.

Non-causal language models can also be trained for masked Language Modeling (\textsc{mlm}) \citep{Zeng2021}. \textsc{mlm} is an \textsc{nlg} task consisting of predicting masked words within a sentence. Some generative systems use bidirectional transformers trained on this task to recursively generate and fill masked tokens \citep{Shen2020}. As these can be filled in any location within the text, these models can produce text in a non-causal way.

Non-causal \textsc{nlg} strategies perform much worse in English than their causal counterparts \citep{Wang2019a}. However, to our knowledge, no prior research has been conducted on non-causal \textsc{nlg} in languages other than English. This work aims to evaluate whether bidirectional transformers trained on the \textsc{mlm} task could be successfully exploited in Spanish \textsc{nlg}.

\subsection{Viterbi algorithm in \textsc{nlp} }
The Viterbi algorithm \citep{Forney1973} is a dynamic programming algorithm used to find the most probable sequence of hidden states, known as the Viterbi path, given a sequence of observed events.

The Viterbi algorithm has been used in \textsc{nlp} applications such as speech recognition \citep{Jo2019,Raj2022}, in which the audio data is the observed sequence of events and the state space is defined by tokenized text strings, and in part-of-speech (\textsc{pos}) tagging \citep{Pattnaik2020}, in which the observed sequence is the given text and the state space is given by all possible \textsc{pos} tags. 

Even conceptually close, the Viterbi algorithm formulation proposed in this paper is different from that of those other \textsc{nlp} works. In our approach, instead of the tokenizer's vocabulary or \textsc{pos} tags, the Markov chain states are given by the initial phrase in all the possible different states of completion. The transition probabilities are computed using the bidirectional transformer language model described in Section \ref{sec:model_results}.

As previously said, our Viterbi algorithm-based approach explained in Section \ref{sec:method} is, to our knowledge, the first to estimate the optimal order for word generation in a language, contributing to the development of new non-causal \textsc{nlg} methodologies.