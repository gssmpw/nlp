\vspace{-5pt}
\section{Preliminaries}\label{sec:pre}

\begin{table}[t]
\setlength{\belowcaptionskip}{-.2\baselineskip}
  \caption{Symbols and Notations.}
  \vspace{-10pt}
  \label{tab:sym}
  \begin{tabular}{@{}cc@{}}
    \toprule
    Symbol &Definition\\
    \midrule
    $\G_1,\G_2$ &input networks\\
    $\mathcal{V}_1, \mathcal{V}_2$ & node sets of $\G_1$ and $\G_2$ \\
    $\mathcal{E}_1, \mathcal{E}_2$ & edge sets of $\G_1$ and $\G_2$ \\
    $\mathbf{A}_1,\mathbf{A}_2$ &adjacency matrices of $\G_1$ and $\G_2$\\
    $\mathbf{X}_1,\mathbf{X}_2$ &node attribute matrices of $\G_1$ and $\G_2$\\
    $\bm{\mu}_1,\bm{\mu}_2$ &probability measures \\
    $n_i,m_i$ & number of nodes/edges in $\G_i$\\
    $\mathcal{L}$ &the set of anchor node pairs\\
    \midrule
    $\mathbf{I},\mathbf{1}$ &an identity matrix and an all-one vector/matrix\\
    $\odot$ &Hadamard product\\
    $\langle\cdot,\cdot\rangle$ &inner product\\
    $\Pi$ &probabilistic coupling\\
    $[\cdot\|\cdot]$ & horizontal concatenation of vectors\\
    \bottomrule
  \end{tabular}
  \vspace{-10pt}
\end{table}

Table~\ref{tab:sym} summarizes the main symbols used throughout the paper. We use bold uppercase letters for matrices (e.g., $\mathbf{A}$), bold lowercase letters for vectors (e.g., $\mathbf{s}$), and lowercase letters for scalars (e.g., $\alpha$).
The transpose of $\mathbf{A}$ is denoted by the superscript $\T$ (e.g., $\mathbf{A}^\T$).
An attributed network with $n$ nodes is represented by $\G=(\mathbf{A},\mathbf{X})$ where $\mathbf{A}\in\mathbb{R}^{n\times n},\mathbf{X}\in\mathbb{R}^{n\times d}$ denote the adjacency matrix and node attribute matrix, respectively. We use $\mathcal{V}$ and $\mathcal{E}$ to denote the node and edge set of a graph, respectively. The semi-supervised attributed network alignment problem can be defined as follows:
\begin{definition}
\textit{Semi-supervised Attributed Network Alignment.}\\
\textbf{Given:} (1) two networks $\G_1=(\mathbf{A}_1,\mathbf{X}_1)$ and $\G_2=(\mathbf{A}_2,\mathbf{X}_2)$; (2) an anchor node set $\mathcal{L}=\{(x,y)|x\in\G_1,y\in\G_2\}$ indicating pre-aligned nodes pairs $(x,y)$.\\
\textbf{Output:} alignment/mapping matrix $\mathbf{S}\in\mathbb{R}^{n_1\times n_2}$, where $\mathbf{S}(x,y)$ indicates how likely node $x\in\G_1$ and node $y\in\G_2$ are aligned.
\end{definition}

\subsection{Embedding-based Network Alignment}
Embedding-based methods learn node embeddings by pulling positive node pairs together while pushing negative node pairs apart in the embedding space via ranking loss functions~\cite{ione, crossmna, bright, nextalign}. Specifically, given a set of anchor node pairs $\mathcal{L}$, the ranking loss can be generally formulated as~\cite{bright}:
\begin{equation}\label{eq:ranking}
    \begin{aligned}
    &\mathcal{J}_{\text{rank}}=\mathcal{J}_1+\mathcal{J}_2+\mathcal{J}_{\text{cross}}\\
        &\text{where }\left\{
        \begin{aligned}
            &\mathcal{J}_1=\sum\nolimits_{x\in\mathcal{L}\cap\G_1}\left(d(x,x_p)-d(x,x_n)\right)\\
            &  \mathcal{J}_2=\sum\nolimits_{y\in\mathcal{L}\cap\G_2}\left(d(y,y_p)-d(y,y_n)\right)\\
            &\mathcal{J}_{\text{cross}}=\sum\nolimits_{(x,y)\in\mathcal{L}}d(x,y)
        \end{aligned}
        \right.,
    \end{aligned}
\end{equation}
where $d(x,y)$ measures the distance between two node embeddings (e.g., $L_1$ norm), $x_p/y_p$ denotes the positive node w.r.t. $x/y$, and $x_n/y_n$ denotes the negative node w.r.t. $x/y$.
In the above equation, $\mathcal{J}_1,\mathcal{J}_2$ are intra-network loss pulling sampled positive nodes (e.g., similar/nearby nodes) together, while pushing sampled negative nodes (e.g., disimilar/distant nodes) far part.
$\mathcal{J}_{\text{cross}}$ is the cross-network loss, which aims to minimize the distance between anchor node pairs.
In general, the objective in Eq.~\eqref{eq:ranking} indirectly models the node relationship between two non-anchor nodes $(x',y')$ via a path through the anchor node pair $(x,y)$, i.e., $((x',x),(x,y),(y,y'))$.

\subsection{Optimal Transport}
OT has achieved great success in graph applications, such as network alignment~\cite{gwl,s-gwl,parrot,slotalign} and graph classification~\cite{qian2024reimagining,dong2020copt,zeng2023generative,zeng2024graph}.
Following a common practice~\cite{titouan2019optimal}, a graph can be represented as a probability measure supported on the product space of node attribute and structure, i.e., $\bm{\mu}=\sum_{i=1}^{n}\mathbf{h}(i)\delta_{\mathbf{A}(x_i),\mathbf{X}(x_i)}$, where $\mathbf{h}\in\Delta_{n}$ is a histogram representing the node weight and $\delta$ is the Dirac function. The fused Gromov-Wasserstein (FGW) distance is the sum of node pairwise distances based on node attributes and graph structure defined as~\cite{tam2019optimal}:
\begin{definition}\label{def:fgw} \textit{Fused Gromov-Wasserstein (FGW) distance.}\\
\textbf{Given:} (1) two graphs $\G_1=(\mathbf{A}_1,\mathbf{X}_1),\G_2=(\mathbf{A}_2,\mathbf{X}_2)$; (2) probability measures $\bm{\mu}_1,\bm{\mu}_2$ on graphs; (3) intra-network cost matrix $\mathbf{C}_1,\mathbf{C}_2$; (4) cross-network cost matrix $\mathbf{M}$.\\
\textbf{Output:} the FGW distance between two graphs $\textup{FGW}_{q,\alpha}(\G_1,\G_2)$
\vspace{-5pt}
\begin{equation}\label{eq:fgwd}
    \begin{aligned}
        &\min_{\mathbf{S}\in\Pi(\bm{\mu}_1,\bm{\mu}_2)}(1-\alpha)\sum_{x\in\G_1, y\in\G_2}\mathbf{M}^q(x,y)\mathbf{S}(x,y)\\
        &+ \alpha \!\!\!\!\!\sum_{x,x'\in\G_1\atop y,y'\in\G_2}\!\!\!\!|\mathbf{C}_1(x,x')-\mathbf{C}_2(y,y')|^q\mathbf{S}(x,y)\mathbf{S}(x',y').
    \end{aligned}
\end{equation}
\end{definition}
\vspace{-5pt}
The first term corresponds to the Wasserstein distance measuring cross-network node distances, and the second term is the Gromov-Wasserstein (GW) distance measuring cross-network edge distances. The hyperparameter $\alpha$ controls the trade-off between two terms, and $q$ is the order of the FGW distance, which is adopted as $q=2$ throughout the paper. The FGW problem aims to find an OT mapping $\mathbf{S}\in\Pi(\bm{\mu}_1, \bm{\mu}_2)$ that minimizes the sum of Wasserstein and GW distances, and the resulting OT mapping matrix $\mathbf{S}$ further serves as the soft node alignment.