\vspace{-5pt}
\section{Experiments}\label{sec:exp}

In this section, we carry out extensive experiments and analyses to evaluate the proposed \algname\ from the following aspects:
\begin{itemize}
    \item \textbf{Q1.} How effective is the proposed \algname?
    \item \textbf{Q2.} How efficient and scalable is the proposed \algname?
    \item \textbf{Q3.} How robust is \algname\ against graph noises?
    \item \textbf{Q4.} How do OT and embedding learning benefit each other?
    \item \textbf{Q5.} To what extent does the OT-based sampling strategy surpass the hand-crafted strategies?
\end{itemize}

\vspace{-3pt}
\subsection{Experiment Setup}
\noindent\textbf{Datasets.} Our method is evaluated on both plain and attributed networks summarized in Table~\ref{tab:datasets}. Detailed descriptions and experimental settings are included in Appendix~\ref{app:exp}\footnote{Code and datasets are available at \href{https://github.com/yq-leo/JOENA-WWW25}{github.com/yq-leo/JOENA-WWW25}.}.

\noindent\textbf{Baselines.} \algname\ is compared with the following three groups of methods, including (1) Consistency-based methods: IsoRank~\cite{isorank} and FINAL~\cite{final}, (2) Embedding-based methods: REGAL~\cite{regal}, DANA~\cite{dana}, NetTrans~\cite{nettrans}, BRIGHT~\cite{bright}, NeXtAlign~\cite{nextalign}, and WL-Align~\cite{wlalign}, and (3) OT-based methods: WAlign~\cite{walign}, GOAT~\cite{goat}, PARROT~\cite{parrot}, and SLOTAlign~\cite{slotalign}. To ensure a fair and consistent comparison, for all unsupervised baselines, we introduce the supervision information in the same way as \algname\ by concatenating the RWR scores w.r.t the anchor nodes with the node input features.

\noindent\textbf{Metrics.} We adopt two commonly used metrics Hits@$K$ and Mean Reciprocal Rank (MRR) to evaluate model performance. Specifically, given $(x, y)\in\mathcal{S}_{\text{test}}$ where $\mathcal{S}_{\text{test}}$ denotes the set of testing node pairs, if node $y\in \mathcal{G}_2$ is among the top-$K$ most similar nodes to node $u\in\mathcal{G}_1$, we consider it as a hit. Then, Hits@$K$ is computed by Hits@$K$=$\frac{\text{\# of hits}}{|\mathcal{S}_{\text{test}}|}$. MRR is computed by the average of the reciprocal of alignment ranking of all testing node pair, i.e., MRR = $\frac{1}{|\mathcal{S}_{\text{test}}|}\sum_{(x, y)\in\mathcal{S}_{\text{test}}}\frac{1}{\text{rank}(x, y)}$. 

\begin{table}[t]
    \centering
    \setlength\tabcolsep{1pt}
    \caption{Performance on plain network alignment.}
    \vspace{-5pt}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lccccccccccc}
        \toprule
        Dataset & \multicolumn{3}{c}{Foursquare-Twitter} & \multicolumn{3}{c}{ACM-DBLP} & \multicolumn{3}{c}{Phone-Email} \\
        \cmidrule(lr){1-1} \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
        Metrics & \multicolumn{1}{c}{Hits@1} & \multicolumn{1}{c}{Hits@10} & \multicolumn{1}{c}{MRR} & \multicolumn{1}{c}{Hits@1} & \multicolumn{1}{c}{Hits@10} & \multicolumn{1}{c}{MRR} & \multicolumn{1}{c}{Hits@1} & \multicolumn{1}{c}{Hits@10} & \multicolumn{1}{c}{MRR}\\
        \midrule
        \textsc{IsoRank} & 0.023 & 0.133 & 0.060 & 0.157 & 0.629 & 0.297 & 0.028 & 0.189 & 0.087 \\
        \textsc{FINAL} & 0.040 & 0.236 & 0.100 & 0.196 & 0.692 & 0.354 & 0.031 & 0.215 & 0.099 \\
        \midrule
       \textsc{DANA} & 0.042 & 0.160 & 0.082 & 0.343 & 0.559 & 0.316 & 0.033 & 0.206 & 0.095 \\
        \textsc{NetTrans} & 0.086 & 0.270 & 0.145 & 0.410 & 0.801 & 0.540 & 0.065 & 0.119 & 0.155 \\
        \textsc{BRIGHT} & 0.091 & 0.268 & 0.149 & 0.394 & 0.809 & 0.534 & 0.043 & 0.255 & 0.113 \\
        \textsc{NeXtAlign} & 0.101 & 0.279 & 0.158 & 0.459 & 0.861 & 0.594 & 0.063 & 0.424 & 0.195 \\
        \textsc{WL-Align} & \underline{0.253} & 0.343 & 0.285 & 0.542 & 0.781 & 0.629 & 0.121 & 0.409 & 0.214 \\
        \midrule
        \textsc{WAlign} & 0.077 & 0.258 & 0.135 & 0.342 & 0.794 & 0.481 & 0.046 & 0.308 & 0.131 \\
        \textsc{PARROT} & 0.245 & \underline{0.409} & \underline{0.304} & \underline{0.619} & \underline{0.912} & \underline{0.719} & \underline{0.323} & \underline{0.749} & \underline{0.469} \\ 
        \midrule
        \algname\ & \textbf{0.403} & \textbf{0.576} & \textbf{0.464} & \textbf{0.635} & \textbf{0.933} & \textbf{0.736} & \textbf{0.384} & \textbf{0.809} & \textbf{0.527} \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:exp_effect_plain}
    \vspace{-10pt}
\end{table}

\begin{table}[t]
    \centering
    \setlength\tabcolsep{1pt}
    \caption{Performance on attributed network alignment.}
    \vspace{-5pt}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccccccccccc}
        \toprule
        Dataset & \multicolumn{3}{c}{Cora1-Cora2} & \multicolumn{3}{c}{ACM(A)-DBLP(A)} & \multicolumn{3}{c}{Douban}\\
        \cmidrule(lr){1-1} \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
        Metrics & \multicolumn{1}{c}{Hits@1} & \multicolumn{1}{c}{Hits@10} & \multicolumn{1}{c}{MRR} & \multicolumn{1}{c}{Hits@1} & \multicolumn{1}{c}{Hits@10} & \multicolumn{1}{c}{MRR} & \multicolumn{1}{c}{Hits@1} & \multicolumn{1}{c}{Hits@10} & \multicolumn{1}{c}{MRR}\\
        \midrule
        \textsc{FINAL}    & 0.710 & 0.881 & 0.773 & 0.398 & 0.833 & 0.542 & 0.468 & 0.914 & 0.625\\
        \midrule
        \textsc{REGAL} & 0.511 & 0.591 & 0.542 & 0.501 & 0.725 & 0.579 & 0.099 & 0.274 & 0.153 \\
        \textsc{NetTrans} & 0.989 & 0.999 & 0.993 & 0.692 & 0.938 & 0.779 & 0.210 & 0.213 & 0.332\\
        \textsc{BRIGHT}    & 0.839 & 0.992 & 0.905 & 0.470 & 0.857 & 0.603 & 0.282 & 0.641 & 0.397\\
        \textsc{NeXtAlign} & 0.439 & 0.703 & 0.538 & 0.486 & 0.867 & 0.615 & 0.245 & 0.655 & 0.385\\
        \midrule
        \textsc{WAlign} & 0.824 & 0.997 & 0.901 & 0.377 & 0.779 & 0.501 & 0.236 & 0.533 & 0.341 \\
        PARROT    & \underline{0.996} & \textbf{1.000} & \underline{0.998} & \underline{0.721} & \underline{0.960} & \underline{0.806} & \underline{0.696} & \underline{0.981} & \underline{0.789}\\
        \textsc{SLOTAlign} & 0.985 & 0.997 & 0.990 & 0.663 & 0.879 & 0.740 & 0.486 & 0.762 & 0.582 & \\ 
        \midrule
        \algname\ & \textbf{0.999} & \textbf{1.000} & \textbf{0.999} & \textbf{0.767} & \textbf{0.967} & \textbf{0.839} & \textbf{0.761} & \textbf{0.986} & \textbf{0.851}\\
        \bottomrule
    \end{tabular}
    }
    \label{tab:exp_effect_attr}
    \vspace{-10pt}
\end{table}

\vspace{-3pt}
\subsection{Effectiveness Results}

We evaluate the alignment performance of \algname, and the results on plain and attributed networks are summarized in Table~\ref{tab:exp_effect_plain} and~\ref{tab:exp_effect_attr}, respectively.
Compared with consistency and embedding-based methods, \algname\ achieves up to 31\% and 22\% improvement in MRR over the best-performing baseline on plain and attributed network tasks, respectively, which indicates that \algname\ is capable of learning noise-reduced node mapping beyond local graph geometry and consistency principles thanks to the OT component.
Compared with OT-based methods, \algname\ achieves a significant outperformance compared with the best competitor PARROT~\cite{parrot} with up to 16\% and 6\% improvement in MRR on plain and attributed networks.
Such outperformance demonstrates the effectiveness of the transport costs encoded by learnable node embeddings. Moreover, the performance improvement over WAlign~\cite{walign} and SLOTAlign~\cite{slotalign} indicates that \algname\ successfully avoids embedding collapse thanks to the learnable transformation $g_\lambda$ on OT mapping and the resulting adaptive sampling strategy $\sn$.

\vspace{-3pt}
\subsection{Scalability Results}
\begin{figure}[ht]
  \vspace{-10pt}
  \includegraphics[width=0.8\linewidth, trim=0 0 0 50, clip]{figures/exp_runtime.jpg}
  \vspace{-5pt}
  \caption{Scalability results. \algname\ achieves the best scalability results with up to 20$\times$ speed-up in inference time and up to 5$\times$ scale-up in network size.}
  \label{fig:exp_runtime}
  \vspace{-10pt}
\end{figure}
We compare the scalability of the propose \algname\ with that of OT-based methods, including GOAT~\cite{goat}, PARROT~\cite{parrot}, and SLOTAlign~\cite{slotalign}. 
We record the inference time as the number of edges increases, and the results are shown in Figure~\ref{fig:exp_runtime}. For networks with 20,000 edges, \algname\ runs 20 times faster than SLOTAlign. Under 300-second running time limit, \algname\ can process networks 5 times the size of SLOTAlign. Besides, we observe that \algname\ runs slightly faster than the pure OT-based method PARROT.
For one thing, we attribute such slight improvement to the lightweight MLP for embedding learning, as PARROT requires hand-crafted embeddings that may be computationally-heavy.
For another, better cost design based on learnable embeddings also benefits the converegence of OT optimization, hence achieving faster computation.

\subsection{Robustness Results}
To show the robustness of the proposed \algname\ , we evaluate the performance of \algname\ under \textit{structural} and \textit{attribute} noise.

\subsubsection{Robustness against Structural Noises}
We first evaluate the robustness of \algname\ against structural (edge) noises. Specifically, for edge noise level $p$, we randomly flip $p\%$ entries in the adjacency matrix, i.e., randomly add/delete edges~\cite{slotalign}. Evaluations are conducted on plain Phone-Email network to eliminate potential interference from node attributes. The results are shown in Figure~\ref{fig:exp_noise}a.

Compared to other baselines, the performance of \algname\ consistently achieves the highest MRR in all cases.
More importantly, thanks to the direct modeling and noise-reduced property of OT, we observe a much slower degradation of the MRR when the noise level increases, validating the robustness of \algname\ against graph structural noises.
Furthermore, embedding-based methods without OT (i.e., WLAlign~\cite{wlalign}, NeXtAlign~\cite{nextalign}, BRIGHT~\cite{bright}) degrades much faster than methods with OT (i.e., \algname, PARROT~\cite{parrot}), demonstrating that embedding-based methods are more sensitive to structural noise due to indirect modeling.

\subsubsection{Robustness against Attribute Noises}
\begin{figure}[t]
  \includegraphics[width=\linewidth]{figures/exp_noise.png}
  \vspace{-20pt}
  \caption{Performance comparison of five alignment methods under different levels of structure and attribute noise.}
  \vspace{-5pt}
  \label{fig:exp_noise}
\end{figure}


We also evaluate the robustness of \algname\ against attribute noises. Specifically, for attribute level $p$, we randomly flip $p\%$ entries in the attribute matrix~\cite{trung2020adaptive}. The results are shown in Figure~\ref{fig:exp_noise}b.

Compared to baselines, the performance of \algname\ consistently achieves the best performance, as well as the mildest degradation when attribute noise level increases, demonstrating the robustness of \algname\ against node attribute noises.
Besides, the performance of embedding-based methods degrades more severely than \algname\ which further illuminates the deficiency of indirect modeling.


\subsection{Further Analysis}
\subsubsection{Mutual Benefits of OT and Embedding Learning}

To verify the mutual benefits of OT and embedding learning, we compare the performance of \algname\ against the following variants on three datasets: (1) \textsc{Emd} infers node alignments by node embeddings learned under the sampling strategy from BRIGHT~\cite{bright}; (2) \textsc{Emd(OT)} infers node alignments by node embeddings learned under our OT-based sampling strategy; (3) \text{OT} infers node alignments by the OT mapping with cost matrices based on RWR encoding; (4) \algname\ infers node alignments by OT mapping with learnable cost matrices; (5) \textsc{OT$\odot$Emb} infers node alignments by the Hadamard product of OT mapping and the inner product of node embeddings; (6) \textsc{OT+Emb} infers node alignments by the sum of OT mapping and the inner product of node embeddings.

\begin{table}[t]
    \centering
    \setlength\tabcolsep{1pt}
    \caption{Mutual benefits of embedding and OT learning}
    \vspace{-5pt}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lccccccccc}
        \toprule
        Dataset & \multicolumn{3}{c}{Foursquare-Twitter} & \multicolumn{3}{c}{ACM-DBLP} & \multicolumn{3}{c}{Phone-Email} \\
        \cmidrule(lr){1-1} \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
        Metrics & \multicolumn{1}{c}{Hits@1} & \multicolumn{1}{c}{Hits@10} & \multicolumn{1}{c}{MRR} & \multicolumn{1}{c}{Hits@1} & \multicolumn{1}{c}{Hits@10} & \multicolumn{1}{c}{MRR} & \multicolumn{1}{c}{Hits@1} & \multicolumn{1}{c}{Hits@10} & \multicolumn{1}{c}{MRR}\\
        \midrule
        \textsc{Emb} & 0.079 & 0.244 & 0.134 & 0.401 & 0.798 & 0.534 & 0.063 & 0.358 & 0.164 \\
        \textsc{Emb(OT)} & \textbf{0.090} & \textbf{0.255} & \textbf{0.140} & \textbf{0.406} & \textbf{0.807} & \textbf{0.538} & \textbf{0.078} & \textbf{0.373} & \textbf{0.173}\\
        \midrule
        \textsc{OT} & 0.243 & 0.407 & 0.298 & 0.600 & 0.916 & 0.707 & 0.224 & 0.581 & 0.343\\
        \textsc{\algname} & \textbf{0.403} & \textbf{0.576} & \textbf{0.464} & \textbf{0.635} & \textbf{0.933} & \textbf{0.736} & \textbf{0.384} & \textbf{0.809} & \textbf{0.527} \\
        \midrule
        \textsc{OT $\odot$ Emb} & 0.243 & 0.407 & 0.297 & 0.601 & 0.916 & 0.707 & 0.224 & 0.593 & 0.337\\
        \textsc{OT $+$ Emb} & 0.244 & 0.408 & 0.299 & 0.600 & 0.917 & 0.707 & 0.226 & 0.583 & 0.345\\
        \bottomrule
    \end{tabular}
    }
    \label{tab:exp_mutual}
    \vspace{-15pt}
\end{table}

The results are shown in Table~\ref{tab:exp_mutual}. Firstly, we observe a consistent outperformance of \textsc{Emb(OT)} compared to $\textsc{Emb}$, showing that the proposed OT-based sampling strategy improves the quality of node embeddings compared to existing sampling strategies. Besides, comparing \textsc{OT} to \algname, without learnable cost matrices, \textsc{OT} drops up to 16\% in Hits@1 compared to \algname, indicating that the cost design on learnable node embeddings improves the performance of OT optimization by a significant margin. Furthermore, we compare the performance of \algname\ to \textsc{OT$\odot$Emb} and \textsc{OT+Emb}, both of which naively integrate the OT and embedding alignments learned separately. It is shown that both \textsc{OT$\odot$Emb} and \textsc{OT+Emb} achieves similar performance as $\text{OT}$ and outperforms \textsc{Emb}. For one thing, this suggests that the outperformance of \algname\ largely attributes to the OT alignment, which provides a more denoised alignment compared with embedding alignment.
For another, naively combining the alignment matrices of embedding or OT-based method at the final stage hardly improves the alignment quality, and it is necessary to combine both components during training.

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\textwidth, trim = 15 0 50 50, clip]{figures/exp_lambda_pe.jpg}
        \caption{Phone-Email}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\textwidth, trim = 15 0 50 50, clip]{figures/exp_lambda_ft.jpg}
        \caption{Foursquare-Twitter}
    \end{subfigure}
    \vspace{-5pt}
    \caption{MRR with different $\lambda$. Our learned $\lambda$ consistently achieves the best MRR on both datasets.}
    \label{fig:exp_lambda}
    \vspace{-15pt}
\end{figure}

\subsubsection{OT-based Sampling Strategy}
We also carry out studies on the effectiveness of the OT-based sampling strategy $g_\lambda(\mathbf{S})$.
As shown in Figure~\ref{fig:exp_lambda}, we report the MRR under different $\lambda$ with the learned $\lambda$ annotated.
It is shown that \algname\ achieves the best performance under the learned $\lambda$.
Besides, we observe a significant performance drop when $\lambda$ is not properly selected.
This is due to the fact that when $\lambda$ is too small/large, most pairs will be sampled as positive/negative pairs exclusively, which further leads to embedding collapse.
To validate this point, we visualize how the embedding space changes along optimization.
As shown in Figure~\ref{subfig:exp_emb_vis_0}, when setting $\lambda=0$, MRR gradually decreases and the learned embeddings collapse into an identical point along optimization.
On the contrary, as shown in Figure~\ref{subfig:exp_emb_vis_opt}, \algname\ is able to learn the optimal $\lambda$, under which, MRR gradually increases and node embeddings are well separated in the embedding space.
