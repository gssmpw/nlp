\vspace{-5pt}
\section{Methodology}\label{sec:method}

In this section, we present the proposed \algname. We first analyze the mutual benefits between embedding and OT-based methods in Section~\ref{subsec:analysis}. Guided by such analysis, a unified framework named \algname\ is proposed for network alignment in Section~\ref{subsec:overview}. We further present the unified model training schema in Section~\ref{subsec:train}, followed by convergence and complexity analysis of \algname\ in Section~\ref{subsec:proof}.

\vspace{-2pt}
\subsection{Mutual Benefits of Embedding and OT}\label{subsec:analysis}
\subsubsection{OT-Empowered Embedding Learning}
The success of ranking loss largely depends on sampled positive and negative node pairs, i.e., $(x,x_p),(x,x_n),(y,y_p),(y,y_n)$ in Eq.~\eqref{eq:ranking}, through which cross-network node pair relationships can be modeled.
To provide a better sampling strategy, the OT mapping improves the embedding learning from two aspects: \textit{direct modeling} and \textit{robustness}.
First (\textit{direct modeling}), while embedding-based methods model cross-network node relationships via an indirect path (see Figure~\ref{fig:sampling} for an example.) sampled by hand-crafted strategies, the OT mapping directly models such cross-network relationships, identifying positive and negative node pairs more precisely.
Second (\textit{robustness}), in contrast to the noisy embedding alignment, thanks to the marginal constraints in Eq.~\eqref{eq:fgwd}, the resulting OT mapping is noise-reduced~\cite{parrot,tam2019optimal}, where each node only aligns with very few nodes.
Therefore,  OT-based sampling strategy can be robust to graph noises.

\subsubsection{Embedding-Empowered OT Learning}
The success of OT-based alignment methods largely depend on the cost design, i.e. $\mathbf{C}_1$, $\mathbf{C}_2$, and $\mathbf{M}$ in Eq.~\eqref{eq:fgwd}, which is often hand-crafted in existing works.
To achieve better cost design, embedding learning benefits OT learning from two aspects: \textit{generalization} and $\textit{effectiveness}$.
For one thing (\textit{generalization}), building transport cost upon learnable embeddings opens the door for end-to-end training paradigm, thus, the OT framework can be generalized to different graphs without extensive parameter tuning. For another (\textit{effectiveness}), neural networks generate more powerful node embeddings via deep transformations, enhancing the cost design for OT optimization.

\vspace{-2pt}
\subsection{Model Overview}\label{subsec:overview}

\begin{figure}[t]
  \includegraphics[width=\linewidth]{figures/model_new.png}
  \caption{An overview of \algname, including RWR encoding, embedding learning and OT optimization. RWR encoding and raw node attributes are processed by a shared MLP, supervised by the ranking loss based on the OT-based sampling strategy. The OT mapping is optimized via cost matrices derived from the learned embeddings, further transformed into a sampling strategy by the learnable transformation $g_\lambda$.
  }
  \vspace{-10pt}
  \label{fig:model}
\end{figure}

The overall framework of \algname\ is given in Figure~\ref{fig:model}, which can be divided into three parts: (1) \textit{RWR encoding} for structure learning, (2) \textit{embedding learning} via multi-level ranking loss with OT-based sampling, (3) $\textit{OT optimization}$ with learnable transport cost.

Positional information plays a pivotal role in network alignment~\cite{bright,parrot}, but most of the GNN architectures fall short in capturing such information for alignment~\cite{you2019position}.
Therefore, we explicitly encode positional information by conducting random walk with restart (RWR)~\cite{tong2006fast}.
By regarding a pair of anchor nodes $(x, y)\in\mathcal{L}$ as identical in the RWR embedding space, we simultaneously perform RWR w.r.t. $x\in\G_1$ and $y\in\G_2$ to construct a unified embedding space, where the RWR score vectors $\mathbf{r}_x\in\mathbb{R}^{n_1}$ and $\mathbf{r}_y\in\mathbb{R}^{n_2}$ can be obtained by~\cite{tong2006fast,bright}
\begin{equation}\label{eq:rwr}
    \mathbf{r}_x = (1-\beta)\mathbf{W}_1\mathbf{r}_x + \beta\mathbf{e}_x,
    ~~~\mathbf{r}_y = (1-\beta)\mathbf{W}_2\mathbf{r}_y + \beta\mathbf{e}_y,
\end{equation}
where $\beta$ is the restart probability, $\mathbf{W}_i=\left(\mathbf{D}_i^{-1}\mathbf{A_i}\right)^\T$ is the transpose of the row-normalized adjacency matrix, $\mathbf{D}_i$ is the diagonal degree matrix of $\G_i$, and $\mathbf{e}_x,\mathbf{e}_y$ are one-hot encoding vectors with $\mathbf{e}_x(x)=1$ and $\mathbf{e}_y(y)=1$.
The concatenation of RWR vectors w.r.t. different anchor nodes $\mathbf{R}_i\in\mathbb{R}^{n_i\times|\mathcal{L}|}$, together with node attribute matrices $\mathbf{X}_i$, i.e., $[\mathbf{R}_i\|\mathbf{X}_i]$, serve as the input for embedding learning.

To learn powerful node embeddings, we train a shared two-layer multi-layer perceptron (MLP) with residual connections $f_\theta$ via a multi-level ranking loss.
To address the limitations of hand-crafted sampling strategies, we apply a simple yet effective transformation $g_\lambda$ on the OT mapping $\mathbf{S}$ to obtain an adaptive sampling strategy $g_\lambda(\mathbf{S})$. Then, the sampled node and edge pairs based on $g_\lambda(\mathbf{S})$ are utilized for learning output embeddings $\mathbf{E}_1$ and $\mathbf{E}_2$, supervised by the multi-level ranking loss.

To improve OT optimization, we construct the cross-network cost matrix $\mathbf{M}$ and intra-network cost matrices $\mathbf{C}_1, \mathbf{C}_2$ based on output embeddings $\mathbf{E}_1$ and $\mathbf{E}_2$ of the MLP as follows
\vspace{-2pt}
\begin{equation}\label{eq:cost}
    \mathbf{M} = e^{-\mathbf{E}_1\mathbf{E}_2^{\T}},~~~\mathbf{C}_i = e^{-\mathbf{E}_i\mathbf{E}_i^{\T}}\odot\mathbf{A}_i,
\end{equation}
\vspace{-2pt}
where $\mathbf{M}(x,y)$ is the cross-network node distance between $x\in\G_1,y\in\G_2$, and $\mathbf{C}_i(a,b)$ is the intra-network node distance between $a,b\in\G_i$\footnote{We use $\mathbf{C}_i$ to encode edge information in two graphs with $\mathbf{C}_i(a,b)=0, \forall (a,b)\notin\mathcal{E}_i$.}. Afterwards, the FGW distance in Eq.~\eqref{eq:fgwd} can be efficiently solved via the proximal point method~\cite{s-gwl,parrot}, whose output OT mapping $\mathbf{S}$ indicates the node alignment between two graphs.

For model training, we propose an objective function which, as we will show in the next subsection, unifies OT optimization and embedding learning as follows:
% \vspace{-2pt}
\begin{equation}\label{eq:object}
    \begin{aligned}
        &\mathop{\min}\limits_{\mathbf{S}\in\Pi(\bm{\mu}_1,\bm{\mu}_2), \lambda,\theta}\mathcal{J}(\mathbf{S,\lambda,\theta})=(1-\alpha)\underbrace{\!\!\!\!\!\!\sum_{x\in\G_1, y\in\G_2}\!\!\!\!\!\!\mathbf{M}(x,y;\theta)\sn(x,y;\lambda)}_{\text{Wasserstein/node-level loss}}\\
        &+ \alpha \!\!\!\!\!\underbrace{\sum_{x,x'\in\G_1\atop y,y'\in\G_2}\!\!\!\!|\mathbf{C}_1(x,x';\theta)-\mathbf{C}_2(y,y';\theta)|^2\sn(x,y;\lambda)\sn(x',y';\lambda)}_{\text{GW/edge-level loss}},
    \end{aligned}
\end{equation}
\vspace{-2pt}
where $\mathbf{S}$ is the OT mapping, $\theta$ is the set of learnable parameters in the MLP model $f_\theta$, $\sn$ is the adaptive sampling strategy after transformation (i.e., $\sn=g_\lambda(\mathbf{S})$) , and $\alpha$ is a hyper-parameter that controls the relative importance between Wasserstein distance/node-level ranking loss and GW distance/edge-level ranking loss. Through alternating optimization, both OT mapping $\mathbf{S}$ and node embeddings $\mathbf{E}_1,\mathbf{E}_2$ can be optimized in a mutually beneficial manner. The overall algorithm is summarized in Algorithm~\ref{algo:jeona} in Appendix~\ref{app:algo}.

\vspace{-2pt}
\subsection{Unified Model Training}\label{subsec:train}
In this subsection, we present the model training framework under a unified objective function. Through a simple yet effective transformation, the FGW distance and multi-level ranking loss are combined into a single objective (Subsection~\ref{subsection:unifyingloss}), which can be efficiently optimized using an alternating optimization scheme with guaranteed convergence (Subsection~\ref{subsection:modeltraining}).

\vspace{-2pt}
\subsubsection{Unifying FGW Distance and Multi-level Ranking Loss}\label{subsection:unifyingloss}

The FGW distance is a powerful objective for network alignment, and has been adopted by several works~\cite{gwl,got2,slotalign,combalign} to supervise embedding learning.
In general, based on the Envelop theorem~\cite{afriat1971theory}, existing methods based on the FGW objective~\cite{gwl,got2,slotalign,combalign} optimize the cost matrices under the fixed OT mapping $\mathbf{S}$, whose gradients further guide the learning of feature encoder $f_\theta$.
However, due to the non-negativity of $\mathbf{S}$, directly minimizing FGW distance leads to trivial solutions where cost matrices $\mathbf{M},\mathbf{C}_1,\mathbf{C}_2$ become zero matrices, hence leading to embedding collapse illustrated in Proposition~\ref{prop:collapse}.
\begin{proposition}
{\normalfont\textsc{(Embedding Collapse).}}
Given two networks $\G_1,\G_2$, directly optimizing feature encoder $f_\theta$ with the FGW distance leads to embedding collapse, that is $\mathbf{E}_1(x)=\mathbf{E}_2(y), \forall x\in\G_1,y\in\G_2$, where $\mathbf{E}_1=f_\theta(\G_1),\mathbf{E}_2=f_\theta(\G_2)$.
\label{prop:collapse}
\end{proposition}
The proof can be be found in Appendix~\ref{app:proof}. In general, due to the non-negativity of FGW distance~\cite{titouan2019optimal}, its minimal value of zero is achieved by projecting all nodes to identical embeddings, hence significantly degrading embeddings' discriminating power.

To alleviate embedding collapse, we propose a transformation $g_\lambda:\mathbb{R}_{\geq 0}^{n_1\times n_2}\to\mathbb{R}^{n_1\times n_2}$ to transform the non-negative OT mapping $\S$ into an adaptive node sampling matrix $\sn=g_\lambda(\mathbf{S})$ to discern the positive samples from the negative ones together with sampling weights.
In this work, we adopt $g_\lambda(\mathbf{S})=\mathbf{S}-\lambda\mathbf{1}_{n_1\times n_2}$, where $\lambda$ is a learnable transformation parameter.
The rationale behind such design is to find the optimal threshold $\lambda$ to distinguish between positive and negative pairs automatically.
Moreover, as the absolute value of $\S_n(x,y)$ indicates the certainty of sampling node pair $(x,y)$ as positive/negative pairs, it helps distinguish easy and hard samples for the ranking loss.
Equipped with such adaptive sampling matrix $\sn$, we quantitatively attribute the effectiveness of FGW distance to the following two aspects: \textit{node-level ranking} and \textit{edge-level ranking}.

\noindent\textbf{Wasserstein Distance as Node-Level Ranking Loss.}
Equipped with the sampling strategy $\sn$, the Wasserstein distance term can be reformulated as a node-level ranking loss as follows
\vspace{-2pt}
\begin{equation}\label{eq:w_rank}
    \begin{aligned}
        \mathcal{J}_{\text{w}}&=\!\!\!\sum_{(x,y)\in\mathcal{V}_1\times \mathcal{V}_2}\!\!\!\mathbf{M}(x,y)\sn(x,y)\\
        &=\!\!\!\sum_{(x,y_p)\in\mathcal{R}^+} \!\!\!\!\!\mathbf{M}(x, y_p)|\sn(x, y_p)|-\!\!\!\!\!\!\!\sum_{(x,y_n)\in\mathcal{R}^-}\!\!\!\!\!\mathbf{M}(x, y_n)|\sn(x, y_n)|\\
        \text{wh}&\text{ere } \mathcal{R}^+\!\!\!=\!\{(x, y_p) | \sn(x, y_p) \!\!\geq\! 0\},\mathcal{R}^-\!\!\!\!=\!\{(x, y_n) | \sn(x, y_n) \!\!<\! 0\}.
    \end{aligned}
\end{equation}
\vspace{-2pt}
$\mathcal{R}^+$ and $\mathcal{R}^-$ are the sets of positive and negative node pairs, respectively. Therefore, Eq.~\eqref{eq:w_rank} can be viewed as a weighted ranking loss function at the \textit{node} level, where the sign of $\sn(x,y)$ is used to distinguish between positive and negative node pairs and the sampling weight $|\sn(x,y)|$ indicates the certainty of the sampled positive/negative node pair. For example, $(x,y)$ is regarded as an uncertain pair and should contribute little to the ranking loss if $\mathbf{S}(x,y)$ is close to the threshold $\lambda$. Similarly, if $\mathbf{S}(x,y)$ is far away from $\lambda$, the relationship between $(x,y)$ is more certain and should contribute more to the ranking loss. Therefore, $\sn$ directly models \textit{all} cross-network pairs $(x,y)\in\mathcal{V}_1\times\mathcal{V}_2$ with noise-reduced certainty values.
To this point, we provide a unified view of the Wasserstein distance and the node-level ranking loss.

Another limitation of the existing ranking loss is that it only considers node relationships while ignores the modeling of edge relationships, hence may fall short in preserving graph structure in the node embedding space~\cite{dlna,slotalign}. To address this issue, we introduce a novel ranking loss function at edge-level and unify it with the GW distance.

\noindent\textbf{Gromov-Wasserstein Distance as Edge-Level Ranking Loss.}
The GW distance term can be reformulated as an edge-level ranking loss as follows,

\vspace{-10pt}
\begin{equation}\label{eq:gw_rank}
\begin{aligned}
    \mathcal{J}_{\text{gw}}=&\sum_{\substack{x, x'\in\mathcal{G}_1,\atop y,y'\in\mathcal{G}_2}}|\mathbf{C}_1(x, x')-\mathbf{C}_2(y, y')|^2\sn(x, y)\sn(x', y')\\
    =&\sum_{(e_{x,x'},e_{y_p,y_p'})\in\mathcal{T}^+}d_e(e_{x,x'},e_{y_p,y_p'})|\se(e_{x,x'},e_{y_p,y_p'})| -\\
    &\sum_{(e_{x,x'},e_{y_n,y_n'})\in\mathcal{T}^-}d_e(e_{x,x'},e_{y_n,y_n'})|\se(e_{x,x'},e_{y_n,y_n'})|\\
    \text{where }&\left\{
    \begin{aligned}
        &d_e(e_{x,x'},e_{y,y'})=|\mathbf{C}_1(x,x')-\mathbf{C}_2(y,y')|^2\\
        &\se(e_{x,x'},e_{y,y'})=\sn(x,y)\sn(x',y')\\
        &\mathcal{T}^+\!\!\!=\!\{(e_{x,x'},e_{y_p,y_p'}) |\se(e_{x,x'},e_{y_p,y_p'}) \!\!\geq\! 0\}\\
        &\mathcal{T}^-\!\!\!=\!\{(e_{x,x'},e_{y_n,y_n'}) | \se(e_{x,x'},e_{y_n,y_n'}) \!\!<\! 0\}
    \end{aligned},
    \right.
\end{aligned}
\end{equation}
\noindent where $e_{x,x'}$ is the edge between $x$ and $x'$, $d_e$ measures the distance between two edges, and $\mathcal{T}^+,\mathcal{T}^-$ are the sets of positive and negative edge pairs sampled by the edge sampling strategy $\se$.
Similar to Eq.~\eqref{eq:w_rank}, Eq.~\eqref{eq:gw_rank} is a weighted ranking loss at the \textit{edge} level, where the sign of $\se(e_{x,x'},e_{y,y'})$ distinguishes between positive and negative edge pairs and the sampling weight $|\se(e_{x,x'},e_{y,y'})|$ indicates the certainty of the sampled positive/negative edge pair.
From the view of line graph~\cite{dlna}, where edges in the original graph are mapped into nodes in the line graph and vice versa, the edge ranking loss in the original graph can be interpreted as the node ranking loss in the corresponding line graph.
% Equipped with ranking loss at both node and edge level, Eq.~\eqref{eq:object} is able to preserve multi-level graph structure in the embedding space.

\subsubsection{Optimization}~\label{subsection:modeltraining}
Combining the node-level ranking loss (Wasserstein distance) and edge-level ranking loss (GW distance) gives the unified optimization objective of \algname\ for both embedding learning and OT optimization as Eq.~\eqref{eq:object}. To optimize this objective, we adopt an alternating optimization scheme where the parameters of feature encoder $f_\theta$, transformation parameter $\lambda$, and OT mapping $\mathbf{S}$ are optimized iteratively.

Specifically, for the $k$-th iteration, we first fix the feature encoder $f_\theta^{(k)}$ and the transformation parameter $\lambda^{(k)}$, and optimize Eq.~\eqref{eq:object} w.r.t $\mathbf{S}$ by the proximal point method~\cite{s-gwl}. 
Due to the non-convexity of the objective, proximal point method decomposes the non-convex problem into a series of convex subproblems plus an additional Bregman divergence between two consecutive solutions, where each subproblem can be formulated as follows

\vspace{-10pt}
\begin{equation}
    \mathbf{S}^{(t+1)} = \mathop{\arg\min}\limits_{\mathbf{S}\in\Pi(\bm{\mu}_1, \bm{\mu}_2)} \mathcal{J}(\mathbf{S};\lambda^{(k)}, \theta^{(k)}) + \gamma_p \text{Div}(\mathbf{S}\|\mathbf{S}^{(t)}),
\label{eq:ot_cpot}
\end{equation}
\noindent where $t$ is the number of proximal point iteration, $\gamma_p$ is the weight for the proximal operator, and $\text{Div}$ is the Bregman divergence between two OT mappings. Then, the resulting subproblem in Eq.~\eqref{eq:ot_cpot} can be transformed into an entropy-regularized OT problem as
\begin{equation}
\begin{aligned}
&\min_{\mathbf{S}\in\Pi(\boldsymbol{\mu}_1,\boldsymbol{\mu}_2)}\underbrace{\langle \mathbf{C}_{\text{total}}^{(t)}, \mathbf{S}\rangle + \gamma_p\langle\log\mathbf{S}, \mathbf{S}\rangle}_{\text{entropy-regularized OT}} - \underbrace{\left<(1-\alpha)\mathbf{M}+\alpha\mathbf{L}^{(t)}_{\text{gw}}, \lambda\right>}_{\text{constant}}\\
&\text{where }\left\{
\begin{aligned}
    &\mathbf{C}_{\text{total}}^{(t)}=(1-\alpha)\mathbf{M}+ \alpha\mathbf{L}^{(t)}_{\text{gw}}-\gamma_p\log\mathbf{S}^{(t)}\\
    &\mathbf{L}_{\text{gw}}^{(t)}=\mathbf{C}_1^2\sn^{(t)}\mathbf{1}_{n_2\times n_2} +\mathbf{1}_{n_1\times n_1}\sn^{(t)}\mathbf{C}_2^{2^\T} -2\mathbf{C}_1\sn^{(t)}\mathbf{C}_2^\T\\
    &\sn^{(t)}=\mathbf{S}^{(t)}-\lambda^{(k)}\mathbf{1}_{n_1\times n_2}
\end{aligned}.
\right.
\end{aligned}
\label{eq:ot_cpot_final}
\end{equation}

Note that $\mathbf{S}^{(t)}$ is the OT mapping from last proximal point iteration and remains fixed in the above equation. Therefore, the objective function of each proximal point iteration in Eq.~\eqref{eq:ot_cpot_final} is essentially an entropy-regularized OT problem with a fixed transport cost $\mathbf{C}_{\text{total}}^{(t)}$ minus a constant term that does not affect the optimization. Eq.~\eqref{eq:ot_cpot_final} can be solved efficiently by the Sinkhorn algorithm~\cite{peyre2019computational}.

Then, we fix the feature encode $f_\theta^{(k)}$ and OT mapping $\mathbf{S}^{(k+1)}$, and optimize Eq.~\eqref{eq:object} w.r.t the transformation parameter $\lambda$. Since the objective function is quadratic w.r.t. $\lambda$, the closed-form solution for $\lambda^{(k+1)}$ can be obtained by setting $\partial\mathcal{J}/\partial{\lambda}=0$ as follows
\begin{equation}\label{eq:opt_lambda}
    \small
    \begin{aligned}
    &\lambda^{(k+1)}=\frac{(1-\alpha)\mathcal{K}_1 +\alpha\mathcal{K}_2}{2\alpha\mathcal{K}_3}\\
    &\text{where }\left\{
        \begin{aligned}
            &\mathcal{K}_1=\bignegspace\sum_{x\in\G_1,y\in\G_2}\bignegspace\mathbf{M}(x,y;\theta^{(k)})\\
            &\mathcal{K}_2=\bignegspace\bignegspace\sum\limits_{\substack{x,x'\in\G_1\\y,y'\in\G_2}} \bignegspace\bignegspace d_e\!\left(\!e_{x,x'},e_{y,y'};\theta^{(k)}\!\right)\!\left(\mathbf{S}^{(k+1)}\!(x,y)\!+\!\mathbf{S}^{(k+1)}\!(x',y')\!\right)\\
            &\mathcal{K}_3=\bignegspace\bignegspace\sum\limits_{\substack{x,x'\in\G_1\\y,y'\in\G_2}} \bignegspace\bignegspace d_e\left(e_{x,x'},e_{y,y'};\theta^{(k)}\right)\\
        \end{aligned}
        \right.
    \end{aligned}
\end{equation}

Finally, to optimize the feature encoder $f_\theta$, we fix the transformation parameter $\lambda^{(k+1)}$ and the OT mapping $\mathbf{S}^{(k+1)}$ to optimize Eq.~\eqref{eq:object} w.r.t $\theta$ via stochastic gradient descent (SGD), that is
\begin{equation}\label{eq:opt_sgd}
    \theta^{(k+1)}={\arg\min}_{\theta}\mathcal{J}(\theta;\mathbf{S}^{(k+1)},\lambda^{(k+1)}).
\end{equation}

As we will show later, by iteratively applying Eq.~\eqref{eq:ot_cpot}-\eqref{eq:opt_sgd}, the objective function in Eq.~\eqref{eq:object} converges under the alternating optimization scheme.
Besides, it is worth noting that alternating optimization is only used for model training, while model inference only requires one-pass, i.e., the forward pass of MLP and the proximal point method for OT optimization, allowing \algname\ to scale efficiently to large networks.

\vspace{-5pt}
\subsection{Proof and Analysis}\label{subsec:proof}
In this subsection, we provide theoretical analysis of the proposed \algname. Without loss of generality, we assume that graphs share comparable numbers of nodes (i.e., $\mathcal{O}(n_1)\approx\mathcal{O}(n_2)\approx\mathcal{O}(n)$) and edges (i.e., $\mathcal{O}(m_1)\approx\mathcal{O}(m_2)\approx\mathcal{O}(m)$).
We first provide the convergence analysis of \algname, followed by complexity analysis.

\begin{theorem}
    {\normalfont \textsc{(Convergence of \algname)}} The unified objective for \algname\ in Eq.~\eqref{eq:object} is non-increasing and converges along the alternating optimization.
\label{theo:convergence}
\end{theorem}

\begin{proposition}\label{prop:complexity}
{\normalfont (\textsc{Complexity of \algname})} The overall time complexity of \algname\ is $\mathcal{O}\left(KTmn+KTNn^2\right)$ at the training phase and $\mathcal{O}\left(Tmn+TNn^2\right)$ at the inference phase, where $K, T, N$ denote the number of iterations for alternating optimization, proximal point iteration, and Sinkhorn algorithm, respectively.
\end{proposition}

All the proofs can be found in Appendix~\ref{app:proof}.
In general, the alternating optimization scheme generates a series of non-increasing objective functions with a bounded minimum hence achieving guaranteed convergence.
In addition, as we can see, \algname\ achieves fast inference with linear complexity w.r.t the number of edges and quadratic complexity w.r.t. the number of nodes.