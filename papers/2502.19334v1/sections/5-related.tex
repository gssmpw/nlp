\vspace{-5pt}
\section{Related Works}\label{sec:related}
\vspace{-3pt}
\subsection{Network Alignment}
Traditional network alignment methods are often built upon alignment consistency principles.
IsoRank~\cite{isorank} conducts random walk on the product graph to achieve topological consistency. FINAL~\cite{final} interprets IsoRank as an optimization problem and introduces consistency at attribute level to handle attributed network alignment.
% MOANA~\cite{moana} aligns networks at multiple granularities to achieve better scalability.
% However, the consistency assumption only considers node relationships within a local neighborhood while ignoring global graph geometry~\cite{parrot}. 
Another line of works \cite{li2022unsupervised,wang2023networked,yan2022dissecting} learn informative node embeddings in a unified space to infer alignment.
REGAL~\cite{regal} conducts matrix factorization on cross-network similarity matrix for node embedding learning.
DANA~\cite{dana} learns domain-invariant embeddings for network alignment via adversarial learning.
% NetTrans~\cite{nettrans} aligns networks based on nonlinear network transformation.
BRIGHT~\cite{bright} bridges the consistency and embedding-based alignment methods, and NeXtAlign~\cite{nextalign} further balances between the alignment consistency and disparity by crafting the sampling strategies.
WL-Align~\cite{wlalign} utilizes cross-network Weisfeiler-Lehman relabeling to learn proximity-preserving embeddings. 
% Nevertheless, the success of embedding-based methods relies heavily on hand-crafted sampling strategies and are sensitive to graph noises.
More related works on network alignment are reviewed in~\cite{du2021new}.

\vspace{-3pt}
\subsection{Optimal Transport on Graphs}
OT has recently gained increasing attention in graph mining and network alignment, whose effectiveness often depends on the pre-defined cost function restricted to specific graphs.
% The key idea is to represent graphs as distributions over the node sets and minimize the total transportation distance based on cost functions defined over the two distributions.
% However, the effectiveness of most OT-based alignment methods depend largely on the pre-defined cost function restricted to specific graphs.
For example, \cite{got, walign, fgot,yan2024trainable} represent graphs as distributions of filtered graph signals, focusing on one specific graph property, while other cost designs are mostly based on node attributes~\cite{got2} or graph structures~\cite{goat}. PARROT~\cite{parrot} integrates various graph properties and consistency principles via a linear combination, but requires arduous parameter tuning.
More recent works combine both embedding and OT-based alignment methods.
% to supervise embedding learning for better cost design.
GOT~\cite{got2} adopts a deep model to encode transport cost. GWL~\cite{gwl} learns graph matching and node embeddings jointly in a GW learning framework. SLOTAlign~\cite{slotalign} utilizes a parameter-free GNN model to encode the GW distance between two graph distributions.
CombAlign~\cite{combalign} further proposes to combine the embeddings and OT-based alignment via an ensemble framework.

\vspace{-3pt}
\subsection{Graph Representation Learning}
Representation learning gained increasing attention in analyzing complex systems with applications in trustworthy ML~\cite{liu2024aim,yoo2024ensuring,fu2023privacy,bao2024adarc}, drug discovery \cite{wei2022impact,wu2023risk,zhang2024clinical,sui2024cancer} and recommender systems~\cite{liu2024collaborative,zeng2024interformer,wei2024towards}.
Early approaches \cite{perozzi2014deepwalk,grover2016node2vec} utilize random walks and process graphs as sequences by a skip-gram model. \cite{Hamilton2017Inductive,zeng2019graphsaint} sample fixed-size neighbors for better scalability to large graphs. More recent studies~\cite{huang2018adaptive,yan2024reconciling} focus on adaptive and unified sampling strategies that benefit various graphs.
Based on these strategies, graph contrastive learning\cite{velivckovic2018deep,jing2022coin,jing2024sterling,zheng2024pyg,Sun2020InfoGraph} learns node embeddings by pulling similar nodes together while pushing dissimilar ones apart.
