\appendix

\vspace{-5pt}
\section{Algorithm}\label{app:algo}
We present the overall algorithm of \algname\ as Algorithm~\ref{algo:jeona}.
\begin{algorithm}[h]
    \caption{Joint OT and embedding learning (\algname)}
    \begin{algorithmic}[1]\label{algo:jeona}
        \REQUIRE (1) networks $\mathcal{G}_1=(\mathbf{A}_1,\mathbf{X}_1), \mathcal{G}_2=(\mathbf{A}_2,\mathbf{X}_2)$, (2) anchor node set $\mathcal{L}$, (3) parameters $\alpha,\beta,\gamma_p$
        \ENSURE the alignment matrix $\mathbf{S}$.
        \STATE Initialize $\bm{\mu}_1 = \frac{\mathbf{1}_{n_1}}{n_1}, \bm{\mu}_2 = \frac{\mathbf{1}_{n_2}}{n_2},\mathbf{S}^{(1)}=\bm{\mu}_1\bm{\mu}_2^\T, \lambda^{(1)}=\frac{1}{n_1\times n_2}$;
        \STATE Compute RWR embedding matrices $\mathbf{R}_1, \mathbf{R}_2$ by Eq.~\eqref{eq:rwr};
        \STATE Concatenate node attributes $\mathbf{X}_1=[\mathbf{R}_1||\mathbf{X}_1]$, $\mathbf{X}_2=[\mathbf{R}_2||\mathbf{X}_2]$
        \FOR{$k=1,...,K$}
            \STATE Update node embeddings $\mathbf{E}^{(k)}_1\!=\!f_\theta^{(k)}(\mathbf{X}_1)$, $\mathbf{E}^{(k)}_2\!=\!f_\theta^{(k)}(\mathbf{X}_2)$;
            \STATE Update cost matrices $\mathbf{M}^{(k)}, \mathbf{C}^{(k)}_1, \mathbf{C}^{(k)}_2$ by Eq.~\eqref{eq:cost};
            \STATE Update OT mapping $\mathbf{S}^{(k+1)}$ by proximal point method in Eq.~\eqref{eq:ot_cpot};
            \STATE Update transformation parameter $\lambda^{(k+1)}$ by Eq.~\eqref{eq:opt_lambda};
            \STATE Update $\theta^{(k+1)}$ by SGD in Eq.~\eqref{eq:opt_sgd};
        \ENDFOR
        \RETURN alignment matrix $\mathbf{S}^{(K+1)}$.
    \end{algorithmic}
\end{algorithm}

\vspace{-5pt}
\section{Proof}\label{app:proof}
\vspace{-2pt}
\subsection{Proof of Proposition~\ref{prop:collapse}}
\begin{proposition*}
{\normalfont\textsc{(Embedding Collapse).}}
Given two networks $\G_1,\G_2$, directly optimizing feature encoder $f_\theta$ with the FGW distance leads embedding collapse, that is $\mathbf{E}_1(x)=\mathbf{E}_2(y), \forall x\in\G_1,y\in\G_2$, where $\mathbf{E}_1=f_\theta(\G_1),\mathbf{E}_2=f_\theta(\G_2)$.
\end{proposition*}
\begin{proof}
Firstly, the Wasserstein term can be written as
\begin{equation}
    \sum_{x\in\mathcal{G}_1,y\in\mathcal{G}_2}\mathbf{M}^q(x, y)\mathbf{S}(x,y)
\label{eq:prop_w}
\end{equation}
Due to the non-negativity of $\mathbf{M}$ and $\mathbf{S}$, i.e., $\mathbf{S}(x,y)\geq 0, \mathbf{M}(x, y)\geq 0, \forall x\in\mathcal{G}_1, y\in\mathcal{G}_2$, the Wasserstein term in Eq.~\eqref{eq:prop_w} has a theoretical minimum of 0.
Since Eq.~\eqref{eq:prop_w} is a linear programming problem w.r.t $\mathbf{S}$ which is computationally demanding to solve, existing works turn to solve the entropy-regularized OT problem to approximate Eq.~\eqref{eq:prop_w}, where the solved $\mathbf{S}$ is strictly positive, i.e. $\mathbf{S}(x,y)>0,\forall x\in\G_1,y\in\G_2$.
We can simply prove by contradiction that Eq.~\eqref{eq:prop_w} reaches 0 if and only if $ \forall x\in\mathcal{G}_1, y\in\mathcal{G}_2, \mathbf{M}(x, y)=0$. According to the universal approximation theorem~\cite{cybenko1989approximation}, such cross-network cost matrix is achievable with a MLP. Therefore, optimizing Eq.~\eqref{eq:prop_w} under a node mapping matrix $\mathbf{S}$ will lead to collapsed node embeddings across two networks, i.e., $\mathbf{E}_1(x)=\mathbf{E}_2(y), \forall x\in\G_1,y\in\G_2$.

Secondly, the GW term can be formulated as
\begin{equation}
    \!\!\!\!\!\sum_{x_1,x_2\in\G_1\atop y_1,y_2\in\G_2}\!\!\!\!|\mathbf{C}_1(x_1,x_2)-\mathbf{C}_2(y_1,y_2)|^q\mathbf{S}(x_1,y_1)\mathbf{S}(x_2,y_2).
\label{eq:prop_gw}
\end{equation}
Similarly, due to the non-negativity of $|\mathbf{C}_1(x_1,x_2)-\mathbf{C}_2(y_1,y_2)|^q$ and the positivity of $\mathbf{S}(x_1,y_1)\mathbf{S}(x_2,y_2)$, the GW term in Eq~\eqref{eq:prop_gw} has a theoretical minimum of 0 if and only if $ \forall x_1, x_2\in\mathcal{G}_1, y_1, y_2\in\mathcal{G}_2, |\mathbf{C}_1(x_1,x_2)-\mathbf{C}_2(y_1,y_2)|^q=0$.
Since $\mathbf{C}_1(x, x)=\mathbf{C}_2(y, y)=0$, $\forall x_1, x_2\in\mathcal{G}_1, y_1, y_2\in\mathcal{G}_2, \mathbf{C}_1(x_1, x_2)=\mathbf{C}_2(y_1, y_2)=0$, which essentially means the embeddings of all nodes in $\G_1$ ($\G_2$) collapse into a single point, i.e., $\mathbf{E}_1(x_1)=\mathbf{E}_1(x_2), \mathbf{E}_2(y_1)=\mathbf{E}_2(y_2), \forall x_1,x_2\in\G_1,y_1,y_2\in\G_2$. By combining Eq.~\eqref{eq:prop_w} and Eq.~\eqref{eq:prop_gw}, the Wasserstein term further causes the embedding of all nodes in both networks to collapse into a single point, i.e., $\mathbf{E}_1(x)=\mathbf{E}_2(y), \forall x\in\G_1,y\in\G_2$. Therefore, directly optimizing feature encoder with the FGW distance leads embedding collapse.
\end{proof}

\vspace{-10pt}
\subsection{Proof of Theorem~\ref{theo:convergence}}
\begin{theorem*}
    {\normalfont \textsc{(Convergence of \algname)}} The unified objective for \algname\ in Eq.~\eqref{eq:object} is non-increasing and converges along the alternating optimization.
\end{theorem*}
\begin{proof}
    We first prove Eq.~\eqref{eq:object} is bounded by a minimum value. We make a common assumption that the parameter set $\theta$ of the MLP is bounded~\cite{slotalign}. Since $\mathbf{S}\in\Pi(\bm{\mu}_1,\bm{\mu}_2)$ is bounded as well, we only need to prove that Eq.~\eqref{eq:object} is bounded w.r.t $\lambda$, which is essentially a quadratic function with a non-negative coefficient for the quadratic term, i.e.,
    \begin{equation*}
        \sum_{\substack{x_1,x_2\in\G_1\\y_1,y_2\in\G_2}}|\mathbf{C}_1(x_1,x_2)-\mathbf{C}_2(y_1,y_2)|^2 \geq 0
    \end{equation*}
    By solving $\lambda$ based on $\partial \mathcal{J}/\partial \lambda=0$ according to Eq.~\eqref{eq:opt_lambda}, we have the optimal $\lambda^*$ minimizing Eq.~\eqref{eq:object} as follows
    \begin{equation*}
        \min\limits_{\lambda}\mathcal{J}(\mathbf{S},\lambda,\theta)=\mathcal{J}(\mathbf{S},\lambda^*,\theta).
    \end{equation*}
    Since both $\theta$ and $\mathbf{S}$ are bounded, there exists a real number $\epsilon\in\mathbb{R}$ satisfying
    \begin{equation*}
        \mathcal{J}(\mathbf{S},\lambda,\theta)\geq \mathcal{J}(\mathbf{S},\lambda^*,\theta)>\epsilon
    \end{equation*}
    In this way, we have prove that Eq.~\eqref{eq:object} is bounded by a minimum value $\epsilon$.
    
    Then, we prove that Eq.~\eqref{eq:object} is non-increasing and converges along the alternating optimization, i.e.,
    \begin{equation}\label{eq:decrease}
        \mathcal{J}(\mathbf{S}^{(k+1)}, \lambda^{(k+1)}, \theta^{(k+1)})\leq \mathcal{J}(\mathbf{S}^{(k)}, \lambda^{(k)}, \theta^{(k)})
    \end{equation}
    To prove Eq.~\eqref{eq:decrease}, we first show that the OT optimization by proximal point method is non-increasing. Specifically, as proved theoretically in~\cite{gwl}, the proximal point method solves Eq.~\eqref{eq:object} w.r.t $\mathbf{S}$ by decomposing the non-convex objective function into a series of convex approximations, which be viewed as a successive upper-bound minimization~\cite{razaviyayn2013unified} problem whose descend property is guaranteed. In this way, we have demonstrated that
    \begin{equation}\label{eq:decrease_ot}
        \mathcal{J}(\mathbf{S}^{(k+1)}, \lambda^{(k)}, \theta^{(k)})\leq \mathcal{J}(\mathbf{S}^{(k)}, \lambda^{(k)}, \theta^{(k)})
    \end{equation}
    Then, we solve $\lambda^{(k+1)}$ optimally based on the closed-form solution in Eq.~\eqref{eq:opt_lambda} with guaranteed global minimum. Therefore, we have
    \begin{equation}\label{eq:decrease_lamda}
        \mathcal{J}(\mathbf{S}^{(k+1)}, \lambda^{(k+1)}, \theta^{(k)})\leq \mathcal{J}(\mathbf{S}^{(k+1)}, \lambda^{(k)}, \theta^{(k)})
    \end{equation}
    Finally, with an appropriate learning rate, the objective of the embedding learning process via SGD is non-increasing at each step, i.e.,
    \begin{equation}\label{eq:decrease_emb}
        \mathcal{J}(\mathbf{S}^{(k+1)}, \lambda^{(k+1)}, \theta^{(k+1)})\leq \mathcal{J}(\mathbf{S}^{(k+1)}, \lambda^{(k+1)}, \theta^{(k)})
    \end{equation}
    Combining Eq.~\eqref{eq:decrease_ot}-\eqref{eq:decrease_emb} gives Eq.~\eqref{eq:decrease}, hence proving Theorem~\ref{theo:convergence}.
\end{proof}

\vspace{-10pt}
\subsection{Proof of Proposition~\ref{prop:complexity}}
\begin{proposition*}
{\normalfont (\textsc{Complexity of \algname})} The overall time complexity of \algname\ is $\mathcal{O}\left(KTmn+KTNn^2\right)$ at the training phase and $\mathcal{O}\left(Tmn+TNn^2\right)$ at the inference phase, where $K, T, N$ denote the number of iterations for alternating optimization, proximal point iteration, and Sinkhorn algorithm, respectively.
\end{proposition*}
\begin{proof}
    The time complexity of \algname\ includes four parts: RWR encoding, MLP computation, calculation of the optimal $\lambda$, and OT optimization. As $\mathbf{C}_i,\mathbf{W}_i$ are sparse matrices with $\mathcal{O}(m)$ non-zero entries, the time complexity of RWR in Eq.~\eqref{eq:rwr} is $\mathcal{O}(mn)$~\cite{parrot}.
    
    For each iteration of the alternating optimization, the time complexity for forward (backward) propagation of the MLP model $\G_1(\G_2)$ are $\mathcal{O}(n|\mathcal{L}|d_1)$ (first layer) and $\mathcal{O}(n|\mathcal{L}|d_2)$ (second layer), respectively. For the calculation of the optimal $\lambda$, the time complexity is $\mathcal{O}(mn)$~\cite{parrot}. For the OT optimization, the time complexity is $\mathcal{O}(Tmn+TNn^2)$ with $T$ iterations of proximal point method and $N$ Sinkhorn iterations~\cite{parrot}.
    
    Combining the above three components gives a total time complexity of $\mathcal{O}\left(K(2n|\mathcal{L}|d_1+2n|\mathcal{L}|d_2+(T+1)mn+TNn^2)\right)$ where $K$ is the number of iteration for the alternating optimization. Since $n\gg |\mathcal{L}|,T\gg1$, the overall training time complexity of \algname\ is $\mathcal{O}(KTmn+KTNn^2)$. Note that model inference is only one-pass without the alternating optimization, hence the inference time complexity is $\mathcal{O}\left(Tmn+TNn^2\right)$.
\end{proof}

\vspace{-5pt}
\section{Experiment Pipeline}\label{app:exp}
\vspace{-5pt}
\begin{table}[ht]
    \small
    \vspace{-5pt}
    \centering
    \caption{Dataset Statistics.}
    \vspace{-5pt}
    \begin{tabular}{ccccc}
        \toprule
        \textbf{Scenarios} & \textbf{Networks} & \textbf{\# nodes} & \textbf{\# edges} & \textbf{\# attributes} \\
        \midrule
        \multirow{6}{*}{Plain} & Foursquare & 5,313 & 54,233 & 0 \\
        & Twitter & 5,120 & 130,575 & 0 \\
        \cmidrule{2-5}
        & ACM & 9,872 & 39,561 & 0 \\
        & DBLP & 9,916 & 44,808 & 0 \\
        \cmidrule{2-5}
        & Phone & 1,000 & 41,191 & 0 \\
        & Email & 1,003 & 4,627 & 0 \\
        \midrule
        \multirow{6}{*}{Attributed} & Cora1 & 2,708 & 6,334 & 1,433 \\
        & Cora2 & 2,708 & 4,542 & 1,433 \\
        \cmidrule{2-5}
        & ACM(A) & 9,872 & 39,561 & 17 \\
        & DBLP(A) & 9,916 & 44,808 & 17 \\
        \cmidrule{2-5}
        & Douban(online) & 3,906 & 16,328 & 538 \\
        & Douban(offline) & 1,118 & 3,022 & 538 \\
        \bottomrule
    \end{tabular}
    \vspace{-5pt}
    \label{tab:datasets}
\end{table}

\noindent\textbf{Dataset Descriptions.} 
We provide dataset descriptions as follows
\begin{itemize}
    \item Foursquare-Twitter~\cite{zhang2015integrated}: A pair of online social networks with nodes as users and edges as follower/followee relationships.
    % Foursquare network contains 5,313 nodes and 5,120 edges. Twitter network contains 5,120 nodes and 130,575 edges. 
    Node attributes are unavailable in both networks. There are 1,609 common users used as ground-truth.
    \item ACM-DBLP~\cite{tang2008arnetminer}: A pair of undirected co-authorship networks with nodes as authors and edges as co-authorship. 
    % ACM network contains 9,916 nodes and 44,808 edges. DBLP network contains 9,872 nodes and 39,561 edges. 
    Node attributes are available in both networks, and we use the dataset for both plain and attributed network alignment tasks named ACM-DBLP and ACM(A)-DBLP(A), respectively. There are 6,325 common authors as ground-truth.
    \item Phone-Email~\cite{zhang2017ineat}: A pair of communication networks with nodes as people and edges as their communications via phone or email.
    % Phone networks contains 1,000 nodes and 41,191 edges. Email networks contains 1,003 nodes and 4,627 edges. 
    Node attributes are unavailable in both networks. There are 1,000 common people used as ground-truth.
    \item Cora1-Cora2~\cite{yang2016revisiting}. A citation network with nodes representing publications and edges as citations among publications. Cora-1 and Cora-2 are two noisy permutation networks generated by inserting 10\% edges into Cora-1 and deleting 15\% edges from Cora-2.
    % Cora-1 contains 2,708 nodes and 6,334 edges. Cora-2 contains 2,708 nodes and 4,542 edges. 
    Both networks contains binary bag-of-words vectors as attributes. There are 2,708 common publications used as ground-truth.
    \item Douban~\cite{final}. A pair of social networks with nodes representing users and edges representing user interactions on the website. 
    % Online network contains 3,906 nodes and 16,328 edges. Offline network contains 1,118 nodes and 3,022 edges. 
    The node attributes are binary vectors that encodes the location of a user. There are 1,118 common user across the two networks used as ground-truth.
\end{itemize}
Dataset statistics are given in Table~\ref{tab:datasets}. We use 20\% ground-truth as the anchor nodes and the rest 80\% of the ground-truth for testing.

\noindent\textbf{Machine and Code.} The proposed model is implemented in PyTorch. We use Apple M1 Pro with 16GB RAM to run PARROT, IsoRank, FINAL, and GOAT. We use NVIDIA Tesla V100 SXM2 as GPU for \algname\ and other baselines.

\noindent\textbf{Implementation Details.} Adam optimizer is used with a learning rate of 1e-4 to train the model. The hidden and output dimension is set to 128. The epoch number of \algname\ is 50. An overview of other hyperparameters settings for \algname\ is shown in Table~\ref{tab:hparam}. For all baselines, hyperparameters are set as default in their official code.

\vspace{-10pt}
\begin{table}[H]
\small
\centering
\caption{Hyperparameters settings}
\label{tab:hparam}
\vspace{-5pt}
\begin{tabular}{cccc}
\toprule
Dataset  & $\alpha$ & $\beta$ & $\gamma_p$ \\
\midrule
Foursquare-Twitter & 0.50 & 0.15 & 1e-3 \\
ACM-DBLP           & 0.90 & 0.15 & 5e-3 \\
Phone-Email        & 0.75 & 0.15 & 1e-2 \\
ACM(A)-DBLP(A)     & 0.90 & 0.15 & 1e-2 \\
Cora1-Cora2        & 0.30 & 0.15 & 5e-4 \\
Douban             & 0.50 & 0.15 & 1e-3 \\
\bottomrule
\end{tabular}
\vspace{-7pt}
\end{table}

\vspace{-10pt}
\begin{figure}[t]
  \includegraphics[width=0.7\linewidth, trim=0 10 0 20, clip]{figures/exp_hparam.png}
  \vspace{-5pt}
  \caption{Hyperparameter study on Foursquare-Twitter.}
  \label{fig:exp_hparams}
  \vspace{-10pt}
\end{figure}

\vspace{-5pt}
\section{Additional Experiements}
\vspace{-2pt}
\subsection{Additional Scalability Results}

To further demonstrate the scalability of the proposed \algname\ on large-scale networks, we compared the inference time of \algname\ with that of three OT-based methods, including GOAT~\cite{goat}, PARROT~\cite{parrot}, and SLOTAlign~\cite{slotalign} on ogbl-biokg~\cite{hu2020open} with 93,773 nodes and 5,088,434 edges. We follow the synthesis process of Cora~\cite{yang2016revisiting} to generate two noisy permutation networks from ogbl-biokg by randomly inserting 10\% edges into the first network and deleting 15\% edges from the second network. The results are shown in Table~\ref{tab:exp_add_runtime}, which shows that \algname\ outperforms all baselines in inference time and MRR under 12-hour runtime limit.

\vspace{-5pt}
\begin{table}[ht]
    \centering
    \caption{Scalability results on ogbl-biokg}
    \vspace{-5pt}
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{ccccc}
        \toprule
        & SLOTAlign & GOAT & PARROT & JOENA \\
        \midrule
        Runtime & >12h & >12h & 2.71h & \textbf{0.66h} \\
        MRR & 0.341 & 0.112 & 0.741 & \textbf{0.876} \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:exp_add_runtime}
\end{table}

\vspace{-10pt}
\subsection{Hyperparameter Sensitivity Study}
We study the sensitivity of \algname\ on the FGW weight $\alpha$ and RWR restart probability $\beta$, with values ranging from 0.1 to 0.9. The results are shown in Figure~\ref{fig:exp_hparams}, which shows that our method is robust to different selections of hyperparameters in a wide range.

