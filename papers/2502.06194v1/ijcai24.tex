%%%% ijcai24.tex

\typeout{IJCAI--24 Instructions for Authors}

% These are the instructions for authors for IJCAI-24.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai24.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai24}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{amsfonts,amssymb}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{bbding}
\usepackage{pifont}
\usepackage{wasysym}
\usepackage{utfsym}
\usepackage{fontawesome}
\usepackage{multirow}

\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{bbding}
\usepackage{pifont}
\usepackage{wasysym}
\usepackage{utfsym}
\usepackage{fontawesome}
% Comment out this line in the camera-ready submission
%\linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2024.0)
}

\title{Multimodal Task Representation Memory Bank vs. Catastrophic Forgetting in Anomaly Detection }
%

% Single author syntax
\author{
%Anonymous
    You Zhou, Jiangshan Zhao, Deyu Zeng, Zuo Zuo,Weixiang Liu, Zongze Wu
    % \affiliations
     % College of Mechatronics and Control Engineering, Shenzhen University,  Shenzhen, Guangdong,518052, China.
    % \emails
    % zzwu@szu.edu.cn
}

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
\iffalse
\author{
First Author$^1$
\and
Second Author$^2$\and
Third Author$^{2,3}$\And
Fourth Author$^4$\\
\affiliations
$^1$First Affiliation\\
$^2$Second Affiliation\\
$^3$Third Affiliation\\
$^4$Fourth Affiliation\\
\emails
\{first, second\}@example.com,
third@other.example.com,
fourth@example.com
}
\fi

\begin{document}

\maketitle

\begin{abstract}
Unsupervised Continuous Anomaly Detection (UCAD) faces significant challenges in multi-task representation learning, with existing methods suffering from incomplete representation and catastrophic forgetting. Unlike supervised models, unsupervised scenarios lack prior information, making it difficult to effectively distinguish redundant and complementary multimodal features.
To address this, we propose the Multimodal Task Representation Memory Bank (MTRMB) method through two key technical innovations: A Key-Prompt-Multimodal Knowledge (KPMK) mechanism that uses concise key prompts to guide cross-modal feature interaction between BERT and ViT.
Refined Structure-based Contrastive Learning (RSCL) leveraging Grounding DINO and SAM to generate precise segmentation masks, pulling features of the same structural region closer while pushing different structural regions apart.
Experiments on MVtec AD and VisA datasets demonstrate MTRMB's superiority, achieving an average detection accuracy of 0.921 at the lowest forgetting rate, significantly outperforming state-of-the-art methods. We plan to open source on GitHub.
\end{abstract}

%版本一：无监督连续异常检测 (UCAD) 在多任务表征学习中面临重大挑战，现有方法存在表征不完整和灾难性遗忘的问题。与监督模型不同，无监督场景缺乏先验信息，难以有效区分冗余和互补的多模态特征。
%为了解决这个问题，我们通过两项关键技术创新提出了多模态任务表征记忆库 (MTRMB) 方法：关键提示多模态知识 (KPMK) 机制，使用简洁的关键提示来指导 BERT 和 ViT 之间的跨模态特征交互。
%基于结构的精细对比学习 (RSCL) 利用 Grounding DINO 和 SAM 生成精确的分割掩码，将同一结构区域的特征拉近，同时将不同的结构区域推开。
%在 MVtec AD 和 VisA 数据集上的实验证明了 MTRMB 的优越性，在最低遗忘率下实现了 0.921 的平均检测准确率，明显优于最先进的方法。我们计划在GitHub上开源。


%版本二：基于记忆库的无监督连续异常检测（UCAD）仍然存在由于表示不完整而导致的灾难性遗忘问题。引入多模态特征构建任务表示记忆库可以有效保留旧任务的内在信息，从而缓解这一问题。显然，多模态提示增量学习应用于无监督异常检测无法像监督模型那样通过先验信息提高模型的多模态感知能力。针对这些问题，提出一种多模态任务表示记忆库（MTMB）来解决异常检测中的灾难性遗忘问题。首先，我们为UCAD引入了一种关键提示多模态知识（KPMK）机制，以简明的关键提示指导来自文本（BERT）和图像（ViT）两种不同模态的特征交叉融合，构建MTMB。此外，提出了一种基于精细结构的对比学习 (RSCL)，利用来自 Grounding DINO 和 Segment Anything 模型 (SAM) 的精确掩码来获得更紧凑的 MTMB。具体来说，RSCL 根据多模态模型交互将同一结构区域的特征拉近，将不同结构区域的特征推远。与 Mvtec AD 和 VisA 上的其他先进方法相比，EMMB 表现出优异的性能，在最低遗忘率下，平均检测精度为 0.921。我们计划开源在GitHub。

%版本三（Selected）：为了持续检测工业制造场景下不可预测的缺陷样本，需要无监督连续异常检测 (UCAD)利用预先训练好的网络和非参数建模来估计多个任务中编码的特征分布。然而，这些方法存在表征不完整和灾难性遗忘的问题。与监督模型不同，无监督场景缺乏先验信息，难以有效区分冗余和互补的多模态特征。
%为了解决这个问题，我们通过两项关键技术创新提出了多模态任务表征记忆库 (MTRMB) 方法：键提示多模态知识 (KPMK) 机制，使用简洁的关键提示来指导 BERT 和 ViT 之间的跨模态特征交互。
%基于结构的精细对比学习 (RSCL) 利用 Grounding DINO 和 SAM 生成精确的分割掩码，将同一结构区域的特征拉近，同时将不同的结构区域推开。
%在 MVtec AD 和 VisA 数据集上的实验证明了 MTRMB 的优越性，在最低遗忘率下实现了 0.921 的平均检测准确率，明显优于最先进的方法。我们计划在GitHub上开源。

%第一段介绍UAD和UCAD
%相关工作
%现有的UCAD不行，为啥不行，基于内存池相对好一点（优势）
%现有内存池存在什么问题，为啥没法解决灾难性遗忘问题
%多模态提示增量没有用于UCAD,原因和难点
%如何解决以上问题。
%贡献

\section{Introduction}

\begin{figure}[t]
	%%\centering
	\includegraphics[width=1\columnwidth, trim=0 0 0 0, clip]{motiva_img.pdf} 
	\caption{Comparison between UCAD with a memory bank and UCAD with a multimodal Task Representation Memory Bank. a) The memory bank-based method has the problem of forgetting old task knowledge due to incomplete representation, resulting in inaccurate new task hyperplane boundary. b) Introducing multimodal features to construct a task representation memory can effectively retain the intrinsic information of the old task and obtain a better new task hyperplane boundary.}
	\label{fig1} 
\end{figure}

%1）基于记忆库的无监督连续异常检测（UCAD）存在因表征不全而遗忘旧任务知识，导致产生不准确的new task hyperplane boundary。2）引入多模态特征构建任务表示记忆库可以有效保留旧任务的内在信息而获得更好的new task hyperplane boundary。

% Anomaly detection (AD) refers to identifying and localizing anomalies with limited, even no, prior knowledge of abnormality. In actual industrial environments, acquiring well-labeled defect data can be challenging and costly. Unsupervised Anomaly detection(UAD) relies only on the inherent distribution of "normal" data for detection. Therefore, UAD plays a key role in the industrial defect detection.
%异常检测 (AD) 是指在对异常的先验知识有限甚至没有的情况下识别和定位异常。在实际的工业环境中，获取标记良好的缺陷数据可能具有挑战性且成本高昂。UIAD仅依赖于“正态”数据的固有分布进行检测。因此，无监督工业异常检测在工业缺陷检测中起着关键作用。
Anomaly detection (AD) \cite{lin2024survey} refers to the identification and location of anomalies when there is limited or no prior knowledge of the anomaly. In recent years, the industry has begun to focus on equipping AD with continuous learning capabilities, that is, continuously learning the intrinsic distribution of normal data in new tasks without forgetting the knowledge of old tasks. This method is very necessary in industrial inspection scenarios because it is very difficult and expensive to collect defect data in new scenarios.
%异常检测 (AD) \cite{lin2024survey} 是指在对异常的先验知识有限甚至没有的情况下识别和定位异常。近年来，工业界开始关注在AD上配备持续学习能力，即在不遗忘旧任务知识的前提下，不断学习新任务中正常数据的内在分布。这一方法在工业检测场景下十分必要。因为收集新场景下的缺陷数据是十分困难且昂贵的。


% As shown in Figure \ref{fig1}, Recent research on UAD involves training different models for various categories, which leads to weak detection model transfer capabilities. In addition, as the number of categories increases, it also leads to excessive computational burden. To alleviate the above problems, multi-class anomaly detection aims to detect anomalies from different object categories using a unified model. Existing MCAD methods mainly follow two training paradigms: parallel training and continuous training. Parallel training-based methods can train a unified model that can handle multiple categories, such as UniAD\cite{you2022unified}, OmniAL\cite{zhao2023omnial}, and LTAD\cite{ho2024long}. However, these methods require that all data be trained simultaneously in actual production, which is impractical. In addition, when constantly adapting to frequent product changes during continuous training, parallel training-based methods still lack the ability to retain previously learned knowledge. Catastrophic forgetting and computational burden hinder the practical application of UAD methods in industrial detection.


Recently, most AD \cite{lin2024survey} focuses on training specific models for detection, which is effective for a single task. However, this suffers from a very serious catastrophic forgetting problem \cite{zhou2024class}. In addition, a single AD model is difficult to migrate, resulting in a waste of computing resources and an inability to address the privacy issues of industrial data. To support anomaly detection for multiple tasks, some AD methods \cite{you2022unified,yao2024prior,ho2024long} aim to train a unified model for multi-class anomaly detection. In actual industrial scenarios, the difficulty in providing training data for all tasks and the heavy computational burden hinder the practical application of such methods. In addition, such methods do not solve the catastrophic forgetting problem from the root.
%Recently，大多数异常检测方法聚焦于训练独立的模型，这在测试阶段不可避免依赖于任务ID，从而增加了额外的检测流程。模型的参数量也会随着任务量的增加会成倍增长。于是，一些AD方法旨在训练一个统一模型而不依赖于任务ID。然而，实际工业场景是一个开放的场景，新任务会随着实际生产情况变化。因此，难以一次性提供所有场景的训练数据，直接在新场景中训练会造成模型的灾难性遗忘\cite。


%相关工作（版本一）：Recently，大多数异常检测聚焦于训练特定模型用于检测，这针对单一任务是有效的。然而，这这存在极严重的灾难性遗忘问题。此外，单一异常检测模型难以迁移导致参数容量过大且无法应对工业数据的隐私问题。为了支持多个任务的异常检测，一些AD方法旨在训练一个统一模型用于多类异常检测。过重的计算负担阻碍了该类方法的实际应用，

%相关工作（版本二）：最近关于 UAD 的研究涉及为各种类别训练不同的模型，这导致检测模型迁移能力弱。此外，随着类别增加也会导致计算负担过重。为了缓解上述问题，多类异常检测旨在使用统一模型检测来自不同对象类别的异常。现有的 MCAD 方法主要遵循两种训练范式：并行训练和持续训练。基于并行训练的方法可以训练出一个可以处理多个类别的统一模型，例如 UniAD、OmniAL、LTAD。然而，这些方法在实际生产中要求同时训练所有数据是不切实际的。此外，在连续训练过程中不断适应频繁的产品变更时，基于并行训练的方法仍然缺乏保留先前学习的知识的能力。灾难性遗忘和计算负担阻碍了 UAD 方法在工业检测的实际应用。

With the development of AD, a few researchers began to realize the importance of Unsupervised Continuous Anomaly Detection(UCAD). DNE \cite{li2022towards} introduced continuous learning into the anomaly detection task for the first time, alleviating the catastrophic forgetting problem in the training phase of the continuous anomaly detection model. However, DNE can only be used to detect anomalies but not to locate abnormal areas. As shown in Figure \ref{fig1} (a), UCAD based on image feature memory \cite{liu2024unsupervised} solves the problem that DNE cannot locate abnormal areas, but there is a catastrophic forgetting problem caused by incomplete representation.
%随着AD的发展，少数研究者开始意识到了UCAD的重要性。DNE首次将持续学习引入异常检测任务，缓解了持续异常检测模型训练阶段的灾难性遗忘问题。然而，DNE 只能用于检测异常，而不能定位异常区域。基于图像特征记忆库的UCAD解决了DNE无法定位异常区域的问题，但存在由于表示不完整而导致灾难性遗忘。


The introduction of multimodal prompts can improve catastrophic forgetting caused by incomplete representation. By applying a small number of prompt parameters in a continuous space to modify the input, it can rely on the learnable prompts \cite{wang2022learning} of the language and visual encoders to achieve continuous learning across tasks and prevent forgetting. However, Prompt-based multimodal continuous learning (MMCL) \cite{yu2024recent} applied to unsupervised anomaly detection cannot improve the multimodal perception ability of the model through prior information like the supervised model. In the absence of supervised information, learnable prompts cannot be used to discriminate the redundant and complementary information of multimodal features, thereby achieving continuous learning in anomaly detection. Therefore, it is crucial to explore the application of multimodal prompt incremental learning in UCAD.
%引入多模态提示\cite{d2023multimodal}可以改善因表征不完全而导致灾难性遗忘，其通过在连续空间中应用少量提示参数来修改输入，便可依靠语言和视觉编码器的可学习提示\cite{wang2022learning,wang2022dualprompt}来实现跨任务的持续学习并防止遗忘。然而，基于提示的多模态增量学习\cite{yu2024recent}应用于无监督异常检测无法像监督模型那样通过先验信息提高模型的多模态感知能力。在没有监督信息的情况下，无法将可学习的提示用于判别多模态特征的冗余和互补信息，进而实现异常检测中的持续学习。因此，探索多模态提示增量学习在UCAD中的应用至关重要。


%多模态连续学习(MMCL)需要有效地集成和处理各种多模态数据流，同时还要在连续学习中设法保留以前获得的知识。其中，基于提示的范式可以通过在连续空间中应用少量提示参数来修改输入，使得模型在学习额外的特定任务信息时能够保留其原有知识。/cite{d2023multimodal}依靠语言和视觉编码器的可学习提示来实现跨会话的迁移学习并防止遗忘。

% L2P\cite{wang2022learning,wang2022dualprompt} dynamically learns hints as task identities. Although incremental methods are effective in supervised tasks, their effectiveness in UAD remains underdeveloped. Due to the scarcity of defect samples and data privacy issues, it is difficult to obtain large-scale anomaly data in the industry. Therefore, it is crucial to explore the application of CL in UAD.
%持续学习（CL）旨在解决单一模型的灾难性遗忘问题，特别涉及到需保护数据隐私。最近关于持续学习的研究可以根据测试阶段对任务身份的要求进行分类。该类方法使用特定任务身份来指导学习过程，并防止任务之间的干扰。然而，在推理过程中并不总是有可能获得任务身份。因此，任务不可知的方法在实际应用是更普遍的。L2P动态地学习提示作为任务身份。尽管增量方法在监督任务中有效，但它们在UAD中的有效性仍未得到充分证实。由于缺陷样本匮乏和数据隐私问题，在行业中获取大规模的异常数据是困难的。因此，探索CL在UAD中的应用至关重要。

% To address these problems, a multimodal Task Representation Memory Bank (MTMB) is proposed to solve catastrophic forgetting in anomaly detection. Firstly, we introduce a Key-Prompt-Multimodal Knowledge (KPMK) mechanism for UCAD, which uses a concise key prompt to guide the cross-fusion of features from two different modalities, text (BERT) and image (ViT), construct an  MTMB. Moreover, a Refined Structure-based Contrastive Learning(RSCL) using accurate masks from the Grounding DINO and Segment Anything Model (SAM) is proposed to acquire a more compact MTMB. Specifically, RSCL pulls the features of the same structural region closer and pushes the features of different structural regions farther away according to multimodal model interaction. Compared with other advanced methods on Mvtec AD and VisA, EMMB showed superior performance

To address the above problems, the proposed multimodal Task Representation Memory Bank (MTRMB) can solve catastrophic forgetting in anomaly detection. MTRMB utilizes a memory space of Key-prompts-Multimodal knowledge (KPMK) to support Continuous Learning in AD, as shown in Figure \ref{fig1} (b). In addition, Refined Structure-based Contrastive Learning (RSCL) leveraging Grounding DINO \cite{liu2025grounding} and Segment Anything Model (SAM) \cite{kirillov2023segment} to generate precise segmentation masks, pulling features of the same structural region closer while pushing different structural regions apart, and make MTRMB becomes more compact. During the training phase, the proposed memory space of Key-prompts-Multimodal knowledge stores the key, prompts, and multimodal knowledge of a specific task. During the testing phase, MMCL implements task matching, task domain adaptation, and the transfer of "normal" knowledge of different classes. Given a test image, MTRMB will automatically query the task key to retrieve the corresponding task prompt, complete the model's adaptation to the task domain through the prompt, and then extract image features and perform similarity calculation with normal knowledge, similar to PatchCore \cite{li2022towards}. However, the frozen backbone (ViT) cannot provide compact feature representations across different tasks. To overcome this problem, RSCL using accurate masks from the Grounding DINO and SAM is proposed to acquire a more compact MTRMB. We obtain more representative contextual features between different classes through structural contrastive learning, where features of the same structure are pulled together and pushed away from features of other structures, which can effectively reduce domain shift \cite{kirillov2023segment}. We conduct extensive experiments and the results demonstrate the advancedness of the proposed MTRMB. Our contributions can be summarized as follows:

%（版本一）为了解决上述问题，the proposed multimodal Task Representation Memory Bank(MTRB) can solve catastrophic forgetting in anomaly detection。MTRB利用了一个Key-prompts-Multimodal knowledge的记忆空间(KPMK)来支持AD中的MMCL。此外，我们构建了二阶段的Mask提取器，利用文本-图像的混合提示GroundingDino和SAM生成准确的部件区域。 我们建议在任务流中使用基于结构的对比学习（SCL）提取更紧凑的特征。在训练阶段时，所提出的Key-prompts-Multimodal knowledge的记忆空间会存储特定任务的Key、提示和多模态知识。 在测试阶段，MMCL实现任务匹配、任务域适应和不同类的“正常”知识的转移。给定一个测试图像，RSCHP将自动查询任务键以检索相应的任务提示，通过提示完成模型对任务域的适应，然后提取图像特征，并与正常知识进行相似度计算，类似于PatchCore（Roth et al. 2022）。然而，冻结的主干（ViT）不能在不同的任务中提供紧凑的特征表示。为了克服这一问题，a Refined Structure-based Contrastive Learning(RSCL) using accurate masks from the Grounding DINO and Segment Anything Model (SAM) is proposed to acquire a more compact MTMB.  我们通过结构对比学习获得不同类之间更具代表性的上下文特征，相同结构的特征被拉在一起，并从其他结构的特征中推开，这能有效减少域偏移。我们进行了大量的实验，结果证明了所提出的RSCHP的先进性。

%（版本二）为了解决上述问题，the proposed multimodal Task Representation Memory Bank(MTRB) can solve catastrophic forgetting in anomaly detection。我们提出了Key-prompts-Multimodal knowledge来构造MTRMB来为AD配备增量学习能力。此外，我们构建了二阶段的Mask提取器，利用文本-图像的混合提示GroundingDino和SAM生成准确的部件区域。 我们建议在任务流中使用基于结构的对比学习（SCL）提取更紧凑的特征。在训练阶段时，所提出的Key-prompts-Multimodal knowledge的记忆空间会存储特定任务的Key、提示和多模态知识。 在测试阶段，MMCL实现任务匹配、任务域适应和不同类的“正常”知识的转移。给定一个测试图像，RSCHP将自动查询任务键以检索相应的任务提示，通过提示完成模型对任务域的适应，然后提取图像特征，并与正常知识进行相似度计算，类似于PatchCore（Roth et al. 2022）。然而，冻结的主干（ViT）不能在不同的任务中提供紧凑的特征表示。为了克服这一问题，a Refined Structure-based Contrastive Learning(RSCL) using accurate masks from the Grounding DINO and Segment Anything Model (SAM) is proposed to acquire a more compact MTMB.  我们通过结构对比学习获得不同类之间更具代表性的上下文特征，相同结构的特征被拉在一起，并从其他结构的特征中推开，这能有效减少域偏移。我们进行了大量的实验，结果证明了所提出的RSCHP的先进性。

\begin{itemize}
\item To the best of our knowledge, the proposed MTRMB is the first to use incremental learning of multimodal prompts for unsupervised anomaly detection. MTRMB proposes a Key-Prompt-Multimodal Knowledge mechanism for task matching, knowledge transfer, unsupervised anomaly detection, and segmentation.
%To the best of our knowledge，所提出的MTRMB是首次将多模态提示增量学习用于无监督异常检测。MTRMB提出Key-Prompt-Multimodal Knowledge mechanism，用于任务匹配、知识转移、无监督异常检测和分割。


\item The proposed RSCL to obtain more compact MTRMB by leveraging accurate masks from Grounding DINO and SAM. Specifically, RSCL pulls features of the same structural region closer and pushes features of different structural regions farther apart according to the multimodal model interaction.
%我们提出了一种基于精细结构的对比学习 (RSCL)，利用来自 Grounding DINO 和 Segment Anything 模型 (SAM) 的精确掩码来获得更紧凑的 MTMB。具体来说，RSCL 根据多模态模型交互将同一结构区域的特征拉近，将不同结构区域的特征推远。

\item We have conducted thorough experiments and introduced a new benchmark for UCAD. Compared with other advanced methods on MVTec AD and VisA, MTRMB showed superior performance, with an average detection accuracy of 0.921 under the lowest forgetting rate.
\end{itemize}


\section{Related Work}
\subsection{Anomaly Detection}
With the release of the MVTec AD dataset \cite{bergmann2019mvtec} and Visa \cite{zou2022spot}, the development of industrial image anomaly detection has shifted from a supervised paradigm to an unsupervised paradigm. research on common AD \cite{liu2024deep} has been divided into two main categories: feature-embedding-based methods and reconstruction-based methods.
\textbf{Feature-embedding-based methods} can be further categorized into four subcategories, including teacher-student model\cite{bergmann2020uninformed,salehi2021multiresolution,deng2022anomaly,tien2023revisiting,batzner2024efficientad}, one-class classification methods \cite{liu2023simplenet,cao2023anomaly}, mapping-based methods \cite{zhou2024msflow,rudolph2022fully} and memory-based methods \cite{li2022towards,xie2023pushing}. \textbf{Reconstruction-based
methods} can be further divided into Autoencoders-based \cite{schluter2022natural,zavrtanik2021draem}, Generative Adversarial Networks-based \cite{peng2024industrial}.

However, existing AD methods train separate anomaly models for different classes, which inevitably suffers from catastrophic forgetting and excessive computational burden. Even the multi-class unified anomaly detection model \cite{you2022unified,zhao2023omnial,ho2024long} does not consider the case of continuous anomaly detection. Our method is specifically designed for the scenario of continuous learning and achieves continuous anomaly detection and segmentation in an unsupervised manner.

%然而，现有的UAD方法为不同类训练单独的异常模型，这不可避免的存在灾难性遗忘和计算负担过重的问题。即使多类统一异常检测模型（You et al. 2022；Zhao 2023）也没有考虑到持续异常检测的情况。而我们的方法是专门为连续学习的场景而设计的，并以无监督的方式实现连续异常检测和分割。
\begin{figure*}[h]
	\centering 
	%%\centering
	\includegraphics[scale=0.32]{framework.pdf} 
	\caption{The framework of UCAD using multimodal Task Representation Memory Bank.
    (a) Text-image data is input during the training phase, and an effective task intrinsic memory bank is formed through the KPMK. In addition, we use RSCL to better utilize task-related contextual information to obtain a more compact MTRMB. (b) When a test image is input during the testing phase, the framework automatically queries the Task key to retrieve the corresponding task prompts, completes the model's transfer of task knowledge through the prompts, then extracts the features of the test image and calculates the similarity with normal knowledge, and finally completes continuous detection of anomalies. (c) The KPMK mechanism uses the concise key to guide the cross-fusion of features from two different modalities, text and image, and generates an effective task representation memory bank.}
	\label{MTRMB} 
\end{figure*}

%（a）在训练阶段输入文本-图像数据，通过KPMK机制构成有效的任务本征内存池。此外，我们使用RSCL更好的利用与任务相关的上下文信息来获得更紧凑的MTMB。（b）在测试阶段输入测试图像，该框架自动查询Task key以检索相应的任务提示，通过提示完成模型对任务知识的迁移，然后提取测试图像的特征并与正常知识进行相似度计算,最后完成持续检测异常。（c）（KPMK）机制利用简明的关键提示指导来自文本（BERT）和图像（ViT）两种不同模态的特征交叉融合，并生成有效的任务表征内存池。

\subsection{Continual Anomaly Detection}
IDDM \cite{zhang2023iddm} proposed an incremental anomaly detection method based on a small number of labeled samples. On the other hand, LeMO \cite{li2023cross} follows the common unsupervised anomaly detection paradigm and performs incremental anomaly detection as normal samples continue to increase. However, both IDDM and LeMO focus on the study of intra-class continuous anomaly detection, but do not address the challenge of inter-class incremental anomaly detection. Li et al. \cite{li2022towards} proposed DNE for image-level anomaly detection in continuous learning scenarios. Due to the limitation that DNE only stores class-level information, it cannot perform fine-grained localization and is therefore not suitable for anomaly segmentation. UCAD solves the problem that DNE cannot locate abnormal areas and further alleviates the problems of catastrophic forgetting and excessive computational burden. However, this method suffers from performance limitations due to the lack of compactness and comprehensiveness of the representation.

%IDDM（Zhang和Chen 2023）提出了一种基于少量标记样本的增量异常检测方法。另一方面，LeMO（Gao et al. 2023）遵循常见的无监督异常检测范式，随着正常样本的不断增加而进行增量异常检测。然而，IDDM和LeMO都关注类内连续异常检测研究，但没有解决类间增量异常检测的挑战。Li等人（Li等人，2022年）提出了DNE，用于连续学习场景中的图像级异常检测。由于DNE仅存储类级信息的局限性，无法进行细粒度定位，因此不适合进行异常分割。UCAD 解决了 DNE 无法定位异常区域的问题，并进一步缓解了灾难性遗忘和计算负担过大的问题。然而，该方法由于表示缺乏紧凑性和全面性而存在性能限制。

\section{Methods}
% \textbf{Problem formulation:}
% The goal of unsupervised continual anomaly detection (UCAD) is to identify abnormal data using only normal data from different tasks. The training set in each task contains only normal samples from various tasks, while the test set in each task contains both normal and abnormal samples, which reflects the actual detection requirements in industrial scenarios. To facilitate the description of the problem, we define the multi-class training set as $T^{total}_{train}=\{T^1_{train}, T^2_{train},..., T^n_{train}\}$, and the multi-class test set as $T^{total}_{test}=\{T^1_{test}, T^2_{test},..., T^n_{test}\}$. $T^{i}_{train}$ and $T^{i}_{test}$ represent the training data and test data of class $i$, respectively. The goal is to train a model that does not forget to identify and locate anomalies in the current and past test sets.
%无监督持续异常检测（AD）的目标是仅使用不同任务中的正常数据来识别异常数据。每个任务中的训练集只包含来自各种任务的正常样本，而每个任务中的测试集同时包含正常样本和异常样本，这反映了工业场景下的实际检测要求。为了方便对问题的描述，我们将多类训练集定义为T=T1、T2、···、Tn，多类测试集定义为T=T1、T2、···、Tn。T和T分别表示i类的训练数据和测试数据。其目标是训练一个不会遗忘的模型来识别和定位当前以及过去测试集中的异常情况。


%基于key-prompt-mutilmodal konwledge结构的回放机制
\subsection{Key-Prompt-Multimodal Knowledge Mechanism}
Using MTRMB to solve the problem of catastrophic forgetting in continuous learning faces three major problems: 1) Existing unsupervised anomaly detection methods have serious catastrophic forgetting problems. 2) Single-modal feature representations are difficult to fully retain the intrinsic information of old tasks. 3) It is costly to collect abnormal data for different tasks in industrial scenarios, and an effective continuous learning mechanism is required. To solve the above problems, we propose a key prompt-multimodal knowledge (KPMK) mechanism, which can effectively fuse text (BERT) and image (ViT) cross-modal features to build a memory bank that can retain the essential knowledge of the task. This structure only stores positive sample knowledge that takes up less memory and lightweight prompt vectors and keys, which can effectively complete task key queries, task adaptation, knowledge transfer, and complete incremental anomaly detection for different tasks.

%采用MTRB解决持续学习中的灾难遗忘问题面对三大问题：1）现有无监督异常检测方法存在严重的灾难性遗忘问题 2）单一模态的特征表示难以完整保留老任务的内在信息 3）工业场景中收集不同任务的异常数据成本高昂，需要有效的持续学习机制。为解决以上问题，我们提出关键提示-多模态知识(KPMK)机制, 该机制能够将文本(BERT)和图像(ViT)跨模态特征有效融合，以此构建一个能保留任务本质知识的记忆库。该结构仅保存占内存较少的正样本知识和轻量的提示向量和key，就能有效完成任务键查询、任务适应、知识转移，完成对不同任务的增量异常检测。

In the task key query stage, we selected the feature vector extracted from the fifth layer of the pretrained vision transformer (ViT) on normal data as the task key. This is because the features of the middle layer often contain rich contextual information, which can better represent the task itself. In order to obtain a more effective key, we used the FPS method to further condense the key set of all tasks.

%在任务键查询阶段，我们选择了pretrained vision transformer(ViT)中第五层在正常数据上提取的特征向量作为任务Key。因为中间层特征的往往包含丰富的上下文信息，这能更好表征任务本身。为了获得更加有效的Key，我们采用FPS方法进一步浓缩了所有任务的key集合。

%在任务适应阶段，我们采用 Prefix Tuning 将可学习提示 $P \in \mathbb{R}^{L_p \times D}$ 插入到预训练 ViT 的不同层中。这里，L_p表示提示长度，D表示嵌入维度。给定一个输入图像X \in \mathbb{R}^{H \times W \times C}，通过 ViT 处理以获得特征嵌入 X_e \in \mathbb{R}^{L \times D}。然后将可学习提示与 X_e连接起来，形成最终输入：
In the task adaptation phase, we adopt Prefix Tuning to insert learnable prompts $P \in \mathbb{R}^{L_p \times D}$ into different layers of a pre-trained ViT. where, $L_p$ denotes the prompt length, and $D$ represents the embedding dimension. Given an input image $X \in \mathbb{R}^{H \times W \times C}$, it is processed through ViT $f$ to obtain the feature embedding $ X_e \in \mathbb{R}^{L \times D} $. The learnable prompt is then concatenated with $X_e$, forming the final input:

\begin{equation}
    f(X_e, P) = \text{MSA}(W_i^Q X_e, [P_k; W_i^K X_e], [P_v; W_i^V X_e])
\end{equation}

where $ W_i^Q, W_i^K, W_i^V $ are projection matrices.

Furthermore, each prompt $ P_i $ is associated with a learnable key, represented as $ (K_i, P_i) $, where $ K_i \in \mathbb{R}^{D} $. A query function $ q(x) $ maps the input features to the key space, and the best-matching key is selected based on cosine similarity:

\begin{equation}
    L_{kp} = \arg\min \sum_{i=1}^{N} \gamma(q(x), K_i)
\end{equation}

where $ \gamma $ denotes cosine similarity, and $ N $ is the number of keys.


%%%In the task key query stage, the goal is to find an efficient key that can represent the current task. We selected the feature vector extracted from the 5 $th$ layer of the pretrained vision transformer (ViT) on normal data as the task key. Because the features of the middle layer often contain rich contextual information, which can better represent the task itself. To obtain a more effective key, we use the $FPS$\cite{eldar1997farthest} method to find the most representative normal sample $X^*$ in the normal data set, and determine the feature vector extracted from the fifth layer of ViT on $X^*$ as the task key. Finally, the key set of all tasks is obtained:$K = \{ K^0, K^1, \dots, K^t \}$where $K^t$ represents all extracted embeddings of task $t$. 



% In the task identification phase, images $x \in \mathbb{R}^{H \times W \times C}$ will go through a frozen pretrained vision transformer(ViT) $f(ViT)$ to extract keys $ k \in K$, also known as task identities. Because task identity contains both textual details and high-level information, we use a specific layer of ViT rather than the last embedding $k = f^i(x), k \in \mathbb{R}^{N_p \times C}$, in which $k$ is the feature and $N_p$ is the num of patches after $i$-th block (in this paper, we use $i$ = 5). However, assuming we have $N_I$ training images for task $t$, all extracted embeddings would have dimension $K^t \in \mathbb{R}^{N_I \times N_p \times C}$, which means a lot of memory space. To make task matching efficient during testing, we propose to use one image’s feature space representing the whole task $\mathbb{R}^{N_I \times N_p \times C} \rightarrow \mathbb{R}^{N_p \times C}$ . Note that a single image’s feature space is negligible compared to the whole task in the continual training setting. We find that the farthest point sampling method is efficient for selecting representative features to serve as keys. So task identities $K$ can be represented as a set:

% \begin{equation}
% \begin{aligned}
% K_e^t &= \text{FPS}(K_e^t), \quad K_e^t \in \mathbb{R}^{N_p \times C}\\
% K_e &= \{ K_e^0, K_e^1, \dots, K_e^t \}, \quad t \in \mathbb{N}_{\text{task}}
% \end{aligned}
% \end{equation}

% where $FPS$ is furthest point sampling, $K^t$ represents all extracted embeddings of task $t$.

%在任务适应阶段，参考A的方式，我们通过由上文获得的Task key选择一个提示子集。然后，将选择的提示加到input tokens，最后将extended tokens提供给模型。

% In the task adaptation phase, we select a subset of prompts using the Task key obtained above. Then, we use prefix tuning to add the selected prompts to the input tokens and finally provide the extended tokens to the multi-head self-attention (MSA) layers in a pre-trained ViT. We define a prompt parameter as $p\in \mathbb{R}^{L_p \times D}$ where $L_p$ is the prompt length (chosen as a hyperparameter) and $D$ is the embedding dimension (768). Consider an MSA layer with input $h \in \mathbb{R}^{L \times D}$, and query, key, and values given as $h_Q$, $h_K$, $h_V$ , respectively. In the ViT model, $h_Q = h_K = h_V = h$. The output of this layer is given as:

%  \begin{equation}
% \begin{aligned}
% MSA(h_Q,h_K,h_V) &= \text{Concat}(h_1,\dots,h_m)W^O \\
% where \quad h_i= \text{Attention}&(h_QW_i^Q,h_KW_i^K,h_VW_i^V)
% \end{aligned}
% \end{equation}

% where $W^O, W_i^Q, W_i^K$, and $W_i^V$ are projection matrices and $m$ is the number of heads. We split our prompt $p$ into ${p_K, p_V } \in \mathbb{R}^{L_p/ 2 \times D}$ and prepend them to $h_K$ and $h_V$ as:
% \begin{equation}
% \begin{aligned}
% f(p,h)=MSA(h_Q,[p_K;h_K],[p_V;h_V]) 
% \end{aligned}
% \end{equation}


% %为了使key与prompt更加匹配，我们引入以下损失函数：
% In order to make the key and prompt more matched, we introduce the following loss function:
% \begin{equation}
% \begin{aligned}
%     K_x &= \mathop{\arg \min}_{m \in \{1,\dots,M\}}\sum_{i=1}^{N}\gamma(q(x),k_t)\\
%     L_{kp}&=\sum_{Kx}\gamma(f(p,h),k_t)
% \end{aligned}
% \end{equation}



%在知识转移阶段，为了获得更加全面的表征，我们通过跨模态注意力机制融合了文本与图像信息.具体而言，Text-to-Image Attention将图像特征I作为查询Q，文本特征T作为K和V，使得模型可以根据文本内容调整图像，从而让图像中包含文本描述的关键信息。Image-to-Text Attention，文本特征T作为Q，图像特征I作为K和V，通过文本特征和图像特征的相似度，模型可以根据图像特征来调整文本特征，从而生成更准确的文本表示。公式如下：

In the knowledge transfer stage, To obtain a more comprehensive representation, we fuse text and image information through a cross-modal attention mechanism, as shown in Figure \ref{MTRMB} (c).  Specifically, Text-to-Image Attention employs image features $I$ as query $Q$ and text features $T$ as the key $K$ and value $V$, enabling the model can enhance the image representations based on textual information,thereby incorporating key content described in the text into the image features. Image-to-Text Attention utilizes text features $T$ as the query $Q$ and image features $I$ as the key $K$ and value $V$. By computing the similarity between text and image features, the model refines the textual representations based on the image features, leading to more accurate text descriptions.The formula is as follows:

\begin{equation}
\begin{aligned}
&Q=IW_Q,\quad K=TW_K,\quad V=TW_V\\
&Attention(Q,K,V)=softmax(QK^T/\sqrt{D_K})
\end{aligned}
\end{equation}



\begin{equation}
\begin{aligned}
&Q=TW_Q,\quad K=IW_K,\quad V=IW_V\\
&Attention(Q,K,V)=softmax(QK^T/\sqrt{D_K})
\end{aligned}
\end{equation}

%最后，形成与任务相关的 $\{K,P,MK_n\}$ 以进行连续异常检测，从而将知识从先前的任务转移到当前图像。但是，存储在 $MK_n$ 中的特征可能不够具有辨别性，因为主干已经过预训练并且不适应当前任务。为了使特征表示更加紧凑，我们开发了 RSCL 来快速对比学习。

Finally, a task-related $\{K,P,MK_n\}$ is formed for continuous anomaly detection, which transferring knowledge from the previous task to the current image. However, the features stored in $MK_n$ may not be discriminative enough because the backbone has been pre-trained and is not adapted to the current task. To make the feature representation more compact, we develop RSCL for fast contrastive learning.

% During the task adaptation phase, inspired by\cite{jia2022visual} which injects new knowledge into models, we design learnable prompts $P$ to transfer task-related information to the current image. Unlike $K$ that is downsampled from the ViT, prompts $p\in P$ are purely learnableto accommodate the current task, We add a prompt $p$ to each layer's input feature to convey task information to the current image, $k^i= f^i(k^{i-1} + p^i)$, where $k^i$ is the output feature of the $i$-th layer, $k^{i-1}$ is the input feature, and $p^i$ is the prompt added to the $i$-th layer to transfer task-specific information to the current image. Then, the task-transferred image features $k^i$ are used to create the mutilmodal knowledge $MK_n$ during training. Since we are not using supervision, $MK_n$ serves as the standard to distinguish anomaly data by comparing it to the test image features. However, image features can be exceedingly large during training accumulation, we use coreset sampling\cite{li2022towards} to reduce storage for $P$.
% \begin{equation}
% \begin{aligned}
% K_n &= \text{CoreSetSampling}(k^i) \\
% &= \mathop{\arg \min}_{M_c \subset M} \max_{m \in M} \min_{n \in M_c} \| m - n \|_2
% \end{aligned}
% \end{equation}

% where $M$ is the nominal image features during training, $M_c$ is the coreset space for patch-level features $k^i$,and i=5 in our experiments since middle features contain both contextand semantic information. After establishing key-prompt-mutilmodal knowledge correspondance for each task, our proposed key-prompt-mutilmodal knowledge mechanism can successfully transfer knowledge from previous tasks to the current image. However, the features stored in $MK_n$ may not be discriminative enough because the backbone $f$ has been pretrained and not adapted to the current task. As the original backbone was trained on natural images, we modified it to improve its feature representation for industrial images. Industrial images mainly contain information about texture and edge structure, and the similarity between different industrial product images is often high. This allows us to use fewer features to representnormal industrial images. To make feature representationsmore compact, we developed a refined structure-based contrastive learning method to learn prompt contrastively.

%为了获得准确的结构区域，我们探索了由SAM和Grounding Dino组成的结构区域生成器。

\subsection{Refined Structure-based Contrastive Learning}
Inspired by ReConPatch \cite{hyun2024reconpatch}, we propose a refined structure-based contrastive learning method aimed at enhancing network representations for patch-level comparisons during testing, as shown in Figure \ref{MTRMB} (a). Our approach leverages Grounding Dino and SAM to consistently provide general structure knowledge, such as segmentation masks, without requiring additional training. 
As illustrated in Figure \ref{fig2}, for each image in the training set, we employed Grounding Dino and SAM to generate segmentation masks, in which different regions represent distinct structures or semantics. Simultaneously, guided by prompts, We use $f(P,X_e)\in \mathbb{R}^{N \times M \times C}$ to represent the patch-level features extracted by ViT, where $N$ represents the batch size, $M$ represents the number of patch features of each sample, and $C$ represents the dimension of each patch feature. Grounding dino and SAM are utilized to segment input images and generate pseudo labels $Y\in \mathbb{R}^{N \times H \times W}$, where $N$ represents the batch size, and $H$ and $W$ represent the height and width of each pseudo label, respectively.

%我们使用F∈RN*M*C表示ViT提取的patch级的特征，其中N表示批次的大小，M表示每个样本的特征数量，C表示每个patch特征的维度。通过grounding dino与SAM进行模型级别，对输入的图像进行分割，生成伪标签Y∈RN*(H*W)，其中N表示批次的大小，H,W分别表示每个伪标签高和宽。

The loss function is:

\begin{equation}
    S_{i,j} = \frac{\cos(f_i(P,X_e), f_j(P,X_e))}{\tau},\quad
        M_{i,j} =
    \begin{cases}
      1 & \text{if } Y_i = Y_j \\
      0 & \text{else}
    \end{cases}
\end{equation}

%区域级对比损失如下
The region-level contrast loss is as follows:

\begin{equation}
    L_{\text{costra}} = \frac{1}{M^2} \sum_{i=1}^{M} \sum_{j=1}^{M} \left( -S_{i,j} * M_{i,j} + (1 - M_{i,j}) * e^{S_{i,j}} \right)
\end{equation}

where $S \in \mathbb{R}^{M \times M}$ represents the similarity matrix between features, and $\tau$ is the temperature coefficient,which represents the similarity between the $i$-th eigenvector and the $j$-th eigenvector, and the similarity measurement uses cosine similarity.$f_i(P,X_e), f_j(P,X_e)$ respectively represent the representation of the $i$-th patch and the $j$-th patch of the current feature map. $M\in\mathbb{R}^{M \times M}$ is a binary mask matrix. $Y_i$ represents the label of the $i$-th patch. When the labels of two patches are the same, the value of the mask matrix is 1, indicating a positive sample pair; when the labels are different, the value of the mask matrix is 0, indicating a negative sample pair.
%其中，SRM*M表示特征之间的相似度矩阵，为温度系数。表示第i个特征向量与第j个特征向量的相似度，相似度度量使用余弦相似度。 ，分别表示当前特征图的第i个patch和第j个patch的表征。为二值掩码矩阵，表示第i个patch的标签。当两个patch的标签相同时，掩码矩阵的值为1，表示正样本对；当标签不同时，掩码矩阵的值为0，表示负样本对。

%跨模态对比损失如下
The cross-modal contrast loss is as follows:

\begin{equation}
\begin{aligned}
    L_{\text{cross}} = - \frac{1}{N \times M} \sum_{i=1}^{N} \sum_{j=1}^{M} \log \left( \frac{e^{\frac{\cos(f_{i,j}(P,X_e), T_{i,j})}{\tau}}}{\sum_{k=1}^{M} e^{\frac{\cos(f_{i,k}(P,X_e), T_{i,k})}{\tau}}} \right)
\end{aligned}
\end{equation}

where $T\in \mathbb{R}^{N\times 1\times D}$ denotes features extracted by the text encoder,$N$ represents the batch size and $D$ is the dimensionality of the text feature.

The Total loss is as follows:

\begin{equation}
    L_{\text{all}} = L_{\text{contra}} + L_{\text{cross}}+ \lambda L_{kp}
\end{equation}





\subsection{Inference Process}
% During testing, the system retrieves stored knowledge for anomaly detection. For a test image $x$, the system first finds a relevant prompt $P$ using a key-value query mechanism. The prompt $P$ is then concatenated with the embedded feature $X_e$ of the image to form a patch-level feature map, guiding the model to extract features more relevant to the task. By calculating the similarity between this feature map and the image knowledge $K_f$ stored in the memory pool, the system identifies the most relevant knowledge $K_1$ corresponding to the current task. Once the relevant knowledge is obtained, the system calculates the maximum and minimum values of the distances between the patch features of the current test image and those of the stored normal images, thereby constructing an anomaly score.

% The related formulas are as follows:

% \begin{equation}
% [ MK_{t} = \arg\min_{k_{1} \in K_{t} \in MK} \sum_{i=1}^{N} \min_{j \in N} ||V_{o}([X_{a}, P]) - K_{I}||_{2} ]


% where $V_o$ represents the patch-level feature extracted by the pre-trained VIT network at layer $o$ after concatenating the embedded feature of the test image with the relevant prompt $P$. Here, $o=5$. $F^{test}$ denotes the patch-level feature extracted by the VIT network at layer $o$ from the test image, and $F$ represents the corresponding patch feature of the stored normal image. By finding the minimum distance between the same patch features, the matched test and stored patches are obtained. Then, the Euclidean distance between these corresponding patch features is calculated to derive their anomaly score.
% \documentclass{article}
% \usepackage{amsmath}
% \begin{document}

As shown in Figure \ref{MTRMB} (b), during the inference, MTRMB retrieves stored knowledge for anomaly detection. For a test image $x$, MTRMB first finds a relevant prompt $P$ using a key-value query mechanism. The prompt $P$ is then concatenated with the embedded feature $X_e$ of the image to form a patch-level feature map, guiding the model to extract features more relevant to the task. By calculating the similarity between this feature map and the image knowledge $K_I$ stored in the memory bank, the system identifies the most relevant knowledge $k_I$ corresponding to the current task. Once the relevant knowledge is obtained, the system calculates the maximum and minimum values of the distances between the patch features of the current test image and those of the stored normal images, thereby constructing an anomaly score.

The related formulas are as follows:
\begin{equation}
\begin{aligned}
    MK_{t} = \mathop{\arg \min}_{k_I \in K_I \in MK}\sum _{i=1}^{N}\min _{j \in N}||V_o([X_e, P])-K_{I}||_{2}
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
F^{test}, F=\mathop{\arg \max}_{F^{test}\in V_o([X_e, P])}\mathop{\arg\min }_{F\in K_{n}^{t}}||F^{test}-F||_{2}
\end{aligned}
\end{equation}


\begin{equation}
\begin{aligned}
 S=||F^{test}-F||_{2} 
\end{aligned}
\end{equation}


where $V_o$ represents the patch-level feature extracted by the pre-trained VIT network at layer $o$ after concatenating the embedded feature of the test image with the relevant prompt $P$. Here, $o=5$. $F^{test}$ denotes the patch-level feature extracted by the VIT network at layer $o$ from the test image, and $F$ represents the corresponding patch feature of the stored normal image. By finding the minimum distance between the same patch features, the matched test and stored patches are obtained. Then, the Euclidean distance between these corresponding patch features is calculated to derive their anomaly score.

% \end{document}

% To automatically determine the task identity during testing, an image $x^{test}$ initially locates its corresponding task by selecting from $K$ based on the highest similarity. The corresponding task identity is selected by the equation below:

% \begin{equation}
% \begin{aligned}
%     MK_e^t &= \mathop{\arg \min}_{m \in MK_e} \, \text{Sim}(m - m^{\text{test}})\\
%     \text{Sim}(m - m^{\text{test}}) &= \sum_{x \in N_p} \min_{y \in N_p} \| m_x - m_y^{\text{test}} \|_2
% \end{aligned}
% \end{equation}

% where $m^test$ is the patch-level feature from $i$-th layer featuremap of ViT containing multiple patches $N_p$, $i$= 5 in this paper as discussed in previous section. Since the utilization of a key-prompt-mutilmodal knowledge mechanism, the associated prompts $P$ and mutilmodal knowledge $MK_n$, can be readily retrieved. By combining the selected prompts with test patches and processing them through ViT, features from the test sample are adapted and extracted. Subsequently, anomaly scores are calculated based on the minimum distance to the task's knowledge $MK_n^t$.


% \subsection{Anomaly Detection and Segmentation}
% To calculate the anomaly score, we compare the image feature $m^{test}$ with the nominal features stored in task-specific knowledge base $MK_n^t$. Building upon the patch-level retrieval, we employed re-weighting to implement the anomaly detection process. $N_b(m^*)$ represents the nearest neighbors of $m^*$ in $MK_n^t$. We use the distance between $m^{test}$ and $m^*$ as the basic anomaly score, and then calculate the distance between mtest and the features in $N_b(m)$ to achieve the re-weighting effect. Through Eqution \ref{eq8}, we set the furthest distance between feature $m^{test,*}$in the test feature set $P(x^{test})$ and memory bank $MK_n^r$ to represent the anomaly score $s^*$ of the sample.
% \begin{equation}
% \begin{aligned}
% m^{\text{test},*}, m^* &= \mathop{\arg\min}_{m^{\text{test}} \in P(x^{\text{test}})} \, \mathop{\arg \min}_{m \in MK_n^{t}} \| m^{\text{test}} - m^l \|_2\\
% s^* &= \| m^{\text{test},*} - m^* \|_2
% \end{aligned}\label{eq8}
% \end{equation}





\section{Experiments and Discussion}\label{exp_dis}
\subsection{Experiments Setup}
\textbf{Datasets:} MVTec AD \cite{bergmann2019mvtec} is the most widely used dataset for industrial image anomaly detection. VisA \cite{zou2022spot} is now the largest dataset for real-world industrial anomaly detection with pixel-level annotations. We conduct experiments on these two datasets.
\begin{figure}[t]
	%\centering 
	%%\centering
	\includegraphics[width=1\columnwidth, trim=0 0 0 0, clip]{tsne_vis.pdf} 
	\caption{The T-SNE Visualization results of MTRMB. The dimensionality reduction visualization results of 15 classes show that the proposed method can generate a compact task representation memory bank for UCAD. It should be noted that the arc\-shaped dimensionality reduction feature sets are all generated by the background area. This is because the background areas of each category in the MVTec AD dataset are very similar.}
	\label{tsne} 
\end{figure}
\textbf{Methods:}Based on the anomaly methods discussed in our related work section and previous benchmark \cite{xie2024iad}, we selected the most representative methods from each paradigm to establish the benchmark. These methods include  CutPaste \cite{li2021cutpaste}, CSFlow \cite{rudolph2022fully},
Fastflow \cite{yu2021fastflow},
FAVAE \cite{dehaene2020anomaly},
SimpleNet \cite{liu2023simplenet},
DRAEM \cite{zavrtanik2021draem},
PaDiM \cite{defard2021padim},
SPADE \cite{cohen2020sub},
STPM \cite{salehi2021multiresolution},
PatchCore \cite{roth2022towards},
CFA \cite{lee2022cfa},
DNE \cite{li2022towards}, 
RD4AD \cite{deng2022anomaly}, 
UniAD \cite{you2022unified} and UCAD \cite{liu2024unsupervised}.

\begin{figure}[t]
	%\centering 
	%%\centering
	\includegraphics[width=0.8\columnwidth, trim=0 0 0 0, clip]{sam_vis.pdf} 
	\caption{The first row shows the test images, the second row shows the structure regions generated by SAM, and the third row shows the structure regions generated by the effective interaction between only Grounding DINO and SAM. The segmented region visualization shows that our method can generate more accurate structure regions to guide the model to learn the key parts of the object.}
	\label{gd} 
\end{figure}

\textbf{Metrics:}Following the common practice, we utilize Area Under the Receiver Operating Characteristics (AUROC/AUC) to assess the model’s ability in anomaly classification. For pixel-level anomaly segmentation capability, we employ Area Under Precision-Recall (AUPR/AP) \cite{bergmann2020uninformed} for model evaluation. In addition, we use Forgetting Measure(FM) \cite{chaudhry2018riemannian} to evaluate models’ ability to prevent catastrophic forgetting.

\begin{equation}
    \resizebox{.91\linewidth}{!}{$
            \displaystyle
            avg FM = \frac{1}{(k-1)k} \sum_{j=1}^{k-1} \mathop{max}\limits_{l\in\{1,...,k-1\}} T_{l,j}-T_{k,j}
        $}
\end{equation}

where \textbf{$T$} represents tasks, $k$ stands for the current training task ID, and $j$ refers to the task ID being evaluated. And $avg FM$ represents the average forgetting measure of the model after completing $k$ tasks. During the inference, we evaluate the model after training on all tasks.

% \textbf{Training Details:} We utilized the $vit-base-patch16-224$ backbone pretrained on ImageNet 21K\cite{deng2009imagenet} for our method. During prompt training, we employed a batch size of 8 and adapt Adam optimizer\cite{kingma2014adam} with a learning rate of 0.0005 and momentum of 0.9. The training process spanned 25 epochs. Our key-prompt-knowledge structure comprised a key of size (15, 196, 1024) float array, a prompt of size (15, 7, 768) float array, and knowledge of size (15, 196, 1024) float array, with an overall size of approximately 23.28MB.


%MTRMB在TSNE的可视化结果.15个类的降维可视化结果表明所提出方法能够生成紧凑的任务表征内存池用于UCAD.需要说明的是，圆弧形的降维特征集合均由背景区域产生，这是因为MVTec AD数据集中每个类别的背景区域极为相似。

\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccccccccccccc}
\hline
Methods  & Bottle & Cable & Capsule & Carpet& Grid& Hazelnut & Leather & Metal$\_$Nut& Pill& Screw & Tile& Toothbrush & Transistor& Wood & Zipper & Average& Avg FM\\
\hline
CFA & 0.309  & 0.489 & 0.275 & 0.834 & 0.571 & 0.903 & 0.935 & 0.464 & 0.528 & 0.528 & 0.763 & 0.519 & 0.320 & 0.923 & 0.984 & 0.623 & 0.361\\
CSFlow & 0.129  & 0.420 & 0.363 & 0.978 & 0.602 & 0.269 & 0.906 & 0.220 & 0.263 & 0.434 & 0.697 & 0.569 & 0.432 & 0.802 & 0.997 & 0.539 & 0.426\\
CutPaste  & 0.111& 0.422 & 0.373 & 0.198& 0.214& 0.578 & 0.007 & 0.517& 0.371& 0.356 & 0.112& 0.158 & 0.340& 0.150 & 0.775 & 0.312& 0.510\\
DRAEM  & 0.793& 0.411 & 0.517& 0.537& 0.799& 0.524 & 0.480 & 0.422& 0.452& 1.000 & 0.548& 0.625 & 0.307& 0.517 & 0.996 & 0.595& 0.371\\
FastFlow  & 0.454& 0.512 & 0.517& 0.489& 0.482& 0.522 & 0.487 & 0.476& 0.575& 0.402 & 0.489& 0.267 & 0.526& 0.616 & 0.867 & 0.512& 0.279\\
FAVAE  & 0.666& 0.396 & 0.357&0.610& 0.644& 0.884 & 0.406 & 0.416& 0.531& 0.624 & 0.563& 0.503 & 0.331& 0.728 &0.544 & 0.547& 0.102\\
PaDiM & 0.458 & 0.544  & 0.418 & 0.454 & 0.704 & 0.635 & 0.418 & 0.446 & 0.449 & 0.578 & 0.581 & 0.678 & 0.407 & 0.549 & 0.855 & 0.545 & 0.368\\
Patchcore  & 0.163 & 0.518 & 0.350 & 0.968 & 0.700 & 0.839 & 0.625 & 0.259 & 0.459 & 0.484 & 0.776 & 0.586 & 0.341 & 0.970 & 0.991 & 0.602 & 0.383\\
RD4AD  & 0.401 & 0.538 & 0.475 & 0.583 & 0.558 & 0.909 & 0.596 & 0.623 & 0.479 & 0.596 & 0.715 & 0.397 & 0.385 & 0.700 & 0.987 & 0.596 & 0.285\\
SPADE  & 0.302 & 0.444 & 0.525 & 0.529 & 0.460 & 0.410 & 0.577 & 0.592 & 0.484 & 0.514 & 0.881 & 0.386 & 0.622 & 0.897 & 0.949 & 0.571& 0.393\\
STPM & 0.329 & 0.539  & 0.610 & 0.462 & 0.569 & 0.540 & 0.740& 0.456 & 0.523 & 0.753 & 0.736 & 0.375 & 0.450 & 0.779 & 0.783 & 0.576 & 0.325\\
SimpleNet & 0.938 & 0.560  & 0.519 & 0.736 & 0.592 & 0.859 & 0.749 & 0.710 & 0.701 & 0.599 & 0.654 & 0.422 & 0.669 & 0.908 & 0.996 & 0.708 & 0.211\\
UniAD  & 0.801  & 0.660 & 0.823 & 0.754 & 0.713 & 0.904 & 0.715 & 0.791 & 0.869 & 0.731 & 0.687 & 0.776 & 0.490 & 0.903 & 0.997 & 0.774 & 0.229\\
\hline
Patchcore$^*$ & 0.533  & 0.505 & 0.351  & 0.865&0.723& 0.959 & 0.854 & 0.456& 0.511& 0.626 & 0.748& 0.600 & 0.427& 0.900 & 0.974 & 0.669& 0.318\\
UniAD$^*$  & 0.997  & 0.701 & 0.765 & \textbf{0.998} & 0.896 & 0.936 & \textbf{1.000} & 0.964 & 0.895 & 0.554 & 0.989 & 0.928 & \textbf{0.966} & 0.982 & \textbf{0.987} & 0.904 & 0.076\\
DNE & 0.990 & 0.619 & 0.609 & 0.984 & \textbf{0.998} & 0.924 & \textbf{1.000} & 0.989 & 0.671 & 0.588 & 0.980 & 0.933 & 0.877 & 0.930 & 0.958 & 0.870 & 0.116\\
UCAD &  0.995  & \textbf{0.731}  &  0.866  & 0.965 & 0.944& \textbf{0.994}& 0.996 & 0.988  & 0.890 &0.739 &  \textbf{0.998}& 0.978 & 0.874 & \textbf{0.995} & 0.938 & 0.926 & 0.056 \\
\hline
Ours & \textbf{1.000} & 0.652 & \textbf{0.926} & 0.970 & 0.976 & 0.990 & \textbf{1.000} & \textbf{0.995} & \textbf{0.892} & \textbf{0.852} & 0.997 & \textbf{1.000} & 0.926 & 0.991 & 0.949 & \textbf{0.941} & \textbf{0.016}\\
\bottomrule  
\end{tabular}
}
\caption{Image-level $AUROC\uparrow$ and corrsponding $FM\downarrow$ on MVTec AD dataset after training on the last subdataset. The best results are highlighted in bold.}
\label{mvtec_image_auc}
\end{table*}



\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccccccccccccc}
\hline
Methods  & Bottle & Cable & Capsule & Carpet& Grid& Hazelnut & Leather & Metal$\_$Nut& Pill& Screw & Tile& Toothbrush & Transistor& Wood & Zipper & Average& Avg FM\\
\hline
CFA & 0.068  & 0.056 & 0.050 & 0.271 & 0.004 & 0.341 & 0.393 & 0.255 & 0.080 & 0.015 & 0.155 & 0.053 & 0.056 & 0.281 & 0.573 & 0.177 & 0.083\\
DRAEM & 0.117  & 0.019 & 0.044 & 0.018 & 0.005 & 0.036 & 0.013 & 0.142 & 0.104 & 0.002 & 0.130 & 0.039 & 0.040 & 0.033 & 0.734 & 0.098 & 0.116\\
FastFlow & 0.044  & 0.021 & 0.013 & 0.013 & 0.005 & 0.028 & 0.007 & 0.090 & 0.029 & 0.003 & 0.060 & 0.015 & 0.036 & 0.037 & 0.264 & 0.044 & 0.214\\
FAVAE & 0.086  & 0.048 & 0.039 & 0.015 & 0.004 & 0.389 & 0.112 & 0.174 & 0.070 & 0.017 & 0.064 & 0.043 & 0.046 & 0.093 & 0.039 & 0.083 & 0.083\\
PaDim  & 0.072 & 0.037 & 0.030 & 0.023 & 0.006 & 0.183 & 0.039 & 0.155 & 0.044 & 0.014 & 0.065 & 0.044 & 0.049 & 0.080 & 0.452 & 0.086 & 0.366\\
Patchcore  & 0.048 & 0.029 & 0.035 & 0.552 & 0.003 & 0.338 & 0.279 & 0.248 & 0.051 & 0.008 & 0.249 & 0.034 & 0.079 & 0.304 & 0.595 & 0.190 & 0.371\\
RD4AD  & 0.055 & 0.040 & 0.064 & 0.212 & 0.005 & 0.384 & 0.116 & 0.247 & 0.061 & 0.015 & 0.193 & 0.034 & 0.059 & 0.097 & 0.562 & 0.143 & 0.425\\
SPADE & 0.122 & 0.052 & 0.044 & 0.117 & 0.004 & 0.512 & 0.264 & 0.181 & 0.060 & 0.020 & 0.096 & 0.043 & 0.050 & 0.172 & 0.531 & 0.151 & 0.319\\
STPM & 0.074 & 0.019 & 0.073 & 0.054 & 0.005 & 0.037 & 0.108 & 0.354 & 0.111 & 0.001 & 0.397 & 0.046 & 0.046 & 0.119 & 0.203 & 0.110 & 0.352\\
SimpleNet & 0.108 & 0.045 & 0.029 & 0.018 & 0.004 & 0.029 & 0.006 & 0.227 & 0.077 & 0.004 & 0.082 & 0.046 & 0.049 & 0.037 & 0.139 & 0.060 & 0.069\\
UniAD  & 0.734 & 0.232 & 0.313 & 0.517 & 0.204 & 0.378 & 0.360 & 0.587 & 0.346 & 0.035 & 0.428 & 0.398 & 0.542& 0.378 & 0.443 & 0.393 & 0.086\\
\hline
UniAD$^*$  & 0.054 & 0.031 & 0.022 & 0.047 & 0.007 & 0.189 & 0.053 & 0.110 & 0.034 & 0.008 & 0.107& 0.040 & 0.045& 0.103 & 0.444 & 0.086 & 0.419\\
Patchcore$^*$ & 0.087 & 0.043 & 0.042 & 0.407& 0.003 & 0.443 & \textbf{0.352} & 0.189& 0.058& 0.017 & 0.124& 0.028 & 0.053& 0.270 & \textbf{0.604} & 0.181& 0.343\\
UCAD &  0.751  & \textbf{0.271}  &  0.339  & 0.622 &0.185 & 0.506  & 0.333  & 0.765 & \textbf{0.634}& 0.214  & \textbf{0.549}& 0.288  & 0.398 & 0.535 & 0.388 & 0.451 & 0.023 \\
\hline
Ours & \textbf{0.753} & 0.244 & \textbf{0.344} & \textbf{0.637} & \textbf{0.190} & \textbf{0.519} & 0.337 & \textbf{0.811} & 0.631 & \textbf{0.232} & 0.546 & \textbf{0.301} & \textbf{0.483}& \textbf{0.573} & 0.420 & \textbf{0.468}& \textbf{0.017}\\
\hline
\end{tabular}
}
\caption{Pixel-level $AUPR\uparrow$ and corrsponding $FM\downarrow$ on MVTec AD dataset after training on the last subdataset.}
\label{mvtec_pixel_pr}
\end{table*}


\begin{figure*}[t]
	\centering 
	%%\centering
	\includegraphics[scale=0.28]{res_vis.pdf} 
	\caption{Visualization examples of continual anomaly detection. The first row displays the original anomaly images, the second row shows the ground truth annotations, and the third to fifth rows depict the heatmaps of our method and other methods.}
	\label{vis} 
\end{figure*} 


\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccccccccccccc}
\hline
Methods  & Candle & Capsules & Cashew & Chewinggum & Fryum & Macaroni1 & Macaroni2 & Pcb1 & Pcb2 & Pcb3 & Pcb4 & Pipe$\_$fryum & Average & AvgFM\\
\hline
CFA & 0.512   & 0.672 & 0.873 & 0.753 & 0.304 & 0.557 & 0.422 & 0.698 & 0.472& 0.449 & 0.407 & 0.998 & 0.593& 0.327\\
RD4AD     & 0.380 & 0.385 & 0.737 & 0.539 & 0.533 & 0.607 & 0.487 & 0.437& 0.672 & 0.343 & 0.187 & 0.999 & 0.525 & 0.423\\
Patchcore  & 0.401 & 0.605 & 0.624 & 0.907 & 0.334 & 0.538 & 0.437 & 0.527& 0.597 & 0.507 & 0.588 & 0.998 & 0.589 & 0.361\\
SimpleNet  & 0.504 & 0.474  & 0.794 & 0.721 & 0.684 & 0.567 & 0.447 & 0.598 & 0.629 & 0.538 & 0.493 & 0.945 & 0.616 & 0.283\\
UniAD  & 0.573  & 0.599 & 0.661 & 0.758 & 0.504 & 0.559 & 0.644 & 0.749 & 0.523 & 0.547 & 0.562 & 0.989 & 0.639 & 0.297\\
\hline
UniAD$^*$  & 0.884  & 0.669 & 0.938 & 0.970 & 0.812 & 0.753 & 0.570 & 0.872 & 0.766 & 0.708 & 0.967 & 0.990 & 0.825 & 0.125\\
DNE & 0.486 & 0.413 & 0.735 & 0.585 & 0.691 & 0.584 & 0.546 & 0.633 & 0.693& 0.642 & 0.562 & 0.747 & 0.610 & 0.179\\
Patchcore$^*$  & 0.647 & 0.579 & 0.669 & 0.735 & 0.431 & 0.631 & 0.624 & 0.617& 0.534 & 0.479 & 0.645 &  \textbf{0.999} & 0.633 & 0.349\\
UCAD &  0.778  & 0.877  &   \textbf{0.960}  & 0.958 &  \textbf{0.945}&  \textbf{0.823}  &  0.667 & \textbf{0.905}  &0.871 & 0.813  & 0.901& 0.988  & 0.874 &0.039 \\
\hline
Ours &  \textbf{0.946} &  \textbf{0.937} & 0.935 &  \textbf{0.989} & 0.874 & 0.783 &  \textbf{0.794} & 0.765&  \textbf{0.914}&  \textbf{0.938} &  \textbf{0.997}& 0.925 &  \textbf{0.900}&  \textbf{0.010} \\
\hline
\end{tabular}
}
\caption{Image-level $AUROC\uparrow$ and corrsponding $FM\downarrow$ on VisA dataset after training on the last subdataset.}
\label{visa_image_auc}
\end{table*}

%第一行为测试图片，第二行由SAM生成的结构区域，第三行通过SAM与Grounding DINO有效交互而产生的结构区域。分割区域可视化表明我们的方法能够产生更精确的结构区域用于引导模型对物体关键部件的学习。






\subsection{Continual Anomaly Detection Benchmark}
We conducted comprehensive evaluations of the aforementioned 15 methods on the MVTec AD and VisA datasets. Among them, UCAD stands as the SOTA method in unsupervised continual AD. Meanwhile, DNE and UniAD are two representative AD methods for continual and unified methods, respectively. Intuitively, these two methods appear to be better suited for the open learning scenario. Due to the famous replay in continual learning methods, we also conducted replay-based experiments on PatchCore and UniAD. Comprehensive experiments verify the superiority of our approach in persistent anomaly detection.
%我们在 MVTec AD 和 VisA 数据集上对上述 15 种方法进行了全面评估。其中，UCAD 是无监督连续 AD 中的 SOTA 方法。同时，DNE 和 UniAD 分别是连续和统一方法的两种代表性 AD 方法。直观地看，这两种方法似乎更适合开放学习场景。由于连续学习方法中著名的重放，我们还对 PatchCore 和 UniAD 进行了基于重放的实验。全面的实验验证了我们方法在持续性异常检测中的优越性。

\subsubsection{Quantitative Analysis}
As shown in Table \ref{mvtec_image_auc}-\ref{visa_image_auc}, our method outperforms other anomaly detection algorithms in both continuous detection and segmentation on the MVtec AD and VisA datasets. The detection and segmentation effects of common anomaly detection methods are greatly reduced in the continuous detection scenario. Since there are few AD studies with continuous anomalies, we add a playback mechanism to Patchcore and UniAD to facilitate further comparison. However, $Patchcore^*$ with continuous learning ability is still far lower than the proposed method in Image AUROC and Pixel AUROC by 0.27, 0.287 (MVTec AD), and $UniAD^*$ is 0.041 and 0.382 lower than our method, respectively. In addition, we also fully compared with the most advanced continuous anomaly detection methods DNE and UCAD. Our method is superior to DNE and UCAD in continuous detection ability while maintaining the lowest forgetting rate. On the MVTec AD dataset, compared with DNE and UCAD, our method has an Image AUROC that is 0.071 and 0.015 higher, respectively. On the VisA dataset, compared with DNE and UCAD, our method has an Image AUROC that is 0.29 and 0.026 higher, respectively. Comprehensive experiments show that our method can effectively resist catastrophic forgetting, and the proposed method has superior continuous anomaly detection capabilities. We attribute this to the task representation memory bank we designed, which obtains comprehensive task representations for continuous learning and unsupervised detection by strengthening the interaction of multimodal information.
%如表\ref{mvtec_image_auc}-\ref{visa_image_auc}所示,我们的方法在MVtech_AD和VisA数据集上的持续检测和分割均优于其它异常检测算法。普通的异常检测方法在持续检测场景下，检测和分割效果大幅度下降。由于目前具有持续异常的AD研究较少，我们为Patchcore和UniAD添加回放机制方便进行进一步比较。然而，具有持续学习能力的Patchcore^*在Image_AUC和Pixel_AUC上仍远低于所提出方法0.27，0.287(MVTec_AD)，UniAD^*分别低出我们的方法0.041和0.382.此外，我们也和最先进的持续异常检测方法DNE和UCAD进行了充分比较，我们的方法在保持最低遗忘率的前提下，在持续检测能力上均高于DNE和UCAD。在MVTec_AD数据集上，相比于DNE和UCAD，我们的方法在Image_AUC上分别高出0.071，0.015。在VisA数据集上，相比于DNE和UCAD，我们的方法在Image_AUC上分别高出0.29，0.026。全面的实验表明我们的方法能够有效的抵抗灾难性遗忘，所提出的方法具有优越的持续异常检测能力。我们把这归因于我们设计的任务表征内存池,其通过加强多模态信息的交互来获取全面的任务表征用于持续学习及无监督检测。


\subsubsection{Qualitative Analysis}
As shown in Fig.\ref{vis}, our method shows two obvious advantages over ADClip and UniAD. First, it demonstrates more accurate anomaly localization. Second, it minimizes false positives in normal image classification. As shown in Fig.\ref{gd}, our method is able to obtain better structural regions for contrastive learning, which greatly improves the model's learning of contextual features. To verify that the proposed method can generate an effective task representation memory bank, we performed T-SNE visualization of the memory bank after training 15 categories in MVTec AD, as shown in Figure\ref{tsne}. The results show that the proposed method can generate a task representation memory bank that is compact and not easy to forget knowledge (after incremental learning of 15 categories, the features of different categories are not confused together) for continuous learning and detection.

%如图\ref{vis}所示，与 ADCLP 和 UniAD 相比，我们的方法表现出两个明显的优势。首先，它展示了更精确的异常定位。其次，它最大限度地减少了正常图像分类中的误报。如图\ref{gd}所示,我们的方法能够获得更好的结构区域用于对比学习，这极大提高模型对上下文特征的学习。为了验证所提出方法能够生成有效的任务表征内存池，我们对在MVTec AD中15个类别进行训练后的内存池进行了T-SNE可视化，如图\ref{tsne}所示。结果表明，所提出方法能够生成紧凑且不易遗忘知识（经过增量学习15个类之后，所属不同类别的特征未混淆在一起）的任务表征内存池用于持续学习及检测。

\subsection{Ablation Study}

\begin{table}[ht]
% \centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{cccccc}
\toprule
RSCL  & KPMK & GD & Image\_AUC & Pixel\_AP \\
\midrule
\ding{55} & \ding{55} & \ding{55} & 0.894 & 0.426 \\
\checkmark & \ding{55} & \ding{55} & 0.924 & 0.447 \\
\checkmark & \checkmark & \ding{55} & 0.935 & 0.458 \\
\checkmark & \checkmark & \checkmark & \textbf{0.941} & \textbf{0.468} \\
\bottomrule
\end{tabular}
}
\caption{Ablation experiments of key modules (RSCL, KPMK, GD), GD is the abbreviation of Grounding DINO. We calculated on the average detection and segmentation metrics of MVTec AD and VisA.}

\label{modules}
\end{table}

%关键模块（RSCL，KPMK，GD）的消融实验，GD是Grounding dino的缩写。
\subsubsection{Module Effectivity}
As shown in Table \ref{modules}, we analyze the impact of three parts: KPMK, RSCL, and Grounding DINO. We observe that the performance of the model is significantly improved with the implementation of these modules. Regarding RSCL, we find that RSCL can effectively improve the continuous detection ability. The addition of this part can improve $Image\_AUC$ and $Pixel\_AP$ by 0.03 and 0.021 respectively. Without KPMK, our model uses a single-modal knowledge base and cannot be fully represented each time a new task is introduced. This approach limits the model's ability to continuously learn without supervision. KPMK improves $Image\_AUC$ and $Pixel\_AP$ by 0.011 and 0.011 respectively by strengthening the interaction of different modal information. By adding Grounding DINO, the model can be guided to better learn contextual features, which improves $Image\_AUC$ and $Pixel\_AP$ by 0.006 and 0.01 respectively. The experimental results show that the key components proposed in this paper can effectively improve the model's continuous learning ability in AD.

%如表\ref{modules}所示，我们分析了KPMK,RSCL和Grounding DINO三部分的影响。我们观察到，随着这些模块的实施，模型的性能得到了显着改善。关于RSCL，我们发现RSCL能够有有效提升持续检测能力，该部分的增加能够使Image_AUC和Pixel_AP分别提高了0.03和0.021。在没有KPMK的情况下，我们的模型使用单一模态的知识库，每次引入新任务时都无法得到全面的表征，这种方法限制了模型无监督持续学习的能力。KPMK通过加强不同模态信息的交互，使Image_AUC和Pixel_AP分别提高了0.011和0.011。通过增加Grounding DINO能够引导模型更好的学习上下文特征，使Image_AUC和Pixel_AP分别提高了0.006和0.01。实验结果表明，本文提出的关键部件能够有效提升模型在AD中的持续学习能力。

\begin{table}[h]
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{cccccc}
        \toprule
        Pt\_Len & Img\_AUC & Pix\_AUC & Img\_AP & Pix\_AP & Pix\_PRO \\
        \midrule
        1  & 0.936 & 0.971 & 0.962 & 0.472 & 0.900 \\
        5  & \textbf{0.941} & \textbf{0.976} & \textbf{0.965} & \textbf{0.488} & \textbf{0.908} \\
        10 & 0.931 & 0.975 & 0.958 & 0.457 & 0.903 \\
        15 & 0.925 & 0.974 & 0.955 & 0.470 & 0.903 \\
        \bottomrule
    \end{tabular}
    }
    \caption{Ablation experiments on prompt size hyperparameters. We calculated on the average detection and segmentation metrics of MVTec AD and VisA.}
    \label{hp}
\end{table}


%Prompt尺寸超参数的消融实验，实验结果是在MVTec_AD和VisA的的平均检测和分割指标。
\subsubsection{Impact of Prompt Hyperparameterss}
To further study the influence of hyperparameters in learnable prompts, we designed an ablation experiment as shown in Table \ref{hp}. By changing the length of the prompt, we observe the trend of changes in the five detection indicators to select the optimal hyperparameters. The prompt length is set to 1, 5, 10, and 15 respectively. From the results, the prompt length of 5 has the best detection and segmentation accuracy, and its excess over the suboptimal situation on $Image\_AUC$ and $Pixel\_AP$ is 0.005 and 0.016 respectively. The size of the prompt directly affects the effect of continuous learning, and choosing the appropriate prompt hyperparameters is critical.

%为了进一步研究可学习的Prompt中超参数的影响，我们设计了表\ref{hp}所示的消融实验。通过改变Prompt长度尺寸的大小来观察在5个检测指标的变化趋势来选择最优的超参数。Prompt长度分别设置为1，5，10，15，由实验结果可知，Prompt长度尺寸为5具有最优的检测和分割精度,其在Img_auc和Pix_ap上超出次优的情况分别为0.005和0.16。Prompt尺寸的大小直接影响持续学习的效果，选择合适的Prompt超参数是十分关键的。


\section*{Conclusion}
In this paper, we study the problem of applying incremental learning to unsupervised anomaly detection for practical applications in industrial manufacturing. To facilitate this research, we establish a comprehensive benchmark for unsupervised persistent anomaly detection and segmentation. In addition, our proposed MTRMB for the UCAD task is effective against catastrophic forgetting, which is the first study to apply multimodal prompt for incremental learning to unsupervised anomaly detection. The novelty of MTRMB relies on the KPMK mechanism and RSML, which utilizes the interaction of multimodal information to construct a compact task representation memory bank, which significantly improves the performance of persistent anomaly detection. Comprehensive experiments highlight the effectiveness and robustness of our framework under different hyperparameters. We also find that the persistent detection effect can be further improved by designing the prompt, which is an important direction for our subsequent optimization.

%在本文中，我们研究了将增量学习应用于无监督异常检测以解决工业制造中的实际应用的问题。为了促进这项研究，我们为无监督的持续异常检测和分割建立了一个全面的基准。此外，我们提出的用于 UCAD 任务的MTRMB能有效对抗灾难性遗忘，这是第一项将多模态提示增量学习应用于无监督异常检测的研究。MTRMB的新颖性依赖于KPMK机制和RSML，利用多模态信息的交互构造紧凑的任务表征内存池，这显著提高了持续异常检测的性能。全面的实验强调了我们的框架在不同超参数下的有效性和稳健性。我们还发现，通过对提示的设计可以进一步提高持续检测效果，这是后续我们优化的重要方向。
\appendix



%被提出的MCPM首次将多模态特征融合技术应用到持续性异常检测任务之中。

% \section*{Acknowledgments}
% This work received funding from the Guangdong Major Project of Basic and Applied Basic Research under Grant 2023B0303000009, the National Major Scientific Instruments and Equipments Development Project of National Natural Science Foundation of China under Grant 62327808, Cross-disciplinary team (Shenzhen University scientific research team cultivation project), under Grant 2023JCT004, Equipment Development Project of Shenzhen University 868/03010315 and the Open Research Fund from Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), National Natural Science Foundation of China Project under Grant No. 62403326.





%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
%\bibliography{ijcai24}
% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{bergmann2019mvtec}
P.~Bergmann, M.~Fauser, D.~Sattlegger, and C.~Steger, ``Mvtec ad--a comprehensive real-world dataset for unsupervised anomaly detection,'' in \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, 2019, pp. 9592--9600.

\bibitem{zou2022spot}
Y.~Zou, J.~Jeong, L.~Pemula, D.~Zhang, and O.~Dabeer, ``Spot-the-difference self-supervised pre-training for anomaly detection and segmentation,'' \emph{arXiv preprint arXiv:2207.14315}, 2022.

\bibitem{liu2024deep}
J.~Liu, G.~Xie, J.~Wang, S.~Li, C.~Wang, F.~Zheng, and Y.~Jin, ``Deep industrial image anomaly detection: A survey,'' \emph{Machine Intelligence Research}, vol.~21, no.~1, pp. 104--135, 2024.

\bibitem{xie2024iad}
G.~Xie, J.~Wang, J.~Liu, J.~Lyu, Y.~Liu, C.~Wang, F.~Zheng, and Y.~Jin, ``Im-iad: Industrial image anomaly detection benchmark in manufacturing,'' \emph{IEEE Transactions on Cybernetics}, 2024.

\bibitem{lee2022cfa}
S.~Lee, S.~Lee, and B.~C. Song, ``Cfa: Coupled-hypersphere-based feature adaptation for target-oriented anomaly localization,'' \emph{IEEE Access}, vol.~10, pp. 78\,446--78\,454, 2022.

\bibitem{gudovskiy2022cflow}
D.~Gudovskiy, S.~Ishizaka, and K.~Kozuka, ``Cflow-ad: Real-time unsupervised anomaly detection with localization via conditional normalizing flows,'' in \emph{Proceedings of the IEEE/CVF winter conference on applications of computer vision}, 2022, pp. 98--107.

\bibitem{rudolph2022fully}
M.~Rudolph, T.~Wehrbein, B.~Rosenhahn, and B.~Wandt, ``Fully convolutional cross-scale-flows for image-based defect detection,'' in \emph{Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision}, 2022, pp. 1088--1097.

\bibitem{li2021cutpaste}
C.-L. Li, K.~Sohn, J.~Yoon, and T.~Pfister, ``Cutpaste: Self-supervised learning for anomaly detection and localization,'' in \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, 2021, pp. 9664--9674.

\bibitem{zhang2023iddm}
F.~Zhang and Z.~Chen, ``Iddm: An incremental dual-network detection model for in-situ inspection of large-scale complex product,'' \emph{Journal of Industrial Information Integration}, vol.~33, p. 100463, 2023.

\bibitem{li2022towards}
W.~Li, J.~Zhan, J.~Wang, B.~Xia, B.-B. Gao, J.~Liu, C.~Wang, and F.~Zheng, ``Towards continual adaptation in industrial anomaly detection,'' in \emph{Proceedings of the 30th ACM International Conference on Multimedia}, 2022, pp. 2871--2880.

\bibitem{li2023cross}
W.~Li, B.-B. Gao, B.~Xia, J.~Wang, J.~Liu, Y.~Liu, C.~Wang, and F.~Zheng, ``Cross-modal alternating learning with task-aware representations for continual learning,'' \emph{IEEE Transactions on Multimedia}, 2023.

\bibitem{zhang2024realnet}
X.~Zhang, M.~Xu, and X.~Zhou, ``Realnet: A feature selection network with realistic synthetic anomaly for anomaly detection,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2024, pp. 16\,699--16\,708.

\bibitem{roth2022towards}
K.~Roth, L.~Pemula, J.~Zepeda, B.~Sch{\"o}lkopf, T.~Brox, and P.~Gehler, ``Towards total recall in industrial anomaly detection,'' in \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, 2022, pp. 14\,318--14\,328.

\bibitem{bergmann2020uninformed}
P.~Bergmann, M.~Fauser, D.~Sattlegger, and C.~Steger, ``Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings,'' in \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, 2020, pp. 4183--4192.

\bibitem{salehi2021multiresolution}
M.~Salehi, N.~Sadjadi, S.~Baselizadeh, M.~H. Rohban, and H.~R. Rabiee, ``Multiresolution knowledge distillation for anomaly detection,'' in \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, 2021, pp. 14\,902--14\,912.

\bibitem{deng2022anomaly}
H.~Deng and X.~Li, ``Anomaly detection via reverse distillation from one-class embedding,'' in \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, 2022, pp. 9737--9746.

\bibitem{tien2023revisiting}
T.~D. Tien, A.~T. Nguyen, N.~H. Tran, T.~D. Huy, S.~Duong, C.~D.~T. Nguyen, and S.~Q. Truong, ``Revisiting reverse distillation for anomaly detection,'' in \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, 2023, pp. 24\,511--24\,520.

\bibitem{batzner2024efficientad}
K.~Batzner, L.~Heckler, and R.~K{\"o}nig, ``Efficientad: Accurate visual anomaly detection at millisecond-level latencies,'' in \emph{Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision}, 2024, pp. 128--138.

\bibitem{liu2025grounding}
S.~Liu, Z.~Zeng, T.~Ren, F.~Li, H.~Zhang, J.~Yang, Q.~Jiang, C.~Li, J.~Yang, H.~Su \emph{et~al.}, ``Grounding dino: Marrying dino with grounded pre-training for open-set object detection,'' in \emph{European Conference on Computer Vision}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2025, pp. 38--55.

\bibitem{kirillov2023segment}
A.~Kirillov, E.~Mintun, N.~Ravi, H.~Mao, C.~Rolland, L.~Gustafson, T.~Xiao, S.~Whitehead, A.~C. Berg, W.-Y. Lo \emph{et~al.}, ``Segment anything,'' in \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, 2023, pp. 4015--4026.

\bibitem{jeong2023winclip}
J.~Jeong, Y.~Zou, T.~Kim, D.~Zhang, A.~Ravichandran, and O.~Dabeer, ``Winclip: Zero-/few-shot anomaly classification and segmentation,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2023, pp. 19\,606--19\,616.

\bibitem{cao2025adaclip}
Y.~Cao, J.~Zhang, L.~Frittoli, Y.~Cheng, W.~Shen, and G.~Boracchi, ``Adaclip: Adapting clip with hybrid learnable prompts for zero-shot anomaly detection,'' in \emph{European Conference on Computer Vision}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2025, pp. 55--72.

\bibitem{chen2023april}
X.~Chen, Y.~Han, and J.~Zhang, ``April-gan: A zero-/few-shot anomaly classification and segmentation method for cvpr 2023 vand workshop challenge tracks 1\&2: 1st place on zero-shot ad and 4th place on few-shot ad,'' \emph{arXiv preprint arXiv:2305.17382}, 2023.

\bibitem{cao2023winning}
Y.~Cao, X.~Xu, C.~Sun, Y.~Cheng, L.~Gao, and W.~Shen, ``Winning solution for the cvpr2023 visual anomaly and novelty detection challenge: Multimodal prompting for data-centric anomaly detection,'' \emph{arXiv preprint arXiv:2306.09067}, 2023.

\bibitem{oquab2023dinov2}
M.~Oquab, T.~Darcet, T.~Moutakanni, H.~Vo, M.~Szafraniec, V.~Khalidov, P.~Fernandez, D.~Haziza, F.~Massa, A.~El-Nouby \emph{et~al.}, ``Dinov2: Learning robust visual features without supervision,'' \emph{arXiv preprint arXiv:2304.07193}, 2023.

\bibitem{yi2020patch}
J.~Yi and S.~Yoon, ``Patch svdd: Patch-level svdd for anomaly detection and segmentation,'' in \emph{Proceedings of the Asian conference on computer vision}, 2020.

\bibitem{li2024promptad}
X.~Li, Z.~Zhang, X.~Tan, C.~Chen, Y.~Qu, Y.~Xie, and L.~Ma, ``Promptad: Learning prompts with only normal samples for few-shot anomaly detection,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2024, pp. 16\,838--16\,848.

\bibitem{massoli2021mocca}
F.~V. Massoli, F.~Falchi, A.~Kantarci, {\c{S}}.~Akti, H.~K. Ekenel, and G.~Amato, ``Mocca: Multilayer one-class classification for anomaly detection,'' \emph{IEEE transactions on neural networks and learning systems}, vol.~33, no.~6, pp. 2313--2323, 2021.

\bibitem{liu2023simplenet}
Z.~Liu, Y.~Zhou, Y.~Xu, and Z.~Wang, ``Simplenet: A simple network for image anomaly detection and localization,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2023, pp. 20\,402--20\,411.

\bibitem{cao2023anomaly}
T.~Cao, J.~Zhu, and G.~Pang, ``Anomaly detection under distribution shift,'' in \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, 2023, pp. 6511--6523.

\bibitem{zhou2024msflow}
Y.~Zhou, X.~Xu, J.~Song, F.~Shen, and H.~T. Shen, ``Msflow: Multiscale flow-based framework for unsupervised anomaly detection,'' \emph{IEEE Transactions on Neural Networks and Learning Systems}, 2024.

\bibitem{xie2023pushing}
G.~Xie, J.~Wang, J.~Liu, F.~Zheng, and Y.~Jin, ``Pushing the limits of fewshot anomaly detection in industry vision: Graphcore,'' \emph{arXiv preprint arXiv:2301.12082}, 2023.

\bibitem{you2022unified}
Z.~You, L.~Cui, Y.~Shen, K.~Yang, X.~Lu, Y.~Zheng, and X.~Le, ``A unified model for multi-class anomaly detection,'' \emph{Advances in Neural Information Processing Systems}, vol.~35, pp. 4571--4584, 2022.

\bibitem{zhao2023omnial}
Y.~Zhao, ``Omnial: A unified cnn framework for unsupervised anomaly localization,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2023, pp. 3924--3933.

\bibitem{ho2024long}
C.-H. Ho, K.-C. Peng, and N.~Vasconcelos, ``Long-tailed anomaly detection with learnable class names,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2024, pp. 12\,435--12\,446.

\bibitem{schluter2022natural}
H.~M. Schl{\"u}ter, J.~Tan, B.~Hou, and B.~Kainz, ``Natural synthetic anomalies for self-supervised anomaly detection and localization,'' in \emph{European Conference on Computer Vision}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2022, pp. 474--489.

\bibitem{d2023multimodal}
M.~D'Alessandro, A.~Alonso, E.~Calabr{\'e}s, and M.~Galar, ``Multimodal parameter-efficient few-shot class incremental learning,'' in \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, 2023, pp. 3393--3403.

\bibitem{zavrtanik2021draem}
V.~Zavrtanik, M.~Kristan, and D.~Sko{\v{c}}aj, ``Draem-a discriminatively trained reconstruction embedding for surface anomaly detection,'' in \emph{Proceedings of the IEEE/CVF international conference on computer vision}, 2021, pp. 8330--8339.

\bibitem{peng2024industrial}
J.~Peng, H.~Shao, Y.~Xiao, B.~Cai, and B.~Liu, ``Industrial surface defect detection and localization using multi-scale information focusing and enhancement ganomaly,'' \emph{Expert Systems with Applications}, vol. 238, p. 122361, 2024.

\bibitem{yao2024prior}
H.~Yao, Y.~Cao, W.~Luo, W.~Zhang, W.~Yu, and W.~Shen, ``Prior normality prompt transformer for multi-class industrial image anomaly detection,'' \emph{arXiv preprint arXiv:2406.11507}, 2024.

\bibitem{liu2024unsupervised}
J.~Liu, K.~Wu, Q.~Nie, Y.~Chen, B.-B. Gao, Y.~Liu, J.~Wang, C.~Wang, and F.~Zheng, ``Unsupervised continual anomaly detection with contrastively-learned prompt,'' in \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, vol.~38, no.~4, 2024, pp. 3639--3647.

\bibitem{chaudhry2018riemannian}
A.~Chaudhry, P.~K. Dokania, T.~Ajanthan, and P.~H. Torr, ``Riemannian walk for incremental learning: Understanding forgetting and intransigence,'' in \emph{Proceedings of the European conference on computer vision (ECCV)}, 2018, pp. 532--547.

\bibitem{deng2009imagenet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei, ``Imagenet: A large-scale hierarchical image database,'' in \emph{2009 IEEE conference on computer vision and pattern recognition}.\hskip 1em plus 0.5em minus 0.4em\relax Ieee, 2009, pp. 248--255.

\bibitem{dehaene2020anomaly}
D.~Dehaene and P.~Eline, ``Anomaly localization by modeling perceptual features,'' \emph{arXiv preprint arXiv:2008.05369}, 2020.

\bibitem{cohen2020sub}
N.~Cohen and Y.~Hoshen, ``Sub-image anomaly detection with deep pyramid correspondences,'' \emph{arXiv preprint arXiv:2005.02357}, 2020.

\bibitem{defard2021padim}
T.~Defard, A.~Setkov, A.~Loesch, and R.~Audigier, ``Padim: a patch distribution modeling framework for anomaly detection and localization,'' in \emph{International Conference on Pattern Recognition}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2021, pp. 475--489.

\bibitem{yu2021fastflow}
J.~Yu, Y.~Zheng, X.~Wang, W.~Li, Y.~Wu, R.~Zhao, and L.~Wu, ``Fastflow: Unsupervised anomaly detection and localization via 2d normalizing flows,'' \emph{arXiv preprint arXiv:2111.07677}, 2021.

\bibitem{hyun2024reconpatch}
J.~Hyun, S.~Kim, G.~Jeon, S.~H. Kim, K.~Bae, and B.~J. Kang, ``Reconpatch: Contrastive patch representation learning for industrial anomaly detection,'' in \emph{Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision}, 2024, pp. 2052--2061.

\bibitem{kingma2014adam}
D.~P. Kingma, ``Adam: A method for stochastic optimization,'' \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{wang2022dualprompt}
Z.~Wang, Z.~Zhang, S.~Ebrahimi, R.~Sun, H.~Zhang, C.-Y. Lee, X.~Ren, G.~Su, V.~Perot, J.~Dy \emph{et~al.}, ``Dualprompt: Complementary prompting for rehearsal-free continual learning,'' in \emph{European Conference on Computer Vision}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2022, pp. 631--648.

\bibitem{wang2022learning}
Z.~Wang, Z.~Zhang, C.-Y. Lee, H.~Zhang, R.~Sun, X.~Ren, G.~Su, V.~Perot, J.~Dy, and T.~Pfister, ``Learning to prompt for continual learning,'' in \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, 2022, pp. 139--149.

\bibitem{jia2022visual}
M.~Jia, L.~Tang, B.-C. Chen, C.~Cardie, S.~Belongie, B.~Hariharan, and S.-N. Lim, ``Visual prompt tuning,'' in \emph{European Conference on Computer Vision}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2022, pp. 709--727.

\bibitem{eldar1997farthest}
Y.~Eldar, M.~Lindenbaum, M.~Porat, and Y.~Y. Zeevi, ``The farthest point strategy for progressive image sampling,'' \emph{IEEE transactions on image processing}, vol.~6, no.~9, pp. 1305--1315, 1997.

\bibitem{zhou2024class}
D.-W. Zhou, Q.-W. Wang, Z.-H. Qi, H.-J. Ye, D.-C. Zhan, and Z.~Liu, ``Class-incremental learning: A survey,'' \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 2024.

\bibitem{lin2024survey}
Y.~Lin, Y.~Chang, X.~Tong, J.~Yu, A.~Liotta, G.~Huang, W.~Song, D.~Zeng, Z.~Wu, Y.~Wang \emph{et~al.}, ``A survey on rgb, 3d, and multimodal approaches for unsupervised industrial anomaly detection,'' \emph{arXiv preprint arXiv:2410.21982}, 2024.

\bibitem{yu2024recent}
D.~Yu, X.~Zhang, Y.~Chen, A.~Liu, Y.~Zhang, P.~S. Yu, and I.~King, ``Recent advances of multimodal continual learning: A comprehensive survey,'' \emph{arXiv preprint arXiv:2410.05352}, 2024.

\end{thebibliography}

\end{document}

% \begin{table*}
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{lccccccccccccccccc}
% \hline
% Methods  & Candle & Capsules & Cashew & Chewinggum & Fryum & Macaroni1 & Macaroni2 & Pcb1& Pcb2 & Pcb3 & Pcb4 & Pipe_fryum & Average & AvgFM\\
% \hline
% CFA & 0.017 & 0.005 & 0.059 & 0.243 & 0.085 & 0.001 & 0.001 & 0.013 & 0.006& 0.008 & 0.015 & 0.592 & 0.087& 0.184\\
% RD4AD   & 0.002 & 0.005 & 0.061 & 0.045 & 0.098 & 0.001 & 0.001 & 0.013& 0.008 & 0.008 & 0.013 & 0.576 & 0.069 & 0.201\\
% Patchcore  & 0.012 & 0.007 & 0.055 & 0.315 & 0.082 & 0.000 & 0.000 & 0.008& 0.004 & 0.007 & 0.010 & 0.585 & 0.090 & 0.311\\
% SimpleNet  & 0.001 & 0.004  & 0.017 & 0.007 & 0.047 & 0.000 & 0.000 & 0.013 & 0.003 & 0.004 & 0.009 & 0.058 & 0.014 & 0.016\\
% UniAD  & 0.006  & 0.013 & 0.040 & 0.185 & 0.087 & 0.002 & 0.002 & 0.015 & 0.005 & 0.015 & 0.013 & 0.576 & 0.080 & 0.218\\
% \hline
% Patchcore^*  & 0.018 & 0.010 & 0.047 & 0.202 & 0.081 & 0.003 & 0.001 & 0.008& 0.004 & 0.008 & 0.010 & 0.443 & 0.070 & 0.327\\
% UniAD^*  & 0.132  & 0.123 & 0.378 & 0.574 & 0.404 & 0.041 & 0.010 & 0.612 & 0.083 & 0.266 & 0.232 & 0.549 & 0.283 & 0.062\\
% UCAD &    &   &    &  & &   &   &  & &   & &   &  &  &  \\
% \hline
% Ours & 0.028 & 0.062 & 0.353 & 0.005 & 0.005 & 0.011 & 0.042 & 0.020& 0.001& 0.472 & 0.025& 0.191 & 0.101& 0.069 \\
% \hline
% \end{tabular}}
% \caption{Pixel-level $AUPR\uparrow$ and corrsponding $FM\downarrow$ on VisA dataset after training on the last subdataset.}
% \label{visa_pixel_pr}
% \end{table*}

% \begin{table*}
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{lccccccccccccccccc}
% \hline
% Methods  & Bottle & Cable & Capsule & Carpet& Grid& Hazelnut & Leather & Metal Nut& Pill& Screw & Tile& Toothbrush & Transistor& Wood & Zipper & Average & avg FM\\
% \hline
% PatchCore & 0.163  & 0.518 & 0.350 & 0.968 & 0.700 & 0.839 & 0.625 & 0.259 & 0.459 & 0.484 & 0.776 & 0.586 & 0.341 & 0.970 & 0.991 & 0.602 & 0.383 \\
% RD4AD & 0.401  & 0.538 & 0.475 & 0.583 & 0.558 & 0.909 & 0.596 & 0.623 & 0.479 & 0.596 & 0.715 & 0.397 & 0.385 & 0.700 & 0.987 & 0.596  &0.393\\
% CutPaste  & 0.111& 0.422 & 0.373 & 0.198& 0.214& 0.578 & 0.007 & 0.517& 0.371& 0.356 & 0.112& 0.158 & 0.340& 0.150 & 0.775 & 0.312  &0.510\\
% STPM  & 0.329& 0.539 & 0.610& 0.462& 0.569& 0.540 & 0.740 & 0.456& 0.523& 0.753 & 0.736& 0.375 & 0.450& 0.779 & 0.783 & 0.576 & 0.325\\
% DRAEM  & 0.793 & 0.411 & 0.517& 0.537& 0.799& 0.524 & 0.480 & 0.422 & 0.452 & 1.000 & 0.548& 0.625 & 0.307& 0.517 & 0.996 & 0.595  & 0.371\\
% UniAD  & 0.801 & 0.660 & 0.823 &0.754 & 0.713 & 0.904 & 0.715 & 0.791& 0.869 & 0.731 & 0.687 & 0.776 & 0.490 & 0.903 &0.997 & 0.774  & 0.229\\
% PaDiM & 0.458 & 0.544  & 0.418 & 0.454 & 0.704 & 0.635 & 0.418 & 0.446 & 0.449 & 0.578 & 0.581 & 0.678 & 0.407 & 0.549 & 0.855 & 0.545 & 0.368 \\
% CFA  & 0.309 & 0.489 & 0.275 & 0.834 & 0.571 & 0.903 & 0.935 & 0.464 & 0.528 & 0.528 & 0.763 & 0.519 & 0.320 & 0.923 & 0.984 & 0.623  &0.361\\
% SimpleNet  & 0.938 & 0.560 & 0.519 & 0.736 & 0.592 & 0.859 & 0.749 & 0.710 & 0.701 & 0.599 & 0.654 & 0.422 & 0.669 & 0.908 & 0.996 & 0.708  &0.211 \\
% CSFlow  & 0.129 & 0.420 & 0.363 & 0.978 & 0.602 & 0.269 & 0.906 & 0.220 & 0.263 & 0.434 & 0.697 & 0.569 & 0.432 & 0.802 & 0.997 & 0.539  &0.426\\
% FAVAE & 0.666 & 0.396  & 0.357 & 0.610 & 0.644 & 0.884 & 0.406 & 0.416 & 0.531 & 0.624 & 0.563 & 0.503 & 0.331 & 0.728 & 0.544 & 0.547  &0.102 \\
% SPADE & 0.302 & 0.444  & 0.525 & 0.529 & 0.460 & 0.410 & 0.577 & 0.592 & 0.484 & 0.514 & 0.881 & 0.386 & 0.622 & 0.897& 0.949 & 0.571  &0.285\\
% PatchCore* & 0.533 & 0.505 & 0.351 & 0.865& 0.723 & 0.959 & 0.854 & 0.456 & 0.511 & 0.626 & 0.748 & 0.600 & 0.427 & 0.900 & 0.974 & 0.669 &0.318 \\
% UniAD* & 0.997 & 0.701 & 0.765 & 0.998 & 0.896 & 0.936 & 1.000 & 0.964 & 0.895 & 0.554 & 0.989 & 0.928 & 0.966 & 0.982 & 0.987 & 0.904 &0.076 \\
% DNE & 0.990 & 0.619 & 0.609 & 0.984 & 0.998 & 0.924 & 1.000 & 0.989 & 0.671 & 0.588 & 0.980 & 0.933 & 0.877 & 0.930 & 0.958 & 0.870 &0.116 \\
% Ours & 1.000 & 0.652 & 0.926 & 0.970 & 0.976 & 0.990 & 1.000 & 0.995 & 0.892 & 0.852 & 0.997 & 1.000 & 0.926 & 0.991 & 0.949 & 0.941 &0.016 \\
% \hline
% \end{tabular}}
% \caption{Average Image-level $AUROC\uparrow$ on MVTec AD and VisA dataset after training on the last subdataset. The best results are highlighted in bold.}
% \label{mvtec_image_auc}
% \end{table*}

% \begin{table*}
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{lccccccccccccccccc}
% \hline
% Methods  & Candle & Capsules & Cashew & Chewinggum & Fryum & Macaroni1 & Macaroni2 & Pcb1 & Pcb2 & Pcb3 & Pcb4 & Pipe_fryum & Average & Avg FM\\
% \hline
% SimpleNet & 0.504  & 0.474 & 0.794 & 0.721 & 0.684 & 0.567 & 0.447 & 0.598 & 0.629 & 0.538 & 0.493 & 0.945 & 0.616 & 0.283 \\
% RD4AD & 0.380  & 0.385 & 0.737 & 0.539 & 0.533 & 0.607 & 0.487 & 0.437 & 0.672 & 0.343 & 0.187 & 0.999 & 0.525 & 0.423\\
% UniAD  & 0.573 & 0.599 & 0.661 & 0.758 & 0.504 & 0.559 & 0.644 & 0.749 & 0.523 & 0.547 & 0.562 & 0.989 & 0.639 & 0.297\\
% PatchCore  & 0.401 & 0.605 & 0.624 & 0.907 & 0.334 & 0.538 & 0.437 & 0.527 & 0.597 & 0.507 & 0.588 & 0.998 & 0.589 & 0.361\\
% CFA & 0.512 & 0.672 & 0.873 & 0.753 & 0.304 & 0.557 & 0.422 & 0.698 & 0.472 & 0.449 & 0.407 & 0.998 & 0.593 & 0.327\\
% UniAD*  & 0.884 & 0.669 & 0.938 &0.970 & 0.812 & 0.753 & 0.570 & 0.872 & 0.766 & 0.708 & 0.967 & 0.990 & 0.825 & 0.125\\
% DNE & 0.486 & 0.413  & 0.735 & 0.585 & 0.691 & 0.584 & 0.546 & 0.633 & 0.693 & 0.642 & 0.562 & 0.747 & 0.610 & 0.179\\
% PatchCore*  & 0.647 & 0.579 & 0.669 & 0.735 & 0.431 & 0.631 & 0.624 & 0.617 & 0.534 & 0.479 & 0.645 & 0.999 & 0.633 & 0.349\\
% Ours  & 0.765 & 0.794 & 0.914 & 0.938 & 0.925 & 0.874 & 0.783 & 0.946 & 0.937 & 0.935 & 0.989 & 0.997 & 0.900 & 0.010\\
% \hline
% \end{tabular}}
% \caption{Average Pixel-level $AUROC\uparrow$ on MVTec AD and VisA dataset after training on the last subdataset. The best results are highlighted in bold.}
% \label{mvtec_image_auc}
% \end{table*}


% \begin{table*}
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{llcccccccccc}
% \hline
% Dataset  & Method & AUROC-cls & AP-cls& AUROC-segm  & AP-segm& PRO-segm\\
% \hline
% \multirow{4}{*}{MvTec AD} & ACR(Li et al.,2023)  & 85.8  & 92.9 & 92.5 & 38.9 & 72.7 \\
%  & APRIL-GAN(Chen et al.,2023)  & 86.1 & 93.5 & 87.6 & 40.8 & 44.0 \\
%   & WinCLIP(Jeong et al.,2023)& 91.8  & 96.5 & 85.1& -& 64.6\\
%   & Ours& 94.1  & 96.5& 97.6& 46.8& 90.8\\
% \hline
% \multirow{4}{*}{VisA}  & WinCLIP(Jeong et al.,2023)& 78.1 & 81.2& 79.6&- & 56.8\\
%   & APRIL-GAN(Chen et al.,2023)& 78.0  & 81.4&94.2& 25.7& 86.8 \\
%  & Ours & 90.0   & 85.3 & 98.5 & 10.1 &80.0\\

% \hline
% \multirow{6}{*}{MvTec AD}  & Patchcore(Roth et al.,2022) & 86.3 & 93.5 & 94.1  & - & 84.5\\
%   & RegAD(Huang et al.,2022) & 89.1 & 94.9 & 96.2 & 48.3 &88.0\\
%   & GraphCore(Xie et al.,2023) & 92.9 & - & 97.4 & - & - \\
%  & APRIL-GAN(Chen et al.,2023) & 92.6  & 96.2 & 95.9 & 54.3 & 91.8  \\
%  & WinCLIP(Jeong et al.,2023) & 93.4  & 96.8 & 95.9 & - & 88.3 \\
% & Ours& 94.1  & 96.5& 97.6& 46.8& 90.8\\
% \hline
% \multirow{4}{*}{VisA}  & Patchcore(Roth et al.,2022)& 83.2 & 85.4 & 96.4 &- & 83.5 \\
%   & WinCLIP(Jeong et al.,2023) & 86.5  & 87.1 & 97.0 & - & 86.3\\
%   & APRIL-GAN(Chen et al.,2023) & 92.2  & 94.2 & 96.2 & 32.1 & 89.1\\
%  & Ours & 90.0   & 85.3 & 98.5 & 10.1 &80.0\\
% \hline
% \end{tabular}}
% \caption{Quantitative comparisons on the MVTec AD and VisA datasets. We compare our MMB
% with some state-of-the-art anomaly detection and continual anomaly detection. Bold indicates the best performance, while underlined denotes the second-best result. All metrics are in \%.}
% \label{mvtec_image_auc}
% \end{table*}

% \begin{table*}[h]
%     \centering
%     \renewcommand{\arraystretch}{0.5}
%     \setlength{\tabcolsep}{6pt}
%     \begin{tabular}{lcccc}
%         \toprule
%         \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{MVTec}} & \multicolumn{2}{c}{\textbf{VisA}} \\
%          & \textbf{Image-AUC} & \textbf{Pixel-AUC} & \textbf{Image-AUC} & \textbf{Pix-AUC} \\
%         \midrule
%         WinCLIP\cite{jeong2023winclip}  & 93.1 & 95.2 & 86.8 & 96.4 \\
%         APRIL-GAN\cite{chen2023april}  & 92.6 & 95.9 & 92.2 & 96.2 \\
%         SAA++\cite{cao2023winning}  & 63.5 & 75.5 & 67.1 & 76.5 \\
%         DINOV2\cite{oquab2023dinov2}  & 74.4 & 85.9 & 75.2 & 95.0 \\
%         SAM\cite{kirillov2023segment}  & 70.8 & 85.4 & 61.9 & 92.6 \\
%         PromptAD\cite{li2024promptad}  & 93.5 & 95.9 & 86.9 & 96.7 \\
%         AdaCLIP\cite{cao2025adaclip}  & 89.2 & 88.7 & 85.8 & 95.5 \\
%         \midrule
%         \textbf{Ours} &  \textbf{94.1} & \textbf{97.6} & \textbf{90.0} & \textbf{98.5} \\
%         \bottomrule
%     \end{tabular}
%     \caption{Performance comparison on MVTec and VisA datasets. "Image-AUC" represents image-level AUC, while "Pix-AUC" represents pixel-level AUC.}
%     \label{tab:results}
% \end{table*}