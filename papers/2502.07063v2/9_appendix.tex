% \section{Appendix}

\section{Interactive vs. Non-Interactive}
\label{sec:interactive}

ZKPs can broadly be classed into two categories: interactive and non-interactive \cite{wu2014survey}. Interactive protocols, as the name suggests, require several rounds interaction before \Vrf is convinced that \Prv's proof is valid. This is done by \Vrf sending random challenges to \Prv until \Vrf is convinced that \Prv's proof is valid. Interactive ZKPs require that both \Prv and \Vrf stay online until \Vrf is convinced. This somewhat limits the utility of interactive ZKPs, as the proofs are \textit{designated-verifier}, meaning that \Prv's proof can only be used to be convinced a single verifier. A separate protocol must be performed for each new \Vrf. Conversely, non-interactive ZKPs are normally \textit{publicly verifiable}, meaning \Prv can generate a single proof in one-shot that any \Vrf can verify. Non-interactive ZKPs often rely on a trusted setup process from a third-party, or in some cases \Vrf, to generate randomness that allows for a proof to be generated that \Vrf accepts as valid without further interaction. Many non-interactive schemes aim to minimize proof size, which results in higher \Prv computational power requirements. This limits the scalability of these schemes, especially in scenarios where \Prv is resource-constrained. The interactivity of interactive ZKPs allows for a more scalable approach in terms of \Prv computation, albeit limiting the amount of verifiers that can verify a proof. If needed, there is a method for turning public-coin interactive ZKPs into non-interactive ZKPs. The Fiat-Shamir transform \cite{kilian1992note} replaces \Vrf's randomness with a random oracle (i.e. a cryptographic hash function), thus removing the interaction and turning interactive ZKPs into non-interactive ZKPs.

\section{Recursive zk-SNARKs}
\label{sec:recursive}
Recent works \cite{bowe2019recursive, kothapalli2022nova, bitansky2013recursive} have shown the usability of recursive zk-SNARKs, which is the process of verifying multiple zk-SNARKs in a single zk-SNARK. As the verification algorithm of zk-SNARKs is simply an arbitrary computation, it can be represented as a circuit \Cir. This enables one \Prv to generate many proofs, then generate a proof that verifies these proofs and send it to \Vrf. While this results in substantially more work on \Prv, \Vrf now only has to generate one proof to verify all of \Prv's data, rather than many individual proofs. 

\section{Features of ZKP Libraries}
\label{sec:app_libraries}
Table \ref{tab:description} provides a compact, high-level description of the 25 frameworks we discuss in this work.
\input{framework_description_table}

% \section{Choosing the best framework for your application}
% \label{sec:bestframework}
% Figure \ref{fig:flow} provides a flow chart that developers can use to find the perfect framework for their applications, based on available resources and preferences. We note that most frameworks that we discuss in this SoK can be used as solutions for building ZKP applications, however we believe that the highlighted frameworks in figure \ref{fig:flow} are the most promising and reliable.

% \begin{figure}
%     \centering
%     {\includegraphics[width=\columnwidth]{SoKflow.pdf}}
%     \caption{Flow chart to guide users to the framework that best fits the requirements of their application and available resources.}
%     \label{fig:flow}
% \end{figure}

\section{ZKP Applications}
\label{sec:applications}
In this section, we discuss some of the cutting-edge ZKP applications that have been introduced in academia and industry. For an extensive view on the more simplistic applications of ZKPs, we refer readers to the excellent work of \cite{Chainlink2023ZeroKnowledgeProof}. 
% \nojan{add more here?}

\textbf{Verifiable Machine Learning. }
Verifiable computation (VC) is a technique enabled by ZKPs that allows one party to prove to another that computation was performed correctly and soundly without revealing any information about the underlying data or computation details \cite{vsimunic2021verifiable}. This is most common when there is a computationally weak verifier that would like to outsource their computation to a strong prover. This scenario lends itself quite nicely to verifiable machine learning (VML), in which a verifier can outsource their inference to a prover who owns a proprietary model. Many academic \cite{weng2021mystique, liu2021zkcnn, lee2020vcnn, feng2021zen} and industry \cite{ZkonduitInc2023EZKL} works have enabled VML, in which a cloud server (the prover) provides a ZKP that attests to the verifier that inference was computed soundly, without revealing any information about the server's proprietary model.

% \nojan{computationaly weak verifier sends computation out to strong prover (look at zilch abstract)}

\textbf{zk-Rollups. }
One of the biggest problems that faces the widespread implementation of ZKPs in modern systems is the difficulty of scalability. This is evident in blockchain applications, like Zcash \cite{hopwood2016zcash} and Monero \cite{Monero2023}, which require heavy computational efforts to protect each transaction on the blockchain that they hope to keep private. zk-Rollups aim to address similar problems, although not specific to Zcash and Monero, by aggregating multiple transactions into a single batch and generating a single proof that validates all of them in one shot. This is mostly enabled by the use of recursive zk-SNARKs \cite{kothapalli2022nova}, in which a ZKP for each transaction is built, followed by a ZKP that validates all of the transactions at once. This significantly lightens the computational load on the verifier. zk-Rollups have become a more prominent solution towards applying ZKPs at scale on the blockchain in several industrial efforts \cite{0xPolygonZero2023Plonky2, PolygonLabs2023PolygonZkEVM}.

\textbf{Robust Federated Learning. }
Byzantine attacks on federated learning refer to a security threat in which malicious users aim to harm the central model \cite{fang2020local}. The introduction of secure aggregation \cite{bonawitz2017practical}, which was devised to secure individual user updates, has made it much easier for malicious users to perform Byzantine attacks. In secure aggregation, malicious users can simply hide amongst benign users and inject poisoned updates that affect the central model's accuracy. Even if a malicious attack is detected, the privacy-preserving nature of secure aggregation, the attacker cannot be identified. Several academic works \cite{ghodsi2023zprobe, so2020byzantine, roy2022eiffel, lycklama2023rofl} have proposed scalable and secure secure aggregation schemes that utilize ZKPs to check individual user gradients, allowing for detection and exclusion of malicious users, while still maintaining end-to-end privacy.

\textbf{Digital Signatures. }
In the search for more post-quantum secure digital signatures, the National Institute of Standards and Technology (NIST) began a search for additional schemes to standardize in 2023 \cite{NIST_PQC_Round2_Signatures}, following up their previous digital signature standardization efforts. Their goal was to identify lightweight digital signature schemes that maintain high privacy and security in the presence of quantum adversaries. In particular, NIST called for general-purpose schemes that do not rely on lattices and maintain fast verification and short signature size. ZKPs have proven to be an excellent cryptographic primitive for the digital signature schemes that have found success in the NIST standardization process. Recently, in October 2024, NIST announced 14 second-round candidates for post-quantum digital signatures. Out of the 14 candidates, 6 of the candidates are built using ZKP schemes. The schemes Mirath \cite{PQC_MIRATH_Website}, MQOM \cite{benadjila2024mq}, PERK \cite{bettaieb2024perk}, RYDE \cite{aragon2023ryde}, and SDitH \cite{SDITH_Website} all utilize the MPCitH ZK scheme, while FAEST \cite{baum2023publicly} uses VOLE-in-the-head, a non-interactive implementation of VOLE-based ZK. These works are currently undergoing a thorough cryptanalysis and evaluation process before they are advanced to standardization. These efforts highlight the effectiveness of ZKP schemes, especially non-interactive ones, in real-world and large-scale applications. ZKPs serve as the perfect underlying technology for post-quantum digital signature due to their public verifiability and succinct proof sizes, which enables fast verification at scale.

\textbf{FHE Integrity. }
Similar to VML, FHE integrity consists of a verifier sending their data to a cloud server for computation. However, in FHE, the computation is done on encrypted data, making the ZKP generation process much more complex. As FHE operations are more computationally intensive and use underlying ring arithmetic, the circuit that expresses the computation for a ZKP grows to be very complex. \cite{ganesh2023rinocchio} introduces a ring-based zk-SNARK enabling verifiable computation over encrypted data, however new works \cite{viand2023verifiable} have shown that, although this make FHE integrity proofs feasible, the overhead makes it an impractical solution. 

\textbf{Data Authenticity. }
ZKPs have been integrated into DECO \cite{zhang2020deco}, a protocol that is being used in practice by Chainlink Labs \cite{breidenbach2021chainlink} which allows users to prove the authenticity of their data without revealing any information about the data itself, including the length of the datapoints. DECO allows users to prove that their data was sourced from a legitimate location, while also allowing them to prove certain attributes of the data (e.g. proving account balance is above a certain threshold). Proofs surrounding data authenticity, validity, and attributes lend themselves very nicely to ZKP settings. Several domains, such as healthcare and finance, which Chainlink has shown feasibility of, can benefit from integrating ZKPs to protect user data while still gathering meaningful information. Recent work \cite{juels2024props} has shown the value of using ZKPs to build protected pipelines, or \textit{props} for short, to provide verifiable, privacy-preserving access to deep web data for machine learning pipelines. This kind of secure access to deep web data is an integral part of advancing machine learning paradigms, as it enables developers to bypass the bottleneck of limited high-quality training data that is not currently accessible.

% \section{Scalability Analysis}
% \label{sec:scalability}

% In figure \ref{fig:scalability}, we perform an extensive scalability test on a subset of our evaluated frameworks to show how the trusted setup, proof generation and verification, and proof size/communication all scale as computation grows. We only do this for the matrix multiplication benchmark to avoid redundancies, and we believe it is sufficiently indicative of the framework's scalability. The subset of frameworks is chosen due to their promising performance, outlined in section \ref{sec:results}, and their ability to handle large circuits. Also,  some frameworks that were evaluated in section \ref{sec:results} ran into memory overflow issues on our machine as circuits scaled.

% We found that proof size/communication and proof verification stay relatively constant for zk-SNARKs and \texttt{Gnark}'s PLONK implementation. Proof size and verification time follow very similar curves in the case of both \texttt{Emp-ZK} and \texttt{RISC Zero}. Most trusted setup times and proof generation times follow similar trajectories as circuits scale, except for \texttt{Emp-ZK}, which stays relatively constant. \texttt{Diet Mac'n'Cheese} provided us with interesting results that seem to separate it quite a bit from \texttt{Emp-ZK}, although they are both VOLE-based ZK solutions. Upon further inspection, we found that the only frontend that is available, \texttt{PicoZK}, serves as the main bottleneck. \texttt{PicoZK} compiles everything to the DARPA SIEVE Intermediate Representation (IR), which can be read by \texttt{Diet Mac'n'Cheese}. However, \texttt{Diet Mac'n'Cheese} is not optimized for operations presented in SIEVE IR format, which is why the presented scalability results make it seem like a less performant candidate. Upon our own code review and discussion with the \texttt{PicoZK} authors, we believe that further work into building a direct bridge between \texttt{PicoZK} and \texttt{Diet Mac'n'Cheese} can result in results that are much closer to \texttt{Emp-ZK}, while still being able to take advantage of \texttt{PicoZK}'s extremely user-friendly development process. \texttt{RISC Zero}'s proof generation time is several orders of magnitude larger than the other evaluated frameworks, however we believe this is due to it's nature as a zkVM, which requires extra underlying RISC-V-based operations to perform these tasks. Finally, we observe that zk-SNARKs exhibit essentially the same pattern of growth as circuits scale. \texttt{PySNARK} proof generation time grows drastically with circuit size, but we believe this is due to Python's compilation process, which results in slower runtimes. We highlight \texttt{Emp-ZK}, \texttt{Arkworks}, and \texttt{Gnark}'s zk-SNARK and PLONK implementation as excellent scalable frameworks for custom applications.

% \textcolor{red}{
% TO DO BEFORE ARXIV
% \begin{enumerate}
%     \item Change MPCitH recommendation \cmark
%     \item Check Table 2 \cmark
%     \item Write digital signatures description \cmark
%     \item Update Table 4 \cmark
%     \item in recs, condense PLONKs into zk-SNARKs \cmark
%     \item Cite plonky \cmark    
%     \item update table 1 \cmark
%     \item update table 3 (just dates modified and diet mc + order) \cmark
%     \item fix citations \cmark
%     \item Add swanky numbers \cmark
%     \item Write swanky description \cmark
%     \item update appendix figure \cmark
%     \item check libiop \cmark
%     \item talk about icicle in gnark \cmark
%     \item double check numbers and stuff \cmark
% \end{enumerate}
% TO DO AFTER
% \begin{enumerate}
%     \item Make a table that summarizes all frameworks
%     \item Make a flow chart to choose a ZK scheme
%     \item Make a flow chart to choose a ZK framework
%     \item Clean up repo
%     \item add hardware acceleration section to appendix
% \end{enumerate}
% } 