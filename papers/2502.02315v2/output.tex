% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{colortbl}
\usepackage{tcolorbox}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{enumitem}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{VaiBot: Shuttle Between the Instructions and Parameters \\ of Large Language Models}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

\author{
 \textbf{Wangtao Sun\textsuperscript{1,2}},
 \textbf{Haotian Xu\textsuperscript{3}},
 \textbf{Huanxuan Liao\textsuperscript{1,2}},
 \textbf{Xuanqing Yu\textsuperscript{1,2}},
\\
 \textbf{Zhongtao Jiang\textsuperscript{1,2}},
 \textbf{Shizhu He\textsuperscript{1,2}},
 \textbf{Jun Zhao\textsuperscript{1,2}},
 \textbf{Kang Liu\textsuperscript{1,2,4}},
% \\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
% \\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
% \\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
% \\
\\
 \textsuperscript{1}Institute of Automation, Chinese Academy of Sciences, Beijing, China, \\
 \textsuperscript{2}School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China, \\
 \textsuperscript{3}Xiaohongshu Inc, \\
 \textsuperscript{4}Shanghai Artificial Intelligence Laboratory, \\
% \textsuperscript{5}Affiliation 5
% \\
%  \small{
%    \textbf{Correspondence:} \href{Kang Liu}{kliu@nlpr.ia.ac.cn}
%  }
}

\begin{document}
\maketitle

\begin{abstract}
How to interact with LLMs through \emph{instructions} has been widely studied by researchers. However, previous studies have treated the emergence of instructions and the training of LLMs on task data as separate processes, overlooking the inherent unity between the two. This paper proposes a novel neural network framework, VaiBot, that integrates VAE and VIB, designed to uniformly model, learn, and infer both instruction \emph{deduction} and instruction \emph{induction} tasks of LLMs. Through experiments, we demonstrate that VaiBot performs on par with existing baseline methods in terms of deductive capabilities while significantly surpassing them in inductive capabilities. We also find that VaiBot can scale up using general instruction-following data and exhibits excellent one-shot induction abilities. We finally synergistically integrate the deduction and induction processes of VaiBot for the task of \emph{inductive reasoning}. Through t-SNE dimensionality reduction, we observe that its inductive-deductive process significantly improves the distribution of training parameters, enabling it to outperform baseline methods in inductive reasoning tasks.
The code and data for this paper can be found at https://anonymous.4open.science/r/VaiBot-021F.
\end{abstract}

\section{Introduction}
\label{sec:intro}

%1. 随着大模型的兴起，越来越多的研究者和应用场景开始思考通过“指令”与大模型进行交互。“指令”是一类刻画任务目标的自然语言，其具备高度抽象性与精炼的任务知识。
With the rise of Large Language Models (LLMs), an increasing number of researchers and application scenarios are beginning to explore interacting with LLMs through \emph{instructions}. Instructions are a type of natural language that delineates task objectives, characterized by a high level of abstraction and refined task knowledge.

%2. 已有的工作从两个目标中考虑“指令”与大模型的交互，即演绎与归纳。具体而言，给定指令k，任务输入x_i，目标y_i，演绎要求模型根据指令k，输入x_i生成目标y_i；而归纳则要求模型根据大量的x_i, y_i作为输入，生成任务指令k。
Existing research has approached the interaction between instructions and LLMs from two objectives: \emph{deduction} and \emph{induction}. Specifically, given an instruction $k$, task inputs $x_i$, and targets $y_i$, deduction requires the model to generate the target $y_i$ based on the instruction $k$ and input $x_i$; whereas induction demands the model to predict the task instruction $k$ based on a large number of $x_i$ and $y_i$ as observations.

%3. 在演绎方面，以IFEval \cite{instruction-following-eval}, InfoBench \cite{qin2024infobench}, RuleBench工作为代表的方法评测了大模型遵循指令的能力，并表明通过指令微调可以有效增强这种能力；Hint, TAGI等工作则利用元学习的方式，试图通过指令生成LoRA，使其增强模型遵循指令的推理。
%In the realm of deduction, experiments carried out by IFEval \cite{instruction-following-eval}, InfoBench \cite{qin2024infobench}, and RuleBench \cite{sun2024beyond} have evaluated the capacity of large models to follow instructions, while also demonstrating that instruction fine-tuning (IFT) can significantly bolster this capability. Concurrently, previous works such as Hint \cite{ivison-etal-2023-hint} and TAGI \cite{liao2024instance} have ventured into the utilization of meta-learning strategies, endeavoring to augment the LLMs' reasoning in instruction-following through the generation of LoRA \cite{lora} via instructions.


\begin{figure}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{concept.pdf}}
\caption{The basic concept of unifying the modeling of instruction deduction and induction of LLMs.}
\label{fig:concept}
\end{center}
% \vskip -0.2in
\end{figure}


%4. 在归纳方面，已有的多项评测工作证明目前大模型并不具备良好的归纳能力；而Hypothesis Search, ItD等方法则将归纳建模为序列生成任务，试图通过采样-后处理与数据增强-微调等方式增强大模型的归纳能力。
%In the realm of induction, numerous existing evaluation efforts have demonstrated that current LLMs do not possess strong inductive capabilities \cite{eval1, eval2, mitchell2023comparing}. Meanwhile, methods such as Hypothesis Search \cite{wang2023hypothesis} and ItD \cite{sun2024itd} have modeled induction as a sequence generation task, attempting to enhance the inductive abilities of large models through approaches like sampling-selecting and augmenting-finetuning.

%5. 然而，以上方法都将指令的涌现与大模型对任务数据的学习割裂开来，而没有注意到两者统一的本质。事实上，指令是人类通过自然语言对任务数据的压缩；而大模型作为一个神经网络，其参数化的梯度下降训练也是对任务数据压缩。因此，指令与大模型训练后的参数应当具有相当高的相关性。因此我们考虑学习两者间的相互映射，来统一建模大模型下指令的演绎与归纳。
However, the previous studies %aforementioned approaches 
have treated the instruction \emph{deduction} and \emph{induction} of LLMs as separate processes (\S\ref{sec:related_work}), overlooking the inherent unity between the two.
In fact, instructions are a compression of task data by humans through natural language, while the parameterized gradient descent training of the LLMs, also constitutes a compression of task data. Therefore, the parameters of an LLM after training on specific tasks should exhibit a high degree of correlation with the task instructions. Consequently, we propose to learn the mutual mapping between the instructions and the parameters, aiming to unify the modeling of instruction deduction and induction of LLMs (Figure~\ref{fig:concept}).

%6. 基于这些认识，通过融合VAE与VIB，我们创新型地提出了VaiBot框架，其能够同时完成演绎和归纳任务的训练与推断。
Building upon this concept, we introduce VaiBot 
(%\textbf{Ne}ural-\textbf{sy}mbolic
\textbf{V}ariational \textbf{a}utoencoder and \textbf{i}nformation \textbf{Bot}tleneck)
, a novel framework that integrates the Variational Autoencoder (VAE) \cite{vae} and Variational Information Bottleneck (VIB) \cite{vib} to uniformly model, learn, and infer both deduction and induction tasks.
VaiBot operates by first encoding the instruction $k$ into a latent representation $z$ via an encoder. This latent $z$ then serves a dual purpose: it is utilized by a decoder to reconstruct the original instruction $k$, thereby facilitating the induction process (VAE). 
Concurrently, $z$ is employed as supplementary parameters for a task LLM, aiming to predict the target $y_i$ from the given input $x_i$, which is a deduction process (VIB).
The two objectives together with the regularization term are end-to-end jointly optimized to learn the functions of the encoder, decoder, and task LLM.

% Based on this idea, 
% %we propose a novel neural-symbolic learning framework, VaiBot, which is a combination of Variational Autoencoder (VAE) and Variational Information Bottleneck (VIB). 
% we propose VaiBot (\textbf{Ne}ural-\textbf{sy}mbolic \textbf{V}ariational \textbf{a}utoencoder and \textbf{i}nformation \textbf{Bot}tleneck), which integrates both the VAE \cite{vae} and VIB \cite{vib} frameworks and their optimization objectives. 
% %In which, the latent must simultaneously support for the task as well as for reconstruction.
% Specifically, VaiBot first encodes the instruction $k$ into the latent $z$ through an encoder,
% and then on the one hand, we use the latent $z$ to reconstruct the instruction $k$ (induction) through a decoder (VAE);
% on the other hand, we use the latent $z$ as the extra parameters of the task LLM in an attempt to predict target $y_i$ given input $x_i$ (VIB).
% The two objectives together with the regularization term are end-to-end jointly optimized to learn the functions of the encoder, decoder, and task LLM.



%7. 我们首先进行了一系列实验去验证VaiBot在演绎和归纳各自任务上的有效性。通过实验结果我们发现，VaiBot 在演绎能力方面不差于sft与meta learning。而在归纳能力方面，相对于传统“基于数据的归纳”方法，VaiBot在归纳性能方面取得了40%以上的分布内提升和20%以上的分布外提升。（ded 和 ind各自表现）
We initially conducted a series of experiments to validate the effectiveness of VaiBot in both deduction and induction tasks. The experimental results revealed that VaiBot's deductive capabilities are on par with those of supervised fine-tuning (SFT) and meta-learning. In terms of inductive capabilities, VaiBot achieved more than a 40\% improvement in in-distribution performance and over a 20\% improvement in out-of-distribution performance compared to traditional induction methods (\S\ref{sec:induction_related_work}). Additionally, by conducting induction tasks with varying numbers of observed samples, we found that VaiBot exhibits superior 1-shot induction capabilities compared to data-based induction methods.

%8. 我们进一步使用通用的指令遵循数据训练VaiBot，发现其随着训练量的增长泛化出跨任务的演绎和归纳能力，证明该架构可以有效的scale up；同时，通过在不同观测样本数下进行归纳任务，我们发现VaiBot相比与基于数据的归纳方法，具有出色的1-shot induction能力。
We further trained VaiBot using general instruction-following data and observed that, as the volume of training increased, it developed the ability to generalize across tasks for both deduction and induction, demonstrating that the architecture can effectively scale up. 

%9. 我们最后尝试将VaiBot的演绎流程与归纳流程协同结合起来，进行归纳式推理。实验证明，VaiBot相比于ICL等基线方法，能够更有效地进行归纳式推理；通过TSNE降维，我们发现其归纳-演绎的过程极大的改进了训练参数的分布，使得任务模型取得了更好的推理性能。（演绎推理结合）
Finally, we attempted to synergistically combine the deductive and inductive processes of VaiBot to perform \emph{inductive reasoning}. Compared to baseline methods such as ICL and Instruction Induction \cite{honovich-etal-2023-instruction}, VaiBot is much more effective in conducting inductive reasoning. Through t-SNE dimensionality reduction, we observed that the induction-deduction process of VaiBot significantly improved the distribution of the latent, thereby enabling the Task LLM to achieve superior reasoning performance.

In summary, this paper makes the following contributions:
\begin{itemize}[itemsep=1pt,topsep=1pt,parsep=0pt,leftmargin=*]
    %我们提出了一个结合VAE和VIB的神经网络框架，用于统一建模、学习和推断演绎和归纳两个任务
    \item We propose a novel neural network framework, VaiBot, that integrates VAE and VIB, designed to uniformly model, learn, and infer both instruction deduction and instruction induction tasks of LLMs.
    %通过实验，我们证明VaiBot在演绎能力方面不差于现有基线方法的同时，在归纳能力方面远远超出现有方法。我们同时发现VaiBot可以通过通用指令遵循数据scale up，并且具有优秀one-shot induction能力。
    \item We demonstrate that VaiBot performs on par with existing baseline methods in terms of deductive capabilities while significantly surpassing them in inductive capabilities. We also find that VaiBot can scale up using general instruction-following data and exhibits excellent one-shot induction abilities.
    %我们将VaiBot的演绎流程与归纳流程协同结合起来，通过TSNE降维，我们发现其归纳-演绎的过程极大的改进了训练参数的分布，因此其相对于基线方法能够更好的完成归纳式推理任务。
    \item We synergistically integrate the deduction and induction processes of VaiBot. Through t-SNE dimensionality reduction, we observe that its induction-deduction process significantly improves the distribution of training parameters, enabling it to outperform baseline methods in inductive reasoning tasks.
\end{itemize}

\begin{figure*}[ht]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\textwidth]{framework.pdf}}
\caption{The framework of VaiBot. The Training process is represented with filled colors and the inference process is represented with border colors.}
\label{fig:framework}
\end{center}
% \vskip -0.2in
\end{figure*}

\section{Related Work}
\label{sec:related_work}
\subsection{Instruction-based LLM Deduction}
\label{sec:deduction_related_work}
Given an instruction, how to ask LLM to perform deduction based on it, i.e. instruction following, has been widely considered by researchers. Previous studies such as IFEval \cite{instruction-following-eval}, InfoBench \cite{qin2024infobench}, and RuleBench \cite{sun2024beyond} have been instrumental in evaluating the capacity of large models to follow the instructions, also demonstrating that instruction fine-tuning (IFT) can significantly bolster this capability. 

Different from the prompt-level instruction-following paradigm, Meta-Learning methods like Hint \cite{ivison-etal-2023-hint} and TAGI \cite{liao2024instance} have tried training a hyper-network to encode the instruction into some extra parameters of LLMs to execute the instruction. However, these Meta-Learning methods rely heavily on supervised training conducted in advance on each subtask to obtain (instruction, parameter) pairs as training data for the hyper-network.

VaiBot employs a similar hyper-network architecture that maps instructions to LLMs' parameters, but it further integrates a reconstruction process, enabling the training of this hyper-network to no longer depend on pre-prepared (parameter, instruction) pairs. Instead, it can be trained on general instruction-following datasets.

\subsection{Instruction-oriented LLM Induction}
\label{sec:induction_related_work}
For the sake of interpretability and generalization, some previous works also try to induce instruction from task observations through LLMs. Some evaluation studies \cite{eval1, eval2, mitchell2023comparing} have consistently demonstrated that current LLMs are poor at the task of induction. To improve LLMs' capability of induction, methods such as Hypothesis Search \cite{wang2023hypothesis} and ItD \cite{sun2024itd} have modeled induction as a sequence generation task, attempting to enhance the inductive abilities of large models through approaches like sampling-selecting and augmenting-finetuning.

However, these methods are confined to \emph{data-based induction} and overlook the fact that the parameters of neural networks, once trained to converge on task data, provide highly indicative cues for the objectives of induction. VaiBot introduces \emph{parameter-based induction}, and our experiments have demonstrated that this approach significantly outperforms the previous series of data-based induction methods.

\section{VaiBot}
\label{sec:VaiBot}

VaiBot is trained to map a given textual knowledge $k$ to a latent $z$, which not only can serve as the extra parameters of an LLM, to solve the downstream task (VIB); 
but can also used for reconstruct the textual knowledge $k$ (VAE). It is mainly composed of three models:

\begin{itemize}[itemsep=1pt,topsep=1pt,parsep=0pt,leftmargin=*]
    \item \textbf{Encoder}. A textual encoder that encode the knowledge $k$ to the latent $z$. This mapping is denoted as $Enc(\cdot)$.
    \item \textbf{Decoder}. An auto-regressive decoder that decode the latent $z$ to the textual knowledge $k$. The distribution of decoder is denoted as $p_{dec}(\cdot)$.
    \item \textbf{Task LLM}. An LLM that solve the downstream task. The distribution of Task LLM is denoted as $p_{task}(\cdot)$.
\end{itemize}

% 在本节接下来的部分中，我们依次介绍VaiBot的训练过程和推理过程。
In the following part of this section, we will introduce how these three models are jointly trained and how they are used for neural-symbolic bidirectional inference.

\begin{figure*}[ht]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\textwidth]{loss_curve.pdf}}
\caption{The loss curve of VaiBot trained on SNI with respect to the training steps.}
\label{fig:loss}
\end{center}
% \vskip -0.2in
\end{figure*}

\subsection{Training}
\label{sec:training}
% 我们的训练数据一组k,x,y的三元组
As shown in Figure~\ref{fig:framework}, our training data consists of triples $(k,x,y)$, where $k$ is the textual knowledge, $x, y$ are the input-target pairs that $y$ can be inferred from $x$ using the textual knowledge $k$.

% 我们首先使用Encoder将知识$k$编码为潜在表示$z$，并计算正则化损失$J_{reg}$。
First, VaiBot uses the Encoder to encode the knowledge $k$ into a high-dimension diagonal normal distribution, and calculate the regularization loss $J_{reg}$ using Kullback–Leibler (KL) divergence.
\begin{align}
    \mu, \Sigma &= Enc(k) \\
    L_{reg} &= D_{KL}(\mathcal{N}(\cdot|\mu, \Sigma) || \mathcal{N}(\cdot|0, I))
\end{align}
% 然后，我们使用Decoder从潜在表示$z$中解码出知识$k$，并计算重构损失$J_{recon}$。
Then, the latent $z$ is sampled from the encoded normal distribution, here, the reparametrization trick \cite{reparameterization} is adopted to maintain the gradient flow.
VaiBot then uses the Decoder to attempt to reconstruct the knowledge $k$ from the latent representation $z$, and calculate the reconstruction loss $L_{recon}$. This corresponds to the objective of VAE.
\begin{align}
    z &\sim \mathcal{N}(\cdot|\mu, \Sigma) \\
    L_{recon} &= -\log p_{dec}(k|z)
\label{eq:original_l_recon}
\end{align}
% 然后，我们使用Task LLM来解决下游任务，并计算任务损失$J_{task}$。
Meanwhile, the latent representation $z$ is taken as the extra parameters of the Task LLM. The Task LLM is asked to infer on the given task instance $x,y$, and calculate the task loss $L_{task}$. This corresponds to the objective of VIB.
\begin{equation}
\label{eq:original_l_task}
    L_{task} = -\log p_{task}(y|z;x)
\end{equation}
To maintain and leverage the existing well-trained natural language distribution of the Task LLM and auto-regressive Decoder, 
we add textual condition $k$ for the Task LLM, and one pair of textual instance $x,y$ for the Decoder. So the Eq~\ref{eq:original_l_recon},\ref{eq:original_l_task} become into:
\begin{align}
    L_{recon} &= -\log p_{dec}(k|z;\underline{x,y}) \\
    L_{task} &= -\log p_{task}(y|z;x,\underline{k})
\end{align}
The final objective function is the weighted sum of the three loss terms, and we minimize it under the distribution of training data.
\begin{equation}
    L = \mathbb{E}_{(k,x,y)\sim p_{data}} w_{0} L_{reg} + w_{1} L_{task} + w_2 L_{recon}
\end{equation}
\subsection{Symbolic to Neural Inference}
\label{sec:symbolic_to_neural_inference}
As indicated by the orange border arrows in the Figure~\ref{fig:framework}, given a textual knowledge $k$, to perform the inference on the task input $x$, 
VaiBot encodes the knowledge $k$ into the latent representation $z$, and uses the Task LLM to generate an output via auto-regressive generation.
\begin{align}
    \mu, \Sigma &= Enc(k) \\
    z &\sim \mathcal{N}(\cdot|\mu, \Sigma) \\
    \hat{y} &\sim p_{task}(\cdot|z;x,k)
\end{align}
\subsection{Neural to Symbolic Inference}
\label{sec:neural_to_symbolic_inference}
As indicated by the blue border arrows in the Figure~\ref{fig:framework},
given the multiple instances $T=(x_i,y_i)_{i=1}^n$, to infer their shared knowledge $k$, 
VaiBot first fine-tune the Task LLM on the instances $T$ to obtain the converged latent representation $z^*$.
Here, we adopt an \emph{indirect training} trick to fine-tune the Task LLM, which is to create a trainable tensor $\Tilde{k}$, 
and then encode $\Tilde{k}$ into the normal distribution to get the trainable latent representation $\Tilde{z}$, instead of directly initializing the $\Tilde{z}$, taking it as the leaf parameters of the computation graph.
\begin{align}
    \Tilde{\mu}, \Tilde{\Sigma} &= Enc(\Tilde{k}) \\
    \Tilde{z} &\sim \mathcal{N}(\cdot|\Tilde{\mu}, \Tilde{\Sigma}) \\
    J_{task}^{\Tilde{k}}(x,y) &= -\log p_{task}(y|x;\Tilde{z})
\end{align}
Through minimizing $J_{task}^{\Tilde{k}}$ on training task samples $x,y$, we obtain the converged $\Tilde{k}$. However, what we want is the converged latent representation $z^*$:
\begin{align}
    k^* &= \arg\min_{\Tilde{k}} \frac{1}{n} \sum_{i=1}^n J_{task}(x_i,y_i) \\
    \mu^*, \Sigma^* &= Enc(k^*) \\
    z^* &\sim \mathcal{N}(\cdot|\mu^*, \Sigma^*)
\end{align}
Finally, we randomly sample a pair of $(x^*,y^*)$ from $T$ to leverage the well-trained natural language distribution of the Decoder. Under this condition, we can decode the trained parameters $z^*$ into explainable textual knowledge $k$:
\begin{align}
    \hat{k} &\sim p_{dec}(\cdot|z^*;x^*, y^*)
\end{align}
% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite

\begin{table*}[t]
\centering
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{lcccccccc}
\toprule
Dataset & \multicolumn{4}{c}{SNI} & \multicolumn{4}{c}{P3} \\
\midrule
\multirow{2}{*}{method} & \multicolumn{2}{c}{seen tasks (90\%)} & \multicolumn{2}{c}{unseen tasks (10\%)} & \multicolumn{2}{c}{seen tasks (90\%)} & \multicolumn{2}{c}{unseen tasks (10\%)} \\
 & deduction & induction & deduction & induction & deduction & induction & deduction & induction \\
\midrule
prompting * & 12.70 & 20.63 & 12.21 & 26.32 & 21.78 & ~~2.78 & 23.28 & ~~4.76 \\
vanilla SFT & 29.42 & 49.20 & 28.56 & 27.78 & 35.89 & 45.00 & 37.31 & 19.05 \\
TAGI & 32.02  & - & 23.33 & - & 36.33 & - & 47.62 & - \\
ItD & - & 43.85 & - & 33.33 & - & 33.33 & - & 28.57 \\
\midrule
VaiBot-in-domain & 33.26 & 85.56 & 21.11 & 44.44 & 48.67 & 78.33 & 58.10 & 28.57 \\
VaiBot-pretrain * & 30.37 & 36.36 & 32.22 & 50.00 & 38.22 & 20.00 & 49.52 & 19.05 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{The induction \& deduction performance of VaiBot and baselines on SNI and P3. Methods marked with * are not trained on seen tasks. - indicates that the method is not applicable to that task.}
\label{tab:main_results}
\end{table*}

\begin{figure}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{generalization.pdf}}
\caption{The OOD induction \& deduction performance of VaiBot-pretrain with respect to the ratio of used pretrained data.}
\label{fig:generalization}
\end{center}
% \vskip -0.2in
\end{figure}


% \section{Experiments}
% \subsection{Setting}
\section{Experiment Settings \& Training}
In this section, we introduce the experiment settings and the training of the VaiBot.
We employ Llama-2-7b-chat \cite{llama2} as the base language model $M$. Task LLM is $M$ itself while Encoder, Decoder is $M$ with two LoRA \cite{lora} of rank 16 and 1, respectively.
To facilitate efficient batch training \& inference, we adopt prompt tuning \cite{prompt-tuning} as the additional parameters of the Task LLM. The number of soft tokens is set to 10, and thus the dimension of $z$ is $10 \times 4096 = 40960$. All other baselines that need training (later introduced in \S\ref{sec:exp_induction_deduction},\ref{sec:exp_inductive_reasoning}) will take the $z$ of the same size as the training parameters for fair comparisons. The weights of the loss terms $w_0, w_1, w_2$ are set to 1e-3, 1.0, 1.0, respectively.

We adopt two popular multi-task instruction datasets: Super-Natural Instructions (SNI, \citealt{sni}) and T0 split of P3 (P3, \citealt{p3}) for evaluation.
We first split each dataset into seen tasks (90\%) and unseen tasks (10\%).
For each subtask, we only leave 5 instances as test samples, and use the rest as training samples.
Therefore, for methods that are trained on seen tasks, the test results on seen tasks reflect their \emph{sample-level generalization} ability, while the test results on unseen tasks reflect their \emph{task-level generalization} ability.

Besides training VaiBot with data from seen tasks for 10 epochs (VaiBot-in-domain), we additionally adopt around 437k instruction following data (Appendix~\ref{app:pretrain_data}) to pretrain VaiBot for 1 epoch (VaiBot-pretrain). Note that, here we mean ``pretrain" by training VaiBot to newly learn to generate and leverage the latent $z$ from general textual data, instead of randomly initializing the base model $M$.


\begin{figure}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{induction.pdf}}
\caption{The induction performance of VaiBot and SFT on SNI with respect to the number of observed samples. The accuracy is the average accuracy over all seen and unseen tasks.}
\label{fig:induction}
\end{center}
% \vskip -0.2in
\end{figure}


%\section{Training}
%\subsection{Training Curve}
In Figure~\ref{fig:loss}, we present the training loss curves for the three components during the training of VaiBot-in-domain on the SNI dataset, plotted against the training steps. The concurrent decrease in both reconstruction loss and task loss demonstrates that VaiBot effectively integrates the training processes of VAE and VIB. Regarding the regularization loss, the weight term $w_0$ controls the trade-off between reconstruction fidelity and the quality of disentanglement within the learned latent $z$ \cite{beta-vae}. Our experimental exploration of various $w_0$ values for the regularization term revealed that the performance of VaiBot remains robust across different settings, indicating a relative insensitivity to this parameter.
%Besides, the increased regularization loss (KL term) shows that our model is learning and storing increased latent patterns for the reconstruction and task loss. This is verified in many VAE practices \cite{bowman2015generating}.


\section{Induction \& Deduction}
\label{sec:exp_induction_deduction}
In this section, we verify the effectiveness of VaiBot on separate \emph{deduction} and \emph{induction} tasks.
In the deduction task, the model is provided with task knowledge $k$ and input $x$, and asked to generate the target $y$;
In the induction task, the model is provided with 5 test samples $\{x,y\}$ as the observation, and asked to generate the task knowledge $k$.
For the evaluation of both tasks, this paper adopts an external LLM (gpt-4o-mini\footnote{https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/}) as a judge to determine whether the prediction is correct, the prompts for the judge are shown in the Appendix~\ref{app:prompt_judge}.

%\subsection{Baseline}
We adopt the following methods as the baselines:
\begin{itemize}[itemsep=1pt,topsep=1pt,parsep=0pt,leftmargin=*]
    \item \textbf{prompting}. This method simply prompts the LLM $M$ with the task knowledge $k$ and input $x$ to infer the target $y$ (deduction) 
    and prompts the LLM $M$ with multiple instances $(x,y)$ to infer the task knowledge $k$ (induction).
    \item \textbf{vanilla SFT}. Based on the prompting method, we fine-tune the LLM $M$ based on the training data of seen tasks to learn the task of deduction and induction.
    \item \textbf{TAGI}. TAGI \cite{liao2024instance} is a typical meta-learning-based method that fuses the knowledge into the Task LLM through hyper-network. 
    It first trains the ``reference'' parameters of the Task LLM on the training data, and then leverages the (knowledge, parameters) pairs to train the hyper-network.
    TAGI can only be used in the \emph{deduction} task.
    \item \textbf{ItD}. ItD \cite{sun2024itd} is a recently proposed method that can empower the induction ability of the language model.
    It first decomposes the joint distribution of $p(x,y,k)$ with a deduction perspective into the knowledge prior $p(k)$ and deduction likelihood $p(y|x,k)p(x|k)$, and sample from them.
    Then, it fine-tunes the language model with the sampled data in the form of induction: $p(k|x,y)$.
    ItD can only be used in the \emph{induction} task.
\end{itemize}

\subsection{Comparison with Baselines}
We first compare the accuracy of VaiBot on deduction and induction with baselines. As shown in Table~\ref{tab:main_results}, while VaiBot-in-domain demonstrates competitive deduction ability compared to SFT and TAGI, it shows impressive induction ability compared to other data-based induction methods, not only outperforming ItD and vanilla SFT on the seen tasks, but also on the unseen tasks by a large margin.
Moreover, the VaiBot-pretrain also demonstrates competitive performance on two datasets although it is not trained on the in-domain data.
These results indicate that VaiBot demonstrates excellent \emph{sample-level generalization} and \emph{task-level generalization} abilities on both tasks of deduction and induction.

\begin{table*}[t]
\centering
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{lcccccccc}
\toprule
Dataset & \multicolumn{4}{c}{SNI} & \multicolumn{4}{c}{P3} \\
\midrule
\multirow{2}{*}{method} & \multicolumn{2}{c}{seen task (90\%)} & \multicolumn{2}{c}{unseen task (10\%)} & \multicolumn{2}{c}{seen task (90\%)} & \multicolumn{2}{c}{unseen task (10\%)} \\
 & deduction & induction & deduction & induction & deduction & induction & deduction & induction \\
\midrule
VaiBot-in-domain & 33.26 & 85.56 & 21.11 & 44.44 & 48.67 & 78.33 & 58.10 & 28.57 \\
\textcolor{gray}{w/o textual condition $x,y$} & 31.65 & ~~0.53 & 16.67 & ~~0.00 & 45.67 & 11.67 & 54.29 & ~~4.76 \\
\textcolor{gray}{w/o textual condition $k$} & 32.94 & 84.49 & ~~4.44 & 22.22 & 48.22 & 80.00 & 56.19 & 33.33\\
\textcolor{gray}{w/o indirect training} & 29.52 & ~~1.59 & 14.74 & ~~0.00 & 38.00 & ~~7.78 & 49.52 & ~~4.76 \\
\midrule
VaiBot-pretrain & 30.37 & 36.36 & 32.22 & 50.00 & 38.22 & 20.00 & 49.52 & 19.05 \\
\textcolor{gray}{w/o textual condition $x,y$} & 28.77 & ~~0.53 & 27.78 & ~~0.00 & 37.00 & ~~0.56 & 47.62 & ~~0.00 \\
\textcolor{gray}{w/o textual condition $k$} & 18.29 & 36.90 & 10.00 & 44.44 & 24.44 & 19.44 & 31.43 & 28.57 \\
\textcolor{gray}{w/o indirect training} & 28.98 & ~~2.14 & 26.67 & ~~0.00 & 40.72 & ~~4.42 & 48.57 & ~~3.57 \\
% \midrule
% VaiBot-in-domain & 33.26 & 85.56 & 21.11 & 44.44 & 48.67 & 78.33 & 58.10 & 28.57 \\
% \textcolor{gray}{w/o textual condition $x,y$} & -1.61 & -85.03 & -4.44 & -44.44 & -2.00 & -66.66 & -3.81 & -23.81 \\
% \textcolor{gray}{w/o textual condition $k$} & -0.32 & -1.07 & -16.67 & -22.22 & -0.45 & +1.67 & -1.91 & +4.76 \\
% \textcolor{gray}{w/o indirect training} & -3.74 & -83.97 & -6.37 & -44.44 & -10.67 & -70.55 & -8.58 & -23.81 \\
% \midrule
% VaiBot-pretrain & 30.37 & 36.36 & 32.22 & 50.00 & 38.22 & 20.00 & 49.52 & 19.05 \\
% \textcolor{gray}{w/o textual condition $x,y$} & -1.60 & +0.00 & -4.44 & -50.00 & -1.22 & +0.56 & -1.90 & -19.05 \\
% \textcolor{gray}{w/o textual condition $k$} & -12.08 & +0.54 & -22.22 & -5.56 & -13.78 & -0.56 & -18.99 & -9.48 \\
% \textcolor{gray}{w/o indirect training} & -1.39 & -34.22 & -5.55 & -50.00 & +2.50 & -15.58 & -0.95 & -15.48 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{The ablation results of VaiBot on SNI and P3.}
\label{tab:ablation}
\end{table*}

\subsection{Ablations}
\label{sec:ablation}
To verify the effectiveness of textual condition and indirect training trick proposed in \S\ref{sec:training}, we conduct an ablation study of VaiBot by dropping these parts. 
As shown in Table~\ref{tab:ablation}, if dropping the textual condition $x,y$ for the Decoder, or tuning $z$ without the indirect training trick, the induction performance will greatly decrease; if dropping the textual condition $k$ for the Task LLM, the deduction performance will be harmed to some extent. These findings verify that the textual conditions and indirect training tricks we adopt are beneficial for NestVaiBot.

\begin{table*}[t]
\centering
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{llcccc}
\toprule
Dataset & & \multicolumn{2}{c}{SNI} & \multicolumn{2}{c}{P3} \\
\midrule
method & & seen task (90\%) & unseen task (10\%) & seen task (90\%) & unseen task (10\%) \\
\midrule
ICL & & 10.91 & 14.44 & 13.22 & 22.86 \\
Instruction Induction & & 12.80 & ~~7.37 & 17.00 & 27.62 \\
\midrule
\multirow{2}{*}{VaiBot-in-domain} & SFT       & 11.98 & ~~5.56  & 27.00 & 30.48 \\
                                      & Refined   & 33.37 & ~~3.33  & 46.89 & 59.05 \\
\multirow{2}{*}{VaiBot-pretrain} & SFT       & ~~3.42 & ~~3.33  & 10.89  & 21.90  \\
                                      & Refined   & 21.39 & 20.00 & 26.56 & 33.33 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{The inductive reasoning results of VaiBot and baselines on SNI and P3.}
\label{tab:inductive reasoning}
\end{table*}

\subsection{Generalization with Scaling Up}
To visualize the generalization process of VaiBot, we pretrain it using varying proportions of the entire pretraining dataset and evaluate its performance on the induction and deduction tasks for SNI and P3. The resulting performance curve is depicted in Figure~\ref{fig:generalization}.
From the curve, it is evident that VaiBot's induction and deduction capabilities improve progressively as the volume of pretraining data increases. Notably, the deduction ability exhibits rapid growth and early convergence with increasing pretraining data, whereas the induction ability converges at a later stage. This observation aligns with the perspective highlighted in prior works \cite{bang2023multitask, semanticThanSymbolic, sun2024itd}, which posit that "induction is harder than deduction for LLMs." These findings further validate the inherent complexity of inductive reasoning compared to deductive reasoning in the context of large language models.

\subsection{Few-shot Induction}
To further highlight the superiority of VaiBot, we conduct a comparative analysis between VaiBot and SFT across varying numbers of observed samples. Specifically, we train SFT with 1 to 6 observed samples and evaluate both VaiBot and SFT using the corresponding number of testing observations.
As illustrated in Figure~\ref{fig:induction}, VaiBot achieves nearly optimal induction performance even when observing just 1 sample, whereas SFT requires a larger number of observed samples to enhance its induction capabilities. These results underscore VaiBot's superiority in few-shot induction, demonstrating its ability to perform effectively even in one-shot induction scenarios.


% \begin{figure}[ht]
% % \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\textwidth]{latent.pdf}}
% \caption{}
% \label{fig:latent}
% \end{center}
% % \vskip -0.2in
% \end{figure}

\begin{figure*}[ht]
\begin{center}
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{sni_latent.pdf}
\caption{The t-SNE result of latent $z$ on SNI.}
\label{fig:sni_latent}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{p3_latent.pdf}
\caption{The t-SNE result of latent $z$ on P3.}
\label{fig:p3_latent}
\end{minipage}
\end{center}
\end{figure*}


\section{Inductive-Deductive Collaborative Reasoning}
\label{sec:exp_inductive_reasoning}
To verify whether VaiBot can effectively collaborate the \emph{induction} and \emph{deduction} processes, we further consider the \emph{inductive reasoning} task.
In this task, models are asked to infer $y$ with input $x$ and some few-shot demonstrations $x_1,y_1;x_2,y_2;...;x_n,y_n$. 
Compared with the task of deduction, \emph{inductive reasoning} provides no task knowledge $k$ to the model, and the model is supposed to \emph{induce} the task knowledge from the given observations and then apply it to the test input. We adopt the following methods for comparison:
\begin{itemize}[itemsep=1pt,topsep=1pt,parsep=0pt,leftmargin=*]
    \item \textbf{ICL}. We adopt in-context learning (ICL) as the basic method of inductive reasoning. Specifically, we splice the observations $x_i, y_i$ and the input $x$ together into a prompt: $x_1,y_1;x_2,y_2;...;x_n,y_n;x$, and let the LLM to generate the correspond $y$.
    \item \textbf{Instruction Induction}. Instruction Induction \cite{honovich-etal-2023-instruction} proposed to explicitly induce textual instruction $k$ from the observations $x_1,y_1;x_2,y_2;...;x_n,y_n$, and then prompt the LLM with the query $x$ and instruction $k$ to perform inductive reasoning.
    \item \textbf{VaiBot SFT}. With the well-trained VaiBot, First, follow the inference process in \S\ref{sec:neural_to_symbolic_inference}, we fine-tune the Task LLM on the demonstrations, to obtain the converged parameters $z^*$. We use this fine-tuned $z^*$ for the Neural to Symbolic Inference $p_{task}(\cdot|z^*;x)$ (\S\ref{sec:neural_to_symbolic_inference}).
    \item \textbf{VaiBot Refined}. In this method, we collaborate the inductive \& deductive ability of VaiBot to perform inductive reasoning. We first leverage the $z^*$ to decode the induced task knowledge $\hat{k}$. Then, follow the inference process in \S\ref{sec:symbolic_to_neural_inference}, we again encoded the knowledge $\hat{k}$ into $\hat{z}$, and finally infer $y$ with $p_{task}(\cdot|\hat{z};x)$. Note that although we have obtained the textual knowledge $k$ and we have proved it beneficial for deduction \S\ref{sec:ablation}, we do not add it as the additional textual condition (i.e. $p_{task}(\cdot|\hat{z};x,k)$) as we want to directly compare the quality of $z$. 
\end{itemize}

\subsection{Comparison with Baselines}
% As shown in Table~\ref{tab:inductive reasoning}, The direct fine-tuned $z^*$ (VaiBot SFT) is not doing well in helping the Task LLM predict the $y$ based on $x$. However, after first decode the $z^*$ into $\hat{k}$, and then again encode the $\hat{k}$ into $\hat{z}$, the reasoning performance of Task LLM with $\hat{z}$ is strongly improved, outperforming the ICL baseline by a large margin. These results indicate that VaiBot can effectively combine its ability of \emph{deduction} and \emph{induction} to perform \emph{inductive reasoning}.

As illustrated in Table~\ref{tab:inductive reasoning}, the direct fine-tuned $z^*$ (VaiBot SFT) demonstrates limited effectiveness in assisting the Task LLM to predict $y$ based on $x$. However, a significant improvement is observed when $z^*$ is first decoded into $\hat{k}$ and subsequently re-encoded into $\hat{z}$. This approach substantially enhances the Task LLM's performance with $\hat{z}$, surpassing the ICL baseline by a considerable margin. These findings suggest that VaiBot effectively integrates its \emph{deductive} and \emph{inductive} capabilities to facilitate \emph{inductive reasoning}.

\subsection{Semantic Distribution of the Latent}
% To understand why the decode-encode collaborative process of $z$ can effectively improve the inductive reasoning ability of VaiBot, we respectively generate three types of $z$:
% \begin{itemize}[itemsep=1pt,topsep=1pt,parsep=0pt,leftmargin=*]
%     \item \textbf{Ground truth}. We use the VaiBot to encode the annotated $k$ of the dataset into $z$.
%     \item \textbf{Trained}. The trained $z^*$ after \textbf{VaiBot SFT}.
%     \item \textbf{Refined}. The $\hat{z}$ that is obtained by \textbf{VaiBot Refined}.
% \end{itemize}
% We adopt t-SNE \cite{tsne} for the dimensionality reduction of all $z$ together and plot them in a 2D plane with different colors marking their types. As shown in Figure~\ref{fig:sni_latent},\ref{fig:p3_latent}, the Trained latent is far away from the Ground truth latent, however, by conducting the induction-deduction collaborative process, we get the refined latent much closer and aligned with the Ground truth. These results indicate that VaiBot can effectively refine the trained latent, adjusting it to align with true semantics for inductive reasoning.

To elucidate why the decode-encode collaborative process of $z$ significantly enhances VaiBot's inductive reasoning capabilities, we generate and analyze three distinct types of $z$:
\begin{itemize}[itemsep=1pt,topsep=1pt,parsep=0pt,leftmargin=*]
    \item \textbf{Ground truth}. We use the VaiBot to encode the annotated $k$ of the dataset into $z$.
    \item \textbf{SFT}. The trained $z^*$ after \textbf{VaiBot SFT}.
    \item \textbf{Refined}. The $\hat{z}$ that is obtained by \textbf{VaiBot Refined}.
\end{itemize}
We employ t-SNE \cite{tsne} for dimensionality reduction, projecting all $z$ into a 2D plane and differentiating their types with distinct colors. As depicted in Figure~\ref{fig:sni_latent} and Figure~\ref{fig:p3_latent}, the trained latent from SFT significantly deviates from the ground truth latent. However, by performing the induction-deduction collaborative process, the refined latent becomes markedly closer to and aligned with the ground truth (green and blue). These findings demonstrate that VaiBot effectively refines the trained latent, adapting it to better align with true semantic representations, thereby enhancing its inductive reasoning performance.

\section{Conclusion}
This paper proposes VaiBot, a novel neural network framework that integrates VAE and VIB, designed to uniformly model, learn, and infer both instruction deduction and instruction induction tasks of LLMs. A series of experiments are conducted to verify the effectiveness of VaiBot, which performs on par with existing baseline methods in terms of deductive capabilities while significantly surpassing them in inductive capabilities. Moreover, by combining the process of induction and deduction in VaiBot, we find that VaiBot can perform excellent inductive reasoning through refining the latent.

\section*{Limitations}
The scope of deduction and induction is limited to \emph{instruction} in this work, while other forms of task information such as \emph{rules} may compress more difficult and informative task knowledge. We will expand and scale up VaiBot to this scope in the future.

\section*{Ethics Statement}
This paper proposes VaiBot, a novel neural network framework that integrates VAE and VIB, designed to uniformly model, learn, and infer both instruction deduction and instruction induction tasks of LLMs. All experiments are conducted on publicly available datasets. Thus there is no data privacy concern. Meanwhile, this paper does not involve human annotations, and there are no related ethical concerns.

\bibliography{custom}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Instruction-following Data for Pretraining VaiBot}
\label{app:pretrain_data}
We collect and process the instruction-following data from the following HuggingFace datasets for the pretraining of VaiBot:
\begin{itemize}[itemsep=1pt,topsep=1pt,parsep=0pt]
    \item \texttt{xzuyn/manythings-translations-alpaca}
    \item \texttt{MBZUAI/LaMini-instruction}
    \item \texttt{tatsu-lab/alpaca}
    \item \texttt{silk-road/alpaca-data-gpt4-chinese}
    \item \texttt{yizhongw/self\_instruct}
\end{itemize}

\section{Prompts for the LLM Judge}
\label{app:prompt_judge}

\begin{figure*}[htbp]
\begin{tcolorbox}[colframe=blue!50!black, colback=blue!2, title=Prompt for Deduction]
\textbf{Role: System} \\
Here are an instruction, an input, an reference answer and a predicted answer. Please help me determine if the predicted answer is correct.
Only return ``True" or ``False". \\
\textbf{Role: User} \\
instruction: \{$k$\} \\
input: \{$x$\} \\
reference answer: \{$y$\} \\
predicted answer: \{$\hat{y}$\}
\end{tcolorbox}
\caption{The prompt for the external LLM to judge if the deduction result $\hat{y}$ is correct for the current case. $k, x, y$ stands for the knowledge, the input, and the target answer of the current cases, respectively.}
\label{fig:prompt_deduction}
\end{figure*}

\begin{figure*}[htbp]
\begin{tcolorbox}[colframe=blue!50!black, colback=blue!2, title=Prompt for Induction]
\textbf{Role: System} \\
Here are two instructions described in natural language. 
Please help me determine if these two instructions are equivalent.
Only return ``True" or ``False".  \\
\textbf{Role: User} \\
transformation A: \{$k$\} \\
transformation B: \{$\hat{k}$\}
\end{tcolorbox}
\caption{The prompt for the external LLM to judge if the induction result $\hat{k}$ is correct for the current case. $k$ stands for the knowledge of the current cases.}
\label{fig:prompt_induction}
\end{figure*}

\end{document}
