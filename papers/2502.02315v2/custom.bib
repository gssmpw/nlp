% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

@article{cot,
  title={Chain of thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022}
}

@article{toolformer,
  title={Toolformer: Language models can teach themselves to use tools},
  author={Schick, Timo and Dwivedi-Yu, Jane and Dess{\`\i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  journal={arXiv preprint arXiv:2302.04761},
  year={2023}
}

@article{react,
  title={React: Synergizing reasoning and acting in language models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  journal={arXiv preprint arXiv:2210.03629},
  year={2022}
}

@article{self-reflection,
  title={Reflexion: an autonomous agent with dynamic memory and self-reflection},
  author={Shinn, Noah and Labash, Beck and Gopinath, Ashwin},
  journal={arXiv preprint arXiv:2303.11366},
  year={2023}
}

@article{self-refine,
  title={Self-Refine: Iterative Refinement with Self-Feedback},
  author={Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others},
  journal={arXiv preprint arXiv:2303.17651},
  year={2023}
}

@inproceedings{huang2022language,
  title={Language models as zero-shot planners: Extracting actionable knowledge for embodied agents},
  author={Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
  booktitle={International Conference on Machine Learning},
  pages={9118--9147},
  year={2022},
  organization={PMLR}
}

@article{ji2023survey,
  title={Survey of hallucination in natural language generation},
  author={Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
  journal={ACM Computing Surveys},
  volume={55},
  number={12},
  pages={1--38},
  year={2023},
  publisher={ACM New York, NY}
}

@article{zhou2023context,
  title={Context-faithful Prompting for Large Language Models},
  author={Zhou, Wenxuan and Zhang, Sheng and Poon, Hoifung and Chen, Muhao},
  journal={arXiv preprint arXiv:2303.11315},
  year={2023}
}

@inproceedings{reverie,
  title={Generative Agents: Interactive Simulacra of Human Behavior},
  author={},
  year={2023}
}

@inproceedings{2023EmergentAS,
  title={Emergent autonomous scientific research capabilities of large language models},
  author={},
  year={2023}
}

@article{bang2023multitask,
  title={A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity},
  author={Bang, Yejin and Cahyawijaya, Samuel and Lee, Nayeon and Dai, Wenliang and Su, Dan and Wilie, Bryan and Lovenia, Holy and Ji, Ziwei and Yu, Tiezheng and Chung, Willy and others},
  journal={arXiv preprint arXiv:2302.04023},
  year={2023}
}

@article{yang2023failures,
  title={Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation},
  author={Yang, Zeyuan and Li, Peng and Liu, Yang},
  journal={arXiv preprint arXiv:2310.15746},
  year={2023}
}

@article{sun2023expnote,
  title={ExpNote: Black-box Large Language Models are Better Task Solvers with Experience Notebook},
  author={Sun, Wangtao and Yu, Xuanqing and He, Shizhu and Zhao, Jun and Liu, Kang},
  journal={arXiv preprint arXiv:2311.07032},
  year={2023}
}

@article{honovich2022instruction,
  title={Instruction induction: From few examples to natural language task descriptions},
  author={Honovich, Or and Shaham, Uri and Bowman, Samuel R and Levy, Omer},
  journal={arXiv preprint arXiv:2205.10782},
  year={2022}
}

@article{pang2023guideline,
  title={Guideline Learning for In-Context Information Extraction},
  author={Pang, Chaoxu and Cao, Yixuan and Ding, Qiang and Luo, Ping},
  journal={arXiv preprint arXiv:2310.05066},
  year={2023}
}

@inproceedings{progres,
  title={A large-scale benchmark for few-shot program induction and synthesis},
  author={Alet, Ferran and Lopez-Contreras, Javier and Koppel, James and Nye, Maxwell and Solar-Lezama, Armando and Lozano-Perez, Tomas and Kaelbling, Leslie and Tenenbaum, Joshua},
  booktitle={International Conference on Machine Learning},
  pages={175--186},
  year={2021},
  organization={PMLR}
}

@article{hit,
  title={Large Language Models can Learn Rules},
  author={Zhu, Zhaocheng and Xue, Yuan and Chen, Xinyun and Zhou, Denny and Tang, Jian and Schuurmans, Dale and Dai, Hanjun},
  journal={arXiv preprint arXiv:2310.07064},
  year={2023}
}

@article{wang2023hypothesis,
  title={Hypothesis search: Inductive reasoning with language models},
  author={Wang, Ruocheng and Zelikman, Eric and Poesia, Gabriel and Pu, Yewen and Haber, Nick and Goodman, Noah D},
  journal={arXiv preprint arXiv:2309.05660},
  year={2023}
}

@article{qiu2023phenomenal,
  title={Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement},
  author={Qiu, Linlu and Jiang, Liwei and Lu, Ximing and Sclar, Melanie and Pyatkin, Valentina and Bhagavatula, Chandra and Wang, Bailin and Kim, Yoon and Choi, Yejin and Dziri, Nouha and others},
  journal={arXiv preprint arXiv:2310.08559},
  year={2023}
}

@article{lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{mitchell2023comparing,
  title={Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks},
  author={Mitchell, Melanie and Palmarini, Alessandro B and Moskvichev, Arseny},
  journal={arXiv preprint arXiv:2311.09247},
  year={2023}
}

@article{lake2017building,
  title={Building machines that learn and think like people},
  author={Lake, Brenden M and Ullman, Tomer D and Tenenbaum, Joshua B and Gershman, Samuel J},
  journal={Behavioral and brain sciences},
  volume={40},
  pages={e253},
  year={2017},
  publisher={Cambridge University Press}
}

@article{chollet2019measure,
  title={On the measure of intelligence},
  author={Chollet, Fran{\c{c}}ois},
  journal={arXiv preprint arXiv:1911.01547},
  year={2019}
}

@misc{nbce,
  title={Naive Bayes-based Context Extension},
  author={Jianlin Su},
  year={2023},
  howpublished={\url{https://github.com/bojone/NBCE}},
}

@article{self-consistency,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}

@article{dettmers2023qlora,
  title={QLoRA: Efficient Finetuning of Quantized LLMs},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2305.14314},
  year={2023}
}

@phdthesis{listfunc,
  title={The child as hacker: building more human-like models of learning},
  author={Rule, Joshua Stewart},
  year={2020},
  school={Massachusetts Institute of Technology}
}

@article{sloman2005problem,
  title={The problem of induction},
  author={Sloman, Steven A and Lagnado, David},
  journal={The Cambridge handbook of thinking and reasoning},
  pages={95--116},
  year={2005}
}

@article{zhao2023expel,
  title={Expel: Llm agents are experiential learners},
  author={Zhao, Andrew and Huang, Daniel and Xu, Quentin and Lin, Matthieu and Liu, Yong-Jin and Huang, Gao},
  journal={arXiv preprint arXiv:2308.10144},
  year={2023}
}

@article{eval1,
  title={Large language models as general pattern machines},
  author={Mirchandani, Suvir and Xia, Fei and Florence, Pete and Ichter, Brian and Driess, Danny and Arenas, Montserrat Gonzalez and Rao, Kanishka and Sadigh, Dorsa and Zeng, Andy},
  journal={arXiv preprint arXiv:2307.04721},
  year={2023}
}

@article{eval2,
  title={Large Language Models Are Not Abstract Reasoners},
  author={Gendron, Ga{\"e}l and Bao, Qiming and Witbrock, Michael and Dobbie, Gillian},
  journal={arXiv preprint arXiv:2305.19555},
  year={2023}
}

@article{semanticThanSymbolic,
  title={Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners},
  author={Tang, Xiaojuan and Zheng, Zilong and Li, Jiaqi and Meng, Fanxu and Zhu, Song-Chun and Liang, Yitao and Zhang, Muhan},
  journal={arXiv preprint arXiv:2305.14825},
  year={2023}
}

@article{nskg,
  title={Neural, symbolic and neural-symbolic reasoning on knowledge graphs},
  author={Zhang, Jing and Chen, Bo and Zhang, Lingxi and Ke, Xirui and Ding, Haipeng},
  journal={AI Open},
  volume={2},
  pages={14--35},
  year={2021},
  publisher={Elsevier}
}

@incollection{grzymala2023rule,
  title={Rule induction},
  author={Grzymala-Busse, Jerzy W},
  booktitle={Machine Learning for Data Science Handbook: Data Mining and Knowledge Discovery Handbook},
  pages={55--74},
  year={2023},
  publisher={Springer}
}

@article{peirce1868questions,
  title={Questions concerning certain faculties claimed for man},
  author={Peirce, Charles S},
  journal={The Journal of Speculative Philosophy},
  volume={2},
  number={2},
  pages={103--114},
  year={1868},
  publisher={JSTOR}
}

@misc{chen2024comm,
      title={CoMM: Collaborative Multi-Agent, Multi-Reasoning-Path Prompting for Complex Problem Solving}, 
      author={Pei Chen and Boran Han and Shuai Zhang},
      year={2024},
      eprint={2404.17729},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{chen2024hytrel,
  title={HYTREL: Hypergraph-enhanced tabular data representation learning},
  author={Chen, Pei and Sarkar, Soumajyoti and Lausen, Leonard and Srinivasan, Balasubramaniam and Zha, Sheng and Huang, Ruihong and Karypis, George},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@misc{li2024mosaic,
      title={Mosaic IT: Enhancing Instruction Tuning with Data Mosaics}, 
      author={Ming Li and Pei Chen and Chenguang Wang and Hongyu Zhao and Yijun Liang and Yupeng Hou and Fuxiao Liu and Tianyi Zhou},
      year={2024},
      eprint={2405.13326},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{ling2023beyond,
  title={Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey},
  author={Ling, Chen and Zhao, Xujiang and Lu, Jiaying and Deng, Chengyuan and Zheng, Can and Wang, Junxiang and Chowdhury, Tanmoy and Li, Yun and Cui, Hejie and others},
  journal={arXiv preprint arXiv:2305.18703},
  year={2023}
}

@misc{xu2024survey,
      title={A Survey on Knowledge Distillation of Large Language Models}, 
      author={Xiaohan Xu and Ming Li and Chongyang Tao and Tao Shen and Reynold Cheng and Jinyang Li and Can Xu and Dacheng Tao and Tianyi Zhou},
      year={2024},
      eprint={2402.13116},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{clutrr,
  title={CLUTRR: A diagnostic benchmark for inductive reasoning from text},
  author={Sinha, Koustuv and Sodhani, Shagun and Dong, Jin and Pineau, Joelle and Hamilton, William L},
  journal={arXiv preprint arXiv:1908.06177},
  year={2019}
}

@article{deer,
  title={Language models as inductive reasoners},
  author={Yang, Zonglin and Dong, Li and Du, Xinya and Cheng, Hao and Cambria, Erik and Liu, Xiaodong and Gao, Jianfeng and Wei, Furu},
  journal={arXiv preprint arXiv:2212.10923},
  year={2022}
}

@article{salad,
  title={SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models},
  author={Li, Lijun and Dong, Bowen and Wang, Ruohui and Hu, Xuhao and Zuo, Wangmeng and Lin, Dahua and Qiao, Yu and Shao, Jing},
  journal={arXiv preprint arXiv:2402.05044},
  year={2024}
}

@inproceedings{theoremqa,
  title={Theoremqa: A theorem-driven question answering dataset},
  author={Chen, Wenhu and Yin, Ming and Ku, Max and Lu, Pan and Wan, Yixin and Ma, Xueguang and Xu, Jianyu and Wang, Xinyi and Xia, Tony},
  booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
  year={2023}
}

@article{ulogic,
  title={Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs},
  author={Wang, Siyuan and Wei, Zhongyu and Choi, Yejin and Ren, Xiang},
  journal={arXiv preprint arXiv:2402.11442},
  year={2024}
}

@article{cail1,
  title={Cail2018: A large-scale legal dataset for judgment prediction},
  author={Xiao, Chaojun and Zhong, Haoxi and Guo, Zhipeng and Tu, Cunchao and Liu, Zhiyuan and Sun, Maosong and Feng, Yansong and Han, Xianpei and Hu, Zhen and Wang, Heng and others},
  journal={arXiv preprint arXiv:1807.02478},
  year={2018}
}

@article{cail2,
  title={Overview of CAIL2018: Legal judgment prediction competition},
  author={Zhong, Haoxi and Xiao, Chaojun and Guo, Zhipeng and Tu, Cunchao and Liu, Zhiyuan and Sun, Maosong and Feng, Yansong and Han, Xianpei and Hu, Zhen and Wang, Heng and others},
  journal={arXiv preprint arXiv:1810.05851},
  year={2018}
}

@article{llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{llama3,
    title={Llama 3 Model Card},
    author={AI@Meta},
    year={2024},
    url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}

@article{mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}


@misc{gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{rules,
  title={Can LLMs Follow Simple Rules?},
  author={Mu, Norman and Chen, Sarah and Wang, Zifan and Chen, Sizhe and Karamardian, David and Aljeraisy, Lulwa and Hendrycks, Dan and Wagner, David},
  journal={arXiv preprint arXiv:2311.04235},
  year={2023}
}

@article{hu2024case,
  title={Case-Based or Rule-Based: How Do Transformers Do the Math?},
  author={Hu, Yi and Tang, Xiaojuan and Yang, Haotong and Zhang, Muhan},
  journal={arXiv preprint arXiv:2402.17709},
  year={2024}
}

@article{young2024yi,
  title={Yi: Open foundation models by 01. ai},
  author={Young, Alex and Chen, Bei and Li, Chao and Huang, Chengen and Zhang, Ge and Zhang, Guanwei and Li, Heng and Zhu, Jiangcheng and Chen, Jianqun and Chang, Jing and others},
  journal={arXiv preprint arXiv:2403.04652},
  year={2024}
}

@article{abdin2024phi,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Jacobs, Sam Ade and Awan, Ammar Ahmad and Aneja, Jyoti and Awadallah, Ahmed and Awadalla, Hany and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}

@article{instructionsrules,
  title={Instructions, rules, and abstraction: A misconstrued relation},
  author={Ribes-Inesta, Emilio},
  journal={Behavior and philosophy},
  pages={41--55},
  year={2000},
  publisher={JSTOR}
}

@article{zero-shot,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@article{instruction-following-eval,
  title={Instruction-following evaluation for large language models},
  author={Zhou, Jeffrey and Lu, Tianjian and Mishra, Swaroop and Brahma, Siddhartha and Basu, Sujoy and Luan, Yi and Zhou, Denny and Hou, Le},
  journal={arXiv preprint arXiv:2311.07911},
  year={2023}
}

@article{qin2024infobench,
  title={InFoBench: Evaluating Instruction Following Ability in Large Language Models},
  author={Qin, Yiwei and Song, Kaiqiang and Hu, Yebowen and Yao, Wenlin and Cho, Sangwoo and Wang, Xiaoyang and Wu, Xuansheng and Liu, Fei and Liu, Pengfei and Yu, Dong},
  journal={arXiv preprint arXiv:2401.03601},
  year={2024}
}

@article{instruction1,
  title={Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections},
  author={Zhong, Ruiqi and Lee, Kristy and Zhang, Zheng and Klein, Dan},
  journal={arXiv preprint arXiv:2104.04670},
  year={2021}
}

@article{instruction2,
  title={Cross-task generalization via natural language crowdsourcing instructions},
  author={Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2104.08773},
  year={2021}
}

@article{instruction3,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}

@inproceedings{yin-etal-2023-llm,
    title = "{LLM}-driven Instruction Following: Progresses and Concerns",
    author = {Yin, Wenpeng  and
      Ye, Qinyuan  and
      Liu, Pengfei  and
      Ren, Xiang  and
      Sch{\"u}tze, Hinrich},
    editor = "Zhang, Qi  and
      Sajjad, Hassan",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-tutorial.4",
    doi = "10.18653/v1/2023.emnlp-tutorial.4",
    pages = "19--25",
    abstract = "The progress of natural language processing (NLP) is primarily driven by machine learning that optimizes a system on a large-scale set of task-specific labeled examples. This learning paradigm limits the ability of machines to have the same capabilities as humans in handling new tasks since humans can often solve unseen tasks with a couple of examples accompanied by task instructions. In addition, we may not have a chance to prepare task-specific examples of large-volume for new tasks because we cannot foresee what task needs to be addressed next and how complex to annotate for it. Therefore, task instructions act as a novel and promising resource for supervision. This tutorial targets researchers and practitioners who are interested in AI and ML technologies for NLP generalization in a low-shot scenario. In particular, we will present a diverse thread of instruction-driven NLP studies that try to answer the following questions: (i) What is task instruction? (ii) How is the process of creating datasets and evaluating systems conducted? (iii) How to encode task instructions? (iv) When and why do some instructions work better? (v) What concerns remain in LLM-driven instruction following? We will discuss several lines of frontier research that tackle those challenges and will conclude the tutorial by outlining directions for further investigation.",
}

@article{fagin1992inference,
  title={What is an inference rule?},
  author={Fagin, Ronald and Halpern, Joseph Y and Vardi, Moshe Y},
  journal={The Journal of symbolic logic},
  volume={57},
  number={3},
  pages={1018--1045},
  year={1992},
  publisher={Cambridge University Press}
}

@article{luo2023chatrule,
  title={Chatrule: Mining logical rules with large language models for knowledge graph reasoning},
  author={Luo, Linhao and Ju, Jiaxin and Xiong, Bo and Li, Yuan-Fang and Haffari, Gholamreza and Pan, Shirui},
  journal={arXiv preprint arXiv:2309.01538},
  year={2023}
}

@article{tot,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{choi2023kcts,
  title={KCTS: knowledge-constrained tree search decoding with token-level hallucination detection},
  author={Choi, Sehyun and Fang, Tianqing and Wang, Zhaowei and Song, Yangqiu},
  journal={arXiv preprint arXiv:2310.09044},
  year={2023}
}

@article{sifo,
  title={The SIFo Benchmark: Investigating the Sequential Instruction Following Ability of Large Language Models},
  author={Xinyi Chen and Baohao Liao and Jirui Qi and Panagiotis Eustratiadis and Christof Monz and Arianna Bisazza and Maarten de Rijke},
  journal={ArXiv},
  year={2024},
  volume={abs/2406.19999},
  url={https://api.semanticscholar.org/CorpusID:270845502}
}

@article{symbol-llm,
  title={Symbol-LLM: Towards foundational symbol-centric interface for large language models},
  author={Xu, Fangzhi and Wu, Zhiyong and Sun, Qiushi and Ren, Siyu and Yuan, Fei and Yuan, Shuai and Lin, Qika and Qiao, Yu and Liu, Jun},
  journal={arXiv preprint arXiv:2311.09278},
  year={2023}
}

@article{zelikman2022star,
  title={Star: Bootstrapping reasoning with reasoning},
  author={Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman, Noah},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={15476--15488},
  year={2022}
}

@article{yu2023survey,
  title={A survey on neural-symbolic learning systems},
  author={Yu, Dongran and Yang, Bo and Liu, Dayou and Wang, Hui and Pan, Shirui},
  journal={Neural Networks},
  year={2023},
  publisher={Elsevier}
}

@incollection{besold2021neural,
  title={Neural-symbolic learning and reasoning: A survey and interpretation 1},
  author={Besold, Tarek R and d’Avila Garcez, Artur and Bader, Sebastian and Bowman, Howard and Domingos, Pedro and Hitzler, Pascal and K{\"u}hnberger, Kai-Uwe and Lamb, Luis C and Lima, Priscila Machado Vieira and de Penning, Leo and others},
  booktitle={Neuro-Symbolic Artificial Intelligence: The State of the Art},
  pages={1--51},
  year={2021},
  publisher={IOS press}
}

@inproceedings{garcez2015neural,
  title={Neural-symbolic learning and reasoning: contributions and challenges},
  author={Garcez, Artur d'Avila and Besold, Tarek R and De Raedt, Luc and F{\"o}ldiak, Peter and Hitzler, Pascal and Icard, Thomas and K{\"u}hnberger, Kai-Uwe and Lamb, Luis C and Miikkulainen, Risto and Silver, Daniel L},
  booktitle={2015 AAAI Spring Symposium Series},
  year={2015}
}

@book{d2009neural,
  title={Neural-symbolic learning systems},
  author={d’Avila Garcez, Artur S and Lamb, Lu{\'\i}s C and Gabbay, Dov M},
  year={2009},
  publisher={Springer}
}

@article{zhang2022prboost,
  title={Prboost: Prompt-based rule discovery and boosting for interactive weakly-supervised learning},
  author={Zhang, Rongzhi and Yu, Yue and Shetty, Pranav and Song, Le and Zhang, Chao},
  journal={arXiv preprint arXiv:2203.09735},
  year={2022}
}

@article{pryzant2022automatic,
  title={Automatic Rule Induction for Interpretable Semi-Supervised Learning},
  author={Pryzant, Reid and Yang, Ziyi and Xu, Yichong and Zhu, Chenguang and Zeng, Michael},
  journal={arXiv preprint arXiv:2205.09067},
  year={2022}
}

@article{liao2024instance,
  title={From Instance Training to Instruction Learning: Task Adapters Generation from Instructions},
  author={Liao, Huanxuan and Xu, Yao and He, Shizhu and Zhang, Yuanzhe and Hao, Yanchao and Liu, Shengping and Liu, Kang and Zhao, Jun},
  journal={arXiv preprint arXiv:2406.12382},
  year={2024}
}

@article{ye2021learning,
  title={Learning to generate task-specific adapters from task description},
  author={Ye, Qinyuan and Ren, Xiang},
  journal={arXiv preprint arXiv:2101.00420},
  year={2021}
}

@article{ivison2022hint,
  title={HINT: Hypernetwork Instruction Tuning for Efficient Zero-\& Few-Shot Generalisation},
  author={Ivison, Hamish and Bhagia, Akshita and Wang, Yizhong and Hajishirzi, Hannaneh and Peters, Matthew},
  journal={arXiv preprint arXiv:2212.10315},
  year={2022}
}

@inproceedings{padalkar2024nesyfold,
  title={NeSyFOLD: A Framework for Interpretable Image Classification},
  author={Padalkar, Parth and Wang, Huaduo and Gupta, Gopal},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={5},
  pages={4378--4387},
  year={2024}
}

@article{bowman2015generating,
  title={Generating sentences from a continuous space},
  author={Bowman, Samuel R and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M and Jozefowicz, Rafal and Bengio, Samy},
  journal={arXiv preprint arXiv:1511.06349},
  year={2015}
}

@article{prompt-tuning,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}
}

@article{sni,
  title={Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks},
  author={Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Arunkumar, Anjana and Ashok, Arjun and Dhanasekaran, Arut Selvan and Naik, Atharva and Stap, David and others},
  journal={arXiv preprint arXiv:2204.07705},
  year={2022}
}

@article{p3,
  title={Multitask prompted training enables zero-shot task generalization},
  author={Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and others},
  journal={arXiv preprint arXiv:2110.08207},
  year={2021}
}

@article{sun2024itd,
  title={ItD: Large Language Models Can Teach Themselves Induction through Deduction},
  author={Sun, Wangtao and Xu, Haotian and Yu, Xuanqing and Chen, Pei and He, Shizhu and Zhao, Jun and Liu, Kang},
  journal={arXiv preprint arXiv:2403.05789},
  year={2024}
}

@article{liao2024instance,
  title={From Instance Training to Instruction Learning: Task Adapters Generation from Instructions},
  author={Liao, Huanxuan and Xu, Yao and He, Shizhu and Zhang, Yuanzhe and Hao, Yanchao and Liu, Shengping and Liu, Kang and Zhao, Jun},
  journal={arXiv preprint arXiv:2406.12382},
  year={2024}
}

@article{tsne,
  title={Visualizing data using t-SNE.},
  author={Van der Maaten, Laurens and Hinton, Geoffrey},
  journal={Journal of machine learning research},
  volume={9},
  number={11},
  year={2008}
}

@article{reparameterization,
  title={Variational dropout and the local reparameterization trick},
  author={Kingma, Durk P and Salimans, Tim and Welling, Max},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@inproceedings{honovich-etal-2023-instruction,
    title = "Instruction Induction: From Few Examples to Natural Language Task Descriptions",
    author = "Honovich, Or  and
      Shaham, Uri  and
      Bowman, Samuel R.  and
      Levy, Omer",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.108/",
    doi = "10.18653/v1/2023.acl-long.108",
    pages = "1935--1952",
    abstract = "Large language models are able to perform a task by conditioning on a few input-output demonstrations - a paradigm known as in-context learning. We show that language models can explicitly infer an underlying task from a few demonstrations by prompting them to generate a natural language instruction that fits the examples. To explore this ability, we introduce the instruction induction challenge, compile a dataset consisting of 24 tasks, and define a novel evaluation metric based on executing the generated instruction. We discover that, to a large extent, the ability to generate instructions does indeed emerge when using a model that is both large enough and aligned to follow instructions; InstructGPT achieves 65.7{\%} of human performance in our execution-based metric, while the original GPT-3 model reaches only 9.8{\%} of human performance. This surprising result suggests that instruction induction might be a viable learning paradigm in and of itself, where instead of fitting a set of latent continuous parameters to the data, one searches for the best description in the natural language hypothesis space."
}

@article{sun2024beyond,
  title={Beyond Instruction Following: Evaluating Inferential Rule Following of Large Language Models},
  author={Sun, Wangtao and Zhang, Chenxiang and Zhang, XueYou and Yu, Xuanqing and Huang, Ziyang and Chen, Pei and Xu, Haotian and He, Shizhu and Zhao, Jun and Liu, Kang},
  journal={arXiv preprint arXiv:2407.08440},
  year={2024}
}

@inproceedings{ivison-etal-2023-hint,
    title = "{HINT}: Hypernetwork Instruction Tuning for Efficient Zero- and Few-Shot Generalisation",
    author = "Ivison, Hamish  and
      Bhagia, Akshita  and
      Wang, Yizhong  and
      Hajishirzi, Hannaneh  and
      Peters, Matthew",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.631/",
    doi = "10.18653/v1/2023.acl-long.631",
    pages = "11272--11288",
    abstract = "Recent NLP models have shown the remarkable ability to effectively generalise {\textquoteleft}zero-shot' to new tasks using only natural language instructions as guidance. However, many of these approaches suffer from high computational costs due to their reliance on concatenating lengthy instructions with every input example, resulting in costly reprocessing of the instruction. To avoid this, we introduce Hypernetworks for INstruction Tuning (HINT), which convert task instructions and examples into parameter-efficient modules inserted into an underlying model using a pretrained text encoder, eliminating the need to include instructions in the model input. The hypernetwork in HINT also produces an encoded instruction, which we concatenate with encoded inputs during decoding to further improve performance. HINT models outperform strong state-of-the-art baselines by over 10{\%} when controlling for compute (measured in FLOPs). By converting instructions into modules, HINT models can effectively disregard the length of instructions and few-shot example inputs in terms of compute usage. As a result, HINT can enhance its performance by up to 25{\%} by incorporating additional few-shot data, while utilizing only up to 5{\%} more compute. This combines the strengths of parameter-efficient fine-tuning and in-context learning."
}

@article{vae,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@article{vib,
  title={Deep variational information bottleneck},
  author={Alemi, Alexander A and Fischer, Ian and Dillon, Joshua V and Murphy, Kevin},
  journal={arXiv preprint arXiv:1612.00410},
  year={2016}
}

@article{beta-vae,
  title={beta-vae: Learning basic visual concepts with a constrained variational framework.},
  author={Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher P and Glorot, Xavier and Botvinick, Matthew M and Mohamed, Shakir and Lerchner, Alexander},
  journal={ICLR (Poster)},
  volume={3},
  year={2017}
}