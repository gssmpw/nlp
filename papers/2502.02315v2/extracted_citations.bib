@article{eval1,
  title={Large language models as general pattern machines},
  author={Mirchandani, Suvir and Xia, Fei and Florence, Pete and Ichter, Brian and Driess, Danny and Arenas, Montserrat Gonzalez and Rao, Kanishka and Sadigh, Dorsa and Zeng, Andy},
  journal={arXiv preprint arXiv:2307.04721},
  year={2023}
}

@article{eval2,
  title={Large Language Models Are Not Abstract Reasoners},
  author={Gendron, Ga{\"e}l and Bao, Qiming and Witbrock, Michael and Dobbie, Gillian},
  journal={arXiv preprint arXiv:2305.19555},
  year={2023}
}

@article{instruction-following-eval,
  title={Instruction-following evaluation for large language models},
  author={Zhou, Jeffrey and Lu, Tianjian and Mishra, Swaroop and Brahma, Siddhartha and Basu, Sujoy and Luan, Yi and Zhou, Denny and Hou, Le},
  journal={arXiv preprint arXiv:2311.07911},
  year={2023}
}

@inproceedings{ivison-etal-2023-hint,
    title = "{HINT}: Hypernetwork Instruction Tuning for Efficient Zero- and Few-Shot Generalisation",
    author = "Ivison, Hamish  and
      Bhagia, Akshita  and
      Wang, Yizhong  and
      Hajishirzi, Hannaneh  and
      Peters, Matthew",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.631/",
    doi = "10.18653/v1/2023.acl-long.631",
    pages = "11272--11288",
    abstract = "Recent NLP models have shown the remarkable ability to effectively generalise {\textquoteleft}zero-shot' to new tasks using only natural language instructions as guidance. However, many of these approaches suffer from high computational costs due to their reliance on concatenating lengthy instructions with every input example, resulting in costly reprocessing of the instruction. To avoid this, we introduce Hypernetworks for INstruction Tuning (HINT), which convert task instructions and examples into parameter-efficient modules inserted into an underlying model using a pretrained text encoder, eliminating the need to include instructions in the model input. The hypernetwork in HINT also produces an encoded instruction, which we concatenate with encoded inputs during decoding to further improve performance. HINT models outperform strong state-of-the-art baselines by over 10{\%} when controlling for compute (measured in FLOPs). By converting instructions into modules, HINT models can effectively disregard the length of instructions and few-shot example inputs in terms of compute usage. As a result, HINT can enhance its performance by up to 25{\%} by incorporating additional few-shot data, while utilizing only up to 5{\%} more compute. This combines the strengths of parameter-efficient fine-tuning and in-context learning."
}

@article{liao2024instance,
  title={From Instance Training to Instruction Learning: Task Adapters Generation from Instructions},
  author={Liao, Huanxuan and Xu, Yao and He, Shizhu and Zhang, Yuanzhe and Hao, Yanchao and Liu, Shengping and Liu, Kang and Zhao, Jun},
  journal={arXiv preprint arXiv:2406.12382},
  year={2024}
}

@article{mitchell2023comparing,
  title={Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks},
  author={Mitchell, Melanie and Palmarini, Alessandro B and Moskvichev, Arseny},
  journal={arXiv preprint arXiv:2311.09247},
  year={2023}
}

@article{qin2024infobench,
  title={InFoBench: Evaluating Instruction Following Ability in Large Language Models},
  author={Qin, Yiwei and Song, Kaiqiang and Hu, Yebowen and Yao, Wenlin and Cho, Sangwoo and Wang, Xiaoyang and Wu, Xuansheng and Liu, Fei and Liu, Pengfei and Yu, Dong},
  journal={arXiv preprint arXiv:2401.03601},
  year={2024}
}

@article{sun2024beyond,
  title={Beyond Instruction Following: Evaluating Inferential Rule Following of Large Language Models},
  author={Sun, Wangtao and Zhang, Chenxiang and Zhang, XueYou and Yu, Xuanqing and Huang, Ziyang and Chen, Pei and Xu, Haotian and He, Shizhu and Zhao, Jun and Liu, Kang},
  journal={arXiv preprint arXiv:2407.08440},
  year={2024}
}

@article{sun2024itd,
  title={ItD: Large Language Models Can Teach Themselves Induction through Deduction},
  author={Sun, Wangtao and Xu, Haotian and Yu, Xuanqing and Chen, Pei and He, Shizhu and Zhao, Jun and Liu, Kang},
  journal={arXiv preprint arXiv:2403.05789},
  year={2024}
}

@article{wang2023hypothesis,
  title={Hypothesis search: Inductive reasoning with language models},
  author={Wang, Ruocheng and Zelikman, Eric and Poesia, Gabriel and Pu, Yewen and Haber, Nick and Goodman, Noah D},
  journal={arXiv preprint arXiv:2309.05660},
  year={2023}
}

