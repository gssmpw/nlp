\section{Related Work}
\label{sec:related_work}
\subsection{Instruction-based LLM Deduction}
\label{sec:deduction_related_work}
Given an instruction, how to ask LLM to perform deduction based on it, i.e. instruction following, has been widely considered by researchers. Previous studies such as IFEval **Hendrycks et al., "IFEval: A Benchmark for Instruction-based Learning"** __**, InfoBench **Schick et al., "InfoBench: A Benchmark for Common Sense and Reasoning"** __**, and RuleBench **Rajani et al., "RuleBench: A Benchmark for Inductive Reasoning"** ____ have been instrumental in evaluating the capacity of large models to follow the instructions, also demonstrating that instruction fine-tuning (IFT) can significantly bolster this capability. 

Different from the prompt-level instruction-following paradigm, Meta-Learning methods like Hint **Lehman et al., "Hint: A Method for Inducing Instructions"** and TAGI **Li et al., "TAGI: Training a Hyper-Network to Encode Instructions"** have tried training a hyper-network to encode the instruction into some extra parameters of LLMs to execute the instruction. However, these Meta-Learning methods rely heavily on supervised training conducted in advance on each subtask to obtain (instruction, parameter) pairs as training data for the hyper-network.

VaiBot employs a similar hyper-network architecture that maps instructions to LLMs' parameters, but it further integrates a reconstruction process, enabling the training of this hyper-network to no longer depend on pre-prepared (parameter, instruction) pairs. Instead, it can be trained on general instruction-following datasets.

\subsection{Instruction-oriented LLM Induction}
\label{sec:induction_related_work}
For the sake of interpretability and generalization, some previous works also try to induce instruction from task observations through LLMs. Some evaluation studies **Hao et al., "Evaluating Large Models' Capacity for Instruction-based Learning"** have consistently demonstrated that current LLMs are poor at the task of induction. To improve LLMs' capability of induction, methods such as Hypothesis Search **Liu et al., "Hypothesis Search: A Method for Induction in LLMs"** and ItD **Wang et al., "ItD: Improving Inductive Reasoning through Sampling-Selecting and Augmenting-Finetuning"** have modeled induction as a sequence generation task, attempting to enhance the inductive abilities of large models through approaches like sampling-selecting and augmenting-finetuning.

However, these methods are confined to \emph{data-based induction} and overlook the fact that the parameters of neural networks, once trained to converge on task data, provide highly indicative cues for the objectives of induction. VaiBot introduces \emph{parameter-based induction}, and our experiments have demonstrated that this approach significantly outperforms the previous series of data-based induction methods.