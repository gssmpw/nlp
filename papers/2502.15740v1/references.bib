@book{em:86,
  editor  = "Engelmore, Robert and Morgan, Anthony",
  title   = "Blackboard Systems",
  year    = 1986,
  address = "Reading, Mass.",
  publisher = "Addison-Wesley",
}

@inproceedings{c:83,
  author  = "Clancey, William J.",
  year    = 1983,
  title   = "{Communication, Simulation, and Intelligent
Agents: Implications of Personal Intelligent Machines
for Medical Education}",
  booktitle="Proceedings of the Eighth International Joint Conference on Artificial Intelligence {(IJCAI-83)}", 
  pages   = "556-560",
  address = "Menlo Park, Calif",
  publisher = "{IJCAI Organization}",
}
@inproceedings{c:84,
  author  = "Clancey, William J.",
  year    = 1984,
  title   = "{Classification Problem Solving}",
  booktitle = "Proceedings of the Fourth National 
              Conference on Artificial Intelligence",
  pages   = "45-54",
  address = "Menlo Park, Calif.",
  publisher="AAAI Press",
}
@article{r:80,
  author = {Robinson, Arthur L.},
  title = {New Ways to Make Microcircuits Smaller},
  volume = {208},
  number = {4447},
  pages = {1019--1022},
  year = {1980},
  doi = {10.1126/science.208.4447.1019},
  publisher = {American Association for the Advancement of Science},
  issn = {0036-8075},
  URL = {https://science.sciencemag.org/content/208/4447/1019},
  eprint = {https://science.sciencemag.org/content/208/4447/1019.full.pdf},
  journal = {Science},
}
@article{r:80x,
  author  = "Robinson, Arthur L.",
  year    = 1980,
  title   = "{New Ways to Make Microcircuits Smaller---Duplicate Entry}",
  journal = "Science",
  volume  =  208,
  pages   = "1019-1026",
}
@article{hcr:83,
title = {Strategic explanations for a diagnostic consultation system},
journal = {International Journal of Man-Machine Studies},
volume = {20},
number = {1},
pages = {3-19},
year = {1984},
issn = {0020-7373},
doi = {https://doi.org/10.1016/S0020-7373(84)80003-6},
url = {https://www.sciencedirect.com/science/article/pii/S0020737384800036},
author = {Diane Warner Hasling and William J. Clancey and Glenn Rennels},
abstract = {This article examines the problem of automatte explanation of reasoning, especially as it relates to expert systems. By explanation we mean the ability of a program to discuss what it is doing in some understandable way. We first present a general framework in which to view explanation and review some of the research done in this area. We then focus on the explanation system for NEOMYCIN, a medical consultation program. A consultation program interactively helps a user to solve a problem. Our goal is to have NEOMYCIN explain its problem-solving strategies. An explanation of strategy describes the plan the program is using to reach a solution. Such an explanation is usually concrete, referring to aspects of the current problem situation. Abstract explanations articulate a general principle, which can be applied in different situations; such explanations are useful in teaching and in explaining by analogy. We describe the aspects of NEOMYCIN that make abstract strategic explanations possible—the representation of strategic knowledge explicitly and separately from domain knowledge— and demonstrate how this representation can be used to generate explanations.}
}
@article{hcrt:83,
  author  = "Hasling, Diane Warner and Clancey, William J. and Rennels, Glenn R. and Test, Thomas",
  year    = 1983,
  title   = "{Strategic Explanations in Consultation---Duplicate}",
  journal = "The International Journal of Man-Machine Studies",
  volume  = 20,
  number  = 1,
  pages   = "3-19",
}
@techreport{r:86,
  author  = "Rice, James",
  year    = 1986,
  title   = "{Poligon: A System for Parallel Problem Solving}",
  type    = "Technical Report", 
  number  = "KSL-86-19", 
  institution = "Dept.\ of Computer Science, Stanford Univ.",
}
@phdthesis{c:79,
  author  = "Clancey, William J.",
  year    = 1979,
  title   = "{Transfer of Rule-Based Expertise
through a Tutorial Dialogue}",
  type    = "{Ph.D.} diss.",
  school  = "Dept.\ of Computer Science, Stanford Univ.",
  address = "Stanford, Calif.",
}
@unpublished{c:21,
  author  = "Clancey, William J.",
  title   = "{The Engineering of Qualitative Models}",
  year    = 2021,
  note    = "Forthcoming",
}
@misc{c:22,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{c:23,
  title        = "Pluto: The 'Other' Red Planet",
  author       = "{NASA}",
  howpublished = "\url{https://www.nasa.gov/nh/pluto-the-other-red-planet}",
  year         = 2015,
  note         = "Accessed: 2018-12-06"
}





% THIS IS WHERE I'M ADDING THE PAPERS FOR REFERENCES

@article{A23,
    title = "Meta-Heuristic Guided Feature Optimization for Enhanced Authorship Attribution in Java Source Code",
    abstract = "Source code authorship attribution is the task of identifying who develops the code based on learning based on the programmer style. It is one of the critical activities which used extensively in different aspects such as computer security, computer law, and plagiarism. This paper attempts to investigate source code authorship attribution by capturing natural language aspects of the code rather than only using minimal set of syntactic and stylistic code features as explored in the previous literature. It proposes an evolutionary feature selection model to improve the accuracy of authorship attribution by implementing two language models (uni-gram and bi-gram). The proposed approach uses K-Nearest Neighbor as a classifier and Genetic Algorithm as a feature selection technique. Two experiments have been demonstrated on a public Authorship Attribution dataset on GitHub, the experiments include various evolutionary feature selection models. Notably, the obtained results in both experiments were compared with the related studies, and show a significant improvement in terms of accuracy.",
    author = "Bilal Al-Ahmad and Nailah Al-Madi and Abdullah Alzaqebah and Alkhawaldeh, {Rami S.} and Khaled Aldebei and Kabir, {Md Faisal} and Ismail Altaharwa and Mua'ad Abu-Faraj and Ibrahim Aljarah",
    note = "Publisher Copyright: {\textcopyright} 2013 IEEE.",
    year = "2023",
    doi = "10.1109/ACCESS.2023.3341395",
    language = "English (US)",
    volume = "11",
    pages = "141657--141673",
    journal = "IEEE Access",
    issn = "2169-3536",
    publisher = "Institute of Electrical and Electronics Engineers Inc."
}

@article{O24,
   title={ChatGPT Code Detection: Techniques for Uncovering the Source of Code},
   volume={5},
   ISSN={2673-2688},
   url={http://dx.doi.org/10.3390/ai5030053},
   DOI={10.3390/ai5030053},
   number={3},
   journal={AI},
   publisher={MDPI AG},
   author={Oedingen, Marc and Engelhardt, Raphael C. and Denz, Robin and Hammer, Maximilian and Konen, Wolfgang},
   year={2024},
   month=jul, 
    pages={1066–1094} 
}

@misc{Y24,
      title={Uncovering LLM-Generated Code: A Zero-Shot Synthetic Code Detector via Code Rewriting}, 
      author={Tong Ye and Yangkai Du and Tengfei Ma and Lingfei Wu and Xuhong Zhang and Shouling Ji and Wenhai Wang},
      year={2024},
      eprint={2405.16133},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2405.16133} 
}

@article{X24, 
    title={Detecting AI-Generated Code Assignments Using Perplexity of Large Language Models}, 
    volume={38}, 
    url={https://ojs.aaai.org/index.php/AAAI/article/view/30361}, 
    doi={10.1609/aaai.v38i21.30361}, 
    abstractNote={Large language models like ChatGPT can generate human-like code, posing challenges for programming education as students may be tempted to misuse them on assignments. However, there are currently no robust detectors designed specifically to identify AI-generated code. This is an issue that needs to be addressed to maintain academic integrity while allowing proper utilization of language models. Previous work has explored different approaches to detect AI-generated text, including watermarks, feature analysis, and fine-tuning language models. In this paper, we address the challenge of determining whether a student’s code assignment was generated by a language model. First, our proposed method identifies AI-generated code by leveraging targeted masking perturbation paired with comperhesive scoring. Rather than applying a random mask, areas of the code with higher perplexity are more intensely masked. Second, we utilize a fine-tuned CodeBERT to fill in the masked portions, producing subtle modified samples. Then, we integrate the overall perplexity, variation of code line perplexity, and burstiness into a unified score. In this scoring scheme, a higher rank for the original code suggests it’s more likely to be AI-generated. This approach stems from the observation that AI-generated codes typically have lower perplexity. Therefore, perturbations often exert minimal influence on them. Conversely, sections of human-composed codes that the model struggles to understand can see their perplexity reduced by such perturbations. Our method outperforms current open-source and commercial text detectors. Specifically, it improves detection of code submissions generated by OpenAI’s text-davinci-003, raising average AUC from 0.56 (GPTZero baseline) to 0.87 for our detector.}, 
    number={21}, 
    journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
    author={Xu, Zhenyu and Sheng, Victor S.}, 
    year={2024}, 
    month={Mar.}, 
    pages={23155-23162}
}

@article{B01,
author = {Breiman, Leo},
title = {Random Forests},
year = {2001},
issue_date = {October 1 2001},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {45},
number = {1},
issn = {0885-6125},
url = {https://doi.org/10.1023/A:1010933404324},
doi = {10.1023/A:1010933404324},
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
journal = {Mach. Learn.},
month = {oct},
pages = {5–32},
numpages = {28},
keywords = {regression, ensemble, classification}
}

@inproceedings{C15,
author = {Caliskan-Islam, Aylin and Harang, Richard and Liu, Andrew and Narayanan, Arvind and Voss, Clare and Yamaguchi, Fabian and Greenstadt, Rachel},
title = {De-anonymizing programmers via code stylometry},
year = {2015},
isbn = {9781931971232},
publisher = {USENIX Association},
address = {USA},
abstract = {Source code authorship attribution is a significant privacy threat to anonymous code contributors. However, it may also enable attribution of successful attacks from code left behind on an infected system, or aid in resolving copyright, copyleft, and plagiarism issues in the programming fields. In this work, we investigate machine learning methods to de-anonymize source code authors of C/C++ using coding style. Our Code Stylometry Feature Set is a novel representation of coding style found in source code that reflects coding style from properties derived from abstract syntax trees.Our random forest and abstract syntax tree-based approach attributes more authors (1,600 and 250) with significantly higher accuracy (94\% and 98\%) on a larger data set (Google Code Jam) than has been previously achieved. Furthermore, these novel features are robust, difficult to obfuscate, and can be used in other programming languages, such as Python. We also find that (i) the code resulting from difficult programming tasks is easier to attribute than easier tasks and (ii) skilled programmers (who can complete the more difficult tasks) are easier to attribute than less skilled programmers.},
booktitle = {Proceedings of the 24th USENIX Conference on Security Symposium},
pages = {255–270},
numpages = {16},
location = {Washington, D.C.},
series = {SEC'15}
}

@inproceedings{C16,
author = {Chen, Tianqi and Guestrin, Carlos},
title = {XGBoost: A Scalable Tree Boosting System},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939785},
doi = {10.1145/2939672.2939785},
abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {785–794},
numpages = {10},
keywords = {large-scale machine learning},
location = {San Francisco, California, USA},
series = {KDD '16}
}


@Article{C22,
AUTHOR = {Czibula, Gabriela and Lupea, Mihaiela and Briciu, Anamaria},
TITLE = {Enhancing the Performance of Software Authorship Attribution Using an Ensemble of Deep Autoencoders},
JOURNAL = {Mathematics},
VOLUME = {10},
YEAR = {2022},
NUMBER = {15},
ARTICLE-NUMBER = {2572},
URL = {https://www.mdpi.com/2227-7390/10/15/2572},
ISSN = {2227-7390},
ABSTRACT = {Software authorship attribution, defined as the problem of software authentication and resolution of source code ownership, is of major relevance in the software engineering field. Authorship analysis of source code is more difficult than the classic task on literature, but it would be of great use in various software development activities such as software maintenance, software quality analysis or project management. This paper addresses the problem of code authorship attribution and introduces, as a proof of concept, a new supervised classification model AutoSoft for identifying the developer of a certain piece of code. The proposed model is composed of an ensemble of autoencoders that are trained to encode and recognize the programming style of software developers. An extension of the AutoSoft classifier, able to recognize an unknown developer (a developer that was not seen during the training), is also discussed and evaluated. Experiments conducted on software programs collected from the Google Code Jam data set highlight the performance of the proposed model in various test settings. A comparison to existing similar solutions for code authorship attribution indicates that AutoSoft outperforms most of them. Moreover, AutoSoft provides the advantage of adaptability, illustrated through a series of extensions such as the definition of class membership probabilities and the re-framing of the AutoSoft system to address one-class classification.},
DOI = {10.3390/math10152572}
}

@article{G10,
title = {A clustering-based discretization for supervised learning},
journal = {Statistics & Probability Letters},
volume = {80},
number = {9},
pages = {816-824},
year = {2010},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2010.01.015},
url = {https://www.sciencedirect.com/science/article/pii/S0167715210000271},
author = {Ankit Gupta and Kishan G. Mehrotra and Chilukuri Mohan},
keywords = {Discretization, Clustering, Binning, Supervised learning},
abstract = {We address the problem of discretization of continuous variables for machine learning classification algorithms. Existing procedures do not use interdependence between the variables towards this goal. Our proposed method uses clustering to exploit such interdependence. Numerical results show that this improves the classification performance in almost all cases. Even if an existing algorithm can successfully operate with continuous variables, better performance is obtained if the variables are first discretized. An additional advantage of discretization is that it reduces the overall computation time.}
}

@inproceedings{H18,
  author={Hozhabrierdi, Pegah and Fuentes Hitos, Dunai and Mohan, Chilukuri K.},
  booktitle={2018 IEEE International Conference on Data Mining Workshops (ICDMW)}, 
  title={Python Source Code De-anonymization Using Nested Bigrams}, 
  year={2018},
  volume={},
  number={},
  pages={23-28},
  keywords={Feature extraction;Python;Syntactics;Vegetation;Training;Computer security;Neural networks;source code de-anonymization, source code stylometry, abstract syntax tree, feature extraction, feature ranking},
  doi={10.1109/ICDMW.2018.00011}}

@inproceedings{H20,
  author={Hozhabrierdi, Pegah and Hitos, Dunai Fuentes and Mohan, Chilukuri K.},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Zero-Shot Source Code Author Identification: A Lexicon and Layout Independent Approach}, 
  year={2020},
  volume={},
  number={},
  pages={1-8},
  keywords={Training;Layout;Feature extraction;Syntactics;Task analysis;Feedforward neural networks;Encoding;Author identification;Source code stylometry;Zero-shot learning;Obfuscation},
  doi={10.1109/IJCNN48605.2020.9207647}}

@inproceedings{H22,
  author={Hao, Pengnan and Li, Zhen and Liu, Cui and Wen, Yu and Liu, Fanming},
  booktitle={2022 IEEE 22nd International Conference on Software Quality, Reliability and Security (QRS)}, 
  title={Towards Improving Multiple Authorship Attribution of Source Code}, 
  year={2022},
  volume={},
  number={},
  pages={516-526},
  keywords={Radio frequency;Java;Codes;Source coding;Plagiarism;Software quality;Machine learning;Multiple authorship attribution;Siamese network;machine learning},
  doi={10.1109/QRS57517.2022.00059}}

@inproceedings{K17,
author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
title = {LightGBM: a highly efficient gradient boosting decision tree},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Gradient Boosting Decision Tree (GBDT) is a popular machine learning algorithm, and has quite a few effective implementations such as XGBoost and pGBRT. Although many engineering optimizations have been adopted in these implementations, the efficiency and scalability are still unsatisfactory when the feature dimension is high and data size is large. A major reason is that for each feature, they need to scan all the data instances to estimate the information gain of all possible split points, which is very time consuming. To tackle this problem, we propose two novel techniques: Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB). With GOSS, we exclude a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size. With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features. We prove that finding the optimal bundling of exclusive features is NP-hard, but a greedy algorithm can achieve quite good approximation ratio (and thus can effectively reduce the number of features without hurting the accuracy of split point determination by much). We call our new GBDT implementation with GOSS and EFB LightGBM. Our experiments on multiple public datasets show that, LightGBM speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {3149–3157},
numpages = {9},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@misc{K20,
      title={When Ensembling Smaller Models is More Efficient than Single Large Models}, 
      author={Dan Kondratyuk and Mingxing Tan and Matthew Brown and Boqing Gong},
      year={2020},
      eprint={2005.00570},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2005.00570}, 
}

@inproceedings{O21,
  author={Omi, Abdul Mannan and Hossain, Monir and Islam, Md Nahidul and Mittra, Tanni},
  booktitle={2021 International Conference on Electronics, Communications and Information Technology (ICECIT)}, 
  title={Multiple Authors Identification from Source Code Using Deep Learning Model}, 
  year={2021},
  volume={},
  number={},
  pages={1-4},
  keywords={Deep learning;Codes;Machine learning algorithms;Plagiarism;Support vector machine classification;Syntactics;Software;Authorship Identification;Multiple Author Identification;Deep Learning;Support Vector Machine;LSTM;cod2seq Model;AST},
  doi={10.1109/ICECIT54077.2021.9641497}}

@inproceedings{P18,
author = {Prokhorenkova, Liudmila and Gusev, Gleb and Vorobev, Aleksandr and Dorogush, Anna Veronika and Gulin, Andrey},
title = {CatBoost: unbiased boosting with categorical features},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper presents the key algorithmic techniques behind CatBoost, a new gradient boosting toolkit. Their combination leads to CatBoost outperforming other publicly available boosting implementations in terms of quality on a variety of datasets. Two critical algorithmic advances introduced in CatBoost are the implementation of ordered boosting, a permutation-driven alternative to the classic algorithm, and an innovative algorithm for processing categorical features. Both techniques were created to fight a prediction shift caused by a special kind of target leakage present in all currently existing implementations of gradient boosting algorithms. In this paper, we provide a detailed analysis of this problem and demonstrate that proposed algorithms solve it effectively, leading to excellent empirical results.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {6639–6649},
numpages = {11},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@misc{R21,
      title={MalBERT: Using Transformers for Cybersecurity and Malicious Software Detection}, 
      author={Abir Rahali and Moulay A. Akhloufi},
      year={2021},
      eprint={2103.03806},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2103.03806}, 
}

@article{Y17,
author = {Yang, Xinyu and Xu, Guoai and Li, Qi and Guo, Yanhui and Zhang, Miao},
year = {2017},
month = {11},
pages = {e0187204},
title = {Authorship attribution of source code by using back propagation neural network based on particle swarm optimization},
volume = {12},
journal = {PLOS ONE},
doi = {10.1371/journal.pone.0187204}
}

@misc{P24_Java,
  author = {Paek, Timothy},
  title = {GPT Java Dataset: A Dataset for LLM-Generated Code Detection},
  year = {2024},
  howpublished = {GitHub Repository},
  url = {https://github.com/tipaek/GPT-Java-Dataset}
}

@misc{P24_GCJ,
  author = {Paek, Timothy},
  title = {GPT Java Dataset: The Largest LLM-Generated Code Dataset from Google Code Jam},
  year = {2024},
  howpublished = {GitHub Repository},
  url = {https://github.com/tipaek/GPT-Java-GCJ-Dataset}
}



