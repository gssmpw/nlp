\section{Related Work}
In prior work \citep{Y17, O21, C22, H22, A23}, code authorship attribution has typically been addressed as a multi-class classification problem, %urrently used methods use 
attributing an entire source code file to a single author. However, this has the following limitations:
\\\\ %\begin{enumerate} \item 
1. Files %are usually made up of 
    often have multiple authors, potentially resulting in model confusion. 
\\    %\item 
2. An LLM may have been used to generate only some snippets of code, which may be undetected because the rest of the code was written by the claimant author(s).
 %   \item 
\\ 3. There is often limited data on %individual students, 
    code from an individual, resulting in %there being in
    unavailability of 
    sufficient data for effective training.\\ %a model to train on for their respective classes.
%\end{enumerate}


Additionally, current LLM detectors are trained on %general input text, which can take any form, including 
both code and non-code text, resulting in inadequate performance on code alone. %This has the following limitations:
%\begin{enumerate}
    %\item 
%Classifying any input text usually implies 
Most utilized features are based on %typical 
natural language processing, 
%and hence %the detector may miss out on 
not addressing syntactic features specific to code, such as Abstract Syntax Tree (AST) features. % cannot be extracted in many cases).  \item 
%A general model for all input text may perform less accurately than a model that is trained on a specific task. 
%\end{enumerate}
Three such recent works are discussed below.

%For example,

%\citepauthor{O24} (2024) %deals with
Oedingen et al. (2024)
address GPT code detection %in this manner 
for Python code by utilizing Term Frequency - Inverse Term Frequency (TF-IDF), code2Vec, and human-generated features, %and various models, 
achieving up to 0.98 accuracy on non-formatted code and 0.94 accuracy on formatted code. Their work % paper
tests %specifically on 
only for code generated by GPT 3.5 (not GPT 4 or GPT 4o)
 based on manually provided %passing
 prompts.
%generated Python code which was . 
%This leaves much to be desired given the release of  GPT 4 and 4o models. 

%\citepauthor{Y24} 
Ye et al. (2024) also %deals with LLM code detection in this manner
follow a similar approach, rewriting both human-authored and machine code, and testing various similarity metrics, %between the original and rewritten code,
achieving up to 0.8325 and 0.8623 accuracy on APS and MBPP benchmarks for Python, respectively, using GPT 3.5 to rewrite the code; they achieved 0.9082 accuracy in detecting C++ code written by GPT 3.5. %This leaves much to be desired in accuracy. 

%\citepauthor{X24}
Xu and Sheng (2024) use masked perturbation and a fine-tuned CodeBERT model on datasets created from CodeNET and AIGCode programming problems in various languages, using text-davinci-003 to generate solutions to problem descriptions or translate code into another language. However, this approach only achieved 0.82 AUC for detecting LLM-authored Java code, 
substantially lower than the results from our approach.