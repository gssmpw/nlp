\documentclass{article}



\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{cite}
\usepackage{hyperref}       % hyperlinks
\hypersetup{hidelinks}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{subcaption}



\title{Detection of LLM-Generated Java Code Using Discretized Nested Bigrams}
%\date{September 9, 1985}	% Here you can change the date presented in the paper title
%\date{} 					% Or removing it

\author{ \href{https://orcid.org/0009-0003-0430-4407}{\includegraphics[scale=0.06]{orcid.pdf}\hspace{1mm}Timothy Paek}\\
	College of Engineering and Computer Science\\
	Syracuse University\\
	Syracuse, NY 13210 \\
	\texttt{tipaek@syr.edu} \\
	%% examples of more authors
	\And
	\href{https://orcid.org/0000-0002-6149-6930}{\includegraphics[scale=0.06]{orcid.pdf}\hspace{1mm}Chilukuri Mohan} \\
	College of Engineering and Computer Science\\
	Syracuse University\\
	Syracuse, NY 13210 \\
	\texttt{ckmohan@syr.edu} \\
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}

% Uncomment to remove the date
%\date{}

% Uncomment to override  the `A preprint' in the header
%\renewcommand{\headeright}{Technical Report}
%\renewcommand{\undertitle}{Technical Report}
\renewcommand{\shorttitle}{Detection of LLM-Generated Java Code}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={Detection of LLM-Generated Java Code Using Discretized Nested Bigrams},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={Timothy Paek, Chilukuri Mohan},
pdfkeywords={LLM-Generated Code, Code Authorship Attribution, GPT Code Detection, Large Language Models (LLMs), Stylometry Features.},
}

\begin{document}
\maketitle

\begin{abstract}
Large Language Models (LLMs) 
  are currently 
  used extensively to 
  generate code by professionals and students, motivating the development of tools 
  to detect LLM-generated code for applications such as academic integrity and cybersecurity. We address this authorship attribution 
  problem as a 
  binary classification task along with feature identification and extraction.
     We propose new    \textit{Discretized Nested Bigram Frequency} features on source code groups of various sizes. 
     Compared to prior work, improvements are obtained  by representing sparse information in dense membership bins.
  Experimental evaluation 
   demonstrated that our approach significantly outperformed a commonly used GPT code-detection API and baseline features, with accuracy exceeding 96\% compared to 72\% and 79\% respectively in detecting GPT-rewritten Java code fragments for 976 files with GPT 3.5 and GPT 4 using 12 features. We also outperformed three prior works on code author identification in a 40-author dataset.  Our approach scales well to larger data sets, and we achieved 99\% accuracy and 0.999 AUC for 76,089 files and over 1,000 authors with GPT 4o using 227 features.
\end{abstract}


% keywords can be removed
\keywords{LLM-Generated Code  \and Code Authorship Attribution \and GPT Code Detection \and Large Language Models (LLMs) \and Stylometry Features.}


  %%% Uncomment this line and comment out the ``thebibliography'' section below to use the external .bib file (using bibtex) .


%%% Uncomment this section and comment out the \bibliography{references} line above to use inline references.
% \begin{thebibliography}{1}

% 	\bibitem{kour2014real}
% 	George Kour and Raid Saabne.
% 	\newblock Real-time segmentation of on-line handwritten arabic script.
% 	\newblock In {\em Frontiers in Handwriting Recognition (ICFHR), 2014 14th
% 			International Conference on}, pages 417--422. IEEE, 2014.

% 	\bibitem{kour2014fast}
% 	George Kour and Raid Saabne.
% 	\newblock Fast classification of handwritten on-line arabic characters.
% 	\newblock In {\em Soft Computing and Pattern Recognition (SoCPaR), 2014 6th
% 			International Conference of}, pages 312--318. IEEE, 2014.

% 	\bibitem{hadash2018estimate}
% 	Guy Hadash, Einat Kermany, Boaz Carmeli, Ofer Lavi, George Kour, and Alon
% 	Jacovi.
% 	\newblock Estimate and replace: A novel approach to integrating deep neural
% 	networks with existing applications.
% 	\newblock {\em arXiv preprint arXiv:1804.09028}, 2018.

% \end{thebibliography}


\section{Introduction}
Detecting the authors of textual materials %source code 
is a problem known as authorship attribution, %which attributes authors to text 
and may be addressed using stylometric  techniques that 
perform quantitative analyses of authors' writing styles.
%However, this can also be used in 
%With the rise of
In addition to analyzing works of literature, the stylometric approach has been explored for the identification of authors of Python code, e.g., \citep{H18}.


The recent extensive use of large language models (LLMs) to generate code 
%in the public eye, such as OpenAIâ€™s ChatGPT, 
%people can now easily generate text to assist in productivity. LLMs are now capable of writing code, 
has raised serious concerns, especially in
academic environments where students 
claim GPT-generated code as their own.
%Note that c
Code generated by the same LLM may vary stylistically between samples, due to the multiplicity of sources used for training the LLM.
%so many students have been using them to cheat on coding homework and exams. Therefore, 
This motivates our research on source code %anomaly detection and 
authorship attribution. %are particularly relevant in distinguishing between code written by students and LLMs.
Applications include academic integrity concerns, software development process evaluation, and assessments of intellectual property ownership.

%cybersecurity (e.g., % to identify source code authors. D
%discovering potentially malicious code). 
% and detecting intellectual dishonesty in academic environments. %scenarios.
%by identifying authors not shown in the positive class of the training set can be used for source code anomaly detection.



%We %attempt to 
%address these issues
We propose a new approach to address this problem, splitting Java source code into code fragments %groups 
before feature extraction, and then using binary classification to identify whether a code fragment %group 
%is anomalous,
varies substantially from other code fragments written by an author, so that
%where the code group is not written by an author within the non-anomalous class. The hope is that by doing so: 
%(1) 
the model can distinguish between code written by different authors in the same file. % (2) we can create more data points per author; and (3) we can extract ASTs from the input file. 

The focus of our research  has been on creating features suited to this task.
We  have also created and tested our approach on datasets with combinations of LLMs,  formatted and non-formatted code. 
We %pass %rewritten
assess
human-authored Java code rewritten by the LLM; this is a scalable approach to large dataset creation while simulating potential prompts.
%Additionally, we
Problem difficulty is increased since the LLM rewrites %only 
human-authored code %with the intuition that this simulates 
written using
various coding styles. %an LLM may have and
%this increases the 
%problem difficulty.
%of distinguishing between the two classes. 


%Researchers have often attempted to identify source code authors %using multiple classification classes, distinguished
%by supervised multi-class training with a neural network model. W
%Unlike prior work, w
We treat LLM source code detection as a binary classification problem, and 
 achieved accuracy exceeding 96\%  in detecting Java code written by GPT 3.5 and GPT 4, %in the GPT rewritten task, 
     %evaluated 
     using a newly created %We create a new 
    dataset of 976 files, % for testing anomaly detection on LLM-generated Java source code using the GPT rewritten task.
   significantly outperforming a commonly used GPT code-detection API. 
   We also achieved accuracy exceeding 98\% %use the same approach 
   on another dataset with 40 authors and 3,021 files. Finally, we achieved 99\% accuracy in detecting GPT 4o with 76,089 files, demonstrating our approach's scalability. % to achieve accuracies of greater than 98 percent. 
   New features (Discretized CodeBERT-embedded Nested Bigrams combined with Nested Bigram Frequencies) improved performance further, to 97.5\% and 99\% accuracy on the %Java GPT Dataset and the 40-author 
   two datasets, respectively. 
   
Section 2 discusses background including related work.
Section 3 presents our approach, and experimental results are provided in Section 4.
Section 5 summarizes and discusses limitations.

\section{Background}
This section summarizes  related work, the stylometric approach, 
and the primary features used in our research.

\subsection{Related Work}

In prior work \citep{Y17, O21, C22, H22, A23}, code authorship attribution has typically been addressed as a multi-class classification problem, %urrently used methods use 
attributing an entire source code file to a single author. However, this has the following limitations:
\\\\ %\begin{enumerate} \item 
1. Files %are usually made up of 
    often have multiple authors, potentially resulting in model confusion. 
\\    %\item 
2. An LLM may have been used to generate only some snippets of code, which may be undetected because the rest of the code was written by the claimant author(s).
 %   \item 
\\ 3. There is often limited data on %individual students, 
    code from an individual, resulting in %there being in
    unavailability of 
    sufficient data for effective training.\\ %a model to train on for their respective classes.
%\end{enumerate}


Additionally, current LLM detectors are trained on %general input text, which can take any form, including 
both code and non-code text, resulting in inadequate performance on code alone. %This has the following limitations:
%\begin{enumerate}
    %\item 
%Classifying any input text usually implies 
Most utilized features are based on %typical 
natural language processing, 
%and hence %the detector may miss out on 
not addressing syntactic features specific to code, such as Abstract Syntax Tree (AST) features. % cannot be extracted in many cases).  \item 
%A general model for all input text may perform less accurately than a model that is trained on a specific task. 
%\end{enumerate}
Three such recent works are discussed below.

%For example,

%\citepauthor{O24} (2024) %deals with
Oedingen et al. (2024)
address GPT code detection %in this manner 
for Python code by utilizing Term Frequency - Inverse Term Frequency (TF-IDF), code2Vec, and human-generated features, %and various models, 
achieving up to 0.98 accuracy on non-formatted code and 0.94 accuracy on formatted code. Their work % paper
tests %specifically on 
only for code generated by GPT 3.5 (not GPT 4 or GPT 4o)
 based on manually provided %passing
 prompts.
%generated Python code which was . 
%This leaves much to be desired given the release of  GPT 4 and 4o models. 

%\citepauthor{Y24} 
Ye et al. (2024) also %deals with LLM code detection in this manner
follow a similar approach, rewriting both human-authored and machine code, and testing various similarity metrics, %between the original and rewritten code,
achieving up to 0.8325 and 0.8623 accuracy on APS and MBPP benchmarks for Python, respectively, using GPT 3.5 to rewrite the code; they achieved 0.9082 accuracy in detecting C++ code written by GPT 3.5. %This leaves much to be desired in accuracy. 

%\citepauthor{X24}
Xu and Sheng (2024) use masked perturbation and a fine-tuned CodeBERT model on datasets created from CodeNET and AIGCode programming problems in various languages, using text-davinci-003 to generate solutions to problem descriptions or translate code into another language. However, this approach only achieved 0.82 AUC for detecting LLM-authored Java code, 
substantially lower than the results from our approach.


\subsection{Stylometry Features}

%When it comes to detecting source code authors, stylometry is common practice, where people 
Stylometry involves describing %source code 
authors' unique coding styles using quantifiable features % metrics 
%that  Common features used for source code authorship attribution are as follows:
such as: % the following:
\\\\ %\begin{enumerate} \item 
1. Lexical features, focusing on %that have to do with
specific tokens in the code, e.g., % Examples include: 
keywords, identifiers, and length.
\\ %\item 
2. syntactic features, addressing the structure of the code, e.g.,  %Examples include: 
n-grams and control structures.
%\item 
\\ 3. %Structural or 
Layout features, relating to code % that have to do with the 
formatting, %of the code. Examples include: 
e.g., white-space, line-breaks, and indentation.\\
%\end{enumerate}

%While all of 
Among these categories, all useful in distinguishing between author coding styles, syntactic features tend to provide the most information specific to certain code authors \citep{C15}.  %By 
Emphasizing syntactic features makes the model %will be 
more robust to layout obfuscation,
%Additionally, those %students
e.g., when those who use LLMs (to generate code)  %often % typically 
reformat the code, changing the amount of white-space.
%
%\subsection{Primary Features}
%In our experiments, we use a combination of lexical, syntactic, and layout (structural) features. However, syntactic features contribute the most information. 

N-gram Abstract Syntax Tree (AST) features have been shown to be powerful in capturing the syntax of source code. 
In particular, \textit{Nested Bigrams (NB)} \citep{H20} %are an improvement on Bigrams that
increase the specificity of features by %making a element of the tuple
focusing on a sub-tree of the AST, instead of just a node. %Nodes (N) are essentially Uni-grams for ASTs. 
%N-gram features on ASTs are highly specific. 



\section{Methods}
This section describes the dataset generation process, followed by the feature extraction approach and the machine learning models used.


\subsection{Dataset Generation}

%As LLM code generation is fairly new, t
To the best of our knowledge,  no publicly available datasets exist for LLM-generated Java code classification. % at the time of this paper. 
%There are multiple LLMs capable of producing code. However, 
%We also note %it's possible that
Also, 
 different LLMs may correspond to different  writing styles, and % Additionally, it's possible that prompt engineering (the formulating of the prompt)
variations in the prompts may also result in variations of writing style. %We hope that the rewriting of different authors' code can emulate the difference in coding style that may result from different prompts.
Hence we created two new datasets: (1) \textit{GPT Dataset}, a smaller one focusing on GPT 3.5 and GPT 4 models; (2) \textit{GPT GCJ Dataset}, a larger one focusing on GPT 4o. We have made both %of these 
datasets publicly available on GitHub \citep{P24_Java, P24_GCJ}. % which will be made publicly available, 
\\ \\
The following process was used to obtain the GPT Dataset:
\begin{enumerate}
    \item 666 Java source code files from 11 different authors' GitHub pages were acquired from another %public 
    dataset \citep{Y17}.
    \item 5 of the 11 authors' files were passed through either ChatGPT 3.5 or Bing GPT 4 in a rewriting task,
%    \item The
using the prompt: %used was: 
"The messages I send you will be in Java code. I want you to rewrite all of it while maintaining functionality."
    \item The entire %ty of the 
    file was passed through BingGPT (4000 character limit) and ChatGPT without additional prompting; the resulting code was then pasted into a new file.
    \item The resulting files were either saved without additional formatting or were formatted using VSCode's format. % when saving the setting.
\end{enumerate}
This resulted in 976  files, including 
   666 files of original authors,
     108 files rewritten  using Bing GPT 4 (61 formatted, 47 non-formatted); and
     202 files rewritten  using ChatGPT 3.5 (59 formatted, 143 non-formatted).
\\ \\    
The following process was used to obtain the GPT GCJ Dataset:

\begin{enumerate}
    \item 58,524 human-authored Java source code files from over 1,000 participants were retrieved from the 2020 Google Code Jam competition.
    \item 17,565 of these files were rewritten by GPT 4o API with the prompt: "This is java code. Rewrite it entirely while maintaining functionality."
\end{enumerate}
This resulted in 76,089 files, including
  58,524 files of original authors, % from the 2020 Google Code Jam, 
 and 17,565 files rewritten using GPT 4o API.
    
By having the LLM rewrite existing code, we attempt to ensure that the LLM code is similar to the original, % as possible
to make it %especially 
difficult to distinguish between the classes. The rewriting task intends to simulate different potential input prompts in a scalable manner. %, by passing in differing input author styles within the prompt.
%positive and negative classes for binary classification. 
The rewritten code typically has rewritten variable names and slightly different formatting while still achieving the same code functionality. Additionally, some rewritten code was reformatted using VSCode to emulate what may occur in real-world scenarios for the GPT Dataset. 
%The hope is that t
%This dataset can be used in the future for evaluating the capability of LLM-generated Java code detection as a general benchmark for initial testing.

Code was broken up into chunks called \textit{code groups}, whose size (number of lines) %These were used to %We
%create datasets based on %feature selection and 
%\textit{code group size}, % The code group size is 
%the number of lines a code group has. Specifically, we test the group sizes:
varied from 10-70, yielding different datasets.
For discretized features, an additional parameter, \textit{bin width}, determines how many features are allocated to a single bin.%Other than these variables, we do not modify any other parameter in the dataset creation for our experiments. 

%For all of our experiments, we include the non-syntactic features: whitespace,  statement words, tabs, underscores, empty lines, mean line length, and mean comment length. All of these



\subsection{Feature Extraction}
For non-syntactic features, we use mean line length, mean comment length, and the numbers of spaces (Whitespace), statement words (e.g., "if", "for", etc.), tabs, underscores, and empty lines.
These are normalized by dividing by the length of the code group in characters; this captures some %additional
contextual information. % by making all of these non-syntactic features ratios with respect to the code group length.

The rest of this section focuses on syntactic features.
We begin by observing that in nested bigrams (NB), nodes %themselves %, already
include attribute information such as the names of input variables, meaning that if another node does not contain the exact attribute information, it will not match. %Combine this 
Feature specificity further increases  %combination with
due to the increased sub-tree relationships captured by nested bigrams. 
%Nested bigrams are an improvement upon bigrams in specificity. 
However, their sparsity (small number of occurences in the dataset) %it's extremely sparse, resulting in typically lower performance despite the amount of information in the dataset. 
appeared to diminish %result in 
performance. % that needed improvement.
Using Principal Component Analysis %(PCA) %(with 0.2, 0.4, 0.6, and 0.8 of the original dimensionality for number of components) 
and Autoencoding %(with dense RELU and Sigmoid layers with 60 epochs) in our experiments 
did not help, motivating the exploration of new features.
%proved to be inconsequential in improving performance and were not included in this paper for space purposes. We use NB-F as a baseline for improving upon. Additionally, we found NB-F performance to be insufficient in the GPT Java Dataset, so we introduce new features to address some of the shortcomings of nested bigrams.

%Having i
Increased feature specificity usually implies that the number of features %for a dataset will also 
increases. For example, a dataset with 1,000 files split into code groups of size 10 can have 90,000+ features from NB alone. %Having highly specific features has the following limitations:
%\begin{enumerate}  \item 
However, using such highly specific features results in %often results in significant 
%increases the number of features which 
increasing the storage requirements for %holding
the feature information.
    %\item Highly specific features often .
%\end{enumerate}
%To deal with these issues, we experiment with 
We address these concerns by exploring new features that use \textit{equal width discretization }(combining multiple features into a single bin) %\citep{G10},
and % We also %experiment with reducing the specificity of the feature by 
%introduce 
\textit{Compressed} Nested Bigrams (CNB) which %is the same feature as NB except without the inclusion of
exclude attribute information, thus reducing the specificity of the feature. %We aim to use d
Discretization  %nested bigram's 
%specificity to create
results in features that represent soft logic membership for subsets of each author's code data.


For syntactic features, we tested the following AST features separately (making different datasets for each); we also experimented with transformer embeddings and frequencies for these features:
%, %BERT has been shown to be performant  
%using CodeBERT to obtain embeddings  specific to code :
\begin{itemize}

\item 
CodeBERT-embedded Nodes 
\textbf{(CBN)} use CodeBERT \citep{R21} to extract CLS token means for AST nodes.%and is used in our experiments to establish the performance of NB-F to another recently developed feature.
 \item
Nested Bigram Frequencies \textbf{(NB-F)}   use subtrees of an AST.
\item 
Compressed Nested Bigrams Frequencies 
\textbf{(CNB-F)}  exclude attribute information, %so there is less 
reducing variation in the number of  features, thus resulting in  a smaller and less sparse dataset, at the potential cost of information loss. 
%\begin{itemize}
\item 
Equal Width Discretized Nested Bigram Frequencies 
\textbf{(EWD-NB-F)} are discretized nested bigrams, where each bin is a simple summation of the frequency of nested bigrams within the range for the bin. %(EWD-NB-F) 
\item 
Equal Width Discretized CodeBERT-Embedded Nested Bigram CLS Means 
\textbf{(EWD-CBNB-CM)} are similar %follow the same philosophy %as EWD-NB-F, except it contains 
but contain additional information from the transformer CLS tokens providing  contextual information. % NB-F  alone. 


%\item 

%\item 
\end{itemize}





%For syntactic features, we focus primarily on the following:
%\begin{itemize}
%      \item  Nodes in the Abstract Syntax Trees (AST);
%      \item Nested Bigrams, i.e., Bigrams on ASTs using subtrees instead of nodes; and 
%    \item Compressed Nested Bigrams, %Nested Bigrams without the inclusion of 
%    which do not include attribute information.
%  \end{itemize}



%Discretizing nested bigrams essentially creates a different representation of nested bigrams that is optimized for classification problems. 
%We process the files in a specific order (each author being processed sequentially). This makes each bin show a degree of membership within a subset of the overall binary classes via soft logic. The combination of these binary class subset memberships is easy for the model to understand compared to simple reduced dimensionality nested bigrams.


%When creating the datasets, the non-syntactic feature selection remains the same, with only the primary syntactic feature being modified. 

\begin{figure*}
   \includegraphics[width=\textwidth]{assets/Feature_Extraction_Diagram_Compressed.png}
    \caption{Feature Extraction %System
Flowchart}
 \label{fig-main}
\end{figure*}

\begin{figure}[h]
\centering
\begin{subfigure}[b]{0.45\textwidth} 
     \includegraphics[width=\textwidth]{assets/Dictionary_Creation.png}
    \caption{Dictionary Creation}
    \label{fig-dictionary}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
  
    
    \includegraphics[width=\textwidth]{assets/dataset_creation.png}%
    \caption{Dataset Creation}
      \label{fig-dataset}
\end{subfigure}       
\caption{Two major components of the Feature Extraction process}
\label{fig-dictionary-dataset}
\end{figure}




%We first go through each file within the input directory where the files are preemptively organized according to whether it's anomalous or not. We then do an initial pass for dictionary creation, where each file is split into code groups based on the input size. 
%During dictionary creation, we map syntactic features to indices to later calculate the dataset column. We start by extracting the AST for the file. Then, the process deviates depending on the specific syntactic feature. 
Our feature extraction process is illustrated in Figure \ref{fig-main}. We take an input dataset and pass it through a dictionary creation block where features are mapped to temporary indices for calculating dataframe columns as shown in  Figure \ref{fig-dictionary}. We then utilize the dictionary in a dataset creation block where feature values are added to appropriate locations as documented by Figure \ref{fig-dataset}, and normalize the data. %We perform a final normalization for the resulting dataset.
\begin{enumerate} 
\item 
For frequency-based features such as NB-F, CNB-F, and EWD-NB-F, we first extract %either NB or CNB 
[compressed] nested bigrams from the file according to the line bounds for the current code group. %Then, all NB or CNB are added to the dictionary, incrementing the key value with each unique addition. 
\item 
For transformer-based features, % such as EWD-CBNB-CM,
we first extract nested bigrams from the file according to the line bounds of the current group, then %tokenize their string representations % of the NB
%using
apply the CodeBERT tokenizer. We use %then do
a sliding window of length 512 tokens (the maximum input for CodeBERT), adding its string representation to the dictionary. %ncrementing the key value with each unique addition. In this paper, we primarily discuss EWD-CBNB-CM. However, w
We also tested CLS maximum and minimum on nested bigrams,
but these did not perform as well as the mean.
%which were found to be less performant than mean and were not included for space purposes.

%We then do our second pass, the dataset creation, where we split each file into code groups, then extract the associated non-syntactic features. We once again extract the NB or CNB from each file's AST. The process again deviates depending on the specific syntactic feature.
\item 
For non-discretized features, such as NB-F and CNB-F, we add the count of that feature in the code group to the index returned by the dictionary for that feature.
\item 
For discretized features such as EWD-NB-F and EWD-CBNB-CM, we again use the dictionary to calculate the dataset column index for each NB or CNB as follows: %which is:
\begin{equation}
    \label{eqn:bin_index_equation}
        \frac{(i - s_1)} {b} + s_2
\end{equation}
where $i$ is the dictionary value associated with the feature; $b$ is the bin width; and $s_1$ is the start position of binning within the dictionary and $s_2$ is the start position of the binning within the dataframe. The first 10 values are reserved for non-syntactic features in both the dictionary and dataset. For datasets with only one discretized syntactic feature, $s_1$ and $s_2$ are $10$. For more than one discretized feature, $s_1$ and $s_2$ vary. Therefore, each bin has the value:
\begin{equation}
    \label{eqn:ewd_bin_equation}
        \sum_{i=a}^{a+w} 
        \frac{F_i}{c}
\end{equation}
where $a$ is the bin's starting index, %associated with the start of the bin, 
$w$ is the width of the bin, $F_i$ %is the feature at that index which
could either be a frequency or the mean of the CLS token, and $c$ is the number of characters in the code group. 
\end{enumerate}

\subsection{Machine Learning Models Used} %Ensembles
%In present-day literature, there are many model structures that have been used for source code authorship attribution and anomaly detection. E

%Several %promising 
%machine learning models and algorithms have been used to address the source code authorship problem. However, o
Our focus in this research has been on identifying the best features to enable identification of LLM-generated code using existing ML models or algorithms, not on improving the latter.
%Hence we attempt to use a simple and well-understood approach in evaluating various features.
For many problems, ensembles of machine learning models have been shown to be 
more effective than individual models  \citep{K20}. %particularly promising for this task, and
%test our datasets on the following:
This motivates us to use %evaluate the use of 
Random Forests \citep{B01}, XGBoost \citep{C16}, Light Gradient Boosting Machine (LGBM) \citep{K17}, and CatBoost \citep{P18} ensemble approaches,  whose code was imported from various publicly available sources, i.e.,  \textit{sklearn.ensemble, xgboost, lightgbm}, and \textit{catboost. }

Ensemble execution is deterministic; % and no statistical tests (such as t-test) were performed.
all random states for these models were initialized to the same value (42) for repeatability, with all other hyperparameters set to constant values. Experiments with other initial states showed little difference. %Without additional parameter tuning, we find that  
The following values were used for the parameters of these machine learning approaches:
\begin{itemize}
    \item \textbf{Random Forest:}
    \begin{itemize}
    \item n\_estimators = 100
    \item criterion = gini
    \item min\_samples\_split = 2
    \item min\_samples\_leaf = 1
    \end{itemize}
\item \textbf{XGBoost:}
\begin{itemize}
    \item booster = gbtree
    \item eta = 0.3
    \item max\_depth = 6
    \item sampling\_method = uniform
    \item grow\_policy = depthwise
\end{itemize}
\item \textbf{LGBM:}
\begin{itemize}
    \item objective = regression
    \item boosting = gbdt
    \item data\_sample\_strategy = bagging
    \item num\_iterations = 100
    \item num\_leaves = 31
    \item tree\_learner = serial
\end{itemize}
\item \textbf{CatBoost:}
\begin{itemize}
    \item iterations = 1000
    \item learning\_rate = 0.03
    \item l2\_leaf\_reg = 3.0
    \item depth = 6
    \item max\_leaves = 31
    \item fold\_len\_multiplier = 2
\end{itemize}
\end{itemize}

We achieved satisfactory results %with this approach,
using default parameter values. 
Performance was sufficiently high, so that additional tuning was not needed. %may be unnecessary.%However, 
%Simpler non-ensemble models %such as KNN 
%may also achieve good performance. % though it will likely be less performant than ensemble approaches.
%\subsection{Feature Extraction}
% I'm not sure how to structure this information, so I'm simply creating these additional paragraphs at the top for now%
%\subsection{Metrics}
Results were evaluated using
well-known measures: 
accuracy, F1 score, 
 Area under the ROC Curve (AUC), and Precision.

%F1 Score:
%\begin{equation}
 %   \label{eqn:F1_score_equation}
  %      2 * \frac{Precision * Recall}{Precision + Recall}
%\end{equation}
%Accuracy:
%\begin{equation}
%    \label{eq:accuracy_equation}
 %       \frac{TP + TN}{TP + TN + FP + FN}
%\end{equation}
%Precision:
%\begin{equation}
 %   \label{eq:precision_equation}
  %      \frac{TP}{TP+FP}
%\end{equation}
%Recall:
%\begin{equation}
 %   \label{eq:recall_equation}
  %      \frac{TP}{TP+FN}
%\end{equation}
%where TP is true positive, TN is true negative, FP is false positive, and FN is false negative respectively.

\section{Experimental Results}
We 
evaluated our approach on %three datasets:
the newly generated GPT and GPT GCJ datasets (described earlier)
as well as 
a 40-author Java Dataset with 3,021 files \citep{Y17} with 15 authors considered the positive class,
%, and the GPT GCJ Dataset (described in Section 3.1), 
splitting training and testing sets. %with random state 42 using sklearn's train\_test\_split (other states are found to be arbitrary). % to compare our results with prior papers.
%For our experiments, we measure performance using AUC, F1 Score, Accuracy, and Precision.
%Following feature extraction, d
Dataset features are subjected to Winsorized normalization;
extreme values are mapped to 0 or 1, and 
 each non-extremal $x$ is mapped to $(x-x_{5\%})/(x_{95\%}-x_{5\%})$, where $x_{k\%}$ refers to the $k$th percentile.
%
%\subsection{GPT Java Dataset Experiment}
%
%We begin our experiment with the feature extraction detailed above. We then normalize the dataset according to the following function:
%
%\begin{equation}
%    \label{eqn:secondary_normalization_equation}
 %       \frac{(x - x_{min})}{(x_{min} + x_{max})} 
%\end{equation}
%\noindent where $x_{min}$ and $x_{max}$ are the 5th and 95th percentile values of the data respectively. This is to avoid extreme cases obscuring additional normalization.
%For discretized features, we initially test %performance according with respect to 
%accuracy to find the optimal bin width. %The non-syntactic feature extraction process is kept constant throughout all experimentation. 

We compared our results with ZeroGPT, an online service (API) that lets users detect whether input text was generated using a GPT model. We tested the API on the same code groups and classified the model's output by checking if
%according to whether 
its probability score (of being generated by GPT) $>$0.5. %We  calculated the AUC score %for the API
%using the probability scores.

\subsection{GPT Dataset Detection Results}

We compare the performance of group sizes 10-70 for various syntactic features, %EWD-NB-F, EWD-CBNB-CM, CBNB-M, CNB-F,  NB-F, and CBN
%and test
using Random Forest, XGBoost, LGBM, and CatBoost. %We test the AUC, F1 Score, Accuracy, and precision. 
%A metric for a given dataset is the 
Results shown are averages % of the metrics for
over all the ensembles tested.  The bin width is optimized for accuracy for discretized features, though %we find that
the bin width %is arbitrary
did not affect results, as long as it is sufficiently large. Table \ref{GPT-results} shows only the results for group sizes 10, 40, and 70, omitting 20, 30, 50, and 60 for space reasons. 

%\begin{figure}
%    \includegraphics[width=\textwidth]{assets/GPT/accuracy VS width.jpeg}
%    \caption{EWD-CBNB-CM Group Size 40 Accuracy vs Width}
%    \label{fig:width}
%\end{figure}

Results are summarized below with average values across all code groups:
%The table values are rounded up.


\begin{table*}[t]
\centering
\caption{GPT Detection Results}
\begin{tabular}{||c||c|c|c|c|c||}
\hline 
\textbf{Dataset} & \textbf{Group Size} & \textbf{Accuracy} & \textbf{F1} & \textbf{AUC} & \textbf{Precision} \\
\hline 
API & 10 & 0.727 & 0.16 & 0.5 & 0.16 \\
%& 20 & 0.7068 & 0.13 & 0.49 & 0.13 \\
%& 30 & 0.71 & 0.13 & 0.48 & 0.14 \\
& 40 & 0.72 & 0.13 & 0.48 & 0.15 \\
%& 50 & 0.73 & 0.15 & 0.48 & 0.18 \\
%& 60 & 0.734 & 0.13 & 0.49 & 0.17 \\
& 70 & 0.73 & 0.13 & 0.48 & 0.18 \\
%& & & & & \\ 
\hline 
CBN& 20 & 0.79 & 0.2 & 0.82 & 0.32 \\
& 40 & 0.76 & 0.17 & 0.78 & 0.35 \\
& 70 & 0.77 & 0.19 & 0.76 & 0.35 \\
\hline
NB-F & 10 & 0.848 & 0.4 & 0.85 & 0.63 \\
%& 20 & 0.84 & 0.35 & 0.86 & 0.61 \\
%& 30 & 0.82 & 0.33 & 0.84 & 0.64 \\
& 40 & 0.79 & 0.28 & 0.85 & 0.52 \\
%& 50 & 0.82 & 0.36 & 0.85 & 0.62 \\
%& 60 & 0.8 & 0.31 & 0.83 & 0.47 \\
& 70 & 0.79 & 0.31 & 0.83 & 0.59 \\
%& & & & & 
\hline %\\
CNB-F & 10 & 0.86 & 0.48 & 0.87 & 0.65 \\
%& 20 & 0.87 & 0.55 & 0.9 & 0.73 \\
%& 30 & 0.88 & 0.6 & 0.89 & 0.8 \\
& 40 & 0.85 & 0.57 & 0.9 & 0.73 \\
%& 50 & 0.88 & 0.6 & 0.93 & 0.85 \\
%& 60 & 0.86 & 0.56 & 0.9 & 0.68 \\
& 70 & 0.84 & 0.54 & 0.9 & 0.76 \\
%& & & & & \\ 
\hline 
EWD-CBNB-CM & 10 & 0.95 & 0.67 & 0.96 & 0.69 \\
%& 20 & 0.96 & 0.74 & 0.97 & 0.8 \\
%& 30 & 0.95 & 0.75 & 0.97 & 0.81 \\
& 40 & 0.96 & 0.82 & 0.98 & 0.84 \\
%& 50 & 0.96 & 0.79 & 0.98 & 0.86 \\
%& 60 & 0.96 & 0.79 & 0.98 & 0.84 \\
& 70 & 0.96 & 0.81 & 0.98 & 0.85 \\
 \hline 
EWD-NB-F & 10 & 0.95 & 0.67 & 0.96 & 0.7 \\
%& 20 & 0.96 & 0.75 & 0.97 & 0.79 \\
%& 30 & 0.96 & 0.77 & 0.97 & 0.85 \\
& 40 & 0.97 & 0.83 & 0.97 & 0.86 \\
%& 50 & 0.96 & 0.81 & 0.98 & 0.86 \\
%& 60 & 0.96 & 0.8 & 0.98 & 0.88 \\
& 70 & 0.96 & 0.79 & 0.98 & 0.84 \\
\hline 
\textbf{EWD-NB-F + EWD-CBNB-CM} & 10 & 0.97 & 0.71 & 0.97 & 0.76 \\
%& 20 & 0.97 & 0.75 & 0.97 & 0.82 \\
%& 30 & 0.97 & 0.77 & 0.98 & 0.83 \\
& 40 & 0.98 & 0.79 & 0.97 & 0.85 \\
%& 50 & 0.98 & 0.82 & 0.99 & 0.85 \\
%& 60 & 0.98 & 0.83 & 0.99 & 0.92 \\
& 70 & 0.97 & 0.8 & 0.98 & 0.84 
\\ \hline 
\end{tabular}
    
    \label{GPT-results}
\end{table*}

\textbf{Accuracy:} 
%Going from the lowest to the highest accuracy scores with respect to code group size (and rounding down): the 
API:  0.72; CBN; 0.78; NB-F: 0.82; CNB-F: 0.86; EWD-CBNB-CM: 0.957; EWD-NB-F: 0.960; EWD-NB-F in combination with EWD-CBNB-CM: \textbf{0.974.}

\textbf{F1:} 
%Going from lowest to highest F1 score with respect to code group size (and rounding down): the 
API: 0.14; CBN: 0.17; NB-F: 0.33; CNB-F: 0.56; EWD-CBNB-CM: 0.767; EWD-NB-F: 0.774; EWD-NB-F in combination with EWD-CBNB-CM: \textbf{0.781.}

\textbf{AUC: }
%Going from lowest to highest AUC based on code group size (and rounding down): the 
API: 0.49; CBN: 0.78; NB-F: 0.84; CNB-F: 0.899; EWD-CBNB-CM: 0.974; EWD-NB-F: 0.972; EWD-NB-F in combination with EWD-CBNB-CM:\textbf{ 0.979.}

\textbf{Precision:} 
%\\ Going from lowest to highest AUC based on code group size (and rounding down): the 
API: 0.16; CBN: 0.33; NB-F: 0.58; CNB-F: 0.74; EWD-CBNB-CM: 0.812; EWD-NB-F: 0.826; EWD-NB-F in combination with EWD-CBNB-CM: \textbf{0.838.}

The performance of ensembles for the same dataset was similar for XGBoost, LGBM, and CatBoost for all metrics tested. Random Forest performs slightly worse.  % in comparison to most metrics. 
Performance was only marginally 
affected by group size.
%changed when changing group sizes. 
%However, they all remained similar.

We %were able to 
achieved high performance in accuracy, F1, AUC, and precision scores in detecting GPT in the Java GPT Dataset. Even the worst performing syntactic feature NB-F significantly outperformed the ZeroGPT API in all code group sizes and in all metrics tested. As the API was primarily meant to be used in detecting GPT generated text, it failed to perform well in detecting GPT generated code, while our approach performed much better. %We believe it may be possible that syntactic feature extraction specific to code can yield greater results than general text extraction in the task of GPT code detection specifically. 

CNB-F performed better than NB-F in all cases, %One explanation for this being the case is that while CNB is significantly less verbose in its requirements for membership compared to NB, it is not overly sparse and 
since it has significantly fewer features. As an example, the NB-F dataset for group size 40 contains 13.5 thousand features, whereas CNB-F contains 52 features for all group sizes. %We believe that features that contain more specific information can suffer from overly dimensional data, possibly making it perform worse than features that contain less information but are easier for the model to understand.

EWD-CBNB-CM and EWD-NB-F perform significantly better than CNB-F, % and are similar to one another in all cases. 
%EWD-NB-F is likely superior to NB-F and CNB-F by
due to reducing the sparsity of the dataset and the density of the information. By representing all the information present in NB-F in dense bins, they retain diverse information %from the specificity of NB while maintaining 
with a small dimensionality, % even smaller than CNB-F. Both EWD-NB-F and EWD-CBNB-CM had a total of 
just 12 features.

Although EWD-CBNB-CM in combination with EWD-NB-F performs about 1\% better than EWD-NB-F, %an additional consideration is that 
generating transformer embeddings for large quantities of Nested Bigram sliding window inputs requires significantly more computation than NB-F alone. %We believe that
Thus, EWD-NB-F alone is perhaps the best approach. %reasonably performant and that including transformer embeddings for a 1 percent improvement in accuracy may not be necessary.

We also performed t-tests for all pairwise combinations of group size 30 feature datasets with respect to accuracy, the same way it was calculated for the Table \ref{GPT-results},
using random initialization and random % from 1-100 for the ensembles and
training vs. testing data splitting. %We find that every 
The results for all pairs of datasets had a combined p-value $<10^{-3}$, %making all 
implying that the differences in performances between datasets are %to be 
not attributable to random variations. %Additionally,
Similar conclusions are drawn from Table \ref{GPT-misc-metrics}. % we find individual performance from syntactic feature selection not attributable to luck.
Deviation in performance is also found to be very low at 0.008 standard deviation at most, with discretized features having up to 0.003 in standard deviation. %The relative performance %with respect to one another i
%was also consistent.
%generally maintained despite the many random states.


\begin{table*}[t]
\centering
\caption{GPT Group Size 30 Associated Accuracy Metrics}
\begin{tabular}{||c||c|c|c|c|c|}
\hline
\textbf{Dataset} & \textbf{Mean} & \textbf{Median} & \textbf{Std Dev} & \textbf{10th Pctl} & \textbf{90th Pctl} \\
\hline
NB-F & 0.821 & 0.821 & 0.008 & 0.808 & 0.831 \\
CNB-F & 0.876 & 0.875 & 0.007 & 0.868 & 0.886 \\
EWD-CBNB-CM & 0.963 & 0.963 & 0.003 & 0.959 & 0.966 \\
EWD-NB-F & 0.962 & 0.962 & 0.003 & 0.958 & 0.965 \\
EWD-NB-F + EWD-CBNB-CM & 0.974 & 0.974 & 0.002 & 0.971 & 0.977 \\
\hline
\end{tabular}
    
    \label{GPT-misc-metrics}
\end{table*}

\subsection{40-Author Anomaly Detection Results}
%\subsection{40 Author Anomaly Detection Experiment}

%We begin our experiment with the feature extraction process detailed above and normalize the dataset according to the secondary normalization equation. 
%We initially tested for the optimal bin width for discretized features with respect to accuracy before testing.

%We also compare our results with other papers on the same dataset. While there is no direct comparison, other papers typically classify entire files into multiple classes. However, it may still be useful as a comparison to gauge performance.

Similar comparisons were carried out with the 40-author data set \citep{Y17}.
Table \ref{40-results} shows only the results for group sizes 10, 40, and 70, again omitting 20, 30, 50, and 60 for space reasons. Results are summarized below with average values across all code groups:
%We compare the performance of group sizes 10-70 for the syntactic features EWD-NB-F, EWD-CBNB-CM, CNB-F, and NB-F and test using Random Forest, XGBoost, LGBM, and CatBoost. We test the AUC, F1 Score, Accuracy, and precision. A metric for a given dataset is the average of the metrics for all the ensembles tested. The table values are rounded up.

\begin{table*}
\caption{Results for 40 Author Dataset}
\centering{
\begin{tabular}{||c||c|c|c|c|c||}
\hline
\textbf{Dataset} & \textbf{Group Size} & \textbf{Accuracy} & \textbf{F1} & \textbf{AUC} & \textbf{Precision} \\ \hline
NB-F & 10 & 0.94 & 0.73 & 0.96 & 0.85 \\
%& 20 & 0.95 & 0.76 & 0.97 & 0.89 \\
%& 30 & 0.95 & 0.76 & 0.97 & 0.93 \\
& 40 & 0.95 & 0.75 & 0.97 & 0.9 \\
%& 50 & 0.95 & 0.75 & 0.97 & 0.92 \\
%& 60 & 0.95 & 0.76 & 0.98 & 0.95 \\
& 70 & 0.95 & 0.77 & 0.98 & 0.95 \\
\hline %& & & & & \\
CNB-F & 10 & 0.95 & 0.75 & 0.97 & 0.9 \\
%& 20 & 0.96 & 0.78 & 0.97 & 0.92 \\
%& 30 & 0.95 & 0.78 & 0.97 & 0.93 \\
& 40 & 0.96 & 0.8 & 0.98 & 0.92 \\
%& 50 & 0.96 & 0.78 & 0.98 & 0.94 \\
%& 60 & 0.95 & 0.79 & 0.99 & 0.96 \\
& 70 & 0.96 & 0.81 & 0.99 & 0.97 \\
\hline %& & & & & \\
EWD-NB-F & 10 & 0.97 & 0.86 & 0.98 & 0.93 \\
%& 20 & 0.98 & 0.89 & 0.99 & 0.94 \\
%& 30 & 0.98 & 0.9 & 0.99 & 0.96 \\
& 40 & 0.98 & 0.91 & 0.99 & 0.95 \\
%& 50 & 0.98 & 0.91 & 1 & 0.97 \\
%& 60 & 0.98 & 0.93 & 1 & 0.99 \\
& 70 & 0.98 & 0.94 & 1 & 0.98 \\
\hline %& & & & & \\
\textbf{EWD-CBNB-CM + EWD-NB-F} & 10 & 0.98 & 0.87 & 0.99 & 0.92 \\
%& 20 & 0.99 & 0.91 & 0.99 & 0.95 \\
%& 30 & 0.99 & 0.93 & 1 & 0.96 \\
& 40 & 0.99 & 0.92 & 0.99 & 0.96 \\
%& 50 & 0.99 & 0.93 & 1 & 0.96 \\
%& 60 & 0.99 & 0.93 & 1 & 0.98 \\
& 70 & 0.99 & 0.95 & 1 & 0.98 \\
\hline
\end{tabular}
}

\label{40-results}  
\end{table*}


\textbf{Accuracy:} %Going from lowest to highest accuracy scores with respect to code group size (and rounding down): 
NB-F: 0.95; CNB-F: 0.96; EWD-NB-F:0.98; EWD-NB-F in combination with EWD-CBNB-CM:\textbf{ 0.99}.

\textbf{F1:} %Going from lowest to highest F1 scores with respect to code group size (and rounding down): 
NB-F: 0.75; CNB-F: 0.78; EWD-NB-F:0.91; EWD-NB-F in combination with EWD-CBNB-CM: \textbf{0.92}.

\textbf{AUC: }
%Going from lowest to highest AUC scores with respect to code group size (and rounding down): 
NB-F:  0.97; CNB-F: 0.98; EWD-NB-F: 0.993; EWD-NB-F in combination with EWD-CBNB-CM: \textbf{0.996}. 

\textbf{Precision:} 
%Going from lowest to highest precision scores with respect to code group size (and rounding down): 
NB-F: 0.91; CNB-F: 0.93; EWD-NB-F in combination with EWD-CBNB-CM: 0.94; EWD-NB-F: \textbf{0.96}.

%The performance %for metrics based on ensemble remained
Results obtained with XGB, CatBoost, and LGBM, were almost identical; the Random Forest approach performed slightly worse.
%in most metrics. We also find that the 
Code group size did not significantly affect the performance in terms of accuracy, F1, and AUC, but affected precision.

%\subsection{40 Author Anomaly Detection Discussion}

We achieved high performance in all code groups using the same methods that we used for the GPT detection task. We also see the same trends in performance for the metrics we tested as measured by the maximum value across code groups; NB-F is outperformed by CNB-F; CNB-F is outperformed by EWD-NB-F; EWD-NB-F is outperformed by EWD-CBNB-CM. Performance for all features is higher in this dataset. Java GPT Dataset contains a subset of the authors in the 40-author dataset which implies that the GPT rewritten task is more difficult than typical binary code classification.
Results in Table \ref{40-comparison} compare our approach with that of three other recent works that address code author identification \citep{A23, O21, Y17}, demonstrating better performance.

\begin{table*}[t] 
\caption{Comparison of performance with different features, for 40 Author Dataset} %Anomaly Detection Comparison
\centering{
\begin{tabular}{||c|c|c|c|c|c|c|c||}
\hline
\textbf{Approach} & \textbf{Type} & \textbf{Group Size} & \textbf{Features} & \textbf{Accuracy} & \textbf{F1} \\ 
\hline
code2seq \citep{O21} & Multi-Class & Full File & -- & 0.967 & 0.9405 \\
Bigram \citep{A23} & Multi-Class & Full File & 1500 & 0.937 & -- \\
PSOBP \citep{Y17} & Multi-Class & Full File & -- & 0.91 & -- \\ 
\textbf{EWD-NB-F} & Binary & 70 & 61 & 0.98 & 0.94 \\
\textbf{EWD-CBNB-CM + EWD-NB-F} & Binary & 70 & 85 & 0.99 & 0.95 \\
\hline
\end{tabular}
}

\label{40-comparison}
\end{table*}

\begin{table*}[h!]
\caption{GPT GCJ Detection Results with EWD-NB-F} % for group size 30, the mean %(and median) 
    %accuracy was 0.986, with s.d. 0.0003
\centering{
\begin{tabular}{%||c
||c||c|c|c|c|}
\hline
%\textbf{Dataset} & 
\textbf{Group Size} & \textbf{Accuracy} & \textbf{F1} & \textbf{AUC} & \textbf{Precision} \\
\hline
%EWD-NB-F & 
10 & 0.98 & 0.87 & 0.987 & 0.92 \\
% 20 & 0.99 & 0.92 & 0.996 & 0.97 \\
% 30 & 0.99 & 0.94 & 0.997 & 0.97 \\
 40 & 0.99 & 0.95 & 0.998 & 0.98 \\
% 50 & 0.99 & 0.95 & 0.998 & 0.98 \\
% 60 & 0.99 & 0.94 & 0.998 & 0.98 \\
 70 & 0.99 & 0.95 & 0.999 & 0.98 \\
\hline
\end{tabular}
}
    
    
  \label{GPT-GCJ-Results} 
\end{table*}

Comparing our results with other approaches applied to the same dataset, we predict binary classes on code groups while previous papers predict multiple classes on entire files. %Comparing our results with other approaches applied to  the same dataset, %
Despite this difference, our approach performed substantially better. We maintain a small dimensionality while improving  performance.  %We believe it may be possible to upscale this approach further and maintain high performance in distinguishing between classes. % 

\subsection{GPT GCJ Dataset Detection Results}

Finally, we %upscale our approach with
addressed the GPT GCJ Dataset, which has up to 143,776 code groups for group size 10 and over 1,000 authors. We focus primarily on EWD-NB-F. Similar %or greater 
performance may % can likely 
be achievable with other features. % EWD-CBNB-CM or EWD-NB-F + EWD-CBNB-CM. 
However, we did not test on those features due to the high %as it's 
computational
requirements, e.g., 
%efficient. 
creation of a single dataset for group size 30 for EWD-CBNB-CM %took longer than
required over 3 days.
%to create. 
%For the results, w
We use bin width 3000, though any width $>$ 2000 performed nearly the same. Results are detailed in Table \ref{GPT-GCJ-Results}. 



Performance remained %remarkably
high despite the significant scale-up in authors and files. Accuracy was maintained at 0.98-0.99; F1 score was %maintained at 
0.92-0.95 (excluding group size 10); AUC was %maintained at
0.987-0.999; and precision was %maintained at 
0.97-0.98 (excluding group size 10). Performance was similar for different ensemble models, with Random Forest models performing slightly better than others.
%has a slight edge. 
Additionally, performance  was stable across various experiments, e.g., with 40 random states for ensemble and train test split initialization, mean accuracy was 0.9855 with standard deviation  0.0003, for group size 30. %as shown in Table \ref{GPT-GCJ-Stats}.

Feature dimensionality remained low at 227-239 total features for bin width size 3,000. However, we find similar performance %to essentially be the same once past
over a certain size. %Therefore, the number of features % quantity
%can be reduced further if desired. 

\subsection{Computational Effort Required}
We used an AMD Ryzen 9 5950X 16-Core Processor with 4.00 GHz, 64 GB RAM, Quadro RTX 5000. 
The feature extraction and dataset generation process required about 3 minutes for the GPT dataset and 10 minutes for the 40-author dataset.
For evaluating discretized GPT datasets using the ensemble models, about 1 minute was required (on average) to complete the experiment for GPT Dataset and 48 minutes for GPT GCJ Dataset, evaluating 7 datasets for each group size. For discretized 40-author datasets, the computational time required by the models was  3 minutes, 28 seconds (on average) 
for all 7 datasets. 


\section{Conclusion}
% need to include broader implications as well
%In summary, w
This paper has addressed the problem of detecting LLM-generated fragments in Java code.
Source code files were split into code groups (10-70  lines each) and categorized using new stylometric features (EWD-NB-F, EWD-CBNB-CM, and CNB-F); the subsequent application of well-known machine learning models %were used, 
resulted in
very high accuracy.
%Our primary contribution is in substantially improving classification performance by introducing new features.
% viz., Equal Width Discretized Nested Bigram Frequencies (EWD-NB-F), Equal Width Discretized CodeBERT-embedded Nested Bigram CLS Means (EWD-CBNB-CM), and Compressed Nested Bigram Frequencies (CNB-F).

Two datasets were created (and have been made available for other researchers) specific to Java GPT code detection: (1) GPT Dataset - with 15 human authors in addition to LLM (GPT 3.5 and GPT 4) rewritten code, (2) GPT GCJ Dataset - with 1k+ human authors in addition to GPT 4o rewritten code, where the LLM was prompted to rewrite the %entirety of the 
code while maintaining functionality. % where the rewritten task aims to simulate differing prompts and contexts.
 Testing was performed using these as well as another  40-author dataset.

 %We maintain 10 non-stylometric features throughout our experiments while varying the inclusion of EWD-NB-F, EWD-CBNB-CM, CNB-F, and Nested Bigram Frequencies (NB-F). We 


We achieved up to 0.977 accuracy on the GPT Dataset by combining EWD-NB-F and EWD-CBNB-CM %while maintaining a mere dimensionality of 
using only 17 features, outperforming the commonly utilized GPTZero API which only achieves 0.73 accuracy. %We use the same approach in a
For the 40-author dataset, %where we classify 15 authors to be non-anomalous, and 
we achieved 0.99 accuracy with %a dimensionality of 
85 features.
%Finally, w
We also achieved up to 0.99 accuracy on the GPT GCJ Dataset with 76k+ files using EWD-NB-F. %which... 

%We believe that the better performance on the 40-author dataset is primarily attributable to the anomaly detection approach (rather than multi-class classification approach), and the discretized features that are informative for this task.

%The GPT Dataset has significantly fewer authors than the 40 author task. However, we perform better on the 40 author anomaly detection task (0.99 accuracy) compared to the GPT detection task (0.977). We believe that the 
%The increased difficulty of the GPT detection task can be attributed to the way the dataset was formulated. Because both the GPT-rewritten and the original human-authored code are present within the same dataset, %it's possible that it's 
%it may be more difficult to distinguish between the two classes in the GPT-rewritten task as the code may be similar when retaining the functionality of the original code. For example, when told to generate code, both GPT and a human author would structure their solution differently in terms of how many methods to create and what parameters they should have. In the GPT-rewritten task, the LLM is told to maintain all functionality which usually results in code with the the same structure as the original in terms of the number of methods and parameters. As a result, detecting GPT code in the GPT rewritten task could require identifying %telling apart 
%more nuanced details, %in comparison to %identifying classes in 
%compared to the 40-author dataset.
%
%The GPT GCJ Dataset has significantly more authors and files than both the GPT Dataset and the 40-author dataset. However, better results were obtained than with the GPT Dataset and on par with the 40-author dataset. We believe this is due to EWD-NB-F being highly scalable, improving in descriptive capability with more input data, without overfitting. 

%Due to limitations of available computational resources, and the deterministic execution of the ensemble machine learning models used, results reported are from single experiments rather than multiple trials.
%We note that c
Our approach %also
significantly outperformed three other recent approaches. % that were tested on the same dataset. 
Consistent results were observed for different settings of code group sizes.  For example, 
we achieved 0.97-0.98 accuracy on the GPT dataset, whereas the ZeroGPT 
API results were substantially worse (accuracy 0.73).  When comparing feature sets, 
the improvement was substantial for one data set (from baseline feature set accuracy 0.79-0.86 to 0.95-0.98); % even in the other, the 
even in the other (easier) task, the accuracy was improved from 0.94-0.95 (baseline feature set) to  0.98-0.99.

Future work includes exploring whether other mathematical operations in discretization could yield better results and making experiments more akin to real-world scenarios (code written by students and professionals). %\section{Limitations}
% probably go over how trained and tested on same dataset, though split. Also, GPT rewritten task. 
%While we achieve extremely promising results on all experiments, we note that the models were trained and tested on the same task, simply splitting the dataset into training and testing. Additionally, it may be possible that the GPT rewritten task is insufficient in representing all coding styles an LLM can output. 
A real world LLM code detector would need to understand many LLM coding styles and be accurate even when given coding styles not seen in the input dataset; this needs to be assessed, and further improvements may be necessary to ensure such robustness. %That said, the GPT rewritten task has been shown to be difficult (as seen comparing performance on the GPT Dataset vs 40 Author Dataset), and discretized nested bigram features appear promising. 
Finally, we note that the focus of this work was to find useful features for the task at hand, not on finding the best machine learning model or algorithm for the author identification task; %it is conceivable that 
performance improvements %that can 
may be obtainable using other machine learning models (or algorithms). %remains to be explored.
 
%A limitation of this study is the exploration of the discretization. 
%While we combined NB features with discretization with a simple summation, it's possible that 

%may provide similar or better performance.


\subsection{Acknowledgements} The first author, an undergraduate student, gratefully acknowledges support from ICCAE
and the Renee Crown 
Honors program 
at Syracuse 
University and the Information Technology Services who provided a GPU in April 2024 for the final experiments whose results are reported.
\newpage
\bibliographystyle{unsrtnat}
\bibliography{references}
\end{document}

