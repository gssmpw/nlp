\documentclass{lmcs} %%% last changed 2014-08-20

%% mandatory lists of keywords
\keywords{Personalization, Federated Learning, Aggregate Computing, Field-based Coordination}

%% read in additional TeX-packages or personal macros here:
%% e.g. \usepackage{tikz}
\usepackage{hyperref}
\usepackage[inline]{enumitem}
\usepackage{mytodonotes}
\usepackage{subcaption}
\renewcommand{\thesubfigure}{\scshape\alph{subfigure}}
\usepackage[capitalise]{cleveref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{tcolorbox}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{acronym}
\usepackage{booktabs}
\usepackage{fontawesome5}
\usepackage{pifont}


\lstdefinelanguage{scala}{
  keywords={abstract,case,catch,class,def,%
    do,else,extends,false,final,finally,%
    for,if,implicit,import,match,mixin,%
    new,null,object,override,package,%
    private,protected,requires,return,sealed,%
    super,this,throw,trait,true,try,lazy,%
    type,val,var,while,with,yield,forSome},
  otherkeywords={=>,<-,<\%,<:,>:,\#},
  sensitive=true,
  columns=fullflexible,
  morecomment=[l]{//},
  morecomment=[n]{/*}{*/},
  morestring=[b]",
  stringstyle=\ttfamily\color{red!50!brown},
  showstringspaces=false,
  morestring=[b]',
  morestring=[b]""",
  basicstyle=\sffamily\lst@ifdisplaystyle\footnotesize\fi\ttfamily,
  emphstyle=\sffamily\bfseries\ttfamily
}
\definecolor{ddarkgreen}{rgb}{0,0.5,0}
\lstdefinelanguage{scafi}{
  frame=single,
  basewidth=0.5em,
  language={scala},
  keywordstyle=\color{blue}\textbf,
  commentstyle=\color{ddarkgreen},
  keywordstyle=[2]\color{teal}\textbf,
  keywords=[2]{rep,nbr,foldhood,foldhoodPlus,aggregate,branch,spawn,mux,mid},
  keywordstyle=[3]\color{gray},
  keywords=[3]{Me,AroundMe,Everywhere,Forever}, %,@@,@@@
  keywordstyle=[4]\color{red}\textbf,
  keywords=[4]{in,out,rd},
  keywordstyle=[5]\color{violet},
  keywords=[5]{evolve,when,andNext,workflow,G,C,broadcast,gossip},
  keywordstyle=[6]\color{orange},
  keywords=[6]{Available,Serving,Done,Waiting,Removing,None,Set}
}

%%\input{myMacros.tex}
%% define non-standard environments BEYOND the ones already supplied
%% here, for example
\theoremstyle{plain}\newtheorem{satz}[thm]{Satz} %\crefname{satz}{Satz}{S\"atze}
%% Do NOT replace the proclamation environments lready provided by
%% your own.

\newlist{questions}{enumerate}{2}
\setlist[questions,1]{label=(RQ\arabic*),ref=RQ\arabic*}
\setlist[questions,2]{label=(\alph*),ref=\thequestionsi(\alph*)}

\def\eg{{\em e.g.,}}
\def\cf{{\em cf.}}
\def\ie{{\em i.e.,}}

\acrodef{fl}[FL]{federated learning}
\acrodef{iid}[IID]{independent and identically distributed}
\acrodef{fbfl}[FBFL]{Field-Based Federated Learning}

%% due to the dependence on amsart.cls, \begin{document} has to occur
%% BEFORE the title and author information:

\begin{document}

% \title{Achieving Personalization in Federated Learning through field-based coordination} 
% \title{Field-Based Coordination Meets Federated Learning} 
\title[Field-Based Coordination for Federated Learning]{FBFL: A Field-Based Coordination Approach for Data Heterogeneity in Federated Learning}

\author[D.~Domini]{Davide Domini\lmcsorcid{0009-0006-8337-8990}}	%required
\address{ Alma Mater Studiorum—Università di Bologna, Via dell'Università, 50, Cesena (FC), Italy }	%required
\email{davide.domini@unibo.it}  %optional

\author[G.~Aguzzi]{Gianluca Aguzzi\lmcsorcid{0000-0002-1553-4561}}	%optional
\address{ Alma Mater Studiorum—Università di Bologna, Via dell'Università, 50, Cesena (FC), Italy }
\email{gianluca.aguzzi@unibo.it}  %optional

\author[L.~Esterle]{Lukas Esterle\lmcsorcid{0000-0002-0248-1552}}	%optional
\address{ Aarhus  University, Aarhus, Denmark }
\email{lukas.esterle@ece.au.dk} %optional

\author[M.~Viroli]{Mirko Viroli\lmcsorcid{0000-0003-2702-5702}}	%optional
\address{ Alma Mater Studiorum—Università di Bologna, Via dell'Università, 50, Cesena (FC), Italy }
\email{mirko.viroli@unibo.it} %optional
%% etc.

%% required for running head on odd and even pages, use suitable
%% abbreviations in case of long titles and many authors:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% the abstract has to PRECEDE the command \maketitle:
%% be sure not to issue the \maketitle command twice!

\begin{abstract}
In the last years, \Ac{fl} has become a popular solution to train machine learning models
 in domains with high privacy concerns.
%
%This paradigm allows the training of models without the need to collect the data in a central server, 
% as only the models themselves are exchanged across the network.
%
However, 
 \ac{fl} scalability and performance face significant challenges 
 in real-world deployments where data across devices are non-independently and identically 
 distributed (non-IID).
%
The heterogeneity in data distribution frequently arises from spatial distribution of devices,
 leading to degraded model performance in the absence of proper handling.
%
Additionally, 
 \ac{fl} typical reliance on centralized architectures introduces bottlenecks
 and single-point-of-failure risks, particularly problematic at scale or in dynamic environments.

To close this gap, 
we propose \Ac{fbfl}, a novel approach leveraging 
 macroprogramming and field coordination to address these limitations through:
\begin{enumerate*}[label=(\roman*)]
  \item distributed spatial-based leader election for personalization to mitigate non-IID data challenges; and
  \item construction of a self-organizing, hierarchical architecture using advanced macroprogramming patterns. 
\end{enumerate*}
%
Moreover, \ac{fbfl} not only overcomes the aforementioned limitations, but also enables the development 
 of more specialized models tailored to the specific data distribution in each subregion.

%
This paper formalizes \ac{fbfl} and evaluates it extensively using 
 MNIST, FashionMNIST, and Extended MNIST datasets. 
%
We demonstrate that, when operating under IID data conditions, \ac{fbfl} performs 
 comparably to the widely-used FedAvg algorithm.
%
Furthermore, in challenging non-IID scenarios, \ac{fbfl} not only outperforms FedAvg but also 
 surpasses other state-of-the-art methods, namely FedProx and Scaffold, which have been 
 specifically designed to address non-IID data distributions.
%
Additionally, we showcase the resilience of \ac{fbfl}'s self-organizing hierarchical architecture against server failures.
\end{abstract}


\maketitle

\section{Introduction}\label{sec:intro}
\paragraph{\emph{Research Context.}} 
Machine learning requires large datasets to train effective and accurate models. 
%
Typically, data are gathered from various sources into one central server where the training process occurs.
%
However, centralizing data on a single device poses significant privacy challenges.
%
These concerns arise not only from the need to share sensitive information but also from the 
 heightened risk of storing all data in a single location, which, if compromised, could result in 
 large-scale data exposure.
%
\Acf{fl} has emerged as a promising solution for training machine learning models 
 in scenarios where \emph{data privacy} is a primary concern,
 enabling devices to 
 collaboratively learn a shared model while keeping their data local~\cite{DBLP:conf/aistats/McMahanMRHA17}.
%
This paradigm not only alleviates the necessity for central data storage but also addresses 
 privacy and efficiency concerns inherent in traditional systems with centralized learning.

\paragraph{\emph{Research Gap.}}
Despite its advantages, in the current landscape of \ac{fl}, training is distributed, but model 
 construction is still predominantly centralized, posing challenges in scenarios characterized 
 by spatial dispersion of devices, 
 heightened risk of single points of failure, and naturally distributed datasets. 
%
Existing peer-to-peer solutions attempt to tackle these concerns; 
 however, they often overlook the spatial distribution of devices 
 and do not exploit the benefits of semantically similar knowledge among nearby devices. 
%
This builds on the assumption that devices in spatial proximity have similar experiences and 
 make similar observations, 
 as the phenomena to capture is intrinsically context dependent~\cite{esterle2022deep}.
%
Moreover, existing approaches often lack the flexibility to seamlessly transition 
 between fully centralized and fully decentralized aggregation methods. 
%
This limitation highlights the potential role of advanced coordination 
 models and programming languages in bridging this gap.
%

\paragraph{\emph{Contribution}}
For these reasons, in this paper, 
 we introduce \acf{fbfl}, a novel approach that leverages computational fields (i.e., global maps from devices to values) as key abstraction 
 to facilitate device coordination \cite{VBDACP-COORDINATION2018,DBLP:journals/corr/Lluch-LafuenteL16} in \ac{fl}. 
 %
By field-based coordination, global-level system behavior can be captured declaratively, 
 with automatic translation into single-device local behavior.
 
We find that this approach
 offers a versatile and scalable solution to \ac{fl}, 
 supporting the formation of personalized model zones.
 %
Most specifically, our approach actually relies on known field-based algorithms of information 
 diffusion and aggregation developed in the context of aggregate computing \cite{VABDP-TOMACS2018}, 
 ultimately defining what we can define ``fields of Machine Learning (sub)models''.
 %
This method enables dynamic, efficient model aggregation without a centralized authority, 
 thereby addressing the limitations of current \ac{fl} frameworks.
%
Therefore, our contributions are twofold:
\begin{itemize}
  \item We demonstrate that field coordination enables performance comparable to centralized approaches under \ac{iid} data settings;
  \item We exploit advanced aggregate computing patterns to efficiently create a self-organizing hierarchical
   architecture that group devices based on spatial proximity, 
   improving performance under non-\ac{iid} data settings.
\end{itemize}
%
By ``self-organizing hierarchical'' we mean a hybrid architecture based on peer-to-peer interactions, but which elects leaders in a distributed manner. 
These leaders will act as aggregators of local models pertaining to sub-portions of the system.
We evaluate our approach in a simulated environment, 
 where well-known computer vision datasets--MNIST~\cite{lecun2010mnist}, FashionMNIST~\cite{DBLP:journals/corr/abs-1708-07747}, 
 and Extended MNIST~\cite{DBLP:journals/corr/CohenATS17}--are 
  synthetically split to create non-IID partitions.
%
As baselines, we employ three state-of-the-art algorithms, namely:
 FedAvg~\cite{DBLP:conf/aistats/McMahanMRHA17}, 
 FedProx~\cite{DBLP:conf/mlsys/LiSZSTS20}, 
 and Scaffold~\cite{DBLP:conf/icml/KarimireddyKMRS20}.
%  demonstrating its effectiveness in comparison to conventional centralized \ac{fl} solutions. 
%
Our findings indicate that this field-based strategy not only matches the performance of existing 
 methods but also provides enhanced scalability and flexibility in \ac{fl} implementations.

\paragraph{\emph{Paper Structure.}} 
This manuscript is an extended version of the conference paper~\cite{DBLP:conf/coordination/DominiAEV24}, providing:
\begin{enumerate*}[label=(\roman*)]
  \item a more extensive and detailed coverage of related work;
  \item an expanded formalization which adds the description of the self-organizing hierarchical
   architecture proposed by this paper;
  \item a largely extended experimental evaluation adding more datasets (i.e., MNIST, Fashion
   MNIST and Expanded MNIST) and more baselines (i.e., FedAvg, FedProx and Scaffold); and
  \item a new experiment for testing the resilience of the 
  self-organizing hierarchical architecture simulating aggregators failures.
\end{enumerate*}

The remainder of this paper is organized as follows: 
 \Cref{sec:rqs} illustrates the research questions tackled in this paper.
 \Cref{sec:related} provides a background and state the motivation for our work.
 \Cref{sec:ac} introduces the main concepts of field-based coordination to understand our approach.
 \Cref{sec:problem-formulation} formalizes the problem statement. 
 \Cref{sec:fbfl} details our field-based coordination strategy for \ac{fl}. 
 \Cref{sec:evaluation} presents an evaluation of our approach in a simulated setting. 
 Finally, \Cref{sec:conclusions} concludes the paper and outlines avenues for future research.

\section{Research Questions}\label{sec:rqs}

The complexity of modern systems where \acf{fl} can be applied, 
 such as the edge-cloud compute continuum, %environments, 
 is rapidly increasing. This 
 poses significant challenges in terms of scalability, adaptability, and contextual relevance~\cite{DBLP:journals/fgcs/PrigentCAC24}. 
% 
Field-based approaches have demonstrated notable advantages in addressing such complexity in various domains, 
 offering robust solutions in both machine learning~\cite{DBLP:conf/acsos/AguzziVE23,DBLP:journals/scp/DominiCAV24,DBLP:conf/woa/DominiFAV24}
  and software engineering~\cite{DBLP:conf/acsos/CortecchiaPCC24,DBLP:conf/acsos/GrushchakKPFABS24,DBLP:conf/acsos/FarabegoliVC24}
  contexts. 
% 
However, research on field-based methodologies within \ac{fl}, 
 particularly in the area of personalized \ac{fl}~\cite{DBLP:conf/acsos/DominiAFVE24,DBLP:conf/acsos/Domini24}, 
 is still at an early stage. 
% 
To advance this research area, our work aims to explore the potential of \ac{fbfl}
 by addressing the following research questions:

\begin{questions}
  \item How does the Field-Based Federated Learning approach perform compared 
   to centralized \ac{fl} under \ac{iid} data? \label{itm:rq1}
  \item Can we increase accuracy by introducing personalized learning zones where \emph{learning devices} are grouped based 
   on similar experiences as often observed in spatially near locations? 
   More precisely, what impact does this have on heterogeneous 
   and non-\ac{iid} data distributions?\label{itm:rq2} 
   \item What is the effect of creating a self-organizing hierarchical architecture through 
   Field Coordination on Federated Learning in terms of 
   resilience, adaptability and fault-tolerance? \label{itm:rq3}
\end{questions}

\section{Background and Motivation}\label{sec:related}
\subsection{Federated Learning}\label{sec:fl}
\begin{figure*}
  \centering
  \begin{subfigure}{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/federated-learning-schema.pdf}
      \caption{Centralized Federated learning schema.}
      \label{fig:cfl}
  \end{subfigure}
  \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/federated-learning-schema-p2p.pdf}
    \caption{Peer-to-peer federated learning schema.}
    \label{fig:p2pfl}
  \end{subfigure}
  \caption{ On the left, a visual representation of Centralized Federated Learning. 
  In the first phase, the server
  shares the centralized model with the clients. In the second
  phase, the clients perform a local learning phase using data that
  is not accessible to the server. In the third phase, these models
  are communicated back to the central server, and finally, in the
  last phase, there is an aggregation algorithm. 
  On the right, the P2P Federated Learning schema.
  Differently from the centralized version, there is no central server. 
  Each client sends its local model to all the other clients, then the aggregation
  is performed locally. 
  }
  \label{fig:tbd}
\end{figure*}

Federated learning~\cite{DBLP:conf/aistats/McMahanMRHA17} is a machine learning technique introduced to
 collaboratively train a joint model using multiple, potentially 
 \emph{spatially distributed}, devices.
% 
Models kind can vary but in this paper, we focus on neural networks.
%
The federation typically consists of multiple \emph{client} devices and 
 one \emph{aggregation} server, which may be located in the cloud. 
%
The key idea behind \ac{fl} is that the data always remains on the device to which it belongs: 
 devices only share the weights of their models, thus making \ac{fl} an excellent solution in 
 contexts where \emph{privacy} is a crucial aspect  (e.g., in health-care 
 applications~\cite{DBLP:journals/jhir/XuGSWBW21,DBLP:journals/csur/NguyenPPDSLDH23}) or where data are \emph{naturally distributed}, and their 
 volume makes it infeasible to transfer them from each client device to a centralized server.
Federated learning can be performed in different ways~\cite{DBLP:journals/tist/YangLCT19}. 
In horizontal FL (a.k.a., sample-based), participants share the same input features but hold distinct sample sets.
They generally follow four steps (see \Cref{fig:cfl}):
\begin{enumerate}
  \item \emph{model distribution}: the server distributes the initial global model to each device. 
   This step ensures that all devices start with the same model parameters before local training begins;
  \item \emph{local training}: each device trains the received model on its local dataset for a specified number of epochs. 
   This step allows the model to learn from the local data while keeping the data on the device, thus preserving privacy;
  \item \emph{model sharing}: after local training, each device sends its updated model parameters back to the server. 
   This step involves communication overhead but is crucial for aggregating the learned knowledge from all devices;
  \item \emph{model aggregation}: the server collects all the updated model parameters from the devices and combines them using an aggregation algorithm (e.g., averaging the weights). 
   This step produces a new global model that incorporates the knowledge learned by all devices. 
   The process then repeats for a predefined number of rounds or until convergence.
\end{enumerate}
Despite its advantages, \ac{fl} faces several challenges, 
 such as \emph{non-IID} data distribution and \emph{resilience} to server failures, which are addressed in the following sections.

\subsubsection{Challenges}

\paragraph{Resilience}
Exploring the network topology presents another avenue for improvement, 
 as the configuration of device communication can significantly influence \ac{fl} performance. 
%
Various structures, such as \emph{peer-to-peer} (P2P), \emph{hierarchical}, and \emph{centralized} networks, 
 offer diverse benefits and challenges. 
%  
Traditionally, \ac{fl} systems have relied on a centralized server for model aggregation. 
%
However, this setup poses scalability challenges and risks introducing a single point of failure. 
%
In response, recent advancements (e.g.,~\cite{DBLP:journals/jpdc/HegedusDJ21,DBLP:conf/dsn/WinkN21,DBLP:conf/icc/Liu0SL20}) 
 have embraced P2P networks to address these limitations, 
 going towards what is called decentralized federated learning---see~\Cref{fig:p2pfl}.
%
P2P architectures, devoid of a central authority, enhance scalability and robustness through 
 the utilization of gossip~\cite{DBLP:conf/aistats/VanhaesebrouckB17} 
 or consensus algorithms~\cite{DBLP:journals/iotj/SavazziNR20} for model aggregation.

\paragraph{Non-IID}
Federated learning is particularly effective when the data distribution across 
 devices is \emph{independent and identically distributed} (IID)~\cite{DBLP:conf/middleware/NilssonSUGJ18}, 
 namely the users experience the same data distribution. 
%
For instance, in a text prediction application, all users may have similar writing styles.
%
However, in real-world scenarios, data distribution is often non-IID, 
 where devices have different data distributions. 
%
This data heterogenity can lead to \emph{model shift} during training, 
 where the model's performance degrades as the training progresses 
 and leading also to slow or unstable convergence.
%
To address these challenges, several approaches have been proposed, 
 such as \emph{FedProx}~\cite{DBLP:conf/mlsys/LiSZSTS20} and \emph{Scaffold}~\cite{DBLP:conf/icml/KarimireddyKMRS20}.

\subsubsection{Approaches}
\paragraph{\emph{FedAvg}}
One of the most common algorithms for horizontal \ac{fl} is \emph{FedAvg}, 
 where the server aggregates the models by averaging the weights of the models received from the client devices.
%
Formally, giving a population of $K$ devices, each of them with a local dataset $D_k$.
Each device $k$ performs $E$ epochs of \emph{local training} on its dataset, 
 computing updates to the weights of its local model, denoted as $\mathbf{w}_k^t \in \mathbb{R}^n$ at 
 the end of the $t$-th global round.
%
The server then computes the weights of the global model $\mathbf{w}^{t+1}$ as the average of the weight 
 vectors from the local models shared by the devices:
\begin{equation}
  \mathbf{w}^{t+1} = \frac{1}{K} \sum_{k=1}^K \mathbf{w}_k^t.
\end{equation}\label{eq:fedavg}
%
This process is repeated for a fixed number of rounds, 
 with the server distributing the global model to the devices at the beginning of each round.

\paragraph{\emph{FedProx}}
This algorithm extends FedAvg by introducing a proximal term to the objective 
 function that penalizes large local model updates.
%
This modification helps address statistical heterogeneity across devices while safely 
 accommodating varying amounts of local computation due to system heterogeneity.

Formally, the local objective function for device $k$ in FedProx is:
\begin{equation} 
  \text{arg}\min_{w} h_k(w; w_t) = F_k(w) + \frac{\mu}{2} \|w - w_t\|^2 
\end{equation}
where $F_k(w)$ is the local loss function, $w_t$ is the global model at round start, 
 and $\mu \geq 0$ is the proximal term coefficient that controls how far local 
 updates can deviate from the global model.

\paragraph{\emph{Scaffold}}
This algorithm introduces control variates to correct model drift caused by data heterogeneity. 
%
Unlike FedAvg, where each client performs model updates locally and communicates its version to the server, 
 Scaffold tries to minimize this drift using control variates that track the direction of gradient descent for each client.
%
In the local model update phase, each client $i$ adjusts its model $y_i$ using the formula 
 $y_i \leftarrow y_i - \eta_l (g_i(y_i) + c - c_i)$, where $\eta_l$ is the local learning rate, 
 $g_i(y_i)$ represents the local gradient, and $c$ and $c_i$ are the server and client control variates respectively.
%
The client's control variate $c_i$ is then updated using either $c^+_i \leftarrow g_i(x)$ or 
 $c^+_i \leftarrow c_i - \frac{1}{K\eta_l}(x - y_i)$, where $x$ is the server's model 
 and $K$ represents the number of local updates.
%
Finally, the server performs global updates. 
%
The global model is updated as $x \leftarrow x + \eta_g \frac{1}{|S|} \sum_{i \in S} (y_i - x)$, 
 where $\eta_g$ is the global learning rate and $S$ is the set of selected clients. 
% 
Simultaneously, the server control variate is adjusted using $c \leftarrow c + \frac{1}{N} \sum_{i \in S} (c^+_i - c_i)$, 
 with $N$ being the total number of clients.

\subsection{Motivation}
 While these approaches to federate networks from multiple learning devices partially address non-IID data distribution challenges,
  they overlook a crucial aspect: the spatial distribution of devices and its relationship
  with data distribution patterns.
%
Research has shown that devices in spatial proximity often exhibit similar data distributions,
  as they typically capture related phenomena or user behaviors~\cite{DBLP:conf/acsos/DominiAFVE24}.
%
This spatial correlation suggests that clustering devices based on spatial proximity
 could enhance model performance and personalization.
%
However, current clustering approaches in federated learning are predominantly centralized
 and rely on traditional clustering algorithms that do not consider the spatial aspects
 of data distribution.
%
This gap highlights the need for an approach that simultaneously addresses:
\begin{enumerate*}[label=(\roman*)]
  \item decentralization,
  \item non-IID data handling, and
  \item spatial correlation in data distributions via distributed spatial-based leader election.
\end{enumerate*}
%
Our work aims to bridge this gap through field-based coordination---see \Cref{tab:methods-comparison} 
 as a comparison between current approaches and our proposal.
 
 
\begin{table}[ht]
  \centering
  \caption{Comparison of Federated Learning approaches.}
  \label{tab:methods-comparison}
  \begin{tabular}{lccc}
  \toprule
  \textbf{Method} & \textbf{Decentralized} & \textbf{Non-IID} & \textbf{Spatial Correlation} \\
  \midrule
  FedAvg   & \ding{55} & \ding{55} & \ding{55} \\
  FedProx  & \ding{55} & \ding{51} & \ding{55} \\
  Scaffold & \ding{55} & \ding{51} & \ding{55} \\
  P2P FL   & \ding{51} & \ding{55} & \ding{55} \\
  FBFL     & \ding{51} & \ding{51} & \ding{51} \\
  \bottomrule
  \end{tabular}
\end{table}

% \begin{figure}[htb]
%   \centering
%     \centering
%     \includegraphics[width=0.6\textwidth]{figures/federated-learning-schema.pdf}
%     \caption{
%       Federated learning schema. 
%       In the first phase, the server shares the centralized model with the clients. 
%       In the second phase, the clients perform a local learning phase using data that is 
%       not accessible to the server. 
%       In the third phase, these models are communicated back to the central server, 
%       and finally, in the last phase, there is an aggregation algorithm.
%     }
%     \label{fig:cfl}
% \end{figure}

% \begin{figure}[htb]
%   \centering
%     \centering
%     \includegraphics[width=0.6\textwidth]{figures/federated-learning-schema-p2p.pdf}
%     \caption{
%       Peer-to-peer federated learning schema. 
%       In the first phase, each node performs a local training process. 
%       In the second phase, the nodes share their models with their neighbors and 
%       finally in the third phase, the nodes aggregate the models received from their neighbors.
%     }
%     \label{fig:p2pfl}
% \end{figure}

% \subsection{Personalized Federated Learning}\label{sec:personalized-FL}


% \begin{figure}[htb]
%   \centering
%     \centering
%     \includegraphics[width=\textwidth]{figures/clustered-FL.pdf}
%     \caption{TBD}
%     \label{fig:clusered-fl}
% \end{figure}

\section{Field-Based Coordination}\label{sec:ac}
Before introducing our approach, we provide a brief overview of field-based coordination, 
 a macroprogramming paradigm that leverages computational fields to facilitate coordination 
 among agents in a distributed system.
\emph{Coordination based on fields}~\cite{DBLP:conf/forte/AudritoVDPB19} (or \emph{field-based coordination}) employs 
 a methodology where computations are facilitated through the concept of \emph{computational fields} (\emph{fields} in brief), 
 defined as distributed data structures that evolve over time and map locations with specific values.
%
This method draws inspiration from foundational works such as Warren's \emph{artificial potential fields}~\cite{DBLP:conf/icra/Warren89} 
 and the concept of \emph{co-fields} by Zambonelli et al.~\cite{DBLP:journals/pervasive/MameiZL04}.
%
Specifically, in the context of co-fields, 
 these computational fields encapsulate contextual data, 
 which is sensed locally by agents and disseminated by either the agents themselves or the infrastructure following a specific distribution rule.

In our discussion, \emph{coordination based on fields} refers to a distinct macroprogramming and computation framework, 
 often referred to as \emph{aggregate computing}~\cite{DBLP:journals/computer/BealPV15}.
%
This framework enables the programming of collective, 
 self-organizing behaviors through the integration of functions that operate on fields, 
 assigning computational values to a collection of individual agents 
 (as opposed to locations within an environment).
%
Thus, fields facilitate the association 
 of a particular domain of agents with their sensory data,
 processed information, and instructions for action within their environment.
%
Computed locally by agents yet under a unified perspective, 
 fields enable the representation of collective instructions, 
 such as a velocity vector field directing swarm movement or 
 a field of numerical values reflecting the swarm's environmental 
 perception~\cite{DBLP:journals/scp/AguzziV25}.
%
To comprehend field-based computing fully, 
 we highlight the system model and the programming model in the following sections.

\subsection{System Model.}\label{ssec:background:sysmodel}
An aggregate system may be defined as a collection of \emph{agents} (or \emph{nodes}) 
 that interact within a shared environment to achieve a common goal.

To better understand the system's behavior, 
 we must first define the system's structure (i.e., the agents and their relationships), 
 the agents' interaction (i.e., how they communicate),
 and their behavior within the environment (i.e., how they process information and act).

\paragraph{\emph{Structure.}}
%
An \emph{agent} represents an autonomous unit furnished with \emph{sensors} and \emph{actuators} to interface 
 with either a logical or physical \emph{environment}.
%
From a conceptual standpoint, an agent possesses \emph{state}, capabilities for \emph{communication} with 
 fellow agents, and the ability to \emph{execute} simple programs.
%
An agent's \emph{neighborhood} is composed of other \emph{neighbor} agents, forming a connected network that can be 
 also represented as a graph---see \Cref{sec:problem-formulation} for more details.
%
The composition of this network is determined by a \emph{neighboring relationship}, designed based on the specific application 
 needs and constrained by the physical network's limitations.
%
Commonly, neighborhoods are defined by physical connectivity or spatial proximity, allowing for communication directly or 
 via infrastructural support, based on certain proximity thresholds.

\paragraph{\emph{Interaction.}}
%
Agents interact by asynchronously sending messages to neighbors,
  which can also occur stigmergically through environmental sensors and actuators.
%
The nature and timing of these messages depend on the agent's programmed behavior.
%
Notably, our focus on modelling continuous, 
 self-organizing systems suggest frequent interactions relative to the dynamics of the problem and environment.

\paragraph{\emph{Behavior.}}
% \begin{figure}[htb]
%     \centering
%     \includegraphics[width=0.8\textwidth]{chapters/img/aggregate-agent-control-architecture.pdf}
%     \caption[Overview of an agent's behaviour in an aggregate system]{Overview of an agent's behaviour in an aggregate system, adapted from~\cite{casadei2021programming}}\label{fig:aggregate-agent-control-architecture}
% \end{figure}
%
%Considering the aforementioned, 
Based on the interaction of agents, an agent's behavior unfolds through iterative \emph{execution rounds}, 
 with each round encompassing the steps below, albeit with some flexibility in the actuation phase:
%
\begin{enumerate}
\item \emph{Context acquisition:} agents accumulate context by considering both their prior state and the
 latest inputs from sensors and neighboring messages.
\item \emph{Computation:} agents process the gathered context, resulting in 
 (i) an \emph{output} for potential actions, and 
 (ii) a \emph{coordination message} for neighborly collective coordination.
\item \emph{Actuation and communication:} agents execute the actions as specified by the output and 
 distribute the coordination message across the neighborhood.
\end{enumerate}
%
By cyclically executing these sense-compute-act rounds, 
 the system exhibits a self-organizing mechanism 
 that integrates and processes fresh data from both the environment and the agents, 
 typically achieving self-stabilization

\subsection{Programming model.}\label{ssec:background:progmodel}
This system model establishes a foundation for collective adaptive behavior,
 necessitating an in-depth elucidation of the ``local computation step,'' 
 facilitated by a \emph{field-based programming model}. 
% 
This model orchestrates the collective behavior through a \emph{field-based program},
 executed by each agent in adherence to the prescribed model. 
Field calculus defines the computation as a composition of \emph{function operations} on fields, and any variants of it allow the developers to express at least 
 i) the temporal evolution of fields, 
 ii) the data interchange among agents, and 
 iii) the spatial partitioning of computation. 
 Various incarnations of this model employ distinct constructs (e.g., \texttt{share}~\cite{DBLP:journals/lmcs/AudritoBDPV20} and
 \texttt{xc}~\cite{DBLP:journals/jss/AudritoCDSV24} calculus). 
%
% In this work, we utilise % reference 
Among them, FScaFi~\cite{DBLP:conf/isola/CasadeiVAD20} is a conceptual framework within the ScaFi~\cite{DBLP:journals/softx/CasadeiVAP22} programming model.
In this variant, the three main operator constructs are \texttt{rep}, \texttt{nbr}, and \texttt{foldhood}. 
 For instance, to model the temporal evolution of a field, 
 one can employ the \lstinline[language=scafi]|rep| construct as follows:
\begin{lstlisting}[language=scafi]
rep(0)(x => x + 1)
\end{lstlisting}
%This snippet illustrates 
Hence, \lstinline[language=scafi]|rep| is the incremental evolution of a field with each cycle,
 representing a non-uniform field. Particularly, the above described code express a field of local counters, 
 where each agent increments its own counter at each cycle.

To facilitate data exchange between agents, 
 ScaFi leverages the \lstinline[language=scafi]|nbr| construct in conjunction with a folding operator:
\begin{lstlisting}[language=scafi]
foldhood(0)(_ + _)(nbr(1))
\end{lstlisting}
Here, \lstinline[language=scafi]|nbr(1)| signifies the neighboring agent's field value, 
 \lstinline[language=scafi]|_ + _| denotes the folding operator, 
 %and
 where  \lstinline|0| is the fold's initial value. 
 This code snippet produces a field where each agent's value it is the number of its neighbors.
 
Lastly, to express spatio-temporal evaluation, a combination of the aforementioned constructs is utilized:
\begin{lstlisting}[language=scafi]
rep(mid) { minId => foldhood(0)(math.min)(nbr(minId)) }
\end{lstlisting}
This combination calculates the minimum value of ID in the entire network.
This demonstrates the integration of spatial 
 and temporal computation through a synergistic application of both constructs.
%
\subsection{Coordination Building Blocks.}
On top of this minimal set of constructs, 
 ScaFi provides a set of building blocks for developing complex coordination algorithms.
Notably, these blocks are designed to be \emph{self-stabilizing}--
 converging to a stable state from any initial condition--
 and \emph{self-organizing}---adapting dynamically to environmental changes and network topology variations. 
% 
A cornerstone among these is the \emph{self-healing gradient} computation, 
a distributed algorithm for maintaining the minimum distance from a designated source node 
to every other node in the network. 
Building upon standard gradient-based approaches, 
this mechanism automatically recomputes and updates distance estimates whenever changes occur in the network (e.g., node arrivals/removals or communication failures),
highlighting the algorithm's \emph{self-healing} property and making it highly suitable for dynamic, large-scale environments.
 Within ScaFi, this is described as follows:
\begin{lstlisting}[language=scafi]
def gradient(source: Boolean): Double
\end{lstlisting}
Building upon this foundation,
 more sophisticated coordination algorithms can be developed, such as:
\begin{itemize}
    \item \textbf{Gradient cast}: a mechanism to disseminate information from a source node across the system using a gradient-based approach. 
    In ScaFi, this is expressed as:
    \begin{lstlisting}[language=scafi]
G[V](source: Boolean, value: V, accumulator: V => V)
    \end{lstlisting}
    Here, 
    \lstinline[language=scafi]|source| identifies the gradient's origin, 
    \lstinline[language=scafi]|value| is the disseminated information, and \lstinline[language=scafi]|accumulator| is the function for aggregating the disseminated value.
    \item \textbf{Collect cast}: conceptually the inverse of gradient cast, 
    it aggregates information from the system back to a specific zone. 
    It is represented as:
\begin{lstlisting}[language=scafi]
C[V](potential: Double, accumulator: V, localValue: V, null: V)
\end{lstlisting}
in this construct, \lstinline|potential| indicates the collection's potential, \lstinline[language=scafi]|accumulator| is the function for aggregating values towards the source, \lstinline[language=scafi]|localValue| is the value being collected, and \lstinline[language=scafi]|null| signifies the absence of value.
    \item \textbf{Sparse choice}: 
    a distributed mechanism for node subset selection and spatial-based leader election, expressed as:
\begin{lstlisting}[language=scafi]
S(radius: Double): Boolean    
\end{lstlisting}
where \lstinline[language=scafi]|radius| specifies the maximum radius within which a leader can influence other nodes. 
The algorithm is ``spatial-based'' as it leverages physical distances between nodes to elect leaders: 
each leader node creates a sphere of influence with radius \lstinline[language=scafi]|radius|, 
and nodes within this radius become part of that leader's region.
\end{itemize}
By integrating these constructs, 
 it becomes possible to execute complex collective behaviors, 
 such as crowd management~\cite{DBLP:journals/computer/BealPV15}, distributed sensing and action~\cite{iee-areas}, and swarm behaviors~\cite{macroswarm}.
%
Furthermore, it is feasible to integrate 
 the aforementioned programming model with deep learning techniques, 
 advancing towards a paradigm known as \emph{hybrid aggregate computing}~\cite{DBLP:conf/icdcs/AguzziCV22}. 
 AC can orchestrate or enhance the learning mechanisms~\cite{DBLP:conf/acsos/AguzziVE23}, 
 and conversely, learning methodologies can refine AC~\cite{DBLP:conf/acsos/AguzziCV22}. 
%
This paper adopts the initial perspective, 
 delineating a field-based coordination strategy for FL. 
 The objective is to create a distributed learning framework that is inherently self-stabilizing and self-organizing. 

\subsection{Self-organizing Coordination Regions.}
% Self-organizing Coordination Regions (SCR) Pattern Description:

Recent advances in field-based coordination introduced a pattern called
Self-organizing Coordination Regions (SCR)~\cite{DBLP:conf/coordination/CasadeiPVN19}.
This strategy enables distributed collective sensing and acting without
relying on a dedicated authority, all while ensuring both self-stabilization
and self-organization of the system.
The SCR pattern operates as follows:
\begin{enumerate}
  \item A distributed multi-leader election process selects a 
   set of regional leaders across the network (e.g., using the \texttt{S} operator);
  \item The system self-organizes into distinct regions, each governed by a single leader (e.g., leveraging the \texttt{G} operator); 
  and
  \item Within each region, a feedback loop is established where the 
   leader collects upstream information from the agents under its influence and, 
   after processing, disseminates downstream directives (using both the \texttt{C} and \texttt{G} operators, see \Cref{fig:scr} for an overview).
\end{enumerate}

This pattern effectively combines the previously discussed building blocks 
and is straightforward to implement in the aggregate computing context.
An example implementation skeleton is provided below:
In the following we show a compact implementation in ScaFi of such complex pattern.

\begin{lstlisting}[language=scafi]
  def SCR(radius: Double): Boolean = {
    // This function implements the SCR pattern, dividing the space into regions
    val leader = S(radius)                     // Elect leaders within the given radius
    val potential = gradient(leader)           // Create the region around each leader
    val collectValue = 
      C(potential, accumulationLogic, localValue, nullValue) // Collect data from region
    // Leaders decide how to process the collected data
    mux(leader) {
      decide(collectValue)
    } {
      nullValue
    }
    // Broadcast the decision to all devices in the region
    G(leader, collectValue, identity)
  }
\end{lstlisting}

\begin{figure*}
  \centering
  \begin{subfigure}{0.32\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/scr1.pdf}
      \caption{Information collection.}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/scr2.pdf}
    \caption{Leader computation.}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/scr3.pdf}
    \caption{Information sharing.}
  \end{subfigure}
  \caption{ Graphical representation of the Self-organizing Coordination Regions pattern.  
    First, information within each area is collected in the respective leader. 
    Then, each leader processes the collected information and shares it back to the clients.
  }
  \label{fig:scr}
\end{figure*}


\section{Problem Formulation}\label{sec:problem-formulation}
% \davide{
%   Considerations:

%   i) Should we mention that datasets are sampled from a distribution which may be unique to each area?
  
%   ii) We are formalizing P2P FL but not stating anything about the semi-centralized version 
% }
Consider a distributed network comprising a set of devices, 
 denoted as $N$, where each device $d \in N$ possesses a dataset unique to it. 
 The dataset on device $d$ is used to train a local model, 
 characterized by a weights vector $\mathbf{w}_d$. 
 The objective function local to device $d$, $f_d(\mathbf{w}_d)$, 
 quantifies the performance of the model on this local dataset, 
 with the aim being to minimize this function.
 
The general problem that FL seeks to solve is constructing a global model by optimizing a global objective function $f(\mathbf{w})$, 
 formulated as the aggregate of all local objective functions, adjusted by the size of the local datasets. 
 This is mathematically represented as:
\begin{equation}
    f(\mathbf{w}) = \sum_{d \in N} \alpha_d f_d(\mathbf{w}_d),
\end{equation}
where $\alpha_d = \frac{n_d}{|D|}$ signifies the weight of device $d$'s data in the global objective, $n_d$ is the count of samples in device $d$'s dataset, and $|D| = \sum_{d \in N} n_d$ represents the total number of samples across all devices.
%
\paragraph{\emph{Peer-to-peer FL}}In the peer-to-peer model of FL,
 even if the problem is conceptually the same,
 the clients, instead of taking a model from a central server,
 directly exchange information to collaboratively 
 learn a global model without the intermediation of a central server. 
This network can be modelled as a graph $\mathcal{G} = (\mathcal{N}, \mathcal{E})$,
 with nodes representing devices and edges symbolizing direct communication links. 
 The neighbors of a device $d$, with which $d$ can exchange messages, 
 are denoted by $\mathcal{N}_d = \{d' \in \mathcal{N} | (d, d') \in \mathcal{E}\}$.

The peer-to-peer federated learning process in this context 
 unfolds over several rounds (see \Cref{algo:p2p-fl}), each encompassing:
\begin{enumerate}
    \item \emph{local training}: devices independently update their local models by training on their datasets for $p$ epochs, 
    aiming to minimize the local objective function $f_d(\mathbf{w}_d)$;
    \item \emph{model sharing}: devices share their updated weights vector $\mathbf{w}_d$ with their neighbors, also expressed as $d \overset{\mathbf{w}_d}{\leadsto} {d^*}$;
    \item \emph{model aggregation}: each device combines the received weights vectors from its neighbors to update its local model to $\mathbf{w}_d'$.
\end{enumerate}

In this approach we directly share the model with all the neighbors (i.e., $\mathcal{N}_d$, point 2), 
but potentially it is possible to incorporate strategies to minimize communication overhead by selectively sharing models or parts thereof 
(e.g., gradients or specific layers).
Each node stores the received model in a buffer $\mathcal{B}_d$,
 and then the aggregation algorithm is executed.
%
The aggregation of models is defined by:
\begin{equation}
    \mathbf{w}_d' = \mathcal{A}(\mathbf{w}_d, \{\mathbf{w}_{d'} | d' \in \mathcal{B}_d\}),
\end{equation}
where $\mathcal{A}$ is the aggregation algorithm, 
 which could range from simple averaging to more sophisticated methods. 
% In this paper, we apply simple averaging for the aggregation of weights.
Through the iterative execution of these steps, 
 the system converges towards a unified model that approximates the outcome of a centrally coordinated federated learning process.

 \begin{algorithm}
    \caption{Peer-to-Peer Federated Learning Process}\label{algo:p2p-fl}
    \begin{tcolorbox}[
      colback=gray!5,
      colframe=gray!50,
      boxrule=0.5pt,
      arc=1mm
    ]
    \begin{algorithmic}[1]
    \Require devices $|N|$, Network graph $\mathcal{G}$, Epochs per round $p$, Maximum rounds $\mathcal{T}$
    \Procedure{P2PFederatedLearning}{}
        \For{$\text{round} = 1$ \textbf{to} $\mathcal{T}$}
            \ForAll{$d \in N$}
                \State \textbf{local training:} Minimize $f_d(\mathbf{w}_d)$ by training on $d$'s dataset for $p$ epochs
                \State \textbf{model sharing:} Share $\mathbf{w}_d$ with $\mathcal{N}_d$ following $\leadsto$
            \EndFor
            \ForAll{$d \in N$}
                \State \textbf{model aggregation:} Update $\mathbf{w}_d$ to $\mathbf{w}_d'$ using $\mathcal{A}(\mathbf{w}_d, \{\mathbf{w}_{d'} | d' \in \mathcal{B}_d\})$
            \EndFor
        \EndFor
    \EndProcedure
\end{algorithmic}
\end{tcolorbox}
\end{algorithm}
\paragraph{\emph{Self-organizing hierarchical FL}}
This peer-to-peer federated learning process is the foundation of our approach, 
 which can also be extended to a self-organizing hierarchical approach, 
 where a subset of nodes is designated as aggregators,
 namely the one responsible for aggregating the models and disseminating the global model to the network.
%
Notably, elected leaders are not fixed but may change over time if network topology evolves (e.g., due to the failure or the
 movement of some nodes).

Consider a distributed leader election algorithm $\mathcal{DL}$ to select the aggregators, (e.g., using the \texttt{Sparse Choice} construct),
 and then the process is similar to the one described in \Cref{algo:p2p-fl} but where the aggregators are responsible for the aggregation and dissemination of the global model.
%
Moreover, each node can only belong to a single leader. 
For simplicity, we use a distance-based leader selection rule, therefore forming a Voronoi-like partitioning of the network.
Formally, let a node~\(d\) be given, and let \(\mathcal{L}\subseteq\mathcal{N}\) be the set of leaders computed by \(\mathcal{DL}\). 
We say that \(d\) is under the influence of a leader \(l \in \mathcal{L}\) if and only if
\[
\forall \, l' \in \mathcal{L}\setminus\{l\},
\quad
\mathrm{dist}(d,l)
<
\mathrm{dist}(d,l'),
\]
where \(\mathrm{dist}(\cdot,\cdot)\) is the chosen distance metric. In case of equality, a predefined tie-breaking rule is applied. 
The set of nodes under the influence of \(l\) is denoted \(\mathcal{L}_l\), 
and let \(d_{\mathcal{L}}\) be the leader to which \(d\) belongs. 
Finally, we define a forward chain from a node \(d'\) to a node \(d''\) (written \(d' \Longrightarrow d''\)) as a sequence
\[
d_1, d_2, \dots, d_k
\quad
\text{such that}
\quad
d_1 = d',
\quad
d_k = d'',
\quad
\text{and}
\quad
d_i \leadsto d_{i+1}
\ \text{for} \
i=1,\dots,k-1.
\]
This formalizes that even if two nodes are not directly connected, there exists a chain of direct influences linking them.
With these definitions, the self-organizing hierarchical federated learning process is described 
 in \Cref{algo:sc-fl}.
\begin{algorithm}[htb]
  \caption{Semi-Centralized Federated Learning Process}\label{algo:sc-fl}
  \begin{tcolorbox}[
    colback=gray!5,
    colframe=gray!50,
    boxrule=0.5pt,
    arc=1mm
  ]
  \begin{algorithmic}[1]
  \Require devices $|N|$, Network graph $\mathcal{G}$, Epochs per round $p$, Maximum rounds $\mathcal{T}$
  \Procedure{SemiCentralizedFederatedLearning}{}
    \For{$\text{round} = 1$ \textbf{to} $\mathcal{T}$}
      \State $\text{Leaders} \gets \mathcal{DL}$
      \ForAll{$d \in N$}
        \State \textbf{local training:} Minimize $f_d(\mathbf{w}_d)$ by training on $d$'s dataset for $p$ epochs
        \State \textbf{leader selection:} Compute $d_\mathcal{L}$ using $\text{Leaders}$
        \State \textbf{model sharing:} Share $\mathbf{w}_d$ with $d_\mathcal{L}$ following $d \Longrightarrow d_\mathcal{L}$
      \EndFor
      \ForAll{$d \in N$}
        \If{$d \in \text{Leaders}$}
          \State \textbf{model aggregation:} Update $\mathbf{w}_d$ to $\mathbf{w}_d'$ using $\mathcal{A}(\mathbf{w}_d, \{\mathbf{w}_{d'} | d' \in \mathcal{B}_d\})$
          \ForAll{$d' \in \mathcal{L}_d$}
            \State Share $\mathbf{w}_d'$ with $d'$ following $d \Longrightarrow d'$
          \EndFor
        \EndIf
      \EndFor
    \EndFor
  \EndProcedure
  \end{algorithmic}
  \end{tcolorbox}
\end{algorithm}

\section{Field-Based Federated Learning}\label{sec:fbfl}

This section describes the contribution of this paper, namely the  
 integration concepts of aggregate computing into the federated learning framework
 to improve adaptability and robustness in decentralized networks.
%
Our approach enables the learning processes to \emph{dynamically} conform to changes
 in network topology, effectively eliminating reliance on centralized coordination points. 
% 
This adaptability ensures that
 the system autonomously reconfigures in case of failures to maintain operational stability and 
 continue the learning process without significant interruptions.

\subsection{Learning Process}\label{sec:learningprocess}

In this section, we mainly discuss the self-organizing hierarchical version, 
because the peer-to-peer version is simply an application of the aggregate constructs.
At its core, self-organizing hierarchical \ac{fl} can be conceptualized as an application of the SCR pattern~\cite{DBLP:conf/coordination/CasadeiPVN19}.
 In our framework, nodes can be designated as \emph{aggregators}---either through pre-assignment (e.g., edge servers) or 
 dynamically selected based on the network's evolving topology, 
 leveraging principles similar to those in \emph{space-fluid} computing~\cite{DBLP:journals/lmcs/CasadeiMPVZ23}.

Initially, each participating node $d \in N$ initializes its model $\mathbf{w}_d^{(0)}$. 
 Subsequently, at each iteration $t$, 
 it undertakes a local learning step to update $\mathbf{w}_d^{(t)}$,
 after which the model updates are directed toward an aggregator node, 
 guided by a dynamically formed potential field.
%
Therefore, $\leadsto$ function is replaced by a field-based model sharing mechanism (namely, \texttt{nbr}) 
 where the potential field guides the model updates to the aggregators, 
 and $\Longrightarrow$ is replaced by collect cast operation to ensure that the model updates are directed to the appropriate aggregators nodes.
%
Aggregator nodes play a crucial role in the system. 
 They are responsible for collecting the model updates from participant nodes, 
 computing a consensus model $\mathbf{w}^{(t+k)}$ (where k is the global epoch value) 
 by averaging, and disseminating the updated model back to the nodes in their zone of influence. 
This process ensures a distributed yet coordinated approach to model learning and sharing across the network.
%
To accommodate communication delays and ensure timely model updates, 
 nodes are capable of adjusting their models with local corrections. 
 This mechanism, denoted as $\Delta \mathbf{w}_d^{(t)}$, 
 accounts for potential discrepancies between the model's state when shared and when the aggregated update is received, 
 enabling a more resilient and responsive learning process.

Note that the aggregation and sharing part can follow different strategies, 
 depending on the specific application requirements and the network's characteristics. 
In this work, we follow a standard FedAvg approach,
 where the aggregation is performed by averaging the models received from the neighbors--see \Cref{eq:fedavg}.
 This ensures that despite the dynamic and decentralized nature of the network,
 a coherent and unified model is learned and shared among the nodes.
\subsection{Implementation Insights}\label{sc:implementationins}
The following ScaFi code snippet provides a self-organizing hierarchical federated learning implementation, 
 based on the abode described SCR pattern.
This variant utilizes potential fields for guiding model sharing and employs a broadcast mechanism to disseminate the aggregated global model back to the nodes:
\\[0.5em]
\noindent\begin{minipage}{\textwidth}
\begin{lstlisting}[language=scafi, caption={Code structure of FBFL drawn from the respective repository.}]
val aggregators = S(area, nbrRange) // Dynamic aggregator selection
rep(init())(model => { // Local model initialization
  // 1. Local training step
  model.evolve() 
  val pot = gradient(aggregators) // Compute potential field for model sharing
  // 2. Model sharing 
  val info = C[Double, Set[Model]](pot, _ ++ _, Set(model), Set.empty) 
  // 3. Model aggregation
  val aggregateModel = aggregation(info) 
  // 4. Global model update
  sharedModel = broadcast(aggregators, aggregateModel) 
  mux(impulsesEvery(epochs)) 
    { combineLocal(sharedModel, model) } { model }
})
\end{lstlisting}
\end{minipage}
%
In this code snippet, the \lstinline[language=scafi]|aggregators| variable represents the set of aggregators, 
 dynamically selected based on the network's topology (i.e., via \lstinline[language=scafi]|S|). 
 The \lstinline[language=scafi]|rep| construct initializes the local model, 
 and the \lstinline[language=scafi]|evolve| method updates the model based on the local dataset via local training. 
 The \lstinline[language=scafi]|gradient| construct computes the potential field for sending the model to the aggregators,
 and the \lstinline[language=scafi]|C| effectively collects the models from the whole region. 
 The \lstinline[language=scafi]|aggregation| method aggregates the models, 
 and the \lstinline[language=scafi]|broadcast| method disseminates the global model back to the nodes. 
 Finally, the \lstinline[language=scafi]|mux| construct ensures that the model is updated at regular intervals and eventually combined with local corrections.
\section{Experimental Evaluation}\label{sec:evaluation}

\subsection{Experimental setup}
To evaluate the proposed approach, 
 we conducted experiments on three well-known used computer vision datasets: 
 MNIST~\cite{lecun2010mnist}, FashionMNIST~\cite{DBLP:journals/corr/abs-1708-07747}, and 
 Extended MNIST~\cite{DBLP:journals/corr/CohenATS17}---their characteristics are summarized in~\Cref{tab:datasets}.
% 
We employed three state-of-the-art federated learning algorithms for comparison: 
 FedAvg~\cite{DBLP:conf/aistats/McMahanMRHA17}, FedProx~\cite{DBLP:conf/mlsys/LiSZSTS20}, 
 and Scaffold~\cite{DBLP:conf/icml/KarimireddyKMRS20}.
%
First, we evaluated FBFL against FedAvg under a homogeneous data distribution 
 to assess its stability in the absence of data skewness. 
%
We then created multiple heterogeneous data distributions through synthetic partitioning and 
 compared FBFL with all three baseline algorithms. 
%
Specifically, we employed two partitioning strategies: 
\begin{enumerate*}[label=(\roman*)]
  \item a Dirichlet-based split, where each party received samples from most classes, 
   but with a highly imbalanced distribution, leading to certain labels 
   being significantly underrepresented or overrepresented; and
  \item a hard data partitioning strategy, where each region contained only a subset of labels, 
  resulting in a more challenging learning scenario. 
\end{enumerate*}
%
A graphical representation of these distributions is given in~\Cref{fig:distributions}.
%   
Experiments were conducted using different numbers of regions, specifically $A \in \{3, 6, 9\}$,
 and $50$ devices.

Finally, we assessed the resilience of our self-organizing hierarchical architecture by simulating 
 a failure scenario. 
%
In this experiment, we randomly killed two servers after a certain period of time and evaluated 
 whether the system could self-stabilize by re-electing new leader nodes to function as aggregator servers.

All experiments were implemented using PyTorch~\cite{paszke2017automatic} for training.
%
Additionally, the proposed approach leverages ScaFi for aggregate computing and 
 ScalaPy~\cite{DBLP:conf/scala/LaddadS20} to enable interoperability between ScaFi and Python.
%
Moreover, we used a well-known simulator for pervasive systems, namely: Alchemist~\cite{DBLP:journals/jos/PianiniMV13}.
%
To ensure robust results, each experiment was repeated five times with different seeds.

To promote the reproducibility of the results, all the source code, 
the data, and instructions for running have been made available on 
 GitHub; both for the baselines\footnote{\url{https://github.com/davidedomini/experiments-2025-lmcs-field-based-FL-baselines}}
 and for the proposed approach\footnote{\url{https://github.com/davidedomini/experiments-2025-lmcs-field-based-FL}}.


\begin{table}
  \centering
  \caption{Overview of the datasets used in the experiments.}
  \label{tab:datasets}
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Dataset} & \textbf{Training Instances} & \textbf{Test Instances} & \textbf{Features} & \textbf{Classes} \\
    \midrule
    MNIST          & 60\,000  & 10\,000 & 784 & 10 \\
    Fashion MNIST  & 60\,000  & 10\,000 & 784 & 10 \\
    Extended MNIST & 124\,800 & 20\,800 & 784 & 27 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure*}
  \centering
  \begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/IID.pdf}
    \caption{IID data.}
    % \label{fig:cfl}
\end{subfigure}
  \begin{subfigure}{0.32\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/Dirichlet.pdf}
      \caption{Dirichlet distribution.}
      % \label{fig:cfl}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/Hard.pdf}
    \caption{Hard partitioning.}
    % \label{fig:cfl}
  \end{subfigure}
  \caption{ A graphical representation of three different data distribution in 5 subregions. 
  Each color represents a different subregion.
  The second and the third images are two examples of non-IID data.}
  \label{fig:distributions}
\end{figure*}

\subsection{Discussion}
In the following, we present the results of our experiments, 
 comparing the proposed approach with the baseline algorithms under different conditions 
 and replying to the research questions posed in~\Cref{sec:rqs}.
% COMPARE UNDER IID 
\begin{tcolorbox}[colback=gray!10!white, colframe=gray!80!black, title=\textbf{\ref{itm:rq1}}]
\emph{How does the Field-Based Federated Learning approach perform compared to centralized FL under IID data?}
\end{tcolorbox}
\noindent To answer~\ref{itm:rq1}, we evaluated FBFL against FedAvg under homogeneous data distribution.
%
The goal of this evaluation was to show how the proposed approach, based on field coordination, 
 achieves comparable performance to that of centralized FL while introducing all 
 the advantages discussed in~\Cref{sec:fbfl}, such as: the absence of a single point 
 of failure and the adaptability.
%
\Cref{fig:exp-iid} shows results on the MNIST (first row) and Fashion MNIST (second row) datasets. 
%
It is worth noting that both the training loss and the validation accuracy exhibit similar trends. 
%
As expected, our decentralized approach shows slightly more pronounced oscillations in both metrics
compared to FedAvg, due to the lack of a central coordinating authority.
%
However, despite these inherent fluctuations in the decentralized setting,
FBFL still achieves comparable final performance after the same number of global communication rounds,
matching the effectiveness of traditional centralized learning methods as observed in the literature.

\begin{figure*}
  \centering
  \begin{subfigure}{0.4\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/charts/TrainingLoss_dataset-MNIST_areas-3_partitioning-IID.pdf}
      % \caption{TBD}
      % \label{fig:cfl}
  \end{subfigure}
  \begin{subfigure}{0.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/charts/ValidationAccuracy_dataset-MNIST_areas-3_partitioning-IID.pdf}
    % \caption{TBD}
    % \label{fig:cfl}
  \end{subfigure}
  \begin{subfigure}{0.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/charts/TrainingLoss_dataset-FashionMNIST_areas-3_partitioning-IID.pdf}
    % \caption{TBD}
    % \label{fig:cfl}
  \end{subfigure}
  \begin{subfigure}{0.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/charts/ValidationAccuracy_dataset-FashionMNIST_areas-3_partitioning-IID.pdf}
    % \caption{TBD}
    % \label{fig:cfl}
  \end{subfigure}
  \caption{ Comparison of the proposed method (FBFL) with FedAvg under IID data. 
  The first row represents results on the MNIST dataset, while the second row on the Fashion MNIST dataset. }
  \label{fig:exp-iid}
\end{figure*}

% COMPARE UNDER NON-IID
\begin{tcolorbox}[colback=gray!10!white, colframe=gray!80!black, title=\textbf{\ref{itm:rq2}}]
\emph{Can we increase accuracy by introducing personalized learning zones where \emph{learning devices} are grouped based 
on similar experiences as often observed in spatially near locations? 
More precisely, what impact does this have on heterogeneous 
and non-\ac{iid} data distributions?}
\end{tcolorbox}
\noindent To rigorously evaluate our approach under non-IID conditions,
 we conducted extensive experiments comparing FBFL against baseline methods.
%
We systematically explored two distinct types of data skewness: 
 Dirichlet-based distribution (creating imbalanced class representations) and 
 hard partitioning (strictly segregating classes across regions).
%
The results reveal that baseline methods suffer from substantial performance degradation 
 under these challenging conditions.
%
These approaches consistently fail to capture the global objective and exhibit significant 
 instability during the learning process.
%
This limitation becomes particularly severe in scenarios with increased skewness, 
 where baseline models demonstrate poor generalization across heterogeneous data distributions.
%
\Cref{fig:exp-non-iid} presents key results from our comprehensive evaluation---which encompassed over $400$ distinct experimental configurations.
% 
The first row shows results from the Extended MNIST dataset using
 hard partitioning across $6$ and $9$ distinct areas.
% 
The performance gap is striking: baseline algorithms plateau at approximately $0.5$ accuracy, 
 while FBFL maintains robust performance above $0.95$.
%
Notably, increasing the number of areas adversely affects baseline models' performance, 
 leading to further accuracy deterioration.
% 
In contrast, FBFL demonstrates remarkable stability, maintaining consistent performance 
 regardless of area count.
%
The second and third rows present results from Fashion MNIST and MNIST experiments, respectively.
% 
While baseline methods show marginally better performance on these datasets 
 (attributable to their reduced complexity compared to EMNIST),
 they still significantly underperform compared to FBFL.
%
These comprehensive findings underscore the fundamental limitations of traditional approaches 
 in handling highly skewed non-IID scenarios.
%
The results provide compelling evidence for~\ref{itm:rq2}, demonstrating FBFL's superior capability 
 in maintaining high performance and stability across diverse data distributions through 
 its innovative field-based coordination mechanism.


\begin{figure*}
  \centering
  \begin{subfigure}{0.4\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/charts/ValidationAccuracy_dataset-EMNIST_areas-6_partitioning-hard.pdf}
      % \caption{TBD}
      % \label{fig:cfl}
  \end{subfigure}
  \begin{subfigure}{0.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/charts/ValidationAccuracy_dataset-EMNIST_areas-9_partitioning-hard.pdf}
    % \caption{TBD}
    % \label{fig:cfl}
  \end{subfigure}
  \begin{subfigure}{0.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/charts/ValidationAccuracy_dataset-FashionMNIST_areas-3_partitioning-dirichlet.pdf}
    % \caption{TBD}
    % \label{fig:cfl}
  \end{subfigure}
  \begin{subfigure}{0.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/charts/ValidationAccuracy_dataset-FashionMNIST_areas-9_partitioning-dirichlet.pdf}
    % \caption{TBD}
    % \label{fig:cfl}
  \end{subfigure}
  \begin{subfigure}{0.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/charts/ValidationAccuracy_dataset-MNIST_areas-3_partitioning-hard.pdf}
    % \caption{TBD}
    % \label{fig:cfl}
  \end{subfigure}
  \begin{subfigure}{0.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/charts/ValidationAccuracy_dataset-MNIST_areas-6_partitioning-dirichlet.pdf}
    % \caption{TBD}
    % \label{fig:cfl}
  \end{subfigure}
  \caption{ Comparison of the proposed method (FBFL) with all the baselines under non-IID data.
  The first row represents results on the Extended MNIST dataset, while the
  second row on the Fashion MNIST dataset and the third on the MNIST dataset.
  }
  \label{fig:exp-non-iid}
\end{figure*}
\begin{tcolorbox}[colback=gray!10!white, colframe=gray!80!black, title=\textbf{\ref{itm:rq3}}]
  \emph{What is the effect of creating a self-organizing hierarchical architecture through Field Coordination on Federated Learning in terms of resilience, adaptability and fault-tolerance?}
\end{tcolorbox}
\noindent The last experiment has been designed to evaluate the resilience of 
 the self-organizing hierarchical architecture proposed by FBFL (\ref{itm:rq3}). 
%
We simulated a scenario involving $4$ distinct areas and a total of $50$ devices. 
%
As for the other experiments, we ran $5$ simulations with varying seeds.
%
After allowing the system to stabilize under normal conditions (\Cref{fig:kill1,fig:kill2,fig:kill3}),
 we introduced a disruption by randomly killing two aggregator devices within the network (\Cref{fig:kill4}). 
% 
The goal was to observe whether the system could autonomously recover, elect new aggregators, 
 and continue the learning process without significant degradation in performance.
%
The results, as depicted in performance charts (\Cref{fig:exp-kill}) and visualized in the 
 Alchemist simulator (\Cref{fig:kill-alchemist}), demonstrate that the FBFL architecture 
 successfully re-stabilizes into a new configuration. 
% 
Specifically, the performance charts indicate only a minor increase in 
 loss during the killing phase (global round $10$), with no significant long-term drops in performance. 
% 
Similarly, the Alchemist simulation shows a temporary transitional period following 
 the removal of the aggregators, during which federations are not correctly formed (\Cref{fig:kill5}). 
% 
However, the system adapts by electing new aggregators, and the configuration stabilizes once more (\Cref{fig:kill6}).


These findings highlight the robustness of the FBFL architecture, 
 emphasizing its capability to recover from disruptions and maintain stable performance. 
% 
This resilience is a critical feature for real-world applications where system stability 
 under failure conditions is essential.


\begin{figure*}
  \centering
  \begin{subfigure}{0.4\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/charts/TrainingLoss_dataset-ServerFailure_areas-3_partitioning-Dirichlet.pdf}
      % \caption{TBD}
      % \label{fig:cfl}
  \end{subfigure}
  \begin{subfigure}{0.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/charts/ValidationAccuracy_dataset-ServerFailure_areas-3_partitioning-Dirichlet.pdf}
    % \caption{TBD}
    % \label{fig:cfl}
  \end{subfigure}
  \caption{ Evaluation metrics in the scenario of aggregators failure. 
  It can be observed that, despite the failure of two aggregators, 
  the evaluation metrics do not undergo significant variations in either the training or validation phases, 
  and the learning process continues. 
  Notably, the only observable effect is a slight increase in the loss at training time step 10, 
  i.e., when the aggregators fail and the system needs to re-stabilize,
  before it quickly resumes its downward trend. }
  \label{fig:exp-kill}
\end{figure*}


\begin{figure*}
  \centering
  \begin{subfigure}{0.32\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/experiment-resilience/leaderkill1.png}
      \caption{Start of the learning.}
      \label{fig:kill1}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/experiment-resilience/leaderkill2.png}
    \caption{Subregions stabilization.}
    \label{fig:kill2}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/experiment-resilience/leaderkill3.png}
    \caption{Learning.}
    \label{fig:kill3}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/experiment-resilience/leaderkill4.png}
    \caption{Aggregators failure.}
    \label{fig:kill4}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/experiment-resilience/leaderkill6.png}
    \caption{Subregions re-stabilization.}
    \label{fig:kill5}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/experiment-resilience/leaderkill7.png}
    \caption{Learning.}
    \label{fig:kill6}
  \end{subfigure}
  \caption{ Graphical representation of the simulation from the Alchemist simulator.
  In this scenario, $50$ nodes (i.e., the squares) are deployed in $4$ different subregions. 
  Background colors represent different data distribution, while nodes color represent 
  their respective federation.
  Black dots inside nodes represent aggregators.
  It can be observed that in unstable configurations, multiple aggregators 
  are present in each subregion, whereas, once the system stabilizes, only a single 
  aggregator remains per subregion.
  After the learning has started and the systems has found a stable configuration, we randomly 
  killed two aggregators node to simulate server failures.
  Notably, it is possible to see how the system is able to automatically re-stabilize. }
  \label{fig:kill-alchemist}
\end{figure*}

It can be observed that in unstable configurations, multiple aggregators are present in each subregion,
 whereas, once the system stabilizes, only a single aggregator remains per subregion.

\section{Conclusions and Future Work}\label{sec:conclusions}

In this article, we presented \emph{Field-based Federated Learning (FBFL)}, leveraging field-based coordination to address key challenges of highly decentralized and reliable federated learning systems.
%
Our approach innovates by exploiting spatial proximity to handle non-IID data distributions and 
 by implementing a self-organizing hierarchical architecture through dynamic leader election.
%
FBFL demonstrates robust performance in both IID and non-IID scenarios while providing inherent resilience against node failures.
%
Our results show that forming personalized learning subregions mitigates the effects of data skewness.
%
Compared to state-of-the-art methods like Scaffold and FedProx, FBFL significantly outperforms in 
 scenarios with heterogeneous data distributions.

Future work could explore advanced field coordination concepts, such as space fluidity~\cite{DBLP:journals/lmcs/CasadeiMPVZ23}, 
 to enable dynamic segmentation of learning zones based on evolving trends in the phenomena 
 being modeled, rather than relying on static, predefined assumptions
%
Additionally, the potential for FBFL to support personalized machine learning models in 
 decentralized environments offers promising applications across domains such as 
 edge computing and IoT ecosystems.
%
Finally, another avenue for improvement lies in integrating sparse neural networks to reduce resource consumption, 
 making the approach more efficient and scalable for resource-constrained devices.

\section*{Acknowledgment}
Lukas Esterle was supported by the Independent Research Fund Denmark through the FLOCKD project under the grant number 1032-00179B.

Gianluca Aguzzi was supported by the Italian PRIN project ``CommonWears'' (2020 HCWWLP) and the EU/MUR FSE PON-R \&I 2014-2020.

\bibliographystyle{alpha}
\bibliography{bibliography}

\end{document}
