\section{Related Work}
\label{sec:Related}
%
\textbf{MNL bandits.}
%
The MNL bandits were initially studied in____, followed by a line of improvements____.
In MNL bandits, the goal is to offer an assortment that maximizes the expected rewards, which are adaptively learned based on user preference feedback from the offered assortment.
However, there are no state transitions, and it is assumed that the value of each item is known, with the value of the outside option fixed at zero.
Our study extends this by not only estimating the MNL model but also the long-term item values.

%
\textbf{Combinatorial RL with preference feedback.}
%
Recently, several studies have demonstrated the empirical success of combinatorial RL with preference feedback____, where a set of items is offered to a user, and (relative) choice feedback along with a reward is received, leading to a transition to the next state.
However, theoretical results quantifying the benefits of such methods are still few and far between.
A closely related work is cascading RL____, which also involves selecting a set of items. 
However, in cascading RL, items are offered to the user one by one, and the user decides only whether to choose the currently offered item. 
As a result, this framework does not capture relative preference feedback across multiple items. 
Furthermore, in cascading RL, the probability of choosing each item is independent of the others, which is not the case in our framework.


Another related line of work is preference-based RL (PbRL)____, where the policy is optimized based on relative, rather than absolute, preference feedback. 
However, our framework differs from PbRL in that our goal is not to offer just a single item, but to offer multiple items (a combinatorial base action).


%
\textbf{RL with nonlinear function approximation.}
RL under nonlinear function approximation has gained attention____ for modeling complex function spaces like neural networks.
Among these,____ achieved the best-known regret guarantees under general function approximation by introducing the concept of generalized Eluder dimension to handle weighted regression. 
Inspired by their work, we estimate the value of items (referred to as \textit{item-level $Q$-value}) using general function approximation in this paper.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%