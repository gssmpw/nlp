%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For preprint
\usepackage[preprint]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{mathabx}
\usepackage{bbm}
\usepackage{mathrsfs}
\usepackage{amsbsy}
\usepackage{amsfonts}


%% Figure subcaption
\usepackage{caption}

%% Figures
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{mwe}
\usepackage{wrapfig}
\usepackage{todonotes}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\usepackage{blkarray}
\usepackage{bm}
\usepackage{enumitem}

\usepackage{algorithm}
\usepackage{algorithmicx} % <===========================================
\usepackage{algpseudocode} % <==========================================

\usepackage{makecell}

\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,fit,decorations.pathreplacing,calc}

% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

%% Our shorthand definitions
\usepackage{abbreviation_definitions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
% \usepackage[textsize=tiny]{todonotes}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%s%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% For "Table of Contents" %%
\usepackage[toc,page,header]{appendix}
\usepackage{minitoc}
% Make the "Part I" text invisible
\renewcommand \thepart{}
\renewcommand \partname{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Combinatorial Reinforcement Learning with Preference Feedback}

%%%%%%%%%%% Table of Contents for main paper %%%%%%%%%%%
\doparttoc % Tell to minitoc to generate a toc for the parts
\faketableofcontents % Run a fake tableofcontents command for the partocs

\begin{document}

\twocolumn[
\icmltitle{Combinatorial Reinforcement Learning with Preference Feedback}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Joongkyu Lee}{sch}
\icmlauthor{Min-hwan Oh}{sch}
% \icmlauthor{Firstname3 Lastname3}{comp}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

% \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
% \icmlaffiliation{comp}{Company Name, Location, Country}
\icmlaffiliation{sch}{Seoul National University, Seoul, Korea}

\icmlcorrespondingauthor{Min-hwan Oh}{minoh@snu.ac.kr}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
    In this paper, we consider combinatorial reinforcement learning with preference feedback,
    where a learning agent sequentially offers an action—an assortment of multiple items—to a user, whose preference feedback follows a multinomial logistic (MNL) model.
    This framework allows us to model real-world scenarios, particularly those involving long-term user engagement, such as in recommender systems and online advertising.
    However, this framework faces two main challenges: (1) the unknown value of each item, unlike traditional MNL bandits that only address single-step preference feedback, and 
    (2) the difficulty of ensuring optimism while maintaining tractable assortment selection in the combinatorial action space with unknown values.
    In this paper, we assume a contextual MNL preference model, where the mean utilities are linear, and the value of each item is approximated by a general function.
    We propose an algorithm,~\AlgName{}, that addresses these challenges, making it both computationally and statistically efficient.
    As a special case, for linear MDPs (with the MNL preference feedback), we establish the first regret lower bound in this framework and show that~\AlgName{} achieves nearly minimax-optimal regret.
    To the best of our knowledge, this is the first work to provide statistical guarantees in combinatorial RL with preference feedback.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:Introduction}

We first formally state the concept of \textit{Combinatorial Reinforcement Learning} (RL), which we refer to as a class of RL problems where the action space is combinatorial, meaning that the agent selects a combination or subset of base actions from a set of possible base actions. 
%
Although some previous studies have addressed problems within this setting---particularly in deep RL~\citep{sunehag2015deep, he2016deep, swaminathan2017off, metz2017discrete, ryu2019caql, ie2019slateq, delarue2020reinforcement, mcinerney2020counterfactual, vlassis2021control, chaudhari2024distributional}, 
with less emphasis on theoretical RL---to the best of our knowledge, it appears that no prior work has formally and theoretically defined the concept of combinatorial RL.\footnote{Surprisingly, this is in contrast to the rich theoretical literature in combinatorial bandits~\citep{chen2013combinatorial, kveton2015combinatorial, kveton2015cascading, combes2015combinatorial}, which extend the multi-armed bandits.} This framework is especially relevant for real-world applications such as recommender systems and online advertising, where multiple items (base actions) must be selected simultaneously, such as a set of products to recommend or advertisements to display. The challenge in combinatorial RL lies in the exponentially large action space and the need to efficiently optimize the agent's action selection while balancing exploration and exploitation (a challenge even for single action selection), while considering the long-term effects of these actions.


One of the most widely encountered settings in combinatorial RL is \emph{preference feedback} over combinatorial actions, commonly seen in streaming services, online retail, and similar platforms.
Despite the broad applicability of this setting, 
theoretical studies have predominantly focused on the multinomial logistic (MNL) \textit{bandit} model~\citep{rusmevichientong2010dynamic, saure2013optimal, agrawal2017thompson, agrawal2019mnl, oh2019thompson, oh2021multinomial, perivier2022dynamic, agrawal2023tractable, zhang2024online, lee2024nearly}. 
The MNL bandit framework focuses on assortment (a set of items) selection by selecting subsets of items and receiving feedback on chosen items, 
modeled by the MNL model~\citep{mcfadden1977modelling}. 
However, these studies take a \textit{myopic} approach, optimizing for immediate, \textit{known} rewards without considering the long-term impact on user behavior.

While MNL bandits have been widely studied, 
the myopic approach is limiting in many real-world scenarios. 
For example in recommender systems,
incorporating the long-term impact of recommendations opens the door to balancing short-term engagement with long-term user satisfaction. 
For instance, 
recommending \textit{junk} product or content might lead to high immediate reward but it can decrease user satisfaction over time due to fatigue. 
This trade-off between immediate and long-term outcomes is not captured by traditional MNL bandit. 
See Appendix~\ref{app:sec_add_setting} for a more details.

On the empirical side, several studies have explored long-term user engagement in recommendation systems, particularly using deep RL~\citep{swaminathan2017off, ie2019slateq, mcinerney2020counterfactual, vlassis2021control, chaudhari2024distributional}. 
However, there is a significant gap in the theoretical understanding of combinatorial actions with preference feedback, 
particularly within the RL framework. 
To the best of our knowledge, no theoretical work has yet explored this important problem setting.

In this paper, we aim to address this gap by rigorously studying combinatorial RL with preference feedback and developing a provably efficient algorithm that maximizes long-term user engagement by incorporating state transitions (e.g., historical behavior)  into decision-making.
%
We consider a setting where the $Q$-function of an assortment is decomposed into two components: the preference model and the item values, inspired by~\citet{ie2019slateq}. 
Specifically, we focus on the contextual MNL preference model with linear mean utilities~\citep{agrawal2013thompson, cheung2017thompson,  agrawal2019mnl, oh2019thompson, oh2021multinomial, amani2021ucb, perivier2022dynamic, agrawal2023tractable, zhang2024online, lee2024nearly} and use general function approximation to estimate item values~\citep{jiang2017contextual, wang2020reinforcement_eluder, jin2021bellman, du2021bilinear, foster2021statistical, agarwal2023vo, zhao2023nearly}. 
%
The key challenges in this framework are: \textbf{(1)} the unknown long-term value of each item due to the stochastic nature of rewards and transitions, 
\textbf{(2)} the difficulty of selecting an assortment that ensures optimism 
while considering tractable assortment optimization in the combinatorial action space, given the unknown values, 
and
\textbf{(3)} achieving a tighter regret bound (for MNL preference model) than simply summing over $H$ MNL bandit regrets.

\textbf{Technical novelties. }
%
To tackle challenge \textbf{(1)}, we estimate the optimistic item values by employing point-wise optimism under general function approximation~\citep{agarwal2023vo}.
Based on these \textit{optimistic} item values (which incorporate uncertainty), we then select an assortment that ensures sufficient exploration and guarantees optimism. 
This step is the most challenging part of our framework. 
Since the true value of each item is unknown, directly applying techniques from MNL bandits (which assume known true values) is not feasible.
Thus, to address challenge \textbf{(2)}, we propose a novel method to estimate the \textit{optimistic preference model} by carefully alternating between \textit{optimistic} and \textit{pessimistic} utilities (for the preference model), using the optimistic item values (Equation~\eqref{eq:optimistic_choice}).
%
Additionally, proving optimism (Lemma~\ref{lemma:optimism}) and other related results (Lemmas~\ref{lemma:tilde_choice}, \ref{lemma:more_extreme_utility}, and \ref{lemma:tight_mnl}) requires fundamentally more sophisticated analytical techniques. 
Finally, we avoid naive combinatorial enumeration when selecting the assortment (\eqref{eq:optvalue_optimization}) by reformulating the optimization problem as a linear program (LP), inspired by~\citet{davis2013assortment}.
%
Finally, for challenge \textbf{(3)}, instead of naively summing over $H$ MNL bandit regrets, we bound the regret of the MNL preference model in terms of the sum of the variances of value functions and apply the \textit{law of total variance}~\citep{lattimore2012pac, gheshlaghi2013minimax}.
This approach reduces MNL regret by a factor of $\sqrt{H}$ compared to directly summing $H$ MNL bandit regrets.
Moreover, in the special case of linear MDPs with preference feedback, we achieve a nearly minimax-optimal regret.

%
Our main contributions are summarized as follows:
%
\begin{itemize}
    \item We propose a~\AlgName{}, which achieves a regret upper bound of $\BigOTilde \big( d  \sqrt{H K} + \sqrt{ \smash[b]{\operatorname{dim}(\mathcal{F}) KH \log |\mathcal{F}| }  } \big)$ while maintaining computational efficiency (Theorem~\ref{thm:regret_upper_general_main}).
    Here, $H$ is the horizon length, $K$ is the total number of episodes, $d$ is the feature dimension of the MNL preference model, and $ \operatorname{dim}(\mathcal{F})$ is the generalized Eluder dimension (see Definition~\ref{def:gen_eluder}) of the function class $\mathcal{F}$.
    To the best of our knowledge, this is the first theoretical regret guarantee in combinatorial RL with preference feedback.

    \item For the special case of linear MDPs (with preference feedback),~\AlgName{} obtains a regret upper bound of $\BigOTilde ( d  \sqrt{H K} + \DimLinearMDP \sqrt{HK} )$, where $\DimLinearMDP$ is the feature dimension of the linear MDPs (Theorem~\ref{thm:regret_upper_linear}).
    Furthermore, we establish a matching regret lower bound of $\Omega ( d  \sqrt{HK} + \DimLinearMDP \sqrt{HK} )$, show the minimax-optimality of our algorithm in linear MDPs (Theorem~\ref{thm:regret_lower_linear_main}).
    % To the best of our knowledge, this is the first regret lower bound in linear MDPs with preference feedback.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\label{sec:Related}
%
\textbf{MNL bandits.}
%
The MNL bandits were initially studied in~\citet{rusmevichientong2010dynamic}, followed by a line of improvements~\citep{filippi2010parametric, rusmevichientong2010dynamic, agrawal2017thompson, oh2019thompson, faury2020improved, abeille2021instance, faury2022jointly, oh2021multinomial, perivier2022dynamic, agrawal2023tractable, lee2024nearly}.
In MNL bandits, the goal is to offer an assortment that maximizes the expected rewards, which are adaptively learned based on user preference feedback from the offered assortment.
However, there are no state transitions, and it is assumed that the value of each item is known, with the value of the outside option fixed at zero.
Our study extends this by not only estimating the MNL model but also the long-term item values.

%
\textbf{Combinatorial RL with preference feedback.}
%
Recently, several studies have demonstrated the empirical success of combinatorial RL with preference feedback~\citep{swaminathan2017off, ie2019slateq, mcinerney2020counterfactual, vlassis2021control, chaudhari2024distributional}, where a set of items is offered to a user, and (relative) choice feedback along with a reward is received, leading to a transition to the next state.
However, theoretical results quantifying the benefits of such methods are still few and far between.
A closely related work is cascading RL~\citep{du2024cascading}, which also involves selecting a set of items. 
However, in cascading RL, items are offered to the user one by one, and the user decides only whether to choose the currently offered item. 
As a result, this framework does not capture relative preference feedback across multiple items. 
Furthermore, in cascading RL, the probability of choosing each item is independent of the others, which is not the case in our framework.


Another related line of work is preference-based RL (PbRL)~\citep{akrour2012april, wirth2017survey, christiano2017deep, ouyang2022training, saha2023dueling, zhu2023principled, zhan2023provable}, where the policy is optimized based on relative, rather than absolute, preference feedback. 
However, our framework differs from PbRL in that our goal is not to offer just a single item, but to offer multiple items (a combinatorial base action).


%
\textbf{RL with nonlinear function approximation.}
RL under nonlinear function approximation has gained attention~\citep{jiang2017contextual, wang2020reinforcement_eluder, jin2021bellman, du2021bilinear, foster2021statistical, agarwal2023vo, zhao2023nearly} for modeling complex function spaces like neural networks.
Among these,~\citet{agarwal2023vo, zhao2023nearly} achieved the best-known regret guarantees under general function approximation by introducing the concept of generalized Eluder dimension to handle weighted regression. 
Inspired by their work, we estimate the value of items (referred to as \textit{item-level $Q$-value}) using general function approximation in this paper.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem Setting}
\label{sec:Problem_setting}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Notations}
% \label{subsec:Notations}
% For an integer $n\!>\!0$, we denote $[n] \!:=\! \{1, 2, \ldots, n \}$.
% For a positive semi-definite matrix $\Lambda \in \RR^{d \times d}$, 
% the norm $\| \cdot \|_\Lambda$ on $\RR^d$ is defined as
% $\|x\|^2_{\Lambda}:= x^\top \Lambda x$.
% Let $|\cdot|$ denote the cardinality of a set.

% matrix $\Lambda$, we denote $\|\Lambda\|_2 \!\!:=\!\! \sup_{x:\|x\|_2=1}\!\|\Lambda x\|_2$ as the maximum singular value of $\Lambda$.
% Let $|\cdot|$ denote the cardinality of a set.
% In this paper, we assume without loss of generality that all solutions to argmax operations are unique.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Combinatorial MDPs with Preference Feedback}
\label{subsec:Assortment_RL}
%
%
In this paper, we  consider a episodic \textit{combinatorial Markov decision processes (MDPs) with preference feedback},  
%
$\MDP(\SetOfStates, \SetOfGroundActions , \SetOfActions,  \MaxAssortment,  \{\ChoiceProbability_h \}_{h=1}^H, \{ \TransitionProbability_h\}_{h=1}^H, \{\Reward_h \}_{h=1}^H , \HorizonLength)$. 
Here, $\SetOfStates$ is the set of states. 
Each state $s \in \SetOfStates$ reflects the user's status, capturing both relatively static user features (e.g., demographics, interests) and relevant user history or past behavior (e.g., past recommendations, items purchased or clicked).
$\SetOfGroundActions := \{ a_1, \dots, a_N, \OutsideOption\}$ is the ground set of items (base actions), where $a_1, \dots, a_N$ are items and $\OutsideOption$ refers to the ``outside option'', meaning the user has chosen none of the items from the offered set of items (referred to as an ``assortment'' throughout the paper).
It is included in every assortment \textit{by default}.
$\SetOfActions$ is the set of candidate assortments that always include the outside option $\OutsideOption$, contain at least one item (other than $\OutsideOption$), and have at most $M$ items (including $\OutsideOption$), i.e.,  $\SetOfActions = \{A \subseteq \SetOfGroundActions: \OutsideOption \in A, 1 \leq |A \setminus \{ \OutsideOption \}| \leq \MaxAssortment  \}$, where $A$ is an assortment.
%
For any $(s,A) \in \SetOfStates \times \SetOfActions$, we denote $\ChoiceProbability_h(a | s, A)$ as the probability of the user choosing on item $a \in A$ (including the outside option $\OutsideOption$).
%
Furthermore, we let $\TransitionProbability_h : \SetOfStates \times \SetOfGroundActions \rightarrow \Delta_{\SetOfStates}$ and $\Reward_h : \SetOfStates \times  \SetOfGroundActions \times \SetOfStates \rightarrow \RR$ characterize the transition kernel and instantaneous reward, respectively, at a given horizon $h \in [H]$.
Throughout this paper, we assume that $\sum_{h=1}^H \Reward_h(s_h,a_h,s_{h+1}) \in [0,1]$ for all possible sequence $(s_1,a_1, \dots, s_H,a_H,s_{H+1})$.
%
$\HorizonLength \in \mathbb{Z_{+}}$ is the length of each episode.
A policy $\pi: \SetOfStates \rightarrow \SetOfActions$ is a mapping from the state space to the assortment space. 
Since the optimal policy is non-stationary in an episodic MDP, we use $\pi$ to refer to the $\HorizonLength$-tuple $\{\pi_h \}_{h=1}^H$.

In each episode $k \in [K]$, an initial state $s^k_1$ is picked \textit{arbitrarily} (e.g., a user arrives at the system).
The agent then follows a policy $\pi^k$ starting from $s^k_1$.
At each step $h \in [H]$, the agent observes the current state $s^k_h$ (e.g., historical behaviors of the user) and offers an assortment $A^k_h = \pi^k_h(s^k_h)$.
The user’s preference feedback $a^k_h \in A^k_h$ is then observed, which is drawn based on the choice probability $\ChoiceProbability_h(\cdot | s^k_h, A^k_h)$.
Next, the system transitions to the next state $s^k_{h+1} \sim \TransitionProbability_h(\cdot | s^k_h, a^k_h)$ and receives a reward $\Reward_h(s^k_h, a^k_h, s^k_{h+1})$.
After $H$ steps, the episode terminates, and the agent proceeds to the next.


For any policy $\Policy = \{ \pi_h \}_{h=1}^H$, we define the value function of policy $\pi$, denoted as $V^{\Policy}_h : \SetOfStates \rightarrow \RR$, as the expected sum of rewards under the policy $\Policy$ until the end of the episode when starting from $s_h = s$, i.e., 
$ V^{\Policy}_h(s) := \EE \left[ \sum_{h'=h}^{\HorizonLength}\! \Reward_{h'} \! \left(s_{h'}, a_{h'}, s_{h'+1} \right)\! \mid\! s_{h} = s \right].$
Moreover, we define the action-value function of policy $\pi$, $Q^\pi_h: \SetOfStates \times \SetOfActions \rightarrow \RR$, as the expected sum of rewards under policy $\pi$, starting from step $h$ until the end of the episode after taking action~$A$ in state~$s$; that is,
$    Q^\pi_h (s, A):=  \EE \left[ \sum_{h'=h}^{\HorizonLength}\! \Reward_{h'}\! \left(s_{h'}, a_{h'}, s_{h'+1} \right)\!\mid\!s_{h} = s, A_h = A \right].$
Furthermore, we define the \textit{item-level $Q$-value function} (also called the \textit{$\GroundQ$-value}) $\GroundQ^\pi_{h}(s,a) \!:=\! \sum_{s' }\TransitionProbability_h(s' | s, a) ( \Reward_h(s,a,s') + V^{\pi}_{h+1}(s') )$.
% and $\GroundQ^\star_{h}(s,a) \!:=\! \sum_{s' }\TransitionProbability_h(s' | s, a) ( \Reward_h(s,a,s') + V^\star_{h+1}(s') )$.
Then, the Bellman equation for assortment RL is denoted as follows:
%
\begin{align*}
    Q^\pi_h(s,A) 
    \!=\! \sum_{a \in A} \ChoiceProbability_h(a | s, A) \GroundQ^\pi_{h}(s,a).
\end{align*}
%
% There exists an optimal policy $\pi^\star$, which gives the optimal value function for all states~\citep{puterman2014markov}, i.e., $V^{\pi^\star}_h(s) = \sup_{\pi} V^{\pi}_h (s) $ for all $(s,h) \in \SetOfStates \times [H]$.
% For notational simplicity, we abbreviate $V^{\pi^\star}$ as $V^{\star}$.
Similarly, we define the optimal value function $V^{\star}_h(s) = \sup_{\pi} V^{\pi}_h (s)$ and the optimal $Q$-value function as
%
% \begin{align*}
    $Q^\star_h(s,A) 
    \!=\!\sum_{a \in A} \ChoiceProbability_h(a | s, A)\GroundQ^\star_{h}(s,a)$, 
% \end{align*}
%
where $\GroundQ^\star \!:=\! \sum_{s' }\TransitionProbability_h(s' | s, a) ( \Reward_h(s,a,s') + V^{\star}_{h+1}(s') )$ is the \textit{item-level optimal $Q$-value function}.
%
For any $V: \SetOfStates \rightarrow \RR$ and  $h \in [H]$,  we define the \textit{item-level Bellman operator} of $V$ as $\mathcal{T}_h V : \SetOfStates \times \SetOfGroundActions \rightarrow \RR$, such that for all $(s,a) \in \SetOfStates \times \SetOfGroundActions$, $\BellmanOperator_h V (s, a) 
    := \EE_{s' \sim \TransitionProbability_{h}(\cdot | s, a)}\left[r_h(s, a, s')+ V(s') \mid s, a\right].$
%
The definition of value functions ensures that they satisfy the equation $\GroundQ^\star_h (s,a) = \BellmanOperator_h V^\star_{h+1}(s,a)$.
We also define the \textit{second moment item-level Bellman operator} of $V$  as 
$\BellmanOperator_h^2 V : \SetOfStates \times \SetOfGroundActions \rightarrow \RR $ such that for all $(s,a) \in \SetOfStates \times \SetOfGroundActions$, 
$\BellmanOperator_h^2 V(s,a) := \EE_{s' \sim  \TransitionProbability_h(\cdot |s,a) }[ \left(r_h(s, a, s')+  V(s') \right)^2 \mid s, a ]$.

Our goal is to minimize the cumulative regret over $K$ episodes $\Regret(\MDP,K) := \sum_{k=1}^K V_1^{\star}(s^k_{1}) - V_1^{\pi_k}(s^k_{1})$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Multinomial Logistic Preference Model}
\label{subsec:MNL_model_general}
%
In this paper, we make a structural assumption about the MDP $\MDP$, where the user's choice probability $\{ \ChoiceProbability_h \}_{h=1}^H$ follows multinomial logistic (MNL) model~\citep{mcfadden1977modelling} parameterized by $\{ \thetab^\star_h \}_{h=1}^H$.
We denote $\ChoiceProbability_h(\cdot | s, A; \thetab^\star_h)$ as equivalent to $\ChoiceProbability_h(\cdot | s, A)$, explicitly showing the dependence on the parameter $\thetab^\star_h$.
Throughout the paper, we use $\ChoiceProbability_h(\cdot | s, A)$ and $\ChoiceProbability_h(\cdot | s, A; \thetab^\star_h)$ interchangeably.
%
\begin{assumption}[MNL preference model] \label{assum:MNL_click_model}
    Let there exist a known feature map $\phi: \SetOfStates \times \SetOfGroundActions \rightarrow \RR^d$ such that  $\|\phi(s,a)\|_2 \!\leq\! 1$,
    and 
    an unknown $\thetab^\star_h \in \Theta$ for all $h \in [H]$, where 
    $\Theta = \left\{\thetab \in \RR^d : \| \thetab \|_2 \leq \BoundMNL = \BigO(1) \right\}$.
    Then, for any $(s,A,H) \in \SetOfStates \times \SetOfActions \times [H]$, the probability of choosing any item $a \in A$ is defined as:
    %
    \begin{align*}
        \ChoiceProbability_h(a | s, A) = 
        \ChoiceProbability_h(a | s, A ; \thetab^\star_h) 
        = 
        \frac{\exp\left( \phi(s,a)^\top \thetab^\star_h \right)}{ \sum_{a' \in A} \exp\left( \phi(s,a')^\top \thetab^\star_h \right) }.  
    \end{align*} 
\end{assumption}
%
Here, without loss of generality, we assume that $\phi(s, \OutsideOption) = 0$ for all $s \in \SetOfStates$
\footnote{By subtracting $\phi(s, \OutsideOption)$ from each $\phi(s,a)$, where $a \in \SetOfGroundActions$, and defining $\phi'(s,a) := \phi(s,a) - \phi(s, \OutsideOption)$, we can ensure that $\phi'(s, \OutsideOption) = 0$. 
This implies that $\exp(\phi'(s, \OutsideOption)^\top \thetab^\star_h) = 1$.
This assumption is commonly made in contextual MNL bandits~\citep{oh2019thompson, oh2021multinomial, perivier2022dynamic, agrawal2023tractable, zhang2024online, lee2024nearly}.
}
, which implies that $\exp(\phi(s, \OutsideOption)^\top \thetab^\star_h ) = 1$.
Thus, the preference model can be equivalently expressed as $
\ChoiceProbability_h(a | s, A ; \thetab^\star_h) 
= 
\frac{\exp\left( \phi(s,a)^\top \thetab^\star_h \right)}{1+ \sum_{a' \in A\setminus \{\OutsideOption\} } \exp\left( \phi(s,a')^\top \thetab^\star_h \right)}$.
%

Following the previous MNL bandits~\citep{oh2021multinomial, perivier2022dynamic, zhang2024online, lee2024nearly}, we also introduce the following constant:
%
\begin{definition}[Problem-dependent constant] 
\label{def:kappa}
There exist $\kappa >0$ such that, for any $A \in \SetOfActions, a \in A \setminus \{\OutsideOption \}, h \in [H]$, we have $\min_{ \thetab \in \Theta} \ChoiceProbability_h(a | s, A, \thetab) \ChoiceProbability_h( \OutsideOption | s, A, \thetab) \geq \kappa$.
\end{definition}
%
A small $\kappa$ indicates a larger deviation from the linear model. 
Note that $1/\kappa$  can be exponentially large, so it is crucial to avoid any dependency on $1/\kappa$ in our regret bound.

%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Generalized Function Approximation for $\GroundQ$} 
\label{subsec:general_function_approx}
%
We estimate the item-level $Q$-functions (referred to as $\GroundQ$-values) using general function approximation. 
Specifically, the agent is given a function class $\mathcal{F}:= \{ \mathcal{F}_h \}_{h=1}^H$, where each set $\mathcal{F}_h$ is composed of functions $f_h : \SetOfStates \times \SetOfGroundActions \rightarrow [0, \BoundFunction]$, where $\BoundFunction = \BigO(1)$.
Since no reward is collected in the $(H+1)^{\text{th}}$ steps, we set $f_{H+1}=0$.
We denote $\mathcal{N}$ as the maximal size of function class, i.e., $\mathcal{N} = \max_{h \in [H]} |\mathcal{F}_h|$.
We assume the completeness and realizability for $\mathcal{F}$.
%
\begin{assumption}[Completeness \& Realizability]
\label{assum:completeness}
    For each $h \in [H]$ and any $V: \SetOfStates \rightarrow [0,1]$, we assume that $\GroundQ^\star_h \in \mathcal{F}_h$, and there exists $f_h, f'_h \in \mathcal{F}_h$ such that, for all $ (s,a) \in \SetOfStates \times \SetOfGroundActions$,
    %
    \begin{align*}
        f_h(s,a) = \BellmanOperator_h V(s,a), 
        \quad
        \text{and}
        \quad
        f'_h(s,a) = \BellmanOperator^2_h V(s,a).
    \end{align*}
\end{assumption}
%
\begin{remark}
    The completeness and realizability assumptions are standard in RL with general function approximation~\citep{ wang2021optimism, jin2021bellman, agarwal2023vo, zhao2023nearly}. 
     Our assumption is the same as those in~\citet{agarwal2023vo, zhao2023nearly}, but stronger than those  in~\citet{wang2021optimism, jin2021bellman}, especially, in terms of the second moment completeness.
    However, this assumption is essential for using point-wise exploration bonuses and achieving a tighter regret bound. Additionally, it naturally holds for both tabular and linear MDPs.  
\end{remark}
%
%
To capture the complexity of exploration in the MDP, we define the \textit{generalized Eluder dimension}, which is a weighted regression version of the original definition~\citep{russo2013eluder}.
%
\begin{definition} [Generalized Eluder dimension,~\citealt{agarwal2023vo}]
\label{def:gen_eluder}
    Let $\rho > 0$, a sequence of state-item pairs $\Zb_k = \{ z^\tau\}_{\tau=1}^k$, where $z^\tau = (s^\tau, a^\tau)$, and a sequence of positive numbers $\sigmab_k= \{\sigma^\tau \}_{\tau=1}^k$.
    The generalized Eluder dimension of a function class $\mathcal{F}: \SetOfStates \times \SetOfGroundActions \rightarrow [0, \BoundFunction]$ with respect to $\rho$ is defined as $\GenEluder_{\nu, K} (\mathcal{F}) := \sum_{\Zb_K, \sigmab_K: \sigma \geq \nu} \GenEluder(\mathcal{F}, \Zb, \sigmab)$, where
    %
    \begin{align*}
        \GenEluder(\mathcal{F}, \Zb_K, \sigmab_K) 
        \!:=\! \sum_{k=1}^K \min \left(1, \frac{D^2_{\mathcal{F}} \left( z^k; \Zb_{k-1}, \sigmab_{k-1}   \right)}{(\sigma^k)^2}  \right)
         .
    \end{align*}
    %
    $D^2_{\mathcal{F}} \left( z^k; \Zb_{k-1}, \sigmab_{k-1}   \right)
        \!\!=\!\! \sup_{f_1, f_2} \frac{\left( f_1(z) - f_2(z) \right)^2}{\sum_{\tau=1}^{k-1} \frac{\left( f_1(z^{\tau}) - f_2(z^{\tau}) \right)^2}{(\sigma^{\tau})^2}  + \rho }$.
    We write $d_{\nu} := \frac{1}{H} \sum_{h=1}^H \GenEluder_{\nu, K} (\mathcal{F}_h)$ for simplicity.
\end{definition}
%
%
By Theorem 4.6 of~\citet{zhao2023nearly}, the generalized Eluder dimension is upper bounded by the standard Eluder dimension~\citep{russo2013eluder} up to logarithmic terms.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Algorithm}
\label{sec:Algorithm}
%
In this section,
we introduce an algorithm, which, to the best of our knowledge, is the first to provide statistical guarantees in combinatorial RL with preference feedback while maintaining computational tractability.
%
\textbf{Step 1} involves online parameter estimation for the MNL preference model, proposed by~\citep{lee2024nearly}.
\textbf{Steps 2, 3, and 4} implement variance-weighted regression to tighten the regret bound, as outlined in~\citep{agarwal2023vo}.
\textbf{Step 5}, which is our main contribution, ensures optimism in a computationally efficient manner, even with uncertainty in item-level $Q$-values.
\textbf{Step 6} introduces an exploration step that accounts for estimation errors from the MNL preference model.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm*}[t!]
   \caption{ \AlgName{}, \textbf{MNL} Preference Model with \textbf{V}ariance-weighted Item-level $\mathbf{Q}$-\textbf{L}earning }
   \label{alg:main}
    \begin{algorithmic}[1]
    \State \textbf{Inputs:} parameter space $\Theta$, function class $\{ \mathcal{F}_h \}_{h=1}^H$, consistent bonus oracle $\BonusOracle$. 
    \State \textbf{Parameters:} $\{\alpha^k_{h}, \beta^k_{h,1}, \beta^k_{h,2}, \bar{\beta}^k_h\}_{(k,h) \in [H] \times [K]} $, $\{u_k\}_{k=1}^K$, $\rho$, bonus error $\epsilon_b$, $\nu$, $\delta$.
    \State \textbf{Initialize:}
    dataset $\mathcal{D}_h^{0} = \emptyset$ for all $h \in [H]$.
    \State Generate $\{\mathcal{D}^1_h\}_{h=1}^H$ from initial state $s^1_1$ by random policy and set  $\sigma^1_h = \bar{\sigma}^{1}_h = 2$ for all $h \in [H]$.
    \For{episode $k=2, \cdots, \TotalEpisodes$}
            \For{horizon $h=\HorizonLength, H-1, \dots, 1$} 
                \label{eq:alg_online_update}
                %
                \Statex \hskip\algorithmicindent\hskip\algorithmicindent \textcolor{blue}{\textsc{// Caculate optimistic and overly optimistic, pessimistic $\GroundQ$-values}}
                \State $\hat{f}^{k}_{h,1} \in \argmin_{f_h \in \mathcal{F}_h} \sum_{\tau=1}^{k-1} \frac{1}{(\bar{\sigma}^{\tau}_h)^2} \left( f_h(s^\tau_h,a^\tau_h) - r^\tau_h - V^{k}_{h+1, 1} (s^\tau_{h+1}) \right)^2 $. \label{eq:alg_optQ_1}
                \State $b^k_{h,1} \leftarrow \BonusOracle\left( \{ \bar{\sigma}^\tau_h \}_{\tau=1}^{k-1}, \mathcal{D}_h^{k-1}, \mathcal{F}_h, \hat{f}^{k}_{h,1}, \beta^k_{h,1}, \rho, \epsilon_b \right)$ (see Definition~\ref{def:bonus_oracle}). \label{eq:bonus_alg_1}
                \State Update $f^k_{h,1}(\cdot,\cdot) \leftarrow \min \left\{ \hat{f}^{k}_{h,1} (\cdot,\cdot) + b^k_{h,1}(\cdot,\cdot), 1 \right\}$.  \label{eq:alg_optQ_2}
                %
                \State $\hat{f}^{k}_{h,j} \in \argmin_{f_h \in \mathcal{F}_h} \sum_{\tau=1}^{k-1}  \left( f_h(s^\tau_h,a^\tau_h) - r^\tau_h - V^{k}_{h+1, j} (s^\tau_{h+1}) \right)^2 $, $j = \pm 2$. \label{eq:alg_ooptQ_1}
                \State  $b^k_{h,2} \leftarrow \BonusOracle\left( \{ \mathbf{1}^{\tau} \}_{\tau=1}^{k-1}, \mathcal{D}_h^{k-1}, \mathcal{F}_h, \hat{f}^{k}_{h,2}, \beta^k_{h,2}, \rho, \epsilon_b \right)$ (see Definition~\ref{def:bonus_oracle}). \label{eq:bonus_alg_2}
                \State  Update $f^k_{h,2}(\cdot,\cdot) \leftarrow \min \left\{ \hat{f}^{k}_{h,2} (\cdot,\cdot) + 2 b^k_{h,1}(\cdot,\cdot) +  b^k_{h,2}(\cdot,\cdot)  , 1 \right\}$. \label{eq:alg_ooptQ_1_1}
                \State  Update $f^k_{h,-2}(\cdot,\cdot) \leftarrow \max \left\{ \hat{f}^{k}_{h,-2} (\cdot,\cdot) -  b^k_{h,2}(\cdot,\cdot)  , 0\right\}$. \label{eq:alg_ooptQ_2}
                %
                \State $\hat{g}^k_h \in \argmin_{g_h \in \mathcal{F}_h} \sum_{\tau=1}^{k-1} \left( g_h(s^\tau_h, a^\tau_h) - \left( r^\tau_h + V^k_{h+1,1}(s^\tau_{h+1}) \right)^2 \right)^2$. \label{eq:alg_g_hat}
                %
                \Statex \hskip\algorithmicindent\hskip\algorithmicindent \textcolor{blue}{\textsc{// update values}}
                \State Update $\widetilde{\ChoiceProbability}^k_{h,j}(\cdot | \cdot, \cdot)$ by~\eqref{eq:optimistic_choice}. 
                \label{eq:alg_optQ_est_1}
                \State Update $Q^k_{h,j}(\cdot, A) \leftarrow \sum_{a \in A  } \widetilde{\ChoiceProbability}^k_{h,j}(a | \cdot, A) f^k_{h,j}(\cdot,a)$
                and 
                $V^k_{h, j}(\cdot) \leftarrow  \max_{A \in \SetOfActions} Q^k_{h,j}(\cdot, A)$, $j = 1, \pm 2$. \label{eq:alg_optQ_est_2}
            \EndFor            
        % \EndIf
        %
        \State Receive initial state $s^k_1$.
        \For{$h=1,2, \dots, H$}  
            \State Offer $A^k_h$ by \eqref{eq:exploration_policy} and receive $a^k_h$, $r^k_h$, and $s^k_{h+1}$. \label{eq:alg_exp_policy}
            \State Update $\mathcal{D}^k_h \leftarrow \mathcal{D}^{k-1}_h \cup \{s^k_h, a^k_h, r^k_h, s^k_{h+1}\}$
            and update $\sigma^k_h$ and $\bar{\sigma}^{k}_h$ by~\eqref{eq:sigma}.
            %
            \label{eq:alg_var}
            \State Update $\hat{\thetab}^{k+1}_h$ using online mirror descent~\eqref{eq:online_update}.
        \EndFor
    \EndFor
    \end{algorithmic}
\end{algorithm*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Step 1. Online parameter estimation for MNL (Line~\ref{eq:alg_online_update}).}
%
At episode $k$ and horizon $h$, given the user's choice feedback $c^k_h \in A^k_h$, the response for each item $a_{i_m} \in A^k_h$ is defined as $y^k_h(a_{i_m}) := \mathbbm{1}(c^k_h = a_{i_m}) \in \{0,1 \}$.
Therefore, the response variable  $\yb^k_h := (y^k_h(\OutsideOption), y^k_h(a_{i_1}), \dots y^k_h(a_{i_l})  )$, where $l \leq M-1$, is sampled from a multinomial distribution: $\yb^k_h \sim \operatorname{MNL} \{ 1, \ChoiceProbability_h(\OutsideOption | s^k_h, A^k_h ; \thetab^\star_h), \dots, \ChoiceProbability_h(a_l | s^k_h, A^k_h ; \thetab^\star_h) \} $, where the parameter $1$ indicates that $\yb^k_h$ is a single-trial sample, i.e., $y^k_h(\OutsideOption) + \sum_{m=1}^l y^k_h(a_{i_m}) = 1$.
%
Then, for any $(k,h) \in [K] \times [H]$, the multinomial logistic loss function is defined as:
%
\begin{align*} 
    \ell^k_h(\thetab) := - \sum_{a \in A^k_h} y^k_{h}(a) \log \ChoiceProbability_h(a | s^k_h, A^k_h ; \thetab).
\end{align*}
%
Inspired by~\citet{zhang2024online, lee2024nearly}, for all $(k,h) \in [K] \times [H]$, we use the online mirror descent algorithm to estimate the true parameter $\thetab^\star_h$ as follows: 
%
\begin{align*} 
    \thetab^{k+1}_h 
    \in \argmin_{\thetab \in \Theta} \langle \nabla \ell^{k}_h(\thetab^k_h), \thetab \rangle
    + \frac{1}{2 \eta} \| \thetab - \thetab^k_h \|_{\tilde{\Hb}^{k}_h}^2 \, , 
    \numberthis \label{eq:online_update}
\end{align*}
%
where $\eta \!= \!\BigO (\log M)$ is the step-size,  
$\tilde{\Hb}^{k}_h \!:= \Hb^k_h + \eta \nabla^2 \ell^k_h(\thetab^k_h)$, 
and $\Hb^k_h \!:= \lambda \Ib_d + \sum_{\tau=1}^{k-1} \nabla^2 \ell^\tau_h(\thetab^{\tau+1}_h)$.
%
\begin{remark} \label{remark:efficient_online_estimate}
    The computation cost of the optimization problem in~\eqref{eq:online_update} is $\mathcal{O}(M d^3)$, which dose not scale with $k$ at all.
\end{remark}

Then, with high probability, $\thetab^\star_h$ lies within the following confidence interval (Corollary~\ref{corollary:good_event_MNL}).
%
\begin{equation} \label{eq:confidence_set}
    \mathcal{C}^k_h 
    := \left\{ \thetab \in \Theta: 
    \left\| \thetab -  \thetab^k_h \right\|_{\Hb^k_h}
    \leq \alpha^k_h
    = \BigOTilde (  \sqrt{d} )
    \right\}.
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Step 2. Weighted regression and optimistic estimation for $\GroundQ$ (Line~\ref{eq:alg_optQ_1}-\ref{eq:alg_optQ_2}).}
%
Using the past dataset, we solve the following (weighted) regression problem to fit $\mathcal{T}_h V^k_{h+1}$:
%
\begin{align}
    \hat{f}^k_{h,1} &\in \argmin_{f_h \in \mathcal{F}_h} \sum_{\tau=1}^{k-1} 
    \frac{\left( f_h(s^\tau_h, a^\tau_h) - r^\tau_h - V^k_{h+1, 1}(s^\tau_{h+1}) \right)^2}{\left( \bar{\sigma}^\tau_h \right)^2}, 
    \label{eq:weighted_regression}
\end{align}
where $(\bar{\sigma}^k_h)^2$ is a variance upper bound, i.e., $(\bar{\sigma}^k_h)^2 \geq \VV [  r_h + V^k_{h+1,1} (s_{h+1})   | s^k_h, a^k_h ] $, which will be specified later.
%
Then, an \textit{optimistic $\GroundQ$-value} estimate at horizon $h$ is defined as $f^k_{h,1} \!:=\! \hat{f}^k_{h,1} \!+ \!b^k_h$, where $b^k_h$
is the optimistic bonus.
The bonus is calculated as $b^k_h(s, a)\! =\! \max_{f_h \in \mathcal{F}_h} \!f_h (s, a) - \min_{f_h \in \mathcal{F}_h}\! f_h(s, a)$.
In general, this uncertainty bonus has a high complexity, as the maximizing and minimizing functions can differ arbitrarily for each $(s,a)\in \SetOfStates \times \SetOfGroundActions$.
To address this, we use a \textit{low-complexity bonus oracle}  $\BonusOracle$~\citep{agarwal2023vo} that approximately dominates the value obtained from the point-wise maximization over $\mathcal{F}_h$.
With the oracle $\BonusOracle$, we can efficiently calculate the bonus $b^k_h$, with an error of $\epsilon_b$.
Due to space constraints, we provide the formal definition of  $\BonusOracle$ in Appendix~\ref{app:sec_bonus_oracle} (Definition~\ref{def:bonus_oracle}).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Step 3. Overly optimistic/pessimistic estimation for $\GroundQ$ (Line~\ref{eq:alg_ooptQ_1}-\ref{eq:alg_ooptQ_2}).}
For a sharp analysis of the convergence of the optimistic estimate $f^k_{h,1}$, we define an \textit{overly optimistic $\GroundQ$-value} estimate  $f^k_{h,2}$, as well as an \textit{overly pessimistic $\GroundQ$-value} estimate $f^k_{h, -2}$.
Similarly to $f^k_{h,1}$, they are calculated by solving an \textit{unweighted} regression problem (Line~\ref{eq:alg_ooptQ_1}), and by adding (or subtracting) a bonus function, which is the output of the bonus oracle $\BonusOracle$ (Line~\ref{eq:alg_ooptQ_1_1}-\ref{eq:alg_ooptQ_2}). 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Step 4. Variance estimation (Line~\ref{eq:alg_g_hat} and~\ref{eq:alg_var}).}
%
To calculate $\bar{\sigma}^k_h$ introduced in~\eqref{eq:weighted_regression}, we first estimate the second moment by solving the \textit{unweighted} regression problem:
%
\begin{align*}
    \hat{g}^k_{h} 
    &\in \argmin_{g_h \in \mathcal{F}_h} \sum_{\tau=1}^{k-1} \left( g_h(s^\tau_h, a^\tau_h) - \left(r^\tau_h + V^k_{h+1, 1}(s^\tau_{h+1}) \right)^2 \right)^2.
\end{align*}
Then, denoting $z^k_h = (s^k_h, a^k_h)$ for simplicity, we calculate the estimated variance as follows 
(this is an informal description; for the precise formulation, see Equations~\eqref{eq:app_sigma} and~\eqref{eq:app_bar_sigma} in Appendix):
%
\begin{align*} 
    \left(\sigma^k_h \right)^2 
    \!\!&\simeq  
        \hat{g}^k_h(z^k_h) - \left( \hat{f}^k_{h,-2}(z^k_h) \right)^2
        \!\!+ \mathfrak{D}^{\mathbf{1}}_{k,h}
    \cdot \BigO\left( \sqrt{\log \mathcal{N}\mathcal{N}_b } \right) 
    \\
    \bar{\sigma}^k_h 
    &\simeq \max \bigg\{ \sigma^k_h,  
    \BigO(\log \mathcal{N}\mathcal{N}_b ) 
    \\&\cdot \bigg( \max \left\{f^k_{h,2}(z^k_h) - f^k_{h, -2}(z^k_h),  
     \mathfrak{D}_{k,h}^{\sigma} \right\}
    \bigg)  \bigg\},
    \numberthis \label{eq:sigma}
\end{align*}
%
where
$\mathfrak{D}_{k,h}^{\mathbf{1}} =  D_{\mathcal{F}_h}\!\! \left( z^k_{h}; \Zb_{k-1}, 
\{\mathbf{1}^{\tau} \}_{\tau=1}^{k-1} \right) $ and
$\mathfrak{D}_{k,h}^{\sigma} =  D_{\mathcal{F}_h}\!\! \left( z^k_{h}; \Zb_{k-1}, 
\sigmab_{k-1}\right) $.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Step 5. Efficient optimistic $Q$-value estimation based on unknown item values (Line~\ref{eq:alg_optQ_est_1}-\ref{eq:alg_optQ_est_2}).}
%
In this step, we address our main challenge: selecting an optimistic assortment based on the optimistic 
$\GroundQ$-values, which incorporate uncertainty, while ensuring computational tractability.

%
To introduce optimism and encourage exploration, we need to solve the following optimization problem using the optimistic (or pessimistic) estimates of the $\GroundQ$-values, specifically $f^k_{h,j}$ for $j = 1, \pm 2$:
% %
\begin{align*}
    A^k_{h,j} \in \argmax_{A \in \SetOfActions} \max_{\thetab \in \mathcal{C}^k_h } \sum_{a \in A  } \ChoiceProbability^k_{h}(a | s^k_h, A; \thetab) f^k_{h,j}(s^k_h,a),
    \numberthis \label{eq:naiv_optimization}
\end{align*}
%
where $\mathcal{C}^k_h$ is defined in~\eqref{eq:confidence_set}.
% Thus, we hope that $V^k_{h,j}(s^k_h) \geq V^\star_{h}(s^k_h)$.
One naive approach to solving the optimization problem in~\eqref{eq:naiv_optimization} is to add bonus terms to the estimation for each assortment $A$
and then enumerate all $A \in \SetOfActions$ to find the maximum. 
However, this approach results in a computational cost of $\BigO (|\SetOfGroundActions|^M)$.

To avoid this exponential computational cost, inspired by~\citet{tran2015composite}, we use optimistic MNL utilities instead of directly adding bonus terms to $\sum_{a \in A  } \ChoiceProbability^k_{h}(a | s^k_h, A; \thetab^k_h) f^k_{h,j}(s^k_h,a)$.
However, unlike traditional MNL bandits~\citep{oh2019thompson, oh2021multinomial, lee2024nearly}, simply using optimistic utilities does not always guarantee optimism because of $f^k_{h,j}$ is not the true values.
%
%
In MNL bandits, the item values are known, and the value of the outside option $\OutsideOption$ is fixed at zero. 
Therefore, increasing the MNL utilities (using the optimistic utilities) lowers the probability of choosing the outside option, which in turn increases the expected value of the item values.


However, in our setting, using the optimistic utilities can decrease the expected value of $f^k_{h,j}$.
To explain why: even if the true value of the outside option, $\GroundQ^\star_h(s, \OutsideOption)$, is the lowest, its estimated value, $f^k_{h,j}(s, \OutsideOption)$, can be the highest---i.e., $f^k_{h,j}(s, \OutsideOption) > f^k_{h,j}(s, a)$ for all $a \in \SetOfGroundActions \setminus \{ \OutsideOption \}$---due to uncertainty. 
In this case, increasing the MNL utilities results in a decrease in the expected value of $f^k_{h,j}$.
This challenge arises from the unknown item values, $\GroundQ^\star_h$, which is one of the main difficulties we face in our framework.

%
To tackle this problem, 
a more refined approach is required to use the utility based on $f^k_{h,j}$.
Given the confidence interval in~\eqref{eq:confidence_set}, we define the \textit{optimistic utility} $\OptUtil^k_h(s,a)$ and the \textit{pessimistic utility} $\PessiUtil^k_h(s,a)$  as:
%
\begin{align*}
    \OptUtil^k_h(s,a) &:= \phi(s,a)^\top \thetab^k_h + \alpha^k_h \| \phi(s,a) \|_{\left(\Hb^k_h\right)^{-1} },
    \\
    \PessiUtil^k_h(s,a) &:= \phi(s,a)^\top \thetab^k_h - \alpha^k_h \| \phi(s,a) \|_{\left(\Hb^k_h\right)^{-1} }.
    \numberthis
    \label{eq:opt_pessi_utility}
\end{align*}
%
We then use the optimistic utility when $f^k_{h,j}(s, \OutsideOption)$ is not the highest estimate to calculate the \textit{optimistic choice} probabilities $\widetilde{\ChoiceProbability}^k_{h,j}$.
Formally, let $I^k_{h,j} \in \{1,0\}$ indicate whether there exists $a \in \SetOfGroundActions \setminus \{\OutsideOption\} $ such that $f^k_{h,j}(s,a) \geq f^k_{h,j}(s,\OutsideOption) $ ($I^k_{h,j}=1$ if such an event occurs). 
Then, we define
%
\begin{align}
    \widetilde{\ChoiceProbability}^k_{h,j}(a | s, A) 
    := 
    \begin{cases}
        &\dfrac{ \exp\left(  \OptUtil^k_{h}(s,a) \right)  }{  \sum_{a' \in A  } \exp\left(  \OptUtil^k_{h}(s,a') \right) }, 
        \quad \text{if}\,\, I^k_{h,j}=1
        \\
        &\dfrac{ \exp\left( \PessiUtil^k_{h}(s,a) \right)  }{  \sum_{a' \in A  } \exp\left(  \PessiUtil^k_{h}(s,a') \right) }, \quad \text{otherwise}.
    \end{cases}
    \label{eq:optimistic_choice}
\end{align}
%
Equipped with $\widetilde{\ChoiceProbability}^k_{h,j}$, for any $j = 1, \pm 2$, we select the assortments $A^k_{h,j}$ as follows:
\begin{align*}
    A^k_{h,j} \in \argmax_{A \in \SetOfActions} 
    \underbrace{
    \sum_{a \in A  } \widetilde{\ChoiceProbability}^k_{h,j}(a | s^k_h, A) f^k_{h,j}(s^k_h,a)}_{=: Q^k_{h,j}(s^k_h, A)}
    ,
    % \quad \forall j = 1, \pm 2,
    \numberthis \label{eq:optvalue_optimization}  
\end{align*}
%
Here, $Q^k_{h,j}(s^k_h, A) := \sum_{a \in A  } \widetilde{\ChoiceProbability}^k_{h,j}(a | s^k_h, A) f^k_{h,j}(s^k_h,a)$ is the \textit{optimistic $Q$-values}.
This construction can induce sufficient exploration and guarantee optimism (Lemma~\ref{lemma:optimism}).
%
Furthermore, by using the optimistic (or pessimistic) utilities for each item, instead of calculating bonus terms for each $A \in \SetOfActions$, the optimization problem in~\eqref{eq:optvalue_optimization} can be solved efficiently~\citep{davis2013assortment}.
\begin{remark} \label{remark:opt_problem}
    The optimization problem in~\eqref{eq:optvalue_optimization} can be transformed into a linear programming (LP), making it solvable in polynomial time with respect to $|\SetOfGroundActions|$
    (see Appendix~\ref{app:eq_optvalue_optimization})
    .
\end{remark}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Step 6. Exploration policy (Line~\ref{eq:alg_exp_policy}).}
%
% Let $ A^k_{h,j} \in \argmax_{A \in \SetOfActions}  Q^k_{h,j}(s^k_h, A)$ for $j=1,2$.
We then offer the assortment $A^k_h$ to the user as follows:
%
\begin{equation} \label{eq:exploration_policy}
    A^k_h 
    = \begin{cases}
        A^k_{h,1} \quad &\text{if}\,\, f^k_{h',1}(s^k_{h'}, a_{h'}) \geq f^k_{h',2}(s^k_{h'}, a_{h'}) - u_k, \\ &
        \quad\forall  a_{h'} \in A^k_{h',1}, \forall h' \leq h,
        \\
        A^k_{h,2} \quad  &\text{otherwise},
    \end{cases}
\end{equation}
where $u_k$ is a carefully chosen threshold (see Table~\ref{tab:param} for the exact value). 
When the optimistic sequence $f^k_{h,1}$ and the overly optimistic sequence $f^k_{h,2}$ diverge beyond a certain threshold, we offer the assortment $A^k_{h,2}$, which is selected based on $f^k_{h,2}$.
This approach ensures that by occasionally using $f^k_{h,2}$, the variance upper bound $\bar{\sigma}^k_h$, estimated from $f^k_{h,2}$, does not become overly pessimistic.
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Main Results}
\label{sec:main_results}
%
%%%%%%%%%%%%%%%%%%%%%
\subsection{Nonlinear Function Approximation for $\GroundQ$}
\label{subsec:result_nonlinear}
%
\begin{theorem}[Informal, Regret upper bound of \AlgName{}] 
\label{thm:regret_upper_general_main} 
Let $d_\nu = \frac{1}{H} \sum_{h=1}^H  \operatorname{dim}_{\nu,K} (\mathcal{F}_h)$ with $\nu = \sqrt{1/KH}$, and set $u_k$ as in Equation~\eqref{eq:u_k_def}.
Suppose Assumptions~\ref{assum:MNL_click_model} and~\ref{assum:completeness} hold.
Then, with probability at least $1-\delta$, the regret of~\AlgName{} is upper-bounded by:
%
    \begin{align*}
        \Regret&(\MDP, K) 
        \lesssim \underbrace{d  \sqrt{HK} 
            %
            +  \frac{1}{\kappa}d^2 H^2}_{\text{regret from MNL model}}
            \\
            & + 
             \underbrace{ \vphantom{ \frac{1}{\kappa} } \sqrt{d_\nu H K  \log \mathcal{N} } 
            + d_\nu H^{5} \log \mathcal{N} \log^2 \!\left( \mathcal{N} \mathcal{N}_b \right)
            }_{\text{regret from general function approximation of } \GroundQ }
        ,   
    \end{align*}
    where $d$ is the feature dimension of the MNL preference model, $\mathcal{N} = \max_{h \in [H]} |\mathcal{F}_h| $, and $\mathcal{N}_b$ is the size of the bonus function class, i.e., $\mathcal{N}_b = |\mathcal{W}|$.
\end{theorem}
%
\textbf{Discussion of Theorem~\ref{thm:regret_upper_general_main}.}
The proof is deferred to Appendix~\ref{app:proof_theorem_upper_general}.
% %
The first two terms arise from the regret of the MNL preference model, while the other two terms come from the regret associated with the general function approximation for item-level $Q$-values.
When $H=1$, reducing our setting to MNL bandits (though not exactly the traditional MNL bandits, as we consider a more general case where item values are unknown and the value of the outside option can be non-zero), the first two terms of our regret simplify to $\BigOTilde(d \sqrt{K} + \frac{1}{\kappa}d^2)$. 
This matches the known minimax optimal regret established by~\citet{lee2024nearly}.
Note that we avoid the detrimental dependence on $\kappa$ in our leading term.
%
The last two terms of our regret, incurred from estimating item-level $Q$-values using general function approximation, similar to \citet{agarwal2023vo} and \citet{zhao2023nearly}.


With respect to computational cost, by using the online sensitivity sub-sampling method (Algorithm~\ref{alg:online_sensitivity}), 
we can efficiently implement the bonus oracle $\BonusOracle$ with  $\log |\mathcal{W}| 
= \log \mathcal{N}_b
        = \BigOTilde \left(
           \max_{h \in [H]} \operatorname{dim}_{\nu, K} (\mathcal{F}_h) 
             \log  \mathcal{N}
           \log |\SetOfStates \times \SetOfGroundActions|
        \right)$.
%
Furthermore, we can avoid the exponential computational cost required to solve the optimization in~\eqref{eq:optvalue_optimization} (see Remark~\ref{remark:opt_problem}).
As a result, our algorithm is both computationally tractable and statistically efficient.
%

\subsection{Technical Comparisons to Related Work}
\paragraph{Comparison to~\citet{lee2024nearly}.}
Our framework addresses a strictly more challenging problem than~\citet{lee2024nearly} because we consider \textit{(1) multiple steps}, \textit{(2) unknown values}, and \textit{(3) nonzero values for the outside option}.
The key challenge in tackling these three aspects lies in ensuring optimism while maintaining computational efficiency, which requires a fundamentally different approach—carefully leveraging optimistic and pessimistic utilities.
Moreover, in Theorem~\ref{thm:regret_upper_general_main}, our regret analysis for the MNL preference model goes beyond merely summing over $H$ MNL bandit regrets.
%
Instead, we introduce a novel regret decomposition, bound the regret in terms of the sum of the variances of value functions, and apply the \textit{law of total variance}~\citep{lattimore2012pac, gheshlaghi2013minimax}.
As a result, we achieve a regret reduction by a factor of $\sqrt{H}$ compared to directly summing over $H$ MNL bandit regrets.
%
\vspace{-0.3cm}

\paragraph{Comparison to~\citet{agarwal2023vo}.}
While the regret analysis for general function approximation largely follows the approach of~\citet{agarwal2023vo}, 
several technical lemmas (e.g., Lemmas~\ref{lemma:b_1_fine_grained_bound} and~\ref{lemma:bounding_Too}) and parameters (e.g., $u_k$) are revised to accommodate the estimation errors specific to MNL models. 

Overall, our results cannot be gleaned by simply piecing together prior techniques. Instead, they arise from an involved analysis, leading to stronger theoretical guarantees in more general and new combinatorial RL settings.




%%%%%%%%%%%%%%%%%%%%%
\subsection{Linear MDPs with Preference Feedback}
\label{subsec:result_linear}
%
As a special case, we also consider linear MDPs (refer Definition~\ref{def:linearMDP}) with preference feedback.
To show the dependency on parameters, we denote the linear MDPs as $\MDP_{\Xi^\star}$, where $\Xi^\star = \{ \{ \thetab_h^\star\}_{h=1}^H, \{\mub_h^\star\}_{h=1}^H, \{ \wb^\star_h\}_{h=1}^H \}$.
%
%
Note that the bonus oracle can be easily implemented using the standard elliptical bonus, which satisfies all the necessary properties (refer Appendix~\ref{app:proof_upper_bound_linearMDP}).
The proof is deferred to Appendix~\ref{app:proof_upper_bound_linearMDP}.

\begin{theorem}[Informal, Regret upper bound for linear MDPs] \label{thm:regret_upper_linear} 
In linear MDPs, under the same conditions as Theorem~\ref{thm:regret_upper_general_main},  with probability at least $1-\delta$, the regret of \AlgName{} is upper-bounded by:
%
    \begin{align*}
         \Regret \left(\MDP_{\Xi^\star},K \right) 
         &\lesssim d  \sqrt{H K} 
             + \frac{1}{\kappa} d^2 H^2
             \\&+ 
             \vphantom{ \frac{1}{\kappa} }  \DimLinearMDP  \sqrt{HK} 
             +(\DimLinearMDP)^6 H^5 
         .
    \end{align*}
\end{theorem}
%
We also establish a matching lower bound by 
constructing a novel multi-layered (linear) MDP (see Figure~\ref{fig:MNL_linear_MDP}) with a preference feedback.
The proof is deferred to Appendix~\ref{app:proof_lower_bound_linearMDP}.
%
%
\begin{theorem} [Informal, Regret lower bound for linear MDPs]
\label{thm:regret_lower_linear_main}
%
For any algorithm and sufficiently large $K$, there exists an episodic linear MDP $\MDP_{\Xi}$ with MNL preference feedback such that the worst-case expected regret is lower bounded as follows:
%
    \begin{align*}
    \sup_{\Xi}
        \EE_{\Xi} \left[ \Regret\left(\MDP_{\Xi}, K \right)\right] 
        = \Omega \left( 
            d \sqrt{HK}
            + \DimLinearMDP \sqrt{HK}
        \right).
    \end{align*}
\end{theorem}
%
\textbf{Discussion of Theorems~\ref{thm:regret_upper_linear} and~\ref{thm:regret_lower_linear_main}.}
%
For sufficiently large $K$, i.e., $K \geq \BigOTilde\!\left( d^2 H^3/ \kappa^2 \!+\! (\DimLinearMDP)^{10} H^9 \right)$, the regret upper bound for linear MDPs scales as $\BigOTilde(d\sqrt{H K} + \DimLinearMDP \sqrt{HK} )$, which matches the lower bound up to logarithmic factors.
%
%
Note that if we rescale the rewards to be $1/H$ in the lower bounds of~\citet{zhou2021nearly} to align with our setting, their regret bound matches the second term of our regret bound, $\Omega(\DimLinearMDP \sqrt{HK})$.
To the best of our knowledge, this is the first theoretical result proving minimax-optimality in linear MDPs with preference feedback.
%


\section{Numerical Experiment}
\label{sec:experiment}
%
In this section, we empirically evaluate the performance of our algorithm, \AlgName{}, in linear MDPs. 
We consider an \textit{online shopping with budget} (refer Figure~\ref{fig:online_shopping}) environment under linear MDPs and an MNL user preference model. 
We denote the set of states as $\mathcal{S} = \{s_1, \dots, s_{|\mathcal{S}|} \}$ 
and  the set of items as $\mathcal{I} = \{a_1, \dots, a_N , \OutsideOption \}$ ($\OutsideOption$ denotes the outside option).  
Each state $s_j \in \mathcal{S}$ corresponds to a \textit{user's budget level}, where a larger index $j$ indicates a higher budget (e.g., $s_{|\mathcal{S}|}$ represents the state with the largest budget). 
The initial state is set to the medium budget state $s_{\lceil  |\mathcal{S}|/2 \rceil }$. 
Furthermore, we let the transition probabilities $\TransitionProbability_h$, rewards $\Reward_h$, and preference model $\ChoiceProbability_h$ be the same for all $h \in [H]$, and thus we omit the subscript $h$.

At state $s_j$, the agent offers an assortment $A \in \SetOfActions$ with a maximum size of $M$.  
The user then either purchases an item $a_i \in A$ 
or opts not to buy anything, represented by the outside option $\OutsideOption \in A$.
Then, the reward is defined as follows:
%
\begin{itemize}
    \item  If the user purchases an item $a_i \in A$, the reward is: $r(s_j,a_i) = \left(\frac{i}{100N} + \frac{j}{|\mathcal{S}|} \right)/H$.
    \item If the user does not buy anything ($\OutsideOption$), the reward is: $r(s_j,\OutsideOption) = 0$.
\end{itemize}
%   
The reward can be regarded as the \textit{user's rating} of the purchased item. 
It is reasonable to assume that, at higher budget states, users tend to be more generous in their ratings, leading to higher ratings (rewards). 
And the transition probability is defined as follows:
%
\begin{itemize}
    \item If the user purchases an item $a_i \in A$, the transition probability is:
    \begin{align*}
        \mathbb{P}(s_{\min(j+1, |\mathcal{S}|) } |s_j, a_i) 
        &= 1 - \frac{i}{N},
        \quad
        \text{and}
        \\
        \mathbb{P}( s_{\max(j-1, 0)} |s_j, a_i) &= \frac{i}{N}.
    \end{align*}

    \item If the user does not buy anything ($\OutsideOption$), the transition probability is: 
    \begin{align*}
        \mathbb{P}(s_{\min(j+1, |\mathcal{S}|) } |s_j, \OutsideOption) = 1
    \end{align*}
\end{itemize}
%
If the user does purchase an item, the budget level decreases with a certain probability that depends on the chosen item. 
Conversely, if the user does not purchase any item ($\OutsideOption$), the budget level increases deterministically. 
%
\begin{figure}[t]
\resizebox{\linewidth}{!}{
    \tikzstyle{every node}=[font=\small]
    \centering
    \begin{tikzpicture}[->,>=stealth',shorten >=2pt, 
        line width=0.7 pt, node distance=1.6cm,
        scale=1, 
        transform shape, align=center, 
        state/.style={circle, minimum size=0.5cm, text width=5mm}]
        \node[state, fill=red!30, draw=red] (one) {$s_1$};
        \node[state, fill=red!70!yellow!20, draw=red!70!yellow!50] (two) [right of=one] {$s_2$};
        \node[state, fill=yellow!90, draw=brown!50!yellow] (dots) [right of=two] {$s_3$};
        
        \node[state, fill=yellow!40!green!20, draw=yellow!40!green!80] (n-1) [right of=dots] {$s_{4}$};
        \node[state, fill=green!70, draw=green!70!black!50] (n) [right of=n-1] {$s_{5}$};
        
        
        \path (one) edge [ bend left ] node [above]{$1-\frac{i}{N}$} (two) ;
        \draw[->] (two.145) [bend left] to node[below]{$\frac{i}{N}$} (one.35);
        \path (two) edge [ bend left ] node [above]{$1-\frac{i}{N}$} (dots) ;
    
        \draw[->] (dots.145) [bend left] to node[below]{$\frac{i}{N}$} (two.35); 
        \path (dots) edge [ bend left ] node [above]{$1-\frac{i}{N}$} (n-1) ;
        \draw[->] (n-1.145) [bend left] to node[below]{$\frac{i}{N}$} (dots.35); 
        \path (n-1) edge [ bend left ] node [above]{$1-\frac{i}{N}$} (n) ;
        \draw[->] (n.145) [bend left] to node[below]{$\frac{i}{N}$} (n-1.35); 
        
        \path (one) edge [ loop left ] node {$\frac{i}{N}$} (one) ;
        \draw[densely dashed, <-] (two.-100) [bend left] to node[below]{$1$} (one.-80);
        \draw[densely dashed, <-] (dots.-100) [bend left] to node[below]{$1$} (two.-80);
        \draw[densely dashed, <-] (n-1.-100) [bend left] to node[below]{$1$} (dots.-80);
        \draw[densely dashed, <-] (n.-100) [bend left] to node[below]{$1$} (n-1.-80);
        
        \path (n) edge [ loop right ] node {$1-\frac{i}{N}$} (n);    
        \draw[->, dashed] (n) to[out=-60, in=-30, looseness=7] node[right]{$1$}  (n);
        
    \end{tikzpicture}
}
    \caption{The ``online shopping with budget'' environment with $|\SetOfStates| = 5$.
    Each state represents the user's budget level of $1,2,3,4$, or $5$.
    The solid line indicates the transition when the user purchases an actual item $a_i$ (with a reward of $\left( i/(100N) + j/|\mathcal{S}| \right)/H$), 
      and the dashed line shows the transition when the user does not purchase any item (with a reward of $0$).
      The initial state is $s_3$.
    }
    \label{fig:online_shopping}
\end{figure}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[htp!]
    \centering
        \includegraphics[clip, trim=5.3cm 0.0cm 4.3cm 0.0cm, width=\textwidth]{figures/experiment.pdf}
    \caption{Episodic returns over 10 independent runs. 
    The dotted lines represent estimated (virtual) episodic returns for cases that could not be run due to excessively long runtimes.
    } 
    \label{fig:experiment}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We construct the feature map $\psi(s,a)$ (for linear MDPs) using SVD. 
Specifically, the transition kernel $\mathbb{P}(\cdot | \cdot, \cdot) \in \mathbb{R}^{|\mathcal{S}||\mathcal{I}| \times |\mathcal{S}|}$ has at most $|\mathcal{S}|$ singular values, and the reward vector $r( \cdot, \cdot) \in \mathbb{R}^{|\mathcal{S}||\mathcal{I}|}$ has one singular value. Consequently, the feature map $\psi(s,a) \in \mathbb{R}^{d_{lin}}$ lies in a space of dimension $|\mathcal{S}| + 1$, i.e., $d_{lin} = |\mathcal{S}| + 1$. 


For MNL preference model, the true parameter $\thetab^\star \in \mathbb{R}^d$, and the feature  $\phi(s,a) \in \mathbb{R}^d$ (for MNL preference model) are randomly sampled from a $d$-dimensional uniform distribution in each instance. 

We set $K = 30000$, $H=5$, $M=6$, $|\mathcal{S}|=5$, $d = 5$ (feature dimension for MNL preference model), $d^{lin} = 6$ (feature dimension for linear MDP), $N \in \{10, 20, 40\}$ (the number of items), and $|\mathcal{A}| = \sum_{m'=1}^{M-1}$$N \choose m$ $\in \{ 637, 21699, 760098\}$ (the number of assortments). 
Moreover, for simplicity, we set $\bar{\sigma}^k_h = 1$  in our algorithm. 
As a result, we use unweighted regression to estimate the $\GroundQ$-values.
%

%
We compare our algorithm with two baselines: \texttt{Myopic} and \texttt{LSVI-UCB}~\citep{jin2020provably}.
\texttt{Myopic} is a variant of \texttt{OFU-MNL+}~\citep{lee2024nearly} adapted for \textit{unknown} rewards. 
It is a myopic algorithm that selects assortments based only on immediate rewards, ignoring state transitions.
\texttt{LSVI-UCB}~\citep{jin2020provably} treats each assortment as a single, atomic (holistic) action, requiring enumeration of all possible assortments.
To demonstrate the effectiveness of our approach, we also include the performance of the optimal policy (\texttt{Optimal}) to highlight that our algorithm is converging toward optimality.
We run the algorithms on 10 independent instances and report the episodic return across all episodes.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[t]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{l|rr|r}
\toprule
                     &   Myopic & LSVI-UCB & MNL-VQL \\
\midrule            
$N = 10, |\SetOfActions| = 637$          & $0.089$ s  & $0.136$ s          & $0.463$ s      \\
$N = 20, |\SetOfActions| = 21,699$       & $0.097$ s  & $4.861$ s          & $0.526$ s      \\
$N = 40, |\SetOfActions| = 760,098$      & $0.113$ s  & $453.641$ s        & $0.620$ s   \\
\bottomrule
\end{tabular}
}
\caption{Average runtime (seconds) per episode.}
\label{table:runtime}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Figure~\ref{fig:experiment} demonstrates that our algorithm significantly outperforms other
baseline algorithms. 
And Table~\ref{table:runtime} shows that our algorithm maintains robust runtime performance even as the total number of assortments $|\SetOfActions|$ increases.
Although the runtime of \texttt{Myopic}  is approximately $5.3$ times faster than ours, its performance is substantially worse, converging to a suboptimal solution.
This underscores a key limitation of the myopic strategy—it can completely fail in certain environments, highlighting the importance of accounting for long-term outcomes.
Additionally, the runtime of \texttt{LSVI-UCB} increases exponentially as $N$ grows, because it requires enumerating all possible assortments. 
Due to the extremely slow runtime of \texttt{LSVI-UCB}, we did not include its performance results for $N=20$ and $N=40$. 
Instead, for these cases, we used dotted lines to represent the average episodic return observed for $N=10$.
Even for the smaller case of $N=10$, \texttt{LSVI-UCB} demonstrated the worst performance.
Based on this observation, we suspect that its performance is unlikely to improve as $N$ increases.
%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}~\label{sec:Conclusion}
%
In this work, we study combinatorial RL with preference feedback, extending MNL bandit problems to account for the influence of user states and state transitions in applications like recommendation systems. 
Under an MNL preference model with linear utilities and general function approximation for item values, we propose an efficient algorithm,~\AlgName{}, which, to the best of our knowledge, provides the first statistical guarantee.
As a special case, in linear MDPs, we show the minimax-optimality of~\AlgName{} by establishing matching upper and lower bounds.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage

\section*{Impact Statement}

This paper presents work whose goal is to advance the field of 
Machine Learning. There are many potential societal consequences 
of our work, none which we feel must be specifically highlighted here.

\section*{Acknowledgements}
%
This work was supported by the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT) (No.  RS-2022-NR071853 and RS-2023-00222663) and by AI-Bio Research Grant through Seoul National University.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{main_bib}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\counterwithin{table}{section}
\counterwithin{theorem}{section}
\counterwithin{algorithm}{section}
\counterwithin{figure}{section}
\counterwithin{equation}{section}
\counterwithin{condition}{section}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
% Table of Contents %
\addcontentsline{toc}{section}{Appendix} % Add the appendix text to the document TOC
\part{Appendix} % Start the appendix part
\parttoc % Insert the appendix TOC
%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Illustrative Explanation for Combinatorial RL with Preference Feedback
}
\label{app:sec_add_setting}
%
\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.27\textwidth}
        \includegraphics[width=\textwidth, trim=0cm 9.5cm 22cm 2.3cm, clip]{figures/comb_RL_1.pdf}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.27\textwidth}
        \includegraphics[width=\textwidth, trim=0cm 9.5cm 22cm 2.3cm, clip]{figures/comb_RL_2.pdf}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.27\textwidth}
        \includegraphics[width=\textwidth, trim=0cm 9.5cm 22cm 2.3cm, clip]{figures/comb_RL_3.pdf}
    \end{subfigure}
    \caption{Illustration of combinatorial RL with preference feedback.}
    \label{fig:problem_setting}
\end{figure}
%
%
In this section, we provide additional explanation of our framework, combinatorial RL with preference feedback, for better clarity. 
In this framework, at each episode, as a user arrives at the system (starting in the initial state, e.g., high loyalty), 
a learning agent selects an assortment $A$ (a set of items) and offers it to the user (the first figure in Figure~\ref{fig:problem_setting}).
The user then chooses an item from the assortment $A$ (the second figure in Figure~\ref{fig:problem_setting}).
The agent receives a reward, along with preference (or choice) feedback, and transitions to the next state (e.g., lower loyalty) (the last figure in Figure~\ref{fig:problem_setting}). 
This process repeats until the episode concludes.

The key advantage of this framework is its ability to capture the long-term value of choosing an item by considering state transitions and avoiding myopic decisions.
For instance, in Figure~\ref{fig:problem_setting}, a user may choose a \textit{junk} item that provides a high immediate reward.
However, repeatedly recommending such items can lead to user fatigue, resulting in a transition to a state of lower satisfaction or loyalty to the system, ultimately leading to a lower cumulative reward.

We compare our framework with other related works.

\textbf{vs MNL bandits.}
Our framework can be considered as a multi-step extension of MNL bandits~\citep{rusmevichientong2010dynamic, saure2013optimal, agrawal2017thompson, agrawal2019mnl, oh2019thompson, oh2021multinomial, perivier2022dynamic, agrawal2023tractable, zhang2024online, lee2024nearly}.
In MNL bandits, there are no state transitions; thus, in Figure~\ref{fig:MNL_linear_MDP}, the user exits the system immediately after receiving a reward.

Another important difference is that, in MNL bandits, the value (reward) of choosing an item is assumed to be known, and the value of choosing the outside option $\OutsideOption$ is always assumed to be zero. 
In contrast, in our framework, the value of choosing an item is unknown due to the stochastic nature of rewards and transition probabilities. 
Additionally, we allow the value of choosing the outside option $\OutsideOption$ to be non-zero.


\textbf{vs Cascading RL.}
In cascading RL~\citep{du2024cascading}, the agent also selects a set of items, and state transitions are taken into account when making decisions.
However, these items are offered to the user one by one, and the user decides whether to choose the currently offered item. 


Cascading RL fundamentally differs from our framework because the user does not compare multiple items at once, so it does not involve relative preference feedback. 
Another key distinction is that in cascading RL, the probability of choosing each item is independent of the others in the chosen set of items
In contrast, in our MNL preference model, the choice probability of an item is influenced by the other items in the assortment.


\textbf{vs PbRL.}
In preference-based RL (PbRL)~\citep{akrour2012april, wirth2017survey, christiano2017deep, ouyang2022training, saha2023dueling, zhu2023principled, zhan2023provable},
the agent learns not from explicit numerical rewards, but through preferences as feedback. 
The user is presented with two (or sometimes multiple) items and chooses a preferred one.

In our framework, if we treat the reward signal generated by the user’s choice as a preference signal instead of a numerical reward, we can learn the policy based on user preferences, similar to PbRL, without relying on explicit rewards.
%
However, our framework differs fundamentally from PbRL because our goal is not just to offer a single item, but multiple items—a combinatorial (base) action—at each timestep.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Efficient Bonus Oracle $\BonusOracle$ using Online-subsampling} 
\label{app:sec_bonus_oracle}
%
The guarantees of Algorithm~\ref{alg:main} rely on a consistent bonus oracle, $\BonusOracle$, that satisfies Definition~\ref{def:bonus_oracle}.
%
\begin{definition} [Oracle $\BonusOracle$,~\citealt{agarwal2023vo}]
\label{def:bonus_oracle}
% Let $z = (s, a) \in \SetOfStates \times \SetOfGroundActions$.
For any $(h,k) \in [H] \times [K]$, sequence of $\{ \bar{\sigma}^\tau_h \}_{\tau=1}^{k-1}$ and dataset $\mathcal{D}^{k-1}_h = \{ (s^\tau_h, a^\tau_h, r^\tau_h, s^\tau_{h+1}) \}_{\tau=1}^{k-1}$, function class $\mathcal{F}_h$ with $\hat{f}_h \in \mathcal{F}_h$, $\beta_h$, $\rho >0$, error parameter $\epsilon_b \geq 0$, the bonus oracle $\BonusOracle( \{ \bar{\sigma}^\tau_h \}_{\tau=1}^{k-1}, \mathcal{D}^{k-1}_h, \mathcal{F}_h, \hat{f}_h, \beta_h, \rho,  \epsilon_b  )$ outputs a bonus function $b_h(\cdot)$ such that, for any $z_h = (s_h, a_h) \in \SetOfStates \times \SetOfGroundActions$, we have
%
\begin{itemize}
    \item $b_h : \SetOfStates \times \SetOfGroundActions \rightarrow \RR_{+}$ belongs to a bonus function class $\BonusClass$
    and denote $\mathcal{N}_b = |\mathcal{W}|$.
    \item $b_h(z_h) \geq \max \left\{ |f_h(z_h) - \hat{f}_h(z_h)|, f_h \in \mathcal{F}_h : \sum_{\tau=1}^{k-1} 
    \frac{1}{\left( \bar{\sigma}^\tau_h \right)^2}
    \left( f_h(z^\tau_h) - \hat{f}^k_h(z^\tau_h) \right)^2
    \!\!\leq\! 
        \left( \beta_h \right)^2 \right\}$.
    \item $b_h(z_h) \leq C \cdot \left( \mathcal{D}_{\mathcal{F}_h} \left(z_h; \{ z^\tau_h \}_{\tau=1}^{k-1},  \{\bar{\sigma}^\tau_h\}_{\tau=1}^{k-1} \right)  
    \cdot \sqrt{\left( \beta_h \right)^2 + \rho} + \epsilon_b \cdot \beta_h\right)$
    with $0<C<\infty$.
\end{itemize}
%
Further we say the oracle $\BonusOracle$ is consistent if for any $k < k'$ with consistent 
$\{\bar{\sigma}^\tau_h\}_{\tau=1}^{k-1} \subseteq \{\bar{\sigma}^\tau_h\}_{\tau=1}^{k'-1}$, $\mathcal{D}^{k-1}_h \subseteq \mathcal{D}^{k'-1}_h$, $\beta^k_h$ non-decreasing in $k$ for each $h \in [H]$ and $ \hat{f}^k_h$ as defined in~\eqref{eq:weighted_regression}, it holds that $\BonusOracle( \{ \bar{\sigma}^\tau_h \}_{\tau=1}^{k-1}, \mathcal{D}^{k-1}_h, \mathcal{F}_h, \hat{f}^k_h, \beta^k_{h}, \rho,  \epsilon_b  ) \geq \BonusOracle( \{ \bar{\sigma}^\tau_h \}_{\tau=1}^{k'-1}, \mathcal{D}^{k'-1}_h, \mathcal{F}_h, \hat{f}^{k'}_h, \beta^{k'}_h, \rho,  \epsilon_b  )$~element-wise.
\end{definition}
%
With the oracle $\BonusOracle$, we can efficiently calculate the optimistic $\GroundQ$-value estimate $f^k_h$ with an error of $\epsilon_b$.
%
%

To implement this oracle, we use the online sensitivity sub-sampling approach described by~\citet{agarwal2023vo}, which builds on the original sensitivity sub-sampling method proposed by~\citet{kong2021online} and~\citet{wang2020reinforcement_eluder}.

For completeness, we include the sub-sampling procedure in Algorithm~\ref{alg:online_sensitivity} and show its guarantees in Proposition~\ref{prop:implementing_B_sub-sampling}.
%
Let $z = (s,a) \in \SetOfStates \times \SetOfGroundActions$.
We first define the weighted data set $\mathcal{Z}$, where each element is $\left(z, \bar{\sigma}(z) \right)$, and introduce the \textit{weighted sensitivity score} as follows:
%
\begin{align*}
    \operatorname{sensitivity}_{\mathcal{Z}, \mathcal{F}, \gamma, \nu  }(z)
    = \min \left\{
        \sup_{f, f' \in \mathcal{F}} \frac{\frac{1}{\bar{\sigma}^2 (z)  } \left( f(z) - f'(z) \right)^2}{
        \min \left\{ 
                \sum_{z' \in \mathcal{Z}} \frac{1}{\bar{\sigma}^2 (z') }\left( f(z') - f'(z') \right)^2,
                \frac{K(H+1)^2}{\nu^2} 
            \right\} 
            + \gamma^2
        },
        1
    \right\}.
\end{align*}
%
Now we introduce the sub-sampling procedure.
%
\begin{algorithm}[h!]
   \caption{Online Sensitivity Sub-sampling with Weights}
   \label{alg:online_sensitivity}
    \begin{algorithmic}[1]
    \State \textbf{Inputs:}   function class $\mathcal{F}$, current sub-sampled dataset $\hat{\mathcal{Z}} \subseteq \SetOfStates \times \SetOfGroundActions$, new state-action pair $s,a$,
    parameter $\gamma$,
    threshold $\nu > 0$,
    failure probability $\delta$.
    \State \textbf{Parameter: $1 \leq C < \infty$} 
    \State Let $p_z$ be the smallest real number such that
        \begin{align*}
            1/ p_z 
            \,\text{ is an integer and }\,
            p_z \geq \min \left\{ 
                    1, C \cdot \operatorname{sensitivity}_{\hat{\mathcal{Z}}, \mathcal{F}, \gamma, \nu  } (z)
                    \cdot \log (K \mathcal{N}/\delta )
                    \right\}.
        \end{align*}
    \State Independently add $1/p_z$ copies of $(z, \bar{\sigma}(z))$ into $\hat{\mathcal{Z}}$ with probability $p_z$.
    \State \textbf{Return:} $\hat{\mathcal{Z}}$.
    \end{algorithmic}
\end{algorithm}
%

For the weighted dataset $\mathcal{Z}^{k-1}_h = \{ (s^\tau_h, a^\tau_h), \bar{\sigma}^\tau_h \}_{\tau=1}^{k-1}$,
we defined $\| f \|^2_{\mathcal{Z}^{k-1}_h } = \sum_{z \in \mathcal{Z}^{k-1}_h } \frac{1}{\bar{\sigma}^2(z) } f^2(z) $, i.e., weighted sum of $\ell_2$-norm square.
We denote $\hat{\mathcal{Z}}^{k-1}_h$ as the dataset sub-sampled from $\mathcal{Z}^{k-1}_h$.
%
At every $(k,h) \in [K] \times [H] $,
we call Algorithm~\ref{alg:online_sensitivity} with the current sub-sampled dataset $\hat{\mathcal{Z}}^{k-1}_h$ and the new state action-pair $z^k_h = (s^k_h, a^k_h)$ to generate the next sub-sampled dataset $\hat{\mathcal{Z}}^{k}_h$.

The following proposition shows that the distance of any two functions measured by the historical dataset $\mathcal{Z}^{k-1}_h$ is well approximated by the subsamping dataset $\hat{\mathcal{Z}}^{k-1}_h$.
Additionally, it shows that both the number of distinct elements in $|\hat{\mathcal{Z}}^{k-1}_h|$ and its the total size (counting repetitions) do not scale with $\operatorname{poly}(\SetOfStates)$.

\begin{proposition} [Guarantees of online sensitivity sub-sampling, Proposition 13 of~\citealt{agarwal2023vo}]
\label{prop:guarantee_online_sensitivity_sub-sampling}
    Let $z = (s,a) \in \SetOfStates \times \SetOfGroundActions$.
    When $\bar{\sigma}(z) \geq \nu $ for any $z$, then with probability at least $1-\delta$, it holds that
    %
    \begin{align*}
        \sup_{f_1, f_1 : \| f_1 - f_2 \|_{\mathcal{Z}^k_h}^2 \leq \gamma^2 } | f_1(z) - f_2(z)  |
        &\leq \sup_{f_1, f_1 : \| f_1 - f_2 \|_{\hat{\mathcal{Z}}^k_h}^2 \leq 10^2\gamma^2 } | f_1(z) - f_2(z)  |
        \\
        &\leq \sup_{f_1, f_1 : \| f_1 - f_2 \|_{\mathcal{Z}^k_h}^2 \leq 10^4\gamma^2 } | f_1(z) - f_2(z)  |
        .
    \end{align*}
    %
    Further, for any $(k,h) \in [K] \times [H]$, the number of distinct elements in sub-sampled dataset $\hat{\mathcal{Z}}^k_h$ is always bounded by
    $\BigO \left( \log \frac{K \mathcal{N}}{\delta} \cdot \max_{h \in [H]} \operatorname{dim}_{\nu, K} (\mathcal{F}_h) \right)$
    and the total size of $\hat{\mathcal{Z}}^k_h$ is bounded by $\BigO (K^3/\delta)$.
\end{proposition}
%
We can assert that the predictive differences between the functions are preserved up to constant factors, while requiring significantly less data.
Then, the size of the bonus class $\mathcal{W}$ in Definition~\ref{def:bonus_oracle} is bounded as follows:
%
\begin{proposition} [Implementing $\BonusOracle$ using online-subsampling, Corollary 14 of~\citealt{agarwal2023vo}]
    \label{prop:implementing_B_sub-sampling}
    There exists an algorithm (see Algorithm~\ref{alg:online_sensitivity}) such that, with probability at least $1-\delta$, implements a consistent bonus oracle $\BonusOracle$ with $\epsilon_b = 0$ for all $(k,h) \in [K] \times [H]$, where
    %
    \begin{align*}
        \log |\mathcal{W}|
        \leq \BigO \left(
           \max_{h \in [H]} \operatorname{dim}_{\nu, K} (\mathcal{F}_h) 
           \cdot  \log \frac{K \mathcal{N}}{\delta} 
           \log \frac{K |\SetOfStates \times \SetOfGroundActions| }{\delta}
        \right).
    \end{align*}
\end{proposition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Efficient combinatorial optimization}
\label{app:eq_optvalue_optimization}
%
In this section, we explain how to solve the combinatorial optimization problem in~\eqref{eq:optvalue_optimization}, following the method outlined in~\citet{davis2013assortment, ie2019slateq}.

To find an assortment $A \in \SetOfActions$ that maximizes the optimistic $Q$-value, a fundamental step in $Q$-learning and crucial for inducing exploration, we must solve the following combinatorial optimization problem:
\begin{align*}
     \max_{A \in \SetOfActions}  \sum_{a \in A  } 
     \widetilde{\ChoiceProbability}^k_{h,j}(a | s^k_h, A) f^k_{h,j}(s^k_h,a),
    \numberthis \label{eq:app_optvalue_optimization}
\end{align*}
%
where $\widetilde{\ChoiceProbability}^k_{h,j}$ is the optimistic choice probability as defined in~\eqref{eq:optimistic_choice} (also in~\eqref{eq:app_notation_opt-choice}), and $f^k_{h,j}$ is the $\GroundQ$-value estimate (item-level $Q$-values) as defined in~\eqref{eq:app_notation_groundQ_estimates}.

Fix $(k,h,s, j) \in [K] \times [H] \times \SetOfStates \times \{1, 2, -2 \}$.
For simplicity, we will abbreviate these indices.
Accordingly, we denote $w_a = \exp\left( \OptUtil^k_h(s^k_h,a) \right)$ or $w_a = \exp\left( \PessiUtil^k_h(s^k_h, a) \right)$, depending on the value of $j$.
Additionally, let $\tilde{f}_a = f^k_{h,j}(s^k_h,a)$ for simplicity.

We can then express the optimization problem in~\eqref{eq:app_optvalue_optimization} in terms of  $w$ as fractional mixed-integer program (MIP), with binary variables $x_a \in \{0,1\}$ for each item $a \in \SetOfGroundActions \setminus \{\OutsideOption \}$, indicating whether $a$ is included in the assortment $A$:
%
\begin{align*}
    \max\,\,& 
    \frac{ w_{\OutsideOption} \tilde{f}_{\OutsideOption}  + \sum_{a \in \SetOfGroundActions \setminus \{\OutsideOption \}}  x_a w_a \tilde{f}_a }{w_{\OutsideOption} + \sum_{a' \in \SetOfGroundActions \setminus \{\OutsideOption \}}  x_{a'} w_{a'} }
    \numberthis \label{eq:app_frac_MIP}
    \\
    \text{s.t.}\,
    & 
     \sum_{a \in \SetOfGroundActions \setminus \{\OutsideOption \}} x_a
     \leq \MaxAssortment - 1;
     \\
    &x_a \in \{ 0,1\},\,\, \forall a \in \SetOfGroundActions \setminus \{\OutsideOption \}.
\end{align*}
%
By~\citet{chen2000mathematical}, the binary indicator in the MIP can be relaxed, resulting in the following fractional linear program (LP):
%
\begin{align*}
    \max\,\,& 
    \frac{ w_{\OutsideOption} \tilde{f}_{\OutsideOption}  + \sum_{a \in \SetOfGroundActions \setminus \{\OutsideOption \}}  x_a w_a \tilde{f}_a }
    {w_{\OutsideOption} + \sum_{a' \in \SetOfGroundActions \setminus \{\OutsideOption \}}  x_{a'} w_{a'} }
    \numberthis \label{eq:app_frac_LP}
    \\
    \text{s.t.}\,
    & 
     \sum_{a \in \SetOfGroundActions \setminus \{\OutsideOption \}} x_a
     \leq \MaxAssortment - 1;
     \\
    &0 \leq x_a \leq 1,   \,\, \forall a \in \SetOfGroundActions \setminus \{\OutsideOption \}.
\end{align*}
%
Since this relaxed problem is a fractional linear program (LP), using the Charnes-Cooper method~\citep{cooper1962programming}, it can be transformed into a (non-fractional) LP. 
To achieve this, we introduce additional variables:
%
\begin{align*}
    t = \frac{1}{w_{\OutsideOption} + \sum_{a' \in \SetOfGroundActions \setminus \{\OutsideOption \}}  x_{a'} w_{a'}  },
    \quad
    y_a = \frac{x_a}{w_{\OutsideOption} + \sum_{a' \in \SetOfGroundActions \setminus \{\OutsideOption \}}  x_{a'} w_{a'}  }.
\end{align*}
%
Then, we can obtain the following LP:
%
\begin{align*}
    \max\,\,& \sum_{a \in \SetOfGroundActions \setminus \{\OutsideOption \} } \tilde{f}_a  w_a  y_a
    + \tilde{f}_{\OutsideOption} w_{\OutsideOption} t 
    \numberthis \label{eq:app_LP}
    \\
    \text{s.t.}\,
     &\sum_{a \in \SetOfGroundActions \setminus \{\OutsideOption \}}
     w_a y_a + w_{\OutsideOption} t = 1;
     \\
    & 
     \sum_{a \in \SetOfGroundActions \setminus \{\OutsideOption \}} y_a
     \leq (\MaxAssortment - 1) t; \quad t \geq 0.
\end{align*}
%
The optimal solution ($y^\star_{a_1}, \dots, y^\star_{a_N}, t^\star $) to this LP in~\eqref{eq:app_LP} provides the optimal $x_i$ values for the fractional LP in \eqref{eq:app_frac_LP}  by setting $x_a = y^\star_a / t^\star$.
This, in ture, determines the optimal assortment in the original fractional MIP (\eqref{eq:app_frac_MIP}) by including any item where  $y^\star_a  > 0$.
Thus, the optimization problem is proven to be solvable in polynomial time.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proof of Theorem~\ref{thm:regret_upper_general_main}} 
\label{app:proof_theorem_upper_general}
%
%
\subsection{Notations and Preliminaries}
\label{app:subsec_notions_thm_upper_general}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[htp!]
\centering
    \caption{Summary of notations}
    \label{tab:notation}
    \resizebox{0.83\textwidth}{!}{
    \begin{tabular}{clc}
         \toprule
         Notation  &   Meaning   & Remark \\
         \midrule
         $\SetOfStates, \SetOfActions, \SetOfGroundActions$   & state space, action (assortment) space, item set   &     \\[0.1cm]
         $k, h$   & $k \in [K]$ episode, $h \in [H+1]$ horizon    &        \\[0.1cm]
         $r^k_h, s^k_h, A^k_h, a^k_h$   & reward, state, action and item at $k,h$    &        \\[0.1cm]
         $r_h, s_h, A_h, a_h$   & random reward, state, action and item $ h$    &        \\[0.1cm]
         $z$   & shorthand for state-item pair $(s,a)$   &        \\[0.1cm]
         $\GroundQ : \SetOfStates \times \SetOfGroundActions \rightarrow \RR$   & item-level $Q$-value function   &        \\[0.1cm]
         $\mathcal{T}_h, \mathcal{T}^2_h$&  Bellman operator and  second-moment operator    &        \\[0.1cm]
         $\mathcal{D}^{k-1}_h$   & data set $\{ (s^\tau_h, a^\tau_h, r^\tau_h, s^\tau_{h+1}) \}_{\tau=1}^{k-1}$    &       \\[0.1cm]
         $\mathcal{F}_h$   & function class for horizon $h \in [H]$    &    Ass.~\ref{assum:completeness}     \\[0.1cm]
         $\FunctionClassLinMDP_h$   & linear function class for horizon $h \in [H]$    &  Eqn.~\ref{eq:linear_function_class}      \\[0.1cm]
         $\FunctionClassLinMDP_h(\epsilon_c)$   & $\epsilon_c$-cover of linear function class $\FunctionClassLinMDP_h$   &        \\[0.1cm]
         $\BonusClass$   & bonus function class defined for bonus oracle $\BonusOracle$    &     Def.~\ref{def:bonus_oracle}   \\[0.1cm]
         $\epsilon_b$   & error paremeter for bonus oracle    &        \\[0.1cm]
         $\mathcal{N}$   & maximal size of function class, i.e., $\max_{h \in [H]} |\mathcal{F}_h|$    &        \\[0.1cm]
         $\mathcal{N}_b$   & size of bonus function class $|\BonusClass|$    &        \\[0.1cm]
         $D^2_{\mathcal{F}} \left( z;  \{z^\tau\}_{\tau=1}^{k-1}, \{\sigma^\tau\}_{\tau=1}^{k-1}   \right)$   & $:= \sup_{f_1, f_2, \in \mathcal{F}} \frac{\left( f_1(z) - f_2(z) \right)^2}{\sum_{\tau=1}^{k-1} \frac{1}{(\sigma^{\tau})^2} \left( f_1(z^{\tau}) - f_2(z^{\tau}) \right)^2 + \rho }$    &    $\rho$ param.   \\[0.1cm]
         $\GenEluder_{\nu, K} (\mathcal{F}) $   & generalized Eluder dimension defined in Definition~\ref{def:gen_eluder}    &    $\nu$ param.    \\[0.1cm]
         $d_\nu$   & $:= \frac{1}{H} \sum_{h=1}^H \GenEluder_{\nu, K} (\mathcal{F}_h) $ (Definition~\ref{def:gen_eluder})   &      $\nu$ param.   \\[0.1cm]
         %
         $ \ell^k_h(\thetab)$   & $- \sum_{a \in A^k_h} y^k_{h}(a) \log \ChoiceProbability_h(a | s^k_h, A^k_h ; \thetab) $, loss for MNL model at $k,h$  &    Eqn.~\ref{eq:MNL_loss_app}    \\[0.1cm]
         $  \Hb^k_h, \tilde{\Hb}^{k}_h$   
            & $ = \lambda \Ib_d + \sum_{\tau=1}^{k-1} \nabla^2 \ell^\tau_h(\thetab^{\tau+1}_h), =\Hb^k_h + \eta \nabla^2 \ell^k_h(\thetab^k_h)$, respectively & Eqn.~\ref{eq:hessian_app}       \\[0.1cm]
         $ \mathcal{C}^k_h$   &  confidence interval for MNL model at $k,h$  &    Eqn.~\ref{eq:confidence_set_app}    \\[0.1cm]
         % %
         $f^k_{h,1}$   & optimistic  $\GroundQ$ at $k,h$    &        \\[0.1cm]
         $\hat{f}^k_{h,1}$ &  solution of fitting weighted regression at $k,h$   &  Eqn.~\ref{eq:weighted_regression_app}      \\[0.1cm]
         $\mathcal{F}^k_{h,1}$ &version space of optimistic  $\GroundQ$ at $k,h$&   Eqn.~\ref{eq:CI_opt_app}     \\[0.1cm]
         $f^k_{h,\pm 2}$ & overly optimistic (pessimistic) $\GroundQ$ at $k,h$   &        \\[0.1cm]
         % 
         $\hat{f}^k_{h,\pm 2}$ &  solution of fitting unweighted regression at $k,h$   &  Eqn.~\ref{eq:unweighted_regression_app}      \\[0.1cm]
         $\mathcal{F}^k_{h,\pm2}$ &version space of overly optimistic (pessimistic) $\GroundQ$ at $k,h$&   Eqn.~\ref{eq:CI_overly_app}     \\[0.1cm]
         $\hat{g}^k_h$ &  solution of fitting second-moment regression at $k,h$   &     Eqn.~\ref{eq:second_regression_app}   \\[0.1cm]
         $\mathcal{G}^k_h$ &  version space of second-moment estimates at $k,h$  & Eqn.~\ref{eq:CI_second_app}       \\[0.1cm]
         %
         $\EventMNL$&    event that $\{  \thetab^\star_h \in \mathcal{C}^k_h $ for all $k \geq 1$ and all $h \in [H] \}$ &        \\[0.1cm]
         %
         $\Event^k_h$ &  event that $\{ \mathcal{T}_h V^k_{h+1, j} \in \mathcal{F}^k_{h, j} \,\,\text{for}\,\, j=1, \pm 2 \,\, \text{and}\,\, \mathcal{T}^2_h V^k_{h+1, 1} \in \mathcal{G}^k_h \}$   &    Ass.~\ref{assum:completeness}    \\[0.1cm]
         $\Event_{\leq k}$&   joint event that $\mcap_{\tau=1}^k \mcap_{h=1}^H \Event^\tau_h$   &        \\[0.1cm]
         %
         $\OptUtil^k_h(s^k_h,a), \PessiUtil^k_h(s^k_h,a)$   &  optimistic (pessimistic) utility defined in~\eqref{eq:opt_pessi_utility} &       \\[0.1cm]
         $\widetilde{\ChoiceProbability}^k_h(a | s, A)$   &  optimistic choice probability defined in~\eqref{eq:optimistic_choice}  &     \\[0.1cm]
         %
         $Q^k_{h,j}(s, A)$&  $:=\sum_{a \in A} \widetilde{\ChoiceProbability}^k_{h,j}(a | s, A) f^k_{h,j}(s,a)$ 
          for $j=1,\pm2$  &        \\[0.1cm]
         $V^k_{h, j}(s)$&  
         $:=\max_{A \in \SetOfActions} Q^k_{h,j}(s, A)$ for $j=1,\pm2$  &        \\[0.1cm]
         %
         $A^k_{h,j} $&   $\in \argmax_{A \in \SetOfActions}  \sum_{a \in A  } \widetilde{\ChoiceProbability}^k_{h,j}(a | s^k_h, A) f^k_{h,j}(s^k_h,a)$ for $j=1,\pm 2$ &      \\[0.1cm]
         $A^k_{h} $& chosen assortment at $k,h$ by assortment selection rule in~\eqref{eq:exploration_policy}  &      \\[0.1cm]
         $Q^k_{h}(s, A), V^k_{h}(s)$& \textit{realized} optimistic values determined by~\eqref{eq:optimistic_Q} &   Eqn.~\ref{eq:exploration_policy}      \\[0.1cm]
         %
         $h_k $&    random $h$ when first taking action $A^k_{h,2}$ at $k$, i.e., $A^k_h = A^k_{h,2}$ &  Eqn.~\ref{eq:exploration_policy}       \\[0.1cm]
         $\OptEpisode, \OOptEpisode $&    disjoint subsets of $[K]$ when $h_k = H+1$ or $h_k \in [H]$  &  Eqn.~\ref{eq:exploration_policy}    \\[0.1cm]
         %
         $b^k_{h,j} $&   bonus term obtained in Line~\ref{eq:bonus_alg_1} and~\ref{eq:bonus_alg_2} using $\BonusOracle$   &   Def.~\ref{def:bonus_oracle}      \\[0.1cm]
         $\EE_{\TransitionProbability} [ \cdot | s^k_h, a^k_h], 
         \mathbb{V}_{\TransitionProbability}[\cdot | s^k_h, a^k_h] $&   $ \EE_{s_{h+1} \sim \TransitionProbability_h(\cdot | s^k_h, a^k_h)} [ \cdot | s^k_h, a^k_h], \mathbb{V}_{s_{h+1} \sim \TransitionProbability_h(\cdot | s^k_h, a^k_h)} [ \cdot | s^k_h, a^k_h]   $  &         \\[0.1cm]
         $\EE_{\ChoiceProbability} [ \cdot | s^k_h, A^k_h] 
         $&   $ \EE_{a_{h} \sim \ChoiceProbability_h(\cdot | s^k_h, A^k_h)} [ \cdot | s^k_h, A^k_h]$  &         \\[0.1cm]
         \bottomrule
    \end{tabular}
}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[h!]
\centering
    \caption{Summary of parameter choices}
    \label{tab:param}
    \resizebox{0.76\textwidth}{!}{
    \begin{tabular}{clc}
         \toprule
         Notation  &   Choice  & Remark \\
         \midrule
         $\delta$   &  $\delta \in (0, 1/(H^2 + 15) )$  &     \\[0.1cm]
         $\delta^k_h$   &  $\delta/\left((K+1) (H+1) \right)$  &     \\[0.1cm]
         $ \eta$   & $\frac{1}{2}\log (M+1) + \BoundMNL +1$, step-size parameter for OMD  &    Eqn.~\ref{eq:app_online_update}    \\[0.1cm]
         $ \lambda$   & $84 \sqrt{2}d \eta$, regularization parameter  &        \\[0.1cm]
         $ \alpha^k_h$   & $\BigO (\sqrt{d} \log k \log M) $, confidence radius of $\mathcal{C}^k_h$  &    Eqn.~\ref{eq:confidence_radius_MNL}    \\[0.1cm]
         %
         $\epsilon_c$   &  error due to taking covering of function class  &     \\[0.1cm]
         $\nu$   &  $\sqrt{1/KH}$  &  Def.~\ref{def:gen_eluder}   \\[0.1cm]
         $\rho$   & $1$  &  Def.~\ref{def:gen_eluder}   \\[0.1cm]
         $o(\delta)$   & $ \sqrt{\log \frac{ \mathcal{N}^2 (2 \log (4 \BoundFunction K / \nu) +2) ( \log(8 \BoundFunction/\nu^2)+2) }{\delta}}  $  &     \\[0.1cm]
         $\iota(\delta)$   & $ 3\sqrt{\log \frac{ \mathcal{N} \mathcal{N}_b (2 \log (4 \BoundFunction K / \nu) +2) ( \log(8 \BoundFunction/\nu^2)+2) }{\delta}}  $   &    \\[0.1cm]
         $\beta^k_{h,1}$   & $\sqrt{\left(6\sqrt{\rho} + 156\right) \cdot\log \frac{\mathcal{N}^2 (K+1)(H+1)(2 \log \frac{4\BoundFunction K }{\nu} + 2)(\log \frac{8 \BoundFunction}{\nu^2} +2) }{\delta}}$, &\\[0.1cm]
            &confidence radius of $\mathcal{F}^k_{h,1}$   &  Eqn.~\ref{eq:CI_opt_app}  \\[0.1cm]
         $\iota' (\delta)$   & $ \sqrt{ 2 \log \frac{ \mathcal{N} \mathcal{N}_b (2 \log (18 \BoundFunction K) +2) (\log(18 \BoundFunction)+2) }{\delta} } $  &     \\[0.1cm]
         $\beta^k_{h,2}$   & $ \sqrt{2 (24 \BoundFunction + 21) (\iota'(\delta^k_h))^2 } $, confidence radius of $\mathcal{F}^k_{h,\pm 2}$   &   Eqn.~\ref{eq:CI_overly_app}   \\[0.1cm]
         $\iota'' (\delta)$   & $ \sqrt{ 2 \log \frac{ \mathcal{N} \mathcal{N}_b (2 \log (32 \BoundFunction K) +2) (\log(32 \BoundFunction)+2) }{\delta} } $   &     \\[0.1cm]
         $\bar{\beta}^k_{h}$   & $ \sqrt{8(11 \BoundFunction + 9 ) (\iota''(\delta^k_h))^2 } $, confidence radius of $\mathcal{G}^k_{h}$  &   Eqn.~\ref{eq:CI_second_app}  \\[0.1cm]
         $\left( \sigma^k_h \right)^2$   & $ \min \bigg\{ 4, \,\,\hat{g}^k_h(z^k_h) - \left( \hat{f}^k_{h,-2}(z^k_h) \right)^2   $  &  Eqn.~\ref{eq:sigma} \\[0.1cm]
         & $
         + D^2_{\mathcal{F}_h} \left( z^k_{h}; \{z^{\tau}_h\}_{\tau=1}^{k-1}, \{\mathbf{1}^\tau\}_{\tau=1}^{k-1} \right) \cdot \left( \sqrt{\left( \bar{\beta}_h^k \right)^2 + \rho} + 2\BoundFunction \sqrt{ \left( \beta^k_{h,2} \right)^2 + \rho }  \right) \bigg\} $ &  \\[0.1cm]
         $ \bar{\sigma}^k_h $   & $\max \bigg\{ \sigma^k_h, \nu, \sqrt{2} \iota(\delta^k_h) \sqrt{ f^k_{h,2}(z^k_h) - f^k_{h, -2}(z^k_h) }, 
         $  &     \\[0.1cm]
         & $\quad\quad   2 \left( \sqrt{o(\delta^k_h)} + \iota(\delta^k_h) \right) \cdot \sqrt{  D_{\mathcal{F}_h}\! \left( z^k_h; \{z^\tau_h\}_{\tau=1}^{k-1}, \{\bar{\sigma}^\tau_h \}_{\tau=1}^{k-1} \right) }  \bigg\}$ & Eqn.~\ref{eq:weighted_regression_app} \\[0.1cm]
         %
         $u_k$   & $ C \cdot 
        \!\Bigg( 
        \sqrt{\log \frac{\mathcal{N}KH}{\nu \delta}}
        \cdot \left( \log \frac{\mathcal{N}\mathcal{N}_b KH}{\nu \delta} \cdot H^{5/2} \sqrt{d_\nu} 
        + \sqrt{k}H \epsilon_b \right)$
        \\[0.1cm]
        &$\quad\quad\quad\quad+
        dH^{5/2} \log K \log M \sqrt{ \log \frac{\mathcal{N}\mathcal{N}_b KH}{\nu \delta} } \Bigg)
        /\sqrt{k}
        $ for $C>0$  &   Eqn.~\ref{eq:exploration_policy}  \\[0.1cm]         
         \bottomrule
    \end{tabular}
    }
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
In this subsection, for easy reference, we introduce notations and definitions used throughout the proof. 
The key notations are summarized in Table~\ref{tab:notation}, and the specific parameter choices are listed in Table~\ref{tab:param}.

\textbf{Online parameter update and confidence interval for MNL preference model.}
We define the multinomial logistic loss function at $(k,h) \in [K] \times [H]$ as follows:
%
\begin{align} 
\label{eq:MNL_loss_app}
    \ell^k_h(\thetab) := - \sum_{a \in A^k_h} y^k_{h}(a) \log \ChoiceProbability_h(a | s^k_h, A^k_h ; \thetab).
\end{align}
%
To achieve constant-time parameter estimation, we use the online mirror descent algorithm to estimate the true parameter  $\thetab^\star_h$:
%
\begin{align*} 
    \thetab^{k+1}_h 
    \in \argmin_{\thetab \in \Theta} \langle \nabla \ell^{k}_h(\thetab^k_h), \thetab \rangle
    + \frac{1}{2 \eta} \| \thetab - \thetab^k_h \|_{\tilde{\Hb}^{k}_h}^2 \, , \quad 
    \text{where}\,\, \Theta = \left\{\thetab \in \RR^d : \| \thetab \|_2 \leq \BoundMNL \right\},
    \numberthis \label{eq:app_online_update}
\end{align*}
%
where  $\eta = \frac{1}{2}\log (M+1) + \BoundMNL +1$ is the step-size parameter, and the related matrices are defined as:
%
\begin{align*}
    \Hb^k_h &:= \lambda \Ib_d + \sum_{\tau=1}^{k-1} \nabla^2 \ell^\tau_h(\thetab^{\tau+1}_h), 
    \\
    \tilde{\Hb}^{k}_h &:= \Hb^k_h + \eta \nabla^2 \ell^k_h(\thetab^k_h),
    \numberthis \label{eq:hessian_app}
\end{align*}
%
where
\begin{align*}
    \nabla^2 \ell^k_h(\thetab) 
    &= 
    \sum_{a \in A^k_h} \ChoiceProbability_h (a | s^k_h, A^k_h ; \thetab) \phi(s^k_h,a) \phi(s^k_h,a)^\top 
    \\
    &-  \sum_{a \in A^k_h}  \sum_{a' \in A^k_h} \ChoiceProbability_h (a | s^k_h, A^k_h ; \thetab)\ChoiceProbability_h (a' | s^k_h, A^k_h ; \thetab) \phi(s^k_h,a) \phi(s^k_h,a')^\top.
\end{align*}
%
By a standard online mirror descent formulation~\citep{orabona2019modern},~\eqref{eq:app_online_update} can be solved using a single projected gradient step through the following equivalent formula:
%
\begin{align}
    \bar{\thetab}^{k+1}_h = \thetab^k_h - \eta \left(\tilde{H}^k_h\right)^{-1} \nabla \ell^k_h(\thetab^k_h),
    \quad
    \text{and}
    \quad
    \thetab^{k+1}_h \in \argmin_{\thetab \in \Theta} \| \thetab - \bar{\thetab}^{k+1}_h \|_{\tilde{H}^k_h},
\end{align}
which enjoys a computational cost of only $\mathcal{O}(Md^3)$, completely independent of $k$~\citep{mhammedi2019lipschitz, zhang2024online, lee2024nearly}.

We define the confidence interval at $(k,h) \in [K] \times [H]$ as follows:
%
\begin{align*}
    \mathcal{C}^k_h 
    := \left\{ \thetab \in \Theta: 
    \left\| \thetab -  \thetab^k_h \right\|_{\Hb^k_h}
    \leq \alpha^k_h
    \right\},
    \numberthis
    \label{eq:confidence_set_app}
\end{align*} 
where the radius of the confidence interval $\mathcal{C}^k_h$ is as follows:
\begin{align*}
    \alpha^k_h 
    &= \sqrt{
        2\eta \Bigg( 
        11 \cdot ( 3\log(1+(M+1)k) + B + 2 )
        \log \left( \frac{2 \sqrt{1 + 2k}}{\delta} \right)
        + 2 
        +  \frac{7\sqrt{6}}{6}  d \eta \log \left( 1 + \frac{k+1}{2\lambda}\right) +  2 \Bigg)
         + 4\lambda B^2
     } \numberthis \label{eq:confidence_radius_MNL}
\end{align*}
%
Then, we define the optimistic and pessimistic utility as follows:
%
\begin{align*}
    \OptUtil^k_h(s,a) := \phi(s,a)^\top \thetab^k_h + \alpha^k_h \| \phi(s,a) \|_{\left(\Hb^k_h\right)^{-1} },
    \quad
    \PessiUtil^k_h(s,a) := \phi(s,a)^\top \thetab^k_h - \alpha^k_h \| \phi(s,a) \|_{\left(\Hb^k_h\right)^{-1} },
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Regression and confidence intervals for item-level functions.}
In this paper, we define $\mathcal{N}:= \max_{h \in [H]}|\mathcal{F}_h|$ as the maximum size of the function classes $\mathcal{F}_1, \dots, \mathcal{F}_H$ and
$\mathcal{N}_b = |\mathcal{W}|$ as the size of the bonus function class $\mathcal{W}$.

For all $(k,h) \in [K] \times [H]$, the weighted regression problem for fitting the optimistic item-level $Q$-functions, $\GroundQ$, along with the version space of these functions, is defined as:
\begin{align}
    \hat{f}^k_{h,1} &\in \argmin_{f_h \in \mathcal{F}_h} \sum_{\tau=1}^{k-1} \frac{1}{\left( \bar{\sigma}^\tau_h \right)^2} \left( f_h(s^\tau_h, a^\tau_h) - r^\tau_h - V^k_{h+1,1}(s^\tau_{h+1}) \right)^2, \label{eq:weighted_regression_app}
    \\
    \mathcal{F}^k_{h,1} &:= \left\{ f_h \in \mathcal{F}_h : \sum_{\tau=1}^{k-1} 
    \frac{1}{\left( \bar{\sigma}^\tau_h \right)^2}
    \left( f_h(s^\tau_h, a^\tau_h) - \hat{f}^k_{h,1}(s^\tau_h, a^\tau_h) \right)^2  \leq \left( \beta^k_{h,1} \right)^2 \right\}. \label{eq:CI_opt_app}
\end{align}
%
Let $z^k_h = (s^k_h, a^k_h)$.
The parameters are as follows (for $k \geq 2$):
\begin{align*}
    \left(\sigma^k_h\right)^2 
    &:=
    \min \Bigg\{ 4, \,\,\hat{g}^k_h(z^k_h) - \left( \hat{f}^k_{h,-2}(z^k_h) \right)^2
    \\
    & \quad\quad\quad 
        + D_{\mathcal{F}_h} \left( z^k_{h}; \{z^{\tau}_h\}_{\tau=1}^{k-1}, \{\mathbf{1}^\tau\}_{\tau=1}^{k-1} \right) 
        \cdot \left( \sqrt{\left( \bar{\beta}^k_h \right)^2 + \rho} 
        + \sqrt{\left( \beta^k_{h,2} \right)^2 + \rho} 
        \right) 
    \Bigg\}
    , 
    \numberthis \label{eq:app_sigma}
\end{align*}
\begin{align*}
    %
    \bar{\sigma}^k_h &:=
    \max \bigg\{ \sigma^k_h, \nu, \sqrt{2} \iota(\delta^k_h) \sqrt{ f^k_{h,2}(z^k_h) - f^k_{h, -2}(z^k_h) }, 
    \\
    & \quad\quad\quad\quad\quad\quad\quad\quad   2 \left( \sqrt{o(\delta^k_h)} + \iota(\delta^k_h) \right) \cdot \sqrt{  D_{\mathcal{F}_h}\! \left( z^k_h; \{z^\tau_h\}_{\tau=1}^{k-1}, \{\bar{\sigma}^\tau_h \}_{\tau=1}^{k-1} \right) }  \bigg\}, 
    \numberthis \label{eq:app_bar_sigma}
    %
    \\
    \beta^k_{h,1} &:= \sqrt{\left(6\sqrt{\rho} + 156\right) \cdot\log \frac{\mathcal{N}^2 (K+1)(H+1)(2 \log \frac{4\BoundFunction K }{\nu} + 2)(\log \frac{8 \BoundFunction}{\nu^2} +2) }{\delta}},
    \\
    o(\delta^k_h) &:=\sqrt{\log \frac{ \mathcal{N}^2 (2 \log (4 \BoundFunction K / \nu) +2) ( \log(8 \BoundFunction/\nu^2)+2) }{\delta^k_h}}, 
    \quad
    \delta^k_h := \frac{\delta}{(K+1)(H+1)},
    \\
    \iota(\delta^k_h) &:=
    3\sqrt{\log \frac{ \mathcal{N} \mathcal{N}_b (2 \log (4 \BoundFunction K / \nu) +2) ( \log(8 \BoundFunction/\nu^2)+2) }{\delta^k_h}}.
\end{align*}
%    
%
%
For all $(k,h) \in [K] \times [H]$, the unweighted regression for fitting overly optimistic and overly pessimistic item-level $Q$-functions, along with the version space of them, is defined as follows:
%
\begin{align}
    \hat{f}^k_{h, \pm 2} 
    &\in \argmin_{f_h \in \mathcal{F}_h} \sum_{\tau=1}^{k-1} \ \left( f_h(s^\tau_h, a^\tau_h) - r^\tau_h - V^k_{h+1, \pm 2}(s^\tau_{h+1}) \right)^2, \label{eq:unweighted_regression_app}
    \\
    \mathcal{F}^k_{h,\pm2} 
    &:= \left\{ f_h \in \mathcal{F}_h : \sum_{\tau=1}^{k-1} \left( f_h(s^\tau_h, a^\tau_h) - \hat{f}^k_{h,\pm2}(s^\tau_h, a^\tau_h) \right)^2  \leq \left( \beta^k_{h,2} \right)^2 \right\}. \label{eq:CI_overly_app}
\end{align}
%
We choose the parameters as follows:
\begin{align*}
    \beta^k_{h,2} &:= \sqrt{2 (24 \BoundFunction + 21) (\iota'(\delta^k_h))^2 },
    \\
    \iota'(\delta^k_h) &:=
    \sqrt{ 2 \log \frac{ \mathcal{N} \mathcal{N}_b (2 \log (18 \BoundFunction K) +2) (\log(18 \BoundFunction)+2) }{\delta^k_h} }, \quad \delta^k_h := \frac{\delta}{(K+1)(H+1)}.
\end{align*}
%
%
For all $(k,h) \in [K] \times [H]$, the unweighted regression for fitting second-moment function values to item-level $Q$-functions, and their version space, is as follows:
%
\begin{align}
    \hat{g}^k_{h} 
    &\in \argmin_{g_h \in \mathcal{F}_h} \sum_{\tau=1}^{k-1} \left( g_h(s^\tau_h, a^\tau_h) - \left(r^\tau_h + V^k_{h+1, 1}(s^\tau_{h+1}) \right)^2 \right)^2, \label{eq:second_regression_app}
    \\
    \mathcal{G}^k_{h} 
    &:= \left\{ g_h \in \mathcal{F}_h : \sum_{\tau=1}^{k-1} \left( g_h(s^\tau_h, a^\tau_h) - \hat{g}^k_{h}(s^\tau_h, a^\tau_h) \right)^2  \leq \left( \bar{\beta}^k_{h} \right)^2 \right\}. \label{eq:CI_second_app}
\end{align}
%
We choose the parameters as follows:
\begin{align*}
    \bar{\beta}^k_h &:= \sqrt{8(11 \BoundFunction + 9 ) (\iota''(\delta^k_h))^2 },
    \\
    \iota''(\delta^k_h)
    &:= \sqrt{ 2 \log \frac{ \mathcal{N} \mathcal{N}_b (2 \log (32 \BoundFunction K) +2) (\log(32 \BoundFunction)+2) }{\delta} }, \quad \delta^k_h := \frac{\delta}{(K+1)(H+1)}.
\end{align*}
%

Given the center of the constructed confidence intervals, $\hat{f}^k_{h,j}$ for $j = 1, \pm 2$, we define the optimistic, overly optimistic, and overly pessimistic $\GroundQ$-values as follows:
%
\begin{align*}
    f^k_{h,1}(\cdot,\cdot) &:= \min \left\{ \hat{f}^{k}_{h,1} (\cdot,\cdot) + b^k_{h,1}(\cdot,\cdot), 1 \right\},
    \\
    f^k_{h,2}(\cdot,\cdot) &:= \min \left\{ \hat{f}^{k}_{h,2} (\cdot,\cdot) + 2 b^k_{h,1}(\cdot,\cdot) +  b^k_{h,2}(\cdot,\cdot)  , 1 \right\},
    \\
    f^k_{h,-2}(\cdot,\cdot) &:= \max \left\{ \hat{f}^{k}_{h,-2} (\cdot,\cdot) -  b^k_{h,2}(\cdot,\cdot)  , 0\right\}.
    \numberthis 
    \label{eq:app_notation_groundQ_estimates}
\end{align*}
%

\textbf{Good events.} We define the following good events:
%
\begin{align*}
    \EventMNL &:= \left\{\forall k \geq 1, \forall h \in [H]:  \thetab^\star_h \in \mathcal{C}^k_h \right\},
    \numberthis \label{eq:good_event_MNL}
    \\
    \Event_{\leq K} &:= \mcap_{k=1}^K \mcap_{h=1}^H \Event^k_h, 
    \\
    \Event^k_h &:= \Event^k_{h,1} \mcap \Event^k_{h,2} \mcap \Event^k_{h,-2} \mcap \bar{\Event}^k_h,
\end{align*}
where $\Event^k_{h,j} := 
    \left\{ \mathcal{T}_h V^k_{h+1,1} \in \mathcal{F}^k_{h,j} \right\}$ for $j=1, \pm2$, and $\bar{\Event}^k_h := \left\{ \mathcal{T}^2_h V^k_{h+1,1} \in \mathcal{G}^k_h \right\}$.

%%%%%%%%%%%%%%%
\textbf{Optimistic $Q$-values.}
For $(k,h,s,A) \in [K] \times [H] \times \SetOfStates \times \SetOfActions$ and for $j = 1, \pm 2$, we define the optimistic choice probability as follows:
%
\begin{align*}
    \widetilde{\ChoiceProbability}^k_{h,j}(a | s, A) 
    := 
    \begin{cases}
        &\dfrac{ \exp\left(  \OptUtil^k_{h}(s,a) \right)  }{  \sum_{a' \in A  } \exp\left(  \OptUtil^k_{h}(s,a') \right) }, 
        \quad \text{if} \,\, \exists a \in \SetOfGroundActions \setminus \{\OutsideOption\} \,\,\text{s.t.} \,\,  f^k_{h,j}(s,a) \geq f^k_{h,j}(s,\OutsideOption) 
        \\
        &\dfrac{ \exp\left( \PessiUtil^k_{h}(s,a) \right)  }{  \sum_{a' \in A  } \exp\left(  \PessiUtil^k_{h}(s,a') \right) }, \quad \text{otherwise},
    \end{cases}
    \numberthis \label{eq:app_notation_opt-choice}
\end{align*}
%
where 
%
\begin{align*}
    \OptUtil^k_h(s,a) \!:= \phi(s,a)^\top \thetab^k_h + \alpha^k_h \| \phi(s,a) \|_{\left(\Hb^k_h\right)^{-1} },
    \,\,\,
    \PessiUtil^k_h(s,a) \!:= \phi(s,a)^\top \thetab^k_h - \alpha^k_h \| \phi(s,a) \|_{\left(\Hb^k_h\right)^{-1} }.
\end{align*}
%
%
Next, we define the optimistic $Q$-values for $j = 1, \pm 2$, each constructed using $f^k_{h,j}$ and $\widetilde{\ChoiceProbability}^k_{h,j}$:
%
\begin{align*}
    Q^k_{h,j}(s, A) 
    = \sum_{a \in A  } \widetilde{\ChoiceProbability}^k_{h,j}(a | s, A) f^k_{h,j}(s,a),
    \quad
    V^k_{h, j}(s) 
    = \max_{A \in \SetOfActions} Q^k_{h,j}(s, A).
\end{align*}
%
For convenience, we also define the \textit{realized} optimistic value functions at $(k,h) \in [K] \times [H]$ as follows:
%
\begin{equation} \label{eq:optimistic_Q}
    Q^k_{h} (s, A) :=  
    \begin{cases}
        Q^k_{h,1}(s, A) \quad &\text{if}\,\, A^k_h = A^k_{h,1},
        \\
        Q^k_{h,2}(s, A) \quad &\text{otherwise},
    \end{cases}   
    \quad
    V^k_h(s) = \max_{A \in \SetOfActions} Q^k_{h} (s, A),
\end{equation}
%
where $A^k_h$ is the assortment offered to the user by the assortment selection rule in~\eqref{eq:exploration_policy} (or equivalently in~\eqref{eq:exploration_policy_app}).
Therefore, we write $\pi^k_h (s^k_h) = \argmax_{A \in \SetOfActions} Q^k_{h} (s^k_h, A)$.

%%%%%%%%%%%%%%%
\textbf{Design of exploration policy.}
At each episode $k$, the agent collects data using both $A^k_{h,1}$ and $A^k_{h,2}$, where $A^k_{h,j} \in \argmax_{A \in \SetOfActions}  \sum_{a \in A  } \widetilde{\ChoiceProbability}^k_{h,j}(a | s^k_h, A) f^k_{h,j}(s^k_h,a)$ for $j = 1,2$.
Given a sequence of pre-specified $\{u_k \}_{k=1}^K$, at episode $k$, the agent select an assortment based on the following rule:
%
\begin{align*}
    A^k_h 
    = \begin{cases}
        A^k_{h,1} \quad &\text{if}\,\, f^k_{h',1}(s^k_{h'}, a_{h'}) \geq f^k_{h',2}(s^k_{h'}, a_{h'}) - u_k, \quad \forall  a_{h'} \in A^k_{h',1}, \forall h' \leq h.
        \\
        A^k_{h,2} \quad  &\text{otherwise},
    \end{cases}
    \numberthis 
    \label{eq:exploration_policy_app}
\end{align*}
%
where 
\begin{align*}
    u_k =
    \BigO\left(
        \frac{
            \sqrt{\log \frac{\mathcal{N}KH}{\nu \delta}}
            \cdot \left( \log \frac{\mathcal{N}\mathcal{N}_b KH}{\nu \delta} \cdot H^{5/2} \sqrt{d_\nu} 
            + \sqrt{k}H \epsilon_b \right)
            +
            dH^{5/2} \log K \log M \sqrt{ \log \frac{\mathcal{N}\mathcal{N}_b KH}{\nu \delta} }
        }{
        \sqrt{k}
        }
         \right).
    \numberthis \label{eq:u_k_def}
\end{align*}
%
We denote $h_k \in [H+1]$ as the (random) horizon at which the agent first begins offering the assortment $A^k_{h,2}$.
More formally, for $h \leq h_k$, the assortment offered is $A^k_h = A^k_{h,1}$, and for $h \geq h_k$, the assortment offered is $A^k_h = A^k_{h,2}$.
We then divide the set of episodes $[K]$ into two disjoint subsets: $\OptEpisode$ and $\OOptEpisode$, such that
%
\begin{align*}
    \OptEpisode := \{ k \in [K] : h_k = H+1  \},
    \quad
    \text{and}
    \quad
    \OOptEpisode := \{ k \in [K] : h_k \leq H  \}.
\end{align*}
%
Later in the proof, we separately bound the regret for each case.
%





\textbf{Other notations.}
Throughout the proof, we use $z= (s,a)$, $z_h = (s_h,a_h)$ and $z^k_h = (s^k_h, a^k_h)$ interchangeably.
%
We sometimes use $\ChoiceProbability_h(\cdot | s, A; \thetab^\star_h)$ instead of $\ChoiceProbability_h(\cdot | s, A)$ to explicitly indicate the dependence on the parameter $\thetab^\star_h$.
%
For simplicity, we denote $\EE_{\TransitionProbability} [ \cdot | s^k_h, a^k_h] = \EE_{s_{h+1} \sim \TransitionProbability_h(\cdot | s^k_h, a^k_h)} [ \cdot | s^k_h, a^k_h]  $, $\mathbb{V}_{\TransitionProbability}[\cdot | s^k_h, a^k_h] =  \mathbb{V}_{s_{h+1} \sim \TransitionProbability_h(\cdot | s^k_h, a^k_h)} [ \cdot | s^k_h, a^k_h] $, and
$\EE_{\ChoiceProbability} [ \cdot | s^k_h, A^k_h] 
         = \EE_{a_{h} \sim \ChoiceProbability_h(\cdot | s^k_h, A^k_h)} [ \cdot | s^k_h, A^k_h]$.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Confidence Intervals and good events}
\label{app:subsec_CI_groundQ}
%
In this subsection, we show that, given the construction of confidence intervals $\mathcal{C}^k_h$ and $\mathcal{F}^k_{h,j}$ for $j=1, \pm 2$, the good events $\EventMNL$ and $\Event_{\leq K}$ occurs with high probability.
%
\begin{proposition}[Online parameter confidence interval, Lemma 1 of~\citealt{lee2024nearly}] \label{prop:online_confidence_set}
    Let $\delta \in (0, 1)$. 
    Under Assumption~\ref{assum:MNL_click_model}, for the confidence interval defined in~\eqref{eq:confidence_set} with 
    \begin{align*}
    \alpha^k_h 
    &= \sqrt{
        2\eta \Bigg( 
        11 \cdot ( 3\log(1+(M+1)k) + B + 2 )
        \log \left( \frac{2 \sqrt{1 + 2k}}{\delta} \right)
        + 2 
        +  \frac{7\sqrt{6}}{6}  d \eta \log \left( 1 + \frac{k+1}{2\lambda}\right) +  2 \Bigg)
         + 4\lambda B^2
     }
     \\
     &+ 
     16 \left( \log\left( \frac{2\sqrt{1+2k}}{\delta} \right) \right)^2 \bigg) 
     +  \frac{7\sqrt{6}}{6}  d \eta \log \left( 1 + \frac{k+1}{2\lambda}\right) +  2 \Bigg)
     + \lambda B^2
     \Bigg]^{1/2} 
     \\&= \mathcal{O}( \sqrt{d} \log k \log  M),
    \end{align*}
    %
    $\eta = \frac{1}{2}\log (M+1) + \BoundMNL +1$ and  $\lambda = 84 \sqrt{2}d \eta$, 
    and for any $h \in [H]$, we have 
    \begin{align*}
        \textup{Pr}[\forall k \geq 1, \thetab^\star_h \in \Ccal^k_h] \geq 1- \delta.
    \end{align*}
\end{proposition}
%
%
Now, we define the good event for the preference model $\EventMNL$ as follows:
%
\begin{align*}
    \EventMNL := \left\{\forall k \geq 1, \forall h \in [H]:  \thetab^\star_h \in \mathcal{C}^k_h \right\}.
    % \numberthis \label{eq:good_event_MNL}
\end{align*}
%
Then, by applying proposition~\ref{prop:online_confidence_set} and using a union bound over $h \in [H]$, we obtain the following corollary:
%
\begin{corollary} [Good event for MNL preference model] \label{corollary:good_event_MNL}
    Under the same assumption and settings as in Proposition~\ref{prop:online_confidence_set},
    for $\delta \in (0,1)$, with probability at least  $1- \delta$, the good event for the preference model $\EventMNL$ happens, i.e., $\thetab^\star_h \in \mathcal{C}^k_h $ for all $k \geq 1$ and all $h \in [H]$.
\end{corollary}
%
The following proposition shows that $\mathcal{T}_h V^k_{h+1, j}$ for $j = 1, \pm 2$, and $\mathcal{T}^2_h V^k_{h+1,1}$ lie within their respective confidence intervals.
%
\begin{proposition} [Good event for general functions, Proposition 33 of~\citealt{agarwal2023vo}] \label{prop:good_event}
    Suppose Algorithm~\ref{alg:main} uses a consistent bonus oracle satisfying Definition~\ref{def:bonus_oracle}.
    Let $\delta \in  (0, 1/5)$.
    Then, with probability at least $1- 5\delta$, the good event $\Event_{\leq K} = \mcap_{k=1}^K \mcap_{h=1}^H \Event^k_h$ happens, that is, $\mathcal{T}_h V^k_{h+1, 1} \in \mathcal{F}^k_{h, 1}$, $\mathcal{T}_h V^k_{h+1, \pm 2} \in \mathcal{F}^k_{h, \pm 2}$, and $\mathcal{T}^2_h V^k_{h+1, 1} \in \mathcal{G}^k_{h}$ for all $(k,h) \in [K] \times [H]$.
\end{proposition}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bound for MNL Preference Model}
\label{subsec:bound_mnl}
%
In this subsection, we provide proofs for several properties of the MNL preference model.

The following lemma presents both the optimistic and pessimistic utilities.
%
\begin{lemma} \label{lemma:utility}
    For any $(k,h,s,a) \in [K] \times [H]\times \SetOfStates \times \SetOfGroundActions$,
    let $\OptUtil^k_h(s,a):= \phi(s,a)^\top \thetab^k_h + \alpha^k_h \| \phi(s,a) \|_{\left(\Hb^k_h\right)^{-1} }$
    and $
    \PessiUtil^k_h(s,a) = \phi(s,a)^\top \thetab^k_h - \alpha^k_h \| \phi(s,a) \|_{\left(\Hb^k_h\right)^{-1} }$.    
    Under the good event $\EventMNL$ defined in~\eqref{eq:good_event_MNL}, it holds that
    \begin{align*}
        0 
        &\leq \OptUtil^k_h(s,a) - \phi(s, a)^\top \thetab^\star_h
        \leq 2 \alpha^k_h \| \phi(s,a) \|_{\left(\Hb^k_h\right)^{-1} },
        \\
        \text{and} \,\,
        0
        &\leq \phi(s, a)^\top \thetab^\star_h -  \PessiUtil^k_h(s,a) 
        \leq 
         2 \alpha^k_h \| \phi(s,a) \|_{\left(\Hb^k_h\right)^{-1} }.
    \end{align*}
    %
    \begin{proof} [Proof of Lemma~\ref{lemma:utility}]
        Conditioning on the good event $\EventMNL$ holds, we have
        %
        \begin{align*}
            \left|  \phi(s, a)^\top \thetab^k_h -  \phi(s, a)^\top \thetab^\star_h
            \right|
            \leq \left\| \phi(s, a) \right\|_{\left(\Hb^k_h\right)^{-1}}
            \left\| \thetab^k_h - \thetab^\star_h \right\|_{\Hb^k_h} 
            \leq \alpha^k_h \left\| \phi(s, a) \right\|_{\left(\Hb^k_h\right)^{-1}},
        \end{align*}
        %
        where the first inequality holds by Hölder's inequality, and the last inequality holds by Corollary~\ref{corollary:good_event_MNL}.
        Therefore, it follows that
        %
        \begin{align*}
            \OptUtil^k_h(s,a) - \phi(s, a)^\top \thetab^\star_h
            &= \phi(s,a)^\top \thetab^k_h 
            - \phi(s, a)^\top \thetab^\star_h
            + \alpha^k_h \| \phi(s,a) \|_{\left(\Hb^k_h\right)^{-1} }
            \\
            &\leq 2 \alpha^k_h \left\| \phi(s, a) \right\|_{\left(\Hb^k_h\right)^{-1}}.
        \end{align*}
        %
        Furthermore, since $ \phi(s, a)^\top \thetab^k_h -  \phi(s, a)^\top \thetab^\star_h \geq
        - \alpha^k_h \left\| \phi(s, a) \right\|_{\left(\Hb^k_h\right)^{-1}}$, we also get
        %
        \begin{align*}
            \OptUtil^k_h(s,a) - \phi(s, a)^\top \thetab^\star_h
            &= \phi(s,a)^\top \thetab^k_h 
            - \phi(s, a)^\top \thetab^\star_h
            + \alpha^k_h \| \phi(s,a) \|_{\left(\Hb^k_h\right)^{-1} }
            \geq 0.
        \end{align*}
        %
        The second statement directly follows from the results mentioned above.
    \end{proof}
\end{lemma}

Lemma~\ref{lemma:tilde_choice} is useful for proving optimism (Lemma~\ref{lemma:optimism}) and bounding the approximation error of the optimistic $\GroundQ$ (Lemma~\ref{lemma:opt_approx_error}).
%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lemma} \label{lemma:tilde_choice}
    For all $(k,h,s,A) \in [K] \times [H] \times  \SetOfStates \times \SetOfActions$ and any $j \in \{1,2 \}$,
    under the good event $\EventMNL$ defined in~\eqref{eq:good_event_MNL},
    there exists a subset $\tilde{A} \subseteq A$
    such that $\tilde{A} \in \SetOfActions$ and
    \begin{align*}
            \max\left\{ \sum_{a \in A} \ChoiceProbability_h (a | s,A) f^k_{h,j}(s,a),
            \sum_{a \in A} \widetilde{\ChoiceProbability}^k_{h,j'} (a | s,A) f^k_{h,j}(s,a) \right\}
            \leq
            \sum_{a \in \tilde{A}} \widetilde{\ChoiceProbability}^k_{h,j} (a | s,\tilde{A}) f^k_{h,j}(s,a) 
            ,        
    \end{align*}  
    where $j' \neq j$.
    %
    \begin{proof}[Proof of Lemma~\ref{lemma:tilde_choice}]
        First, recall that, without loss of generality, we assumed that $\phi(s, \OutsideOption) = 0$ for all $s \in \SetOfStates$.
        Therefore, the true preference model and the optimistic preference model can be written as 
        \begin{align*}
            \ChoiceProbability_h(a | s, A) 
            &= 
            \frac{\exp\left( \phi(s,a)^\top \thetab^\star_h \right) }{ 1+ \sum_{a' \in A\setminus \{\OutsideOption\} } \exp\left( \phi(s,a')^\top \thetab^\star_h \right) },
            \\
            \widetilde{\ChoiceProbability}^k_{h,j} (a | s,A) &=
            \begin{cases}
            &\dfrac{ \exp\left(  \OptUtil^k_{h}(s,a) \right)  }{ 1 + \sum_{a' \in A\setminus \{\OutsideOption\}   } \exp\left(  \OptUtil^k_{h}(s,a') \right) }, 
            \quad \text{if} \,\, \exists a \in \SetOfGroundActions \setminus \{\OutsideOption\} \,\,\text{s.t.} \,\,  f^k_{h,j}(s,a) \geq f^k_{h,j}(s,\OutsideOption) 
            \\
            & \dfrac{ \exp\left( \PessiUtil^k_{h}(s,a) \right)  }{ 1+ \sum_{a' \in A\setminus \{\OutsideOption\}   } \exp\left(  \PessiUtil^k_{h}(s,a') \right) }, \quad \text{otherwise}.
        \end{cases}
        \numberthis \label{eq:MNL_prob_one}
        \end{align*}    
        %
        Fix $s \in \SetOfStates$ and $A \in \SetOfActions$.
        We now present the proof by considering two cases: (i) $f^k_{h,j}(s,\OutsideOption) >  f^k_{h,j}(s,a)$ for all $a \in A$ and  (ii) $\exists a \in A \setminus \{\OutsideOption\}$  such that  $f^k_{h,j}(s,\OutsideOption) \leq  f^k_{h,j}(s,a)$.
        
        \textbf{Case (i) } $f^k_{h,j}(s,\OutsideOption) >  f^k_{h,j}(s,a)$ for all $a \in A$.
        \\
        We denote $\tilde{a} \in \argmax_{a \in A \setminus \{\OutsideOption\}} f^k_{h,j}(s,a)$.
        Let $\tilde{A} = \{ \OutsideOption, \tilde{a} \}$.
        Since $f^k_{h,j}(s,\OutsideOption) >  f^k_{h,j}(s,a)$ for all $a \in A$ and $\OutsideOption$ is always included in $A$, removing any item $a \in A \setminus \{\OutsideOption\}$ from $A$ increases the expected value of $f^k_{h,j}$.
        Thus, we get
        %
        \begin{align*}
            \sum_{a \in A} \ChoiceProbability_h (a | s,A) f^k_{h,j}(s,a)
            &\leq \sum_{a \in \tilde{A}} \ChoiceProbability_h (a | s,\tilde{A}) f^k_{h,j}(s,a)
            \\ \text{and} \,\,
            \sum_{a \in A} \widetilde{\ChoiceProbability}^k_{h,j'} (a | s,A) f^k_{h,j}(s,a)
            &\leq \sum_{a \in \tilde{A}}  \widetilde{\ChoiceProbability}^k_{h,j'} (a | s,\tilde{A}) f^k_{h,j}(s,a).
            \numberthis \label{eq:tilde_choice_case1_1}
        \end{align*}
        %
        By the definition of $\widetilde{\ChoiceProbability}^k_{h,j}$ in~\eqref{eq:optimistic_choice}, we use the pessimistic utility $\PessiUtil^k_{h}(s,a)$ in this case.
        %
        Since $ \PessiUtil^k_{h}(s,a) 
        \leq \phi(s, a)^\top \thetab^\star_h
        $ by Lemma~\ref{lemma:utility}, using this utility, $\PessiUtil^k_{h}(s,a)$, reduces the probability of selecting $\tilde{a}$ (compared to the true choice probability $\ChoiceProbability_h$).
        Moreover, we know that $f^k_h(s, \OutsideOption)\geq f^k_h(s,\tilde{a})$, 
        we have
        %
        \begin{align*}
            \sum_{a \in \tilde{A}} \ChoiceProbability_h (a | s,\tilde{A}) f^k_{h,j}(s,a)
            \leq \sum_{a \in \tilde{A}} \widetilde{\ChoiceProbability}^k_{h,j}(a | s,\tilde{A}) f^k_{h,j}(s,a).
            \numberthis \label{eq:tilde_choice_case1_2}
        \end{align*}
        %
        Furthermore, if $\widetilde{\ChoiceProbability}^k_{h,j'}$ is constructed using the pessimistic utility $\PessiUtil^k_{h}(s,a)$, then, $\widetilde{\ChoiceProbability}^k_{h,j'} = \widetilde{\ChoiceProbability}^k_{h,j}$.
        However, if $\widetilde{\ChoiceProbability}^k_{h,j'}$ is constructed using the optimistic utility $\OptUtil^k_{h}(s,a)$, replacing  $\OptUtil^k_{h}(s,a)$ with $\PessiUtil^k_{h}(s,a)$ 
        (which is equivalent to replacing $\widetilde{\ChoiceProbability}^k_{h,j'}$
        with $\widetilde{\ChoiceProbability}^k_{h,j}$
        ) 
        decreases the   probability of choosing $\tilde{a}$, meaning increase the expected value of $f^k_{h,j}$.
        Thus, we get
        %
        \begin{align*}
            \sum_{a \in \tilde{A}}  \widetilde{\ChoiceProbability}^k_{h,j'} (a | s, \tilde{A}) f^k_{h,j}(s,a)
            \leq \sum_{a \in \tilde{A}} \widetilde{\ChoiceProbability}^k_{h,j} (a | s,\tilde{A}) f^k_{h,j}(s,a).\numberthis \label{eq:tilde_choice_case1_3}
        \end{align*}
        %
        Combining \eqref{eq:tilde_choice_case1_1}, \eqref{eq:tilde_choice_case1_2}, and~\eqref{eq:tilde_choice_case1_3}, we have
        %
        \begin{align*}
             \max\left\{ \sum_{a \in A} \ChoiceProbability_h (a | s,A) f^k_{h,j}(s,a),
            \sum_{a \in A} \widetilde{\ChoiceProbability}^k_{h,j'} (a | s,A) f^k_{h,j}(s,a) \right\}
            \leq
            \sum_{a \in \tilde{A}} \widetilde{\ChoiceProbability}^k_{h,j} (a | s,\tilde{A}) f^k_{h,j}(s,a) 
            .
        \end{align*}
        %

        \textbf{Case (ii) } $\exists a \in A \setminus \{\OutsideOption\}$  such that  $f^k_{h,j}(s,\OutsideOption) \leq  f^k_{h,j}(s,a)$.
        \\
        Let $\tilde{A} = \left\{ A' \subseteq A :  f^k_{h,j}(s,a) \geq f^k_{h,j}(s,\OutsideOption),   \forall a \in A'   \right\}$.
        Note that $|\tilde{A}| \geq 2$ and $\OutsideOption \in \tilde{A}$ by definition of action space $\SetOfActions$.
        By selecting $\tilde{A}$ instead of $A$, we exclude items with the small values of $f^k_{h,j}(s,a)$, thereby increasing the expected value of $f^k_{h,j}$, i.e.,
        %
        \begin{align*}
            \sum_{a \in A} \ChoiceProbability_h (a | s,A) f^k_{h,j}(s,a)
            &\leq \sum_{a \in \tilde{A}} \ChoiceProbability_h (a | s,\tilde{A}) f^k_{h,j}(s,a)
            \\ \text{and} \,\,
            \sum_{a \in A} \widetilde{\ChoiceProbability}^k_{h,j'} (a | s,A) f^k_{h,j}(s,a)
            &\leq \sum_{a \in \tilde{A}}  \widetilde{\ChoiceProbability}^k_{h,j'} (a | s,\tilde{A}) f^k_{h,j}(s,a).
            \numberthis \label{eq:tilde_choice_case2_1}           
        \end{align*}
        %
        By the definition of $\widetilde{\ChoiceProbability}^k_{h,j}$ in~\eqref{eq:optimistic_choice}, we use the optimistic utility $\OptUtil^k_{h}(s,a)$ in this case.
        Since $ \phi(s, a)^\top \thetab^\star_h
        \leq \OptUtil^k_{h}(s,a)$ by Lemma~\ref{lemma:utility}, this choice of utility increases the   probability of choosing item $a \neq \tilde{A} \setminus \{\OutsideOption\}$
        compared to the true  $\ChoiceProbability_h$), implying that
        %
        \begin{align*}
            \sum_{a \in \tilde{A}} \ChoiceProbability_h (a | s,\tilde{A}) f^k_{h,j}(s,a)
            \leq \sum_{a \in \tilde{A}} \widetilde{\ChoiceProbability}^k_{h,j}(a | s,\tilde{A}) f^k_{h,j}(s,a).
            \numberthis \label{eq:tilde_choice_case2_2}
        \end{align*}
        %
        Moreover, if $\widetilde{\ChoiceProbability}^k_{h,j'}$ is constructed using
        the pessimistic utility $\PessiUtil^k_{h}(s,a)$, replacing  $\PessiUtil^k_{h}(s,a)$ with $\OptUtil^k_{h}(s,a)$ 
        (which is equivalent to replacing $\widetilde{\ChoiceProbability}^k_{h,j'}$
        with $\widetilde{\ChoiceProbability}^k_{h,j}$
        )
        increases the   probability of choosing item $a \neq \tilde{A} \setminus \{\OutsideOption\}$.
        However, if $\widetilde{\ChoiceProbability}^k_{h,j'}$ is constructed using the optimistic utility $\OptUtil^k_{h}(s,a)$, we have $\widetilde{\ChoiceProbability}^k_{h,j} = \widetilde{\ChoiceProbability}^k_{h,j'}$.
        To this end, we get
        %
        \begin{align*}
            \sum_{a \in \tilde{A}}  \widetilde{\ChoiceProbability}^k_{h,j'} (a | s,\tilde{A}) f^k_{h,j}(s,a)
            \leq \sum_{a \in \tilde{A}} \widetilde{\ChoiceProbability}^k_{h,j} (a | s,\tilde{A}) f^k_{h,j}(s,a).\numberthis \label{eq:tilde_choice_case2_3}
        \end{align*} 
        Combining \eqref{eq:tilde_choice_case2_1}, \eqref{eq:tilde_choice_case2_2}, and~\eqref{eq:tilde_choice_case2_3}, we have
        %
        \begin{align*}
            \max\left\{ \sum_{a \in A} \ChoiceProbability_h (a | s,A) f^k_{h,j}(s,a),
            \sum_{a \in A} \widetilde{\ChoiceProbability}^k_{h,j'} (a | s,A) f^k_{h,j}(s,a) \right\}
            \leq \sum_{a \in \tilde{A}} \widetilde{\ChoiceProbability}^k_{h,j} (a | s,\tilde{A}) f^k_{h,j}(s,a) 
            .
        \end{align*}
        %
        This concludes the proof of Lemma~\ref{lemma:tilde_choice}.
    \end{proof}
\end{lemma}
%%%%%%%%%%%%%%%%%%%%%%%%
%
Denote $J(k,h) \in \{1,2\}$ as the chosen index of function $f^k_{h,j}$ at $(k,h)$.
Then, we show that the value estimates for the chosen assortment, $f^k_{h,J(k,h)}(s^k_h, a)$ for all $a \in A^k_h$, are greater than equal to $\sum_{a \in A^k_h} \ChoiceProbability^k_{h,J(k,h)}(a | s, A^k_h)f^k_{h, J(k,h)}(s^k_h,a)$.
%
\begin{lemma} \label{lemma:r_geq_R}
    For any $(k,h) \in [K] \times [H]$, let  $J(k,h):\! \mathcal{K} \times [H] \!\rightarrow\! \{1,2\}$ be the one-to-one function that maps from $\mathcal{K} \times [H]$ to the index set $\{1,2\}$ such that $A^k_h = A^k_{h, J(k,h)} \in  \argmax_{A \in \SetOfActions}  \sum_{a \in A  } \widetilde{\ChoiceProbability}^k_{h,J(k,h)}(a | s^k_h, A) f^k_{h, J(k,h)}(s^k_h,a)$.
    Then, under the good event $\EventMNL$ defined in~\eqref{eq:good_event_MNL}, we have
    \begin{align*}
         f^k_{h,J(k,h)}(s^k_h, a)
         \geq \sum_{a \in A^k_h} \ChoiceProbability_{h}(a | s^k_h, A^k_h)f^k_{h, J(k,h)}(s^k_h,a),
         \quad \forall a \in A^k_h.
    \end{align*}
\end{lemma}
%
\begin{proof} [Proof of Lemma~\ref{lemma:r_geq_R}]
    By Lemma~\ref{lemma:tilde_choice}, there exists a subset $\tilde{A} \subseteq A^k_h$ and $\tilde{A} \in \SetOfActions$ such that 
    %
    
    \begin{align*}
        \sum_{a \in A^k_h} \ChoiceProbability_{h}(a | s^k_h, A^k_h)f^k_{h, J(k,h)}(s^k_h,a)
        &\leq
        \sum_{a \in \tilde{A}} \widetilde{\ChoiceProbability}^k_{h,J(k,h)} (a | s^k_h,\tilde{A}) f^k_{h,J(k,h)}(s^k_h,a) 
        \\
        &\leq \sum_{a \in A^k_h} \widetilde{\ChoiceProbability}^k_{h,J(k,h)} (a | s^k_h, A^k_h) f^k_{h,J(k,h)}(s^k_h,a) 
        ,        
    \end{align*}  
    where the second inequality holds the assortment selection rule.
    %
    Thus, it is sufficient to show that $f^k_{h,J(k,h)}(s^k_h, a) \geq \sum_{a \in A^k_h} \widetilde{\ChoiceProbability}^k_{h,J(k,h)} (a | s^k_h, A^k_h) f^k_{h,J(k,h)}(s^k_h,a)$ for all $a \in A^k_h$.
    
    We prove this by contradiction. 
    Suppose there exists an item $a \in A^k_h$ such that 
    \begin{align*}
        f^k_{h,J(k,h)}(s^k_h, a) < \sum_{a \in A^k_h} \widetilde{\ChoiceProbability}^k_{h,J(k,h)} (a | s^k_h, A^k_h) f^k_{h,J(k,h)}(s^k_h,a).
    \end{align*}
    %
    If we remove item $a$ from the assortment $A^k_h$,
    it would result in higher value. 
    This contradicts the optimality of $A^k_h$.
    Hence, we conclude
    %
    \begin{align*}
        f^k_{h,J(k,h)}(s^k_h, a) \geq
        \sum_{a \in A^k_h} \widetilde{\ChoiceProbability}^k_{h,J(k,h)} (a | s^k_h, A^k_h) f^k_{h,J(k,h)}(s^k_h,a),
    \end{align*}
    %
    which completes the proof.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%
Lemma~\ref{lemma:elliptical} is an elliptical potential lemma used for bounding the regret incurred from the MNL preference model (Lemma~\ref{lemma:crude_mnl} and~\ref{lemma:tight_mnl}).
%
\begin{lemma} [Elliptical potential lemma, Lemma E.2 and H.3 of~\citealt{lee2024nearly}]  \label{lemma:elliptical}
Assume that $\lambda \geq 2$ and $\phi(s, \OutsideOption) = 0$ for all $s \in \SetOfStates$.
For any $(k,h,a) \in [K] \times [H] \times \SetOfGroundActions$, we define $\widetilde{\phi}(s^k_h, a) = \phi(s^k_h, a) - \EE_{ a' \sim \ChoiceProbability_h \left(\cdot | s^k_h, A^k_h ; \thetab^{k+1}_h \right) }[\phi(s^k_h, a)]$.
Then, 
for $\Hb^k_h$ defined in~\eqref{eq:hessian_app}, 
and for any $h \in [H]$, the following statements hold true:
%
\begin{align*}
    \sum_{\tau=1}^{k} \sum_{a \in A^\tau_h}\! \ChoiceProbability_h\! \left(a | s^\tau_h, A^\tau_h; \thetab^{\tau+1}_h \right)
    \ChoiceProbability_h \!\left(\OutsideOption | s^\tau_h, A^\tau_h; \thetab^{\tau+1}_h \right)
     \| \phi(s^\tau_h, a) \|_{ \left(\Hb^\tau_h\right)^{-1}}^2
    &\leq  2d \log \left( 1+ \frac{k}{d \lambda} \right),
    \\
    \sum_{\tau=1}^{k} \sum_{a \in A^\tau_h}\! \ChoiceProbability_h\! \left(a | s^\tau_h, A^\tau_h; \thetab^{\tau+1}_h \right) 
     \| \widetilde{\phi}(s^\tau_h, a) \|_{ \left(\Hb^\tau_h\right)^{-1}}^2
    &\leq  2d \log \left( 1+ \frac{k}{d \lambda} \right),
    \\
    \sum_{\tau=1}^{k}
    \max\left\{  \max_{a \in A^\tau_h}   \| \phi(s^\tau_h, a) \|_{ \left(\Hb^\tau_h\right)^{-1}}^2
    ,  \max_{a \in A^\tau_h}  \| \widetilde{\phi}(s^\tau_h, a) \|_{ \left(\Hb^\tau_h\right)^{-1}}^2
    \right\}
    &\leq \frac{2}{\kappa} d \log \left( 1+ \frac{k}{d \lambda} \right).
\end{align*}
\end{lemma}
%
%%%%%%%%%%%%%%%%%%%%%%%%
Lemma~\ref{lemma:second_pd} is used to derive the tight bound for the second-order regret term of the MNL preference model (Lemma\ref{lemma:tight_mnl}).
%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lemma} [Lemma E.3 of~\citealt{lee2024nearly}]
\label{lemma:second_pd}
Let $M \in \Zb^{+}$.
Define $R:\RR^M \rightarrow \RR$, such that for any $\upsilonb = (\upsilon_1, \dots, \upsilon_M) \in \RR^M$, $R(\upsilonb) = \sum_{m=1}^M \frac{\exp(\upsilon_m)}{1 + \sum_{l=1}^M \exp(\upsilon_l)}$.
Let $p_m(\upsilonb) = \frac{\exp(\upsilon_m)}{1 + \sum_{l=1}^M \exp(\upsilon_l)}$.
Then, for all $m \in [M]$, we have
%
\begin{align*}
    \left| \frac{\partial^2 R}{\partial m \partial n} \right|
    \leq
    \begin{cases}
        3 p_m(\upsilonb) & \text{if} \,\,\, m=n,
        \\
        2p_m(\upsilonb) p_n(\upsilonb) & \text{if} \,\,\, m \neq n.
    \end{cases}
\end{align*}
\end{lemma}
%%%%%%%%%%%%%%%%%%%%%%%%

Lemma~\ref{lemma:more_extreme_utility} is crucial for deriving the $\kappa$-improved bound for the MNL preference model (Lemma\ref{lemma:tight_mnl}), enabling the analysis.
%
\begin{lemma} [Overly optimistic choice probability] \label{lemma:more_extreme_utility}
    We define 
    \begin{align*}
    \dbtilde{\ChoiceProbability}^k_{h,j}(a | s, A) 
    = 
    \begin{cases}
        \dfrac{ \exp\left(  \phi(s, a)^\top \thetab^\star_h + 2 \alpha^k_h \| \phi(s,a) \|_{\left(\Hb^k_h\right)^{-1} } \right)  }
        {  \sum_{a' \in A  } \exp\left(   \phi(s, a')^\top \thetab^\star_h + 2 \alpha^k_h \| \phi(s,a') \|_{\left(\Hb^k_h\right)^{-1} } \right) }, 
        \; 
        &\text{if} \,\, \exists a \in \SetOfGroundActions \setminus \{\OutsideOption\} \,\, \text{s.t.}
        \vspace{-0.3cm}
        \\
        &f^k_{h,j}(s,a) \geq f^k_{h,j}(s,\OutsideOption), 
        \\
        \dfrac{ \exp\left( \phi(s, a)^\top \thetab^\star_h - 2 \alpha^k_h \| \phi(s,a) \|_{\left(\Hb^k_h\right)^{-1} } \right)  }{  \sum_{a' \in A  } \exp\left(  \phi(s, a')^\top \thetab^\star_h - 2 \alpha^k_h \| \phi(s,a') \|_{\left(\Hb^k_h\right)^{-1} } \right) }, \; &\text{otherwise}.
    \end{cases}
    \numberthis \label{eq:dtilde_opt_choice}
    \end{align*}
    %
    Let $A^k_{h,j} \in  \argmax_{A \in \SetOfActions}  \sum_{a \in A  } \widetilde{\ChoiceProbability}^k_{h,j}(a | s^k_h, A) f^k_{h, j}(s^k_h,a)$, where $j \in \{1,2\}$.
    Then, under the good event $\EventMNL$, for all $(k,h,j) \in [K] \times [H] \times \{ 1,2 \}$, we have
    %
    \begin{align*}
        \sum_{a \in A^k_{h,j}  } \widetilde{\ChoiceProbability}^k_{h,j}(a | s^k_h, A^k_{h,j}) f^k_{h, j}(s^k_h,a) 
        \leq \sum_{a \in A^k_{h,j}  } \dbtilde{\ChoiceProbability}^k_{h,j}(a | s^k_h, A^k_{h,j}) f^k_{h, j}(s^k_h,a).
    \end{align*}
    %
    \begin{proof} [Proof of Lemma~\ref{lemma:more_extreme_utility}]
        Fix $(k,h,j) \in [K] \times [H] \times \{ 1,2 \}$.
        We consider the two cases: (i) $f^k_{h,j}(s^k_h,\OutsideOption) >  f^k_{h,j}(s^k_h,a)$ for all $a \in \SetOfGroundActions$ and  (ii) $\exists a \in \SetOfGroundActions \setminus \{\OutsideOption\}$  such that  $f^k_{h,j}(s^k_h,\OutsideOption) \leq  f^k_{h,j}(s^k_h,a)$.

        \textbf{Case (i) } $f^k_{h,j}(s^k_h,\OutsideOption) >  f^k_{h,j}(s^k_h,a)$ for all $a \in \SetOfGroundActions$.
        \\
        Recall that, by the definition of $\widetilde{\ChoiceProbability}^k_{h,j}$ in~\eqref{eq:optimistic_choice}, we use the pessimistic utility $\PessiUtil^k_{h}(s,a)$ to construct $\widetilde{\ChoiceProbability}^k_{h,j}$ in this case.
        Note that the outside option $\OutsideOption$ must be included in the assortment, i.e.,  $\OutsideOption \in A^k_h$.
        Moreover, under the event $\EventMNL$, by Lemma~\ref{lemma:utility}, we have
        %
        \begin{align*}
             \PessiUtil^k_h(s^k_h, a) 
        \geq 
         \phi(s^k_h, a)^\top \thetab^\star_h - 2 \alpha^k_h \| \phi(s^k_h,a) \|_{\left(\Hb^k_h\right)^{-1} }.
        \end{align*}
        %
        Thus, since we assume, without loss of generality, that $\phi(s^k_h, \OutsideOption) = 0$ (refer~\eqref{eq:MNL_prob_one}), using $\dbtilde{\ChoiceProbability}^k_{h,j}$ instead of $\widetilde{\ChoiceProbability}^k_{h,j}$ decreases the probability of choosing any item $a \in A^k_h \setminus \{\OutsideOption\}$.
        As a result, the expected value of $f^k_{h,j}$ increases, since $f^k_{h,j}(s, \OutsideOption) \geq f^k_{h,j}(s, a)$ for all $a \in A^k_h$.
        Formally, we have 
        \begin{align*}
            \sum_{a \in A^k_{h,j}  } \widetilde{\ChoiceProbability}^k_{h,j}(a | s^k_h, A^k_{h,j}) f^k_{h, j}(s^k_h,a) 
        \leq \sum_{a \in A^k_{h,j}  } \dbtilde{\ChoiceProbability}^k_{h,j}(a | s^k_h, A^k_{h,j}) f^k_{h, j}(s^k_h,a).
        \end{align*}
        
        \textbf{Case (ii) } $\exists a \in \SetOfGroundActions \setminus \{\OutsideOption\}$  such that  $f^k_{h,j}(s^k_h,\OutsideOption) \leq  f^k_{h,j}(s^k_h,a)$.
        \\
        First, we show that for all $a \in A^k_h \setminus \{\OutsideOption\}$, we have $f^k_{h,j} (s^k_h, a) \geq \sum_{a \in A^k_{h,j}  } \widetilde{\ChoiceProbability}^k_{h,j}(a | s^k_h, A^k_{h,j}) f^k_{h, j}(s^k_h,a) $.
        Suppose that there exists  $a \in A^k_h \setminus \{\OutsideOption\}$ for which $f^k_{h,j} (s^k_h, a) < \sum_{a \in A^k_{h,j}  } \widetilde{\ChoiceProbability}^k_{h,j}(a | s^k_h, A^k_{h,j}) f^k_{h, j}(s^k_h,a) $.
        Then, removing item $a$ from the assortment $A^k_h$ results in the increase in the expected value of $f^k_{h,j}$.
        Consequently, this contradicts the optimality of $A^k_h$.
        Hence, we get
        %
        \begin{align*}
            f^k_{h,j} (s^k_h, a) \geq \sum_{a \in A^k_{h,j}  } \widetilde{\ChoiceProbability}^k_{h,j}(a | s^k_h, A^k_{h,j}) f^k_{h, j}(s^k_h,a), \quad \forall a \in A^k_h \setminus \{\OutsideOption\}.
        \end{align*}
        %
        On the other hand, recall that, by the definition of $\widetilde{\ChoiceProbability}^k_{h,j}$ in~\eqref{eq:optimistic_choice}, we use the pessimistic utility $\OptUtil^k_{h}(s,a)$ to construct $\widetilde{\ChoiceProbability}^k_{h,j}$ in this case.
        Furthermore, by Lemma~\ref{lemma:utility}, we know that
        %
        \begin{align*}
             \OptUtil^k_h(s^k_h,a) 
        \leq 
         \phi(s^k_h, a)^\top \thetab^\star_h + 2 \alpha^k_h \| \phi(s^k_h,a) \|_{\left(\Hb^k_h\right)^{-1} }.
        \end{align*}
        %
        If we increase $\OptUtil^k_h(s^k_h,a) $ to $\phi(s^k_h, a)^\top \thetab^\star_h + 2 \alpha^k_h \| \phi(s^k_h,a) \|_{\left(\Hb^k_h\right)^{-1} }$ for all $a \in A^k_h \setminus \{\OutsideOption\}$, the probability of choosing the outside option decreases (because $\phi(s^k_h, \OutsideOption) = 0$).
        In other words, the sum of probabilities of choosing $a \in A^k_h \setminus \{\OutsideOption\}$ increases.
        Since $f^k_{h,j} (s^k_h, a) \geq \sum_{a \in A^k_{h,j}  } \widetilde{\ChoiceProbability}^k_{h,j}(a | s^k_h, A^k_{h,j}) f^k_{h, j}(s^k_h,a)$ for all $a \in A^k_h \setminus \{\OutsideOption\}$, the expected value of $f^k_{h,j}$ increases.
        Formally, we get 
        \begin{align*}
            \sum_{a \in A^k_{h,j}  } \widetilde{\ChoiceProbability}^k_{h,j}(a | s^k_h, A^k_{h,j}) f^k_{h, j}(s^k_h,a) 
        \leq \sum_{a \in A^k_{h,j}  } \dbtilde{\ChoiceProbability}^k_{h,j}(a | s^k_h, A^k_{h,j}) f^k_{h, j}(s^k_h,a).
        \end{align*}
        %
        This concludes the proof.
    \end{proof}
    %
\end{lemma}
%%%%%%%%%%%%%%%%%%%%%%%%

Lemma~\ref{lemma:crude_mnl} will be used to carefully bound the sum of $b^k_{h,1}$ (Lemma~\ref{lemma:b_1_fine_grained_bound}).
Note that the following MNL bandit regret improves upon the one proposed in~\citep{oh2021multinomial} by a factor of $1/\sqrt{\kappa}$, which can be exponentially large.
%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lemma}[Crude bound for MNL bandits] \label{lemma:crude_mnl}
    For any $h \in [H]$, $j= \{1, 2, -2\}$ and subset $\mathcal{K} \in [K]$, under the good event $\EventMNL$ defined in~\eqref{eq:good_event_MNL}, we have
    \begin{align*}
        \sum_{k \in \mathcal{K}}
        \left|
             \sum_{a \in A^k_{h}} \!\!\! \left(\widetilde{\ChoiceProbability}^k_{h,j}(a|s^k_{h}, A^k_{h})  - \ChoiceProbability_{h}(a|s^k_{h}, A^k_{h}) \right) f^k_{h, j}(s^k_{h}, a)
        \right|
             \leq \BigO \left(   \frac{1}{\sqrt{\kappa}} 
             d \sqrt{ |\mathcal{K}|} 
            \cdot (\log K)^{3/2} \log M
            \right),
    \end{align*}
    %
    where $M$ is the maximum size of the assortment.
    %
    \begin{proof} [Proof of Lemma~\ref{lemma:crude_mnl}]
        We denote $M^k_h$ as the size of the assortment at horizon $h$ in episode $k$, i.e., $M^k_h = |A^k_h|$. 
        For any $j \in \{1, 2, -2\}$, we define a function $R_j: \RR^{M^k_h} \rightarrow \RR$ such that, for all $\upsilonb \in \RR^{M^k_h}$, $R_j(\upsilonb) = \sum^{M^k_h}_{m=1} \frac{\exp(\upsilon_m) f^k_{h,j}(s^k_h, a_{i_m}) }{1 + \sum_{l=1}^{M^k_h} \exp \left( \upsilon_l\right) }$.
        %

        For simplicity, we denote $\upsilon^k_{h,j}(s,a)$ as the utility, which can represent either the optimistic utility $\OptUtil^k_{h}(s,a)$ or the pessimistic utility $\PessiUtil^k_h(s,a)$, as determined by~\eqref{eq:optimistic_choice}, depending on $f^k_{h,j}$.
        Let $\upsilonb^k_{h,j} (s^k_h) 
        =\left(\upsilon^k_{h,j}(s^k_h, a) \right)_{a \in A^k_h} \in \RR^{M^k_h} $
        and
        $\upsilonb^\star_h(s^k_h) = 
        \left(\phi(s^k_h, a)^\top \thetab^\star_h \right)_{a \in A^k_h}  \in \RR^{M^k_h}$.
        Then, by the mean value theorem, there exists a vector $\bar{\upsilonb}^k_{h,j}(s^k_h)$, which is a convex combination of $\upsilonb^k_{h,j}(s^k_h)$ and $\upsilonb^\star_h(s^k_h)$, such that 
        %
        \begin{align*}
            \sum_{k \in \mathcal{K}}
            \left|
                \sum_{a \in A^k_{h}} \!\!\! \left(\widetilde{\ChoiceProbability}^k_{h,j}(a|s^k_{h}, A^k_{h})  - \ChoiceProbability_{h}(a|s^k_{h}, A^k_{h}) \right) f^k_{h, j}(s^k_{h}, a)
            \right|
            % \\
            &=  \sum_{k \in \mathcal{K}}
            \left|
                R_j \left(\upsilonb^k_{h,j} (s^k_h)\right)
                - R_j  \left(\upsilonb^\star_h(s^k_h)\right)
            \right|
            \\
            &=  \sum_{k \in \mathcal{K}}
            \left|
              \nabla  R_j \left(\bar{\upsilonb}^k_{h,j}(s^k_h)\right)^\top
               \left( \upsilonb^k_{h,j}(s^k_h) - \upsilonb^\star_h(s^k_h) \right)
            \right|.
        \end{align*}
        %
        Therefore, we get
        %
        \begin{align*}
            &\sum_{k \in \mathcal{K}}
            \left|
              \nabla  R_j \left(\bar{\upsilonb}^k_{h,j}(s^k_h)\right)^\top
               \left( \upsilonb^k_{h,j}(s^k_h) - \upsilonb^\star_h(s^k_h) \right)
            \right|
            \\
            &= \sum_{k \in \mathcal{K}}
            \Bigg|
                \sum_{a \in A^k_h} \frac{\exp\left(\bar{\upsilon}^k_{h,j}(s^k_h,a) \right)f^k_{h, j}(s^k_{h}, a) }{ \sum_{a'' \in A^k_h} \exp\left(\bar{\upsilon}^k_{h,j}(s^k_h,a'') \right) }
                \left( \upsilon^k_{h,j}(s^k_h, a) - \phi(s^k_h,a)^\top \thetab^\star_h \right)
            \\    
            &- \sum_{a \in A^k_h} \sum_{a' \in A^k_h} 
                \frac{ \exp\left(\bar{\upsilon}^k_{h,j}(s^k_h,a) \right)f^k_{h, j}(s^k_{h}, a) \exp\left(\bar{\upsilon}^k_{h,j}(s^k_h,a') \right)}{\left( \sum_{a'' \in A^k_h} \exp\left(\bar{\upsilon}^k_{h,j}(s^k_h,a'') \right) \right)^2} \left( \upsilon^k_{h,j}(s^k_h, a') - \phi(s^k_h,a')^\top \thetab^\star_h \right)
            \Bigg|
            \\
            &= \sum_{k \in \mathcal{K}}\Bigg|
                \sum_{a \in A^k_h} \ChoiceProbability_{h}\left(a|s^k_{h}, A^k_{h}; \bar{\upsilonb}^k_{h,j}(s^k_h)\right)
                \left( \upsilon^k_{h,j}(s^k_h, a) - \phi(s^k_h,a)^\top \thetab^\star_h \right)
                \\
                &\quad \quad \quad \quad
                \cdot\bigg(f^k_{h, j}(s^k_{h}, a) 
                    - \EE_{a' \sim  \ChoiceProbability_{h}\left(\cdot|s^k_{h}, A^k_{h}; \bar{\upsilonb}^k_{h,j}(s^k_h)\right)} \left[ f^k_{h, j}(s^k_{h}, a') \right]
                    \bigg)
            \Bigg|
            \\
            &\leq 
            2  \sum_{k \in \mathcal{K}}
                \sum_{a \in A^k_h}  \ChoiceProbability_{h}\left(a|s^k_{h}, A^k_{h}; \bar{\upsilonb}^k_{h,j}(s^k_h)\right)
               \left| \left( \upsilon^k_{h,j}(s^k_h, a) - \phi(s^k_h,a) \right)^\top \thetab^\star_h \right|
            \numberthis \label{eq:crude_MNL_1}
            ,
        \end{align*}
        %
        where the inequality is from $f^k_{h,j} \leq 1$.
        Recall that  $\upsilon^k_{h,j}(s^k_h,a)$ can be either  $\OptUtil^k_{h}(s^k_h,a)$ or  $\PessiUtil^k_h(s^k_h,a)$.
        Then, by Lemma~\ref{lemma:utility}, we have $\left|( \upsilon^k_{h,j}(s^k_h,a) - \phi(s^k_h,a)^\top \thetab^\star_h \right| \leq 2\alpha^k_h \| \phi(s^k_h, a) \|_{\left( \Hb^k_h \right)^{-1}}$.
        Hence, we can further bound the right-hand side of~\eqref{eq:crude_MNL_1}.
        %
        \begin{align*}
            &2  \sum_{k \in \mathcal{K}}
                \sum_{a \in A^k_h}  \ChoiceProbability_{h}\left(a|s^k_{h}, A^k_{h}; \bar{\upsilonb}^k_{h,j}(s^k_h)\right)
                \left| \left( \upsilon^k_{h,j}(s^k_h, a) - \phi(s^k_h,a) \right)^\top \thetab^\star_h \right|
            \\
            &\leq 4    \alpha^K_h \sum_{k \in \mathcal{K}}
                \sum_{a \in A^k_h}  \ChoiceProbability_{h}\left(a|s^k_{h}, A^k_{h}; \bar{\upsilonb}^k_{h,j}(s^k_h)\right)
                 \| \phi(s^k_h, a) \|_{\left( \Hb^k_h \right)^{-1}}
            \\
            &\leq 4    \alpha^K_h 
            \sqrt{ \sum_{k \in \mathcal{K}} \sum_{a \in A^k_h}  \ChoiceProbability_{h}\big(a|s^k_{h}, A^k_{h}; \bar{\upsilonb}^k_{h,j}(s^k_h)\big) } 
            \sqrt{\sum_{k \in \mathcal{K}} \sum_{a \in A^k_h}  \ChoiceProbability_{h}\big( a|s^k_{h}, A^k_{h}; \bar{\upsilonb}^k_{h,j}(s^k_h)\big)
                 \| \phi(s^k_h, a) \|_{\left( \Hb^k_h \right)^{-1}}^2 }
            \\
            &\leq  4    \alpha^K_h \sqrt{ |\mathcal{K}|}
            \cdot \sqrt{\sum_{k=1}^K \sum_{a \in A^k_h} 
                 \| \phi(s^k_h, a) \|_{\left( \Hb^k_h \right)^{-1}}^2 }
            \\
            &=  4    \alpha^K_h \sqrt{ |\mathcal{K}|}
            \cdot \sqrt{\sum_{k=1}^K \sum_{a \in A^k_h} 
            \frac{\ChoiceProbability_{h}\big(a|s^k_{h}, A^k_{h}; \thetab^{k+1}_h\big)
            \ChoiceProbability_{h}\big(\OutsideOption|s^k_{h}, A^k_{h}; \thetab^{k+1}_h\big)}{\ChoiceProbability_{h}\big(a|s^k_{h}, A^k_{h}; \thetab^{k+1}_h\big)
            \ChoiceProbability_{h}\big(\OutsideOption|s^k_{h}, A^k_{h}; \thetab^{k+1}_h\big)}
                 \| \phi(s^k_h, a) \|_{\left( \Hb^k_h \right)^{-1}}^2 }
            \\
            &\leq 4    \alpha^K_h \sqrt{ |\mathcal{K}|}
            \cdot \sqrt{\frac{1}{\kappa} \cdot \sum_{k=1}^K \sum_{a \in A^k_h} 
            \ChoiceProbability_{h}\big(a|s^k_{h}, A^k_{h}; \thetab^{k+1}_h\big)
            \ChoiceProbability_{h}\big(\OutsideOption|s^k_{h}, A^k_{h}; \thetab^{k+1}_h\big)
                 \| \phi(s^k_h, a) \|_{\left( \Hb^k_h \right)^{-1}}^2 }
            \\
            &\leq 4    \alpha^K_h \sqrt{ |\mathcal{K}|}
            \cdot \sqrt{\frac{1}{\kappa} \cdot 2 d \log \left(1 + \frac{K}{d \lambda} \right)}  
            \numberthis \label{eq:crude_MNL_2}
            ,
        \end{align*}
        %
        where the first inequality holds because $\alpha^1_h \leq \dots \leq \alpha^K_h$,
        the second inequality follows from the Cauchy-Schwarz inequality, 
        the second-to-the last inequality holds due to the definition of $\kappa$,
        and the last inequality holds by Lemma~\ref{lemma:elliptical}.

        Combining~\eqref{eq:crude_MNL_1} and~\eqref{eq:crude_MNL_2}, and plugging in the value of $\alpha^k_h$, we derive that
        %
        \begin{align*}
            \sum_{k \in \mathcal{K}}
            \left|
                \sum_{a \in A^k_{h}} \!\!\! \left(\widetilde{\ChoiceProbability}^k_{h,j}(a|s^k_{h}, A^k_{h})  - \ChoiceProbability_{h}(a|s^k_{h}, A^k_{h}) \right) f^k_{h, j}(s^k_{h}, a)
            \right|
            = \BigO \left(   \frac{1}{\sqrt{\kappa}} 
             d \sqrt{ |\mathcal{K}|} 
            \cdot (\log K)^{3/2} \log M
            \right).
        \end{align*}
        %
    \end{proof}
\end{lemma}
%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
\begin{lemma}
    \label{lemma:gap_p_theta1_theta2}
    For any $(k,h) \in [K] \times [H]$, $\thetab_1, \thetab_2 \in \Ccal^k_h$, and $\omega^k_h(a) \geq 0$,  under the event $\EventMNL$ defined in~\eqref{eq:good_event_MNL}, we have
    \begin{align*}
        \sum_{a \in A^k_h} 
        \left|
            \ChoiceProbability_h(a | s^k_h, A^k_h; \thetab_1) 
            - \ChoiceProbability_h(a | s^k_h, A^k_h; \thetab_1) 
        \right|
        \omega^k_h(a)
        \leq 
         4 \alpha^k_h
        \max_{a \in A^k_h} \omega^k_h(a)
        \max_{a \in A^k_h} 
        \| \phi(s^k_h,a) \|_{(\Hb^k_h)^{-1}}.
    \end{align*}
\end{lemma}
%
\begin{proof}[Proof of Lemma~\ref{lemma:gap_p_theta1_theta2}]
    By the mean value theorem, there exists $\xib = (1-c) \thetab_1 + c \thetab_2$ for some $c \in (0,1)$ such that
    %
    \begin{align*}
        &\sum_{a \in A^k_h} 
        \left|
            \ChoiceProbability_h(a | s^k_h, A^k_h; \thetab_1) 
            - \ChoiceProbability_h(a | s^k_h, A^k_h; \thetab_1) 
        \right|
        \omega^k_h(a)
        = 
        \sum_{a \in A^k_h} 
        \left|
            \nabla \ChoiceProbability_h(a | s^k_h, A^k_h; \xib)^\top
            (\thetab_1 - \thetab_2)
        \right|
        \omega^k_h(a)
        \\
        &=  \sum_{a \in A^k_h} 
        \left|
             \left(
             \ChoiceProbability_h(a | s^k_h, A^k_h; \xib) \phi(s^k_h, a)
             - \ChoiceProbability_h(a | s^k_h, A^k_h; \xib)
             \sum_{a' \in A^k_h} 
             \ChoiceProbability_h(a' | s^k_h, A^k_h; \xib)
              \phi(s^k_h, a')
             \right)^\top
            (\thetab_1 - \thetab_2)
        \right|
        \omega^k_h(a)
        \\
        &\leq 
        \sum_{a \in A^k_h}
            \ChoiceProbability_h(a | s^k_h, A^k_h; \xib) 
        \left|
                \phi(s^k_h, a)^\top (\thetab_1 - \thetab_2)
        \right|
        \omega^k_h(a)
        % \\
        + \sum_{a \in A^k_h}
            \ChoiceProbability_h(a | s^k_h, A^k_h; \xib) \omega^k_h(a)
        \sum_{a' \in A^k_h}
        \ChoiceProbability_h(a' | s^k_h, A^k_h; \xib) 
        \left|
                \phi(s^k_h, a)^\top (\thetab_1 - \thetab_2)
        \right|
        \\
        &\leq 2 \alpha^k_h 
        \sum_{a \in A^k_h}
            \ChoiceProbability_h(a | s^k_h, A^k_h; \xib) 
        \|\phi(s^k_h, a) \|_{(\Hb^k_h)^{-1}}
        \omega^k_h(a)
        + \max_{a \in A^k_h}  \omega^k_h(a)
         \sum_{a \in A^k_h}
            \ChoiceProbability_h(a | s^k_h, A^k_h; \xib) 
        \|\phi(s^k_h, a) \|_{(\Hb^k_h)^{-1}}
        \\
        &\leq 4 \alpha^k_h
        \max_{a \in A^k_h}  \omega^k_h(a)
        \max_{a \in A^k_h} \|\phi(s^k_h, a) \|_{(\Hb^k_h)^{-1}},
    \end{align*}
    where the second-to-last inequality holds  under the good event $\EventMNL$ defined in~\eqref{eq:good_event_MNL}.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
Lemma~\ref{lemma:TVL} pertains to the \textit{law of total variance}~\citep{lattimore2012pac, gheshlaghi2013minimax} that the variance of the value function is smaller than its magnitude by a factor $\sqrt{H}$.
%
\begin{lemma} [Total variance lemma, Lemma C.5 of~\citealt{jin2018qlearning}]
    \label{lemma:TVL}
    Let $f^k_{h,j} \in [0,1]$.
    Then, with probability at least $1-\delta$, we have
    %
    \begin{align*}
        \sum_{k=1}^K \sum_{h=1}^H 
        [\VV_h f^k_{h,j}](s^k_h)
        = \BigO \left( K + H \log (1/\delta) \right).
    \end{align*}
\end{lemma}
%
Lemma~\ref{lemma:tight_mnl} is crucial for obtaining a $\kappa$-independent regret in our leading term. 
While the proof is largely inspired by~\citet{lee2024nearly}, extending their result to our setting is non-trivial because the unknown item values $f^k_{h,j}$ 
 add complexity to the analysis.
Moreover, thanks to Lemma~\ref{lemma:TVL}, we can obtain a tight bound by a factor of $\sqrt{H}$, instead of naively summing over $H$ MNL bandit regrets.
%
\begin{lemma}[$\kappa$-improved bound for MNL bandits] \label{lemma:tight_mnl}
    For any subset $\mathcal{K} \in [K]$, let  $J(k,h):\! \mathcal{K} \times [H] \!\rightarrow\! \{1,2\}$ be the one-to-one function that maps from $\mathcal{K} \times [H]$ to the index set $\{1,2\}$ such that $A^k_h = A^k_{h, J(k,h)} \in  \argmax_{A \in \SetOfActions}  \sum_{a \in A  } \widetilde{\ChoiceProbability}^k_{h,J(k,h)}(a | s^k_h, A) f^k_{h, J(k,h)}(s^k_h,a)$.
    Then, under the good event $\EventMNL$ defined in~\eqref{eq:good_event_MNL}, with probability at least $1-\delta$, we have
    \begin{align*}
        \sum_{k \in \mathcal{K}}\sum_{h=1}^H
             \sum_{a \in A^k_{h}} &\!\!\! \left(\widetilde{\ChoiceProbability}^k_{h,J(k,h)}(a|s^k_{h}, A^k_{h})  - \ChoiceProbability_{h}(a|s^k_{h}, A^k_{h}) \right) f^k_{h, J(k,h)}(s^k_{h}, a)
             \\
             &= \BigO\left(
                d\sqrt{ H |\mathcal{K}|} 
                (\log K)^{3/2}
                \log M
                +
                \frac{1}{\kappa}
                d^2 H 
                (\log K)^3
                (\log M)^2
            \right).
    \end{align*}
    %
    \begin{proof} [Proof of Lemma~\ref{lemma:tight_mnl}]
        %
        We begin by defining $\dbtilde{\ChoiceProbability}^k_{h,j}(a | s, A)$ as given in~\ref{eq:dtilde_opt_choice}.
        Then, by Lemma~\ref{lemma:more_extreme_utility}, we have
        %
        \begin{align*}
            \sum_{k \in \mathcal{K}}
            \sum_{h=1}^H
             &\sum_{a \in A^k_{h}} \!\!\! \left(\widetilde{\ChoiceProbability}^k_{h,J(k,h)}(a|s^k_{h}, A^k_{h})  - \ChoiceProbability_{h}(a|s^k_{h}, A^k_{h}) \right) f^k_{h, J(k,h)}(s^k_{h}, a)
             \\
             &\leq \sum_{k \in \mathcal{K}}
             \sum_{h=1}^H
             \sum_{a \in A^k_{h}} \!\!\! \left(\dbtilde{\ChoiceProbability}^k_{h,J(k,h)}(a|s^k_{h}, A^k_{h})  - \ChoiceProbability_{h}(a|s^k_{h}, A^k_{h}) \right) f^k_{h, J(k,h)}(s^k_{h}, a).
        \end{align*}
        %
        We denote $M^k_h$ as the size of the assortment at horizon $h$ in episode $k$, i.e., $M^k_h = |A^k_h|$. 
        We define a function $\tilde{R}: \RR^{M^k_h} \rightarrow \RR$ such that for all $\upsilonb \in \RR^{M^k_h}$, $\tilde{R}(\upsilonb) = \sum^{M^k_h}_{m=1} \frac{\exp(\upsilon_m) f^k_{h,J(k,h)}(s^k_h, a_{i_m}) }{1 + \sum_{l=1}^{M^k_h} \exp \left( \upsilon_l\right) }$.
        %

        For any $(k,h) \in \mathcal{K} \times [H]$ and all $a \in \SetOfGroundActions$, we denote $\upsilon^k_{h}(s^k_h,a)$ as the utility, which can be either $ \phi(s^k_h, a)^\top \thetab^\star_h + 2 \alpha^k_h \| \phi(s^k_h,a) \|_{\left(\Hb^k_h\right)^{-1} } $ or  $\phi(s^k_h, a)^\top \thetab^\star_h - 2 \alpha^k_h \| \phi(s^k_h,a) \|_{\left(\Hb^k_h\right)^{-1} }$, determined deterministically based on the history up to $(k,h)$:
        %
        \begin{align*}
             \upsilon^k_{h}(s^k_h,a)
             = 
             \begin{cases}
                \phi(s^k_h, a)^\top \thetab^\star_h + 2 \alpha^k_h \| \phi(s^k_h,a) \|_{\left(\Hb^k_h\right)^{-1} }, 
                \quad 
                &\text{if} \,\, \exists a \in \SetOfGroundActions \setminus \{\OutsideOption\} 
                \,\,\text{s.t.} \,\,  f^k_{h,J(k,h)}(s^k_h,a) \geq f^k_{h,J(k,h)}(s^k_h,\OutsideOption) 
                \\
                \phi(s^k_h, a)^\top \thetab^\star_h - 2 \alpha^k_h \| \phi(s^k_h,a) \|_{\left(\Hb^k_h\right)^{-1} }, \quad  
                &\text{if} \,\, \forall a \in \SetOfGroundActions \setminus \{\OutsideOption\} 
                \,\,  f^k_{h,J(k,h)}(s^k_h,a) < f^k_{h,J(k,h)}(s^k_h,\OutsideOption).
             \end{cases}
        \end{align*}
        %
        Let $\upsilonb^k_{h} (s^k_h) 
        =\left(\upsilon^k_{h}(s^k_h, a) \right)_{a \in A^k_h} \in \RR^{M^k_h} $
        and
        $\upsilonb^\star_h(s^k_h) = 
        \left(\phi(s^k_h, a)^\top \thetab^\star_h \right)_{a \in A^k_h}  \in \RR^{M^k_h}$.
        Thanks to exact second-order Taylor expansion, we obtain that
        %
        \begin{align*}
            &\sum_{k \in \mathcal{K}}\sum_{h=1}^H
             \sum_{a \in A^k_{h}} \!\!\! \left(\widetilde{\ChoiceProbability}^k_{h,J(k,h)}(a|s^k_{h}, A^k_{h})  - \ChoiceProbability_{h}(a|s^k_{h}, A^k_{h}) \right) f^k_{h, J(k,h)}(s^k_{h}, a)
             %
             \\
            &= \sum_{k \in \mathcal{K}}\sum_{h=1}^H
                \tilde{R}(\upsilonb^k_{h}(s^k_h)) - \tilde{R}(\upsilonb^\star_h(s^k_h))
            \\\
            &= \underbrace{\sum_{k \in \mathcal{K}}\sum_{h=1}^H
                \nabla  \tilde{R}(\upsilonb^\star_h(s^k_h))^\top \left( \upsilonb^k_{h}(s^k_h) - \upsilonb^\star_h(s^k_h) \right)
                }_{\texttt{(A)}}
            + \underbrace{\frac{1}{2} \sum_{k \in \mathcal{K}}\sum_{h=1}^H \left( \upsilonb^k_{h}(s^k_h) - \upsilonb^\star_h(s^k_h) \right)^\top 
                \nabla^2 \tilde{R}(\bar{\upsilonb}^k_{h}(s^k_h))
                \left( \upsilonb^k_{h}(s^k_h) - \upsilonb^\star_h(s^k_h) \right)}_{\texttt{(B)}}
            \numberthis \label{eq:MNL_improved_regret_decomposition}
            ,
        \end{align*}
        %
        where $\bar{\upsilonb}^k_{h}(s^k_h)= 
        \left(\bar{\upsilon}^k_{h}(s^k_h, a) \right)_{a \in A^k_h} \in \RR^{M^k_h} $  is the convex combination of $\upsilonb^k_{h}(s^k_h)$ and $\upsilonb^\star_h(s^k_h)$. 

        We first bound the term \texttt{(A)} in~\eqref{eq:MNL_improved_regret_decomposition}.
        \begin{align*}
            &\sum_{k \in \mathcal{K}}\sum_{h=1}^H
                \nabla  \tilde{R}(\upsilonb^\star_h(s^k_h))^\top \left( \upsilonb^k_{h}(s^k_h) - \upsilonb^\star_h(s^k_h) \right)
            \\
            &= \sum_{k \in \mathcal{K}} \sum_{h=1}^H
            \Bigg(
                \sum_{a \in A^k_h} \frac{\exp\left(\phi(s^k_h, a)^\top \thetab^\star_h \right)f^k_{h, J(k,h)}(s^k_{h}, a) }{ \sum_{a'' \in A^k_h} \exp\left(\phi(s^k_h, a'')^\top \thetab^\star_h\right) }
                \left( \upsilon^k_{h}(s^k_h, a) - \phi(s^k_h,a)^\top \thetab^\star_h \right)
            \\    
            &\quad\quad\quad - \sum_{a \in A^k_h} \sum_{a' \in A^k_h} 
                \frac{ \exp\left(\phi(s^k_h, a)^\top \thetab^\star_h \right)f^k_{h, J(k,h)}(s^k_{h}, a) \exp\left(\phi(s^k_h, a')^\top \thetab^\star_h\right)}{\left( \sum_{a'' \in A^k_h} \exp\left(\phi(s^k_h, a'')^\top \thetab^\star_h \right) \right)^2} \left( \upsilon^k_{h}(s^k_h, a') - \phi(s^k_h,a')^\top \thetab^\star_h \right)
                \Bigg)
            \\
            &= \sum_{k \in \mathcal{K}} \sum_{h=1}^H
                \sum_{a \in A^k_h} \ChoiceProbability_{h}\left(a|s^k_{h}, A^k_{h}; \thetab^\star_h\right)
                f^k_{h, J(k,h)}(s^k_{h}, a) 
            \\
            &\quad\quad\quad 
            \cdot \bigg(
                   \left( \upsilon^k_{h}(s^k_h, a) - \phi(s^k_h,a)^\top \thetab^\star_h \right)
            - \sum_{a' \in A^k_h}\ChoiceProbability_{h}\left(a'|s^k_{h}, A^k_{h}; \thetab^\star_h\right)
                    \left( \upsilon^k_{h}(s^k_h, a') - \phi(s^k_h,a')^\top \thetab^\star_h \right)
                \bigg).
            \numberthis \label{eq:tight_MNL_first}
        \end{align*}
        %
        We bound the right-hand side of~\eqref{eq:tight_MNL_first} by examining two separate cases.
        %
        For any fixed $h \in [H]$, let $\mathcal{K}_{(i)}$ denote the set of episodes where \textbf{Case (i)} holds, and $\mathcal{K}_{(ii)}$ denote the set of episodes where \textbf{Case (ii)} holds.
        More formally, we define:
        \begin{align*}
            \mathcal{K}_{(i)} 
            &= \left\{ k \in \mathcal{K} :  \upsilon^k_{h}(s^k_h,a) = \phi(s^k_h, a)^\top \thetab^\star_h + 2 \alpha^k_h \| \phi(s^k_h,a) \|_{\left(\Hb^k_h\right)^{-1} }  \right\}
            \tag{\textbf{Case (i)}}
            \\
            \mathcal{K}_{(ii)} 
            &= \left\{ k \in \mathcal{K} :  \upsilon^k_{h}(s^k_h,a) = \phi(s^k_h, a)^\top \thetab^\star_h - 2 \alpha^k_h \| \phi(s^k_h,a) \|_{\left(\Hb^k_h\right)^{-1} }  \right\}.
            \tag{\textbf{Case (ii)}}
        \end{align*}
        %
        
        \textbf{Case (i) } For $(k,h) \in \mathcal{K} \times [H]$ such that $ \upsilon^k_{h}(s^k_h,a) = \phi(s^k_h, a)^\top \thetab^\star_h + 2 \alpha^k_h \| \phi(s^k_h,a) \|_{\left(\Hb^k_h\right)^{-1} } $.
        \\
        %
        Denoting $\EE_{\thetab} [\cdot] = \EE_{a' \sim \ChoiceProbability_h (\cdot | s^k_h, A^k_h ; \thetab) } [\cdot]$ and $\bar{\alpha}_K:= \max_{h \in [H]}\alpha^K_h$ for simplicity, we get
        %
        \begin{align*}
            &\sum_{k \in \mathcal{K}_{(i)}} \sum_{h=1}^H
            \nabla  \tilde{R}(\upsilonb^\star_h(s^k_h))^\top \left( \upsilonb^k_{h}(s^k_h) - \upsilonb^\star_h(s^k_h) \right)
            \\
            &=  \sum_{k \in \mathcal{K}_{(i)}} \sum_{h=1}^H
            2 \alpha^k_h
                \sum_{a \in A^k_h} \ChoiceProbability_{h}\left(a|s^k_{h}, A^k_{h}; \thetab^\star_h\right)
                f^k_{h, J(k,h)}(s^k_{h}, a) 
                \bigg(
                     \| \phi(s^k_h,a) \|_{\left(\Hb^k_h\right)^{-1} } 
                    - \EE_{\thetab^\star_h} \left[\| \phi(s^k_h,a') \|_{\left(\Hb^k_h\right)^{-1} }  \right]
                \bigg)
            \\
            &\leq 2   \bar{\alpha}_K
                \sum_{k \in \mathcal{K}_{(i)}}
                \sum_{h=1}^H
                \sum_{a \in A^k_h} \ChoiceProbability_{h}\left(a|s^k_{h}, A^k_{h}; \thetab^\star_h\right)
                f^k_{h, J(k,h)}(s^k_{h}, a) 
                \bigg(
                     \| \phi(s^k_h,a) \|_{\left(\Hb^k_h\right)^{-1} } 
                    - \EE_{\thetab^\star_h} \left[\| \phi(s^k_h,a') \|_{\left(\Hb^k_h\right)^{-1} }  \right]
                \bigg)
            \\
            &=  2   \bar{\alpha}_K
                \sum_{k \in \mathcal{K}_{(i)}}
                \sum_{h=1}^H
                \EE_{\thetab^\star_h} \left[
                    \left(
                        f^k_{h, J(k,h)}(s^k_{h}, a)
                        - \EE_{\thetab^\star_h} \left[
                        f^k_{h, J(k,h)}(s^k_{h}, a') 
                        \right]
                    \right)
                    \left(
                        \| \phi(s^k_h,a) \|_{\left(\Hb^k_h\right)^{-1} } 
                        - \EE_{\thetab^\star_h} \left[\| \phi(s^k_h,a') \|_{\left(\Hb^k_h\right)^{-1} }  \right]
                    \right)
                \right]
            \\
            &\leq 2   \bar{\alpha}_K
                \sum_{k \in \mathcal{K}_{(i)}}
                \sum_{h=1}^H
                \EE_{\thetab^\star_h} \Bigg[
                    \bigg(
                        \underbrace{f^k_{h, J(k,h)}(s^k_{h}, a)
                        - \EE_{\thetab^\star_h} \left[
                        f^k_{h, J(k,h)}(s^k_{h}, a') 
                        \right]}_{\geq 0}
                    \bigg)
                        \| \phi(s^k_h,a) 
                        - \EE_{\thetab^\star_h} [ \phi(s^k_h,a') ] \|_{\left(\Hb^k_h\right)^{-1} } 
                \Bigg]
            , \numberthis \label{eq:tight_MNL_first_case1}
        \end{align*}
        %
        where, in the first inequality, we use the fact that $\alpha^K_h$ is non-decreasing with respect to $k$, and by Lemma~\ref{lemma:r_geq_R}, we have
        %
        \begin{align*}
            \sum_{a \in A^k_h} &\ChoiceProbability_{h}\left(a|s^k_{h}, A^k_{h}; \thetab^\star_h\right)
                f^k_{h, J(k,h)}(s^k_{h}, a) 
                \bigg(
                     \| \phi(s^k_h,a) \|_{\left(\Hb^k_h\right)^{-1} } 
                    - \EE_{\thetab^\star_h} \left[\| \phi(s^k_h,a') \|_{\left(\Hb^k_h\right)^{-1} }  \right]
                \bigg)
            \\
            &= \sum_{a \in A^k_h} \ChoiceProbability_{h}\left(a|s^k_{h}, A^k_{h}; \thetab^\star_h\right)
                \| \phi(s^k_h,a) \|_{\left(\Hb^k_h\right)^{-1} }
                \bigg(
                    f^k_{h, J(k,h)}(s^k_{h}, a) 
                    - 
                    \underbrace{\EE_{\thetab^\star_h} \left[
                        f^k_{h, J(k,h)}(s^k_{h}, a') 
                    \right]}_{= \sum_{a' \in A^k_h} \ChoiceProbability_{h}(a' | s^k_h, A^k_h)f^k_{h, J(k,h)}(s^k_h,a')}
                \bigg)
            \\
            &\geq 0.
        \end{align*}
        %
        And the last inequality of Equation~\eqref{eq:tight_MNL_first_case1} holds because 
        %
        \begin{align*}
            \| \phi(s^k_h,a) \|_{\left(\Hb^k_h\right)^{-1} } 
                - \EE_{\thetab^\star_h} \left[\| \phi(s^k_h,a') \|_{\left(\Hb^k_h\right)^{-1} }  \right]
            &\leq  \| \phi(s^k_h,a) \|_{\left(\Hb^k_h\right)^{-1} } 
                - \| \EE_{\thetab^\star_h} \left[ \phi(s^k_h,a') \right] \|_{\left(\Hb^k_h\right)^{-1} }  
            \\
            &\leq \| \phi(s^k_h,a) 
                        - \EE_{\thetab^\star_h} [ \phi(s^k_h,a') ] \|_{\left(\Hb^k_h\right)^{-1} },
        \end{align*}
        %
        where the first inequality holds by Jensen's inequality and the last inequality holds due to the fact that $\| \ab \|  = \| \ab - \bb + \bb\| \leq \| \ab-\bb \| + \|\bb\|$ for any vectors $\ab, \bb \in \RR^d$.
        
        We further decompose the right-hand side of~\eqref{eq:tight_MNL_first_case1} as follows:
        %
        \begin{align*}
            &\sum_{k \in \mathcal{K}_{(i)}} \sum_{h=1}^H
                \EE_{\thetab^\star_h} \left[
                    \left(
                        f^k_{h, J(k,h)}(s^k_{h}, a)
                        - \EE_{\thetab^\star_h} \left[
                        f^k_{h, J(k,h)}(s^k_{h}, a') 
                        \right]
                    \right)
                    \| \phi(s^k_h,a) 
                        - \EE_{\thetab^\star_h} [ \phi(s^k_h,a') ] \|_{\left(\Hb^k_h\right)^{-1} }
                \right]
            \\
            &= \sum_{k \in \mathcal{K}_{(i)}}\sum_{h=1}^H
            \sum_{a \in A^k_h} 
                \sqrt{\ChoiceProbability_{h}\left(a|s^k_{h}, A^k_{h}; \thetab^\star_h\right)
                    \ChoiceProbability_{h}\left(a|s^k_{h}, A^k_{h}; \thetab^{k+1}_h\right)
                }
                \left(
                    f^k_{h, J(k,h)}(s^k_{h}, a)
                    - \EE_{\thetab^\star_h} \left[
                    f^k_{h, J(k,h)}(s^k_{h}, a') 
                    \right]
                \right)
                \\
                &\quad\quad\quad\quad\quad\quad\quad  \cdot \| \phi(s^k_h,a) 
                    - \EE_{\thetab^{k+1}_h} [ \phi(s^k_h,a') ] \|_{\left(\Hb^k_h\right)^{-1} }
            \\
            &+ \sum_{k \in \mathcal{K}_{(i)}}\sum_{h=1}^H
            \sum_{a \in A^k_h}
            \left(
                \sqrt{\ChoiceProbability_{h}\left(a|s^k_{h}, A^k_{h}; \thetab^\star_h\right)
                }
                -
                \sqrt{
                    \ChoiceProbability_{h}\left(a|s^k_{h}, A^k_{h}; \thetab^{k+1}_h\right)
                }
            \right)
            \sqrt{\ChoiceProbability_{h}\left(a|s^k_{h}, A^k_{h}; \thetab^\star_h\right)
                }
            \\
            &\quad\quad\quad\quad\quad\quad\quad  \cdot 
            \left(
                    f^k_{h, J(k,h)}(s^k_{h}, a)
                    - \EE_{\thetab^\star_h} \left[
                    f^k_{h, J(k,h)}(s^k_{h}, a') 
                    \right]
                \right)
            \| \phi(s^k_h,a) 
                - \EE_{\thetab^{k+1}_h} [ \phi(s^k_h,a') ] \|_{\left(\Hb^k_h\right)^{-1} }
            \\
            &+ \sum_{k \in \mathcal{K}_{(i)}}\sum_{h=1}^H
            \sum_{a \in A^k_h}
            \ChoiceProbability_{h}\left(a|s^k_{h}, A^k_{h}; \thetab^\star_h\right)
            \left(
                    f^k_{h, J(k,h)}(s^k_{h}, a)
                    - \EE_{\thetab^\star_h} \left[
                    f^k_{h, J(k,h)}(s^k_{h}, a') 
                    \right]
                \right)
            \\
            &\quad\quad\quad\quad\quad\quad\quad  \cdot
            \left(
                \| \phi(s^k_h,a) 
                - \EE_{\thetab^{\star}_h} [ \phi(s^k_h,a') ] \|_{\left(\Hb^k_h\right)^{-1} }
                - 
                \| \phi(s^k_h,a) 
                - \EE_{\thetab^{k+1}_h} [ \phi(s^k_h,a') ] \|_{\left(\Hb^k_h\right)^{-1} }
            \right).
             \numberthis \label{eq:tight_MNL_first_case1_decomposition}
        \end{align*}
        %
        For simplicity,  let
         $\bar{\phi}(s^k_h,a) = \phi(s^k_h,a) 
        -  \EE_{\thetab^\star_h} \left[ \phi(s^k_h,a')  \right] $
        and
        $\widetilde{\phi}(s^k_h,a) = \phi(s^k_h,a) 
        -  \EE_{\thetab^{k+1}_h} \left[ \phi(s^k_h,a')  \right] $.
        %
        Now, we bound the terms on the right-hand side of~\eqref{eq:tight_MNL_first_case1_decomposition} individually.
        For the first term, with probability at least $1-\delta$, we get
        %
        \begin{align*}
            \sum_{k \in \mathcal{K}_{(i)}}&\sum_{h=1}^H
            \sum_{a \in A^k_h} 
                \sqrt{\ChoiceProbability_{h}\left(a|s^k_{h}, A^k_{h}; \thetab^\star_h\right)
                    \ChoiceProbability_{h}\left(a|s^k_{h}, A^k_{h}; \thetab^{k+1}_h\right)
                }
                \left(
                    f^k_{h, J(k,h)}(s^k_{h}, a)
                    - \EE_{\thetab^\star_h} \left[
                    f^k_{h, J(k,h)}(s^k_{h}, a') 
                    \right]
                \right)
                 \| \widetilde{\phi}(s^k_h,a) \|_{\left(\Hb^k_h\right)^{-1} }
            \\
            &\leq \usqrt{ 
                \sum_{k \in \mathcal{K}}  \sum_{h=1}^H
                \ubrace{
                    \sum_{a \in A^k_h} 
                    \ChoiceProbability_{h}\left(a|s^k_{h}, A^k_{h}; \thetab^\star_h\right)
                    \left(
                        f^k_{h, J(k,h)}(s^k_{h}, a)
                        - \EE_{\thetab^\star_h} \left[
                        f^k_{h, J(k,h)}(s^k_{h}, a') 
                        \right]
                    \right)^2
                    }{=: [\VV_h f^k_{h,J(k,h)}](s^k_h)}
                }
            \\
            &\quad\quad\cdot 
                \sqrt{
                \sum_{k \in \mathcal{K}} \sum_{h=1}^H
                \sum_{a \in A^k_h} 
                \ChoiceProbability_{h}\left(a|s^k_{h}, A^k_{h}; \thetab^{k+1}_h\right)
                \| \widetilde{\phi}(s^k_h,a) \|_{\left(\Hb^k_h\right)^{-1} }^2
                }
            \\
            &\leq \sqrt{\sum_{k \in \mathcal{K}} \sum_{h=1}^H [\VV_h f^k_{h,J(k,h)}](s^k_h)}
            \sqrt{2 d H \log \left( 1 + \frac{K}{d \lambda} \right) }
            \\
            &= \BigO\left( \sqrt{ |\mathcal{K}| + H \log (1/\delta) } \right) \cdot 
            \sqrt{2 d H \log \left( 1 + \frac{K}{d \lambda} \right) }
            \numberthis \label{eq:tight_MNL_first_case1_decomposition_frist_bound}
            ,
        \end{align*}
        %
        where the first inequality follows from the Cauchy-Schwarz inequality,
        the second-to-last inequality holds by Lemma~\ref{lemma:elliptical},
        and the last equality holds by Lemma~\ref{lemma:TVL}.
        %
        Additionally, the second term in~\eqref{eq:tight_MNL_first_case1_decomposition} can be bounded as follows:
        %
        \begin{align*}
            &\sum_{k \in \mathcal{K}_{(i)}}\sum_{h=1}^H
            \sum_{a \in A^k_h}
            \left(
                \sqrt{\ChoiceProbability_{h}\left(a|s^k_{h}, A^k_{h}; \thetab^\star_h\right)
                }
                -
                \sqrt{
                    \ChoiceProbability_{h}\left(a|s^k_{h}, A^k_{h}; \thetab^{k+1}_h\right)
                }
            \right)
            \sqrt{\ChoiceProbability_{h}\left(a|s^k_{h}, A^k_{h}; \thetab^\star_h\right)
                }
            \\
            &\quad\quad\quad\quad\quad\quad\quad  \cdot 
            \left(
                    f^k_{h, J(k,h)}(s^k_{h}, a)
                    - \EE_{\thetab^\star_h} \left[
                    f^k_{h, J(k,h)}(s^k_{h}, a') 
                    \right]
                \right)
            \| \widetilde{\phi}(s^k_h,a) 
                  \|_{\left(\Hb^k_h\right)^{-1} }
            \\
            &\leq \sum_{k \in \mathcal{K}_{(i)}}\sum_{h=1}^H
            \sum_{a \in A^k_h}
            \frac{|\ChoiceProbability_{h}\left(a|s^k_{h}, A^k_{h}; \thetab^\star_h\right)
            - \ChoiceProbability_{h}\left(a|s^k_{h}, A^k_{h}; \thetab^{k+1}_h\right)|}{
                \sqrt{\ChoiceProbability_{h}\left(a|s^k_{h}, A^k_{h}; \thetab^\star_h\right)
                }
                +
                \sqrt{
                    \ChoiceProbability_{h}\left(a|s^k_{h}, A^k_{h}; \thetab^{k+1}_h\right)
                }
            }
            \sqrt{\ChoiceProbability_{h}\left(a|s^k_{h}, A^k_{h}; \thetab^\star_h\right)
                }
            \| \widetilde{\phi}(s^k_h,a) 
                  \|_{\left(\Hb^k_h\right)^{-1} }
            \\
            &\leq \sum_{k \in \mathcal{K}_{(i)}}\sum_{h=1}^H
            \sum_{a \in A^k_h}
            |\ChoiceProbability_{h}\left(a|s^k_{h}, A^k_{h}; \thetab^\star_h\right)
            - \ChoiceProbability_{h}\left(a|s^k_{h}, A^k_{h}; \thetab^{k+1}_h\right)|
             \| \widetilde{\phi}(s^k_h,a) 
                  \|_{\left(\Hb^k_h\right)^{-1} }
            \\
            &\leq 4 \bar{\alpha}_K \sum_{k \in \mathcal{K}_{(i)}}\sum_{h=1}^H
            \max_{a \in A^k_h} \| \widetilde{\phi}(s^k_h,a) 
                  \|_{\left(\Hb^k_h\right)^{-1} }
            \max_{a \in A^k_h} \| \phi(s^k_h,a) \|_{\left(\Hb^k_h\right)^{-1} }
            \\
            &\leq 4 \bar{\alpha}_K 
            \sqrt{
                \sum_{k=1}^K\sum_{h=1}^H
            \max_{a \in A^k_h} \| \widetilde{\phi}(s^k_h,a) 
                  \|_{\left(\Hb^k_h\right)^{-1} }^2
            }
            \sqrt{
                 \sum_{k=1}^K\sum_{h=1}^H
                 \max_{a \in A^k_h} \| \phi(s^k_h,a) \|_{\left(\Hb^k_h\right)^{-1} }^2
            }
            \\
            &\leq \frac{4}{\sqrt{\kappa}} \bar{\alpha}_K
            \sqrt{
                \sum_{k=1}^K\sum_{h=1}^H
            \sum_{a \in A^k_h}  
            \ChoiceProbability_h (a | s^k_h, A^k_h, ; \thetab^{k+1}_h)
            \| \widetilde{\phi}(s^k_h,a) 
                  \|_{\left(\Hb^k_h\right)^{-1} }^2
            }
            \sqrt{
                 \sum_{k=1}^K\sum_{h=1}^H
                 \max_{a \in A^k_h} \| \phi(s^k_h,a) \|_{\left(\Hb^k_h\right)^{-1} }^2
            }
            \\
            &\leq \frac{8}{\sqrt{\kappa}} \bar{\alpha}_K
            d H \log \left( 1 + \frac{K}{d \lambda} \right),
            \numberthis \label{eq:tight_MNL_first_case1_decomposition_second_bound}
        \end{align*}
        where the third inequity holds by
        Lemma~\ref{lemma:gap_p_theta1_theta2} and by the definition of $\bar{\alpha}_K = \max_{h \in [H]}\alpha^K_h$,
        the second-to-last inequality holds by the definition of $\kappa$,
        and the last inequality holds by Lemma~\ref{lemma:elliptical}.
        
        Finally, we bound the last term in~\eqref{eq:tight_MNL_first_case1_decomposition}.
        Using the inequality $\| \ab \| -  \|\bb\| \leq \| \ab-\bb \| $ for any vectors $\ab, \bb \in \RR^d$, we have
        %
        \begin{align*}
            &\sum_{k \in \mathcal{K}_{(i)}}\sum_{h=1}^H
            \sum_{a \in A^k_h}
            \ChoiceProbability_{h}\left(a|s^k_{h}, A^k_{h}; \thetab^\star_h\right)
            \bigg(\underbrace{
                    f^k_{h, J(k,h)}(s^k_{h}, a)
                    - \EE_{\thetab^\star_h} \left[
                    f^k_{h, J(k,h)}(s^k_{h}, a') 
                    \right]
                    }_{\geq 0}
                \bigg)
            \left(
                \| \widetilde{\phi}(s^k_h,a) 
                \|_{\left(\Hb^k_h\right)^{-1} }
                - 
                \| \bar{\phi}(s^k_h,a) 
                \|_{\left(\Hb^k_h\right)^{-1} }
            \right)
            \\
            &\leq 
            \sum_{k \in \mathcal{K}_{(i)}}\sum_{h=1}^H
            \sum_{a \in A^k_h}
            \ChoiceProbability_{h}\left(a|s^k_{h}, A^k_{h}; \thetab^\star_h\right)
            \left\| 
                \sum_{a' \in A^k_h}
                \left(
                    \ChoiceProbability_{h}\left(a'|s^k_{h}, A^k_{h}; \thetab^\star_h\right)
                    - \ChoiceProbability_{h}\left(a'|s^k_{h}, A^k_{h}; \thetab^{k+1}_h\right)
                \right)
                \phi(s^k_h, a')
            \right\|_{\left(\Hb^k_h\right)^{-1} }
            \\
            &\leq \sum_{k=1}^K\sum_{h=1}^H
            \sum_{a \in A^k_h}
            \left|
                \ChoiceProbability_{h}\left(a|s^k_{h}, A^k_{h}; \thetab^\star_h\right)
                - \ChoiceProbability_{h}\left(a|s^k_{h}, A^k_{h}; \thetab^{k+1}_h\right)
            \right|
            \|\phi(s^k_h, a) \|_{\left(\Hb^k_h\right)^{-1} }
            \\
            &\leq 4 \bar{\alpha}_K \sum_{k=1}^K\sum_{h=1}^H
            \max_{a \in A^k_h}
            \|\phi(s^k_h, a) \|_{\left(\Hb^k_h\right)^{-1} }^2
            \\
            &\leq \frac{8}{\kappa} \bar{\alpha}_K
            d H \log \left( 1 + \frac{K}{d \lambda} \right),
             \numberthis \label{eq:tight_MNL_first_case1_decomposition_third_bound}
        \end{align*}
        %
        where the third inequity holds by
        Lemma~\ref{lemma:gap_p_theta1_theta2} and by the definition of $\bar{\alpha}_K = \max_{h \in [H]}\alpha^K_h$ 
        and the last inequality holds by Lemma~\ref{lemma:elliptical}.

        By plugging~\eqref{eq:tight_MNL_first_case1_decomposition_frist_bound},~\eqref{eq:tight_MNL_first_case1_decomposition_second_bound}, and~\eqref{eq:tight_MNL_first_case1_decomposition_third_bound} into~\eqref{eq:tight_MNL_first_case1_decomposition}, and ccombining the result with~\eqref{eq:tight_MNL_first_case1}, we obtain
        %
        \begin{align*}
            \sum_{k \in \mathcal{K}_{(i)}} \sum_{h=1}^H
            &\nabla  \tilde{R}(\upsilonb^\star_h(s^k_h))^\top \left( \upsilonb^k_{h}(s^k_h) - \upsilonb^\star_h(s^k_h) \right)
            \\
            &\leq \BigO\left( \sqrt{ |\mathcal{K}| + H \log (1/\delta) } \right)
            \cdot 
             2\bar{\alpha}_K \sqrt{2 d H \log \left( 1 + \frac{K}{d \lambda} \right) }
            + \frac{32}{\kappa} \bar{\alpha}_K^2
            d H \log \left( 1 + \frac{K}{d \lambda} \right)
            \\
            &= \BigO\left(
                d\sqrt{ H |\mathcal{K}|} 
                (\log K)^{3/2}
                \log M
                +
                \frac{1}{\kappa}
                d^2 H 
                (\log K)^3
                (\log M)^2
            \right).
            \numberthis \label{eq:tight_MNL_first_case1_result}
        \end{align*}
        %
        Now, we consider the second case to bound the term \texttt{(A)} in Equation~\eqref{eq:MNL_improved_regret_decomposition}.   

        %%%%%%%%%
        \textbf{Case (ii) } For $(k,h) \in \mathcal{K} \times [H]$ such that $ \upsilon^k_{h}(s^k_h,a) = \phi(s^k_h, a)^\top \thetab^\star_h - 2 \alpha^k_h \| \phi(s^k_h,a) \|_{\left(\Hb^k_h\right)^{-1} } $.
        \\
        In this case, we know that $
         f^k_{h,J(k,h)}(s^k_h,a) < f^k_{h,J(k,h)}(s^k_h,\OutsideOption) $ for all $a \in \SetOfGroundActions \setminus \{\OutsideOption\} $.
        This implies that $|A^k_h| = 2$, since adding any item  $a \in \SetOfGroundActions \setminus \{\OutsideOption\} $ to the set $\{\OutsideOption\}$ always decreases the expected value of $f^k_{h, J(k,h)}$.
        Furthermore, since we assume $\phi(s^k_h, \OutsideOption) = 0$ (which also implies $\upsilon^k_h(s^k_h, \OutsideOption) = 0$), and denoting $A^k_h = \{\OutsideOption, \tilde{a}^k_h\}$, we have:
        %
        %
        \begin{align*}
            &\sum_{k \in \mathcal{K}_{(ii)}} \sum_{h=1}^H
            \nabla  \tilde{R}(\upsilonb^\star_h(s^k_h))^\top \left( \upsilonb^k_{h}(s^k_h) - \upsilonb^\star_h(s^k_h) \right)
            \\
            &= \!\!\sum_{k \in \mathcal{K}_{(ii)}} \sum_{h=1}^H 
                2 \alpha^k_h
                \ChoiceProbability_{h}\left(\tilde{a}^k_h|s^k_{h}, A^k_{h}; \thetab^\star_h\right)
                f^k_{h, J(k,h)}(s^k_{h}, \tilde{a}^k_h) 
            \bigg(
                     \| \phi(s^k_h,\tilde{a}^k_h) \|_{\left(\Hb^k_h\right)^{-1} } 
                    - \ChoiceProbability_{h}\left(\tilde{a}^k_h|s^k_{h}, A^k_{h}; \thetab^\star_h\right) \left[\| \phi(s^k_h,\tilde{a}^k_h) \|_{\left(\Hb^k_h\right)^{-1} }  \right]
                \bigg)
            \\
            &\leq  2 \bar{\alpha}_K \!\! \sum_{k \in \mathcal{K}_{(ii)}} \sum_{h=1}^H
                \EE_{\thetab^\star_h} \Bigg[
                    \bigg(
                        f^k_{h, J(k,h)}(s^k_{h}, a)
                        - \EE_{\thetab^\star_h} \left[
                        f^k_{h, J(k,h)}(s^k_{h}, a') 
                        \right]
                    \bigg)
                        \| \phi(s^k_h,a) 
                        - \EE_{\thetab^\star_h} [ \phi(s^k_h,a') ] \|_{\left(\Hb^k_h\right)^{-1} } 
                \Bigg]
                ,
                \numberthis \label{eq:tight_MNL_first_case2}
        \end{align*}
        %
        where in the inequality, we use the definition $\bar{\alpha}_K:= \max_{h \in [H]}\alpha^K_h$.
        The rest of the analysis is similar to that in \textbf{Case (i)}.
        Therefore, we derive
        \begin{align*}
            \sum_{k \in \mathcal{K}_{(ii)}} \sum_{h=1}^H
            &\nabla  \tilde{R}(\upsilonb^\star_h(s^k_h))^\top \left( \upsilonb^k_{h}(s^k_h) - \upsilonb^\star_h(s^k_h) \right)
            = \BigO\left(
                d\sqrt{ H |\mathcal{K}|} 
                (\log K)^{3/2}
                \log M
                +
                \frac{1}{\kappa}
                d^2 H 
                (\log K)^3
                (\log M)^2
            \right).
            \numberthis \label{eq:tight_MNL_first_case2_result}
        \end{align*}
        %
        %
        Now, we bound the term \texttt{(B)} in~\eqref{eq:MNL_improved_regret_decomposition}.
        Let $p_a(\bar{\upsilonb}^k_h(s^k_h) ) = \frac{\exp\left( \bar{\upsilon}^k_h(s^k_h,a) \right)}{1 + \sum_{a'' \in A^k_h}\exp\left(\bar{\upsilon}^k_h(s^k_h,a'') \right)}$.
        %
        \begin{align*}
            &\frac{1}{2} \sum_{k \in \mathcal{K}} 
            \sum_{h=1}^H
            \left( \upsilonb^k_{h}(s^k_h) - \upsilonb^\star_h(s^k_h) \right)^\top 
                \nabla^2 \tilde{R}(\bar{\upsilonb}^k_{h}(s^k_h))
                \left( \upsilonb^k_{h}(s^k_h) - \upsilonb^\star_h(s^k_h) \right)
            \\
            &= \frac{1}{2} \sum_{k \in \mathcal{K}}
            \sum_{h=1}^H
            \sum_{a \in A^k_h}
            \sum_{a' \in A^k_h}
            \left(\upsilon^k_{h}(s^k_h,a) - \phi(s^k_h, a)^\top \thetab^\star_h \right)
            \frac{\partial^2 \tilde{R}(\bar{\upsilonb}^k_{h}(s^k_h))}{\partial a \partial a'}
            \left(\upsilon^k_{h}(s^k_h,a') - \phi(s^k_h, a')^\top \thetab^\star_h \right)
            \\
            &= 
            \frac{1}{2} \sum_{k \in \mathcal{K}}
            \sum_{h=1}^H
            \sum_{a \in A^k_h}
            \sum_{ \substack{a' \in A^k_h \\ a' \neq a} }
            \left(\upsilon^k_{h}(s^k_h,a) - \phi(s^k_h, a)^\top \thetab^\star_h \right)
            \frac{\partial^2 \tilde{R}(\bar{\upsilonb}^k_{h}(s^k_h))}{\partial a \partial a'}
            \left(\upsilon^k_{h}(s^k_h,a') - \phi(s^k_h, a')^\top \thetab^\star_h \right)
            \\
            &+ \frac{1}{2} \sum_{k \in \mathcal{K}}
            \sum_{h=1}^H
            \sum_{a \in A^k_h}
            \left(\upsilon^k_{h}(s^k_h,a) - \phi(s^k_h, a)^\top \thetab^\star_h \right)^2
            \frac{\partial^2 \tilde{R}(\bar{\upsilonb}^k_{h}(s^k_h))}{\partial a \partial a}
            \\
            &\leq  \sum_{k \in \mathcal{K}}
            \sum_{h=1}^H
            \sum_{a \in A^k_h}
            \sum_{ \substack{a' \in A^k_h \\ a' \neq a} }
            \left|\upsilon^k_{h}(s^k_h,a) - \phi(s^k_h, a)^\top \thetab^\star_h \right|
            p_a(\bar{\upsilonb}^k_h(s^k_h) )
            p_{a'}(\bar{\upsilonb}^k_h(s^k_h) )
            \left|\upsilon^k_{h}(s^k_h,a') - \phi(s^k_h, a')^\top \thetab^\star_h \right|
            \\
            &+ \frac{3}{2} \sum_{k \in \mathcal{K}}
            \sum_{h=1}^H
            \sum_{a \in A^k_h}
            \left(\upsilon^k_{h}(s^k_h,a) - \phi(s^k_h, a)^\top \thetab^\star_h \right)^2
            p_a(\bar{\upsilonb}^k_h(s^k_h) ),
            \numberthis \label{eq:improve_MNL_second_mid}
        \end{align*}
        %
        where the inequality holds by Lemma~\ref{lemma:second_pd} and $f^k_{j, J(k,h)}\leq 1$.
        To bound the first term in~\eqref{eq:improve_MNL_second_mid}, by applying the AM-GM inequality, we get
        %
        \begin{align*}
            & \sum_{k \in \mathcal{K}}
            \sum_{h=1}^H
            \sum_{a \in A^k_h}
            \sum_{ \substack{a' \in A^k_h \\ a' \neq a} }
            \left|\upsilon^k_{h}(s^k_h,a) - \phi(s^k_h, a)^\top \thetab^\star_h \right|
            p_a(\bar{\upsilonb}^k_h(s^k_h) )
            p_{a'}(\bar{\upsilonb}^k_h(s^k_h) )
            \left|\upsilon^k_{h}(s^k_h,a') - \phi(s^k_h, a')^\top \thetab^\star_h \right|
            \\
            &\leq \sum_{k \in \mathcal{K}}
            \sum_{h=1}^H
            \sum_{a \in A^k_h}
            \sum_{ a' \in A^k_h  }
            \left|\upsilon^k_{h}(s^k_h,a) - \phi(s^k_h, a)^\top \thetab^\star_h \right|
            p_a(\bar{\upsilonb}^k_h(s^k_h) )
            p_{a'}(\bar{\upsilonb}^k_h(s^k_h) )
            \left|\upsilon^k_{h}(s^k_h,a') - \phi(s^k_h, a')^\top \thetab^\star_h \right|
            \\
            &\leq 
            \frac{1}{2}
            \sum_{k \in \mathcal{K}}
            \sum_{h=1}^H
            \sum_{a \in A^k_h}
            \sum_{ a' \in A^k_h  }
            \left(\upsilon^k_{h}(s^k_h,a) - \phi(s^k_h, a)^\top \thetab^\star_h \right)^2
            p_a(\bar{\upsilonb}^k_h(s^k_h) )
            p_{a'}(\bar{\upsilonb}^k_h(s^k_h) )
            \\
            &+ \frac{1}{2} \sum_{k \in \mathcal{K}}
            \sum_{h=1}^H
            \sum_{a \in A^k_h}
            \sum_{ a' \in A^k_h  }
            p_a(\bar{\upsilonb}^k_h(s^k_h) )
            p_{a'}(\bar{\upsilonb}^k_h(s^k_h) )
            \left(\upsilon^k_{h}(s^k_h,a') - \phi(s^k_h, a')^\top \thetab^\star_h \right)^2
            \\
            &= \sum_{k \in \mathcal{K}} 
            \sum_{h=1}^H
            \sum_{a \in A^k_h}
            \left(\upsilon^k_{h}(s^k_h,a) - \phi(s^k_h, a)^\top \thetab^\star_h \right)^2
            p_a(\bar{\upsilonb}^k_h(s^k_h) )
            \numberthis \label{eq:improve_MNL_second_mid_2}
            . 
        \end{align*}
        % 
        Plugging~\eqref{eq:improve_MNL_second_mid_2} into~\eqref{eq:improve_MNL_second_mid}, we have
        %
        \begin{align*}
            \frac{1}{2} \sum_{k \in \mathcal{K}}\sum_{h=1}^H
            & \left( \upsilonb^k_{h}(s^k_h) - \upsilonb^\star_h(s^k_h) \right)^\top 
                \nabla^2 \tilde{R}(\bar{\upsilonb}^k_{h}(s^k_h))
                \left( \upsilonb^k_{h}(s^k_h) - \upsilonb^\star_h(s^k_h) \right)
            \\
            &\leq
            \frac{5}{2} \sum_{k \in \mathcal{K}} 
            \sum_{h=1}^H
            \sum_{a \in A^k_h}
            p_a(\bar{\upsilonb}^k_h(s^k_h) )
            \left(\upsilon^k_{h}(s^k_h,a) - \phi(s^k_h, a)^\top \thetab^\star_h \right)^2
            \\
            &\leq 10 \left(\bar{\alpha}_K \right)^2
             \sum_{k \in \mathcal{K}}
             \sum_{h=1}^H
             \sum_{a \in A^k_h}
            p_a(\bar{\upsilonb}^k_h(s^k_h) )
            \| \phi(s^k_h,a) \|_{\left(\Hb^k_h\right)^{-1}}
            \\
            &\leq 10 \left(\bar{\alpha}_K \right)^2
             \sum_{k \in \mathcal{K}}
             \sum_{h=1}^H
             \max_{a \in A^k_h}
            \| \phi(s^k_h,a) \|_{\left(\Hb^k_h\right)^{-1}}
            = \BigO
            \left( \frac{1}{\kappa} d^2 H  \left(\log K \right)^3 \left(\log M \right)^2
            \right),
            \numberthis \label{eq:improved_mnl_second_final}
        \end{align*}
        where the last inequality holds by Lemma~\ref{lemma:elliptical}.

        Combining~\eqref{eq:tight_MNL_first_case1_result},~\eqref{eq:tight_MNL_first_case2_result} and~\eqref{eq:improved_mnl_second_final}, we obtain
        %
        \begin{align*}
            \sum_{k \in \mathcal{K}}\sum_{h=1}^H
             \sum_{a \in A^k_{h}} &\!\!\! \left(\widetilde{\ChoiceProbability}^k_{h,J(k,h)}(a|s^k_{h}, A^k_{h})  - \ChoiceProbability_{h}(a|s^k_{h}, A^k_{h}) \right) f^k_{h, J(k,h)}(s^k_{h}, a)
             \\
             &= \BigO\left(
                d\sqrt{ H |\mathcal{K}|} 
                (\log K)^{3/2}
                \log M
                +
                \frac{1}{\kappa}
                d^2 H 
                (\log K)^3
                (\log M)^2
            \right).
        \end{align*}
        This conclude the proof of Lemma~\ref{lemma:tight_mnl}.
        %
    \end{proof}
\end{lemma}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Optimism}
\label{subsec:optimism}
%
In this subsection, we prove the optimism of our value estimates $V^k_h$. 
%
\begin{lemma} [Point-wise monotonicity, Lemma 31 of~\citealt{agarwal2023vo}] \label{lemma:monotonocity}
    Suppose Algorithm~\ref{alg:main} uses a consistent bonus oracle satisfying Definition~\ref{def:bonus_oracle}.
    For any fixed $(k,h) \in [K] \times [H]$, conditioning on events $\Event_{\leq k-1} \mcap \left( \mcap_{h'=h}^H \Event^k_{h'} \right)$, for all $(s_h, a_h) \in \SetOfStates \times \SetOfGroundActions$, we have
    %
    \begin{enumerate}
        \item $\GroundQ^\star_h(s_h, a_h) \leq f^k_{h,1}(s_h, a_h)$;
        \item $f^k_{h, -1} (s_h, a_h) \leq \GroundQ^\star_h(s_h, a_h)$;
        \item $f^\tau_{h,2} (s_h, a_h) \geq \max \left\{ \mathcal{T}_h V^k_{h+1,1}(s_h,a_h), f^k_{h,1}(s_h, a_h) \right\}, \quad \forall \tau \in [k]$.
    \end{enumerate}
\end{lemma}

\begin{lemma} [Optimism] \label{lemma:optimism}
    Let $V^k_h$ be the \textit{realized} optimistic value function defined in~\eqref{eq:optimistic_Q}.
    Suppose Algorithm~\ref{alg:main} uses a consistent bonus oracle satisfying Definition~\ref{def:bonus_oracle}.
    On the even conditioning on the good event $\EventMNL \mcap \Event_{\leq K}$, for all $(k,h) \in [K] \times [H]$, we have
    %
    \begin{align*}
        V^k_h(s^k_h) \geq V^\star_h (s^k_h).
    \end{align*}
    %
    \begin{proof} [Proof of Lemma~\ref{lemma:optimism}]
        %
        We denote $A^{k,\star}_h \in \argmax_{A} \sum_{a \in A} \ChoiceProbability_h(a|s^k_h, A) \GroundQ^\star_h(s^k_h, a)$.
        %
        If $A^k_h = A^k_{h,1}$, by the definition of the optimal value function $V^\star_h$, we have
        \begin{align*}
            V^\star_h(s^k_h) 
            &= \max_{A \in \SetOfActions} \sum_{a \in A} \ChoiceProbability_h(a|s^k_h, A) \GroundQ^\star_h(s^k_h, a)
            \\
            &=  \sum_{a \in A^{k,\star}_h} \ChoiceProbability_h(a|s^k_h, A^{k,\star}_h) \GroundQ^\star_h(s^k_h, a)
            \\
            &\leq \sum_{a \in A^{k,\star}_h} \ChoiceProbability_h(a|s^k_h, A^{k,\star}_h) f^k_{h,1}(s^k_h, a)
            \\
            &\leq \sum_{a \in \tilde{A}^k_h} \widetilde{\ChoiceProbability}^k_{h,1} (a | s^k_h, \tilde{A}^k_h) f^k_{h,1} (s^k_h, a)
            \\
            &\leq \sum_{a \in A^k_{h,1}} \widetilde{\ChoiceProbability}^k_{h,1} (a | s^k_h, A^k_{h,1}) f^k_{h,1} (s^k_h, a)
            \\
            &= \max_{A \in \SetOfActions} \sum_{a \in A} \widetilde{\ChoiceProbability}^k_{h,1} (a | s^k_h, A) f^k_{h,1} (s^k_h, a)
            = V^k_h (s^k_h)
            ,
        \end{align*}
        %
        where the first inequality holds by Lemma~\ref{lemma:monotonocity},
        in the second inequality, we use the fact that, by Lemma~\ref{lemma:tilde_choice}, there exists a subset $\tilde{A}^k_h \subseteq A^{k,\star}_h$ with $\tilde{A}^k_h \in \SetOfActions$ such that the inequality holds,
        and the last inequality holds by the definition of $A^k_{h,1}$.

        The case where $A^k_h = A^k_{h,2}$ can be proven using the same reasoning.
    \end{proof}
    %
\end{lemma}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Variances}
\label{subsec:variance}
%
In this subsection, we present properties related to variances.
%
\begin{lemma} [Upper bound of variance estimator, 
Lemma 34 of~\citealt{agarwal2023vo}] \label{lemma:upper_bound_variance_estimator}
    Let $z^k_h = (s^k_h, a^k_h)$.
    We denote $\EE_{\TransitionProbability} [ \cdot | s^k_h, a^k_h] = \EE_{s_{h+1} \sim \TransitionProbability_h(\cdot | s^k_h, a^k_h)} [ \cdot | s^k_h, a^k_h]  $ and $\mathbb{V}_{\TransitionProbability}[\cdot | s^k_h, a^k_h] =  \mathbb{V}_{s_{h+1} \sim \TransitionProbability_h(\cdot | s^k_h, a^k_h)} [ \cdot | s^k_h, a^k_h] $, where the expectation in only taken over $s_{h+1}$ due to the model transition for shorthand.
    Suppose Algorithm~\ref{alg:main} uses a consistent bonus oracle satisfying Definition~\ref{def:bonus_oracle}.
    For any episode $k \geq 2$ conditioning on the good event $\Event_{\leq K}$, the variance estimator $\sigma^k_h$ satisfies
    %
    \begin{align*}
        \left( \sigma^k_h \right)^2
        &\leq \mathbb{V} \left[ r_h + V^k_{h+1, 1} (s_{h+1}) \mid z^k_h \right]
        +4 \left( f^k_{h,2}(z^k_h ) - f^k_{h,-2}(z^k_h) \right)
        \\
        &+ 4 \min \left\{1, D_{\mathcal{F}_h}\!\left(z^k_h ; \{ z^\tau_h \}_{\tau=1}^{k-1}, \{ \mathbf{1}^\tau \}_{\tau=1}^{k-1}  \right) 
        \cdot \left( 2\sqrt{\left( \bar{\beta}^k_h \right)^2  + \rho }+ 4 \BoundFunction\sqrt{ \left( \beta^k_{h,2} \right)^2 +\rho}    \right) \right\}.
    \end{align*}
    %
\end{lemma}


\begin{lemma} [Sum of variances, Corollary 50 of~\citealt{agarwal2023vo}] \label{lemma:sum_of_var}
    % For any $(k,h) \in [K] \times [H]$, we define the filtration as
    % \begin{align*}
    %     \mathcal{H}^{k-1}_H = \sigma \left( s^1_1, A^1_1, a^1_1, r^1_1, \dots, r^1_H, s^1_{H+1} ;
    %     s^2_1, A^2_1, a^2_1, r^2_1
    %     \dots,  s^{k-1}_H, A^{k-1}_H, r^{k-1}_H, s^{k-1}_{H+1} \right).
    % \end{align*}
    Let $z^k_h = (s^k_h, a^k_h)$.
    We denote $\EE_{\TransitionProbability} [ \cdot | s^k_h, a^k_h] = \EE_{s_{h+1} \sim \TransitionProbability_h(\cdot | s^k_h, a^k_h)} [ \cdot | s^k_h, a^k_h]  $ and $\mathbb{V}_{\TransitionProbability}[\cdot | s^k_h, a^k_h] =  \mathbb{V}_{s_{h+1} \sim \TransitionProbability_h(\cdot | s^k_h, a^k_h)} [ \cdot | s^k_h, a^k_h] $, where the expectation in only taken over $s_{h+1}$ due to the model transition for shorthand.
    When $\BoundFunction = \BigO(1)$, with probability at least $1-\delta$, we have
    %
    \begin{align*}
        \sum_{k=1}^K &\sum_{h=1}^H  \mathbb{V}\left[ r_h + V^k_{h+1, 1} (s_{h+1}) | z^k_h \right] 
        \\
        &\leq \BigO \left( H \sum_{k=1}^K \sum_{h=1}^H 
            \left( f^k_{h,2} (z^k_h) 
            -  f^k_{h,-2} (z^k_h)  \right)  
            + K 
            +KH^2 \delta
            + H^2 |\OOptEpisode|
            +  H^4  \log^2 \frac{KH}{\delta}  \right).
    \end{align*}
    %
\end{lemma}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Approximation Error of Optimistic, Overly Optimistic (Pessimistic) $\GroundQ$-values}
\label{app:approximation_error}
%
In this section, we provide some inequalities for bounding the optimistic values, overly optimistic values, and overly pessimistic values sequence, which are useful for the proofs in Subsection~\ref{app:subsec_bonus}.
%
%
\begin{lemma}[Approximation error of overly pessimistic $\GroundQ$] \label{lemma:pess_approx_error}
    Suppose Algorithm~\ref{alg:main} uses a consistent bonus oracle satisfying Definition~\ref{def:bonus_oracle}. 
    Conditioning on the good event $\Event_{\leq K}$, for any $(k,h) \in [K] \times [H]$, it holds that
    %
    \begin{align*}
        (f^k_{h,-2} - \GroundQ^{\pi_k}_h) (s^k_h, a^k_h)
            &\geq
            \sum_{h'=h+1}^H \sum_{a' \in A^k_{h'}} \!\!\! \left(\widetilde{\ChoiceProbability}^k_{h', -2}(a'|s^k_{h'}, A^k_{h'})  - \ChoiceProbability_{h'}(a'|s^k_{h'}, A^k_{h'}) \right) f^k_{h', -2}(s^k_{h'}, a')
            \\
            &-2 \sum_{h'=h}^H b^k_{h',2}(s^k_{h'}, a^k_{h'}) 
            + \sum_{h'=h+1}^H \MDSp^k_{h',-2}
            + \sum_{h'=h+1}^H\MDSc^k_{h', -2}
            ,
    \end{align*}
    where  $\MDSp^k_{h,-2} := \EE_{\TransitionProbability}\left[  (V^k_{h, -2} -  V^{\pi_k}_{h})(s_{h})  \mid s^k_{h-1}, a^k_{h-1} \right] - (V^k_{h, -2} -  V^{\pi_k}_{h})(s^k_{h})$ and $\MDSc^k_{h, -2} := \EE_{\ChoiceProbability}
         \left[ 
         \left(f^k_{h, -2}
             -\GroundQ^{\pi_k}_{h} \right)(s^k_{h}, a_{h}) \mid s^k_{h}, A^k_{h}  \right] - \left(f^k_{h, -2}
             -\GroundQ^{\pi_k}_{h} \right)(s^k_{h}, a^k_{h}) $.
    %
    \begin{proof} [Proof of Lemma~\ref{lemma:pess_approx_error}]
    Under the event $\Event_{\leq K}$, we have
        \begin{align*}
            (f^k_{h,-2} &- \GroundQ^{\pi_k}_h) (s^k_h, a^k_h)
            \\
            &= (f^k_{h,-2} 
            - \mathcal{T}_h V^k_{h+1,-2}) (s^k_h, a^k_h)
            + (\mathcal{T}_h V^k_{h+1,-2} 
            - \GroundQ^{\pi_k}_h) (s^k_h, a^k_h)
            \\
            &\geq - 2b^k_{h,2}(s^k_h, a^k_h) 
            + \EE_{\TransitionProbability}\left[(r_h - r_h) +  (V^k_{h+1, -2} -  V^{\pi_k}_{h+1})(s_{h+1})  | s^k_h, a^k_h \right]
            \\
            &= - 2b^k_{h,2}(s^k_h, a^k_h) 
            +  (V^k_{h+1, -2} -  V^{\pi_k}_{h+1})(s^k_{h+1})
            + \MDSp^k_{h+1,-2}
            \\
            &\geq - 2b^k_{h,2}(s^k_h, a^k_h) 
            +  (Q^k_{h+1, -2} -  Q^{\pi_k}_{h+1})(s^k_{h+1}, A^k_{h+1})
            +\MDSp^k_{h+1,-2}
            \\
            &= \!\!\sum_{a' \in A^k_{h+1}} \!\!\!\left(\widetilde{\ChoiceProbability}^k_{h+1, -2}(a'|s^k_{h+1}, A^k_{h+1}) f^k_{h+1, -2}(s^k_{h+1}, a')
            - \ChoiceProbability_{h+1}(a'|s^k_{h+1}, A^k_{h+1}) \GroundQ^{\pi_k}_{h+1}(s^k_{h+1}, a')  \right)
            \\
            &- 2b^k_{h,2}(s^k_h, a^k_h) 
            + \MDSp^k_{h+1,-2}
            \\
            &= \!\!\sum_{a' \in A^k_{h+1}} \!\!\! \left(\widetilde{\ChoiceProbability}^k_{h+1, -2}(a'|s^k_{h+1}, A^k_{h+1})  - \ChoiceProbability_{h+1}(a'|s^k_{h+1}, A^k_{h+1}) \right) f^k_{h+1, -2}(s^k_{h+1}, a')
            \\
            &+\EE_{\ChoiceProbability}
             \left[ 
             \left(f^k_{h+1, -2}
             -\GroundQ^{\pi_k}_{h+1} \right)(s^k_{h+1}, a_{h+1}) | s^k_{h+1}, A^k_{h+1}  \right]
            - 2b^k_{h,2}(s^k_h, a^k_h) 
            + \MDSp^k_{h+1,-2}
            \\
            &= \!\!\sum_{a' \in A^k_{h+1}} \!\!\! \left(\widetilde{\ChoiceProbability}^k_{h+1, -2}(a'|s^k_{h+1}, A^k_{h+1})  - \ChoiceProbability_{h+1}(a'|s^k_{h+1}, A^k_{h+1}) \right) f^k_{h+1, -2}(s^k_{h+1}, a')
            \\
            &+ \left(f^k_{h+1, -2}
             -\GroundQ^{\pi_k}_{h+1} \right)(s^k_{h+1}, a^k_{h+1})  
            - 2b^k_{h,2}(s^k_h, a^k_h) 
            + \MDSp^k_{h+1,-2} + \MDSc^k_{h+1, -2}
            ,
        \end{align*}
        %
        where the first inequality holds because $\mathcal{T}_h V^k_{h+1,-2} \in \mathcal{F}^k_{h,-2}$ under the event $\Event_{\leq K}$  and definition of $b^k_{h,2}$, 
        and
        the last inequality holds since $V^k_{h+1,-2}(s^k_{h+1}) \geq Q^k_{h+1,-2}(s^k_{h+1}, A^k_{h+1})$.

        Hence, by recursion we obtain that
        %
        \begin{align*}
            (f^k_{h,-2} - \GroundQ^{\pi_k}_h) (s^k_h, a^k_h)
            &\geq
            \sum_{h'=h+1}^H \sum_{a' \in A^k_{h'}} \!\!\! \left(\widetilde{\ChoiceProbability}^k_{h', -2}(a'|s^k_{h'}, A^k_{h'})  - \ChoiceProbability_{h'}(a'|s^k_{h'}, A^k_{h'}) \right) f^k_{h', -2}(s^k_{h'}, a')
            \\
            &-2 \sum_{h'=h}^H b^k_{h',2}(s^k_{h'}, a^k_{h'}) 
            + \sum_{h'=h+1}^H \MDSp^k_{h',-2}
            + \sum_{h'=h+1}^H\MDSc^k_{h', -2}
            .
        \end{align*}
        %
    \end{proof}
\end{lemma}


\begin{lemma}[Approximation error of overly optimistic $\GroundQ$] \label{lemma:overly_opt_approx_error}
    Suppose Algorithm~\ref{alg:main} uses a consistent bonus oracle satisfying Definition~\ref{def:bonus_oracle}. 
    Conditioning on the good event $\Event_{\leq K}$, for any $k \in [K]$ and any $h \geq h_k$, it holds that
    %
    \begin{align*}
            (f^k_{h,2} - \GroundQ^{\pi_k}_h) (s^k_h, a^k_h)
            &\leq
            \sum_{h'=h+1}^H \sum_{a' \in A^k_{h'}} \!\!\! \left(\widetilde{\ChoiceProbability}^k_{h',2}(a'|s^k_{h'}, A^k_{h'})  - \ChoiceProbability_{h'}(a'|s^k_{h'}, A^k_{h'}) \right) f^k_{h', 2}(s^k_{h'}, a')
            \\
            &+ 2 \sum_{h'=h}^H b^k_{h',1}(s^k_{h'}, a^k_{h'})
            +2 \sum_{h'=h}^H b^k_{h',2}(s^k_{h'}, a^k_{h'}) 
            + \sum_{h'=h+1}^H \MDSp^k_{h',2}
            + \sum_{h'=h+1}^H\MDSc^k_{h', 2}
            ,
    \end{align*}
    %
    where $\MDSp^k_{h,2} := \EE_{\TransitionProbability}\left[  (V^k_{h, 2} -  V^{\pi_k}_h)(s_{h})  \mid s^k_{h-1}, a^k_{h-1} \right] - (V^k_{h, 2} -  V^{\pi_k}_h)(s^k_{h})$
        and $\MDSc^k_{h,2} := \EE_{\ChoiceProbability}
         \left[ 
         \left(f^k_{h,2}
             -\GroundQ^{\pi_k}_{h} \right)(s^k_{h}, a_{h}) \mid s^k_{h}, A^k_{h}  \right] - \left(f^k_{h,2}
             -\GroundQ^{\pi_k}_{h} \right)(s^k_{h}, a^k_{h}) $.
    %
    \begin{proof} [Proof of Lemma~\ref{lemma:overly_opt_approx_error}]
        Under the event $\Event_{\leq K}$, at $h \geq h_k$, we have
        %
        \begin{align*}
            (f^k_{h,2} &- \GroundQ^{\pi_k}_h) (s^k_h, a^k_h)
            = (f^k_{h,2} 
            - \mathcal{T}_h V^k_{h+1,2}) (s^k_h, a^k_h)
            + (\mathcal{T}_h V^k_{h+1,2} 
            - \GroundQ^{\pi_k}_h) (s^k_h, a^k_h)
            \\
            &\leq 2b^k_{h,1}(s^k_h, a^k_h)+ 2b^k_{h,2}(s^k_h, a^k_h) 
            + \EE\left[  (V^k_{h+1, 2} -  V^{\pi_k}_{h+1})(s_{h+1})  | s^k_h, a^k_h \right]
            \\
            &=  2b^k_{h,1}(s^k_h, a^k_h)+ 2b^k_{h,2}(s^k_h, a^k_h) 
            +  (V^k_{h+1, 2} -  V^{\pi_k}_{h+1})(s^k_{h+1})
            + \MDSp^k_{h+1,2}
            \\
            &= 2b^k_{h,1}(s^k_h, a^k_h)+ 2b^k_{h,2}(s^k_h, a^k_h) 
            +  (Q^k_{h+1, 2} -  Q^{\pi_k}_{h+1})(s^k_{h+1}, A^k_{h+1})
            +\MDSp^k_{h+1,2}
            \\
            &= \!\!\sum_{a' \in A^k_{h+1}} \!\!\!\left(\widetilde{\ChoiceProbability}^k_{h+1,2}(a'|s^k_{h+1}, A^k_{h+1}) f^k_{h+1,2}(s^k_{h+1}, a')
            - \ChoiceProbability_{h+1}(a'|s^k_{h+1}, A^k_{h+1}) \GroundQ^{\pi_k}_{h+1}(s^k_{h+1}, a')  \right)
            \\
            &+ 2b^k_{h,1}(s^k_h, a^k_h)+ 2b^k_{h,2}(s^k_h, a^k_h) 
            + \MDSp^k_{h+1,2}
            \\
            &= \!\!\sum_{a' \in A^k_{h+1}} \!\!\! \left(\widetilde{\ChoiceProbability}^k_{h+1,2}(a'|s^k_{h+1}, A^k_{h+1})  - \ChoiceProbability_{h+1}(a'|s^k_{h+1}, A^k_{h+1}) \right) f^k_{h+1,2}(s^k_{h+1}, a')
            \\
            &+\EE_{\ChoiceProbability}
             \left[ 
             \left(f^k_{h+1,2}
             -\GroundQ^{\pi_k}_{h+1} \right)(s^k_{h+1}, a_{h+1}) | s^k_{h+1}, A^k_{h+1}  \right]
            +2b^k_{h,1}(s^k_h, a^k_h)+ 2b^k_{h,2}(s^k_h, a^k_h) 
            + \MDSp^k_{h+1,2}
            \\
            &= \!\!\sum_{a' \in A^k_{h+1}} \!\!\! \left(\widetilde{\ChoiceProbability}^k_{h+1,2}(a'|s^k_{h+1}, A^k_{h+1})  - \ChoiceProbability_{h+1}(a'|s^k_{h+1}, A^k_{h+1}) \right) f^k_{h+1,2}(s^k_{h+1}, a')
            \\
            &+ \left(f^k_{h+1,2}
             -\GroundQ^{\pi_k}_{h+1} \right)(s^k_{h+1}, a^k_{h+1})  
            + 2b^k_{h,1}(s^k_h, a^k_h)+ 2b^k_{h,2}(s^k_h, a^k_h) 
            + \MDSp^k_{h+1,2} + \MDSc^k_{h+1,2},
        \end{align*}
        where
        the first inequality holds based on the assumption that $\mathcal{T}_h V^k_{h+1,2} \in \mathcal{F}^k_{h,2}$ and definition of $b^k_{h,2}$,
        and
        the third equality holds because for $h \geq h_k$, we know that $A^k_{h+1}  
        \in \argmax_A Q^k_{h+1}(s^k_{h+1}, A)
        = \argmax_{A}  \sum_{a \in A  } \widetilde{\ChoiceProbability}^k_{h+1,2}(a | s^k_{h+1}, A) f^k_{h+1,2}(s^k_{h+1},a)$ by the data collection policy in~\eqref{eq:exploration_policy}.

        Therefore, by recursion we get
        %
        \begin{align*}
            (f^k_{h,2} - \GroundQ^{\pi_k}_h) (s^k_h, a^k_h)
            &\leq
            \sum_{h'=h+1}^H \sum_{a' \in A^k_{h'}} \!\!\! \left(\widetilde{\ChoiceProbability}^k_{h',2}(a'|s^k_{h'}, A^k_{h'})  - \ChoiceProbability_{h'}(a'|s^k_{h'}, A^k_{h'}) \right) f^k_{h', 2}(s^k_{h'}, a')
            \\
            &+ 2 \sum_{h'=h}^H b^k_{h',1}(s^k_{h'}, a^k_{h'})
            +2 \sum_{h'=h}^H b^k_{h',2}(s^k_{h'}, a^k_{h'}) 
            + \sum_{h'=h+1}^H \MDSp^k_{h',2}
            + \sum_{h'=h+1}^H\MDSc^k_{h', 2}
            .
        \end{align*}
        %
    \end{proof}
\end{lemma}
    
\begin{lemma}[Approximation error of optimistic $\GroundQ$] \label{lemma:opt_approx_error}
    Suppose Algorithm~\ref{alg:main} uses a consistent bonus oracle satisfying Definition~\ref{def:bonus_oracle}. 
    Conditioning on the good event $\EventMNL \mcap \Event_{\leq K}$, for any $k \in [K]$ and any $h \leq h_k $, we have
    %
    \begin{align*}
            &(f^k_{h,1} - \GroundQ^{\pi_k}_h) (s^k_h, a^k_h)
            \\
            &\leq
            \sum_{h'=h+1}^{h_k-1} \sum_{a' \in A^k_{h'}} \!\!\! \left(\widetilde{\ChoiceProbability}^k_{h',1}(a'|s^k_{h'}, A^k_{h'})  - \ChoiceProbability_{h'}(a'|s^k_{h'}, A^k_{h'}) \right) f^k_{h', 1}(s^k_{h'}, a')
            \\
            &+ \sum_{h'=h_k}^{H} \sum_{a' \in A^k_{h'}} \!\!\! \left(\widetilde{\ChoiceProbability}^k_{h',2}(a'|s^k_{h'}, A^k_{h'})  - \ChoiceProbability_{h'}(a'|s^k_{h'}, A^k_{h'}) \right) f^k_{h', 2}(s^k_{h'}, a')
            \\
            &+ 2 \sum_{h'=h}^H b^k_{h',1}(s^k_{h'}, a^k_{h'})
            +2 \sum_{h'=h_k}^H b^k_{h',2}(s^k_{h'}, a^k_{h'}) 
            + \sum_{h'=h+1}^{h_k-1} (\MDSp^k_{h',1}
            + \MDSc^k_{h', 1} )
            + \sum_{h'=h_k}^H (\MDSp^k_{h',2}
            + \MDSc^k_{h', 2})
            ,            
    \end{align*}
    %
    where
    \begin{align*}
        \MDSp^k_{h,1} &:= \EE_{\TransitionProbability}\left[  (V^k_{h,1} -  V^{\pi_k}_{h})(s_{h}) \mid s^k_{h-1}, a^k_{h-1} \right] - (V^k_{h,1} -  V^{\pi_k}_{h})(s^k_{h}),
        \\
        \MDSc^k_{h,1} &:= \EE_{\ChoiceProbability}
         \left[ 
         \left(f^k_{h,1}
             -\GroundQ^{\pi_k}_{h} \right)(s^k_{h}, a_{h}) \mid s^k_{h}, A^k_{h}  \right] - \left(f^k_{h,1}
             -\GroundQ^{\pi_k}_{h} \right)(s^k_{h}, a^k_{h}), 
        \\
        \MDSp^k_{h,2} &:= \EE_{\TransitionProbability}\left[  (V^k_{h,2} -  V^{\pi_k}_h)(s_{h})  \mid s^k_{h-1}, a^k_{h-1} \right] - (V^k_{h,2} -  V^{\pi_k}_h)(s^k_{h}),
        \\
        \MDSc^k_{h,2} &:= \EE_{\ChoiceProbability}
        \left[ 
        \left(f^k_{h,2}
         -\GroundQ^{\pi_k}_{h} \right)(s^k_{h}, a_{h}) \mid s^k_{h}, A^k_{h}  \right] - \left(f^k_{h,2}
         -\GroundQ^{\pi_k}_{h} \right)(s^k_{h}, a^k_{h}).
    \end{align*}
    Here, following the convention, we use the empty sum notation, i.e., $\sum_{i=a}^b x_i = 0$, when $b\leq a$.
    %
    \begin{proof} [Proof of Lemma~\ref{lemma:opt_approx_error}]
        Under the event $\Event_{\leq K}$, by Lemma~\ref{lemma:monotonocity} it holds that $f^k_{h,1}(s,a) \leq f^k_{h,2}(s,a)$ for all $(s,a) \in \SetOfStates \times \SetOfGroundActions$.
        Therefore, for $h = h_k$, by Lemma~\ref{lemma:overly_opt_approx_error}, we get
        %
        \begin{align*}
            (f^k_{h,1} &- \GroundQ^{\pi_k}_h) (s^k_h, a^k_h)
            \leq (f^k_{h,2} - \GroundQ^{\pi_k}_h) (s^k_h, a^k_h)
            \\
            &\leq \sum_{h'=h+1}^H \sum_{a' \in A^k_{h'}} \!\!\! \left(\widetilde{\ChoiceProbability}^k_{h',2}(a'|s^k_{h'}, A^k_{h'})  - \ChoiceProbability_{h'}(a'|s^k_{h'}, A^k_{h'}) \right) f^k_{h', 2}(s^k_{h'}, a')
            \\
            &+ 2 \sum_{h'=h}^H b^k_{h',1}(s^k_{h'}, a^k_{h'})
            +2 \sum_{h'=h}^H b^k_{h',2}(s^k_{h'}, a^k_{h'}) 
            + \sum_{h'=h+1}^H \MDSp^k_{h',2}
            + \sum_{h'=h+1}^H\MDSc^k_{h', 2}
            . \numberthis \label{eq:opt_approx_error_1}
        \end{align*}
        %
        For $h = h_k -1$, we have
        %
        \begin{align*}
            (f^k_{h,1} &- \GroundQ^{\pi_k}_h) (s^k_h, a^k_h)
            = (f^k_{h,1} 
            - \mathcal{T}_h V^k_{h+1,1}) (s^k_h, a^k_h)
            + (\mathcal{T}_h V^k_{h+1,1} 
            - \GroundQ^{\pi_k}_h) (s^k_h, a^k_h)
            \\
            &\leq 2b^k_{h,1}(s^k_h, a^k_h) 
            + \EE\left[  (V^k_{h+1,1} -  V^{\pi_k}_h)(s_{h+1})  | s^k_h, a^k_h \right]
            \\
            &\leq 2b^k_{h,1}(s^k_h, a^k_h) 
            + \EE\left[  (V^k_{h+1,2} -  V^{\pi_k}_h)(s_{h+1})  | s^k_h, a^k_h \right]
            \\
            &= 2b^k_{h,1}(s^k_h, a^k_h) 
            +  (V^k_{h+1,2} -  V^{\pi_k}_h)(s^k_{h+1})   
            + \MDSp^k_{h+1,2}
            \\
            &= 2b^k_{h,1}(s^k_h, a^k_h) 
            +  (Q^k_{h+1,2} -  Q^{\pi_k}_h)(s^k_{h+1}, A^k_{h+1})
            +\MDSp^k_{h+1,2}
            \\
            &\leq \!\!\sum_{a' \in A^k_{h+1}} \!\!\!\left(\widetilde{\ChoiceProbability}^k_{h+1,2}(a'|s^k_{h+1}, A^k_{h+1}) f^k_{h+1,2}(s^k_{h+1}, a')
            - \ChoiceProbability_{h+1}(a'|s^k_{h+1}, A^k_{h+1}) \GroundQ^{\pi_k}_{h+1}(s^k_{h+1}, a')  \right)
            \\
            &+ 2b^k_{h,1}(s^k_h, a^k_h) 
            + \MDSp^k_{h+1,2}
            \\
            &= \!\!\sum_{a' \in A^k_{h+1}} \!\!\! \left(\widetilde{\ChoiceProbability}^k_{h+1,2}(a'|s^k_{h+1}, A^k_{h+1})  - \ChoiceProbability_{h+1}(a'|s^k_{h+1}, A^k_{h+1}) \right) f^k_{h+1,2}(s^k_{h+1}, a')
            \\
            &+\EE_{\ChoiceProbability}
             \left[ 
             \left(f^k_{h+1,2}
             -\GroundQ^{\pi_k}_{h+1} \right)(s^k_{h+1}, a_{h+1}) | s^k_{h+1}, A^k_{h+1}  \right]
            +2b^k_{h,1}(s^k_h, a^k_h)
            + \MDSp^k_{h+1,2}
            \\
            &= \!\!\sum_{a' \in A^k_{h+1}} \!\!\! \left(\widetilde{\ChoiceProbability}^k_{h+1,2}(a'|s^k_{h+1}, A^k_{h+1})  - \ChoiceProbability_{h+1}(a'|s^k_{h+1}, A^k_{h+1}) \right) f^k_{h+1,2}(s^k_{h+1}, a')
            \\
            &+ \left(f^k_{h+1,2}
             -\GroundQ^{\pi_k}_{h+1} \right)(s^k_{h+1}, a^k_{h+1})  
            + 2b^k_{h,1}(s^k_h, a^k_h)
            + \MDSp^k_{h+1,2} + \MDSc^k_{h+1,2}
            \numberthis \label{eq:opt_approx_error_2_mid}
            ,        
        \end{align*}
        where
        the first inequality holds based on the assumption that $\mathcal{T}_h V^k_{h+1,1} \in \mathcal{F}^k_{h,1}$ and definition of $b^k_{h,1}$,
        and
        the second inequality holds because for any $s_{h+1} \in \SetOfStates$, we have
        %
        \begin{align*}
            V^k_{h+1,1}(s_{h+1})
            &= \sum_{a' \in A_{h+1,1}} \widetilde{\ChoiceProbability}^k_{h+1,1} (a'|s_{h+1}, A_{h+1,1}) f^k_{h+1,1}(s_{h+1},a')
            \\
            &\leq \sum_{a' \in A_{h+1,1}} \widetilde{\ChoiceProbability}^k_{h+1,1} (a'|s_{h+1}, A_{h+1,1}) f^k_{h+1,2}(s_{h+1},a')
            \\
            &\leq \sum_{a' \in \tilde{A}_{h+1,1}} \widetilde{\ChoiceProbability}^k_{h+1,2} (a'|s_{h+1}, \tilde{A}_{h+1,1}) f^k_{h+1,2}(s_{h+1},a')
            \\
            &\leq \sum_{a' \in A^k_{h+1,2}} \widetilde{\ChoiceProbability}^k_{h+1,2} (a'|s_{h+1}, A^k_{h+1,2}) f^k_{h+1,2}(s_{h+1},a')
            = V^k_{h+1,2}(s_{h+1}),
        \end{align*}
         where in the first equality, we denote $A_{h+1,1} \in \argmax_A \sum_{a' \in A} \widetilde{\ChoiceProbability}^k_{h+1,1} (a'|s_{h+1}, A) f^k_{h+1,1}(s_{h+1},a')$,
         for the first inequality, we use the fact that $f^k_{h+1,1}(s,a) \leq f^k_{h+1,2}(s,a)$ (Lemma~\ref{lemma:monotonocity}),
        the second inequality holds  for some $\tilde{A}_{h+1,1} \subseteq A_{h+1,1}$  (Lemma~\ref{lemma:tilde_choice}),
        and
        the last inequality follows from the definition of $A^k_{h+1,2}$.
        Moreover, the  third equality of~\eqref{eq:opt_approx_error_2_mid} holds because for $h +1 = h_k$, we know that $A^k_{h+1}  
        = A^k_{h+1,2}$ by the data collection policy in~\eqref{eq:exploration_policy}.
        
        Therefore, by recursion, we have
        %
        \begin{align*}
            (f^k_{h,1} &- \GroundQ^{\pi_k}_h) (s^k_h, a^k_h)
            \\
            &\leq \sum_{h'=h+1}^H \sum_{a' \in A^k_{h'}} \!\!\! \left(\widetilde{\ChoiceProbability}^k_{h',2}(a'|s^k_{h'}, A^k_{h'})  - \ChoiceProbability_{h'}(a'|s^k_{h'}, A^k_{h'}) \right) f^k_{h', 2}(s^k_{h'}, a')
            \\
            &+ 2 \sum_{h'=h}^{H} b^k_{h',1}(s^k_{h'}, a^k_{h'})
            +2 \sum_{h'=h+1}^H b^k_{h',2}(s^k_{h'}, a^k_{h'}) 
            + \sum_{h'=h+1}^H \MDSp^k_{h',2}
            + \sum_{h'=h+1}^H\MDSc^k_{h', 2}
            \numberthis \label{eq:opt_approx_error_2}
        \end{align*}


        Finally, we consider the case where $h < h_k - 1$.
        %
        \begin{align*}
            (f^k_{h,1} &- \GroundQ^{\pi_k}_h) (s^k_h, a^k_h)
            = (f^k_{h,1} 
            - \mathcal{T}_h V^k_{h+1,1}) (s^k_h, a^k_h)
            + (\mathcal{T}_h V^k_{h+1,1} 
            - \GroundQ^{\pi_k}_h) (s^k_h, a^k_h)
            \\
            &\leq 2b^k_{h,1}(s^k_h, a^k_h) 
            + \EE\left[  (V^k_{h+1,1} -  V^{\pi_k}_{h+1})(s_{h+1})  | s^k_h, a^k_h \right]
            \\
            &=  2b^k_{h,1}(s^k_h, a^k_h) 
            +  (V^k_{h+1,1} -  V^{\pi_k}_{h+1})(s^k_{h+1})
            + \MDSp^k_{h+1,1}
            \\
            &= 2b^k_{h,1}(s^k_h, a^k_h) 
            +  (Q^k_{h+1,1} -  Q^{\pi_k}_{h+1})(s^k_{h+1}, A^k_{h+1})
            +\MDSp^k_{h+1,1}
            \\
            &= \!\!\sum_{a' \in A^k_{h+1}} \!\!\!\left(\widetilde{\ChoiceProbability}^k_{h+1,1}(a'|s^k_{h+1}, A^k_{h+1}) f^k_{h+1,1}(s^k_{h+1}, a')
            - \ChoiceProbability_{h+1}(a'|s^k_{h+1}, A^k_{h+1}) \GroundQ^{\pi_k}_{h+1}(s^k_{h+1}, a')  \right)
            \\
            &+ 2b^k_{h,1}(s^k_h, a^k_h) 
            + \MDSp^k_{h+1,1}
            \\
            &= \!\!\sum_{a' \in A^k_{h+1}} \!\!\! \left(\widetilde{\ChoiceProbability}^k_{h+1,1}(a'|s^k_{h+1}, A^k_{h+1})  - \ChoiceProbability_{h+1}(a'|s^k_{h+1}, A^k_{h+1}) \right) f^k_{h+1,1}(s^k_{h+1}, a')
            \\
            &+\EE_{\ChoiceProbability}
             \left[ 
             \left(f^k_{h+1,1}
             -\GroundQ^{\pi_k}_{h+1} \right)(s^k_{h+1}, a_{h+1}) | s^k_{h+1}, A^k_{h+1}  \right]
            +2b^k_{h,1}(s^k_h, a^k_h)
            + \MDSp^k_{h+1,1}
            \\
            &= \!\!\sum_{a' \in A^k_{h+1}} \!\!\! \left(\widetilde{\ChoiceProbability}^k_{h+1,1}(a'|s^k_{h+1}, A^k_{h+1})  - \ChoiceProbability_{h+1}(a'|s^k_{h+1}, A^k_{h+1}) \right) f^k_{h+1,1}(s^k_{h+1}, a')
            \\
            &+ \left(f^k_{h+1,1}
             -\GroundQ^{\pi_k}_{h+1} \right)(s^k_{h+1}, a^k_{h+1})  
            + 2b^k_{h,1}(s^k_h, a^k_h)
            + \MDSp^k_{h+1,1} + \MDSc^k_{h+1,1}
            ,        
        \end{align*}
        where the first inequality holds based on the assumption that $\mathcal{T}_h V^k_{h+1,1} \in \mathcal{F}^k_{h,1}$ and definition of $b^k_{h,1}$ 
        and
        the third equality holds because for $h + 1 < h_k$, we have $A^k_{h+1}  
        = A^k_{h+1,1}$ by the data collection policy in~\eqref{eq:exploration_policy}.

        Hence, by recursion we have
        %
        \begin{align*}
            (f^k_{h,1} - \GroundQ^{\pi_k}_h) (s^k_h, a^k_h)
            &\leq
            \sum_{h'=h+1}^{h_k-1} \sum_{a' \in A^k_{h'}} \!\!\! \left(\widetilde{\ChoiceProbability}^k_{h',1}(a'|s^k_{h'}, A^k_{h'})  - \ChoiceProbability_{h'}(a'|s^k_{h'}, A^k_{h'}) \right) f^k_{h', 1}(s^k_{h'}, a')
            \\
            &+ \sum_{h'=h_k}^{H} \sum_{a' \in A^k_{h'}} \!\!\! \left(\widetilde{\ChoiceProbability}^k_{h',2}(a'|s^k_{h'}, A^k_{h'})  - \ChoiceProbability_{h'}(a'|s^k_{h'}, A^k_{h'}) \right) f^k_{h', 2}(s^k_{h'}, a')
            \\
            &+ 2 \sum_{h'=h}^H b^k_{h',1}(s^k_{h'}, a^k_{h'})
            +2 \sum_{h'=h_k}^H b^k_{h',2}(s^k_{h'}, a^k_{h'}) 
            + \sum_{h'=h+1}^{h_k-1} (\MDSp^k_{h',1}
            + \MDSc^k_{h', 1} )
            \\
            &+ \sum_{h'=h_k}^H (\MDSp^k_{h',2}
            + \MDSc^k_{h', 2})
            . \numberthis \label{eq:opt_approx_error_3}
        \end{align*}
        %
        Combining \eqref{eq:opt_approx_error_1}, \eqref{eq:opt_approx_error_2}, and \eqref{eq:opt_approx_error_3}, we conclude the proof.
        %
    \end{proof}

\end{lemma}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bounds on bonuses and $|\OOptEpisode|$}
\label{app:subsec_bonus}
%
In this subsection, we provide proofs for the bounds on the sum of bonuses (Lemma~\ref{lemma:b_1_crude_bound}, Lemma~\ref{lemma:b_2_crude_bound}, and Lemma~\ref{lemma:b_1_fine_grained_bound}) as well as the bound on the size of $\OOptEpisode$ (Lemma~\ref{lemma:bounding_Too}).
%
\begin{lemma}[Crude bound on $b^k_{h,1}$, Lemma 39 of~\citealt{agarwal2023vo}] 
\label{lemma:b_1_crude_bound}
    Let $z^k_h = (s^k_h, a^k_h)$.
    Given 
    $b^k_{h,1}(\cdot) \leq C \cdot \left( D_{\mathcal{F}_h } \!\left( \cdot ; \{z^\tau_h \}_{\tau=1}^{k-1}, \{ \bar{\sigma}^\tau_h \}_{\tau=1}^{k-1} \right)  
    \cdot \sqrt{\left( \beta^k_{h,1} \right)^2 + \rho } + \epsilon_b \beta^k_{h,1} \right) $,
    when $\rho = 1, \nu \leq 1$, 
    it holds that for any subset $\mathcal{K} \in [K]$, we have
    %
    \begin{align*}
        \sum_{k \in \mathcal{K}}& \sum_{h=1}^H 
        \min \left\{1+L, b^k_{h,1}(z^k_{h})  \right\}
        \\
        &= \BigO \left(\sqrt{\log \frac{\mathcal{N}  KH}{\nu \delta}} \cdot \left( 
        \sqrt{\log \frac{\mathcal{N} \mathcal{N}_b KH}{\nu \delta}}\cdot H \sqrt{d_\nu |\mathcal{K}|}
       +\log \frac{\mathcal{N} \mathcal{N}_b KH}{\nu \delta}
       \cdot  d_\nu H
        + |\mathcal{K}|H \epsilon_b \right)  \right).
    \end{align*}    
\end{lemma}

\begin{lemma}[Crude bound on $b^k_{h,2}$, Lemma 38 of~\citealt{agarwal2023vo}] 
\label{lemma:b_2_crude_bound}
    Let $z^k_h = (s^k_h, a^k_h)$.
    Given 
    $b^k_{h,2}(\cdot) \leq C \cdot \left( D_{\mathcal{F}_h } \!\left( \cdot ; \{z^\tau_h \}_{\tau=1}^{k-1}, \{ \mathbf{1}^\tau \}_{\tau=1}^{k-1} \right)  
    \cdot \sqrt{\left( \beta^k_{h,2} \right)^2 + \rho } + \epsilon_b \beta^k_{h,1} \right) $,
    when $\rho = 1, \nu \leq 1$, 
    it holds that for any subset $\mathcal{K} \in [K]$, we have
    %
    \begin{align*}
        \sum_{k \in \mathcal{K}} \sum_{h=1}^H 
        \min \left\{1+L, b^k_{h,2}(z^k_{h})  \right\}
        = \BigO \left(\sqrt{\log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}} \cdot \left(  H \sqrt{d_\nu |\mathcal{K}|} 
        + d_\nu H 
        + |\mathcal{K}|H \epsilon_b \right)  \right).
    \end{align*}
\end{lemma}

\begin{lemma} [Fine-grained bound on $b^k_{h,1}$] \label{lemma:b_1_fine_grained_bound}
    Let $z^k_h = (s^k_h, a^k_h)$.
    Recall that the bonus oracle $\BonusOracle$ outputs a bonus function such that
    $b^k_{h,1}(\cdot) \leq C \cdot \!\left( D_{\mathcal{F}_h } \!\left( \cdot ; \{z^\tau_h \}_{\tau=1}^{k-1}, \{ \bar{\sigma}^\tau_h \}_{\tau=1}^{k-1} \right)  
    \!\cdot\! \sqrt{\left( \beta^k_{h,1} \right)^2 \!+ \!\rho } + \epsilon_b \beta^k_{h,1}\! \right) $.
    When $\rho = 1, \nu = 1/\sqrt{KH}$, $\delta < (0, 1/7)$ and the event $\Event_{\leq K}$ holds, 
    with probability at least $1-7\delta$,
    we have
    %
    \begin{align*}
        &\sum_{k=1}^K \sum_{h=1}^H \min \left\{ 1+\BoundFunction, b^k_{h,1} (z^k_h) \right\}
        \\
        &= \BigO \left(
                \sqrt{d_\nu H K \cdot \log  \frac{\mathcal{N}KH}{\delta}} 
                    + 
                    \frac{1}{\sqrt{\kappa}} d H^{7/2} \sqrt{d_\nu}  (\log K)^{3/2} \log M
                \cdot \log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}
                \cdot \sqrt{\log \frac{\mathcal{N}KH}{\delta}}
                \right)
            \\
            &+ \BigO\left( 
            d_\nu H^{7/2} 
                \log \frac{\mathcal{N} KH}{\delta}
                \cdot \left(\log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}\right)^{3/2} 
                +
                \sqrt{\log \frac{\mathcal{N}KH}{\delta}}
                \cdot 
                \left(KH \epsilon_b
                    +  \sqrt{ d_\nu     
                        KH^3 \delta}
                \right)
            \right)
            \\
            &+ \BigO \left(
                \sqrt{\log \frac{\mathcal{N}KH}{\delta} \log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}}
                 \cdot \sqrt{d_\nu H} 
                \cdot
                \left(\sqrt{ H^2 \sum_{k \in \OptEpisode} u_k}
                +  \sqrt{H^2 |\OOptEpisode|}
                    \right)
            \right).
    \end{align*}
    %
    \begin{proof} [Proof of Lemma~\ref{lemma:b_1_fine_grained_bound}]
        By the definition of the oracle $\mathcal{B}$ (Definition~\ref{def:bonus_oracle}), we have
        %
        \begin{align*}
            &\sum_{k=1}^K \sum_{h=1}^H \min \left\{ 1+\BoundFunction, b^k_{h,1} (z^k_h) \right\}
            \\
            &= \BigO \left( \sum_{k=1}^K \sum_{h=1}^H 
            \min \left\{ 1,D_{\mathcal{F}_h } \!\left( z^k_h ; \{z^\tau_h \}_{\tau=1}^{k-1}, \{ \bar{\sigma} \}_{\tau=1}^{k-1} \right)  
            \cdot \sqrt{\left( \beta^k_{h,1} \right)^2 + \rho }\right\} 
            + KH\epsilon_b \cdot \max_{k,h} \beta^k_{h,1}
            \right)
            \\
            &= \BigO \left( \sqrt{\log \frac{\mathcal{N}KH}{\delta}} \cdot \sum_{k=1}^K \sum_{h=1}^H 
            \min \left\{ 1,D_{\mathcal{F}_h } \!\left( z^k_h ; \{z^\tau_h \}_{\tau=1}^{k-1}, \{ \bar{\sigma} \}_{\tau=1}^{k-1} \right)  
            \right\} 
            + KH\epsilon_b
            \right), 
            \numberthis \label{eq:fine_grained_f_2_gap_b_bound}
        \end{align*}
        where the last equality holds by the definition of $\beta^k_{h,1}$.

        Now, we bound the summation terms
        %
        \begin{align*}
            \sum_{k=1}^K \sum_{h=1}^H 
            \min &\left\{ 1,D_{\mathcal{F}_h } \!\left( z^k_h ; \{z^\tau_h   \}_{\tau=1}^{k-1}, \{ \bar{\sigma} \}_{\tau=1}^{k-1} \right)  
            \right\}      
            \\
            &= \sum_{k=1}^K \sum_{h=1}^H 
            \min \left\{ 1, \bar{\sigma}^k_h  \cdot \left(\bar{\sigma}^k_h  \right)^{-1}  D_{\mathcal{F}_h } \!\left( z^k_h ; \{z^\tau_h \}_{\tau=1}^{k-1}, \{ \bar{\sigma} \}_{\tau=1}^{k-1} \right)  
            \right\}
        \end{align*}
        by dividing into the following cases:
        %
        \begin{align*}
            \mathcal{I}_1 = 
            &\left\{ (k,h) \in [K] \times [H] : \left( \bar{\sigma}^k_h \right)^{-1} D_{\mathcal{F}_h } \!\left( z^k_h ; \{z^\tau_h \}_{\tau=1}^{k-1}, \{ \bar{\sigma} \}_{\tau=1}^{k-1} \right) \geq 1 \right\},
            \\
            \mathcal{I}_2 = &\left\{ (k,h) \in [K] \times [H] : \bar{\sigma}^k_h = \nu, (k,h) \neq \mathcal{I}_1 \right\},
            \\
            \mathcal{I}_3 = &\Big\{ (k,h) \in [K] \times [H] : \bar{\sigma}^k_h =  2 \left( \sqrt{o(\delta^k_h)} + \iota(\delta^k_h) \right) \cdot \sqrt{  D_{\mathcal{F}_h}\! \left( z^k_h; \{z^\tau_h\}_{\tau=1}^{k-1}, \{\bar{\sigma}^\tau_h \}_{\tau=1}^{k-1} \right)}, 
            \\
            &(k,h) \neq \mathcal{I}_1 \Big\},
            \\
            \mathcal{I}_4 = &\left\{ (k,h) \in [K] \times [H] : 
            \bar{\sigma}^k_h = \sqrt{2} \iota(\delta^k_h) \sqrt{ f^k_{h,2}(z^k_h) - f^k_{h, -2}(z^k_h) }, 
            (k,h) \neq \mathcal{I}_1 \right\}
            \\ 
            \mathcal{I}_5 = &\left\{ (k,h) \in [K] \times [H] : \bar{\sigma}^k_h = \sigma^k_h, (k,h) \neq \mathcal{I}_1 \right\},
            .
        \end{align*}
        %
        For the case of $\mathcal{I}_1$, we have
        %
        \begin{align*}
            \sum_{(k,h)\in \mathcal{I}_1} 
            \min &\left\{ 1, \bar{\sigma}^k_h  \cdot \left(\bar{\sigma}^k_h  \right)^{-1}  D_{\mathcal{F}_h } \!\left( z^k_h ; \{z^\tau_h \}_{\tau=1}^{k-1}, \{ \bar{\sigma} \}_{\tau=1}^{k-1} \right)  
            \right\}
            \\
            &\leq  \sum_{(k,h)\in \mathcal{I}_1} \left(\bar{\sigma}^k_h  \right)^{-1}  D_{\mathcal{F}_h } \!\left( z^k_h ; \{z^\tau_h \}_{\tau=1}^{k-1}, \{ \bar{\sigma} \}_{\tau=1}^{k-1} \right)
            \leq \sum_{h=1}^H  \GenEluder_{\nu, K} (\mathcal{F}_h)
            = d_\nu H. \numberthis \label{eq:fine_grained_f_2_gap_I1}
        \end{align*}
        %
        For $\mathcal{I}_2$, we use the Cauchy-Schwarz inequality to get
        %
        \begin{align*}
            &\sum_{(k,h)\in \mathcal{I}_2} 
            \min \left\{ 1, \bar{\sigma}^k_h  \cdot \left(\bar{\sigma}^k_h  \right)^{-1}  D_{\mathcal{F}_h } \!\left( z^k_h ; \{z^\tau_h \}_{\tau=1}^{k-1}, \{ \bar{\sigma} \}_{\tau=1}^{k-1} \right)  
            \right\}
            \\
            &\leq \sqrt{\nu^2 KH} \cdot \sqrt{\sum_{(k,h)\in \mathcal{I}_2} \left( \bar{\sigma}^k_h \right)^{-2} D^2_{\mathcal{F}_h } \!\left( z^k_h ; \{z^\tau_h \}_{\tau=1}^{k-1}, \{ \bar{\sigma} \}_{\tau=1}^{k-1} \right)  }
            \leq \sqrt{\sum_{h=1}^H  \GenEluder_{\nu, K} (\mathcal{F}_h)}
            &= \sqrt{d_\nu H}.
            \numberthis \label{eq:fine_grained_f_2_gap_I2}
        \end{align*}
        %
        For $\mathcal{I}_3$, we have
        %
        \begin{align*}
            \sum_{(k,h)\in \mathcal{I}_3} 
            \min &\left\{ 1, \bar{\sigma}^k_h  \cdot \left(\bar{\sigma}^k_h  \right)^{-1}  D_{\mathcal{F}_h } \!\left( z^k_h ; \{z^\tau_h \}_{\tau=1}^{k-1}, \{ \bar{\sigma} \}_{\tau=1}^{k-1} \right)  
            \right\}
            \\
            &\leq \sum_{(k,h)\in \mathcal{I}_3} \left( 8 o(\delta^k_h) + \iota^2(\delta^k_h) \right) \cdot
            \min \left\{ 1, \left(\bar{\sigma}^k_h  \right)^{-2}  D^2_{\mathcal{F}_h } \!\left( z^k_h ; \{z^\tau_h \}_{\tau=1}^{k-1}, \{ \bar{\sigma} \}_{\tau=1}^{k-1} \right)  
            \right\}
            \\
            &= \BigO \left(  \left( \sqrt{\log \frac{\mathcal{N}KH}{\delta}} + \log\frac{\mathcal{N} \mathcal{N}_b KH}{\delta} \right) \cdot \sum_{h=1}^H \GenEluder_{\nu, K} (\mathcal{F}_h) \right)
            \\
            &= \BigO \left(  \left( \sqrt{\log \frac{\mathcal{N}KH}{\delta}} + \log\frac{\mathcal{N} \mathcal{N}_b KH}{\delta} \right) 
            d_\nu H\right), 
            \numberthis \label{eq:fine_grained_f_2_gap_I3}
        \end{align*}
        %
        where the inequality holds because, by dividing both sides of $ \bar{\sigma}^k_h =  2 \left( \sqrt{o(\delta^k_h)} + \iota(\delta^k_h) \right) \cdot \sqrt{  D_{\mathcal{F}_h}\! \left( z^k_h; \{z^\tau_h\}_{\tau=1}^{k-1}, \{\bar{\sigma}^\tau_h \}_{\tau=1}^{k-1} \right)}$  by $\sqrt{ \bar{\sigma}^k_h }$ and rearranging terms, we get:
        \begin{align*}
            \bar{\sigma}^k_h \leq \left( 8 o(\delta^k_h) + \iota^2(\delta^k_h) \right) D_{\mathcal{F}_h } \!\left( z^k_h ; \{z^\tau_h \}_{\tau=1}^{k-1}, \{ \bar{\sigma} \}_{\tau=1}^{k-1} \right).    
        \end{align*}
        We also use the property that $\left( \bar{\sigma}^k_h \right)^{-1} D_{\mathcal{F}_h } \!\left( z^k_h ; \{z^\tau_h \}_{\tau=1}^{k-1}, \{ \bar{\sigma} \}_{\tau=1}^{k-1} \right) \leq 1$ 
        for $(k,h) \in \mathcal{I}_3$, which follows directly from the definition of $\mathcal{I}_3$.

        For $\mathcal{I}_4$, we have
        %
        \begin{align*}
            \sum_{(k,h)\in \mathcal{I}_4} 
            \min &\left\{ 1, \bar{\sigma}^k_h  \cdot \left(\bar{\sigma}^k_h  \right)^{-1}  D_{\mathcal{F}_h } \!\left( z^k_h ; \{z^\tau_h \}_{\tau=1}^{k-1}, \{ \bar{\sigma} \}_{\tau=1}^{k-1} \right)  
            \right\}
            \\
            &\leq \sum_{(k,h)\in \mathcal{I}_4} \bar{\sigma}^k_h  \cdot \left(\bar{\sigma}^k_h  \right)^{-1}  D_{\mathcal{F}_h } \!\left( z^k_h ; \{z^\tau_h \}_{\tau=1}^{k-1}, \{ \bar{\sigma} \}_{\tau=1}^{k-1} \right)
            \\
            &= \sum_{(k,h)\in \mathcal{I}_4} 
            \sqrt{2} \iota(\delta^k_h) \sqrt{ f^k_{h,2}(z^k_h) - f^k_{h, -2}(z^k_h) }
            \cdot \left(\bar{\sigma}^k_h  \right)^{-1}  D_{\mathcal{F}_h } \!\left( z^k_h ; \{z^\tau_h \}_{\tau=1}^{k-1}, \{ \bar{\sigma} \}_{\tau=1}^{k-1} \right)
            \\
            &\leq \BigO \left( 
            \sqrt{\log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}}
            \sqrt{\sum_{k=1}^K \sum_{h=1}^H f^k_{h,2}(z^k_h) - f^k_{h, -2}(z^k_h) }
            \cdot \sqrt{d_\nu H}
            \right),
            \numberthis \label{eq:fine_grained_f_2_gap_I4}
        \end{align*}
        %
        where the last inequality holds by the Cauchy-Schwarz inequality together with the definition of $\iota(\delta^k_h)$.

        Lastly, restricting on $\mathcal{I}_5$, if the event $\Event_{\leq K}$ holds, we have
        %
        \begin{align*}
            &\sum_{(k,h)\in \mathcal{I}_5} \min \left\{ 1, \bar{\sigma}^k_h  \cdot \left(\bar{\sigma}^k_h  \right)^{-1}  D_{\mathcal{F}_h } \!\left( z^k_h ; \{z^\tau_h \}_{\tau=1}^{k-1}, \{ \bar{\sigma} \}_{\tau=1}^{k-1} \right)  
            \right\}
            \\
            &\leq \sum_{(k,h)\in \mathcal{I}_5}  \bar{\sigma}^k_h  \cdot \left(\bar{\sigma}^k_h  \right)^{-1}  D_{\mathcal{F}_h } \!\left( z^k_h ; \{z^\tau_h \}_{\tau=1}^{k-1}, \{ \bar{\sigma} \}_{\tau=1}^{k-1} \right)  
            \\
            &\leq \sqrt{ \sum_{(k,h)\in \mathcal{I}_5}  \left(\sigma^k_h \right)^2 } \cdot 
            \sqrt{ \sum_{(k,h)\in \mathcal{I}_5} \left(\bar{\sigma}^k_h  \right)^{-2}  D^2_{\mathcal{F}_h } \!\left( z^k_h ; \{z^\tau_h \}_{\tau=1}^{k-1}, \{ \bar{\sigma} \}_{\tau=1}^{k-1} \right)    } 
            \\
            &\leq \BigO \left(\sqrt{   \sum_{k,h} \mathbb{V} \left[ r_h + V^k_{h+1, 1} (s_{h+1}) \mid z^k_h \right]
            + \sum_{k,h} \left( f^k_{h,2}(z^k_h ) - f^k_{h,-2}(z^k_h) \right)
            } \cdot \sqrt{ d_\nu H} \right)
            \\
            &+ \BigO \left(\sqrt{ \sum_{k,h} \min \left\{1, D_{\mathcal{F}_h}\!\left(z^k_h ; \{ z^\tau_h \}_{\tau=1}^{k-1}, \{ \mathbf{1}^\tau \}_{\tau=1}^{k-1}  \right) \right\} 
            \sqrt{\log \frac{\mathcal{N}\mathcal{N}_b KH }{\delta} } } \cdot \sqrt{d_\nu H} \right),
            \numberthis \label{eq:fine_grained_f_2_gap_I5_mid}
        \end{align*}
        %
        where the second inequality holds by the Cauchy-Schwarz inequality, the last inequality holds by Lemma~\ref{lemma:upper_bound_variance_estimator} and the definition of Eluder dimension.
        
        To further bound the first term on the right-hand side of~\eqref{eq:fine_grained_f_2_gap_I5_mid}, we apply Lemma~\ref{lemma:sum_of_var}.
        Therefore, with probability at least $1-\delta$, we have
        %
        \begin{align*}
            &\sum_{(k,h)\in \mathcal{I}_5} \min \left\{ 1, \bar{\sigma}^k_h  \cdot \left(\bar{\sigma}^k_h  \right)^{-1}  D_{\mathcal{F}_h } \!\left( z^k_h ; \{z^\tau_h \}_{\tau=1}^{k-1}, \{ \bar{\sigma} \}_{\tau=1}^{k-1} \right)  
            \right\}
            \\
            &\leq \BigO \left(\sqrt{  H \sum_{k,h}
            \left( f^k_{h,2} (z^k_h) 
            -  f^k_{h,-2} (z^k_h)  \right)  
            + K 
            +KH^2 \delta
            + H^2 |\OOptEpisode|
            +  H^4  \log^2 \frac{KH}{\delta} 
            } \cdot \sqrt{ d_\nu H} \right)
            \\
            &+ \BigO \left( \sqrt{ \sum_{k,h} \left( f^k_{h,2}(z^k_h ) - f^k_{h,-2}(z^k_h) \right)} \cdot \sqrt{ d_\nu H} \right)
            + \BigO \left(\sqrt{ \left( d_\nu H + H \sqrt{d_\nu K} \right) 
            \sqrt{\log \frac{\mathcal{N}\mathcal{N}_b KH }{\delta} } } \cdot \sqrt{d_\nu H} \right)
            \\
            &\leq \BigO \left(\sqrt{   K 
            + KH^2 \delta
            + H^2 |\OOptEpisode|
            +  H^4  \log^2 \frac{KH}{\delta}
            } \cdot \sqrt{ d_\nu H} 
            +\sqrt{H \sum_{k,h} \left( f^k_{h,2}(z^k_h ) - f^k_{h,-2}(z^k_h) \right)} \cdot \sqrt{ d_\nu H} \right)
            \\
            &+ \BigO \left(d_\nu H^{1.5}   
            \sqrt{\log \frac{\mathcal{N}\mathcal{N}_b KH }{\delta}  } \right)
            ,\numberthis \label{eq:fine_grained_f_2_gap_I5}
        \end{align*}
        %
        where the first inequality holds by the fact that 
        \begin{align*}
            \sum_{k,h} \min \left\{1, D_{\mathcal{F}_h}\!\left(z^k_h ; \{ z^\tau_h \}_{\tau=1}^{k-1}, \{ \mathbf{1}^\tau \}_{\tau=1}^{k-1}  \right) \right\}
            &\leq d_\nu H
            \\
            \sum_{k,h} \min \left\{1, D_{\mathcal{F}_h}\!\left(z^k_h ; \{ z^\tau_h \}_{\tau=1}^{k-1}, \{ \mathbf{1}^\tau \}_{\tau=1}^{k-1}  \right) \right\}
            &\leq \sqrt{KH} \sqrt{\sum_{k,h} D^2_{\mathcal{F}_h}\!\left(z^k_h ; \{ z^\tau_h \}_{\tau=1}^{k-1}, \{ \mathbf{1}^\tau \}_{\tau=1}^{k-1}  \right)  }
            \leq H \sqrt{d_\nu K},
        \end{align*}
        and the last inequality holds by the AM-GM inequality such that 
        %
        \begin{align*}
            H \sqrt{K \cdot d_\nu \log\frac{\mathcal{N}\mathcal{N}_b KH }{\delta}}
            \leq K + H^2 d_\nu \log\frac{\mathcal{N}\mathcal{N}_b KH }{\delta}.
        \end{align*}
        %
        Combining Equation~\ref{eq:fine_grained_f_2_gap_I1},~\ref{eq:fine_grained_f_2_gap_I2},~\ref{eq:fine_grained_f_2_gap_I3},~\ref{eq:fine_grained_f_2_gap_I4}, and~\ref{eq:fine_grained_f_2_gap_I5}, we get
        %
        \begin{align*}
            &\sum_{k=1}^K\sum_{h=1}^H \min \left\{ 1,  D_{\mathcal{F}_h } \!\left( z^k_h ; \{z^\tau_h \}_{\tau=1}^{k-1}, \{ \bar{\sigma} \}_{\tau=1}^{k-1} \right)  
            \right\}
            \\
            &\leq \BigO \left(
                 \sqrt{\log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}}
                 \cdot \sqrt{d_\nu H} 
                \cdot \sqrt{H\sum_{k=1}^K \sum_{h=1}^H f^k_{h,2}(z^k_h) - f^k_{h, -2}(z^k_h) }
            \right)
            \\
            &+ \BigO \left(\sqrt{   K 
            + KH^2 \delta
            + H^2 |\OOptEpisode|
            +  H^4  \log^2 \frac{KH}{\delta}
            } \cdot \sqrt{ d_\nu H} 
            + d_\nu H^{1.5}   
            \sqrt{\log \frac{\mathcal{N}\mathcal{N}_b KH }{\delta}  } \right).
            \numberthis \label{eq:fine_grained_f_2_gap_I_all}
        \end{align*}
        %
        Now, we bound the term $\sum_{k,h} \left( f^k_{h,2}(z^k_h ) - f^k_{h,-2}(z^k_h) \right)$.
        For $k \in \OOptEpisode$, we have
        %
        \begin{align*}
            \sum_{k \in \OOptEpisode}\sum_{h=1}^H \left( f^k_{h,2}(z^k_h ) - f^k_{h,-2}(z^k_h) \right)
            = \BigO(|\OOptEpisode | H).
        \end{align*}
        %
        Otherwise, for episodes $k \in \OptEpisode$, we know that it holds true that $f^k_{h,1}(z^k_h) \geq f^k_{h,2}(z^k_h) - u_k$ by~\eqref{eq:exploration_policy}.
        Therefore, under the event $\Event_{\le K}$, we have
        %
        \begin{align*}
            &\sum_{k \in \OptEpisode}\sum_{h=1}^H  \left( f^k_{h,2} (z^k_h ) - f^k_{h,-2}(z^k_h) \right)
            \leq \sum_{k \in \OptEpisode}\sum_{h=1}^H  \left( f^k_{h,1} (z^k_h ) - f^k_{h,-2}(z^k_h) \right) 
            + H \sum_{k \in \OptEpisode} u_k
            \\
            &= \sum_{k \in \OptEpisode}\sum_{h=1}^H \left( (f^k_{h,1} - \GroundQ^{\pi_k}_h) (z^k_h ) +
            (\GroundQ^{\pi_k}_h - f^k_{h,-2})(z^k_h) \right)
            + H \sum_{k \in \OptEpisode} u_k
            \\
            &\leq 
            \sum_{k \in \OptEpisode}\sum_{h=1}^H 
             \sum_{h'=h+1}^H \sum_{a' \in A^k_{h'}} \!\!\! \left(\widetilde{\ChoiceProbability}^k_{h',1}(a'|s^k_{h'}, A^k_{h'})  - \ChoiceProbability_{h'}(a'|s^k_{h'}, A^k_{h'}) \right) f^k_{h', 1}(s^k_{h'}, a')
            \\
            &-\sum_{k \in \OptEpisode}\sum_{h=1}^H \sum_{h'=h+1}^H \sum_{a' \in A^k_{h'}} \!\!\! \left(\widetilde{\ChoiceProbability}^k_{h', -2}(a'|s^k_{h'}, A^k_{h'})  - \ChoiceProbability_{h'}(a'|s^k_{h'}, A^k_{h'}) \right) f^k_{h', -2}(s^k_{h'}, a')
            \\
            &+ 2 \sum_{k \in \OptEpisode}\sum_{h=1}^H \min\left\{ 1+\BoundFunction, \sum_{h'=h}^H b^k_{h',1}(s^k_{h'}, a^k_{h'}) \right\}
            +2 \sum_{k \in \OptEpisode}\sum_{h=1}^H
            \min\left\{ 1+\BoundFunction,\sum_{h'=h}^H b^k_{h',2}(s^k_{h'}, a^k_{h'}) \right\}
            \\
            & + \underbrace{\sum_{k \in \OptEpisode}\sum_{h=1}^H\sum_{h'=h+1}^H
            \left(\MDSp^k_{h',1} + \MDSc^k_{h', 1} - \MDSp^k_{h',-2} - \MDSc^k_{h', -2} \right)}_{\text{martingale difference sequences (MDSs)}}
            + H \sum_{k \in \OptEpisode} u_k,
            \numberthis\label{eq:fine_grained_f_2_gap_I5_K_o}
        \end{align*}
        %
        where the second inequality holds by Lemma~\ref{lemma:pess_approx_error} and~\ref{lemma:opt_approx_error}.
        
        To further the right-hand side of~\eqref{eq:fine_grained_f_2_gap_I5_K_o}, we apply Lemma~\ref{lemma:crude_mnl} (which holds with probability at least $1-2\delta$) to the first and the second terms, 
        Lemma~\ref{lemma:b_1_crude_bound} to the third term,
        Lemma~\ref{lemma:b_2_crude_bound} to the forth term,
        and we bound the fifth term using the Azuma-Hoeffding inequality (which holds with probability at least $1-4\delta$).
        As a result, absorbing the low-order terms, we obtain that        
        %
        \begin{align*}
            &\sum_{k=1}^K\sum_{h=1}^H  \left( f^k_{h,2} (z^k_h ) - f^k_{h,-2}(z^k_h) \right)
            \\
            &\leq
            \BigO \left( 
            \frac{1}{\sqrt{\kappa}} dH^2 \sqrt{K} \cdot (\log K)^{3/2} \log M \right)
            \\
            &+ \BigO \left(
            \sqrt{\log \frac{\mathcal{N}  KH}{ \delta}} \cdot \left( 
            \sqrt{\log \frac{\mathcal{N} \mathcal{N}_b KH}{ \delta}}\cdot H^2 \sqrt{d_\nu K}
           +\log \frac{\mathcal{N} \mathcal{N}_b KH}{ \delta}
           \cdot  d_\nu H^2 \right)\right)
            \\
            &+ \BigO \left(
            \sqrt{\log \frac{\mathcal{N}\mathcal{N}_b  KH}{ \delta}}\cdot   KH^2 \epsilon_b 
            + H \sqrt{KH \log \frac{KH}{\delta}}   + |\OOptEpisode | H 
            +  H \sum_{k \in \OptEpisode} u_k
            \right)
            ,            \numberthis \label{eq:fine_grained_f_2_gap_I5_K_o_2}
        \end{align*}
        %
        where the last inequality holds by the AM-GM inequality.
        
        Plugging~\eqref{eq:fine_grained_f_2_gap_I5_K_o_2} to~\eqref{eq:fine_grained_f_2_gap_I_all}, 
        we have
        %
        \begin{align*}
            &\sum_{k=1}^K\sum_{h=1}^H \min \left\{ 1, \bar{\sigma}^k_h  \cdot \left(\bar{\sigma}^k_h  \right)^{-1}  D_{\mathcal{F}_h } \!\left( z^k_h ; \{z^\tau_h \}_{\tau=1}^{k-1}, \{ \bar{\sigma} \}_{\tau=1}^{k-1} \right)  
            \right\}
            \\
            &\leq \BigO \left(
                 \sqrt{\log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}}
                 \cdot \sqrt{d_\nu H} 
                \cdot \left(\sqrt{
                    \frac{1}{\sqrt{\kappa}} d H^3 (\log K)^{3/2} \log M \cdot \sqrt{K} 
                }
                +  H^2 \sum_{k \in \OptEpisode} u_k
                \right)
            \right)
            \\
            &+ \BigO \left(
                \left(\log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta} \right)^{3/4}
                 \!\!\!\!\!
                 \cdot \sqrt{d_\nu H} 
                \cdot 
                    \sqrt{
                        \sqrt{\log \frac{\mathcal{N}  KH}{ \delta}} \cdot \left( 
                         H^3 \sqrt{d_\nu K}
                       +\sqrt{\log \frac{\mathcal{N} \mathcal{N}_b KH}{ \delta}}
                       \cdot  d_\nu H^3 \right)
                   }
            \right)
            \\
            &+ \BigO \left(
                \sqrt{\log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}}
                 \cdot \sqrt{d_\nu H} 
                \cdot 
                    \sqrt{
                        \sqrt{\log \frac{\mathcal{N}\mathcal{N}_b  KH}{ \delta}}\cdot   KH^3 \epsilon_b 
                        + H^2 \sqrt{KH \log \frac{KH}{\delta}}   
                        + |\OOptEpisode | H^2                         
                   }
            \right)
            \\
            &+ \BigO \left(\sqrt{   K 
            + KH^2 \delta
            % + H^2 |\OOptEpisode|
            +  H^4  \log^2 \frac{KH}{\delta}
            } \cdot \sqrt{ d_\nu H} 
            + d_\nu H^{1.5}   
            \sqrt{\log \frac{\mathcal{N}\mathcal{N}_b KH }{\delta}  } \right)
            \\
            &\leq \BigO \left(
            \sqrt{\log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}}
                 \cdot \sqrt{d_\nu H} 
                \cdot
                \sqrt{
                \frac{K}{\log \frac{\mathcal{N}\mathcal{N}_b KH }{\delta} }
                + \frac{1}{\kappa}d^2 H^6 \left( (\log K)^{3/2} \log M \right)^2
                \cdot \log \frac{\mathcal{N}\mathcal{N}_b KH }{\delta} 
                }
            \right)
            \\
            &+ \BigO \left(
                \sqrt{\log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}}
                 \cdot \sqrt{d_\nu H} 
                \cdot
                \sqrt{
                d_\nu H^6 \log \frac{\mathcal{N} KH}{\delta}
                \left(\log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}\right)^2
                + H^2 \sum_{k \in \OptEpisode} u_k
                }
            \right)
            \\
            &+ \BigO \left(
                \sqrt{\log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}}
                 \cdot \sqrt{d_\nu H} 
                \cdot
                \sqrt{
                H^2 |\OOptEpisode|
                }
                + KH \epsilon_b
                +  \sqrt{ d_\nu H} \cdot \sqrt{    
                    KH^2 \delta
                    } 
                \right)
            \\
            &= \BigO \left(
                \sqrt{d_\nu H K} 
                + \frac{1}{\sqrt{\kappa}} d H^{7/2} \sqrt{d_\nu}  (\log K)^{3/2} \log M
                \cdot \log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}
            \right)
            \\
            &+ \BigO \left(
                d_\nu H^{7/2} 
                \sqrt{\log \frac{\mathcal{N} KH}{\delta}}
                \cdot \left(\log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}\right)^{3/2}
            \right)
            \\
            &+ \BigO \left( 
                \sqrt{\log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}}
                 \cdot \sqrt{d_\nu H} 
                \cdot
                \left(\sqrt{ H^2 \sum_{k \in \OptEpisode} u_k}
                +\sqrt{H^2 |\OOptEpisode|} \right)
                %
                +KH \epsilon_b
                +  \sqrt{ d_\nu     
                    KH^3 \delta
                    } \right)
            , \numberthis \label{eq:fine_grained_f_2_gap_D_f}
        \end{align*}
        %
        where the second inequality holds by applying the AM-GM inequality and absorbing the lower-order terms.

        Finally, plugging~\eqref{eq:fine_grained_f_2_gap_D_f} to~\eqref{eq:fine_grained_f_2_gap_b_bound}, we derive that
        %
        \begin{align*}
            &\sum_{k=1}^K \sum_{h=1}^H \min \left\{ 1+\BoundFunction, b^k_{h,1} (z^k_h) \right\}
            \\
            &\leq \BigO \left(
                \sqrt{d_\nu H K \cdot \log  \frac{\mathcal{N}KH}{\delta}} 
                    + 
                    \frac{1}{\sqrt{\kappa}} d H^{7/2} \sqrt{d_\nu}  (\log K)^{3/2} \log M
                \cdot \log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}
                \cdot \sqrt{\log \frac{\mathcal{N}KH}{\delta}}
                \right)
            \\
            &+ \BigO\left( 
            d_\nu H^{7/2} 
                \log \frac{\mathcal{N} KH}{\delta}
                \cdot \left(\log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}\right)^{3/2} 
                +
                \sqrt{\log \frac{\mathcal{N}KH}{\delta}}
                \cdot 
                \left(KH \epsilon_b
                    +  \sqrt{ d_\nu     
                        KH^3 \delta}
                \right)
            \right)
            \\
            &+ \BigO \left(
                \sqrt{\log \frac{\mathcal{N}KH}{\delta} \log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}}
                 \cdot \sqrt{d_\nu H} 
                \cdot
                \left(\sqrt{ H^2 \sum_{k \in \OptEpisode} u_k}
                +  \sqrt{H^2 |\OOptEpisode|}
                    \right)
            \right)
            . 
        \end{align*}
        %
        This concludes the proof of Lemma~\ref{lemma:b_1_fine_grained_bound}.
        %
    \end{proof}
\end{lemma}

\begin{lemma} [Bounding size of $\OOptEpisode$] \label{lemma:bounding_Too}
Suppose $\nu \leq 1$ and we set
    \begin{align*}
        u_k 
        &\geq C \cdot 
        \!\Bigg( \frac{
        \sqrt{\log \frac{\mathcal{N}KH}{\nu \delta}}
        \cdot \left( \log \frac{\mathcal{N}\mathcal{N}_b KH}{\nu \delta} \cdot H^{5/2} \sqrt{d_\nu} 
        + \sqrt{k}H \epsilon_b \right)
        }{\sqrt{k}}
        + \frac{dH^{5/2} (\log K)^{3/2} \log M \sqrt{ \log \frac{\mathcal{N}\mathcal{N}_b KH}{\nu \delta} } 
        }{\sqrt{k}}
        \Bigg)
        ,
    \end{align*}
    for some large enough constant $0 < C  < \infty$, 
    when the event $\Event_{\leq K}$ holds true,
    then with probability at least $1-2\delta$, it holds that
    %
    \begin{align*}
        |\OOptEpisode|
        \leq 
        \BigO \left(
             \frac{K}{H^3 \log \frac{\mathcal{N}\mathcal{N}_b KH}{\nu \delta}} 
             + \frac{d_\nu}{H^3} 
             + \frac{d^4 ((\log K)^{3/2} \log M)^4 }{\kappa^2 d_\nu H^3 
                 \cdot 
                 \log \frac{\mathcal{N} KH}{\nu \delta} 
                 \left(\log  \frac{\mathcal{N}\mathcal{N}_b KH}{\nu \delta}\right)^2
                 }
             \right).
    \end{align*}
    %
    \begin{proof}[Proof of Lemma~\ref{lemma:bounding_Too}]
        By the definition of $h_k$, for each $k \in \OOptEpisode$, we have $f^k_{h_k,2} (s^k_{h_k}, a^k_{h_k}) \geq f^k_{h_k,1}(s^k_{h_k}, a^k_{h_k}) + u_k$, which implies that
        %
        \begin{align*}
            &\sum_{k \in \OOptEpisode}(f^k_{h_k,2} - f^k_{h_k,1})(s^k_{h_k}, a^k_{h_k})
            \geq \sum_{k \in \OOptEpisode} u_k
            \\
            &\geq 
            C \cdot \Bigg(
            \sqrt{\log \frac{\mathcal{N}KH}{\nu \delta}}
            \cdot \left( 
                \log \frac{\mathcal{N}\mathcal{N}_b KH}{\nu \delta} \cdot 
                H^{5/2} \sqrt{d_\nu}
                \cdot \frac{|\OOptEpisode|}{\sqrt{K}}
            + |\OOptEpisode| H \epsilon_b
            \right)
            \\
            &+
             d H^{5/2} (\log K)^{3/2} \log M 
            \sqrt{ \log \frac{\mathcal{N}\mathcal{N}_b KH}{\nu \delta} }
            \cdot \frac{|\OOptEpisode|}{\sqrt{K}} 
            \Bigg).
            \numberthis \label{eq:bounding_Too_lower}
        \end{align*}
        %
        Furthermore, under the event $\Event_{\leq K}$, by Lemma~\ref{lemma:monotonocity}, it holds that $f^k_{h,1} (s_h,a_h)\geq \GroundQ^\star_h (s_h,a_h)\geq \GroundQ^{\pi_k}_h (s_h,a_h)$  for all $(s_h,a_h) \in \SetOfStates \times \SetOfGroundActions$.
        Thus, we get
        %
        \begin{align*}
            &\sum_{k \in \OOptEpisode} (f^k_{h_k,2} - f^k_{h_k,1})(s^k_{h_k}, a^k_{h_k})
            \leq \sum_{k \in \OOptEpisode} (f^k_{h_k,2} - \GroundQ^{\pi_k}_{h_k})(s^k_{h_k}, a^k_{h_k})
            \\
            &\leq
            \sum_{k \in \OOptEpisode} \sum_{h'=h_k+1}^H \sum_{a' \in A^k_{h',2}} \!\!\! \left(\widetilde{\ChoiceProbability}^k_{h',2}(a'|s^k_{h'}, A^k_{h',2})  - \ChoiceProbability_{h'}(a'|s^k_{h'}, A^k_{h',2}) \right) f^k_{h', 2}(s^k_{h'}, a')
            \\
            &+ 2 \sum_{k \in \OOptEpisode}\sum_{h'=h_k}^H 
            \min \left\{ 1+\BoundFunction, b^k_{h',1}(s^k_{h'}, a^k_{h'}) \right\}
            +2 \sum_{k \in \OOptEpisode}\sum_{h'=h_k}^H
            \min \left\{ 1+\BoundFunction, b^k_{h',2}(s^k_{h'}, a^k_{h'}) \right\}
            +\sum_{k \in \OOptEpisode} \sum_{h'=h_k+1}^H 
            \left(\MDSp^k_{h',2}
            + \MDSc^k_{h', 2} \right)
            \\
            &\leq \BigO \left(
            dH \sqrt{|\OOptEpisode|} \cdot (\log K)^{3/2} \log M
            + \frac{1}{\kappa} d^2 H \left((\log K)^{3/2} \log M \right)^2
            + \sqrt{|\OOptEpisode|  H \log \frac{KH}{\delta}}
            \right)
            \\
            &+ \BigO \left(\sqrt{\log \frac{\mathcal{N}  KH}{\nu \delta}} \cdot \left( 
            \sqrt{\log \frac{\mathcal{N} \mathcal{N}_b KH}{\nu \delta}}\cdot H \sqrt{d_\nu |\OOptEpisode|}
           +\log \frac{\mathcal{N} \mathcal{N}_b KH}{\nu \delta}
           \cdot  d_\nu H
            + |\OOptEpisode|H \epsilon_b \right)  \right),
            \numberthis \label{eq:bounding_Too_upper}
        \end{align*}
        where in the first inequality, we note that $A^k_{h'} = A^k_{h',2}$ for $h' \geq h_k$,
        the second inequality follows from~\ref{lemma:overly_opt_approx_error}.
        And for the third inequality, we apply Lemma~\ref{lemma:tight_mnl} to the first term,
        Lemma~\ref{lemma:b_1_crude_bound} to the second term,
        Lemma~\ref{lemma:b_2_crude_bound} to the third term.
        Finally, we bound the last term using the Azuma-Hoeffding inequality with probability at least $1-2\delta$.

        Thus, in order for the two inequalities~\ref{eq:bounding_Too_lower} and~\ref{eq:bounding_Too_upper} to hold simultaneously,
         the following condition must be satisfied:
         \begin{align*}
             |\OOptEpisode | 
             \leq \BigO \Bigg( 
             \max \Bigg\{
             \frac{K}{ H^3  \log \frac{\mathcal{N}\mathcal{N}_b KH}{\nu \delta} },
             % \\
             \frac{
             \left(d_\nu   \sqrt{ \log \frac{\mathcal{N} KH}{\nu \delta} }  \log \frac{\mathcal{N}\mathcal{N}_b KH}{\nu \delta} + \frac{1}{\kappa} d^2 ((\log K)^{3/2} \log M)^2 \right)
             \cdot \sqrt{K}
             }{H^{3/2}  \sqrt{ \log \frac{\mathcal{N}\mathcal{N}_b KH}{\nu \delta} }  \left( 
             \sqrt{d_\nu  \log \frac{\mathcal{N} KH}{\nu \delta} \log \frac{\mathcal{N}\mathcal{N}_b KH}{\nu \delta}
             }
             + d (\log K)^{3/2} \log M
             \right) }
             \Bigg\}
             \Bigg).
         \end{align*}
         Using the AM-GM inequality, we can further bound the second term inside the max operation.
         %
         \begin{align*}
             &
             \BigO \left(
                 \frac{
                 \left(d_\nu   \sqrt{ \log \frac{\mathcal{N} KH}{\nu \delta} }  \log \frac{\mathcal{N}\mathcal{N}_b KH}{\nu \delta} + \frac{1}{\kappa} d^2 ((\log K)^{3/2} \log M)^2 \right)
                 \cdot \sqrt{K}
                 }{H^{3/2}  \sqrt{ \log \frac{\mathcal{N}\mathcal{N}_b KH}{\nu \delta} }  \left( 
                 \sqrt{d_\nu  \log \frac{\mathcal{N} KH}{\nu \delta} \log \frac{\mathcal{N}\mathcal{N}_b KH}{\nu \delta}
                 }
                 + d (\log K)^{3/2} \log M
                 \right) }
             \right)
             \\
             &\leq \BigO \left(
             \frac{K}{H^3 \log \frac{\mathcal{N}\mathcal{N}_b KH}{\nu \delta}} 
             %
             + \left(\frac{d_\nu   \sqrt{ \log \frac{\mathcal{N} KH}{\nu \delta} }  \log \frac{\mathcal{N}\mathcal{N}_b KH}{\nu \delta} + \frac{1}{\kappa} d^2 ((\log K)^{3/2} \log M)^2
             }{
             H^{3/2}   
                 \sqrt{d_\nu  \log \frac{\mathcal{N} KH}{\nu \delta}}
                 \log \frac{\mathcal{N}\mathcal{N}_b KH}{\nu \delta}
                 %
             } \right)^2
             \right)
             \\
             &\leq \BigO \left(
             \frac{K}{H^3 \log \frac{\mathcal{N}\mathcal{N}_b KH}{\nu \delta}} 
             +  \left( 
             \sqrt{ \frac{d_\nu}{H^3}   }
             + \frac{d^2 ((\log K)^{3/2} \log M)^2 }{
             \kappa \cdot  H^{3/2}   
                 \sqrt{d_\nu  \log \frac{\mathcal{N} KH}{\nu \delta}}
                 \log \frac{\mathcal{N}\mathcal{N}_b KH}{\nu \delta}}
             \right)^2
             \right)
             \\
             &\leq 
             \BigO \left(
             \frac{K}{H^3 \log \frac{\mathcal{N}\mathcal{N}_b KH}{\nu \delta}} 
             + \frac{d_\nu}{H^3} 
             + \frac{d^4 ((\log K)^{3/2} \log M)^4 }{\kappa^2 d_\nu H^3 
                 \cdot 
                 \log \frac{\mathcal{N} KH}{\nu \delta} 
                 \left(\log  \frac{\mathcal{N}\mathcal{N}_b KH}{\nu \delta}\right)^2
                 }
             \right),
         \end{align*}
         where the second inequality also follow from the AM-GM inequality
         and the last inequality holds due to the fact that $(a + b)^2 \leq 2a^2 +2 b^2$ for any $a,b \in \RR^{+}$.
         This concludes the proof.
    \end{proof}
\end{lemma}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Theorem~\ref{thm:regret_upper_general_main}}
\label{app:subsec_proof_upper_general}
%
Now, we are ready to provide the proof of Theorem~\ref{thm:regret_upper_general_main}.
To start, we formally restate the thorem.
%
\begin{theorem}[Restatement of Theorem~\ref{thm:regret_upper_general_main}, Regret upper bound of \AlgName{}] 
\label{thm:regret_upper_general_app} 
Suppose Assumptions~\ref{assum:MNL_click_model} and~\ref{assum:completeness} hold.
We assume that we have the generalized Eluder dimension $\operatorname{dim}_{\nu,K} (\mathcal{F}_h)$, for $h \in [H]$, as  defined in Definition~\ref{def:gen_eluder} with $\rho = 1$,
and access to a consistent bonus oracle $\BonusOracle$ satisfying Definition~\ref{def:bonus_oracle} with $\epsilon_b = \BigO(1/KH)$.
Let $d_\nu = \frac{1}{H} \sum_{h=1}^H  \operatorname{dim}_{\nu,K} (\mathcal{F}_h)$ with $\nu = \sqrt{1/KH}$, and set
$u_k = \BigO \big(\sqrt{\log \mathcal{N}} \cdot ( \log \mathcal{N} \mathcal{N}_b \cdot H^{5/2} \sqrt{d_\nu}   + d H^{5/2} \sqrt{\log \mathcal{N} \mathcal{N}_b} ) /\sqrt{K} \big)$.
Then, for any $\delta < 1/(H^2 + 15)$, with probability at least $1-\delta$, the regret of ~\AlgName{} is upper-bounded by:
%
    \begin{align*}
        \Regret(\MDP, K)
        &= \BigO \left(  
        d\sqrt{ H K} 
                (\log K)^{3/2}
                \log M
        + \sqrt{d_\nu H K \cdot \log \frac{\mathcal{N}KH}{\delta}}
        + \frac{1}{\kappa} d^2 H^2 (\log K)^3 (\log M)^2
        \right)
        \\
        &+ \BigO \left(
                d_\nu H^5  \log \frac{\mathcal{N} KH}{\delta}
                \cdot \left(\log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}\right)^{2}
            + \sqrt{\log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}} \cdot KH \epsilon_b 
                \right)
        ,   
    \end{align*}
    where $d$ is the feature dimension of the MNL preference model, $\mathcal{N}$ is the maximum size of the function class, i.e., $\mathcal{N} = \max_{h \in [H]} |\mathcal{F}_h| $, and $\mathcal{N}_b$ is the size of the bonus function class, i.e., $\mathcal{N}_b = |\mathcal{W}|$.
\end{theorem}

%
\begin{proof} [Proof of Theorem~\ref{thm:regret_upper_general_main}]
    When the event $\EventMNL \mcap \Event_{\leq K}$ happens (with probability at least $1-2\delta$), we can bound the regret as follows:
    %
    \begin{align*}
        \Regret(\MDP, K)
        &= \sum_{k=1}^K \left( V^\star_1  - V^{\pi_k}_1  \right)(s^k_{1})
        \leq \sum_{k=1}^K \left( V^k_1  - V^{\pi_k}_1  \right)(s^k_{1})
        = \sum_{k=1}^K \left( Q^k_1  - Q^{\pi_k}_1  \right)(s^k_{1}, A^k_1)
        \\
        &\leq \BigO(1)
        + \sum_{k=2}^K \left( Q^k_1  - Q^{\pi_k}_1  \right)(s^k_{1}, A^k_1)
        \\
        &= \BigO(1)
        + \sum_{k \in \OptEpisode \setminus \{ 1\}} \left( Q^k_1  - Q^{\pi_k}_1  \right)(s^k_{1}, A^k_1)
        + \sum_{k \in \OOptEpisode \setminus \{ 1\}} \left( Q^k_1  - Q^{\pi_k}_1  \right)(s^k_{1}, A^k_1) 
        , \numberthis \label{eq:regret_decomposition}
    \end{align*}
    %
    where the first inequality holds by Lemma~\ref{lemma:optimism}.
    
    For $k \in \OptEpisode \setminus \{ 1\}$, recall that 
    $Q^k_{h} (s, A^k_h) = \sum_{a_1 \in A^k_h  } \widetilde{\ChoiceProbability}^k_{h,1}(a_1 | s, A^k_h) f^k_{h,1}(s^k_h,a_1)$ for all $h \in [H]$, as defined in~\eqref{eq:optimistic_Q}.
    Therefore, we have
    %
    \begin{align*}
        \left( Q^k_1  - Q^{\pi_k}_1  \right)(s^k_{1}, A^k_1)
        &= \sum_{a_1 \in A^k_{1} } \widetilde{\ChoiceProbability}^k_{1,1} (a_1 | s^k_1, A^k_{1}) f^k_{1,1}(s^k_h, a_1) 
        - \sum_{a_1 \in A^k_{1} } \ChoiceProbability_1(a_1 | s^k_1, A^k_1) \GroundQ^{\pi_k}_1 (s^k_1, a_1)
        \\
        &= \sum_{a_1 \in A^k_{1} } 
        \left(\widetilde{\ChoiceProbability}^k_{1,1} (a_1 | s^k_1, A^k_{1})
            -  \ChoiceProbability_1(a_1 | s^k_1, A^k_1)
        \right)
        f^k_{1,1}(s^k_h, a_1) 
        \\
        &+ \sum_{a_1 \in A^k_{1} } \ChoiceProbability_1(a_1 | s^k_1, A^k_1) 
        \left( f^k_{1,1}- \GroundQ^{\pi_k}_1   \right)(s^k_1, a_1)
        \\
        &= \sum_{a_1 \in A^k_{1} } 
        \left(\widetilde{\ChoiceProbability}^k_{1,1} (a_1 | s^k_1, A^k_{1})
            -  \ChoiceProbability_1(a_1 | s^k_1, A^k_1)
        \right) f^k_{1,1}(s^k_h, a_1) 
        + \left( f^k_{1,1}- \GroundQ^{\pi_k}_1   \right)(s^k_1, a_1)
        \\
        &+  \EE_{\ChoiceProbability}\left[ \left( f^k_{1,1}- \GroundQ^{\pi_k}_1   \right)(s^k_1, a_1) \mid  s^k_1, A^k_1 \right]
        - \left( f^k_{1,1}- \GroundQ^{\pi_k}_1   \right)(s^k_1, a)
        .
        % 
    \end{align*}
    %
    Then, by applying Lemma~\ref{lemma:opt_approx_error} with $h_k = H+1$, we have
    %
    \begin{align*}
        \left( Q^k_1  - Q^{\pi_k}_1  \right)(s^k_{1}, A^k_1)
        &\leq \sum_{h=1}^H \sum_{a_h \in A^k_{h} } 
        \left(\widetilde{\ChoiceProbability}^k_{h,1} (a_h | s^k_h, A^k_{h})
            -  \ChoiceProbability_h(a_h | s^k_h, A^k_h)
        \right) f^k_{h,1}(s^k_h, a_h) 
        \\
        & + 2 \sum_{h=1}^H b^k_{h,1}(s^k_{h}, a^k_{h})
        + \sum_{h=1}^{H} \MDSc^k_{h, 1} 
        +  \sum_{h=2}^{H} \MDSp^k_{h,1}
        ,
        \numberthis \label{eq:regret_optepisode}
    \end{align*}
    where $\MDSp^k_{h,1} = \EE_{\TransitionProbability}\left[  (V^k_{h,1} -  V^{\pi_k}_{h})(s_{h})  \mid s^k_{h-1}, a^k_{h-1} \right] - (V^k_{h,1} -  V^{\pi_k}_{h})(s^k_{h})$
    and $\MDSc^k_{h,1} = \EE_{\ChoiceProbability}
         \left[ 
         \left(f^k_{h,1}
             -\GroundQ^{\pi_k}_{h} \right)(s^k_{h}, a_{h}) \mid s^k_{h}, A^k_{h}  \right] - \left(f^k_{h,1}
             -\GroundQ^{\pi_k}_{h} \right)(s^k_{h}, a^k_{h})$.
    %
    
    Now, we consider the case where $k \in \OOptEpisode \setminus \{ 1\}$.
    In this cases, note that $h_k \in [H]$.
    Similar to the above analysis, by Lemma~\ref{lemma:opt_approx_error}, we get
    %
    \begin{align*}
        \left( Q^k_1  - Q^{\pi_k}_1  \right)(s^k_{1}, A^k_1)
        &\leq \sum_{h=1}^{h_k -1} \sum_{a_h \in A^k_{h} } 
            \left(\widetilde{\ChoiceProbability}^k_{h,1} (a_h | s^k_h, A^k_{h})
                -  \ChoiceProbability_h(a_h | s^k_h, A^k_h)
            \right) f^k_{h,1}(s^k_h, a_h)
        \\
        &+ \sum_{h=h_k}^{H} \sum_{a_h \in A^k_{h} } 
            \left(\widetilde{\ChoiceProbability}^k_{h,2} (a_h | s^k_h, A^k_{h})
                -  \ChoiceProbability_h(a_h | s^k_h, A^k_h)
            \right) f^k_{h,2}(s^k_h, a_h)
        \\
        &+ 2 \sum_{h=1}^H b^k_{h,1}(s^k_{h}, a^k_{h})
        + 2 \sum_{h=h_k}^H b^k_{h,2}(s^k_{h}, a^k_{h})
        + \sum_{h=1}^{h_k-1} \MDSc^k_{h, 1} 
        +  \sum_{h=2}^{h_k-1} \MDSp^k_{h,1}
        + \sum_{h=h_k}^{H} \MDSc^k_{h, 2} 
        +  \sum_{h=h_k}^{H} \MDSp^k_{h,2}, \numberthis \label{eq:regret_ooptepisode}
    \end{align*}
    %
    where $  \MDSp^k_{h,2} = \EE_{\TransitionProbability}\left[  (V^k_{h,2} -  V^{\pi_k}_h)(s_{h})  \mid s^k_{h-1}, a^k_{h-1} \right] - (V^k_{h,2} -  V^{\pi_k}_h)(s^k_{h})$
    and $\MDSc^k_{h,2} = \EE_{\ChoiceProbability}
        \left[ 
        \left(f^k_{h,2}
         -\GroundQ^{\pi_k}_{h} \right)(s^k_{h}, a_{h}) \mid s^k_{h}, A^k_{h}  \right] - \left(f^k_{h,2}
         -\GroundQ^{\pi_k}_{h} \right)(s^k_{h}, a^k_{h})$.
    %
    
    Plugging~\eqref{eq:regret_optepisode} and~\eqref{eq:regret_ooptepisode} into~\eqref{eq:regret_decomposition}, 
    and denoting $J(k,h):\! [K] \times [H] \!\rightarrow\! \{1,2\}$ as the one-to-one function that maps from $[K]\times [H]$ to the index set $\{1,2\}$ such that $A^k_h = A^k_{h, J(k,h)} \in  \argmax_{A \in \SetOfActions}  \sum_{a \in A  } \widetilde{\ChoiceProbability}^k_{h,J(k,h)}(a | s^k_h, A) f^k_{h, J(k,h)}(s^k_h,a)$, 
    we obtain that
    %
    \begin{align*}
        &\Regret(\MDP, K)
        \\
        &\leq \BigO(1)
        + \sum_{k=2}^K \sum_{h=1}^H \sum_{a_h \in A^k_{h} } 
        \left(\widetilde{\ChoiceProbability}^k_{h,J(k,h)} (a_h | s^k_h, A^k_{h})
            -  \ChoiceProbability_h(a_h | s^k_h, A^k_h)
        \right) f^k_{h,J(k,h)}(s^k_h, a_h) 
        \\
        &+ 2 \sum_{k=2}^K \sum_{h=1}^H \min \left\{ 1+ \BoundFunction, b^k_{h,1}(s^k_{h}, a^k_{h}) \right\}
        + 2 \sum_{k \in \OOptEpisode  } \sum_{h=h_k}^H \min \left\{ 1+ \BoundFunction, b^k_{h,2}(s^k_{h}, a^k_{h}) \right\}
        \\
        &+ \sum_{k=2}^K \sum_{h=1}^{h_k-1} \MDSc^k_{h, 1} 
        +  \sum_{k=2}^K\sum_{h=2}^{h_k-1} \MDSp^k_{h,1}
        + \sum_{k=2}^K\sum_{h=h_k}^{H} \MDSc^k_{h, 2} 
        +  \sum_{k=2}^K\sum_{h=h_k}^{H} \MDSp^k_{h,2}.
    \end{align*}
    %
    Now, by applying the results from Lemma~\ref{lemma:tight_mnl} to bound the first term (which holds with probability at least $1-2\delta$),
    Lemma~\ref{lemma:b_1_fine_grained_bound} for the second term (which holds with probability at least $1-7\delta$), 
    Lemma~\ref{lemma:b_2_crude_bound} for the third term, 
    and applying the Azuma-Hoeffding inequality to the remaining terms (which holds with probability at least $1-4\delta$), we get
    %
    \begin{align*}
        &\Regret(\MDP, K)
        \\
        &\leq \BigO\left(
                d\sqrt{ H K|} 
                (\log K)^{3/2}
                \log M
                +
                \frac{1}{\kappa}
                d^2 H 
                (\log K)^3
                (\log M)^2
            \right)
        \\
        &+ \BigO \left(
                \sqrt{d_\nu H K \cdot \log  \frac{\mathcal{N}KH}{\delta}} 
                    + 
                    \frac{1}{\sqrt{\kappa}} d H^{7/2} \sqrt{d_\nu}  (\log K)^{3/2} \log M
                \cdot \log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}
                \cdot \sqrt{\log \frac{\mathcal{N}KH}{\delta}}
                \right)
            \\
            &+ \BigO\left( 
            d_\nu H^{7/2} 
                \log \frac{\mathcal{N} KH}{\delta}
                \cdot \left(\log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}\right)^{3/2} 
                +
                \sqrt{\log \frac{\mathcal{N}KH}{\delta}}
                \cdot 
                \left(KH \epsilon_b
                    +  \sqrt{ d_\nu     
                        KH^3 \delta}
                \right)
            \right)
            \\
            &+ \BigO \left(
                \sqrt{\log \frac{\mathcal{N}KH}{\delta} \log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}}
                 \cdot \sqrt{d_\nu H} 
                \cdot
                \left(\sqrt{ H^2 \sum_{k \in \OptEpisode} u_k}
                +  \sqrt{H^2 |\OOptEpisode|}
                    \right)
            \right)
            \\
            &+ \BigO \left(
                \sqrt{\log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}} \cdot \left(  H \sqrt{d_\nu |\OOptEpisode|} 
                + |\OOptEpisode|H \epsilon_b \right) 
                \right)
        \\
        %%
        &\leq \BigO\left(
                d\sqrt{ H K|} 
                (\log K)^{3/2}
                \log M
                +
                \frac{1}{\kappa}
                d^2 H 
                (\log K)^3
                (\log M)^2
            \right)
        \\
        &+ \BigO \left(
                    \sqrt{d_\nu H K \cdot \log \frac{\mathcal{N}KH}{\delta}} 
                    + \frac{1}{\sqrt{\kappa}} d H^{7/2} \sqrt{d_\nu}  (\log K)^{3/2} \log M
                    \cdot \log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}
                    \cdot \sqrt{\log \frac{\mathcal{N}KH}{\delta} }
                \right) 
            \\
            &+ \BigO \left(
                d_\nu H^{7/2} 
                \log \frac{\mathcal{N} KH}{\delta}
                \cdot \left(\log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}\right)^{3/2}      
                +  \sqrt{\log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}} \cdot 
                    \left(KH \epsilon_b  
                    + \sqrt{ d_\nu     
                        KH^3 \delta}\right)
                \right)
            \\
            &+ \BigO\left( 
                    \sqrt{\log \frac{\mathcal{N}KH}{\delta} \log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}}
                 \cdot \sqrt{d_\nu H} 
                \cdot\!\!
                    \sqrt{ H^2 \sum_{k \in \OptEpisode} u_k
                    }
            \right)
            %
            , \numberthis \label{eq:thm1_regret_mid}
    \end{align*}
    %
    where the second inequality holds by Lemma~\ref{lemma:bounding_Too} with probability at least $1-2\delta$, 
    and use the fact that $|\OOptEpisode|H \epsilon_b \leq KH \epsilon_b$.
    Now, we apply the AM-GM inequality to the term $\BigO\left(
        \frac{1}{\sqrt{\kappa}} d H^{7/2} \sqrt{d_\nu}  (\log K)^{3/2} \log M
        \cdot \log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}
        \cdot \sqrt{\log \frac{\mathcal{N}KH}{\delta} }
    \right)$, thus we get
    %
    \begin{align*}
        &\BigO\left(
        \frac{1}{\sqrt{\kappa}} d H^{7/2} \sqrt{d_\nu}  (\log K)^{3/2} \log M
        \cdot \log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}
        \cdot \sqrt{\log \frac{\mathcal{N}KH}{\delta} }
    \right)
    \\
    &\leq
    \BigO \left(
        \frac{1}{\kappa}d^2 H^2 (\log K)^3 (\log M )^2
        + d_\nu H^5  \left(\log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}\right)^2
        \cdot
        \log \frac{\mathcal{N}KH}{\delta}
    \right).
    \numberthis \label{eq:thm1_regret_1}
    \end{align*}
    %
    Furthermore, by substituting the chosen values of $u_k$ and applying the AM-GM inequality, we get
    %
    \begin{align*}
        &\BigO\left( 
                    \sqrt{\log \frac{\mathcal{N}KH}{\delta} \log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}}
                 \cdot \sqrt{d_\nu H} 
                \cdot\!\!
                    \sqrt{ H^2 \sum_{k \in \OptEpisode} u_k
                    }
        \right)
        \\
        &= \BigO\left( 
           \sqrt{d_\nu H K \cdot \log \frac{\mathcal{N}KH}{\delta}}         
           + d^2 H^2 (\log K)^3 (\log M )^2
           + d_\nu H^5 \log \frac{\mathcal{N} KH}{\delta}
                \cdot \left(\log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}\right)^{2}
        \right)
        \\
        &+ \BigO\left(
            \sqrt{\log \frac{\mathcal{N} KH}{\delta}} \cdot KH \epsilon_b 
        \right)
        .
        \numberthis \label{eq:thm1_regret_2}
    \end{align*}
    %
    Then, by plugging~\eqref{eq:thm1_regret_1} and~\eqref{eq:thm1_regret_2} into~\eqref{eq:thm1_regret_mid}, and
     setting $\delta < \frac{1}{H^2 + 15}$,
    we derive that
    %
    \begin{align*}
        \Regret(\MDP, K)
        &= \BigO \left(  
        d\sqrt{ H K} 
                (\log K)^{3/2}
                \log M
        + \sqrt{d_\nu H K \cdot \log \frac{\mathcal{N}KH}{\delta}}
        + \frac{1}{\kappa} d^2 H^2 (\log K)^3 (\log M)^2
        \right)
        \\
        &+ \BigO \left(
                d_\nu H^5  \log \frac{\mathcal{N} KH}{\delta}
                \cdot \left(\log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}\right)^{2}
            + \sqrt{\log \frac{\mathcal{N} \mathcal{N}_b KH}{\delta}} \cdot KH \epsilon_b 
                \right)
            .
    \end{align*}
    %
    We conclude the proof of Theorem~\ref{thm:regret_upper_general_main}.
    %
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Technical Lemmas}
% \label{app:subsec_technical_lemma}


% \begin{lemma} [Freedman’s inequality variant, Corollary 18 of~\citealt{agarwal2023vo}]
% \label{lemma:freedman_variant}
%     Let $M > m > 0$, $V > v > 0$ be constants, and $\{x_i\}_{i \in [t]}$ be a stochastic process adapted to a filtration $\{\mathcal{H}_i\}_{i \in [t]}$. Suppose $\mathbb{E}[x_i | \mathcal{H}_{i-1}] = 0$, $|x_i| \leq M$, and $\sum_{i=1}^t \mathbb{E}[x_i^2 | \mathcal{H}_{i-1}] \leq V^2$ almost surely. Then for any $\delta, \epsilon > 0$, let 
%     $
%     \iota = \sqrt{\frac{2 \log\left(\frac{2 \log(V/v) + 2}{\delta}\right) \cdot (\log(M/m) + 2)}{\delta}}.
%     $
%     Then, we have 
%     \[
%     \mathbb{P}\left( \sum_{i =1}^t x_i > \iota \sqrt{2 \left( 2 \sum_{i =1}^t \mathbb{E}[x_i^2 | \mathcal{H}_{i-1}] + v^2 \right) } 
%     + \frac{2}{3} \iota^2 \left( 2 \max_{i \in [t]} |x_i| + m \right) \right) \leq \delta.
%     \]
% \end{lemma}


% \begin{lemma} [Adapted version using LTV, Proposition 20 of~\citealt{agarwal2023vo}] \label{lemma:adapted_LTV}
%     Suppose at episode $k$, the agent explores based on rule~\ref{eq:optimistic_Q}.
%     For simplicity, we denote $\EE [ \cdot | s^k_h, a^k_h] = \EE_{s_{h+1} \sim \TransitionProbability_h(\cdot | s^k_h, a^k_h)} [ \cdot | s^k_h, a^k_h]  $ and $\mathbb{V}[\cdot | s^k_h, a^k_h] =  \mathbb{V}_{s_{h+1} \sim \TransitionProbability_h(\cdot | s^k_h, a^k_h)} [ \cdot | s^k_h, a^k_h] $.
%     Then, conditioning on the past $\mathcal{H}^{k-1}_H = \sigma \left( s^1_1, A^1_1, a^1_1, r^1_1, \dots,  s^{k-1}_H, A^{k-1}_H, r^{k-1}_H, s^{k-1}_{H+1} \right)$, we have
%     %
%     \begin{align*}
%         \EE \bigg[ \sum_{h=1}^H \VV [  r_h &+ V^k_{h+1,1} (s_{h+1})   | s^k_h, a^k_h ]  \mid  \mathcal{H}^{k-1}_H \bigg]
%         \leq 2 \EE \left[ \left( \sum_{h=1}^H r_h - V^k_{1,1}(s^k_1) \right)^2  \mid  \mathcal{H}^{k-1}_H\right]
%         \\
%         &+ 2 \EE \left[ \left( \sum_{h=1}^H f^k_{h,1} (s^k_h, a^k_h) - \EE \left[ r_h + V^k_{h+1, 1}(s_{h+1}) | s^k_h, a^k_h \right] \right)^2  \mid  \mathcal{H}^{k-1}_H\right].
%     \end{align*}
    
% \end{lemma}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proof of Theorem~\ref{thm:regret_upper_linear}}
\label{app:proof_upper_bound_linearMDP}
%
In this section, we introduce several properties of linear function class.
We formally define the linear MDP as follows:
%
\begin{definition} [Linear MDPs,~\citealt{yang2019sample, jin2020provably}]
\label{def:linearMDP}
An MDP $\MDP$ is a linear MDP if we have a known feature mapping $\psi: \SetOfStates \times \SetOfGroundActions \rightarrow \RR^{\DimLinearMDP}$, and there exist $\DimLinearMDP$ unknown (signed) measures $\mub^\star_h = (\mu_h^{(1)}, \dots, \mu_h^{(\DimLinearMDP)})$ over $\SetOfStates$ and unknown vector $\wb^\star_h \in \RR^{\DimLinearMDP}$, such that for any $(s,a) \in \SetOfStates \times \SetOfGroundActions$, we have $\TransitionProbability_h(\cdot | s,a) = \langle \psi(s,a), \mub^\star_h(\cdot) \rangle$ and $r_h(s,a) = \langle \psi(s,a), \wb_h^\star
\rangle$. 
We assume that $\sup_{(s,a) \in \SetOfStates \times \SetOfGroundActions} \| \psi(s,a) \|_2 \leq 1$, $\max \{ \| \sum_{s \in \SetOfStates} |\mub^\star_h(s)| \|_2, \| \wb^\star_h \|_2 \} \leq \sqrt{\DimLinearMDP} $ for all $h \in [H]$.
\end{definition}
%
In this proof, to explicitly indicate the dependency on parameters, we denote the linear MDPs as $\MDP_{\thetab^\star,\mub^\star,\wb^\star}$, where $\thetab^\star \!\!=\!\!\{ \thetab_h^\star\}_{h=1}^H, \mub^\star \!\!=\!\!  \{\mub_h^\star\}_{h=1}^H$, and $\wb^\star \!\!=\!\! \{ \wb^\star_h\}_{h=1}^H$.
%

We also assume that $\sum_{h=1}^H r_h \in [0,1]$.
Proposition 2.3 of~\citet{jin2020provably} shows that linear MDPs  satisfy Assumption~\ref{assum:completeness} under the linear function class $\FunctionClassLinMDP_h$ defined as follows:
\begin{equation} \label{eq:linear_function_class}
    \FunctionClassLinMDP_h := 
    \left\{ \langle \psi(\cdot, \cdot), \omegab_h \rangle : \omegab_h \in \RR^{\DimLinearMDP}, \| \omegab_h \|_2 \leq \BoundLinMDP \right\}, \quad
    \text{for any}\, h \in [H].
\end{equation}
%


For linear MDPs, let $\FunctionClassLinMDP_h(\epsilon_c)$ be an $\epsilon_c$-cover of $\FunctionClassLinMDP_h$ under the $\ell_{\infty}$ norm, so that 
\begin{align*}
    \log | \FunctionClassLinMDP_h (\epsilon_c) |  = \BigO\left(\DimLinearMDP \log \frac{\BoundLinMDP}{\epsilon_c}\right)
    = \BigOTilde \left( \DimLinearMDP  \right)
    \numberthis \label{eq:linear_functionclass_size}
    .
\end{align*}
Then, the definition of generalized Eluder dimension for the linear function class $\FunctionClassLinMDP_h$ can be expressed as:
%
%
\begin{lemma} [Lemma 3 of~\citealt{agarwal2023vo}]
\label{lemma:eluder_linearMDP}
    For the class $\FunctionClassLinMDP_h$ defined in~\eqref{eq:linear_function_class}, letting $\FunctionClassLinMDP_h(\epsilon_c)$ be the $\epsilon_c$-cover of $\FunctionClassLinMDP_h$ for some $\epsilon_c > 0$, we have 
    \begin{align*}
        \GenEluder_{\nu,K}(\FunctionClassLinMDP_h(\epsilon_c)) \leq \GenEluder_{\nu,K} (\FunctionClassLinMDP_h) 
        = \BigO \left( \DimLinearMDP \log \left(1 + \frac{K}{\nu^2 \rho} \right) \right)
        = \BigOTilde(\DimLinearMDP)
        .
    \end{align*}
\end{lemma}
%
The bonus oracle for linear MDPs can be easily instantiated using the standard elliptical bonus, and, as demonstrated in the next lemma, satisfies all the required properties for a bonus oracle.
%
\begin{lemma} [Bonus oracle $\mathcal{B}$ for linear MDPs, Lemma 7 of~\citealt{agarwal2023vo}]
\label{lemma:bonus_oracle_linearMDP}
    Given $K, H \in \mathbb{Z}_{+}$, suppose all $\beta^k_h \leq \beta$ and $\beta^k_h$ is non-decreasing in $k \in [K]$ for each $h \in [H]$.
    For any $k \geq 1, h \in [H]$, 
    variances $\{ \bar{\sigma}^\tau_h \}_{\tau=1}^h$ satisfying $ \bar{\sigma}^\tau_h \geq \nu$ for some $\nu > 0$,
    dataset $\mathcal{D}^{k-1}_h = \{ \psi(s^\tau_h, a^\tau_h), a^\tau_h, r^\tau_h, \psi(s^\tau_{h+1}, a^\tau_{h+1})  \}_{\tau=1}^{k-1} $, 
    function class $\mathcal{F}^k_h$
    and $\hat{f}^k_h \in \mathcal{F}^k_h$
    defined via weighted regression in~\eqref{eq:weighted_regression},
    and parameters $\rho, \epsilon_c > 0$,
    let $\BonusOracle ( \{ \bar{\sigma}^\tau_h \}_{\tau=1}^h, \mathcal{D}^{k-1}_h, \mathcal{F}^k_h, \hat{f}^k_h, \beta^k_h, \rho,  \epsilon_c)
    = \| \psi(s,a) \|_{\left( \Sigma^k_h \right)^{-1}} \sqrt{ \left(\beta^k_h \right)^2 + \rho}
    $,
    where $\Sigma^k_h = \frac{\rho}{16d} I + \sum_{\tau=1}^{k-1} \frac{1}{ \left( \bar{\sigma}^\tau_h \right)^2 } \psi(s^\tau_h,a^\tau_h) \psi(s^\tau_h,a^\tau_h)^\top $.
    For any choice of covering radius $\epsilon_c \leq \nu \sqrt{\rho / 8K}$,
    the oracle satisfies all the properties of Definition~\ref{def:bonus_oracle} with
    \begin{align*}
        \log \mathcal{N}_b
        = \log | \mathcal{W}|
        = \BigO \left(
            (\DimLinearMDP)^2 \log \left( 1 + \DimLinearMDP\sqrt{\DimLinearMDP} \beta / (\rho \epsilon_c^2) \right)
        \right)
        = \BigOTilde \left( (\DimLinearMDP)^2 \right)
        .
    \end{align*}
\end{lemma}
%
\begin{theorem}[Formal version of Theorem~\ref{thm:regret_upper_linear}, Regret upper bound of \AlgName{} for linear MDPs] \label{thm:regret_upper_linear_full} 
Under the same conditions with Theorem~\ref{thm:regret_upper_general_main}, suppose that the underlying MDP has linear transition probabilities and rewards, so that the function class for linear MDPs,  $\FunctionClassLinMDP_h$, satisfies Assumption~\ref{assum:completeness}.
%
%
Let $\FunctionClassLinMDP_h(\epsilon_c)$ be an $\epsilon_c$-cover of $\FunctionClassLinMDP_h$ under $\ell_\infty$ norm.
We set $\rho= 1$, $u_k = \tilde{\Theta} \left( (\DimLinearMDP)^3 H^{5/2} + d (\DimLinearMDP)^{3/2} H^{5/2} )/ \sqrt{K}  \right)$,
    $\nu =  \sqrt{1/HK}$,
    $\epsilon_b = \epsilon_c \leq 1/(8HK)$
    and $ \delta < 1/ (H^2 + 15)$.
Then, with probability at least $1-\delta$, the cumulative regret of \AlgName{}, with bonus oracle defined in Lemma~\ref{lemma:bonus_oracle_linearMDP}, is upper-bounded by
%
    \begin{align*}
         \Regret \left(\MDP_{\thetab^\star,\mub^\star,\wb^\star},K \right) 
         = \BigOTilde \bigg( \underbrace{d \sqrt{H K} 
             + \frac{1}{\kappa} d^2 H^2}_{\text{regret from MNL model}}
             + \underbrace{\vphantom{ \frac{1}{\kappa} }  \DimLinearMDP  \sqrt{HK} 
             +(\DimLinearMDP)^6 H^5 }_{\text{regret from linear MDPs}}
         \bigg).
    \end{align*}
\end{theorem}
%
%
\begin{proof} [Proof of Theorem~\ref{thm:regret_upper_linear}]
    We apply the above results to linear MDPs $\MDP_{\thetab^\star,\mub^\star,\wb^\star}$ with function class $\FunctionClassLinMDP_h (\epsilon_c)$, $h \in [H]$, and bonus oracle $\BonusOracle$.
    From~\eqref{eq:linear_functionclass_size}, we know that $\mathcal{N} = \BigOTilde(\DimLinearMDP)$.
    Additionally, Lemma~\ref{lemma:eluder_linearMDP} shows that $d_\nu = \BigOTilde(\DimLinearMDP )$.
    Therefore, by combining these results with Theorem~\ref{thm:regret_upper_general_main} and Lemma~\ref{lemma:bonus_oracle_linearMDP},
    we can establish the upper regret bounds for linear MDPs.
    \begin{align*}
        \Regret\left(\MDP_{\thetab^\star,\mub^\star,\wb^\star},K \right) = 
        \BigOTilde &\Big( d  \sqrt{H K} 
            +  \DimLinearMDP \sqrt{H K} 
            + \frac{1}{\kappa}d^2 H^2
            + (\DimLinearMDP)^6 H^{5}
        \Big),
    \end{align*}
    where we set $\rho = 1$, $u_k = \tilde{\Theta} \left( (\DimLinearMDP)^3 H^{5/2} + d (\DimLinearMDP)^{3/2} H^{5/2} \right)/ \sqrt{K} $,
    $\nu =  \sqrt{1/HK}$,
    $\epsilon_b = \epsilon_c \leq 1/(8HK)$
    and $ \delta < 1/ (H^2 + 15)$.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proof of Theorem~\ref{thm:regret_lower_linear_main}}
\label{app:proof_lower_bound_linearMDP}
%

In this section, we provide a regret lower bound for linear MDPs with preference model.
%
We construct a hard instance~$\MDP(\SetOfStates, \SetOfGroundActions , \SetOfActions,  \MaxAssortment,  \{\ChoiceProbability_h \}_{h=1}^H, \{ \TransitionProbability_h\}_{h=1}^H, \{\Reward_{h=1}^H \} , \HorizonLength)$,   illustrated as in Figure~\ref{fig:MNL_linear_MDP}.
%
This instance is based on an $H+1$-layered structure, where each layer is a variation of the hard-to-learn MDPs introduced in~\citet{zhou2021provably}.

Without loss of generality, we assume that $\DimLinearMDP \geq 6$ and that $\DimLinearMDP-5$ is divisible by $2$.\footnote{If $\DimLinearMDP-5$ is not divisible by $2$, we can set $\DimLinearMDP \leftarrow \DimLinearMDP +1$ by adding zero padding.}
Let $i \in [H+2]$ represent the layer index. 
For each layer $i \in [H+2]$, there are $H-i+3$ states, denoted as $x^{(i)}_i, \dots, x^{(i)}_{H+2}$,
where $ x^{(i)}_{H+2}$ is the absorbing state.
% Additionally, there is another absorbing state in layer $H+2$, denoted as $x^{(H+2)}_{H+2}$. 
%
Furthermore, there is a \textit{global} absorbing state $x_0$, which can only be reached at any state and horizon through the user's choice of the outside option $\OutsideOptionVector$ (not choosing any item in the assortment).
Thus, there are $(H+1)(H+2)/2 +1$ states in total in the set of states $\SetOfStates$.
%
There are $2^{(\DimLinearMDP-5)/2 } + 1$ items, so the item set is $\SetOfGroundActions = \{ -1, 1 \}^{(\DimLinearMDP-5)/2 } \cup \{ \OutsideOptionVector \}$.
%
The set of candidate assortments follows the definition in Section~\ref{sec:Problem_setting}, i.e.,  $\SetOfActions = \{A \subseteq \SetOfGroundActions: \OutsideOptionVector \in A, 1 \leq |A \setminus \{ \OutsideOptionVector \}| \leq \MaxAssortment  \}$.
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[h!]
\tikzstyle{every node}=[font=\normalsize]
    \centering
    \resizebox{0.9\linewidth}{!}{
    \begin{tikzpicture}[->,>=stealth',shorten >=2pt, 
    line width=0.7 pt, node distance=1.4cm,
    scale=1, 
    transform shape, align=center, 
    state/.style={circle, minimum size=0.5cm, text width=8mm}]
% states
% layer 1
\node[state, draw] (one) {$x^{(1)}_1$};
\node[state, ] (one_one) [below= 1.2cm of one]{};
\node[state, draw] (two) [right= 2.4cm of one] {$x^{(1)}_2$};
\node[state, ] (two_two) [below= 1.2cm of two]{};
\node[state, draw] (three) [right= 2.4cm of two] {$x^{(1)}_3$};
\node[state, ] (three_three) [below= 1.2cm of three]{};
\node[state, draw=none] (dots) [right= 2.4cm of three] {$\vphantom{x^{1}_1}\dots$};
\node[state, ] (dots_dots) [below= 1.2cm of dots]{};
\node[state, draw] (H+1) [right= 2.4cm of dots]{$x^{(1)}_{H+1}\,$};
\node[state, ] (H+1_H+1) [below= 1.2cm of H+1]{};
\node [rounded corners, draw] (H+2) [below right= 1.4cm and -0.9cm of one] [ minimum width=15.15cm]{$x^{(1)}_{H+2}$};
\node[state, ] (H+2_H+2) [right= -1.1cm of H+2]{};
% layer 2
\node[state, ] (l2_one) [below= 0.5cm of one_one]{};
\node[state, ] (l2_one_one) [below= 1.2cm of l2_one]{};
\node[state, draw] (l2_two) [below= 0.5cm of two_two] {$x^{(2)}_2$};
\node[state, ] (l2_two_two) [below= 1.2cm of l2_two]{};
\node[state, draw] (l2_three) [right= 2.4cm of l2_two] {$x^{(2)}_3$};
\node[state, ] (l2_three_three) [below= 1.2cm of l2_three]{};
\node[state, draw=none] (l2_dots) [right= 2.4cm of l2_three] {$\vphantom{x^{1}_1}\dots$};
\node[state, ] (l2_dots_dots) [below= 1.2cm of l2_dots]{};
\node[state, draw] (l2_H+1) [right= 2.4cm of l2_dots]{$x^{(2)}_{H+1}\,$};
\node[state, ] (l2_H+1_H+1) [below= 1.2cm of l2_H+1]{};
\node [rounded corners, draw] (l2_H+2) [below right= 1.6cm and -0.9cm of l2_one] [ minimum width=15.15cm]{$x^{(2)}_{H+2}$};
\node[state, ] (l2_H+2_H+2) [right= -1.1cm of l2_H+2]{};
% layer 3
\node[state, ] (l3_one) [below= 0.5cm of l2_one_one]{};
\node[state, ] (l3_one_one) [below= 1.2cm of l3_one]{};
\node[state, ] (l3_two) [below= 0.5cm of l2_two_two] {};
\node[state, ] (l3_two_two) [below= 1.2cm of l3_two]{};
\node[state, draw] (l3_three) [below= 0.5cm of l2_three_three] {$x^{(3)}_3$};
\node[state, ] (l3_three_three) [below= 1.2cm of l3_three]{};
\node[state, draw=none] (l3_dots) [right= 2.4cm of l3_three] {$\vphantom{x^{1}_1}\dots$};
\node[state, ] (l3_dots_dots) [below= 1.2cm of l3_dots]{};
\node[state, draw] (l3_H+1) [right= 2.4cm of l3_dots]{$x^{(3)}_{H+1}\,$};
\node[state, ] (l3_H+1_H+1) [below= 1.2cm of l3_H+1]{};
\node [rounded corners, draw] (l3_H+2) [below right= 1.75cm and -0.9cm of l3_one] [ minimum width=15.15cm]{$x^{(3)}_{H+2}$};
\node[state, ] (l3_H+2_H+2) [right= -1.1cm of l3_H+2]{};
% layer 4
\node[state, ] (l4_one) [below= 0.5cm of l3_one_one]{};
% \node[state, ] (l4_one_one) [below= 1.2cm of l4_one]{};
\node[state, ] (l4_two) [below= 0.5cm of l3_two_two] {};
% \node[state, ] (l4_two_two) [below= 1.2cm of l4_two]{};
\node[state, ] (l4_three) [below= 0.5cm of l3_three_three] {$\vphantom{x^{1}_1}\dots$};
\node[state, ] (l4_three_three) [left= -0.3cm of l4_three]{};
\node[state, ] (l4_dots) [right= 2.4cm of l4_three] {$\vphantom{x^{1}_1}\ddots$};
\node[state, ] (l4_dots_dots) [left= -0.3cm of l4_dots]{};
\node[state, ] (l4_dots_dots_dots) [left= 0.33cm of l4_dots_dots]{};
\node[state, ] (l4_H+1) [right= 2.4cm of l4_dots]{$\vphantom{x^{1}_1}\vdots$};
\node[state, ] (l4_H+1_H+1) [left= -0.3cm of l4_H+1]{};
\node[state, ] (l4_H+1_H+1_H+1) [left= 0.33cm of l4_H+1_H+1]{};
% \node [rounded corners, draw] (l4_H+2) [below right= 1.75cm and -0.9cm of l4_one] [ minimum width=15.15cm]{$x^{(H+1)}_{H+2}$};
% \node[state, ] (l4_H+2_H+2) [right= -1.1cm of l4_H+2]{};
% x_0
\node [rounded corners, draw] (zero) [below left= -1.0cm and 1.4cm of one] [minimum width=0.8cm, minimum height = 12cm ]{$x_0$};
\node[state, ] (zero_below) [below= -1.0cm of zero]{};
\node[state, ] (zero_one) [left= 1.1cm of one]{};
\node[state, ] (zero_two) [above left= -0.4cm and 4.8cm of l2_two]{};
\node[state, ] (zero_three) [above left= -0.4cm and 8.4cm of l3_three]{};


% Arrows
% x1-1
\draw[ ->] (one) [] to node[above]{
\shortstack[c]{$1-\delta- \langle \mub_{h},\actionvector^\star_h \rangle$, \\ $r= 1/H$ } 
} (two);
\draw[->] (one) [] to node[left]{\shortstack[c]{$\delta +$ \\ $\langle \mub_{h},\actionvector^\star_h\rangle$, \\$r=1/H$ }} (one_one);
%
\draw[densely dashed, ->] (one.340) [] to node[above]{
\shortstack[c]{$1-\delta-$ \\ $ \langle \mub_{h},\actionvector \rangle$, \\ $r= \gamma/H$ \\  \textcolor{white}{b}  }
} (l2_two.100);
\draw[densely dashed, ->] (one) [] to node[left]{\shortstack[c]{ $\delta + \langle \mub_{h},\actionvector \rangle$, \\ $ r= \gamma /H$ }} (l3_two_two);
%
\draw [dotted, ->] (one) -- node {} ++(0,1.0cm) -| (zero_one) node[pos=0.25] {} node[pos=0.75] {};

% x1-2
\draw[ ->] (two) [] to node[above]{
\shortstack[c]{}
} (three);
\draw[ ->] (two) [] to node[left]{} (two_two);
%
\draw[densely dashed, ->] (two.340) [] to node[above]{} (l2_three.100);
\draw[densely dashed, ->] (two) [] to node[below]{} (l3_three_three);
%
\draw [dotted, ->] (two) -- node {} ++(0,1.05cm) -| (zero_one) node[pos=0.25] {} node[pos=0.75] {};

% x1-3
\draw[ ->] (three) [] to node[above]{
\shortstack[c]{}
} (dots);
%
\draw[densely dashed, ->] (three) [] to node[above]{} (l2_dots.100);
\draw[densely dashed, ->] (three) [] to node[below]{} (l3_dots_dots);
%
\draw[ ->] (three) [] to node[left]{} (three_three);
\draw [dotted, ->] (three) -- node {} ++(0,1.1cm) -| (zero_one) node[pos=0.25] {} node[pos=0.75] {};

% 1-dots
\draw[ ->] (dots) [] to node[above]{
\shortstack[c]{ }
} (H+1);
%
\draw[densely dashed, ->] (dots) [] to node[above]{} (l2_H+1.100);
\draw[densely dashed, ->] (dots) [] to node[below]{} (l3_H+1_H+1);
%
\draw[->] (dots) [] to node[left]{} (dots_dots);
\draw [dotted, ->] (dots) -- node {} ++(0,1.15cm) -| (zero_one) node[pos=0.25] {} node[pos=0.75] {};

% x1-H+1
% \draw [densely dashed, ->] (H+1.75) -- node {} ++(0,1.9cm) -| (one.105) node[pos=0.25] [above]{$1-\delta - \langle \mub_{h},\actionvector_1 \rangle$} node[pos=0.75] {};
% \draw [ ->] (H+1) -- node {} ++(0,1.6cm) -| (one) node[pos=0.25] {} node[pos=0.75] {};
\draw[ ->] (H+1) [] to node[left]{} (H+1_H+1);
\path (H+1) edge [ loop right ] node {\vphantom{$10$}} (H+1) ;
\draw [dotted, ->] (H+1) -- node {} ++(0,1.2cm) -| (zero_one) node[pos=0.25] [above] {$1, r=0$} node[pos=0.75] {};
% x1-H+2
\path (H+2_H+2) edge [blue, loop right ] node {\shortstack[l]{$1$, \\ $r=1/H$ }} (H+2_H+2) ;


% x2-2
\draw[ ->] (l2_two) [] to node[above]{
\shortstack[c]{}
} (l2_three);
%
\draw[densely dashed, ->] (l2_two.340) [] to node[above]{} (l3_three.100);
\draw[densely dashed, -] (l2_two) [] to node[below]{} (l4_three_three);
%
\draw[ ->] (l2_two) [] to node[left]{} (l2_two_two);
\draw [dotted, ->] (l2_two) |- node {} +(0,1.0cm) -- (zero_two.90)  {}; 


% x2-3
\draw[ ->] (l2_three) [] to node[above]{
\shortstack[c]{}
} (l2_dots);
%
\draw[densely dashed, ->] (l2_three.340) [] to node[above]{} (l3_dots.100);
\draw[densely dashed, -] (l2_three) [] to node[below]{} (l4_dots_dots);
%
\draw[ ->] (l2_three) [] to node[left]{} (l2_three_three);
\draw [dotted, ->] (l2_three) |- node {} +(0,1.0cm) -- (zero_two.90)  {}; 

% 2-dots
\draw[ ->] (l2_dots) [] to node[above]{
\shortstack[c]{ }
} (l2_H+1);
%
\draw[densely dashed, ->] (l2_dots.340) [] to node[above]{} (l3_H+1.100);
\draw[densely dashed, -] (l2_dots) [] to node[below]{} (l4_H+1_H+1);
%
\draw[->] (l2_dots) [] to node[left]{} (l2_dots_dots);
\draw [dotted, ->] (l2_dots) |- node {} +(0,1.0cm) -- (zero_two.90)  {}; 
% x2-H+1
\draw[ ->] (l2_H+1) [] to node[left]{} (l2_H+1_H+1);
\draw [dotted, ->] (l2_H+1) |- node {} +(0,1.0cm) -- (zero_two.90)  {}; 
% x2-H+2
\path (l2_H+2_H+2) edge [blue, loop right ] node {\shortstack[l]{$1$, \\ $r=\gamma/H$ }} (l2_H+2_H+2);
\path (l2_H+1) edge [ loop right ] node {\vphantom{$10$}} (l2_H+1) ;

% x3-3
\draw[ ->] (l3_three) [] to node[above]{
\shortstack[c]{ }
} (l3_dots);
%
\draw[densely dashed, ->] (l3_three.340) [] to node[above]{} (l4_dots.100);
\draw[densely dashed, -] (l3_three) [] to node[below]{} (l4_dots_dots_dots); 
%
\draw[ ->] (l3_three) [] to node[left]{} (l3_three_three);
\draw [dotted, ->] (l3_three) |- node {} +(0,1.0cm) -- (zero_three.90)  {}; 
% 3-dots
\draw[ ->] (l3_dots) [] to node[above]{
\shortstack[c]{}
} (l3_H+1);
\draw[->] (l3_dots) [] to node[left]{} (l3_dots_dots);
%
\draw[densely dashed, ->] (l3_dots.340) [] to node[above]{} (l4_H+1.100);
\draw[densely dashed, -] (l3_dots) [] to node[below]{} (l4_H+1_H+1_H+1);
%
\draw [dotted, ->] (l3_dots) |- node {} +(0,1.0cm) -- (zero_three.90)  {}; 
% x3-H+1
\draw[ ->] (l3_H+1) [] to node[left]{} (l3_H+1_H+1);
\draw [dotted, ->] (l3_H+1) |- node {} +(0,1.0cm) -- (zero_three.90)  {}; 
% x3-H+2
\path (l3_H+2_H+2) edge [blue, loop right ] node {\shortstack[l]{$1$, \\ $r=\gamma^{2}/H$ }} (l3_H+2_H+2);
\path (l3_H+1) edge [ loop right ] node {\vphantom{$10$}} (l3_H+1) ;

% x_0
\path (zero_below) edge [red, dotted, loop below ] node { $1, r=0$ } (zero_below) ;

\end{tikzpicture}
}
  \caption{Inhomogeneous, hard-to-learn linear MDPs with MNL preference model. 
  The solid line indicates the transition caused by the user choosing the item $\actionvector^\star_h$ (with a reward of $r_h = \gamma^{i-1}/H$), 
  the dashed line shows the transition caused by the user choosing any item $\actionvector \neq \actionvector^\star_h, \OutsideOptionVector$ (with a reward of $r_h = \gamma^{i}/H$), 
  and the dotted line represents the transition caused by the user choosing the outside option $\OutsideOptionVector$ (with a reward of $r_h = 0$).
  The \textcolor{blue}{blue} solid line indicates a transition from the absorbing state back to itself, caused by the user choosing any item (with a reward of $r_h = \gamma^{i-1}/H$),
  and 
  the \textcolor{red}{red} dotted line indicates a transition from the \textit{global} absorbing state back to itself, caused by the user choosing any item (with a reward of $r_h =0 $).
  }
\label{fig:MNL_linear_MDP}
\end{figure*}
%

\subsection{Construction of linear transitions and rewards}
\label{app:lower_construction_linearMDP}
%
At each episode $k \in [K]$, the agent starts from the fixed initial state $x^{(1)}_1$.
We define $\actionvector^\star_h$ as an item such that $\actionvector^\star_h \in \argmax_{\actionvector \in \SetOfGroundActions \setminus \{\OutsideOptionVector\}} \langle \mub_h, \actionvector \rangle$,
where
$\mub_h \in \{-\Delta, \Delta \}^{(\DimLinearMDP-5)/2}$ with $\Delta= \sqrt{\delta/K}/(4\sqrt{2})$ and $\delta = 1/\HorizonLength$.
%

If the state is $x^{(i)}_h$ with $i \in [H+1]$ and $h \in [i, H+1]$, and the user chooses the item $\actionvector^\star_h$, the agent remains in the same layer $i$ and receives a reward of $\gamma^{i-1}/H$, where $\gamma = \frac{H}{1+H}$.
The next state will be either 
$x^{(i)}_{h+1 \wedge H+1}$ or $x^{(i)}_{H+2}$, with probabilities $1- (\delta + \langle \mub_h, \actionvector \rangle)$ and $\delta + \langle \mub_h, \actionvector \rangle$, respectively.
%
If the user chooses an item $\actionvector \neq \OutsideOptionVector, \actionvector^\star_h$ in the state $x^{(i)}_h$ with $i \in [H+1]$ and $h \in [i, H+1]$, the agent obtains a reward of $\gamma^i/H$ 
and transitions to
$x^{(i+1)}_{h+1 \wedge H+1}$ or $x^{(i+2 \wedge H+2)}_{H+2}$, with probabilities $1- (\delta + \langle \mub_h, \actionvector \rangle)$ and $\delta + \langle \mub_h, \actionvector \rangle$, respectively.
%
If the user does not choose any item, i.e., chooses the outside option $\OutsideOptionVector$, in the state $x^{(i)}_h$ with $i \in [H+1]$ and $h \in [i, H+1]$, the agent will deterministically transition to the global absorbing state $x_0$ and receive no reward.
%

If the agent is in any of the absorbing states--$x^{(i)}_{H+2}$ for $i \in [H+2]$--the agent will remain in the same state and receive a reward of $\gamma^{i-1}/H$, regardless of which item (including the outside option) the user chooses.
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Formally, we construct transition probabilities $\TransitionProbability_h(s' | s, \actionvector) = \langle \psi(s,\actionvector), \mub^\star_h(s') \rangle$, with
%
\begin{align*}
    &\psi(s,\actionvector) =
    \begin{cases}
        (\alpha, \beta \actionvector^\top, 0, \mathbf{0}, 0, 0, \frac{\gamma^{i-1}}{\sqrt{2}} )^\top, &s = x^{(i)}_h, \,\actionvector = \actionvector^\star_h,  i \in [H+1], h \in [i,H+1];\\
        (0, \mathbf{0}, \alpha, \beta \actionvector^\top, 0, 0, \frac{\gamma^{i}}{\sqrt{2}})^\top, &s = x^{(i)}_h, \actionvector \neq  \actionvector^\star_h, \OutsideOptionVector, i \in [H+1], h \in [i,H+1];\\
        (0, \mathbf{0}^\top, 0, \mathbf{0}^\top, 0, 1, 0)^\top, &s = x^{(i)}_h, \actionvector = \OutsideOptionVector, i \in [H+1], h \in [i,H+1];\\
        (0, \mathbf{0}^\top, 0, \mathbf{0}^\top, \frac{1}{\sqrt{2}}, 0,  \frac{\gamma^{i-1}}{\sqrt{2}})^\top, &s=x^{(i)}_{H+2}, i \in [H+2],
    \end{cases}
    \numberthis \label{eq:lower_psi}
\end{align*}
    and
\begin{align*}    
    \mub^\star_h(s') =
    \begin{cases}
        ( \frac{1-\delta}{\alpha}, -\frac{\mub_h^\top}{\beta}, 0, \mathbf{0}, 0, 0, 0 )^\top, &s' = x_{h+1 \wedge H+1}^{(i)};\\
        ( \frac{\delta}{\alpha}, \frac{\mub_h^\top}{\beta}, 0, \mathbf{0}, \sqrt{2}, 0, 0)^\top, &s'=x^{(i)}_{H+2};\\
        (0, \mathbf{0}, \frac{1-\delta}{\alpha}, -\frac{\mub_h^\top}{\beta},  0, 0, 0 )^\top, &s' = x_{h+1}^{(i+1)};\\
        (0, \mathbf{0}, \frac{\delta}{\alpha}, \frac{\mub_h^\top}{\beta}, 0, 0, 0)^\top, &s'=x^{(i+2 \wedge H+2)}_{H+2};\\
        (0, \mathbf{0}^\top, 0, \mathbf{0}, 0, 1, 0)^\top, &s' = x_0;\\
        (0, \mathbf{0}^\top, 0, \mathbf{0}, 0, 0, 0)^\top, &\text{otherwise},
    \end{cases}
    \numberthis \label{eq:lower_mu}
\end{align*}
%
where we denote $\mathbf{0} \in \RR^{(\DimLinearMDP-5)/2}$ as the zero vector of dimension $(\DimLinearMDP-5)/2$, and set $\gamma = \frac{H}{H+1}$ as the discount factor for transitioning to the next layer.
Additionally, we choose $\delta = 1/\HorizonLength$, 
$\mub_h \in \{-\Delta, \Delta \}^{(\DimLinearMDP-5)/2}$ with $\Delta= \sqrt{\delta/K}/(4\sqrt{2})$,   $\alpha = \sqrt{1/(2 + \Delta \cdot (\DimLinearMDP-5))}$, and $\beta = \sqrt{\Delta/(2 + \Delta \cdot (\DimLinearMDP-5))}$.
%%%%

And the parameter vectors for the linear rewards $r_h(s, \actionvector) = \langle \psi(s, \actionvector), \wb^\star_h \rangle$ are as follows:
%
\begin{align*}
    \wb^\star_h = (0, \mathbf{0}^\top, 0, \mathbf{0}, 0, 0, \sqrt{2}/H)^\top,
\end{align*}
%
which ensures that the reward function satisfies:
%
\begin{align*}
    r_h\left( s, \actionvector \right) = 
    \begin{cases}
        \gamma^{i-1}/H, \quad &s = x^{(i)}_h, \actionvector = \actionvector^\star_h, i \in [H+1], h \in [i,H+1];
        \\
        \gamma^{i}/H, \quad &s = x^{(i)}_h, \actionvector \neq \actionvector^\star_h, \OutsideOptionVector, i \in [H+1], h \in [i,H+1];
        \\
        0, \quad &s = x^{(i)}_h, \actionvector = \OutsideOptionVector, i \in [H+1], h \in [i,H+1];
        \\
        \gamma^{i-1}/H, \quad &s = x^{(i)}_{H+2}, i \in [H+2],
    \end{cases}
\end{align*}
%
where $0< \gamma \leq \frac{H}{H+1}$ is the discount factor for transitioning to the next layer.

This parameter setting satisfies the boundedness assumption of linear MDPs (refer Definition~\ref{def:linearMDP}). 
%
First, we show that
$\| \psi(s, \actionvector) \|_2 \leq 1$:
%
\begin{align*}
    \| \psi(s, \actionvector) \|^2_2 &\leq \alpha^2 + \frac{\DimLinearMDP -5 }{2}  \beta^2 + \frac{1}{2}  = 1, \quad &\text{(the first and second cases of~\eqref{eq:lower_psi})},
    \\
    \| \psi(s, \actionvector) \|^2_2 &= 1, \quad &\text{(the third case of~\eqref{eq:lower_psi})}, 
    \\
    \| \psi(s, \actionvector) \|^2_2 &\leq \frac{1}{2} + \frac{1}{2} = 1, \quad &\text{(the fourth case of~\eqref{eq:lower_psi})}, 
\end{align*}
%
Moreover, since 
$\DimLinearMDP \geq 6$ and $K \geq 13 (\DimLinearMDP-5)^2 / H$, we ensure that
$\max \left\{ \| \sum_{s \in S} \mub_h(s) \|_2 , \| \wb^\star_h \|_2 \right\} \leq \sqrt{\DimLinearMDP}$: 
%
\begin{align*}
    \left\| \sum_{s \in S} |\mub_h(s)| \right\|_2^2
    &= \frac{ 2(1-\delta)^2 + 2\delta^2 }{\alpha^2} 
        +\frac{\| \mub_h \|_2}{\beta^2} +3
    \\
    &\leq 2 \left( 2 + \Delta \cdot (\DimLinearMDP - 5) \right) 
    + 2\Delta \cdot (\DimLinearMDP - 5)   \left( 2 + \Delta \cdot (\DimLinearMDP - 5) \right) 
    \\
    &\leq  \left( 2 + 2\Delta \cdot (\DimLinearMDP - 5) \right)^2 
    \leq \DimLinearMDP,
    \\
    \text{and} \quad
    \| \wb^\star_h \|_2^2
    &\leq \frac{2}{H^2} \leq \DimLinearMDP.
\end{align*}
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Construction of MNL preference model}
\label{app:lower_construction_MNL}
%
Inspired by the lower bound proposed in~\citet{lee2024nearly}, we construct an adversarial setting for the MNL preference model.

We assume that $d \geq 2$ and that $d-1$ is divisible by $4$ (without loss of generality).
Let $\epsilon \in \left( 0, \frac{1}{(d-1)\sqrt{d-1}} \right)$ be a small positive parameter.
Throughout the proof, we set 
$\epsilon = \sqrt{\frac{d-1}{144 C \cdot K } \cdot \frac{(H + 1)^2}{H}}$, for some $C > 0$.
For every subset $W \subseteq [d-1]$, we define the corresponding parameter $\thetab_{W} \in \RR^{d-1}$ as $[\thetab_{W}]_{j} = \epsilon$ for all $j \in W$, and $[\thetab_{W}]_j = 0$ for all $j \notin W$.

Next, for any $h \in [H]$, we define the parameter set as:
%
\begin{align*}
    \thetab^\star_h
    \in \Theta 
    &:= \{ (\thetab_{W}^\top, - \log H )^\top  : W \in \mathcal{W}_{(d-1)/4} \}
    \\
    &= \{ (\thetab_{W}^\top, - \log H)^\top: W \subseteq [d-1], |W| = (d-1)/4 \},
\end{align*}
%
where $\mathcal{W}_k$ denotes the class of all subsets of $[d-1]$ of size $k$.

The feature vector $\phi(s,\actionvector)$ is invariant across the state $s$.
For each  $U \in \mathcal{W}_{(d-1)/4}$, we define vectors $z_U \in \RR^{d-1}$ as follows:
%
\begin{align*}
    [z_{U}]_{j} = 1/\sqrt{d-1} \quad \text{for} \,\, j \in U;
    \quad
    [z_{U}]_{j} = 0 \quad \text{for} \,\, j \notin U.
\end{align*}
%
Let $\mathcal{Z} := \{ z_U: U \in  \mathcal{W}_{(d-1)/4}\}$.
We define the function $Z: \SetOfGroundActions \rightarrow \mathcal{Z}$, so that $Z(\actionvector) \in \mathcal{Z}$.
Then, the feature vector $\phi(s, \actionvector)$ is constructed as follows:
%
\begin{align*}
    \phi(s,\actionvector) &=
    \begin{cases}
        (Z(\actionvector)^\top, 0 )^\top, &\actionvector \neq \OutsideOptionVector;\\
        (\mathbf{0}, 1 )^\top, &\actionvector = \OutsideOptionVector,
    \end{cases}
\end{align*}
%
where $\mathbf{0} \in \RR^{d-1}$.
For all $V\in \mathcal{V}_{d/4}$ and $(s, \actionvector) \in \SetOfStates \times \SetOfGroundActions$, it can be verified that $\thetab_V$ and $\phi(s, \actionvector)$  satisfy the boundedness in Assumption~\ref{assum:MNL_click_model} as follows:
%
\begin{align*}
    \|\phi(s, \actionvector) \|_2 &\leq \sqrt{(d-1) \cdot 1/(d-1) } = 1,
    \\
    \| \thetab^\star_h \|_{2} &\leq \sqrt{(d-1) \epsilon^2 + (- \log H)^2 } \leq \sqrt{2} \log H =: \BoundMNL.
\end{align*}
%
Let $\actionvector^\star_h$ (defined in the previous subsection) also have the maximum utility, i.e., $\actionvector^\star_h \in \argmax_{a \in \SetOfGroundActions \setminus \{ \OutsideOptionVector\}} \langle \thetab^\star_h, \phi(s, \actionvector) \rangle$ (note that $\phi(\cdot, \cdot)$ is  identical for all $s \in \SetOfStates$).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Theorem~\ref{thm:regret_lower_linear_main}}
\label{app:subsec_proof_lower_linear}
%
A good policy is one that quickly reaches the state $x^{(i)}_{H+2}$ while remaining in the lower layers (i.e., with lower $i$).
Recall that the item $\actionvector^\star_h$ has the highest utility and, therefore, the highest choice probability. 
It also has the best chance of quickly reaching the state $x^{(i)}_{H+2}$ while staying within the same layer.
%
In other words, a good policy encourages the user to frequently select the item $\actionvector^\star_h \in \argmax_{a \in \SetOfGroundActions \setminus \{ \OutsideOptionVector\}} \langle \mub_h, \actionvector \rangle
= \argmax_{a \in \SetOfGroundActions \setminus \{ \OutsideOptionVector\}} \langle \thetab^\star_h, \phi(s, \actionvector) \rangle
$. 
Note that $\actionvector^\star_h $ is unique due to the way the action space and transition probabilities are constructed.

We formally restate Theorem~\ref{thm:regret_lower_linear_main} as follows.
%
\begin{theorem} [Restatement of Theorem~\ref{thm:regret_lower_linear_main}, Regret lower bound for linear MDPs with preference feedback]
\label{thm:regret_lower_linear}
Suppose that
$d \geq 2$,
$\DimLinearMDP \geq 6$, 
$H \geq 3$,
and $K \geq \max\{ C\cdot (\DimLinearMDP -5)^2 H (H+1)^2, C' \cdot (d-1)^4 (1 + H) / H  \}$ for some constant $C, C'>0$.
Then, for any algorithm, there exists an episodic linear MDP $\MDP_{\thetab, \mub, \wb}$ with MNL preference feedback such that the worst-case expected regret is lower bounded as follows:
%
    \begin{align*}
    \sup_{\thetab, \mub, \wb}
        \EE_{\thetab, \mub, \wb} \left[ \Regret\left(\MDP_{\thetab, \mub, \wb}, K \right)\right] 
        = \Omega \left( 
            d \sqrt{HK}
            + \DimLinearMDP \sqrt{HK}
        \right).
    \end{align*}
\end{theorem}
%


\begin{proof} [Proof of Theorem~\ref{thm:regret_lower_linear_main}]
    Fix $\thetab$ and $\mub$ so that we can omit the parameter dependency of $\TransitionProbability$ and $\ChoiceProbability$ throughout the proof.
    %
    Based on the construction of the hard instance $\MDP$ discussed in the previous subsections, the following lemma shows that the optimal assortment at horizon $h \in [H]$ is $\{\OutsideOptionVector, \actionvector^\star_h\}$.
    %%%%
    \begin{lemma} \label{lemma:lower_linear_optimal_A}
        For any $h \in [H]$, we have $A^\star_h = \{\OutsideOptionVector, \actionvector^\star_h \}$.
        %
    \end{lemma}
    %
    Furthermore, we can bound the expected value of $\GroundQ^\star$ for any assortment as follows:
    %
    \begin{lemma} \label{lemma:lower_linear_upper_revenue}
        For any $(A, i, h) \in \SetOfActions \times [H] \times [H]$, let $\tilde{\actionvector}^{(i)}_h \in \argmax_{\actionvector \in A \setminus \{\OutsideOptionVector\}} \phi(x^{(i)}_h, \actionvector)^\top \thetab^\star_h$, 
        $\tilde{A}^{(i)}_h = \{ \tilde{\actionvector}^{(i)}_h, \OutsideOptionVector \}$,
        and
        $\bar{\actionvector}^{(i)}_h \in \argmax_{\actionvector \in A \setminus \{ \OutsideOptionVector \} } \GroundQ^\pi_h (x^{(i)}_h, \actionvector)$.
        %
        For any $\actionvector' \neq \OutsideOptionVector$, we define 
        \begin{align*}
            \widetilde{Q}^\pi_h &(x^{(i)}_h, \actionvector^\star_h, \actionvector') 
            \\
            &:= 
            \begin{cases}
                \frac{\gamma^{i-1}}{H} + \TransitionProbability_h(x^{(i)}_{h+1} | x^{(i)}_{h}, \actionvector^\star_h) V^\pi_{h+1}(x^{(i)}_{h+1}) 
                    + \TransitionProbability_h(x^{(i)}_{H+2}| x^{(i)}_{h}, \actionvector') \frac{(H-h) \gamma^{i-1}}{H},
                & \actionvector' = \actionvector^\star_h,
                \\
                \frac{\gamma^{i-1}}{H} + \TransitionProbability_h(x^{(i)}_{h+1} | x^{(i)}_{h}, \actionvector^\star_h) V^\pi_{h+1}(x^{(i)}_{h+1}) 
                    + \TransitionProbability_h(x^{(i+2)}_{H+2}| x^{(i)}_{h}, \actionvector') \frac{(H-h) \gamma^{i-1}}{H},
                &\actionvector' \neq \actionvector^\star_h.
            \end{cases}
        \end{align*}
        %
        Then, for any policy $\pi$, if $K \geq 4 (\DimLinearMDP -5)^2 H (H+1)^2 $,
        we have
        %
        \begin{align*}
            \sum_{\actionvector \in A} \ChoiceProbability_h (\actionvector | x^{(i)}_h, A) \GroundQ^\pi_h (x^{(i)}_h, \actionvector)
            \leq 
             \ChoiceProbability_h (\tilde{\actionvector}^{(i)}_{h} | x^{(i)}_h, \tilde{A}^{(i)}_{h}) 
              \widetilde{Q}^\pi_h (x^{(i)}_h, \actionvector^\star_h, \bar{\actionvector}^{(i)}_h)
              .
        \end{align*}
    \end{lemma}
    %
    Now, we are ready to provide the proof of Theorem~\ref{thm:regret_lower_linear_main}.
    
    For any $h \in [H]$ and any $A_h \in \SetOfActions$, let $\tilde{\actionvector}_h \in \argmax_{\actionvector \in A_h \setminus \{ \OutsideOptionVector \} } \phi(x^{(i)}_h, \actionvector)^\top \thetab^\star_h 
    $.
    We also denote $\tilde{A}_h = \{ \tilde{\actionvector}_h, \OutsideOptionVector \}$
    and
    $\bar{\actionvector}_h \in \argmax_{\actionvector \in A \setminus \{ \OutsideOptionVector \} } \GroundQ^\pi_h (x_h, \actionvector)$.
    %
    Recall that the index $i$ can be omitted for  $\tilde{\actionvector}_h $ because the transition and choice probabilities are identical across all $x^{(1)}_h, \dots x^{(H)}_h$ given $\actionvector \in \SetOfGroundActions$.
    The change in layer $i$ only affects the scaling of rewards, and consequently the $\GroundQ$-values, but the item that maximizes $\GroundQ(x^{(i)}_h, \actionvector)$ remains the same across layers.
    
    
    By applying Lemma~\ref{lemma:lower_linear_upper_revenue}, the value of policy $\pi$ in state $x^{(1)}_1$ can be bounded as follows:
    %
    \begin{align*}
        V^\pi_1(x^{(1)}_1)
        = \sum_{\actionvector \in A_1} \ChoiceProbability_1 (\actionvector | x^{(1)}_1, A_1) \GroundQ^\pi_1 (x^{(1)}_1, \actionvector)
        \leq \ChoiceProbability_1(\tilde{\actionvector}_1 | x^{(1)}_1, \tilde{A}_1) 
        \widetilde{Q}^\pi_h (x^{(1)}_1, \actionvector^\star_1, \bar{\actionvector}_1)
        . \numberthis \label{eq:lower_linear_V_pi_upper}
    \end{align*}
    %
    Moreover, according to Lemma~\ref{lemma:lower_linear_optimal_A}, the optimal assortment for horizon $h \in [H]$ is 
    $A^\star_h = \{\OutsideOptionVector, \actionvector^\star_h \}$.
    Thus, the optimal value function in  state $x^{(1)}_1$ can be written as follows:
    %
    \begin{align*}
        V^\star_1 (x^{(1)}_1) 
        = \sum_{\actionvector \in A^\star_1} \ChoiceProbability_1 (\actionvector | x^{(1)}_1, A^\star_1) \GroundQ^\star_1 (x^{(1)}_1, \actionvector)
        =  \ChoiceProbability_1 (\actionvector^\star_1 | x^{(1)}_1, A^\star_1) \GroundQ^\star_1 (x^{(1)}_1, \actionvector^\star_1),
    \end{align*}
    %
    where the last equality holds because $\GroundQ^\star_h (x^{(i)}_h, \OutsideOptionVector) = 0$.
    We denote $s_{H+2}$ can be either $x^{(1)}_{H+2}$ or $x^{(3)}_{H+2}$, depending on whether the item (for transition) is $\actionvector^\star_h$ or any $\actionvector \neq \actionvector^\star_h, \OutsideOptionVector$.
    Then, we have
    %
    \begin{align*}
        \left(V^\star_1 - V^\pi_1 \right) (x^{(1)}_1)
        &\geq   \ChoiceProbability_1 (\actionvector^\star_1 | x^{(1)}_1, A^\star_1) \GroundQ^\star_1 (x^{(1)}_1, \actionvector^\star_1)
        - \ChoiceProbability_1(\tilde{\actionvector}_1 | x^{(1)}_1, \tilde{A}_1) 
        \widetilde{Q}^\pi_h (x^{(1)}_1, \actionvector^\star_1, \bar{\actionvector}_1)
        \\
        &= \left( \ChoiceProbability_1 (\actionvector^\star_1 | x^{(1)}_1, A^\star_1)
        - \ChoiceProbability_1(\tilde{\actionvector}_1 | x^{(1)}_1, \tilde{A}_1)
        \right) \GroundQ^\star_1 (x^{(1)}_1, \actionvector^\star_1)
        \\
        &+ \ChoiceProbability_1(\tilde{\actionvector}_1 | x^{(1)}_1, \tilde{A}_1)
        \left(
            \GroundQ^\star_1 (x^{(1)}_1, \actionvector^\star_1)
            - \widetilde{Q}^\pi_h (x^{(1)}_1, \actionvector^\star_1, \bar{\actionvector}_1)
        \right)
        \\
        &= \left( \ChoiceProbability_1 (\actionvector^\star_1 | x^{(1)}_1, A^\star_1)
        - \ChoiceProbability_1(\tilde{\actionvector}_1 | x^{(1)}_1, \tilde{A}_1)
        \right) \GroundQ^\star_1 (x^{(1)}_1, \actionvector^\star_1)
        \\
        &+ \ChoiceProbability_1(\tilde{\actionvector}_1 | x^{(1)}_1, \tilde{A}_1)
        \Bigg(
            \frac{1}{H} + \TransitionProbability_1(x^{(1)}_{2} | x^{(1)}_{1}, \actionvector^\star_1) V^\star_{2}(x^{(1)}_{2}) 
            + \TransitionProbability_1(x^{(1)}_{H+2}| x^{(1)}_{1}, \actionvector^\star_1) \frac{(H-1)}{H}
            \\
            &\quad\quad\quad\quad\quad\quad\quad\quad
            - \left(\frac{1}{H} + \TransitionProbability_1(x^{(1)}_{2} | x^{(1)}_{1}, \actionvector^\star_1) V^\pi_{2}(x^{(1)}_{2}) 
                    + \TransitionProbability_1(s_{H+2}| x^{(1)}_{1}, \bar{\actionvector}_1) \frac{(H-1) }{H}\right)
        \Bigg)
        \\
        &= \left( \ChoiceProbability_1 (\actionvector^\star_1 | x^{(1)}_1, A^\star_1)
        - \ChoiceProbability_1(\tilde{\actionvector}_1 | x^{(1)}_1, \tilde{A}_1)
        \right) \GroundQ^\star_1 (x^{(1)}_1, \actionvector^\star_1)
        \\
        &+ \ChoiceProbability_1(\tilde{\actionvector}_1 | x^{(1)}_1, \tilde{A}_1)
             \TransitionProbability_1(x^{(1)}_{2} | x^{(1)}_{1}, \actionvector^\star_1)
            ( V^\star_{2} - V^\pi_{2})(x^{(1)}_{2})  
        \\
        &+ \ChoiceProbability_1(\tilde{\actionvector}_1 | x^{(1)}_1, \tilde{A}_1)
        \left(
        \TransitionProbability_1(x^{(1)}_{H+2}| x^{(1)}_{1}, \actionvector^\star_1) 
            - \TransitionProbability_1(s_{H+2}| x^{(1)}_{1}, \bar{\actionvector}_1)
            \right) \frac{(H-1) }{H}
        ,\numberthis \label{eq:lower_linear_regret1}
    \end{align*}
    where the first inequality holds by~\eqref{eq:lower_linear_V_pi_upper}.
    Note that, by construction, for any $h \in [H]$, we have
    %
    \begin{align*}
        \GroundQ^\star_h (x^{(1)}_h, \actionvector^\star_h) &= \frac{H-h+1}{H},
        \\
        \ChoiceProbability_h(\tilde{\actionvector}_h | x^{(1)}_h, \tilde{A}_h) &\geq \frac{1}{1/H+1} = \frac{H}{1+H},
        \\
        \TransitionProbability_h(x^{(1)}_{h+1} | x^{(1)}_{h}, \actionvector^\star_h)
        &= 1 - \delta - (\DimLinearMDP -5) \Delta,
        \\
        \TransitionProbability_h(x^{(1)}_{H+2}| x^{(1)}_{h}, \actionvector^\star_h) 
            - \TransitionProbability_h(s_{H+2}| x^{(1)}_{h}, \bar{\actionvector}_h)
        &= (\DimLinearMDP -5) \Delta 
        - \langle \mub_h, \bar{\actionvector}_h \rangle.
        \numberthis \label{eq:lower_linear_condition}
    \end{align*}
    %
    Hence, by plugging~\eqref{eq:lower_linear_condition} into~\eqref{eq:lower_linear_regret1} and applying recursion, we get
    %
    \begin{align*}
        &\left(V^\star_1 - V^\pi_1 \right) (x^{(1)}_1)
        \\
        &\geq 
        \sum_{h=1}^H 
            \left(
                \ChoiceProbability_h (\actionvector^\star_h | x^{(1)}_h, A^\star_h)
                - \ChoiceProbability_h(\tilde{\actionvector}_h | x^{(1)}_h, \tilde{A}_h)
            \right)
            \frac{H-h+1}{H}
            \cdot \left( \frac{H}{H+1} \right)^{h-1}
            \cdot \left( \left( 1 - \delta - (\DimLinearMDP -5) \Delta \right)  \right)^{h-1}
        \\
        &+ \sum_{h=1}^H
            \left( (\DimLinearMDP -5) \Delta 
        - \langle \mub_h, \bar{\actionvector}_h \rangle \right)
            \frac{H-h}{H} 
        \cdot 
        \left( \frac{H}{H+1} \right)^{h}
            \cdot \left( \left( 1 - \delta - (\DimLinearMDP -5) \Delta \right)  \right)^{h-1}
        \\
        %
        &- \sum_{h=1}^H \frac{(H-h) }{H\sqrt{K}}
        \cdot 
        \left( \frac{H}{H+1} \right)^{h}
        \cdot \left( \left( 1 - \delta - (\DimLinearMDP -5) \Delta \right)  \right)^{h-1}
            . 
            \numberthis \label{eq:lower_linear_regret2}
    \end{align*}
    %
    Furthermore, since $H \geq 3$ and $3 (\DimLinearMDP - 5) \Delta \leq \delta = 1/H  $, we have
    \begin{align*}
        \left( \frac{H}{H+1} \right)^{h}
        &\geq \left( \frac{H}{H+1} \right)^{H+1}
        \geq \frac{3}{10},
        \\
        \left( \left( 1 - \delta - (\DimLinearMDP -5) \Delta \right)  \right)^{h-1}
        &\geq \left( 1- \frac{4 \delta}{3} \right)^H
        \geq \frac{1}{3}.
        \numberthis \label{eq:lower_linear_product_bound}
    \end{align*}
    %
    Therefore, by substituting~\eqref{eq:lower_linear_product_bound} into~\eqref{eq:lower_linear_regret2}, and considering the terms where $h \geq H/2$, we obtain
    %
    \begin{align*}
        \left(V^\star_1 - V^\pi_1 \right) (x^{(1)}_1)
        &\geq
        \frac{1}{20} \sum_{h=1}^{H/2} 
                \left(\ChoiceProbability_h (\actionvector^\star_h | x^{(1)}_h, A^\star_h)
                - \ChoiceProbability_h(\tilde{\actionvector}_h | x^{(1)}_h, \tilde{A}_h)\right)
        + \frac{1}{20} \sum_{h=1}^{H/2} 
        \left((\DimLinearMDP -5) \Delta 
        - \langle \mub_h, \bar{\actionvector}_h \rangle \right)
        \\
        &= \frac{1}{20} \sum_{h=1}^{H/2} 
                \underbrace{\left(\ChoiceProbability_h (\actionvector^\star_h | x^{(1)}_h, A^\star_h)
                - \ChoiceProbability_h(\tilde{\actionvector}_h | x^{(1)}_h, \tilde{A}_h)\right)}_{
                \text{MNL bandit regret}
                }
        + \frac{1}{20} \sum_{h=1}^{H/2} 
        \underbrace{\left(  \max_{\actionvector \in \SetOfGroundActions}\, \langle \mub_h, \actionvector  \rangle 
        - \langle \mub_h, \bar{\actionvector}_h \rangle \right)}_{\text{linear bandit regret}}
        \numberthis \label{eq:lower_linear_decomposition}
        .
    \end{align*}
    %
    On the right-hand side of~\eqref{eq:lower_linear_decomposition}, the first term corresponds to an MNL bandit problem.
    Recall that $|A^\star_h| = |\tilde{A}_h |=2$ and, by construction, we have 
    %
    \begin{align*}
        \ChoiceProbability_h (\actionvector^\star_h | x^{(1)}_h, A^\star_h)
        &= \frac{\exp\left( \phi(x^{(1)}_h, \actionvector^\star_h )^\top \thetab^\star_h
        \right)}{1/H + \exp\left( \phi(x^{(1)}_h, \actionvector^\star_h )^\top \thetab^\star_h  \right)}
        , 
        \quad
        %
        \ChoiceProbability_h (\tilde{\actionvector}_h | x^{(1)}_h, \tilde{A}_h )
        = \frac{\exp\left( \phi(x^{(1)}_h, \tilde{\actionvector}_h )^\top \thetab^\star_h  \right)}{1/H + \exp\left( \phi(x^{(1)}_h, \tilde{\actionvector}_h )^\top \thetab^\star_h  \right)}.
    \end{align*}
    Hence, this corresponds to an MNL bandit problem with a maximum assortment size of $M = 2$, where the \textit{attraction parameter for the outside option} (the constant in the denominator) is $1/H$.
    
    Furthermore, the second term on the right-hand side of~\eqref{eq:lower_linear_decomposition} represents a linear bandit problem.
    To sum up,  the learning problem is not harder than minimizing the regret on $\Omega(H/2)$ MNL and linear bandit problems.

    
    To bound each term of~\eqref{eq:lower_linear_decomposition}, we introduce the following propositions:
    %%%%
    \begin{proposition} [Regret lower bound of MNL bandits,~\citealt{lee2024nearly}] \label{prop:lower_MNLbandit}
        Let $v_0$ denote the attraction parameter for the outisde option.
        Let $d$ be divisible by $4$.
        Suppose $K \geq C \cdot d^4 M / (M-1)$ for some constant $C>0$.
        Then, in the uniform reward setting (where rewards are identical) with the reward for the outside option being zero, for any policy and the MNL preference model parameterized by $\thetab$, there exists a worst-case problem instance such that the worst-case expected regret is lower bounded as follows:
        %
        \begin{align*}
            \sup_{\thetab} \EE_{\thetab} \left[ {\normalfont \textbf{MNLBanditRegret}}(\thetab, K) \right]
            = \Omega\left( \frac{ \sqrt{v_0 (M-1)} }{v_0 + M-1}\cdot d\sqrt{K} \right).
        \end{align*}
    \end{proposition}
    %%%%
    \begin{proposition}[Lemma C.8 in~\citealt{zhou2021nearly}]\label{prop:lower_linearbandit}
        Fix $0 < \delta< 1/3$.
        Consider the linear bandit problem parameterized with a vector $\mub \in \{ -\Delta, \Delta\}^d$ and action set $\SetOfGroundActions = \{ -1, 1 \}^d$.
        And the reward distribution for taking action $a \in \SetOfGroundActions$ is a Bernoulli distribution denoted as $B (\delta + \langle \mub, \actionvector \rangle)$.
        Let $K$ be the number of time steps playing this bandit problem.
        Assume $K \geq d^2/(2\delta)$ and $\Delta = \sqrt{\delta/{K}}/(4 \sqrt{2})$.
        Then, for any bandit algorithm $\mathcal{B}$, there exists $\mub$ such that  the expected pseudo-regret of $\mathcal{B}$ over $K$ steps is lower bounded as follows:
        \begin{align*}
            % \mathbb{E}_{\UnknownMeasure^*}[\text{Regret}(K)] 
            \mathbb{E}_{\mub}[ {\normalfont \textbf{LinearBanditRegret}}(\mub, K)] 
            \geq \frac{d \sqrt{K \delta}}{8\sqrt{2}}.
        \end{align*}
        %
        where the expectation is with respect to the reward distribution that depends on $\UnknownMeasure$.
    \end{proposition}
    %%%%
    Now, by using Proposition~\ref{prop:lower_MNLbandit} and~\ref{prop:lower_linearbandit}, we can bound the regret as follows:
    %
    \begin{align*}
        \sup_{\thetab, \mub, \wb}
        \EE_{\thetab, \mub, \wb} \left[ \Regret\left(\MDP_{\thetab, \mub, \wb}, K \right)\right]
        &\geq 
        \frac{1}{20}  \sum_{h=1}^{H/2} 
            \sup_{\thetab}
            \EE_{\thetab}\left[
                \sum_{k=1}^K \left(\ChoiceProbability_h (\actionvector^\star_h | x^{(1)}_h, A^\star_h)
                - \ChoiceProbability_h(\tilde{\actionvector}_h | x^{(1)}_h, \tilde{A}_h)\right)
            \right]
        \\
        &+ \frac{1}{20} \sum_{h=1}^{H/2}  
            \sup_{\mub}
            \EE_{\mub}\left[
                \sum_{k=1}^K
                \left(  \max_{\actionvector \in \SetOfGroundActions}\, \langle \mub_h, \actionvector  \rangle 
                - \langle \mub_h, \bar{\actionvector}_h \rangle \right)
            \right]
        \\
        &=
        \Omega \left( 
            d \sqrt{HK}
            + \DimLinearMDP \sqrt{HK}
        \right),
    \end{align*}
    where, in the last equality, we use $v_0 = 1/H$, $M=2$, 
    and $\delta = 1/H$.
    This concludes the proof of Theorem~\ref{thm:regret_lower_linear_main}.
    %
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Lemmas for Theorem~\ref{thm:regret_lower_linear_main}}
\label{app_subsec:proof_lemma_lower_linear}
%
\subsubsection{Proof of Lemma~\ref{lemma:lower_linear_optimal_A}}
%
        \begin{proof} [Proof of Lemma~\ref{lemma:lower_linear_optimal_A}]
            For any $i \in [H]$, we can write the optimal $\GroundQ$-value in state $x^{(i)}_h$ at horizon $h \in [H]$ as follows:
            %
            \begin{align*}
                \GroundQ^\star_h (x^{(i)}_h, \actionvector)
                &= 
                \begin{cases}
                    \frac{\gamma^{i-1}}{H} + \TransitionProbability_h(x^{(i)}_{h+1} | x^{(i)}_{h}, \actionvector) V^\star_{h+1}(x^{(i)}_{h+1}) 
                    + \TransitionProbability_h(x^{(i)}_{H+2}| x^{(i)}_{h}, \actionvector) \frac{(H-h)\gamma^{i-1}}{H}
                    ,
                    & \actionvector = \actionvector^\star_h;
                    \\
                    \frac{\gamma^{i}}{H} + \TransitionProbability_h(x^{(i+1)}_{h+1} | x^{(i)}_{h}, \actionvector) V^\star_{h+1}(x^{(i+1)}_{h+1}) 
                    + \TransitionProbability_h(x^{(i+2)}_{H+2}| x^{(i)}_{h}, \actionvector) \frac{(H-h)\gamma^{i+1}}{H} ,
                    & \actionvector = \actionvector^\star_h, \OutsideOptionVector;
                    \\
                    0, & \actionvector = \OutsideOptionVector.
                \end{cases}
            \end{align*}
            %
            First, we show that
            for any $(i,h) \in [H] \times [H]$,
            we have
            %
            \begin{align}
                \GroundQ^\star_h (x^{(i)}_h, \actionvector) 
                \geq \sum_{\actionvector' \in A^\star_h} \ChoiceProbability_h (\actionvector' | x^{(i)}_h, A^\star_h) \GroundQ^\star_h (x^{(i)}_h, \actionvector')
                , \quad \forall \actionvector \in A^\star_h \setminus \{ \OutsideOptionVector \}
                . \label{eq:lemma_lower_linear_optimal_A_opt_Q}
            \end{align}
            %
            We prove this by contradiction.
            Suppose there exists $\actionvector \in A^\star_h$ such that $\GroundQ^\star_h (x^{(i)}_h, \actionvector) <  \sum_{\actionvector' \in A^\star_h} \ChoiceProbability_h (\actionvector' | x^{(i)}_h, A^\star_h) \GroundQ^\star_h (x^{(i)}_h, \actionvector')$.
            In that case, removing the item $\actionvector$ from the assortment $A^\star_h$ results in a higher expected value of $\GroundQ^\star_h$.
            This contradicts the optimality of $A^\star_h$.
            Therefore,~\eqref{eq:lemma_lower_linear_optimal_A_opt_Q}  must hold.

            By the definition of $\GroundQ^\star_h (x^{(i)}_h, \actionvector)$, for any $\actionvector \in \SetOfGroundActions \setminus \{ \actionvector^\star_h\}$, we have
            %
            \begin{align*}
                \GroundQ^\star_h (x^{(i)}_h, \actionvector)
                &\leq \frac{\gamma^{i-1}}{H} + \TransitionProbability_h(x^{(i+1)}_{h+1} | x^{(i)}_{h}, \actionvector) V^\star_{h+1}(x^{(i)}_{h+1}) 
                    + \TransitionProbability_h(x^{(i+2)}_{H+2}| x^{(i)}_{h}, \actionvector) \frac{(H-h)\gamma^{i-1}}{H} 
                \\
                &\leq 
                \frac{\gamma^{i-1}}{H} + \TransitionProbability_h(x^{(i)}_{h+1} | x^{(i)}_{h}, \actionvector^\star_h) 
                V^\star_{h+1}(x^{(i)}_{h+1}) 
                    + \TransitionProbability_h(x^{(i)}_{H+2}| x^{(i)}_{h}, \actionvector^\star_h) \frac{(H-h)\gamma^{i-1}}{H}
                \\
                &= \GroundQ^\star_h (x^{(i)}_h, \actionvector^\star_h)
                ,
            \end{align*}
            where the first inequality holds since $V^\star_{h+1}(x^{(i+1)}_{h+1}) \leq V^\star_{h+1}(x^{(i)}_{h+1}) $,
            and the second inequality holds due to the fact that $V^\star_{h+1}(x^{(i)}_{h+1}) \leq \frac{(H-h)\gamma^{i-1}}{H}$ and
            $ \TransitionProbability_h(x^{(i+2)}_{H+2}| x^{(i)}_{h}, \actionvector)
            \leq \TransitionProbability_h(x^{(i)}_{H+2}| x^{(i)}_{h}, \actionvector^\star_h) $.

            Since $\GroundQ^\star_h (x^{(i)}_h, \actionvector^\star_h)$ has the highest value among all items, the optimal assortment $A^\star_h$ should include $\actionvector^\star_h$.
            Thus, we have $\actionvector^\star_h, \OutsideOptionVector \in A^\star_h$.
            In other words, when $A^\star_h = \{\actionvector^\star_h, \OutsideOptionVector \}$, the condition in~\eqref{eq:lemma_lower_linear_optimal_A_opt_Q} is satisfied. 
            %
            Thus, we  begin with  $A^\star_h = \{\actionvector^\star_h, \OutsideOptionVector \}$ and check if there exist an item $\actionvector \neq \actionvector^\star_h, \OutsideOptionVector$ that can increase the expected value of $\GroundQ^\star_h$.
            To this end, for $A^\star_h = \{\actionvector^\star_h, \OutsideOptionVector \}$, we get
            %
            \begin{align*}
                &\sum_{\actionvector' \in A^\star_h} \ChoiceProbability_h (\actionvector' | x^{(i)}_h, A^\star_h) \GroundQ^\star_h (x^{(i)}_h, \actionvector')
                = \ChoiceProbability_h (\actionvector^\star_h | x^{(i)}_h, A^\star_h)
                \GroundQ^\star_h (x^{(i)}_h, \actionvector^\star_h)
                \geq \gamma \GroundQ^\star_h (x^{(i)}_h, \actionvector^\star_h)
                , \numberthis  \label{eq:lemma_lower_linear_optimal_A_opt_E_Q}
            \end{align*}
            where the equality holds since $\GroundQ^\star_h (x^{(i)}_h, \OutsideOption)=0$,
            and the inequality holds by the definition of $\gamma$:
            %
            \begin{align*}
                \gamma 
                &= \frac{H}{1+H}
                = \frac{1}{1/H+1}
                \leq \min_{h \in [H]} 
                \min_{s \in \SetOfStates}
                \min_{A \in \SetOfActions}
                \min_{\actionvector \in A \setminus \{\OutsideOption\} } 
                \frac{\exp\left( \phi(s,\actionvector)^\top \thetab^\star_h \right)}{1/H + \exp\left( \phi(s,\actionvector)^\top \thetab^\star_h \right)}
                \\
                &\leq 
                \frac{\exp\left( \phi(x^{(i)}_h,\actionvector^\star_h)^\top \thetab^\star_h \right)}{1/H + \exp\left( \phi(x^{(i)}_h,\actionvector^\star_h)^\top \thetab^\star_h \right)}
                = \ChoiceProbability_h (\actionvector^\star_h | x^{(i)}_h, A^\star_h)
                . \numberthis \label{eq:lemma_lower_linear_gamma}
            \end{align*}
            Here, we rely on the fact that the sigmoid function is monotonically increasing to establish the inequalities.
            
            On the other hand, for any item $\actionvector \neq \actionvector^\star_h, \OutsideOptionVector$, we have
            %
            \begin{align*}
                \GroundQ^\star_h (x^{(i)}_h, \actionvector)
                &= \frac{\gamma^{i}}{H}  + \TransitionProbability_h(x^{(i+1)}_{h+1} | x^{(i)}_{h}, \actionvector) V^\star_{h+1}(x^{(i+1)}_{h+1}) 
                    + \TransitionProbability_h(x^{(i+2)}_{H+2}| x^{(i)}_{h}, \actionvector) \frac{(H-h) \gamma^{i+1}}{H} 
                \\
                &\leq  \frac{\gamma^{i}}{H}  
                    + \TransitionProbability_h(x^{(i+1)}_{h+1} | x^{(i)}_{h}, \actionvector) V^\star_{h+1}(x^{(i+1)}_{h+1}) 
                    + \TransitionProbability_h(x^{(i+2)}_{H+2}| x^{(i)}_{h}, \actionvector) \frac{(H-h) \gamma^{i}}{H} 
                \\
                &\leq \frac{\gamma^{i}}{H} + \TransitionProbability_h(x^{(i)}_{h+1} | x^{(i)}_{h}, \actionvector^\star_h) V^\star_{h+1}(x^{(i+1)}_{h+1}) 
                    + \TransitionProbability_h(x^{(i)}_{H+2}| x^{(i)}_{h}, \actionvector^\star_h) \frac{(H-h) \gamma^{i}}{H}
                \\
                &= \gamma \cdot \left( 
                \frac{\gamma^{i-1}}{H} + \TransitionProbability_h(x^{(i)}_{h+1} | x^{(i)}_{h}, \actionvector^\star_h) V^\star_{h+1}(x^{(i)}_{h+1}) 
                    + \TransitionProbability_h(x^{(i)}_{H+2}| x^{(i)}_{h}, \actionvector^\star_h) \frac{(H-h) \gamma^{i-1}}{H}
                \right)
                \\
                &= \gamma  \GroundQ^\star_h (x^{(i)}_h, \actionvector^\star_h)
                ,\numberthis \label{eq:lemma_lower_linear_optimal_A_other_Q}
            \end{align*}
            where the second inequality holds because $ V^\star_{h+1}(x^{(i+1)}_{h+1}) \leq \frac{(H-h) \gamma^{i} }{H} $, $ \TransitionProbability_h(x^{(i+2)}_{H+2}| x^{(i)}_{h}, \actionvector) 
             \leq \TransitionProbability_h(x^{(i)}_{H+2}| x^{(i)}_{h}, \actionvector^\star_h)$,
            and the second equality follows from the fact that $\gamma V^\star_{h+1}(x^{(i)}_{h+1}) = V^\star_{h+1}(x^{(i+1)}_{h+1})$ by construction.
            
            Combining~\eqref{eq:lemma_lower_linear_optimal_A_opt_E_Q} and~\eqref{eq:lemma_lower_linear_optimal_A_other_Q}, when $A^\star_h = \{ \actionvector^\star_h, \OutsideOptionVector \}$, for any item $\actionvector \neq \actionvector^\star_h, \OutsideOptionVector$,  we get
            %
            \begin{align*}
                \sum_{\actionvector' \in A^\star_h} \ChoiceProbability_h (\actionvector' | x^{(i)}_h, A^\star_h) \GroundQ^\star_h (x^{(i)}_h, \actionvector')
                \geq  \GroundQ^\star_h (x^{(i)}_h, \actionvector).
            \end{align*}
            %
            Since $\GroundQ^\star_h (x^{(i)}_h, \actionvector)$ for $\actionvector \neq \actionvector^\star_h, \OutsideOptionVector$ is not greater than the expected value of $\GroundQ^\star_h$ for $A^\star_h$, adding any item $\actionvector \neq \actionvector^\star_h, \OutsideOptionVector$ to $A^\star_h$ does not increase the expected value of $\GroundQ^\star_h$.
            This confirms the optimality of $A^\star_h$.
        \end{proof}
%%%%%%%%%%%%%%%%%%%%%%%

%
\subsubsection{Proof of Lemma~\ref{lemma:lower_linear_upper_revenue}}
%
    \begin{proof} [Proof of Lemma~\ref{lemma:lower_linear_upper_revenue}]
        For any $i \in [H]$, we can write the $\GroundQ$-value for the policy $\pi$ in state $x^{(i)}_h$ at horizon $h \in [H]$ as follows:
            %
            \begin{align*}
                \GroundQ^\pi_h (x^{(i)}_h, \actionvector)
                &= 
                \begin{cases}
                    \frac{\gamma^{i-1}}{H} + \TransitionProbability_h(x^{(i)}_{h+1} | x^{(i)}_{h}, \actionvector) V^\pi_{h+1}(x^{(i)}_{h+1}) 
                    + \TransitionProbability_h(x^{(i)}_{H+2}| x^{(i)}_{h}, \actionvector) \frac{(H-h) \gamma^{i-1}}{H},
                    & \actionvector = \actionvector^\star_h;
                    \\
                    \frac{\gamma^{i}}{H} + \TransitionProbability_h(x^{(i+1)}_{h+1} | x^{(i)}_{h}, \actionvector) V^\pi_{h+1}(x^{(i+1)}_{h+1}) 
                    + \TransitionProbability_h(x^{(i+2)}_{H+2}| x^{(i)}_{h}, \actionvector) \frac{(H-h) \gamma^{i+1}}{H},
                    & \actionvector = \actionvector^\star_h, \OutsideOptionVector;
                    \\
                    0, & \actionvector = \OutsideOptionVector.
                \end{cases}
            \end{align*}
            %
        We provide a proof by considering the following cases:

        \textbf{Case (i) }
        $\actionvector^\star_h \in A$.

        Recall that, by~\eqref{eq:lemma_lower_linear_gamma}, we have
        %
        \begin{align*}
            \gamma = \frac{H}{1+H}
            \leq \frac{\exp\left( \phi(x^{(i)}_h, \actionvector^\star_h)^\top \thetab^\star_h \right)}{1/H + \exp\left( \phi(x^{(i)}_h, \actionvector^\star_h)^\top \thetab^\star_h \right)}.
            \numberthis \label{eq:lower_linear_gamma_upper}
        \end{align*}
        %
        % For any $\actionvector' \neq \OutsideOptionVector$, we define 
        % \begin{align*}
        %     \widetilde{Q}^\pi_h &(x^{(i)}_h, \actionvector^\star_h, \actionvector') 
        %     \\
        %     &:= 
        %     \begin{cases}
        %         \frac{\gamma^{i-1}}{H} + \TransitionProbability_h(x^{(i)}_{h+1} | x^{(i)}_{h}, \actionvector^\star_h) V^\pi_{h+1}(x^{(i)}_{h+1}) 
        %             + \TransitionProbability_h(x^{(i)}_{H+2}| x^{(i)}_{h}, \actionvector') \frac{(H-h) \gamma^{i-1}}{H},
        %         & \actionvector' = \actionvector^\star_h,
        %         \\
        %         \frac{\gamma^{i-1}}{H} + \TransitionProbability_h(x^{(i)}_{h+1} | x^{(i)}_{h}, \actionvector^\star_h) V^\pi_{h+1}(x^{(i)}_{h+1}) 
        %             + \TransitionProbability_h(x^{(i+1)}_{H+2}| x^{(i)}_{h}, \actionvector') \frac{(H-h) \gamma^{i-1}}{H},
        %         &\actionvector' \neq \actionvector^\star_h.
        %     \end{cases}
        % \end{align*}
        %
        By multiplying $\widetilde{Q}^\pi_h(x^{(i)}_h, \actionvector^\star_h, \actionvector')$ on both sides of~\eqref{eq:lower_linear_gamma_upper}, we get
        %
        \begin{align*}
            &\gamma \cdot \widetilde{Q}^\pi_h(x^{(i)}_h, \actionvector^\star_h, \actionvector')
            \leq \frac{
                \exp\left( \phi(x^{(i)}_h, \actionvector^\star_h)^\top \thetab^\star_h \right) 
                    \widetilde{Q}^\pi_h(x^{(i)}_h, \actionvector^\star_h, \actionvector')
                }{
                1/H + \exp\left( \phi(x^{(i)}_h, \actionvector^\star_h)^\top \thetab^\star_h \right)
            }
            \\ 
            &\Leftrightarrow
            \left(
                \sum_{\actionvector \in A \setminus \{ \actionvector^\star_h, \OutsideOptionVector  \} }
                \exp \left( \phi(x^{(i)}_h, \actionvector)^\top \thetab^\star_h \right)
            \right)
            \gamma \cdot \widetilde{Q}^\pi_h(x^{(i)}_h, \actionvector^\star_h, \actionvector')
            \left(
                1/H + \exp\left( \phi(x^{(i)}_h, \actionvector^\star_h)^\top \thetab^\star_h \right)
            \right)
            \\
            &\quad \leq \left(
                \sum_{\actionvector \in A \setminus \{ \actionvector^\star_h, \OutsideOptionVector  \} }
                \exp \left( \phi(x^{(i)}_h, \actionvector)^\top \thetab^\star_h \right)
            \right)
            \exp\left( \phi(x^{(i)}_h, \actionvector^\star_h)^\top \thetab^\star_h \right)
                \widetilde{Q}^\pi_h(x^{(i)}_h, \actionvector^\star_h, \actionvector')
            \\ 
            &\Leftrightarrow
            \left(
                \exp \left( \phi(x^{(i)}_h, \actionvector^\star_h)^\top \thetab^\star_h \right)
                +
                \!\!\! 
                \sum_{\actionvector \in A \setminus \{ \actionvector^\star_h, \OutsideOptionVector  \} }
                \!\!\!
                \gamma \cdot \exp \left( \phi(x^{(i)}_h, \actionvector)^\top \thetab^\star_h \right)
            \right)
            \widetilde{Q}^\pi_h(x^{(i)}_h, \actionvector^\star_h, \actionvector')
            \\
            &\quad\cdot \left(
                1/H + \exp\left( \phi(x^{(i)}_h, \actionvector^\star_h)^\top \thetab^\star_h \right)
            \right)
            \\
            &\quad \leq
            \left( 1/H + 
                \exp \left( \phi(x^{(i)}_h, \actionvector^\star_h)^\top \thetab^\star_h \right)
                + \!\!\! 
                \sum_{\actionvector \in A \setminus \{ \actionvector^\star_h, \OutsideOptionVector  \} }
                \!\!\!
                \exp \left( \phi(x^{(i)}_h, \actionvector)^\top \thetab^\star_h \right)
            \right)
            \exp\left( \phi(x^{(i)}_h, \actionvector^\star_h)^\top \thetab^\star_h \right) 
            \\
            &\quad\cdot  \widetilde{Q}^\pi_h(x^{(i)}_h, \actionvector^\star_h, \actionvector')
            \\ 
            &\Leftrightarrow
            \frac{
                \exp \left( \phi(x^{(i)}_h, \actionvector^\star_h)^\top \thetab^\star_h \right) \widetilde{Q}^\pi_h(x^{(i)}_h, \actionvector^\star_h, \actionvector')
                + 
                \sum_{\actionvector \in A \setminus \{ \actionvector^\star_h, \OutsideOptionVector  \} }
                \exp \left( \phi(x^{(i)}_h, \actionvector)^\top \thetab^\star_h \right)
                \gamma  \widetilde{Q}^\pi_h(x^{(i)}_h, \actionvector^\star_h, \actionvector')
                }{
                1/H + 
                \sum_{\actionvector \in A \setminus \{ \OutsideOptionVector  \} }
                \exp \left( \phi(x^{(i)}_h, \actionvector)^\top \thetab^\star_h \right)
                }
            \\
            &\quad \leq
            \frac{
                 \exp\left( \phi(x^{(i)}_h, \actionvector^\star_h)^\top \thetab^\star_h \right) \widetilde{Q}^\pi_h(x^{(i)}_h, \actionvector^\star_h, \actionvector')
                }{
                1/H + \exp\left( \phi(x^{(i)}_h, \actionvector^\star_h)^\top \thetab^\star_h \right)
                }. \numberthis \label{eq:lower_linear_upper_revenue_case1_1}
        \end{align*}
        %
        On the other hand, by the definition of $\widetilde{Q}^\pi_h(x^{(i)}_h, \actionvector^\star_h, \actionvector')$, for any $\actionvector' \neq \actionvector^\star_h, \OutsideOptionVector$, we have
        %
        \begin{align*}
            \gamma  \widetilde{Q}^\pi_h(x^{(i)}_h, \actionvector^\star_h, \actionvector')
            &= \gamma \cdot \left(
                \frac{\gamma^{i-1}}{H} + \TransitionProbability_h(x^{(i)}_{h+1} | x^{(i)}_{h}, \actionvector^\star_h) V^\pi_{h+1}(x^{(i)}_{h+1}) 
                    + \TransitionProbability_h(x^{(i+2)}_{H+2}| x^{(i)}_{h}, \actionvector') \frac{(H-h) \gamma^{i-1}}{H}
            \right)
            \\
            &= \frac{\gamma^{i}}{H} + \TransitionProbability_h(x^{(i)}_{h+1} | x^{(i)}_{h}, \actionvector^\star_h) V^\pi_{h+1}(x^{(i+1)}_{h+1}) 
                    + \TransitionProbability_h(x^{(i+2)}_{H+2}| x^{(i)}_{h}, \actionvector') \frac{(H-h) \gamma^{i}}{H} 
            \\
            &\geq
            \frac{\gamma^{i}}{H}  + \TransitionProbability_h(x^{(i)}_{h+1} | x^{(i)}_{h}, \actionvector') V^\pi_{h+1}(x^{(i+1)}_{h+1}) 
                    + \TransitionProbability_h(x^{(i+2)}_{H+2}| x^{(i)}_{h}, \actionvector') \frac{(H-h) \gamma^{i+1}}{H}
            \\
            &= \GroundQ^\pi_h (x^{(i)}_h, \actionvector')
            , \numberthis \label{eq:lower_linear_upper_revenue_case1_2}
        \end{align*}
        where the second equality holds since $\gamma V^\pi_{h+1}(x^{(i)}_{h+1}) = V^\pi_{h+1}(x^{(i+1)}_{h+1})$,
        and 
        the inequality holds because, for $K \geq 4 (\DimLinearMDP -5)^2 H (H+1)^2 $, the following inequality holds:
        %
        \begin{align*}
            &\TransitionProbability_h(x^{(i)}_{h+1} | x^{(i)}_{h}, \actionvector') V^\pi_{h+1}(x^{(i+1)}_{h+1}) 
                    + \TransitionProbability_h(x^{(i+2)}_{H+2}| x^{(i)}_{h}, \actionvector') \frac{(H-h) \gamma^{i+1}}{H}
            \\
            &\quad\leq  \TransitionProbability_h(x^{(i)}_{h+1} | x^{(i)}_{h}, \actionvector^\star_h) V^\pi_{h+1}(x^{(i+1)}_{h+1}) 
                    + \TransitionProbability_h(x^{(i+2)}_{H+2}| x^{(i)}_{h}, \actionvector') \frac{(H-h) \gamma^{i}}{H}
            \\
            &\Leftrightarrow
            \left(
                \TransitionProbability_h(x^{(i)}_{h+1} | x^{(i)}_{h}, \actionvector')
                - \TransitionProbability_h(x^{(i)}_{h+1} | x^{(i)}_{h}, \actionvector^\star_h)
            \right) V^\pi_{h+1}
            %
            \leq
                \TransitionProbability_h(x^{(i+2)}_{H+2}| x^{(i)}_{h}, \actionvector')
                \frac{(H-h) }{H} 
                \left(
                    \gamma^{i} - \gamma^{i+1}
                \right)
                .
        \end{align*}
        %
        Specifically, if the upper bound of the left-hand side is less than or equal to the lower bound of the right-hand side, the inequality holds. 
        To demonstrate this, we have:
        %
        \begin{align*}
            \left(
                \TransitionProbability_h(x^{(i)}_{h+1} | x^{(i)}_{h}, \actionvector')
                - \TransitionProbability_h(x^{(i)}_{h+1} | x^{(i)}_{h}, \actionvector^\star_h)
            \right) V^\pi_{h+1}
            \leq 2 (\DimLinearMDP - 5)\Delta \cdot \frac{(H-h)}{H}, 
            \numberthis \label{eq:lower_linear_Q_pi_case1_1}
        \end{align*}
        %
        and, since $\gamma^i
            = \left( \frac{H}{H+1} \right)^{i}
            \geq
            \left( \frac{H}{H+1} \right)^{H+1}
            \geq \frac{3}{10}$, 
        we get
        %
        \begin{align*}
             \TransitionProbability_h(x^{(i+2)}_{H+2}| x^{(i)}_{h}, \actionvector')
                \frac{(H-h) }{H} 
                \left(
                    \gamma^{i} - \gamma^{i+1}
                \right)
            &\geq
            \left(\delta - (\DimLinearMDP - 5) \Delta \right) 
                \frac{(H-h) }{H} 
                 \gamma^{i}
                \left(
                    1 - \gamma
                \right)
            \\
            &\geq
            \left(\frac{1}{H} - (\DimLinearMDP - 5) \Delta \right) 
                \frac{(H-h) }{H} 
                \cdot 
                \frac{3}{10}
                \cdot \frac{1}{H+1}.
            \numberthis \label{eq:lower_linear_Q_pi_case1_2}
        \end{align*}
        %
        Combining~\eqref{eq:lower_linear_Q_pi_case1_1} and~\eqref{eq:lower_linear_Q_pi_case1_2}, and rearranging the terms, we get
        %
        \begin{align*}
            (\DimLinearMDP - 5)\Delta \cdot
            \left(
                2 + \frac{3}{10(H+1)}
            \right)
            \leq 
            \frac{3}{10 H (H+1)},
        \end{align*}
        which holds when $K \geq 4 (\DimLinearMDP -5)^2 H (H+1)^2 $.
        This explains how the inequality in~\eqref{eq:lower_linear_upper_revenue_case1_2} is satisfied.

        Let $\bar{\actionvector}^{(i)}_h \in \argmax_{\actionvector \in A_h \setminus \{ \OutsideOptionVector \} } \GroundQ^\pi_h (x^{(i)}_h, \actionvector)
        = \actionvector^\star_h$.
        Note that $\bar{\actionvector}^{(i)}_h$  is unique due to the way the action space and transition probabilities are constructed.
        %
        Then, by combining~\eqref{eq:lower_linear_upper_revenue_case1_1} and~\eqref{eq:lower_linear_upper_revenue_case1_2}, and using the fact that $\GroundQ^\pi_h (x^{(i)}_h, \OutsideOptionVector)= 0$, we obtain that
        %
        \begin{align*}
            \sum_{\actionvector \in A} \ChoiceProbability_h(\actionvector | x^{(i)}_h, A ) \GroundQ^\pi_h (x^{(i)}_h, \actionvector)
            &\leq
            \sum_{\actionvector \in A} \ChoiceProbability_h(\actionvector | x^{(i)}_h, A ) \GroundQ^\pi_h (x^{(i)}_h, \bar{\actionvector}^{(i)}_h)
            \\
            &\leq 
            \frac{
                 \exp\left( \phi(x^{(i)}_h, \actionvector^\star_h)^\top \thetab^\star_h \right) \widetilde{Q}^\pi_h(x^{(i)}_h, \actionvector^\star_h, \bar{\actionvector}^{(i)}_h)
                }{
                1/H + \exp\left( \phi(x^{(i)}_h, \actionvector^\star_h)^\top \thetab^\star_h \right)
                }
            \\
            &= \frac{ \max_{\actionvector \in A \setminus \{\OutsideOptionVector\}} 
            \exp \left( 
                \phi(x^{(i)}_h, \actionvector)^\top \thetab^\star_h    
                \right) 
                \widetilde{Q}^\pi_h(x^{(i)}_h, \actionvector^\star_h, \bar{\actionvector}^{(i)}_h) }{
            1/H+   \max_{\actionvector \in A \setminus \{\OutsideOptionVector\}} \exp \left( 
                \phi(x^{(i)}_h, \actionvector)^\top \thetab^\star_h    
                \right)
                }    
            ,
        \end{align*}
        %
        where 
        the first inequality holds since $\bar{\actionvector}^{(i)}_h$ is the action that maximizes the $\GroundQ$-value.
        The second inequality follows from~\eqref{eq:lower_linear_upper_revenue_case1_1} and~\eqref{eq:lower_linear_upper_revenue_case1_2}, and from the fact that
        $\GroundQ^\pi_h (x^{(i)}_h, \bar{\actionvector}^{(i)}_h) = 
        \GroundQ^\pi_h (x^{(i)}_h, \actionvector^\star_h)
        = \widetilde{Q}^\pi_h(x^{(i)}_h, \actionvector^\star_h, \bar{\actionvector}^{(i)}_h)
        $.
        Finally,
        the last equality holds by the definition of $\actionvector^\star_h$.
        
        %
        \textbf{Case (ii) }
        $\actionvector^\star_h \notin A$.

        Again, by~\eqref{eq:lemma_lower_linear_gamma}, for any $A \in \SetOfActions$, we have
        %
        \begin{align*}
            \gamma 
            &\leq
            \min_{\actionvector \in A \setminus \{\OutsideOption\} } 
                \frac{\exp\left( \phi(x^{(i)}_h, \actionvector)^\top \thetab^\star_h \right)}{1/H + \exp\left( \phi(x^{(i)}_h, \actionvector)^\top \thetab^\star_h \right)}
            \\
            &\leq \frac{ \max_{\actionvector \in A \setminus \{\OutsideOption\} }  \exp\left( \phi(x^{(i)}_h, \actionvector)^\top \thetab^\star_h \right)}{1/H + \max_{\actionvector \in A \setminus \{\OutsideOption\} } \exp\left( \phi(x^{(i)}_h, \actionvector)^\top \thetab^\star_h \right)}
            \\
            &\leq 
            \frac{ \max_{\actionvector \in A \setminus \{\OutsideOption\} }  \exp\left( \phi(x^{(i)}_h, \actionvector)^\top \thetab^\star_h \right)}{1/H + \max_{\actionvector \in A \setminus \{\OutsideOption\} } \exp\left( \phi(x^{(i)}_h, \actionvector)^\top \thetab^\star_h \right)}
            \cdot \frac{1/H + \sum_{\actionvector \in A \setminus \{ \OutsideOptionVector \}} \exp\left( \phi(x^{(i)}_h, \actionvector)^\top \thetab^\star_h \right)  }{
            \sum_{\actionvector \in A \setminus \{ \OutsideOptionVector \}} \exp\left( \phi(x^{(i)}_h, \actionvector)^\top \thetab^\star_h \right) 
            }, 
        \end{align*}
        where the second inequality holds since the sigmoid function is a monotonically increasing function.
        
        We denote $\bar{\actionvector}^{(i)}_h \in \argmax_{\actionvector \in A_h \setminus \{ \OutsideOptionVector \} } \GroundQ^\pi_h (x^{(i)}_h, \actionvector)$.
        Then, multiplying $\widetilde{Q}^\pi_h(x^{(i)}_h, \actionvector^\star_h, \bar{\actionvector}^{(i)}_h)$ (note that $\bar{\actionvector}^{(i)}_h \neq \actionvector^\star_h, \OutsideOptionVector$) on both sides and rearranging terms, we get
        %
        \begin{align*}
            &\frac{\sum_{\actionvector \in A \setminus \{ \OutsideOptionVector \}} 
            \exp\left( \phi(x^{(i)}_h, \actionvector)^\top \thetab^\star_h \right) \gamma \cdot \widetilde{Q}^\pi_h(x^{(i)}_h, \actionvector^\star_h, \bar{\actionvector}^{(i)}_h) }{
            1/H + \sum_{\actionvector \in A \setminus \{ \OutsideOptionVector \}} \exp\left( \phi(x^{(i)}_h, \actionvector)^\top \thetab^\star_h \right)
            }
            \\
            &\quad\quad\quad\quad\quad\quad\quad\leq 
            \frac{ \max_{\actionvector \in A \setminus \{\OutsideOption\} }  \exp\left( \phi(x^{(i)}_h, \actionvector)^\top \thetab^\star_h \right) \widetilde{Q}^\pi_h(x^{(i)}_h, \actionvector^\star_h, \bar{\actionvector}^{(i)}_h)  }
            {
            1/H + \max_{\actionvector \in A \setminus \{\OutsideOption\} } \exp\left( \phi(x^{(i)}_h, \actionvector)^\top \thetab^\star_h \right)}
            .
        \end{align*}
        %
        Recall that for any $\actionvector' \neq \actionvector^\star_h, \OutsideOptionVector$, we have $ \gamma \cdot \widetilde{Q}^\pi_h(x^{(i)}_h, \actionvector^\star_h, \actionvector') \geq  \GroundQ^\pi_h (x^{(i)}_h, \actionvector')$ by \eqref{eq:lower_linear_upper_revenue_case1_2}.
        Thus, we get
        %
        \begin{align*}
            \sum_{\actionvector \in A} \ChoiceProbability_h(\actionvector | x^{(i)}_h, A ) \GroundQ^\pi_h (x^{(i)}_h, \actionvector)
            &\leq \sum_{\actionvector \in A} \ChoiceProbability_h(\actionvector | x^{(i)}_h, A ) \GroundQ^\pi_h (x^{(i)}_h, \bar{\actionvector}^{(i)}_h)
            \\
            &\leq \sum_{\actionvector \in A} \ChoiceProbability_h(\actionvector | x^{(i)}_h, A ) 
            \gamma \cdot \widetilde{Q}^\pi_h(x^{(i)}_h, \actionvector^\star_h, \bar{\actionvector}^{(i)}_h) 
            \\
            &\leq 
            \frac{ \max_{\actionvector \in A \setminus \{\OutsideOption\} }  \exp\left( \phi(x^{(i)}_h, \actionvector)^\top \thetab^\star_h \right)  \widetilde{Q}^\pi_h(x^{(i)}_h, \actionvector^\star_h, \bar{\actionvector}^{(i)}_h) 
            }{
            1/H + \max_{\actionvector \in A \setminus \{\OutsideOption\} } \exp\left( \phi(x^{(i)}_h, \actionvector)^\top \thetab^\star_h \right)}.
        \end{align*}
        This concludes the proof of Lemma~\ref{lemma:lower_linear_upper_revenue}.
    \end{proof}
    %



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
