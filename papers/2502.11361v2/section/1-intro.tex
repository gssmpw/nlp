\section{Introduction}

\begin{figure}[ht]
   \centering
    \includegraphics[width=0.44\textwidth]{figures/example.pdf} % Adjusts to column width
    \caption{Visual \& Textual Disinformation Example: Amplifying fear (left: false biohazard imagery) and controversy (gender biases in sports), distorting perception through fabricated associations and emotional manipulation.}
    \label{fig:multimodal_bias_framework}
\end{figure}

The global spread of false or misleading content has reached alarming levels: with 60\% of individuals worldwide reporting encounters with false stories on digital platforms \cite{RedlineFakeNews2024}, and 94\% of journalists identifying fabricated news as a major threat to public trust \cite{pew2022}. The World Economic Forum \cite{WEF2024GlobalRisks} ranks \textbf{misinformation} (unintentionally shared false content) and \textbf{disinformation} (deliberately deceptive information) among the top global risks for 2024. This distinction is critical: while misinformation spreads without malicious intent, disinformation is weaponized, exploiting the ambiguity of digital ecosystems to erode trust. Despite advancements in AI safety research, mitigating bias disinformation detection—especially in multimodal contexts—remains largely underexplored, posing significant societal risks and technical challenges in analyzing text-visual interplay (see Figure \ref{fig:multimodal_bias_framework}).

\input{tables/data_comparison}

Governments and tech firms are actively combating disinformation. Regulations like the EU Action Plan, the U.S. Digital Fairness Act, and Canada AI initiatives emphasize accountability and transparency (see Appendix~\ref{sec:disinformation_regulations}). Industry leaders—Google, Meta, Microsoft, and Anthropic—have committed to technical countermeasures. These efforts also align with the AI Risk Repository 2024 (AIRR-2024), which classifies disinformation as a systemic threat (Appen. \ref{app:airr}).

Existing LLM safety benchmarks, such as SafetyBench \cite{zhang2023safetybench} (LLM safety), AISB \cite{vidgen2024introducingv05aisafety} (hazard assessment) focus on generic harms (e.g., toxicity, bias). 
% , and SALAD-Bench \cite{li-etal-2024-salad} (attack/defense) focus on generic harms (e.g., toxicity, bias). 
Multimodal benchmarks, such as MM-SafetyBench \cite{liu2025mm}, SB-Bench \cite{narnaware2025sb} (Social Bias), VLGuard \cite{zong2024safety}, ALM-Bench \cite{vayani2024all} (Cultural Bias), and LLM evaluations (Rainbow Teaming \cite{samvelyan2024rainbowteaming}, 
% HarmBench \cite{mazeika2024harmbench}) 
prioritize robustness over intentional deception. Although frameworks like \cite{papado2023misinformer,Qi_2024_CVPR} integrate text and visuals, and \cite{li2024towards} targets deepfakes, none explicitly address disinformation (e.g., political propaganda, health scams) in multi modal context  (see Table \ref{tab:comparison}).

To address these challenges, we introduce \textsf{\textbf{\textsc{VLDBench}}}, an extensive and thorough human-verified multimodal benchmark for disinformation detection. It is the first community-driven, iteratively refined benchmark designed to assess both unimodal and multimodal open-source models, with a strong focus on the depth of diverse news categories. Our evaluation of 19 open-source models (10 VLMs, 9 LLMs) shows that VLMs consistently outperform text-only LLMs, emphasizing the need for both textual and visual cues in disinformation detection (Section~\ref{sec:multimodal_vs_unimodal}). These models demonstrate resilience to minor perturbations in individual modalities (such as image noise or textual paraphrasing) but their performance drops significantly when adversarial attacks target both text and image inputs simultaneously (Section~\ref{sec:resilience}). This weakness is most evident in cross-modal misalignment, where contradictory captions or mismatched text-image pairs confuse the models. Our contributions are summarized as:
\begin{itemize}[itemsep=0pt]
    \item We present \textsf{\textbf{\textsc{VLDBench}}}, the most comprehensive human-verified multimodal benchmark for disinformation detection. Curated from 58 diverse news sources, it contains 31.3k news article-image pairs spanning 13 distinct categories. Designed in accordance with ethical guidelines \cite{uwaterloo_ethics_review}, \textsf{\textbf{\textsc{VLDBench}}} supports both binary classification and multimodal evaluation.
    \item \textsf{\textbf{\textsc{VLDBench}}} is meticulously curated and verified by 22 domain experts, with over 300 hours of human annotation. This rigorous process ensures high-quality data curation, achieving high label accuracy (Cohen’s $\kappa = 0.78$).
    \item We benchmark existing LLMs and VLMs on \textsf{\textbf{\textsc{VLDBench}}}, identifying performance gaps and areas for improvement in addressing the challenges of multimodal disinformation in various contexts. 
\end{itemize}
