\begin{abstract}
\textit{The rapid rise of AI-generated content has made detecting disinformation increasingly challenging. In particular, multimodal disinformation, i.e., online posts/articles that contain images and texts with fabricated information are specially designed to deceive. While existing AI safety benchmarks primarily address bias and toxicity, multimodal disinformation detection remains largely underexplored. 
To address this challenge, we present the Vision-Language Disinformation Detection Benchmark \textbf{(VLDBench)}—the first comprehensive benchmark for detecting disinformation across both unimodal (text-only) and multimodal (text and image) content, comprising \textbf{31,000} news article-image pairs, spanning 13 distinct categories, for robust evaluation. \textbf{VLDBench} features a rigorous semi-automated data curation pipeline, with \textbf{22 domain experts dedicating 300+ hours} to annotation, achieving a strong inter-annotator agreement (Cohen’s $\kappa = 0.78$).  
We extensively evaluate state-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs), demonstrating that integrating textual and visual cues in multimodal news posts improves disinformation detection accuracy by 5–35\% compared to unimodal models. Developed in alignment with AI governance frameworks such as the EU AI Act, NIST guidelines, and the MIT AI Risk Repository 2024, \textbf{VLDBench} is expected to become a benchmark for detecting disinformation in online multi-modal contents. 
% Our code and data will be publicly available.
} \url{https://vectorinstitute.github.io/VLDBench}
% \href{https://vectorinstitute.github.io/VLDBench/}{\textcolor{purple}{\textbf{here}}}.}
\end{abstract}