

\begin{table*}[ht]
\centering
\footnotesize
\resizebox{\textwidth}{!}{ 
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Accuracy (\%)} & \textbf{Precision (\%)} & \textbf{Recall (\%)} & \textbf{F1-Score (\%)} \\
\midrule
\multicolumn{5}{c}{\textit{Language-only LLMs}} \\
\midrule
Phi-3-mini-128k-Instruct \cite{phi3mini2024} & 57.15 & 55.12 & 58.21 & 56.62 \\
Vicuna-7B-v1.5    \cite{zheng2023judging}       & 55.21 & 56.78 & 52.48 & 54.55 \\
Mistral-7B-Instruct-v0.3 \cite{mistral20237b} & 68.58 & 68.10 & 65.09 & 66.56 \\
Qwen2-7B-Instruct  \cite{yang2024qwen2}       & 69.92 & 68.34 & 69.10 & 68.72 \\
InternLM2-7B \cite{cai2024internlm2} & 51.19 & 49.85 & 51.16 &  50.50 \\
DeepSeek-V2-Lite-Chat \cite{liu2024deepseek} & 51.96 & 52.61 & 53.53 & 51.96 \\
 GLM-4-9B-chat \cite{glm2024chatglm}& 51.14& 60.19& 53.28& 56.52\\
LLaMA-3.1-8B-Instruct \cite{meta2024llama}    & 68.21 & 62.13 & 62.10 & 62.11 \\
LLaMA-3.2-1B-Instruct \cite{meta2024llama}   & 70.29 & 69.78 & 68.92 & 69.35 \\
\midrule
\multicolumn{5}{c}{\textit{Vision-Language Models (VLMs)}} \\
\midrule
Phi-3-Vision-128k-Instruct \cite{agrawal2024pixtral} & 64.18 \textcolor{green}{(10.06\% $\uparrow$)} & 63.18 \textcolor{green}{(13.35\% $\uparrow$)} & 62.88 \textcolor{green}{(9.18\% $\uparrow$)} & 63.03 \textcolor{green}{(11.25\% $\uparrow$)} \\
LLaVA-v1.5-Vicuna7B  \cite{NEURIPS2023LLaVA}       & 72.32 \textcolor{green}{(27.37\% $\uparrow$)} & 68.12 \textcolor{green}{(16.34\% $\uparrow$)} & 64.88 \textcolor{green}{(18.71\% $\uparrow$)} & 66.46 \textcolor{green}{(17.54\% $\uparrow$)} \\
LLaVA-v1.6-Mistral-7B   \cite{NEURIPS2023LLaVA}     & 72.38 \textcolor{green}{(2.52\% $\uparrow$)} & 70.18 \textcolor{green}{(3.45\% $\uparrow$)} & 70.03 \textcolor{green}{(6.53\% $\uparrow$)} &  70.10 \textcolor{green}{(5.00\% $\uparrow$)} \\
Pixtral   \cite{mistral2024pix}                    & 70.18 \textcolor{green}{(2.33\% $\uparrow$)} & 72.32 \textcolor{green}{(6.20\% $\uparrow$)} & 70.23 \textcolor{green}{(7.90\% $\uparrow$)} & 71.26 \textcolor{green}{(7.06\% $\uparrow$)}  \\
Qwen2-VL-7B-Instruct  \cite{Qwen2VL}        & 67.28 \textcolor{red}{(-3.77\% $\downarrow$)} & 69.18 \textcolor{red}{(-1.46\% $\downarrow$)} & 68.45 \textcolor{red}{(-6.72\% $\downarrow$)} & 68.81 \textcolor{red}{(-4.16\% $\downarrow$)} \\
InternVL2-8B \cite{chen2024internvl} & 63.57 \textcolor{green}{(24.18\% $\uparrow$)} & 68.34 \textcolor{green}{(37.09\% $\uparrow$)} & 65.10 \textcolor{green}{(27.25\% $\uparrow$)} & 66.68 \textcolor{green}{(32.04\% $\uparrow$)} \\

Deepseek-VL2-small \cite{wu2024deepseek} & 68.15 \textcolor{green}{(31.16\% $\uparrow$)} & 64.84 \textcolor{green}{(23.25\% $\uparrow$)} & 75.67 \textcolor{green}{(41.36\% $\uparrow$)} & 69.84 \textcolor{green}{(34.41\% $\uparrow$)} \\

Deepseek Janus-Pro-7B \cite{chen2025janus} & 70.04 \textcolor{green}{(34.80\% $\uparrow$)} & 69.97 \textcolor{green}{(33.00\% $\uparrow$)} & \textbf{74.14} \textcolor{green}{(38.50\% $\uparrow$)} & 71.99 \textcolor{green}{(38.55\% $\uparrow$)} \\

GLM-4V-9B      \cite{glm2024chatglm}             & 62.13\textcolor{green}{(21.49\% $\uparrow$)} & 63.35\textcolor{green}{(5.25\% $\uparrow$)} & 75.33\textcolor{green}{(41.385\% $\uparrow$)} & 68.82\textcolor{green}{(21.76\% $\uparrow$)} \\

LLaMA-3.2-11B-Vision \cite{meta2024llama} & \textbf{74.82} \textcolor{green}{(6.44\% $\uparrow$)} & \textbf{72.62} \textcolor{green}{(4.07\% $\uparrow$)} & 72.28 \textcolor{green}{(4.88\% $\uparrow$)} & \textbf{72.45} \textcolor{green}{(4.47\% $\uparrow$)} \\

\bottomrule
\end{tabular}
}
\caption{Zero-shot Performance Comparison: Language-Only Models (Top) vs. Vision-Language Models (Bottom) on \textbf{VLDBench}. \textbf{Bold} indicates the best result in each metric. Values are averaged across three runs. The \textcolor{green}{$\uparrow$} means increase (\%) and the \textcolor{red}{$\downarrow$} means decrease (\%).}

\vspace{-1em}
\label{tab:main}
\end{table*}


\section{Benchmarking Multimodal Models on VLDBench}
\label{sec:results}

We benchmark ten state-of-the-art open-source VLMs and nine LLMs on \textsf{\textbf{\textsc{VLDBench}}}, evaluating LLMs on text-only tasks and VLMs on multimodal analysis (text + images). The entire dataset is used for evaluation, while for instruction fine-tuning (IFT), it is split into 70\% training, 15\% validation, and 15\% testing subsets. We focus on open-source LLMs and VLMs to promote accessibility and transparency in our research. Additionally, since GPT-4o is used for annotations in this study, evaluating open models allows for a more comprehensive comparison. The evaluation process includes both quantitative and qualitative assessments. Quantitative metrics including accuracy, precision, recall, and F1-score are conducted on both zero-shot and IFT LLMs and VLMs. Domain experts verify model predictions and rationales to ensure alignment with real-world disinformation detection standards. Our investigation focuses on three core questions: 
(1)~Does multimodal (text+image) data improve disinformation detection compared to text alone? 
(2)~Does instruction fine-tuning enhance generalization and robustness? 
(3)~How vulnerable are models to adversarial perturbations in text and image modalities?


\subsection{Multimodal Models Surpass Unimodal Baselines}
\label{sec:multimodal_vs_unimodal}
Table~\ref{tab:main} shows that VLMs generally outperform language-only LLMs. For example, LLaMA-3.2-11B-Vision outperforms LLaMA-3.2-1B (text-only). Similarly, Phi, LLaVA, Pixtral, InternVL, DeepSeek-VL, and GLM-4V perform better than their text-only counterparts. We observe LLaMA-3.2-11B-Vision achieving state-of-the-art results (74.82\% accuracy, 72.45\% F1), demonstrating the multimodal advantage. The performance gains between these two sets of models are quite pronounced. For instance, LLaVA-v1.5-Vicuna7B improves accuracy by 27\% over its unimodal base (Vicuna-7B), highlighting the critical role of visual context. However, Qwen2-VL-7B marginally lags behind its text-only counterpart (67.28\% vs. 69.92\% accuracy), suggesting that the effectiveness of modality integration can vary depending on the model's architecture. While top LLMs (\textit{e.g.}, Llama3.2-1B-Instruct, 70.29\% accuracy) remain competitive, VLMs excel in recall (up to 75.33\% for GLM-4V-9B), a vital trait for minimizing missed disinformation in adversarial scenarios.

\subsection{IFT on VLDBench Enhances Performance}
\label{sec:instruction_ft}

In our study, we implemented incremental fine-tuning (IFT) on a range of models including Phi-3-mini-128K, Phi-3-Vision-128K, Mistral-LLaVA, Qwen2-7B, Qwen2-VL-7B, LLaMA-3.2-1B, and LLaMA-3.2-11B-Vision, utilizing the training subset of \textsf{\textbf{\textsc{VLDBench}}}. As depicted in Figure~\ref{fig:ift}, IFT contributed to significant performance improvements across all models compared to their zero-shot capabilities. For instance, the Phi-3-mini-128K and its VLM counterpart both showed a notable 7\% increase in F1-score over their zero-shot baselines. This enhancement is not solely due to better output formatting; rather, it reflects the model's ability to adapt to and learn from disinformation-specific cues in the data.
\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figures/ift.png}
 \caption{Performance comparison of zero-shot vs. instruction-fine-tuned (IFT) performance, with 95\% confidence intervals computed from three independent runs.}
    \label{fig:ift}
\end{figure}

\subsection{Adversarial Robustness: Combined Modality is More Vulnerable}
\label{sec:resilience}
\paragraph{Text and Image Attacks:}
We tested each model under controlled perturbations (zero-shot evaluation), including textual and image perturbations. Textual perturbations include synonym substitution, misspellings, and negations. Image perturbations include blurring (Gaussian blur with kernel size (3,3), Gaussian noise with $\mu=0$, $\sigma=0.1$ applied to pixel values and images are scaled to 50\% and 200\% of the original size (details in Appendix \ref{app:perturbations} and Table \ref{tab:perturbations_desc}). We categorize this experiment for \textit{single modality attacks}: text perturbations (T-P) (Figure \ref{fig:perturbation_text}),  image perturbations (I-P), and \textit{multi modality attacks}: cross-modal misalignment (C-M) (e.g., mismatched image captions) and both modality perturbations (B-P) (both text and image distortions)  (Figure \ref{fig:perturbation_image}).

\input{tables/perturbations}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/perturbations/perturbation_text.pdf}
    \caption{We describe the text perturbations in the caption, introducing \textit{Synonym}, \textit{Misspelling}, and \textit{Negation}. Our analysis shows that \textbf{text negation} leads to a majority of disinformation cases.}
    \label{fig:perturbation_text}
    \vspace{-1em}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/perturbations/perturbation_image.pdf}
    \caption{We describe the image perturbations in the caption, introducing \textit{Blur}, \textit{Noise}, and \textit{Resizing}, \textit{Cross-Modal (C-M) Mismatch}, \textit{Both-Modality (B-M)}. Our analysis shows that \textbf{C-M} and \textbf{B-M} leads to a majority of disinformation cases.}
    \label{fig:perturbation_image}
    \vspace{-1em}
\end{figure}

Table~\ref{tab:perturbations1} shows the model robustness under controlled perturbations. Key findings include: (1) VLMs generally outperform LLMs on single modality perturbations (Text and Image), but they show much drops in F1 scores under cross-modal (C-M) and combined attacks (B-P). (2)  Unimodal LLMs, e.g., LLaMA-3.2-1B and Mistral-7B, show significant vulnerability to text perturbations, with an average F1 drop of $3.3\%$ for text perturbations compared to $2.5\%$ for image perturbations. (3) VLMs such as Phi-3-vision degrade more severely under combined attacks (B-P), losing up to $11.2\%$, indicating a pronounced vulnerability compared to their larger counterparts like LLaMA-3.2-11B, which shows a drop of $10.2\%$.
Overall, these results show that results are most severely impacted by combined text and image perturbations, which highlights a crucial area for research to improve their robustness.

\paragraph{Combined Attacks.}
Table~\ref{tab:adversarial_performance} shows that combining text+image adversarial attacks can cause catastrophic performance drops in high-capacity models (IFT LLama3.2-11B: 26.4\% decrease). These findings illustrate that multimodal methods, despite generally higher baseline accuracy, remain susceptible when adversaries deliberately target both modalities. Future work may explore more robust architectures or data augmentation to mitigate these coordinated attacks.

\begin{table}[h]
\centering
\resizebox{1\columnwidth}{!}{
\begin{tabular}{@{}llr@{}}
\toprule
\textbf{Model/Condition} & \textbf{Scenario} & \textbf{Acc. ($\Delta$\%)} \\ 
\midrule
LLama3.2-1B-Original & No adversaries & \cellcolor{green!30} 75.90 (-) \\  
LLama3.2-1B-Text & Synonym/misspell/negate & \cellcolor{yellow!40} 60.85 ($\Delta$15.05) \\  
LLama3.2-11B-Combined & Text+image attacks\textsuperscript{§} & \cellcolor{red!40} 53.54 ($\Delta$22.36) \\  
\bottomrule
\end{tabular}
}
\caption{Adversarial Performance Drops ($\Delta$ = Accuracy Decrease).  
\textsuperscript{§} Combined text+img. attacks.}
\vspace{-1em}
\label{tab:adversarial_performance}
\end{table}

\subsection{Human Evaluation Establishes Reliability and Reasoning Depth}  
\label{sec:human_eval}  

\begin{table}[ht]
    \centering
    \small
    \resizebox{1\columnwidth}{!}{
    \begin{tabular}{lccc}
    \toprule
    \textbf{Model} & \textbf{Acc.(\%)}& \textbf{PC (1--5)} & \textbf{RC (1--5)} \\
    \midrule
    LLaMA-3.2-11B-V & 75.2& 3.8 $\pm$ 0.7 & 3.5 $\pm$ 0.8 \\
    % Pixtral      & 72.4 & 3.5 $\pm$ 0.8 & 3.2 $\pm$ 0.9 \\
    LLaVA-v1.6   & 72.0& 3.1 $\pm$ 0.9 & 3.0 $\pm$ 1.0 \\
    \bottomrule
    \end{tabular}}
    \caption{Human evaluation on a 500-sample test set. Acc. = accuracy, PC = prediction correctness, RC = reasoning clarity. Mean $\pm$ std.\ shown.}
    \label{tab:reasons}
\end{table}

To assess model performance, we conducted a human evaluation of two IFT VLMs (LlaMA-3.2-11B, LLaVA-v1.6) on a balanced 500-sample test set (250 disinformation, 250 neutral). Each model classified each sample as disinformation or neutral and provided a rationale for its decision. Three independent reviewers, blinded to model identities, assessed the outputs based on two key criteria: \textit{Prediction Correctness (PC)}—how well the model classification aligned with the ground truth on a scale of 1–5, and \textit{Reasoning Clarity (RC)}—the clarity and coherence of the model explanation, also rated on a scale of 1–5. Table~\ref{tab:reasons}, LLaMA~3.2-11B achieves a good accuracy (75.2\%) with coherent explanations, while  LLaVA-v1.6 shows slightly lower correctness and lacks required verbosity. Figure~\ref{fig:reasoning-example} shows a representative example of model reasoning and highlights variation in the explanatory quality of different systems.


\begin{figure}[h] 
    \centering
    \includegraphics[width=\columnwidth]{figures/reasoning-example.pdf} 
   \caption{Human evaluation results on a 500-sample test set. Models were tasked with classifying disinformation and justifying their predictions. PC = prediction correctness, RC = reasoning clarity (mean $\pm$ std.).}
   \vspace{-1em}
    \label{fig:reasoning-example}
\end{figure}
\paragraph{AI Risk Mitigation:} \label{sec:arr}  

Our methodology and results aligns with the identified risk mitigation as defined in AIRR-2024 risk framework. For instance, we enforce strict data handling to mitigate privacy risks (\#2.1) and apply systematic filtering to curb information pollution (\#3.2). Human review ensures contextual accuracy (\#5.1), while iterative benchmarking and audits enhance robustness (\#7.3) and transparency (\#7.4). Our evaluation includes repeated runs (three per model), diverse adversarial tests, and independent human validation, ensuring model reliability and responsible AI deployment.
