
\section{Related Work}
\paragraph{Disinformation as a Global Threat:}
Disinformation is a pervasive societal risk, explicitly flagged as a systemic threat in the AI Risk Repository 2024 (AIRR-2024). Legislative efforts—including the EU Digital Services Act, UK Online Safety Act, New York Digital Fairness Act, and Canada’s Online News Act (Appendix~\ref{sec:disinformation_regulations})—emphasize the urgent need to align regulatory mandates with technological countermeasures. These frameworks underscore the dual challenge of mitigating disinformation while preserving free expression.
\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/vld_architecture.pdf}
    
\caption{\textsf{\textbf{\textsc{VLDBench}}} is a multimodal disinformation detection benchmark, focusing on LLM/VLM benchmarking, human-AI collaborative annotation, and risk mitigation. We operates through a three-stage pipeline to curate \textsf{\textbf{\textsc{VLDBench}}} data, evaluate and to study its effectiveness:  
(1) \emph{Data} (collection, filtering, and quality assurance of text-image pairs),  
(2) \emph{Annotation} (GPT-4 labeling with human validation),  
(3) \emph{Benchmarking} (prompt-based evaluation and robustness testing).}  
    \vspace{-1em}
    \label{fig:vlbias}
\end{figure*}
\paragraph{Detection Methodologies:}
Disinformation, characterized by intentional deception, manifests through misleading contextual framing, partisan narratives, or fabricated content~\cite{hivemind_disinformation}. Unlike misinformation (unintentional falsehoods), its adversarial nature demands robust detection strategies. Early approaches relied on linguistic patterns and rule-based classifiers~\cite{rubin2016fake}, later evolving into machine learning models using RNNs, CNNs, and variational autoencoders~\cite{bahad2019fake,khattar2019mvae}. Multimodal frameworks like CARMN~\cite{song2021multimodal} (cross-modal attention), MCNN~\cite{segura2022multimodal} (multimodal CNNs), and MCAN~\cite{wu2021multimodal} (context-aware fusion) integrate text and visuals, offering foundational insights for disinformation detection.

The rise of LLMs like ChatGPT has enhanced semantic analysis for detection tasks~\cite{chen2023combating}, though their potential to amplify disinformation remains a concern. Recent work leverages LLMs to identify deceptive patterns in online content~\cite{jiang2024disinformation,hasanain2024gpt,huang2024fakegpt,pan2024enhancing}, while VLMs like \cite{papado2023misinformer,Qi_2024_CVPR,xuan2024lemma} improve accuracy through text-image alignment. Frameworks such as FakeNewsGPT4~\cite{liu2024fakenewsgpt4} and LEMMA~\cite{xuan2024lemma} integrate external knowledge for adversarial robustness, yet struggle with disinformation’s inherent subjectivity and contextual ambiguity.
\paragraph{Limitations of Existing Datasets:}
Most existing datasets focus on misinformation (not much focus on disinformation), with variability in modality, annotation, and accessibility hindering standardization. Text-based datasets (LIAR, FEVER, BuzzFace) lack multimodal signals, while multimodal collections (Fakeddit, Factify 2~\cite{suryavardan2023factify2multimodalfake}, MuMiN) often use distant supervision or lack disambiguation (Table~\ref{tab:comparison}). Two critical research gaps persist: (1) disinformation’s rapid evolution renders pre-2022 datasets as obsolete; and (2) most datasets lack alignment with modern generative AI capabilities.

\textsf{\textbf{\textsc{VLDBench}}} addresses these gaps as the first benchmark explicitly designed for disinformation detection. Unlike prior work, it curates recent, temporally relevant data to reflect current disinformation tactics. The dataset provides multiple news categories explicitly labeled for disinformation detection that are validated through human review.
It aligns with regulatory best practices for AI accountability and transparency.