\documentclass[10pt]{article}
\usepackage{times}
\usepackage{latexsym}
\usepackage{url}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{microtype} % Helps optimize text layout
\usepackage{hyperref} % Enables clickable links
\usepackage{lineno}
\usepackage{xcolor}
\usepackage{graphics}
\usepackage[table,dvipsnames]{xcolor}
\linenumbers

% Adjust spacing to fit within one page
\setlength{\textfloatsep}{6pt}
\setlength{\parskip}{0pt} 
\setlength{\parindent}{0.5em}

\begin{document}
\title{Response to Reviewers - ACL ARR Resubmission}
\date{}

\maketitle

\noindent \textbf{Paper URL}: \href{https://openreview.net/forum?id=rowRNeCMcW#discussion}{https://openreview.net/forum?id=rowRNeCMcW} \\

\noindent
Dear ACL ARR Reviewers and Action Editors, \\

We appreciate the insightful feedback from the reviewers and AC. We are grateful they found the usefulness of this disinformation detection dataset. Their suggestions have helped refine our work.  Below is a summary of key revisions:

  \centerline{\rule{12cm}{0.4pt}}
    \bigskip

\textcolor{purple}{\textbf{Area Chair RqXN}}  \\
\textcolor{purple}{(1) An error analysis of the type of perturbations that most influences the prediction.} 

\textcolor{blue}{\textbf{Response}} We have revised our work based on the initial review. 

First, as requested, we have added more details and examples of textual attacks. To summarize, there are four types of perturbations: (1) \textbf{Text Perturbations (T-P),} (2) \textbf{Image Perturbations (I-P),} (3) \textbf{Cross-Modal Misalignment (C-M)}, and (4) \textbf{Both-Modality Perturbations (B-P).} [line 399 - 415 in main paper, and 1338 -1374 and Table 11 in Appendix]

\begin{enumerate}
\vspace{-0.3cm}
    \item \textbf{T-P} involves modifications to the text, such as paraphrasing, word substitutions, or adversarial noise. For example, Synonym substitution, misspellings (20\% chance), negation (adding ``not"/``never").
    \item \textbf{I-P} includes alterations to images, such as blurring, cropping, or adding adversarial noise.  Blurring (3,3 kernel), noise ($\mu=0$, $\sigma=0.1$), resizing (50\%/200\%).
    \item \textbf{C-M} refers to inconsistencies between text and images, like mismatched captions or misleading visual-text relationships. Mismatched image-text pairs, contradictory captions.
    \item \textbf{B-P} affects both modalities (combined text and image distortions) simultaneously to create more challenging adversarial cases.
    \vspace{-0.3cm}

\end{enumerate}


\begin{table*}[h!]
\centering
\footnotesize
\renewcommand{\arraystretch}{1}
\setlength{\tabcolsep}{8pt}
\begin{tabular}{|p{3cm}|p{8cm}|}
\hline
\textbf{Perturbation Type} & \textbf{Description} \\ \hline
\textbf{Text Perturbations (T-P)} & Adversarial modifications applied to textual inputs, such as word substitutions, paraphrasing, or negation-based changes. These perturbations test the model's robustness to textual manipulations. \newline
\textbf{Synonym Substitution:} Word substitutions via TextAttack. \newline
\textbf{Misspellings:} Character swaps/insertions, 20\% chance per word. \newline
\textbf{Negation:} Insert ``not" or ``never" to invert meaning.\\ \hline  
\textbf{Image Perturbations (I-P)} & Visual modifications applied to images, including noise addition, blurring, or adversarial transformations. These perturbations assess the model’s sensitivity to altered visual inputs. \newline
\textbf{Blurring:} Apply Gaussian blur, kernel size (3,3). \newline
\textbf{Noise:} Add Gaussian noise, $\mu=0$, $\sigma=0.1$. \newline
\textbf{Resizing:} Scale images to 50\% or 200\% of original size.\\ \hline  
\textbf{Cross-Modal Misalignment (C-M)} & Disruptions in the alignment between textual and visual inputs. Examples include mismatched image captions, misleading textual descriptions, or contradictory multimodal content. \newline
\textbf{Mismatched Pairs:} Swap captions with unrelated images. \newline
\textbf{Contradictory Captions:} Reword captions to contradict image content.\\ \hline 
\textbf{Both-Modality Perturbations (B-P)} & Combined perturbations where both text and image distortions are applied simultaneously. This simulates real-world misinformation scenarios where misleading text and visuals coexist. \\ 
\hline 
\end{tabular}
\caption{Perturbation Types}
\label{tab:perturbations_desc}
\end{table*}

We now include examples of text perturbations (Figure 6 in the paper, referenced as Figure \ref{fig:perturbation_text} here) and multimodal perturbations (Figure 7, referenced as Figure \ref{fig:perturbation_image} herein). Additionally, we provide a detailed breakdown of each perturbation type in Appendix A9, with further clarification in Table 11 and referenced as Table (\ref{tab:perturbations_desc}) herein.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/perturbations/perturbation_text.pdf}
  \caption{Text perturbations: Synonym, Misspelling, and Negation. Our analysis shows that text negation leads to a majority of disinformation cases.}
  \label{fig:perturbation_text}
  
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/perturbations/perturbation_image.pdf}
   \caption{Image perturbations: Blur, Noise, Resizing, Cross-Modal (C-M) Mismatch, and Both-Modality (B-M). Our analysis shows that C-M and B-M lead to a majority of disinformation cases.}

    \label{fig:perturbation_image}
\end{figure}

We also conducted an additional analysis of perturbation types and their impact on model performance. Our findings indicate that \textbf{Cross-Modal (C-M)} perturbations (e.g., mismatched image captions) and \textbf{Both-Modality Perturbations (B-P)} (simultaneous text and image distortions) lead to the highest misclassification rates. In contrast, \textbf{Text Perturbations (T-P)} such as misspellings and synonyms are more resilient, whereas \textit{negation} significantly increases misclassification. This analysis is now detailed in Section 4.3 [line 399 - 447] and some in Appendix A9. \\ \\



\textcolor{blue}{\textbf{Response}}  
In response to the recommendation (\textcolor{purple}{(2)}), we have extended our analysis to include a broader range of Large Language Models (LLMs) and Vision Language Models (VLMs). Specifically, our expanded dataset now comprises results from the following models:

\begin{itemize}
    \item \textbf{LLMs:} DeepSeek-V2-Lite-Chat, GLM-4-9B-chat, InternLM2-7B
    \item \textbf{VLMs:} DeepSeek-VL2-Small, DeepSeek Janus-Pro-7B, InternVL2-8B, InternVL2-26B, and Llama 3.2-90B Vision
\end{itemize}

The inclusion of these models allows us to provide a more comprehensive analysis of the scaling effects on performance. Our findings, are given in Table 3 (line 350). The scale of larger LLMs is summarized in Table 11 (Appendix, given as Table \ref{tab:scale} below).  Overall, these results demonstrate that our dataset is a strong resource for detecting disinformation, particularly when utilized with a greater number of LLMs or VLMs. Additionally, the findings affirm that increasing model size enhances the ability to process and understand complex visual contexts, resulting in better accuracy.

\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Accuracy (\%)} & \textbf{Precision (\%)} & \textbf{Recall (\%)} & \textbf{F1-Score (\%)} & \textbf{Performance Increase (\%)} \\ \hline
Llama-3.2-11B-Vision & 74.82 & 72.62 & 72.28 & 72.45 & - \\ \hline
Llama-3.2-90B-Vision & 76.8  & 78.43 & 78.06 & 78.24 & 3.42\% (from 11B) \\ \hline
InternVL2-8B & 63.57 & 68.34 & 65.1  & 66.68 & - \\ \hline
InternVL2-26B & 68.62 & 73.8  & 70.31 & 72.01 & 8.05\% (from 8B) \\ \hline
\end{tabular}
}
\caption{Performance metrics of vision models scaled up with increased weights and parameters, indicating performance improvements.}
\label{tab:scale}
\end{table*}


\textcolor{purple}{3. It would be beneficial if you could include a section that summarizes and clarifies the methodology pipeline.}

\textcolor{blue}{\textbf{Response}} We have added a full paragraph explaining the methodology in Section 3 [line 211 - 225]. Additionally, we expanded Figure 2 with a more detailed caption for clarity. We also illustrate our methodology with working examples, such as Figure 3, which demonstrates the response of the annotation pipeline. We have also added details in the writing in this revision for better clarity.

\par Following the receipt of responses from ACL reviewers, we resumed the human review of the remaining two-thirds of the data using the same pool of reviewers and implemented revisions where feasible.

  \centerline{\rule{12cm}{0.4pt}}
    \bigskip

\textcolor{magenta}{\textbf{Reviewer KxM3}}  \\
We thank \textcolor{magenta}{\textbf{Reviewer KxM3}} for the constructive comments and for finding this dataset useful. We have incorporated the Reviewer’s suggestions in the revised manuscript. We have also proofread the paper multiple times during revision and corrected any typographical errors.
% \textbf{Summary Of Strengths:}

% \begin{itemize}
%     \item The paper addresses a relevant topic
%     \item Authors measured the inter-annotator agreement
%     \item The proposed dataset collects recent data, that is considered compliant to recent regulations
% \end{itemize}
% \textbf{Summary Of Weaknesses:}Main concerns regard the experimentations, that in general do not offer particular insights.

% \begin{itemize}
%     \item impact of finetuning shows that it generally yields some gains (0-5\% absolute points) in performance. Confidence intervals would have been important to better quantify its benefits. What is the main reason for such improvements? Is it solely the compliance to the required output labels or not?
%     \item evaluating model resilience. Some more details about textual perturbation should be included. From what it is described, synonyms, misspellings and negation are added. An error analysis of which kind of perturbation affects the most the prediction would have offered better insights. Indeed, negation can actually completely change the meaning, thus swapping the expected classification outcome.
%     \item What about replacing a misinformative text with an informative one from another example? Or viceversa, replacing the image with another one. That would give better hints to what affects models decisions.
%     \item In table 4, there is no Adv. img only ablation experiment.
% \end{itemize}
% \textbf{Comments Suggestions And Typos:}
% The abstract states that accuracy increases by 508\% from unimodal to multimodal models. This looks like a typo.
 
\textcolor{magenta}{(1) Experiments and insights.} \\
\textcolor{blue}{\textbf{Response}} We have revised our work based on the reviews and made significant improvements to the experimental section. We evaluated more baselines (Table 3 and Table 11) and provided deeper insights, and highlighted key takeaways. For example, we mentioned that VLMs (which process both modalities) generally perform better. For example, our results show 5-35\% improvement when we use multi modal for evaluation , compared to text only LLM [Table 3]. Additionally, we analyzed model resilience to perturbations [Table 4, Figure 6, Figure 7, Table 5] and found that distortions in both modalities negatively impact results. Finally, we detailed key findings after each table/figure in the results section.  

\textcolor{magenta}{(1) Reason for improvement in results with instruction fine tuing} \\
\textcolor{blue}{\textbf{Response}} \textcolor{blue}{\textbf{Response}} Our results show that the 1-5\% performance boost from instruction fine-tuning (IFT) comes from actual learning improvements, not just formatting labels [Figure 5] . IFT helps models capture subtle cues, like deceptive wording in disinformation, that standard models miss. Essentially, IFT transfers knowledge from larger models (our GPT-4o annotations) to smaller ones (1B-11B) or other LLMs/VLMs in our results (Table 3). We also examined output differences to understand these capability shifts.

\textcolor{magenta}{(1) Models Resilience and additions in the revision} \\
\textcolor{blue}{\textbf{Response}}
We have added a detailed subsection on model resilience in the revised manuscript.
We have also expanded the discussion on textual attacks and categorized perturbations into four types: (1) Text Perturbations (T-P), (2) Image Perturbations (I-P), (3) Cross-Modal Misalignment (C-M), and (4) Both-Modality Perturbations (B-P) [line 399 - 415 in main paper, and 1338 -1374 and Table 11 in Appendix]

We now provide examples of text perturbations (Figure 6, as \ref{fig:perturbation_text} below) and multimodal perturbations (Figure 7, as \ref{fig:perturbation_image} below). Additionally, we include a detailed breakdown in Appendix A9 and Table 11 (\ref{tab:perturbations_desc}) for further clarity.
Our results indicate that negation in T-P caused the most performance drop, while C-M and B-P (both modalities perturbed) significantly reduced accuracy. These findings are now detailed in Section 4.3 and Appendix A9 (Table 13, 14 and working examples). We hope these qualitative and quantitative analyses sufficiently address the concerns.

As suggested by Reviewer, yes we have shown the cross modality pertubations (C-M) and found it has resulted in lot performance drop as shown in Table 4 ( Table \ref{tab:perturbations1} herein). We have also included image ablation in this table as suggested by the Reviewer.

\begin{table}[ht]
\footnotesize
\centering
\renewcommand{\arraystretch}{1.0}
\setlength{\tabcolsep}{2pt} 
\resizebox{0.7\columnwidth}{!}{ 
\begin{tabular}{p{3.4cm} p{1.2cm} p{3.5cm} p{1.3cm}} 
\hline
\rowcolor{gray!15} 
\textbf{Model} & \textbf{Orig F1 (\%)} & \textbf{F1 Drop} \newline(\textbf{T-P, I-P, C-M, B-P}) & \textbf{Avg Drop(\%)} \\ 
\hline
\textbf{LLMs} & & & \\ \hline
Llama-3.2-1B & 69.35 & \textbf{-3.2}, -2.6, -6.3, \cellcolor{red!35}\textbf{-11.2} & \cellcolor{red!25}5.83 \\ 
Mistral-7B & 66.56 & \textbf{-3.5}, -2.5, -6.2, \cellcolor{red!34}\textbf{-11.0} & \cellcolor{red!24}5.80 \\ 
Internlm2-7b & 66.68 & -2.7, -3.1, \textbf{-6.8}, \cellcolor{red!40}\textbf{-11.8} & \cellcolor{red!30}6.10 \\ 
Vicuna-7B & 54.55 & \textbf{-4.1}, -2.0, -6.0, \cellcolor{red!30}\textbf{-10.3} & \cellcolor{red!22}5.60 \\ 
Qwen2-7B & 68.72 & \textbf{-3.0}, -2.2, -5.9, \cellcolor{red!28}\textbf{-10.1} & \cellcolor{red!20}5.30 \\ 
Phi-3-mini-128k & 55.71 & \textbf{-3.2}, -1.8, -5.5, \cellcolor{orange!25}\textbf{-9.4} & \cellcolor{orange!18}4.98 \\ 
DeepSeek-V2-Lite & 51.96 & -2.7, -3.2, \textbf{-6.3}, \cellcolor{red!28}\textbf{-10.1} & \cellcolor{red!20}5.58 \\ 
\hline
\textbf{VLMs} & & & \\ 
\hline
Llama-3.2-11B-Vision & 72.45 & -2.0, -2.8, \textbf{-5.6}, \cellcolor{red!28}\textbf{-10.2} & \cellcolor{red!20}5.15 \\ 
LLaVA-v1.6-mistral7B & 70.10 & -2.1, -2.9, \textbf{-5.8}, \cellcolor{red!32}\textbf{-10.7} & \cellcolor{red!24}5.38 \\ 
InternVL2-8B & 66.68 & -2.7, -3.1, \textbf{-6.8}, \cellcolor{red!40}\textbf{-11.8} & \cellcolor{red!30}6.10 \\ 
LLaVA-v1.5-vicuna7B & 70.10 & -2.8, -3.4, \textbf{-7.0}, \cellcolor{red!40}\textbf{-12.6} & \cellcolor{red!30}6.45 \\ 
Qwen2-VL-7B & 65.86 & -2.3, -3.0, \textbf{-6.4}, \cellcolor{red!35}\textbf{-11.5} & \cellcolor{red!25}5.80 \\ 
Phi-3-vision-128k & 63.03 & -2.5, -3.2, \textbf{-6.1}, \cellcolor{red!35}\textbf{-11.2} & \cellcolor{red!25}5.75 \\ 
Pixtral & 71.26 & -1.9, -2.7, \textbf{-5.4}, \cellcolor{orange!30}\textbf{-9.8} & \cellcolor{orange!20}4.95 \\ 
Deepseek-VL2-small & 69.84 & -2.2, -3.0, -6.5, \cellcolor{red!32}\textbf{-10.7} & \cellcolor{red!24}5.60 \\ 
\hline
\end{tabular}
}
\caption{Robustness to perturbations: T-P (text), I-P (image), C-M (cross-modal), B-P (combined). 
\textbf{Bold} = weakest performance per category. Red/orange gradients indicate severity (darker = larger drop). 
LLMs show higher text vulnerability (T-P), while VLMs struggle with cross-modal and combined attacks.}
\label{tab:perturbations1}
\end{table}
\begin{table}[h!]
\centering
\resizebox{0.7\textwidth}{!}{
\begin{tabular}{@{}llr@{}}
\toprule
\textbf{Model/Condition} & \textbf{Scenario} & \textbf{Acc. ($\Delta$\%)} \\ 
\midrule
LLama3.2-1B-Original & No adversaries & \cellcolor{green!30} 75.90 (-) \\  
LLama3.2-1B-Text & Synonym/misspell/negate & \cellcolor{yellow!40} 60.85 ($\Delta$15.05) \\  
LLama3.2-11B-Combined & Text+image attacks\textsuperscript{§} & \cellcolor{red!40} 53.54 ($\Delta$22.36) \\  
\bottomrule
\end{tabular}
}
\caption{Adversarial Performance Drops ($\Delta$ = Accuracy Decrease).  
\textsuperscript{§} Combined text+img. attacks.}
\label{tab:adversarial_performance}
\end{table}
\vspace{1em}


  \centerline{\rule{12cm}{0.4pt}}
    \bigskip


\textcolor{violet}{\textbf{Reviewer pDGe}}

\textcolor{blue}{\textbf{Response}} We thank the reviewer for the constructive feedback and for recognizing the importance of our work on disinformation. We have carefully addressed the reviewer’s comments, performed additional experiments, and improved typos and logical clarity throughout the manuscript.

%  \textbf{Summary Of Strengths:}Strengths:
%     \item The authors tackle the important problem of disinformation (which is deliberately sharing false or manipulated information).
%     \item The authors are very cognizant of general AI related safety policies (e.g AIRR-2024)
%     \item The authors state (lines 124-125) that this benchmark would be community driven and can be iteratively updated.

% \textbf{Summary Of Weaknesses:}Weanesses:
% The 33k sampled dataset is majorly annotated by GPT-4o. Although the authors have stated that these annotations have high corelation with human annotations, the fact that the whole dataset is basically annotated by an LLM results in a few limits. For example, it wouldn't be probably unfair to conclude that:

% a. GPT4-o itself would perform exceedingly well (close to 100\%) on this dataset. This is actually true for similarly large LLMs as well.
%     \item Continuing on the weakness 1 above, does it mean that the dataset is essentially solved by the larger LLMs. The fact that results in Table3 contain small sized LLMs also indicate that. It would be great to see results from bigger/more advanced LLMs too.
% \end{enumerate}
% \textbf{Comments Suggestions And Typos:}Questions to the authors:

% \begin{enumerate}
%     \item Section 3.3, lines 342-347. Did the human annotators review all of the 33k samples in the dataset? Or was it spot-checking?
%     \item What's the difference between "human annotations" and "ground truths"?
%     \item What's the distribution of the two classes in your dataset? It seems like an important stat, so you may want to include it in the main text (perhaps in Table 2)?
%     \item Figure 4, the grouping depend on the number of initial seeds chosen for k-means. What was that number?
%     \item How do you plan on keeping the dataset up-to-date (lines 124-126)?
%     \item A larger question: how can one quantify the "difficulty" of disinformation? Going by Section 3.3, humans have an F1 of 0.91. And LLMs also achieved an F1 of 0.89. Does this imply that the proposed dataset is "easy" or "almost already solved"? In general, wouldn't checking for disinformation require large amounts of manual research and fact-checking, etc.?
% \end{enumerate}
% Typo: Lines 409 "Figure 2" ->? "Table 2"


 
\textcolor{violet}{(1) Results from more/larger models} \\
\textcolor{blue}{\textbf{Response}}
We appreciate your comments and understand your concerns regarding the perceived value of our dataset in evaluating the capability of LLMs. However, the inclusion of a variety of models, both LLMs and VLMs, is crucial for a comprehensive assessment of the dataset's challenges and the models' abilities to address them.

In our revised analysis, we included results from multiple models, including smaller-scale models like DeepSeek-V2-Lite-Chat and GLM-4-9B-chat,  InternLM2-7B, as well as larger models such as and various scales of VLMs like Llama 3.2-90B Vision and InternVL2-26B. The results demonstrate scaling effects on performance, which are essential to understanding both the capabilities and limitations of current LLMs and VLMs in recognizing and countering disinformation.

Our findings, summarized in the revised Table 3 and Table 11 (appendix), show distinct performance variations among the models:
Larger models generally exhibit improved accuracy, precision, recall, and F1-scores, underscoring their enhanced ability to process and analyze complex datasets.
The performance increase, particularly noted in models like Llama 3.2-90B Vision and InternVL2-26B, highlights significant improvements as model size and complexity increase. Despite their computational intensity, larger models contribute to better understanding of disinformation, which can be critical in practical applications where high accuracy is paramount.
Thus, our dataset provides essential insights into the scalability and efficiency of various models to handle complex, real-world misinformation challenges.

% We want to clarify that while the dataset was initially annotated using GPT-4o (with three API calls to control randomness and resolve ties), it has been human-verified, making it not entirely AI-generated. 
% Secondly, the value of our work lies in providing a free, high-quality dataset to the community. Specifically, 22 domain experts spent 300+ hours curating the annotations, achieving high label accuracy (Cohen’s $\kappa = 0.78$). 
% To further validate the dataset, and agreeing to Reviewer suggestion, we have evaluated it on more models, so now we across 9 LLMs and 10 VLMs of varying sizes. This demonstrates its robustness, generalizability, and usefulness for benchmarking models in disinformation detection. These revisions demonstrate dataset’s value and its broader applicability. \\

\textcolor{violet}{2. Did the human annotators review all of the 33k samples in the dataset? Or was it spot-checking?}\\
 \textcolor{blue}{\textbf{Response}} Initially, 1/3 of the dataset was human-annotated. However, based on reviewer feedback, we made rigorous efforts to review the entire dataset. Now, all 33K samples have been fully reviewed, with annotators examining both text and images and agreeing on the labels. Any discrepancies were resolved to ensure consistency. With these changes, we made changes in our experiments (e.g., re-running certain baselines and repeating experiments where applies). \\


\textcolor{violet}{3  How can one quantify the ``difficulty" of disinformation, wouldn't checking for disinformation require large amounts of manual research and fact-checking, etc.}\\
 \textcolor{blue}{\textbf{Response}}  Quantifying the difficulty of disinformation can be approached by breaking down the verification process into measurable stages.
 
(1) Data Curation: We gather real-world news items from multiple sources and compare them against known fact-checking databases. The difficulty here can be measured by the number of cross-references needed or the level of domain expertise required.

(2) Annotation: Human reviewers label content with potential disinformation. We track annotator agreement rates and the frequency/length of external research needed—both good indicators of difficulty.

(3) Human Review \& Post-Evaluation: We analyze how often automated methods fail to detect or accurately classify disinformation. Higher error rates may suggest higher difficulty. Additionally, we measure reviewer time per claim, complexity of source materials, and the extent of manual cross-verification.


 
 % We have assessed LLMs and VLMs based on their ability to identify disinformation in our dataset. It is important to clarify that disinformation involves intent rather than just factual inaccuracies. In our corpus, as shown in Table 7, all news sources are from reputable outlets that typically ensure fact-checking due to their credibility. However, the challenge lies in detecting the intent and how information is presented, which can subtly influence the perception of the truth. We have now explicitly stated in Table 1 and the Introduction that our focus is on finding [specific focus, e.g., disinformation patterns, multimodal inconsistencies, etc.] to ensure clarity.



  \centerline{\rule{12cm}{0.4pt}}
    \bigskip


\textcolor{teal}{\textbf{Reviewer 8jd5}}

\textcolor{blue}{\textbf{Response}} We sincerely thank the reviewer for their thoughtful feedback and for acknowledging our detailed responses. We appreciate that the reviewer found our dataset useful and recognized our efforts in addressing the raised concerns. As suggested, we have incorporated the explanations into the revised manuscript to ensure greater clarity.

\textcolor{teal}{Clarification on methodology} \\
 \textcolor{blue}{\textbf{Response}} We have added a full paragraph explaining the methodology in Section 3 [line 211-225] and expanded Figure 3 with a more detailed caption for clarity. Additionally, we have improved the writing for better readability.
To summarize: We curated real-world news from 58 sources over one year.
We studied disinformation tactics observed in real news and annotated data accordingly. The dataset was initially labeled by GPT-4o using three API calls for consistency and randomness reduction, but now fully human-verified. We expanded evaluation to include 9 LLMs and 10 VLMs.
Our work aligns with AI risk mitigation frameworks, including NIST guidelines, AI Acts in the EU and Canada, and broader global standards. This is the first benchmark that explicitly addresses the subjectivity of disinformation detection. We evaluated also for resiliance of the models on perturbations and highlights the key findings.


\textcolor{teal}{Addressing Data Integrity, Annotation Process, and Benchmark Findings}\\
 \textcolor{blue}{\textbf{Response}}
We ensured data integrity by removing duplicate samples during collection. For annotation, GPT-4o generated three annotations per sample to reduce variance, with human reviewers validating the final labels. While the paper initially mentioned  10,000 (30\% of the dataset), we have now expanded it to all data with rigorous efforts. To improve benchmark findings, we added a summary aligning with each section. Regarding disinformation categories, these were used in prompts and are detailed in Appendix A1 and prompts in Appendix A11, in supplementary material, covering 12 categories such as Politics, Business, Health, and Technology. We also conducted EDA to analyze disinformation prevalence across these categories.


  \centerline{\rule{12cm}{0.4pt}}
    \bigskip

In this revision, we have included new authors who repeated the experiment with us, enhanced the human review process to improve overall data reliability, and accelerated the revision process in terms of both experiments and writing. Due to the double-blind review policy, their names are not mentioned here but are included in ARR.

\end{document}
