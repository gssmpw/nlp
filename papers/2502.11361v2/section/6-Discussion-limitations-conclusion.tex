\section{Limitations}
\label{limits}
While \textsf{\textbf{\textsc{VLDBench}}} advances multimodal disinformation research, it shares common limitations of empirical studies, highlighting opportunities for broader collaboration and improvement.

First, reliance on pre-verified news sources introduces sampling bias. This approach may underrepresent disinformation tactics prevalent on the fringe or less-regulated platforms (e.g., social media), limiting adaptability to evolving misinformation strategies. Future work should incorporate data from decentralized platforms to improve generalizability.

Second, AI-assisted annotations (via LLMs/VLMs) risk inheriting model biases \cite{gilardi2023chatgpt} and stochasticity \cite{bender2021dangers}. While we mitigated these through hyperparameter tuning, multi-pass labeling (with majority voting), and human validation, the probabilistic nature of LLMs raises fairness concerns in high-stakes scenarios. Misclassifications could disproportionately harm marginalized groups through discrimination or biased outcomes. Persistent gaps—such as propagated training-data biases or annotator subjectivity—warrant future adversarial testing and bias-detection frameworks.

Third, model performance degrades under adversarial attacks manipulating text and visuals (e.g., subtly altered images or misleading captions). This underscores the need for robust defense strategies and frequent model updates to counter emerging disinformation tactics.

Fourth, our English-language focus limits relevance in multilingual and culturally diverse contexts. Expanding to non-English content and region-specific disinformation patterns is critical for global applicability.

Fifth, we only evaluate the baseline models and perform instruction fine-tuning to do a fair analysis. We do not test approaches that leverage auxiliary steps such as self-feedback mechanisms, critique blocks, and visual programming approaches \cite{mahmood2024vurf}.

Finally, the computational demands of training multimodal models create barriers for resource-constrained researchers, risking centralization of detection capabilities among large entities. Democratizing access through lightweight models or federated learning could improve inclusivity.

Despite these limitations, \textsf{\textbf{\textsc{VLDBench}}} provides the first dedicated foundation for benchmarking multimodal disinformation detection, enabling future work to refine methodologies, expand coverage, and address ethical risks.

\section*{Social Impact Statement}
\label{sec:social-impact }

Disinformation threatens democratic institutions, public trust, and social cohesion, with generative AI exacerbating the problem by enabling sophisticated multimodal disinformation campaigns. These campaigns exploit cultural, political, and linguistic nuances, demanding solutions that transcend purely technical approaches.

\textsf{\textbf{\textsc{VLDBench}}} addresses this challenge by establishing the first multimodal benchmark for disinformation detection, combining text and image analysis with ethical safeguards. To ensure cultural sensitivity, annotations account for regional and contextual variations, while bias audits, and human-AI hybrid validation mitigate stereotyping risks. Ground-truth labels are derived from fact-checked sources with provenance tracking, aligning the benchmark with transparency and accountability principles.

The societal value of \textsf{\textbf{\textsc{VLDBench}}} lies in its dual role as a technical resource and a catalyst for interdisciplinary collaboration. By open-sourcing the benchmark and models, we lower barriers for researchers and practitioners in resource-constrained settings, democratizing access to state-of-the-art detection tools. Quantitative performance gaps—such as vulnerability to adversarial attacks—highlight systemic risks in current systems, incentivizing safer and more reliable alternatives. Crucially, the benchmark is designed to foster partnerships across academia, industry, journalists, and policymakers, bridging the gap between computational research and real-world impact.

Ethical risks are carefully addressed. While malicious actors could misuse detection models to refine disinformation tactics, we restrict dataset access (request-only) and exclude synthetic generation tools. Overreliance on automated judgments is mitigated through human oversight requirements (Section~\ref{limits}), and representation gaps in non-English content are explicitly documented to guide culturally adaptive extensions.

\textbf{Responsible use} is foundational to \textsf{\textbf{\textsc{VLDBench}}}’s design. The dataset is curated ethically without personal data and focuses exclusively on disinformation detection—never propagation. While the dataset may contain offensive content inherent to disinformation examples, we rigorously document known risks (e.g., stereotyping, inflammatory language) in datasheets (Appendix~\ref{app:datasheet }), and users must adhere to binding agreements prohibiting harmful applications such as censorship, surveillance, or targeted disinformation campaigns. To ensure accountability, we provide full transparency into annotation protocols, model training procedures, and evaluation criteria, enabling reproducibility and critical scrutiny of results.

We envision \textsf{\textbf{\textsc{VLDBench}}} as a foundation for media literacy initiatives, fact-checking and unbiased workflows, and policy discussions on AI governance. By prioritizing ethical design and equitable access, our work aims to empower communities and institutions to combat disinformation while fostering trust in digital ecosystems.