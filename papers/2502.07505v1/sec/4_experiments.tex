\section{Experiments}
\label{sec:experiments}


We conduct experiments on object classification, and semantic segmentation to validate our methods. Due to space constraints, additional experiments, ablation studies, detailed dataset description and implementation are provided in the supplementary materials.

\subsection{Baselines.}
In our main experiments, we compare our convolution operation, \method{Ours}, to the same model where the integral is solved using \ac{MC}~\cite{finzi2020generalizing}, \method{MC}, and a model using standard convolutions, \method{STD}.
Moreover, we also compare to additional rotation equivariant networks, relying on global and local equivariant designs.

\subsection{Shape classification}
We use the task of shape classification to measure the equivariant capabilities of the models \wrt global rotations.
For this task, predictions must be invariant of the rotation applied to the model.
We use a global pooling operation as the projection layer (~\cref{eq:proj-layer}) at the end of our encoder to transform the equivariant features into invariant ones.

\textbf{Dataset.}
We use the ModelNet40 dataset~\cite{wu2015modelnet} since this is a standard benchmark for rotation equivariant networks~\cite{deng2021vector}.
Our model only takes as input point coordinates, and performance is measured with overall accuracy.

\textbf{Experimental setup.}
We provide different configuration setups in our experiments.
All models are evaluated when trained and tested without any rotation, I / I.
Further, we evaluate all models trained without any rotation but random rotations during testing, I / SO(3).
Lastly, we evaluate our models with random rotations during training and testing.
Additionally, to compare to other state-of-the-art methods, we take the commonly used setup where random rotations are applied along the up vector during training and random rotations on SO(3) during testing, z / SO(3).
Although this setup is less challenging than I / SO(3), it allows us to compare to additional rotation equivariant models.

\textbf{Main results.}
In our main results, we compare our method, \method{Ours}, to \method{MC} and \method{STD} for different samples taken during training and testing.

\begin{table*}%[!h]
\caption{Results for different configurations for the classification task on the ModelNet40 dataset. 
The results show that using our sampling approach increases the performance significantly, leading to better results with fewer samples.}
\label{tbl:main-results-modelnet}
\setlength{\tabcolsep}{12pt}
\centering
\small
\begin{tabular}{lcrrrrrrrrr}
    \toprule
    \multicolumn{1}{c}{Method} & \multicolumn{1}{c}{\# samp.} & \multicolumn{3}{c}{I / I}  & \multicolumn{3}{c}{I / SO(3)} & \multicolumn{3}{c}{SO(3) / SO(3)}  \\
    \cmidrule(l{2pt}r{2pt}){3-5}\cmidrule(l{2pt}r{2pt}){6-8}\cmidrule(l{2pt}r{2pt}){9-11}
    & train $\downarrow$/ test $\rightarrow$ & 1 & 2 & 4 & 1 & 2 & 4 & 1 & 2 & 4 \\
    \midrule
    \multirow{3}{*}{\method{MC}}
    & 1 & \cbg 85.4 & \cbg 84.6 & \cbg 83.1 & 78.8 & 74.1 & 70.1 & \cbg 86.5 & \cbg 85.6 & \cbg 84.4  \\
    & 2 & \cbg 86.2 & \cbg 87.0 & \cbg 87.1 & 80.3 & 82.3 & 82.3  & \cbg 87.1 & \cbg 87.0 & \cbg 87.0 \\
    & 4 & \cbg 84.2 & \cbg 87.4 & \cbg 87.5 & 78.4 & 85.6 & 86.2  & \cbg 85.4 & \cbg 88.3 & \cbg 88.2 \\
    \cmidrule{2-11}
    \multirow{3}{*}{\method{Ours}}
    & 1 & \cbg 86.9 & \cbg 86.8 & \cbg 86.7 & 85.5 & 85.3  & 85.3 & \cbg 88.7 & \cbg 88.5 & \cbg 88.5 \\
    & 2 & \cbg 87.9 & \cbg 87.9 & \cbg 87.7 & 86.6 & \textbf{86.9} & 86.8 & \cbg 88.9 & \cbg 88.7 & \cbg 88.7\\
    & 4 & \cbg 73.2 & \cbg 87.6 & \cbg 87.8 & 61.4 & 85.7 & 86.5 & \cbg 59.7 & \cbg \textbf{89.0} & \cbg 88.7 \\
    \midrule
    \method{STD} & & \cbg & \cbg \textbf{90.7} &\cbg & \multicolumn{3}{c}{12.3} & \cbg & \cbg 87.5 & \cbg \\
    \bottomrule
\end{tabular}
\end{table*}

\Cref{tbl:main-results-modelnet} presents the results of this experiment.
As expected, we can see that the standard method \method{STD} achieves good accuracy for I / I.
\method{Ours} and \method{MC}, as it is typical for rotation equivariant networks in this setup, achieve competitive performance but are below \method{STD}.
However, when we look at the more challenging setup, I / SO(3), we can see that \method{Ours} is able to maintain similar accuracy as in the I / I setup, $86.9\,\%$, a drop by only one point in accuracy, while \method{STD} achieves $12.3\,\%$.
\method{MC}, although it can also achieve competitive performance, for most of the cases, the drop in performance is significant compared to the I / I results.
When we look at the SO(3) / SO(3) setup, all three methods achieve good performance; \method{MC} and \method{Ours} are able to outperform \method{STD}, while \method{Ours} achieves the best accuracy. 

Analyzing the effect of different samples used to compute the integral over SO(3) for training and testing, we can see that \method{Ours}, even with 1 sample, can achieve similar results than when using 4 samples.
With only 2 samples, our method is able to match or even surpass the accuracy of using the full frame, 4 samples.
Moreover, using only 1 or 2 samples appears to be more robust than using the full frame, 4 samples, when tested with different numbers of samples. We hypothesize that training with random 1 or 2 samples, rather than using the full frame, introduces stochasticity that acts as a regularizer, enhancing robustness to errors in SO(3) integral estimation. In contrast, \method{MC} is more sensitive to the number of samples, exhibiting significant performance degradation with 1 or 2 samples.





\textbf{Comparison to other methods.}
First, we compare our model to existing non-equivariant point-based network architectures, architectures that rely on global equivariance, and models like ours that use group convolutions to achieve local equivariance.
In ~\cref{tbl:modelnet_sota}, we can see that our model achieves the best performance among the group convolution tested by a large margin in the I/SO(3) setting.
This is due to the discretization of the group SO(3) used by the \method{EPN}~\cite{chen2021equivariant} and \method{E2PN} methods~\cite{zhu2023e2pn}. Also, in the z / SO(3) and SO(3) / SO(3) settings, we outperform all local rotation equivariant networks.
When compared to global equivariant networks, our method falls behind in the I / SO(3) setup and achieves similar performance on the z / SO(3) and SO(3) / SO(3) setup.
However, as we will show later in the segmentation task, while some global equivariant networks only slightly outperform ours on this task, they fail to solve tasks requiring local rotation equivariance. % while ours is robust to these changes.

\begin{table}[htbp]
\caption{Comparison to equivariant models on the classification task of ModelNet40 for different setups.}
\label{tbl:modelnet_sota}
\setlength{\tabcolsep}{3pt}
\centering
\small
\begin{tabular}{llccc}
    \toprule
    Equiv. & \multicolumn{1}{c}{Method} & \multicolumn{1}{c}{I / SO(3)} & \multicolumn{1}{c}{z / SO(3)} & \multicolumn{1}{c}{SO(3) / SO(3)}\\

    \midrule

    \multirow{5}{*}{\rotatebox[origin=c]{90}{None}}
    & \method{PointNet}~\cite{qi2017pointnet} & -- & \cbg 19.6 & \textbf{84.9} \\
    & \method{PointNet++}~\cite{qi2017plusplus} & 13.8 & \cbg 28.4 & \textbf{84.9} \\
    & \method{DGCNN}~\cite{dgcnn} &  \textbf{17.3} & \cbg 33.8 & 84.8 \\
    & \method{PointCNN}~\cite{YangyanPCNN} & -- & \cbg \textbf{41.2} & 84.8 \\   
    & \method{KPConv}~\cite{thomas2019KPConv} & 12.7 & \cbg  -- & 81.2 \\
    \midrule
    \multirow{5}{*}{\rotatebox[origin=c]{90}{Global}}
    & \method{GC-Conv}~\cite{zhang2020gc} & -- & \cbg 89.1 & 89.2 \\
    & \method{FA-PointNet}~\cite{puny2022frame} & 85.9 & \cbg 85.5  & 85.8 \\
    & \method{FA-DGCNN}~\cite{puny2022frame} & 88.4 & \cbg 88.9 & 88.5\\
    & \method{VN-PointNet}~\cite{deng2021vector} & 77.2 & \cbg 77.5 & 77.2 \\
    & \method{VN-DGCNN}~\cite{deng2021vector} & \textbf{90.0} & \cbg \textbf{89.5} &\textbf{90.2} \\

    \midrule
    \multirow{6}{*}{\rotatebox[origin=c]{90}{Local}}
    & \method{TFN}~\cite{thomas2018tensor} & -- & \cbg 85.3 & 87.6 \\
    & \method{ClusterNet}~\cite{chen2019clusternet} & -- & \cbg 86.4 & 86.4 \\
    & \method{RI-Conv}~\cite{zhang-riconv-3dv19} & -- & \cbg 86.4 & 86.4 \\
    & \method{SPHNet}~\cite{poulenard2019rotinv} & -- & \cbg 86.6 & 87.6 \\
    & \method{EPN}~\cite{chen2021equivariant} &  32.3 & \cbg-- &87.8 \\
    & \method{E2PN}~\cite{zhu2023e2pn} &  44.4 &\cbg -- & 88.6 \\

    \cmidrule{2-5}

    & \method{Ours} & \textbf{86.9} & \cbg \textbf{87.0} & \textbf{89.0} \\
  
    \bottomrule
\end{tabular}
\end{table}

\begin{figure*}[htbp]
    \centering
    \includegraphics*[width=\linewidth]{img/qualitative}
    \caption{\textbf{Qualitative results.} Global equivariant methods such as \method{VN}, or \method{FA} struggle with out-of-distribution models.
    Our method, on the other hand, achieves almost perfect predictions.
    Lastly, \method{MC} also achieves good performance but falls behind our method, which better approximates the group convolution integral.}
    \label{fig:qualitative}
\end{figure*}

\subsection{Semantic segmentation}
\label{sub-sec:segmentation}

In semantic segmentation, incorporating symmetries like SE(3) equivariance is key for generalization, especially due to the varying orientations and part compositions in point clouds. We evaluate our method on body part segmentation and scene understanding.

\subsubsection{Human body parts}
For semantic segmentation of human body parts, the local equivariance property is essential to distinguish correctly between parts undergoing diverse SE(3) transformations within the kinematic tree. 
Due to the additional symmetry information, we show that our models can generalize to unseen, out-of-distribution poses.

\textbf{Dataset.}
For training and testing, we use two subsets of the AMASS meta-dataset~\cite{mahmood2019amass}, DFAUST~\cite{bogo2017dynamic} and PosePrior~\cite{akhter2015pose}, respectively. 
The PosePrior dataset consists of challenging poses significantly divergent from those executed in DFAUST, which we use to test our model for generalization to unseen, out-of-distribution poses. 


\textbf{Experimental setup.}
To assess the ability of our method to generalize to local transformations, we adopt a setup in which we do not use any rotation during training or testing.
Since the testing data is composed of rare poses not seen during training, the models must become invariant to transformations of the different local parts.


\textbf{Main results.}
\Cref{tbl:main-results-dfaust} presents the results of the main experiment.
We can see that \method{STD} struggles to generalize to these out-of-distribution poses, achieving a mAcc of $85.3$ and mIoU of $74.5$.
\method{Ours}, on the other hand, achieves better performance with $95.0$ mAcc and $90.8$ mIoU.
\method{MC} can also achieve competitive performance, but, as in the classification task, this is lower than our proposed approach.




\begin{table}%[!h]
    \caption{Semantic segmentation results for different models trained on DFAUST and tested on PosePrior. By using our sampling approach, mAcc, and mIoU increase significantly with only a few samples of the frame.}

    \centering
    \small
    \setlength{\tabcolsep}{4pt}
    \label{tbl:main-results-dfaust}
    \begin{tabular}{lcrrrrrr}
        \toprule
        \multicolumn{1}{c}{Method} & \multicolumn{1}{c}{\# samp.} & \multicolumn{3}{c}{mAcc} & \multicolumn{3}{c}{mIoU} \\
        \cmidrule(l{2pt}r{2pt}){3-5}\cmidrule(l{2pt}r{2pt}){6-8}
        & {\small train $\downarrow$/ test $\rightarrow$} &  1 & 2 & 4 & 1 & 2 & 4\\
        \midrule
        \multirow{3}{*}{\method{MC}}
        & 1 & \cbg 93.1 & \cbg93.0 & \cbg92.7 &  87.7 &  87.6 &  87.2 \\
        & 2 & \cbg 93.8 &\cbg 93.9 & \cbg93.8 &  88.8 &  89.0 &  88.7\\
        & 4 & \cbg 93.4 &\cbg 94.2 & \cbg94.4 &  87.9 &  89.3 &  89.7\\
        \cmidrule{2-8}
        \multirow{3}{*}{\method{Ours}}
        & 1 & \cbg 93.8 &\cbg 93.9 & \cbg93.9 &  88.9 &  88.9 &  89.0\\
        & 2 & \cbg 94.3 &\cbg 94.4 & \cbg94.5 &  89.7 &  89.9 &  89.9\\
        & 4 & \cbg 32.6 & \cbg92.4 & \cbg\textbf{95.0} &  21.6 &  86.8 &  \textbf{90.8} \\
        \midrule
        \method{STD} &   &\cbg & \cbg85.3 & \cbg & \multicolumn{3}{c}{74.5} \\
        \bottomrule
    \end{tabular}
\end{table}




\begin{table}%[!h]
    \caption{Comparison of our method to other rotation equivariant models on the segmentation task for out-of-distribution poses.}
    \centering
    \small
    \setlength{\tabcolsep}{13pt}
    \label{tbl:faust_sota}
    \begin{tabular}{llcc}
        \toprule
        Equiv. & \multicolumn{1}{c}{Method} & \multicolumn{1}{c}{mAcc} & \multicolumn{1}{c}{mIoU}\\
        \midrule
        \multirow{4}{*}{\rotatebox[origin=c]{90}{Global}}
        & \method{FA-PointNet}~\cite{puny2022frame} & \cbg 77.4 & 64.7  \\
        & \method{FA-DGCNN}~\cite{puny2022frame} & \cbg 81.7 & 71.0  \\
        & \method{VN-PointNet}~\cite{deng2021vector} & \cbg  63.1 & 47.5  \\
        & \method{VN-DGCNN}~\cite{deng2021vector} & \cbg  61.1 & 46.6  \\
    
        \midrule
        \multirow{3}{*}{\rotatebox[origin=c]{90}{Local}}
         & \method{EPN}~\cite{chen2021equivariant} & \cbg 89.9 & 82.3\\
         & \method{E2PN}~\cite{zhu2023e2pn} & \cbg  94.8 & 90.7\\
         & \method{Ours} & \cbg  \textbf{95.0} & \textbf{90.8} \\
      
        \bottomrule
    \end{tabular}

\end{table}


When evaluating the model robustness to the number of samples in the SO(3) integral, \method{Ours} outperforms \method{MC} in all cases except when trained on 4 samples but tested on one, as seen in the classification task.

\textbf{Comparison to other methods.}
In Tbl.~\ref{tbl:faust_sota}, we present the results of comparing our method to other global and local equivariant point-based networks.
We can see that \method{Ours} achieves an impressive performance of $95.0$ mAcc and $90.8$ mIoU.
Contrary to the task of shape classification, global equivariant models struggle to generalize to out-of-distribution local transformations not seen during training.
Fig.~\ref{fig:qualitative} depicts predictions for different models tested on the dataset.
The results show that global equivariant methods such as \method{VN} or \method{FA} struggle with out-of-distribution models, confusing legs and arms and right and left. 
The same is true for our non-equivariant version, \method{STD}. 
The training data contain mostly upright positions, i.e., feet are, on average, further down on the z-axis. In contrast, the hands and the head are further up, leading to generalization errors in those models, e.g., the handstand pose as shown in Fig.~\ref{fig:qualitative}.
\method{Ours}, on the other hand, achieves predictions comparable to the ground truth annotations despite never seen those extreme poses during training.
\method{MC} also achieves remarkable performance but performs several prediction mistakes due to inefficient sampling of Frame elements as Tbl.~\ref{tbl:main-results-dfaust} indicates.

When comparing to current state-of-the-art local equivariant methods, we can see that while they also outperform global equivariant methods by a large margin, our method gives superior results, with \method{E2PN}~\cite{zhu2023e2pn} reaching a slightly lower performance. 


\begin{table}
\caption{Computational and memory resources of a single convolution layer for our approach and state-of-the-art methods.}
\label{tbl:memory}
\centering
\small
\setlength{\tabcolsep}{12pt}
\begin{tabular}{lcrr}
    \toprule
    \multicolumn{1}{c}{Method} & \# samp. & \multicolumn{1}{c}{Mem. (Mb) $\downarrow$} & \multicolumn{1}{c}{FPS $\uparrow$}\\
    \midrule
    \method{STD} & & 37.1 & 704.2 \\
    \cmidrule{2-4}
    \multirow{3}{*}{\method{Ours}} 
    & 1 &  37.1  & 581.4\\
    & 2 &  76.9  & 432.9\\
    & 4 &  165.2 & 255.8\\
    \cmidrule{2-4}
    \method{E2PN}~\cite{zhu2023e2pn} & & 1211.6 & 45.0 \\
    \method{EPN}~\cite{chen2021equivariant} & & 1636.4 & 10.2 \\
    \bottomrule
\end{tabular}
\end{table}

Tbl.~\ref{tbl:memory} compares a forward pass of a single convolution layer using 1024 points and $256$ input and output features.
We can see that using only one sample to approximate the integral over SO(3) has approximately similar memory consumption and frames per second (FPS) as the non-SO(3) equivariant version of our model. 
This shows that with our method, we can introduce the equivariant property without extra costs, demonstrating the efficiency of our proposed model.
When we analyze the two-sample version of our group convolution, we can see that memory and computation increase by a factor of 2, still making it suitable for its applicability.
When using 4 samples, the memory and computations increase significantly. 
Compared to other state-of-the-art local rotation equivariant methods, \method{E2PN}~\cite{zhu2023e2pn} and \method{EPN}~\cite{chen2021equivariant}, the computational resources needed for our approach are significantly lower even when using 4 samples.


    

\subsubsection{Scene understanding}

Scenes consist of multiple parts or objects with arbitrary orientations, making local equivariance essential for generalizing to unseen configurations.


\textbf{Dataset.}
We test our method on ScanNet~\cite{dai2017scannet}, a dataset composed of several indoor 3D scene scans, to show its applicability to real-world scenarios. 


\textbf{Experimental Setup.}
Since our surroundings have a notion of an up orientation, we fix the z-axis and conduct our experiments for SO(2).
We sample only one orientation from the frame for all experiments, which does not pose additional memory or computational burden on the model. This is a crucial property for processing such large point clouds, making it intractable for the other methods to run reasonable-sized networks for this task. 

\textbf{Main Results.}
Tbl.~\ref{tbl:scannet} shows that our method outperforms \method{STD} in all three configurations, underlining the benefits of baking SE(3) equivariance in the model architecture.
Compared to \method{MC}, we can see that our approach obtains better predictions in all but one configuration.


\begin{table}[t]
\caption{Results for the semantic segmentation task on ScanNet20 show that using our sampling approach increases the performance.}
\label{tbl:scannet}
\setlength{\tabcolsep}{5.5pt}
\centering
\small
\begin{tabular}{lcccccc}
    \toprule
    Method &  \multicolumn{2}{c}{I / I}  & \multicolumn{2}{c}{I / SO(2)} & \multicolumn{2}{c}{SO(2) / SO(2)}  \\

    \cmidrule(l{2pt}r{2pt}){2-3}\cmidrule(l{2pt}r{2pt}){4-5}\cmidrule(l{2pt}r{2pt}){6-7}
    & mAcc & mIoU & mAcc & mIoU & mAcc & mIoU \\
    \midrule
    \method{MC} & 73.4 & 64.5   & \cbg \textbf{74.1} & \cbg 65.2 & 74.2  & 65.7 \\
    \cmidrule(l{2pt}r{2pt}){2-7}
    \method{Ours} & \textbf{73.6} & \textbf{65.6} & \cbg 72.7 & \cbg \textbf{65.4} & \textbf{75.6}  & \textbf{67.5} \\
    \midrule
    \method{STD} & 73.0 & 64.4 & \cbg 70.9 & \cbg 63.5 & 74.5 & 66.4 \\
    \bottomrule
\end{tabular}
\end{table}


