\clearpage
\setcounter{page}{1}
\maketitlesupplementary
\appendix



\section{Proof of \Cref{theorem}}
$\Phi_{\mathcal{F}}$ is equivaraint to SE$(3)$ iff $\forall g \in \text{SE}(3):  ( \Phi_\mathcal{F} \circ \rho^{\mathcal{X}(g)})(f)= (\rho^{\mathcal{Y}}(g)\circ \Phi_\mathcal{F})(f)$. 
\begin{proof}
Using the definition 
\begin{equation}
    k_{\text{R}}(\text{x, R'}) \coloneq k(\text{R}^{-1}\text{x}, \text{R}^{-1} \text{R'}),
\end{equation}
omitting the normalization constant $\frac{1}{\lvert \mathcal{F}(x_j) \rvert}$ for brevity and using $g=\text{(t}_g, \text{ R}_g\text{)}$ we can write
\begin{equation}
\small
    \begin{split}
        &\big[( \Phi_\mathcal{F} \circ \rho^{\mathcal{X}(g)})(f)\big](\text{x, R})= \\
        &\sum_{j} \sum_{(\text{t, R'})\in \mathcal{F}(x_j)}  f(g^{-1}\text{(t, R')})k(\text{(x, R)}^{-1}\text{(t, R')})=\\
        &\sum_{j} \sum_{(\text{t, R'})\in \mathcal{F}(x_j)}  f(g^{-1}\text{(t, R')})k_{\text{R}}(\text{t} - \text{x},\text{R'})=\\
        &\sum_{j} \sum_{(\text{t, R'})\in \mathcal{F}(x_j)}  f(\text{R}_g^{-1}(\text{t}-\text{t}_g), \text{R}_g^{-1}\text{R'})k_{\text{R}}(\text{t} - \text{x},\text{R'})\overset{x_j \leftarrow \text{R}_g x_j + \text{t}_g}=\\
        &\sum_{j} \sum_{(\text{t, R'})\in \mathcal{F}(\text{R}_g x_j + \text{t}_g)}  f(\text{R}_g^{-1}(\text{t}-\text{t}_g), \text{R}_g^{-1}\text{R'})k_{\text{R}}(\text{t} - \text{x},\text{R'})\overset{\text{equiv. of }\mathcal{F}}=\\
        &\sum_{j} \sum_{(\text{t, R'})\in \text{R}_g\mathcal{F}( x_j )+ \text{t}_g}  f(\text{R}_g^{-1}(\text{t}-\text{t}_g), \text{R}_g^{-1}\text{R'})k_{\text{R}}(\text{t} - \text{x},\text{R'})=\\
        &\sum_{j} \sum_{(\text{R}_g\text{t} +\text{t}_g, \text{R}_g\text{R'})\in \mathcal{F}( x_j )}  f(\text{R}_g^{-1}(\text{t}-\text{t}_g), \text{R}_g^{-1}\text{R'})k_{\text{R}}(\text{t} - \text{x},\text{R'})=\\
        &\sum_{j} \sum_{(\text{t, R'})\in \mathcal{F}( x_j )}  f(\text{t, R'})k_{\text{R}}(\text{R}_g\text{t} +\text{t}_g - \text{x},\text{R}_g\text{R'})=\\
        &\sum_{j} \sum_{(\text{t, R'})\in \mathcal{F}( x_j )}  f(\text{t, R'})k(\text{(x, R)}^{-1} (\text{t}_g, \text{ R}_g)(\text{t}, \text{ R'}))=\\
        &\sum_{j} \sum_{(\text{t, R'})\in \mathcal{F}( x_j )}  f(\text{t, R'})k\big((\text{(t}_g, \text{ R}_g\text{)}^{-1}\text{(x, R)})^{-1} \text{(t, R')}\big)=\\
        &\big[(\Phi_\mathcal{F})(f)\big](g^{-1}\text{(x, R)})= \big[(\rho^{\mathcal{Y}}(g)\circ \Phi_\mathcal{F})(f)\big]\text{(x, R)} \\
    \end{split}
\end{equation}
\end{proof}


\section{Relation to concurrent work}
Previous works have also achieved (piecewise) equivariance by averaging over a frame~\cite{xiao2020pca,puny2022frame}.
While Xiao \etal~\cite{xiao2020pca} only explore this idea to obtain global rotation equivariance, Puny \etal~\cite{puny2022frame} propose a more general framework based on the same idea. The concurrent work of Atzmon~\etal~\cite{atzmonapproximately} uses this idea to achieve piecewise (local) equivariance by applying frame averaging for partitions of individually transforming regions. In these works, the same frame is used for each point in the point cloud (global) or partitions (piecewise/local) for the symmetrization of, e.g., a neural network by transforming their (partitions') domain. 
For Atzmon~\etal~\cite{atzmonapproximately}, a partition prediction network is necessary since simply increasing the number of partitions to reduce partition errors limits the expressivity of the resulting equivariant point network; when each point belongs to one partition, the only shared equivariant function is constant.
In contrast, this work is based on group convolutions, where point convolutions in a neural network are lifted to the SE(3) group so the kernel can detect rotated patterns. In this context, the concept of equivariant frames is used to define a point-specific grid on SE(3) to solve the convolution integral over the SE(3) group efficiently. Since we create point-specific frames, we avoid the need to group points into regions that can rotate jointly. The locality of the features created by the proposed convolution operator is determined by the point neighborhoods used for feature aggregation. Stacking several layers of such convolution operators results in a network capable of detecting local equivariant features up to features equivariant to the accumulated receptive field among the layers. Using efficient SE(3) equivariant convolutions to construct a network, while it can potentially overfit to global features, provides a more general framework than applying symmetrization of the network for constructed regions.




\section{Pose estimation}
\label{sec:posestimation}
Additionally, we evaluate our convolutions in the pose estimation task.
In this task, the model aims to recover the relative rotation between two point clouds of the same shape.

\textbf{Dataset.}
We follow Zhu et al.~\cite{zhu2023e2pn} and use the airplane category from the ModelNet40 dataset for this task, composed of $626$ models in the training set and $100$ in the test set.
As Zhu et al.~\cite{zhu2023e2pn}, we also sample the surface of the shapes with $1024$ points to generate the point clouds, which are randomly rotated to form a pair.

\textbf{Experimental setup.}
Existing methods such as \method{EPN}~\cite{chen2021equivariant} and \method{E2PN}~\cite{zhu2023e2pn} rely on the discretization of the SO(3) group to achieve equivariance.
Therefore, their pose estimation network must predict an assignment between the two discrete sets of rotations plus a displacement to cover the full SO(3) space.
Our convolutions, on the other hand, do not rely on a discrete sampling of the SO(3) space and work directly in continuous space.
Therefore, our models only need to predict an assignment between the $4$ reference frames, which can be framed as a contrastive learning problem. 

\textbf{Main results.}
\Cref{tab:pose-estimation} presents the results of this experiment.
We can see that our model, thanks to operating in continuous space, can achieve an angular error orders of magnitude smaller than the existing methods for any number of samples used in the layers of the network.

\begin{table}[]
    \centering
    \small
    \caption{Error in degrees of different methods on the task of pose estimation on ModelNet40.}
    \label{tbl:pose}
    \setlength{\tabcolsep}{5pt}
    \begin{tabular}{ccccc}
        \toprule
       Metrics & \# samp. & Mean(\degree) & Median(\degree) & Max(\degree)\\
       \midrule
       \method{EPN}~\cite{chen2021equivariant} & & 1.10 & 1.36 & 7.06\\
       \method{E2PN}~\cite{zhu2023e2pn} & & 1.20 & 0.96 & 6.71\\
       \multirow{3}{*}{\method{Ours}}  & 4 & $4.4 \times 10^{-5}$ & $6.7\times 10^{-5}$  & $2\times 10^{-3}$ \\
       & 2 & $4.9 \times 10^{-5}$ & $6.7\times 10^{-5}$  & $2\times 10^{-3}$ \\
       & 1 & $4.9 \times 10^{-5}$ & $6.7\times 10^{-5}$  & $2 \times 10^{-3}$ \\
       \bottomrule
    \end{tabular}
    
    \label{tab:pose-estimation}
\end{table}


\section{Dataset details}

\paragraph{Shape classification.}The ModelNet40 dataset~\cite{wu2015modelnet} is composed of synthetic CAD models from $40$ different classes.
The dataset is divided into two splits, where $9,843$ objects are used for training and $2,468$ for testing.
Since each model is composed of multiple faces, we sample $4,096$ points using farthest point sampling on the surface. 
\paragraph{Semantic segmentation: human body parts.}
For training, we use the train split of DFAUST~\cite{bogo2017dynamic} used in \cite{atzmon2022frame, chen2021snarf} and follow Feng~\etal~\cite{feng2023generalizing} to create $15,430$ point clouds by sampling $4,096$ points across the mesh surface. 
The PosePrior dataset~\cite{akhter2015pose} consists of challenging poses significantly divergent from those executed in DFAUST, which we use to test our model for generalization to unseen, out-of-distribution poses. 
Following the procedure of the train set, we derive $3,760$ point clouds with $4,096$ points each from this dataset for testing.
\paragraph{Semantic segmentation: scene understanding.} 
We follow the standard train and validation split of ScanNet~\cite{dai2017scannet} and use color $[r,g,b]$ as input point features in addition to the 3D coordinates. 


\section{Implementation details}
In this section, implementation details are given, and the architecture of the network used is introduced. Classification, pose estimation, and segmentation tasks share the same encoder structure, yet a decoder is used to provide point-wise predictions for the latter. 

\paragraph{Frame computation.}
To compute the local PCA for each point in the point cloud, we select $16$ neighbors using \ac{kNN}.
We compute the covariance matrix from the points and define the frame from the axes of PCA;  \cref{tbl:knn_size} shows an ablation of $k$.

\paragraph{Rotation representation.}
To represent the relative rotations between neighboring points that we give as input to the learnable kernel, we use the 6D representation proposed by Zhou~\etal~\cite{zhou2019continuity}.
However, other viable representations, such as quaternions or rotation matrices, could be used.

\subsection{Network architecture}
For our experiments, our model uses ResNetFormer blocks~\cite{yu2022metaformer} as the main computational blocks in the encoder and an FPN decoder~\cite{kirillov2019pfn} for tasks requiring per-point predictions.
The different point cloud resolutions are computed using Cell Averaging~\cite{thomas2019KPConv}.
In our convolution operation, we define our kernel as a single layer \ac{MLP} with $32$ hidden neurons and GELU activation functions. The output of our network is several feature vectors for each point that correspond to the sampled rotations. We use mean pooling as the projection layer \cref{eq:proj-layer} to get the final output feature per point, but any other pooling can be used. 

\subsubsection{Encoder}
\label{arch:encoder}
The input point cloud is transformed into $n$ down-scaled versions of itself using the Cell Average (CA) method~\cite{thomas2019KPConv}. For the first down-scaling, the size of the voxel cells used in the CA algorithm is a hyper-parameter, $d$, which is then sequentially doubled for each of the following down-scaling steps. The initial features are obtained with a patch encoder similar to the one used in vision transformers~\cite{dosovitskiy2010image}. The patch encoder allows us to extract features from a smaller cell size, which usually increases the model's performance as more points are available while keeping computational costs within limits. We use one additional level with two convolutions for the patch encoder for the classification and segmentation task on DFAUST; for ScanNet20, we skip the patch encoder. The (extracted) initial features are further processed with a set of Metaformer blocks~\cite{yu2022metaformer} before being transferred down to the next down-scaled point cloud via a convolution operation. This procedure is iterated until we reach the final down-scaled point cloud. For pose estimation and classification, we use $n=5$ and mean-aggregation of features in the case of classification. The aggregated feature vector is then passed through a linear layer to perform the final prediction. For the segmentation task on DFAUST and ScanNet20, the features of each level of the encoder serve as input to the decoder, with $n=4$ and $n=5$, respectively. 

\paragraph{Metaformer blocks.} We incorporate the block design defined by Yu \etal~\cite{yu2022metaformer} into our architecture, replacing the attention module of transformers with our point convolution. Each block consists of two residual blocks. In the first one, feature updates are computed using point convolution, while in the second one, updates are determined through a point-wise MLP with two layers. In this MLP, the initial layer doubles the feature count, while the second layer reduces it to the desired output number.


\subsubsection{Decoder}
\label{arch:decoder}
Our Decoder architecture is based on the feature pyramid network proposed by Kirillov~\etal~\cite{kirillov2019pfn}. The input to the decoder is the feature map of the down-scaled point cloud for which a stepwise up-sampling with our point convolutions is employed, progressing from the lowest level to the first down-scaled point cloud. To enhance information and gradient flow, we incorporate skip connections, where features from both the encoder and decoder are summed, producing a distinct feature map for each down-scaled point cloud. Subsequently, each feature map is up-sampled to the initial down-scaled point cloud through a singular up-sampling operation. The resulting $n$ feature maps are then aggregated through summation. If applicable, this feature map is put through a patch decoder, inverting the patch encoder operation. Finally, it is up-sampled to the intended prediction positions by a final convolution and processed by a one-layer MLP to obtain the point-wise predictions.

\section{Experimental setup}

In all experiments, we use AdamW~\cite{loshchilov2018decoupled} as optimizer with a weight decay value of $1^{-4}$ and OneCycleLr~\cite{smith2019super} as learning rate scheduler. Moreover, we employ drop residual paths depending on the depth of the layer~\cite{larsson2017ultra} and gradient clipping for gradient norms exceeding 100. We used label smoothing with a parameter of $0.2$ to prevent overfitting. All models were trained on a single NVIDIA GeForce RTX 3090. The experiment-specific setup is given below. 
 


\paragraph{Classification.}
For the point cloud classification experiments on ModelNet40, we use the encoder architecture explained in~\cref{arch:encoder} with the number of blocks and the number of features for each level equal to [2, 3, 4, 6, 4] and [32, 64, 128, 256, 512], respectively. The initial grid resolution was $d= 0.05$ and a maximum drop rate of $0.2$. All models were trained for 500 epochs using a batch size of 12, with a learning rate of $0.01$ and an initial and final division factor of $100$ and $10000$. We used jitter coordinates, mirroring, and random scale augmentation during training. 

\paragraph{Pose estimation.}
For the pose estimation experiment on the airplane category of ModelNet40, we use the same model as for classification.
Note that the projection layer is omitted for this experiment since equivariance instead of invariance is needed. The model using all four frame elements was trained for 500 epochs with a batch size of 8. Using only 2 or 1 element takes longer for the model to converge; we trained for 2k and 4k epochs with batch sizes of 16 and 64, respectively.


\paragraph{Segmentation.}
For the segmentation task on the DFAUST dataset, we again used the encoder architecture introduced in~\cref{arch:encoder} with two blocks per level and a number of features equal to [32, 64, 128, 256]. We trained all models with a batch size of 32 for 150 epochs. The maximum learning rate was 0.005, with an initial division factor of 10 and 1000 as the final factor. Jitter coordinates are used as augmentation during training; the initial grid resolution was $d= 0.05$, and a maximum drop rate of $0.5$. To get the point-wise prediction, the decoder architecture of \cref{arch:decoder} was employed. 

For the segmentation task on the ScanNet20 dataset, we used the same encoder and decoder architecture as for DFAUSt, described in \cref{arch:encoder} and \cref{arch:decoder}, but used five levels with [2, 3, 4, 6, 4] blocks and 
[64, 128, 192, 256, 320] feature dimensions. The initial grid resolution was $d=0.1$, and all models were trained using 250 batches for 600 epochs using standard augmentations such as jitter coordinates, mirroring, random scaling, elastic distortion, and translation.


\paragraph{Projection layer.} Our proposed method provides features for each sample of the SO3 group; if four samples are used, four features per point result. Hence, we must aggregate those features to get to the final point-wise prediction or before averaging in the classification task to get the point-wise invariant feature vectors. We use mean-pooling over the features corresponding to the same coordinates.

\section{Additional qualitative results}
 \Cref{fig:supp_seq} and \cref{fig:supp_qual} provide additional qualitative results.

\begin{figure*}
    \centering
    \includegraphics*[width=\linewidth]{img/supps_sequ.png}
    \caption{\textbf{Additional Qualitative results.} Global equivariant methods such as \method{VN}, or \method{FA} struggle with out-of-distribution models, especially up-side down models.
    Our method, on the other hand, achieves almost perfect predictions.
    Lastly, \method{MC} also achieves good performance but falls behind our method, as seen in the leftmost columns when looking at the left upper arm prediction.}
    \label{fig:supp_seq}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics*[width=\linewidth]{img/supps_figure.png}
    \caption{\textbf{Additional Qualitative results.} Global equivariant methods such as \method{VN}, or \method{FA} struggle with out-of-distribution models.
    Our method, on the other hand, achieves almost perfect predictions.
    Lastly, as seen in all three examples, \method{MC} also achieves good performance but falls behind our method.}
    \label{fig:supp_qual}
\end{figure*}

\section{Additional ablations}
In this section, we provide ablation experiments to evaluate the robustness of our approach against the number of neighbors used in the PCA computation and against noise and point density. Further, we provide results for allowing different numbers of samples to be used for the approximation of the SO(3) integral during training, called \emph{sample mixing}. Finally, we discuss the effect of stochastic sampling of the point-specific grid on SE(3). 


\paragraph{Sample mixing.}
To achieve the best results with minimal training and inference time, we explored \emph{sample mixing}. Instead of always using the same amount of samples of SO3 elements, the number of samples used changes per step/forward pass and is chosen with the following probability. 1 sample is used with a probability of 50\%, 2 samples with a 35\% and 4 with a 15\% probability. \Cref{tbl:mix-supp-modelnet} and \cref{tbl:mix-supp-dfaust} show the results for the classification and segmentation tasks, respectively. We can see that training with sample mixing and testing with one sample equals or exceeds the performance of training with one sample only for \method{MC} and \method{Ours}. Further, the results are more stable concerning different numbers of samples used during testing than when training with a fixed number of samples. Sample mixing is thus a viable option when limited resources are available. Moreover, training with larger batch sizes becomes feasible by further allowing different numbers of samples not only between but also within batches. \Cref{tbl:time-supp} shows the number of minutes each epoch approximately takes during training with the mixing strategy, 1, 2, and 4 samples. Sample mixing with the proposed probabilities takes, on average, as long as training with two samples while delivering a more robust approximation of the integral.


\begin{table*}
\caption{Results for mixing the number of used samples throughout training for the classification task on the ModelNet40 dataset. 
}
\label{tbl:mix-supp-modelnet}
\setlength{\tabcolsep}{10pt}
\centering
\begin{tabular}{lcrrrrrrrrr}
    \toprule
    \multicolumn{1}{c}{Method} & \multicolumn{1}{c}{\# samp.} & \multicolumn{3}{c}{I / I}  & \multicolumn{3}{c}{I / SO(3)} & \multicolumn{3}{c}{SO(3) / SO(3)} \\
    \cmidrule(l{2pt}r{2pt}){3-5}\cmidrule(l{2pt}r{2pt}){6-8}\cmidrule(l{2pt}r{2pt}){9-11}
    & train $\downarrow$/ test $\rightarrow$ & 1 & 2 & 4 & 1 & 2 & 4 & 1 & 2 & 4 \\
    \midrule
    \method{MC}  & mix & \cbg 84.9 & \cbg 85.7 & \cbg 85.9 & 78.1 & 77.9 & 77.51 & \cbg 86.9 & \cbg 86.8 & \cbg  86.6  \\
    
    \cmidrule{2-11}
    \method{Ours} & mix & \cbg 87.2 & \cbg 86.9 & \cbg 87.0 & 86.3 & 86.2   & 86.3 & \cbg 88.4 & \cbg 88.5 & \cbg 88.4 \\
   
    \midrule
    \method{STD} & & \cbg & \cbg 90.7 &\cbg & \multicolumn{3}{c}{12.3} & \cbg & \cbg 87.5 & \cbg \\
    \bottomrule
\end{tabular}
\end{table*}

\begin{table*}
\caption{Results for mixing the number of used samples throughout training for the segmentation task on the DFAUST dataset. 
}
\label{tbl:mix-supp-dfaust}
\setlength{\tabcolsep}{17.5pt}
\centering
\begin{tabular}{lcrrrrrr}
    \toprule
    \multicolumn{1}{c}{Method} & \multicolumn{1}{c}{\# samp.} & \multicolumn{3}{c}{mAcc} & \multicolumn{3}{c}{mIoU} \\
    \cmidrule(l{2pt}r{2pt}){3-5}\cmidrule(l{2pt}r{2pt}){6-8}
    & train $\downarrow$/ test $\rightarrow$ & 1 & 2 & 4 & 1 & 2 & 4\\
    \midrule
    \method{MC} & mix &  93.8 & 93.7 & 93.7 & \cbg  88.7& \cbg 88.6 & \cbg 88.5 \\
    \cmidrule{2-8}
    \method{Ours} & mix &  94.0 & 94.1 & 94.1 & \cbg 89.2 & \cbg 89.3 & \cbg 89.3\\
    \midrule
    \method{STD} &  & \multicolumn{3}{c}{85.3} & \cbg & \cbg 74.5 &\cbg\\
    \bottomrule
\end{tabular}
\end{table*}

\begin{table}
\caption{Time of 1 epoch in minutes during training with different numbers of samples on the DFAUST dataset.}
\label{tbl:time-supp}
\setlength{\tabcolsep}{15pt}
\centering
\small
\begin{tabular}{rrrrr}
    \toprule
     \# samp. & mix & 1 & 2 & 4 \\
     \cmidrule(l{2pt}r{2pt}){2-5}
      time (min) & 6.1 & 3.6 & 6.1 & 15.0 \\
    \bottomrule
\end{tabular}
\end{table}

\paragraph{PCA computation.}
We also analyze the effect of the receptive field used to compute the PCA for each point on the final performance of the network using one sample to estimate the integral over SO(3).
We can see in \cref{tbl:knn_size} that using a low number of neighboring points makes the resulting frames noisy and hampers the model's performance, becoming similar to the results of \method{MC} using a random grid. 
However, with 16 neighbors, the PCA computation becomes robust, and we do not see significant improvement when we increase this receptive field.

\begin{table}
\caption{Effect of the \emph{k} chosen for the kNN operation in the PCA computation on the model's performance with one frame element on the ModelNet40 dataset.}
\label{tbl:knn_size}
\setlength{\tabcolsep}{16pt}
\centering
\small
\begin{tabular}{ccccc}
    \toprule
    4 & 8 & 16 & 32 & 64\\
    \midrule
    80.6 & 83.1 & 85.5 & 85.4 & 85.9\\
        
    \bottomrule
\end{tabular}

\end{table}

\begin{table}
    \caption{Robustness \wrt noise variations.}
    \centering
    \setlength{\tabcolsep}{19pt}
    \begin{tabular}{cccc}
       \multicolumn{4}{c}{Noise} \\
       \cmidrule(l{2pt}r{2pt}){1-4}
       train & test & mAcc & mIoU \\
       \cmidrule(l{2pt}r{2pt}){1-4}

       \multirow{3}{*}{0.005}
       & 0.005 & \cbg94.8 & 90.6  \\
       & 0.010 & \cbg93.9 & 90.0 \\
       & 0.015 & \cbg38.6 & 25.1 \\
       \cmidrule(l{2pt}r{2pt}){1-4}
       0.015 & 0.015 & \cbg93.2 & 88.2 \\ 
       \bottomrule
    \end{tabular}
    
    \label{tab:abl-robust_noise}
\end{table}
\begin{table}
    \caption{Robustness \wrt density variations.}
    \centering
    \setlength{\tabcolsep}{19pt}
    \begin{tabular}{cccc}
       \multicolumn{4}{c}{Point Density}\\
       \cmidrule(l{2pt}r{2pt}){1-4}
        train & test & mAcc & mIoU\\
       \cmidrule(l{2pt}r{2pt}){1-4}

   \multirow{3}{*}{4096} & 4096 & \cbg94.5 & 89.7 \\
        & 2048 & \cbg93.3 & 87.7 \\
        & 1024 & \cbg30.4 & 20.1 \\
       \cmidrule(l{2pt}r{2pt}){1-4}

        1024 & 1024 & \cbg92.7 & 86.8 \\ 
       \bottomrule
    \end{tabular}
    
    \label{tab:abl-robust_density}
\end{table}

\paragraph{Robustness to noise and density.} We experimented with the robustness of our model \wrt noise and density variations in the input point cloud.
Results on the DFAUST dataset using two samples are reported in \cref{tab:abl-robust_noise} and \cref{tab:abl-robust_density}, respectively.
We can see that our model is robust against increased levels of noise and reduced point density during testing.
However, if the noise increases significantly ($0.015$ std. dev.) or the number of points is reduced substantially ($1024$ points), the PCA computation is affected, and the model's performance decreases.
This can be easily solved by training the model with high noise levels or with a reduced number of points as shown in \cref{tab:abl-robust_noise} and \cref{tab:abl-robust_density}, where the model achieves similar performance to the model trained without point corruptions.





\paragraph{Effects of stochastic sampling}
One of the main contributions of our work is to sample one or two LRF stochastically during training.
How the learning is affected by this sampling boils down to how noisy the gradient estimation for the kernel parameters is.
If this gradient were computed for a single point, the gradient would be noisy.
However, this noise is significantly reduced since the gradient is computed as the expectation over multiple samples, multiple points, and multiple point clouds in the batch.
In our experiments, models trained with 1 LRF or 2 LRF during training perform only marginally worse than those trained with 4.
During testing, sampling 1 or 2 LRF can result in noisy predictions.
However, these predictions remain equivariant since the same sampling of LRF will produce the same results for random SO(3) rotations.



\section{Limitations}
\label{sec:limitations}

Although the proposed convolution operation is local equivariant via the restricted receptive field, when multiple layers are combined in a deep network, the whole model does not become local equivariant and remains global equivariant.
However, from the experiments presented in~\cref{sub-sec:segmentation}, where the network aims to perform local predictions, our model shows robustness to such scenarios, indicating that the model relies on local features to perform the predictions. 


