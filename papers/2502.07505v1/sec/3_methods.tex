\section{Methods}
\label{sec:methods}

In this section, we describe our proposed approach.
First, the reader is introduced to the concept of group equivariant convolutions. Then, our efficient continuous group convolution is described in detail.


%%%%%%%%%%%%%%%%% BACKGROUND
\subsection{Group equivariant convolution}

An intuitive way of thinking about convolutions is the notion of template matching, where a kernel $k$ is 
shifted over a feature map $f$ to detect patterns. In the continuous case, we consider a feature map $f: X \rightarrow \mathbb{R}^c$ as a multi-channel scalar field and $\mathcal{X} = (\text{L}^2(X))^c$ as the space of feature maps over some space $X$. A more formal definition of a convolution layer is then given as a learnable kernel operator $\Phi: \mathcal{X}\rightarrow \mathcal{Y}$ that transforms feature maps $f$ as follows
\begin{equation}
[\Phi f](y) = (f \star k)(y) = \int_{X} f(x) k(x-y) d x,
\end{equation}
with $X=Y=\mathbb{R}^d$, where $d=3$ for point clouds. (Note that the definition given is cross-correlation instead of convolution since this aligns better with template-matching.) It is well known that convolution layers are translation equivariant due to the shifted kernel, i.e., the kernel is only dependent on relative distances: if the input feature map is shifted, the output feature map follows the same transformation. Yet, since relative distances hold directional information that changes under rotations, it is self-evident that a convolution layer is not equivariant to rotations. One solution is to use $\| x- y \|$ as input to the kernel at the cost of losing the capacity to capture directional features.

We say that an operator $\Phi$ is equivariant to a specific Group $G$ if it commutes with group representations on the input and output feature maps, meaning $\forall g \in  G: \rho^\mathcal{Y}(g)\circ\Phi = \Phi \circ \rho^\mathcal{X}(g)$, where $\rho^\mathcal{X}(g)$ is the regular group representation of $g$ that transforms a function $f\in \mathcal{X}$ by shifting its domain via $g^{-1}$. If the output feature map is left unaltered, $\Phi$ is $G$-invariant. 
Various important works in the field of equivariant deep learning~\cite{cohen2019general,bekkers2020bspline,kondor2018generalization} show or conclude that a linear operator $\Phi$ that maps between feature maps on homogeneous spaces $X$, $Y$ of a group $G$, is $G$-equivariant iff it is a kernel operator (also often called integral operator) with a single-valued kernel (only dependent on relative values). Further, considering $Y=G/H$ as quotient space with $H=\{g\in G|g y_0 = y_0\}$ as the stabilizer subgroup $\text{Stab}_G(y_0)$, which consists of group elements that leave a chosen origin $y_0 \in Y$ unchanged, the kernel of a $G$-equivariant $\Phi$ must be invariant towards elements of $H$ (invariance constraint). 

When looking at the concrete example $Y=\mathbb{R}^d$, $G=\text{SE}(d)$, we say $\mathbb{R}^d \equiv \text{SE}(d)/\text{SO}(d)$ is a quotient space with stabilizer subgroup SO$(d)$; an intuition is given in the following.
Since $\mathbb{R}^d$ is a homogeneous space of SE$(d)$, every point $x\in \mathbb{R}^d$ can be reached from the origin $\mathbf{0}\in \mathbb{R}^d$ by a group element, a roto-translation, $(\text{t, R}) \in \text{SE}(d)$. 
In fact there exist several group elements such that $x = (\text{t, R})\mathbf{0} = \text{R}\mathbf{0}+\text{t}$, namely any group element with $\text{t}=x$ regardless of the rotation part as any rotation $\text{R}\in \text{SO}(d)$ leaves the origin unchanged. 
Hence if $Y=\mathbb{R}^d$ and $G=\text{SE}(d)$, the kernel of an $\text{SE}(d)$-equivariant $\Phi$ must be $\text{SO}(d)$-invariant, meaning one could only use isotropic kernels, which severely limits the expressivity of patterns that can be detected e.g. using  $\| x- y \|$ as discussed above. In order to not limit the representation power of the kernel while achieving SE$(d)$ equivariance, the feature maps need to be lifted to 

the group itself $Y=G$ since then the stabilizer subgroup only consists of the trivial element $ H =\{\mathbf{e}\}$, and the kernel is no longer constrained. Note that $Y$ and $X$ do not necessarily have to be the same space.
Consequently, to extend the translation equivariance of convolution layers to arbitrary affine Lie groups 

three types of layers can be used~\cite{bekkers2020bspline}:
\begin{itemize}
    \item \textbf{Lifting layer} ($X=\mathbb{R}^d$, $Y=G$, $H=\{\mathbf{e}\}$): 
    \begin{equation}
    (f \star k)(g) = \int_{R^d} f(x) k(g^{-1}x) dx
    \end{equation}
    For $G=$SE$(d)$, this can be viewed as a template matching various rotated versions of the kernel, creating a feature map for different positions and rotations.
    \item \textbf{Group convolution layer} ($X=G, Y=G, H=\{\mathbf{e}\})$:
    \begin{equation}
    (f \star k)(g) = \int_{G} f(g') k(g^{-1}g') d\mu(g')
    \label{eq:Gconv}
    \end{equation}
    This layer constitutes the convolution on the full group, e.g., it conducts template matching over all possible combinations of positions and rotations from the input and output feature map.
    \item \textbf{Projection layer} ($X=G$, $Y=\mathbf{R}^d$, $H=\text{Stab}_G(\mathbf{0})$):
    \begin{equation}
    (f \star k)(x) = \int_{H} f(x,h')  d\mu(h')
    \label{eq:proj-layer}
    \end{equation}

    For tasks like point-wise classification, the final prediction must be invariant, so feature maps or rotations are projected to their corresponding point in $\mathbb{R}^d$. This layer is omitted for tasks like pose estimation.
\end{itemize}






%%%%%%%%%%%%%%%%% EFFICIENT GROUP CONVOLUTION
\subsection{Efficient group convolution}
Since group convolution layers map between higher dimensional feature maps and must compute the integral over the entire group, they can introduce a computational bottleneck. In the case of 3D point clouds and the affine group $\text{SE(3)} = \mathbb{R}^3 \rtimes \text{SO(3)}$, ~\cref{eq:Gconv} turns into a 6D convolution $(f \star k)(g)$ with $g=(\text{x, R}) \in \text{SE(3)}$, which can be written as a double integral
\begin{equation}
\int_{\mathbb{R}^3} \int_{\text{SO(3)}} f(\text{t, R'})k(\text{R}^{-1}(\text{t} - \text{x}), \text{R}^{-1}\text{R'}) d\text{t} d\mu(\text{R'}),
\label{eq:double-int}
\end{equation}
with $\mu(\cdot)$ being the Haar measure on SO$(3)$. 


In addition to the computational burden of a 6D convolution, another difficulty lies in how to define a grid on SE$(3)$ or, more specifically, on the SO$(3)$ part to compute the integral of ~\cref{eq:double-int}. 
Previous works such as \cite{zhu2023e2pn, chen2021equivariant} have relied on the discretization of SO(3) using platonic solids that assign to each spatial component the same finite grid on SO(3) to make it tractable, yet at the loss of continuity and exact equivariance.
To stay in the continuous domain, similarly to the work of Finzi~\etal~\cite{finzi2020generalizing}, one can use \ac{MC} approximation for both the spatial and rotational part to solve the double integral
\begin{equation}
    \sum_{j} \frac{1}{\lvert H'_j \rvert}\sum_{(\text{t, R'})\in H'_j}  f(\text{t, R'})k(\text{R}^{-1}(\text{t} - \text{x}), \text{R}^{-1}\text{R'}),
    \label{eq:lie-conv}
\end{equation}
\noindent where $j$ are the indices of the points $x_j \in \mathbb{R}^3$ of the point cloud
and $H'_j = \{(\text{t, R'}) | \text{t}=x_j, \text{R'}\in \text{SO(3)}\}$ is the set of SE(3) group elements that result form lifting points $x_j$ to SE(3) by repeating the point coordinate with uniformly sampled rotations. Note that the point cloud is treated as a sparse feature map that defines the sampling of the spatial component.







 Using \ac{MC} approximation can be thought of as defining a random grid on SE(3). Hence, the approximation quality of this integral depends on the number of sampled group elements or, more precisely, on the number of rotations $\lvert H'_j \rvert = O$ sampled per point $x_j$; the approximation error converges towards zero for $O \rightarrow \infty$.
However, sampling $O$ rotations per point increases the model's memory by a factor of $O$.
Moreover, the required computations for the convolution also increase by a factor of $O^2$. Hence, using \ac{MC} results in a trade-off between computational efficiency and preciseness of equivariance property, showing that an efficient grid on SE(3) that allows for exact equivariance with finite rotation elements is crucial to make continuous group convolutions practical for point-based networks.


\textbf{Efficient grid on SE(3).}
To achieve exact equivariance with tractable computational load, we propose a carefully constructed grid $\mathcal{F}(x_j) \subset SE(3)$ specific to each point $x_j \in \mathbb{R}^3$.
%meaning it has different group elements for each point in space. 
Note that while $H_j$ in \cref{eq:lie-conv} was also dependent on $x_j$, the grid was still the same for each point, namely the entire group, where the dependency merely came from approximation by sampling.

We call $\mathcal{F}(x):\mathbb{R}^3 \rightarrow 2^{\text{SE}(3)}$ a frame, which is a set-valued function and maps a point in space to a set of group elements such that $\forall (\text{t, R}) \in \mathcal{F}(x): x = \text{t}$. A frame is called $G$-equivariant if $\forall g \in G: g\mathcal{F}(x) = \mathcal{F}(gx)$. Using $\mathcal{F}(x)$ as grid, we define a 3D sparse point cloud group convolution layer $\Phi_{\mathcal{F}}$ as

\begin{equation}
    \sum_{j} \frac{1}{\lvert \mathcal{F}(x_j) \rvert}\sum_{(\text{t, R'})\in \mathcal{F}(x_j)}  f(\text{t, R'})k(\text{R}^{-1}(\text{t} - \text{x}), \text{R}^{-1}\text{R'}).
    \label{eq:loco-roto-conv}
\end{equation}
$\Phi_{\mathcal{F}}$ thus transforms feature maps $f: X \rightarrow \mathbb{R}^c$, defined on the domain $X = \{\mathcal{F}(x)|x\in \mathbb{R}^3\}$.
Using those definitions, we can formulate the following.
\begin{theorem}
    Let $\mathcal{F}$ be an SE(3)-equivariant frame. Then, $\Phi_{\mathcal{F}}$ is SE(3)-equivariant. 
    \label{theorem}
\end{theorem}
\begin{proof}
    See suppl. mat.
\end{proof}

Since $\mathcal{F}(x)$ can be constructed with local PCA, as explained below, it only consists of a few elements and the amount of computations is significantly reduced.%, while 






\begin{figure}[t]
    \centering
    \includegraphics*[width=\linewidth]{img/main_idea}
    \caption{
    Overview of our convolution operation. 
    Given a central point with an orientation, 
    first, we sample neighboring points. 
    For each point, we use PCA to build a frame from it.
    Then, we sample an orientation from the frame.
    Then, the input to the group convolution kernel is the relative position plus the relative orientations between points.}
    \label{fig:main_idea}
\end{figure}












\textbf{Frame Construction.}
We compute PCA over a region around the point to construct $\mathcal{F}(x)$.
Due to the ambiguity of PCA \wrt the direction of the different axes, we follow Xiao \etal~\cite{xiao2020pca} and Puny \etal~\cite{puny2022frame} and construct 4 different \ac{LRF} by inverting the sign of the different directions.
Given the eigenvectors $[v_1, v_2, v_3]$ of the covariance matrix $C$ of the point coordinates, the frame can be defined as $\mathcal{F}(x) = \{ ( [ \alpha_1 v_1, \alpha_2 v_2, \alpha_3 v_3], t ) | \alpha_i \in \{1, -1\}\}$.
To be equivariant to the SE(3) group, $\mathcal{F}(x)$ is restricted to orthogonal, positive rotation matrices.
This results in a frame with a finite number of elements, $| \mathcal{F} (X) | = 2^{3-1} = 4$.


\textbf{Stochastic Approximation.}
Although $\mathcal{F}(x)$ only has 4 elements, this might still be restrictive for modern state-of-the-art deep architectures used to process large 3D scenes.
Therefore, we propose to perform a stochastic approximation of~\cref{eq:loco-roto-conv} during training by only sampling a subset of the elements of $\mathcal{F}(x)$ for input and output domains of the feature maps.
In particular, we propose randomly sampling two or even only one element of $\mathcal{F}(x)$ for each point $x$ in the point cloud where the convolution will be computed.
Then, during the computation of ~\cref{eq:loco-roto-conv}, only the sampled elements for points $x_j$ are used to approximate the SO$(3)$ integral.
Our approach is illustrated in~\cref{fig:main_idea}.

While using all elements of $\mathcal{F}(x)$ would increase the memory consumption of a standard model by a factor of $4$ and the number of computations by a factor of $16$, sampling $2$ elements would only increase the memory by a factor of $2$ and the computations by a factor of $4$.
More importantly, randomly sampling only $1$ element will maintain the memory consumption and computations equal to the model with standard convolutions.
During testing, since large batches are not necessary, we can use the full frame $\mathcal{F}(x)$ to compute ~\cref{eq:loco-roto-conv}. The error introduced by stochastic approximation by subsampling 2 or 1 element instead of using all 4 is discussed in the supplementary materials.

\textbf{Local vs Global Equivariance.} 
In practice, the locality of the kernel is enforced by calculating the convolution for a local neighborhood $N_x=\{x_j \in \mathbb{R}^3|\|x_j - x\| <r\}$ of $x$ only. 
Equivariance of~\cref{eq:lie-conv} and ~\cref{eq:loco-roto-conv} is ensured on a scale that depends on the receptive field used. 
Since we only consider a small receptive field around each point, our operations become equivariant \wrt rotations of the local geometry within this receptive field. By incorporating several layers in our architectures with increasing receptive fields, the model is able to capture patterns at different scales in an equivariant manner.
Ultimately, the whole model also covers the global equivariance scale since the last layers have an effective receptive field covering the entire scene.








 




