\section{Related work}
\label{sec:related_work}
This section gives an overview of networks that can process unstructured data, such as point clouds, with a focus on specific point architectures that are equivariant to rotations.

\textbf{Point-based neural networks.}
The first neural network architecture specifically designed to process point clouds was PointNet~\cite{qi2017pointnet}. The idea of directly processing point clouds was followed by several works incorporating concepts and designs from convolutional neural networks for images into the continuous domain.
Atzmon~\etal~\cite{atzmon2018pccnn}, Thomas~\etal~\cite{thomas2019KPConv}, and Boulch~\etal~\cite{boulch2020convpoint} propose a convolution operation based on a set of points located inside a receptive field and a correlation function as the kernel function.
Another line of research, including Hermosilla~\etal~\cite{hermosilla2018mccnn} and Wu~\etal~\cite{wu2019pointconv} uses a convolution operation with a kernel function represented by an \ac{MLP}, that takes the relative position between points as input. 
In this work, similarly, we use an \ac{MLP} as our kernel, but with the relative orientation in addition to the relative position between points as input.

\textbf{Rotation equivariant point networks.}
Equivariance or invariance to SE(3) can be achieved by modifying the model's input through data augmentation or by adapting the internal operations of the model to have such equivariance by construction. 
Several works have designed equivariant network architectures by aligning the input point cloud to a reference frame before being processed by the model.
Gojcic~\etal~\cite{gojcic20193DSmoothNet} align local patches of a point cloud to their \ac{LRF} defined by the normal to solve the task of keypoint matching.
Xiao~\etal~\cite{xiao2020pca} uses \ac{PCA} to build a frame to transform the input point cloud and an attention network to aggregate features over the different transformations.
Other works, instead, adopt a different approach, in which they compute invariant local features and use these as input to a convolution operation.
Zhang~\etal~\cite{zhang-riconv-3dv19} use angles and distances as input to a local PointNet architecture to achieve rotation invariance.
Later, Zhang~\etal~\cite{zhang2022riconv2} extended this work with additional local features.
Yu~\etal~\cite{yupositionalfeature2020} also used distances and angles to achieve global equivariance.
However, none of these approaches achieved the goal of our work, local SE(3) equivariance.
Recently, Puny~\etal~\cite{puny2022frame} suggested a general framework to achieve equivariance on any neural network by averaging the model's output over a subset of the group elements. Concurrently with our work, Atzmon~\etal~\cite{atzmonapproximately} introduced a piecewise E(3) equivariant approach applying ~\cite{puny2022frame} to multiple parts proposed by a partition prediction model, where the locality is determined by the partition of the objects.
In this work we use the concepts of~\cite{puny2022frame} to achieve local SE(3) equivariance efficiently without the need for a partition prediction.

Another line of work achieves local equivariance by making the model's internal features \emph{steerable}~\cite{cohen2016steerable}, \,  i.e., the feature values transform predictably as the input transforms.
In 3D, these works usually rely on the theory of spherical harmonics to obtain \emph{steerable} features~\cite{thomas2018tensor, fuchs2020se3transformers, weiler20183dsteercnn}. 
Vector Neurons~\cite{deng2021vector} also uses higher-order features, representing each feature as a 3D vector, to achieve global SO(3) equivariance.
Unfortunately, this increases the memory consumption of the models and restricts the kernel representation used.

More related to our work is the concept of group convolutions~\cite{cohen2016group}.
These operations generalize the concept of convolutions and extend the equivariance to the translation group of standard convolution to any group.
These ideas have been applied to the SE(3) group for voxelized representations, where the group has a finite number of elements~\cite{worrall2018cubenet}, and to point clouds in the continuous domain by discretizing the continuous SE(3) group using the icosahedral group~\cite{chen2021equivariant, zhu2023e2pn}.
Although the computation of such group convolution can be implemented with permutation matrices, a large number of group elements requires a significant computational burden.
To address this issue, Chen~\etal~\cite{chen2021equivariant} proposed a separable convolution allowing for fast computation of the group convolution.
Zhu~\etal~\cite{zhu2023e2pn} instead proposes to use the SO(2) group as the stabilizer subgroup to form spherical quotient feature fields.
Unfortunately, these discretizations require lifting the feature representation to the size of the discrete group used, increasing the memory requirement of the model by a factor equal to the size of the discrete group.
Recent works have suggested solving the group convolution integral by using \ac{MC} sampling on the continuous group~\cite{finzi2020generalizing, Hutchinson2021lietransformer}.
These works randomly sample the group and use farthest point sampling to select a subset from which the integral is approximated with \ac{MC} integration.
However, this approach might require large samples to obtain a reasonable estimation of the integral and hence suffer from substantial memory load.

In this work, we also suggest using group convolutions on the continuous domain to achieve local rotation equivariance.
However, our sampling strategy allows us to solve this integral only with a few samples on the SO(3) group, rendering group convolutions a viable solution to achieve equivariance on standard deep point-based architectures with negligible computational or memory requirements.