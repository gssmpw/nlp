\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_delta,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{float}
\usepackage{subcaption}
\usepackage{cancel}
\usepackage{wrapfig}
% \usepackage{graphicx}

\newcommand{\mt}[1]{\textcolor{cyan}{Mingtian: #1}}
\newcommand{\ruleline}[1]{%
    \par\noindent
    \raisebox{0.1cm}{%
        \makebox[\linewidth]{\textcolor{cyan}{\hrulefill}%
        \hspace{0.8ex}\raisebox{-0.5ex}{#1}\hspace{0.8ex}%
        \textcolor{cyan}{\hrulefill}}%
    }%
}
\title{Towards Training One-Step Diffusion Models Without Distillation}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
\iclrfinalcopy
\author{%
  Mingtian Zhang$^{1}$\thanks{Equal contribution.}, 
  Jiajun He$^{2}$\footnotemark[1], 
  Wenlin Chen$^{2,4}$\footnotemark[1], 
  Zijing Ou$^{3}$, \\
  \textbf{José Miguel Hernández-Lobato$^{2}$, 
  Bernhard Schölkopf$^{4}$, 
  David Barber$^{1}$} \\
  $^1$University College London, 
  $^2$University of Cambridge, 
  $^3$Imperial College London, \\
  $^4$MPI for Intelligent Systems, Tübingen, \\
  \texttt{m.zhang@cs.ucl.ac.uk \ jh2383@cam.ac.uk \ wc337@cam.ac.uk}
}



% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\jj}[1]{{\color{cyan}#1}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.


\begin{document}
\maketitle

\begin{abstract}
Recent advances in one-step generative models typically follow a two-stage process: first training a teacher diffusion model and then distilling it into a one-step student model. This distillation process traditionally relies on both the teacher model's score function to compute the distillation loss and its weights for student initialization. In this paper, we explore whether one-step generative models can be trained directly without this distillation process. First, we show that the teacher's score function is not essential and propose a family of distillation methods that achieve competitive results without relying on score estimation. Next, we demonstrate that initialization from teacher weights is indispensable in successful training. Surprisingly, we find that this benefit is not due to improved ``input-output" mapping but rather the learned feature representations, which dominate distillation quality. Our findings provide a better understanding of the role of initialization in one-step model training and its impact on distillation quality.
\end{abstract}

\section{Introduction}
Diffusion models~\citep{sohl2015deep,ho2020denoising,song2019generative} have achieved remarkable success across various domains~\citep{rombach2022high,li2022diffusion,poole2022dreamfusion,ho2022video,hoogeboom2022equivariant,liu2023audioldm}, with several approaches enhancing generation speed~\citep{jolicoeur2021gotta,liu2022pseudo,lu2022dpm,wang2021deep,de2021diffusion,xiao2021tackling,wang2022diffusion,bao2022analytic,bekas2007estimator,ou2024improving}. Recently, distillation techniques have gained popularity for one-step generation, achieving state-of-the-art results~\citep{zhou2024score}. These methods fall into two categories: trajectory-based distillation~\citep{salimans2022progressive,berthelot2023tract,song2023consistency,heek2024multistep,kim2023consistency,li2024bidirectional}, which integrates multi-step training with distillation, and score-based distillation~\citep{luo2024diff,salimans2024multistep,xie2024distillation,zhou2024score}, which first pre-trains a diffusion teacher model and then distils it into a one-step model.

In this paper, we focus on the latter score-based strategy, as it provides a simpler training scheme. Specifically, we investigate whether a one-step model can be effectively trained without relying on a pre-trained first-stage teacher model. In the following sections, we first introduce the two-stage distillation method and then explore (1) whether a one-step model can be trained without using the teacher's scores and (2) whether it can be trained without initializing with the teacher's weights.

\subsection{Background of Score-based Distillation}

Given data samples $\{x^{(1)}, \dots, x^{(N)}\} \sim p_d(x_0)$, we define a one-step implicit model~\citep{goodfellow2014generative,huszar2017variational,zhang2020spread} as $q_\theta(x_0) = \int \delta(x_0-g_\theta(z))p(z)dz$ to match the data distribution $p_d(x_0)$. Inspired by diffusion models, one can use a set of (scaled) Gaussian convolution kernels $\mathcal{K}=\{k_1,\cdots, k_T\}$ where $k_t(x_t|x_0)=\mathcal{N}(x_t|\alpha_t x_0, \sigma_t^2I)$ and define the Diffusive KL divergence between $q_\theta(x_0)$ and $p_d(x_0)$ as
\begin{align}
    \mathrm{DiKL}_{\mathcal{K}}(q_\theta(x_0)||p_d(x_0))\equiv\sum_{t=1}^T w(t)\mathrm{KL}( q_\theta(x_t)|| p_d(x_t)),
\end{align}
where $q_\theta(x_t)=\int q_\theta(x_0) k_t(x_t|x_0)dx_0$ and $p_d(x_t)=\int p_d(x_0) k_t(x_t|x_0)dx_0$. In addition to the diffusion distillation~\citep{luo2024diff, xie2024distillation}, this divergence has successfully been used in 3D generative models~\citep{pooledreamfusion,wang2024prolificdreamer} or training neural samplers~\cite{he2024training}. For a single Gaussian kernel, the divergence was previously known as \emph{Spread KL divergence}~\citep{zhang2020spread,zhang2019variational}. It is straightforward to show that it is a valid divergence, i.e., $\mathrm{DiKL}_{\mathcal{K}}(q_\theta||p_d)=0\Leftrightarrow q_\theta=p_d$, see \cite{zhang2020spread} for a proof.

The gradient of $\theta$ is derived as follows, considering a single Gaussian kernel for simplicity:
\begin{align}
    \nabla_\theta \mathrm{KL}(q_\theta (x_t) || p_d(x_t)) = \int q_\theta(x_t) \left( \nabla_{x_t} \log q_\theta(x_t) - \nabla_{x_t} \log p_d(x_t) \right) \frac{\partial x_t}{\partial \theta} dx_t.\label{eq:kl:gradient}
\end{align}
However, both $\nabla_{x_t} \log q_\theta(x_t)$ and $\nabla_{x_t} \log p_d(x_t)$ are not directly accessible. Fortunately, since we have access to samples of $p_d$ and $\nabla_{x_t} \log p_d(x_t)$ remains fixed, we can approximate it once with \emph{denoising score matching} (DSM)~\citep{vincent2011connection} using a score network $s_{\psi_1}^{p_d}(x_t,t) \approx \nabla_{x_t} \log p_d(x_t)$:
\begin{align}
    \mathcal{L}_{\text{DSM}}(\psi_1) = \iint \frac{1}{2} \|s_{\psi_1}^{p_d}(x_t,t) - \nabla_{x_t} \log k_t(x_t|x_0)\|_2^2 p_d(x_0)p(x_t|x_0)dx_t dx_0. \label{eq:dsm_loss:pd}
\end{align}
To approximate the score of the student model, we note that since we can efficiently sample from the student model, we can approximate $\nabla_{x_t} \log q_\theta(x_t)$ using a score network $s_{\psi_2}^{q_\theta}(x_t,t) \approx \nabla_{x_t} \log q_\theta(x_t)$, trained with the following DSM loss:
\begin{align}
    \mathcal{L}_{\text{DSM}}(\psi_2) = \iint \frac{1}{2} \|s_{\psi_2}^{q_\theta}(x_t,t) - \nabla_{x_t} \log k_t(x_t|x_0)\|_2^2 q_\theta(x_0)p(x_t|x_0)dx_t dx_0. \label{eq:dsm_loss:q}
\end{align}
Thus, the gradient with respect to $\theta$ is estimated as follows, a method known as Variational Score Distillation (VSD)~\citep{poole2022dreamfusion,wang2024prolificdreamer,luo2024diff}:
\begin{align}
    \nabla_\theta \mathrm{DiKL}(q_\theta (x_0) || p_d(x_0)) \approx \sum_{t=1}^T w(t) \int q_\theta(x_t) \big(s^{q_\theta}_{\psi_2}(x_t,t) - s^{p_d}_{\psi_1}(x_t,t)\big) \frac{\partial x_t}{\partial \theta} dx_t.   \label{eq:kl_gradient}
\end{align}
However, unlike $\nabla_{x_t} \log p_d(x_t)$, which remains fixed, $\nabla_{x_t} \log q_\theta(x_t)$ dynamically changes during training. Therefore, we need to update the score network $s_{\psi_2}^{q_\theta}(x_t,t) \approx \nabla_{x_t} \log q_\theta(x_t)$ at each gradient step when optimizing $\theta$. The full training procedure is detailed in Algorithm \ref{alg:one_step_train}.



\begin{algorithm}
    \caption{Score-based Distillation of  One-Step Generative Models}
    \label{alg:one_step_train}
    \begin{algorithmic}[1]
        \Require Data samples $\{x^{(1)}, \dots, x^{(N)}\} \sim p_d(x_0)$
        \ruleline{\color{cyan}{Stage 1: Train a multi-step teacher diffusion model}}
        \State Train teacher score network $s_{\psi_1}^{p_d}(x_t,t)$ using  DSM until convergence        \ruleline{\color{cyan}{Stage 2: Train a one-step student generative model}}
        \State Initialize the student network with the teacher's score network
        $g_{\theta_{\text{init}}}(\cdot)\equiv s_{\psi_1}^{p_d}(\cdot,t=t_{\text{init}})$
       \For{each training iteration}
            \State Train student score network $s_{\psi_2}^{q_\theta}(x_t,t)$ using DSM
            \State Estimate the DiKL gradient with score network $s_{\psi_2}^{q_\theta}(x_t,t)$ and $s_{\psi_1}^{p_d}(x_t,t)$
         \State Update one-step generator's parameters $\theta$ with the estimated DiKL gradient
        \EndFor
    \end{algorithmic}
\end{algorithm}

\section{Training One-Step Model without Teacher's Score}
In Algorithm \ref{alg:one_step_train}, the DiKL gradient estimation relies on the difference score difference, \(s_{\psi_1}^{q_\theta}(x_t,t) - s_{\psi_2}^{p_d}(x_t,t)\). To eliminate the dependency on the teacher's score network, we observe that the score difference can be computed via the gradient of this ratio:
$
\nabla_{x_t} \log q_\theta(x_t) - \nabla_{x_t} \log p_d(x_t) = \nabla_{x_t} \log (q_\theta(x_t)/p_d(x_t)) 
$
Rather than estimating the two scores separately, we can directly estimate the density ratio between the student and teacher models using class-ratio estimation~\citep{sugiyama2012density,qin1998inferences,gutmann2010noise}.
% \subsection{Class-Ratio Estimation}
Specifically, we first denote distributions \( q_{\theta}(x_t) \) and \( p_{d}(x_t) \) as two conditional distributions \( m(x_t|y=0) \) and \( m(x_t|y=1) \), respectively. With Bayes' rule, we can transform the ratio estimation as a binary classification problem:
\begin{align}
    \frac{q_{\theta}(x_t)}{p_{d}(x_t)} &\equiv \frac{m(x_t|y=0)}{m(x_t|y=1)} = \frac{p(y=0|x_t) \cancel{m(x_t)}}{\cancel{p(y=0)}} \Big/ \frac{p(y=1|x_t) \cancel{m(x_t)}}{\cancel{p(y=1)}} = \frac{p(y=0|x_t)}{p(y=1|x_t)}. \label{eq:class:ratio}
 \end{align}
 where the mixture distribution \( m(x)\equiv m(x_t|y=1) p(y=1) + m(x_t|y=0) p(y=0) \) 
 and the Bernoulli prior distribution \( p(y) \) can be simply set as a uniform prior \( p(y=1) = p(y=0) = 0.5 \). In practice, we sample a batch of data from the $p_d$ and $q_\theta$ and with the labels $y=0$ and $y=1$, we train a neural network $c_{\eta}(x_t,t)$ classifier that conditional on $t$ to learn the probability of $y=1$ given $x_t$, $c^*(x_t,t)=p(y=1|x_t,t)$. The log-ratio can be estimated by 
 \begin{align}
    \nabla_{x_t} \log (q_\theta(x_t)/p_d(x_t))\approx \nabla_{x_t} \log (1-c_\eta(x_t,t))/c_\eta(x_t,t)=\nabla_{x_t} \text{logit}(1-c_\eta(x_t,t)).
 \end{align}
We can then plug in this estimator to Equation~\ref{eq:kl:gradient} to form the DiKL gradient estimation:
\begin{align}
    \nabla_\theta \mathrm{DiKL}(q_\theta (x_0) || p_d(x_0)) \approx \sum_{t=1}^T w(t) \int q_\theta(x_t)  \nabla_{x_t} \text{logit}(1 - c_\eta(x_t, t)) \frac{\partial x_t}{\partial \theta} dx_t, \label{eq:dikl:cle}
\end{align}
In addition to the DiKL, we can use the learned classifier function \( c_\eta \) to define alternative learning objectives. For instance, replacing the logit function with the logarithm yields an objective that minimizes the probability of generated samples being classified as fake. This formulation aligns with the GAN~\citep{goodfellow2014generative, nowozin2016f} across different diffusion timesteps, which is equivalent to minimizing the diffusive JS divergence: 
\begin{align}
    \nabla_\theta \mathrm{DiJS}(q_\theta (x_0) || p_d(x_0)) \approx \sum_{t=1}^T w(t) \int q_\theta(x_t)  \nabla_{x_t} \text{log}(1 - c_\eta(x_t, t)) \frac{\partial x_t}{\partial \theta} dx_t. \label{eq:diJS}
\end{align}
% When optimized to optimum, we have
% \begin{multline}
%       c_\eta(x_t, t) = p(y = 1|x_t, t)  = \frac{ p(y = 1|x_t, t)p(x_t, t)}{p(x_t, t) } \\=  \frac{m(x_t|y=1 )p(y=1)}{m(x_t|y=1 )p(y=1)+ m(x_t|y=0)p(y=0)} = \frac{m(x_t|y=1 )}{m(x_t|y=1 )+ m(x_t|y=0)}
% \end{multline}
% Therefore, the objective Eq. \ref{eq:diJS} corresponds to optimizing 
% \begin{align}
%     & \sum_{t=1}^T w(t) \int m(x_t|y=0)    \text{log}(\frac{m(x_t|y=0 )}{m(x_t|y=1 )+ m(x_t|y=0)})  dx_t.\\
%      =& \sum_{t=1}^T w(t) \int m(x_t|y=0)    \text{log}(\frac{m(x_t|y=0 )}{m(x_t|y=1 )+ m(x_t|y=0)})  dx_t. \\ & \quad \quad \quad \quad\quad \quad\quad \quad+ \sum_{t=1}^T w(t) \int m(x_t|y=1)    \text{log}(\frac{m(x_t|y=1 )}{m(x_t|y=1 )+ m(x_t|y=0)})  dx_t. 
% \end{align}
% The second term does not depend on the parameter and hence can be freely added to the objective without changing the gradient.
% We recognize this as 
% \begin{align}
%   \sum_{t=1}^T w(t)  D_\text{JS}[ m(x_t|y=0) ,  m(x_t|y=1) ]
% \end{align}
This objective was first used in DiffusionGAN~\citep{wang2022diffusion} and has also shown promise in one-step video generation~\citep{lin2025diffusion} from a recent concurrent work. However, unlike DiffusionGAN, which heavily depends on the StyleGAN2 architecture~\citep{karras2020training} with gradient penalty~\citep{arjovsky2017wasserstein}, our method is compatible with a UNet~\citep{ronneberger2015u} generator without requiring additional GAN techniques, while still maintaining stable training. 


Alternatively, rather than minimizing the probability that generated images are classified as fake as used in GAN, we can maximize the probability that they are classified as real. We refer to this approach as \emph{Diffusive Realness Maximization (DiRM)}, and define the loss gradient as 
\begin{align}
    \nabla_\theta \mathrm{DiRM}(\theta) \approx -\sum_{t=1}^T w(t) \int q_\theta(x_t)  \nabla_{x_t} \text{log}(c_\eta(x_t, t)) \frac{\partial x_t}{\partial \theta} dx_t. \label{eq:diRM}
\end{align}
\begin{wraptable}{r}{0.55\linewidth}
\vspace{-0.5cm}
    \centering
    \caption{Sample quality on CIFAR-10. 
    }\label{tab:generation-result}
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{@{}lccc@{}}
        \toprule
        \textsc{METHOD}                                              & \textsc{NFE} ($\downarrow$) & \textsc{FID} ($\downarrow$) & \textsc{IS} ($\uparrow$) \vspace{2pt}\\
        \textbf{Accelerated Diffusion models} \\ 
        \toprule
        EDM \citep{karras2022elucidating}                                       & 35     &  2.04     &  9.84   \\
        DDIM \citep{song2020denoising}                                          & 10     & 8.23     &           -  \\
        DPM-solver-fast \citep{lu2022dpm}                                       & 10     &   4.70    &     -          \\
        AMED-plugin \citep{zhou2024fast}                                        & 5       & 6.61     & - \\
        % Progressive Distillation \citep{salimansprogressive}                & 1     &   8.34    &         8.69       \\
        iCT \citep{songimproved}                                            & 1     &  2.83     &  9.54             \\
        CTM \citep{kim2023consistency}                                          & 1     &  1.98     &  -            \\
        BCM  \citep{li2024bidirectional}                                                                & 1     & 3.10      &  9.45             \\
        sCT  \citep{lu2024simplifying}                                                                & 1     & 2.97      &  -\vspace{3pt}           \\
        \textbf{ Score-based Distillation}        \\
        \toprule                                         
        Diff-Instruct \citep{luo2024diff}  &   1  &   4.53  &     -     \\
        SID $(\alpha=1)$ \citep{zhou2024score} &   1  &   2.03  &     10.02      \\
        SIDA $(\alpha=1)$ \citep{zhou2024adversarial} &   1  &  1.52   &  10.32      \\
        SID$^2$A $(\alpha=1)$ \citep{zhou2024adversarial} &   1  &  1.40   &    10.19    \\
        Diff-GAN \citep{wang2022diffusion} &   1  &   3.19 &     -     \\
        \textbf{ Score-free / Class-ratio-based Distillation (Ours)}        \\
        \toprule 
        DiRM                                    & 1     &  4.87   &  9.85          \\     
        DiKL                                   & 1     &  3.81    &     9.90      \\    
        DiJS                                   & 1     &  2.39   &  9.93   \\
        \bottomrule
    \end{tabular}}
    \vspace{-1cm}
    \end{wraptable}
We implement the proposed methods using the EDM~\citep{karras2022elucidating} codebase (see Algorithm~\ref{alg:one_step_train:score:free} for training details). Our discriminator employs the encoder part of the U-Net, outputting a logit scalar at half the size of the score network, which utilizes a full UNet. The generator  is initialized with EDM pre-training, and experiments are conducted on unconditional CIFAR-10~\citep{krizhevsky2009learning}. Additional details are provided in Appendix~\ref{app:exp}. As shown in Table~\ref{tab:generation-result}, DiJS, without teacher score estimation, outperforms DiKL and DiRM and remains competitive with state-of-the-art one-step generation methods.






% \end{table}



\section{Training One-Step  Model without Teacher's Weights}


In previous results, student models were initialized from the teacher's weights.
Training from random initialization led to mode collapse, see Figure~\ref{fig:img4} for an example of mode collapse. One possible explanation is that mode collapse arises from the training objectives (RKL or JS divergence), a phenomenon also observed in GAN literature~\cite{goodfellow2014generative}. To understand why the teacher’s weights help prevent mode collapse in student model training, we investigate two hypotheses:


\textbf{Function Space Hypothesis}: \emph{Weight initialization provides a more structured latent-to-output functional mapping—i.e., different locations in the latent space are initially mapped to distinct images, preventing mode collapse.}  This hypothesis arises from visualizing initialized samples (see Figure~\ref{fig:img1}), which show that initialization already induces diverse mappings, with the second stage primarily refining these into sharper images. Although intuitive, our findings surprisingly show that functional initialization alone is insufficient to prevent mode collapse. To show this, instead of training the teacher model across different timesteps \( t \) and selecting the \( t_{\text{init}} \) for initialization, we only pre-train the teacher model at the target timestep \( t_{\text{init}} \) and use its weight to initialize the one-step model. This setup ensures identical latent-to-output mappings for the student model at initialization, see Figure~\ref{fig:img2}. However, with this initialization, the student model still exhibits mode collapse early in second-stage training, which suggests that the functional mapping perspective alone does not fully explain one-step model training.

\begin{wraptable}{r}{0.6\textwidth}
\vspace{-0.4cm}
    \centering
    \caption{FID scores for different initialization methods on various datasets.}
    \begin{tabular}{@{}llc@{}}
    \toprule
    Initialization & Initialization Dataset & FID \\ \midrule
    No initialization & - & collapsed \\ \midrule
    Single-level DSM & full CIFAR-10 & collapsed \\ \midrule
    \multirow{4}{*}{Multi-level DSM} & 10 classes in CIFAR-100 & collapsed \\
     & 50 classes in CIFAR-100 & 6.20 \\
     & 90 classes in CIFAR-100 & 6.01 \\
     & full CIFAR-10 & 2.39 \\ \bottomrule
    \end{tabular}
    % \caption{FID scores for different initialization methods on various datasets.}
    \label{tab:init_results}
    \vspace{-0.2cm}
\end{wraptable}

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{./img/resized_init_dsm.png}
        \caption{Single-level DSM Init.}
        \label{fig:img1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{./img/resized_init_diffusion.png}
        \caption{Multi-level DSM Init.}
        \label{fig:img2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{./img/resized_collapse.png}
        \caption{Collapsed Samples}
        \label{fig:img4}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{./img/resized_DIJS.png}
        \caption{DiJS Samples}
        \label{fig:img3}
    \end{subfigure}
    
    \caption{Sample visualizations of different methods, see Appendix~\ref{app:exp} for full images visualizations.}
    \label{fig:init_comparison}
\end{figure}




\textbf{Feature Space Hypothesis}: \emph{Weight initialization provides a rich set of multi-level features learned in training the diffusion, which help prevent mode collapse.} 
To verify this hypothesis and isolate the role of learned features from functional mapping effects, we pre-trained the teacher model on CIFAR-100 while excluding any classes that overlap with CIFAR-10. This ensures that the second-stage generation targets are absent during pre-training, allowing us to focus solely on the contribution of learned features. We then trained the teacher model using progressively larger subsets of CIFAR-100 with (10, 50, 90) classes, creating a setting with increasing feature diversity. Table~\ref{tab:init_results} shows the FID scores of one-step model on CIFAR-10 with different numbers of CIFAR-100 classes used for initialization. We find that when the teacher model is trained on only 10 classes, mode collapse still occurs. However, as the number of training classes increases, the model no longer collapses, indicating that feature richness plays a crucial role in preventing mode collapse. Nevertheless, despite mitigating mode collapse, this initialization strategy achieves an FID of 6.01, which is significantly worse than the 2.39 FID obtained when using CIFAR-10 as the pre-training dataset. This suggests that while feature richness is essential for stabilizing training, functional mapping initialization remains important for achieving higher sample quality.




\section{Conclusion and Discussion}
In this paper, we investigate training a one-step diffusion model without a pre-trained teacher and propose score-estimation-free methods for training one-step generative models. Additionally, our study identifies key pre-training components, highlighting the role of feature richness in preventing mode collapse and the necessity of functional mapping for high-quality samples. Future work could explore unsupervised or self-supervised pre-training, in addition to diffusion pre-training, to enhance feature diversity and improve one-step models across modalities like images, audio, or videos.

\section*{Acknowledgments}
MZ and DB acknowledge funding from AI Hub in Generative Models, under grant EP/Y028805/1. JH is supported by the University of Cambridge Harding Distinguished Postgraduate Scholars Programme. ZO is supported by the Lee Family Scholarship.  
JMHL acknowledges support from a Turing AI Fellowship under grant EP/V023756/1. 
\bibliography{iclr2025_delta}
\bibliographystyle{iclr2025_delta}
\newpage
\appendix
\section{Algorithm}
\begin{algorithm}
    \caption{Score-free Training of  One-Step Generative Models}
    \label{alg:one_step_train:score:free}
    \begin{algorithmic}[1]
        \Require Data samples $\{x^{(1)}, \dots, x^{(N)}\} \sim p_d(x_0)$
        \ruleline{\color{cyan}{Stage 1: Train a multi-step teacher diffusion model}}
        \State Train teacher score network $s_{\psi_2}^{p_d}(x_t,t)$ using  Eq.~\ref{eq:dsm_loss:pd} until convergence        \ruleline{\color{cyan}{Stage 2: Train a one-step student generative model}}
        \State Initialize the student network with the teacher's score network
        $g_{\theta_{\text{init}}}(\cdot)\equiv s_{\psi_1}^{p_d}(\cdot,t=t_{\text{init}})$
       \For{each training iteration}
            \State Estimate the ratio $r_\eta$ using Eq.~\ref{eq:dikl:cle} or Eq.~\ref{eq:diJS} or Eq.~\ref{eq:diRM}
            \State Estimate the DiKL gradient (Eq.~\ref{eq:kl_gradient}) with the ratio network $r_\eta$
         \State Update one-step generator's parameters $\theta$ with the estimated DiKL gradient
        \EndFor
    \end{algorithmic}
\end{algorithm}
\section{Experimental Setup and Additional Results} \label{app:exp}
We conduct all our experiments on a single Nvidia H100-80GB GPU. The generator is initialized using the EDM pre-trained model from \url{https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-cifar10-32x32-uncond-vp.pkl}. We adopt the variance-exploding (VE) parameterization, consistent with EDM~\cite{karras2022elucidating} for the corresponding settings. Additionally, we apply non-leaky data augmentation~\cite{karras2020training}.

Our training setup includes a batch size of 64, an exponential moving average (EMA) decay of 0.5, a learning rate of 0.00001, and a fixed timestep $t_{\text{fix}} = 2.5$ with weight function $w(t) = \sigma_t^2$. 

For each generator update, we take one gradient step for ratio estimation to ensure efficient training. We observed that multiple-step updates can accelerate generator convergence without introducing instability—unlike GANs, where multiple ratio updates often cause training instability. However, multiple ratio steps significantly slow down the overall training process. Therefore, we use a single-step gradient update in all our experiments, which is consistent with the settings in~\cite{luo2024diff,zhou2024score} and leave the exploration of multi-step ratio estimation for future work.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{img/init_diffusion.png}
    \caption{Visualization of the samples from the multi-level DSM Initialization}
    \label{fig:init_diffusion}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{img/init_dsm.png}
    \caption{Visualization of the samples from the single-level DSM Initialization}
    \label{fig:init_dsm}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{img/collapse.png}
    \caption{Visualization of the collapsed samples}
    \label{fig:collapse}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{img/DIJS.png}
    \caption{Visualization of the DiJS samples (FID=2.39, IS=9.93)}
    \label{fig:dijs}
\end{figure}

\newpage


% \begin{align*}
%     &\nabla_\theta \mathrm{DiKL}(q_\theta (x_0) || p_\phi(x_0)) \approx \sum_{t=1}^T w(t) \int q_\theta(x_t)  \nabla_{x_t} \text{logit}(1 - c_\eta(x_t, t)) \frac{\partial x_t}{\partial \theta} dx_t;\\
%     &\nabla_\theta \mathrm{DiJS}(q_\theta (x_0) || p_\phi(x_0)) \approx \sum_{t=1}^T w(t) \int q_\theta(x_t)  \nabla_{x_t} \text{log}(1 - c_\eta(x_t, t)) \frac{\partial x_t}{\partial \theta} dx_t;\\
%      &\nabla_\theta \mathrm{DiRM}(\theta) \approx -\sum_{t=1}^T w(t) \int q_\theta(x_t)  \nabla_{x_t} \text{log}(c_\eta(x_t, t)) \frac{\partial x_t}{\partial \theta} dx_t
% \end{align*}


\end{document}
