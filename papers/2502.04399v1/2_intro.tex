\section{Introduction}
\begin{figure}[ht]
\centerline{\includegraphics[width=1.0\linewidth]{figure/introduction.png}}
\caption{
% \broken{
An illustration of the proposed vehicle scheduling framework that integrates order fulfillment and model fine-tuning tasks in a smart city environment: Box 1 shows idle vehicles dispatched to neighboring areas in preparation for future tasks, while Box 2 illustrates vehicles providing services to order customers. Box 3 depicts the process where a vehicle retrieves the latest model and data from the nearest RSU, fine-tunes the model during movement, and uploads the updated model to a nearby RSU. Each RSU, equipped with a server, stores a complete base model, enabling vehicles to perform real-time fine-tuning as they collect data and transfer the refined models to other RSUs.
% }
% \xiaoxi{Boken, please change this figure, adding icons to show model fine-tuning tasks, e.g., data is distributed at each PoI (an icon of RSU equipped with servers), a full base model is  also stored at each RSU, and each vehicle fetches the model and collects the data from the RSU that is closest to the vehicle's current location, to do model fine-tuning when it is moving, and then pushes the updated model to another RSU where it arrives later. --- Then, add another figure (Fig. 2) with two sub-figures: (left) showing a request not fulfilled by a deadline will become zero utility for the vehicle; (right) showing increased delay to collect the data will gradually decreases the inference accuracy of using such data for model fine-tuning.
}
\label{fig:system overview}
\end{figure}

%% Smart city -> MSC, VCS, general scenario introduction: vehicles collect data for AI-driven applications using its mobility and the assistance from RSUs equippeed with edge servers which can store data for centain periods of time and some base foundation models. 
% So I need to synergize the current two paragraphs into one first.

%% Applications: Introduce what types of AI applications are important and innovative for smart city development. Provide some envisioned future scenarios and the practical cases of IVI systems that can support these applications. 

With the rapid growth of urban population worldwide, urban management faces increasing challenges, giving rise to the concept of smart cities, which aim at improving urban lives through environmental monitoring \cite{EnvironmentalPollution}, traffic control \cite{PervasiveandMobileComputing}, healthcare \cite{ExpertSystems}, etc. Urban data is essential for smart city applications, and how to obtain and use data effectively is a key issue. Fortunately, mobile crowdsensing (MCS) \cite{ITJ19} provides a useful way to collect data, making use of users' mobile devices (or users themselves) as sensing units to complete large-scale and complex social sensing tasks. Compared to MCS, the drawbacks of using fixed devices for data collection include mainly high installation and maintenance costs, as well as very limited coverage \cite{BigData}. 
%In the context of the rapidly expanding number of mobile devices, such as mobile phones and smartwatches, MCS has gained significant attention in recent years and has become an appealing paradigm for urban sensing. 
%
Building on the success of MCS, vehicle crowd-sensing (VCS) present unprecedented opportunitiesleveraging the mobility of vehicles, including unmanned and electric types, equipped with high-precision sensors, can collect various types of data, such as air quality data, traffic conditions, and street view images, to assist government agencies in better city management. 
Notably, ride-hailing vehicles are particularly advantageous for VCS tasks, due to their centralized ride-hailing platform management, which reduces the cost of deploying and executing crowd-sensing tasks, and utilizes the data and computing resources from ride-hailing vehicles to maximize the VCS task utilities.


\noindent {\bf Opportunities and challenges.} In the meanwhile, foundation model (FM)-powered AI applications have revolutionized numerous aspects of human lives, including healthcare, education, industry, etc. FMs, e.g., BERT, GPT-4, ViT, serve as foundation for different downstream tasks in languages, vision, graph processing, and multimodal applications. Among them, a notable emerging category is urban foundation models (UFMs), which are trained on diverse modalities of urban data and are designed to interpret them effectively. UFMs supported applications include environmental monitoring, urban planning, energy management, and our envisioned future in-vehicle infotainment (IVI) applications\cite{you2024raccoon}. %~\xiaoxi{cite Yufei's bigcomm}. 
While a substantial body work has developed various UFMs and downstream tasks~\cite{zhang2024urban},~\cite{mai2023opportunities},~\cite{lu2024ai},~\cite{gao2024survey},
% ~\xiaoxi{Boken, cite at least 3 surveys}
optimization-based strategies that leverage existing infrastructures of MCS and ride-hailing vehicles to {\em execute} UFM-based tasks for {\em utility maximization}, along with fulfilling traditional order serving tasks, are under-explored. This is mainly due to: (i) UFMs trained by various, multi-modal urban data still require fine-grained adaptation for distinct downstream urban tasks; but fine-tuning using spatio-temporal heterogeneous data and performed by a large number of moving vehicles typically needs complex quantitative modeling to maximize the VCS utility. (ii) Deploying model fine-tuning has to be balanced with the conventional order tasks of ride-hailing vehicles. %(iii.) In addition to performing UFM fine-tuning, ride-hailing vehicles have to balance the utility of fulfilling order serving, and these two tasks are coupled in that they may encourage the vehicles to arrive at different locations, resulting in diverging optimal routes for the vehicles. 
%% Transition to VCS's pros and cons to motivate: they do model fine-tuning on the way, rather than pushing all the data to the RSUs or the cloud. RSUs should be shared by vehicles rather than processing specialized and multiple computational-intensive tasks. 
%% But ride-hailing cars cannot give up their major tasks, order serving. --> therefore, we consider joint task optimization.

\begin{figure}[ht]
\centering
\begin{minipage}[t]{0.48\linewidth}
  \centering
  \includegraphics[width=\linewidth]{figure/order_timeout.png}
  \caption*{(a)}  % 子图a的标题
\end{minipage}%
\hfill
\begin{minipage}[t]{0.48\linewidth}
  \centering
  \includegraphics[width=\linewidth]{figure/fine-tune-delay.png}
  \caption*{(b)}  % 子图b的标题
\end{minipage}
\caption{
% \broken{
(a) Illustrates how an order request that is not completed within the specified deadline results in zero utility for the vehicle.
(b) Depicts how an increasing delay in data collection progressively diminishes the inference accuracy when such data is used for model fine-tuning.}
% }
\label{fig:impact_on_model}
\end{figure}


\noindent {\bf Model Fine-Tuning and Order Serving Tasks Scenario.} In this work, we take the first attempt to comprehensively study the scenario where ride-hailing vehicles participate in VCS tasks using parameter efficient fine tuning (PEFT) techniques~\cite{han2024parameter} to adapt distinct UFMs, while maintaining the opportunities to pick up passengers for order serving. As illustrated in Fig.~\ref{fig:system overview}, each vehicle can cruise in the city and gain monetary utilities in both tasks. There are a set of data points of interest (PoI) distributed in cities, which represent the locations of RSUs equipped with edge servers, where any one or more types of labeled dataset can be generated in real-time nearby and stored. Due to the large volume, data stored in the RSU server can be discarded in a certain period of time. In practice, these data can be descriptive features and feedbacks (labels) of recommendation or generative AR applications, generated by nearby visitors or residents. They can also be traffic/environment monitoring data with labels generated by running efficient deep learning model inference deployed in the RSU. The government or any company that collaborates with the ride-hailing vehicle company has multiple types of VSC tasks to fulfill, each of which needs certain locations of data for fine-tuning UFMs. Whenever a vehicle chooses to perform a VSC task, it needs to download the corresponding UFM from the cloud, move to associated locations for collect real-time data, fine-tune the Low-Rank Adapters (LoRA) of the UFM on the route, and push the fine-tuned adapters to a nearby RSU or the cloud for the task owner to use in UFM updates and inference.
%that need to be sent to the data center, %but the LTE network is not available in some situations \cite{TMC19}. 
Meanwhile, passengers' order requests also emerge at various times and locations, waiting for ride-hailing vehicles to serve. Therefore, at any given time, each vehicle can cruise to a certain location (squared grid), pick up the passengers to serve the orders before order acceptance deadlines, and/or collect data from a PoI along the route, or just stay in its current place. As depicted in Fig.~\ref{fig:impact_on_model}, both delays in order acceptance and in data collection for fine-tuning can significantly impact the vehicle's utility. Our goal is to provide best next-step location decisions on behalf of each vehicle, to maximize the overall utility of all the vehicles in performing the joint tasks of order serving and UFM model fine-tuning. 

Despite promising benefit that can be brought to smart city development by leveraging UFMs, PETF techniques, and the advantages (mobility, resources) of ride-hailing vehicles, there are three types of technical challenges that we need to address. 

\begin{itemize}
\item{{\bf High-dimensional states in vehicle location planning for spatio-temporal heterogeneous UFM fine-tuning.} 
In this work, our goal is to maximize the long-term, overall utility of all the tasks that fulfilled by the ride-hailing vehicles. Given the uncertain effects of different factors on the performance of model fine-tuning using urban data, a natural and advantageous choice of optimization framework is reinforcement learning (RL), which can effectively learn the optimal online decisions through sequential interactions with dynamic environments.
However, employing global information for centralized decision-making frequently results in state space expansion and poor scalability. Distributed decision-making methods, e.g, multi-agent RL (MARL), prove more apt in addressing these issues when compared to centralized approaches. But an associated challenge arises, i.e., the optimization performance can be suboptimal, resulting from independent decisions based solely on local states. Moreover, the widely adopted performance metric for UFM-based downstream is inference accuracy, but the controllability of our framework is in vehicles' locations and their choices of tasks. The underline mapping between our decisions and the final model accuracy can be affected by various factors in uncertain forms. This results in complex, high-dimensional states, and thus potentially slow convergence of MARL algorithms.} %Moreover, the efficient capture of unique characteristics pertaining to each order and data PoI, including price, destination, data freshness, etc., presents an additional challenge.}

\smallskip
\item{{\bf Inconsistent geo-distributions of data PoIs and orders.} %The first challenge is that the distributions of PoIs and orders may not be consistent. 
Ride-hailing orders are mostly concentrated in certain areas of the city, such as the central business district. There may be very few ride-hailing orders in remote areas, such as the outskirts of the city. Unlike order-serving, many UFM based VCS tasks require vehicles to collect data from every corner of the city~\cite{ding2021multi}, ~\cite{chen2016intelligent}, ~\cite{luo2019data}.
% ~\xiaoxi{Boken, cite more}
 There is a potential scenario where increased attention is directed towards PoIs located in remote suburban areas. An example of such a scenario is the collection of air quality data from environmental monitoring stations situated in forested regions. Collecting data distributed in areas beyond the city center in a timely manner becomes challenging when vehicles are overly focused on seeking ride orders and concentrated in the city center. To simultaneously enhance overall the efficiency of order fulfillment and data collection, it is imperative to strategically allocate vehicles among different regions based on the distribution of orders and data points. In order to increase the effectiveness of MARL algorithms, it is crucial to select the right association between an algorithmic ``agent'' and an environmental item (a location, a vehicle, or a task). Besides, adopting MARL may fall short of handling this inconsistent geo-distributions and balancing the vehicles onto different location, as each agent's decisions may only focus on its local perspective, while allowing full environmental states for each vehicle hinders the scalability and violates our original goal to mitigate the high-dimensionality of state space.}%However, the complexity involved in establishing suitable mathematical models renders traditional optimization methods unsuitable for addressing such diverse and time-varying distribution scenarios. This necessitates the development of novel algorithms specifically designed for data collection in the context of ride-hailing services.}

\smallskip
\item{{\bf Different utility characteristics and requirements of order-serving and model fine-tuning.} %The second challenge is that order-serving and data-collecting are two tasks with different purposes. 
Since each order-serving task has a pickup deadline and a fixed route determined exogenously (e.g., shortest path or minimum travel time), the objective is to maximize high-value order completions within certain distance constraints. In contrast, fine-tuning FMs requires vehicles to collect fine-tuning data of sufficient {\em volume and quality}. Achieving sufficient data volume may involve guiding the RL agent to visit data-rich locations or multiple locations for broader collection. Data quality factors may further include similarity to the base foundation model's training data and alignment with downstream tasks' inference data. %Owning to network transmission limitations, devices often need to maintain a certain distance from the PoI or stay in place when collecting data, which results in non-overlapping order-serving time and data-collecting time for a vehicle. Vehicles providing services to passengers will take up time for data collection, and vice versa. 
Moreover, order serving and data collection tasks have different {\em time-sensitivity}. Long delays in response to an order prompt the passenger to cancel the request, rendering it an invalid order. Differently, since fine-tuning data is generated in real time, it possesses a characteristic of {\em data freshness} which should effect the model performance in a fine-grained manner. For instance, inference accuracy of a fine-tuned model probably benefits from vehicles that promptly reach data collection points and quickly update fine-tuned PEFT adapters to minimize latency between fine-tuning and inference. These varying demands add complexity to RL agents' learning processes in balancing the gains of the two jointly executed tasks.}

\end{itemize}
%The second challenge is that both ride-hailing orders and PoI data can be somewhat time-sensitive. \tianxiang{in fact, this challenge is tradeoff between timely poi and orders, should rewrite} When a passenger submits an order, the platform finds a nearby vehicle to provide the service. If no vehicle takes the order for a long time, the passenger may cancel the order resulting in a failed order. It is worth noting that the value of data collected from PoIs may also decrease over time. For example, the use of vehicles to collect data generated by POIs, such as real-time images recorded by CCTV cameras \cite{INFO20}. When these data are needed to make real-time decisions for city management, it is natural to expect the data collected to be freshly generated. In fact, there is a trade-off between order serving and data collection. When the focus is too much on taking orders it may result in not enough fresh data being collected or not enough data being collected, and vice versa. In addition, the two behaviors of order taking and data collection are coupled to dispatching decisions. When a vehicle is dispatched to an area with more orders, it is more likely to be successfully matched with an order; when the data in the area is fresher, the value of the data it can collect will be higher. 
%（耦合性和tradeoff， 平衡，所有需要设计一个online joint op的框架，加在第三个挑战讲%完之后，不光是online优化，还有大规模等问题）
%{可以说这两种任务的目标是不同的。其中一个是尽可能的多的为需要服务的地方提供服务，另一个则希望车辆能够收集足够多、满足质量要求的分散在城市各处的信息。两种不同甚至会互相冲突的任务结合在一起可能对会对其中某个任务造成影响，很难在不损失接单效果的前提下满足较高的传感性能。且问题是一个长期决策问题，需要考虑订单、数据分布以及每一辆车的状态。}
% \begin{itemize}
% \item[$\bullet$] {\bf Complex and dynamic environmental state and decision spaces.} 
% The third challenge entails the acquisition of the environmental state and the coordination of vehicle decisions. Employing global information for centralized decision-making frequently results in state space expansion and poor scalability. Distributed decision-making methods prove more apt in addressing these issues when compared to centralized approaches. An associated challenge arises from the potential suboptimal optimization performance resulting from independent decisions based solely on local states. Moreover, the efficient capture of unique characteristics pertaining to each order and PoI, including price, destination, data freshness, etc., presents an additional challenge.

% The third challenge lies in the often large size of the fleet. Algorithms based on single-agent reinforcement learning will stack the states of each vehicle, making the state space explode and seriously affecting the training results. If multi-agent reinforcement learning is used, how to coordinate the agents due to the existence of interference between agents is also an important issue. 
%\end{itemize}

% The appeal challenges require us to design a joint online optimization framework that helps vehicles make online decisions with real-time information. 
To address the above identified technical challenges simultaneously, we propose an online optimization framework based on multi-agent reinforcement learning, fusing several customized techniques, in particular, to handle the complex structure introduced by vehicles' mobility, spatio-temporal heterogeneity in environmental data, as well as the coupled and possibly conflicting interests in performing the two types of tasks. 
%The deployment of RL-based methods in varied and dynamic urban settings can pose challenges related to agent collaboration, state space expansion, and long-term optimization problems. To enhance our optimization objectives, the incorporation of novel technologies becomes imperative. In this paper, 
To achieve this, we make the following contributions.

\smallskip
{\em Firstly}, to make full use of edge computing, vehicle-road collaborative infrastructures, and the emerging foundation model related technologies, we propose a new scenario that can benefit various types of smart city applications. We then mathematically model our online joint optimization of order-serving and vehicular model fine-tuning. To enhance our method's applicability in smart city contexts, our decision variables include moving decisions and data collecting decisions for {\em idle} vehicles rather than merely accepting orders, which should strike a balance in gaining high order utility and crowdsensing quality. This approach seeks to exploit the vehicles' untapped potential for both dynamic exploration of orders and service for data collection, diverging from current platforms that limit decisions to route planning for order serving. Besides, we integrate the data freshness consideration into model fine-tuning tasks. Our optimization objective is then defined as the weighted sum of the driver's total utility derived from both order serving and VCS tasks. %, representing a  quality-of-service (QoS). %In our optimization problem, vehicles need to make collaborative decisions to achieve reasonable dispatching based on order and PoI distributions and balance the benefits of order-serving and VCS.
% We illustrate the dependencies between order serving, data collection, and vehicle scheduling. 
% The vehicles achieve reasonable dispatching based on orders and PoI distributions, and provide services for orders or collect data from PoIs at appropriate times.
%\tianxiang{the content after should be rewritten, the benefit of rl and marl should be a clause rather than whole sentences.} %{In addition, reinforcement learning algorithms are applicable to our problems because they can handle non-static, non-deterministic environments and make online decisions based on long-term returns. Furthermore, since the actual fleet decision-making is a coordinated and cooperative process, multi-agent reinforcement learning (MARL) has shown better performance than single-agent reinforcement learning. We treat each vehicle as an agent and use multi-agent reinforcement learning algorithms to learn how to make decisions in a dynamically changing environment.} 

%{First, in order to make decisions in an environment with dynamic changes in orders and PoIs distribution, we treat each vehicle as an agent and use multi-agent reinforcement learning algorithms to learn how to make decisions in a dynamically changing environment. }

\smallskip
{\em Secondly}, we propose an MARL framework that captures the interaction between vehicles and the urban environment by graph neural networks (GNN). The relationship between vehicles and environmental factors is conceptualized as a topology graph structure. Then, we use relational graph convolutional networks (R-GCN) to encode the state space and provide a global view for the agent through information propagation on the graph topology. GNN helps agents capture global information while ensuring that the input dimension of the agent's policy network is fixed. 
% \broken{
To further optimize the fine-tuning process, we introduce the RankTuner module, which dynamically adjusts the LoRA rank to balance fine-tuning accuracy and efficiency.
% }
This ensures that agents can adaptively approach optimal performance in unknown and evolving environments. Diverse reward functions tailored to specific decisions are designed within the framework. 
% \xiaoxi{We need to enhance this part}
% Second, we consider that vehicles can take actions including dispatching, orders serving, and data collection, and design three reward function calculation methods for the three actions. In order to avoid the dimensional explosion problem caused by large-scale agents and better coordinate the actions between agents, we introduce mean-field multi-agent reinforcement learning (MFMARL). MFMARL considers the interaction between each agent and its neighbors, reducing the complexity of the interaction between agents. 
%{Second, we consider the freshness of the data collected from PoIs as a factor that affects the utility of the data. We evaluate the utility of the collected data by combining the volume of the collected data and the AoI. And we take the weighted sum of the total revenue of the driver and the utility of the data as our optimization objective, and max the weighted sum using the multi-agent reinforcement learning in expectation of achieving a trade-off on the two metrics.}


\smallskip
{\em Finally}, we design a simulator to capture the environmental dynamics and complex vehicles' activities. We then evaluate our algorithm using the real-world New York Taxi dataset to simulate ride order generation and conduct extensive experiments of UFM fine-tuning for various tasks. Through comparative analysis with baselines, our method demonstrates superior performance in both order-serving and model fine-tuning, as well as the aggregated utility.
% \xiaoxi{I need to enhance this contribution once our experiments are done.}

%{Finally, for the training problem of large-scale agents, we introduce mean field multi-agent reinforcement learning to solve the coordination problem among agents. }

\smallskip
We organize the rest of the paper as follows. The related work will be introduced in Section II. Then we present our formulation in Section III, as well as our MARL solution method in Section IV. In Section V we describe the experimental setup and show the experimental results. In Section VI, we summarize and conclude the full paper.


